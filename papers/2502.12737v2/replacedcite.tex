\section{Related Work}
\label{sec:literature}

\paragraph{Knowledge Base Question Answering}

Most KBQA 
%(a.k.a. knowledge graph question answering____, KGQA) %\footnote{This problem is also referred to as knowledge graph question answering____. We use KBQA to refer to both problems, since most KBs are organized in a graph.}  
solutions use {information retrieval-based} (IR-based) or {semantic parsing-based} (SP-based) methods____. IR-based methods construct a question-specific subgraph starting from the retrieved entities (i.e., the \emph{topic entities}). They then reason over  the  subgraph to derive the answer. SP-based methods focus on transforming input questions into logical forms, which are then executed to retrieve answers. %Compared to IR-based methods, SP-based methods can produce a more interpretable reasoning process through converting the natural language questions into executable logical forms. Moreover, 
SOTA solutions are mostly SP-based, as detailed next.
%on popular benchmarks (e.g., GrailQA____ WebQuestionsSP____) 

%JHL1: does SP-based = generation-based approach? does IR-based = ranking-based (in the intro)? it seems like these things are all the same, but we have two different terms; let be more consistent. Also, it doesn't look like IR/ranking based method is all that important to us, so let's drop that discussion in the intro and focus on contrasting our method to generation-based/SP-based models


% SP-based methods can be further categorized into ranking-based and generation-based methods____. 

% \jz{Two sentences to explain ranking-based and generation-based methods, rep.} \sxfix{Ranking-based methods perform path traversal and ranking in the KB, starting from the retrieved entities____. Generation-based methods directly transform the input question into the target logical form using a Seq2Seq model____.}

\paragraph{KBQA under I.I.D. Settings}

%Benefiting from the powerful natural language understanding and logical form generation capabilities of Large Language Models (LLMs), 
Recent KBQA studies under I.I.D. settings fine-tune LLMs to map input questions to rough KB elements and generate approximate logical form drafts____. %, exploiting LLMs' semantic capabilities to understand input natural language questions. 
The approximate (i.e., inaccurate or ambiguous) KB elements are then aligned to exact KB elements through a subsequent retrieval stage. These solutions often fail over test questions that refer to KB elements unseen during training. While we also use LLMs for logical form generation, we ground the generation with retrieved relations, entities, and  schema contexts, thus addressing the non-I.I.D. issue. 
%Our model first retrieves KB elements through schema-guided retrieval, and then uses the retrieved KB elements along with their schema context to guide the generation of logical forms. This approach enables better generalization to questions containing unseen knowledge base elements, while also enhancing performance under the i.i.d. setting.


\paragraph{KBQA under Non-I.I.D. Settings}

Studies considering non-I.I.D. settings can be largely classified into \emph{ranking-based} and \emph{generation-based} methods. 

Ranking-based methods start from retrieved entities, traverse the KB, and construct the target logical form by ranking the traversed paths.  
%Ranking-based methods reduce the search space based on the KB structure and the retrieved entities. 
____ enumerate and rank all possible logical forms within two hops  of retrieved entities, while ____ incrementally expand and rank paths from retrieved entities. % to obtain the target logical form. %They then obtain the candidate logical form that best matches the question by ranking. %They evaluated both supervised fine-tuning LMs (e.g., T5____, BERT____) and few-shot in-context learning LLMs (e.g., Codex____) as the partial logical form discriminators. 

Generation-based methods transform an input question into a logical form using a Seq2Seq model (e.g., T5____).
They often use additional contexts beyond the question to augment the input of the Seq2Seq model and enhance its generalizability. For example,____ use  top-5 candidate logical forms enumerated from retrieved entities as the additional context. 
____ further use top-ranked relations, \emph{disambiguated entities}, and classes (retrieved \emph{separately}) as the additional context. ____ use connected pairs of retrieved KB elements. 

Our \model\ is generation-based. We use schema contexts (relations and classes) from retrieved relations and entities, rather than separate class retrieval (as in ____) which could introduce noise. We also defer entity disambiguation to the logical form generation stage, thus avoiding error propagation induced by premature entity disambiguation without considering the generation context, as done in existing works outlined below. 

%employ an additional middle-grained component that converts the retrieved KB elements into connected pairs of KB elements. A Seq2Seq model then transforms the concatenation of the question, the retrieved KB element pairs, and logical form sketches generated based on the question into the target logical form. 

%The methods above retrieve KB elements to serve as additional contexts to enable the models to generalize in non-I.I.D. settings. 
%Existing ranking-based and generation-based methods limit the subsequent logical form search space through KB element retrieval, thereby improving generalization capability. 
%However, errors in knowledge base element retrieval can directly mislead the logical form generation stage. Moreover, the capacity of language models to reason about the correct element combinations and generate logical forms in search spaces that are noisy (contain ambiguous candidates) has not yet been explored. Therefore, our method introduces SGLG, which defers entity disambiguation and relation classification to the logical form generation stage. By incorporating the schema context of KB elements, it helps the language model reason and generate the correct and executable logical form from a global perspective.

% \jz{Need to say first what these studies haven't done.}
% By explicitly encoding the semantic connections between entities and relations within the schema structure, our RLG module performs entity disambiguation and relation classification during the logical form generation process, reducing error propagation in the traditional retrieve-then-generate framework.





% \paragraph{Semantic Parsing-Based Method} SP-based methods focus on transforming the input questions into structured, executable queries --- typically 
% \emph{logical forms} --- which are then executed over KBs to retrieve answers____. SP-based methods can be further categorized into step-wise ranking and Seq2Seq generation methods____. Step-wise ranking methods____ incrementally expand a graph query (i.e., a logical form) with a search step to find possible paths in the KB at each step, followed by a ranking step to select the most relevant paths to be explored next. Seq2Seq generation methods____ transform an input question into a logical form in one go using a Seq2Seq model. Our model follows the general idea of such methods. \jz{We detail these relevant studies next.?}


%due to the important practical applications of both knowledge bases~(KB) and the question answering~(QA) problem over KBs. We start by an overview of the studies on this problem (Section~\ref{subsec:related_work_kbqa}). Then, we focus on KBQA solutions using semantic parsing and Seq2Seq models, as our model also falls into this category (Section~\ref{subsec:related_work_sp}). We also cover techniques for entity retrieval in KBQA, to set the context of our GER module (Section~\ref{subsec:related_work_er}).

%\subsection{Knowledge Base Question Answering}\label{subsec:related_work_kbqa}

%KBQA aims to achieve a natural language-based user interface for non-expert users to interact with KBs without knowing specialized query languages such as SPARQL. 


% Several recent studies____ propose \emph{knowledge injection-based} (KI-based) methods. These studies focus on training large \jz{or pre-trained?} language models to learn knowledge from the KB, while the trained models to can be fine-tuned to generate answers to \jz{Shengxiang to complete....} 
%inject knowledge from the KB directly into language models to by training language models with linearize knowledge triples from the KB. The trained models 
%are further fine-tuned on KBQA datasets, leveraging the acquired knowledge to answer questions. 

%\jz{What's the limitation of these methods? Can we compare with one of these in the experiments? Or why not?} 

% IR-based methods first retrieve a question-specific subgraph \jz{how? (e.g., by matching the entities in the kB with the entity mentions in the question?)}

% \paragraph{Information Retrieval-Based Methods} IR-based methods first retrieve entities related to the input question, one of which is selected as the \emph{topic entity}. The neighbors of the topic entity form a question-specific subgraph. A neural model is then used to score the nodes in the subgraph (i.e., the \emph{candidate answers}), and a score threshold is applied to produce the final answer set. The IR-based methods suffers from complex multi-hop questions, which often lead to retrieving large subgraphs that are difficult to score accurately____. A latest study____ scores the connections between nodes and edges and expands the subgraph step by step accordingly, which helps reduce  
% the subgraph size. KICP____ linearizes the KB triples into sentences to pre-train a language model, which is then fine-tuned on a KBQA dataset to serve as the answer scorer. Even with these enhancements, the IR-based methods typically have lower accuracy than the SP-based ones____. 

%Compared to LMs pre-trained on natural language corpora, the LM pre-trained on KB corpus achieved a higher accuracy in selecting answers from candidate answers. } 

%\ssf{[generally IR-based methods produce lower accuracy, even the sota ____ achieve nearly 8\% F1 less then SP-based methods] [not sure to compara with IR-based methods] [move KI-based methods in IR-based methods]}

%\ssf{can delete this paragraph} \emph{evidence pattern retrieval} technique to reduce the nodes retrieved for the subgraph by \jz{formulating structural dependencies in the KB as evidence patterns [need a more intuitive description on what it does and why it offers better results]}, thereby achieving competitive KBQA accuracy \jz{Need to give a reason why we don't compare with it in the experiments}. \jz{However, in general, the IR-based methods produce lower accuracy____, and hence we will not consider these methods in the rest of the paper.}
% even on complex KBQA questions. However, it is worth noting that the majority of state-of-the-art KBQA methods are are SP-based, as semantic parsing offers greater interpretability than IR-based approaches.

 \paragraph{KBQA Entity Retrieval}%\label{subsec:related_work_er} 
%\jz{The discussions above have focused on logical form generation of existing KBQA models.} 
KBQA entity retrieval typically has three steps: {entity mention detection}, {candidate entity retrieval}, and {entity disambiguation}. BERT____-based named entity recognition  is widely used for entity mention detection from input questions. %____. %TIARA____ treats entity mention detection as a span classification task. It scores question spans of varying lengths with BERT and takes the ones with top scores as entity mentions. 
%\ssf{tuning the threshold of detecting a candidate mention in order to improve coverage} \jz{tuning the threshold of detecting a candidate mention in order to improve coverage [Not sure how it works]}. 
To retrieve KB entities corresponding to entity mentions, the FACC1 dataset____ is commonly used, which contains over 10 billion surface forms (with popularity scores) of Freebase entities. ____ use the popularity scores for entity disambiguation, while ____ and ____ adopt a BERT reranker. %after pruning by popularity. 

%A key issue here is that NER systems may fail to identify  entity mentions precisely, which in turn fail entity retrieval afterwards. 
%Our \model\ model addresses this issue with the help of relation-augmented logical form sketches generation, which enable detecting entity mentions (and hence the entities) more accurately.