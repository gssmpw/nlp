% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}
\usepackage{tablefootnote}
% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{pifont}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{color}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{array}
\usepackage{booktabs}
\usepackage{float} 
\usepackage{arydshln}
\usepackage{subcaption}
\usepackage{xspace}
\usepackage{makecell}
%\usepackage{tabularray}
%\usepackage{tikz}
%\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
%            \node[shape=circle,draw,inner sep=2pt] (char) {#1};}}
            
\newcommand{\jz}[1]{{\color{red}{\bf{[JZ:]}} #1}}
\newcommand{\addexp}[1]{{\color{orange}{\bf{[AddExp:]}} #1}}
\newcommand{\sxfix}[1]{{\color{blue}#1}}

%\newcommand{\model}{RG$^2$-KBQA}
\newcommand{\model}{\textsc{SG-KBQA}\xspace}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

% \title{Knowledge Base Question Answering with Generalizable Logical Form Generation}
%\title{Schema-Guided Generalizable Knowledge Base Question Answering}
\title{Beyond Seen Data: Improving KBQA Generalization Through Schema-Guided Logical Form Generation}
%JHL1: how about something cuter like "Beyond Seen Data: Improving KBQA Generalization Through Schema-Guided Logical Form Generation"


% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{
  Shengxiang Gao  \hspace{10mm} Jey Han Lau  \hspace{10mm} Jianzhong Qi \vspace{3mm} \\
  School of Computing and Information Systems, The University of Melbourne \\
  \texttt{shengxiang.gao1@student.unimelb.edu.au} \\ 
  \texttt{\{jeyhan.lau, jianzhong.qi\}@unimelb.edu.au}\\
}


% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
Knowledge base question answering (KBQA) aims to answer user questions in natural language using rich human knowledge stored in large KBs. As current KBQA methods struggle with unseen knowledge base elements at test time,
%State-of-the-art KBQA solutions are based on semantic parsing and have two core steps: (1) Generation: generate a sequence of structured query operators, and (2) Retrieval: retrieve KB elements (entities and relations). The operators and KB elements together form a structured query (so-called ``logical form'') over the KB to answer user questions. We observe that solutions starting with either step miss guidance from the other step, hence yielding suboptimal outcomes.
%To address this limitation, we propose a model named \textbf{\model} with a novel processing paradigm that consists of a \emph{\underline{g}enerative entity \underline{r}etrieval} module and a \emph{\underline{r}etrieval-guided logical form \underline{g}eneration} module. The generative entity retrieval module generates primitive logical forms based on user questions and relations extracted from the questions, to guide KB entity retrieval with higher accuracy. 
%The retrieval-guided logical form generation module then generates the final logical forms based on the KB elements extracted.
we introduce \textbf{\model}: a novel model that injects schema contexts into entity retrieval and logical form generation to tackle this issue. 
It uses the richer semantics and awareness of the knowledge base structure provided by schema contexts to enhance generalizability. 
%\sxfix{The schema contexts describes relationships between elements in the knowledge base, providing richer semantics and awareness of its structure.}
%JHL1: can we give an intuitive, high level explanation on the idea? just 1-2 lines max to capture the core idea
We show that \model\ achieves strong generalizability, outperforming state-of-the-art models on two commonly used benchmark datasets across a variety of test settings. 
%Our source code is available at \url{https://anonymous.4open.science/r/SG-KBQA-7895}. 
Our source code is available at \url{https://github.com/gaosx2000/SG_KBQA}.
%Code will be released upon paper publication.
%Our source code is available at \url{https://anonymous.4open.science/r/SG-KBQA-7895}. 
%JHL1: use anoymised github (https://anonymous.4open.science/)
%with a novel processing paradigm that consists of a \emph{\underline{g}enerative entity \underline{r}etrieval} module and a \emph{\underline{r}etrieval-guided logical form \underline{g}eneration} module. The generative entity retrieval module generates primitive logical forms based on user questions and relations extracted from the questions, to guide KB entity retrieval with higher accuracy. 
%Experimental results confirm the effectiveness of \model, which outperforms state-of-the-art models on two commonly used benchmark datasets GrailQA and WebQSP across a variety of test settings. 
\end{abstract}


\section{Introduction}



% These instructions are for authors submitting papers to *ACL conferences using \LaTeX. They are not self-contained. All authors must follow the general instructions for *ACL proceedings,\footnote{\url{http://acl-org.github.io/ACLPUB/formatting.html}} and this document contains additional instructions for the \LaTeX{} style files.

% The templates include the \LaTeX{} source of this document (\texttt{acl\_latex.tex}),
% the \LaTeX{} style file used to format it (\texttt{acl.sty}),
% an ACL bibliography style (\texttt{acl\_natbib.bst}),
% an example bibliography (\texttt{custom.bib}),
% and the bibliography for the ACL Anthology (\texttt{anthology.bib}).

{Knowledge base question answering} (KBQA) aims to answer user questions expressed in natural language with information from a {knowledge base}~(KB). This offers user-friendly access to rich human knowledge stored in large KBs such as Freebase~\cite{bollacker_freebase_2008}, DBPedia~\cite{auerDBpediaNucleusWeb2007} and Wikidata~\cite{vrandecic_wikidata_2014}, and it has broad applications in QA systems~\cite{zhou_commonsense_2018}, recommender systems~\cite{guo_survey_2022}, and information retrieval systems~\cite{jalota_lauren_2021}.

\begin{figure}[t]
\small
    \centering
    \includegraphics[width=\columnwidth]{figures/kbqa_example_new.png}
    \caption{Example of KBQA and SP-based solutions.}
    \label{fig:kbqa_example}
\end{figure}

State-of-the-art (SOTA) solutions often take a {semantic parsing} (SP)-based approach. They translate an input natural language question into a structured, executable form (AKA {logical form}~\cite{lan_survey_2021}), which is then executed to retrieve the question answer. Figure~\ref{fig:kbqa_example} shows an example. The input question, \textsf{Who is the author of Harry Potter}, is expressed using the \emph{S-expression}~\cite{gu_beyond_2021} (a type of logical form), which is formed by a set of functions (e.g., \textsf{JOIN}) operated over elements of the target KB (e.g., entity \textsf{m.078ffw} refers to book series \textsf{Harry Potter}, \textsf{book.author} a class of entities, and \textsf{book.literary\_series.author} a relation in Freebase).

% \sxfix{However, the rich semantics and complex structure of KBs lead to two key challenges: (1) KB elements mapping: how to learn a mapping between mentions of entities and relations in the input question to corresponding KB elements? (2) Executable logical form generation: how to generate a logical form that aligns with the question's semantics and adheres to the structural constraints (schema) of the KB?

A key challenge here is to learn a mapping between mentions of entities and relations in the input question to corresponding KB elements to form the logical form. Meanwhile, the mapping of KB element compositions has to adhere to the structural constraints (schema) of the KB. The schema defines entities' classes and the relationships between these classes within the KB. Take the KB subgraph in Figure~\ref{fig:kbqa_example} as an example, the relationship between the entity \textsf{Harry Potter} and the entity \textsf{J.K. Rolwing} is defined by the relation \textsf{book.literary\_series.author} between their respective classes (i.e., class \textsf{book.literary\_series} and class \textsf{book.author}).

\begin{figure}[t]
    \small
    \centering
    \includegraphics[width=\columnwidth]{figures/core_modules.png}
    \caption{Relation-guided entity mention detection and schema-guided logical form generation.}
    \label{fig:core_novelty}
\end{figure}

However, due to the vast number of entities, relations, classes, and their compositions, it is difficult (if not impossible) to train a model with all feasible compositions of the KB elements. For example, Freebase~\cite{bollacker_freebase_2008} has over 39 million entities, 8,000 relations, and 4,000 classes. Furthermore, some KBs (e.g., NED~\cite{mitchell_ned_2018}) are not static as they continue to grow. 

A few studies consider model generalizability to non-I.I.D. settings, where the test set contains schema items (i.e., relations and classes) or compositions that are unseen during training (i.e., \emph{zero-shot} and \emph{compositional generalization}, respectively). In terms of methodology, these studies typically use {ranking-based} or {generation-based} models. 
Ranking-based models~\cite{gu_beyond_2021, gu_dont_2023} retrieve entities relevant to the input question and then, starting from them, perform path traversal in the KB to obtain the target logical form by ranking. Generation-based models~\cite{shu_tiara_2022, zhang_fc-kbqa_2023} retrieve relevant KB contexts (e.g., entities and relations) for the input question, and then feed these contexts into a Seq2Seq model together with the input question to generate the logical form.



%~\cite{gu_arcaneqa_2022,shu_tiara_2022,ye_rng-kbqa_2022,gu_dont_2023,zhang_fc-kbqa_2023,faldu_retinaqa_2024}

%To solve the generalization problem, most existing KBQA approaches follow the retrieve-and-generate framework, which enhances logical form generation using retrieved KB elements (entities, relations, and classes).~\cite{shu_tiara_2022, faldu_retinaqa_2024, gu_dont_2023,zhang_fc-kbqa_2023,ye_rng-kbqa_2022, gu_arcaneqa_2022}. Despite the promising results achieved by these works, significant challenges remain: 

We observe that both types of models terminate their entity retrieval prematurely, such that each entity mention in the input question is mapped to only a single entity before the logical form generation stage. As a result, the logical form generation stage loses the freedom to explore the full combination space of relations and entities. This leads to inaccurate logical forms (as validated in our study).

%

To address this issue, our strategy is to defer entity disambiguation --- i.e., to determine the most relevant entity for an entity mention (Section~\ref{sec:literature}) --- to the logical form generation stage. This allows our model to explore a larger combination space of the relations and entities, and ultimately leads to stronger model generalizability because low-ranked (but correct) relations or entities would still be considered during generation.
%A larger search space brings new challenges to identify the correct combination.
We call our approach \model (\underline{s}chema-\underline{g}uided logical form generator for \underline{KBQA}). Concretely, \model\ follows the generation-based approach but with deferred entity disambiguation. As shown in Figure~\ref{fig:core_novelty}, it feeds the input question, the retrieved candidate relations and entities, plus their corresponding schema information (the domain and range of classes of relations and entities; Section~\ref{sec:method}) into a large language model (LLM)
%JHL1: are they LLMs? if so let's just use LLMs henceforth (and avoid introducing another acronym)
for logical form generation. The schema information reveals the connectivity between the candidate relations and entities, hence guiding the LLM to uncover their correct combination in the large search space. 
%JHL1: I struggle to understand figure 2 - i can see they are different, but not sure what the yellow box means, what the pink highlighted boxes mean. and what is schema information? can we have a toy example of what the input looks like to the LM? I think what's important in figure 2 is to give a concrete example of the input to the LM for our model; and if there's space, contrast that with the input in SOTA models


Further exploiting the schema-guided idea, we propose a relation-guided module for \model\  to enhance its entity mention detection from the input question. As shown in Figure~\ref{fig:core_novelty}, this module adapts a Seq2Seq model to generate logical form sketches based on the input question and candidate relations, where relations, classes, and literals are masked by special tokens, such that the entity mentions can be identified more easily without confusions caused by these elements. 
 %which extracts entity mentions from  generated by a generator that consumes the input question and the selected relations. The extracted entity mentions are further utilized to retrieve and select top-ranked candidate entities from the KB, guided by the schemas provided by the selected relations. 
 
 %Our approach leverages the selected schema items to guide the entity retrieval process and effectively incorporates the schema context through GenMD to achieve mention detection with a more global perspective. This significantly improves the accuracy of entity retrieval in compositional and zero-shot settings.


%\textbf{Entity retrieval (linking) remains challenging in zero-shot and compositional generalization settings.} Traditional methods first perform mention detection and then retrieve candidate entities from the KB. For each mention, a ranking model is used for entity disambiguation, selecting the candidate entity most relevant to the question for use in the subsequent generation stage. However, mention detection methods proposed in the existing literature (e.g., NER or span classification) often fail when faced with questions containing unseen schema items. This is because some schema items in the KB contain nouns that could potentially be recognized as entities. For example,\ldots. Unseen schema items introduce ambiguous information in the question, which confuses the model and makes it challenging to accurately identify entity mention boundaries.



%\textbf{Error propagation and lack of global reasoning in the disjointed traditional retrieve-and-generate framework}. Previous KBQA works adopt a disjoint retrieve-and-generate framework, where candidate entities are disambiguated before logical form generation to narrow down the search space~\cite{shu_tiara_2022, pang_survey_2022, zhang_fc-kbqa_2023, ye_rng-kbqa_2022,faldu_retinaqa_2024}. However, this approach fixes entity choices without considering their interactions with relations and other entities in the query, leading to locally optimal but globally inconsistent entity-relation selections. Moreover, errors in entity disambiguation propagate through the pipeline, misleading subsequent logical form generation.
% Furthermore, due to encountering new semantic relations or contexts that are not present in the training data, the model often fails to match the unseen schema items or compositions with correct entities in the KB. 

% \textbf{The completely decoupled retrieval and generation processes lead to error propagation through the pipeline.} To achieve stronger generalization capabilities, most existing KBQA approaches follow the retrieve-then-generate framework~\cite{shu_tiara_2022, faldu_retinaqa_2024, gu_dont_2023,zhang_fc-kbqa_2023,ye_rng-kbqa_2022, gu_arcaneqa_2022}. They employ an independent retrieval module to retrieve KB elements (e.g. entities, relations, classes) relevant to the input question before generating the target logical form. The retrieved KB elements are then leveraged to narrow down the search space and provide KB context, thereby enhancing the generalization capability. For example, some approaches incorporate the retrieved KB elements as additional inputs to a seq2seq model~\cite{shu_tiara_2022,zhang_fc-kbqa_2023,ye_rng-kbqa_2022}, while others use the retrieved entities as anchors to incrementally expand the logical form through path traversal in the KB~\cite{gu_dont_2023, gu_beyond_2021}. Although retrieval results can enhance the generalization ability of various logical form generation methods, incorrect retrievals can mislead the subsequent generation of logical forms. 

% To address the issues above and achieve a strong zero-shot and compositional generalization capability, we propose \model, a novel KBQA model that has two core modules: \emph{generative entity refinement} (GER) and \emph{refinement-guided logical form generation} (RLG). \model\ retrieves relations relevant to the question and generates \emph{logical form sketches} (that mask the relations and classes which may confuse the detection of the boundaries of entity mentions) by feeding the top-ranked relations and the question into a Seq2Seq model. 
% It then obtains the top-ranked entities from the KB based on the entity mentions in the generated logical form sketches, \emph{leveraging retrieved relations to enhance the zero-shot and compositional generalization of entity refinement}.

% The RLG module integrates entity and relation selection directly into the logical form generation process, to mitigate error propagation between the retrieval and generation stages. Specifically, for each relation included in the input, we provide its two connected classes to capture the semantic constraints of the KB schema. Similarly, for each entity, its associated class is provided to clarify its semantic role within the KB. By integrating these schema annotations into the input of the Seq2Seq model, our approach enables more accurate selection of entities and relations and generates logical forms that are more consistent with the underlying KB structure.





% \model~mitigate the error propagation between the retrieval and genration stages by defering both relation and entity disambiguation to the generation stage. }





% Specifically, it leverages the KB structural context by utilizing the classes to which entities belong and the classes connected by relations to provide connectivity between entities and relations. This context supports the model in selecting the correct combinations of entities and relations. A seq2seq model is then fine-tuned to transform the question, refined entities and relations, and class annotations into the target logical form.


% However, the primary source of errors in existing KBQA systems still lies in the failure of entity retrieval, which propagates through the pipeline and leads to errors in subsequent logical form generation.

% Recent SP-based KBQA approaches typically consist of two key steps: KB element retrieval and logical form generation~\cite{luo_chatkbqa_2024, ye_rng-kbqa_2022, shu_tiara_2022, faldu_retinaqa_2024, zhang_fc-kbqa_2023}. The retrieval of KB element mainly aims to retrieve the KB elements relevant to the input question. Then, these retrieved KB elements are then utilized to generate a complete and executable logical form (e.g., SPARQL, S-expression). However, collecting sufficient training data to cover all possible KB elements and their compositions that may appear in user queries is highly challenging, especially for large-scale KBs with a large number KB elements. \textbf{The broad coverage and combinatorial explosion require KBQA model to handle unseen KB elements (i.e. zero-shot generalization) and unseen compositions of them (i.e. compositional generalization), which remains a significant challenge.}

% It is important to note that in most existing SP-based KBQA systems, the majority of errors primarily arise from inaccuracies in the retrieval of KB elements, particularly in entity retrieval~\cite{gu_dont_2023,shu_tiara_2022,ye_rng-kbqa_2022,faldu_retinaqa_2024, zhang_fc-kbqa_2023}. These errors propagate to the subsequent logical form generation step which takes the retrieved KB elements as part of the input. Previous KBQA studies have proposed various methods for retrieving KB elements, aiming to separately identify the most relevant entities, relations, and classes for a given question. However, \textbf{the lack of consideration for the semantic relationships between KB elements in the question across these independent retrieval processes often leads to errors in the retrieval results.} This issue is particularly pronounced when handling questions involving unseen KB elements, where the independent retrieval processes may misidentify the same part of a question as different types of KB elements. 

%generation side ? thinking about the word limit for this intro....

% The retrieval of KB elements is a critical step in SP-based KBQA methods, aiming to retrieve the KB elements relevant to the input question. 

% SP-based KBQA methods not only need to retrieve KB elements related to the input query, but also generate the operators and functions that align with the semantic of user's query to form a complete and executable logical form. 

% Inspired by the strong generalization ability demonstrated by pre-trained language models (PLMs) across various NLP tasks, researchers have explored leveraging PLMs to address the generalization challenges in KBQA problem~\cite{shu_tiara_2022, ye_rng-kbqa_2022, zhang_fc-kbqa_2023, faldu_retinaqa_2024, gu_dont_2023}.

% To achieve strong generalization, recent SP-based KBQA studies primarily leverage pre-trained language models (PLMs) to retrieve KB elements in the input question and generate the final logical forms based on the retrieved KB elements. 

% Recent SP-based KBQA works have achieved promising results under the I.I.D. assumption, which posits a strong correspondence between the distribution of schema items (classes and relations) in the training data and the test data. However, this assumption does not hold due to user queries potentially involving schema items or novel compositions of them that have not been encountered in the training data. Collecting sufficient training data to cover all entities, schema items, and compositions of them is challenging, especially for large-scale KBs with numerous entities and schema items. 

% Figure~\ref{fig:kbqa_example} shows an example, where the logical form of the input question, `\textsf{Who is the author of Harry Potter}', is expressed using the \emph{S-expression}~\cite{gu_beyond_2021} (a type of logical form), which is formed by a set of functions (e.g., \textsf{JOIN}) operated over elements of the target KB (e.g., entity `\textsf{m.078ffw}' which refers to the book series `\textsf{Harry Potter}', class of entities `\textsf{book.author}', and relation `\textsf{book.literary\_series.author}' in the Freebase KB). More details about the example and the S-expression will be given in Section~\ref{sec:preliminary}. 

% Meanwhile, large language models (LLMs), such as Chat-GPT \citep{brown_language_2020} and LLaMA \citep{touvron_llama_2023}, have demonstrated strong results in various NLP tasks. Previous works have demonstrated the generalization capability of LLMs in understanding natural language and generating formal language \cite{rony_sgpt_2022, li_few-shot_2023}. These works have inspired researchers to enhance KBQA systems by leveraging LLMs as the semantic parser. However, the vast scale and complex structure of KBs present significant challenges for leveraging LLMs in real-world generalization scenarios within KBQA.

% recent SP-based studies~\cite{shu_tiara_2022, zhang_fc-kbqa_2023, faldu_retinaqa_2024, yu_decaf_2023} follow a \emph{sequence-to-sequence} (Seq2Seq)-based framework. They fine-tune 
% \emph{pre-trained language models} (PLMs) such as T5~\cite{raffel_exploring_2023} to translate input questions into logical forms, exploiting the strong semantic understanding capabilities of such models. Due to the special representations adopt by the KBs, the entity IDs, relation names, and class names are not necessarily directly translatable from the mentions of such elements in the input question, e.g., `\textsf{m.078ffw}' vs. `\textsf{Harry Potter}' in Figure~\ref{fig:kbqa_example}. An additional KB element retrieval module is commonly used by existing Seq2Seq SP-based methods, forming either \emph{generate-then-retrieve} (GnR) or \emph{retrieve-then-generate} (RnG) processing paradigms.
To summarise:

\begin{itemize}
    \item We introduce \model\ to solve the KBQA problem under non-I.I.D. settings, where test input contains unseen schema items or compositions during training.
    
    \item We propose to defer entity disambiguation to logical form generation, and additionally guide this generation step with corresponding schema information, allowing us to explore a larger combination space of relations and entities to consider unseen relations, entities, and compositions. We further propose a relation-guided module to strengthen entity retrieval by generating logical form sketches. 
    
    %introduce a generative mention retrieval method that leverages the context of retrieved schema items to address the generalization issues of entity retrieval in compositional and zero-shot settings from a global perspective.
    
    % \item We propose (1) a GER module that guides entity retrieval with logical form sketches that are generated based on retrieved relations, to achieve more accurate entity retrieval, and (2) an RLG module that guides logical form generation with the class contexts of the entities and relations, to achieve more generalizable logical form generation. 
    %\item We bridge the retrieval and generation stages by integrating entity and relation disambiguation into the logical form generation process through the incorporation of the KB schema, thereby reducing error propagation and generating more accurate logical forms that align with the KB structure.
    %novel entity retrieval approach — generative entity retrieval, to address the generalization challenges faced by existing KBQA entity retrieval methods.
    
    \item We conduct experiments on two popular benchmark datasets and find \model\ outperforming SOTA models on both datasets. In particular, on non-I.I.D GrailQA our model tops all three leaderboards for the overall, zero-shot, and compositional generalization settings, outperforming SOTA models by 3.3\%, 2.9\%, and 4.0\% (F1) respectively.

\end{itemize}


%JHL1: intro is mostly good; there's quite a bit of technical novelty to explain so it's not easy to write. Some suggestions to improve it:
%- simplify figure 1: since we mostly use it to explain the conversion process, we can probably drop the later half of the figure (i.e. we just need to keep the NLQ and logical form).
%- we need a figure that gives an example what 'schema information' and 'relation-guided module' and 'logical form sketches' look like. For someone outside this space it's far too abstract at the moment and they are just keywords to me. With that figure, use it to explain the core novelty of our model in the intro; focus on the intuition (leave the details to the later sections).
%-general comment: use emph or it more sparingly. we don't need to emph every new keyword that we introduce; use it when you really want the reader to notice the word - often they are not even special nouns (e.g. sometimes you might use emph to emphasise a negation (we do *not* consider...))

\section{Related Work}\label{sec:literature}

\paragraph{Knowledge Base Question Answering}

Most KBQA 
%(a.k.a. knowledge graph question answering~\cite{liu_knowledge_2023}, KGQA) %\footnote{This problem is also referred to as knowledge graph question answering~\cite{liu_knowledge_2023}. We use KBQA to refer to both problems, since most KBs are organized in a graph.}  
solutions use {information retrieval-based} (IR-based) or {semantic parsing-based} (SP-based) methods~\cite{wu_survey_2019,lan_survey_2021}. IR-based methods construct a question-specific subgraph starting from the retrieved entities (i.e., the \emph{topic entities}). They then reason over  the  subgraph to derive the answer. SP-based methods focus on transforming input questions into logical forms, which are then executed to retrieve answers. %Compared to IR-based methods, SP-based methods can produce a more interpretable reasoning process through converting the natural language questions into executable logical forms. Moreover, 
SOTA solutions are mostly SP-based, as detailed next.
%on popular benchmarks (e.g., GrailQA~\cite{gu_beyond_2021} WebQuestionsSP~\cite{yih_value_2016}) 

%JHL1: does SP-based = generation-based approach? does IR-based = ranking-based (in the intro)? it seems like these things are all the same, but we have two different terms; let be more consistent. Also, it doesn't look like IR/ranking based method is all that important to us, so let's drop that discussion in the intro and focus on contrasting our method to generation-based/SP-based models


% SP-based methods can be further categorized into ranking-based and generation-based methods~\cite{gu_arcaneqa_2022, lan_complex_2023}. 

% \jz{Two sentences to explain ranking-based and generation-based methods, rep.} \sxfix{Ranking-based methods perform path traversal and ranking in the KB, starting from the retrieved entities~\cite{gu_beyond_2021, gu_arcaneqa_2022,lan_topic_unit_2019, gu_dont_2023}. Generation-based methods directly transform the input question into the target logical form using a Seq2Seq model~\cite{luo_chatkbqa_2024, wang_no_2024, ye_rng-kbqa_2022}.}

\paragraph{KBQA under I.I.D. Settings}

%Benefiting from the powerful natural language understanding and logical form generation capabilities of Large Language Models (LLMs), 
Recent KBQA studies under I.I.D. settings fine-tune LLMs to map input questions to rough KB elements and generate approximate logical form drafts~\cite{luo_chatkbqa_2024, wang_no_2024}. %, exploiting LLMs' semantic capabilities to understand input natural language questions. 
The approximate (i.e., inaccurate or ambiguous) KB elements are then aligned to exact KB elements through a subsequent retrieval stage. These solutions often fail over test questions that refer to KB elements unseen during training. While we also use LLMs for logical form generation, we ground the generation with retrieved relations, entities, and  schema contexts, thus addressing the non-I.I.D. issue. 
%Our model first retrieves KB elements through schema-guided retrieval, and then uses the retrieved KB elements along with their schema context to guide the generation of logical forms. This approach enables better generalization to questions containing unseen knowledge base elements, while also enhancing performance under the i.i.d. setting.


\paragraph{KBQA under Non-I.I.D. Settings}

Studies considering non-I.I.D. settings can be largely classified into \emph{ranking-based} and \emph{generation-based} methods. 

Ranking-based methods start from retrieved entities, traverse the KB, and construct the target logical form by ranking the traversed paths.  
%Ranking-based methods reduce the search space based on the KB structure and the retrieved entities. 
\citet{gu_beyond_2021} enumerate and rank all possible logical forms within two hops  of retrieved entities, while \citet{gu_dont_2023} incrementally expand and rank paths from retrieved entities. % to obtain the target logical form. %They then obtain the candidate logical form that best matches the question by ranking. %They evaluated both supervised fine-tuning LMs (e.g., T5~\cite{raffel_exploring_2023}, BERT~\cite{devlin_bert_2019}) and few-shot in-context learning LLMs (e.g., Codex~\cite{chen_evaluating_2021}) as the partial logical form discriminators. 

Generation-based methods transform an input question into a logical form using a Seq2Seq model (e.g., T5~\cite{raffel_exploring_2023}).
They often use additional contexts beyond the question to augment the input of the Seq2Seq model and enhance its generalizability. For example,~\citet{ye_rng-kbqa_2022} use  top-5 candidate logical forms enumerated from retrieved entities as the additional context. 
\citet{shu_tiara_2022} further use top-ranked relations, \emph{disambiguated entities}, and classes (retrieved \emph{separately}) as the additional context. \citet{zhang_fc-kbqa_2023} use connected pairs of retrieved KB elements. 

Our \model\ is generation-based. We use schema contexts (relations and classes) from retrieved relations and entities, rather than separate class retrieval (as in \citet{shu_tiara_2022}) which could introduce noise. We also defer entity disambiguation to the logical form generation stage, thus avoiding error propagation induced by premature entity disambiguation without considering the generation context, as done in existing works outlined below. 

%employ an additional middle-grained component that converts the retrieved KB elements into connected pairs of KB elements. A Seq2Seq model then transforms the concatenation of the question, the retrieved KB element pairs, and logical form sketches generated based on the question into the target logical form. 

%The methods above retrieve KB elements to serve as additional contexts to enable the models to generalize in non-I.I.D. settings. 
%Existing ranking-based and generation-based methods limit the subsequent logical form search space through KB element retrieval, thereby improving generalization capability. 
%However, errors in knowledge base element retrieval can directly mislead the logical form generation stage. Moreover, the capacity of language models to reason about the correct element combinations and generate logical forms in search spaces that are noisy (contain ambiguous candidates) has not yet been explored. Therefore, our method introduces SGLG, which defers entity disambiguation and relation classification to the logical form generation stage. By incorporating the schema context of KB elements, it helps the language model reason and generate the correct and executable logical form from a global perspective.

% \jz{Need to say first what these studies haven't done.}
% By explicitly encoding the semantic connections between entities and relations within the schema structure, our RLG module performs entity disambiguation and relation classification during the logical form generation process, reducing error propagation in the traditional retrieve-then-generate framework.





% \paragraph{Semantic Parsing-Based Method} SP-based methods focus on transforming the input questions into structured, executable queries --- typically 
% \emph{logical forms} --- which are then executed over KBs to retrieve answers~\cite{lan_query_2020}. SP-based methods can be further categorized into step-wise ranking and Seq2Seq generation methods~\cite{lan_complex_2023}. Step-wise ranking methods~\cite{yih_value_2016, lan_query_2020, gu_dont_2023} incrementally expand a graph query (i.e., a logical form) with a search step to find possible paths in the KB at each step, followed by a ranking step to select the most relevant paths to be explored next. Seq2Seq generation methods~\cite{liang_querying_2021, yin_neural_2021} transform an input question into a logical form in one go using a Seq2Seq model. Our model follows the general idea of such methods. \jz{We detail these relevant studies next.?}


%due to the important practical applications of both knowledge bases~(KB) and the question answering~(QA) problem over KBs. We start by an overview of the studies on this problem (Section~\ref{subsec:related_work_kbqa}). Then, we focus on KBQA solutions using semantic parsing and Seq2Seq models, as our model also falls into this category (Section~\ref{subsec:related_work_sp}). We also cover techniques for entity retrieval in KBQA, to set the context of our GER module (Section~\ref{subsec:related_work_er}).

%\subsection{Knowledge Base Question Answering}\label{subsec:related_work_kbqa}

%KBQA aims to achieve a natural language-based user interface for non-expert users to interact with KBs without knowing specialized query languages such as SPARQL. 


% Several recent studies~\cite{lin_knowledge-injected_2024, yu_decaf_2023} propose \emph{knowledge injection-based} (KI-based) methods. These studies focus on training large \jz{or pre-trained?} language models to learn knowledge from the KB, while the trained models to can be fine-tuned to generate answers to \jz{Shengxiang to complete....} 
%inject knowledge from the KB directly into language models to by training language models with linearize knowledge triples from the KB. The trained models 
%are further fine-tuned on KBQA datasets, leveraging the acquired knowledge to answer questions. 

%\jz{What's the limitation of these methods? Can we compare with one of these in the experiments? Or why not?} 

% IR-based methods first retrieve a question-specific subgraph \jz{how? (e.g., by matching the entities in the kB with the entity mentions in the question?)}

% \paragraph{Information Retrieval-Based Methods} IR-based methods first retrieve entities related to the input question, one of which is selected as the \emph{topic entity}. The neighbors of the topic entity form a question-specific subgraph. A neural model is then used to score the nodes in the subgraph (i.e., the \emph{candidate answers}), and a score threshold is applied to produce the final answer set. The IR-based methods suffers from complex multi-hop questions, which often lead to retrieving large subgraphs that are difficult to score accurately~\cite{bordes_large-scale_2015, dong_question_2015, zhang_subgraph_2022, liu_knowledge_2023}. A latest study~\cite{ding_enhancing_2024} scores the connections between nodes and edges and expands the subgraph step by step accordingly, which helps reduce  
% the subgraph size. KICP~\cite{lin_knowledge-injected_2024} linearizes the KB triples into sentences to pre-train a language model, which is then fine-tuned on a KBQA dataset to serve as the answer scorer. Even with these enhancements, the IR-based methods typically have lower accuracy than the SP-based ones~\cite{lan_query_2020, gu_knowledge_2022}. 

%Compared to LMs pre-trained on natural language corpora, the LM pre-trained on KB corpus achieved a higher accuracy in selecting answers from candidate answers. } 

%\ssf{[generally IR-based methods produce lower accuracy, even the sota \cite{ding_enhancing_2024} achieve nearly 8\% F1 less then SP-based methods] [not sure to compara with IR-based methods] [move KI-based methods in IR-based methods]}

%\ssf{can delete this paragraph} \emph{evidence pattern retrieval} technique to reduce the nodes retrieved for the subgraph by \jz{formulating structural dependencies in the KB as evidence patterns [need a more intuitive description on what it does and why it offers better results]}, thereby achieving competitive KBQA accuracy \jz{Need to give a reason why we don't compare with it in the experiments}. \jz{However, in general, the IR-based methods produce lower accuracy~\cite{}, and hence we will not consider these methods in the rest of the paper.}
% even on complex KBQA questions. However, it is worth noting that the majority of state-of-the-art KBQA methods are are SP-based, as semantic parsing offers greater interpretability than IR-based approaches.

 \paragraph{KBQA Entity Retrieval}%\label{subsec:related_work_er} 
%\jz{The discussions above have focused on logical form generation of existing KBQA models.} 
KBQA entity retrieval typically has three steps: {entity mention detection}, {candidate entity retrieval}, and {entity disambiguation}. BERT~\cite{devlin_bert_2019}-based named entity recognition  is widely used for entity mention detection from input questions. %~\cite{gu_beyond_2021, zhang_fc-kbqa_2023}. %TIARA~\cite{shu_tiara_2022} treats entity mention detection as a span classification task. It scores question spans of varying lengths with BERT and takes the ones with top scores as entity mentions. 
%\ssf{tuning the threshold of detecting a candidate mention in order to improve coverage} \jz{tuning the threshold of detecting a candidate mention in order to improve coverage [Not sure how it works]}. 
To retrieve KB entities corresponding to entity mentions, the FACC1 dataset~\cite{gabrilovich_facc1_2013} is commonly used, which contains over 10 billion surface forms (with popularity scores) of Freebase entities. \citet{gu_beyond_2021} use the popularity scores for entity disambiguation, while \citet{ye_rng-kbqa_2022} and \citet{shu_tiara_2022} adopt a BERT reranker. %after pruning by popularity. 

%A key issue here is that NER systems may fail to identify  entity mentions precisely, which in turn fail entity retrieval afterwards. 
%Our \model\ model addresses this issue with the help of relation-augmented logical form sketches generation, which enable detecting entity mentions (and hence the entities) more accurately. 

\section{Preliminaries}\label{sec:preliminary}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=1\linewidth]{figures/framework_new.png}
    \caption{Overview of \model. The model has two stages: \emph{retrieval} and \emph{generation}.  In the retrieval stage, we first retrieve and rank candidate relations based on the input question $q$ (\textcircled{1}). Using $q$ and the top-ranked candidate relations $R_q$, we generate logical form sketches and extract entity mentions from them  (\textcircled{2}). Based on the entity mentions and retrieved relations, we retrieve candidate entities from the KB  (\textcircled{3}) and rank them (the top-ones being $E_q$, \textcircled{4}). In the generation stage, $q$, $R_q$, $E_q$, and their class contexts, are fed into a fine-tuned language model for logical form generation (\textcircled{5}). Here, the colored modules come with our new design.}  
    \label{fig:framwork}
\end{figure*}

A graph structured-KB $\mathcal{G}$ is composed of a set of relational facts $\{ \langle s, r, o \rangle |s \in \mathcal{E}, r \in \mathcal{R}, o \in \mathcal{E} \cup \mathcal{L}\}$ and an ontology $\{ \langle c_d, r, c_r \rangle |c_d, c_r \in \mathcal{C}, r \in \mathcal{R} \}$.
Here, $\mathcal{E}$ denotes a set of entities, $\mathcal{R}$ denotes a set of relations, and $\mathcal{L}$ denotes a set of literals, e.g., textual labels, numerical values, or date-time stamps. In a relational fact $\langle s, r, o \rangle$, $s \in \mathcal{E}$ is the 
\textit{subject}, $o \in \mathcal{E} \cup \mathcal{L}$ is the \textit{object}, and $r \in \mathcal{R}$ represents the relationship between the \textit{subject} and the \textit{object}.

The ontology defines the rules governing the composition of relational facts within $\mathcal{G}$. In its formulation,  $\mathcal{C}$ denotes a set of classes, each of which defines a set of entities (or literals) sharing common properties (relations). Note that an entity can belong to multiple classes. 
In an ontology triple $\langle c_d, r, c_r \rangle$, $c_d$ is called a \textit{domain class}, and it refers to the class of  subject entities that satisfy relation $r$; 
$c_r$ is called the \textit{range class}, and it refers to the class of object entities or literals satisfying $r$. Each ontology triple can be instantiated as a set of relational facts. In Figure~\ref{fig:kbqa_example}, \textsf{<book.literary\_series, book.literary\_series.author, book.author>} is an ontology triple. An instance of it is \textsf{<Harry Potter, book.literary\_series.author, J.K. Rowling>}, where \textsf{Harry Potter} is an entity that belongs to class \textsf{book.literary\_series}.

%JHL1: great technical writing above - this is all very clear to me; I take back what I wrote earlier about simplifying Figure 1 earlier, looks like you need the latter half here

% We consider a graph structured-KB $\mathcal{G} = \{ \langle s, r, o \rangle |s \in \mathcal{E}, r \in \mathcal{R}, o \in \mathcal{E} \cup \mathcal{L}\}$ 
% stored in the form of triples $\langle s, r, o \rangle$, where $\mathcal{E}$ is a set of entities, $\mathcal{R}$ is a set of relations, and $\mathcal{L}$ is a set of literals, e.g., textual labels, numerical values, or date-time stamps. In every triple $\langle s, r, o \rangle$, $s \in \mathcal{E}$ is the 
% \textit{subject}, $o \in \mathcal{E} \cup \mathcal{L}$ is the \textit{object}, and $r \in \mathcal{R}$ represents the  relationship between the subject and the object. 

% Triples in the KB are instances of the \emph{ontology} defined for the KB~\cite{gu_knowledge_2022}, which in turn is also represented as triples  
% in the form of $\langle c_d, r, c_r \rangle$. Here, $c_d$ denotes the class of subject entities that satisfy relation $r$, and $c_r$ denotes the class of object entities or literals satisfying $r$. A \emph{class} defines a set of entities sharing common properties (relations), while an entity can belong to multiple classes. In Figure~\ref{fig:kbqa_example}, `\textsf{<book.literary\_series, book.literary\_series.author, book.author>}' is an ontology triple, while an instance of it is `\textsf{<Harry Potter, book.literary\_series.author, J.K. Rowling>}', where `\textsf{Harry Potter}' is an entity that belongs to class `book.literary\_series'.

\paragraph{Problem Statement} Given a KB $\mathcal{G}$ and a question $q$ expressed in natural language, i.e., a sequence of word tokens, {knowledge base question answering} (KBQA) aims to find a subset (the {answer set}) $\mathcal{A} \subseteq \mathcal{E} \cup \mathcal{L}$ of elements from $\mathcal{G}$ that --- with optional application of some aggregation functions (e.g., \textsc{count}) --- answers $q$. 

%find the answer $\mathcal{A} \text{ for } q $, where $\mathcal{A}$ is a set of entities or literals $\mathcal{A} \subseteq \mathcal{E} \cup \mathcal{L}$. $\mathcal{A}$ can serve directly as the answer or can be applied some aggregation functions (e.g. counting function) to get the answer.



\paragraph{Logical Form}
We solve the KBQA problem by translating the input question $q$ into a structured query that can be executed on $\mathcal{G}$ to fetch the answer set $\mathcal{A}$. Following previous works~\cite{shu_tiara_2022, ye_rng-kbqa_2022, gu_dont_2023, zhang_fc-kbqa_2023}, we use logical form as the structured query language, expressed with the \emph{S-expression}~\cite{gu_beyond_2021}.  
The S-expression offers a readable representation well-suited for KBQA. It uses set semantics where functions operate on entities or entity tuples without requiring variables~\cite{ye_rng-kbqa_2022}.  Figure~\ref{fig:kbqa_example} shows an example: the S-expression of the given question \textsf{Who is the author of Harry Potter?} is \textsf{(AND book.author (JOIN (R book.literary\_series.author) m.078ffw))}. This S-expression queries a set of entities that belong to the class \textsf{book.author} from the objects of triples whose subject entity is \textsf{m.078ffw} while the relation is \textsf{book.literary\_series.author}. More details about the  S-expression is in Appendix~\ref{sec:app_sexpression}.


\section{The \model\ Model}\label{sec:method}

As shown in Figure~\ref{fig:framwork}, \model\ follows the common structure of generation-based models. It has two overall stages: \emph{relation and entity retrieval} and \emph{logical form generation}. We propose novel designs in both stages to strengthen model generalizability.

In the relation and entity retrieval stage (Section~\ref{subsec:ger}), \model\ retrieves candidate relations and entities from KB $\mathcal{G}$ which may be relevant to the input question $q$. It starts with a BERT-based relation ranking model to retrieve candidate relations relevant to $q$. Together with $q$, the set of top-ranked candidate relations are fed into a novel, relation-guided  Seq2Seq model to generate logical form sketches that contain entity mentions while masking the relations and classes. We harvest the entity mentions and use them to retrieve candidate entities from $\mathcal{G}$. % with the help of an entity dictionary FACC1~\cite{Gabrilovich2013FACC1} (following existing studies~\cite{shu_tiara_2022,luo_chatkbqa_2024}, although other entity retrieval models can be used).  
We propose a combined relation-based strategy to prune the entities (as there may be many). The remaining entities are ranked by a BERT-based model, indicating their likelihood of being the entity that matches each entity mention. 

Leveraging relations to guide both entity mention extraction and candidate entity pruning enhances the model generalizability over entities unseen during training. This in turn helps the logical form generation stage to filter false positive matches for unseen relations or their combinations. 

In the logical form generation stage (Section~\ref{subsec:rlg}), 
\model\ feeds $q$, the top-ranked relations and entities (corresponding to each entity mention), and the schema contexts (i.e., domain and range classes of the relations and classes of the entities), into an adapted LLM to generate the logical form  and produce answer set $\mathcal{A}$. 

Our schema-guided logical form generation procedure is novel in that it takes (1) multiple candidate entities (instead of one in existing models) for each entity mention and (2) the schema contexts as the input. Using multiple candidate entities essentially defers  \emph{entity disambiguation}, which is usually done in the retrieval stage by existing models~\cite{shu_tiara_2022,gu_dont_2023}, to the generation stage, thus mitigating error propagation. This strategy also brings challenges, as the extra candidate entities (which are ambiguous as they often share the same name) may confuse the logical form generation model. We address the challenges with the schema contexts, which instruct the model the connectivity structures between the candidate entities and relations. The connectivity structures further help \model\ generalize to unseen entities, relations, or their combinations. 

%\sxfix{The classes shared by the entities and the relations indicate the connectivity of these elements. This guides our LM to to select the correct combination of entities and relations from the input's top-ranked entities and top-ranked relations, thereby generate executable logical forms. By deferring entity disambiguation to the logical form generation process and leveraging class information to provide semantic structural context from the KB, we reduce error propagation within the traditional retrieve-then-generate framework.} 

%retrieves the classes of the KB entities from the GER module. The relations between the classes offer contexts about the connectivity between the KB elements, to power the zero-shot and compositional generalization capability of \model\ to \emph{handle unseen KB elements and compositions}. We then fine-tune an open-sourced \emph{large language model} (LLM), which takes a question, the retrieved top-ranked entities and relations with class annotations as input to generate the target logical form.

\subsection{Relation and Entity Retrieval}\label{subsec:ger}

\paragraph{Relation Retrieval} For relation retrieval, we follow the schema retrieval model of TIARA~\cite{shu_tiara_2022}, as it has high accuracy. We extract a  set $R_q$ of top-$k_R$ (system parameter) relations with the highest semantic similarity to $q$. This is done by a BERT-based cross-encoder that learns the semantic similarity $\text{sim}(q, r)$ between $q$ and a relation $r \in \mathcal{R}$: %(recall that $\mathcal{R}$ is the set of relations of KB $\mathcal{G}$):  
\begin{equation}
\small
    \text{sim}(q,r)=\text{\large L\small INEAR}(\text{\large B\small ERT\large C\small LS}([q;r])), 
    \label{eqn:relation_retriever}
\end{equation} 
where `$;$' denotes concatenation.
This model is trained with the sentence-pair classification objective~\cite{devlin_bert_2019}, where a relevant question-relation pair has a similarity of 1, and 0 otherwise.




%\subsubsection{Relation-Augmented Logical Form sketch Parser}
\paragraph{Relation-Guided Entity Mention Detection}

%Previous work has employed Named Entity Recognition (NER) tools or regard entity mention detection as a span classification task to extract entity mentions in the questions \cite{gu_beyond_2021, shu_tiara_2022,zhang_fc-kbqa_2023}. However, detecting zero-shot entity mentions from short texts remains a challenging task. A common error in past mention detection methods is that certain components of relations in some questions are mistakenly detected as entity mentions, especially in questions containing unseen relations.
%To address the above problem, 
Given $R_q$, we propose a relation-guided logical form sketch parser to parse $q$ into a logical form sketch $s$. Entity mentions in $q$ are extracted from $s$. 

The parser is an adapted Seq2Seq model. The model input of each training sample takes the form of ``$q$ \textless relation\textgreater \text{ } $r_1;r_2;\ldots;r_{k_R}$'' ($r_i \in R_q$, hence ``relation-guided''). 
In the ground-truth logical form corresponding to $q$, we mask the relations, classes, and literals with special tokens `\textless relation\textgreater', `\textless class\textgreater', and `\textless literal\textgreater', to form the ground-truth logical form sketch $s$. Entity IDs are also replaced by the corresponding entity names (entity mentions), to enhance the Seq2Seq model's understanding of the semantics of entities.

%We fine-tune T5~\cite{raffel_exploring_2023} as the Seq2Seq model to transform a question into the corresponding logical form sketch. 

%We concatenate the question $q$ with the retrieved top-$k_R$ relations as the context  to form the input of the Seq2Seq model, i.e., the relation-augmented logical form sketch parser, to enhance the model understand of the input semantics. 
%The input of the model is in the form of ``$q$ \textless relation\textgreater \text{ } $r_1;r_2;\ldots;r_{k_R}$'' ($r_i \in R_q$). 

At model inference, from the output top-$k_L$ (system parameter) logical form sketches  (using beam search), we extract the entity mentions.

\paragraph{Relation-Guided Candidate Entity Retrieval}
We follow previous studies~\cite{gu_beyond_2021, shu_tiara_2022, faldu_retinaqa_2024, luo_chatkbqa_2024} and use an entity name dictionary FACC1~\cite{gabrilovich_facc1_2013} to map extracted entity mentions to entities (i.e., their IDs in KB), although other retrieval models can be used. Since different entities may share the same name, the entity mentions may be mapped to many entities. For pruning, existing studies use  popularity scores associated to  entities~\cite{shu_tiara_2022, ye_rng-kbqa_2022}. 

To improve the recall of candidate entity retrieval, we propose a combined pruning strategy based on both popularity and relation connectivity. As Figure~\ref{fig:candidate_entity_retrieval} shows, we first select the top-$k_{E1}$ (system parameter) entities for each entity mention based on popularity and then extract $k_{E2}$ (system parameter) entities from the remaining candidates that are connected to the retrieved relations $R_q$. Together, these form the candidate entity set $E_c$.
%for alias mapping of entity mentions \cite{.  A branch of works selects the entity with the highest popularity for each mention \cite{gu_beyond_2021, gu_arcaneqa_2022, zhang_fc-kbqa_2023}, while others choose the top-popularity entities and then perform entity disambiguation \cite{ye_rng-kbqa_2022, shu_tiara_2022, zhang_fc-kbqa_2023}. However, a popularity-based pruning strategy may exclude low-popularity ground-truth entities. 

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/candidate_entity_retrieval.png}
     \caption{Candidate entity retrieval for the mention `\textsf{aloha}'. The candidate entity in red is the ground-truth.}
    \label{fig:candidate_entity_retrieval}
\end{figure}


\paragraph{Entity Ranking} We follow existing works~\cite{shu_tiara_2022, ye_rng-kbqa_2022} to score and rank each candidate entity in $E_c$ by jointly encoding $q$ and the context (entity name and its linked relations) of the entity using a cross-encoder (like Eq.~\ref{eqn:relation_retriever}). %The context of a candidate entity includes . 
We select the top-$k_{E3}$ (system parameter) ranked entities for each mention as the entity set $E_q$ for each question.


\subsection{Schema-Guided Logical Form Generation}\label{subsec:rlg}

Given relations $R_q$ and entities $E_q$, we fine-tune an open-souce LLM (LLaMA3.1-8B~\cite{touvron_llama_2023} by default) to generate the final logical form.  

Before being fed into the model, each relation and entity is augmented with its schema context (i.e., class information) to help the model to learn their connections and generalize to unseen entities, relations, or their compositions.  The context of a relation $r$ is described by concatenating the relation's  domain class $c_d$ and range class $c_r$, formatted as ``[D] $c_d$ [N] $r$ [R] $c_r$''. For an entity $e$, its context is described by its ID (``$id_e$''), name (``$name_e$''), and the intersection of its set of classes $C_e$ and the set of all domain and range classes $C_R$ of all relations in $R_q$, formatted as ``[ID] $id_e$ [N] $name_e$ [C] class($C_e\cap C_R$)''.

As Figure~\ref{fig:framwork} shows, we construct the input to the logical form generation model by concatenating $q$ with the context of each relation in $R_q$ and the context of each entity in $E_q$. The model is fine-tuned with a cross-entropy-based objective:
\begin{equation}
\small
\mathcal{L}_{generator}=-\sum_{t=1}^n \log p\left(l_t \mid l_{<t}, q, K_q\right),
\end{equation}
where $l$ denotes a logical form of $n$ tokens and $l_t$ is its $t$-th token, and $K_q$ is the retrieved knowledge (i.e., relations and entities with contexts) for $q$. At inference, the model runs beam search to generate top-$k_O$ logical forms -- the executable one with the highest score is selected as the output. See Appendix~\ref{app:prompt} for a prompt example used for inference. 

%JHL1: I don't quite follow this last bit; dumb question: if we already have the logical forms, isn't it a straightforward thing to check whether an output logical form is executable, and just take the first executable one? It's a bit unclear why we need the enumeration step and also why we need a BERT reranker.
It is possible that no generated logical forms are executable. In this case, we fall back to following~\citet{shu_tiara_2022} and~\citet{ye_rng-kbqa_2022} and retrieve candidate logical forms in two stages: enumeration and ranking. During enumeration, we search the KB by traversing paths starting from the retrieved entities. Due to the exponential growth in the number of candidate paths with each hop, we start from the top-1 entity for each mention and searches its neighborhood for up to two hops. The paths retrieved are converted into logical forms. During ranking, a BERT-based ranker scores $q$ and each enumerated logical form $l$ (like Eq.~\ref{eqn:relation_retriever}). We train the ranker using a contrastive objective: 
\begin{equation}
\small
    \mathcal{L}=-\frac{\text{exp}(\text{sim}(q, l^*))}{\text{exp}(\text{sim}(q, l^*))+\sum_{l \in C_l \wedge l \neq l^*} \text{exp}(\text{sim}(q, l))},
\end{equation}
where $l^*$ is the ground-truth logical form and $C_l$ is the set of enumerated logical forms. We run the ranked logical forms from the top and return the first executable one. 

%JHL1: Good job on the technical writing; it was quite clear and I think I understood most of the model details. Thoughts: at a high level, I think the core novelty of the work is about the deferring the entity disambiguation to the generation step - this is a pretty cool idea. I think the intro gets this idea across quite well. But with that we run into a large search space, and introducing schema information into the input is a straightforward way to solve the issue - I think what we should do in the intro is to explain what schema information is (e.g. book.author is the schema information for m.078ffw), and then including an example input (to the LLM) in a figure and that should do it (figure 3 almost did this, but the input still just has some abstract variables, so isn't good enough). The relation-guided entity mention detection, on the other hand, feels like a very small touch and isn't that important to talk about in the intro. I'd drop it to increase clarity of our novel contribution.


\section{Experiments}\label{sec:experiment}
We run experiments to answer:
\textbf{Q1}:~How does \model\ compare with SOTA models in their accuracy for the KBQA task? 
%\textbf{Q2}: How does \model\ compare with the SOTA KBQA models  under I.I.D. settings? 
\textbf{Q2}:~How do model components impact the accuracy of \model? 
\textbf{Q3}:~How do our techniques generalize to other KBQA models? 

%is our generative entity retrieval module?
%\textbf{Q3}:~How effective is our retrieval-guided logical form generation module?



\subsection{Experimental Setup}

\paragraph{Datasets}
Following SOTA competitors~\cite{shu_tiara_2022, gu_dont_2023, zhang_fc-kbqa_2023}, we use two  benchmark datasets built upon Freebase.

\textbf{GrailQA}~\cite{gu_beyond_2021} is a dataset for evaluating the generalization capability of KBQA models. It contains 64,331 questions with annotated target S-expressions, including complex questions requiring up to 4-hop reasoning over Freebase, with aggregation functions including comparatives, superlatives, and counting. The dataset comes with training (70\%), validation (10\%), and test (20\%, hidden and only known by the leaderboard organizers) sets. In the validation and the test sets, 50\% of the questions include KB elements that are unseen in the training set (\textbf{zero-shot} generalization tests), 25\% consist of unseen compositions of KB elements seen in the training set (\textbf{compositional} generalization tests), and the remaining 25\% are randomly sampled from the training set (\textbf{I.I.D.} tests).

{WebQuestionsSP} (\textbf{WebQSP})~\cite{yih_value_2016} is a dataset for the I.I.D. setting. While our focus is on non-I.I.D. settings, we include results on this dataset to show the general applicability of \model. WebQSP contains 4,937 questions. 
More details of WebQSP are included in Appendix~\ref{app:WebQSP}.


%collected from Google query logs, including 3,098 questions for training and 1,639 for testing, each annotated with a target SPARQL query. %We convert each SPARQL query into the corresponding S-expression and extract 
%We follow GMT-KBQA~\cite{hu_logical_2022} to separate 200 questions from the training questions to form the validation set.

%\noindent \textbf{ComplexWebQuestions} (CWQ) \cite{talmor_web_2018} is an extended version of WebQSP with 34,689 questions in total. All the questions in it are derived from WebQSP but have been made more complex, incorporating more hops and constraints.

\begin{table*}[!ht]
\centering
\small
%(``I.I.D.'' means random samples from the training set; ``Compositional'' means unseen compositions of KB elements seen at training; ``Zero-shot'' means unseen compositions of unseen KB elements; ``Overall'' means a mix of the aforementioned). 
\begin{tabular}{
>{\centering\arraybackslash}m{0.1\linewidth}
>{}p{0.26\linewidth}
>{\centering\arraybackslash}m{0.04\linewidth}
>{\centering\arraybackslash}m{0.04\linewidth} 
>{\centering\arraybackslash}m{0.04\linewidth} 
>{\centering\arraybackslash}m{0.04\linewidth} 
>{\centering\arraybackslash}m{0.04\linewidth} 
>{\centering\arraybackslash}m{0.04\linewidth} 
>{\centering\arraybackslash}m{0.04\linewidth} 
>{\centering\arraybackslash}m{0.04\linewidth} }
\toprule
\multicolumn{1}{l}{\textbf{}} & \textbf{} & \multicolumn{2}{c}{\textbf{Overall}} & \multicolumn{2}{c}{\textbf{I.I.D.}} & \multicolumn{2}{c}{\textbf{Compositional}} & \multicolumn{2}{c}{\textbf{Zero-shot}} \\ \cline{3-10}
\multicolumn{1}{l}{} & \rule{0pt}{10pt}\textbf{Model} & \textbf{EM} & \textbf{F1} & \textbf{EM} & \textbf{F1} & \textbf{EM} & \textbf{F1} & \centering \textbf{EM} & \textbf{F1} \\ \midrule
\multirow{7}{*}{\begin{tabular}[c]{@{}c@{}} SP-based \\(SFT) \\ \end{tabular}} & RnG-KBQA (ACL 2021) & 68.8 & 74.4 & 86.2 & 89.0 & 63.8 & 71.2 & 63.0 & 69.2 \\
 & TIARA (EMNLP 2022) & 73.0 & 78.5 & 87.8 & 90.6 & 69.2 & 76.5 & 68.0 & 73.9 \\
 & Decaf (ICLR 2023) & 68.4 & 78.7 & 84.8 & 89.9 & 73.4 & \underline{81.8} & 58.6 & 72.3 \\
 & Pangu (T5-3B) (ACL 2023) & 75.4 & \underline{81.7} & 84.4 & 88.8 & \underline{74.6} & 81.5 & 71.6 & \underline{78.5} \\
 & FC-KBQA (ACL 2023) & 73.2 & 78.7 & \underline{88.5} & \underline{91.2} & 70.0 & 76.7 & 67.6 & 74.0 \\
 & TIARA+GAIN (EACL 2024) & \underline{76.3} & 81.5 & \underline{88.5} & \underline{91.2} & 73.7 & 80.0 & \underline{71.8} & 77.8 \\
 & RetinaQA (ACL 2024) & 74.1 & 79.5 & - & - & 71.9 & 78.9 & 68.8 & 74.7 \\ \midrule
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}} SP-based \\(Few-shot) \\ \end{tabular}} 
 & KB-Binder (6)-R (ACL 2023) & 53.2 & 58.5 & 72.5 & 77.4 & 51.8 & 58.3 & 45.0 & 49.9 \\
 & Pangu (Codex) (ACL 2023) & 56.4 & 65.0 & 67.5 & 73.7 & 58.2 & 64.9 & 50.7 & 61.1 \\
 & FlexKBQA (AAAI 2024) & 62.8 & 69.4 & 71.3 & 75.8 & 59.1 & 65.4 & 60.6 & 68.3 \\ \midrule
\multirow{2}{*}{\centering \makecell{\textbf{Ours} \\ (SFT)}} &\textbf{\model} & \textbf{79.1} & \textbf{84.4} & \textbf{88.6} & \textbf{91.6} & \textbf{77.9} & \textbf{85.1} & \textbf{75.4} & \textbf{80.8} \\
 &\hspace{3pt} - Improvement & +3.6\% & +3.3\% & +0.1\% & +0.4\% & +4.4\% & +4.0\% & +5.0\% & +2.9\% \\ \bottomrule 
\end{tabular}
\caption{\emph{Hidden} test results (\%) on GrailQA (best results are in boldface; best baseline results are underlined; ``SFT'' means supervised fine-tuning; ``few-shot'' means few-show in-context learning).}
%JHL1: ours is SFT too right? in that case let's put that in the table
\label{tab:grailqa}
\end{table*}

\paragraph{Competitors} 
We compare with both IR-based and SP-based methods including the SOTA models. 

On GrailQA, we compare with models that top the leaderboard\footnote{https://dki-lab.github.io/GrailQA/}, 
including \textbf{RnG-KBQA}~\cite{ye_rng-kbqa_2022}, \textbf{TIARA}~\cite{shu_tiara_2022}, \textbf{DecAF}~\cite{yu_decaf_2023}, 
\textbf{Pangu}
%(using T5-3B for scoring partial logical forms; 
(previous {SOTA} as of 15th February, 2025)~\cite{gu_dont_2023},
%JHL1: give a date for the current SOTA (as this may change in the future)
\textbf{FC-KBQA}~\cite{zhang_fc-kbqa_2023}, \textbf{TIARA+GAIN}~\cite{shu_data_2024}, and \textbf{RetinaQA}~\cite{faldu_retinaqa_2024}.
We also compare with few-shot LLM  (training-free) methods: KB-BINDER (6)-R~\cite{li_few-shot_2023}, Pangu
%(using Codex for scoring partial logical forms)
~\cite{gu_dont_2023}, and FlexKBQA~\cite{li_flexkbqa_2024}. These models are SP-based. On the non-I.I.D. GrailQA, IR-based methods are uncompetitive and excluded.

On WebQSP, we compare with IR-based models \textbf{SR+NSM}~\cite{zhang_subgraph_2022}, \textbf{UNIKGQA}~\cite{jiang_unikgqa_2023}, and
\textbf{EPR+NSM}~\cite{ding_enhancing_2024}, plus SP-based models  \textbf{ChatKBQA} ({SOTA})~\cite{luo_chatkbqa_2024} and \textbf{TFS-KBQA} ({SOTA})~\cite{wang_no_2024}, both of which use a fine-tuned LLM to generate logical forms.
We also compare with TIARA, Pangu, and FC-KBQA as above, which represent SOTA models using pre-trained language models (PLMs). 
Appendix~\ref{sec:app_baselines} details these models. The baseline results are collected from their papers or the GrailQA leaderboard (if available).


% On WebQSP, we compare with SOTA models including TFS-KBQA \cite{wang_no_2024}, ChatKBQA \cite{luo_chatkbqa_2024}, GMT-KBQA \cite{hu_logical_2022}, FC-KBQA \cite{zhang_fc-kbqa_2023}, and Pangu \cite{gu_dont_2023}. The first two methods are based on a fine-tuned LLM, while the latter deploys the LLM within a generate-then-retrieve framework. GMT-KBQA \cite{hu_logical_2022} and FC-KBQA \cite{zhang_fc-kbqa_2023} are representative state-of-the-art models utilizing PLM in retrieve-then-generate framework. Pangu \cite{gu_dont_2023} achieves state-of-the-art performance across multiple datasets, which is a generic framework for grounded language understanding.



\paragraph{Implementation Details}
% All our experiments are run on a machine with an NVDIA A100 GPU and 120 GB of RAM. We fine-tuned three \texttt{bert-base-uncased} models for a maximum of three epochs each, for relation retrieval, entity ranking, and fallback logical form ranking.
% For relation retrieval, we randomly sample 50 negative samples for each question to train the model to distinguish between relevant and irrelevant relations. 

% For each dataset, a \texttt{T5-base} model is fine-tuned for 5 epochs as our logical form sketch parser, with a beam size of 3 (i.e., $k_L = 10$) for GrailQA, 4 for WebQSP. In candidate entity retrieval, we use the same number (i.e., 10) of candidate entities per mention as the baselines~\cite{shu_tiara_2022, ye_rng-kbqa_2022}. The retrieved candidate entities for a mention consist of entities with the top-$k_{E1}$ popularity scores and $k_{E2}$ entities connected to the top-ranked relations in $R_q$, where $k_{E1} = 1$, $k_{E2} = 9$ for GrailQA, $k_{E1} = 3$, $k_{E2} = 7$ for WebQSP.

% We select the top-20 (i.e., $k_R$ = 20) relations and the top-2 (i.e., $k_{E3} = 2$) entities (for each entity mention) retrieved by our model. For WebQSP, we also use the entities obtained from the off-the-shelf entity linker ELQ~\cite{li_efficient_2020}. 

% Finally, we fine-tune \texttt{LLaMA3.1-8B} with LoRA~\cite{hu_lora_2021} for logical form generation. On GrailQA, \texttt{LLaMA3.1-8B} is fine-tuned for 1 epoch with a learning rate of $0.0001$. On WebQSP, it is fine-tuned for 15 epochs with the same learning rate (as it is an I.I.D. dataset where more epochs are beneficial). During inference, we generate logical forms by beam search with a beam size of 10 (i.e., $K_O = 10$). The generated logical forms are executed on the KB to filter non-executable ones. If none of the logical forms are executable, we check the candidate logical forms from the fallback procedures, and the result of the first executable one is returned as the answer set.
% %\jz{Any updates needed for this subsection?} 


% Our system parameters have been chosen empirically. While there are a few of them, their exact values do not have strong impact on the final model performance, and the choice of parameter values generalize well across  datasets. The same parameter values are used on both datasets. 
% \addexp{Add parameter study to appendix.}
All our experiments are run on a machine with an NVDIA A100 GPU and 120 GB of RAM. We fine-tuned three \texttt{bert-base-uncased} models for a maximum of three epochs each, for relation retrieval, entity ranking, and fallback logical form ranking.
%JHL1:  for these three models, can we ref to figure 3 and the module number in the figure?
For each dataset, a T5-base model is fine-tuned for 5 epochs as our logical form sketch parser. Finally, we fine-tune a LLaMA3.1-8B with LoRA~\cite{hu_lora_2021} for 5 epochs on GrailQA and 20 epochs on WebQSP to serve as the logical form generator. Our system parameters have been chosen empirically, and a parameter study is provided in Appendix~\ref{app:paramater_study}. More implementation details  are in Appendix~\ref{app:implemention_details}.




\paragraph{Evaluation Metrics}
On GrailQA, we report the exact match (\textbf{EM}) and \textbf{F1} scores, following the leaderboard. EM counts the percentage of test samples where the model generated logical form (an S-expression) that is semantically equivalent to the ground truth. F1  measures the answer set correctness, i.e., the F1 score of each answer set, average over all test samples. 
On WebQSP, we report the F1 score as there are no ground-truth S-expressions. 
%Following previous SP-based methods \cite{shu_tiara_2022, zhang_fc-kbqa_2023}, here hits@1 is calculated by randomly selecting one answer for each question 100 times and averaging the results. This approach is used because the answers obtained from SP-based methods are typically unordered.

\begin{table}[t]
\centering
\small
\setlength{\tabcolsep}{6pt}
\begin{tabular}{clc}
\toprule
 & \textbf{Model} & \textbf{F1}\\ \midrule
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}IR-based\\ \end{tabular}}  
& SR+NSM (ACL 2022)     & 69.5 \\
& UniKGQA (ICLR 2023)  & 75.1    \\
& EPR+NSM (WWW 2024)  & 71.2   \\
 \midrule
\multirow{5}{*}{\begin{tabular}[c]{@{}c@{}}SP-based \\ (SFT) \\ \end{tabular}} 
& TIARA (EMNLP 2022)   & 76.7\\
& Pangu (T5-3B, ACL 2023) & 79.6\\
& FC-KBQA (ACL 2023)   & 76.9 \\
& ChatKBQA (ACL 2024) & 79.8\\ 
& TFS-KBQA (LREC-COLING 2024) & \underline{79.9}\\
\midrule
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}SP-based \\(Few-shot)\\ \end{tabular}} 
& KB-Binder (6)-R (ACL 2023)   & 53.2\\
& Pangu (Codex) (ACL 2023)   & 54.5\\
& FlexKBQA (AAAI 2024)   & 60.6\\
\midrule
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{Ours} \\ (SFT)\end{tabular}} 
& \textbf{\model} & \textbf{80.3} \\
&\hspace{6pt} - Improvement & \multicolumn{1}{l}{+0.5\%} \\
% \cdashline{2-3}
% & \rule{0pt}{10pt} \hspace{6pt}w/o RG-EMD & 78.4 \\
% & \hspace{6pt} w/o RG-CER & 79.5 \\ 
% %& \hspace{6pt} w/o SG-ER & 78.3 \\ 
% &\hspace{6pt} w/o DED & 78.2 \\ 
% &\hspace{6pt} w/o SG-LF & 77.1 \\
\bottomrule
\end{tabular}
\caption{Test results (\%) on WebQSP (I.I.D.).}
\label{tab:webqsp}
\end{table}


\subsection{Overall Results (Q1)}
Tables~\ref{tab:grailqa} and~\ref{tab:webqsp} show the overall comparison of \model\ with the baseline models for GrailQA and WebQSP, respectively. \model\ shows the best results across both datasets. 

\paragraph{Results on GrailQA} 
On the overall hidden test set of GrailQA, \model\ outperforms the best baseline Pangu by 4.9\% and 3.3\% in the EM and F1 scores, respectively. Under the compositional and zero-shot generalization settings (both are non-I.I.D.), similar performance gaps are observed, i.e., 4.0\% and 2.9\% in F1 compared to the best baseline models, respectively. This validates that \model\ can extract relations and entities more accurately from the input question, even when these are unseen in the training set, and it creates more accurate logical forms to answer the questions. %\sxfix{baseline models do not use schema context of retrieved KB elements (other model use class as a set of retrieved KB elements. We use the class or schema context of each relation and entity to feed its schema (connectivity to other elements)) into our generator to guide a logical form generation that are consistent to the KB structure (schema)} 

The fine-tuned baseline models do not use relation semantics to enhance entity retrieval, and they either omit the class contexts in logical form generation or use these classes separately for retrieval. As such, they do not generalize as well in the non-I.I.D. settings. 
%which may be noisy and do not indicate the schema of the  the entities and relations. These  explain for their lower accuracy.
The few-shot LLM-based competitors are generally not very competitive, especially under the non-I.I.D. settings. This suggests that {the current generation of LLMs are unable to infer from a few input demonstrations the process of logical form generation from user questions}. Fine-tuning is still required.  

\paragraph{Results on WebQSP} 


On WebQSP, which has an I.I.D. test set, the performance gap of the different models are closer. Even in this case, \model\ still performs the best, showing its general applicability. Comparing with  TFS-KBQA (SOTA) and ChatKBQA, \model\ improves the F1 score by 0.5\%.  
Among IR-based methods, UniKGQA (SOTA) still performs substantially worse compared to \model. The lower performance of IR-based methods is consistent with existing results~\cite{gu_knowledge_2022}.
%which also reported lower performance from the IR-based methods.





\subsection{Ablation Study (Q2)}


\begin{table}[t]

\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{lccccc}
\toprule
\multirow{2}{*}{\rule{0pt}{28pt}\textbf{Model}}                  & \multicolumn{4}{c}{\textbf{GrailQA}}                                 & \textbf{WebQSP}  \\ \cmidrule{2-6} 
                                        & \textbf{Overall} & \textbf{I.I.D.} & \textbf{Comp.} & \textbf{Zero.} & \textbf{Overall} \\ \midrule
\textbf{\model}          & \textbf{88.5}    & \textbf{94.6}   & \textbf{84.6}  & \textbf{87.9}  & \textbf{80.3}    \\
\hspace{3pt}w/o RG-EMD & 85.3             & 92.4            & 80.2           & 84.3           & 78.4             \\
\hspace{3pt}w/o RG-CER & 86.5             & 92.1            & 81.1           & 86.3           & 79.5             \\
\hspace{3pt}w/o DED    & 87.8             & 94.0            & 82.4           & 87.2           & 78.2             \\
\hspace{3pt}w/o SC  & 79.2             & 92.9            & 77.4           & 73.9           & 77.1             \\ \bottomrule
\end{tabular}
}
\caption{Ablation study results (F1 score) on the validation set of GrailQA and the test set of WebQSP.}
\label{tab:ablation}
\end{table}

% \begin{table}[H]
% \small
% \centering
% \begin{tabular}{lcccc}
% \toprule
% \textbf{Model}           & \textbf{Overall} & \textbf{I.I.D.} & \textbf{Comp.} & \textbf{Zero.} \\ \midrule
% % TIARA           & 81.9   & 91.2  & 74.8  & 80.7  \\
% % FC-KBQA         & 83.8   & 91.5  & 77.3  & 83.1  \\
% % RetinaQA        & 83.3   & 91.2  & 77.5  & 82.3  \\ 
% % \midrule
% \textbf{\model}            & \textbf{88.5}   & \textbf{94.6}  & \textbf{84.6}  & \textbf{87.9}  \\
% \hspace{3pt}w/o RG-EMD   &   85.3     &    92.4   &   80.2   &    84.3  \\ 
% \hspace{3pt}w/o RG-CER   &   86.5     &    92.1   &   81.1    &   86.3    \\ 
% %\hspace{3pt}w/o SGER        & 86.4   & 92.3  & 82.4  & 85.5  \\
% \hspace{3pt}w/o DED      & 87.8  & 94.0  & 82.4  & 87.2  \\
% \hspace{3pt}w/o SG-LF    & 79.2   & 92.9  & 77.4  & 73.9  \\
% %\hspace{3pt}w/o Fallback LF \jz{to appendix} & 84.6 & 94.1 & 81.8 & 81.5 \\ 
% \bottomrule
% \end{tabular}
% \caption{Ablation study results (F1 score) on the validation set of GrailQA}
% \label{tab:ablation}
% \end{table}
%JHL1: I'd drop the baseline models, and move WebQSP results to this table. since it's about studying the impact of different components, I don't see why we need to include the baseline models

Next, we run an ablation study with the following variants of \model: \textbf{w/o~RG-EMD} replaces our relation-guided entity mention detection with SpanMD~\cite{shu_tiara_2022} which is commonly used in existing models~\cite{pang_survey_2022, faldu_retinaqa_2024}; \textbf{w/o~RG-CER} omits  candidate entities retrieved from the top relations; \textbf{w/o~DED} uses the top-1 candidate entity for each entity mention without deferring entity disambiguation; \textbf{w/o~SC} omits schema contexts from logical form generation. 
%JHL1: can we use shorter acronyms for these things? it's not obvious what RG, SG, etc all means anyway, why not just use a 2-3 letter acronym? Also, let's be consistent with the acronymn used in 5.4 (i see RG-CER vs. SG-ER), but they look like the same thing)

Table~\ref{tab:ablation} shows the results on the validation set of GrailQA and the test set of WebQSP. Only F1 scores are reported for conciseness, as the EM scores on GrailQA exhibit similar comparative trends and are provided in Appendix~\ref{app:ablation}.

% Table~\ref{tab:ablation} shows the results on the validation set of GrailQA \sxfix{and the test set of WebQSP}, benchmarking against baseline models with released code. We only show F1 scores. The EM scores exibit similar comparative patterns and are included in Appendix~\ref{app:ablation}.
%For WebQSP, the results are included in Table~\ref{tab:webqsp}.

All model variants have lower F1 scores than those of the full model, confirming the effectiveness of the model components. SG-KBQA w/o DED (with schema contexts) reduces the F1 scores across various generalization settings on both datasets, demonstrating the effectiveness of our DED strategy in reducing error propagation during the retrieval and generation stages. Furthermore, \model~w/o SC (with deferred entity disambiguation) has the most significant drops in the F1 score under the compositional (7.2) and zero-shot (14.0) generalization tests. It highlights the importance of schema contexts in constraining the larger search space introduced by DED and in generalizing to unseen KB elements and their combinations. Meanwhile, the lower F1 of \model~w/o RG-EMD emphasizes the capability of our relation-guided entity mention detection module in strengthening KBQA entity retrieval.
%JHL1: the ablation results is admittedly a little bit disappointing. MY read is:
%- the most novel bit, which is deferring entity disambiguation to generation, seems to have only marginal impact (DED)
%- adding schema context information (SG-LF), adding relation to entity mention detection (EMD), are fairly straightforward innocation, on the other hand, seem to have the most impact!
%- For the first point, does that mean the premise that we said about errors due to premature entity disambiguation are empirically... quite rare? So it's a somewhat non-issue hmm....
%- So, what does all this mean for us? I think we can be honest here, and talk about how the more 'novel' idea turns out to be empirically less impactful, but that the more simple changes (like injecting more info into the input) turn out to have huge impact. Smooth it out a bit by saying but overall all the individual modules still contribute to the big performance gain ultimately so everything is cool. But up to you - it's your paper after all; this is just a suggestion.

% , the lower F1 scores of  emphasizes the importance of the schema context. \jz{Meanwhile, \model~w/o RG-EMD  This highlights the capability of our relation-guided entity mention detection module in strengthening entity retrieval under non-i.i.d settings.}

%Next, we conduct an ablation study to show the effectiveness of our generative entity retrieval module and the retrieval-guided generation module. 


%\addexp{can add \model-w/o fallback logical form generation;  (and others).} 



% \begin{table*}[ht]
% \small
% \centering
% \begin{tabular}{
% >{}p{0.2\linewidth}
% >{\centering\arraybackslash}m{0.05\linewidth}
% >{\centering\arraybackslash}m{0.05\linewidth} 
% >{\centering\arraybackslash}m{0.05\linewidth} 
% >{\centering\arraybackslash}m{0.05\linewidth} 
% >{\centering\arraybackslash}m{0.05\linewidth} 
% >{\centering\arraybackslash}m{0.05\linewidth} 
% >{\centering\arraybackslash}m{0.05\linewidth} 
% >{\centering\arraybackslash}m{0.05\linewidth} }
% \toprule
% & \multicolumn{2}{c}{\textbf{Overall}} & \multicolumn{2}{c}{\textbf{I.I.D.}} & \multicolumn{2}{c}{\textbf{Compositional}} & \multicolumn{2}{c}{\textbf{Zero-shot}} \\ \cline{2-9} 
% \multirow{-2}{*}{} \rule{0pt}{10pt}  \textbf{\vspace{-0.5cm}Model} & \textbf{EM}                   & \textbf{F1}  & \textbf{EM} & \textbf{F1} & \textbf{EM} & \textbf{F1}                     & \textbf{EM}  & \textbf{F1}  \\ \midrule
% TIARA & 75.3 & 81.9 & 88.4 & 91.2 & 66.4 & 74.8 & 73.3 & 80.7 \\
% FC-KBQA & 79.0 & 83.8 & 89.0 & 91.5 & 70.4 & 77.3 & 78.1 & 83.1 \\
% RetinaQA & 77.8 & 83.3 & 88.6 & 91.2 & 70.5 & 77.5 & 76.2 & 82.3 \\
% % TIARA + Generative Entity Retrieval &79.5 &84.3 &90.3 &92.3 &71.2 &78.1 &78.3 &83.3 \\
% % TIARA (LLaMA3-8B) & 79.9 & 85.6 & 88.6 & 92.3 & 72.7& 79.8 & 79.0 & 85.0 \\
% \midrule
% \textbf{\model} (Ours) \rule{0pt}{10pt} & 83.8 & 88.0 & 91.1 & 93.3 & 76.6 & 82.6 & 83.6 & 87.9 \\
% \hdashline
%   \rule{0pt}{10pt}\hspace{6pt} w/o SGER\tablefootnote{we replace schema-guided entity retrieval by SpanMD (mention dection method used in other SOTA studies.)} & 80.9 & 86.4 & 89.1 & 92.3 & 75.4 & 82.4 & 79.7 & 85.5 \\
%  \hspace{6pt} w/o SGLF\footnotemark[3] & 82.8 & 86.8 & 89.9 & 92.4 & 75.3 & 81.5 & 82.8 & 86.6\\
% \hspace{6pt} w/o SGER(top-1) & 83.0 & 86.7 & 89.2 & 91.2 & 75.4 & 80.8 & 83.4 & 86.9 \\ 
% \hspace{6pt} w/o R for LFS \\ 
% % \hline
% % \rule{0pt}{10pt}Top-1 Refined Entity (GER) + TIARA &79.5 &84.3 &90.3 &92.3 &71.2 &78.1 &78.3 &83.3\\
% %  TIARA's Entity Retrieval + RLG & 79.9 & 85.6 & 88.6 & 92.3 & 72.7& 79.8 & 79.0 & 85.0 \\

% % \hspace{6pt} w Top1-entity per mention & 83.0 & 86.7 & 89.2 &81.8 & 75.4 & 80.8 & 83.4 & 86.9 \\
% % \hspace{6pt} w/o Class & 82.8 & 86.8 & 89.9 & 92.4 & 75.3 & 81.5 & 82.8 & 86.6 \\ 
% % \hspace{6pt} w/o Generative Entity Retrieval  & 80.9 &	86.4 &	89.1 &	92.3 &	75.4 &	82.4 &	79.7 &	85.5 \\
% % \hspace{6pt} w/o Candidate Logical Forms & 80.1 & 83.9 & 90.7 & 92.5 & 75.2 & 80.6 & 77.5 & 81.5 \\
% % \hspace{6pt} w T5-Base & 78.7 & 83.2 & 88.1 & 90.4 & 69.5 & 75.6 & 78.4 & 83.3 \\
% % \hspace{6pt} w/o Context & 74.7 & 79.3 & 81.9 & 85.6 & 65.3 & 79.7 & 75.5 & 80.1 \\

% \bottomrule
% \end{tabular}
% \caption{Ablation study results on the validation set of GrailQA.}
% \label{tab:ablation}
% \end{table*}




%\paragraph{Generative Entity Retrieval} \jz{\model\ w/o generative entity retrieval exhibited a significant performance drop across both datasets. On GrailQA (Table~\ref{tab:ablation}), replacing the generative entity retrieval with TIARA's entity retrieval results led to a 3.4\% drop in the F1 score. Table~\ref{tab:entity_retrieval} further shows entity retrieval performance results on the GrailQA validation set, comparing GER method with commonly used entity retrieval methods in existing KBQA methods. Our GER method improves the F1 score by 7.0\%. Furthermore, our proposed candidate entity pruning strategy that combines the entity popularity-based pruning and relation-based pruning boosts the F1 score for entity retrieval by 2.1\%. To further validate the effectiveness of the GER module, we applied its retrieved entities to an open-source generate-then-retrieve method, TIARA (TIARA + Generative Entity Retrieval) in Table~\ref{tab:ablation}. We see that GET improves TIARA's F1 score by 3.0\% and EM by 5.5\%. On WebQSP, removing the GER from the merged entities set led to an 8.0\% drop in F1, while removing the ELQ entities resulted in only a 1.7\% drop (Table~\ref{tab:webqsp}). The experimental results demonstrate that using a relation-enhanced logical form sketch parser to generate entity mentions improves the identification of entity mentions in questions, even in zero-shot and compositional generalization settings.

%\paragraph{Retrieval-Guided Generation} 
%As shown in Table~\ref{tab:ablation}, \model\ w Top1-entity per mention negatives impacts F1 by 1.3 points overall, indicating that our generator (LLM) has the ability to select the correct entity from a set that includes false positive entities. The classes of the retrieved entities and the domain and range classes of the retrieved relations provide the generator with more KB context, resulting in a 1.3\% performance gain. We also replace LLaMA3-8B with T5-Base, a model widely used in our baselines~\cite{shu_tiara_2022, zhang_fc-kbqa_2023}, and find that the F1 score decreases by 4.8 points, while the EM score drops by 5.1 points with \model\ w T5-Base. Using candidate logical forms as a supplement when the generator fails to produce executable logical forms improves the F1 score by 4.9\% on GrailQA and by 2.1\% on WebQSP, suggesting the effectiveness of utilizing candidate logical forms as a supplement. Without providing any context (KB elements), using the generator to directly convert natural language questions into logical forms only achieves an EM score of 74.7 and an F1 score of 79.3, with particularly low F1 scores of 79.7 and 80.1 for compositional and zero-shot generalization, respectively. This highlights the necessity of the retrieval module for compositional and zero-shot generalization.}

\subsection{Module Applicability (Q3)}


Our relation-guided entity retrieval (\textbf{RG-EMD \& RG-CER}) module and schema-guided logical form generation (\textbf{DED \& SC}) module can be applied to existing KBQA models. We showcase such applicability with the TIARA model. As shown in Table~\ref{tab:exp_applicability}, by replacing the retrieval and generation modules of TIARA with ours, the F1 scores increase consistently for the non-I.I.D. tests.


Table~\ref{tab:exp_applicability} further reports F1 scores of \model\ when we replace LLaMA3.1-8B with \textbf{T5-base} (which is used by TIARA), and DeepSeek-R1-Distill-Llama-8B (\textbf{DS-R1-8B})~\cite{guo_deepseek_2025} for logical form generation. We see that, even with the same T5-base model for the logical form generator, \model\ outperforms TIARA consistently. This further confirms the effectiveness of our model design. As for DS-R1-8B, it offers accuracy slightly lower than that of the default LLaMA3.1-8B model. We conjecture that this is because DS-R1-8B is distilled from DeepSeek-R1-Zero, which focuses on reasoning capabilities and is not specifically optimized for the generation task.


%\subsection{Additional Results}
We also have results on parameter impact, model running time, a case study, and error analyses. They are documented in Appendices~\ref{app:paramater_study} to~\ref{app:error_analysis}.

\begin{table}[t]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{lcccc}
\toprule
\textbf{Model}                & \textbf{Overall} & \textbf{I.I.D.}& \textbf{Comp.} & \textbf{Zero.} \\ \midrule
TIARA (T5-base)   & 81.9   & 91.2  & 74.8  & 80.7  \\ 
\hspace{3pt} w RG-EMD \& RG-CER           & 84.3   & 92.3  & 78.1  & 83.3  \\
\hspace{3pt} w DED \& SC     & 85.6   & 92.3  & 79.8  & 85.0  \\
%\hspace{3pt} \jz{w DED}             &        &       &       &       \\
\midrule
\textbf{\model}            & \textbf{88.5}   & \textbf{94.6}  & \textbf{83.6}  & \textbf{87.9}  \\
\hspace{3pt} w T5-base       & 84.9   & 92.6  & 81.0  & 83.3  \\
%\hspace{3pt} w T5-large \jz{T5-3B?}      & 87.5   & 96.3  & 82.0  & 86.0  \\
 \hspace{3pt} w DS-R1-8B &   87.5     &  94.0     &  82.4     &  86.7     \\ \bottomrule
\end{tabular}
}
\caption{Module applicability results (F1 score) on the validation set of GrailQA. EM scores are in Appendix~\ref{app:applicability}.}\label{tab:exp_applicability}
\end{table}

\section{Conclusion}\label{sec:conclusion}
We proposed \model for the KBQA task. Our core innovations include: (1) using relation to guide the retrieval of entities; (2) deferring entity disambiguation to the logical form generation stage; and (3) enriching logical form generation with schema contexts to constrain search space. Together, we achieve a model that tops the leaderboard of a popular non-I.I.D. dataset GrailQA, outperforming SOTA models by 4.0\%, 2.9\%, and 3.3\% in F1 under compositional generalization, zero-shot generalization, and overall test settings, respectively. Our model also performs well in the I.I.D. setting, outperforming SOTA models on WebQSP.


\section*{Limitations}
%Despite the strong reported performance of \model, there are potentials to further improve the model.
First, like any other supervised models, \model requires annotated samples for training which may be difficult to obtain for many domains. Exploiting LLMs to generate synthetic training data is a promising direction to address this issue. Second, as discussed in the error analysis in Appendix~\ref{app:error_analysis}, errors can still arise from the relation retrieval, entity retrieval, and logical form generation modules. There are rich opportunities in further strengthening these modules. 
Particularly, as we start from relation extraction, the overall model accuracy relies on highly accurate relation extraction. It would be interesting to explore how well \model performs on even larger KBs with more relations.

\section*{Ethics Statement}
This work adheres to the ACL Code of Ethics and is based on publicly available datasets, used in compliance with their respective licenses. As our data contains no sensitive or personal information, we foresee no immediate risks. To promote reproducibility and further research, we also open-source our code.

%we acknowledge two major limitations in this study.Firstly, our method is trained on annotated question-logical form pairs, but the annotation cost for such data is expensive. The second The sencond major limitation lies in the time efficiency of our method. We report and discuss the training and inferecne runtime of our method in Appendix~\ref{app:time}. To guide the retreival and generation processes by KB schema, \model~utilizes additional KB queries to obtain the schema information for the corresponding KB elements. For example, In SG-LF, the class information of the selected KB elements are queried from the KB. The guidance from schema information significantly enhances the generalization capability of our method, but it also incurs an increase in time comsuption and computational cost. Compared to TIARA, our method taks on average 3.2 seconds longer inference time per questions.
%\section*{Acknowledgment}

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{references}

\newpage

\appendix


\section{S-Expression}\label{sec:app_sexpression}

S-expressions~\cite{gu_beyond_2021} use set-based semantics defined over a set of operators and operands. The operators are represented as functions. 
Each function takes a number of arguments (i.e., the operands). Both the arguments and the return values of the functions are either a set of entities or entity tuples (or tuples of an entity and a literal). The functions available in S-expressions are listed in Table~\ref{tab:logical_form_operators}, where a set of entities typically refers to a class (recall that a class is defined as a set of entities sharing common properties) or individual entities, and a binary tuple typically refers to a relation. %By applying those functions defined in our grammar, we are able to get more complex set of entities and binary tuples. 

\begin{table*}[h]
\small
\setlength{\tabcolsep}{2pt}
%\begin{tabular}{p{0.15\linewidth}p{0.3\linewidth}p{0.4\linewidth}}
\begin{tabular}{lll}
\toprule
\multicolumn{1}{l}{\textbf{Function}}                                             & \multicolumn{1}{l}{\textbf{Return value}} & \multicolumn{1}{l}{\textbf{Description}}                                                               \\ \midrule
(\texttt{AND} $u_1$ $u_2$)                                                                       & a set of entities                    & The \texttt{AND} function returns the intersection of two sets $u_1$ and $u_2$                                               \\ \hline
(\texttt{COUNT} $u$)                                                                         & a singleton set of integers           & The \texttt{COUNT} function returns the cardinality of set $u$                                                 \\ \hline
(\texttt{R} $b$)                                                                             & a set of (entity, entity) tuples     & The \texttt{R} function reverses each binary tuple $(x, y)$ in set $b$ to $(y, x)$                                   \\ \hline
(\texttt{JOIN} $b$ $u$)                                                                        & a set of entities                    & Inner \texttt{JOIN} based on entities in set $u$ and the second element of tuples in set $b$                                    \\ \hline
(\texttt{JOIN} $b_1$ $b_2$)                                                                      & a set of (entity, entity) tuples     & Inner \texttt{JOIN} based on the first element of tuples in set $b_2$ and the second element\\
& &  of tuples in set $b_1$             \\ \hline
\begin{tabular}[c]{@{}l@{}}(\texttt{ARGMAX} $u$ $b$)\\ (\texttt{ARGMIN} $u$ $b$)\end{tabular}                & a set of entities                    & These functions return $x$ in $u$ such that $(x,y) \in b$ and $y$ is the largest / smallest                                   \\ \hline
\begin{tabular}[c]{@{}l@{}}(\texttt{LT} $b$ $n$)\\ (\texttt{LE} $b$ $n$)\\ (\texttt{GT} $b$ $n$)\\ (\texttt{GE} $b$ $n$)\end{tabular} & a set of entities                    & These functions return all $x$ such that $(x, v) \in b$ and $v$ $<$ / $\le$ / $>$ / $\ge$ $n$ \\ \bottomrule
\end{tabular}
\caption{Functions (operators) defined in S-expressions ($u$: a set of entities, $b$: a set of (entity, entity or literal) tuples, $n$: a numerical value).}\label{tab:logical_form_operators}
\end{table*}

\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/prompt_example_new.png}
    \caption{Example prompt to our fine-tuned LLM-based logical form generator for an input question: \textsf{Captain pugwash makes an appearance in which comic strip?}}
    \label{fig:prompt_example}
\end{figure*}
\section{Prompt Example}\label{app:prompt}

We show an example prompt to our fine-tuned LLM-based logical form generator containing top-20 relations and top-2 entities per mention retrieved by our model in Figure~\ref{fig:prompt_example}.

\section{Additional Details on the WebQSP Dataset}\label{app:WebQSP}
\textbf{WebQuestionsSP} (WebQSP)~\cite{yih_value_2016} is an I.I.D. dataset. It contains 4,937 questions collected from Google query logs, including 3,098 questions for training and 1,639 for testing, each annotated with a target SPARQL query. %We convert each SPARQL query into the corresponding S-expression and extract 
We follow GMT-KBQA~\cite{hu_logical_2022}, TIARA~\cite{shu_tiara_2022} to separate 200 questions from the training questions to form the validation set.

\section{Baseline Models}\label{sec:app_baselines}
The following models are tested against \model\ on the GrailQA dataset:
\begin{itemize}
    \item RnG-KBQA~\cite{ye_rng-kbqa_2022} enumerates and ranks all possible logical forms within two hops from the entities retrieved by an entity retrieval step. It  uses a Seq2Seq model to generate the target logical form based on the input question and the top-ranked candidate logical forms.
    \item TIARA~\cite{shu_tiara_2022} shares the same overall procedure with RnG-KBQA. It further retrieves entities, relations, and classes based on the input question and feeds these KB elements into the Seq2Seq model together with the question and the top-ranked candidate logical forms to generate the target logical form.  
    
    
    %\jz{It demonstrates the connectivity of the KB elements through example logical forms starting from the entities. [How are these example logical forms obtained and how are they used?]} Finally, a Seq2Seq model converts the question and retrieved KB elements into the target logical form.

    \item TIARA+GAIN~\cite{shu_data_2024} enhances TIARA using a training data augmentation strategy. It synthesizes additional question-logical form pairs for model training to enhance the model's capability to handle more entities and relations. This is done by a 
     graph traversal to randomly sample logical forms from the KB  and a PLM to generate questions corresponding to the logical forms (i.e., the ``GAIN'' module). TIARA+GAIN is first tuned using the synthesized data and then tuned on the target dataset, for its retriever and generator modules which both use PLMs.
    
    \item Decaf~\cite{yu_decaf_2023} uses a Seq2Seq model that takes as input a question and a linearized question-specific subgraph of the KG and jointly decodes into both a  logical form and an answer candidate. The logical form is then executed, which produces a second answer candidate if successful. The final answer is determined from these two answer candidates with a scorer model. 

    \item Pangu~\cite{gu_dont_2023} formulates logical form generation as an iterative  enumeration process starting from the entities retrieved by an entity retrieval step. 
    At each iteration, partial logical forms generated so far are extended following paths in the KB to generate more and longer partial logical forms. A language model is used to select the top partial logical forms to be explored in the next iteration, under either fined-tuned models (T5-3B) or few-shot in-context learning (Codex). 

    
    \item FC-KBQA~\cite{zhang_fc-kbqa_2023} employs an intermediate module to test the connectivity between the retrieved KB elements, and it  generates the target logical form using the connected pairs of the retrieved KB elements through a Seq2Seq model.

     
    \item RetinaQA~\cite{faldu_retinaqa_2024} uses both a ranking-based method and a generation-based method (TIARA) to generate logical forms, which are then scored by a discriminative model to determine the output logical form.
    
    \item KB-BINDER~\cite{li_few-shot_2023} uses a training-free few-shot in-context learning model based on LLMs. It generates a draft logical form by showcasing the LLM examples of questions and logical forms (from the training set) that are similar to the given test question. Subsequently, a retrieval module grounds the surface forms of the KB elements in the draft logical form to specific KB elements.
    
    % \item FlexKBQA~\cite{li_flexkbqa_2024} is also a few-shot in-context learning model based on LLMs. To address the issue that the model generated logical forms are often unexecutable, it samples executable logical forms from the KB (like GAIN above) and instructs an LLM to generate a corresponding user question. These logical form-question pairs can then be used to fine-tune a lightweight model for logical form generation given an input question. \jz{Double check}
    \item FlexKBQA~\cite{li_flexkbqa_2024} considers limited training data and leverages an 
    LLM to generate additional training data. 
    It samples executable logical forms from the KB and utilizes an LLM with few-shot in-context learning to convert them into natural language questions, forming synthetic training data. These data, together with a few real-world training samples, are used to train a KBQA model. Then, the model is used to generate logical forms with more real world questions (without ground truth), which are filtered through an execution-guided module to prune the erroneous ones. The remaining logical forms and the corresponding real-world questions are used to train a new model. This process is repeated, to align the distributions of  synthetic training data and real-world questions. 
    
    %It introduces an execution-guided teacher-student iterative training method to bridge the gap between synthetic and real-world questions. The teacher model generates pseudo logical forms for unlabeled questions, which are filtered for quality, and used to iteratively train a student model, refining the logical form parser.}
    
    
    % is a flexible few-shot KBQA framework that leverages few-shot in-context learning to generate synthetic data using an LLM for training a lightweight model. It first extracts logical form templates from the few-shot annotated samples by replacing entities and relations with variables (e.g. ent0, rel0, ent1). These templates are then step-wise grounded with KB elements collected from the KB, generating a substantial number of executable logical forms. A LLM translates the obtained logical forms into natural language questions through in-context learning, constructing a synthetic dataset of logical form-question pairs. 
    
    % To address the distribution discrepancy between synthetic and real-world questions, it proposes an execution-guided teacher-student iterative training method. First, a teacher model is trained using synthetic data and a few annotated samples. The teacher model then generates pseudo logical forms for unlabeled real-world user questions, and an execution-guided filtering mechanism removes unexecutable or low-quality data. The filtered data, along with the synthetic data, is used to train a student model, which becomes the new teacher model in the next iteration. This process is repeated until the model converges, resulting in the final logical form parser.}
    
\end{itemize}


The following models are tested against \model\ on the WebQSP dataset:
\begin{itemize}
    % \item Subgraph Retrieval (SR)~\cite{zhang_subgraph_2022} uses a sequential decision process to progressively expand the subgraph corresponding to the question starting from the topic entity.
    \item Subgraph Retrieval (SR)~\cite{zhang_subgraph_2022} focuses on retrieving a KB subgraph relevant to the input question. It does not concern retrieving the exact question answer by  reasoning over the subgraph. Starting from the topic entity, it performs a top-$k$ beam search at each step to progressively expand into a subgraph, using a scorer module to score the candidate relations to be added to the subgraph next. 

    %In each expansion step, it uses a dual-encoder to encode a  candidate relation for the expansion and the concatenation of the input question and the historical relation path from previous steps, respectively.    
    %The dot product of the obtained embeddings is used to select the top-ranked candidate relations for the current step or terminating the expansion. 
    
    \item Evidence Pattern Retrieval (EPR)~\cite{ding_enhancing_2024} aims to extract subgraphs with fewer noise entities. It starts from the topic entities and expands by retrieving and ranking atomic (topic entity-relation or relation-relation) patterns relevant to the question. This forms a set of relation path graphs (i.e., the candidate \emph{evidence patterns}). The relation path graphs are then ranked to select the most relevant one. By further retrieving the entities on the selected relation path graph, EPR obtains the final subgraph relevant to the input question. 
    

    
    \item Neural State Machine (NSM)~\cite{he_improving_2021} is a reasoning model to find answers for the KBQA problem from a subgraph (e.g., retrieved by SR or EPR). It address the issue of lacking intermediate-step supervision signals when reasoning through the subgraph to reach the answer entities. This is done by training a so-called teacher model that follows a bidirectional reasoning mechanism starting from both the topic entities and the answer entities. During this process, the  ``distributions'' of entities, which represent their probabilities to lead to the answer entities (i.e., intermediate-step supervision signal), are propagated. 
    A second model, the so-called student model, learns from the teacher model to generate the entity distributions, with knowledge of the input question and the topic entities but not the answer entities. Once trained, this model can be used for KBQA answer reasoning. 

    
    
    % KBQA~\cite{he_improving_2021} is a widely used answer reasoning model on subgraphs, often combined with different subgraph retrieval methods (e.g. SR~\cite{zhang_subgraph_2022}, EPR~\cite{ding_enhancing_2024}), which employs a teacher-student framework to solve multi-hop KBQA tasks. The student model is based on NSM~\cite{hudson_nsm_2019} that consists of two components: the instruction component and the reasoning component. The instruction component uses an LSTM to encode the question and generate a series of instruction vectors that guide the reasoning process. The reasoning component relies on a "propagation-aggregation" mechanism to aggregate and update information about entities and relations in the KB based on the current instruction, gradually maintaining and updating the entity distribution. On the other hand, the teacher model generates intermediate entity distributions by simultaneously performing forward (starting from the topic entity) and backward (starting from the answer entity) reasoning. It uses a consistency constraint between the forward and backward reasoning processes, specifically minimizing the difference between the two using the Jensen-Shannon divergence at each intermediate step. This ensures the reliability of the intermediate entity distributions. Finally, the intermediate supervision signal generated by the teacher model is used to guide the student model, optimizing its reasoning path and improving the overall answering reasoning accuracy.
    
    \item UniKGQA~\cite{jiang_unikgqa_2023} integrates both retrieval and reasoning stages to enhance the accuracy of multi-hop KBQA tasks. It trains a PLM to learn the semantic relevance between every relation and the input question. The semantic relevance information is  propagated and aggregated through the KB to form the semantic relevance between the entities and the input question. The entity with the highest semantic relevance is returned as the answer.
        
    
    \item ChatKBQA~\cite{luo_chatkbqa_2024} fine-tunes an open-source LLM to map questions into draft logical forms. The  ambiguous KB items in the draft logical forms are replaced with specific KB elements by a separate retrieval module.
    
    \item TFS-KBQA~\cite{wang_no_2024} fine-tunes an LLM for more accurate logical form generation with three strategies.  The first strategy directly fine-tunes the LLM to map natural language questions into draft logical forms containing entity names instead of entity IDs. The second strategy breaks the mapping process into two steps, first to generate relevant KB elements, and then to generate draft logical forms using the KB elements. The third strategy fine-tunes the LLM to directly generate the answer to an input question. 
    After applying the three fine-tuning strategies, the LLM is used to map natural language questions into draft logical forms at model inference. A separate entity linking module is used to further map the entity names in draft logical forms into entity IDs. 

\end{itemize}




% \begin{figure*}[h]
%     \centering
%     \includegraphics[width=0.7\linewidth]{figures/case_study.png}
%     \caption{Case Study of logical form generation by \model\ and other models on the GrailQA validation set. Incorrect relations and entities are marked in red, while the correct relations and entities are colored in green and blue, respectively}
%     \label{fig:case_study}
% \end{figure*}

% \begin{figure*}[h]
%     \centering
%     \includegraphics[width=0.7\linewidth]{figures/md_case.png}
%     \caption{Case study of entity mention detection by our model and SpanMD (a mention detection method commonly used by SOTA KBQA models) on the GrailQA validation set. The incorrect entity mention detected is colored in red, while the correct entity mentions detected are colored in green and blue, respectively. \jz{``GER'' $\rightarrow$ ``Ours''} }
%     \label{fig:md_case}
% \end{figure*}



% Please add the following required packages to your document preamble:
% \usepackage{multirow}




% \begin{figure*}[h]
%     \centering
%     \includegraphics[width=0.8\linewidth]{figures/rlg_case.png}
%     \caption{Case study of logical form generation by our \model\ model and \jz{two representative baseline models TIARA and Pangu} on the GrailQA validation set. Incorrect relations and entities retrieved are colored in red, while correct relations and entities retrieved are colored in green and blue, respectively. The same sets of noisy entities and relations are retrieved by all three models, while only our model \model\ is able to produce the correct logical form. \jz{Use table instead, separate TIARA/Pangu from ours, same for the figure above.}}
%     \label{fig:rlg_case}
% \end{figure*}

% \section{Case Study}

% Figure~\ref{fig:case_study} shows an example case from the GrialQA validation set, with the prediction from our \model, the SOTA Pangu, and TIARA. In this example, both the top-1 candidate for entity retrieval and relation retrieval are non-optimal. Unlike previous methods PANGU and TIARA, which solely rely on the top-1 candidate entity to generate logical forms, our model can combine KB elements using class-based contextualization and select the optimal combination of KB elements based on their relevance to the question to form the final logical form, without overly depending on the performance of the retrieval module.


\begin{table*}[ht]
\small
\centering
\begin{tabular}{
>{}p{0.3\linewidth}
>{\centering\arraybackslash}m{0.05\linewidth}
>{\centering\arraybackslash}m{0.05\linewidth} 
>{\centering\arraybackslash}m{0.05\linewidth} 
>{\centering\arraybackslash}m{0.05\linewidth} 
>{\centering\arraybackslash}m{0.05\linewidth} 
>{\centering\arraybackslash}m{0.05\linewidth} 
>{\centering\arraybackslash}m{0.05\linewidth} 
>{\centering\arraybackslash}m{0.05\linewidth} }
\toprule
& \multicolumn{2}{c}{\textbf{Overall}} & \multicolumn{2}{c}{\textbf{I.I.D.}} & \multicolumn{2}{c}{\textbf{Compositional}} & \multicolumn{2}{c}{\textbf{Zero-shot}} \\ 
\cmidrule{2-9}
\multirow{-2}{*}{}  \textbf{\vspace{-0.5cm}Model} & \textbf{EM}                   & \textbf{F1}  & \textbf{EM} & \textbf{F1} & \textbf{EM} & \textbf{F1}                     & \textbf{EM}  & \textbf{F1}  \\ \midrule
% TIARA & 75.3 & 81.9 & 88.4 & 91.2 & 66.4 & 74.8 & 73.3 & 80.7 \\
% FC-KBQA & 79.0 & 83.8 & 89.0 & 91.5 & 70.4 & 77.3 & 78.1 & 83.1 \\
% RetinaQA & 77.8 & 83.3 & 88.6 & 91.2 & 70.5 & 77.5 & 76.2 & 82.3 \\
% TIARA + Generative Entity Retrieval &79.5 &84.3 &90.3 &92.3 &71.2 &78.1 &78.3 &83.3 \\
% TIARA (LLaMA3-8B) & 79.9 & 85.6 & 88.6 & 92.3 & 72.7& 79.8 & 79.0 & 85.0 \\
%\midrule
%\textbf{\model} & \textbf{83.8} & \textbf{88.0} & \textbf{91.1} & \textbf{93.3} & \textbf{76.6} & \textbf{82.6} & \textbf{83.6} & \textbf{87.9} \\
\textbf{\model} & \textbf{85.1} & \textbf{88.5} & \textbf{93.1} & \textbf{94.6} & \textbf{78.4} & \textbf{83.6} & \textbf{84.4} & \textbf{87.9} \\
\hdashline
  \rule{0pt}{10pt}\hspace{6pt} w/o RG-EMD & 81.3 & 85.3 & 90.6 & 92.4 & 74.4 & 80.2 & 80.2 & 84.3 \\
 \hspace{6pt} w/o RG-CER &  82.8 & 86.5 & 90.2 & 92.1 & 75.4 & 81.1 & 82.7 & 86.3 \\ 
\hspace{6pt} w/o DED & 84.3 & 87.8 & 92.6 & 94.0 & 77.1 & 82.4 & 83.7 & 87.2 \\ 
\hspace{6pt} w/o SC & 76.6 & 79.2 & 91.7 & 92.9 & 72.3 & 77.4 & 71.7 & 73.9\\
\hspace{6pt} w/o Fallback LF & 81.8 & 84.6 & 92.8 & 94.1 & 77.3 & 81.8 & 78.7 & 81.5\\
% \hline
% \rule{0pt}{10pt}Top-1 Refined Entity (GER) + TIARA &79.5 &84.3 &90.3 &92.3 &71.2 &78.1 &78.3 &83.3\\
%  TIARA's Entity Retrieval + RLG & 79.9 & 85.6 & 88.6 & 92.3 & 72.7& 79.8 & 79.0 & 85.0 \\

% \hspace{6pt} w Top1-entity per mention & 83.0 & 86.7 & 89.2 &81.8 & 75.4 & 80.8 & 83.4 & 86.9 \\
% \hspace{6pt} w/o Class & 82.8 & 86.8 & 89.9 & 92.4 & 75.3 & 81.5 & 82.8 & 86.6 \\ 
% \hspace{6pt} w/o Generative Entity Retrieval  & 80.9 &	86.4 &	89.1 &	92.3 &	75.4 &	82.4 &	79.7 &	85.5 \\
% \hspace{6pt} w/o Candidate Logical Forms & 80.1 & 83.9 & 90.7 & 92.5 & 75.2 & 80.6 & 77.5 & 81.5 \\
% \hspace{6pt} w T5-Base & 78.7 & 83.2 & 88.1 & 90.4 & 69.5 & 75.6 & 78.4 & 83.3 \\
% \hspace{6pt} w/o Context & 74.7 & 79.3 & 81.9 & 85.6 & 65.3 & 79.7 & 75.5 & 80.1 \\

\bottomrule
\end{tabular}
\caption{Ablation study results on the validation set of GrailQA.}
\label{tab:ablation_full}
\end{table*}

\section{Implementation Details}\label{app:implemention_details}

All our experiments are run on a machine with an NVDIA A100 GPU and 120 GB of RAM. We fine-tuned three \texttt{bert-base-uncased} models for a maximum of three epochs each, for relation retrieval, entity ranking, and fallback logical form ranking.
For relation retrieval, we randomly sample 50 negative samples for each question to train the model to distinguish between relevant and irrelevant relations. 

For each dataset, a \texttt{T5-base} model is fine-tuned for 5 epochs as our logical form sketch parser, with a beam size of 3 (i.e., $k_L = 3$) for GrailQA, and 4 for WebQSP. For candidate entity retrieval, we use the same number (i.e., $k_{E1} + k_{E2}  = 10$) of candidate entities per mention as that used by the baseline models~\cite{shu_tiara_2022,ye_rng-kbqa_2022}. The retrieved candidate entities for a mention consist of entities with the top-$k_{E1}$ popularity scores and $k_{E2}$ entities connected to the top-ranked relations in $R_q$, where $k_{E1} = 1$, $k_{E2} = 9$ for GrailQA, $k_{E1} = 3$, $k_{E2} = 7$ for WebQSP. We select the top-20 (i.e., $k_R$ = 20) relations and the top-2 (i.e., $k_{E3} = 2$) entities (for each entity mention) retrieved by our model. For WebQSP, we also use the candidate entities obtained from the off-the-shelf entity linker ELQ~\cite{li_efficient_2020}. 

Finally, we fine-tune LLaMA3.1-8B with LoRA~\cite{hu_lora_2021} for logical form generation. On GrailQA, LLaMA3.1-8B is fine-tuned for 5 epochs with a learning rate of $0.0001$. On WebQSP, it is fine-tuned for 20 epochs with the same learning rate (as it is an I.I.D. dataset where more epochs are beneficial). During inference, we generate logical forms by beam search with a beam size of 10 (i.e., $K_O = 10$). The generated logical forms are executed on the KB to filter non-executable ones. If none of the logical forms are executable, we check candidate logical forms from the fallback procedures, and the result of the first executable one is returned as the answer set.
%\jz{Any updates needed for this subsection?} 


% Our system parameters have been chosen empirically. While there are a few of them, their exact values do not have strong impact on the final model performance, \jz{and the choice of parameter values generalize well across  datasets. The same parameter values are used on both datasets. [not true any more?]} 
Our system parameters are selected empirically. There are only a small number of parameters to consider. As shown in the parameter study later, our model performance shows stable patterns against the choice of parameter values. The parameter values do not take excessive fine-tuning. 

\section{Full Ablation Study Results (GrailQA)}\label{app:ablation}
%\jz{Fix table and add discussion on the results}

Table~\ref{tab:ablation_full} presents the full ablation study results on the validation set of GrailQA. We observe a similar trend to that of the F1 score results reported earlier --  all ablated model variants yield lower EM scores compared to the full model. 

For the retrieval modules, RG-EMD improves the F1 score by 3.2 points and the EM score by 3.8 points on GrailQA (i.e., \model\ vs. \model w/o RG-EMD for overall results), while achieving a 1.9-point increase in the F1 score on WebQSP (see Table~\ref{tab:webqsp} earlier). It achieves an increase of 3.4 points or larger in the F1 score on the compositional and zero-shot tests, which is larger than the 2.2-point improvement on the I.I.D. tests. This shows that relation-guided mention detection effectively enhances the generalization capability of KBQA entity retrieval. For the other module RG-CER, removing it (\model w/o RG-CER) results in a 2.5-point drop in the F1 score for both the I.I.D. and compositional tests, while the impact is smaller on the zero-shot tests (1.6 points). This is because the lower accuracy in relation retrieval under zero-shot tests leads to error propagation into relation-guided candidate entity retrieval, reducing the benefits of this module.  

For the generation modules, \model\ w/o DED negatively impacts the F1 scores on both GrailQA and WebQSP, confirming that deferring entity disambiguation effectively mitigate error propagation between the retrieval and generation stages. For \model w/o SC, it reduces the F1 score by 1.7 points and 3.2 points on the GrialQA I.I.D. tests and on WebQSP. The drop is more significant on the compositional and zero-shot tests, i.e., by 6.2 points and 14.0 points, respectively. This indicates that schema contexts can effectively guide the LLM to reason and identify the correct combinations of KB elements unseen at training.


In Table~\ref{tab:ablation_full}, we present an additional model variant, \model w/o Fallback LF, which removes the fall back logical form generation strategy from \model. We see that \model\ has lower accuracy without the strategy. %Importantly, even without this fall back strategy, our model has high accuracy results in both EM and F1 comparing with the baseline models as shown in the table. 
We note that this fallback strategy is \emph{not} the reason why \model\ outperforms the baseline models. 
TIARA also uses this fallback strategy, while RetinaQA uses the top executable logical form from the fallback strategy as one of the options to be selected by its  discriminator to determine the final logical form output.

\section{Full Module Applicability  Results}\label{app:applicability}


\begin{table*}[t]
\small
\centering
\begin{tabular}{
>{}p{0.3\linewidth}
>{\centering\arraybackslash}m{0.05\linewidth}
>{\centering\arraybackslash}m{0.05\linewidth} 
>{\centering\arraybackslash}m{0.05\linewidth} 
>{\centering\arraybackslash}m{0.05\linewidth} 
>{\centering\arraybackslash}m{0.05\linewidth} 
>{\centering\arraybackslash}m{0.05\linewidth} 
>{\centering\arraybackslash}m{0.05\linewidth} 
>{\centering\arraybackslash}m{0.05\linewidth} }
\toprule
& \multicolumn{2}{c}{\textbf{Overall}} & \multicolumn{2}{c}{\textbf{I.I.D.}} & \multicolumn{2}{c}{\textbf{Compositional}} & \multicolumn{2}{c}{\textbf{Zero-shot}} \\ \cline{2-9} 
\multirow{-2}{*}{} \rule{0pt}{10pt}  \textbf{\vspace{-0.5cm}Model} & \textbf{EM}                   & \textbf{F1}  & \textbf{EM} & \textbf{F1} & \textbf{EM} & \textbf{F1}                     & \textbf{EM}  & \textbf{F1}  \\ \midrule 
TIARA (T5-base) & 75.3 & 81.9 & 88.4 & 91.2 & 66.4 & 74.8 & 73.3 & 80.7 \\
\hspace{6pt} w RG-EMD \& RG-CER & 79.5 &84.3 &90.3 &92.3 &71.2 &78.1 &78.3 &83.3 \\
\hspace{6pt} w DED \& SC &79.9 & 85.6 & 88.6 & 92.3 & 72.7& 79.8 & 79.0 & 85.0\\  
%FC-KBQA (COLING 2024) & 79.0 & 83.8 & 89.0 & 91.5 & 70.4 & 77.3 & 78.1 & 83.1 \\
%RetinaQA (ACL 2024) & 77.8 & 83.3 & 88.6 & 91.2 & 70.5 & 77.5 & 76.2 & 82.3 \\
% TIARA + Generative Entity Retrieval &79.5 &84.3 &90.3 &92.3 &71.2 &78.1 &78.3 &83.3 \\
% TIARA (LLaMA3-8B) & 79.9 & 85.6 & 88.6 & 92.3 & 72.7& 79.8 & 79.0 & 85.0 \\
\midrule
%\textbf{\model} (Ours) \rule{0pt}{10pt} & 83.8 & 88.0 & 91.1 & 93.3 & 76.6 & 82.6 & 83.6 & 87.9 \\
\textbf{SG-KBQA} & \textbf{85.1} & \textbf{88.5} & \textbf{93.1} & \textbf{94.6} & \textbf{78.4} & \textbf{83.6} & \textbf{84.4} & \textbf{87.9}\\
\hspace{6pt} w T5-base & 80.6 & 84.9 & 89.9 & 92.6 & 73.8 & 81.0 & 79.4 & 83.3\\
\hspace{6pt} w DS-R1-8B & 83.6 & 87.5 & 92.3 & 94.0 & 75.4 & 82.4 & 83.1 & 86.7 \\


%   SGER + TIARA & 79.5 &84.3 &90.3 &92.3 &71.2 &78.1 &78.3 &83.3 \\
%  TIARA + SGLF & 79.9 & 85.6 & 88.6 & 92.3 & 72.7& 79.8 & 79.0 & 85.0\\
%  TIARA + multiple entities  \\ 
% SG-KBQA w T5-base & 81.7 & 85.6 & 93.8 & 95.6 & 73.3 & 80.5 & 79.9 & 83.3 \\
% SG-KBQA w T5-large & 83.5 & 87.5 & 95.2 & 96.3 & 73.5 & 82.0 & 82.5 & 86.0\\
% SG-KBQA w deepseek-r1-distilled-8B \\
% \hline
% \rule{0pt}{10pt}Top-1 Refined Entity (GER) + TIARA &79.5 &84.3 &90.3 &92.3 &71.2 &78.1 &78.3 &83.3\\
%  TIARA's Entity Retrieval + RLG & 79.9 & 85.6 & 88.6 & 92.3 & 72.7& 79.8 & 79.0 & 85.0 \\

\bottomrule
\end{tabular}
\caption{Full module applicability results on the validation set of GrailQA.}
\label{tab:applicability_full}
\end{table*}

To evaluate the applicability of our proposed modules, we conduct a module applicability study with TIARA (an open-source retrieve-then-generate baseline) and different generation models (i.e., T5-base and DeepSeek-R1-Distill-Llama-8B). 


Table~\ref{tab:applicability_full} reports the results. Replacing TIARA's entity retrieval module with ours (TIARA w RG-EMD \& RG-CER) helps boost the EM and F1 scores by 4.2 and 2.4 points overall, comparing against the original TIARA model. This improvement is primarily from the tests with KB elements or compositions that are unseen at training, as evidenced by the larger performance gains on the compositional and zero-shot tests, i.e., 3.3 and 2.6 points in the F1 score, respectively. Similar patterns are observed for TIARA w DED \& SC that replaces TIARA's logical form generation module with ours. 
These results demonstrate that our proposed modules can enhance the retrieval and generation steps of other compatible models, especially under non-I.I.D. settings. 

Further, using the same language model (i.e., T5-base in TIARA) to form logical form generation modules, our model \model\ w T5-base still outperforms TIARA by 5.3 points  3.0 points in the EM and F1 scores for the overall tests. This confirms that the overall effectiveness of our model stems from its design rather than the use of a larger model for logical form generation. As for \model w/ DS-R1-8B, it reports close performance to \model, indicating that \model does not rely on a particular LLM.


% \jz{Fix table and add discussion on the results}
% \jz{Also results on WebQSP?}


% \section{Entity Retrieval Results}\label{app:er_results}

% \begin{table}[H]
%     \small
%     \centering
%     \begin{tabular}{lccc} 
%     \toprule \textbf{Model} & \textbf{P} & \textbf{R} & \textbf{F1} \\
%     \midrule 
%     RnG-KBQA  & 84.1 & 86.8 & 80.4 \\
%     TIARA  & 87.2 & 88.6& 85.4 \\
%     \midrule
%     \textbf{SG-ER (Top-1)} & \textbf{91.9} & \textbf{93.6} & \textbf{90.5} \\
%     \hspace{6pt}w/o RG-EMD & 88.9 & 91.3 & 88.2 \\
%     \hspace{6pt}w/o RG-CER & 88.7 & 90.0 & 86.9 \\
%     \bottomrule
%     \end{tabular}
%         \caption{Precision (P), recall (R) and F1 of entity retrieval (\%) on the validation set of GrailQA.}
%     \label{tab:entity_retrieval}
% \end{table}


% \sxfix{We report the performance of our SG-ER (Top-1) on the GrailQA validation set in Table~\ref{tab:entity_retrieval}. We compare against the following baselines: 1)\textbf{RnG-KBQA}~\cite{ye_rng-kbqa_2022} which adopts a BERT-NER system to detect entity mentions. 2)\textbf{TIARA}~\cite{shu_tiara_2022} which models entity mention detection to span classification task to detect entity spans. To ensure a fair comparison with the baselines, we follow their approaches by extracting the top-1 entity for each mention from our SG-ER. It can be observed that our SG-ER siginificantly surpasses all the baselines by at least 5.1 F1 points.

% Furthermore, \textbf{w/o RG-EMD} shares the same mention detector with TIARA, indicating that our RG-CER module is effective by improving the entity retrieval F1 by 2.8 points. \textbf{w/o RG-CER} shares the same candidate entity retrieval methods with the baselines but boosts entity retrieval F1 by at least 1.5 points. This demonstrates that our RG-EMD can more accurately identify entity boundaries in the input question.}


\section{Parameter Study}\label{app:paramater_study}

We conduct a parameter study to investigate the impact of the choice of values for our system parameters. When the value of a parameter is varied, default values as mentioned in Appendix~\ref{app:implemention_details} are used for the other parameters. 

%\jz{Add results and discussion}
\begin{figure}[htb] 
    \begin{minipage}[b]{0.5\linewidth}  
        \centering
        \includegraphics[width=\textwidth]{figures/KL.pdf} 
        \captionsetup{font=small}
        \subcaption{$k_L$}
       % \label{fig:sub1}
    \end{minipage}%
    \hfill 
    \begin{minipage}[b]{0.5\linewidth}  
        \centering
        \small
        \includegraphics[width=\textwidth]{figures/KE1.pdf}
        \captionsetup{font=small}
        \subcaption{$k_{E1}$}
        %\label{fig:sub2}
    \end{minipage}
    \caption{Impact of $k_L$ and $k_{E1}$ on the recall of candidate entity retrieval.} %\jz{font size in figure too small, could reduce the data points if needed more space; candidate entity coverage $\rightarrow$ recall of candidate entity retrieval?}}  
    \label{fig:kl_ke1}
\end{figure}


Figure~\ref{fig:kl_ke1} presents the impact of  $k_L$ and $k_{E1}$ on the recall of candidate entity retrieval (i.e., the average percentage of ground-truth entities returned by our candidate entity retrieval module for each test sample). Here, for the GrailQA dataset, we report the results on the overall tests (same below). 
Recall that $k_L$ means the number of logical form sketches from which entity mentions are extracted, while $k_{E1}$ refers to the number of candidate entities retrieved based on the popularity scores. 

As $k_L$ increases, the recall of candidate entity retrieval grows, which is expected. The growth diminishes gradually. This is because a small number of questions contain complex entity mentions that are difficult to handle (see error analysis in Appendix~\ref{app:error_analysis}). As $k_L$ increases, the precision of the retrieval also reduces, which brings noise into the entity retrieval results and additional computational costs. 
To strike a balance, we set $k_L = 3$ for GrailQA and  $k_L = 4$ for WebQSP. We also observe that the recall on WebQSP is lower than that on GrailQA. This is because  WebQSP has a smaller training set to learn from. 


As for $k_{E1}$, when its value increases, the candidate entity recall generally drops. This is because an increase in $K_{E1}$ means to select more candidate entities based on popularity while fewer from those connected to the top retrieved relations but with lower popularity scores. 
Therefore, we default $k_{E1}$ at $1$ for GrailQA and $3$ for WebQSP, which yield the highest recall for the two datasets, respectively. 
Recall that we set the total number of candidate entities for each entity mention to 10 ($K_{E1} + K_{E2} = 10$), following our baselines (e.g., TIARA, RetinaQA, and Pangu). Therefore, we omit another study on $K_{E2}$, as it varies with $K_{E1}$.

\begin{figure}[htb] 
    \begin{minipage}[b]{0.5\linewidth}  
        \centering
        \includegraphics[width=\textwidth]{figures/KR.pdf} 
        \captionsetup{font=small}
        \subcaption{$k_R$}
        %\label{fig:sub1}
    \end{minipage}%
    \hfill 
    \begin{minipage}[b]{0.5\linewidth}  
        \centering
        \small
        \includegraphics[width=\textwidth]{figures/KE3.pdf}
        \captionsetup{font=small}
        \subcaption{$k_{E3}$}
        %\label{fig:kr_ke3}
    \end{minipage}
    \caption{Impact of $k_R$ and $k_{E3}$ on the overall F1 score.}
    \label{fig:kr_ke3}
\end{figure}

\begin{table*}[h]
\small
\centering
\begin{tabular}{ll}
\hline
\textbf{Question:} What is the name for the atomic units of length? \\ \hline \addlinespace[2pt]
\textbf{SpanMD:}  What is the name for the atomic units of \textcolor{red}{length}? & (\ding{55})  \\ \hline \addlinespace[2pt]
\textbf{Ours:}\\
\textbf{\hspace{6pt}Retrieved Relations:} measurement\_unit.measurement\_system.length\_units,\\
\hspace{87.5pt}measurement\_unit.time\_unit.measurement\_system, \\
\hspace{87.5pt}measurement\_unit.measurement\_system.time\_units... \\
\textbf{Generated Logical Form Sketch:}  (AND \textless{}class\textgreater~(JOIN \textless{}relation\textgreater~{[} \textcolor{blue}{atomic units} {]}))\hspace{10pt} &(\ding{51})\\ \hline
                                                                          
\end{tabular}
\caption{Case study of entity mention detection by our model and SpanMD (a mention detection method commonly used by SOTA KBQA models) on the GrailQA validation set. The incorrect entity mention detected is colored in red, while the correct entity mention detected is colored in blue.}
\label{tab:md_case}
\end{table*}

Figure~\ref{fig:kr_ke3} further shows the impact of $k_R$ and $k_{E3}$ -- recall that  $k_R$ is the number of top candidate relations considered, and $k_{E3}$ is the number of candidate entities matched for each entity mention. 
Now we show the F1 scores, as these parameters are used by 
our schema-guided logical form generation module. They directly affect the accuracy of the generated logical form and the corresponding question answers.

On GrailQA, increasing either $k_R$ or $k_{E3}$ leads to higher F1 scores, although the growth becomes marginal eventually. On WebQSP, the F1 scores peak at $k_R=25$ and $k_{E3}=4$. These results suggest that feeding an excessive number of candidate entities and relations to the logical form generator module has limited benefit. 
To avoid the extra computational costs (due to more input tokens) and to limit the input length for compatibility with smaller Seq2Seq models (e.g., T5-base), we use $k_R=20$ and $k_{E3}=2$ on both datasets. 
%we did not adopt larger values for $K_{R}$ and $K_{E3}$. 

%[A bit strange, why not $k_R=25$ and $k_{E3}=4$?]}


\section{Model Running Time}\label{app:time}
\model\ takes 26 hours to train on the GrailQA dataset and 13.6 seconds to run inference for a test sample. It is faster on WebQSP which is a smaller dataset. Note that more than 10 hours of the training time were spent on the fallback logical form generation. If this step is skipped (which does not impact our model accuracy substantially as shown earlier), \model\ can be trained in about half a day. Another five hours were spent on fine-tuning the LLM for logical form generation, which can also be reduced by using a smaller model. 

As there is no full released code for the baseline models, it is infeasible to benchmark against them on model training time. For model  inference tests, TIARA has a partially released model (with a closed-source mention detection module). The model takes 11.4 seconds per sample (excluding the entity mention detection module) for inference on GrailQA, which is close to that of \model. Therefore, we have achieved a model that is more accurate than the baselines while being at least as fast in inference as one of the top performing baselines (i.e., TIARA+GAIN which shares the same inference procedure with TIARA).



%\jz{Add discussion on the results}

%\jz{Table~\ref{tab:run_time} reports the overall model training and inference time (per instance) of \model\ on GrailQA, as well as a detailed breakdown. Overall, \model\ takes about a day to train, while it takes 13.6 seconds to infer the answer of an input question. On WebQSP, we observe a similar time breakdown, while the overall model training time is smaller. We omit the detail results for simplicity. 

%We note that none of the top performing baselines have released 

%For benchmarking ... [Should add running time of TIARA, FC-KBQA, and RetinaQA for comparison] 
%\sxfix{no baseline completely opensouce, TIARA almost did it, but still has a mention detector (important module) not yet open-sourced. No training time reported in their paper. Some has inference time but with different experiment setting.}}


%The training time varies significantly across modules, with fallback logical form retrieval taking the longest (10 hours and 23 minutes) due to the time-consuming nature of the logical form enumeration process.  Similarly, it accounts for over 40\% of the inference time for each test question. Furthermore, since our logical form generator is an LLM, it has a large size, which also takes some time to fine-tune (5 hours and 33 minutes) and run the  inference process (4.3 seconds). The training time for relation retrieval is shorter than that of RG-EMD, while the inference time for relation retrieval is longer. This is because, during inference, the relation retrieval module is used to score each relation in the KB relation pool to obtain the top-ranked relations, which takes time, while \sxfix{RG-EMD adopts a T5-base, which has a small parameter size and fast inference speed, to generate logical form sketches.} These results reveal research opportunities to improve the time efficiency of relation retrieval and logical form enumeration.

%Despite the parameter size of our logical form generator is considerably larger, the overall training time is shorter since it is trained for only 1 epoch. 

% For inference, 

% }



%\begin{table}[H]
%\centering
%\small
%\begin{tabular}{lrr}
%\toprule
%\textbf{Modules}        & \textbf{Training} & \textbf{Inference} \\ 
%\midrule
%Relation retrieval         &  4h21m   & 3.3s                \\ 
%RG-EMD        &   4h30m     & 0.14s               \\ 
%RG-CER &    -    & 0.2s                \\
%Entity ranking         &  1h30m  & 0.003s              \\ 
%SG-LF    &  5h33m   & 4.3s                \\ 
%Fallback LF retrieval     &    10h23m       & 5.7s                     \\
%Total                   &    26h17m      & 13.6s                \\ 
%\bottomrule
%\end{tabular}
%\caption{Model training time and average inference time (per instance) on GrailQA (h: hours; m: minutes; s: seconds).}
%\label{tab:run_time}
%\end{table}
%\addexp{Add running time results}

%\jz{\subsection{Case Study} To further show \model's capability, we include a case study from the GrailQA validation set as shown in Figure~\ref{fig:case_study}. It shows that when the ground-truth KB element is not the top-1-ranked candidate from our retrieval modules, the generator can still select the correct KB elements and generate the correct logical form through the class-based context, without being overly dependent on the performance of the retrieval module.}

\section{Case Study}\label{app:case_study}


\begin{table*}[h]
\centering
\small
\begin{tabular}{m{1cm} m{6cm} m{4cm}}
\hline
\multicolumn{3}{l}{\textbf{Question:} Captain pugwash makes an appearance in which comic strip?} \\
\hline
                       & \multicolumn{1}{l}{\textbf{Relation Retrieval}}                                      & \multicolumn{1}{l}{\textbf{Entity Retrieval}}          \\
\hline
\multirow{4}{*}{\textbf{TIARA}} & \textcolor{red}{\ldots comic\_strips\_appeared\_in}                                               & Captain Pugwash \textcolor{blue}{m.04fgkzf}         \\
                       & \textcolor{blue}{\ldots character}                                                                 &                                    \\
\cline{2-3} 
                       \addlinespace[2pt]& \multicolumn{2}{l}{\begin{tabular}[c]{@{}l@{}}(AND comic\_strips.comic\_strip\_character (JOIN \\ \hspace{8pt}\textcolor{red}{comic\_strips.comic\_strip\_character.comic\_strips\_appeared\_in } \textcolor{red}{m.04fgkzf}))\end{tabular}}   (\ding{55}) \\
\hline \addlinespace[2pt] 
\multirow{7}{*}{\centering\textbf{ Ours}} 
            & {[}D{]} comic\_strip\_character      & {[}ID{]} \textcolor{red}{m.04fgkzf}                 \\
                       & {[}N{]} \textcolor{red}{comic\_strips\_appeared\_in}  & {[}N{]} Captain Pugwash            \\
                       & {[}R{]} comic\_strip                 & {[}C{]} comic\_strip               \\ \cdashline{2-3} \addlinespace[2pt]

                       & {[}ID{]} comic\_strip                 & {[}ID{]} \textcolor{blue}{m.02hcty}                  \\
                       & {[}N{]} \textcolor{blue}{character}                    & {[}N{]} Captain Pugwash            \\
                       & {[}R{]} comic\_strip\_character      & {[}C{]} comic\_strip\_character    \\
\cline{2-3} 
                        \addlinespace[2pt]& \multicolumn{2}{l}{(AND comic\_strips.comic\_strip (JOIN \textcolor{blue}{comic\_strips.comic\_strip.characters} \textcolor{blue}{m.02hcty}))}  (\ding{51})\\
\hline
\end{tabular}
\caption{Case study of logical form generation by \model\ and a representative competitor TIARA on the GrailQA validation set. Incorrect relations and entities are marked in red, while the correct relations and entities are colored in blue.}
\label{tab:lfg_case}
\end{table*}


To further show \model's generalizability to non-I.I.D. KBQA applications, we include a case study from the GrailQA validation set as shown in Tables~\ref{tab:md_case} and~\ref{tab:lfg_case}. 

\paragraph{Entity Mention Detection} 
Figure~\ref{tab:md_case} shows an entity mention detection example, comparing our entity detection module with SpanMD which is a mention detection method commonly used by SOTA KBQA models~\cite{shu_tiara_2022,ye_rng-kbqa_2022,faldu_retinaqa_2024}. In this case, SpanMD incorrectly detects \textsf{length} as an entity mention, which is actually part of the ground-truth relation (\textsf{measurement\_unit.$\ldots$.length\_units}) that is unseen in the training data. Our entity mention detection module, on the other hand, leverages the retrieved relations to generate a logical form sketch. The correct entity mention, \textsf{atomic units}, is isolated from the relations and can be corrected extracted, even though this entity mention has not been seen at training. %This example demonstrates that our entity mention detection module enhances the compositional generalization and zero-shot generalization capabilities of \model.   

%enabling it to detect different KB elements in the input question from a more comprehensive perspective. Compared to previous entity retrieval methods, it demonstrates stronger compositional generalization and zero-shot generalization capabilities.

\paragraph{Logical Form Generation}
Table~\ref{tab:lfg_case} shows a logical form generation example.
Here, \model\ and TIARA (a representative generation-based model) have both retrieved the same sets of relations in the retrieval stage which include false positives. The two models also share the same top-1 retrieved entity \textsf{m.04fgkzf}, while \model\ has retrieved a second entity \textsf{m.02hcty} in addition. 
TIARA is misled by the erroneous KB relations retrieved and produces an incorrect logical form. 
\model, on the other hand, is able to produce the correct logical form by leveraging the schema information (i.e., the entity's class and the relation's domain and range classes).



%the seq2seq model with KB context. This enables the model to understand the connections between KB elements and generate executable logical forms that align with the semantics of the question. 







\section{Error Analysis}\label{app:error_analysis}
Following TIARA~\cite{shu_tiara_2022} and Pangu~\cite{gu_dont_2023}, we analyze 200 incorrect predictions randomly sampled from each of the GrailQA
validation set and the WebQSP test set where our model predictions are different from the ground truth. The errors of \model\ largely fall into the following three types:

\begin{itemize}
    \item \textbf{Relation retrieval errors} (35\%). Failures in the relation retrieval step (e.g., failing to retrieve any ground-truth relations) can impinge the capability of our entity mention detection module to generate correct logical form sketches, which in turn leads to incorrect entity mention detection and entity retrieval.

    \item \textbf{Entity retrieval errors} (32\%). Errors in the entity mentions generated by the logical form sketch parser can still occur even when the correct relations are retrieved, because some complex and unseen entity mentions require domain-specific knowledge. An example of such entity mentions is `\textsf{Non-SI units mentioned in the SI}', which refers to units that are not part of the International System (SI) of Units but are officially recognized for use alongside SI units. This entity mention involves two concepts that are very similar in their surface forms (\textsf{Non-SI} and \textsf{SI}). Without a thorough understanding of the  domain knowledge (\textsf{SI} standing for \textsf{International System of Units}), it is difficult for the entity mention detection module to identify the correct entity boundaries. 


    \item \textbf{Logical form generation errors} (31\%). Generation of inaccurate or inexecutable logical forms can still occur when the correct entities and relations are retrieved. The main source of such errors is questions involving operators rarely seen in the training data (e.g., \textsf{ARGMIN} and \textsf{ARGMAX}). Additionally, there are highly ambiguous candidate entities that may confuse the model, leading to incorrect selections of entity-relation combinations. For example, for the question \textsf{Who writes twilight zone}, two candidate entities \textsf{m.04x4gj} and \textsf{m.0d\_rw} share the same entity name \textsf{twilight zone}. The former refers to a reboot of the TV series \textsf{The Twilight Zone} produced by Rod Serling and Michael Cassutt, while the latter is the original version of \textsf{The Twilight Zone} independently produced by Rod Serling. They share the same entity name and class (\textsf{tv.tv\_program}). There is insufficient contextual information for our logical form generator to  differentiate between the two. The generator eventually selected the higher-ranked entity which was incorrect, leading to producing an incorrect answer to the question \textsf{Rod Serling and Michael Cassutt}.
    
    % \jz{and the classes to which these entities belong overlap. [can you give a couple of these entities and what they are referring to?] These are difficult to disambiguate.} 
    \item The remaining errors (2\%) stem from incorrect annotations of comparative questions in the dataset. For example, \textsf{larger than} in a question is annotated as \textsf{LE} (less equal) in the ground-truth logical form.

\end{itemize}

% \jz{The remaining errors (2\%) ... [can we say something about these errors? Just 4 anyway?]}



%\begin{itemize}
% \item \textbf{Relation retrieval errors} (37\%). Failures in the relation retrieval step (e.g., failing to retrieve any ground-truth relations) can impinge the capability of our entity mention detection module to generate correct logical form sketches, which in turn leads to incorrect entity mention detection and entity retrieval. 

% \item \textbf{Entity retrieval errors} (32\%). Errors in the entity mentions generated by the logical form sketch parser can still occur even when the correct relations are retrieved, \jz{because...?}. 

% \item \textbf{Logical form generation errors} (31\%). Such  errors mainly arise from questions with complex semantics. \jz{example?} The limited number of complex questions in the training data makes it difficult for the LLM to learn and generate logical forms for such questions. 

%in the model making syntactic errors (such as in operators and functions) when generating logical forms for such complex questions. 
%\end{itemize}




\end{document}
