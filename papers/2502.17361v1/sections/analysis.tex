\section{Heterogeneity Mechanisms Analysis}\label{sec:heterogeneous}
As described in~\autoref{sec:relimiary}, \ours handles data heterogeneity by embedding a $d$-dimensional input vector into a $k$-dimensional space with size $d \times k$. This enables \ours to adapt to diverse downstream datasets with varying feature dimensions seamlessly. 
In this section, we analyze this mechanism and revisit it as a variant of the token-based deep tabular method. Further analyses of the key components in \ours are provided in~\autoref{sec:appendix_ablation}.

\subsection{Analysis of the Heterogeneity Handling Strategy}
Traditional deep tabular models often struggle with dataset heterogeneity due to their reliance on fixed feature semantics. For example, token-based approaches like AutoInt \cite{SongS0DX0T19AutoInt} and FT-Transformer (FT-T)~\cite{GorishniyRKB21Revisiting} learn dataset- and attribute-specific feature tokens during training, which become ineffective when applied to new datasets with different feature meanings. These methods typically generate $d$ attribute-specific tokens $[\vr_1, \ldots, \vr_d] \in \sR^{k \times d}$, one for each dimension, and transform an instance $\vx_i$ into $[{x_i^1} \cdot \vr_1, \ldots, {x_i^d} \cdot \vr_d] \in \sR^{k \times d}$, where $x_i^j$ is the $j$-th element of $\vx_i$.
However, when transitioning to a new dataset with $d'$ features, these learned tokens cannot be reused, requiring feature tokens to be learned anew. Previous methods address this by relying on semantic descriptions of attributes~\cite{Yan2024Making} or assuming overlap between feature sets to construct a mapping~\cite{Onishi2023TabRet, Zhou2023TabToken}.

In contrast, \ours eliminates this dependency through randomized feature tokens. Specifically, \ours applies a shared $k$-dimensional vector $\vu\in\R^k$ to lift raw attribute values into a $k$-dimensional space. To differentiate attributes, \ours adds a random $k$-dimensional perturbation for each attribute, consistent within a dataset but distinct across datasets. Formally, the randomized tokens are denoted as:
\begin{equation} \mR = \mW \mP \in \sR^{k \times d},
\end{equation}
where $\mP \in \sR^{k' \times d}$ is randomly generated, and $\mW \in \sR^{k \times k'}$ is a learnable pre-condition projection matrix that maps the random values to the required dimensionality ($k'<k$).The $j$-th column of $\mR$, $\vr_j \in \sR^k$, serves as the specific token for the $j$-th attribute and is combined with the shared vector $\vu$. Then, an instance $\vx_i$ is transformed through
\begin{equation} [{x_i^1} \cdot (\vu + \vr_1), \ldots, {x_i^d} \cdot (\vu + \vr_d), \tilde{\vy}_i] \in \sR^{k \times (d+1)} \;.\label{eq:tokenizer} \end{equation}
This randomized tokenization ensures that distinct attributes—irrespective of their semantic meanings—are mapped to unique, nearly orthogonal directions in the high-dimensional space. This is akin to one-hot encoding but operates in a continuous over-parameterized regime, where random vectors naturally approximate orthogonality.

Through its transformer layers, \ours dynamically combines these basis vectors during inference, weighting their contributions based on the input context. This process is analogous to inverse PCA: \emph{while PCA decomposes data into orthogonal components, \ours reconstructs heterogeneous features by mixing randomized bases scaled by their attribute values.} This approach avoids the need for pre-defined feature semantics, while the shared vector $\vu$ introduces a stable inductive bias that supports cross-dataset generalization. Pre-training on diverse and abundant synthetic datasets further equips \ours with the capability to adapt these bases across diverse tasks, unifying disparate tabular datasets into a coherent latent framework.

This randomized token strategy effectively removes the need for dataset- and attribute-specific feature token learning, addressing a significant limitation of prior methods and improving transferability to diverse downstream datasets.

\begin{figure}[t]
    \centering
            \vspace{-1mm}
    \includegraphics[width=\linewidth]{files/random_token/rank.pdf}
        \vspace{-8mm}
    \caption{The change of ranks between FT-T, MNCA, and the variants with randomized tokens over 45 tabular datasets. (The rank is calculated together with the 26 compared baselines.)
    Values in the parentheses denote the average rank.
    }
    \vspace{-1mm}
    \label{fig:random_token}
\end{figure}

\subsection{Effectiveness of Randomized Feature Tokens}
To further evaluate the effectiveness of randomized tokens, we integrate this strategy with two representative methods: FT-Transformer (FT-T) by~\citet{GorishniyRKB21Revisiting} and ModernNCA (MNCA) by~\citet{Ye2024ModernNCA}.

\noindent{\bf FT-T with Randomized Tokens (FT-T$^*$).}
FT-T is a token-based method that learns specific tokens $\{\vr_1, \ldots, \vr_d\} \in \sR^{d \times k}$ for each attribute. After transforming an instance into a set of tokens, a transformer processes these tokens together with a learned class token. The output token corresponding to the class token is then fed into a linear classifier. 

We replace FT-T's learned tokens with the randomized tokens used in \ours, retrain FT-T (denoted as FT-T$^*$), and evaluate the resulting performance.

\begin{figure*}[t]
  \centering
  \vspace{-2mm}
  \begin{minipage}{0.16\linewidth}
    \includegraphics[width=\textwidth]{files/embedding1/churn_raw_pca.pdf}
    \centering
    \end{minipage}
    \begin{minipage}{0.16\linewidth}
    \includegraphics[width=\textwidth]{files/embedding1/churn_original_PFN_layer11_pca.pdf}
    \centering
    \end{minipage}
    \begin{minipage}{0.16\linewidth}
    \includegraphics[width=\textwidth]{files/embedding1/churn_PFN_layer2_pca.pdf}
    \centering
    \end{minipage}
    \begin{minipage}{0.16\linewidth}
    \includegraphics[width=\textwidth]{files/embedding1/churn_PFN_layer5_pca.pdf}
    \centering
    \end{minipage}
    \begin{minipage}{0.16\linewidth}
    \includegraphics[width=\textwidth]{files/embedding1/churn_PFN_layer8_pca.pdf}
    \centering
    \end{minipage}
    \begin{minipage}{0.16\linewidth}
    \includegraphics[width=\textwidth]{files/embedding1/churn_PFN_layer11_pca.pdf}
    \centering
    \end{minipage}

    \begin{minipage}{0.16\linewidth}
    \includegraphics[width=\textwidth]{files/embedding1/bank_raw_pca.pdf}
    \centering
    \end{minipage}
    \begin{minipage}{0.16\linewidth}
    \includegraphics[width=\textwidth]{files/embedding1/bank_original_PFN_layer11_pca.pdf}
    \centering
    \end{minipage}
    \begin{minipage}{0.16\linewidth}
    \includegraphics[width=\textwidth]{files/embedding1/bank_PFN_layer2_pca.pdf}
    \centering
    \end{minipage}
    \begin{minipage}{0.16\linewidth}
    \includegraphics[width=\textwidth]{files/embedding1/bank_PFN_layer5_pca.pdf}
    \centering
    \end{minipage}
    \begin{minipage}{0.16\linewidth}
    \includegraphics[width=\textwidth]{files/embedding1/bank_PFN_layer8_pca.pdf}
    \centering
    \end{minipage}
    \begin{minipage}{0.16\linewidth}
    \includegraphics[width=\textwidth]{files/embedding1/bank_PFN_layer11_pca.pdf}
    \centering
    \end{minipage}

    \begin{minipage}{0.16\linewidth}
    \includegraphics[width=\textwidth]{files/embedding1/KDD_raw_pca.pdf}
    \centering
    \end{minipage}
    \begin{minipage}{0.16\linewidth}
    \includegraphics[width=\textwidth]{files/embedding1/KDD_original_PFN_layer11_pca.pdf}
    \centering
    \end{minipage}
    \begin{minipage}{0.16\linewidth}
    \includegraphics[width=\textwidth]{files/embedding1/KDD_PFN_layer2_pca.pdf}
    \centering
    \end{minipage}
    \begin{minipage}{0.16\linewidth}
    \includegraphics[width=\textwidth]{files/embedding1/KDD_PFN_layer5_pca.pdf}
    \centering
    \end{minipage}
    \begin{minipage}{0.16\linewidth}
    \includegraphics[width=\textwidth]{files/embedding1/KDD_PFN_layer8_pca.pdf}
    \centering
    \end{minipage}
    \begin{minipage}{0.16\linewidth}
    \includegraphics[width=\textwidth]{files/embedding1/KDD_PFN_layer11_pca.pdf}
    \centering
    \end{minipage}
    
    \begin{minipage}{0.16\linewidth}
    \includegraphics[width=\textwidth]{files/embedding1/website_phishing_raw_pca.pdf}
    \centering
    {\small \mbox{(a) {Raw Feature}}}
    \end{minipage}
    \begin{minipage}{0.16\linewidth}
    \includegraphics[width=\textwidth]{files/embedding1/website_phishing_original_PFN_layer11_pca.pdf}
    \centering
    {\small \mbox{(b) {Vanilla Extraction}}}
    \end{minipage}
\begin{minipage}{0.16\linewidth}
    \includegraphics[width=\textwidth]{files/embedding1/website_phishing_PFN_layer2_pca.pdf}
    \centering
    {\small \mbox{(c) {Layer 3}}}
    \end{minipage}
    \begin{minipage}{0.16\linewidth}
    \includegraphics[width=\textwidth]{files/embedding1/website_phishing_PFN_layer5_pca.pdf}
    \centering
    {\small \mbox{(d) {Layer 6}}}
    \end{minipage}
       \begin{minipage}{0.16\linewidth}
    \includegraphics[width=\textwidth]{files/embedding1/website_phishing_PFN_layer8_pca.pdf}
    \centering
    {\small \mbox{(e) {Layer 9}}}
    \end{minipage}
    \begin{minipage}{0.16\linewidth}
    \includegraphics[width=\textwidth]{files/embedding1/website_phishing_PFN_layer11_pca.pdf}
    \centering
    {\small \mbox{(f) {Layer 12}}}
    \end{minipage}
    \vspace{-1mm}
  \caption{
Visualization of extracted embeddings for four datasets: {\it churn} (first row, two classes), {\it bank} (second row, two classes), {\it KDD} (third row, two classes), and {\it website\_phishing} (fourth row, three classes).
We use crosses to denote training examples and circles to denote test examples.
(a) shows the raw features, while (b) presents the embeddings extracted using the vanilla strategy.
(c)-(f) depict the embeddings obtained using our proposed methods across different layers.
The accuracy value is calculated by training a linear model (logistic regression) over the extracted embeddings on the training set and predicting on the test set.
  }
      \vspace{-2mm}
  \label{fig:our_extraction}
\end{figure*}

\noindent{\bf MNCA with Randomized Tokens (MNCA$^*$).} 
ModernNCA is a neighborhood-based method that predicts an instance based on its similarity to training instances in a latent space. While ModernNCA typically operates on raw features, we adapt it into a token-based method by applying ModernNCA to the token output from FT-T before the final linear layer, incorporating the randomized tokens as above. 

We evaluate these FT-T and MNCA variants over the tiny-bechmark2 from~\citet{Ye2024Closer}, which contains 45 representative tabular datasets. The results in~\autoref{fig:random_token} demonstrate how the performance rank changes when replacing the specific tokens with the randomized tokens. 

\noindent{\bf Remark.} Our experiments confirm that feature tokens do not significantly rely on semantic meanings. Using randomized tokens only slightly increases the average ranks as long as they are distinct across attributes. This highlights the robustness and universality of the randomized token strategy, demonstrating its effectiveness in handling heterogeneous datasets and its potential to improve generalization across diverse tasks once pre-trained.