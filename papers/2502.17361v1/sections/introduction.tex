\section{Introduction}
Tabular data is ubiquitous across diverse applications, including healthcare~\cite{hyland2020early}, finance~\cite{kovalerchuk2005data}, and scientific research~\cite{ivanciuc2007applications,hyland2020early}. In this format, each instance is represented as a vector of attributes, and the task of a machine learning model is to map these vector inputs to their corresponding labels~\cite{Borisov2024Deep}. Traditionally, tree-based models~\cite{Prokhorenkova2018Catboost,chen2016xgboost} have dominated the tabular domain, but deep tabular models are increasingly showing potential to advance the field and close the performance gap~\cite{GorishniyRKB21Revisiting,David2024RealMLP,Ye2024ModernNCA}.

However, unlike vision and language data, where pre-trained foundation models have driven significant progress~\cite{Kirillov2023Segment,zhou2024comprehensive}, tabular data is still desperately awaiting a similar breakthrough \cite{Somepalli2021SAINT,Hollmann2022TabPFN,Onishi2023TabRet,zhu2023xtab,Ye2023TabPTM,BreugelS24PositionTabular}. \emph{A primary challenge arises from the inherent heterogeneity of tabular datasets, which often vary in dimensionality and attribute meanings, making the development of effective and versatile foundation models difficult.}  Additionally, there is an urgent need for such models, as many tabular datasets are small-scale---such as medical data with limited patient numbers. Training individual models from scratch for these datasets is highly sensitive to hyperparameters and struggles with insufficient data for generalization~\cite{FeurerKESBH15,guyon2019analysis,Han2024FeatLLM}.

Recently, the Tabular Prior-Fitted Network v2 (\ours), proposed by \citet{hollmann2025TabPFNv2}, has emerged as a significant step forward. Built on transformer architectures \cite{vaswani2017attention} and pre-trained on gigantic synthetic datasets \cite{Hollmann2022TabPFN,hollmann2025TabPFNv2}, \ours can be directly applied to downstream tasks without the need for additional tuning, effectively addressing the heterogeneity of tabular data. Specifically, \ours takes both a labeled training set and an unlabeled test instance as input, predicting the test label in an ``in-context learning'' manner. When evaluated on multiple datasets covering both classification and regression tasks, \ours achieved unprecedented accuracy compared to previous methods.

Motivated by the remarkable performance of \ours, this paper pursues four timely objectives to deepen our understanding and advance the field.
First, we aim to expand the evaluation of \ours across a diverse set of scenarios to investigate its generalizability. Second, we seek to understand the underlying mechanisms by which \ours effectively handles data heterogeneity.
Third, we strive to uncover how \ours achieves its high prediction accuracy. Last but not least, we aim to extend \ours's applicability beyond the comfort zone defined in \citet{hollmann2025TabPFNv2}â€”datasets with no more than 10,000 samples, 500 dimensions, and 10 classes.

\noindent\textbf{\emph{\ours consistently outperforms existing methods on small- to medium-scale datasets.}} We evaluate \ours extensively on over 300 tabular datasets spanning various domains, scales, dimensionalities, and tasks~\cite{Grinsztajn2022Why,McElfreshKVCRGW23when,Ye2024Closer,Rubachev2024TabRed}. Our results demonstrate \ours's exceptional generalizability on small- to medium-scale datasets, outperforming both tree-based models and state-of-the-art deep tabular models. However, \ours's performance diminishes on large-scale and high-dimensional datasets, indicating areas where further improvements are needed.

\noindent\textbf{\emph{\ours effectively handles data heterogeneity through randomized feature tokens.}} A key strength of \ours is its ability to manage heterogeneous feature spaces, enabling applications to diverse downstream datasets without additional tuning.
Given a $d$-dimensional input vector $\in \R^d$, \ours transforms it into a $d \times k$ matrix and leverages a transformer architecture to handle variability in $d$, following \cite{SongS0DX0T19AutoInt,GorishniyRKB21Revisiting,Yan2024Making}.
The critical innovation lies in the vector-to-matrix transformation: \ours uses a ``shared'' $k$-dimensional vector to lift each attribute value into a $k$-dimensional space. \emph{To differentiate attributes, a random $k$-dimensional perturbation is added to the shared vector for each attribute}, consistent within a dataset but distinct across datasets\footnote{This detail was identified through the supplementary material and code of~\cite{hollmann2025TabPFNv2}.}.
We view this transformation as a variant of token-based tabular methods, where \ours removes the need for dataset- and attribute-specific feature token learning, a limitation that has hindered the transferability of previous methods to diverse downstream datasets.
Our analysis confirms that these feature tokens require no semantic meaning as long as they are distinct across attributes, demonstrating their efficacy.


\noindent\textbf{\emph{\ours simplifies feature distributions among categories.}} To deepen our understanding of \ours, we investigate whether its superior in-context learning capability is accompanied by the creation of more separable feature representations. This analysis is challenging due to the distinct roles of labeled training data and unlabeled test data in \ours's in-context learning process, resulting in non-comparable embeddings. To overcome this, we propose a leave-one-fold-out strategy to extract features from the training data that align more closely with the test data. Our findings reveal that \ours effectively transforms tabular datasets into a nearly separable embedding space. Remarkably, training a linear model on \ours's extracted features achieves accuracy comparable to its in-context learner, underscoring \ours's potential as a feature encoder. This approach not only offers valuable insights into \ours but also paves the way for broader applications and further analysis of its capabilities.

\noindent\textbf{\emph{Divide-and-conquer extends \ours's applicability.}} As noted in~\citet{hollmann2025TabPFNv2} and confirmed by our study, \ours encounters challenges with large-scale, high-dimensional, and many-category datasets. To address these limitations without extensive retraining, we introduce \emph{post-hoc} divide-and-conquer strategies, reminiscent of Chain-of-Thought (CoT) prompting used in large-language models \cite{Wei2022CoT}. For large-scale datasets, we split the training data into support and query sets, using the support set to extract \ours's features for the query and test sets. The resulting features then form a new tabular dataset compatible with linear models, akin to representation learning strategies in the early days~\cite{VincentLLBM10Stacked-auto-encoder}. For high-dimensional datasets, we subsample attributes into subsets~\cite{Breiman01RandomForest}, apply \ours to each subset in parallel, and ensemble the predictions, similar to random forests. For a multi-class task with over 10 categories, we introduce decimal encoding inspired by Error Correcting Output Codes (ECOC)~\cite{Dietterich1995ECOC}, decomposing it into multiple 10-class problems. 
Empirical results validate these strategies, demonstrating significant gains in \ours's accuracy across these data regimes and highlighting the potential of advanced post-hoc methods to further expand the capabilities of tabular foundation models.

\textbf{Contributions.}
This paper provides a timely and in-depth investigation into \ours's strengths, limitations, and potential extensions, aiming to offer valuable insights to advance tabular foundation models. In particular:
\begin{itemize}[nosep, topsep=2pt, parsep=2pt, partopsep=2pt, leftmargin=*]
\item We confirm \ours's remarkable in-context learning capability across diverse small- to medium-scale tasks, positioning it as a new go-to method for these data regimes.
\item We highlight randomized feature tokens as the key driver behind \ours's handling of heterogeneous datasets and show their applicability to other deep tabular models.
\item We introduce a simple leave-one-fold-out approach to turn \ours into a feature encoder, enhancing its understanding and suggesting broader applications (e.g., for visualization and error diagnosis).
\item We demonstrate the effectiveness of post-hoc mechanisms to extend \ours beyond its designated data regimes, similar to prompting strategies like CoT for LLMs.
\end{itemize}

