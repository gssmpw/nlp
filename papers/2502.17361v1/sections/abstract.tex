\begin{abstract} 
Tabular datasets are inherently heterogeneous, posing significant challenges for developing pre-trained foundation models. The recently introduced transformer-based Tabular Prior-data Fitted Network v2 (\ours) achieves unprecedented \emph{in-context learning} accuracy across multiple tabular datasets, marking a pivotal advancement in tabular foundation models. In this paper, we comprehensively evaluate \ours on over 300 datasets, confirming its exceptional generalization capabilities on small- to medium-scale tasks. Our analysis identifies \textbf{randomized feature tokens} as a key factor behind \ours's success, as they unify heterogeneous datasets into a fixed-dimensional representation, enabling more effective training and inference. To further understand \ours's predictions, we propose a \textbf{leave-one-fold-out approach}, transforming \ours into a feature extractor and revealing its capability to simplify data distributions and boost accuracy. Lastly, to address \ours's limitations in high-dimensional, large-scale, and many-category tasks, we introduce a \textbf{divide-and-conquer mechanism} inspired by Chain-of-Thought prompting, enabling scalable inference. By uncovering the mechanisms behind \ours's success and introducing strategies to expand its applicability, this study provides key insights into the future of tabular foundation models. 
\end{abstract}