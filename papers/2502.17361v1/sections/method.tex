\section{Method}
Given the inherent heterogeneity in attribute and label spaces across various tabular datasets, the core idea behind \ours is to standardize these diverse datasets, enabling the application of a joint deep neural network.
We begin by drawing inspiration from neighborhood-based methods and introduce the concept of meta-representation, which serves as the foundation for {\sc TabPTM}'s pre-training strategy. 
\ours applies to both classification and regression tasks, which is illustrated in~\autoref{fig:meta_representation}. 

% Considering the inherent heterogeneity in attribute and label spaces across various tabular datasets, the core idea of TabPTM is to standardize diverse datasets so that a joint deep neural network can be applied.
% We transform the heterogeneous tabular tasks into homogeneous local prediction tasks, based on an instance's relationship w.r.t. the top-$K$ nearest neighbors within the training set. 
% Then the instance becomes a set of $K$-dimensional vectors irrespective of its original dimension.
% Based on this meta-representation, a multi-layer perceptron is trained across multiple datasets, which helps recognize the local patterns in the neighborhood context and subsequently extract instance-specific prediction scores.
% The learned general model can be directly applied to any downstream dataset or fine-tuned without additional learnable components.
% \ours is illustrated in~\autoref{fig:meta_representation}. 

\subsection{Motivation from Neighborhood Embedding}\label{sec:motivation}
Accurate estimation of the posterior $\Pr(y_i \mid \vx_i, \gD)$ is crucial for all classification and regression tasks~\citep{murphy2022probabilistic}. Utilizing the balloon kernel density estimator~\citep{terrell1992variable} with a uniform class prior, we can transform this posterior density into a neighborhood estimation problem, which calculates the probability via the relationship between $\vx_i$ and its $K$ nearest neighbors in $\gD$: 
\begin{equation}
    \Pr(y_i=c \mid \vx_i, \gD) = \frac{1}{K} \sum_{j\in \gN_K(\vx_i, \gD)} \mathbb{I}[y_j = c]\;.\label{eq:kde_estimator}
\end{equation}
$\mathbb{I}[\cdot]$ is the indicator function, and $\gN(\vx_i; \gD)$ denotes the set of $K$ nearest neighbors of $\vx_i$ w.r.t. a specified distance $\operatorname{dist}(\vx_i, \vx_j)$.
Instead of considering all instances in $\gN(\vx_i; \gD)$ equally, a variant of the estimator re-weights the influence of neighbors, allocating larger weight to closer neighbors and potentially enhancing prediction accuracy~\citep{bishop2006pattern,RasmussenW06}:
\begin{equation}
    \Pr(y_i \mid \vx_i, \gD) = \left(\operatorname{softmax}\left(\left[-\operatorname{dist}(\vx_i, \vx_1),\ldots,-\operatorname{dist}(\vx_i, \vx_{K})\right]\right)\right)^\top Y_K\;.
    \label{eq:kde_vector}
\end{equation}
$Y_K\in\{0,1\}^{K\times C}$ is the label matrix for $\mathcal{N}_K(\vx_i;\mathcal{D})$, with each row corresponding to the one-hot label of $\vx_i$. This formulation also extends to regression, and we will discuss details in~\autoref{sec:kNN_view}.

\autoref{eq:kde_vector} indicates that an instance's relationship to a particular label $c$ can be inferred from its relationship to the nearest neighbors associated with that label. 
This provides a {\em general} way to deal with heterogeneous datasets, as the relationships among instances serve as a shareable vocabulary across datasets, independent of dimensionality or semantic meaning. Thus, we can use these relationships to construct a meta-representation that encodes {\em transferable} knowledge.

\subsection{Meta-Representation of an Instance}\label{sec:meta_Rep}
\noindent{\bf Vanilla meta-representation.}
We describe how to obtain the vanilla meta-representation for any instance in a tabular {\em classification} dataset $\gD$ with $C$ classes, and later for the regression case. 
Based on the label of each instance, we partition the same-class instances in $\gD$ into $C$ sets: $\gD_{y=c} = \{(\vx_i, y_i)\;|\;y_i=c\}\;, \forall c=1,.\ldots, C$. 
Given a class $c$, we calculate the distance between an instance $\vx_i$ to instances in $\gD_{y=c}$ (with $|\gD_{y=c}|$ instances in total), and sort them in an {\em ascending} order: 
\begin{align}
    &\{\operatorname{dist}(\vx_i, \vx_1),\ldots,\operatorname{dist}(\vx_i, \vx_j),\ldots,\operatorname{dist}(\vx_i, \vx_{|\gD_{y=c}|})\}\notag\\
    {\rm s.t.}\;
    &\operatorname{dist}(\vx_i, \vx_1)\le\ldots\le\operatorname{dist}(\vx_i, \vx_{|\gD_{y=c}|})\;.\label{eq:calculate_distance}
\end{align}
% Here $\operatorname{dist}(\cdot,\cdot)$ measures the instance-wise distance, \eg, Euclidean distance and Manhattan distance. 
We then select the $K$ smallest distance values in the set, which constructs the local context with $K$ nearest neighbors for the instance $\vx_i$. We define a mapping $\phi_c$ from $\vx_i$ to its meta-representation $\phi_c(\vx_i)\in\sR^{2K}$ for the $c$-th class by:
\begin{equation}
    \phi_c(\vx_i) = [(\operatorname{dist}(\vx_i, \vx_1), \hat{y}_1),\ldots (\operatorname{dist}(\vx_i, \vx_{K}), \hat{y}_K)]\;.\label{eq:meta_representation}
\end{equation}
$\phi_c(\vx_i)$ captures the neighborhood distribution of an instance, containing both the distance between an instance to the neighbors and the corresponding labels of the neighbors. 
$\hat{y}_j$ in~\autoref{eq:meta_representation} provides auxiliary label information for $\vx_i$, for example, the semantic vector of the label. In our implementation, we set $\hat{y}_j$ in a one-vs.-rest manner based on its true label $y_j$, which means $\hat{y}_j=1$ if $y_j = c$, or $\hat{y}_j=-1$ otherwise. This sparse implementation indicates the relationship between $\vx_j$ and class $c$. In our experiments, we will demonstrate if we use richer supervision from a stronger tabular model's prediction, the quality of the meta-representation as well as the discriminative ability of \ours can be further improved. 

\begin{figure*}
    \centering
\includegraphics[width=0.9\textwidth]{files/method.pdf}
    \vspace{-3mm}
    \caption{An illustration of the Meta-Representation (MR) for classification. MR transforms heterogeneous tabular datasets with different dimensions into a homogeneous form. 
    A dataset has a set of $K$-dimension MRs, one for each class. 
    The prediction scores for different classes could be obtained via MRs. We pre-train a joint model on MRs of different datasets and extend its generalization ability to downstream datasets.
    The right figure shows the MR of an instance, containing distances from an instance to the nearest neighbors of a certain class, characterizes the class membership patterns.}
    \label{fig:meta_representation}
    \vspace{-5mm}
\end{figure*}

$\phi_c(\vx_i)$ reveals the membership of $\vx_i$ to a particular class based on its neighborhood context. 
If an instance resides within a high-density region of a class (akin to being near the class center), the majority of values in $\phi_c(\vx_i)$ would typically be small, indicating proximity to neighboring instances of that class.
Conversely, if only a few values in $\phi_c(\vx_i)$ are small, while most are large, it indicates that the instance $\vx_i$ unlikely belongs to class $c$.
%likely located at the boundary among classes.
In sum, the heterogeneous tabular classification tasks become homogeneous prediction tasks over the local context when we take advantage of $\phi_c(\vx_i)$. No matter what value the original dimension $d$ of $\vx_i$ is, $\phi_c(\vx_i)$ has a fixed dimension with value $K$, standardizing the vectors and facilitating pre-training over heterogeneous tabular datasets. 
We set $\Phi(\vx_i)=\{\phi_c(\vx_i)\}_{c=1}^C$ as the meta-representation for the instance $\vx_i$ given the dataset.

In the {\em regression} case, labels in $\gD$ are scalars. We obtain the $K$ nearest neighbors from the whole dataset, and use the similar form as~\autoref{eq:meta_representation}, \ie, 
\begin{equation}
    \Phi(\vx_i) = \big[(\operatorname{dist}(\vx_i, \vx_1), y_1),\ldots,(\operatorname{dist}(\vx_i, \vx_{K}), y_K)\big]\;.\label{eq:meta_representation_reg}
\end{equation}
This $K$-dimensional meta-representation in the regression scenario depicts the relationship between $\vx_i$ to the neighbors. 

\noindent{\bf Metric-based meta-representation.} The distance measure $\operatorname{dist}(\cdot,\cdot)$ in~\autoref{eq:calculate_distance} plays an important role when making decisions via the relationship between a given instance and others~\citep{ScholkopfHS01, ScholkopfS02}. Yet, in the presence of high-dimensional features (large $d$), relying on all attributes becomes computationally challenging. Moreover, the distance might be unduly influenced by redundant or noisy attributes. To address this, we implement a distance metric over raw attributes, which ensures that our final meta-representation accurately captures both the properties of individual instances and the dataset.

The main challenge lies in designing an adaptive metric compatible with heterogeneous tasks. In this paper, we first formulate the distance measure in the following form (more distances are investigated in~\autoref{appendix_sec:exp_setup}):
\begin{equation}
    \operatorname{dist}(\vx_i, \vx_j) = \left(\sum_{l=1}^d w_l \cdot |\vx_{il} - \vx_{jl}|^p\right)^{\frac{1}{p}}\;,\label{eq:weighted_dist}
\end{equation}
where $\vx_{il}$ denote the $l$-th dimension of $\vx_{i}$.
We set $p\in\{1,2\}$ and $w_l > 0$ is a weight for each dimension. When $w_l = 1$, the distance in~\autoref{eq:weighted_dist} degenerates to Euclidean distance ($p=2$) or Manhattan distance ($p=1$). Given the training set $\gD=\{\mX, \mY\}$ of a dataset where $\mX$ and $\mY$ denote the instance matrix and label vector, respectively, we derive feature weights from the mutual information shared between individual attributes and their labels
\begin{equation}
    w_l = \operatorname{normalize}\left(\operatorname{MI}(\mX_{:l},\; \mY)\right)\;.
\end{equation}
$\mX_{:l}$ is the $l$-th column of $\mX$, \ie, the vector containing values of the $l$-th attribute of all instances. $\operatorname{MI}(\cdot, \cdot)$ calculates the mutual information between two sets, which measures the dependency between an attribute and the labels~\citep{Brown2012Conditional}. The larger the mutual information, the more important an attribute is, so that we increase its weight in~\autoref{eq:weighted_dist}.
The $\operatorname{normalize}(\cdot)$ normalizes input values by dividing their cumulative sum. The experiments validate that integrating this distance metric in meta-representation significantly enhances the model's generalization ability.

\noindent{\bf Meta-representation in the few-shot scenario.}
The previously discussed meta-representation assumes there are at least $K$ neighbors in the set $\gD_{y=c}$. However, in some applications like few-shot classification or the existence of minority class, $\gD_{y=c}$ might only contain a limited number of neighbors smaller than $K$.
To address the data scarcity challenge, we pad the meta-representation with its last value (the largest distance)~\citep{Yang2012Multilabel}. 
% We also investigate augmenting the instances to size $K$ by swapping their attributes~\citep{Ucar2021Subtab}. 
% In detail, the $l$-th element in $\vx_i$ is replaced by another value in $\mX_{:l}$ {\em with the same label} for a given probability. This supervised swap strategy mimics the distribution of the training data in a particular class. 
% In experiments, both strategies help in few-shot scenarios, while the former one performs better, so we set the padding strategy as our default choice.

\begin{figure}[t] 
    \centering
    \begin{minipage}{0.315\linewidth}
    \includegraphics[width=\textwidth]{files/BCW-TSNE.pdf}
    \centering
    {\small \mbox{(a) {breast-cancer-wisc (binary)}}}
    \end{minipage}
    \begin{minipage}{0.315\linewidth}
    \includegraphics[width=\textwidth]{files/DER-TSNE.pdf}
    \centering
    {\small \mbox{(b) {dermatology (multi-class)}}}
    \end{minipage}
    \begin{minipage}{0.32\linewidth}
    \includegraphics[width=\textwidth]{files/meta_feature_pilotstudy.pdf}
    \centering
    {\small \mbox{(c) {Joint Space with MR.}}}
    \end{minipage}
    \vspace{-2mm}
    \caption{Pilot study of Meta-Representation (MR) over two datasets ``breast-cancer-wisc'' (binary, denoted by ``+'') and ``dermatology'' (multi-class, denoted by ``$\circ$''). 
We use colors to distinguish between different classes. (a) and (b) display the unique characteristics of each dataset. In (c), we homogenize the datasets using MR, with red to indicate the MR for the target class ($\phi_{y_i}(\vx_i)$) and blue for non-target classes ($\phi_{c \neq y_i}(\vx_i)$, those not matching the true label). MR effectively integrates both datasets into a joint space where their classifications can be implemented by the dotted line.
    }
    \vspace{-4mm}
    \label{fig:pilot_study}
\end{figure}
\subsection{A Pilot Study on Meta-Representation}
Based on the definition of meta-representation, we use classification as an example to show its discriminative ability and demonstrate it is shareable across different datasets.

We consider two datasets, \ie., ``breast-cancer-wisc'' (binary) and ``dermatology'' (multi-class). For each dataset, we calculate the meta-representation $\Phi(\vx_i)$ for all classes in their datasets. We partition the meta-representation for the target class ($\phi_{y_i}(\vx_i)$), and the ones for non-target classes ($\phi_{c\neq y_i}(\vx_i)$) into two sets. We set $K=8$. As shown in~\autoref{fig:pilot_study}, we use red/blue to denote the previous two types of meta-representation, respectively, and use different shapes to differentiate datasets.

We have several observations. First, the meta-representation w.r.t target and non-target classes are separated (as shown by the dotted line), which indicates it is possible to determine the target class of an instance by discerning the target meta-representation. Then, the meta-representations for different datasets are mixed, which means meta-representation is able to be a general strategy for extracting shareable knowledge across datasets. Moreover, different datasets have diverse patterns w.r.t. their meta-representations. For example, the target-class meta-representations denoted by ``+'' are clustered while those denoted by ``$\circ$'' are not. 
Thus, joint training on multiple datasets to predict via meta-representations is necessary.

\subsection{Making Predictions via Meta-Representation}
\noindent{\bf Classification}. Given a dataset $\gD$, we represent an instance $\vx_i$ with $\Phi(\vx_i)=\{\phi_c(\vx_i)\}_{c=1}^C$. Based on the meta-representation, we need to obtain the prediction score for each class in a classification task.
Define the score for each class as 
\begin{equation}
[s(\vx_i)_1,\ldots,s(\vx_i)_C] = {\bf T}_{\Theta}\left([\phi_1(\vx_i), \ldots, \phi_C(\vx_i)]\right).\label{eq:transformation}
\end{equation}
${\bf T}_{\Theta}$ is a transformation that captures the class membership patterns from the meta-representation for each class and then outputs the corresponding class-wise classification scores based on the local neighborhood context. ${\Theta}$ denotes the learnable parameters in ${\bf T}$. In \ours, we implement ${\bf T}$ with Multi-Layer Perceptron (MLP), \ie, 
\begin{equation}
    s(\vx_i)_c = {\bf MLP}(\phi_c(\vx_i)),\;\forall c=1,\ldots,C\;. \label{eq:mlp_mapping}
\end{equation}
The parameters of MLP are shared for all classes, whose detailed architecture is described in~\autoref{appendix_sec_method}.
When multiple types of distances are used, we concatenate them together at first and then use \autoref{eq:mlp_mapping} to map the concatenated meta-representation vectors to a scalar. 

Based on the scores, the predicted class for $\vx_i$ is 
\begin{equation}
    \hat{y}_i = \argmax_c\; \left\{s(\vx_i)_1,\ldots,s(\vx_i)_C\right\}\;.\label{eq:prediction}
\end{equation}
The classification strategy based on meta-representation fits heterogeneous tasks with different attributes and class spaces.
The meta-representation-based classification enables the usage of a joint model over heterogeneous tasks.

\noindent{\bf Regression}. Meta-representation could be applied to the tabular regression tasks in a similar manner. Since the label is a continuous value, we map the meta-representation to a scalar
\begin{equation}
    s(\vx_i) = {\bf T}_{\Theta}\left(\Phi(\vx_i)\right)\;.\label{eq:transformation_reg}
\end{equation}
${\bf T}_{\Theta}$ captures the continuous values in the neighborhood and sets the prediction for $\vx_i$ as a weighted combination of its neighbor's labels. 
We also implement ${\bf T}$ with MLP with learnable parameter ${\Theta}$.
% Considering that the label ranges of different regression tasks may vary significantly, we apply standard normalization on the label values for each dataset directly before calculating the meta-representation, and the predicted $s(\vx_i)$ is de-normalized to the original range with the saved dataset-specific statistics. 

\subsection{Pre-training with Meta-Representation}
Based on the previous discussions, we pre-train a joint model, \ie, the transformation ${\bf T}_{\Theta}$, over $\sD$, whose parameters are shared across multiple seen tabular datasets.
\begin{equation}
\label{eq:objective}
\min_{\Theta}\;\sum_{t=1}^T\;\sum_{i=1}^{N_t}\ell\left({\bf T}_{\Theta}(\Phi(\vx_i^t)),\; y^t_i\right)\;.
\end{equation}
The transformation ${\bf T}_{\Theta}$, pre-trained across $T$ datasets, links the meta-representation to the final score. 
The loss $\ell(\cdot,\cdot)$ measures the discrepancy between a prediction and the label, and we set it as cross-entropy for classification and mean square error for regression.
Given a downstream dataset $\gD_u$, we first obtain the meta-representation for each instance. Then the learned ${\bf T}_{\Theta}$ could be applied directly without further training, or acts as a good initialization for fine-tuning without additional parameters. 
\autoref{appendix_sec_method} provides more details of \ours, including the pre-training and deployment workflows of \ours in Algorithm~\autoref{alg:pretrain} and Algorithm~\autoref{alg:downstream}, respectively. 

In summary, we treat the relationships among instances as a form of shareable vocabulary. Meta-representation leverages these relationships between an instance and its nearest neighbors, transforming the tabular prediction task into inferring an instance's label based on its relationship to neighbors of that label. \ours not only standardizes heterogeneous datasets but also facilitates the learning of transferable knowledge.
