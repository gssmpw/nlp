\section{Related Work}\label{sec:related_work}
\noindent{\bf Foundation Tabular Models}.
Pre-trained models have revolutionized vision and language domains~\citep{Kirillov2023Segment, zhou2024comprehensive}, but their adoption in tabular data remains limited due to the inherent heterogeneity of tabular datasets. Differences in feature spaces, dimensionalities, and class distributions pose significant challenges for joint training and transferability. % and scalability. 
One solution is to utilize semantic meanings of features, as demonstrated by methods that transform tabular examples into textual representations for large language models (LLMs)~\citep{Hegselmann2022TabLLM, Zhang2023Generative, Wang2023AnyPredict, WenZZXB24From}. Pre-computing feature tokens based on semantic embeddings has also been explored to improve transferability~\citep{Yan2024Making, Kim2024CARTE}.

However, semantic meanings are often unavailable due to privacy concerns, annotation costs, or lack of describability. To address this, \citet{Ye2023TabPTM} propose representing examples through similarity relationships with a fixed number of reference examples, mapping them into a latent space with a fixed dimension. The TabPFN family of models~\citep{Hollmann2022TabPFN, hollmann2025TabPFNv2} leverages the in-context learning capabilities of transformers, directly predicting labels by positioning test instances within the context of training examples. While TabPFN v1 pads features to a fixed dimension, TabPFN v2 introduces a specialized feature tokenizer to better handle heterogeneity. Meta-learning has also been explored to obtain the weights of tabular models for downstream tasks with limited examples~\citep{Iwata2020Meta, BonetMGI2024HyperFast}. Other pre-trained models require lightweight fine-tuning on downstream datasets to adapt to heterogeneous feature and class spaces~\citep{Liu2022Distribution, Zhang2023Meta, Shen2023Cross, zhu2023xtab}.

\noindent{\bf Variants of TabPFN}.
TabPFN’s success stems from its pre-training on gigantic synthetic datasets, enabling strong in-context learning performance on small-scale classification tasks~\cite{Hollmann2022TabPFN}. Inspired by its capabilities, various applications have been investigated, including tabular data generation~\cite{Ma2024TabPFGen}, anomaly detection~\cite{RuizVillafrancaGGMM24detection}, and time series forecasting~\cite{Shi2024TabPFNSeries}. The efficiency of TabPFN is highly sensitive to context size, prompting strategies to enhance scalability and performance~\cite{Feuer2023ScalePFN, Xu2024MixPFN}. Some approaches enhance TabPFN’s performance on downstream tasks by adapting the context~\cite{Thomas2024LocalPFN}, fine-tuning specific parts of the model~\cite{Feuer2024TuneTables,liu2025tabpfnunleashedscalableeffective}, or even pre-training the architecture on real-world datasets~\citep{Ma2024TabDPT}. 
\citet{Nagler2023Statistical} analyzes TabPFN from a bias-variance perspective, shedding light on its generalization capabilities. The recently introduced \ours extends support to regression tasks and handles larger contexts~\cite{hollmann2025TabPFNv2}. In this paper, we conduct a comprehensive evaluation of \ours, analyze its strengths and limitations, and propose solutions to overcome scalability and applicability challenges.