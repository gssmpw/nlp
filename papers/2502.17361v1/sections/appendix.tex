\section{Additional Related Work}
\noindent{\bf Learning with Tabular Data}.
Tabular data is prevalent across diverse fields, including healthcare, finance, and education~\citep{kovalerchuk2005data, hyland2020early, romero2010educational, amatriain2010data}. Tree-based models, such as XGBoost~\citep{chen2016xgboost}, LightGBM~\citep{ke2017lightgbm}, and CatBoost~\citep{Prokhorenkova2018Catboost}, have long dominated this domain. However, recent advances in deep neural networks (DNNs) have demonstrated strong potential for tabular data~\cite{Borisov2024Deep}. Popular architectures like multi-layer perceptrons~\citep{GorishniyRKB21Revisiting, Kadra2021Well} and Transformers~\citep{Huang2020TabTransformer} have been adapted to tabular tasks, alongside custom architectures designed specifically for tabular data~\citep{KlambauerUMH17Self, WangSCJLHC21DCNv2}.

Deep tabular methods can be broadly categorized into two types. The first type directly processes raw features~\cite{David2024RealMLP, gorishniy2023tabr, Ye2024ModernNCA}, sometimes incorporating feature-specific encoding strategies~\cite{Gorishniy2022On}. The second type tokenizes features, transforming an example into a set of tokens~\citep{SongS0DX0T19AutoInt, Huang2020TabTransformer, Rubachev2022revisiting}. Comprehensive benchmarks have been developed to evaluate these methods across diverse datasets~\cite{Grinsztajn2022Why, McElfreshKVCRGW23when, Ye2024Closer, Rubachev2024TabRed}, highlighting the strengths and weaknesses of deep tabular models in various scenarios.

\section{Evaluation Details}
Please refer~\citet{Ye2024Closer} for details of the 300 small to medium datasets. For high-dimensional datasets, we selected 18 datasets with more than 2000 features from the~\href{https://jundongl.github.io/scikit-feature/datasets}{scikit-feature repository}. Detailed statistics of high-dimensional datasets and large-scale datasets are reported in~\autoref{tab:high_dataset_info} and~\autoref{tab:large_dataset_info}. 


We follow~\citet{Ye2024Closer} and use different colors to represent various categories of methods in the result figures, ensuring clarity and easy comparison. In~\autoref{fig:critical_difference} and~\autoref{fig:pama}, we compare the following methods:

\begin{itemize}
    \item \textbf{Classical Methods} (\raisebox{-0.3mm}{\tikz{\draw[white,fill=CoralOrange] (0,0) rectangle (0.3,0.3);}}): The classical methods include Dummy, Logistic Regression (LR), K-Nearest Neighbors (KNN), Support Vector Machines (SVM), Naive Bayes, Linear Regression, and DNNR~\citep{NaderSL22DNNR}, which serve as basic baselines for classification and regression tasks.
    
    \item \textbf{Tree-based Methods} (\raisebox{-0.3mm}{\tikz{\draw[white,fill=VibrantGreen] (0,0) rectangle (0.3,0.3);}}): Tree-based methods such as Random Forest~\citep{Breiman01RandomForest}, XGBoost~\citep{chen2016xgboost}, LightGBM~\citep{ke2017lightgbm}, and CatBoost~\citep{Prokhorenkova2018Catboost} are known for their high performance on tabular data.
    
    \item \textbf{MLP variants} (\raisebox{-0.3mm}{\tikz{\draw[white,fill=RichRed] (0,0) rectangle (0.3,0.3);}}): MLP variants, including vanilla MLP, MLP-PLR, Self-Normalizing Neural Networks~\citep{KlambauerUMH17Self}, and Residual Network~\citep{GorishniyRKB21Revisiting}, enhance the flexibility and generalization of traditional MLP architectures through advanced regularization and residual connections.
    
    \item \textbf{Special Architectures} (\raisebox{-0.3mm}{\tikz{\draw[white,fill=SoftIndigo] (0,0) rectangle (0.3,0.3);}}): Methods with specially designed architectures, such as DCNv2~\citep{WangSCJLHC21DCNv2}, DANets~\citep{ChenLWCW22DAN}, and TabCaps~\citep{Chen2023TabCaps}, focus on improving feature interaction and abstraction to capture complex relationships in tabular data.
    
    \item \textbf{Token-based Methods} (\raisebox{-0.3mm}{\tikz{\draw[white,fill=BrightCyan] (0,0) rectangle (0.3,0.3);}}): Token-based methods like AutoInt~\citep{SongS0DX0T19AutoInt}, TabTransformer~\citep{Huang2020TabTransformer}, FT-Transformer~\citep{GorishniyRKB21Revisiting}, and ExcelFormer~\citep{Chen2023Excel} represent features as tokens, enabling models to capture higher-order interactions through attention mechanisms.
    
    \item \textbf{Regularization-based Methods} (\raisebox{-0.3mm}{\tikz{\draw[white,fill=FreshLime] (0,0) rectangle (0.3,0.3);}}): Regularization-based methods, including TANGOS~\citep{jeffares2023tangos}, SwitchTab~\citep{Wu2024SwitchTab}, and PTaRL~\citep{PTARL}, aim to improve model generalization by incorporating regularization techniques during training to enhance the robustness of predictions.
    
    \item \textbf{Tree-mimic Methods} (\raisebox{-0.3mm}{\tikz{\draw[white,fill=EmeraldTeal] (0,0) rectangle (0.3,0.3);}}): Tree-mimic methods, such as NODE~\citep{PopovMB20Neural}, GrowNet~\citep{Badirli2020GrowNet}, and TabNet~\citep{ArikP21TabNet}, combine the interpretability of decision trees with the power of deep learning, employing attention mechanisms to select important features.
    
    \item \textbf{Context-based Methods} (\raisebox{-0.3mm}{\tikz{\draw[white,fill=VividPurple] (0,0) rectangle (0.3,0.3);}}): Context-based methods like TabR~\citep{gorishniy2023tabr} and ModernNCA~\citep{Ye2024ModernNCA} leverage contextual information from the training data to improve predictions by utilizing neighborhood-based and in-context learning strategies.
\end{itemize}
In addition to the aforementioned methods, for other experimental results, we will demonstrate the performance of \textbf{\ours and its variants}, which are represented by emerald teal (\raisebox{-0.3mm}{\tikz{\draw[white,fill=EmeraldTeal!70] (0,0) rectangle (0.3,0.3);}}), ensuring that their experimental effects are clearly distinguished from the other methods.

\begin{table}[ht]
\centering
\caption{Dataset Information for High-Dimensional Data Experiments: A collection of 18 datasets with varying numbers of instances, features, and classes used in our high-dimensional experiments.}
\begin{tabular}{lccc|lccc}
\toprule
\textbf{Dataset} & \textbf{\#Instances} & \textbf{\#Features} & \textbf{\#Classes} &\textbf{Dataset} & \textbf{\#Instances} & \textbf{\#Features} & \textbf{\#Classes} \\
\midrule
BASEHOCK&	1993&	4862&	2  &lung&	203&	3312&	5  \\
PCMAC	&1943	&3289	&2  & warpPIE10P&	210&	2420&	10\\
RELATHE	&1427	&4322	&2  &orlraws10P &	100&	10304&	10 \\
ALLAML	& 72	&7129	&2  & Prostate\_GE	&102&	5966&	2 \\
CLL\_SUB\_111&	111&	11340&	3 & SMK\_CAN\_187&	187&	19993&	2 \\
colon&	62&	2000&	2  & warpAR10P	&130	&2400	&10 \\
GLI\_85&	85	&22283	&2  & arcene&	200&	10000&	2\\
GLIOMA&	50	&4434&	4  & gisette	&7000&	5000&	2 \\
leukemia&	72&	7070&	2  & TOX\_171&	171&	5748&	4 \\
\bottomrule
\end{tabular}
\label{tab:high_dataset_info}
\end{table}

\begin{table}[ht]
\centering
\caption{Dataset Information for Large-scale Data Experiments.}
\resizebox{17cm}{!}{
\begin{tabular}{lccc|lccc}
\toprule
\textbf{Dataset} & \textbf{\#Instances} & \textbf{\#Features} & \textbf{\#Classes} &\textbf{Dataset} & \textbf{\#Instances} & \textbf{\#Features} & \textbf{\#Classes} \\
\midrule
BNG(credit-a)&	1,000,000&15&		2  &CDC\_Indicators
& 253,680&	21&		2  \\
Higgs	&1,000,000 &28		&2  & Smoking\_signal&991,346 &	23&		2\\
nomao	&34,465 &118		&2  &sf-police-incidents &	2,215,023& 8&		2 \\
Data\_Crowdfunding	&671,025 & 11		&4  & Fashion-MNIST	& 70,000& 784&		10 \\
covertype &	581,012& 54&		7 & jannis&	83,733& 54&		4 \\
poker-hand &	1,025,009 &10&	10  & volkert	&58,310&180		&10 \\
Airlines\_DepDelay&10,000,000&	9		& -  & Wave\_Energy\_Farm&	36,043 &	99&	-\\
UJIndoorLoc &21,048 &	520	&	- & blogfeedback	&60,021&276&		- \\
microsoft&	1,200,192&136&		-  & yahoo&	709,877&699&		- \\
\bottomrule
\end{tabular}
}
\label{tab:large_dataset_info}
\end{table}


\section{Detailed Results}
We list the detailed results of \ours and our extensions on various benchmarks.

\begin{itemize}
    \item We present the \textbf{main results of TabPFN v2} on 300 datasets in \autoref{tab:main_results}. The table includes accuracy for classification tasks and RMSE (Root Mean Squared Error) for regression tasks, along with the corresponding mean and standard deviation for each dataset. Notably, we excluded 27 datasets from these results in \autoref{tab:main_results}, as they were used by \ours to select the best checkpoint. These excluded datasets, which are not shown in~\autoref{fig:critical_difference} and~\autoref{fig:pama}, include:  
    \begin{itemize}
        \item[(1)] ada\_prior, allbp, baseball, delta\_ailerons, eye\_movements, eye\_movements\_bin, GAMETES\_Epistasis\_2-Way\_20atts\_0.1H\_EDM-1\_1, hill-valley, JapaneseVowels, jungle\_chess\_2pcs\_raw\_endgame\_complete, led24, longitudinal-survey, page-blocks, ringnorm, rl, thyroid-ann, waveform-5000,  
        \item[(2)] debutanizer, delta\_elevators, mauna-loa-atmospheric, puma32H, stock\_fardamento02, treasury, weather\_izmir, wind.
    \end{itemize}

    \item In \autoref{tab:high_dimension_results}, we showcase the \textbf{performance of various models on 18 high-dimensional datasets}. The results display the mean accuracy of different models, including ModernNCA (MNCA), MLP, KNN, RealMLP, XGBoost (XGB), Random Forest (RForest), Logistic Regression (LogReg), and TabPFN v2 (PFN-v2), along with variants like TabPFN v2-pca and TabPFN v2*. This highlights the ability of these models to handle high-dimensional data with many features.

    \item We demonstrate the \textbf{performance of various models on 12 multi-class classification tasks} with more than 10 classes in \autoref{tab:res_multi_class_results}. The table provides the mean accuracy of models like KNN, TabPFN-v2*, XGBoost (XGB), CatBoost (CatB), Random Forest (RForest), ModernNCA (MNCA), MLP, Logistic Regression (LogReg), and RealMLP, showcasing how they perform on multi-class tasks with a larger number of classes. Additionally, we compare \textbf{PFN-v2-ECOC}, a multi-class classification solution provided by~\citet{hollmann2025TabPFNv2}. This method extends TabPFN-v2 by leveraging Error-Correcting Output Codes (ECOC) to enhance multi-class classification performance\footnote{\href{https://github.com/PriorLabs/tabpfn-community/blob/main/src/tabpfn_extensions/many_class/many_class_classifier.py}{https://github.com/PriorLabs/tabpfn-community/blob/main/src/tabpfn\_extensions/many\_class/many\_class\_classifier.py}}. 

    \item In \autoref{tab:large_scale}, we compare the \textbf{performance of various models on 18 large-scale datasets}. The results show the mean accuracy or RMSE for MLP, Logistic/Linear Regression (LR), KNN, XGBoost (XGB), Random Forest (RForest), CatBoost (CatB), ModernNCA (MNCA), RealMLP, and different versions of TabPFN v2 (PFNv2, PFNv2 with K-means, PFNv2 with Bagging, and PFNv2*). This illustrates the models' performance on large-scale datasets.

    \item We present a comparison of \textbf{FT-T~\cite{GorishniyRKB21Revisiting} and MNCA~\cite{Ye2024ModernNCA} with and without randomized feature tokens} on tiny benchmark datasets from \citet{Ye2024Closer} in \autoref{tab:random_token_results}. The first two columns represent FT-T$^\star$ and MNCA$^\star$ with randomized feature tokens, while the last two show the original FT-T and MNCA methods. The table demonstrates the effect of randomized feature tokens on model performance, with accuracy for classification tasks and RMSE for regression tasks. FT-T and MNCA implementations, including their hyperparameter search space, follow the approach described in \citet{Liu2024Talent}, and the results were obtained after 100 rounds of hyperparameter tuning.

    \item In \autoref{embeddings_results}, we show the \textbf{performance of \ours and the extracted feature embeddings} across 29 classification datasets. The table includes average classification accuracy for each dataset when using feature embeddings from different transformer layers (Layer 6, Layer 9, Layer 12), as well as a combined approach where embeddings from multiple layers are concatenated. The ``selected layers'' column indicates the layers chosen based on validation set performance, offering insights into how different layers contribute to overall model performance. In addition to evaluating the performance of \ours and the extracted feature embeddings, we also compared the results with embeddings obtained using the vanilla strategy (Vanilla). 
\end{itemize}

\begin{longtable}{lc|lc} 
\caption{Main results of TabPFN v2 on 300 datasets, including accuracy (for classification tasks) and RMSE (for regression tasks), along with the corresponding mean and standard deviation for each dataset. Among the 300 datasets, 200 are classification datasets, and 100 are regression datasets. The results demonstrate the effectiveness of TabPFN v2 across both classification and regression tasks.}
    \label{tab:main_results}\\
\toprule
Dataset & Mean + Std & Dataset & Mean + Std \\ 
\midrule
ASP-POTASSCO-classification & 43.50 $\pm$ 1.27  &Amazon\_employee\_access & 94.22 $\pm$ 0.04 \\ 
BLE\_RSSI\_localization & 73.37 $\pm$ 0.15 & BNG(breast-w) & 98.56 $\pm$ 0.07 \\ 
BNG(cmc) & 57.69 $\pm$ 0.17 & BNG(tic-tac-toe) & 79.42 $\pm$ 0.26 \\ 
Bank\_Customer\_Churn\_Dataset & 87.53 $\pm$ 0.12 & Basketball\_c & 70.65 $\pm$ 0.47 \\ 
California-Housing-Classification & 91.47 $\pm$ 0.17 & Cardiovascular-Disease-dataset & 72.92 $\pm$ 0.13 \\ 
Click\_prediction\_small & 83.29 $\pm$ 0.03 & Contaminant-10.0GHz & 94.42 $\pm$ 0.36 \\ 
Contaminant-10.5GHz & 95.17 $\pm$ 0.32 & Contaminant-11.0GHz & 93.93 $\pm$ 0.50 \\ 
Contaminant-9.0GHz & 93.01 $\pm$ 0.47 & Contaminant-9.5GHz & 93.21 $\pm$ 0.50 \\ 
Credit\_c & 69.98 $\pm$ 0.15 & Customer\_Personality\_Analysis & 90.03 $\pm$ 0.21 \\ 
Diabetic\_Retinopathy\_Debrecen & 72.81 $\pm$ 1.07 & E-CommereShippingData & 67.54 $\pm$ 0.21 \\ 
Employee & 84.80 $\pm$ 0.30 & FICO-HELOC-cleaned & 75.35 $\pm$ 0.21 \\ 
FOREX\_audcad-day-High & 74.51 $\pm$ 0.51 & FOREX\_audcad-hour-High & 71.01 $\pm$ 0.20 \\ 
FOREX\_audchf-day-High & 76.66 $\pm$ 0.45 & FOREX\_audjpy-day-High & 78.00 $\pm$ 0.28 \\ 
FOREX\_audjpy-hour-High & 71.41 $\pm$ 0.32 & FOREX\_audsgd-hour-High & 69.81 $\pm$ 0.39 \\ 
FOREX\_audusd-hour-High & 69.57 $\pm$ 0.48 & FOREX\_cadjpy-day-High & 71.68 $\pm$ 0.53 \\ 
FOREX\_cadjpy-hour-High & 70.55 $\pm$ 0.40 & Firm-Teacher-Direction & 84.42 $\pm$ 0.47 \\ 
Fitness\_Club\_c & 79.67 $\pm$ 0.24 & GAMETES\_Epistasis & 68.75 $\pm$ 0.82 \\ 
GAMETES\_Heterogeneity & 65.90 $\pm$ 1.84 & Gender\_Gap\_in\_Spanish\_WP & 60.58 $\pm$ 0.24 \\ 
GesturePhaseSegmentationProcessed & 71.36 $\pm$ 1.15 & HR\_Analytics & 80.02 $\pm$ 0.13 \\ 
Heart-Disease-Dataset & 91.23 $\pm$ 0.54 & INNHotelsGroup & 87.98 $\pm$ 0.23 \\ 
Indian\_pines & 96.41 $\pm$ 0.23 & Insurance & 75.75 $\pm$ 0.00 \\ 
Intersectional-Bias-Assessment & 94.73 $\pm$ 0.13 & Is-this-a-good-customer & 88.41 $\pm$ 0.00 \\ 
JapaneseVowels & 99.68 $\pm$ 0.08 & KDD & 80.14 $\pm$ 0.46 \\ 
KDDCup09\_upselling & 81.06 $\pm$ 0.26 & Long & 99.88 $\pm$ 0.00 \\ 
MIC & 90.20 $\pm$ 0.56 & MagicTelescope & 88.13 $\pm$ 0.21 \\ 
Marketing\_Campaign & 88.11 $\pm$ 0.41 & Mobile\_Price\_Classification & 97.10 $\pm$ 0.29 \\ 
Nutrition\_Health\_Survey & 83.45 $\pm$ 0.22 & Performance-Prediction & 73.23 $\pm$ 0.61 \\ 
PhishingWebsites & 96.74 $\pm$ 0.13 & PieChart3 & 87.31 $\pm$ 0.28 \\ 
Pima\_Indians\_Diabetes\_Database & 75.93 $\pm$ 0.66 & PizzaCutter3 & 88.20 $\pm$ 0.45 \\ 
Pumpkin\_Seeds & 87.93 $\pm$ 0.21 & QSAR\_biodegradation & 88.50 $\pm$ 0.50 \\ 
Rain\_in\_Australia & 83.88 $\pm$ 0.11 & SDSS17 & 97.33 $\pm$ 0.06 \\ 
Shipping & 68.73 $\pm$ 0.40 & Telecom\_Churn\_Dataset & 95.18 $\pm$ 0.50 \\ 
UJI\_Pen\_Characters & 45.71 $\pm$ 2.16 & VulNoneVul & 98.95 $\pm$ 0.00\\
Water\_Quality\_and\_Potability & 65.49 $\pm$ 0.50 & Waterstress & 71.37 $\pm$ 0.96 \\ 
Wilt & 99.28 $\pm$ 0.06 & abalone & 63.58 $\pm$ 0.38 \\ 
accelerometer & 73.96 $\pm$ 1.32 & ada & 85.40 $\pm$ 0.25 \\ 
ada\_agnostic & 83.99 $\pm$ 0.34 & ada\_prior & 85.32 $\pm$ 0.19 \\ 
adult & 85.93 $\pm$ 0.12 & airlines\_2000 & 62.28 $\pm$ 0.48 \\ 
allbp & 97.85 $\pm$ 0.19 & allrep & 98.65 $\pm$ 0.12 \\ 
analcatdata\_authorship & 99.72 $\pm$ 0.29 & artificial-characters & 73.90 $\pm$ 0.99 \\ 
autoUniv-au4-2500 & 69.81 $\pm$ 1.00 & autoUniv-au7-1100 & 41.18 $\pm$ 1.58 \\ 
bank & 90.86 $\pm$ 0.19 & banknote\_authentication & 55.64 $\pm$ 0.18 \\ 
baseball & 93.81 $\pm$ 0.40 & car-evaluation & 98.29 $\pm$ 0.22 \\ 
churn & 96.33 $\pm$ 0.28 & cmc & 59.59 $\pm$ 0.49 \\ 
company\_bankruptcy\_prediction & 97.33 $\pm$ 0.07 & compass & 71.05 $\pm$ 0.29 \\ 
connect-4 & 76.78 $\pm$ 0.35 & contraceptive\_method\_choice & 62.10 $\pm$ 0.37 \\ 
credit & 78.10 $\pm$ 0.11 & credit-g & 79.50 $\pm$ 0.81 \\ 
customer\_satisfaction\_in\_airline & 94.79 $\pm$ 0.11 & dabetes\_130-us\_hospitals & 63.08 $\pm$ 0.07 \\ 
default\_of\_credit\_card\_clients & 82.63 $\pm$ 0.08 & delta\_ailerons & 95.47 $\pm$ 0.09 \\ 
dis & 99.07 $\pm$ 0.14 & dna & 97.25 $\pm$ 0.20 \\ 
drug\_consumption & 40.32 $\pm$ 0.00 & dry\_bean\_dataset & 92.76 $\pm$ 0.10 \\ 
eeg-eye-state & 98.34 $\pm$ 0.12 & electricity & 86.57 $\pm$ 0.45 \\ 
estimation\_of\_obesity\_levels & 98.66 $\pm$ 0.24 & eucalyptus & 72.88 $\pm$ 1.17 \\ 
eye\_movements & 77.03 $\pm$ 1.68 & eye\_movements\_bin & 67.28 $\pm$ 2.60 \\ 
first-order-theorem-proving & 61.12 $\pm$ 0.70 & gas-drift & 99.47 $\pm$ 0.04 \\ 
golf\_play\_dataset\_extended & 92.60 $\pm$ 0.44 & helena & 33.32 $\pm$ 0.21 \\
heloc & 72.75 $\pm$ 0.20 & hill-valley & 98.33 $\pm$ 0.52 \\ 
house\_16H & 88.55 $\pm$ 0.18 & htru & 97.95 $\pm$ 0.06 \\ 
ibm-employee-performance & 100.0 $\pm$ 0.00 & in\_vehicle\_coupon& 73.20 $\pm$ 0.35 \\ 
internet\_firewall & 92.85 $\pm$ 0.30 & internet\_usage & 54.34 $\pm$ 2.64 \\ 
jasmine & 81.34 $\pm$ 0.42 & jm1 & 81.32 $\pm$ 0.10 \\ 
jungle\_chess\_2pcs\_raw\_endgame & 85.97 $\pm$ 1.82 & kc1 & 86.65 $\pm$ 0.34 \\ 
kdd\_ipums\_la\_97-small & 88.50 $\pm$ 0.12 & kr-vs-k & 78.46 $\pm$ 1.01 \\
kr-vs-kp & 99.64 $\pm$ 0.15 & kropt & 77.96 $\pm$ 0.63  \\
law-school-admission-bianry & 100.0 $\pm$ 0.00 & led24 & 73.29 $\pm$ 0.62 \\
led7 & 73.99 $\pm$ 0.31 & letter & 97.57 $\pm$ 0.10 \\
madeline & 90.72 $\pm$ 0.48 & mammography & 98.71 $\pm$ 0.05 \\ 
maternal\_health\_risk & 83.28 $\pm$ 0.64 & mfeat-factors & 96.98 $\pm$ 0.28 \\ 
mfeat-fourier & 89.85 $\pm$ 0.86 & mfeat-karhunen & 96.42 $\pm$ 0.24 \\ 
mfeat-morphological & 76.63 $\pm$ 0.50 & mfeat-pixel & 96.10 $\pm$ 0.32 \\ 
mfeat-zernike & 84.10 $\pm$ 0.87 & mice\_protein\_expression & 100.0 $\pm$ 0.00 \\ 
microaggregation2 & 62.80 $\pm$ 0.14 & mobile\_c36\_oversampling & 98.11 $\pm$ 0.08 \\ 
mozilla4 & 93.58 $\pm$ 0.16 & naticusdroid+android+permissions & 96.41 $\pm$ 0.10 \\ 
national-longitudinal-survey-binary & 100.0 $\pm$ 0.00 & okcupid\_stem & 74.47 $\pm$ 0.12 \\ 
one-hundred-plants-margin & 88.56 $\pm$ 0.74 & one-hundred-plants-shape & 79.52 $\pm$ 0.72 \\ 
one-hundred-plants-texture & 90.94 $\pm$ 0.75 & online\_shoppers & 90.65 $\pm$ 0.10 \\
optdigits & 98.59 $\pm$ 0.12 & ozone-level-8hr & 94.92 $\pm$ 0.25 \\ 
ozone\_level & 97.86 $\pm$ 0.11 & page-blocks & 97.67 $\pm$ 0.10 \\ 
pc1 & 93.51 $\pm$ 0.46 & pc3 & 88.78 $\pm$ 0.27 \\ 
pc4 & 90.87 $\pm$ 0.36 & pendigits & 99.56 $\pm$ 0.06 \\ 
philippine & 84.20 $\pm$ 1.24 & phoneme & 88.47 $\pm$ 0.35 \\ 
pol & 98.80 $\pm$ 0.09 & predict\_students\_dropout & 78.11 $\pm$ 0.38 \\ 
rice\_cammeo\_and\_osmancik & 92.74 $\pm$ 0.23 & ringnorm & 98.00 $\pm$ 0.13 \\ 
rl & 86.04 $\pm$ 0.44 & satimage & 92.30 $\pm$ 0.29 \\ 
segment & 93.91 $\pm$ 0.19 & seismic+bumps & 93.40 $\pm$ 0.08 \\ 
semeion & 92.41 $\pm$ 0.94 & shill-bidding & 90.31 $\pm$ 0.18 \\ 
shrutime & 86.97 $\pm$ 0.11 & shuttle & 99.86 $\pm$ 0.04 \\ 
spambase & 94.85 $\pm$ 0.19 & splice & 96.61 $\pm$ 0.22 \\ 
sports\_articles & 84.93 $\pm$ 0.40 & statlog & 72.13 $\pm$ 0.97 \\ 
steel\_plates\_faults & 84.68 $\pm$ 0.55 & svmguide3 & 85.54 $\pm$ 0.54 \\ 
sylvine & 97.30 $\pm$ 0.27 & taiwanese\_bankruptcy\_prediction & 97.20 $\pm$ 0.07 \\ 
telco-customer-churn & 80.29 $\pm$ 0.28 & texture & 100.0 $\pm$ 0.00 \\ W
thyroid & 99.48 $\pm$ 0.06 & thyroid-ann & 99.34 $\pm$ 0.08 \\ 
thyroid-dis & 68.75 $\pm$ 0.34 & turiye\_student\_evaluation & 51.74 $\pm$ 0.18 \\ 
twonorm & 97.94 $\pm$ 0.08 & vehicle & 84.31 $\pm$ 1.29 \\ 
walking-activity & 61.22 $\pm$ 0.22 & wall-robot-navigation & 99.44 $\pm$ 0.10 \\ 
water\_quality & 90.12 $\pm$ 0.12 & waveform-5000 & 86.29 $\pm$ 0.26 \\ 
waveform\_v1 & 86.59 $\pm$ 0.25 & website\_phishing & 90.48 $\pm$ 0.48 \\ 
wine & 75.12 $\pm$ 0.72 & wine-quality-red & 58.35 $\pm$ 0.76 \\ 
wine-quality-white & 64.15 $\pm$ 0.69 & yeast & 60.18 $\pm$ 0.65 \\
\midrule
1000-Cameras-Dataset & 607.71 $\pm$ 6.61 & 2dplanes & 1.01 $\pm$ 0.00 \\ 
RSSI\_Estimation & 0.00068 $\pm$ 0.00 & RSSI\_Estimation1 & 0.00092 $\pm$ 0.00 \\ 
Abalone\_reg & 2.08 $\pm$ 0.00 & Ailerons & 0.00015 $\pm$ 0.00 \\ 
Fiat & 716.20 $\pm$ 4.05 & BNG(echoMonths) & 11.41 $\pm$ 0.03 \\ 
BNG(lowbwt) & 455.27 $\pm$ 0.78 & BNG(mv) & 4.63 $\pm$ 0.01 \\ 
BNG(stock) & 2.95 $\pm$ 0.02 & Bias\_correction\_r & 0.60 $\pm$ 0.01 \\ 
Bias\_correction\_r\_2 & 0.52 $\pm$ 0.01 & Brazilian\_houses\_reproduced & 0.01 $\pm$ 0.00 \\ 
CPMP-2015-regression & 478.02 $\pm$ 5.40 & CPS1988 & 364.02 $\pm$ 0.24 \\ 
CookbookReviews & 1.52 $\pm$ 0.02 & Data\_Science\_Salaries & 60237.28 $\pm$ 102.97 \\ 
Diamonds & 533.30 $\pm$ 6.30 & Facebook\_Comment\_Volume & 23.16 $\pm$ 0.20 \\ 
Food\_Delivery\_Time & 7.55 $\pm$ 0.03 & Goodreads-Computer-Books & 0.43 $\pm$ 0.00 \\ 
IEEE80211aa-GATS & 0.02 $\pm$ 0.00 & Job\_Profitability & 13.14 $\pm$ 0.02 \\ 
bike\_sharing\_demand & 68.41 $\pm$ 0.60 & Laptop\_Prices\_Dataset & 439.87 $\pm$ 3.10 \\ 
Wave\_Energy\_Perth\_100 & 15507.90 $\pm$ 104.31 & Wave\_Energy\_Sydney\_100 & 14737.67 $\pm$ 150.43 \\ 
Wave\_Energy\_Sydney\_49 & 4567.97 $\pm$ 64.02 & MIP-2016-regression & 20966.10 $\pm$ 454.90 \\ 
MiamiHousing2016 & 83101.09 $\pm$ 507.30 & Mobile\_Phone\_Market & 714.87 $\pm$ 11.15 \\ 
Moneyball & 19.42 $\pm$ 0.08 & NASA\_PHM2008 & 40.24 $\pm$ 0.06 \\ 
NHANES\_age\_prediction & 15.47 $\pm$ 0.04 & OnlineNewsPopularity & 8606.54 $\pm$ 7.04 \\ 
Parkinson\_Sound\_Record & 14.58 $\pm$ 0.09 & Parkinsons\_Telemonitoring & 0.60 $\pm$ 0.04 \\ 
Physicochemical\_r & 3.45 $\pm$ 0.04 & SAT11-HAND-runtime & 1232.03 $\pm$ 58.01 \\ 
Shop\_Customer\_Data & 28.56 $\pm$ 0.01 & Superconductivty & 10.17 $\pm$ 0.07 \\ 
Wine\_Quality\_red & 0.65 $\pm$ 0.00 & Wine\_Quality\_white & 0.68 $\pm$ 0.00 \\ 
airfoil\_self\_noise & 1.16 $\pm$ 0.02 & analcatdata\_supreme & 0.09 $\pm$ 0.00 \\ 
archive2 & 342.64 $\pm$ 3.20 & archive\_r56\_Portuguese & 2.86 $\pm$ 0.02 \\ 
auction\_verification & 1145.54 $\pm$ 146.94 & avocado\_sales & 0.09 $\pm$ 0.00 \\ 
bank32nh & 0.08 $\pm$ 0.00 & bank8FM & 0.03 $\pm$ 0.00 \\ 
boston & 4.25 $\pm$ 0.19 & chscase\_foot & 0.95 $\pm$ 0.00 \\ 
colleges & 0.14 $\pm$ 0.00 & combined\_cycle\_power\_plant & 3.22 $\pm$ 0.05 \\ 
communities\_and\_crime & 0.13 $\pm$ 0.00 & concrete\_compressive\_strength & 4.63 $\pm$ 0.07 \\ 
cpu\_act & 2.65 $\pm$ 0.03 & cpu\_small & 3.06 $\pm$ 0.02 \\ 
dataset\_sales & 4.04 $\pm$ 0.02 & debutanizer & 0.04 $\pm$ 0.00 \\ 
delta\_elevators & 0.0014 $\pm$ 0.00 & elevators & 0.0019 $\pm$ 0.00 \\ 
fifa & 0.78 $\pm$ 0.00 & fried & 1.01 $\pm$ 0.00 \\ 
garments\_worker\_productivity & 0.13 $\pm$ 0.00 & gas\_turbine\_emission & 0.44 $\pm$ 0.00 \\ 
healthcare\_insurance\_expenses & 4716.87 $\pm$ 36.52 & house\_16H\_reg & 29631.75 $\pm$ 251.56 \\ 
house\_8L & 28617.41 $\pm$ 202.41 & house\_prices\_nominal & 30676.02 $\pm$ 2455.48 \\ 
house\_sales\_reduced & 132655.03 $\pm$ 1847.33 & houses & 42559.98 $\pm$ 928.78 \\ 
housing\_price\_prediction & 1009361.62 $\pm$ 8758.05 & kin8nm & 0.08 $\pm$ 0.00 \\ 
mauna-loa-atmospheric-co2 & 0.39 $\pm$ 0.01 & mv & 0.02 $\pm$ 0.00 \\ 
pol\_reg & 3.84 $\pm$ 0.10 & pole & 3.21 $\pm$ 0.14 \\ 
puma32H & 0.01 $\pm$ 0.00 & puma8NH & 3.24 $\pm$ 0.00 \\ 
qsar\_aquatic\_toxicity & 1.05 $\pm$ 0.01 & qsar\_fish\_toxicity & 0.86 $\pm$ 0.01 \\ 
satellite\_image & 0.65 $\pm$ 0.00 & sensory & 0.77 $\pm$ 0.01 \\ 
socmob & 19.53 $\pm$ 0.64 & space\_ga & 0.09 $\pm$ 0.00 \\ 
steel\_industry\_energy & 0.37 $\pm$ 0.03 & stock & 0.65 $\pm$ 0.01 \\ 
stock\_fardamento02 & 17.57 $\pm$ 0.08 & sulfur & 0.03 $\pm$ 0.00 \\ 
topo\_2\_1 & 0.03 $\pm$ 0.00 & treasury & 0.23 $\pm$ 0.00 \\ 
us\_crime & 0.14 $\pm$ 0.00 & volume & 52.09 $\pm$ 0.34 \\ 
weather\_izmir & 1.09 $\pm$ 0.01 & wind & 2.83 $\pm$ 0.00 \\ 
wine+quality & 0.72 $\pm$ 0.00 & yprop\_4\_1 & 0.03 $\pm$ 0.00 \\ 
\bottomrule
\end{longtable}



\begin{table}[h]
    \centering
    \caption{Performance of various models on 18 high-dimensional datasets. The results show the mean accuracy of different models, including ModernNCA (MNCA), MLP, KNN, RealMLP, XGBoost (XGB), Random Forest (RForest), Logistic Regression (LogReg), TabPFN v2 (PFN-v2), TabPFN v2 with PCA (v2-pca), TabPFN v2 with subsampling (v2*), ProtoGate (ProtoG), and CatBoost (CatB). The performance is evaluated on high-dimensional datasets, with the values representing mean accuracy for each model.}
\resizebox{17cm}{!}{
\begin{tabular}{lllllllllllll}
\toprule
Dataset & MNCA & MLP & KNN & RealMLP & XGB & RForest & LogReg & PFN-v2 & v2-pca & v2* & ProtoG & CatB \\
\midrule
CLL\_SUB\_111 & 62.90 & 72.46 & 57.39 & 70.43 & 73.04 & 70.14 & 73.91 & 70.14 & 57.68 & 73.17 & 65.51 & 71.59 \\
BASEHOCK & 96.31 & 97.01 & 71.88 & 97.46 & 95.29 & 96.73 & 96.99 & 69.09 & 97.41 & 97.47 & 96.32 & 95.87 \\
Prostate\_GE & 81.27 & 86.98 & 80.00 & 87.94 & 89.52 & 87.94 & 91.43 & 95.24 & 88.57 & 94.60 & 84.13 & 94.92 \\
PCMAC & 88.21 & 88.53 & 66.48 & 90.15 & 91.64 & 92.20 & 87.15 & 92.70 & 90.76 & 86.86 & 88.21 & 92.01 \\
GLI\_85 & 81.57 & 85.49 & 76.47 & 89.80 & 82.35 & 83.92 & 90.59 & 80.39 & 86.27 & 92.55 & 81.96 & 80.78 \\
RELATHE & 88.18 & 90.54 & 75.03 & 90.23 & 87.11 & 87.30 & 90.49 & 86.36 & 87.65 & 86.81 & 89.92 & 90.35 \\
SMK\_CAN\_187 & 63.51 & 66.84 & 69.47 & 69.82 & 66.49 & 70.70 & 72.11 & 71.05 & 71.75 & 73.28 & 70.71 & 71.40 \\
warpPIE10P & 98.41 & 99.05 & 92.38 & 100.0 & 94.92 & 98.57 & 100.0 & 100.0 & 100.0 & 99.84 & 97.79 & 98.89 \\
leukemia & 90.22 & 95.11 & 86.67 & 94.67 & 97.78 & 92.00 & 96.00 & 92.44 & 93.33 & 95.33 & 94.00 & 94.22 \\
orlraws10P & 97.67 & 98.33 & 92.00 & 99.00 & 84.33 & 99.00 & 99.00 & 92.00 & 99.33 & 98.33 & 92.67 & 99.00 \\
GLIOMA & 58.00 & 60.67 & 68.00 & 67.33 & 66.67 & 64.00 & 64.00 & 62.67 & 69.33 & 68.00 & 69.91 & 66.67 \\
warpAR10P & 83.08 & 85.64 & 53.08 & 97.44 & 81.28 & 87.18 & 97.69 & 90.77 & 95.38 & 92.05 & 90.04 & 87.44 \\
TOX\_171 & 76.00 & 88.19 & 70.86 & 90.48 & 78.10 & 78.67 & 90.29 & 80.95 & 82.48 & 86.86 & 85.52 & 83.05 \\
lung & 91.54 & 95.45 & 93.66 & 95.28 & 93.66 & 92.68 & 95.12 & 95.28 & 93.50 & 95.45 & 95.43 & 93.01 \\
ALLAML & 87.56 & 95.56 & 81.33 & 96.89 & 96.00 & 96.44 & 92.00 & 92.89 & 93.78 & 92.44 & 91.14 & 94.67 \\
colon & 78.46 & 78.97 & 76.92 & 83.08 & 74.87 & 82.56 & 86.15 & 81.54 & 78.46 & 80.00 & 78.46 & 77.95 \\
gisette & 97.21 & 97.57 & 95.04 & 97.86 & 97.55 & 96.82 & 97.51 & 97.35 & 97.26 & 97.37 & 97.18 & 97.78 \\
arcene & 81.67 & 85.50 & 84.50 & 81.00 & 75.00 & 86.83 & 88.00 & 83.67 & 88.33 & 90.67 & 85.33 & 85.00 \\
\midrule
Mean & 82.86 & 87.11 & 77.29 & 88.83 & 84.76 & 86.87 & 89.36 & 85.25 & 87.29 & 88.95 & 86.37 & 87.48 \\
\bottomrule
\end{tabular}
}
    \label{tab:high_dimension_results}
\end{table}

\begin{table}
    \centering
    \caption{Performance of various models on 12 multi-class classification tasks with more than 10 classes. The results show the mean accuracy of different models, including KNN, PFN-v2*, PFN-v2-ECOC, XGB (XGBoost), CatBoost (CatB), Random Forest (RForest), ModernNCA (MNCA), Multi-layer Perceptron (MLP), Logistic Regression (LogReg), and RealMLP. The performance is evaluated on 12 multi-class datasets with more than 10 classes, with accuracy values presented for each model on the respective datasets.}
\resizebox{17cm}{!}{
\begin{tabular}{lcccccccccc}
\toprule
Dataset & KNN & PFN-v2* &PFN-v2-ECOC& XGB & CatB & RForest & MNCA & MLP & LogReg & RealMLP \\
\midrule
100-plants-texture & 79.69 & 90.94 &84.92& 77.06 & 89.73 & 82.65 & 80.52 & 83.92 & 86.88 & 88.35 \\
100-plants-margin & 77.50 & 88.56&79.40 & 74.25 & 84.06 & 82.79 & 77.60 & 80.44 & 79.69 & 83.58 \\
100-plants-shape & 60.31 & 79.52 &63.38 & 56.15 & 65.19 & 64.33 & 70.10 & 47.33 & 65.94 & 72.08 \\
UJI\_Pen\_Characters & 36.26 & 45.71 &44.20 & 30.35 & 38.88 & 34.24 & 44.03 & 37.75 & 19.41 & 46.37 \\
texture & 98.45 & 100.0&100.0 & 98.55 & 99.13 & 96.76 & 99.68 & 99.40 & 99.64 & 99.95 \\
letter & 94.90 & 97.57 &97.78 & 96.26 & 96.75 & 91.56 & 97.96 & 96.40 & 75.80 & 98.31 \\
walking-activity & 60.29 & 61.22 &61.92 & 65.06 & 64.92 & 61.74 & 64.85 & 60.64 & 27.02 & 65.13 \\
helena & 28.94 & 33.31 &19.20& 32.42 & 37.90 & 33.91 & 36.58 & 37.91 & 33.40 & 38.55 \\
internet\_usage & 30.17 & 54.34&50.86 & 51.08 & 37.90 & 33.91 & 52.09 & 43.00 & 37.73 & 52.23 \\
kropt & 71.22 & 77.96 &77.11 & 86.95 & 79.26 & 71.77 & 78.27 & 64.45 & 28.08 & 92.03 \\
kr-vs-k & 70.78 & 78.46&76.29 & 87.26 & 74.81 & 71.60 & 76.83 & 65.03 & 28.03 & 91.85 \\
ASP-POTASSCO & 34.75 & 43.50 & 45.27 &42.24& 41.08 & 42.86 & 37.45 & 29.63 & 35.14 & 41.70 \\
\midrule
Mean & 61.94&70.93&66.69&66.47 &67.47 &64.01 &68.00&62.16&51.40&72.51\\
\bottomrule
\end{tabular}
}
    \label{tab:res_multi_class_results}
\end{table}


\begin{table}[h]
    \centering
    \caption{Performance of various models on 18 large-scale datasets. The results show the mean accuracy/RMSE of different models, including MLP, Logistic Regression/Linear Regression (LR), KNN, XGBoost (XGB), Random Forest (RForest), CatBoost (CatB), ModernNCA (MNCA), RealMLP, and various versions of TabPFN v2: original TabPFN v2 (PFNv2), TabPFN v2 with K-means (PFNv2-K), TabPFN v2 with Bagging (PFNv2-B), and PFNv2* (TabPFNv2*).}
    \label{tab:large_scale}
% \tiny{
\resizebox{17cm}{!}{
\begin{tabular}{lccccccccccccc}
\toprule
Dataset & MLP & LR & KNN & XGB & RForest & CatB & MNCA & RealMLP & PFNv2 & PFNv2-K & PFNv2-B & PFNv2* &PFNv2-DT\\
\midrule
BNG(credit-a) & 90.07 & 85.98 & 87.41 & 90.21 & 89.25 & 91.13 & 89.98 & 90.91 & 89.55 & 89.01 & 89.66 & 89.89 &89.96\\
CDC\_Indicators & 86.79 & 86.55 & 86.39 & 86.76 & 86.60 & 86.78 & 86.76 & 86.76 & 86.65 & 86.68 & 86.69 & 86.74 &86.22\\
Higgs & 75.53 & 64.29 & 65.16 & 73.33 & 71.87 & 74.81 & 73.28 & 75.36 & 71.64 & 71.56 & 72.01 & 72.13 &72.44\\
Smoking\_signal & 73.90 & 72.53 & 72.36 & 73.87 & 73.08 & 73.99 & 73.63 & 74.00 & 73.47 & 73.37 & 73.55 & 73.69 &72.94 \\
nomao & 96.19 & 94.59 & 95.20 & 96.92 & 96.07 & 97.03 & 96.68 & 96.37 & 96.08 & 96.29 & 96.12 & 96.18 &96.51\\
sf-police-incidents & 87.84 & 87.84 & 85.87 & 87.68 & 87.84 & 87.87 & - & 87.84 & 87.84 & 87.84 & 87.84 & 87.84 &87.84\\
Data\_Crowdfunding & 96.48 & 67.04 & 93.70 & 96.89 & 95.29 & 96.81 & 96.53 & 96.71 & 94.59 & 91.81 & 94.96 & 95.07 &96.93\\
Fashion-MNIST & 89.54 & 85.69 & 86.00 & 90.03 & 86.57 & 90.24 & 89.36 & 90.25 & 68.40 & 82.82 & 83.89 & 86.26 &78.58\\
covertype & 94.01 & 72.54 & 92.76 & 96.30 & 78.30 & 90.77 & 97.31 & 97.38 & 83.54 & 82.95 & 84.16 & 86.85 &96.89\\
jannis & 71.99 & 64.60 & 65.67 & 71.83 & 69.19 & 72.26 & 72.57 & 73.00 & 70.24 & 70.26 & 70.59 & 71.31 &71.90\\
poker-hand & 99.99 & 50.12 & 54.01 & 99.51 & 64.63 & 97.69 & 76.31 & 99.88 & 41.97 & 38.86 & 36.80 & 54.12 &85.60\\
volkert & 69.85 & 58.75 & 67.41 & 69.74 & 62.71 & 70.88 & 77.18 & 73.76 & 62.82 & 62.15 & 62.81 & 64.84 &68.53\\
Airlines\_DepDelay ($\times 10^1$) & 2.905 & 2.933 & 3.170 & 2.891 & 2.907 & 2.881 & - & 2.482 & 2.937 & 2.933 & 2.937 & 2.915 &2.906 \\
Wave\_Energy\_Farm ($\times 10^3$) & 8.199 & 13.19 & 32.29 & 6.917 & 7.294 & 7.173 & 6.148 & 59.05 & 7.214 & 8.375 & 7.063 & 10.506 &6.489\\
UJIndoorLoc ($\times 10^0$)  & 9.958 & $\infty$ & 9.004 & 10.47 & 23.19 & 9.139 & 5.990 & 65.34 & 66.49 & 7.825 & 7.435 & 9.538 &7.024\\
blogfeedback ($\times 10^1$ ) & 2.387 & $\infty$ & 2.410 & 2.093 & 2.026 & 2.044 & 1.953 & 2.105 & 3.073 & 2.687 & 2.700 & 2.014 &2.011\\
microsoft ($\times 10^{-1}$ ) & 7.577 & 7.782 & 8.284 & 7.514 & 7.566 & 7.453 & 7.573 & 5.077 & 7.735 & 7.981 & 7.720 & 7.612 &7.952\\
yahoo ($\times 10^{-1}$ ) &7.692 & 7.997 & 8.504 & 7.629 & - & 7.514 & - & 5.671 & 8.148 & 8.332 & 8.132 & 7.961 &8.118\\
\bottomrule
\end{tabular}
}
\end{table}

\begin{table}[h]
    \centering
        \caption{Performance comparison of FT-T and MNCA with and without randomized feature tokens across tiny benchmark datasets of~\citet{Ye2024Closer}. The first two columns (FT-T$^\star$ and MNCA$^\star$) show results using randomized tokens, while the last two columns (FT-T and MNCA) represent the original methods. All values are the mean of 15 runs with different random seeds. For classification tasks, the performance is measured by accuracy, while for regression tasks, the metric used is Root Mean Squared Error (RMSE). The table demonstrates the effect of randomized feature tokens on model performance in both types of tasks.}
    \label{tab:random_token_results}
\begin{tabular}{lcccc}
\toprule
Dataset & FT-T* & MNCA* & FT-T & MNCA \\
\midrule
BNG(breast-w) & 98.62 & 98.66 & 98.60 & 98.59 \\
BNG(cmc) & 58.35 & 58.39 & 58.50 & 58.56 \\
BNG(tic-tac-toe) & 80.92 & 81.10 & 81.31 & 81.08 \\
Cardiovascular-Disease-dataset & 73.35 & 73.29 & 73.22 & 73.20 \\
FOREX\_audchf-day-High & 52.90 & 72.93 & 51.79 & 75.44 \\
FOREX\_audsgd-hour-High & 69.53 & 66.86 & 61.84 & 71.09 \\
FOREX\_cadjpy-hour-High & 69.62 & 70.99 & 68.80 & 71.18 \\
Gender\_Gap\_in\_Spanish\_WP & 59.16 & 59.16 & 59.86 & 59.25 \\
KDD & 80.15 & 80.12 & 80.28 & 79.58 \\
VulNoneVul & 98.93 & 98.81 & 98.93 & 98.79 \\
baseball & 93.56 & 93.71 & 93.76 & 92.74 \\
credit & 75.83 & 75.39 & 75.88 & 72.98 \\
dis & 98.29 & 98.31 & 98.76 & 98.68 \\
eye\_movements\_bin & 59.01 & 61.31 & 59.45 & 90.88 \\
jungle\_chess\_2pcs\_raw\_endgame\_complete & 97.11 & 99.67 & 97.61 & 99.50 \\
law-school-admission-bianry & 99.00 & 100.0 & 98.02 & 100.0 \\
mfeat-fourier & 86.28 & 85.78 & 85.23 & 86.48 \\
online\_shoppers & 89.70 & 89.65 & 90.19 & 90.53 \\
page-blocks & 96.89 & 97.23 & 96.71 & 96.68 \\
pc3 & 88.54 & 89.16 & 89.78 & 88.75 \\
pendigits & 99.20 & 99.39 & 99.35 & 99.40 \\
rl & 66.99 & 79.74 & 71.96 & 83.75 \\
satimage & 89.61 & 90.77 & 89.59 & 91.04 \\
segment & 91.49 & 91.40 & 91.37 & 92.77 \\
sylvine & 94.48 & 94.22 & 94.71 & 95.88 \\
taiwanese\_bankruptcy\_prediction & 96.90 & 96.68 & 96.74 & 96.74 \\
waveform-5000 & 85.89 & 86.02 & 86.11 & 86.24 \\
website\_phishing & 89.45 & 89.94 & 90.14 & 87.53 \\
wine-quality-white & 54.98 & 64.41 & 55.17 & 63.27 \\
\midrule
Ailerons & 0.000157 & 0.000158 & 0.000155 & 0.000158 \\
CookbookReviews & 1.560657 & 1.491684 & 1.520183 & 1.486840 \\
IEEE80211aa-GATS & 0.030513 & 0.027608 & 0.029589 & 0.026295 \\
Large-scale\_Wave\_Energy\_Farm\_Sydney\_49 & 4504.285 & 4369.826 & 4369.902 & 47399.63 \\
Superconductivty & 10.53080 & 10.39726 & 10.64536 & 10.37837 \\
archive2 & 388.8410 & 379.5506 & 390.6574 & 361.8616 \\
bank8FM & 0.028792 & 0.029047 & 0.028635 & 0.028826 \\
communities\_and\_crime & 0.143486 & 0.139045 & 0.136859 & 0.138984 \\
fried & 1.013058 & 1.013431 & 1.008568 & 1.010952 \\
healthcare\_insurance\_expenses & 4935.203 & 4743.384 & 4615.580 & 4698.968 \\
house\_16H\_reg & 31647.43 & 31333.32 & 30671.63 & 31017.80 \\
kin8nm & 0.068684 & 0.070110 & 0.067355 & 0.069678 \\
mv & 0.027143 & 0.019089 & 0.030948 & 0.027385 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{Performance of \ours and the extracted feature embeddings across 29 classification datasets. The table shows the average classification accuracy for each dataset when using different layers (Layer 6, Layer 9, Layer 12) of the transformer as feature embeddings, as well as the ``combined'' approach, where embeddings from up to three selected layers are concatenated. The ``selected layers'' column indicates the specific layers chosen for each dataset based on validation set performance. ``Vanilla'' refers to the embeddings extracted using the vanilla strategy, which utilizes only the 12th layer of the transformer.}
\label{embeddings_results}
\resizebox{17cm}{!}{
\begin{tabular}{lccccccc}
\toprule
  & PFN-v2 & Vanilla & layer-6 & layer-9 & layer-12 & combined& selected layers \\
\midrule
FOREX\_audchf-day-High & 77.38 & 72.48 & 68.39 & 73.57 & 74.11 & 77.11&(5, 9, 11) \\
taiwanese\_bankruptcy\_prediction & 96.99 & 96.70 & 97.14 & 96.77 & 97.07 & 97.14&(6) \\
rl & 85.51 & 85.41 & 66.90 & 69.52 & 86.72 & 87.53 &(11, 12)\\
pc3 & 89.46 & 89.78 & 90.10 & 88.82 & 88.82 & 88.82 &(8) \\
eye\_movements\_bin & 61.83 & 62.75 & 59.72 & 59.40 & 62.16 & 62.16 &(6, 9, 12)\\
BNG(breast-w) & 98.43 & 98.51 & 98.34 & 98.46 & 98.67 & 98.51&(6, 9) \\
FOREX\_cadjpy-hour-High & 69.53 & 70.77 & 62.12 & 64.87 & 70.66 & 70.88 &(4, 5, 6)\\
dis & 99.34 & 99.07 & 98.41 & 98.28 & 99.34 & 99.47&(4, 5, 6) \\
sylvine & 97.46 & 97.27 & 92.49 & 93.95 & 97.27 & 96.49 &(1, 11)\\
BNG(tic-tac-toe) & 78.04 & 78.75 & 73.96 & 73.71 & 78.75 & 79.03&(5, 10, 12) \\
online\_shoppers & 90.59 & 90.11 & 90.02 & 90.11 & 90.63 & 90.02&(8) \\
Cardiovascular-Disease-dataset & 72.84 & 73.08 & 72.96 & 73.06 & 73.14 & 73.09 &(5, 8, 12)\\
credit & 78.04 & 77.06 & 77.62 & 77.80 & 77.95 & 77.59 &(4, 6, 9)\\
FOREX\_audsgd-hour-High & 67.26 & 69.30 & 57.24 & 61.06 & 69.62 & 70.41  &(7, 10, 12)\\
waveform-5000 & 86.00 & 85.30 & 85.60 & 85.60 & 86.40 & 86.90 &(1, 6, 11)\\
jungle\_chess & 85.65 & 85.30 & 78.55 & 80.44 & 86.66 & 86.85 &(10, 11, 12)\\
BNG(cmc) & 57.40 & 57.78 & 56.19 & 56.72 & 57.72 & 57.88 &(9, 10, 12)\\
page-blocks & 97.35 & 97.17 & 96.07 & 96.71 & 97.17 & 97.35 &(6, 7, 12) \\
segment & 93.07 & 93.29 & 91.99 & 88.10 & 93.51 & 92.64 &(1, 12)\\
website\_phishing & 90.77 & 88.93 & 85.98 & 87.08 & 91.88 & 91.88 &(7, 10) \\
baseball & 93.66 & 94.03 & 93.28 & 94.03 & 93.66 & 95.15 &(10, 11)\\
pendigits & 99.50 & 99.36 & 92.81 & 93.04 & 99.41 & 99.45&(3, 4, 12) \\
Gender\_Gap\_in\_Spanish\_WP & 60.84 & 55.79 & 59.68 & 60.32 & 60.53 & 60.84  &(2, 12)\\
wine-quality-white & 62.35 & 62.04 & 54.08 & 55.31 & 63.57 & 64.39&(8, 11, 12) \\
satimage & 91.21 & 92.07 & 88.72 & 88.65 & 91.91 & 91.91 &(8, 11, 12)\\
mfeat-fourier & 90.00 & 89.75 & 77.75 & 82.25 & 89.50 & 89.50 &(2, 7, 12)\\
VulNoneVul & 98.95 & 98.95 & 98.95 & 98.95 & 98.95 & 98.95 & (1)\\
law-school-admission-bianry & 100.0 & 100.0 & 100.0 & 100.0 & 100.0 & 100.0 &(6)\\
KDD & 80.34 & 78.95 & 79.34 & 78.35 & 81.23 & 79.94 &(1, 8, 10)\\
\bottomrule
\end{tabular}
}
\end{table}


\section{Influence of Key Modules}\label{sec:appendix_ablation}
We investigate the influence of two key components in \ours, \ie, the feature engineering that pre-process the raw features of a given tabular dataset and the post-hoc ensembleing.

\noindent{\bf Feature engineering}. \ours pre-processes the features of a given tabular datset with various strategies, such as quantile, category shuffling, SVD, and power transform. Specifically, we examine the effects of adding fingerprint features (\texttt{add\_fingerprint\_feature}) and polynomial features (\texttt{polynomial\_features}) to the raw tabular data. The results indicate that \ours performs well even without the use of these engineered features, suggesting that, for the benchmark datasets of~\citet{Ye2024Closer}, these specific feature engineering techniques do not provide a significant improvement. This finding highlights the robustness of \ours and its ability to handle raw features effectively, without the need for extensive pre-processing or feature construction. We show the influence of this step in~\autoref{fig:engineering}.

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{files/Ablation/Feature_engineering.pdf}
    \caption{Scatter plot comparing the normalized Accuracy/R$^2$ scores. The x-axis represents the normalized Accuracy/R$^2$ scores without Feature Engineering, while the y-axis represents the normalized Accuracy/R$^2$ scores with Feature Engineering. The red dashed line ($y = x$) serves as a reference, indicating equal performance.}
    \vspace{-5mm}
    \label{fig:engineering}
\end{figure}

\noindent{\bf Model ensemble}. Post hoc ensembling (PHE) involves applying \ours to the datasets multiple times with different perturbations and aggregating the predictions of these base models at different temperatures. We show the change of performance of \ours w.r.t. the number of ensemble numbers (\ie, the number of base models) in~\autoref{fig:ensemble}. On the benchmark of~\citet{Ye2024Closer}, we observe that, overall, ensemble methods improve performance, with larger ensemble sizes yielding better results. However, we also note that even without ensembling, \ours performs exceptionally well, and the relative performance gain from ensembling is limited. This suggests that while ensembling can provide further improvements, the base \ours model is already highly effective on its own. 
The equivariant property described in~\cite{Arbel2025EquiTabPFN} provides insight into this phenomenon. Since \ours introduces random tokens to handle heterogeneous features, the model becomes less sensitive to the arbitrary ordering of features, effectively enforcing equivariance in this aspect. As a result, the benefits of ensembling through feature order permutations are less pronounced compared to TabPFN v1.


\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{files/Ablation/Ensemble.pdf}
    \caption{Box plot of relative performance improvements of \ours with post hoc ensembling (PHE) across different ensemble sizes (2, 4, 8, and 16 base models). The relative improvement is calculated as the performance gain over the non-ensemble model, where higher values indicate stronger performance. The box plots show the median, interquartile range (IQR), and outliers for each ensemble size.}
    \vspace{-5mm}
    \label{fig:ensemble}
\end{figure}



