\section{Comprehensive Evaluation of TabPFN v2}
Building upon the remarkable performance of \ours, we conduct an extensive evaluation across diverse scenarios to assess its generalizability. Specifically, we evaluate \ours on over 300 tabular datasets spanning a wide range of domains, scales, dimensionalities, and tasks.
\begin{figure}[t]
    \centering
                \vspace{-2mm}
    \includegraphics[width=0.95\linewidth]{files/critdd/261_Full.pdf}
            \vspace{-5mm}
    \caption{Wilcoxon-Holm test at a significance level of 0.05 over 261 small- to medium-scale datasets.
    We omit the 12 datasets with more than 10 classes and the 27 datasets used to select the \ours checkpoint from the 300 datasets in~\cite{Ye2024Closer}.
    }
    \vspace{-6mm}
    \label{fig:critical_difference}
\end{figure}
\subsection{Setups}
To ensure a comprehensive evaluation, we adopt the benchmark proposed in~\citet{Ye2024Closer}, which includes 300 datasets covering 120 binary classification tasks, 80 multi-class classification tasks, and 100 regression tasks across various domains, dataset sizes, and feature types. This benchmark addresses common issues, such as mislabeled datasets and dataset redundancies due to multi-version overlaps~\cite{kohli2024towards}, ensuring high-quality evaluations.

Out of the 300 datasets, 12 classification datasets contain more than 10 classes, and 27 datasets overlap with the validation set used for checkpoint selection during \ours's pre-training~\cite{hollmann2025TabPFNv2}. To avoid evaluation bias, we exclude these datasets and report comparison results on the remaining 261 datasets.

Following the protocol in~\citet{GorishniyRKB21Revisiting,gorishniy2023tabr}, each dataset is randomly split into training, validation, and test partitions in proportions to 64\%/16\%/20\%. %\ours predicts the test set labels directly using the training set without hyper-parameter tuning. 
\ours predicts the test set labels directly in an in-context learning fashion without further parameter and hyper-parameter tuning.
Compared (deep) tabular methods perform hyper-parameter tuning using Optuna~\citep{akiba2019optuna} with 100 trials over the training set, and are early stopped on the validation set. All methods are evaluated with 15 random seeds, and the average performance across seeds is reported.

For classification tasks, we use accuracy (higher is better) as the evaluation metric, while regression tasks are evaluated using Root Mean Square Error (RMSE, lower is better).

\begin{figure}[t]
    \centering
                    \vspace{-2mm}
    \includegraphics[width=\linewidth]{files/PAMA/PAMA_261.pdf}
        \vspace{-8mm}
    \caption{Probability of Achieving the Best Accuracy/RMSE (PAMA) across 261 small- to medium-scale datasets. The values inside the rectangles represent the percentage of datasets where each method achieves the best accuracy.
    }
    \vspace{-2mm}
    \label{fig:pama}
\end{figure}
\subsection{Results}

We compare \ours against \textbf{26} representative (deep) tabular methods on 261 datasets (detailed references are in the appendix). To assess statistical significance, we apply the Wilcoxon-Holm test at a 0.05 significance level~\citep{Demsar06Statistical}. The critical difference analysis results are presented in~\autoref{fig:critical_difference}.

The results demonstrate that \ours consistently outperforms both tree-based methods, such as CatBoost \cite{Prokhorenkova2018Catboost}, and deep tabular models, including RealMLP~\cite{David2024RealMLP}, ModernNCA \cite{Ye2024ModernNCA}, TabR~\cite{gorishniy2023tabr}, and FT-Transformer~\cite{GorishniyRKB21Revisiting}, with statistically significant differences. 


To further analyze performance, we consider the Probability of Achieving the Best Accuracy/RMSE (PAMA)~\citep{DelgadoCBA14}, which measures the percentage of datasets where a method achieves the best performance. As shown in~\autoref{fig:pama}, \ours achieves the highest PAMA score, with 25.06\% of datasets yielding the best results, outperforming other methods like ModernNCA (with a percentage of 13.73\%) and CatBoost (9.48\%). These results validate \ours's exceptional generalizability on small- to medium-scale tabular datasets, further enhanced by its efficiency and lack of reliance on hyper-parameter tuning.
\begin{table}[t]
\caption{Average rank (lower is better) of \ours and representative comparison methods on 18 high-dimensional datasets and 18 large-scale datasets.
Full results with our extensions are in~\autoref{fig:high_dimension} and~\autoref{fig:large_scale}.
}
\label{tab:sub_compare}
\tabcolsep 1.5pt
\begin{tabular}{cccccc}
\addlinespace
\toprule
$\downarrow$ & \ours & CatBoost & MNCA & RealMLP & LR \\     
\midrule
High-Dim & 3.36 & 2.82 & 4.41 &   2.14 &  2.27 \\
Large-Scale & 3.97 & 1.89 & 2.27 & 1.94 & 4.47  \\
\bottomrule
\end{tabular}
\vspace{-6mm}
\end{table}
\subsection{Additional Evaluations}
To examine \ours's performance on larger and more complex datasets, we conduct additional evaluations on 18 large-scale datasets with $N \times d > 1,000,000$ and 18 high-dimensional datasets with $d \ge 2,000$~\cite{Jiang2024ProtoGate}. For high-dimensional datasets, we follow the same evaluation protocol as before. For large-scale datasets, due to the prohibitive cost of hyper-parameter tuning, default hyper-parameters are used for all methods.
The average ranks of some representative methods are summarized in~\autoref{tab:sub_compare}. Full results are in~\autoref{sec:extension} with our extensions.

While \ours performs superiorly on small- to medium-scale datasets, its performance degrades on large-scale and high-dimensional datasets. For instance, on large-scale datasets, \ours ranks lower than both CatBoost and RealMLP, highlighting its scalability limitations. On high-dimensional datasets, \ours fails to outperform even the simple Logistic Regression (LR) model, further emphasizing the need for enhancements to better handle these challenging scenarios.

These results highlight \ours's limitations in handling high-dimensional and large-scale datasets, which may result from two key factors. First, \ours is pre-trained exclusively on small- and medium-scale synthetic datasets, creating a mismatch when applied to large-scale datasets. Second, the complexity of transformers in \ours scales with both $N$ (the number of instances) and $d$ (the number of features), meaning that increasing either value significantly can lead to a substantial rise in computational complexity, which in turn may reduce the model's effectiveness.

In summary, while \ours demonstrates exceptional performance on small- to medium-scale datasets, scaling it up to handle larger datasets, higher-dimensional feature spaces, and multi-class tasks exceeding 10 classes is a crucial and promising direction for future research.