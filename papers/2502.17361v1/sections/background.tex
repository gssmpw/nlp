\section{Preliminary}\label{sec:relimiary}
We formalize the problem of learning with tabular datasets and briefly describe the core mechanism of \ours.
\begin{figure}
    \centering
        \vspace{-2mm}
    \includegraphics[width=\linewidth]{files/teaser.pdf}
    \vspace{-7mm}
    \caption{Illustration of the \ours mechanism~\cite{hollmann2025TabPFNv2} for classification.
    $\{A_1, \ldots, A_d\}$ denote $d$ attributes of the task. 
    Training examples and test instances are combined into a tabular context set and transformed into a ${(N+1) \times d \times k}$ tensor using a combination of learnable and randomized tokens.
    Two types of self-attentions are applied alternately along rows (inter-sample) and columns (inter-feature).
    The output token corresponding to the (dummy) label of the test instance is processed through an MLP to generate a 10-class logit.
    }
    %\vspace{-5mm}
    \label{fig:tabpfnv2}
\end{figure}

\noindent{\bf Learning with a Single Tabular Dataset}.
A tabular dataset $\gD = \{(\vx_i, y_i)\}_{i=1}^N$ contains $N$ training examples, corresponding to the rows in a table. Each instance $\vx_i$ is represented by $d$ features or attributes (columns in the table); its label $y_i$ belongs to $[C] = \{1, \ldots, C\}$ for a classification task or is a numerical value for a regression task.
We assume all attributes of an instance are numerical (continuous). If categorical (discrete) attributes are present, they are transformed with ordinal/one-hot encoding beforehand. The goal of tabular machine learning is to construct a mapping $f$ from an instance to its label. Specifically, given an unseen instance $\vx^* \in \sR^d$ sampled from the same distribution as $\gD$, the learned mapping $f$ predicts its label as $\hat{y}^* = f(\vx^* \mid \gD)$. A lower prediction discrepancy between $\hat{y}^*$ and the true label $y^*$ indicates a stronger generalization ability of $f$.

\noindent{\bf TabPFN Variants}.
The original TabPFN implements $f$ for classification via a Transformer-like architecture~\cite{Hollmann2022TabPFN}. Both training and test instances are padded to a fixed dimension $k'$ (\eg, 100) with zeros.
Then, $\vx_i$ and $y_i$ are further mapped to $\tilde{\vx}_i \in \sR^k$ and $\tilde{\vy}_i \in \sR^k$ with linear transformations. TabPFN takes both a labeled training set and an unlabeled test instance as input, predicting the test label in an ``in-context learning'' manner. The task context is formulated as:
\begin{equation} \gC = \{(\tilde{\vx}_1 + \tilde{\vy}_1), \ldots, (\tilde{\vx}_N + \tilde{\vy}_N), (\tilde{\vx}^*)\}\in\sR^{k \times (N+1)} \;,\notag \end{equation}
with $N+1$ tokens of $k$ dimensions each. These tokens are processed through multiple transformer layers, which support variable-length inputs. The output token corresponding to the test instance is further mapped to a 10-class logit using a multi-layer perceptron (MLP).

The recently introduced \ours incorporates several modifications to the original TabPFN. 
For example, each attribute in $\gD$ is mapped to a $k$-dimensional space through linear projection. Random perturbations are added to differentiate attributes in the $k$-dimensional space, similar to a kind of positional encoding.
The core idea is illustrated in~\autoref{fig:tabpfnv2}. We view this strategy as a variant of the token-based method~\cite{SongS0DX0T19AutoInt,GorishniyRKB21Revisiting,Yan2024Making} and will discuss the construction of tokens and analyze how they handle the challenges of heterogeneous attribute spaces in~\autoref{sec:heterogeneous}. Together with the mapped label embedding $\tilde{\vy}_i$, an instance $\vx_i$ is transformed into tokens with size $(d+1)\times k$. For a test instance $\vx^*$, since its label is unknown, a dummy label (\eg, the average of training set labels) is used to generate the label embedding $\tilde{\vy}^*$.

Subsequently, the set of training and test instances is transformed into a ${(N+1) \times (d+1) \times k}$ tensor. Two types of self-attentions are alternately applied for inter-sample and inter-feature in-context learning. Finally, the output token corresponding to the test instance's dummy label $\tilde{\vy}^*$ is extracted and mapped to either a 10-class logit for classification or a single-value logit for regression.

The weights in \ours are pre-trained on synthetic datasets generated using structural causal models (SCMs); the checkpoint is selected based on real-world datasets. This pre-training enables \ours to be directly applied to downstream tasks without additional tuning, effectively addressing the heterogeneity of tabular data. 
However, the complexity of transformers with respect to the input size limits \ours's scalability to small and medium datasets (\eg, $N < 10,000$) and its ability to handle classification with fewer than 10 classes. Please refer to~\citet{hollmann2025TabPFNv2} for further details, including feature pre-processing, feature grouping, acceleration, and post-hoc ensembling.