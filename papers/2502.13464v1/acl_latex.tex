% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\usepackage{amsmath}
\usepackage{amsfonts,amssymb}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{epstopdf}
\usepackage{colortbl}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{tabulary}
\usepackage[inline]{enumitem}

\title{Estimating Commonsense Plausibility through Semantic Shifts}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Wanqing Cui, Keping Bi, Jiafeng Guo, Xueqi Cheng \\
        CAS Key Lab of Network Data Science and Technology, \\ 
        Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China\\
        University of Chinese Academy of Sciences, Beijing, China\\
        \texttt{\{cuiwanqing18z, bikeping, guojiafeng, cxq\}@ict.ac.cn} \\}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}

% Accurately estimating commonsense plausibility is critical for ensuring that the content generated by large language models (LLMs) is consistent with commonsense knowledge. Inspired by the linguistic phenomenon that omitting commonsense information typically causes minimal semantic changes, we propose a novel zero-shot approach for estimating \textbf{COM}monsense \textbf{P}l\textbf{a}usibility through \textbf{S}emantic \textbf{S}hifts (ComPaSS) from a discriminative perspective, which quantifies plausibility by measuring how much a sentence's meaning changes when incorporating specific information: plausible information causes smaller semantic shifts, while implausible information leads to larger shifts. Extensive experiments on attribute value ranking and commonsense frame completion tasks demonstrate that ComPaSS consistently outperforms existing methods across diverse model architectures, including both language models (LMs) and visual language models (VLMs). Our analysis further reveals that discriminative approaches are inherently better suited than generative methods for capturing fine-grained differences in commonsense reasoning, providing valuable insights for future research.
% The results also reveal that LMs tend to underutilize their internal knowledge during generation, whereas VLMs excel at modeling visual-related commonsense through multimodal alignment. 
% Our findings highlight the importance of semantic representation quality and multimodal integration for commonsense modeling, offering a scalable, training-free approach to evaluating and improving LLM text generation.

Commonsense plausibility estimation is critical for evaluating language models (LMs), yet existing generative approaches--reliant on likelihoods or verbalized judgments--struggle with fine-grained discrimination. In this paper, we propose ComPaSS, a novel discriminative framework that quantifies commonsense plausibility by measuring semantic shifts when augmenting sentences with commonsense-related information. Plausible augmentations induce minimal shifts in semantics, while implausible ones result in substantial deviations. Evaluations on two types of fine-grained commonsense plausibility estimation tasks across different backbones, including LLMs and vision-language models (VLMs), show that ComPaSS consistently outperforms baselines. It demonstrates the advantage of discriminative approaches over generative methods in fine-grained commonsense plausibility evaluation. Experiments also show that (1) VLMs yield superior performance to LMs, when integrated with ComPaSS, on vision-grounded commonsense tasks. (2) contrastive pre-training sharpens backbone models’ ability to capture semantic nuances, thereby further enhancing ComPaSS.

\end{abstract}

\section{Introduction}

Commonsense knowledge--the shared understanding of everyday phenomena and human experiences ~\cite{schank1983dynamic, winograd1986understanding, hobbs1990granularity}--is foundational to natural language understanding and generation. Despite the remarkable progress in large language models' (LLMs) text generation capabilities, ensuring commonsense plausibility in their outputs remains an unresolved challenge ~\cite{Marcus2020TheND, Elazar2021MeasuringAI, Mahowald2023DissociatingLA, Chen2023SayWY}. This challenge arises not only from the inherent difficulty of acquiring and applying commonsense knowledge but also from the absence of reliable frameworks for evaluating textual plausibility. Effective evaluation of commonsense plausibility addresses this gap twofold: it identifies commonsense violations while offering quantifiable metrics to guide the development of techniques that augment LLM outputs.

In this work, we focus on developing generalizable methods for commonsense plausibility estimation (CSPE) that can be applied across diverse domains and tasks. This leads us to investigate zero-shot and few-shot approaches based on pre-trained LMs, which leverage their inherent knowledge without requiring additional training data or domain-specific fine-tuning.

Previous studies on zero or few-shot CSPE primarily adopt a generative perspective and can be categorized into two main approaches, likelihood estimation and verbalized judgments. The likelihood-based methods~\cite{Trinh2018ASM, tamborrino2020pre, holtzman2021surface} utilize token prediction probabilities from language models as an indicator, with the assumption that sentences consistent with commonsense knowledge tend to have a higher likelihood for their component tokens. The verbalization-based methods~\cite{brown2020language, krause2024data} ask pre-trained LMs to answer the plausibility of a sentence through natural language. The models can generate the answer based on knowledge stored in their parameters.

However, approaches based on the generative perspective could be suboptimal for CSPE, since it is essentially a discriminative task. In this paper, we adopt a discriminative perspective for CSPE. In communication, commonsense knowledge is often assumed and left unstated, yet such omissions rarely hinder mutual understanding~\cite{clark1996using, noveck2004experimental}. Inspired by this, we propose ComPaSS, a method that measures \textbf{Com}monsense \textbf{P}l\textbf{a}usibility through \textbf{S}emantic \textbf{S}hifts introduced when augmenting sentences with commonsense-related information. Plausible additions yield minimal semantic shifts, whereas implausible ones result in substantial deviations. For instance, adding `black' to `There is a penguin' results in a minor semantic shift, aligning with the penguins' natural coloration. By contrast, introducing `green' creates a substantial shift, highlighting the implausibility of such an atypical attribute. To quantify semantic shifts, ComPaSS computes the similarity between embeddings of the original sentence (without explicit commonsense references) and its modified counterpart augmented with commonsense-related information.

Two aspects of semantic representations could influence the capability of ComPaSS in CSPE: the inclusion of commonsense knowledge and the discrimination of semantic nuances. These correspond to two key aspects of models used for obtaining sentence embeddings: 1) Modality. Language Models (LMs) often suffer from \textit{reporting bias}~\cite{Gordon2013ReportingBA}, which involves systematic distortions due to omitted commonsense details (e.g., `penguins are black' is rarely stated) and statistical biases from fixed linguistic patterns (e.g., `black sheep'). In contrast, vision-language models (VLMs) incorporate visual information, thus mitigating reporting bias, especially for visually-grounded commonsense knowledge (e.g., object colors or spatial relations)~\cite{paik2021world, zhang2022visual}. 2) Contrastive learning. By training a model to distinguish between semantically similar and dissimilar instances, it enhances the model's discriminative power. Representations from contrastively trained models exhibit sharper separability, which directly impacts the precision of semantic shift measurements. Given these considerations, we study how ComPaSS performs based on various backbones of both LMs and VLMs, with and without contrastive learning.

We evaluate ComPaSS against baselines on two fine-grained CSPE tasks that require ranking candidate answers by plausibility rather than binary classification. These tasks prioritize nuanced plausibility judgments, where answers may hold varying degrees of validity. The first task, attribute value ranking, involves ranking candidate attribute values (e.g., color, shape, material) for objects using structured triplets (e.g., determining that "black" is more plausible than "green" for penguin-color), evaluated on datasets like CoDa~\cite{paik2021world} and ViComTe~\cite{zhang2022visual}. The second task, commonsense frame completion~\cite{cheng-etal-2024-every}, challenges models to rank plausible completions for open-ended prompts (e.g., selecting `farm' over `truck' for `Where are farmers with newly harvested crops?'), testing alignment with human preferences and broader commonsense reasoning. Together, these tasks assess ComPaSS across input formats (structured triplets vs. free-form text) and knowledge types (object-specific attributes vs. contextual, inferential commonsense).

Our experiments reveal three critical insights. First, as a discriminative approach, ComPaSS consistently outperforms prior generative methods in fine-grained plausibility estimation, achieving superior results across diverse model backbones. This highlights the advantage of discriminative methods in capturing subtle plausibility distinctions. Second, utilizing ComPaSS, VLMs significantly outperform LMs for vision-grounded commonsense (e.g., object colors or shapes), demonstrating that visual information enhances representations and benefits CSPE. Third, models with contrastive pre-training yield significantly better results than those without, emphasizing the importance of representations that capture semantic nuances in plausibility measurement through ComPaSS.


% 可加可不加（看篇幅）
% Our contributions can be summarized as follows:
% (1) We propose a novel zero-shot discriminative perspective for estimating the commonsense plausibility based on semantic shifts.
% (2) We conduct thorough experiments on two types of fine-grained CSPE tasks and show that ComPaSS is effective across various models.
% (3) We show that VLMs are more efficient at learning visual-related commonsense knowledge, and highlight the effectiveness of visual information for specific types of commonsense.


% \begin{figure*}[t]
%     \centering
%     \begin{subfigure}{0.27\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figure/penguin.pdf}
%         \caption{}
%         \label{fig:intro}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}{0.72\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figure/method.pdf}
%         \caption{}
%         \label{fig:method}
%     \end{subfigure}
%     \caption{(a) The text and image representation spaces of CLIP about `penguin'. (b) How ComPaSS works under multiple templates ensemble.}
%     \vskip -0.15in
% \end{figure*}

\begin{figure*}[t]
  \centerline{\includegraphics[width=1\linewidth]{figure/method_new.pdf}}
  \caption{How ComPaSS works on different tasks.}
  \label{fig:method}
  \vskip -0.15in
\end{figure*}

\section{Related Work}
% 加上和我们的关系
\subsection{CSPE Based on Internal Knowledge}

The sentence probability and perplexity computed by LMs can serve as indicators of commonsense plausibility, even in zero-shot settings~\cite{Trinh2018ASM, Feldman2019CommonsenseKM, liu2021constrained}. For LLMs with instruction-following capability, they can be directly prompted to judge whether a given input is consistent with commonsense or not~\cite{zhao2024large}. Beyond directly judging plausibility, some methods~\cite{Jung2022MaieuticPL, Tafjord2022EntailerAQ} evaluate the plausibility of hypotheses by scoring the validity of entailment paths generated by the LLMs, i.e., the reasoning chains justifying `reasonable' or `unreasonable' conclusions, and selecting the final prediction based on the highest-scoring path. VERA~\cite{liu2023vera} adopts a discriminative approach, training a classification head to make predictions based on model representations, which fine-tunes LLMs on\textasciitilde7 million commonsense statements. In contrast, our approach also leverages internal knowledge from a discriminative perspective but does not require additional training.

\subsection{CSPE Based on External Knowledge}

Language models (LMs) may have insufficient or inaccurate knowledge, which led to some methods to incorporate external knowledge to better estimate commonsense plausibility. A typical approach is to augment the model's knowledge by retrieving relevant sentences from external sources~\cite{Zhang2021AlleviatingTK, Yu2022RetrievalAF}. Commonsense knowledge bases (KBs)~\cite{Speer2016ConceptNet5A, sap2019atomic, Hwang2020COMETATOMIC2O} store extensive commonsense knowledge, enabling the extraction of relevant subgraphs to evaluate sentence consistency with commonsense~\cite{Choi2022ALBERTWK}. To alleviate the coverage limitations of the KBs while leveraging the extensive knowledge encoded in LMs, COMET~\cite{Bosselut2019COMETCT} introduced a dynamic KB by pre-training LM on existing commonsense KBs. Methods that utilize this dynamic KB~\cite{Ghazarian2023ACCENTAA, Tian2023BOOSTHB} demonstrate improved generalization across various commonsense reasoning tasks.

\section{Task Definition}
Formally, given an input instance $x_i = (c; a^c_i)$ consisting of a context $c$ and a candidate information $a^c_i \in A$, where $A^c = \{a^c_1, a^c_2, ..., a^c_K\}$ denotes the context-dependent candidate set with size $K$, the task is to predict a plausibility score set $\mathcal{P}^c = \{ p^c_1, p^c_2, ..., p^c_K \}$ for all candidates, where each $p^c_i \in \mathbb{R}$ quantifies the plausibility of augmenting $c$ with $a^c_i$. The ground-truth scores are denoted as $\mathcal{G}^c = \{ g^c_1, g^c_2, ..., g^c_K \}$, where $g^c_i$ indicates the true score of $a^c_i$. Performance is measured by the correlation between $\mathcal{P}^c$ and $\mathcal{G}^c$.  

The input can take two specific forms: for \textit{attribute value ranking} task, the input is a structured triplet $x_i=(o, \text{has property }p; a^c_i)$. The context $c = (o, \text{has property }p)$, where $o$ is a common object and $p$ is a property. The candidate $a^c_i$ represents the $i$-th attribute value for the specified property. For the \textit{commonsense frame completion} task, the context $c=q$ is a free-form question, the input is a question-answer pair $x_i= (q; a^c_i)$, where $a^c_i$ is the $i$-th plausible answer to this question.


\section{ComPaSS}
Our method, ComPaSS, is a zero-shot approach for estimating commonsense plausibility. We demonstrate in Figure~\ref{fig:method} how this method works on different tasks. For each input, we first construct an anchor sentence (omitting the commonsense-related detail) and a candidate sentence (augmenting that detail). We then encode both sentences individually to obtain their semantic representations. Next, we calculate their semantic similarity, where the degree of semantic shift—inversely proportional to similarity—quantifies plausibility. 
% Our method, ComPaSS, is a zero-shot approach for estimating commonsense plausibility through discriminative semantic shift measurement. ComPaSS operates on the principle that plausible commonsense additions induce minimal semantic shifts. We demonstrate in Figure~\ref{fig:method} how this method works on different tasks. The framework comprises three stages: (1) Contextual augmentation to construct anchor sentence (omitting the commonsense-related detail) and candidate sentence (augmenting that detail); (2) Sentence encoding with pre-trained LMs/VLMs; (3) Semantic shift quantification via representation similarity.

\subsection{Constructing Sentences}

% 缩短这部分的篇幅

% \begin{table}[t]
% \begin{center}
% \begin{adjustbox}{max width=1.\linewidth}
% \begin{tabular}{l|c|c}
%     \toprule
%     Property &  Type & Templates \\
%     \midrule
%     \multirow{2}{*}{Color} & Anch. & A photo of a [$o$]. \\
%     & Cand. & A photo of a [$c$] [$o$].\\
%     \multirow{2}{*}{Shape} & Anch. & This is a [$o$]. \\
%     & Cand. & This is a [$o$] with [$c$] shape.\\
%     \multirow{2}{*}{Material} & Anc. & This is an image of a [$o$]. \\
%     & Cand. & This is an image of a [$o$] made of [$c$]. \\
%     \bottomrule
% \end{tabular}
% \end{adjustbox}
% \end{center}
% \caption{Examples of templates we used for constructing anchor (Anch.) and candidate (Cand.) sentences for triplet input. The templates for CoDa are the same as Color.}
% \label{tab:temp_exp}
% \vskip -0.15in
% \end{table}

% 以一个candidate为例，不用1~K了
% template缩写叫tpl
For each input context $c$ and the candidate to be evaluated $a^c_i$, we construct two types of sentences: an anchor sentence $s_\text{anchor}$ that contains only the base context $c$ while omitting target details, and a candidate sentence $s_\text{candi}$ that further incorporates commonsense-related information $a^c_i$. The construction process varies based on input type but follows a unified framework:
\begin{equation}
s_{\text{anchor}} = f_{\text{anchor}}(c, z_{\text{anchor}}),
\end{equation}
\begin{equation}
s_{\text{candi}} = f_{\text{candi}}(c, a^c_i, z_{\text{candi}}),
\end{equation}
where $f(\cdot) \in \{f_{\text{anchor}}(\cdot), f_{\text{candi}}(\cdot)\}$ denotes the construction function, and $z \in \{z_{\text{anchor}}, z_{\text{candi}}\}$ denotes task-specific templates or prompts. 

As illustrated in Figure~\ref{fig:method}, the framework is instantiated differently based on the input format: For \textit{triplet inputs}, we employ template-based construction, where $z$ represents a pre-defined template (see Appendix~\ref{app:temp}) and $f(\cdot)$ represents applying this template to generate a sentence. For \textit{question-answer pairs}, we query GPT-4~\cite{achiam2023gpt} for sentence transformation, where $z$ denotes the prompt (see Appendix~\ref{app:prompt}) and $f(\cdot)$ represents query GPT-4 using the specified prompt. Since questions cannot be directly converted into coherent statements, we use a blank space as a placeholder when constructing anchor sentences.

\subsection{Representing Sentences} 
\label{sec:rep_sentence}

Given anchor and candidate sentences, we encode them into dense semantic representations using a pre-trained model $\theta$, which can be either a LM or a VLM. For each sentence $s \in \{s_{\text{anchor}}, s_{\text{candi}}\}$, the model first processes the sentence along with special tokens (e.g., [CLS], [EOS], or others depending on the model architecture) and then outputs token hidden states:
\begin{equation} 
H = \theta(s) = \{h_0, h_1, ..., h_l \},
\end{equation}
where $l$ denotes the sequence length, including the special tokens. The final sentence representation $r \in \{r_{\text{anchor}}, r_{\text{candi}}\}$ is derived through architecture-specific strategies.

For encoder models, we use the hidden state of the designated semantic aggregation token as sentence representation. Some models (e.g., RoBERTa~\cite{liu2021robustly}) use the initial `[CLS]' token for sentence representation ($r=h_0$), while others (e.g., CLIP~\cite{radford2021learning}) utilize the final `[EOS]' token embedding ($r=h_l$). 

For decoder models, we use the hidden state of the last token as sentence representation $r=h_l$, which naturally encapsulates the accumulated context. Alternatively, PromptReps~\cite{zhuang2024promptreps} prompts the model to generate a new representative token at position $l+1$, using its hidden state as the sentence representation ($r=h_{l+1}$). We apply this strategy to models that are not enhanced by contrastive learning.

This architecture-aware representation strategy ensures ComPaSS's flexibility across different model backbones while maintaining optimal performance for each specific architecture.

% For models with an encoder architecture, we represent the entire sentence using the hidden state of the special token, such as `[CLS]' or `[EOS]'. Specifically, for a given sentence $s$ of length $l$, which is prepended with the [CLS] token, the model $\theta$ outputs $l+1$ token hidden states $H = \{h_0, h_1, ..., h_{l+1} \}$:
% \begin{equation} 
% H = \theta([CLS], s). 
% \end{equation}
% The final sentence representation $r$ is derived from $h_0$ or $\mathrm{avg}(H)$, depending on which one works better for a particular model.

% In the case of decoder models, sentence representation can be more flexible. One common approach is to use the hidden state of the last token in the sentence $s$, which naturally encapsulates the context built up throughout the sequence: 
% \begin{equation}
% H = \theta(s), 
% \end{equation}
% where $H = \{h_1, h_1, ..., h_l \}$ and $r$ is derived from $h_l$, i.e., the last token hidden state. Another alternative way is the PromptReps method~\cite{zhuang2024promptreps}, where the model is prompted to represent the input text using a single word. The hidden state of this generated word is then used as the sentence representation. 

% By selecting the appropriate sentence representation method based on the model architecture and pre-training strategy, ComPaSS is flexible and can adapt to different backbones.

\subsection{Ranking with Semantic Shifts}

We rank the candidate option $a^c_i$ by measuring how naturally it integrates into the context, quantified through semantic similarity between the anchor sentence representation $r_{\text{anchor}}$ and the candidate sentence representation $r_{\text{candi}}$. The underlying principle is that the more plausible the information, the smaller the semantic shifts it induces when added to the context, leading to higher semantic similarity. Formally, we define the commonsense plausibility score $p^c_i$ for each candidate $a^c_i$ as:
\begin{equation} 
p^c_i \propto \text{sim}(r_{\text{anchor}}, r_{\text{candi}}), 
\end{equation} 
where $\text{sim}(\cdot)$ denotes a similarity function (e.g., cosine similarity or dot product). Candidates are then ranked by their plausibility scores descendingly, with higher-ranked candidates representing more commonsense-consistent answers.

% To rank an answer $a_i$ with respect to the context, we compute the semantic similarity between the anchor sentence representation $r_{\text{anchor}}$ and the candidate sentence representation $r_{\text{candi}}$. A greater semantic similarity indicates a smaller semantic shift. In other words, the introduction of specific information (the answer or the attribute value) can be naturally integrated into the context without significantly altering the semantics of the sentence. This suggests a higher degree of plausibility in terms of commonsense. Therefore, the commonsense plausibility $p_i$ of $a_i^q$ or $a_i^p$ is defined as proportional to this similarity: 
% \begin{equation} 
% p_i = \text{sim}(r_{\text{anchor}}, r_{\text{candi}}), 
% \end{equation} 
% where $\text{sim}(r_0, r_i)$ denotes a similarity function (e.g., cosine similarity or dot product). The larger the value of $p_i$, the higher the rank assigned to $a_i^q$ or $a_i^p$.

\subsection{Discussion of Applicable LMs}
This paragraph discusses the differences in applicable LMs between ComPaSS and generative methods based on likelihoods and verbalization. ComPaSS can utilize both encoder and decoder style models as long as they can yield reasonable sentence representations. Likelihood-based approaches can also leverage these two types of LMs. Candidate likelihoods can be estimated based on masked/next token prediction for encoders and decoders respectively. In contrast, verbalization-based approaches require LLMs--decoder-only LMs--to answer the plausibility estimation questions. This indicates the broader applicability of ComPaSS.

\section{Experimental Setup}
\subsection{Datasets}

% \begin{table}
% \begin{center}
% \begin{adjustbox}{max width=1.\linewidth}
% \begin{tabular}{l|cc|cc|cc}
%     \toprule
%      & \multicolumn{2}{c}{color} & \multicolumn{2}{|c|}{shape} & \multicolumn{2}{c}{material} \\
%      & obj & cand & obj & cand & obj & cand \\
%     \midrule
%     CoDa & 521  & 11 & - & - & - & - \\
%     ViComTe & 2,877 & 12 & 706 & 12 & 1,423 & 18 \\
%     \bottomrule
% \end{tabular}
% \end{adjustbox}
% \caption{Number of objects (obj) and attribute candidates (cand) in CoDa and ViComTe.}
% \label{tab:data}
% \end{center}
% \vskip -0.15in
% \end{table}

We evaluate methods through two types of fine-grained commonsense plausibility estimation (CSPE) tasks, where candidates should be ranked based on commonsense plausibility. These tasks are carefully chosen to comprehensively evaluate methods across varying input formats (from structured triplets to free-form text) and commonsense knowledge levels (from specific attribute knowledge to general commonsense knowledge).

\subsubsection{Structured Attribute Knowledge}

\textbf{Color Dataset (CoDa)}~\footnote{\url{https://github.com/nala-cub/coda}}~\cite{paik2021world} is a human-annotated dataset used for attribute value ranking, which provides color distributions for commonly recognized objects. It contains 521 objects, each with 11 candidate color attributes.

\textbf{Visual Commonsense Tests (ViComTe)}~\footnote{\url{https://github.com/ChenyuHeidiZhang/VL-commonsense}} ~\cite{zhang2022visual} is another dataset used for attribute value ranking, which derived from the multimodal dataset, i.e., Visual Genome~\cite{krishna2017visual}. It offers attribute value distributions across a broader set of properties, including color, shape, and material. It contains 2,877 objects with 12 candidate color attributes, 706 objects with 12 candidate shape attributes, and 1,423 objects with 18 candidate material attributes.

\subsubsection{Free-form General Knowledge}

\textbf{Commonsense Frame Completion (CFC)}~\footnote{\url{https://github.com/qxc101/PROBEVAL_CFC/}} ~\cite{cheng-etal-2024-every} is a dataset designed to evaluate implicit commonsense reasoning, which consists of questions accompanied by multiple plausible answers with human-annotated preference scores. It requires models to make probabilistic judgments about answer plausibility. The evaluation protocol employs a probabilistic framework that measures how well a model's predicted answer distribution aligns with human preferences. As the test set is not public, we use the validation set containing 55 questions for zero-shot evaluation. 

% Objects in these two datasets are further divided into three groups based on their attribute distribution: single, multi, and any. The `single' group includes objects with one typical attribute (e.g., the color of snow). The `multi' group consists of objects whose attributes are primarily distributed among the top four attribute values (e.g., the color of a penguin). The `any' group represents objects with a broader attribute distribution (e.g., the color of a T-shirt). 

% They provide attribute distributions for common objects. Additionally, they include data for binary comparisons, i.e., (object, positive attribute value, negative attribute value). Objects in the dataset are further divided into three groups based on their attribute distribution: single, multi, and any. The `single' group includes objects with one typical attribute (e.g., the color of snow). The `multi' group consists of objects whose attributes are primarily distributed among the top four attribute values (e.g., the color of a penguin). The `any' group represents objects with a broader attribute distribution (e.g., the color of a T-shirt). 

\subsection{Evaluation Metrics}
\textbf{Spearman's rank correlation coefficient $\rho$}: We choose this as the primary metric following CoDa and ViComTe. It measures the correlation between the predicted ranks of candidates and their ground-truth ranks, focusing on the relative ordering rather than exact values. This emphasis on relative ordering aligns with the nature of commonsense plausibility assessment, where the exact probability values are less important than correctly identifying more plausible options over less plausible ones.
% It is computed as: 
% \begin{equation} 
% \rho = 1 - \frac{6 \sum_{i=1}^{K} (p_i - q_i)^2}{K(K^2 - 1)}, 
% \end{equation} 
% where $p_i$ and $q_i$ are the predicted and true ranks of candidate $c_i$. 
A $\rho$ value of 1 indicates perfect correlation, 0 indicates no correlation, and -1 indicates perfect negative correlation.

% 解释binary comparisons
\textbf{Accuracy}: CoDa and ViComTe include binary comparison tasks where each object is paired with two attribute values, with one being more plausible than the other. The model need to rank the more plausible value higher. We use accuracy as the evaluation metric, which measures the proportion of correct rankings. This metric is particularly suitable for cross-attribute comparisons as it is unaffected by variations in the number of candidates, unlike the Spearman's rank correlation coefficient.

\subsection{Methods for Comparison}

\subsubsection{ComPaSS with Various Backbones}
We evaluate ComPaSS across diverse model architectures to assess its adaptability: 

For LMs, we consider various open-source models, including RoBERTa-Large~\cite{liu2021robustly} (RoBERTa), a widely-used encoder-only LM, along with two decoder-only LLMs, Mistral-7B-Instruct~\cite{jiang2023mistral} (Mistral) and Qwen2-7B-instruct~\cite{qwen2} (Qwen2), both demonstrating strong instruction-following capabilities. We also evaluate their \textbf{contrastive learning-enhanced} variants, i.e., sup-SimCSE-RoBERTa-Large~\cite{gao2021simcse} (RoBERTa$_\text{w/ CL}$), E5-Mistral-7B-Instruct~\cite{wang2023improving, wang2022text} (Mistral$_\text{w/ CL}$) and gte-Qwen2-7B-instruct~\cite{li2023towards} (Qwen2$_\text{w/ CL}$). 

For VLMs, we test CLIP-ViT-L/14~\cite{radford2021learning} (CLIP), a multimodal representation model trained on image-text pairs using \textbf{contrastive learning}, which aligns semantically similar images and text into closely matching representations. We also consider its advanced variant EVA-CLIP-8B~\cite{sun2023eva} (EVA-CLIP) with improved performance.

\subsubsection{Baselines}
We compare against two categories of baseline methods:

Commonsense models (CSMs): These models are specifically designed for modeling commonsense knowledge: COMET-Atomic-2020-Bart~\cite{Bosselut2019COMETCT} (COME-Atomic) is a commonsense LM pre-trained on commonsense KBs. COMET is suitable for processing triple input, which can generate a probability score for each candidate. ACCENT~\cite{Ghazarian2023ACCENTAA} assesses the commonsense plausibility of a sentence by first extracting structured tuples and then scoring them based on their compatibility with a commonsense KB. VERA-T5-XXL~\cite{liu2023vera} (VERA-T5) is trained on \textasciitilde7M commonsense statements and can directly estimate the commonsense plausibility of statements. 

Language models (LMs): We evaluate all open-source LMs used as the backbone of ComPaSS with both likelihood-based and verbalization-based approaches. For the \textit{likelihood-based} method, the plausibility of a sentence is determined by the probability of predicting each token in the sentence sequentially, normalized by sentence length. A higher probability indicates greater plausibility. In the case of \textit{verbalization-based} method, pre-trained language models are prompted in natural language (see Appendix~\ref{app:verb_prompt})  to rank all candidate responses based on their plausibility. We also test closed-source LLMs including gpt-3.5-turbo-0125~\cite{OpenAI2022IntroChat} (GPT-3.5) and gpt-4-0125-preview~\cite{achiam2023gpt} (GPT-4), the latter introduces multi-modality and has superior capabilities.


\begin{table*}[t]
\begin{center}
\begin{adjustbox}{max width=1.\linewidth}
\begin{tabular}{clcccccc}
    \toprule
    & \textbf{Model (\#Inference Parameters)} & \textbf{CoDa} & \textbf{Color} & \textbf{Shape} & \textbf{Material} & \textbf{CFC}\\
    \midrule 
    \multicolumn{7}{c}{Baselines} \\
    \midrule
    \multirow{3}{*}{\rotatebox{90}{CSM}} & ACCENT (440M) & 10.07 & 10.35 & -2.10 & 16.99 & 35.04\\
    & COMET-Atomic (440M) & 22.91 & 26.98 & 40.44 & 25.72 & - \\
    & VERA-T5  (5B) & 58.93 & 45.08 & 30.31 & 33.51 & 45.81 \\
    \cline{2-7}
    \multirow{8}{*}{\rotatebox{90}{LM}} & RoBERTa$+\text{likelihood}$ (355M) & 24.37 & 33.63 & 36.12 & 24.23 & 42.46\\
    & RoBERTa$_\text{w/ CL}$$+\text{likelihood}$ (355M) & 23.36 & 31.51 & 26.69 & 22.23 & 38.03 \\
    
    & Mistral$+\text{verbal.}$ (7B) & 46.64 & 38.63 & 30.46 & 36.34 & 32.06\\
    & Mistral$+\text{likelihood}$ (7B) & 51.30 & 34.31 & 26.70 & 37.03 & 47.98 \\
    & Mistral$_\text{w/ CL}$$+\text{likelihood}$ (7B) & 25.70 & 4.72 & 18.81 & 5.96 & 35.46 \\
    
    & Qwen2$+\text{verbal.}$ (7B) & 57.40 & 41.59 & 38.30 & 36.76 & 29.32 \\
    & Qwen2$+\text{likelihood}$ (7B) & 50.25 & 40.99 & 32.52 & 37.13 & 45.10 \\
    
    % & gte-Qwen2-7B-instruct$+\text{verbal.}$ (7B) & 11.12 & 15.28 & -24.21 & 0.45 & 21.39 \\
    & Qwen2$_\text{w/ CL}$$+\text{likelihood}$ (7B) & 49.65 & 41.75 & 32.80 & 37.30 & 43.00 \\
    % & GPT-3.5 (N/A) & 63.96 & 52.51 & \underline{43.36} & 37.38 \\
    % & GPT-4 (N/A) & \underline{65.53} & \underline{54.42} & 42.91 & 36.00 \\
    % \midrule
    \hline\hline
    \multicolumn{7}{c}{ComPASS} \\
    \midrule
    \multirow{3}{*}{\rotatebox{90}{LM}} 
    % & RoBERTa (355M) & 24.63 & 22.68 & 26.77 & 19.93 \\
    % & \quad + unsup-CL & 32.67 & 32.00 & 42.18 & 31.12 \\
    & RoBERTa$_\text{w/ CL}$ (355M) & 44.59 & 38.92 & 42.92 & 33.55 & 44.46 \\
    % & Mistral (7B) & 53.14 & 39.07 & 39.75 & 31.88 \\
    & Mistral$_\text{w/ CL}$ (7B) & 58.54 & 42.20 & 43.75 & \textbf{38.77} & \textbf{49.01}\\
    & Qwen2$_\text{w/ CL}$ (7B) & \underline{59.16} & 44.61 & \underline{47.51} & 38.49 & 46.41 \\
    \cline{2-7}
    \multirow{2}{*}{\rotatebox{90}{VLM}} 
    & CLIP (124M) & 58.10 & \underline{45.55} &45.82 & 33.56 & 35.13 \\
    & EVA-CLIP (695M) & \textbf{62.87} & \textbf{51.73} & \textbf{48.05} & \underline{38.67} & 41.46\\
    \bottomrule
\end{tabular}
\end{adjustbox}
\end{center}
\caption{Spearman's rank correlation coefficient $\rho$ between the predicted ranks of candidates and their ground-truth on CoDa, ViComTe (Color, Shape, and Material), and CFC, shown in percentage. The \textbf{best} and \underline{second best} results are highlighted in bold and underlined, respectively. `$+\text{verbal.}$' indicates using the verbalization-based method.}
\label{tab:overall_result}
\vskip -0.15in
\end{table*}


\subsection{Implementation Details}
For closed-source models, we obtain the generated results via their official APIs. For open-source models, all experiments are conducted on a single NVIDIA A800 80G GPU. Notably, all experiments are carried out in a zero-shot or in-context few-shot setting. For ACCENT, we set the beam number to $10$ as specified in the original paper. 
% For baseline LMs, we employ two detection methods: One is the likelihood-based method ($+\text{likelihood}$). This approach involves using the predicted probability of the masked word or the perplexity of the sentence. The other one is the verbalization-based method ($+\text{verbal.}$). Here, we prompt the model to generate answers based on instructions. 
When testing on the CFC dataset using the verbalization method, we sample the model $100$ times for each question with a temperature of $0.7$. 



% \begin{table*}[t]
% \begin{center}
% % \begin{adjustbox}{max width=1.\linewidth}
% \begin{tabular}{clccccc}
%     \toprule
%     & Model & Parameters & CoDa & Color & Shape & Material \\
%     \midrule
%     \multirow{5}{*}{\rotatebox{90}{Baselines}} & ACCENT & 440M & 10.07 & 10.35 & -2.10 & 16.99 \\
%     & COMET & 440M & 22.91 & 26.98 & 40.44 & 25.72 \\
%     & VERA & 5B & 58.93 & 45.08 & 30.31 & 33.51 \\
%     \cline{2-7}
%     & GPT-3.5 & - & 63.96 & 52.51 & \underline{43.36} & \underline{37.38} \\
%     & GPT-4 & - & \underline{65.53} & \underline{54.42} & 42.91 & 36.00 \\
%     \midrule
%     \multirow{4}{*}{\rotatebox{90}{LM}} & RoBERTa & 355M & 20.02 & 34.62 & 36.41 & 24.83 \\
%     & \quad + ComPaSS & 355M & 24.63 & 22.68 & 26.77 & 19.93 \\
%     & \quad + unsup-CL \& ComPaSS & 355M & 32.67 & 32.00 & 42.18 & 31.12 \\
%     & \quad + sup-CL \& ComPaSS & 355M & 44.59 & 38.92 & 42.92 & 33.55 \\
%     \midrule
%     \multirow{3}{*}{\rotatebox{90}{LLM}} & Mistral & 7B & 46.64 & 38.63 & 30.46 & 36.34 \\
%     & \quad + ComPaSS & 7B & 53.14 & 39.07 & 39.75 & 31.88 \\
%     & \quad + CL \& ComPaSS & 7B & 53.72 & 39.81 & 42.90 & \textbf{38.83} \\
%     \midrule
%     \multirow{2}{*}{\rotatebox{90}{VLM}} & CLIP & 124M & 57.77 & 45.30 & 45.35 & 33.20 \\
%     & EVA-CLIP & 695M & \textbf{62.51} & \textbf{50.77} & \textbf{48.12} & 38.68 \\
%     \bottomrule
% \end{tabular}
% % \end{adjustbox}
% \end{center}
% \caption{Test results on CoDa and ViComTe (Color, Shape, and Material). The best results among baselines and with ComPaSS are underlined and bolded, respectively. All Spearman's $\rho$ are scaled by a factor of 100. For each method, we experiment with several templates and report the results of the one template that achieves the overall best performance. `+ CL' denotes the models enhanced with contrastive learning. `unsup' and `sup' refer to unsupervised and supervised training. We also show the parameters of the text module, which is used for ranking.}
% \label{tab:overall_result}
% \end{table*}


% unified best template
% average score
% average representation



\section{Results and Analysis}

% baseline和compass比，以及baseline内部
% compass内部比（多模态）
% 对比学习好，sup对比学习更好。简单提一下对比学习对于baseline不行，prob能看，verb极差（在附录
% 参数量

\subsection{Overall Results}

The overall experimental results comparing baseline methods with our approach are presented in Table~\ref{tab:overall_result}, which reveals several key findings:

\textbf{ComPaSS achieves the best performance compared to baselines.} Further comparison between RoBERTa, Mistral, and Qwen2, with and without ComPaSS, shows a consistent improvement when ComPaSS is applied. This validates our method's architecture-agnostic effectiveness. Notably, even VERA, which was specifically fine-tuned for CSPE, achieves only comparable performance to ComPaSS-enhanced models. 
% This suggests that simply training alone may be insufficient for fine-grained plausibility assessment. 
Comparing the performance of different methods on LMs in the baseline, we find that verbalization-based methods fail to consistently outperform likelihood-based approaches, even when applied to generative models. This limitation highlights the challenges such methods face in making fine-grained distinctions required for precise plausibility estimation.

\begin{table}[t]
\begin{center}
\begin{adjustbox}{max width=1.\linewidth}
\begin{tabular}{lcccc}
    \toprule
    Method &  CoDa & Color & Shape & Material \\
    \midrule
    likelihood & 24.37 & 33.63 & 36.12 & 24.23 \\
    ComPaSS & 24.63 & 22.68 & 26.77 & 19.93 \\
    \quad w/ unsup-CL & 32.67 & 32.00 & 42.18 & 31.12 \\
    \quad w/ sup-CL & 44.59 & 38.92 & 42.92 & 33.55 \\
    % E5-Mistral & 53.72 & 39.81 & 42.90 & 38.83 \\
    % -CL & 53.14 & 39.07 & 39.75 & 31.88 \\
    \bottomrule
\end{tabular}
\end{adjustbox}
\end{center}
\caption{Performance of different Roberta variants. By default we use the vanilla RoBERTa. `w/ unsup-CL' and `w/ sup-CL' denote RoBERTa pre-trained with unsupervised and supervised contrastive learning, respectively.}
\label{tab:wo_cl}
\vskip -0.15in
\end{table}

% \textbf{ComPaSS consistently enhances performance across various backbones.} A direct comparison between RoBERTa, Mistral, and Qwen2, with and without ComPaSS, shows a consistent improvement when ComPaSS is applied. This indicates that ComPaSS effectively boosts the performance of models across different architectures. Notably, our approach also demonstrates that CLIP, which was previously deemed unsuitable for commonsense reasoning without fine-tuning~\cite{paik2021world, zhang2022visual}, can still harness valuable knowledge for commonsense reasoning without additional training. This highlights the model's latent capacity to support commonsense inference effectively.

\begin{figure*}[t]
    \centering
    \begin{subfigure}{0.245\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/Coda.pdf}
        % \caption{Caption 1}
        % \label{fig:sub1}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.245\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/Color.pdf}
        % \caption{Caption 2}
        % \label{fig:sub2}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.245\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/Shape.pdf}
        % \caption{Caption 3}
        % \label{fig:sub3}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.245\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/Material.pdf}
        % \caption{Caption 3}
        % \label{fig:sub3}
    \end{subfigure}
    \caption{Binary classification accuracy of models with ComPaSS on different groups. }
    \label{fig:diff_type}
    \vskip -0.15in
\end{figure*}

\begin{figure*}[t]
    \centering
    \begin{subfigure}{0.43\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/phrase_sentence.pdf}
        % \caption{Caption 1}
        % \label{fig:phrase_sentence}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.54\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/ensemble.pdf}
        % \caption{Caption 2}
        % \label{fig:ensemble}
    \end{subfigure}
    \caption{ComPaSS performance with different template types and template ensemble settings.}
    \label{fig:diff_type}
    \vskip -0.15in
\end{figure*}

\textbf{VLMs demonstrate superior effectiveness in learning visual-related commonsense knowledge.} Comparing the ComPaSS methods based on various backbones, we find VLMs exhibit particular strength in visual attribute ranking, with EVA-CLIP achieving the highest scores on CoDa (62.87), Color (51.73), and Shape (48.05), significantly outperforming even 7B parameter LLMs. This performance gap persists despite the LLMs' access to large-scale text corpora and additional parameters, underscoring the unique value of visual supervision. This performance gap highlights the limitations of text-only training, as even extensive textual data and additional parameters cannot fully compensate for the lack of visual grounding, which underscores the importance of multimodal learning for comprehensive commonsense understanding.

\textbf{Discriminative approaches may offer a more parameter-efficient pathway compared to generative methods.} Our experiments reveal that encoder-only models with millions of parameters like RoBERTa and CLIP-series models achieve comparable or even superior results to much larger decoder-only models (with billions of parameters) when combined with ComPaSS. This suggests that our discriminative method effectively leverages the semantic representation strengths of encoder models, which are generally more parameter-efficient than generative models. By focusing on representation-level semantics rather than token generation, ComPaSS aligns closely with the pre-training objectives of encoder models, maximizing their representation power.

% \textbf{LLMs do not fully leverage their internal knowledge during generation.} 
% While LLMs excel at generative tasks, our results suggest that methods like verbalization ($\ddagger$) do not consistently outperform other techniques for fine-grained commonsense plausibility estimation. This reveals a critical gap between LLMs' internal knowledge and their generated outputs. Moreover, previous work has primarily focused on likelihood-based or verbalization-based methods for extracting commonsense knowledge, our experiments show that much of this knowledge is embedded in the semantic representations. By developing specialized methods, we can unlock even greater potential in LLMs.

\textbf{The ability to discern semantic nuances in sentence representations is crucial for the performance of ComPaSS.} As shown in Table~\ref{tab:wo_cl}, experiments with different RoBERTa variants reveal that applying ComPaSS to vanilla RoBERTa, which has weaker representation capabilities, leads to performance degradation. However, incorporating contrastive learning significantly improves performance, with even unsupervised contrastive training yielding substantial gains. Contrastive pre-training enables even subtle plausibility distinctions to manifest as measurable shifts in embedding space, making it essential to the performance of ComPaSS.
% These results highlight that representation quality, rather than additional supervised training data, constitutes the primary performance driver. This dependency stems from ComPaSS's core mechanism: since it estimates commonsense plausibility via semantic similarity, the discriminative power of sentence representations fundamentally determines its effectiveness. 

% We analyze the performance of Mistral+ComPaSS (hereafter referred to as Mistral) and EVA-CLIP across different settings and data types to further explore the differences when ComPaSS is applied to various model architectures.

\begin{table}[t]
\begin{center}
\begin{tabular}{lcccc}
    \toprule
    Model &  CoDa & Color & Shape & Material \\
    \midrule
    GPT-3.5 & 94.05 & 92.25 & 90.08 & 89.60 \\
    GPT-4 & 94.63 & \textbf{93.29} & 89.24 & 88.76 \\
    \midrule
    Mistral$_\text{w/ CL}$ & 94.97 & 86.06 & 91.50 & \textbf{91.27} \\
    Qwen2$_\text{w/ CL}$ & 94.71 & 86.79 & 94.04 & 90.42 \\
    EVA-CLIP & \textbf{95.39} & \textbf{93.29} & \textbf{94.33} & 90.79 \\
    \bottomrule
\end{tabular}
\end{center}
\caption{Binary comparison accuracy on CoDa and ViComTe. The best results are highlighted in bold. All results are shown in percentage. Both Mistral and EVA-CLIP use the ComPaSS method.}
\label{tab:acc_result}
\vskip -0.15in
\end{table}

\subsection{Further Analyses}

\begin{figure*}[t]
  \centerline{\includegraphics[width=0.9\linewidth]{figure/case_new.pdf}}
  \caption{The ranking of sheep colors by humans and different models, along with corresponding images from the physical world (from Google).  The `*' in the upper right represents the model with ComPaSS method.}
  \label{fig:case}
  \vskip -0.15in
\end{figure*}

\subsubsection{Comparisons to Closed-source Models}
We extend our evaluation to include state-of-the-art closed-source models, with results presented in Table~\ref{tab:acc_result}. Notably, our method outperforms even GPT-4 across multiple tasks, demonstrating its effectiveness in fine-grained CSPE. This performance gap further highlights the limitations of verbalization-based approaches in capturing subtle distinctions required for precise plausibility estimation. 

% \subsubsection{Sentence Representation Quality is Crucial for ComPaSS}

% As ComPaSS estimates commonsense plausibility through semantic similarity measurement, the quality of sentence representations fundamentally determines its effectiveness. As shown in Table~\ref{tab:wo_cl}, applying ComPaSS to vanilla RoBERTa, which has weaker representation capabilities, yields even performance degradation. Incorporating contrastive learning to enhance the sentence representation ability can substantially improve their effectiveness in our method. Notably, using the unsupervised trained model (unsup-simCSE) still leads to performance Improvement. This confirms that representation quality, rather than additional training data, constitutes the primary performance driver.

\subsubsection{Granular Analysis of Attribute Types}
We analyze binary comparison results on CoDa and ViComTe across three attribute groups: \textit{single}: includes objects with one dominant attribute value (e.g., snow's color), \textit{multi}: includes objects with attributes mainly distributed among the top four values (e.g., a penguin's color), and \textit{any}: includes objects with a broader attribute distribution (e.g., a T-shirt's color). As shown in Figure~\ref{fig:diff_type}, VLMs demonstrate particular strength in the single group. This advantage primarily stems from how visual information overcomes reporting bias in textual data. For objects in the \textit{single} category, their most common attribute is often not explicitly mentioned in text due to its widespread acceptance as common knowledge. However, these attributes are consistently and explicitly depicted in visual data. This inherent visual grounding allows VLMs to capture stereotypical attributes more effectively than text-based LLMs.

\subsubsection{Effect of Template Format}
We investigate the importance of sentence-level context in semantic shift measurement by comparing two approaches: \textit{word collocation} comparison (e.g., `penguin' and `black penguin') and \textit{full sentence construction} (e.g., `There is a penguin' and `There is a black penguin'). As shown in Figure~\ref{fig:diff_type}(a), sentence-level inputs consistently outperform word-level comparisons for both LLMs and VLMs. This performance gap underscores the importance of complete sentence construction for ComPaSS, as sentence-level inputs better align with models' pre-training data formats. 
% The benefit is particularly pronounced for LLMs, which are typically trained on document-level texts and thus better utilize full sentence semantics. 

\subsubsection{Template Ensemble Methods}

We investigate three template utilization strategies: The \textit{single-optimal ensemble} approach uses the unified best-performing template, serving as an implicit ensemble. For explicit ensemble methods, \textit{score-level ensemble} averages prediction scores across multiple templates, and \textit{representation-level ensemble} fuses sentence representations from several templates before computing the final score. As shown in Figure~\ref{fig:diff_type} (b), both explicit ensemble strategies significantly improve LLM performance, with score-level ensemble showing more consistent gains. However, VLM shows limited improvement from ensemble methods, likely due to their simpler pre-training data structure. This contrast highlights LLMs' sensitivity to linguistic variations and their ability to benefit from diverse syntactic structures.

% We analyzed the impact of using templates ensemble. Specifically, the score ensemble combines the scores from different templates to generate the final prediction. The hidden ensemble fuses the sentence representations before calculating the final score. As shown in Figure~\ref{fig:diff_type} (b), both ensemble strategies significantly improve performance for LLMs, particularly in the CoDa and Color tasks, but offer limited benefit for VLM, i.e., EVA-CLIP. This may be because LLMs rely heavily on textual input and benefit from the diverse linguistic contexts provided by multiple templates. In contrast, the sentence structure in VLM pre-training data is relatively simpler, resulting in more consistent performance even with a single template.

\subsection{Case Study}
We use the classic `black sheep problem' to intuitively explain why ComPaSS is effective. Since `black sheep' is an idiom, one is much more likely to mention a `black sheep' than to specify the color of a sheep. Such reporting bias confuses the LMs that learn knowledge through probabilistic modeling. As shown in Figure~\ref{fig:case}, GPT-3.5 and GPT-4 both overestimate the probability of `black' being the color of sheep even though sheep in black are rare. In contrast, our approach relies on semantic rather than probabilistic likelihood is able to distinguish between the linguistic meaning and the visual recognition of `a black sheep', resulting in a more accurate estimation of the sheep's color. In addition, VLM calibrates the color distribution well by incorporating visual information.


\section{Conclusion}

We introduce ComPaSS, a discriminative framework for fine-grained commonsense plausibility estimation via semantic shift measurement. By leveraging the idea that plausible commonsense augmentations cause minimal semantic deviation, ComPaSS offers a generalizable approach for various tasks and model architectures. Our experiments show that discriminative methods outperform generative approaches in capturing nuanced plausibility distinctions, with ComPaSS consistently surpassing likelihood-based and verbalization-based baselines. Vision-language models also excel on visually-grounded commonsense tasks, addressing reporting bias through multimodal alignment. Finally, we emphasize the role of contrastive pre-training in improving semantic representation quality, directly enhancing plausibility estimation accuracy. Overall, ComPaSS highlights the value of utilizing semantic embeddings to extract commonsense knowledge from pre-trained models.


% In this work, we propose ComPaSS, a novel discriminative method designed to estimate fine-grained commonsense plausibility via sentence semantics. By leveraging the semantic shifts between anchor and candidate sentences, ComPaSS allows for a more nuanced understanding of how plausible a detailed information is for a given context. Our extensive experiments demonstrated that ComPaSS consistently enhances performance across various model architectures, including LMs and VLMs, without fine-tuning requirements. We also find that multimodal models like EVA-CLIP, which integrate visual information, show higher efficiency in capturing visual-related commonsense knowledge than LMs. This work demonstrates that a substantial amount of commonsense knowledge is embedded in semantic representations, and through the design of effective methods, this knowledge can be more effectively extracted and utilized by models.

% Additionally, the importance of sentence representation quality was highlighted, with contrastive learning significantly improving the effectiveness of models with weaker sentence encoding capabilities. Through ComPaSS, we show that leveraging semantic shifts and integrating visual information are promising directions for commonsense plausibility estimation.

\section{Limitations and Ethical Considerations}
ComPaSS faces challenges in making absolute pointwise judgments. The method's reliance on semantic shift measurement inherently provides comparative assessments rather than definitive plausibility scores. This limitation stems from the difficulty in establishing absolute semantic distance thresholds for plausibility classification. Future work could explore calibration techniques to bridge this gap. In addition, for attribute value ranking task, our method relies on predefined templates to construct sentences for objects and candidate attributes. Automating template generation could be an important avenue for future improvement. 

As our method relies on LLMs and VLMs, it inherits potential biases present in the training data. These biases, whether related to societal stereotypes or uneven distribution of information across certain attributes, could affect the model's judgment in ranking attribute plausibility. Consequently, our method may inadvertently perpetuate or amplify these biases, especially in scenarios where the model's understanding of an attribute is skewed by biased representations in the data. Addressing these biases is an important avenue for future work.






% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}

\appendix

\section{Templates for Sentence Construction}
\label{app:temp}

The templates we used to construct anchor sentences and candidate sentences of different property are shown in Table~\ref{app:temp}.

\begin{table*}[t]
\begin{center}
\begin{tabular}{l|c|c}
    \toprule
    Property &  Templates for anchor & Templates for candidate \\
    \midrule
    \multirow{15}{*}{Color} & A photo of a [$o$].   &   A photo of a [$c$] [$o$].\\
    & A picture of a [$o$]. &   A picture of a [$c$] [$o$].  \\
    & An image of a [$o$].   &   An image of a [$c$] [$o$]. \\
    & An image of a [$o$].   &   An image of a [$o$] which is [$c$]. \\
    & There is an image of a [$o$].   &   There is an image of a [$c$] [$o$]. \\
    & There is a photo of a [$o$].  &   There is a photo of a [$c$] [$o$]. \\
    & There is a picture of a [$o$].  &   There is a picture of a [$c$] [$o$]. \\
    & There is an image of a [$o$].   &   There is an image of a [$o$] which is [$c$]. \\
    & There is a photo of a [$o$].   &   There is a photo of a [$o$] which is [$c$]. \\
    & It is an image of a [$o$].   &   It is an image of a [$o$] which is [$c$]. \\
    & It is a photo of a [$o$].   &   It is a photo of a [$o$] which is [$c$]. \\
    & There is a [$o$].   &   There is a [$o$] in [$c$]. \\
    & There is a [$o$].   &   There is a [$o$] which is [$c$]. \\
    & Everyone knows [$o$].   &   Everyone knows that [$o$] is [$c$]. \\
    & Everyone knows [$o$].   &   Everyone knows that [$o$] is [$c$]. \\
    \midrule
    
    \multirow{17}{*}{Shape} & This is a [$o$]. & This is a [$o$] with [$c$] shape. \\
    & There is a [$o$]. & There is a [$c$] [$o$]. \\
    & There is a [$o$]. & There is a [$o$] which shape is [$c$]. \\
    & It is an image of a [$o$]. & It is an image of a [$o$] which shape is [$c$]. \\
    & There is an image of a [$o$]. & It is an image of a [$o$] which shape is [$c$]. \\
    & There is an image of a [$o$]. & There is an image of a [$c$] [$o$]. \\
    & There is a picture of a [$o$]. & There is a picture of a [$c$] [$o$]. \\
    & There is a picture of a [$o$]. & There is an picture of a [$o$] which shape is [$c$]. \\
    & There is a picture of a [$o$]. & There is an picture of a [$c$] [$o$]. \\
    & This is a picture of a [$o$]. & This is a picture of a [$o$] has [$c$] shape. \\
    & A picture of a [$o$]. & A picture of a [$o$] has [$c$] shape. \\
    & An image of a [$o$]. & An image of a [$c$] [$o$]. \\
    & A photo of a [$o$]. & A photo of a [$c$] [$o$]. \\
    & A picture of a [$o$]. & A picture of a [$c$] [$o$]. \\
    & [$o$] is of shape . & [$o$] is of shape [$c$]. \\
    & The shape of [$o$]. & The shape of [$o$] can be [$c$]. \\
    & The shape of the [$o$]. & The shape of the [$o$] is [$c$]. \\
    \midrule
    \multirow{17}{*}{Material} & This is an image of a [$o$]. & This is an image of a [$o$] made of [$c$]. \\ 
    & This is an image of a [$o$]. & This is an image of a [$o$] which made from [$c$]. \\
    & This is an image of a [$o$]. & This is an image of a [$o$] which made of [$c$]. \\
    & This is a photo of a [$o$]. & This is a photo of a [$o$] made of [$c$]. \\ 
    & This is a picture of a [$o$]. & This is a picture of a [$o$] made of [$c$]. \\ 
    & This is a picture of a [$o$]. & This is a picture of a [$o$] which made of [$c$]. \\ 
    & It is a picture of a [$o$]. & It is a picture of a [$o$] made of [$c$]. \\ 
    & A picture of a [$o$]. & A picture of a [$o$] which made from [$c$]. \\
    & A picture of a [$o$]. & A picture of a [$o$] which made of [$c$]. \\
    & A picture of a [$o$]. & A picture of a [$c$] [$o$]. \\
    & There is an image of a [$o$]. & There is an image of a [$c$] [$o$]. \\
    & There is a photo of a [$o$]. & There is an photo of a [$c$] [$o$]. \\
    & There is a picture of a [$o$]. & There is an picture of a [$c$] [$o$]. \\
    & An image of a [$o$]. & An image of a [$c$] [$o$]. \\
    & A photo of a [$o$]. & A photo of a [$c$] [$o$]. \\
    & A picture of a [$o$]. & A picture of a [$c$] [$o$]. \\
    \bottomrule
\end{tabular}
\end{center}
\caption{Templates we used for constructing anchor sentences and candidate sentences. The templates for CoDa are the same as Color.}
\label{tab:temp}
\end{table*}

\section{Prompt for Sentence Transformation}
\label{app:prompt}
The prompt we use for converting question-answer pair can be found in Figure~\ref{fig:prompt}.

\begin{figure*}[t]
  \centerline{\includegraphics[width=1\linewidth]{figure/para_prompt.pdf}}
  \caption{The prompt for converting question-answer pair into sentence. The blue part is the instruction, the green part is the 3-shot example, and the red part is the placeholder for the specific input.}
  \label{fig:prompt}
\end{figure*}


\section{Prompt for Verbalization-based Method}
\label{app:verb_prompt}
The prompt we use for the verbalization-based method can be found in Figure~\ref{fig:verb_prompt}.

\begin{figure*}[t]
  \centerline{\includegraphics[width=1\linewidth]{figure/verb_prompt.pdf}}
  \caption{The prompt for attribute value ranking task and commonsense frame completion task.}
  \label{fig:prompt}
\end{figure*}

\section{More Experimental Results}

Since not all models are compatible with all methods, we exclude the results of incompatible model-method combinations from the main text. The complete results are provided in Table~\ref{tab:all_result}. Notably, the results of Mistral$_\text{w/ CL}$ with the verbalization-based method is 0, as this model, trained via contrastive learning, has significantly lost its ability to follow instructions, preventing it from generating reasonable responses based on prompts.

\begin{table*}[t]
\begin{center}
\begin{adjustbox}{max width=1.\linewidth}
\begin{tabular}{clcccccc}
    \toprule
    & \textbf{Model (\#Inference Parameters)} & \textbf{CoDa} & \textbf{Color} & \textbf{Shape} & \textbf{Material} & \textbf{CFC}\\
    \midrule 
    \multicolumn{7}{c}{Baselines} \\
    \midrule
    \multirow{3}{*}{\rotatebox{90}{CSM}} & ACCENT (440M) & 10.07 & 10.35 & -2.10 & 16.99 & 35.04\\
    & COMET-Atomic-2020-Bart (440M) & 22.91 & 26.98 & 40.44 & 25.72 & - \\
    & VERA-T5-XXL  (5B) & 58.93 & 45.08 & 30.31 & 33.51 & 45.81 \\
    \cline{2-7}
    \multirow{8}{*}{\rotatebox{90}{LM}} & RoBERTa$+\text{likelihood}$ (355M) & 24.37 & 33.63 & 36.12 & 24.23 & 42.46\\
    & RoBERTa$_\text{w/ CL}$$+\text{likelihood}$ (355M) & 23.36 & 31.51 & 26.69 & 22.23 & 38.03 \\
    
    & Mistral$+\text{verbal.}$ (7B) & 46.64 & 38.63 & 30.46 & 36.34 & 32.06\\
    & Mistral$+\text{likelihood}$ (7B) & 51.30 & 34.31 & 26.70 & 37.03 & 47.98 \\
    & Mistral$_\text{w/ CL}$$+\text{verbal.}$ (7B) & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
    & Mistral$_\text{w/ CL}$$+\text{likelihood}$ (7B) & 25.70 & 4.72 & 18.81 & 5.96 & 35.46 \\
    
    & Qwen2$+\text{verbal.}$ (7B) & 57.40 & 41.59 & 38.3 & 36.76 & 29.32 \\
    & Qwen2$+\text{likelihood}$ (7B) & 50.25 & 40.99 & 32.52 & 37.13 & 45.10 \\
    
    & Qwen2$_\text{w/ CL}$$+\text{verbal.}$ (7B) & 11.12 & 15.28 & -24.21 & 0.45 & 21.39 \\
    & Qwen2$_\text{w/ CL}$$+\text{likelihood}$ (7B) & 49.65 & 41.75 & 32.8 & 37.3 & 43.00 \\
    % & GPT-3.5 (N/A) & 63.96 & 52.51 & \underline{43.36} & 37.38 \\
    % & GPT-4 (N/A) & \underline{65.53} & \underline{54.42} & 42.91 & 36.00 \\
    % \midrule
    \hline\hline
    \multicolumn{7}{c}{ComPASS} \\
    \midrule
    \multirow{3}{*}{\rotatebox{90}{LM}} 
    % & RoBERTa (355M) & 24.63 & 22.68 & 26.77 & 19.93 \\
    % & \quad + unsup-CL & 32.67 & 32.00 & 42.18 & 31.12 \\
    & RoBERTa$_\text{w/ CL}$ (355M) & 44.59 & 38.92 & 42.92 & 33.55 & 44.46 \\
    % & Mistral (7B) & 53.14 & 39.07 & 39.75 & 31.88 \\
    & Mistral$_\text{w/ CL}$ (7B) & 58.54 & 42.20 & 43.75 & \textbf{38.77} & \textbf{49.01}\\
    & Qwen2$_\text{w/ CL}$ (7B) & \underline{59.16} & 44.61 & \underline{47.51} & 38.49 & 46.41 \\
    \cline{2-7}
    \multirow{2}{*}{\rotatebox{90}{VLM}} 
    & CLIP (124M) & 58.10 & \underline{45.55} &45.82 & 33.56 & 35.13 \\
    & EVA-CLIP (695M) & \textbf{62.87} & \textbf{51.73} & \textbf{48.05} & \underline{38.67} & 41.46\\
    \bottomrule
\end{tabular}
\end{adjustbox}
\end{center}
\caption{Spearman's rank correlation coefficient $\rho$ between the predicted ranks of candidates and their ground-truth on CoDa, ViComTe (Color, Shape, and Material), and CFC, shown in percentage. The \textbf{best} and \underline{second best} results are highlighted in bold and underlined, respectively. `$+\text{likelihood}$' indicates using the likelihood-based method and `$+\text{verbal.}$' indicates using the verbalization-based method.}
\label{tab:all_result}
\end{table*}

\end{document}
