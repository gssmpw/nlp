\section{Related Work}
% 加上和我们的关系
\subsection{CSPE Based on Internal Knowledge}

The sentence probability and perplexity computed by LMs can serve as indicators of commonsense plausibility, even in zero-shot settings~\cite{Trinh2018ASM, Feldman2019CommonsenseKM, liu2021constrained}. For LLMs with instruction-following capability, they can be directly prompted to judge whether a given input is consistent with commonsense or not~\cite{zhao2024large}. Beyond directly judging plausibility, some methods~\cite{Jung2022MaieuticPL, Tafjord2022EntailerAQ} evaluate the plausibility of hypotheses by scoring the validity of entailment paths generated by the LLMs, i.e., the reasoning chains justifying `reasonable' or `unreasonable' conclusions, and selecting the final prediction based on the highest-scoring path. VERA~\cite{liu2023vera} adopts a discriminative approach, training a classification head to make predictions based on model representations, which fine-tunes LLMs on\textasciitilde7 million commonsense statements. In contrast, our approach also leverages internal knowledge from a discriminative perspective but does not require additional training.

\subsection{CSPE Based on External Knowledge}

Language models (LMs) may have insufficient or inaccurate knowledge, which led to some methods to incorporate external knowledge to better estimate commonsense plausibility. A typical approach is to augment the model's knowledge by retrieving relevant sentences from external sources~\cite{Zhang2021AlleviatingTK, Yu2022RetrievalAF}. Commonsense knowledge bases (KBs)~\cite{Speer2016ConceptNet5A, sap2019atomic, Hwang2020COMETATOMIC2O} store extensive commonsense knowledge, enabling the extraction of relevant subgraphs to evaluate sentence consistency with commonsense~\cite{Choi2022ALBERTWK}. To alleviate the coverage limitations of the KBs while leveraging the extensive knowledge encoded in LMs, COMET~\cite{Bosselut2019COMETCT} introduced a dynamic KB by pre-training LM on existing commonsense KBs. Methods that utilize this dynamic KB~\cite{Ghazarian2023ACCENTAA, Tian2023BOOSTHB} demonstrate improved generalization across various commonsense reasoning tasks.