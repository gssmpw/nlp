\section{Related Work}
% Privacy concerns in machine learning have gained considerable attention in recent years, especially with the rise of deep learning models deployed in sensitive applications. In this section, we review existing research on gradient-based privacy attacks, privacy-preserving mechanisms using differential privacy (DP) and federated learning, and the role of mutual information in privacy risk assessment. This overview contextualizes the current study on classifying data sensitivity levels based on mutual information within the broader scope of privacy research and highlights limitations in current approaches that this work seeks to address.

\subsection{Gradient-Based Privacy Attacks}

Gradients, as core components of model optimization, contain information about the training data that adversaries can exploit to infer sensitive attributes or reconstruct raw training samples. Numerous attack methods demonstrate the feasibility of extracting private information from gradients, including model inversion \cite{fredrikson2015model} \cite{zhang2020secret} \cite{wang2021variational}, gradient inversion \cite{zhu2019deep} \cite{zhao2021exploiting} \cite{liang2023egia} \cite{ye2024gradient} \cite{fang2023gifd}, and membership inference attacks (MIA) \cite{shokri2017membership} \cite{truex2019demystifying} \cite{choquette2021label} \cite{hu2022membership}. These approaches reveal critical vulnerabilities in gradient sharing, particularly in collaborative settings such as federated learning. However, these methods often rely on specific assumptions about the attacker's prior knowledge and capabilities, which may not be realistic in practice. For example, classical model inversion and gradient inversion attacks, as explored by \cite{fredrikson2015model} and \cite{zhu2019deep}, generally assume that the attacker has detailed access to gradients or partial knowledge of the data distribution. Classical MIAs, such as those proposed by \cite{shokri2017membership}, also depend on assumptions about the attacker's access to similar or representative data samples. These assumptions can be idealized, failing to account for the variability and constraints of real-world attack scenarios. In addition, attack-based approaches cannot comprehensively evaluate data sensitivity, as it is infeasible to anticipate and test every possible privacy attack. Furthermore, they lack a mechanism for monitoring privacy risks during the training phase, leaving model trainers unaware of possible data breaches the risks become irreversible. These limitations emphasize the need for a generalized privacy metric that does not rely solely on specific attacks to assess privacy risks timely. Our work extends this body of research by focusing on quantifying the amount of privacy leakage present in the gradients, without relying on adversarial attacks.

\subsection{Differentially Private Privacy-Preserving Mechanisms}

In response to gradient-based privacy risks, privacy-preserving techniques using differential privacy (DP) have been developed to secure machine learning models. DP aims to lower the sensitivity level of data by limiting information leakage in model training, achieved by introducing controlled noise into gradients or model outputs, ensuring a quantifiable measure of privacy protection as explored by \cite{abadi2016deep} \cite{jayaraman2019evaluating} \cite{nasr2020improving}. However, while DP can lower the sensitivity level of data, it does not explicitly quantify the relationship between the decreased data privacy level and the potential increase in an attackerâ€™s information gain. Current privacy-preserving methods do not provide a direct, continuous measure of the impact these techniques have on data sensitivity relative to potential adversarial capabilities, limiting their utility in assessing how protective measures affect privacy risk in practice. Our work attempts to quantify sensitivity levels between data and gradients, providing insights for privacy budget allocation in DP. And it could also present the reduction of privacy leakage when applying DP in deep learning.

\subsection{Mutual Information and Privacy Risk Assessment}

Mutual information (MI) has been widely used to quantify dependencies between random variables across various fields and is emerging as a potential metric for privacy risk assessment in machine learning \cite{wang2021improving} \cite{he2021drmi} \cite{gao2023detecting}. This concept can be extended to the privacy domain, where MI can quantify the extent to which data characteristics are retained in gradients.

However, directly estimating MI for high-dimensional data is challenging, leading to the development of neural network-based estimators called Mutual Information Neural Estimator (MINE) \cite{belghazi2018mine}. MINE approximates MI by training a neural network to distinguish between joint and product distributions, enabling MI estimation in complex, high-dimensional deep learning settings. 

Although MI has not been widely applied to privacy quantification, it holds significant potential for measuring information leakage. Recent studies have started to explore this avenue \cite{farokhi2020modelling} \cite{wang2021privacy}, examining the utility of MI in quantifying privacy risks by measuring the dependencies between sensitive attributes and model outputs or gradients. This work builds on these foundations by applying MI as a privacy risk metric specifically for gradient leakage. By estimating the MI between training data and gradients, we aim to provide a continuous and theoretically grounded measure of privacy risk that complements existing binary privacy evaluations.

\subsection{Intermediate Representations in Deep Learning}
Intermediate representations, including feature maps and activations, have been studied in the context of model interpretability and adversarial attacks. Researchers have investigated how these intermediate features can expose information about the input data~\cite{kurakin2016adversarial}~\cite{machado2021adversarial}. The use of intermediate gradients, however, has been less explored, especially in the context of privacy leakage. By drawing an analogy to the class token in Vision Transformers, which acts as a global summary of the input data, we propose that the proper embeddings of intermediate gradients can serve as \textit{privacy tokens}. These tokens offer a compact and continuous representation of privacy risks, allowing model trainers to monitor the privacy status of the training data timely throughout the training process. This novel perspective of analyzing intermediate gradients as privacy tokens has not been explored in existing literature and constitutes a key contribution of our work.

% \subsection{Summary: Limitations of Current Privacy Assessment Frameworks}

% Most existing privacy assessment techniques, such as attack simulations, focus on post-hoc evaluations or assume an idealized adversary with specific knowledge. While privacy-preserving techniques and attack-based privacy assessments have advanced considerably, significant limitations remain: 
% \begin{itemize} 
% \item \textbf{Inadequacy of Attack-Based Privacy Measures}: Current methods rely on testing specific attacks, which cannot cover all potential privacy vulnerabilities, thus lacking completeness. 
% \item \textbf{Assumption Dependency}: Attack-based assessments require assumptions about an attacker's capabilities, which may not accurately represent real-world conditions. 
% \item \textbf{Lack of Continuous Privacy Quantification}: Current methods offer binary outcomes, such as attack success or failure, without providing a continuous or quantifiable metric for privacy leakage, leading to a limited understanding of data sensitivity levels. 
% \end{itemize}

% Our approach stands apart by offering a proactive framework for privacy assessment based on MI. By estimating the MI between the privacy toke of gradients and feature representations of training data, we can continuously monitor and quantify privacy risks during training, providing model trainers with actionable insights and enabling privacy protection without needing explicit attacks and equipping researchers and practitioners with a nuanced understanding of privacy risks in deep learning models.