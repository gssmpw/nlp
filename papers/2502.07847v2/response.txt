\section{Related work}
\textbf{Vision-language foundation models.} Foundation models open new horizons for machine learning tasks, particularly in supervised learning **Dosovitskiy et al., "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"**. Foundation models like CLIP can be easily adapted for downstream tasks. To further improve, downstream task classification, several efficient methods are available, including parameter-efficient prompt tuning (PEFT) **Zhang et al., "Parameter-Efficient Transfer Learning with Multitask Fine-tuning"**, prompt tuning, vanilla fine-tuning **Brown et al., "Language Models play by Ear"**, adapter tuning **Houlsby et al., "Parameter-Efficient Transfer Learning for NLP with Pre-trained Multitask Transformers"**, and CLIP fine-tuning **Radford et al., "Learning Transferable Visual Models"**.

The CLIP foundation model  **Touvron et al., "Training JFT-300M-scale CNNs for Image Classification"**, consist of an image encoder \(f(x)\) and a text encoder \(g(\textbf{t})\). Both encoders are jointly trained using contrastive loss. The pre-trained CLIP provide support of few-shot learning which can then be used for downstream tasks like \textit{prompt learning} using fixed or hand-crafted prompts.
Context Optimization (CoOp)  **Xu et al., "Content-Aware Contextualization"** introduced learnable task-specific prompts by replacing the manually designed prompts. CoOp provides two different prompt implementation i.e. \textit{Unified context vectors} where prompts shared across classes and \textit{Class-specific context vectors} provides individual prompts per class. But admittedly,  CoOp struggles with generalization to unseen classes and distribution shift.



\noindent
\textbf{Distribution shift in  foundation models.}Distribution shifts present unique challenges for foundation models compared to traditional domain generalization or adaptation  **Koh et al., "Wasserstein Distributional Robustness"**. In classical approaches, models are trained to learn invariant features under the assumption that these invariances hold at test time. In contrast, foundation models like CLIP  **Touvron et al., "Training JFT-300M-scale CNNs for Image Classification"**, ALIGN  **Radford et al., "Learning Transferable Visual Models"**, and Flamingo  **Zhou et al., "DeCLR: Decomposed Contrastive Learning for Few-Shot Image Classification"** are pre-trained on large-scale data and later adapted to downstream tasks. However, deploying these models in low-shot settings introduces challenges such as covariate shift (where the input distribution \(P(x)\) differs between pretraining (source) and target data) and confidence misalignment (where the model becomes overconfident on shifted inputs)  **Li et al., "Adversarial Calibration of Deep Neural Networks"**.  **Li et al. highlighted how distribution shifts can induce confidence misalignment in VLMs and proposed \say{vaccine} approach, applying perturbation-aware confidence alignment during fine-tuning to mitigate harmful embedding shifts. We address confidence alignment in low-shot settings through calibration-aware regularization.



\noindent\textbf{Confidence calibration in deep learning models.}
Extensive research focuses on confidence calibration methods to ensure model accuracy is aligned with its predicted confidence scores. These methods includes regularization based approaches, such as implicit regularization  **Guo et al., "On Calibration of Modern Neural Networks"** ____, \(L_{2}\) regularization  **Kumar et al., "Calibrating Deep Neural Networks with Bayesian Inference"**, and entropy regularization  **Thulasidasan et al., "Zero-Shot Overfitting: How Generalization Can Fail"** to align the model predicted accuracy with confidence. Some augmentation-based approaches, such as label-smoothing  **MÃ¼ller et al., "When Does Label Smoothing Help?"** and mix-up training methods  **Berard et al., "Mixup as Local Linear Embedding"** are also explored for confidence calibration in deep learning models. 
____ **Guo et al. empirically investigated the calibration of deep learning models under distribution shift. A comprehensive survey ____**, which addresses the recent development about calibration for deep learning models. 


\noindent\textbf{Confidence calibration in foundation models.}
____**Li et al. addresses the issue of under-confidence in pre-trained foundation models when these models are fine-tuned for downstream tasks in few-shot settings. Similarly, ____** highlights the miscalibration problem in CLIP-based adaptive approaches. Their proposed method empirically demonstrates that CLIP-based adaptive approaches, such as prompt learning, adapters, parameter-efficient fine-tuning (PEFT), and test-time adaptation, are prone to miscalibration in low-shot settings under distribution shifts. Furthermore, ____** investigates the prediction uncertainty of the foundation model CLIP for image classification tasks, focusing on variations in visual factors. Recent work ____** observed observed the post-hoc confidence miscalibration in fine-tuned VLMs and proposed Distance-Aware Calibration (DAC) to align calibration of VLMs.  


In this work, we propose a regularization-based approach to ensure that the predictive probabilities of foundation models align with confidence scores in a prompt-learning setting under distribution shifts.