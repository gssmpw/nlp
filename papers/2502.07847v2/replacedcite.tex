\section{Related work}
\textbf{Vision-language foundation models.} Foundation models open new horizons for machine learning tasks, particularly in supervised learning ____. Foundation models like CLIP can be easily adapted for downstream tasks. To further improve, downstream task classification, several efficient methods are available, including parameter-efficient prompt tuning (PEFT) ____, prompt tuning, vanilla fine-tuning ____, adapter tuning ____, and CLIP fine-tuning ____. 

The CLIP foundation model  ____, consist of an image encoder \(f(x)\) and a text encoder \(g(\textbf{t})\). Both encoders are jointly trained using contrastive loss. The pre-trained CLIP provide support of few-shot learning which can then be used for downstream tasks like \textit{prompt learning} using fixed or hand-crafted prompts.
Context Optimization (CoOp) ____ introduced learnable task-specific prompts by replacing the manually designed prompts. CoOp provides two different prompt implementation i.e. \textit{Unified context vectors} where prompts shared across classes and \textit{Class-specific context vectors} provides individual prompts per class. But admittedly,  CoOp struggles with generalization to unseen classes and distribution shift.



\noindent
\textbf{Distribution shift in  foundation models.}Distribution shifts present unique challenges for foundation models compared to traditional domain generalization or adaptation ____. In classical approaches, models are trained to learn invariant features under the assumption that these invariances hold at test time. In contrast, foundation models like CLIP ____, ALIGN ____, and Flamingo ____ are pre-trained on large-scale data and later adapted to downstream tasks. However, deploying these models in low-shot settings introduces challenges such as covariate shift (where the input distribution \(P(x)\) differs between pretraining (source) and target data) and confidence misalignment (where the model becomes overconfident on shifted inputs) ____. ____ highlighted how distribution shifts can induce confidence misalignment in VLMs and proposed \say{vaccine} approach, applying perturbation-aware confidence alignment during fine-tuning to mitigate harmful embedding shifts. We address confidence alignment in low-shot settings through calibration-aware regularization.



\noindent\textbf{Confidence calibration in deep learning models.}
Extensive research focuses on confidence calibration methods to ensure model accuracy is aligned with its predicted confidence scores. These methods includes regularization based approaches, such as implicit regularization ____ ____, \(L_{2}\) regularization ____ , and entropy regularization ____ to align the model predicted accuracy with confidence. Some augmentation-based approaches, such as label-smoothing ____ and mix-up training methods ____ are also explored for confidence calibration in deep learning models. 
____ empirically investigated the calibration of deep learning models under distribution shift. A comprehensive survey ____, which addresses the recent development about calibration for deep learning models. 


\noindent\textbf{Confidence calibration in foundation models.}
____ addresses the issue of under-confidence in pre-trained foundation models when these models are fine-tuned for downstream tasks in few-shot settings. Similarly, ____ highlights the miscalibration problem in CLIP-based adaptive approaches. Their proposed method empirically demonstrates that CLIP-based adaptive approaches, such as prompt learning, adapters, parameter-efficient fine-tuning (PEFT), and test-time adaptation, are prone to miscalibration in low-shot settings under distribution shifts. Furthermore, ____ investigates the prediction uncertainty of the foundation model CLIP for image classification tasks, focusing on variations in visual factors. Recent work ____ observed observed the post-hoc confidence miscalibration in fine-tuned VLMs and proposed Distance-Aware Calibration (DAC) to align calibration of VLMs.  


In this work, we propose a regularization-based approach to ensure that the predictive probabilities of foundation models align with confidence scores in a prompt-learning setting under distribution shifts.