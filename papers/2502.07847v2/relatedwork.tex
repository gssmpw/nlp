\section{Related work}
\textbf{Vision-language foundation models.} Foundation models open new horizons for machine learning tasks, particularly in supervised learning \cite{radford2021learning, jia2021scaling}. Foundation models like CLIP can be easily adapted for downstream tasks. To further improve, downstream task classification, several efficient methods are available, including parameter-efficient prompt tuning (PEFT) \cite{zhou2022learning}, prompt tuning, vanilla fine-tuning \cite{khattak2023maple}, adapter tuning \cite{zhang2021tip}, and CLIP fine-tuning \cite{ gao2024clip}. 

The CLIP foundation model  \cite{radford2021learning}, consist of an image encoder \(f(x)\) and a text encoder \(g(\textbf{t})\). Both encoders are jointly trained using contrastive loss. The pre-trained CLIP provide support of few-shot learning which can then be used for downstream tasks like \textit{prompt learning} using fixed or hand-crafted prompts.
Context Optimization (CoOp) \cite{zhou2022learning} introduced learnable task-specific prompts by replacing the manually designed prompts. CoOp provides two different prompt implementation i.e. \textit{Unified context vectors} where prompts shared across classes and \textit{Class-specific context vectors} provides individual prompts per class. But admittedly,  CoOp struggles with generalization to unseen classes and distribution shift.



\noindent
\textbf{Distribution shift in  foundation models.}Distribution shifts present unique challenges for foundation models compared to traditional domain generalization or adaptation \cite{hendrycks2019robustness,wortsman2022robust}. In classical approaches, models are trained to learn invariant features under the assumption that these invariances hold at test time. In contrast, foundation models like CLIP \cite{radford2021learning}, ALIGN \cite{jia2021scaling}, and Flamingo \cite{alayrac2022flamingo} are pre-trained on large-scale data and later adapted to downstream tasks. However, deploying these models in low-shot settings introduces challenges such as covariate shift (where the input distribution \(P(x)\) differs between pretraining (source) and target data) and confidence misalignment (where the model becomes overconfident on shifted inputs) \cite{ wang2023calibration, wang2024understanding,khan2025technical}. \cite{huang2024vaccine} highlighted how distribution shifts can induce confidence misalignment in VLMs and proposed \say{vaccine} approach, applying perturbation-aware confidence alignment during fine-tuning to mitigate harmful embedding shifts. We address confidence alignment in low-shot settings through calibration-aware regularization.



\noindent\textbf{Confidence calibration in deep learning models.}
Extensive research focuses on confidence calibration methods to ensure model accuracy is aligned with its predicted confidence scores. These methods includes regularization based approaches, such as implicit regularization \cite{ross2017focal} \cite{baloch2019focused}, \(L_{2}\) regularization \cite{guo2017calibration} , and entropy regularization \cite{pereyra2017regularizing} to align the model predicted accuracy with confidence. Some augmentation-based approaches, such as label-smoothing \cite{muller2019does} and mix-up training methods \cite{thulasidasan2019mixup, zhang2022and} are also explored for confidence calibration in deep learning models. 
\cite{ovadia2019can} empirically investigated the calibration of deep learning models under distribution shift. A comprehensive survey \cite{wang2023calibration}, which addresses the recent development about calibration for deep learning models. 


\noindent\textbf{Confidence calibration in foundation models.}
\cite{pandeyconfident} addresses the issue of under-confidence in pre-trained foundation models when these models are fine-tuned for downstream tasks in few-shot settings. Similarly, \cite{murugesan2025robust} highlights the miscalibration problem in CLIP-based adaptive approaches. Their proposed method empirically demonstrates that CLIP-based adaptive approaches, such as prompt learning, adapters, parameter-efficient fine-tuning (PEFT), and test-time adaptation, are prone to miscalibration in low-shot settings under distribution shifts. Furthermore, \cite{tu2024toward} investigates the prediction uncertainty of the foundation model CLIP for image classification tasks, focusing on variations in visual factors. Recent work \cite{wangopen} observed observed the post-hoc confidence miscalibration in fine-tuned VLMs and proposed Distance-Aware Calibration (DAC) to align calibration of VLMs.  


In this work, we propose a regularization-based approach to ensure that the predictive probabilities of foundation models align with confidence scores in a prompt-learning setting under distribution shifts.