\section{EXPERIMENTAL RESULTS}% (1.25P)}
\label{sec:results}
%{\color{red}
%Textwidth: \printinunitsof{in}\prntlen{\textwidth}
%Linewidth: \printinunitsof{in}\prntlen{\linewidth}
%}
\subsection{\gls{mpc} Solve Time Analysis}
\begin{figure*}[htbp]
\centering
\includegraphics[width=0.99\linewidth]{plots/solve_time.pdf}
\caption{Mean \gls{mpc} solution time for all solvers, target platforms and condensation levels for $N=20$ and $N=10$ in the trotting experiment.  
The qpOASES and DAQP solvers only show single points, as no comparison of condensation levels is possible here. Note that higher $N_p$ means the \gls{qp} is sparser.}
\label{fig:mpc_solve_time}
\end{figure*}
The mean \gls{mpc} solve time for the different solvers, target platforms and condensing levels for the \textit{trotting} experiment is depicted in \autoref{fig:mpc_solve_time}. The plotted time includes the condensing time, which is less than \qty{1}{\milli\second}.
%\subsubsection{Level of condensing}
Overall, solve time increases with increasing density of the \gls{qp}. 
% For $N=20$, some solvers become very slow for dense problems, which leads to unstable quadrupedal walking. 
For $N=20$, the dense formulation takes too long to solve for certain solvers leading to failed experiments, as indicated by red circles in \autoref{fig:mpc_solve_time}.
For the sparse solvers that support different levels of condensing (\gls{hpipm}, \acrshort{osqp}),  for $N_p \leq \frac{N}{2}$ the solve time increases exponentially with higher density, while it is approximately constant for $N_p > \frac{N}{2}$. 
%Interestingly, on both x86 systems, the minimum solve time of \gls{hpipm} balance and  speed\_abs is at $N_p = \frac{N}{2}$ (half-dense problem), while it slightly increases with higher sparsity. 
The dense solvers (\acrshort{qpoases}, \acrshort{daqp}) perform better for fully condensed \gls{qp}s, but are outperformed by solvers that exploit sparsity.
% Generally, each curve splits into two areas. The left and the right half, where the split is exactly where the condensed size equals half of the prediction horizon, i.e., $ N_p = \frac{N}{2}$.
% In the left half, the increase of the solve time with higher density is exponential, while on the right side, it rather resembles a straight line.
% For \gls{osqp}, the solve time in the right half, almost decreases monotonically, with the minimum being at the fully sparse formulation for all target systems. 
%\gls{hpipm} shows on the Jetson the same results with an almost constant solve time on the right half of the plots and a minimum at the sparse formulation.
%However, on both x86 systems, the minimum solve time of \gls{hpipm}-balance lies at $N_p = \frac{N}{2}$. It slightly increases with more sparsity. 
%For \gls{hpipm}-speed\_abs, the same effect applies, however, it is less prominent and on the Desktop the minimum is at $N_p \approx 0.75N$
%The dense \gls{hpipm} interface results indicate that the solve time for this configuration is somewhere betwetween the sparse interface with $N_p=1$ and $N_p=2$.
%\subsubsection{Solvers}
As expected, all solvers show lower solve times for $N=10$ than for $N=20$. For all computer architectures, condensing levels, and horizons, the best-performing solver is \gls{hpipm} speed\_abs. 
%, assuming the sparse solvers solve a \gls{qp} with a sparsity that minimizes solve time for the given architecture and $N$.
%However, the order of the best-performing solvers changes, although  stays the fastest solver anywhere.
For $N=20$, the second-fastest solver is \acrshort{osqp}, followed by \gls{hpipm} balance, although the difference is marginal on x86 systems.
\acrshort{daqp} and \acrshort{qpoases} are much slower for this prediction horizon.
For $N=10$, however, both \acrshort{qpoases} and \acrshort{daqp} perform almost as well as \acrshort{osqp} with $N_p=20$ (sparse problem).
%Much slower are both \gls{daqp} and \gls{qpoases}, with a solve time being higher (Jetson and LattePanda) than or close (Desktop) to the MPC target solve time of \qty{0.01}{\second}.
% For $N=10$ on the Jetson, the second-fastest solver is \gls{asm} solver \gls{qpoases}. This is followed in sequence by \gls{osqp}, \gls{daqp} and \gls{hpipm} balance within \qty{1}{\milli \second}
% On both x86 systems for $N=10$ the second-fastest solver is \gls{hpipm}-balance, in sequence followed by \gls{daqp}, \gls{qpoases} and \gls{osqp} with a difference $< \qty{0.5}{\milli\second}$.
% Considering the overall speed, \gls{osqp} is still faster by a factor of three than the requirement.
%\subsubsection{Target systems}
The x86 desktop architecture is the fastest among the target systems compared, with \gls{hpipm} speed\_abs achieving sub-\unit{\milli\second} solve times. 
The Jetson and the LattePanda perform similarly, with the LattePanda being only slightly faster.
%Overall, for all target systems and both prediction horizons, there are at least three solver configurations that enable the desired control frequency of \gls{mpc} of \qty{100}{\hertz}.
%\subsection{MPC variance analysis}
\begin{figure}
    \centering
    \includegraphics[width=0.99\linewidth]{plots/mpc_hist_n10.pdf}
    \caption{Histogram of the \gls{mpc} ($N=10$) solve times for selected solvers on all target platforms for dynamic trotting compared to standing. 
    %Half dense solver configuration refers to condensed size $N_p=\frac{N}{2}$
    }   \label{fig:mpc-solve-time-variance-n10}
\end{figure}
To compare the \gls{mpc} solve time between different test scenarios (standing, trotting), we use the normalized solve time histograms for all solvers on all computer architectures (see \autoref{fig:mpc-solve-time-variance-n10}). 
%As for \gls{hpipm}, it has been shown that the lowest solve time on the x86 systems is with a condensed problem size of half of the prediction horizon, leading to the \textit{half-dense} formulation. However, the improvement is small, and the histograms look similar to the \textit{sparse} formulation. Hence, they are left out.
%Since there were no fails, the figure shows only $N=10$. The histograms for $N=20$ also show the same patterns.
The histograms show that the mean solve time is less for the standing scenario than for trotting on all solvers. 
The most significant differences can be seen in \acrshort{daqp}, \acrshort{osqp} and \acrshort{qpoases}, where the histogram peaks are separated from each other. This effect is most prominent for \acrshort{qpoases}.
% The \acrshort{qpoases} solver shows the largest difference between the two scenarios.
%For this scenario, the solution times come even close to those of \gls{hpipm}-speed\_abs.
For the individual scenarios in themselves, the solution times for \gls{hpipm} are approximately normally distributed, while they show two significant peaks for the other solvers in trotting.
When comparing target computers, the slowest system (LattePanda) shows the highest variance in solve times.    
% Significant differences are evident in \gls{daqp}, \gls{osqp}, and \gls{qpoases}, where the histogram peaks are separate from each other. \gls{qpoases} shows the largest gap, where the standing solve time is almost half of trotting. 
% Here, the solve times for the standing scenario are even similar to those of \gls{hpipm}-speed\_abs.
% %It has for the standing experiment solve times in the range of \gls{hpipm}-speed\_abs.
% For the standing experiment, the \gls{hpipm} distributions overall describe a bell-shaped curve. 
% All other solvers show variations of a normal distribution as in \gls{daqp} and \gls{qpoases} or deviations as in \gls{osqp} (two peaks).
% For the trotting experiment, \gls{hpipm} again shows similarities of a normal distribution, whereas for \gls{osqp}, \gls{qpoases}, and \gls{daqp}, two peaks can be observed; the first peak is thereby consistently higher than the second.
% Comparing the target computers, the slowest system (LattePanda) has the highest variance in data and, hence, the highest distance between peaks or widest distributions.    

%\begin{figure}
%    \centering
%    \includegraphics[width=0.9\linewidth]{plots/mpc_hist_n20.pdf}
%    \caption{Histogram of the \gls{mpc} ($N=20$) solve times for selected solvers on all target platforms for dynamic trotting compared to standing. \textcolor{red}{maybe leave out}}
%    \label{fig:mpc-solve-time-variance-n20}
%\end{figure}

\subsection{WBC Solve Time Analysis}
\begin{figure}[!htpb]
    \centering
    \includegraphics[width=0.99\linewidth]{plots/wbc_bars.pdf}
    \caption{Computation time for different solvers using the \textit{reduced} and \textit{full TSID} \gls{wbc} in the trotting scenario.}
    \label{fig:wbc-total_time}
\end{figure}
\autoref{fig:wbc-total_time} shows the total computation time for one WBC cycle (set up time for the QP + solve time) using different solvers and two different WBC types. The set up time for the QP is solver-independent and thus not considered here separately.
The plots show that the Eiquadprog solver performs best in all comparisons. However, all solvers show a rather low computation time with $<0.5 ms$ on average. The computation time is slightly larger for the \textit{full TSID}, as compared to the \textit{reduced TSID} formulation on average.

\subsection{Efficiency Analysis}
\autoref{tbl:efficiency_all} shows the efficiency (measured in \gls{sfpw}) of different solvers in dynamic trotting over different target computers, planning horizons ($N=10$ and $N=20$), and \gls{wbc} formulations.
The most efficient solver per target computer is marked in bold.
The results the  show that the efficiency of the solvers per target system scales inversely with the solve time. 
Therefore, \gls{hpipm} speed\_abs has the highest efficiency on all systems for \gls{mpc} and Eiquadprog in case of the \gls{wbc}.

When comparing the efficiency of the three target computers, the Jetson Orin performs best: In case of \gls{mpc}, it is more than twice as efficient as the LattePanda and around three times as efficient as the desktop PC.
In case of \gls{wbc}, it is roughly four times faster than the others. 
% A slight difference, however, is that for $N=20$ and \gls{hpipm} speed\_abs not the half dense \gls{qp} formulation (which has a lower solve time), but the fully sparse formulation is the most efficient.
\begin{table}[!htpb]
\centering
\caption{Efficiency of \gls{mpc} and \gls{wbc} solvers on different \gls{hw}}
\label{tbl:efficiency_all}
\ra{1.3}
\begin{tabular}{@{}cclccc@{}}\toprule
 \multicolumn{6}{c}{\emph{ Mean \gls{sfpw}} (\unit{\hertz\per\watt})} \\
\midrule
  & & & Jetson Orin & Desktop & LattePanda  \\ 
\cmidrule{2-6}
\parbox[t]{1mm}{\multirow{10}{*}{\rotatebox[origin=c]{90}{\gls{mpc}}}} & \parbox[t]{1mm}{\multirow{5}{*}{\rotatebox[origin=c]{90}{$N=10$}}} &\acrshort{daqp} & 77.47 & 17.41 & 24.69\\
&&\acrshort{hpipm} \scriptsize{balance} \scriptsize{(sparse)} & 70.08 & 22.01& 26.75 \\
%\acrshort{hpipm} balance & \scriptsize{(half dense)}& 67.64 & 23.32 & 29.34\\
&&\acrshort{hpipm} \scriptsize{speed\_abs} \scriptsize{(sparse)} & \textbf{159.40} & \textbf{50.80} &  \textbf{66.65} \\
%\acrshort{hpipm} speed\_abs& (\scriptsize{half dense)} & 143.32 & 50.61 & 66.50\\
&&\acrshort{osqp}  \scriptsize{(sparse)} & 90.35 & 17.68 & 24.22\\
&&\acrshort{qpoases} & 93.94 & 18.70 & 25.33\\
%\midrule
%\gls{mpc}, $N=20$ & & Jetson Orin & Desktop & LattePanda  \\ 
\cmidrule{2-6}
&\parbox[t]{1mm}{\multirow{5}{*}{\rotatebox[origin=c]{90}{$N=20$}}} &\acrshort{daqp} & \num{8.82} & \num{1.20} & \num{2.64}\\
&&\acrshort{hpipm} \scriptsize{balance} \scriptsize{(sparse)} & \num{22.35} & \num{7.50}& \num{8.80} \\
%\acrshort{hpipm} balance & \scriptsize{(half dense)}& \num{21.26} & \num{8.34} & \num{10.57}\\
&&\acrshort{hpipm} \scriptsize{speed\_abs} \scriptsize{(sparse)} & \textbf{67.30} & \textbf{23.19} & \textbf{31.06} \\
%\acrshort{hpipm} speed\_abs& (\scriptsize{half dense)} & \num{60.11} &  \textbf{{23.32}} &  \textbf{31.32}\\
&&\acrshort{osqp}  \scriptsize{(sparse)} & \num{42.79} & \num{8.58} & \num{11.77} \\
&&\acrshort{qpoases} & \num{11.36} & \num{2.46} & \num{2.20} \\
%\midrule
%\multicolumn{2}{l}{\gls{wbc}, Full TSID } & Jetson Orin & Desktop & LattePanda  \\ 
\midrule
\parbox[t]{1mm}{\multirow{8}{*}{\rotatebox[origin=c]{90}{\gls{wbc}}}} & \parbox[t]{1mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{Full TSID }}} & Eiquadprog & \textbf{958.93} & \textbf{212.66} & \textbf{257.58}\\
&&\acrshort{hpipm} & 576.14 & 177.92 & 200.99\\
&&Proxqp & 795.34 & 190.80 & 216.11\\
&&\acrshort{qpoases} & 713.63 & 142.82 & 180.80\\
%\midrule
%\multicolumn{2}{l}{\gls{wbc}, Reduced TSID } & Jetson Orin & Desktop & LattePanda  \\ 
\cmidrule{2-6}
&\parbox[t]{1mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{Red. TSID}}}  &Eiquadprog & \textbf{1259.37} & \textbf{304.99} & \textbf{319.45}\\
&& \acrshort{hpipm} & \num{753.73} & \num{198.35} & \num{234.55}\\
&& Proxqp & 954.99 & 231.86 & 265.41\\
&& \acrshort{qpoases} & 977.83 & 238.09 & 263.70\\
 \bottomrule
\end{tabular}
\end{table}

%\begin{table}\centering
%\caption{Efficiency of \gls{wbc} solvers on different \gls{hw}}
%\label{tbl:efficiency_wbc}
%\ra{1.3}
%\begin{tabular}{@{}rlccc@{}}\toprule
%\multicolumn{5}{c}{\emph{ Mean \gls{sfpw}} (\unit{\hertz\per\watt})} \\
%\midrule
%Full TSID & & Jetson Orin & Desktop & LattePanda  \\ 
%\midrule
%Eiquadprog && \textbf{958.93} & \textbf{212.66} & \textbf{257.578}\\
%\acrshort{hpipm} && 576.14 & 177.92 & 200.99\\
%Proxqp && 795.34 & 190.80 & 216.11\\
%\acrshort{qpoases} && 713.63 & 142.82 & 180.80\\
%\midrule
% Reduced TSID & & Jetson Orin & Desktop & LattePanda  \\ 
%\midrule
%Eiquadprog && \textbf{1259.37} & \textbf{304.99} & \textbf{319.45}\\
% \acrshort{hpipm} && \num{753.73} & \num{198.35} & \num{234.55}\\
% Proxqp && 954.99 & 231.86 & 265.41\\
% \acrshort{qpoases} && 977.83 & 238.09 & 263.70\\
% \bottomrule
% \end{tabular}
% \end{table}

\section{DISCUSSION}% (0.5P)}
\label{sec:discussion}
The experimental results provide valuable insights into the performance of different solvers in dynamic quadrupedal walking regarding (1) sparsity, (2) variance over different tasks, and (3) \gls{hw} efficiency.

\paragraph{Sparsity} For larger \gls{qp}s, as they occur in \gls{mpc}, sparse formulations, when tackled by sparse solvers, generally seem to be advantageous over dense formulations. 
This effect becomes more significant with increasing prediction horizon. 
Here, condensing the \gls{qp} and solving it has the opposite effect: the solution time increases significantly. 
%The sparse solvers also benefit from keeping the \gls{qp} sparse. 
%While \gls{hpipm} shows a slightly improved performance on the x86 architectures with semi-dense formulations, this effect is minimal, and much more important is the choice of the appropriate solver.
This finding is in line with the theory of \cite{frison_efficient_2016} and \cite{axehill_controlling_2015} that in \gls{mpc}, if the input vector has almost the same size as the state vector, the fully sparse formulation is a good choice.
For robots with fewer control inputs, however, condensing could improve performance.
The results also indicate that for even smaller prediction horizons than $N=10$ or smaller formulations, as in \cite{di_carlo_dynamic_2018}, \gls{asm} solvers such as \acrshort{qpoases} could be advantageous. 
The \gls{mpc} problem analyzed here seems to be at the limit of the problem size where \gls{asm} is outperformed by other methods. 
It should also be considered that the \gls{hpipm} speed\_abs solver, which outperforms the other solvers even at $N=10$, does not provide accurate results for some applications. 
In the case of \gls{wbc}, where significantly smaller problems are considered, the dense solvers and especially \gls{asm} perform well for the reduced formulation. 
The results for the full TSID formulation show that if the problem gets bigger, other methods such as \gls{ipm} could indeed be considered as an option.
%Also for the \gls{mpc} the the results indicate that for larger horizons, \gls{ipm} are preferable as they scale better with $N$.
In contrast to \gls{mpc}, the choice of the \gls{qp} solver is not of great relevance for \gls{wbc} problems. Instead, efforts should be directed towards researching robust and stable \gls{qp} formulations for \gls{wbc}.

\paragraph{Variance over different tasks} The comparison of different motion tasks shows that in static cases such as standing, the \gls{asm}-based solvers, especially \acrshort{qpoases}, has a lower solve time than when trotting. 
This is in line with the general finding that \gls{asm} benefit from a stable problem structure, as the active-set can be warm-started~\cite{kuindersma_efficiently_2014, bartlett_active_2000}. 
The same finding applies to \acrshort{osqp}.
\gls{ipm} solvers, here \gls{hpipm}, show a certain robustness against changing problem structures, as already stated by \cite{frison_hpipm_2020}. 
In contrast to all other solvers, \gls{hpipm} does not exhibit a second peak in the solve time histogram while the robot is trotting.
The compromise could be that, for robot tasks where the problem is very dynamic, \gls{ipm} provides constant performance, while for in static tasks \gls{asm} or \gls{alm} might be advantageous.

\paragraph{ \gls{hw} efficiency} The comparison of the different computer architectures shows that the solution times of the Jetson Orin and the Latte Panda are comparable, while the desktop delivers significantly faster solutions. 
However, the efficiency measurement shows that the faster solution times come at the expense of power consumption, resulting in a comparable efficiency for both x86 systems, with the LattePanda being more efficient in the range of $\qty{10}{\hertz\per\watt}$ depending on the prediction horizon and selected solver.
On the other hand, the Jetson Orin is at least twice as efficient as the Desktop, and for the fastest solver and $N=10$ even up to three times as efficient.
This makes the Jetson Orin and ARM an ideal platform for applications with energy constraints, which is almost always the case in legged robotics.