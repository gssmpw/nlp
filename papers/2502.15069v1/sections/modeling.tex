The \methodname scalable synthetic generation pipeline provides us with a sizable dataset of 575 rare disease history-taking conversations.  However, our expert system doesn't provide information on a variety of common diseases, such as the Common Cold.  A real-world diagnostic system will need to account for these diseases.  Given the high diagnostic performance of many LLMs \cite{generalistMedicalLangaugeModel, Rutledge2024DiagnosticAO,zhou2024largelanguagemodelsdisease,tu2024conversationaldiagnosticai}, it would be challenging to meet their performance even with a corpus of similar non-rare chats.

We therefore combine existing LLM diagnostic performance with a specialized rare-disease model.  To do this, we train a rare disease candidate generation system which proposes a set of at most 5 rare diseases.  This list is recall-oriented, to allow several possibilities to be considered.  These rare disease candidates are then passed through a prompt of a larger LLM to produce a final diagnostic list.  The combination can leverage the niche expertise of an expert system while also leveraging the broader knowledge of general LLMs.

\subsection{Candidate Generation} \label{sec:candidate_gen_training}
To generate rare disease candidates for a given history-taking conversation, we train a smaller LLM to generate a list of at most 5 rare diseases. We train using Llama 3.1 7b as our base model \cite{grattafiori2024llama3herdmodels} using the chat text as input (training details in  Appendix \S \ref{sec:training_details}).   We exclude the structured findings and all other information used in the previous section. Note that these history-taking chats do not include a final diagnosis from the provider, so the model must make inferences from the chat alone. As the target output, we use the differential diagnosis list produced by our expert systems which contains up to five diseases.  We only include the disease names ordered by likelihood per the expert system score but do not include the scores themselves.



\subsection{Diagnosis Generation}\label{sec:ddx_gen}

For the final step of generating the differential diagnosis for the conversation,  we use a larger black-box general LLMs\footnote{We explored using the publicly-available MEDITRON-70B\cite{chen2023meditron70bscalingmedicalpretraining}, but it could not follow the DDx instructions in the prompt, instead outputting unrelated text.} (gpt-4o, claude, llama 3.3 70b) to fuse common and rare diseases seamlessly. We leave the role of identifying the relevant common diseases to the LLM, but infuse the candidate rare diseases through inferencing on the smaller model we trained according to \S~\ref{sec:candidate_gen_training}

The prompt (Appendix Prompt \ref{prompt:ddx})  uses multi-stage instructions, instructing the LLM to generate a list of 5 diagnoses without considering the rare list.  Then, it is instructed to consider the rare disease candidate list. Finally, it selects whichever from the two sections is most appropriate, including discarding all rare candidates if best. The prompt is also flexible, so we can remove instructions for incorporating candidate rare diseases or include other means of candidate rare diseases when available. We study these variations in \S~\ref{sec:eval_ddx}.
