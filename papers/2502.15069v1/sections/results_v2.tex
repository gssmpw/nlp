

To understand the performance of \methodname, we frame our evaluation around three questions.   First, does the \methodname end-to-end approach improve rare disease diagnoses over black-box LLMs (\S \ref{sec:eval_ddx})?  
Second, how does our expert candidate generation model perform at producing an accurate list of rare diseases for each case (\S \ref{sec:eval_cand})? Finally, how accurate are the simulated chats as judged by experts (\S \ref{sec:eval_exp})?  We include details on dataset creation in Appendix \S \ref{sec:eval_app}.


\subsection{DDx Improvements with \methodname}
\label{sec:eval_ddx}

We compare the end-to-end \methodname to two baselines.  First, in \textbf{base gpt-4o}, we generate a DDx with no external candidate list. \textbf{gpt-4o rare candidates} uses the \methodname prompt described in \S~\ref{sec:ddx_gen} without any candidate list. Similar to \methodname, \textbf{gpt-4o rare candidates} uses rare disease candidate list, but obtained using a separate LLM prompt. In contrast, \methodname uses a smaller trained model to generate the candidate list. For all three methods, we use the same prompt setup ( \S~\ref{sec:ddx_gen})  except for the additional candidate list and instructions. 

To be able to compare the generated DDx to the ground seed disease,  we use judge prompts adopted from \citet{tu2024conversationaldiagnosticai} (Appendix Prompts \ref{prompt:judge_binary} and \ref{prompt:judge_classification}), with gpt-4o as judge model. Previously, these prompts were shown to be well-calibrated with the human expert as a judge.  The first judge prompt returns a binary judgment, iterating through the DDx list and taking the first match present, if any.  The second judge prompt compares the full DDx list to the seed disease and returns a degree of similarity (unrelated to exact match) We use Top-1 accuracy, Top-5 accuracy, and mean reciprocal rank (MRR) as the metrics. 


\paragraph{Results} Table \ref{tab:ddx} shows the performance of the combination of our best-performing candidate generation model (gpt-4o + claude, see \S \ref{sec:eval_cand}) with gpt-4o serving as a DDx generator. We find that \methodname produces the best performance across the board, including statistically significant gains over the no candidate baseline. Notably, we also find that performance is worse when using gpt-4o as the rare disease candidate generator, reinforcing the utility of a specialized model.


We believe the most crucial improvement is seen in the Top-5 performance, as adding the rare disease candidate in the DDx would allow a provider to consider that disease.  The degree of improvement -- 17.42\% -- illustrates this impact. While we also see improvements in MRR performance, we also observe that the rare disease candidate is not very likely to be in the first position.  

Including the rare disease candidate but not over-relying on the candidate list is likely the preferred behavior (see \S~\ref{sec:eval_cand}).  In many cases, a common disease should be considered first, but the inclusion of a rare possibility can help a provider to best identify next steps.  Using only our smaller rare disease candidate generation model would likely overproduce rare diagnoses, whereas providing a black box LLM with options allows it to weigh the entire picture.  Alternatively, the LLM may not be able to properly diagnose a specific rare condition because it has no knowledge of it to begin with (see the analysis for further discussion).

In Table \ref{tab:ddx_by_sim}, we compare LLM performance on the differential diagnosis task with and without \methodname broken down by similarity level. We combine the gpt-4o and claude chats for this analysis. Since this prompt uses varying levels of granularity and compares the entire DDx, the distribution is different than with the binary prompt, which only compares diseases one-to-one. 

In the case of gpt-4o, we see broad improvements when adding \methodname.  This includes a 33\% improvement in the number of Exact Matches.  Yet we see even larger performance improvements when applying \methodname to claude-sonnet's DDx capabilities.  We see a 25.2\% reduction in unrelated diagnoses and a 37\% increase in exact matches.  Surprisingly, we also see similar performance gains on Llama 3.3 70b, including a 27\% increase in exact matches and an 11.7\% reduction in unrelated matches. While there is a gap between the closed- and open-weight models, the margin is small.

\paragraph{Analysis}   We break down the results of the \methodname gpt-4o dataset results in Table \ref{tab:ddx} into performance by disease hierarchical category as taken from our expert system.  A disease can align with one or more categories. The full results are shown in Appendix Table \ref{tab:ddx_by_cat}.  The group with the largest number of diseases -- Infectious diseases -- performs slightly less than the average.  This could potentially be due to the overall broader number of diseases in this category, many of them closely related.  Other large groups, such as \textit{Neoplastic disease}, \textit{Impaired cardiovascular function}, and \textit{non-infectious inflammatory diseases} have roughly average performance.  On the lower end, as expected,  \textit{Congenital disorders due to abnormal fetal development} performs worst at 53.3\% Top-5 accuracy, whereas \textit{Disorders of smooth muscle contraction and/or relaxation} performs best at 94.4\%, although both are small categories.

\subsection{\methodname Candidate Generation}
\label{sec:eval_cand}
We measure the performance of \methodname candidate generation by comparing it to the seed disease. The candidate generator outputs a list  of at most $5$ candidates, and we compare to the seed disease using exact match. As before, we use  MRR and Top-1 and Top-5 accuracy.  Since GPT-4o and Claude required different approaches to generate the chat data, we considered different dataset variations to understand differences in the performance. 

\paragraph{Results} Table \ref{tab:cand_gen} reports the performance of \methodname on candidate generation.
Using the combined training set during training leads to the highest performance -- 88.8\% Top-5 on gpt-4o, and 77.82\% on claude on the test sets.   When only trained on one of the data sources,  performance is best when applied to data from the same source.   In our initial experiments, we found that including roughly $40$ to $50$ training examples per disease would achieve the same performance as more samples (\textit{e.g.} 100), but fewer would result in worse performance.  

While the gpt-4o test set performs similarly on gpt-4o trained models and combined training data, the claude chat test set sees a sizable performance bump when using all training data.   Since our claude dataset is smaller in size, we hypothesize that the additional signal from the gpt-4o training examples helps improve performance on the claude test set.  To illustrate this, we train a model on only gpt-4o data but randomly (per-seed disease basis) downsampled to roughly the same amount of data in claude's training set.  We find a similar pattern -- the training data reduction (40.5\% of the original size) results in a 17\% drop in Top-5 accuracy, and similar reductions for other metrics.  This suggests the lower claude performance is due to dataset size instead of worse corpus simulation performance.

While we only consider exact matches in Table \ref{tab:cand_gen}, we also run the similarity judge prompt (Prompt \ref{prompt:judge_classification}) on the gpt-4o + claude trained model, gpt-4o test set results. In only 5.2\% of cases, the model returns a DDx list unrelated to the seed diagnosis.  In the remaining cases, non-exact matches have some degree of similarity. We also include validation results in Appendix Table \ref{tab:validation_performance}
 



\subsection{Expert Evaluation of Chats}\label{sec:eval_exp}
We evaluate whether our synthetically generated chats are possible manifestations of the seed rare disease. For this, we use an external reviewing service that provides medical annotators and use third-year medical students for this task as they have been recently trained in similar tasks. They can also use additional resources (\textit{e.g.} medical references) during annotation. We also included hard negative examples during annotations to serve as distractors. We include details in Appendix \S \ref{sec:eval_app_eval}.


\paragraph{Results}
On our annotation set of 651 cases (with 73 annotation-specific negatives), we found the overall agreement rate to be 88.6\% with a Cohen's kappa of 0.53.  For the expected positive cases, \textit{i.e.} the disease was indicated for that patient case during simulation, the agreement rate was 90.48\%.  For the negative cases, the agreement rate was 73.97\%.  The high agreement rate of the positive examples used for training illustrates the performance of our synthetic generation pipeline. We include additional discussion in Appendix Section \ref{sec:eval_app_results}.

