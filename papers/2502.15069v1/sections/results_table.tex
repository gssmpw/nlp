




\begin{table*}[tb]
    \centering
    \begin{tabular}{l | c c c | c c c} \toprule
        \multirow{2}{*}{\textbf{ Differential Diagnosis}} & \multicolumn{3}{c|}{\textbf{gpt-4o test set (n=3403)}} & \multicolumn{3}{c}{\textbf{claude test set (n=2868)}} \\ \cmidrule(r){2-4} \cmidrule(l){5-7}
        & \textbf{Top-5} & \textbf{Top -1} & \textbf{MRR} & \textbf{Top-5} & \textbf{Top -1} & \textbf{MRR} \\ \midrule
        \textbf{baseline} & 56.80\% & 28.65\% & 0.390 & 56.69\% & 30.65\% & 0.406 \\ 
        \textbf{gpt-4o rare candidates} & 52.66\% & 25.95\% & 0.357 & 55.47\% & 29.04\% & 0.388 \\ 
        \textbf{\methodname candidates} & \textbf{74.38\%} & \textbf{33.12\%} & \textbf{0.471} & \textbf{71.41\%} & \textbf{33.23\%} & \textbf{0.461} \\ \bottomrule
    \end{tabular}
    \caption{Performance on generated gpt-4o ddx task. All metrics for \methodname on both datasets (see bolded) are significant using a two-sided Wilcoxon signed-rank test with $p<0.01$ compared to the no candidates baseline.}
    \label{tab:ddx}
\end{table*}

\begin{table*}[tb]
\centering
\begin{tabular}{l|cccccc} \toprule
\textbf{DDx LLM} & \textbf{Exact} & \textbf{Extremely Rel.} & \textbf{Relevant} & \textbf{Somewhat Rel.} & \textbf{Unrelated} \\ 
\midrule
\textbf{baseline gpt-4o} & 22.8\% & 19.9\% & 4.9\% & 21.0\% & 31.3\% \\ 
\textbf{\methodname gpt-4o} & 55.8\% & 8.8\% & 2.3\% & 12.8\% & 20.2\% \\ \midrule

\textbf{baseline claude} & 19.2\% & 16.9\% & 3.9\% & 14.5\% &45.6\%  \\ 
\textbf{\methodname claude} & 56.8\% & 10.7\% & 1.6\% & 10.6\% & 20.4\% \\ \midrule

\textbf{baseline Llama 3.3 70b} & 20.3\% & 19.3\% & 5.3\% & 21.7\% & 33.5\% \\ 
\textbf{\methodname Llama 3,3 70b} & 47.3\% & 12.2\% & 3.3\% & 15.4\% & 21.9\% \\ \bottomrule
\end{tabular}
\caption{We compare LLM baseline DDx generation performance to LLMs with addition of \methodname candidates.  We report the LLM as judge results across several categories of similarity, ranging from Exact Match to Unrelated. We combine gpt-4o and claude test sets for this analysis.}
\label{tab:ddx_by_sim}
\end{table*}


\begin{table*}[tb]
    \centering
    \begin{tabular}{l | c | c c c | c c c}
        \toprule
        \multirow{2}{*}{\textbf{Training Dataset}} & \multirow{2}{*}{\textbf{Training Size}} & \multicolumn{3}{c|}{\textbf{gpt-4o test set (n=3403)}} & \multicolumn{3}{c}{\textbf{claude test set (n=2868)}} \\ \cmidrule(r){3-5} \cmidrule(l){6-8}
         & & \textbf{Top-5} & \textbf{Top-1} & \textbf{MRR} & \textbf{Top-5} & \textbf{Top-1} & \textbf{MRR} \\ \midrule
        \textbf{claude} & 8837 & 48.37\% & 34.12\% & 0.4007 & 64.92\% & 45.64\% & 0.5371 \\ 
        \textbf{gpt-4o} & 21782 & 88.04\% & 63.88\% & 0.7410 & 44.18\% & 28.45\% & 0.3490 \\ 
        \textbf{gpt-4o downsampled} & 8813 & 70.88\% & 47.90\% & 0.5742 & 37.20\% & 23.25\% & 0.2884 \\ 
        \textbf{gpt-4o + claude} & 30619 & 88.80\% & 64.21\% & 0.7463 & 77.82\% & 56.35\% & 0.6526 \\ 
        \bottomrule
    \end{tabular}
    \caption{Evaluation on the candidate generation task, with MRR, Top-5 and Top-1 Accuracy.  We evaluate on models only trained on claude data, gpt-4o data, and both, and evaluate separately on claude and gpt-4o test sets. We include a model trained on a downsampled set of gpt-4o data that approximates the size of the claude training set.}

    \label{tab:cand_gen}
\end{table*}