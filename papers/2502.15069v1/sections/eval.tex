

To understand the performance of \methodname, we frame our evaluation around three questions.  First, how well does our candidate generation system perform at producing an accurate list of rare diseases for each case (\S \ref{sec:eval_cand})?  Second, does the inclusion of a candidate list improve the DDx performance of LLMs (\S \ref{sec:eval_ddx})?  Finally, how accurate are the simulated chats (\S \ref{sec:eval_exp})?  We include details on dataset creation in Appendix \S \ref{sec:eval_app}.


\subsection{Candidate Generation Performance}
\label{sec:eval_cand}

We evaluate the performance of our candidate generation tasks Top-1 accuracy, Top-5 accuracy, and mean reciprocal rank (MRR). The candidate generation system produces a list of at most $5$ candidates, and we consider an entry to be a match if it is an exact string match to the seed disease.  We report the results for models trained with gpt-4o chats only, claude chats only, and combined gpt-4o and claude chats.  We separately evaluate the performance for claude and gpt-4o test set chats. Additionally, given the larger size of the gpt-4o training set, we also report a model trained on a downsampled amount of training data roughly equivalent to the size of the claude training set.  Given that the claude training set is 40.5\% of the size of the gpt-4o, we downsample to that percentage but apply the downsampling at the disease level. We include validation results in Appendix Table \ref{tab:validation_performance}.

\subsection{Differential Diagnosis Performance}
\label{sec:eval_ddx}
For the final LLM-generated DDx, we similarly report Top-1 accuracy, Top-5 accuracy, and MRR. We compare the LLM's generated differential diagnosis list to the seed disease using judge prompts adopted from previous work (Appendix Prompts \ref{prompt:judge_binary} and \ref{prompt:judge_classification}; \citet{tu2024conversationaldiagnosticai}).  This allows us to account for diseases that are the same but written differently.  We report results using a judge prompt that returns a binary judgment, iterating through the DDx list and take the first match present, if any.  Second, we use a prompt that compares the full DDx list to the seed disease and returns a label of unrelated, somewhat related, relevant, extremely relevant, and exact match.  

We compare the performance of our approach with two baselines.  First, we evaluate the performance without candidates.  Second, we separately prompt the same LLM to generate a list of 5 rare diseases for consideration and use that list instead of the list produced by our candidate generation approach.  This is designed to show whether specifically prompting black-box LLMs for rare disease candidates will achieve the same results as \methodname.  We separately report results on our gpt-4o and claude generated data.


\subsection{Expert Evaluation of Rare Disease Chats}\label{sec:eval_exp}
To evaluate whether our synthetically generated chats are possible manifestations of the seed rare disease, we do an expert evaluation task on one chat example taken from every disease in our dataset.  We use an external reviewing service that provides medical annotators and use third-year medical students for this task as they have been recently trained in similar tasks. We include further details about this task in Appendix \S \ref{sec:eval_app_eval}, including information about negative example generation which is only used for annotations.
