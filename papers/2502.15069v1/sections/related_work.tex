Using LLMs for diagnosis is increasingly powerful \cite{generalistMedicalLangaugeModel, Rutledge2024DiagnosticAO,zhou2024largelanguagemodelsdisease,tu2024conversationaldiagnosticai}.  Previous rare disease investigations \cite{hu2023can,shyr2024identifying,sandmann2024systematic,chen2024rarebench,yang2024rdguru,mehnen2023chatgpt,shyr2024identifying,Olmo2024AssessingDD} focus on generating diagnoses from smaller sets of vignettes, which are concise summaries of a patient's health issue. Vignettes are commonly used in narrow settings.  Comparatively, using the history-taking chat replicates a primary care setting where confirmatory testing has not been performed. Previous chat-related work focused on a much smaller set \cite{yang2024rdmaster}. 

Prompting methods such as chain of thought \cite{wei2022cot} may improve rare disease performance to a degree but cannot enable the integration of external knowledge.  Other techniques such as retrieval augmented generation \cite{lewis2020rag} are unlikely to capture the complex nuances of symptoms as stated by patients. LLM performance on emerging data has also been studied \cite{mitchell2021fast,gu2024model}.  A related approach is \citet{yao2024scalable}, which uses a smaller expert model, but their approach seeks to update the original model.  \cite{gupta-etal-2024-model} illustrates that model editing can also lead to the model losing knowledge.  Other work \cite{zhao2024wildhallucinations} studies LLM performance in other rare data situations, and studies the related issue of  hallucinations\cite{fabbri-etal-2022-qafacteval,min-etal-2023-factscore}. 
