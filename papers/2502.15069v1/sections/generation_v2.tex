
The use of synthetic data from large language models has been widely studied \cite{li-etal-2023-synthetic,yu2023training,liu2024best}. However, our task requires generating history-taking conversational data with labeled differential diagnosis. Since identifying differential diagnosis is the task we are trying to solve with \methodname, we took a different approach to generate labeled history taking chats to avoid simply evaluating an LLM based on it's existing knowledge.


As shown in Figure, \ref{fig:expert_sim}, we first simulate an example using expert system knowledge.  We start with a seed disease and generate a structured patient case (represented as a list of findings). 
Such a generation can have ambiguity about the final diagnosis. Therefore, we use the same expert system to assign a full differential diagnosis. The structured case is then used to simulate a history-taking conversation as detailed in \S ~\ref{sec:chat_generation} so that LLM needs to only generate a patient-facing question with the provided finding, while maintaining the conversational flow. We repeat this process independently for all diseases. The resulting dataset of history-taking conversation and differential diagnosis pairs is used to train rare diseases candidate generation model (\S~\ref{sec:cand_gen}).

\subsection{Generation of structured cases}
\label{sec:vignette}

We use a rare disease expert system, evolved from Internist-1 \cite{miller_internist-1_1982} and QMR \cite{miller_qmr}, to generate structured cases using expert-curated diagnostic rules based on a knowledge base of diseases, findings, and their relationships. Findings include symptoms, signs, lab results, demographics, or medical history. In this study, we focus on patient-answerable findings and {\it only} focus on symptoms. Each finding-disease link is defined by \emph{evoking strength} and \emph{frequency}, scored from 1 to 5 by a team of medical experts based on the studies for that disease. \emph{Evoking strength} measures the association between a finding and a disease, while \emph{frequency} indicates how often a finding occurs in patients with the disease. Each finding also has a disease-independent \emph{import} variable, indicating its global importance.

Our simulation algorithm, adapted from medical case simulator \citet{qmr-simulated-cases, pmlr-v85-ravuri18a}, starts by sampling a seed disease and sequentially constructing a set of findings. First, demographic variables are sampled, followed by predisposing factors, and then other findings are made to decrease frequency relative to the disease. Each finding is randomly determined to be present or absent, with impossible findings excluded and high co-occurrence findings prioritized. The simulation concludes once all findings in the knowledge base for that disease are considered, operating under a closed-world assumption that limits diseases and findings to those in the knowledge base.  Unlike previous approaches to simulation, we also use the expert system to compute differential diagnosis after the first six findings are sampled.  We use this to consider findings from other diagnoses in the differential diagnosis to prioritize sampling additional negative findings that overlap with the seed disease. This allows the sample to be slightly more targeted at the seed disease. In turn, we widen the gap between the seed disease and the remainder of the differential diagnosis.

Each simulation includes a differential diagnosis of up to 5 diseases as ranked by expert system scoring. A DDx may consist of multiple diseases in cases where a final diagnosis may not be ascertained without laboratory testing. We maintain this differential diagnosis list for training in \S \ref{sec:cand_gen}. We iterate through 630 rare diseases in the expert system, using each as a seed disease. We keep the generated structured case if the seed disease is top-scoring in the differential to reduce the effect of the noise in the simulation process. We set a minimum of $50$ valid simulations out of $200$ attempts. Any diseases falling below that are excluded (\textit{e.g.} Friedreich's ataxia), as this typically indicates that the disease is unidentifiable from symptoms alone. This results in 575 diseases in our dataset. We include an example in Appendix \S \ref{fig:chat_simulation_ex_app}.

\subsection{Generation of chats from structured cases}
\label{sec:chat_generation}
For each (structured case, DDx) pair, we start with creating a complete demographic profile.  This includes name, gender, age, race, education, and location by first selecting from the structured case.  If not present in the case, we randomly generate from the LLM. We incorporate location and education to introduce additional variability in patient language. This diversifies the profiles and reduces sensitivity to names and locations across different simulations, which could lead to misleading patterns.

We use the structured case along with the expanded demographic profile to anchor LLM-powered history-taking chat simulation. We prompt an LLM to create a conversation that closely adheres to that case.
The LLM-based chat simulator has access to the disease name, finding set, and demographic profile.  For each finding, we include a definition if the expert system provides one so the LLM's understanding of the term aligns with the expert system.  For each disease, we persist a database of messages that correspond to each finding (\textit{e.g.} ``I feel hot'' corresponds to \textit{fever (present)}). We include the previous message in the prompt for each subsequent generation and ask the LLM to generate a different one so it does not repeat.

We enforce a format for the chat, which begins with a "system" message with the patient's demographic information. The provider simulator starts the conversation open-ended, and the patient simulator reports the symptoms drawn from the findings set. For each patient utterance, the LLM outputs both the message and which findings are included.  This is to encourage the message to be consistent with the findings. We prompt LLM  for the patient simulator to report only at most, three findings 
in a single message and also prompt the LLM  for the provider simulator not to generate leading questions.

For the simulation, we use two different LLMs to generate our chats -- OpenAI's Gpt-4o and Anthropic's Claude-3.5-sonnet -- and use the same approach unless noted.  All components of the generation pipeline use only one of the LLMs. For chats generated by GPT-4o, we found that doing this process in a single prompt is sufficient (see Appendix Prompt \ref{prompt:chat_gpt4o}).  For Claude, we found that this generated chats which were very terse.  Therefore, we prompt Claude with the same information and ask it to generate one patient-provider turn at a time  (see Appendix Prompt \ref{prompt:chat_claude}).  The findings provided to Claude are separated into findings already included and those that need to be added.  

For chats generated by both LLMs, we finally run a checker prompt that ensures that the resulting chats include all symptoms in the finding set and edits the chat if not all are included.  If the first edit fails, we try two more times.  If all other attempts fail, we exclude this chat from our dataset.  This results in a corpus of rare disease chats -- 28,589 using GPT-4o and 14,573 using Claude (further statistics are included in Appendix Table \ref{tab:data_details}).  We include a full example chat using GPT-4o in Appendix Figure \ref{fig:example_gpt4o} and using Claude in Figure \ref{fig:example_claude}. 
