\documentclass{article}


\usepackage{PRIMEarxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{fancyhdr}       % header
\usepackage{graphicx}       % graphics
\graphicspath{{media/}}     % organize your images and other figures under media/ folder
\usepackage{natbib}
\bibliographystyle{plainnat}  % or another style of 
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmic}


\usepackage{graphicx}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{amsmath}
\usepackage{todonotes}
%Header
\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }} 

% Update your Headers here
\fancyhead[LO]{Binned Spectral Power Loss for Improved Prediction of Chaotic Systems}
% \fancyhead[RE]{Firstauthor and Secondauthor} % Firstauthor et al. if more than 2 - must use \documentclass[twoside]{article}
\usepackage{array}
\usepackage{multicol}

  
%% Title
\title{Binned Spectral Power Loss for Improved Prediction of Chaotic Systems
%%%% Cite as
%%%% Update your official citation here when published 
\thanks{\textit{\underline{Paper in Review}}} 
% \textbf{Authors. Title. Pages.... DOI:000000/11111.}} 
}

\author{
  Dibyajyoti Chakraborty\thanks{Corresponding author: d.chakraborty@psu.edu} \\
  College of Information Sciences and Technology\\
  Pennsylvania State University\\
  University Park, PA, USA.  \\
  %% examples of more authors
  \And
  Arvind T. Mohan\\
  Computational Physics and Methods\\
   Los Alamos National Laboratory\\
   Los Alamos, NM, USA\\
   \And
  Romit Maulik \\
  College of Information Sciences and Technology \\
  Pennsylvania State University \\
  University Park, PA, USA.  \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}


\begin{document}
\renewcommand{\arraystretch}{1.5}
\maketitle
\begin{abstract}
Forecasting multiscale chaotic dynamical systems with deep learning remains a formidable challenge due to the spectral bias of neural networks, which hinders the accurate representation of fine-scale structures in long-term predictions. This issue is exacerbated when models are deployed autoregressively, leading to compounding errors and instability. In this work, we introduce a novel approach to mitigate the spectral bias which we call the Binned Spectral Power (BSP) Loss. The BSP loss is a frequency-domain loss function that adaptively weighs errors in predicting both larger and smaller scales of the dataset. Unlike traditional losses that focus on pointwise misfits, our BSP loss explicitly penalizes deviations in the energy distribution across different scales, promoting stable and physically consistent predictions. We demonstrate that the BSP loss mitigates the well-known problem of spectral bias in deep learning. We further validate our approach for the data-driven high-dimensional time-series forecasting of a range of benchmark chaotic systems which are typically intractable due to spectral bias. Our results demonstrate that the BSP loss significantly improves the stability and spectral accuracy of neural forecasting models without requiring architectural modifications. By directly targeting spectral consistency, our approach paves the way for more robust deep learning models for long-term forecasting of chaotic dynamical systems.
\end{abstract}
\twocolumn
\section{Introduction}
The improved forecasting of complex nonlinear dynamical systems is of vital importance to several real-world applications such as in engineering\citep{kong2022digital}, geoscience\citep{sun2024probabilistic}, public health\citep{wang2021bridging}, and beyond. Frequently, the accurate modeling of such systems is complicated by their multiscale nature and chaotic behavior. Physics-based models for such systems are generally described as partial differential equations (PDE), the numerical solutions of which require significant computational effort. For instance, the presence of multiscale behavior require very fine spatial and temporal resolutions, when numerically solving such PDEs, which can be severely limiting for real-time forecasting tasks\citep{harnish2021multiresolution}. Chaotic systems also require the assessment of statistics using ensembles of simulations, adding significant costs. This is one of they key bottlenecks in a variety of applications in earth sciences, energy engineering and aeronautics.

One approach to addressing the aforementioned challenges is through the use of data-driven methods for learning the time-evolution of such systems. In such methods, function approximation techniques such as neural networks\citep{cybenko1989approximation,mcculloch1943logical}, Gaussian processes\citep{santner2003design}, and neural operators\citep{chen1995universal}, among others, are utilized to learn the map between subsequent time-steps from training data. Subsequently, these trained models are deployed autoregressively to perform roll-out forecasts for dynamics into the future. This approach holds particular promise for systems where large volumes of data are available from open-sourced simulations or observations. Recently, this approach to forecasting has been applied with remarkable success to dynamical systems emerging in applications such as weather \citep{bi2022pangu,lam2022graphcast,pathak2022fourcastnet,nguyen2023scaling}, climate \citep{guan2024lucie,watt2023ace,ruhling2024dyffusion}, nuclear fusion \citep{mehta2021neural,burby2020fast,li2024surrogate}, renewable energy \citep{sun2019short,wang2019review}, etc. 

However, for several multiscale applications, purely data-driven forecast models suffer from a common limitation that degrades their performance in comparison with physics-based solvers. This pertains to an inability to capture the information at smaller scales in the spatial domain of the dynamical system \citep{bonavita2024some,olivetti2024data,pasche2025validating,mahesh2024huge}. In the spectral space, these refer to the energy associated at higher wavenumbers. Consequently, data-driven models may be over or under-dissipative during autoregressive predictions which eventually cause a significant disagreement with ground-truth and in worse-case scenarios, leading to completely non-physical behavior \citep{chattopadhyay2023long}. These errors are commonly understood to be caused by so-called \textit{spectral biases} \citep{rahaman2019spectral}, defined by the tendency of a neural network trained on a typical mean-squared-error loss function to optimize the larger wavenumbers first while training. This phenomena has been observed across a variety of architectures like generative adversarial networks \citep{schwarz2021frequency,chen2021ssd}, transformers \citep{bhattamishra2022simplicity}, state space models \citep{yu2024tuning}, physics-informed neural networks \citep{chai2024overcoming}, Kolmogorov-Arnold networks \citep{wang2024expressiveness}, etc. The mathematical relation to spectral biases is presented later in this manuscript. 
% Additionally, for chaotic systems, a sufficiently large data-driven model may arrange spectral content appropriately but still predict invariant measures with severe biases\citep{linot2023stabilized}. 


In response, there has been significant research into the development of new data-driven methods that can mitigate these failure modes for predicting multiscale dynamical systems. Most research attempts to construct novel neural architectures that can aid the function approximation to prioritize the higher wavenumbers during learning. For example, \citep{tancik2020fourier} uses problem-specific Fourier feature mapping to improve the performance of fully connected neural network approximations. Novel neural network architectures such as diffusion models \citep{gao2023implicit,oommen2024integrating,luo2023image} and Gestalt autoencoders \citep{liu2023devil} have also shown significant promise in resolving fine-scaled features. The former constructs a neural network approximation of the forecast as a sample from an unknown (but learnable) probability density function, whereas the latter reconstructs images (here referred as the state being predicted) in both the spatial and wavenumber domain to reduce spectral bias. Frequency augmentations are also performed to improve the spectral quality of neural network outputs \citep{lin2023catch}. They devise a dynamic weighting scheme to provide differing importance to various wavenumbers during the learning process. Hierarchical multiscale neural approximation techniques have also shown promise for learning multiscale tasks \citep{liu2024mitigating,barwey2023multiscale}. In such techniques, function approximations leverage representations of the data on hierarchical representations (discretizations) of the state. These lead to more effective capture of inter-scale information exchange in the process of predictions. Other approaches propose choices for hyperparameters or data processing to improve the quality of the predictions \citep{cai2024batch}.

We aim to address the following open question: \textit{How can we develop a universally adaptable method that seamlessly integrates into any existing deep learning forecast architecture to mitigate spectral bias and improve stability while maintaining computational efficiency?} In this work, we propose a novel approach to tackle this challenge, with a particular focus on its application in forecasting chaotic dynamical systems.

\textbf{Contributions} The contributions of this paper is as follows: First, we introduce the Binned Spectral Power (BSP) Loss, a novel approach to address the spectral bias of arbitrary neural forecasting models. By focusing on preserving the distribution of energy across different spatial scales instead of relying solely on pointwise comparisons, our method enhances the stability and quality of long-term predictions. Second, our proposed framework is architecture agnostic, easily deployable, and requires minimum additional hyperparameter tuning. This ensures that our approach remains broadly applicable, computationally feasible, and adaptable to a variety of dynamical systems. Third, we show that the BSP loss can actually mitigate the spectral bias using a synthetic example from \citep{rahaman2019spectral}. Fourth, we further examine the effectiveness of our method through extensive testing on the forecasting of the following complex and high-dimensional chaotic systems: Kolmogorov flow \citep{obukhov1983kolmogorov}, a 2D benchmark for chaotic systems used for various studies\citep{kochkov2021machine}, a high Reynolds number flow over NACA0012 airfoil\citep{towne2023database} and the 3D homogeneous isotropic turbulence\citep{mohan2020spatio}. Our results indicate that the proposed loss function significantly improves both predictive stability and spectral accuracy, mitigating common limitations of deep learning models in capturing fine-scale structures over long forecasting horizons.

\section{Background}
We consider an operator \( G \) that maps one timestep of the state \( x \) of a dynamical system to the next. This operator can be viewed as the \textit{optimal} data-driven process that bypasses the direct solution of the governing differential equation for each timestep, effectively describing the system's dynamics. The evolution of the state can thus be expressed as follows:
\begin{equation}
    x_t = G(x_{t-1}) = G(G(G(\ldots G(x_0)))) = G^t(x_0),
\end{equation}
where \( x_t \) represents the state at time \( t \). The operator \( G \) can be approximated using a neural network model \( F_\phi(x) \), parameterized by learnable variables \( \phi \). Such an approximation is backed by the universal approximation theorem for operators\citep{chen1995universal}. These parameters of \( F_\phi(x) \) are optimized by minimizing the discrepancy from the ground truth data (indexed discretely by \( j \)) using a one-step loss function defined as:
\begin{equation}\label{MSE-1}
L_1 = \mathbb{E}_j\left[\left\|F_{\phi}(x_j) - G(x_j)\right\|^2\right].
\end{equation}
A commonly employed multi-rollout loss function\citep{keisler2022forecasting}, \( L_R \), utilized in training many state-of-the-art models, is defined as:
\begin{equation}\label{MSE-2}
L_R = \mathbb{E}_j \left[\sum_{t=1}^{t=m} \left\| \gamma(t) \big(F_{\phi}^t(x_j) - G^t(x_j)\big)\right\|^2\right],
\end{equation}
where \( m \) denotes the number of rollouts included during training, and \( \gamma(t) \)
% \footnote{\footnotesize We use \( \gamma(t) = 0.9^{t-1} \) empirically following \citep{schiff2024dyslim}.} 
is a hyperparameter that assigns diminishing weights to errors in trajectories further along in time\citep{kochkov2023neural}. It has an effect similar\footnote{\footnotesize Although the discount factor in RL is unrelated directly to the \( \gamma(t) \) used here, there might be interesting theoretical connections which we leave for future exploration.} to the discount factor used in reinforcement learning(RL) \citep{amit2020discount}. Furthermore, to enhance computational efficiency and improve stability, the \textit{Pushforward Trick}, introduced in \citep{brandstetter2022message}, is often used. This approach reduces computational overhead by detaching the computational graph at intermediate rollouts. However, such methods alone cannot address neither the phenomenon of spectral bias of neural networks nor stability \citep{chakraborty2024divide,schiff2024dyslim}. 

\subsection{Spectral Bias in Operator Learning}
\citep{rahaman2019spectral} showed that a combination of the theoretical properties of gradient descent optimization, the architecture of neural networks, and the nature of function approximation in high-dimensional spaces causes the network to learn lower frequencies faster and more effectively. Mathematically, for $N$ samples in a training batch, Equation \ref{MSE-1} can be estimated as,
\begin{equation}\label{MSE-1-act}
L_1 = \frac{1}{N}\sum_{j=0}^N\left\|F_{\phi}(x_j) - G(x_j)\right\|^2.
\end{equation}
The gradient of this loss function with respect to parameters $\phi$ is
\begin{equation}\label{MSEgrad}
    \nabla_\phi L_1 = \frac{2}{N}\sum_{j=0}^N\left(F_{\phi}(x_j) - G(x_j)\right)\nabla_\phi F_\phi(x_j)
\end{equation}
which may be utilized in a gradient descent algorithm as 
\begin{equation}
    \phi_{k+1} = \phi_k - \alpha \nabla_\phi L_1
\end{equation}
where $\alpha$ is the learning rate. 
Intuitively, gradient descent naturally favors changes that yield the most substantial reduction in loss early in training. In the spectral space, this is reflected in the components that have higher values in the Fourier series representation of $F_\phi$ \citep{oommen2024integrating}. This causes the lower frequencies to be learned first which correspond to global patterns that tend to dominate the error landscape in the initial phases of training. For more details, readers are directed to Section 3 in \citep{rahaman2019spectral} and Section 4.1 in \citep{oommen2024integrating}.

\subsection{Energy Spectrum}\label{Sec:Energy_Spec}
The energy spectrum $E(k)$ characterizes the distribution of energy among different frequency or wavenumber components\citep{kolmogorov1941local}. In our work the Fourier Transform is always taken spatially. However, we use the terms frequency and wavenumber interchangeably henceforth. The energy spectrum provides insights into the distribution of energy across different scales. For an arbitrary field $u(x)$(can be $F_\phi(x)$ or $G(x)$ from Equation \ref{MSE-1}) in a periodic domain of length $L$, the \textit{Fourier transform} $\mathcal{F}$ is defined as:
\begin{equation}
    \hat{u}(k) =\mathcal{F}(u(x)) =  \frac{1}{L} \int_0^L u(x) e^{-ikx} dx,
\end{equation}
where $\hat{u}(k)$ represents the spectral coefficients corresponding to wavenumber $k$. For higher-dimensional fields $u(x,y,t)$ or $u(x,y,z,t)$, the Fourier transform is extended to multiple dimensions, and the energy density is computed by summing over all wavevectors of the same magnitude:
\begin{equation}
    E(k) = \frac{1}{2} \sum_{|\mathbf{k}| = k} |\hat{u}(\mathbf{k})|^2,
\end{equation}
where $\mathbf{k} = (k_x, k_y, k_z)$ is the wavevector, and summation is performed over spherical shells in Fourier space. In computational settings, we often work with discretized fields defined on a uniform grid. The discrete Fourier transform (DFT) is used to approximate the energy spectrum:
\begin{equation}
    \hat{u}(\mathbf{k}) = \frac{1}{N} \sum_{n=0}^{N-1} u_n e^{-i 2\pi kn/N},
\end{equation}
where $N$ is the number of grid points. For handling discrete wavenumbers in computational grids, binning helps to efficiently average the energy over wavenumber shells, ensuring a smooth representation of the spectrum. The magnitude of each wavenumber $k$ is given as 
\begin{equation}\label{wavenumber_mag}
    k = \sqrt{k_x^2+k_y^2+k_z^2}
\end{equation}
The bins can be logarithmically or linearly spaced. In our experiments, we use linearly spaced bins for computing the energy contributions into wavenumber shells as:
\begin{equation}\label{eq:binned_specra}
    E(k) = \sum_{k - \Delta k/2 \leq |\mathbf{k}| < k + \Delta k/2} \frac{1}{2} |\hat{u}(\mathbf{k})|^2,
\end{equation}
where $\Delta k$ is the width of the bin. In several scenarios, a major portion of the energy is stored in the lower wavenumbers, highlighted by the rapid decay of their energy spectrum. However, in complex real-world systems, the energy spectrum typically exhibits a slow decay, preserving substantial energy and valuable information at higher wave numbers. For example, in weather data, the small and intermediate scale details correspond to anomalies like initial phases of storms \citep{ritchie1997scale}, especially in a model with coarser grids.

\subsection{Related Works}
The challenges of learning the evolution of high frequency structures in spatiotemporal dynamical systems has been given considerable study in recent literature \citep{karniadakis2021physics,lai2024machine,chakraborty2024divide,chen2024physics}. Researchers have tried to solve this problem through various instruments, in particular, by modifying the deep neural network architecture. \citep{liu2024mitigating} proposed a Hierarchical Attention Neural Operator (HANO) inspired by multilevel matrix methods, featuring hierarchical self-attentions and local aggregations to effectively capture multi-scale features. By leveraging insights from diffusion models, \citep{lippe2023modeling} proposed the PDE-Refiner which iteratively refines predictions, focusing on modeling both dominant and low-amplitude spatial frequency components. Hybrid methods combining classical numerical methods and deep learning has also been used to capture both lower and higher modes of the energy spectrum accurately \citep{shankar2023differentiable,zhang2024blending}. Other techniques like multiscale networks \citep{wang2020multi,liu2020multi} and diffusion models \citep{oommen2024integrating} have also been explored for the same. However, these techniques heavily exploit architectural modifications that are difficult to devise and frequently require significant computational overhead. 
% For example, diffusion models come with additional cost both during training and inference.

Another intuitive solution to the problem of capturing the fine scales can be to penalize the mismatch of the Fourier transform of the model outputs from the ground truth \citep{chattopadhyay2024oceannet,guan2024lucie,kochkov2023neural}. This is typically done by a mean absolute error loss in the Fourier space :
\begin{equation}\label{MSE-Fspace}
L_f = \frac{1}{N}\sum_{j=0}^Nw_j \left\|\mathcal{F}(F_{\phi}(x_j)) - \mathcal{F}(G(x_j))\right\|.
\end{equation}
where $\mathcal{F}$ is the Fourier transform, and $w_j$ is a hyperparameter used to weigh or cut-off some modes. 
% Although this seems promising in their applications, the effect of Equation \ref{MSE-Fspace} is same as the loss function given in Equation \ref{MSE-1-act} since Fourier transforms are \textcolor{red}{$L^2$} isometries. Consequently, it will also be heavily biased to the larger values in the Fourier spectrum which correspond to lower frequency modes.
It is evident that Equation \ref{MSE-Fspace} will also be heavily biased towards the larger values in the Fourier spectrum which typically correspond to the lower frequency modes. Consequently, the effect of Equation \ref{MSE-Fspace} is same as the loss function in Equation \ref{MSE-1-act}. To overcome this, \citep{chattopadhyay2024oceannet} used a cutoff to empirically ignore some of the lower frequencies. However, for the higher frequencies with extremely low values, it is not judicious to try to match them exactly in a point wise manner. In the following section we come up a new strategy to solve the mentioned problems without modifying the network architecture or incurring a heavy cost during training and inference.


\section{Methodology}

\begin{algorithm}[!h]
   \caption{Binned Spectral Power (BSP) Loss Computation}
   \label{alg:spectral_loss}
\begin{algorithmic}
   \REQUIRE $u_j, v_j \in \mathbb{R}^{C \times H \times W \times ...}$, $\epsilon > 0$, $\lambda_i$
   \ENSURE $L_\text{spec}^{(j)}$
   \STATE $\hat{u} \leftarrow \mathcal{F}(u_j)$, $\hat{v} \leftarrow \mathcal{F}(v_j)$ \hfill \textit{\# Fourier transform}
   \STATE $E_u \leftarrow \frac{1}{2} |\hat{u}|^2$, $E_v \leftarrow \frac{1}{2} |\hat{v}|^2$ \hfill \textit{\# Energy computation}
   \STATE $k \leftarrow \sqrt{k_x^2 + k_y^2 + ...}$ \hfill \textit{\# Magnitude of wavenumbers}
   \STATE \textit{\# Spatial binning of energy}
   \FOR{$i = 1$ to $N_k$}                
      \STATE $E_u^\text{bin}(c, i) \leftarrow \frac{1}{N_i} \sum_{k \in \text{bin}(k)} E_u(c, k)$
      \STATE $E_v^\text{bin}(c, i) \leftarrow \frac{1}{N_i} \sum_{k \in \text{bin}(k)} E_v(c, k)$
   \ENDFOR
   \STATE $L_\text{BSP}^{(j)} \leftarrow \frac{1}{N_k} \sum_{c=1}^C \sum_{i=1}^{N_k} \lambda_i \left( 1 - \frac{E^{\text{bin}}_u(c,i)+ \epsilon}{E^{\text{bin}}_v(c,i) + \epsilon} \right)^2$
\end{algorithmic}
\end{algorithm}

We introduce a novel Binned Spectral Power (BSP) loss function mentioned in Algorithm \ref{alg:spectral_loss}. This is designed to evaluate discrepancies between predicted and target data fields by comparing their spatial energy spectra at different scales. We reuse the concept of energy spectrum mentioned in Section \ref{Sec:Energy_Spec}. First, the predicted and target samples are transformed into the wavenumber domain using the Fourier transform. The magnitudes of energy components are computed by squaring the Fourier coefficients. The wavenumber magnitudes are then computed using Equation \ref{wavenumber_mag} to group spatial frequency components into scalar values. The energy components are binned by wavenumber ranges, averaging the energy within each bin $E^{bin}$ using Equation \ref{eq:binned_specra}. Here every bin $(k)$ is defined as $(k - \Delta k/2 )\leq |\mathbf{k}| < (k + \Delta k/2)$. The BSP loss is calculated by comparing the binned energy spectra of the predicted and target samples. 

Unlike traditional loss functions like Mean Squared Error (MSE), which operate point-wise in the physical domain, the BSP loss provides a robust learning of the various scales in the data, as explained in the following. To ensure the accurate capturing of different scales we aim to get the ratio of the energy in different bins close to identity. This squared relative error loss is successful to provide equal weights to energy component at all wavenumber bins. The BSP Loss is defined as:
\begin{equation}
    L_\text{BSP}(u,v) = \frac{1}{N_k} \sum_{c=1}^C \sum_{i=1}^{N_k} \lambda_i \left( 1 - \frac{E^{\text{bin}}_u(c,i)+ \epsilon}{E^{\text{bin}}_v(c,i) + \epsilon} \right)^2
\end{equation}
where $N_k$ is the number of bins, $i$ refers to a specific bin spanning a range of wavenumbers, and $C$ is the number of features  (channels) in input $u$ and target $v$. $\epsilon$ is used to eliminate the effect of extremely small values in $E^{bin}$. A parameter $ \lambda_i$ is used to variably weight different bins based on the requirements of the application. For computational purposes, we empirically suggest to use predicted values and true values as $u$ and $v$ respectively in $L_{BSP}$. The algorithm can be written in a differentiable programming language to efficiently compute the gradients required to minimize the BSP loss. A differentiable histogram can also be used to efficiently perform the binning using latest libraries like Jax \citep{jax2018github}. 

The BSP loss can be combined with the multi-step rollout loss given in Equation \ref{MSE-2} for short term accuracy, long term stability and spectral bias mitigation.
\begin{equation}\label{Loss eqn}
L_R^* = \mathbb{E}_j \left[\sum_{t=1}^{t=m} \left\| \gamma(t) \big(F_{\phi}^t(x_j) - G^t(x_j)\big)\right\|^2 +  \mu L_{BSP}^{(j,t)}\right]
\end{equation}
where
\begin{equation}
    L_{BSP}^{(j,t)} = L_{BSP}(F_{\phi}^t(x_j),G^t(x_j))
\end{equation}
is the BSP loss at $t^{th}$ autoregressive rollout step of the model and $\mu$ is a hyper-parameter that is used to weigh the two loss terms differently. The gradient of the BSP loss is
\begin{equation}
        \begin{aligned}
            \nabla_\phi L_\text{BSP} = &
            \frac{-2}{N} \sum_{j=1}^N \frac{1}{N_k} \sum_{i=1}^{N_k} \sum_{c=1}^C \lambda_i \\ & \left( 1 - 
            \frac{E^{bin}_F(c,i) + \epsilon}{E^{bin}_G(c,i) + \epsilon} \right)
          \frac{\nabla_\phi E^{bin}_F(c,i)}{E^{bin}_G(c,i) + \epsilon}
        \end{aligned}
\end{equation}
It can be shown, following a similar treatment as for the MSE Loss (Equation \ref{MSEgrad}) in Section 4.1 from \citep{oommen2024integrating}, that the ratio term present in the gradient of the BSP loss leads to equal importance to all ranges of the energy spectrum. However, combining the BSP loss with the mean square error loss gives slightly higher importance to the lower wavenumbers, which is desirable as they contain the maximum energy. The weight $\mu$ can be adjusted to compensate for this when needed. The BSP loss mentioned henceforth is the combined MSE + BSP loss mentioned in Equation \ref{Loss eqn}. 

\subsection{Complexity}
The BSP loss introduces minimal computational overhead compared to the baseline objectives. The additional cost of the BSP loss scales linearly with batch size($n_b$) and quasi-linearly with the state dimension ($d$). The quasi-linear cost comes from the FFT computation and assuming that $N_k<<d$ for the binning. As shown in Table \ref{Comp_table}, the space and time complexity of the BSP loss is better than the MMD loss mentioned in \citep{schiff2024dyslim}.
\begin{table}[!h]
\vskip -0.1in
\caption{Computational complexity and memory footprint for various objectives. Here, $d$ represents the state dimension, $|\phi|$ denotes the number of neural network parameters, $NN$ is the complexity of a single neural network evaluation, $n_b$ is the batch size, and $n_t$ is the number of maximum rollout steps. MSE$_1$ and MSE$_t$ represent mean squared error objectives for 1-step and multi-step rollouts. Pfwd is the push-forward trick \citep{brandstetter2022message}. MMD is the maximum mean discrepancy loss mentioned in \citep{schiff2024dyslim}. The complexity of BSP loss is computed assuming that the number of bins $N_k << d$.}
\label{Comp_table}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcc}
\toprule
Objective & Cost $\mathcal{O}(\cdot)$ & Memory $\mathcal{O}(\cdot)$ \\
\midrule
MSE$_1$  & $n_b d + n_b NN$        & $n_b d + n_b |\phi|$ \\
MSE$_t$  & $n_t n_b d + n_t n_b NN$ & $n_t n_b d + n_t n_b |\phi|$ \\
Pfwd     & $n_b d + n_b NN$        & $n_b d + n_b |\phi|$ \\
MMD      & $n_b^2 d + n_b NN$       & $n_b^2 d + n_b |\phi|$ \\
BSP      & $n_b d \log d + n_b NN$  & $n_b d + n_b |\phi|$ \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\section{Experiments}
We test our proposed methodology for several benchmark problems. These experiments aim to test the capabilities of our proposed loss function function to preserve the small scale structures when applied to high-dimensional dynamical systems using existing deep learning architectures.



\subsection{Mitigating the Spectral Bias}\label{SB_section}
\begin{figure}[!h]
\vskip 0.2in
    \begin{center}
        \centerline{\includegraphics[width=0.99\linewidth]{images/spec_bias_inc_amp_func.png}}
    \caption{Evolution of function approximation over training iterations. The top row shows predictions using BSP Loss, while the bottom row shows predictions using just MSE Loss. Each column represents different training iterations (2000, 31000 60000). BSP Loss leads to a more accurate reconstruction of true function $g(x)$, whereas MSE Loss struggles.}
    \label{fig:spec_bias_inc_amp_func}
    \end{center}
    \vskip -0.2in
\end{figure}
In this section, we perform a synthetic experiment following \citep{rahaman2019spectral} and aim to mitigate the spectral bias with our BSP loss function. Given a set of frequencies $\kappa = (k_1, k_2, \dots)$ with corresponding amplitudes $\alpha = (A_1, A_2, \dots)$ and phases $\phi = (\phi_1, \phi_2, \dots)$, they define the mapping $g: [0,1] \to \mathbb{R}$ as:

\begin{equation}
    g(x) = \sum_{i} A_i \sin(2\pi k_i x + \phi_i).
\end{equation}
We use the architecture mentioned in \citep{rahaman2019spectral} - a 6-layer deep, 256-unit wide ReLU neural network $f_{\phi}$ which approximates $g$. The frequency components used are $\kappa = (5, 10, \dots, 45, 50)$ with amplitudes ranging from $0.08$ to $1.2$. The amplitude values are set such that the increase to a maximum value and then decrease (refer Figure \ref{fig:spec_bias_inc_amp_fft}). $N = 200$ input samples uniformly spaced over $[0,1]$ are used for training the network. We compare the same model architecture, one trained with MSE loss and the other trained with BSP loss for 60000 iterations. Since this is a synthetic 1D problem where the wavenumbers corresponds directly to a mode in the Fourier transform, binning is unnecessary. In higher dimensions, the energy spectrum is typically computed in terms of an isotropic wavenumber (refer Equation \ref{wavenumber_mag}), by summing over modes with similar wavenumbers in each dimension, which refers to binning in a Cartesian grid. Therefore, in this case the simplified version of the BSP loss used is
\begin{equation}\label{Syn_bsp}
L =  \left\|f_{\phi}(x) - g(x)\right\|^2 +  \mu \left[ 1 - \frac{\|\mathcal{F}(f_{\phi}(x))\|+ \epsilon}{\|\mathcal{F}(g(x))\|+ \epsilon} \right]^2
\end{equation}
where $\mathcal{F}$ is the Fourier transform. We use a value of $\mu=5$ and $\epsilon=1$ . We do an ablation study of the hyperparameters in Appendix \ref{ablation_sec}.

\begin{figure}[!h]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{images/spec_bias_inc_amp_fft.png}}
\caption{Frequency domain representation of function approximations over training iterations. The top row shows the Fourier Transform of predictions using BSP Loss, while the bottom row shows predictions using MSE Loss. Each column represents different training iterations (2000, 31000, 60000). BSP Loss captures the high-frequency components of the true function $g(k)$ more accurately compared to MSE Loss, especially in early and intermediate stages of training.}
\label{fig:spec_bias_inc_amp_fft}
\end{center}
\vskip -0.2in
\end{figure}
\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{images/spec_bias_mse.png}}
\caption{Mean Squared Error (MSE) from the true function - $g(x)$ over training iterations. The model trained with BSP Loss (blue) exhibits a faster convergence compared to the MSE training approach (orange) and the FFT loss (green) mentioned in Equation \ref{MSE-Fspace} \citep{chattopadhyay2024oceannet}.}
\label{fig:spec_bias_mse}
\end{center}
\vskip -0.2in
\end{figure}


The impact of BSP Loss on function approximation and frequency learning is evident across the training iterations. As shown in Figure~\ref{fig:spec_bias_inc_amp_func}, the model trained with BSP Loss reconstructs the true function $g(x)$ with higher accuracy compared to those trained with MSE Loss, particularly in the earlier training stages. The advantage of BSP Loss is further highlighted in Figure~\ref{fig:spec_bias_inc_amp_fft}, where its Fourier Transform representations capture high-frequency components of the true function $g(k)$ more effectively than MSE Loss, which struggles to learn these components. Additionally, in Figure~\ref{fig:spec_bias_mse} we indicate the Mean Squared Error (MSE) throughout training iterations for the MSE loss, the BSP loss and the FFT regularizer mentioned in \citep{chattopadhyay2024oceannet}. Although the FFT loss performs slightly better than just using the MSE loss, BSP clearly outperforms all of them illustrating its superior convergence properties. Additionally we would like to mention that we can not use the MMD loss here as it is a simple function approximation task and there is no concept of underlying distribution or attractor (in other words, we do not have any batches to compute the MMD). These results collectively demonstrate that BSP Loss mitigates spectral bias and enhances function approximation by preserving the higher-frequency information in the learning process.




\subsection{Two-dimensional turbulence}
\begin{figure}[!h]
    \centering
    \centerline{\includegraphics[width=1.2\linewidth]{images/Turb_2D.png}}
    \vspace{-1cm}
    \caption{Vorticity fields for various model predictions compared with ground truth(bottom) at different timesteps (left to right). For shorter time scales(t=0,5,15) all models are stable and visually accurate. The NODE and MP-NODE models have clear blurring effect unlike the DCNN+MMD and DCNN+BSP models. At t=100, none of the models match the ground truth as it is a chaotic system. However, the DCNN model(trained with just MSE) clearly does not have physical predictions. The DCNN + MMD Loss model has better performance at intermediate timescales but it shows unstable behavior at t=900 shown by the blank images having $nan$ values. \textbf{Gist:} The DCNN+BSP loss model shows better physical consistency without losing any spatial information(blurring) at all timesteps.}
    \label{fig:2d turb}
\vskip -0.1in
\end{figure}

\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{images/Spec_2D.png}}
\caption{The time-averaged energy spectrum comparison for ground truth and various model predictions. The total timesteps considered for ground truth, NODE, MP-NODE, and DCNN + BSP loss is 900. For DCNN (trained with just MSE), we only use the first 100 timesteps as it becomes unstable after that. The time-averaged energy spectrum for DCNN+MMD loss matches the ground truth closely for timesteps 0 to 100. However, the DCNN+BSP loss matches the ground truth energy spectrum best for the entire length of the test trajectory (900 timesteps). The ensemble, shown as the shaded region, represents variations with respect to different initial conditions.}
\label{fig:spec}
\end{center}
\vskip -0.2in
\end{figure}


\begin{figure}[ht]
\vskip -0.11in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{images/Corr_2D.png}}
\caption{The correlation with DNS comparison for various models. The BSP loss does not improve the correlation significantly, unlike the MMD loss. The ensemble, shown as the shaded region, represents variations with respect to different initial conditions.}
\label{fig:correlation}
\end{center}
\vskip -0.2in
\end{figure}


 Forced two-dimensional turbulence, which exhibits the classical characteristics of chaotic dynamics, has become a standard benchmark for machine learning algorithm development in dynamical system prediction~\citep{stachenfeld2021learned,schiff2024dyslim,frerix2021variational}. In this section, we evaluate the performance of the proposed loss function on two-dimensional homogeneous isotropic turbulence with Kolmogorov forcing, governed by the incompressible Navier-Stokes equations. Further details on the dataset is provided in Appendix \ref{2D_turb_data}. 
 
 The baseline models are trained using the multi-step rollout loss function mentioned in Equation \ref{MSE-2} and the \textit{pushforward-trick}. The architectures used for this experiment is the dilated Convolutional Neural Network (DCNN) \citep{stachenfeld2021learned}. Further details on the model hyperparemters are mentioned in Appendix \ref{hyperparams}.  We also use the DCNN model trained with Maximum Mean Discrepancy (MMD) loss \citep{schiff2024dyslim} as a benchmark architecture. This loss function attempts to learn the attractor of the underlying dynamical system which improves its stability. We use similar hyperparameters as mentioned in \citep{schiff2024dyslim} due to the similarity in the dataset. However, we use a smaller time horizon while training and fewer number of parameters in the model to be consistent with other models. Moreover, we also use Neural Ordinary Differential Equations (NODE) \citep{chen2018neural}, both with and without the Multi-step Penalty (MP) loss function (MP-NODE) \citep{chakraborty2024divide} as a benchmark architecture. For the NODE and MP-NODE, we directly use the results from \citep{chakraborty2024divide}. Details on these architectures and loss functions are mentioned in Appendix \ref{baselines}.
 
 In Figure \ref{fig:2d turb}, our experiments show that the DCNN model(trained with just MSE) predictions are unstable at longer rollouts as observed in previous works \citep{schiff2024dyslim}. The DCNN model trained with MMD Loss (DCNN + MMD) performs better in intermediate time scales (t=100). However, it gets unstable on auto-regressively rolling out even further in time (t=900). For the first 100 timesteps, the predictions of DCNN + MMD match the energy spectrum of the ground truth closely, but divergence is observed eventually (see Figure \ref{fig:spec})). The larger values at higher wavenumbers cause both DCNN and DCNN + MMD models become unstable. Here instability is seen to arise from not identifying the right energy at the finer frequencies \citep{maulik2019subgrid}. We also observe that the NODE and MP-NODE models are stable, but they cannot preserve the small structures in the flow, which is a key requirement for accurate forecasting of such systems. However, the predictions of the DCNN model trained with BSP loss (DCNN + BSP) stay stable and physical for the entire forecast trajectory preserving both larger and smaller scale features of the flow. Figure \ref{fig:spec} also shows that it is actually capable of preserving the energy spectrum (an invariant metric) of the flow for the entire trajectory. We would also like to highlight that unlike the MMD loss, the BSP loss does not minimize distance in the physical space. This causes it to have no significant improvement in metrics like correlation (see Figure \ref{fig:correlation}). However, for chaotic systems like turbulence which is inherently stochastic, invariant metrics like energy spectrum are more significant than short term metrics like correlation.
 % We would like to highlight that we can achieve this using a much lower number of timesteps in training than previous works which use 10 timesteps \citep{schiff2024dyslim}.  




\subsection{3D Turbulence}
\begin{figure}[ht]
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{images/Spec_3D.png}}
\caption{Comparison of energy spectra $E(k)$ as a function of wavenumber at different time steps ($T = 1, 15, 30$) and averaged over time. The plots show results from DNS (blue solid line), UNet (orange dashed line), and UNet model trained with BSP loss (green dashed line), along with the theoretical $k^{-5/3}$ scaling (red solid line). The inclusion of BSP improves the spectral accuracy at high wavenumbers compared to the standalone UNet approach.}
\label{fig:3dspec}
\end{center}
\vskip -0.2in
\end{figure}

This experiment uses data from a three-dimensional direct numerical simulation (DNS) of incompressible, homogeneous, isotropic turbulence \citep{mohan2020spatio}. Further details of this dataset are mentioned in Appendix \ref{data_3d_trub}. We use a UNet based architecture for both MSE and BSP loss implementation.The hyperparemters of the model is mentioned in Appendix \ref{hyperparams}. We observe here that both the models show minimal spectral bias and improved stability. 
% (given the limited length of test trajectory available).
This is related to the reduced spectral bias of models with larger parameter space(refer Appendix A.5. in \citep{rahaman2019spectral}). However, it is evident from Figure \ref{fig:3dspec} that the BSP loss shows a marked accuracy in the energy spectrum at high wavenumbers, corresponding to dynamically important small-scale structures in chaotic systems. Moreover, we present two more tests to further explore the performance of our method in Appendix \ref{data_3d_trub}. With these evidences, we can conclude that by matching the energy spectrum the BSP loss helps in preserving the distribution of energy across different scales and spatial structures.

\section{Conclusion}
Capturing features across a wide range of spatial and temporal scales in complex, real-world dynamical systems is a significant challenge for data-driven forecasting techniques. While recent studies have started to address the issue, they often require specialized neural architectures or end up adding substantial computational costs both during training and forecasting. To address this, we introduce a novel Binned Spectral Power (BSP) loss function that steps away from point-wise comparisons in the physical domain and instead measures differences in terms of spatial energy distributions. By applying a Fourier transform to the input fields and binning the magnitude of the Fourier coefficients by wavenumbers, we minimize discrepancies between the predicted fields and target data across multiple scales. The BSP loss offers a more balanced and efficient way to capture both large and small features without heavily modifying the model or incurring significant extra costs. Our experiments demonstrate that we can effectively reduce the spectral bias of neural networks in function approximation. We also showcase the advantages of BSP loss using challenging test cases such as turbulent flow forecasting. These results empirically show that the BSP loss function improves the ability of a neural network model to mitigate spectral bias and capture information at different scales in the data.

\textbf{Limitation :} We would like to emphasize that it is non-trivial to define the BSP loss in an unstructured grid. As demonstrated in Appendix \ref{airfoil}, when applied to a problem with a non-uniform grid using interpolation, the resulting improvement is minimal. While we propose some potential solutions there, addressing this challenge in a broader context remains an avenue for future research.

\bibliography{templateArxiv}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Baseline Models and Loss Functions}\label{baselines}
\subsection{Dilated Convolutional Neural Networks}
Dilated Convolutional Neural Networks (DCNNs) enhance traditional convolutional layers by introducing a dilation rate $d$ into the convolution operation. This allows the receptive field to expand exponentially without increasing the number of parameters. This architecture is used in several dynamical systems forecasting models\citep{schiff2024dyslim,chai2024overcoming,stachenfeld2021learned}.

In our work we use the architecture similar to \citep{schiff2024dyslim}. It has an encoder, CNN blocks, and a decoder. The Encoder first transforms the input through two Convolutional layers with circular padding and GELU activation, ensuring smooth feature extraction. The CNN block then applies a sequence of dilated convolutions with varying dilation rates [1,2,4,8,4,2,1], allowing the network to efficiently capture both local and long-range dependencies while preserving resolution. A residual connection is added to stabilize learning and maintain input information. We employ 4 such CNN blocks. The Decoder then reconstructs the output using a couple of Convolutional layers with circular padding. The model operates recursively over multiple rollout steps, where each prediction is fed back into the network, making it particularly effective for sequence forecasting tasks.


\subsection{Maximum Mean Discrepancy (MMD) Loss}
Maximum Mean Discrepancy (MMD) used in \citep{schiff2024dyslim} is a statistical measure that quantifies the difference between two probability distributions in a reproducing kernel Hilbert space (RKHS). Given two distributions $P$ and $Q$ over a space $\mathcal{X}$, the squared MMD is defined as:

\begin{equation}
\mathrm{MMD}^2(P, Q) = \mathbb{E}_{x, x' \sim P}[k(x, x')] + \mathbb{E}_{y, y' \sim Q}[k(y, y')] - 2\mathbb{E}_{x \sim P, y \sim Q}[k(x, y)],
\end{equation}
where $k: \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ is a positive-definite kernel. In the context of chaotic systems, MMD loss is used to match the empirical invariant measure $\mu$ with the learned distribution $\hat{\mu}$. Given observed samples $\{x_i\}_{i=1}^{N}$ and generated samples $\{\hat{x}_j\}_{j=1}^{M}$, the empirical MMD estimate is:
\begin{equation}
\hat{\mathrm{MMD}}^2 = \frac{1}{N^2} \sum_{i,j} k(x_i, x_j) + \frac{1}{M^2} \sum_{i,j} k(\hat{x}_i, \hat{x}_j) - \frac{2}{NM} \sum_{i,j} k(x_i, \hat{x}_j).
\end{equation}
Minimizing this loss ensures that the learned model captures the long-term statistical properties of the chaotic system.


\subsection{Neural Ordinary Differential Equations}
Neural Ordinary Differential Equations (NODEs) provide a continuous-time approach to modeling dynamic systems by parameterizing the derivative of the state variable using a neural network \citep{chen2018neural}. It is described as follows:
\begin{equation}
\begin{aligned}
        &\frac{d\textbf{u}(t)}{dt} = \mathcal{R}(\textbf{u}(t),t,\boldsymbol{\Theta}), \quad \text{for} \quad t \in [t_0, T],
\end{aligned}
\end{equation}
where \( \mathcal{R}(\textbf{u}(t),t,\boldsymbol{\Theta}) \) is a neural network parameterized by \( \boldsymbol{\Theta} \). The initial condition is given as:
\begin{equation}
    \textbf{u}(t_0) = \textbf{u}_0.
\end{equation}
The solution \( \textbf{u}(t) \) is obtained by integrating the system over time using numerical solvers such as Euler's method or higher-order solvers like Runge-Kutta. In our case it can be the state of the dynamical system. The parameters \( \boldsymbol{\Theta} \) are learned by minimizing a loss function (typically MSE from ground truth) using backpropagation through the solver or with the adjoint method. Neural ODEs are particularly useful for modeling time-series data, continuous normalizing flows, and various physical systems where the dynamics are governed by differential equations \citep{chen2018neural}. Their continuous nature provides a flexible alternative to traditional discrete-layer neural networks.

\subsection{Multi-step Penalty Neural ODE}
The Multi-step Penalty Neural ODE (MP-NODE) is formulated by \citep{chakraborty2024divide} as:
\begin{equation}
\begin{aligned}
        &\frac{d\textbf{u}(t)}{dt} - \mathcal{R}(\textbf{u}(t),t,\boldsymbol{\Theta}) = 0, \quad \text{for} \quad t \in [t_k, t_{k+1}) \\
        &\textbf{u}(t_k) = \textbf{u}_k^+, \quad \text{for} \quad k = 0, \ldots, n-1.
\end{aligned}
\end{equation}
The corresponding loss function incorporates a penalty term and is expressed as:
\begin{equation}\label{MP_loss_equation}
    \mathcal{L} = \mathcal{L}_{GT} + \frac{\mu}{2} \mathcal{L}_{P},
\end{equation}
where:
\begin{equation}
   \mathcal{L}_{GT} = \frac{\sum_{i=1}^{N} |\textbf{u}_i - \textbf{u}_i^{true}|^2 }{2N}, \quad \mathcal{L}_{P} = \frac{\sum_{k=1}^{n-1} |\textbf{u}_k^+ - \textbf{u}_k^-|^2 }{n-1},
\end{equation}
represent the loss with respect to ground truth and the penalty loss enforcing continuity, respectively. For \( k = 1,2,\dots,n \), the term \( \textbf{u}_k^- \) is computed as:
\begin{equation}
    \textbf{u}_k^- = \textbf{u}_{k-1} + \int_{t_{k-1}^+}^{t_k^-} \mathcal{R}(\textbf{u}(t),t,\boldsymbol{\Theta})\,dt.
\end{equation}
The penalty strength \( \mu \)(here) plays a critical role in handling local discontinuities (quantified by \( |\textbf{u}_k^+ - \textbf{u}_k^-| \)). The update strategy for \( \mu \) follows a heuristic approach, where adjustments are made based on the observed loss curves \citep{chung2022optimization}. \citep{chakraborty2024divide} show that the MP-NODE performs better for forecasting of chaotic systems.




\section{Additional Information and Experiments}
\subsection{Kolmogorov Flow}\label{2D_turb_data}
\textbf{Dataset :} The two-dimensional Navier-Stokes equations are given by:

\begin{equation}
\begin{aligned}
\frac{\partial \mathbf{u}}{\partial t} + \nabla \cdot (\mathbf{u} \otimes \mathbf{u}) &= \frac{1}{Re} \nabla^2 \mathbf{u} - \frac{1}{\rho} \nabla p + \mathbf{f}, \\
\nabla \cdot \mathbf{u} &= 0,
\end{aligned}
\end{equation}

where $\mathbf{u} = (u, v)$ is the velocity vector, $p$ is the pressure, $\rho$ is the density, $Re$ is the Reynolds number, and $\mathbf{f}$ represents the forcing function, defined as:

\begin{equation}
\mathbf{f} = A \sin(ky)\hat{\mathbf{e}} - r\mathbf{u},
\end{equation}

with parameters $A = 1$ (amplitude), $k = 4$ (wavenumber), $r = 0.1$ (linear drag), and $Re = 1000$ (Reynolds number) selected for this study as given in \citep{shankar2023differentiable}. Here, $\hat{\mathbf{e}}$ denotes the unit vector in the $x$-direction. The initial condition is a random divergence-free velocity field~\citep{Kochkov2021-ML-CFD}.
The ground truth datasets are generated using direct numerical simulations (DNS)~\citep{kochkov2021machine} of the governing equations within a doubly periodic square domain of size $L = 2\pi$, discretized on a uniform $512 \times 512$ grid and filtered to a coarser $64 \times 64$ grid. The trajectories are sampled temporally after the flow reaches the chaotic regime, with snapshots spaced by $T = 256\Delta t_{DNS}$, ensuring sufficient distinction between consecutive states. Details of the dataset construction can be found in the work by \citep{shankar2023differentiable}.



\subsection{3D Homogeneous Isotropic Turbulence}\label{data_3d_trub}
\begin{figure*}[!h]
    \centering
    \includegraphics[width=0.3\linewidth]{images/3D_true.png}
    \includegraphics[width=0.3\linewidth]{images/3D_pred.png}
    \includegraphics[width=0.3\linewidth]{images/3D_pred_spec.png}
    \caption{Velocity magnitude 3D plot for ground truth(left), UNet prediction(mid), and UNet + BSP loss prediction(right) after 5 auto-regressive rollouts. Clearly the UNet prediction has some blurring effect compared to other two.}
    \label{fig:3D_turb_diagram}
\end{figure*}
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.89\linewidth]{images/intermittency_3D.png}
    \caption{The figure illustrates the comparison of the intermittency plots for UNet models trained with MSE loss (orange) and UNet trained with BSP loss (green) across different time steps (T). }
    \label{fig:3dintermittency}
\end{figure}
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.99\linewidth]{images/qr_3D.png}
    \caption{The figure illustrates the comparison of the QR plots for UNet models trained with MSE loss (orange) and UNet trained with BSP loss (green) across different time steps (T) and resolutions (r). The QR plots signify the theree dimensional chaos in turbulence.}
    \label{fig:3dqr}
\end{figure}
\textbf{Dataset :} The computational domain is a cubic box with dimensions of $128^3$ grid points. Two scalar fields, each with distinct probability density function (PDF) characteristics, are advected as passive scalars by the turbulent flow. This dataset is taken from \citep{mohan2020spatio}. They refer to this dataset as \textit{ScalarHIT}, following \citep{daniel2018reaction}. The DNS is performed with a pseudo-spectral code, ensuring incompressibility via
\begin{equation}
\partial_{x_i} v_i = 0,
\end{equation}
and solving the Navier--Stokes equations
\begin{equation}
\partial_t v_i + v_j \partial_{x_j} v_i = -\frac{1}{\rho}\partial_{x_i} p + \nu \nabla^2 v_i + f^v_i.
\end{equation}
Low-wavenumber forcing ($k<1.5$) maintains a statistically steady state. Dealiasing is performed through phase-shifting and truncation, achieving a resolved maximum wavenumber of $k_{\max} \approx 60$ with spectral resolution $\eta k_{\max}\approx 1.5$. Scalar transport is governed by
\begin{equation}
\partial_t \phi + v_j \partial_{x_j}\phi = D \nabla^2 \phi + f^\phi,
\end{equation}
where $\phi$ is a passive scalar and $D$ is its diffusivity. Both the viscosity $\nu$ and diffusivity $D$ are chosen so that the Schmidt number $Sc=\nu/D=1$. The integral-scale Reynolds number is expressed in terms of the Taylor microscale as
\begin{equation}
Re_\lambda = \sqrt{\frac{20}{3}}\frac{\text{TKE}}{\nu},
\end{equation}

where TKE denotes the turbulent kinetic energy. They use a novel scalar forcing approach, inspired by chemical reaction kinetics \citep{daniel2018reaction} to achieve desired stationary scalar PDFs and ensure scalar boundedness. Assuming scalar bounds $\phi_l=-1$ and $\phi_u=+1$, the forcing term is modeled as
\begin{equation}
f^\phi = \operatorname{sign}(\phi) f_c |\phi|^n (1 - |\phi|)^m,
\end{equation}
where $f_c$, $m$, and $n$ adjust PDF shape and scalar distribution. By appropriate parameter choices, different scalar PDFs are realized. For the present dataset, one scalar exhibits near-Gaussian behavior (kurtosis $\approx 3$) while the other has a lower kurtosis ($\approx 2.2$). With this forcing, the velocity and scalar fields reach a statistically stationary state at $Re_\lambda \approx 91$. Two scalars with distinct PDFs allow for testing model capabilities to reproduce both Gaussian-like and bounded scalar distributions. 

\textbf{Additional Results :} In Fig.~\ref{fig:3dintermittency}, we present the intermittency plots. Intermittency refers to the fluctuations in velocity gradients, leading to deviations from Gaussian statistics. This can be analyzed using the probability density function (PDF) of the velocity gradient tensor, which often exhibits heavy tails due to strong localized fluctuations and is a harder quantity to learn correctly \citep{mohan2020spatio}. The tensor, defined as the spatial derivatives of the velocity components, captures small-scale structures where intermittency effects are most pronounced. We observe near perfect prediction at high frequencies, represented by the tails of the PDF.

Finally, the most stringent test of this method is presented in the Q-R plane spectra in Fig.~\ref{fig:3dqr}, which represents the three-dimensional chaos in turbulence. QR plots are used to analyze the local flow topology by examining the invariants of the velocity gradient tensor~\citep{chertkov1999lagrangian}. The second invariant, Q, represents the balance between rotational and strain effects, while the third invariant, R, characterizes the nature of vortex stretching and flow structures. The spectra at $r=0$ indicate high frequencies, while those at $r=8$ and $r=32$ indicate intermediate frequencies and low frequencies, respectively. Historically, ML methods have struggled to capture the $r=0$ spectra and instead predict Gaussian-like noise~\citep{mohan2020spatio}, but we show that the BSP loss accurately captures these dynamics without compromising dynamics at $r=8, 32$. These plots show that even after conserving the smaller structures in the flow, the predictions do not deviate from key characteristics of turbulence.



\section{Turbulent flow over an airfoil}\label{airfoil}
\begin{figure*}[!h]
    \centering
    \includegraphics[width=0.99\linewidth]{images/airfoil.png}
    \vspace{-2cm}
    \caption{Comparison of model predictions at different timesteps for UNet (trained with MSE loss) and UNet + BSP Loss. The red dot is the point where the PDF is computed.}
    \label{fig:airfoil}
\end{figure*}
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.49\linewidth]{images/spec_af.png}
    \includegraphics[width=0.49\linewidth]{images/PDF.png}
    \caption{(left)Square root of the energy spectra for ground truth and model predictions. The energy spectra shown here is the mean of first 10 timestep predictions. (right)Distribution of velocity field at a location downstream of the airfoil. It shows the comparison of PDFs of ground truth and various model predictions.}
    \label{fig:airfoil-spec}
\end{figure}

In this section, we examine the turbulent wake flow downstream of a NACA0012 airfoil operating at a Reynolds number of 23,000, a free-stream Mach number of 0.3, and an angle of attack of $6^\circ$. We utilize a large eddy simulation (LES) dataset provided by \citep{towne2023database}, available through the publicly accessible \emph{Deep Blue Data} repository from the University of Michigan. The flow features have coherent structures associated with Kelvin-Helmholtz instability over the separation bubble and Von-K\'arm\'an vortex shedding in the wake, while exhibiting features at multiple scales characteristic of turbulent flows. This makes it an ideal test case for several experiments including validating computational fluid dynamics (CFD) models, analyzing flow dynamics, and exploring reduced-order modeling approaches. For more details on the dataset refer Section VII in \citep{towne2023database}. We follow the same data pre-processing strategy as given in \citep{oommen2024integrating}. The field is interpolated to convert it to a rectangular domain (200x400 pixels). We implement a UNet architecture \citep{ronneberger2015u} for the base model and improve it by using our BSP loss. The hyperparameters of the model are mentioned in Appendix \ref{hyperparams}.

Contrary to the previous case, here we observed that the energy spectrum of the UNet model prediction is very close to the ground truth even without the BSP loss. Therefore, we use the square root of the Fourier amplitudes in the energy spectrum to highlight the difference following \citep{oommen2024integrating}. Although it is difficult to compare the results visually from Figure \ref{airfoil}, we observe that the BSP loss enhances the model's ability to capture smaller scale structures given by the higher wavenumbers in the energy spectrum ($\sqrt{E(k)}$ in this case) in Figure \ref{fig:airfoil-spec}(left). The improvement here is marginal as the model without BSP loss itself does a good job in preserving the energy spectrum of the flow field.

To determine the performance of the BSP loss further, we compare it with a larger(as per number of parameters) state-of-the-art, Continuous Vision Transformer(CVIT) \citep{wang2024bridging} model. Due to the stochastic nature of the flow field, we compare the probability density function for the velocity values at a probe in the flow mentioned by the red dot in Figure \ref{fig:airfoil}. In Figure \ref{fig:airfoil-spec}(right), we observe that the UNet (trained with MSE loss) model does not preserve the probability distribution of the velocity field at the probe. However, the BSP loss improves its performance which is comparable to the approximately 60 times larger CVIT model. The UNet has a narrower distribution due to the spectral bias shifting the flow towards its mean after several rollouts. However, UNet with BSP loss has a wider distribution encompassing a  wide range of values. The BSP loss can also be implemented with the CVIT model for further comparison. Since CVIT is operated point-wise, defining the BSP loss can be challenging. The \textit{vmap} function can be used to overcome this and reshape the output to a 2D grid. Moreover, models like geo-FNO \citep{li2023fourier} can be used to extend the predictive model to non-uniform grids and BSP loss can be applied in the uniform latent dimension. We leave these paradigms for future research.

\section{Ablation Study}\label{ablation_sec}

\begin{table}[!h]
\caption{Comparison of mean square error at the end of optimization metrics for different values of $\mu$ in Equation \ref{Syn_bsp}. The table compares Mean Squared Error (MSE) loss, BSP loss, and FFT loss \citep{chattopadhyay2024oceannet}. The MSE loss is just for comparison and it does not have the hyperparameter $\mu$. The best performing model is highlighted in bold.}
\label{ablation_table}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccc}
\toprule
$\mu$ & MSE & BSP & FFT \\
\midrule
0.1   &      &0.206  $\pm$ 0.190        & 0.302 $\pm$ 0.213      \\
1     &     &0.026 $\pm$ 0.011 & 0.081  $\pm$ 0.027     \\
5     &  0.202$\pm$0.057 & \textbf{0.018 $\pm$ 0.007}& 0.226 $\pm$ 0.045      \\
7.5   &      & 0.048  $\pm$ 0.033       & 0.260  $\pm$ 0.024     \\
10    &      & 0.081  $\pm$  0.045      & 0.381  $\pm$  0.012    \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

In this section we perform ablation study for the hyperparamer $\mu$ in the BSP loss function given by Equation \ref{Syn_bsp} in Section \ref{SB_section}. From Table \ref{ablation_table}, it is observed that for all values of $\mu$ that we considered, the BSP loss consistently shows better performance by an order of magnitude from other baselines. We did not find considerable impact of other hyperparameters like $\epsilon$ in the performance of BSP loss. An extensive ablation study for all hyperparameters and other experiments is left for future works.


\section{Hyperparameters} \label{hyperparams}
In this section we declare the model hyperparametrs in Table \ref{tab:hyperparams}. All model hyperparameters are kept same for both baselines and the model trained with BSP loss. The hyperparameters of CVIT model is taken form \citep{wang2024bridging}. The length of trajectory used in training is started from 1 and gradually increased to Max Timesteps(t).

\begin{table}[!h]
\caption{Hyperparameters for different models and datasets.}
\label{tab:hyperparams}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccc}
\toprule
& 2D Turbulence & Airfoil & 3D Turbulence & Airfoil Large \\
\midrule
Model Name & DCNN & UNet & UNet & CVIT \\
Parameters & 1.1M & 0.6M & 90M & 37M \\
Learning Rate & $10^{-3}$ to $10^{-5}$ & $0.0005$ to $10^{-6}$ & $0.0005$ to $10^{-6}$ & $10^{-3}$ to $10^{-6}$ \\
Max Timesteps ($t$) & 4 & 5 & 3 & 1 \\
$\gamma(t)$ & $0.9^{t-1}$ & $0.9^{t-1}$ & $0.9^{t-1}$ & NA \\
$\mu$ & 1 & 0.1 & 1 & NA \\
$\lambda$ & 1 & 1 & 1 & NA \\
Optimizer & Adam & Adam & Adam & Adam \\
Scheduler & Cosine & ReduceLROnPlateau & Cosine & NA \\
Batch Size & 32 & 32 & 8 & 32 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

% \section{Instability in Approximating Neural Operators}
% Let \( S \) be a nonlinear operator that evolves the state \( q_t \) according to  

% \begin{equation}
%     q_{t+1} = S(q_t).
% \end{equation}

% Suppose that \( N \) is an approximate neural operator that aims to model \( S \) but instead evolves the state via  

% \begin{equation}
%     q'_{t+1} = N(q'_t).
% \end{equation}

% The primary assumption is that \( N \) does not exactly match \( S \) in preserving the \textbf{binned spatial power spectrum}, meaning that for some initial condition \( q_0 \), the power spectrum at \( t = 1 \) differs:  

% \begin{equation}
%     P_1^N(B_j) \neq P_1^S(B_j),
% \end{equation}

% where \( P_t^S(B_j) \) and \( P_t^N(B_j) \) denote the true and approximated binned spatial power spectra in frequency bin \( B_j \). The goal is to show that this spectral mismatch prevents \( N \) from maintaining stability in long-term autoregressive applications.  

% Consider the Fourier representations of the state evolutions under \( S \) and \( N \):  

% \begin{equation}
%     \hat{q}_{t+1}(k) = G^S(k, \hat{q}_t),
% \end{equation}

% \begin{equation}
%     \hat{q}'_{t+1}(k) = G^N(k, \hat{q}'_t),
% \end{equation}

% where \( G^S \) and \( G^N \) represent the nonlinear spectral mappings induced by \( S \) and \( N \). The spatially binned power spectrum is given by  

% \begin{equation}
    % P_t^S(B_j) = \sum_{k \in B_j} |\hat{q}_t(k)|^2, \quad P_t^N(B_j) = \sum_{k \in B_j} |\hat{q}'_t(k)|^2.
% \end{equation}

% Define the spectral deviation in bin \( B_j \) at time \( t \) as  

% \begin{equation}
%     \Delta_t(B_j) = P_t^N(B_j) - P_t^S(B_j).
% \end{equation}

% By assumption, at \( t = 1 \), there exists at least one bin where \( \Delta_1(B_j) \neq 0 \). The evolution of the power spectrum follows  

% \begin{equation}
%     P_{t+1}^N(B_j) = \sum_{k \in B_j} |G^N(k, \hat{q}'_t)|^2.
% \end{equation}

% Expanding the difference equation,  

% \begin{equation}
%     \Delta_{t+1}(B_j) = \sum_{k \in B_j} |G^N(k, \hat{q}'_t)|^2 - \sum_{k \in B_j} |G^S(k, \hat{q}_t)|^2.
% \end{equation}

% \textcolor{red}{Need to think about this part.} For small deviations, a first-order expansion gives the recurrence  

% \begin{equation}
%     \Delta_{t+1}(B_j) \approx \Lambda(B_j) \Delta_t(B_j),
% \end{equation}

% where the spectral growth factor is  

% \begin{equation}
%     \Lambda(B_j) = \max_{k \in B_j} \left| \frac{G^N(k, \hat{q}'_t)}{G^S(k, \hat{q}_t)} \right|^2.
% \end{equation}

% Since the initial condition already has \( \Delta_1(B_j) \neq 0 \), there must exist at least one bin where \( \Lambda(B_j) \neq 1 \). This leads to two possible scenarios:  

% - If \( \Lambda(B_j) > 1 \), then the spectral deviation grows exponentially as  

% \begin{equation}
%     \Delta_t(B_j) = \Delta_1(B_j) \prod_{\tau=1}^{t} \Lambda(B_j),
% \end{equation}

% which results in  

% \begin{equation}
%     P_t^N(B_j) \to \infty.
% \end{equation}

% This implies unbounded growth and instability.  

% - If \( \Lambda(B_j) < 1 \), then the spectral deviation shrinks exponentially, leading to  

% \begin{equation}
%     P_t^N(B_j) \to 0 \quad \text{as} \quad t \to \infty.
% \end{equation}

% This means that the approximate operator artificially damps energy, leading to vanishing solutions.  

% Since at least one bin satisfies \( \Lambda(B_j) \neq 1 \), the power spectrum of \( N \) cannot remain bounded, and the system will necessarily either diverge or decay to zero. This gives a sketch of the proof that \( N \) is never stable under long-term autoregressive iteration. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.