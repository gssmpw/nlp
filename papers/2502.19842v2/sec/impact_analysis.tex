


We have developed an initial approach to address the identified bias in the CLIP model, which is presented in Appendix \ref{sec:pre-method}. While this method is specific to our current dataset, it represents a promising step toward addressing these challenges and can inspire further advancements. This work demonstrates our commitment to exploring practical solutions while maintaining the primary focus of this study on the analysis of bias and its implications.

\section{Practical Impacts of Encoder Biases}
The biases observed in CLIP's image and text encoders significantly impact model performance in real-world applications. This section explores how these biases manifest in image-text matching tasks, while further analyses of text-to-image generation impacts are presented in Appendix \ref{sec:appendix_text_to_image}.


Our analysis in this section serves two primary purposes. First, it provides concrete evidence of how these theoretical biases can translate into practical limitations. Second, it offers insights into potential areas for improvement in vision-language models, particularly in handling complex, multi-object scenarios. Through a series of carefully designed experiments, we illustrate how the biases in both text and image encoders can lead to unexpected or suboptimal results in tasks that are crucial for many downstream applications.
\subsection{Image-Text Matching}
\label{sec:impact}


Building upon our findings of biases in CLIP’s image and text encoders, we now demonstrate how these biases tangibly affect the model’s performance in image-caption matching tasks. We designed two experimental scenarios, conducted on both the ComCO and COCO datasets, to evaluate these biases. The results of these experiments are summarized in Table \ref{table:performance_drop}. To better illustrate the differences between these two scenarios, an example of the caption structures is shown in Figure \ref{fig:image-text-match}.
In each scenario, we created incorrect captions by switching one object in the caption with an object that is not present in the image. Additionally, GPT-4O \cite{achiam2023gpt} was used to rewrite the captions in the COCO dataset.

\paragraph{First Scenario}
In the first scenario, biases assist the model in distinguishing between the correct and incorrect captions. In the correct captions, the largest object in the image is placed at the beginning, aligning with the model’s bias towards prioritizing first-mentioned objects and larger objects. For the incorrect captions, the non-existent object is deliberately placed at the beginning, which helps the model recognize the difference between the correct and incorrect captions more effectively. This positioning emphasizes the discrepancy early on, allowing the model to better detect the mismatch between the caption and the image. The performance of different models in this scenario can be seen in Table \ref{table:performance_drop} under the "First Scenario" column.





\paragraph{Second Scenario}
In the second scenario, biases lead the model to make errors. The correct captions place the largest object at the end of the sentence, disrupting the model’s bias towards objects mentioned earlier and its preference for larger objects. In the incorrect captions, the non-existent object is placed at the end, making it more difficult for the model to differentiate between correct and incorrect captions as its attention is drawn away from the critical discrepancies. The performance of different models in this scenario is shown in Table \ref{table:performance_drop} under the "Second Scenario" column.






\begin{table}[ht]
\centering
\scriptsize
\setlength{\tabcolsep}{5pt} % Adjust for compactness
\renewcommand{\arraystretch}{1.2} % Adjust row spacing for readability

\caption{Performance Comparison on Image-Text Matching for ComCO and COCO Datasets}
\label{table:performance_drop}
\begin{tabular}{l l c c}
\toprule
\rowcolor[HTML]{EFEFEF}
\textbf{Dataset} & \textbf{Model} & \textbf{First Scenario} & \textbf{Second Scenario} \\ 
\midrule

\multirow{6}{*}{ComCO} 
    & \textit{CLIP Datacomp} \cite{gadre2024datacomp} & \textbf{99.99} & 67.50 \\
    & \textit{CLIP Roberta} & \textbf{99.98} & 64.75 \\
    & \textit{SIGLIP} \cite{zhai2023sigmoid} & \textbf{99.49} & 72.36 \\
    & \textit{CLIP openAI} & \textbf{99.59} & 52.23 \\
    & \textit{NegCLIP} & \textbf{96.82} & 46.94 \\
    & \textit{SugarCrepe} & \textbf{98.55} & 60.43 \\
\midrule

\multirow{6}{*}{COCO} 
    & \textit{CLIP Datacomp} \cite{gadre2024datacomp} & \textbf{71.2} & 54.2 \\
    & \textit{CLIP Roberta} & \textbf{72.2} & 54.1 \\
    & \textit{SIGLIP} \cite{zhai2023sigmoid} & 64.8 & 39.5 \\
    & \textit{CLIP openAI} & \textbf{63.5} & 26.4 \\
    & \textit{NegCLIP} & \textbf{72} & 28.7 \\
    & \textit{SugarCrepe} & \textbf{80.0} & 40.9 \\

\bottomrule
\end{tabular}
\end{table}





By comparing these two scenarios, we demonstrate that biases in CLIP can either help or hinder the model’s performance depending on how captions are structured. The experimental results, particularly with the use of GPT-4O for caption rephrasing in the COCO dataset, reveal how such biases can influence the accuracy of image-text matching tasks. These biases must be addressed to improve CLIP’s robustness in real-world multi-object scenarios.




For further insights on how these biases affect text-to-image generation, refer to our extended experiments in Appendix \ref{sec:appendix_text_to_image}.
















% \subsection{Text to image geneation}
% The biases observed in CLIP's encoders have significant implications beyond image-text matching, particularly for text-to-image generation models that incorporate CLIP components. To investigate this impact, we focused on Stable Diffusion, a popular text-to-image generation model that utilizes CLIP's text encoder in its pipeline.
% Stable Diffusion employs CLIP's text encoder to process input prompts, creating text embeddings that guide the image generation process. Given our identification of biases in CLIP's text encoder, especially the preference for objects mentioned earlier in text descriptions, we hypothesized that these biases would manifest in the generated images.
% To test this hypothesis, we designed an experiment using prompts containing multiple objects from the COCO dataset. Our goal was to observe whether the order of objects in the text prompt influences their prominence or likelihood of appearance in the generated images.

% Our experimental methodology consisted of three main steps. First, we created 1,000 multi-object prompts, each containing four distinct objects from the COCO dataset. Second, we used these prompts to generate images using three versions of Stable Diffusion: v1.4 \cite{Rombach_2022_CVPR}, v2, and SD-XL \cite{podell2023sdxl}. Finally, to evaluate the presence of objects in the generated images, we employed YOLO v8 \cite{reis2023real}, a state-of-the-art object detection model. We configured YOLO v8 with a detection threshold of 0.25 and used it to validate which objects from the original prompt were present in the generated image.

% This approach allowed us to quantitatively assess how CLIP's text encoder biases propagate through the Stable Diffusion pipeline and manifest in the generated images. By comparing the frequency of object detection with their position in the input prompt, we could directly observe the impact of the text-side bias on the image generation process.

% \begin{table}[ht]
% \centering
% \scriptsize
% \setlength{\tabcolsep}{3pt}
% \renewcommand{\arraystretch}{1.2}
% \caption{Object presence in Stable Diffusion-generated images }
% \label{tab:image_gen}
% \begin{tabular}{lcccc}
% \toprule
% \rowcolor[HTML]{EFEFEF}
% Model & \textbf{First Obj} & \textbf{Second Obj} & \textbf{Third Obj} & \textbf{Fourth Obj} \\ 
% \midrule 
% \textit{SD v1.4} & 57.7 & 44.7 & 38.1 & 35.4 \\
% \textit{SD V2} & 62.5 & 49.7 & 47.5 & 42.2 \\
% \textit{SD-XL}  & 79.2 & 69.3 & 59.4 & 64.0 \\
% \bottomrule
% \end{tabular}
% \end{table}

% Our findings, presented in Table \ref{tab:image_gen}, demonstrate a clear correlation between an object's position in the text prompt and its likelihood of appearing in the generated image. This correlation aligns with our earlier observations of CLIP's text encoder bias, suggesting that these biases significantly influence the output of text-to-image generation models.