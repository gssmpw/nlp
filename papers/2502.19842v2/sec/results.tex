% \section{Results and Analysis}
% \label{sec:results}
% Our experiments revealed significant biases in both the text and image encoders of the CLIP model. This section presents our findings, organized by encoder type and experiment.

% \subsection{Text Encoder Analysis}



% \subsubsection{Text-based Object Classification (TOC)}



% The performance of the text encoder in the TOC experiment demonstrated a significant bias towards the first object mentioned in the text descriptions. As shown in Table \ref{tab:text_base_exp}, the classification accuracy for the first object was considerably higher than for subsequent objects. This suggests that the first object mentioned in a textual description is represented more prominently in the final textual representation.


% \subsubsection{Text-based Object Retrieval (TOR)}

% The TOR experiment further reinforced the presence of bias in the text encoder. Table \ref{tab:text_base_exp} presents the retrieval accuracy, where the first object mentioned in the descriptions consistently showed the highest retrieval accuracy. This indicates that the initial object exerts a dominant influence on the overall text representation, making it more likely to be retrieved accurately compared to subsequent objects.

 


% \subsection{Image Encoder Analysis}



% \subsubsection{Image-based Object Classification (IOC)}
% IOC experiment revealed that larger objects in an image significantly influence the final visual representation more than smaller objects. This size-related bias is evident across various models and datasets. As detailed in Table \ref{tab:image_base_exp}, the classification accuracy for larger objects was consistently higher, indicating that the image encoder prioritizes these objects in its representations.


% \subsubsection{Image-based Object Retrieval (IOR)}
% The IOR experiment confirmed the significant influence of larger objects on the image encoder's performance. Table \ref{tab:image_base_exp} shows that larger objects in multi-object images were more frequently and accurately identified in single-object image searches. This suggests a strong size-related bias in the image encoder, where larger objects are given more weight in the final image representation.

\section{Results and Analysis}
\label{sec:results}

Our experiments revealed significant biases in both the text and image encoders of the CLIP model. This section presents our findings, organized by encoder type and focusing on retrieval tasks. 
\subsection{Text Encoder Biases}
We observed a consistent bias in the text encoder towards the first object mentioned in descriptions. In the TOR experiment, the retrieval accuracy (as shown in Table \ref{tab:text_base_exp}) was highest for the first object, indicating its dominant influence on the overall text representation. This suggests that the text encoder prioritizes the initial object, leading to its more accurate retrieval compared to subsequent objects. The detailed results for the scenarios involving 2, 3, and 5 objects can be found in the appendix \ref{app:tor}, and experiments on longer caption templates are in Appendix \ref{app:toc-long} and \ref{app:tor-long}.


\subsection{Image Encoder Biases}

In multi-object images, the image encoder exhibited a strong bias towards larger objects. The Image-based Object Retrieval IOR experiment, detailed in Table \ref{tab:image_base_exp}, shows that larger objects were more frequently and accurately retrieved during single-object image searches. This finding highlights the image encoder's bias towards larger objects, which receive disproportionate emphasis in the final image representation.
Further detailed results, specifically for scenarios with 2, 3, and 5 objects, are provided in the appendix \ref{app:ior}.
\input{tables/result_tab1}

\subsection{COCO Dataset Experiments}

To validate the generalizability of our findings from the synthetic dataset, we conducted similar experiments on the COCO dataset, which comprises real images with accompanying captions. This real-world dataset allowed us to investigate whether the previously observed biases persist in more naturalistic settings.

% For IOR, we used single-object images from the DomainNet dataset \cite{peng2019moment} as our retrieval targets.
% Additionally, we conducted a text-based variation of IOR, using class labels as queries instead of single-object images, to account for potential discrepancies between objects in single and multi-object contexts.
Due to the absence of single-object images for COCO objects, we approached the IOR experiment in two ways. First, we used single-object images from the DomainNet dataset \cite{peng2019moment} as retrieval targets. Second, we introduced an alternative approach called Image-to-Text Object Retrieval (I2TOR). In I2TOR, we used the textual names of COCO objects instead of single-object images. These object names were embedded using CLIP's text encoder, allowing us to perform a retrieval task consistent with the IOR methodology while adapting to the constraints of the COCO dataset.

\input{tables/coco_based_result_table_IOR}

% Tables \ref{tab:text_base_exp_coco} and \ref{tab:image_base_exp_coco} presents the results of our COCO dataset experiments. In IOR, larger objects in COCO images were retrieved more accurately, consistent with our synthetic dataset findings. In TOR, the first-mentioned object in COCO captions showed higher retrieval accuracy, aligning with our previous text encoder bias observations.


Tables \ref{tab:text_base_exp_coco} and \ref{tab:image_base_exp_coco} present the results of our COCO dataset experiments. In TOR, the first-mentioned object in COCO captions was retrieved with higher accuracy, which aligns with our earlier findings of bias in the text encoder. Similarly, in IOR, larger objects in COCO images were retrieved more accurately, consistent with the trends observed in our synthetic dataset experiments. The I2TOR results further confirmed this bias, demonstrating that even when using textual object representations, the bias towards larger objects persists.

Our experiments reveal two significant biases in the CLIP model: the text encoder shows a strong preference for the first mentioned object in textual descriptions, while the image encoder exhibits greater sensitivity to larger objects in images. These biases can significantly impact the overall system performance in various vision-language tasks, particularly in multi-object scenarios.




% Our experiments reveal two significant biases in the CLIP model:

% \begin{enumerate}
%     \item \textbf{Text Encoder Bias:} The model shows a strong preference for the first mentioned object in textual descriptions.
%     \item \textbf{Image Encoder Bias:} The model exhibits greater sensitivity to larger objects in images.
% \end{enumerate}

% These biases can significantly impact the overall system performance in various vision-language tasks, particularly in multi-object scenarios.