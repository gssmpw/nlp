\section{Origin of Bias in CLIP Models}
\label{sec:origin}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.9\linewidth]{figs/attention_map.pdf}
\caption{Attention allocation from the CLS token to objects of different sizes in the ComCO dataset. a) Qualitative results showing the CLS token's attention to each object. b) Quantitative analysis of attention distribution across 8,000 images, with each image containing one large and two small objects. The bar chart shows the average attention allocated to the large object versus the smaller ones, demonstrating a bias towards larger objects.}
    \label{fig:attention_map}
\end{figure*}

In this section, we investigate the potential origins of the biases observed in CLIP models and provide evidence supporting our hypotheses.

\subsection{Bias in the Image Encoder}

The observed bias favoring larger objects within the image domain can be attributed to the architectural characteristics of Vision Transformers (ViT) \cite{alexey2020image} utilized in CLIP's image encoder. Our hypothesis is that larger objects, which occupy a greater number of patches in the ViT's patch-based image representation, exert a more significant influence on the final class (CLS) token representation. This bias is not exclusive to CLIP; it appears to be a consistent feature across ViT models, as demonstrated by our experiments detailed in the appendix.

To substantiate this hypothesis, we designed an experiment to quantify the attention allocated by the CLS token to each image patch. By calculating the cumulative attention received by each object from the CLS token, we could assess the influence of object size on attention allocation. We applied this analysis to our three-object ComCO dataset, and the results are illustrated in Figure~\ref{fig:attention_map}. The findings confirm our hypothesis: larger objects indeed receive more attention from the CLS token.



\subsection{Bias in the Text Encoder}

We explore the bias present in the text encoder from two perspectives: the attention mechanism in the model structure and the model's training method.

\subsubsection{Impact of Attention Mechanism}

Text encoder models can be categorized based on their attention mechanisms: uni-directional (causal) attention and bi-directional attention. In models with causal attention, each token attends only to preceding tokens, whereas in bi-directional models, each token attends to all tokens in the sequence.

When OpenAI introduced the CLIP model, its text encoder employed causal attention, meaning each token could only attend to tokens before it and itself. This differs from typical self-attention mechanisms, where tokens attend to all other tokens. Most CLIP models use causal self-attention, with the exception of the variant using the XLM-Roberta text encoder, which also employs self-attention. However, as shown in Table~\ref{tab:text_base_exp}, even this model exhibits the mentioned bias. This indicates that the bias does not originate from the attention mechanism itself.

\subsubsection{Role of Training Method}
\input{tables/diff_training_models}
To determine whether the observed bias is specific to CLIP models, we compared CLIP's text encoder with two other models designed to embed sentences into a meaningful semantic space: Sentence-BERT (SBERT) \cite{reimers2019sentence} and SimCSE \cite{gao2021simcse}. The primary distinction is that CLIP's embedding space is shared between images and text, whereas SBERT and SimCSE operate solely in the text domain.




We conducted the TOR experiment on our dataset using these models. As presented in Table \ref{tab:diff_training_models}, the bias observed in CLIP differs from that in the other models. This suggests that CLIP's unique training method, which aligns images and text in a shared embedding space through contrastive learning, contributes to the bias. Therefore, to uncover the root cause of the bias, we focus on the specifics of CLIP's training procedure.


\subsection{Hypothesized Origin of Text-Side Bias in CLIP}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/origin_plotsv3.pdf}
    
\caption{a) Top-1 Object Retrieval accuracy comparison for sentences where the first object is either large or small. The higher TOR accuracy for sentences beginning with large objects supports the hypothesis that larger objects, when mentioned first, exert a stronger influence on text embeddings due to cross-modal alignment with their prominent visual representation in images. b) Distribution of the position of the largest object within image captions from the LAION datasets. The results show a consistent bias where larger objects tend to be mentioned earlier in text descriptions. c) Progression of TOR rates across different training stages, indicating that text-side bias strengthens as the model is exposed to more data, suggesting the cumulative effect of image-side bias being transferred to the text encoder through contrastive learning.}
    \label{fig:claims}
\end{figure*}

We hypothesize that the text-side bias in CLIP, which favors objects mentioned earlier in text descriptions, originates from the image-side bias toward larger objects and is transferred to the text encoder during contrastive training. We present evidence supporting this hypothesis through two key claims and an analysis of the training progression.

\paragraph{Claim 1: Larger Objects Have More Influence on Text Embeddings.}
% Building upon the established image-side bias discussed earlier, we posit that objects with larger physical sizes exert more influence on CLIP's text embeddings due to the alignment enforced during contrastive training. To test this, we categorized objects in the DomainNet dataset into large, medium, and small groups. We constructed two sets of sentences, each containing four objects: one set with a large object mentioned first followed by three medium-sized objects, and another with a small object mentioned first followed by three medium-sized objects.
Building upon the established image-side bias discussed earlier, we posit that objects with larger physical sizes exert more influence on CLIP's text embeddings due to the alignment enforced during contrastive training. To test this, we categorized objects in the DomainNet dataset into large, medium, and small groups based on their relative physical sizes in real-world (with the full list of objects provided in the appendix \ref{app:categorized_domainnet}). Specifically, objects smaller than a school bag were categorized as small, objects sized between a school bag and a medium-sized car were classified as medium, and objects larger than a car—up to significantly larger items—were considered large. We then constructed two sets of sentences, each containing four objects: one set with a large object mentioned first followed by three medium-sized objects, and another with a small object mentioned first followed by three medium-sized objects. 

Figure \ref{fig:claims}.a compares the TOR accuracy for the first object in these two groups. The higher TOR accuracy for sentences beginning with large objects supports our hypothesis that larger objects, when mentioned first, have a more significant impact on the text embeddings due to the cross-modal alignment with their prominent representation in images.



\paragraph{Claim 2: Caption Bias in Training Datasets.} To investigate potential biases in CLIP's training data, we analyzed both the LAION \cite{schuhmann2022laion} and COCO datasets. Due to limited computational resources and the large size of the LAION dataset, which contains over 2 billion image-text pairs, we randomly selected a subset of 200,000 samples for our analysis. Using the Llama3 model, we extracted objects from the image captions and employed the Language Segment-Anything tool to generate object masks in the corresponding images, calculating their areas based on these masks. A detailed description of our LAION dataset analysis methodology can be found in Appendix \ref{subsec:laion_analysis}.

Figure\ref{fig:claims}.b shows the position of the largest object within each caption. The results indicate that, in the majority of cases, the largest object in an image is mentioned earlier in its caption. The same experiment was conducted on the COCO dataset, with detailed results and the distribution for two to five object scenarios provided in Appendix \ref{app:coco-anlysis}. This demonstrates a consistent bias in the training data, where larger objects are not only more visually prominent but are also described earlier in text annotations.



\paragraph{Analysis of Bias Development During Training.}
To further validate our hypothesis, we examined the progression of text-side bias during CLIP's training. We utilized model checkpoints from the LAION dataset at five training stages, corresponding to exposure to 2, 4, 6, 8, and 10 billion samples. We conducted TOR experiments at each stage, focusing on the retrieval accuracy for the first object mentioned in text descriptions.

Figure\ref{fig:claims}.c depicts the evolution of the TOR rate across different training stages for scenarios with varying numbers of objects (from 3 to 8). The consistent upward trend in the TOR rate as the model is exposed to more training data suggests that the text-side bias strengthens over time, likely due to the cumulative effect of the image-side bias being transferred to the text encoder through contrastive learning.


\paragraph{Incomplete Text Representation of CLIP}

Here we want to theoretically highlight why the CLIP text encoder could learn an incomplete representation of the text. 
Let $\mathbf{z}$ and $\mathbf{w}$ represent a latent representation of an image content described in the caption, and such visual content not mentioned in the text, respectively. For example, $\mathbf{z}$ represents the fact that an image contains ``a horse that is eating the grass.'' In this case, $\mathbf{w}$ might represent other details in the image, like the ``horse color,'' ``where the horse is located,'' etc. We assume a data generative process as follows:
\begin{align*}
    & I := g({\mathbf z}, {\mathbf w}) \\
    & T := h({\mathbf z}),
\end{align*}
where $I$ is the image, and $T$ is its corresponding caption. 

Now we want to learn a joint embedding of the image and text through the CLIP. Here, we assume that $f_\theta(.)$ and $i_\omega(.)$ as learnable functions that map the image and text into the joint embedding space, respectively. 
\begin{theorem}
Let elements of ${\mathbf z}$ be independent, zero-mean, and unit-variance. The contrastive loss for the ideal text encoder, $i_\omega(T) = {\mathbf z}$ converges to that of a non-ideal incomplete one, i.e. $i_{\omega^\prime}(T) = {\mathbf z}_s$, where ${\mathbf z}_s$ is the first $d-k$ dimensions of   ${\mathbf z}$, with $k$ being a constant, and $d \rightarrow \infty$. 
\end{theorem}

Proof: The contrastive loss in making this learning happen can be written as:
\begin{align}
    \mathbb{E}_{\mathbf{z}, \mathbf{z}^\prime, \mathbf{w}} \Bigg\{ 
    \frac{\exp(sim(\mathbf{z}, \mathbf{z}))}{\exp(sim(\mathbf{z}, \mathbf{z})) + \sum_k \exp(sim(\mathbf{z}, \mathbf{z}^\prime_k))} 
    \Bigg\}
\end{align}

with 
$$
sim(\mathbf{z}, \mathbf{z}') = S(f_\theta(g(\mathbf{z}, \mathbf{w}), i_\omega(h(\mathbf{z}')))),
$$
and $\mathbf{z}$ and $\{\mathbf{z}^\prime_k | 1 \leq k \leq b\} $ are $b+1$  i.i.d. samples of the content in the representation space, and $S$ is some normalized similarity metric, e.g. cosine similarity, and $b+1$ is the batch size. We assume that elements of ${\mathbf z}$ are independent, unit-variance, and zero mean. We further assume that the dimensionality of $\mathbf z$, denoted as $d$, goes to infinity. 

Under such conditions, and based on Law of Large Numbers, $\| {\mathbf z} \| \xrightarrow{p} \sqrt{d}$, when $d$ is large. Therefore, for any two independent copies of $\mathbf z$, ${\mathbf z}^\prime_k$, we have $sim({\mathbf z}, {\mathbf z}^\prime_k) = {\mathbf z}^\top {\mathbf z}^\prime_k / (\| {\mathbf z} \| \| {\mathbf z}^\prime_k \|) \xrightarrow{p} 0.$

It is evident that in the ideal case, $f_\theta(g(\mathbf{z}, \mathbf{w})) = \mathbf{z}$ and also $i_\omega(h(\mathbf{z})) = \mathbf{z}$, so the contrastive loss would converge to $e/(e + b)$, as the numerator is $e$, and the second term in the denominator converges to $\exp(0) = 1$, according to the Mann-Wald's theorem. 

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figs/image-text-match-fig2.pdf}
    \caption{An example of the correct and incorrect caption structures in the first and second scenarios.}
    \label{fig:image-text-match}
    \vspace{-0.2cm}
\end{figure*}
However, we show that other learning of this representation could achieve the same amount of loss. For instance, let $\mathbf{z}_s$ be the first $d-k$ elements of $\mathbf{z}$, with $k$ being a {\it constant}. We show that if $f_{\theta^\prime}(I) = \mathbf{z}_s$ and $i_{\omega^\prime}(T) = \mathbf{z}_s$, the same loss would be achieved in the limit of large $d$. To see this, note that the numerator stays the same, i.e. $e$, while the second term in the denominator still converges to $b \exp(0) = b$.

This means that even if the image and text encoder of the CLIP only partially recover the content embedding, they reach an excellent loss. But such possible incomplete representations of $\mathbf{z}$ are combinatorially large, making convergence of the CLIP to such local minima pretty likely. This makes the text encoding of CLIP be far from ideal. Furthermore, the text encoder would become {\it biased}, depending on which of such local minima it converges to. Based on this explanation, we would expect a text encoder that has learned a complete representation to exhibit such biases to a lesser degree. As mentioned earlier, the subject of learning text representations in VLMs that are discriminative of hard negatives (e.g. NegCLIP) has been around for few years. We tested one of strongest such models, \cite{hsieh2024sugarcrepe}, in our benchmark to validate the hypothesis that an incomplete text representation is one of the causes of the bias in the VLMs. We noticed that this model shows lower bias based on our benchmark (see the SugarCrepe model in  tables \ref{tab:text_base_exp} and \ref{tab:image_base_exp}).


