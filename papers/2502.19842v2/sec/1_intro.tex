\section{Introduction}

The convergence of vision and language in artificial intelligence has led to the development of Vision-Language Models (VLMs) that can interpret and generate multimodal content. Among these, OpenAI's Contrastive Language-Image Pre-training (CLIP) model~\cite{radford2021learningtransferablevisualmodels} has been particularly influential, demonstrating remarkable capabilities in zero-shot image classification and setting new standards for multimodal understanding~\cite{Cherti_2023, gadre2023datacompsearchgenerationmultimodal, schuhmann2021laion400mopendatasetclipfiltered, thrush2022winoground}. The success of CLIP has catalyzed a wide array of applications---from image retrieval and visual question answering to text-to-image generation---signifying a paradigm shift in how models perceive and relate visual and linguistic information.




Visual Language Models like CLIP face significant challenges in understanding and reasoning about complex scenes with multiple objects and intricate relationships. CLIP struggles to identify distinct objects and model their relationships accurately, especially when captions contain the same objects but differ in their relationships. This results in difficulty distinguishing between similar captions with different object relationships. Several benchmark datasets have been introduced to elucidate the limitations of existing models in capturing subtle relational nuances. Notably, Winoground \cite{thrush2022winoground}, VL-CheckList \cite{zhao2022vl}, ARO \cite{yuksekgonul2023and}, and CREPE \cite{ma2023crepe} have been instrumental in evaluating models' capacities to accurately match images with semantically appropriate captions. 



\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figs/main_fig_v5.pdf}
    \caption{Overview of our key contributions. Step 1: We create ComCO dataset for controlled multi-object experiments. Step 2: We identify biases in CLIP's image encoder (favoring larger objects) and text encoder (prioritizing first-mentioned objects). Step 3: We investigate the origin of these biases, finding a connection to training data characteristics. Step 4: We demonstrate the practical impacts of these biases on image-text matching task, showing how they affect model performance in multi-object scenarios.}
    \label{fig:mainfig}
    \vspace{-0.5cm} 
\end{figure*}

Numerous efforts have been made to address compositionality challenges in the multi-object setting. These studies have predominantly employed end-to-end methodologies, including fine-tuning techniques with hard-negative samples \cite{yuksekgonul2023and}, to enhance model performance. The efficacy of these approaches have been criticized and improved recently, SUGARCREPE \cite{hsieh2024sugarcrepe}, and \cite{sahin2024enhancing}. Specifically, a common methodology in these works involves the generation of negative captions through minor structural alterations, or through LLMs, to the original positive ones, emphasizing the identification of semantic disparities between captions that share structural similarities but differ conceptually. Hence, these approaches helped in capturing nuances in the text domain that is necessary in the compositional multi-object scenarios.   


While these efforts have primarily focused on assessing CLIP's ability to differentiate between captions with minor structural variations but significant conceptual divergences, there remains a paucity of research examining CLIP's performance on captions that are semantically equivalent but structurally distinct. The work of Dumpala et al. \cite{dumpala2024sugarcrepe++} represents one of the few forays into this domain. However, while such studies have introduced novel benchmarks, they have not comprehensively explored the underlying mechanisms that contributes to the CLIP unstable performance when given semantically equivalent prompts.  





While previous studies have made significant strides in understanding CLIP's limitations, our work distinguishes itself in several key aspects. Firstly, we shift the focus from evaluating CLIP's ability to differentiate between conceptually distinct captions to examining its performance with semantically equivalent but structurally varied captions. This approach allows us to probe deeper into the model's understanding of language and visual content beyond surface-level differences. Here, model systematic mistakes give an indication the potential baises. Secondly, unlike many previous works that primarily introduced benchmarks or proposed end-to-end solutions, we conduct a thorough investigation into the underlying causes of CLIP's behavior. Our study delves into the internal mechanisms of both the image and text encoders, providing insights into why the model is biased and lacks invariance to certain types of linguistic and visual variations. 

To facilitate this in-depth analysis, we introduce the \textbf{ComCO} dataset, specifically designed to isolate and examine different aspects of CLIP's performance in {\it controlled} multi-object scenarios. Furthermore, our research spans multiple versions of CLIP trained on various datasets and architectures, ensuring the broad applicability and generalizability of our findings. By focusing on these underexplored areas and employing a more comprehensive analytical approach, our work aims to provide a deeper understanding of CLIP's limitations and pave the way for more robust and versatile vision-language models. It is important to note that such an analysis not only benefits the improvement of CLIP but also has significant implications for related models, such as text-to-image (T2I) generative models and multimodal large language models (MLLMs). Understanding the intricacies of CLIP's encoding process can inform and enhance the development of these technologies, potentially leading to advancements across various domains of artificial intelligence. As shown in Figure \ref{fig:mainfig}, our key contributions are as follows:









\begin{itemize} \item \textbf{Development of Novel Dataset}: We introduce \textit{ComCO}, a specialized dataset specifically designed to create {\it controlled} multi-object scenarios. Here, unlike previous benchmarks, we can control the object size in the image, and their ordering in the caption. Hence, this dataset enables precise, fine-grained analysis of model performance across a spectrum of compositional challenges, facilitating a deeper understanding of VLMs' strengths and weaknesses.

    \item \textbf{Comprehensive Encoder Analysis}: We perform an in-depth examination of both the image and text encoders in CLIP when processing multi-object scenes and descriptions. This includes text-based, and object-based image retrievals, that reveal each text and image encoder weaknesses in preserving the information necessary to discern various objects. By analyzing the embedding space, we identify the stages at which compositional information is lost or distorted, providing insights into the internal mechanisms of the model.
    
    \item \textbf{Identification of Specific Biases}: Our research uncovers significant biases in CLIP models. The image encoder prefers larger objects in multi-object images, while the text encoder favors first-mentioned objects and also objects that are usually visually larger in real-world. These biases reveal the complex interplay between visual and linguistic information processing in CLIP, influencing its interpretation of multi-object scenarios.
    
    \item \textbf{Investigation of the Bias Origin }: We explore the origins of observed biases in CLIP's performance, particularly in various multi-object scenarios. Our investigation delves into both the image and text encoders. We hypothesize that the visually larger objects are mostly mentioned earlier in the caption in CLIP training datasets. But it is evident that the image encoding naturally favors such objects in the embedding due to the abundance of their visual tokens. Therefore, the text encoder may get biased towards such objects, and consequently earlier mentioned text tokens. We provide evidence for these biases through analyses of the LAION dataset and CLIP's training progression, revealing a consistent trend where larger objects tend to be mentioned earlier in image captions.
    
    \item \textbf{Practical impacts of encoder biases}: We demonstrate how the identified biases in CLIP's image and text encoders significantly impact performance in multi-object analysis/synthesis scenarios. Using our ComCO dataset, we show substantial drops in image-text matching accuracy when manipulating object sizes and caption order. We further reveal how these biases propagate to text-to-image generation models like Stable Diffusion, influencing the prominence and likelihood of object appearance in generated images based on prompt order.
    
    \end{itemize}

% These observations highlight how biases in both the text and image encoders lead to a substantial decrease in CLIP's performance in multi-object scenarios. Our findings underscore the importance of addressing these biases to improve the robustness and versatility of vision-language models in complex visual environments. This work contributes valuable insights into CLIP's behavior in multi-object contexts and opens up new avenues for enhancing the performance of vision-language models in real-world applications.




