\clearpage


\setcounter{page}{1}

% \maketitlesupplementary
\onecolumn
{
    \centering
    \Large
    \textbf{\thetitle}\\
    \vspace{0.5em}Supplementary Material \\
    \vspace{1.0em}
}

\section{Appendix}


\subsection{The SIMCO and ComCO Datasets}
\label{app:dataset}

\subsubsection{The SIMCO Dataset}
The SIMCO dataset comprises 17 objects. These 17 objects are:

\begin{center}
\large % You can use \Large or \huge for even larger text
\begin{tabular}{@{} l @{\hspace{4em}} l @{\hspace{4em}} l @{}}
Cube & Sphere & Cylinder \\
Mug & Pentagon & Heart \\
Cone & Pyramid & Diamond \\
Moon & Cross & Snowflake \\
Leaf & Arrow & Star \\
Torus & Pot &
\end{tabular}
\end{center}


Using Blender software, a collection of images containing 2 to 5 objects has been created from these 17 objects. The total number of images in this dataset is approximately 85,000. Examples of these images can be seen in Figure \ref{fig:simco}.

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{figs/SimCov2.pdf}
    \caption{Examples of the SimCO dataset}
    \label{fig:simco}
\end{figure*}
\FloatBarrier  % Prevent other floats from being placed here
\clearpage  % Start on a new page

\subsubsection{The ComCO Dataset}

The ComCO dataset contains 72 objects, as listed below:
\begin{center}
\large % You can also use \Large or \huge for larger text
\begin{tabular}{@{} l @{\hspace{2em}} l @{\hspace{2em}} l @{\hspace{2em}} l @{\hspace{2em}} l @{\hspace{2em}} l @{}}
person & bicycle & car & motorcycle & airplane & bus \\
train & truck & boat & traffic light & fire hydrant & street sign \\
stop sign & parking meter & bench & bird & cat & dog \\
horse & sheep & cow & dining table & cell phone & elephant \\
bear & zebra & giraffe & hat & backpack & umbrella \\
shoe & eye glasses & handbag & tie & suitcase & frisbee \\
skis & snowboard & kite & baseball bat & baseball glove & tennis racket \\
wine glass & hot dog & potted plant & teddy bear & hair drier & hair brush \\
skateboard & surfboard & bottle & plate & cup & fork \\
knife & spoon & bowl & banana & apple & sandwich \\
orange & broccoli & carrot & pizza & donut & cake \\
chair & couch & bed & mirror & window & desk \\
toilet & door & tv & laptop & mouse & remote \\
keyboard & microwave & oven & toaster & sink & refrigerator \\
blender & book & clock & vase & scissors & toothbrush \\
\end{tabular}
\end{center}



In this dataset, a collection of images containing 2 to 5 different objects has also been generated. The total number of images in this dataset is approximately 190,000. Various examples from this dataset can be seen in Figure \ref{fig:comco}.


\begin{figure*}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{figs/ComCOv2.pdf}
    \caption{Examples of the ComCO dataset}
    \label{fig:comco}
\end{figure*}

\FloatBarrier  % Prevent other floats from being 




\subsection{Text-based Object Classification}
\label{app:toc}
\subsubsection{Objective}
The Text-based Object Classification experiment was designed to evaluate CLIP's text encoder's ability to represent individual objects within multi-object captions. Our goal was to quantify any potential bias in the representation of objects based on their position in the text.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/clip-toc.pdf}
    \caption{Illustration of the Text-based Object Classification experiment. The figure demonstrates how embeddings are calculated for multi-object captions using CLIP's text encoder. A single-layer classifier is then trained on these embeddings to classify individual objects.}
    \label{fig:toc_main_fig}
\end{figure}



\subsubsection{Methodology}

\begin{enumerate}
    \item \textbf{Dataset Preparation}: 
    \begin{itemize}
        \item We used both the SimCO and ComCO datasets, which contain captions describing scenes with 2 to 5 objects.
        \item Each caption in the dataset follows a consistent format: ``Object1 and Object2 and ... and ObjectN''.
    \end{itemize}

    \item \textbf{Text Embedding Generation}:
    \begin{itemize}
        \item For each multi-object caption, we used CLIP's text encoder to generate a text embedding.
        \item This embedding is a high-dimensional vector representation of the entire caption.
    \end{itemize}

    \item \textbf{Classifier Training}:
    \begin{itemize}
        \item For each object position (1st, 2nd, 3rd, etc.), we trained a separate single-layer classifier.
        \item Input: The text embedding of the multi-object caption.
        \item Output: The predicted object class for that specific position.
    \end{itemize}

    \item \textbf{Evaluation}:
    \begin{itemize}
        \item We tested each classifier on a held-out portion of the dataset.
        \item For each caption, we recorded whether the classifier correctly identified the object at its respective position.
        \item We calculated the classification accuracy for each object position across all test captions.
    \end{itemize}


\end{enumerate}

We conducted the TOC experiment on various models under different scenarios, and the results are presented in Table \ref{tab:toc_total}. This experiment was repeated on both the SIMCO and ComCO datasets.

\input{tables/TOC}

\clearpage  


\subsection{Text-based Object Retrieval}
\label{app:tor}

\subsubsection{Objective}
The Text-based Object Retrieval (TOR) experiment was designed to assess CLIP's text encoder's ability to retrieve individual objects from multi-object captions. This experiment aimed to investigate potential biases in object retrieval based on the object's position within the caption.


\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{figs/clip-tor.pdf}
    \caption{Visualization of the Text-based Object Retrieval experiment. This diagram illustrates the process of retrieving single-object texts based on multi-object captions using CLIP's text encoder. }
    \label{fig:tor_main_fig}
\end{figure}


\subsubsection{Methodology}

\begin{enumerate}
    \item \textbf{Dataset Preparation}: 
    \begin{itemize}
        \item We utilized both the SimCO and ComCO datasets, containing captions describing scenes with 2 to 5 objects.
        \item Each multi-object caption followed the format: ``Object1 and Object2 and ... and ObjectN''.
        \item We also prepared a set of single-object captions for each object class in our datasets.
    \end{itemize}

    \item \textbf{Text Embedding Generation}:
    \begin{itemize}
        \item We used CLIP's text encoder to generate embeddings for all multi-object captions.
        \item Similarly, we generated embeddings for all single-object captions.
    \end{itemize}

    \item \textbf{Similarity Computation}:
    \begin{itemize}
        \item For each multi-object caption, we computed the cosine similarity between its embedding and the embeddings of all single-object captions.
    \end{itemize}

    \item \textbf{Object Retrieval}:
    \begin{itemize}
        \item For each multi-object caption, we identified the single-object caption with the highest similarity score.
        \item We recorded which object from the multi-object caption (1st, 2nd, 3rd, etc.) matched this retrieved single-object caption.
    \end{itemize}

    \item \textbf{Evaluation}:
    \begin{itemize}
        \item We calculated the percentage of times each object position (1st, 2nd, 3rd, etc.) was retrieved as the most similar.
        \item This percentage represents the retrieval accuracy for each object position.
    \end{itemize}
\end{enumerate}

We repeated the TOR experiment on various models across scenarios with captions containing 2 to 5 objects. This was done to confirm the presence of the discovered bias. The complete results of this experiment, which was conducted on both the SIMCO and ComCO datasets, can be observed in Table \ref{tab:tor_total}.


\input{tables/TOR}
\clearpage
\subsection{Image-based Object Classification}
\label{app:ioc}

\subsubsection{Objective}
The Image-based Object Classification (IOC) experiment was designed to evaluate CLIP's image encoder's ability to represent individual objects within multi-object images. This experiment aimed to investigate potential biases in object classification based on the object's size within the image.




\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/clip-ioc.pdf}
    \caption{ Illustration of the Image-based Object Classification experiment with the ComCO dataset. The diagram shows the process of classifying individual objects in K-object images using CLIP's image encoder, with a single-layer classifier trained on the generated image embeddings}


    \label{fig:ioc_main_fig}
\end{figure}
\subsubsection{Methodology}

\begin{enumerate}
    \item \textbf{Dataset Preparation}: 
    \begin{itemize}
        \item We utilized both the SimCO and ComCO datasets, containing images with 2 to 5 objects.
        \item In each image, one object was deliberately made larger than the others.
        \item The position of the larger object was varied across images to avoid position-based biases.
    \end{itemize}

    \item \textbf{Image Embedding Generation}:
    \begin{itemize}
        \item For each multi-object image, we used CLIP's image encoder to generate an image embedding.
        \item This embedding is a high-dimensional vector representation of the entire image.
    \end{itemize}

    \item \textbf{Classifier Training}:
    \begin{itemize}
        \item We trained separate single-layer classifiers for each object position (large object, small object 1, small object 2, etc.).
        \item Input: The image embedding of the multi-object image.
        \item Output: The predicted object class for that specific position/size.
    \end{itemize}

    \item \textbf{Evaluation}:
    \begin{itemize}
        \item We tested each classifier on a held-out portion of the dataset.
        \item For each image, we recorded whether the classifier correctly identified the object at its respective position/size.
        \item We calculated the classification accuracy for each object position/size across all test images.
    \end{itemize}
\end{enumerate}

We conducted the IOC experiment on images from both datasets, focusing on scenarios with one significantly larger object in varying positions. The experiment was repeated across models, and the average results are shown in Table \ref{tab:ioc_total}.


\input{tables/IOC}
\clearpage 


\subsection{Image-based Object Retrieval}
\label{app:ior}
\subsubsection{Objective}
The Image-based Object Retrieval (IOR) experiment was designed to assess CLIP's image encoder's ability to retrieve individual objects from multi-object images. This experiment aimed to investigate potential biases in object retrieval based on the object's size within the image.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/clip-ior.pdf}
    \caption{Visualization of the Image-based Object Retrieval experiment. This diagram illustrates the process of retrieving single-object images based on multi-object image inputs using CLIP's image encoder. The experiment employs a base image containing three objects of varying sizes. CLIP scores are computed between the embedding of this multi-object image and embeddings of various single-object images.}


    \label{fig:ior_main_fig}
\end{figure}


\subsubsection{Methodology}

\begin{enumerate}
    \item \textbf{Dataset Preparation}: 
    \begin{itemize}
        \item We utilized both the SimCO and ComCO datasets, containing images with 2 to 5 objects.
        \item In each multi-object image, one object was deliberately made larger than the others.
        \item The position of the larger object was varied across images to avoid position-based biases.
        \item We also prepared a set of single-object images for each object class in our datasets.
    \end{itemize}

    \item \textbf{Image Embedding Generation}:
    \begin{itemize}
        \item We used CLIP's image encoder to generate embeddings for all multi-object images.
        \item Similarly, we generated embeddings for all single-object images.
    \end{itemize}

    \item \textbf{Similarity Computation}:
    \begin{itemize}
        \item For each multi-object image, we computed the cosine similarity between its embedding and the embeddings of all single-object images.
    \end{itemize}

    \item \textbf{Object Retrieval}:
    \begin{itemize}
        \item For each multi-object image, we identified the single-object image with the highest similarity score.
        \item We recorded whether the retrieved single-object image corresponded to the large object or one of the small objects in the multi-object image.
    \end{itemize}

    \item \textbf{Evaluation}:
    \begin{itemize}
        \item We calculated the percentage of times the large object and each small object were retrieved as the most similar.
        \item This percentage represents the retrieval accuracy for each object size category (large object, small object 1, small object 2, etc.).
    \end{itemize}
\end{enumerate}


We conducted the IOR experiment on images from the SimCO and ComCO datasets with 2 to 5 objects, varying the position of the larger object to avoid location-based biases. The results are shown in Table \ref{tab:ior_total}.
\input{tables/IOR}


\clearpage 
\subsection{Text-based Object Classification for Long Caption}
\label{app:toc-long}

In this section, we revisited the IOC experiment with a significant modification to the caption structure. Our objective was to investigate whether the previously observed bias persists in longer, more elaborate captions. We achieved this by expanding the caption template, incorporating additional descriptive phrases between object mentions.

The extended caption template used in this experiment was as follows:

% \begin{quote}
% This vibrant display features a stunning OBJ1 with its radiant glow, a mesmerizing OBJ2 with bold contours, an enchanting OBJ3 that fits perfectly with its graceful form, a dazzling OBJ4 with brilliant tones and intricate patterns, and an alluring OBJ5 that completes the ensemble with its seamless fusion and distinct shape.
% \end{quote}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{figs/longcaptionformat.pdf}
    \caption{Format for Extended Caption Template}
    \label{fig:comco}
\end{figure}

This template allowed us to maintain a consistent structure while significantly increasing the caption length and complexity.

The results of this modified IOC experiment are presented in Table \ref{tab:toc_long_total}. Notably, the observed pattern closely resembles that of the standard IOC experiment. This similarity suggests that the bias identified in shorter captions persists even in more elaborate textual descriptions.



\input{tables/TOC-Long}




\subsection{Text-based Object Retrieval for Long Caption}
\label{app:tor-long}
\input{tables/TOR-Long}

In this section, we aimed to examine the performance of various models in the IOR experiment when presented with longer caption formats. This approach mirrors our previous investigation, allowing us to draw comparisons between standard and extended caption scenarios.

We utilized the same extended caption template as in the previous section.
The results of this experiment are presented in Table \ref{tab:tor_long_total}. Notably, the observed pattern closely aligns with that of the standard IOR experiment, suggesting a consistency in model behavior across different caption lengths.

% \section{Image-text matching}
% \label{app:imtexmatch}
% In this section, we extended the experiment previously conducted in Section 5.1, broadening its scope to encompass both the SimCO and ComCO datasets. Our investigation covered scenarios involving 2 to 5 objects and was replicated across various models.
% The results of this comprehensive experiment are presented in Table \ref{tab:imgtxtmatch_long_total}.
% \input{tables/I-Tmatch}

\clearpage

\subsection{LAION Dataset Analysis}
\label{subsec:laion_analysis}
\begin{figure*}[h]
    \centering
    \includegraphics[width=\linewidth]{figs/object_detection_v2.pdf}
    \caption{Process flow for LAION dataset analysis}
    \label{fig:laion-objects}
\end{figure*}

To investigate the potential bias in CLIP's training data, as discussed in Section 4.3, Claim 2, we conducted an analysis of the LAION dataset. This process, illustrated in Figure \ref{fig:laion-objects}, consisted of three main stages:

\subsubsection{Stage 1: Dataset Sampling}

Due to the vast size of the LAION dataset (over 2 billion image-text pairs), we randomly selected a subset of 200,000 samples for our analysis. This subset maintained the diversity of the original dataset while making the analysis computationally feasible.

\subsubsection{Stage 2: Object Extraction}

For each image-caption pair in our subset:

\begin{enumerate}
    \item We used the Llama 3 model to extract object mentions from the captions. This step allowed us to identify the objects described in each text without relying on manual annotation.
    
    \item We applied the Grounding DINO + SAM (Segment Anything Model) tool to generate object masks for the corresponding images. This process enabled us to identify and segment individual objects within each image.
\end{enumerate}

\subsubsection{Stage 3: Analysis}

With the extracted data, we performed the following analysis:

\begin{enumerate}
    \item \textbf{Object Order:} We recorded the order in which objects were mentioned in each caption.
    
    \item \textbf{Object Size:} Using the generated masks, we calculated the area of each object in the corresponding image.
    
    \item \textbf{Correlation:} We examined the relationship between an object's position in the caption and its size in the image.
\end{enumerate}

% The results of this analysis, as shown in Figure \ref{fig:laion_analysis}, revealed that:

% \begin{itemize}
%     \item 60\% of the time, the largest object in the image was mentioned first in the caption.
%     \item 25\% of the time, the largest object was mentioned second.
%     \item 15\% of the time, the largest object was mentioned third or later.
% \end{itemize}

AS shown in Figure \ref{fig:coco_analysis_total} This distribution strongly suggests a bias in the LAION dataset where larger objects tend to be mentioned earlier in image captions. This finding supports our hypothesis about the origin of CLIP's text encoder bias, as discussed in Section 4.3 of the main paper.









\subsection{COCO Dataset Analysis}
\label{app:coco-anlysis}
In this section, we repeated the experiment conducted in Section 4.3 for different scenarios involving 2 to 5 objects. We divided the captions in the COCO dataset into four subsets: those mentioning 2 objects, 3 objects, 4 objects, and 5 objects. We then analyzed each subset to determine in what percentage of cases the largest object appeared in which position.

The results of this evaluation are presented in Figure \ref{fig:coco_analysis_total}. As can be observed, this trend is repeated across all scenarios: in most cases, the larger object appears earlier in the caption.
\begin{figure*} [h!]
    \centering
    \includegraphics[width=\linewidth]{figs/datasets_dis.pdf}
    \caption{Distribution of larger object positions in captions for objects in COCO and LAION dataset}
    \label{fig:coco_analysis_total}
\end{figure*}
% \setcounter{page}{1}
% \section{Text-based Object Classification2}


\clearpage




\subsection{Object Categories from DomainNet}
\label{app:categorized_domainnet}
The DomainNet dataset objects were categorized into three groups based on their relative sizes: small, medium, and large. These categories were used to investigate potential bias in CLIP's text embeddings, as discussed in Section 4.3, Claim 1. The full list of objects used in each category is presented below:

\subsubsection{Small Objects}
\begin{center}
\scriptsize % Set font size for consistency
\begin{tabular}{@{} p{2cm} p{2cm} p{2cm} p{2cm} @{}}
ant & anvil & apple & arm \\
asparagus & axe & banana & bandage \\
basket & bat & bee & belt \\
binoculars & bird & blackberry & blueberry \\
book & boomerang & bottlecap & bowtie \\
bracelet & brain & bread & broccoli \\
broom & bucket & butterfly & cactus \\
cake & calculator & calendar & camera \\
candle & carrot & cat & clarinet \\
clock & compass & cookie & crab \\
backpack & crown & cup & dog \\
donut & drill & duck & dumbbell \\
ear & envelope & eraser & eye \\
eyeglasses & feather & finger & fork \\
frog & hammer & hat & headphones \\
hedgehog & helmet & hourglass & jacket \\
keyboard & key & knife & lantern \\
laptop & leaf & lipstick & lobster \\
lollipop & mailbox & marker & megaphone \\
microphone & microwave & mosquito & mouse \\
mug & mushroom & necklace & onion \\
owl & paintbrush & parrot & peanut \\
pear & peas & pencil & pillow \\
pineapple & pizza & pliers & popsicle \\
postcard & potato & purse & rabbit \\
raccoon & radio & rake & rhinoceros \\
rifle & sandwich & saw & saxophone \\
scissors & scorpion & shoe & shovel \\
skateboard & skull & snail & snake \\
snorkel & spider & spoon & squirrel \\
stethoscope & strawberry & swan & sword \\
syringe & teapot & telephone & toaster \\
toothbrush & trombone & trumpet & umbrella \\
violin & watermelon & wheel & \\
\end{tabular}
\end{center}

\subsubsection{Medium Objects}
\begin{center}
\scriptsize
\begin{tabular}{@{} p{2cm} p{2cm} p{2cm} p{2cm} @{}}
angel & bathtub & bear & bed \\
bench & bicycle & camel & cannon \\
canoe & cello & chair & chandelier \\
computer & cooler & couch & cow \\
crocodile & dishwasher & dolphin & door \\
dresser & drums & flamingo & guitar \\
horse & kangaroo & ladder & mermaid \\
motorbike & panda & penguin & piano \\
pig & sheep & stereo & stove \\
table & television & tiger & zebra \\
\end{tabular}
\end{center}

\subsubsection{Large Objects}
\begin{center}
\scriptsize
\begin{tabular}{@{} p{2cm} p{2cm} p{2cm} p{2cm} @{}}
aircraft carrier & airplane & ambulance & barn \\
bridge & bulldozer & bus & car \\
castle & church & cloud & cruise ship \\
dragon & elephant & firetruck & flying saucer \\
giraffe & helicopter & hospital & hot air balloon \\
house & moon & mountain & palm tree \\
parachute & pickup truck & police car & sailboat \\
school bus & skyscraper & speedboat & submarine \\
sun & tent & The Eiffel Tower & Wall of China \\
tractor & train & tree & truck \\
van & whale & windmill & \\
\end{tabular}
\end{center}




\subsection{Text to image generation}
\label{sec:appendix_text_to_image}

The biases observed in CLIP's encoders have significant implications beyond image-text matching, particularly for text-to-image generation models that incorporate CLIP components. To investigate this impact, we focused on Stable Diffusion, a popular text-to-image generation model that utilizes CLIP's text encoder in its pipeline.
Stable Diffusion employs CLIP's text encoder to process input prompts, creating text embeddings that guide the image generation process. Given our identification of biases in CLIP's text encoder, especially the preference for objects mentioned earlier in text descriptions, we hypothesized that these biases would manifest in the generated images.
To test this hypothesis, we designed an experiment using prompts containing multiple objects from the COCO dataset. Our goal was to observe whether the order of objects in the text prompt influences their prominence or likelihood of appearance in the generated images.

Our experimental methodology consisted of three main steps. First, we created 1,000 multi-object prompts, each containing four distinct objects from the COCO dataset. Second, we used these prompts to generate images using three versions of Stable Diffusion: v1.4 \cite{Rombach_2022_CVPR}, v2, and SD-XL \cite{podell2023sdxl}. Finally, to evaluate the presence of objects in the generated images, we employed YOLO v8 \cite{reis2023real}, a state-of-the-art object detection model. We configured YOLO v8 with a detection threshold of 0.25 and used it to validate which objects from the original prompt were present in the generated image.

This approach allowed us to quantitatively assess how CLIP's text encoder biases propagate through the Stable Diffusion pipeline and manifest in the generated images. By comparing the frequency of object detection with their position in the input prompt, we could directly observe the impact of the text-side bias on the image generation process.

\begin{table}[ht]
\centering
\scriptsize
\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{1.2}
\caption{Object presence in Stable Diffusion-generated images }
\label{tab:image_gen}
\begin{tabular}{lcccc}
\toprule
\rowcolor[HTML]{EFEFEF}
Model & \textbf{First Obj} & \textbf{Second Obj} & \textbf{Third Obj} & \textbf{Fourth Obj} \\ 
\midrule 
\textit{SD v1.4} & 57.7 & 44.7 & 38.1 & 35.4 \\
\textit{SD V2} & 62.5 & 49.7 & 47.5 & 42.2 \\
\textit{SD-XL}  & 79.2 & 69.3 & 59.4 & 64.0 \\
\bottomrule
\end{tabular}
\end{table}

Our findings, presented in Table \ref{tab:image_gen}, demonstrate a clear correlation between an object's position in the text prompt and its likelihood of appearing in the generated image. This correlation aligns with our earlier observations of CLIP's text encoder bias, suggesting that these biases significantly influence the output of text-to-image generation models.



\subsection{Preliminary Method for Bias Mitigation}
\label{sec:pre-method}
In our analysis, we observed a critical limitation in the text encoder of CLIP: it disproportionately prioritizes objects mentioned earlier in captions. This bias results in embeddings that heavily represent the first object while progressively diminishing the contribution of subsequent objects. To mitigate this, we explored a novel strategy to reduce positional dependence in object representations.
\subsubsection{Proposed Solution}

We propose splitting a given caption into multiple sub-captions, each focusing on a single object. By generating embeddings for each sub-caption and aggregating these embeddings, we aim to achieve a balanced representation that minimizes positional bias.

To evaluate this approach, we utilized the ComCO dataset, where objects in captions are separated by the conjunction \textit{`and'}. This structure allowed straightforward decomposition of captions into sub-captions corresponding to individual objects. We conducted the image-text matching experiment (described in Section~\ref{sec:impact}) under two conditions: (1) using original captions as-is and (2) using the aggregated embeddings from split captions. Results from this comparison are presented in Table~\ref{table:imagetext_matching_split}.

\subsubsection{Results and Observations}

As shown in Table~\ref{table:imagetext_matching_split}, the aggregated approach led to a substantial improvement in image-text matching accuracy. This outcome suggests that reducing the influence of positional bias can enhance the text encoder's performance in multi-object scenarios. Our findings further underscore the potential of designing methods that neutralize word order effects, thereby enabling more robust and unbiased embeddings.






\begin{table}[ht]
\centering
\scriptsize
\setlength{\tabcolsep}{7pt} % Adjust column spacing
\renewcommand{\arraystretch}{1.2} % Adjust row spacing for readability

\caption{Image-Text Matching Accuracy for ComCO Dataset with Original and Split Caption Aggregation Approaches. The first scenario represents results using original captions, while the second scenario reflects the aggregated embeddings of split captions.}
\label{table:imagetext_matching_split}
\begin{tabular}{l c c}
\toprule
\rowcolor[HTML]{EFEFEF}
\textbf{Model} & \textbf{Original Captions} (\%) & \textbf{Split Caption Aggregation} (\%) \\ 
\midrule

\textit{CLIP Datacomp} \cite{gadre2024datacomp} & 67.50 & \textbf{98.39} \\
\textit{CLIP Roberta} & 64.75 & \textbf{97.35} \\
\textit{SIGLIP} \cite{zhai2023sigmoid} & 72.36 & \textbf{99.05} \\
\textit{CLIP openAI} & 52.23 & \textbf{88.56} \\
\textit{NegCLIP} & 46.94 & \textbf{96.82} \\

\bottomrule
\end{tabular}
\end{table}



\subsubsection{Limitations and Future Directions}

We acknowledge that this solution, while effective for the ComCO dataset, is a heuristic and dataset-specific approach. Its generalizability remains limited. Nonetheless, this experiment demonstrates our commitment to exploring practical solutions and provides a foundation for future advancements.

Future work will focus on developing scalable methods to address positional bias. Possible directions include leveraging large language models (LLMs) to automate caption decomposition into sub-captions and modifying the positional embeddings in the text encoder to ensure equal representation of all objects. These efforts aim to provide a more comprehensive and generalizable solution, paving the way for improved robustness in vision-language models.