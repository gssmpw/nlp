% \section{Methodology}
% \label{sec:methodology}
% \subsection{Dataset Design}
% To thoroughly evaluate the performance of CLIP models in multi-object scenarios and ensure the experiments are fully controlled, we constructed two distinct datasets: \textbf{SimCO} and \textbf{CompCO}, leveraging the Blender software for precise control over the number, location, and dimensions of objects.(see Fig \ref{fig:datasets})
% % \begin{figure*}[t]

% %     \centering
% %     \includegraphics[width=0.8\textwidth]{sec/images/A4 - 1.png}
% %     \caption{Your caption here}
% %     \label{fig:yourlabel}
% % \end{figure*}

% \begin{figure}[t]
%     \centering
%     \includesvg[width=\columnwidth]{sec/images/Group 1.svg}
%     \caption{Examples from the SimCO and CompCO datasets.}
%     \label{fig:datasets}
% \end{figure}
% \subsubsection{SimCO Dataset}
% The \textbf{SimCO} (Simple Objects) dataset, inspired by the CLEVER dataset \cite{johnson2016clevrdiagnosticdatasetcompositional}, includes 17 basic geometric objects compared to the original CLEVER dataset, which only featured 3 objects. This dataset allows us to test the modelâ€™s ability to handle simple shapes and configurations under controlled conditions.

% \subsubsection{CompCO Dataset}
% The \textbf{CompCO} (Complex COCO Objects) dataset, designed to incorporate more complex and commonly occurring objects, comprises 72 objects derived from the COCO dataset \cite{lin2015microsoftcococommonobjects}. This dataset is intended to assess the model's performance with more realistic and intricate object arrangements, which are often found in everyday scenarios.

% In both datasets, we generated images containing  2, 3, 4, and 5 objects. Each image is paired with a specific caption that accurately describes the objects present. This method ensures that the dataset maintains a high degree of control and minimizes the influence of confounding factors, providing a robust platform for evaluating the CLIP models.

% We deliberately chose not to use text-to-image models for generating these datasets for two main reasons. First, these models often lack the capability to produce high-quality, fully controlled multi-object images. Second, using CLIP itself in these models could introduce unwanted biases into our evaluations.

% % \subsection{Dataset Design}

% % To accurately evaluate the performance of CLIP models in multi-object scenarios and conduct fully controlled experiments, we developed specialized datasets. Most existing datasets, collected through internet crawling, lack precise control over the number, location, and dimensions of objects. Inspired by the CLEVER dataset \cite{clever_dataset}, we created two dataset collections: CLEVER-simple and CLEVER-COCO, using the Blender software.

% % The primary advantage of these custom datasets is the complete control over objects, their dimensions, and positions. This allows for model evaluation with minimal influence from confounding factors. We deliberately chose not to use text-to-image models for dataset generation due to two main reasons. First, most text-to-image models lack the ability to produce fully controlled and high-quality multi-object images. Second, the use of CLIP in most of these models could introduce unwanted bias in our evaluations.

% % The CLEVER-SIMPLE dataset, inspired by CLEVER, includes 17 very simple objects, a significant expansion from the original CLEVER dataset which had only 3 objects. This dataset allows for basic multi-object scenario testing with easily distinguishable, simple shapes. In contrast, the CLEVER-COCO dataset aims to use more complex objects and examine the performance of the CLIP model with more common objects. It utilizes 72 objects from the COCO dataset \cite{coco_dataset}, providing a more realistic and challenging set of multi-object scenarios.

% % Both CLEVER-COCO and CLEVER-SIMPLE datasets were generated for scenarios with 2, 3, and 4 objects. Each image in these datasets has a specific caption that includes the objects present in that image. This design allows for systematic evaluation of CLIP's performance as the number of objects increases.

% % The key features of our datasets include precise control over object number, size, and position, scenarios with varying numbers of objects, and a mix of simple and common objects. All images are high-quality, computer-generated, and accompanied by accurate captions describing the objects in each image.

% % By utilizing these carefully designed datasets, we aim to provide a controlled environment for evaluating CLIP's performance in multi-object scenarios, allowing for more precise analysis of the model's strengths and limitations. This approach enables us to isolate the effects of object number, size, and position on CLIP's performance, providing valuable insights into the model's behavior in complex visual environments.

% % % All text must be in a two-column format.
% % % The total allowable size of the text area is $6\frac78$ inches (17.46 cm) wide by $8\frac78$ inches (22.54 cm) high.
% % % Columns are to be $3\frac14$ inches (8.25 cm) wide, with a $\frac{5}{16}$ inch (0.8 cm) space between them.
% % % The main title (on the first page) should begin 1 inch (2.54 cm) from the top edge of the page.
% % % The second and following pages should begin 1 inch (2.54 cm) from the top edge.
% % % On all pages, the bottom margin should be $1\frac{1}{8}$ inches (2.86 cm) from the bottom edge of the page for $8.5 \times 11$-inch paper;
% % % for A4 paper, approximately $1\frac{5}{8}$ inches (4.13 cm) from the bottom edge of the
% % % page.

% %-------------------------------------------------------------------------
% % \subsection{Dataset Design}

% % To accurately evaluate the performance of CLIP models in multi-object scenarios and conduct fully controlled experiments, we need a specialized dataset. Most existing datasets are collected through internet crawling and lack precise control over the number, location, and dimensions of objects. For this reason, inspired by the CLEVER dataset, we have created two dataset collections: CLEVER-simple and CLEVER-COCO, using the Blender software.

% % The advantage of these datasets is complete control over objects, their dimensions, and positions, allowing model evaluation with minimal influence from confounding factors. We did not use text-to-image models to generate this dataset for two reasons: firstly, most of these models lack the ability to produce fully controlled and high-quality multi-object images, and secondly, the use of CLIP in most of these models could introduce unwanted bias in our evaluations.

% % The CLEVER-SIMPLE dataset, inspired by CLEVER, includes 17 very simple objects, while the original CLEVER dataset had only 3 objects. On the other hand, the CLEVER-COCO dataset aims to use more complex objects and examine the performance of the CLIP model with more common objects, utilizing 72 objects from the COCO dataset.
% % In both CLEVER-COCO and CLEVER-SIMPLE datasets, data was generated for scenarios with 2, 3, and 4 objects. Each image in these datasets has a specific caption that includes the objects present in that image.

% % % \begin{figure*}[t]

% % %     \centering
% % %     \includegraphics[width=0.8\textwidth]{sec/images/A4 - 1.png}
% % %     \caption{Your caption here}
% % %     \label{fig:yourlabel}
% % % \end{figure*}

% % \begin{figure}[t]
% %     \centering
% %     \includesvg[width=\columnwidth]{sec/images/Group 1.svg}
% %     \caption{Your figure caption here}
% %     \label{fig:your_label}
% % \end{figure}

% %-------------------------------------------------------------------------
% % \subsection{Experimental Setup}

% % % Using the two datasets at our disposal, we design multiple experiments on the image encoder and text encoder of the CLIP model in a multi-object scenario. The aim of these experiments is to examine the overall performance of each encoder when faced with a multi-object scenario.


% % To evaluate the performance of CLIP's image and text encoders in multi-object scenarios, we design a series of experiments utilizing two available datasets. The primary objective is to assess the overall effectiveness of each encoder when confronted with multiple objects in their respective modalities.


% % \subsubsection{Image Encoder Analysis}

% % For the image encoder experiments, we focus exclusively on visual inputs and the CLIP image encoder's processing of multi-object images. We hypothesize that the image encoder exhibits biases related to object size and position within the image. In multi-object scenarios, we posit that larger objects and those centrally positioned in the image have a more significant impact on the model's final visual representation. To investigate this hypothesis, we conduct two distinct experiments using multi-object images:

% % \textbf{Single-Layer Classifier:} For each object present in a multi-object image, we train a classifier on the vector representations generated by the CLIP image encoder. We then calculate the accuracy of these classifiers for individual objects within the images.

% % \textbf{Individual Retrieval:} For each multi-object image, we identify the most similar single-object images using the CLIP image encoder's representations. We then compute the percentage of cases where the nearest single-object image corresponds to the first object, the second object, and so on in the multi-object image.

% % \subsubsection{Text Encoder Analysis}

% % In parallel, we conduct experiments focusing solely on textual inputs and the CLIP text encoder's processing of multi-object textual descriptions. Our evaluation aims to determine the extent to which each object mentioned in a multi-object textual description contributes to CLIP's final text representation, relative to its position in the text. We employ two experiments analogous to those used in the image analysis, but applied to text:

% % \textbf{Single-Layer Classifier: }For each object mentioned in a multi-object text description, we train a classifier on the vector representations produced by the CLIP text encoder. We then assess the accuracy of these classifiers for individual objects within the text.

% % \textbf{Individual Retrieval:} For each multi-object text description, we identify the most similar single-object text using the CLIP text encoder's representations. We then calculate the percentage of cases where the nearest single-object text corresponds to the first mentioned object, the second object, and so forth in the multi-object text description.


% %-------------------------------------------------------------------------
% \subsection{Experimental Setup}

% To evaluate CLIP's performance in multi-object scenarios, we designed a series of experiments focusing on both the image and text encoders. Our goal was to assess how each encoder processes and represents multiple objects in their respective modalities.

% \subsubsection{Image Encoder Analysis}

% We hypothesized that the image encoder exhibits biases related to object size. To investigate this, we conducted two experiments:
% \begin{enumerate}
%     \item \textbf{Image-based Object Classification (IOC):} For each object in a multi-object image, we trained a single-layer classifier on the vector representations generated by the CLIP image encoder. We then calculated the classification accuracy for individual objects within the images.
    
%     \item \textbf{Image-based Object Retrieval (IOR):} For each multi-object image, we identified the most similar single-object images using the CLIP image encoder's representations. We then computed the percentage of cases where the nearest single-object image corresponded to each object in the multi-object image (e.g., first object, second object, etc.).
% \end{enumerate}

% These experiments aimed to determine whether larger objects in the image have a more significant impact on the model's final visual representation.

% \subsubsection{Text Encoder Analysis}

% For the text encoder, we designed experiments to determine the extent to which each object mentioned in a multi-object textual description contributes to CLIP's final text representation, relative to its position in the text:

% \begin{enumerate}
%     \item \textbf{Text-based Object Classification (TOC):} For each object mentioned in a multi-object text description, we trained a single-layer classifier on the vector representations produced by the CLIP text encoder. We then assessed the classification accuracy for individual objects within the text.
    
%     \item \textbf{Text-based Object Retrieval (TOR):} For each multi-object text description, we identified the most similar single-object text using the CLIP text encoder's representations. We then calculated the percentage of cases where the nearest single-object text corresponded to each mentioned object in the multi-object text description (e.g., first mentioned, second mentioned, etc.).
% \end{enumerate}

% These experiments were designed to reveal any potential biases in the text encoder related to the order of object mentions in the textual descriptions.






\section{Methodology}
\label{sec:methodology}






% \begin{figure}[t]
%     \centering
%     \includesvg[width=\columnwidth]{sec/images/Group 1.svg}
%     \caption{Examples from the SimCO and CompCO datasets.}
%     \label{fig:datasets}
% \end{figure}

\subsection{Dataset Design}
To thoroughly evaluate the performance of CLIP models in multi-object scenarios under controlled conditions, we constructed the \textbf{ComCO} (Complex COCO Objects) dataset. Utilizing Blender software allowed us precise control over the number, location, and dimensions of objects in the images (see Appendix \ref{app:dataset}).
The \textbf{ComCO} dataset comprises 72 objects derived from the COCO dataset. We generated images containing 2, 3, 4, and 5 objects. Each image is paired with a specific caption that accurately describes the objects present. This approach ensures high control over the dataset and minimizes confounding factors, providing a robust platform for evaluating the CLIP models.

We deliberately chose not to use text-to-image models for generating these datasets due to two main reasons. First, these models often lack the capability to produce high-quality, fully controlled multi-object images. Second, since CLIP is used in many of these models, utilizing them could introduce unwanted biases into our evaluations.




% \subsection{Investigating Image and Text Encoders}

% To comprehensively assess CLIP's performance in multi-object scenarios, we designed four key experiments evaluating both the text and image encoders. These experiments were conducted using our SimCO and CompCO datasets, covering scenarios with 2 to 5 objects per image or caption, and replicated on the real-world COCO dataset to ensure our findings generalize beyond our custom datasets. Our experimental framework comprises:

% \begin{itemize}
%     \item Text-based Object Classification (\textbf{TOC}, see Appendix \ref{app:toc})
%     \item Text-based Object Retrieval (\textbf{TOR}, see Appendix \ref{app:tor})
%     \item Image-based Object Classification (\textbf{IOC}, see Appendix \ref{app:ioc})
%     \item Image-based Object Retrieval (\textbf{IOR}, see Appendix \ref{app:ior})
% \end{itemize}

% The text-based experiments (\textbf{TOC} and \textbf{TOR}) examine how CLIP's text encoder processes multi-object captions, focusing on potential biases related to object mention order. The image-based experiments (\textbf{IOC} and \textbf{IOR}) investigate the image encoder's handling of multi-object scenes, with particular attention to potential size-related biases.

% We replicated each experiment across various CLIP model variants to ensure robust findings. This comprehensive approach allows for a thorough examination of CLIP's performance in complex visual and linguistic contexts, revealing key insights into the model's behavior when processing multiple objects simultaneously.Detailed methodologies for each experiment, including data preparation and evaluation processes, are provided in their respective appendix sections.


\subsection{Experimental Framework for Encoder Analysis}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/ior-tor-v3.pdf}
    \caption{Experimental setup for Text-based Object Retrieval (TOR) and Image-based Object Retrieval (IOR) tasks.
    a) TOR: The CLIP text encoder generates embeddings for multi-object and single-object texts. Cosine similarity scores are calculated between the base text embedding and single-object text embeddings to identify the most similar object.
    b) IOR: The CLIP image encoder generates embeddings for multi-object and single-object images. Cosine similarity scores are calculated between the base image embedding and single-object image embeddings to identify the most similar object.
}
    \label{fig:retrievals}
        \vspace{-0.5cm} 
\end{figure*}



% Our study aims to assess CLIP's performance in multi-object scenarios by examining both its text and image encoders. We conducted experiments using our custom SimCO and CompCO datasets, which feature images and captions containing two to five objects. To ensure the generalizability of our findings, we also validated our results on the widely-used COCO dataset \cite{lin2014microsoft}.


The main goal of this study is to evaluate the performance of CLIP's text and image encoders separately in multi-object scenarios. We aim to analyze the impact and contribution of each object in the final output of the encoders. To achieve this, we conducted experiments using our designed ComCO dataset, with images and captions containing two to five objects. To ensure the generalizability of our findings, we also validated our results on the widely-used COCO dataset. We designed two sets of experiments: retrieval-based experiments and classification-based experiments. Given the consistency of the results in both types of experiments, we have included the classification results in the appendix \ref{app:toc} and \ref{app:ioc} and explain the retrieval-based experiments bellow.

\subsubsection{TEXT-BASED OBJECT RETRIEVAL (TOR)}

% The Text-based Object Retrieval task evaluates how well CLIP's text encoder can identify individual objects within multi-object captions. This experiment involves several steps:
% First, we use CLIP's text encoder to create embeddings for both multi-object captions and single-object captions. We then measure the similarity between each multi-object caption embedding and all single-object caption embeddings. The single-object caption with the highest similarity score is considered the ``retrieved'' object. To assess performance, we calculate retrieval accuracy for each object position in the multi-object captions. This helps us identify any biases related to an object's position within a caption, such as favoring objects mentioned first or last.
The Text-based Object Retrieval task evaluates how well CLIP's text encoder can identify individual objects within multi-object captions. As illustrated in Figure \ref{fig:retrievals}a, this experiment involves several steps: First, we use CLIP's text encoder to create embeddings for both multi-object captions and single-object captions. We then measure the similarity between each multi-object caption embedding and all single-object caption embeddings. The single-object caption with the highest similarity score is considered the "retrieved" object. To assess performance, we calculate retrieval accuracy for each object position in the multi-object captions. This helps us identify any biases related to an object's position within a caption, such as favoring objects mentioned first or last.

\subsubsection{IMAGE-BASED OBJECT RETRIEVAL (IOR)}

The Image-based Object Retrieval task is similar to TOR but focuses on CLIP's image encoder. As shown in Figure \ref{fig:retrievals}b, this experiment involves several steps: We begin by using CLIP's image encoder to generate embeddings for multi-object images and single-object images. We then compute similarity scores between each multi-object image embedding and all single-object image embeddings. The single-object image with the highest similarity score is considered the "retrieved" object. To evaluate performance, we calculate retrieval accuracy for different object size categories (e.g., large, small) within the multi-object images. This allows us to determine if the image encoder shows any preference for objects of a particular size.

We also experimented with a variation of ComCO, called SimCO, where objects were replaced with simple geometric shapes from the CLEVR dataset. This was done to confirm that bias persists even with non-natural, geometric objects. Further details are provided in Appendix \ref{app:dataset}.

% The Image-based Object Retrieval task is similar to TOR but focuses on CLIP's image encoder. This experiment involves several steps:
% We begin by using CLIP's image encoder to generate embeddings for multi-object images and single-object images. We then compute similarity scores between each multi-object image embedding and all single-object image embeddings. The single-object image with the highest similarity score is considered the ``retrieved'' object. To evaluate performance, we calculate retrieval accuracy for different object size categories (e.g., large, small) within the multi-object images. This allows us to determine if the image encoder shows any preference for objects of a particular size.

% \subsubsection{Comprehensive Evaluation}

% To ensure robust findings, we replicated these experiments across various CLIP model variants. Our analysis considered several factors:

% 1. Dataset diversity: We used SimCO, CompCO, and COCO datasets to cover a range of scenarios.
% 2. Object count variation: We examined cases with two, three, four, and five objects per image or caption.
% 3. Model architecture: Multiple CLIP variants were tested to assess the consistency of observed biases across different model architectures.

% This comprehensive approach allows us to systematically investigate potential biases in both the text and image encoders of CLIP. By doing so, we aim to provide valuable insights into the model's limitations when dealing with multi-object scenarios, particularly in retrieval tasks.