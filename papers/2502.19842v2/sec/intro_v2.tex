\section{Introduction}

The convergence of vision and language in artificial intelligence has led to the development of Vision-Language Models (VLMs) that can interpret and generate multimodal content. Among these, OpenAI's Contrastive Language-Image Pre-training (CLIP) model~\cite{radford2021learningtransferablevisualmodels} has been particularly influential, demonstrating remarkable capabilities in zero-shot image classification and setting new standards for multimodal understanding~\cite{Cherti_2023, gadre2023datacompsearchgenerationmultimodal, schuhmann2021laion400mopendatasetclipfiltered, thrush2022winoground}. The success of CLIP has catalyzed a wide array of applications---from image retrieval and visual question answering to text-to-image generation---signifying a paradigm shift in how models perceive and relate visual and linguistic information.




Visual Language Models like CLIP face significant challenges in understanding and reasoning about complex scenes with multiple objects and intricate relationships. CLIP struggles to identify distinct objects and model their relationships accurately, especially when captions contain the same objects but differ in their relationships. This results in difficulty distinguishing between similar captions with different object relationships. Several benchmark datasets have been introduced to elucidate the limitations of existing models in capturing subtle relational nuances. Notably, Winoground \cite{thrush2022winoground}, VL-CheckList \cite{zhao2022vl}, ARO \cite{yuksekgonul2023and}, and CREPE \cite{ma2023crepe} have been instrumental in evaluating models' capacities to accurately match images with semantically appropriate captions. 



\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figs/main_fig_v5.pdf}
    \caption{Overview of our key contributions. Step 1: We create ComCO dataset for controlled multi-object experiments. Step 2: We identify biases in CLIP's image encoder (favoring larger objects) and text encoder (prioritizing first-mentioned objects). Step 3: We investigate the origin of these biases, finding a connection to training data characteristics. Step 4: We demonstrate the practical impacts of these biases on image-text matching task, showing how they affect model performance in multi-object scenarios.}
    \label{fig:mainfig}
    \vspace{-0.5cm} 
\end{figure*}

Numerous studies have addressed compositionality challenges in multi-object scenarios, often through end-to-end methods like fine-tuning with hard-negative samples \cite{yuksekgonul2023and} to improve model performance. However, these approaches have faced criticism and subsequent refinement, as seen in methods like SUGARCREPE \cite{hsieh2024sugarcrepe} and \cite{sahin2024enhancing}, which generate negative captions with minor structural changes or LLMs to highlight semantic distinctions. While most focus on CLIP’s ability to distinguish structurally similar yet conceptually different captions, few studies, such as Dumpala et al. \cite{dumpala2024sugarcrepe++}, explore CLIP’s performance on semantically equivalent but structurally distinct captions, revealing a gap in understanding CLIP's inconsistency with such prompts.




% While previous studies have made significant strides in understanding CLIP's limitations, our work distinguishes itself in several key aspects. Firstly, we shift the focus from evaluating CLIP's ability to differentiate between conceptually distinct captions to examining its performance with semantically equivalent but structurally varied captions. This approach allows us to probe deeper into the model's understanding of language and visual content beyond surface-level differences. Here, model systematic mistakes give an indication the potential baises. Secondly, unlike many previous works that primarily introduced benchmarks or proposed end-to-end solutions, we conduct a thorough investigation into the underlying causes of CLIP's behavior. Our study delves into the internal mechanisms of both the image and text encoders, providing insights into why the model is biased and lacks invariance to certain types of linguistic and visual variations. 

% To facilitate this in-depth analysis, we introduce the \textbf{ComCO} dataset, specifically designed to isolate and examine different aspects of CLIP's performance in {\it controlled} multi-object scenarios. Furthermore, our research spans multiple versions of CLIP trained on various datasets and architectures, ensuring the broad applicability and generalizability of our findings. By focusing on these underexplored areas and employing a more comprehensive analytical approach, our work aims to provide a deeper understanding of CLIP's limitations and pave the way for more robust and versatile vision-language models. It is important to note that such an analysis not only benefits the improvement of CLIP but also has significant implications for related models, such as text-to-image (T2I) generative models and multimodal large language models (MLLMs). Understanding the intricacies of CLIP's encoding process can inform and enhance the development of these technologies, potentially leading to advancements across various domains of artificial intelligence. As shown in Figure \ref{fig:mainfig}, our key contributions are as follows:


While previous studies have advanced our understanding of CLIP's limitations, our work uniquely focuses on CLIP's performance with semantically equivalent but structurally varied captions rather than simply distinguishing conceptually different captions. This shift enables a deeper examination of the model’s grasp of language and visual content, where systematic errors reveal potential biases. Unlike prior works that primarily propose benchmarks or end-to-end solutions, we investigate the root causes of CLIP's behavior, delving into the mechanisms of both image and text encoders to uncover why the model displays biases and lacks robustness to certain linguistic and visual variations. To support this analysis, we introduce the \textbf{ComCO} dataset, purpose-built for examining CLIP's performance under {\it controlled} multi-object scenarios. Our study spans multiple versions of CLIP trained on diverse datasets and architectures, ensuring the broad applicability of our findings. This comprehensive approach aims to deepen our understanding of CLIP’s limitations and pave the way for more adaptable vision-language models. Beyond CLIP, our insights have significant implications for text-to-image (T2I) generative models and multimodal large language models (MLLMs), where decoding CLIP’s encoding intricacies can inform advancements in artificial intelligence across domains. As shown in Figure \ref{fig:mainfig}, our key contributions are as follows:



\begin{itemize} \item \textbf{Development of Novel Dataset}: We introduce \textit{ComCO}, a specialized dataset for creating {\it controlled} multi-object scenarios. Unlike previous benchmarks, ComCO allows control over object size and caption order, enabling precise analysis of model performance across compositional challenges and enhancing understanding of VLMs' strengths and weaknesses.

\item \textbf{Encoder Analysis}: We conduct an in-depth examination of CLIP’s image and text encoders in multi-object scenes, revealing weaknesses in preserving information for object distinction and identifying where compositional information is lost.

\item \textbf{Bias Identification}: Our study reveals that CLIP’s image encoder prefers larger objects, while the text encoder favors first-mentioned and visually larger objects, highlighting biases in CLIP's handling of visual and linguistic information.

\item \textbf{Investigation of Bias Origins}: We explore the origins of these biases, showing that larger objects are often mentioned earlier in CLIP’s training captions, and are favored in embeddings due to the abundance of their visual tokens. We substantiate this with analyses of the LAION dataset and CLIP’s training progression.

\item \textbf{Practical Impact}: We show how these biases affect performance in multi-object tasks, with significant drops in image-text matching accuracy in ComCO and COCO ~\cite{lin2015microsoftcococommonobjects}. These biases also extend to text-to-image models, influencing object prominence based on prompt order.

\end{itemize}



% \begin{itemize} \item \textbf{Development of Novel Dataset}: We introduce \textit{ComCO}, a specialized dataset specifically designed to create {\it controlled} multi-object scenarios. Here, unlike previous benchmarks, we can control the object size in the image, and their ordering in the caption. Hence, this dataset enables precise, fine-grained analysis of model performance across a spectrum of compositional challenges, facilitating a deeper understanding of VLMs' strengths and weaknesses.

%     \item \textbf{Comprehensive Encoder Analysis}: We perform an in-depth examination of both the image and text encoders in CLIP when processing multi-object scenes and descriptions. This includes text-based, and object-based image retrievals, that reveal each text and image encoder weaknesses in preserving the information necessary to discern various objects. By analyzing the embedding space, we identify the stages at which compositional information is lost or distorted, providing insights into the internal mechanisms of the model.
    
%     \item \textbf{Identification of Specific Biases}: Our research uncovers significant biases in CLIP models. The image encoder prefers larger objects in multi-object images, while the text encoder favors first-mentioned objects and also objects that are usually visually larger in real-world. These biases reveal the complex interplay between visual and linguistic information processing in CLIP, influencing its interpretation of multi-object scenarios.
    
%     \item \textbf{Investigation of the Bias Origin }: We explore the origins of observed biases in CLIP's performance, particularly in various multi-object scenarios. Our investigation delves into both the image and text encoders. We hypothesize that the visually larger objects are mostly mentioned earlier in the caption in CLIP training datasets. But it is evident that the image encoding naturally favors such objects in the embedding due to the abundance of their visual tokens. Therefore, the text encoder may get biased towards such objects, and consequently earlier mentioned text tokens. We provide evidence for these biases through analyses of the LAION dataset and CLIP's training progression, revealing a consistent trend where larger objects tend to be mentioned earlier in image captions.
    
%     \item \textbf{Practical impacts of encoder biases}: We demonstrate how the identified biases in CLIP's image and text encoders significantly impact performance in multi-object analysis/synthesis scenarios. Using our ComCO dataset, we show substantial drops in image-text matching accuracy when manipulating object sizes and caption order. We further reveal how these biases propagate to text-to-image generation models like Stable Diffusion, influencing the prominence and likelihood of object appearance in generated images based on prompt order.
    
%     \end{itemize}

These findings reveal how biases in CLIP’s text and image encoders significantly reduce its performance in multi-object scenarios, emphasizing the need to address these biases to enhance vision-language models' robustness. Our work offers key insights into CLIP's behavior and lays groundwork for improving model performance in real-world applications.




