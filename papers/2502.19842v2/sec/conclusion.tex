\section{Conclusion}


This study uncovers biases in CLIP’s encoders, with the text encoder favoring first-mentioned objects and the image encoder emphasizing larger ones, which impacts performance in multi-object tasks. Using the ComCO dataset, we highlighted these biases' effects on object representation and positioning, underscoring the need for balanced training. We attribute these biases to CLIP's contrastive framework, where alignment issues propagate across modalities. Addressing these biases is essential for vision-language advancements, as seen with models like Stable Diffusion. Future work should explore counterfactual data augmentation and attention regularization to reduce such biases.

% This study reveals substantial biases in CLIP's text and image encoders, with tendencies toward prioritizing first-mentioned and larger objects, respectively, which complicates performance in multi-object scenarios. Through our introduction of the ComCO dataset, we demonstrated the tangible impact of these biases on tasks requiring nuanced object representation and positioning, emphasizing the need for balanced training approaches.

% Our findings suggest that these biases stem from CLIP’s contrastive learning framework, where image-text alignment can propagate biases from one modality to the other. Addressing these biases is crucial for advancing vision-language models, as shown by our experiments with models like Stable Diffusion, where bias affects image generation based on prompt order. Future work should explore methods like counterfactual data augmentation and attention regularization to mitigate these issues, supported by datasets designed to assess multi-object biases across model architectures.

