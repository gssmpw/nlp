@article{bergamin2024riemannian,
  title={Riemannian Laplace approximations for Bayesian neural networks},
  author={Bergamin, Federico and Moreno-Mu{\~n}oz, Pablo and Hauberg, S{\o}ren and Arvanitidis, Georgios},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{arvanitidis2021pulling,
  title={Pulling back information geometry},
  author={Arvanitidis, Georgios and Gonz{\'a}lez-Duque, Miguel and Pouplin, Alison and Kalatzis, Dimitris and Hauberg, S{\o}ren},
  journal={arXiv preprint arXiv:2106.05367},
  year={2021}
}

@article{yu2023riemannian,
  title={Riemannian Laplace Approximation with the Fisher Metric},
  author={Yu, Hanlin and Hartmann, Marcelo and Williams, Bernardo and Girolami, Mark and Klami, Arto},
  journal={arXiv preprint arXiv:2311.02766},
  year={2023}
}

@inproceedings{hartmann2022lagrangian,
  title={Lagrangian manifold monte carlo on monge patches},
  author={Hartmann, Marcelo and Girolami, Mark and Klami, Arto},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={4764--4781},
  year={2022},
  organization={PMLR}
}

@inproceedings{lin2014microsoft,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13},
  pages={740--755},
  year={2014},
  organization={Springer}
}

@inproceedings{kim2022fisher,
  title={Fisher sam: Information geometry and sharpness aware minimisation},
  author={Kim, Minyoung and Li, Da and Hu, Shell X and Hospedales, Timothy},
  booktitle={International Conference on Machine Learning},
  pages={11148--11161},
  year={2022},
  organization={PMLR}
}

@article{foret2020sharpness,
  title={Sharpness-aware minimization for efficiently improving generalization},
  author={Foret, Pierre and Kleiner, Ariel and Mobahi, Hossein and Neyshabur, Behnam},
  journal={arXiv preprint arXiv:2010.01412},
  year={2020}
}

@article{hauberg2024differential,
    title={Differential Geometry for Generative Modeling},
    author={Hauberg, Søren},
    journal={Lecture notes},
    year={2024}
}

@article{smith2014optimization,
  title={Optimization techniques on Riemannian manifolds},
  author={Smith, Steven Thomas},
  journal={arXiv preprint arXiv:1407.5965},
  year={2014}
}

@inproceedings{liu2023same,
  title={Same pre-training loss, better downstream: Implicit bias matters for language models},
  author={Liu, Hong and Xie, Sang Michael and Li, Zhiyuan and Ma, Tengyu},
  booktitle={International Conference on Machine Learning},
  pages={22188--22214},
  year={2023},
  organization={PMLR}
}

@article{zhou2024sharpness,
  title={Sharpness-Aware Minimization Efficiently Selects Flatter Minima Late in Training},
  author={Zhou, Zhanpeng and Wang, Mingze and Mao, Yuchen and Li, Bingrui and Yan, Junchi},
  journal={arXiv preprint arXiv:2410.10373},
  year={2024}
}

@article{behdin2023sharpness,
  title={Sharpness-aware minimization: An implicit regularization perspective},
  author={Behdin, Kayhan and Mazumder, Rahul},
  journal={stat},
  volume={1050},
  pages={23},
  year={2023}
}

@article{yun2024riemannian,
  title={Riemannian SAM: Sharpness-Aware Minimization on Riemannian Manifolds},
  author={Yun, Jihun and Yang, Eunho},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{ahn2023escape,
  title={How to escape sharp minima},
  author={Ahn, Kwangjun and Jadbabaie, Ali and Sra, Suvrit},
  journal={arXiv preprint arXiv:2305.15659},
  year={2023}
}

@article{kaddour2022flat,
  title={When do flat minima optimizers work?},
  author={Kaddour, Jean and Liu, Linqing and Silva, Ricardo and Kusner, Matt J},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={16577--16595},
  year={2022}
}

@inproceedings{agarwala2023sam,
  title={SAM operates far from home: eigenvalue regularization as a dynamical phenomenon},
  author={Agarwala, Atish and Dauphin, Yann},
  booktitle={International Conference on Machine Learning},
  pages={152--168},
  year={2023},
  organization={PMLR}
}

@article{xie2020diffusion,
  title={A diffusion theory for deep learning dynamics: Stochastic gradient descent exponentially favors flat minima},
  author={Xie, Zeke and Sato, Issei and Sugiyama, Masashi},
  journal={arXiv preprint arXiv:2002.03495},
  year={2020}
}

@inproceedings{kwon2021asam,
  title={Asam: Adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks},
  author={Kwon, Jungmin and Kim, Jeongseop and Park, Hyunseo and Choi, In Kwon},
  booktitle={International Conference on Machine Learning},
  pages={5905--5914},
  year={2021},
  organization={PMLR}
}

@article{dai2024crucial,
  title={The crucial role of normalization in sharpness-aware minimization},
  author={Dai, Yan and Ahn, Kwangjun and Sra, Suvrit},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{andriushchenko2022towards,
  title={Towards understanding sharpness-aware minimization},
  author={Andriushchenko, Maksym and Flammarion, Nicolas},
  booktitle={International Conference on Machine Learning},
  pages={639--668},
  year={2022},
  organization={PMLR}
}

@article{li2024friendly,
  title={Friendly sharpness-aware minimization},
  author={Li, Tao and Zhou, Pan and He, Zhengbao and Cheng, Xinwen and Huang, Xiaolin},
  journal={arXiv preprint arXiv:2403.12350},
  year={2024}
}

@article{ujvary2022rethinking,
  title={Rethinking sharpness-aware minimization as variational inference},
  author={Ujv{\'a}ry, Szilvia and Telek, Zsigmond and Kerekes, Anna and M{\'e}sz{\'a}ros, Anna and Husz{\'a}r, Ferenc},
  journal={arXiv preprint arXiv:2210.10452},
  year={2022}
}

@inproceedings{dinh2017sharp,
  title={Sharp minima can generalize for deep nets},
  author={Dinh, Laurent and Pascanu, Razvan and Bengio, Samy and Bengio, Yoshua},
  booktitle={International Conference on Machine Learning},
  pages={1019--1028},
  year={2017},
  organization={PMLR}
}

@article{keskar2016large,
  title={On large-batch training for deep learning: Generalization gap and sharp minima},
  author={Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  journal={arXiv preprint arXiv:1609.04836},
  year={2016}
}

@article{hochreiter1997flat,
  title={Flat minima},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={1},
  pages={1--42},
  year={1997},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{chaudhari2019entropy,
  title={Entropy-sgd: Biasing gradient descent into wide valleys},
  author={Chaudhari, Pratik and Choromanska, Anna and Soatto, Stefano and LeCun, Yann and Baldassi, Carlo and Borgs, Christian and Chayes, Jennifer and Sagun, Levent and Zecchina, Riccardo},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2019},
  number={12},
  pages={124018},
  year={2019},
  publisher={IOP Publishing}
}

@article{jastrzkebski2018finding,
  title={Finding flatter minima with sgd},
  author={Jastrzebski, Stanis{\l}aw and Kenton, Zachary and Arpit, Devansh and Ballas, Nicolas and Fischer, Asja and Bengio, Yoshua and Storkey, Amos},
  year={2018},
}

@article{jastrzkebski2017three,
  title={Three factors influencing minima in sgd},
  author={Jastrzebski, Stanis{\l}aw and Kenton, Zachary and Arpit, Devansh and Ballas, Nicolas and Fischer, Asja and Bengio, Yoshua and Storkey, Amos},
  journal={arXiv preprint arXiv:1711.04623},
  year={2017}
}

@article{xing2018walk,
  title={A walk with sgd},
  author={Xing, Chen and Arpit, Devansh and Tsirigotis, Christos and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1802.08770},
  year={2018}
}

@article{zhu2018anisotropic,
  title={The anisotropic noise in stochastic gradient descent: Its behavior of escaping from sharp minima and regularization effects},
  author={Zhu, Zhanxing and Wu, Jingfeng and Yu, Bing and Wu, Lei and Ma, Jinwen},
  journal={arXiv preprint arXiv:1803.00195},
  year={2018}
}

@inproceedings{smith2020generalization,
  title={On the generalization benefit of noise in stochastic gradient descent},
  author={Smith, Samuel and Elsen, Erich and De, Soham},
  booktitle={International Conference on Machine Learning},
  pages={9058--9067},
  year={2020},
  organization={PMLR}
}

@article{ahn2024learning,
  title={Learning threshold neurons via edge of stability},
  author={Ahn, Kwangjun and Bubeck, S{\'e}bastien and Chewi, Sinho and Lee, Yin Tat and Suarez, Felipe and Zhang, Yi},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}



@article{kristiadi2024geometry,
  title={The geometry of neural nets' parameter spaces under reparametrization},
  author={Kristiadi, Agustinus and Dangel, Felix and Hennig, Philipp},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{kerekes2021depth,
  title={Depth Without the Magic: Inductive Bias of Natural Gradient Descent},
  author={Kerekes, Anna and M{\'e}sz{\'a}ros, Anna and Husz{\'a}r, Ferenc},
  journal={arXiv preprint arXiv:2111.11542},
  year={2021}
}

@article{amari1996neural,
  title={Neural learning in structured parameter spaces-natural Riemannian gradient},
  author={Amari, Shun-ichi},
  journal={Advances in neural information processing systems},
  volume={9},
  year={1996}
}

@book{lee2012smooth,
  title={Smooth manifolds},
  author={Lee, John M and Lee, John M},
  year={2012},
  publisher={Springer}
}

@article{pascanu2013revisiting,
  title={Revisiting natural gradient for deep networks},
  author={Pascanu, R},
  journal={arXiv preprint arXiv:1301.3584},
  year={2013}
}

@article{martens2020new,
  title={New insights and perspectives on the natural gradient method},
  author={Martens, James},
  journal={Journal of Machine Learning Research},
  volume={21},
  number={146},
  pages={1--76},
  year={2020}
}

@article{santos2022avoiding,
  title={Avoiding overfitting: A survey on regularization methods for convolutional neural networks},
  author={Santos, Claudio Filipi Gon{\c{c}}alves Dos and Papa, Jo{\~a}o Paulo},
  journal={ACM Computing Surveys (CSUR)},
  volume={54},
  number={10s},
  pages={1--25},
  year={2022},
  publisher={ACM New York, NY}
}

@article{jiang2019fantastic,
  title={Fantastic generalization measures and where to find them},
  author={Jiang, Yiding and Neyshabur, Behnam and Mobahi, Hossein and Krishnan, Dilip and Bengio, Samy},
  journal={arXiv preprint arXiv:1912.02178},
  year={2019}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Toronto, ON, Canada}
}

@article{loshchilov2016sgdr,
  title={Sgdr: Stochastic gradient descent with warm restarts},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1608.03983},
  year={2016}
}

@article{dziugaite2017computing,
  title={Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data},
  author={Dziugaite, Gintare Karolina and Roy, Daniel M},
  journal={arXiv preprint arXiv:1703.11008},
  year={2017}
}

@article{liu2020bad,
  title={Bad global minima exist and sgd can reach them},
  author={Liu, Shengchao and Papailiopoulos, Dimitris and Achlioptas, Dimitris},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={8543--8552},
  year={2020}
}

@article{kim2023stability,
  title={Stability analysis of sharpness-aware minimization},
  author={Kim, Hoki and Park, Jinseong and Choi, Yujin and Lee, Jaewook},
  journal={arXiv preprint arXiv:2301.06308},
  year={2023}
}

@article{belkin2019reconciling,
  title={Reconciling modern machine-learning practice and the classical bias--variance trade-off},
  author={Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  journal={Proceedings of the National Academy of Sciences},
  volume={116},
  number={32},
  pages={15849--15854},
  year={2019},
  publisher={National Acad Sciences}
}

@article{khanh2024fundamental,
  title={Fundamental Convergence Analysis of Sharpness-Aware Minimization},
  author={Khanh, Pham Duy and Luong, Hoang-Chau and Mordukhovich, Boris S and Tran, Dat Ba},
  journal={arXiv preprint arXiv:2401.08060},
  year={2024}
}

@article{shin2023effects,
  title={The Effects of Overparameterization on Sharpness-aware Minimization: An Empirical and Theoretical Analysis},
  author={Shin, Sungbin and Lee, Dongyeop and Andriushchenko, Maksym and Lee, Namhoon},
  year={2023}
}

@article{si2024practical,
  title={Practical sharpness-aware minimization cannot converge all the way to optima},
  author={Si, Dongkuk and Yun, Chulhee},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{huh2024platonic,
  title={The platonic representation hypothesis},
  author={Huh, Minyoung and Cheung, Brian and Wang, Tongzhou and Isola, Phillip},
  journal={arXiv preprint arXiv:2405.07987},
  year={2024}
}

@article{springer2024sharpness,
  title={Sharpness-Aware Minimization Enhances Feature Quality via Balanced Learning},
  author={Springer, Jacob Mitchell and Nagarajan, Vaishnavh and Raghunathan, Aditi},
  journal={arXiv preprint arXiv:2405.20439},
  year={2024}
}

@article{du2022sharpness,
  title={Sharpness-aware training for free},
  author={Du, Jiawei and Zhou, Daquan and Feng, Jiashi and Tan, Vincent and Zhou, Joey Tianyi},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={23439--23451},
  year={2022}
}

@inproceedings{mulayoff2020unique,
  title={Unique properties of flat minima in deep networks},
  author={Mulayoff, Rotem and Michaeli, Tomer},
  booktitle={International conference on machine learning},
  pages={7108--7118},
  year={2020},
  organization={PMLR}
}

@inproceedings{compagnoni2023sde,
  title={An sde for modeling sam: Theory and insights},
  author={Compagnoni, Enea Monzio and Biggio, Luca and Orvieto, Antonio and Proske, Frank Norbert and Kersting, Hans and Lucchi, Aurelien},
  booktitle={International Conference on Machine Learning},
  pages={25209--25253},
  year={2023},
  organization={PMLR}
}

@article{chen2024does,
  title={Why does sharpness-aware minimization generalize better than SGD?},
  author={Chen, Zixiang and Zhang, Junkai and Kou, Yiwen and Chen, Xiangning and Hsieh, Cho-Jui and Gu, Quanquan},
  journal={Advances in neural information processing systems},
  volume={36},
  year={2024}
}

@article{chen2021vision,
  title={When vision transformers outperform resnets without pre-training or strong data augmentations},
  author={Chen, Xiangning and Hsieh, Cho-Jui and Gong, Boqing},
  journal={arXiv preprint arXiv:2106.01548},
  year={2021}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@article{zhang2021understanding,
  title={Understanding deep learning (still) requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={Communications of the ACM},
  volume={64},
  number={3},
  pages={107--115},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{izmailov2018averaging,
  title={Averaging weights leads to wider optima and better generalization},
  author={Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},
  journal={arXiv preprint arXiv:1803.05407},
  year={2018}
}
