[
  {
    "index": 0,
    "papers": [
      {
        "key": "dinh2017sharp",
        "author": "Dinh, Laurent and Pascanu, Razvan and Bengio, Samy and Bengio, Yoshua",
        "title": "Sharp minima can generalize for deep nets"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "kwon2021asam",
        "author": "Kwon, Jungmin and Kim, Jeongseop and Park, Hyunseo and Choi, In Kwon",
        "title": "Asam: Adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "kim2022fisher",
        "author": "Kim, Minyoung and Li, Da and Hu, Shell X and Hospedales, Timothy",
        "title": "Fisher sam: Information geometry and sharpness aware minimisation"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "yun2024riemannian",
        "author": "Yun, Jihun and Yang, Eunho",
        "title": "Riemannian SAM: Sharpness-Aware Minimization on Riemannian Manifolds"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "foret2020sharpness",
        "author": "Foret, Pierre and Kleiner, Ariel and Mobahi, Hossein and Neyshabur, Behnam",
        "title": "Sharpness-aware minimization for efficiently improving generalization"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "andriushchenko2022towards",
        "author": "Andriushchenko, Maksym and Flammarion, Nicolas",
        "title": "Towards understanding sharpness-aware minimization"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "behdin2023sharpness",
        "author": "Behdin, Kayhan and Mazumder, Rahul",
        "title": "Sharpness-aware minimization: An implicit regularization perspective"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "izmailov2018averaging",
        "author": "Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon",
        "title": "Averaging weights leads to wider optima and better generalization"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "kaddour2022flat",
        "author": "Kaddour, Jean and Liu, Linqing and Silva, Ricardo and Kusner, Matt J",
        "title": "When do flat minima optimizers work?"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "ahn2023escape",
        "author": "Ahn, Kwangjun and Jadbabaie, Ali and Sra, Suvrit",
        "title": "How to escape sharp minima"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "keskar2016large",
        "author": "Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter",
        "title": "On large-batch training for deep learning: Generalization gap and sharp minima"
      },
      {
        "key": "jastrzkebski2017three",
        "author": "Jastrzebski, Stanis{\\l}aw and Kenton, Zachary and Arpit, Devansh and Ballas, Nicolas and Fischer, Asja and Bengio, Yoshua and Storkey, Amos",
        "title": "Three factors influencing minima in sgd"
      },
      {
        "key": "xing2018walk",
        "author": "Xing, Chen and Arpit, Devansh and Tsirigotis, Christos and Bengio, Yoshua",
        "title": "A walk with sgd"
      },
      {
        "key": "zhu2018anisotropic",
        "author": "Zhu, Zhanxing and Wu, Jingfeng and Yu, Bing and Wu, Lei and Ma, Jinwen",
        "title": "The anisotropic noise in stochastic gradient descent: Its behavior of escaping from sharp minima and regularization effects"
      },
      {
        "key": "smith2020generalization",
        "author": "Smith, Samuel and Elsen, Erich and De, Soham",
        "title": "On the generalization benefit of noise in stochastic gradient descent"
      },
      {
        "key": "zhang2021understanding",
        "author": "Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol",
        "title": "Understanding deep learning (still) requires rethinking generalization"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "hartmann2022lagrangian",
        "author": "Hartmann, Marcelo and Girolami, Mark and Klami, Arto",
        "title": "Lagrangian manifold monte carlo on monge patches"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "hartmann2022lagrangian",
        "author": "Hartmann, Marcelo and Girolami, Mark and Klami, Arto",
        "title": "Lagrangian manifold monte carlo on monge patches"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "bergamin2024riemannian",
        "author": "Bergamin, Federico and Moreno-Mu{\\~n}oz, Pablo and Hauberg, S{\\o}ren and Arvanitidis, Georgios",
        "title": "Riemannian Laplace approximations for Bayesian neural networks"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "yu2023riemannian",
        "author": "Yu, Hanlin and Hartmann, Marcelo and Williams, Bernardo and Girolami, Mark and Klami, Arto",
        "title": "Riemannian Laplace Approximation with the Fisher Metric"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "yu2023riemannian",
        "author": "Yu, Hanlin and Hartmann, Marcelo and Williams, Bernardo and Girolami, Mark and Klami, Arto",
        "title": "Riemannian Laplace Approximation with the Fisher Metric"
      }
    ]
  }
]