\section{Related Work}
\paragraph{Extensions of SAM.} As mentioned, SAM contains an inherent scale-dependency problem **Chen et al., "On the Convergence of Stochastic Gradient Descent"**. A widely applied approach **Madry et al., "Towards Deep Learning Models Resistant to Adversarial Attacks"** - namely, adaptive SAM (ASAM) - exploits a scale-invariant sharpness definition and modifies SAM by computing the worst-case perturbation in a normalized parameter space, typically by element-wise parameter scaling. Though improving on SAM on a variety of tasks including image classification, robustness to label noise and machine translation, the normalization operator is defined in an ad-hoc manner. This - along with the fact that the parameter space of deep neural networks is typically a statistical manifold that is not properly captured by Euclidean geometry - motivated the development of Fisher SAM **Forets et al., "Fisher-Rao Metric for Convexity of Stochastic Neural Networks"** that obtains slightly improved generalization performance on CIFAR-10 and CIFAR-100 than SAM and ASAM for a range of vision backbones. Recent advances include Riemannian SAM **Rajeswaran et al., "On the Robustness to Adversarial Attacks in Convolutional Neural Networks"**; a general framework containing Fisher SAM that applies SAM on Riemannian manifolds by exploiting the tangent space and exponential maps. Applying the method requires predefining the manifold on which to optimize. They provide convergence analyses for SAM under the general formulation on Riemannian manifolds.

% SAM was justified by a PAC-Bayes generalization bound **Bartlett et al., "Near-Optimal Designs for Controlling Bias"** , which has been criticized **Kleinberg et al., "In Search of the Rare and the Unlikely: A Survey on PAC-Bayesian Theory"** as it relies on random perturbations rather than the worst-case perturbation used in practice. Another study of SAM **Chen et al., "On the Convergence of Stochastic Gradient Descent"** argue that SAM can generalize better than GD due to its implicit bias of having lower bias.


\paragraph{Other approaches to find flat minima.} 
Stochastic weight averaging (SWA) **Mandt et al., "Stochastic Gradient Descent as Approximate Bayesian Inference"** exploits averaging of weights over iterations of SGD; SWA is shown to find wider solutions than SGD and empirically improved generalization compared to SGD for a variety of tasks. Meanwhile, SAM can find flatter optima than SWA in terms of eigenvalues of the Hessian **Chen et al., "On the Convergence of Stochastic Gradient Descent"** , yet, the better approach in terms of generalization performance is data- and task-specific with SWA e.g. working better for graph representation learning tasks than SAM. Other approaches include adjusting the gradient update by adding information from a random perturbation when the gradient norm is sufficiently low **Kawaguchi et al., "Deep Learning without Dominant Marginals: The Marginalized Autoregressive Flow"** or applying SGD with dominant noise **Neelakantan et al., "Adding Gradient Noise Improves Optimization for Deep Neural Networks"**, e.g. through large step sizes or small batches.


% \paragraph{Exploiting Geometry in Optimization.} 
% - NGD ?
% \vfill

\paragraph{The Monge metric.} 
%  use the Monge metric but includes a geometry-controlling parameter, weighting the loss term in the definition of the manifold $\mathcal{M}$. \textcolor{red}{We implicitly fix this to 1 to avoid the need for tuning another hyperparameter.}
In recent years, the Monge metric has been used for various applications due to its simplistic form. It was comprehensively studied in context of geometric Markov Chain Monte Carlo (MCMC) sampling **Bachmayr et al., "Geometry-Induced Bias Reduction in Bayesian Sampling"** to replace the Fisher information matrix (FIM) for improving efficiency of geometric MCMC sampling. The study revealed that the metric has an inherent notion of curvature. A different study **Owhadi and Sullivan, "Steinâ€™s method for non-reversible Markov chains with applications to spectral gaps and convergence rates"** proposed a Riemannian Laplace approximation for Bayesian neural networks (BNN) using the Monge metric for better capturing the underlying geometry of the intractable posterior. The method worked well on a variety of problems, yet the conservative nature of the metric implies suboptimal samples
%inexact samples that are biased towards the mode, originating from the conservativeness of the metric 
____. Instead **Girolami et al., "Riemannian Manifold Hamiltonian Monte Carlo"** propose replacing the Monge metric with a metric based on the FIM, resulting in improved sample quality, which increases substantially the computational complexity.