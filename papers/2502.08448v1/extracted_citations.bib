@article{ahn2023escape,
  title={How to escape sharp minima},
  author={Ahn, Kwangjun and Jadbabaie, Ali and Sra, Suvrit},
  journal={arXiv preprint arXiv:2305.15659},
  year={2023}
}

@inproceedings{andriushchenko2022towards,
  title={Towards understanding sharpness-aware minimization},
  author={Andriushchenko, Maksym and Flammarion, Nicolas},
  booktitle={International Conference on Machine Learning},
  pages={639--668},
  year={2022},
  organization={PMLR}
}

@article{behdin2023sharpness,
  title={Sharpness-aware minimization: An implicit regularization perspective},
  author={Behdin, Kayhan and Mazumder, Rahul},
  journal={stat},
  volume={1050},
  pages={23},
  year={2023}
}

@article{bergamin2024riemannian,
  title={Riemannian Laplace approximations for Bayesian neural networks},
  author={Bergamin, Federico and Moreno-Mu{\~n}oz, Pablo and Hauberg, S{\o}ren and Arvanitidis, Georgios},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{dinh2017sharp,
  title={Sharp minima can generalize for deep nets},
  author={Dinh, Laurent and Pascanu, Razvan and Bengio, Samy and Bengio, Yoshua},
  booktitle={International Conference on Machine Learning},
  pages={1019--1028},
  year={2017},
  organization={PMLR}
}

@article{foret2020sharpness,
  title={Sharpness-aware minimization for efficiently improving generalization},
  author={Foret, Pierre and Kleiner, Ariel and Mobahi, Hossein and Neyshabur, Behnam},
  journal={arXiv preprint arXiv:2010.01412},
  year={2020}
}

@inproceedings{hartmann2022lagrangian,
  title={Lagrangian manifold monte carlo on monge patches},
  author={Hartmann, Marcelo and Girolami, Mark and Klami, Arto},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={4764--4781},
  year={2022},
  organization={PMLR}
}

@article{izmailov2018averaging,
  title={Averaging weights leads to wider optima and better generalization},
  author={Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},
  journal={arXiv preprint arXiv:1803.05407},
  year={2018}
}

@article{jastrzkebski2017three,
  title={Three factors influencing minima in sgd},
  author={Jastrzebski, Stanis{\l}aw and Kenton, Zachary and Arpit, Devansh and Ballas, Nicolas and Fischer, Asja and Bengio, Yoshua and Storkey, Amos},
  journal={arXiv preprint arXiv:1711.04623},
  year={2017}
}

@article{kaddour2022flat,
  title={When do flat minima optimizers work?},
  author={Kaddour, Jean and Liu, Linqing and Silva, Ricardo and Kusner, Matt J},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={16577--16595},
  year={2022}
}

@article{keskar2016large,
  title={On large-batch training for deep learning: Generalization gap and sharp minima},
  author={Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  journal={arXiv preprint arXiv:1609.04836},
  year={2016}
}

@inproceedings{kim2022fisher,
  title={Fisher sam: Information geometry and sharpness aware minimisation},
  author={Kim, Minyoung and Li, Da and Hu, Shell X and Hospedales, Timothy},
  booktitle={International Conference on Machine Learning},
  pages={11148--11161},
  year={2022},
  organization={PMLR}
}

@inproceedings{kwon2021asam,
  title={Asam: Adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks},
  author={Kwon, Jungmin and Kim, Jeongseop and Park, Hyunseo and Choi, In Kwon},
  booktitle={International Conference on Machine Learning},
  pages={5905--5914},
  year={2021},
  organization={PMLR}
}

@inproceedings{smith2020generalization,
  title={On the generalization benefit of noise in stochastic gradient descent},
  author={Smith, Samuel and Elsen, Erich and De, Soham},
  booktitle={International Conference on Machine Learning},
  pages={9058--9067},
  year={2020},
  organization={PMLR}
}

@article{xing2018walk,
  title={A walk with sgd},
  author={Xing, Chen and Arpit, Devansh and Tsirigotis, Christos and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1802.08770},
  year={2018}
}

@article{yu2023riemannian,
  title={Riemannian Laplace Approximation with the Fisher Metric},
  author={Yu, Hanlin and Hartmann, Marcelo and Williams, Bernardo and Girolami, Mark and Klami, Arto},
  journal={arXiv preprint arXiv:2311.02766},
  year={2023}
}

@article{yun2024riemannian,
  title={Riemannian SAM: Sharpness-Aware Minimization on Riemannian Manifolds},
  author={Yun, Jihun and Yang, Eunho},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{zhang2021understanding,
  title={Understanding deep learning (still) requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={Communications of the ACM},
  volume={64},
  number={3},
  pages={107--115},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{zhu2018anisotropic,
  title={The anisotropic noise in stochastic gradient descent: Its behavior of escaping from sharp minima and regularization effects},
  author={Zhu, Zhanxing and Wu, Jingfeng and Yu, Bing and Wu, Lei and Ma, Jinwen},
  journal={arXiv preprint arXiv:1803.00195},
  year={2018}
}

