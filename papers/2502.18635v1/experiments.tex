\section{Experiments}
\label{sec:experiments}

We tested our optimization approaches on two RAG tasks from different industries.  We use a standard RAG setup with retrieval using vector embeddings, a reranker to filter out unnecessary context chunks, and an LLM prompted to generate a response using the context. The exact configuration solution space we use is given in Appendix~\ref{appendix:experiments}.

We optimize for the four objectives of cost, latency, safety and alignment. We run BO using the train question set, and then report results on a held out test set. We use \emph{Ax} \citep{bakshy2018ae} and \emph{BoTorch} \citep{balandat2020botorch} to run and manage experiments. For all algorithms we use 50 total iterations, and for BO methods, the first 20 iterations are chosen using Sobol sampling. In MOO, a reference point is used to calculate the HV improvement, and represents the minimum acceptable solution. Based on our industry experience, we use a reference point with cost of \$2000 per million queries, latency of 20s per query, safety of 50 and alignment of 50. This choice of reference point prevents degenerate solutions (\eg a degenerate RAG system which does not retrieve any chunks) from contributing to the HV improvement.

\textit{Throughout this work, we report the cost in USD per million queries (\$ / million queries) and latency in seconds (s). Safety and alignment scores are dimensionless and range from 0 to 100. Cost and latency are objectives to minimize, while safety and alignment are objectives to be maximized.}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/hypervolume-improvement.png}
    \caption{HV improvement on train and test splits for both datasets. Our proposed acquisition function for BO (\texttt{qLogNEHVI}) outperforms its noiseless variant (\texttt{qLogEHVI}) and both BO algorithms perform significantly better than the baselines. There is a noticeable increase in HV after iteration 20 (dotted line), indicating the end of Sobol sampling initializations for the BO algorithms, and the start of acquisition function-guided selections.}
    \label{fig:hvi}
\end{figure}

\subsection{Datasets}
\label{sec:datasets}
Existing datasets for Q\&A tasks generally exist in the form of (question, context, answer) triplets. However, in industry RAG use-cases, the context is often retrieved at runtime from a set of documents. As such, we want to include the retrieval of the context as part of of our evaluation. To this end, we adapt two known tasks to fit the needs of our experimental setup\footnote{We publicly release the train and test questions for these documents at \url{https://huggingface.co/datasets/Trustwise/optimization-benchmark-dataset}}:
\paragraph{FinancialQA.}
This task uses a publicly available document covering revenue recognition from a leading global accounting firm.\footnote{Financial document available at: \url{https://kpmg.com/kpmg-us/content/dam/kpmg/frv/pdf/2024/handbook-revenue-recognition-1224.pdf}} The document includes more than 1000 pages of text, representing a significant Q\&A context retrieval challenge. Since the document does not come with a set of questions, we synthetically generate 50 questions by prompting GPT-4o, using the method described in Section~\ref{sec:synthetic_data}. We then randomly split this set into 30 train and 20 test questions.
\paragraph{MedicalQA.}
We create our medical dataset using the existing FACTS benchmark \citep{jacovi2025facts} which includes (question, context, answer) triplets.\footnote{FACTS dataset available at: \url{https://www.kaggle.com/facts-leaderboard}} We take the medical Q\&A subset, and the authors manually filter out unrepresentative questions. We then combine all the individual context chunks from each question in the medical Q\&A split into one large document. The final dataset includes one large medical document, and independent sets of 43 train and 43 test questions.

\subsection{Baselines}
For our tasks, we lack access to a ``ground truth'' set of Pareto optimal configurations because we cannot evaluate the objective functions directly, and grid search is computationally unfeasible. Hence we use three baseline approaches that are applicable in MOO settings. The first is \emph{uniform sampling}, which generates configurations independently, where each configuration from the solution space is equally probable. %This represents a lower bound on the minimum of what an optimization algorithm should achieve.
The second is \emph{Sobol sampling.} Like latent hypercube sampling~\citep{loh1996latin},  this method generates configurations that guarantee good high-dimensional uniformity. It is commonly used to initialize optimization algorithms, including BO, and represents a sensible alternative to grid-search. Finally, we use \texttt{qLogEHVI} BO, the noiseless variant of our chosen acquisition function \citep{daulton2020differentiable}.

\section{Results}
\label{sec:results}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/financialqa-pareto.png}
    \includegraphics[width=\linewidth]{figures/medicalqa-pareto.png}
    \caption{2D projections of the 4D Pareto frontier for each algorithm for a fixed random seed on both datasets. We see our proposed algorithm (\texttt{qLogNEHVI} BayesOpt) obtains a superior Pareto front, with solutions concentrated towards high safety, high alignment, low cost, and low latency.}
    \label{fig:pareto-front}
\end{figure}
\begin{figure}[t]
\centering
   \includegraphics[width=0.4\linewidth]{figures/financialqa-radar.png}
   \includegraphics[width=0.4\linewidth]{figures/medicalqa-radar.png}
    \caption{Radar charts comparing the four objective function evaluations for iterations chosen to optimize each objective. We see that improved safety can be achieved at the expense of increased cost and latency. \textbf{N.B. Lower is better for cost and latency but higher is better for safety and alignment.}}
    \label{fig:radar}
\end{figure}


We report the HV improvement on the train and test splits of both datasets, across five random seeds in Figure \ref{fig:hvi}. We find that BO methods significantly outperform other baseline approaches on both tasks, and that \texttt{qLogNEHVI} outperforms its noiseless variant. 
%We find that BO methods significantly outperform other baseline approaches on both tasks. Our preferred BO acquisition function, \texttt{qLogNEHVI} outperforms its noiseless variant.
%\jamiecomment{This sentence feels slightly clunky. Perhaps it could be combined with the previous one - ``We find that BO methods significantly outperform other baselines approaches on both tasks, and that \texttt{qLogNEHVI} outperforms its noiseless variant.''}
Both BO methods show significant improvement compared to baselines after iteration 20, when the BO acquisition function is used to select inputs rather than Sobol sampling. %We also note that there are relatively large error bars due to variances in API latencies and LLM responses.

Figure \ref{fig:pareto-front} shows the Pareto front for a fixed seed for the FinancialQA and MedicalQA datasets. Since the overall Pareto frontier lies in $\mathbb{R}^4$ (as there are 4 objectives), we project onto three $\mathbb{R}^2$ plots for visualization purposes. We find that \texttt{qLogNEHVI} BO obtains a superior Pareto front compared to the baselines, also finding a wider spread of solutions. In particular, we notice significant clustering around sub-optimal solutions when using \texttt{qLogEHVI} BO, as observed by \cite{daulton2021parallel}.

Figure \ref{fig:radar} depicts the objective function evaluations across different configurations optimized for each objective. We see that safety and alignment optimized configurations come at the expense of cost and latency, and vice versa. We find that significant cost and latency reduction can be achieved at the cost of minimal safety and alignment reduction. All configuration settings and objective evaluations are detailed in Table \ref{tab:configurations}, where we observe similar chosen parameters across both datasets, especially the choice of LLM and embedding model. From these examples and others, we observe general patterns that help to optimize each objective. Safety optimized configurations often use the large embedding model and a large chunk size. In contrast, latency optimized configurations use the small embedding model and a small chunk size. This is intuitive: high safety requires sufficient high-quality context tokens, whereas low latency necessitates fewer context tokens. 