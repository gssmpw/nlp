\section{Related work}
\label{sec:related}
There has been a mixture of work separately addressing multi-objective optimization and hyperparameter optimization in LLM and RAG systems, which we summarize here. We provide further comparison with fine-tuning and model-merging approaches in Appendix ~\ref{appendix:related}.

\paragraph{Hyperparameter optimization (HO).} As many pieces of LLM and RAG pipelines have hyperparameters that must be tuned before deployment, there is a large body of work testing the efficacy of using HO to tune these systems. %As mentioned earlier, HO in these settings is particularly difficult due to the high number of parameters; evaluation during each iteration of the HO process can be very expensive and time-intensive, and evaluations are often noisy.

\citet{wang2023cost} propose a cost-based pruning strategy to hyperparameter tune LLM systems under budget constraints. They focus on hyperparameters like the type of model (\eg text-davinci-003, gpt-3.5-turbo, or gpt-4), the maximum number of tokens that can be generated in a response, the model temperature\footnote{A parameter for which low or high values sharpen or soften the probability distribution of a token being outputted by an LLM, respectively.}, and the model top-$p$.\footnote{A parameter that restricts the domain of tokens that can be outputted by an LLM to those whose cumulative probability is greater than $p$.} They use a search method called BlendSearch~\citep{wang2021economic}, which combines Bayesian optimization and local search~\citep{wu2021frugal}, to find the optimal combinations of parameters, and measure performance on the tasks APPS~\citep{hendrycks2021measuring}, XSum~\citep{narayan2018don}, MATH~\citep{hendrycks2021measuring}, and HumanEval~\cite{chen2021evaluating}. In comparison with our work, \citet{wang2023cost} do not consider a RAG system.

Most related to our work, \citet{kim2024autorag} proposed AutoRAG, an open-source framework designed for RAG experimentation and hyperparameter optimization. They use a a greedy algorithm for selecting the hyperparameters governing RAG modules like query expansion, retrieval, passage augmentation, passage re-ranking, prompt making, and generating (the LLM). In concurrent work, \citet{fu2024autorag} proposed AutoRAG-HP, which frames hyperparameter selection as an online multi-armed bandit (MAB) problem. To carry out HO, they introduce a novel two-level Hierarchical MAB (Hier-MAB) method, where a high-level MAB guides the optimization of modules, and several low-level MABs search for optimal settings within each module. Significantly, our work is distinct from both \citet{kim2024autorag} and \citet{fu2024autorag} in that they do not consider multi-objective settings.

\paragraph{Multi-objective alignment.} Several researchers have proposed methods for incorporating multiple objectives directly into the LLM fine-tuning and alignment processes. ~\citet{li2020deep} developed an approach for multi-objective alignment from human feedback using scalar linearization. ~\cite{mukherjee2024multi} expanded on that approach by developing an algorithm that finds a diverse set of Pareto-optimal solutions that maximize the hypervolume, given a set of objectives.~\cite{zhou2024beyond} proposed a reward-function free extension called Multi-Objective Direct Preference Optimization (MODPO). The latter showed that MODPO can effectively find a Pareto-optimal frontier of fine-tuned models, trading off objectives like ``helpfulness'' and ``harmlessness''. While these works have demonstrated success in multi-objective LLM alignment, we focus on RAG pipelines and avoid aligning and fine-tuning models altogether. 
\input{tables/system_params}