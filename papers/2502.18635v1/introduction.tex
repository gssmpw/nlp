\section{Introduction}
Retrieval Augmented Generation (RAG) has emerged as a popular technique for improving the performance of Large Language Models (LLMs) on question-answering tasks over specific datasets. A benefit of using RAG pipelines is that they can often achieve high performance on specific tasks \emph{without} the need for extensive alignment and fine-tuning~\citep{gupta2024rag}, a costly and time-consuming process. However, the end-to-end pipeline of a RAG system is dependent on many parameters that span different components (or modules) of the system, such as the choice of LLM, the embedding model used in retrieval, the number of chunks retrieved and hyperparameters governing a reranking model. Examples of choices, parameters, and hyperparameters that are often made or tuned when implementing a RAG pipeline are listed in Table~\ref{tab:system_params}. Importantly, the performance of a RAG pipeline is dependent on these choices~\citep{fu2024autorag}, many of which can be difficult to tune manually. While those building RAG pipelines might avoid fine-tuning costs, they often spend time and resources on hyperparameter optimization (HO).%---in other words, there are no free lunches.

Despite this, there is little research exploring methods for collectively optimizing all the hyperparameters in a given LLM and RAG pipeline~\citep{fu2024autorag}. Further, to the best of our knowledge, there is no work that addresses this challenge in \emph{multi-objective settings}, where the RAG pipeline must achieve high performance across a range of objectives,
%\jamiecomment{``where the RAG pipeline must achieve high performance across a range of objectives,''?}
like 
%\jamiecomment{``for example''?}
minimizing a system's inference time while maximizing its helpfulness. In this work, we aim to fill this gap by introducing an approach for collectively optimizing the hyperparameters of a RAG system in a multi-objective setting.

The authors of this paper are from a mixture of both academia and industry, and this work was motivated by real-world challenges faced by industry practitioners. Use of RAG pipelines within industry often requires balancing multiple requirements which are in competition with one another. For example, at one financial services firm developing an in-house Q\&A chatbot to support internal workflows, practitioners aimed to both maximize accuracy and minimize the time taken to generate a response. However, there is a tension in these two objectives: a RAG system utilizing larger models may yield more accurate responses, but consequently requires longer computation time. As another example, a large bank developing an external insurance policy Q\&A chatbot was primarily concerned with the alignment of generated responses to policies and regulations. Objectives that we have observed practitioners frequently consider when building RAG pipelines include: cost, response latency, safety (hallucination risk), and alignment (response helpfulness).

Multi-objective HO over a RAG pipeline is particularly challenging for several reasons. First, RAG pipelines naturally have a high number of parameters, leading to a large solution space. We identify at least 15 example choices, parameters, and hyperparameters in Table~\ref{tab:system_params}. Even if one has just a handful of possible values for each choice, the parameter space becomes intractably large for simple algorithms like grid search.
\footnote{\url{https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html}} Second, evaluating a RAG pipeline during the HO process is costly, with respect to both compute resources and time: it requires running the RAG system over multiple queries (where each iteration is bounded by the per-token inference time of the LLM), and then evaluating each output. Third, the objective evaluations can be noisy. The true characteristic of a RAG system cannot be computed directly, and requires sampling the evaluations from many queries. Relatedly, since in most cases LLMs are non-deterministic, combinations of hyperparameters need to be tested over multiple seeds.

\begin{figure}[t]
    \centering  \input{figures/figure1}
    \caption{A high-level overview of our approach. First, we source the datasets that we will use to optimize our RAG pipeline, define the choices, parameters and hyperparameters that will be optimized over (see Table~\ref{tab:system_params}), and select the objectives for optimization (\eg cost, latency, safety, and alignment). 
    Second, we introduce a train-test paradigm for evaluating the performance of RAG pipelines, and use Bayesian optimization (BO) to find the optimal parameter configurations. We find that using BO with the \texttt{qLogNEHVI}~\citep{daulton2021parallel, ament2023unexpected} acquisition function is well-suited for this problem, since it is adapted for noisy objective evaluations and makes use of a single composite objective called \emph{hypervolume improvement} that allows for an arbitrary number of objectives. Third, we explore the Pareto frontier of parameter configurations, finding the best solutions over different objectives.}
    \label{fig:enter-label}
\end{figure}

\paragraph{Contributions.} In this work, we make four main contributions:

\begin{enumerate}
    \item[(1)] We introduce an approach for multi-objective optimization over a unique set of hyperparameters of a RAG pipeline, including choices for the LLM and embedding models themselves. Our approach implements a single composite objective value called the hypervolume improvement~\citep{guerreiro2021hypervolume}, and uses Bayesian optimization with an acquisition function that allows for an arbitrary number of noisy objective functions (\texttt{qLogNEHVI}) ~\citep{daulton2021parallel, ament2023unexpected} to find the best RAG pipeline configuration.
    
    \item[(2)] We empirically show the effectiveness of our approach to identify optimal RAG pipeline configurations across two tasks (one related to financial services Q\&A, and another related to medical Q\&A) using a train-test paradigm, as compared to random parameter choices and other baseline optimization approaches.

    \item[(3)] We publicly release two novel benchmarks for evaluating RAG systems called \texttt{FinancialQA} and \texttt{MedicalQA}.\footnote{\url{https://huggingface.co/datasets/Trustwise/optimization-benchmark-dataset}} Importantly, these benchmarks more closely mimic industry RAG use-cases than currently available benchmarks, since the context must be retrieved at runtime from an available document, rather than being given in the dataset.
    
    \item[(4)] We frame our discussion (Section~\ref{sec:discussion}) as guidance to practitioners who seek to improve their own RAG systems. We highlight two important considerations: the first is what we call ``task dependence'', meaning that an optimal configuration for a RAG pipeline on a task in a specific setting may not generalize to another setting. The second is ``objective dependence'', where objective evaluations follow different trends (or have no trend) across different configurations. Task and objective dependence can also compound, highlighting the challenge of collectively optimizing the parameters of a RAG system.
\end{enumerate}
