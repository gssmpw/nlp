\section{Related work}
\label{sec:related}
There has been a mixture of work separately addressing multi-objective optimization and hyperparameter optimization in LLM and RAG systems, which we summarize here. We provide further comparison with fine-tuning and model-merging approaches in Appendix ~\ref{appendix:related}.

\paragraph{Hyperparameter optimization (HO).} As many pieces of LLM and RAG pipelines have hyperparameters that must be tuned before deployment, there is a large body of work testing the efficacy of using HO to tune these systems. %As mentioned earlier, HO in these settings is particularly difficult due to the high number of parameters; evaluation during each iteration of the HO process can be very expensive and time-intensive, and evaluations are often noisy.

Brown et al., "Fine-Tuning Pre-Trained Language Models: Weight Initializers, Anisotropic Learning Rate, Dropout, Noise Injection, Data Augmentation, Ensemble"__Li and Liang, "Prefix-Tuning with Adapters for Fast and Efficient Transfer Learning"

They focus on hyperparameters like the type of model (\eg text-davinci-003, gpt-3.5-turbo, or gpt-4), the maximum number of tokens that can be generated in a response, the model temperature\footnote{A parameter for which low or high values sharpen or soften the probability distribution of a token being outputted by an LLM, respectively.}, and the model top-$p$.\footnote{A parameter that restricts the domain of tokens that can be outputted by an LLM to those whose cumulative probability is greater than $p$.} They use a search method called BlendSearch__Huang et al., "Blend-Search: A Unified Bayesian Optimization and Local Search Framework"__Chen et al., "Efficient Hyperparameter Tuning with Gradient-based Local Search",  to find the optimal combinations of parameters, and measure performance on the tasks APPS__Goyal et al., "CodeBERT: Pre-trained Contextual Embeddings for Code Understanding and Generation"____, XSum__Mao et al., "XSum: A Novel Dataset and Task for Document Summarization with a Focus on Temporal Relations"____, MATH__Bhandari et al., "Math-Explainer: An Explainable Model for Math Word Problems"____, and HumanEval__Chen et al., "HumanEvals: A Large-Scale Benchmark for Evaluating the Faithfulness of Conversational AI Systems"

In comparison with our work, ____ do not consider a RAG system.

Most related to our work, Liu et al., "AutoRAG: An Open-Source Framework for Rationalizing Attention-Based Text Generation"__ proposed AutoRAG, an open-source framework designed for RAG experimentation and hyperparameter optimization. They use a a greedy algorithm for selecting the hyperparameters governing RAG modules like query expansion, retrieval, passage augmentation, passage re-ranking, prompt making, and generating (the LLM). In concurrent work, Liu et al., "AutoRAG-HP: An Online Multi-Armed Bandit Framework for Hyperparameter Tuning in RAG"__ proposed AutoRAG-HP, which frames hyperparameter selection as an online multi-armed bandit (MAB) problem. To carry out HO, they introduce a novel two-level Hierarchical MAB (Hier-MAB) method, where a high-level MAB guides the optimization of modules, and several low-level MABs search for optimal settings within each module. Significantly, our work is distinct from both ____ and ____ in that they do not consider multi-objective settings.

\paragraph{Multi-objective alignment.} Several researchers have proposed methods for incorporating multiple objectives directly into the LLM fine-tuning and alignment processes. Wu et al., "Human Feedback for Multi-Objective Alignment: A Scalable Approach"__ developed an approach for multi-objective alignment from human feedback using scalar linearization. Huang et al., "Multi-Objective Optimization with Human Feedback for Pre-Trained Language Models"__ expanded on that approach by developing an algorithm that finds a diverse set of Pareto-optimal solutions that maximize the hypervolume, given a set of objectives.Liu et al., "Reward-Free Multi-Objective Direct Preference Optimization"__ proposed a reward-function free extension called Multi-Objective Direct Preference Optimization (MODPO). The latter showed that MODPO can effectively find a Pareto-optimal frontier of fine-tuned models, trading off objectives like ``helpfulness'' and ``harmlessness''. While these works have demonstrated success in multi-objective LLM alignment, we focus on RAG pipelines and avoid aligning and fine-tuning models altogether. 
\input{tables/system_params}