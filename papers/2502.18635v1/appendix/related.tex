\section{Related Work on Fine-tuning and Model Merging}
\label{appendix:related}

While we do not explore fine-tuning or model-merging in our work, several authors have studied the effectiveness of HO to improve the alignment process. 

\paragraph{Fine-tuning.}\citet{wu2024beta} explored the importance of hyperparameter tuning the $\beta$-parameter\footnote{$\beta$ governs the extent to which the policy model's behavior can diverge from the original model.} for Direct Preference Optimization (DPO)~\citep{rafailov2024direct}, a popular framework for human-preference tuning LLMs. Significantly, they uncovered settings in which increasing $\beta$ improves DPO performance, and others where increasing $\beta$ has the exact opposite effect and decreases performance. %To this end, they propose a dynamic approach for setting $\beta$, called $\beta$-DPO. %\matthew{not sure if we want to introduce LLM finetuning as part of this paper - that adds a significant level of complexity.}
\citet{wang2024llm} introduce an approach called Hyperparameter Aware Generation
(HAG), that allows LLMs to ``self-regulate'' hyperparameters like temperature, top-$p$, top-$k$, and repetition penalty during inference. They observed that different configurations of these hyperparameters lead to different performances on tasks like reasoning, creativity, translation, and math. %Their approach involves learning the relationship between hyperparameter settings and task performance.
\citet{tribes2023hyperparameter} used hyperparameter optimization (HO) to improve the instruction fine-tuning process, adjusting the hyperparameters rank and scaling $\alpha$ for Low-Rank Adaptation (LoRA)~\citep{hu2021lora}\footnote{Low-Rank Adaptation (LoRA) is a method that reduces the number of trainable parameters for a fixed model for downstream tasks like fine-tuning.}, as well as the model dropout rate and learning rate. In their experiments, they fine-tuned a Llama 2 7B parameter model\footnote{\url{https://huggingface.co/meta-llama/Llama-2-7b}}, and found that HO-fine-tuning resulted in better performance on tasks like MMLU~\citep{hendrycks2020measuring}, BBH~\citep{suzgun2022challenging}, DROP~\citep{dua2019drop}, and HumanEval~\citep{chen2021evaluating}, as compared to vanilla fine-tuning. Methodologically, they tested two HO approaches: Tree-structured Parzen Estimator tuning\footnote{TPE is a Bayesian optimization~\citep{bergstra2011algorithms} algorithm that uses a probabilistic model for HO. It is a Sequential Model-Based Optimization (SMBO) method~\citep{hutter2010sequential}.}~\citep{bergstra2011algorithms} and Mesh Adaptive Direct Search~\citep{audet2006mesh}, and found better performance with the latter. Overall, their results confirm the necessity of careful HO in instruction-tuning.

\paragraph{Model merging.} \citet{li2024s} frame LLM model merging, or combining different ``source'' (or base) models to create a unified model that retains the strengths of each model, as a multi-objective optimization (MOO) problem. They use parallel multi-objective Bayesian optimization (qEHVI)~\citep{daulton2020differentiable} to search over a range of model merging techniques like Model Soup~\citep{wortsman2022model} and TIES-Merging~\citep{yadav2024ties} (and the associated hyperparameters of those techniques), and evaluate the performance of the merged model on benchmarks like MMLU and Big-Bench Hard~\citep{suzgun2022challenging}. Our work is distinct from approaches using model merging in that we search over choices for LLMs rather than combine them. However, similar to ~\citet{li2024s}, we test the effectiveness of the qEHVI and qNEHVI~\citep{daulton2021parallel} (a variation allowing for noisy objectives) for HO.