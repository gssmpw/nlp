\section{Experimental details}
\label{appendix:experiments}

We use the following search space for hyperparameters:
\begin{itemize}
    \item $c_s \in \mathbb{Z}^+$: Maximum number of tokens in each document chunk.
    \item $c_n \in \mathbb{Z}^+$: Number of chunks retrieved from the vector database for each query.
    \item $o \in \mathbb{Z}^+$: Number of tokens which overlap between adjacent chunks in a document.
    \item $t \in [0,1.2]$: Temperature of the LLM when generating responses.
    \item $r \in [0, 1]$: Rerank threshold used to set the minimum similarity between the context chunk and query, as evaluated by the reranker\footnote{We use a fixed rerank model \texttt{Salesforce/Llama-Rank-V1} provided by TogetherAI for all RAG systems.}. Retrieved documents which are below this threshold are ignored and not passed to the LLM as context. If no chunks exceed this threshold, we choose only the highest scoring chunk as context.
    \item $\ell \in \{\text{gpt-4o}, \text{gpt-4o-mini}, \text{llama-3.2-3B}, \text{llama-3.1-8B}\}$: Choice of LLM used to generate the response.
    \item $e \in \{\text{text-embedding-3-large},\text{text-embedding-3-small}
    \}$: Choice of embedding model when embedding the queries and document chunks.
\end{itemize}