\section{Related Work}
A branch of previous works has focused on specific safety areas in LLMs, such as toxicity~\citep{Hartvigsen2022ToxiGenAL}, bias~\citep{Dhamala2021BOLDDA, Wan2023BiasAskerMT}, copyright~\citep{Chang2023SpeakMA} and psychological safety~\citep{Huang2023EmotionallyNO}. There is also some work on the development of holistic safety datasets. \cite{Ganguli2022RedTL} collected 38,961 red team attack samples across different categories. \newcite{Ji2023BeaverTailsTI} collected 30,207 question-answer (QA) pairs to measure the helpfulness and harmlessness of LLMs. \newcite{Sun2023SafetyAO} released a comprehensive manually written safety prompt set on 14 kinds of risks. 
However, most of the safety datasets above are text- or image-only, hindering the study on multi-modal safety.


More recently, with the popularity of MLLMs, a few concurrent works have also worked on the safety of multimodal LLMs~\cite{wang2024chain, wang2024new}. For example, MM-Safety~\cite{Liu2023MMSafetyBenchAB} is a dataset designed for conducting safety-critical evaluations of MLLMs. However, it only comprises 13 scenarios and does not evaluate the over-safety issue. MossBench~\cite{li2024mossbench} is a multimodal oversensitivity benchmark with 3 types of over-safety scenarios. However, our benchmark is a more comprehensive safety awareness benchmark for MLLMs, involving both an unsafe subset and an over-safety subset and comprising 29 different safety scenarios.