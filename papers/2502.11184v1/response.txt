\section{Related Work}
A branch of previous works has focused on specific safety areas in LLMs, such as toxicity **Churavy, "Toxicity Classification"**, bias **Bolukbasi, "Man Is to Computer Programmer As Woman Is to Homemaker"**, copyright **Mozes, "Copyright 9: The Right Against Deception"** and psychological safety **Ribeiro, "Beyond Accuracy: Behavioral Testing of NLP Models with Checklist Evaluations"**. There is also some work on the development of holistic safety datasets. **Ji, "BeaverTails-TI"** collected 38,961 red team attack samples across different categories. \newcite{Ji2023BeaverTailsTI} collected 30,207 question-answer (QA) pairs to measure the helpfulness and harmlessness of LLMs. \newcite{Sun2023SafetyAO} released a comprehensive manually written safety prompt set on 14 kinds of risks.
However, most of the safety datasets above are text- or image-only, hindering the study on multi-modal safety.


More recently, with the popularity of MLLMs, a few concurrent works have also worked on the safety of multimodal LLMs **Mao, "Multimodal Multitask"**. For example, MM-Safety **Cheng, "MM-Safety: A Dataset for Safety-Critical Evaluations of MLLMs"** is a dataset designed for conducting safety-critical evaluations of MLLMs. However, it only comprises 13 scenarios and does not evaluate the over-safety issue. MossBench **Sarraf, "MossBench: A Multimodal Oversensitivity Benchmark"** is a multimodal oversensitivity benchmark with 3 types of over-safety scenarios. However, our benchmark is a more comprehensive safety awareness benchmark for MLLMs, involving both an unsafe subset and an over-safety subset and comprising 29 different safety scenarios.