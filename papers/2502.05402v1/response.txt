\section{Related Work}
With the rise of deep learning, many different studies have focused on a broad range of deep learning techniques and model architectures to accomplish the task of adding color to black and white images **Zhang et al., "Colorization using Deep Learning"**. These approaches can quickly and easily handle large amounts of data and have demonstrated effective results. Since the first deep learning image colorization method in 2015 **Larsson et al., "Learning Representations for AlexNet Architecture with One-Billion-Parameter Autoencoders"** , the field has rapidly grown. Various approaches have been applied, such as convolutional neural networks **Long et al., "Fully Convolutional Networks for Semantic Segmentation"** , which allow for retaining spatial information, generative adversarial networks (GANs) **Goodfellow et al., "Generative Adversarial Networks"** , which are advantageous for preserving details and high-quality image generation, and transformer networks **Vaswani et al., "Attention Is All You Need"** that excel at applying color with context to the entire image. Previous studies have also identified that color fidelity was improved when some original color from the image was retained **Isola et al., "Image-to-Image Translation with Conditional Adversarial Networks"** . In **Ummels et al., "On the Evaluation of Colorization Algorithms"** , they applied two different automated approaches for choosing the pixel location(s) for color retention. Such approaches included grid-based, where every n-th pixel remained colored, and segment-based, where segments in the images were identified and a pixel within each segment was chosen to retain color. Another recent study of particular interest for our paper is **Xiao et al., "Deep Image Recolorization using Convolutional U-net Architecture"** which proposes a convolutional, U-net architecture for image recolorization. Hence forth we will refer to this model architecture as \textbf{XiaoNet}. Our study will focus on the marriage of two methods: the use of grid-based, automated color retention like the method described in **Tulyakov et al., "MUNIT: Model-Based Inversion of Conditional GANs for Image-to-Image Translation"** with a convolutional U-net architecture derived from the XiaoNet architecture. Furthermore, we aim to find the optimal amount of color-information retention to maximize the image compression while maintaining enough information to make highly accurate predictions using our XiaoNet-like architecture More information on our architecture is available in Section \ref{sec:Model Arch}.