@article{Boutarfass2020,
   abstract = {Colorization is the process of converting a black and white image-lot of different shades of gray-into a realistic colour image. Our contribution highlights the fact that the CNNs used for colorization (and specifically the data set used for their training) are not adequate to colourize legacy BW pictures. In fact the data sets are exclusively composed of colour images, and turning these colour images into greyscale pushes the CNN to learn the contribution of each colour to the resulting luminance. Anyway, CNN are quite good in classification task and can easily recognize skies, trees and foliage, faces and persons. The colorization works well for these elements, but it fails even on memorable objects such as flags and well-known monuments albeit their representation is present in the data sets. We make improvement by using a pretrained network and add hints in the colorization process. Global hints (a colour palette given to the CNN conjointly to the image) still leave a high degree of automation, and these hints could be provided once for a collection of images related to the same theme, which is very suitable for movie colorization. Adding manual hints by scribbling colours onto the BW image leads to even better results but needs user interaction.},
   author = {Sanae Boutarfass and Bernard Besserer},
   doi = {10.1109/IPAS50080.2020.9334930},
   isbn = {9781728175744},
   journal = {4th International Conference on Image Processing, Applications and Systems, IPAS 2020},
   keywords = {CNN,Colorization,Colour palette,Deep learning,Interactive colorization},
   month = {12},
   pages = {96-101},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Improving CNN-based colorization of BW photographs},
   year = {2020},
}

@generic{Cheng2015,
   abstract = {This paper investigates into the colorization problem which converts a grayscale image to a colorful version. This is a very difficult problem and normally requires manual adjustment to achieve artifact-free quality. For instance, it normally requires human-labelled color scribbles on the grayscale target image or a careful selection of colorful reference images (e.g., capturing the same scene in the grayscale target image). Unlike the previous methods, this paper aims at a high-quality fully-automatic colorization method. With the assumption of a perfect patch matching technique, the use of an extremely large-scale reference database (that contains sufficient color images) is the most reliable solution to the colorization problem. However , patch matching noise will increase with respect to the size of the reference database in practice. Inspired by the recent success in deep learning techniques which provide amazing modeling of large-scale data, this paper re-formulates the colorization problem so that deep learning techniques can be directly employed. To ensure artifact-free quality, a joint bilateral filtering based post-processing step is proposed. Numerous experiments demonstrate that our method outperforms the state-of-art algorithms both in terms of quality and speed.},
   author = {Zezhou Cheng and Shanghai Jiao and Qingxiong Yang and Bin Sheng},
   pages = {415-423},
   title = {Deep Colorization},
   year = {2015},
}

@article{Dong2022,
   abstract = {Images taken underwater suffers from color shift and poor visibility because the light is absorbed and scattered when it travels through water. To handle the issues mentioned above, we propose an underwater image enhancement method via integrated RGB and LAB color models (RLCM). In the RGB color model, we first fully consider the leading causes of underwater image color shift, and then the poor color channels are corrected by dedicated fractions, which are designed via calculating the differences between the well and poor color channels. In the LAB color model, wherein the local contrast of the L channel is enhanced by a histogram with local enhancement and exposure cut-off strategy, whereas the difference between the A and B channels is traded-off by a gain equalization strategy. Besides, a normalized guided filtering strategy is incorporated into the histogram enhancement process to mitigate the effects of noise. Ultimately, the image is inverted from the LAB color model to the RGB color model, and a detail sharpening strategy is implemented in each channel to obtain a high-quality underwater image. Experiments on various real-world underwater images demonstrate that our method outputs better results with natural color and high visibility.},
   author = {Lili Dong and Weidong Zhang and Wenhai Xu},
   doi = {10.1016/J.IMAGE.2022.116684},
   issn = {0923-5965},
   journal = {Signal Processing: Image Communication},
   keywords = {Color correction,Gain equalization,Local enhancement,Underwater image color shift},
   month = {5},
   pages = {116684},
   publisher = {Elsevier},
   title = {Underwater image enhancement via integrated RGB and LAB color models},
   volume = {104},
   year = {2022},
}

@article{FatimaAroosh2021Gitn,
abstract = {GAN-based image colorization techniques are capable of producing highly realistic color in real-time. Subjective assessment of these approaches has demonstrated that humans are unable to differentiate between a true RGB image and a colorized image. In this work, we evaluate the fidelity of such colorization and for the first time analyze the GAN-based image colorization scheme in the context of image compression. Our analysis shows that the palette (set of colors) recommended by the GAN-based framework is very limited even for highly realistic interactive colorization. We propose two novel methods of automatic palette generation that allows for the GAN-based framework to be useful for image compression. We demonstrate that provided true colors at a few pixel locations, GAN-based approach results in good spread of color to other image regions. Subjective analysis on a number of public datasets shows that the current system has low fidelity but performs better than JPEG at low data rate regimes.},
author = {Fatima, Aroosh and Hussain, Wajahat and Rasool, Shahzad},
address = {New York},
copyright = {Springer Science+Business Media, LLC, part of Springer Nature 2020},
issn = {1380-7501},
journal = {Multimedia tools and applications},
keywords = {Accuracy ; Analysis ; Color ; Colorization ; Computer Communication Networks ; Computer Science ; Computer Science, Information Systems ; Computer Science, Software Engineering ; Computer Science, Theory & Methods ; Data compression ; Data Structures and Information Theory ; Electrical engineering ; Engineering ; Engineering, Electrical & Electronic ; Image compression ; Liquors ; Multimedia Information Systems ; Science & Technology ; Special Purpose and Application-Based Systems ; Subjective assessment ; Technology},
language = {eng},
number = {3},
pages = {3775-3791},
publisher = {Springer US},
title = {Grey is the new RGB: How good is GAN-based image colorization for image compression?},
volume = {80},
year = {2021},
}

@article{He2018,
   abstract = {We propose the first deep learning approach for exemplar-based local colorization. Given a reference color image, our convolutional neural network directly maps a grayscale image to an output color...},
   author = {Mingming He and Dongdong Chen and Jing Liao and Pedro V. Sander and Lu Yuan},
   doi = {10.1145/3197517.3201365},
   issn = {15577368},
   issue = {4},
   journal = {ACM Transactions on Graphics (TOG)},
   keywords = {colorization,deep learning,exemplar-based colorization,vision for graphics},
   month = {7},
   publisher = {
		ACM
		PUB27
		New York, NY, USA
	},
   title = {Deep exemplar-based colorization},
   volume = {37},
   url = {https://dl.acm.org/doi/10.1145/3197517.3201365},
   year = {2018},
}

@article{Kuang2020,
   abstract = {Transforming a thermal infrared image into a realistic RGB image is a challenging task. In this paper we propose a deep learning method to bridge this gap. We propose learning the transformation mapping using a coarse-to-fine generator that preserves the details. Since the standard mean squared loss cannot penalize the distance between colorized and ground truth images well, we propose a composite loss function that combines content, adversarial, perceptual and total variation losses. The content loss is used to recover global image information while the latter three losses are used to synthesize local realistic textures. Quantitative and qualitative experiments demonstrate that our approach significantly outperforms existing approaches on the KAIST multispectral pedestrian dataset, achieving more plausible RGB images. Our code is available online.},
   author = {Xiaodong Kuang and Jianfei Zhu and Xiubao Sui and Yuan Liu and Chengwei Liu and Qian Chen and Guohua Gu},
   doi = {10.1016/J.INFRARED.2020.103338},
   issn = {1350-4495},
   journal = {Infrared Physics \& Technology},
   keywords = {Colorization,Convolutional neural networks,Deep learning,Infrared images},
   month = {6},
   pages = {103338},
   publisher = {Pergamon},
   title = {Thermal infrared colorization via conditional generative adversarial network},
   volume = {107},
   year = {2020},
}

@article{Kumar2021,
   abstract = {We present the Colorization Transformer, a novel approach for diverse high
fidelity image colorization based on self-attention. Given a grayscale image,
the colorization proceeds in three steps. We first use a conditional
autoregressive transformer to produce a low resolution coarse coloring of the
grayscale image. Our architecture adopts conditional transformer layers to
effectively condition grayscale input. Two subsequent fully parallel networks
upsample the coarse colored low resolution image into a finely colored high
resolution image. Sampling from the Colorization Transformer produces diverse
colorings whose fidelity outperforms the previous state-of-the-art on
colorising ImageNet based on FID results and based on a human evaluation in a
Mechanical Turk test. Remarkably, in more than 60% of cases human evaluators
prefer the highest rated among three generated colorings over the ground truth.
The code and pre-trained checkpoints for Colorization Transformer are publicly
available at
https://github.com/google-research/google-research/tree/master/coltran},
   author = {Manoj Kumar and Dirk Weissenborn and Nal Kalchbrenner},
   month = {2},
   title = {Colorization Transformer},
   url = {https://arxiv.org/abs/2102.04432v2},
   year = {2021},
}

@article{Larsson2016,
   abstract = {We develop a fully automatic image colorization system. Our approach leverages recent advances in deep networks, exploiting both low-level and semantic representations. As many scene elements naturally appear according to multimodal color distributions, we train our model to predict per-pixel color histograms. This intermediate output can be used to automatically generate a color image, or further manipulated prior to image formation. On both fully and partially automatic colorization tasks, we outperform existing methods. We also explore colorization as a vehicle for self-supervised visual representation learning.},
   author = {Gustav Larsson and Michael Maire and Gregory Shakhnarovich},
   doi = {10.1007/978-3-319-46493-0_35/TABLES/6},
   isbn = {9783319464923},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {577-593},
   publisher = {Springer Verlag},
   title = {Learning representations for automatic colorization},
   volume = {9908 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-319-46493-0_35},
   year = {2016},
}

@generic{Su2020,
   abstract = {/projects/instaColorization Figure 1. Instance-aware colorization. We present an instance-aware colorization method that is capable of producing natural and colorful results on a wide range of scenes containing multiple objects with diverse context (e.g., vehicles, people, and man-made objects). Abstract Image colorization is inherently an ill-posed problem with multi-modal uncertainty. Previous methods leverage the deep neural network to map input grayscale images to plausible color outputs directly. Although these learning-based methods have shown impressive performance, they usually fail on the input images that contain multiple objects. The leading cause is that existing models perform learning and colorization on the entire image. In the absence of a clear figure-ground separation, these models cannot effectively locate and learn meaningful object-level semantics. In this paper, we propose a method for achieving instance-aware colorization. Our network architecture leverages an off-the-shelf object detector to obtain cropped object images and uses an instance colorization network to extract object-level features. We use a similar network to extract the full-image features and apply a fusion module to full object-level and image-level features to predict the final colors. Both colorization networks and fusion modules are learned from a large-scale dataset. Experimental results show that our work outperforms existing methods on different quality metrics and achieves state-of-the-art performance on image colorization.},
   author = {Jheng-Wei Su and Hung-Kuo Chu and Jia-Bin Huang},
   pages = {7968-7977},
   title = {Instance-Aware Image Colorization},
   url = {https://cgv.cs.nthu.edu.tw/projects/instaColorization},
   year = {2020},
}

@article{XiaoYi2022IDCa,
abstract = {Recent methods based on deep learning have shown promise in converting grayscale images to colored ones. However, most of them only allow limited user inputs (no inputs, only global inputs, or only local inputs), to control the output colorful images. The possible difficulty lies in how to differentiate the influences of different inputs. To solve this problem, we propose a two-stage deep colorization method allowing users to control the results by flexibly setting global inputs and local inputs. The key steps include enabling color themes as global inputs by extracting <inline-formula><tex-math notation="LaTeX">K</tex-math> <mml:math><mml:mi>K</mml:mi></mml:math><inline-graphic xlink:href="xiao-ieq1-3021510.gif"/> </inline-formula> mean colors and generating <inline-formula><tex-math notation="LaTeX">K</tex-math> <mml:math><mml:mi>K</mml:mi></mml:math><inline-graphic xlink:href="xiao-ieq2-3021510.gif"/> </inline-formula>-color maps to define a global theme loss, and designing a loss function to differentiate the influences of different inputs without causing artifacts. We also propose a color theme recommendation method to help users choose color themes. Based on the colorization model, we further propose an image compression scheme, which supports variable compression ratios in a single network. Experiments on colorization show that our method can flexibly control the colorized results with only a few inputs and generate state-of-the-art results. Experiments on compression show that our method achieves much higher image quality at the same compression ratio when compared to the state-of-the-art methods.},
author = {Xiao, Yi and Wu, Jin and Zhang, Jie and Zhou, Peiyao and Zheng, Yan and Leung, Chi-Sing and Kavan, Ladislav},
address = {United States},
issn = {1077-2626},
journal = {IEEE transactions on visualization and computer graphics},
keywords = {Color ; Deep convolutional neural network ; Gray-scale ; Histograms ; Image coding ; Image color analysis ; image colorization ; image compression ; Image quality ; interactive ; Loss measurement ; residual learning},
language = {eng},
number = {3},
pages = {1557-1572},
publisher = {IEEE},
title = {Interactive Deep Colorization and its Application for Image Compression},
volume = {28},
year = {2022},
}

@article{Zhang2017,
   abstract = {We propose a deep learning approach for user-guided image colorization. The
system directly maps a grayscale image, along with sparse, local user "hints"
to an output colorization with a Convolutional Neural Network (CNN). Rather
than using hand-defined rules, the network propagates user edits by fusing
low-level cues along with high-level semantic information, learned from
large-scale data. We train on a million images, with simulated user inputs. To
guide the user towards efficient input selection, the system recommends likely
colors based on the input image and current user inputs. The colorization is
performed in a single feed-forward pass, enabling real-time use. Even with
randomly simulated user inputs, we show that the proposed system helps novice
users quickly create realistic colorizations, and offers large improvements in
colorization quality with just a minute of use. In addition, we demonstrate
that the framework can incorporate other user "hints" to the desired
colorization, showing an application to color histogram transfer. Our code and
models are available at https://richzhang.github.io/ideepcolor.},
   author = {Richard Zhang and Jun Yan Zhu and Phillip Isola and Xinyang Geng and Angela S. Lin and Tianhe Yu and Alexei A. Efros},
   doi = {10.1145/3072959.3073703},
   issn = {15577368},
   issue = {4},
   journal = {ACM Transactions on Graphics},
   keywords = {Colorization,Deep learning,Edit propagation,Interactive colorization,Vision for graphics},
   month = {5},
   publisher = {Association for Computing Machinery},
   title = {Real-Time User-Guided Image Colorization with Learned Deep Priors},
   volume = {36},
   url = {https://arxiv.org/abs/1705.02999v1},
   year = {2017},
}

@article{Zhang2018,
   abstract = {Sketch or line art colorization is a research field with significant market demand. Different from photo colorization which strongly relies on texture information, sketch colorization is more chall...},
   author = {Lv Min Zhang and Chengze Li and Tien Tsin Wong and Yi Ji and Chun Ping Liu},
   doi = {10.1145/3272127.3275090},
   isbn = {9781450360081},
   issn = {15577368},
   journal = {ACM Transactions on Graphics (TOG)},
   keywords = {colorization,line arts,sketch},
   month = {12},
   publisher = {
		ACM
		PUB27
		New York, NY, USA
	},
   title = {Two-stage sketch colorization},
   url = {https://dl.acm.org/doi/10.1145/3272127.3275090},
   year = {2018},
}

