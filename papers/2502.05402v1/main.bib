@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})

@article{ZegerIvana2021GICM,
abstract = {Colorization is a process of converting grayscale images into visually acceptable color images. The main goal is to convince the viewer of the authenticity of the result. Grayscale images that need to be colorized are, in most cases, images with natural scenes. Over the last 20 years a wide range of colorization methods has been developed - from algorithmically simple, yet time- and energy-consuming because of unavoidable human intervention to more complicated, but simultaneously more automated methods. Automatic conversion has become a challenging area that combines machine learning and deep learning with art. This paper presents an overview and evaluation of grayscale image colorization methods and techniques applied to natural images. The paper provides a classification of existing colorization methods, explains the principles on which they are based, and highlights their advantages and disadvantages. Special attention is paid to deep learning methods. Relevant methods are compared in terms of image quality and processing time. Different metrics for color image quality assessment are used. Measuring the perceived quality of a color image is challenging due to the complexity of the human visual system. Multiple metrics used to evaluate colorization methods provide results by determining the difference between the predicted color value and the ground truth, which in several cases is not in coherence with image plausibility. The results show that user-guided neural networks are the most promising category for colorization because they successfully combine human intervention and neural network automation.},
author = {Zeger, Ivana and Grgic, Sonja and Vukovic, Josip and Sisul, Gordan},
address = {PISCATAWAY},
copyright = {Copyright The Institute of Electrical and Electronics Engineers, Inc. (IEEE) 2021},
issn = {2169-3536},
journal = {IEEE access},
keywords = {Automatic methods ; Automation ; black-and-white image ; Color imagery ; colorfulness ; Colorization ; Computer Science ; Computer Science, Information Systems ; Deep learning ; deep learning methods ; Engineering ; Engineering, Electrical & Electronic ; Evaluation ; example-based methods ; Gray scale ; grayscale image ; Image classification ; Image color analysis ; Image quality ; image quality assessment ; Image segmentation ; Indexes ; Licenses ; Machine learning ; Neural networks ; Quality assessment ; Science & Technology ; scribble-based methods ; Technology ; Telecommunications ; user-guided methods},
language = {eng},
pages = {113326-113346},
publisher = {IEEE},
title = {Grayscale Image Colorization Methods: Overview and Evaluation},
volume = {9},
year = {2021},
}

@article{FatimaAroosh2021Gitn,
abstract = {GAN-based image colorization techniques are capable of producing highly realistic color in real-time. Subjective assessment of these approaches has demonstrated that humans are unable to differentiate between a true RGB image and a colorized image. In this work, we evaluate the fidelity of such colorization and for the first time analyze the GAN-based image colorization scheme in the context of image compression. Our analysis shows that the palette (set of colors) recommended by the GAN-based framework is very limited even for highly realistic interactive colorization. We propose two novel methods of automatic palette generation that allows for the GAN-based framework to be useful for image compression. We demonstrate that provided true colors at a few pixel locations, GAN-based approach results in good spread of color to other image regions. Subjective analysis on a number of public datasets shows that the current system has low fidelity but performs better than JPEG at low data rate regimes.},
author = {Fatima, Aroosh and Hussain, Wajahat and Rasool, Shahzad},
address = {New York},
copyright = {Springer Science+Business Media, LLC, part of Springer Nature 2020},
issn = {1380-7501},
journal = {Multimedia tools and applications},
keywords = {Accuracy ; Analysis ; Color ; Colorization ; Computer Communication Networks ; Computer Science ; Computer Science, Information Systems ; Computer Science, Software Engineering ; Computer Science, Theory & Methods ; Data compression ; Data Structures and Information Theory ; Electrical engineering ; Engineering ; Engineering, Electrical & Electronic ; Image compression ; Liquors ; Multimedia Information Systems ; Science & Technology ; Special Purpose and Application-Based Systems ; Subjective assessment ; Technology},
language = {eng},
number = {3},
pages = {3775-3791},
publisher = {Springer US},
title = {Grey is the new RGB: How good is GAN-based image colorization for image compression?},
volume = {80},
year = {2021},
}

@article{XiaoYi2022IDCa,
abstract = {Recent methods based on deep learning have shown promise in converting grayscale images to colored ones. However, most of them only allow limited user inputs (no inputs, only global inputs, or only local inputs), to control the output colorful images. The possible difficulty lies in how to differentiate the influences of different inputs. To solve this problem, we propose a two-stage deep colorization method allowing users to control the results by flexibly setting global inputs and local inputs. The key steps include enabling color themes as global inputs by extracting <inline-formula><tex-math notation="LaTeX">K</tex-math> <mml:math><mml:mi>K</mml:mi></mml:math><inline-graphic xlink:href="xiao-ieq1-3021510.gif"/> </inline-formula> mean colors and generating <inline-formula><tex-math notation="LaTeX">K</tex-math> <mml:math><mml:mi>K</mml:mi></mml:math><inline-graphic xlink:href="xiao-ieq2-3021510.gif"/> </inline-formula>-color maps to define a global theme loss, and designing a loss function to differentiate the influences of different inputs without causing artifacts. We also propose a color theme recommendation method to help users choose color themes. Based on the colorization model, we further propose an image compression scheme, which supports variable compression ratios in a single network. Experiments on colorization show that our method can flexibly control the colorized results with only a few inputs and generate state-of-the-art results. Experiments on compression show that our method achieves much higher image quality at the same compression ratio when compared to the state-of-the-art methods.},
author = {Xiao, Yi and Wu, Jin and Zhang, Jie and Zhou, Peiyao and Zheng, Yan and Leung, Chi-Sing and Kavan, Ladislav},
address = {United States},
issn = {1077-2626},
journal = {IEEE transactions on visualization and computer graphics},
keywords = {Color ; Deep convolutional neural network ; Gray-scale ; Histograms ; Image coding ; Image color analysis ; image colorization ; image compression ; Image quality ; interactive ; Loss measurement ; residual learning},
language = {eng},
number = {3},
pages = {1557-1572},
publisher = {IEEE},
title = {Interactive Deep Colorization and its Application for Image Compression},
volume = {28},
year = {2022},
}
@article{Zhao2020,
   abstract = {In recent years, the development of convolutional neural networks (CNNs) has promoted continuous progress in scene classification of remote sensing images. Compared with natural image datasets, however, the acquisition of remote sensing scene images is more difficult, and consequently the scale of remote sensing image datasets is generally small. In addition, many problems related to small objects and complex backgrounds arise in remote sensing image scenes, presenting great challenges for CNN-based recognition methods. In this article, to improve the feature extraction ability and generalization ability of such models and to enable better use of the information contained in the original remote sensing images, we introduce a multitask learning framework which combines the tasks of self-supervised learning and scene classification. Unlike previous multitask methods, we adopt a new mixup loss strategy to combine the two tasks with dynamic weight. The proposed multitask learning framework empowers a deep neural network to learn more discriminative features without increasing the amounts of parameters. Comprehensive experiments were conducted on four representative remote sensing scene classification datasets. We achieved state-of-the-art performance, with average accuracies of 94.21%, 96.89%, 99.11%, and 98.98% on the NWPU, AID, UC Merced, and WHU-RS19 datasets, respectively. The experimental results and visualizations show that our proposed method can learn more discriminative features and simultaneously encode orientation information while effectively improving the accuracy of remote sensing scene classification.},
   author = {Zhicheng Zhao and Ze Luo and Jian Li and Can Chen and Yingchao Piao},
   doi = {10.3390/RS12203276},
   issn = {2072-4292},
   issue = {20},
   journal = {Remote Sensing 2020, Vol. 12, Page 3276},
   keywords = {CNN,NWPU,deep learning,multitask,scene classification,self,supervised},
   month = {10},
   pages = {3276},
   publisher = {Multidisciplinary Digital Publishing Institute},
   title = {When Self-Supervised Learning Meets Scene Classification: Remote Sensing Scene Classification Based on a Multitask Learning Framework},
   volume = {12},
   url = {https://www.mdpi.com/2072-4292/12/20/3276/htm https://www.mdpi.com/2072-4292/12/20/3276},
   year = {2020},
}
@article{Kumar2021,
   abstract = {We present the Colorization Transformer, a novel approach for diverse high
fidelity image colorization based on self-attention. Given a grayscale image,
the colorization proceeds in three steps. We first use a conditional
autoregressive transformer to produce a low resolution coarse coloring of the
grayscale image. Our architecture adopts conditional transformer layers to
effectively condition grayscale input. Two subsequent fully parallel networks
upsample the coarse colored low resolution image into a finely colored high
resolution image. Sampling from the Colorization Transformer produces diverse
colorings whose fidelity outperforms the previous state-of-the-art on
colorising ImageNet based on FID results and based on a human evaluation in a
Mechanical Turk test. Remarkably, in more than 60% of cases human evaluators
prefer the highest rated among three generated colorings over the ground truth.
The code and pre-trained checkpoints for Colorization Transformer are publicly
available at
https://github.com/google-research/google-research/tree/master/coltran},
   author = {Manoj Kumar and Dirk Weissenborn and Nal Kalchbrenner},
   month = {2},
   title = {Colorization Transformer},
   url = {https://arxiv.org/abs/2102.04432v2},
   year = {2021},
}
@article{Kuang2020,
   abstract = {Transforming a thermal infrared image into a realistic RGB image is a challenging task. In this paper we propose a deep learning method to bridge this gap. We propose learning the transformation mapping using a coarse-to-fine generator that preserves the details. Since the standard mean squared loss cannot penalize the distance between colorized and ground truth images well, we propose a composite loss function that combines content, adversarial, perceptual and total variation losses. The content loss is used to recover global image information while the latter three losses are used to synthesize local realistic textures. Quantitative and qualitative experiments demonstrate that our approach significantly outperforms existing approaches on the KAIST multispectral pedestrian dataset, achieving more plausible RGB images. Our code is available online.},
   author = {Xiaodong Kuang and Jianfei Zhu and Xiubao Sui and Yuan Liu and Chengwei Liu and Qian Chen and Guohua Gu},
   doi = {10.1016/J.INFRARED.2020.103338},
   issn = {1350-4495},
   journal = {Infrared Physics \& Technology},
   keywords = {Colorization,Convolutional neural networks,Deep learning,Infrared images},
   month = {6},
   pages = {103338},
   publisher = {Pergamon},
   title = {Thermal infrared colorization via conditional generative adversarial network},
   volume = {107},
   year = {2020},
}
@article{Zhang2018,
   abstract = {Sketch or line art colorization is a research field with significant market demand. Different from photo colorization which strongly relies on texture information, sketch colorization is more chall...},
   author = {Lv Min Zhang and Chengze Li and Tien Tsin Wong and Yi Ji and Chun Ping Liu},
   doi = {10.1145/3272127.3275090},
   isbn = {9781450360081},
   issn = {15577368},
   journal = {ACM Transactions on Graphics (TOG)},
   keywords = {colorization,line arts,sketch},
   month = {12},
   publisher = {
		ACM
		PUB27
		New York, NY, USA
	},
   title = {Two-stage sketch colorization},
   url = {https://dl.acm.org/doi/10.1145/3272127.3275090},
   year = {2018},
}
@article{Zhang2017,
   abstract = {We propose a deep learning approach for user-guided image colorization. The
system directly maps a grayscale image, along with sparse, local user "hints"
to an output colorization with a Convolutional Neural Network (CNN). Rather
than using hand-defined rules, the network propagates user edits by fusing
low-level cues along with high-level semantic information, learned from
large-scale data. We train on a million images, with simulated user inputs. To
guide the user towards efficient input selection, the system recommends likely
colors based on the input image and current user inputs. The colorization is
performed in a single feed-forward pass, enabling real-time use. Even with
randomly simulated user inputs, we show that the proposed system helps novice
users quickly create realistic colorizations, and offers large improvements in
colorization quality with just a minute of use. In addition, we demonstrate
that the framework can incorporate other user "hints" to the desired
colorization, showing an application to color histogram transfer. Our code and
models are available at https://richzhang.github.io/ideepcolor.},
   author = {Richard Zhang and Jun Yan Zhu and Phillip Isola and Xinyang Geng and Angela S. Lin and Tianhe Yu and Alexei A. Efros},
   doi = {10.1145/3072959.3073703},
   issn = {15577368},
   issue = {4},
   journal = {ACM Transactions on Graphics},
   keywords = {Colorization,Deep learning,Edit propagation,Interactive colorization,Vision for graphics},
   month = {5},
   publisher = {Association for Computing Machinery},
   title = {Real-Time User-Guided Image Colorization with Learned Deep Priors},
   volume = {36},
   url = {https://arxiv.org/abs/1705.02999v1},
   year = {2017},
}
@article{Dong2022,
   abstract = {Images taken underwater suffers from color shift and poor visibility because the light is absorbed and scattered when it travels through water. To handle the issues mentioned above, we propose an underwater image enhancement method via integrated RGB and LAB color models (RLCM). In the RGB color model, we first fully consider the leading causes of underwater image color shift, and then the poor color channels are corrected by dedicated fractions, which are designed via calculating the differences between the well and poor color channels. In the LAB color model, wherein the local contrast of the L channel is enhanced by a histogram with local enhancement and exposure cut-off strategy, whereas the difference between the A and B channels is traded-off by a gain equalization strategy. Besides, a normalized guided filtering strategy is incorporated into the histogram enhancement process to mitigate the effects of noise. Ultimately, the image is inverted from the LAB color model to the RGB color model, and a detail sharpening strategy is implemented in each channel to obtain a high-quality underwater image. Experiments on various real-world underwater images demonstrate that our method outputs better results with natural color and high visibility.},
   author = {Lili Dong and Weidong Zhang and Wenhai Xu},
   doi = {10.1016/J.IMAGE.2022.116684},
   issn = {0923-5965},
   journal = {Signal Processing: Image Communication},
   keywords = {Color correction,Gain equalization,Local enhancement,Underwater image color shift},
   month = {5},
   pages = {116684},
   publisher = {Elsevier},
   title = {Underwater image enhancement via integrated RGB and LAB color models},
   volume = {104},
   year = {2022},
}
@generic{Su2020,
   abstract = {/projects/instaColorization Figure 1. Instance-aware colorization. We present an instance-aware colorization method that is capable of producing natural and colorful results on a wide range of scenes containing multiple objects with diverse context (e.g., vehicles, people, and man-made objects). Abstract Image colorization is inherently an ill-posed problem with multi-modal uncertainty. Previous methods leverage the deep neural network to map input grayscale images to plausible color outputs directly. Although these learning-based methods have shown impressive performance, they usually fail on the input images that contain multiple objects. The leading cause is that existing models perform learning and colorization on the entire image. In the absence of a clear figure-ground separation, these models cannot effectively locate and learn meaningful object-level semantics. In this paper, we propose a method for achieving instance-aware colorization. Our network architecture leverages an off-the-shelf object detector to obtain cropped object images and uses an instance colorization network to extract object-level features. We use a similar network to extract the full-image features and apply a fusion module to full object-level and image-level features to predict the final colors. Both colorization networks and fusion modules are learned from a large-scale dataset. Experimental results show that our work outperforms existing methods on different quality metrics and achieves state-of-the-art performance on image colorization.},
   author = {Jheng-Wei Su and Hung-Kuo Chu and Jia-Bin Huang},
   pages = {7968-7977},
   title = {Instance-Aware Image Colorization},
   url = {https://cgv.cs.nthu.edu.tw/projects/instaColorization},
   year = {2020},
}
@article{He2018,
   abstract = {We propose the first deep learning approach for exemplar-based local colorization. Given a reference color image, our convolutional neural network directly maps a grayscale image to an output color...},
   author = {Mingming He and Dongdong Chen and Jing Liao and Pedro V. Sander and Lu Yuan},
   doi = {10.1145/3197517.3201365},
   issn = {15577368},
   issue = {4},
   journal = {ACM Transactions on Graphics (TOG)},
   keywords = {colorization,deep learning,exemplar-based colorization,vision for graphics},
   month = {7},
   publisher = {
		ACM
		PUB27
		New York, NY, USA
	},
   title = {Deep exemplar-based colorization},
   volume = {37},
   url = {https://dl.acm.org/doi/10.1145/3197517.3201365},
   year = {2018},
}
@article{Larsson2016,
   abstract = {We develop a fully automatic image colorization system. Our approach leverages recent advances in deep networks, exploiting both low-level and semantic representations. As many scene elements naturally appear according to multimodal color distributions, we train our model to predict per-pixel color histograms. This intermediate output can be used to automatically generate a color image, or further manipulated prior to image formation. On both fully and partially automatic colorization tasks, we outperform existing methods. We also explore colorization as a vehicle for self-supervised visual representation learning.},
   author = {Gustav Larsson and Michael Maire and Gregory Shakhnarovich},
   doi = {10.1007/978-3-319-46493-0_35/TABLES/6},
   isbn = {9783319464923},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {577-593},
   publisher = {Springer Verlag},
   title = {Learning representations for automatic colorization},
   volume = {9908 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-319-46493-0_35},
   year = {2016},
}
@generic{Cheng2015,
   abstract = {This paper investigates into the colorization problem which converts a grayscale image to a colorful version. This is a very difficult problem and normally requires manual adjustment to achieve artifact-free quality. For instance, it normally requires human-labelled color scribbles on the grayscale target image or a careful selection of colorful reference images (e.g., capturing the same scene in the grayscale target image). Unlike the previous methods, this paper aims at a high-quality fully-automatic colorization method. With the assumption of a perfect patch matching technique, the use of an extremely large-scale reference database (that contains sufficient color images) is the most reliable solution to the colorization problem. However , patch matching noise will increase with respect to the size of the reference database in practice. Inspired by the recent success in deep learning techniques which provide amazing modeling of large-scale data, this paper re-formulates the colorization problem so that deep learning techniques can be directly employed. To ensure artifact-free quality, a joint bilateral filtering based post-processing step is proposed. Numerous experiments demonstrate that our method outperforms the state-of-art algorithms both in terms of quality and speed.},
   author = {Zezhou Cheng and Shanghai Jiao and Qingxiong Yang and Bin Sheng},
   pages = {415-423},
   title = {Deep Colorization},
   year = {2015},
}
@article{Boutarfass2020,
   abstract = {Colorization is the process of converting a black and white image-lot of different shades of gray-into a realistic colour image. Our contribution highlights the fact that the CNNs used for colorization (and specifically the data set used for their training) are not adequate to colourize legacy BW pictures. In fact the data sets are exclusively composed of colour images, and turning these colour images into greyscale pushes the CNN to learn the contribution of each colour to the resulting luminance. Anyway, CNN are quite good in classification task and can easily recognize skies, trees and foliage, faces and persons. The colorization works well for these elements, but it fails even on memorable objects such as flags and well-known monuments albeit their representation is present in the data sets. We make improvement by using a pretrained network and add hints in the colorization process. Global hints (a colour palette given to the CNN conjointly to the image) still leave a high degree of automation, and these hints could be provided once for a collection of images related to the same theme, which is very suitable for movie colorization. Adding manual hints by scribbling colours onto the BW image leads to even better results but needs user interaction.},
   author = {Sanae Boutarfass and Bernard Besserer},
   doi = {10.1109/IPAS50080.2020.9334930},
   isbn = {9781728175744},
   journal = {4th International Conference on Image Processing, Applications and Systems, IPAS 2020},
   keywords = {CNN,Colorization,Colour palette,Deep learning,Interactive colorization},
   month = {12},
   pages = {96-101},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Improving CNN-based colorization of BW photographs},
   year = {2020},
}
@article{Baldassarre2017,
   abstract = {We review some of the most recent approaches to colorize gray-scale images
using deep learning methods. Inspired by these, we propose a model which
combines a deep Convolutional Neural Network trained from scratch with
high-level features extracted from the Inception-ResNet-v2 pre-trained model.
Thanks to its fully convolutional architecture, our encoder-decoder model can
process images of any size and aspect ratio. Other than presenting the training
results, we assess the "public acceptance" of the generated images by means of
a user study. Finally, we present a carousel of applications on different types
of images, such as historical photographs.},
   author = {Federico Baldassarre and Diego González Morín and Lucas Rodés-Guirao},
   keywords = {CNN,Colorization,Deep Learning,Inception-ResNet-v2,Keras,TensorFlow,Transfer Learning},
   month = {12},
   title = {Deep Koalarization: Image Colorization using CNNs and Inception-ResNet-v2},
   url = {https://arxiv.org/abs/1712.03400v1},
   year = {2017},
}
@article{Zhang2016,
   abstract = {Given a grayscale photograph as input, this paper attacks the problem of
hallucinating a plausible color version of the photograph. This problem is
clearly underconstrained, so previous approaches have either relied on
significant user interaction or resulted in desaturated colorizations. We
propose a fully automatic approach that produces vibrant and realistic
colorizations. We embrace the underlying uncertainty of the problem by posing
it as a classification task and use class-rebalancing at training time to
increase the diversity of colors in the result. The system is implemented as a
feed-forward pass in a CNN at test time and is trained on over a million color
images. We evaluate our algorithm using a "colorization Turing test," asking
human participants to choose between a generated and ground truth color image.
Our method successfully fools humans on 32% of the trials, significantly higher
than previous methods. Moreover, we show that colorization can be a powerful
pretext task for self-supervised feature learning, acting as a cross-channel
encoder. This approach results in state-of-the-art performance on several
feature learning benchmarks.},
   author = {Richard Zhang and Phillip Isola and Alexei A. Efros},
   doi = {10.1007/978-3-319-46487-9_40},
   isbn = {9783319464862},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {CNNs,Colorization,Self-supervised learning,Vision for graphics},
   month = {3},
   pages = {649-666},
   publisher = {Springer Verlag},
   title = {Colorful Image Colorization},
   volume = {9907 LNCS},
   url = {https://arxiv.org/abs/1603.08511v5},
   year = {2016},
}
@article{Jwa2021,
   abstract = {Image coloration refers to adding plausible colors to a grayscale image or video. Image coloration has been used in many modern fields, including restoring old photographs, as well as reducing the time spent painting cartoons. In this paper, a method is proposed for colorizing grayscale images using a convolutional neural network. We propose an encoder-decoder model, adapting FusionNet to our purpose. A proper loss function is defined instead of the MSE loss function to suit the purpose of coloring. The proposed model was verified using the ImageNet dataset. We quantitatively compared several colorization models with ours, using the peak signal-to-noise ratio (PSNR) metric. In addition, to qualitatively evaluate the results, our model was applied to images in the test dataset and compared to images applied to various other models. Finally, we applied our model to a selection of old black and white photographs.},
   author = {Minje Jwa and Myungjoo Kang},
   doi = {10.12941/jksiam.2021.25.026},
   issue = {2},
   journal = {J. KSIAM},
   pages = {26-38},
   title = {GRAYSCALE IMAGE COLORIZATION USING A CONVOLUTIONAL NEURAL NETWORK},
   volume = {25},
   url = {http://dx.doi.org/10.12941/jksiam.2021.25.026},
   year = {2021},
}
@INPROCEEDINGS{6236598,

  author={Semary, Noura. A. and Hadhoud, Mohiy. M. and El-Sayed, Hatem. M.},

  booktitle={2012 8th International Conference on Informatics and Systems (INFOS)}, 

  title={Morphological decolorization system for colors hiding}, 

  year={2012},

  volume={},

  number={},

  pages={MM-125-MM-131},

  doi={}}

