In this section we discuss Gaussian processes (GP), deep GP, and existing GP networks as well as their limitations.
\subsection{Gaussian process}\label{sec:stgp}
Considering the multi-process system in Figure~\ref{fig:multi_process}, it is conventionally modeled as a single-process black box with inputs $\cusvector{\params}_{\numobservation}:=\cusvector{\params}_{\numobservation}^{(1)} \in \paramsset$ and observed outputs $\tilde{\obsouts}_{\numobservation}=\tilde{\obsouts}_{\numobservation}^{(2)}$. A Gaussian process represents a stochastic process as a distribution over the infinite-dimensional functions, which can be evaluated at an input location in $\paramsset$. A finite set of observed outputs evaluated at respective input locations represents a multivariate normal distribution. For a given input location, $\cusvector{\params}_{\numobservation}$, of dimension $\dims{\params}$, the GP prior is represented as
\begin{equation}\label{eq:exact_gp_prior}
      \underbrace{\probability(\processfunction_{\numobservation}\vert\cusvector{\params}_{\numobservation})}_{\text{exact GP prior}} =\mathcal{N}\big(\gpmean(\cusvector{\params}_{\numobservation}), \gpkernel(\cusvector{\params}_{\numobservation}, \cusvector{\params}_{\numobservation}')\big),
\end{equation}
where $\gpmean(\cdot):\mathbb{R}^{\dims{\params}}\rightarrow\mathbb{R}$ is the mean function and $\gpkernel(\cdot,\cdot'):\paramsset\times\paramsset\rightarrow\mathbb{R}$ is the covariance function or kernel~\citep{rasmussen2003gaussian}.

For $\Numobservation$ observations, the hyperparameters of the GP are optimized by minimizing the negative MLL for the observed outputs using~\eqref{eq:exact_gp_prior} as
\begin{equation}\label{eq:STGP_MLL_exact}
      \mathcal{L}_\text{GP} = - \sum^{\Numobservation}_{\numobservation=1}\log\mathbb{E}_{\probability(\processfunction_{\numobservation}\vert\cusvector{\params}_{\numobservation})}\big[\probability(\obsouts_{\numobservation}\vert\processfunction_{\numobservation})\big],
\end{equation}
which can be calculated in closed form for a Gaussian likelihood and scales with $\mathcal{O}(\Numobservation^3)$~\citep{rasmussen2003gaussian}. The concepts of the single task GP can be extended to vector-valued stochastic process $\cusvector{\processfunction}_{\numobservation}(\cdot):\mathbb{R}^{\dims{\params}}\rightarrow\mathbb{R}^{\dims{\obsouts}}$ in which the observed output is a vector, of dimension $\dims{\obsouts}$, for each input location. A popular way of modeling correlation between the outputs is using the Linear Model of Coregionalization (LMC) as explained by~\cite{alvarez2012kernels,van2020framework}.

% The concepts of the single task GP can be extended to vector-valued stochastic process $\cusvector{\processfunction}(\cdot):\mathbb{R}^{\dims{\params}}\rightarrow\mathbb{R}^{\dims{\obsouts}}$ in which the observed output is a vector for each input location~\citep{alvarez2012kernels}. A popular way of modeling correlation between the outputs is using the Linear Model of Coregionalization (LMC) as explained by~\cite{alvarez2012kernels,van2020framework}. Outputs $\cusvector{\processfunction}(\cdot)$ are modeled as a linear combination of $\Numlmclatentgp$ independent latent GP outputs $\cusvector{\lmclatentfunction}(\cdot)=\{\lmclatentfunction_{\numlmclatentgp}(\cdot)\}_{\numlmclatentgp=1}^{\Numlmclatentgp}$, where
% \begin{equation}\label{eq:lmc_base_eq}
%     \lmclatentfunction_{\numlmclatentgp}(\cdot)\sim\mathcal{GP}\big(\gpmean_{\numlmclatentgp}(\cdot), \gpkernel_{\numlmclatentgp}(\cdot,\cdot')\big);\;\;\;\;\;
%     \cusvector{\processfunction}(\cdot) = \cusmatrix{\LMCmatrix}\cusvector{\lmclatentfunction}(\cdot),
% \end{equation}
% where $\gpmean_{\numlmclatentgp}:\paramsset\rightarrow\mathbb{R}$, $\gpkernel_{\numlmclatentgp}:\paramsset\times\paramsset\rightarrow\mathbb{R}$ and $\cusmatrix{\LMCmatrix}\in\mathbb{R}^{\dims{\obsouts}\times\Numlmclatentgp}$.
% % If $\Numlmclatentgp=1$, it resembles the Intrinsic Coregionalization Model (ICM)~\citep{alvarez2012kernels}. One can also make an underexplained model with $\Numlmclatentgp<\dims{\obsouts}$. As explained by~\cite{van2020framework}, 
% The joint distribution of the multi-task GP can be expressed as
% \begin{equation}\label{eq:MTexact_gp_prior}
%     \probability(\cusmatrix{\Processfunction};\cusmatrix{\Params})=\mathcal{N}\big(\cusvector{\gpmean}(\cusmatrix{\Params}), \cusmatrix{\gpkernel}(\cusmatrix{\Params}, \cusmatrix{\Params}')\big),
% \end{equation}
% where $\cusvector{\gpmean}(\cdot)=\cusmatrix{\LMCmatrix}\cusvector{\gpmean}_{\numlmclatentgp}(\cdot)$ is the mean function and covariance $\cusmatrix{\gpkernel}:\paramsset\times\paramsset\rightarrow\mathbb{R}^{\dims{\obsouts}\times\dims{\obsouts}}$ can be expressed in closed form as
% \begin{equation}\label{eq:LMC_kernel}
%     \cusmatrix{\gpkernel}(\cdot,\cdot') = \cusmatrix{\LMCmatrix}\text{diag}(\gpkernel_{\numlmclatentgp}(\cdot, \cdot'))\cusmatrix{\LMCmatrix}^\top.
% \end{equation}


% For regression where the $\obsouts_{\numobservation}$ is modeled using additive Gaussian noise $\sigma_{\obsouts}^2$,~\eqref{eq:STGP_MLL_exact} can be expressed in closed form as the marginal distribution $\probability(\obsouts_{\numobservation}\vert\cusvector{\params}_{\numobservation})$ becomes a normal distribution $\mathcal{N}\big(\gpmean(\cusvector{\params}_{\numobservation}), \gpkernel(\cusvector{\params}_{\numobservation}, \cusvector{\params}_{\numobservation}')+\sigma_{\obsouts}^2\big)$. 
% However, other approximation methods must be used for non-Gaussian likelihood and big data. One such method is discussed in section~\ref{sec:SVGP}.
%  we observe $\Numobservation$ outputs $\cusvector{\obsouts}\in\mathbb{R}^{\Numobservation}$ at inputs locations $\cusmatrix{\Params}\in\mathbb{R}^{\Numobservation\times\dims{\params}}$, with a likelihood  $\obsouts_{\numobservation} = \probability(\obsouts_{\numobservation}\vert\processfunction_{\numobservation})$ for $\numobservation\in\{1,\dots\Numobservation\}$.
% For a given input location $\cusvector{\params}$, the observed output $\obsouts$ is defined using the latent output $\trueouts$ and observation likelihood as $\probability(\obsouts\vert\trueouts)$ where $\trueouts\vert\cusvector{\params}\sim\mathcal{GP}(\gpmean(\cusvector{\params}), \gpkernel(\cusvector{\params}, \cusvector{\params}'))$, $\gpmean: \mathbb{R}^{\dims{\params}}\rightarrow\mathbb{R}$ is the mean function and $\gpkernel:\mathbb{R}^{\dims{\params}}\times\mathbb{R}^{\dims{\params}}\rightarrow\mathbb{R}$ is the covariance function~\citep{rasmussen2003gaussian}. GPs provide an elegant way to model stochastic processes by providing a function space to sample $\trueouts$ from. 
% Using this, one can model the stochasticity of the process differently from the observation noise.

% Gaussian likelihood with additive noise variance $\noisevar$, and zero mean; latent output $\trueouts_*$ at test input $\cusvector{\params}_{*}$, can be conditioned on $\cusmatrix{\Params}, \cusvector{\obsouts}$ as 
% \begin{align*}
% p(\trueouts_{*}\vert\cusvector{\params}_{*}\cusmatrix{\Params}, \cusvector{\obsouts}) &= \mathcal{N} \sim \left(\text{mu}(\trueouts_{*}), \text{cov}(\trueouts_{*}) \right) \\
% \text{mu}(\trueouts_{*}) &= \cusvector{k}^T\left(\cusmatrix{K} + \noisevar\cusmatrix{I}\right)^{-1}\cusvector{\obsouts} \nonumber\\
% \text{cov}(\trueouts_{*}) &= k(\cusvector{\params}_*, \cusvector{\params}_*) - \cusvector{k}^T\left(\cusmatrix{K} + \noisevar\cusmatrix{I}\right)^{-1}\cusvector{k}\nonumber
% \end{align*}
% where $\cusvector{k} = \gpkernel(\cusvector{\params}_{*}, \cusmatrix{\Params})$ and 
% $\cusmatrix{K} = \gpkernel(\cusmatrix{\Params}, \cusmatrix{\Params})$. The predictive posterior for $\obsouts_*$ at the test location can expressed using likelihood as \begin{equation*}
%     \probability(\obsouts_*\vert\cusvector{\params}_*)=\int\probability(\obsouts_*\vert\processfunction_*)\probability(\processfunction_*\vert\cusvector{\params}_*)d\processfunction.
% \end{equation*}

% \textbf{Put a few lines about LMC and multi-output GP. Predictive distribution for multi-output observations $\cusmatrix{\obsouts}\in\mathbb{R}^{\Numobservation\times\dims{\obsouts}}$ at input locations $\cusmatrix{\Params}$ can expressed as}
% \begin{align}
%  \probability(\cusmatrix{\obsouts}\vert\cusmatrix{\Params}) &= \int\probability(\cusmatrix{\obsouts}\vert\cusmatrix{\Processfunction})\probability(\cusmatrix{\Processfunction};\cusmatrix{\Params})d\processfunction\label{eq:MTGP_prdictive}\\
%  \probability(\cusmatrix{\Processfunction};\cusmatrix{\Params}) &=\mathcal{GP}\big(\gpmean(\cusmatrix{\Params}), \gpkernel_{\text{LMC}}(\cusmatrix{\Params}, \cusmatrix{\Params}')\big),
% \end{align}
\subsection{Stochastic Variational Gaussian Process}\label{sec:SVGP}
% Inference for Gaussian process using MLL in~\eqref{eq:STGP_MLL_exact} is not possible for big data or non-Gaussian likelihood~\citep {titsias2009variational}. Stochastic variational Gaussian Process (SVGP) by~\cite{hensman2013gaussian, hensman2015scalable} is a method that provides solutions for both cases.

% We assume a set of inducing points $\cusvector{\inducingpoints}\in\mathbb{R}^{\Numinducing}$, where $\inducingpoints_{\numinducing}\in\mathbb{R}$ represents the function value sampled from the GP evaluated at a pseudo inducing location $\cusvector{\inducinglocations}_{\numinducing}\in \mathbb{R}^{\dims{\params}}$. For inducing locations $\cusmatrix{\Inducinglocations}=\{\cusvector{\inducinglocations}_{\numinducing}\}^{\Numinducing}_{\numinducing=1}$, the joint distribution of inducing points $\cusvector{\inducingpoints}$ can represented as
% \begin{equation}
%  \probability(\cusvector{\inducingpoints};\cusmatrix{\Inducinglocations}) = \mathcal{GP}\big(\gpmean(\cusmatrix{\Inducinglocations}), \gpkernel(\cusmatrix{\Inducinglocations}, \cusmatrix{\Inducinglocations}')\big).
% \end{equation}
Stochastic Variational Gaussian Processes (SVGP) by~\cite{hensman2013gaussian, hensman2015scalable} are useful for large datasets and non-Gaussian likelihoods. It assumes a set of inducing locations, $\cusmatrix{\Inducinglocations}=\{\cusvector{\inducinglocations}_{\numinducing}\}^{\Numinducing}_{\numinducing=1}\in \paramsset$, and
% where $\cusvector{\inducinglocations}_{\numinducing}$.
inducing points which are, function value evaluations $\cusvector{\inducingpoints}=\{\inducingpoints_{\numinducing}\}_{\numinducing=1}^{\Numinducing}$ at $\{\cusvector{\inducinglocations}_{\numinducing}\}_{\numinducing=1}^{\Numinducing}$. The joint distribution of inducing points $\cusvector{\inducingpoints}$ can be represented as
\begin{equation}\label{eq:svgp_inducing_prior}
      \probability(\cusvector{\inducingpoints};\cusmatrix{\Inducinglocations}) = \mathcal{N}\big(\gpmean(\cusmatrix{\Inducinglocations}), \gpkernel(\cusmatrix{\Inducinglocations}, \cusmatrix{\Inducinglocations}')\big).
\end{equation}

Additionally, the inducing points are also assumed to have a marginal distribution $\variationalprob(\cusvector{\inducingpoints})=\mathcal{N}\big(\cusvector[bm]{\distmean}_{\cusvector{\inducingpoints}}, \cusmatrix[bm]{\distcov}_{\cusvector{\inducingpoints}}\big)$.
% Since $\cusvector{\inducingpoints}$ is evaluated using the same GP as the one being used to describe the observations, a joint multivariate normal distribution $\probability(\cusvector{\processfunction},\cusvector{\inducingpoints};\cusmatrix{\Params},\cusmatrix{\Inducinglocations})$ can be defined using~\eqref{eq:exact_gp_prior} and~\eqref{eq:svgp_inducing_prior} . For further details, readers are requested to refer to~\cite{leibfried2020tutorial}.
A joint multivariate normal distribution $\probability(\processfunction_{\numobservation},\cusvector{\inducingpoints}\vert\cusvector{\params}_{\numobservation},\cusmatrix{\Inducinglocations})$ can be defined using~\eqref{eq:exact_gp_prior} and~\eqref{eq:svgp_inducing_prior} . For further details, readers are encouraged to refer to~\cite{leibfried2020tutorial}.
% Following this, one can also find conditional probability $\probability(\cusvector{\processfunction}\vert\cusvector{\inducingpoints})$.

SVGP approximates the exact posterior with a variational posterior \begin{align}\label{eq:svgp_marginal_qf}
      \variationalprob(\processfunction_{\numobservation}\vert \cusvector{\params}_{\numobservation}, \cusmatrix{\Inducinglocations}) & =\mathbb{E}_{\variationalprob(\cusvector{\inducingpoints})}\big[\probability(\processfunction_{\numobservation}\vert\cusvector{\inducingpoints})\big]=\mathcal{N}\big(\distmean_{\variationalprob(\processfunction)}, \distcov_{\variationalprob(\processfunction)}\big),
\end{align}
where
\begin{align}\label{eq:svgp_marginal_qf_dist}
      \distmean_{\variationalprob(\cusvector{\processfunction})}            & = \gpmean(\cusvector{\params}_{\numobservation}) + \cusmatrix[bm]{\SvgpDistConst}(\cusvector{\params}_{\numobservation})^{\top}\big(\cusvector[bm]{\distmean}_{\cusvector{\inducingpoints}}-\gpmean(\cusmatrix{\Inducinglocations})\big),\nonumber                                                                                                                                                \\
      % new eq
      \distcov_{\variationalprob(\cusvector{\processfunction})}             & = \gpkernel(\cusvector{\params}_{\numobservation}, \cusvector{\params}_{\numobservation}) - \cusmatrix[bm]{\SvgpDistConst}(\cusvector{\params}_{\numobservation})^{\top}\big(\gpkernel(\cusmatrix{\Inducinglocations}, \cusmatrix{\Inducinglocations})-\cusmatrix[bm]{\distcov}_{\cusvector{\inducingpoints}}\big)\cusmatrix[bm]{\SvgpDistConst}(\cusvector{\params}_{\numobservation}),\nonumber \\
      % new eq
      \cusmatrix[bm]{\SvgpDistConst}(\cusvector{\params}_{\numobservation}) & =\gpkernel(\cusmatrix{\Inducinglocations}, \cusmatrix{\Inducinglocations})^{-1}\gpkernel(\cusmatrix{\Inducinglocations}, \cusvector{\params}_{\numobservation}).
\end{align}
Since the variational posterior $\variationalprob(\processfunction_{\numobservation}; \cusvector{\params}_{\numobservation}, \cusmatrix{\Inducinglocations})$, abbreviated as $\variationalprob(\processfunction_{\numobservation})$,
% is then used to predict new data points. Since the variational posterior 
is conditioned on a fixed number of inducing points, SVGP can scale and be used for non-Gaussian likelihoods.
% The Gaussian process hyperparameters and the variational approximation hyperparameters are optimized 
Hyperparameters are optimized by minimizing the negative Evidence Lower BOund (ELBO)
\begin{equation}\label{eq:svgp_elbo}
      \mathcal{L}_{\substack{\text{SVGP} \\ \text{ELBO}}} = -\sum^{\Numobservation}_{\numobservation=1} \Big[\mathbb{E}_{\variationalprob(\processfunction_{\numobservation})}[\log \probability(\obsouts_{\numobservation}\vert\processfunction_{\numobservation})]\Big]+\klconst \text{KL}\big(\variationalprob(\cusvector{\inducingpoints})\vert\vert\probability(\cusvector{\inducingpoints})\big),
\end{equation}
where~\eqref{eq:svgp_marginal_qf} and~\eqref{eq:svgp_marginal_qf_dist} are used to calculate $\mathbb{E}_{\variationalprob(\processfunction_{\numobservation})}$~\citep{hensman2015scalable}. ~\cite{jankowiak2020parametric} introduced another variational loss, namely, Parametric Predictive GP Regressor (PPGPR), which minimizes the negative Predictive Log Likelihood (PLL) for SVGP as
\begin{equation}\label{svgp:pll}
      \mathcal{L}_{\substack{\text{SVGP} \\ \text{PLL}}} = -\sum^{\Numobservation}_{\numobservation=1} \Big[\log \mathbb{E}_{\variationalprob(\processfunction_{\numobservation})}[\probability(\obsouts_{\numobservation}\vert\processfunction_{\numobservation})]\Big]+\klconst \text{KL}\big(\variationalprob(\cusvector{\inducingpoints})\vert\vert\probability(\cusvector{\inducingpoints})\big).
\end{equation}
The PPGPR claims to provide better predictive performance than ELBO. However, for non-conjugate likelihoods, the expectation for PLL cannot be calculated in closed form~\citep{jankowiak2020deep}.~\cite{van2020framework} show how inducing points approximation can be extended to multi-task GP.
% \subsection{Multi-output Gaussian Process}
% The concepts of the single task GP in section~\ref{sec:stgp} can be extended to vector-valued stochastic process $\cusvector{\processfunction}(\cdot):\mathbb{R}^{\dims{\params}}\rightarrow\mathbb{R}^{\dims{\obsouts}}$ in which the observed output is a vector for each input location~\citep{alvarez2012kernels}.
% % We assume at each input location $\cusvector{\params}_{\numobservation}$, we observe the vector-valued stochastic function with a likelihood $\probability({\cusvector{\obsouts}_{\numobservation}}\vert\cusvector{\processfunction}_{\numobservation})$. 
% A popular way of modeling correlation between the outputs is using the Linear Model of Coregionalization (LMC) as explained by~\cite{alvarez2012kernels,van2020framework}. Outputs $\cusvector{\processfunction}(\cdot)$ are modeled as a linear combination of $\Numlmclatentgp$ independent latent GP outputs $\cusvector{\lmclatentfunction}(\cdot)=\{\lmclatentfunction_{\numlmclatentgp}(\cdot)\}_{\numlmclatentgp=1}^{\Numlmclatentgp}$, where
% \begin{equation}\label{eq:lmc_base_eq}
%     \lmclatentfunction_{\numlmclatentgp}(\cdot)\sim\mathcal{GP}\big(\gpmean_{\numlmclatentgp}(\cdot), \gpkernel_{\numlmclatentgp}(\cdot,\cdot')\big);\;\;\;\;\;
%     \cusvector{\processfunction}(\cdot) = \cusmatrix{\LMCmatrix}\cusvector{\lmclatentfunction}(\cdot),
% \end{equation}
% where $\gpmean_{\numlmclatentgp}:\paramsset\rightarrow\mathbb{R}$, $\gpkernel_{\numlmclatentgp}:\paramsset\times\paramsset\rightarrow\mathbb{R}$ and $\cusmatrix{\LMCmatrix}\in\mathbb{R}^{\dims{\obsouts}\times\Numlmclatentgp}$. If $\Numlmclatentgp=1$, it resembles the Intrinsic Coregionalization Model (ICM)~\citep{alvarez2012kernels}. One can also make an underexplained model with $\Numlmclatentgp<\dims{\obsouts}$. As explained by~\cite{van2020framework}, the joint distribution of the multi-task GP can be expressed as
% \begin{equation}\label{eq:MTexact_gp_prior}
%     \probability(\cusmatrix{\Processfunction};\cusmatrix{\Params})=\mathcal{N}\big(\cusvector{\gpmean}(\cusmatrix{\Params}), \cusmatrix{\gpkernel}(\cusmatrix{\Params}, \cusmatrix{\Params}')\big),
% \end{equation}
% where $\cusvector{\gpmean}(\cdot)=\cusmatrix{\LMCmatrix}\cusvector{\gpmean}_{\numlmclatentgp}(\cdot)$ is the mean function and covariance $\cusmatrix{\gpkernel}:\paramsset\times\paramsset\rightarrow\mathbb{R}^{\dims{\obsouts}\times\dims{\obsouts}}$ can be expressed in closed form as
% \begin{equation}\label{eq:LMC_kernel}
%     \cusmatrix{\gpkernel}(\cdot,\cdot') = \cusmatrix{\LMCmatrix}\text{diag}(\gpkernel_{\numlmclatentgp}(\cdot, \cdot'))\cusmatrix{\LMCmatrix}^\top.
% \end{equation}

% For inference, exact MLL can be used~\cite{alvarez2012kernels}, and for big data or non-Gaussian likelihood, inducing points approximation can be extended to LMC multi-task GP as explained in~\cite{van2020framework}.

\subsection{Deep Gaussian Process}\label{sec:DGP}
Deep Gaussian Process (DGP) introduced by~\cite{damianou2013deep} provides a hierarchical model, in which independent GPs are stacked in layers as $\{\cusvector{\processfunction}^{(\numDGPlayer)}\in\mathbb{R}^{\dims{\numDGPlayer}}\}^{\NumDGPlayer}_{\numDGPlayer=1}$. Vector-valued outputs $\cusvector{\processfunction}_{\numobservation}^{(\numDGPlayer-1)}$ from the previous layer GPs become the inputs for the next layer output $\cusvector{\processfunction}_{\numobservation}^{(\numDGPlayer)}$.
% Owing to the hierarchical structure, the distribution $\probability(\cusvector{\processfunction}^{(\NumDGPlayer)}_{\numobservation};\cusvector{\params}_{\numobservation})$ is no longer Gaussian, due to which, inference in closed form is not possible. 
A prominent inference method for DGP is doubly stochastic variational inference by~\cite{salimbeni2017doubly}, which builds on the inference method of SVGP.
% and extends it using MC samples propagated through the layers, hence the name "doubly stochastic."

Similar to the idea of inducing points in the input domain of SVGP, inducing points $\{\cusmatrix{\Inducingpoints}^{(\numDGPlayer)}\in\mathbb{R}^{\Numinducing(\numDGPlayer)\times\dims{\numDGPlayer}}\}^{\NumDGPlayer}_{\numDGPlayer=1}$ are introduced at inducing locations $\{\cusmatrix{\Inducinglocations}^{(\numDGPlayer-1)}\in\mathbb{R}^{\Numinducing(\numDGPlayer)\times\dims{\numDGPlayer-1}}\}^{\NumDGPlayer}_{\numDGPlayer=1}$, where $\dims{0}=\dims{\params}$ and $\Numinducing(\numDGPlayer)$ is the number of inducing points for layer $\numDGPlayer$. Since the GPs are independent of each other~\cite{salimbeni2017doubly}, the marginal distribution for a layer depends only on the distribution of the previous layer, and the marginal distribution for layer $\NumDGPlayer$ can be expressed using~\eqref{eq:svgp_marginal_qf}~and~\eqref{eq:svgp_marginal_qf_dist} as
\begin{align}\label{eq:DGP_marginal}
      \variationalprob(\cusvector{\processfunction}_{\numobservation}^{(\NumDGPlayer)}) = \int \prod^{\NumDGPlayer}_{\numDGPlayer=1} \variationalprob(\cusvector{\processfunction}_{\numobservation}^{(\numDGPlayer)})\text{d}\cusvector{\processfunction}_{\numobservation}^{(\numDGPlayer-1)},
\end{align}
where $\variationalprob(\cusvector{\processfunction}_{\numobservation}^{(\numDGPlayer)}) = \variationalprob(\cusvector{\processfunction}_{\numobservation}^{(\numDGPlayer)}\vert\cusvector{\processfunction}_{\numobservation}^{(\numDGPlayer-1)}, \cusmatrix{\Inducinglocations}^{(\numDGPlayer-1)})$ and $\cusvector{\trueouts}_{\numobservation}^{(0)}=\cusvector{\params}_{\numobservation}$. For scalar-valued observation, the latent function $\processfunction^{(\NumDGPlayer)}$ is a scalar, and the variational posterior can be approximated by minimizing the negative ELBO for DGPs using~\eqref{eq:DGP_marginal} as
\begin{align}\label{eq:DGP_ELBO}
      \mathcal{L}_{\substack{\text{DGP} \\ \text{ELBO}}} = -&\sum^{\Numobservation}_{\numobservation=1}\mathbb{E}_{\variationalprob\big(\processfunction_{\numobservation}^{(\NumDGPlayer)}\big)}\big[\log\probability(\obsouts_{\numobservation}\vert\processfunction_{\numobservation}^{(\NumDGPlayer)})\big] \nonumber\\+ \klconst&\sum^{\NumDGPlayer}_{\numDGPlayer=1}\text{KL}\big(\variationalprob(\cusmatrix{\Inducingpoints}^{(\numDGPlayer)})\vert\vert\probability(\cusmatrix{\Inducingpoints}^{(\numDGPlayer)})\big).
\end{align}
Since $\variationalprob(\cusvector{\processfunction}_{\numobservation}^{(\NumDGPlayer)})$ represents a distribution rather than a scalar value, an exact solution is intractable. \cite{salimbeni2017doubly} proposed the use of $\MCsamples$ Monte Carlo (MC) samples as, $\cusvector{\trueouts}^{(\numDGPlayer)}_{\numobservation, \mcsamples}\sim\variationalprob(\cusvector{\processfunction}_{\numobservation}^{(\numDGPlayer)}\vert\cusvector{\trueouts}_{\numobservation}^{(\numDGPlayer-1)}, \cusmatrix{\Inducinglocations}^{(\numDGPlayer-1)})$, to calculate the expected log probability.