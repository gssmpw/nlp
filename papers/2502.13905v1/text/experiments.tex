In this section, we conduct a comprehensive comparison of the performance of POGPN with various models, including independent GPs (IGP), Semi-Parameteric Latent Factor Model (SLFM), GPRN~\cite{wilson2011gaussian} and GPAR~\cite{requeima2019gaussian}.
% We base our comparison on two well-known datasets.
% and the results for the other models are derived from the reported numbers by~\cite{requeima2019gaussian}.
POGPN is implemented using the gpytorch package~\cite{gardner2018gpytorch}. Here, we use a squared exponential kernel and constant mean for all experiments, as in other models. Similarly, we take the number of inducing locations the same as used by the D-GPAR-NL model from~\cite{requeima2019gaussian} to ensure proper comparison. We use the ICM variational approximation to model the multi-task nodes as proposed by~\cite{van2020framework}. The detailed construction procedure of POGPNs has been included in the supplementary section.

\textbf{Jura dataset}\footnote{The dataset can be downloaded from https://r-spatial.github.io/gstat/reference/jura.html.}. There are 259 locations from a mining area, for which the amount of zinc, nickel, and cadmium found is given. Along with these locations, there are another 100 locations with only zinc and nickel values available. The existing experiments use this information to predict the amount of cadmium for the remaining 100 locations. However, the original dataset also records two categorical observations: land use (4 classes) and the type of rock (5 classes) found at every location. Since all previous models cannot do classification and regression using one model, they do not use this information. However, we make a POGPN, as shown in Fig.~\ref{fig:jura_dag}, that can also use the categorical observations to predict the final output. We use softmax likelihood to model the multi-class observations and multi-task Gaussian likelihood (using LMC) to model mineral observations. For the latent function, we assume a two-dimensional multi-task GP as the latent function for categorical nodes "Rock" and "Land" and a three-dimensional GP for regression node "Zn, Ni, Cd." A detailed explanation of the POGPN structure has been shown in the supplementary section.

\begin{figure}[h]
      \centering
      \input{figures/jura_dag.tex}
      % \vspace{-1em}
      \caption{Structure of POGPN with root node location "Loc", softmax likelihoods for "Rock" and "Land", and multi-task Gaussian likelihood for minerals.}
      \vspace{-1em}
      \label{fig:jura_dag}
\end{figure}


\begin{table}[h]
      \centering
      % Constrain the table width to fit within a single column
      \begin{tabular}{@{}c@{}}
            % First sub-table
            \begin{tabular}{@{\hskip 0pt}lcccc@{\hskip 0pt}}
                  \toprule
                  Model & IGP\textsuperscript{\textdagger} & SLFM\textsuperscript{\textdagger} & GPRN\textsuperscript{\textdagger} & D-GPAR-NL\textsuperscript{\textdagger} \\
                  \midrule
                  MAE   & 0.5753                           & 0.4145                            & 0.4040                            & 0.3996                                 \\
                  \bottomrule
            \end{tabular}
            \\[2em] % Add vertical spacing between sub-tables

            % Second sub-table
            \begin{tabular}{@{\hskip 0pt}lccc@{\hskip 0pt}}
                  \toprule
                  Model & POGPN-AL\textsuperscript{\textdaggerdbl} & POGPN-NL\textsuperscript{\textdaggerdbl} & POGPN-AL\textsuperscript{\textasteriskcentered} \\
                  \midrule
                  MAE   & 0.3991
                        & \textbf{0.3989}                          & 0.5035                                                                                     \\
                  \bottomrule
            \end{tabular}
      \end{tabular}
      \caption{Prediction results for Jura dataset. Mean absolute error (MAE) (lower is better). Models marked with \textsuperscript{\textdagger} indicate cited results from~\cite{requeima2019gaussian}. POGPN-AL\textsuperscript{\textdaggerdbl} and POGPN-NL\textsuperscript{\textdaggerdbl} are calculated using PLL. POGPN-AL\textsuperscript{\textasteriskcentered} is calculated using ELBO.}
      \vspace{-1em}
\end{table}

The number of inducing locations is 259, equal to the number of locations for fully observed data. The values are log standardized for evaluation, used for training, and then transformed back, and the mean absolute error is calculated. It can be seen that POGPN outperforms all other models. This shows POGPN as a new state-of-the-art multi-task that can even use multimodal intermediate information.

\textbf{EEG dataset}\footnote{The dataset can be downloaded from https://archive.ics.uci.edu/dataset/121/eeg+database.}. The dataset consists of electrode measurements from the scalp of different subjects. Each sensor records 256 voltage measurements. The data focuses on the measurements from sensors F1, F2, F3, F4, F5, F6, and FZ from the first trial of control subject 337. The task is to predict F1, F2, and FZ measurements for the last 100 timestep, given the full observation of F3, F4, F5, and F6, and the first 156 measurements of F1, F2, and FZ. The values are standardized before training. We make a POGPN as shown in Fig.~\ref{fig:eeg_dag}, where the intermediate node is a four-dimensional multi-task GP, and the final node is a three-dimensional multi-task GP with respective multi-task Gaussian likelihoods.

\begin{figure}[h]
      \centering
      \begin{subfigure}[t]{\columnwidth}
            \centering
            \input{figures/eeg_dag.tex}
            \caption{POGPN for EEG dataset}
            \label{fig:eeg_dag}
      \end{subfigure}
      \hfill
      \begin{subfigure}[t]{\columnwidth}
            \centering
            \input{figures/popgn_eeg_pred.tex}
            % \includegraphics{figures/popgn_eeg_pred.pdf}
            \caption{Prediction results for F2 sensor from EEG dataset.}
            \label{fig:eeg_pred}
      \end{subfigure}
      \caption{Structure of POGPN for EEG dataset in Figure~\ref{fig:eeg_dag}. Figure~\ref{fig:eeg_pred} shows prediction results for sensor F2 using POGPN-AL (PLL) and GPN with a similar concept as described by Figure~\ref{fig:toy_process_gpn}.} % Overall caption for the figure
      \label{fig:eeg_combined} % Overall label for the figure
      \vspace{-1em}
\end{figure}

\begin{table}[h]
      % Reduce the default column padding for the enclosed tables.
      \setlength{\tabcolsep}{4pt}
      \centering
      % Constrain the table width to fit within a single column
      \begin{tabular}{@{}c@{}}
            % First sub-table
            \begin{tabular}{@{\hskip 0pt}lcccc@{\hskip 0pt}}
                  \toprule
                  Model & IGP\textsuperscript{\textdagger} & SLFM\textsuperscript{\textdagger} & GPAR-NL\textsuperscript{\textdagger} & POGPN-AL (PLL) \\
                  \midrule
                  SMSE  &
                  1.75  &
                  1.06  &
                  0.26  &
                  \textbf{0.24}
                  \\
                  MLL   &
                  2.60  & 4.00                             & 1.63                              & 1.04
                  % $\pm$0.11
                  \\
                  \bottomrule
            \end{tabular}
            \\[2em] % Add vertical spacing between sub-tables

            % Second sub-table
            \begin{tabular}{@{\hskip 0pt}lcc@{\hskip 0pt}}
                  \toprule
                  Model & POGPN-NL (PLL) & POGPN-AL (ELBO) \\
                  \midrule
                  SMSE  & 0.28           & 0.31            \\
                  MLL   & \textbf{0.18}  & 1.40            \\
                  \bottomrule
            \end{tabular}
      \end{tabular}
      \caption{Prediction results for EEG dataset of different models. Standardized mean squared error (SMSE) and Mean Log Loss (MLL)~\cite{rasmussen2003gaussian} comparison (lower is better). Models marked with \textsuperscript{\textdagger} indicate cited results from~\cite{requeima2019gaussian}.}
      \vspace{-1em}
\end{table}
% \vspace{-1em}
Inducing locations are in the "Time" domain and kept the same as the total time steps (256 points) as used by~\cite{requeima2019gaussian} and remain constant throughout the training process. For evaluation, the values are standardized before training and transformed back before prediction evaluation. POGPN consistently outperforms GPAR, showing significant improvements in SMSE and MLL. The results demonstrate the robustness of POGN against process stochasticity and the potential for even better confidence intervals, as shown in Figure~\ref{fig:eeg_pred}.

\textbf{Synthetic experiment.} We use the synthetic experiment from~\cite{requeima2019gaussian} but change it to have categorical observations to test the performance of POGPN for non-Gaussian noise along with categorical and continuous observations from subprocesses. Since the existing GPNs cannot incorporate categorical intermediate observations, we compare against DGP with the same layer structure but without intermediate subprocess likelihoods. For $x\in[0, 1]$, the system is described as
\begin{align*}
      f_{1}(x)
       & = -\frac{\sin\!\bigl(10\pi\,(x + 1)\bigr)}{2x + 1} - x^{4}           \\
      f_{2}(x)
       & = \cos^{2}\!\bigl(f_{1}(x)\bigr) + \sin(3x),                         \\
      f_{3}(x)
       & =\begin{cases}
                f_{2}(x)\,f_{1}(x)^{2} + 3\,x - 2.5, & \text{if } f_{2}(x) < 1.5,   \\
                f_{2}(x)\,f_{1}(x)^{2} + 3\,x + 2.5, & \text{if } f_{2}(x) \ge 1.5,
          \end{cases}
\end{align*}
and the intermediate observations are defined as
\begin{align*}
      y_{1} & = f_{1}(x) + \delta_{\text{obs}}, \\
      y_{2} & =
      \begin{cases}
            0, & \text{if } f_{2}(x) < 1.5,   \\
            1, & \text{if } f_{2}(x) \ge 1.5,
      \end{cases}
\end{align*}
where final output $y_3=f_3 + \delta_{\text{obs}}$ and $\delta_{\text{obs}} \sim \mathcal{N}(0, 0.1)$.
\begin{figure}[ht]
      \centering
      \begin{subfigure}[t]{\columnwidth}
            \centering
            \input{figures/synthetic_dag.tex}
            \caption{Structure of POGPN for synthetic experiment with Gaussian likelihood for $f^{(1)}$ and $f^{(3)}$, and Bernoulli likelihood for $f^{(2)}$.}
            % \vspace{-1em}
            \label{fig:synthetic_dag}
      \end{subfigure}
      \hfill
      \begin{subfigure}[t]{\columnwidth}
            \centering
            \input{figures/synthetic_pred.tex}
            % \includegraphics{figures/synthetic_pred.pdf}
            \caption{Prediction results for $y_3$ for synthetic experiment.}
            \label{fig:synthetic_pred}
      \end{subfigure}
      \caption{Structure of POGPN for synthetic experiment in Figure~\ref{fig:synthetic_dag}. Figure~\ref{fig:synthetic_pred} shows prediction results for output $y_3$ test data using POGPN-AL and PLL loss with $\klconst=0.5$. Deep GP has the same hierarchical structure as GPs.} % Overall caption for the figure
      \label{fig:synthetic_combined} % Overall label for the figure
      \vspace{-1em}
\end{figure}

We train on 40 equally spaced inputs in the range of $x$, and the number of inducing points is the same as training data points. As shown in Figure~\ref{fig:synthetic_pred}, POGPN can incorporate categorical intermediate subprocess observations, learn complex non-Gaussian process structure, and is robust against noise. The DGP cannot learn the process structure and provides wide confidence intervals due to high uncertainty, which has been discussed in detail by~\cite{duvenaud2014avoiding}. This also states the benefit of POGPN: intermediate node likelihoods can help the hierarchical GP model learn process structure better, thereby eliminating the pathology of deep networks and improving the predictive performance.