We present our main contribution, the Partially Observable Gaussian Process Network (POGPN). Instead of node observations sharing a common distribution space, we propose that the latent functions reside in the same space and influence the child subprocess nodes. POGPN represents the process network in section~\ref{sec:multi_process} using a DAG where the nodes, $\numprocess\in\numprocessset$, are topologically ordered such that $\numprocess'<\numprocess,\forall \numprocess' \in \nodeparent{\numprocess}$. Each node is modeled as a $\mathcal{GP}^{(\numprocess)}$ similar to the GPN setup in section~\ref{sec:gp_related_work}, generalized as a vector-valued function $\cusvector{\trueouts^{(\numprocess)}}_{\numobservation}\sim\mathcal{GP}^{(\numprocess)}(\cdot, \cdot')$ and can or cannot be observed using an arbitrary likelihood $\probability(\cusvector{\obsouts}^{(\numprocess)}_{\numobservation}\vert\cusvector{\processfunction}^{(\numprocess)}_{\numobservation})$. The generalized notation allows for nodes to have different dimensionality. This formulation allows us to consider DGP a special POGPN case where only the last node observations are available. POGPN, with its assumption of arbitrary observation likelihood and common latent function space, provides a way to model continuous and categorical observations with the same model.

Unlike the existing GPNs, a subprocess $\process^{(\numprocess)}$ takes the parent node latent GP functions $\cusvector{\processfunction}^{\nodeparent{\numprocess}}$ as the input rather than an instance of the noisy indirect observations $\cusvector{\tilde{\obsouts}}^{\nodeparent{\numprocess}}$. Since we can express the node's latent function using a distribution, we can take the expectation over the parent node distribution. The expectation over possible parent node output(s) provides robustness against parent subprocess stochasticity and can separate the observation lens from the actual process. This setup also allows for arbitrary observation likelihoods as the observation is separated from the network. Using POGPN, the process network $\Process$ in Figure~\ref{fig:multi_process} would be represented as Figure~\ref{fig:toy_process_pogpn}.

\textbf{Evidence Lower BOund (ELBO).} Similar to DGP in section~\ref{sec:DGP}, we introduce inducing points, $\cusmatrix{\Inducinglocations}^{\nodeparent{\numprocess}}, \cusmatrix{\Inducinglocations}^{\paramsset^{(\numprocess)}}$, in the space of the parent nodes and node inputs respectively, such that $\cusmatrix{\Inducinglocations}^{(\numprocess)}=(\cusmatrix{\Inducinglocations}^{\nodeparent{\numprocess}}, \cusmatrix{\Inducinglocations}^{\paramsset^{(\numprocess)}})$. We wish to approximate the posterior $\probability\big(\{\cusvector{\processfunction}_{\numobservation}^{(\numprocess)}\}_{\numprocess\in\numprocessset}\vert\{\cusvector{\obsouts}_{\numobservation}^{(\numprocess)}; \cusvector{\params}_{\numobservation}^{(\numprocess)}\}_{\numprocess\in\numprocessset}\big)$ with the variational posterior $\variationalprob\big(\{\cusvector{\processfunction}_{\numobservation}^{(\numprocess)}\}_{\numprocess\in\numprocessset}\big)$ which can be expressed using~\eqref{eq:DGP_marginal} as
\begin{equation}\label{eq:POGPN_variational_marginal}
      \variationalprob \big(\{\cusvector{\processfunction}_{\numobservation}^{(\numprocess)}\}_{\numprocess\in\numprocessset}\big) = \prod_{\numprocess\in\numprocessset}\variationalprob\big(\cusvector{\processfunction}_{\numobservation}^{(\numprocess)};\{\cusvector{\processfunction}_{\numobservation}^{\nodeparent{\numprocess}},\cusvector{\params}_{\numobservation}^{(\numprocess)}\}, \cusmatrix{\Inducinglocations}^{(\numprocess)}\big).
\end{equation}
The Kullback Leibler (KL)~\citep{shlens2014notes} divergence between the variational posterior and true posterior can be expressed as
\begin{equation}
      \begin{gathered}\label{eq:POGPN_KL_variational}
            - \text{KL}\big(\variationalprob(\{\cusvector{\processfunction}_{\numobservation}^{(\numprocess)}\}_{\numprocess\in\numprocessset})\Vert\probability(\{\cusvector{\processfunction}_{\numobservation}^{(\numprocess)}\}_{\numprocess\in\numprocessset}\vert\{\cusvector{\obsouts}_{\numobservation}^{(\numprocess)},\cusvector{\params}_{\numobservation}^{(\numprocess)}\}_{\numprocess\in\numprocessset})\big) \\
            =\underbrace{\mathbb{E}_{\variationalprob(\{\cusvector{\processfunction}_{\numobservation}^{(\numprocess)}\}_{\numprocess\in\numprocessset})}\big[\log\probability(\{\cusvector{\obsouts}_{\numobservation}^{(\numprocess)}\}_{\numprocess\in\numprocessset}\vert\{\cusvector{\processfunction}_{\numobservation}^{(\numprocess)},\cusvector{\params}_{\numobservation}^{(\numprocess)}\}_{\numprocess\in\numprocessset})\big]}_{\text{Log Likelihood Loss (LL loss)}} \\ \underbrace{-\text{KL}\big(\variationalprob(\{\cusvector{\processfunction}_{\numobservation}^{(\numprocess)}\}_{\numprocess\in\numprocessset})\Vert\probability(\{\cusvector{\processfunction}_{\numobservation}^{(\numprocess)}\}_{\numprocess\in\numprocessset})\big)}_{\text{KL loss}} + \text{ Evidence}
            % \underbrace{-\log\probability(\{\cusvector{\obsouts}_{\numobservation}^{(\numprocess)}\}_{\numprocess\in\numprocessset}\vert\{\cusvector{\params}_{\numobservation}^{(\numprocess)}\}_{\numprocess\in\numprocessset})}_{\text{Evidence (E)}}.
      \end{gathered}
\end{equation}
The ELBO for POGPN can then be defined as the combination of the "LL loss" and the "KL loss" term of~\eqref{eq:POGPN_KL_variational}. We now show how the terms of the ELBO can be simplified so that inference can be performed.

The conditional distribution $\probability(\{\cusvector{\obsouts}_{\numobservation}^{(\numprocess)}\}_{\numprocess\in\numprocessset}\vert\{\cusvector{\processfunction}_{\numobservation}^{(\numprocess)}\}_{\numprocess\in\numprocessset})$ can be simplified using the DAG structure as
\begin{align}\label{eq:DAG_cond_independent}
       & \probability(\{\cusvector{\obsouts}_{\numobservation}^{(\numprocess)}\}_{\numprocess\in\numprocessset}\vert\{\cusvector{\processfunction}_{\numobservation}^{(\numprocess)}\}_{\numprocess\in\numprocessset})\nonumber                                                                                                                                                                        \\
       & = \probability(\cusvector{\obsouts}_{\numobservation}^{(\Numprocess)}\vert\{\cusvector{\processfunction}_{\numobservation}^{(\numprocess)}\}_{\numprocess\in\numprocessset})\probability(\cusvector{\obsouts}_{\numobservation}^{(\Numprocess-1)},\{\cusvector{\processfunction}_{\numobservation}^{(\numprocess)}\}_{\numprocess\in\numprocessset})\nonumber                                 \\
       & = \prod_{\numprocess\in\numprocessset}\probability(\cusvector{\obsouts}_{\numobservation}^{(\numprocess)}\vert\{\cusvector{\processfunction}_{\numobservation}^{(\numprocess)}\}_{\numprocess\in\numprocessset})= \prod_{\numprocess\in\numprocessset}\probability(\cusvector{\obsouts}_{\numobservation}^{(\numprocess)}\vert\cusvector{\processfunction}_{\numobservation}^{(\numprocess)})
\end{align}
where $\Numprocess=\vert\numprocessset\vert$. Using~\eqref{eq:POGPN_variational_marginal} and~\eqref{eq:DAG_cond_independent}, the "LL loss" in Equation\eqref{eq:POGPN_KL_variational}, can be expressed as
\begin{align}\label{eq:pogpn_elbo_ll}
        & \mathbb{E}_{\variationalprob(\{\cusvector{\processfunction}_{\numobservation}^{(\numprocess)}\}_{\numprocess\in\numprocessset})}\big[\log\probability(\{\cusvector{\obsouts}_{\numobservation}^{(\numprocess)}\}_{\numprocess\in\numprocessset}\vert\{\cusvector{\processfunction}_{\numobservation}^{(\numprocess)},\cusvector{\params}_{\numobservation}^{(\numprocess)}\}_{\numprocess\in\numprocessset})\big]\nonumber \\
      = & \mathbb{E}_{\variationalprob(\{\cusvector{\processfunction}_{\numobservation}^{(\numprocess)}\}_{\numprocess\in\numprocessset})}\big[\sum_{\numprocess\in\numprocessset}\log\probability(\cusvector{\obsouts}_{\numobservation}^{(\numprocess)}\vert\cusvector{\processfunction}_{\numobservation}^{(\numprocess)})\big]\nonumber                                                                                          \\
      = & \sum_{\numprocess\in\numprocessset} \mathbb{E}_{\variationalprob(\cusvector{\processfunction}_{\numobservation}^{(\numprocess)})}\big[\log\probability(\cusvector{\obsouts}_{\numobservation}^{(\numprocess)}\vert\cusvector{\processfunction}_{\numobservation}^{(\numprocess)})\big],
\end{align}
where the marginal $\variationalprob(\cusvector{\processfunction}_{\numobservation}^{(\numprocess)})$ can be calculated using~\eqref{eq:svgp_marginal_qf} and~\eqref{eq:svgp_marginal_qf_dist} as
\begin{align}\label{eq:POGPN_marginal}
      \variationalprob(\cusvector{\processfunction}_{\numobservation}^{(\numprocess)}) = \int\prod^{\numprocess}_{j\in\numprocessset}\variationalprob\big(\cusvector{\processfunction}_{\numobservation}^{(j)}\vert\cusvector{\processfunction}_{\numobservation}^{\nodeparent{j}}, \cusvector{\params}_{\numobservation}^{(j)}, \cusmatrix{\Inducinglocations}^{(j)}\big)\text{d}\cusvector{\processfunction}_{\numobservation}^{\nodeparent{j}}.
\end{align}
For $\Numobservation^{(\numprocess)}$ observations for each node $\numprocess$, the inference can be made by minimizing the negative EBLO for POGPN as
\begin{align}\label{eq:pogpn_elbo}
      \mathcal{L}_{\substack{\text{POGPN}                                                                                                                                                                                          \\\text{ELBO}}}^{(\numprocessset)} = &\sum_{\numprocess\in\numprocessset} \mathcal{L}_{\substack{\text{Node} \\ \text{ELBO}}} ^{(\numprocess)} \nonumber \\=&\underbrace{-\sum_{\numprocess\in\numprocessset}\frac{1}{\pogpnnormconst^{(\numprocess)}}\sum^{\Numobservation^{(\numprocess)}}_{\numobservation}\mathbb{E}_{\variationalprob(\cusvector{\processfunction}^{(\numprocess)}_{\numobservation})}\big[\log\probability(\cusvector{\obsouts}^{(\numprocess)}_{\numobservation}\vert\cusvector{\processfunction}^{(\numprocess)}_{\numobservation})\big]}_{\text{LL loss}}\nonumber \\
      + & \underbrace{\klconst\sum_{\numprocess\in\numprocessset}\text{KL}\big(\variationalprob(\cusvector{\inducingpoints}^{(\numprocess)})\Vert\probability(\cusvector{\inducingpoints}^{(\numprocess)})\big)}_{\text{KL loss}},
\end{align}
a normalization constant $\pogpnnormconst^{(\numprocess)}$ is introduced to keep the likelihood loss from different nodes comparable when the dimensionality of the node is not the same. We propose to keep $\pogpnnormconst^{(\numprocess)}=\dims{\obsouts^{(\numprocess)}}$ to keep the "LL loss" term of different nodes comparable and give equal importance to each node, where $\dims{\obsouts^{(\numprocess)}}$ is the dimension of the observed output $\obsouts^{(\numprocess)}$. However, $\pogpnnormconst^{(\numprocess)}$ incorporate importance-based training where the emphasis lies on a particular node. If a node $\numprocess'$ has no observation likelihood, then $\numprocess'$ will contribute to only the "KL loss" and not to the "LL loss".

\textbf{Predictive Log Likelihood (PLL) loss.} Using the inspiration from the PLL loss in~\ref{svgp:pll}~\citep{jankowiak2020parametric}, the "LL loss" for PLL can be defined for POGPN as
\begin{equation}\label{eq:pogpn_pll}
      \text{LL}_{\text{PLL}} = \underbrace{-\sum_{\numprocess\in\numprocessset}\frac{1}{\pogpnnormconst^{(\numprocess)}}\sum^{\Numobservation^{(\numprocess)}}_{\numobservation=1}\log\mathbb{E}_{\variationalprob(\cusvector{\processfunction}^{(\numprocess)}_{\numobservation})}\big[\probability(\cusvector{\obsouts}^{(\numprocess)}_{\numobservation}\vert\cusvector{\processfunction}^{(\numprocess)}_{\numobservation})\big]}_{\text{LL loss}},
      % \nonumber \\ & +\underbrace{\klconst\sum_{\numprocess\in\numprocessset}\text{KL}\big(\variationalprob(\cusvector{\inducingpoints}^{(\numprocess)})\Vert\probability(\cusvector{\inducingpoints}^{(\numprocess)})\big)}_{\text{KL loss}}
\end{equation}
while the "KL loss" is the same as~\eqref{eq:pogpn_elbo}. Like DGP inference, we use MC samples to calculate the "LL loss" for POGPN ELBO in~\eqref{eq:pogpn_elbo}. It is common to calculate log probabilities to avoid loss of precision, and a direct summation of log probabilities of MC samples would lead to expected log marginal likelihood rather than log expected marginal likelihood in~\eqref{eq:pogpn_pll}. Using Jensen's inequality, one can prove that the former is an unbiased estimator of the latter. \cite{jankowiak2020deep} provides the sigma point method as a solution, but it is not easy to scale. We propose a more straightforward approach, where we use \verb|logsumexp| to calculate the log of expected likelihood marginal (LL loss) of PLL using MC samples as
\begin{align}\label{eq:mc_pogpn_pll_ll}
      \text{LL}_\text{PLL} & = -\sum_{\numprocess\in\numprocessset} \frac{1}{\pogpnnormconst^{(\numprocess)}} \sum^{\Numobservation^{(\numprocess)}}_{\numobservation=1} \log \Big(\frac{1}{\MCsamples}\sum^{\MCsamples}_{\mcsamples=1} \probability \big(\cusvector{\obsouts}^{(\numprocess)}_{\numobservation} \big| \cusvector{\trueouts}^{(\numprocess)}_{\numobservation, \mcsamples} \big)\Big)
\end{align} where $\cusvector{\trueouts}^{(\numprocess)}_{\numobservation, \mcsamples}\sim\variationalprob(\cusvector{\processfunction}_{\numobservation}^{(\numprocess)})$. This formulation has a tighter lower bound to the log expected marginal likelihood in comparison to the unbiased estimator. For generalization, we call ELBO and PLL the "loss" for POGPN. MC samples, used while training, can be considered analogous to training the child process on many hypothesized parent true/latent outputs, and the variational inference allows for robustness against the stochasticity of parent subprocesses.

We now present two methods, namely ancestor-wise and node-wise, for training POGPN . These methods can use either of the factorized losses, ELBO~\eqref{eq:pogpn_elbo} or PLL~\eqref{eq:pogpn_pll}. For $\Numprocess$ nodes, $\Numobservation$ observations, and $\Numinducing$ points for each node, the computational complexity is $\mathcal{O}(\Numprocess(\Numobservation\Numinducing^2+\Numinducing^3))$.

\textbf{Ancestor-wise Training.}
Algorithm~\ref{alg:pogpn_ancestorwise}  called POGPN-AL can be implemented using~\eqref{eq:pogpn_elbo} and~\eqref{eq:pogpn_pll}, where $\text{Anc}(\numprocessset_{\text{obs}})$ represents the set of all ancestors of each node $\numprocess\in\numprocessset_{\text{obs}}$; $\cusvector[bm]{\gphyperparam}^{(\numprocess)}$ represent the GP hyperparameters (mean, kernel and variational) and $\cusvector[bm]{\likelihoodparam}^{(\numprocess)}$ and likelihood hyperparameters of node $\numprocess$.
\begin{algorithm}
      \SetAlgoLined
      \caption{POGPN Ancestor-wise Loss (POGPN-AL) training. Given $\{\cusmatrix{\Obsouts}^{(\numprocess)}\}_{\numprocess\in\numprocessset_{\text{obs}}}$ observations for $\numprocessset_{\text{obs}}$,  GP hyperparameters of observed nodes $\cusvector[bm]\gphyperparam^{(\numprocessset_{\text{obs}})}$ and their ancestors $\cusvector[bm]\gphyperparam^{(\nodeancestor{\numprocessset_{\text{obs}}})}$ along with hyperparameters of observed likelihoods $\cusvector[bm]{\likelihoodparam}^{(\numprocessset_{\text{obs}})}$ are trained. One can use either ELBO or PLL loss from~\ref{eq:pogpn_elbo} or~\ref{eq:pogpn_pll} as $\mathcal{L}_{\text{POGPN}}$. $\cusvector[bm]{\gamma}^{(\numprocessset_{\text{obs}})}=(\cusvector[bm]{\gphyperparam}^{(\numprocessset_{\text{obs}})}, \cusvector[bm]{\gphyperparam}^{(\nodeancestor{\numprocessset_{\text{obs}}})},  \cusvector[bm]{\likelihoodparam}^{(\numprocessset_{\text{obs}})})$.}\label{alg:pogpn_ancestorwise}

      % \SetKwOutput{Output}{Output} % Define the Output keyword
      \SetKwInput{Input}{Input} % Define the Input keyword

      \Input{Training data: $\{\mathcal{D}^{(\numprocess)}\}_{\numprocess\in\numprocessset_{\text{obs}}} = \{\cusmatrix{\Obsouts}^{(\numprocess)}, \cusmatrix{\Params}^{(\numprocess)}\}_{\numprocess\in\numprocessset_{\text{obs}}}$}
      \SetInd{2em}{1em}
      \Indp % Start indentation
      Loss: $\mathcal{L}_{\text{POGPN}}$\\
      Hyperparameters: $\cusvector[bm]{\gamma}^{(\numprocessset_{\text{obs}})}$\\
      Gradient optimizer: \texttt{optim}\\
      \Indm % End indentation
      \SetInd{1em}{1em}
      \While{not converged}{
      Compute $\variationalprob(\{\cusmatrix{\Processfunction}^{(\numprocess)}\}_{\numprocess \in \numprocessset_{\text{obs}}})$ using MC samples\;
      Compute $\mathcal{L}_{\text{POGPN}}(\numprocessset_{\text{obs}})$ using $\variationalprob(\{\cusmatrix{\Processfunction}^{(\numprocess)}\}_{\numprocess \in \numprocessset_{\text{obs}}})$ and $\{\mathcal{D}^{(\numprocess)}\}_{\numprocess\in\numprocessset_{\text{obs}}}$\;
      Gradient step: $\cusvector[bm]{\gamma}^{(\numprocessset_{\text{obs}})} \leftarrow \texttt{optim}(\mathcal{L}_{\text{POGPN}}^{(\numprocessset_{\text{obs}})})$\;
      }
      \KwOut{Optimized hyperparameters: $\cusvector[bm]{\gamma}^{(\numprocessset_{\text{obs}})}$}
\end{algorithm}
We call Algorithm~\ref{alg:pogpn_ancestorwise} ancestor-wise training as it updates the parameters of all ancestor GPs of the observed nodes. This method is similar to the traditional method of training DGP, just that we consider multiple observation nodes in POPGN. It is beneficial when either all network nodes are observed or the nodes further in the graph are observed, and one wishes to condition the ancestor node(s) based on the observations of the child/successor node(s).

\begin{figure}[h]
      \centering
      \input{figures/training.tex}
      % \vspace{-1em}
      \caption{Training methods POGPN for a given structure. If $\numprocessset_{\text{obs}}=\{\cusvector{\obsouts}^{(4)}, \cusvector{\obsouts}^{(5)}\}$, POGPN-AL includes hyperparameters for node $\cusvector[bm]{\gamma}^{(\numprocessset_{\text{obs}})}=(\cusvector[bm]{\gphyperparam}^{(\numprocessset_{\text{obs}})}, \cusvector[bm]{\gphyperparam}^{(\nodeancestor{\numprocessset_{\text{obs}}})},  \cusvector[bm]{\likelihoodparam}^{(\numprocessset_{\text{obs}})})$ bounded by the blue dashed box. POGPN-NL trains hyperparameters, $\cusvector[bm]{\gamma}^{(\numprocess)} = (\cusvector[bm]{\gphyperparam}^{(\numprocess)}, \cusvector[bm]{\likelihoodparam}^{(\numprocess)}), \forall\numprocess\in\numprocessset_{\text{obs}}$, node-wise as bounded by red dashed boxes. Gray nodes represent observed output nodes (likelihood), and white nodes represent latent output nodes (GP).}
      \vspace{-1em}
      \label{fig:training}
\end{figure}

\begin{algorithm}
      \SetAlgoLined
      \caption{POGPN Node-wise Loss (POGPN-NL) training. Given $\{\Numobservation^{(\numprocess)}\}_{\numprocess\in\numprocessset_{\text{obs}}}$ observations for $\numprocessset_{\text{obs}}$, the GP hyperparameters $\cusvector[bm]\gphyperparam^{(\numprocess)}$ and likelihood hyperparameters $\cusvector[bm]{\likelihoodparam}^{(\numprocess)}$ are trained for one node at a time for $\numprocess\in\numprocessset_{\text{obs}}$. One can use either ELBO or PLL loss from~\ref{eq:pogpn_elbo} or~\ref{eq:pogpn_pll} as $\mathcal{L}_{\text{POGPN}}$. $\cusvector[bm]{\gamma}^{(\numprocessset_{\text{obs}})}=(\cusvector[bm]{\gphyperparam}^{(\numprocessset_{\text{obs}})}, \cusvector[bm]{\likelihoodparam}^{(\numprocessset_{\text{obs}})})$.}\label{alg:pogpn_node-wise}

      \SetKwInput{Input}{Input} % Define the Input keyword

      \Input{Training data: $\{\mathcal{D}^{(\numprocess)}\}_{\numprocess\in\numprocessset_{\text{obs}}} = \{\cusmatrix{\Obsouts}^{(\numprocess)}, \cusmatrix{\Params}^{(\numprocess)}\}_{\numprocess\in\numprocessset_{\text{obs}}}$}
      \SetInd{2em}{1em}
      \Indp % Start indentation
      Loss: $\mathcal{L}_{\text{POGPN}}$\\
      Gradient optimizer: \texttt{optim}\\
      \Indm % End indentation
      \SetInd{1em}{1em}

      \While{not converged}{
      \For{$\numprocess \in \text{\texttt{topological\_sort}}(\numprocessset_{\text{obs}})$}{
      Hyperparameters: $\cusvector[bm]{\gamma}^{(\numprocess)} = (\cusvector[bm]{\gphyperparam}^{(\numprocess)}, \cusvector[bm]{\likelihoodparam}^{(\numprocess)})$\\
      Compute $\variationalprob(\cusmatrix{\Processfunction}^{(\numprocess)})$ using MC samples\;
      Compute $\mathcal{L}_{\text{node}}^{(\numprocess)}$ using $\variationalprob(\cusmatrix{\Processfunction}^{(\numprocess)})$ and  $\mathcal{D}^{(\numprocess)}$\;
      Gradient step $\cusvector[bm]{\gamma}^{(\numprocess)} \leftarrow \texttt{optim}(\mathcal{L}_{\text{node}}^{(\numprocess)})$\;
      }
      }
      \KwOut{Optimized hyperparameters: $\cusvector[bm]{\gamma}^{(\numprocessset_{\text{obs}})}$}
\end{algorithm}
% \vspace{-1.5em}

\textbf{Node-wise Training.} Algorithm~\ref{alg:pogpn_node-wise}  called POGPN-NL, follows a coordinate ascent method for updating individual node GP hyperparameters $\cusvector[bm]{\gphyperparam}^{(\numprocess)}$ and likelihood hyperparameters $\cusvector[bm]{\likelihoodparam}^{(\numprocess)}$ for $\numprocess\in\numprocessset_{\text{obs}}$. With experimentation, we found that calculating updated $\variationalprob(\cusmatrix{\Processfunction}^{(\numprocess)})$ by looping over the observed nodes helps node-wise training converge to a global minimum. This is not the case when $\variationalprob(\cusmatrix{\Processfunction}^{(\numprocess)})$ is calculated only once outside the loop over nodes. Algorithm~\ref{alg:pogpn_node-wise} explains the node-wise coordinate ascent method.