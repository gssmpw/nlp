Gaussian Process Networks (GPN) coined by~\cite{friedman2000gpn} and extended by \cite{giudice2024bayesian} addresses the learning the Bayesian network structure and not the inference, which is different from our work. Gaussian Process Regression Networks (GPRN) by~\cite{friedman2000gpn, wilson2011gaussian} provide a different perspective by combining modeling the final output as a linear combination of Gaussian processes (like neural network structure).
GPRN does not incorporate the intermediate observations and caters to a problem statement different from ours.

GPN introduced by~\cite{astudillo2021bayesian, aglietti2020causal} as the surrogate model would translate the toy process network shown in Figure~\ref{fig:multi_process} into the DAG shown in Figure~\ref{fig:toy_process_gpn}. The grey-shaded nodes represent the observed outputs, and the unshaded nodes represent the unobserved latent outputs. The so far introduced GPNs model each subprocess $\process^{(\numprocess)}$ as a Gaussian process with mean $\gpmean^{(\numprocess)}(\cdot)$ and variance $\gpkernel^{(\numprocess)}(\cdot, \cdot')$, where $\processfunction^{(\numprocess)}(\cdot)\sim\mathcal{GP}^{(\numprocess)}(\gpmean^{(\numprocess)}(\cdot),\gpkernel^{(\numprocess)}(\cdot, \cdot'))$ and the observation(s) of the subprocess $\process^{(\numprocess)}$ can be expressed using the respective Gaussian likelihood as $\probability(\obsouts^{\numprocess}\vert\processfunction^{(\numprocess)})$. The model assumes that each node GP is independent of the other GP given the observed input-output pairs and uses closed-form marginal log-likelihood (MLL)~\citep{rasmussen2003gaussian} for inference.
% Since a GP provides a distribution as output(s) rather than point prediction(s), 
MC samples are used to estimate the final output during prediction. The implemented setup poses four major limitations:
\begin{enumerate}
      \item In most real-world cases, one can only observe the state space partially using an indirect observation lens. The latent outputs of the subprocesses are often hidden. Because of this, it can be considered that it is the latent/true outputs $\cusvector{\trueouts}^{(\numprocess)}$ that influence the process and not the indirect observations $\cusvector{\obsouts}^{(\numprocess)}$. Figure~\ref{fig:toy_process_gpn} represents that the indirect observations become the input to the respective child node. It holds only for direct noise-free observations.
      \item Using GP, the output of a parent node GP is a distribution, not a point value. Due to this reason, closed-form MLL cannot be used. Closed-form MLL can be used for deterministic inputs. Also, the prediction contradicts the training as MC samples are used to calculate from predictive distribution.
      \item The use of exact MLL also limits the usage to only Gaussian observation likelihood. Although this has computational benefits, it cannot be applied to cases where the intermediate observations are observed using a non-Gaussian lens.
            % In addition, current GPNs do not allow for amortized likelihood in the case of high-dimensional intermediate observations.
      \item Using the inference method of existing GPN, one can only condition a particular node when observed. However, since the model is a network, one should be able to condition the connected subprocesses based on a particular subprocess observation.
\end{enumerate}

The models, demonstrated in~\cite{sussex2022model, kusakawa2022bayesian}, show promising results by overcoming the first limitation. However, they still rely on closed-form MLL for independent node inference, leaving the other limitations unaddressed. The models of~\cite{aglietti2020causal, sussex2022model, astudillo2021bayesian} were primarily proposed as a surrogate model for Bayesian optimization with intermediate observations, hinting at the potential for further development and improvement.

Another recently proposed variant of GPN is the Gaussian Process Autoregressive Regression model~\citep{requeima2019gaussian}, where the focus is on autoregressive modeling of each observed output. The prediction(s) of the previous output(s) are used as input(s) for the GP, which is further down the autoregressive flow. The outputs are ordered greedily, using an exhaustive search, but scalability is not well discussed. Although GPAR mentions the use of inducing points in D-GPAR-NL, it does not provide a training method for joint distribution loss and either assume fixed inducing locations or individual node GP training. Additionally, GPAR does not cater to the second and third limitations or provide an optimization method for inducing point formulation.