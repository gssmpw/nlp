We consider a stochastic process $\Process$ comprised of subprocesses, $\{\process^{(\numprocess)}\}$ for $\numprocess\in\numprocessset$, (which can be stochastic) as shown in Figure~\ref{fig:multi_process}. The process of stochasticity can come from either a lack of knowledge of the process or other hidden influences or random noise $\cusvector{\tilde{\noise}}^{(\numprocess)}\sim\probability(\cusvector{\noise}^{(\numprocess)})$. Process $\Process$ is a DAG where the nodes represent the subprocesses. Each subprocess $\process^{(\numprocess)}$ is governed by a transformation function $\cusvector{\processfunction}^{(\numprocess)}$ which takes as input(s) some adjustable parameters $\cusvector{\params}^{(\numprocess)}$ (represented with green arrow in Figure~\ref{fig:multi_process}) and the output(s) of parent subprocesses $\process^{\nodeparent{\numprocess}}$, where $\nodeparent{\numprocess}$ represents the direct parents of $\numprocess$. Using the transformation function, the contactenated input(s) $(\cusvector{\params}^{(\numprocess)}, \process^{\nodeparent{\numprocess}})$ are transformed into output(s) $\cusvector{\trueouts}^{(\numprocess)}$ (represented with red arrow).

Figure~\ref{fig:multi_process} shows an example stochastic process with two subprocesses that can be expressed using a distribution over the function space, which the transformation function $\cusvector{\processfunction}^{(\numprocess)}$ is a sampled from. Using this definition, the outputs of the two-process system shown in Figure~\ref{fig:multi_process} as red arrows can be redefined as
% \begin{equation*}
%       \cusvector{\trueouts}^{(1)} \sim \probability\big(\cusvector{\processfunction}^{(1)}\vert\cusvector{\params}^{(1)}\big) ;\;\;\cusvector{\trueouts}^{(2)} \sim \probability\big(\cusvector{\processfunction}^{(2)}\vert\cusvector{\params}^{(2)}, \cusvector{\processfunction}^{(1)}\big).
% \end{equation*}
\begin{equation*}
      \cusvector{\trueouts}^{(\numprocess)} \sim \probability\big(\cusvector{\processfunction}^{(\numprocess)}\vert\cusvector{\params}^{(\numprocess)}, \cusvector{\processfunction}^{(\nodeparent{\numprocess})}\big), \forall \numprocess\in\{1, 2\}.
\end{equation*}
In most cases, the output(s) $\cusvector{\trueouts}^{(\numprocess)}$ of a subprocess $\process^{(\numprocess)}$ cannot be fully observed and are observed indirectly/partially using an "observation lens" (represented with blue arrow) as $\cusvector{\tilde{\obsouts}}^{(\numprocess)}$, hence the name partially observable process network. An example of such an observation lens can be the Gaussian observation noise of a sensor. In probabilistic modeling, the "observation lens" is often modeled as the likelihood function $\cusvector{\tilde{\obsouts}}^{(\numprocess)}\sim\probability(\cusvector{\obsouts}^{(\numprocess)}\vert\cusvector{\processfunction}^{(\numprocess)})$, which could be as simple as additive Gaussian noise or something more complex. The true output(s) $\cusvector{\trueouts}^{(\numprocess)}$ remain latent. We assume that, the observation lens is always present whenever an output is referred to as "observed" unless stated as "latent" or "true" output. The "latent" output of a parent subprocess becomes the input for a child subprocess.

At this point we are able to define each subprocess $\process^{(\numprocess)}$ using a tuple $\langle\probability\big(\cusvector{\processfunction}^{(\numprocess)}\vert\cusvector{\params}^{(\numprocess)}, \cusvector{\trueouts}^{\nodeparent{\numprocess}}\big), \probability(\cusvector{\obsouts}^{(\numprocess)}\vert\cusvector{\processfunction}^{(\numprocess)})\rangle$, where $\cusvector{\params}^{(\numprocess)}$ represents the adjustable input parameters, $\cusvector{\trueouts}^{\nodeparent{\numprocess}}$ represents the latent output(s) from the parent subprocess(es), $\probability\big(\cusvector{\processfunction}^{(\numprocess)}\vert\cusvector{\params}^{(\numprocess)}, \cusvector{\trueouts}^{\nodeparent{\numprocess}}\big)$ represents the probability distribution over the transformation function and $\probability(\cusvector{\obsouts}^{(\numprocess)}\vert\cusvector{\processfunction}^{(\numprocess)})$ represents the observation likelihood or lens with which the latent output of the subprocess $\process^{(\numprocess)}$ is observed.

The process network $\Process$ can be represented using a DAG, $\graph$. The nodes are topologically ordered such that $\numprocess'<\numprocess ,\forall \numprocess' \in \nodeparent{\numprocess}$ for all $\numprocess\in\numprocessset$. We use the terms subprocess and node interchangeably to represent a subprocess in $\graph$. $\cusvector{\params}^{(\numprocess)}$ are addressed as adjustable input nodes or parameters. The final/end process of the process $\Process$ is defined as the subprocess(es) $\process^{(\numprocess)}$ for which there are no child nodes. The corresponding observed output(s) $\cusvector{\tilde{\obsouts}}^{(\numprocess)}$ are called the observed final output(s).

The problem statement is to model the subprocess output(s), and the end process output using the adjustable inputs of different subprocess(es) and all indirect observations of the process network made using different observation likelihoods. We assume that the data generation DAG or the causal path is known. We refrain from augmenting the intermediate observations with adjustable inputs to avoid blowing up the input dimensionality for the used model. We discuss the proposed solution in section~\ref{sec:pogpn}.