\appendix
\section{Experimental setup of Jura dataset}
We make a POGPN, as shown in Fig.~\ref{fig:jura_dag_appendix}, that can also use the categorical observations to predict the final output. The values are log standardized before modeling and then transformed back for evaluation.

\begin{figure}[h]
      \centering
      \input{figures/jura_dag_appendix.tex}
      % \vspace{-1em}
      \caption{Detailed structure of POGPN with root node location "Loc", softmax likelihoods for "Rock" and "Land" and multi-task Gaussian likelihood for minerals. Gray nodes represent observed nodes, and white nodes represent latent nodes.}
      \vspace{-1em}
      \label{fig:jura_dag_appendix}
\end{figure}

Latent functions $\cusvector{\processfunction}^{(\text{Rock})}$ and $\cusvector{\processfunction}^{(\text{Land})}$ are vector-valued GP of size two where the LMC kernel is used. $\cusvector{\processfunction}^{(\text{Ni, Zn, Cd})}$ is a vector-valued GP of size three where the LMC kernel is used. As mentioned in the main document, softmax likelihood is used for the "Rock" and "Land" nodes, and multi-task Gaussian likelihood is used for "Ni, Zn, Cd". The number of inducing locations is initialized as the fully observed 259 input locations. $\beta=2.5$ for the loss functions PLL and ELBO. For the fully observed dataset where "Rock" type, "Land" type, and "Zn, Ni, Cd" are observed for 156 locations, the number of epochs = 200. For the rest of the partially observed dataset of only "Rock" type, "Land" type, and "Zn, Ni," the number of epochs = 50. A squared exponential kernel and constant mean were used for all the kernels. For the test locations, the predicted multi-variate normal is conditioned on the known locations of Zn and Ni to get a normal distribution of Cd. We use ADAM~\citep{kingma2014adam} optimizer from PyTorch, with a learning rate of 0.01, to optimize the loss function.

\begin{table}[h]
      \centering
      % Constrain the table width to fit within a single column
      \begin{tabular}{@{}c@{}}
            % First sub-table
            \begin{tabular}{@{\hskip 0pt}lcccc@{\hskip 0pt}}
                  \toprule
                  Model & IGP\textsuperscript{\textdagger} & SLFM\textsuperscript{\textdagger} & GPRN\textsuperscript{\textdagger} & D-GPAR-NL\textsuperscript{\textdagger} \\
                  \midrule
                  MAE   & 0.5753                           & 0.4145                            & 0.4040                            & 0.3996                                 \\
                  \bottomrule
            \end{tabular}
            \\[2em] % Add vertical spacing between sub-tables

            % Secondsub-table
            \begin{tabular}{@{\hskip 0pt}lccc@{\hskip 0pt}}
                  \toprule
                  Model & POGPN-AL\textsuperscript{\textdaggerdbl} & POGPN-NL\textsuperscript{\textdaggerdbl} & POGPN-AL\textsuperscript{\textasteriskcentered} \\
                  \midrule
                  MAE   & 0.3991$\pm$0.003
                        & \textbf{0.3989}$\pm$\textbf{0.002}       & 0.5035$\pm$0.0012                                                                          \\
                  \bottomrule
            \end{tabular}
      \end{tabular}
      \caption{Prediction results for Jura dataset. Mean absolute error (MAE) (lower is better). Models marked with \textsuperscript{\textdagger} indicate cited results from~\cite{requeima2019gaussian}. POGPN-AL\textsuperscript{\textdaggerdbl} and POGPN-NL\textsuperscript{\textdaggerdbl} are calculated using PLL. POGPN-AL\textsuperscript{\textasteriskcentered} is calculated using ELBO.}
      \vspace{-1em}
\end{table}

\section{Experimental setup of EEG dataset}

\begin{figure}[h]
      \centering
      \input{figures/eeg_dag_appendix.tex}
      % \vspace{-1em}
      \caption{Detailed structure of POGPN with root node time, and multi-task Gaussian likelihoods for "F3, F4, F5 and F6" and "F1, F2 and FZ". Gray nodes represent observed nodes, and white nodes represent latent nodes.}
      % \vspace{-1em}
      \label{fig:eeg_dag_appendix}
\end{figure}

We make a POGPN as shown in Figure~\ref{fig:eeg_dag_appendix}, where the values are standardized before training and then back-standardized before prediction. The number of inducing locations for the "Time" node is initialized as the 256-time steps and is kept non-learnable. For the fully observed dataset where F1, F2, F3, F4, F5, F6 and FZ are observed for 156 timesteps, number of epochs = 300 and for the partially observed dataset of only F3, F4, F5 and F6 number of epochs = 150. For all kernels, squared exponential kernel and constant mean have been used, and the number of MC samples = 20. Latent functions $\cusvector{\processfunction}^{(\text{F3, F4, F5, F6})}$ and $\cusvector{\processfunction}^{(\text{F1, F2, FZ})}$ are four and three dimensional respectively. For each of the latent functions, the likelihood is multi-task Gaussian likelihood. We use ADAM~\citep{kingma2014adam} optimizer from PyTorch, with a learning rate of 0.02, to optimize the loss function. The detailed prediction results for the EEG dataset are explained in Table~\ref{tb:eeg_results_extended}.

\begin{table}[h]
      % Reduce the default column padding for the enclosed tables.
      \setlength{\tabcolsep}{4pt}
      \centering
      % Constrain the table width to fit within a single column
      \begin{tabular}{@{}c@{}}
            % First sub-table
            \begin{tabular}{@{\hskip 0pt}lcccc@{\hskip 0pt}}
                  \toprule
                  Model & IGP\textsuperscript{\textdagger} & SLFM\textsuperscript{\textdagger} & GPAR-NL\textsuperscript{\textdagger} & POGPN-AL (PLL) \\
                  \midrule
                  SMSE  &
                  1.75  &
                  1.06  &
                  0.26  &
                  \textbf{0.24}
                  $\pm$\textbf{0.016}
                  \\
                  MLL   &
                  2.60  & 4.00                             & 1.63                              & 1.04
                  $\pm$0.11
                  \\
                  \bottomrule
            \end{tabular}
            \\[2em] % Add vertical spacing between sub-tables

            % Second sub-table
            \begin{tabular}{@{\hskip 0pt}lccc@{\hskip 0pt}}
                  \toprule
                  Model & POGPN-NL (PLL)                  & POGPN-AL (ELBO) & POGPN-NL (ELBO) \\
                  \midrule
                  SMSE  & 0.28$\pm$0.02                   & 0.31            & 0.34            \\
                  MLL   & \textbf{0.18}$\pm$\textbf{0.05} & 1.40            & 0.354           \\
                  \bottomrule
            \end{tabular}
      \end{tabular}
      \caption{Prediction results for EEG dataset of different models. Standardized mean squared error (SMSE) and Mean Log Loss (MLL)~\cite{rasmussen2003gaussian} comparison (lower is better). Models marked with \textsuperscript{\textdagger} indicate cited results from~\cite{requeima2019gaussian}.}\label{tb:eeg_results_extended}
\end{table}