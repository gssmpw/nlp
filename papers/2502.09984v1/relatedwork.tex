\section{Previous works}
\label{sec:pre}
\subsection{Notation}

In this paper, $\Lambda(A)$ represents the spectrum (the set of eigenvalues) and $\Sigma(A)$ is the set of singular values of $A\in\mathbb{C}^{n\times n}$.
Suppose that $\lambda_i(A)\in\Lambda(A)$ and $\sigma_i(A)\in\Sigma(A)$ are ordered such that $|\lambda_1(A)|\leq\dots\leq |\lambda_n(A)|$ and $\sigma_1(A)\geq\dots\geq \sigma_n(A)$.
We define
\begin{align}
    %\lambda_{\max}(A):=\max(|\lambda_1(A)|,|\lambda_n(A)|),\quad \lambda_{\min}(A):=\min\{|\lambda|\ |\ \lambda\in\Lambda(A)|\},
    \lambda_{\max}(A):=\max\{|\lambda|\ |\ \lambda\in\Lambda(A)|\},\quad \lambda_{\min}(A):=\min\{|\lambda|\ |\ \lambda\in\Lambda(A)|\},\label{eq:lamminmax}
\end{align}
and
\begin{align}
    \sigma_{\max}(A):=\sigma_1(A),\quad \sigma_{\min}(A):=\sigma_{n}(A).\label{eq:sigminmax}
\end{align}
Unless otherwise stated, the norm $\|\cdot\|$ for a vector or a matrix denotes the spectral norm $\|\cdot\|_2$.
$\kappa_2(A)$ for a nonsingular matrix $A$ denotes the condition number of $A$ such that $\kappa_2(A)=\|A\|\cdot\|A^{-1}\|$.
The number of positive, negative, and zero eigenvalues is denoted by $\mathrm{npe}(A)$, $\mathrm{nne}(A)$, and $\mathrm{nze}(A)$, respectively.
We write $A\succ 0$ ($A\succeq 0$) to denote that a square Hermitian matrix $A$ is positive (semi-) definite.

Let $\mathbb{IC}$ denote the set of complex intervals. In this paper, intervals are represented in boldface italics; for example, $\bm{A} \in \mathbb{IC}^{m \times n}$ denotes an interval matrix. An interval matrix is defined as follows:
\begin{align}
    \bm{A}=\{A\in\mathbb{C}^{m\times n}\ |\ \forall (i, j),\  a_{ij}\in\bm{a}_{ij}\in\mathbb{IC}\}.
\end{align}
For a real interval $\bm{a}$ containing $a\in\bm a$, the infimum $\mathrm{inf}(\bm{a})$ and supremum $\mathrm{sup}(\bm{a})$ are defined such that
\begin{align}
    \mathrm{inf}(\bm{a})\leq  a\leq\mathrm{sup}(\bm{a}),\quad \mathrm{inf}(\bm{a}), \mathrm{sup}(\bm{a})\in \bm{a}.
\end{align}
For a complex interval $\bm{a} \in \mathbb{IC}$, the infimum and supremum are denoted as:
\begin{align}
    \mathrm{inf}(\bm{a})&=\mathrm{inf}(\mathrm{Re}(\bm{a}))+\mathrm{inf}(\mathrm{Im}(\bm{a}))\cdot i,\\ 
    \mathrm{sup}(\bm{a})&=\mathrm{sup}(\mathrm{Re}(\bm{a}))+\mathrm{sup}(\mathrm{Im}(\bm{a}))\cdot i,
\end{align}
where $\mathrm{Re}(\cdot), \mathrm{Im}(\cdot)$, and $i$ are the real part, imaginary part, and imaginary unit, respectively.
Additionally, the midpoint $\mathrm{mid}(\bm{a})$ and radius $\mathrm{rad}(\bm{a})$ of an interval are given by
\begin{align}
    \mathrm{mid}(\bm{a}):=\frac{\mathrm{inf}(\bm{a})+\mathrm{sup}(\bm{a})}{2}\quad\text{and}\quad \mathrm{rad}(\bm{a}):=\frac{|\mathrm{sup}(\bm{a})-\mathrm{inf}(\bm{a})|}{2}.
\end{align}
Additional discussion is required for machine-interval operations but is omitted from this paper (for details, see \cite{moore2009introduction}).

\subsection{Estimation of singular values and the matrix norm}

Assume that the given matrix $A$ is nonsingular.
It is known that $\|A\|=\sigma_{\max}(A)$ and $\|A^{-1}\|=\sigma_{\min}(A)^{-1}$.
For the summation and product of matrices $A$ and $E$,
\begin{align}
    \sigma_i(A)-\|E\|\leq \sigma_i(A+E)\leq \sigma_i(A)+\|E\|\label{eq:sum}\\
    \sigma_i(A)\sigma_{\min}(E)\leq \sigma_i(AE)\leq \sigma_i(A)\|E\|\label{eq:prod}
\end{align}
are satisfied~\cite[Theorem 3.3.16]{horn1994topics}.
Similarly, it follows 
\begin{align}
    \sigma_{\min}(A)\sigma_{i}(E)\leq \sigma_i(AE)\leq \|A\|\sigma_{i}(E).\label{eq:prod2}
\end{align}
Next, we consider the estimations for approximate unitary matrix $X$.
Assume that $X^HX=I+F$, where $I$ is the identity matrix and $\|F\|\leq \epsilon<1$.
Then
\begin{align}
    \sqrt{1-\epsilon}\leq \sigma_{\min}(X),\quad \|X\|\leq \sqrt{1+\epsilon}
\end{align}
are satisfied~\cite{rump2011verified}.
Similarly, for a Hermitian positive definite $B\in\mathbb{C}^{n\times n}$, assume that $X_B$ satisfies $X_B^HBX_B=I+F$ and $R$ is the Cholesky factor as $B=R^HR$.
Then, if $\|F\|\leq \epsilon<1$,
\begin{align}
    \sqrt{1-\epsilon}\leq \sigma_{\min}(RX_B),\quad \|RX_B\|\leq \sqrt{1+\epsilon}\label{eq:borth}
\end{align}
are satisfied~\cite{miyajima2010fast}.

The spectral norms $\|A\|$ and $\||A|\|$ for Hermitian matrix $A$ can be bounded by the infinity norm $\|A\|_{\infty}$, i.e., 
let $e$ be the ones vector as $e=(1,1,\dots,1)^T$, then it follows
\begin{align}
    \|A\|\leq \||A|\|\leq \|A\|_{\infty}=\max_i(|A|e)_i.
\end{align}
Define $\mathrm{mag}(\bm A)=|\mathrm{mid}(\bm A)|+\mathrm{rad}(\bm A)$.
Then, for any Hermitian matrix $A\in\bm A$, it follows
\begin{align}
    \|A\|\leq \|\mathrm{mag}(\bm A)\|\leq \|\mathrm{mag}(\bm A)\|_{\infty}=\max_i(\mathrm{mag}(\bm A)e)_i.
\end{align}

For a general matrix $A$, because
\begin{align}
    \|A\|\leq \||A|\|=\sqrt{\||A|^T|A|\|},
\end{align}
it holds that
\begin{align}
    \|A\|_2&\leq \max_{i}\sqrt{(|A|^T(|A|e))_i}\\
    &\leq\max_i(\mathrm{mag}(\bm A^T)\cdot(\mathrm{mag}(\bm A)e))_i.
\end{align}


\subsection{Estimation of eigenvalues}

It is well-established that for any matrix $A \in \mathbb{C}^{n \times n}$:
\begin{align}
\lambda_{\min}(A) \geq \sigma_{\min}(A),\quad \lambda_{\max}(A) \leq \sigma_{\max}(A)\label{eq:eig1}
\end{align}
from \cite[Theorem~5.6.9]{horn2012matrix}, where the notation was introduced in \eqref{eq:lamminmax} and \eqref{eq:sigminmax}, and it follows
\begin{align}
\lambda_{i}(AB) = \lambda_{i}(BA)\label{eq:eig2}
\end{align}
for all $i$~\cite[p. 55]{horn2012matrix}. 
These relationships are fundamental to understanding the interplay between the eigenvalues and singular values of matrices.
Assume symmetric matrix $A$, nonzero vector $v$, and scalar $\alpha$. 
For the cases $\alpha>0$, $\alpha<0$, and $\alpha=0$, it follows
\begin{align}
    \begin{cases}
        \lambda_i(A)<\lambda_i(A+\alpha vv^H), & \alpha>0,\\
        \lambda_i(A)>\lambda_i(A+\alpha vv^H), & \alpha<0,\\
        \lambda_i(A)=\lambda_i(A+\alpha vv^H), & \alpha=0
    \end{cases}
\end{align}
for all $i$~\cite{golub1973some,bunch1978rank}, respectively.
From this, for symmetric positive definite matrix $B$ and scalar $\beta>0$,
\begin{align}
    \lambda_i(A)<\lambda_i(A+\beta B)=\lambda_i\left(A+\beta\sum_j\lambda_j(B)x_jx_j^H\right)
\end{align}
is satisfied, where $x_j$ is the eigenvector corresponding to $\lambda_j(B)$.

For the special case in which $A$ is Hermitian and $B$ is Hermite positive definite, Rump proposed a verification method for the minimum eigenvalue of $B^{-1}A$~\cite{rump2011verified}.
For $n$-by-$n$ Hermitian matrices $A$ and $0\prec B$, it holds that
\begin{align}
    \beta>0,\quad A^2-\beta B^2\succ 0 \quad \Longrightarrow \quad  \lambda_{\min}(B^{-1}A)^2\geq \beta.\label{eq:mineig2}
\end{align}
In particular,
\begin{align}
    0\prec A,\quad \beta>0,\quad A-\beta B\succ 0 \quad \Longrightarrow \quad  \lambda_{\min}(B^{-1}A)\geq \beta.\label{eq:mineig}
\end{align}
Using the methodologies and relationships previously discussed, we can estimate a lower bound for the smallest eigenvalue, which corresponds to the minimum singular value. The focus of this paper is on a general square matrix $A$. We intend to systematically apply these evaluations to understand and characterize the spectral properties of such matrices.




\subsection{Previous work regarding the issue}

Here, we consider interval matrices $\bm{A}$ and Hermitian $\bm{B}$.
We introduce the previous work to obtain a rigorous enclosure of $\sigma_{\min}:=\sigma_{\min}(\bm{R}^{-H}\bm{A}\bm{R}^{-1})$ in~\cite[p. 440]{nakao2019numerical}, where $\bm{R}$ is the interval Cholesky factor of $\bm{B}$.
Here, $\bm{R}$ is identified as the interval Cholesky factor of the matrix $\bm{B}$. For the implementation of Algorithm~\ref{alg:prework}, we employ MATLAB/INTLAB \cite{Ru99a}, a computational environment that facilitates interval arithmetic operations. Within this framework, the matrices $\bm{A}$ and $\bm{B}$ are represented using the {\tt intval} class provided by INTLAB, ensuring that our calculations adhere to the principles of interval arithmetic for rigorous numerical analysis.

\begin{algorithm}
\caption{Previous work  to obtain a rigorous enclosure of $\|RA^{-1}R^{H}\|$.}\label{alg:prework}
\begin{algorithmic}
\STATE{$\bm{R}=\mathrm{verchol}(\bm{B})$;\hfill Interval Cholesky decomposition for $B$~\cite{alefeld1993cholesky}}
\STATE{$\rho=\mathrm{norm}(\bm{R}\bm{A}^{-1}\bm{R}^{H})$;}
\end{algorithmic}
\end{algorithm}

Note that all operations must be carried out strictly.
A key function is $\mathrm{verchol}$, which computes the interval Cholesky factor of the matrix $\bm{B}$, as described in the works of Alefeld et al. \cite{alefeld1993cholesky,alefeld2009new}.
To compute the matrix $\bm{R}\bm A^{-1}\bm{R}^{H}$, we employ verified numerical computation techniques. 
Initially, we solve the linear equation $\bm{S}\supset \bm A^{-1}\bm{R}^{H}$ using the methods outlined by Rump in \cite{rump2013accurate1,rump2013accurate2}. Subsequently, the product $\bm{R}\bm{S}$ is calculated through interval matrix multiplication, following the efficient algorithms proposed in \cite{oishi2002fast,rump2012fast,ogita2005fast,ozaki2012fast}.
The $\mathrm{norm}$ function for an interval matrix returns the rigorous enclosure of the spectral norm~\cite{rump2011verified}.