%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[acmsmall]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2025}
\acmYear{2025}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
%\acmConference[Communications of the ACM]
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
%\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Influence Operations in Social Networks}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Javier Pastor-Galindo}
%\email{javier.pastor.galindo@upm.es}
\affiliation{%
  \institution{Computer Systems Engineering Department, Universidad Politecnica de Madrid}
  \city{Madrid}
  \country{Spain}
}

\author{Pantaleone Nespoli}
%\email{pantaleone.nespoli@um.es}
\author{José A. Ruipérez-Valiente}
%\email{jruiperez@um.es}
\affiliation{%
  \institution{Department of Information and Communications Engineering, University of Murcia}
  \city{Murcia}
  %\state{Murcia}
  \country{Spain}
}

\author{David Camacho}
%\email{david.camacho@upm.es}
\affiliation{%
  \institution{Computer Systems Engineering Department, Universidad Politecnica de Madrid}
  \city{Madrid}
  \country{Spain}
}






%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
%\renewcommand{\shortauthors}{Trovato et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
An important part of online activities are intended to control the public opinion and behavior, being considered currently a global threat. This article identifies and conceptualizes seven online strategies employed in social media influence operations. These procedures are quantified through the analysis of 80 incidents of foreign information manipulation and interference (FIMI), estimating their real-world usage and combination. Finally, we suggest future directions for research on influence operations.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10002978.10003022.10003027</concept_id>
       <concept_desc>Security and privacy~Social network security and privacy</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Security and privacy~Social network security and privacy}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{social networks, disinformation, misinformation}

%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
%\begin{teaserfigure}
%  \includegraphics[width=\textwidth]{sampleteaser}
%  \caption{Seattle Mariners at Spring Training, 2010.}
%  \Description{Enjoying the baseball game from the third-base
%  seats. Ichiro Suzuki preparing to bat.}
%  \label{fig:teaser}
%\end{teaserfigure}

\received{20 February 2007}
\received[revised]{12 March 2009}
\received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{THE INFLUENCE CYBER BATTLEGROUND}
The rise of social media and other online communication channels has provided unprecedented reach and efficiency for intentional influence campaigns, having an important escalation from Russia's interference in the 2016 U.S. election. Governments are worried about these risks and social platforms seriously fight against the so-called coordinated inauthentic behavior (CIB) ~\cite{PASTORGALINDO2022161}.

In 2023, the landscape of influence operations saw a significant rise in detected Foreign Information Manipulation and Interference (FIMI) incidents, with the European External Action Service (EEAS) report~\cite{EuropeanUnionExternalAction2024} identifying 750 cases between December 1, 2022, and November 30, 2023, nearly doubling the previous year's figures. FIMI attacks were globally distributed, targeting 53 countries, with Ukraine, the United States and several European nations being the most affected. The content dissemination involved the coordinated use of websites and social media to create illusions of authentic discourse and obscure origins, with cross-platform reach as a standard practice. Unfortunately, this global threat is expected to be massively automated with generative artificial intelligence, as stated by OpenAI~\cite{Goldstein2023}, Microsoft~\cite{Microsoft2024} or Google~\cite{Google2025}.

Consequently, the study of online influence operations has grown in the last few years~\cite{gabriel2023inductive}. Existing case studies have focused on characterizing disinformation methods, inferring strategies from observed behaviors, assessing political impacts, analyzing interactions or identifying users likely to spread deceptive content~\cite{PASTORGALINDO2022161}, among others. Particularly, there is a clear tendency towards proposing unsupervised and supervised machine learning methods for detection and analysis. However, the former are limited in scalability and timeliness, while the latter do not fully address variability and detectability over time and across campaigns~\cite{Alizadeh2020}. That is why the conclusions reported in the literature usually have a partial, biased, or limited understanding of the problem~\cite{Budak2024}. 

In this context, apart from existing applied research and empirical studies, more analytical research is needed to abstract the concept of an influence operation, set academic foundations and homogenize the vision of this global challenge~\cite{9451574}. Particularly, the interpretation of influence operations stands multidisciplinary, with several dimensions coming into play such as psychology, sociology, political science, communication studies or computer science. That is why there is little consensus on how to best describe and address influence operations~\cite{blazek2021scotch}. In this line, and recognizing that systematic modeling of influence operations is far from established, particularly on social networks, we propose seven broader influence strategies that allow for a unified and structured analysis of campaigns across social platforms.


\section{INFLUENCE OPERATIONS: TOWARDS A COMMON UNDERSTANDING}

Influence operations are deliberate activities performed by threat actors to sway public opinion and manipulate decision-making globally~\cite{Budak2024}. Their analysis is crucial for interpretation, standardized knowledge, and shareable intelligence, often guided by models adopted by agencies, organizations, and researchers~\cite{canovas2024analyzing}.

Several frameworks can aid in structuring influence operation analysis. The \textit{ALERT} taxonomy~\cite{DESOUZA2020101606} categorizes political disruption tactics, while the \textit{ABCDE} framework~\cite{pamment2020eu} assists EU institutions and platforms in identifying key operational components. The Carnegie Mellon \textit{BEND} framework~\cite{carley2020social} details structural and narrative maneuvers, and the U.S. government’s \textit{SCOTCH} framework~\cite{blazek2021scotch} integrates technological, organizational, and human factors for adversarial operations. One promising perspective is the consideration of them as cyber operations, calling for efforts to understand the problem from the cybersecurity discipline. The Norwegian Defense Research Establishment adapted the \textit{cyber kill chain} for social media influence campaigns~\cite{bergh2020understanding}, mapping the phases from reconnaissance to impact. Researchers at NYU~\cite{mirza2023tactics} developed a similar framework based on interviews with fact-checkers and analysts, characterizing threat actors, attack patterns, channels, and targets. More rigorous proposals have been proposed by the Misinfosec Working Group’s \textit{AMITT} framework~\cite{Walker2019}, later enhanced by MITRE and Florida International University into \textit{SP!CE}, extending MITRE ATT\&CK to influence campaigns. In 2022, these merged into the \textit{DISARM framework}~\cite{terp2022disarm}, now supported by the DISARM Foundation. While \textit{SP!CE} remains used by the U.S. Department of Defense, MITRE continues collaborating with \textit{DISARM} to counter cyber-enabled influence operations~\cite{Sixto2023}. Finally, Meta researchers introduced the \textit{Online Operations Kill Chain}~\cite{nimmo2023phase}, detailing ten steps used to analyze takedowns of influence campaigns on Facebook. 

While the reviewed frameworks effectively characterize general online operations, they lack a clear focus on understanding adversary campaigns on social media. Nevertheless, they provide a foundation to propose key strategies of influence operations in social networks. In particular, the \textit{DISARM} framework~\cite{terp2022disarm} is one of the most comprehensive and up-to-date alternatives, being currently employed in several projects (e.g., Attribution Data Analysis Countermeasures Interoperability~\cite{adacdisarm}, ATHENA~\cite{athenea} or the European Digital Media Observatory~\cite{edmoelections}). It has also been adopted by important organizations (such as EU DisinfoLab~\cite{disinfolabeufimi}, the European Center of Excellence for Countering Hybrid Threats (Hybrid CoE)~\cite{HybridCoE2022}, the European Union Agency for Cybersecurity (ENISA)~\cite{enisathreat}, the European External Action Service (EEAS)~\cite{EuropeanUnionExternalAction2024} or the Alliance for Securing Democracy~\cite{reportasd}), demonstrating its practical utility and reliability. The \textit{DISARM framework}\footnote{\url{https://disarmframework.herokuapp.com/}} follows the philosophy of the MITRE ATT\&CK matrix\footnote{\url{https://attack.mitre.org/}} and considers that influence operations are structured hierarchically into phases, tactics, and techniques. It defines the following four phases: 1) \textit{plan} (set clear goals and strategies), 2) \textit{prepare} (assemble resources for readiness, including people, networks, channels and content), 3) \textit{execute} (deploy the operation from start to finish, maintaining presence as needed), and 4) \textit{assess} (review the outcome to improve future maneuvers). As detailed in Table~\ref{tab:disarm}, each phase aggregates a set of tactics. Finally, each tactic can be achieved through specific fine-grained actions, called techniques. There are more than one hundred predefined techniques under the sixteen tactics, which can be explored in detail through the DISARM Framework Explorer\footnote{https://disarmframework.herokuapp.com/technique/}. For instance, in the phase of \textit{execute}, the tactic \textit{deliver content} includes techniques like \textit{deliver ads}, \textit{post content}, \textit{comment or reply on content}, and \textit{attract traditional media}. 

The resulting matrix is a robust tool for modeling and evaluating various types of influence operations in social networks.  Among other benefits, this cybersecurity-inspered modeling facilitates the understanding of adversary strategies, analyzing indicators of compromise (IOC), building tailored reactions to the precategorized techniques, increasing cooperative and collective resilience, and countering complex problems such as hybrid threats, advanced persistent threats (APT), or the last tailored term of advanced persistent manipulators (APM)~\cite{EuropeanUnionExternalAction2024,Microsoft2024}. 

\begin{table}[t!]
\footnotesize
\begin{tabular}{cccc}
\toprule
\textbf{Plan} & \textbf{Prepare} & \textbf{Execute} & \textbf{Assess} \\ \midrule
Plan strategy & Develop narratives & Conduct pump priming & Assess effectiveness \\ 
Plan objectives & Develop content & Deliver content &  \\
Target audience analysis & Establish social assets & Maximize exposure &  \\ 
& Establish legitimacy & Drive online harms &  \\
& Microtarget & Drive offline activity &  \\
& Select channels and affordances & Persist in the information environment &  \\ \bottomrule
\end{tabular}
\caption{DISARM phases and associated tactics in adversary influence operations.}
\end{table}
\label{tab:disarm}

\section{INFLUENCE OPERATIONS STRATEGIES IN SOCIAL NETWORKS}

Strategy refers to high-level procedures aimed at achieving long-term objectives. In influence operations on social networks, strategy represents a calculated approach to manipulate beliefs, perceptions, and behaviors. To address the lack of clear definitions in the literature, we infer influence strategies from the \textit{DISARM framework} as follows:

\begin{enumerate}
\item \textit{Identify execution-phase DISARM techniques specific to social networks}. While the \textit{DISARM} phases of plan, prepare, and assess are medium-agnostic, execution techniques are tied to communication channels. We select and focus on seven techniques exploiting social networks as the primary attack vector, shown in the right part of Figure~\ref{fig:disarm_lite}: \textsf{(1) Post content}, \textsf{(2) Amplify Narratives}, \textsf{(3) Incentivize Sharing}, \textsf{(4) Comment or reply on content}, \textsf{(5) Deliver ads}, \textsf{(6) Harass}, and \textsf{(7) Flood the Information Space}.  

\item \textit{Map execution techniques to their predecessor techniques from previous phase}. This reverse mapping exposes the strategic decisions and resource allocation required for executing influence strategies. By traversing the DISARM matrix, we form disjoint groups of techniques to define decoupled strategies. Consequently, the planning and assessment phases are excluded due to their universal applicability, while certain preparation and execution tactics are omitted as they do not serve as clear preconditions for the seven execution techniques.

\item \textit{Define influence strategies from resulting pipelines}. The sequential flow from preparation to execution forms structured activity pipelines, outlining influence patterns of threat actors. Each colored pipeline is inductively named and defined as a high-level influence strategy.  

\end{enumerate}



\begin{figure*}[t!]
    \centering
    \includegraphics[width=\textwidth]{imgs/disarm_lite.pdf}
    \caption{Seven strategies based on DISARM techniques for influence operations in social networks}
    \label{fig:disarm_lite}
\end{figure*}

\subsubsection*{Narrative Release}
This strategy introduces a narrative into the social network that aligns with the intended influence objective. The fundamental technique is \textit{posting content}, which involves publishing original posts or multimedia directly on social platforms. These posts can be text updates, images, videos, or links. To enable the execution of this technique, adversaries must first develop narratives from scratch, develop the textual or multimedia content, and localize artifacts to ensure that messaging is culturally and linguistically adapted. For instance, an actor might post a publication on a Facebook group to shape public perception of a controversial issue. The goal is to establish an anchor point for influence, making the narrative recognizable and available for reinforcement, expansion, or manipulation~\cite{tyushka2022weaponizing}.

\subsubsection*{Narrative Support}
Once a narrative is introduced, this strategy strengthens its credibility and makes it more widely accepted. Adversaries rely on \textit{amplifying narratives} to make posts appear more popular than they naturally would be by leveraging existing narratives and reusing content. A common example is the forced posting or interaction with a narrative to increase reliability and engagement. The goal is to solidify public acceptance, making the narrative more resistant to scrutiny or opposing views~\cite{blumenstock2025migration}.

\subsubsection*{Narrative Amplification}
This strategy focuses on expanding the reach and virality of a narrative. By increasing exposure, the narrative appears more widespread and influential for a broader audience. Adversaries increase content spread by sharing and \textit{incentivizing sharing}, creating search artifacts that optimize content for visibility on search engines and social platforms while designing clickbait that exploits cognitive biases. For example, a well-crafted Instagram reel with provocative imagery and a catchy caption can encourage mass sharing. The goal is to raise its visibility, making it harder to ignore~\cite{hristakieva2022spread}.


\subsubsection*{Counter-Narrative Reaction}
Deployed in response to competing narratives, this strategy works to challenge, weaken, or redirect attention from opposing perspectives. Influence actors rely on \textit{commenting or replying on content} to shape discussions and control narratives. By strategically posting comments on public threads, they can reinforce messages, attack opposition, or create the illusion of widespread support. To execute this, adversaries develop competing narratives that counter opposition and distort facts to undermine credibility. This strategy is frequently used in YouTube comment sections, where adversaries flood replies with endorsements or criticism to sway public perception. The goal is to protect the dominant narrative, ensuring it remains the primary frame of reference~\cite{kaiser2022partisan}.


\subsubsection*{Narrative Manipulation}
Rather than simply reinforcing or amplifying a message, this strategy modifies, distorts, or falsifies existing narratives to reshape public perception. Adversaries use links, news articles, or social media promotions to redirect users to manipulated external content through \textit{delivering ads}. This strategy is supported by leveraging conspiracy narratives, creating inauthentic groups, and developing fake news sites as reference points for misleading claims. A Facebook ad campaign, for instance, might target users with deceptive links to fabricated news articles tailored to their biases. The goal is to redefine reality, aligning public perception with a preferred version of events~\cite{ren2022authoritarian}.

 
\subsubsection*{Target Degradation}
This strategy seeks to discredit, intimidate, or silence individuals or groups that challenge an influence operation. It diminishes their credibility with \textit{harassment}, weakening their influence, and discourages engagement to remove opposition from public debate. To prepare for such actions, adversaries integrate target vulnerabilities, gathering intelligence on their targets to personalize attacks. A common example is a Twitter harassment campaign targeting journalists or political figures to discredit or silence them. The goal is to reduce the ability of key figures to contest the dominant narrative~\cite{9451574}.

 
\subsubsection*{Information Pollution}
By overwhelming the information environment with excessive or conflicting content, this strategy creates confusion and distrust in the reliability of information sources. Adversaries can \textit{flood the information space}, not necessarily promoting a particular narrative but diluting clarity and credibility, flooding social networks with excessive content and making it difficult for users to discern credible information. This execution technique requires generating an excess of low-quality or misleading content and creating fake accounts to emit and amplify it. A typical example involves posting hundreds of tweets quickly to saturate a conversation and obscure opposing viewpoints. The goal is to increase uncertainty, leaving audiences more susceptible to influence~\cite{pan-etal-2023-risk}.

Overall, these strategies provide a generalized framework for understanding complex influence operations in the real world, being correlated with other studies~\cite{buchanan2021truth}.

\section{INFLUENCE OPERATIONS IN THE REAL-WORLD}
The literature on cross-analysis of influence operations using a standardized framework remains limited. However, over the past year, a dataset comprising 81 FIMI campaigns targeting elections and referendums from 2014 to 2024 has been manually tagged using DISARM techniques by the SIPA Institute of Global Politics (IGP)~\cite{FuldeHardy2024}, leveraging intelligence reports. By identifying the execution techniques employed in each incident, we can infer the overarching influence strategies conceptualized in this work.

\subsection{Prevalence of strategies in real influence operations}

Out of the 81 documented incidents, 80 could be mapped to the influence strategies proposed in this study. Most of them integrating more than one strategy. This indicates that our methodology effectively captures the critical techniques within the complex DISARM framework. Figure~\ref{fig:strategies_distribution} reveals the usage of strategies by adversaries to influence social networks audience.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{imgs/influence_strategies.pdf}
    \caption{Prevalence of influence strategies in past incidents from 2014 to 2024}
    \label{fig:strategies_distribution}
\end{figure*}

Among the identified strategies, \textit{Narrative Release} emerges as the most prevalent, present in 97.5\% of cases. This finding underscores the importance of content seeding as the foundation of influence campaigns, where adversaries introduce new narratives, fabricate stories, or selectively leak information to set the stage for broader manipulation efforts. Following closely, \textit{Narrative Manipulation} (66.2\%) and \textit{Information Pollution} (63.7\%) highlight the widespread use of content distortion. These strategies involve reshaping existing narratives, amplifying conspiracy theories, or overwhelming audiences with misleading or conflicting information. Their prevalence suggests that influence operations are not solely about creating falsehoods but also about controlling the information environment to erode trust and manipulate perceptions. \textit{Narrative Support}, observed in 48.8\% of cases, plays a crucial role in reinforcing and sustaining the visibility of manipulated content. Techniques such as coordinated engagement, automated amplification, and deceptive endorsements ensure that seeded narratives gain traction and appear credible. Similarly, \textit{Narrative Amplification} (42.5\%) extends the reach of influence campaigns by leveraging social media algorithms, influencers, and mass-sharing tactics to maximize exposure. Less common but equally significant, \textit{Counter-Narrative Reaction} (32.5\%) and \textit{Target Degradation} (30.0\%) represent more aggressive forms of influence operations.

These findings highlight two key takeaways. First, influence operations extend beyond narrative manipulation (e.g., disinformation), encompassing a broader manipulation of the information ecosystem. Second, real campaigns rarely rely on a single strategy but rather combine them to reinforce and amplify their impact, as inspected next.


\subsection{Dominance of multi-strategy influence operations}
\label{sec:multilayered}

The majority of influence actors (92.5\%, 74 out of 80 cases) rely on multi-strategy operations, rarely deploying a single approach in isolation. The most common tactic (33\%, 24 cases) involves orchestrating four strategies, balancing complexity and efficiency. More sophisticated adversaries execute five-strategy operations (20\%), while three-strategy combinations (19\%) remain a flexible option. Simpler two-strategy operations (15\%) are uncommon, and more intricate campaigns employing six or seven strategies (8\% and 5\%) are rare, likely due to coordination challenges.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{imgs/multristrategy_ops.pdf}
    \caption{Combination of influence strategies in past incidents from 2014 to 2024}
    \label{fig:strategies_combination}
\end{figure*}

From a total of 120 potential strategy combinations, about 30 distinct patterns emerge from the dataset, revealing preferred multi-strategy playbooks in Figure~\ref{fig:strategies_combination}. On the left, the co-occurrence network illustrates how influence actors interweave different strategies to enhance their impact. A tendency is observed among those who rely on \textit{Narrative Release} as a core strategy, frequently coupling it with \textit{Information Pollution} and \textit{Narrative Manipulation} to both introduce and distort narratives while overwhelming the information space. This structured approach appears in seven incidents. Additionally, some adversaries intensify their manipulation efforts by combining \textit{Information Pollution} and \textit{Narrative Manipulation}, flooding feeds with deceptive content while embedding misleading elements into ongoing discussions. Many further reinforce these efforts by deploying \textit{Narrative Amplification} for wide spread, as observed in 11 cases where all four strategies were used together. Other influence actors rely on \textit{Narrative Support} to strengthen their operations, aligning it with \textit{Narrative Release}, \textit{Narrative Amplification}, \textit{Information Pollution}, and \textit{Narrative Manipulation} to consolidate a coherent influence strategy, appearing together in six operations. While this graph provides insights into dominant playbooks, frequent pairings do not necessarily imply a strong interdependence between strategies.

On the right, the conditional probability network reveals how likely influence actors are to deploy one strategy given the presence of another, uncovering tactical dependencies in influence operations. Two primary trends emerge. First, adversaries who launch \textit{Narrative Release} often combine with \textit{Target Degradation}, \textit{Narrative Support}, and \textit{Counter-Narrative Reaction}, reinforcing their narratives by undermining opposing voices and injecting reactionary content. Second, those relying on \textit{Information Pollution}, \textit{Narrative Manipulation}, and \textit{Narrative Amplification} tend to saturate the information ecosystem with deceptive content, reinforcing misleading narratives through coordinated amplification. Notably, \textit{Narrative Manipulation} appears as a linchpin, frequently used regardless of the strategic combination.


\section{CONFRONTING DIGITAL MANIPULATION: STRATEGIC RESPONSES AND FUTURE DIRECTIONS}

The increasing automation and sophistication of influence operations are transforming them into a persistent digital threat, where adversaries (often APM) deploy coordinated tactics to manipulate narratives, erode trust, and exploit vulnerabilities across social networks~\cite{Fredheim2024}. This study has identified and categorized key influence strategies in social networks, grounded in the DISARM framework. We revealed the prevalence of those strategies through an analysis of 80 FIMI incidents. The co-occurrence and conditional probability perspectives provide complementary insights into how adversaries coordinate multiple strategies, where key narratives are not only introduced and manipulated but also strategically reinforced through amplification and support mechanisms. This analytical and structured approach based on observable strategies in social networks set the stage for a proactive response, integrating cybersecurity methodologies, intelligence-sharing, real-time situational awareness, and mitigation strategies~\cite{Walker2019}.

One of the most promising countermeasures is the integration of Cyber Threat Intelligence (CTI) into influence operation monitoring, allowing analysts to track adversarial tactics and coordinate rapid responses using established intelligence-sharing platforms such as MISP and OpenCTI~\cite{HybridCoE2022}. This structured approach treats disinformation as a cybersecurity threat, fostering real-time collaboration between governments, platforms, and security researchers. However, detection alone is insufficient, and achieving Cyber Situational Awareness (CSA) in social networks is vital to understanding the broader manipulation landscape~\cite{10553682}. By combining OSINT, SIGINT, and HUMINT, and applying AI-driven behavioral analytics, CSA frameworks can anticipate influence campaigns before they escalate, shifting from reactive defense to proactive intervention.

Beyond intelligence and monitoring, risk assessment and mitigation must evolve to counter systemic vulnerabilities~\cite{Walker2019}. Influence operations thrive on media manipulation and institutional weaknesses, making adaptive counter-narratives and targeted disruption strategies essential. The development of AI-driven realistic simulations provides an effective means to evaluate adversary influence, model adversarial behavior, and refine response tactics~\cite{10492674}. These controlled environments enable preemptive stress-testing of emerging disinformation strategies, ensuring that countermeasures are validated before real-world deployment.

Yet, technical solutions alone cannot neutralize influence operations. Education and cognitive resilience remain central to long-term defense. Initiatives like NATO’s InfoRange and Red Team–Blue Team exercises offer practical training in recognizing and countering manipulation tactics~\cite{Walker2019}. Widespread digital literacy programs are needed to prepare both practitioners and the public to critically assess online content~\cite{HybridCoE2022}.

As Generative AI accelerates the automation of information manipulation, the need for cross-disciplinary collaboration becomes more pressing~\cite{blazek2021scotch}. The DISARM framework, serving as a foundation for structuring adversarial influence operations in this work, must continuously adapt to new deception techniques, AI-powered strategies, and cross-platform operations. The future of influence operation mitigation will not rely on a single solution but on a resilient, adaptive ecosystem of intelligence, technology, and education. By embracing this holistic, multi-domain approach, researchers, policymakers, and cybersecurity experts can build more effective defenses against the next generation of digital manipulation threats.


\begin{acks}
This study was partially funded by: (a) the strategic project ``Development of Professionals and Researchers in Cybersecurity, Cyberdefense and Data Science (CDL-TALENTUM)" from i) the Spanish National Institute of Cybersecurity (INCIBE) and ii) by the Recovery, Transformation and Resilience Plan, Next Generation EU; (b) by a ``Juan de la Cierva'' Postdoctoral Fellowship (JDC2023-051658-I) funded by the i) Spanish Ministry of Science, Innovation and Universities (MCIU), ii) by the Spanish State Research Agency (AEI/10.13039/501100011033) and iii) by the European Social Fund Plus (FSE+); (c) by H2020 TMA-MSCA-DN TUAI project "Towards an Understanding of Artificial Intelligence via a transparent, open and explainable perspective" (HORIZON-MSCA-2023-DN-01-01, Grant agreement nº: 101168344); (d) by project PCI2022-134990-2 (MARTINI) of the CHISTERA IV Cofund 2021 program; (e) by EMIF managed by the Calouste Gulbenkian Foundation, in the project MuseAI; and by (f) Comunidad Autonoma de Madrid, and by CIRMA-CAM Project (TEC-2024/COM-404).
\end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{biblio}

\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
