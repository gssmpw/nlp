\subsection{Datasets}
We evaluate our methods using three publicly available datasets: TSI880 \cite{tsi_dataset}, ASI16 \cite{tsi_dataset}, and SKIPP'D \cite{skippd_dataset}. The TSI880 and ASI16 datasets, both collected from the NREL Solar Radiation Research Laboratory in Golden, Colorado, provide sky images captured every 10 minutes along with corresponding GHI values and auxiliary data such as air temperature and relative humidity and only differ in camera setup and sensors, with the ASI16 dataset capturing higher-resolution images. The SKIPP'D dataset, collected from Stanford University, consists of raw sky images captured every minute and PV power output data, prioritizing finer temporal granularity at the expense of image quality. For more details, refer to Appendix~\ref{sec:appendix_dataset_details}.

We utilize the TSI880 and ASI16 datasets to investigate the impact of camera setup at the same location. To explore location and task shifts, we use the SKIPP'D dataset to evaluate the performance of models trained on GHI data in predicting PV power output. The SKIPP'D dataset features lower-resolution images and lacks meteorological data, thereby presenting a more challenging task by limiting the contextual information typically leveraged by prior models \cite{wacv2022, talha2019}. To ensure the models learn from higher-quality, information-rich datasets, we train exclusively on the TSI and ASI datasets while evaluation is done across all the datasets, including the more challenging SKIPP'D, allowing us to assess how well the models generalize to lower-quality data and increased domain shifts.


\subsection{Performance Metrics}
We assess the effectiveness of the predicted values using the normalized Mean Absolute Percentage error (nMAP), defined as:

\begin{equation}
\text{nMAP} = \frac{1}{N} \sum_{i=1}^{N} \frac{|y_i - \hat{y}_i|}{\frac{1}{N} \sum_{i=1}^{N} y_i} \times 100 
\end{equation}

where \( y_i \) represents the actual value and \( \hat{y}_i \) represents the predicted value for the \( i \)-th sample, with \( i \in \{1, \dots, N\} \). It is commonly used for solar irradiance prediction as the normalization ensures that models can be assessed uniformly on datasets with varied value ranges, avoiding biased assessments due to scale differences.


\subsection{Baselines}
To benchmark our proposed method, we compare its performance against the state-of-the-art baseline, Gao \emph{et al.}~\cite{wacv2022}, who demonstrate state-of-the-art performance for nowcasting and forecasting by training a vision transformer \cite{google_vit} from the ground up using 10 years of site-specific data \cite{tsi_dataset, wacv2022, talha2019}. Their approach for forecasting utilizes a temporal transformer \cite{vaswani2017attention}, also trained on the same duration of data. To ensure a fair comparison, we reproduced their architecture and conducted experiments under the same conditions for both Gao \emph{et al.}'s~\cite{wacv2022} model and SPIRIT.


\input{zeroshot_results_tables}

\subsection{Zero-shot Transfer Learning}
\label{subsec:zero_shot_transfer_learning}
To evaluate the zero-shot generalization performance of our models, we analyze two distinct transfer learning scenarios. The first scenario examines intra-location generalization, where the models are trained and tested in the same geographic location but under varying camera setups. While the environmental conditions remain consistent, variations in camera setup, viewing angles, and image resolutions exist between the training and testing phases. When image-based models are trained on data from a particular camera setup, they learn to associate specific regions of the image with key features—such as the position of the sun, cloud formations, or atmospheric conditions—that influence the predicted output. However, when the camera setup is altered, the spatial mapping of these features within the image shifts. To assess how well the models handle such variations, we train them using the TSI dataset and evaluate them on the ASI dataset, and vice versa. 

The second scenario focuses on cross-location and cross-task generalization, where models trained in one geographic location are tested in another with different environmental and sensor characteristics. We train on the TSI and ASI datasets and evaluate on the SKIPP'D dataset, with the task shifting from predicting GHI to PV power output. Since GHI and PV output have a nearly linear correlation \cite{ghi_pv_linear}, this serves as a valid example of heterogeneous transfer learning. To account for the significant scale difference between GHI and PV output, model outputs are normalized for comparability. We conduct experiments for both nowcasting and forecasting tasks, training the models for one year and testing them on another year to account for seasonal variations, thus ensuring a fair evaluation. The nMAP errors are reported in Table~\ref{tab:zeroshot_nowcast} for nowcasting and Table~\ref{tab:zeroshot_forecast} for forecasting, comparing SPIRIT with the state-of-the-art in both-the zero-shot transfer learning setups and the traditional settings, where the models are trained and tested using data from the same location and setup, but different years.

\input{fewshot_results_figures}


\subsection{Fine-tuning with Limited Data}
Building upon our zero-shot transfer learning experiments, we now investigate the adaptability of our models in a fine-tuning framework, where a limited amount of labeled data from the target domain is available for fine-tuning. This scenario closely resembles practical deployment conditions, where prolonged data collection is often infeasible, and models must quickly adapt to new locations with minimal supervision. We evaluate transfer learning with limited data in two scenarios: intra-location adaptation and cross-location adaptation, as in Subsection~\ref{subsec:zero_shot_transfer_learning}.

For both experimental setups, we perform fine-tuning using progressively increasing amounts of labeled data from the target domain—specifically, one, two, three, and four weeks of data from a full year for nowcasting, and two, four, eight, twelve, and sixteen weeks of data for forecasting with testing done on the remaining data from the year. Given the greater complexity of forecasting, we extend the fine-tuning experiment to a larger time frame. Additionally, due to the requirement for temporal consistency in the time series data, as discussed in  Subsection~\ref{subsec:appendix_temporal_consistency_in_forecasting}, the number of nowcasting samples for a given time period exceeds that of forecasting samples. We implement a selective fine-tuning approach, where only the regressors (see Figure~\ref{fig:crown_jewel}) are updated, while the rest of the model is frozen. This ensures that the pre-trained feature representations, which capture generalizable spatiotemporal patterns, remain intact while allowing the model to adapt to location- and camera-specific variations. As demonstrated in prior work \cite{yuhao_transfer_learning, uses_freezing_for_tl, transfer_learning_strategies_comparison}, fine-tuning only the final layers achieves competitive adaptation performance while mitigating the risk of overfitting to the limited target data The nowcasting metrics are shown in Figure~\ref{fig:finetune_nowcast}, and the forecasting metrics are depicted in Figure~\ref{fig:finetune_forecast}.



