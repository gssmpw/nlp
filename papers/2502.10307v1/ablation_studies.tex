% \subsection{Impact of System Components}
% To better understand the contributions of different input components in our model, we conduct an ablation study where we evaluate the importance of image embeddings, auxiliary features, and physics-inspired features. The goal of this study is to quantify the effect of each component on the overall performance of our nowcasting and forecasting frameworks. 

% We perform this experiment by training models on the TSI 2020 dataset and testing them on the TSI 2021 in five different configurations: (i) using only image embeddings from the vision encoder, (ii) combining image embeddings with the auxiliary data available in the dataset, (iii) combining image embeddings with the physics-inspired features, (iv) using only auxiliary and physics-inspired features without image embeddings, and (v) using all three components together. 

% As shown in Table~\ref{tab:ablation_components}, the results highlight that image embeddings are essential for achieving high predictive accuracy. The model trained with only auxiliary and physics-inspired features performs significantly worse, reinforcing the importance of visual information for solar irradiance forecasting. Furthermore, incorporating auxiliary or physics-inspired features alongside image embeddings improves performance, as observed in the models trained with either auxiliary data or physics-based inputs in addition to embeddings. This demonstrates that the auxiliary values and physics-based features individually contribute to our architecture, each providing valuable information that enhances predictive accuracy. However, the best performance is achieved when all three components are used together, demonstrating that leveraging all available data sources yields the most robust and reliable predictions. Notably, our engineered physics-inspired features play a crucial role in capturing atmospheric patterns such as the actual position of the sun more accurately. Our final architecture effectively combines all available information, leveraging both available and engineered features to achieve the best performance.


\subsection{Investigating Different Vision Encoders}
We examine the impact of different vision models on SPIRIT's performance, also highlighting the versatility of our system across different foundation models. We evaluate the CNN-based ResNet-152 \cite{resnet}, the vision transformer-based DINOv2 Giant \cite{dinov2}, and our implementation using Google ViT-Huge \cite{google_vit}. Results summarized in Table \ref{tab:encoders_ablation}, demonstrate that the ViT-based models consistently outperform the ResNet-152 CNN model, which can be attributed to the superior capability of ViT architectures in capturing global image features \cite{vits_gt_cnns1}.
\input{ablation_foundation_models}


\subsection{Foundation Model Size}
Table \ref{tab:encoder_sizes_ablation} presents an analysis of how the size of the foundation model influences the performance of our nowcasting and forecasting architectures. Although increasing model size has traditionally been linked to performance gains, we observe that beyond a certain threshold, further scaling yields diminishing returns. This suggests that larger models do not always lead to better performance. In fact, models with 304M and 86M parameters outperform their larger counterparts with 632M parameters in forecasting and nowcasting, respectively. This aligns with recent work, which highlights that adjusting model size based on a computational budget, rather than blindly increasing model size, can lead to more efficient architectures with reduced inference costs \cite{getting_vit_in_shape}.
\input{ablation_foundation_model_size}


