
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{url}
\usepackage{alertmessage}
\usepackage{xspace}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{rotating}

% For comments
\usepackage[colorinlistoftodos,textsize=tiny,textwidth=35pt]{todonotes}
\newcommand{\Comments}{1}
\newcommand{\mynote}[2]{\ifnum\Comments=1\textcolor{#1}{#2}\fi}
\newcommand{\mytodo}[2]{\ifnum\Comments=1\todo[linecolor=#1!80!black,backgroundcolor=#1,bordercolor=#1!80!black]{#2}\fi}
% add your own here!
\newcommand{\ozzy}[1]{\mynote{green!50!black}{[OE: #1]}}
\newcommand{\ozzyt}[1]{\mytodo{green!20!white}{OE: #1}}
\newcommand{\jari}[1]{\mynote{red!50!black}{[JK: #1]}}
\newcommand{\jarit}[1]{\mytodo{red!20!white}{JK: #1}}
\newcommand{\gab}[1]{\mynote{blue!50!black}{[GA: #1]}}
\newcommand{\gabt}[1]{\mytodo{blue!20!white}{GA: #1}}
\newcommand{\ben}[1]{\mynote{orange!50!black}{[GA: #1]}}
\newcommand{\bent}[1]{\mytodo{orange!20!white}{GA: #1}}
\ifnum\Comments=1
\paperwidth=\dimexpr \paperwidth + 50pt\relax
\oddsidemargin=\dimexpr\oddsidemargin + 25pt\relax
\evensidemargin=\dimexpr\evensidemargin + 25pt\relax
\marginparwidth=\dimexpr \marginparwidth + 25pt\relax
\fi

% Useful aliases for formatting table columns
\usepackage{array}
\newcolumntype{P}{>{\centering\arraybackslash}p{2.5cm}}
\newcolumntype{M}{>{\centering\arraybackslash\footnotesize}m{.78cm}}
\newcolumntype{S}{>{\centering\arraybackslash\tiny}m{2cm}}

\usepackage{tikz,lipsum,lmodern}
\usepackage[most]{tcolorbox}

\newtcbtheorem[auto counter,number within=section]{obs}%
  {Observation}{fonttitle=\bfseries\upshape, fontupper=\slshape,
     arc=0mm, colback=cyan!5!white,colframe=cyan!75!white}{theorem}

\newtcbtheorem[auto counter,number within=section]{ins}%
  {Insight}{fonttitle=\bfseries\upshape, fontupper=\slshape,
     arc=0mm, colback=green!5!white,colframe=green!75!white}{theorem}

\title{HDEE: Heterogeneous Domain Expert Ensemble}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{O\u{g}uzhan Ersoy, Jari Kolehmainen \& Gabriel Passamani Andrade \\ \\
Gensyn}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

% Adding some useful custom commands/typesetting options
\newcommand{\reals}{\mathbb{R}}
\newcommand{\X}{\mathbb{X}}
\newcommand{\Y}{\mathbb{Y}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Ebar}{\tilde{\mathcal{E}}}
\newcommand{\One}{\mathbbm{1}}

\newcommand{\x}{\textbf{x}}
\newcommand{\y}{\textbf{y}}
\newcommand{\z}{\textbf{z}}
\newcommand{\p}{\textbf{p}}
\newcommand{\e}{\textbf{e}}
\newcommand{\h}{\textbf{h}}
\newcommand{\g}{\textbf{g}}
\newcommand{\0}{\textbf{0}}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\newcommand{\hete}{$\texttt{M}_\texttt{He}$-$\texttt{I}_\texttt{Ho}$\xspace}
\newcommand{\homoeq}{$\texttt{M}_\texttt{Ho}$-$\texttt{I}_\texttt{Ho}$\xspace}
\newcommand{\homouneq}{$\texttt{M}_\texttt{Ho}$-$\texttt{I}_\texttt{He}$\xspace}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Training dense LLMs requires enormous amounts of data and centralized compute, which introduces fundamental bottlenecks and ever-growing costs for large models.
Several studies aim to reduce this dependency on centralization by reducing the communication overhead of training dense models.
Taking this idea of reducing communication overhead to a natural extreme, by training embarrassingly parallelizable ensembles of small independent experts, has been shown to outperform large dense models trained in traditional centralized settings.
However, existing studies do not take into account underlying differences amongst data domains and treat them as monolithic, regardless of their underlying complexity, size, or distribution.
In this paper, we explore the effects of introducing heterogeneity to these ensembles of domain expert models.
Specifically, by allowing models within the ensemble to vary in size--as well as the number of training steps taken depending on the training data's domain--we study the effect heterogeneity has on these ensembles when evaluated against domains included in, and excluded from, the training set.
We use the same compute budget to train heterogeneous ensembles and homogeneous baselines for comparison.
We show that the heterogeneous ensembles achieve the lowest perplexity scores in $20$ out of the $21$ data domains used in the evaluation. Our code is available at \url{https://github.com/gensyn-ai/hdee}.

\end{abstract}

\section{Introduction}
Large Language Models (LLMs) have seen significant improvements in performance on both language and cognitive tasks in recent years~\citep{gpt,llama,gpt3,palm,BERT,megatron}.
These performance boosts can largely be accredited to aggregating a large number of GPUs (and wall-clock time) in order to train LLMs on an ever-growing corpus of data.
However, the unprecedented scale of data--and the concentration of compute power required to train these models (while running ablations, etc.)--introduces costs that are infeasible for all but a handful of companies. 

In order to reduce the current dependency on centralized compute, and thereby improve cost efficiency in large model training, several works aim to reduce the communication overheads of parallelized training.
Methods that cut down the synchronization frequency within data parallel training have shown significant promise and have successfully trained large models using highly distributed resources~\citep{diloco,demo,opendiloco}.
By reducing synchronization frequency, it is possible to train LLMs over nodes that are geo-distributed while achieving similar throughput compared to centralised training~\citep{intellect1}.
Furthermore, techniques such as mixture of experts~(MoE) and ensembling enable us to combine several models in order to facilitate efficient training and inference~\citep{survey_moe_2024}.
By using these techniques, and by reducing synchronization frequency to a natural extreme, independently trained domain-specific expert models can outperform large dense models by being combined~(i.e.~parameter averaging or MoE) or ensembled~\citep{btm,btx}.
Despite promising results shown by domain-specific experts, the effects of heterogeneity when training the experts remains largely unexplored other than for specific architectures~\citep{hmoe}.




In this paper, we explore the effects of heterogeneity in ensembles of domain expert models. 
Specifically, we investigate the impact of varying model sizes and total training steps depending on the training data domain's ``difficulty''.\footnote{In this paper we assign difficulty levels based on the perplexities of the models with respect to each task-specific domain and the similarities between the pre-training dataset and domain-specific dataset. For example, we consider the Mathematics dataset~(from S2ORC)~\citep{M2D2_2022} more difficult than the Tiny Stories dataset~\citep{tiny_stories_2023}.}
This paper analyses two special cases of heterogeneity that correspond to~(simplifications of) real-world constraints and compares them against a homogeneous baseline, leaving scope to compare additional domain-specific heterogeneities in future work.
Specifically, the three cases we compare in this paper are:
(i)~\homoeq (baseline): homogeneous model sizes and an equal number of steps for all models, (ii)~\homouneq: homogeneous model sizes and unequal number of steps, (iii)~\hete: heterogeneous model sizes and equal number of steps.



Our results show that both forms of heterogeneity considered~(\hete and \homouneq) achieve the lowest perplexity compared to the homogeneous baseline~(\homoeq) in $20$ out of $21$ domains. 
Among the heterogeneous models, \homouneq usually outperforms the other methods, especially in the difficult domains. 
We find that \hete performs slightly worse than \homouneq in difficult domains, but for some evaluation-only datasets, it achieves the best perplexity results.
Finally, our results show that increasing the heterogeneity level (the differences in model size or iterations) improves the performance of the ensemble.


\section{ELMForests, BTM, and Heterogeneity}
We explore the effects of heterogeneity in ELMForests--embarrassingly parallel ensembles of expert language models~(ELMs) first introduced by~\citet{btm}.
ELMForests are defined by a set $\mathcal{E}=\{\texttt{Expert}^i\}_{i=1}^n$ of ELMs, where each $\texttt{Expert}^i$ is independently trained on a specialized (sub-)domain $D_i$ of a corpus $\mathcal{D}=\{D_1,\dots,D_n\}$.
As with \citet{demix}~and~\citet{btm} we define domains based on \textit{data provenance}~(e.g.~whether a document came from a computer science publication vs.~a news article), and hence our ELMs are domain experts over interpretable segments of data.

The training of ELMForests is embarrassingly parallel and incremental due to the Branch-Train-Merge~(BTM) algorithm introduced by~\citet{btm}.
Starting from a pre-trained seed model $\texttt{Expert}^0$, BTM can be intuitively summarized as a three step process:
\begin{itemize}
    \item \textbf{Step $1$ (Branch):}~For each domain $D_i \in \mathcal{D}$ sprout a new ELM from the seed model (e.g.~create a ``clone'' to be independently trained) or, if prior iterations of BTM have already been executed, from a function of the existing expert set $\mathcal{E}$.
    \item \textbf{Step $2$ (Train):}~Each $\texttt{Expert}^i$ is trained on data from its corresponding domain $d_i$.
    Critically, each $\texttt{Expert}^i$ is trained completely independently from any other ELM, i.e.~no other ELMs (of the same branch training step) are involved in the training nor are any other data domains used.
    \item \textbf{Step $3$ (Merge):}~Form the ELMForest $\mathcal{E}$ by combining all of the independently trained ELMs.
\end{itemize}
In future iterations of the BTM algorithm, we can incrementally grow the ELMForest to capture new data domains by following this same process.

After training, ELMForests can perform inference in two natural ways - either by ensembling output probabilities across ELMs or aggregating all models in the ELMForest into a single LM via parameter averaging.
In this paper, for simplicity in adapting to heterogeneous model settings, we focus on the former and utilize a simplified variant of the cached prior method proposed by both~\citet{demix}~and~\citet{btm}.
Taking a probabilistic formulation of language modelling, where we estimate $p(X_t|\x_{<t})$ and introduce a domain variable $D$ alongside each sequence $\x$, we can denote the next-step conditional distribution over the history $\x_{<t}$ as:
\begin{equation}\label{eq:domain_posterior}
    p(X_t | \x_{<t}) = \sum_{i=1}^n p(X_t|\x_{<t}, D=D_i) \cdot p(D=D_i|\x_{<t})~.
\end{equation}
The ELMForest $\mathcal{E}$ naturally defines $p(X_t|\x_{<t}, D=D_i)$, but the \textbf{domain posterior} $p(D=D_i|\x_{<t})$ needs to be estimated. 
To do this, we use Bayes' rule:
\begin{equation}\label{eq:next_step_prob}
    p(D=D_i|\x_{<t}) = \frac{p(\x_{<t} | D=D_i) \cdot p(D=D_i)}{p(\x_{<t})} = \frac{p(\x_{<t} | D=D_i) \cdot p(D=D_i)}{\sum_{j=1}^n p(\x_{<t} | D=D_j) \cdot p(D=D_j)}~.
\end{equation}
The likelihood over sequences given a domain label are computed using the ELMs.
To compute the prior we experimented with several approaches, including the exact cached priors used by~\citet{demix}~and~\citet{btm}, but ultimately found that simply using a uniform prior over domains led to the best results, i.e.~$p(D=D_i)=1/n$ for all $i \in [1,\dots, n]$.

All results reported below are from ELMs trained using BTM and ensembled via the domain posterior~(Eq.~\ref{eq:domain_posterior}) with a uniform domain prior.




\subsection{Heterogeneous Domain Expert Ensembles}
The ELMForests studied in~\citet{btm} are entirely homogeneous; all ELMs in the forest are the same size and trained over the same amount of steps per domain, which we refer to as our baseline \homoeq.
We explore two heterogeneous training settings for ELMForests, where the model sizes or the number of training steps per model may differ across domains.
In \homouneq we keep the model sizes the same, but train \textit{easy} (resp.~\textit{difficult}) domains with fewer~(resp.~more) steps and thereby effectively force ELMs to utilize less~(resp.~more) data.
In \hete we keep the number of steps the same, but use different expert sizes depending on whether a data domain is \textit{easy}~(i.e.~smaller models) or \textit{difficult}~(i.e.~larger models).

\begin{figure}[htbp!]
	\centering
	\includegraphics[width=\textwidth]{figures/hde3-dx2b.pdf}
	\caption{An iteration of BTM-style domain training in HDEE. In \homoeq all models are the same size and are trained for the same number of steps. In \homouneq all models are the same size, but are trained for more or fewer steps depending on the data domain. In \hete models are different sizes depending on the data domain they will specialize in, but they are all trained for the same number of steps.}
	\label{fig:hde}
\end{figure}

In Figure~\ref{fig:hde}, we illustrate BTM-style training iterations for the three cases we consider when three model sizes are used. 
In theory, the number of models can be arbitrary, here we use three model sizes ($\texttt{Expert}_\text{S}$, $\texttt{Expert}_\text{M}$ and $\texttt{Expert}_\text{L}$) and similarly three different numbers of training steps ($\texttt{Iter}_\text{S}$, $\texttt{Iter}_\text{M}$ and $\texttt{Iter}_\text{L}$).



\section{Experimental Setup}

In our experiments, we analyse whether heterogeneity in the expert forest improves the perplexity on the trained (and evaluation-only) domains.
To test heterogeneity and compare with the homogeneous baseline, we assume that the trainer has a fixed compute budget for training and uses the same budget in each case. 
Here, considering that the computationally heavy part of a transformer layer is the FFN, we train each scenario by ensuring the following equations:
\begin{equation*}
    |\texttt{FFN}_\text{S}| \cdot \texttt{Iter}_\text{M} \approx |\texttt{FFN}_\text{M}| \cdot \texttt{Iter}_\text{S} \quad \text{and} \quad |\texttt{FFN}_\text{L}| \cdot \texttt{Iter}_\text{M} \approx |\texttt{FFN}_\text{M}| \cdot \texttt{Iter}_\text{L}
\end{equation*}
where $|\texttt{FFN}_\text{i}|$ corresponds to the size of FFN layer of $\texttt{Expert}_\text{L}$.


In the \homoeq case, we have three models of the same size $\texttt{Expert}_\text{M}$, each trained for $\texttt{Iter}_\text{M}$ steps per domain.
In the \homouneq case, we have again three models of the same size $\texttt{Expert}_\text{M}$ and \textit{easy}, \textit{moderate}, \textit{difficult} domains are trained for $\texttt{Iter}_\text{S}$, $\texttt{Iter}_\text{M}$ and $\texttt{Iter}_\text{L}$ steps respectively.
In the \hete case, we have three models of sizes $\texttt{Expert}_\text{S}$, $\texttt{Expert}_\text{M}$ and $\texttt{Expert}_\text{L}$ and \textit{easy}, \textit{moderate}, \textit{difficult} domains are trained for $\texttt{Iter}_\text{M}$ steps on their corresponding models.
Finally, for the next iterations in HDEE, the experts are trained over the ones from same difficulty domains, e.g., in \hete setting, a \textit{difficult} domain expert will be trained over the latest trained $\texttt{Expert}_\text{L}$ of that forest.


\subsection{Domains}

Table \ref{domains:data} shows a summary of the different datasets used in this study for domain adaptation~\citep{M2D2_2022}. We designate the S2ORC datasets as `\textit{difficult}, Wiki datasets as \textit{moderate}, and the remaining datasets as \textit{easy}. The designation is based on the seed model perplexities for the datasets that were trained using the OpenWebText corpus. For the validation and testing data, we hold out $\sim 10 \mathrm{M}$ tokens from each corpus that are not used for training. 

For training, we use a selection of datasets from M2D2 \citep{M2D2_2022}; Caselaw~\citep{caselaw}; Tiny stories~\citep{tiny_stories_2023}; Simple wikipedia~\citep{simple_wikipedia_2015}.\footnote{Simple Wikipedia may have overlapping data with the domains from Wikipedia. Considering the same applies to all ensemble cases, and also considering the significant difference in perplexities of these datasets, this should not have any meaningful impact in the results.}
In addition to the aforementioned training domains, we also evaluate the ensemble models in an out-of-domain scenario (evaluation-only) using other datasets from \citep{M2D2_2022}; Fineweb~\citep{fineweb_2024}; Hacker news~\citep{hacker_news_2015}; CC news~\citep{cc_news_2024}; and Reddit~\citep{reddit_2020}.

\begin{table}
\begin{tabular}{l r | l r | l r}
\multicolumn{2}{c}{S2ORC} &  \multicolumn{2}{c}{Wiki} & \multicolumn{2}{c}{Other} \\
\toprule
Name & Tokens & Name & Tokens & Name & Tokens \\
\midrule
Mathematics& $1.4\mathrm{B}$ & History $\&$ events & $226\mathrm{M}$  & Caselaw & $14.5\mathrm{B}$ \\
Physics& $737\mathrm{M}$ & Human activities & $343\mathrm{M}$ & Simple wikipedia & $70\mathrm{M}$\\
Computer science &$1.1\mathrm{B}$ & Philosophy & $ 165\mathrm{M}$ & Tiny stories & $1.3\mathrm{B}$
\end{tabular}
\caption{Summary of training datasets and their sizes in tokens.}
\label{domains:data}
\end{table}


\subsection{Models and iterations}
\paragraph{Seed Models}
All seed models use the Llama architecture and are pre-trained with the OpenWebText corpus~\citep{llama8b, Gokaslan2019OpenWebText}. They share the same vocabulary size ($128000$) and sequence length ($1024$). Input text is tokenized using the sentence piece tokenizer from \citep{llama8b}. Table \ref{models:seed} shows the seed model hyperparameters. Pre-training of the seed models, consisting of $20,000$ optimizer steps, is performed using linear warm-up and a cosine annealing learning rate scheduler. The warm-up schedule consists of $1,000$ steps at the beginning of the training and the cosine annealing scheduler is set to reduce learning rate by one magnitude over remaining training steps. We use B-float16 for numerical precision and flash-attention from \cite{flash_attention_tri_dao_2022} for attention head computations.

\begin{table}
\begin{tabular}{l | r r r r r r r r}
Model & 5M & 7.5M & 10M & 12.5M & 15M & 90M & 115M & 135M \\
\toprule
Hidden size & $272$ & $272$ & $320$ & $330$ & $340$ & $768$ & $768$ & $768$ \\
Intermediate size & $1088$ & $1088$ & $1280$ &  $1320$ & $1360$ & $2304$ & $3072$ & $3840$ \\
Attention heads & $8$ & $8$ & $10$ & $11$ & $10$ & $12$ & $12$ & $12$ \\
Number of layers & $4$ & $6$ & $6$ & $7$ & $8$ & $12$ & $12$ & $12$ \\
\midrule
Batch size & $262\mathrm{k}$ & $262\mathrm{k}$ & $262\mathrm{k}$ & $262\mathrm{k}$ & $262\mathrm{k}$ & $688\mathrm{k}$ & $688\mathrm{k}$ & $688\mathrm{k}$ \\
(Max) Learning rate & $.005$ & $.005$ & $.005$ & $.005$ & $.005$ & $.0006$ & $.0006$ & $.0006$ \\
\bottomrule
\end{tabular}
\caption{Seed model hyper-parameters used for training. Model size denotes the number of transformer layer parameters, excluding both input and output embeddings. Batch size is expressed as the number of tokens in one optimizer step.}
\label{models:seed}
\end{table}

\paragraph{Trained Domain Expert Models}
After pre-training, the seed models are trained using the datasets listed in Table \ref{domains:data}. Each model is trained with a single domain and the data is not mixed within any single training run. The training batch size is kept the same as listed in Table \ref{models:seed}, but the maximum learning rate is reduced to the final learning rate of the pre-training. We also employ warm-up and cosine annealing learning rate schedulers similar to the seed pre-training, but reduce the number of warm-up steps from $1000$ to $50$ steps and the total number of optimizer steps from $20,000$ to $600$.
For the second and third iterations of the domain training, each expert is trained over the previous iteration of the same category of expert. To avoid overfitting on the previous domains, we use the checkpoints of the previous experts at 400 steps, which is also the commonly used checkpoint for evaluating the experts (depending on the case, experts at different steps are evaluated in experiments, disclosed in results). 


\section{Results}

We test all three expert forests via three setups:
\begin{itemize}
    \item \textbf{Tiny Spread} where the model sizes are $\texttt{Expert}_\text{S}$: 5M, $\texttt{Expert}_\text{M}$:10M and $\texttt{Expert}_\text{L}$:15M and training steps are $\texttt{Iter}_\text{S}$:200, $\texttt{Iter}_\text{M}$:400 and $\texttt{Iter}_\text{L}$:600.
    \item \textbf{Tiny Close} where the model sizes are $\texttt{Expert}_\text{S}$: 7.5M, $\texttt{Expert}_\text{M}$:10M and $\texttt{Expert}_\text{L}$:12.5M and training steps are $\texttt{Iter}_\text{S}$:300, $\texttt{Iter}_\text{M}$:400 and $\texttt{Iter}_\text{L}$:500.
    \item \textbf{Small Close} where the model sizes are $\texttt{Expert}_\text{S}$: 90M, $\texttt{Expert}_\text{M}$:115M and $\texttt{Expert}_\text{L}$:135M and training steps are $\texttt{Iter}_\text{S}$:300, $\texttt{Iter}_\text{M}$:400 and $\texttt{Iter}_\text{L}$:500.
\end{itemize}

For each setup, we train three iterations of the corresponding forest (starting with seed models). 
In the i$^{th}$ iteration, we use the domains given in the i$^{th}$ row of Table~\ref{domains:data}.
The perplexity results after all three iterations are given in Table~\ref{tab:result_summary}.
Note that these perplexities are based on the ensemble of the final experts (each trained on three domains) using the domain posterior~(Eq.~\ref{eq:domain_posterior}) with a uniform domain prior. 
Our code is available at \url{https://github.com/gensyn-ai/hdee}.

\begin{table}
\centering
\begin{tabular}{|c|S|M M M|M M M|M M M|}
\hline
\multirow{2}{*}{} & \multirow{2}{*}{{\small Dataset}} & \multicolumn{3}{c|}{{\footnotesize Tiny~(Spread)}} & %
    \multicolumn{3}{c|}{{\footnotesize Tiny~(Close)}} & \multicolumn{3}{c|}{{\footnotesize Small~(Close)}}\\
\cline{3-11}
 & & {\scriptsize \hete} & {\scriptsize \homoeq} & {\scriptsize \homouneq} & {\scriptsize \hete} & {\scriptsize \homoeq} & {\scriptsize \homouneq} & {\scriptsize \hete} & {\scriptsize \homoeq} & {\scriptsize \homouneq}\\ \hline
\multirow{9}{*}{\begin{turn}{90}Trained Domains\end{turn}} & Math & \textbf{46.0} & 46.2 & \textbf{46.0} & 46.7 & 46.2 & \textbf{45.8} & 32.0 & 31.9 & \textbf{31.5} \\ 
 & Physics & \textbf{72.2} & 73.2 & 72.6 & 73.3 & 73.2 & \textbf{72.6} & \textbf{48.0} & 48.1 & \textbf{48.0} \\ 
 & CS & 61.0 & 61.6 & \textbf{60.8} & 61.6 & 61.6 & \textbf{61.3} & 40.6 & 40.5 & \textbf{40.4} \\ 
 & History & 68.8 & 68.8 & 68.8 & 68.8 & 68.8 & 68.8 & 41.9 & 41.9 & 41.9 \\ 
 & Human Activities & 66.7 & 66.7 & 66.7 & 66.7 & 66.7 & 66.7 & 40.3 & 40.3 & 40.3 \\ 
 & Philosophy & 64.6 & 64.6 & 64.6 & 64.6 & 64.6 & 64.6 & 39.2 & 39.2 & 39.2 \\ 
 & Caselaw & 74.2 & 67.7 & \textbf{64.7} & 72.4 & 67.7 & \textbf{67.1} & 33.1 & 31.8 & \textbf{31.6} \\ 
 & Simple Wiki. & 49.8 & 47.5 & \textbf{45.6} & 48.8 & 47.5 & \textbf{47.0} & 24.0 & 23.1 & \textbf{23.0} \\ 
 & TinyStories & 9.3 & \textbf{8.8} & 9.6 & 9.2 & \textbf{8.8} & 9.0 & 6.2 & \textbf{6.1} & 6.3 \\ \hline
\multirow{12}{*}{\begin{turn}{90}Evaluation-Only Domains\end{turn}} & Quant.~Bio & 73.7 & 74.2 & \textbf{73.1} & 74.2 & 74.2 & \textbf{73.7} & 46.6 & 46.7 & \textbf{46.5} \\ 
 & Astro-Physics & 73.2 & 74.1 & \textbf{73.1} & 73.7 & 74.1 & \textbf{73.4} & \textbf{46.7} & 47.0 & 47.0 \\ 
 & Cond. Matter & \textbf{53.1} & 53.7 & 53.3 & 53.8 & 53.7 & \textbf{53.1} & 37.3 & 37.3 & \textbf{37.1} \\ 
 & Statistics & 42.8 & 43.2 & \textbf{42.7} & 43.1 & 43.2 & \textbf{42.8} & 37.9 & 30.1 & \textbf{29.7} \\ 
 & Natural~Sciences & 85.3 & 85.3 & 85.3 & 85.3 & 85.3 & 85.3 & 49.7 & 49.7 & 49.7 \\ 
 & Tech.~\&~Appl.~Sci. & 70.6 & 70.6 & 70.6 & 70.6 & 70.6 & 70.6 & 42.5 & 42.5 & 42.5 \\ 
 & Social~Sciences & 62.8 & 62.8 & 62.8 & 62.8 & 62.8 & 62.8 & 38.1 & 38.1 & 38.1 \\ 
 & Culture~Arts & 63.3 & 63.3 & 63.3 & 63.3 & 63.3 & 63.3 & 37.2 & 37.2 & 37.2 \\ 
 & Fineweb & 150.3 & 144.1 & \textbf{139.9} & 147.7 & 144.1 & \textbf{143.2} & 61.2 & 59.5 & \textbf{59.2} \\ 
 & Hacker~News & \textbf{115.6} & 118.8 & 118.2 & \textbf{110.1} & 118.8 & 118.5 & \textbf{28.7} & 29.3 & 29.3 \\ 
 & CC~news & 107.4 & 104.2 & \textbf{102.0} & 104.4 & 104.2 & \textbf{103.8} & 41.4 & 40.1 & \textbf{39.8} \\ 
 & Reddit & 134.9 & 134.6 & \textbf{134.5} & 134.8 & \textbf{134.6} & \textbf{134.6} & 73.4 & 73.4 & 73.4 \\ \hline
\end{tabular}
\caption{Perplexity results of the ensemble models for the evaluated data domains.}
\label{tab:result_summary}
\end{table}

The experimental results show that the heterogeneous approaches (\hete and \homouneq) achieve the best results in almost all of the trained domains (8 out of 9) and in all 12 evaluation-only domains. The baseline \homoeq performs better only for the Tiny stories dataset. We observe that for the \textit{moderate} datasets, which are all from the Wikipedia cluster of M2D2, all three methods achieve the same perplexity results. This is because they each share the same $\texttt{Expert}_\text{M}$ expert which is trained for the same $\texttt{Iter}_\text{M}$ number of iterations, and because of the domain posterior ensembling, those models dominate the corresponding outputs. We observe the same effect in the evaluation-only Wikipedia domains.

For the domains that are trained with larger experts or more iterations (Math, Physics and CS), \homouneq achieves the best results in most cases, while for some cases \hete has the same or slightly better perplexities. These results show that heterogeneity of the expert sizes or iterations are beneficial for such domains, which is not surprising since they undergo \textit{more} training.
Yet, for the domains that are trained \textit{less}, \homouneq achieves better results than \homoeq for two datasets.
We believe that this is caused by these expert models forgetting the previous datasets, which is also supported by the fact that \homoeq has the lowest perplexities for the latest trained dataset.

\paragraph{Degree of Heterogeneity}
The setup for Tiny Spread (5M, 10M, 15M) and Tiny Close (7.5M, 10M, 12.5M) are similar in the order of model sizes and differ at the degree of the heterogeneity. We observe that \hete performs better in the higher degree of heterogeneity, which is most likely caused by having the largest model (15M) among Tiny setups. 
Whereas, \homouneq performs relatively better in a low degree of heterogeneity group (18 out of 21). This implies that when the model sizes are closer, training with more data is more impactful than training with a larger model.
Nonetheless, compared to the baseline \homoeq, \textbf{heterogeneous models, when combined, perform better as the degree of heterogeneity increases}.

\paragraph{Expert Size}
Tiny Close (7.5M, 10M, 12.5M) and Small Close (90M, 115M, 135M) share a similar degree of heterogeneity but the order of magnitude of the model sizes is different. 
In general, we observe no significant difference when the expert sizes are increased by more than 10 times.
Yet, \hete performs relatively better for the evaluation-only domains when the experts are larger.
This can be caused by the fact that it has the largest model (135M) and it does not overfit into the trained domains.


\section{Conclusion}


In this paper, we explored the effects of heterogeneity in ensembles of domain expert models regarding both the model sizes and the number of training steps.
We tested heterogeneity in both model size (\hete) and the number of training iterations (\homouneq).  
Our results show that heterogeneity (\hete and \homouneq) almost always (20 out of 21 domains) achieves the lowest perplexities compared with the homogeneous baseline (\homoeq). 

We believe that independent and parallel training of expert models together with heterogeneity would increase the diversity of expert contributions for training state-of-the-art (ensemble) models.
In this way, parties with different capacities can collectively contribute resources towards the training (and inference) of such models.

In future work, we plan to explore additional and domain-specific heterogeneities, in addition to the categorised difficulty groups evaluated in this work.
Also, we intend to mix the experts in HDEE into a single MoE model to reduce the inference cost.
In heterogeneous expert models, this can be achieved by keeping all the parameters the same except for the intermediate size.
However, the effects of such restrictions on the model performance need further exploration. 


\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\end{document}
