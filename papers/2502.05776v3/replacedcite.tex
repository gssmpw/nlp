\section{Related Works}
\label{sec:related}
The linear valuation model for contextual dynamic pricing, as defined in \cref{def:v_t_initial}, has been extensively studied under various assumptions. Recent works have explored statistical models—both linear and their extensions—for the pricing problem, assuming that $F_0$ (the noise distribution) is either known, partially known\footnote{Meaning $F_0$ is unknown but belongs to a parameterized family.} ____, or fully unknown ____. For comprehensive overviews of dynamic pricing from a broader perspective, we refer readers to ____ and ____.

We focus on the results most relevant to our 
work—specifically, the case where both the parameter $\theta_0$ and the distribution $F_0$ are fully unknown. In this setting, ____ estimate $F_0$ using kernel methods and derive a regret upper bound of $\widetilde{\mathcal{O}}((dT)^{\frac{2m+1}{4m-1}})$, where $m \geq 2$ denotes the degree of smoothness of $F_0$. In the realm of reinforcement learning, ____ introduces the Explore-then-UCB strategy, which balances revenue maximization, estimation of the linear valuation parameter, and nonparametric learning of the noise distribution. Under Lipschitz continuity on $F_0$, their approach achieves a regret rate of $\widetilde{\mathcal{O}}(d T^{3/4})$, and under (an additional) second-order smoothness assumption, a regret of $\widetilde{\mathcal{O}}(d^2T^{2/3})$. However, their regret bounds depend on a regularization parameter $\lambda > 0$, which is hard to tune dynamically, and the impact of the choice of $\lambda$ on the regret is not clearly described. ____ propose the D2-EXP4 algorithm, which is based on discretizing both the parameter space of $\theta_0$ and $F_0$. With appropriate choices of the discretization parameters, they establish a regret upper bound of $\widetilde{O}(T^{3/4} +\sqrt{d} T^{2/3})$. However, as noted in ____, they were unable to perform numerical experiments on D2-EXP4 due to the exponential time complexity of the EXP4 learner with a policy set of size $2^{T^{1/4}}$, making their algorithm impractical for application. 
%As noted in \cref{Comparison}, their upper bound only requires the revenue function $r(p) = pF_0(p-x_t^{\top}\theta_0)$ to be \emph{half-Lipschitz}; however, 
Furthermore, Assumption 1 of their paper requires their $x_t$ and $\theta_0$ to have \emph{non-negative entries}. While they claim that this assumption entails no loss of generality, this assumption is heavily used in the proofs of Theorem 6 and Theorem 5 of their work, and it is far from clear whether their derivations are generalizable to the situation when such sign constraints are not imposed. While the positivity of covariates can be ensured under boundedness by adding constants and adjusting the intercept parameter, the assumption that all covariates have a positive impact on the valuation is quite unrealistic for any regression model. 
%(when they use $u \leq \hat u$ where $u = x_t^{\top}\theta^*$ and $\hat u = x_t ^{\top} \lceil \theta^* \rceil_{\Delta}$ and several consequent inequalities) and in Theorem 5 (see the final part of the Sketch of Theorem 5). Moreover, assuming that all the covariates have a positive impact on the valuation is a strong assumption (more generally in any regression problem) as well as for the covariates.
 
In contrast to ____, we propose a tuning parameter-free policy that achieves a regret upper bound of order $\widetilde{\mathcal{O}}(T^{3/4} d^{1/3})$ when $\alpha =1$ (i.e. $F_0$ is Lipschitz). Furthermore, estimating the parameters $\theta_0$ and $F_0$ is computationally efficient: $\theta_0$ is estimated using ordinary least squares (OLS), and $F_0$ is estimated via isotonic regression\footnote{Alternatively, estimating $S_0=1 - F_0$ using antitonic regression.} using the Pool Adjacent Violators Algorithm (PAVA) introduced by ____, which, in our problem, has a computational complexity of $\mathcal{O}(d^{\nicefrac{\alpha}{2+\alpha}}T^{\nu(\alpha)})$ (see \cref{sec:complexity}). 

Very recent work by ____ provides a \textit{UCB-LCB-based} algorithm named VAPE (Valuation Approximation-Price Elimination). The main idea is to update the estimate of $\theta_0$ at time $t$ when $x_t$ is far from previously observed covariate values; otherwise, update the UCB-LCB around $F_0$ and deploy the optimal price. They prove that their regret is upper bounded by $\widetilde{\mathcal{O}}((dT)^{2/3})$ under Lipschitz assumption on $F_0$ (i.e. $\alpha =1$), which attains the lower bound in $T$, $\Omega(T^{2/3})$ established in ____. We summarize the regret upper bounds and the underlying assumptions in \cref{Comparison}. 

\begin{table*}[t]
\caption{Comparison of customer valuation model-based contextual dynamic pricing algorithms with stochastic contexts under the same assumptions on $\theta_0$ and similar smoothness assumptions on $F_0$. Notes: 
% \\ ($-$) For the assumptions in ____'s work we defer to a complete discussion in Section \ref{sec:related}.
($\mathdollar$): $\nu(\alpha)$ is defined in \cref{eq:zeta}.}

\label{Comparison}
\centering
\begin{small}
\begin{sc}
\begin{tabular}{lccccc}
\toprule
Method  & \thead{Regret\\ Upper Bound} & \thead{H\"older\\ Continuity}& \thead{Lipschitz\\ Continuity} & \thead{2nd Order\\ Smoothness} \\
\midrule
____    & $\widetilde{O}((dT)^{\frac{2m+1}{4m-1}} )$ & $\times$ &  $\times$ & $\surd$  \\
____  & $\widetilde{O}(T^{3/4} d)$ & $\times$ & $\surd$ & $\times$ \\
% ____  & $\widetilde{O}(T^{\frac{3}{4}} +\sqrt{d} T^{\frac{2}{3}})$ & $-$ & $-$ & $-$\\
____  & $\widetilde{O}((dT)^{2/3})$ & $\times$ & $\surd$ & $\times$ \\
This Work & $\widetilde{O}(T^{\nu(\alpha)}d^{\nicefrac{\alpha}{2+\alpha}})^{\mathdollar}$ & $\surd$ & $\surd$ & $\times$ & \\
\bottomrule
% Lower Bound &  & \text{not established yet} & $\Omega(T^{2/3})$ & \text{not established yet} & \\
% \bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{table*}