\section{Related Work}
\subsection{Speculative Decoding}

Speculative decoding is an effective approach for accelerating inference in LLMs by parallelizing token generation and verification. \citet{leviathan2023speculative} introduced the foundational framework, employing a small draft model to propose multiple tokens that a larger model verifies, significantly reducing inference latency. Medusa \cite{cai2024medusa} expanded this idea by leveraging multi-head decoding to enable simultaneous token generation and verification, improving throughput. 

Dynamic verification pipelines balance speed and accuracy by adjusting verification depth based on output quality \cite{liu2024adaptive}. Token tree verification accelerates serving \cite{specinfer2023}, while pipelined exact decoding handles compute-latency trade-offs \cite{predictive2023pipelined}. Knowledge distillation enhances draft–target model interaction \cite{distillspec2023}, and retrieval-based token validation improves efficiency \cite{rest2023retrievalaugmented}. Speculative decoding has been further optimized in recent works. SpecHub \cite{sun2024spechubprovableaccelerationmultidraft} uses optimal transport to improve draft token acceptance rates, and SPEED \cite{he2023speed} leverages early-layer hidden states for parallel token execution.

While speculative decoding enables efficient token generation, our work addresses a distinct challenge: leveraging predictable token patterns without introducing significant additional computation. CopySpec acts as an intelligent copying mechanism within the speculative decoding framework, reducing redundancy and improving efficiency across various tasks. By identifying and reusing repeated patterns in the context, CopySpec not only accelerates inference but also complements speculative decoding by extending its applicability to scenarios with high redundancy, such as multi-turn interactions and tasks with self-correction. This integration demonstrates the potential of combining these techniques to achieve greater efficiency in large-scale language models.

\subsection{Copying Mechanisms in Language Models}

Copying mechanisms are widely adopted in NLP to handle tasks that require replicating predictable patterns or segments. \citet{gu2016incorporating} introduced CopyNet, a method that enables RNN sequence-to-sequence models to predict words based on a mixed probabilistic model of two modes, where one selects words from the source sequence.
% selectively generate or copy tokens, addressing challenges like rare or repetitive sequences. 
Similarly, in summarization tasks, Pointer Networks \cite{vinyals2015pointer} and Pointer-Generator Networks \cite{DBLP:journals/corr/SeeLM17} demonstrated the effectiveness of combining copying and generation to improve output fidelity and handle out-of-vocabulary tokens. 

More recently, \citet{mccoy2023how} analyzed the extent to which transformers copy from their training data, providing insights into copying behaviors in modern LLMs. \citet{jelassi2024repeat} showed that transformers outperform state space models in copying repetitive patterns. 

Lastly, in a different domain, \citet{andronov2024acceleratinginferencestringgenerationbased} introduced a copying mechanism into a transformer-based encoder-decoder that models chemical reactions by observing that  portions of the input chemicals often remain unchanged in the output.

%a similar idea has been proposed enhancing the efficiency of chemical reaction modeling, where fragment continuity was shown by recognizing that large portions of input often remain unchanged. \cite{andronov2024acceleratinginferencestringgenerationbased}.
%\todo{@Prof. Mihai, I add it not under the speculative decoding section because these two reasons, 1. it's only applied to chemistry reaction's substring, 2.  didn't use decoder-only model, it's a encoding-decoding model, just use SD for decoding parts. Please also refer to the poster here \url{https://arxiv.org/pdf/2407.09685}.}

While previous works have emphasized the importance of copying mechanisms in various applications, our work is the first to explore this concept in the specific context of LLM inference. \textit{CopySpec} integrates a 
% intelligent % ms: no need to call your ideas intelligent
copying mechanism into speculative decoding, effectively reducing redundancy and enhancing efficiency across a wide range of tasks. By leveraging repeated patterns in the model's context, \textit{CopySpec} introduces a novel approach to accelerate inference while maintaining high performance.

% ms: removed the text below. seems weak to me
% This approach is not an advancement of copying methods but a novel application that builds upon these foundational ideas.

\begin{table*}[h!]
\centering
%\renewcommand{\arraystretch}{1.1} % Adjusts row spacing
\footnotesize
\resizebox{\textwidth}{!}{%
\begin{tabular}{l l c c c c c c}
\toprule
\textbf{Model} & \textbf{Variant} & \textbf{Metrics}  & \textbf{MT-Redundant} & \textbf{CNN/DM} & \textbf{GSM-8K} & \textbf{MT-Bench} & \textbf{HumanEval} \\
\textit{(Instruct)} &    &    & \textit{0-shot} & \textit{0-shot} & \textit{3-turn} & \textit{0-shot} & \textit{0-shot} \\
 &    &    & GPT-4 Score (↑)  & ROUGE-L (↑) & Accuracy (↑) & GPT-4 Score (↑) & Accuracy (↑) \\
\midrule
\multirow{4}{*}{\textbf{Qwen2.5-72B}} & Both & Score & 9.28 & 0.213 & 96\% & 9.18 & 87.8\% \\
\cmidrule(lr){2-8}
            & CopySpec & Tokens/Sec & \textbf{6.42}$\pm0.01$ & \textbf{8.68}$\pm0.01$ & \textbf{7.01}$\pm0.01$ & \textbf{5.55}$\pm0.01$ & \textbf{7.01}$\pm0.01$ \\
            &          & Copied  & 32.35\% & 82.48\% & 47.59\% & 20.53\% & 37.47\% \\
            %\cdashline{2-8}
            & Base model & Tokens/Sec & 4.82$\pm0.01$ & 3.70$\pm0.01$ & 4.55$\pm0.01$ & 4.83$\pm0.01$ & 4.98$\pm0.01$ \\
%\addlinespace
\midrule
\multirow{4}{*}{\textbf{Qwen2.5-32B}} & Both & Score & 9.10 & 0.214 & 93\% & 8.97 & 89.6\% \\
\cmidrule(lr){2-8}
            & CopySpec & Tokens/Sec & \textbf{13.82}$\pm0.01$ & \textbf{18.34}$\pm0.03$ & \textbf{14.84}$\pm0.01$ & \textbf{12.15}$\pm0.01$ & \textbf{14.41}$\pm0.01$ \\
            &          & Copied  & 33.17\% & 81.82\% & 44.93\% & 22.61\% & 34.23\% \\
            & Base model & Tokens/Sec & 10.26$\pm0.01$ & 7.79$\pm0.01$ & 9.76$\pm0.01$ & 10.29$\pm0.01$ & 10.46$\pm0.01$ \\
%\addlinespace
\midrule
\multirow{4}{*}{\textbf{Qwen2.5-7B}} & Both & Score & 8.53 & 0.230 & 85\% & 8.41 & 82.3\% \\
\cmidrule(lr){2-8}
            & CopySpec & Tokens/Sec & \textbf{54.05}$\pm0.11$ & \textbf{47.15}$\pm0.08$ & \textbf{63.37}$\pm0.54$ & \textbf{46.85}$\pm0.08$ & \textbf{48.79}$\pm0.01$ \\
            &          & Copied  & 34.42\% & 65.67\% & 53.01\% & 22.86\% & 32.68\% \\
            & Base model & Tokens/Sec & 39.88$\pm0.02$ & 25.25$\pm0.05$ & 38.58$\pm0.03$ & 39.98$\pm0.01$ & 33.63$\pm0.06$ \\
%\addlinespace
\midrule
\multirow{4}{*}{\textbf{Llama3.1-70B}} & Both & Score & 8.74 & 0.204 & 90\% & 8.72 & 77.4\% \\
\cmidrule(lr){2-8}
            & CopySpec & Tokens/Sec & \textbf{6.57}$\pm0.01$ & \textbf{5.49}$\pm0.01$ & \textbf{6.06}$\pm0.01$ & \textbf{5.83}$\pm0.01$ & \textbf{6.24}$\pm0.01$ \\
            &          & Copied  & 31.42\% & 38.35\% & 30.07\% & 21.83\% & 27.54\% \\
            & Base model & Tokens/Sec & 4.98$\pm0.01$ & 4.19$\pm0.01$ & 4.77$\pm0.01$ & 4.98$\pm0.01$ & 5.05$\pm0.01$ \\
%\addlinespace
\midrule
\multirow{4}{*}{\textbf{Llama3.1-8B}} & Both & Score & 8.03 & 0.185 & 79\% & 7.54 & 65.9\% \\
\cmidrule(lr){2-8}
            & CopySpec & Tokens/Sec & \textbf{49.28}$\pm0.08$ & \textbf{37.44}$\pm0.19$ & \textbf{49.60}$\pm0.01$ & \textbf{45.84}$\pm0.07$ & \textbf{46.49}$\pm0.48$ \\
            &          & Copied  & 35.45\% & 38.32\% & 38.01\% & 30.01\% & 26.44\% \\
            & Base model & Tokens/Sec & 35.51$\pm0.01$ & 26.57$\pm0.11$ & 35.19$\pm0.09$ & 35.43$\pm0.01$ & 37.57$\pm0.22$ \\
%\addlinespace
\midrule
\bottomrule
\end{tabular}
}
\caption{Performance comparison across five models (Qwen2.5-72B, Qwen2.5-32B, Qwen2.5-7B, Llama3.1-70B, and Llama3.1-8B) using CopySpec versus baseline configurations on multiple datasets, including MT-Redundant, CNN/DM, GSM-8K, MT-Bench, and HumanEval. Metrics include model-specific scores 
(GPT-4, using the 0613 checkpoint: Score, ROUGE-L, Accuracy), token generation rates (tokens/sec), and percentage of tokens copied. Results demonstrate the effectiveness of CopySpec in enhancing computational efficiency without compromising quality, achieving notable speed-ups and high token-copying rates in diverse tasks and model sizes.}

%Performance comparison across five models (Qwen2.5-72B, Qwen2.5-32B, Qwen2.5-7B, Llama3.1-70B, and Llama3.1-8B) using CopySpec versus baseline configurations on multiple datasets, including MT-Redundant, CNN/DM, GSM-8K, MT-Bench, and HumanEval. Metrics include model-specific scores 
%(GPT-4, using the 0613 checkpoint: Score, ROUGE-L, Accuracy), token generation rates (tokens/sec), and percentage of tokens copied. Notably, larger Qwen models produce more extractive summaries on CNN/DM, leading to slightly lower ROUGE-L scores. Results demonstrate the effectiveness of CopySpec in enhancing computational efficiency without compromising quality, achieving notable speed-ups and high token-copying rates in diverse tasks and model sizes.
\label{tab:performance}
\end{table*}



\subsection{Fill-in-the-Middle (FIM) Techniques}

Fill-in-the-Middle (FIM) enables language models to generate text segments within a given context, enhancing flexibility in tasks such as text and code infilling. \citet{bavarian2022efficient} introduced a data transformation approach for autoregressive models to learn infilling without sacrificing left-to-right generative performance, while \citet{shen2023film} proposed FiLM, enabling flexible generation by masking arbitrary positions.

In code generation, FIM techniques are crucial for editing and repair tasks. Models like Code Llama \cite{roziere2023codellama} and InCoder \cite{fried2023incoder}, a utilize bidirectional context for structured prompts, achieving state-of-the-art results on benchmarks such as HumanEval. Frameworks such as Self-Infilling \cite{zheng2024selfinfillingcodegeneration} and benchmarks like SAFIM further enhance these methods with backward generation and syntax-aware metrics \cite{safim2023,gong2024evaluationllmssyntaxawarecode}. Recent advancements, models like Codestral and CodeGemma, refine FIM techniques to improve alignment \cite{ mistral2024codestral,codegemmateam2024codegemmaopencodemodels}.

However, it is important to emphasize the distinct advantages of our method compared to the FIM approach. Unlike FIM, which relies on labeled tokens such as \texttt{<prefix>} and \texttt{<suffix>} to guide the model in fixing a specific section of code bidirectionally. Our method operates label-free, enabling a more flexible and generalizable approach. Additionally, while FIM is constrained to modifying a single code segment (typically the middle), CopySpec extends this capability by allowing modifications in multiple regions, such as quite a few distinct places within the input. Furthermore, we maintain the architectural simplicity of a left-to-right LLM, ensuring that our method remains compatible with existing LLM frameworks while offering significant improvements in efficiency and versatility.