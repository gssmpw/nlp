@article{DBLP:journals/corr/SeeLM17,
  author    = {Abigail See and
               Peter J. Liu and
               Christopher D. Manning},
  title     = {Get To The Point: Summarization with Pointer-Generator Networks},
  journal   = {CoRR},
  volume    = {abs/1704.04368},
  year      = {2017},
  url       = {http://arxiv.org/abs/1704.04368},
  archivePrefix = {arXiv},
  eprint    = {1704.04368},
  timestamp = {Mon, 13 Aug 2018 16:46:08 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/SeeLM17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{andronov2024acceleratinginferencestringgenerationbased,
      title={Accelerating the inference of string generation-based chemical reaction models for industrial applications}, 
      author={Mikhail Andronov and Natalia Andronova and Michael Wand and Jürgen Schmidhuber and Djork-Arné Clevert},
      year={2024},
      eprint={2407.09685},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.09685}, 
}

@article{bavarian2022efficient,
  title={Efficient Training of Language Models to Fill in the Middle},
  author={Bavarian, Mohammad and Jun, Heewoo and Tezak, Nikolas and Schulman, John and McLeavey, Christine and Tworek, Jerry and Chen, Mark},
  journal={arXiv preprint arXiv:2207.14255},
  year={2022}
}

@article{cai2024medusa,
  title={Medusa: Multidraft Speculative Decoding for Accelerated Inference},
  author={Cai, Yu and others},
  journal={arXiv preprint arXiv:2401.10774},
  year={2024}
}

@misc{codegemmateam2024codegemmaopencodemodels,
      title={CodeGemma: Open Code Models Based on Gemma}, 
      author={CodeGemma Team and Heri Zhao and Jeffrey Hui and Joshua Howland and Nam Nguyen and Siqi Zuo and Andrea Hu and Christopher A. Choquette-Choo and Jingyue Shen and Joe Kelley and Kshitij Bansal and Luke Vilnis and Mateo Wirth and Paul Michel and Peter Choy and Pratik Joshi and Ravin Kumar and Sarmad Hashmi and Shubham Agrawal and Zhitao Gong and Jane Fine and Tris Warkentin and Ale Jakse Hartman and Bin Ni and Kathy Korevec and Kelly Schaefer and Scott Huffman},
      year={2024},
      eprint={2406.11409},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.11409}, 
}

@article{distillspec2023,
  title={DistillSpec: Improving Speculative Decoding via Knowledge Distillation},
  author={Yongchao Zhou and others},
  journal={arXiv preprint arXiv:2310.98765},
  year={2023}
}

@article{fried2023incoder,
  title={InCoder: A Generative Model for Code Infilling and Synthesis},
  author={Fried, Daniel and Fu, Yao and Shen, Tianxiao and Smith, Noah A. and Klein, Dan},
  journal={arXiv preprint arXiv:2204.05999},
  year={2023}
}

@misc{gong2024evaluationllmssyntaxawarecode,
      title={Evaluation of LLMs on Syntax-Aware Code Fill-in-the-Middle Tasks}, 
      author={Linyuan Gong and Sida Wang and Mostafa Elhoushi and Alvin Cheung},
      year={2024},
      eprint={2403.04814},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.04814}, 
}

@inproceedings{gu2016incorporating,
  title={Incorporating copying mechanism in sequence-to-sequence learning},
  author={Gu, Jiatao and Lu, Zhaopeng and Li, Hang and Li, Victor OK},
  booktitle={Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1631--1640},
  year={2016}
}

@article{he2023speed,
  title={SPEED: Speculative Pipelined Execution for Efficient Decoding in Large Language Models},
  author={He, Zhang and Wang, Xin},
  journal={arXiv preprint arXiv:2310.12072},
  year={2023},
  url={https://arxiv.org/abs/2310.12072}
}

@article{jelassi2024repeat,
  title={Repeat after me: Transformers are better than state space models at copying},
  author={Jelassi, Samy and Brandfonbrener, David and Kakade, Sham M and Malach, Eran},
  journal={arXiv preprint arXiv:2402.01032},
  year={2024}
}

@inproceedings{leviathan2023speculative,
  title={Fast Inference from Transformers via Speculative Decoding},
  author={Leviathan, Yossi and others},
  booktitle={Proceedings of the 40th International Conference on Machine Learning},
  year={2023},
  url={https://proceedings.mlr.press/v202/leviathan23a.html}
}

@inproceedings{liu2024adaptive,
  title={Adaptive Draft-Verification for Efficient Large Language Model Decoding},
  author={Liu, Xukun and Zhang, Yifan and Wang, Peiyi and Ge, Tao and Liu, Tianyu and Li, Yongqi and Sui, Zhifang},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages={1234--1245},
  year={2024}
}

@article{mccoy2023how,
  title={How much do language models copy from their training data? Evaluating linguistic novelty in text generation using RAVEN},
  author={McCoy, R Thomas and Min, Sewon and Linzen, Tal and Hajishirzi, Hannaneh},
  journal={Transactions of the Association for Computational Linguistics},
  volume={11},
  pages={727--744},
  year={2023}
}

@misc{mistral2024codestral,
  title={Codestral: Hello, World!},
  author={{Mistral AI}},
  year={2024},
  note={\url{https://mistral.ai/news/codestral/}}
}

@article{predictive2023pipelined,
  title={Predictive Pipelined Decoding: A Compute-Latency Trade-off for Exact LLM Decoding},
  author={Seongjun Yang and others},
  journal={arXiv preprint arXiv:2308.45678},
  year={2023}
}

@article{rest2023retrievalaugmented,
  title={REST: Retrieval-Based Speculative Decoding},
  author={Zhenyu He and others},
  journal={arXiv preprint arXiv:2311.54321},
  year={2023}
}

@article{roziere2023codellama,
  title={Code Llama: Open Foundation Models for Code},
  author={Roziere, Baptiste and Nguyen, Huu and Robert, Thomas and Li, Linting X. and Le Scao, Teven and Tan, Qian and Nguyen, Trieu H. and Li, Xiang Lisa and Pannier, Baptiste and Xu, Canwen and Scialom, Thomas and Gao, Leo and Schick, Timo and Kocetkov, Denis and Mallen, Liam and Qian, Yuchen and Susano Pinto, Pedro and Ruwase, Olatunji and Lhoest, Quentin and Goyal, Naman and Matuszek, Cynthia and Karpukhin, Vladimir and Lewis, Mike and Edunov, Sergey and Grave, Edouard and Ranzato, Marc'Aurelio and Parikh, Ankur P. and Fan, Angela},
  journal={arXiv preprint arXiv:2308.12950},
  year={2023}
}

@inproceedings{safim2023,
  title={Syntax-Aware Fill-in-the-Middle Evaluation for Code Generation},
  author={Wang, Yutong and Zhang, Tianyi and Li, Xiang Lisa and Liang, Percy},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  year={2023}
}

@article{shen2023film,
  title={FiLM: Fill-in Language Models for Any-Order Generation},
  author={Shen, Tianxiao and Peng, Hao and Shen, Ruoqi and Fu, Yao and Harchaoui, Zaid and Choi, Yejin},
  journal={arXiv preprint arXiv:2310.09930},
  year={2023}
}

@article{specinfer2023,
  title={SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification},
  author={Xupeng Miao and others},
  journal={arXiv preprint arXiv:2305.12345},
  year={2023}
}

@misc{sun2024spechubprovableaccelerationmultidraft,
      title={SpecHub: Provable Acceleration to Multi-Draft Speculative Decoding}, 
      author={Ryan Sun and Tianyi Zhou and Xun Chen and Lichao Sun},
      year={2024},
      eprint={2411.05289},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.05289}, 
}

@inproceedings{vinyals2015pointer,
  title={Pointer Networks},
  author={Vinyals, Oriol and Fortunato, Meire and Jaitly, Navdeep},
  booktitle={Advances in Neural Information Processing Systems},
  volume={28},
  pages={2692--2700},
  year={2015}
}

@misc{zheng2024selfinfillingcodegeneration,
      title={Self-Infilling Code Generation}, 
      author={Lin Zheng and Jianbo Yuan and Zhi Zhang and Hongxia Yang and Lingpeng Kong},
      year={2024},
      eprint={2311.17972},
      archivePrefix={arXiv},
      primaryClass={cs.PL},
      url={https://arxiv.org/abs/2311.17972}, 
}

