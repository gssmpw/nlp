[
  {
    "index": 0,
    "papers": [
      {
        "key": "leviathan2023speculative",
        "author": "Leviathan, Yossi and others",
        "title": "Fast Inference from Transformers via Speculative Decoding"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "cai2024medusa",
        "author": "Cai, Yu and others",
        "title": "Medusa: Multidraft Speculative Decoding for Accelerated Inference"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "liu2024adaptive",
        "author": "Liu, Xukun and Zhang, Yifan and Wang, Peiyi and Ge, Tao and Liu, Tianyu and Li, Yongqi and Sui, Zhifang",
        "title": "Adaptive Draft-Verification for Efficient Large Language Model Decoding"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "specinfer2023",
        "author": "Xupeng Miao and others",
        "title": "SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "predictive2023pipelined",
        "author": "Seongjun Yang and others",
        "title": "Predictive Pipelined Decoding: A Compute-Latency Trade-off for Exact LLM Decoding"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "distillspec2023",
        "author": "Yongchao Zhou and others",
        "title": "DistillSpec: Improving Speculative Decoding via Knowledge Distillation"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "rest2023retrievalaugmented",
        "author": "Zhenyu He and others",
        "title": "REST: Retrieval-Based Speculative Decoding"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "sun2024spechubprovableaccelerationmultidraft",
        "author": "Ryan Sun and Tianyi Zhou and Xun Chen and Lichao Sun",
        "title": "SpecHub: Provable Acceleration to Multi-Draft Speculative Decoding"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "he2023speed",
        "author": "He, Zhang and Wang, Xin",
        "title": "SPEED: Speculative Pipelined Execution for Efficient Decoding in Large Language Models"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "gu2016incorporating",
        "author": "Gu, Jiatao and Lu, Zhaopeng and Li, Hang and Li, Victor OK",
        "title": "Incorporating copying mechanism in sequence-to-sequence learning"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "vinyals2015pointer",
        "author": "Vinyals, Oriol and Fortunato, Meire and Jaitly, Navdeep",
        "title": "Pointer Networks"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "DBLP:journals/corr/SeeLM17",
        "author": "Abigail See and\nPeter J. Liu and\nChristopher D. Manning",
        "title": "Get To The Point: Summarization with Pointer-Generator Networks"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "mccoy2023how",
        "author": "McCoy, R Thomas and Min, Sewon and Linzen, Tal and Hajishirzi, Hannaneh",
        "title": "How much do language models copy from their training data? Evaluating linguistic novelty in text generation using RAVEN"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "jelassi2024repeat",
        "author": "Jelassi, Samy and Brandfonbrener, David and Kakade, Sham M and Malach, Eran",
        "title": "Repeat after me: Transformers are better than state space models at copying"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "andronov2024acceleratinginferencestringgenerationbased",
        "author": "Mikhail Andronov and Natalia Andronova and Michael Wand and J\u00fcrgen Schmidhuber and Djork-Arn\u00e9 Clevert",
        "title": "Accelerating the inference of string generation-based chemical reaction models for industrial applications"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "andronov2024acceleratinginferencestringgenerationbased",
        "author": "Mikhail Andronov and Natalia Andronova and Michael Wand and J\u00fcrgen Schmidhuber and Djork-Arn\u00e9 Clevert",
        "title": "Accelerating the inference of string generation-based chemical reaction models for industrial applications"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "bavarian2022efficient",
        "author": "Bavarian, Mohammad and Jun, Heewoo and Tezak, Nikolas and Schulman, John and McLeavey, Christine and Tworek, Jerry and Chen, Mark",
        "title": "Efficient Training of Language Models to Fill in the Middle"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "shen2023film",
        "author": "Shen, Tianxiao and Peng, Hao and Shen, Ruoqi and Fu, Yao and Harchaoui, Zaid and Choi, Yejin",
        "title": "FiLM: Fill-in Language Models for Any-Order Generation"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "roziere2023codellama",
        "author": "Roziere, Baptiste and Nguyen, Huu and Robert, Thomas and Li, Linting X. and Le Scao, Teven and Tan, Qian and Nguyen, Trieu H. and Li, Xiang Lisa and Pannier, Baptiste and Xu, Canwen and Scialom, Thomas and Gao, Leo and Schick, Timo and Kocetkov, Denis and Mallen, Liam and Qian, Yuchen and Susano Pinto, Pedro and Ruwase, Olatunji and Lhoest, Quentin and Goyal, Naman and Matuszek, Cynthia and Karpukhin, Vladimir and Lewis, Mike and Edunov, Sergey and Grave, Edouard and Ranzato, Marc'Aurelio and Parikh, Ankur P. and Fan, Angela",
        "title": "Code Llama: Open Foundation Models for Code"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "fried2023incoder",
        "author": "Fried, Daniel and Fu, Yao and Shen, Tianxiao and Smith, Noah A. and Klein, Dan",
        "title": "InCoder: A Generative Model for Code Infilling and Synthesis"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "zheng2024selfinfillingcodegeneration",
        "author": "Lin Zheng and Jianbo Yuan and Zhi Zhang and Hongxia Yang and Lingpeng Kong",
        "title": "Self-Infilling Code Generation"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "safim2023",
        "author": "Wang, Yutong and Zhang, Tianyi and Li, Xiang Lisa and Liang, Percy",
        "title": "Syntax-Aware Fill-in-the-Middle Evaluation for Code Generation"
      },
      {
        "key": "gong2024evaluationllmssyntaxawarecode",
        "author": "Linyuan Gong and Sida Wang and Mostafa Elhoushi and Alvin Cheung",
        "title": "Evaluation of LLMs on Syntax-Aware Code Fill-in-the-Middle Tasks"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "mistral2024codestral",
        "author": "{Mistral AI}",
        "title": "Codestral: Hello, World!"
      },
      {
        "key": "codegemmateam2024codegemmaopencodemodels",
        "author": "CodeGemma Team and Heri Zhao and Jeffrey Hui and Joshua Howland and Nam Nguyen and Siqi Zuo and Andrea Hu and Christopher A. Choquette-Choo and Jingyue Shen and Joe Kelley and Kshitij Bansal and Luke Vilnis and Mateo Wirth and Paul Michel and Peter Choy and Pratik Joshi and Ravin Kumar and Sarmad Hashmi and Shubham Agrawal and Zhitao Gong and Jane Fine and Tris Warkentin and Ale Jakse Hartman and Bin Ni and Kathy Korevec and Kelly Schaefer and Scott Huffman",
        "title": "CodeGemma: Open Code Models Based on Gemma"
      }
    ]
  }
]