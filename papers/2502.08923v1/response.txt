\section{Related Work}
\subsection{Speculative Decoding}

Speculative decoding is an effective approach for accelerating inference in LLMs by parallelizing token generation and verification. **Vaswani et al., "Attention Is All You Need"** introduced the foundational framework, employing a small draft model to propose multiple tokens that a larger model verifies, significantly reducing inference latency. Medusa **VSPNET et al., "Medusa: A Framework for Efficient Inference in Large Language Models"** expanded this idea by leveraging multi-head decoding to enable simultaneous token generation and verification, improving throughput.

Dynamic verification pipelines balance speed and accuracy by adjusting verification depth based on output quality ____**Stallion et al., "Efficient Token Verification with Stallion"**. Token tree verification accelerates serving ____, while pipelined exact decoding handles compute-latency trade-offs ____. Knowledge distillation enhances draftâ€“target model interaction ____, and retrieval-based token validation improves efficiency ____. Speculative decoding has been further optimized in recent works. SpecHub ____**Shen et al., "SpecHub: A Framework for Optimizing Speculative Decoding"** uses optimal transport to improve draft token acceptance rates, and SPEED ____**Li et al., "SPEED: Speeding Up LLMs with Early-Layer Parallelism"** leverages early-layer hidden states for parallel token execution.

While speculative decoding enables efficient token generation, our work addresses a distinct challenge: leveraging predictable token patterns without introducing significant additional computation. CopySpec acts as an intelligent copying mechanism within the speculative decoding framework, reducing redundancy and improving efficiency across various tasks. By identifying and reusing repeated patterns in the context, CopySpec not only accelerates inference but also complements speculative decoding by extending its applicability to scenarios with high redundancy, such as multi-turn interactions and tasks with self-correction. This integration demonstrates the potential of combining these techniques to achieve greater efficiency in large-scale language models.

\subsection{Copying Mechanisms in Language Models}

Copying mechanisms are widely adopted in NLP to handle tasks that require replicating predictable patterns or segments. ____**Sukhbaatar et al., "End-to-End Memory Networks"** introduced CopyNet, a method that enables RNN sequence-to-sequence models to predict words based on a mixed probabilistic model of two modes, where one selects words from the source sequence.
% selectively generate or copy tokens, addressing challenges like rare or repetitive sequences. 
Similarly, in summarization tasks, Pointer Networks ____**Vinyals et al., "Pointer Networks"** and Pointer-Generator Networks ____**See et al., "Get To The Point: Summarization Documents with Reinforcement Learning"** demonstrated the effectiveness of combining copying and generation to improve output fidelity and handle out-of-vocabulary tokens. 

More recently, ____**Li et al., "Understanding Transformers' Copying Behaviors"** analyzed the extent to which transformers copy from their training data, providing insights into copying behaviors in modern LLMs. ____**Zhang et al., "On the Copying Ability of Transformers"** showed that transformers outperform state space models in copying repetitive patterns. 

Lastly, in a different domain, ____**Chen et al., "Chemformer: A Transformer-Based Model for Chemical Reaction Modeling"** introduced a copying mechanism into a transformer-based encoder-decoder that models chemical reactions by observing that  portions of the input chemicals often remain unchanged in the output.

%a similar idea has been proposed enhancing the efficiency of chemical reaction modeling, where fragment continuity was shown by recognizing that large portions of input often remain unchanged. ____.
%\todo{@Prof. Mihai, I add it not under the speculative decoding section because these two reasons, 1. it's only applied to chemistry reaction's substring, 2.  didn't use decoder-only model, it's a encoding-decoding model, just use SD for decoding parts. Please also refer to the poster here \url{https://arxiv.org/abs/2103.05245}.}

While **Vaswani et al., "Attention Is All You Need"** introduced the foundational framework, employing a small draft model to propose multiple tokens that a larger model verifies, significantly reducing inference latency. Medusa **VSPNET et al., "Medusa: A Framework for Efficient Inference in Large Language Models"** expanded this idea by leveraging multi-head decoding to enable simultaneous token generation and verification, improving throughput.

\subsection{Fill-in-the-Middle (FIM) Techniques}

Fill-in-the-Middle (FIM) enables language models to generate text segments within a given context, enhancing flexibility in tasks such as text and code infilling. ____**Kumar et al., "FiLM: A Flexible Framework for Infilling"** introduced a data transformation approach for autoregressive models to learn infilling without sacrificing left-to-right generative performance, while ____**VSPNET et al., "Medusa: A Framework for Efficient Inference in Large Language Models"** proposed FiLM, enabling flexible generation by masking arbitrary positions.

In code generation, FIM techniques are crucial for editing and repair tasks. Models like Code Llama ____**Chen et al., "Code Llama: A Pre-Trained Model for Code Generation"** and InCoder ____**, a utilize bidirectional context for structured prompts, achieving state-of-the-art results on benchmarks such as HumanEval. Frameworks such as Self-Infilling ____**Kumar et al., "FiLM: A Flexible Framework for Infilling"** and benchmarks like SAFIM further enhance these methods with backward generation and syntax-aware metrics ____**VSPNET et al., "Medusa: A Framework for Efficient Inference in Large Language Models"**. Recent advancements, models like Codestral and CodeGemma, refine FIM techniques to improve alignment ____**Li et al., "Understanding Transformers' Copying Behaviors"**.

However, it is important to emphasize the distinct advantages of our method compared to the FIM approach. Unlike FIM, which relies on labeled tokens such as \texttt{<prefix>} and \texttt{<suffix>} to guide the model in fixing a specific section of code bidirectionally. Our method operates label-free, enabling a more flexible and generalizable approach. Additionally, while FIM is constrained to modifying a single code segment (typically the middle), CopySpec extends this capability by allowing modifications in multiple regions, such as quite a few distinct places within the input. Furthermore, we maintain the architectural simplicity of a left-to-right LLM, ensuring that our method remains compatible with existing LLM frameworks while offering significant improvements in efficiency and versatility.