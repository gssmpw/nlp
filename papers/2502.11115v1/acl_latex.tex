% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
\usepackage[T5]{fontenc} 
\usepackage{CJKutf8}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb}

\usepackage{booktabs}

\usepackage{caption}
\usepackage{subcaption}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}







\usepackage[normalem]{ulem}
\useunder{\uline}{\ul}{}







% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Are Generative Models Underconfident? An Embarrassingly Simple Quality Estimation Approach}

% Author information can be set in various styles:
% For several authors from the same institution:
\author{Tu Anh Dinh \and Jan Niehues \\
        Karlsruhe Institute of Technology \\ Karlsruhe, Germany \\ \texttt{\{firstname\}.\{lastname\}@kit.edu}}


% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
Quality Estimation (QE) is estimating the quality of model output when the ground truth reference is not available. Looking at model uncertainty from its own output probabilities is the most trivial and low-effort way to estimate the output quality. However, for generative model, output probabilities might not be the best quality estimator. At an output step, there can be multiple correct options, making the probability distribution spread out more. Thus, lower token probability does not necessarily mean lower output quality. In other words, the model can be considered underconfident. In this paper, we propose a QE approach called \textbf{\textit{Dominant Mass Probability}} (\textsc{DMP)}, that boosts the model confidence in cases where there are multiple viable output options. We show that, with no increase in complexity, \textsc{DMP} is notably better than sequence probability when estimating the quality of different models (Whisper, Llama, etc.) on different tasks (translation, summarization, etc.). Compared to sequence probability, DMP achieves on average +0.208 improvement in Pearson correlation to ground-truth quality. 

 


\end{abstract}

\section{Introduction}
Text generation models, such as transcription and translation systems like Whisper \cite{radford2023robust} or Large Language Models like Llama \cite{touvron2023llama}, have demonstrated remarkable effectiveness across various applications \cite{amorese2023automatic, xie2024me, masalkhi2024side}.
However, these models are not perfect, as they would still make mistakes in certain cases, such as when the input is noisy or when the context involves ambiguous phrasing or domain-specific jargon \cite{katkov2024benchmarking,huang2023survey}.
Consequently, it is crucial to inform users about the reliability of model outputs by offering a quality assessment. This task is formally recognized in the research community as Quality Estimation.

Particularly, Quality Estimation (QE) is the task of providing quality scores on model output when the ground truth is not available. The most straightforward way is to infer the output quality from the model uncertainty by looking at model's output probability. 
However, for free-form text generation tasks, such as translation or summarization, model probability might not be the best estimator. 
For these tasks, there can be multiple correct outputs for a single input sequence. This leads to models being \textbf{underconfident}: lower probability does not necessarily indicate lower quality output, but could mean that the probability distribution is spread out on multiple correct options. 

In this paper, we propose a simple QE approach called \textbf{\textit{Dominant Mass Probability}} (\textsc{DMP}), which only utilizes the model output probability distribution. Without any added complexity, \textsc{DMP} tackles the underconfident phenomena mentioned above by boosting the confidence scores in the cases where there are multiple tokens with dominating probability values in the model output distribution. In particular, our contributions are as follows:

\begin{enumerate}
    \item We perform analysis showing that there indeed exist clusters of dominant tokens in the model output distribution that lead to underconfidence for free-form text generation tasks.
    \item We propose a Quality Estimation approach called \textbf{\textit{Dominant Mass Probability}} (\textsc{DMP})
    % \footnote{Code submitted as zip file.} 
    to tackle the underconfidence phenomena. \textsc{DMP} is easy to implement and does not add any complexity overhead compared to using raw model output probabilities. 
    \item We show that \textsc{DMP} is notably better as a quality estimator than the raw model probabilities across different tasks and different models, with an average increase of +0.208 in Pearson correlation to the ground truth quality.
    
\end{enumerate}

% At each output step, our approach checks whether there are tokens with dominant probability mass in the probability distribution. If a dominant token is selected in the final output, the uncertainty score assigned to that output step would be the sum of the probibality mass of all dominant tokens. If a non-dominant token is selected, we set the uncertainty score to zero. The uncertainty score of an output sequence would then be the average of the uncertainty score of each output step.




\section{Related Work}

\subsection{Quality Estimation} 
Model probability is the most trivial estimator of the output quality. However, previous works have shown that using the probability of the final output alone is not optimal, as neural models tend to be overconfident \cite{nguyen2015deep,li2021confidence}. Another way of utilizing the model output probability for quality estimation is to calculate the entropy of the whole probability distribution \cite{fomicheva-etal-2020-unsupervised}. However, probability entropy does not take into account which option is selected in the end. These methods utilizing model probabilities are generally low-effort, with the only drawback that output probability might not always be accessible for API-only models. Therefore, probability-based QE has been successfully employed in many use cases. For example, in dialog systems, the model probability of speech recognition output is used to decide whether to ask the user to repeat \cite{jm3}. In early exiting models, the probability entropy is used to decide at which layer the model can stop the forward pass and output the final prediction \cite{teerapittayanon2016branchynet, xin-etal-2020-deebert}.

Other lines of Quality Estimation approaches are usually more costly. They either require more inference runs, such as ensemble-based approaches \cite{kuhn2023semantic, malinin2020uncertainty} and self-validation approaches \cite{kadavath2022language}; or require access to the model training data to detect out-of-distribution instances during inference \cite{NEURIPS2018_abdeb6f5, ren2023outofdistribution}; or requires an external model to measure the output quality \cite{rei-etal-2022-cometkiwi,cohen-etal-2023-lm}.

One outstanding case of using external module for quality measure is supervised Quality Estimation models for the task of text translation. Unlike other text generation tasks, for machine translation, there exists abundant data of (source, model translation, human-labeled scores) tuples, which enable training supervised models that output quality scores. Quality Estimation has been widely adopted in the field of machine translation, and is even getting close to the performance of translation metrics that use reference ground-truth translation \cite{freitag-etal-2022-results}.

\subsection{Dominant Tokens} \label{sec:sampling}
Previous works have taken into account that there can be multiple dominant tokens in the probability distribution at an output step. However, they mostly focus on the case of sampling, rather than for quality estimation. They try finding the set of dominant tokens to sample from during generation in order to maintain high quality but also have diversity in the output. Popular sampling strategies includes top-$k$ \cite{fan-etal-2018-hierarchical}, top-$p$ \cite{holtzmancurious}, $\epsilon$-cut \cite{hewitt-etal-2022-truncation}, $\eta$-cut \cite{hewitt-etal-2022-truncation} and min-$p$ \cite{nguyen2024turning}. For top-$k$, the hidden assumption is that, the top $k$ tokens with the highest probability are the most important ones. For top-$p$, the most important tokens are ones with top probabilities that sum up to $p$. 
For $\epsilon$-cut, the most important token probabilities are larger than $\epsilon$. 
For $\eta$-cut, the most important token probabilities are larger than either $\eta$ or $\sqrt{\eta} * exp(-entropy(\mathbb{P}))$, where $\mathbb{P}$ is output probability distribution.
For min-$p$, the most important tokens have probabilities that is larger than the top-1 probability multiplied by $p$.


% TODO link to your work?



\section{Method}
\subsection{Problem definition} \label{sec:motivation}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{images/Example.drawio.png}
    \caption{Example of Whisper Large V3's probability distributions when translating from Vietnamese audio to English text. In the first case, the model gives high probability to the correct translation of "\textit{elephants}". In the second case, the model gives low probability to all tokens in the probability, and ends up outputting the wrong translation ("\textit{raccoons}" instead of "\textit{giraffes}"). In the last case, the probability of the tokens is lower due to probability mass being spread out between multiple correct options (the comma ",", "\textit{like}" and "\textit{such as}" are all reasonable next word), and \textbf{\textit{do not indicate lower quality}}.}
    \label{fig:example}
\end{figure*}

\paragraph{X-to-one vs. X-to-many} Our Quality Estimation method is inspired by the difference in model behavior between X-to-one and X-to-many tasks. An example of X-to-one task is Automatic Speech Recognition (ASR), where for each input audio, there is only one correct transcription. Here the models would assign most probability mass to one token that it deems correct at each output step. 

In contrast, we have X-to-many tasks such as Speech Translation (ST), where for each input sentence, multiple translations can be correct. This introduces aleatoric uncertainty, i.e., uncertainty coming from the data, but not from model's incompetency. This aleatoric uncertainty makes the models appear underconfident, as they need to spread out the probability mass over multiple correct options.

\paragraph{Illustrative example} Consider the example in Figure \ref{fig:example}, where Whisper Large V3 translate a Vietnamese audio sentence to English. The first two cases (correct translation to "\textit{elephants}" and wrong translation to "\textit{raccoons}") are intuitive: higher probabilities indicate better output quality. However, in the third case, most probability mass are spread between three options:  the comma ",", "\textit{like}" and "\textit{such as}", which are all reasonable next word. The probability values here are lower, but do not indicate low output quality. 



\paragraph{Dominant tokens} We refer to the set of tokens with the most probability mass at an output step as \textit{dominant cluster}, and the tokens themselves as \textit{dominant tokens}. We propose a heuristic to find these dominant clusters later in Section \ref{sec:method}. We find that, dominant clusters with sizes larger than 1 only exist for X-to-many tasks. We gather the statistics from all output steps of Whisper Large V3 on the ASR and ST task of the Fleurs test set \cite{conneau2023fleurs}, and report them in Figure \ref{fig:asr_vs_st}. For the ASR task, most finally chosen tokens have very high probability values that are close to 1. On the other hand, for the ST task, the probability of the finally chosen tokens spread out much more. We can also see this from Figure \ref{fig:asr_st_nr_dominant}, where for the ASR task, most dominant clusters only contain one element. For the ST task, the number of tokens in the dominant clusters is often more than one.

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.2\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/prob_asr_vs_st.pdf}
        \caption{Probabilities of final output token at every step.}
        \label{fig:asr_st_prob}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.2\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/nr_dominant_tokens.pdf}
        \caption{\#tokens in the dominant cluster at every step.}
        \label{fig:asr_st_nr_dominant}
    \end{subfigure}
    \caption{Comparision between the x-to-one ASR task and the x-to-many ST task. 
    % Outliers are removed from the plots for easier intepretation.
    }
    \label{fig:asr_vs_st}
\end{figure}

\paragraph{Motivation for QE} Given the above analysis, we confirm that the existence of dominant clusters with sizes larger than 1 is due to data uncertainty (aleatoric uncertainty), and not from model's uncertainty (epistemic uncertainty). The dominant tokens in these clusters would have lower probability that do not correctly reflect their quality as an output. Therefore, we propose a quality estimation approach, called \textbf{\textit{Dominant Mass Probability}} (\textsc{DMP)}, that favors these dominant tokens. 
% The details are as follows.

\subsection{Quality Estimation with Dominant Mass Probability} \label{sec:method}
\paragraph{Finding dominant token} As the first step, we need to identify which tokens are in the dominant cluster given the output distribution. We propose a heuristic approach that looks for a sudden drop in the sorted probability values which separate dominant tokens from non-dominant tokens.  

In particular: let $X=x_1,..,x_{|X|}$ be the input sequence, and $Y=y_1,..,y_{|Y|}$ be the model output sequence. At an output step $t$, let the model probability distribution over the vocabulary $V$ be $\mathcal{P} = (p_1, p_2, \dots, p_{|V|})$, where $p_i = \mathbb{P}(y_t = w_i \mid y_{<t}, X)$ represents the probability assigned by the model to token $w_i$ at output step $t$. First, we sort the values in the probability distribution $\mathcal{P}$ and obtain:
$$
\mathcal{P}_{\text{sorted}} = (p_{(1)}, p_{(2)}, \dots, p_{(|V|)}),
$$  
where \( p_{(1)} \geq p_{(2)} \geq \dots \geq p_{(|V|)} \) are the probabilities sorted in descending order. Then, we calculate the drops at each position, i.e., the differences between two consecutive probability values and get:

\begin{align*}
\mathcal{P}_{\text{diff}} & = \mathcal{P}_{\text{sorted}}  - \text{Shift}(\mathcal{P}_{\text{sorted}}) \\
 & = (p_{(1)}, p_{(2)}, \dots, p_{(|V|-1)}) \\
 & \hspace{2cm}- (p_{(2)}, p_{(3)}, \dots, p_{(|V|)})
\end{align*}

We then check at which positions the drops are significant. We propose a heuristic for the check: if the drop is larger than $x\%$ of the probability value (empirically, we find 30\% and 40\% are suitable values), then it is significant:
\begin{align*}
&\mathcal{P}_{\text{isSignificantDrop}} \\
&= \mathcal{P}_{\text{diff}} > \mathcal{P}_{\text{sorted}} * x\% \\
&= (p_{(i)} - p_{(i+1)} > p_{(i)} * x\% \text{\hspace{0.2cm}for } i = 1..|V|-1)
\end{align*}
Towards the tail of the distribution, the probabilities get close to zero, thus many drops satisfy the above condition although they are not significant drops that intuitively separate dominant from non-dominant tokens. Therefore, we add another condition for the drop to be significant: the drop itself should be larger than a threshold $\epsilon$ (empirically, we find 0.01 and 0.1 are suitable values):
\begin{align*}
& \mathcal{P}_{\text{isSignificantDrop}} \\
&= (\mathcal{P}_{\text{diff}} > \mathcal{P}_{\text{sorted}} * x\%) \textsc{ and } (\mathcal{P}_{\text{diff}} > \epsilon )  \\
&= (p_{(i)} - p_{(i+1)} > \text{max}(p_{(i)} * x\%, \epsilon)  \\
& \text{\hspace{0.6cm}for } i = 1..|V|-1)
\end{align*}

We then choose the last significant drop as the cutting point $c$:
$$
c = max\{i \text{ | } \mathcal{P}_{\text{isSignificantDrop\_i}} = True\}
$$ 
where tokens with probability values above the cutting point are dominant, and other tokens are non-dominant. An illustration is shown in Figure \ref{fig:method}.


\begin{figure}[h]
    \centering    
    \includegraphics[width=0.7\linewidth]{images/annotated_fixed_whisper_vi_en_validation_sent0_tok2.pdf}
    \caption{A dominant cluster found by our heuristic.}
    \label{fig:method}
\end{figure}



\paragraph{Extracting token quality estimation} Once we have found the dominant tokens, we extract quality estimation scores. If the finally selected output token is non-dominant, then we consider its probability to be the quality score as usual.
If the finally selected token is dominant, we consider the total probability mass of the whole dominant cluster as the quality score. That is, we take the sum of the probabilities of all dominant tokens as the quality score. Particularly:
\begin{align*}
&QE(w_{(i)}) =
\begin{cases} 
p_{(i)}, & \text{if } i > c \\
\sum\limits_{j=1}^{c} p_{(j)}, & \text{otherwise } i \leq c \\
\end{cases}
\end{align*}
In this way, we favor the dominant tokens whose probability mass was spread amongst multiple sensible options, as described in Section \ref{sec:motivation}.

\paragraph{Extracting sequence quality estimation} The QE score for the output sequence $Y=y_1,..,y_{|Y|}$ is defined as the average of token-level QE scores:
$$
QE(Y) = \sum\limits_{t=1}^{|Y|} QE(y_t)
$$


\section{Experimental Setup}
We test our Quality Estimation method on four different tasks: Speech Translation, Text Translation, Summarization and Question Answering. 

% Details of the experiment setup for each task are described as follows.



\begin{table*}[t]
\small
% \centering
\begin{tabular}{llll}
Task               & Dataset       & \#samples & Language                    \\ \hline
Speech Translation & Fleurs \cite{conneau2023fleurs}       & 350       & vi-en, de-en, es-en, cmn-en \\
Text Translation   & ParaCrawl \cite{banon-etal-2020-paracrawl}    & 5000      & en-de                       \\
                   & WMT22 General \cite{kocmi-etal-2022-findings} & 2000      & en-de                       \\
                   & HJQE \cite{yang-etal-2023-rethinking}         & 1000      & en-de                       \\
Summarization      & XSum \cite{narayan-etal-2018-dont}         & 3000      & en                          \\
Question Answering & GSM8k \cite{cobbe2021training}        & 3000      & en                         \\
\end{tabular}
\caption{Data used in our experiments.}
\label{tab:data}
\end{table*}


\begin{table*}[t]
\small
% \centering
\begin{tabular}{llr}
Task                 & Model            & \multicolumn{1}{l}{\#parameters} \\ \hline
Speech Translation   & Whisper Large V3 \cite{radford2023robust} & 1550M                            \\
Text Translation     & Scratch *        & 62M                              \\
                     & DeltaLM Large \cite{ma2021deltalm}  & 1374M                            \\
Summarization        & Bloomz \cite{muennighoff-etal-2023-crosslingual}          & 560M                             \\
+ Question Answering & Llama 3.2 \cite{touvron2023llama}       & 3B                               \\
                     & Llama 3.3 Instruct  \cite{touvron2023llama}      & 70B   \\
                     
\end{tabular}
\caption{Models used in our experiments. 
*: \textit{Scratch} is a transformer model trained on 5M ParaCrawl samples.}
\label{tab:model}
\end{table*}

\subsection{Data} \label{sec:data}
The datasets used in our experiments are listed in Table \ref{tab:data}. All datasets contain the input and ground truth of the corresponding task. One exception is WMT22 General \cite{kocmi-etal-2022-findings}, which additionally contains translation output of participating systems from the WMT22 Shared Task, along with human-annotated quality score ranging from 0 to 100 on the \textbf{segment level}. Another exception is HJQE \cite{yang-etal-2023-rethinking}, which additionally contains model translation output from the WMT20 Quality Estimation Shared Task \cite{specia-etal-2020-findings-wmt} along with human-annotated quality labels (OK/BAD) on the \textbf{token level}.

\subsection{Models}
The models used in our experiments are listed in Table \ref{tab:model}. \textit{Scratch} is a model trained from scratch on 5M samples from the ParaCrawl dataset, filtered by Bicleaner AI \cite{zaragoza-bernabeu-etal-2022-bicleaner,de-gibert-etal-2024-new}. \textit{DeltaLM Large} is fine-tuned on the Machine Translation task on the same ParaCrawl data. The Llama 3.3 70B model is used with 4-bit quantization.

\subsection{Baselines}
\paragraph{Probability-based baselines} We consider 2 baselines: \textit{sequence probability} and \textit{mean token entropy}. \textit{Sequence probability} is the product of token probabilities in an output sequence. \textit{Mean token entropy} is the average entropy of all tokens in an output sequence.
% TODO:  where the entropy of a token is defined as 
These two baselines are the most comparable to our approach, as they require only the probability distribution of output tokens.

\paragraph{Supervised Quality Estimation baseline} For some translation tasks, we use a supervised Quality Estimation (QE) model, WMT22 CometKiwi DA \cite{rei-etal-2022-cometkiwi}. The model is trained on tuples of (\textsc{src}, \textsc{mt}, \textsc{da}), where \textsc{src} is the input source sentence, \textsc{mt} is the machine translation output sentence, and \textsc{da} is the Direct Assessment scores on the given by human annotators. \textsc{da} scores range from 0 to 100, where 0 is assigned to the worst translation and 100 is assigned to the best translation. Note that this kind of supervised QE model trained on human-labeled quality annotations is mostly common for translation tasks. For other tasks such as summarization or question-answering, it would be more costly to obtain such human-annotated quality data. We regard this approach as an upper baseline for our approach.

\subsection{Hyperparameters}
We choose the hyperparameters for our approach, i.e., the values for $x$ and $\epsilon$, by tuning on the development splits of the datasets. We use 5000 samples from ParaCrawl for the Text Translation task, and use the development split of Fleurs for the Speech Translation task. We arrive at $x=30\%$ and $\epsilon=0.1$ for Text Translation tasks, and $x=40\%$ and $\epsilon=0.01$ for Speech Translation tasks. We observe that these values are close to each other and make little changes to the final QE performance. Therefore, we apply them directly on the remaining tasks (Summarization and Question Answering), with $x=30\%$ and $\epsilon=0.01$.

\subsection{Evaluation}









\begin{table*}[htbp]
\small
\centering
\begin{tabular}{llllrrc}
                    & Model        & Test Set      & Language & \multicolumn{1}{l}{Probability} & \multicolumn{1}{l}{Entropy} & \multicolumn{1}{l}{DMP (Ours)} \\ \hline
Speech Translation  & Whisper      & Fleurs        & vi-en    & 0.112                           & 0.379                       & \textbf{0.408}                 \\
                    &              & Fleurs        & de-en    & 0.213                           & 0.402                       & 0.396                          \\
                    &              & Fleurs        & es-en    & 0.193                           & 0.295                       & \textbf{0.312}                 \\
                    &              & Fleurs        & cmn-en   & 0.053                           & 0.387                       & \textbf{0.418}                 \\ \hline
Machine Translation & Scratch      & ParaCrawl     & en-de    & 0.155                           & 0.070                       & \textbf{0.221}                 \\
                    &              & WMT22 General & en-de    & 0.197                           & 0.147                       & \textbf{0.370}                 \\
                    & DeltaLM      & ParaCrawl     & en-de    & 0.131                           & 0.053                       & \textbf{0.386}                 \\
                    &              & WMT22 General & en-de    & 0.165                           & 0.169                       & \textbf{0.297}                 \\ \hline
Summarization       & Bloomz 560M  & XSum          & en       & 0.111                           & 0.189                       & \textbf{0.215}                 \\
                    & Llama3.2 3B  & XSum          & en       & 0.139                           & \textbf{0.236}              & 0.227                 \\
                    & Llama3.3 70B & XSum          & en       & -0.006                          & 0.002                       & 0.004                          \\ \hline
Question Answering  & Bloomz 560M  & GSM8K         & en       & -0.007                          & 0.107                       & \textbf{0.142}                 \\
                    & Llama3.2 3B  & GSM8K         & en       & 0.006                           & 0.295                       & \textbf{0.366}                 \\
                    & Llama3.3 70B & GSM8K         & en       & -0.267                          & \textbf{0.377}              & 0.341                          \\ \hline
\textbf{Average}             &              &               &          & 0.085                           & 0.222                       & \textbf{0.293}                
\end{tabular}
\caption{Performance of QE methods, in Pearson correlation to gold quality, across different tasks, models, test sets.}
\label{tab:overall}
\end{table*}











On the segment level, we use Pearson correlation to measure how well the scores from the quality estimation methods correlate with the gold quality annotation. The higher the correlation, the better the quality estimation methods perform. On the token level (HJQE dataset with OK/BAD labels), we use the Matthews correlation coefficient (MCC) scores \cite{matthews1975comparison}. The gold quality annotation is either automatically generated, or annotated by humans on pre-generated model output.

\subsubsection{Automatically Generated Gold Quality}
\paragraph{Speech and Text Translation} We use XCOMET-XL \cite{guerreiro-etal-2024-xcomet} as the gold quality of translation output. XCOMET-XL is a neural model trained to predict the quality of translations given the source, model translation and ground truth translation. \cite{fixed_dinh-etal-2024-quality} showed that, for machine translation, such reference-based neural metrics are good enough to be used as gold quality annotation to rank reference-free QE metrics.

\paragraph{Summarization and Question Answering} We use BART Score \cite{NEURIPS2021_e4d2b6e6} to annotate the quality of each output summary. The quality scores here are calculated as the mean token log probability from BART \cite{DBLP:journals/corr/abs-1910-13461}  of the summary output given the original text.

\subsubsection{Human-labeled Gold Quality} \label{sec:forced_decoding}
As described in Section \ref{sec:data}, the WMT22 General and the HJQE datasets contain human-annotated quality labels on pre-generated model output. In order to utilize these labels, we use the translation models of consideration ("Scratch" and \textit{DeltaLM Large}) to re-generate the output from the other models from the dataset with forced decoding. In this way, we avoid the biases from using an external model (XCOMET-XL) to create gold quality score.

% \subsection{Speech Translation}
% \paragraph{Model} We use Whisper Large V3 \cite{radford2023robust}, which is a speech-to-text model with encoder-decoder transformer architecture, containing 1550M parameters.  
% \paragraph{Test data} We use the FLEURS \cite{conneau2023fleurs}, a n-way parallel speech dataset. We use the test split of the data which contains 350 sentences, and consider 4 language pairs: Vietnamese-English, German-English, Spanish-English and Chinese-English. 
% \paragraph{Gold quality annotation} We use XCOMET-XL \cite{guerreiro-etal-2024-xcomet} to annotate the quality of the translation output. XCOMET-XL is a neural model trained to predict the quality of translations given the source and the reference sentences. \cite{fixed_dinh-etal-2024-quality} has shown that, in the case of machine translation, such reference-based neural metrics are good enough to be used as gold quality annotation to rank reference-free quality estimation metrics.


% \subsection{Text Translation} \label{sec:ExSetupMT}
% \paragraph{Model} We consider the task of translating from English to German. We train two models on 5M samples of ParaCrawl \cite{banon-etal-2020-paracrawl}, which are filtered using Bicleaner AI \cite{zaragoza-bernabeu-etal-2022-bicleaner,de-gibert-etal-2024-new}. The first model is a transformer model trained from scratch, containing 62M parameters. The second model is finetuned from DeltaLM Large, containing 1374M parameters.

% We consider two cases: sentence-level and word-level uncertainty/quality estimation.

% \subsubsection{Sentence-level Quality Estimation}
% \paragraph{Test data} As one test set, we use 5000 samples from ParaCrawl, also filtered by Bicleaner AI. Another test is is from the WMT22 General shared task \cite{kocmi-etal-2022-findings}, containing around 2000 samples. For both test sets, we use the two language pairs of consideration: English-German and Chinese-English.

% \paragraph{Gold quality annotation} Similar as for Speech Translation, we use XCOMET-XL to annotate the quality of the translation output. Additionally, we also use human-labeled quality data from the WMT22 General Shared Task. This data contains the machine translation output of several participating systems on the WMT22 General Shared Task, along with human-labled quality score on each output sentence.

% \subsubsection{Word-level Quality Estimation}
% \paragraph{Test data} We use the HJQE dataset \cite{yang-etal-2023-rethinking}. HJQE is a word-level quality estimation dataset, containing source and machine translated output sentences from the WMT20 Quality Estimation shared task \cite{specia-etal-2020-findings-wmt}, along with human-anonated OK/BAD quality labels on each machine translated word. We opt for this dataset since its quality labels are directly created by human annotators, and not by automatic alignment between raw and post-edited MT output like other datasets \cite{TODO}. 

% \paragraph{Gold quality annotation} 

% \subsection{Summarization}
% \paragraph{Model} We consider three Large Language Models: Bloomz 560M \cite{muennighoff-etal-2023-crosslingual}, Llama 3.2 3B and Llama 3.3 70B \cite{touvron2023llama}.

% \paragraph{Test data} We use 3000 samples from XSum \cite{narayan-etal-2018-dont}, which is a summarization dataset on online articles from the British Broadcasting Corporation (BBC).

% \paragraph{Gold quality annotation} We use BART Score \cite{NEURIPS2021_e4d2b6e6} to annotate the quality of each output summary. The quality scores here is calculated as the mean token log probability of the summary output given the original text generated by the BART model \cite{DBLP:journals/corr/abs-1910-13461}. 

% % TODO this might not be perfect


% \subsection{Question Answering}
% \paragraph{Model} Similar to the Summarization task, for the Question Answering task, we also consider three Large Language Models: Bloomz 560M, Llama 3.2 3B and Llama 3.3 70B. 

% \paragraph{Test data} We uses 3000 samples from GSM8K \cite{cobbe2021training}, a  linguistically diverse grade school math word problems.

% \paragraph{Gold quality annotation}
% TODO


\section{Results and Discussion}
\subsection{Overall Performance}



The overall performance of our approach, DMP, in comparison with the \textit{sequence probability} and \textit{mean token entropy} baselines, is shown in Table \ref{tab:overall}. DMP consistently outperforms the \textit{sequence probability} baseline by a large margin (+0.208 Pearson correlation on average).
% Especially, on the task of speech translation from Chinese to English, the sequence probability have almost no correlation (at 0.053) to the actual quality of the output, while our approach obtained 0.418 Pearson correlation.
% TODO hypothesis language different, model performance?
The \textit{mean token entropy} appears to be a stronger baseline. This is expected since this method takes into account the whole probability distribution at each output step. However, it does not take into account which token was finally selected. Therefore, our approach on average still has better performance than \textit{mean token entropy} (+0.071 in Pearson correlation).
% TODO range of improvement
% This is possible due to our approach both looks at the whole probabily distribution as well as taking into account which token is selected in the end. 
% TODO  verify?

The performance of our approach, DMP, is more consistent on translation tasks. It obtains $>$ 0.2 Pearson correlation across all settings. On the other hand, we observe cases where the two baselines fail. On the ParaCrawl test set, \textit{mean token entropy} obtains 0.070 and 0.053 Pearson correlation with the gold quality scores on the \textit{Scratch} and \textit{DeltaLM} model, respectively, while DMP achieves 0.221 and 0.388. On the Fleurs test set, Chinese-English translation, the \textit{sequence probability} baseline obtained 0.053 Pearson correlation, as opposed to our approach with 0.418. This is possibly due to our approach both looking at the whole probability distribution as well as taking into account which token is selected in the end. 

The performance on the Summarization and Question Answering tasks are more inconsistent. For Summarization with Llama3.3 70B, all three methods fail. For Question Answering with Llama3.3 70B, the \textit{sequence probability} has negative Pearson correlation. This either could be due to the complexity of the task, or that using automatically created gold quality labels from BartScore is not sufficient to rank quality estimation methods. 






\subsection{Scoring Other Models' Output}
In this experiment, we focus on evaluating the QE approaches when being used on one model to evaluate output created by other models for the Text Translation task. As described in Section \ref{sec:forced_decoding}, we use our model of consideration, i.e., \textit{Scratch} and \textit{DeltaLM Large}, to generate output from the other models from the dataset with forced decoding.

\subsubsection{Sentence-level Quality Estimation}

\begin{table}[h]
\small
\centering
\begin{tabular}{lccc}
                       & Best MT              & Worst MT             & \multicolumn{1}{l}{Average} \\ \hline
{\ul \textbf{Scratch}} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{}        \\
Probability            & 0.071                & 0.054                & 0.063                       \\
Entropy                & 0.147                & 0.240                & 0.194                       \\
Dominant               & 0.156                & 0.267                & 0.212                       \\ \hline
{\ul \textbf{DeltaLM}} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} &                             \\
Probability            & 0.070                & 0.064                & 0.067                       \\
Entropy                & 0.161                & 0.308                & 0.235                       \\
Dominant               & 0.178                & 0.338                & 0.258                       \\ \hline
Supervised QE          & 0.202                & 0.453                & 0.328                      
\end{tabular}
\caption{Performance of quality estimation methods, in Pearson correlation to human-labeled quality score}
\label{tab:mt_fd}
\end{table}





We make use of the WMT22 General Shared task data. We select the best and the worst participation systems from the shared task, by taking the average of the human-labeled quality scores on all output sentences of each system. We refer to them as \textit{Best MT} and \textit{Worst MT}. 
% We then use the models in consideration, i.e., Scratch and DeltaLM, to generate \textit{Best MT}'s and \textit{Worst MT}'s output by foreced decoding.
We calculate the correlation between the scores from the QE approaches to the human-labeled quality score. 

The results are shown in Table \ref{tab:mt_fd}. The \textit{sequence probability} baseline does not work in this setting, obtaining less than 0.1 Pearson correlation to the human-labeled quality scores on all settings. The \textit{mean token entropy} baseline performs better, at 0.194 using the \textit{Scratch} model and 0.235 using the \textit{DeltaLM Large} model.
Among the probability-based approaches, our approach has the best performance, at 0.212 using the \textit{Scratch} model and 0.258 using the \textit{DeltaLM Large} model.
It still lags behind the supervised QE baseline by around 0.1. However, this gap is not as large as expected, given that the supervised QE baseline is more complex, in terms of both computation and training data. 

\subsubsection{Word-level Quality Estimation} \label{sec:wl_qe}
We evaluate the performance of the QE methods on annotating pre-created output with OK/BAD quality labels on the HJQE dataset. As the probability-based quality estimation methods provide a continuous score for each token, we use the development split of HJQE to find the best threshold to convert the scores to the OK/BAD binary labels, and apply the threshold on the test set.

\begin{table}[htbp]
\small
\centering
\begin{tabular}{lcc}
                       & HJQE dev             & HJQE test            \\ \hline
{\ul \textbf{Scratch}} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} \\
Probability            & 0.169                & 0.110                \\
Entropy                & 0.001                & -0.005               \\
Dominant               & 0.197                & 0.134                \\ \hline
{\ul \textbf{DeltaLM}} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} \\
Probability            & 0.234                & 0.138                \\
Entropy                & -0.009               & 0.001                \\
Dominant               & \textbf{0.280}       & 0.156                \\ \hline
Supervised QE          & 0.240                & \textbf{0.165}      
\end{tabular}
\caption{Performance of quality estimation methods on the token level, in MCC scores compared to the gold human labeled quality.}
\label{tab:mcc}
\end{table}

The QE performance in MCC score is shown in Table \ref{tab:mcc}. We again observe that DMP achieves the best performance amongst the probability-based quality estimation methods, and closely approaches the performance of the supervised QE model. In this experiment, we can see that the \textit{mean token entropy} baseline fails. This is probably due to the negative effect of this baseline not taking into account the final output token. When evaluating on the sentence level, we hypothesize that the \textit{mean token entropy} would at least indicate the quality of the model prefix during autoregressive generation, thus having reasonable performance, while failing completely in this case where each token is evaluated independently.




% TODO more analysis on this, precision and recall, which case was BAD but now become OK

\subsection{Effect of Generative Performance} \label{sec:fail}
We investigate whether our QE methods work for models of different quality. We focus on the case of Speech Translation, where we investigate Whisper models of varying sizes for a more controlled experiment: Whisper Tiny, Whisper Base, Whisper Small, Whisper Medium, and Whisper Large V3. We run the models on the same Fleurs test set on four different language pairs as before. We report the model translation performance and the quality estimation performance alongside each other in Figure \ref{fig:sys_performance_effect}. The model performance is calculated as the average XCOMET score over all translation segments. The quality estimation performance is calculated as the Pearson performance to the segment-level XCOMET scores, similar to before.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\linewidth]{images/sys_performance_effect.pdf}
    \caption{Relationship between model translation performance and QE performance.}
    \label{fig:sys_performance_effect}
\end{figure}

Looking at Figure \ref{fig:sys_performance_effect}, DMP's QE performance is better for higher performing models, while model probability QE performance is more consistent across different models (but the performance is poor). This is somewhat expected, since the motivation of DMP is to improve cases when the model is underconfident. It does not consider the cases when a low-quality model is overconfident and constantly assigns high probability values to the wrong token. To test whether this is truly the cause, we manually look some output by the worse-performing model, Whisper Tiny on Chinese to English test data. One example is as follows:\\
\textbf{\textit{Source}}:
\begin{CJK*}{UTF8}{gbsn}
\small 
 "\textit{有了它，我们才有了火车、汽车和许多其他交通工具}"
\end{CJK*}\\
\textbf{\textit{Reference}}: "\textit{It has brought us the train, the car, and many other transportation devices.}"\\
\textbf{\textit{Model output}}: "\textit{There we have it.}"

Observe that the model exhibits signs of hallucination, as the output is quite irrelevant to the input sentence and the ground-truth reference. However, when we look at the probability distributions of the output tokens, they do form dominant clusters. For example, at the third output step after "\textit{There we ...}", the dominant next tokens assigned by the model are "\textit{are}", "\textit{have}" and "\textit{go}", as shown in Figure \ref{fig:example_hallu}. These tokens seem to be hallucinated: they are common words that might come after  "\textit{There we ...}", but are quite irrelevant to the input sentence. In cases like this, by favoring the dominant tokens, our approach emphasizes the models' overconfidence, thus leading to bad quality estimation performance. 


\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\linewidth]{images/fail_whisper_tiny_zh_en_test_sent533_tok2.pdf}
    \caption{Example of Whisper Tiny's hallucinated probability distribution at an output step.}
    \label{fig:example_hallu}
\end{figure}





\subsection{Finding Dominant Cluster}
\begin{table}[h]
\small
\centering 
\begin{tabular}{lccl}
                       & HJQE                 & HJQE                 & \multicolumn{1}{c}{Best}          \\
                       & dev                  & test                 & \multicolumn{1}{c}{hyperparams *} \\ \hline
{\ul \textbf{Scratch}} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} &                                   \\
top-$k$                  & 0.199                & 0.130                & $k$=2                               \\
$\epsilon$-cut            & 0.197                & 0.128                & $\epsilon$=0.05                      \\
$\eta$-cut                & 0.169                & 0.108                & $\eta$=0.1                       \\
top-$p$                  & 0.134                & 0.077                & $p$=0.9                             \\
min-$p$                  & 0.169                & 0.109                & $p$=0.9                             \\
difference-jump        & \textbf{0.207}       & \textbf{0.134}       & $x$=30\%, $\epsilon$=0.01               \\ \hline
{\ul \textbf{DeltaLM}} & \multicolumn{1}{l}{} &                      &                                   \\
top-$k$                  & \textbf{0.280}       & 0.147                & $k$=5                               \\
$\epsilon$-cut            & 0.254                & 0.149                & $\epsilon$=0.05                      \\
$\eta$-cut                & 0.219                & 0.124                & $\eta$=0.1                       \\
top-$p$                  & 0.153                & 0.088                & $p$=0.7                             \\
min-$p$                  & 0.234                & 0.137                & $p$=0.9                             \\
difference-jump        & \textbf{0.280}       & \textbf{0.156}       & $x$=30\%, $\epsilon$=0.1                \\ \hline
\multicolumn{4}{l}{* Best hyperparameters found on the dev split.}                                      
\end{tabular}
\caption{Performance of DMP on token-level QE when using different methods for finding dominant clusters. We denote our method as "difference-jump".}
\label{tab:diff_find}
\end{table}

We experiment with common methods, originally used for sampling, to find the dominant tokens. Refer to Section \ref{sec:sampling} for an explanation of these methods. We use the same experiment setup as in Section \ref{sec:wl_qe}: token-level quality estimation on the HJQE dataset. We use the HJQE development split to find the best hyperparameter for each setting, and apply them to the HJQE test split.

The results are shown in Table \ref{tab:diff_find}. Our method of finding dominant cluster performs generally better than the other methods, however, not by a large margin. Surprisingly, top-$k$ performs quite well despite being a naive approach that always assumes the number of dominant tokens in a cluster to be fixed. The MCC score of Quality Estimation using top-$k$ as dominant-cluster-finding method have almost the same performance as our approach. However, this might be due to the HJQE dev and test set being similar, thus tuning a good $k$ value is enough to achieve good performance. Observe that, for top-$k$, the best $k$ value for the \textit{Scratch} model is $k=2$, while for the \textit{DeltaLM Large} model is $k=5$. In contrast, the best hyperparameters found for other approaches are quite similar between the \textit{Scratch} model and the \textit{DeltaLM Large} model, indicating top-$k$ is more sensitive to hyperparameters.

\section{Conclusion}
In this paper, we first perform analysis showing the existence of dominant clusters with sizes larger than 1 in the model output probability distribution, which happens exclusively for x-to-many tasks. We show that the tokens in the dominant clusters are underconfident, as their probability is spread between other dominant options. Then, we proposed \textbf{\textit{Dominant Mass Probability}} (\textsc{DMP)} - a Quality Estimation method that favors the dominant tokens to encounter generative models being underconfidence. Since \textsc{DMP} only utilizes the model probability distribution, it is low-cost, easy to implement, and can be applied to many model architectures. We show that \textsc{DMP} performs notably better than model probability, and better than probability entropy. For QE on Machine Translation, when using DMP on a translation model to evaluate other models' output, DMP is reaching close to the performance of supervised QE approaches.


\section*{Limitations}
As discussed in Section \ref{sec:fail}, our method does not tackle cases where low-quality models are overconfident in their bad output. It's also unlikely to work for x-to-one text generation tasks like Automatic Speech Recognition, or multiple-choice Question-Answering, since the dominant clusters with sizes larger than 1 are unlikely to appear.

% Bibliography entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
% Custom bibliography entries only
% \bibliography{custom}

\appendix

\section{Tools and Hardwares}
The Speech Translation experiments are conducted using Huggingface \cite{wolf2019huggingface}. The Text Translation experiments are conducted using Fairseq \cite{ott2019fairseq}. The Summarization and Question Answering experiments are conducted using LM-Polygraph \cite{fadeeva-etal-2023-lm}. For all experiments, we use A100 GPUs with 40GB of memory.

\section{License For Artifacts}
The license for artifacts used in our paper is as follows:

\begin{itemize}
    \item Fleurs dataset \cite{conneau2023fleurs}: CC BY 4.0
    \item ParaCrawl dataset \cite{banon-etal-2020-paracrawl}: Creative Commons CC0
    \item WMT22 General dataset \cite{kocmi-etal-2022-findings}: Apache License 2.0
    \item XSum dataset \cite{narayan-etal-2018-dont}: MIT License
    \item GSM8k dataset \cite{cobbe2021training}: MIT License
    \item Whisper models \cite{radford2023robust}: Apache License 2.0
    \item DeltaLM model \cite{ma2021deltalm}: MIT License
    \item Bloomz model \cite{muennighoff-etal-2023-crosslingual}: The BigScience RAIL License
    \item Llama 3.2 models \cite{touvron2023llama}: Llama 3.2 Community License Agreement
    \item Llama 3.3 models \cite{touvron2023llama}: Llama 3.3 Community License Agreement
\end{itemize}

\end{document}
