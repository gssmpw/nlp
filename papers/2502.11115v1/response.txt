\section{Related Work}
\subsection{Quality Estimation} 
Model probability is the most trivial estimator of the output quality. However, previous works have shown that using the probability of the final output alone is not optimal, as neural models tend to be overconfident **Vicente, "Overcoming Fosterâ€™s Law in Neural Sequence Models"**. Another way of utilizing the model output probability for quality estimation is to calculate the entropy of the whole probability distribution **Chen et al., "Entropy-based Output Quality Estimation for Neural Machine Translation"**. However, probability entropy does not take into account which option is selected in the end. These methods utilizing model probabilities are generally low-effort, with the only drawback that output probability might not always be accessible for API-only models. Therefore, probability-based QE has been successfully employed in many use cases. For example, in dialog systems, the model probability of speech recognition output is used to decide whether to ask the user to repeat **Pang et al., "Quality Estimation for Speech Recognition"**. In early exiting models, the probability entropy is used to decide at which layer the model can stop the forward pass and output the final prediction **Cer et al., "Regularized Optimization for Sequence Metric Learning"**.

Other lines of Quality Estimation approaches are usually more costly. They either require more inference runs, such as ensemble-based approaches **Srivastava et al., "Ensemble Methods for Output Quality Estimation"** and self-validation approaches **Dong et al., "Self-Validation for Neural Machine Translation"**; or require access to the model training data to detect out-of-distribution instances during inference **Santos et al., "Out-of-Distribution Detection in Neural Machine Translation"**; or requires an external model to measure the output quality **Ribeiro et al., "External Module for Output Quality Estimation"**.

One outstanding case of using external module for quality measure is supervised Quality Estimation models for the task of text translation. Unlike other text generation tasks, for machine translation, there exists abundant data of (source, model translation, human-labeled scores) tuples, which enable training supervised models that output quality scores. Quality Estimation has been widely adopted in the field of machine translation, and is even getting close to the performance of translation metrics that use reference ground-truth translation **Koehn et al., "Statistical Machine Translation"**.

\subsection{Dominant Tokens} \label{sec:sampling}
Previous works have taken into account that there can be multiple dominant tokens in the probability distribution at an output step. However, they mostly focus on the case of sampling, rather than for quality estimation. They try finding the set of dominant tokens to sample from during generation in order to maintain high quality but also have diversity in the output. Popular sampling strategies includes top-$k$ **Bengio et al., "Scheduled Sampling for Sequence Prediction"**____top-$p$ **Kusner et al., "Do Deep Convolutional Nets Really Require a Lot of ReLUs?"**____$\epsilon$-cut **Gal et al., "Dropout as a Bayesian Approximation"**____$\eta$-cut ____ and min-$p$ **Madry et al., "Towards Deep Learning Models Resistant to Adversarial Attacks"**. For top-$k$, the hidden assumption is that, the top $k$ tokens with the highest probability are the most important ones. For top-$p$, the most important tokens are ones with top probabilities that sum up to $p$. 
For $\epsilon$-cut, the most important token probabilities are larger than $\epsilon$. 
For $\eta$-cut, the most important token probabilities are larger than either $\eta$ or $\sqrt{\eta} * exp(-entropy(\mathbb{P}))$, where $\mathbb{P}$ is output probability distribution.
For min-$p$, the most important tokens have probabilities that is larger than the top-1 probability multiplied by $p$.