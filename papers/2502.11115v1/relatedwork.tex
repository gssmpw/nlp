\section{Related Work}
\subsection{Quality Estimation} 
Model probability is the most trivial estimator of the output quality. However, previous works have shown that using the probability of the final output alone is not optimal, as neural models tend to be overconfident \cite{nguyen2015deep,li2021confidence}. Another way of utilizing the model output probability for quality estimation is to calculate the entropy of the whole probability distribution \cite{fomicheva-etal-2020-unsupervised}. However, probability entropy does not take into account which option is selected in the end. These methods utilizing model probabilities are generally low-effort, with the only drawback that output probability might not always be accessible for API-only models. Therefore, probability-based QE has been successfully employed in many use cases. For example, in dialog systems, the model probability of speech recognition output is used to decide whether to ask the user to repeat \cite{jm3}. In early exiting models, the probability entropy is used to decide at which layer the model can stop the forward pass and output the final prediction \cite{teerapittayanon2016branchynet, xin-etal-2020-deebert}.

Other lines of Quality Estimation approaches are usually more costly. They either require more inference runs, such as ensemble-based approaches \cite{kuhn2023semantic, malinin2020uncertainty} and self-validation approaches \cite{kadavath2022language}; or require access to the model training data to detect out-of-distribution instances during inference \cite{NEURIPS2018_abdeb6f5, ren2023outofdistribution}; or requires an external model to measure the output quality \cite{rei-etal-2022-cometkiwi,cohen-etal-2023-lm}.

One outstanding case of using external module for quality measure is supervised Quality Estimation models for the task of text translation. Unlike other text generation tasks, for machine translation, there exists abundant data of (source, model translation, human-labeled scores) tuples, which enable training supervised models that output quality scores. Quality Estimation has been widely adopted in the field of machine translation, and is even getting close to the performance of translation metrics that use reference ground-truth translation \cite{freitag-etal-2022-results}.

\subsection{Dominant Tokens} \label{sec:sampling}
Previous works have taken into account that there can be multiple dominant tokens in the probability distribution at an output step. However, they mostly focus on the case of sampling, rather than for quality estimation. They try finding the set of dominant tokens to sample from during generation in order to maintain high quality but also have diversity in the output. Popular sampling strategies includes top-$k$ \cite{fan-etal-2018-hierarchical}, top-$p$ \cite{holtzmancurious}, $\epsilon$-cut \cite{hewitt-etal-2022-truncation}, $\eta$-cut \cite{hewitt-etal-2022-truncation} and min-$p$ \cite{nguyen2024turning}. For top-$k$, the hidden assumption is that, the top $k$ tokens with the highest probability are the most important ones. For top-$p$, the most important tokens are ones with top probabilities that sum up to $p$. 
For $\epsilon$-cut, the most important token probabilities are larger than $\epsilon$. 
For $\eta$-cut, the most important token probabilities are larger than either $\eta$ or $\sqrt{\eta} * exp(-entropy(\mathbb{P}))$, where $\mathbb{P}$ is output probability distribution.
For min-$p$, the most important tokens have probabilities that is larger than the top-1 probability multiplied by $p$.


% TODO link to your work?