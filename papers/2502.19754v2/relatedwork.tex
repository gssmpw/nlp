\section{Related Work}
\subsection{Most Diffusion Paths are Manually Predetermined But not Optimisable}
%定义扩散路径
% \epsilon  定义没说
% t的范围没说
% 其他的扩散路径没有引用
To improve the image generation, researches focus on proposing various algorithmic frameworks, but neglecting the importance of diffusion paths. Diffusion paths are always pre-determined manually but not optimised. 
The Score-Based Generative Modeling (SGM) \cite{songscore} introduces different degrees of Gaussian noise to create diffusion path between the images $x_0$ and Gaussian noise $\epsilon$ based on Langevin dynamic sampling across diffusion steps $t \in \{0,\dots,T\}$. And It gradually generates images from noise along the diffusion path. The degree of Gaussian noise added is artificially set, with two examples provided: Variance Exploding (VE) and Variance Preserving (VP). The VP is:
\begin{equation}
	\begin{split}
		&x_t=\sqrt{\overline{\alpha}_t} x_0+ \sqrt{1 - \overline{\alpha}_t} \epsilon\\
		&x_{t+1} = \sqrt{1-\beta_{t+1}}x_t+\sqrt{\beta_{t+1}}\epsilon		
	\end{split}
	\label{ddim}
\end{equation}
where $\overline{\alpha}_t = \prod_{i=1}^t (1-\beta_i)$, and $\beta_t$ is an artificially set function of $t$, which is linear, constant, or sigmoid, ensuring that $\sqrt{\overline{\alpha}_0} \approx 1$ and $\sqrt{\overline{\alpha}_T} \approx 0$. It is similar to DDPM\cite{ho2020denoising} and DDIM\cite{song2020denoising}.

The VE is:
\begin{equation}
	\begin{split}
		&x_t = x_0+\sigma_t \epsilon\\
		&x_{t+1}=x_t+\sqrt{\sigma_{t+1}^2-\sigma_t^2}\epsilon
	\end{split}
\end{equation}
the larger $\sigma$ is better, as $x_T$ should be as close to the noise as possible. It is similar to NCSN \cite{song2019generative}.

Flow matching \cite{song2019generative} enhances the generality of the diffusion model based on the Continuous Normalizing Flows (CNFs). It overcomes the limitations of the necessity to use Gaussian noise and theoretically achieves the transformation of any image domain. For formal consistency, images are still generated from Gaussian noise in the image generation task. It emphasizes a natural and intuitive linear diffusion path in original paper:
\begin{equation}
	x_t = (1-t) x_0+(t+\sigma_{min}(1-t))\epsilon
	\label{FM}
\end{equation}
In subsequent work, Recited Flow \cite{liu2022flow} considers the $\sigma_{min}$ is $0$, focusing on the potential of linear paths to improve sampling efficiency.

In addition, there are some other studies \cite{bansal2024cold,hoogeboom2022blurring,liu2024residual,liu2023i2sb,chen2023schrodinger,karras2022elucidating,nichol2021improved,luo2023image} also utilising different diffusion paths. In summary, the diffusion paths can be modeled as a general paradigm: 
\begin{equation}
	\label{path paradigm}
	x_t=f_A(t)x_0+f_B(t)\epsilon
\end{equation}
different methods define the diffusion path with distinct $f_A(t)$ and $f_B(t)$, placing them within the diffusion path subspace $\mathcal{P}_{f_A,f_B} $ generated by $f_A$ and $f_B$.

\begin{table}[htpb]
	\centering
	\caption{Several classical diffusion paths in the subspace $\mathcal{P}_{f_A,f_B}$ follow the paradigm $x_t=f_A(t)x_0+f_B(t)\epsilon$.}
	\begin{tabular}{c|cc} % 控制表格的格式
		\toprule
		% 这部分是画一条横线在2-6 排之间
		\multicolumn{1}{c}{Diffusion Path} & \multicolumn{1}{c}{$f_A(t)$} & \multicolumn{1}{c}{$f_B(t)$}\\
		\midrule													
		Variance Exploding (VE) \cite{songscore,song2019generative} & $1$ &$\sigma_t$ \\
		Variance Preserving (VP) \cite{songscore,ho2020denoising,song2020denoising} & $\sqrt{\overline{\alpha}_t}$ & $ \sqrt{1 - \overline{\alpha}_t}$ \\
		Linear Path \cite{lipman2022flow,liu2022flow}& $1-t$ & $t$ \\
		\bottomrule
	\end{tabular}
	\label{tab1}
\end{table}

\subsection{Schrödinger Bridge Auto Find Diffusion Paths But face Challenges}
The Schrödinger Bridge (SB) problem \cite{schrodinger1932theorie,chetrite2021schrodinger} aims to find the least costly path between two distributions, but in practice it suffers from training difficulties and struggles to improve performance on complex data. Specifically, it is to find the optimal solution $\pi^*$ of the following optimization problem:
\begin{equation}
	\begin{split}
		\pi^*=arg min\{& KL(\pi|p_{ref}):\pi \in \mathcal{P}_{N+1}\\
		,& \pi_0=P_{data},\pi_N=p_{prior}\} \\\
	\end{split}
\end{equation}
where $\mathcal{P}_{N+1}$ represents the path space, $\pi_0$ and $\pi_N$ denote the distributions at both ends of the path, and $p_{ref}$ is the preset reference path. If $\pi^*$ is available, it can be sampled from $p_{prior}$ to $p_{data}$ or from $p_{data}$ to $p_{prior}$.

Typically, Schrödinger Bridge problem lacks closed-form solution. Most works use Iterative Proportional Fitting (IPF) \cite{fortet1940resolution,kullback1968probability,ruschendorf1993note} to approximate the Schrödinger bridge, which is optimised iteratively:
\begin{equation}
	\begin{split}
		&\pi^{2n+1}=arg min\{KL(\pi|\pi^{2n}):\pi \in \mathcal{P}_{N+1},\pi_N=p_{prior}\}\\
		&\pi^{2n+2}=arg min\{KL(\pi|\pi^{2n+1}):\pi \in \mathcal{P}_{N+1},\pi_0=P_{data}\}\\
	\end{split}
	\label{IPF}
\end{equation}
\textbf{Intuition: }The end of $\pi^{2n+1}$ is $p_{prior}$, and the start of $\pi^{2n+2}$ is $p_{data}$. 
Through iterations, $\pi^{2n+1}$ and $\pi^{2n+2}$ become similar increasingly, with the final paths beginning at $p_{prior}$ and ending at $p_{data}$.\\
The initial $\pi^0$ is a preset reference path. However, IPF requires calculating the joint probability density to obtain $\pi^n$, which significantly increases computational complexity and makes it impractical \cite{DSB,tang2024simplified}.

The Diffusion Schrödinger Bridge (DSB) \cite{DSB} is an approximate implementation of IPF. It decomposes the joint density into a series of conditional density optimization problems. Specifically, the path $\pi$ is decomposed into $\pi_{k+1|k}$ and $\pi_{k|k+1}$ by introducing a time step $t$ in the diffusion model:
\begin{equation}
	\begin{split}
		\pi^{2n+1}=arg min\{&KL(\pi_{k|k+1}|\pi^{2n}_{k|k+1}):\\
		&\pi \in \mathcal{P}_{N+1}, \pi_N=p_{prior}\}\\
		\pi^{2n+2}=arg min\{&KL(\pi_{k+1|k}|\pi^{2n+1}_{k+1|k}):\\
		&\pi \in \mathcal{P}_{N+1},\pi_0=P_{data}\}\\
	\end{split}\label{DIPF}
\end{equation}
In practice, DSB employs two independent neural networks, $F^n_k$ and $B^n_k$, to predict each transformation step of the forward and reverse diffusion, respectively. Through mathematical derivations and approximations \cite{DSB}, the training loss of DSB is formulated as follows: 
\begin{equation}
	\begin{split}
		L_{B^n_{k+1}}&=\mathbb{E}_{(x_k,x_{k+1})\sim p^n_{k,k+1}}\\
		&\left[||B^n_{k+1}(x_{k+1})-(x_{k+1}+F^n_k(x_k)-F^n_k(x_{k+1}))||^2\right]\\
		L_{F^{n+1}_{k}}&=\mathbb{E}_{(x_k,x_{k+1})\sim q^n_{k,k+1}}\\
		&\left[||F^{n+1}_{k}(x_{k})-(x_{k}+B^n_{k+1}(x_{k+1})-B^n_{k+1}(x_{k}))||^2\right]\\
	\end{split}
\end{equation}
where $p^n$ and $q^n$ represent the forward and reverse joint probability distributions, respectively, corresponding to $\pi_{2n}$ and $\pi_{2n+1}$ in the Eq. \ref{DIPF}. The DSB iteratively optimizes the $F^n_k$ and $B^n_k$ for $n \in {0, 1, …, L}$ to minimize Eq.\ref{DIPF}.

Training DSB is challenging with complex data. The network of the DSB is trained by the inference of another network and the unsupervised search for the diffusion path. 
In addition, the large input and output sizes broaden the path search range $\mathcal{P}_{N+1}$ to global levels, increasing network complexity and resulting in challenging, time-consuming, and computationally costly optimization \cite{tang2024simplified}.
%In addition, the input and output size of networks are both the target image size. The large input and output sizes makes the path search range $\mathcal{P}_{N+1}$ particularly wide to almost global, and the trained network more complex, leading to difficult training, time-consuming and expensive computational resources \cite{tang2024simplified}.

Subsequently, the Simplified Diffusion Schr\"odinger Bridge (S-DSB) \cite{tang2024simplified} is dedicated to simplifying and accelerating the diffusion Schrödinger bridge. It simplifies the training loss and reduces the amount of computation, as:
\begin{equation}
	\begin{split}
		L_{B^n_{k+1}}=&\mathbb{E}_{(x_k,x_{k+1})\sim p^n_{k,k+1}}\left[||B^n_{k+1}(x_{k+1})-x_{k}||^2\right]\\
		L_{F^{n+1}_{k}}=&\mathbb{E}_{(x_k,x_{k+1})\sim q^n_{k,k+1}}\left[||F^{n+1}_{k}(x_{k})-x_{k+1}||^2\right]\\
	\end{split}
\end{equation}
However, S-DSB still relies on unsupervised and iterative training of two complex networks, and faces similar challenges with training difficulty as DSB. 
This paper strengthens the connection between Schr\"odinger Bridge and diffusion models by solving the SB problem within path subspace $\mathcal{P}_{f_A,f_B}$ (optimising the $f_A$ and $f_B$ in Eq. \ref{path paradigm}), while keeping the pre-trained denoising network unchanged.


\subsection{KAN is Capable of Predicting Diffusion Path}
The Kolmogorov-Arnold Network (KAN) excels in predicting diffusion paths. According to the Kolmogorov-Arnold theorem \cite{kolmogorov1957representation}, any continuous function can be represented as a combination of continuous unary functions of finite variables. Leveraging this theorem, KAN \cite{liu2024kan} is introduced as an innovative network architecture. Unlike the Multilayer Perceptron (MLP), which optimizes a static weight matrix and employs a fixed nonlinear activation function, KAN utilizes multiple spline functions as its basis and optimizes the univariate weights of these nonlinear basis functions as learnable activation functions. 

KAN is particularly suitable for optimizing diffusion paths because of (1) resistance to forgetting and (2) its output is continuous.
(1) The local plasticity of KAN helps to prevent catastrophic forgetting, since the spline basis function is local, a sample only influences nearby spline function weights, leaving distant spline weights unaffected. In contrast, MLPs typically use global activations, where local changes propagate uncontrollably to distant areas, disrupting stored information. 
Our method employs a step-by-step optimization method similar to the Diffusion Schrödinger Bridge (DSB) during training, such that the current path step $t$ is optimized based on the previous steps $[0,\dots, t-1]$. The resistant forgetting capability of the KAN ensures that the previous steps is remained as possible when optimising the current step. 
(2) The continuous output of KAN arises from the inherent continuity of spline functions. An efficient diffusion path requires a smooth transition from the prior to the target domain. The continuous output of KAN aligns closely with smooth transition, making it more consistent with the diffusion path.