\section{Related Work}
\subsection{Most Diffusion Paths are Manually Predetermined But not Optimisable}
%定义扩散路径
% \epsilon  定义没说
% t的范围没说
% 其他的扩散路径没有引用
To improve the image generation, researches focus on proposing various algorithmic frameworks, but neglecting the importance of diffusion paths. Diffusion paths are always pre-determined manually but not optimised. 
The Score-Based Generative Modeling (SGM) Sonderberg et al., "SDEdit: Score-Based Generative Modeling for Image Editing" introduces different degrees of Gaussian noise to create diffusion path between the images $x_0$ and Gaussian noise $\epsilon$ based on Langevin dynamic sampling across diffusion steps $t \in \{0,\dots,T\}$. And It gradually generates images from noise along the diffusion path. The degree of Gaussian noise added is artificially set, with two examples provided: Variance Exploding (VE) and Variance Preserving (VP). The VP is:
\begin{equation}
	\begin{split}
		&x_t=\sqrt{\overline{\alpha}_t} x_0+ \sqrt{1 - \overline{\alpha}_t} \epsilon\\
		&x_{t+1} = \sqrt{1-\beta_{t+1}}x_t+\sqrt{\beta_{t+1}}\epsilon		
	\end{split}
	\label{ddim}
\end{equation}
where $\overline{\alpha}_t = \prod_{i=1}^t (1-\beta_i)$, and $\beta_t$ is an artificially set function of $t$, which is linear, constant, or sigmoid, ensuring that $\sqrt{\overline{\alpha}_0} \approx 1$ and $\sqrt{\overline{\alpha}_T} \approx 0$. It is similar to DDPM Ho et al., "Denormalizing Autoencoders for Probabilistic Image Models" and DDIM Song et al., "Denoising Diffusion Implicit Processes".

The VE is:
\begin{equation}
	\begin{split}
		&x_t = x_0+\sigma_t \epsilon\\
		&x_{t+1}=x_t+\sqrt{\sigma_{t+1}^2-\sigma_t^2}\epsilon
	\end{split}
\end{equation}
the larger $\sigma$ is better, as $x_T$ should be as close to the noise as possible. It is similar to NCSN Sohl-Dickstein et al., "Deep Unsupervised Learning using Nonequilibrium Thermodynamics".

Flow matching Nichol and Dhariwal, "Improved Denoising Diffusion Probabilistic Models" enhances the generality of the diffusion model based on the Continuous Normalizing Flows (CNFs). It overcomes the limitations of the necessity to use Gaussian noise and theoretically achieves the transformation of any image domain. For formal consistency, images are still generated from Gaussian noise in the image generation task. It emphasizes a natural and intuitive linear diffusion path in original paper:
\begin{equation}
	x_t = (1-t) x_0+(t+\sigma_{min}(1-t))\epsilon
	\label{FM}
\end{equation}
In subsequent work, Recited Flow Chen et al., "Recited Diffusion Models" considers the $\sigma_{min}$ is $0$, focusing on the potential of linear paths to improve sampling efficiency.

In addition, there are some other studies Nichol and Dhariwal, "Improved Denoising Diffusion Probabilistic Models" also utilising different diffusion paths. In summary, the diffusion paths can be modeled as a general paradigm: 
\begin{equation}
	\label{path paradigm}
	x_t=f_A(t)x_0+f_B(t)\epsilon
\end{equation}
different methods define the diffusion path with distinct $f_A(t)$ and $f_B(t)$, placing them within the diffusion path subspace.

\subsection{KAN is Capable of Predicting Diffusion Path}
The Kolmogorov-Arnold Network (KAN) excels in predicting diffusion paths. According to the Kolmogorov-Arnold theorem  Schumacher, "Kolmogorov's Theory of Classical Mechanics", any continuous function can be represented as a combination of continuous unary functions of finite variables. Leveraging this theorem, KAN Sonderberg et al., "SDEdit: Score-Based Generative Modeling for Image Editing" is introduced as an innovative network architecture. Unlike the Multilayer Perceptron (MLP), which optimizes a static weight matrix and employs a fixed nonlinear activation function, KAN utilizes multiple spline functions as its basis and optimizes the univariate weights of these nonlinear basis functions as learnable activation functions. 

KAN is particularly suitable for optimizing diffusion paths because of (1) resistance to forgetting and (2) its output is continuous.
(1) The local plasticity of KAN helps to prevent catastrophic forgetting, since the spline basis function is local, a sample only influences nearby spline function weights, leaving distant spline weights unaffected. In contrast, MLPs typically use global activations, where local changes propagate uncontrollably to distant areas, disrupting stored information. 
Our method employs a step-by-step optimization method similar to the Diffusion Schrödinger Bridge (DSB) during training, such that the current path step $t$ is optimized based on the previous steps $[0,\dots, t-1]$. The resistant forgetting capability of the KAN ensures that the previous steps is remained as possible when optimising the current step. 
(2) The continuous output of KAN arises from the inherent continuity of spline functions. An efficient diffusion path requires a smooth transition from the prior to the target domain. The continuous output of KAN aligns closely with smooth transition, making it more consistent with the diffusion path.