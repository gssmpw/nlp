\documentclass{article}

\usepackage{arxiv}
\usepackage{kpfonts}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}		% Can be removed after putting your text content
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}

\usepackage{algorithm}
\usepackage{algorithmic}

%
% authors defined
%
\usepackage{amsfonts} % to write \mathbb
\usepackage{amsmath} % to write \text in math
\usepackage{amsthm} % to write proofs
\usepackage{bm} % to write \bm
\usepackage{dsfont} % to write indicator function
\usepackage[inline]{enumitem} % to write enumerate
\usepackage{subcaption} % to use subfigure
\usepackage[table,xcdraw]{xcolor} % to write \color
\usepackage{cleveref}
\usepackage{booktabs}
\usepackage{multirow} % for tables in experiments section

% \usepackage{tikz} % to draw causal graphs
% \usetikzlibrary{bayesnet}
% \usetikzlibrary{patterns} % to fill nodes with patterns

\newcommand{\indep}{\perp \!\!\! \perp} % to denote statistical independence
\newcommand{\dep}{\not\!\perp\!\!\!\perp} % stat. dependence

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{example}[theorem]{Example}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}



\title{Strategic Learning with Local Explanations as Feedback}

%\date{September 9, 1985}	% Here you can change the date presented in the paper title
%\date{} 					% Or removing it

\author{ 
Kiet Q. H. Vo\textsuperscript{1}\thanks{Authors are also associated with the Saarland University, Saarbr\"ucken, Germany.} ,\hspace{2mm}
Siu Lun Chau\textsuperscript{1},\hspace{2mm}
Masahiro Kato\textsuperscript{2},\hspace{2mm}
Yixin Wang\textsuperscript{3},\hspace{2mm}
Krikamol Muandet\textsuperscript{1}
\\ \\
\textsuperscript{1}CISPA Helmholtz Center for Information Security, Saarbr\"ucken, Germany\\
\textsuperscript{2}Mizuhoâ€“DL Financial Technology, Co., Ltd., Tokyo, Japan\\
\textsuperscript{3}University of Michigan, Ann Arbor, MI, USA
}

% Uncomment to remove the date
%\date{}

% Uncomment to override  the `A preprint' in the header
%\renewcommand{\headeright}{Technical Report}
%\renewcommand{\undertitle}{Technical Report}
\renewcommand{\shorttitle}{Strategic Learning with Local Explanations as Feedback}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
% \hypersetup{
% pdftitle={A template for the arxiv style},
% pdfsubject={q-bio.NC, q-bio.QM},
% pdfauthor={David S.~Hippocampus, Elias D.~Striatum},
% pdfkeywords={First keyword, Second keyword, More},
% }

\input{math_symbols}

\begin{document}
\maketitle

\begin{abstract}
We investigate algorithmic decision problems where agents can respond strategically to the decision maker's (DM) models. The demand for clear and actionable explanations from DMs to (potentially strategic) agents continues to rise. While prior work often treats explanations as full model disclosures, explanations in practice might convey only partial information, which can lead to misinterpretations and harmful responses. When full disclosure of the predictive model is neither feasible nor desirable, a key open question is how DMs can use explanations to maximise their utility without compromising agent welfare. In this work, we explore well-known local and global explanation methods, and establish a necessary condition to prevent explanations from misleading agents into self-harming actions. Moreover, with conditional homogeneity, we establish that action recommendation (AR)-based explanations are sufficient for non-harmful responses, akin to the revelation principle in information design. To operationalise AR-based explanations, we propose a simple algorithm to jointly optimise the predictive model and AR policy to balance DM outcomes with agent welfare. Our empirical results demonstrate the benefits of this approach as a more refined strategy for safe and effective partial model disclosure in algorithmic decision-making.
\end{abstract}

% keywords can be removed
\keywords{Strategic Learning \and Explainable ML}


\section{Introduction}
Modern regulatory frameworks emphasise transparency in algorithmic decision making, mandating that decision makers (DMs) provide clear and understandable justifications for automated decisions~\citep{selbst2017meaningful,wachter2017right}. For example, the General Data Protection Regulation (GDPR) includes provisions commonly referred to as the \textit{right to explanation}, which require DMs to inform agents (i.e., individuals affected by automated decisions) about the basis of these decisions in a comprehensible manner~\citep{goodman2017european}. These provisions aim to help agents understand and potentially contest the rationale behind algorithmic decisions.
However, transparency can incentivise agents to manipulate their inputs to secure more favorable outcomes, triggering strategic adaptations by both agents and DMs~\citep{hardt2016strategic}. This dynamic has inspired extensive research into modeling strategic behavior and optimising decision making in such interactions~\citep{miller2020strategic}. 
Within this strategic learning domain, explainability is frequently interpreted as requiring full disclosure of the decision making model, including its structure and parameters~\citep{shavit2020causal,harris2022strategic,vo2024causal}. This perspective assumes that full disclosure of the predictive model inherently satisfies the need for transparency, enabling agents to simulate and assess different scenarios using the disclosed information.

%%%%%
\begin{figure}[t!]
    \centering
    \includegraphics[width=0.99\columnwidth]{figures/strategic-explanation.pdf}
    \caption{When fully disclosing a predictive model (left) is infeasible or undesirable, we consider strategic learning scenarios where only partial information is available, and study how this affects decision makers' choice of explanations (right).}
    \label{fig:enter-label}
\end{figure}
%%%%%



While such full disclosure satisfies transparency requirements, this may lead to DMs adopting overly simplistic models that prioritise interpretability at the expense of predictive performance. In complex models with billions of parameters, such transparency does not necessarily provide actual interpretability for agents and can instead overwhelm them with excessive information. This raises important questions about whether full disclosure truly aligns with the original intent of the right to explanation. Moreover, full disclosure may conflict with the interests of DMs, e.g., when dealing with sensitive intellectual property. 
% Disclosing the entire model, while meeting regulatory requirements, could jeopardise the competitive advantage of the DM. 
For instance, in car insurance pricing, an insurance company that invests significant resources into developing a state-of-the-art pricing model could face substantial risks if full disclosure is mandated. Competitors could exploit the proprietary information without incurring the same development costs, ultimately undermining the company's competitive edge.

In practice, explaining a DM's predictive model does not necessitate full disclosure. A variety of explanation methods in machine learning focus on conveying partial information~\citep{molnar2020interpretable,christoph2020interpretable}.
Although many explanation methods have been examined in the context of strategic agents~\citep{tsirtsis2020decisions, xie2024non, cohen2024bayesian}, it remains unclear which approach would best serve DMs.
Furthermore, whether there exists an optimal class of explanation methods that can be universally applied across different predictive models is an open question. Finally, when explanations omit certain details of the underlying model, agents' responses to this incomplete information may result in suboptimal or even detrimental outcomes for them. This is because popular explanation methods are primarily designed to report certain model characteristics rather than guiding agents' strategic responses~\citep{lundberg2017unified,tsirtsis2020decisions, chau2022rkhs, chau2023explaining}. As a result, our work addresses the following question: 

\emph{When full disclosure of the predictive model is neither feasible nor desirable, what kind of explanations should be communicated to ensure transparency while balancing the interests of all parties involved?}

Our contributions can be summarised as follows:
% \begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]
\begin{itemize}[leftmargin=*,]
    \item We demonstrate that, unlike agents with full access to the predictive model, those who rely solely on explanations may overestimate the benefits of their strategic actions. This can result in actions whose costs outweigh the anticipated gains, ultimately diminishing their overall utility.
    
    % This misjudgement can lead them to take actions whose costs outweigh the actual benefits, ultimately reducing their overall utilities.
    
    \item Under commonly assumed agents' reaction models, we establish a necessary condition to ensure that the DM's explanations do not mislead agents into taking actions that harm their utilities (\Cref{theorem:surrogate-necessary,corollary:general-necessary}).
    
    \item Additionally, under a conditional homogeneity assumption, we establish the existence of a sufficient class of explanations that do not mislead agents (\Cref{theorem:ar-sufficiency}). 
    
    % We term it the class of action recommendation (AR)-based explanations, a well-known example of which are counterfactual explanations~\citep{wachter2017counterfactual}.
    
    \item To put the theory in practice, we provide a simple algorithm for the DM to jointly optimise for their predictive model and the AR-based explanation policy.
\end{itemize}
All proofs are provided in \Cref{apx:proofs}.


%%%
\section{Preliminaries}

To set the scene, we adopt the car insurance pricing scenario considered in \citet{shavit2020causal} as our running example. In this scenario, a car insurance company (the DM) receives each customer (agent)'s profile and predicts their future accident cost to determine their insurance premium. Legal transparency requirements mandate the company to provide the customers with explanations for their decisions.
In response, customers may strategically adjust their profiles---such as purchasing a higher-tier car model or enrolling in a defensive driving course---to influence both their actual accident risk and the premiums assigned by the insurer.

We start with the standard setup in which the DM fully discloses their predictive model. We then extend this framework to a more practical setting where agents are provided with only local explanations---a scenario that better reflects current industry practices.

% We start with a standard setup where the DM fully publishes their predictive model, and extend it to a setting where only local explanations are given to agents.

% then show how a car insurance company, acting as the decision maker, can benefit from providing personalised explanations, i.e., explanations that are tailored to each of the customers, who act as strategic agents. Specifically, this car insurance company can set a different insurance premium for each of their customers, based on their accident risk as well as the expected cost they would incur. Upon knowing the insurance price, a customer can make an effort to improve their profile in order to be perceived as having lower risk and to obtain lower insurance price (\textcolor{red}{adverse selection}). Since this strategic behaviour could lead to suboptimal prediction result for the insurance company, e.g., gaming as in [REFs], it is important that the company adjusts their customers' risk predictive model accordingly. Throughout this work, we assume that this insurance company would want to minimise the risk of their predictive model, since this allows them to set the price optimally, without being too high or too low. The former situation would, hypothetically, give the company more profit but allow their competitors to win over customers while the latter would incur their loss.

\paragraph{Notations.} We denote random variables and their realisations with uppercase and lowercase letters, respectively. $\{1,\ldots,T\}$ are denoted as $[T]$. We use $\mathcal{X}$ and $\mathcal{Z}$ to denote the respective spaces of agents' covariates that are observed and unobserved to the DM. $\mathcal{Y}$ denotes the outcome space of agents and $\mathcal{G}$ denotes the hypothesis space of the DM. More details about their interactions are given in the setup below.

% \subsection*{Strategic Learning}
\paragraph{Strategic Learning.} We consider the scenario in which a DM interacts with a set of $T$ agents, indexed by $t\in[T]$. Agents interact with the DM separately and independently. Each agent $t$ interacts with the DM exactly once, referred to as round $t$. 
The agent $t$ is characterised by the covariates $\big(x_t^{(b)},z_t\big)\in\mathcal{X}\times\mathcal{Z}$ drawn independently from the same distribution $P_{X^{(b)},Z}$. Furthermore, the agent has an unrealised outcome $y_t^{(b)}:=h\big(x_t^{(b)}, z_t\big)$, where $h:\mathcal{X}\times\mathcal{Z}\to\mathcal{Y}$ is a deterministic potential outcome function and $y_t^{(b)}\in\mathcal{Y}\subseteq\mathbb{R}$. In our running example, this setup corresponds to a customer with an observable feature vector $x_t^{(b)}$, an unobservable feature vector $z_t$, and a potential monetary cost $y_t^{(b)}$ that they would incur from future car accidents.
% \Alan{What is $\mathcal{X} \times \mathcal{Z}$? Why don't we introduced these in the notation paragraph? What do you mean by {\color{red}unrealised} outcome? that doesn't sound standard. You have observable feature x, unobservable feature z, and then unrealised outcome y? Clarifying this is important.}

At each round $t$, the DM begins by publishing their predictive model $g:\mathcal{X}\to\mathcal{Y}$ that will be used to predict the outcome of this agent $t$. With this information, agent $t$ strategically modifies their base covariate value from $x_t^{(b)}$ to $x_t$, and reports it to the DM. For instance, this corresponds to a customer's strategic adaptation of the profile to obtain an insurance contract with a lower price, e.g., $g\big(x_t\big)\leq g\big(x_t^{(b)}\big)$. Following prior work~\citep{harris2022strategic,vo2024causal}, we assume the agent responds with $x_t:=\arg\max_{x}u_t(g,x)$ where their utility function $u_t$ is defined as
\begin{align}\label{eq:ag-true-utility}
u_t(g,x) := -\Big(g(x)+c_t\big(x_t^{(b)},x\big)\Big),
\end{align}
and their goal is to minimise the predictive value while keeping the cost of their action, $c_t(x_t^{(b)},x)$, as low as possible.
% Following prior work~\citep{shavit2020causal,harris2022strategic,vo2024causal}, we assume the agent aims at minimising the DM's prediction value of them while maintaining the cost of their strategic action as follows:
% $x_t := \arg\min_{x} (g(x)+c_t(x_t^{(b)},x))$ and, equivalently, we define the agent's utility function, to be maximised, as
% \begin{align}\label{eq:ag-true-utility}
% u_t(g,x) := -\Big(g(x)+c_t\big(x_t^{(b)},x\big)\Big),
% \end{align}
% while $c_t$ denotes the agent's cost function.
Similarly, we assume the DM does not have access to agents' cost functions $c_t$ and allow these cost functions to be heterogeneous. We define the cost function as follows:

\begin{definition}[Cost function]\label{def:cost-function}
A function $c_t:\mathcal{X}\times\mathcal{X}\to[0,\infty)$ is a cost function for agent $t$ if it satisfies
$c_t\big(x_t^{(b)}, x_t^{(b)}\big) = 0$ and 
$c_t\big(x_t^{(b)}, x\big) > 0, \forall x\neq x_t^{(b)}$.
\end{definition}

% \Alan{That doesn't sound like a definition to me. You can't define a cost function by saying a cost function satisfies this or that. It should be "A function blah blah is a valid cost function if it satisfies blah blah".}

% \Alan{It seems like we never talk about whether the DM has access to this cost function or not, at least in this section. Perhaps worth mentioning this.} {\color{red}Done.}

After agent $t$ reports $x_t$, they receive the final prediction $\hat{y}_t:=g(x_t)$ from the DM and realises the outcome $y_t:=h(x_t,z_t)$. The round concludes.

% \Alan{Worth clarifying that at the same round or the next round? You ended a round just now, does the DM keep getting agents? or the DM moves on to do minimisation? From the text it seems to be the latter but it is not sure because you used sentence "concluding the round".} {\color{red}Done, edited the setup.}

Since all agents have the same inital distribution $P_{X^{(b)},Z}$ and interact with the DM through the same mechanism, we use $P_{X,Z}$ to denote the joint distribution that governs the pairs $(x_t,z_t)$ of all agents and similarly, $P_{X,Y}$ for the case of $(x_t,y_t)$.
Then, the DM's objective is to minimise their expected predictive error: 
$$\min_{g}\mathbb{E}_{P_{X,Y}}\left[\big(g(X_t)-Y_t\big)^2\right] 
= \min_{g}\mathbb{E}_{P_{X,Z}}\left[\big(g(X_t)-h(X_t,Z_t)\big)^2\right],$$
where $P_{X,Z}$ and $P_{X,Y}$ depends on the DM's choice for $g$ due to agents' strategic behaviour. In the context of insurance pricing, this objective reflects the companyâ€™s effort to align insurance premiums with customersâ€™ expected accident costs. Premiums that are too low may lead to financial losses, while setting them too high could drive potential customers to competitors, resulting in lost business opportunities.

% \Alan{It is now unclear under your setting where agents come sequentially to the DM, and there is only one agent per round t, how could they even perform such optimisation? there is only one realisation from the random variable $X_t$? You also explicitly stated that agents are independent of each other, which I assume it is the random variables that are independent.} {Done, I rewrote the setting to avoid the sequential nature of rounds.}


\section{Strategic Learning with Explanations}\label{sec:sl-with-explanations}

%\subsection{General Setup}\label{subsec:slwe}
We formalise the strategic learning scenario in which DM provides model explanations to agents instead of directly disclosing the predictive model. In this setting, to enable the DM to offer explanations tailored to individual agents---commonly referred to as \emph{local explanations} in the explainable machine learning community---we modify the interaction process so that each agent takes two sequential actions within a single round. 
% As in the standard setup, the DM interacts with $T$ agents separately; however
Specifically, in each round $t$, the agent first submits their base covariate $x_t^{(b)}$, upon which the DM provides a preliminary decision $\hat{y}_t^{(b)}$ computed with $g\big(x_t^{(b)}\big)$. Instead of disclosing the full predictive model $g$ to the agent, the DM now provides an explanation detailing how the preliminary decision $\hat{y}_t^{(b)}$ was computed.

% In our setting, to equip the DM with explanations tailored to individual agents, referred to as local explanations, we modify the interaction order in each round $t$ so that every agent makes two moves. Like in the standard setting, we assume that a DM interacts with $T$ agents sequentially, whereas in each round $t$, the agent will first report their base covariate $x_t^{(b)}$ and receive the preliminary decision $\hat{y}_t^{(b)}$ computed with $g\big(x_t^{(b)}\big)$. Instead of disclosing the model $g$, the DM will now provide an explanation on how $\hat{y}_t^{(b)}$ is computed. 

\begin{definition}[Explanations]
    An explanation method is a tuple $(\mathcal{E}, \sigma)$ where $\mathcal{E}$ is the space of feasible explanations and $\sigma:\mathcal{X}\times\mathcal{G}\to\mathcal{E}$ is an explanation policy that picks an explanation $e\in\mathcal{E}$ for the agent with base covariate $x\in\mathcal{X}$ w.r.t. the predictive model $g\in\mathcal{G}$. It is said to be a \emph{global} explanation if $\sigma$ is a constant function w.r.t. the value $x$. Otherwise, $\sigma$ is said to generate \emph{local} explanations. 
    % \Alan{A constant function with respect to the first argument only. Otherwise your definition of global explanation is weird.}
\end{definition}

% \Alan{The notation $\mathcal{G}$ was not introduced a priori. }

%\textbf{Explanations.} An explanation method employed by the DM is characterised by a tuple $(\mathcal{E}, \sigma)$ where $\mathcal{E}$ denotes the space of all possible explanations that the DM could provide while $\sigma:\mathcal{X}\times\mathcal{G}\to\mathcal{E}$ is an explanation policy that dictates how to pick an explanation $e_t\in\mathcal{E}$ for the agent with base covariate $x_t^{(b)}$ and is subject to the predictive model $g\in\mathcal{G}$.
% \paragraph{The modified Stackelberg game.} 
% We assume that there exists a DM who engages sequentially with different agents over $T$ rounds, indexed by $t\in[T]$, with one unique agent participating in each round. In each round $t$, an agent arrives with their base covariate $x_t^{(b)}$ that is drawn randomly from the space $\mathcal{X}$ that is fixed for all agents. This agent $t$ has the potential outcome $y_t^{(b)}:=h(x_t^{(b)},z_t)$ (corresponding to the specific value $x_t^{(b)}$) that is governed by the underlying outcome function $h$, the base covariate $x_t^{(b)}$ and the unobserved variable $z_t\in\mathcal{Z}$. The outcome function $h:\mathcal{X}\times\mathcal{Z}\to\mathcal{Y}$ is the same for all agents. After the base covariate $x_t^{(b)}$ is reported to the DM, they attempt to predict this agent's outcome with the model $g:\mathcal{X}\to\mathcal{Y}$ with the prediction $\hat{y}_t^{(b)}:=g(x_t^{(b)})$. The DM also commits to an explanation method, which is a tuple $(\mathcal{E}, \sigma)$, where $\mathcal{E}$ denotes the space of all possible explanations that the DM could give to an agent while $\sigma:\mathcal{X}\times\mathcal{G}\to\mathcal{E}$ is an explanation policy that dictates how to pick an explanation $e_t\in\mathcal{E}$ for this agent $t$ who has the base covariate $x_t^{(b)}$ and is subject to the predictive model $g\in\mathcal{G}$. 
%Such an explanation policy $\sigma$ gives individual agents the possibility to receive personalised explanations. Specifically, when an explanation policy $\sigma$ is a constant function, i.e., agents receive the same explanation $e\in\mathcal{E}$ regardless of $x_t^{(b)}$, we say the DM uses a global explanation method, otherwise, $\sigma$ is said to generate local explanations.
The explanation space $\mathcal{E}$ is general and depends on the choice of the explanation method.
For example, the DM could employ a global surrogate model, such as a linear model or decision tree, to approximate the predictive model $g$~\citep{molnar2020interpretable}. In this case, all agents receive the same explanation $e_t=f_\bullet$ where $f_\bullet:\mathcal{X}\to\mathcal{Y}$ is a surrogate function belonging to the explanation space $\mathcal{E}\subseteq\mathcal{F}=\{f\,|\,f:\mathcal{X}\to\mathcal{Y}\}$. Alternatively, when the covariate $x_t^{(b)}\in\mathcal{X}\subseteq\mathbb{R}^d$ consists of $d$ features, attribution-based methods such as SHAP~\citep{lundberg2017unified} provide explanations $e_t=(e_{t1}, \ldots, e_{td})\in\mathcal{E}\subseteq\mathbb{R}^d$ in the form of importance scores for each features in $x_t^{(b)}$. Several other explanation methods fit within this framework, and we provide a more comprehensive discussion of them in \Cref{apx:explanation-methods}.

Upon receiving their tailored explanation $e_t$, agent $t$ strategically modifies their covariate from $x_t^{(b)}$ to $x_t$. To formalise this procedure, since agents may respond differently to different types of explanations, we assume here a general form of agent's reaction model with $x_t:=\psi(x_t^{(b)},e_t,z_t)$ where $\psi$ is a deterministic and measurable function. 
% The agent's strategic modification can thus be expressed as $\Delta x_t := x_t - x_t^{(b)}$. 
Under this setting, the agents cannot query their true utility (\Cref{eq:ag-true-utility}) nor respond optimally without knowledge of $g$, unlike in the previous setting. Instead, their best response has to be based on the provided explanation $e_t$ and their reaction model $\psi$.

% While agent $t$'s true utility function is still the one in \Cref{eq:ag-true-utility}, they cannot best respond against it without knowledge of $g$, unlike in the previous setting. Consequently, their best response now is with respect to the provided explanation $e_t$ and is dictated by the reaction model $\psi$.

% \Alan{What do they cannot best respond even mean?}

After reporting $x_t$, the agent receives the final prediction $\hat{y}_t$ and realises the outcome $y_t$, concluding the round. We consider the scenario in which the DM's objective is to choose a predictive model $g$ and explanation method, characterised by $(\mathcal{E}, \sigma)$, that jointly minimise the predictive error, under the agents' strategic behaviour, as follows:
\begin{align}\label{eq:dm-objective}
% \MoveEqLeft\min_{g,(\mathcal{E},\sigma)}\mathbb{E}_{X_t,Y_t}\left[\big(g(X_t)-Y_t\big)^2\right]
% \\
% = &
\min_{g,(\mathcal{E},\sigma)}\mathbb{E}_{P_{X,Z}}\left[\big(g(X_t)-h(X_t,Z_t)\big)^2\right]
,
\end{align}
where the DM's choice of both models affect the distribution of agents' strategic responses $X_t$.\footnote{For brevity, we omit the explicit connection between $X_t$ and its optimisation arguments from the objective function, though a full expansion is provided in \Cref{subapx:expanded-obj}.}


\subsection{Optimality of Local Explanations}
% We show that as local explanations allow the DM to incentivise individual agents in a personalised manner, they can lead to better outcomes than simply giving all agents the same explanation.

We demonstrate that local explanations, by enabling the DM to incentivise individual agents, can achieve more favorable outcomes compared to global explanations, which provide the same, non-personalised information to all agents.

\begin{proposition}\label{prop:localisbetter}
Suppose that the DM's objective follows \Cref{eq:dm-objective} and denote $l\big(x_t^{(b)},e_t,z_t\big)=(g(x_t)-y_t)^2$. For a given predictive model $g$ and an explanation space $\mathcal{E}$, 
$$\mathbb{E}_{X_t^{(b)}}\left[\min_{e_t\in\mathcal{E}}\mathbb{E}_{Z_t}\left[l\big(X_t^{(b)},e_t, Z_t\big)\ \big\lvert\ X_t^{(b)}\right]\right] 
\leq \min_{e\in\mathcal{E}}\mathbb{E}_{X_t^{(b)},Z_t}\left[l\big(X_t^{(b)},e, Z_t\big)\right].$$
\end{proposition}

This proposition implies that, for a given choice of $g$ and $\mathcal{E}$, the minimal predictive loss had we release local explanations is at least as small as had we released global explanations. This is because local explanations provide the DM with more fine-grained control over, and better anticipation of, agents' responses. 
% \Alan{Describe in english what the last two inequalities means. Something like "the minimal predictive loss had we release local explanation is at least as small had we released global explanations." It's not nice just to write an equation and leave it there fore people to decipher your work.}
% Consequently, for a given choice of $g$ and $\mathcal{E}$, it would be in the DM's best interest to design an explanation policy $\sigma$ that generates local explanations.
This result holds regardless of the choice of DM's objective function, such as classification loss considered in \citet{cohen2024bayesian}, agents' improvement in \citet{vo2024causal}, or social welfare in \citet{xie2024non}. 

However, since the DM can strategically design explanations to serve their own interests, it becomes crucial to ensure that agents are not misled into making decisions that ultimately harm their own utilities. In the next section, we explore strategies to prevent explanations from leading agents to unfavorable actions.

% This result can be generalised to any objective function of the DM that relies on the strategic behaviour of agents, such as classification loss~\citep{cohen2024bayesian}, agents' improvement~\citep{vo2024causal}, or social welfare~\citep{xie2024non}. This is because local explanations allow the DM to have more fine-grained control over and better anticipation of agents' reactions. Given that the DM can manipulate explanations for their own benefit, in the next section, we look at how to prevent explanations from misleading agents into taking actions that harm their own utilities.

\section{Agents' Reactions under Local Explanations}

Since the agent does not have access to the actual predictive model $g$, their best response $x_t$, which is influenced by the explanation $e_t$, might be suboptimal and can even lead to a reduction in their utility (in \Cref{eq:ag-true-utility}). We formally define such a property of an agent's responses below.

\begin{definition}[Non-harmful responses]\label{def:no-harm}
Let $\nu_t=\left\{x\in\mathcal{X}: u_t(g,x)\geq u_t\big(g,x_t^{(b)}\big)\right\}$. If an agent's response $x_\bullet$ belongs to this set, we call it a non-harmful response.
\end{definition}

% \Alan{The part above and below feels very disconnected. Why are you defining what is desirable, and then suddenly say you analyse three types of local explanations that are commonly use? what is your analysis for?}

To better understand the extent to which explanation methods might induce harmful responses from agents and to develop preventive measures, we examine several common types of explanation methods in this section. This analysis uncovers patterns and provides a foundation for identifying and characterising classes of explanation methods that ensure only non-harmful responses from strategic agents.

\subsection{Feature Attributions}
Feature-attribution methods, such as LIME~\citep{ribeiro2016should} and SHAP~\citep{lundberg2017unified}, assign importance scores to input features in an attempt to explain each featureâ€™s contribution to a specific prediction. While SHAP has been applied in sequential decision-making settings~\citep{rodemann2024explaining,adachi2024looping}, these applications typically do not account for agentsâ€™ strategic responses. However, as highlighted by \citet{wachter2017counterfactual} and \citet{molnar2020interpretable}, feature-attribution methods do not offer direct guidance on how features should be adjusted to achieve a desired prediction outcome. To illustrate this limitation further, we present a toy example in \Cref{apx:shapley}.

% Feature-attribution methods, such as LIME~\citep{ribeiro2016should} and SHAP~\citep{lundberg2017unified}, assign importance scores to input features as an attempt to explain each feature's contribution to a particular prediction. SHAP has also been utilised for explanation under sequential decision-making settings, but without considering strategic responses~\citep{rodemann2024explaining,adachi2024looping}. In fact, as noted by \citet{wachter2017counterfactual} and \citet{molnar2020interpretable}, attribution methods do not provide any direct guidance on how features should be modified to improve the prediction. To further enhance this point, we provide a toy example in \Cref{apx:shapley}.

% Although these methods are effective in explaining each feature's contributions to a prediction, they do not provide direct guidance on how a feature should be modified to improve the prediction, as noted by \citet{wachter2017counterfactual, molnar2020interpretable}. Although the use of Shapley values for improving predictions through feature modification has been studied before by \citet{chen2020true}, their heuristic approach might not always be applicable in practice. To further illustrate these points, we provide a toy example in \Cref{apx:shapley}.

% how feature attributions might not correctly reflect the local structure of the true predictive model, {\color{red}we present an example for the case of Shapley values below}.

% \Cref{example:shapley} in \Cref{apx:shapley} highlights that feature-attribution explanations do not provide clear guidance on how to modify features, potentially misleading agents into taking actions that result in unfavorable outcomes. 

%In the next subsection, we examine how local explanations, even when providing a clear guidance on the direction of improvement, e.g., the gradient, do not always guarantee that agents' actions will be free of negative consequences, as they fail to take into account the heterogeneity of agents' cost functions.

% (There is a work that looks into how agents can change their values using Shapley values, by \citet{chen2020true}, by changing their features to the feature mean of the population, however, their work is heuristic and in reality, agents do not know the mean to do the imputation.)


\subsection{Local Surrogate Models}
% Similar to the common best-response model in Strategic Learning literature [REFs], we assume the agent $t$ wants to minimise the predictive score while taking into account the cost and thus has the following utility function:
% \begin{align}
% u_t(g, x) = -\Big(g(x)+c_t(x_t^{(b)},x)\Big),
% \end{align}
% where $x$ denotes an arbitrary covariate value in $\mathcal{X}$ that this agent $t$ can modify $x_t^{(b)}$ into.

When the agent only has access to the surrogate function $f_t$  as an approximation to the actual predictive model $g$, it is commonly assumed that the agent will optimise their action based on the corresponding surrogate utility function~\citep{jagadeesan2021alternative,bechavod2022information,xie2024non}: 
$$u_t(f_t, x) = -\Big(f_t(x)+c_t\big(x_t^{(b)},x\big)\Big).$$
Hence, the agent's best response becomes
$$x_t := \arg\min_{x}\Big(f_t(x)+c_t\big(x_t^{(b)},x\big)\Big).$$ 
Since the surrogate utility function differs from the true utility function defined in \Cref{eq:ag-true-utility}, the agent's best response $x_t$ might lead to a reduction in their true utility. To mitigate such risks, we establish the following necessary condition to safeguard against such situations.

\begin{theorem}[Necessary condition]\label{theorem:surrogate-necessary}
Given an agent $t$ with the base covariate $x_t^{(b)}$ who best responds against the surrogate utility function $u_t(f_t,\cdot)$. If it holds, for every possible cost function $c_t$ (\Cref{def:cost-function}), that the resulting best response $x_t$ belongs to the non-harmful set $\nu_t$ (\Cref{def:no-harm}), i.e.,
$u_t(g,x_t)\geq u_t\big(g,x_t^{(b)}\big)$,
then the following also holds:
\begin{align}\label{eq:surrogate-necessary-cond}
f_t\big(x_t^{(b)}\big) - f_t(x) \leq g\big(x_t^{(b)}\big) - g(x) \quad \forall x \in \mathcal{X}_t^{g^{-}},
\end{align}
where $\mathcal{X}_t^{g^{-}}:=\{x : g(x) < g(x_t^{(b)})\}$ is the set of potential responses with lower prediction scores for the agent.
% Let $\mathcal{X}_t^{g^{-}}=\{x : g(x) < g(x_t^{(b)})\}$ be the set of covariate values that reduce the agent's predictive score and $f_t$ be a local surrogate function designed based on $x_t^{(b)}$, if the condition 
% \begin{align}\label{eq:surrogate-necessary-cond}
% f_t(x_t^{(b)}) - f_t(x) \leq g(x_t^{(b)}) - g(x) \quad \forall x \in \mathcal{X}_t^{g^{-}}
% \end{align}
% is violated then there exists a cost function $c_t$ such that the agent's best response $x_t$ (w.r.t $f_t)$ reduces their true utility, i.e.,
% \begin{align}
% u_t(g, x_t) < u_t(g, x_t^{(b)}).
% \end{align}
\end{theorem}

This theorem implies that if the DM provides a surrogate function $f_t$ as an explanation to an agent $t$, but the condition in \Cref{eq:surrogate-necessary-cond} is violated, then there exists a cost function $c_t$ under which the agent is misled into taking a response $x_t$ whose cost outweighs the gain, ultimately resulting in a reduction in their true utility. Consequently, \Cref{eq:surrogate-necessary-cond} is a necessary condition that safeguard against such situations and if it holds for all $x\in\mathcal{X}$ instead of $\mathcal{X}_t^{g^-}$, it becomes a sufficient condition. A toy example in \Cref{apx:misled-agent-example} is provided to illustrate a scenario where an agent is misled into taking an overly costly action.

% The following theorem shows that when the surrogate function $f_t$ has larger rate of change than that of $g$, this might give an agent $t$ a false sense of improvement and thus, the action that is optimal for the surrogate objective $u_t(f_t,\cdot)$ might  harm the true objective $u_t(g,\cdot)$ of this agent. We refer to the \Cref{eq:surrogate-necessary-cond} as the necessary condition for ensuring no reduction of an agent's true utility (\Cref{def:no-harm}). Moreover, if the condition in \Cref{eq:surrogate-necessary-cond} holds for all $x\in\mathcal{X}$, then we obtain a sufficient condition.



In short, when an agent receives a surrogate function as an explanation, the DM must \textit{at least} ensure that this surrogate function does not exaggerate the gain this agent would obtain, as outlined in \Cref{eq:surrogate-necessary-cond}, in order to avoid misleading them into taking harmful actions.


\Cref{theorem:surrogate-necessary} also applies to more general agents' reaction models where agents interpret explanations as some surrogate functions such as in feature-additive methods~\citep{luexplainable} or where agents have some beliefs about the unknown function $g$ ~\citep{cohen2024bayesian}.


%\begin{corollary}[Necessary condition]\label{corollary:general-necessary}
%For any agent with a tuple $(x_t^{(b)}, c_t, z_t)$, suppose that 
%\begin{enumerate*}[label=(\arabic*)]
%    \item $\Tilde{g}_t$ is a random variable distributed according to the agent's ``prior'' belief $\Tilde{p}(\Tilde{g}_t)$, about the unknown predictive model $g$, assuming $\Tilde{p}(\Tilde{g}_t)$ is well defined, following \citet{cohen2024bayesian},
%    \item $\Tilde{p}\big(\Tilde{g}_t|e_t\big)\propto\Tilde{p}\big(e_t|\Tilde{g}_t\big)\Tilde{p}\big(\Tilde{g}_t\big)$, is the posterior belief of this agent upon receiving the explanation $e_t$,
%    \item $f_t$ denotes the agent's updated belief about the unknown $g$, defined as $f_t(x) := \mathbb{E}_{\Tilde{g}}\left[\Tilde{g}_t(x)\ \big\lvert\ e_t\right] \quad \forall x\in\mathcal{X}$,
%\end{enumerate*}
%then the result in \Cref{theorem:surrogate-necessary} applies.
%\end{corollary}

\begin{corollary}[Necessary condition]\label{corollary:general-necessary}
For any agent with a tuple $(x_t^{(b)}, c_t, z_t)$, suppose that 
\begin{enumerate}[label=(\arabic*)]
    \item $\theta_t\in\Theta\subset\mathbb{R}^d$ is a random variable distributed according to the agent's prior $p(\theta_t)$ over the unknown parameter $\theta_0$ of the true predictive model $g_{\theta_0}$,
    \item $p(\theta_t|e_t)\propto p(e_t|\theta_t)p(\theta_t)$ is the posterior belief of this agent upon receiving the explanation $e_t$,
    \item $f_t$ represents the agent's updated belief about $g_{\theta_0}$ defined as $f_t(x) := \mathbb{E}_{\theta_t}[g_{\theta_t}(x)\ \lvert\ e_t],  \forall x\in\mathcal{X}$.
\end{enumerate}
Then, the result in \Cref{theorem:surrogate-necessary} extends to this setting.
\end{corollary}


\subsection{Action Recommendation-based Explanations}\label{subsec:action-rec}

Next, we consider the scenario in which the explanation provided to agent $t$ takes the form $e_t=(x_t^{(r)}, \hat{y}_t^{(r)})$, where  $x_t^{(r)}$ is a recommended covariate update suggested by the DM, and $\hat{y}^{(r)}_t$ is the corresponding predicted outcome if the agent follows this recommendation.
% contains a recommended covariate update $x_t^{(r)}$ suggested by the DM and its corresponding prediction $\hat{y}^{(r)}_t$ that the agent will receive if they follow this recommendation. 
We refer to this type of explanations as \textit{action recommendation (AR)-based explanations}. There are various ways to design the explanation policy $\sigma:\mathcal{X}\times\mathcal{G}\to\mathcal{X}\times\mathcal{Y}$ in practice, one of which is to have $\sigma$ recommend a minimal change in an agent's features that allows this agent to obtain the desired prediction value. The explanations generated by such a $\sigma$ is commonly referred to as counterfactual explanations in the explainable ML literature~\citep{molnar2020interpretable}. From here onwards, we use the term AR-based explanation to refer to any explanation of the general form $e_t=(x_t^{(r)}, \hat{y}_t^{(r)})$, and when it is clear from the context that $e_t$ is generated by a specific $\sigma$ used in the counterfactual explanations literature (e.g., \citealt{wachter2017counterfactual}), then we call it a counterfactual explanation. This allows us to provide an analysis for this class of explanations without restricting ourselves to any specific assumption on the behaviour of $\sigma$. It also emphasises the fact that the explanation $e_t=(x_t^{(r)}, \hat{y}_t^{(r)})$ provided to the strategic agent $t$ is a guidance, rather than a counterfactual scenario.

Following \citet{tsirtsis2020decisions}, we assume the agent's best response $x_t = \psi(x_t^{(b)},e_t,z_t)$ is as follows:
\begin{align}\label{eq:ar-agent-model}
x_t 
% &:= \psi(x_t^{(b)},e_t,z_t) \nonumber
% \\
&:= 
\left\{\begin{aligned}
    &x_t^{(r)}\quad \text{if}\ u_t\big(g,x_t^{(r)}\big)\geq u_t\big(g,x_t^{(b)}\big)
    \\
    &x_t^{(b)}\quad \text{otherwise},
\end{aligned}\right.
\end{align}
where the agent will update their covariate into the recommended value $x_t^{(r)}$ if the corresponding utility value is at least as good as that of the base value $x_t^{(b)}$.
The reason an agent might break ties in favour of the recommended action is because in many application domains, obtaining a more favourable prediction $\hat{y}$ is likely to result in better long-term well-being for them, such as an improvement in one's financial status or in our insurance-pricing example, lowering one's risk of getting into car accidents.

Since the agent has access to both $\hat{y}_t^{(b)}$, $\hat{y}_t^{(r)}$, and their own cost function $c_t$, they can evaluate the two utility values, $u(g,x_t^{(r)})$ and $u(g,x_t^{(b)})$. Under this scenario, the agent's best response can never harm their true utility.

\begin{remark}\label{remark:ar-noharm}
For an agent $t$, any AR-based explanation policy $\sigma:(x_t^{(b)},g)\mapsto(x_t^{(r)},\hat{y}_t^{(r)})$ will induce a best response $x_\bullet$ that belongs to the set of this agent's non-harmful actions $\nu_t$. The response $x_\bullet$ does not have to coincide with $x_t^{(r)}$.
\end{remark}

In the following, we study the sufficiency property of AR-based explanations. To facilitate 
the analysis, the following assumption is introduced.

\begin{assumption}[Conditional homogeneity of agents' responses]\label{assumption:subhomo-response}
Given an arbitrary explanation method characterised by $(\mathcal{E},\sigma)$, for any subset of $T^\prime$ agents who share the same base covariate and receive the same explanation, i.e., $(x_t^{(b)}, e_t)=(x^{(b)},e)$ for all $t\in[T^\prime]$, their responses must be identical: $x_t=x_\bullet$ for all $t\in[T^\prime]$ and for some $x_\bullet\in\mathcal{X}$.
\end{assumption}

\Cref{assumption:subhomo-response} may seem too strong at first glance, but it is not unusual and is typically implied in prior works of information design~\citep{bergemann2019information} and when Bayesian persuasion is applied in strategic learning~\citep{harris2022bayesian}. In these works, a decision maker (or equivalently, information designer) is expected to know how agents will react so that their action recommendation policy can induce obedience. In contrast, our \Cref{assumption:subhomo-response} is a weaker condition. Instead of requiring the DM to have full knowledge of the agents' reaction model, it assumes that the DM possess ``\textit{just enough}" information---captured by $x_t^{(b)}$---such that when conditioning on such information, the unobserved parts of agents that influencing their responses are homogeneous. This means that while agents may respond in diverse ways when considered across the entire population, their responses become \emph{conditionally} homogeneous given the available information. 


% Consequently, even though their responses are heterogeneous population-wise, \textit{conditionally}, those responses are homogeneous.

In reality, this assumption can be enforced by letting the DM query more information about agents when generating explanations, even though such information might not be used for generating predictions. For instance, for an insurance company to propose a recommended action to a customer, they might ask the customer about what actions are feasible or not feasible, and how much they are willing to invest into modifying their features.

% Thanks to the assumed agents' behaviour as specified in \Cref{eq:ar-agent-model}, AR-based explanations constitute a sufficient class of explanation methods that induce non-harmful agents' responses. Specifically, any non-harmful response (\Cref{def:no-harm}) that is induced by an arbitrary explanation method $\sigma:\mathcal{X}\times\mathcal{G}\to\mathcal{E}$, can also be induced by an AR-based explanation policy $\sigma^\prime:\mathcal{X}\times\mathcal{G}\to\mathcal{X}\times\mathcal{Y}$. We formally state this result in the next theorem.

% \begin{theorem}\label{theorem:ar-sufficiency}
% Given any agent $t$ and 
% \begin{enumerate}
%     \item let $\nu_t$ denote the set of responses that do not harm this agent's true utility (\Cref{def:no-harm}),
%     \item assume that this agent's reaction model in the case of AR-based explanations is as specified in \Cref{eq:ar-agent-model},
% \end{enumerate} 
% for any local explanation method $\sigma:(x_t^{(b)},g)\mapsto e_t$ that induces a best response $x_\bullet=\psi(x_t^{(b)},e_t,z_t)$ such that $x_\bullet\in\nu_t$, then there exists an AR-based explanation policy $\sigma^\prime:(x_t^{(b)},g)\mapsto(\Delta x_t^{(r)},\hat{y}_t^{(r)})$ that induces the very same best response $x_\bullet$.
% \end{theorem}

\begin{theorem}[Sufficiency of AR-based explanations]\label{theorem:ar-sufficiency}
For a subset of $T^\prime$ agents with the same base covariate $x_t^{(b)}=x^{(b)}, \forall t\in[T^\prime]$, let $\nu_t$ be the set of non-harmful responses of agent $t$ (\Cref{def:no-harm}) and assume that these agents' reaction model follows \Cref{eq:ar-agent-model}.
If the agents' responses are conditionally homogeneous (\Cref{assumption:subhomo-response}), all $T^\prime$ agents have the same set of non-harmful responses, i.e., $\nu_t=\nu, \forall t\in[T^\prime]$. 

Moreover, for any explanation $e$ generated by an arbitrary local explanation method $(\mathcal{E}, \sigma)$, i.e., $\sigma:(x^{(b)},g)\mapsto e$, such that $e$ induces a best response where $x_\bullet=\psi(x^{(b)},e,z)\in\nu$, there exists an AR-based explanation method $(\mathcal{E}^\prime,\sigma^\prime)$ such that $\sigma^\prime:(x_t^{(b)},g)\mapsto(x_t^{(r)},\hat{y}_t^{(r)})$ induces the same best response $x_\bullet$.
\end{theorem}

This theorem implies that when evaluating the impact of a DM's predictive model, it suffices to focus solely on AR-based explanations. If an optimal explanation method is to be identified, searching within the AR-based explanation class is sufficient. Consequently, if the no-harm constraint is imposed on any other class of explanations, e.g., LIME, then the optimal explanation policy in this class cannot do better than the optimal AR-based explanation policy, regardless of the DM's objective function.

% This theorem implies that, when analysing the consequences of a DM's predictive model, it is sufficient to restrict our attention to the class of AR-based explanations and if an optimal explanation method is to be found, it is sufficient to search within this class of AR-based explanations. It also means that, when using specific classes of explanation policies, such as Taylor's expansions~\citep{xie2024non} or subsets of hypothesis class~\citep{cohen2024bayesian}, if the no-harm constraint in \Cref{def:no-harm} is imposed, then the optimal policies of those classes will not be better than the optimal AR-based explanation policy, regardless of the DM's objective function.

Our result is similar in spirit to the revelation principle in information design; see, e.g., Proposition 1 of \citet{bergemann2019information}, which says that the class of Bayes correlated equilibria (BCE)\footnote{This result of \citet{bergemann2019information} generalises the idea of \textit{straightforward signal} from Bayesian persuasion~\citep{kamenica2011bayesian} to the multi-agent environment, such signal is also called Bayesian incentive-compatible~\citep{harris2022bayesian}.} suffices to rationalise any outcome in an incomplete information game, i.e., where players have only partial information. 
However, a key distinction is that BCE is a stronger notion than our setup of AR-based explanations because BCE can be viewed as an AR-based explanation policy that satisfies the obedience-inducing constraint. In addition, while the class of BCE is sufficient to rationalise any agents' behaviour, the class of AR-based explanations (in our strategic learning setup) is sufficient to rationalise any \textit{non-harmful} agents' behaviour.

% Our result is similar in spirit to the Revelation Principle in Information Design; see, e.g., Proposition 1 of \citet{bergemann2019information}, which says that the class of Bayes Correlated Equilibria (BCE)\footnote{This result of \citet{bergemann2019information} is a generalisation of the \textit{straightforward signal} from Bayesian persuasion~\citep{kamenica2011bayesian} to the multi-agent environment. Such a \textit{straightforward signal} is also called Bayesian incentive-compatible (BIC)~\citep{harris2022bayesian}.} suffices to rationalise any outcome in an incomplete information game, i.e., where players have only partial information. 
% However, several core distinctions must be noted. Firstly, BCE is a stronger notion than our setup of AR-based explanations because a BCE can be viewed as an AR-based explanation policy satisfying an obedience-inducing constraint. The concept of BCE is useful when considering a more general setting without any explicit restriction on the set of agents' responses (e.g., without no-harm in \Cref{def:no-harm}). On the other hand, when restricting explanation methods to the ones that must induce non-harmful reponses (i.e., $\nu_t$ in \Cref{def:no-harm}) no additional constraint (on obedience or persuasion) is necessary for our setup of AR-based explanations. Secondly, identifying a BCE is harder than simply picking an arbitrary AR-based explanation policy since evaluating the obedience-inducing constraint requires complete knowledge of an agent's reaction model, e.g., knowledge of the unobserved variable $z_t$ or cost function $c_t$.


\section{Algorithm with AR-based Explanations}

% As shown in Section~\ref{subsec:action-rec}, AR-based explanations are desirable because they do not mislead agents into taking harmful actions and under a mild assumption on conditional homogeneity, they are also sufficient to induce any agents' behaviour. In this section, we provide a simple algorithm for the DM to operationalise AR-based explanations such that they can maximise their utility, i.e., minimising the prediction error.

As demonstrated in Section~\ref{subsec:action-rec}, AR-based explanations are desirable because they prevent agents from being misled into taking harmful actions. Furthermore, under a mild assumption of conditional homogeneity, these explanations are sufficient to induce any agentsâ€™ non-harmful responses. In this section, we present an algorithm that enables the DM to operationalise AR-based explanations, allowing them to maximise utility by minimising prediction error.


Specifically, given the space of AR-based explanations $\mathcal{E}=\mathcal{X}\times\mathcal{Y}$, the DM aims to find the optimal predictive model $g$ and the explanation policy $\sigma$ that jointly minimise the predictive loss in \Cref{eq:dm-objective}, which is now rewritten as
{\begin{align}
% \MoveEqLeft\min_{g,\sigma}\mathbb{E}_{X_t,Z_t}\left[\Big(g(X_t)-h(X_t,Z_t)\Big)^2\right]
% \nonumber \\
% =&
\min_{g,\sigma^r}\mathbb{E}_{P_{X,Z}}\left[\Big(g(X_t)-h(X_t,Z_t)\Big)^2\right] \label{eq:dm-objective-with-ar}
,
\end{align}}
where $\sigma^r:x_t^{(b)}\mapsto x_t^{(r)}$ denotes the mapping that generates the recommended action $x_t^{(r)}$. In particular, the AR-based explanation policy $\sigma:\big(x_t^{(b)},g\big)\mapsto\big(x_t^{(r)},g\big(x_t^{(r)}\big)\big)$ is decomposed into two independent components: $g$ and $\sigma^r$. Consequently, \Cref{eq:dm-objective-with-ar} follows and it is possible to jointly optimise both $g$ and $\sigma^r$.

\paragraph{Repeated risk minimization (RRM).} In practice, because the DM does not have access to the outcome function $h$ and the unobserved variable $z_t$, an efficient way to solve \Cref{eq:dm-objective-with-ar} is through the repeated risk minimisation (RRM) procedure~\citep{perdomo2020performative}. While training $g$ using RRM is straightforward, the same does not apply to $\sigma^r$, as this requires the DM to simulate how the loss function changes whenever $\sigma^r$ is updated. This simulation is only possible if the DM can anticipate how agents will respond to changes in $\sigma^r$. The idea of learning a response function of heterogeneous agents has been considered in prior work~\citep{xie2024non}, although for the case of local surrogate models as explanations. Here, we propose a similar approach tailored to AR-based explanations.
Specifically, the DM can interact with agents through randomised recommended actions to learn a binary classifier $\xi:\big(x_t^{(b)}, x_t^{(r)}, \Delta g_t^{(r)}\big)\mapsto \hat{w}_t$ that predicts their compliance behaviour $w_t$. Compliance is defined as $w_t=1$ when the agent follows the recommendation, i.e., $x_t=x_t^{(r)}$,  and $w_t=0$ otherwise. The term $\Delta g_t^{(r)}:=g\big(x_t^{(b)}\big)-g\big(x_t^{(r)}\big)$ denotes the gain in prediction value for the agent $t$, serves as a useful feature for this classifier, as we utilise the assumed agents' behaviour defined in \Cref{eq:ar-agent-model} and \Cref{eq:ag-true-utility}. 
% \Alan{Is this justified empirically? did someone else also did that? the justification of including this as an additional feature is unclear.} {Done, edited.}

To generate necessary data to learn the classifier $\xi$, the DM can employ a sampler $\pi$ to generate randomised recommended actions as follows: $X_t^{(r)}\mid x_t^{(b)} \sim \pi(X_t^{(r)}, x_t^{(b)})$.
Once $\xi$ is learned, the DM can simulate an agent's response as $\hat{x}_t := \hat{w}_t x_t^{(r)} + (1-\hat{w}_t) x_t^{(b)}$.

Putting everything together, in the finite-sample case, the empirical objective for the DM in the $i$-th iteration of the RRM procedure is then
$$(g_{i},\sigma_{i}^r)=\arg\min_{g,\sigma^r}\frac{1}{T_i}\sum_{t\in[T_i]}(g(\hat{x}_t)-y_t)^2,$$
where $\hat{x}_t$ refers to the simulated agent's response based on the recommended action $x_t^{(r)}:=\sigma^r_i(x_t^{(b)})$ and the inferred reaction model $\xi$. In contrast, $\{y_t\}_{t\in[T_i]}$ are the outcomes of $T_i$ agents collected from when the previous models $(g_{i-1},\sigma^{r}_{i-1})$ are deployed, as usually done in the RRM procedure~\citep{perdomo2020performative}. We provide more details in \Cref{algo:main}.

\begin{algorithm}[t!]
\caption{Learning $g$ and $\sigma$.}
\label{algo:main}
\textbf{Require:} Dataset $D_1=\{x_t,y_t\}_{t\in[T]}$ and the sampler $\pi$.\\
\textbf{Parameters:} $T, T^\prime$, and $\{T_1,\ldots,T_m\}$.

\begin{algorithmic}[1]
\STATE Pre-train $g$ and $\sigma^r$ on $D_1=\{x_t,y_t\}_{t\in[T]}$ with the objectives:
\begin{align*}
g_0 &= \arg\min_g \sum_t (g(x_t)-y_t)^2,
\\
\sigma^r_0 &= \arg\min_{\sigma^r} \sum_t (\sigma^r(x_t)-x_t)^2.
\end{align*}
\STATE Interact with agents over $T^\prime$ rounds, with $g_0$ and a sampler $\pi$ to collect the dataset $D_2=\{x_t^{(b)}, x_t^{(r)}, \Delta g_t^{(r)}, w_t\}_{t\in[T^\prime]}$, then train $\xi$:
$$\min_{\xi} \sum_{t\in[T]}\big(-w_t\log(\hat{w}_t)-(1-w_t)\log(1-\hat{w}_t)\big).$$
\FOR{$i\in\{1,\ldots,m\}$}
\STATE Interact with agents over $T_i$ rounds with $g_{i-1}$ and $\sigma^r_{i-1}$ to collect the dataset $D_{3,i}=\{x_t^{(b)},y_t\}_{t\in[T_i]}$
\STATE Update $(g_{i},\sigma^r_{i})$ by solving
\begin{align}\label{eq:rrm-update}
\big(g_{i},\sigma^{r}_{i}\big)=\min_{g,\sigma^{r}}\sum_{t\in[T_i]}\left(g(\hat{x}_t)-y_t\right)^2.
\end{align}
\ENDFOR
\STATE Set $(g,\sigma^r):=(g_m,\sigma^r_m)$.
\end{algorithmic}
\end{algorithm}



%%%%%
\section{Experiments}\label{sec:experiments}

\subsection{Operationalising AR-based Explanations}
In this experiment, we apply \Cref{algo:main} to jointly train the predictive model $g$ and the (local) AR-based explanation policy $\sigma$ to minimise the DM's prediction error in \Cref{eq:dm-objective-with-ar}. To demonstrate the effectiveness of our proposed procedure, we compare its performance against two other procedures. In one of which we use RRM to jointly train the predictive model $g$ and a global AR-based explanation policy while in the other case, we also run RRM to train the predictive model $g$ but counterfactual explanations are used as the recommended actions. 

To avoid confusion, we use $g_\text{loc}$ and $\sigma^{r}_\text{loc}$ to denote the trained models resulted from our procedure, whereas $g_\text{glo}$ and $x^{(r)}_\text{glo}$ are used to denote the trained predictive model and the optimal global AR-based explanation resulted from one of the two said baselines. Finally, for the other baseline, we use $g_\text{ce}$ to denote the trained predictive model and $\sigma_\text{ce}$ to denote the policy that generates counterfactual explanations.

\paragraph{Experimental setup.} We construct a synthetic dataset with agents of 3-dimensional (observable) feature vectors $x^{(b)}_t\in\mathbb{R}^3$ and scalar (unobservable) features $z_t\in\mathbb{R}$. Each agent $t$ has the cost function $c_t(x_t^{(b)},x)=|\alpha_t|\|x_t^{(b)}-x\|_2^2$, where $\alpha_t$ is correlated with $x_t^{(b)}$ and $z_t$. All DM's models such as $g,\sigma^{(r)},\xi$ are constructed with 3-layer ReLU networks. For any given predictive model $g$, we use a fixed counterfactual explanations policy $\sigma_\text{ce}$ where an explanation $e_t$ is generated as $(x_t^{(r)},\hat{y}_t^{(r)}) := \sigma_\text{ce}(x_t^{(b)}, g)$. Specifically,
\begin{align*}
x_t^{(r)} &:= \arg\min_{x}\big(g(x) + \|x-x_t^{(b)}\|_2^2\big),
\\
\hat{y}_t^{(r)} &:= g\big(x_t^{(r)}\big).
\end{align*}
See \Cref{apx:detailed-exp} for the additional details.
% $$(x_t^{(r)},\hat{y}_t^{(r)}) := \sigma_\text{ce}(x_t^{(b)}, g),$$
% \begin{align*}
% x_t^{(r)} &:= \arg\min_{x}(g(x) + \|x-x_t^{(b)}\|_2^2)
% \\
% \hat{y}_t^{(r)} &:= g(x_t^{(r)})
% \end{align*}

\begin{table}[t!]
    \centering
    \caption{Our approach achieves the lowest test error, without sacrificing persuasive power. The compliance rates show the portions of agents that follow the DM's recommended actions.}
    \label{tab:result-on-test}
    \begin{tabular}{c|c|c|c|}
    \cline{2-4}
    & $g_\text{loc}$ \& $\sigma^r_\text{loc}$ & $g_\text{glo}$ \& $x^r$ & $g_\text{ce}$ \& $\sigma^r_\text{ce}$ \\ \hline
    \multicolumn{1}{|c|}{strategic nMSE} & \textbf{2e-4} & 123e-4 & 241e-4 \\ \hline
    \multicolumn{1}{|c|}{compliance rate} & \textbf{1.0} & 0.3 & 1.0 \\ \hline
    \end{tabular}
\end{table}

Following \Cref{algo:main}, the DM first pre-trains a predictive model $g_0$ and a local AR policy $\sigma^r_0$ with an offline dataset $D_1=\{x_t,y_t\}_{t\in[T]}$.  Using the sampler $\pi$, the DM then interacts with $T^\prime$ agents to collect the dataset $D_2$. The sampler $\pi$ generates random recommended actions $x_t^{(r)}$ around the neighbourhood of either $x_t^{(b)}$ or $x_t^{(r)}$, which is generated either by $\sigma^{r}_0$ or $\sigma_\text{ce}$. Then, $\xi$ is trained on $D_2$.

In the RRM procedure, if the predictive model $g_0$ is jointly trained with $\sigma^r_0$, this results in the final model $g_\text{loc}$ and $\sigma^r_\text{loc}$ at the end. If $g_0$ is jointly trained with a global recommended action $x^{(r)}$, then we obtain $g_\text{glo}$ and $x^{(r)}_\text{glo}$. This can be done by treating $x^{(r)}$ as a trainable parameter vector and \Cref{eq:rrm-update} can be replaced with 
$$(g_{i},x^{(r)}_{i})=\min_{g,x^{r}}\sum_{t\in[T_i]}\left(g(\hat{x}_t)-y_t\right)^2.$$ 
Finally, if $g_0$ is trained with a fixed policy $\sigma_\text{ce}$, we obtain $g_\text{ce}$.

We then compare the prediction errors between the three approaches on a hold-out test set of strategic agents. For ease of presentation, we scale the mean-squared errors by dividing them with the constant $nc=\frac{1}{T}\sum_{t\in[T]}y_t^{(b)}$ that is independent of the DM's choice of models. We refer to the scaled errors as normalised mean-squared errors (nMSE). If the loss is computed on offline data, i.e., without agents' strategic responses, we simply refer to it as nMSE, otherwise, strategic nMSE.

\begin{figure}
    \centering
    \includegraphics[scale=0.7]{figures/convergence_plot.png}
    \caption{Our approach has the lowest training-loss curve (nMSE) out of the three pairs of predictive model and explanation policy. Each of $g_\text{loc}$ and $g_\text{glo}$ is jointly trained with their respective explanation policy, i.e., either $\sigma^r_\text{loc}$ or $x^{(r)}_\text{glo}$, whereas $g_\text{ce}$ is trained with a fixed counterfactual explanation policy $\sigma_\text{ce}$.}
    \label{fig:result-training}
\end{figure}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.42\linewidth]{figures/no-harm-quartic-2-th-hist.png}
    \hspace{10mm}
    \includegraphics[width=0.42\linewidth]{figures/no-harm-quartic-AR-th-hist.png}
    \caption{2nd-order Taylor expansions as explanations mislead agents into reducing their utilities while AR-based explanations do not. The histograms show changes in agents' utilities after performing best responses.}
    \label{fig:result-noharm}
\end{figure*}

% \begin{figure*}[t]
%     \centering
%     \resizebox{0.99\textwidth}{!}{
%         \includegraphics[height=1in]{figures/convergence_plot.png}
%         \includegraphics[height=1in]{figures/no-harm-quartic-2-th-hist.png}
%         \includegraphics[height=1in]{figures/no-harm-quartic-AR-th-hist.png}
%     }
%     \caption{(left) Our approach has the lowest training-loss curve (nMSE) out of the three pairs of predictive model and explanation policy. Each of $g_\text{loc}$ and $g_\text{glo}$ is jointly trained with their respective explanation policy, i.e., either $\sigma^r_\text{loc}$ or $x^{(r)}_\text{glo}$, whereas $g_\text{ce}$ is trained with a fixed counterfactual explanation policy $\sigma_\text{ce}$. (middle $\&$ right) 2nd-order Taylor expansions as explanations mislead agents into reducing their utilities while AR-based explanations do not. The histograms show changes in agents' utilities after performing best responses. }
%     \label{fig:result-combined}
% \end{figure*}




\paragraph{Results.} \Cref{fig:result-training} shows the training loss (nMSE) with RRM and \Cref{tab:result-on-test} shows the result on the test set. Although three approaches optimise the predictive model $g$ while taking into account agents' strategic behaviour, our result confirms that jointly optimising both the predictive model $g$ and the explanation policy $\sigma$ gives the DM a more favourable outcome than simply employing a fixed explanation scheme (i.e., counterfactual explanations). In addition, not only our approach has the lowest prediction error, when facing strategic agents, we also achieve perfect compliance from agents. This high compliance rate shows that although \textit{persuasion} is not the focus of our approach, we can still achieve it as the optimised AR policy favours recommendations that could help agents achieve self-improvement.


\subsection{On the No-Harm Guarantee of Explanations}

\paragraph{Experimental setup.} We use a quartic function as the predictive model of the DM where $g(x)=x^4-x^2+1$ and use 2nd-order Taylor expansions as explanations. We choose this baseline because it has a clear reaction model~\citep{xie2024non}, unlike other explanation methods, and because agents' best responses can be computed exactly, allowing for precise experiment result.
To illustrate that Taylor expansions as explanations do not guarantee no-harm, we generate a simple dataset of 100 agents with scalar features $x_t^{(b)}\in\mathbb{R}$ and cost factors $\alpha_t\in\mathbb{R}$. The cost function for agent $t$ is the same as that of the previous experiment, i.e., $c_t(x_t^{(b)},x)=|\alpha_t|\|x_t^{(b)}-x\|_2^2$. We compare the result of Taylor expansions against AR-based explanations, which we generate randomly, for the sake of simplicity.

\paragraph{Results.}
\Cref{fig:result-noharm} shows the histograms of the change in agents' utility values before and after performing best responses. With 2nd-order Taylor expansions as explanations, $49\%$ of agents have reduced utility values after best responding. That is, even though Taylor expansions can approximate local structures of the true predictive model, they may exaggerate the agents' gains and thus mislead them into taking costly actions. On the other hand, with AR-based explanations, agents cannot be mislead, even if the recommended actions are generated randomly.


%%%
\section{Related Work}
In this section, we overview related work and provide further details in \Cref{apx:extended-related-work}.


% In this section, we highlight closely related work and provide the extended version of this section in \Cref{apx:extended-related-work}.

\paragraph{Strategic learning.} Several studies have examined the effects of sharing partial information about predictive models with agents and analysed how agents make decisions based on this information~\citep{jagadeesan2021alternative, ghalme2021strategic, bechavod2022information, harris2022bayesian, haghtalab2024calibrated, xie2024non, cohen2024bayesian}. 
% Several studies in this area have examined the impact of partial information release when agents do not have full knowledge of the predictive model used in decision-making~\citep{jagadeesan2021alternative, ghalme2021strategic, bechavod2022information, harris2022bayesian, haghtalab2024calibrated, xie2024non, cohen2024bayesian}. 
In particular, the works of \citet{harris2022bayesian} and \citet{cohen2024bayesian} are closest to ours. As discussed throughout \Cref{subsec:action-rec}, \citet{harris2022bayesian} focuses on the obedience-inducing property (also known as the Bayesian incentive compatibility) of a subclass of action recommendations, whereas we focus on the no-harm property of action recommendations. 
% As we also discuss in \Cref{subsec:action-rec}, identifying an AR-based explanation policy that can induce obedience for each \textit{individual} agent is hard, especially when agents are heterogeneous (e.g., in \citealt{harris2022strategic,shao2024strategic}), and such \textit{individual}-level identification might not be necessary if the DM only cares about optimising their expected utility, which is computed over the population of agents.
Unlike action recommendations, \citet{cohen2024bayesian} instead releases a subset of the hypothesis class to all agents, aligning with global explanations in our framework (\Cref{sec:sl-with-explanations}). However, interpreting a set of modelsâ€”such as neural networksâ€”can be difficult for agents. In contrast, the AR-based explanations are not only more interpretable but also provide guidance that cannot mislead agents.

% However, interpreting such disclosures, such as subsets of deep neural networks, remains practically unclear.
% On the other hand, in \citet{cohen2024bayesian}, the DM releases a subset of the hypothesis class to all agents. This can be viewed as a type of global explanation in our framework (\Cref{sec:sl-with-explanations}). However, it remains unclear how agents can interpret such information in reality, for instance, when the disclosed information is a subset of deep neural networks.


\paragraph{Counterfactual explanations and algorithmic recourse.} \citet{tsirtsis2020decisions,karimi2022survey} explore counterfactual explanations and algorithmic recourse, for strategic agents. Although algorithmic recourse focuses on recommending actions to achieve better outcomes, it actual implementation often requires strong causal assumptions. These assumptions can render it impractical in more general settings where such causal knowledge is not justified.
In contrast, our work adopts a weaker notion of desirability centred on agents' welfare---ensuring non-harmful responses---and examines a broader range of explanation types beyond counterfactuals.
% Instead of recourse, our work concerns with a weaker notion of agents' welfare, i.e., non-harmful responses, and instead of counterfactual explanations, we study a wide range of explanation types. 
In particular, \citet{tsirtsis2020decisions} focuses on designing an efficient counterfactual explanation algorithm that balances the DM's utility while preventing model leakage. This approach can be interpreted as optimally reducing the explanation space within our framework. In contrast, we present a formal analysis across a range of explanation types and demonstrate the sufficiency of AR-based explanations.

% focuses on counterfactual explanations, aiming to design an efficient learning algorithm that balances the DM's utility while preventing model leakage. This can be viewed as optimally shrinking the explanation space in our framework. In contrast, we provide a formal analysis across various explanation types and establish the sufficiency of AR-based explanations.
% Similarly, \citet{tsirtsis2020decisions} focuses entirely on counterfactual explanations and examines their consequences on strategic agents. Their objective is to design an efficient learning algorithm while balancing the DM's utility and preventing model leakage. This can be seen as a specific case of optimally shrinking the explanation space in our framework. In contrast, our work provides a formal analysis on several types of explanations and prove the sufficiency of AR-based explanations.

\paragraph{Information design.} The extensive literature on information design, as surveyed by \citet{bergemann2019information}, studies how to design information disclosure policies in a game of two parties. While our results are inspired by these works, e.g., \Cref{theorem:ar-sufficiency}, the goals differ significantly. As discussed in \Cref{subsec:action-rec}, information design aims at \textit{persuading} agents with a general response model and does not necessarily ensure the no-harm property (\Cref{def:no-harm}). In contrast, we study explanation methods that prioritise the no-harm property, ensuring agents' welfare is not compromised.
By incorporating specific agent models in strategic settings, we establish the sufficiency of AR-based explanations without requiring the DM to account for agents' heterogeneous reaction models.
% This allows us to prove the sufficiency property of AR-based explanations without forcing the DM to know about agents' heterogeneous reaction models. 
% Consequently, while the class of BCE suffices to rationalise any agents' behaviour, our setup of AR-based explanations suffice to rationalise any \textit{non-harmful} agents' behaviour.

%%%%
\section{Conclusion}
To summarise, we address the challenge of providing actionable and safe explanations in strategic learning scenarios where DMs must balance transparency with utility optimisation. We formalise the class of action recommendation (AR)-based explanations, which ensure that agents act without incurring detrimental outcomes. By introducing the no-harm property and assuming a sub-homogeneity condition, we demonstrate that AR-based explanations enable DMs to achieve optimal outcomes while safeguarding agent welfare. Consequently, our work clarifies the distinctions of different explanation methods, through the lens of strategic learning. Last but not least, we propose a framework to jointly optimise predictive models and explanation policies, aligning the DMâ€™s objectives with agents' best responses. 

Our findings rest on commonly adopted assumptions about agents' behavior, such as their utility functions or reaction models. While these assumptions may limit the generalisability of our approach, they do not diminish its broader relevance. Intuitively, when explanations omit certain information, conditions are necessary to ensure that agentsâ€™ inferred gains are not exaggerated, maintaining realistic and actionable guidance. AR-based explanations succeed in this regard by focusing exclusively on actionable recommendations that agents can safely choose without fear of being misled, thereby ensuring both predictive accuracy for the DM and safety for the agents. 
% This highlights the critical role of coupling utility optimisation with safety guarantees in strategic learning. 
Future work could explore extensions to diverse agent behavior models, dynamic environments, and more scalable learning algorithms to enhance the applicability and efficiency of this approach.



\section*{Acknowledgments} 
We sincerely thank the members of our research group, Abbavaram Gowtham Reddy, Anurag Singh, and Swathi Suhas, for their insightful discussions, constructive feedback, and invaluable contributions to this work. We also extend our gratitude to the visiting researchers, Amin Charusaie, Masaki Adachi, Rattaya Kaewvichai, and Saptarshi Saha, for their stimulating discussions and fresh perspectives, which enriched our understanding of the problem. Their contributions have been greatly appreciated.

Yixin Wang was supported in part by the Office of Naval Research under grant number N00014-23-1-2590, the National Science Foundation under Grant No. 2231174, No. 2310831, No. 2428059, No. 2435696, No. 2440954, and a Michigan Institute for Data Science Propelling Original Data Science (PODS) grant.



\bibliographystyle{unsrtnat}
\bibliography{references} 

\newpage
\appendix
\input{sections/proofs}


\end{document}
