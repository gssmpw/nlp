\section{More Detailed Examples}
This section contains more examples to illustrate our theory.

\subsection{Examples of Explanation Methods}\label{apx:explanation-methods}

Below we provide some specific examples for global and local explanation methods that fit into our setting:

\begin{itemize}
    \item Global surrogate models such as linear models or decision trees that approximate the true predictive model $g$ \citep{molnar2020interpretable}. In this scenario, a constant explanation $e\in\mathcal{E}$, regardless of $x_t^{(b)}$, is some surrogate model $f:\mathcal{X}\to\mathcal{Y}$ for the true model $g:\mathcal{X}\to\mathcal{Y}$.
    
    \item Partial descriptions of the model \citep{cohen2024bayesian}. In this scenario, a constant explanation $e\in\mathcal{E}$, regardless of $x_t^{(b)}$, is some subset $\mathcal{G}_S\subseteq\mathcal{G}$ such that $g\in\mathcal{G}_S$.
    
    \item Feature attribution-based explanation methods that assign importance scores to features such as SHAP \citep{lundberg2017unified}. When the covariate $x_t^{(b)}\in\mathcal{X}\subseteq\mathbb{R}^d$ is a vector of $d$ features, an explanation $e_t=(e_{t1}, \ldots, e_{td})\in\mathcal{E}\in\mathbb{R}^d$ is a vector containing the importance scores of each features in $x_t^{(b)}$.
    
    \item Local surrogate models such as Taylor's expansion \citep{xie2024non}. An explanation $e_t$ is some function $f_t:\mathcal{X}\to\mathcal{Y}$ that approximates $g$ in a local neighbourhood of $x_t^{(b)}$ and the space of explanations is the set of all such functions, e.g., $\mathcal{E}\subseteq\mathcal{F}=\{f\mid f:\mathcal{X}\to\mathcal{Y}\}$.
    
    \item Counterfactual explanations \citep{wachter2017counterfactual}. In this scenario, $e_t=\big(x^{(r)}, \hat{y}^{(r)}\big)$ and $\mathcal{E}=\Big\{\big(x^{(r)}, \hat{y}^{(r)}\big):\hat{y}^{(r)}<\hat{y}^{(b)}\Big\}$, where $x^{(r)}$ denotes the recommended covariate for the agent to change to, in order to receive a more favourable prediction $\hat{y}^{(r)}<\hat{y}^{(b)}$ from the DM.
\end{itemize}


\subsection{Shapley Value Example}\label{apx:shapley}

\begin{example}\label{example:shapley}
Consider the predictive model $g(x)=x_1-x_2^2$ for any $x=[x_1\quad x_2]^\top\in\mathbb{R}^2$, and an agent with $x^{(b)}=[16\quad 4]^\top$. For simplicity, suppose further that the 2 features $X_1, X_2$ are statistically independent and that $X_2\sim\mathcal{U}([2,5])$. The Shapley value of this agent's 2nd feature is
\begin{align*}
&\phi_2(g, x^{(b)}) 
\\
&= \frac{1}{2}\Big(g(x^{(b)}) - \mathbb{E}\left[g(X_1=16, X_2)\right]
\\
&\hspace{12mm} +\mathbb{E}\left[g(X_1, X_2=4)\right] - \mathbb{E}\left[g(X_1, X_2)\right]\Big)
\\
&= \frac{1}{2}\Big(0 - \mathbb{E}\left[16-X_2^2\right] + \mathbb{E}\left[X_1-16\right] - \mathbb{E}\left[X_1-X_2^2\right]\Big) 
\\
&= \frac{1}{2}\Big(-32+2\mathbb{E}\left[X_2^2\right]\Big) = -3 < 0
.
\end{align*}

However, if we consider $X_2\sim\mathcal{U}([2,8])$, then $\mathbb{E}\left[X_2^2\right]=28$ and the Shapley value for this 2nd feature is
\begin{align*}
\phi_2(g, x^{(b)}) = \frac{1}{2}\Big(-32+ 2\mathbb{E}\left[X_2^2\right]\Big)=12>0
.
\end{align*}

On the other hand, the partial derivative of the function $g$ at the point $x_2=4$ is
\begin{align*}
\frac{dg}{dx_2}(x_2=4) = -8,
\end{align*}
which implies that this agent can achieve a lower prediction score by increasing the value of their 2nd feature $x_2$. However, the sign of the Shapley value for this feature, as we have shown, can vary depending on the distribution of the data, as a result, this Shapley value cannot say how this agent should change their feature to obtain better prediction score.

% If this agent wants to modify $x_2$ to reduce the prediction value $g(x)$, they must decrease it to the range $[0,4)$. However, the Shapley values for this 2nd feature can vary a lot (and can even take opposite signs) depending on the distribution of the data, thus they are not informative for the agent to decide whether to increase or decrease the value of $x_2$.
\end{example}


\subsection{A Misled Agent}\label{apx:misled-agent-example}
\begin{example}
Suppose that an insurance company uses the $g(x)=x^2$ to predict the risk of a customer whose feature takes on the base value $x^{(b)}=5$, which corresponds to the prediction $\hat{y}^{(b)}=25$. Suppose further that this customer has the following cost function for changing their feature:
\begin{align*}
c(\Delta x) = 
\left\{\begin{aligned}
&3(\Delta x)^2 \quad \forall \Delta x\in (-\infty,-3]\\
&9|\Delta x| \hspace{6mm} \forall \Delta x\in[-3,0]\\
&>0 \hspace{8mm} \forall \Delta x\in(0,\infty),
\end{aligned}\right.
\end{align*}
where we use $\Delta x$ to denote $x - x_t^{(b)}$ for any $x\in\mathcal{X}$.
The DM discloses a localized linear model $f(x)=10x-25$ tangent to $g(x)$ at the base value $x^{(b)}=5$. As the agent wants to minimise their predictive risk, their true utility function and surrogate utility function, to be maximised, respectively, are
\begin{align*}
u(g,x) &= -\Big(g\big(x^{(b)}+\Delta x\big)+c(\Delta x)\Big) 
= -\big((5+\Delta x)^2 + c(\Delta x)\big),
\\
u(f,x) &= -\Big(f\big(x^{(b)}+\Delta x\big)+c(\Delta x)\Big)
= -\big(10(5+\Delta x)-25 + c(\Delta x)\big).
\end{align*}
Hence, $x=2$ (or equivalently $\Delta x=-3$) is the customer's best response as it maximises their surrogate utility function $u(f,x)$. However, this leads to a reduction in the true utility function, i.e., $u(g,2) < u(g,5)$, because the customer has paid a high cost only to achieve a small reduction in their prediction value.
\end{example}



\section{Proofs of the Main Results}\label{apx:proofs}

This section contains the derivations and proofs of our main results. 

\subsection{DM's Objective}\label{subapx:expanded-obj}
We expand \Cref{eq:dm-objective} as
\begin{align*}
&\min_{g,(\mathcal{E},\sigma)}\mathbb{E}_{P_{X,Z}}\left[\big(g(X_t)-h(X_t,Z_t)\big)^2\right]
\\
=&\min_{g,(\mathcal{E},\sigma)}\mathbb{E}_{P_{X^{(b)},Z}}\left[\Big(g\big(\psi(X_t^{(b)},\sigma(X_t^{(b)},g),Z_t)\big)-h\big(\psi(X_t^{(b)},\sigma(X_t^{(b)},g),Z_t),Z_t\big)\Big)^2\right]
\end{align*}


\subsection{Proof for \Cref{prop:localisbetter}}
\begin{proof}
For any pair of $\big(e,x_t^{(b)}\big)\in\mathcal{E}\times\mathcal{X}$, we have
\begin{align}
\mathbb{E}_{Z_t}\left[l\big(x_t^{(b)},e,Z_t\big)\ \big\lvert\ x_t^{(b)}\right] 
\geq 
\min_{e_t\in\mathcal{E}} \mathbb{E}_{Z_t}\left[l\big(x_t^{(b)},e_t,Z_t\big)\ \big\lvert\ x_t^{(b)}\right]
.
\end{align}

Because $l\geq 0$, we have the following for any $e\in\mathcal{E}$:
\begin{align}
\mathbb{E}_{X_t^{(b)},Z_t}\left[l\big(X_t^{(b)},e,Z_t\big)\right] 
\geq 
\mathbb{E}_{X_t^{(b)}}\left[\min_{e_t\in\mathcal{E}} \mathbb{E}_{Z_t}\left[l\big(X_t^{(b)},e_t,Z_t\big)\ \big\lvert\ X_t^{(b)}\right]\right]
\end{align}

Therefore,
\begin{align}
\min_{e\in\mathcal{E}}\mathbb{E}_{X_t^{(b)},Z_t}\left[l\big(X_t^{(b)},e, Z_t\big)\right] 
\geq 
\mathbb{E}_{X_t^{(b)}}\left[\min_{e_t\in\mathcal{E}}\mathbb{E}_{Z_t}\left[l\big(X_t^{(b)},e_t, Z_t\big)\ \big\lvert\ X_t^{(b)}\right]\right].
\end{align}
\end{proof}


\subsection{Proof for \Cref{theorem:surrogate-necessary}}
\begin{proof}
Suppose the necessary condition (\Cref{eq:surrogate-necessary-cond}) is violated, then there exist a value $x_\bullet \in \mathcal{X}_t^{g^{-}}$ and a cost function $c_t$ such that
\begin{align}
\left\{\begin{aligned}
&0 < g(x_t^{(b)}) - g(x_\bullet) < c_t(x_t^{(b)}, x_\bullet),
\\
&c_t(x_t^{(b)}, x_\bullet) < f_t(x_t^{(b)}) - f_t(x_\bullet),
\\
&c_t(x_t^{(b)},x_\bullet) + \Big(f_t(x_\bullet)-f_t(x)\Big) < c_t(x_t^{(b)},x)\quad \forall x \in\mathcal{X}_t^{g^{-}}\setminus\{x_\bullet\}.
\end{aligned}\right.
\end{align}

The first line follows from the fact that $\Delta x_\bullet\in\mathcal{X}_t^{g^{-}}$ and that $c_t(x_t^{(b)},x_\bullet)>0$ (because $x_\bullet\neq x_t^{(b)}$). The second line follows from the fact that the constraint in the theorem is violated and the rate of change of $f_t$ is greater than the rate of change of $g$ at the point $x_\bullet$. The third line follows from the fact that $c_t(x_t^{(b)}, x)>0$ for all $x\in\mathcal{X}_t^{g^{-}}$ because $x_t^{(b)}\not\in\mathcal{X}_t^{g^{-}}$.

% (
% The first line has nothing to do with \Cref{eq:surrogate-necessary-cond}, this is only a property of the cost function and because we are looking at some arbitrary point $x_\bullet\in\mathcal{X}_t^{g^-}$.

% $c_t(x_t^{(b)}, x) = f_t(x_t^{(b)}) - f(x)$.

% The third line has nothing to do with \Cref{eq:surrogate-necessary-cond}, we only say that we can always design a cost function that has this property.)

Then we can see that, if the best response of this agent $t$, against the local surrogate function $f_t$, lies in the set $\mathcal{X}_{t}^{g^{-}}\cup\{x_t^{(b)}\}$, then $x_\bullet$ is the solution, since
\begin{align}
x_\bullet &:= \arg\min_{x\in\mathcal{X}_{t}^{g^{-}}\cup\{x_t^{(b)}\}} f_t(x) + c_t(x_t^{(b)},x)
\\
&:= \arg\max_{x\in\mathcal{X}_{t}^{g^{-}}\cup\{x_t^{(b)}\}} u_t(f_t,x)
.
\end{align}

Moreover, because $g(x_t^{(b)}) - g(x_\bullet) < c_t(x_t^{(b)},x_\bullet)$, we have
\begin{align}
-\Big(g(x_\bullet) + c_t(x_t^{(b)},x_\bullet)\Big) &< -g(x_t^{(b)})
\\
\Rightarrow u_t(g,x_\bullet) &< u_t(g,x_t^{(b)}).
\end{align}

On the other hand, if the best response is some $x_\square\not\in(\mathcal{X}_t^{g^{-}}\cup\{x_t^{(b)}\})$, we have
\begin{align}
g(x_\square) + \underbrace{c_t(x_t^{(b)},x_\square)}_{>0} > g(x_\square) \geq g(x_t^{(b)})
,
\end{align}

which results in $u_t(g, x_\square) < u_t(g,x_t^{(b)})$. This concludes the proof.
\end{proof}


\subsection{Proof for the Sufficient Condition Following \Cref{theorem:surrogate-necessary}}

\begin{proof}
For any arbitrary agent $t$ with $x_t^{(b)}$ and $c_t$, suppose we have
\begin{align*}
f_t\big(x_t^{(b)}\big) - f_t(x) \leq g\big(x_t^{(b)}\big) - g(x) \quad \forall x \in \mathcal{X}.
\end{align*}

When this agent best responds against the local surrogate function $f_t$ with $x_t\in\mathcal{X}$, it follows that
\begin{align*}
&f_t(x_t) + c_t\big(x_t^{(b)}, x_t\big) \leq f_t\big(x_t^{(b)}\big)
\\
\Rightarrow\ &c_t\big(x_t^{(b)}, x_t\big) \leq \underbrace{f_t\big(x_t^{(b)}\big) - f_t(x_t)
\leq g\big(x_t^{(b)}\big) - g(x_t)}_\text{The assumed condition.}
\\
\Rightarrow\ &c_t\big(x_t^{(b)}, x_t\big) \leq g\big(x_t^{(b)}\big) - g(x_t)
\\
\Rightarrow\ &g(x_t) + c_t\big(x_t^{(b)}, x_t\big) \leq g\big(x_t^{(b)}\big)
\\
\Rightarrow\ &g(x_t) + c_t\big(x_t^{(b)}, x_t\big) \leq g\big(x_t^{(b)}\big) + \underbrace{c_t(x_t^{(b)},x_t^{(b)})}_{=0\ \text{by \Cref{def:cost-function}}.}
\\
\Rightarrow\ &u_t(g,x_t) \geq u_t(g,x_t^{(b)}),
\end{align*}
where the last inequality follows from the definition of an agent's true utility function $u_t$. This concludes the proof.
\end{proof}





\subsection{Proof for \Cref{theorem:ar-sufficiency}}
\begin{proof}[Proof]
Because these agents have the same response for any explanation (\Cref{assumption:subhomo-response}), they must have the same set of non-harmful responses, i.e., $\nu_t=\nu$ for all $t\in[T^\prime]$.

Then for any non-harmful explanation method that induces a best response $x_\bullet$ of these agents, it means that $u(g,x_\bullet)\geq u(g,x^{(b)})$.

Consequently, there exists an AR-based explanation method that provides the explanation $(x^{(r)},\hat{y}^{(r)})=(x_\bullet,g(x_\bullet))$ and by \Cref{eq:ar-agent-model}, all agents will follow the recommendation.
\end{proof}

% \begin{proof}[{\color{red}Old proof}]
% Given an arbitrary agent $t$, for any local explanation method $\sigma:x_t^{(b)}\mapsto m_t$ that induces a best response $\Delta x_\bullet=\psi(x_t^{(b)},m_t,u_t)$ such that $\Delta x_\bullet\in\nu_t$, there exists an action recommendation policy that recommends the very same action, i.e., $\sigma^\prime:x_t^{(b)}\mapsto(\Delta x_t^{rec},\hat{y}_t^{rec})$ where $\Delta x_t^{rec}=\Delta x_\bullet$ and $\hat{y}^{rec}=g(x_t^{(b)}+\Delta x_\bullet)$.

% Under this action recommendation policy $\sigma^\prime$, the agent $t$ complies with the recommendation because $u(g,\Delta x_\bullet)\geq u(g, \Vec{0})$ and this expressed can be evaluated by the agent. This proves the 'if' direction.

% For the 'only-if' direction, it can be seen that for any action recommendation policy $\sigma^\prime:x_t^{(b)}\mapsto(\Delta x_t^{rec},\hat{y}_t^{rec})$ that induces the best response $\Delta x_\square$, this response is always within $\nu_t$. Therefore, $\sigma^\prime$ counts as an explanation method that produces a message $m_t=(\Delta x_t^{rec}, \hat{y}_t^{rec})$ which, in turn, induces a best response in $\nu_t$. This concludes the proof.
% \end{proof}


% \subsection{Optimality of AR-based explanations}

% We introduce the notion of optimal solution, with respect to the DM's utility. This most-preferred equilibrium is the best possible outcome the DM can attain, when the no-self-harm property for agents (\Cref{def:no-harm}) is imposed.

% \begin{definition}[DM's most-preferred equilibrium]\label{def:preferred-equilibrium}
% Given a population of $T$ agents, a predictive model $g$ and an explanation method $(\mathcal{E}, \sigma)$ that always induces non-harmful responses from agents, {\color{red}i.e., $x_t\in\nu_t$ for all $t\in[T]$}, then the most-preferred equilibrium for the DM is the equilibrium in which every agent's response $x_t$ minimises the DM's respective prediction error, i.e.,
% \begin{align*}
% x_t:=\arg\min_{x\in\nu_t}\big(g(x)-h(x,z_t)\big)^2 \quad \forall t\in[T]
% \end{align*}
% \end{definition}

% Before stating the result, we introduce the following assumption, which is stronger than and implies \Cref{assumption:subhomo-response}.

% \begin{assumption}[Conditional homogeneity of unobserved factors]\label{assumption:subhomo-unobserved}
% For any subset of $T^\prime$ agents who share the same base covariate vector, i.e., $x_t^{(b)}=x^{(b)}$ for all $t\in[T^\prime]$, they must have the same unobserved variable and cost function, i.e., $(z_t,c_t)=(z,c)$ for all $t\in[T^\prime]$, where $c:\mathcal{X}\times\mathcal{X}\to[0,\infty)$ as defined in \Cref{def:cost-function}.
% \end{assumption}

% \begin{theorem}[Optimality of AR-based explanations]
% If \Cref{assumption:subhomo-unobserved} holds, then there exists a deterministic mapping $\sigma:(x_t^{(b)},g)\mapsto(x_t^{(r)}, \hat{y}_t^{(r)})$ that induces the DM's most-preferred equilibrium.
% \end{theorem}

% \begin{proof}[Proof sketch]
% When \Cref{assumption:subhomo-unobserved} holds, for a given AR-based explanation $e=(x^{(r)},\hat{y}^{(r)})$, all agents who have the same base covariate vector $x^{(b)}$ will produce the same response $x^\bullet$, according to \Cref{eq:ar-agent-model}. 

% Furthermore, all these agents induce the same loss function, according to \Cref{def:preferred-equilibrium} and because their unobserved factors are conditionally homogeneous \Cref{assumption:subhomo-unobserved}.

% For any agent $t$ with a base value $x_t^{(b)}$, let $x_t=x_\dagger$ denote this agent's best response that also belongs to the DM's most-preferred equilibrium, then $u_t(g,x_\dagger)\geq u_t(g,x_t^{(b)})$ because $x_\dagger\in\nu_t$ by \Cref{def:preferred-equilibrium}.

% Then there exists a AR policy that provides the explanation $(x_t^{(r)},\hat{y}_t^{(r)})=(x_\dagger,g(x_\dagger))$ and by \Cref{eq:ar-agent-model}, the agent will follow the recommendation. 
% \end{proof}


\section{Additional Details of the Experiments}\label{apx:detailed-exp}
We provide here additional details to the setups for experiments in \Cref{sec:experiments}.

\subsection{Operationalising AR-based Explanations}
The synthetic dataset contains agents of 3-dimensional feature vectors $x^{(b)}_t\in\mathbb{R}^3$ as follows: $Z_t \sim \mathcal{U}(\{0,1,2,3\})$, $X_t^{(b)}\mid z_t \sim \mathcal{N}(\mathbf{m},\ I\times 2)$ where $\mathbf{m}:=[10+z_t,10+z_t,10+z_t]^\top$, and $\alpha_t\mid z_t \sim \mathcal{N}(0.02+0.1z_t,\ 0.01^2)$ with the cost function being $c_t(x_t^{(b)},x)=|\alpha_t|\|x_t^{(b)}-x\|_2^2$.
In our experiments, we use 3-layer ReLU network for constructing all three models $g$, $\sigma^r$, and $\xi$. To pre-train the predictive model $g_0$ and the local AR mapping $\sigma^r_0$ as outlined in \Cref{algo:main}, we use a sets $D_1=\{x_t,y_t\}_{t\in[5000]}$.

Then, we interact with the next $10^6$ agents to collect another data set $D_2=\{x_t^{(b)}, x_t^{(r)}, w_t, \Delta g_t^{(r)}\}_{t\in[10^6]}$ to train the reaction model $\xi$. We construct sampler $\pi$ to generate recommended actions $X_t^{(r)}\mid x_t^{(b)} \sim \mathcal{N}(x_t^\diamond,4)$,
where $x_t^\diamond$ could be any of $\{x_t^{(b)},\sigma^r_0\big(x_t^{(b)}\big), x_t^{(\text{ce})}\}$, with $x_t^{(\text{ce})}$ being part of the counterfactual explanation generated by $\sigma_\text{ce}(x_t^{(b)},g)$.
% We make three copies of the pretrained model $g$ and denote them as $g_\text{loc}$, $g_\text{glo}$, and $g_\text{ce}$.

Next, we run RRM over $m=100$ iterations, in each of which we deploy $g_{i-1}$ and $\sigma^r_{i-1}$ to interact with $10^4$ agents to collect a data set $\{x_t,y_t\}_{t\in[10^4]}$, where the subscript $i$ denotes an iteration, as outlined in \Cref{algo:main}. The models $g_i$ and $\sigma^r_i$ are sequentially updated over $100$ iterations as specified in \Cref{eq:rrm-update} to eventually obtain $g_\text{loc}$ and $\sigma^r_\text{loc}$. We perform a similar procedure to obtain $g_\text{glo}$ and a \textit{global} AR mapping, which is in fact just a constant mapping that generate the same recommended action $x^{(r)}_\text{glob}$ for all agents . Then, \Cref{eq:rrm-update} can be replaced with the following objective:
$(g_{i},x^{(r)}_{i})=\min_{g,x^{r}}\sum_{t\in[T_i]}\left(g(\hat{x}_t)-y_t\right)^2$.


We conduct a similar procedure for the case of counterfactual explanations. Specifically, the objective of this approach in each iteration of RRM is $g_{\text{ce},i}=\min_{g_{\text{ce},i-1}}\sum_{t\in[T_{i}]}(g(\hat{x}_t)-y_t)^2$
where we do not optimise for the counterfactual explanations generator and simply use the following scheme: $x^\text{ce}_t := \arg\min_{x}(g_\text{ce}(x) + \|x-x_t^{(b)}\|_2^2)$.

Finally, we compare the prediction errors between the three approaches (i.e., local AR mapping, global AR mapping, and counterfactual explanations) on a hold-out test set of $10^6$ strategic agents.


\subsection{On the No-Harm Guarantee of Explanations}
We use a quartic function as the predictive model of the DM where $g(x)=x^4-x^2+1$ and use 2nd-order Taylor expansions as explanations.
To illustrate that Taylor expansions \citep{xie2024non} as explanations (because this explanation method has a clear reaction model, while others do not) do not guarantee no-harm, we generate a simple data set of 100 agents with 1-dimensional features as follows: $X_t^{(b)} \sim \mathcal{N}(0,0.4^2), \alpha_t \sim \mathcal{U}([1,1.2])$ where $\alpha_t$ denotes the cost factor of agent $t$, similar to the previous experiment.
The cost function for agent $t$ is the same as that of the previous experiment, i.e., $c_t(x_t^{(b)},x)=|\alpha_t|\|x_t^{(b)}-x\|_2^2$. We compare the result of Taylor expansions against AR-based explanations, which we generate randomly, for the sake of simplicity.


\section{Related Work (Extended)}\label{apx:extended-related-work}

\paragraph{Strategic learning.} Strategic classification was introduced by \citet{bruckner2012static} and further developed by \citet{hardt2016strategic}, where they presented the first computationally efficient algorithms to learn near-optimal classifiers in strategic environments. Their key assumption was that agents have complete knowledge of the classifier due to information leakage, even when the system is designed to obscure the model. In contrast, our work weaken this assumption by considering scenarios where the learner (or DM) releases partial information about their model.

% However, they did not explore scenarios where the learner (or decision maker) releases only partial information about their model.

Since then, numerous studies have further expanded the field of strategic classification by developing more efficient algorithms \citep{dong2018strategic, levanon2021strategic, ahmadi2021strategic} or by incorporating new aspects such as social welfare \citep{hu2018disparate, milli2019social}, randomisation \citep{sundaram2023pac, ahmadi2023fundamental, shao2024strategic}, and repeated interactions \citep{harris2021stateful, cohen2023sequential}. Another line of works concerns with providing incentive for strategic agents to improve \citep{kleinberg2020classifiers, shavit2020causal, harris2022strategic, horowitz2023causal, vo2024causal} with many of them focusing on regression settings and incorporating causal reasoning.

In particular, the works of \citet{harris2022bayesian} and \citet{cohen2024bayesian} are closest to ours. As discussed throughout \Cref{subsec:action-rec}, \citet{harris2022bayesian} focuses on the obedience-inducing property (also known as the Bayesian incentive compatibility) of a subclass of action recommendations, whereas we focus on the no-harm property of action recommendations. 
As we also discuss in \Cref{subsec:action-rec}, identifying an AR-based explanation policy that can induce obedience for each \textit{individual} agent is hard, especially when agents are heterogeneous (e.g., in \citet{harris2022strategic,shao2024strategic}), and such \textit{individual}-level identification might not be necessary if the DM only cares about optimising their expected utility, which is computed over the population of agents.
Unlike action recommendations, \citet{cohen2024bayesian} instead releases a subset of the hypothesis class to all agents, aligning with global explanations in our framework (\Cref{sec:sl-with-explanations}). However, interpreting a set of models—such as neural networks—can be difficult for agents. In contrast, the AR-based explanations are not only more interpretable but also provide guidance that cannot mislead agents.

% Several studies have specifically examined the impact of partial information release in strategic settings. For instance, \citet{jagadeesan2021alternative, ghalme2021strategic, bechavod2022information, harris2022bayesian, haghtalab2024calibrated, xie2024non, cohen2024bayesian} relaxed the assumption that agents have full knowledge of the DM's model and explored how opacity affects both the DM's and agents’ utilities. Out of these, notably related to ours are the works of \citet{harris2022bayesian} and \citet{cohen2024bayesian}. While \citet{harris2022bayesian} focuses on the obedience-inducing property, or Bayesian incentive compatibility (BIC), of a subclass of action recommendations, our focus is on the no-harm property of action recommendations. As we also discuss in \Cref{subsec:action-rec}, identifying an AR-based explanation policy that can induce obedience for each \textit{individual} agent is hard, especially when agents are heterogeneous (e.g., in \citet{harris2022strategic,shao2024strategic}), and such \textit{individual}-level identification might not be necessary if the DM only cares about optimising their expected utility, which is computed over the population of agents. On the other hand, \citet{cohen2024bayesian} consider only the case where all agents receive the same information, a type of global explanation that we discuss in \Cref{sec:sl-with-explanations}, while our work's main focus is on local explanations, i.e., ones that are tailored to specific agents. In addition, their approach of releasing a subset of the hypothesis class to all agents can be viewed as a generalisation of the classical setup where the entire model is disclosed. However, it remains unclear how agents can interpret such information in reality, for instance, when the disclosed information is a subset of deep neural networks.

% (Cite some papers to justify why agents have heterogeneous cost functions, this is important to justify why individual-level persuasion is hard in reality).

% (Their approach is a generalisation of the classical approach where you release the entire model, by releasing a subset of the hypothesis class. But it is unclear how people would interpret it. It would be difficult for agents to interpret this, e.g., how would you interpret a subset of deep neural networks)

% (It is hard for agents to interpret the subset of hypothesis space in \citet{cohen2024bayesian}).

% \Alan{It feels like you are still writing a literature review for strategic learning, but most importantly you didn't introduce the general settings of strategic learning at all here. If a related work is put that early in a paper shouldn't we also introduce some preliminary knowledge before telling people how existing work is different to ours?} {DONE.}


\paragraph{Counterfactual explanations and algorithmic recourse.} 
\citet{tsirtsis2020decisions,karimi2022survey} explore counterfactual explanations and algorithmic recourse, for strategic agents. Although algorithmic recourse focuses on recommending actions to achieve better outcomes, it actual implementation often requires strong causal assumptions. These assumptions can render it impractical in more general settings where such causal knowledge is not justified.
In contrast, our work adopts a weaker notion of desirability centred on agents' welfare---ensuring non-harmful responses---and examines a broader range of explanation types beyond counterfactuals.
In particular, \citet{tsirtsis2020decisions} focuses on designing an efficient counterfactual explanation algorithm that balances the DM's utility while preventing model leakage. This approach can be interpreted as optimally reducing the explanation space within our framework. In contrast, we present a formal analysis across a range of explanation types and demonstrate the sufficiency of AR-based explanations.

% There are several works that are related to ours where counterfactual explanations are provided to strategic agents, such as those of \citet{tsirtsis2020decisions,karimi2022survey}. 
% While the algorithmic recourse literature aims at providing recommended actions to decision subjects so that they could receive better decisions, as pointed out by \citet{karimi2020algorithmic,karimi2021algorithmic}, ensuring recourse is hard and, in general, impossible without knowledge of or assumptions on the causal relationship between a decision subject's features. On the other hand, rather than recourse, which might be impossible to operationalise due to stringent requirements, our work considers how a DM should operationalise explanations when facing strategic agents who have the right to explanation.
% \Alan{You didn't hyphenated right to explanation in the introduction. Be consistent with wordings and notations.}{Done}

% not be required by law, due to its impossibility nature, 

% Another related work by \citet{tsirtsis2020decisions} examines the strategic behavior of agents through the lens of counterfactual explanations. While their setup bears similarities to ours, there are notable distinctions between the two studies. First, their work focuses exclusively on counterfactual explanations, whereas we explore a broader range of explanation types and analyse how agents respond to these variations. Secondly, by focusing on the discrete feature space and the classification problem, they develop an efficient learning algorithm for the DM to identify optimal subsets of counterfactual explanations with the goal of balancing the DM's utility and model leakage. Their approach can be interpreted within our framework (\Cref{sec:sl-with-explanations}) as optimally shrinking the explanation space. On the other hand, we adopt a more comprehensive perspective by providing a formal analysis on what condition an explanation method should satisfy to guarantee non-harmful agents' responses and why it is both desirable and sufficient for a DM to focus on AR-based explanations.
% \Alan{This paragraph needs rewriting, we shouldn't assume the reader knows what counterfactual explanations are. You did not properly introduce this term or at least give it a brief overview. (Same with strategic learning). I would suggest writing more concisely. For example, your first point is the difference in the research question being tackled, then you should directly say it. Your second point is about the constraints/focus/desirable properties on the counterfactual explanations. Directly saying this is much clearer and save reader's time to help you formulate these points.} {IN PROGRESS}

% (Discuss the counterfactual explanations, their original motivation, and why they fit well into the SL scenarios.)

% (Prior work in explanation methods does not consider the optimality under strategic behaviour, except \citet{tsirtsis2020decisions}. Prior work in explainable ML focuses on the explanation part, we balance the explanation part with the strategic feedback from agents.)



\paragraph{Information design.} 
The extensive literature on information design, as surveyed by \citet{bergemann2019information}, studies how to design information disclosure policies in a game of two parties. While our results are inspired by these works, e.g., \Cref{theorem:ar-sufficiency}, the goals differ significantly. As discussed in \Cref{subsec:action-rec}, information design aims at \textit{persuading} agents with a general response model and does not necessarily ensure the no-harm property (\Cref{def:no-harm}). In contrast, we study explanation methods that prioritise the no-harm property, ensuring agents' welfare is not compromised.
By incorporating specific agent models in strategic settings, we establish the sufficiency of AR-based explanations without requiring the DM to account for agents' heterogeneous reaction models.
Consequently, while the class of BCE suffices to rationalise any agents' behaviour, our setup of AR-based explanations suffice to rationalise any \textit{non-harmful} agents' behaviour.
% The vast literature on information design in economics examines how to design information disclosure policies in a game of two parties (i.e., the sender and the receivers), as surveyed by \citet{bergemann2019information}. Although our results are inspired by these works, particularly in \Cref{corollary:general-necessary} and \Cref{theorem:ar-sufficiency}, there are key distinctions in our modelling and analyses. While the field of information design assumes a general agents' response model and mainly revolves around persuading agents (or receivers) into taking specific actions, our work instead considers specific agent models that are common in strategic learning literature and focuses on the no-harm property (\Cref{def:no-harm}), ensuring that agents' actions, based on incomplete information, do not lead to harmful outcomes.



% \section{Contrasting AR-based Explanations and Persuasion.}
% {\color{blue}The core idea of AR-based explanations is about providing guarantee on what you will get if you follow. So that's why Bayesian persuasion, by construction, does not guarantee no-self-harm.

% When we first introduce the concept of compliance, we might include the following paragraph:
% }

% {\color{orange}
% There is a difference between anticipated utility and actual utility. The former is constructed by what the agent anticipates the predictive model would look like because of the lack of full transparency, while latter requires the actual predictive model that the agent does not have access to. Bayesian persuasion is about (in addition with a bunch of assumptions) that DM would recommend actions that guarantee increase in the anticipated utility of the agent, but NOT NECESSARILY THE ACTUAL UTILITY. What's our contribution then? We want to at least introduce a mechanism that allows the agent to make decision/compliance based on the TRUE UTILITY, this allows us to circumvent the whole idea of anticipated models. We do this by providing pointwise evaluation of the predictive model, which subsequently allows the agent to pointwise evaluate the suggested action based on the TRUE UTILITY. This simple construction thus allow no self-harm.
% }
