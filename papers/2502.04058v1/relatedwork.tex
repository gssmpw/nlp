\section{Related Work}
In this section, we overview related work and provide further details in \Cref{apx:extended-related-work}.


% In this section, we highlight closely related work and provide the extended version of this section in \Cref{apx:extended-related-work}.

\paragraph{Strategic learning.} Several studies have examined the effects of sharing partial information about predictive models with agents and analysed how agents make decisions based on this information~\citep{jagadeesan2021alternative, ghalme2021strategic, bechavod2022information, harris2022bayesian, haghtalab2024calibrated, xie2024non, cohen2024bayesian}. 
% Several studies in this area have examined the impact of partial information release when agents do not have full knowledge of the predictive model used in decision-making~\citep{jagadeesan2021alternative, ghalme2021strategic, bechavod2022information, harris2022bayesian, haghtalab2024calibrated, xie2024non, cohen2024bayesian}. 
In particular, the works of \citet{harris2022bayesian} and \citet{cohen2024bayesian} are closest to ours. As discussed throughout \Cref{subsec:action-rec}, \citet{harris2022bayesian} focuses on the obedience-inducing property (also known as the Bayesian incentive compatibility) of a subclass of action recommendations, whereas we focus on the no-harm property of action recommendations. 
% As we also discuss in \Cref{subsec:action-rec}, identifying an AR-based explanation policy that can induce obedience for each \textit{individual} agent is hard, especially when agents are heterogeneous (e.g., in \citealt{harris2022strategic,shao2024strategic}), and such \textit{individual}-level identification might not be necessary if the DM only cares about optimising their expected utility, which is computed over the population of agents.
Unlike action recommendations, \citet{cohen2024bayesian} instead releases a subset of the hypothesis class to all agents, aligning with global explanations in our framework (\Cref{sec:sl-with-explanations}). However, interpreting a set of models—such as neural networks—can be difficult for agents. In contrast, the AR-based explanations are not only more interpretable but also provide guidance that cannot mislead agents.

% However, interpreting such disclosures, such as subsets of deep neural networks, remains practically unclear.
% On the other hand, in \citet{cohen2024bayesian}, the DM releases a subset of the hypothesis class to all agents. This can be viewed as a type of global explanation in our framework (\Cref{sec:sl-with-explanations}). However, it remains unclear how agents can interpret such information in reality, for instance, when the disclosed information is a subset of deep neural networks.


\paragraph{Counterfactual explanations and algorithmic recourse.} \citet{tsirtsis2020decisions,karimi2022survey} explore counterfactual explanations and algorithmic recourse, for strategic agents. Although algorithmic recourse focuses on recommending actions to achieve better outcomes, it actual implementation often requires strong causal assumptions. These assumptions can render it impractical in more general settings where such causal knowledge is not justified.
In contrast, our work adopts a weaker notion of desirability centred on agents' welfare---ensuring non-harmful responses---and examines a broader range of explanation types beyond counterfactuals.
% Instead of recourse, our work concerns with a weaker notion of agents' welfare, i.e., non-harmful responses, and instead of counterfactual explanations, we study a wide range of explanation types. 
In particular, \citet{tsirtsis2020decisions} focuses on designing an efficient counterfactual explanation algorithm that balances the DM's utility while preventing model leakage. This approach can be interpreted as optimally reducing the explanation space within our framework. In contrast, we present a formal analysis across a range of explanation types and demonstrate the sufficiency of AR-based explanations.

% focuses on counterfactual explanations, aiming to design an efficient learning algorithm that balances the DM's utility while preventing model leakage. This can be viewed as optimally shrinking the explanation space in our framework. In contrast, we provide a formal analysis across various explanation types and establish the sufficiency of AR-based explanations.
% Similarly, \citet{tsirtsis2020decisions} focuses entirely on counterfactual explanations and examines their consequences on strategic agents. Their objective is to design an efficient learning algorithm while balancing the DM's utility and preventing model leakage. This can be seen as a specific case of optimally shrinking the explanation space in our framework. In contrast, our work provides a formal analysis on several types of explanations and prove the sufficiency of AR-based explanations.

\paragraph{Information design.} The extensive literature on information design, as surveyed by \citet{bergemann2019information}, studies how to design information disclosure policies in a game of two parties. While our results are inspired by these works, e.g., \Cref{theorem:ar-sufficiency}, the goals differ significantly. As discussed in \Cref{subsec:action-rec}, information design aims at \textit{persuading} agents with a general response model and does not necessarily ensure the no-harm property (\Cref{def:no-harm}). In contrast, we study explanation methods that prioritise the no-harm property, ensuring agents' welfare is not compromised.
By incorporating specific agent models in strategic settings, we establish the sufficiency of AR-based explanations without requiring the DM to account for agents' heterogeneous reaction models.
% This allows us to prove the sufficiency property of AR-based explanations without forcing the DM to know about agents' heterogeneous reaction models. 
% Consequently, while the class of BCE suffices to rationalise any agents' behaviour, our setup of AR-based explanations suffice to rationalise any \textit{non-harmful} agents' behaviour.

%%%%