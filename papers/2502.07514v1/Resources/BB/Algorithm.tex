\subsection{Batched Bandits}
\label{sec:BB}

\begin{algorithm}[t]
    \LinesNumbered
    \SetAlgoLined
    \caption{BB-BARBAT: Batched Bandits-BARBAT}
    \label{algs:BB-BARBAT}
    
    Initial round $T_0 = 0$ and $\Delta_k^0 = 1$ for all $k \in [K]$.
    
    \For{epochs $m = 1,2,\cdots$}{
        Set $a \leftarrow \max \{T^{\frac{1}{2(\cM+1)}}, 2\}$.
    
        Set $\zeta_m \leftarrow (m + 4)a^{2(m+4)}\ln (aK)$ and  $\delta_m \leftarrow 1/(K\zeta_m)$.
        
        Set $\lambda_m \leftarrow a^8 \ln{\left(4K / \delta_m\right)}$ and $\beta_m \leftarrow \delta_m / K$.
        
        Set $n_k^m = \lambda_m (\Delta_k^{m-1})^{-2}$ for all arms $k \in [K]$.
        
        Set $N_m \leftarrow \lceil K \lambda_m a^{2(m-1)} \rceil$ and $T_m \leftarrow T_{m-1} + N_m$.
    
        Select the arm \( k_m = \mathop{\arg\max}_{k \in [K]} r_k^{m-1} \).
        
        Set 
        $   
            \widetilde{n}_k^m = \begin{cases}
                n_k^m & k \neq k_m \\
                N_m - \sum_{k \neq k_m}n_k^m & k = k_m
            \end{cases}
        $.

        \For{$t = T_{m-1} + 1$ to $T_m$}{
            Choose arm $I_t\sim p_m$ where $p_m(k)= \widetilde{n}_k^m / N_m$.

            Observe the corrupted reward $\widetilde{r}_{I_t}$ and update the total reward $S_{I_t}^m = S_{I_t}^m + \widetilde{r}_{I_t}$.
        }

        Set $r_k^m \leftarrow \min \{S_k^m / \widetilde{n}_k^m, 1\}$. 
        
        Set $r_*^m \leftarrow \max_{k \in [K]}\left\{r_k^m - \sqrt{\frac{4\ln(4/\beta_m)}{\widetilde{n}_k^m}}\right\}$.

        Set $\Delta_k^m \leftarrow \max\{a^{-m}, r_*^m - r_k^m\}$.
    }
\end{algorithm}

% In contrast to the traditional multi-armed bandit (MAB) setting, the batched bandit framework assumes that data arrives in discrete batches. In this framework, the agent can only utilize data from a given batch after that batch has concluded. The total time horizon $T$ is divided into $\cM$ batches, represented by a grid $\cT = \bigl\{t_1,t_2,\dots,t_{\cM}\bigr\},$
% where $1 \leq t_1 < t_2 < \cdots < t_{\cM} = T$. The grid can fall into one of the following two categories:
% \begin{itemize}
%     \item \textbf{Static Grid:} The grid is predetermined and remains fixed before any arm sampling occurs;
%     \item \textbf{Adaptive Grid:} The value of $t_j$ can be decided dynamically after observing rewards up to time $t_{j-1}$, potentially incorporating external randomness.
% \end{itemize}
% Although \cite{gao2019batched} argues that the adaptive grid is more powerful and practical, we believe that, in many real-world scenarios, the static grid is more suitable. For example, in clinical trials, it is often necessary to inform patients in advance which batch of the trial they will join. With an adaptive grid, one must wait until the end of each batch, analyze the data, and then determine the size of the next batch, leaving some patients uncertain about when they will participate. By contrast, a static grid allows this scheduling information to be established before the trial begins, providing greater clarity for all participants.

% In previous works~\citep{pmlr-v40-Perchet15,gao2019batched,zhang2020inference,esfandiari2021regret,jin2021double}, researchers have focused on stochastic batched bandits without considering corruption. However, in many real-world scenarios, such as malicious comments in product recommendation systems or accidental machine failures, erroneous information may arise and render existing policies ineffective. Consequently, it is essential to investigate the stochastic batched bandit problem under adversarial corruptions. 

% Motivated by these considerations, we extend BARBAT to the batched bandit setting in an algorithm called \emph{BB-BARBAT}, presented in Algorithm~\ref{algs:BB-BARBAT}. Building on the epoch-based structure of BARBAT, a simple yet effective approach is to increase the epoch length $N_m$ as we proceed through the batches. To achieve this, we introduce a parameter $a \leftarrow \max \{T^{\frac{1}{2(\cM+1)}}, 2\}$,
% which ensures
% \[
% \sum_{m=1}^{M - 1} N_m \;<\; T
% \quad\text{and}\quad
% \sum_{m=1}^{M} N_m \;\ge\; T,
% \]
% where $M$ represents that the number of epochs.
% In this way, if $\cM \le \log(T)$, we can guarantee that BB-BARBAT runs for exactly $\cM$ epochs. However, if $\cM > \log(T)$, then $a \ge 2$, forcing the number of epochs in BB-BARBAT to be $\log(T)$ in order to maintain robustness. In the special case $a = 2$, BB-BARBAT reduces to BARBAT, illustrating that BARBAT inherently possesses good scalability. 

% We establish that the BB-BARBAT algorithm achieves the following regret bound, with the proof provided in Appendix~\ref{ape:bb}.
% \begin{theorem}
% \label{the:bb-erb}    % The expected regret of Algorithm BB-BARBAT    
%     Consider an $\cM$-batched, $K$-armed bandit problem where the time horizon is $T$. The epochs numbers satisfies $M = \min \{\cM, \log(T)\}$, so the expected regret of BB-BARBAT satisfies
%     \begin{align*}
%         R(T) = O\left(CT^{\frac{1}{M+3}} + T^{\frac{4}{M+3}}\left(\sum_{\Delta_k > 0}\frac{M\log(KT)}{\Delta_k} + \frac{K\log(T)\log(1/\Delta)\log(K/\Delta)}{M\Delta}\right)\right).
%     \end{align*}
% \end{theorem}
% \begin{remark}
%     Compared with the recent works~\citep{gao2019batched,esfandiari2021regret}, where the horizon-dependent ratio term is $T^{\frac{1}{\cM}}$ (which is smaller than $T^{\frac{2}{\cM+1}}$), these studies do not take into account adversarial corruptionsâ€”an aspect frequently encountered in stochastic multi-armed bandits. To the best of our knowledge, our work is the first to propose an algorithm that attains near-optimal regret in this setting.
% \end{remark}

In this setting, the agent can only use data from a batch after it has concluded. The time horizon $T$ is divided into $\cM$ batches, represented by a grid $\cT = \{t_1, t_2, \dots, t_{\cM}\}$, where $1 \leq t_1 < t_2 < \cdots < t_{\cM} = T$. This grid can be one of two types: 1) \textbf{Static Grid:} The grid is fixed and predetermined before any arm sampling occurs; 2) \textbf{Adaptive Grid:} The value of $t_j$ is determined dynamically after observing the rewards up to time $t_{j-1}$, which may incorporate external randomness.
Previous works~\citep{pmlr-v40-Perchet15,gao2019batched,zhang2020inference,esfandiari2021regret,jin2021double} have focused on stochastic batched bandits. To the best of our knowledge, no existing works study  stochastic batched bandit problem under adversarial corruptions.

We extend BARBAT to the batched bandit setting by introducing \emph{BB-BARBAT}, presented in Algorithm~\ref{algs:BB-BARBAT}. Building upon BARBAT's epoch-based structure, we increase the epoch length $N_m$ as we progress through the batches. We introduce a parameter $a \leftarrow \max \{T^{\frac{1}{2(\cM+1)}}, 2\}$, ensuring
$\sum_{m=1}^{M-1}N_m < T$ and $\sum_{m=1}^{M-1}N_m \geq T$,
where $M$ is the number of epochs. This structure ensures that if $\cM \leq \log(T)$, BB-BARBAT runs for exactly $\cM$ epochs. If $\cM > \log(T)$, then we have $a = 2$, leading to $\log(T)$ epochs in BB-BARBAT to maintain robustness. In this  case, BB-BARBAT is exactly the same as BARBAT.
We show that BB-BARBAT achieves the following regret bound, with the proof provided in Appendix~\ref{ape:bb}:

\begin{theorem}
\label{the:bb-erb}
For an $\cM$-batched, $K$-armed stochastic bandit problem with time horizon $T$, where the number of epochs satisfies $M = \min\{\cM, \log(T)\}$, the expected regret of BB-BARBAT is
\[
    \BE\left[R(T)\right] = O\left( C T^{\frac{1}{M+3}} + T^{\frac{4}{M+3}} \left( \sum_{\Delta_k > 0} \frac{M \log(KT)}{\Delta_k} + \frac{K \log(T) \log(1/\Delta) \log(K/\Delta)}{M \Delta} \right) \right).
\]
\end{theorem}
We provide the lower bound of corrupted batched bandits in the following theorem.

\begin{theorem}
\label{the:bb-lb}
    For an $\cM$-batched, $K$-armed stochastic bandit problem with time horizon $T$, corruption level $C$ and any static grids, the regrets can be lower bounded as
    
    %for any algorithm that can achieve the optimal regret  of the form $O\left(T^{\frac{1}{\cM}} \sum_{\Delta_k > 0} \frac{\log(T)}{\Delta_k} \right),$
    %under adversarial corruptions, there exists a corruption level $C$ such that the algorithm incurs the following regret:
    \[
    R(T) \geq \Omega\left(T^{\frac{1}{\cM}}\left(K + C^{1 - \frac{1}{\cM}}\right)\right).
    \]
\end{theorem}
\begin{remark}
Our upper bound nearly matches the proposed lower bound when $\cM\ge \log(T)$. However, there is still a gap between our upper bound and the lower bound. We think that our lower bound is nearly tight, and how to achieve optimal regret upper bounds for corrupted batched bandits is an interesting open question. 
    %Compared to recent works~\citep{gao2019batched,esfandiari2021regret}, where the horizon-dependent ratio term is $T^{\frac{1}{\cM}}$ (smaller than $T^{\frac{4}{M+3}}$), but these studies do not account for adversarial corruptions, which are commonly encountered in stochastic multi-armed bandit problems. According the lower bound in Theorem~\ref{the:bb-lb}, to the best of our knowledge, our work is the first to propose an robust algorithm that achieves near-optimal regret in this setting.
\end{remark}
