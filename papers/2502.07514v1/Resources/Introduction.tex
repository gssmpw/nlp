\section{Introduction}
\label{sec:intr}

The multi-armed bandit (MAB) problem is one of the most fundamental online learning tasks with partial information feedback, and it has a long and rich history~\citep{lai1985asymptotically,auer2002finite}. 
Recently, as security concerns have grown, many studies focus on the so called adversarial corruption regime where an adversary is allowed to manipulate reward observations to an otherwise stochastic environment.
Existing approaches can be broadly categorized into two classes: the Follow-The-Regularized-Leader (FTRL) family of algorithms~\citep{zimmert2021tsallis,ito2021parameter,jin2024improved,tsuchiya2024stability} and elimination-based algorithms~\citep{lykouris2018stochastic,gupta2019better,liu2021cooperative,lu2021stochastic}. FTRL-based methods perform optimally in both stochastic and adversarial settings  and also achieve optimal regrets in the corruption setting. However, these methods necessitate solving an optimization problem in each round, which may incur high computational costs. Additionally, FTRL-based methods are challenging to extend to parallelizable settings, such as batched bandits and multi-agent bandits. On the other hand, elimination-based methods is simple and computationally efficient. However, whether these methods can achieve optimal regret is still unknown.

In this paper, we improve upon the BARBAR~\citep{gupta2019better} method and propose a novel elimination-based framework called BARBAT (\textbf{B}ad \textbf{A}rms get \textbf{R}ecourse, \textbf{B}est \textbf{A}rm gets \textbf{T}rust) for stochastic bandits method and robust to adversarial corruptions.
Unlike BARBAR, which employs dynamic epoch lengths, BARBAT adopts static epoch lengths by increasing the probability of selecting the estimated best arm. In addition, BARBAT utilizes epoch-varying failure probabilities $\delta_m$ instead of a global failure probability $\delta$ used in BARBAR, allowing us to achieve near-optimal regret bounds. We further demonstrate our BARBAT framework is scalable and parallelizable by extending it to various scenarios, such as the strongly observable graph bandits~\citep{ito2022nearly,dann2023blackbox}, $d$-set semi-bandits~\citep{zimmert2019beating,ito2021hybrid,tsuchiya2023further}, batched bandits~\citep{Perchet_2016,gao2019batched,esfandiari2021regret} and cooperative multi-agent multi-armed bandits (CMA2B)~\citep{wang2022achieving,liu2021cooperative,ghaffari2024multi}. The regret bounds of our methods along with comparisons to recent works are summarized in Table~\ref{tab:rw}.

%With the growing significance of social networks, data centers, and communication devices, cooperative multi-agent multi-armed bandits (CMA2B)~\citep{wang2022achieving,liu2021cooperative,ghaffari2024multi} have also attracted extensive attention. By coordinating multiple agents, one can expedite the identification of optimal arms. We further show that BARBAT can be adapted to CMA2B with minor modifications, highlighting its potential for parallelization across a wide range of settings.


% The main contribution of this paper can be summarized as follows:
% \begin{itemize}
%     \item For the MAB with adversarial corruption, we propose a novel algorithm, called BARBAT, which enjoys the following regret upper bound:
%     \[O\left(C + \sum_{\Delta_k > 0}\frac{\log^2(T)}{\Delta_k} + \frac{K\log^2(\frac{1}{\Delta})}{\Delta}\right).\]
%     Where $\Delta_k$ is the suboptimality gap of arm $k$ and $\Delta$ is the minimal suboptimality gap of $K$ arms.
%     \item For the strongly observable graph bandits, we propose a novel algorithm, called SOG-BARBAT (\textbf{S}trongly \textbf{O}bservable \textbf{G}raph Bandits - BARBAT), which enjoys the following regret upper bound:
%     \[O\left(C + \sum_{k \in \cI^*}\frac{\log^2(T)}{\Delta_{k}} + \frac{K\log^2(\frac{1}{\Delta})}{\Delta}\right).\]
%     where the set $\cI^*$ as the set of suboptimal arms with the top $\alpha (1 +2\ln(K/\alpha))$ highest rewards. Especially, if the graph is acyclic, which includes the undirected graph, $\cI^*$ is the the set of suboptimal arms with the top $\alpha$ highest rewards.
%     % \item For the weakly observable graph bandits (see Section \ref{sec:wog}), we propose a novel algorithm, called WOG-BARBAT (\textbf{W}eakly \textbf{O}bservable \textbf{G}raph Bandits - BARBAT), which enjoys the following regret upper bound:
%     % \[O\left(\sum_{k \in \mathcal{T}^*}\frac{C}{\gamma \Delta_k} + \sum_{k \in \mathcal{T}^*}\frac{\log^2(T)}{\Delta_{k}^2} + \frac{\gamma}{\Delta}\right).\]
%     % Where $\gamma$ is the size of smallest domain arm set for graph $\mathcal{G}$. Regarding $\mathcal{T}^*$, the inequality $|\mathcal{T}^*| \leq \gamma$ must hold, the specific definition details are in Theorem \ref{tne:wog-erm}.
%     % \item For the special case ($d$-sets) in the semi-bandits, we propose a novel algorithm, called DS-BARBAT (\textbf{d}-\textbf{S}ets - BARBAT), which enjoys the following regret upper bound:
%     % \[O\left(C + \sum_{\Delta_k > 0}\frac{\log^2(T)}{\Delta_{k}} + \frac{K}{ \Delta}\right).\]
%     % It is worth noting that in this environment, there are at least $d$ arms that satisfy $\Delta_k \leq 0$, which is different from the regret of MAB.
%     \item For batched bandits, we propose a novel algorithm with static grid, called BB-BARBAT (\textbf{B}atched \textbf{B}andits - BARBAT), which enjoys the following regret upper bound:
%     \[O\left(C + T^{\frac{2}{M+1}}\left(\sum_{\Delta_k > 0} \frac{M\log(T)}{\Delta_k} + \frac{K\log^2(\frac{1}{\Delta})}{\Delta}\right)\right).\]
%     Where $M = \min \{ \cM, \log(T)\}$ and $\cM$ is the number of batches. To the best of our knowledge, BB-BARBAT is the first algorithm designed for the stochastic batched bandits with adversarial corruptions.
%     \item For the various single-agent environments mentioned above, single-agent algorithms can be smoothly transformed into corresponding multi-agent algorithms with only small changes, which causes each agent's regret to decrease at a rate of $V$ times. In this paper, we only give an example of converting the algorithm from MAB to CMA2B, called MA-BARBAT (\textbf{M}ulti-\textbf{A}gent - BARBAT), which enjoys the following regret upper bound for each agent $i$:
%     \[O\left(\frac{C}{V} + \sum_{\Delta_k > 0}\frac{\log^2(VT)}{V\Delta_k} + \frac{K\log^2(\frac{1}{\Delta})}{V\Delta}\right).\]
% \end{itemize}

% \paragraph{Paper Organization}
% In Section \ref{sec:rw}, we introduce some related works in recent years. Then, we formulate the problem setting in Section \ref{sec:ps}. In Section \ref{sec:alg}, we present some algorithms mentioned before and analyze design ideas. Finally, we conclude the paper with discussion in Section \ref{sec:con}.

We summarized our contribution as follows:
\begin{itemize}
    \item \textbf{BARBAT framework.} We propose the BARBAT framework for stochastic bandits with adversarial corruptions. Notably, BARBAT resolves the open problem raised by \citet{gupta2019better}: eliminating the multiplicative factor $K$ from the corruption-dependent term and achieves near-optimal regret bounds. Moreover, BARBAT does not require prior knowledge of time horizon $T$. %Within this framework, we mainly need to adjust the computation procedure for the arm-selection probability and a few parameters. These modifications allow BARBAT to be extended to various settings while preserving near-optimal regret.
    \item \textbf{Extensions to various corrupted bandit settings.} We extend our BARBAT framework to strongly observable graph bandits, $d$-set semi-bandits and batched bandits, demonstrating that all resulting algorithms achieve near-optimal regret. To the best of our knowledge, our BB-BARBAT algorithm for corrupted batched bandits is the first robust algorithm for stochastic batched bandits. We further adapt BARBAT to cooperative multi-agent multi-armed bandits (CMA2B) with adversarial corruption and propose the MA-BARBAT algorithm, which achieves an optimal $V$-fold reduction in regret of each agent. Our MA-BARBAT algorithm only requires $O(V\log(T))$ communication cost.
    %In addition, the single-agent algorithms derived from the BARBAT framework can be seamlessly extended to multi-agent settings. 

    \item \textbf{Comparison with FTRL.} Compared to FTRL-based methods, our framework has advantages in three folds. First, our methods are more efficient. To compute sample probabilities, FTRL-based methods require iterative processes in each round to find an approximate solution to a constrained convex optimization problem, which can be costly, particularly in semi-bandit settings. Second, our BARBAT framework can be extended to batched and multi-agent settings, whereas extending FTRL to these contexts is quite challenging. Lastly, as shown in the last column of Table~\ref{tab:rw}, our methods do not require the assumption of a unique optimal action, which is required by FTRL algorithms in environments other than MAB.

    
\end{itemize}
\paragraph{Technique}
Notice that in the BARBAR~\citep{gupta2019better} algorithm,  the arm $k$ is approximately drawn $n_k^m=O(1 / \tilde\Delta_k^2)$ times in expectation during epoch $m$, where $\tilde\Delta_k$ is the estimated gap computed in the previous epoch. Thus each epoch having a length of approximately $N_m \approx O(\sum_{k} 1 / \tilde\Delta_k^2)$. Consequently, the adversary can significantly extend the length of an epoch by utilizing all corruption budget in the previous epoch, thereby implicitly increasing the number of pulls for suboptimal arms. On the other hand, our BARBAT sets the epoch length of epoch $m$ to be $N_m \approx O(K 2^{2(m-1)})$ which cannot be affected by the adversary.
We still pull each arms approximately $n_k^m=O(1 / \tilde\Delta_k^2)$ times in epoch $m$, with the exception of the arm $k_m$ that has the best mean rewards in the previous epoch $m-1$. For the estimated optimal arm $k_m$, we allocate all remaining pulls, resulting in approximately $N_m - \sum_{k \neq k_m} n_k^m$ pulls dedicated to arm $k_m$.
%This design choice cleanly resolves the open problem of eliminating the factor $K$ in the corruption-dependent term. To further stabilize epoch lengths, we select an arm $k_m$ with the best empirical performance in the previous epoch and allocate more pulls to this arm, i.e., $N_m - \sum_{k \neq k_m} n_k^m$.

However, pulling the estimated optimal arm $k_m$ too much times may lead to additional regret, as arm $k_m$ could be suboptimal. To mitigate this additional regret, we adopt a epoch-varying failure probabilities $\delta_m\approx O(1/(mK2^{2m}))$ for epoch $m$ rather than using a global failure probability $\delta$. 
This choice of $\delta_m$ also eliminate the need for knowing time horizon $T$.
Notice that a concurrent work~\cite{ghaffari2024multi} also employs static epoch lengths and remove the factor $K$ from the regret bound. However, they adopt a global $\delta$ across  all epochs, resulting in a regret of $O(C+K\log^2(T)/\Delta)$, which could be much worse than our regret $O\left(C + \sum_{\Delta_k > 0}\frac{\log^2(T)}{\Delta_k}\right)$. Here $\Delta_k$ is the suboptimality gap of arm $k$ and $\Delta=\min_{\Delta_k>0}\Delta_k$ is the minimum positive suboptimality gap. 
%Moreover, for a variety of extended settings, we need only to adjust the expected pulling times of the arms according to setting-specific prior knowledge and $n_k^m$. As a result, our framework remains both scalable and parallelizable across a wide range of scenarios.
\paragraph{Related Work}

% Bridging stochastic and adversarial bandits has been a central theme in recent years, leading to \emph{Best-of-Both-Worlds} (BOBW) results that achieve $\sqrt{T}$-type regret in adversarial settings and $\log(T)$-type regret in purely stochastic settings \citep{lu2021stochastic,bubeck2012best,seldin2014one,auer2016algorithm,wei2018more,zimmert2021tsallis,ito2021parameter,jin2024improved,tsuchiya2024stability}. When adversarial corruptions are limited by $C$, these BOBW methods often incur $O(C) + \widetilde{O}\bigl(\sum_{k \neq k^*}1 / \Delta_k\bigr)$ regret, but many require frequent, potentially expensive probability updates. To mitigate this, elimination-based algorithms \citep{lykouris2018stochastic,gupta2019better} avoid heavy distributional calculations while maintaining near-optimal corruption dependence. Similar ideas apply to graph bandits, semi-bandits, and batched bandits, each of which can benefit from specialized techniques to address exponential action spaces, partial observability, or limited adaptivity~\citep{rouyer2022near,ito2022nearly,dann2023blackbox,zimmert2019beating,ito2021hybrid,tsuchiya2023further,Perchet_2016,gao2019batched,esfandiari2021regret}. Lastly, multi-agent extensions~\citep{liu2021cooperative,wang2022achieving,chawla2023collaborative,ghaffari2024multi} allow agents to pool information and expedite learning but typically face larger regret bounds under adversarial corruptions or rely on group-level performance guarantees rather than individual regrets.
In recent years, the study of stochastic bandits with adversarial corruptions has garnered significant attention. Existing approaches can be broadly classified into two main categories: elimination-based algorithms~\citep{lykouris2018stochastic, gupta2019better, liu2021cooperative, lu2021stochastic} and FTRL-based algorithms~\citep{zimmert2021tsallis, ito2021parameter, jin2024improved, tsuchiya2024stability}. Elimination-based methods, with the Active Arm Elimination (AAE) algorithm~\citep{even2006action} being a prominent example, progressively discard suboptimal arms based on empirical reward estimates. By removing poorly performing arms, these algorithms focus their sampling efforts on a shrinking set of promising candidates. \cite{lykouris2018stochastic} introduced a multi-layer extension of the AAE algorithm, achieving a regret bound of $\widetilde{O}\left(KC\sum_{\Delta_k > 0} 1 / \Delta_k\right)$. Building on this, \cite{gupta2019better} proposed the BARBAR algorithm and reduce the regret to $\widetilde{O}\left(KC + \sum_{\Delta_k > 0} 1 / \Delta_k\right)$. On the other hand, FTRL-based methods~\citep{bubeck2012best, seldin2014one, auer2016algorithm, wei2018more, zimmert2021tsallis, ito2021parameter, jin2024improved, tsuchiya2024stability} can achieve optimal regret bound in both stochastic setting, adversarial setting and the adversarial corruption setting. However, these methods are computationally expensive than elimination-based methods since they need to solve an optimization problem in each round. 

Stochastic bandits with adversarial corruptions can be extended to graph bandits, semi-bandits, and batched bandits, each of which benefits from specialized techniques to handle challenges such as exponential action spaces, partial observability, or limited adaptivity~\citep{rouyer2022near, ito2022nearly, dann2023blackbox, zimmert2019beating, ito2021hybrid, tsuchiya2023further, Perchet_2016, gao2019batched, esfandiari2021regret}. Finally, multi-agent extensions~\citep{liu2021cooperative, wang2022achieving, chawla2023collaborative, ghaffari2024multi} enable agents to pool information and expedite the learning process but often encounter larger regret bounds under adversarial corruptions or rely on group-level performance guarantees rather than individual regrets.
\begin{table*}[!ht]
    \centering
    \footnotesize
    \renewcommand\arraystretch{2}
    \begin{tabular}{|c|c|c|c|c|}
    \hline
    Setting & Algorithm & Regret & Prop.~1 & Prop.~2\\
    \hline
    \multirow{3}{*}{Multi-arm bandits}
    & \cite{zimmert2021tsallis} & $C + \sum_{\Delta_k > 0}\frac{\log(T)}{\Delta_k}$ & & \\
    \cline{2-5}
    & \cite{jin2024improved} & $C + \sum_{\Delta_k > 0}\frac{\log(T)}{\Delta_k}$ &  & \checkmark \\
    \cline{2-5}
    & Algorithm~\ref{algs:BARBAT} & $C + \sum_{\Delta_k > 0}\frac{\log^2(T)}{\Delta_k}$ & \checkmark & \checkmark\\
    \cline{1-5}

    \multirow{5}{*}{\makecell{\makecell[c]{Strongly observable \\graph bandits }}}
    & \cite{lu2021stochastic}$^\dagger$ & $\alpha C + \sum_{k \in \cD^*}\frac{\log^2(T)}{\Delta_k}$ & \checkmark & \checkmark \\
    \cline{2-5}
    & \cite{ito2024adaptive}$^\dagger$ & $C + \frac{\alpha(\ln(K/ \alpha + 1))\log(T)}{\Delta}$ &  & \\
    \cline{2-5}
    & \cite{ito2022nearly} & $C + \frac{\alpha \log^3(T)}{\Delta}$ & & \\
    \cline{2-5}
    & \cite{dann2023blackbox} & $C + \frac{\min \{\widetilde{\alpha}, \alpha\log(K)\}\log(T)}{\Delta}$ &  & \\
    \cline{2-5}
    & Algorithm~\ref{algs:SOG-BARBAT} & $C + \sum_{k \in \cI^*}\frac{\log^2(T)}{\Delta_{k}}$ & \checkmark & \checkmark\\
    \cline{1-5}

    \multirow{2}{*}{\makecell{\makecell[c]{$d$-set \\semi-bandits$^*$}}}
    & \makecell{\cite{zimmert2019beating,ito2021hybrid} \\ \cite{tsuchiya2023further}} & $ dC + \sum_{\Delta_k > 0}\frac{\log(T)}{\Delta_k}$ & &\\
    \cline{2-5}
    & Algorithm~\ref{algs:DS-BARBAT} & $dC + \sum_{\Delta_k > 0}\frac{\log^2(T)}{\Delta_{k}}$ & \checkmark & \checkmark \\
    \cline{1-5}
    Batched bandits & Algorithm~\ref{algs:BB-BARBAT} & \makecell{$C T^{\frac{1}{M+3}} 
        + T^{\frac{4}{M+3}}\Bigl( $\\ $
            \sum_{\Delta_k > 0} \frac{M \log(T)}{\Delta_k} 
            + \frac{K \log(T)}{M \Delta} 
        \Bigr)$} & \checkmark & \checkmark \\
    \cline{1-5}

    \multirow{3}{*}{\makecell{\makecell[c]{Multi-agent\\ multi-arm bandits}}}
    & \cite{liu2021cooperative} & $VC + \frac{K\log^2(T)}{\Delta}$ & \checkmark & \checkmark \\
    \cline{2-5}
    & \cite{ghaffari2024multi} & $\frac{C}{V} + \frac{K\log^2(T)}{V\Delta}$ & \checkmark & \checkmark \\
    \cline{2-5}
    & Algorithm~\ref{algs:MA-BARBAT} & $\frac{C}{V} + \sum_{\Delta_k > 0}\frac{\log^2(T)}{V\Delta_k}$ &
    \checkmark & \checkmark \\
    \cline{1-5}

    \end{tabular}
    \caption{
    ``Prop.~1'' indicates the property that the algorithm can efficiently obtain the sampling probablity of arms. ``Prop.~2'' indicates the property that the assumption of unique optimal action is not required. $\Delta_k$ denote the suboptimality gap of arm $k$, and let $\Delta$ is the smallest positive suboptimality gap. For graph bandits, $\alpha$ denotes the \emph{independence number} of the feedback graph $G$ and  $\cI^*$ denotes the set of at most $O\!\Bigl(\alpha \ln\bigl(\frac{K}{\alpha} + 1\bigr)\Bigr)$ arms with the smallest gap. %Furthermore, let $Ind(G)$ be the collection of all independent sets in $G$. We denote by $\cD^*$ the set of arms with the smallest gap within $Ind(G)$, and by  
    ``$\dagger$": these works only consider undirected graphs while rest works for graph bandits consider directed graphs.
    %As shown in prior studies~\cite{lu2021stochastic,ito2024adaptive}, which focus on undirected graphs, one obtains $|\cI^*| \leq \alpha$ under those assumptions. 
    ``$*$": the definition of $d$-set semi-bandits is introduced in Section~\ref{sec:ds}. Here, we assume without loss of generality that $\mu_1 \geq \mu_2 \geq \cdots \geq \mu_K$. and overloading the notation $\Delta_k = \mu_k - \mu_d$  for each arm $k > d$. 
    %Notably, in the context of $d$-sets and following previous works~\cite{zimmert2019beating,dann2023blackbox}, we assume without loss of generality that $\mu_1 \leq \mu_2 \leq \cdots \leq \mu_K$. Consequently, for each arm $k > d$, $\Delta_k = \mu_k - \mu_d$. 
    }
    \label{tab:rw} % Related Works
\end{table*}