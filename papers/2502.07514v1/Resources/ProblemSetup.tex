\section{Preliminaries}
\label{sec:ps}

% Let $[K]:= \{1,2,...,K\}$ be the set of $K$ arms. In each round $t$, the interactions between the agent and the environment can be divided into the following steps:
% \begin{itemize}
%     \item Initially, the environment for the agent generates random rewards according to a fixed unknown distribution, denoted by $\{r_k(t)\}_{k \in [K]}$.
%     \item Upon observing the initial reward sequence $\{r_k(t)\}_{k \in [K]}$, the adversary attack the rewards to be $\{\widetilde{r}_k(t)\}_{k \in [K]}$ based on previous history.
%     \item The agent selects an arm $I_t$ to pull according its policy, and then only observes the reward $\widetilde{r}_{I_t}(t)$.
% \end{itemize}
% Following the work~\citep{gupta2019better}, we use the pseudo-regret to evaluate the agent's performance. Let $\mu_k$ denote the expected value of the reward distribution of arm $k \in [K]$ and $k^* = \arg\max_{k \in [K]} \mu_k$ is the optimal arm. The pseudo-regret can be formulated as follows:
% \[R(T) = \sum_{t=1}^T \mu_{k^*} - \sum_{t=1}^T \mu_{I_t} = \sum_{t=1}^T \Delta_{I_t},\]
% where we define the sub-optimality gap $\Delta_k = \mu_{k^*} - \mu_k$. We define the minimum sub-optimality gap as $\Delta=\min_{k:\delta_k>0}\delta_k$. Naturally, the corruption level $C$ should be represented as the cumulative difference in rewards after attacks
% \[C = \sum_{t=1}^T \max_{1\leq k \leq K} |\widetilde{r}_k(t) - r_k(t)|.\]

We consider stochastic multi-armed bandits with adversarial corruptions. In this setting, the agent interacts with the environment over $T$ rounds by selecting an arm from a set of $K$ arms, denoted by $[K]$. In each round $t$, the environment generates a random reward vector $\{r_{t,k}\}_{k \in [K]}$. An adversary, having access to the reward vector, subsequently attack these rewards to produce the corrupted reward vector $\{\widetilde{r}_{t,k}\}_{k \in [K]}$. The agent then selects an arm $I_t$ according to its strategy and observes the corresponding corrupted reward $\widetilde{r}_{t,I_t}$. Let $\mu_k$ denote the mean reward of arm $k\in[K]$, and let $k^* \in \arg\max_{k \in [K]} \mu_k$
be an optimal arm. The suboptimality gap for arm $k$ is defined as  $\Delta_k = \mu_k - \mu_{k^*}$, and we denote $\Delta=\min_{\Delta_k>0}\Delta_k$ as the smallest suboptimality positive gap. The corruption level is define  as $C = \sum_{t=1}^T \max_{k \in [K]} \left| \widetilde{r}_{t,k} - r_{t,k} \right|$. %Notably, unlike most FTRL algorithms that assume a unique optimal arm, our setting allows for multiple optimal arms.
Our goal is to minimize the expectation of the pseudo-regret $R(T)$
\[
R(T) = \sum_{t=1}^T \mu_{k^*} - \sum_{t=1}^T \mu_{I_t} = \sum_{t=1}^T \Delta_{I_t}.%, \quad 
%C = \sum_{t=1}^T \max_{k \in [K]} \left| \widetilde{r}_{t,k} - r_{t,k} \right|.
\]
Notice that FTRL-based methods~\citep{rouyer2022near,ito2022nearly,dann2023blackbox,zimmert2019beating,ito2021hybrid,tsuchiya2023further,Perchet_2016,gao2019batched,esfandiari2021regret} usually consider another pseudo-regret $\widetilde{R}(T) = \max_{k} \BE\Bigl[\sum_{t=1}^T \bigl(r_{t,I_t} - r_{t,k}\bigr)\Bigr]$.
Theorem~3 in \citet{liu2021cooperative} presents the conversion between the two definition of pseudo-regrets in the adversarial corruption as
$\BE\bigl[R(T)\bigr] = \Theta\bigl(\widetilde{R}(T) + O(C)\bigr)$.
%which implies that the existing upper bounds for FTRL algorithms on $R(T)$ should include an additional term of $O(C)$ in our setting.
