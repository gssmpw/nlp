\subsection{$d$-Set Semi-Bandits}
\label{sec:ds}
\begin{algorithm}[t]
    \LinesNumbered
    \SetAlgoLined
    \caption{DS-BARBAT: d-Sets-BARBAT}
    \label{algs:DS-BARBAT}
    
    \textbf{Initialization:} 
    Set the initial round \( T_0 = 0 \), \( \Delta_k^0 = 1 \), and \( r_k^0 = 0 \) for all \( k \in [K] \).

    \For{epochs $m = 1,2,\cdots$}{
        Set $\zeta_m \leftarrow (m + 4)2^{2(m+4)}\ln (K)$, and $\delta_m \leftarrow 1/(K\zeta_m)$
        
        Set $\lambda_m \leftarrow 2^8 \ln{\left(4K / \delta_m\right)}$ and $\beta_m \leftarrow \delta_m / K$.
        
        Set $n_k^m = \lambda_m (\Delta_k^{m-1})^{-2}$ for all arms $k \in [K]$.
        
        Set $N_m \leftarrow \lceil K \lambda_m 2^{2(m-1)}/d \rceil$ and $T_m \leftarrow T_{m-1} + N_m$.

        Select the arm subsets \( \cK_m = \mathop{\arg\max}_{k \in [K]} r_k^{m-1} \) with $|\cK_m| = d$. 

        Set
        $   
            \widetilde{n}_k^m = \begin{cases}
                n_k^m & k \not\in \cK_m \\
                N_m - \sum_{k \not\in k_m}n_k^m / d & k \in \cK_m
            \end{cases}
        $.

        \For{$t = T_{m-1} + 1$ to $T_m$}{
            Choose arm $I_t\sim p_m$ where $p_m(k)= \widetilde{n}_k^m / N_m$.

            Observe the corrupted reward $\widetilde{r}_{t,I_t}$ and update the total reward $S_{I_t}^m = S_{I_t}^m + \widetilde{r}_{t,I_t}$.
        }

        Set $r_k^m \leftarrow \min \{S_k^m / \widetilde{n}_k^m, 1\}$.

        Set $r_*^m \leftarrow \top(d)_{k \in [K]}\left\{r_k^m - \sqrt{\frac{4\ln(4/\beta_m)}{\widetilde{n}_k^m}}\right\}$, where $\top(d)$
        represents that the $d$ largest number.
        
        Set $\Delta_k^m \leftarrow \max\{2^{-m}, r_*^m - r_k^m\}$.
    }
\end{algorithm}

% In this paper, we only consider the d-sets setting, which is a special case of semi-bandits. In d-sets setting, the agent needs to select arbitrary $d$ arms in each round, which means that we need to consider $d$ arms instead of just one arm.

% In previous works~\citep{wei2018more,zimmert2019beating,ito2021hybrid,tsuchiya2023further}, the algorithms require explicitly computing the arm-selection probability distribution over all arms. Although the computational complexity is typically $O(K)$ in many cases, this can become problematic in d-sets setting, where the number of possible actions grows exponentially in $K$.

% Motivated by this issue, we extend BARBAT to \emph{DS-BARBAT}, presented in Algorithm~\ref{algs:DS-BARBAT}. In this setting, the agent observes the rewards of $d$ arms for each pull. Following a similar approach to MA-BARBAT, we set $N_m \leftarrow \lceil K \lambda_m 2^{2(m-1)} / d \rceil$.
% However, unlike MAB in which the optimal action is a single arm, here we consider a $d$-set setting in which the optimal action is a subset of $d$ arms. Consequently, we choose $\cK_m$ to be the $d$ arms with the highest empirical rewards in the previous epoch. When estimating the suboptimality gap, we similarly focus on the $d$ largest empirical values, reflecting the fact that all $d$ selected arms can be viewed as optimal.

% We show that the DS-BARBAT algorithm have the following regret bound. The proof is given in Appendix \ref{ape:ds}.
% \begin{theorem}
% \label{the:ds-erb}    % The expected regret of Algorithm DS-BARBAT 
%     Following \cite{zimmert2019beating}, let $\mu_1 \geq \mu_2 \geq \cdots \geq \mu_K$ be the ordering of mean rewards for the $K$ arms and $\Delta_k = \mu_k - \mu_d$ for all arms $k > d$. The expected regret of DS-BARBAT satisfies
%     \[R(T) = O\left(dC + \sum_{k=d+1}^{K}\frac{\log(T)\log(KT)}{\Delta_{k}} + \frac{K\log(1/\Delta)\log(K/\Delta)}{\Delta}\right).\]
% \end{theorem}
% \begin{remark}
%     Unlike previous works~\citep{wei2018more,zimmert2019beating,ito2021hybrid,tsuchiya2023further}, DS-BARBAT does not require computing the arm-selection probability in each round, thereby reducing the computational complexity to $O(K\log(T))$, which is very low.
% \end{remark}

In this paper, we focus on the $d$-sets setting, which is a special case of semi-bandits where the agent must select $d$ arms in each round. Specifically, let $d \in \{1, 2, \ldots, K - 1\}$ be a fixed parameter, and define the $d$-sets as
\[
\mathcal{X} = \left\{ \mathbf{x} \in \{0, 1\}^K \mid \sum_{k=1}^K x_k = d \right\},
\]
where $x_k = 1$ indicates that arm $k$ is selected, and $x_k = 0$ indicates that arm $k$ is not selected.
Following~\citep{zimmert2019beating,dann2023blackbox}, let $\mu_1 \geq \mu_2 \geq \cdots \geq \mu_K$ and $\Delta_k = \mu_k - \mu_d$ for all arms $k > d$.

%In previous works~\citep{wei2018more,zimmert2019beating,ito2021hybrid,tsuchiya2023further}, algorithms require explicitly computing the arm-selection probability distribution over all arms. While the computational complexity is typically $O(K)$ in many cases, this becomes problematic in the d-sets setting, where the number of possible actions grows exponentially with $K$.

We extend BARBAT to DS-BARBAT, presented in Algorithm~\ref{algs:DS-BARBAT}. In this setting, the agent observes the rewards of $d$ arms per pull. Similar to MA-BARBAT, we set $N_m \leftarrow \lceil K \lambda_m 2^{2(m-1)} / d \rceil$. However, unlike the traditional MAB where the optimal action is a single arm, here the optimal action is a subset of $d$ arms. Thus, we choose $\cK_m$ to be the $d$ arms with the highest empirical rewards in the previous epoch. When estimating the suboptimality gap, we focus on the $d$ largest empirical values, reflecting the fact that all $d$ selected arms are considered optimal.

The regret bound for DS-BARBAT is as follows, with the proof provided in Appendix~\ref{ape:ds}:
\begin{theorem}
\label{the:ds-erb}
The expected regret of DS-BARBAT satisfies
\[
\BE\left[R(T)\right] = O\left(dC + \sum_{k=d+1}^{K}\frac{\log(T)\log(KT)}{\Delta_{k}} + \frac{dK\log\left(1/\Delta\right)\log\left(K/\Delta\right)}{\Delta}\right).
\]
\end{theorem}


    Notice that our DS-BARBAT algorithm can efficient compute the sampling probability $p_m$, while FTRL-based methods~\citep{wei2018more,zimmert2019beating,ito2021hybrid,tsuchiya2023further} need to solve a complicated convex optimization problem in each round, which is rather expensive.

