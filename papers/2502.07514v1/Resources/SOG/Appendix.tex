\section{Proof of Theorem \ref{the:sog-erb}}
\label{ape:sog}

\subsection{Lemmas for Proving Theorem \ref{the:sog-erb}}

By employing the methods described above, we establish that Lemmas \ref{lem:tne}, \ref{lem:rkc}, \ref{lem:trl}, and \ref{lem:tsl} remain valid. In contrast to the standard multi-armed bandit setting, the feedback structure in our model implies that, for each arm $k$, the expected number of observable pulls $\hat{n}_k^m$ does not necessarily equal the expected number of actual pulls $\widetilde{n}_k^m$.


\begin{lemma}
\label{lem:ocnosg}
    For any epoch $m$, all arms $k \neq k_m$ must satisfy $\widetilde{n}_k^m \leq n_k^m$ and $\hat{n}_k^m \geq n_k^m$.
\end{lemma}
\begin{proof}
    By Algorithm \ref{algs:SOG-BARBAT}, we can easily guarantee that the inequality $\widetilde{n}_k^m \leq n_k^m$ holds for all arms $k \neq k_m$.
    Recalling the setting of strongly observable graph, for each arm $k$, either it has a self-loop or all other arms have an edge pointing to it. According to Algorithm \ref{algs:SOG-BARBAT}, we can get the guarantee as follows for each arm $k_j \in [K]$:
    \[\sum_{(k_i,k_j) \in E, k_i \neq k_j} \widetilde{n}_{k_i}^m + \widetilde{n}_{k_j}^m \geq n_{k_j}^m.\]
    So for each arm $k$ which has a self-loop, we have the following inequality:
    \[\hat{n}_k^m = \sum_{(k_i,k_j) \in E, k_i \neq k_j} \widetilde{n}_{k_i}^m + \widetilde{n}_{k_j}^m \geq n_k^m.\]
    For each arm $k$ which hasn't a self-loop, since $\widetilde{n}_{k_m}^m \geq 2^{-m} \geq n_{k}^m$ for all arms $k \neq k_m$, so we complete the proof.
\end{proof}
Same as the proof in Appendix \ref{ape:mab}, we only need to change $\widetilde{n}_k^m$ to $\hat{n}_k^m$ to obtain the following lemmas.

\begin{lemma}
\label{lem:bersog} % Bound the experimental reward (SOG)
    For any fixed $k, m$ and $\beta_m$, Algorithm \ref{algs:BARBAT} satisfies
    \[\Pr\left[|r_k^m - \mu_k| \geq \sqrt{\frac{4\ln(4 / \beta_m)}{\hat{n}_k^m}} + \frac{2C_m}{N_m}\right] \leq \beta_m.\]
\end{lemma}
We also define an event $\cE_m$ for epoch $m$ as follows:
\begin{equation*}
    \cE_m \triangleq \left\{ \forall\ k: |r_k^m - \mu_k| \leq \sqrt{\frac{4\ln(4 /\beta_m)}{\hat{n}_k^m}} + \frac{2C_m}{N_m} \right\}.
\end{equation*}

\begin{lemma}
\label{lem:pemsog} % The probability of cE_m (SOG)
     For any epoch $m$, event $\cE_m$ holds with probability at least $1 - \delta_m$. And after rigorous calculation, we have $1 / \delta_m \geq N_m$.
\end{lemma}
We also can always guarantee the following inequality:
\[|r_k^m - \mu_k| \leq \sqrt{\frac{4\ln(4 /\beta_m)}{\hat{n}_k^m}} + \frac{D_m}{N_m}.\]
It is worth noting that $\Pr[D_m = 2C_m] \geq 1 - \delta_m$ and $\Pr[D_m = N_m] \leq \delta_m$.

Next, we will bound $\Delta_k^m$, which is also the turning point of our proof. To start, we define the discounted offset rate as
\[\rho_m := \sum_{s=1}^m \frac{D_s}{8^{m-s}N_s}.\]

\begin{lemma}
\label{lem:bsgsog} %   Bound the suboptimality-gap (SOG)
    For all epochs $m$ and arms $k \neq k^*$, we can have
    \[\frac{4}{7}\Delta_k - \frac{3}{4}2^{-m} - 6\rho_m \leq \Delta_k^m \leq \frac{8}{7}\Delta_k + 2^{-(m-1)} + 2\rho_m,\]
    and for the optimal arm $k^*$, we have
    \[-\frac{3}{7}\Delta - \frac{3}{4}2^{-m} - 6\rho_m \leq \Delta_{k^*}^m \leq \frac{1}{7}\Delta + 2^{-(m-1)} + 2\rho_m.\]
\end{lemma}
\begin{proof}
    Since $|r_k^m - \mu_k| \leq \sqrt{\frac{4\ln(4 /\beta_m)}{\hat{n}_k^m}} + \frac{D_m}{N_m}$, we have
    \[-\frac{D_m}{N_m} - \sqrt{\frac{4\ln(4 /\beta_m)}{\hat{n}_k^m}} \leq r_{k}^m - \mu_{k} \leq \frac{D_m}{N_m} + \sqrt{\frac{4\ln(4 /\beta_m)}{\hat{n}_k^m}}.\]
    By Lemma \ref{lem:ocnosg}, we can get $\hat{n}_k^m \geq n_k^m$ for all arms $k \neq k_m$ and $\hat{n}_{k_m}^m \geq \sum_{k \neq k_m} \widetilde{n}_k^m$ for arm $k_m$. Then we have the following inequalities:
    \[\forall k \neq k_m: \sqrt{\frac{4\ln(4/\beta_m)}{\hat{n}_k^m}} \leq \sqrt{\frac{4\ln(4/\beta_m)}{n_k^m}} = \frac{\Delta_k^{m-1}}{8},\]
    and
    \[\sqrt{\frac{4\ln(4/\beta_m)}{\hat{n}_{k_m}^m}} \leq \max_{k \neq k_m}\sqrt{\frac{4\ln(4/\beta_m)}{n_{k_m}^m}} = \min_{k \neq k_m}\frac{\Delta_k^{m-1}}{8}.\]
    Additionally, given that
    \[r_{*}^m \leq \max_k \left\{\mu_{k} + \frac{D_m}{N_m} + \sqrt{\frac{4\ln(4 /\beta_m)}{\hat{n}_k^m}} - \sqrt{\frac{4\ln(4 /\beta_m)}{\hat{n}_k^m}}\right\} \leq \mu_{k^*} + \frac{D_m}{N_m},\]
    and
    \begin{equation*}
        \begin{split}
        r_{*}^m &= \max_k \left\{r_k^m - \sqrt{\frac{4\ln(4/\beta_m)}{\hat{n}_k^m}}\right\} \\
        &\geq r_{k^*}^m - \sqrt{\frac{4\ln(4/\beta_m)}{\hat{n}_{k^*}^m}} \\
        &\geq \mu_{k^*} - \frac{1}{4}\max \{\Delta_{k^*}^{m-1}, \min_{k \neq k_m}\Delta_{k}^{m-1}\} - \frac{D_m}{N_m} \\
        &\geq \mu_{k^*} - \frac{1}{4}\max \{\Delta_{k^*}^{m-1}, \Delta^{m-1}\} - \frac{D_m}{N_m}
        \end{split}
    \end{equation*}
    It follows that
    \[-\frac{D_m}{N_m} - \frac{1}{4}\max \{\Delta_{k^*}^{m-1}, \Delta^{m-1}\} \leq r_{*}^m - \mu_{k^*} \leq \frac{D_m}{N_m}.\]
    We now establish the upper bound for $\Delta_k^m$ using induction on epoch $m$.
    
    For the base case $m = 1$, the statement is trivial as $\Delta_k^1 = 1$ for all $k \in [K]$.

    Assuming the statement is true for $m-1$, for arm $k \neq k^*$, since there are at least two arms: one optimal arm and one sub-optimal arm, we can obtain
    \[\min_{k \neq k_m}\Delta_k^{m-1} \leq \max\{\Delta_{k^*}^{m-1}, \Delta^{m-1}\} \leq \frac{8}{7}\Delta + 2^{-(m-1)} + 2\rho_m.\]    
    Then we have
    \begin{equation*}
    \begin{split}
        \Delta_k^m = r_*^m - r_k^m
        &= (r_*^m - \mu_*) + (\mu_* - \mu_k) + (\mu_k - r_k^m) \\
        &\leq \frac{D_m}{N_m}+ \Delta_k + \frac{D_m}{N_m} + \frac{1}{8}\max \{\Delta_k^{m-1}, \min_{k \neq k_m}\Delta_k^{m-1}\} \\
        &\leq \frac{2D_m}{N_m} + \Delta_k + \frac{1}{8}\left(\frac{8 \max\{\Delta_k, \Delta\}}{7} + 2^{-(m-1)} + 2\rho_{m-2}\right) \\
        &\leq \frac{8\Delta_k}{7} + 2^{-(m-1)} + 2\rho_m.
    \end{split}
    \end{equation*}
    Where the second inequality follows from the induction hypothesis.

    For arm $k^*$, we have
    \begin{equation*}
    \begin{split}
        \Delta_{k^*}^m = r_*^m - r_{k^*}^m
        &= (r_*^m - \mu_*) + (\mu_* - \mu_{k^*}) + (\mu_{k^*} - r_{k^*}^m) \\
        &\leq \frac{D_m}{N_m} + \frac{D_m}{N_m} + \frac{1}{8} \max \{\Delta_{k^*}^{m-1}, \min_{k \neq k_m}\Delta_{k}^{m-1}\} \\
        &\leq \frac{2D_m}{N_m} + \frac{1}{8}\left(\frac{8 \Delta}{7} + 2^{-(m-1)} + 2\rho_{m-1}\right) \\
        &\leq \frac{1}{7}\Delta + 2^{-(m-1)} + 2\rho_m.
    \end{split}
    \end{equation*}
    Next, we establish the lower bound for $\Delta_k^m$. Specifically, for arm $k \neq k^*$ we demonstrate that
    \begin{equation*}
    \begin{split}
        \Delta_k^m =  r_*^m - r_k^m
        &= (r_*^m - \mu_*) + (\mu_* - \mu_k) + (\mu_k - r_k^m) \\
        &\geq -\frac{D_m}{N_m} - \frac{1}{4}\max \{\Delta_{k^*}^{m-1}, \Delta^{k-1}\} + \Delta_k -\frac{D_m}{N_m} - \frac{1}{8}\Delta_k^{m-1} \\
        &\geq -\frac{2D_m}{N_m} + \Delta_k - \frac{3}{8}(\frac{8 \max\{\Delta_k, \Delta\}}{7} + 2^{-(m-2)} + 2\rho_{m-1}) \\
        &\geq \frac{4}{7}\Delta_k - \frac{3}{2}2^{-m} - 6\rho_m.
    \end{split}
    \end{equation*}
    where the third inequality comes from the upper bound of $\Delta_k^{m-1}$.

    For arm $k^*$, we have
    \begin{equation*}
    \begin{split}
        \Delta_{k^*}^m =  r_*^m - r_{k^*}^m
        &= (r_*^m - \mu_*) + (\mu_* - \mu_{k^*}) + (\mu_{k^*} - r_{k^*}^m) \\
        &\geq -\frac{D_m}{N_m} - \frac{1}{4}\max \{\Delta_{k^*}^{m-1}, \Delta^{k-1}\} -\frac{D_m}{N_m} - \frac{1}{8}\Delta_{k^*}^{m-1} \\
        &\geq -\frac{2D_m}{N_m} - \frac{3}{8}(\frac{8\Delta}{7} + 2^{-(m-2)} + 2\rho_{m-1}) \\
        &\geq -\frac{3}{7}\Delta - \frac{3}{2}2^{-m} - 6\rho_m.
    \end{split}
    \end{equation*}
    This proof is complete.
\end{proof}

\begin{lemma}
\label{lem:sdsog}    % The size of D (SOG)
    For any strongly observable directed graph $G$ with the independence number $\alpha$, the obtained out-dominating set $\cD$ must satisfies $|\cD| \leq \alpha(1 + 2\ln(K/\alpha))$ by Algorithm \ref{algs:OODS}. Especially, when $G$ is a acyclic graph (include undirected graphs), we have $|\cD| \leq \alpha$.
\end{lemma}
\begin{proof}
    Recalling the definition of the no-root vertex, for any strongly observable directed graph $G_1$ with the independence number $\alpha$, we can get that if we remove an no-root vertex $k$ and its out-degree neighbors, the remaining graph $G_2$ whose independence number at most $\alpha - 1$. That's because no vertex in graph $G_2$ is connected to the vertex $k$. By the definition of strongly observable directed acyclic graph, we can always get the no-root vertex for the remaining graph, which means that $|\cD| \leq \alpha$.

    By Lemma 43 in \cite{dann2023blackbox}, for any strongly observable directed graph $G_s = (V_s,E)$ with the independence number $\alpha$, we set $p_1 = p_2 = \cdots = p_{|V_s|} = \frac{1}{|V_s|}$, which can derive the following inequality:
    \[\exists \quad k_j \in [V_s]: \quad 1 + \sum_{(k_i,k_j) \in E} 1 \geq \frac{|V_s|}{2\alpha}.\]
    By Algorithm \ref{algs:OODS} removes the vertex and its out-degree neighbors, which has the largest out-degree neighbors, in the remaining graph, the number of all vertices satisfies $|V_{s+1}| \leq (1-\frac{1}{2\alpha})|V_s| \leq |V_s| e^{-1/(2\alpha)}$.
    
    When the graph $G$ has cycles, by Algorithm \ref{algs:OODS}, given that $G_s = (V_s,E)$ with the independence number $\alpha_s$, which is the final graph without cycles. We can discuss this in two cases.

    \paragraph{Case 1:} $|V_s| < \alpha$.

    We can get an graph $G_{\eta} = (|V_{\eta}|,E)$ with $\eta \leq s$, which satisfies $|V_{\eta}| < \alpha$ and $|V_{\eta - 1}| \geq \alpha$, since $V_{\eta + 1} \leq K e^{-1/(2\alpha)}$, we have $\eta - 1 \leq 2\alpha \ln(K/|V_{\eta}|) \leq 2\alpha \ln(K/\alpha)$. So we can guarantee 
    \[|\cD| \leq |V_{\eta}| + 2\alpha \ln(K/\alpha) + 1 \leq \alpha(1 + 2\ln(K/\alpha)).\]

    \paragraph{Case 2:} $|V_s| \geq \alpha$.
    
    By $V_{s} \leq K e^{-1/(2\alpha)}$, we have $s \leq 2\alpha \ln(K/|V_{s}|) \leq 2\alpha \ln(K/\alpha)$. Because $G_s$ has no cycles and the independence number $\alpha_s \leq \alpha$, we can get
    \[|\cD| \leq \alpha_s + 2\alpha \ln(K/\alpha) \leq \alpha(1 + 2\ln(K/\alpha)).\] 
    The proof is complete.
\end{proof}

\subsection{Proof for Theorem \ref{the:sog-erb}}

Recalling Lemma \ref{lem:bsgsog}, for all arms $k \neq k^*$, we have
\[\frac{4}{7}\Delta_k - \frac{3}{4}2^{-m} - 6\rho_m \leq \Delta_k^m \leq \frac{8}{7}\Delta_k + 2^{-(m-1)} + 2\rho_m.\]
We first define the regret $R_k^m$ generated by arm $k$ in epoch $m$ as 
\[R_k^m\triangleq\Delta_k\widetilde{n}_{k}^m=\begin{cases}
    \Delta_kn_{k}^m & k\neq k_m\\
    \Delta_k\widetilde{n}_{k}^m & k=k_m
\end{cases}.\]
Then we analyze the regret in following three cases:

\paragraph{Case 1:} $0<\Delta_k\le 64\rho_{m-1}$.\\ %$k\in\cA^m$.
If $k \neq k_m$, then we have \[R_k^m=\Delta_k n_{k}^m\le 64\rho_{m-1}n_k^m .\] 
If $k=k_m$, then we have \[R_k^m=\Delta_k\widetilde{n}_{k_m}^m\le 64\rho_{m-1}N_m.\]

\paragraph{Case 2:} $\Delta_k \leq 8 \cdot 2^{-m}$ and $\rho_{m-1} \leq \frac{\Delta_k}{64}$.\\
If $k \neq k_m$, then we have \[R_k^m=n_{k}^m\Delta_k =\lambda_m(\Delta_k^{m-1})^{-2} \Delta_k \leq \lambda_m 2^{2(m-1)} \Delta_k  \leq \frac{16 \lambda_m}{\Delta_k}.\] 
If $k=k_m$, since $\Delta_{k_m}^{m-1} = 2^{-(m-1)}$, we can get:
\begin{align*}
    R_k^m=\widetilde{n}_{k_m}^m \Delta_{k_m} \leq 
    N_m\Delta_{k_m} = \lceil K\lambda_m2^{2(m-1)} \rceil \Delta_{k_m} \leq \frac{16K\lambda_m}{\Delta_{k_m}} +\Delta_{k_m} \leq \frac{16K\lambda_m}{\Delta} + 1.
\end{align*}

\paragraph{Case 3:} $\Delta_k > 8 \cdot 2^{-m}$ and $\rho_{m-1} \leq \frac{\Delta_k}{64}$.\\
By Lemma \ref{lem:bsg} we have
\[\Delta_k^{m-1} \geq \frac{4}{7}\Delta_k - \frac{3}{2}2^{-m} - \frac{6}{64}\Delta_k \geq \Delta_k\left(\frac{4}{7} - \frac{3}{16} - \frac{6}{64}\right) \geq 0.29 \Delta_k.\]
In this case, it is impossible that $k=k_m$ because $\Delta_{k_m}^{m-1} = 2^{-(m-1)}<0.29\cdot 8\cdot 2^{-m}<0.29\Delta_k$.
So we can obtain
\begin{align*}
   R_k^m= n_k^m \Delta_k = \lambda_m(\Delta_k^{m-1})^{-2} \Delta_k
    \leq \frac{\lambda_m}{0.29^2 \Delta_k} 
    \leq \frac{16\lambda_m}{\Delta_k}.
\end{align*}

We define $\cA^m\triangleq\left\{ k\in[K]\,\big|\,0<\Delta_k\le 64\rho_{m-1} \right\}$ for epoch $m$. By combining all three cases, we can upper bound the regret as
\begin{equation}\label{eq:sog-mab-regret}
\begin{split}
R(T)=&\sum_{m=1}^M\Bigg(\sum_{k \in \mathcal{A}^m} R_k^m + \sum_{k \notin \mathcal{A}^m} R_k^m\Bigg)\\    
\le& \sum_{m=1}^M\Bigg( 64\rho_{m-1}N_m+\sum_{k \in \mathcal{A}^m,k\neq k_m} 64\rho_{m-1}n_k^m + \sum_{k \notin \mathcal{A}^m, \Delta_k>0} \frac{16\lambda_m}{\Delta_k}\\
&+ \left(\frac{16K\lambda_m}{\Delta} + 1\right)\BI(0<\Delta_{k_m} \le 8 \cdot 2^{-m}) \Bigg)\\ 
\le& \sum_{m=1}^M\Bigg( 64\rho_{m-1}N_m+\sum_{\Delta_k>0} 64\rho_{m-1}n_k^m + \sum_{\Delta_k>0} \frac{16\lambda_m}{\Delta_k}\\
&+ \left(\frac{16K\lambda_m}{\Delta} + 1\right)\BI\left(m\le \log_2\left(8/\Delta\right)\right)\Bigg)\\ 
\le& \sum_{m=1}^M\Bigg( 128\rho_{m-1}N_m + \sum_{\Delta_k>0} \frac{16\lambda_m}{\Delta_k}\Bigg)+ \sum_{m=1}^{\log_2(8/\Delta)}\left(\frac{16K\lambda_m}{\Delta} + 1\right)
\end{split}
\end{equation}

where the last inequality uses the fact that $\sum_{\Delta_k>0} n_k^m\le N_m$. Notice that we can bound the expectation of the offset level as
\[\mathbb{E}[D_m] = 2(1-\delta_m)C_m + \delta_m N_m \leq 2C_m + 1\]
and we can bound $\sum_{m=1}^M \rho_{m-1} N_m$ as
\begin{equation}\label{eq:sog-mab-rho}
    \begin{split}
        \sum_{m=1}^M \rho_{m-1} N_m
        &\leq \sum_{m=1}^M \left(\sum_{s=1}^{m-1}\frac{D_s}{8^{m-1-s}N_s}\right)N_m \\
        &\leq 4\sum_{m=1}^M \left(\sum_{s=1}^{m-1}\frac{(4^{m-1-s} + 1)\lambda_m}{8^{m-1-s}\lambda_s}D_s\right) \\
        &= 4\sum_{m=1}^M \left(\sum_{s=1}^{m-1}((7/12)^{m-1-s} + (7/48)^{m-1-s})D_s\right) \\
        &= 4\sum_{s=1}^{M-1} D_s \sum_{m=s+1}^M (7/12)^{m-1-s} + (7/48)^{m-1-s}\\
        &\leq 4\left(\sum_{m=1}^{M-1} D_m\right)\sum_{j=0}^{\infty} \left(7/12\right)^{j} + \left(7/48\right)^{j} 
        \leq 11\sum_{m=1}^{M-1} D_m.
    \end{split}
\end{equation}
For each arm $k \not\in \cA^m$, we have $n_k^m \leq \frac{16\lambda_m}{\Delta_k^2}$. Recalling the process of setting the actual expected number of pulls, for ease of analysis, if removing multiple arms from $H^m$ in one loop, we also consider this case as multi loops where the $\cH_s^m$ is set by zero. Defining $b = K - |\cA^m|$, by Lemma \ref{lem:sdsog}, we can set a parameter $\gamma^m = \max_{s = 1,2,\cdots,b} |D_s^m|$. With loss of generality, for each arm $k \in \cA^m$, we set $\Delta_1 \geq \Delta_2 \geq \cdots \geq \Delta_{b}$. Recalling the process of Algorithm \ref{algs:SOG-BARBAT}, we only need to pull $\cH_s^m$ times for all arms in the out-dominating set $D_s^m$, where we can observe at least $\cH_s^m$ times for all arms $k \in [K]$. By this way, to maximize the value of $\sum_{k \in \cA^m} R_k^m$, we should try to select the larger part of the suboptimal arm from the arms that have not been removed. In addition, we can have the following inequality:
\[\cH_1^m \leq n_1^m = \frac{16\lambda_m}{\Delta_1^2}, \cH_2^m - \cH_1^m \leq \frac{16\lambda_m}{\Delta_2^2}, \cdots , \cH_{b}^m - \cH_{b - 1}^m \leq \frac{16\lambda_m}{\Delta_{b}^2}.\]
By this way, we can get the following equation:
\begin{equation}\label{eq:sog-mab-scale}
\begin{split}
    \sup \sum_{k \in \cA^m} R_k^m &= \sup \sum_{s=1}^{b} \cH_s^m \left(\sum_{k \in \cA^m, k \in D_s^m}\Delta_k \right)\\
    &= \sum_{s = 1}^{b - \gamma^m + 1} \left(\frac{16\lambda_m}{\Delta_s^2} - \frac{16\lambda_m}{\Delta_{s-1}^2}\right) \left(\sum_{k = s}^{s + \gamma^m} \Delta_k\right) 
    + \sum_{s = b - \gamma^m + 2}^{b} \left(\frac{16\lambda_m}{\Delta_s^2} - \frac{16\lambda_m}{\Delta_{s-1}^2}\right) \left(\sum_{k=s}^{b} \Delta_k\right)\\
    &=\sum_{k=1}^{\gamma^m} \frac{16\lambda_m}{\Delta_k} + \sum_{k=\gamma^m+1}^{b}\left(\frac{16\lambda_m}{\Delta_k^2} - \frac{16\lambda_m}{\Delta_{k-\gamma^m}^2}\right)\Delta_k\\
    &\leq \sum_{k=\gamma^m + 1}^{2\gamma^m} \frac{20\lambda_m}{\Delta_k} + \sum_{k=2\gamma^m+1}^{b}\left(\frac{16\lambda_m}{\Delta_k^2} - \frac{16\lambda_m}{\Delta_{k-1}^2}\right)\Delta_k 
    \leq \cdots \leq \sum_{k=b-\gamma^m +1}^{b} \frac{32\lambda_m}{\Delta_k}.
\end{split}
\end{equation}

In the second inequality, for ease of write, we set $\frac{16\lambda_m}{\Delta_0^2} = 0$. For the above inequalities, we use the inequality $\frac{\beta_0}{\Delta_k} + \left(\frac{1}{\Delta_{\gamma^m+k}^2} - \frac{1}{\Delta_k^2}\right)\Delta_{\gamma^m + k} \leq \frac{\beta_1}{\Delta_{\gamma^m + k}}$ holds for all $2\geq \beta_1 > \beta_0 \geq 1$. For example, in the first inequality, we have
\begin{flalign*}
    & \sum_{k=1}^{\gamma^m} \frac{16\lambda_m}{\Delta_k} + \sum_{k=\gamma^m + 1}^{2\gamma^m}\left(\frac{16\lambda_m}{\Delta_k^2} - \frac{16\lambda_m}{\Delta_{k-\gamma^m}^2}\right)\Delta_k & \\
    & = 16\lambda_m \sum_{k=1}^{\gamma^m} \left(\frac{1}{\Delta_k} + \left(\frac{1}{\Delta_{\gamma^m+k}^2} - \frac{1}{\Delta_k^2}\right)\Delta_{\gamma^m + k}\right)
    \leq \sum_{k=\gamma^m + 1}^{2\gamma^m} \frac{20\lambda_m}{\Delta_k}. &
\end{flalign*}
Combining Eq.~\eqref{eq:sog-mab-regret}, Eq.~\eqref{eq:sog-mab-rho} and Eq.~\eqref{eq:sog-mab-scale}, by Lemma \ref{lem:tsl}, we can get
\begin{equation*}
    \begin{split}
        \BE[R(T)]
        &\leq \sum_{m=1}^{M-1} 1440 \BE[D_m] + \sum_{m=1}^M \sum_{\Delta_k > 0}\frac{16\lambda_m}{\Delta_k} + \sum_{m=1}^{\log_2(8/\Delta)}\left(\frac{16K\lambda_m}{\Delta} + 1\right) \\
        &\leq \sum_{m=1}^{M-1} 1440 \BE[D_m] + \sum_{\Delta_k > 0}\frac{2^{12}(\log^2(T) + 3\log(T)\log(30K))}{\Delta_k} \\
        &\quad + \frac{2^{12}K(\log^2(8/\Delta) + 3\log(8/\Delta)\log(30K)))}{\Delta} + \log(8/\Delta)\\
        &\leq \sum_{m=1}^{M-1} 2880 C_m + 1440 + \sum_{\Delta_k > 0}\frac{2^{14}\log(T)\log(30KT)}{\Delta_k} \\
        &\quad+ \frac{2^{14}K\log(8/\Delta)\log(240K / \Delta))}{\Delta} + \log(8/\Delta) \\
        &\leq \sum_{m=1}^{M-1} 2880 C_m + 1440 + \sum_{k \in \cI^*}\frac{2^{15}\log(T)\log(30KT)}{\Delta_k} \\
        &\quad+ \frac{2^{14}K\log(8/\Delta)\log(240K / \Delta))}{\Delta} + \log(8/\Delta) \\ 
        &= O\left(C + \sum_{k \in \cI^*}\frac{\log(T)\log(KT)}{\Delta_k} + \frac{K\log(1 / \Delta)\log(K / \Delta)}{\Delta}\right).
    \end{split}
\end{equation*}