\section{Related Work}
\label{sec:rw}

In this section, we briefly review the related work. In addition, we provide table \ref{tab:rw} to compare our work with some works in recent years.

\begin{table*}[!ht]
    \centering
    \small
    \renewcommand\arraystretch{2}
    \begin{tabular}{|c|c|c|c|c|c|}
    \hline
    Setting & Algorithm & Regret & Rd. 1 & Rd. 2 & Rd. 3 \\
    \hline
    \multirow{3}{*}{MAB}
    & \cite{jin2024improved} & $C + \sum_{\Delta_k > 0}\frac{\log(T)}{\Delta_k}$ & & &  \\
    \cline{2-6}
    & \cite{honda2023follow} & $C + \sum_{\Delta_k > 0}\frac{\log(T)}{\Delta_k} + \frac{K}{\Delta}$ & \checkmark & & \\
    \cline{2-6}
    & BARBAT & $C + \sum_{\Delta_k > 0}\frac{\log^2(T)}{\Delta_k} + \frac{K}{\Delta}$ & \checkmark & \checkmark & \checkmark \\
    \cline{1-6}

    \multirow{3}{*}{\makecell{\makecell[c]{Graph Bandits \\ Strongly observable}}}
    & \cite{ito2022nearly} & $C + \frac{\alpha \log^3(T)}{\Delta}$ & & & \\
    \cline{2-6}
    & \cite{dann2023blackbox} & $C + \frac{\min \{\widetilde{\alpha}, \alpha\log(K)\}\log(T)}{\Delta}$ & &  &  \\
    \cline{2-6}
    & SOG-BARBAT & $C + \sum_{k \in \cI^*}\frac{\log^2(T)}{\Delta_{k}} + \frac{K}{\Delta}$ & \checkmark & \checkmark & \checkmark \\
    \cline{1-6}

    % \multirow{3}{*}{\makecell{\makecell[c]{Graph Bandits \\ Weakly observable}}}
    % & \cite{ito2022nearly} & $C + \frac{\gamma \log^2(T)}{\Delta}$ & & & \\
    % \cline{2-6}
    % & \cite{dann2023blackbox} & $C + \frac{\gamma \log(K) \log(T)}{\Delta}$ & &  &  \\
    % \cline{2-6}
    % & WOG-BARBAT & $\sum_{k \in \mathcal{T}^*}\frac{C}{\gamma \Delta_k} + \sum_{k \in \mathcal{T}^*}\frac{\log^2(T)}{\Delta_{k}^2} + \frac{\gamma}{\Delta}$ & \checkmark & \checkmark & \checkmark \\
    % \cline{1-6}

    \multirow{3}{*}{\makecell{\makecell[c]{Semi-bandits \\ d-sets}}}
    & \cite{zimmert2019beating} & $C + \sum_{\Delta_k > 0}\frac{\log(T) + \log^2(K)}{\Delta_k}$ & & & \\
    \cline{2-6}
    & \cite{ito2021hybrid} & $C + \sum_{\Delta_k > 0}\frac{\log(T)}{\Delta_k}$ & & & \\
    \cline{2-6}
    & DS-BARBAT & $C + \sum_{\Delta_k > 0}\frac{\log^2(T)}{\Delta_{k}} + \frac{K}{ \Delta}$ & \checkmark & \checkmark & \checkmark \\
    \cline{1-6}
    \end{tabular}
    \caption{Overview of regret bounds. "Rd. 1" represents that the algorithm does not need to solve the optimization problem to obtain the arm pulling probability. "Rd. 2" represents that the algorithm does not need to calculate the the arm pulling probability, which means that the algorithm only needs to pay a sub-linear computational cost. "Rd. 3" indicates whether the algorithm can be easily extended to the corresponding multi-agent environment.}
    \label{tab:rw} % Related Works
\end{table*}

\paragraph{Stochastic Bandits with Adversarial Corruption}
In recent years, considerable attention has been directed towards bridging the gap between stochastic and adversarial bandits. In adversarial bandits, the reward observed by the agent is arbitrarily determined by an adversary. However, the real environment is often not so extreme, which is why it is necessary to propose stochastic bandits with adversarial corruption. Research efforts in this domain have predominantly focused on achieving optimal guarantees for both settings, also called Best-of-Both-Worlds (BOBW) guarantees~\citep{bubeck2012best,seldin2014one,auer2016algorithm,seldin2017improved,wei2018more,zimmert2021tsallis,ito2021parameter,jin2024improved,tsuchiya2024stability}. The primary objective has been to secure a regret of the order of $\sqrt{T}$ in adversarial settings while maintaining a regret of $\log T$ in stochastic settings. In addition, they can also achieve the optimal regret $O(C) + \widetilde{O}(\sum_{k \neq k^*}1 / \Delta_k)$ in the stochastic MAB with adversarial corruption. However, a notable limitation of most existing Best-of-Both-Worlds (BOBW) policies is the requirement to explicitly compute the list of arm-selection probabilities for all arms. While the computational complexity is typically $O(K)$
in many standard scenarios, this approach can become computationally infeasible in more intricate settings, such as semi-bandits, where the number of possible actions grows exponentially with $K$. Recent work by~\cite{lykouris2018stochastic} addressed these challenges by improving the AAE algorithm. Their approach achieves a regret bound of $\widetilde{O}\left(KC\sum_{\Delta_k > 0}1 / \Delta_k\right)$, where $C$ represents the level of adversarial corruption. Building on this foundation,~\cite{gupta2019better} introduced the Bandit Algorithm with Robustness: Bad Arms Get Recourse (BARBAR) algorithm, which achieves a regret bound of $\widetilde{O}\left(KC + \sum_{\Delta_k > 0}1 / \Delta_k\right)$. These advancements illustrate how incorporating robustness into bandit algorithms can mitigate the adverse effects of corruption, albeit with a moderate trade-off in performance. 

\paragraph{Graph Bandits}
The study of directed graph bandits was specifically defined by~\cite{alon2015online}, who proposed three types of graphs, e.g. strongly observable graph, weakly observable graph and not observable graph. However, their algorithm is only aimed at adversarial bandits. Recently, many researchers extends Follow-The-Regularized-Learning (FTRL) algorithm to the graph bandits, achieve near optimal regret in BOBW~\citep{erez2021towards,rouyer2022near,ito2022nearly,dann2023blackbox}. Among these works,~\cite{dann2023blackbox} proposed a novel algorithm, which can achieve $O(C) + \widetilde{O}\{\min (\widetilde{\alpha},\alpha \log(K)) / \Delta\}$, where $\widetilde{\alpha}$ is the independence number of the graph $G$ obtained by removing all one-sided edges. 

\paragraph{Semi-Bandits}
\cite{gyorgy2007line} and \cite{gai2012combinatorial} initiated research on semi-bandits in the adversarial setting, and since then, many algorithms with $O(\sqrt{T})$-regret bounds have been developed
\cite{neu2013efficient,audibert2014regret,neu2015first,wei2018more}.
In the stochastic setting, the algorithms in the literature are significantly different from those in the adversarial regime. Most are based on index-based approaches, where the algorithm estimates the loss means for each base-arm and pessimistically predicts the true value of the losses. \cite{kveton2015tight} and \cite{wang2018thompson} gave a regret which rely on $\Delta$ rather than $\Delta_k$. Recently, many researchers extends FTRL algorithm to the semi-bandits, achieve near optimal in various setting \cite{wei2018more,zimmert2019beating,ito2021hybrid,tsuchiya2023further}.

\paragraph{Batched Bandits}
In recent years, many studies on batched bandits have been published~\citep{Perchet_2016,agarwal2017learning,gao2019batched,han2020sequential,jin2021double,ruan2021linear,esfandiari2021regret,feng2022lipschitz}.~\cite{gao2019batched} studies the stochastic
batched bandits and and proposed any algorithm to achieve the optimal regret, which requires at least $\log(T)$ batches. They also proposed a novel algorithm, which achieves the regret $\widetilde{O}(K T^{1/\cM} / \Delta)$.~\cite{esfandiari2021regret} further proposed a novel algorithm, which can suffer from less regret as $\widetilde{O}(\sum_{\Delta_k > 0}T^{1/\cM} / \Delta_k)$. But to the best of our knowledge, no research is about the stochastic batched bandits with adversarial corruption, which is a bit strange because there are a lot of studies on stochastic bandits with adversarial corruptions. So we hope that this paper can provide some basis in this direction.

\paragraph{Multi-Agent Bandits}
The CMA2B has garnered significant attention in recent research~\citep{cesa2016delay,chawla2020gossiping,huang2021federated,liu2021cooperative,wang2022achieving,chawla2022multiagent,chawla2023collaborative}, most of which are either for stochastic settings or only for adversarial settings. To the best of our knowledge, the work by~\cite{liu2021cooperative} is the only study on CMA2B with adversarial corruption before us. Their algorithm achieves a group expected regret of $O(VC) + \widetilde{O}(K / \Delta)$, which is significantly inferior to the performance of our algorithm. Additionally, their approach lacks guarantees for individual regret, suggesting that individual regret could potentially be as substantial as the group regret. But recently, we find a concurrent work~\citep{ghaffari2024multi}, where they proposed a novel algorithm that suffer from the regret $O(C) + \widetilde{O}(K / \Delta)$, the latter term is much larger than the corruption-independent term MA-BARBAT.
