\begin{table*}[t]
\centering

\resizebox{1.0\linewidth}{!}{
\begin{tabular}{l@{\hskip-30pt}rrrrrrrrrrrrr}\toprule
&
    &\multicolumn{4}{c}{$T$=256k} 
    &\multicolumn{4}{c}{$T$=512k} 
    &\multicolumn{4}{c}{$T$=1024k} \\
\cmidrule(lr){3-6}\cmidrule(lr){7-10}\cmidrule(lr){11-14}
&
    &\multicolumn{2}{c}{VRAM (GB)}&\multicolumn{2}{c}{Latency ($\mu$s)}
    &\multicolumn{2}{c}{VRAM (GB)}&\multicolumn{2}{c}{Latency ($\mu$s)}
    &\multicolumn{2}{c}{VRAM (GB)}&\multicolumn{2}{c}{Latency ($\mu$s)}
\\
\cmidrule{1-14}
FA2 (1M window)* & Runtime
    &\multicolumn{2}{r}{\cellcolor[HTML]{fec54e}20.0 (100\%)} 
    &\multicolumn{2}{r}{\cellcolor[HTML]{fec74d}1,193 (100\%)} 
    &\multicolumn{2}{r}{\cellcolor[HTML]{feaf59}36.0 (100\%)} 
    &\multicolumn{2}{r}{\cellcolor[HTML]{feb158}2,325 (100\%)} 
    &\multicolumn{2}{r}{\cellcolor[HTML]{ff8370}68.0 (100\%)} 
    &\multicolumn{2}{r}{\cellcolor[HTML]{ff8370}4,645 (100\%)} \\
InfLLM (12K) & Runtime
    &\multicolumn{2}{r}{\cellcolor[HTML]{00b1b0}4.8 (23.8\%)} 
    &\multicolumn{2}{r}{\cellcolor[HTML]{fec74d}1,186 (99.4\%)} 
    &\multicolumn{2}{r}{\cellcolor[HTML]{00b1b0}4.8 (13.2\%)} 
    &\multicolumn{2}{r}{\cellcolor[HTML]{fec74d}1,194 (51.4\%)} 
    &\multicolumn{2}{r}{\cellcolor[HTML]{00b1b0}4.8 (6.99\%)} 
    &\multicolumn{2}{r}{\cellcolor[HTML]{fec64e}1,234 (26.6\%)} \\
\cdashline{1-14}[0.5pt/1pt]\rule{0pt}{2.6ex}%
\multirow{12}{*}{\makecell[l]{\textbf{Ours}\\with\\Extend \&\\Offload\\(3K-fast)}} 
    & Runtime (Fast) 
    &\multicolumn{2}{r}{\cellcolor[HTML]{00b1b0}6.1 (30.4\%)} 
    &\multicolumn{2}{r}{\cellcolor[HTML]{1db3a5}532 (44.6\%)} 
    &\multicolumn{2}{r}{\cellcolor[HTML]{00b1b0}6.1 (16.9\%)} 
    &\multicolumn{2}{r}{\cellcolor[HTML]{abc06d}902 (38.8\%)} 
    &\multicolumn{2}{r}{\cellcolor[HTML]{00b1b0}6.1 (8.93\%)} 
    &\multicolumn{2}{r}{\cellcolor[HTML]{feba54}1,864 (40.1\%)} \\
& Runtime (Flash) 
    &\multicolumn{2}{r}{\cellcolor[HTML]{00b1b0}6.1 (30.4\%)} 
    &\multicolumn{2}{r}{\cellcolor[HTML]{00b1b0}325	(27.2\%)} 
    &\multicolumn{2}{r}{\cellcolor[HTML]{00b1b0}6.1 (16.9\%)} 
    &\multicolumn{2}{r}{\cellcolor[HTML]{08b1ad}475	(20.4\%)} 
    &\multicolumn{2}{r}{\cellcolor[HTML]{00b1b0}6.1 (8.93\%)} 
    &\multicolumn{2}{r}{\cellcolor[HTML]{95be76}844	(18.2\%)} \\
\cmidrule{2-14}
\multicolumn{2}{r}{Cached Stages} 
    & None & S1 & S1\&2 &All 
    & None & S1 & S1\&2 &All 
    & None & S1 & S1\&2 &All \\
\cdashline{2-14}[0.5pt/1pt]\rule{0pt}{2.6ex}%
&Latency ($\mu$s) 
    &9,803 &2,579 &779 &110 
    &19,541 &4,416 &836 &116 
    &47,157 &6,955 &1,104 &119 \\
&Stage 0 ($\mu$s) 
    &2,267 &- &- &- 
    &8,354 &- &- &- 
    &30,097 &- &- &- \\
&Stage 1 ($\mu$s) 
    &2,854 &520 &- &- 
    &3,747 &1,498 &- &- 
    &6,192 &2,903 &- &- \\
&Stage 2 ($\mu$s) 
    &2,247 &784 &130 &- 
    &3,015 &1,461 &137 &- 
    &4,420 &2,224 &150 &- \\
&BSA ($\mu$s) 
    &235 &200 &37 &31 
    &277 &177 &34 &31 
    &326 &189 &85 &30 \\
&Offload ($\mu$s) 
    &2,039 &869 &503 &- 
    &3,901 &1,110 &569 &- 
    &5,857 &1,533 &786 &- \\
&Extra ($\mu$s) 
    &161 &206 &110 &79 
    &247 &170 &96 &89
    &265 &106 &83 &89 \\
\cdashline{2-14}[0.5pt/1pt]\rule{0pt}{2.6ex}%
&Mask Hit Ratio (\%) 
    &71.67 &85.12 &98.75 &- 
    &52.66 &74.74 &98.42 &- 
    &28.91 &56.88 &98.38 &- \\
&SA Hit Ratio (\%) 
    &58.92 &69.25 &88.61 &99.8 
    &54.45 &68.05 &89.76 &99.8 
    &51.38 &67.73 &88.97 &99.8 \\
\bottomrule
\end{tabular}
}

\caption{
\textbf{Decoding Attention Latency of InfiniteHiP with Offloading.} When \textit{Cached stages} is \textit{None}, all pruning stages from stage 1 through 3 are re-computed, and if it is \textit{All}, then all pruning stages are skipped and only the BSA step is performed. In \textit{S1}, the first stage is skipped, and in \textit{S1\&2}, the first two stages are skipped. \textit{Offload} indicates the latency overhead of offloading and the cache management mechanism. The latencies are measured with a single RTX 4090 on PCIe 4.0 x8. The model used is AWQ Llama3.1 with FP8 KV cache.\\
(*) FA2 does not support KV cache offloading and thus cannot run decoding with a context window exceeding 128K tokens using a single RTX 4090. We estimate FA2 results by layer-wise simulation with the same model architecture.
}
\label{tab:latency_offload}
\end{table*}