\begin{table}[h]
\centering
\caption{\textbf{End-to-End Decoding Throughput (token/sec) on RTX4090 24GB.} We use AWQ Llama 3.1 8B with FP8 KV cache data type. We measured the latency of a one batch size with a passkey example. Estimated latencies are measured with estimated attention latency considering previous trends.}
\label{tab:appendix_e2e_4090}
\vspace{1em}
\begin{tabular}{lrrrrrrrrrr}\toprule
T (k) &64 &96 &128 &192 &256 &384 &512 &768 &1024 \\\midrule
SRT &88.8 &74.3 &63.2 &49.4 &- &- &- &- &- \\
SRT (Estimated) &88.8 &73.8 &63.2 &49.0 &40.1 &29.3 &23.1 &16.3 &12.5 \\
\ours 3K-Fast &113.3 &112.5 &112.0 &110.6 &- &- &- &- &- \\
\ours 3K-Fast (Estimated) &113.3 &112.5 &112.0 &110.6 &109.6 &107.3 &105.0 &100.8 &97.0 \\
\ours 3K-Fast (Offload) &64.5 &59.6 &55.9 &51.1 &46.6 &39.9 &31.8 &21.6 &17.3 \\
\ours 3K-Flash (Offload) &66.0 &62.7 &60.3 &58.2 &56.6 &53.5 &49.5 &44.0 &40.1 \\
\bottomrule
\end{tabular}
\end{table}