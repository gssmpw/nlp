\begin{figure*}[t]
\vspace{-0.4em}
\centering
\begin{subfigure}[b]{0.44\textwidth}
    \caption{\textbf{Chunk sparsity.} In the given 128K context, \textit{Left:} A histogram which plots the frequency of chunks ($y$) which contain a certain percentage ($x$) of the top 2048 keys. \textit{Right:} Percentage of chunks that contain none of the top 2048 keys by varying chunk size ($l_c$). We use the Llama 3.1 8B model and extract data from one of the attention layers.}
    \label{fig:motivation}%
    \includegraphics[height=0.68\linewidth,trim={0 .1cm 0 0}]{figures/images/mot1.pdf}%
    \includegraphics[height=0.68\linewidth,trim={0 .1cm 0 0}]{figures/images/mot2.pdf}%
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.54\textwidth}
    \caption{\textbf{Modular context pruning.} We design our context pruning module based on the observation in (a). A single pruning stage is shown above. The keys selected in the previous stage are divided into chunks, and a representative token is selected for each chunk. Each chunk's score is estimated from these representative tokens. Finally, the top $l_c/k$ chunks are selected for the next stage.}
    \label{fig:context_pruning}%
    \includegraphics[width=\linewidth,trim={0.0cm 0 0.0cm 0.0cm}]{figures/images/InfiniteHip-Pruning.pdf}%
\end{subfigure}
\vspace{-0.0em}
\caption{\textbf{Design of our Context Pruning Algorithm.}}
\vspace{-0.8em}
\label{fig:algorithms}
\end{figure*}
