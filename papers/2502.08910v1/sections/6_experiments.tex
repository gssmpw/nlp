\section{Experiments}
\label{sec:experiments}

\subsection{Experiment Setting}
\label{sec:exp_setting}

\textbf{Hyperparameters.}
We described details in~\cref{sec:appendix_hyperparameter}.

\textbf{Baselines.}
We compare the performance of \ours against the following baselines, mostly chosen for their long-context capabilities.
(1) \textbf{Truncated FA2}: The input context is truncated in the middle to fit in each model's pre-trained limit, and we perform dense attention with FlashAttention2 (FA2)~\citep{dao_flashattention_2022}.
(2) \textbf{DynamicNTK}~\citep{bloc97_ntk-aware_2023} and (3) \textbf{Self-Extend}~\citep{jin_llm_2024} adjust the RoPE for OOL generalization. We perform dense attention with FA2 without truncating the input context for these baselines.
Both (4) \textbf{LM-Infinite}~\citep{han_lm-infinite_2024} and (5) \textbf{StreamingLLM}~\citep{xiao_efficient_2024} use a combination of sink and streaming tokens while also adjusting the RoPE for OOL generalization.
(6) \textbf{H2O}~\citep{zhang_h_2o_2023} is a KV cache eviction strategy which retains the top-$k$ KV tokens at each decoding step. 
(7) \textbf{InfLLM}~\citep{xiao_infllm_2024} selects a set of representative tokens for each chunk of the context, and uses them for top-$k$ context selection.
%(8) \textbf{Double Sparse Attention}~\citep{yang_post-training_2024} estimates the top-$k$ tokens by sampling few channels of the key vectors.
(8) \textbf{HiP Attention}~\citep{lee_training-free_2024} uses a hierarchical top-$k$ token selection algorithm based on attention locality.

\textbf{Benchmarks.}
We evaluate the performance of \ours on mainstream long-context benchmarks. 
(1) LongBench~\citep{bai_longbench_2023}, whose sequence length averages at around 32K tokens, 
and (2) $\infty$Bench~\citep{zhang_inftybench_2024} with a sequence length of over 100K tokens. 
Both benchmarks feature a diverse range of tasks, such as long document QA, summarization, multi-shot learning, and information retrieval.
We apply our method to the instruction-tuned Llama 3 8B model~\citep{grattafiori_llama_2024} and the instruction-tuned Mistral 0.2 7B model~\citep{jiang_mistral_2023}. As our framework is training-free, applying our method to these models incurs zero extra cost.

\input{figures/fig_sglang}
\subsection{Results}
\textbf{LongBench.}
In \Cref{tab:longbench}, our method achieves about 7.17\%p better relative score using Llama 3 and 3.19\%p better using Mistral 0.2 compared to the best-performing baseline InfLLM.
What makes this significant is that our method processes 4$\times$ fewer key tokens through sparse attention in both models compared to InfLLM, leading to better decoding latency as shown in \cref{tab:latency}.

\textbf{$\infty$Bench.}
We show our results on $\infty$Bench in \Cref{tab:infbench}. The \textit{3K-fast and 3K-flash} window option of ours uses the same setting as \textit{3K} except using a longer mask refreshing interval as detailed in \Cref{sec:exp_setting}.
Our method achieves 9.99\%p better relative score using Llama 3 and 4.32\%p better using Mistral 0.2 compared to InfLLM. The performance gain is larger than in LongBench, which has a fourfold shorter context. This suggests that our method is able to better utilize longer contexts than the baselines.

To further demonstrate our method's superior OOL generalization ability, we compare $\infty$Bench's En.MC score in various context lengths with Llama 3.1 8B in \cref{fig:infbench_llama3.1}.
While \ours keeps gaining performance as the context length gets longer, baselines with no OOL generalization capability degrade significantly beyond the pretrained context length (128K).
In \cref{fig:infbench_gemma_exaone}, we experiment with other short-context LLMs: Exaone 3 (4K)~\citep{research_exaone3_2024}, Exaone 3.5 (32K)~\citep{research_exaone_2024} and Gemma2 (8K)~\citep{team_gemma_2024}.
We observe the most performance gain in an extended context with these short-context models. For instance, with Gemma2, we gain an impressive +24.45\%p in En.MC and +22.03\%p in En.QA compared to FA2.

\subsection{Analysis}
In this section, we analyze the latency and the effect of each of the components of our method.


\textbf{Latency.}
We analyze the latency of our method on a 1-million-token context and compare it against baselines with settings that yield similar benchmark scores. In \cref{tab:latency}, we measure the latencies of attention methods.
%InfLLM uses a 12K context window, HiP uses a 1K window, and ours uses the 3K window setting.
During a 1M token prefill, our method is 20.29$\times$ faster than FlashAttention2 (FA2), 6\% faster than InfLLM, and achieves similar latency with the baseline HiP.
During decoding with a 1M token context, our method significantly outperforms FA2 by 19.85$\times$, InfLLM by 4.98$\times$, and HiP by 92\%.
With context extension (dynamic RoPE) enabled, our method slows down about 1.6$\times$ in prefill and 5\% in decoding due to overheads incurred by additional memory reads of precomputed cos and sin vectors.
Therefore, our method is 50\% slower than InfLLM in context extension-enabled prefill, but it is significantly faster in decoding because decoding is memory-bound:
Our method with a 3K token context window reads fewer context tokens than InfLLM with a 12K token context window.

\textbf{Latency with KV Offloading.} In \cref{tab:latency_offload}, we measure the decoding latency with KV cache offloading enabled on a Passkey retrieval task sample.
We keep FA2 in the table for reference, even though FA2 with UVM offloading is 472$\times$ slower than the baseline HiP.
Among the baseline methods, only InfLLM achieves KV cache offloading in a practical way.
In 256K context decoding, we outperform InfLLM by 3.64$\times$.
With KV cache offloading, the attention mechanism is extremely memory-bound, because accessing the CPU memory over PCIe is 31.5$\times$ more expensive in terms of latency than accessing VRAM.
InfLLM chooses not to access the CPU memory while executing its attention kernel, so it has to sacrifice the precision of its top-k estimation algorithm. This makes larger block and context window sizes necessary to maintain the model's performance on downstream tasks.
In contrast, we choose to access the CPU memory during attention kernel execution like baseline HiP.
This allows more flexibility for the algorithm design, performing better in downstream NLU tasks.
Moreover, our UVM implementation makes the KV cache offloaded attention mechanism a graph-capturable operation, which allows us to avoid CPU overheads, unlike InfLLM.
In contrast to the offloading framework proposed by \citet{lee_training-free_2024}, we cache the sparse attention mask separately for each pruning stage. 
This enables us to reduce the frequency of calling the costly initial pruning stage, which scales linearly.

\textbf{Throughput.} In \cref{fig:sglang_decoding}, we present the decoding throughput of our method using RTX 4090 (24GB) and L40S (48GB) GPUs. On the 4090, our method achieves a throughput of 3.20$\times$ higher at a 1M context length compared to the estimated decoding throughput of SRT (SGlang Runtime with FlashInfer). Similarly, on the L40S, our method surpasses SRT by 7.25$\times$ at a 3M context length.
Due to hardware limitations, we estimated the decoding performance since a 1M and 3M context requires approximately 64GB and 192GB of KV cache, respectively, which exceeds the memory capacities of 24GB and 48GB GPUs.
We further demonstrate that adjusting the mask refreshing interval significantly enhances decoding throughput without substantially affecting performance. The \textit{Flash} configuration improves decoding throughput by approximately 3.14$\times$ in a 3M context compared to the \textit{Fast} configuration.

\input{figures/fig_topk_recall}
\textbf{Accuracy of top-$k$ estimation.}
In \cref{fig:topk_recall}, we demonstrate our method has better coverage of important tokens, which means higher recall of attention probabilities of selected key tokens. 
Our method performs 1.57\%p better than InfLLM and 4.72\%p better than baseline HiP.
The better recall indicates our method follows pretrained attention patterns more closely than the baselines. 

%\input{tables/tab_stage_ablation}
\textbf{Ablation on Depth of Stage Modules.}
In \Cref{tab:stage_ablation}, we perform an ablation study on a number of stages ($N$) that are used in ours. The latency-performance optimal pruning module combination for each setting is found empirically.

\input{tables/tab_rope_ablation}
\textbf{Ablation on RoPE interpolation strategies.}
In \cref{tab:rope_ablation}, we perform an ablation study on the dynamic RoPE extrapolation strategy in masking and sparse attention.
We choose the best-performing RT/ST combination for our method.