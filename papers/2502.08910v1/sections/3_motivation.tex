\section{Motivations and Observations}
\label{motivation}

\textbf{Chunk sparsity of the attention mechanism.}
To devise an algorithm that estimates the locations of the top-$k$ key tokens for block sparse attention, we first analyze the characteristics of the attention score distribution.
We observe distinct patterns in the distribution of top-$k$ tokens within a typical LLM attention context.

% TODO:: Improve clarity

\Cref{fig:motivation} suggests that the top-$k$ tokens are concentrated in a small number of context chunks. As shown in the left chart, fewer than 2\% of the chunks contain more than 12.5\% of the top-2K tokens in a 128K-token context. Furthermore, the right chart tells us that around 75\% of the 64-token context chunks do not contain any top-2K tokens at all.
%These observations suggest that we can effectively utilize the top-$k$ tokens by using the few context chunks containing them.
These observations suggest that selecting the few context chunks containing top-$k$ tokens can act as a good approximation for selecting the individual top-$k$ tokens. 
To this end, we devise an efficient algorithm that divides the context into fixed-size chunks and filters out irrelevant chunks based on their estimated maximum attention scores.
