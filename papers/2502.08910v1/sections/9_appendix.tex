\section{Complete Description of Algorithms}
\label{sec:algorithm}
\addtocontents{toc}{Complete Description of Algorithms}

\subsection{Context Pruning}
We describe our multi-stage context pruning algorithm in \Cref{alg:mask_selection}, which uses the pruning stage described in \Cref{alg:pruning_stage}.
%TODO: add an explanation for ApplyRoPE()

\paragraph{Multi-stage context pruning.} 
Our pruning algorithm generates a sparse binary mask $\bm{M}$ of size $T_q/b_q \times T_{kv}$ for each attention layer, where $T_q$ is the length of the queries, $b_q$ is the size of each query block, and $T_{kv}$ is the length of the keys. This sparse binary mask can be more efficiently represented in memory with a set of arrays of indices $\left\{ \mathcal{I}_m \right\}_{m=1}^{T_q/b_q}$, where $\mathcal{I}_m$ contains every integer $j$ such that $M_{m,j} \neq 0$.
\begin{algorithm}[H]
\caption{\ours Context Pruning Algorithm}\label{alg:mask_selection}
\begin{algorithmic}[1]
\INPUT Number of pruning stages $N$, Pruning stages $\mathcal{S}^{(1)}, \dots, \mathcal{S}^{(N)}$, where each stage $\mathcal{S}^{(i)} = (b_q^{(i)}, l_c^{(i)}, k^{(i)})$, Query length $T_q$, Key length $T_{kv}$, Number of sink tokens $n_\text{sink}$, Number of streaming tokens $n_\text{stream}$.
\STATE $\mathcal{I}^{(0)}_m := [n_\text{sink}, \dots, b_q^{(1)} \cdot m - n_\text{stream}]$ for $m = 1~..~T_q/b_q^{(1)}$. \COMMENT{Exclude sink and streaming tokens without breaking causality.}
\FOR{\textbf{each} pruning stage $i = 1~..~N$}
    \FOR{\textbf{each} query block $m = 1~..~T_q/b_q^{(i)}$}
        \STATE $\mathcal{I}'^{(i)}_m := \text{PruningStage}(\mathcal{S}^{(i)}, \mathcal{I}^{(i-1)}_m)$ \textbf{if} not cached.  (\Cref{alg:pruning_stage})
        \FOR{all $m'$ such that $m' = \lceil m \cdot b_q^{(i)} / b_q^{(i+1)} \rceil$}
            \STATE $\mathcal{I}^{(i)}_{m'} := \mathcal{I}'^{(i)}_m$. \COMMENT{Subdivide query blocks for the next stage.}
        \ENDFOR
    \ENDFOR
\ENDFOR
\STATE \textbf{return} resulting mask indices $\mathcal{I}^{(N)}_m$ for $m = 1~..~T_q/b_q^{(N)}$.
\end{algorithmic}
\end{algorithm}

\paragraph{Pruning stage.} Each pruning stage narrows down the selection of key tokens for a given query block.
\begin{algorithm}[H]
\caption{\ours Pruning Stage (PruningStage)}\label{alg:pruning_stage}
\begin{algorithmic}[1]
\INPUT Pruning stage $\mathcal{S} = (b_q, l_c, k)$, Previous stage's key indices $\mathcal{I}_m$ for the $m$th query block, Queries $\bm{Q}\in \mathbb{R}^{H\times T_q\times d}$, Keys $\bm{K}\in \mathbb{R}^{H\times T_{kv}\times d}$, where $H$ is the number of attention heads, $T_q$ and $T_{kv}$ are the number of query and key tokens each, and $d$ is the model dimension, Current layer index $l$.
\OUTPUT Filtered key indices $\mathcal{I}'_m$.
\STATE $n_\text{block} := T_{q} / b_q$.
\STATE $\bm{q}_{h,m} := \bm{Q}_{h, m\cdot b_q~:~(m+1)b_q-1}$ for $h = 1~..~H$. \COMMENT{Divide the queries into $n_\text{block}$ blocks for each head.}
\STATE $\tilde{\bm{q}}_{h,m} := \text{ApplyRopeQ}_{l}(\bm{q}_{h,m})$.
\STATE $n_\text{chunk} := |\mathcal{I}_m|/l_c$.
\STATE $\mathcal{C}_j := \big[\mathcal{I}_m [j\, l_c],\dots,\mathcal{I}_m [(j+1) l_c-1]\big]$ for $j=1~..~n_\text{chunk}$. \COMMENT{Divide the key indices into $n_\text{chunk}$ chunks.}
\FOR{\textbf{each} chunk $j = 1~..~n_\text{chunk}$}
    \FOR{\textbf{each} head $h = 1~..~H$}
        \STATE $r_{h,m,j} := \text{SelectRep}(\bm{q}_{h,m}, \mathcal{C}_j).$ (\Cref{alg:rep_select}) \COMMENT{Select the representative token for this chunk.}
    \ENDFOR
    \STATE $\tilde{\bm{k}}_{h,r_{h,m,j}} := \text{ApplyRopeK}_{l,2}(\bm{k}_{h,r_{h,m,j}}).$
    \STATE $s_{m,j} := \max_{h=1..H, t=1..b_q} \left[\tilde{\bm{q}}_{h,m}\right]_{t}^\top \tilde{\bm{k}}_{h,r_{h,m,j}}.$ \COMMENT{Compute the estimated chunk attention score.}
\ENDFOR
\STATE $\mathcal{T}_m := \underset{j}{\text{arg\,top\,}}\!_{k/l_c} (s_{m,j}).$ \COMMENT{Discard chunks with low estimated attention scores.}
\STATE $\mathcal{I}'_m := \bigcup_{\hat{\jmath}\in\mathcal{T}} \mathcal{C}_{\hat{\jmath}}.$
\end{algorithmic}
\end{algorithm}

\paragraph{Representative token selection.} Although largely unchanged from \citet{lee_training-free_2024}, we again present the representative token selection (SelectRep) algorithm in \Cref{alg:rep_select} for completeness. 
The SelectRep algorithm is designed to approximately estimate the the location of the top-1 key token with the highest attention score in the given key chunk, without evaluating all of the keys in the chunk. It runs in $O(\log_2 l_c)$ time, where $l_c$ is the key chunk size.
\begin{algorithm}[H]
\caption{Representative Token Selection (SelectRep) by Hierarchical Top-1 Selection~\citep{lee_training-free_2024}}\label{alg:rep_select}
\resizebox{\linewidth}{!}{
\begin{minipage}{\linewidth}
\begin{algorithmic}[1]
\INPUT Query block $\bm{q} \in \mathbb{R}^{b_q\times d}$, Indices of key chunk $\mathcal{C}\in \mathbb{N}^{l_c}$, Keys $\bm{K}\in \mathbb{R}^{T_{kv}\times d}$, Current layer index $l$.
\OUTPUT A representative token index $r \in \mathcal{C}$.
\STATE $\tilde{\bm{q}} := \text{ApplyRopeQ}_{l}(\bm{q})$.
\STATE $\bm{k} := \begin{bmatrix}\bm{K}_{\mathcal{C}_1}& \cdots & \bm{K}_{\mathcal{C}_{l_c}}\end{bmatrix}^\top \in \mathbb{R}^{l_c\times d}$. \COMMENT{Load key tokens with the given indices.}
\STATE $n_\text{iter} := \lceil \log_2(l_c) \rceil$.
\STATE $(n_\text{first}^{(1)}, n_\text{last}^{(1)}) := (1, l_c)$.
\FOR{\textbf{each} iteration $i = 1~..~n_\text{iter}$}
    \STATE $m^{(i)} := \lfloor (n_\text{first}^{(i)} + n_\text{last}^{(i)}) / 2 \rceil$.
    \STATE $\left(\mathcal{B}_1^{(i)}, \mathcal{B}_2^{(i)}\right) := \left( (n_\text{first}^{(i)}: m^{(i)} - 1), (m^{(i)} : n_\text{last}^{(i)}) \right).$
    \FOR{\textbf{each} branch index $j = 1~..~2$}
        \STATE Pick the first index $r_{j}^{(i)}$ from the range $\mathcal{B}_j^{(i)}$.
        \STATE $\tilde{\bm{k}} \leftarrow \text{ApplyRopeK}_{l,j}(\bm{k}_{r_j^{(i)}})$.
        \STATE Compute scores $\sigma_j^{(i)} := \max_{t}\left(\tilde{\bm{q}}_{t}^\top \tilde{\bm{k}}\right)$.
    \ENDFOR
    \STATE $t^{(i)} := \text{arg\,max}_j \sigma_j^{(i)}.$ \COMMENT{Pick the top-1 index.}
    \STATE $(n_\text{first}^{(i+1)} : n_\text{last}^{(i+1)}) := \mathcal{B}_{t^{(i)}}^{(i)}.$ \COMMENT{Update range.}
\ENDFOR
\STATE $r := n_\text{first}^{(n_\text{iter})}$.
\end{algorithmic}
\end{minipage}}
\end{algorithm}

The ApplyRopeQ and ApplyRopeK functions used in \Cref{alg:pruning_stage,alg:rep_select} are defined as follows.
\begin{align}
    \text{ApplyRopeQ}_l(\bm{q}) &:= \begin{cases}
        \text{ApplyRope}(\bm{q}, \bm{p}[n_\text{stream} + 1]) & \text{if $l > 3$} \\
        \text{ApplyRope}(\bm{q}, \bm{p}[\min\left\{i_\text{orig}, l_c + n_\text{stream}\right\}]) & \text{otherwise,} \\
    \end{cases} \\
    \text{ApplyRopeK}_{l,j}(\bm{k}) &:= \begin{cases}
        \text{ApplyRope}(\bm{k}, \bm{p}[j - 1]) & \text{if $l > 3$} \\
        \text{ApplyRope}(\bm{k}, \bm{p}[c_\text{orig}]) & \text{otherwise,} \\
    \end{cases}
\end{align}
where $i_\text{orig}$ denotes the original position of the given $\bm{q}$, and $c_\text{orig}$ denotes the index of the chunk that the given $\bm{k}$ comes from, $\bm{p}_i \in \mathbb{R}^{d}$ refers to the rotary positional embedding vector for the $i$th position, and the $\text{ApplyRope}(\cdot, \bm{p}_i)$ function denotes the classic application of RoPE $\bm{p}_i$ on the given vector as described in \citet{su_roformer_2023}. The condition $l > 3$ is for applying Relative RoPE instead of Chunk-indexed RoPE; See \Cref{sec:visualization_streaming} for an in-depth explanation of this choice.

Note that the initial pruning stage of \ours's context pruning algorithm runs in $O(T_q T_{kv})$ time, and all subsequent pruning stages run in $O(T_q)$ time. This makes the initial pruning stage the most expensive one as the number of tokens increases. So, asymptotically, \ours's context pruning algorithm has a higher time complexity compared to HiP~\citep{lee_training-free_2024}.
% However, since the chunk size $l_c$ is chosen to fit inside a GPU thread block, the SelectRep algorithm can be implemented with a single GPU kernel without any global synchronizations between each iteration.
However, since only two tokens per chunk are accessed and computed at most during the whole process, the SelectRep algorithm can be implemented with a single GPU kernel, without any global synchronizations between each iteration, while providing key sequence dimension parallelism like FlashDecode~\citep{dao_flash_decoding} which is not possible in HiP due to internal top-k.
This allows \ours's context pruning algorithm to run faster in practice with modern GPUs, thanks to its increased parallelism, as shown in \cref{tab:latency}.

Additionally, during decoding, the mask refresh rate of the first pruning stage $n_\text{refresh}^{(1)}$ can be set very high without a significant amount of performance degradation, as shown in \cref{tab:infbench}. This reduces the impact of the initial pruning stage's latency to the average latency of the entire token generation process.


\subsection{Decoding}
In \Cref{alg:decoding}, we show our decoding algorithm complete with our KV offloading mechanism. In \cref{fig:refresh_interval_visualization}, we visualize the stage caching mechanism in our decoding algorithm.

\begin{algorithm}[H]
\caption{\ours Decoding Algorithm}\label{alg:decoding}
\begin{algorithmic}[1]
\INPUT The model $\mathcal{M}$, number of layers $L$, number of pruning stages $N$, mask refresh interval $n_\text{refresh}^i$.
\OUTPUT Generated sequence $y$.
\STATE Initialize $y$ with an empty sequence.
\STATE $c^{(i)} \leftarrow 0$ for $i = 1~..~N$.
\WHILE{generation has not ended}
\FOR{\textbf{each} layer $l=1~..~L$}
    \FOR{\textbf{each} stage $i=1~..~N$}
        \IF{$(c^{(i)}$ mod $n_\text{refresh}^i) = 0$}
            \STATE $\mathcal{I}^{(l, i)} \leftarrow$ Run the $i$th pruning stage with $\mathcal{I}^{(l, <i)}$ and  the $l$th layer's query and keys with \cref{alg:mask_selection}.
            \STATE Obtain a list of GPU cache misses that occurred during the above process.
        \ENDIF
    \ENDFOR
    \STATE Perform block sparse attention with $\mathcal{I}^{(l, N)}$.
    \STATE Obtain a list of GPU cache misses that occurred during the above process.
    \STATE Evict selected cold tokens from the GPU cache, and replace them with the cache misses, depending on LRU policy.
\ENDFOR
\STATE Sample a new token and append it to $y$.
\STATE Increment $c^{(i)} \leftarrow (c^{(i)} + 1) \text{ mod } n_\text{refresh}^{(i)}$ for $i = 1~..~N$.
\ENDWHILE
\end{algorithmic}
\end{algorithm}

\begin{figure}[h]
\centering
\includegraphics[width=1.0\linewidth]{figures/images/refresh_interval_vis.pdf}
\caption{\textbf{Visualization of Stage Caching During Decoding.} The visualized mask refresh interval hyperparameter $n_{\text{refresh}}^{(1,2,3)}=(16,8,4)$ for simplicity.}
\label{fig:refresh_interval_visualization}
\end{figure}

\newpage
\section{Visualization of RoPE Adjustment}
\label{sec:appendix_rope}

\begin{figure}[h]
\centering
\includegraphics[width=1.0\linewidth]{figures/images/InfiniteHiP_ContextExtend.pdf}
\caption{\textbf{Visualziation of RoPE Adjustment.}}
\label{fig:appendix_rope}
\end{figure}

In \cref{fig:appendix_rope}, we visualize how we adjust RoPE in more detail. Relative-style RoPE is only used during context pruning because it depends on which branch the token takes during the hierarchical top-1 approximation process. As shown in \cref{tab:rope_ablation}, four types of RoPE indexing can be used in masking, and three kinds of RoPE indexing in block sparse attention. 

\section{Visualization of Each Pruning Stages (Modules)}
\label{sec:visualization}

\input{figures/fig_stages}

In \cref{fig:stages}, we visualize the attention mask generated by various RoPE adjustment methods. In SelfExtend-style RoPE, we extend the RoPE depending on the context length. 
Therefore, some stretching is observed from the right half of the image beyond the pretrained context length limit. 
In Chunk-indexed RoPE, we observe curved wiggly artifacts in the second and third stages, which is probably caused by the sliding windows. 
Since the chunk index position of each token is dynamically adjusted by previous stages, the sliding patterns change dynamically depending on the inputs. 
In Relative- and InfLLM-style RoPE, we observe strong vertical patterns because they rely only on the content information in the key vectors rather than the positional information.

\section{Discussion on Chunk-indexed RoPE}
\label{sec:visualization_streaming}

This section explains the importance of Chunk-indexed RoPE in addressing the challenges posed by dense attention layers in the baseline HiP model \citep{lee_training-free_2024}. Dense attention layers significantly slow down processing for long-context sequences, particularly when dealing with millions of tokens. While HiP Attention mitigates this issue by limiting experiments to shorter sequence lengths of 32K to 128K due to pretrained context length constraints, our approach must effectively handle much longer sequences, necessitating a more efficient solution.

\input{figures/fig_layer_mask_example}

\cref{fig:layer_mask_example} visualizes the attention score patterns. We observe that the earlier layers (e.g., layers up to 5) strongly exhibit dynamic sliding window-like attention, which signifies that these layers focus on relative positional key tokens. This behavior suggests that the model prioritizes positional information in the early layers to establish accurate representations. Once these representations are built, the model can efficiently process long-range information in subsequent layers by leveraging learned semantics instead of positional cues. These observations highlight the critical role of early layers in maintaining coherence while processing large token sequences.

\input{figures/fig_bsa_sw}

The sliding window patterns in the initial layers play a crucial role in constructing relative positional representations, a task which the block sparse attention struggles to replicate. Block sparse attention often results in staircase-step patterns, leading to inconsistent relative positional attention, as shown in \cref{fig:appendix_bsa_sw}. To address this limitation, we employ two key strategies. First, we increase the retention rate to cover pretrained patterns better by reducing masking jitters. Second, we carefully guide the pruning algorithm using our RoPE adjustment strategies (Chunk-indexed or SelfExtend-style). These adjustments generate sliding window-style artifacts, which leads to sliding window-like masks that effectively capture the diagonal patterns. By integrating these two methods, we minimize the reliance on dense layers while preserving essential positional information.

\input{tables/tab_streaming_mix}

To validate our configuration, we conduct an empirical ablation study. As shown in Table \cref{tab:appendix_streaming_mix}, combining Chunk-indexed RoPE and Relative-style RoPE within a single model enhances long-context performance. However, as highlighted in Table \cref{tab:rope_ablation}, using Chunk-indexed RoPE in every layer causes significant performance degradation. Thus, our default configuration strategically incorporates both Chunk-indexed and Relative styles, ensuring optimal performance while addressing the efficiency challenges of long-context processing.

% Chunk-indexed RoPE has an interesting artifact in the final pruned attention mask, and therefore, it is selectively used in the first few layers in \ours to replace the needs of dense attention in baseline HiP~\citep{lee_training-free_2024}.
% In this section, we visualize the attention pattern and discuss why using Chunk-indexed RoPE is required to replace dense attention.

% The dense layers in HiP were a significant problem in our study because just adding a few dense attention layers slows down the inference latency in millions of tokens scale. 
% In this case, the latency becomes heavily dominated by a few dense layers. 
% \citet{lee_training-free_2024} seems to not struggle with this latency dominance because they only experiment on relatively short sequence lengths of 32K-128K due to the pretrained context length limitation.

% \input{figures/fig_layer_mask_example}

% In \cref{fig:layer_mask_example}, we visualize every layer's first head's attention mask generated by InfiniteHiP without RoPE adjustment. We observe that the first few layers (2-5) have a strong tendency to select recent tokens (sliding window patterns), while most of the other layers do not.
% We hypothesize that this is because the Transformer model tries to exchange relative positional information between tokens during the first few layers to build a precise positional representation of each token in the given context. 
% Only after the token representation is well built with the first few layers, the model may be able to exchange long-range information using the built token representations based on the token's learned semantics rather than their positions.
% If this is true, then our observed attention patterns make sense because the model needs a well-constructed token relative positional representation to keep the temporal order intact while looking for wanted information from hundreds of thousands of tokens using scaled dot product attention.

% \input{figures/fig_bsa_sw}

% To sum up, we hypothesize that the first few layers have sliding window patterns to build the relative positional representation, while others do not. 
% However, sliding window patterns are hard to approximate with block sparse attention because block sparse attention will make a stair-like pattern and lose consistent relative positional attention, as illustrated in \cref{fig:appendix_bsa_sw}. 

% Therefore, by analyzing the attention patterns, we build the hypothesis that the need for dense layers in HiP is due to relative positional attention, a.k.a. the sliding window pattern; the sliding window pattern is extremely hard to approximate by a narrow block mask in block sparse attention.
% We believe this could be overcome with a higher retention rate and slightly forcing the pruning algorithm to make a sliding window-like mask. 
% We observe that SelfExend and Chunk-indexed RoPE adjustment forces \ours to make sliding window-style masks. We use Chunk-indexed RoPE for this purpose because we observe SelfExtend-style RoPE struggle with significant performance drops in long-context benchmarks as shown in ~\cref{tab:rope_ablation}.

% \input{tables/tab_streaming_mix}

% We validate our configuration by conducting an empirical ablation study, because theoretically validating this hypothesis would be extremely complicated and out of the scope of this research. 
% In \cref{tab:appendix_streaming_mix}, we show that mixing Chunk-indexed RoPE and Relative-style RoPE in a single model helps improve the long context performance.
% However, as shown in \cref{tab:rope_ablation}, we observe significant performance degradation if we use Chunk-indexed RoPE in every layer.
% Therefore, we choose a design that uses both the Chunk-indexed and Relative styles in the same model as the default.

\newpage
\section{Additional Experiment Results}

\subsection{Passkey Result on Deepseek R1 Distilled Qwen2}

\input{tables/tab_deepseek_passkey}

In \cref{tab:deepseek_passkey}, we demonstrate our context extension ability on Deepseek R1 Distilled Qwen 2.5 14B~\citep{deepseekai2025deepseekr1incentivizingreasoningcapability}. Our method extends the pretrained context window of Deepseek R1 from 128K to 1M without performance degradation.

\subsection{RULER Results}

\input{tables/tab_ruler_seqlen}
\input{tables/tab_ruler_subset}

In \cref{tab:tab_ruler_seqlen,tab:tab_ruler_subset}, we benchmark the RULER benchmark with \ours and baselines in Llama 3.1 8B model. The baseline (FA2 and HiP) failed to generalize the out-of-length (OOL) situation. 5K and 3K settings are the same as the definition in \cref{sec:appendix_hyperparameter}, and 3K+5K uses the 3K setting for prefill and 5K setting for decoding. Lastly, the 16K setting uses a single pruning stage with a chunk size of 32 and uses a 128K size sliding window in the first three layers.

\subsection{InfiniteBench Results in Gemma2 and EXAONEs}

\input{tables/tab_appendix_infbench}

In \cref{tab:appendix_infbench}, we show the performance of \ours context extension in Gemma2~\citep{team_gemma_2024} and EXAONE~\citep{research_exaone3_2024}. This table is the raw data of \cref{fig:infbench_gemma_exaone}.

\subsection{Detailed Result of SGlang End-to-end Decoding Throughput}

\input{tables/tab_appendix_e2e_4090}
\input{tables/tab_appendix_e2e_l40s}

In \cref{tab:appendix_e2e_4090} and \cref{tab:appendix_e2e_l40s}, we demonstrate the decoding throughput on each system: RTX 4090 24GB and L40S 48GB. This is raw data of \cref{fig:sglang_decoding}. We test only single-batch scenarios because we expect a single sequence to be larger than GPU VRAM. We chose RTX4090 because it is the best consumer-grade GPU and is easily accessible to local LLM end-users; therefore, it will represent real-world decoding throughput well. We chose L40S because it is the best cost-performance effective GPU available in Amazon Web Services (AWS) in 2025 to simulate practical serving scenarios. 

For the L40S 48GB system, we used the AWS \texttt{g6e.48xlarge} node. The specification of the RTX 4090 24GB system is as follows:

\hspace{1in}
\begin{tabular}{p{1in} p{3in}}
CPU & AMD Ryzen 7950X, 16 Core, 32 Thread\\
RAM & 128GB, DDR5 5600 Mhz\\
GPU & Nvidia RTX 4090, VRAM 24GB\\
PCIe & Gen 4.0 x8\\
OS & Ubuntu 22.04.4 LTS \\
GPU Driver & 535.171.04\\
\end{tabular}

\newpage
\section{Hyperparameters}
\label{sec:appendix_hyperparameter}

We use the following default setting across our experiments unless stated otherwise:

\hspace{1.0in}
\begin{tabular}{p{1in} p{2.5in} p{1in}}
$n_\text{sink}$ & Number of sink tokens & 256 \\
$n_\text{stream}$ & Number of streaming tokens & 1024 \\
$N$ & Number of pruning stages & 3 \\
$b_q^{(1, 2, 3)}$ & Query block size (Stage 1, 2, 3) & 64 \\
$l_c^{(1, 2, 3)}$ & Chunk size (Stage 1, 2, 3) & 256, 32, 8 \\
$k^{(1, 2)}$ & Tokens to keep (Stage 1, 2) & 32K, 8K \\
$k^{(3)}$ & Tokens to keep (Stage 3) & {\textit{(see below)}} \\
$n_\text{refresh}^{(1, 2, 3)}$ & Mask refresh interval (Stage 1, 2, 3) & 16, 8, 4 \\
\end{tabular}

We set $k^{(3)}$ = 2048 (4096 for $l\leq 3$) for the default 3K window preset and $k^{(3)}$ = 4096 for the 5K window preset.
For the `fast' and `flash' settings used for the specified rows in \Cref{tab:infbench,tab:latency_offload}, we use $(n_\text{refresh}^{(1)}, n_\text{refresh}^{(2)}, n_\text{refresh}^{(3)})$ = (32, 16, 8) (fast) and (96, 24, 8) (flash) each, with all other hyperparameters unchanged from the default setting.

We use the following 5K setting across our experiment unless stated otherwise. The unmentioned hyperparameters are the same as with a default setting:

\hspace{1.0in}
\begin{tabular}{p{1in} p{2.5in} p{1in}}
$l_c^{(1, 2, 3)}$ & chunk size (stage 1, 2, 3) & 64, 32, 16 \\
$k^{(1, 2)}$ & tokens to keep (stage 1, 2, 3) & 32K, 16K, 4K \\
\end{tabular}

\newpage
\section{Remaining Challenges And Future Directions}

While our novel framework enhances the speed and memory efficiency of Transformer inference, several challenges yet remain in long-context processing. 

First, the issues related to InfiniteHiP are as follows:
\begin{itemize}
\item The combination of pruning modules should be studied more in future research. In this study, we focus on introducing a novel sparse attention framework based on a novel modular hierarchical pruned attention mechanism. However, we discovered numerous module design choices during our research. For example, increasing block sizes can reduce latency in masking and increase the retention rates. However, this comes at a cost of performance loss in NLU tasks (e.g., LongBench and InfiniteBench) that require more fine-grained masking. Conservely, larger block sizes can enhance local context retention (e.g., passkey and UUID, which are used in synthetic tasks). These trade-offs highlight the potential for future research into task-dependent module configurations.
\end{itemize}

Secondly, the issues related to general challenges in serving long-context language models are as follows:
\begin{itemize}

\item Significant bottlenecks in the prefill stage. Even after replacing the quadratic attention mechanism with an near-linear alternative like \ours, serving over 1M tokens still takes more than 10 minutes in many consumer grade hardwares. While this is significantly faster than Flash Attention 2, it remains impractical for end-users—after all, who would use ChatGPT if it took over 10 minutes just to generate the first token?
Thus, reducing or eliminating TTFT (time to first token) and prefilling will be critical for future serving systems.
We believe strategies such as lazy initialization and speculative inference—similar to prior work~\citep{fu2024lazyllmdynamictokenpruning, lee2023sttabt} will be essential. 
Moreover, \ours is well-suited for both attention speculation and main forward computation, as it can approximate attention patterns akin to~\citet{lee2024sea}.

Despite achieving linear complexity, current Transformer architectures still result in long wait times for users with long-context prompts. While some argue that better hardware and distributed inference will resolve this issue, we see these approaches as neither scalable nor future-proof. Instead, we aim to enhance \ours to efficiently handle extremely long contexts while maintaining limited computational costs and achieving significant speedups with practical latencies.

\item The linear growth of memory. Although we use KV cache offloading in \ours to save GPU memory, in practice we are still limited to CPU memory, which is around 2TB (512GB per GPU; AWS provides around 2TB CPU memory for 8 GPU machines). At this point, we have several options: KV quantization~\citep{hooper2024kvquant10millioncontext}, KV eviction~\citep{li2024snapkv, willette2024cascade}, KV compression~\citep{deepseekv2mla}. However, we believe that linear memory complexity is necessary to achieve superior AI models because it enables ability to retain all previously processed information. Therefore, it is crucial to further improve KV cache memory efficiency with quantization and compression. In this regard, our KV cache offloading framework will provide a practical foundation for efficiently managing large working sets.

\end{itemize}
