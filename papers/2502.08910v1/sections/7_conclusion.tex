\section{Conclusion}
\label{conclusion}
In this paper, we introduced \textit{\ours}, a training-free LLM inference framework for efficient long context inference that supports out-of-length generalization and dynamic KV cache offloading.
\ours effectively addresses the three major challenges that arise in long context LLM inference:
(1) Efficient inference with long contexts,
(2) Out-of-length generalization,
(3) GPU memory conservation through KV cache offloading without `forgetting'.
The experiments on LongBench and $\infty$Bench, and the latency benchmarks demonstrate our method's superior performance and practicality over previous state-of-the-art methods.

\section*{Impact Statement}

We believe our method can significantly enhance energy efficiency and reduce inference latency. Since our approach focuses solely on accelerating the existing Transformer model without altering its trained behavior, we do not expect any notable social impact concerns. Additionally, our method demonstrates strong results in performance recovery, indicating that it can maintain performance levels comparable to the original Transformer while achieving faster processing. We anticipate that this method will offer substantial benefits for production use in the future.