\section{Introduction}
\label{introduction}

\input{figures/fig_concept}
\input{figures/fig_pruning}

In modern Transformer-based generative large language models (LLMs), extending the context length is essential for improving comprehension and coherence in long-context, multi-modal, and retrieval-augmented language generation. However, achieving this poses significant challenges, primarily due to the attention mechanism~\citep{vaswani_attention_2023}, a fundamental component of these models. The attention mechanism computes relationships between each input token and all preceding tokens, causing computational and memory costs to scale quadratically as the input sequence length increases. Another problem arising from the attention mechanism is the key-value (KV) cache. During generation, previously computed attention keys and values are cached on GPU memory for reuse. However, the KV cache size scales linearly with context length, creating a challenge for long context inference.

Various methods have been proposed to reduce the high costs of the attention mechanism. 
FlashAttention (FA2)~\citep{dao_flashattention_2022} significantly reduces memory consumption and bandwidth utilization by avoiding writing the entire attention score matrix to global GPU memory. However, it does not reduce the arithmetic computation cost. Other approaches~\citep{xiao_efficient_2024, lee_training-free_2024} selectively attend to a fixed number of key tokens, either statically or dynamically, during attention inference.

Many efforts have also been made to mitigate the memory burden of the KV cache. KV cache eviction methods selectively `forget' past contexts to conserve GPU memory~\citep{zhang_h_2o_2023, oren_transformers_2024}. However, these methods permanently erase past contexts, which may be needed again later. HiP attention~\citep{lee_training-free_2024} offloads infrequently accessed `cold' tokens to larger and cheaper host memory, dynamically fetching them back to GPU during generation only when needed while keeping only frequently accessed `hot' tokens on the GPU.

Despite these optimizations, another problem with context extension still remains: pre-trained LLMs cannot handle inputs longer than their trained context length. Since the attention mechanism is permutation invariant, they utilize positional embedding methods such as Rotary Positional Embeddings (RoPE)~\cite{su_roformer_2023} to model the temporal order of tokens. However, as LLMs are typically pre-trained on sequences truncated to a fixed length, they fail to adapt to unseen positions when prompted with longer contexts.

One option for overcoming this problem is long context fine-tuning~\citep{roziere_code_2024}, i.e., fine-tuning the model on a set of longer inputs. However, fine-tuning, especially on long sequences, requires exorbitant training costs and high-quality training data. Thus, \textit{out-of-length} (OOL) generalization, i.e., the capability for pre-trained models to perform well beyond their pre-trained limits without training, becomes increasingly important. Self-Extend~\citep{jin_llm_2024} proposes a training-free way of scaling the RoPE embeddings beyond the pre-trained limit.

In this paper, we propose \ours, a long-context LLM framework that combines the strengths of all the above methods. To alleviate the computational burden of attention, \ours proposes a novel modular sparse attention scheme that minimizes computation for less important contexts. For optimizing KV cache offloading, \ours enhances HiP attention~\citep{lee_training-free_2024}'s offloading strategy with a sophisticated LRU-based cache policy. Finally, \ours achieves OOL generalization by carefully applying various RoPE adjustment strategies within different components of LLMs according to their internal attention patterns. By providing a unified solution to all the aforementioned problems as a whole, \ours demonstrates strong practicality and is well suited for real-world deployment.

What sets \ours apart is its innovative use of pruning modules, as illustrated in \Cref{fig:concept}. These modules employ a novel modular hierarchical pruning algorithm to selectively discard less important input tokens. The algorithm leverages common patterns observed in attention matrices of popular LLMs -- namely, their sparsity and the spatial locality of nonzero entries within a sequence -- to prune irrelevant tokens effectively. Each pruning module partitions the input sequence into chunks of fixed length $b_k$, and efficiently identifies the approximate top-1 token with the highest attention score within each chunk in parallel. Only the top-$K$ most significant chunks (where $K$ is constant) are passed to the next module, while the rest are discarded. By stacking multiple pruning modules, \ours iteratively refines a block sparse attention mask. 

While our work is based upon HiP~\citep{lee_training-free_2024}, it introduces several key improvements. First, our hierarchical pruning modules achieve higher accuracy compared to HiP's heuristic-based hierarchical pruning. Second, the pruning algorithm within each module is significantly faster due to its enhanced parallelizability. Lastly, its modular design enables fine-grained control over pruning-stage caches, leading to much faster decoding than HiP. 

\ours enables extremely long-context inference with pre-trained LLMs, surpassing their original context length limits without quality degradation while overcoming GPU memory limitations with efficient KV cache offloading. As a training-free solution, \ours can be used as a drop-in replacement for any pretrained Transformer-based LLM, providing faster inference and extending usable context length at both the model and hardware levels.

Our contributions can be summarized as follows:
\vspace{-1.5em}
\begin{itemize}[itemsep=0.5mm, parsep=2pt, leftmargin=12pt]
\item We propose a modular, highly parallelizable training-free hierarchically pruned attention mechanism that enables out-of-length generalization while significantly speeding up LLM inference on long contexts.
\item We demonstrate that our method does not degrade the LLM's long-context language understanding, reasoning, and text generation capabilities compared to other SoTA efficient long-context inference methods.
\item We efficiently implement \ours on the SGLang LLM serving framework, achieving a 7.24$\times$ speedup in end-to-end decoding on a 3M token context while using only 3.34\% of the VRAM required by FA2, and design an efficient KV cache offloading algorithm that utilizes modular pruning algorithm, making it practical for real-world scenarios.
\end{itemize}
