\section{Related Works}
\label{related_works}

Previous studies have proposed dynamic token selection for efficient LLM inference for long contexts. MInference~\citep{jiang_minference_2024} classifies attention heads into two types to estimate the sparse attention pattern, which is used to drop less important tokens before the dot product.
%: vertical-slash heads and block-sparse heads. Vertical-slash heads use the last few queries to estimate the attention pattern for the rest of the queries, whereas block-sparse heads use mean-pooled queries and keys to estimate a block-sparse attention pattern. 
While this method considerably speeds up the prefill stage, it cannot be applied in the decoding stage, which takes up most of the inference time.
%
HiP Attention~\citep{lee_training-free_2024} estimates the top-k context blocks with the highest attention scores in a hierarchical and iterative manner, significantly speeding up both prefill and decoding in long contexts. However, the iterative algorithm involves many global thread synchronizations, which hinders parallelism.
Quest~\citep{tang_quest_2024} divides the context into fixed-size pages and estimates the maximum attention score by using cached element-wise min and max vectors.
%TokenSelect~\citep{xiao_infllm_2024} selects tokens by performing a global top-$k$ operation, and updates the selected token mask whenever the cosine distance of the query exceeds a threshold during decoding. For computational efficiency, it ignores the difference between attention heads.
InfLLM~\citep{xiao_infllm_2024} divides the context sequence into blocks and selects representative tokens in each block. For each new query, the top-k blocks whose representative tokens give the highest attention scores are selected.
In contrast to our \ours, the representative tokens of each block are prechosen and do not change with the current query.
Both HiP Attention and InfLLM enables KV cache offloading, which makes long context inference context possible within a single GPU.
