\section{Related Work}
\subsection{VLMs for Remote Sensing}
VLMs are capable of converting images into natural language descriptions and parsing the relationships between objects, demonstrating remarkable performance in tasks such as text-image retrieval, image captioning, and visual question answering. Recently, models like RemoteClip \citep{liu2024remoteclip} have been applied to the field of remote sensing images, primarily focusing on cross-modal retrieval and zero-shot classification. However, these models have not addressed tasks such as image description generation and visual grounding. The RSGPT model has achieved text description and visual question answering for remote sensing images, but it has not expanded to tasks such as classification and detection. The GeoChat model has advanced multi-task conversational processing of high-resolution remote sensing imagery, including scene classification, visual question answering, multi-turn dialogue, visual grounding, and reference object detection. However, these models, including GeoChat, predominantly rely on optical remote sensing training data, leading to suboptimal performance in SAR-specific interpretation tasks. EarthGPT \citep{zhang2024earthgpt} has extended the application of multimodal large language models to the remote sensing field through instruction tuning, but its performance in SAR image multi-task processing still needs improvement. Compared with natural images, the interpretation of SAR images is more challenging, which poses higher demands on the model's processing capabilities and adaptability.

\subsection{Remote Sensing Vision-Language Datasets}
% \noindent \textbf{Data Augmentation for LLMs} 
Remote sensing datasets are essential for models that interpret remote sensing imagery. Existing datasets such as UCM Captions \cite{7546397}, Sydney Captions \cite{qu2016deep}, RSICD \citep{lu2017exploring}, RSITMD \citep{yuan2022exploring}, and RSVG \citep{zhan2023rsvg} provide preliminary resources for studying the correlation between remote sensing images and text. However, these datasets are limited not only in scale but also in modality, containing only optical images without SAR data, leaving SAR interpretation capabilities largely unexplored. Although large-scale datasets like MillionAID \citep{long2021creating}, FMoW \citep{christie2018functional}, and BigEarthNet \citep{sumbul2019bigearthnet} exist, they lack text-image pairs. The RS5M  dataset \citep{zhang2023rs5m}, containing 5 million image-text pairs, is still limited to optical images. The MMRS-1M dataset \citep{zhang2024earthgpt}, which covers optical, infrared, and SAR modes, has a very low proportion of SAR image-text data. Therefore, this paper constructs the \ourmethod-2M dataset, which focuses on SAR images and contains over 2 million image-text pairs, covering tasks such as classification, detection, caption generation, VQA, and visual grounding.