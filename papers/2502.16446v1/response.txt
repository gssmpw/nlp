\section{Related Work}
Previous GAN-based models for SMILES sequence generation include SeqGAN and ORGAN. These foundational approaches were later extended with downstream networks such as **Kingma, "Improving Rare-Event Generators"**,  **Che et al., "Adversarial Training for Semi-Supervised Text Classification"** and **Meng et al., "Auto-encoder-based Adversarial Learning for Image-to-Image Translation"**, which tailored the generative process to specific application objectives. %The primary goal of these methods has been to minimize various types of divergences between real and generated distributions. However, their discriminators are typically trained from scratch without leveraging pre-trained networks, making them prone to overfitting on the training set and harder to converge, especially when working with small datasets. Additionally, the average length and distribution of input molecular sequences significantly influence training outcomes, further complicating the optimization process.

Since the introduction of GANs**Goodfellow et al., "Generative Adversarial Networks"**, advancements in architectures **Radford et al., "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks"**,  training strategies **Arjovsky et al., "Wasserstein Generative Adversarial Networks"** and objective functions **Arjovsky et al., "Toward Principled Methods for Training Generative Adversarial Networks"** have led to significant progress. Despite their success in image-based tasks, many methods have not yet been applied to GANs for sequence generation.%For example, ACGAN is a classic model in computer vision, demonstrating improved image generation quality by incorporating auxiliary label information. Pre-trained discriminators have also proven effective in GAN frameworks, enabling high-quality image generation with minimal training data. Despite their success in image-based tasks, these methods have yet to be applied to GANs for sequence generation. WGANs, which utilize the Wasserstein distance as the objective function, address critical issues like mode collapse and have been applied in ORGAN to improve generative stability. 

Molecules, due to their structured nature and extensive prior knowledge, are particularly well-suited for transfer learning. Descriptor-generation tools like RDKit**Landrum, "RDKit: Open-Source Cheminformatics"** and OpenBabel**O'Boyle et al., "Open Babel: An Open Chemical Expertise Platform for Chemistry"** allow for the extraction of rich molecular features, which can be effectively transferred to unseen tasks, datasets, and domains. In our work, we leverage these transferable molecular property representations for unsupervised model training, enabling the generation of high-quality SMILES strings even with limited training data.

GANs can amplify data to address data scarcity issue in molecule predicting task **Goodfellow et al., "Generative Adversarial Networks"**, outperforming traditional methods like Synthetic Minority Oversampling Technique (SMOTE). At present, data enhancement methods have not been used to generate molecules in GANs. For highly imbalanced datasets, we employ over-sampling to enrich minority classes.