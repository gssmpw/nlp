\documentclass[10pt, twocolumn]{article}
\usepackage[a4paper, top=1in, bottom=1in, left=0.8in, right=0.8in]{geometry}
\usepackage[utf8]{inputenc} % 设置编码
\usepackage{cite}
\usepackage[numbers]{natbib}
\usepackage[T1]{fontenc}    % 设置字体编码
\usepackage{amsmath, amssymb, amsthm} % 数学公式和符号
\usepackage{graphicx}       % 图片支持
\usepackage{hyperref}       % 超链接支持
\usepackage{geometry}       % 页面设置
\usepackage{lipsum} 		 % 用于生成占位文本
\usepackage{placeins} % 提供 \FloatBarrier
\usepackage{afterpage} % 用于更精准地控制图片输出
\usepackage{amsmath}
\usepackage{booktabs} % 用于美化表格
\usepackage{threeparttable} % 允许在表格内使用脚注
\usepackage{placeins}
\usepackage{caption}
\usepackage{booktabs}
\usepackage{threeparttable}
\usepackage{multirow}
%\geometry{a4paper, margin=1in} % 页面边距

\setlength{\parindent}{2em}  % 设置首行缩进为 2 个字符宽度
%\setlength{\parskip}{0.25em}    % 设置段间距为 0.5em
\linespread{0.95}  % 减小行间距，0.9 表示缩小
%\vspace{-0.5em}

\title{\textbf{Auxiliary Discrminator Sequence Generative Adversarial Networks (ADSeqGAN) for Few Sample Molecule Generation}}

\author{
    Haocheng Tang\textsuperscript{1}, Jing Long\textsuperscript{2}, Junmei Wang\textsuperscript{1,*} \\
    \textsuperscript{1}School of Pharmacy, University of Pittsburgh \\
    \textsuperscript{2}School of Software \& Microelectronics, Peking University
}

\date{\today} % 设置日期，可以改为具体日期或去掉自动生成日期

\begin{document}

\maketitle

\begin{abstract}
In this work, we introduce Auxiliary Discriminator Sequence Generative Adversarial Networks (ADSeqGAN), a novel approach for molecular generation in small-sample datasets. Traditional generative models often struggle with limited training data, particularly in drug discovery, where molecular datasets for specific therapeutic targets, such as nucleic acids binders and central nervous system (CNS) drugs, are scarce. ADSeqGAN addresses this challenge by integrating an auxiliary random forest classifier as an additional discriminator into the GAN framework, significantly improves molecular generation quality and class specificity.

Our method incorporates pretrained generator and Wasserstein distance to enhance training stability and diversity. We evaluate ADSeqGAN on a dataset comprising nucleic acid-targeting and protein-targeting small molecules, demonstrating its superior ability to generate nucleic acid binders compared to baseline models such as SeqGAN, ORGAN, and MolGPT. Through an oversampling strategy, ADSeqGAN also significantly improves CNS drug generation, achieving a higher yield than traditional de novo models. Critical assessments, including docking simulations and molecular property analysis, confirm that ADSeqGAN-generated molecules exhibit strong binding affinities, enhanced chemical diversity, and improved synthetic feasibility.

Overall, ADSeqGAN presents a novel framework for generative molecular design in data-scarce scenarios, offering potential applications in computational drug discovery. We have demonstrated the successful applications of ADSeqGAN in generating synthetic nucleic acid-targeting and CNS drugs in this work.


\end{abstract}

\section{Introduction}

Non-supervised molecular generation has become a cornerstone of modern computational drug discovery, offering innovative approaches for designing novel compounds with desired properties. Over time, diverse methodologies have emerged, categorized by generative objectives and molecular representations. For instance, models have been developed for molecular property optimization, probabilistic distribution learning, and site-specific design\cite{Du2024}. Molecular representations vary from simplified molecular-input line-entry system (SMILES) strings and molecular graphs to molecular fingerprints and 3D point clouds\cite{Tong2021,Xie2022,Benoit2023}. Meanwhile, neural network architectures like Word2Vec (W2V), sequence-to-sequence (Seq2Seq) models, transformers, graph convolutional networks (GCNs), graph attention networks (GATs), message-passing neural networks (MPNNs), and 3D-point networks have powered these advancements. Generative models such as recurrent neural network (RNN), generative adversarial networks (GANs), variational autoencoders (VAEs), adversarial autoencoders (AAEs), normalizing flows, and diffusion models have further diversified the toolkit for molecular design\cite{Tong2021,Pang2024,Mswahili2024}.

Among all the molecular representation, SMILES notations stand out due to their simplicity, widespread database availability, and extensive tool support. Their sequential representation makes them particularly amenable to natural language processing (NLP) techniques, which further reduce computational and storage costs. This positions SMILES-based approaches as highly advantageous for expanding compound spaces guided by molecular properties.

GANs remain a classic and versatile class of generative models, offering key advantages over VAEs and Diffusion models. By avoiding assumptions of Gaussian priors, GANs are well-suited for datasets with non-Gaussian distributions. Additionally, GANs avoid maximum likelihood estimation(MLE), which, while stabilizing optimization, can constrain generative diversity\cite{Cao2018}. Over the years, many GAN variants have been proposed to address specific challenges in sequence generation,including Sequence GAN (SeqGAN)\cite{Yu2016} and Objective Reinforced GAN (ORGAN)\cite{Guimaraes2017}. SeqGAN leverages policy gradients to optimize sequence outputs, while ORGAN incorporates task-specific rewards to guide generation through reinforcement learning (RL). On the other hand, Auxiliary Classifier GANs (ACGANs)\cite{Odena2016} incorporate class labels into the both generator and discriminator to stabilize training, while pre-trained GAN discriminators\cite{Kumari2021}, ensembles of shallow and deep classifiers, have reduced data requirements in computer vision tasks. However, such mechanisms have not yet been explored for sequence-based molecular generation tasks, leaving a gap in integrating these frameworks for SMILES-based generative models.

For SMILES-based generative models, two primary objectives must be addressed during training: (1) learning the syntactic rules of SMILES notation to ensure valid molecule generation; (2) capturing the structural and functional features of molecules within the dataset. Achieving these goals often requires extensive data and carefully tuned network parameters\cite{Lucic2017,Elton2019}. The scarcity of high-quality datasets for specific drug categories poses a significant challenge. Besides, the length of SMILES strings varies. However, many drug molecules share common features, such as small molecule binders targeting nucleic acids and proteins, which both fall under the broader category of small-molecule therapeutics. Similarly, central nervous system (CNS)-targeted drugs and other non-CNS drugs share overlapping characteristics under the drug discovery framework.

Some methods have been explored to generate new molecules on small datasets based on pre-trained models and molecular scaffolds \cite{Moret2020,Fang2023}, but the results are limited for some data sets with narrow distribution of molecular properties.

In this paper, we introduce a novel approach that integrates a pretrained random forest classifier as a auxiliary descriminator into the SeqGAN framework to improve the quality of SMILES generation. To enhance adversarial training stability, we integrate MLE generator pretraining\cite{Yu2016,Guimaraes2017,Putin2018a} and Wasserstein GANs (WGANs)\cite{Arjovsky2017,Guimaraes2017} into the architecture. To the best of knowledge, this GAN architechture has not been explored so far. Our method demonstrates superior performance in generating nucleic acid-targeting molecules on a nucleic acid and protein mixed dataset, achieving higher verified SMILES rates and yields for nucleic acid binders, compared to models trained exclusively on nucleic acid-targeting datasets.

We also address dataset imbalances by introducing targeted data augmentation strategies. For datasets with extreme biases, such as CNS drug datasets, we employ a novel strategy of over-sampling minority molecules while training on a mixed dataset. This approach significantly increases the generation rate of CNS molecules while maintaining diversity and validity.

Our contributions highlight the synergy between sequence-based GANs and auxiliary classifier techniques in molecular generation and provide a practical framework for addressing dataset imbalances in low-data regimes.

\section{Related Work}

Previous GAN-based models for SMILES sequence generation include SeqGAN and ORGAN. These foundational approaches were later extended with downstream networks such as ORGANIC\cite{Sanchez-Lengeling2017}, RANC\cite{Putin2018a} and ATNC\cite{Putin2018b}, which tailored the generative process to specific application objectives. %The primary goal of these methods has been to minimize various types of divergences between real and generated distributions. However, their discriminators are typically trained from scratch without leveraging pre-trained networks, making them prone to overfitting on the training set and harder to converge, especially when working with small datasets. Additionally, the average length and distribution of input molecular sequences significantly influence training outcomes, further complicating the optimization process.

Since the introduction of GANs\cite{Goodfellow2020}, advancements in architectures\cite{Radford2015,Odena2016,Karras2018,Karras2021,Huang2025}, training strategies\cite{Karras2017}, and objective functions\cite{Durugkar2016,Arjovsky2017,Albuquerque2019,Kumari2021} have led to significant progress. Despite their success in image-based tasks, many methods have not yet been applied to GANs for sequence generation. In this work, we integrate these advancements including ACGAN, pre-trained discriminators, and WGAN objective function into a sequence-generation GAN framework, with a particular focus on molecular sequence generation.%For example, ACGAN is a classic model in computer vision, demonstrating improved image generation quality by incorporating auxiliary label information. Pre-trained discriminators have also proven effective in GAN frameworks, enabling high-quality image generation with minimal training data. Despite their success in image-based tasks, these methods have yet to be applied to GANs for sequence generation. WGANs, which utilize the Wasserstein distance as the objective function, address critical issues like mode collapse and have been applied in ORGAN to improve generative stability. 

Molecules, due to their structured nature and extensive prior knowledge, are particularly well-suited for transfer learning. Descriptor-generation tools like RDKit\cite{rdkit} and OpenBabel\cite{O'Boyle2011} allow for the extraction of rich molecular features, which can be effectively transferred to unseen tasks, datasets, and domains. In our work, we leverage these transferable molecular property representations for unsupervised model training, enabling the generation of high-quality SMILES strings even with limited training data.

GANs can amplify data to address data scarcity issue in molecule predicting task\cite{arigye2020}, outperforming traditional methods like Synthetic Minority Oversampling Technique (SMOTE). At present, data enhancement methods have not been used to generate molecules in GANs. For highly imbalanced datasets, we employ over-sampling to enrich minority classes. 

\section{Methods}

%\lipsum[1] % 占位文本

\begin{figure*}[!htb]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/figure1.pdf}
    \caption{\small Scheme of ADSeqGAN. $Build \ Dataset$: The dataset contains 2 part, including class labels and SMILES strings. It must contains 2 or more different molecule classes. $Pretrain \ Discriminators$: Using RDKit and Open Babel to calculate molecule descriptors for every datapoints and then choose the descriptors with the strong resolution to build the classifier. To get the pretrained $D$, structural limitation is added. $Train \ ADSeqGAN$: $G$ is fed with labeled start tokens and traind by RL to generate synthetic data of different classes. $D$ with CNN is designed to ditinguish generated data from real samples, while pretrained $D$ is used to classify the samples. Reward is a combination of adversarial and auxiliary reward, and is passed back to the policy function via Monte Carlo
sampling. We add length weight and repetition penalization.}
    \label{fig:fig1}
\end{figure*}

%\lipsum[2-3] % 占位文本

Classic GANs consist of two parts: generator $G$, parameterized by $\theta$ to produce sample $Y$ and discriminator $D$, parameterized by $\phi$ to distinguish synthetic data from real ones. The generator and discriminator of original GAN models are trained in alternation, following a minimax game:
\begin{equation}
\begin{aligned}
\min_{G}\max_{D}V(D,G) &= \mathbb{E}_{Y \sim p_{\text{data}}(Y)}[\log D(Y)] \\
&\quad + \mathbb{E}_{Y \sim p_{G_{\theta}}(Y)}[\log (1-D(Y))]
\end{aligned}
\tag{1}
\end{equation}

However, these traditional GANs can not be directly applied to sequence generation tasks, so GAN+RL based SeqGAN and ORGAN were developed. Nonetheless, in our test task, these models struggled to learn enough features with limited real data. To solve these problems, we propose Auxiliary Discrminator Sequence Generative Adversarial Networks (ADSeqGANs). The process of applying ADSeqGANs to molecular generation is shown in Figure 1: Firstly, a hybrid database consisting of a small number of samples of the desirable class and samples of auxiliary classes is constructed. Next the molecule descriptors with strong classification ability are selected by logistic regression method as parameters for pre-training to get a classifier, which is then added to GAN training as auxiliary discriminators. At the same time, labels of the real data are also entered into the generator to assist in generating samples. Last, we fine-tune the structure and parameters of the network.


Overall, this new network can benefit us in three ways: First, training shallow classifiers on pre-trained features is a common way to adapt deep networks to small data sets while reducing overfitting\cite{Girshick2014,Chen2019,Liang2025}; Second, the discriminator constructed based on prior physical and chemical knowledge may be more in line with human perception\cite{Zhang2018}; Third, adding standardization based on real sample or prior knowledge can reduce mode collapse\cite{Dai2023}.
\subsection{Formulation}
In this work, we introduce a set of molecular features $\mathcal{F}$ and a set molecule classes $\mathcal{C}$, used to train classifiers as aided discriminators ${\{D_n\}}^N_{n=1}$, through corresponding classifier function ${\{C_n\}}^N_{n=1}$. During discriminator training, only adversarial discriminator is updated and the other auxiliary classifiers are frozen. $a_c$ and $b_c$ are structural restrictions based on prior knowledge and certain molecule class $c$. For each auxiliary discriminator, the optimize function is:
\begin{equation}
\begin{aligned}
&\min_{G}V(D_n,G) = \sum_{c \in \mathcal{C}}\mathbb{E}_{Y|c \sim p_{\text{data}}(Y|c)}[\log D_n(Y|c)] \\
&\text{where }D_n = a_cC_n(\mathcal{F})+b_c
\end{aligned}
\tag{2}
\end{equation}

Then the total training is to find the optimal solution of:
\begin{equation}
\begin{aligned}
&\min_{G}\max_{D}\lambda_0 V(D_{\phi},G) + \sum_{n=1}^N\lambda_nV(D_n,G) \\
&\text{where }\sum_{n=0}^N\lambda_n=1
\end{aligned}
\tag{3}
\end{equation}

In this paper, we simply use pretrained random forest as a single auxiliary discriminator. Using more pretrained discriminators is computationally and memory-intensive and does not significantly improve the model's performance.

For discrete dataset, like SMILES strings of molecules, the sampling process is undifferentiable. One successful approach is to train $G_{\theta}$ using a RL model via policy gradient\cite{Williams1992,Yu2016,Guimaraes2017}. Considering a full length sequence $Y_{1:T} = string(t_1t_2...t_T)$, representing a discrete data, $Y_{1:t}$ is an incomplete subsequence belonging to $Y_{1:T}$. One can maximize the long term reward in the policy gradient process to mimic the expectation in $eq(1)$:
\begin{equation}
\begin{aligned}
J(\theta)&=E\left[R_{T} \mid s_{0}, \theta\right] \\
&=\sum_{y_{1} \in Y} G_{\theta}\left(y_{1} \mid s_{0}\right) \cdot Q\left(s_{0}, y_{1}\right)
\end{aligned}
\tag{4}
\end{equation}

where $R_{T}$ is the reward for a complete sequence with the length of $T$ from the discriminator to generator, $s_{0}$ is a fixed initial state at time 0, $y_{1}$ is the next token at time 1, $G_{\theta}\left(y_{1} \mid s_{0}\right)$ is the policy that action $a$ will be taken as state $s_{0}$ to get token $y_{1}$, and $Q(s,a)$ is the action-value function that represents the expected reward at state $s$ of taking action $a$ and following our current policy $G_{\theta}$ to complete the rest of the sequence. According to $eq(2)$ and $eq(3)$, we can decompose $Q\left(s_{0}, y_{1}\right)$ as:
\begin{equation}
\begin{aligned}
Q\left(s|Y_{1: T-1}, a|y_{T}\right)=\lambda_0 R_{\phi,T} + \sum_{n=1}^N\lambda_nR_{n,T} \\
\end{aligned}
\tag{5}
\end{equation}

However, the above equation is only for full sequence $Y_{1: T}$. We also want to get the initial state $s_0$ and $Q$ for partial sequences. $s_0$ cannot directly be used, since the RL processes will consume more computational resources and may reduce the diversity of generated molecules. Instead, $s_x$ is set as the initial state, directly sampling $x$ tokens, as suggested by ORGAN. For the following generation steps, we perform $M$-time Monte Carlo search with the canonical policy $G_{\theta}$ to calculate $Q$ at intermediate time steps. Thus we can evaluate the final reward when the sequence is completed:
\begin{equation}
\begin{aligned}
\operatorname{MC}^{G_{\theta}}\left(Y_{1: t} ; M\right)=\left\{Y_{1: T}^{1}, \ldots, Y_{1: T}^{M}\right\} 
\end{aligned}
\tag{6}
\end{equation}

where $Y_{1: t}^{m}=Y_{1: t}$ and $Y_{t+1: T}^{m}$ is stochastically sampled via the policy $G_{\theta}$. Now $Q(s, a)$ becomes $Q(t)$:
\[
Q(t) = 
\begin{cases} 
\frac{1}{M} \sum_{m=1}^{M} (\lambda_0 R^m_{\phi,T}+\sum_{n=1}^N\lambda_nR^m_{n,T})  \\
\text{with } Y_{1:T}^{m} \in \mathrm{MC}^{G_{\theta}}\left(Y_{1:t}; M\right), \\
\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad \text{if } t < T. \\ 
\lambda_0 R_{\phi,T}+\sum_{n=1}^N\lambda_nR_{n,T}, \quad \text{ if } t = T.
\end{cases}
\tag{7}
\] 

Finally, according to SeqGAN, we can get an unbiased estimation of the gradient $J(\theta)$:

\begin{equation}
\begin{aligned}
\nabla_{\theta} J(\theta) \simeq \frac{1}{T} & \sum_{t=1, \ldots, T} \mathbb{E}_{y_{t} \sim G_{\theta}\left(y_{t} \mid Y_{1: t-1}\right)} \\ &[\left.\nabla_{\theta} \log G_{\theta}\left(y_{t} \mid Y_{1: t-1}\right) \cdot Q(t)\right] 
\end{aligned}
\tag{8}
\end{equation}
\subsection{Implementation Details}
$G_{\theta}$ is a RNN with long short-term memory (LSTM) cells for sequence generation, while $D_{\phi}$ is a convolutional neural network (CNN) for text classification tasks\cite{Kim2014}. Unlike SeqGAN and ORGAN, ADSeqGAN uses labeled start token as input, and different classes share the same RNN.

To avoid problems of GAN convergence like "perfect discriminator" and improve the stability of learning, we introduce the Wasserstein-1 distance, also known as earth mover's distance\cite{Gulrajani2017}, to $D_{\psi}$. In essence, Wasserstein distance turns the discriminator's classification task into a regression task, with the goal of reducing the distance between the real data distribution and the model distribution, which is more in line with the goals of RL.

For pretrained discriminator $D_n$, we add some structural restrictions to the classifier, in order to get more reasonable molecules. 

Other additional mechanisms to prevent mode collapse and over-fitting includes: (1) Using $0-1$ standardization in rollout policy when computing rewards. (2) Penalizing repeated sequences by dividing the reward of a non-unique sample by the number of copies. (3) Applying length weight to make sure the length of molecules generated correspond the real length distribution of samples.

The dataset of small molecules targeting nucleic acids and protein is built from ChEMBL345 \cite{Gaulton2012,Zdrazil2024} and all open structures on DrugBank\cite{Knox2023}. For the dataset of CNS and non-CNS drugs, we collect the CNS data and non-CNS data from DrugBank. We reconstruct all the SMILES strings to make them more suitable to train our models, removing the complex 3D structural information, as well as the inorganic drug, the inorganic ionic components of the drug, molecules too long, and molecules containing other than halogens and B, S, P, and canonize the sample using RDKit. All the code of tensorflow version and parameters are available online: \href{https://github.com/ClickFF/ADSeqGAN}{https://github.com/ClickFF/ADSeqGAN}. 

\section{Results}


\renewcommand{\arraystretch}{1.5} % 调整行间距

\begin{table*}[h]
    \centering
    \small
    \begin{threeparttable}
	    \caption{Evaluation of metrics on several generative algorithms}
	    \label{tab:first}
	    \begin{tabular}{l c cc c c c c c c}
	        \toprule 
	        \multirow{2}{*}{Algorithm} & \multirow{2}{*}{Dataset} & \multicolumn{2}{c}{Novelty/\%} & \multirow{2}{*}{$\text{SA}^{\Uparrow}$} & \multirow{2}{*}{$\text{QED}^{\Uparrow}$} & \multirow{2}{*}{$\text{FCD}^{\Downarrow}$} & \multirow{2}{*}{NA ratio/\%} & \multirow{2}{*}{NA yield/\%} & \multirow{2}{*}{Size} \\  
	        \cmidrule(lr){3-4}
	                 &         &\footnotesize Validity/\% &\footnotesize Unique/\% &  &  &  &  &  &  \\  
	        \midrule 
	        MLE RNN          & NA       & 17.7 & 99.3 & 0.53 & 0.58 & 0.90 & 17.6 & 3.1 &6400\\ 
	                         & NA+Pro   & 19.3 & 97.1 & 0.44 & 0.53 & 0.81 & 3.8  & 0.7 &6400\\ 
	        Native RL        & NA       & 68.2 & 99.2 & 0.30 & 0.51 & 0.74 & 29.8 & 20.1&6400\\ 
	                         & NA+Pro   & 51.7 & 99.4 & 0.02 & 0.35 & 0.72 & 1.2  & 0.6 &6400\\ 
	        SeqGan           & NA       & 0    & --   & --   & --   & --   & --   & --  &6400\\ 
	        ORGAN            & NA       & 1.3  & 98.8 & 0.01 & 0.44 & 0.64 & --   & --  &6400\\ 
	                         & NA+Pro   & 0    & --   & --   & --   & --   & --   & --  &6400\\   
	        MolGPT           & NA       & 0.4  & 22.0 & 0.00 & 0.014& 0.49 & --   & --  &10000\\ \hline	        
	        $\text{CLM}^*$   & NA       &\multicolumn{2}{c}{8.8} & 0.21 & 0.61 & 0.81 & 31.7 & 2.9 &5000\\  
	        $\text{MolGen}^*$& NA       &\multicolumn{2}{c}{1.6} & 0.17 & 0.21 & 0.86 & 38.8 & 0.6 &924800\\ \hline
	        ADSeqGAN         & NA+Pro   & 94.5 & 71.6 & 0.78 & 0.40 & 0.66 & 82.7 & 56.0&6400\\
	        \bottomrule
	    \end{tabular}
	    \begin{tablenotes}	
			\item[*] Using pretrained models.
			\item[] \hspace{-1em} $\Uparrow$ means the larger the better, while $\Downarrow$ means the smaller the better.
	    \end{tablenotes}
	\end{threeparttable}
\end{table*}

\renewcommand{\arraystretch}{1} % 恢复默认行间距

Here we conduct experiments to test ADSeqGAN in two representative scenarios: a moderately biased dataset consists of 4894 nucleic acid binders (NA) and 8191 protein targeting molecules (Pro) with high bioactivity, and an extremely biased dataset consisting of 548 CNS drugs and 7728 other small molecule drugs. Our objective is to prove that our model can generate functional drug molecules with only a small number of target samples while promoting synthesizability and drug-likeness, diversity and similar molecule length distribution. To estimate the quality of generated samples, score functions designed for synthesizability (SA)\cite{Ertl2009},quantitative estimate of drug-likeness (QED)\cite{Lipinski2001,Bickerton2012,Ivanović2020} and Frechet ChemNet Distance (FCD)\cite{Preuer2018} are used, and we use fingerprint ECFP4\cite{Rogers2010} to compute Tanimoto similarity\cite{Nikolova2003} for diversity evaluation.

During training, every generator model was pretrained for 250 epochs based on MLE, and the discrminator was pre-trained for 10 steps. Unless otherwise stated, all GAN models use Wasserstein distance. Unless specified,$\lambda$ is set to 0.2. MLE-based RNN, SeqGAN, RNN-pretrained RL, chemical language model (CLM)\cite{Moret2020}, MolGPT\cite{Bagal2022}, MolGen\cite{Fang2023} are selected for contrast experiments. All GAN-related models and RL-based models are set to be pretrained for 250 epochs of MLE-based $G_{\theta}$, and then 50 epochs for downstream training. Meanwhile, in pretraining steps of GANs, $D_{\psi}$ is set to be pretrained for 10 steps. For other models, the training parameters are based on original papers. Notably, for all drug molecules in GAN and RL relative models, grammar restrictions are loose and the later generated samples are dealt with openbabel's gen2d function to reconstruct strictly valid molecules, since lightly invalid SMILES are beneficial rather than detrimental to chemical language models\cite{Skinnider2024}.

\subsection{Nucleic Acid Binder Generation}


Nucleic acid-targeting drugs remain relatively scarce compared to their protein-targeting counterparts, despite the increasing recognition of nucleic acids as crucial therapeutic targets. In contrast to nucleic acid drugs, nucleic acid binders, though still not so many, are much more diverse and have been widely studied for their interactions with DNA and RNA. These binders play essential roles in regulating gene function and dying nucleic acids. Meanwhile, small molecules and protein-based drugs with high bioactivity have been extensively developed for protein targets, demonstrating well-characterized pharmacological properties.

Given the limited number of nucleic acid binders and the established chemical diversity of small molecules targeting proteins, it is of significant interest to explore a molecular generation model that integrates the chemical characteristics of both. The primary goal is to construct a generative model which is capable of learning the distinct yet complementary features of nucleic acid binders and bioactive small molecules that target proteins. By capturing the underlying chemical space shared between these two classes, such a model could facilitate the discovery of novel nucleic acid-targeting molecules with enhanced specificity and efficacy.

\subsubsection*{Build Pretrained Discriminators}

To distinguish nucleic acid binders from protein-targeting small molecules, we computed molecular descriptors for a dataset of compounds and evaluated their discriminatory power using logistic regression. Following an initial regression-based analysis, we manually selected 18 descriptors with distinct physicochemical properties that showed relevance in distinguishing these two classes. A random forest classifier trained on these selected descriptors achieved an AUC of 0.91 (Figure S1), indicating strong classification performance.

Based on the classifier, we further developed a scoring function, integrating multiple molecular properties, including atomic composition, ring systems, and physicochemical descriptors, to assess molecular fitness. The final score is obtained by combining classifier prediction with the structural penalties, ensuring that molecules adhere to predefined chemical criteria.


\begin{figure*}[!htb]
    \centering
	%\vspace{-0.8cm}
    %\hspace{-3.6cm} % 负值表示左移，调整 cm 值以达到满意的偏移量
    \includegraphics[width=0.95\textwidth]{figures/figure2.png}
    \caption{\small ADSeqGAN training results on the nucleic acid and protein dataset. $A.$ The yield of nucleic acid binders using "NA" as the input label changes with the increase of epoch. Yield is calculated by $unique\_ratio\times verified\_ratio \times NA\_ratio$. $B.$ NLDock docking results of generated molecules with various nucleic acid targets. Use flexible and local modes. 1C9Z and 1EEL are DNA targets, while 2L94 and 6XB7 are RNA targets. The grey balls are original binders with native conformations, and the red ones are generated samples. $C.$ Plot of label responsiveness with the increase of epoch. The responsiveness metric is calculated by taking the $Log10$ of the NA binder yield ratio, with the numerator as the yield of NA binder after entering "NA" and the denominator as the yield after entering "Pro". }
    \label{fig:fig2}
\end{figure*}

\subsubsection*{ADSeqGAN Performs Best on The Nucleic Acid Binder Generation Task}

In this study, we compare our ADSeqGAN model with representative classic SMILES-based generative models (Table 1). The reference models, including MLE RNN, Native RL, SeqGAN, ORGAN, and MolGPT, were all trained from scratch on the dataset, while CLM and MolGen utilized pre-trained models and parameters to do downstream fine-tuning without incorporating predefined conditions or molecular scaffolds. Under the given experimental conditions, ADSeqGAN outperforms the other non-pretrained models in terms of sample generation success rate, synthetic feasibility, and yield of nucleic acid-targeting molecules. These non-pretrained models struggled to produce a substantial number of small molecules targeting nucleic acids. On the other hand, while testing nucleic acid binder generation task on pretrained models, the generated samples did not meet the corresponding expectations either, especially for NA yield. As training progresses, CLM improves in generating the correct types of molecules, but its validity ratio declines. In contrast, MolGEN exhibits strong responsiveness to certain molecules, producing a diverse set of structures, while for others, it fails to differentiate and repeatedly generates identical structures. 

We then randomly test different random number seeds to observe the training process (Figure 2A). Although the performance was different, the trend showed that the NA Generator gradually learned the characteristics of nucleic acid molecules, and the optimal yield could all be greater than 50\%, far higher than the representative baseline models.  

Additionally, ADSeqGAN achieved impressive FCD scores, indicating that our model learned richer molecular structures, especially comparing with pretrained models. Specifically, the generated nucleic acid binders captured some features of protein-targeting small molecule. As shown in principal component analysis (PCA) in Figure S2, the distributions of the characteristic molecular fingerprint fragments of generated NA samples scattered around the Pro molecular database, rather than just the NA binder dataset. Besides, the introduction of repetition penalties also contribute to more diverse molecular sampling. 

The QED scores were lower compared to pre-trained models, MLE RNN, and some RL-based models. It is worth noting that both MLE RNN and Native RL models, trained on a mixed dataset, exhibited lower QED scores compared to those trained exclusively on nucleic acid data. This suggests that the mixed dataset increased the difficulty of learning QED. Moreover, QED correlates with molecular mass. For instance, the average length of molecules generated by Native RL, which was trained on nucleic acid data alone, was 30.7, whereas ADSeqGAN generated molecules with an average length of 52.8. The higher QED values in CLM and MolGEN are attributed to their pre-training on highly bioactive molecules, which also contributed to their larger QED scores.

\begin{figure*}[!htb]
    \centering
	%\vspace{-0.8cm}
    %\hspace{-3.6cm} % 负值表示左移，调整 cm 值以达到满意的偏移量
    \includegraphics[width=0.95\textwidth]{figures/figure3.jpg}
    \caption{\small Ablation experiments. $A.$ Effect of MinMax regularization on molecular length during training. Orange indicates without, blue indicate with MinMax regulation. The solid line indicates NA and the dotted line indicates Pro. $B.$ The effect of length weight on the length, verified ratio and unique ratio of the molecules generated during training running under the SeqGAN framework. $C.$ The effect of random number on molecular length during training. $D.$ Tanimoto similarity matrix of samples generated by different random numbers at the 40th epoch. }
    \label{fig:fig3}
\end{figure*}

To further validate the effectiveness of the generated molecules, we selected two DNA targets (1C9Z\cite{Shui2000}, 1EEL\cite{Mazur2000}) and two RNA targets (2L94\cite{Marcheschi2011}, 6XB7\cite{Davila-Calderon2020}) for docking experiments using NLDock\cite{Feng2021}, which is specially developed for simulate the interaction between nucleic acids and ligands. The docking process was configured in the local and flexible mode. We used a virtual screening dataset consisting of 6,400 molecules generated from a well-trained generator (Figure 2B). The results showed that for three out of the four targets (1EEL, 2L94, and 6XB7), many of our generated molecules exhibited stronger binding affinities than the original binders in their native conformations. Besides, compared with the original molecule, the structures are very diverse. However, for 1C9Z, due to its unique pocket shape, accurate molecular conformation generation played a crucial role in docking performance. When using only the SMILES of the native small molecule as input without specifying its experimental binding conformation, some of our generated molecules displayed superior affinity performance than the native ligand of this nucleic acid target.  

It is worth noting that, compared to real molecules, the model struggles to generate highly symmetrical structures, such as those found in 1C9Z, 1EEL, and 2L94. Additionally, certain targets, such as 1PBR \cite{Fourmy1996} and 1QD3 \cite{Faber2000}, have small-molecule ligands that entirely lack aromatic rings. However, in our experiments, we did not observe any generated samples without aromatic rings or double bonds. This limitation may stem from the scarcity of such molecules in the training dataset, making it difficult for the model to learn their characteristics. The proportion of fused aromatic rings tends to decrease because the SMILES notation for such structures is more complex. For example, we find molecules with fused aromatic rings targeting 1C9Z most in the first 25 epochs.

%我们主要将代码与经典的基于SMILES式的生成模型进行了比较（table 1），其中作为 参考的MLE RNN，Natiive RL，SeqGAN, ORGAN, MolGPT是完全从头利用数据集进行训练，而CLM和MolGen则直接使用了预训练模型和参数，微调时没有使用预设条件或分子的scaffold。在给定的参数条件下，训练结果显示ADSeqGAN生成样品的成功率，样品的可合成性和核酸靶向分子的产率均为最佳。其它的模型难以得到大量的靶向核酸的小分子。FCD指标也表现出不错的性能，说明我们的分子学习到了更加丰富的结构：一方面，生成的核酸binder捕捉到了部分蛋白质小分子药物的部分特征，另一方面，惩罚的引入使得分子更趋于多样化采样。QED指标则逊于预训练模型，基于MLE的RNN和部分RL模型。值得注意的是，使用混合数据集的MLE RNN和Native RL模型的QED也低于单纯使用NA数据集的模型。可能原因是混合模型增大了学习到QED的难度。此外，QED与分子质量相关。仅使用NA的Native RL的生成的分子的平均长度为30.7，而ADSeqGAN则为52.8。值得一提的是，由于CLM和MolGEN均使用了高生物活性的分子进行了预训练，也导致了最后的QED值更大。
%为了进一步验证生成分子的效果，挑选了2个DNA靶点：1C9Z，1EEL和2个RNA靶点：1PBR，1QD3使用NLDock进行Docking实验。docking程序被设定为local和flexible。我们仅仅使用了从一个训练好的生成器生成的6400个分子作为虚拟筛选数据集。结果表明，给定的数据集在除了1C9Z之外的另外三个核酸靶点中，均能筛选得到结合能力比原来的binder和原有构象更强的分子。并且与原分子相比，结构差异很大。对于1C9Z而言，由于其特殊的口袋形状，对软件分子构象的生成能力要求也较高。如果单纯地使用原有小分子作为输入，而不指定结合时的构象，则我们的数据集中有分子明显表现更加。
%需要注意的是，与真实存在的分子相比，模型很难生成极具对称性的分子，例如1C9Z,1EEL和2L94中的分子。此外，某些靶点如1PBR\cite{Fourmy1996}, 1QD3\cite{Faber2000}，它们的小分子ligand完全不含有芳香环。我们的实验中同样也没有观察到不含有芳香环或者双键的样品。这可能是由于训练集中此类分子太少，难以学习到它们的特征。

\subsubsection*{ADSeqGAN Differentially Generates Nucleic Acid and Protein Binders}

ADSeqGAN generates distinct molecular outputs depending on the input label. By calculating the $Log10$ ratio of the proportion of nucleic acid binders among valid molecules when given the NA label to the proportion of nucleic acid binders when given the Pro label at each epoch, we observe that the generator exhibits strong label responsiveness. Specifically, when the NA label is provided, the model preferentially generates nucleic acid binders, whereas inputting the Pro label results in a higher proportion of protein binders. This trend becomes increasingly pronounced as training progresses (Figure 2C).

Furthermore, analyzing the SMILES sequence lengths at each epoch reveals a notable difference in molecular size. As shown in Figure S3, molecules generated with the Pro label tend to be shorter than those generated with the NA label. This pattern is consistent with the length distribution observed in the real dataset, which is 40.4 for Pro and 46.1 for NA, further validating the model's ability to learn and replicate intrinsic structural characteristics of nucleic acid and protein binders.

%ACSeqGAN根据标签输入的不同会得到不同的结果。通过计算每一个Epoch下，输入NA标签产生的核酸药物占据valid molecuels的比例与输入Pro标签产生的核酸药物的占据valid molecuels的比例之比的Log10对数，可以发现，随着训练的进行，生成器会对标签拥有强响应性（Figure 2C）。进一步，观察每一个Epoch形成的SMILES式长度，如Figure S3所示，可以发现Pro标签生成的分子短于NA标签生成的分子。这也与真实数据集的长度分布相符。

\subsubsection*{Standardization Reduce Mode Collapse and Over-fitting}


GANs are prone to mode collapse and overfitting during training, often resulting in repetitive sequences such as "c1ccccc1" and "NCCCNCCCN." Unlike traditional SeqGAN and ORGAN, we applied a min-max transformation to map the rewards from both $D_{\psi}$ and $D_n$ to the range [0,1], preventing excessive rewards for certain sequences or bias toward a single discriminator (Figure 3A). This adjustment stabilized the training process, making the length fluctuate less.

%GAN在训练过程中，容易出现Mode Collapse和over-fitting的问题，例如重复的“c1ccccc1”和“NCCCNCCCN”序列。与经典的SeqGAN和ORGAN不同，我们将$D_{\psi}$和$D_{n}$的奖励同时使用minmax变换映射到0-1之间，防止对某些序列奖励过大或是奖励偏向于某一个判别器（figure 3A）。可以发现加上minmax变换后，训练过程更加稳定且最终产生的核酸binder的比例更高。

%此外随着实验的进行，能够观察到在同一个random number时，$s_x$容易出现重复的采样。在我们的实验中x的长度设置为10。刚开始时，我们对生成这10个token采样时都加入了noise，结果发现当noise加的偏大时，训练会不稳定并且生成样本的合法率会下降；而noise偏小时，$s_x$出现重复模式的时间步略有延后，但最终仍然会陷入相同的模式。最终我们选择了只在生成头两个token的时候引入较大的noise，这样能够使得头两个token保持多样化并减少$s_x$生成失败的概率。通过比较组内分子的Tanimoto Similarity可以发现生成的分子的多样性更高。

\subsubsection*{Length Weight and Penalizing Repetition Give Higher Generation Quality}

Although the authors of ORGAN claimed that GAN-generated molecules exhibit similar lengths to those in real molecular datasets, our experiments revealed that without length penalties, the generated molecules tend to be shorter than those in our dataset. We replicated their findings on the QM9\_5K dataset, where the average SMILES length is only 15.4. However, in our NA+Pro dataset, the average length is 42.8, with more complex structural expressions. As molecular length increases, the success rate of generating valid molecules decreases. Additionally, since the model learns molecular syntax by rewarding only valid molecules, this further biases the generation toward shorter sequences. A similar trend is observed in Native RL trained on NA data, where the final generated molecules have an average length of only 32.7—significantly shorter than the NA dataset’s original average of 46.1.

To address this, we applied both length weighting and repetition penalization during training. As shown in Figure 3B, this resulted in more stable sequence lengths, with generated molecules maintaining an average length around 40. Notably, longer generated molecules exhibited a higher proportion of unique samples and a lower proportion of verified SMILES, suggesting a positive correlation between sequence length and molecular diversity and a negative correlation between sequence length and validation. Therefore, we propose adjusting training parameters dynamically to further enhance generation quality: gradually increasing the weight of length constraint while reducing repetition penalization as the model generates longer SMILES sequences. 

%尽管在ORGAN中作者声称GAN模型生成的分子与实际的分子数据集的长度相似，我们也在他们的qm9_5k数据集上复现了这一点，然而在我们的数据集上则发现不加入长度惩罚时生成的分子往往比原数据集的长度更短。可能的原因是，qm5_9k中SMILES式的平均长度仅为15.4，而NA+Pro则为42.8并且后者的表达式更加复杂。因此随着生成长度的增加，生成合法分子的成功率也在下降。再加上为了让模型学习到分子的语法，只有合法的分子才能受到奖励，进一步使得生成分子的长度下降。这一点也在Native RL使用NA的方法中得以体现，其最终生成分子的平均长度仅为32.7，远低于NA本身的46.1。

%因此，我们在训练时同时应用了Length Weight和repetition penalization。可以发现（Figure 3B）,随着训练的进行，生成的samples的平均长度明显能稳定在40附近。值得注意的是，分子生成的长度越长，生成独特样本的比率就更高，似乎呈现正比关系（Figure 3D）。因此随着生成SMILES长度的增加，可以增加Light Weight的权重，减少repetition penalization的权重进行训练。

\subsubsection*{Random Number Leads to Diverse Molecules}

RL is highly sensitive to random seeds, with different seeds potentially leading to vastly different results\cite{Henderson2017, Islam2017}. By comparing samples generated from different random seeds (Figure 3C,D), we observe that while all molecules are classified as nucleic acid binders, their specific structures vary significantly. To achieve greater molecular diversity, we strongly recommend conducting experiments with multiple random seeds to obtain a broader range of generated molecules.

%RL对随机数非常敏感，不同的随机种子可能会导致相当不同的结果。通过比较不同随机数的输入得到的样本（Figure 3E），能够发现它们虽然都被判别为核酸binder，但具体结构天差地别。为了得到更加多样化的结果，我们强烈推荐用多个随机种子进行实验以得到多样化的分子。

\subsection{CNS Drug Generation}

Despite the pressing need, the number of approved CNS drugs remains limited. One major obstacle in CNS drug development is the blood-brain barrier (BBB), a selective membrane that restricts the entry of many compounds into the brain, complicating the delivery of therapeutic agents. Recent advancements in artificial intelligence and machine learning offer promising avenues to overcome these hurdles by enabling the design of novel compounds with optimized properties for CNS activity, but the few samples are still a problem\cite{Morofuji2020}. We demonstrate the potential of our small sample based generative model to expand the library of candidate molecules for CNS drugs.

\subsubsection*{Oversampling Enable ADSeqGAN to Generate CNS Drugs}

\begin{figure}[h] % 普通 figure 仅占单列
    \centering
    \includegraphics[width=\linewidth]{figures/figure4.png}
    \caption{\small Changes in the yield of CNS drugs with training under different sampling methods}
    \label{fig:fig4}
\end{figure}

Due to the limited availability of CNS drugs in the dataset (only 548 samples), their contribution to the parameter updates during the pretraining of the RNN is minimal. As a result, the initial generation of CNS drugs is rare, which negatively impacts the subsequent adversarial and reinforcement learning training. Furthermore, the discriminator is prone to being biased by the majority class, further reducing the likelihood of CNS drug generation.

To address this issue, we employed an oversampling technique during mixed training by tripling the sampling frequency of CNS drug data. This approach significantly improved the yield of CNS drugs, reaching over 5\%, which is substantially higher than training without oversampling. However, further increasing the yield remains challenging due to two main reasons: (1) molecules of other classes also receive rewards, thus, the model tends to learn majority-class features to maximize local reward when sample distribution is imbalanced; (2) the limited number of CNS drug samples results in a less effective classifier during pretraining, as illustrated in Figure S4.

Overall, ADSeqGAN combined with oversampling provides a novel strategy for generating molecules from small-sample datasets, demonstrating its potential for addressing data scarcity in drug discovery.

%由于，CNS drug只有548个，数目非常少，因此在预训练RNN过程中对参数的贡献过低，进而导致初始成功生成的CNS drug的分子少，不利于后续的对抗和强化学习训练。此外，使用判别器进行判别时也容易受到多数样本的影响。基于此，我们尝试了在进行混合训练时，对原始的cns的进行over-sampling，重复采样三次。可以发现，CNS的药物产率最高可以超过5%，远超于不进行over-sampling的训练。但值得注意的是，产率很难进一步提高，这是因为两方面的原因：一是因为生成其它种类的分子也同样会有奖励，而当数据集中不同种类的样本数目偏差过大，模型更倾向于学习到多数样本的特征，因为这样可以使奖励达到局部最大化；二是，因为样本过少，预训练得到的分类器效果并不好，如Figure S4所示。但整体而言，ADSeqGAN和over-sampling为小样本生成模型提供了一个新的思路。

\section{Conclusion}
In this study, we proposed ADSeqGAN, a novel sequence-based GAN framework incorporating auxiliary discriminators for small-sample molecular generation. By integrating a pretrained classifier as an additional discriminator, ADSeqGAN improves the generation of specific molecular classes, i.e., nucleic acid binders and CNS drugs. Our results demonstrate that ADSeqGAN outperforms traditional non-pretrained generative models in terms of molecular validity, diversity, and class specificity.

Through a combination of MLE pretraining generator, Wasserstein loss, and data augmentation techniques such as oversampling, ADSeqGAN effectively addresses mode collapse and enhances the generative process. The model exhibits strong label responsiveness, successfully differentiating nucleic acid binders from protein binders and generating CNS drugs at a significantly higher rate than standard approaches. Furthermore, docking experiments confirm the ability of ADSeqGAN in generating high-quality molecules, thus it has a great application in enlarging compound library for nucleic acid drug discovery.

Future work will focus on incorporating molecular scaffold information and SMILES syntax rules into the generation process to improve the success rate of valid molecule generation. We also aim to refine training strategies to further optimize molecular properties and integrate more advanced reinforcement learning techniques to enhance chemical space exploration.

\let\thefootnote\relax\footnotetext{*Corresponding author: Junmei Wang (\texttt{JUW79@pitt.edu})}

%\bibliographystyle{unsrt}
\bibliographystyle{plain}     % 使用数字引用的样式
\bibliography{references} % 如果使用 .bib 文件

\appendix
\clearpage % 另起一页
\onecolumn  % 切换回单列格式

\section*{Appendix}  
\addcontentsline{toc}{section}{Appendix} % 让目录显示 Appendix

\section{Supplementary Information}
This is appendix A.
\begin{figure*}[htbp]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/figureS1.jpg}
    \renewcommand{\thefigure}{S1}  % 仅修改这个 figure 的编号
    \caption{Evaluation of random forest classifier of NA and Pro datasets}
    \label{figS1:figS1}
    \renewcommand{\thefigure}{\arabic{figure}}  % 恢复默认编号
\end{figure*}



\begin{figure*}[htbp]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/figureS2.jpg}
    \renewcommand{\thefigure}{S2}  % 仅修改这个 figure 的编号
    \caption{Principal component analysis of the characteristic molecular fingerprint fragments of NA dataset and generated NA samples with original Pro dataset}
    \label{figS2:figS2}
    \renewcommand{\thefigure}{\arabic{figure}}  % 恢复默认编号
\end{figure*}

\begin{figure*}[!htb]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/figureS3.png}
    \renewcommand{\thefigure}{S3}  % 仅修改这个 figure 的编号
    \caption{The impact of random number on the training process.}
    \label{figS3:figS3}
    \renewcommand{\thefigure}{\arabic{figure}}  % 恢复默认编号
\end{figure*}

\begin{figure*}[!htb]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/figureS4.pdf}
    \renewcommand{\thefigure}{S4}  % 仅修改这个 figure 的编号
    \caption{Evaluation of random forest classifier of CNS and non-CNS datasets}
    \label{figS4:figS4}
    \renewcommand{\thefigure}{\arabic{figure}}  % 恢复默认编号
\end{figure*}

%\section{More Data}
%This is appendix B.

\end{document}
