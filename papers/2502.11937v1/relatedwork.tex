\section{Related Work}
\vspace{-1ex}
To improve the performance of TSC, various methods based on RL have been proposed. For example, PressLight \cite{wei2019presslight}, CoLight \cite{wei2019colight}, MPLight \cite{chen2020toward}, MetaLight \cite{zang2020metalight}, and RTLight \cite{ye2023brief} performed TSC optimization based on the concept of pressure from the Max Pressure (MP) control theory \cite{varaiya2013max} to design the state and reward. Unlike these methods, IPDALight \cite{zhao2022ipdalight} proposed a new concept named intensity, which investigates both the speed of vehicles and the influence of neighboring intersections. To reflect the fairness of individual vehicles, FairLight \cite{ye2022fairlight} and FELight~\cite{du2024felight} considered the relationship between waiting time and driving time, and the extra waiting time of vehicles, deceptively. To exploit the cooperation among RL agents in the road network, FedLight \cite{ye2021fedlight} and RTLight \cite{ye2023brief} adopt federated reinforcement learning to share knowledge. HiLight \cite{xu2021hierarchically} cooperatively controls traffic signals to directly optimize average travel time by using hierarchical reinforcement learning. UniLight \cite{jiang2022multi} uses a universal communication form between intersections to implement cooperation. However, the RL agents of these methods are trained from the randomly initialized models, resulting in a long training time before obtaining the final control strategy.

To improve learning efficiency, imitation learning \cite{agarwal2019reinforcement,argall2009survey,hussein2017imitation,osa2018algorithmic} that makes the RL agent learn from the expert demonstration is a promising way. Currently, imitation learning can be divided into two categories: behavioral cloning \cite{bain1995framework,pomerleau1991efficient} and adversarial imitation learning \cite{abbeel2004apprenticeship,syed2007game,ziebart2008maximum}, both of which have been applied in TSC. Specifically, DemoLight \cite{xiong2019learning} is a behavioral cloning-based method that reduces the imitation learning task to a common classification task \cite{ross2010efficient,syed2010reduction} by minimizing the action difference between the agent strategy and the expert strategy. However, since this method is trained in the single-intersection environment and relies on the pre-collected expert trajectory from the same environment, it cannot be applied to multi-intersection scenarios and is very specific to the training environment. On the other hand, as an adversarial imitation learning-based method, InitLight \cite{ye2023initlight} uses a generative adversarial framework to learn expert's behaviors, where the discriminator iteratively differentiates between pre-collected expert and agent trajectories (generated through real-time agent-environment interactions). Although InitLight can use trajectories from different environments to train RL agents, it still needs a pre-training process to obtain an initial model.

To the best of our knowledge, FitLight is the first federated imitation learning framework for TSC to enable RL agents to plug-and-play for any traffic environment without additional pre-training cost, where the RL agent can achieve a high-quality initial solution in the first episode and then converge to an even better final result.

\vspace{-1ex}