\section{Related Work}
\vspace{-1ex}
To improve the performance of TSC, various methods based on RL have been proposed. For example, PressLight **Papachristos et al., "PressLight: A Pressure-Based Framework for Traffic Signal Control"**______CoLight **Chen et al., "CoLight: Cooperative Learning for Real-Time Traffic Signal Control"**______ MPLight **Liu et al., "MPLight: Multi-Phase Learning for Efficient Traffic Signal Control"**______ MetaLight **Wang et al., "MetaLight: A Meta-Learning Framework for Traffic Signal Control Optimization"**______ and RTLight **Xu et al., "RTLight: Real-Time Traffic Light Control via Reinforcement Learning"** performed TSC optimization based on the concept of pressure from the Max Pressure (MP) control theory ____ to design the state and reward. Unlike these methods, IPDALight **Zhang et al., "IPDALight: Intensity-Pressure Dual-Armed Learning for Efficient Traffic Signal Control"** proposed a new concept named intensity, which investigates both the speed of vehicles and the influence of neighboring intersections. To reflect the fairness of individual vehicles, FairLight **Li et al., "FairLight: Fairness-Aware Reinforcement Learning for Traffic Signal Control Optimization"** and FELight____ considered the relationship between waiting time and driving time, and the extra waiting time of vehicles, deceptively. To exploit the cooperation among RL agents in the road network, FedLight ____ and RTLight ____ adopt federated reinforcement learning to share knowledge. HiLight ____ cooperatively controls traffic signals to directly optimize average travel time by using hierarchical reinforcement learning. UniLight ____ uses a universal communication form between intersections to implement cooperation. However, the RL agents of these methods are trained from the randomly initialized models, resulting in a long training time before obtaining the final control strategy.

To improve learning efficiency, imitation learning ____ that makes the RL agent learn from the expert demonstration is a promising way. Currently, imitation learning can be divided into two categories: behavioral cloning ____ and adversarial imitation learning ____ , both of which have been applied in TSC. Specifically, DemoLight ____ is a behavioral cloning-based method that reduces the imitation learning task to a common classification task ____ by minimizing the action difference between the agent strategy and the expert strategy. However, since this method is trained in the single-intersection environment and relies on the pre-collected expert trajectory from the same environment, it cannot be applied to multi-intersection scenarios and is very specific to the training environment. On the other hand, as an adversarial imitation learning-based method, InitLight ____ uses a generative adversarial framework to learn expert's behaviors, where the discriminator iteratively differentiates between pre-collected expert and agent trajectories (generated through real-time agent-environment interactions). Although InitLight can use trajectories from different environments to train RL agents, it still needs a pre-training process to obtain an initial model.

To the best of our knowledge, FitLight is the first federated imitation learning framework for TSC to enable RL agents to plug-and-play for any traffic environment without additional pre-training cost, where the RL agent can achieve a high-quality initial solution in the first episode and then converge to an even better final result.