\appendix
\supptitle

\section{Prompt template for chain-of-rank}
\begin{tcolorbox}[fonttitle=\small\bfseries,
fontupper=\scriptsize\sffamily,
fontlower=\fon{put},
colback=gray!5!white, colframe=gray!75!black,
enhanced,
left=2pt, right=2pt, top=2pt, bottom=2pt,
title=Prompt template for domain-specific RAG with CoR]
\begin{lstlisting}[]
Contexts and Question are given. 

Let's think step by step to make 
correct output. 

First, reranking goal: select the 
relevant contexts, important to answer 
the question correctly. 

Then, answering goal: Focusing on the 
selected context, answer the question. 


Question: {question}
Context1: {context_1}
Context2: {context_2}
...
ContextN: {context_N}

Output:
## Relevant Context ID: {IDs}
## Answer: {answer}
\end{lstlisting}
\end{tcolorbox}


\section{Feasibility in task-agnostic RAG-LLM.} 
To identify the potential of the proposed chain-of-rank as the general reasoning technique in RAG beyond domain-specific RAG, we applied the proposed method on the pre-trained model (LLaMA3-8B). As shown in the Table~\ref{tab:agnostic}, the CoR is comparable to CoT.
Further, we combined the reasoning of the CoT and CoR. On top of the CoT-style reasoning, the reasoning of CoR makes meaningful synergistic results using a small cost.



\section{Datasets}
\noindent \textbf{HotPotQA.} HotPotQA is a large-scale question-answering dataset designed to evaluate both factual reasoning and multi-hop question answering. The training set with approximately 90,000 examples and development (dev.) set containing around 7,400 examples. For each question, ten contexts are provided where a context consists of several sentences and the key sentences (supporting facts to the query) are annotated. We experimented with the whole sentences in every context for a challenging set-up.

\begin{table}[t]
  \centering
  \begin{adjustbox}{width=0.8\linewidth}
  \begin{tabular}{@{}lcc@{}}
    \toprule
    Method & EM & F1 Score \\
    \midrule
    LLaMA3-8B pre-trained &  40.84 & 52.47 \\
    \; + CoT & 42.41 & 53.96\\
    \; + CoR & 41.20 & 55.92 \\
    \; + CoR\&CoT & 44.15& 58.09 \\
    \bottomrule
    
  \end{tabular}
  \end{adjustbox}
  \caption{\textbf{Results on a pre-trained LLM by applying the CoT, the proposed CoR, the mixture of CoT and CoR.} EM and F1 score are reported on the HotPotQA dataset.}
  \label{tab:agnostic}
\end{table}

\noindent \textbf{Gorilla API.} Gorilla API is multi-faceted, comprising three domains: TensorFlow, HuggingFace, TorchHub where training data includes 6190, 8191, 337 entries and testing data does 688, 911, 186 entries, respectively. Each entry of a domain conveys a detailed description for an API call. In specific, it consists of the following fields: \{domain, framework, functionality, api\_name, api\_call, api\_arguments, environment\_requirements, example\_code, performance, and description\}.

\noindent \textbf{Reasoning dataset for baseline training.}
Gorilla API dataset provides the explanation for every API document, and hence we use that as the reasoning following \cite{RAFT} for domain-specific training. In HotPotQA dataset which does not include reasoning, we utilized a significant-scale LLM to generate the intricate reasoning dataset for domain-specific training. We used the prompt of~\cite{RAFT} to generate the reasoning.

%tf 688, hf 911, th 186 in testing
% 6190, 8191, 837



\section{Evaluation Metric}
\noindent \textbf{Exact Match.} Exact Match (EM) evaluates whether the model’s generated response is identical to the ground truth answer. It is computed as the percentage of predictions where the generated output exactly matches the reference answer, including the order and wording. EM is strict, meaning any deviation results in a score of 0 for that prediction, and only exact matches count as correct.

\noindent \textbf{F1 score.} F1 score is a measure that balances precision and recall. It is computed by comparing the overlap of tokens between the generated response and the ground truth. Precision is the ratio of correct tokens in the generated response to the total number of tokens in the response, while recall is the ratio of correct tokens to the total number of tokens in the ground truth. The F1 score is the harmonic mean of precision and recall, allowing for partial credit when the generated answer partially overlaps with the correct answer.

\noindent \textbf{AST accuracy}. AST (Abstract Syntax Tree) accuracy is a metric used to evaluate the correctness of generated API calls by comparing their structural representation to reference APIs. The generated API call is parsed into an AST, and its structure is matched against the corresponding reference API from the dataset. The accuracy is determined by how well the generated API’s function names and key arguments align with the reference. If the AST of the generated call matches a subtree of the reference API, it is considered correct.
 






\section{Prompt template to evaluate the reasoning}

\begin{tcolorbox}[fonttitle=\small\bfseries,
fontupper=\scriptsize\sffamily,
fontlower=\fon{put},
colback=gray!5!white, colframe=gray!75!black,
enhanced,
left=2pt, right=2pt, top=2pt, bottom=2pt,
title=Prompt template to evaluate reasoning]
\begin{lstlisting}[]
You are an expert at evaluating 
reasoning based on provided 
information. Given a question, five 
retrieved contexts, and reasoning, 
your task is to determine whether 
the reasoning is based on the correct 
context. The correct context is the 
one that contains the most relevant 
and accurate information to answer 
the question.

Follow these steps:
1. Identify which retrieved context 
contains the most accurate information 
to answer the question (the "golden 
context").
2. Evaluate if the reasoning is based 
primarily on this golden context.
3. Provide a clear answer (Yes or No).

### Question:
{question}

### Retrieved Contexts:
1. {context_1}
2. {context_2}
3. {context_3}
4. {context_4}
5. {context_5}

### Reasoning:
{reasoning}

### Evaluation:
Is the reasoning based on the correct 
context? Answer with "Yes" or "No".
\end{lstlisting}
\end{tcolorbox}


\section{LoRA-based training details}
We implemented the proposed and baseline approaches based on the Huggingface PEFT library~\cite{peft}. We set the rank $r$ and scaling factor of a LoRA as 128 and 16, respectively. In training, we use the AdamW optimizer with a learning rate 0.0003 which is cosine annealed. We also set the batch size as 128 and the maximum epoch as 1. All the proposed and baseline methods are implemented with PyTorch 2.0.1 and executed on two NVIDIA A5000 GPUs.



