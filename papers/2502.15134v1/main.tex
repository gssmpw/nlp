 % This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}

% Remove the "review" option to generate the final version.
\usepackage[]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{comment}
\usepackage{color}
\usepackage{listings}

\lstset{
   basicstyle=\ttfamily\small,  % Typewriter font, smaller size
   backgroundcolor=\color{gray!10}, % Light gray background
   frame=single, % Single line border around the box
   breaklines=true, % Enable line breaking within the box
   %postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space}, % Show an arrow for wrapped lines
   tabsize=2, % Tab size
   language=, % No specific language
   keepspaces=true, % Keep spaces in text
   prebreak=\mbox{}, % Do not indent when a line is broken
   postbreak=\mbox{}
}


%\usepackage{xcolor}
\usepackage{colortbl}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
\usepackage[most]{tcolorbox}
% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

\usepackage{mathtools}
\usepackage{makecell}



\usepackage{array,multirow,graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
%\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage[export]{adjustbox}
\usepackage{float}

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
%\Crefname{section}{Section}{Secs.}
\Crefname{table}{Table}{Tables}
% \crefname{table}{Tab.}{Tabs.}
%\newcommand\algorithmautorefname{Algorithm}
\def\subsectionautorefname{Sec.}
\def\sectionautorefname{Sec.}

%\renewcommand*{\algorithmautorefname}{Algorithm}
\renewcommand*{\figureautorefname}{Fig.}
\renewcommand*{\equationautorefname}{Eq.}
\renewcommand*{\tableautorefname}{Tab.}


% \renewcommand\jt[1]{\textcolor{black}{#1}}

\input{commands/mathcommand}
\input{commands/mycommand}

\newcommand{\llama}{LLaMA}
\newcommand{\tr}{\textrm{tr}}
\newcommand{\per}{\textrm{c}}
\newcommand{\pool}{pool}
\newcommand{\mytitle}{Chain-of-Rank: Enhancing Large Language Models \\ for Domain-Specific RAG in Edge Device}
%\newcommand{\cmark}{\ding{51}}%
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

%\title{\alg: Instant Personalized LoRA Generation for On-device and Hybrid Decoding}
% \title{OPA: On-the-Fly Personalized Adapter \& Device-Server Consistent Inference for On-Device LLM}
%\title{On-Palette: On-the-fly Person-Adapted On-device LLM and Edge-to-server Transfer for Hybrid Inference}
% \title{On-Device Palette: Personalized On-the-Fly Adapter and Edge-Server Hybrid Inference}
\title{\mytitle}
% \title{Palette: Person-Aaptation of LLM On-the-Fly and Edge-Server Transit for }


%\title{Palette: Personalized Adapter for LLM Edge device T... Toward End device.}
% PersonAr,
% Personalized On-the-Fly Adapter and Device-Server Hybrid Inference for On-Device LLM.  


% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

%\author{Jihwan Bang$^*$\hspace{1em}Juntae Lee$^*$\hspace{1em}Kyuhong Shim\hspace{1em}Seunghan Yang\hspace{1em}Simyung Chang$^\dag$\\
%{Qualcomm AI Research$^\ddag$, Qualcomm Korea YH, Seoul, Republic of Korea} \\ 
%{\texttt {\small\{jihwbang, juntlee, kshim, seunghan, simychan\}@qti.qualcomm.com}}}

\author{Juntae Lee\hspace{1em}Jihwan Bang\hspace{1em}Seunghan Yang\hspace{1em}Kyuhong Shim\hspace{1em}Simyung Chang\\
{Qualcomm AI Research$^\dag$} \\ 
{\texttt {\small\{juntlee, jihwbang, seunghan, kshim, simychan\}@qti.qualcomm.com}}}

\begin{document}
\maketitle

\blfootnote{\hspace{-1.8em}$^\dag$Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc. and/or its subsidiaries.}

\begin{abstract}
%Retrieval-augmented generation (RAG) with large language models (LLMs) has proven effective in addressing factual hallucination by enabling dynamic access to external knowledge. This is particularly valuable in specialized domains, where precision is critical. 
Retrieval-augmented generation (RAG) with large language models (LLMs) is especially valuable in specialized domains, where precision is critical. 
To more specialize the LLMs into a target domain, domain-specific RAG has recently been developed by allowing the LLM to access the target domain early via finetuning. 
The domain-specific RAG makes more sense in resource-constrained environments like edge devices, as they should perform a specific task (e.g. personalization) reliably using only small-scale LLMs.
%The domain-specific RAG is more crucial when computational resources are limited such as edge devices since only small LLM is needed to perform well on the target task
%a small-scale LLM needs to focus on doing several selected tasks well. 
%Domain-specific RAG is useful for edge devices with limited computational resources, since smaller, specialized LLMs can efficiently handle certain tasks.
While the domain-specific RAG is well-aligned with edge devices in this respect, it often relies on widely-used reasoning techniques like chain-of-thought (CoT). The reasoning step is useful to understand the given external knowledge, and yet it is computationally expensive and difficult for small-scale LLMs to learn it. %Parameter-efficient fine-tuning methods, such as LoRA adapters, offer a solution to resource constraints but further limit the model's ability to perform complex reasoning tasks. 
Tackling this, we propose the Chain of Rank (CoR) which shifts the focus from intricate lengthy reasoning to simple ranking of the reliability of input external documents. Then, CoR reduces computational complexity while maintaining high accuracy, making it particularly suited for resource-constrained environments. We attain the state-of-the-art (SOTA) results in benchmarks, and analyze its efficacy.

\end{abstract}
%\jt{}
\input{Intro}
\input{Related}
\input{Method}
\input{Exp}


\section{Conclusions}
\vspace{-0.25cm}
We proposed the Chain of Rank (CoR) to address the limitations of the existing intricate reasoning processes like chain-of-thought in training-based, domain-specific RAG. For domain-specific RAG training, annotation expense for the reasoning data is required. Also, especially in testing on smaller LLMs in resource-constrained environments, it poses challenges in terms of the accuracy as well as computational cost. We observed that the inaccurate reasoning adversely affect the quality of final answer. By shifting the focus from elaborate reasoning to a simplified ranking of the reliability of retrieved documents, CoR significantly reduced computational complexity while attaining higher accuracy. Our experimental results demonstrated that CoR achieves SOTA results on RAG benchmarks, confirming its effectiveness in improving the domain-specific RAG performance of small-scale LLMs. %Furthermore, we showed that CoR is well-suited for specialized domains, offering a more efficient alternative to traditional reasoning methods without sacrificing precision. 
%Our findings also highlight the potential of CoR to enhance various RAG-based solutions, especially in the environments where resource efficiency is critical, such as edge devices.


\section{Limitations} 
\label{sec:limitation}
This work acknowledges the significance of reasoning in domain-specific RAG models and presents an efficient approach that reduces the need for complex training data labeling and significantly lowers reasoning costs during testing. However, we did not thoroughly investigate whether the proposed method would be equally effective in more general RAG frameworks that do not rely on task-specific training. That said, preliminary results presented in the appendix indicate the potential for success in general RAG settings, suggesting that this area warrants deeper exploration in future work. Therefore, our findings provide a promising foundation for future research.


\section{Ethical Consideration}
In the field of domain-specific RAG, if the applications involve sensitive areas such as personal information, special caution must be taken during the model training process to ensure privacy and data protection. Beyond this consideration, methodologically, our research focuses on improving the accuracy and efficiency of RAG in LLMs, we do not foresee any direct negative ethical concerns stemming from our contributions. Nonetheless, it is important to recognize that generative AI technologies, including those using LLMs, come with potential risks. As such, careful consideration of their broader ethical and societal implications is necessary when these systems are applied in the real world.

% Entries for the entire Anthology, followed by custom entries
% \bibliography{anthology,custom}
\bibliography{bib}

\newpage
\input{appendix}


\end{document}
