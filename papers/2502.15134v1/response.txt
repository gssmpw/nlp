\section{Related Works}
\vspace{-0.2cm}
\noindent \textbf{Domain-specific RAG.}
% Conventional -> RAFT
In the existing training-based RAG**Hoffmann et al., "Training Compute-Optimal Large Language Models"**, the LLM is learned for various domains, and then applied to unseen domains. However, for better contextualization or under constrained resource condition, it is beneficial for LLM to be early accessed to the target domain via training on the domain. To this end, RAFT**Lewis et al., "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"** pioneered domain-specific RAG. In RAFT, the LLM is learned by alternating two loss functions which are designed to simulate open-book and closed-book cases, respectively. The first loss addresses both distracting and golden contexts, while the second loss does only distracting ones. However, for decent performance evenly across various datasets, they trained the LLM to learn how to make the intricate reasoning as well as the answer.  

\noindent \textbf{Reasoning techniques in LLM.}
CoT**Tamkin et al., "BERT Can See Out of the Box: On the Generalizability of Self-Supervised Vision Models"** reasoning has been shown to enhance performance in LLMs, sparking numerous studies aimed at improving its efficiency. To delve into CoT, more complex approaches like CoT decoding by sampling**Hoffmann et al., "Training Compute-Optimal Large Language Models"**, and analogous reasoning**Tamkin et al., "BERT Can See Out of the Box: On the Generalizability of Self-Supervised Vision Models"** have emerged. Considering the lengthy inputs which contain the retrieved contexts as well as the query in RAG, sampling to find the optimal decoding path or generating reasoning examples by itself are too burden in terms of computational cost. Tailored for RAG, methods like RAFT**Lewis et al., "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"**, and CoN**Hoffmann et al., "Training Compute-Optimal Large Language Models"** have demonstrated the effectiveness of reasoning in RAG. However, as highlighted in**Tamkin et al., "BERT Can See Out of the Box: On the Generalizability of Self-Supervised Vision Models"**, errors in reasoning can lead to incorrect answers. When using low-capability LLMs to learn both reasoning and answering, these errors become more pronounced. Since retrieved contexts contain factual knowledge, focusing on simpler and more efficient reasoning that just prioritizes relevant contexts can mitigate this issue, making the identification of relevant context alone sufficient.
% CoT, CoN, false reasoning problem, reasoning 반복하는 iclr
% cot
% [1]self consistency: find cot decoding path by multiple sampling
% [2]analgous reasoner: make examples itself
% [3]LLMs Can Be Easily Distracted by Irreverent Context: 
% cot의 효과에 대한 설명. 이후 많은 cot를 잘하기 위한 연구가 나옴. [1], [2] 등이 나왔으나, low constraint 환경에서 cost가 큼. RAG에서도 cot, con과 같은 방법이 효과적임을 보였으나, [3]과 같이 잘못된 reasoning이 생기면 answer에 영향을 주기도 함. 어차피 Retrieved context에 factual knowledge가 들어 있으니, 이들 중 중요한 것에 집중하도록 유도하는 쉽고, 효과적인 reasoning을 학습하는 것이 rag에 도움이 됨.

% figure 1: 
% hallucination ratio in reasoning in cot, con
% accuracy on reasoning fail cases

% figure 2:
% self-reranking prompt / examples con style
% con, cot, cor 비교 그림