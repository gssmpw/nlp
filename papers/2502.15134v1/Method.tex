

\section{Method}

\subsection{RAG Problem Set-up}
In RAG, an LLM can be formalized as $p(y|x) = \sum_{D} p(y|x, D) p(D|x)$, where $x$ denotes the input query, $y$ represents the LLM generated answer, and $D=\{d^k\}_{i=1}^{K}$ contains the $K$ individual contexts. This formulation takes into account the joint probability of retrieving a set of contexts, rather than assuming the contexts are selected independently.

As this sum over all the context sets is impractical, generally an off-the-shelf retriever selects the top-$K$ most relevant contexts. This leads to the approximation: $p(y|x) \approx p(y| x, D)$. Furthermore, when a reasoning step is considered, it becomes as follows.
\begin{equation}
\label{eq1} 
    p(y|x) \approx \sum_{R} p(y|x, D, R) p(R|x,D)
\end{equation}
where $R$ represents the generated reasoning.

\begin{comment}
In the context of retrieval-augmented generation (RAG), a large language model (LLM) can be formalized as \( p(y|x) = \sum_{\{d_1, d_2, \dots, d_k\}} p(y|x, \{d_1, d_2, \dots, d_k\}) p(\{d_1, d_2, \dots, d_k\}|x) \), where \( x \) denotes the input query, \( y \) represents the model’s generated output, and \( \{d_i\}_{i=1}^{k} \) are the documents retrieved from a large corpus. This formulation takes into account the joint probability of retrieving a set of documents \( \{d_1, d_2, \dots, d_k\} \), rather than assuming the documents are selected independently.

Since summing over all possible document sets is computationally infeasible, an off-the-shelf retriever is employed to select the top-\( k \) documents most relevant to the query. This leads to the following approximation: 
\[
p(y|x) \approx p(y|x, d_1, d_2, \dots, d_k)
\]
where \( d_1, d_2, \dots, d_k \) are the top-\( k \) documents provided as input to the model. This allows the model to focus on a limited set of relevant information, reducing complexity and improving response quality.

When a single reasoning step is incorporated, the process can be expressed as:
\[
p(y|x) \approx \sum_{R} p(y|x, d_1, d_2, \dots, d_k, R) p(R|x, d_1, d_2, \dots, d_k)
\]
where \( R \) denotes the reasoning step, and \( p(R|x, d_1, d_2, \dots, d_k) \) represents the probability of selecting a specific reasoning process given the input query and the retrieved documents. This refinement accounts for the reasoning involved in generating the response from the retrieved documents, ensuring that the model can better integrate and reason over the relevant information.
\end{comment}

%In RAG, an LLM can be formalized as $p(y|x) = \sum_{d_1, d_2, \ldots, d_k}p(y|x, d_1, d_2, \ldots, d_k) \Pi_{i=1}^{k} p(d_i|x)$ where $x$ denotes the input query, $y$ represents the model’s generated output, and $\{d_i\}_{i=1}^{k}$ is retrieved documents. Since this sum over all documents is not feasible in practice, the top-$k$ documents, most relevant to the query are selected from an off-the-shelf retriever. This leads us to the approximation: $p(y|x) \approx p(y| x, d_1, d_2, \ldots, d_k)$. Moreover, when a single reasoning step is considered, the approximation can be represented as $p(y|x) \approx p(y| x, d_1, d_2, \ldots, d_k, R)$ where $R$ denotes the reasoning. 
%$\sum_{R}p(y|x, d_1, ..., d_k)p(R| x, d_1, ... d_k)$


%In the context of retrieval-augmented generation (RAG), a large language model (LLM) can be formalized as $p(y|x) = \sum_{i}p(y|d_i,x)p(d_i|x)$ where $x$ denotes the input query and $y$ represents the model’s generated output. However, due to the extensive number of potential document sources, directly computing this sum across all possible documents is impractical.
%Since this sum over all documents is not feasible in practice, the top-$k$, most relevant to the query is selected from an off-the-shelf retriever. This leads us to the approximation: $p(y|x) \approx p(y| \{d_0, \ldots d_k\}, x)$ where $d_1, . . . , d_k$ are the selected top-$k$ documents provided as input to the model. This approach allows the model to focus on the most relevant information, thereby enhancing the quality of the generated responses. When reasoning step is included, $p(y|x) \approx \sum_{i=1}^{k} p(y | d_i, x) p(d_i | x)$

%extensive number of potential document sources, directly computing this sum across all possible documents is impractical.To address this, a common strategy is to approximate the sum using the top $k$ documents.

\subsection{Chain-of-Rank} 

\noindent\textbf{Framework.}
We streamline the reasoning process by shifting the focus from complex reasoning to identifying the IDs of the given contexts that correspond to the most relevant ones for $x$. With just this process, the model can reduce cognitive overhead on less relevant information, and more pay attention to the relevant information. As illustrated in Fig.~\ref{fig:main}, each context is identified by its unique ID. Then, CoR involves two main steps: (1) selecting the ID of relevant contexts (\ie $R$) and (2) generating the final answer $y$.
%the prompt for the proposed CoR method structures the model’s task around identifying and ranking key information. 
Consequently, the CoR framework significantly simplifies the reasoning step, making it a practical solution for scenarios with limited computational resources while enhancing performance in domain-specific applications.

%By concentrating on context relevance, we ensure that the model directs its attention to the most pertinent information, thus improving the quality and accuracy of the generated responses $y$. This selective attention mechanism enables more efficient operation, particularly in resource-constrained environments, as it eliminates the need for complex reasoning that can overwhelm smaller LLMs. Consequently, the CoR framework significantly simplifies the reasoning step, making it a practical solution for scenarios with limited computational resources while enhancing performance in domain-specific applications.

%We streamline the reasoning process by shifting the focus from complex reasoning to the identification and ranking of context IDs that correspond to the most relevant information for the input query $x$. Rather than requiring the model to engage in multi-step chain-of-thought (CoT) reasoning, we prioritize selecting context IDs that are closely aligned with the query. This approach enables the model to filter out less relevant information early on, reducing cognitive overhead and allowing for more efficient information retrieval. As illustrated in Fig. xx, the prompt for the Chain of Rank (CoR) method facilitates this process by structuring the model’s task around identifying and ranking key contexts, rather than performing intricate reasoning.

%By focusing on the ranking of context relevance, we ensure that the model allocates its attention to only the most pertinent information, which in turn improves the quality and accuracy of the generated responses $y$. This selective attention mechanism allows the model to operate more efficiently, particularly in resource-constrained environments, as it eliminates the need for complex reasoning that can burden smaller-scale LLMs. As a result, the CoR framework simplifies the reasoning step significantly, making it a more practical solution for scenarios where computational resources are limited while maintaining or even improving the overall performance in domain-specific applications.

%We streamline the reasoning process by focusing on generating context IDs that correspond to the most relevant information pertaining to the input query $x$. Fig. xx describes the prompt for chain-of-rank. By prioritizing these context IDs, we allow the model to concentrate its attention on the most relevant contexts, enhancing the quality and accuracy of the generated responses $y$. This framework drastically simplifies the reasoning step.

\noindent\textbf{Model training.}
We concatenate the instruction, question, and retrieved documents into a single prompt, allowing the model to learn in a standard supervised manner. The model is trained to optimize both the selection of relevant document IDs and the accuracy of the generated answer by minimizing a joint loss function. 
\begin{equation}
\label{eq2} 
    \mathcal{L} = - \sum_{i=1} \log p(y_i | x_i, D_i, R_i) - \log p(R_i | x_i, D_i)
\end{equation}
We designed the top-$k$ documents in $D_i$ to include at least one positive document for a query $x_i$ during training. Also, we employed LoRA to efficiently fine-tune the model parameters assuming the low resource constraint (details are in Appendix).


