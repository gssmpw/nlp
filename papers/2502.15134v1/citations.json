[
  {
    "index": 0,
    "papers": [
      {
        "key": "linra",
        "author": "Lin, Xi Victoria and Chen, Xilun and Chen, Mingda and Shi, Weijia and Lomeli, Maria and James, Richard and Rodriguez, Pedro and Kahn, Jacob and Szilvasy, Gergely and Lewis, Mike and others",
        "title": "RA-DIT: Retrieval-Augmented Dual Instruction Tuning"
      },
      {
        "key": "wanginstructretro",
        "author": "Wang, Boxin and Ping, Wei and McAfee, Lawrence and Xu, Peng and Li, Bo and Shoeybi, Mohammad and Catanzaro, Bryan",
        "title": "InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining"
      },
      {
        "key": "asai2023self",
        "author": "Asai, Akari and Wu, Zeqiu and Wang, Yizhong and Sil, Avirup and Hajishirzi, Hannaneh",
        "title": "Self-rag: Learning to retrieve, generate, and critique through self-reflection"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "RAFT",
        "author": "Unknown",
        "title": "Unknown"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "wei2022chain",
        "author": "Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others",
        "title": "Chain-of-thought prompting elicits reasoning in large language models"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "wang2022self",
        "author": "Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny",
        "title": "Self-consistency improves chain of thought reasoning in language models"
      },
      {
        "key": "wang2024chain",
        "author": "Wang, Xuezhi and Zhou, Denny",
        "title": "Chain-of-thought reasoning without prompting"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "yasunaga2023large",
        "author": "Yasunaga, Michihiro and Chen, Xinyun and Li, Yujia and Pasupat, Panupong and Leskovec, Jure and Liang, Percy and Chi, Ed H and Zhou, Denny",
        "title": "Large language models as analogical reasoners"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "RAFT",
        "author": "Unknown",
        "title": "Unknown"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "con",
        "author": "Wenhao Yu and Hongming Zhang and Xiaoman Pan and Kaixin Ma and Hongwei Wang and Dong Yu",
        "title": "Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "shi2023large",
        "author": "Shi, Freda and Chen, Xinyun and Misra, Kanishka and Scales, Nathan and Dohan, David and Chi, Ed H and Sch{\\\"a}rli, Nathanael and Zhou, Denny",
        "title": "Large language models can be easily distracted by irrelevant context"
      }
    ]
  }
]