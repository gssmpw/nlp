\section{Related Works}
\vspace{-0.2cm}
\noindent \textbf{Domain-specific RAG.}
% Conventional -> RAFT
In the existing training-based RAG~\cite{linra,wanginstructretro,asai2023self}, the LLM is learned for various domains, and then applied to unseen domains. However, for better contextualization or under constrained resource condition, it is beneficial for LLM to be early accessed to the target domain via training on the domain. To this end, RAFT~\cite{RAFT} pioneered domain-specific RAG. In RAFT, the LLM is learned by alternating two loss functions which are designed to simulate open-book and closed-book cases, respectively. The first loss addresses both distracting and golden contexts, while the second loss does only distracting ones. However, for decent performance evenly across various datasets, they trained the LLM to learn how to make the intricate reasoning as well as the answer.  

\noindent \textbf{Reasoning techniques in LLM.}
CoT~\cite{wei2022chain} reasoning has been shown to enhance performance in LLMs, sparking numerous studies aimed at improving its efficiency. To delve into CoT, more complex approaches like CoT decoding by sampling~\cite{wang2022self,wang2024chain} and analogous reasoning~\cite{yasunaga2023large} have emerged. Considering the lengthy inputs which contain the retrieved contexts as well as the query in RAG, sampling to find the optimal decoding path or generating reasoning examples by itself are too burden in terms of computational cost. Tailored for RAG, methods like RAFT~\cite{RAFT} and CoN~\cite{con} have demonstrated the effectiveness of reasoning in RAG. However, as highlighted in~\cite{shi2023large}, errors in reasoning can lead to incorrect answers. When using low-capability LLMs to learn both reasoning and answering, these errors become more pronounced. Since retrieved contexts contain factual knowledge, focusing on simpler and more efficient reasoning that just prioritizes relevant contexts can mitigate this issue, making the identification of relevant context alone sufficient.
% CoT, CoN, false reasoning problem, reasoning 반복하는 iclr
% cot
% [1]self consistency: find cot decoding path by multiple sampling
% [2]analgous reasoner: make examples itself
% [3]LLMs Can Be Easily Distracted by Irreverent Context: 
% cot의 효과에 대한 설명. 이후 많은 cot를 잘하기 위한 연구가 나옴. [1], [2] 등이 나왔으나, low constraint 환경에서 cost가 큼. RAG에서도 cot, con과 같은 방법이 효과적임을 보였으나, [3]과 같이 잘못된 reasoning이 생기면 answer에 영향을 주기도 함. 어차피 Retrieved context에 factual knowledge가 들어 있으니, 이들 중 중요한 것에 집중하도록 유도하는 쉽고, 효과적인 reasoning을 학습하는 것이 rag에 도움이 됨.

% figure 1: 
% hallucination ratio in reasoning in cot, con
% accuracy on reasoning fail cases

% figure 2:
% self-reranking prompt / examples con style
% con, cot, cor 비교 그림



\begin{comment}

1. **"Training Compute-Optimal Large Language Models" (Hoffmann et al., 2022)**  
   이 논문에서는 모델 크기, 데이터 크기, 그리고 계산 비용 간의 최적 관계를 탐구하면서, 작은 모델이 대규모 언어 모델에 비해 복잡한 reasoning 작업을 수행하는 데 어려움을 겪을 수 있다는 점을 지적합니다. 작은 모델은 충분한 학습 능력을 갖추지 못해 잘못된 추론 경로를 따를 확률이 높으며, 그 결과 오류가 더 많이 발생하게 됩니다.

2. **"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks" (Lewis et al., 2020)**  
   이 논문에서는 retrieval-augmented generation (RAG)의 성능을 평가하면서, 작은 모델이 retrieved context에서 부정확한 정보를 적절히 걸러내지 못해 reasoning 오류가 발생할 수 있음을 언급합니다. 특히, retrieved context를 적절히 해석하지 못할 때 정답 도출 과정에서 혼란을 일으킬 수 있습니다.

3. **"BERT Can See Out of the Box: On the Generalizability of Self-Supervised Vision Models" (Tamkin et al., 2021)**  
   이 연구는 self-supervised 학습을 활용한 모델들이 다양한 task에서 얼마나 일반화되는지에 대해 논의하며, 작은 모델이 complex reasoning task에 취약하며 오류가 전파될 수 있음을 보여줍니다.
\end{comment}