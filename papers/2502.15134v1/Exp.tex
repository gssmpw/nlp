\section{Experiments}
We provide more details and analysis in Appendix.

\noindent \textbf{Datasets.}
In our experiments, we use the following datasets to evaluate the proposed method. We selected these datasets to represent both popular and diverse domains including Wikipedia and Coding/API documents. 

%we designed the datasets to evaluate the domain-specific RAG performance without being influenced via the off-the-shelf document retriever. 

\begin{table*}[t]
  \centering
  \begin{adjustbox}{width=0.8\linewidth}
  \begin{tabular}{@{}clccccc@{}}
    \toprule
    & \multirow{2}{*}{Method}      &      \multicolumn{2}{c}{HotPotQA}      &      \multirow{2}{*}{TensorFlow}    &      \multirow{2}{*}{HuggingFace}      &      \multirow{2}{*}{TorchHub} \\
    \cmidrule(ll){3-4}
    && EM& F1 score& & & \\ 
    \midrule
    & LLaMA3-8B & 40.84 & 52.47 & 32.11 & 10.14 & 22.13 \\
    \midrule
    \multirow{4}{*}{Domain-specific} & DSF & 44.98 & 59.15 & 83.91 & 87.42 & 70.08\\
    & RAFT (DSF-CoT) & 46.79 & 60.59 & 88.98 & 89.68 & 74.05\\
    & DSF-CoN & 48.60 & 62.04 & 84.52 & 79.05 & 76.21 \\
    \cmidrule(ll){2-7}
    & \textbf{DSF-CoR (Ours)} & \textbf{49.23} & \textbf{64.11} & \textbf{95.68} & \textbf{92.52} & \textbf{80.54} \\
    %\midrule
    %& LLaMA3-8B & 46.99 & 62.98 & 32.11 & 10.14 & 22.13 \\
    %\midrule
    %\multirow{4}{*}{Domain-specific} & DSF & 47.46 & 63.73 & 83.91 & 87.42 & 70.08\\
    %& RAFT (DSF-CoT) & 49.34 & 63.54 & 88.98 & 89.68 & 74.05\\
    %& DSF-CoN & 49.84 & 65.42 & 84.52 & 79.05 & 76.21 \\
    %\cmidrule(ll){2-7}
    %& \textbf{DSF-CoR (Ours)} & \textbf{50.12} & \textbf{66.04} & \textbf{95.68} & \textbf{92.52} & \textbf{80.54} \\

    \bottomrule
    
  \end{tabular}
  \end{adjustbox}
  \caption{\textbf{Comparative results on domain-specific RAG.} EM and F1 score for the HotPotQA, and AST sub-tree matching accuracy scores (\%) for the Gorilla API (TensorFlow, HuggingFace, TorchHub) are reported.}
  %\vspace{-0.2cm}
  \label{tab:main}
\end{table*}

\begin{table}[t]
  \centering
  \begin{adjustbox}{width=\linewidth}
  \begin{tabular}{@{}lccc@{}}
    \toprule
    Method & DSF-CoT & DSF-CoN & \textbf{DSF-CoR} \\
    \midrule
    Reasoning Accuracy ($\uparrow$) & 68.21 & 69.02 & 72.31 \\
    Reasoning Tokens ($\downarrow$) & 90.15 & 143.18 & 8.00\\
    \bottomrule
    
  \end{tabular}
  \end{adjustbox}
  %\vspace{-0.1cm}
  \caption{\textbf{Analyses on reasoning.} Accuracy (\%) and cost (used tokens) for reasoning are on the HotPotQA.}
  \label{tab:analysis}
  \vspace{-0.5cm}
\end{table}

%In specific, we select HotPotQA~\cite{yang2018Hotpotqa} and Gorilla API datasets~\cite{patil2023gorilla}. HotPotQA is the open-domain question-answers based on Wikipedia, mainly focused on common knowledge. We used `HotpotQA fullwiki dev set' which is designed to provide 10 contexts including at least one relevant context for a query. Then, we randomly select $K$ contexts including the relevant contexts. Since this dataset provides no training set sharing the knowledge pool with the test set, we have a pre-trained language model of significant scale generate the triplets of question, answer, reason for domain-specific training. TensorFlow, HuggingFace, and TorchHub of the Gorilla API bench are to measure how to generate the correct, functional, and executable API calls based on the documentation. For each of HuggingFace, TorchHub, and TensorFlow, train and test splits are provided, which share the API pool. Also, following the officially-released code\footnote{https://github.com/ShishirPatil/gorilla}, we select top-$(K-1)$ documents from BM25 and a golden document for a query in both training and testing. For all datasets, $K$ is set to 5.
In specific, we select HotPotQA~\cite{yang2018Hotpotqa} and Gorilla API datasets~\cite{patil2023gorilla}. The HotPotQA is the open-domain question-answers based on Wikipedia, mainly focused on common knowledge. In testing, we used `HotpotQA distractor dev. set' which is designed to provide ten contexts including at least one relevant context for a query. TensorFlow, HuggingFace, and TorchHub of the Gorilla API are to measure how to generate the correct, functional, and executable API calls based on the documentation. For each of HuggingFace, TorchHub, and TensorFlow, train and test splits are provided, which share the API pool. Also, following the officially-released code\footnote{https://github.com/ShishirPatil/gorilla}, we utilized BM25 retriever. 



\noindent \textbf{Evaluation.} We set $K$ as $10$ for the HotPotQA, then all the available irrelevant contexts distract the relevant ones for a query. For the Gorilla API, $K$ is set to $5$. To minimize the influence of the quality of the off-the-shelf retriever, we set up our experiments to include at least one relevant context for each input query. 
For the HotPotQA, we used two standard metrics: Exact Match (EM) and F1 score, following prior work~\cite{chen1704reading,karpukhin2020dense,zhu2021retrieving}. An answer is correct in the EM if its normalized form, based on~\citep{karpukhin2020dense}, matches exactly the ground-truth answer. F1 score calculates token overlap between the prediction and ground truth~\cite{zhu2021retrieving}. For the Gorilla API, following the official benchmark, we perform AST sub-tree matching to identify which API the LLM is calling by matching key arguments, and report AST accuracy.

\noindent \textbf{Baselines.} 
We consider the following baselines for our experiments based on LLaMA3-8B: Naive LLaMA3-8B, Domain-specific fine-tuning (DSF) without reasoning, RAFT (DSF with CoT)~\cite{RAFT}, DSF + CoN (chain-of-note)~\cite{con}. In the DSF baselines, we commonly used the LoRA adapter. And, all the baselines are in the zero-shot setting. In addition, RAFT suggested to alternating two losses of the irrelevant contexts-only and the mixing irrelevant and relevant contexts. For a fair comparison, we tried to find the optimal combination ratio of the two losses for this baseline. 



%$ 90.15, 8.00$



\subsection{Comparative Results}
\vspace{-0.2cm}
We evaluate our CoR and demonstrate the efficacy in Table~\ref{tab:main}. The non-specified pre-trained LLM (LLaMA3-8B) shows severely degraded scores in the API datasets than in the natural questions of HotPotQA, which proves the requirements and importance of domain-specific RAG. The reasoning-based methods, RAFT and CoN, attain better results than DSF. However, in F1 score, the effect of CoT is marginal. Also, although the noting strategy of CoN is tailored for RAG, it sometimes shows lower performance than the straightforward DSF as well as CoT (see TensorFlow and Huggingface results). Whereas, we see that the proposed CoR consistently and significantly outperforms the baselines in all the datasets. It means that learning the complex reasoning can be a burden to the PEFT on the smaller-scale LLM, and thus simply identifying the IDs of the relevant contexts is more beneficial.
We also study the extension of the proposed CoR to domain-agnostic RAG in Appendix. 



\subsection{Analysis}
\noindent \textbf{Reasoning quality.}
In RAG, the reasoning can be utilized to support the answer. Therefore, the quality of reasoning is also substantial, then we quantitatively compare the reasoning of CoT, CoN, and our CoR. We evaluate CoT and CoN using a pre-trained LLM (e.g. GPT) in a massive scale. Since CoT and CoR produce lengthy reasoning that incorporates details of the retrieved contexts which may lead to some errors in detail. Hence, to ensure a fair comparison, we prompt the LLM evaluator to assess whether the reasoning is related with the relevant golden context. Nevertheless, in the top row of Table~\ref{tab:analysis}, the proposed CoR attains clearly higher reasoning accuracy.

\noindent \textbf{Cost in reasoning.} We also evaluate the proposed method in terms of the cost for reasoning. To this end, we compare CoT, CoN, and our CoR according to the number of the tokens used for reasoning. As shown in the bottom row of Table~\ref{tab:analysis}, CoR uses significantly lower tokens for reasoning, which shows the efficiency of the proposed approach.

\noindent \textbf{Importance of correct ranking}: To see this, we obtain the answer giving incorrect ranking for DSF-CoR. Despite domain-specific learning, it yields severely degraded results, 24.20\% EM and 32.34\% F1 score. 





