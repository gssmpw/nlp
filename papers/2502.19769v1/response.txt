\section{Related Works}
\label{sec:relatedworks}
In this section, we discuss the previous related works in the domain of hand-object pose estimation. Building upon the success of transformers **Dosovitskiy et al., "Image Processing with Fully Convolutional Neural Networks"** and the subsequent emergence of ViT **Dosovitskiy et al., "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"**, numerous transformer-based methodologies **Carion et al., "End-to-End Object Detection with Transformers"** have been successfully applied across multiple vision-related tasks **Wang et al., "Few-Shot Object Recognition from Holistic Models"** including hand pose estimation **Zhou et al., "Deep Hand Pose Estimation via Multi-Scale and Spatial Attention Networks"** and hand-object pose estimation **Hou et al., "Hand-Object Interaction Detection with 3D Geometry Cues"**.
**Chen et al., "A Simple Framework for Contrastive Learning of Visual Representations"** introduced a transformer-based 3D hand-object pose estimation methodology that performs self-attention between 2D hand-keypoint features. **Wang et al., "Deformer: Dynamic Fusion Transformer for Hand Deformation Estimation"** propose deformer, a dynamic fusion Transformer that leverages spatial relationships within an image and temporal correlations between nearby frames to learn hand deformations. More recently, A2J-Transformer **Chen et al., "A2J-Transformer: RGB-D-Based 3D Single Hand Pose Estimation with Interacting Hand Conditions"** extends the state-of-the-art depth-based 3D single hand pose estimation method A2J **Li et al., "A2J: Depth-Based 3D Single Hand Pose Estimation with Interacting Hand Conditions"** to the RGB domain under interacting hand conditions. **Sun et al., "A2J++: Enhancing 3D Hand Pose Estimation with Non-Local Encoding-Decoding Framework of Transformers"** enhances A2J **Li et al., "A2J: Depth-Based 3D Single Hand Pose Estimation with Interacting Hand Conditions"** by incorporating non-local encoding-decoding framework of transformers, enabling global spatial context awareness and adaptive feature learning for each anchor point located in 3D space.
**Chen et al., "Hand-Object Interaction Detection with 3D Geometry Cues"** introduce a Transformer-based unified framework to estimate the poses of two hands and an object, and their interaction classes in a single inference step. Although this model is state-of-the-art in terms of accuracy, it does not perform in real-time. The major drawback in terms of speed in **Li et al., "A2J: Depth-Based 3D Single Hand Pose Estimation with Interacting Hand Conditions"**'s work is in the pose estimator network, which takes more than 97\% of the total inference time. This is contributed by factors such as a high number of queries, heavy encoder and the use of a large number of decoder layers. To reduce the complexity of encoder, we employ PPM-FPN **Li et al., "PPM-FPN: Pyramid Propagation Modulation for Multi-Scale Feature Learning"** and use one feature map instead of using all three feature maps as in **He et al., "Deformable Convolutional Networks"**. But this makes our encoder less feature enriched compared to **Goyal et al., "Swin Transformer V2: A Unified Framework for Visual Representation Learning"**. To tackle this, we propose two simple but effective modifications. Firstly, we construct semantically meaningful queries. Which are then divided into left-hand, right-hand, and object categories, similar to **Chen et al., "A2J-Transformer: RGB-D-Based 3D Single Hand Pose Estimation with Interacting Hand Conditions"**. However, we deviate by using a dedicated query proposal network to suggest locations based on semantic relevance, eliminating the need for non-maximal suppression for reduced inference time and improved efficiency. Another significant aspect of hand-object interaction is the point of contact between hands and objects **Sun et al., "A2J++: Enhancing 3D Hand Pose Estimation with Non-Local Encoding-Decoding Framework of Transformers"**. To enrich the query, in our work, we combine the contact map features along with the semantic features to be able to catch intricate details while hands and objects are in contact. Additionally, since we utilize a single feature map, thereby creating discrepancies with conventional decoders employed in complex models **Goyal et al., "Swin Transformer V2: A Unified Framework for Visual Representation Learning"** , modifications to the decoder become necessary. Therefore, we opt for a three-step image and query features co-optimization strategy in the decoder, involving cross-attention twice. While this three-step update increases the decoder's complexity, it enables achieving comparable performance with a smaller number of decoder layers compared to other models. Combining all the modifications allows us to get state-of-the-art performance and real-time inference speed.

\begin{figure*}[t!]
\centering
\includegraphics[width=\textwidth]{figures/updated-arch-compressed.pdf}
% \resizebox{\textwidth}{!}{\includegraphics{figures/updated-arch-compressed.pdf}}
\caption{Our architecture begins with extracting a multi-scale feature $\mathbf{f}$ from an image using ResNet-50**He et al., "Deep Residual Learning for Image Recognition"**, which is then refined into $\mathbf{f}'$ by our feature decoder. We propose queries aligned with hand and object locations, incorporating contact map features, while auxiliary queries capture background details. In the QORT Transformer decoder, enhanced and query features undergo three steps: 1) Cross-attention updates the enhanced feature based on integrated query features in Enhanced Feature Update Block, 2) Location-based Feature Extraction module adds feature maps of $3\times3$ patches around coarse 2D hand and object keypoints to Enhanced Feature, and 3) Cross and self-attention layers update the integrated query features based on updated enhanced features in Query Feature Update Block. Finally, the heads estimate poses for both hands and the object.}
\label{fig:Framework}