\section{Related Works}
\label{sec:relatedworks}
In this section, we discuss the previous related works in the domain of hand-object pose estimation. Building upon the success of transformers \cite{vaswani2017attention} and the subsequent emergence of ViT \cite{dosovitskiy2020image}, numerous transformer-based methodologies \cite{carion2020end, wang2022anchor, yao2021efficient, li2022dn, liu2022dab, cha2024text2hoi} have been successfully applied across multiple vision-related tasks \cite{han2022survey}, including hand pose estimation \cite{huang2020hand, Jiang_2023_a2j-tran, Fu_2023_ICCV, Zhang_2024_WACV, pavlakos2024reconstructing} and hand-object pose estimation \cite{hampali2022keypoint,liu2021semi, cho2023transformer}.
\citeauthor{hampali2022keypoint} introduced a transformer-based 3D hand-object pose estimation methodology that performs self-attention between 2D hand-keypoint features. \citeauthor{Fu_2023_ICCV} propose deformer, a dynamic fusion Transformer that leverages spatial relationships within an image and temporal correlations between nearby frames to learn hand deformations. More recently, A2J-Transformer \cite{Jiang_2023_a2j-tran} extends the state-of-the-art depth-based 3D single hand pose estimation method A2J \cite{Xiong_2019_a2j} to the RGB domain under interacting hand conditions. \cite{Jiang_2023_a2j-tran} enhances A2J \cite{Xiong_2019_a2j} by incorporating non-local encoding-decoding framework of transformers, enabling global spatial context awareness and adaptive feature learning for each anchor point located in 3D space.
\citeauthor{cho2023transformer} introduce a Transformer-based unified framework to estimate the poses of two hands and an object, and their interaction classes in a single inference step. Although this model is state-of-the-art in terms of accuracy, it does not perform in real-time. The major drawback in terms of speed in \citeauthor{cho2023transformer}'s work is in the pose estimator network, which takes more than 97\% of the total inference time. This is contributed by factors such as a high number of queries, heavy encoder and the use of a large number of decoder layers. To reduce the complexity of encoder, we employ PPM-FPN \cite{cheng2022sparse} and use one feature map instead of using all three feature maps as in \cite{cho2023transformer}. But this makes our encoder less feature enriched compared to \cite{cho2023transformer}. To tackle this, we propose two simple but effective modifications. Firstly, we construct semantically meaningful queries. Which are then divided into left-hand, right-hand, and object categories, similar to \cite{hampali2022keypoint}. However, we deviate by using a dedicated query proposal network to suggest locations based on semantic relevance, eliminating the need for non-maximal suppression for reduced inference time and improved efficiency. Another significant aspect of hand-object interaction is the point of contact between hands and objects \cite{karunratanakul2020grasping, yang2021cpf}. To enrich the query, in our work, we combine the contact map features along with the semantic features to be able to catch intricate details while hands and objects are in contact. Additionally, since we utilize a single feature map, thereby creating discrepancies with conventional decoders employed in complex models \cite{cho2023transformer, Cheng_2022_CVPR}, modifications to the decoder become necessary. Therefore, we opt for a three-step image and query features co-optimization strategy in the decoder, involving cross-attention twice. While this three-step update increases the decoder's complexity, it enables achieving comparable performance with a smaller number of decoder layers compared to other models. Combining all the modifications allows us to get state-of-the-art performance and real-time inference speed. 
\begin{figure*}[t!]
\centering
\includegraphics[width=\textwidth]{figures/updated-arch-compressed.pdf}
% \resizebox{\textwidth}{!}{\includegraphics{figures/updated-arch-compressed.pdf}}
\caption{Our architecture begins with extracting a multi-scale feature $\mathbf{f}$ from an image using ResNet-50~\cite{he2016deep}, which is then refined into $\mathbf{f}'$ by our feature decoder. We propose queries aligned with hand and object locations, incorporating contact map features, while auxiliary queries capture background details. In the QORT Transformer decoder, enhanced and query features undergo three steps: 1) Cross-attention updates the enhanced feature based on integrated query features in Enhanced Feature Update Block, 2) Location-based Feature Extraction module adds feature maps of $3\times3$ patches around coarse 2D hand and object keypoints to Enhanced Feature, and 3) Cross and self-attention layers update the integrated query features based on updated enhanced features in Query Feature Update Block. Finally, the heads estimate poses for both hands and the object.}
\label{fig:Framework}
\end{figure*}