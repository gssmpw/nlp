\section{Appendix}

\subsection{Data Preprocessing}\label{data_preprocess}
The requirement of two or more turns helped ensure we had sufficient signal to assess the level of expertise of both user and \abr{LLM}. Additionally, the SAT score classifier \cite{lin-etal-2024-interpretable} is also based on multi-turn conversations. We then use the \texttt{langdetect}\footnote{https://pypi.org/project/langdetect/} library \cite{nakatani2010langdetect} to determine the text language at each interaction turn to further filter to conversations where the majority language detected overall was either English, or in case of a tie, English was one of the tied languages. The selection of English only conversations was done to ensure human judges could read a random sub-sample of conversations in order to human-validate our classifications of user and \abr{LLM} expertise.

Using the domain classification methodology introduced in \citet{suri2024usegenerativesearchengines}, we further sampled the conversations at random from a set of topical domains to generate a final set of $25033$ fully anonymized conversations. All personal, private, or sensitive information was scrubbed and masked before the conversations were used for this research. The access to the dataset is strictly limited to the authors who conducted hands-on analysis. This domain filtering step was performed to remove conversations in domains minimally or not at all relevant to the concept of expertise (e.g., travel).

\paragraph{Ethics:}~As part of the production process\footnote{Our use of Copilot conversations falls within the Terms of Use outlined at \url{https://www.bing.com/new/termsofuse}}, the Bing Copilot data is anonymized, and each conversation is formed by aggregating turns based on a unique conversation \abr{ID}. Thus, none of the researchers who analyzed the data are able to recover and identify the conversations from any individual user. In addition, this research study was reviewed and approved by representatives from our institutional review board (\abr{IRB}), as well as our ethics and security teams. No formal \abr{IRB} certificate was required since we did not conduct any human studies for this work.

\subsection{Details on Computational Experiments}
GPT-4 was run on a CPU machine and was allocated three hours to run all experiments with temperature 0. We did not perform a hyper-parameter search. All results are obtained from a single run. For detecting the conversation language, a V100 GPU was used for a total of 1 hour.

\subsection{Usage of AI Assistants}
Our expertise classifier is based on GPT-4. We only use AI Assistants to assist our writing to identify grammar errors, typos and rephrase terms for readability.

\input{tables/domain_counts}

\input{tables/expertise_distributions}

\input{tables/sat_domains}

\subsection{Reproducibility}\label{wildchat}
Owing to privacy concerns, the Copilot conversations used in our study cannot be made public. However, to ensure the generalisability of our expertise classifier, we rerun our expertise classifier on a randomly sampled set of $6400$ conversations from the WildChat dataset \cite{zhao2024wildchat1mchatgptinteraction}. We observe a similar distribution of user, gauged user and LLM expertise labels on this sampled set of conversations. However, it is important to note that, conversations in the WildChat dataset reflect a technological bias, since the service was hosted on Hugging Face Spaces, predominantly attracting developers or individuals connected to the IT domain. As such, it may not fully represent the diversity of conversations that naturally occur with chat-based models. In contrast, Copilot conversations span $25$ topically diverse domains and includes a more representative sample of conversations from the general population.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.9\columnwidth]{figures/gauged_expertise_stacked.pdf}
    \caption{Barplot showing the distribution of Gauged User Expertise on different domains of Copilot conversations.}\label{fig:gauged_user_dist}
\end{figure}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/heatmap_fill_size_new.pdf}
    \caption{Heatmaps between User and AI expertise (left) and User and Gauged User expertise (right).}\label{heatmap_size}
\end{figure*}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/heatmap_fill_SAT_median_new.pdf}
    \caption{Heatmaps between User and AI expertise (left) and User and Gauged User expertise (right) with density as median \abr{SAT} scores.}\label{heatmap_sat}
\end{figure*}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/heatmap_fill_turnwords_mean_new.pdf}
    \caption{Heatmaps between User and AI expertise (left) and User and Gauged User expertise (right) with density as median number of words in user turns from a conversation.}\label{heatmap_turn_words}
\end{figure*}

\begin{figure*}[t!]
    \centering
    \begin{subfigure}[b]{0.49\linewidth}
        \includegraphics[width=\linewidth]{figures/binary_complexity_user_ai_new.pdf}
        \caption{}
        \label{fig:binary_comp_user_ai}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\linewidth}
        \includegraphics[width=\linewidth]{figures/binary_complexity_user_gauged_new.pdf}
        \caption{}
        \label{fig:binary_comp_gauged}
    \end{subfigure}
    \caption{Regression plots showing the correlation between Expertise Difference and \abr{SAT} scores for low and high complexity tasks.}
\end{figure*}

\subsection{Prompts}\label{prompt_details}
We release the prompts used for expertise classification. All the prompts were human validated before being used for classification. For human validation of the measured expertise labels, three of the authors independently labeled both the user and \abr{LLM} portions of $50$ Bing Copilot conversations each (total of $300$ assessments: the user and the \abr{LLM} side of $150$ conversations) as "Beginner", "Intermediate", or "Expert". We then compared these labels to a corresponding 3-class version of the final classifier, noting $70\%$ or greater agreement in all cases. The 3-class classifier was used for human validation, as discriminating among the five classes in the full classifier was very challenging for the human labelers.

\clearpage

\begin{figure*}
    \centering
    \fbox{\includegraphics[width=0.75\linewidth]{Expertise_Prompts/User_Expertise.pdf}}
    \caption{Prompt for classifying the User Expertise based on the user only turns of Copilot conversations.}\label{fig:user_prompt}
\end{figure*}

\begin{figure*}
    \centering
    \fbox{\includegraphics[width=0.75\linewidth]{Expertise_Prompts/AI_Expertise.pdf}}
    \caption{Prompt for classifying the AI Expertise based on the AI only turns of Copilot conversations.}\label{fig:ai_prompt}
\end{figure*}

\begin{figure*}
    \centering
    \fbox{\includegraphics[width=0.75\linewidth]{Expertise_Prompts/Gauged_User_Expertise.pdf}}
    \caption{Prompt for predicting the Gauged User Expertise based on the AI only turns of Copilot conversations.}\label{fig:gauged_prompt}
\end{figure*}

