\begin{abstract}
%With the advent of conversational Large Language Models (\abr{LLMs}) like ChatGPT and Bing Copilot, there has been an increase in the different domains, types and complexities of tasks that users do with these systems, ranging from a simple quick lookup of information to creating something entirely new. These advances also necessitate research into whether these models are aligned with human values or not. Not all users can digest the same information. Subject matter experts can probably digest complex information much more easily than a beginner or a novice, who might experience a higher cognitive load or get over-whelmed. At the same time, presenting too basic information to proficient or expert users can cause them to have a dis-satisfactory experience with the agent. In this work, studying over $25K$ Bing Copilot conversations sampled from the month of June 2024, we study how the agent responds to users of varying expertise and it's impact on user satisfaction along multiple dimensions. Our findings show that mis-alignment between user and \abr{AI} expertise has a negative impact on the overall user satisfaction score, with the impact more profound for more complex tasks. We also show that when the \abr{AI} is proficient or expert, it leads to more engagement with the model. Our work highlights an important missing aspect of human-centered \abr{LLMs}, .... \shramay{need to add one more sentence here.}

%Using a sample of $25000$ Bing Copilot conversations, we study how the agent responds to users of varying levels of domain expertise and the resulting impact on user experience along multiple dimensions. Our findings show that, across a variety of topical domains, misalignment between user and agent expertise, such that the agent responds at a level of expertise below that of the user, has a negative impact on overall user satisfaction, with the impact more profound for more complex tasks. We also show that when the agent responds to the user at proficient or expert levels of expertise ($77\%$ of conversations), it leads to more user engagement with the agent from higher expertise users, while conversely lower expertise users engage equally or even more with lower expertise agent responses. Our findings underscore the importance of alignment between user and \abr{AI} when designing human-centered AI systems, to ensure satisfactory and productive interactions.

Using a sample of $25,000$ Bing Copilot conversations, we study how the agent responds to users of varying levels of domain expertise and the resulting impact on user experience along multiple dimensions. Our findings show that across a variety of topical domains, the agent largely responds at proficient or expert levels of expertise (77\% of conversations) which correlates with positive user experience regardless of the user's level of expertise. Misalignment, such that the agent responds at a level of expertise below that of the user, has a negative impact on overall user experience, with the impact more profound for more complex tasks. We also show that users engage more, as measured by the number of words in the conversation, when the agent responds at a level of expertise commensurate with that of the user. Our findings underscore the importance of alignment between user and \abr{AI} when designing human-centered \abr{AI} systems, to ensure satisfactory and productive interactions.
\end{abstract}

% Supplementary materials include human-validated prompt-based classifiers for both user and agent level of expertise.