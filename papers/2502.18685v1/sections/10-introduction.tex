\section{Introduction}
%We have seen significant advancements in \abr{LLM} development, which has enhanced the capabilities of model-based agents like ChatGPT, Bing Copilot, and Google Gemini, to allow them to excel on tasks like image generation, code generation, and mathematical reasoning. These capabilities are not only attested to by their performance on standardized benchmarks but are also reflected in their use across a diverse set of real-world domains \cite{suri2024usegenerativesearchengines}. Recently developed \abr{LLMs} are also able to assist humans across a variety of fields like teaching \cite{wang2024tutorcopilothumanaiapproach, ALSAFARI2024100101} and the clinical domain \cite{han2024ascleai}.

%Moreover, they have also led to an increase in user productivity \cite{peng2023productivitygithub, cambon2023early}, with millions of users relying on them for a variety of tasks ranging from quick information retrieval to more creative or technical pursuits such as drafting essays, writing code, designing artworks, and solving mathematical problems. Their growing integration across platforms like GitHub and Microsoft Office products (via Copilot) and Google (via Gemini) has also made them readily available to users in their everyday tasks.

We have seen significant advancements in \abr{LLM} development, which has enhanced the capabilities of model-based agents like ChatGPT that allow them to excel on tasks ranging from quick information retrieval to more creative or technical pursuits such as drafting essays, writing code, and designing artworks. These capabilities are not only attested to by their performance on standardized benchmarks but are also reflected in their use across a diverse set of real-world domains \cite{suri2024usegenerativesearchengines}. Recently developed \abr{LLMs} are able to assist humans across a variety of fields like teaching \cite{wang2024tutorcopilothumanaiapproach, ALSAFARI2024100101} and the clinical domain \cite{han2024ascleai}, and have led to an increase in user productivity \cite{peng2023productivitygithub, cambon2023early}.

\begin{figure}[t!]
    \centering    
    \includegraphics[width=\columnwidth]{figures/Teaser_Figure.pdf}
    \caption{An overview of our expertise classifier pipeline.}
    \label{fig:example}
\end{figure}

However, while millions of people utilize these models for a variety of tasks, their expectations, backgrounds, and interactions with these tools can differ significantly. One key aspect where users might differ is their domain expertise in the conversation topic with the agent \abr{LLM}\footnote{Here on called \abr{LLM}.}. Not all end users share the same level of domain knowledge and thus may have different preferences and abilities to process the information that the model would return to them. A ``beginner'' user might want simple, general purpose information on a topic and could possibly be overwhelmed if presented with too ``high level'' information. On the other hand, a domain ``expert'' might have an unsatisfactory experience with the \abr{LLM} if not given a deeper and detailed response.% Understanding and addressing the potential misalignment between user expertise and the model’s responses becomes vital to enhancing user satisfaction and ensuring that \abr{LLMs} are capable of meeting diverse user needs.

Thus, we ask: \textit{What is the ideal expertise level of the LLM, and what are the consequences of any misalignment between the user and the LLM on domain expertise?}

To answer this, we develop an ordinal 5-point scale-based expertise classifier (shown in \autoref{fig:example}) that we apply to a corpus of over $25,000$ Bing Copilot conversations sampled across a variety of domains. We generate three measures of expertise for each conversation. First and second, we classify the level of expertise of the user and the \abr{LLM} respectively in the topic of the conversation. Third, we classify the gauged expertise of the user, defined as the judged level of expertise of the user based on the responses made by the \abr{LLM}. We show similarities and differences in these three types of expertise labels within the same Copilot conversation to identify cases where the \abr{LLM} is aligned or misaligned with the user. We then assess the impact of (mis)alignment on three measures of user interaction experience: user satisfaction, level of engagement, and complexity of task.

%We then assess the impact of (mis)alignment on three measures of user interaction experience: user satisfaction, level of engagement, and complexity of task. We show that users have an unsatisfactory experience (as measured using methodology discussed in~\cref{user_exp_metrics}) when underestimated by the \abr{LLM} or when the \abr{LLM} responds at a low expertise level, with the impact more profound for more complex tasks. We also show that more expert users engage more with the \abr{LLM} when the \abr{LLM} responds at a higher degree of expertise, while more novice users tend to engage more when the \abr{LLM} responds at a lower degree of expertise. Broadly these findings underscore the importance to human-centered \abr{NLP} systems of aligning the \abr{LLM} with dimensions of the user that are critical to the interaction. %In this case, better "alignment" in which the \abr{LLM} maintains a relatively high degree of expertise, particularly in relation to the user's expertise and task complexity, helped foster more satisfying and engaging interactions.%These findings underscore the importance of aligning the model’s expertise with that of the user. Ensuring such alignment is critical for creating human-centered NLP systems that adapt to users' needs, fostering more satisfying and productive interactions.

%To summarize, our contributions are:
%\begin{itemize}[nosep]
%    \item We introduce an ordinal 5-point scale-based expertise classifier to apply %to user-\abr{LLM} interactions.
%    \item We characterize the distribution of \abr{LLM} responses with respect to domain expertise (77\% proficient or higher, 23\% intermediate or lower).% observe that, surprisingly, in over $20\%$ of the cases, the \abr{LLM} is not ``proficient'' or ``expert''. 
%    \item We quantify the impact on user satisfaction when the \abr{LLM} responds at an expertise level below that of the user or underestimates the user's level of expertise. 
%    \item We show that users engage more with the \abr{LLM} when it responds at a level of expertise commensurate to that of the user.
%\end{itemize}

% The ``WHAT'':
% \begin{itemize}
%     \item Expertise classification of users from CoPilot Conversations
%     \item Is this 'User Expertise' aligned or calibrated with what the \abr{LLM} ``thinks''
%     \item If there is (mis)alignment, does it impact the user experience in any way?
%     \item How can we measure this impact?
% \end{itemize}

% The ``WHY'':
% \begin{itemize}
%     \item Human-centered \abr{LLMs} are ideal.
%     \item Expert users are more capable at handling higher cognitive loads than novice and beginners. They might have a bad experience if they are underestimated $\rightarrow$ might be dissatisfied.
%     \item Novice and beginner users could possibly get overwhelmed if the information provided is too ``expert'' level.
%     \item There is also a potential societal impact. Prior research has shown that productivity increases the most for ``Intermediate'' users. 
% \end{itemize}

% The ``How'':
% \begin{outline}
%     \1 Develop a 5-point scale based Zero Shot expertise classifier.
%     \1 Scale $\rightarrow$ Novice, Beginner, Intermediate, Proficient, Expert
%     \1 Scale is ordinal.
%     \1 Three different types of expertise classification:
%       \2 User Expertise: Based on User Turns and Full Conversation.
%       \2 User Gauged Expertise: Based on AI turns. The LLM is asked to predict what level of user is the AI response geared towards. 
%       \2 AI Expertise: Based on AI turns.    
% \end{outline}

% The ``WHAT TO DO WITH THIS'':
% \begin{outline}
%     \1 User - AI Agent Expertise Heatmap and User - Gauged User Expertise Heatmap to show misalignment. 
%     \1  User - AI Agent Expertise Heatmap and User - Gauged User Expertise Heatmap with SAT as density to show impact
%     \1 Show overall distribution graphs as well.
%     \1 Misalignment as $f(SAT)$
%     \1 Correlation between (User, AI) Expertise Difference or (User, Gauged User) Expertise Difference and:
%         \2 SAT Scores
%         \2 Task Complexity
%         \2 User Turn Word Count heatmap: Expert and Proficient AI causes 10-16x more words from the user.
% \end{outline}

% ``Some Insights'':
% \begin{itemize}
%     \item For lower expertise users, overestimation $\rightarrow$ positive SAT.
%     \item Higher complexity tasks hurt the most when \big\downarrow AI expertise or \big\uparrow underestimation.
%     \item Important for AI to be Proficient or Expert. Huge impact on SAT.
%     \item Misalignment between user and AI $\rightarrow$ hurts SAT.
%     \item 1 in 5 times, the AI is not proficient or expert.
% \end{itemize}
