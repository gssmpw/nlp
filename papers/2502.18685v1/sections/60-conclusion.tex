\section{Conclusion}
% We examined the alignment between the user and the AI with respect to domain expertise. Implicitly we are asking whether there is an optimal alignment. Perhaps the user and the model should operate at the same level of expertise so that the LLM can convey ideas in a manner most likely to be understood by the user. Or maybe the LLM should be more expert than the user, but only by an amount such that the LLM responses to user queries are both understandable and novel to the user.
% Using a random sample of real-world interactions with an LLM-based chat agent, we find that the user is lower in expertise than the LLM in the majority ($80.05\%$) of conversations. This makes sense in that most users are likely to be non-experts, while the model should be more expert in order to provide value to the user. Our results show that generally it is preferable for the LLM to be quite expert regardless of the user's level of expertise, with lower than proficient responses from the LLM generally leading to decreased user satisfaction scores. The one exception may be that lower expertise users engage the LLM in conversations of equal or even greater length when the LLM is of lower expertise.
% We also saw $4.23\%$ of conversations where the user was underestimated, which had a significant impact on the user experience: satisfaction, engagement, and the complexity of the interaction were all lower when the level of gauged expertise of the user was lower than the actual user expertise. Given a goal of enabling rich, complex experiences, and the already widespread and growing use of LLM systems, this highlights the importance of the LLM always avoiding underestimating users, whether through the system prompt, or through in-session adaptation.
We examined the alignment between \abr{LLMs} and users along a dimension relevant to the user experience: expertise. We show that the \abr{LLM}'s expertise is largely proficient or expert, which correlated with positive user satisfaction and exceeded user expertise in a majority of the cases. Further, underestimating the user's level of expertise correlated with lower and even negative user satisfaction, with the effect stronger for more complex tasks. Users tended to engage more, however, when the \abr{LLM} responded at a level of expertise similar to their own, suggesting that the system strike a balance between generally high expertise which is liked by all users and matched expertise to best engage users. Future work may explore intervention strategies to strike this balance and mitigate obvious cases of user underestimation in real time.
%Future work may explore intervention strategies to mitigate cases of expertise mismatch between users and \abr{LLMs} and user underestimation to ensure rich, complex and satisfactory user experiences. By ensuring that \abr{LLMs} better understand human intent and provide contextually  and expertise appropriate responses, we can expect these systems to evolve into even more capable and user-friendly assistants, facilitating everything from decision-making processes to knowledge creation. Future work may explore intervention strategies to mitigate cases of expertise mismatch between users and \abr{LLMs} and user underestimation to ensure rich, complex and satisfactory user experiences.