\section{Methodology}

Consider a conversation $C_{i}$ from a corpus of conversations $C$ consisting of $t$ interaction turns of user-agent utterances $C_i = [U_1, A_1, ..., U_t, A_t]$\footnote{Here $U_t$ refers to a \textit{user} utterance and $A_t$ refers to an \textit{AI Agent} utterance.}. We take a random sample of such conversations from Bing Copilot from the month of June 2024 with at least $t \geq 2$ interaction turns for both the user and the agent, yielding a set of $677,801$ conversations. Using the preprocessing steps discussed in Appendix \ref{data_preprocess}, we get a final set of $25,033$ conversations. We present the distribution of the conversations across different domains in \autoref{tab:domain_counts}.
% The requirement of two or more turns helped ensure we had sufficient signal to assess the level of expertise of both user and \abr{LLM}. We then use the \texttt{langdetect}\footnote{https://pypi.org/project/langdetect/} library \cite{nakatani2010langdetect} to determine the text language at each interaction turn to further filter to conversations where the majority language detected overall was either English, or in case of a tie, English was one of the tied languages. The selection of English only conversations was done to ensure human judges could read a random sub-sample of conversations in order to human-validate our classifications of user and \abr{LLM} expertise.

% Using the domain classification methodology introduced in \citet{suri2024usegenerativesearchengines}, we further sampled the conversations at random from a set of topical domains to generate a final set of $25033$ fully anonymized conversations.\footnote{All personal, private, or sensitive information was scrubbed and masked before the conversations were used for this research. The access to the dataset is strictly limited to the authors who conducted hands-on analysis.} This domain filtering step was performed to remove conversations in domains minimally or not at all relevant to the concept of expertise (e.g., shopping). We present the distribution of the conversations across different domains in \autoref{tab:domain_counts}.

\begin{figure*}[ht!]
    \centering
    \begin{subfigure}[b]{0.49\linewidth}
        \includegraphics[width=\linewidth]{figures/user_expertise_stacked.pdf}
        \caption{}
        \label{fig:user_dist}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\linewidth}
        \includegraphics[width=\linewidth]{figures/ai_expertise_stacked.pdf}
        \caption{}
        \label{fig:ai_dist}
    \end{subfigure}
    \caption{Barplots showing the distribution of User Expertise (left) and Agent Expertise (right) on different domains of Copilot conversations.}
\end{figure*}

\subsection{Expertise Labels}\label{exp_label}
We expect that users of varying expertise levels interact with Bing Copilot, and thus in order to determine the alignment between the user and the \abr{LLM} expertise, we compute three different types of expertise labels as follows: \\
\textbf{User Expertise}: Expertise of the user in the conversation domain based on the User only side ($U_t$) of a conversation $C_{i}$. \\
\textbf{Gauged User Expertise}: Predicted expertise of the user in the conversation domain, based on the \abr{LLM} ($A_t$) only side of a conversation $C_{i}$. \\
\textbf{Agent Expertise}: Expertise of the agent in the conversation domain, based on the \abr{LLM} ($A_t$) only side of a conversation $C_{i}$.

We prompt \texttt{GPT-4-Turbo} \cite{openai2023gpt4} to compute these three types of expertise labels for each conversation using a $5$-point ordinal scale as ``Novice'', ``Beginner'', ``Intermediate'', ``Proficient'', and ``Expert''. We present the definitions of each of these labels along with the human-validated system prompts in \autoref{fig:user_prompt}, \autoref{fig:ai_prompt} and \autoref{fig:gauged_prompt}. We also provide details on the human validation of the predicted expertise labels in Appendix \ref{prompt_details}.

\subsection{Metrics for User Experience}\label{user_exp_metrics}
We use the following three metrics to understand the impact of expertise (mis)alignment on user experience:
\begin{itemize}[nosep]
    \item SAT Score:~\citet{lin-etal-2024-interpretable} introduced \abr{SPUR} (\textbf{S}upervised \textbf{P}rompting for \textbf{U}ser satisfaction \textbf{R}ubrics), an iterative prompting framework using supervision from labeled examples to estimate the user satisfaction score from a multi-turn conversation with an \abr{LLM} agent. We adopt this framework and define the overall satisfaction score, denoted as \abr{SAT}, as the difference between the satisfaction and dissatisfaction scores\footnote{\abr{SPUR} enables the computation of both satisfaction and dissatisfaction scores.}. The \abr{SAT} score ranges from $-100$ to $100$. The SAT score rubric is human-validated, the details of which are mentioned in ~\citet{lin-etal-2024-interpretable}.
    \item Task Complexity: \citet{suri2024usegenerativesearchengines} introduced a task complexity classification method based on Anderson and Krathwohlâ€™s Taxonomy of learning domains \cite{armstrong2010bloom} which categorizes the task complexity into six levels from lowest complexity to highest: \textit{Remember, Understand, Apply, Analyze, Evaluate}, and \textit{Create}. For simplicity, we group \textit{Remember} and  \textit{Understand} as \texttt{Low Complexity} and \textit{Apply, Analyze, Evaluate} and \textit{Create} as \texttt{High Complexity} tasks. The Task Complexity metric is human-validated, the details of which are mentioned in \citet{suri2024usegenerativesearchengines}. 
    \item Conversation Length: As all our conversations are multi-turn, we look at the number of words across all user turns as a proxy for the user engagement level.
\end{itemize}

