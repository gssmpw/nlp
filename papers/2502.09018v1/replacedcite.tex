\section{Related Work}
CBMs____ are inherently interpretable deep neural network models that predict concept labels and then predict final class labels from the predicted concepts.
In contrast to the other explanation styles such as post-hoc attribution heatmaps____, CBMs provide semantic ingredients consisting the final label prediction through the bilevel prediction of input-to-concept and concept-to-label.
The original CBMs have the challenge of requiring human annotations of concept labels, which are more difficult to obtain than target task labels.
Another challenge is the performance degradation from backbone black-box models____ due to the difficulty of learning long-tailed concept distributions____.
Post-hoc CBMs____, Label-free CBMs____, and LaBo____ addressed these challenges by automatically collecting concepts corresponding to target task labels by querying LLMs (e.g., GPT-3____) and leveraging multi-modal feature spaces of pre-trained VLMs (e.g., CLIP____) for learning the input-to-concept mapping.
Subsequently, the successor works have basically assumed the use of LLMs or VLMs, further advancing CBMs____.
In particular, ____ and ____ are related to our work in terms of using sparse modeling to select concepts for input images.
However, all of these existing CBMs still require training specialized neural networks on target datasets, incurring additional target data collection and training resources.
Handling the bi-level prediction in a zero-shot manner for unseen input is unobvious because it can not be solved by na\"ive application of the existing zero-shot classification methods, which depend on task-specific vocabularies for single label predictions____.
Furthermore, current CBMs and the recent interpretable framework for CLIP____ limit the number of concepts up to a few thousand due to training and computational constraints, restricting the generality.
In contrast to the previous CBMs, our Z-CBMs can perform fully zero-shot inference based on a large-scale concept bank with millions of vocabulary for arbitrary input images in various domains as shown in the experiments in Sec.~\ref{sec:ex_multi_dataset}.\looseness-1