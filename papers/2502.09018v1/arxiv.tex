%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass[x11names]{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% User Defined
\usepackage{graphicx} % DO NOT CHANGE THIS
\usepackage{booktabs}       % professional-quality tables
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{pifont}% http://ctan.org/pkg/pifont
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{colortbl}
\usepackage{multirow}
\usepackage{rotating}
\usepackage{ulem}
\usepackage{subcaption}


\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\newcommand{\retop}[3]{\underset{#1 \in #2}{\operatorname{Ret}_{#3}}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newif\ifproofread
\newcommand{\rev}[1]{%
\ifproofread
\textcolor{red}{#1}%
\else
#1%
\fi
}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Zero-shot Concept Bottleneck Models}

\begin{document}

\twocolumn[
\icmltitle{Zero-shot Concept Bottleneck Models}
% \\ via Sparse Regression of Retrieved Concepts}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
% \icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Shin'ya Yamaguchi}{ntt,kyoto}
\icmlauthor{Kosuke Nishida}{ntt}
\icmlauthor{Daiki Chijiwa}{ntt}
\icmlauthor{Yasutoshi Ida}{ntt}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{ntt}{NTT}
\icmlaffiliation{kyoto}{Kyoto University}

\icmlcorrespondingauthor{Shin'ya Yamaguchi}{shinya.yamaguchi@ntt.com}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
% \icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
    Concept bottleneck models (CBMs) are inherently interpretable and intervenable neural network models, which explain their final label prediction by the intermediate prediction of high-level semantic \textit{concepts}.
    However, they require target task training to learn input-to-concept and concept-to-label mappings, incurring target dataset collections and training resources.
    In this paper, we present \textit{zero-shot concept bottleneck models} (Z-CBMs), which predict concepts and labels in a fully zero-shot manner without training neural networks.
    Z-CBMs utilize a large-scale concept bank, which is composed of millions of vocabulary extracted from the web, to describe arbitrary input in various domains.
    For the input-to-concept mapping, we introduce \textit{concept retrieval}, which dynamically finds input-related concepts by the cross-modal search on the concept bank.
    In the concept-to-label inference, we apply \textit{concept regression} to select essential concepts from the retrieved concepts by sparse linear regression.
    Through extensive experiments, we confirm that our Z-CBMs provide interpretable and intervenable concepts without any additional training.
    Code will be available at \url{https://github.com/yshinya6/zcbm}.
\end{abstract}

\proofreadtrue

\begin{figure*}[t]
    \vspace{-3mm}
    \centering
    \includegraphics[width=0.9\linewidth]{fig/top.pdf}
    \vspace{-3mm}
    \caption{
    Zero-shot concept bottleneck models (Z-CBMs). Z-CBMs predict concepts for input by retrieving them from a large-scale concept bank. Then, Z-CBMs predict labels based on the weighted sum of the retrieved concept vectors with importance weights yielded by sparse linear regression.
    }
    \label{fig:top}
    \vspace{-3mm}
\end{figure*}

\section{Introduction}\label{sec:introduction}
One of the primary interests of the deep learning research community is developing a human-interpretable model.
Concept bottleneck model (CBM, \citet{Koh_ICML20_concept_bottleneck}) is an inherently interpretable neural network model, which aims to explain their final prediction via the intermediate \textit{concept} predictions.
CBMs are trained on a target task to learn the input-to-concept and concept-to-label mappings in an end-to-end manner.
A concept is composed of high-level semantic vocabulary for describing objects of interest in input data.
For instance, CBMs can predict the final label ``apple" from the linear combination of the concepts ``red sphere," "green leaf," and ``glossy surface."
These intermediate concept predictions not only provide interpretability but also intervenability in the final prediction by editing the predicted concepts.

In the original CBMs~\citep{Koh_ICML20_concept_bottleneck}, a concept set for each class label is defined by manual annotations, incurring massive labeling costs greater than ones of the class labels.
% Another challenge of CBMs is the degradation of target task performance from black-box models due to the long-tailed distribution of the concepts, which is more difficult to learn than the label distribution~\citep{Zarlenga_NeurIPS22_concept_embedding}.
To reduce the costs, \citet{Oikarinen_ICLR23_label-free_CBMs} and \citet{Yuksekgonul_ICLR23_post-hoc_CBMs} automatically generate the concept sets by large language models (LLMs, e.g., GPT-3~\citep{brown_NIPS20_gpt3}) and use the multi-modal embedding space of vision-language models (VLMs, e.g., CLIP~\citep{Radford_ICML21_CLIP}) to learn the input-to-concept mapping through similarities in the multi-modal feature space.
% Thanks to the powerful representations of VLMs for mapping input-to-concept, this also alleviates the performance degradation problem of CBMs.
Although modern CBMs are free from manual pre-defined concepts, we argue that the practicality is still restricted by the requirements of training input-to-concept and concept-to-label mappings on target datasets.
In other words, CBMs are not available without manually collecting target datasets and additional training of model parameters on them.

To overcome this limitation, this paper tackles a new problem setting of CBMs in a zero-shot manner for target tasks.
% , where we do not assume any target datasets and additional training.
In this setting, we can access pre-trained VLMs, but we cannot know the concepts composing target data in advance.
This setting forces models to perform two-stage zero-shot inference of input-to-concept and concept-to-label for unseen input samples.
The zero-shot input-to-concept inference can not be solved by a na\"ive application of VLMs as the ordinary zero-shot classification of input-to-label because it is required to infer a subset of relevant concepts, not a single label, from the large set of all concepts.
Furthermore, the zero-shot concept-to-label inference is difficult because the concept-to-label mapping is not obvious without target data and training, which are unavailable in this setting.
Therefore, we aim to answer the following research question: \textit{how can we provide interpretable and intervenable concepts by the zero-shot input-to-concept/concept-to-label inference without target datasets and training?}\looseness-1

We present a novel CBM class called \textit{zero-shot concept bottleneck models} (Z-CBMs).
Z-CBMs are zero-shot interpretable models that employ off-the-shelf pre-trained VLMs with frozen weights as the backbone (Fig.~\ref{fig:top}).
Conceptually, Z-CBMs dynamically find concepts related to the input from a broad concept bank (\textbf{concept retrieval}) and then predict the final label by simulating zero-shot classification of black-box VLMs via reconstructing the original input embedding from the concept embeddings (\textbf{concept regression}).
Our primary contribution is to achieve zero-shot input-to-concept and concept-to-label inference with this framework without additional training.

We implement the components of Z-CBMs with simple yet carefully designed and effective techniques.
For concept retrieval, Z-CBMs should cover broad domains to provide sufficient concepts for unseen inputs.
To cover broad concepts, we build a large-scale concept bank, which is composed of millions of vocabulary extracted from large-scale text caption datasets such as YFCC~\citep{Thomee_ACM16_yfcc100m}.
Given an input sample, Z-CBMs dynamically retrieve concept candidates from the concept bank with an efficient and scalable cross-modal search algorithm.
For concept regression, Z-CBMs estimate the importance of concepts for the input feature and then predict labels by the importance-weighted concept features.
However, many of the retrieved concept candidates semantically overlap each other, and thus, the semantically duplicated concepts with high importance by a na\"ive estimation method can harm the interpretability and intervenability for humans.
To overcome this challenge, Z-CBMs find essential and mutually exclusive concepts for the final label prediction by leveraging sparse linear regression (e.g., lasso) to reconstruct the input visual feature vector by a weighted sum of the concept candidate vectors.
Combining concept retrieval and concept regression enables Z-CBMs to predict final task labels with interpretable concepts for various domain inputs without any target task training.

Our extensive experiments on 12 datasets demonstrate that Z-CBMs can provide interpretable and intervenable concepts without any additional training.
Specifically, we confirm that Z-CBMs' sparse concepts are well correlated to input images and cover the annotated concepts in the existing training-based CBMs.
Furthermore, the performance of Z-CBMs can be enhanced by human intervention in the predicted concepts, emphasizing the reliability of the concept-based prediction.
We also show that Z-CBMs can perform accurately at a competitive level with black box VLMs and existing CBMs with training.
These results suggest the practicality of Z-CBMs for various domains.

\section{Related Work}
CBMs~\citep{Koh_ICML20_concept_bottleneck} are inherently interpretable deep neural network models that predict concept labels and then predict final class labels from the predicted concepts.
In contrast to the other explanation styles such as post-hoc attribution heatmaps~\citep{Lundberg_NeurIPS17_shap_attribute,Selvaraju_ICCV17_gradcam,Sundararajan_ICML17_axiomatic_attribution}, CBMs provide semantic ingredients consisting the final label prediction through the bilevel prediction of input-to-concept and concept-to-label.
The original CBMs have the challenge of requiring human annotations of concept labels, which are more difficult to obtain than target task labels.
Another challenge is the performance degradation from backbone black-box models~\citep{Zarlenga_NeurIPS22_concept_embedding,Moayeri_ICML23_text-to-concept,Xu_ICLR24_energy-based_CBMs} due to the difficulty of learning long-tailed concept distributions~\citep{Ramaswamy_CVPR23_overlooked_factors_CBMs}.
Post-hoc CBMs~\citep{Yuksekgonul_ICLR23_post-hoc_CBMs}, Label-free CBMs~\citep{Oikarinen_ICLR23_label-free_CBMs}, and LaBo~\citep{Yang_CVPR23_LaBo} addressed these challenges by automatically collecting concepts corresponding to target task labels by querying LLMs (e.g., GPT-3~\cite{Brown_NeurIPS20_GPT3}) and leveraging multi-modal feature spaces of pre-trained VLMs (e.g., CLIP~\cite{Radford_ICML21_CLIP}) for learning the input-to-concept mapping.
Subsequently, the successor works have basically assumed the use of LLMs or VLMs, further advancing CBMs~\citep{Panousis_ICCV23_CDM,Rao_arXiv24_DN_CBMs,Tan_arXiv24_OpenCBM,Srivastava_arXiv_vlg_cbm}.
In particular, \citet{Panousis_ICCV23_CDM} and \citet{Rao_ECCV24_DN_CBMs} are related to our work in terms of using sparse modeling to select concepts for input images.
However, all of these existing CBMs still require training specialized neural networks on target datasets, incurring additional target data collection and training resources.
Handling the bi-level prediction in a zero-shot manner for unseen input is unobvious because it can not be solved by na\"ive application of the existing zero-shot classification methods, which depend on task-specific vocabularies for single label predictions~\cite{Norouzi_ICLR14_ConSe,Demirel_ICCV17_attributes2classname,Menon_ICLR23_classification_via_llm_description}.
Furthermore, current CBMs and the recent interpretable framework for CLIP~\cite{Bhalla_NeurIPS24_splice} limit the number of concepts up to a few thousand due to training and computational constraints, restricting the generality.
In contrast to the previous CBMs, our Z-CBMs can perform fully zero-shot inference based on a large-scale concept bank with millions of vocabulary for arbitrary input images in various domains as shown in the experiments in Sec.~\ref{sec:ex_multi_dataset}.\looseness-1

\section{Zero-shot Concept Bottleneck Models}
In this section, we formalize the framework of Z-CBMs, which perform a zero-shot inference of input-to-concept and concept-to-label without target datasets and additional training (Fig.~\ref{fig:top}).
Z-CBMs are composed of \textit{concept retrieval} and \textit{concept regression}.
Concept retrieval finds a set of the most input-related concept candidates from millions of concepts by querying an input image feature with a semantic similarity search (Fig.~\ref{fig:concept_ret}).
Concept regression estimates the importance scores of the concept candidates by sparse linear regression to reconstruct the input feature (Fig.~\ref{fig:concept_reg}).
Finally, Z-CBMs provide the final label predicted by the reconstructed vector and concept explanations with importance scores.\looseness-1

\subsection{Problem Setting}
We inherit the problem setting of existing vision-language-based CBMs~\citep{Oikarinen_ICLR23_label-free_CBMs} except for not updating any neural network parameters.
The goal is to predict the final task label \(y\in\mathcal{Y}\) of input \(x\in\mathcal{X}\) based on $K$ interpretable textual concepts \(\{c_i\in\mathcal{C}\subset\mathcal{T}\}_{i=1}^{K}\), where \(\mathcal{X}\), \(\mathcal{Y}\), \(\mathcal{C}\), and \(\mathcal{T}\) are the input, label, concept, and text space, respectively.
To this end, we predict the final task label by the bi-level prediction \(h\circ g(x)\), where \(g:\mathcal{X}\to\mathcal{C}^K\) is a concept predictor and \(h:\mathcal{C}^K\to\mathcal{Y}\) is a label predictor.
This setting allows to access a vision encoder \(f_\mathrm{V}:\mathcal{X}\to\mathbb{R}^{d}\) and a text encoder \(f_\mathrm{T}:\mathcal{T}\to\mathbb{R}^{d}\) provided by a VLM like CLIP~\citep{Radford_ICML21_CLIP}, and a concept bank \(C=\{c_i\}^{N_\mathrm{c}}_{i=1}\).
The concept bank \(C\) is composed of unique concepts from arbitrary sources, including manually collected concepts and automatically generated concepts by LLMs like GPT-3~\citep{brown_NIPS20_gpt3}.\looseness-1

\begin{figure*}[t]
\vspace{-3mm}
  \centering
  \begin{minipage}[b]{0.40\linewidth}
    \centering
    \includegraphics[width=0.9\linewidth]{fig/concept_retrieval.pdf}
    \subcaption{Concept Retrieval}\label{fig:concept_ret}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.55\linewidth}
    \centering
    \includegraphics[width=0.9\linewidth]{fig/concept_regression.pdf}
    \subcaption{Concept Regression}\label{fig:concept_reg}
  \end{minipage}
  \vspace{-2mm}
  \caption{Concept retrieval and concept regression.
  (a) Concept retrieval searches concept candidates close to an input image in the VLM feature space and returns the top-\(K\) concepts, enabling Z-CBMs to use a large-scale concept bank for general input images.
  (b) Concept regression selects the important concepts through sparse linear regression, which approximates the input feature vectors by the weighted sum of concept candidate vectors with sparse coefficients. This sparse linear regression is helpful in selecting unique concepts.
  }
\vspace{-3mm}
\end{figure*}

\subsection{Zero-shot Inference}
\textbf{Concept Retrieval.}
We first find the most semantically closed concept candidates to input images from the large spaces in a concept bank (Fig.~\ref{fig:concept_ret}).
Given an input \(x\), we retrieve the set of $K$ concept candidates \(C_x \subset C\) by using image and text encoders of pre-trained VLMs \(f_\mathrm{V}\) and \(f_\mathrm{T}\) as
\begin{equation}\label{eq:retrieval}
    C_x = \retop{c}{C}{K}(f_\mathrm{V}(x), f_\mathrm{T}(c)) = \underset{c\in C}{\operatorname{Top-K}}~\operatorname{Sim}(f_\mathrm{V}(x), f_\mathrm{T}(c)),
\end{equation}
where \({\operatorname{Top-K}}\) is an operator yielding top-\(K\) concepts in \(C\) from a list sorted in descending order according to a similarity metric \(\operatorname{Sim}\).
Throughout this paper, we use cosine similarity as \(\operatorname{Sim}\) by following~\cite{Conti_NeurIPS23_vocabulary_free_classification}.
Thanks to the scalability of the similarity search algorithm~\citep{Johnson_IEEE19_faiss_gpu, Douze_arXiv24_faiss}, Eq.~(\ref{eq:retrieval}) can efficiently find the concept candidates in an arbitrary concept bank \(C\), which contains millions of concepts to describe inputs in various domains.

\textbf{Concept Regression.}
Given a concept candidate set \(C_x=\{c_1,...,c_K\}\), we predict the final label \(\hat{y}\) by selecting essential concepts from \(C_x\).
Conventional CBMs infer the mapping between \(C_x\) and \(\hat{y}\) by training neural regression parameters on target tasks, which incurs the requirements of target dataset collections and additional training costs.
Instead, we solve this task with a different approach leveraging the zero-shot performance of VLMs.
As shown in the previous studies~\citep{Radford_ICML21_CLIP,Jia_ICML21_ALIGN}, VLMs can be applied to zero-shot classification by inferring a label \(\hat{y}\) by matching input \(x\) and a class name text \(t_y\in \mathcal{T}\) in the multi-modal feature spaces as follows.
\begin{equation}\label{eq:zero_shot_cls}
    \hat{y} = \argmax_{y\in\mathcal{Y}}~\operatorname{Sim}(f_\mathrm{V}(x), f_\mathrm{T}(t_y)).
\end{equation}
If the feature vector \(f_\mathrm{V}(x)\) can be approximated by \(C_x\), we can achieve the zero-shot performance of black-box features by interpretable concept features.
Based on this idea, we approximate \(f_\mathrm{V}(x)\) by the weighted sum of the concept features \(F_{C_x} = [f_\mathrm{T}(c_1),...,f_\mathrm{T}(c_K)]\in\mathbb{R}^{d\times K}\) with an importance weight \(W\in\mathbb{R}^K\) (Fig.~\ref{fig:concept_reg}).
To obtain \(W\), we solve the linear regression problem defined by
\begin{equation}\label{eq:regression_obj}
\min_W \|f_\mathrm{V}(x) - F_{C_x}W\|^2_2 + \lambda\|W\|_1.
\end{equation}
Through this objective, we can achieve \(W\) not only for approximating image features but also for effectively estimating the contribution of each concept to the label prediction owing to the sparse regularization \(\|W\|_1\).
Since \(C_x\) is retrieved from large-scale concept bank \(C\), it often contains noisy concepts that are similar to each other, undermining interpretability due to semantic duplication.
In this sense, the sparse regularization enhances interpretability since it can eliminate unimportant concepts for the label prediction~\citep{Hastie_2015_statistical_sparsity}.

\begin{figure}[t]
\vspace{-5mm}
\begin{algorithm}[H]
    \caption{Zero-shot Inference of Z-CBMs}\label{alg:zcbm_inference}
    \begin{algorithmic}[1]
    {\footnotesize
        \REQUIRE{Input \(x\), concept bank \(C\), image encoder \(f_\mathrm{V}\), text encoder \(f_\mathrm{T}\)}
        \ENSURE{Predicted label \(\hat{y}\), concepts \(C_x\), importance weight \(W_{C_x}\)}
        \STATE{\texttt{\color{gray}\# Retrieving top-K concepts from input}}
        \STATE{\(C_x \leftarrow \retop{c}{C}{K}(f_\mathrm{V}(x), f_\mathrm{T}(c))\)}
        \STATE{\(F_{C_x} \leftarrow [f_\mathrm{T}(c_1), ..., f_\mathrm{T}(c_K)]\)}
        \STATE{\texttt{\color{gray}\# Predicting importance weights by sparse linear regression}}
        \STATE{\(W_{C_x} \leftarrow \argmin_{W\in\mathbb{R}^K} \|f_\mathrm{V}(x) - F_{C_x}W\|^2_2 + \lambda\|W\|_1\)}
        \STATE{\texttt{\color{gray}\# Predicting label by importance weighted sum concept vectors}}
        \STATE{\(\hat{y} \leftarrow \argmax_{y\in\mathcal{Y}}~\operatorname{Sim}(F_{C_x}W_{C_x}, f_\mathrm{T}(t_y))\)}
    }
    \end{algorithmic}
\end{algorithm}
\vspace{-10mm}
\end{figure}

\textbf{Final Label Prediction.}
Finally, we compute the output label with \(F_{C_x}\) and \(W\) in the same fashion as the zero-shot classification by Eq.~(\ref{eq:zero_shot_cls}), i.e.,
\begin{equation}\label{eq:zero_shot_cls_zcbms}
    \hat{y} = \argmax_{y\in\mathcal{Y}}~\operatorname{Sim}(F_{C_x}W, f_\mathrm{T}(t_y)).
\end{equation}
Algorithm~\ref{alg:zcbm_inference} shows the overall protocol of the zero-shot inference of Z-CBM.
This zero-shot inference algorithm can be applied not only to pre-trained VLMs but also to their linear probing, i.e., fine-tuning a linear head layer on the fixed feature extractor of VLMs for target tasks.

\section{Implementation}\label{sec:implementation}
In this section, we present the detailed implementations of Z-CBMs, including backbone VLMs, concept bank construction, concept retrieval, and concept regression.

\textbf{Vision-Language Models.}
Z-CBMs allow to leverage arbitrary pre-trained VLMs for \(f_\mathrm{V}\) and \(f_\mathrm{T}\).
We basically use the official implementation of OpenAI CLIP~\citep{Radford_ICML21_CLIP} and the publicly available pre-trained weights.\footnote{https://github.com/openai/CLIP}
Specifically, by default, we use ViT-B/32 as \(f_\mathrm{V}\) and the base transformer with 63M parameters as \(f_\mathrm{T}\) by following the original CLIP.
In Section~\ref{sec:ex_multi_vlms}, we show that other VLM backbones (e.g., SigLIP~\citep{Zhai_ICCV23_SigLIP} and OpenCLIP~\citep{Cherti_CVPR23_openclip}) are also available for Z-CBMs.

\textbf{Concept Bank Construction.}
Here, we introduce the construction protocols of the concept bank \(C\) of Z-CBMs.
Since Z-CBMs can not know concepts of input image features in advance, a concept bank should contain sufficient vocabulary to describe the various domain inputs.
To this end, we extract concepts from multiple image caption datasets and integrate them into a single concept bank.
Specifically, we automatically collect concepts as noun phrases by parsing each sentence in the caption datasets including Flickr-30K~\citep{Young_2014_Flikr30K}, CC-3M~\citep{Sharma_ACL18_CC3M}, CC-12M~\citep{Changpinyo_CVPR21_CC12M}, and YFCC-15M~\citep{Thomee_ACM16_yfcc100m}; we use the parser implemented in \texttt{nltk}~\citep{Bird_ACL06_nltk}.
At this time, the concept set size is \(|C|\approx\) 20M.
Then, we filter out nonessential concepts from the large base concept set according to policies based on \citet{Oikarinen_ICLR23_label-free_CBMs}; please see Appendix~\ref{sec:append_filtering}.
Finally, after filtering concepts, we obtain the concept bank containing \(|C|\approx\) 5M concepts. We also discuss the effect of varying caption datasets used for collecting concepts in Sec.~\ref{sec:ex_multi_dataset}~and~\ref{sec:ex_concept_bank}.

\textbf{Similarity Search in Concept Retrieval.}
Concept retrieval searches the concept candidates from input feature vectors.
To this end, we implement the concept search component by the open source library of Faiss~\citep{Johnson_IEEE19_faiss_gpu,Douze_arXiv24_faiss}.
First, we create a search index based on the text feature vectors of all concepts in a concept bank \(C\) using \(f_\mathrm{T}\).
At inference time, we retrieve the concept vectors via similarity search on the concept index by specifying the concept number \(K\).
% We found that the choice of \(K\) is important because it determines the trade-off between final accuracy and search speed; larger \(K\) contributes to finding more effective concepts in concept regression but increases the time for concept retrieval.
We set \(K=2048\) as the default value and empirically show the effect of \(K\) in Appendix~\ref{sec:ex_concept_search}.\looseness-1

\textbf{Sparse Linear Regression in Concept Regression.}
In concept regression, we can use arbitrary sparse linear regression algorithms, including lasso~\citep{Tibshirani_1996_lasso}, elastic net~\citep{Zou_2005_elasticnet}, and sparsity-constrained optimization like hard thresholding pursuit~\citep{Yuan_ICML14_sparsity-constrained_htp}.
The efficient implementations of these algorithms are publicly available on the {\tt sklearn}~\citep{sklearn} and {\tt skscope}~\citep{skscope} libraries.
The choice of sparse linear regression algorithm depends on the use cases. 
For example, lasso is useful when one wants to naturally obtain important concepts from a large number of candidate concepts, elastic net is effective for high target task performance, and sparsity-constrained optimization satisfies rigorous requirements regarding the number of concepts for explanations.
We use lasso with \(\lambda=1.0\times10^{-5}\) as the default algorithm (see Appendix~\ref{sec:append_setting}~and~\ref{sec:ex_lambda}), but we confirm that arbitrary sparse linear regression algorithms are available for Z-CBMs in Sec~\ref{sec:ex_analysis}.

\section{Experiments}\label{sec:experiment}
We evaluate Z-CBMs on multiple visual classification datasets and pre-training VLMs.
We conduct experiments on two scenarios: \textit{zero-shot} and \textit{training head}; the former uses pre-trained VLMs for inference without any training, while the latter learns only the classification heads.

\subsection{Settings}\label{sec:ex_settings}
\textbf{Datasets.}
We used 12 classification datasets of various image domains: \uline{Aircraft (Air)}~\citep{maji_13_aircraft}, \uline{Bird}~\citep{Welinder_10_cub2002011}, \uline{Caltech-101 (Cal)}~\citep{FeiFei_caltech101} \uline{Car}~\citep{krause_3DRR2013_stanford_cars}, \uline{DTD}~\citep{cimpoi_CVPR14_DTD}, \uline{EuroSAT (Euro)}~\citep{Helber_IEEE_eurosat}, \uline{Flower (Flo)}~\citep{Nilsback_08_flowers}, \uline{Food}~\citep{bossard14_Food101}, \uline{ImageNet (IN)}~\citep{russakovsky_imagenet}, \uline{Pet}~\citep{parkhi_CVPR12_oxford_pets}, \uline{SUN397}~\citep{Xiao_CVPR10_sun397}, and \uline{UCF-101}~\citep{Soomro_arXiv12ucf101}.
They are often used to evaluate the zero-shot generalization performance of VLMs~\citep{Radford_ICML21_CLIP,Zhou_IJCV_CoOp}.
In the training head scenario, we randomly split a training dataset into \(9:1\) and used the former as the training set and the latter as the validation set.
For ImageNet, we set the split ratio \(99:1\).

\textbf{Zero-shot Baselines.}
Since no zero-shot baselines of CBMs exist, we compare our Z-CBMs with the zero-shot inference of a black-box VLM and variants of Z-CBMs in terms of regression algorithms and concept banks.
For more details, please see Appendix~\ref{sec:append_setting}.

\textbf{Training Head Baselines.}
To compare Z-CBMs with existing vision-language-based CBMs, we evaluated models in a relaxed setting where the models are trained on target datasets.
In this setting, we applied Z-CBMs to linear probing of VLMs, i.e., fine-tuning only a linear head layer on the feature extractors of VLMs; we refer to this pattern LP-Z-CBM.
As the baselines, we used \uline{Lable-free CBM}~\citep{Oikarinen_ICLR23_label-free_CBMs}, \uline{LaBo}~\citep{Yang_CVPR23_LaBo}, and \uline{CDM}~\citep{Panousis_ICCV23_CDM}.
We performed these methods based on their publicly available code repositories.

\textbf{Evaluation Metrics.}
For evaluating predicted concepts, we measured \uline{CLIP-Score}~\citep{Radford_ICML21_CLIP,Hessel_EMNLP21_clipscore}, which is the cosine similarity between image and text embeddings on CLIP, i.e., higher is better.
CLIP-Score between input images and concepts intuitively indicates how well the predicted concept explains the image.
Thus, it performs as an indicator to evaluate the quality of the input-to-concept inference.
Concretely, we measured averaged CLIP-Scores between test images and the predicted concept texts, where we extracted the top 10 concepts from sorted concepts in descending order by absolute concept importance scores for each model.
Note that, to compute CLIP-Score, we used CLIP ViT-L/14, which is a different pre-trained model from CLIP ViT-B/32.
Furthermore, we used \uline{concept coverage} to evaluate the Z-CBM's predicted concepts.
Concept coverage \({|\{c^\mathrm{Z}_i\} \cap \{c^\mathrm{R}_i\}|}/{|\{c^\mathrm{R}_i\}|}\) is the ratio of overlap between Z-CBM's concepts with non-zero coefficients \(\{c^\mathrm{Z}_i\}\subset C\) and reference concepts \(\{c^\mathrm{R}_i\}\subset C\) predicted by vision-language-based CBMs that require training.
This metric evaluates the extent to which the Z-CBM yields concepts that are close to those derived in the target training when using the shared concept bank \(C\).
Specifically, we computed the average concept coverage across test samples by using the GPT-generated concept banks by \cite{Oikarinen_ICLR23_label-free_CBMs}, and reference concepts of Label-free CBMs; we used concepts with contribution scores greater than 0.05 as \(\{c^\mathrm{R}_i\}\) by following~\cite{Oikarinen_ICLR23_label-free_CBMs}.
We also report \uline{top-1 test accuracy} as the target classification task performance.

\subsection{Quantitative Evaluation of Predicted Concepts}\label{sec:ex_quantitative_eval_concept}

We first quantitatively evaluate the predicted concepts of Z-CBMs from the perspective of their factuality to represent image features.
We measured averaged CLIP-Score and concept coverage across the 12 datasets.

Table~\ref{tb:clip_score} shows the results of CLIP-Score. 
For all datasets, our Z-CBM predicted concepts that are strongly correlated to input images, and it largely outperformed the CBM baselines that require training.
This can be caused by the choice of concept bank.
Existing CBMs perform concept-to-label inference with learnable parameters, making it difficult to handle millions of concepts at once.
Thus, they often limit their concept vocabularies to a few thousand to ensure learnability.
In contrast, our Z-CBMs can treat millions of concepts without training by dynamically retrieving concepts of interest and inferring essential concepts with sparse linear regression.
That is, paradoxically, Z-CBMs succeed in providing accurate image explanations through an abundant concept vocabulary by eliminating training.

On the other hand, Table~\ref{tb:concept_coverage} shows the results of concept coverage when using the concepts predicted by Label-free CBMs as the reference concepts. 
We also list the results of Z-CBMs using cosine similarity on CLIP and linear regression to compute the importance coefficients instead of lasso; since all of their coefficients are non-zero values, we measured the concept coverage scores by using the top 128 concepts.
Z-CBMs with lasso achieved the best concept coverage; the average score was 85.27\%.
This indicates that Z-CBMs can predict most of the important concepts found by trained CBMs, and sparse linear regression is a key factor for finding important concepts without training.

\begin{table}[!tbp]
  \centering
  \caption{
    CLIP-Score on 12 classification datasets. 
    We compute the averaged CLIP-Scores between images and concepts with top-10 absolute coefficients.
    Complete results appear in Table~\ref{tb:clip_score_full}.
   }
  \label{tb:clip_score}%
  \vspace{-3mm}
  % \resizebox{1.0\columnwidth}{!}{
    \begin{tabular}{lc}
    \toprule
    \multicolumn{1}{l}{\textbf{Method}} & \textbf{Avg. of 12 datasets} \\
    \midrule
    Label-free CBM & 0.6982  \\
    LaBo & 0.7078\\
    CDM & 0.7141 \\
    \rowcolor{blue!20}  % set next line gray background color
    Z-CBM (ALL) & \textbf{0.7754} \\
    \bottomrule
    \end{tabular}
    % } 
  \vspace{-3mm}
\end{table}%
\begin{table}[!tbp]
  \centering
  \caption{
    Concept coverage (\%) of Z-CBMs on 12 classification datasets.
    Complete results appear in Table~\ref{tb:concept_coverage_full}.
   }
  \label{tb:concept_coverage}%
  \vspace{-3mm}
  % \resizebox{1.0\textwidth}{!}{
    \begin{tabular}{lc}
    \toprule
    \multicolumn{1}{l}{\textbf{Method}} & \textbf{Avg. of 12 datasets} \\
    \midrule
    Z-CBM (Cosine Similarity) & 58.51\\  
    Z-CBM (Linear Regression) & 76.87 \\
    \rowcolor{blue!20}  % set next line gray background color
    Z-CBM (Lasso) & \textbf{85.27} \\
    \bottomrule
    \end{tabular}
    % }
  \vspace{-3mm}
\end{table}%

\subsection{Evaluation of Human Intervention}\label{sec:ex_reliability}

Human intervention in the output concept is an essential feature shared by the CBM family for debugging models and modifying the output concepts to make the final prediction accurate.
Here, we evaluate the reliability of Z-CBMs through two types of intervention: (i) concept deletion and (ii) concept insertion.
In concept deletion, we confirm the dependence on the predicted concepts by removing the concept with non-zero coefficients in ascending, descending, and random orders.
Fig.~\ref{fig:concept_deletion} is the results on Bird by varying the deletion ratio.
The accuracy of Z-CBMs significantly dropped with the smaller deletion ratio in the case of descent.
This indicates that Z-CBM accurately selects the important concepts through concept regression and predicts the final label based on the concepts.
In the case of ascent, the accuracy slowly and steadily decreases, suggesting that the Z-CBMs are not biased toward limited concepts and that all of the selected concepts are essential.

\begin{figure}[t]
  % \centering
  % \begin{minipage}[h]{0.48\linewidth}
    \centering
    \includegraphics[width=0.8\linewidth]{fig/concept_deletion.pdf}
    \vspace{-2mm}
    \caption{Concept Deletion (Bird)}\label{fig:concept_deletion}
  % \end{minipage}
\vspace{-3mm}
\end{figure}
  % \hfill
\begin{figure}[t]
  % \begin{minipage}[h]{0.48\linewidth}
    \centering
    \includegraphics[width=0.8\linewidth]{fig/concept_insertion.pdf}
    \vspace{-2mm}
    \caption{Concept Insertion (Bird)}\label{fig:concept_insertion}
  % \end{minipage}
\vspace{-3mm}
\end{figure}

In concept insertion, we add ground truth concepts to the predicted concepts with non-zero coefficients and then re-compute concept regression on the intervened concept set.
Specifically, we used linear regression as the algorithm in concept regression and then predicted target labels by the weighted averaged intervened concept vectors by Eq.~(\ref{eq:zero_shot_cls_zcbms}).
As the ground truth concepts, we used the fine-grained multi-labels annotated for Bird~\citep{Welinder_10_cub2002011}.
Fig.~\ref{fig:concept_insertion} demonstrates the top-1 accuracy of the intervened Z-CBMs.
The performance improved as the number of inserted concepts per sample increased.
This indicates that Z-CBMs can correct the final output by modifying the concept of interest through intervention.

\begin{figure*}[t]
  \centering
    \centering
    \includegraphics[width=\linewidth]{fig/concept_visualization.pdf}
    \caption{
    Qualitative evaluation of predicted concepts on the ImageNet validation set.
    While Label-free CBMs sometimes hallucinate invisible concepts or ignore important concepts, Z-CBMs with lasso consistently provide realistic and dominant concepts in input images with diverse vocabulary. \textbf{NOT} prefix denotes that the concept has negative coefficients.
    }\label{fig:qualitative_eval}
  \vspace{-5mm}
\end{figure*}

\subsection{Qualitative Evaluation of Predicted Concepts}\label{sec:ex_qualitative_eval_concept}
We demonstrate the qualitative evaluation of predicted concepts by Label-free CBMs and Z-CBMs when inputting the ImageNet validation examples in Fig.~\ref{fig:qualitative_eval}; we also show the results of Z-CBMs using linear regression to compute the importance coefficients instead of lasso.
Overall, Z-CBMs tend to accurately predict realistic and dominant concepts that appear in input images even though they are not trained on target tasks.
For instance, in the first row, Z-CBM predicts various concepts related to dogs, clothes, and background, whereas Label-free CBM focuses on clothes and ignores dogs and background.
This difference may be caused by the fact that the image-to-concept mapping of Z-CBMs is not biased toward the label information because it does not train on the target data.
Conversely, like the second row, Z-CBMs tend to concentrate on global regions and miss the concepts in local regions; this can be alleviated by intervening in the concept prediction (see Sec.~\ref{sec:ex_reliability}).

For the comparison of linear regression and lasso,  we can see that Z-CBM (Linear Reg.) tends to produce concepts that are related to each other.
In fact, quantitatively, we also found that the averaged inner CLIP-Scores among the top-10 concepts of lasso (0.6855) is significantly lower than that of linear regression (0.7826).
These results emphasize the advantage of using sparse modeling in concept regression to select mutually exclusive concepts based on the concept bank containing abundant vocabulary.\looseness-1

\subsection{Zero-shot Image Classification Performance}\label{sec:ex_multi_dataset}
Table~\ref{tb:multiple_dataset} summarizes the averaged top-1 accuracy across the 12 image classification datasets.
It also shows the ablation of concept banks; the brackets in the Z-CBM rows represent the dataset used to construct the concept bank.
In the zero-shot setting, we observed that our Z-CBMs outperformed the zero-shot CLIP baseline.
This is beyond our expectations and may be due to the fact that Z-CBMs approximate image features with the weighted sum of concept text features, reducing the modality gap between the original image and the label text (see Appendix~\ref{sec:ex_modality_gap}).
The ablation of concept banks demonstrates that higher accuracy tends to be achieved by larger concept banks.
This indicates that image features are more accurately approximated by selecting concepts from a rich vocabulary.
We further explore the impacts of concept banks in Sec.~\ref{sec:ex_concept_bank}.

In the training head setting, Z-CBMs based on linear probing models (\textbf{LP-Z-CBMs}) reproduced the accuracy of linear probing well.
Further, LP-Z-CBMs stably outperformed existing methods that require additional training for special modules.
This suggests that our concept retrieval and concept regression using the original CLIP features are sufficient for input-to-concept and concept-to-label inference in terms of target task performance.

\subsection{Detailed Analysis}\label{sec:ex_analysis}

\subsubsection{Effects of Backbone VLMs}\label{sec:ex_multi_vlms}
We show the impacts on Z-CBMs when varying backbone VLMs.
Since vision-language models are being intensively studied, it is important to confirm the compatibility of Z-CBMs with successor models with better zero-shot performance.
In addition to the CLIP models, we used OpenCLIP~\citep{Cherti_CVPR23_openclip}, SigLIP~\citep{Zhai_ICCV23_SigLIP}, and DFN~\citep{Fang_ICLR24_DFN}.
Table~\ref{tb:vlm_backbone} demonstrates the results, including the original zero-shot classification accuracy and the accuracy with Z-CBMs, and CLIP-Score.
The performance of Z-CBMs improved in proportion to the zero-shot performance of the VLMs.
In particular, the gradual improvement in CLIP-Score indicates that input-to-concept inference becomes more accurate with more powerful VLMs.
We also observed that the improvement phenomenon over black-box baselines discussed in Sec.~\ref{sec:ex_multi_dataset} appears especially in small models where the multi-modal alignment capability is relatively weak.
These results suggest that Z-CBM is universally applicable across generations of VLMs, and that its practicality will improve as VLMs evolve in future work.

\subsubsection{Effects of Concept Bank}\label{sec:ex_concept_bank}
As shown in Sec.~\ref{sec:ex_multi_dataset} and Table~\ref{tb:multiple_dataset}, the choice of concept bank is crucial for the performance.
Here, we provide a more detailed analysis of the concept banks.
Table~\ref{tb:concept_bank} summarizes the results when varying concept banks.
For comparison, we added the concept bank generated by GPT-3 from ImageNet class names, which is used in Label-free CBMs~\citep{Oikarinen_ICLR23_label-free_CBMs}; we used the concept sets published in the official repository.
Although it is competitive with the existing CBM baseline (Label-free CBMs), Z-CBMs with the GPT-3 concepts significantly degraded the top-1 accuracy from Zero-shot CLIP, and the CLIP score was much lower than that of our concept banks composed of noun phrases extracted from caption datasets.
This indicates that the concept bank used in the existing method is limited in its ability to represent image concepts.
Meanwhile, our concept bank scalably improved in accuracy and CLIP-Score as its size increased, and combining all of them achieved the best results. \looseness-1

\setlength{\tabcolsep}{6pt}
\begin{table}[!tbp]
  \centering
  \caption{
    Top-1 accuracy on 12 classification datasets with CLIP ViT-B/32.
    Complete results appear in Table~\ref{tb:multiple_dataset_full}.
   }
  \label{tb:multiple_dataset}
  \vspace{-3mm}
  \resizebox{1.0\columnwidth}{!}{
    \begin{tabular}{llc}
    \toprule
    \multicolumn{1}{l}{\textbf{Setting}} &\multicolumn{1}{l}{\textbf{Method}} & \textbf{Avg. of 12 datasets} \\
    \midrule
    \multirow{6}{*}{Zero-Shot} & \multicolumn{1}{l}{Zero-shot CLIP} & 53.73  \\
     % & \multicolumn{1}{l}{\rev{ConSe}} & \rev{0.99}  & \rev{1.87}  & \rev{11.68}  & \rev{1.42}  & \rev{12.23} & \rev{15.32}  & \rev{3.51}  & \rev{10.99}  & \rev{25.19}  & \rev{19.16}  & \rev{9.65}  & \rev{17.76}  & \rev{10.82} \\
    \cmidrule{2-3}
    & \multicolumn{1}{l}{Z-CBM (Flickr30K)} & 52.62\\
    & \multicolumn{1}{l}{Z-CBM (CC3M)} & 52.98 \\
    & \multicolumn{1}{l}{Z-CBM (CC12M)} & 53.97 \\
    & \multicolumn{1}{l}{Z-CBM (YFCC15M)} & 53.94  \\
    \rowcolor{blue!20}  % set next line gray background color
    \cellcolor{white} & \multicolumn{1}{l}{Z-CBM (ALL)} & \textbf{54.28}\\
    \midrule
    \multirow{5}{*}{Training Head} & \multicolumn{1}{l}{Linear Probe CLIP} & \textbf{78.98} \\\cmidrule{2-3}
    & \multicolumn{1}{l}{Label-free CBM} & 74.87 \\
    & \multicolumn{1}{l}{LaBo} & 74.04 \\
    & \multicolumn{1}{l}{CDM} & 76.39\\
    \rowcolor{blue!20}  % set next line gray background color
    \cellcolor{white} & \multicolumn{1}{l}{LP-Z-CBM (ALL)} & \text{78.31}\\
    \bottomrule
    \end{tabular}}
  \vspace{-3mm}
\end{table}%

\begin{table}[!tbp]
  \centering
  \caption{
    Performance of Z-CBMs varying backbone VLMs on ImageNet.
   }
  \label{tb:vlm_backbone}%
  \vspace{-3mm}
  \resizebox{1.0\columnwidth}{!}{
    \begin{tabular}{lccc}
    \toprule
    \multirow{2}{*}{\textbf{Backbone VLM}} & \textbf{Top-1 Acc.} & \textbf{Top-1 Acc.}  & \textbf{CLIP-Score}  \\
    & \textbf{(Black Box)} & \textbf{(Z-CBM)}  & \textbf{(Z-CBM)} \\
    \midrule
    CLIP ViT-B/32        & 61.88 & 62.70 & 0.7766 \\  
    CLIP ViT-L/14        & 72.87 & 73.19 & 0.7881 \\
    OpenCLIP ViT-H/14    & 77.20 & 77.81 & 0.7910 \\
    OpenCLIP ViT-G/14    & 79.03 & 78.27 & 0.8095 \\
    SigLIP ViT-SO400M/14 & 82.27 & 81.74 & 0.8241 \\
    DFN ViT-H/14         & 83.85 & 83.40 & 0.8337 \\
    \bottomrule
    \end{tabular}
    }
  \vspace{-3mm}
\end{table}%

\subsubsection{Effects of Concept Regressor}\label{sec:ex_concept_regressor}
Z-CBMs allow users to choose arbitrary sparse linear regression algorithms according to their demands, as discussed in Sec.~\ref{sec:implementation}.
Here, we compare the performance of Z-CBMs with multiple sparse linear regression algorithms: lasso~\citep{Tibshirani_1996_lasso}, elastic net~\citep{Zou_2005_elasticnet}, and sparsity-constrained optimization with HTP~\citep{Yuan_ICML14_sparsity-constrained_htp}.
Further, we evaluate these sparse algorithms by comparing them with non-sparse algorithms to compute the importance of concepts: CLIP Similarity, which uses the cosine similarity computed on CLIP as the importance, and linear regression.
Table~\ref{tb:concept_regressor} shows the performance, where sparsity is a ratio of non-zero importance coefficients to the total number of concept candidates.
While the sparse linear regression algorithms achieved top-1 accuracy scores at the same level, the non-sparse algorithms failed to accurately predict labels from importance-weighted concepts.
Additionally, linear regression has unstable numerical computation due to the rank-deficient of the Gram matrix of $F_{C_x}$ when the feature dimension $d$ is smaller than the concept retrieval size $K$. 
In contrast, lasso can avoid this by sparse regularization.
These results indicate that the concept selection by sparse linear regression is crucial in Z-CBMs.
In this sense, we can interpret our concept regression as a re-ranking method of the CLIP similarity.
Elastic net was the best in accuracy, but it selected more concepts than the other sparse algorithms. This is because elastic net selects all highly correlated concepts to derive a unique solution by combining \(\ell_1\) and \(\ell_2\) regularization~\citep{Hastie_2015_statistical_sparsity}.
HTP explicitly limits the number of concepts selected to 256, so while it achieves the highest sparsity, it had the lowest accuracy of the sparse algorithms due to the shortage of concepts for explanation.

\begin{table}[!tbp]
  \centering
  \caption{
    Performance of Z-CBMs varying concept banks on ImageNet with CLIP ViT-B/32.
   }
  \label{tb:concept_bank}%
  \vspace{-3mm}
  \resizebox{1.0\columnwidth}{!}{
    \begin{tabular}{lccc}
    \toprule
    \multicolumn{1}{l}{\textbf{Concept Bank}} & \textbf{Vocab. Size} & \textbf{Top-1 Acc.} & \textbf{CLIP-Score} \\
    \midrule
    Zero-shot CLIP & N/A & 61.88 & N/A \\
    \midrule
    Label-free CBM w/ GPT-3 (ImageNet Class) & 4K& 58.00 & 0.7056 \\
    CDM w/ GPT-3 (ImageNet Class) & 4K & 62.52 & 0.7445 \\
    \midrule
    GPT-3 (ImageNet Class) & 4K & 59.18 & 0.6276  \\  
    Noun Phrase (Flickr30K) & 45K & 61.52 & 0.6770 \\
    Noun Phrase (CC3M) & 186K & 62.38 & 0.7109 \\
    Noun Phrase (CC12M) & 2.58M & 62.42 & 0.7671  \\
    Noun Phrase (YFCC15M) & 2.20M & 62.45 & 0.7679  \\
    \rowcolor{blue!20}  % set next line gray background color
    Noun Phrase (ALL) & \textbf{5.12M} & \textbf{62.70} & \textbf{0.7746} \\
    \bottomrule
    \end{tabular}
    }
  \vspace{-3mm}
\end{table}%

\begin{table}[!tbp]
  % \begin{minipage}[h]{0.48\textwidth}
  \centering
  \caption{
    Performance of Z-CBMs varying concept regressor on ImageNet with CLIP ViT-B/32.
   }
  \label{tb:concept_regressor}%
  \vspace{-3mm}
  \resizebox{1.0\columnwidth}{!}{
    \begin{tabular}{lccc}
    \toprule
    \multicolumn{1}{l}{\textbf{Concept Regressor}} & \textbf{Top-1 Acc.} & \textbf{Sparsity} & \textbf{CLIP-Score} \\%& \textbf{CLIP-Score} \\
    \midrule
    CLIP Similarity            & 14.66 & 0.0000 & 0.8117 \\  
    Linear Regression          & 52.88 & 0.0000 & 0.7076 \\
    Lasso                      & 62.70 & 0.8201 & 0.7746 \\
    Elastic Net                & 62.84 & 0.7311 & 0.7818 \\
    Sparsity-Constrained (HTP) & 62.54 & 0.8750 & 0.7795 \\
    \bottomrule
    \end{tabular}
    }
  % \end{minipage}
  \vspace{-3mm}
\end{table}%


\section{Conclusion}
In this paper, we presented zero-shot CBMs (Z-CBMs), which predict input-to-concept and concept-to-label mappings in a fully zero-shot manner.
To this end, Z-CBMs first search input-related concept candidates by concept retrieval, which leverages pre-trained VLMs and a large-scale concept bank containing millions of concepts to explain outputs for unseen input images in various domains.
For the concept-to-label inference, concept regression estimates the importance of concepts by solving the sparse linear regression approximating the input image features with linear combinations of selected concepts.
Our extensive experiments show that Z-CBMs can provide interpretable and intervenable concepts comparable to conventional CBMs that require training.
Since Z-CBMs can be built on any off-the-shelf VLMs, we believe that it will be a good baseline for zero-shot interpretable models based on VLMs in future research.

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}
\newpage
\paragraph{Impact Statements.}
This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.


\bibliography{ref}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Details of Concept Filtering}\label{sec:append_filtering}
We basically follow the policies introduced by \cite{Oikarinen_ICLR23_label-free_CBMs}, which removes (i) too long concepts, (ii) too similar concepts to each other, and (iii) too similar concepts to target class names (optional).
However, the second policy is computationally intractable because it requires the \(\mathcal{O}(|C|^2)\) computation of the similarity matrix across all concepts.
Thus, we approximate this using a similarity search by Eq.~(\ref{eq:retrieval}) that yields the most similar concepts.
We retrieve the top 64 concepts from a concept and remove them according to the original policy.

\section{Details of Settings}\label{sec:append_setting}
\paragraph{Zero-shot Baselines.}
For the black-box baseline, according to the previous work~\citep{Radford_ICML21_CLIP}, we construct a class name prompt \(t_y\) by the scheme of ``\texttt{a photo of [class name]}'', and make VLMs predict a target label \(\hat{y}\) by Eq.~(\ref{eq:zero_shot_cls}).
ConSe is a zero-shot cross-modal classification method that infers a target label from a semantic embedding composed of the weighted sum of concepts of the single predicted ImageNet label.
% We implemented ConSe with pre-trained CLIP and concept bank, which were the same as Z-CBMs.
For Z-CBMs, we selected $1.0\times10^{-5}$ as $\lambda$ by searching from $\{1.0\times10^{-2},1.0\times10^{-3},1.0\times10^{-4},1.0\times10^{-5},1.0\times10^{-6},1.0\times10^{-7},1.0\times10^{-8}\}$ to choose the minimum value achieving over 10\% non-zero concept ration when using $K=2048$ on the subset of ImageNet training set. We used the same $\lambda$ for all experiments.
To make 

\section{Additional Experiments}
\subsection{Detailed Results for All Datasets}
Table~\ref{tb:clip_score_full}, \ref{tb:concept_coverage_full}, and~\ref{tb:multiple_dataset_full} shows all of the results on the 12 datasets omitted in Table~\ref{tb:clip_score}, \ref{tb:concept_coverage}, and~\ref{tb:multiple_dataset}, respectively.

\setlength{\tabcolsep}{6pt}
\begin{table*}[!tbp]
  \centering
  \caption{
    CLIP-Score on 12 classification datasets. 
    We compute the averaged CLIP-Scores between images and concepts with top-10 absolute coefficients.
   }
  \label{tb:clip_score_full}%
  \vspace{-3mm}
  \resizebox{1.0\textwidth}{!}{
    \begin{tabular}{lccccccccccccc}
    \toprule
    \multicolumn{1}{l}{\textbf{Method}} & \textbf{Air} & \textbf{Bird} & \textbf{Cal} & \textbf{Car} & \textbf{DTD} & \textbf{Euro} & \textbf{Flo} & \textbf{Food} & \textbf{IN} & \textbf{Pet} & \textbf{SUN} & \textbf{UCF} & \textbf{Avg.} \\
    \midrule
    Label-free CBM &0.6824 & 0.7818 & 0.7023 & 0.7106 & 0.6552 & 0.6179 & 0.6988 & 0.6959 & 0.7202 & 0.7119 & 0.7327 & 0.6688 & 0.6982  \\
    LaBo & 0.6980 & 0.7626 & 0.7211 & 0.7411 & 0.6299 & 0.6202 & 0.7138 & 0.7526 & 0.7272 & 0.7235 & 0.7060 & 0.6978 & 0.7078\\
    CDM & 0.6887 & 0.7655 & 0.7164 & 0.7221 & 0.7000 & 0.6584 & 0.7239 & 0.7151 & 0.7618 & 0.7257 & 0.7049 & 0.6870 & 0.7141 \\
    \rowcolor{blue!20}  % set next line gray background color
    Z-CBM (ALL) &\textbf{0.7811} & \textbf{0.8100} & \textbf{0.7748} & \textbf{0.7582} & \textbf{0.7661} & \textbf{0.7457} & \textbf{0.7767} & \textbf{0.7785} & \textbf{0.7766} & \textbf{0.7477} & \textbf{0.7925} & \textbf{0.7965} & \textbf{0.7754} \\
    \bottomrule
    \end{tabular}}
  \vspace{-3mm}
\end{table*}%
\begin{table*}[!tbp]
  \centering
  \caption{
    Concept coverage (\%) of Z-CBMs on 12 classification datasets}
  \label{tb:concept_coverage_full}%
  \vspace{-3mm}
  \resizebox{1.0\textwidth}{!}{
    \begin{tabular}{lccccccccccccc}
    \toprule
    \multicolumn{1}{l}{\textbf{Method}} & \textbf{Air} & \textbf{Bird} & \textbf{Cal} & \textbf{Car} & \textbf{DTD} & \textbf{Euro} & \textbf{Flo} & \textbf{Food} & \textbf{IN} & \textbf{Pet} & \textbf{SUN} & \textbf{UCF} & \textbf{Avg.} \\
    \midrule
    Z-CBM (Cosine Similarity) & 66.83 & 41.42 & 37.13 & 60.95 & 71.85 & 90.37 & 50.39 & 77.50 & 48.80 & 90.07 & 29.76 & 37.04 & 58.51\\  
    Z-CBM (Linear Regression) & 96.45 & 81.98 & 51.82 & 58.06 & 91.40 & 90.91 & 90.82 & 90.88 & 71.51 & 95.37 & 40.84 & 62.43 & 76.87 \\
    \rowcolor{blue!20}  % set next line gray background color
    Z-CBM (Lasso) & \textbf{98.95} & \textbf{86.01} & \textbf{69.97} & \textbf{96.43} & \textbf{94.26} & \textbf{91.91} & \textbf{93.57} & \textbf{96.74} & \textbf{86.92} & \textbf{97.37} & \textbf{42.86} & \textbf{68.20} & \textbf{85.27} \\
    \bottomrule
    \end{tabular}}
  \vspace{-3mm}
\end{table*}%
\begin{table*}[!tbp]
  \centering
  \caption{
    Top-1 accuracy on 12 classification datasets with CLIP ViT-B/32.
   }
  \label{tb:multiple_dataset_full}
  \vspace{-3mm}
  \resizebox{1.0\textwidth}{!}{
    \begin{tabular}{llccccccccccccc}
    \toprule
    \multicolumn{1}{l}{\textbf{Setting}} &\multicolumn{1}{l}{\textbf{Method}} & \textbf{Air} & \textbf{Bird} & \textbf{Cal} & \textbf{Car} & \textbf{DTD} & \textbf{Euro} & \textbf{Flo} & \textbf{Food} & \textbf{IN} & \textbf{Pet} & \textbf{SUN} & \textbf{UCF} & \textbf{Avg.} \\
    \midrule
    \multirow{6}{*}{Zero-Shot} & \multicolumn{1}{l}{Zero-shot CLIP} & 18.93  & 51.80  & 24.50  & 60.38  & 43.24  & 35.54  & 63.41  & 78.61  & 61.88  & 85.77  & 61.21  & 59.48 & 53.73  \\
     % & \multicolumn{1}{l}{\rev{ConSe}} & \rev{0.99}  & \rev{1.87}  & \rev{11.68}  & \rev{1.42}  & \rev{12.23} & \rev{15.32}  & \rev{3.51}  & \rev{10.99}  & \rev{25.19}  & \rev{19.16}  & \rev{9.65}  & \rev{17.76}  & \rev{10.82} \\
    \cmidrule{2-15}
    & \multicolumn{1}{l}{Z-CBM (Flickr30K)} & 18.27 & 46.70 & 24.26 & 56.46 & 43.56 & 34.32 & 59.80 & 78.17 & 61.52 & 85.46 & 62.23 & 60.67 & 52.62\\
    & \multicolumn{1}{l}{Z-CBM (CC3M)} & 18.09 & 48.53 & 24.30 & 55.58 & 43.51 & 35.09 & 61.44 & 78.89 & 62.68 & 85.29 & 62.18 & 60.45 & 52.98 \\
    & \multicolumn{1}{l}{Z-CBM (CC12M)} & 18.66 & 51.03 & 24.42 & \textbf{59.22} & 43.72 & \textbf{36.73} & 63.31 & 79.26 & 62.42 & \textbf{85.98} & 62.11 & 60.75 & 52.98 \\
    & \multicolumn{1}{l}{Z-CBM (YFCC15M)} & 18.81 & \textbf{51.87} & 24.54 & 58.72 & 43.40 & 35.96 & 63.38 & 79.22 & 62.42 & 85.94 & 62.07 & 60.96 & 53.97  \\
    \rowcolor{blue!20}  % set next line gray background color
    \cellcolor{white} & \multicolumn{1}{l}{Z-CBM (ALL)} & \textbf{19.00} & 51.75 & \textbf{25.42} & 58.87 & \textbf{43.86} & 36.12 & \textbf{63.78} & \textbf{82.44} & \textbf{62.70} & 85.95 & \textbf{62.89} & \textbf{61.49} & \textbf{54.28}\\
    \midrule
    \multirow{5}{*}{Training Head} & \multicolumn{1}{l}{Linear Probe CLIP} & \textbf{45.06} & \textbf{72.72} & \textbf{95.70} & \textbf{79.75} & \textbf{74.84} & \textbf{92.99} & \textbf{94.02} & \textbf{87.06} & \textbf{68.54} & \textbf{88.72} & \textbf{65.20} & \textbf{83.14} & \textbf{78.98} \\\cmidrule{2-15}
    & \multicolumn{1}{l}{Label-free CBM} & 42.72 & 67.05 & 94.12 & 71.81 & \textbf{74.31} & 91.30 & 91.23 & 81.91 & 58.00 & 83.29 & 62.00 & 80.68 & 74.87 \\
    & \multicolumn{1}{l}{LaBo} &43.43 & 69.38 & 94.82 & 77.78 & 73.59 & 88.17 & 91.67 & 84.29 & 59.16 & 87.24 & 57.70 & 81.26 & 74.04 \\
    & \multicolumn{1}{l}{CDM} & 44.58 & 69.75 & 95.78 & 77.27 & 74.80 & \textbf{92.16} & 92.99 & 81.85 & 62.52 & 86.59 & 56.48 & 81.93 & 76.39\\
    \rowcolor{blue!20}  % set next line gray background color
    \cellcolor{white} & \multicolumn{1}{l}{LP-Z-CBM (ALL)} & \text{44.80} & \text{71.67} & \text{95.50} & \text{78.09} & 73.94 & 91.22 & \text{93.28} & \text{86.73} & \text{67.99} & \text{88.58} & \text{65.53} & \text{82.37} & \text{78.31}\\
    \bottomrule
    \end{tabular}}
  \vspace{-3mm}
\end{table*}%

\begin{figure*}[!tbp]
  \centering
\begin{minipage}[h]{0.48\linewidth}
    \centering
    \includegraphics[width=\textwidth]{fig/pca_feat_viz.pdf}
    \caption{PCA feature visualization of Z-CBMs}\label{fig:pca_feat_viz}
  \end{minipage}
  \hfill
  \begin{minipage}[h]{0.48\linewidth}
    \centering
    \includegraphics[width=\textwidth]{fig/lambda.pdf}
    \caption{Effects of varying $\lambda$ in Eq.~\ref{eq:regression_obj}}\label{fig:lambda}
  \end{minipage}
  \vspace{-3mm}
\end{figure*}%


\subsection{Analysis on Modality Gap}\label{sec:ex_modality_gap}
In Section~\ref{sec:ex_multi_dataset}, Table~\ref{tb:multiple_dataset} shows that Z-CBMs improved the zero-shot CLIP baselines.
We hypothesize that the reason is reducing the modality gap~\citep{Liang_NeurIPS22_clip_modality_gap} between image and text features by the weighted sum of concept features to approximate \(f_\mathrm{V}(x)\) by Eq.~\ref{eq:regression_obj}.
To confirm this, we conduct a deeper analysis of the effects of Z-CBMs on the modality gap with quantitative and qualitative evaluations.
For quantitative evaluation, we measured the L2 distance between image-label features and concept-label features as the modality gap by following~\citep{Liang_NeurIPS22_clip_modality_gap}.
The L2 distances were $1.74\times 10^{-3}$ in image-to-label and $0.86\times 10^{-3}$ in concept-to-label, demonstrating that Z-CBMs largely reduce the modality gap by concept regression.
We also show the PCA feature visualizations in Figure~\ref{fig:pca_feat_viz}, indicating that the weighted sums of concepts (reconstructed concepts) bridge the image and text modalities.

\subsection{Effects of $\lambda$}\label{sec:ex_lambda}
Here, we discuss the effects when changing $\lambda$ in Eq.~(\ref{eq:regression_obj}).
We varied $\lambda$ in $\{1.0\times10^{-2},1.0\times10^{-3},1.0\times10^{-4},1.0\times10^{-5},1.0\times10^{-6},1.0\times10^{-7},1.0\times10^{-8}\}$.
Figure~\ref{fig:lambda} plots the accuracy and the sparsity of predicted concepts on ImageNet.
Using different lambda varies the sparsity and accuracy. Therefore, selecting appropriate $\lambda$ is important for achieving both high sparsity and high accuracy.

\subsection{Effects of \(K\) in Concept Retrieval}\label{sec:ex_concept_search}
As discussed in Sec.~\ref{sec:implementation}, the retrieved concept number \(K\) in concept retrieval controls the trade-off between the accuracy and inference time.
We assess the effects of \(K\) by varying it in \([128, 256, 512, 1024, 2048]\) and measuring the top-1 accuracy and averaged inference time for processing an image.
Note that we set \(2048\) as the maximum value of \(K\) because it is the upper bound in the GPU implementation of Faiss~\citep{Johnson_IEEE19_faiss_gpu}.
Figure~\ref{fig:accuracy_vs_time_k} illustrates the relationship between the accuracy and total inference time.
As expected, the size of \(K\) produces a trade-off between accuracy and inference time.
Even so, the increase in inference time with increasing \(K\) is not explosive and is sufficiently practical since the inferences can be completed in around 55 milliseconds per sample.
The detailed breakdowns of total inference time when \(K=2048\) were 0.11 for extracting image features, 5.35 for concept retrieval, and 49.23 for concept regression, indicating that the computation time of concept regression is dominant for the total.
In future work, we explore speeding up methods for Z-CBMs to be competitive with the existing CBMs baseline that require training (e.g., Label-free CBMs, which infer a sample in 3.30 milliseconds).

\begin{figure}[t]
  \centering
    \centering
    \includegraphics[width=0.5\columnwidth]{fig/inference_time_vs_acc.pdf}
    \captionof{figure}{Accuracy vs. inference time by varying retrieved concept number \(K\).}\label{fig:accuracy_vs_time_k}
\end{figure}


% \section{Extended Related Work}
% \paragraph{Cross-modal zero-shot classification.}
% In zero-shot or supervised learning settings, several works \citep{Lampert_TPAMI13_AwA,Norouzi_ICLR14_ConSe,Mensink_CVPR14_costa,Jain_ICCV15_objects2action,Elhoseiny_ICCV13_write} have explored cross-modal classification methodologies by using textual attributes/concepts as a proxy of image features.
% ConSe~\citep{Norouzi_ICLR14_ConSe} infers a target label from a semantic embedding composed of a weighted sum of concepts of the single predicted ImageNet label with word2vec embeddings in a fully zero-shot manner.
% While ConSe is conceptually similar to our Z-CBMs,  the zero-shot inference depends on the ImageNet label space, i.e., it cannot accurately predict target labels if there are no target-related labels in ImageNet.
% In contrast, our Z-CBMs directly decompose an input image feature into concepts via a concept bank, so they are not restricted to any external fixed-label spaces. 
% As a successor work of ConSe, A2C~\citep{Demirel_ICCV17_attributes2classname} learns input-to-attribute and attribute-to-label mapping by using attributed image datasets for zero-shot inference.
% While A2C succeeds in outperforming ConSe, the concepts to represent images are restricted to the training datasets, whereas our Z-CBMs are available without additional training and datasets.
% More recently, \cite{Menon_ICLR23_classification_via_llm_description} proposed a zero-shot classification method based on the correlation between the input features and the task-specialized texts generated by LLMs for each target class.
% However, it requires generating the task-specialized texts with LLM and restricting the inference algorithm to the CLIP style zero-shot classification. In contrast, Z-CBMs can be used for arbitrary tasks without external LLMs and arbitrary inference algorithms (e.g., linear probing).

\paragraph{Ethics Statement.} 
A potential ethical risk of our proposed method is the possibility that biased vocabulary contained in the concept bank may be output as explanations.
Since the concept bank is automatically generated from the caption dataset, it should be properly pre-processed using a filtering tool such as Detoxify~\citep{Detoxify} if the data source can be biased.

\paragraph{Reproducibility Statement.}
As described in Sec.~\ref{sec:implementation}~and~\ref{sec:experiment} , the implementation of the proposed method uses a publicly available code base. For example, the VLMs backbones are publicly available in the OpenAI CLIP\footnote{https://github.com/openai/CLIP} and Open CLIP\footnote{https://github.com/mlfoundations/open\_clip} GitHub repositories. All datasets are also available on the web; see the references in Sec.~\ref{sec:ex_settings} for details.
For the computation resources, we used a 24-core Intel Xeon CPU with an NVIDIA A100 GPU with 80GB VRAM.
More details of our implementation can be found in the attached code in the supplementary materials and we will make the code available on the public repository if the paper is accepted.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
