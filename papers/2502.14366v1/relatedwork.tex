\section{Related Works}
\subsection{Information Entropy}

Information entropy\cite{shannon} states that any information has redundancy, and the size of the redundancy is related to the probability or uncertainty of each symbol (number, letter or word) in the information. 

\paragraph{Entropy}
Entropy measures the unpredictability of a token given its context:
\[
H(s|C) = - \sum_{i} P(s_i|C) \log P(s_i|C),
\]
where \( P(s_i|C) \) is the probability of token \( s_i \) given the context \( C \). Higher entropy reflects greater diversity.

\subsection{Uniform Information Density Hypothesis}

The UID hypothesis posits that speakers optimize the communicative properties of their utterances by avoiding spikes in information, thereby maintaining a relatively uniform information profile over time. An example is \citet{FLORIANJAEGER201023} found that speakers are more likely to include optional elements, such as "that" in English subordinate clauses, when local information density increases. 

\paragraph{Surprisal}
Surprisal quantifies the unexpectedness of a token based on its likelihood:
\[
\text{Surprisal}(s|C) = -\log P(s|C).
\]
Tokens with lower surprisal values align better with the UID principle by maintaining smoother information density.

\subsection{Combining Entropy and UID in Language Generation}

While entropy and UID have been independently studied in various NLP tasks, their integration as complementary principles for generation optimization is a novel direction. Existing work has primarily focused on either enhancing diversity through entropy-based methods or achieving uniformity via UID. Our approach bridges this gap by leveraging both principles to achieve fluency, coherence, and balanced information density in generated outputs