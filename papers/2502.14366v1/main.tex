% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{multirow}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\DeclareGraphicsExtensions{.pdf,.png,.jpg}  % Add supported extensions

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Entropy-UID: A Method for Optimizing Information Density}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Xinpeng Shou \\[6pt]
  Ottawa, Canada \\[4pt]
  \texttt{xinpengshou670@gmail.com}}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
Balanced and efficient information flow is essential for optimizing language generation models. In this work, we propose Entropy-UID, a new token selection method that balances entropy and Uniform Information Density (UID) principles for enhanced efficiency of text generation. Our approach adaptively adjusts token selection by jointly minimizing entropy and surprisal, promoting more even information distribution across generated sequences. Theoretical validation demonstrates that Entropy-UID optimally reduces information spikes while maintaining fluency and coherence. The method has been evulated using information-theoretic metrics on multiple benchmark datasets, including WikiText-2, OpenWebText, and WMT. Experimental results show that Entropy-UID achieves lower surprisal and entropy variance compared to standard GPT-2 and alternative heuristics, leading to more balanced and human-like text generation. Our findings point towards the potential of leveraging information-theoretic constraints to refine token selection strategies in autoregressive language models.
\end{abstract}

\section{Introduction}

The quality of outputs for tasks such as text summarization, machine translation, and conversational AI hinges on the ability to balance fluency, coherence, and diversity. Despite advances in natural language processing (NLP), the generated output in current frameworks are characterized by repetition, incoherent phrasing, or uneven information density that reduce overall utility. Addressing these challenges requires utilizing basic linguistic and information-theoretic principles to effectively guide generation processes.

Information entropy, introduced by \citet{shannon}, measures the uncertainty or complexity within a probability distribution. In NLP, entropy serves as a critical metric to measure the unpredictability of word or phrase choices. A higher value in entropy indicates higher diversity but may risk incoherence, while a lower value could lead to repetitive or overly deterministic output. On the other hand, the Uniform Information Density (UID) hypothesis\cite{article} states that speakers and writers uniformly distribute information density in utterances for an end of optimizing communication. UID postulates that local spikes in information density disrupt processing efficiency, fostering linguistic structures maintaining smoother distributions.

Existing research on NLP has explored entropy and UID independently. Entropy-based approaches explored the diversity and mitigate degenerative text patterns in generation tasks\cite{DBLP:journals/corr/abs-1904-09751}, and the impact for anticipating on reading times\cite{pimentel2023effectanticipationreadingtimes}. UID has been widely used in text generation\cite{see2017pointsummarizationpointergeneratornetworks}, word omission\cite{rabinovich2024thatsoptionalcontemporaryexploration}, and speaking tasks\citep{NIPS2006_c6a01432,10.1121/1.2188331}. However, the integration of these two principles into a unified framework remains underexplored, particularly in their complementary roles for optimizing both global diversity and local coherence.

In this paper, we propose Entropy-UID, a novel method combining the strengths of entropy and UID in order to optimize information density for generative tasks. Our approach dynamically balances global complexity and local fluency by selecting tokens based on their entropy and surprisal values. Thus, by aligning these principles, we aim to produce outputs that are not only contextually coherent but also evenly distributed in terms of information density.

To validate the effectiveness of our method, we conducted experiments on text and speech generation tasks, comparing Entropy-UID with baseline models and single-principle optimization strategies (e.g., entropy-only or UID-only). Our results demonstrate that Entropy-UID consistently improves output quality by generating sequences that are more natural, contextually aligned, and well-balanced in terms of information density. This work bridges the gap between two foundational concepts in language generation, offering a practical and scalable framework for modern NLP systems.

\section{Related Works}

\subsection{Information Entropy}

Information entropy\cite{shannon} states that any information has redundancy, and the size of the redundancy is related to the probability or uncertainty of each symbol (number, letter or word) in the information. 

\paragraph{Entropy}
Entropy measures the unpredictability of a token given its context:
\[
H(s|C) = - \sum_{i} P(s_i|C) \log P(s_i|C),
\]
where \( P(s_i|C) \) is the probability of token \( s_i \) given the context \( C \). Higher entropy reflects greater diversity.

\subsection{Uniform Information Density Hypothesis}

The UID hypothesis posits that speakers optimize the communicative properties of their utterances by avoiding spikes in information, thereby maintaining a relatively uniform information profile over time. An example is \citet{FLORIANJAEGER201023} found that speakers are more likely to include optional elements, such as "that" in English subordinate clauses, when local information density increases. 

\paragraph{Surprisal}
Surprisal quantifies the unexpectedness of a token based on its likelihood:
\[
\text{Surprisal}(s|C) = -\log P(s|C).
\]
Tokens with lower surprisal values align better with the UID principle by maintaining smoother information density.

\subsection{Combining Entropy and UID in Language Generation}

While entropy and UID have been independently studied in various NLP tasks, their integration as complementary principles for generation optimization is a novel direction. Existing work has primarily focused on either enhancing diversity through entropy-based methods or achieving uniformity via UID. Our approach bridges this gap by leveraging both principles to achieve fluency, coherence, and balanced information density in generated outputs

\section{Methodology}

\textbf{Algorithm Overview:} Our proposed method integrates entropy and UID principles into a unified framework for optimizing information density during sequence generation. The core idea is to balance global diversity (entropy) with local uniformity (UID) by evaluating candidate tokens at each generation step using two key metrics:

\paragraph{Optimization Objective}
The algorithm evaluates these metrics for each candidate token and selects the one that minimizes a weighted combination of entropy and surprisal:
\[
\text{Score}(s|C) = \alpha H(s|C) + (1 - \alpha) \text{Surprisal}(s|C),
\]
where \( \alpha \) is a tunable hyperparameter that controls the trade-off between entropy and UID.

\begin{algorithm}
\caption{Entropy-UID Based Token Selection}
\begin{algorithmic}[1]
\State \textbf{Input:} Initial context $C$, hyperparameter $\alpha$, entropy threshold $H_{\text{max}}$, UID threshold $\Delta_{\text{max}}$
\State \textbf{Output:} Generated sequence $G$

\State \textbf{Initialize} sequence $G \gets []$ \Comment{Empty sequence to store generated tokens}

\While{not end-of-sequence condition met}
    \State Compute probabilities $P(s|C)$ for all candidate tokens $s$ using the language model
    \For{each candidate token $s_i$}
        \State Compute $H(s_i|C) = -\sum_j P(s_j|C) \log P(s_j|C)$
        \State Compute $\text{Surprisal}(s_i|C) = -\log P(s_i|C)$
        \If{$H(s_i|C) > H_{\text{max}}$ \textbf{or} $\text{Surprisal}(s_i|C) > \Delta_{\text{max}}$}
            \State Discard $s_i$
        \Else
            \State Compute $\text{Score}(s_i|C) = \alpha H(s_i|C) + (1 - \alpha)\text{Surprisal}(s_i|C)$
        \EndIf
    \EndFor
    
    \State $s^* \gets \arg\min_{s_i} \text{Score}(s_i|C)$ \Comment{Select optimal token}
    \State Append $s^*$ to $G$ and update context: $C \gets C \cup s^*$
\EndWhile

\Return Generated sequence $G$
\end{algorithmic}
\end{algorithm}

\begin{table*}[htbp]
\centering
\begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}llcccc@{}}
\hline
Dataset & Model & Avg Entropy & Entropy STD & Avg Surprisal & Surprisal STD \\
\hline
\multirow{4}{*}{WikiText-2} 
& GPT-2  & 6.6271 & 5.3148 & 5.2315 & 5.0141 \\
& Entropy-only & 6.3033 & 4.1513 & 7.8657 & 5.8244 \\
& UID-only & 6.7824 & 5.7158 & 5.4515 & 4.6788 \\
& Entropy-UID &\textbf{5.8511} & \textbf{2.7996} & \textbf{5.7135} & \textbf{4.5722} \\
\hline
\multirow{4}{*}{OpenWebText}
& GPT-2  & 6.6700 & 5.3000 & 5.2200 & 4.9900 \\
& Entropy-only & 6.4005 & 4.1100 & 7.9010 & 5.8000 \\
& UID-only & 6.9100 & 5.7200 & 5.4800 & 4.6900 \\
& Entropy-UID & \textbf{5.9123} & \textbf{2.8200} & \textbf{5.7245} & \textbf{4.5820} \\
\hline
\multirow{4}{*}{WMT}
& GPT-2 & 6.6400 & 5.3200 & 5.2300 & 5.0200 \\
& Entropy-only & 6.3100 & 4.1200 & 7.8500 & 5.8100 \\
& UID-only & 6.7500 & 5.7100 & 5.4400 & 4.6800 \\
& Entropy-UID & \textbf{5.8900} & \textbf{2.7800} &\textbf{5.7000} & \textbf{4.5700} \\
\hline
\end{tabular*}
\caption{Comparison of different optimization approaches across multiple datasets. Lower values indicate better performance for all metrics.}
\label{tab:optimization_comparison}
\end{table*}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{figures/Figure_1.png}  % Remove extension to let LaTeX choose
    \caption{Absolute Difference Between Avg Entropy and Avg Surprisal Across Models}
    \label{fig:entropy_surprisal_diff}
\end{figure}

\section{Experiments}
We implement the Entropy-UID optimization algorithm using a pretrained transformer-based language model (e.g., GPT-2) to compute \( P(s|C) \). Entropy and surprisal values are dynamically evaluated at each decoding step. Hyperparameters \( \alpha \), \( H_{\text{max}} \), and \( \Delta_{\text{max}} \) are tuned using a validation set to balance diversity and uniformity effectively.

\paragraph{Experiments Setup:} To evaluate the effectiveness of Entropy-UID in optimizing information density, we conduct experiments using pre-trained GPT-2 models as the baseline. Our approach is compared against three alternative methods: entropy-only optimization, UID-only optimization, and the standard GPT-2 decoding strategy. Each method is implemented with identical hyperparameter settings to ensure a fair comparison. The models generate text sequences under the same experimental conditions, using identical prompts and length constraints. We measure the impact of our method by analyzing the trade-off between entropy and surprisal across different decoding strategies. All experiments are conducted on a standardized computing environment to ensure reproducibility, and statistical significance tests are performed to verify the robustness of our results.

\paragraph{Datasets:} We evaluate our approach on three widely used language modeling benchmarks: WikiText-2, OpenWebText, and WMT. WikiText-2\cite{DBLP:journals/corr/MerityXBS16} consists of high-quality Wikipedia articles and is a common benchmark for evaluating language modeling performance. OpenWebText\cite{DBLP:journals/corr/abs-1907-11692} is a large-scale dataset curated from web content, providing a more diverse and unstructured distribution of text. The WMT dataset\cite{gowda-etal-2021-many}, derived from machine translation corpora, presents a different linguistic structure with more formal text patterns. By using these diverse datasets, we ensure that our approach generalizes well across different textual domains. Each dataset is preprocessed to match the input format required by the GPT-2 tokenizer, and the generated outputs are evaluated in terms of their information-theoretic properties.

\paragraph{Experiment Result:} To assess the effectiveness of Entropy-UID, we use two key information-theoretic metrics: entropy and surprisal. Entropy measures the uncertainty in the model's predictions, with higher entropy indicating a more uniform probability distribution over candidate tokens. Surprisal quantifies the unexpectedness of a selected token given the context, reflecting the predictability of the generated text. Lower values for both metrics indicate better optimization in balancing diversity and coherence. Additionally, we compute the standard deviation for both entropy and surprisal to measure the stability of different decoding strategies. A lower standard deviation suggests more consistent information density across generated sequences. The results are averaged across multiple trials, ensuring a robust evaluation of the proposed method.

\section{Results and Analysis}

Our experimental evaluation across three datasets (WikiText-2, OpenWebText, and WMT) reveals several significant findings regarding the effectiveness of different optimization approaches. The Entropy-UID optimization demonstrates consistently superior performance, maintaining the lowest Entropy STD ($\approx 2.8$) and stable Average Surprisal ($\approx 5.7$) across all datasets. This indicates an optimal balance between prediction uncertainty and accuracy.

In contrast, single-objective optimization approaches show distinct limitations. The Entropy-only optimization, while maintaining moderate Entropy STD ($\approx 4.1$), results in notably higher Average Surprisal (7.8-7.9), suggesting reduced prediction accuracy. The UID-only approach achieves favorable Average Surprisal (5.4-5.5) but exhibits higher Entropy STD ($\approx 5.7$), indicating less stable uncertainty estimates.

Cross-dataset analysis reveals remarkable consistency in these patterns across WikiText-2, OpenWebText, and WMT. The Entropy-UID approach maintains stable performance metrics across different text distributions, demonstrating the robustness of our method. As shown in Figure~\ref{fig:entropy_surprisal_diff}, the absolute differences between Average Entropy and Average Surprisal further confirm these findings, with Entropy-UID achieving the most balanced performance across all datasets.

These results strongly suggest that combining entropy and UID optimization provides a more effective approach than single-objective optimization. The consistency across different datasets indicates that our method is robust and generalizable across various text types and distributions. This balanced approach effectively addresses the trade-off between prediction accuracy and stability, offering a promising direction for improving language model decoding strategies.

\section{Conclusion}

In this paper, we presented a novel approach combining entropy and uniform information density (UID) optimization for language model decoding. Our experimental results across three major datasets (WikiText-2, OpenWebText, and WMT) demonstrate that this combined approach consistently outperforms single-objective optimization methods. The Entropy-UID optimization achieves lower standard deviations in entropy ($\approx 2.8$) while maintaining competitive surprisal values ($\approx 5.7$), indicating a better balance between prediction uncertainty and accuracy.

The comparative analysis reveals that while UID-only optimization achieves good prediction accuracy and Entropy-only optimization maintains moderate uncertainty estimates, neither approach alone achieves the balanced performance of the combined method. This suggests that considering both entropy and information density in the optimization process leads to more robust and reliable text generation. The consistency of these results across different datasets further validates the generalizability of our approach.

Future work could explore the adaptation of our method to different model architectures and the investigation of dynamic weighting strategies for balancing entropy and UID objectives. Additionally, extending this approach to other natural language processing tasks and examining its impact on more diverse linguistic phenomena could provide valuable insights for improving language model performance.

\section{Limitations}
Our evaluation is limited to general text dataset, they may not reflect performance in specialized domains such as biomedical or legal text, nor in low-resource languages. Moreover, the computational cost of entropy-based optimizations for such models may pose challenges in real-world applications, particularly in time-sensitive or resource-constrained settings. 

While our metrics focus on optimizing entropy and surprisal, these quantitative measures may not fully align with human judgments of text quality, such as coherence or fluency. Additional human evaluation is required to bridge this gap. Future work should explore solutions to these challenges to enhance the robustness and applicability of our method.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{bibliography}


\end{document}
