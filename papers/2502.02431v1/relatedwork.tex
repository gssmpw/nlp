\section{Related Work}
We review the existing literature on accelerated SGD variants and optimization algorithms that are directly relevant to our work.

\citet{jain18accelerate} introduced an accelerated SGD variant that demonstrated improved convergence rates for the least-squares problem. \citet{kidambi2018on} further simplified the update rule for this variant and formally established that momentum does not provide acceleration in this specific case. Subsequent works~\citep{Liu2020Accelerating, vaswaniconvergence, agnes} extended these results to general convex and strongly convex functions under various theoretical assumptions.

Over the years, several optimizers have been proposed that exhibit similarities to the accelerated SGD variants described above. \citet{lucas2018aggregated} introduced a method that incorporates a weighted sum of multiple momentum terms, each with distinct coefficients, to compute the final update. \citet{ma2018quasihyperbolic} developed an optimizer explicitly inspired by the theoretical framework established in \citet{jain18accelerate}. More recently, \citet{lion} proposed an optimizer discovered via a genetic search algorithm, which, similar to previous accelerated SGD variants, assigns different weights to the gradient and the momentum coefficient in the update step. Additionally, \citet{ademamix} introduced a method that blends two distinct momentum scales in the final update.