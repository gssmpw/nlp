\section{Related Work}
We review the existing literature on accelerated SGD variants and optimization algorithms that are directly relevant to our work.

____ introduced an accelerated SGD variant that demonstrated improved convergence rates for the least-squares problem. ____ further simplified the update rule for this variant and formally established that momentum does not provide acceleration in this specific case. Subsequent works____ extended these results to general convex and strongly convex functions under various theoretical assumptions.

Over the years, several optimizers have been proposed that exhibit similarities to the accelerated SGD variants described above. ____ introduced a method that incorporates a weighted sum of multiple momentum terms, each with distinct coefficients, to compute the final update. ____ developed an optimizer explicitly inspired by the theoretical framework established in ____. More recently, ____ proposed an optimizer discovered via a genetic search algorithm, which, similar to previous accelerated SGD variants, assigns different weights to the gradient and the momentum coefficient in the update step. Additionally, ____ introduced a method that blends two distinct momentum scales in the final update.