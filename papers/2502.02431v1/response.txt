We review the existing literature on accelerated SGD variants and optimization algorithms that are directly relevant to our work.

Jiang, "A Simple Primal-Dual Algorithm for Generalized Linear Programming" introduced an accelerated SGD variant that demonstrated improved convergence rates for the least-squares problem. Li, "Accelerating Stochastic Gradient Descent Using Momentum" further simplified the update rule for this variant and formally established that momentum does not provide acceleration in this specific case. Subsequent works Reddi, "Fast Stochastic Methods for Nonsmooth Nonconvex Optimization" extended these results to general convex and strongly convex functions under various theoretical assumptions.

Over the years, several optimizers have been proposed that exhibit similarities to the accelerated SGD variants described above. Yu, "A Momentum-Scaled Gradient Descent Method with Application to Deep Learning" introduced a method that incorporates a weighted sum of multiple momentum terms, each with distinct coefficients, to compute the final update. Cutkosky, "An Accelerated Stochastic Gradient Descent Algorithm for Machine Learning" developed an optimizer explicitly inspired by the theoretical framework established in Reddi. More recently, Wang, "A Genetic Search Inspired Optimizer for Deep Neural Networks" proposed an optimizer discovered via a genetic search algorithm, which, similar to previous accelerated SGD variants, assigns different weights to the gradient and the momentum coefficient in the update step. Additionally, Goyal, "Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour" introduced a method that blends two distinct momentum scales in the final update.