



Structured sparsity is currently supported by various commercial architectures.
In particular, the 2:4 sparsity is supported by state-of-the-art GPUs, \emph{i.e.,} Nvidia A100 \cite{NVIDIA_A100}, Nvidia H100 \cite{NVIDIA_H100} and AMD MI300 \cite{AMD_CDNA_3}, as well as the new AMD Versal AIE-ML 
processors \cite{AMD_AIE_ML_architecture_manual, AMD_AIE_ML_kernel_guide}.
In academic research, hardware support for structured sparsity has also been investigated in ASIC DNN accelerators \cite{S2TA_HPCA_2022, STA_arch_letters_2020, HighLight_MIT_2023}.
Moreover, prior academic research \cite{Sparse_tensor_GPUs_2019} introduces structured sparsity features in the tensor cores of modern GPUs.
Finally, structured sparsity support has also been proposed inside the matrix engines \cite{Vegeta_HPCA_2023} and vector units \cite{RISC_V_CPU_structured_sparsity_DATE_2024} of CPUs.
Considering all prior work, we are the first to incorporate structured sparsity support into the FPGA architecture.
Moreover, our proposed in-fabric blocks support multiple levels of structured sparsity (beyond merely 2:4), to enable efficient acceleration for most contemporary DNN models.




In-fabric hard blocks have been commercially incorporated into several AI-optimized FPGAs.
More specifically, the Intel Stratix 10 NX \cite{Stratix_10_NX_FPGA_2021} replaces the traditional DSP slices with tensor blocks.
These tensor blocks comprise multiple dot-product engines to more efficiently support AI applications.
Very recently, Intel announced the Agilex-5 FPGAs \cite{Sergey_Intel_TB_Agilex_5_FCCM_2024, Intel_Agilex_5_tensor_blocks}, which introduce new AI-enhanced DSP blocks.
These new DSPs include multiple dot-product operations, while also maintaining various features of the traditional Agilex DSP blocks.
Finally, the Achronix Speedster7t FPGAs incorporate machine learning processor (MLP) blocks \cite{Achronix_Speedster, Achronix_Speedster_2024}.
These MLP hard blocks feature multiplier arrays, adder trees, accumulators and tightly coupled memories to the computational blocks.

In-fabric enhancements have also been proposed in academia.
In \cite{TS_Aman_FPGA_2021, Aman_TS_TRETS_2022}, the authors enrich the FPGA architecture with 2D systolic tensor slices, showing substantial efficiency benefits in DNN acceleration over traditional FPGAs.
Moreover, in \cite{PIR_DSP_FCCM_2019}, a special pattern of dedicated wires between DSP blocks is proposed, which enables more efficient mapping of systolic arrays on FPGAs.
However, all aforementioned commercial and academic in-fabric blocks target primarily dense computation.
In this work, we architect in-fabric slices that enable both sparse and dense computation.
We propose incorporating the SST slices, which employ a 2D systolic dataflow similar to \cite{TS_Aman_FPGA_2021, Aman_TS_TRETS_2022}, but are augmented with sparsity features.
Moreover, we introduce vertical dedicated wires between the SSTs to more efficiently map GEMM accelerators into FPGA architectures.



Finally, prior research \cite{LSTM_fine_grained_sparsity_FPGA_2019, Fine_grained_structured_sparsity_FPL_2021, N_M_sparse_transformers_FPGA_VLSI_2022, Fine_grained_Neural_ODE_FPGA_2023, LAMPS_FCCM_2024} implement DNN accelerators that support multi-level structured sparsity on traditional (non AI-optimized) FPGAs. 
In contrast, we also introduce the novel 1:3 sparsity pattern, which is absent from any prior work and offers increased sparsity flexibility.
In this work, we demonstrate the importance of this flexibility for various state-of-the-art DNNs.
Furthermore, our SST-based GEMM designs show remarkable performance and area efficiency gains compared to traditional FPGAs.




























