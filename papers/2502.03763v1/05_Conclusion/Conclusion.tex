


Structured sparsity support has been incorporated in state-of-the-art GPUs, \emph{e.g.,} Nvidia H100 and AMD MI300, as well as the new Versal AIE-ML processors.
Although FPGA architectures have been enriched with in-fabric blocks for DNN acceleration, these blocks are primarily designed for dense operation.
This leads to insufficient computation for most contemporary DNN models, which display varying levels of sparsity.
To effectively address this challenge, we propose flexible in-fabric blocks, named SST slices, that support multiple levels of structured sparsity. 
We show that our sparse GEMM accelerators exploiting the SST slices offer substantial performance, scalability, and area advantages over traditional FPGAs. 
Demonstration on various state-of-the-art sparse DNN models utilizing our SST slices, exhibits up to 3.52$\times$ speedup with marginal accuracy degradation compared to dense in-fabric acceleration.

\vspace{-1.2mm}
