Sparsity, which refers to zeros in weight or activation tensors, is an inherent attribute of contemporary deep neural networks (DNNs) \cite{Sparsity_Hoefler_2021}.
Sparsity arises from complex interactions among various  optimization techniques in modern DNN models \cite{HighLight_MIT_2023}.
For instance, over-parameterization is prevalent because it enables easier training and better generalization \cite{Sparsity_Hoefler_2021}.
Pruning is typically applied for this over-parameterization redundancy, resulting in zeros within the weight tensors (sparsity).
The exploitation of sparsity to reduce computational and memory requirements has been a common strategy in the design of efficient DNN accelerators  \cite{S2TA_HPCA_2022}. 


Typically, zeros in DNNs are distributed randomly within the data tensors \cite{Sparsity_Hoefler_2021, S2TA_HPCA_2022}. 
This random sparsity is commonly referred to as \textit{unstructured} sparsity.
Leveraging unstructured sparsity has been the main focus of numerous sparse hardware accelerators over the past years, targeting both ASICs \cite{EIE_ISCA_2016, Cambricon_X_MICRO_2016, OuterSpace_HPCA_2018, SMT_SA_2019, ExTensor_2019, SparTen_2019, Eyeriss_v2_2019, SpArch_2020} as well as FPGAs \cite{unstructured_sparsity_SpWA_FPGA_DAC_2018,
unstructured_sparsity_CNN_FPGA_FCCM_2019,
unstructured_sparsity_FPGA_UIUC_2019,
unstructured_sparsity_FPGA_Abhishek_2021,
unstructured_sparsity_CNN_TCS_2021, unstructured_FPGA_TCAD_2022}.
However, unstructured sparse accelerators require complex hardware structures, which result in high area overheads \cite{HighLight_MIT_2023}. 
This area increase leads to substantial rise in energy consumption, \emph{e.g.,} up to 71\% \cite{S2TA_HPCA_2022}, compared to dense architectures.
Moreover, the random location of zeros in unstructured sparsity causes unpredictable and low hardware utilization, rendering inference speedup inefficient as well as challenging \cite{STA_arxiv_2020, STA_arch_letters_2020, EIE_ISCA_2016}.   



To address these challenges in sparsity acceleration, 
\textit{structured} sparsity has been proposed recently \cite{Nvidia_accelerate_sparse_2021, S2TA_HPCA_2022, Vegeta_HPCA_2023}.
% Fine-grained 
Structured sparsity imposes constraints on the sparsity patterns in data tensors, enabling low area overheads and highly energy-efficient hardware enhancements for sparsity exploitation.
To this end, Nvidia A100 GPUs introduced support for the 2:4 structured sparsity, 
where two out of every four \textit{consecutive} values must be non-zero.
Furthermore, structured sparse ASIC accelerators have also been proposed, demonstrating up to 3.1$\times$ less energy consumption per inference compared to state-of-the-art unstructured sparse accelerators \cite{S2TA_HPCA_2022}. 


Alongside GPUs and ASICs, FPGAs have emerged as a promising candidate for accelerating the rapidly evolving DNNs, due to their high flexibility of reconfiguration.
Contemporary FPGA architectures have been enhanced to more  
effectively support the high computational demands of DNNs.
In particular, Intel employs in-fabric blocks comprising multiple dot-product engines \cite{Stratix_10_NX_FPGA_2021, Sergey_Intel_TB_Agilex_5_FCCM_2024}, while AMD introduced out-of-fabric programmable vector processors \cite{AMD_AIE_ML_architecture_manual}.
Academic researchers have also proposed in-fabric blocks that employ a 2D systolic dataflow \cite{TS_Aman_FPGA_2021, Aman_TS_TRETS_2022}, exhibiting substantial benefits over traditional FPGAs for various DNN workloads.



However, these in-fabric blocks are primarily designed for dense DNN acceleration, limiting their efficiency and applicability to most real-world DNNs, which inherently exhibit sparsity.
To address this challenge, we propose incorporating flexible in-fabric slices into the FPGA architecture to efficiently support both \textit{sparse} and \textit{dense} DNN workloads.
We employ 2D systolic slices similar to \cite{TS_Aman_FPGA_2021, Aman_TS_TRETS_2022}, which are augmented with structured sparsity features and further optimizations.
Our novel systolic sparse tensor (SST) slices are architected with the following principal properties: 

\vspace{-0.10cm}
\begin{itemize}
  \item \textbf{Efficiency}. (\textbf{i}) Sparsity features should exhibit low area overheads. (\textbf{ii}) Translation of sparsity to actual DNN acceleration. (\textbf{iii}) Maximization of data reuse in sparse and dense DNNs. (\textbf{iv}) Efficient sparse format for storing the non-zero values. 
  \item \textbf{Sparsity Flexibility}. These in-fabric blocks must be flexible, \emph{i.e.,} support multiple structured sparsity levels (percentage of zeros) to accelerate a wide range of DNN workloads.
  % the majority of DNN workloads. 
\end{itemize}
\vspace{-0.10cm}



The SST slices are systolic-based and utilize a highly efficient index-based sparse format, effectively meeting the aforementioned key properties for \textit{efficiency}.
Regarding the \textit{sparsity flexibility}, SSTs support multiple sparsity levels, \emph{i.e.,} dense, 2:4 (50\% sparsity), 1:3 (66.7\%) and 1:4 (75\%).
These levels align with the most \textit{common} sparsity levels in DNNs, since sparsity higher than 75\% typically leads to significant accuracy degradation \cite{Sparse_tensor_GPUs_2019, STA_arxiv_2020, N_M_sparse_transformers_FPGA_VLSI_2022, Learning_N_M_ICLR_2021, Learning_best_N_M_NeurIPS_2022}.
Prior works have exploited the 2:4 and 1:4 sparsity patterns to design sparse accelerators using traditional FPGAs \cite{LAMPS_FCCM_2024, N_M_sparse_transformers_FPGA_VLSI_2022}, while others have incorporated these patterns into CPU datapaths \cite{Vegeta_HPCA_2023, RISC_V_CPU_structured_sparsity_DATE_2024}.
In contrast, we propose a novel in-fabric FPGA block that additionally supports a new 1:3 sparsity pattern.
The 
1:3 sparsity bridges the gap between 50\% and 75\% sparsity, increasing flexibility and providing both acceleration and storage benefits.
Furthermore, 1:3 sparsity can be supported in SSTs 
\textit{without incurring additional area overheads},
by effectively reusing the area allocated for 2:4 and 1:4.
This sparsity flexibility enables the development of tailored solutions for DNN models, since each model exhibits distinct levels of sparsity.




 
The proposed SST slices support 8-bit integer (int8) and brain floating-point (bfloat16) precisions, since these are the most commonly used data types in DNN accelerators \cite{TPUv42021, TPUV2_v3_2021}.
% As an additional FPGA-specific optimization, 
Additionally, we introduce \textit{dedicated} interconnects between the SST slices to facilitate their efficient integration within the FPGA architecture.
These dedicated interconnects demonstrate substantial routing savings in FPGAs compared to prior work on 2D systolic blocks \cite{TS_Aman_FPGA_2021, Aman_TS_TRETS_2022}, where such interconnects are not employed. 





The SST slices deliver highly efficient acceleration of sparse and dense general matrix multiplication (GEMM) in the FPGA fabric.
Our main focus is GEMM, as most current state-of-the-art DNNs across various applications are Transformer-based \cite{Transformers_SOTA_2023, Transformer_SOTA_Elsevier_2024}, with GEMM serving as the core computation. 
To the best of our knowledge, this is the first work to support structured sparsity in the FPGA architecture.
Our key contributions are summarized below:



\vspace{-0.10cm}
\begin{itemize}
  \item  A \textit{generalizable methodology} for incorporating structured sparsity into in-fabric FPGA blocks. Our proposed SST slices employ a 2D systolic dataflow and support multiple levels of sparsity. Besides dense, 2:4 and 1:4, we also introduce a \textit{new 1:3 sparsity pattern} to provide greater flexibility, enabling efficient acceleration for the majority of DNN models.
  \item  We introduce dedicated interconnects to effectively integrate the SST slices in the FPGA architecture. These interconnects show \textit{significant routing savings, up to \textbf{31.2\%}} on GEMM accelerators, compared to utilizing only global routing resources, as proposed in prior work on 2D systolic blocks.
  \item Demonstration on sparse GEMM accelerators leveraging our SST slices show considerably \textit{higher attainable FPGA frequency, up to \textbf{4.4$\times$} for int8 and \textbf{5$\times$} for bfloat16}, as well as \textit{remarkable area reduction, up to \textbf{7$\times$} for int8 and \textbf{10.9$\times$} for bfloat16}, when comparing with traditional FPGAs.
  \item Our evaluation on state-of-the-art sparse ViT and CNN models showcases up to \textit{\textbf{3.52$\times$} speedup} when exploiting our SST slices compared to dense in-fabric blocks, with \textit{low area overheads of \textbf{10.2\%} and \textbf{13.3\%} for int8 and bfloat16}, respectively.
\end{itemize}
\vspace{-0.20cm}


