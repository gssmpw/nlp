




\begin{table}[t]
\vspace{-0.30cm}
\centering
\caption{FPGA logic tile and routing parameters in COFFE.}

\setlength\tabcolsep{1pt}
% \renewcommand{\arraystretch}{1.0}
\resizebox{0.92\linewidth}{!}{
% \vspace{-0.10cm}
\begin{tabular}{cc|cc}
\Xhline{2.5\arrayrulewidth} 

\textbf{Parameter} & \textbf{Value} & \textbf{Parameter} & \textbf{Value} \\

\hline
\hline

FLEs per CLB ($N$) & 10 & Frac. LUT size ($K$) & 6\\
FLE independent inputs & 2 & Adders per FLE & 2\\
Channel width ($W$) & 300 & Wire length ($L$) & 4/16\\
CLB inputs ($I$) & 60 & CLB outputs ($O$) & 40\\
FLE outputs to routing ($O_{r}$) & 4 & Feedback FLE outputs ($O_{fb}$) & 2\\
SB flexibility ($F_{s}$) & 3 & Input connection flex. ($F_{cin}$) & 0.15\\
Output connection flex. ($F_{cout}$) & 0.1 & Input crossbar flex. ($F_{clocal}$) & 0.5\\

\Xhline{2.5\arrayrulewidth}

\end{tabular}
}

\label{tb:FPGA_tile_architectural_parameters}

\vspace{-0.50cm}

\end{table}



\subsection{FPGA Architecture}
\label{subsec:FPGA_architecture}

The traditional FPGA architecture comprises 
% configurable logic blocks (CLBs), 
CLBs, BRAMs, DSP slices and routing resources, \emph{i.e.,} connection and switch boxes (CBs and SBs).
We enrich the FPGA architecture with our proposed in-fabric SST slices.
The SST columns repeat every 15 FPGAs columns, occupying only 14\% of the total FPGA area, which is sufficient for DNN applications as also found in \cite{Aman_TS_TRETS_2022} (refer to Fig. \ref{fig:GEMM_SST_3x3_int8} for the layout of the proposed FPGA architecture).
We exploit the automated transistor sizing tool, COFFE \cite{3D_FPGAs_COFFE_Boutros_FPT23, COFFE2_TRETS_2019}, to model the delays and areas of the FPGA components. 
These delays and areas are subsequently used to describe the FPGA architecture in the Verilog-to-Routing (VTR) tool flow \cite{VTR_8_2020}, facilitating FPGA architecture exploration.
% COFFE performs SPICE simulations to iteratively optimize the transistor sizes of various FPGA circuits.    
We utilize the 7nm FinFET ASAP7 predictive process design kit (PDK) \cite{ASAP7_2016}, for the SPICE simulations conducted by COFFE.
% In this work,
Specifically, we exploit the typical corner (\emph{TT}) ASAP7 model \cite{ASAP7_2016, ASAP7_invited_2017}. 
% where transistors operate at 0.7V \cite{ASAP7_2016, ASAP7_invited_2017}. 
Finally, COFFE was run with cost function of $area \times delay$ for four transistor sizing iterations (2--4 iterations are typically sufficient 
% as found in 
\cite{Original_COFFE_2013}).

We model a modern Agilex-like FPGA similar to \cite{TS_Aman_FPGA_2021, Aman_TS_TRETS_2022}.
Table \ref{tb:FPGA_tile_architectural_parameters} shows the FPGA logic (CLB) and routing (CBs and SBs) architectural parameters used in COFFE.
Each CLB contains 10 fracturable logic elements (FLEs), which consist of 6-input lookup tables (LUTs), registers and two adder bits.
Since COFFE does not model 7nm optimized memory circuits, we obtain the model of the 20Kbit BRAMs used in the recent work \cite{3D_FPGAs_COFFE_Boutros_FPT23} for 7nm FPGAs.
The delay and area values of these BRAMs are conservatively estimated using the 14nm Stratix 10 architecture values, as described in \cite{3D_FPGAs_COFFE_Boutros_FPT23}. 
Finally, we leverage the DSP used in \cite{Aman_TS_TRETS_2022}, which supports multiple precisions and modes, closely matching the commercial Agilex DSP \cite{Intel_Agilex_DSP_2020}.
However, since this DSP is modeled in 22nm, we scale down its area and delay values to 7nm, exploiting the scaling factors from \cite{Scale_VLSI_2017}.






\subsection{SST Slices Implementation}
\label{subsec:SST_slices_implementation}

We implement the SST slices utilizing the  ASAP7 7.5--track standard cell library \cite{ASAP7_invited_2017, ASAP7_github}.
Specifically, we use the regular threshold voltage (RVT) cells of the typical (\emph{TT}) corner of the library.
% which operate at 0.7V.
COFFE's hybrid flow was exploited, where the core of the SSTs is implemented using the standard cell flow, while the interface to the programmable routing (local crossbar, drivers for dedicated wires, etc.) is implemented in the full custom flow using SPICE simulations.
The standard cell flow uses Synopsys Design Compiler for synthesis, Cadence Innovus for PnR and Synopsys Primetime for timing analysis.
The full custom flow uses the \emph{TT} ASAP7 SPICE model and COFFE was run with the same configurations as in the previous section.



\begin{table}[t]
\vspace{-0.30cm}
\centering
\caption{Area and freq. of SST \emph{vs.} SDT\_GIO slices (post PnR).}

\setlength\tabcolsep{2.5pt}
\renewcommand{\arraystretch}{1.0}
\resizebox{0.80\linewidth}{!}{
% \vspace{-0.10cm}
\begin{tabular}{c|ccc}
\Xhline{2.5\arrayrulewidth}

\multicolumn{2}{c}{\textbf{In-fabric slice}} & \textbf{SST} & \textbf{SDT\_GIO}\\
\hline
\hline
& Standard-cell core & 4530.8 (+22.9\%) & 3687.8 \\

& Input crossbar & 1511.4 & 985.6\\

\textbf{Area} & Dedicated output routing & 40.2 & 0\\

\textbf{($\boldsymbol{\mu m^2}$)} & Switch Box (SB) & 1374.3 & 1603.4\\

& Connection Box (CB) & 734.9 & 571.6\\

& \textbf{Total tile} & \textbf{8191.6 (+19.6\%)} & \textbf{6848.4}\\

\hline

\textbf{Freq.} & 
int8 & 928.6 & 935.3 \\

\textbf{(MHz)} & bfloat16 & 838.1 & 850.5\\


\Xhline{2.5\arrayrulewidth}

\end{tabular}
}

\label{tb:SST_delay_area_vs_SDT_GIO}

\vspace{-0.55cm}

\end{table}



To accurately quantify the area overhead of the sparsity enhancements described in Sec. \ref{subsec:Sparse_Processing_Element}, we remove the sparsity features of the SST slices, to implement a 2D systolic dense tensor slice.
We retain all the design features delineated in Sec. \ref{subsec:Sparse_Tensor_slices_architecture}, except the dedicated wires.
For this dense tensor slice, we utilize the approach proposed in \cite{TS_Aman_FPGA_2021, Aman_TS_TRETS_2022}, where all I/O ports are connected to global routing.
We refer to this tensor slice as SDT\_GIO, which is similar to \cite{TS_Aman_FPGA_2021, Aman_TS_TRETS_2022}.
The dense SDT\_GIO slices are used as the baseline comparison with our proposed sparse SSTs, while also allowing for quantification of the routing savings due to dedicated wires in SSTs 
% in GEMM 
(Sec. \ref{subsec:Dedicated_wires_benefits}).




Table \ref{tb:SST_delay_area_vs_SDT_GIO} shows the area and frequency results for both the SST and SDT\_GIO slices obtained from COFFE. 
At the standard-cell core level, we observe a low area overhead of 22.9\% for the sparsity enhancements of the SST slices.
For both SST and SDT\_GIO slices, we implement a 50\% populated input crossbar to enhance their routability inside the FPGA fabric \cite{COFFE2_TRETS_2019}. 
% \todo{crossbar delay not mentioned. Maybe that's ok?}
Each SST slice has 333 global routing inputs and 202 outputs, along with 256 dedicated inputs/outputs.
In contrast, the SDT\_GIO slice has 259 global routing inputs and 258 outputs, without dedicated I/Os.
Given the aforementioned global routing I/Os, the SST slices need to access 6 SBs, thus spanning 6  CLB (logic) tiles.
However, since more outputs are needed for the SDT\_GIOs (primarily due to absence of dedicated wires), they need to span 7 CLB tiles.
Therefore, when calculating the total tile area for both slices (standard-cell core and routing interface), we obtain a low area increase of \textbf{19.6\%} for 
SSTs \emph{vs.} SDT\_GIOs (Table \ref{tb:SST_delay_area_vs_SDT_GIO}).
Finally, we observe that both slices can operate at high frequencies for the supported precisions (> 838 MHz in all cases), similar to commercial 7nm FPGA hard blocks \cite{Versal_DSP_frequencies, Agilex_5_frequencies}. 




% SST -> inputs: 333, outputs: 202, ded inputs/outputs : 256
% 50\% crossbar delay: 177.1 ps

% SDT\_GIO -> inputs: 259, outputs: 258
% 50\% crossbar delay: 149.9 ps

\begin{figure}[t]
\vspace{-0.50cm}
\centering
\subfloat[int8]{\includegraphics[width=0.48\linewidth]{04_Evaluation/wirelength_SST_vs_GIO_int8.pdf}
\label{fig:wirelength_SST_vs_GIO_int8}}
\hfill
\subfloat[bfloat16]{\includegraphics[width=0.48\linewidth]{04_Evaluation/wirelength_SST_vs_GIO_bf16.pdf}
\label{fig:wirelength_SST_vs_GIO_bf16}}

\vspace{-0.3cm}

\caption{Routing wirelength comparison of \textit{dense} GEMM mapped to SST and SDT\_GIO slices, for various SA sizes.}
\label{fig:wirelength_SST_vs_GIO}
\vspace{-0.45cm}
\end{figure}


\subsection{Wirelength Gains Utilizing Dedicated Wires}
\label{subsec:Dedicated_wires_benefits}



To quantify the benefits of the proposed dedicated wires in SSTs, we implement \textit{dense} GEMMs utilizing our SSTs slices (Fig. \ref{fig:GEMM_2D_array_SSTs}a) and compare them with \textit{dense} GEMMs using SDT\_GIOs slices, which support only global routing.
To this end, similar to the enhanced FPGA architecture with SST slices, we create an architecture that has SDT\_GIOs instead of SSTs. 
To facilitate the exploration, we implement a parametric Python script that generates the Verilog code for arbitrary GEMM sizes.
% the parameters $X, Y$, which determine the SA size of the GEMM accelerator (Sec. \ref{subsec:Matrix_multiplication_mapping}).
The Verilog designs are synthesized and implemented on the two aforementioned FPGA architectures (one with SST and one with SDT\_GIO slices), exploiting the VTR flow.
In all cases, we perform a 10 seed-sweep in VTR, and report the design that attains the maximum FPGA frequency.


% minimum amount of BRAMs in this work => no just say 512 deep 40-bits mode

% -------wirelength reduction up to 
% ------- for int8 from 15.5\% up to 29.9\%
% ------- for bfloat16 from 23.9\% up to 31.2\%

Fig. \ref{fig:wirelength_SST_vs_GIO} illustrates the total routing wirelength of the two dense GEMM accelerators, for various SA sizes at both int8 and bfloat16 precisions.
These designs use 512-element deep banks for buffers $A$, $B$ and $C$ (Sec. \ref{subsec:Matrix_multiplication_mapping}), exploiting the 512x40-bit BRAM configuration mode (also found in recent Intel FPGAs \cite{Intel_Agilex_5_M20Ks, Intel_Agilex_7_M20Ks, Intel_Stratix_10_M20Ks}).
For the SA sizes shown in Fig. \ref{fig:wirelength_SST_vs_GIO}, we observe a significant wirelength reduction due to dedicated wires in the SSTs, ranging from \textbf{15.5--29.9\%} for int8 precision, compared to the SDT\_GIO-based GEMM designs.
Similarly, for bfloat16, the wirelength reduction ranges from \textbf{23.9--31.2\%}, showcasing the importance of vertical dedicated wires for 2D GEMM implementations on FPGA architectures.


Regarding the maximum attainable FPGA frequency, both dense GEMM implementations 
% mapped to SST and SDT\_GIO slices 
achieve high frequencies ranging from 668--731 MHz.
In particular, the smallest designs, \emph{i.e.,} 20x20 SA size, attain the highest frequency of $\sim$731 MHz for both SSTs and SDT\_GIOs at int8 and bfloat16 precisions.
Similarly, the largest designs, \emph{i.e.,} 40x40 SA size, 
% ($X,Y$=10 in this case, utilizing 100 SSTs / SDT\_GIOs), 
both attain $\sim$668 MHz. 
We notice that dense GEMMs mapped to SST and SDT\_GIO slices attain almost similar FPGA frequencies (<1\% difference for all tested SA sizes depicted in Fig. \ref{fig:wirelength_SST_vs_GIO}).
This can be explained by the fact that the critical path is typically in the control logic implemented in CLBs, which is the same for both dense GEMMs mapped to SSTs and SDT\_GIOs. 

% Finally, our results demonstrate the high performance and scalability of utilizing in-fabric 2D systolic slices for dense GEMM accelerators.




\subsection{Sparse SST-based GEMM Implementation}
\label{subsec:Sparse_GEMM_implementation}


In this section, we present the \textit{sparse} GEMM implementation shown in Fig. \ref{fig:GEMM_2D_array_SSTs}b, which allows dynamic configuration for all supported sparsity modes in SSTs, \emph{i.e.,} dense, 2:4, 1:3 and 1:4.
We use two baselines for our comparison.
First, we compare with a GEMM design mapped to SDT\_GIOs using a FPGA architecture that replaces SSTs with SDT\_GIOs. 
Since SDT\_GIOs have a 2D dense systolic dataflow that does not support sparse computation (Sec. \ref{subsec:SST_slices_implementation}), 
we retain the zeros of the sparse matrices, therefore operating similarly to dense. 
Hence, this implementation is similar to that shown in the previous section (Sec. \ref{subsec:Dedicated_wires_benefits}), however it has the same amount of on-chip memory as the sparse SST-based design. 
In particular, we implement four memory banks in the vertical dimension to feed each SDT\_GIO (similar to Fig. \ref{fig:GEMM_2D_array_SSTs}b).
This allows for the same data reuse opportunities as in the SST-based design, thus enabling a fair comparison.
Second, we implement the same sparse SST-based design, mapped to CLBs and DSPs of a traditional FPGA architecture, \emph{i.e.,} without SSTs and SDT\_GIOs.
Finally, we note that all tested designs utilize 512-element deep banks and are based on a 10 seed-sweep maximization of FPGA frequency, similar to Sec. \ref{subsec:Dedicated_wires_benefits}.





\subsubsection{FPGA Frequency}
Fig. \ref{fig:freq_SST_vs_GIO_vs_DSP} depicts the FPGA frequency of the sparse GEMMs mapped to SST slices, SDT\_GIO slices as well as CLBs and DSPs.
First, we observe the high FPGA frequency of GEMMs mapped to SST and SDT\_GIO slices, ranging from 596--625 MHz for int8 and from 578--610 MHz for bfloat16. 
We note that the slightly lower frequencies of the SDT\_GIO-based GEMMs compared to the previous section are attributed to the higher BRAM usage.
For all tested SA sizes, we notice a negligible difference in FPGA frequency (<1\%) between the SST-based and the SDT\_GIO-based GEMMs. 
However, when comparing the SST-based \emph{vs.} the CLB+DSP GEMMs, we observe considerably higher frequencies, ranging from \textbf{2.8--4.4$\times$} and from \textbf{2.7--5$\times$} for int8 and bfloat16, respectively.







Second, note that the sparse GEMMs utilizing SSTs scale effectively when increasing the SA size.
In particular, we observe a minor decrease in FPGA frequency (4.6\% and 5.2\% for int8 and bfloat16, respectively), when considering the 40x40 \emph{vs.} the 20x20 SA size.
In contrast, the sparse GEMM mapped to CLBs and DSPs, does not scale as effectively, since we notice a substantial frequency drop of 36.6\% and 49.6\% for int8 and bfloat16, respectively.
These results emphasize the significant performance advantages and scalability of in-fabric sparse hard blocks compared to traditional FPGAs.



% int8 frequency results:
% difference in frequency of SST and SDT\_GIO within 1\% in all cases, ranging from 596--625 MHz.

% SST vs CLB+DSP ranges from 2.8--4.4$\times$

% bfloat16 frequency results:
% difference in frequency of SST and SDT\_GIO within 1\% in all cases, ranging from 578--610 MHz.

% SST vs CLB+DSP ranges from 2.7--5$\times$







\subsubsection{GEMM Area}

We calculate the total area of our GEMM designs as the summation of the utilized logic area and the FPGA routing area.
While the utilized logic area is reported in VTR, the tool  does not report the used routing area.
Thus, we estimate the routing area by summing the area of all utilized multiplexers in both the SB and CB resources of our designs.
Fig. \ref{fig:area_SST_vs_GIO_vs_DSP} shows the total area of our sparse GEMM designs in minimum width transistor area (MWTA) units \cite{VTR_8_2020}.
% for several SA sizes.
As shown in Table \ref{tb:SST_delay_area_vs_SDT_GIO}, we observed a 19.6\% area overhead of the SSTs \emph{vs.} the SDT\_GIOs at the \textit{tile level}.
% We are now interested in 
However, here we calculate this area overhead at the \textit{GEMM implementation level}, as this provides more accurate estimations for DNN accelerators.
When comparing the sparse GEMM mapped to SSTs \emph{vs.} the GEMM using SDT\_GIOs, we observe a small area increase ranging from \textbf{10.2--15.9\%} for int8, and from \textbf{13.3--19.4\%} for bfloat16, across all SA sizes.
We note that the lowest area increase, \emph{i.e.,} \textbf{10.2\%} and \textbf{13.3\%} for int8 and bfloat16, respectively, occurs for the highest, 40x40 SA size.
This is mainly attributed to the dedicated wires in SSTs, where the increase of the SA size (thus the total slices), leads to more routing area in the GEMM design using SDT\_GIOs. 
% in VTR.


\begin{figure}[t]
\vspace{-0.75cm}
\centering
\subfloat[int8]{\includegraphics[width=0.48\linewidth]{04_Evaluation/freq_SST_vs_GIO_vs_DSP_int8.pdf}
\label{fig:freq_SST_GIO_DSP_int8}}
\hfill
\subfloat[bfloat16]{\includegraphics[width=0.48\linewidth]{04_Evaluation/freq_SST_vs_GIO_vs_DSP_bf16.pdf}}
\label{fig:freq_SST_GIO_DSP_bf16}

\vspace{-0.3cm}

\caption{FPGA frequency comparison of \textit{sparse} GEMM using SSTs \emph{vs.} SDT\_GIOs \emph{vs.} CLBs+DSPs, for various SA sizes.}
\label{fig:freq_SST_vs_GIO_vs_DSP}
\vspace{-0.55cm}
\end{figure}


\begin{figure}[t]

\vspace{-0.25cm}

\centering
\subfloat[int8]{\includegraphics[width=0.48\linewidth]{04_Evaluation/area_SST_vs_GIO_vs_DSP_int8.pdf}}
\label{fig:area_SST_GIO_DSP_int8}
\hfill
\subfloat[bfloat16]{\includegraphics[width=0.48\linewidth]{04_Evaluation/area_SST_vs_GIO_vs_DSP_bf16.pdf}}
\label{fig:area_SST_GIO_DSP_bf16}

\vspace{-0.3cm}

\caption{Total area comparison of various \textit{sparse} GEMM implementations utilizing SSTs \emph{vs.} SDT\_GIOs \emph{vs.} CLBs+DSPs.}
\label{fig:area_SST_vs_GIO_vs_DSP}

\vspace{-0.55cm}

\end{figure}


As shown in Fig. \ref{fig:area_SST_vs_GIO_vs_DSP}, we notice a substantial area increase of the sparse GEMM mapped to CLBs and DSPs \emph{vs.} using SSTs, of up to \textbf{7$\times$} for int8, and \textbf{10.9$\times$} for bfloat16.
Moreover, we observe that this area difference increases with the SA size.
In particular, for 20x20 SA size, we obtain a difference of 5.5$\times$ and 8.3$\times$ for int8 and bfloat16, respectively.
However, for 40x40 size, the difference increases to 7$\times$ and 10.9$\times$ for int8 and bfloat16, respectively.
Hence, our results exhibit the significant area efficiency and scalability of the proposed sparse GEMM designs using in-fabric SSTs, 
% compared to 
over traditional FPGAs.




% 20x20 -> 5.5$\times$ and 8.3$\times$ for int8 and bfloat16, respectively.

% 40x40 -> higher difference, 7$\times$ and 10.9$\times$ for int8 and bfloat16, respectively.


% utilized multiplexers of our designs.
% explain how calculate the FPGA routing area (SB + CB) only multiplexers that are utilized of the SB and CB routing resources.

% int8 total area results:
% SST higher area over SDT\_GIO ranging from 10.2--15.9\%.

% CLB+DSP has 5.5--7$\times$ higher area than SST.


% bfloat16 total area results:
% SST higher area over SDT\_GIO ranging from 13.3--19.4\%.

% CLB+DSP has 8.3--10.9$\times$ higher area than SST.






\subsubsection{Resource Usage \& Efficiency of Sparse SST-based GEMM}

Table \ref{tb:GEMM_SST_vs_SDT_GIO_vs_DSP_40x40_SA} presents the resource usage and several evaluation metrics of the sparse GEMM designs, for SA size of 40x40.
First, we notice a small CLB increase for the SST \emph{vs.} the SDT\_GIO GEMM, of 26.2\% and 29.1\% for int8 and bfloat16, respectively.
This is ascribed to the additional control logic of including all supported sparsity levels, compared to only control logic for dense (since for SDT\_GIOs the zeros of the sparse matrices are retained, effectively operating similar to dense).
Second, note that the BRAM usage is the same in all GEMM designs, despite the additional bandwidth requirement due to the indices of the compressed sparse format (Sec. \ref{subsec:Matrix_multiplication_mapping}).
This is due to the use of the 512x40-bit BRAM mode (Sec. \ref{subsec:Dedicated_wires_benefits}). 
For example, for int8, buffer $A$ banks in the dense design of Fig. \ref{fig:GEMM_2D_array_SSTs}a, need a bitwidth of 4$\cdot$8=32-bits, which is smaller than the 40-bits width of the BRAMs.
However, for the sparse operation of Fig. \ref{fig:GEMM_2D_array_SSTs}b, they need additionally 8-bits for the indices (40-bits total), which is exactly the same as the BRAMs bitwidth.
Similar conclusions can be drawn for bfloat16.
Hence, no additional BRAMs are needed for the higher bandwidth requirements of sparse matrices in compressed format.


% Furthermore, 
Table \ref{tb:GEMM_SST_vs_SDT_GIO_vs_DSP_40x40_SA} shows the \textit{effective} throughput and area efficiency of our GEMM designs.
The \textit{effective} throughput (TOPs) is calculated on the GEMM \textit{native} size (40x40 in this case, see Sec. \ref{subsec:Matrix_multiplication_mapping}), and similar to \cite{Nvidia_accelerate_sparse_2021, NVIDIA_A100, AMD_CDNA_3} when acceleration can be attained from sparsity.
In particular, since 1:4 sparsity is included in our SST-based and CLB+DSP designs, their \textit{effective} throughput is increased by a factor of four.
When comparing the SST-based GEMM \emph{vs.} the SDT\_GIO-based design, we observe a throughput gain of \textbf{4.03$\times$} and \textbf{3.98$\times$} for int8 and bfloat16, respectively.
Moreover, we observe \textbf{3.66$\times$} (int8) and \textbf{3.51$\times$} (bfloat16) higher area efficiency (TOPs/GMWTA, area in Giga MWTAs).
In addition, when comparing with the GEMM using CLBs and DSPs, we obtain \textbf{4.34$\times$} (int8) and \textbf{5$\times$} (bfloat16) higher throughput for the SST-based GEMM.
Regarding the area efficiency, an immense \textbf{30.62$\times$} (int8) and \textbf{54.45$\times$} (bfloat16) difference is observed, primarily due to the increased usage
% , and thus area, 
of the traditional FPGA resources.
To this end, notice the very high CLB and wirelength usage for the CLB+DSP \emph{vs.} the SST-based GEMM. 
For instance, for int8, there is a 34.1$\times$ higher CLB and a 15$\times$ higher wirelength usage.


Finally, in Fig. \ref{fig:GEMM_SST_3x3_int8} we show a screenshot of a sparse SST-based design implemented in VTR.
Notice the physically contiguous vertical SST chains due to dedicated wires.
Also, observe the 2D \textit{physical} layout after VTR PnR, compared to the \textit{logical} 2D array of Fig. \ref{fig:GEMM_2D_array_SSTs}b.


\begin{figure}[t]
\vspace{-0.30cm}
\centering
\includegraphics[width=0.52\linewidth]{04_Evaluation/SST_GEMM_3x3_int8.png}

\vspace{-0.3cm}

\caption{SST-based GEMM implemented in VTR (SA size: 12x12). Blue: CLB, Orange: BRAM, Yellow: DSP, Purple: SST.}
\label{fig:GEMM_SST_3x3_int8}

\vspace{-0.50cm}

\end{figure}






\begin{table*}[t]
\vspace{-0.40cm}
 \centering
\caption{Evaluation and comparison of sparse GEMM designs using SSTs \emph{vs.} SDT\_GIOs \emph{vs.} CLBs+DSPs (SA size: 40x40).}
\vspace{-0.10cm}
\setlength\tabcolsep{2.8pt}
\renewcommand{\arraystretch}{1.15}
\resizebox{0.81\textwidth}{!}{
\vspace{-0.10cm}
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c}
\Xhline{2.5\arrayrulewidth}

% \multirow{2}{*}{\textbf{BRAMs}}

\multirow{2}{*}{\textbf{Pr.}} & \textbf{GEMM} & \multirow{2}{*}{\textbf{CLBs}} & \multirow{2}{*}{\textbf{BRAMs}} & \textbf{DSP} & \textbf{SST} & \textbf{SDT\_GIO} & \textbf{Freq.} & \textbf{Area} & \textbf{Wirelength} & \textbf{Eff. Thrpt.} & \textbf{Area Eff.}\\


& \textbf{Cfg.} &  &  & \textbf{slices} & \textbf{slices} & \textbf{slices} & \textbf{(MHz)} & \textbf{(GMWTA)} & \textbf{(CLB seg. units)} & \textbf{(TOPs)} & \textbf{(TOPs/GMWTA)}\\

\hline
\hline


\multirow{3}{*}{\rotatebox[]{90}{\textbf{int8}}} & SST  & 665 & 450 & 0 & 100 & -- & 601 &  0.145 & 493K & \textbf{7.69} & \textbf{53.03}\\

& SDT\_GIO & 527 & 450 & 0 & -- & 100 & 596 & 0.132 & 578K & 1.91 & 14.47\\

& CLB+DSP & 22650 & 450 & 400 & -- & -- & 138 & 1.022 & 7373K & 1.77 & 1.732 \\

\hline

\multirow{3}{*}{\rotatebox[]{90}{\textbf{bfloat16}}} & SST & 719 & 500 & 0 & 100 & -- & 578 &  0.162 & 684K & \textbf{7.40} & \textbf{45.68}\\

& SDT\_GIO & 557 & 500 & 0 & -- & 100 & 580 & 0.143 & 725K & 1.86 & 13.01\\

& CLB+DSP & 27065 & 500 & 2400 & -- & -- & 116 & 1.764 & 15212K & 1.48 & 0.839 \\


\Xhline{2.5\arrayrulewidth}

\end{tabular}
}
\label{tb:GEMM_SST_vs_SDT_GIO_vs_DSP_40x40_SA}

\vspace{-0.20cm}

\end{table*}


\begin{table*}[t]

% \vspace{-0.30cm}

\centering
\caption{Speedup estimation, accuracy and weight mem. reduction of various DNNs when mapped to SSTs \emph{vs.} SDT\_GIOs.}
\vspace{-0.10cm}
\setlength\tabcolsep{2.5pt}
\renewcommand{\arraystretch}{1.0}
\resizebox{0.81\textwidth}{!}{
\vspace{-0.10cm}
\begin{tabular}{c|c|c|c|c|c|c|c|c|c||c|c}
\Xhline{2.5\arrayrulewidth}

% \multirow{2}{*}{\textbf{BRAMs}}

\textbf{DNN} & \textbf{Sparsity} & \multicolumn{4}{c|}{\textbf{Number of layers}} & \textbf{In-fabric} & \multirow{2}{*}{\textbf{Speedup}} & \multicolumn{2}{c||}{\textbf{Top-1 accuracy (\%)}} & \multicolumn{2}{c}{\textbf{Weight reduction}} \\

\cline{3-6} 
\cline{9-12} 

\textbf{model} & \textbf{levels} & \textbf{Dense} & \textbf{2:4} & \textbf{1:3} & \textbf{1:4} & \textbf{slice} & & \textbf{int8} & \textbf{bfloat16} & \textbf{int8} & \textbf{bfloat16} \\

\hline
\hline
 
 & dense & 48 & -- & -- & -- & SDT\_GIO & 1$\times$ & 79.56 (-0.00\%) & 79.79 (-0.00\%) & 1$\times$ & 1$\times$\\


\textbf{DeiT-S} & \textbf{[dense, 2:4]} & \textbf{--} & \textbf{48} & \textbf{--} & \textbf{--} & \textbf{SST} & \textbf{1.88$\times$} & \textbf{79.08 (-0.48\%)} & \textbf{79.20 (-0.59\%)} & \textbf{1.58$\times$} & \textbf{1.76$\times$} \\

\textbf{(4.7 GFLOPs)} & [dense, 2:4, 1:3, 1:4] & 3 & 26 & 16 & 3 & SST & 2.14$\times$ & 77.95 (-1.61\%) & 78.33 (-1.46\%) & 1.81$\times$ & 2.02$\times$ \\

& [dense, 1:3] & -- & -- & 48 & -- & SST & 2.61$\times$ & 73.36 (-6.20\%) & 73.95 (-5.84\%) & 2.32$\times$ & 2.61$\times$ \\

\hline

 & dense & 48 & -- & -- & -- &  SDT\_GIO & 1$\times$ & 81.40 (-0.00\%) & 81.80 (-0.00\%) & 1$\times$ & 1$\times$\\

\textbf{DeiT-B} & [dense, 2:4] & -- & 48 & -- & -- & SST & 1.91$\times$ & 81.45 (+0.05\%) & 81.73 (-0.07\%) & 1.59$\times$ & 1.77$\times$ \\

\textbf{(17.6 GFLOPs)} & [dense, 2:4, 1:3, 1:4] & 6 & 15 & 13 & 14 & SST & 2.35$\times$ & 81.28 (-0.12\%) & 81.60 (-0.20\%) & 2.05$\times$ & 2.26$\times$ \\

& [dense, 1:3] & -- & -- & 48 & -- & SST & 2.75$\times$ & 81.10 (-0.30\%) & 81.25 (-0.55\%) & 2.36$\times$ & 2.64$\times$ \\

& \textbf{[dense, 1:4]} & \textbf{--} & \textbf{--} & \textbf{--} & \textbf{48} & \textbf{SST} & \textbf{3.52$\times$} & \textbf{80.37 (-1.03\%)} & \textbf{80.69 (-1.11\%)} & \textbf{3.12$\times$} & \textbf{3.50$\times$} \\

\hline

& dense & 108 & -- & -- & -- & SDT\_GIO & 1$\times$ & 82.99 (-0.00\%) & 83.09 (-0.00\%) & 1$\times$ & 1$\times$\\

\textbf{ConvNeXt-S}& [dense, 2:4] & 36 & 72 & -- & -- & SST & 1.93$\times$ & 82.52 (-0.47\%) & 82.62 (-0.47\%) & 1.46$\times$ & 1.66$\times$ \\

\textbf{(8.7 GFLOPs)}& \textbf{[dense, 2:4, 1:3, 1:4]} & \textbf{36} & \textbf{59} & \textbf{7} & \textbf{6} & \textbf{SST} & \textbf{2.07$\times$} & \textbf{82.30 (-0.69\%)} & \textbf{82.39 (-0.70\%)} & \textbf{1.52$\times$} & \textbf{1.74$\times$} \\

& [dense, 1:3] & 36 & -- & 72 & -- & SST &  2.81$\times$ & 81.44 (-1.55\%) & 81.53 (-1.56\%) & 1.95$\times$ & 2.32$\times$ \\

& [dense, 1:4] & 36 & -- & -- & 72 & SST & 3.63$\times$ & 81.05 (-1.94\%) & 81.16 (-1.93\%) & 2.34$\times$ & 2.89$\times$ \\




\Xhline{2.5\arrayrulewidth}

\end{tabular}
}
\label{tb:DNN_sparsity_results}

\vspace{-0.40cm}

\end{table*}


% [dense, 2:4, 1:3, 1:4]

\subsection{DNN Speedup Estimation Using SST Slices}
\label{subsec:Performance_estimation_DNNs}

We provide a speedup estimation for actual DNNs when leveraging our proposed SST slices.
In particular, we utilize the DeiT \cite{DeiT_2021} and ConvNeXt \cite{Convnext_2022} models, which attain state-of-the-art accuracy for vision tasks.
% Since DNNs exhibit distinct degrees of sparsity, 
We apply two types of sparsity to determine the optimal sparsity configuration for each aforementioned models. 
First, we utilize \textit{uniform} sparsity, \emph{i.e.,} all layers have the same sparsity, by pruning weights based on their magnitudes \cite{Nvidia_accelerate_sparse_2021}.
% However, this might lead to severe accuracy degradation for high sparsity levels, \emph{e.g.,} 1:4 (75\%), in some models.
Second, we exploit \textit{layer-wise} sparsity by adopting the neural architecture search (NAS) methodology proposed in \cite{Huang_2024_CVPR} to identify the optimal sparsity level of each layer, aiming to minimize accuracy degradation. 
Accuracy is evaluated on the ImageNet-1K dataset \cite{deng2009imagenet}, with int8 quantization applied using the post-training method proposed in \cite{PTQ4ViT_arixv2022}.




For speedup estimation, we develop an analytical model based on our results in Sec. \ref{subsec:Sparse_GEMM_implementation}, comparing the sparse SST-based GEMM to the SDT\_GIO-based GEMM, with the latter operating on dense matrices.
Our model exploits the matrix sizes for each layer of the DeiT and ConvNeXt models, and applies zero padding to align with the \textit{native} size of the GEMM accelerator (Sec. \ref{subsec:Matrix_multiplication_mapping}).
We utilize our largest implemented GEMM design, \emph{i.e.,} 40x40 SA size, and assume 100 GB/s bandwidth for DRAM modeling, which is typical in modern FPGAs \cite{Speedster_product_brief, VCK_5000}.
Utilizing GEMM computations provides a reliable model for estimating speedup, since GEMM is the core computation in contemporary DNNs, accounting for more than 90\% of the total execution time \cite{adolf2016fathom, wang_gemm_breakdown}.
Other operations \emph{e.g.,} softmax, layernorm, can be effectively overlapped by DNN accelerators.
Moreover, GEMM-based estimation (convolutions implemented as GEMM for ConvNeXt) is also performed in multiple works targeting speedup calculations due to sparsity exploitation \cite{HighLight_MIT_2023, Vegeta_HPCA_2023, Sparse_tensor_GPUs_2019, STA_arxiv_2020, SDP_sparse_2023}.



\vspace{2mm}
 
Table \ref{tb:DNN_sparsity_results} presents the speedup estimation and accuracy for our two ViT models, \emph{i.e.,} DeiT-S and DeiT-B, and the ConvNeXt-S model. 
When applying uniform 2:4 sparsity, we observe a negligible accuracy decrease in DeiT-B for bfloat16 (0.07\%), or even slightly higher accuracy for int8 (by +0.05\%).
However, we notice a larger decrease for DeiT-S, \emph{e.g.,} 0.48\% for int8.
This is due to the fact that smaller models (DeiT-S with 4.7 GFLOPs) exhibit less redundancy compared to larger models (DeiT-B with 17.6 GFLOPs), making them more resilient to sparsity.
Note that even with uniform 2:4 sparsity across all layers, dense computation is still needed (Table \ref{tb:DNN_sparsity_results}), due to QKV computation in ViTs (Sec. \ref{subsec:Matrix_multiplication_mapping}).
In addition, for ConvNeXt-S, we retain its depth-wise convolution layers as dense, since we observed a severe accuracy degradation when attempting to sparsify them.
For instance, when applying 2:4 sparsity to all other layers except the (36) depth-wise layers in ConvNeXt-S, we obtain an accuracy degradation of only 0.47\%.
However, uniform 2:4 sparsity can result in significant speedup, \emph{e.g.,} 1.93$\times$ in ConvNeXt-S, 
% (depth-wise layers show negligible contribution to total execution time), 
compared to dense operation.
We note that both int8 and bfloat16 cases deliver nearly indistinguishable speedups over dense computation, since the frequency difference between the SST-based and SDT\_GIO-based GEMM accelerators  is less than 1\% (Sec. \ref{subsec:Sparse_GEMM_implementation}).

% \vspace{0.4mm}

When applying layer-wise sparsity (all supported levels), we observe higher speedup in all cases, \emph{e.g.,} 2.35$\times$ in DeiT-B.
In this case, for DeiT-B, accuracy degradation is still negligible, \emph{e.g.,} 0.12\%. 
However, we observe higher degradation for DeiT-S (1.61\% for int8) and ConvNeXt-S (0.69\% for int8), since they are both small models. 
When applying higher sparsity (uniform 1:3 and 1:4), accuracy degrades even further, but higher speedup is attained, showcasing the accuracy-speedup trade-off.
If we constraint the accuracy degradation to be $\sim$1\% (acceptable in the vast majority of applications \cite{Accuracy_degradation_1_percent_NEURIPS2020}), the optimal speedup is attained for different sparsity configurations in DNNs (bolded in Table \ref{tb:DNN_sparsity_results}).
Specifically, for DeiT-S, uniform 2:4 is optimal (\textbf{1.88$\times$} speedup), while for DeiT-B, uniform 1:4 shows best results (\textbf{3.52$\times$} speedup).
For ConvNeXt-S, layer-wise sparsity (all supported levels) exhibits optimal results (\textbf{2.07$\times$} speedup).
However, when allowing higher accuracy degradation (for instance, within 2\%) higher speedup can be attained, \emph{e.g.,} 3.63$\times$ for uniform 1:4 in ConvNeXt-S.
Finally, notice the higher weight memory reduction as sparsity increases (Table \ref{tb:DNN_sparsity_results}).
Our results demonstrate the importance of supporting multiple sparsity levels in the SST slices.


% \vspace{-1.8mm}




\vspace{-0.20cm}


\subsection{Versal AIE-ML Sparsity Comparison}
\label{subsec:AIE_ML_comparison}

The new Versal AIE-ML \cite{AMD_AIE_ML_architecture_manual, AMD_AIE_ML_kernel_guide} includes out-of-fabric 
% SIMD 
processors that support 2:4 sparsity.
Therefore, we aim to compare the attainable acceleration 
% compute utilization 
due to sparsity and the efficiency of the compressed sparse format between our SSTs and the AIE-ML.
We exploit AMD's \textit{optimized} codes 
% running on a single AIE-ML core, 
for dense/sparse GEMM kernels for the AIE-ML \cite{AMD_dense_GEMM_int8_github, AMD_AIE_user_guide_2023.2}, using 16-bits for the outputs as this leads to substantially higher compute utilization \cite{AMD_dense_GEMM_int8_github}.
We compile and simulate the AIE-ML designs using Vitis 2023.2 on the VEK280 platform \cite{AMD_VEK_280}.




Fig. \ref{fig:compute_utilization_SST_vs_AIE_ML} shows the compute utilization of a 64$\times$64$\times$64 GEMM, mapped to both SST slices and the AIE-ML.
We notice that the AIE-ML achieves higher than 90\% utilization in dense GEMM for both precisions. 
However, for 2:4 sparsity, we observe a substantially lower utilization, \emph{i.e.,} 51.6\% for int8 and 50.4\% for bfloat16.
We note that the compute utilization is calculated as the \textit{effective} utilization of only non-zero computation in the sparse case.
Thus, we obtain a marginal speedup of 13\% (int8) and 4.5\% (bfloat16), for the 2:4 sparse GEMM over the dense case for the AIE-ML.
Moreover, for higher sparsity, \emph{i.e.,} 1:3 and 1:4, the compute utilization in AIE-ML drops more significantly (Fig. \ref{fig:compute_utilization_SST_vs_AIE_ML}).
While higher than 2:4 sparsity can be stored in a compressed format in the AIE-ML, only 2:4 sparsity is inherently supported.
Consequently, no further acceleration can be achieved, leading to significantly decreased \textit{effective} utilization.

This marginal speedup due to sparsity in the AIE-ML is primarily attributed to limited vector load/store bandwidth, causing the sparse GEMM to become I/O bound
\cite{AMD_dense_GEMM_int8_github, AMD_AIE_ML_architecture_manual}.
In contrast, as depicted in Fig. \ref{fig:compute_utilization_SST_vs_AIE_ML}, our SST-based GEMM achieves 100\% utilization for all supported sparsity levels.
This is because we architect our SST slices in a fashion that guarantees the corresponding speedup of each sparsity level, \emph{e.g.,} 4$\times$ for 1:4 sparsity (refer to Sec. \ref{subsec:Sparse_Processing_Element} for design details).




Fig. \ref{fig:compression_ratio_SST_vs_AIE_ML} shows the compression ratios achieved by the SSTs \emph{vs.} the AIE-ML.
The AIE-ML uses a bitmap-based compressed format
% representation 
\cite{AMD_AIE_ML_kernel_guide}, which is also used in various works accelerating structured sparsity \cite{S2TA_HPCA_2022, STA_arxiv_2020, N_M_sparse_transformers_FPGA_VLSI_2022, Fine_grained_Neural_ODE_FPGA_2023, LAMPS_FCCM_2024}.
Instead, we utilize an index-based format (Sec. \ref{subsec:Fine_grained_structured_sparsity}).
For 2:4 sparsity, both approaches present the same compression ratio (Fig. \ref{fig:compression_ratio_SST_vs_AIE_ML}).
However, for 1:3 and 1:4, our approach leads to higher compression ratio, up to \textbf{20\%} for int8 and \textbf{11\%} for bfloat16, showcasing the storage efficiency of the index-based format.
Moreover, as shown in \cite{N_M_sparse_transformers_FPGA_VLSI_2022}, bitmap achieves higher compression ratio over traditional sparse formats, \emph{e.g.,} CSR, CSC, COO \cite{Sparsity_Hoefler_2021}, for sparsity levels of 50--87.5\%.
This highlights that the supported index-based sparse format provides superior compression over other formats. 







\begin{figure}[t]
\vspace{-0.40cm}
\centering
\subfloat[]{\includegraphics[width=0.48\linewidth]{04_Evaluation/compute_utilization_SST_vs_AIE_ML.pdf}
\label{fig:compute_utilization_SST_vs_AIE_ML}}
\hfill
\subfloat[]{\includegraphics[width=0.455\linewidth]{04_Evaluation/compression_ratio_SST_vs_AIE_ML.pdf}
\label{fig:compression_ratio_SST_vs_AIE_ML}}

\vspace{-0.3cm}

\caption{Compute utilization (a) and compression ratio (b) comparison of a 64$\times$64$\times$64 GEMM using SSTs \emph{vs.} AIE-ML.}
\label{fig:utilization_compres_ratio_SST_vs_AIE_ML}
\vspace{-0.55cm}
\end{figure}


\subsection{Impact on Non-AI Benchmarks}
\label{subsec:Non_AI_benchmarks}


We demonstrate the flexibility of our enhanced FPGA architecture with SST slices, by 
exploring the maximum attainable frequency of non-AI benchmarks.
We select non-AI benchmarks from the VTR benchmark suite \cite{VTR_8_2020}, which target various application domains, \emph{i.e.,} $sha$, $mmcl$, $arm\_core$, $stereovision2$, $LU32PEEng$, $bgm$, $raygentop$, and $blob\_merge$  (see \cite{VTR_benchmarks} for domains).
When comparing our enhanced FPGA with a traditional FPGA on the aforementioned benchmarks, we observe, on average, a negligible decrease (\textbf{<1\%}) in maximum frequency. 
Hence, our proposed FPGAs maintain the performance and flexibility of traditional FPGAs, while offering highly efficient sparse and dense DNN acceleration (Sec. \ref{subsec:Dedicated_wires_benefits}--\ref{subsec:Performance_estimation_DNNs}).



\subsection{Insights \& Discussion  }
\label{subsec:Discussion_insights}


\subsubsection{Very High Sparsity in DNN Layers} In the previous sections, we exhibit the importance and efficiency of our proposed in-fabric SST slices for accelerating sparse DNNs.
However, our SSTs support up to 75\% (1:4) structured sparsity.
Prior work \cite{EIE_ISCA_2016, Song_Han_learning_2015} has shown that a few DNN layers can have very high \textit{random} sparsity, \emph{e.g.,} >90\%. 
In this case, \textit{unstructured} sparse FPGA accelerators mapped to traditional CLB and DSP resources, \emph{e.g.,}
\cite{Fowers_sparse_FCCM_2014, Abhishek_sparsity_FPL_2020, unstructured_sparsity_CNN_FPGA_FCCM_2019}, can be exploited to efficiently compute these few high-sparsity layers.
All other DNN layers with less sparsity can be mapped to our SSTs (Sec. \ref{subsec:Sparse_GEMM_implementation} \& \ref{subsec:Performance_estimation_DNNs}), enabling both \textit{structured} and \textit{unstructured} sparse FPGA accelerators to operate synergistically, for maximized efficiency.
Moreover, for very high sparsity (>90\%), the CSR, CSC, COO formats utilized in unstructured sparse accelerators, offer the most efficient compression \cite{Sparsity_Hoefler_2021}.
Instead, for lower sparsity, the index-based format utilized in SSTs is the most efficient (Sec. \ref{subsec:AIE_ML_comparison}).
This emphasizes the custom flexibility of the enhanced FPGAs with in-fabric SST slices in supporting highly efficient sparse DNN accelerators.




\subsubsection{Efficient In-Fabric Slices} 
In-fabric slices have attained commercial success in 
% premier 
AI-optimized FPGAs, \emph{e.g.,} the Intel tensor blocks \cite{Stratix_10_NX_FPGA_2021} and the new Intel AI-enhanced DSPs \cite{Sergey_Intel_TB_Agilex_5_FCCM_2024, Intel_Agilex_5_tensor_blocks}.
Moreover, recent research \cite{Versal_vs_Stratix_FCCM_2024} comparing leading in-fabric (Intel Stratix 10 NX) with out-of-fabric (AMD Versal ACAP) AI-optimized FPGAs, has shown higher energy efficiency in GEMM for in-fabric solutions. 
However, these efficient in-fabric slices primarily target dense AI acceleration, rendering them insufficient for most DNNs, which are sparse.
Instead, our proposed flexible SST slices support both dense and sparse computation (multiple sparsity levels), showcasing substantial advantages 
over dense in-fabric blocks and traditional FPGAs (Sec. \ref{subsec:Sparse_GEMM_implementation}), for actual sparse DNN workloads (Sec. \ref{subsec:Performance_estimation_DNNs}).
Note that although we are inspired by prior academic research that utilize dense 2D systolic in-fabric blocks \cite{TS_Aman_FPGA_2021, Aman_TS_TRETS_2022}, the methodology described in Sec. \ref{sec:Architecture_Overview} for introducing sparsity support can be generalized in straightforward fashion to other in-fabric FPGA blocks.





\subsubsection{Future Directions} 
Our SST slices support \textit{static} sparsity targeting weights in DNNs.
Future work could also explore supporting \textit{dynamic} sparsity in activations.
The SST slices could also enable other modes for increased flexibility, \emph{e.g.,} element-wise modes, similar to \cite{TS_Aman_FPGA_2021, Aman_TS_TRETS_2022}.
However, this is beyond the scope of this paper and is therefore left for future work.
We note that general matrix-vector (GEMV) can be directly supported in SSTs at batch size of four with 100\% utilization (by setting $X$=1 in designs of Sec. \ref{subsec:Matrix_multiplication_mapping}). 
Additionally, multiplexing logic can be incorporated into SSTs to increase GEMV utilization for batch size of one, as shown in \cite{Aman_TS_TRETS_2022}, which we leave as future work.
Finally, another extension is to include multiple input crossbars, which would further reduce the small area overhead of the SSTs, and quantify the trade-offs in area and FPGA routability.

% Instead, our focus is to accurately evaluate and quantify the benefits and overheads of supporting sparsity features compared to dense in-fabric blocks.

% \vspace{-1.6mm}
