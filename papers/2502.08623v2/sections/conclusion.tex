\section{Conclusion}

In this work, we propose the \fullname (\abv) procedure as a method for data curation in robot imitation learning. Specifically, we motivate mutual information as a useful basis for measuring the quality of individual demonstrations, and instantiate mutual information estimators as a way to rank and select demonstrations. Across several datasets of human-teleoperated demonstrations in both the real-world and simulation, we find that the \abv outperforms several prior methods at measuring the quality of demonstrations. \looseness=-1

\subsection{Limitations}

\noindent \textbf{Action Entropy.} \abv considers the mutual information between states and actions to curate demonstrations. However, this ignores the fact that in the sequential setting, action entropy directly determines state entropy through time, e.g. a more random policy will visit more states. In this sense, \abv may be over-indexing on $H(A|S)$. A better approach might be to consider measuring $I(S_1;A_1, ... A_T)$, or the mutual information between initial states and action sequences. However, doing so reduces the amount of data available for estimation by a factor of $T$, which is typically 100-1000, making this approach more challenging in practice.

\noindent \textbf{State Entropy.} Because of the $H(S)$ component, \abv could prefer outlier trajectories that visit a new part of the state space, so long as there are no nearby trajectories with conflicting actions. For this reason as well, the aforementioned approach of measuring $I(S_1;A_1, ... A_T)$ might be preferred in a world of infinite data.

\noindent \textbf{Pauses.} Because \abv considers the average estimated $\hat{I}$ across a trajectory, it is susceptible to preferring data that is predictable, but might not make progress towards completing the task. For example, if a robot pauses for an extended amount of time, the action distribution is very predictable. However, this behavior is not desired in practice. To mitigate this effect, we recommend ensuring that all data completes the task and pauses are filtered.

\noindent \textbf{Greediness.} Note that \abv's curation procedure is greedy and not globally optimal -- once we remove an episode we have changed the data distribution, which in turn affects the true mutual information. However, re-running the mutual information estimator on the entire dataset for each filtered demonstration would be far more computationally expensive.

\subsection{Future Work}
Exciting avenues for future work remain. For instance, extending \abv to the multi-task setting will require disentangling task conditioning from mutual information estimation as to not retain only the easiest tasks. This problem becomes harder in settings where task definitions are not enumerable, like natural language. Other directions include scaling \abv to larger datasets such as \citet{openx} and \citet{droid} to curate better subsets for the robot learning community. Finally, integrating \abv into an online data collection interface could improve data collection efficiency. Though there is more work to do, we believe \abv is a step towards addressing the data problem in robotics. \looseness=-1

