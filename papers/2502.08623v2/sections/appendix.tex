
\subsection{Extended Results}
\label{app:results}
Here we provide results and ablations that could not fit in the main text.

\noindent \textbf{Additional Results}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.4\linewidth]{plots/franka_mis.pdf}
    \vspace{-0.1in}
    \caption{The performance of different mutual information estimators on the Franka Datasets, cut from the main text due to space.}
\end{figure}

\noindent \textbf{Additional Ablations}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{plots/crowd_ks.pdf}
    \vspace{-0.1in}
    \caption{The effect of different values of $k$ on RoboCrowd}
\end{figure}


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\linewidth]{plots/rm_image_ks.pdf}
    \vspace{-0.1in}
    \caption{The effect of different values of $k$ on RoboMimic Image}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\linewidth]{plots/rm_image_zs.pdf}
    \vspace{-0.1in}
    \caption{The effect of different latent dimension sizes for $z_s$ and $z_a$ on RoboMimic Image. we find that performance is relatively robust to this parameter. Unlike all others, this experiment was run over 2, not 3, seeds.}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\linewidth]{plots/rm_image_beta.pdf}
    \vspace{-0.1in}
    \caption{The effect of different values of VAE $\beta$ on RoboMimic Image. We find that performance is relatively robust to this parameter for RoboMimic. Unlike all others, this experiment was run over 2, not 3, seeds.}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{plots/crowd_action.pdf}
    \vspace{-0.1in}
    \caption{The effect of using different action spaces for the RoboCrowd dataset.}
\end{figure}

\noindent \textbf{Plots with All Baselines and Estimators.} The below plots show the performance of all methods on the same exact plot, allowing for direct comparison. We additionally consider another baseline ``Policy Loss'', which simply measures the loss of a BC policy.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.495\linewidth]{plots/rm_state_all.pdf}
    \includegraphics[width=0.495\linewidth]{plots/rm_image_all.pdf}
    \vspace{-0.1in}
    \caption{RoboMimic results for all methods.}
\end{figure}


\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{plots/crowd_all.pdf}
    \vspace{-0.1in}
    \caption{RoboCrowd results for all methods.}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\linewidth]{plots/franka_all.pdf}
    \vspace{-0.1in}
    \caption{Franka results for all methods.}
\end{figure}




\subsection{Method Details}
Here we provide more details on each of the different methods used to for data curation. For each method we train the requisite model(s), then run inference over the whole dataset. As we filter at the demonstration level, we aggregate scores for all methods over each demonstration. Mathematically, all scoring functions take the form:
\begin{equation*}
    S(\tau) = \frac{1}{T} \sum_{t=1}^{T} h(s_t, a_t ; \D_n)
\end{equation*}
where $h$ is some function of the state-action pairs in a set of data $\D$ comprised of trajectories $\tau$. We use the subscript $t$ to denote that we index over the steps of a trajectory $\tau$. In practice we clip state-action scores from $h$ at the 1st and 99th percentile. Below we provide the scoring function used for all methods.

\vspace{0.075in}

\noindent \textbf{\fullname (\abv)}. We first fit an action VAE $z_a = f_a(a)$ and state VAE $z_s = f_s(s)$. Then, we iterate over the entire dataset 4 times, computing scores in random batches of size 1024. The score function is then
\begin{equation*}
        h(s_i, a_i; B) = \hat{I}(s_i, a_i ; B) \propto -\psi(n(z_{s,i}) + 1) - \psi(n(z_{a,i}) + 1)
\end{equation*}
where $B$ is a random batch and $n$ is defined as \cref{sec:method:score}.

\vspace{0.075in}
\noindent \textbf{BiKSG}. We follow the same approach as in \abv, except the mutual information is estimated as
\begin{equation*}
        h(s_i, a_i; B) = \hat{I}(s_i, a_i ; B) \propto -\log n(z_{s,i}) - \log n(z_{a,i})
\end{equation*}
and we use the $l2$ distance metric over $\mathcal{Z}_\mathcal{S} \times \mathcal{Z}_\mathcal{A}$ without the $l_\infty$ norm:
\begin{equation*}
    ||[z_s, z_a] - [z_s', z_a'] || = ||[z_s, z_a] - [z_s', z_a'] ||_2
\end{equation*}

\vspace{0.075in}
\noindent \textbf{KL}. We follow the same approach as in \abv, except the mutual information is estimated using separate terms for $H(S)$, $H(A)$ and $H(S, A)$ where each term is given by the differentiable entropy estimator from \citet{kozachenko1987sample}. Let $z^k_{s,i}$ be $z_{s,i}$'s $k$-nearest-neighbor. Then, the estimator is given as
\begin{equation*}
        h(s_i, a_i; B) = \hat{I}(s_i, a_i ; B) \propto \log ||z_{s,i} - z^k_{s,i}||_2^{|\mathcal{Z}_\mathcal{S}|} + \log ||z_{a,i} - z^k_{a,i}||_2^{|\mathcal{Z}_\mathcal{A}|} - \log ||[z_s, z_a]_{i} - [z_s, z_a]^k_{i}||_2^{|\mathcal{Z}_\mathcal{S}| + |\mathcal{Z}_\mathcal{A}|}
\end{equation*}
where $|\mathcal{Z}_\mathcal{S}|$ is the dimension of the latent space.

\vspace{0.075in}
\noindent \textbf{MINE}. MINE optimizes a critic function $f_\theta(s,a)$ to predict the mutual information using the objective
\begin{equation*}
    \max_\theta \E_{(s,a)\sim \D}[f_\theta(s,a)] - \log \left(\E_{s \sim \D, a \sim D}[e^{f_\theta(s,a)}\right)
\end{equation*}
where the first term is sampled from the joint and the second is sampled from the marginals. The scoring function is then simply:
\begin{equation*}
    h(s_i, a_i; B) = f_\theta(s_i, a_i)
\end{equation*}
In practice MINE uses an exponential moving average of gradient's denominator to un-bias the estimator. We refer to this parameter as $\alpha$ as in the original paper and leave it at 0.9.

\vspace{0.075in}
\noindent \textbf{InfoNCE}. We optimize the symmetric InfoNCE objective from CLIP, which converges to the mutual information up to a constant \citep{ma2018noise}. To do so, we train a state encoder $f_s$ and an action encoder $f_a$. After training, the scoring function becomes:
\begin{equation*}
    h(s_i, a_i; B) = f_s(s_i) \cdot f_a(a_i)
\end{equation*}
or simply the dot product between the two representations.

\vspace{0.075in}

\noindent \textbf{VIP}. VIP \citep{ma2023vip} uses the dual form of the goal-conditioned RL problem, with the negative L2 distance between encoded states as a proxy for the value function $V(s,g) = - ||f(s) - f(g)||_2$. The VIP training objective is 
\begin{equation*}
    \min_\theta \E_{s_1 \sim \rho_1, g \sim D}[||f_\theta(s_1) - f_\theta(g)||_2] + \log \E_{s_t, s_{t+1}, g \sim \D}[\exp\left(||f_\theta(s_t) - f_\theta(g)||_2 - \mathds{1}\{s_t = g\} - \gamma ||f_\theta(s_{t+1}) - f_\theta(g)||_2 \right)]
\end{equation*}
Then, using the learned value function we estimate the ``reward'' of each transition by
\begin{equation*}
     h(s_t, s_{t+1}, g; B) = -||f_\theta(s_{t+1}) - f_\theta(g)||_2 + ||f_\theta(s_{t}) - f_\theta(g)||_2
\end{equation*}
which captures the progress of the transition towards the goal. During training we sample goals uniformly from the future, but during quality estimation we set the goals to be the final state in each demonstration.

\vspace{0.075in}
\noindent \textbf{Compatibility.} Following \citet{ghandi2023eliciting} we train an ensemble of 5 policies. Then, the compatibility score is estimated as:
\begin{equation*}
    h(s_i, a_i; B) = \begin{cases}
        1 - \min \left(\text{L2Loss}(\pi_\theta(s_i), a_i) / \lambda, 1 \right) & \text{if} \text{ std}(\pi_\theta(s_i)) < \eta \\
        1 & \text{otherwise}
    \end{cases}
\end{equation*}
where L2Loss is the average L2 loss of the ensemble and std is the standard deviation of the predictions.

\vspace{0.075in}
\noindent \textbf{Uncertainty.} The uncertainty score is estimated from the same ensemble of 5 policies by the standard deviation of the predictions:

\begin{equation*}
    h(s_i, a_i; B) = \text{std}(\pi_\theta(s_i))
\end{equation*}


\vspace{0.075in}
\noindent \textbf{Policy Loss.} The Policy Loss metric is simply the negative L2 Loss of the network, such that demonstrations with lower loss have a higher score.

\begin{equation*}
    h(s_i, a_i; B) = -\text{L2Loss}(\pi_\theta(s_i), a_i)
\end{equation*}



\subsection{Implementation Details}
\label{app:implementation}
\noindent \textbf{Architectures.} We use the same architectures for all methods whenever possible. For state-based experiments we simply use MLPs with two hidden layers of size 512 with ReLU activations. When training BC policies, we add dropout of 0.5 as we found it to be important to performance. For VAEs we use a symmetric decoder.

For Image experiments, we use ResNet18 architectures followed by a spatial softmax layer, similar to the original setup in \citet{robomimic}. We concatenate representations from all cameras along with the state information, and then feed that to information to an MLP. For RoboMimic we use a three layer MLP with hidden dimension of size 512. For Franka and RoboCrowd we use an MLP with two hidden layers of size 1024. For all methods using a state encoder, we use this architecture. For BC policies we ensemble the MLP, add dropout and use the L2 Loss function for training. MINE additionally concatenates the action before the MLP and InfoNCE trains a separate action encoder using just the MLP architecture. For action encoders and decoders, we use the same architecture as for state. For training VAEs on images, we use the same architecture but in reverse, with ResNet18 Decoders. 

We trained all models on TPU v4-8 VMs provided by the Google TPU Research Cloud.

\vspace{0.05in}
\noindent \textbf{Hyperparameters}
We set hyper-parameters consistently across settings, e.g. RoboCrowd and try to choose the same parameters for all methods when possible. Hyperparameters for all methods are shown in \cref{tab:hyperparameters}. 



\begin{table*}[ht]
\centering
{\renewcommand{\arraystretch}{1.05} %<- modify value to suit your needs
\begin{tabular}{cccccc}
\textbf{Method}                & \textbf{Parameter} & \textbf{RoboMimic State} & \textbf{RoboMimic Image} & \textbf{Franka} & \textbf{RoboCrowd} \\ \hline
\multirow{6}{*}{All}           & Optimizer          & \multicolumn{4}{c}{Adam}                                                                   \\
                               & Learning Rate      & \multicolumn{4}{c}{0.0001}                                                                 \\
                               & Batch Size         & \multicolumn{4}{c}{256}                                                                    \\
                               & Training Steps     & 50,000                   & \multicolumn{3}{c}{100,000}                                     \\
                               & Action Chunk       & 1                        & 1                        & 4               & 10                 \\
                               & Image Resolution   & --                       & (84, 84)                 & (128, 128)      & (128, 128)         \\ \hline
\multirow{5}{*}{\abv}           & Augmentations      & --                       & \multicolumn{3}{c}{Random Scale and Crop (0.9, 0.95)}           \\
                               & $\beta$            & 0.05                     & \multicolumn{3}{c}{0.01}                                        \\
                               & Image Recon Weight & --                       & \multicolumn{3}{c}{0.005}                                       \\
                               & $z_s$              & 12                       & 16                       & 24              & 16                 \\
                               & $z_a$              & 6                        & 6                        & 16              & 12                 \\
                               & $k$                & \multicolumn{4}{c}{(5,6,7)}                                                                \\ \hline
\multirow{2}{*}{VIP}           & $z$                & 8                        & 16                       & 24              & 16                 \\
                               & $\gamma$           & \multicolumn{4}{c}{0.98}                                                                   \\ \hline
InfoNCE                        & $z$                & 8                        & 16                       & 24              & 16                 \\ \hline
\multirow{2}{*}{MINE}          & $z$                & 8                        & 16                       & 24              & 16                 \\
                               & $\alpha$           & \multicolumn{4}{c}{0.9}                                                                    \\ \hline
\multirow{4}{*}{Compatibility} & Ensemble Size      & \multicolumn{4}{c}{5}                                                                      \\
                               & Dropout            & \multicolumn{4}{c}{0.5}                                                                    \\
                               & $\eta$             & 0.025                    & 0.05                     & 0.1             & 0.05               \\
                               & $\lambda$           & 8                        & 4                        & 2               & 4                  \\ \hline
\multirow{2}{*}{Uncertainty}   & Ensemble Size      & \multicolumn{4}{c}{5}                                                                      \\
                               & Dropout            & \multicolumn{4}{c}{0.5}                                                                   
\end{tabular}
}
\caption{Hyperparameters for all methods.}
\label{tab:hyperparameters}
\end{table*}
\vspace{0.05in}

\noindent \textbf{Randomized $k$-NN Estimation.} We estimate the mutual information using random batches for $k$-NN estimators. When doing so, we use a batch size of 1024 and iterate over the entire dataset 4 times.
\vspace{0.05in}

\noindent \textbf{Checkpoint Selection.} We train all state-based models for 50K timesteps and all image-based models for 100K timesteps. For VAEs, BC policies, and VIP we select final checkpoints (e.g. 50K or 100K steps). We found that InfoNCE and MINE tended to overfit quite fast. For InfoNCE we used checkpoints after 20K for state and 40K for images. For MINE we used 50K for state and 60K for images.


\subsection{Evaluation Details} 

\noindent \textbf{RoboMimic} For training policies in robomimic, we use the same architecture as in the image-based data quality experiments with an MLP action head using L2 loss. We train for 100K timesteps before running 200 evaluation episodes. Episodes are truncated after 400 timesteps.

\vspace{0.05in}

\noindent \textbf{RoboCrowd.} We use an ALOHA robot setup to evaluate performance on the RoboCrowd benchmark, with ten trials per method. As in \citet{mirchandani2024robocrowd} each trial assigned one of the following scores: 1 point for successfully grasping any number of candies, 2 points for returning any number of candies, and 3 points for returning exactly one candy. 0 points are given otherwise. Policies are trained for 200K timesteps using the same architecture ad hyperparameters from ACT, e.g. encoder-decoder transformer with L1 loss and action chunks of size 100.

\vspace{0.05in}


\noindent \textbf{Franka.} We use the franka robot setup from DROID \citet{droid}, and run 15 trials per method. We score trails of ``DishRack'' as fully successful (1) if the robot puts both items in the dish rack, and partially successful (0.5) if it puts only one item of the bowl and fork in the dish rack. We score trials of ``PenInCup'' as fully successful (1) if the pen ends up completely in the cup and partially successful (0.5) if it ends up on top of the cup. We use the same architectures and hyperparameters as in \citet{droid} for evaluation, e.g. Diffusion Policy \citep{chi2023diffusion} with action chunks of size 16 and execution size of 8 with a few differences. Instead of using pre-trained ResNet-50s as in \citet{droid}, we use ResNet34s initialized from scratch with GroupNorm instead of BatchNorm. To compensate, we train for 200K steps as opposed to 50K.





