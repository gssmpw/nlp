\section{Experiments}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/datasets.png}
    \vspace{-0.2in}
    \caption{Visualization of the tasks represented in the datasets we use in this work, including the Can MH, Lift MH, and Square MH datasets from RoboMimic; real-world PenInCup and DishRack datasets collected on a Franka robot; and the real-world TootsieRoll, HiChew, and HersheyKiss datasets from RoboCrowd for the ALOHA robot.}
    \label{fig:datasets}
\end{figure}

We aim to answer the following questions: (1) How well does \abv curate robot data? (2) How do different mutual information estimators affect performance? (3) Can data curation via mutual information improve performance on downstream policy learning? and (4) What is important to \abv's performance? Additional results are presented in \cref{app:results}.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.49\linewidth]{plots/rm_state_baseline.pdf}
    \includegraphics[width=0.49\linewidth]{plots/rm_image_baseline.pdf}
    \vspace{-0.15in}
    \caption{Average quality of demonstrations remaining in datasets after filtering with different choices of $S$ on the Lift, Can, and Square Multi-Human (Mh) datasets from the Robomimic benchmark with states (Left) and images (right). Results are shown as an average of 3 seeds.}
    \label{fig:robomimic_baseline}
\end{figure*}


\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{plots/crowd_baseline.pdf}
    \caption{Average quality of demonstrations remaining in datasets after filtering with different choices of $S$ on the Hi-Chew, Tootsie-Roll, and Hershey-Kiss crowdsourced datasets from the RoboCrowd benchmark. We include results for datasets with a combination of expert and only task-relevant data (left), and a version of the data that contains additional unstructured play data (right).  Results are shown as an average of 3 seeds.}
    \label{fig:robocrowd_image}
\end{figure*}



\subsection{Experimental Setup}

\subsubsection{Datasets}
To assess the performance of different robot demonstration curation techniques, we perform experiments on a broad set of datasets spanning simulated, real single-arm, and real bi-arm robots with varying levels of data quality as depicted in \cref{fig:datasets}. Notably, we use datasets where human experts have provided quality labels, allowing us to easily assess different demonstration curation metrics:

\noindent \textbf{RoboMimic.} The multi-human datasets from the RoboMimic benchmark \citep{robomimic} include 100 demonstrations from each of three robot operators for three tasks in increasing difficulty: ``Lift'' where the robot simply lifts a cube, ``Can'' where the robot moves a can from one bin to another, and ``Square'' where the robot places a nut onto a peg. RoboMimic provides quality labels for each operator, which we use to assign quality scores (with scores of 1, 2, and 3 for the ``worse'', ``okay'', and ``better'' demonstrations respectively). We measure the performance of different data curation methods from both state, in which ground truth object information is provided, as well as third-person images. 

\noindent \textbf{Franka.} Using the setup from \citet{droid} with a Franka Panda robot we collect 60 and 80 demonstrations for each of two tasks, ``PenInCup'' and ``DishRack'' respectively. Within each task, we collect 50\% expert demonstrations (quality 1) and 50\%  poor demonstrations (quality 0), where the operator intentionally makes a mistake (e.g. dropping an object, taking a long inefficient path, jerky motion). We use a single third person camera and a wrist camera to train policies and action chunks of size 4. 

\noindent \textbf{RoboCrowd.} The RoboCrowd benchmark from \citet{mirchandani2024robocrowd} contains crowdsourced robot data on the bimanual ALOHA \citep{zhao2023aloha} platform from real, novice users in a natural environment. Data in RoboCrowd varies widely in quality -- many trajectories contain suboptimal data or sequences of ``play'' data that are irrelevant to the target task. RoboCrowd serves as a suitable platform to study data curation as it has a small number of expert demonstrations for each task and human expert quality labels ranging from 0 to 3 for all crowdsourced data. Specifically, we use the ``HiChew Play,'' ``TootsieRoll Play,'' and ``HersheyKiss Play'' datasets which contain both expert demonstrations and crowdsourced demonstrations for candy bin-picking tasks. Every demonstration contains some amount of task-relevant data, with the potential of irrelevant play data in the crowdsourced demonstrations as well. We additionally evaluate on versions of these datasets (``HiChew'', ``TootsieRoll', ``HersheyKiss'') where the unstructured play data has been removed, but where demonstrations still contain task-relevant data of varying quality. The HiChew and TootsieRoll datasets contain 40 demonstrations each and the HersheyKiss dataset contains 100 demonstrations, half of which are expert demonstrations. We use the wrist cameras, overhead camera, and action chunks of size 10 for data curation. 


\subsubsection{Baselines}
We compare against a number of different data quality estimators from prior work in addition to a number of alternative mutual information estimators, which we label with ``(MI)''.

\noindent \textbf{Uncertainty.} Following prior works in active learning for imitation learning \citep{cui2019uncertainty, hoque2021thriftydagger}, we select data based on the uncertainty of an ensemble of 5 policies. Note that while this metric makes sense for active learning, it does not necessarily make sense in the offline setting, and in some ways may be inversely correlated with quality if the ensemble converges better on high quality data. 

\noindent \textbf{Compatibility.} Following \citet{ghandi2023eliciting}, we use a measure of demonstration ``compatibility'' to score data. Namely, a demonstration is compatible with respect to a policy if it has either high ``novelty'' as measured by the prediction variance of an ensemble, or low novelty and high likelihood as measured by the average loss. In some sense, this could be akin to mutual information if novelty captures $H(S)$ and likelihood captures $H(A|S)$. Though this method was originally designed to be used in the online setting, we adopt it to the offline setting by training a policy on all data, then estimating the ``compatibility'' for each demonstration with respect to the overall policy.  

\noindent \textbf{VIP.} Value Implicit Pre-training \citep{ma2023vip} is an action-free method that leverages the dual formulation of the goal-conditioned RL problem to learn a ``universal'' value function. We use VIP to estimate data quality by considering the total predicted reward over a demonstration. 

\noindent \textbf{InfoNCE (MI).} We use the symmetric InfoNCE \citep{oord2018representation} objective used to train CLIP \citep{radford2021learning} which converges to an estimate of mutual information. We compare to InfoNCE as CLIP is commonly used to curate datasets in vision and language \citep{schuhmann2022laion}.

\noindent \textbf{MINE (MI).} MINE \citep{belghazi2018mine} leverages the dual form of the KL divergence to estimate the mutual information using a learned critic function. 


\subsubsection{Architectures}
For all state-based experiments we use MLPs with two hidden layers of size 512. For image-based experiments we use ResNet-18 Encoders \citep{he2016deep} with spatial softmax activations following \citet{robomimic}, which are concatenated with state information as input to a MLP with two hidden layers of size 1024. When training VAEs from images we use matching ResNet-18 Decoder networks for each view. For each dataset we use the same architecture for all methods, where the latent $z$ dimension is set to be consistent across both \abv and baselines. For all experiments we use the Adam optimizer with learning rate 0.0001 and a batch size of 256. State-based models are trained for 50,000 steps and image based models are trained for 100,000 steps using VMs provided by a Google TPU Research Cloud Grant. We run three seeds for all methods. More details and hyper-parameters can be found in \cref{app:implementation}.



\subsection{How well does \abv curate data?}

To assess how well \abv can curate data, we plot the number of episodes filtered from each dataset against the average resulting expert quality label. This amounts to considering every possible dataset generated by sweeping the threshold level $\kappa$ in the sub-setted dataset according to scoring function $S$ (see \cref{eq:filter_ds}). Doing so allows us to simultaneously assess how well each method would does at every threshold level. The closer the curve is to the ``oracle'' or curating directly by the expert labels, the better. Note that one should not over-index on the right-hand size of the plot as with a typical learning curve, as that represents performance only as the dataset reaches 10\% of its original size.


\noindent \textbf{State-Based Results.}
We depict results on the state-based RoboMimic benchmark on the left side of \cref{fig:robomimic_baseline}. \abv performs as well or better than baselines in all environments, though there is not a particularly large gap with VIP. 

\noindent \textbf{Image-Based Results.}
In the image-based settings we find that \abv performs even better, surpassing all methods on both RoboMimic (\cref{fig:robomimic_baseline}) and Franka (\cref{fig:franka_baseline}). On the ``DishRack'' task, \abv is able to exactly match the oracle. VIP performs comparably worse in this setting, likely because its bootstrapping-based RL objective is more difficult to optimize in higher dimensions. Conversely, the uncertainty based metrics perform better in RoboMimic. The  compatibility metric performs quite well on the Franka tasks, likely because the low quality data was explicitly collected with higher entropy, making it easy to distinguish with policy loss alone.



\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{plots/franka_baseline.pdf}
    \vspace{-0.1in}
    \caption{Average quality of demonstrations remaining in datasets after filtering with different choices of $S$ on the Franka Datasets. Average of 3 seeds.}
    \label{fig:franka_baseline}
\end{figure}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.495\linewidth]{plots/rm_state_mis_no_legend.pdf}
    \includegraphics[width=0.495\linewidth]{plots/rm_image_mis_no_legend.pdf}
    \includegraphics[width=\linewidth]{plots/crowd_mis.pdf}
    \vspace{-0.2in}
    \caption{Average quality of demonstrations remaining in datasets across RoboMimic and RoboCrowd after filtering with different mutual information estimators. Again, all experiments are averaged over 3 seeds. We found InfoNCE and MINE to exhibit higher variance than \abv and struggle with higher dimensional inputs, especially with lower amounts of data.}
    \label{fig:mi_estimators}
\end{figure*}

\noindent \textbf{Crowdsourced Data.}
To assess \abv's ability to filter data from a wide variety of operators, styles, and quality levels we turn to the RoboCrowd benchmark. We again find that \abv most consistently filters out low-quality data with respect to the expert labels. The extreme diversity of these datasets, combined with the limited number of demonstrations available (40-100) proves extremely challenging for all baselines, which often provide only a small edge over random sampling. Selecting based on uncertainty performs quite poorly here -- demonstrating that when learning offline, uncertainty is a poor metric, and certainty (its inverse) may perform better. These results suggest that methods designed for active learning and interactive data collection are not sufficient for the problem of offline data curation. When comparing the left side of \cref{fig:robocrowd_image} with the right side, we see that VIP is able to perform better on the ``Play'' datasets can contain task-irrelevant sequences in the demonstrations. While this might be counter-intuitive at first, VIP is goal-conditioned and scores state, next-state tuples based on perceived progress towards the goal. Thus, data with large amounts of irrelevant data extending the length of trajectories, VIP has an easier time filtering. 


\subsection{Mutual Information Estimators}


\cref{fig:mi_estimators} shows the performance of different mutual information estimators across RoboMimic and RoboCrowd. While InfoNCE and MINE perform acceptably in state-based settings, they begin to perform significantly worse in image based settings as the dimensionality of the data increases. InfoNCE in particular performs far worse in RoboMimic, underscoring the raw amount of data needed to train a high quality contrastive representation as documented by prior work. Both MINE and InfoNCE perform poorly in the more data-limited regime in RoboCrowd while \abv, which uses non-parametric estimation no top of representation learning, is able to retain performance. Moreover, we find that \abv exhibits far lower variance across seeds, while the parametric estimators were more unstable and had one or two runs that performed far worse than the others. This is particularly problematic for downstream data curation, as one often does not have ground truth labels to check the quality of the scoring function. \looseness=-1

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{plots/robomimic_policy.pdf}
    \vspace{-0.2in}
    \caption{Performance of ResNet18 + MLP BC Policies trained on filtered subsets of RoboMimic from Images. Evaluations are averaged over 200 trials for each of 3 seeds (600 total) after 100K training steps. Each dataset begins with 300 demonstrations.}
    \label{fig:robomimic_policy}
\end{figure}

\subsection{Does demonstration curation affect policy performance?}
While comparing to ground truth labels allows us to assess the quality of different approaches to filtering, we ultimately care about the performance of downstream BC policies. In \cref{fig:robomimic_policy} we train BC policies on RoboMimic Can MH and Square MH from images when filtering different numbers of episodes from the dataset $\D_N$ according to the best baselines for the tasks. Overall, we find that at all data scales \abv performs better than baselines, which exhibit far less consistent performance trends overall. For example, uncertainty often shows little improvement until the majority of the dataset is filtered. Crucially, we see that filtering data with \abv performs \emph{better} than training on all of the data by over 10\% in Can and is the only method to improve upon training on all of the data in Square. 


\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{plots/real_policy.pdf}
    \vspace{-0.1in}
    \caption{Performance of ACT trained on filtered versus non-filtered RoboCrowd (HiChew and TootsieRoll) and Diffusion Policy trained on filtered versus non-filtered data for the Franka tasks (DishRack and PenInCup). Evaluations for RoboCrowd are shown using the same scoring procedure from \citet{mirchandani2024robocrowd} over 10 trials, and evaluations for Franka are completed over 15 trials.}
    \label{fig:robocrowd_policy}
\end{figure*}

This trend continues in real world evals for both RoboCrowd and Franka where we compare training policies on all of the demonstrations (no filter), training policies on a random 50\% subset of the demonstrations, and filtering 50\% of the demonstrations with \abv in \cref{fig:robocrowd_policy}. We train ACT \citep{zhao2023aloha} for RoboCrowd and Diffusion Policy \citep{chi2023diffusion} for Franka. In RoboCrowd, after scoring trials according to the methodology in \citet{mirchandani2024robocrowd} (1 for grasping any number of candies, 2 for returning it to the user, and 3 for returning only one as in the demonstrations), we find that the \abv policy not only more commonly successfully completes the task, but also exhibits better motion when compared to training on all of the data. When considering the same number of demonstrations randomly selected from the dataset, we find that the gap in score is larger, indicating that better performance can be attained by collecting only good data. On the Franka tasks \abv always outperforms the random subset, but training on all data does slightly better than filtering with \abv on ``PenInCup''. This is likely because ``PenInCup'', being the harder task, requires more data for representation learning even if the quality is poor. However, when dataset size is kept at parity (50\% random subset) we see a huge difference in performance, indicating that in all tasks data quality matters.


\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{plots/ks_ablation.pdf}
    \vspace{-0.1in}
    \caption{Performance of 
    \abv with different ranges of $k$ for $k$-NN}
    \label{fig:ks_ablation}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{plots/knn_ablation.pdf}
    \vspace{-0.15in}
    \caption{Performance of 
    \abv with different $k$-NN mutual information estimators}
    \label{fig:knn_ablation}
\end{figure}


\subsection{Ablations}
Finally, we consider which design choices of \abv affect its ability to curate demonstration data. Here, we consider the value of $k$ used for $k$-NN and the type of non-parameteric mutual information estimator. In \cref{app:results} we include additional ablations over the size of the latent dimensions of $z_s$ and $z_a$ and the value of $\beta$ for the VAEs. \cref{fig:ks_ablation} shows the performance of \abv with different ranges of $k$ used to compute the mutual information. We average final predictions over this range. \abv's performance is generally robust to this parameter, with no substantial change in performance in both HersheyKiss and Square MH. However, the story is different for the choice of $k$-NN estimator. As shown in \cref{fig:knn_ablation}, we found the KSG estimator from \citet{kraskov2004estimating} to be superior to both the BiKSG estimator from \citet{belghazi2018mutual} and the na\"ive application of the differential entropy estimator from \citet{kozachenko1987sample} (KL). This indicates that the quality of the latent space, as well as the quality of the estimator, are important for downstream performance.