\section{Preliminaries}
\label{sec:preliminaries}

\subsection{Imitation Learning}
Broadly, the objective of imitation learning is to learn a policy $\pi_\theta: \mathcal{S} \rightarrow \mathcal{A}$ parameterized by $\theta$ that is able to effectively reproduce the behavior of an expert $\pi_E$ within an environment with state space $\mathcal{S}$, action space $\mathcal{A}$ and horizon $T$.
% initial state distribution $\rho_1$, and transition dynamics $p(\cdot|s,a)$. 
Typically, we measure the similarity between the policy and expert using a divergence between their state visitation distributions:
\begin{equation}
    \min_\theta \kl\left(\rho_{\pi_\theta} || \rho_{\pi_E} \right),
\label{eq:obj}
\end{equation}
where $\rho^t_{\pi}(s)$ is the probability that the policy visits state $s$ at time $t$ and $\rho_\pi(s) = \frac{1}{T}\sum_{t=1}^T \rho^t_{\pi}(s)$ is the average visitation across time. In essence, the above objective states that we want the learned policy to visit the same states as the expert. However, optimizing \cref{eq:obj} is challenging as it requires sampling from the learned policy, which can usually only be done accurately by interacting with the environment. 

Instead, the most common approach to imitation learning, Behavior Cloning (BC), reduces the problem to standard supervised learning \citep{ross2010efficient}. Using the \textit{opposite} direction of the KL divergence with respect to \cref{eq:obj}, $\pi_\theta$ can be learned purely offline.
\begin{equation}
\Ls_\text{BC}(\theta) = \E_{s \sim \rho_{\pi_E}}\left[\kl(\pi_E(\cdot | s) || \pi_\theta( \cdot | s)) \right]
\label{eq:bc}
\end{equation}
In this case, we only need samples from $\pi_E$ typically in the form of a dataset of $N$ demonstrations $\D_N = \{\tau_1, \dots, \tau_N\}$, where each demonstration $\tau_i = (s_1, a_1, s_2, a_2, \dots, s_{T_i}, a_{T_i} )$  of length $T_i$ is a valid sequence through the state-action space according to the dynamics. 

Demonstrations are assumed to be sampled from an absolute expert $\pi_E$---however, this assumption in practice is unrealistic. As an example, though we might only care about completing a ``task'' when learning robot policies, there are often several strategies of doing so, and even when using the same strategy, different demonstrators may be subtly different in how they complete the task, which ends up affecting our empirical estimate of the expert. In robot demonstration curation, we ask how we can better define the empirical expert.

\subsection{Demonstration Curation.}
While theoretical analyses fix the expert distribution, in practice it is empirically defined by the users and practitioners who collect data. In turn, choices made during data collection can affect the performance of a policy trained with behavior cloning. For example, a novice data collector may produce less predictable actions than an experienced one, and pooling together the data from multiple demonstrators may lead to a more complex action distribution. Moreover, choices made within individual demonstrations $\tau$, such as using differing strategies or varied approaches to complete a task, might make learning from the overall dataset $\D_N$ more difficult. Thus, the problem of demonstration curation in imitation learning is concerned with how we can shape the expert policy distribution $\rho_{\pi_E}$ such that we can attain the highest performance at a given task. Mathematically, we do so by adjusting the empirical expert distribution, $\hat{\rho}_{\pi_E}(s) = \frac{1}{n} \sum_{i=1}^n \frac{1}{T} \sum_{t=1}^T \mathds{1}(s = \tau_{i,t})$ of the dataset, where $\tau_{i,t}$ is the $t$th state of the $i$th demonstration.

We consider the general problem of shaping the empirical expert distribution $\hat{\rho}_{\pi_E}(s)$ tabula rasa at the demonstration level. Specifically, our goal is to determine a score function $S(\tau)$ in a purely offline fashion that is able to predict the \textit{quality} of demonstrations, where quality is determined by the performance of a policy trained with behavior cloning on the score-filtered demonstration dataset
\begin{equation}
    \D_N(\kappa, S) = \{\tau_i \mid S(\tau_i) > \kappa,  \forall i = 1, \dots, n \}
    \label{eq:filter_ds}
\end{equation}
for some quality threshold value $\kappa$. This is a more difficult problem than considered in prior work. Data mixing approaches \citet{hejna2024remix} only modify $\hat{\rho}_{\pi_E}(s)$ at the mixture level, i.e. adjusting coarse coefficients $\alpha$ over groups of demonstrations. Instead, considering data curation at the individual demonstration allows us to have a fine-grained understanding of what strategies and expert distributions lead to the best performing policy downstream. Works in interactive data curation necessitate both online access to the environment and expensive oracle feedback  \citep{hoque2021thriftydagger, cui2019uncertainty} for curation. Our setting is purely offline and unsupervised, allowing methods we develop to be applied to virtually any robotics dataset available. However, given we have no explicit signal from the environment in the demonstration curation setting, we aim to define $S$ according to unsupervised objectives, namely mutual information. In the next section, we discuss why mutual information between states and actions can be a valuable scoring function for behavior cloning.

% \section{Method}
% First, we detail why mutual information is a good metric for data curation when learning policies with behavior cloning (\cref{sec:method:mutual-information}). Then, we instantiate \abv, our practical mutual information based method for robot data curation (\cref{sec:method:score}). 

\section{Mutual Information as a Quality Metric}
\label{sec:method:mutual-information}
Mutual information captures the bits of knowledge one gains about one random variable by observing another, in essence measuring predictability. In BC, we want to train a policy $\pi_\theta$ to predict the action $a$ from the state $s$. Thus the mutual information between states and actions is a rather natural choice for a quality metric. In this section we interpret the following factorization of mutual information in the context of robot data curation:
\begin{equation}
    I(S;A) = H(S) - H(A \mid S)
\label{eq:mi}
\end{equation}
where $S$ and $A$ represent random variables for the state and action. First, we will discuss why minimizing the conditional action entropy allows for more accurate policies. Second, we discuss why maximizing state entropy can specifically improve performance in the sequential decision making setting. 

\begin{figure*}
    \centering
    \includegraphics[width=0.925\linewidth]{figures/method_figure.pdf}
    \vspace{-0.1in}
    \caption{A graphical depiction of the \abv method. First, we begin by learning VAEs for states and action chunks to produce latent representations $z_a$ and $z_s$. Using these latent representations, we apply the KSG $k$-nearest-neighbor based mutual information estimator. Finally, we filter demonstrations based on their estimated mutual information.}
    \label{fig:method}
\end{figure*}

\subsection{Minimizing Action Entropy}
Our overall objective is to align the distribution of the learned policy with that of expert data (\cref{eq:obj}). Following Theorem 4.1 of \citet{belkhale2024data}, we can bound the distribution matching objective from \cref{eq:obj} using the log-sum inequality in terms of the divergence between the learned policy and expert policy at each time step:
\begin{equation*}
    \kl\left(\rho_{\pi_\theta} || \rho_{\pi_E} \right) \leq \frac{1}{T} \sum_{t=1}^T (T - t) \E_{s \sim \rho^t_{\pi_\theta}}\left[\kl\left( \pi_\theta(\cdot | s) || \pi_E( \cdot | s)\right)\right].
\end{equation*}
Intuitively, if we can keep the policies close enough to each other at every state, then we should be able to better reproduce the desired state distribution. Below, we use this fact to argue why low conditional action entropy $H(A \mid S)$ (term 2 in \cref{eq:mi}) leads to better BC performance \citep{belkhale2024data}. 

 
\smallskip \noindent \textbf{Ease of Fit.} Lower entropy distributions are generally simpler, possibly making them easier to match. For example, an action distribution that can only take on a single value has zero conditional entropy. Note that BC (\cref{eq:bc}) optimizes the opposite direction of the KL-divergence with respect to the above abound. The forward and reverse KL-divergences are only equal when $\pi_E$ and $\pi_\theta$ are the same. This is more likely to happen for simple distributions, allowing us to make progress towards the true state matching objective in \cref{eq:obj}. 

\smallskip \noindent \textbf{Multimodality.} Lower entropy distributions often have fewer modes or peaks. Given the forward and reverse KL-divergences have different behaviors around modes, e.g., mode-seeking versus mode-covering, they are more likely to exhibit similar behaviors on unimodal datasets.  

\smallskip \noindent \textbf{Privileged Information.} It can be difficult for a policy to fit demonstrations when the data collector has access to information unavailable to the policy. For example, a data collector may have extra sensory information--such as direct line-of-sight to observe objects that are occluded in the robot's camera views. The resulting actions might only be predictable when given access to the unobserved variable $Z$. Mathematically, we can bound the mutual information between the unobserved $Z$ and actions $A$ by $H(A|S) \geq I(A;Z|S)$ \citep{cover1999elements}. Thus, by minimizing the entropy of the action distribution we ensure that unobserved factors have a smaller effect on the data.

\subsection{Maximizing State Entropy}
In addition to minimizing conditional action entropy, mutual information encourages high entropy in the state marginal distribution $H(S)$ (the first term of \cref{eq:mi}). Assuming we are unable to perfectly match the expert, during evaluation the learned policy $\pi_\theta$ is likely to make an imperfect prediction at a given state $s_t$ leading to a new state $s_{t+1}$. If this state is not close to any other state in the training dataset, the policy is unlikely to recover. However, if the dataset $\D_N$ contains higher coverage over the state space, we might expect to have a state similar to $s_{t+1}$ in the dataset. 

Though mutual information is perhaps a natural metric for data curation, it can be practically difficult to estimate \citep{beyond-normal-2023}. In the next section, we detail our approach to obtaining accurate estimates.

\section{Method}
\label{sec:method:score}

In this section we propose the \fullname (\abv) method for computationally estimating mutual information. Though mutual information is usually considered at the distribution or dataset level, we are interested in scoring individual demonstrations for data curation. Thus, we measure the contribution of individual episodes to the overall mutual information of the dataset. Fortunately, this can easily be done as the majority of of empirical mutual information estimators can be decomposed into an average of sample-wise estimators. 
\begin{equation}
    \hat{I}(S;A) = \frac{1}{|\D_N|} \sum_{(s_i,a_i) \in \D_N} \hat{I}(s_i, a_i ; \D_N)
    \label{eq:empirical_mi}
\end{equation}
As previously outlined in \cref{sec:related_work}, there are several possible neural estimators of mutual information which can be applied to high dimensional robotics data. However, the majority of existing methods like InfoNCE \citep{oord2018representation} and MINE \citep{belghazi2018mine} have extremely high sample requirements for effective estimation which are unrealistic for real world BC datasets. To overcome this challenge we propose \fullname, which uses $k$-nearest-neighbor ($k$-NN) estimates of mutual information. Our method involves three steps -- representation learning, mutual information estimation, and scoring -- which we outline below.


\subsection{Representation Learning.} 
As $k$-NN estimators of mutual information do not require training a deep neural network, they have been found to be more sample efficient than other estimators. However, they are typically applied to low-dimensional datasets in contrast with robotics datasets which often contain multiple images and sensors. Directly applying $k$-NN estimators to raw image data may suffer poor performance as distances as become meaningless due to the curse of dimensionality \citep{curseofdim_knn}. To remedy this problem and provide a space suitable for non-parametric estimation we train separate Variational Auto-Encoders (VAEs) \citep{kingma2013auto} to embed both the states and actions into low-dimensional representations. 

We denote embedded states as $z_{s,i} = f_s(s_i)$ and embedded actions as $z_{a,i} = f_a(a_i)$. Though other techniques for representation learning exist, we choose to learn VAEs because they enforce an isotropic Gaussian constraint onto the latent distribution $p(z)$. This is particularly desirable for $k$-NN based mutual information estimators for two reasons. First, enforcing a prior over the latent distribution ensures that distances between embedded states and actions are meaningful -- a necessary prerequisite for statistics based on $k$-NN. Second, $k$-NN based mutual information estimators are commonly assessed on Gaussian distributed data, where they are known to perform well \citep{beyond-normal-2023}. When training the VAEs $f_s$ and $f_a$ we try to select the smallest latent dimension that we believe can sufficiently capture the variable. 

\subsection{k-NN MI Estimation.}
Given a latent representation of the states and actions, we can estimate the contribution of an individual state-action pair to the overall mutual information of the dataset using $k$-NN based estimators. The general intuition behind these estimators is that the probability density function around a sample is proportional to how many other data points are near it, which can be measured with the nearest neighbors. If the density function is high near a data point, then we expect there to be many samples around it and thus have a small $k$-NN distance. Conversely, if the density function is low we expect a large $k$-NN distance. Averaging these density estimates allows us to estimate entropy \citep{kozachenko1987sample}, which can be extended to mutual information. In particular, we use the KSG estimator from \citet{kraskov2004estimating}, which we outline below. 

Let $\rho_{k,i}$ be the $k$-NN distance of the $i$th state action pair $[z_s, z_a]$ in the joint space $\mathcal{Z}_\mathcal{S} \times \mathcal{Z}_\mathcal{A}$ defined using the metric
\begin{equation*}
    ||[z_s, z_a] - [z_s', z_a'] || = \max \{||z_s - z_s'||_2, ||z_a - z_a'||_2 \}.
\end{equation*}
The L2 norm between individual latents follows the Gaussian distribution learned by the VAEs. The infinity norm between the $\mathcal{Z}_\mathcal{S}$ and $\mathcal{Z}_\mathcal{A}$ spaces allows the errors from estimates of $S$ and $A$ to cancel in the final KSG estimator. Then, in the context of \cref{eq:empirical_mi}, the KSG estimator is given by:
\begin{equation*}
    \hat{I}(s_i, a_i ; \D_N) \propto -\psi(n(z_{s,i}) + 1) - \psi(n(z_{a,i}) + 1)
\end{equation*}
where $\psi$ is the di-gamma function and $n(z_{s,i)}$ is
\begin{equation*}
    n(z_{s,i}) = \sum_{j \ne i} \mathds{1}\{ ||z_{s,i} - z_{s,j}||_2 \leq \rho_{k,i} \}
\end{equation*}
or the number of latent states $z_s$ less than or equal to the $k$-nearest-neighbor distance $\rho_{k,i}$ in $\mathcal{Z}_\mathcal{S} \times \mathcal{Z}_\mathcal{A}$. The same quantity is analogously defined for actions. We omit constant terms that do not affect the relative contribution of different state-action pairs to the mutual information. We refer the reader to \cref{fig:method} for a pictoral example.

As computing $k$-NN is computationally prohibitive as the dataset size increases, we take a randomized approach. Using a large batch size, we iterate over the dataset multiple times, each time with a distinct shuffling order. We then compute the mutual information contribution $\hat{I}(s, a ; B)$ within each batch $B$ for multiple values of $k$ and average.



\subsection{Scoring}
Given a set of mutual information estimates, our goal is to determine a scoring function $S$ for each episode $\tau$. Intuitively, we can then define the scoring function for each demonstration as the average contribution of that demonstration $\tau$ to the overall mutual information estimator $\hat{I}(S,A)$. 
\begin{equation*}
    S(\tau) = \frac{1}{T} \sum_{t=1}^{T} \hat{I}(s_t, a_t ; \D_N)
\end{equation*}
Since we are filtering datasets by the score, we primarily care about the relative ordering of mutual information estimates rather than their absolute values. In practice, we standardize the dataset by first clipping state-action estimates $\hat{I}(s, a)$ to lie between the 1st and 99th percentiles to prevent excessive influence of outliers.

\begin{figure}
    \centering
    \includegraphics[width=0.68\linewidth]{figures/square_mh_mi_timestep.pdf}
    \vspace{-0.1in}
    \caption{The average estimated $\hat{I}(s;a)$ per timestep for high quality data (``better'' demonstrators) in ``Square MH'' from RoboMimic \citep{robomimic}. Notice that at the start of the trajectory and after the grasp (75-100 steps), $\hat{I}$ is highest, while it is low during the grasp period (50-75 steps).}
    \label{fig:per_timestep}
\end{figure}

Note that even though we have scores for each state-action pair $\hat{I}(s, a)$, we do not use them to directly filter the data. Such an approach would not only be noisier, but also remove all parts of a task that are inherently harder to predict, but necessary for success. For example, free motion towards an object is likely easy to predict, but the exact time-step at which the gripper should close is hard to predict. We show this in \cref{fig:per_timestep} for the high quality demonstrations in one dataset, where $\hat{I}(s, a)$ is significantly higher at the start during free motion and lowest when grasping the object ($\sim$ 50--75 steps). Filtering data by mutual information at the state-action level thus might drop data for crucial parts of a task that inherently have lower mutual information in favor of easily predictable motion. 


Using the score function $S$, we can subset the dataset to include only demonstrations that contribute positively towards the average mutual information estimate of the dataset.
