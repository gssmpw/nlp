\section{Related Work}
\label{sec:related_work}
As deep learning models have continued to scale, data quality estimation has become an area of increasing interest. Here we review works most relevant to our approach.

\textbf{Data Quality in Vision and Language.} Data quality has most often been studied in the vision and language domains, where modern training pipelines often include multiple steps of quality estimation and de-duplication \citep{albalak2024survey, together2023redpajama, penedo2024fineweb}. For text data, this often consists of simple $n$-gram classifiers, or meta-data filtering, which have been shown to have a large impact on performance \citep{xu2024demystifying}. Other more advanced techniques use unsupervised clustering  \citep{vo2024automatic, NEURIPS2023_a8f8cbd7, abbas2023semdedup, birodkar2019semantic}, most commonly for de-duplication and balancing across clusters. Though these methods improve the diversity of large datasets, they work mostly at scale and independent of label (or in our case action) quality. Methods in group mixing have been shown to increase learning efficiency by improving the dataset's distributional properties, but do so only at the coarse group level \citep{chen2024aioli, chen2024skill, doremi, fandoge}. This is problematic in robotics as we often are actively collecting data, and want to assess trajectories individually. Most related to our approach are techniques based on pre-trained models such as CLIP \citep{fang2024data}. Though these methods do not explicitly make the connection, contrastive models precisely estimate a bound on mutual information \citep{ma2018noise, oord2018representation}. Due to the data requirements of contrastive learning, such techniques rely on large pre-trained models, e.g., CLIP \citep{radford2021learning}, as priors for curation \citep{fang2024data}. Unfortunately, such priors are useless for estimating the mutual information between states and actions. Moreover, training a similar contrastive model from scratch requires hundreds of millions of training examples \citep{xu2024demystifying}. This is rather unrealistic for the current behavior cloning paradigm where even the largest datasets have less than 100,000 demonstrations \citep{droid}, and we do not have access to strong pre-trained action priors.

\textbf{Data Quality in Robotics.} Orthogonal to us, several works have focused on increasing the size of robotics datasets, through the development of tools \citep{pmlr-v155-young21a, chi2024universal}, human teleoperation \citep{droid,openx,jang2022bc,dasari2019robonet,sharma2018multiple,mandlekar2018roboturk,pinto2016supersizing, bharadhwaj2024roboagent, robocasa2024}, or automatic data augmentation  \citep{ha2023scalingup,mandlekar2023mimicgen, belkhale2023hydra, mandi2022cacti}, with the aim of training large-scale robot policies \citep{rt1, rt2, levine2018learning, octo}. Through this process, data quality in the context of robot learning has come into question, but largely through the lens of inter-demonstration compositional generalization to new objects or scenes \citep{burns2023makes, xie2023decomposing,gao2024efficient, lin2024datascalinglawsimitation}. Unlike our work, such approaches do not consider intra-demonstration transition quality, e.g., how good the action labels are which can ultimately determine the performance of imitation learning methods. Other works that consider action quality do so at an extremely coarse level. ReMix \citep{hejna2024remix} learns group weights over large robot datasets using robust optimization. Such dataset mixing approaches require datasets to be partitioned into groups a priori, and are thus unable to determine the quality of individual demonstrations. Retrieval methods \citep{nasiriany2022sailor,du2023behavior,lin2024flowretrieval} use a target dataset to retrieve state-action pairs from unstructured data, but do not explicitly measure data quality, only similarity. Perhaps most related to our work, \citet{kuhar2023learning} directly estimate the quality of individual demonstrations using a latent space from temporal contrastive learning. However, to actually produce quality estimates they assume access to a dataset of human quality labels. Moreover, the choice of temporal contrastive learning means that the learnability of actions is not explicitly considered. \abv on the other hand is completely unsupervised, and thus can be applied to broad amounts of robot data without any hand annotation.

\textbf{Mutual Information Estimation.} Mutual information estimation has been a long studied problem in both statistics and deep learning \citep{poole2019variational}. Direct mutual information objectives like InfoNCE \citep{oord2018representation} are often used for representation learning in vision \citep{chen2020simple} and language \citep{zhang2020unsupervised}. Other works have used the dual formulation \citep{belghazi2018mutual}. Unfortunately, these parametric methods techniques often require on the order of a million samples for accurate estimation, but having access to this scale of data is rather uncommon when trying to measure data quality for imitation learning in a specific environment. Instead, \abv uses non-parametric estimators based $k$-nearest-neighbors \citep{kozachenko1987sample, singh2003nearest}, specifically the KSG estimator \citep{kraskov2004estimating,gao2018demystifying}. Prior works have used $k$-nn estimators in unsupervised RL, but do so for maximizing state entropy \citep{liu2021behavior, kim2024accelerating}, not mutual information or data quality.

