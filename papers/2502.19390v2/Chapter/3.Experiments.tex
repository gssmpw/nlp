\section{Experimental settings}
\subsection{Dataset}
The BraSyn-2023 dataset \cite{baid2021rsna,karargyris2023federated}, sourced from the RSNA-ASNR-MICCAI BraTS 2021 dataset, includes multi-parametric MRI (mpMRI) scans of brain tumors collected retrospectively across various institutions. 
The dataset comprises four modalities: pre-contrast T1-weighted (T1), post-contrast T1-weighted (T1-ce), T2-weighted (T2), and T2 Fluid Attenuated Inversion Recovery (FLAIR). 
These scans were acquired under standard clinical conditions, albeit with different equipment and protocols, leading to variations in image quality.
Expert neuroradiologists reviewed and annotated tumor subregions, including the Gd-enhancing tumor (ET), peritumoral edematous/infiltrated tissue (ED), and necrotic tumor core (NCR). 
The dataset consists of 1,251 scans for training, 219 scans for validation, and 570 scans for the private testing set.
For training, participants receive complete sets of all four modalities with segmentation labels, while in the validation and test sets, one modality is omitted to assess image synthesis methods. 
Standardized preprocessing, including DICOM to NIfTI conversion, co-registration, resampling, and skull-stripping, has been applied, ensuring high-quality data for the challenge \cite{li2023BraSyn}.
\subsection{Evaluation metric}
The inference task in the BraSyn challenge requires algorithms to predict a missing MRI modality from a test set where one of the four modalities (T1, T1-ce, T2, FLAIR) is absent.
The generated images will be evaluated on two aspects: (i) Image Quality—using the Structural Similarity Index Measure (SSIM)~\cite{wang2004image} to compare the synthesized images with real clinical images in both tumor and healthy brain regions, providing two SSIM scores per test subject; (ii) Segmentation Performance—using Dice scores for three tumor structures to evaluate the efficacy of the synthesized images in improving segmentation results. 
The segmentation will be performed using the final FeTS\footnote{https://github.com/FETS-AI/Front-End/} algorithm \cite{pati2022fets}, pre-trained on the FETS brain tumor segmentation dataset. Due to time constraints, we were unable to include segmentation results within the validation phase. However, we believe that our method has the potential to significantly enhance segmentation performance.

\subsection{Implementation Details}
We implement our proposed models using Pytorch\footnote{https://pytorch.org}.
To train the proposed model, \textcolor{black}{Adam optimizer is applied with momentum parameters $\beta_{1}$ and $\beta_{2}$ set to 0.5 and 0.999, respectively.}
The \textcolor{black}{batch size} is set to 4, and the initial learning rate is set to 0.0001. \textcolor{black}{For the training stability~\cite{cao2023ACA}, the self-representation network is pre-trained for 200 epochs. We then loaded the pre-trained parameters into the multi-modal translation network, which is trained for 300 epochs. To perform contrastive learning, we applied the global attention of QS-Attn~\cite{hu2022qs}, using an attention matrix sized $256\times256$. After sorting the attention matrix in ascending order, we utilized only the top 256 features~\cite{hu2022qs}. The features utilized for contrastive learning included those obtained after multi-modal attention as well as those processed through a convolution layer following the multi-modal attention.}
\textcolor{black}{Since our model is designed for 2D images, we transform the 1,251 3D scans into 193,905 2D slices along the axial plane. Out of these slices, only 81,437 containing tumors, as identified by segmentation masks, are used for training.}
In the inference phase, \textcolor{black}{we concatenate the 2D MRI slices generated by our model to construct the final 3D MRI imaging.}
Our four models are trained by adopting a `dedicated training' strategy that trains a series of models individually for each missing situation.
\textcolor{black}{The experimental results are analyzed for each of the four models separately on the provided validation set, using fixed missing modalities rather than randomly dropping them.}

\section{Results and Discussion}
Table \ref{T:result_official} presents the performance of our model based on the validation set from the BraSyn2024 challenge. 
Since the validation set does not contain segmentation masks, we evaluate our model on the SSIM scores for the entire MRI images, without separately analyzing tumor and non-tumor regions.
Shown in the table \ref{T:result_official}, our model achieves promising performance in generating missing FLAIR and T1-ce scenarios, achieving SSIM scores of 0.9071 and 0.9046, respectively. 
For missing T2 and T1 scenarios, the model also demonstrates exceptional performance, achieving notably higher SSIM scores of 0.9327 and 0.9258, respectively.
Moreover, the average SSIM score of 0.9182 indicates that our model maintains high image quality across various missing modality situations.

\input{table_official_val}

\begin{figure*}[hbt!]
\begin{center}
\includegraphics[width=\linewidth]{Figure/Discussion.png}
\end{center}
\caption{Qualitative results of missing T2, FLAIR, T1-ce, and T1 scenarios. The missing target MRI generated by our model is labeled as `-syn', and the missing modality is underlined.} 
\label{fig:qualt}
\end{figure*}

Fig. \ref{fig:qualt}. shows the visual results for the qualitative evaluation of our model. For all missing scenarios, the generated target modality's soft tissue details are generally clear, and the intensity resembles the real missing modality. This indicates that our multi-modal contrastive learning approach has been effective in generating high-quality images. Moreover, the structure of tumor regions are particularly well-preserved, as our translation model contains additional segmentation decoder.
