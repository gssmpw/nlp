\section{Methods}
\begin{figure*}[t]
\begin{center}
\includegraphics[width=\linewidth]{Figure/Overview.png}
\end{center}
\caption{Overall framework of our missing MRI generative model.} 
\label{fig:overview}
\end{figure*}

The goal of missing modality synthesis is to generate images for a missing target modality by utilizing multiple available source images. To achieve this, \textcolor{black}{our model is designed} by incorporating multi-modal contrastive learning and a segmentation decoder, enhancing the ability to learn from multi-source modalities. Our \textcolor{black}{model} builds upon the multi-modal translation network $T$ and self-representation autoencoder network $SR$ of the previously studied ACA-GAN~\cite{cao2023ACA}. Specifically, the multi-modal translation network $T$ comprises a generator $G$ and a discriminator $\mathcal{D}$. 
\textcolor{black}{The $G$ comprises a multi-branch encoder ${G}_{E}$  with an attached single attention module, multi-modal fusion module, a decoder ${G}_{D}$ for generating missing modality, and an additional segmentation decoder ${G}_{SD}$.}
\textcolor{black}{The overall framework of our model is shown in Fig.\ref{fig:overview}.}

\subsection{Multi-modal contrastive learning}
\textcolor{black}{Unlike conventional contrastive learning methods \cite{park2020contrastive,hu2022qs}, focusing on single-source modality, our task requires a distinct approach for multi-source modalities.}
\textcolor{black}{To address this, we integrate multi-modal contrastive learning into our multi-modal translation network.}
\textcolor{black}{By utilizing fused features from all modalities, this approach enables contrastive learning on more robust and generalized features that incorporate both intra- and inter-modal complementary information.}

\textcolor{black}{When the source images are $X=\{x_1, x_2, x_3\}$, and the missing target real image is $y$, each of $x_i$ (where $i\in{1,2,3}$) is processed through its respective multi-branch encoder designed for its specific modality.
These modality-specific features are then processed through the multi-modal fusion module to produce the final fused features $f^{X}_{fusion}$.}
\textcolor{black}{The generated image $G(X)$ follows the same processing steps, resulting in its corresponding feature $f^{G(X)}_{fusion}$. We then compute the multi-modal contrastive loss between $f^{X}_{fusion}$ and $f^{G(X)}_{fusion}$.}
The anchor feature $q$ is extracted from the $f^{G(X)}_{fusion}$, serving as the query. The positive $k^+$ is obtained from the feature at the same location in $f^{X}_{fusion}$, and the negative features ${k}^{-}$ are selected from the remaining locations.
The contrastive loss is defined as follows, ensuring that the query is close to the positive $k^+$ while being pushed away from the negative $k^-$~\cite{park2020contrastive}:

\begin{equation}
    {L}_{con}=-\log\Biggl[\cfrac{\exp(q\cdot{k}^{+}/\tau)}{\exp(q\cdot{k}^{+}/\tau)+\sum\nolimits_{n=1}^{N-1}\exp(q\cdot{k}^{-}_{n}/\tau)}\Biggr], \label{eq8}
\end{equation}
\textcolor{black}{where ${k}^{-}_{n}$ denotes $n$-th negative, and $\tau$ is a temperature parameter that controls the scale of the similarity score~\cite{park2020contrastive}.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Moreover, the effectiveness of contrastive learning depends on the \textcolor{black}{significance} of the information contained in the query~\cite{hu2022qs}. \textcolor{black}{Therefore, rather than using features from randomly selected locations, we employ the Query-selected Attention (QS-Attn)~\cite{hu2022qs} module to select queries based on their measured significance.} In our study, the QS-Attn module selects features based on the entropy of fused multi-modal features ($f^{X}_{fusion}$), thereby enabling the extraction of the most informative features from multi-source modalities. The detailed process begins by converting the $f^{X}_{fusion}$ into a 2D matrix $\mathcal{Q}$. This matrix is then multiplied by its transpose $\mathcal{K}$, resulting in a matrix that undergoes a softmax operation to generate the global attention matrix $A_g$. \textcolor{black}{For each row in $A_g$, the entropy $H_g$ is calculated using the following formula~\cite{hu2022qs}}:


\begin{equation}
 {H}_g(i)=-\sum_{j=1}^{HW}{A}_g(i,j)\log{{A}_g(i,j)}.\label{eq3}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\textcolor{black}{By measuring feature similarity, this module helps identify the most distinct feature for contrastive learning, ensuring that the network focuses on the most informative features during training~\cite{hu2022qs}.}


% Segmentation Decoder
\subsection{Segmentation Decoder}
To enhance our multi-modal translation network, we incorporate an additional segmentation decoder (${G}_{SD}$) into the framework. 
This enhancement allows the network to generate the missing target modality image and predict the segmentation output, simultaneously.
Specifically, the feature $f^{X}_{fusion}$ is fed into both the segmentation decoder (${G}_{SD}$) and the general decoder (${G}_{D}$), producing the segmentation output and the pseudo-target image, respectively.
The output from the segmentation decoder is then compared to the ground-truth segmentation maps.
The segmentation loss is calculated using the cross-entropy loss function as follows~\cite{yi2004automated}:

\begin{equation}
 {L}_{seg} = -\frac{1}{N}\sum_{i=1}^{N}\sum_{c=1}^{C}t_{i,c}\cdot{\log{(p_{i,c})}},
\end{equation}
\textcolor{black}{where $p_{i,c}$ and $t_{i,c}$ represent predicted probabilites and the ground-truth labels for class $c$ at pixel $i$, respectively~\cite{yi2004automated}.} \textcolor{black}{$N$ represents the total pixel count, and $C$ indicates the number of classes~\cite{yi2004automated}, with a total of four.}

\subsection{Overall loss function}
We employ multiple self-representation losses to ensure that target-specific information is effectively conveyed to the generator at all stages.
Similar to AE-GAN~\cite{cao2023AE}, we apply the self-representation losses within the decoder. This loss~\cite{cao2023AE} is particularly crucial and effective as it imposes a constraint ensuring the fused features from multiple source modalities remain similar to the target image features during the upsampling process.
Consequently, this self-representa-tion loss aids the multi-modal translation network in better matching the distribution of the target images, enabling the network to generate pseudo-target images that more closely resemble the target modality images. 
This self-representat-ion loss, defined as $L_{SR\_decoder}$, is calculated as the KL Divergence loss between the decoder features of ${G}_{D}$ and ${SR}_{D}$~\cite{cao2023AE}:

\begin{equation}
\mathcal{L}_{SR\_decoder} = \sum_{i=1}^{N} \text{KL}\left( \log \left(\left({G_D(f_i)} \right) \right) \parallel \left({SR_D(f_i)} \right) \right).
\end{equation}

Moreover, following ACA-GAN~\cite{cao2023ACA}, we employ two self-representation losses at different stages: $L_{SMR}$ and $L_{MMR}$. The $L_{SMR}$ is the $L2$ loss between the multi-branch encoder features and the encoder features of the self-representation network~\cite{cao2023ACA}.
The $L_{MMR}$ is the $L2$ loss between the features passing through the multi-modal fusion module and the final encoder features of the self-representation network~\cite{cao2023ACA}. 
Therefore, the total objective loss is defined as follows:
\begin{equation}
 {L}_{G}={L}_{adv}+\alpha\cdot{L}_{con}+ \beta\cdot{L}_{seg}+\gamma\cdot{L}_{SR\_decoder}+\delta\cdot{L}_{SMR}+\eta\cdot{L}_{MMR},
\end{equation}
\textcolor{black}{where $L_{adv}$ represents the adversarial loss~\cite{goodfellow2020gan}, $\alpha$, $\beta$, $\gamma$, $\delta$ and $\eta$ are hyperparameters used to balance the overall losses. $\beta$ is set to 0.05, while the others are set to 0.1.}




