\section{Introduction}
Reliable brain tumor segmentation in magnetic resonance imaging (MRI) plays a crucial role in diagnosis and treatment \cite{cao2020auto,li2023BraSyn}. 
Since performing this task manually is tedious and highly variable, the various advantages of deep learning-based automated tumor segmentation methods have been studied recently \cite{li2023BraSyn}. 
Most of these recent methods utilize all four commonly used MRI modalities in clinical scenarios, including T1-weighted (T1), contrast enhanced T1-weighted (T1-ce) image, T2-weighted (T2),and Fluid Attenuated Inversion Recovery (FLAIR) image \cite{azad2022review,cao2020auto,li2023BraSyn,wang2023miccai}. 
These various multi-modal MRIs provide complementary information about brain anatomy and pathology, leading to the most accurate diagnoses and effective treatments \cite{azad2022review,meng2024diff}.
\textcolor{black}{However, in clinical practice, MRI sequences are often missing due to time constraints, patient movement artifacts, costs, or high radiation doses \cite{li2023BraSyn,meng2024diff}.} 
To address this issue, there has been growing interest in developing generative models that can synthesize missing modalities from the available ones \cite{li2023BraSyn,meng2024diff}. 
\textcolor{black}{This necessity is also highlighted by the Brain MR Image Synthesis for Tumor Segmentation (BraSyn) challenge, one of the tasks of the Brain Tumor Segmentation (BraTS) challenge 2024 \cite{li2023BraSyn}, in which our team, PLAVE, participated.}

\textcolor{black}{Several studies~\cite{cao2020auto,cao2023AE,cao2023ACA,baltruschat2024pix2} have employed generative adversarial network (GAN) -based models to generate missing target modality from multi-source modalities. Specifically, Cao \textit{et al.}~\cite{cao2020auto} proposed Auto-GAN, which introduces an autoencoder network with a novel self-supervision constraint to provide target-specific information to guide the generator training. An extended version, i.e., AE-GAN, replaces the traditional discriminator with an autoencoder-based discriminator \cite{cao2023AE}.
Another version, i.e., ACA-GAN, effectively extracts complementary information from available modalities by deploying both single-/multi-modal attention modules \cite{cao2023ACA}. These previous studies have shown that GAN-based generative models, self-supervision constraints, and single/multi-modal attention modules can significantly enhance the performance of generating missing modalities.}

\textcolor{black}{Therefore, by extending ACA-GAN~\cite{cao2023ACA}, we design a missing MRI generative model that utilizes contrastive learning with integrated multi-source modalities, with a particular focus on accurately generating tumor regions within the target images.}
Firstly, we integrate multi-modal contrastive learning to ensure that the generator remains invariant to multi-source style variations and learns to focus on the structural information of multi-source modalities. The contrastive learning in natural image translation task is generally used to enhance the learning of structural information within a single source domain \cite{hu2022qs,park2020contrastive}. Meanwhile, since our task deals with multi-modal MRIs, we conduct the multi-modal contrastive learning based on multi-source modalities. We also employ the QS-Attn module~\cite{hu2022qs} to mitigate the issue where limited information in features used for contrastive learning can negatively impact the learning process~\cite{hu2022qs}. This module selects features based on entropy, ensuring that only the most informative features are utilized during our multi-modal contrastive learning. Secondly, our multi-modal translation network not only generates the missing target modality images but also predicts the segmentation output simultaneously.
By incorporating segmentation masks into the training process through segmentation decoder, the network can learn the distinctive features of tumors more effectively from multi-source images.
This approach eventually improves the generator's ability to accurately produce tumor regions, leading to better performance in downstream segmentation tasks.
Additionally, our models are effectively trained by combining contrastive loss, segmentation loss, generator loss, and multiple self-representation losses. To ensure that target-specific information is effectively conveyed to the generator at all stages, we use self-representation losses for decoder features as well as single-modal encoder features and multi-modal fusion features~\cite{cao2023ACA}.
Consequently, through the BraSyn challenge, we demonstrate the performance of our \textcolor{black}{model} by successfully generating high-quality missing modalities from available multi-modal data.