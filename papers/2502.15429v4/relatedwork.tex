\section{Related Work}
%In the academic literature, 
% The %prevalence 
% presence of fraudulent academic articles is increasing, with thousands of fake manuscripts being published in peer-reviewed journals every year~\cite{fakepaper2023nature,parker2024paper}. 
The increasing scientific fraud presents a serious challenge to both the research community and society at large. Previous studies addressing this challenge primarily focus on rule-based heuristics%. These methods are 
, based on analyzing indicators such as author affiliations~\cite{sabel2023fake}, citation patterns~\cite{shepperd2023analysis,feng2024citation}, journal impact factors and author networks~\cite{perez2022threats}. 
Recently, traditional machine learning techniques~\cite{dadkhah2023detection, cabanac2022problematic} as well as BERT-based models~\cite{freedman2024detecting} have been proposed to help with fraudulent article detection. 
%More recently, machine learning techniques such as Decision Trees~\cite{dadkhah2023detection}, trained to distinguish human-written vs. AI-generated text, and BERT-based models~\cite{freedman2024detecting}, %which are specifically trained to distinguish human-written vs. AI-generated text, or 
%trained to distinguish retracted vs accepted manuscripts, have %significantly advanced the field by enhancing 
%been proposed to help with %the accuracy and efficiency of 
%fraud detection.


However,  %Despite 
the rapid growth in the number of fraudulent articles %, there has not been sufficient 
has not been matched by dedicated attention within the NLP field % devoted  to this problem
~\cite{byrne2024call}%, particularly within the NLP research field
. Currently, there is an urgent need for more research into two key aspects.  (1) \emph{Benchmarks}.  
%The current largest open-source dataset related to this task is Retraction Watch~\cite{RetractionWatchDatabase}, which brings together about 50,000 retracted articles in various fields. 
Existing studies generally employ Retraction Watch~\cite{RetractionWatchDatabase}, a blog that monitors and reports on retractions of scientific papers, to analyze retraction behaviors~\cite{shepperd2023analysis, byrne2022protection, freedman2024detecting}.
The Retraction Watch provides a collection of retracted records, but it does not constitute a benchmark on its own for comparing existing methods.
While benchmarks exist for specific fraud types, such as plagiarism~\cite{wahle2021neural} and machine-generated text~\cite{mosca2023distinguishing, abdalla2023benchmark, liyanage2022benchmark}, there is currently no standardized benchmark including diverse fraud types using real-world articles.
(2) \emph{Open-Source Tools}. While some systems for detecting fraudulent articles %already 
exist, they are often proprietary and controlled by commercial entities that are reluctant to share their technologies~\cite{christopher2021raw}.
Even though some LLM-based applications have been developed to address news misinformation~\cite{cao2024can} and plagiarism~\cite{wahle2022large}, there is a need for open-source LLMs specifically tailored to scientific fraud detection.
Such models would enhance transparency and foster collaboration in the fight against scientific fraud.

Note that the development of open-source tools alone is not sufficient; effective fraud detection systems must also be transparent and interpretable. The necessity for systems to provide explanations for their outputs in tasks such as fact-checking~\cite{kotonya2020explainable} and misinformation detection~\cite{explain-misinformation} is well-acknowledged. 
While previous research has explored incorporating explainable AI to explain decision-making processes~\cite{explainLLMs, kotonya2024towards}, the study of explainability in fraud detection systems remains limited.



%\todo[inline]{should we add also relations to debate-based frameworks?}
\begin{table*}[htbp] 
        \centering
        \small
	\setlength{\tabcolsep}{1.8mm}{
		\begin{threeparttable} 
			\begin{tabular}{c|ccc|cccc}  
				\toprule 
                    &\multicolumn{3}{c|}{General}&\multicolumn{4}{c}{Cancer}\cr
                    Dataset &\textbf{\underline{\texttt{Train}}}&\textbf{\underline{\texttt{Validation}}}& \textbf{\underline{\texttt{Test}}}  &\textbf{\underline{\texttt{Breast}}} &\textbf{\underline{\texttt{Lung}}}
                    &\textbf{\underline{\texttt{Ovarian}}}
                    &\textbf{\underline{\texttt{Colorectal}}}\cr
				\midrule
                    Sample &10,000&500&500&300&300&292&300\cr
                    %Retraction 
                    Fraudulence Rate & 24.5\% & 25.6\% & 22.4\%& 25.0\%& 38.7\%& 38.0\%&33.0\%\cr
                    High Profile Rate & 34.0\%&  35.9\% &  33.9\%& 38.7\%& 22.4\%& 29.7\%&28.3\%\cr
				\bottomrule
			\end{tabular}
			\caption{Basic statistics of  \emph{Pubmed Retraction}.
            High Profile Rate refers to the articles deemed fraudulent despite their meta-data being ranked high (details in Appendix~\ref{sec:app_benchmark}). 	\label{tab:stat_pubmed_retraction}}
		\end{threeparttable}
	}
	%\end{minipage}%
 \vspace{-10pt}
\end{table*}