\section{Related Work}
\label{sec:related_work}

%Parallel data mining from web is aimed at alleviating the data scarcity problem for LRLs. However, the quality audits~\cite{ranathunga2024quality,bane2022comparison} conducted on such data revealed that the parallel data is noisy.~\citet{khayrallah2018impact} showed that NMT systems are sensitive to noise and hence a line of research evolved to filter out noise from such parallel data~\cite{sloto2023findings,bucc-2024-building}. 
%~\cite{costa2022nllb,banon2020paracrawl,schwenk2021ccmatrix}
%A summary of the noise categories they have considered in their work is shown in Table~\ref{tab:noise_categories}.

%Early works of PDC techniques focused on using language-agnostic shallow heuristics for filtration~\cite{koehn2005europarl}. Alternatively, more recent PDC techniques have been deep learning-based. Among them, NMT-based scoring~\cite{junczys2018dual} and multiPLM-based scoring~\cite{chaudhary2019low} mechanisms were prominent. It could be observed that the latter was more reliable for LRLs, while the former was most suitable for high-resource language pairs. Other existing work trained a classifier~\cite{accarcciccek2020filtering}, used translation phrase-tables~\cite{minh2023fast} and combinations of the above techniques~\cite{steingrimsson2023filtering,lu2020alibaba} in the PDC task.

%\subsection{multiPLM-based Parallel Data Curation}

In multiPLM-based PDC~\cite{sloto2023findings,haddad2023proceedings,koehn2020findings}, first, the parallel sentences are ranked in descending order using a similarity score calculated between the source and target sentence embeddings obtained from the multiPLM. Then the top-ranked parallel sentence pairs are used to train the NMT system, yielding superior results compared to training with the full corpus. %However, the selection of multiPLMs in related PDC studies remains a subject of debate. \\ 

%In the WMT2023 shared task, LASER2~\cite{artetxe2019massively} was utilized by the task organizers to set the baseline NMT scores, whereas~\citet{steingrimsson2023sentence} implemented a filtration pipeline using LaBSE~\cite{feng2022language}. Additionally,~\citet{gala2023indictrans2} employed LaBSE instead of LASER3 for their filtration task, when both multiPLMs supported the language pairs under consideration. However, the rationale for the choice of multiPLM in the PDC, and its impact on the NMT performance had not been studied in-depth.

While the common practice is to simply use of one those multiPLMs to derive sentence representations~\citet{steingrimsson2023sentence,gala2023indictrans2},\citet{ranathunga2024quality} showed that NMT results vary when they are trained using the samples (top, bottom and random) obtained from a ranked parallel corpora with LASER3, XLMR, and LaBSE. Overall, the top sample produced superior NMT results when trained with the LASER3 ranked corpus. On the other hand, \citet{moon2023doubts} used LASER3 and LaBSE, for scoring and observed that the topmost sample was biased towards certain characteristics of the dataset, such as short sentences, high overlapping of untranslated text~\cite{herold2022detecting} etc. From the NMT perspective, such sentences are considered as noise and they adversely affect the NMT performance.

\subsection{Noise in the web-mined Parallel Data}
Human quality audits conducted by ~\citet{khayrallah2018impact,kreutzer2022quality} have provided insights into the types of noise present in web-mined corpora. Alternatively,~\citet{ranathunga2024quality,bane2022comparison} have conducted human audits to quantify the noise that was present in the ranked corpora using multiPLMs. All such noise classes identified during these works are summarized in Table~\ref{tab:flt_heuristics}.\\

%Identified noise classes
\begin{table}[h]\centering
\scriptsize
\renewcommand{\arraystretch}{1.3}
\resizebox{0.45\textwidth}{!}{%
\begin{tabular}{lrrrrr}
\toprule
Noise Type &\rotatebox{90}{\citet{khayrallah2018impact}} &\rotatebox{90}{\citet{bane2022comparison}} &\rotatebox{90}{\citet{herold2022detecting}} &\rotatebox{90}{\citet{kreutzer2022quality}} &\rotatebox{90}{\citet{ranathunga2024quality}} \\
\midrule
Deduplication - Identical & -& -& -&Y &Y\\
Misaligned sentences &Y &Y &Y &Y &Y\\
Misordered words &Y &Y &Y &Y &Y\\
Spelling permutations & -&Y & -&Y &Y\\
Wrong Language &Y &Y &Y &Y &Y\\
Untranslated Sentences &Y &Y &Y & -&Y\\
Short Sentences &Y & -&Y & -&-\\
Over/Under translation & -&Y &Y &Y&-\\
Mismatch numbers & &Y & -& -&-\\
Machine Translated Sentences & -& -&Y &Y &-\\
Not a language* & -& -& -&Y &Y\\
Correct translation - Short & -& -& -&Y &Y\\
Correct translation - Low quality & -&Y & -&Y &Y\\
Perfect translations & -& -& -&Y &Y\\
Near perfect translation & -& -& -& -&Y\\
\hline
\end{tabular}}
\vspace{1mm}
\caption{Summarized noise classes in existing work.}\label{tab:flt_heuristics}
\end{table}