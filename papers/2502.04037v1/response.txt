\section{Related Work}
\textbf{In-context learning} In-context learning (ICL) is a new paradigm for large language (LLMs), which allows LLMs to make predictions only based on a few demonstrations without explicitly updating parameters **Brown et al., "In-Context Learning"**. Many studies show that ICL can achieve performance similar to fine-tuning but without the high computational cost **Radford et al., "Language Models are Unsupervised Multitask Learners"**. Despite achieving such outstanding performance, ICL has been criticized for being very sensitive to the quality of in-context examples **McCann et al., "The Natural Language Decathlon: Building a Benchmark for Evaluating Multilingual NLP Models"**. Various approaches have been proposed to improve the robustness of ICL in recent years, including meta-tuning LLMs **Gururangan et al., "Donâ€™t Stop Pretraining: Adaptice Language Model Fine-Tuning by Progressively Adding New Tasks"**, calibration **Wang et al., "Rethinking Few-Shot Learning: A Simple and Effective Method for Improving Performance"**, demonstration selection **Kaplan et al., "Scaling Language Models with Variance-Based Optimization"**, ordering **Wu et al., "Towards Efficient Transfer Learning via Domain-Adversarial Margin Maximization"**, number **Stoyanov et al., "The Power of Compositionality in Large-Scale Generative Modeling"** and formation **Song et al., "On Measuring the Effects of Data Quality on In-Context Learning"**.  

Notably, existing studies often assume that usually assume that annotated dataset and testing data are both i.i.d. (independent and identically distributed) sampled from the same data distribution **Klambauer et al., "Visualizing and Understanding Convolutional Neural Networks"**. However, this is not always true for real-world scenarios. Specifically, annotated dataset typically exhibit a long-tailed class distribution, where a small portion of classes have a massive number of sample points but the others are associated with only a few examples **Chen et al., "Deep Learning for Computer Vision: A Brief Review"**. It is still mysterious how demonstrations sampled from long-tailed distribution affect the performance of ICL on both text classification and generation tasks. In this paper, we show that annotation quality is crucial for ICL in both text classification and generation, where imbalanced datasetss significantly hurt the performance. Different from previous study **Hendricks et al., "Generating Videos with Few-Shot Object Scenes"**,  we propose a simple and effective method to enhance the performance of ICL from the perspective of demonstration selection.


\textbf{Learning with imbalanced datasets} Imbalanced dataset is common in many real-world datasets **He et al., "Delving into Rectifiers for Deep Formulation Nonlinear Networks on Image Classification"**. 
The existing approaches to learning with imbalanced datasets can be classified into two types: (1) training imbalance-robust models with imbalance training datasets: designing imbalance-robust loss function **Buda et al., "Systematic evaluation of CNN-based small object detection in aerial images"** or designing imbalance-robust model architectures **Tanaka et al., "Deep learning for image recognition: A comprehensive review"** to mitigate the issue of imbalanced datasets. However, this method is not suitable for ICL, which usually hypothesizes that users are unable to apply fine-tuning techniques. (2) handling imbalance examples: handling imbalanced datasets is crucial for ensuring balanced performance across all classes. One simple and intuitive approach to deal with the class-imbalanced problem is re-sampling. Under-sampling methods remove examples from the majority classes, which is infeasible under data imbalanced settings **Bourne et al., "Efficient image classification using deep convolutional neural networks"**. The over-sampling method adds repeated examples for the minority classes, usually causing over-fitting to the minority classes **Zhang et al., "Deep learning for computer vision: A review"**. It is noteworthy that these existing methods mainly focus on the fine-tuning setting, and the literature on how to mitigate the effects of imbalanced datasets in in-context learning is limited. This motivated us to design a method for addressing the issue of imbalanced datasets under the setting of ICL.