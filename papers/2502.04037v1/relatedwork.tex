\section{Related Work}
\textbf{In-context learning} In-context learning (ICL) is a new paradigm for large language (LLMs), which allows LLMs to make predictions only based on a few demonstrations without explicitly updating parameters \citep{akyrek2023what,hendel2023context,agarwal2024many, dong2024survey,edwards-camacho-collados-2024-language,falck2024martingale}. Many studies show that ICL can achieve performance similar to fine-tuning but without the high computational cost \citep{gonen-etal-2023-demystifying, mosbach-etal-2023-shot,muller2024bayes,panwar2024incontext}. Despite achieving such outstanding performance, ICL has been criticized for being very sensitive to the quality of in-context examples \citep{fei-etal-2023-mitigating,gao2024noise}. Various approaches have been proposed to improve the robustness of ICL in recent years, including meta-tuning LLMs \citep{brunet2023icl}, calibration \citep{abbas2024enhancing},  demonstration selection \citep{zhang-etal-2022-active,nguyen2023context,qin2023context,ye-etal-2023-complementary,gao2024unifying,luo2024context,mo-etal-2024-c}, ordering \citep{lu-etal-2022-fantastically,liu2024let}, number \citep{zhang2025more} and formation \citep{voronov2024mind, yao-etal-2024-samples}.  

Notably, existing studies often assume that usually assume that annotated dataset and testing data are both i.i.d. (independent and identically distributed) sampled from the same data distribution \citep{luo2024context,van2024context}. However, this is not always true for real-world scenarios. Specifically, annotated dataset typically exhibit a long-tailed class distribution, where a small portion of classes have a massive number of sample points but the others are associated with only a few examples \citep{jamal2020rethinking,schultheis2024generalized}. It is still mysterious how demonstrations sampled from long-tailed distribution affect the performance of ICL on both text classification and generation tasks. In this paper, we show that annotation quality is crucial for ICL in both text classification and generation, where imbalanced datasetss significantly hurt the performance. Different from previous study \citep{hong2024mixtures},  we propose a simple and effective method to enhance the performance of ICL from the perspective of demonstration selection.


\textbf{Learning with imbalanced datasets} Imbalanced dataset is common in many real-world datasets \citep{cui2018large,cui2019class,jamal2020rethinking, schultheis2024generalized}. 
The existing approaches to learning with imbalanced datasets can be classified into two types: (1) training imbalance-robust models with imbalance training datasets: designing imbalance-robust loss function \citep{jamal2020rethinking,tan2020equalization,park2023robust,bhat2023robust,garcin2022stochastic} or designing imbalance-robust model architectures \citep{long2022retrieval,pan2024lt} to mitigate the issue of imbalanced datasets. However, this method is not suitable for ICL, which usually hypothesizes that users are unable to apply fine-tuning techniques. (2) handling imbalance examples: handling imbalanced datasets is crucial for ensuring balanced performance across all classes. One simple and intuitive approach to deal with the class-imbalanced problem is re-sampling. Under-sampling methods remove examples from the majority classes, which is infeasible under data imbalanced settings \citep{liu2008exploratory,wei2022open}. The over-sampling method adds repeated examples for the minority classes, usually causing over-fitting to the minority classes \citep{chawla2002smote,shi2023re}. It is noteworthy that these existing methods mainly focus on the fine-tuning setting, and the literature on how to mitigate the effects of imbalanced datasets in in-context learning is limited. This motivated us to design a method for addressing the issue of imbalanced datasets under the setting of ICL.


%