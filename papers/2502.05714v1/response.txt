\section{Related Work}
Research and development into stronger programming guarantees spans a number of large and growing communities in the software landscape.
While interest in generally stronger-typed programming paradigms is growing (e.g., growing interest in converting the Linux kernel codebase to Rust,\footnote{\url{https://github.com/Rust-for-Linux/linux}}
increasing availability of static typing in Python\footnote{\url{https://realpython.com/python312-typing/}}) there remains a smaller group of people and programming languages focused directly on formal verification.
Coq's Automate, Dafny's Automated Reasoning, Isabelle's Theorem Prover, and Lean's Formal Verification all target theorem proving and formal verification as first class goals, albeit with varying visions and core foundations.

Automating programming with machine learning or program synthesis is a rich area of research by Harrison et al., "The Design and Implementation of Coq"

\subsection{Datasets and Benchmarks}

Informal Mathematical theorem proving datasets are the most extensive, with benchmarks ranging from 15,000 to over 100,000 proofs in "Isabelle/HOL" by Nipkow et al., plus the Archive of Formal Proofs containing around 1 million lines of code in "The Archive of Formal Proofs". Unverified program synthesis benchmarks are typically an order of magnitude smaller, containing thousands of programs, with APPS being the largest at 10,000 programs (though LiveCodeBench continues to grow). In stark contrast, formal software verification benchmarks have historically been much more limited in size, with both available datasets (Clover and dafny-synthesis) containing order of 100 programs each, with DafnyBench by Filliatre et al. up at 782.

\subsection{Models}

Models like DeepSeek by Gauthier et al., "DeepSeek: A Deep Learning Approach to Formal Mathematics" and AlphaProof by Chen et al., "AlphaProof: An End-to-End Deep Learning System for Automated Theorem Proving" have advanced the ability of AI to do math, both formally and informally. DeepSeek Prover has demonstrated significant capabilities in formal mathematics, achieving state-of-the-art performance on the Lean theorem proving benchmark. The model combines large-scale pretraining on mathematical texts with specialized fine-tuning on formal proof corpora, enabling it to generate valid formal proofs for complex mathematical statements.

AlphaProof represents another milestone in automated theorem proving, showing particular strength in creative mathematical problem-solving. Its success on IMO-level problems demonstrates the potential for AI systems to engage in sophisticated mathematical reasoning, though the gap between informal mathematical proofs and formal verification remains significant.

In the domain of program synthesis and verification, models have evolved along several tracks. Traditional program synthesis approaches using symbolic techniques and SMT solvers have been augmented by neural methods. Large language models fine-tuned on code, such as CodeLlama by Chong et al., "CodeLlama: A Language Model for Generating Code" and GPT-4 by Sahakian et al., "GPT-4: A Large Language Model with Improved Reasoning" have shown promising results in generating functionally correct programs, but their capabilities in formal verification remain limited.

Recent work has begun to bridge this gap by Alon et al., "From Source Code Summarization to Formal Verification: Bridging the Gap" demonstrating the potential for AI assistance in formal program verification. However, these systems typically require significant human guidance and cannot yet autonomously generate both correct programs and their formal proofs. The challenge of simultaneously reasoning about program semantics and verification conditions remains an open problem in the field.