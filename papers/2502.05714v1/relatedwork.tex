\section{Related Work}
Research and development into stronger programming guarantees spans a number of large and growing communities in the software landscape.
While interest in generally stronger-typed programming paradigms is growing (e.g., growing interest in converting the Linux kernel codebase to Rust,\footnote{\url{https://github.com/Rust-for-Linux/linux}}
increasing availability of static typing in Python\footnote{\url{https://realpython.com/python312-typing/}}) there remains a smaller group of people and programming languages focused directly on formal verification.
Coq~\cite{coq}, Dafny~\cite{dafnybook}, Isabelle~\cite{isabellebook}, and Lean~\cite{demoura2015lean} all target theorem proving and formal verification as first class goals, albeit with varying visions and core foundations.

Automating programming with machine learning or program synthesis is a rich area~\cite{ellis2020dreamcodergrowinggeneralizableinterpretable, austin2021programsynthesislargelanguage}

\subsection{Datasets and Benchmarks}

Informal Mathematical theorem proving datasets are the most extensive, with benchmarks ranging from 15,000 to over 100,000 proofs~\cite{coqgym,leandojo,jiang2021lisa,naturalproofs}, plus the Archive of Formal Proofs containing around 1 million lines of code~\cite{afp}. Unverified program synthesis benchmarks are typically an order of magnitude smaller, containing thousands of programs, with APPS being the largest at 10,000 programs~\cite{Hendrycks2021MeasuringCC} (though LiveCodeBench continues to grow~\cite{livecodebench}). In stark contrast, formal software verification benchmarks have historically been much more limited in size, with both available datasets (Clover and dafny-synthesis) containing order of 100 programs each~\cite{clover,dafny-synthesis}, with DafnyBench~\cite{loughridge2024dafnybench} up at 782.

\subsection{Models}

Models like DeepSeek \cite{xin2024deepseekproveradvancingtheoremproving,xin2024deepseekproverv15harnessingproofassistant} and AlphaProof \cite{deepmind2024imo} have advanced the ability of AI to do math, both formally and informally. DeepSeek Prover has demonstrated significant capabilities in formal mathematics, achieving state-of-the-art performance on the Lean theorem proving benchmark. The model combines large-scale pretraining on mathematical texts with specialized fine-tuning on formal proof corpora, enabling it to generate valid formal proofs for complex mathematical statements.

AlphaProof~\cite{deepmind2024imo} represents another milestone in automated theorem proving, showing particular strength in creative mathematical problem-solving. Its success on IMO-level problems demonstrates the potential for AI systems to engage in sophisticated mathematical reasoning, though the gap between informal mathematical proofs and formal verification remains significant.

In the domain of program synthesis and verification, models have evolved along several tracks. Traditional program synthesis approaches using symbolic techniques and SMT solvers have been augmented by neural methods~\cite{wang2023legoprover}. Large language models fine-tuned on code, such as CodeLlama \cite{Rozire2023CodeLO} and GPT-4\cite{gpt4}, have shown promising results in generating functionally correct programs, but their capabilities in formal verification remain limited.

Recent work has begun to bridge this gap \cite{Baif2024DafnyCopilot, yang2024autoverusautomatedproofgeneration} demonstrating the potential for AI assistance in formal program verification. However, these systems typically require significant human guidance and cannot yet autonomously generate both correct programs and their formal proofs. The challenge of simultaneously reasoning about program semantics and verification conditions remains an open problem in the field.