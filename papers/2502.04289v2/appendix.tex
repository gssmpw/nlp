\appendix
\onecolumn

\section{Notation} \label{app:notation}

\begin{table}[htb]
\centering
\caption{Mathematical notation overview.}
\label{tab:notation}
\begin{tabular}{lll}
\toprule

Symbol & Domain & Definition \\
\hline
\multicolumn{3}{l}{\textbf{General}} \\
$\mathbf{x} = (x_1, \dots, x_d)$ & $\mathbb{R}^d$ & Material composition \\
$x_i$ & $\mathbb{R}$ & Stochiometric fraction of element $i$ \\
$\tilde{\mathbf{x}}$ & $\mathbb{R}^{h}$ & Material embedding \\
$\textit{P}$ & $\mathbb{R}^{d}$ & Precursor material \\
$\textit{S}$ & $\mathbb{R}^{m\times d}$ & Precursor set \\
$\textit{T}$ & $\mathbb{R}^{d}$ & Target material \\
$p$ & $\mathbb{P}$ & Probability of a precursor or set \\
$y$ & $\{0,1\}$ & Whether a target-precursor pair in dataset \\
$\theta$ & -- & Parameterized learned model \\
$\mathbb{B}$ & -- & Binary classifier \\
$\mathcal{L}$ & -- & Loss function \\
\hline
\multicolumn{3}{l}{\textbf{Dimensions}} \\
$d$ & $\mathbb{N}$ & Dimension of composition vector \\
$h$ & $\mathbb{N}$ & Hidden dimension \\
$m$ & $\mathbb{N}$ & Number of precursors per set \\
% $r$ & $\mathbb{N}$ & Number of precursor sets per target \\
$n$ & $\mathbb{N}$ & Number of unique precursors per set \\
$N$ & $\mathbb{N}$ & Number of unique precursor in dataset \\
$K$ & $\mathbb{N}$ & Top-$K$ ranked precursor sets \\
\hline
\multicolumn{3}{l}{\textbf{MTEncoder}} \\
$\mathbf{e}$ & $\mathbb{R}^{h}$ & Learned chemical element embedding \\
$\mathbf{f}$ & $\mathbb{R}^{h}$ & Sinusoidal fractional embedding \\
$\mathbf{z}$ & $\mathbb{R}^{h}$ & Per-element embedding \\
$\mathbf{t}$ & $\mathbb{R}^{h}$ & Compound embedding \\
$\mathbf{s}$ & $\mathbb{R}^{(k+1) \times h}$ & MTE input sequence \\
$k$ & $\mathbb{N}$ & Number distinct elements in composition \\
\bottomrule
\end{tabular}
\end{table}

Example: 

\begin{equation}
    T \leftarrow \underbrace{\{P_1, P_2, P_3 \}}_{r=3} \equiv \{ \underbrace{\{A,B\}}_{m_1=2}, \{A,C,F\}, \{C,G\} \}
\end{equation}

\begin{itemize}
    \item $n=5$ (explanation: there are five unique precursors $\{A,B,C,F,G\}$)
    \item $N$ is a large dataset-dependent number.
\end{itemize}





\section{Dataset}

\begin{table}[h]
    \centering
    \caption{Dataset Statistics including Train, Validation, and Test Splits}
    \label{tab:dataset_stats}
    \begin{tabular}{lccc}
        \toprule
        Dataset & Train & Validation & Test \\
        \midrule
        Complete Reaction Archive    & 9715  & 2430       & 6659 \\
        Distinct Reactions  & 5091  & 1274       & 2893 \\
        Novel Materials Systems    & 3012  & 753        & 2892 \\
        \bottomrule
    \end{tabular}
\end{table}

\label{Appendix:Dataset}

\section{Implementation Details}
Our code will be made available on GitHub upon publication of the manuscript.
% The code for this paper is available at \href{https://github.com/tum-ai/Retro-Rank-In}{https://github.com/tum-ai/Retro-Rank-In}.\AT{Update Link}



%\section{Learning Problem}
 % We denote the embedding of a chemical compound with $\mathbf{x} \in \mathbb{R}^d$. A common method for embedding a compound is through its compositional information, i.e., in a vector of size $d$ being the number of unique chemical elements, each entry corresponds to the ratio of the corresponding atom. For example, ammonia ($NH_3$) can be written as $\mathbf{x}$ with all zeros except for the hydrogen and nitrogen entries $x_1=3/4$ and $x_7=1/4$, respectively. An alternative would be to use a large-scale pre-trained encoder $\mathcal{E}: \text{SMILES} \rightarrow \mathbf{x}$, where \AT{"SMILES is the compositional information" - does the encoder use smiles?} and $d$ is a hyperparameter. We point out that these representations are generic and do not depend on whether a compound is a precursor or a target.

%Based on these embeddings, we aim at solving the inverse design problem of finding the right precursor set $\mathcal{S}=\{\mathbf{x}_1,...,\mathbf{x}_m \}$, where $m$ is the set size. In the following, we explore ranking-based approaches to this problem rather than the commonly used multi-label classification \AT{CITATIONS}.

\subsection{MTEncoder}
\label{appendix_mte}
\cref{fig:plotmte} presents a schematic representation of the MTEncoder architecture, illustrating how material compositions are processed using a transformer-based encoder. The input consists of element tokens (Na, Fe, O), along with a Compound special token (\textit{CPD}), which aggregates information from the constituent elements. These inputs are passed through a transformer model, which learns a contextualized representation of the material composition. The CPD token serves as the learned materials representation, and is then fed into MLP classifiers for property prediction. %Additionally, the diagram highlights a denoising pretraining approach, where a fraction of the input elements is masked, and the model learns to reconstruct them, reinforcing robust and generalizable material representations.

% \begin{table}[h]
%     \centering
%     \begin{tabular}{l}
%         \toprule
%         \textbf{Pretraining Tasks} \\
%         \midrule
%         Stress \\
%         Band Gap (Direct) \\
%         Band Gap (Indirect) \\
%         DOS at Fermi Level \\
%         Energy Above Hull \\
%         Formation Energy \\
%         Corrected Energy \\
%         Phase Separation Energy \\
%         Number of Sites \\
%         Total Magnetization \\
%         Space Group \\
%         Masked Element Modelling (Self-supervised) \\
%         \bottomrule
%     \end{tabular}
%     \caption{Pretraining Tasks}
%     \label{tab:pretraining_tasks}
% \end{table}

\begin{figure}[htb!]
    \centering
    % \begin{minipage}[b]{0.55\textwidth}
        \centering
        \includegraphics[width=0.35\columnwidth]{figures/transformer.png}
        \caption{\textbf{MTEncoder architecture overview.}
        This diagram illustrates the MTEncoder framework, where material compositions are tokenized and processed through a transformer model.}
        \label{fig:plotmte}
\end{figure}

\begin{table}[h]
    \centering
    \begin{tabular}{l}
        \toprule
        \textbf{Pretraining Tasks} \\
        \midrule
        Stress \\
        Band Gap (Direct) \\
        Band Gap (Indirect) \\
        DOS at Fermi Level \\
        Energy Above Hull \\
        Formation Energy \\
        Corrected Energy \\
        Phase Separation Energy \\
        Number of Sites \\
        Total Magnetization \\
        Space Group \\
        Masked Element Modelling (Self-supervised) \\
        \bottomrule
    \end{tabular}
    \caption{\textbf{Pretraining tasks}
    Tasks during MTEncoder pretraining, data is used from alexandria.}
    \label{tab:pretraining_tasks}
\end{table}


\subsection{Training Details}


\paragraph{Model Training.}
We ablate over the number of layers in Retro-Rank-In to assess robustness against this hyperparameter. As shown in \cref{fig:layers}, the model performs consistently well under various depths, peaking around three layers for most metrics. Top-1 accuracy dips slightly at both extremes (one and five layers) but remains stable near the center. Similar trends hold for the other metrics (Top-3, Top-5, Top-10), suggesting that while some tuning of depth may help refine results, the method is generally resilient to layer variations.

\begin{figure}[htb!]
    \label{figure_layer_ablation}
    \centering
    % \begin{minipage}[b]{0.55\textwidth}
        \centering
        \includegraphics[width=0.6\columnwidth]{figures/layer_ablation.png}
        \caption{\textbf{Ablation for layers.} Retro-Rank-In tested for various numbers of layers. Results show the robustness of our method regarding hyperparameter choice.}
        \label{fig:layers}
\end{figure}

\paragraph{Hyperparameters}
For our model, we conducted further hyperparameter tuning with ranges specified in Table \ref{tab:hyperparamter_table}. We explored the following parameter spaces: batch size B in \{128, 256, 512\}, number of attention heads H in \{1, 4, 8\}, number of feedforward (FFWD) layers L in \{1,2,3,4\}, learning rate $\eta$ in [$10^{-5}$, $10^{-3}$] and MTEncoder learning rate $\eta_{MT}$ in [$10^{-6}$, $10^{-4}$]. The optimal configuration was determined based on model performance on the validation set, resulting in the following selected values: batch size of 128, 1 attention head, 3 FFWD layers, learning rate of $6.81 \times 10^{-5}$ and MTEncoder learning rate of $6.37 \times 10^{-5}$. We report test performance using these optimized parameters.
\begin{table}[htb]
\centering
\caption{Hyperparameter configuration of Retro-Rank-In}
\label{tab:hyperparamter_table}
% \small % Makes the text smaller
% \resizebox{\linewidth}{!}{
\begin{tabular}
{l@{\hspace{0.6cm}}c@{\hspace{0.6cm}}c@{\hspace{0.6cm}}c@{\hspace{0.55cm}}c}
\toprule
\multirow{2}{*}{\textbf{Hyperparameters}} & \multicolumn{2}{c}{\textbf{Configuration}} \\ 
\cmidrule(l){2-3} 
 & Search Space & Selected Values \\ 
\midrule
Batch Size (B)& \{128, 256, 512\} & 128 \\
Attention Heads (H)& \{1, 4, 8\} & 1 \\
FFWD Layers (L)& \{1,2,3,4\} & 3 \\
Learning Rate ($\eta$) & [$10^{-5}, 10^{-3}$] & $6.81 \times 10^{-5}$ \\
MT Learning Rate ($\eta_{MT}$) & [$10^{-6}, 10^{-4}$] & $6.37 \times 10^{-5}$ \\
\bottomrule

\end{tabular}
% }
% \normalsize % Resets the font size after the table
\end{table}




\section{Baseline Methods} 

\subsection{Retrieval-Retro}
Retrieval-Retro~\cite{noh2024retrieval} proposes a two-stage approach to inorganic retrosynthesis that implicitly extracts the precursor information of reference materials. First, for each target material, reference materials from the knowledge base of previously synthesized materials are elaborately retrieved by two complementary models: Inspired by Synthesis Similarity~\cite{he2023precursor}, the MPC retriever, trained for Masked Precursor Completion (MPC), selects reference materials sharing similar precursors. The Neural Reaction Energy (NRE) Retriever integrates domain knowledge and leverages the thermodynamic relationships between materials to identify precursor sets with a high probability of synthesizing the target. Representing target and reference materials as fully connected composition graphs, the final Retrieval-Retro stage then employs self-attention and cross-attention mechanisms to implicitly extract relevant precursor information from the reference materials and predict precursor sets based on the probability for each individual precursor.

\subsection{ElemwiseRetro}
ElemwiseRetro~\cite{kim2022element} proposes a template-based approach to inorganic retrosynthesis that represents target materials as fully connected composition graphs. To guide the retrosynthesis process, the researchers distinguish between two types of elements: "source elements," provided as precursors, and "non-source elements," which either appear or disappear during the reaction. For each source element in a target composition, their model predicts the most likely anionic framework—a composition of non-source elements—from a predefined set of templates. The selected source element and its template are then concatenated to form the actual precursor compound, which may be reformulated using a stoichiometric lookup table to ensure frequent and chemically valid compositions.


\subsection{Synthesis Similarity}
\citet{he2023precursor} uses a similarity-based approach to identify precursor sets for inorganic retrosynthesis. They introduce a vector representation for Masked Precursor Completion (MPC) and chemical composition recovery tasks and use this encoding to retrieve reference materials similar to a given target. They initialize the prediction with the precursor set of the reference material and use the MPC network to complement the prediction for a valid precursor set.

\subsection{Mistral 7B}
We employ the Mistral 7B model for LM-based precursor prediction, initially experimenting with few-shot prompting. However, due to insufficient performance, we shifted to a more structured prompting approach. The prompt was as follows: \begin{quote} "You are tasked with identifying precursors for synthesizing the target material \( \text{Mn}_{0.71}\text{Zn}_{0.21}\text{Fe}_{2.08}\text{O}_4 \). You should choose exactly 3 precursors. Generate 20 possible precursor material combinations in descending order of probability. Each route should represent a unique combination of precursors likely to result in the target material. Use the chemical formulas for all precursors instead of their common names. Output them in a Python list format, where each precursor is a string, and each possible combination is a list. Each list should have a length of 3. The response should only include a list of lists where the smaller the index of the list, the higher the probability that the precursor combination has. Do not add any other sentences to the response, only print the list."
\end{quote}This structured prompt, directing the model to rank precursor combinations by probability, improved accuracy. Post-processing, including element validation and duplicate removal, further refined results, making the structured approach more efficient than few-shot prompting

\begin{table*}[htb!]
\centering
\small % Reduce the font size
\caption{\textbf{Performance results for Mistral.} Mistral evaluated across three datasets: (a) Complete Reaction Archive, (b) Distinct Reactions, and (c) Novel Material Systems. Bold values indicate the best performance and underline the second best. All scores are reported as averages over five runs, with standard deviations in parentheses.}
\resizebox{\linewidth}{!}{
\begin{tabular}
{lcccccccccccccc}
\toprule
 & \multicolumn{4}{c}{\textbf{(a) Complete Reaction Archive}} & \multicolumn{6}{c}{\textbf{(b) Distinct Reactions}} & \multicolumn{4}{c}{\textbf{(c) Novel Materials Systems}} \\
\cmidrule(lr){2-5} \cmidrule(lr){7-10} \cmidrule(l){12-15}
\textbf{Model}      & \multicolumn{4}{c}{\textbf{Top-K Accuracy $\uparrow$}} & \multicolumn{6}{c}{\textbf{Top-K Accuracy $\uparrow$}} & \multicolumn{4}{c}{\textbf{Top-K Accuracy $\uparrow$}} \\
\cmidrule(lr){2-5} \cmidrule(lr){7-10} \cmidrule(l){12-15}
      & Top-1 & Top-3 & Top-5 & Top-10 & & 
      Top-1 & Top-3 & Top-5 & Top-10 & &
      Top-1 & Top-3 & Top-5 & Top-10 \\
\midrule
Mistral 7B & 24.75 & 35.34 & 37.62 & 39.78 & & 16.91 & 22.30 & 24.27 & 25.93 & & 16.91 & 22.30 & 24.27 & 25.93 \\
\bottomrule
\end{tabular}}
\end{table*}

\section{Evaluation Protocol}
\label{appendix:eval}

To mitigate the combinatorial explosion, we first select the $30$ precursor candidates with the highest probabilities from the model's predictions. \cref{tab:top-k-analysis} illustrates how increasing this sample size improves Top-K match accuracy but also amplifies computational complexity, as more precursor combinations must be evaluated. These selected precursors are then combined to form candidate sets, and a set is deemed valid if the union of its elements contains all elements of the target composition. Each valid set is assigned a joint probability derived from the probabilities of its individual precursors, and the valid sets are subsequently ranked in descending order by this joint probability. For the Top-K match accuracy, we focus on the subset of $K$ highest-ranked valid sets. If the ground-truth precursor set $S_{\text{true}}$ matches any of these K sets, we assign a score of 1 for that target; otherwise, we assign 0. Finally, we compute the overall score by averaging these individual scores across the entire test set.

We evaluate all approaches in this way, except for Mistral and ElemwiseRetro, which already output the precursor sets. Therefore, we skip the step of constructing the sets from the candidates. The remaining part of the evaluation stays the same.

\vspace{-5mm}
\begin{table}[htb]
\centering
\caption{\textbf{Top-K Match Accuracy across candidate precursor sample sizes $n$.} Investigating the effect of varying amounts of top $n$ candidate precursors sampled based on highest probabilities (see \cref{appendix:eval})), on example Top-K accuracy of Retro-Rank-In, and on the total number of evaluated precursor combinations $N$.}
\label{tab:top-k-analysis}
\small % Makes the text smaller
\begin{tabular}
{l@{\hspace{0.6cm}}c@{\hspace{0.6cm}}c@{\hspace{0.6cm}}c@{\hspace{0.6cm}}c@{\hspace{0.6cm}}c}
\toprule
\multirow{2}{*}{\textbf{$n$}} & \multicolumn{4}{c}{\textbf{Top-K Accuracy $\uparrow$}} & \multirow{2}{*}{\textbf{$N$}} \\ 
\cmidrule(l){2-5} 
 & Top-1 & Top-3 & Top-5 & Top-10\\ 
\midrule
10 & 43.22 & 57.05  & 62.34 & 67.95 & 357,416\\
20 & 43.26 & 57.26  & 62.59 & 68.78 & 7,333,299\\
{\textbf{30}} & 43.30 & 57.43  & 62.90 & 69.29 & 58,386,823\\
40 & 43.57 & 57.81  & 63.31 & 69.81 & 320,996,717\\
\bottomrule

\end{tabular}
\normalsize % Resets the font size after the table
\end{table}




\section{Further Analysis}



\subsection{Hyperparameter ablation} 
We examined the impact of network depth on our model's performance by training Retro-Rank-In with feedforward layers ranging from 1 to 5. With each additional layer, we reduced the dimensionality by a factor of two. Our findings indicate that the model is robust to these hyperparameter variations, with a three-layer architecture yielding the highest Top-1, Top-3, and Top-5 accuracy scores. Consequently, we selected this as our default configuration (\cref{figure_layer_ablation}).

\subsection{Pretraining encoder ablation} 
\cref{tab:ablation_table_3} further examines the impact of no pretraining versus pretraining for the encoder model. We observe the Top-K accuracy drastically decrease without having a pretrained encoder.

\begin{table}[H]%[tb]
\centering
\caption{\textbf{Ablation pretraining.} Investigating the impact of pretraining for the encoder with the Top-K accuracy.}
\label{tab:ablation_table_3}
\small % Makes the text smaller
\resizebox{0.5\linewidth}{!}{
\begin{tabular}
{l@{\hspace{0.6cm}}c@{\hspace{0.6cm}}c@{\hspace{0.6cm}}c@{\hspace{0.55cm}}c}
\toprule
\multirow{2}{*}{\textbf{Pretrained Encoder}} & \multicolumn{4}{c}{\textbf{Top-K Accuracy $\uparrow$}} \\ 
\cmidrule(l){2-5} 
 & Top-1 & Top-3 & Top-5 & Top-10 \\ 
\midrule
\textcolor{red}{\xmark }            &  33.24 & 53.13 & 62.70 & 71.22 \\
& \scriptsize (8.20) & \scriptsize (5.52) & \scriptsize (5.45) & \scriptsize (2.98)\\
\textcolor{blue}{\checkmark }           & \textbf{47.04} & \textbf{62.64} & \textbf{70.05} & \textbf{76.61}  \\
& \scriptsize (1.76) & \scriptsize (1.73) & \scriptsize (2.13) & \scriptsize (1.66)\\

\bottomrule
\end{tabular}}
\normalsize % Resets the font size after the table
\end{table}
%\subsubsection{Influence of pretrained embeddings.}
%Here, we want to investigate the influence of the pretrained MTEncoder. 




\subsection{Performance across different chemistries}

\begin{figure}%[htb!]
    \centering
    % \begin{minipage}[b]{0.55\textwidth}
        \centering
        \includegraphics[width=0.55\columnwidth]{figures/pca_result_ranks_and_targets.png}
        \caption{\textbf{PCA Visualization of target embeddings with rank-based coloring.} 
        PCA of target embeddings, where each point is color-coded based on rank. Higher-ranked points are shown in warmer tones, while lower-ranked ones appear in cooler shades, illustrating the distribution of rankings in the embedding space.}
        \label{tab:pca_dec}
\end{figure}


In Figure~\ref{tab:pca_dec}, we illustrate the correlation between the target embeddings and the ranks assigned by our model, namely the Retro-Rank-In. To achieve this, we first process the chemical composition of each target material using the \texttt{Composition} class from the \texttt{pymatgen} library, which parses the input material string and standardizes its representation to ensure consistency in the interpretation of the chemical formula. The standardized composition was then encoded using the MTEncoder model \cite{prein2023mtencoder}, which maps the material string to a $h$-dimensional tensor representation. In this study, an embedding dimension of $h=512$ was used to capture the essential features of each material for further computational analysis. After acquiring the MTE embeddings, we projected them into a 2D space using Principal Component Analysis (PCA), where each point's color represents its assigned rank. Warmer hues (red) represent higher ranks, while cooler tones (blue to white) indicate lower ranks. The predominance of red points on this plane indicates that most embeddings were correctly classified as Rank 1, demonstrating strong model performance. In contrast, lower-ranking embeddings (e.g., those in blue) appear sparser, occupying smaller regions of the plot. This suggests that while the majority of materials achieve Rank 1, fewer are assigned to lower ranks. In summary, this two‐dimensional projection thus highlights the distribution of performance across the embedding space, revealing that the majority of embeddings cluster at higher ranks while relatively few reside in the lower‐rank region.


\subsection{Precusor correlation}

The top plot of \cref{fig:prec_pairs_correlation} is a significant degree of positive and negative correlation for a fair amount of precursor pairs. The frequency of occurrence for the same pairs is visualized in the bottom figure. While a few unique precursor pairs show strong positive or negative correlations, the vast majority of frequently occurring pairs exhibit little to no correlation.

\begin{figure}[htb!]
    \centering
% \begin{minipage}[b]{0.55\textwidth}s
        \centering
        \includegraphics[width=0.45\columnwidth]{figures/pairs.png}
        \caption{\textbf{Precursor pair correlation.} 
        The plot illustrates the correlation between pairs of precursors. Each point corresponds to a unique precursor pair, sorted along the x-axis by the strength of their correlation. Correlation is quantified here by the logarithm of the ratio of their joint probability to the product of their individual probabilities.}
 \label{fig:prec_pairs_correlation}
\end{figure}

\subsection{Prediction diversity}

\begin{figure}%[htb!]
    \centering
    % \begin{minipage}[b]{0.55\textwidth}
        \centering
        \includegraphics[width=0.4\columnwidth]{figures/unique_precursors_k50_k100_comparison.png}
        \caption{\textbf{Precursor set diversity.} Comparison of the number of unique precursor combinations generated by Retrieval-Retro and Retro-Rank-In.}
        \label{fig:plot10}
\end{figure}


\cref{fig:plot10} compares the number of unique precursor combinations identified by the two models, RetrievalRetro and Retro-Rank-In, at Top-K thresholds of K=50 and K=100. We observe that Retro-Rank-In consistently generates a greater number of unique precursor combinations than RetrievalRetro. This result suggests that Retro-Rank-In can capture a broader space of potential synthetic routes, which is advantageous for identifying novel and efficient strategies in retrosynthetic planning.

\begin{figure}%[htb!]
    \centering
    % \begin{minipage}[b]{0.55\textwidth}
        \centering
        \includegraphics[width=0.9\columnwidth]{figures/set_diversity.png}
        \caption{\textbf{Retro-Rank-In achieves higher diversity of predicted precursors.} PCA plot of a MTEncoder-encoded target material (red color) and precursors predicted by Retrieval-Retro (green triangles, left) and Retro-Rank-In (blue crosses, right). The intensity/alpha of each point is proportional to the probability assigned by each model. Clearly, Retrieval-Retro assigns high probabilities to a small number of precursors, leading to low diversity. In contrast, Retro-Rank-In assigns significant probabilities to a higher number of precursors, leading to higher diversity. Importantly, this improvement in diversity does not come at the expense of accuracy, as shown in Table \ref{table1:performance_comparison}.
        }
        \label{fig:plot9}
\end{figure}

This is further supported by the findings of \cref{fig:plot9}, which shows how each model allocates probability mass across its proposed precursor sets. In particular, RetrievalRetro tends to concentrate most of its probability on just a few highly ranked precursor combinations, as evidenced by a steep probability drop‐off after its top‐ranked suggestions. By contrast, Retro‐Rank‐In spreads its probability more evenly across a larger set of potential combinations, indicating a more diverse exploration of the synthetic space. Crucially, this broader coverage means Retro‐Rank‐In is less likely to overlook innovative or less obvious routes during retrosynthetic planning, offering a more comprehensive foundation for subsequent experimental validation.

\begin{figure}[htb!]
    \centering
    % \begin{minipage}[b]{0.55\textwidth}
        \centering
        \includegraphics[width=0.6\columnwidth]{figures/prob_hist.png}
        \caption{\textbf{Distribution of predicted probabilities.} A comparison of the top 60 highest predicted precursor probabilities across the test set of the Distinct Reactions dataset. Our approach demonstrates improved probability calibration compared to the previous state-of-the-art \citep{noh2024retrieval}. We attribute this improvement to the class-balanced learning of a pairwise ranker, which translates to enhanced throughout performance at higher values of $K$ in Top-$K$ exact match accuracy.}
        \label{fig:rr_ours_prob_dist}
\end{figure}

Lastly, \cref{fig:rr_ours_prob_dist} compares the distribution of predicted precursor probabilities (the top 60 highest values) for Retro-Rank-In (top) and Retrieval-Retro (bottom) on the Distinct Reactions test set. Retro-Rank-In produces a wider spread of mid-to-high probabilities, suggesting more nuanced confidence estimates, while Retrieval-Retro’s predictions cluster near zero or one. This pattern indicates that Retro-Rank-In is better calibrated, resulting in stronger performance at higher values of K in Top-K exact match accuracy.




