% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.
\clubpenalty1000000
\widowpenalty10000000
\documentclass[11pt, table]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding additional package(s)
\usepackage{graphicx}

% Breaks in cells of tables
\usepackage{makecell}

\usepackage{multirow}

% Subfigures
\usepackage{subcaption}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \title{Using Machine Learning Approaches to Affiliate Languages to Families}
% \title{From Isolates to Families: Using Neural Networks to Identify Long-Distance Language Relationships}
\title{From Isolates to Families: Using Neural Networks for Automated Language Affiliation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\author{
 \textbf{Frederic Blum\textsuperscript{1,2}},
 \textbf{Steffen Herbold\textsuperscript{3}},
 \textbf{Johann-Mattis List\textsuperscript{1,2}} \\
%  \textbf{Fifth Author\textsuperscript{1,2}},
 \textsuperscript{1}Department of Linguistic and Cultural Evolution, Max-Planck Institute for Evolutionary Anthropology, 04103 Leipzig, \\
 \textsuperscript{2}University of Passau, Chair of Multilingual Computational Linguistics, Passau, 94032, Germany, \\
 \textsuperscript{3}University of Passau, Chair of AI Engineering, Passau, 94032, Germany \\
 \small{\textbf{Correspondence:} \href{mailto:frederic_blum@eva.mpg.de}{frederic\_blum@eva.mpg.de}}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\maketitle
\begin{abstract}
% Background
In historical linguistics, the affiliation of languages to a common language family is traditionally carried out using a complex workflow that relies on manually comparing individual languages. Large-scale standardized collections of multilingual wordlists and grammatical language structures might help to improve this and open new avenues for developing automated language affiliation workflows.
% Our approach
Here, we present neural network models that use lexical and grammatical data from a worldwide sample of more than 1,000 languages with known affiliations to classify individual languages into families.
% Results
In line with the traditional assumption of most linguists, our results show that models trained on lexical data alone outperform models solely based on grammatical data, whereas combining both types of data yields even better performance.
% Case Studies
In additional experiments, we show how our models can identify long-ranging relations between entire subgroups, how they can be employed to investigate potential relatives of linguistic isolates, and how they can help us to obtain first hints on the affiliation of so far unaffiliated languages. 
% Conclusion
We conclude that models for automated language affiliation trained on lexical and grammatical data provide comparative linguists with a valuable tool for evaluating hypotheses about deep and unknown language relations.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% What is affiliation
One of the central tasks in historical linguistics is the grouping of languages into families. While this is often done to propose new language families, this task also includes the affiliation of individual languages into existing families. The affiliation of languages to a common language family or a subgroup within that family is usually carried out manually and relies on comparing individual languages in depth.
% Two kinds of language classification are distinguished in historical linguistics: the internal sub-classification of languages within language families, and the external classification, that is, the establishing of language families itself. The comparative method is used for both kinds of classification. For this study, we are primarily concerned with affiliating languages to existing families, that is, external language classification.

% Language classification in traditional historical linguistics
The traditional `comparative method' in historical linguistics uses lexical and grammatical features to assign individual languages to one of the more than 200 language families proposed so far \cite{Osthoff1878, Anttila1972, Durie1996}. The main part of this comparison starts with the initial assumption that two languages are related. However, such family relationships do not come as given and must first be established \cite{Hoenigswald1978, Nichols1996, Donohue2012, Campbell2017}. While the rise of computational methods in historical linguistics has largely replaced traditional techniques for subgrouping with computational approaches in phylogenetic reconstruction \cite{Greenhill2020, Wu2020, Blum2023}, the classical workflow for language \emph{affiliation} of the comparative method is still considered the state-of-the-art in the field.
But given that the affiliation of languages to families can be viewed as a computational \emph{classification task}, it is possible to model the task in a setting that benefits from a large number of digital cross-linguistic datasets that have recently been published \citep{List2022e, DoReCo-1.2, ASJPv20, Skirgaard2023, Blum2024}.
% Automated Classification methods
%With large-scale standardized collections of multilingual wordlists and databases for grammatical data having become available in recent years \cite{wals, Skirgaard2023}, automated workflows for language classification at scale have gained increasing interest among scholars \cite{Holman2008, Hammarstr√∂m2014, Wichmann2017}. % The general idea of this task is to test the relationship of languages that are not yet known by comparing lexical data from that language across a large dataset to find possible cognates, traces of deep genealogical relationships. % Most of these approaches use edit distance between word forms of the same concept, compared to an average of edit distance between word forms of two languages, and base their analysis of the comparison between two individual languages.

We show how \emph{automated language affiliation} can be implemented computationally, how this approach can recover known long-distance genealogical relationships, and how it can shed new light on the affiliation of linguistic isolates and small language families. We train models on known language families and affiliate languages of unknown classification with the existing ones while aiming for worldwide coverage. When reporting the results, we pay special attention to the success of classifying small language families. The supervised training approach enables us to build upon the vast existing knowledge of historical linguistics. Having a large baseline of classes corresponding to true language families makes it possible to go beyond individual comparisons and to compare against all families simultaneously.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Background}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Starting with the detection of the Indo-European \citep{Bopp1816} and Uralic \citep{Gyarmathi1799} families towards the beginning of the 19th century, linguists have been able to group the more than 7000 languages still spoken today into several hundred language families. However, while major parts of the traditional workflow of the comparative method have been intensively discussed and partially formalized, the first step of this workflow, the \emph{affiliation} of languages to a family by \emph{proving} their genetic relationship \citep[``proof of relationship'', see][]{Durie1996} still lacks formalization. 
Although many quantitative and qualitative approaches have been proposed throughout the 20th and 21st centuries, none except the comparative method has gained general acceptance.
% among practitioners of historical language comparison. 
Some methods, such as the application of superficial ``mass comparison'' techniques to large wordlists \cite{Greenberg1957}, have been heavily criticized because of their lack of standardized data and clear criteria, and ultimately fell out of vogue \citep{Campbell1988}. Other methods, such as the proposal by \citet{Dolgopolsky1964}, who suggested looking for matching consonant classes to identify potential cognates, were ignored by most scholars for a long time before they were re-adopted in different contexts.
 
While scholars working in the traditional paradigm of the comparative method usually agree that lexical and grammatical evidence combined is best to prove language relationship \citep{Campbell2008}, the former is given preference in those cases where grammatical evidence is hard to obtain \citep{Dybo2008}. Quantitative and statistical methods typically restrict themselves to either lexical \emph{or} grammatical evidence.

Quantitative methods that take lexical data as their primary source can be divided into two basic types, depending on the evidence they try to obtain. Some approaches concentrate on the regularity of sound correspondences, trying to show that pairs of related languages exhibit significantly more matches in sound correspondences than unrelated ones \citep{Ringe1992, Kessler2001, Blevins2021}. Other methods do not use specific sounds as observed in the languages in question and convert them to broader classes (\emph{sound classes} or \emph{consonant classes}, as originally proposed by \citealt{Dolgopolsky1964}, see \citealt{List2014d}) to identify cognate words. These methods argue that words that share direct matches in a certain number of sound classes are likely to be etymologically related and that languages for which a certain number of matches can be observed are likely to be genetically related \citep{Baxter2000, Turchin2010, Kassian2023}. Among the latter approaches, the \emph{Automated Similarity Judgment Program} (ASJP, \href{https://asjp.clld.org}{https://asjp.clld.org}, \citealt{ASJPv20}) deserves special mention, given that it can be seen as a first attempt to automatically classify as many of the languages of the world as possible with the help of phylogenetic methods. Using a specific sound class alphabet by which speech sounds are reduced to 40 classes, ASJP computationally compares word forms in language pairs, using traditional methods for sequence alignment \citep{Wagner1974}, to infer distances between language pairs. These are later used to reconstruct a phylogenetic tree with the help of the \emph{Neighbor-joining} algorithm \citep{Saitou1987}. 

Quantitative methods that exclusively use grammatical data as their primary evidence have less frequently been proposed than their lexical counterparts. Despite this, \citet{Dunn2005} suggest that analyzing grammatical data could lead further back in time than the traditional comparative method. Today, these claims have lost supporters due to other studies indicating that grammatical features alone are less well suited for language classification because they diffuse easily in cases of language contact \cite{Gray2010, Greenhill2010}. The high potential for such diffusion is due to the limited amount of variation that grammatical features exhibit \cite{Wichmann2017}. However, these dynamics remain understudied, and we lack further case studies to analyze the behavior of grammatical data in large-scale classification settings.

%Proposing hypotheses for language classifications at scale were off to a difficult start in historical linguistics. This is partially due to the superficial application of `mass comparisons' by \citet{Greenberg1987}, which, despite its public attention, was heavily criticized by linguists for several methodological shortcomings \cite{Campbell1988}. Those included, among others, the lack of standardized methodology, like concept lists for semantic comparisons, the lack of clear criteria of what is a match or not, as well as the lack of phonetic standardization of word forms.

% Progress
%Despite the shortcomings of such early approaches, some scholars call for a new perspective on large-scale approaches %, improving upon the failed first implementations of `mass comparison'
%\cite{Wichmann2017}. A general comparison of the different methods that can be used for external language classification is presented in Table~\ref{tab:frameworks}. Compared to the first methods of automated classification, recent methods do use some form of standardization of both semantic concepts and phonological forms, and also tend to make their criteria explicit \cite{Brown2008}. In a comparison between traditional classification methods and an automated basic vocabulary comparison for languages in South America, for example, \citet{Hammarstr√∂m2014} finds that both traditional and automated methods tend to come to similar classifications. 

% Early methods of computational approaches
%\textcolor{red}{+++ now mention three method types here: lexical vs. grammatical data, and then two lexical approaches, Ringe and Kessler vs. Baxter, Turchin, and Mortarino}
%\textcolor{red}{+++ part needs reframing, compariong (a) methods based on sound classes vs method based on correspondences, and also methods based on grammar vs. methods based on lexicon, literature seems a bit selective towards what is quoted by Jena / Leipzig folk, e.g, Dolgopolsky 1964 is a method for proving language relationship :-), turchin et al. 2010 is not mentioned, etc.}
%Perhaps the first form of testing language relationships statistically were the permutation tests proposed by \citet{Oswalt1970}. The core idea of those permutations is to calculate similarities between many word forms of different languages. The resulting distribution can then be compared to the similarity of forms from the same concepts to judge statistically whether they are more similar than by chance. %Oswalt's work and later implementations from other scholars \citep{Ringe1992} form the basis of many studies that followed and implemented, in some form or another, instances of the \textit{permutation test}. For example,
%Recent implementations of this test have been used to classify four Papuan languages on Solomun islands \citep{Dunn2012}, for simulating the likelihood of a genealogical relationship between Basque and Proto-Indo-European \cite{Blevins2021}, and for classifying languages from the circumpolar area \citep{Kassian2023}. All those tests, however, remain within the realm of comparing individual sets of languages or language families with each other, without taking the worldwide linguistic diversity into account. 

% Problems with permutations tests - revise, is only in state of first draft
%Other scholars highlight severe shortcomings in the design of permutation tests \citep{Baxter2000}. Despite their generally optimistic stance towards extending the possibilities of historical linguistics through new computational models, the authors criticize the lack of statistical adequacy of the models used. % Another point of criticism is the lack of model validation through testing the models on simulated data and known language relationships.
%They also highlight some key requirements to improve upon this: a) a predetermined list of semantic concepts, b) explicit algorithms for establishing phonetic matches, and c) testing the proposed methods on known language relationships or simulated data. % A first such validation attempt is provided by \citet{Blevins2021}, who test their method on several known language relationships and can show convincingly that they reach a low number of false positives and negatives for their case study. However, further studies are needed to see if those methods can be applied to other cases as well.

% ASJP
%While many approaches focus on specific case-studies to test individual hypotheses \citep{Oswalt1970, Kessler2008, Blevins2021}, the first attempt at large-scale classification of languages was the \emph{Automated Similarity Judgement Program} (ASJP) \citep{Holman2008, Brown2008, ASJPv20}. Using distance measures, they propose the probability of relationships between different languages of different language families. Their approach is similar to Oswalt's in reducing the complexity of the phoneme inventories to a fixed number of symbols and calculating an average score of similarity across scrambled word lists to assess the similarity of actual matches.

% Language classification using typological features
% Use of morphological data: pronouns, more stable grammatical features; RojasBerscia2020?
%Not only lexical data has been used for language classification, but also grammatical data \cite{Wichmann2007}. Some scholars even hoped that typological features could inform about genealogical relationships of languages further back in time than the comparative method %, and have published studies defending this claim
%\cite{Dunn2005}. On the other side are studies which show that typological features are less well suited for language classification due to their easy diffusion in situations of language contact \cite{Gray2010, Greenhill2010}. A likely reason for this is the limited amount of possible variation in the value of grammatical features \cite{Wichmann2017}. However, the dynamics of this have not been well understood, and we lack further case studies to analyze the behavior of typological data in such large-scale classification settings.

% What do we offer
We can now refine those early automated classification methods thanks to the release of new databases \cite{List2022e, Skirgaard2023}. 
% Language Affiliation
The key difference between automated language affiliation and previous computational approaches to language classification is adopting a supervised learning approach. Our method directly benefits from previously established classifications based on the comparative method. It allows us to affiliate previously unclassified languages to existing language families, therefore strictly following an incremental approach to language classification. This approach also allows us to test hypotheses of deep language families, as we will show in our case studies, or to re-consider the affiliation of language isolates to other language families.

We test the model predictions in three case studies (Indo-European, Sino-Tibetan, and Uto-Aztecan) to evaluate the model classification on established language families sharing a long common history. Further, we test the affiliation of four language isolates: Basque, Bangime, Kusunda, and Mapudungun. We also show how this method can contribute to affiliating historical data of unknown classification to existing language families. At the same time, we preserve a conservative approach by including linguistic isolates in the training to restrain the model from unsubstantiated speculation in the form of false positives. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Materials and Methods}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Cross-Linguistic Data}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We use Lexibank \citep[v2.0,][]{Blum2025} and Grambank \citep[v1.0.3,][]{Skirgaard2023}, the two currently largest collections of standardized lexical and grammatical data, to train our model. Both databases are created and published using the \emph{Cross-Linguistic Data Formats} (CLDF, \citealt{Forkel2018}, \href{https://cldf.clld.org}{https://cldf.clld.org}), in which common linguistic constructs, such as \emph{language}, \emph{concept}, and \emph{sound}, are linked to reference catalogs, such as Glottolog \citep[\href{https://glottolog.org}{https://glottolog.org},][]{Glottolog} for languages, Concepticon \citep[\href{https://concepticon.clld.org}{https://concepticon.clld.org},][]{Concepticon} for concepts, and CLTS \citep[\href{https://clts.clld.org}{https://clts.clld.org},][]{CLTS} for speech sounds. This ensures the standardization and comparability of data both within and across datasets. %We use this comparability by combining both kinds of data into a joint model for automated language affiliation. 
To test the quality of the data provided by Lexibank, we train an additional lexical model using the ASJP data \citep[v20,][]{ASJPv20}, which is also available in CLDF.

%\textcolor{red!50}{Do we introduce the consonant-matching baseline here?}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Data Vectorization}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[th]
    \resizebox{\textwidth}{!}{%
    \tabular{cc}
        \begin{tabular}[t]{|l|l|l|l|}
        \hline
            \bfseries Segments &\bfseries  Sound Cl. &\bfseries  Cons. Cl. &\bfseries  Vector  \\ \hline\hline
            d e & \ttfamily TV & \ttfamily T~- & \makecell[l]{\ttfamily {[}1000000000{]} \\ \ttfamily {[}0000000000{]}} \\ \hline
            n a l a &\ttfamily  NVRV &\ttfamily  NR & \makecell[l]{\ttfamily {[}0100000000{]} \\ \ttfamily {[}0010000000{]}} \\ \hline
            n e r a &\ttfamily  NVRV &\ttfamily  NR & \makecell[l]{\ttfamily {[}0100000000{]} \\ \ttfamily {[}0010000000{]}} \\ \hline
            k o k o n &\ttfamily  KVKVN &\ttfamily KK & \makecell[l]{\ttfamily {[}0001000000{]} \\ \ttfamily {[}0000100000{]}} \\ \hline
        \end{tabular} &
        \begin{tabular}[t]{|l|l|c|c|}
            \hline
                \bfseries  Parameter &\bfseries  Type &\bfseries  Value &\bfseries  Vector  \\ \hline\hline
                Fixed order S/A/P & Bin & 0 & \ttfamily [10] \\ \hline
                Fixed order S/A/P & Bin & 1 & \ttfamily [01] \\ \hline
                Fixed order S/A/P & Bin & - &\ttfamily  [00] \\ \hline 
                \multirow{4}*{Order of NUM and N} & 1-3 & 1 & \ttfamily [10] \\
                 & 1-3 & 2 & \ttfamily [01] \\
                & 1-3 & 3 & \ttfamily [11] \\
                & 1-3 & - & \ttfamily [00] \\ \hline
            \end{tabular}
\endtabular}
    \caption{Vectorization of words and grammatical features into vectors. The left table presents the conversion of segments in lexical forms into Dolgopolsky classes. Each class is assigned an index value in the vector, and the corresponding index set to 1. The same procedure applies to the binary Grambank features in the right table. In some cases, the value `3' represents the meaning `both orders are attested', hence both vector indices are set to `1'.}
    \label{tab:lb_vector}
\end{table*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Lexibank and ASJP}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
For Lexibank, we use the 100 concepts that are part of the Swadesh-100 list \citep{Swadesh1955}, given that this list has sufficient coverage across the dataset. However, the method can be used with any concept list that has been standardized in Concepticon \citep{Concepticon}. For ASJP, we use their original 40-item wordlist \cite{Holman2008}.

We convert the lexical forms into vectors that can be used as input for the neural network. This processing step is illustrated in Table~\ref{tab:lb_vector}. Each segment in Lexibank and ASJP is standardized phonetically. To improve the comparability, we convert the sounds to their corresponding \emph{Dolgopolsky class}, based on the sound classes proposed by \citet{Dolgopolsky1964}. Those ten classes are based on the likelihood of sound change between individual sounds. Such sound change happens frequently between sounds within a class of similar articulation, but not between sounds of different classes \citep[for details, see][]{List2014d}. Following the original proposal, only the first two consonant classes are considered. All additional consonants as well as vowels -- except for word-initial vowels, which are considered a special consonant class \textsc{h} -- are removed from the representation. We then convert the classes to indices of a one-hot vector of length 10, the number of Dolgopolsky sound classes, with the index of the attested class set to 1. All vector indices remain zero if a word is not attested in the language. All individual word vectors are concatenated for the language, and given as input to the model.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Grambank}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Grambank is a database of 195 typological features from more than 2,000 languages \cite{Skirgaard2023}. Since most features are coded as binary, converting them into vectors is more straightforward than with Lexibank. Each value is mapped to an index in a one-hot vector of length two, and the index related to the attested value in a language is set to 1. Some word order features have three possible values, where `3' represents the meaning `both 1 and 2 are attested'. Therefore, both indices in the vector are set to 1.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Baseline}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We designed a fast and simple test to have a baseline comparison for our neural network models, based on the idea that the number of matching consonant classes can give hints on cognate words \cite{Dolgopolsky1964, Turchin2010}. In this baseline, we compare one language with unknown affiliation $L_1$ against all the other language varieties in the training data and assign it to the language family of the language $L_{max}$ that shares the largest number of words matching in the two first consonant classes with $L_1$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Neural Network Model Training}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The neural network models are trained on the input vector of lexical, grammatical, or combined data as well as the labels of the language families. We train the models on an 80\% sample of the data stratified by family labels and evaluate the performance of each model on the remaining 20\% to find the best-performing one based on the balanced accuracy across language families \cite{Brodersen2010}. This split is necessary to avoid over-fitting the model to the training data \cite{dietterich1995overfitting}. Even though a split into an additional development- or tuning-set would improve the model training even further \cite{van-der-goot-2021}, the data scarcity makes this a difficult enterprise. To test the robustness of our results despite the small sample size, we run each model 100 times with random seeds, so that different train/test sets are used \cite{Gorman2019, Vabalas2019, Coltekin2020}. As an additional measure against over-fitting, we implement an early-stopping strategy during training \cite{Ying2019}. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Evaluation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We conduct four different experiments. The first experiment (¬ß~\ref{test1}) consists of a model comparison using a common subset of languages attested in ASJP, Lexibank, and Grambank. This comparison tests our baseline and the neural network models using an identical selection of languages.

In the second experiment (¬ß~\ref{test2}) we evaluate how well our models can affiliate entire subgroups with the correct language family. We select three language families -- Indo-European, Sino-Tibetan, and Uto-Aztecan -- in which larger branches have split off considerably early during their evolution. We then train our models without the languages corresponding to these branches and test how automated language affiliation succeeds in assigning these languages to the correct family.

In the third experiment (¬ß~\ref{test3}), we investigate how our models affiliate language isolates, that is, languages which could so far not been convincingly assigned to \emph{any} established language family. We use \href{https://glottolog.org/resource/languoid/id/bang1363}{Bangime}, \href{https://glottolog.org/resource/languoid/id/basq1248}{Basque}, \href{https://glottolog.org/resource/languoid/id/kusu1250}{Kusunda}, and \href{https://glottolog.org/resource/languoid/id/mapu1245}{Mapudungun} as exemplary case studies.

In the fourth and final experiment (¬ß~\ref{test4}), we demonstrate with the example of Carar√≠ \cite{Natterer} how historical languages that have not been affiliated with any language family so far can be investigated with our automated language affiliation models.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Implementation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We implemented our models with PyTorch (v2.5.1, \citealt{Ansel2024}) in a feed-forward neural network with two hidden layers with a ReLU activation function. The hidden layers have a size of four times the number of language families. We process the datasets with SQLite (\href{https://www.sqlite.org/}{https://www.sqlite.org/}) after conversion from CLDF via PyCLDF (v1.40.4, \citealt{PyCLDF}). We converted the segments to sound classes with LingPy (v2.6.13, \citealt{LingPy}).

We use a weighted CrossEntropy loss function to better account for the many small language families present in our data \cite{Zhang2018}. We used an Adam optimizer with a learning rate of 1e-3. The batch size we used is 2048. The hyperparameters were chosen on individual model comparisons between common values. Each training run consists of 5,000 epochs and is canceled if no improvement is made for 500 epochs. The models were trained on a V100 GPU node on a high-performance cluster, taking approximately 90 minutes. They can also be trained on ordinary computers due to the small size of the underlying data.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Initial Model Comparison}\label{test1}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t]
\centering
        \includegraphics[width=\textwidth]{images/violin-scores.pdf}
        \caption{Classification results for all models, based on 100 runs with random seeds for the train/test split. Vertical lines indicate minimum, maximum, 25th, and 75th percentile, as well as the mean. The comparison is based on the balanced accuracy across language families to account for the difficulty of classifying small language families.}
        \label{fig:results}
\end{figure*}

We compared our two baseline models (ASJP and Lexibank) to four neural network models for automated language classification (ASJP, Grambank, Lexibank, and Grambank/Lexibank combined). In this test, we selected all 1057 languages common to ASJP, Lexibank, and Grambank, which belong to language families with at least five members. The classification target consisted of 29 different language families, including one for \emph{isolates} (languages not assigned to any family).

\begin{table}[t]
\centering
\tabular{|l|c|c|}
\hline
\bfseries Model & \bfseries Accuracy & \bfseries SD \\\hline\hline
ASJP Baseline & 83.74 &  3.25 \\\hline
Lexibank Baseline & 83.36 & 3.35 \\\hline
ASJP ALA &  80.13 & 3.85 \\\hline
Grambank ALA & 68.11 & 5.07 \\\hline
Lexibank ALA & 83.73 & 3.64 \\\hline
Combined ALA & 87.75 & 3.59 \\\hline
\endtabular
\caption{Results of all models in the model comparison.}
\label{tab:res}
\end{table}

The results in Figure \ref{fig:results} and Table \ref{tab:res} show the performance of all six models, based on the balanced accuracies from each of the 100 runs. The lexical models strongly outperform the model relying exclusively on grammatical data. The baseline models perform similarly to the Lexibank neural network model, while the neural network model using ASJP data falls off. The combined Lexibank/Grambank neural network model outperforms all models by about four points in accuracy.

Our results show that the simple baseline models perform on par with the more complex model structures. The combined model is the only exception, outperforming all other models in the comparison. This suggests that language affiliation benefits from a holistic approach combining lexicon and grammar data, confirming traditional assumptions from historical linguists. To further explore the potential of neural network approaches to automated language affiliation, we concentrate on the models based on Grambank, Lexibank, and their combined data in the following experiments. 
%From these results we can conclude that the simple and computationally fast baselines for language affiliation do a surprisingly good job in comparison with neural network models based on lexical data. Given that the Combined model outperforms all models, however, shows nicely that  
%In general, we find that the lexical models strongly outperform the model that relies on purely grammatical data. The two strongest models in our comparison (lexibank, combined) achieve an F\textsubscript{1}-macro average between 86\% and 89\%. For further comparison, we have computed a Robust Bayesian Estimation to compare the performance across the different models. This approach is similar, but for a variety of reasons superior to classical t-tests \cite{Kruschke2013}. The results show largely overlapping posterior distributions for the Lexibank and the combined model, with the latter being slightly ahead of the purely lexical model. We can therefore conclude the near equal performance of the lexical and combined models on the basis of descriptive and inferential statistics.
%In contrast to previous studies \cite{Holman2008}, we exclusively report the F\textsubscript{1}-macro average over language families. This prevents the inflated accuracy score that stems from language families consisting of several hundred members, such as Austronesian, which have a much higher micro-average than the small language families with only a handful of members. While we easily achieve over 90\% overall accuracy for the lexical data and over 85\% for the grammatical data, which is much higher than the original ASJP classification scores \cite{Holman2008}, we propose the macro average as a better balanced score to represent the efficiency of the classification model in order to account for the difficulty of classifying small language families.

%We build four neural network models for language classification with different sets of input, a) lexical data from Lexibank \cite{List2022e}, b) grammatical data from Grambank \cite{Skirgaard2023}, c) the combination of both datasets, as well as d) a model trained on the ASJP lexical data. The latter is a dataset with more languages, but less concepts than Lexibank. For all datasets, we use a minimum of 5 languages in the data, for the simple reason that this assures that on average, we have one language in the test split and four in the training split, considering our 80/20 split during training. This is implemented through a stratified split on the classification label.


% In Fig.~\ref{fig:results}, we also present the accuracy across the individual families, plotted against the number of languages present in the data. As one can observe, there is a general trend that the classification has a higher accuracy for large language families. Even though we to balance this through a weighted loss function, it seems intuitive that having more languages in a family, we are able to train on a wider spectrum of variety for that language family than with only a handful of representatives. Adding to that, some of the families that show continuous bad results across models, like Nuclear Macro-J√™, are known to represent extremely deep language families with a lot of family-internal variation \cite{Michael2021}. The large language families, on the other hand, tend to classify extremely well.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Finding Deep Genealogical Relations}\label{test2}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 %using scenarios from Indo-European, Sino-Tibetan, and Uto-Aztecan.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Indo-European}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We conducted three case studies testing the affiliation of entire subgroups. Our first test is based on the Indo-European language family, spoken mainly in Europe, Northern India, and the Iranian plateau. The time depth for the initial split of the first branches, Anatolian and Tocharian, from the rest of the language family, is contested, with individual proposals ranging from 6,000 years ago \cite{Anthony2015} up to 8,000 years before present \cite{Heggarty2023}. We separate the languages from those two branches from the training data. The Lexibank model correctly classifies the languages as Indo-European (100\%). Additional tests could not be carried out, since the languages in question are not coded for Grambank.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Sino-Tibetan}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Estimates for the age of the Sino-Tibetan language family range between 5,900 years \cite{Zhang2019} and 7,200 years \cite{Sagart2019}. In our test, we separate the languages of the Sinitic branch (the Chinese languages), commonly believed to be one of the earliest branches to split off from the ancestral proto-language, from the training data, and train our models without them.
Similar to the test on Indo-European languages, the Lexibank model successfully classifies the Sinitic branch to be part of the Sino-Tibetan language family, with an overall accuracy of 87.5\%. The combined model surpasses the Lexibank model in accuracy, reaching 98\%.
The Grambank model classifies only one variety (\href{https://glottolog.org/resource/languoid/id/wutu1241}{wutu1241}) primarily as Sino-Tibetan, whereas the other varieties from Sinitic tend to be classified either as Hmong-Mien (\href{https://glottolog.org/resource/languoid/id/mand1415}{mand1415}, \href{https://glottolog.org/resource/languoid/id/wuch1236}{wuch1236}) or Austroasiatic (\href{https://glottolog.org/resource/languoid/id/hakk1236}{hakk1236}). At least in the case of Hmong-Mien, the classification of the grammatical model points to an important role of areality.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Uto-Aztecan}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Uto-Aztecan is one of the largest language families spoken in North America, located primarily on the west coast of the Pacific \cite{Campbell1997}. It consists of two main branches, the northern and the southern languages. This split is estimated to have occurred between 3,258 and 5,025 years ago \cite{Greenhill2023}. The Lexibank model classifies the northern branch as Uto-Aztecan in about 40\% of cases. The grammatical model fails in this classification task and only achieves 10\% accuracy. Instead, the languages are classified as Cariban, Chibchan, or Pama-Nyungan. In this case, the grammatical data also seems to drag down the accuracy of the combined model (26\%), which performs worse than the Lexibank data alone.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Affiliation of Isolates}\label{test3}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{images/isolate-plot.pdf}
    \caption{Results for the experiment on isolate affiliation. Results are limited to the first three families to which an isolate is affiliated, showing the proportion of the remaining families under the label \textit{Rest} in the charts.}
    \label{fig:isolates}
\end{figure*}

We test our models on four language isolates that have found recent attention in the literature. Bangime, a language spoken in central-eastern Mali, is considered an isolate that has been in contact with many surrounding languages for such a long time that the speakers consider it related to the neighboring Dogon languages \cite{Hantgan2022}. The data on Bangime in Lexibank is based on \citet{Hantgan2022}. Basque has been recently hypothesized to be part of a Proto-Euskarian-Indo-European language family using both traditional and computational methodology \citep{Blevins2018, Blevins2021}. The data on Basque in Lexibank is taken from \citep{Dellert2020}. Kusunda is spoken in Nepal, but has previously been hypothesized to be related to Papuan languages \cite{Whitehouse2004}, the Dene family, or Yenisseien \cite{Gerber2017, vanDriem2014}. However, no classification has found broad acceptance, and the language remains classified as isolate. A new wordlist recorded in 2020 by \citet{Aaley2020} has been included in Lexibank. Finally, Mapudungun is a language spoken in south-eastern South America. The available evidence suggests that Mapudungun's genealogical relations with other languages are restricted to close relatives that have since become dormant, with no indication of deeper connections. The data for Mapudungun in Lexibank is based on \citet{Tadmor2009} with phonetic mappings by \citet{Miller2020}.

% Results Lexibank
The results are presented in Figure~\ref{fig:isolates}. Bangime is classified consistently as Dogon in the Lexibank model (95\%) and as Mande (66\%) or Atlantic-Congo (29\%) in the Grambank model. Consequently, the combined model primarily has Bangime unclassified (86\%). Basque on the other hand remains mostly unclassified in both the Lexibank (46\%) and the Grambank model (47\%), although the latter also tends to propose an affiliation with Sino-Tibetan (39\%). The combined model proposes an affiliation with Indo-European (23\%) or Sino-Tibetan (18\%) but also includes the unclassified affiliation (32\%). In the Lexibank model, Kusunda is affiliated either with Nuclear Trans-New Guinea (51\%), Austroasiatic (19\%), or left unclassified (17\%). The Grambank model mostly affiliates Kusunda with the Sino-Tibetan family (60\%). The combined model mostly proposes no affiliation of Kusunda with other language families (79\%). Mapudungun is split between several language families in the Lexibank model: Timor-Alor-Pantar (29\%), Austronesian (18\%), or Unclassified (11\%). The Grambank model mostly suggests a Salishan affiliation (68\%) or no affiliation (17\%). The combined model, again, finds no clear affiliation pattern (88\%).

Given that the status of all four languages concerning their affiliation with other language families has been disputed without a result for a long time, it would go too far to speculate on any particular finding presented in the charts here. What we can see, however, is a tendency for the combined model to affiliate the four isolates with the large group of unclassified (i.e., isolate) languages in our sample. The Lexibank and Grambank models differ quite remarkably in this regard, often giving preference to particular language families. 

The Lexibank model classifies Bangime as a Dogon language, reflecting the well-known fact that the lexicon shares many words with this family \citep{Hantgan2022b}, whereas the grammatical model suggests an affiliation with Mande languages. If we consider the results from the previous tests indicative and the affiliation of the grammatical model as being prone to contact phenomena, heavy grammatical restructuring for Bangime based on Mande languages seems likely. This mirrors previous arguments that typological features reflect geographical distributions, rather than genealogical relations \cite{Greenhill2010, Gray2010, Donohue2011}, and contributes directly to our understanding of the linguistic layers of Bangime, which so far focused on the lexicon \cite{Hantgan2022}. Given that genetic studies show that the speakers are unique in the region concerning their genetic history \cite{Babiker2020}, it is reasonable to classify Bangime as an isolate, as does the Combined model. 

For Kusunda, we find a lexical link to Nuclear Trans-New Guinea languages, while the grammar fits well into the Sino-Tibetan neighborhood where the language is spoken. This finding aligns with previous research that suggests that Kusunda is genealogically a Trans-New Guinea language that has recently migrated to its current location \cite{Whitehouse2004, vanDriem2014}. This is where Kusunda would have come into contact with Sino-Tibetan languages, adopting several grammatical features from this language family.

The closeness between Basque and Indo-European suggested by the combined model has been recently suggested using both traditional and computational methods \cite{Blevins2018, Blevins2021}. The connection of Basque with Sino-Tibetan suggested by the Grambank model and the connection with Nakh-Daghestanian suggested by the Lexibank model would fit with the far-ranging proposal of a Sino-Caucasian macro-family, in which scholars at times include Basque \citep{Starostin2017}.

For Mapudungun, few prominent classification hypotheses exist in the literature \cite{Campbell2012}. All our models tend to affiliate the language to some degree with Austronesian, with some suggesting a Timor-Alor-Pantar (lexical) or Salishan (grammatical) affiliation. We are not aware of previous mentions of those affiliations. One way to explore them would be to analyze the individual shared data points to evaluate the possibility of chance similarities. This points also to the major drawbacks of the neural network approach, as it does not allow us to directly determine the concrete words or grammatical features that contribute to a particular decision.

% The ASJP model has similar, yet a bit different results. Both Basque (85\%) and Mapudungun (99\%) are firmly classified as Isolates. Bangime is mostly classified as Atlantic-Congo (88\%), and Kusunda as Nuclear Trans-New Guinea language (NTNG, 74\%). One interpretation when comparing the ASJP and Lexibank results would be that the vocabulary interpreted as being more `basic' \cite{Wichmann2017} points to an affiliation of Bangime to Atlantic-Congo, and Kusunda as NTNG - with more recent contact to the other language families, reflecting in the shared affiliations with Dogon (Bangime) and Sino-Tibetan and Austroasiatic (Kusunda). On the other hand, both Atlantic-Congo and NTNG are large families in the dataset and tend to pool a lot of diversity, hence increasing the likelihood of a language being classified to them, especially with a reduced dataset that might be more prone to individual cases of undetected borrowing like ASJP.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Classification of Unaffiliated Languages}\label{test4}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t!]
    \centering
    \resizebox{\textwidth}{!}{%
    \tabular{cc}
        \includegraphics[height=1.25cm]{images/carari_original.png}
        &
        \includegraphics[height=1.25cm]{images/carari_transcribed.png}
    \endtabular}
    \caption{The left shows some of the original Carar√≠ data published by \citet{Natterer}. The right shows our standardization of the same entries using the EDICTOR tool \cite{EDICTOR-3.1}, with the original transcriptions given in the column `Form'.}
    \label{fig:carari}
\end{figure*}

The lexical model can also affiliate newly identified or historical languages documented in ancient sources to language families. To illustrate this, we affiliated data from Carar√≠, a language documented during the early 19th century by Johann Natterer at the confluence of the Mucuim River and the Pur√∫s in the Brazilian Amazon \cite{Natterer}. According to \citet{Adelaar2014}, some of the languages documented by \citeauthor{Natterer} could not yet be classified -- Carar√≠ being one of them. However, they suggest that an Arawak affiliation might be identifiable with more comparative work done in the future.

In the first step, we digitized the data and converted the wordlist into the Lexibank format. Figure~\ref{fig:carari} presents the original and the standardized versions of the documented word forms. Through this formatting, we can easily input this data into the lexical model by adding the database with the same workflow as the rest of the data. The results are clear: the Lexibank model strongly suggests an Arawak affiliation of the language (80\%).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion and Conclusion}
% \textcolor{red}{+++ The framing must be modified. We must say that we have a new approach to language affiliation that is based on classification not on phylogenetic reconstruction, and that this architecture offers several ways to address outstanding problems in historical linguistics, then we summarize these problems and say that we learn that lexical models by and large perform well, but that grammatical models may offer an interesting additional perspective on areal connections}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Check old hypotheses of language relationships to construct new hypotheses that can be verified using the comparative method
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Short summary of results - Lexicon beats Grammar
We have presented in this study a new approach to \textit{automated language affiliation}. The lexical and combined models show good classification results despite their simple architecture. Given the strong performance, we can use the models for downstream tasks such as testing long-distance relationships between subgroups of language families and the affiliation of unclassified language varieties. The first experiment shows that our models correctly identify such long-distance relationships at a time depth of 5,000 years and more. When testing the affiliation of linguistic isolates, our models reflect the actual discussions in the linguistic literature. This shows that our method can extract information from sparse data in a way that can be compared with language-specific studies.

From our findings, we can make three conclusions, (a) language affiliation achieves promising results even for language relations way back in time, (b) grammar alone is not sufficient for a successful affiliation, and (c) combined models seem to work very well, reflecting that languages are best affiliated by using lexicon plus a bit of grammar \cite{Campbell2008}. However, the combined data is only available for a small subset of languages. In cases of data scarcity, the lexical models are almost on par with the combined model and strongly outperform comparable models based on grammatical data \cite{Holman2008}. In individual cases (e.g. testing Uto-Aztecan), the lexical model even outperforms the combined model. 
% The role of areal spread
% Our results further strengthen the argument that typological features are not well suited for automated classification tasks. As previously shown in the ASJP studies, the typological data performs worse than their lexical counterparts \cite{Holman2008}. % In many tests, the languages are affiliated with neighboring language families when using the grammatical model.% We conclude that grammatical data is much less suited for such classification tasks once worldwide linguistic diversity is taken into account. This conclusion reflects the need for looking beyond WEIRD languages in all of computational, cognitive, and historical linguistics \cite{Blasi2022, Blum2024c, Cwiek2024}.

% Combining lexical and grammatical data
% oes not increase the success of classification. Possibly, this is related to our rich lexical features, which include 100 instead of 40 lexical items. Previous studies used a reduced Swadesh list of 40 items and achieved a micro-average of around 80\% \cite{Brown2008, Holman2008}. By combining their data with typological data from WALS \citep{wals}, the average micro-accuracy score is pushed up to just below 90\%. However, they make no mention of how isolates are treated, and the lack of detailed results and lack of shared code makes it difficult to establish more a detailed comparison to the ASJP classification.

% Classifying historical material
A particular use case of our method is the affiliation of historical data with contemporary language families. In many cases, the material is so scarce that cannot be affiliated with any language family based on a traditional analysis alone. Our models provide a quantitative perspective, and the case of Carar√≠ shows that it might even be possible to provide strong arguments for a specific affiliation.

% Warning against using this as evidence - rather, generating hypotheses
We do not see automated language affiliation replacing the traditional comparative method. While our model of language affiliation can be used to evaluate hypotheses about long-range genealogical relationships between languages, it cannot provide conclusive proof in favor or against such relationships. The strength of our approach is a principled comparison of the data across a large range of languages that can find hints at a shared descent between languages. This can be a starting point for a linguistic evaluation of such hypotheses, which would be verified, for example, through the means of cognate reflex prediction \cite{Blum2024d} or other traditional workflows \cite{Durie1996}. The task of evaluating those relations will have to remain with the comparative method, which could now target specific proposals to shed further light on the history of human languages.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Data and Code Availability}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%All data used are available from GitHub and Zenodo. The repository for this study includes a {\ttfamily README.md} with instructions for accessing the data. The code for this study is curated and will be released through GitHub. For the review, the code and data are uploaded to OSF (\href{https://osf.io/wqt2j/?view_only=016a4e833dec445ebc4341fdf0e23f37}{here}) in order to ensure anonimity.
All data and code needed to replicate this study along with detailed instructions can be accessed from the Open Science Framework via the following link: \url{https://osf.io/wqt2j/?view_only=016a4e833dec445ebc4341fdf0e23f37}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Competing Interests}
% To include, in this order: \textbf{Accession codes} (where applicable); \textbf{Competing interests} (mandatory statement). 
The authors declare no competing interests.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Limitations}
% Needs to be part of every ACL submission
% Does not count towards page limit
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Data for many small language families is scarce. Even though we use datasets with data from more than 2,000 languages, they only represent about a third of the world's language families. All models showed that classification is much more difficult for small language families. Considering the availability of training data in those settings, this is expected. While projects like ASJP \cite{ASJPv20} opt for smaller concept lists from more languages due to this reason, we are convinced that there are several advantages of using the explicit orthography conversion from Lexibank, even though this means a trade-off in terms of languages available.

This also influences the comparability of our models. Only a subset of around 1,000 languages is available in all three major datasets. We only use this set of common languages to compare all models using the same data. Ideally, we would have a larger subset with a better representation of the worldwide linguistic diversity.

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Ethical Considerations}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We see no potential risks involved, except for the potential of creating unwarranted hypotheses about long-distance genealogical relationships between languages. We call for all users of our models to sensibly analyze the linguistic data behind the proposed classifications.

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Acknowledgements}
% % Acknowledgements should be brief, and should not include thanks to anonymous referees and editors, or effusive comments. Grant or contribution numbers may be acknowledged.
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% This research was supported by the Max Planck Society Research Grant CALC3 (JML, FB, https://digling.org/calc/), the ERC Consolidator Grant ProduSemy (JML, Grant No. 101044282, see: https://doi.org/10.3030/101044282). All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher. The funders have/had no role in study design, data collection and analysis, decision to publish or preparation of the manuscript.

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Author contributions statement}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% J.-M.L. had the original idea and prepared a manual implementation of a neural model.
% F.B. expanded the first implementation and led the application of the model to individual cases.
% S.H. and R.F. assisted with the technical implementation of the models.
% F.B. analyzed and described the results.
% F.B. wrote the initial draft.
% All authors reviewed the manuscript. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{acl_latex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%