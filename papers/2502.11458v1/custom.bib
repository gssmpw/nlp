% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").
@article{wortsman2023stable,
  title={Stable and low-precision training for large-scale vision-language models},
  author={Wortsman, Mitchell and Dettmers, Tim and Zettlemoyer, Luke and Morcos, Ari and Farhadi, Ali and Schmidt, Ludwig},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={10271--10298},
  year={2023}
}

@article{xi2023training,
  title={Training transformers with 4-bit integers},
  author={Xi, Haocheng and Li, Changhao and Chen, Jianfei and Zhu, Jun},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={49146--49168},
  year={2023}
}

@inproceedings{chmiel2023accurate,
  title={Accurate neural training with 4-bit matrix multiplications at standard formats},
  author={Chmiel, Brian and Banner, Ron and Hoffer, Elad and Ben-Yaacov, Hilla and Soudry, Daniel},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2023}
}

@article{fu2021cpt,
  title={Cpt: Efficient deep neural network training via cyclic precision},
  author={Fu, Yonggan and Guo, Han and Li, Meng and Yang, Xin and Ding, Yining and Chandra, Vikas and Lin, Yingyan},
  journal={arXiv preprint arXiv:2101.09868},
  year={2021}
}

@article{wang2025optimizing,
  title={Optimizing Large Language Model Training Using FP4 Quantization},
  author={Wang, Ruizhe and Gong, Yeyun and Liu, Xiao and Zhao, Guoshuai and Yang, Ziyue and Guo, Baining and Zha, Zhengjun and Cheng, Peng},
  journal={arXiv preprint arXiv:2501.17116},
  year={2025}
}

@inproceedings{wang2024fp4,
  title={Fp4-quantization: Lossless 4bit quantization for large language models},
  author={Wang, Jie and Liu, Huanxi and Feng, Dawei and Ding, Jie and Ding, Bo},
  booktitle={2024 IEEE International Conference on Joint Cloud Computing (JCC)},
  pages={61--67},
  year={2024},
  organization={IEEE}
}

@article{liu2023ultra,
  title={Ultra-low Precision Multiplication-free Training for Deep Neural Networks},
  author={Liu, Chang and Zhang, Rui and Zhang, Xishan and Hao, Yifan and Du, Zidong and Hu, Xing and Li, Ling and Guo, Qi},
  journal={arXiv preprint arXiv:2302.14458},
  year={2023}
}

@article{sun2019hybrid,
  title={Hybrid 8-bit floating point (HFP8) training and inference for deep neural networks},
  author={Sun, Xiao and Choi, Jungwook and Chen, Chia-Yu and Wang, Naigang and Venkataramani, Swagath and Srinivasan, Vijayalakshmi Viji and Cui, Xiaodong and Zhang, Wei and Gopalakrishnan, Kailash},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{wang2018training,
  title={Training deep neural networks with 8-bit floating point numbers},
  author={Wang, Naigang and Choi, Jungwook and Brand, Daniel and Chen, Chia-Yu and Gopalakrishnan, Kailash},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{liu2023llm,
  title={Llm-fp4: 4-bit floating-point quantized transformers},
  author={Liu, Shih-yang and Liu, Zechun and Huang, Xijie and Dong, Pingcheng and Cheng, Kwang-Ting},
  journal={arXiv preprint arXiv:2310.16836},
  year={2023}
}

@inproceedings{wang2024fp4,
  title={Fp4-quantization: Lossless 4bit quantization for large language models},
  author={Wang, Jie and Liu, Huanxi and Feng, Dawei and Ding, Jie and Ding, Bo},
  booktitle={2024 IEEE International Conference on Joint Cloud Computing (JCC)},
  pages={61--67},
  year={2024},
  organization={IEEE}
}

@article{micikevicius2022fp8,
  title={Fp8 formats for deep learning},
  author={Micikevicius, Paulius and Stosic, Dusan and Burgess, Neil and Cornea, Marius and Dubey, Pradeep and Grisenthwaite, Richard and Ha, Sangwon and Heinecke, Alexander and Judd, Patrick and Kamalu, John and others},
  journal={arXiv preprint arXiv:2209.05433},
  year={2022}
}

@article{peng2023fp8,
  title={Fp8-lm: Training fp8 large language models},
  author={Peng, Houwen and Wu, Kan and Wei, Yixuan and Zhao, Guoshuai and Yang, Yuxiang and Liu, Ze and Xiong, Yifan and Yang, Ziyue and Ni, Bolin and Hu, Jingcheng and others},
  journal={arXiv preprint arXiv:2310.18313},
  year={2023}
}

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}

@article{1,
  title={Ultra-low precision 4-bit training of deep neural networks},
  author={Sun, Xiao and Wang, Naigang and Chen, Chia-Yu and Ni, Jiamin and Agrawal, Ankur and Cui, Xiaodong and Venkataramani, Swagath and El Maghraoui, Kaoutar and Srinivasan, Vijayalakshmi Viji and Gopalakrishnan, Kailash},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1796--1807},
  year={2020}
}

@inproceedings{2,
  title={Accurate neural training with 4-bit matrix multiplications at standard formats},
  author={Chmiel, Brian and Banner, Ron and Hoffer, Elad and Ben-Yaacov, Hilla and Soudry, Daniel},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2023}
}

@article{3,
  title={Mixed precision training},
  author={Micikevicius, Paulius and Narang, Sharan and Alben, Jonah and Diamos, Gregory and Elsen, Erich and Garcia, David and Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii and Venkatesh, Ganesh and others},
  journal={arXiv preprint arXiv:1710.03740},
  year={2017}
}

@article{4,
  title={Hybrid 8-bit floating point (HFP8) training and inference for deep neural networks},
  author={Sun, Xiao and Choi, Jungwook and Chen, Chia-Yu and Wang, Naigang and Venkataramani, Swagath and Srinivasan, Vijayalakshmi Viji and Cui, Xiaodong and Zhang, Wei and Gopalakrishnan, Kailash},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{5,
  title={Training deep neural networks with 8-bit floating point numbers},
  author={Wang, Naigang and Choi, Jungwook and Brand, Daniel and Chen, Chia-Yu and Gopalakrishnan, Kailash},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{6,
  title={Training and inference with integers in deep neural networks},
  author={Wu, Shuang and Li, Guoqi and Chen, Feng and Shi, Luping},
  journal={arXiv preprint arXiv:1802.04680},
  year={2018}
}

@article{7,
  title={Training high-performance and large-scale deep neural networks with full 8-bit integers},
  author={Yang, Yukuan and Deng, Lei and Wu, Shuang and Yan, Tianyi and Xie, Yuan and Li, Guoqi},
  journal={Neural Networks},
  volume={125},
  pages={70--82},
  year={2020},
  publisher={Elsevier}
}

@inproceedings{8,
  title={Fixed-point back-propagation training},
  author={Zhang, Xishan and Liu, Shaoli and Zhang, Rui and Liu, Chang and Huang, Di and Zhou, Shiyi and Guo, Jiaming and Guo, Qi and Du, Zidong and Zhi, Tian and others},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={2330--2338},
  year={2020}
}

@article{9,
  title={Training transformers with 4-bit integers},
  author={Xi, Haocheng and Li, Changhao and Chen, Jianfei and Zhu, Jun},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={49146--49168},
  year={2023}
}

@article{10,
  title={Cpt: Efficient deep neural network training via cyclic precision},
  author={Fu, Yonggan and Guo, Han and Li, Meng and Yang, Xin and Ding, Yining and Chandra, Vikas and Lin, Yingyan},
  journal={arXiv preprint arXiv:2101.09868},
  year={2021}
}
@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and RÃ©mi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
    pages = "38--45"
}
@misc{eval-harness,
  author       = {Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and Le Noac'h, Alain and Li, Haonan and McDonell, Kyle and Muennighoff, Niklas and Ociepa, Chris and Phang, Jason and Reynolds, Laria and Schoelkopf, Hailey and Skowron, Aviya and Sutawika, Lintang and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  month        = 07,
  year         = 2024,
  publisher    = {Zenodo},
  version      = {v0.4.3},
  doi          = {10.5281/zenodo.12608602},
  url          = {https://zenodo.org/records/12608602}
}

@article{11,
  title={Scaling laws for precision},
  author={Kumar, Tanishq and Ankner, Zachary and Spector, Benjamin F and Bordelon, Blake and Muennighoff, Niklas and Paul, Mansheej and Pehlevan, Cengiz and R{\'e}, Christopher and Raghunathan, Aditi},
  journal={arXiv preprint arXiv:2411.04330},
  year={2024}
}

@article{12,
  title={Llm-fp4: 4-bit floating-point quantized transformers},
  author={Liu, Shih-yang and Liu, Zechun and Huang, Xijie and Dong, Pingcheng and Cheng, Kwang-Ting},
  journal={arXiv preprint arXiv:2310.16836},
  year={2023}
}

@article{fishman2024scaling,
  title={Scaling FP8 training to trillion-token LLMs},
  author={Fishman, Maxim and Chmiel, Brian and Banner, Ron and Soudry, Daniel},
  journal={arXiv preprint arXiv:2409.12517},
  year={2024}
}

@article{weber2025redpajama,
  title={Redpajama: an open dataset for training large language models},
  author={Weber, Maurice and Fu, Dan and Anthony, Quentin and Oren, Yonatan and Adams, Shane and Alexandrov, Anton and Lyu, Xiaozhong and Nguyen, Huu and Yao, Xiaozhe and Adams, Virginia and others},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={116462--116492},
  year={2025}
}

@article{wang2018glue,
  title={Glue: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex},
  journal={arXiv preprint arXiv:1804.07461},
  year={2018}
}

@article{merity2016pointer,
  title={Pointer sentinel mixture models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  journal={arXiv preprint arXiv:1609.07843},
  year={2016}
}

@inproceedings{xijetfire,
  title={Jetfire: Efficient and Accurate Transformer Pretraining with INT8 Data Flow and Per-Block Quantization},
  author={Xi, Haocheng and Chen, Yuxiang and Zhao, Kang and TEH, KAI JUN and Chen, Jianfei and Zhu, Jun},
  booktitle={Forty-first International Conference on Machine Learning}
}

@article{13,
  title={Fp8 formats for deep learning},
  author={Micikevicius, Paulius and Stosic, Dusan and Burgess, Neil and Cornea, Marius and Dubey, Pradeep and Grisenthwaite, Richard and Ha, Sangwon and Heinecke, Alexander and Judd, Patrick and Kamalu, John and others},
  journal={arXiv preprint arXiv:2209.05433},
  year={2022}
}

@inproceedings{14,
  title={SPDF: Sparse pre-training and dense fine-tuning for large language models},
  author={Thangarasa, Vithursan and Gupta, Abhay and Marshall, William and Li, Tianda and Leong, Kevin and DeCoste, Dennis and Lie, Sean and Saxena, Shreyas},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={2134--2146},
  year={2023},
  organization={PMLR}
}

@article{15,
  title={Accelerating Transformer Pre-Training with 2: 4 Sparsity},
  author={Hu, Yuezhou and Zhao, Kang and Huang, Weiyu and Chen, Jianfei and Zhu, Jun},
  journal={arXiv preprint arXiv:2404.01847},
  year={2024}
}

@article{16,
  title={S-STE: Continuous Pruning Function for Efficient 2: 4 Sparse Pre-training},
  author={Hu, Yuezhou and Zhu, Jun and Chen, Jianfei},
  journal={arXiv preprint arXiv:2409.09099},
  year={2024}
}

@article{17,
  title={Estimating or propagating gradients through stochastic neurons for conditional computation},
  author={Bengio, Yoshua and L{\'e}onard, Nicholas and Courville, Aaron},
  journal={arXiv preprint arXiv:1308.3432},
  year={2013}
}

@inproceedings{18,
  title={Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters},
  author={Rasley, Jeff and Rajbhandari, Samyam and Ruwase, Olatunji and He, Yuxiong},
  booktitle={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={3505--3506},
  year={2020}
}

@article{19,
  title={Megatron-lm: Training multi-billion parameter language models using model parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}

@article{20,
  title={Flashattention: Fast and memory-efficient exact attention with io-awareness},
  author={Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={16344--16359},
  year={2022}
}

@article{21,
  title={Flashattention-2: Faster attention with better parallelism and work partitioning},
  author={Dao, Tri},
  journal={arXiv preprint arXiv:2307.08691},
  year={2023}
}

@article{22,
  title={Massive activations in large language models},
  author={Sun, Mingjie and Chen, Xinlei and Kolter, J Zico and Liu, Zhuang},
  journal={arXiv preprint arXiv:2402.17762},
  year={2024}
}

@article{23,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

@article{24,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec},
  year={2018}
}

@article{25,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{26,
  title={Deepseek-v3 technical report},
  author={Liu, Aixin and Feng, Bei and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and others},
  journal={arXiv preprint arXiv:2412.19437},
  year={2024}
}

@article{27,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{28,
  title={Training compute-optimal large language models. arXiv},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and de Las Casas, Diego and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022},
  publisher={Retrieved 2023-01-02, from http://arxiv. org/abs/2203.15556}
}

@article{29,
  title={Efficient training of large language models on distributed infrastructures: A survey},
  author={Duan, Jiangfei and Zhang, Shuo and Wang, Zerui and Jiang, Lijuan and Qu, Wenwen and Hu, Qinghao and Wang, Guoteng and Weng, Qizhen and Yan, Hang and Zhang, Xingcheng and others},
  journal={arXiv preprint arXiv:2407.20018},
  year={2024}
}

@misc{30,
  author = {Nvidia},
  title = {NVIDIA Blackwell Architecture Technical Brief},
  howpublished = {\url{https://resources.nvidia.com/en-us-blackwell-architecture}},
}
@misc{31,
  author       = {NVIDIA},
  title        = {Transformer Engine},
  howpublished = {\url{https://github.com/NVIDIA/TransformerEngine}},
}

@article{32,
  title={GPT-3: Its nature, scope, limits, and consequences},
  author={Floridi, Luciano and Chiriatti, Massimo},
  journal={Minds and Machines},
  volume={30},
  pages={681--694},
  year={2020},
  publisher={Springer}
}

@article{33,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{34,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={240},
  pages={1--113},
  year={2023}
}

@article{35,
  title={Opt: Open pre-trained transformer language models, 2022},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={URL https://arxiv. org/abs/2205.01068},
  volume={3},
  pages={19--0},
  year={2023}
}

@article{36,
  title={Learning to summarize with human feedback},
  author={Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={3008--3021},
  year={2020}
}

@article{37,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Alexey, Dosovitskiy},
  journal={arXiv preprint arXiv: 2010.11929},
  year={2020}
}

@article{38,
  title={Unified language model pre-training for natural language understanding and generation},
  author={Dong, Li and Yang, Nan and Wang, Wenhui and Wei, Furu and Liu, Xiaodong and Wang, Yu and Gao, Jianfeng and Zhou, Ming and Hon, Hsiao-Wuen},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{39,
  title={Pact: Parameterized clipping activation for quantized neural networks. arXiv 2018},
  author={Choi, J and Wang, Z and Venkataramani, S and Chuang, PI-Jen and Srinivasan, V and Gopalakrishnan, K},
  journal={arXiv preprint arXiv:1805.06085},
  year={2018}
}

@article{40,
  title={Layer normalization},
  author={Lei Ba, Jimmy and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={ArXiv e-prints},
  pages={arXiv--1607},
  year={2016}
}

@misc{41,
  author       = {NVIDIA},
  title        = {APEX},
  howpublished = {\url{https://github.com/NVIDIA/apex}},
}

@article{42,
  author    = {Wang, R and Gong, Y and Liu, X and others},
  title     = {Optimizing Large Language Model Training Using FP4 Quantization},
  journal   = {arXiv preprint},
  volume    = {arXiv:2501.17116},
  year      = {2025}
}
