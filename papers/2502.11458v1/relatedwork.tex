\section{Related Work}
Low-precision training enhances deep learning efficiency by reducing computational costs. Many existing studies focus on the training of deep neural networks (DNNs) \cite{wang2018training,chmiel2023accurate, sun2019hybrid,xi2023training,fu2021cpt}, whose architecture and performance differ from LLM pre-training. %However, as the training of deep neural networks (DNNs) differs significantly from the pre-training of large language models, their performance in low-precision training varies accordingly.
In the context of low-precision training for large model pre-training, some progress has been made in FP8. For example, \cite{micikevicius2022fp8} introduced new FP8 floating-point formats (E4M3 and E5M2), % deep learning training and inference, 
and \cite{fishman2024scaling} extends FP8 to trillion-token large-scale model pretraining. In terms of FP4 training, \cite{wang2025optimizing} improved FP4 computational precision using a differentiable quantization estimator and outlier clamping and compensation strategy. However, most existing methods fail to fully account for the varying sensitivity to precision across different model modules.

% \vspace{-0.2cm}