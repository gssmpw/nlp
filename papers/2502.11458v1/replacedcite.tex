\section{Related Work}
Low-precision training enhances deep learning efficiency by reducing computational costs. Many existing studies focus on the training of deep neural networks (DNNs) ____, whose architecture and performance differ from LLM pre-training. %However, as the training of deep neural networks (DNNs) differs significantly from the pre-training of large language models, their performance in low-precision training varies accordingly.
In the context of low-precision training for large model pre-training, some progress has been made in FP8. For example, ____ introduced new FP8 floating-point formats (E4M3 and E5M2), % deep learning training and inference, 
and ____ extends FP8 to trillion-token large-scale model pretraining. In terms of FP4 training, ____ improved FP4 computational precision using a differentiable quantization estimator and outlier clamping and compensation strategy. However, most existing methods fail to fully account for the varying sensitivity to precision across different model modules.

% \vspace{-0.2cm}