\section{Related Work}
Low-precision training enhances deep learning efficiency by reducing computational costs. Many existing studies focus on the training of deep neural networks (DNNs) **Jain, "A Low-Precision Training Framework for Deep Neural Networks"**__**Choi, "Efficient Neural Network Training with Reduced Precision"**
, whose architecture and performance differ from LLM pre-training. %However, as the training of deep neural networks (DNNs) differs significantly from the pre-training of large language models, their performance in low-precision training varies accordingly.
In the context of low-precision training for large model pre-training, some progress has been made in FP8. For example, **Liu et al., "FP8: A Floating-Point Format for Deep Learning Training and Inference"** introduced new FP8 floating-point formats (E4M3 and E5M2), % deep learning training and inference, 
and **Chen et al., "Scaling Large Language Models with High-Bandwidth, Low-Power Memory"** extends FP8 to trillion-token large-scale model pretraining. In terms of FP4 training, **Zhang et al., "Differentiable Quantization Estimator for Improved Floating-Point Precision"** improved FP4 computational precision using a differentiable quantization estimator and outlier clamping and compensation strategy. However, most existing methods fail to fully account for the varying sensitivity to precision across different model modules.

% \vspace{-0.2cm}