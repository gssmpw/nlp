\section{Related Works}
\label{sec:preliminaries}

\paragraph{Machine Learning in Financial Forecasting}
Early approaches to predicting stock market movements relied heavily on classical statistical models. One foundational method, the Autoregressive Integrated Moving Average (ARIMA) \cite{box1970time}, utilized time series data to forecast trends. Subsequent models, such as \textit{Generalized Autoregressive Conditional Heteroskedasticity} (GARCH)~\cite{bollerslev1986generalized}, \textit{Vector Autoregression} (VAR)~\cite{sims1980macroeconomics}, and \textit{Holt-Winters exponential smoothing}~\cite{holt1957forecasting}, extended these capabilities by capturing more intricate patterns in financial time series. Other notable contributions include techniques for cointegration analysis \cite{engle1987cointegration}, Kalman filtering \cite{kalman1960new}, and Hamiltonâ€™s regime-switching models \cite{hamilton1989new}.

While effective, these classical models were primarily limited to tabular datasets and struggled with nonlinear relationships and multimodal inputs. The rise of Large Language Models (LLMs) transformed financial forecasting by enabling the incorporation of richer, more complex data sources. For example, integrating financial news articles \cite{yang2020finbert}, sentiment analysis \cite{yang2020finbert}, social media activity \cite{bollen2011twitter}, and earnings call transcripts \cite{tsai2016forecasting} significantly enhanced market movement predictions, demonstrating the versatility and power of LLMs in handling diverse financial modalities.

\paragraph{Contrastive Learning} 
Contrastive learning has emerged as a powerful paradigm in unsupervised and self-supervised learning, focusing on representation learning through comparisons. The core idea is to bring similar data points closer in the representation space while distancing dissimilar ones. A key milestone in this field was SimCLR \cite{chen2020simple}, which used data augmentations and contrastive loss to learn high-quality representations without requiring labels. MoCo~\cite{he2020momentum} further advanced this approach by introducing a memory bank to efficiently manage negative examples, making it more scalable for larger datasets.

Recent innovations like SimSiam \cite{chen2021exploring} have shown that competitive representations can be learned without relying on negative pairs, streamlining computation and improving accessibility. These advancements are particularly relevant for financial applications, where large-scale and heterogeneous datasets are common, enabling contrastive learning to uncover nuanced relationships in financial data.
% More recent advancements such as SimSiam \cite{chen2021exploring} have shown that competitive representations can be learned without negative pairs, further improving efficiency and reducing computational requirements, making it more accessible for large-scale datasets commonly found in financial applications.


%----------------------------------------%
% FLATTENED CONTENT FROM: {ICLR-25/sections/03_IG-CluPE.tex}
%----------------------------------------%