\section{Related Work}
In this section, we introduce the background of the LLM-based agent and existed studies on slides generations.
\subsection{LLM-based Agent}
LLMs have demonstrated impressive capabilities for complicated, interactive tasks \cite{yao2023react, yao2022webshop, xi2024agentgym, yang2024swe, ma2024coco}. LLM-based autonomous agents have achieved remarkable progress in a wide range of domains, including logic reasoning \cite{qi2024mutual, khattab2022demonstrate}, tool use \cite{qin2024toolllm, zhang2023igniting}, and social activities \cite{Park2023GenerativeAgents}. 
The current paradigm of agents relies on the language intelligence of LLMs.
The mainstream work pattern encompasses environment perceiving, planning, reasoning, and executing, forming a workflow to dive and conquer intricate challenges.

Empowered by the recent progress of multi-modal pre-training, those agents can understand image, video, and audio channels \cite{wu2023nextgpt, liu2023llava}.
% MM reasoning 
(i) Visual knowledge can largely facilitate reasoning and is integrated into Chain-of-Thoughts  \cite{zhang2023multicot, xu2024llavacot}. 
(ii) Multi-modal reasoning enables divergent thinking cross modalities and takes advantage of those different modalities. 
Sketchpad \cite{hu2024visual} allows LLMs to draw drafts to assist its planning and reasoning, i.e., to draw auxiliary lines for geometry problems.
Visualization-of-Thought \cite{wu2024minds} generates visual rationales for spatial reasoning tasks like mazes.
For each stage of complex multi-modal tasks, selecting an appropriate modality as the main modality for reasoning can leverage the natural characteristics of the modality and stimulate the potential of LLMs \cite{park2025generalizingsimplehardvisual}.

\subsection{Slide Generation} 
Previous studies have explored extractive methods and simplified this task as sentence selection, e.g., to calculate the importance score and extract top sentences \cite{Wang2017PhraseBasedPS}. With the development of small language models \cite{lewis-etal-2020-bart, 2020t5}, slide generation is unified as abstractive, query-based document summarization \cite{sun-etal-2021-d2s}. 

Despite their early success, the emergence of LLMs exhibits exceptional performance and stimulates the demands of intelligent slide generation.
Slide generation poses intricate challenges for autonomous agents, as it requires document reading comprehension and precise tool use to generate layouts.
Pioneer work focuses on modifying target elements, asking agents to execute a series of specific instructions \cite{guo-etal-2024-pptc}. The agent needs to understand the status of the slide, navigate to the element, and generate precise API calls.
Recent studies first plan the outlines and then generate each page.
To further control the style of presentations, \citet{mondal-etal-2024-presentations} introduce a reward model trained on human feedback to guide both topic generation and content extraction.
Considering the visual quality of slides, \citet{bandyopadhyay-etal-2024-enhancing-presentation} employ a visual LM to insert images. 
DOC2PPT \cite{Fu2021DOC2PPTAP} integrates an object placer to predict the position and size of each element by training small models.
PPTAgent \cite{zheng2025pptagent} directly utilizes slide templates to fix the layout and then fill textboxes, ensuring visual harmony and aesthetic appeal.