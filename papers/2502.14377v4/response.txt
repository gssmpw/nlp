\section{Related works}
\subsection{Diffusion-based models}
In recent years, diffusion-based methods have garnered significant success in the field of generation **Ho et al., "Diffusion Models"**__, particularly in text-to-image (T2I) generation **Sohl-Dickstein et al., "Deep Unsupervised Learning using Nonequilibrium Thermodynamics"**__. These methods utilize text embeddings derived from pre-trained language encoders such as CLIP **Radford et al., "Learning Transferable Visual Models"**, Bert  **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** and T5  **Raffel et al., "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"** to generate images with high fidelity and diversity through an iterative denoising process. Recently, the introduction of the latent diffusion model**Sohl-Dickstein et al., "Deep Unsupervised Learning using Nonequilibrium Thermodynamics"** has marked a significant advancement in this field, enhancing the quality and efficiency of generated content. In pursuit of greater scalability and enhanced generation quality, models like DiT **An et al., "Diffusion Transformers"** integrate large-scale Transformer architectures into the diffusion framework, pushing the boundaries of generative performance. Building on this foundation, Flux  **Soyer et al., "Flux: Improving Generative Models with Variational Inference"** synthesizes flow-matching  and Transformer-based architecture to achieve state-of-the-art performance.

\subsection{Controllable generation with diffusion models}
Controllable generation has emerged as a prominent area of research in diffusion models  **Ho et al., "Diffusion Models"**. Currently, there are two main approaches to incorporating controllable conditions into image generation: (1) training a large diffusion model from scratch to enable control under multiple conditions, and (2) fine-tuning a lightweight structure while keeping the original pre-trained model frozen. However, the first approach demands significant computational resources, which limits its accessibility for broader dissemination and personal use. In contrast, recent studies have explored the addition of supplementary network structures to pre-trained diffusion models, allowing for control over the generated outputs without the need to retrain the entire model.

ControlNet  **Alayrac et al., "Declarative Vision and Language Research"** enables image generation that aligns with control information by reproducing specific layers within the network and connecting them to the original layers using zero convolution. Building on this foundation, Uni-Control  **Chen et al., "Uni-Control: Unified Multi-Condition Image Generation via Mixture-of-Experts"** introduces a Mixture-of-Experts (MoE) framework, which unifies control across multiple spatial conditions. ControlNet-XS  **Hao et al., "ControlNet-XS: A Lightweight and Efficient Architecture for Controllable Image Generation"** further improves the interaction bandwidth and frequency between the control and main branches within the ControlNet architecture, drawing inspiration from principles of feedback control systems. Nonetheless, these approaches are primarily based on the U-Net structure and may not yield the desired results when directly applied to Diffusion Transformers (DiT) without modification  **An et al., "Diffusion Transformers"**. PixArt-$\delta$  **Chen et al., "PixArt: A Design Methodology for Efficient Controllable Image Generation with DiT"** proposed a design methodology specifically tailored for DiT, but directly copying the first half of the network results in a 50\% increase in both parameter count and computational complexity, resulting in high computational cost and inconvenient for community research and practical deployment.