%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025_arxiv}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage{multirow}
\usepackage{arydshln}
\usepackage{colortbl}
\definecolor{Gray}{gray}{0.95}
\newcommand{\gb}[1]{{\cellcolor{gray!18}{#1}}}
\usepackage{booktabs}
% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{RelaCtrl: Relevance-Guided Efficient Control for Diffusion Transformers}

\begin{document}

\twocolumn[
\icmltitle{RelaCtrl: Relevance-Guided Efficient Control for Diffusion Transformers
}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Ke Cao}{ustc,cas,company,equal}
\icmlauthor{Jing Wang}{company,sysu,equal}
\icmlauthor{Ao Ma}{company,equal}
\icmlauthor{Jiasong Feng}{company}
\icmlauthor{Zhanjie Zhang}{company,zhejiang}




\icmlauthor{Xuanhua He}{ustc,cas}
\icmlauthor{Shanyuan Liu}{company}
\icmlauthor{Bo Cheng}{company}
\icmlauthor{Dawei Leng}{company}
\icmlauthor{Yuhui Yin}{company}
\icmlauthor{Jie Zhang}{ustc,cas}
\end{icmlauthorlist}

\icmlaffiliation{ustc}{University of Science and Technology of China}
\icmlaffiliation{cas}{Hefei Institutes of Physical Science, Chinese Academy of
Sciences}
\icmlaffiliation{company}{360 AI Research}
\icmlaffiliation{sysu}{Sun Yat-sen University}
\icmlaffiliation{zhejiang}{Zhejiang University}

\icmlcorrespondingauthor{Dawei Leng}{lengdawei@360.cn}
\icmlcorrespondingauthor{Jie Zhang}{zhangjie@iim.ac.cn}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
The Diffusion Transformer plays a pivotal role in advancing text-to-image and text-to-video generation, owing primarily to its inherent scalability. However, existing controlled diffusion transformer methods incur significant parameter and computational overheads and suffer from inefficient resource allocation due to their failure to account for the varying relevance of control information across different transformer layers.
To address this, we propose the Relevance-Guided Efficient Controllable Generation framework, \textbf{RelaCtrl}, enabling efficient and resource-optimized integration of control signals into the Diffusion Transformer.
First, we evaluate the relevance of each layer in the Diffusion Transformer to the control information by assessing the ``ControlNet Relevance Score"—i.e., the impact of skipping each control layer on both the quality of generation and the control effectiveness during inference.
Based on the strength of the relevance, we then tailor the positioning, parameter scale, and modeling capacity of the control layers to reduce unnecessary parameters and redundant computations. Additionally, to further improve efficiency, we replace the self-attention and FFN in the commonly used copy block with the carefully designed Two-Dimensional Shuffle Mixer (\textbf{TDSM}), enabling efficient implementation of both the token mixer and channel mixer.
Both qualitative and quantitative experimental results demonstrate that our approach achieves superior performance with only 15\% of the parameters and computational complexity compared to PixArt-$\delta$. Our project homepage is https://360cvgroup.github.io/RelaCtrl/.
\end{abstract}

\section{Introduction}
The Diffusion Transformer (DiT)~\cite{peebles2023dit}, with its strong scalability and multi-modal alignment capabilities, has significantly advanced the fields of text-to-image and text-to-video generation (like, PixArt-$\alpha$~\cite{chen2023PixArta}, Flux~\cite{blackforestlabs2024flux}, Stable Diffusion 3~\cite{stabilityai2024sd3}, CogVideoX~\cite{yang2024cogvideox}, Sora~\cite{videoworldsimulators2024}, HunyuanVideo~\cite{li2024hunyuan}, and Qihoo-T2X~\cite{wang2024qihoo}, etc). By leveraging its robust architecture and scalability, the fidelity of the generated results and consistency with the given textual description are dramatically improved. Recent studies, such as PixArt-$\delta$~\cite{chen2024PixArtc} and OminiControl~\cite{tan2024ominicontrol}, focus on controlled text-to-image generation based on the DiT framework, promoting its application in real-world scenarios such as AI-driven content creation and e-commerce shopping.

\begin{figure}[t]
% \captionsetup{type=figure}
\centering
\includegraphics[width=\linewidth]{images/intro_fidandhdd.jpg}
\vspace{-2.5em}
\caption{\label{fig:intro_fidandhdd}Effect of skipping a specific position within the ControlNet block on the quality of the generated image. Higher FID and HDD indicate a more significant impact of the skipped layer on the quality of the final results, reflecting a stronger correlation with the generated image quality.
}
\vspace{-1.5em}
\end{figure}

However, current controlled generation methods for DiT face two main shortcomings.
\textbf{Firstly}, a significant number of additional parameters and computations are introduced, increasing the burden on training and inference. For example, PixArt-$\delta$ directly duplicates the first half of the network's blocks (i.e., 13 blocks), resulting in a 50\% increase in both the number of parameters and computational complexity. Similarly, the control token concatenation in OminiControl adds only a limited number of parameters but doubles the number of tokens involved in the attention and linear layers, leading to a nearly 70\% increase in overall computational complexity.
\textbf{Secondly}, the varying relevance of control information across different layers of the network is often overlooked, resulting in inefficient allocation of computational resources. Our experiments on ``ControlNet Relevance Score", in which we trained a controlled generative model by copying all blocks and removing control blocks from different layers during inference, revealed that different layers in DiT exhibit varying levels of relevance for control information. As shown in Fig. \ref{fig:intro_fidandhdd}, this relevance follows a trend of increasing and then decreasing, with higher relevance observed in the front-center layers and lower relevance in the deeper layers of the network, resulting in only a slight performance loss (for a more detailed explanation, please refer to Sec.~\ref{paper: DiT-ControlNet Relevance Prior}). Existing methods neglect this variation and apply uniform settings to all layers that introduce control information, resulting in inefficient allocation of parameters and computational resources, such as redundant parameters or computations in layers with low relevance.

To address the above issues, we propose the Relevance-Guided Efficient Controllable Generation framework (i.e., \textbf{RelaCtrl}) for diffusion transformer, based on control information relevance analysis. 
Specifically, to achieve efficient utilization of computational resources, we design relevance-guided allocation and steering strategies. Control blocks are placed at locations with high control information relevance, while locations with weak relevance are left without control blocks. 
To further reduce the number of parameters and computational complexity introduced by the copy control block operation, we design a lightweight Two-Dimensional Shuffle Mixer (\textbf{TDSM}) to replace the self-attention and FFN layers in the copy block.
Self-attention plays the role of token mixer and  FFN plays the role of channel mixer. Therefore, to efficiently perform the same functions as the token mixer and channel mixer, TDSM first randomly selects a varying number of feature channel groups, then randomly divides the token groups within each channel group, and finally computes the attention within each token-channel group along the token dimension.
Theoretical analysis demonstrates that TDSM can overcome the limitations of local grouping and enable non-local modeling in the channel-token dimensions. In addition, we regulate the number of channel division groups in TDSM based on the correlation. In regions with stronger correlation, we reduce the number of channel groups and expand the feature dimensions involved in attention to enhance its modeling capability.
The results from multiple controlled experiments demonstrate that our approach achieves superior performance with only a 45M parameter increase (7.4\% of PixArt-$\alpha$) and an additional 46.7 GFLOPs (8.6\% of PixArt-$\alpha$).
% 17\% of PixArt-$\delta$ controlnet branch and 

% Experimental results show that our approach achieves improved performance with an increase of only 45M parameters (15\% of PixArt-$\delta$) and XXX GFLOPs (15\% of PixArt-$\delta$).
% Specifically, this framework includes relevance allocation and guidance strategy, along with a lightweight Two-Dimensional Shuffle Mixer (TDSM), to enhance the efficiency of computational resource allocation and control signaling.

The main contribution of this paper can be summarized as follows:
\begin{itemize}
\setlength{\itemsep}{0.5em}    % 控制item之间的间距
\setlength{\parskip}{0.3em}    % 控制段落之间的间距
\item We investigate in detail the relevance of control information across different layers of the network, finding that the shallower layers are more sensitive to the control signal, while the deeper layers exhibit weaker relevance to the control effect.
\item Based on the relevance analysis, we propose a relevance-guided controlled generation strategy (RelaCtrl), which efficiently allocates the embedding positions of control blocks and the strength of the TDSM modeling capability. This approach minimizes the number of parameters introduced by the control branch and reduces computational complexity without compromising performance.
\item We propose a Two-Dimensional Shuffle Mixer (TDSM), which efficiently replaces the self-attention and FFN in the original copy block by calculating attention within randomly divided channel and token groups. Theoretical analysis demonstrates that this design overcomes the limitations of local group modeling, ensuring efficient token-mixing and channel-mixing.
\item The experimental results across four different conditional guidance tasks and two text-to-image generation models show that RelaCtrl consistently achieves superior performance while maintaining efficiency, validating the generalization of both the relevance-guided strategy and the proposed TDSM.
\end{itemize}
\section{Methods}
\subsection{DiT-ControlNet Relevance Prior}
In prior studies, the majority of studies on ControlNet have centered on the U-Net architecture. However, the DiT framework~\cite{chen2023PixArta}, constructed by stacking a sequence of transformer blocks without an explicit encoder-decoder structure, poses significant challenges for direct adaptation to achieve effective controllability~\cite{chen2024PixArtc}. To tackle this issue,  PixArt-${\delta}$ introduced a method that duplicates the first 13 Transformer blocks from the DiT model, integrates the outputs of these copied blocks with their corresponding frozen blocks, and forwards the combined results to subsequent frozen modules for further processing. While this approach has demonstrated qualitatively favorable results, it brings notable drawbacks. Simply duplicating the first half of the frozen modules results in a considerable increase in model parameters and computational overhead, leading to prohibitively high training and inference costs, particularly for high-resolution image generation. Moreover, our observations indicate that different replicated layers in DiT-ControlNet contribute unequally to the overall generation quality and control fidelity. Blindly copying the initial Transformer blocks can introduce unnecessary computational redundancy without proportional performance gains.
\label{paper: DiT-ControlNet Relevance Prior}
\begin{figure}[t]
% \captionsetup{type=figure}
\centering
\includegraphics[width=\linewidth]{images/method_importance.png}
\vspace{-1.5em}
\caption{\label{img:method_importance}The relevance diagram of different layers in the DiT-ControlNet was calculated based on the FID and HDD ranks. The overall trend shows an initial increase followed by a decrease. The selected placement positions of RelaCtrl in PixArt-$\alpha$ are marked with white numbers.
}
\vspace{-1.5em}
\end{figure}
To systematically evaluate the relevance of individual layers within DiT-ControlNet to generation and controlled quality, we trained a fully controlled PixArt-${\alpha}$ network containing 27 replicated modules. During inference, we systematically skip each control block layer and assess its impact on the final generation. For quantitative assessment, we employed the Fréchet Inception Distance (FID) to measure image generation quality and the Hausdorff Distance (HDD) to evaluate control accuracy. These metrics enabled us to analyze the impact of skipping individual blocks from the control branch on overall performance, providing relevant scores for each control block. Finally, we get the ControlNet Relevance Score $CRS$ based on the combination of these two metrics:
\begin{equation}\label{equ: cis}
{{CRS}_{i}}=\frac{1}{2}\left( \frac{{{F}_{i}}-{{F}_{\min }}}{{{F}_{\max }}-{{F}_{\min }}}+\frac{{{H}_{i}}-{{H}_{\min }}}{{{H}_{\max }}-{{H}_{\min }}} \right)
\end{equation}
Where  $F$ and $H$ represent the rank of initial FID and HDD indicators respectively, $i$ shows the index of the control branch block that is removed, and $\min $ and $\max $ denote the minimum and maximum values within the corresponding rank sequence. If ${{F}_{i}}$ or ${{H}_{i}}$ is higher, it indicates that removing the control block with index $i$ significantly affects the final performance, implying that this module is critically important. Using this approach, we performed single-layer deletions across all replicated blocks in the ControlNet model and derived the regularization metrics and qualitative observations presented in Fig.~\ref{fig:intro_fidandhdd}. According to the specified formula~\ref{equ: cis}, the relevance distribution of the ControlNet blocks can be obtained. As illustrated in Fig.~\ref{img:method_importance}, the numbers below represent the indices of the control blocks, while the intensity of the shading in each rectangle reflects its relevance to the generation and control performance of the model.

Our findings can be summarized as follows. The most critical modules of DiT-ControlNet are concentrated in the early-middle layers (e.g., blocks 5, 6, and 7). In contrast, removing the last few modules results in only a minimal decline in performance. Overall, the ControlNet Relevance Score exhibits a trend of initially increasing and then decreasing, which contrasts with observations from prior studies of large language models \cite{gromov2024unreasonable, zhong2024blockpruner, men2024shortgpt} or the main branches of the original DiT architecture \cite{lee2024ditPruner}. This indicates that simply increasing or decreasing the number of replicated front transformer blocks in DiT-ControlNet does not offer an effective trade-off between performance and computational cost. Such an approach risks removing control modules essential for maintaining optimal performance. Consequently, we propose dynamically guiding the placement and design of control modules within the network by ranking each layer in the DiT model's control branch according to its relevance. This strategy ensures a more targeted and efficient utilization of network resources.
\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{images/method_arch.png}
    \vspace{-1.5em}
    \caption{\label{img:method_arch}The overall architecture of RelaCtrl. Control block locations are prioritized based on the ControlNet Relevance Score, ranked from highest to lowest. The direct duplication of the main branch in the original ControlNet is replaced with the carefully designed Reference-Guided Lightweight control block. Additionally, the Two-Dimensional Shuffle Mixer effectively reduces model parameters and computational overhead while preserving performance.}
    \vspace{-1.5em}
    \label{img:method_arch}
\end{figure*}
\subsection{Overall Architecture}
Fig.~\ref{img:method_arch} depicts the overall pipeline of our proposed method. Based on the ranking of ControlNet Relevance Score derived in Sec.~\ref{paper: DiT-ControlNet Relevance Prior} and further validated through ablation studies, we identified and selected the 11 most critical control positions—ranked by relevance from high to low—for integrating the control modules, as is shown in Fig.~\ref{img:method_importance}. With this approach, we achieve control performance comparable to PixArt-${\delta}$, which utilizes 13 copied modules, while reducing the parameter count by approximately 15\%. Although this method effectively decreases the model size and computational overhead, significant redundancy remains in the internal design of the control modules. The transformative power of the Transformer architecture, as emphasized by MetaFormer~\cite{yu2022metaformer}, lies in its holistic design, wherein the attention mechanism serves as a token mixer by enabling token-level information mixing, while the remaining components, such as the FFN layers, function as channel mixers to facilitate the integration of channel-wise information. To address the substantial redundancy in the FFN layers within the channel mixer~\cite{pires2023sharedFFN}, we introduce a lightweight module, the Relevance-Guided Lightweight Control Block (RGLC), which unifies token mixing and channel mixing into a single operation. Specifically, we replace the attention and FFN layers in the original PixArt Transformer block with a novel Two-Dimensional Shuffle Mixer (TDSM) design, streamlining the architecture for enhanced efficiency. This method facilitates information interaction and modeling across both the token and channel dimensions, significantly reducing the parameter count and computational demands of the replicated blocks.

\subsection{Relevance-Guided Lightweight Control Block}
The third part of Fig.~\ref{img:method_arch} illustrates the detailed structure of the RGLC Block. The module takes three inputs: the control condition input $c$, the diffusion timesteps embedding $t$ used to compute the weights for feature normalization, and the input $x$ from the corresponding frozen block. To enhance information interaction between the control branch and the frozen backbone network, $x$ is passed through a zero convolution layer and added to the conditional input $c$, producing ${{c}_{in}}$. The resulting ${{c}_{in}}$ is then processed by the Two-Dimensions Shuffle Mixer (TDSM). Following the processing, the output is passed through a zero convolution layer, resulting in ${{c}_{cond}}$ which is added to the main branch to provide control guidance. This process can be formally expressed as follows:
\begin{equation}\label{}
{{c}_{cond}}=\text{ZC}(\text{TDSM}({{c}_{in}})+{{c}_{in}})
\end{equation}
Where $\text{ZC}$ refers to the operation of zero convolution. To address the additional computational overhead introduced by the self-attention mechanism, TDSM employs locally grouped self-attention with shuffle characteristics. This design significantly reduces the computational complexity of the network while preserving non-local information interactions within the groups. A detailed analysis of this process is provided in Sec.~\ref{paper: Two Dimensions Shuffle Mixer}.

\subsection{Two Dimensions Shuffle Mixer}
\label{paper: Two Dimensions Shuffle Mixer}
From the perspective of MetaFormer~\cite{yu2022metaformer}, the effectiveness of Transformers can be attributed to two key components: the token mixer, implemented via the self-attention mechanism, and the channel mixer, realized through the feed-forward network (FFN) layer. However, studies have revealed that the FFN is often highly redundant despite consuming a significant proportion of the model parameters~\cite{pires2023sharedFFN}. To alleviate the computational burden of the control branch, we propose grouping tokens for computation while employing specific strategies to enhance interaction and modeling capacity across token groups ~\cite{huang2021shuffle,cao2024shufflemamba}. Departing from traditional shuffle methods that operate exclusively on the token dimension, we extend the token mixer in the Transformer architecture to model non-local interactions within local windows by introducing a novel approach that jointly operates on both the token and channel dimensions. This dual-dimension strategy enables more efficient and effective modeling. Specifically, We begin by performing random channel selection, followed by random shuffling of the input sequence across the 3D dimension space. Afterward, local self-attention calculations are applied. Although the subsequent attention mechanism is confined to a fixed group, the tokens involved may originate outside this group. This operation effectively disrupts the inherent relationships between tokens and introduces the information flow between channels to some extent, thereby breaking the interaction constraints typically imposed by local attention. To provide theoretical validation, we first present the following definition:
\begin{definition}
\label{def:Local 3D Partition}
(Local Partition). For the local group where self-attention calculations are performed, its 3D size $s\times s \times d$ satisfies $s\ll H$, $s\ll W$, and $d\ll D$, where $H\times W\times D$ denotes the dimensions of the module input after the token positions have been arranged.
\end{definition}
\begin{definition}
\label{def: Random Selection and Shuffle Function}
(Random Selection and Shuffle Function). $S:{{c}_{in}}\to c_{rs}^{i},i\in [1,n]$ denote the random selection and shuffle function. This function randomly divides the input ${{c}_{in}}\in {{\mathbb{R}}^{H\times W\times D}}$ into $n$ parts along the channel dimension, and then scrambles the elements within each part in the 3D space. As a result, $c_{rs}^{i}\in {{\mathbb{R}}^{H\times W\times {{d}_{i}}}},i\in [1,n]$, where $\sum\limits_{i=1}^{n}{{{d}_{i}}}=D$. 

Regardless of the initial distances between tokens, the random selection and shuffle operations may place any two tokens within the same window, making it possible to model non-local relationships at both the token and channel levels within local windows.
\end{definition}
\begin{definition}
\label{def: Interactive Set and Interactive distance}
(Interactive Set and Interactive Distance). Consider $c_{rs}^{i}\in {{\mathbb{R}}^{H\times W\times {{d}_{i}}}}$, where we define the set of token pairs within the same local group after the random shuffle operation as ${{I}_{S}}=\{({{t}_{j}},{{t}_{k}})\}$. This set indicates that, after the random shuffle function $S$, the token indices ${{t}_{j}}$ and ${{t}_{k}}$ are placed within the same local group. At this point, the interactive distance between the two tokens can be defined as ${{d}_{S}}$:
\end{definition}
\begin{equation}\label{}
    {{d}_{S}}({{t}_{j}},{{t}_{k}}) =
    \begin{cases}
        {{\left\| {{t}_{j}}-{{t}_{k}} \right\|}_{2}}, & \text{if} \, ({{t}_{j}},{{t}_{k}}) \in {{I}_{S}} \\
        \infty, & \text{if} \, ({{t}_{j}},{{t}_{k}}) \notin {{I}_{S}}
    \end{cases}
\end{equation}
Here, $\infty$ indicates that the interaction between the two tokens cannot be captured within the current local group, which does not affect the derivation process of distance modeling. Therefore, the expected interactive distance $d({{t}_{j}})$ of token ${{t}_{j}}$ can be defined as follows:
\begin{equation}\label{}
d({{t}_{j}})=\underset{\left. {{t}_{k}} \right|({{t}_{j}},{{t}_{k}})\in {{I}_{S}}}{\mathop{\mathbb{E}}}\,\left[ {{d}_{S}}({{t}_{j}},{{t}_{k}}) \right]
\end{equation}
It can be proven that the lower bound of $d({{t}_{j}})$ is $\Omega (\frac{\sqrt{2}}{4}(H+W{{d}_{i}}))$ . The detailed derivation steps can be found in the Appendix ~\ref{Proof of Theorem 3.4}.
\begin{theorem}
\label{thm: lower bound}
The lower bound of $d({{t}_{j}})$ is:
\end{theorem}
\begin{equation}
\begin{split}
  d({{t}_{j}}) &\ge \frac{\sqrt{2}}{4(HW{{d}_{i}}-1)} \Big[ H{{t}_{wj}}({{t}_{wj}}+1) \\
  & \quad + W{{d}_{i}}{{t}_{hj}}({{t}_{hj}}+1) \\
  & \quad + H(W{{d}_{i}}-{{t}_{wj}})(W{{d}_{i}}-{{t}_{wj}}-1) \\
  & \quad + W{{d}_{i}}(H-{{t}_{hj}})(H-{{t}_{hj}}-1) \Big]
\end{split}
\end{equation}
\begin{corollary}
\label{cor2.5}
Let $\bar{d}({{t}_{j}})$ denote the average interactive distance in $c_{rs}^{i}$, and it can be proved that $\bar{d}({{t}_{j}})\approx d({{t}_{j}})$, with the detailed proof provided in the Appendix~\ref{Proof to Corollary 3.5}. 
\end{corollary}
\begin{table*}[!htb]
    \centering
    \small
    \renewcommand{\arraystretch}{1.2}
\renewcommand{\tabcolsep}{3pt}
\caption{Quantitative comparisons of different methods on the COCO validation set \cite{lin2014microsoftcoco}. The best results are highlighted in \textbf{bold}, while the second-best results are marked with \underline{underline}.}
    \label{table_metric_main}
\begin{tabular}{cc|cccc|cccc}
\toprule[1.2pt]
\multirow{3}{*}{Model} & \multirow{3}{*}{Method}   & Controllability & \multicolumn{2}{c}{Quality}   
& Text Consistency & Controllability & \multicolumn{2}{c}{Quality}   
& Text Consistency
\\ 
% \cline{3-10}
& & \multicolumn{4}{c}{\gb{Canny}} & \multicolumn{4}{c}{\gb{Hed}} \\
& & HDD$\downarrow$ & FID$\downarrow$  & CLIP-Ae$\uparrow$   & CLIP-Score$\uparrow$ & HDD$\downarrow$ & FID$\downarrow$  & CLIP-Ae$\uparrow$   & CLIP-Score$\uparrow$ 
\\ \midrule[1.2pt]

  \multirow{2}{*}{SD1.5} 
& Uni-ControlNet & \underline{95.40}  & 33.81  & 5.207   & 0.259 & \underline{98.78}  & 59.72  & 5.086 & 0.252     \\ 
 & Uni-Control & 97.90 & 91.29  & 4.965 & 0.249 & 100.52 & 91.94  & 4.819 & 0.251   \\ 
% \cmidrule{1-2}
% \cline{1-2}
 \multirow{2}{*}{SDXL} 
& ControlNet-XS & 101.34  & 21.57  & 5.134 & \textbf{0.286}  & - & - & - & -     \\ 
 & ControlNext & 117.59 & 49.32  & 4.816 & 0.275  & - & - & - & -   \\ 
% \cline{1-2}
 \multirow{2}{*}{PixArt-$\alpha$} 
& PixArt-$\delta$ & 96.26 & \underline{21.38}  & \underline{5.508} & 0.279  & 98.91 &\underline{29.22}  & \underline{5.243} & \underline{0.275}    \\ 
 & RelaCtrl & \textbf{94.04}  & \textbf{20.34}  & \textbf{5.584} & \underline{0.282} & \textbf{96.11}  & \textbf{27.73}  & \textbf{5.451} & \textbf{0.276}      \\ 
\bottomrule[1.2pt]
\multirow{2}{*}{Model} & \multirow{2}{*}{Method} & \multicolumn{4}{c}{\gb{Depth}}  & \multicolumn{4}{c}{\gb{Segmentation}} \\
 &    & MSE-d$\downarrow$ & FID$\downarrow$  & CLIP-Ae$\uparrow$   & CLIP-Score$\uparrow$ & mIoU$\uparrow$ & FID$\downarrow$  & CLIP-Ae$\uparrow$   & CLIP-Score$\uparrow$   \\ \midrule[1.2pt]
  \multirow{2}{*}{SD1.5} 
& Uni-ControlNet & 102.75  & 43.17  & 5.230 & 0.250 & 0.316  & 40.83  & 5.270 & 0.255      \\ 
 & Uni-Control & 102.46 & 91.94  & 5.327 & 0.249 & \underline{0.382} & 40.74  & 5.462 & 0.258   \\ 
% \cline{1-2}
 \multirow{2}{*}{SDXL} 
& ControlNet-XS & \underline{99.20}  & \underline{34.38}  & 5.235 & 0.281  & - & - & - & -     \\ 
 & ControlNext & 101.63 & 73.26  & 4.919 & 0.253  & - & - & - & -   \\ 
% \cline{1-2}
 \multirow{2}{*}{PixArt-$\alpha$} 
& PixArt-$\delta$ & 99.69 & 35.21  & \underline{5.723} & \underline{0.283}  & 0.379 & \underline{35.50}  & \underline{5.668} & \underline{0.282}    \\ 
 & RelaCtrl & \textbf{99.11}  & \textbf{33.93}  & \textbf{5.887} & \textbf{0.285} & \textbf{0.405}  & \textbf{33.76}  & \textbf{5.702} & \textbf{0.287}      \\ 
 \bottomrule[1.2pt]
\end{tabular}
\end{table*}
According to Corollary~\ref{cor2.5}, the average distance that can be captured by grouped attention in TDSM is $\Omega (\frac{\sqrt{2}}{4}(H+W{{d}_{i}}))$, enabling the modeling of non-local interactions. In summary, the random selection in the channel and shuffle operation we introduced disrupts the positional order of tokens. However, since the token order is not explicitly modeled within the self-attention mechanism for visual tasks, this operation does not compromise the effectiveness of the process. Nonetheless, the arrangement of input tokens may affect the semantic information embedded in the latent code during recovery. To resolve this problem, we propose an inverse recovery operation applied to both the token and channel dimensions following the self-attention computation. This overall method with shuffle and recovery is termed the Two Dimensions Shuffle Mixer (TDSM), leverages the capability of this reversible transformation pairs to ensure information preservation during self-attention calculations, thereby enabling efficient non-local information interaction across both the channel and token dimensions.
\section{Experiment}
\subsection{Experiment Setup}
\textbf{Evaluation Metrics.}To comprehensively assess the quality of the generated images, we employed multiple evaluation metrics. The Fréchet Inception Distance (FID) \cite{heusel2017fid} and CLIP-Aesthetics score \cite{schuhmann2022clipae}measure the visual fidelity of the generated images, while the CLIP Score \cite{hessel2021clipscore} evaluates text consistency. The control fidelity is explicitly assessed through the distance between the generated and target images. For images guided by HED and Canny conditions, we utilized the Hausdorff Distance (HDD) \cite{huttenlocher1993comparing} for evaluation. For depth conditions, we used MSE-depth, and for segmentation map control, we employed the mean Intersection over Union (mIoU).

\textbf{Baseline.} We conducted a comparative analysis of RelaCtrl against state-of-the-art (SOTA) control techniques, including Uni-ControlNet \cite{zhao2024unicontrolnet}, UniControl \cite{qin2023unicontrol}, ControlNet-XS \cite{zavadski2025controlnetxs}, ControlNext \cite{peng2024controlnext}, and PixArt-$\delta$ \cite{chen2024PixArtc}. For the first four methods, we utilized their officially released pre-trained weights. It is worth noting that ControlNet-XS and ControlNext only provided weights for Canny and Depth conditions, with the Canny weights for ControlNext being trained on an animation dataset. To further highlight the performance of our proposed method, we trained the control branch for both PixArt-$\delta$ and our method entirely from scratch under identical experimental settings, enabling a rigorous quantitative and qualitative comparison.

\subsection{Compared with SOTA methods}
As shown in Table \ref{table_metric_main}, we comprehensively evaluated the proposed method against existing controllable generation techniques across four conditional control tasks. Our method consistently outperforms alternatives in control accuracy, as demonstrated by the superior performance on control indicators for various conditions, highlighting its precision in generating controlled images. Furthermore, our approach achieves consistently better results in terms of FID and CLIP aesthetic scores, reflecting enhanced image quality. In text similarity evaluations, the CLIP Score confirms that our method achieves superior text-image consistency across diverse tasks, demonstrating improved semantic alignment while maintaining high control accuracy and visual fidelity. 

Fig.~\ref{img:exp_compare} presents a visual comparison of RelaCtrl and other methods under Canny and Depth conditions. Our method significantly reduces computational complexity and parameter usage while maintaining generation quality comparable to PixArt-ControlNet, outperforming other approaches based on SD1.5 and SDXL. Additional controllable generation results of RelaCtrl are shown in Fig.~\ref{img:exp_visual}, further illustrating its effectiveness in extracting and injecting information from conditional images during the generation process. This enables the production of results that align seamlessly with conditional controls.
\begin{figure*}[!htb]
% \captionsetup{type=figure}
\centering
\includegraphics[width=0.9\linewidth]{images/exp_compare.jpg}
\caption{\label{img:exp_compare}Qualitative comparison of different methods. Please zoom in for better details.
}
% \vspace{-0.5em}
\end{figure*}
\begin{figure*}[!htb]
% \captionsetup{type=figure}
\centering
\includegraphics[width=0.9\linewidth]{images/exp_visual.jpg}
\caption{\label{img:exp_visual}Generation effects of RelaCtrl under varying control conditions.
}
% \vspace{-0.5em}
\end{figure*}
\subsection{Algorithm efficiency analysis}
We conducted a comprehensive analysis of RelaCtrl and ControlNet with 13 replicated modules (PixArt-$\delta$) under consistent experimental settings. Specifically, the input data was in bfloat16 format with a resolution of 512, the test batch size was set to 1, and all experiments were performed on an NVIDIA A100 GPU. Table~\ref{tab: effective} presents a comparative evaluation of the two methods, with each indicator expressed as a percentage relative to the original base model. Notably, memory consumption excludes usage by the CLIP and T5 encoders. Compared to the original PixArt-$\alpha$, RelaCtrl incurred a modest 7.38\% increase in parameters and an 8.61\% increase in computational complexity. These increments are significantly lower than those of the ControlNet method, which demonstrates nearly a 50\% increase in both parameters and complexity. Additionally, RelaCtrl showed some advantages in memory usage and inference time, although it is worth noting that inference time is predominantly influenced by the speed of the main network rather than the control branch, leading to similar small increases in this metric for both methods. Overall, RelaCtrl outperforms the original ControlNet method applied to the DiT model by achieving comparable or superior metrics and visual results while significantly reducing computational resource consumption. This demonstrates the effectiveness and efficiency of the proposed RelaCtrl framework.
\begin{table}[!htb]
    \centering
    \small
    \renewcommand{\arraystretch}{1.2}
\renewcommand{\tabcolsep}{3pt}
\caption{Evaluation of the proposed method's effectiveness, with the following units for the four metrics: Parameters (M), Complexity (GFLOPs), Inference Time (s), and Memory Usage (MiB)}
    \label{tab: effective}
\begin{tabular}{ccccccc}
\hline
 Method   & Parameters & Complexity & Inference  & Memory \\ \hline
    PixArt-$\alpha$
  & 611.15 & 542.56  & 3.81 & 2114     \\ 
   \multirow{2}{*}{w/ ControlNet} 
  & +294.34 & +270.57  & +0.51 & +1694     \\ 
  &(+48.16\%)  & (+49.87\%)  & (+13.39\%) & (+80.13\%) \\
 \multirow{2}{*}{w/ RelaCtrl} 
  & +\textbf{45.15}  & +\textbf{46.71}  & +\textbf{0.24} & +\textbf{395}      \\ 
  &(+\textbf{7.38\%})  & (+\textbf{8.61\%})  & (+\textbf{6.30\%}) & (+\textbf{18.70\%})  \\
\hline
\end{tabular}
\end{table}
\subsection{Ablation study}
We conducted quantitative experiments to evaluate the efficacy of guiding control block placement based on DiT-ControlNet Relevance. Utilizing relevance scores, we ranked the control block positions from highest to lowest and placed copy blocks at the top-ranked positions, evaluating configurations with 13, 12, 11, and 10 blocks. The results are summarized in Table~\ref{tab:abl1}. As anticipated, reducing the number of control blocks resulted in a gradual decline in FID and HDD metrics. Notably, under the guidance of relevance scores, using only 11 control blocks achieved performance comparable to the original ControlNet with 13 replicated blocks. When further reduced to 10 blocks, the quality of the generated results slightly declined relative to the 13-block setup. Consequently, we selected the top 11 positions ranked by relevance scores for control placement, as illustrated by the white numbers in Fig.~\ref{img:method_importance}.

To further validate the significance of the RGLC block and the incorporation of Prior 2 based on revelation, we conducted additional quantitative experiments which are shown in Table~\ref{tab:abl2}. Employing the ControlNet with 13 copy blocks as the baseline and the RelaCtrl network as the foundation, we evaluated the impact of removing the RGLC block and relevance prior 2, respectively. The results demonstrate that the absence of either component leads to a decline in image quality and control performance. Replacing the RGLC block with the original copy block not only significantly increases the parameter scale but also results in performance degradation, highlighting the efficacy of the RGLC block. In the experiment (w/o Prior 2), we applied a uniform TDSM channel division across all RGLC blocks and eliminated our correlation-specific settings. The poorer HDD and FID metrics observed indicate that allocating more parameters and computational resources to control positions with higher correlation is crucial for achieving high-quality controllable generation. These results underscore the importance of both the RGLC block and relevance prior 2 in enhancing generation quality and control precision.
\begin{table}[!htb]
    \centering
    \small
    \renewcommand{\arraystretch}{1.2}
\renewcommand{\tabcolsep}{3pt}
\caption{The impact of control block placement guided by DiT-ControlNet Relevance. ControlNet-top13, which directly replicates the first 13 blocks of the main branch, serves as the baseline for parameter volume comparison.}
    \label{tab:abl1}
\begin{tabular}{cccccccc}
\hline
 Setting   & HDD$\downarrow$ & FID$\downarrow$  & Para Ratio
\\ \hline
 ControlNet-top13 & 96.26  & 21.38   & 100\% \\ 
 Relevance-top13  & 94.57  & 20.31   & 100\%  \\ 
 Relevance-top12  & 95.88  & 20.79   & 92.5\%   \\  
 \rowcolor{gray!20}
 Relevance-top11  & 95.57  & 21.28   & 84.6\% \\ 
 Relevance-top10  & 96.36  & 22.24   & 76.9\% \\ 
\hline
\end{tabular}
\end{table}
\begin{table}[!htb]
    \centering
    \small
    \renewcommand{\arraystretch}{1.2}
\renewcommand{\tabcolsep}{3pt}
\caption{The impact of the RGLC block and the number of TDSM partitions within it on generation performance. The PixArt-$\delta$ with 13 coped blocks serves as the baseline for parameter comparison.}
    \label{tab:abl2}
\begin{tabular}{cccccccc}
\hline
 Setting   & HDD$\downarrow$ & FID$\downarrow$ & Para Ratio
\\ \hline
 \rowcolor{gray!20}
 RelaCtrl      & 94.04  & 20.34    & 15.3\%  \\ 
 w/o RGLC      & 95.57  & 21.28    & 84.6\% \\  
 w/o Prior2    & 97.30  & 22.47    & 17.1\%  \\ 
 Baseline      & 96.26  & 21.38    & 100\%  \\ 
\hline
\end{tabular}
\end{table}
\subsection{RelaCtrl in Flux}
To further validate the effectiveness and generalizability of the proposed method, we conducted additional experiments on Flux.1-dev, with the results provided in the Appendix~\ref{appendix:flux_explore}

\section{Conclusion}
In this paper, we explore the control information relevance across different layers in the diffusion transformer by the "ControlNet Relevance Score" experiments. We discover that layers with strong relevance to control information are located in the shallow to middle layers, while the deeper layers exhibit weaker relevance. 
% \textcolor{red}{Existing controlled generation methods based on DiT overlook this characteristic, adopting uniform control information injection strategies across all layers. This not only introduces a large number of additional parameters and computational overhead but also results in inefficient allocation.}
Then, we propose a relevance-guided strategy (RelaCtrl) for allocating control block insertion positions and adjusting the block's modeling capabilities, enhancing the efficiency of control information integration. Additionally, we design TDSM, which efficiently replaces the original self-attention and FFN through a randomized channel-token grouping attention mechanism.
% Theoretical analysis demonstrates that TDSM overcomes the limitations of local modeling.
Experimental results show that RelaCtrl achieves superior performance across two T2I models and four conditional guidance generation tasks, while also exhibiting significant efficiency advantages. We hope RelaCtrl provides valuable insights and references for controlled generation research based on diffusion transformers.
% 在本文中，我们通过消除影响的方式探究了diffusion transformer中的不同层的控制信息相关性，发现与控制信息相关性强的层位于中浅层，深层的控制信息相关性较弱。现有基于dit的可控生成研究忽略了这一特性，对不同层采取同样的控制信息注入策略，并引入了大量额外的参数和计算同时存在不合理分配的问题。本文设计了基于相关性指导的控制block插入位置分配和module建模能力调节的策略，提升控制信息引入的效率。同时，我们设计了TDSM，通过随机channel-token分组计算attention的设计高效替代原本的self-attention和FFN，并且提供了理论上的分析表明其突破局部建模缺陷的能力。实验结果表明Relactrl能够在两种t2i模型和4种控制条件引导生成的任务上，实现更优的性能，同时具有显著的效率优势。希望Relactrl能够为基于dit的可控生成研究提供参考和借鉴。

\section*{Impact Statement}
This paper presents work whose goal is to advance the field of 
Machine Learning. There are many potential societal consequences 
of our work, none of which we feel must be specifically highlighted here.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Related works}
\subsection{Diffusion-based models}
In recent years, diffusion-based methods have garnered significant success in the field of generation \cite{he2024idanimator, feng2024fancyvideo, zhang2024artbank}, particularly in text-to-image (T2I) generation \cite{guo2023animatediff,shuai2024survey}. These methods utilize text embeddings derived from pre-trained language encoders such as CLIP \cite{radford2021clip}, Bert \cite{devlin2018bert} and T5 \cite{raffel2020t5} to generate images with high fidelity and diversity through an iterative denoising process. Recently, the introduction of the latent diffusion model\cite{rombach2022ldm} has marked a significant advancement in this field, enhancing the quality and efficiency of generated content. In pursuit of greater scalability and enhanced generation quality, models like DiT \cite{peebles2023dit,chen2023PixArta}integrate large-scale Transformer architectures into the diffusion framework, pushing the boundaries of generative performance. Building on this foundation, Flux \cite{blackforestlabs2024flux} synthesizes flow-matching \cite{lipman2022flow} and Transformer-based architecture to achieve state-of-the-art performance.

\subsection{Controllable generation with diffusion models}
Controllable generation has emerged as a prominent area of research in diffusion models \cite{zhang2023controlnet, qin2023unicontrol, zhao2024unicontrolnet, chen2024PixArtc, peng2024controlnext}. Currently, there are two main approaches to incorporating controllable conditions into image generation: (1) training a large diffusion model from scratch to enable control under multiple conditions, and (2) fine-tuning a lightweight structure while keeping the original pre-trained model frozen. However, the first approach demands significant computational resources, which limits its accessibility for broader dissemination and personal use. In contrast, recent studies have explored the addition of supplementary network structures to pre-trained diffusion models, allowing for control over the generated outputs without the need to retrain the entire model.

ControlNet~\cite{zhang2023controlnet} enables image generation that aligns with control information by reproducing specific layers within the network and connecting them to the original layers using zero convolution. Building on this foundation, Uni-Control~\cite{qin2023unicontrol} introduces a Mixture-of-Experts (MoE) framework, which unifies control across multiple spatial conditions. ControlNet-XS~\cite{zavadski2025controlnetxs} further improves the interaction bandwidth and frequency between the control and main branches within the ControlNet architecture, drawing inspiration from principles of feedback control systems. Nonetheless, these approaches are primarily based on the U-Net structure and may not yield the desired results when directly applied to Diffusion Transformers (DiT) without modification~\cite{chen2024PixArtc}. PixArt-$\delta$~\cite{chen2024PixArtc} proposed a design methodology specifically tailored for DiT, but directly copying the first half of the network results in a 50\% increase in both parameter count and computational complexity, resulting in high computational cost and inconvenient for community research and practical deployment.

\section{Proof to Theorem}
\subsection{Proof to Theorem 3.4}
\label{Proof of Theorem 3.4}

For token, ${{t}_{j}}=({{t}_{hj}},{{t}_{wj}})$, each remaining token in the input sequence has an equal probability of being shuffled into the same group as ${{t}_{j}}$. Under this condition, the expected interactive distance between tokens can be calculated as follows:
\begin{equation}
\begin{split}
  d({{t}_{j}}) &= \underset{\left. {{t}_{k}} \right|({{t}_{j}},{{t}_{k}})\in {{I}_{S}}}{\mathop{\mathbb{E}}}\,\left[ {{d}_{S}}({{t}_{j}},{{t}_{k}}) \right] \\
  &= \frac{1}{HW{{d}_{i}}-1} \sum\limits_{h=0}^{H-1} \sum\limits_{w=0}^{W{{d}_{i}}-1} \sqrt{{{(h-{{t}_{hj}})}^{2}}+{{(w-{{t}_{wj}})}^{2}}}
\end{split}
\end{equation}
Using the following mean inequality
\begin{equation}
\sqrt{\frac{{{x}^{2}}+{{y}^{2}}}{2}}\ge \frac{x+y}{2}
\end{equation}
we can calculate a lower bound on $d({{t}_{j}})$:
\begin{equation}
\begin{split}
  d({{t}_{j}}) &=\frac{1}{HW{{d}_{i}}-1}\sum\limits_{h=0}^{H-1}{\sum\limits_{w=0}^{W{{d}_{i}}-1}{\sqrt{{{(h-{{t}_{hj}})}^{2}}+{{(w-{{t}_{wj}})}^{2}}}}} \\
  &\ge \frac{1}{HW{{d}_{i}}-1}\sum\limits_{h=0}^{H-1}{\sum\limits_{w=0}^{W{{d}_{i}}-1}{\frac{\sqrt{2}}{2}(\left| h-{{t}_{hj}} \right|+\left| w-{{t}_{wj}} \right|)}}
\end{split}
\end{equation}
Due to the following formulas:
\begin{equation}
\begin{split}
  \sum\limits_{h=0}^{H-1}{\left| h-{{t}_{hj}} \right|}
  &=\sum\limits_{h=0}^{{{t}_{hj}}}{\left| {{t}_{hj}}-h \right|}+\sum\limits_{h={{t}_{hj}}}^{H-1}{\left| h-{{t}_{hj}} \right|} \\ 
  & =\frac{{{t}_{hj}}({{t}_{hj}}+1)}{2}+\frac{(H-{{t}_{hj}})(H-{{t}_{hj}}-1)}{2}
\end{split}
\end{equation}
and
\begin{equation}
\begin{split}
  \sum\limits_{w=0}^{W{{d}_{i}}-1}{\left| w-{{t}_{wj}} \right|}
  &=\sum\limits_{w=0}^{{{t}_{hj}}}{\left| {{t}_{wj}}-w \right|}+\sum\limits_{w={{t}_{hj}}}^{W{{d}_{i}}-1}{\left| w-{{t}_{wj}} \right|} \\ 
  &=\frac{{{t}_{wj}}({{t}_{wj}}+1)}{2}+\frac{(W{{d}_{i}}-{{t}_{wj}})(W{{d}_{i}}-{{t}_{wj}}-1)}{2}
\end{split}
\end{equation}
we can obtain 
\begin{equation}
\begin{split}
  d({{t}_{j}})
  &\ge \frac{\sqrt{2}}{4(HW{{d}_{i}}-1)}[H{{m}_{wj}}({{m}_{wj}}+1)+W{{d}_{i}}{{m}_{hj}}({{m}_{hj}}+1)\\ 
  &+H(W{{d}_{i}}-{{m}_{wj}})(W{{d}_{i}}-{{m}_{wj}}-1)+W{{d}_{i}}(H-{{m}_{hj}})(H-{{m}_{hj}}-1)]
\end{split}
\end{equation}
thus proving the original theorem.
\subsection{Proof to Corollary 3.5}
\label{Proof to Corollary 3.5}
For tokens within the same local group, we calculate their average interactive distance:
\begin{equation}
\bar{d}({{t}_{j}})=\frac{1}{{{s}^{2}}-1}\sum\limits_{j=1}^{{{s}^{2}}-1}{d({{t}_{j}}\left| HW{{d}_{i}}-j \right.)}
\end{equation}
Among these, $d({{t}_{j}}\left| HW{{d}_{i}}-j \right.)$ represents the expected interactive distance when the number of remaining tokens is $HW{{d}_{i}}-j$. Based on Assumption~\ref{def:Local 3D Partition}, we know $j\in [2,{{s}^{2}}-1]$, and we can derive the following approximation:
\begin{equation}
d({{t}_{j}}\left| HW{{d}_{i}}-j \right.)\approx d({{t}_{j}}\left| HW{{d}_{i}}-1 \right.)=d({{t}_{j}})
\end{equation}
Therefore, the value of $\bar{d}({{t}_{j}})$ can be estimated as follows:
\begin{equation}
\begin{split}
    \bar{d}({{t}_{j}})
  & = \frac{1}{{{s}^{2}}-1} \sum\limits_{j=1}^{{{s}^{2}}-1} d({{t}_{j}}\left| HW{{d}_{i}}-j \right.) \\ 
  & \approx \frac{1}{{{s}^{2}}-1} \sum\limits_{j=1}^{{{s}^{2}}-1} d({{t}_{j}}\left| HW{{d}_{i}}-1 \right.) \\
  & = \frac{1}{{{s}^{2}}-1} \sum\limits_{j=1}^{{{s}^{2}}-1} d({{t}_{j}}) \\
  & = d({{t}_{j}})
\end{split}
\end{equation}

\section{Training Details}
\textbf{Training and Testing Datasets.} We curated a dataset containing 1.73 million high-resolution, high-quality images, each with an aesthetic score of 5.5 or higher. Following the production methodology outlined in ControlNet \cite{zhang2023controlnet}, we generated various types of conditional images, including HED, Canny, Depth, and Segment. For the quantitative experiments, the model was trained at a resolution of 512 and evaluated on the COCO validation dataset \cite{lin2014microsoftcoco}, consisting of 5000 images. For qualitative visual evaluation, the model was trained at a resolution of 1024 and tested on a high-quality, high-resolution test set of 1000 images. To ensure a fair evaluation, all experiments were conducted with a training of 5 epochs on 16 NVIDIA A100 GPUs, using identical settings across all trials.
\section{Additional experimental results}
\subsection{More exploration on Relevance}
\begin{figure*}[!htb]
% \captionsetup{type=figure}
\centering
\includegraphics[width=\linewidth]{images/appendix_full_image1.png}
\caption{\label{img:appendix_full1}Impact of deleting specific locations on the generated metrics in ControlNet with 27 and 13 blocks.
}
% \vspace{-0.5em}
\end{figure*}
In the main text, we conducted a study of DiT-ControlNet Relevance based on the PixArt-$\delta$ with 27 control blocks and obtained the following insights: the most critical modules of DiT-ControlNet are concentrated in the early-middle layers (e.g., blocks 5, 6, and 7). Overall, the ControlNet Relevance Score exhibits a trend of initially increasing and then decreasing.

To further validate the generalizability of this observation, we also conducted a similar study on a PixArt model with only the first 13 control blocks. Fig.~\ref{img:appendix_full1} presents the normalized FID and HDD metrics of the generated images after removing the control module at a specific position. It is evident that the experimental results align with our previous findings: the most influential blocks in the control branch remain blocks 5, 6, and 7, which follow the same trend of increasing and then decreasing in relevance. This demonstrates the robustness of our observations, which can provide valuable guidance for subsequent specific design choices.

In Fig.~\ref{img:append_remove}, we present additional visual demonstrations of skipping specific ControlNet layers, specifically layers 7, 9, and 27, which correspond to the highest, moderate, and lowest impact on the generated image. The results illustrate that removing layer 7 significantly degrades both the quality of the generated image and the control accuracy. In contrast, skipping the later layers, such as layer 27, has minimal negative effects on the overall performance.
\begin{figure*}[!htb]
% \captionsetup{type=figure}
\centering
\includegraphics[width=0.5\linewidth]{images/appendix_remove3.jpg}
\caption{\label{img:append_remove}Additional visual results of skipping specific ControlNet layers (7, 9, and 27), correspond to the highest, moderate, and lowest impact on the generated image.
}
% \vspace{-0.5em}
\end{figure*}
\subsection{Experiment on Flux}
\label{appendix:flux_explore}
In this section, we apply RelaCtrl to the latest Flux.1-dev with 12 billion parameters for experimentation. The comparison methods include OminiControl \cite{tan2024ominicontrol} and Flux ControlNet which is implemented based on the Diffusers library. Flux ControlNet uses the standard configuration of a double-stream layer with the first four replication layers, alongside a single-stream layer with the first ten replication layers.

When applying RelaCtrl to Flux, we aim to leverage as many insights and designs from the main text as possible. Accordingly, we retain the control positions of the double-stream layer unchanged and select seven positions out of ten on the single-stream layer for control. Table~\ref{tab:flux1} presents the efficiency comparison between the methods, with GFLOPs calculated at a resolution of 512, and inference time measured at 512 resolution with 30 DDIM sample steps. Although OminiControl introduces only a modest increase in parameters, it leads to a significant rise in computational load and reasoning time, severely limiting its practical efficiency. Flux ControlNet, on the other hand, shows moderate increases across several efficiency metrics. In contrast, RelaCtrl for Flux significantly reduces both computational complexity and reasoning time, achieving an optimal balance between efficiency and parameter volume. Fig.~\ref{img:append_flux} illustrates the visual results of various control methods on Flux.1-dev. Among them, RelaCtrl not only achieves precise and detailed control effects but also offers significant advantages in computational efficiency, demonstrating the applicability and generalization of the proposed method across different DiT architectures.
\begin{table}[!htb]
    \centering
    \small
    \renewcommand{\arraystretch}{1.2}
\renewcommand{\tabcolsep}{3pt}
\caption{Effectiveness comparison experiment conducted on Flux.1-dev model.}
    \label{tab:flux1}
\begin{tabular}{cccccccc}
\hline
 Setting   & Parameters (M) & Complexity (GFLOPs) & Inference (s)
\\ \hline
 Flux.1-dev     & 11901.39  & 9925.78    & 4.78  \\ 
 +OminiControl  & +14.49    & +6637.41   & +3.11 \\  
 +ControlNet    & +2952.65  & +2578.33   & +0.79  \\ 
  \rowcolor{gray!20}
 +RelaCtrl      & +549.19     & +495.03    & +0.34  \\ 
\hline
\end{tabular}
\end{table}

\begin{figure*}[!htb]
% \captionsetup{type=figure}
\centering
\includegraphics[width=0.5\linewidth]{images/appendix_flux.png}
\caption{\label{img:append_flux}Visual comparison of different control methods on Flux.1-dev.
}
% \vspace{-0.5em}
\end{figure*}

\subsection{Inference with Community Models}
We perform inference using the PixArt weights which were fine-tuned with Lora. Although RelaCtrl has not previously trained in these weights, it can still utilize them effectively. Fig.~\ref{img:append_style} showcases the model's generated paint, oil, gufeng, and pixel-style images under the specified conditions.
\begin{figure*}[!htb]
% \captionsetup{type=figure}
\centering
\includegraphics[width=\linewidth]{images/appendix_style.jpg}
\caption{\label{img:append_style}The control effect of RelaCtrl on the fine-tuned PixArt model. The upper and lower rows show the four transitions: (1) Canny to paint, (2) Depth to oil, (3) HED to gufeng, and (4) Segmentation to pixel.
}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
