\section{Related works}
\subsection{Diffusion-based models}
In recent years, diffusion-based methods have garnered significant success in the field of generation \cite{he2024idanimator, feng2024fancyvideo, zhang2024artbank}, particularly in text-to-image (T2I) generation \cite{guo2023animatediff,shuai2024survey}. These methods utilize text embeddings derived from pre-trained language encoders such as CLIP \cite{radford2021clip}, Bert \cite{devlin2018bert} and T5 \cite{raffel2020t5} to generate images with high fidelity and diversity through an iterative denoising process. Recently, the introduction of the latent diffusion model\cite{rombach2022ldm} has marked a significant advancement in this field, enhancing the quality and efficiency of generated content. In pursuit of greater scalability and enhanced generation quality, models like DiT \cite{peebles2023dit,chen2023PixArta}integrate large-scale Transformer architectures into the diffusion framework, pushing the boundaries of generative performance. Building on this foundation, Flux \cite{blackforestlabs2024flux} synthesizes flow-matching \cite{lipman2022flow} and Transformer-based architecture to achieve state-of-the-art performance.

\subsection{Controllable generation with diffusion models}
Controllable generation has emerged as a prominent area of research in diffusion models \cite{zhang2023controlnet, qin2023unicontrol, zhao2024unicontrolnet, chen2024PixArtc, peng2024controlnext}. Currently, there are two main approaches to incorporating controllable conditions into image generation: (1) training a large diffusion model from scratch to enable control under multiple conditions, and (2) fine-tuning a lightweight structure while keeping the original pre-trained model frozen. However, the first approach demands significant computational resources, which limits its accessibility for broader dissemination and personal use. In contrast, recent studies have explored the addition of supplementary network structures to pre-trained diffusion models, allowing for control over the generated outputs without the need to retrain the entire model.

ControlNet~\cite{zhang2023controlnet} enables image generation that aligns with control information by reproducing specific layers within the network and connecting them to the original layers using zero convolution. Building on this foundation, Uni-Control~\cite{qin2023unicontrol} introduces a Mixture-of-Experts (MoE) framework, which unifies control across multiple spatial conditions. ControlNet-XS~\cite{zavadski2025controlnetxs} further improves the interaction bandwidth and frequency between the control and main branches within the ControlNet architecture, drawing inspiration from principles of feedback control systems. Nonetheless, these approaches are primarily based on the U-Net structure and may not yield the desired results when directly applied to Diffusion Transformers (DiT) without modification~\cite{chen2024PixArtc}. PixArt-$\delta$~\cite{chen2024PixArtc} proposed a design methodology specifically tailored for DiT, but directly copying the first half of the network results in a 50\% increase in both parameter count and computational complexity, resulting in high computational cost and inconvenient for community research and practical deployment.