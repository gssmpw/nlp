% Table generated by Excel2LaTeX from sheet 'database1'
\begin{table*}[htbp]
  \centering
  
  % \hyphenpenalty=10000
  \scalebox{0.5}{
    \begin{tabular}{p{30mm} p{30mm} p{50mm} p{50mm} p{80mm} rr}
    \cline{1-7}
    \textbf{Paper} & \textbf{Category} & \textbf{Task} & \textbf{Architecture} & \textbf{Datasets Used} & \textbf{Model Range} & \textbf{Data Range} \\
    \cline{1-7}
    \citet{kaplan_scaling_2020} & Pre-Training & Language Generation & Decoder-only Transformer & WebText2 & 0M - 1B & 22M - 23B \\
    \citet{hoffmann_training_2022} & Pre-Training  & Language Generation & Decoder-only Transformer & MassiveText,Github, C4 & 70M - 16B & 5B - 500B \\
    \citet{tay_scaling_2022} & Pre-Training , Transfer Learning & Language Generation & Switch, T5 Encoder-Decoder, Funnel, MoS, MLP-mixer, GLU, Lconv, Evolved, Dconv, Performer,Universal, ALBERT 
  & Pretraining: C4, Fine-Tuning: GLUE, SuperGLUE, SQuAD & 173M - 30B &  \\
    \citet{hu_minicpm:_2024} & Pre-Training & Language Generation & Decoder-only Transformer & Large mixture & 40M - 2B &  \\
    \citet{caballero_broken_2023} & Pre-Training & Downstream Image Recognition and Language Generation & ViT, Transformers, LSTM & Vision pretrained: JFT-300M, downstream : Birds200, Caltech101, CIFAR-100; Language : BigBench &       &  \\
    \citet{hernandez_scaling_2021} & Transfer Learning & Code Generation & Decoder-only Transformer & Pre-train: WebText2, CommonCrawl, English Wikipedia, Books; FineTune: Github repos &   &  \\
    \citet{abnar_exploring_2021} & Transfer Learning & Image Recognition & ViT, MLP-Mixers, ConvNets & Pre-train: JFT, ImageNet21K & 10M - 10B &  \\
    \citet{mikami_scaling_2021} & Transfer learning & Image Recognition & ConvNets &    Syntheic Data   &       &  \\
    \citet{zhang_when_2024} & Transfer Learning & Machine Translation and Language Generation & Decoder-only Transformer &  WMT14 English-German (En-De) and WMT19 English-Chinese (En-Zh), CNN/Daily-Mail, MLSUM  & 1B - 16B & 84B - 283B \\
    \citet{chen_scaling_2024} & Transfer learning & Language Generation & Decoder-only Transformer & Pre-Train: RedPajama v1, Validation: GitHub,ArXiv,Wikipedia, C4, RedPajama validation sets, ProofPile & 43M - 3B &  \\
    \citet{lin_selecting_2024} & Transfer learning & Language Generation & Decoder-only Transformer, Encoder-Decoder Transformer, Multilingual, MoE & Fine Tune: WMT19 English-Chinese (En-Zh), Gigaword, FLAN & 100M - 7B &  \\
    \citet{dettmers_case_2023} & Quantization Inference & Language Generation & Decoder-only Transformer &  The Pile, Lambada, PiQA, HellaSwag, Windogrande     & 19M - 176B &  \\
    \citet{cao_scaling_2024} & Quantization Inference & Language Generation & Decoder-only Transformer & WikiText2, SlimPajama, MMLU, Alpaca & 500M - 70B &  \\
    \citet{kumar_scaling_2024} & Quantization Pre-Training, Quantization Inference & Language Generation & Decoder-only Transformer & Dolma V1.7 & 30M - 220M & 1B - 26B \\
    \citet{chen_are_2024} & Inference & Language Generation & Decoder-only Transformer & MMLU Physics, TruthfulQA, GPQA, Averitec &       &  \\
    \citet{snell_scaling_2024} & Inference & Language Generation & Decoder-only Transformer & MATH &       &  \\
    \citet{brown_large_2024} & Inference & Language Generation & Decoder-only Transformer & GSM8K, MATH, MiniF2F-MATH, CodeContests, SWE-bench lite & 70M - 70B &  \\
    \citet{wu_inference_2024} & Inference & Language Generation & Decoder-only Transformer & MATH500, GSM8K & 410M - 34B &  \\
    \citet{sardana_beyond_2024} & Inference &    Language Generation   &  Decoder-only Transformer   & Jeopardy, MMLU, BIG bench, WikiData, ARC, COPA, PIQA, OpenBook QA, AGI Eval, GSM8k, etc  & 150M-6B   & 1.5B - 1.2T  \\
    \citet{clark_unified_2022} & Sparsity & Language Generation & Decoder-only Transformer, MoE &  MassiveText   & 0 - 200B & 0-130B  \\
    \citet{frantar_scaling_2023} & Sparsity & Language Generation, Image Recognition & Encoder-decoder, ViT & JFT-4B, C4 & 1M - 85M & 0 - 1B \\
    \citet{krajewski_scaling_2024} & Sparsity & Language generation & Decoder-only Transformer, MoE & C4   & 129M - 3B & 16B - 130B \\
    \citet{yun_toward_2024} & Sparsity  & Language generation & Decoder-only Transformer, MoE & Slim Pajama & 100M - 730M & 2B - 20B \\
    \citet{chen2024scaling} & Sparsity & Language Generation &  Decoder-only Transformer   &   SlimPajama    & 500M - 8B   & 0.5B \\
    \citet{busbridge2025distillation} & Distillation  & Language generation & Teacher-Student Decoder-only Transformer & C4 & 100M - 12B & 0 - 500B \\
    \citet{henighan_scaling_2020} & Multimodality & Generative Image Modeling, Video Modeling, Language Generation & Decoder-only Transformer &  FCC100M, and various modal datasets  & 0.1M-100B      & 100M \\
    \citet{zhai_scaling_2022} & Multimodality & Image Recognition & ViT & ImageNet-21K & 5M - 2B & 1M - 3B \\
    \citet{alabdulmohsin_revisiting_2022} & Multimodality & Image Recognition, Machine Translation & ViT, MLP Mixers, Encoder-decoder, Decoder-only Transformer, Transformer encoder-LSTM decoder & JFT-300M, ImageNet, Birds200, CIFAR100, Caltech101, Big-Bench  & 10M-1B     &  32M-494M\\
    \citet{aghajanyan_scaling_2023} & Multimodality & Multimodal Tasks & Decoder-only Transformers & OPT, Common Crawl, LibriSpeech , CommonVoice, VoxPopuli, Spotify Podcast, InCoder, SMILES from Zincand Peopleâ€™s Speech & 8M - 30B & 5B - 100B \\
    \citet{li_are_2024} & Multimodality & Multimodal tasks & ViT, Decoder-only Transformer & CC12M, LAION-400M & 7B - 13B & 1M - 10M \\
    \citet{jones_scaling_2021} & Multi-agent RL & Hex & AlphaZero with neural networks &       &       &  \\
    \citet{neumann_scaling_2023} & Multi-agent RL & Pentago, ConnectFour & AlphaZero with neural networks &       &       &  \\
    \citet{gao_scaling_2022} & RL    & Reward Model training with Best of n or RL & Decoder-only Transformers &       &       &  \\
    \citet{hilton_scaling_2023} & Single-agent RL & ProcGen Benchmark, 1v1 version of Dota2, toy MNIST & ConvNets, LSTM &       & 0M - 10M &  \\
    \citet{ye_data_2024} & Data Mixture & Language Generation & Decoder-only Transformer & RedPajama & 70M - 410M &  \\
    \citet{liu_regmix:_2024} & Data Mixture & Language Generation & Decoder-only Transformer & Pile &       &  \\
    \citet{kang_autoscale:_2024} & Data Mixture & Language Generation & Decoder-only Transformer , Encoder-only Transformer & RedPajama &       &  \\
    \citet{que_d-cpt_2024} & Data Mixture & Language Generation, Continual Pre-training & Decoder-only Transformer &  various mixture of Code, Math, Law, Chemistry, Music, Medical    &  0.5B-4B   & 0.1B-26B \\
    \citet{tao_scaling_2024} & Vocabulary & Language Generation & Decoder-only Transformer &  SlimPajama  & 33M - 3B & 0 - 500B \\
    \citet{circuits_nodate} & Sparse Autoencoder & Training Autoencoder & Decoder-only Transformer &       &       &  \\
    \citet{gao_scaling_2024} & Sparse Autoencoder & Find Interpretable Latents & Decoder-only Transformer &       &       &  \\
    \citet{shao_scaling_2024} & Retrieval & Language Generation & Decoder-only Transformer & language modelling:RedPajama, S2ORC,  Downstream : TriviaQA, NQ, MMLU, MedQA &       &  \\
    \citet{muennighoff_scaling_2023} & Pre-Training & Language Generation & Decoder-only transformer & C4 & 10M - 9B & 0 - 900B \\
    \citet{allen-zhu_physics_2024} & Knowledge Capacity & Language Generation & Decoder-only transformer &   bioD    &       &  \\
    \citet{ma_neural_2024} & Graph Supervised learning & Graph Classification Task & InfoGraph, GraphCL, JOAO, GraphMAE & reddit-threads , ogbg-molhiv,ogbg-molpcba &       &  \\
    \citet{diaz_scaling_2024} & Criticize &       &       &       &       &  \\
    \citet{sorscher_beyond_2023} & Criticize & Image Recognition & ConvNets, ViT & SVHN, CIFAR-10, and ImageNet &       &  \\
    \citet{bahri_explaining_2021} & Theoretical &       &       &       &       &  \\
    \citet{bordelon_dynamical_2024} & Theoretical &       &       &       &       &  \\
    \citet{hutter_learning_2021} & Theoretical &       &       &       &       &  \\
    \citet{lin2024scalinglawslinearregression} & Theoretical &       &       &       &       &  \\
    \citet{sharma_neural_2020} & Theoretical &       &       &       &       &  \\
    \citet{jin_cost_2023} & Downscaling &       &       &       &       &  \\
    \cline{1-7}
    \end{tabular}%
    }
    \caption{Details on task, architecture of models and training setup for each paper surveyed.}
  \label{tab:database1}%
\end{table*}%
