\begin{table*}[!htb]
    \centering
    \scalebox{0.95}{
    \begin{tabular}{p{3.5cm} p{5.2cm} p{5.9cm}}
        \cline{1-3}
        \textbf{Paper} & \textbf{Key insights} & \textbf{Applicability} \\
        \cline{1-3}
        \multirow{4}{*}{\citet{ye_data_2024}} & Predicts optimal data compositions before training, reducing compute costs by up to 27\% while maintaining performance. & Pre-training optimization, data efficiency improvements. \\
        \cdashline{1-3}
        \multirow{3}{*}{\citet{liu_regmix:_2024}} & REGMIX optimizes data mixtures using proxy models, achieving 90\% compute savings. & Compute-efficient training, automated data selection, large-scale models. \\
        \cdashline{1-3}
        \multirow{4}{*}{\citet{allen-zhu_physics_2024}} & Language models can store 2 bits of knowledge per parameter, with knowledge retention dependent on training exposure. & Knowledge encoding, model compression, retrieval-augmented models. \\
        \cline{1-3}
    \end{tabular}
    }
     \caption{Critical scaling laws for data mixing and knowledge capacity.}
     \label{tab:scaling_laws6}
\end{table*}
