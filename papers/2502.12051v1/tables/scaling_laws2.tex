\begin{table*}[!tb]
    \centering
    \begin{tabular}{p{3.5cm} p{5.2cm} p{5.2cm}}
        \cline{1-3}
        \textbf{Paper} & \textbf{Key insights} & \textbf{Applicability} \\
        \cline{1-3}
        \citet{zhai_scaling_2022} & ViTs follow power-law scaling but plateau at extreme compute levels, with benefits primarily seen in datasets >1B images. & Image classification, object detection, large-scale vision datasets. \\
        \cline{1-3}
        \citet{aghajanyan_scaling_2023} & Multimodal models experience competition at smaller scales but transition into synergy as model and token count grow, following a "competition barrier." & Multimodal learning, mixed-modal generative models, cross-domain AI. \\
        \cline{1-3}
        \citet{li_are_2024} & Scaling vision encoders in vision-language models (VLMs) does not always improve performance, reinforcing the importance of data quality over raw scaling. & Vision-language models, image-text alignment, multimodal scaling challenges. \\
        \cline{1-3}
    \end{tabular}
    \caption{Summary of key insights found in scaling laws paper for computer vision and multimodal domains.}
    \label{tab:scaling_laws2}
\end{table*}
