\begin{table*}[!htb]
    \centering
    \scalebox{0.9}{
    \begin{tabular}{c p{3.5cm} p{5.6cm} p{5cm} }
        \cline{1-4}
        \textbf{Modality} & \textbf{Paper} & \textbf{Key insights} & \textbf{Applicability} \\
        \cline{1-4}
        \multirow{14}{*}{Language} & \multirow{4}{*}{\citet{kaplan_scaling_2020}} & Larger models are more sample-efficient, needing fewer training examples to generalize well. & Predicts model loss decreases with increasing parameters, used in early LMs like GPT-3.\\
        \cdashline{2-4}
        & \multirow{3}{*}{\citet{hoffmann_training_2022}} & The best performance comes from balancing model size and data, rather than just increasing parameters. & Balances compute, model size, and dataset size for optimal efficiency, as seen in Chinchilla.\\
        \cdashline{2-4}
        & \multirow{3}{*}{\citet{caballero_broken_2023}} & Performance does not always improve smoothly; there are inflection points where scaling stops working. & Identifies phase transitions, minimum data thresholds, and unpredictability in scaling behavior. \\
        \cdashline{2-4}
        & \multirow{4}{*}{\citet{hu_minicpm:_2024}} & Smaller models with better training can rival much larger models. & Demonstrates that smaller models with optimized training can outperform larger undertrained models.\\
        \cline{1-4}
        \multirow{4}{*}{Vision} & \multirow{4}{*}{\citet{zhai_scaling_2022}} & ViTs follow power-law scaling but plateau at extreme compute levels, with benefits primarily seen in datasets >1B images. & Image classification, object detection, large-scale vision datasets. \\
        \cline{1-4}
        \multirow{10}{*}{Multimodal} & \multirow{4}{*}{\citet{aghajanyan_scaling_2023}} & Multimodal models experience competition at smaller scales but transition into synergy as model and token count grow. & Multimodal learning, mixed-modal generative models, cross-domain AI. \\
        \cdashline{2-4}
        & \multirow{5}{*}{\citet{li_are_2024}} & Scaling vision encoders in vision-language models does not always improve performance, reinforcing the importance of data quality over raw scaling. & Vision-language models, image-text alignment, multimodal scaling challenges. \\
        \cline{1-4}
    \end{tabular}
    }
    \caption{Critical neural scaling laws for language, vision and multimodal models.}
    \label{tab:scaling_laws1}
\end{table*}
