\begin{table*}[htbp]
    \centering
    \scalebox{0.9}{
    \begin{tabular}{lccc}
        \cline{1-4}
        \textbf{Paper} & \textbf{Training code} & \textbf{Analysis code} & \textbf{Github link} \\ \cline{1-4}
        \citet{kaplan_scaling_2020} & N & N & ~ \\ 
        \citet{hoffmann_training_2022} & N & N & ~ \\ 
        \citet{hoffmann_training_2022} & N & N & ~ \\ 
        \citet{hu_minicpm:_2024} & Y & N & \href{https://github.com/OpenBMB/MiniCPM}{Link} \\ 
        \citet{caballero_broken_2023} & N & Y & \href{https://github.com/ethancaballero/broken_neural_scaling_laws}{Link} \\ 
        \citet{hernandez_scaling_2021} & N & N & ~ \\ 
        \citet{abnar_exploring_2021} & N & N & ~ \\ 
        \citet{mikami_scaling_2021} & N & Y & \href{https://github.com/pfnet-research/cg-transfer}{Link} \\ 
        \citet{zhang_when_2024} & N & N & ~ \\ 
        \citet{chen_scaling_2024} & N & N & ~ \\ 
        \citet{lin_selecting_2024} & N & Y & \href{https://github.com/linhaowei1/Fine-tuning-Scaling-Law/blob/main/benchmark/flan.csv}{Link}  \\ 
        \citet{dettmers_case_2023} & N & N & ~ \\ 
        \citet{cao_scaling_2024} & N & N & ~ \\ 
        \citet{kumar_scaling_2024} & N & N & ~ \\ 
        \citet{chen_are_2024} & Y & Y & \href{https://github.com/lchen001/CompoundAIScalingLaws}{Link}  \\ 
        \citet{snell_scaling_2024}& N & N & ~ \\ 
        \citet{brown_large_2024} & Y & Y & \href{https://github.com/ScalingIntelligence/large\_language\_monkeys/tree/main}{Link}  \\ 
        \citet{wu_inference_2024} & Y & N & \href{https://github.com/thu-wyz/inference_scaling}{Link}  \\ 
        \citet{sardana_beyond_2024} & N & N & ~ \\ 
        \citet{clark_unified_2022} & N & Y & \href{https://github.com/google-deepmind/scaling_laws_for_routing}{Link}  \\ 
        \citet{frantar_scaling_2023} & N & N & ~ \\ 
        \citet{krajewski_scaling_2024} & Y & Y & \href{https://github.com/llm-random/llm-random}{Link}  \\ 
        \citet{yun_toward_2024} & N & N & ~ \\ 
        \citet{chen2024scaling} & N & N & ~ \\ 
        \citet{henighan_scaling_2020} & N & N & ~ \\ 
        \citet{zhai_scaling_2022} & Y & N & \href{https://github.com/google-research/vision_transformer}{Link}  \\ 
        \citet{alabdulmohsin_revisiting_2022} & N & Y & \href{https://github.com/google-research/google-research/tree/master/revisiting_neural_scaling_laws}{Link}  \\ 
        \citet{aghajanyan_scaling_2023} & N & N & ~ \\ 
        \citet{li_are_2024} & N & N & ~ \\ 
        \citet{jones_scaling_2021} & Y & Y & \href{https://github.com/andyljones/boardlaw}{Link}  \\ 
        \citet{neumann_scaling_2023} & Y & Y & \href{https://github.com/OrenNeumann/AlphaZero-scaling-laws}{Link}  \\ 
        \citet{gao_scaling_2022} & N & N & ~ \\ 
        \citet{hilton_scaling_2023} & N & N & ~ \\ 
        \citet{ye_data_2024} & Y & Y & \href{https://github.com/yegcjs/mixinglaws}{Link}  \\ 
        \citet{liu_regmix:_2024} & Y & Y & \href{https://github.com/sail-sg/regmix}{Link}  \\ 
         \citet{kang_autoscale:_2024} & Y & Y & \href{https://github.com/feiyang-k/AutoScale }{Link} \\ 
        \citet{que_d-cpt_2024} & N & N & ~ \\ 
        \citet{tao_scaling_2024} & Y & Y & \href{https://github.com/sail-sg/scaling-with-vocab}{Link}  \\ 
        \citet{circuits_nodate} & N & N & ~ \\ 
        \citet{gao_scaling_2024} & Y & Y & \href{https://github.com/openai/sparse_autoencoder}{Link}  \\ 
        \citet{shao_scaling_2024} & Y & Y & \href{https://github.com/RulinShao/retrieval-scaling}{Link}  \\ 
        \citet{muennighoff_scaling_2023} & Y & Y & \href{https://github.com/huggingface/datablations}{Link}  \\ 
        \citet{allen-zhu_physics_2024} & N & N & ~ \\ 
        \citet{ma_neural_2024} & Y & N & \href{https://github.com/HaitaoMao/Graph-Neural-Scaling-Law}{Link}  \\ 
        \citet{sorscher_beyond_2023} & N & Y & \href{https://github.com/rgeirhos/dataset-pruning-metrics}{Link}  \\
        \cline{1-4}
    \end{tabular}
    }
    \caption{Reproducibility of different neural scaling law papers.}
    \label{tab:reproducibility}
\end{table*}