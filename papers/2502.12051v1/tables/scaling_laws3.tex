\begin{table*}[!htb]
    \centering
    \scalebox{0.95}{
    \begin{tabular}{p{3.5cm} p{6cm} p{5.2cm}}
        \cline{1-3}
        \textbf{Paper} & \textbf{Key insights} & \textbf{Applicability} \\
        \cline{1-3}
        \multirow{4}{*}{\citet{hernandez_scaling_2021}} & Pre-training amplifies fine-tuning, particularly for small datasets, and benefits larger models even under data constraints. & Transfer learning, pre-training optimization, few-shot learning. \\
        \cdashline{1-3}
        \multirow{5}{*}{\citet{abnar_exploring_2021}} & Large-scale pre-training improves downstream performance, but effectiveness depends on upstream-downstream interactions, not task complexity. & Vision transfer learning, upstream-downstream performance interactions. \\
        \cdashline{1-3}
        \multirow{4}{*}{\citet{zhang_when_2024}} & Optimal fine-tuning strategy depends on dataset size: PEFT for small, LoRA for mid-scale, and full fine-tuning for large-scale datasets. & Fine-tuning strategies, parameter-efficient tuning, LoRA, full fine-tuning. \\
        \cdashline{1-3}
        \multirow{4}{*}{\citet{lin_selecting_2024}} & Fine-tuning follows a two-phase transition: slow early adaptation followed by power-law improvements, guiding compute-efficient model selection. & Compute-efficient fine-tuning, early stopping, model selection strategies. \\
        \cline{1-3}
    \end{tabular}
    }
    \caption{Key highlights from scaling of fine-tuned and domain-adapted models.}
    \label{tab:scaling_laws3}
\end{table*}
