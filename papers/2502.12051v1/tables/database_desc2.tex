% Table generated by Excel2LaTeX from sheet 'database2'
\begin{table*}[htbp]
  \centering
  
  \scalebox{0.64}{
    \begin{tabular}{p{35mm} p{35mm} p{45mm} p{125mm}}
    \cline{1-4}
    Paper & Dependent variable & Scaling variable & Functional form \\
     \cline{1-4}
    \citet{kaplan_scaling_2020} & Pre-Training Loss & Model Parameters, Compute, Data, Training Steps& \(L(N,D) = \left[\left(\frac{N_c}{N}\right)^{\frac{\alpha_N}{\alpha_D}} + \frac{D_c}{D}\right]^{\alpha_D} \) \\
    \citet{hoffmann_training_2022} & Pre-Training Loss & Model Parameters, Data & \(L(N,D) = \frac{A}{N^\alpha} + \frac{B}{D^\beta} + E\) \\
    \citet{tay_scaling_2022} & Performance metric & Compute & \( P \propto  C^\alpha \) \\
    \citet{hu_minicpm:_2024} & Pre-Training Loss & Model Parameters, Data & \(L(P,D) = \frac{A}{N^\alpha} + \frac{B}{D^\beta} + E\) \\
    \citet{caballero_broken_2023} & Performance metric 
    % (e.g. prediction error, cross entropy, calibration error, AUROC, BLEU score percentage, F1 score, reward, Elo rating, or FID score) 
    & Model Parameters, Compute, Data, Input Size, Training Steps & \(y = a + \left(bx^{-c_0}\right)\prod_{i=1}^n\left(1 + \left(\frac{x}{d_i}\right)^{1/f_i}\right)^{-c_i*f_i}\) \\
    \citet{hernandez_scaling_2021} & Data Transferred & Model Parameters, Fine-tuning Data & \(D_t(D_f,N) = k(D_f)^\alpha(N)^\beta\) \\
    \citet{abnar_exploring_2021} & Downstream Error & Upstream Error & \(e_{DS} = k(e_{US})^a + c\) \\
    \citet{mikami_scaling_2021} & Downstream Error & Pre-training Data & \(e_{DS} = aD^{-\alpha} + c\) \\
    \citet{zhang_when_2024} & Downstream Loss & Fine-tuning Data, Data, Model Parameters, PET parameter & \(\hat{L}(X, D_f) = A * \frac{1}{X^\alpha} * \frac{1}{D_f^\beta} + E\) \\
    \citet{chen_scaling_2024} & Downstream performance & Pre-training Loss, Compute & \(L(C) = (\frac{C}{C_N})^\alpha\); \(P(L) = w_0 + w1 \cdot L\)  \\
    \citet{lin_selecting_2024} & Downstream Loss & Data, Fine-tuning Data & \(L(D) = \frac{B}{D_t + D^\beta} + E\) \\
    \citet{dettmers_case_2023} & Accurancy & Total Model Bits After Quantization &  \\
    \citet{cao_scaling_2024} & Total parameters & Quantization Ratio &  \\
    \citet{kumar_scaling_2024} & Loss & Data, Model Parameters, Training Precision, Post-train Precision & \(L(N, D, P_w, P_a, P_{\text{kv}}, P_{\text{post}}) =AN_{\text{eff}}^{-\alpha} + BD^{-\beta} + E + \delta_{PTQ}\) \\
    \citet{chen_are_2024} & Optimal LLM Calls & Fraction Of Easy And Difficult Queries &  \\
    % \citet{snell_scaling_2024} &       &       &  \\
    \citet{brown_large_2024} & Coverage & Number Of Samples & \(\log(C) = ak^{-b}\) \\
    \citet{wu_inference_2024} & Optimal Compute   & Model Parameters & \(\log_{10}(C) = 1.19\log_{10}(N) + 2.03\) \\
    \citet{sardana_beyond_2024} & Pre-Training Loss & Model Parameters, Data & \(L(N,D) = \frac{A}{N^\alpha} + \frac{B}{D^\beta} + E\) \\
    \citet{clark_unified_2022} & Loss & Model Parameters, Number Of Experts , Data & \(\log(L(N,E)) = a\log N + b\log E + c\log N\cdot\log E + d\)  \\
    \citet{frantar_scaling_2023} & Loss & Sparsity, Model Parameters, Data & \(L=(a_S(1-S)^{b_S} + c_S) \cdot \left(\frac{1}{N}\right)^{b_N} + \left(\frac{a_D}{D}\right)^{b_D} + c\) \\
    \citet{krajewski_scaling_2024} & Loss & Granularity, Model Parameters, Data & \(\mathcal{L}(N,D,G) = c + \left(\frac{g}{G^\gamma} + a\right)\frac{1}{N^\alpha} + \frac{b}{D^\beta}\) \\
    \citet{yun_toward_2024} & Loss & Model Parameters, Number Of Experts , Data & \( \log L(N,D,E) \triangleq \log\left(\frac{A}{N^\alpha} + \frac{B}{E^\beta} + \frac{C}{D^\gamma} + F\right) + d\log N\log E \)\\
    \citet{chen2024scaling} &  Post-Training Loss  & Uncompressed Model Loss, pruned ratio, Model parameters before pruning, Post-training Data      & \(L(N_0, D, \rho, L_0) =  L_0 + \left( \frac{1}{\rho} \right)^\gamma  \left( \frac{1}{N_0} \right)^\delta  \left( \frac{N_C}{N_0^\alpha} + \frac{D_C}{D^\beta} + E \right)\) \\
    \citet{henighan_scaling_2020} & Loss & Model Parameters, Compute, Data & \(L(x) = Ax^{-\alpha} + B\) \\
    \citet{zhai_scaling_2022} & Downstream Error & Compute & \(E = aC^b +c\) \\
    \citet{alabdulmohsin_revisiting_2022} & Loss & Compute, Model Parameters, Data & \(\frac{L_x - L_\infty}{(L_0 - L_x)^\alpha} = \beta x^c\) \\
    \citet{aghajanyan_scaling_2023} & Loss & Model Parameters, Data & \(\mathcal{L}(N, D_i, D_j) = \left[\frac{\mathcal{L}(N, D_i) + \mathcal{L}(N, D_j)}{2}\right] - C_{i,j} + \frac{A_{i,j}}{N^{\alpha_{i,j}}} + \frac{B_{i,j}}{|D_i| + |D_j|^{\beta_{i,j}}}\) \\
    \citet{li_are_2024} & Loss & Model Parameters, Data &  \\
    \citet{jones_scaling_2021} & Elo & Compute, Board Size & \(Elo = \left( m_{\text{boardsize}}^{\text{plateau}} \cdot \text{boardsize} + c^{\text{plateau}}\right) \cdot clamp( m_{\text{boardsize}}^{\text{incline}} \cdot \text{boardsize} + m_{\text{flops}}^{\text{incline}} \cdot \log \text{flop} + c^{\text{incline}}, 0)\) \\
    \citet{neumann_scaling_2023} & Game Score  & Model Parameters, Compute & \(E_i = \frac{1}{1 + (X_j/X_i)^{\alpha_X}}\) \\
    \citet{gao_scaling_2022} & Gold Reward model scores & Root Of KL Between Initial Policy And Optimized Policy (d) & \(R(d) = d(\alpha - \beta\log d)\) \\
    \citet{hilton_scaling_2023} & Intrinsic performance 
    %- minimum compute required to train a model to reach the same return 
    & Model Parameters, Environment Interactions 
    & \(I^{-\beta} = \left(\frac{N_c}{N}\right)^{\alpha_N} + \left(\frac{E_c}{E}\right)^{\alpha_E}\) \\
    \citet{ye_data_2024} & Loss on domain i & Proportion Of Training Domains & \(L_i(r_{1...M}) = c_i + k_i \exp\left(\sum_{j=1}^M t_{ij}r_j\right)\) \\
    % \citet{liu_regmix:_2024} &       &       &  \\
    % \citet{kang_autoscale:_2024} &       &       &  \\
    \citet{que_d-cpt_2024} & Validation loss & Model Parameters, Data, Mixture Ratio & \(L(N, D, r) = E + \frac{A}{N^\alpha} + \frac{B \cdot r^\eta}{D^\beta} + \frac{C}{(r + \epsilon)^\gamma}\) \\
    \citet{tao_scaling_2024} & Unigram-Normalised loss & Non-vocabulary Parameter, Vocabulary Parameters, Data & \(\mathcal{L}_u = -E + \frac{A_1}{N_{\text{nv}}^{\alpha_1}} + \frac{A_2}{N_{\text{v}}^{\alpha_2}} + \frac{B}{D^\beta}\) \\
    \citet{circuits_nodate} & Reconstruction error & Compute, Number Of Latents &  \\
    \citet{gao_scaling_2024} & Reconstruction loss & Number Of Latents, Sparsity Level & \(L(n,k) = \exp(\alpha + \beta_k \log(k) + \beta_n \log(n) + \gamma \log(k)\log(n)) + \exp(\zeta + \eta \log(k))\) \\
    \citet{shao_scaling_2024} &  Downstream Accuracy   & Datastore , Model Parameters, Data, Compute    &  \\
    \citet{muennighoff_scaling_2023} & Loss & Data, Model Parameters, Epochs & \(L(N,D) = \frac{A}{N'^\alpha} + \frac{B}{D'^\beta} + E\) \\
    \citet{busbridge2025distillation} & Student Loss & Teacher Loss, Student Parameters, Distillation Tokens  & \(L_S(N_S,D_S,L_T) = L_T + \frac{1}{L_T^{c_0}} \left(1 + \left(\frac{L_T}{L_{S,d_1}}\right)^{1/f_1}\right)^{-c_1/f_1} \left(\frac{A}{N_S^{\alpha'}} + \frac{B}{D_S^{\beta'}}\right)^{\gamma'}\) \\
    % \citet{allen-zhu_physics_2024} &       &       &  \\
    % \citet{ma_neural_2024} &       &       &  \\
    % \citet{diaz_scaling_2024} &       &       &  \\
    % \citet{sorscher_beyond_2023} &       &       &  \\
    % \citet{bahri_explaining_2021} &       &       &  \\
    % \citet{bordelon_dynamical_2024} &       &       &  \\
    % \citet{hutter_learning_2021} &       &       &  \\
    % \citet{lin2024scalinglawslinearregression} &       &       &  \\
    % \citet{sharma_neural_2020} &       &       &  \\
    % \citet{jin_cost_2023} &       &       &  \\
     \cline{1-4}
    \end{tabular}%
    }
    \caption{Scaling law forms proposed in different papers we surveyed.}
  \label{tab:database2}%
\end{table*}%
