\begin{table*}[h]
   \centering
   \begin{tabular}{|p{4cm}|p{6cm}|p{4cm}|}
       \cline{1-3}
       \textbf{Scaling Aspect} & \textbf{Guideline} & \textbf{Key Reference} \\
       \cline{1-3}
       \textbf{Model-Data Scaling} & Balance model parameters and data size using Chinchilla scaling laws; prioritize data scaling for compute-limited scenarios. & \citep{hoffmann_training_2022} \\
       \cline{1-3}
       \textbf{Compute Efficiency} & Optimize training compute via retrieval-based architectures, MoEs, and adaptive test-time compute strategies. & \citep{brown_large_2024} \\
       \cline{1-3}
       \textbf{Training Duration} & Longer training with repeated data exposure (4-16 epochs) is more effective than parameter expansion. & \citep{muennighoff_scaling_2023} \\
       \cline{1-3}
       \textbf{Sparse vs. Dense Models} & Sparse Mixture of Experts (MoE) models can achieve up to 40× compute efficiency gains compared to dense models. & \citep{krajewski_scaling_2024} \\
       \cline{1-3}
       \textbf{Data Composition} & Use REGMIX and AUTOSCALE to optimize data mixtures dynamically based on model scale. & \citep{liu_regmix:_2024, kang_autoscale:_2024} \\
       \cline{1-3}
       \textbf{Inference Optimization} & Use tree search and retrieval augmentation to allow smaller models to match larger models’ performance. & \citep{wu_inference_2024} \\
       \cline{1-3}
       \textbf{Quantization} & 4-bit precision optimizes zero-shot performance while reducing compute costs. & \citep{dettmers_case_2023} \\
       \cline{1-3}
   \end{tabular}
   \label{tab:scaling_guidelines}
      \caption{Scaling Strategies and Practical Recommendations}
\end{table*}