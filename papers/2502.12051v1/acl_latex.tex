% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
%\usepackage[review]{acl}
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{forest}
\usetikzlibrary{trees,positioning,shapes,shadows,arrows.meta}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{todonotes}
\usepackage{subfig}
\usepackage{arydshln}
\usepackage{enumitem}
% \usepackage{natbib}


% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{enumitem}

\usepackage{multirow}
\usepackage{tabularx}
\usepackage{longtable}
\usepackage{array}
\usepackage{lineno}
% \usepackage{breakurl}
% \usepackage{refcheck}

% \usepackage{ltxtable}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

%\title{\textit{How to Scale My Model?} A Practical Guideline for Utilizing Neural Scaling Laws for Upscaling Neural Models}

\title{How to Upscale Neural Networks with Scaling Law? \\A Survey and Practical Guidelines}

\author{
   Ayan Sengupta$^*$\hspace{10pt}
   Yash Goel$^*$\hspace{10pt} 
   Tanmoy Chakraborty
   \\
   Indian Institute of Technology Delhi, India
   \\
   \texttt{\{ayan.sengupta, ee1210984, tanchak\}@ee.iitd.ac.in}
 }
 
% Author information can be set in various styles:
% For several authors from the same institution:
%\author{Ayan Sengupta \and Yash Goel \and Tanmoy Chakraborty \\
%         IIT Delhi, India}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{Ayan Sengupta \\
%   IIT Delhi \\
%   % Affiliation / Address line 2 \\
%   % Affiliation / Address line 3 \\
%   \texttt{ayan.sengupta@ee.iitd.ac.in} \\\And
%   Yash Goel \\
%   IIT Delhi \\
%   % Affiliation / Address line 2 \\
%   % Affiliation / Address line 3 \\
%   \texttt{ee1210984@iitd.ac.in} \\\And
%   Tanmoy Chakraborty \\
%   IIT Delhi \\
%   % Affiliation / Address line 2 \\
%   % Affiliation / Address line 3 \\
%   \texttt{tanchak@iitd.ac.in} \\}

%\author{
% \textbf{Ayan Sengupta\textsuperscript{1}},
% \textbf{Yash Goel\textsuperscript{1}},
% \textbf{Tanmoy Chakraborty\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
% \textsuperscript{1}IIT Delhi,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
% \small{
%   % \textbf{Correspondence:} 
%   \href{ayan.sengupta@ee.iitd.ac.in}{ayan.sengupta@ee.iitd.ac.in}, \href{ee1210984@iitd.ac.in}{ee1210984@iitd.ac.in}, \href{tanchak@iitd.ac.in}{tanchak@iitd.ac.in}
% }
%}

\begin{document}

\addtocontents{toc}{\protect\setcounter{tocdepth}{-1}}

\maketitle

\def\thefootnote{*}
\textsuperscript{*}\footnotetext[0]{Equal contribution}
\def\thefootnote{\arabic{footnote}}


\begin{abstract}
Neural scaling laws have revolutionized the design and optimization of large-scale AI models by revealing predictable relationships between model size, dataset volume, and computational resources. Early research established power-law relationships in model performance, leading to compute-optimal scaling strategies. However, recent studies highlighted their limitations across architectures, modalities, and deployment contexts. Sparse models, mixture-of-experts, retrieval-augmented learning, and multimodal models often deviate from traditional scaling patterns. Moreover, scaling behaviors vary across domains such as vision, reinforcement learning, and fine-tuning, underscoring the need for more nuanced approaches. In this survey, we synthesize insights from over 50 studies, examining the theoretical foundations, empirical findings, and practical implications of scaling laws. We also explore key challenges, including data efficiency, inference scaling, and architecture-specific constraints, advocating for adaptive scaling strategies tailored to real-world applications. We suggest that while scaling laws provide a useful guide, they do not always generalize across all architectures and training strategies.


\end{abstract}

\section{Introduction}
\text{Scaling laws} have become a fundamental aspect of modern AI development, especially for large language models (LLMs). In recent years, researchers have identified consistent relationships between model size, dataset volume, and computational resources, demonstrating that increasing these factors leads to systematic improvements in performance. These empirical patterns have been formalized into mathematical principles, known as scaling laws, which provide a framework for understanding how the capabilities of neural networks evolve as they grow. Mastering these laws is crucial for building more powerful AI models, optimizing efficiency, reducing costs, and improving generalization.

The study of neural scaling laws gained prominence with the foundational work of~\citet{kaplan_scaling_2020}, who demonstrated that model performance follows a power-law relationship with respect to size, data, and compute. Their findings suggested that larger language models (LMs) achieve lower loss when trained on sufficiently large datasets with increased computational resources. Later, \citet{hoffmann_training_2022} refined these ideas, introducing the notion of compute-optimal scaling, which revealed that training a moderate-sized model on a larger dataset is often more effective than scaling model size alone. However, recent studies~\cite{muennighoff_scaling_2023,caballero_broken_2023,krajewski_scaling_2024} have challenged the universality of these laws, highlighting cases where sparse models, mixture-of-experts architectures, and retrieval-augmented methods introduce deviations from traditional scaling patterns. These findings suggested that while scaling laws provide a useful guide, they do not always generalize across all architectures and training strategies.

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{figures/paper_count.pdf}
    \caption{Papers surveyed under different categories. The detailed paper list is provided in Table~\ref{tab:database1} of Appendix~\ref{appx:details}.}
    \label{fig:paper_count}
    \vspace{-5mm}
\end{figure}

\if 0
\begin{figure*}[t]
    \centering
    \tikzset{
    basic/.style  = {draw, text width=5cm, align=center, font=\sffamily, rectangle},
    root/.style   = {basic, rounded corners=4pt, thin, align=center, fill=green!30},
    onode/.style = {basic, thin, rounded corners=2pt, align=center, fill=green!60,text width=5cm,},
    tnode/.style = {basic, thin, align=left, fill=pink!60, text width=13em, align=left},
    ttnode/.style = {basic, thin, align=left, fill=pink!60, text width=13em, align=left},
    xnode/.style = {basic, thin, rounded corners=2pt, align=center, fill=blue!20,text width=3cm,},
    xznode/.style = {basic, thin, rounded corners=2pt, align=center, fill=blue!20,text width=6cm,},
    wnode/.style = {basic, thin, align=left, fill=pink!10!blue!80!red!10, text width=6.5em},
    edge from parent/.style={draw=black, edge from parent fork right}

}
%
\scalebox{0.45}{
\begin{forest} for tree={
    grow=east,
    growth parent anchor=west,
    parent anchor=east,
    child anchor=west,
    edge path={\noexpand\path[\forestoption{edge},->, >={latex}] 
         (!u.parent anchor) -- +(2pt,0pt) |-  (.child anchor) 
         \forestoption{edge label};}
}
% l sep is used for arrow distance
[Neural scaling laws, basic,  l sep=5mm,
    [\textbf{Criticisms} ~\citet{sorscher_beyond_2023,diaz_scaling_2024}, ttnode]
    [Commendation, xznode,  l sep=10mm,
        [Model scaling, xznode, l sep=5mm,
        [Pre-training, xnode, l sep=10mm, 
        [Language models, xznode, l sep=13mm,
        [\textbf{Large LMs} ~\citet{kaplan_scaling_2020,hoffmann_training_2022,tay_scaling_2022,caballero_broken_2023}, tnode]
        [\textbf{Small LMs} ~\citet{hu_minicpm:_2024}, tnode]
        [\textbf{Sparse LMs} ~\citet{clark_unified_2022,krajewski_scaling_2024,yun_toward_2024}, tnode]
        ]
        [\textbf{Vision models} ~\citet{zhai_scaling_2022,alabdulmohsin_revisiting_2022}, tnode]
        [\textbf{Multimodal models} ~\citet{henighan_scaling_2020,aghajanyan_scaling_2023,li_are_2024}, tnode]
        ]
        [Post-training, xnode, l sep=50mm,
        [\textbf{Transfer learning/Fine tuning} ~\citet{hernandez_scaling_2021,zhang_when_2024,chen_scaling_2024,lin_selecting_2024}, tnode]
        [\textbf{Model compression} ~\citet{frantar_scaling_2023,chen2024scaling}, tnode]
        ]
        [Inference, xnode, l sep=20mm,
        [\textbf{Inference compute} ~\citet{brown_large_2024,wu_inference_2024,sardana_beyond_2024}, tnode]
        [\textbf{Retrieval} ~\citet{shao_scaling_2024}, tnode]
        [\textbf{Model compression} ~\citet{dettmers_case_2023,cao_scaling_2024,kumar_scaling_2024}, tnode]]
        ]
        [\textbf{Data scaling} ~\citet{muennighoff_scaling_2023, ye_data_2024, liu_regmix:_2024, kang_autoscale:_2024, que_d-cpt_2024, allen-zhu_physics_2024, tao_scaling_2024}, ttnode]
        ]
        ]
\end{forest}
}
\caption{Taxonomy of neural scaling laws.}
\label{fig:lit_surv}
\end{figure*}
\fi

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/taxonomy2.pdf}
\caption{A taxonomy of neural scaling laws.}
\label{fig:lit_surv}
\end{figure*}

Despite the growing importance of scaling laws, existing research remains fragmented, with limited synthesis of theoretical foundations, empirical findings, and practical implications. Given the rapid evolution of this field, there is a need for a structured analysis that consolidates key insights, identifies limitations, and outlines future research directions. While theoretical studies have established the mathematical principles governing scaling, their real-world applications, such as efficient model training, optimized resource allocation, and improved inference strategies, are less explored. To address this gap, we reviewed over 50 research articles (Figure~\ref{fig:paper_count} highlights papers on scaling laws on different topics) to comprehensively analyze scaling laws, examining their validity across different domains and architectures.

While prior surveys have made valuable contributions to understanding scaling laws, they have primarily focused on specific aspects of the scaling phenomenon (See Table \ref{tab:survey_differences}).~\citet{choshen2024hitchhikersguidescalinglaw} emphasized statistical best practices for estimating and interpreting scaling laws using training data, while \citet{li_misfitting_2024} emphasized on methodological inconsistencies in scaling studies and the reproduction crisis in scaling laws. Our survey distinguishes itself by offering comprehensive coverage of architectural considerations, data scaling implications, and inference scaling -- areas that previous surveys either overlooked or addressed only partially. %The survey also uniquely examines domain-specific scaling behaviors across different modalities like vision and reinforcement learning, paying particular attention to emerging architectures such as sparse models and mixture-of-experts that deviate from traditional power-law scaling patterns. By consolidating theoretical foundations with practical constraints and real-world considerations, our survey bridges the gap between idealized scaling laws and their application in contemporary AI systems.

\input{tables/survey_differences}
\label{tab:survey_differences}

%The remainder of this paper is structured as follows: Section~\ref{sec:lit_surv} presents an overview of neural scaling laws, discussing their role in pre-training and fine-tuning across domains such as language and computer vision. Section~\ref{sec:questions} provides practical guidelines for leveraging scaling laws in large-scale AI deployments. Finally, in Section~\ref{sec:criticism}, we discuss criticisms of existing scaling law research and propose directions for future advancements.\todo{This para is not needed}

\begin{figure*}[!t]
    \centering
    \subfloat[Architecture-wise statistics]{\includegraphics[width=0.33\linewidth]{figures/architecture.pdf}\label{fig:architecture}}
    \subfloat[Variable-wise statistics]{\includegraphics[width=0.31\linewidth]{figures/scaling_variable.pdf}\label{fig:scaling_variable}}
    \subfloat[Task-wise statistics]{\includegraphics[width=0.32\linewidth]{figures/tasks.pdf}\label{fig:tasks}}
    \caption{Number of paper studied in this survey paper for different model architectures (a), scaling variables (b) and scaling tasks (c). The detailed paper list is provided in Table~\ref{tab:database1} of Appendix~\ref{appx:details}. }
\end{figure*}


\section{Taxonomy of neural scaling laws}
\label{sec:lit_surv}
Understanding the scaling laws of neural models is crucial for optimizing performance across different domains. We predominantly explore the scaling principles for language models, extending to other modalities such as vision and multimodal learning. We also examine scaling behaviors in domain adaptation, inference, efficient model architectures, and data utilization. We highlight the taxonomy tree of scaling laws research in Figure~\ref{fig:lit_surv}. As highlighted in Figure~\ref{fig:paper_count}, neural scaling laws have been proposed predominantly for pre-training and fine-tuning scaling of large neural models. Among the models studied, as highlighted in Figure~\ref{fig:architecture}, decoder-only Transformers dominate the subject, followed by vision transformers (ViT) and Mixture-of-Experts (MoE). 

The most common neural scaling laws take the form of power laws (Equation~\ref{eq:basic_scaling}), where the model's loss ($L$) or performance metric assumes to follow a predictable relationship with different scaling variables, 
\begin{equation}
% L \propto N^{-\alpha} + D^{-\beta}
L(P_{i....n}) = \sum_{i=1}^{n} \alpha_i \cdot P_i^{-\beta_i}
\label{eq:basic_scaling}
\end{equation}
with appropriate scaling parameters $\beta_i$ and fitting parameters $\alpha_i$ for different scaling parameter $P_i$. Figure~\ref{fig:scaling_variable} highlights that the number of model parameters and data size are the most common used scaling factors. The exact forms of all the scaling laws are highlighted in Table~\ref{tab:database2} of Appendix~\ref{appx:details}. Among all the tasks, Figure~\ref{fig:tasks} suggests that language generation is the most common task used for developing these scaling laws, where the training cross-entropy loss is widely used to fit the laws. Based on the values obtained empirically, the scaling laws are fitted with non-linear optimization, most commonly by running algorithms like least square and BFGS (Broyden-Fletcher-Goldfarb-Shanno). Statistical methods like goodness-of-fit metrics are used to validate the correctness of the fitted curves. We elaborate on the evaluation of neural scaling laws in Appendix~\ref{appx:validation}. \\

In the following sections, we review the existing literature on neural scaling across various domains.

\subsection{Scaling laws of language models}
\citet{kaplan_scaling_2020} suggested that larger LMs improve performance by reducing loss through power-law scaling. However, this view evolved when studies showed that many large models were undertrained, and data scaling plays an equally crucial role in compute efficiency~\citep{hoffmann_training_2022}. More recent breakthroughs challenged traditional scaling assumptions. Broken Neural Scaling Law (BNSL) introduced non-monotonic trends, meaning that model performance can sometimes worsen before improving, depending on dataset thresholds and architectural bottlenecks~\citep{caballero_broken_2023}. Another exciting development came from small LMs, where optimized training strategies, such as a higher data-to-parameter ratio and adaptive learning schedules, enable models ranging from 1.2B to 2.4B parameters to rival significantly larger 7B-13B models~\citep{hu_minicpm:_2024}. These findings reshape the fundamental assumptions of scaling laws, proving that strategic training can outperform brute-force model expansion. 

\input{tables/scaling_laws1}

\subsection{Scaling laws in other modalities}
%\input{tables/scaling_laws2}

In computer vision, ViTs exhibit power-law scaling when model size, compute, and data grow together, but their performance plateaus at extreme compute levels, with noticeable gains only when trained on datasets exceeding 1B images~\citep{zhai_scaling_2022}. Meanwhile, studies on scaling law extrapolation revealed that while larger models generally scale better, their efficiency declines at extreme sizes, requiring new training strategies to maintain performance~\citep{alabdulmohsin_revisiting_2022}. In multimodal learning, an interesting phenomenon called the ``competition barrier'' has been observed where at smaller scales different input modalities compete for model capacity, but as models grow, they shift into a synergistic state, enabling accurate performance predictions based on model size and token count~\citep{aghajanyan_scaling_2023}.

However, not all scaling trends align with expectations. Contrary to the assumption that larger is always better, scaling vision encoders in vision-language models can sometimes degrade performance, highlighting the fact that data quality and modality alignment are more critical than brute-force scaling~\citep{li_are_2024}. These findings collectively emphasize that scaling laws are domain-dependent -- optimal scaling strategies require a careful balance between compute efficiency, dataset quality, and architecture rather than simply increasing model size. Table~\ref{tab:scaling_laws1} summarizes the scaling laws of pre-trained models for language and other modalities.

\subsection{Scaling laws for domain adaptation}
Pre-training and fine-tuning techniques have accelerated the adoption of large-scale neural models, yet the extent to which these models transfer across tasks and domains remains a key research question tied to scaling principles. Studies show that transfer learning follows a power-law where pre-training amplifies fine-tuning effectiveness, especially in small data regimes. Even with limited downstream data, larger models benefit significantly from pre-training, improving generalization~\citep{hernandez_scaling_2021}. In vision, pre-training saturation occurs due to upstream-downstream interactions, rather than just task complexity. Lower network layers quickly specialize in simple tasks, while higher layers adapt to complex downstream objectives~\citep{abnar_exploring_2021}. Similarly, in synthetic-to-real transfer, larger models consistently reduce transfer gaps, enhancing generalization across domains~\citep{mikami_scaling_2021}.

Fine-tuning strategies scale differently depending on dataset size. Parameter-efficient fine-tuning (PEFT) is well-suited for small datasets, low-rank adaptation (LoRA)~\citep{hu2021lora} performs best for mid-sized datasets, and full fine-tuning is most effective for large datasets. However, PEFT methods provide better generalization in large models, making them attractive alternatives to full-scale fine-tuning~\citep{zhang_when_2024}.

Scaling laws are also being utilized to accurately predict the fine-tuning performance of models. The FLP method~\citep{chen_scaling_2024} estimates pre-training loss from FLOPs, enabling accurate forecasts of downstream performance, particularly in models up to 13B parameters. Further refinements like FLP-M improve mixed-dataset predictions and better capture emergent abilities in large models. Finally, the Rectified scaling law~\citep{lin_selecting_2024} introduces a two-phase fine-tuning transition, where early-stage adaptation is slow before shifting into a power-law improvement phase. This discovery enables compute-efficient model selection using the ``Accept then Stop'' (AtS) algorithm to terminate training at optimal points.

\input{tables/scaling_laws3}

We summarize these findings in Table~\ref{tab:scaling_laws3}, suggesting that transfer learning is highly scalable, but effective scaling requires precise tuning strategies rather than just increasing model size.

\subsection{Scaling laws for model inference}

Simply scaling up models is not always the best way to improve model performance. ~\citet{chen_are_2024} suggested that more efficient test-time compute strategies can dramatically reduce inference costs while maintaining or even exceeding performance. Instead of blindly increasing LLM calls, they further suggested for allocating resources based on query complexity, ensuring that harder queries receive more compute while simpler ones use fewer resources. The importance of test-time compute strategies becomes even clearer when dealing with complex reasoning tasks. While sequential modifications work well for simple queries, parallel sampling and tree search dramatically improve results on harder tasks. Adaptive compute-optimal techniques have been shown to reduce computational costs by 4$\times$ without degrading performance, allowing smaller models with optimized inference strategies to surpass much larger models~\citep{snell_scaling_2024, brown_large_2024}. Advanced inference approaches, such as REBASE tree search~\citep{wu_inference_2024}, further push the boundaries of efficiency, enabling small models to perform on par with significantly larger ones. \\

Another breakthrough came from retrieval augmented models, where increasing the datastore size consistently improves performance without hitting saturation \citep{shao_scaling_2024}. This allows smaller models to outperform much larger ones on knowledge-intensive tasks, reinforcing that external datastores provide a more efficient alternative to memorizing information in model parameters.

\input{tables/scaling_laws4}

\subsection{Scaling laws for efficient models}
%\input{tables/scaling_laws5}

Scaling laws have expanded beyond simple parameter growth, introducing new methods to optimize routing, sparsity, pruning, and quantization for efficient LLM scaling. Routing-based models benefit from optimized expert selection, but their returns diminish at extreme scales, requiring careful expert configuration~\citep{clark_unified_2022}. In contrast, fine-grained MoE models consistently outperform dense transformers, achieving up to 40$\times$ compute efficiency gains when expert granularity is properly tuned~\citep{krajewski_scaling_2024}. However, balancing the number of experts ($E$) is crucial, where models with 4-8 experts offer superior inference efficiency, but require $2.5-3.5\times$ more training resources, making 16-32 expert models more practical when combined with extensive training data~\citep{yun_toward_2024}. Sparse model scaling offers another efficiency boost. Research has demonstrated that higher sparsity enables effective model scaling, allowing $2.15\times$ more parameters at 75\% sparsity, improving training efficiency while maintaining performance~\citep{frantar_scaling_2023}. Additionally, pruning laws ($\text{P}^{2}$ scaling laws) predict that excessive post-training data does not always improve performance, helping optimize resource allocation in pruned models~\citep{chen2024scaling}. \citet{dettmers_case_2023} showed that 4-bit quantization provides the best trade-off between accuracy and model size, optimizing zero-shot performance while reducing storage costs. Larger models tolerate lower precision better, following an exponential scaling law where fewer high-precision components are needed to retain performance~\citep{cao_scaling_2024}. Meanwhile, training precision scales logarithmically with compute budgets, with 7-8 bits being optimal for balancing size, accuracy, and efficiency~\citep{kumar_scaling_2024}. Recent reserach has expanded into distillation as well, developing a mathematical framework that predicts how well a student model will perform based on the student model's size, the teacher model's performance and the compute budget allocation between the teacher and the student~\citep{busbridge2025distillation}.
We summarize these practical insights in Table~\ref{tab:scaling_laws4} for better readability. 

\subsection{Data scaling laws}
\input{tables/scaling_laws6}
Scaling models involves more than just increasing parameters; optimizing data mixtures, training duration, and vocabulary size also plays a crucial role in enhancing performance and efficiency. Data mixing laws allow AI practitioners to accurately predict optimal data compositions before training, leading to 27\% fewer training steps without compromising accuracy~\citep{ye_data_2024}. Techniques like REGMIX optimize data selection using proxy models and regression, reducing compute costs by 90\% compared to manual data selection~\citep{liu_regmix:_2024}. Meanwhile, AUTOSCALE revealed that data efficiency depends on model scale, where high-quality data like Wikipedia helps small models but loses effectiveness for larger models, which benefit from diverse datasets like CommonCrawl~\citep{kang_autoscale:_2024}. For continual learning, the D-CPT Law provided a theoretical framework for balancing general and domain-specific data, guiding efficient domain adaptation and long-term model updates~\citep{que_d-cpt_2024}. Additionally, Chinchilla scaling assumptions were challenged by evidence showing that training models for more epochs on limited data can outperform simply increasing model size~\citep{muennighoff_scaling_2023}. Repeated data exposure remains stable up to 4 epochs, but returns diminish to zero after around 16 epochs, making longer training a more effective allocation of compute resources. Furthermore, the vocabulary scaling law suggested that as language models grow larger, their optimal vocabulary size should increase according to a power law relationship~\citep{tao_scaling_2024}. Finally, knowledge capacity scaling laws established that language models store 2 bits of knowledge per parameter, meaning a 7B model can encode 14B bits of knowledge -- surpassing English Wikipedia and textbooks combined~\citep{allen-zhu_physics_2024}.
Table~\ref{tab:scaling_laws6} summarizes the data scaling laws for developing neural models when data is not available in abundance. 

% \input{tables/scaling_laws6}

\section{Practical guidelines for utilizing scaling laws}

Large-scale adoption of neural models requires careful determination of several key factors:

\begin{itemize}
[noitemsep,nolistsep,topsep=0pt,leftmargin=1em]
   \item \textbf{Model size vs. data size}: How should the number of parameters and dataset size be balanced to achieve optimal performance?
   \item \textbf{Compute budget}: Given a fixed computational budget, how can training and inference be optimized for efficiency?
   \item \textbf{Training strategy}: Should longer training duration be prioritized over increased model size, especially in data-constrained environments?
   \item \textbf{Architecture choice}: Should the model be dense or sparse (e.g., MoE) to optimize performance for a given budget?
   \item \textbf{Inference efficiency}: What test-time strategies, such as retrieval augmentation or tree search, can be employed to enhance model output quality?
   \item \textbf{Quantization and pruning}: How can quantization and structured sparsity be used to minimize memory footprint while maintaining accuracy?
\end{itemize}

Based on the selections of the above choices, one can follow the steps highlighted in Figure~\ref{fig:strategy} for developing large-scale neural models. Figure~\ref{fig:training_strategy} illustrates that for developing large-scale models with multimodal capabilities, multimodal scaling laws~\cite{aghajanyan_scaling_2023} can be followed. For developing domain-specific nuanced application, one can follow D-CPT law~\cite{que_d-cpt_2024}, which provides a comprehensive guideline for domain-continuous pre-training. For developing general-purpose LLMs, one may need to follow Kaplan/Chinchilla scaling laws, if sufficient data is available and the model choice is dense; otherwise, MoE scaling laws~\cite{krajewski_scaling_2024} be utilized to develop sparser models. If sufficient data is not available, one can either train the model for longer epochs, following~\citet{sardana_beyond_2024}, or use data mixing strategies~\cite{ye_data_2024} for deriving optimal data composition for pre-training the model. 

Post-training, inference strategies (c.f. Figure~\ref{fig:inference_strategy}) can be followed to ensure that the model is utilized efficiently for the downstream applications. If the high memory consumption of the model is a concern, compression strategies like pruning~\cite{chen2024scaling} or low-precision quantization~\cite{cao_scaling_2024} can be used. To make larger models inference efficient, inference scaling laws~\cite{wu_inference_2024} or test-time scaling laws~\cite{chen_are_2024} can be utilized.
Appendix~\ref{appx:rq} elaborates more practical guidelines in terms of using the existing literature of neural scaling laws for developing large-scale neural models. 

%\input{tables/practical_guidelines}

\begin{figure*}
    \subfloat[Training strategies]{\includegraphics[width=0.59\linewidth]{figures/training.pdf}\label{fig:training_strategy}}
    \subfloat[Inference strategies]{\includegraphics[width=0.4\linewidth]{figures/inference.pdf} \label{fig:inference_strategy}}
    \caption{Practical guidelines for utilizing neural scaling laws while developing large-scale models.}
    \label{fig:strategy}
\end{figure*}


\section{Criticisms of scaling laws}
\label{sec:criticism}

~\citet{diaz_scaling_2024} challenged the generalizability of neural scaling laws, arguing that they fail in diverse real-world AI applications. They argued that scaling laws do not always hold when AI models serve heterogeneous populations with conflicting criteria for model performance. Larger datasets inherently reflect diverse communities, making it difficult to optimize a single model for all users. Similar to issues in multilingual AI, increasing data diversity often leads to performance degradation rather than improvement. Universal evaluation metrics are inadequate for capturing these complexities, potentially reinforcing biases against underrepresented groups. The authors further argued that smaller, localized AI models may be more effective for specific communities, highlighting the need to move beyond one-size-fits-all scaling assumptions.

Beyond dataset expansion, data pruning contradicts traditional scaling laws by demonstrating that performance improvements do not always require exponentially more data. Strategic pruning achieves comparable or superior results with significantly fewer training samples~\citep{sorscher_beyond_2023}. Not all data contributes equally, and selecting the most informative examples enables more efficient learning. Experimental validation on CIFAR-10, SVHN, and ImageNet shows that careful dataset curation can surpass traditional power-law improvements, questioning the necessity of brute-force scaling.

Despite their significant impact, many studies on scaling laws suffer from limited reproducibility (see Table~\ref{tab:reproducibility} in Appendix~\ref{appx:reproducibility}) due to proprietary datasets, undisclosed hyperparameters, and undocumented training methodologies. The inability to replicate results across different computing environments raises concerns about their robustness. Large-scale experiments conducted by industry labs often depend on private infrastructure, making independent verification challenging. This lack of transparency undermines the reliability of scaling law claims and highlights the urgent need for open benchmarks and standardized evaluation frameworks to ensure reproducibility.

\section{Future recommendations}
While neural scaling laws have provided valuable insights into model performance, their current formulations often fail to account for recent advancements in architecture, data efficiency, and inference strategies. %As AI models continue to evolve, future research must refine these laws to ensure their applicability across different domains and computational constraints. 
The following directions highlight key areas where scaling laws should be adapted to improve their predictive power and practical utility.

\textbf{Inference-aware scaling:} Scaling laws should incorporate test-time computation, as compute-efficient inference strategies (e.g., iterative refinement, tree search, retrieval-based augmentation) can allow smaller models to outperform larger ones. Future research should focus on balancing training vs. inference compute costs to develop models that scale efficiently in real-world applications.

\textbf{Compute-optimal model selection:} Scaling laws should not only predict performance improvements but also guide model selection given a fixed compute budget. Future work should explore multi-objective optimization frameworks that balance performance, energy efficiency, and cost to drive more sustainable AI development.

\textbf{Efficient data scaling and pruning:} The optimization of model scaling necessitates a shift from volume-based to quality-focused data selection. Future frameworks should prioritize informative examples and integrate diversity metrics to enhance generalization, moving beyond simple dataset expansion.


\section{Conclusion}
This survey provided a comprehensive analysis of neural scaling laws, exploring their theoretical foundations, empirical findings, and practical implications. It synthesized insights across various modalities, including language, vision, multimodal learning, and reinforcement learning, to uncover common trends and deviations from traditional power-law scaling. While early research established predictable relationships between model size, dataset volume, and computational resources, more recent studies have shown that these relationships are not universally applicable. Sparse architectures, retrieval-augmented models, and domain-specific adaptations often exhibit distinct scaling behaviors, challenging the notion of uniform scalability. Furthermore, advancements in fine-tuning, data pruning, and efficient inference strategies have introduced new perspectives on compute-optimal scaling. Despite their significance, scaling laws remain an evolving area of research, requiring further refinement to address real-world deployment challenges and architectural innovations.

\section*{Limitations}
While this survey provides a broad synthesis of neural scaling laws, it primarily focuses on model size, data scaling, and compute efficiency. Other important aspects, such as hardware constraints, energy consumption, and the environmental impact of large-scale AI training, are not deeply explored. Another limitation is the reliance on prior empirical findings, which may introduce variability due to differing experimental setups and proprietary datasets. Without access to fully reproducible scaling law experiments, some conclusions remain dependent on the methodologies employed in original studies.

\section*{Ethical Considerations}
Scaling laws, while effective in optimizing AI performance, can also raise issues of accessibility and fairness. The development of increasingly large models favors institutions with substantial computational resources, creating a divide between well-funded research groups and smaller organizations. Furthermore, as scaling laws often assume uniform data utility, they may amplify biases present in large-scale datasets, potentially leading to skewed outcomes in real-world applications. Ethical concerns also arise from the energy-intensive nature of training large models, contributing to environmental concerns. Addressing these issues requires more inclusive AI development strategies, ensuring that scaling laws consider broader societal impacts rather than focusing solely on performance optimization.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}

\clearpage

\appendix

\tableofcontents
\addtocontents{toc}{\protect\setcounter{tocdepth}{2}} % Restore normal ToC depth for appendix


\section{Fitting and validating scaling laws}
\label{appx:validation}

Fitting scaling laws involves several key methodological choices that can significantly impact the final results and conclusions. The choice of optimization approach, loss function, initialization strategy, and validation method all play crucial roles in determining the reliability and reproducibility of scaling law studies.

\subsection{Optimization methods}
The most common approaches for fitting scaling laws involve non-linear optimization algorithms like BFGS (Broyden-Fletcher-Goldfarb-Shanno) (used by ~\citet{frantar_scaling_2023}), L-BFGS (used by ~\citet{tao_scaling_2024}) and least squares (used by ~\citet{caballero_broken_2023}). Some studies~\citep{covert2024scalinglawsvalueindividual, hashimoto21a} also use optimizers like Adam or Adagrad, though these may be less suitable for scaling law optimization due to their data-hungry nature and assumptions about gradient distributions.

\subsection{Loss functions and objectives}
Several loss functions are commonly used for fitting scaling laws:
\begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]
    \item \textbf{Mean squared error (MSE)}: Emphasizes larger errors due to quadratic scaling (used by ~\citet{ghorbani2021scaling}).
\item \textbf{Mean absolute error (MAE)}: Provides more robust fitting less sensitive to outliers (used by ~\citet{hilton_scaling_2023}).
\item \textbf{Huber loss}: Combines MSE's sensitivity to small errors with MAE's robustness to outliers (used by ~\citet{hoffmann_training_2022}).
% \item Log-transformed variants: Help manage the wide range of scales in the data
\end{itemize}


\subsection{Initialization strategies}
The initialization of scaling law parameters proves to be critically important for achieving good fits. Common approaches include grid search over parameter spaces~\citep{aghajanyan_scaling_2023}, random sampling from parameter ranges~\citep{frantar_scaling_2023}, and multiple random restarts to avoid local optima~\citep{caballero_broken_2023}.
% \begin{itemize}
%     \item Grid search over parameter spaces
% \item Random sampling from parameter ranges
% \item Initialization based on prior studies or theoretical expectations
% \item Multiple random restarts to avoid local optima

% \end{itemize}

\subsection{Validation methods}
It is hugely important to understand if the scaling law fit achieved is accurate and valid. Most of the papers surveyed lack in validating their fits. Several approaches can help validating the effectiveness of scaling law fits. Statistical methods like computing confidence intervals can act as a goodness-of-fit metric~\citep{alabdulmohsin_revisiting_2022}. Furthermore, researchers can perform out-of-sample testing by extrapolation to larger scales~\citep{hoffmann_training_2022}.

\subsection{Limitations of fitting techniques}
\citet{li_misfitting_2024} revealed several critical methodological considerations in fitting scaling laws. Different optimizers can converge to notably different solutions even with similar initializations, underscoring the need for careful justification of optimizer choice. Similarly, the analysis showed that different loss functions can produce substantially different fits when working with real-world data containing noise or outliers, suggesting that loss function selection should be guided by specific data characteristics and desired fit properties. Perhaps most importantly, the paper demonstrated that initialization can dramatically impact the final fit, with some methods exhibiting high sensitivity to initial conditions. Together, these findings emphasize the importance of thorough methodology documentation across all aspects of the fitting process - from optimizer selection and loss function choice to initialization strategy - to ensure reproducibility and reliability in scaling law studies.

\section{Other scaling laws}
\label{appx:details}

\subsection{Scaling laws for reinforcement learning}

Scaling laws in reinforcement learning (RL) and reward model optimization reveal both similarities and differences with generative modeling. Single-agent RL follows power-law scaling with model size and environment interactions, with optimal scaling exponents between 0.4-0.8 across tasks lower than the 0.5 exponent observed in language models~\citep{hilton_scaling_2023}. RL tasks require orders of magnitude smaller models than generative tasks, correlating with task horizon length, which dictates environment interaction scaling. Task difficulty increases compute needs but does not affect scaling exponents, highlighting horizon length as a key factor in RL scaling efficiency.

In board games like Hex which involves multi-agent RL, \citet{jones_scaling_2021} showed that AlphaZero performance follows predictable scaling trends, with compute requirements increasing 7$\times$ per board size increment for perfect play and 4$\times$ for surpassing random play~\citep{jones_scaling_2021}. 
\citet{neumann_scaling_2023} extended this study to Pentago and ConnectFour, proposing scaling laws which show that player strength scales with network size as $\alpha_N \approx 0.88$, performance with compute as $\alpha_C \approx 0.55$, and optimal network size with compute budget as $\alpha_{\text{opt}} \approx 0.63$~\citep{neumann_scaling_2023}. Larger multi-agent models exhibit higher sample efficiency, though these trends may not generalize to highly complex games like Chess and Go.

Reward model overoptimization in RLHF follows distinct functional forms: Best-of-$n$ (BoN) reward optimization is governed by $d(\alpha_{\text{bon}} - \beta_{\text{bon}} d)$, whereas RL reward optimization follows $d(\alpha_{\text{RL}} - \beta_{\text{RL}} \log d)$, where $d$ represents KL divergence from the initial policy~\citep{gao_scaling_2022}. RL requires higher KL divergence than BoN for optimization, and reward model overoptimization scales logarithmically with model size, while policy size has minimal impact. 
% A minimum of 2,000 training examples is needed for reward models to exceed random performance, and KL penalties only influence convergence speed without improving gold reward scores, functioning similarly to early stopping. 
These findings reinforce the importance of balancing compute allocation, environment complexity, and optimization techniques to achieve scalable and efficient RL models.

\subsection{Sparse autoencoders}

Recent research has established scaling laws for dictionary learning, providing insights into how latent representations and sparsity impact reconstruction error and computational efficiency. Sparse autoencoders with top-$K$ selection follow power-law scaling for reconstruction error (MSE) in terms of the number of latents $n$ and sparsity $k$, though this relationship only holds for small $k$ relative to model dimension~\citep{gao_scaling_2024}. Larger language models require more latents to maintain the same MSE at a fixed sparsity, reinforcing that latent dimensionality must scale with model size for effective reconstruction. Additionally, MSE follows a power-law relationship with the compute used during training, suggesting that efficient scaling strategies must balance sparsity, latent size, and training compute to minimize error effectively. This is reinforced by \citet{circuits_nodate}, showing that feature representations follow predictable scaling trends, where larger models develop richer, more interpretable dictionaries as the number of learned features increases.

% In dictionary learning, emerging scaling laws indicate that feature representations follow predictable scaling trends, where larger models develop richer, more interpretable dictionaries as the number of learned features increases~\citep{circuits_nodate}. 
% These findings suggest that structured sparsity techniques can enhance model efficiency by leveraging compact latent spaces while maintaining strong representational capacity. Together, these studies highlight that autoencoder scaling and dictionary learning exhibit power-law behavior similar to traditional deep learning models, but require specialized tuning of sparsity, latent space, and compute allocation to achieve optimal performance.

\subsection{Graph neural networks}
Unlike in computer vision and natural language processing, where larger datasets typically improve model generalization, graph self-supervised learning (Graph SSL) methods fail to exhibit expected scaling behavior and performance fluctuates unpredictably across different data scales~\citep{ma_neural_2024}. However, self-supervised learning pretraining loss does scale with more training data, but this improvement does not translate to better downstream performance. The scaling behavior is method-specific, with some approaches like InfoGraph showing more stable scaling than others like GraphCL.

\input{tables/database_desc1}
\input{tables/database_desc2}

\section{Reproducibility of scaling laws papers}
\label{appx:reproducibility}

The reproducibility status of neural scaling law papers presents a mixed landscape in terms of research transparency. We consolidate and provide the links to github code repositories in the Table \ref{tab:reproducibility}. Among the 45 surveyed papers proposing scaling laws, 
% 15 papers (40.5\%) made their training code publicly available, while 13 papers (35.1%) shared their analysis code. 
22 papers (48.9\%) provided repository links, indicating some level of commitment to open science practices.  However, more than half of the papers still lack basic reproducibility elements, with 29 papers (64.4\%) not sharing training code and 27 papers (60\%) withholding analysis code. This comprehensive survey suggests that while there is a growing trend toward reproducibility in neural scaling law research, there remains substantial room for improvement in establishing standard practices for code sharing and result verification.

\input{tables/github_link}
% \label{tab:reproducibility}

\section{Additional research questions and guidelines}
\label{appx:rq}

% \subsection{Pre-Training Scaling Laws}

\paragraph{RQ1. How does the relationship between model size, training data, and compute efficiency influence LLM performance across different tasks?} 

For optimal LLM scaling, \citet{kaplan_scaling_2020} proposed that model performance follows a power law:
\begin{equation}
% L \propto N^{-\alpha} + D^{-\beta}
L(N,D) = \left[\left(\frac{N_c}{N}\right)^{\frac{\alpha_N}{\alpha_D}} + \frac{D_c}{D}\right]^{\alpha_D}, 
\end{equation}
where \( L \) is the loss, \( N \) is the model size, \( D \) is the dataset size. They suggest an increase of 8$\times$ model size requires a 5$\times$ data increase to maintain efficiency:
\begin{equation}
D \propto N^{0.74}.
\end{equation}
\citet{hoffmann_training_2022} refined this by adding an irreducible loss term:
\begin{equation}
    L(N,D) = \frac{A}{N^\alpha} + \frac{B}{D^\beta} + E.
    \label{eq:chinchilla}
\end{equation}
They also suggested balancing model and data scaling equally:
\begin{equation}
D \propto N.
\end{equation}
Small LMs require a higher data-to-model ratio (\( D:N = 192:1 \)) and benefit from continuous training and domain adaptation~\citep{hu_minicpm:_2024}. Broken Neural Scaling Laws (BNSL) describe non-monotonic scaling behaviors, modeled by a piecewise function~\citep{caballero_broken_2023}:
\begin{equation}
L(N, D) = 
\begin{cases} 
  aN^{-\alpha} + bD^{-\beta}, & \text{if } N < N_c \\
  cN^{-\alpha'} + dD^{-\beta'}, & \text{if } N \geq N_c
\end{cases}
\end{equation}
where \( N_c \) represents the transition threshold. When data is constrained, smaller models with more epochs perform better, with performance gains plateauing around 4 epochs but still beneficial up to 16 epochs~\citep{sardana_beyond_2024, muennighoff_scaling_2023}:
\begin{equation}
L(E) = L_0 E^{-\delta}, \quad E < E_c,
\end{equation}
where \( E \) is the number of epochs and \( E_c \) is the saturation point. To maximize efficiency, prioritizing data-to-parameter ratios over raw model expansion ensures that smaller, well-trained models outperform larger undertrained ones. High-quality, diverse data and efficient training remain keys to scalable and robust AI models.

\paragraph{RQ2. How do different model architectures scale differently, and what are the architectural bottlenecks?}
According to \citet{tay_scaling_2022}, the vanilla Transformer consistently demonstrates superior scaling properties compared to other architectures, even though alternative designs might perform better at specific sizes. Architectural bottlenecks manifest differently across these designs. For instance, linear attention models like Performer and lightweight convolutions show inconsistent scaling behavior, while ALBERT demonstrates negative scaling trends. This finding helps explain why most LLMs maintain relatively standard architectures rather than adopting more exotic variants.
\begin{equation}
    P \propto  C^\alpha,
\end{equation}
where \( P \) is the performance metric, \( C \) represents compute, and \( \alpha \) are fitting parameters. Furthermore, ~\citet{zhai_scaling_2022} revealed that ViT reveals that these models exhibit \textit{double saturation} --performance plateaus at both very low and very high compute levels, suggesting architectural limitations specific to the vision domain (Equation \ref{eq:vit}). However, as shown by \citet{li_are_2024}, simply scaling up vision encoders in multimodal models does not consistently improve performance, indicating that architectural scaling benefits are not uniform across modalities.
\begin{equation}
    E = a (C + d)^{-b} + c,
    \label{eq:vit}
\end{equation}
where \( E \) denotes downstream error, \( C \) represents compute, and \( a, b, c, d \) are fitting parameters.

These findings suggest several practical implications for practitioners. First, when selecting an architecture for scaling, it is crucial to evaluate its scaling behavior rather than just its performance at a single size point. The indicates that practitioners should be cautious about adopting architectures that drastically modify core Transformer components, especially for large-scale deployments. Additionally, the optimal architecture may depend on the specific compute scale, what works best at a small scale may not be ideal for large models.

% \subsection{Data Mixing Scaling Laws}
\paragraph{RQ3. What are the optimal data mixture strategies to achieve maximum performance at different scales?}

Recently, techniques like REGMIX~\citep{liu_regmix:_2024} and AUTOSCALE~\citep{kang_autoscale:_2024} have been proposed to optimize data mixtures for various model scales. \citet{ye_data_2024} introduced the following predictive model (Equation \ref{eq:data_mixing}), demonstrating that performance can be estimated using exponential functions that combine proportions from different domains, enabling pre-optimization of training mixtures before full-scale training begins.
\begin{equation}
    L_i(r_{1...M}) = c_i + k_i \exp\left(\sum_{j=1}^M t_{ij}r_j\right),
    \label{eq:data_mixing}
\end{equation}
where \( L_i \) denotes validation loss for domain \( i \), \( r_{1\ldots M} \) represents the proportions of \( M \) domains, and \( c_i, k_i\), and $t_{ij}$ are fitting parameters.

Additionally, domain interactions in continual pre-training have proven to be more complex than previously assumed. The optimal mixture ratio between general and domain-specific data follows predictable scaling patterns, as described by the domain-continual pre-training law~\citep{que_d-cpt_2024} in Equation \ref{eq:d-cpt}.
\begin{equation}
    L(N, D, r) = E + \frac{A}{N^\alpha} + \frac{B \cdot r^\eta}{D^\beta} + \frac{C}{(r + \epsilon)^\gamma},
    \label{eq:d-cpt}
\end{equation}
where \( N \) represents the number of model parameters, \( D \) is the dataset size, \( r \) is the mixture ratio, and \( E, A, B, C, \alpha, \beta, \gamma, \eta, \epsilon \) are fitting parameters.

For practitioners, research suggests adopting a dynamic approach to data mixture optimization that adjusts composition with scale rather than maintaining fixed ratios throughout training. Carefully considering compute budgets and target domains when designing training strategies can significantly improve model performance while reducing computational costs through more efficient data utilization.

% \subsection{Inference Scaling Laws}
\paragraph{RQ4. How does test-time compute scaling compare to model size scaling in terms of efficiency and performance?}

Recent research examining the relationship between test-time computation and model size scaling has revealed key insights. \citet{brown_large_2024} proposed that repeated sampling during inference significantly enhances model performance, with coverage \( C \) (fraction of problems solved) following an exponentiated power law relationship with the number of samples \( k \), as described by Equation \ref{eq:coverage}.
\begin{equation}
    \log(C) = ak^{-b},
    \label{eq:coverage}
\end{equation}
where \(a,b\) are fitting parameters.

Further exploration by \citet{wu_inference_2024} suggested that employing sophisticated test-time computation strategies (such as iterative refinement or tree search) with smaller models may be more cost-effective than using larger models with simple inference methods. Their work establishes a relationship between inference computational budget and optimal model size for compute-efficient inference, expressed as:
\begin{equation}
    \log_{10}(C) = 1.19\log_{10}(N) + 2.03.
\end{equation}
These findings indicate that hybrid approaches combining model size optimization with test-time computation strategies may offer the best trade-off for organizations deploying language models. Practitioners should assess the complexity of their target tasks and consider adaptive computation strategies that adjust inference resources based on problem difficulty. This approach can lead to significant cost savings while maintaining performance, particularly for routine tasks that do not require the full capacity of larger models.


% \subsection{Transfer Learning Scaling Laws}
\paragraph{RQ5. How does scaling fine-tuning parameters affect performance on downstream tasks? } \citet{hernandez_scaling_2021} proposed scaling laws for transfer by fine-tuning decoder-only transformer models on python code. They introduced a concept of effective data transferred $D_t$, i.e., the amount of additional python data that a model of the same size trained on only python would have needed to achieve the same loss on python as a model pre-trained on language, as a function of fine tuning data $D_f$. The law is given as :
\begin{equation}
    D_t(D_f,N) = k(D_f)^\alpha(N)^\beta.
\end{equation}
Along the same tracks, \citet{lin_selecting_2024} proposed a rectified scaling law given by Equation \ref{eq:rectified}. They introduced the term pre-learned data size $D_l$ that indicates how much amount of downstream data a model has learned from pre-training: 
\begin{equation}
    L(D) = \frac{B}{D_t + D^\beta} + E,
    \label{eq:rectified}
\end{equation}
where $D$ is the fine-tuning data size and $B,E,\beta$ are fitting parameters.

\citet{abnar_exploring_2021} predicted, downstream error $e_{DS}$ for image recognitions tasks on ViTs and ResNets as a function of upstream error $e_{US}$, given by the equation:
\begin{equation}
    e_{DS} = k(e_{US})^a + c.
\end{equation}
This was further explored by \citet{mikami_scaling_2021}, modelling pre-training data size, consisting of syntheic dataset to predict downstream error with following equation:
\begin{equation}
    e_{DS} = aD^{-\alpha} + c
\end{equation}
\citet{chen_scaling_2024} proposed a framework for predicting downstream performance, called FLP method which first, maps FLOPS to training loss and then maps the pre-training loss to downstream task performance. \citet{zhang_when_2024} studied scaling effects under parameter-efficient fine-tuning (PEFT and LoRA). They found that LoRA and prompt tuning~\citep{lester2021power} scale poorly when PEFT parameters are increased, although LoRA has superior training stability. LLM fine-tuning scaling is task- and data-dependent, making the choice of the best fine-tuning approach for a downstream task non-trivial. They proposed a multiplicative joint scaling law:
\begin{equation}
    \hat{L}(X, D_f) = A \times \frac{1}{X^\alpha} \times \frac{1}{D_f^\beta} + E,
\end{equation}
where $D_f$ denotes fine-tuning data, $X$ denotes either LLM model size or pretraining data size or PEFT parameter size, and $A,E,\alpha,\beta$ are fitting parameters.


% \subsection{Efficient LM Scaling Laws}
\paragraph{RQ5. How do sparse model scaling laws (e.g., Mixture-of-Experts, Pruning) compare to dense model scaling laws in terms of efficiency and performance?}

Recent research has revealed important insights into how sparse models scale compared to dense models. \citet{frantar_scaling_2023} demonstrated that sparsity acts as a multiplicative constant to model size scaling rather than fundamentally altering the scaling behavior (Equation \ref{eq:sparse_law}). This implies that while sparse models can be more efficient, they follow similar underlying trends as dense models.
\begin{align}
L(S,N,D)&=(a_S(1-S)^{b_S} + c_S) \cdot \left(\frac{1}{N}\right)^{b_N} \notag\\
&+ \left(\frac{a_D}{D}\right)^{b_D} + c,
\label{eq:sparse_law}
\end{align}
% \noindent
where \( L \) represents the validation loss, \( S \) represents sparsity, \( N \) is the number of non-zero parameters, \( D \) is the training dataset size, and \( a_S, b_S, c_S, b_N, a_D, b_D, c \) are fitting parameters.

For Mixture-of-Experts (MoE) models, \citet{clark_unified_2022} proposed the following law for a constant dataset size:
\begin{equation}
    \log(L) = a\log N + b\log E + c\log N\cdot\log E + d\ ,
    \label{eq:clark_moe}
\end{equation}
where \( L \) is the final loss, \( N \) represents the number of parameters an input encounters, and \( E \) is the expansion rate, \(a,b,c,d\) are fitting parameters. The scalability of MoE models is constrained by the need to balance training samples with computational resources for optimal utilization. As noted by \citet{kaplan_scaling_2020} and \citet{hoffmann_training_2022}, maintaining a constant dataset size while increasing model size can lead to undertraining, limiting model performance.

To address this, \citet{yun_toward_2024} extended Equation \ref{eq:clark_moe} to incorporate dataset size:
\begin{multline}
    \log L(N,D,E) \triangleq \\ \log\left(\frac{a}{N^\alpha} + \frac{b}{E^\beta} + \frac{c}{D^\gamma} + f\right) + d\log N\log E
\end{multline}
where \(a,b,c,d,f,\alpha,\beta,\gamma\) are fitting parameters.

Furthermore, \citet{krajewski_scaling_2024} introduced a granularity parameter (\( G \)) to refine Equation \ref{eq:chinchilla} for MoE models. Their research demonstrates that well-configured MoE models, with optimal granularity and training parameters, are consistently more efficient than dense Transformers across all scales:
\begin{equation}
    \mathcal{L}(N,D,G) = c + \left(\frac{g}{G^\gamma} + a\right)\frac{1}{N^\alpha} + \frac{b}{D^\beta}.
    \label{eq:granularity}
\end{equation}
where \(a,b,c,g,\alpha,\beta,\gamma\) are fitting parameters.

Additionally, \citet{chen2024scaling} introduced the \( P^2 \) Law (Equation \ref{eq:p2_law}) for understanding the performance of pruned models post-training. Their findings suggest that post-training loss after pruning is influenced by four key factors: the pre-pruning model size, the number of post-training tokens, the pruning rate, and the model's pre-pruning loss. This provides a structured approach for optimizing sparse model training.
\begin{multline}
L(N_0, D, \rho, L_0) = \\ L_0 + \left( \frac{1}{\rho} \right)^\gamma \left( \frac{1}{N_0} \right)^\delta \left( \frac{N_C}{N_0^\alpha} + \frac{D_C}{D^\beta} + E \right),
\label{eq:p2_law}    
\end{multline}
where \( L_0 \) is the uncompressed model loss, \( \rho \) is the pruning ratio, \( N_0 \) is the pre-pruning model size, \( D \) represents the number of post-training tokens, and \( N_C, D_C, E, \alpha, \beta, \gamma \) are fitting parameters.

These findings collectively suggest that properly implemented sparse architectures could provide a more compute-efficient pathway for scaling language models. This is particularly relevant as the field moves toward sustainable AI practices. The key to success lies in careful optimization -- whether through precise expert size configuration in MoE models or strategic post-training approaches for pruned models.






% \textbf{1. What is the optimal number of experts in MoE models for achieving the best trade-off between inference efficiency and accuracy?}

% Based on the research literature, the optimal number of experts in Mixture of Experts (MoE) models represents a careful balance between model performance and practical constraints. According to \citet{yun_toward_2024}, models with 4-8 experts demonstrate superior inference efficiency, but with a trade-off, as these models require 2.5-3.5x more training resources to achieve performance parity with models using more experts. The research suggests an "over-training" strategy using smaller models with more experts (16/32) and increased training data. This approach represents a middle ground that balances the theoretical benefits of more experts with the practical constraints of deployment and inference. 

% % \citet{krajewski_scaling_2024}introduces the concept of "granularity" in MoE architecture, suggesting that the traditional approach of making experts the same size as feed-forward layers is suboptimal. Their research demonstrates that MoE models consistently outperform dense Transformers across all scales when properly optimized, with the efficiency gap actually widening as compute budget increases.

% For practitioners entering the field, these findings suggest starting with a moderate number of experts (4-8) if inference efficiency is a primary concern. However, if training resources are available and the focus is on optimizing for both performance and inference costs, consider implementing the over-training strategy with 16-32 experts. The key is to recognize that the optimal configuration depends heavily on your specific use case, available compute resources, and whether training or inference costs are your primary constraint. Future research may continue to refine these recommendations as new architectures and training strategies emerge.

% \textbf{2. What are the limits of post-training scaling laws after model pruning, and how can post-training be optimized for sparsely connected models?}

% Recent research has shed light on the relationship between model pruning and post-training performance. According to the "P2 Law: Scaling Law for Post-Training After Model Pruning," researchers discovered that post-training loss after model pruning is influenced by four key factors: the model size before pruning, the number of post-training tokens, the pruning rate, and the model's loss before pruning. This finding was validated across multiple model series including Llama-3 and Qwen-2.5, and different pruning methods such as depth pruning, width pruning, and 2:4 semi-structured pruning. Furthermore, research on "Scaling Laws for Sparsely-Connected Foundation Models" demonstrated that sparsity acts as a multiplicative constant to model size scaling, rather than fundamentally changing the scaling behavior. The study found that 75\% sparsity provides approximately 2.15x effective parameter count compared to dense models.

% Complementing these findings, "The Cost of Down-Scaling Language Models" revealed that moderate pruning (>30\% of weights removed) significantly impairs language models' ability to recall facts learned during pre-training, while in-context learning capabilities remain largely intact even with aggressive pruning (60-70\% of weights removed). This pattern holds true for both pruning and "dense scaling" approaches across different model families like OPT and LLaMA.

% These findings suggest several practical implications for optimizing post-training in sparse models. First, practitioners should consider that it's not always necessary to use massive amounts of data for post-training, as there's an optimal point where additional training data provides diminishing returns. The P2 Law can effectively predict this optimal point, helping researchers make efficient decisions about post-training resource allocation. Additionally, when working with sparse models, focusing on hardware-friendly sparsity patterns (like n:m sparsity) can maintain similar scaling trends while providing practical benefits. For tasks primarily requiring in-context learning abilities rather than fact recall, more aggressive pruning can be applied without significant performance degradation, potentially leading to more efficient deployment scenarios.

% These insights particularly benefit practitioners new to the field by providing clear guidelines for post-training optimization and pruning strategies. Memory augmentation (providing facts in context) could be an effective strategy for pruned models, especially when fact recall is important. Moreover, the findings suggest that pruning pre-trained models is more compute-efficient than training sparse models from scratch if dense checkpoints already exist, offering a practical pathway for model optimization.

% \subsection{Model Compression Scaling Laws}
\paragraph{RQ7. What are the optimal quantization strategies to maintain model performance while reducing inference cost?}

According to~\citet{dettmers_case_2023}, 4-bit precision appears to be the optimal sweet spot for maximizing model performance while minimizing model size. Additionally, research on scaling with mixed quantization~\citep{cao_scaling_2024}, demonstrated that larger models can handle higher quantization ratios while maintaining performance, following an exponential relationship where larger models require exponentially fewer high-precision components to maintain a given performance level.

\citet{kumar_scaling_2024} developed a unified scaling law (Equation \ref{eq:unified_scaling_quant}) that predicts both training and post-training quantization effects. It further suggests that effects of quantizing weights, activations, and attention during training are independent and multiplicative. 
\begin{multline}
    L(N, D, P_w, P_a, P_{\text{kv}}, P_{\text{post}}) =\\  AN_{\text{eff}}^{-\alpha} + BD^{-\beta} + E + \delta_{PTQ},
\label{eq:unified_scaling_quant}
\end{multline}
where $P_w,P_a,P_{kv}$ denote training precision of weights, activations and attentions, respectively, $P_{post}$ denote end-time weight-precision, $\delta_{PTQ}$ denotes loss due to post training quantization, and \(\alpha,\beta\) are fitting parameters.

These findings suggest opportunities for developing specialized architectures optimized for ultra-low precision at scale. The discovery that quantization effects are independent across model components points to potential benefits of dynamic precision adjustment based on computational needs. Combined with insights about the relationship between training precision and compute budgets, this could lead to more efficient training approaches and architectures inherently designed for robust quantization performance.

% \textbf{4. Can retrieval-based models scale more efficiently than fully parameterized models, and what are the optimal datastore sizes for different model sizes?}

% Based on recent research, retrieval-based language models present a compelling alternative to fully parameterized models for efficient scaling. According to \citet{shao_scaling_2024} increasing datastore sizes consistently improves both language modeling performance and downstream tasks without obvious saturation points. A key finding demonstrates that smaller models augmented with large datastores can outperform larger baseline models on knowledge-intensive tasks – for instance, a LLAMA-2 7B model with retrieval capabilities can surpass the performance of a LLAMA-2 13B baseline model on certain tasks.

% This aligns with broader scaling research findings from \citet{kaplan_scaling_2020}, which showed that model performance typically follows power-law relationships with model size and compute. However, retrieval-based approaches offer a potentially more efficient path to scaling, as storing knowledge in a datastore proves more computationally efficient than encoding it directly into model parameters during training.

% For practitioners entering the field, these findings suggest several practical implications. First, when working with constrained compute budgets, implementing retrieval-augmented architectures may provide better performance-per-compute than simply increasing model size. This is particularly relevant for applications requiring extensive knowledge access, where retrieval can effectively supplement a smaller model's capabilities.

% The research also indicates that optimal datastore sizing should be approached incrementally, as benefits persist across different model architectures (tested with LLAMA, Pythia, and OLMO models). Rather than immediately implementing massive datastores, practitioners might start with moderate sizes and scale up based on performance requirements, since the relationship between datastore size and performance improvement appears to be relatively consistent and predictable. This allows for more efficient resource allocation while maintaining the flexibility to scale as needed.


% \textbf{3.Are there alternative scaling strategies (e.g., data pruning, optimal token selection) that can outperform traditional power-law scaling?}
% Recent research has revealed several promising alternatives to traditional power-law scaling in language models. A groundbreaking study by \citet{sorscher_beyond_2023} demonstrated that strategic data pruning can achieve exponential rather than power-law scaling of model performance with dataset size. The key insight is that not all training examples contribute equally to model learning, and by carefully selecting which examples to keep and discard, similar or better performance can be achieved with significantly less data. This is complemented by findings from \citet{hu_minicpm:_2024} which showed that small language models (1.2B and 2.4B parameters) can achieve performance comparable to 7B-13B models through careful training strategies and their novel Warmup-Stable-Decay learning rate scheduler.

% Furthermore, \citet{que_d-cpt_2024} introduced an approach to optimize the mixture ratios between general and domain-specific data during continual pre-training. Their research revealed that domain-specific pre-training follows predictable scaling laws, and proper optimization of mixture ratios can lead to significant improvements without requiring expensive grid searches. This is particularly relevant when considering "Data Mixing Laws"~\citep{ye_data_2024} which discovered quantitative relationships that can predict language model performance based on training data mixture proportions.

% Looking forward, these alternative scaling strategies suggest a shift in the field from pure scaling to more nuanced approaches combining multiple optimization techniques. As evidenced by \citet{zhang_when_2024}, successful scaling strategies need to consider the interplay between model size, training data, and fine-tuning methods. This suggests that newcomers to the field should focus on understanding these relationships and trade-offs rather than simply pursuing larger models or datasets.

% \subsection{Multimodal Model Scaling Laws}
\paragraph{RQ8. Do scaling laws hold in multimodal models, and how does modality interaction affect performance at scale?}

\citet{henighan_scaling_2020} first explored the scaling laws for multimodals, proposing the form $\mathcal{M}_2$ : 
\begin{equation}
    L(x) = Ax^{-\alpha} + B,
\end{equation} 
where $x$ can be either model size, data or compute. This was further improved by \citet{alabdulmohsin_revisiting_2022} by proposing the form $\mathcal{M}_4$ :
\begin{equation}
    \frac{L_x - L_\infty}{(L_0 - L_x)^\alpha} = \beta x^c
\end{equation}
While $\mathcal{M}_2$ assumes that model performance follows a simple power law relationship (meaning performance improves at a constant rate as you add more training data), real models often don't behave so cleanly. $\mathcal{M}_4$ introduces additional flexibility through a sigmoid-like shape that can smoothly transition between different phases of learning.

Further research has revealed fascinating dynamics in how different modalities interact as models grow larger. According to \citet{aghajanyan_scaling_2023}, there exists an inherent competition between modalities in smaller models, which can transform into synergy as model size increases after crossing ``competition barrier''. They used Chinchilla form (Equation \ref{eq:chinchilla}) as a uni-modal scaling law, and improvised on it to propose a bi-modal scaling law:
\begin{align}
\mathcal{L}(N, D_i, D_j) &= \left[\frac{\mathcal{L}(N, D_i) + \mathcal{L}(N, D_j)}{2}\right] \notag\\ & - C_{i,j} + \frac{A_{i,j}}{N^{\alpha_{i,j}}} + \frac{B_{i,j}}{|D_i| + |D_j|^{\beta_{i,j}}},
\label{eq:multi_modal}
\end{align}
where $L$ denotes loss, N denotes model size, $D_i$ and $D_j$ denote two datasets being used and $C_{i,j}$ represent the maximum level of synergy. 

The findings indicate that for multimodal models, practitioners should ensure sufficient model size to enable synergistic rather than competitive interaction between modalities, while emphasizing that scaling strategies should prioritize data quality and modality alignment over mere size increases. High-quality image-text pairs and thoughtful consideration of inter-modal relationships in architecture design are more crucial than simply scaling up individual components.

\end{document}
