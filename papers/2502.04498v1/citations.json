[
  {
    "index": 0,
    "papers": [
      {
        "key": "ouyang2022training",
        "author": "Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "key": "bang2023multitask",
        "author": "Bang, Yejin and Cahyawijaya, Samuel and Lee, Nayeon and Dai, Wenliang and Su, Dan and Wilie, Bryan and Lovenia, Holy and Ji, Ziwei and Yu, Tiezheng and Chung, Willy and others",
        "title": "A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity"
      },
      {
        "key": "wang2023pandalm",
        "author": "Wang, Yidong and Yu, Zhuohao and Zeng, Zhengran and Yang, Linyi and Wang, Cunxiang and Chen, Hao and Jiang, Chaoya and Xie, Rui and Wang, Jindong and Xie, Xing and others",
        "title": "Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization"
      },
      {
        "key": "xu2023wizardlm",
        "author": "Xu, Can and Sun, Qingfeng and Zheng, Kai and Geng, Xiubo and Zhao, Pu and Feng, Jiazhan and Tao, Chongyang and Lin, Qingwei and Jiang, Daxin",
        "title": "WizardLM: Empowering large pre-trained language models to follow complex instructions"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "lin-chen-2023-llm",
        "author": "Lin, Yen-Ting  and\nChen, Yun-Nung",
        "title": "{LLM}-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models"
      },
      {
        "key": "liu2303g",
        "author": "Liu, Yang and Iter, Dan and Xu, Yichong and Wang, Shuohang and Xu, Ruochen and Zhu, Chenguang",
        "title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment (2023)"
      },
      {
        "key": "qin2024infobench",
        "author": "Yiwei Qin and Kaiqiang Song and Yebowen Hu and Wenlin Yao and Sangwoo Cho and Xiaoyang Wang and Xuansheng Wu and Fei Liu and Pengfei Liu and Dong Yu",
        "title": "InFoBench: Evaluating Instruction Following Ability in Large Language Models"
      },
      {
        "key": "he2024complex",
        "author": "He, Qianyu and Zeng, Jie and He, Qianxi and Liang, Jiaqing and Xiao, Yanghua",
        "title": "From Complex to Simple: Enhancing Multi-Constraint Complex Instruction Following Ability of Large Language Models"
      },
      {
        "key": "sun2024conifer",
        "author": "Haoran Sun and Lixin Liu and Junjie Li and Fengyu Wang and Baohua Dong and Ran Lin and Ruohui Huang",
        "title": "Conifer: Improving Complex Constrained Instruction-Following Ability of Large Language Models"
      },
      {
        "key": "xia2024fofo",
        "author": "Xia, Congying and Xing, Chen and Du, Jiangshu and Yang, Xinyi and Feng, Yihao and Xu, Ran and Yin, Wenpeng and Xiong, Caiming",
        "title": "FOFO: A Benchmark to Evaluate LLMs' Format-Following Capability"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "jain2023bring",
        "author": "Jain, Neel and Saifullah, Khalid and Wen, Yuxin and Kirchenbauer, John and Shu, Manli and Saha, Aniruddha and Goldblum, Micah and Geiping, Jonas and Goldstein, Tom",
        "title": "Bring your own data! self-supervised evaluation for large language models"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "ifeval",
        "author": "Zhou, Jeffrey and Lu, Tianjian and Mishra, Swaroop and Brahma, Siddhartha and Basu, Sujoy and Luan, Yi and Zhou, Denny and Hou, Le",
        "title": "Instruction-{Following} {Evaluation} for {Large} {Language} {Models}"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "jiang2023followbench",
        "author": "Jiang, Yuxin and Wang, Yufei and Zeng, Xingshan and Zhong, Wanjun and Li, Liangyou and Mi, Fei and Shang, Lifeng and Jiang, Xin and Liu, Qun and Wang, Wei",
        "title": "Followbench: A multi-level fine-grained constraints following benchmark for large language models"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "ouyang2022training",
        "author": "Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others",
        "title": "Training language models to follow instructions with human feedback"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "brown2020language",
        "author": "Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario",
        "title": "Language {Models} are {Few}-{Shot} {Learners}."
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "alpaca",
        "author": "Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto ",
        "title": "Stanford Alpaca: An Instruction-following LLaMA model"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "vicuna2023",
        "author": "Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.",
        "title": "Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\\%* ChatGPT Quality"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "touvron2023llama",
        "author": "Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others",
        "title": "Llama 2: Open foundation and fine-tuned chat models"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "xu2023wizardlm",
        "author": "Xu, Can and Sun, Qingfeng and Zheng, Kai and Geng, Xiubo and Zhao, Pu and Feng, Jiazhan and Tao, Chongyang and Lin, Qingwei and Jiang, Daxin",
        "title": "WizardLM: Empowering large pre-trained language models to follow complex instructions"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "rafailov2024direct",
        "author": "Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea",
        "title": "Direct preference optimization: Your language model is secretly a reward model"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "jiang2023preference",
        "author": "Jiang, Zaifan and Huang, Xing and Wei, Chao",
        "title": "Preference as Reward, Maximum Preference Optimization with Importance Sampling"
      },
      {
        "key": "ethayarajh2024kto",
        "author": "Ethayarajh, Kawin and Xu, Winnie and Muennighoff, Niklas and Jurafsky, Dan and Kiela, Douwe",
        "title": "Kto: Model alignment as prospect theoretic optimization"
      },
      {
        "key": "hong2024orpo",
        "author": "Hong, Jiwoo and Lee, Noah and Thorne, James",
        "title": "Orpo: Monolithic preference optimization without reference model"
      },
      {
        "key": "meng2024simpo",
        "author": "Meng, Yu and Xia, Mengzhou and Chen, Danqi",
        "title": "SimPO: Simple Preference Optimization with a Reference-Free Reward"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "openai2023structured",
        "author": "{OpenAI}",
        "title": "Introducing Structured Outputs in the API"
      },
      {
        "key": "outlines2023",
        "author": "{Outlines Development Team}",
        "title": "Outlines - GitHub Repository"
      },
      {
        "key": "lmql2023",
        "author": "{ETH SRI}",
        "title": "LMQL - GitHub Repository"
      },
      {
        "key": "jsonformer2023",
        "author": "{1rgs}",
        "title": "jsonformer - GitHub Repository"
      },
      {
        "key": "dong2024xgrammar",
        "author": "Dong, Yixin and Ruan, Charlie F and Cai, Yaxing and Lai, Ruihang and Xu, Ziyi and Zhao, Yilong and Chen, Tianqi",
        "title": "Xgrammar: Flexible and efficient structured generation engine for large language models"
      }
    ]
  }
]