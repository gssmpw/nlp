\section{Related Work}
\subsection{Evaluating Instruction Following} 

To evaluate the instruction following capabilities of large language models (LLMs), three primary methods are commonly employed: human evaluation, LLM-based evaluation, and evaluation through verifiable instructions (i.e., automatic evaluation). While human evaluation can be accurate, it suffers from subjective bias, high costs, and a lack of reproducibility~\citep{ouyang2022training, bang2023multitask, wang2023pandalm, xu2023wizardlm}. 
LLM-based evaluation methods offer more scalable and robust alternatives for assessing instruction following performance~\citep{lin-chen-2023-llm, liu2303g, qin2024infobench, he2024complex, sun2024conifer, xia2024fofo}. 
For automatic evaluations, \citet{jain2023bring} analyze the sensitivity of the slight changes in LLM outputs as a means of measuring their reliability, while \citet{ifeval} introduce the IFEval benchmark, which directly verifies LLMs' instruction-following abilities through simple code execution. Similarly, FollowBench~\citep{jiang2023followbench} proposes a benchmark that integrates LLMs with Python scripts to act as an evaluator.


First, this paper focuses on evaluating the format following ability of LLMs instead of verifying the following of the instruction. For example, checking the hallucinations of the response content is beyond our research focus.
Next, to effectively evaluate the format following ability, we propose \dataset dataset which uses automatic evaluation through Python functions.
Compared to human or LLM-based evaluations, our method can provide a more reliable and efficient way to examine whether LLMs adhere to specific format controls.
In addition to existing verifiable benchmarks, our dataset covers more types of format constraints such as JSON output. Further, we propose a training pipeline to enhance LLM's format control ability with such verifiable feature. 

\subsection{Enhancing Instruction Following}
InstructGPT~\citep{ouyang2022training}  proposes RLHF to train the GPT-3 model~\citep{brown2020language} to follow human instructions.
Subsequent research has focused on developing open-domain general instruction following datasets including Alpaca~\citep{alpaca} and Vicuna~\citep{vicuna2023}, both of which played a key role in enabling LLaMA~\citep{touvron2023llama} to follow instructions.
Additionally, WizardLM~\citep{xu2023wizardlm} utilizes AI-generated data for instruction fine-tuning, offering control over the complexity and difficulty of the instructions.
Following the introduction of Direct Preference Optimization (DPO)~\citep{rafailov2024direct}, a number of recent works~\citep{jiang2023preference, ethayarajh2024kto, hong2024orpo, meng2024simpo} have proposed preference learning algorithms aimed at enhancing the instruction following and alignment performance of LLMs.
Another line of work explores constrained decoding methods to enhance the structured generation performance~\citep{openai2023structured,outlines2023,lmql2023,jsonformer2023,dong2024xgrammar}. However, the range of supported structured formats may be limited.

In this paper, our goal is to improve the format following ability of widely used 7B-level LLMs, rather than focusing on general instruction following. 
By leveraging self-generated training data, we propose a progressive training approach that iteratively trains small LLMs to follow format constraints of varying difficulty levels. 
An additional benefit of this method is its scalability, as the training data can be easily synthesized due to the verifiable nature of our pre-curated \dataset dataset.