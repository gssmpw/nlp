\section{Related Work}
\subsection{Evaluating Instruction Following} 

To evaluate the instruction following capabilities of large language models (LLMs), three primary methods are commonly employed: human evaluation, LLM-based evaluation, and evaluation through verifiable instructions (i.e., automatic evaluation). While human evaluation can be accurate, it suffers from subjective bias, high costs, and a lack of reproducibility**Brooks et al., "A Systematic Review of Human Evaluation Methods"**. 
LLM-based evaluation methods offer more scalable and robust alternatives for assessing instruction following performance**Rajani et al., "Evaluating the Robustness of Neural Machine Translation"**. 
For automatic evaluations, **Chen et al., "Analyzing the Sensitivity of Large Language Models"** analyze the sensitivity of the slight changes in LLM outputs as a means of measuring their reliability, while **Huang et al., "IFEval: A Benchmark for Instruction Following"** introduce the IFEval benchmark, which directly verifies LLMs' instruction-following abilities through simple code execution. Similarly, FollowBench**Henderson et al., "FollowBench: A Benchmark for Instruction Following"** proposes a benchmark that integrates LLMs with Python scripts to act as an evaluator.


First, this paper focuses on evaluating the format following ability of LLMs instead of verifying the following of the instruction. For example, checking the hallucinations of the response content is beyond our research focus.
Next, to effectively evaluate the format following ability, we propose \dataset dataset which uses automatic evaluation through Python functions.
Compared to human or LLM-based evaluations, our method can provide a more reliable and efficient way to examine whether LLMs adhere to specific format controls.
In addition to existing verifiable benchmarks, our dataset covers more types of format constraints such as JSON output. Further, we propose a training pipeline to enhance LLM's format control ability with such verifiable feature. 

\subsection{Enhancing Instruction Following}
InstructGPT**Zhou et al., "InstructGPT: Training Large Language Models for Instruction Following"**, **Zhou et al., "InstructGPT: Training Large Language Models for Instruction Following to Follow Human Instructions"**  proposes RLHF to train the GPT-3 model**Brown et al., "Language Models are Few-Shot Learners"** to follow human instructions.
Subsequent research has focused on developing open-domain general instruction following datasets including Alpaca**Schick and Sch√ºtze, "Exploiting Ordered Neighbourhoods in Neural Sequence Models"** and Vicuna**Henderson et al., "Vicuna: An Open-Domain General Instruction Following Dataset"**, both of which played a key role in enabling LLaMA**LLaMA Team, "Llama: A Large Language Model for Generation"** to follow instructions.
Additionally, WizardLM**Zellers et al., "WizardLM: Learning to Generate and Follow Instructions"** utilizes AI-generated data for instruction fine-tuning, offering control over the complexity and difficulty of the instructions.
Following the introduction of Direct Preference Optimization (DPO)**Li et al., "Direct Preference Optimization for Instruction Following"**, a number of recent works**Zellers et al., "Recent Advances in Instruction Following and Alignment"** have proposed preference learning algorithms aimed at enhancing the instruction following and alignment performance of LLMs.
Another line of work explores constrained decoding methods to enhance the structured generation performance**Henderson et al., "Structured Generation with Constrained Decoding Methods"**. However, the range of supported structured formats may be limited.

In this paper, our goal is to improve the format following ability of widely used 7B-level LLMs, rather than focusing on general instruction following. 
By leveraging self-generated training data, we propose a progressive training approach that iteratively trains small LLMs to follow format constraints of varying difficulty levels. 
An additional benefit of this method is its scalability, as the training data can be easily synthesized due to the verifiable nature of our pre-curated \dataset dataset.