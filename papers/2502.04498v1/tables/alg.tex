\begin{algorithm}[t!]
    \small
    \centering
    \caption{Enhancing Format Control.}
    \begin{algorithmic}[1]
    \Require Pre-trained model $M$, format following dataset $\mathcal{D}_{V}$, verifiable functions $\{F_k(\ast)\}$
    \State \textbf{Stage 1: Response Sampling}
    \For{each instruction $x \in \mathcal{D}_{V}$}
        \State Sample $4$ responses $\{y_i\}$ from $M$ using $x$
    \EndFor
    \State
    \State \textbf{Stage 2: Training Data Annotation}
    \For{each response $y_i$}
        \State Compute $I(y_i) = \prod_{k=1}^{c} F_k(y_i)$
        \If{$I(y_i) = 1$}
            \State Mark $y_i$ as preferred response $y_w$
        \Else
            \State Mark $y_i$ as dis-preferred response $y_l$
        \EndIf
    \EndFor
    \State Obtained the training data $\mathcal{D}=\{x, y_w, y_l\}$
    \State
    \State \textbf{Stage 3: Progressive Training}
    \For{difficulty level $c$ from $1$ to $3$}
            \State Fine-tune model $M$ on $\mathcal{D}$ via Eq.~\ref{eq:sft}
            \State Fine-tune model $M$ on $\mathcal{D}$ via Eq.~\ref{eq:dpo} 
            \State Re-sample and re-annotate with fine-tuned model $M$, updating $\mathcal{D}$
    \EndFor
    \end{algorithmic}
    \label{alg:enhance}
\end{algorithm}
