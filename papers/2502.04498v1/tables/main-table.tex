

\begin{table*}[t]
\centering
\small
\resizebox{0.97\linewidth}{!}{
\begin{tabular}{l|lll|ll|lll} 

\toprule


\multirow{2}{*}{Model} & \multicolumn{3}{c|}{\dataset}  & \multicolumn{2}{c|}{IFEval} & \multicolumn{3}{c}{InfoBench}                  \\
                       & level-1 & level-2 & level-3  & Prompt & Instruction & \multicolumn{1}{l}{Easy} & Hard  & Avg.  \\ 
\midrule
GPT-3.5                & 62.93   & 34.07   & 16.40   & 56.56        & 67.51        &  -                        &  -  & 86.71*       \\
GPT-4-turbo            & 76.29   & 53.33   & 35.31    & 79.71        & 85.67        &                        -  &    -   & 89.42*      \\
\midrule
LLaMA-2-13B            & 48.08   & 21.40   & 9.65     & 33.00        & 44.24        & 80.40                    & 77.10 & 78.12      \\
LLaMA-2-70B            & 55.57   & 26.47   & 11.89    & 44.36       &54.43               & 81.28                     &79.87     &80.30      \\
LLaMA-3-70B            & 65.63   & 40.02   & 23.55   &77.81      &84.30           & 86.07                     &86.80     &86.61          \\
Qwen-1.5-7B            & 58.66   & 29.49   & 13.87   & 39.00       & 50.96        & 77.82                         &75.13    &75.95      \\
WizardLM-7B            & 55.37   & 28.63   & 14.00    & 43.25        & 55.63      & 80.58                    & 77.70 & 78.58        \\
\midrule
\midrule

Mistral-7B             & 52.18   & 22.82   & 9.49     & \textbf{40.85}        & 50.84     & \textbf{76.67}                    & \textbf{71.15} & \textbf{72.84}         \\
Mistral-7B (ours)       & \textbf{61.85}   & \textbf{32.66}   & \textbf{15.97}      & 37.50        & \textbf{51.19} &72.17                      &68.25      &70.48              \\
\midrule
LLaMA-2-7B             & 50.91   & 22.01   & 9.31    & 31.42        & 44.96          & 18.43                    & 10.57 & 12.98     \\
LLaMA-2-7B (ours)        & \textbf{57.59}   &\textbf{27.72}   & \textbf{13.28}    & \textbf{40.48}        & \textbf{54.08}          & \textbf{73.00}                    & \textbf{68.08} & \textbf{69.59}    \\
\midrule
LLaMA-3-8B             & 60.36   & 31.86   & 15.81    & 68.22        & 77.14         & \textbf{81.88}                    & \textbf{83.72} & \textbf{83.10}     \\
LLaMA-3-8B (ours)       & \textbf{85.56}   & \textbf{59.67}   & \textbf{38.36}   &\textbf{68.50}        & \textbf{77.24}  & 79.10                    & 76.50 & 78.13             \\

\bottomrule
\end{tabular}
}



\caption{Results of various LLMs on three benchmarks. The best performance is highlighted in bold. Results with are from~\citet{sun2024conifer}. The strict mode is adopted for IFEval benchmark. IFEval and InfoBench are testing the performance of out-of-domain format following and general instruction following, respectively.}
\label{tab:main_table}
\end{table*}

