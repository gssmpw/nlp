% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.




% ---------------------------------------------------------------------------- %
%                                custom package                                %
% ---------------------------------------------------------------------------- %
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{tabularx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{rotating}
\usepackage{array}
\usepackage{xspace}
\usepackage{bbm}
\usepackage{scalerel}
\usepackage{microtype}
\usepackage{color}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{bbding}
\usepackage{makecell}
\usepackage{amsmath}
\usepackage{natbib}
\usepackage{ifthen}
\usepackage{enumerate}
\usepackage{float} % H option for placing the table or figure
\usepackage{colortbl} % for coloring the row / column of the table
\usepackage{blindtext} % for dummy text
\usepackage{xcolor, soul} % for coloring text
\usepackage[super]{nth} % for 1st, 2nd, 3rd, etc.
\usepackage[capitalize]{cleveref}
\usepackage{bookmark}




\usepackage{tcolorbox}
\usepackage{pifont}       % \ding{xx}
\usepackage{bbding}       % \Checkmark,\XSolid,...
\usepackage{fontawesome}
\usepackage{soul}
\usepackage{multirow}



\title{Verifiable Format Control for Large Language Model Generations}


\makeatletter
\def\@fnsymbol#1{\ensuremath{\ifcase#1\or \dagger\or *\or \ddagger\or
   \mathsection\or \mathparagraph\or \|\or **\or \dagger\dagger
   \or \ddagger\ddagger \else\@ctrerr\fi}}
\makeatother

\def\myand{\end{tabular}\hss\egroup \hfil\hfil\egroup
           \hbox to \linewidth\bgroup\large \hfil\hfil
             \hbox to 0pt\bgroup\hss \begin{tabular}[t]{c}\bf}


\author{
    Zhaoyang Wang\thanks{~Equal contribution.}$\,^{1}$~~~Jinqi Jiang\footnotemark[1]$\,^{1}$~~~Huichi Zhou\footnotemark[1]$\,^{2}$\\
    \textbf{~Wenhao Zheng}$^1$~~~\textbf{Xuchao Zhang}$^3$~~~\textbf{Chetan Bansal}$^3$~~~\textbf{Huaxiu Yao}$^{1}$\\
    $^1$University of North Carolina at Chapel Hill\\
    $^2$Imperial College London\quad$^3$Microsoft Research\\
    {\texttt \{zhaoyang,~huaxiu\}@cs.unc.edu}
}



% ---------------------------------------------------------------------------- %
%                                   variables                                  %
% ---------------------------------------------------------------------------- %

\newcommand{\dataset}{\texttt{VFF}\xspace}


% ---------------------------------------------------------------------------- %




\begin{document}
\maketitle
\begin{abstract}

    Recent Large Language Models (LLMs) have demonstrated satisfying general instruction following ability. However, small LLMs with about 7B parameters still struggle fine-grained format following (e.g., JSON format), which seriously hinder the advancements of their applications. Most existing methods focus on benchmarking general instruction following while overlook how to improve the specific format following ability for small LLMs. Besides, these methods often rely on evaluations based on advanced LLMs (e.g., GPT-4), which can introduce the intrinsic bias of LLMs and be costly due to the API calls. In this paper, we first curate a fully verifiable format following dataset \dataset. In contrast to existing works often adopting external LLMs for instruction-following validations, every sample of \dataset can be easily validated with a Python function. Further, we propose to leverage this verifiable feature to synthesize massive data for progressively training small LLMs, in order to improve their format following abilities. Experimental results highlight the prevalent limitations in the format following capabilities of 7B level open-source LLMs and demonstrate the effectiveness of our method in enhancing this essential ability.



\end{abstract}



\section{Introduction}

Recent advancements in Large Language Models (LLMs) have demonstrated a series of foundation abilities including in-context learning, reasoning, and the essential instruction following ability~\citep{brown2020language,wei2022emergentabilitieslargelanguage,openai2023gpt4,bubeck2023sparks}.
Pre-trained LLMs such as GPT-3~\citep{brown2020language} hardly follow human instructions due to the mismatch issue between their pre-training objectives and human preferences~\citep{zhang2023instruction}. 
To address this issue, a series of works employ instruction tuning to enable LLMs to respond fluently to natural questions~\citep{longpre2023flan,touvron2023llama,xu2023wizardlm}, which effectively align LLMs with human preferences. 
Specifically, these methods may first collect instruction-response pairs from human~\citep{vicuna2023,zhou2023lima,mishra2021cross} or more powerful LLMs~\citep{alpaca,wang-etal-2023-self-instruct,xu2023wizardlm} (e.g., ChatGPT~\citep{ouyang2022training}). Then, these collected data can be used to fine-tune LLMs to follow human desired responses. 
Further, \citet{ouyang2022training} propose Reinforcement Learning from Human Feedback (RLHF) to enhance the alignment of LLMs, improving the helpfulness and harmfulness of the generations~\citep{bai2022training}.
Today's advanced LLMs such as GPT-4~\citep{openai2023gpt4} can follow most human instructions even those with fine-grained format control requirements (e.g., JSON output).
However, more widely used open-source 7B-level LLMs such as Mistral~\citep{jiang2023mistral} and LLaMA series~\citep{llama2,llama3} often struggle with fine-grained format control despite achieving satisfactory results in general instruction following.
In this paper, we focus on enhancing such fine-grained format control ability of small LLMs to benefit LLM-based applications especially for the format-sensitive ones.

\paragraph{Evaluating.} First, we propose to evaluate the fine-grained format control ability of LLMs.
Most existing research~\citep{qin2024infobench,ifeval,jiang2023followbench, yizhi2024cifbench,ma2024activate} in this area proposes general instruction following benchmarks, while paying less attention to specific format control.
Also, in terms of verifying and evaluation, most of them are driven by LLMs based evaluations which heavily rely on the capability of the selected LLMs~\citep{chiang-lee-2023-large,fu2023gptscore,liu2023calibrating,chan2023chateval}. Further, few of them consider improving the format following ability of small LLMs.

To address these challenges, we curate a fully \textbf{V}erifiable \textbf{F}ormat \textbf{F}ollowing (\dataset) dataset. 
This dataset starts with a few GPT-4 annotated meta constraints. And we can use off-the-shelf Alpaca dataset~\citep{alpaca} as the prompt (question) source.
As illustrated in Figure~\ref{fig:data-instance}, the prompt combined with the meta constraints can finally form an instruction with various format controls.
Specifically, each meta constraint consists of several variables and candidate values (which formulates the constraint instance), and a corresponding Python function (which verifies the format following).
Note that the final instantiated  instruction can include multiple different constraints, referred to as multi-level constraints. These multi-level constraints can go up to $3$ levels, considering the needs in realistic scenarios.
We also detail the differences between our \dataset and existing datasets in Table~\ref{tab:benchmark_comparison}.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{images/instance2.pdf}
\caption{An example with level-3 constraints of \dataset.}
\label{fig:data-instance}
\end{figure}

\input{tables/benchmark_comparison}

\paragraph{Enhancing.} After having \dataset to evaluate the format following ability, we then propose to leverage \dataset's easily verifiable feature to improve such important abilities for 7B level LLMs.
Previous methods~\citep{vicuna2023,xu2023wizardlm,wang-etal-2023-self-instruct} often do not specially introduce fine-grained format following data, leading to inferior performance in this area.
Besides, the training data used by these methods are often collected via human sharing like ShareGPT\footnote{\url{https://sharegpt.com/}} or advanced LLMs like GPT-4, which can be costly and time-consuming.
Thanks to the easily verifiable feature of our \dataset, we can synthesize training data to improve the format following ability of LLMs in a self-improvement paradigm.
This paradigm ensures the training data is entirely generated by the LLM itself.
Specifically, this paradigm consisted of three stages: (1) Sampling multiple responses from the LLM for each instruction with constraints from \dataset. (2) Annotating the responses using the verifiable Python function to identify whether they strictly follow the format controls.  (3) Fine-tuning the LLM via supervised fine-tuning and preference learning (e.g., DPO~\citep{rafailov2024direct}) with those annotated responses.
Furthermore, these steps can be repeated in a progressive training manner,  starting by training the LLM to follow a single constraint (level-1) and advancing to adhere to multiple constraints (level-3).

In summary, our contributions are as follows:
\begin{enumerate}[1)]
    \item We curate a fully verifiable format following dataset \dataset, showing that 7B level LLMs hold potential for enhanced format control ability.
    \item With verifiable feature of \dataset, we propose the progressive training to enhance LLMs' format control ability with  self-generated training data.
    \item Empirical results on existing benchmarks with several trained 7B-level LLMs demonstrate the effectiveness of the proposed method.
\end{enumerate}



\section{Related Work}

\subsection{Evaluating Instruction Following} 

To evaluate the instruction following capabilities of large language models (LLMs), three primary methods are commonly employed: human evaluation, LLM-based evaluation, and evaluation through verifiable instructions (i.e., automatic evaluation). While human evaluation can be accurate, it suffers from subjective bias, high costs, and a lack of reproducibility~\citep{ouyang2022training, bang2023multitask, wang2023pandalm, xu2023wizardlm}. 
LLM-based evaluation methods offer more scalable and robust alternatives for assessing instruction following performance~\citep{lin-chen-2023-llm, liu2303g, qin2024infobench, he2024complex, sun2024conifer, xia2024fofo}. 
For automatic evaluations, \citet{jain2023bring} analyze the sensitivity of the slight changes in LLM outputs as a means of measuring their reliability, while \citet{ifeval} introduce the IFEval benchmark, which directly verifies LLMs' instruction-following abilities through simple code execution. Similarly, FollowBench~\citep{jiang2023followbench} proposes a benchmark that integrates LLMs with Python scripts to act as an evaluator.


First, this paper focuses on evaluating the format following ability of LLMs instead of verifying the following of the instruction. For example, checking the hallucinations of the response content is beyond our research focus.
Next, to effectively evaluate the format following ability, we propose \dataset dataset which uses automatic evaluation through Python functions.
Compared to human or LLM-based evaluations, our method can provide a more reliable and efficient way to examine whether LLMs adhere to specific format controls.
In addition to existing verifiable benchmarks, our dataset covers more types of format constraints such as JSON output. Further, we propose a training pipeline to enhance LLM's format control ability with such verifiable feature. 

\subsection{Enhancing Instruction Following}
InstructGPT~\citep{ouyang2022training}  proposes RLHF to train the GPT-3 model~\citep{brown2020language} to follow human instructions.
Subsequent research has focused on developing open-domain general instruction following datasets including Alpaca~\citep{alpaca} and Vicuna~\citep{vicuna2023}, both of which played a key role in enabling LLaMA~\citep{touvron2023llama} to follow instructions.
Additionally, WizardLM~\citep{xu2023wizardlm} utilizes AI-generated data for instruction fine-tuning, offering control over the complexity and difficulty of the instructions.
Following the introduction of Direct Preference Optimization (DPO)~\citep{rafailov2024direct}, a number of recent works~\citep{jiang2023preference, ethayarajh2024kto, hong2024orpo, meng2024simpo} have proposed preference learning algorithms aimed at enhancing the instruction following and alignment performance of LLMs.
Another line of work explores constrained decoding methods to enhance the structured generation performance~\citep{openai2023structured,outlines2023,lmql2023,jsonformer2023,dong2024xgrammar}. However, the range of supported structured formats may be limited.

In this paper, our goal is to improve the format following ability of widely used 7B-level LLMs, rather than focusing on general instruction following. 
By leveraging self-generated training data, we propose a progressive training approach that iteratively trains small LLMs to follow format constraints of varying difficulty levels. 
An additional benefit of this method is its scalability, as the training data can be easily synthesized due to the verifiable nature of our pre-curated \dataset dataset.






\section{Method}

In this section, we first introduce the curation of our verifiable format following dataset \dataset, which is for accurately evaluating the format control ability of LLM generations.
By leveraging the easily verifiable feature of \dataset, we then propose a progressive training manner to  enhance such format control ability of 7B level LLMs.

\subsection{\dataset dataset}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{images/category.pie.pdf}
\caption{Category distribution of meta constraints. The ``specific number format'' often involves constraints on the generated numbers (e.g., time format). The ``limited grammar'' includes constraints for writing styles such as active or passive voice. The ``limited structure'' includes common structure output formats such as JSON, YAML and etc. The ``limited punctuation'' requires the LLMs to use specific punctuations in the generations. The ``limited word count'' directly limits the length of the output from LLMs.  The ``limited content'' constrains generations within specific topics, which can limit the output scope of the response.}
\label{fig:dist-vff}
\end{figure}

\begin{figure*}[t]
\centering
\includegraphics[width=0.98\textwidth]{images/main-fig.pdf}
\caption{The pipeline of the proposed method for enhancing format control ability. First, the model takes response sampling for instantiated instructions from \dataset dataset. Next, in the training data annotation stage, by utilizing the verifiable Python functions, we can collect the responses that satisfy the constraints as the SFT data, while pair with the negative responses to form the DPO training data.
Finally, the LLM is first trained with SFT, followed by direct preference learning (DPO) to improve its format following.
These steps are repeated in a progressive training manner with the increased levels of difficulty, in order to exploit the potential of the LLM.
}
\label{fig:train-pipeline}
\end{figure*}


\paragraph{Verifiable Meta Constraint.} 
We begin with a small set of human-annotated meta constraint pool $\mathcal{D}_{M} = \{(C_k, V_k, F_k) \mid 1 \leq k \leq 16\}$, where $C_k$ represents the $k$-th constraint containing variables, $V_k$ is the set of candidate values that can be selected to fill in the variables, $F_k$ is the corresponding Python bool function that can efficiently and accurately verify whether the generated responses satisfy the format constraints.
For example, $C_k$ is ``Respond in [[VAR1]] language.'', $V_k$ is a set of values ``\{English, Spanish, French, Chinese, Japanese\}'' for ``[[VAR1]]'' to fill in, and $F_k$ is an executable Python bool function for language detection.
We extend this pool $\mathcal{D}_{M}$ by leveraging the in-context learning ability of GPT-4~\citep{openai2023gpt4}. 
In evaluating the format following scenario, the adopted Python function serving as the judgment method is as accurate as human judgment, while being more efficient and time-saving compared to LLM-based evaluations. These advantages make it highly suitable for large-scale verifications. 

Some constraints may seriously conflict with user's instruction, for example, the instruction is writing a long story, while the constraint may limit the number of generated words to $10$.
Thus, we also take manual check for each generated sample. 
Finally, to maximize the applicability and universality of our meta constraints, we only reserve about $60$ unique meta constraints, which are publicly available at \url{https://huggingface.co/datasets/jinqij/VFF}. We visualize the categories of these constraints in Figure~\ref{fig:dist-vff}. These categories can cover common realistic output format needs for LLM based applications.



\paragraph{Format Following Dataset.}
After obtaining the meta constraints, we then need to instantiate the instruction with specific format controls. These instantiated instructions form the format following dataset that can be directly used to evaluate the performance of LLMs.
Specifically, each instruction sample $x$ of \dataset dataset $\mathcal{D}_{V}$ consists of a detailed question coupled with several constraint instances. 
In details, we use the existing Alpaca dataset~\citep{alpaca} as the question source $Q$ which comprises about $52$K questions generated by text-davinci-003~\citep{brown2020language}.
Then, considering the risks in conflicts between questions and constraints, we randomly select up to $3$ ($1 \le c \le 3$) unique meta constraints from $\mathcal{D}_M$, and instantiate them by filing the variables of the constraints $C_k$ with their respective candidate values $V_k$.
The instruction sample $x$ illustrated in Figure~\ref{fig:data-instance} can be obtained by concatenating the question $q$ and the constraint instance $d$, i.e., $x = [q, d]$ where $d$ includes $c$ unique constraint instances, reflecting a $\text{level-}c$ difficulty as categorized in this study.
Simultaneously, the corresponding binary functions $F_{k \le c}(\ast)$ can serve together to verify the correctness of the generated response $y$ with $I = \prod_{k=1}^{c} F_k(y)$, where $I = 1$ represents as the model can fully adhere to this format control instruction $x$. 
Note that the size of $\mathcal{D}_{V}$ can be as large as $|Q| \times (\sum_k^{|\mathcal{D}_M|} |V_k|)$, which is considerably larger than previous benchmarks. This can be viewed as an advantage of our dataset especially in the era of scaling synthetic data.

\input{tables/alg.tex}

\subsection{Enhancing Format Control}
As shown in Figure~\ref{fig:train-pipeline}, the proposed method for enhancing the format control ability of small LLMs mainly consists of three stages: (1) Response Sampling stage, (2) Training Data Annotation stage, and (3) Progressive Training stage.

\paragraph{Response Sampling.}
To collect diverse enough responses, we instruct the LLM to sample multiple responses for the same question with constraints.
Specifically, for each instruction $x \in \mathcal{D}_V$, we sample $k=4$ responses from the LLM. However, given the limitations of 7B-level LLMs, these may all be incorrect. 
To improve sampling efficiency for correct responses, inspired by recent self-improvement studies~\citep{huang-etal-2023-large,wang-etal-2023-democratizing,gou2024critic}, we add one generated wrong response as the one-shot demonstration to help the LLM to generate better response.

\paragraph{Training Data Annotation.}
To identify the correctness of the massively generated responses, we propose to leverage the fully verifiable feature of \dataset dataset.
Specifically, the collected response samples can be efficiently annotated with format following judgments by the verifiable functions $I$, which reduces the costs of calling GPT-4 APIs compared to previous methods.
Through these verifiable functions, we can identify the response that satisfies the format constraints ($I=1$) as preferred response $y_w$, and responses that not following the constraints ($I=0$) as dis-preferred response $y_l$.
Finally, we can use the preferred responses to form training data $\mathcal{D} = \{x, y_w, y_l\}$.
In this dataset, the preferred responses $\{y_w\}$ can be used for SFT training, while the preference pairs $\{y_w, y_l\}$ can be used for DPO training. 
Note that this training data is fully generated and annotated by LLM itself, without any needs for human or external LLMs.



\paragraph{Progressive Training.}
The small LLM's fine-grained format following ability can be enhanced by aligning it with human desired output format.
Specifically, we first apply Supervised Fine-Tuning (SFT) to train the LLM $\pi$ on the self-generated good responses $y_w$, aimed at improving basic capability of format following.
The SFT training objective is detailed as follows:
\begin{equation}
    \mathcal{L}_\text{SFT} = - \mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \log \pi(y_w|x) .
    \label{eq:sft}
\end{equation}
With recent advancements of preferece learning methods~\citep{dong2023raft,rafailov2024direct,ethayarajh2024kto}, we can apply Direct Preference Optimization~\citep{rafailov2024direct} (DPO) to align the fine-tuned LLM more closely with the desired response formats. The DPO training objective can be formulated as follows:
\begin{equation}
\begin{array}{l}
\quad \,\, \mathcal{L}_{\text{DPO}} = -\mathbb{E}_{(x,y_{l},y_{w}) \sim \mathcal{D}} \\[0.3em]
\left[ \log \sigma
\left(
\beta \log \frac{\pi(y_{w} | x)}{\pi_{\text{ref}}(y_{w} | x)}
- \beta \log \frac{\pi(y_{l} | x)}{\pi_{\text{ref}}(y_{l} | x)}
\right) \right] ,
\end{array}
\label{eq:dpo}
\end{equation}
where $\sigma(\ast)$ denotes the logistic function, $\beta=0.1$ is a hyperparameter, and $\pi_\text{ref}$ is the frozen reference model typically the model after SFT training. 

Despite these efforts, pilot experiments suggest that small LLMs struggle with generating good responses for complex (level-3) instructions.
To mitigate this issue, we adopt a progressive training strategy, scaling from simpler (level-1) to more complex (level-3) instructions.
This strategy can maximize the sampling efficiency in collecting self-generated data and ensure the consistent improvements, since each level's training is based on the trained checkpoint of the last level. 
After this progressive training, the small LLM is expected to follow instructions more precisely, making it better suited for LLM-based applications. The full pipeline of the proposed method is listed in Alg.~\ref{alg:enhance}.


\input{tables/main-table}



\section{Experiment}
In this section, we take experiments to comprehensively validate the effectiveness of our method in enhancing 7B level LLMs' format control ability.


\subsection{Experimental Setup}
\paragraph{Data.}
For each level of \dataset dataset, we curate $10$k and $7$k samples for training and testing, respectively.
Considering the training costs and diversity, we do not fully extend this data. Here, the ideal maximum numbers of producible instructions for each level of \dataset  are about $60$k, $360$k and $2160$k.


\paragraph{Benchmarks.}
We validate our method on two instruction following benchmarks: (1) InfoBench~\citep{qin2024infobench}, which utilizes GPT-4-based evaluations to test the general instruction following ability. 
(2) IFEval~\citep{ifeval}, which adopts Python-based function evaluations for evaluating the format control ability, similar to our \dataset. 
Note that both benchmarks consist of approximately $500$ test samples, which may introduce higher variance and bias compared to our dataset.


\paragraph{Baselines.}
For comparison, we select various open-source LLMs, including Mistral~\citep{jiang2023mistral}, the LLaMA family of models~\citep{llama2, llama3}, Qwen~\citep{qwen}, and WizardLM~\citep{xu2023wizardlm}, all of which have been fine-tuned for alignment with human preferences. In addition, we also include GPT-3.5 and GPT-4 as reference baselines to show the gap between small LLMs and advanced LLMs.

\paragraph{Training Settings}
In this paper, our experiments follow the default settings of most hyperparameters in the SFTTrainer and DPOTrainer from LLaMA-Factory~\citep{zheng2024llamafactory}. The models are trained for a total of 8 epochs using a batch size of 4 on NVIDIA A6000 GPUs, which will take up to 1 hour. We employ the AdamW optimizer~\citep{loshchilov2018decoupled} with a learning rate of $5e-6$, coupled with a cosine learning rate scheduler. To accelerate the training and save the computation resources, we fine-tune the LLM with the LoRA~\citep{hu2021lora} adapter for which we set the rank and $\alpha$ to 64 and 128, respectively. Despite the query and value heads of attention blocks, all other parameters are frozen.


\subsection{Main Results}
The main results are shown in Table~\ref{tab:main_table}.

\paragraph{Prevalent Limitations.}
The results first suggest that 7B level open-source LLMs struggle with level-2 and level-3 format following instructions of our \dataset dataset and IFEval, while they commonly have acceptable performance on general instruction following dataset InfoBench. This clearly demonstrates the limitations of such small LLMs in adhering to specific formats.

\paragraph{Effectiveness of Enhancing Format Control.}
Due to limited resources, we select only three popular LLMs for training. Their superior performance on both in-domain data (\dataset) and out-of-domain data (IFEval) confirms the effectiveness of the proposed method in enhancing format following ability. 
Notably, the trained LLaMA-3-8B model outperforms GPT-4 in the level-3 format following task.
However, we observed a slight decline for Mistral and LLaMA-3-8B in the general instruction following data (InfoBench) though LLaMA-2-7B greatly improves.
This may be due to:
(1) Overfitting on general instruction following tasks (they have significantly better performance than LLaMA-2-7B), where training on format following may slightly affect performance. 
(2) The limited number of test samples and LLM-based evaluations, both of which introduce additional evaluation bias.

\section{Analysis}

\input{tables/humangpt}

\subsection{Quality of \dataset dataset}
The instruction of \dataset is made by randomly pairing  the question and constraints, which may introduce the conflicts in the content. We sample $100$ instructions for manual and GPT-4 evaluations shown in Table~\ref{tab:human-gpt}. The results suggest that the curated instruction remain reasonable and have low conflicts. The difficulty of the instruction is consistent with the number of added constraints. 

\input{tables/one-shot}

\subsection{Analysis of Enhancing Format Control}

\paragraph{Sampling Efficiency.} 
Table~\ref{tab:ab-sampling} shows details of the adopted one-shot demonstration in response sampling, which indicate that the self-generated wrong example can serve as a useful demonstration to help LLMs to generate better responses, leading to higher sampling efficiency than directly sampling multiple times.
\input{tables/ab-progressive}

\paragraph{Training Strategies.}
As shown in Table~\ref{tab:ab-progressive}, we compare different training strategies with the progressive training strategy. First, the SFT training can effectively improve the format control performance, while the gains for level-2 and level-3 training are decreasing. For DPO-Only training, it will even harm the performance as the training proceeds. For the adopted strategy (SFT-DPO), the progressive training can consistently enhance the LLM's format following performance.

\subsection{LLM-based v.s. Python-based Judgment}

\input{tables/gpt4mini}

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{images/json_case.pdf}
\caption{Example of the generated responses to the question with JSON format constraint. }
\label{fig:json_case}
\end{figure*}


\paragraph{Accuracy \& Time \& Cost.}
Recent advancements in lightweight LLMs such as GPT-4o-mini motivate the community to choose LLM-based evaluations~\citep{qin2024infobench,jiang2023followbench,chan2023chateval}.
We compare the LLM-based judgment with Python-based judgment in Table~\ref{tab:gpt4omini}.
The results show that in the format following evaluation scenario, Python based method has a significant advantage over LLM-based approach even with lightweight LLMs (GPT-4o-mini), especially considering the 100x speed up.

\input{tables/consistency}


\paragraph{Consistency of LLM-based Judgment.} 
We further show the inconsistency and instability of using LLM-based judgments for format following in Table~\ref{tab:consistency}. The results suggest that LLMs are not reliable even by setting the temperature to $0.1$. Moreover, the advanced GPT-4o is still inconsistent in judging the correctness of format following, which shows the limitations of LLM-based evaluations.


\input{tables/content_score}


\subsection{Quality of Generated Responses}
We use manual and GPT-4o evaluations to assess the response quality considering both the content and format in Table~\ref{tab:content_score}. The first evaluation criteria is whether the generated content is relevant to the input and whether the requirements of the input are fulfilled with high quality. The second criterion is whether the generated content satisfies the constraints added in the input. 
From the results, we surprisingly observed that the LLM trained with our method not only improves the completion of constraints, but also enhances the quality of the content generation.
Additionally, we find that our method can achieve a satisfactory performance on out-of-domain IFEval data, indicating its superior generalizatio in format following.

\paragraph{Case Study.} To intuitively understand the effects of the proposed progressive training, Figure~\ref{fig:json_case} presents an example of different methods responding to JSON format instruction.
First, we find that the LLaMA-3-8B fails to follow the desired JSON output format.
However, the model trained with our level-1 constraints (ROUND1) shows noticeable improvements in generating outputs with better format control, though it still does not fully adhere to the given instructions.
Further training on level-2 data (ROUND2) can also help improve its format following ability, demonstrating the effectiveness of our progressive training method.



\section{Conclusion}
In this paper, we focus on enhancing the format following ability of 7B level LLMs.
First, to evaluate the format following ability of LLMs, we curate a fully verifiable format following dataset \dataset which uses Python scripts to accurately judge the correctness of the format following.
Then, by leveraging the verifiable feature, we can synthesize massive training data to enhance such format control generations of small LLMs in a self-improvement paradigm.
Our experiments and analysis reveal the limitations of small LLMs in format following, while demonstrating the effectiveness of our method in improving format control generations.
We believe these findings will benefit the  research community and advance the LLM applications.


\clearpage
\newpage
\section*{Limitations}
In this section, we discuss the observed limitations and offer useful suggestions for future research.
\begin{enumerate}[1)]
    \item The contributions of this paper heavily rely on the fact that output formats can be efficiently verified by Python functions, thus it may not be easily extended to general instruction following. Fine-grained format control remains a significant challenge for current open-source 7B-level LLMs. Moreover, generating specific output formats is crucial for many LLM-based applications. Although constrained decoding is promising, it can be further improved by supporting a broader range of formats.
    \item The richness of the proposed verifiable format following dataset \dataset is based on about $60$ meta constraints which may not cover the whole categories of human desired output formats in real world applications. In the future, it can be integrated with online human feedback to collect more format categories. 
    \item The adopted training methods for improving LLM's format control ability mainly use the supervised fine-tuning and DPO training~\citep{rafailov2024direct}, which leverage the verifiable feature of our \dataset dataset for annotations. However, the verifiable rule can be viewed as a reward function to explore reinforcement learning algorithms for further improvements.
    \item Due to the limited computation resources, the proposed training method is validated on only three 7B-level LLMs (i.e., Mistral~\citep{jiang2023mistral}, LLaMA-2~\citep{llama2} and LLaMA-3~\citep{llama3}). Also, we only evaluate on other two instruction following benchmarks (i.e., InfoBench~\citep{qin2024infobench} and IFEval~\citep{ifeval}). And the analysis can be more comprehensive by exploring the relationship between training data size and the performance.  Besides, the results indicate that our training method may slightly harm some general instruction following performance, which needs more investigation.
\end{enumerate}



% ethics statements

\section*{Ethics Statement}
All datasets and trained LLMs employed in this paper are publicly available. This paper mainly studies the format following issue of LLMs, while does not cover issues of 
evaluating the correctness of the content such as detecting hallucinations. This indicates that the proposed training method may not enhance other foundational abilities of LLMs.
We use ChatGPT at the sentence level (e.g., fixing grammar) to assist the paper writing.


% \section*{Acknowledgments}

\bibliography{custom,anthology}

\input{appendix.tex}


\end{document}


% limitations


% This document has been adapted
% by Steven Bethard, Ryan Cotterell and Rui Yan
% from the instructions for earlier ACL and NAACL proceedings, including those for
% ACL 2019 by Douwe Kiela and Ivan Vuli\'{c},
% NAACL 2019 by Stephanie Lukin and Alla Roskovskaya,
% ACL 2018 by Shay Cohen, Kevin Gimpel, and Wei Lu,
% NAACL 2018 by Margaret Mitchell and Stephanie Lukin,
% Bib\TeX{} suggestions for (NA)ACL 2017/2018 from Jason Eisner,
% ACL 2017 by Dan Gildea and Min-Yen Kan,
% NAACL 2017 by Margaret Mitchell,
% ACL 2012 by Maggie Li and Michael White,
% ACL 2010 by Jing-Shin Chang and Philipp Koehn,
% ACL 2008 by Johanna D. Moore, Simone Teufel, James Allan, and Sadaoki Furui,
% ACL 2005 by Hwee Tou Ng and Kemal Oflazer,
% ACL 2002 by Eugene Charniak and Dekang Lin,
% and earlier ACL and EACL formats written by several people, including
% John Chen, Henry S. Thompson and Donald Walker.
% Additional elements were taken from the formatting instructions of the \emph{International Joint Conference on Artificial Intelligence} and the \emph{Conference on Computer Vision and Pattern Recognition}.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only





