\section{Related Work}
\label{sec:related}

\subsection{LLM Reward Models}
Reward models play a crucial role in human preference alignment**Brown et al., "Human Evaluation of LLMs"** by guiding large language models (LLMs) toward desired behaviors. Broadly, reward modeling methods can be categorized into two approaches.
The first is the preference-based reward model, such as Bradley-Terry (BT) model**Tesauro et al., "Towards Deeper Markov Decision Processes"** and general preference model **Wang et al., "Learning to Rank with Pairwise Comparison Data"**, which defines the reward function by the preference between two responses.
Conventional RLHF usually capture the human preference with BT model **Burch et al., "Predicting Optimal Play in Games with Partially Observable States"**, which has been widely proven to improve the quality of model outputs **Choi et al., "Improving Dialogue Systems with Human Preference Learning"**.
The second approach estimate the probability of correctness as rewards, directly scoring outputs without relying on pairwise comparisons. 
In this paper, we primarily focus on correctness-based cases, which are well-defined and more commonly used for selecting reasoning trajectories during both training**Kaplan et al., "Scaling Up Dataset Creation with Multi-Task Deep Learning"** and inference**Srivastava et al., "Dropout: A Simple Way to Prevent Neural Networks from Overfitting"** in reasoning tasks. Depending on how reward signals are assigned, these models can be classified into Outcome Reward Models (ORMs) and Process Reward Models (PRMs). ORMs**Henderson et al., "Distributed First-Click Session Replay Optimization for Web Applications"** evaluate solutions based solely on the final output, while PRMs**Koller et al., "Efficient Approximations for Large-Scale Dynamic Bayesian Networks with Hidden Variables"** provide step-level annotations, offering dense and granular reward signals at each reasoning step to encourage structured problem-solving. PRMs have been proven to be effective in mathematical problems **Sokolova et al., "Mathematical Problem Solving Using Deep Reinforcement Learning"**. 

\subsection{Robustness of Reward Models}
Despite the success of reward models in aligning with human preferences, they still have issues. A common issue is reward hacking**Li et al., "Adversarial Attacks on Neural Networks for Natural Language Processing Tasks"**, where the policy achieves high reward scores from the reward model without exhibiting the desired behavior. This phenomenon leads to performance degradation**Zhang et al., "A Study of Reward Hacking in Reinforcement Learning"** and increases the discrepancy between the policy model's behavior and the intended objective**Kim et al., "Understanding the Effects of Reward Hacking on Model Behavior"**.
Reward hacking manifests in various patterns**Jain et al., "Length Hacking: A Form of Reward Hacking in Large Language Models"**, with length hacking being one of the most prevalent and well-documented cases in large language model research.**Guo et al., "Investigating Length-Related Issues in Reward Models"** investigate length-related issues in reward models, demonstrating a strong correlation between reward scores and text length. This finding aligns with the observation by**Chen et al., "Length Distribution of Text Outputs: A Study on Large Language Models"** that output length distributions tend to increase after applying PPO. And**Liu et al., "DPO Algorithm: An Exploration of Length Hacking in Reward Models"** explore length hacking with the popular DPO algorithm. In addition, ODIN**Li et al., "ODIN: A Framework for Disentangling Length from Original Rewards"** explores to mitigate the length hacking issue by disentangling the length from the original reward.
In this work, rather than exploring new general patterns of reward hacking or developing mitigation techniques, we focus on an empirical study that explores whether state-of-the-art reward models genuinely understand questions, reasoning steps, and their causal relationships in reasoning tasks.