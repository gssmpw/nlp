\section{Related Work}
\label{sec:related}

\subsection{LLM Reward Models}
Reward models play a crucial role in human preference alignment____ by guiding large language models (LLMs) toward desired behaviors. Broadly, reward modeling methods can be categorized into two approaches.
The first is the preference-based reward model, such as Bradley-Terry (BT) model____ and general preference model ____, which defines the reward function by the preference between two responses.
Conventional RLHF usually capture the human preference with BT model ____, which has been widely proven to improve the quality of model outputs ____.
The second approach estimate the probability of correctness as rewards, directly scoring outputs without relying on pairwise comparisons. 
In this paper, we primarily focus on correctness-based cases, which are well-defined and more commonly used for selecting reasoning trajectories during both training____ and inference____ in reasoning tasks. Depending on how reward signals are assigned, these models can be classified into Outcome Reward Models (ORMs) and Process Reward Models (PRMs). ORMs____ evaluate solutions based solely on the final output, while PRMs____ provide step-level annotations, offering dense and granular reward signals at each reasoning step to encourage structured problem-solving. PRMs have been proven to be effective in mathematical problems ____. 
% verifier -> step by step 
% process reward model: -> mc -> 



\subsection{Robustness of Reward Models}
Despite the success of reward models in aligning with human preferences, they still have issues. A common issue is reward hacking____, where the policy achieves high reward scores from the reward model without exhibiting the desired behavior. This phenomenon leads to performance degradation____ and increases the discrepancy between the policy model's behavior and the intended objective____. 
Reward hacking manifests in various patterns____, with length hacking being one of the most prevalent and well-documented cases in large language model research.____ investigate length-related issues in reward models, demonstrating a strong correlation between reward scores and text length. This finding aligns with the observation by____ that output length distributions tend to increase after applying PPO. And____ explore length hacking with the popular DPO algorithm. In addition, ODIN____ explores to mitigate the length hacking issue by disentangling the length from the original reward.
In this work, rather than exploring new general patterns of reward hacking or developing mitigation techniques, we focus on an empirical study that explores whether state-of-the-art reward models genuinely understand questions, reasoning steps, and their causal relationships in reasoning tasks.