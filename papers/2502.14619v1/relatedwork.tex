\section{Related Work}
\label{sec:related}

\subsection{LLM Reward Models}
Reward models play a crucial role in human preference alignment~\citep{christiano2017deep,bai2022training,casper2023open} by guiding large language models (LLMs) toward desired behaviors. Broadly, reward modeling methods can be categorized into two approaches.
The first is the preference-based reward model, such as Bradley-Terry (BT) model~\citep{bradley1952rank,zhao2023slic,rafailov2024direct,ethayarajh2024kto,xiong2024iterative} and general preference model \citep{llm-blender-2023,munos2023nash,tang2024generalized,ye2025online,azar2024general}, which defines the reward function by the preference between two responses.
Conventional RLHF usually capture the human preference with BT model \citep{ouyang2022training,chatgpt}, which has been widely proven to improve the quality of model outputs \citep{dubey2024llama,dong2024rlhf,guo2024direct}.
The second approach estimate the probability of correctness as rewards, directly scoring outputs without relying on pairwise comparisons. 
In this paper, we primarily focus on correctness-based cases, which are well-defined and more commonly used for selecting reasoning trajectories during both training~\citep{chen2024alphamath,wang2024math} and inference~\citep{brown2024large} in reasoning tasks. Depending on how reward signals are assigned, these models can be classified into Outcome Reward Models (ORMs) and Process Reward Models (PRMs). ORMs~\citep{yu2023outcome} evaluate solutions based solely on the final output, while PRMs~\citep{lightman2023let} provide step-level annotations, offering dense and granular reward signals at each reasoning step to encourage structured problem-solving. PRMs have been proven to be effective in mathematical problems \citep{shao2024deepseekmath,snell2024scaling,luo2024improve,liao2025reward}. 
% verifier -> step by step 
% process reward model: -> mc -> 



\subsection{Robustness of Reward Models}
Despite the success of reward models in aligning with human preferences, they still have issues. A common issue is reward hacking~\citep{ibarz2018reward, denison2024sycophancy}, where the policy achieves high reward scores from the reward model without exhibiting the desired behavior. This phenomenon leads to performance degradation~\citep{bai2022training} and increases the discrepancy between the policy model's behavior and the intended objective~\citep{stienon2020learning}. 
Reward hacking manifests in various patterns~\citep{park2024offsetbias}, with length hacking being one of the most prevalent and well-documented cases in large language model research.~\citet{singhal2024long} investigate length-related issues in reward models, demonstrating a strong correlation between reward scores and text length. This finding aligns with the observation by~\citet{dubois2023alpacafarm} that output length distributions tend to increase after applying PPO. And~\citet{liu2024iterative} explore length hacking with the popular DPO algorithm. In addition, ODIN~\citep{denison2024sycophancy} explores to mitigate the length hacking issue by disentangling the length from the original reward.
In this work, rather than exploring new general patterns of reward hacking or developing mitigation techniques, we focus on an empirical study that explores whether state-of-the-art reward models genuinely understand questions, reasoning steps, and their causal relationships in reasoning tasks.