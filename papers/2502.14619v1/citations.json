[
  {
    "index": 0,
    "papers": [
      {
        "key": "christiano2017deep",
        "author": "Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario",
        "title": "Deep reinforcement learning from human preferences"
      },
      {
        "key": "bai2022training",
        "author": "Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others",
        "title": "Training a helpful and harmless assistant with reinforcement learning from human feedback"
      },
      {
        "key": "casper2023open",
        "author": "Casper, Stephen and Davies, Xander and Shi, Claudia and Gilbert, Thomas Krendl and Scheurer, J{\\'e}r{\\'e}my and Rando, Javier and Freedman, Rachel and Korbak, Tomasz and Lindner, David and Freire, Pedro and others",
        "title": "Open problems and fundamental limitations of reinforcement learning from human feedback"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "bradley1952rank",
        "author": "Bradley, Ralph Allan and Terry, Milton E",
        "title": "Rank analysis of incomplete block designs: I. The method of paired comparisons"
      },
      {
        "key": "zhao2023slic",
        "author": "Zhao, Yao and Joshi, Rishabh and Liu, Tianqi and Khalman, Misha and Saleh, Mohammad and Liu, Peter J",
        "title": "Slic-hf: Sequence likelihood calibration with human feedback"
      },
      {
        "key": "rafailov2024direct",
        "author": "Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea",
        "title": "Direct preference optimization: Your language model is secretly a reward model"
      },
      {
        "key": "ethayarajh2024kto",
        "author": "Ethayarajh, Kawin and Xu, Winnie and Muennighoff, Niklas and Jurafsky, Dan and Kiela, Douwe",
        "title": "Kto: Model alignment as prospect theoretic optimization"
      },
      {
        "key": "xiong2024iterative",
        "author": "Xiong, Wei and Dong, Hanze and Ye, Chenlu and Wang, Ziqi and Zhong, Han and Ji, Heng and Jiang, Nan and Zhang, Tong",
        "title": "Iterative preference learning from human feedback: Bridging theory and practice for rlhf under kl-constraint"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "llm-blender-2023",
        "author": "Jiang, Dongfu and Ren, Xiang and Lin, Bill Yuchen",
        "title": "LLM-Blender: Ensembling Large Language Models with Pairwise Comparison and Generative Fusion"
      },
      {
        "key": "munos2023nash",
        "author": "Munos, R{\\'e}mi and Valko, Michal and Calandriello, Daniele and Azar, Mohammad Gheshlaghi and Rowland, Mark and Guo, Zhaohan Daniel and Tang, Yunhao and Geist, Matthieu and Mesnard, Thomas and Michi, Andrea and others",
        "title": "Nash learning from human feedback"
      },
      {
        "key": "tang2024generalized",
        "author": "Tang, Yunhao and Guo, Zhaohan Daniel and Zheng, Zeyu and Calandriello, Daniele and Munos, R{\\'e}mi and Rowland, Mark and Richemond, Pierre Harvey and Valko, Michal and Pires, Bernardo {\\'A}vila and Piot, Bilal",
        "title": "Generalized preference optimization: A unified approach to offline alignment"
      },
      {
        "key": "ye2025online",
        "author": "Chenlu Ye and Wei Xiong and Yuheng Zhang and Hanze Dong and Nan Jiang and Tong Zhang",
        "title": "Online Iterative Reinforcement Learning from Human Feedback with General Preference Model"
      },
      {
        "key": "azar2024general",
        "author": "Azar, Mohammad Gheshlaghi and Guo, Zhaohan Daniel and Piot, Bilal and Munos, Remi and Rowland, Mark and Valko, Michal and Calandriello, Daniele",
        "title": "A general theoretical paradigm to understand learning from human preferences"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "ouyang2022training",
        "author": "Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "key": "chatgpt",
        "author": "OpenAI",
        "title": "Open{AI}: Introducing {ChatGPT}"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "dubey2024llama",
        "author": "Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others",
        "title": "The llama 3 herd of models"
      },
      {
        "key": "dong2024rlhf",
        "author": "Dong, Hanze and Xiong, Wei and Pang, Bo and Wang, Haoxiang and Zhao, Han and Zhou, Yingbo and Jiang, Nan and Sahoo, Doyen and Xiong, Caiming and Zhang, Tong",
        "title": "Rlhf workflow: From reward modeling to online rlhf"
      },
      {
        "key": "guo2024direct",
        "author": "Guo, Shangmin and Zhang, Biao and Liu, Tianlin and Liu, Tianqi and Khalman, Misha and Llinares, Felipe and Rame, Alexandre and Mesnard, Thomas and Zhao, Yao and Piot, Bilal and others",
        "title": "Direct language model alignment from online ai feedback"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "chen2024alphamath",
        "author": "Chen, Guoxin and Liao, Minpeng and Li, Chengxi and Fan, Kai",
        "title": "AlphaMath Almost Zero: process Supervision without process"
      },
      {
        "key": "wang2024math",
        "author": "Wang, Peiyi and Li, Lei and Shao, Zhihong and Xu, Runxin and Dai, Damai and Li, Yifei and Chen, Deli and Wu, Yu and Sui, Zhifang",
        "title": "Math-shepherd: Verify and reinforce llms step-by-step without human annotations"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "brown2024large",
        "author": "Brown, Bradley and Juravsky, Jordan and Ehrlich, Ryan and Clark, Ronald and Le, Quoc V and R{\\'e}, Christopher and Mirhoseini, Azalia",
        "title": "Large language monkeys: Scaling inference compute with repeated sampling"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "yu2023outcome",
        "author": "Yu, Fei and Gao, Anningzhe and Wang, Benyou",
        "title": "Outcome-supervised verifiers for planning in mathematical reasoning"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "lightman2023let",
        "author": "Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and Edwards, Harri and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl",
        "title": "Let's verify step by step"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "shao2024deepseekmath",
        "author": "Shao, Zhihong and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Song, Junxiao and Bi, Xiao and Zhang, Haowei and Zhang, Mingchuan and Li, YK and Wu, Y and others",
        "title": "Deepseekmath: Pushing the limits of mathematical reasoning in open language models"
      },
      {
        "key": "snell2024scaling",
        "author": "Snell, Charlie and Lee, Jaehoon and Xu, Kelvin and Kumar, Aviral",
        "title": "Scaling llm test-time compute optimally can be more effective than scaling model parameters"
      },
      {
        "key": "luo2024improve",
        "author": "Luo, Liangchen and Liu, Yinxiao and Liu, Rosanne and Phatale, Samrat and Lara, Harsh and Li, Yunxuan and Shu, Lei and Zhu, Yun and Meng, Lei and Sun, Jiao and others",
        "title": "Improve Mathematical Reasoning in Language Models by Automated Process Supervision"
      },
      {
        "key": "liao2025reward",
        "author": "Liao, Baohao and Xu, Yuhui and Dong, Hanze and Li, Junnan and Monz, Christof and Savarese, Silvio and Sahoo, Doyen and Xiong, Caiming",
        "title": "Reward-Guided Speculative Decoding for Efficient LLM Reasoning"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "ibarz2018reward",
        "author": "Ibarz, Borja and Leike, Jan and Pohlen, Tobias and Irving, Geoffrey and Legg, Shane and Amodei, Dario",
        "title": "Reward learning from human preferences and demonstrations in atari"
      },
      {
        "key": "denison2024sycophancy",
        "author": "Denison, Carson and MacDiarmid, Monte and Barez, Fazl and Duvenaud, David and Kravec, Shauna and Marks, Samuel and Schiefer, Nicholas and Soklaski, Ryan and Tamkin, Alex and Kaplan, Jared and others",
        "title": "Sycophancy to subterfuge: Investigating reward-tampering in large language models"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "bai2022training",
        "author": "Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others",
        "title": "Training a helpful and harmless assistant with reinforcement learning from human feedback"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "stienon2020learning",
        "author": "Nisan Stiennon and Long Ouyang and Jeff Wu and Daniel M. Ziegler and Ryan Lowe and Chelsea Voss and Alec Radford and Dario Amodei and Paul Christiano",
        "title": "Learning to summarize from human feedback"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "park2024offsetbias",
        "author": "Park, Junsoo and Jwa, Seungyeon and Ren, Meiying and Kim, Daeyoung and Choi, Sanghyuk",
        "title": "Offsetbias: Leveraging debiased data for tuning evaluators"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "singhal2024long",
        "author": "Prasann Singhal and Tanya Goyal and Jiacheng Xu and Greg Durrett",
        "title": "A Long Way to Go: Investigating Length Correlations in RLHF"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "dubois2023alpacafarm",
        "author": "Dubois, Yann and Li, Xuechen and Taori, Rohan and Zhang, Tianyi and Gulrajani, Ishaan and Ba, Jimmy and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B",
        "title": "Alpacafarm: A simulation framework for methods that learn from human feedback"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "liu2024iterative",
        "author": "Liu, Jie and Zhou, Zhanhui and Liu, Jiaheng and Bu, Xingyuan and Yang, Chao and Zhong, Han-Sen and Ouyang, Wanli",
        "title": "Iterative Length-Regularized Direct Preference Optimization: A Case Study on Improving 7B Language Models to GPT-4 Level"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "denison2024sycophancy",
        "author": "Denison, Carson and MacDiarmid, Monte and Barez, Fazl and Duvenaud, David and Kravec, Shauna and Marks, Samuel and Schiefer, Nicholas and Soklaski, Ryan and Tamkin, Alex and Kaplan, Jared and others",
        "title": "Sycophancy to subterfuge: Investigating reward-tampering in large language models"
      }
    ]
  }
]