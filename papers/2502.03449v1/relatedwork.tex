\section{Related Work}
\subsection{Multi-view Diffusion} 

Owing to their powerful predictive ability, Diffusion Probabilistic Models \cite{ho2020denoising} have been applied to image \cite{nichol2021glide, zhang2023adding, dhariwal2021diffusion, ruiz2023dreambooth, saharia2022palette}, video \cite{chen2024videocrafter2, ho2022video}, and 3D shape synthesis tasks \cite{long2023wonder3d, yu2023surf, tang2023dreamgaussian}, etc. However, applying image diffusion models to generate multi-view images separately poses significant challenges in maintaining consistency across different views. To address multi-view inconsistency, multi-view attentions and camera pose controls are adopted to fine-tune pre-trained image diffusion models, enabling the simultaneous synthesis of multi-view images \cite{shi2023mvdream, wang2023imagedream, xu2023dmv3d, yang2024consistnet, shi2023zero123++, long2024wonder3d}, though these methods might result in compromised geometric consistency due to the lack of inherent 3D biases. To ensure both global semantic consistency and detailed local alignment in multi-view diffusion models, 3D-adapters \cite{chen20243d} propose a plug-in module designed to infuse 3D geometry awareness. Nevertheless, the generated images by these models are sparse views. To address this issue, CAT3D \cite{gao2024cat3d} introduces an efficient parallel sampling strategy to generate a large set of camera poses, and MVDiffusion++ \cite{tang2025mvdiffusion++} adopts a pose-free architecture and a view dropout strategy to reduce computational costs, generating dense, high-resolution images. 

Generating consistent images from multi-view diffusions offers guidance for further 3D shape reconstruction \cite{gao2024cat3d}. PSHuman \cite{li2024pshuman} integrates a body-face cross-scale diffusion with an SMPL-X conditioned multi-view diffusion for clothed human reconstruction with high-quality face details. Recent work, MagicMan \cite{he2024magicman}, utilizes a hybrid human-specific multi-view diffusion model with 3D SMPL-X-based body priors and 2D diffusion priors to consistently generate dense multi-view RGB images and normal maps, supporting high-quality human mesh reconstruction. Different from these works, we exploited multi-view diffusions to generate multi-view normals and RGB images as guidance to optimize sewing patterns and stitches instead of human meshes.



\subsection{Garment Reconstruction}

Previous work focusing on clothed human reconstruction \cite{xiu2022icon, xiu2023econ} typically generates garments fused with digital human models, limiting them to basic skinning-based animations and requires extra segmentation and editing to separate the garments from the human body. In contrast, our approach focuses on reconstructing separately wearable, simulation-ready garments and human models. Other closely related works include \citet{li2024diffavatar}, which also generates simulation-ready clothes, but at the cost of creating clothing templates by artists and precise point clouds by scanners. NeuralTailor \cite{korosteleva2022neuraltailor} exploits point-level attention for pattern shape and stitching information regression, enabling the reconstruction of garment meshes from point clouds. In contrast, our paper focuses on reconstructing non-watertight garments and humans separately from a single image without additional inputs. 

To reconstruct separated non-watertight garments from a single image, GarVerseLOD \cite{luo2024garverselod} recovers garment details hierarchically in a coarse-to-fine framework. However, it fails to reconstruct complex skirts or dresses with slits or with complex human poses due to the limited representation of such features in the training data. ClothWild \cite{moon20223d} exploits a weakly supervised pipeline with DensePose-based loss to further increase robustness on in-the-wild images. BCNet \cite{jiang2020bcnet} introduces a layered garment representation and a generic skinning weight generation network to model garments with different topologies. Deep Fashion3D \cite{zhu2020deep} refines adaptable templates with rich annotations to fit garment shapes. While they are limited to garment categories in their training datasets, these works fail to reconstruct complex categories such as jumpsuits. Additionally, they require nearly frontal images as input, limiting reconstruction from different views. AnchorUDF \cite{zhao2021learning} explores a learnable unsigned distance function to query both 3D position features and pixel-aligned image features via anchor points, which reconstructs the coarse garment shape but lacks the generation of high-quality geometric details.

Instead of directly reconstructing garment meshes, some works \cite{liu2023towards, he2024dresscode, korosteleva2023garmentcode} treat sewing patterns as intermediate representations to generate garments by stitching them together. Recent work, GarmentRecovery \cite{li2024garment}, introduces implicit sewing patterns (ISP) to provide shape priors integrated with deformation priors for further garment recovery, though it builds specialized models for each individual garment or garment type. Both SewFormer \cite{liu2023towards} and PanelFormer \cite{chen2024panelformer} utilize Transformers to predict sewing patterns and stitches. However, their garment results lack physical material parameters. Therefore, they fail to reconstruct diverse shapes for garments with different physical materials. In addition, these feed-forward methods require large amounts of garment data for training, failing to synthesize garments that fall outside the distribution of the training data. Our work aims to generate diverse, image-aligned, simulation-ready garments with high-quality details from in-the-wild images by optimizing sewing patterns and stitches with physical parameters via differentiable simulations.


\subsection{Differentiable Simulation}

Differentiable simulation has seen widespread application in recent research, particularly for system identification and the inference of material parameters from both synthetic~\cite{li2023pac, li2024neuralfluid} and real-world~\cite{huang2024differentiable, si2024difftactile} observations. The scope of exploration spans various domains, including fluid dynamics and control~\cite{mcnamara2004fluid, schenck2018spnets, li2023difffr, li2024neuralfluid}, rigid-body dynamics~\cite{freeman2021brax, strecke2021diffsdfsim, xu2023efficient}, articulated systems~\cite{geilinger2020add, qiao2021efficient, xu2021end}, soft-body dynamics~\cite{hahn2019real2sim, hu2019chainqueen, du2021diffpd, jatavallabhula2021gradsim, huang2024differentiable}, cloth~\cite{li2022diffcloth, stuyck2023diffxpbd, li2024diffavatar}, inelasticity~\cite{huang2021plasticinelab, li2023pac}, inflatable structures~\cite{panetta2021computational}, and Voronoi diagrams~\cite{numerow2024differentiable}.

Cloth-based applications, whether for static optimization or dynamic simulation, frequently involve extensive frictional contact. Consequently, many works focus on robust methods for handling dry frictional contact in differentiable simulations. \citet{bartle2016physics} proposes a physics-driven pattern adjustment for garment editing using fixed-point optimization, which does not account for gradients. \citet{liang2019differentiable} is the first to introduce a fully functional differentiable cloth simulator with frictional contact and self-collision, formulating a quadratic programming problem. \citet{jatavallabhula2021gradsim} employs a penalty-based frictional contact model, while \citet{du2021diffpd} and \citet{li2022diffcloth} leverage the adjoint method for Projective Dynamics~\cite{bouaziz2014projective} with friction. Building on Position-Based Dynamics~\cite{muller2007position, macklin2016xpbd}, \citet{stuyck2023diffxpbd} and \citet{li2024diffavatar} introduce differentiable formulations for compliant constraint dynamics, and \citet{huang2024differentiable} presents an adjoint-based framework for differentiable Incremental Potential Contact (IPC)~\cite{li2020incremental, li2020codimensional}.


The finite difference (FD) method~\cite{renardy2006introduction} is a standard approach to numerical differentiation. The complex-step finite difference technique~\cite{luo2019accelerated, shen2021high} offers an alternative that mitigates issues such as subtractive cancellation and accumulated numerical errors by leveraging complex Taylor expansions~\cite{brezillon1981numerical}. They can be used to optimize low-DoF system \cite{zheng2025physavatar}. Automatic differentiation (AD)~\cite{naumann2011art, margossian2019review} and code transformation libraries like NVIDIA Warp~\cite{macklin2022warp}, DiffTaichi~\cite{hu2019chainqueen, hu2019difftaichi}, and others~\cite{herholz2024mesh} automatically compute gradients based on forward simulation, allowing for greater reuse of existing code. However, they can introduce code constraints, incur a high memory footprint, and may cause gradient explosion if applied naively. Our framework combines NVIDIA Warpâ€™s AD with an adjoint method to achieve both development efficiency and high performance.