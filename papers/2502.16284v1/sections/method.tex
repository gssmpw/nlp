\section{The proposed \themodel method}

Considering the complementarity of different spectra, we introduce multiple spectra into molecular representation learning. To effectively comprehend molecular spectra, we designed a Transformer-based multi-spectrum encoder, SpecFormer, along with a masked reconstruction objective to guide its training. Finally, a contrastive objective is employed to align the 3D encoding guided by the denoising objective with the spectra encoding guided by the reconstruction objective, endowing the 3D encoding with the capability to understand spectra and the knowledge they encompass.

\begin{figure}
\begin{center}
\includegraphics[width=\linewidth]{./figures/2_overview.pdf}
\end{center}
\caption{Overview of the \themodel pre-training framework. Our pre-training framework comprises three sub-objectives: the denoising objective and the MPR objective, which respectively guide the representation learning of the 3D and spectral modalities, and the contrastive objective, which aligns the representations of both modalities.}
\label{fig:overview}
\end{figure}

\subsection{SpecFormer: a single-stream encoder for multi-modal energy spectra}\label{sec:specformer}

For different types of spectra, each spectrum is independently patched and initially encoded. Then, all the resulting patch embeddings are concatenated and encoded using a Transformer-based encoder.

\textbf{Patching.}
Compared to directly encoding individual frequency points, we divided each spectrum into multiple patches. This approach offers two distinct advantages: (\romannumeral 1) By forming patches from adjacent frequency points, local semantic features, such as absorption peaks, can be captured more effectively. (\romannumeral 2) It reduces the computational overhead of subsequent Transformer layers.
Technically, each spectrum $\vs_i \in \mathbb{R}^{L_i}$ where $i=1,\cdots,|\mathcal{S}|$ is first divided into patches according to the patch length $P_i$ and the stride $D_i$. When $0<D_i<P_i$, the consecutive patches will be overlapped with overlapping region length $P_i-D_i$. When $D_i=P_i$, the consecutive patches will be non-overlapped. $L_i$ denotes the length of $\vs_i$. The patching process on each spectrum will generate a sequence of patches $\vp_i \in \mathbb{R}^{N_i\times P_i}$, where $N_i = \left\lfloor \frac{L_i - P_i}{D_i} \right\rfloor + 1$ is the number of patches.

\textbf{Patch encoding and position encoding.}
Prior to be fed into the encoder, the patches of the $i$-th spectrum are mapped to the latent space of dimension $d$ via a trainable linear projection $\mW_i \in \mathbb{R}^{P_i \times d}$. A learnable additive position encoding $\mW_i^{\text{pos}} \in \mathbb{R}^{N_i \times d}$ is applied to maintain the order of the patches: $\vp_i^{\prime} = \vp_i \mW_i + \mW_i^{\text{pos}}$, where $\vp_i^{\prime} \in \mathbb{R}^{N_i \times d}$ denotes the latent representation of the spectrum $\vs_i$ that will be fed into the subsequent SpecFormer encoder.


\textbf{SpecFormer: multi-spectrum Transformer encoder.}
Although several encoders have been proposed to map molecular spectrum into implicit representations, such as the CNN-AM~\citep{CNN-AM} based on one-dimensional convolution, these encoders are designed to encode only a single type of spectrum. In our approach, multiple molecular spectra (UV-Vis, IR, Raman) are jointly considered. When encoding multiple spectra of a molecule simultaneously, an observation caught our attention and led us to adopt a Transformer-based encoder with multiple spectra as input, similar to the single-stream Transformer in multi-modal learning~\citep{single-stream}. 
\begin{wrapfigure}[15]{r}{0.6\linewidth}
\begin{center}
\vspace{-14pt}
\includegraphics[width=\linewidth]{./figures/3_dependencies.pdf}
\end{center}
\vspace{-12pt}
\caption{Illustrate of intra-spectrum (left) and inter-spectrum (right) dependencies.}
\label{fig:dependencies}
\end{wrapfigure}
The observation refers to the fact that the same functional group not only causes multiple peaks within a single spectrum, but also generates peaks across different spectra. 
As shown on the left of \cref{fig:dependencies}, the different vibrational modes of the methyl group ($\text{-CH}_3$) in methanol ($\text{CH}_3\text{OH}$) result in three peaks in the IR spectrum, indicating \textit{intra-spectrum dependencies} among these peaks. 
A similar phenomenon occurs with the hydroxyl group ($\text{-OH}$) in methanol. 
Additionally, the aromatic ring in phenol ($\text{C}_6\text{H}_5\text{OH}$), shown on the right of \cref{fig:dependencies}, not only produces multiple peaks in the IR spectrum due to different vibrational modes but also causes an absorption peak near 270 nm in the UV-Vis spectrum due to the $\pi \rightarrow \pi^*$ transition in the aromatic ring, demonstrating the existence of \textit{inter-spectrum dependencies}.
Such dependencies have been theoretically studied, for example, in the context of vibronic coupling~\citep{Vibronic-Coupling}.



To capture intra-spectrum and inter-spectrum dependencies, we concatenate the embeddings obtained from patch encoding and position encoding of different spectra: $\hat{\vp} = \vp_1^{\prime}\| \cdots \| \vp_{|\mathcal{S}|}^{\prime} \in \mathbb{R}^{(\sum_{i=1}^{|\mathcal{S}|} N_i) \times d}$, and then input them into the Transformer encoder as depicted in \cref{fig:overview}.
Then each head $h = 1, \ldots, H$ in multi-head attention will transform them into query matrices $\mQ_h = \hat{\vp} \mW_h^Q$, key matrices $\mK_h = \hat{\vp} \mW_h^K$ and value matrices $\mV_h = \hat{\vp} \mW_h^V$, where $\mW_h^Q, \mW_h^K \in \mathbb{R}^{d \times d_k}$ and $\mathbf{W}_h^V \in \mathbb{R}^{d \times \frac{d}{H}}$.  Afterward, a scaled product is utilized to obtain the attention output $\mO_h \in \mathbb{R}^{(\sum_{i=1}^{|\mathcal{S}|} N_i) \times \frac{d}{H}}$:
\begin{equation}
\mO_h = \text{Attention}(\mQ_h, \mK_h, \mV_h) = \text{Softmax}\left(\frac{\mQ_h \mK_h^{\top}}{\sqrt{d_k}}\right)\mV_h.
\end{equation}
The multi-head attention block also includes BatchNorm layers and a feed forward network with residual connections as shown in \cref{fig:overview}. After combining the outputs of all heads, it generates the representation denoted as $\vz \in \mathbb{R}^{(\sum_{i=1}^{|\mathcal{S}|} N_i) \times d}$. Finally, a flatten layer with representation projection head is used to obtain the molecular spectra representation $\vz_s \in \mathbb{R}^{d}$.


\subsection{Masked patches reconstruction pre-training for spectra}

Before distilling the spectra information into 3D molecular representation learning, we need first ensure that the spectrum encoder can effectively comprehend molecular spectra and generate spectral representations. Considering the success of masking modeling across various domains~\citep{Bert,MAE,GraphMAE,Mole-Bert,AUG-MAE,PatchTST}, we propose a masked patches reconstruction (MPR) objective to guide the training of SpecFormer.

After the patching step, we randomly select a portion of patches according to the mask ratio $\alpha$ and replace them with zero vectors to implement the masking. Subsequently, the masked patches undergo patch encoding and position encoding. In this way, the semantics of the masked patches (the absorption intensity at specific wavelengths) are obscured during patch encoding, while the positional information is retained to facilitate the reconstruction of the original semantics.

After encoding by SpecFormer, the encoded results corresponding to the masked patches are input into a spectrum-specific reconstruction head to reconstruct the original spectral values that were masked. The mean squared error (MSE) between the reconstruction results and the original masked spectra serves as the loss function for the MPR task, guiding the training of SpecFormer:
\begin{equation}
\begin{aligned}
    \mathcal{L}_{\mathrm{MPR}} = \sum_{i=1}^{|\mathcal{S}|} \mathbb{E}_{p_{i,j} \in \widetilde{\mathcal{P}}_i} \|\hat{\vp}_{i,j}- \vp_{i,j}\|_2^2, 
\end{aligned}
\label{sce}
\end{equation}
where $\widetilde{\mathcal{P}}_i$ denotes the set of masked patches in the $i$-th type of molecular spectra, and $\hat{\vp}_{i,j}$ denotes the reconstructed patch corresponding to the masked patch $\vp_{i,j}$.

\subsection{Contrastive learning between 3D structures and spectra}

Under the guidance of the denoising objective for 3D representation learning and the MPR objective for spectral representation learning, we further introduce a contrastive objective to align the representations across these two modalities. We treat the 3D representation $\vz_x \in \mathbb{R}^{d}$ and spectral representation $\vz_s \in \mathbb{R}^{d}$ of the same molecule as positive samples, and negative samples otherwise. Subsequently, the consistency between positive samples and the discrepancy between negative samples are maximized through the contrastive objective. Given the theoretical and empirical effectiveness, we employ InfoNCE~\citep{InfoNCE} as the contrastive objective:
\begin{equation}
\resizebox{\textwidth}{!}{$
    \mathcal{L}_{\text{Contrast}} = -\frac{1}{2} \mathbb{E}_{p(\vz_x, \vz_s)} \left[ \log \frac{\exp(f_x(\vz_x, \vz_s))}{\exp(f_x(\vz_x, \vz_s)) + \sum_j \exp(f_x(\vz_x^j, \vz_s))} + \log \frac{\exp(f_s(\vz_s, \vz_x))}{\exp(f_s(\vz_s, \vz_x)) + \sum_j \exp(f_s(\vz_s^j, \vz_x))} \right],
$}
\label{eq:contrast}
\end{equation}
where $\vz_x^j, \vz_s^j$ are randomly sampled 3D and spectra views regarding to the positive pair $(\vz_x, \vz_s)$. $f_x(\vz_x, \vz_s)$ and $f_s(\vz_s, \vz_x)$ are scoring functions for the two corresponding views, with flexible formulations. Here we adopt $f_x(\vz_x, \vz_s) = f_s(\vz_s, \vz_x) = \langle \vz_x, \vz_s \rangle$.

Note that the denoising objective can utilize any form from existing 3D molecular representation pre-training studies, enabling seamless integration of our method into these frameworks.

\subsection{Two-stage pre-training pipeline}\label{sec:two-stage}

Previous pre-training efforts for 3D molecular representation have been conducted on unlabeled datasets using denoising objective. These datasets typically provide only equilibrium 3D structures without offering spectra for all molecules. To enhance the pre-training effect by incorporating spectra while leveraging denoising pre-training, we employ a two-stage pre-training approach. The first stage involves training on a larger dataset~\citep{PCQM} without spectra using only the denoising objective. Subsequently, the second stage involves training on a dataset that includes spectra using the complete objective as follows:
\begin{equation}
    \mathcal{L} = \beta_{\text{Denoising}} \mathcal{L}_{\text{Denoising}} + \beta_{\text{MPR}} \mathcal{L}_{\text{MPR}} + \beta_{\text{Contrast}} \mathcal{L}_{\text{Contrast}},
    \label{eq:objective}
\end{equation}
where $\beta_{\text{Denoising}}$, $\beta_{\text{MPR}}$, and $\beta_{\text{Contrast}}$ denote the weights of each sub-objective.