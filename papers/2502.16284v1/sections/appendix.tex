\setcounter{theorem}{0}
% \setcounter{definition}{0}
\setcounter{table}{0}
\setcounter{figure}{0}
\setcounter{equation}{0}

\renewcommand{\thefigure}{A\arabic{figure}}
\renewcommand{\thetable}{A\arabic{table}}
\renewcommand{\theequation}{A\arabic{equation}}

\resumetocwriting

{
\centering
\LARGE
{\textbf{Appendix}}
\par
}

\renewcommand{\contentsname}{\normalsize Contents of the appendix}
{
  \hypersetup{linkcolor=black}
    \tableofcontents
}

\section{Proof of theoretical results}\label{appendix:proof}
\begin{theorem}[Equivalence between the denoising objective and the learning of molecular force fields~\citep{Coord}]
    Assume the conformation distribution is a mixture of Gaussian distribution centered at the equilibriums:
    \begin{equation} \small
    p(\vx) =\int p ( \vx | \vx_0)p(\vx_0),\ p (\vx|\vx_0)\sim \mathcal{N}(\vx_0,\tau^2 I_{3N})
    \end{equation}
    $\vx_0,\ \vx\in \mathbb{R}^{3N}$ are equilibrium and noisy conformation respectively, $N$ is the number of atoms in the molecule. It relates to molecular energy by Boltzmann distribution $p(\vx) \propto exp(-E(\vx))$.
    
    Then given a sampled molecule $\mathcal{M}$, the denoising loss on the conformation coordinates is an equivalent optimization target to force field prediction:
\textnormal{
\begin{align}\small
     \mathcal{L}_{\text{Denoising}} (\mathcal{M})& =\mathbb{E}_{p (\vx|\vx_0)p (\vx_0)}||\text{GNN}_{\theta} (\vx) - (\vx-\vx_0)||^2\label{eq:app coord loss} \\
    &\simeq  \mathbb{E}_{p (\vx)}||\text{GNN}_{\theta} (\vx) -(- \nabla _{\vx} E(\vx))||^2,\label{eq:app coord target}
\end{align}
}
where \textnormal{$\text{GNN}_{\theta} (\vx)$} denotes a graph neural network with parameters $\theta$ which takes conformation $\vx$ as an input and returns node-level noise predictions, $\simeq $ denotes equivalence. 
\end{theorem}
\begin{proof}
 According to Boltzmann distribution, \cref{eq:app coord target} is equal to $\mathbb{E}_{p (\vx)}||GNN_{\theta} (\vx) - \nabla _{\vx} \log p (\vx)||^2 $. By using a conditional score matching lemma~\citep{ScoreMatching-DAE}, the equation above is further equal to $\mathbb{E}_{p (\vx|\vx_0)p(\vx_0)}||GNN_{\theta} (\vx) - \nabla _{\vx} \log p (\vx|\vx_0)||^2+T_1$, where $T_1$ is constant independent of $\theta$. Then with the Gaussian assumption, it becomes $\mathbb{E}_{p (\vx|\vx_0)p(\vx_0)}||GNN_{\theta} (\vx) - \frac{\vx_0-\vx}{\tau_c^2}||^2+T_1 $. Finally, since coefficients $-\frac{1}{\tau^2}$ do not rely on the input $\vx$, it can be absorbed into $\text{GNN}_{\theta}$, thus obtaining \cref{eq:app coord loss}.
\end{proof}

\section{Visualization and analysis of spectra}

\begin{figure}[h]
\begin{center}
\includegraphics[width=\linewidth]{./figures/spectra_plots.pdf}
\end{center}
\caption{Randomly sampled examples of molecular energy spectra.}
\label{fig:visualization}
\end{figure}

In this section, we visualize the three types of spectra we utilize (UV-Vis, IR, Raman) and standardize the initial spectral data based on data analysis. In \cref{fig:visualization}, we visualize 20 randomly sampled spectra from QM9S for each type of spectrum. A notable pattern observed is that, although each spectrum consists of numerous absorption peaks, there are significant differences in the heights (absorption intensities) of these peaks. For instance, in the IR spectra, the absorption intensity at most peaks is around 200, but a few peaks reach an intensity of 800. However, in qualitative analysis, the position and shape of the peaks are more critical than their heights. Therefore, the differences in peak absorption intensities can interfere with model training under the MSE loss metric. To address this issue, we pre-process the absorption intensities of the spectra by applying a $\log_{10}$ transformation to mitigate the interference caused by peak intensity differences.


\section{Implementation details}\label{appendix:hyper-parameters}
\subsection{Hardware and software}
Our experiments are conducted on Linux servers equipped with
184 Intel Xeon Platinum 8469C CPUs,
920GB RAM, and 8 NVIDIA H20 96GB GPUs. Our model is implemented in PyTorch version 2.3.1, PyTorch Geometric version 2.5.3 (https://pyg.org/) with CUDA version 12.1, and Python 3.10.14. 

\subsection{Model configuration}\label{appendix:model-config}
The SpecFormer is implemented using a 3-layer Transformer with 16 attention heads.
Following previous works, we set both $d$ and $d_k$ as 256.
TorchMD-Net~\citep{TorchMD-Net} is adopted as the 3D molecular encoder.
We tune the mask ratio (i.e., $\alpha$) in \{0.05, 0.10, 0.15, 0.20, 0.25, 0.30\}, tune the ``stride/patch length" pair  (i.e., $D_i/P_i$) in \{5/20, 10/20, 15/20, 20/20, 8/16, 15/30\}, and tune the weights of sub-objectives (i.e., $\beta_{\text{Denoising}}$, $\beta_{\text{MPR}}$, and $\beta_{\text{Contrast}}$ ) in \{0.01, 0.1, 1, 10\}.
Since our goal is to align the 3D representations and spectra representations of molecules during the pre-training phase, and not rely on molecular spectra data during downstream fine-tuning, these hyper-parameters related to molecular spectra are tuned on the pre-training dataset.
Based on the results of hyper-parameter tuning, we adopt $\alpha=0.10, D_i=10, P_i=20, \beta_{\text{Denoising}}=1.0, \beta_{\text{MPR}}=1.0$, and $\beta_{\text{Contrast}}=1.0$.

Following SimCLR~\citep{SimCLR}, the contrastive loss in our \cref{eq:contrast} is implemented using in-batch contrastive loss, where positive and negative pairs are constructed within each data batch. Therefore, for each anchor representation in a batch, there is one positive sample and $bs-1$ negative samples, where $bs$ is the batch size. In our method, $bs=128$.

In both pre-training stages, we use the noise generation method and denoising objective provided by Coord~\citep{Coord}, specifically energy function \Romannum{1} as described in \cref{sec:denoising}. The noise is added to atom positions as scaled mixture of isotropic Gaussian noise, with a scaling factor of 0.04. The denoising objective is defined in \cref{eq:coord}.

For baselines, we follow their recommended settings.

\section{Limitations and potential future directions}
One limitation of our method is the availability, scale, and diversity of molecular spectral data. Our current dataset comprises geometric structures of 134,000 molecules, each with three types of spectra (UV-Vis, IR, Raman). To effectively explore the scaling laws of pre-training methods, larger and more diverse molecular spectral datasets are necessary. Encouragingly, molecular spectroscopy has been gaining increasing attention in the research community, with larger and more diverse datasets being released, such as the recent multimodal spectroscopic dataset~\citep{multimodal-spectra}. This development supports advancements in molecular representation learning and other related tasks.

Another limitation is that our proposed SpecFormer can currently only handle one-dimensional molecular spectra. For higher-dimensional spectra, such as two-dimensional NMR and two-dimensional correlation spectra, further development of sophisticated spectrum encoder is needed.

Looking ahead, we envision several future directions in this field. First, there is potential in investigating the scaling laws of pre-training on larger and more diverse molecular spectral datasets. Second, expanding the scope of molecular spectrum encoding to include a wider range, such as NMR, mass spectra, and two-dimensional spectra, could be highly beneficial. Third, while a pre-trained spectral encoder has been developed in our method, we have so far only applied the pre-trained 3D encoder to downstream tasks. Exploring the use of the pre-trained spectral encoder for molecular spectrum-related downstream tasks, such as automated molecular structure elucidation from spectra, represents an promising opportunity. Finally, current molecular 3D pre-training methods are designed based on TorchMD-Net~\citep{TorchMD-Net}. With the development of equivariant message passing neural networks, more expressive backbone architectures, such as Allegro~\citep{Allegro} and MACE~\citep{MACE} have been proposed, improving the prediction of molecular properties when trained from scratch. Extending pre-training strategies to these state-of-the-art architectures holds the promise of further advancing downstream tasks.

\section{More experimental results and discussions}

In addition to Coord, we evaluate the effect of incorporating SliDe into our \themodel.
SliDe~\citep{SliDe} is also a denoising-based pre-training method, utilizing the TorchMD-Net~\citep{TorchMD-Net} as its encoder backbone, consistent with previous pre-training work~\citep{Coord,Frad}. The results are presented in \cref{tab:more-exp} and \cref{tab:more-exp-2}.
\begin{table}[h]
\setlength{\tabcolsep}{4pt}
    \caption{Performance (MAE$\downarrow$) on QM9 dataset. The better result between the two variants of each pretraining method, w/ and w/o MolSpectra, is highlighted in bold.}
    \label{tab:more-exp}
    \begin{center}
    \begin{footnotesize}
    \scalebox{0.96}{
    \begin{tabular}{lccccccc}
    \toprule
    &  \makecell[c]{$\mu$}	&  \makecell[c]{$\alpha$ }		&  \makecell[c]{homo }		& \makecell[c]{lumo}		& \makecell[c]{gap}	& \makecell[c]{$R^2$ }	& \makecell[c]{ZPVE}
     \\
     &  makecell[c]{\scriptsize (D)}	&  \makecell[c]{\scriptsize ($a_0^3$)}		& \makecell[c]{\scriptsize  (meV)}		&  \makecell[c]{\scriptsize (meV)}		&  \makecell[c]{\scriptsize (meV)}	& \makecell[c]{\scriptsize($a_0^2$)}	&  \makecell[c]{\scriptsize (meV)}
     \\
    \midrule
    Coord &	0.016 &	0.052 &	17.7 &	14.7 &	31.8 &	0.450 &	\first{1.71}    \\  
    Coord w/ MolSpectra & \first{0.011} & \first{0.048} & \first{15.5} & 
    \first{13.1} & \first{26.8} & \first{0.410} & \first{1.71} \\
    \midrule
    SliDe &	0.015 &	0.050  & 18.7 &	16.2 &	28.8  &		0.606  &		1.78     \\  
    SliDe w/ MolSpectra & \first{0.012} & \first{0.043} & {\first{17.0}} & {\first{15.8}} & {\first{28.5}} & {\first{0.424}} & {\first{1.73}} \\
    \bottomrule
    \end{tabular}
    }
    \end{footnotesize}
    \end{center}
\end{table}

\begin{table}[h]
\setlength{\tabcolsep}{4pt}
    \caption{Performance (MAE$\downarrow$) on MD17 dataset. The better result between the two variants of each pretraining method, w/ and w/o MolSpectra, is highlighted in bold.}
    \label{tab:more-exp-2}
    \begin{center}
    \begin{footnotesize}
    \scalebox{0.96}{
    \begin{tabular}{lcccccccc}
    \toprule
    & Aspirin & Benzene & Ethanol & \makecell[c]{Malonal\\-dehyde} & \makecell[c]{Naphtha\\-lene} & \makecell[c]{Salicy\\-lic Acid} & Toluene & Uracil \\ 
    \midrule
    Coord & 	0.211  &  	{0.169}  &  	0.096  &  {0.139}  &  	\first{0.053}  &  	0.109  &  	\first{0.058}  &  	\first{0.074}\\ 
    Coord w/ MolSpectra &	\first{0.099} &		\first{0.097} &	\first{0.052} &	\first{0.077} &	{0.085} &	\first{0.093}	& {0.075} &	0.095\\
    \midrule
    SliDe & 	0.174  &  	{0.169}  &  	0.088  &  {0.154}  &  	\first{0.048}  &  	0.101  &  	\first{0.054}  &  	\first{0.083}\\ 
    SliDe w/ MolSpectra &	\first{0.160} &		\first{0.054} &	\first{0.055} &	\first{0.088} &	{0.073} &	\first{0.098}	& {0.077} &	0.097\\
    \bottomrule
    \end{tabular}
    }
    \end{footnotesize}
    \end{center}
\end{table}

Integrating our method with SliDe effectively reduces the error in property prediction on the QM9 dataset and the MD17 dataset. Given that our method enhances both Coord and SliDe, this suggests that our approach is broadly effective across various denoising-based pretraining strategies. Furthermore, incorporating molecular spectra can guide the pre-trained model to acquire knowledge beyond what denoising objectives can offer, which proves beneficial for downstream property prediction.


\section{Visualization of attention patterns and learned spectra representations in SpecFormer}
\begin{figure}[h]
\begin{center}
\includegraphics[width=0.9\linewidth]{./figures/visualization.pdf}
\end{center}
\caption{(a-c) Attention maps from three attention heads in SpecFormer. Different heads model distinct dependencies. (d) t-SNE visualization of the spectra representations produced by SpecFormer.}
\label{fig:vis}
\end{figure}
We visualize the attention patterns and learned spectra representations in SpecFormer. Based on the visualizations presented in \cref{fig:vis}, we have made the following observations.

In \cref{fig:vis}(a-c), we visualize attention maps from three attention heads in SpecFormer's second layer. 
The attention weights within the three blocks along the main diagonal indicate intra-spectrum dependencies, while those outside reveal inter-spectrum dependencies, as explained in \cref{sec:specformer}.
It can be observed that different attention heads model distinct dependencies: Head 11 focuses on intra-spectrum dependencies, Head 13 focuses on inter-spectrum dependencies, and Head 12 models both types simultaneously. In inter-spectrum dependencies, the interaction between IR spectra and Raman spectra is relatively pronounced, which may be related to their mutual association with vibrational modes. Additionally, because the intensity peaks and dependencies in molecular spectra are sparse, the attention maps in SpecFormer are generally sparse as well.

In \cref{fig:vis}(d), we use t-SNE to visualize the spectra representations produced by the final layer of SpecFormer. It can be observed that the distribution of representations in the latent space is relatively uniform and forms several potential clusters. This well-shaped distribution of representations reveals effective spectra representation learning and supports the structure-spectrum alignment.