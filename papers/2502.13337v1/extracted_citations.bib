@incollection{bonthu2021automated,
  title      = {Automated {Short} {Answer} {Grading} {Using} {Deep} {Learning}: {A} {Survey}},
  isbn       = {978-3-030-84059-4},
  shorttitle = {Automated {Short} {Answer} {Grading} {Using} {Deep} {Learning}},
  abstract   = {Automated Short Answer Grading (ASAG) is the task of assessing short answers authored by students by leveraging computational methods. The task of ASAG is investigated for many years, but this task continues to draw attention because of the associated research challenges. One of the core constraints of ASAG is the limited availability of domain-relevant training data. The task of ASAG can be tackled with several approaches and they can be broadly categorized into the traditional approaches based on handcrafted features and the Deep Learning based approaches. Researchers are applying Deep Learning Approaches for the past five years to address this problem owing to the increasing popularity of this area. The paper aims to summarize various existing deep learning approaches researchers followed to address this problem and to investigate whether Deep Learning based techniques are outperforming traditional approaches from the selected 38 papers. The paper also outlines several state-of-the-art datasets that can be used to do this work and the evaluation metrics to be used for both Regression and Classification settings.},
  author     = {Bonthu, Sridevi and Sripada, Rama Sree and Prasad, M.},
  month      = aug,
  year       = {2021},
  doi        = {10.1007/978-3-030-84060-0_5},
  pages      = {61--78},
  file       = {Full Text PDF:C\:\\Users\\victo\\Zotero\\storage\\K6NV9ZE3\\Bonthu et al. - 2021 - Automated Short Answer Grading Using Deep Learning.pdf:application/pdf}
}

@inproceedings{denny2024explaining,
author = {Denny, Paul and Smith, David H. and Fowler, Max and Prather, James and Becker, Brett A. and Leinonen, Juho},
title = {Explaining Code with a Purpose: An Integrated Approach for Developing Code Comprehension and Prompting Skills},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649217.3653587},
doi = {10.1145/3649217.3653587},
abstract = {Reading, understanding and explaining code have traditionally been important skills for novices learning programming. As large language models (LLMs) become prevalent, these foundational skills are more important than ever given the increasing need to understand and evaluate model-generated code. Brand new skills are also needed, such as the ability to formulate clear prompts that can elicit intended code from an LLM. Thus, there is great interest in integrating pedagogical approaches for the development of both traditional coding competencies and the novel skills required to interact with LLMs. One effective way to develop and assess code comprehension ability is with "Explain in plain English'' (EiPE) questions, where students succinctly explain the purpose of a fragment of code. However, grading EiPE questions has always been difficult given the subjective nature of evaluating written explanations and this has stifled their uptake. In this paper, we explore a natural synergy between EiPE questions and code-generating LLMs to overcome this limitation. We propose using an LLM to generate code based on students' responses to EiPE questions -- not only enabling EiPE responses to be assessed automatically, but helping students develop essential code comprehension and prompt crafting skills in parallel. We investigate this idea in an introductory programming course and report student success in creating effective prompts for solving EiPE questions. We also examine student perceptions of this activity and how it influences their views on the use of LLMs for aiding and assessing learning.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
pages = {283–289},
numpages = {7},
keywords = {code comprehension, cs1, eipe, explain in plan english, introductory programming, large language models, llms, prompting},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@inproceedings{duong2024automatic,
  title     = {Automatic {Grading} of {Short} {Answers} {Using} {Large} {Language} {Models} in {Software} {Engineering} {Courses}},
  url       = {https://ieeexplore.ieee.org/document/10578839},
  doi       = {10.1109/EDUCON60312.2024.10578839},
  abstract  = {Short-answer based questions have been used widely due to their effectiveness in assessing whether the desired learning outcomes have been attained by students. However, due to their open-ended nature, many different answers could be considered entirely or partially correct for the same question. In the context of computer science and software engineering courses where the enrolment has been increasing recently, manual grading of short-answer questions is a time-consuming and tedious process for instructors. In software engineering courses, assessments concern not just coding but many other aspects of software development such as system analysis, architecture design, software processes and operation methodologies such as Agile and DevOps. However, existing work in automatic grading/scoring of text-based answers in computing courses have been focusing more on coding-oriented questions. In this work, we consider the problem of autograding a broader range of short answers in software engineering courses. We propose an automated grading system incorporating both text embedding and completion approaches based on recently introduced pre-trained large language models (LLMs) such as GPT-3.5/4. We design and implement a web-based system so that students and instructors can easily leverage autograding for learning and teaching. Finally, we conduct an extensive evaluation of our automated grading approaches. We use a popular public dataset in the computing education domain and a new software engineering dataset of our own. The results demonstrate the effectiveness of our approach, and provide useful insights for further research in this area of AI-enabled education.},
  urldate   = {2025-01-02},
  booktitle = {2024 {IEEE} {Global} {Engineering} {Education} {Conference} ({EDUCON})},
  author    = {Duong, Ta Nguyen Binh and Meng, Chai Yi},
  month     = may,
  year      = {2024},
  note      = {ISSN: 2165-9567},
  keywords  = {automatic grading, Computer science, Costs, embedding, Encoding, Focusing, large language models, Large language models, short answers, Software, software engineering courses, Training},
  pages     = {1--10},
  file      = {Full Text PDF:C\:\\Users\\victo\\Zotero\\storage\\M2Y5ESNC\\Duong and Meng - 2024 - Automatic Grading of Short Answers Using Large Lan.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\victo\\Zotero\\storage\\TWLIWGM2\\10578839.html:text/html}
}

@article{impey2025using,
  title    = {Using {Large} {Language} {Models} for {Automated} {Grading} of {Student} {Writing} about {Science}},
  issn     = {1560-4292, 1560-4306},
  url      = {http://arxiv.org/abs/2412.18719},
  doi      = {10.1007/s40593-024-00453-7},
  abstract = {Assessing writing in large classes for formal or informal learners presents a significant challenge. Consequently, most large classes, particularly in science, rely on objective assessment tools such as multiple-choice quizzes, which have a single correct answer. The rapid development of AI has introduced the possibility of using large language models (LLMs) to evaluate student writing. An experiment was conducted using GPT-4 to determine if machine learning methods based on LLMs can match or exceed the reliability of instructor grading in evaluating short writing assignments on topics in astronomy. The audience consisted of adult learners in three massive open online courses (MOOCs) offered through Coursera. One course was on astronomy, the second was on astrobiology, and the third was on the history and philosophy of astronomy. The results should also be applicable to non-science majors in university settings, where the content and modes of evaluation are similar. The data comprised answers from 120 students to 12 questions across the three courses. GPT-4 was provided with total grades, model answers, and rubrics from an instructor for all three courses. In addition to evaluating how reliably the LLM reproduced instructor grades, the LLM was also tasked with generating its own rubrics. Overall, the LLM was more reliable than peer grading, both in aggregate and by individual student, and approximately matched instructor grades for all three online courses. The implication is that LLMs may soon be used for automated, reliable, and scalable grading of student science writing.},
  urldate  = {2025-01-26},
  journal  = {Int J Artif Intell Educ},
  author   = {Impey, Chris and Wenger, Matthew and Garuda, Nikhil and Golchin, Shahriar and Stamer, Sarah},
  month    = jan,
  year     = {2025},
  note     = {arXiv:2412.18719 [cs]},
  keywords = {Computer Science - Computation and Language},
  file     = {Preprint PDF:C\:\\Users\\victo\\Zotero\\storage\\53R4JCXF\\Impey et al. - 2025 - Using Large Language Models for Automated Grading .pdf:application/pdf;Snapshot:C\:\\Users\\victo\\Zotero\\storage\\4IHKPFBP\\2412.html:text/html}
}

@misc{kortemeyer2023performance,
  title     = {Performance of the {Pre}-{Trained} {Large} {Language} {Model} {GPT}-4 on {Automated} {Short} {Answer} {Grading}},
  url       = {http://arxiv.org/abs/2309.09338},
  doi       = {10.48550/arXiv.2309.09338},
  abstract  = {Automated Short Answer Grading (ASAG) has been an active area of machine-learning research for over a decade. It promises to let educators grade and give feedback on free-form responses in large-enrollment courses in spite of limited availability of human graders. Over the years, carefully trained models have achieved increasingly higher levels of performance. More recently, pre-trained Large Language Models (LLMs) emerged as a commodity, and an intriguing question is how a general-purpose tool without additional training compares to specialized models. We studied the performance of GPT-4 on the standard benchmark 2-way and 3-way datasets SciEntsBank and Beetle, where in addition to the standard task of grading the alignment of the student answer with a reference answer, we also investigated withholding the reference answer. We found that overall, the performance of the pre-trained general-purpose GPT-4 LLM is comparable to hand-engineered models, but worse than pre-trained LLMs that had specialized training.},
  urldate   = {2024-12-04},
  publisher = {arXiv},
  author    = {Kortemeyer, Gerd},
  month     = sep,
  year      = {2023},
  note      = {arXiv:2309.09338 [cs]},
  keywords  = {Computer Science - Computation and Language},
  file      = {Preprint PDF:C\:\\Users\\victo\\Zotero\\storage\\QGIYCZXE\\Kortemeyer - 2023 - Performance of the Pre-Trained Large Language Mode.pdf:application/pdf;Snapshot:C\:\\Users\\victo\\Zotero\\storage\\3IGAS2A6\\2309.html:text/html}
}

@inproceedings{meyer2024combined,
  address    = {New York, NY, USA},
  series     = {{SIGCSE} {Virtual} 2024},
  title      = {{ASAG2024}: {A} {Combined} {Benchmark} for {Short} {Answer} {Grading}},
  isbn       = {9798400706042},
  shorttitle = {{ASAG2024}},
  url        = {https://dl.acm.org/doi/10.1145/3649409.3691083},
  doi        = {10.1145/3649409.3691083},
  abstract   = {Open-ended questions test a more thorough understanding compared to closed-ended questions and are often a preferred assessment method. However, open-ended questions are tedious to grade and subject to personal bias. Therefore, there have been efforts to speed up the grading process through automation. Short Answer Grading (SAG) systems aim to automatically score students' answers in examinations. Despite growth in SAG methods and capabilities, there exists no comprehensive short-answer grading benchmark across different subjects, grading scales, and distributions. Thus, it is hard to assess the capabilities of current automated grading methods in terms of their generalizability. In this preliminary work, we introduce the combined ASAG2024 benchmark to facilitate the comparison of automated grading systems. Combining seven commonly used short-answer grading datasets in a common structure and grading scale. For our benchmark, we evaluate a set of recent SAG methods, revealing that while LLM-based approaches reach new high scores, they still are far from reaching human performance. This opens up avenues for future research on human-machine SAG systems.},
  urldate    = {2025-01-08},
  booktitle  = {Proceedings of the 2024 on {ACM} {Virtual} {Global} {Computing} {Education} {Conference} {V}. 2},
  publisher  = {Association for Computing Machinery},
  author     = {Meyer, Gérôme and Breuer, Philip and Fürst, Jonathan},
  month      = dec,
  year       = {2024},
  pages      = {322--323},
  file       = {Full Text PDF:C\:\\Users\\victo\\Zotero\\storage\\ZHRAW3ID\\Meyer et al. - 2024 - ASAG2024 A Combined Benchmark for Short Answer Gr.pdf:application/pdf}
}

@inproceedings{senanayake2024rubric-asag,
  title     = {Rubric {Based} {Automated} {Short} {Answer} {Scoring} using {Large} {Language} {Models} ({LLMs})},
  volume    = {7},
  url       = {https://ieeexplore.ieee.org/abstract/document/10550624},
  doi       = {10.1109/SCSE61872.2024.10550624},
  abstract  = {The manual grading of short answers presents challenges in education due to time constraints, especially with larger student populations, and suffers from subjectivity and bias, leading to inconsistencies. Larger student populations increase the time needed for individual assessment, leading to potential delays and subjectivity introduces biases, resulting in inconsistent evaluations. Moreover, as student numbers rise, the imbalance in teacher-to-student ratios affects grading quality, impacting fairness and effectiveness in educational assessments. Automated grading systems have emerged as a solution to address these issues. These grading systems mainly prioritize appearance, emphasizing grammar and format. However, they struggle to accurately assess content quality, often missing contextual relevance. This problem can potentially be resolved by employing highly trained domain-specific models. However, a drawback arises as these models are limited to evaluating answers only within predefined domains. While these specialized models excel in assessing responses within their designated fields, their utility is restricted when evaluating answers outside of these predefined domains. This limitation poses a challenge in achieving broader applicability for assessing answers outside the specific areas the models were trained for. This study proposes a rubric-based method paired with Large Language Models (LLMs) to introduce objectivity, ensuring fairness and reliability in evaluations while achieving generalizability. Rubric provides a clear and customizable marking schema for assessing short answers across various domains. By using predetermined marking criteria and conditions, the grading process becomes more objective and transparent. The proposed method efficiently evaluates short answers in various domains using Large Language Models (LLMs), based on these established criteria, reducing subjective biases. This research aims to revolutionize education by creating a robust automated short answer scoring system that comprehensively evaluates contents across domains and addresses teacher-to-student ratio issues.},
  urldate   = {2024-12-04},
  booktitle = {2024 {International} {Research} {Conference} on {Smart} {Computing} and {Systems} {Engineering} ({SCSE})},
  author    = {Senanayake, Chamuditha and Asanka, Dinesh},
  month     = apr,
  year      = {2024},
  note      = {ISSN: 2613-8662},
  keywords  = {Education, Automated short answer grading, Computational modeling, Delays, Grammar, Large Language Models, Manuals, Rubric-based, Sociology, Time factors},
  pages     = {1--6},
  file      = {Full Text PDF:C\:\\Users\\victo\\Zotero\\storage\\INW5U3E3\\Senanayake and Asanka - 2024 - Rubric Based Automated Short Answer Scoring using .pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\victo\\Zotero\\storage\\MGNTANCF\\10550624.html:text/html}
}

@inproceedings{smith2024evaluating,
author = {Smith, David H. and Zilles, Craig},
title = {Evaluating Large Language Model Code Generation as an Autograding Mechanism for "Explain in Plain English" Questions},
year = {2024},
isbn = {9798400704246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626253.3635542},
doi = {10.1145/3626253.3635542},
abstract = {The ability of students to ''Explain in Plain English'' (EiPE) the purpose of code is a critical skill for students in introductory programming courses to develop. EiPE questions serve as both a mechanism for students to develop and demonstrate code comprehension skills. However, evaluating this skill has been challenging as manual grading is time consuming and not easily automated. The process of constructing a prompt for the purposes of code generation for a Large Language Model, such OpenAI's GPT-4, bears a striking resemblance to constructing EiPE responses. In this paper, we explore the potential of using test cases run on code generated by GPT-4 from students' EiPE responses as a grading mechanism for EiPE questions. We applied this proposed grading method to a corpus of EiPE responses collected from past exams, then measured agreement between the results of this grading method and human graders. Overall, we find moderate agreement between the human raters and the results of the unit tests run on the generated code. This appears to be attributable to GPT-4's code generation being more lenient than human graders on low-level descriptions of code.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1824–1825},
numpages = {2},
keywords = {autograding, eipe, gpt-4, large language models},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

