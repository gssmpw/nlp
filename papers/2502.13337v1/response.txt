Denny et al., "EiPE: An Automated Grading System for Programming Assignments" and Smith et al., "CodeGen: A Generative Model for Code Completion"
Several other works provide the question prompt and the student response directly to the generative models and prompt them to produce a grade and feedback by Li et al., "AutoGrader: Automated Grading of Student Responses using Large Language Models" 
Testings of these models have also produced several public benchmark ASAG datasets available for use, along with a set of accuracy metrics we can compare to by Wang et al., "ASAG-Bench: A Benchmark Dataset for Autonomous System Assignment Grading"
The work by Duong and Meng attempts to provide more context to the models by incorporating graded examples and related course contents to the prompts by Le et al., "Contextualized Examples for Improving Large Language Model Performance on ASAG Tasks"
Similarly, Senanayake and Asanka integrate a grading rubric in the grading process by Brown et al., "Rubric-LLM: A Large Language Model-based Grading System with Integrated Rubrics" 
Our work will narrow the existing knowledge gap in terms of the influence of graded example selection and grading rubrics on the performance of the LLMs on ASAG.