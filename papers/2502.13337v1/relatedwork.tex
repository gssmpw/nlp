\section{Related Work}
% I think here we should mention in more details what was done that is similar to our work, potentially mentioning the work that we use for comparison in this paper with a couple of sentences. We can highlight, byt comparing to previous work, how ours addresses an existing gap. It will be also easier to introduce the "Data" in the next section, instead of just throwing the datasets without much context.


% Most of the more recent ASAG work leverages the generative models such as ChatGPT for grades and feedback. For example, the EiPE autograder by Denny et al.~\cite{denny2024explaining} and Smith et al.~\cite{smith2024evaluating} generates code based on students' high-level explanation of some program and runs the code through a code autograder. 
% Several other works provide the question prompt and the student response directly to the generative models and prompt them to produce a grade and feedback~\cite{impey2025using,kortemeyer2023performance,meyer2024combined}. 
% Testings of these models have also produced several public benchmark ASAG datasets available for use, along with a set of accuracy metrics we can compare to~\cite{bonthu2021automated}. 
% Although these existing results are mostly satisfying, little emphasis has been put on techniques to futhermore improve the accuracy. 
% The work by Duong and Meng attempts to provide more context to the models by incorporating graded examples and related course contents to the prompts~\cite{duong2024automatic}. However, there is no comparison on different methods of selecting these graded examples in that work. 
% Similarly, Senanayake and Asanka integrate a grading rubric in the grading process~\cite{senanayake2024rubric-asag}, but does not include a full analysis on the effect of the rubric. 
% Our work will narrow the existing knowledge gap in terms of the influence of graded example selection and grading rubrics on the performance of the LLMs on ASAG.