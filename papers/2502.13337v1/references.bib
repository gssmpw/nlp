

@article{ringer2019qed,
  url     = {http://dx.doi.org/10.1561/2500000045},
  year    = {2019},
  volume  = {5},
  journal = {Foundations and Trends® in Programming Languages},
  title   = {QED at Large: A Survey of Engineering of Formally Verified Software},
  doi     = {10.1561/2500000045},
  issn    = {2325-1107},
  number  = {2-3},
  pages   = {102-281},
  author  = {Talia Ringer and Karl Palmskog and Ilya Sergey and Milos Gligoric and
             Zachary Tatlock}
}

@misc{sundaram2022nlp,
  title         = {Why are NLP Models Fumbling at Elementary Math? A Survey of Deep
                   Learning based Word Problem Solvers},
  author        = {Sowmya S Sundaram and Sairam Gurajada and Marco Fisichella and
                   Deepak P and Savitha Sam Abraham},
  year          = {2022},
  eprint        = {2205.15683},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{patel2021nlp,
  title         = {Are NLP Models really able to Solve Simple Math Word Problems?},
  author        = {Arkil Patel and Satwik Bhattamishra and Navin Goyal},
  year          = {2021},
  eprint        = {2103.07191},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@inproceedings{lerner2015polymorphic,
  title     = {Polymorphic blocks: Formalism-inspired UI for structured connectors},
  author    = {Lerner, Sorin and Foster, Stephen R and Griswold, William G},
  booktitle = {Proceedings of the 33rd Annual ACM Conference on Human Factors in 
               Computing Systems},
  pages     = {3063--3072},
  year      = {2015}
}

@article{sivaraman2022data,
  author     = {Sivaraman, Aishwarya and Sanchez-Stern, Alex and Chen, Bretton and
                Lerner, Sorin and Millstein, Todd},
  title      = {Data-Driven Lemma Synthesis for Interactive Proofs},
  year       = {2022},
  issue_date = {October 2022},
  publisher  = {ACM},
  address    = {NY, NY, USA},
  volume     = {6},
  number     = {OOPSLA2},
  url        = {https://doi.org/10.1145/3563306},
  doi        = {10.1145/3563306},
  abstract   = {Interactive proofs of theorems often require auxiliary helper lemmas
                to prove the desired theorem. Existing approaches for automatically synthesizing
                helper lemmas fall into two broad categories. Some approaches are goal-directed,
                producing lemmas specifically to help a user make progress from a given proof
                state, but they have limited expressiveness in terms of the lemmas that can be
                produced. Other approaches are highly expressive, able to generate arbitrary
                lemmas from a given grammar, but they are completely undirected and hence not
                amenable to interactive usage. In this paper, we develop an approach to lemma
                synthesis that is both goal-directed and expressive. The key novelty is a
                technique for reducing lemma synthesis to a data-driven program synthesis problem,
                whereby examples for synthesis are generated from the current proof state. We also
                describe a technique to systematically introduce new variables for lemma
                synthesis, as well as techniques for filtering and ranking candidate lemmas for
                presentation to the user. We implement these ideas in a tool called lfind, which
                can be run as a Coq tactic. In an evaluation on four benchmark suites, lfind
                produces useful lemmas in 68% of the cases where a human prover used a lemma to
                %make progress. In these cases lfind synthesizes a lemma that either enables a
                %fully automated proof of the original goal or that matches the human-provided
                %lemma.},
  journal    = {Proc. ACM Program. Lang.},
  month      = {oct},
  articleno  = {143},
  numpages   = {27},
  keywords   = {Type-Directed Synthesis, Interactive Theorem Proving, Lemma Synthesis,
                Data-Driven Synthesis}
}

@misc{zhang2023getting,
  title         = {Getting More out of Large Language Models for Proofs},
  author        = {Shizhuo Dylan Zhang and Talia Ringer and Emily First},
  year          = {2023},
  eprint        = {2305.04369},
  archiveprefix = {arXiv},
  primaryclass  = {cs.FL}
}

@article{first2020tactok,
  author     = {First, Emily and Brun, Yuriy and Guha, Arjun},
  title      = {TacTok: Semantics-Aware Proof Synthesis},
  year       = {2020},
  issue_date = {November 2020},
  publisher  = {ACM},
  address    = {NY, NY, USA},
  volume     = {4},
  number     = {OOPSLA},
  url        = {https://doi.org/10.1145/3428299},
  doi        = {10.1145/3428299},
  abstract   = {Formally verifying software correctness is a highly manual process.
                However, because verification proof scripts often share structure, it is possible
                to learn from existing proof scripts to fully automate some formal verification.
                The goal of this paper is to improve proof script synthesis and enable fully
                automating more verification. Interactive theorem provers, such as the Coq proof
                assistant, allow programmers to write partial proof scripts, observe the semantics
                of the proof state thus far, and then attempt more progress. Knowing the proof
                state semantics is a significant aid. Recent research has shown that the proof
                state can help predict the next step. In this paper, we present TacTok, the first
                technique that attempts to fully automate proof script synthesis by modeling proof
                scripts using both the partial proof script written thus far and the semantics of
                the proof state. Thus, TacTok more completely models the information the
                programmer has access to when writing proof scripts manually. We evaluate TacTok
                on a benchmark of 26 software projects in Coq, consisting of over 10 thousand
                theorems. We compare our approach to five tools. Two prior techniques, CoqHammer,
                the state-of-the-art proof synthesis technique, and ASTactic, a proof script
                synthesis technique that models proof state. And three new proof script synthesis
                technique we create ourselves, SeqOnly, which models only the partial proof script
                and the initial theorem being proven, and WeightedRandom and WeightedGreedy, which
                use metaheuristic search biased by frequencies of proof tactics in existing,
                successful proof scripts. We find that TacTok outperforms WeightedRandom and
                WeightedGreedy, and is complementary to CoqHammer and ASTactic: for 24 out of the
                26 projects, TacTok can synthesize proof scripts for some theorems the prior tools
                cannot. Together with TacTok, 11.5% more theorems can be proven automatically than
                %by CoqHammer alone, and 20.0% than by ASTactic alone. Compared to a combination
                %%of
                %%CoqHammer and ASTactic, TacTok can prove an additional 3.6% more theorems,
                %%%proving
                %%%115 theorems no tool could previously prove. Overall, our experiments provide
                %%%evidence that partial proof script and proof state semantics, together, provide
                %%%useful information for proof script modeling, and that metaheuristic search is
                %%%a
                %%%promising direction for proof script synthesis. TacTok is open-source and we
                %%%make
                %%%public all our data and a replication package of our experiments.},
  journal    = {Proc. ACM Program. Lang.},
  month      = {nov},
  articleno  = {231},
  numpages   = {31},
  keywords   = {automated proof script synthesis, proof script synthesis, formal
                software verification, Coq}
}

@misc{openai2023gpt4,
  title         = {GPT-4 Technical Report},
  author        = {OpenAI},
  year          = {2023},
  eprint        = {2303.08774},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{brown2020gpt3,
  title     = {Language {Models} are {Few}-{Shot} {Learners}},
  url       = {http://arxiv.org/abs/2005.14165},
  abstract  = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  urldate   = {2024-01-10},
  publisher = {arXiv},
  author    = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  month     = jul,
  year      = {2020},
  note      = {arXiv:2005.14165 [cs]},
  keywords  = {Computer Science - Computation and Language},
  file      = {arXiv.org Snapshot:C\:\\Users\\victo\\Zotero\\storage\\DHWXWMGZ\\2005.html:text/html;Full Text PDF:C\:\\Users\\victo\\Zotero\\storage\\DE8V3AXK\\Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf}
}

@article{sanchez-stern2020generating,
  title    = {Generating {Correctness} {Proofs} with {Neural} {Networks}},
  abstract = {Foundational verification allows programmers to build software
              which
              has been empirically shown to have high levels of assurance in a variety of
              important domains. However, the cost of producing foundationally verified
              software remains prohibitively high for most projects, as it requires
              significant manual effort by highly trained experts. In this paper we present
              Proverbot9001, a proof search system using machine learning techniques to
              produce proofs of software correctness in interactive theorem provers. We
              demonstrate Proverbot9001 on the proof obligations from a large practical proof
              project, the CompCert verified C compiler, and show that it can effectively
              automate what were previously manual proofs, automatically producing proofs for
              28\% of theorem statements in our test dataset, when combined with solver-based
              tooling. Without any additional solvers, we exhibit a proof completion rate
              that
              is a 4X improvement over prior state-of-the-art machine learning models for
              generating proofs in Coq.},
  language = {en},
  author   = {Sanchez-Stern, Alex and Alhessi, Yousef and Saul, Lawrence and
              Lerner,
              Sorin},
  year     = {2020},
  pages    = {14},
  file     = {Sanchez-Stern et al. - 2020 - Generating Correctness Proofs with Neural
              Networks.pdf:/home/seth/Zotero/storage/R4RPPIUW/Sanchez-Stern et al. - 2020 -
              Generating Correctness Proofs with Neural Networks.pdf:application/pdf}
}

@inproceedings{poulsen2023efficiency,
  title     = {Efficiency of Learning from Proof Blocks Versus Writing Proofs},
  author    = {Poulsen, Seth and Gertner, Yael and Cosman, Benjamin and West, Matthew and Herman, Geoffrey L},
  booktitle = {Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 1},
  pages     = {472--478},
  year      = {2023}
}

@inproceedings{poulsen2023efficiencyanon,
  title     = {Efficiency of Learning from Proof Blocks Versus Writing Proofs},
  author    = {Anonymous Authors},
  booktitle = {Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 1},
  pages     = {472--478},
  year      = {2023}
}

@inproceedings{poulsen2024disentangling,
  title     = {Disentangling the Learning Gains from Reading a Book Chapter and Completing Proof
               Blocks Problems},
  author    = {Poulsen, Seth and Gertner, Yael and Chen, Hongxuan  and Cosman, Benjamin and West, Matthew and Herman, Geoffrey L},
  booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
  year      = {2024}
}

@inproceedings{poulsen2024disentanglinganon,
  title     = {Disentangling the Learning Gains from Reading a Book Chapter and Completing Proof
               Blocks Problems},
  author    = {Anonymous Authors},
  booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
  year      = {2024}
}

@inproceedings{poulsen2024solving,
  title     = {Solving Proof Block Problems Using Large Language Models},
  author    = {Poulsen, Seth and Sarsa, Sami and Prather, James and Leinonen, Juho and Becker, Brett A and Hellas, Arto and Denny, Paul and Reeves, Brent N},
  booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
  year      = {2024}
}

@misc{poulsen2023measuring,
  title         = {Measuring the Impact of Distractors on Student Learning Gains while Using Proof Blocks},
  author        = {Seth Poulsen and Hongxuan Chen and Yael Gertner and Benjamin Cosman and Matthew West and Geoffrey L Herman},
  year          = {2023},
  eprint        = {2311.00792},
  archiveprefix = {arXiv},
  primaryclass  = {cs.HC}
}

@misc{poulsen2023measuringanon,
  title         = {Measuring the Impact of Distractors on Student Learning Gains while Using Proof Blocks},
  author        = {Anonymous Authors},
  year          = {2023},
  eprint        = {2311.00792},
  archiveprefix = {arXiv},
  primaryclass  = {cs.HC}
}

@inproceedings{binglin2022eipe,
  author    = {Chen, Binglin and West, Matthew and Zilles, Craig},
  title     = {Peer-Grading "Explain in Plain English": A Bayesian Calibration Method for Categorical Answers},
  year      = {2022},
  isbn      = {9781450390705},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3478431.3499409},
  doi       = {10.1145/3478431.3499409},
  abstract  = {"Explain in plain English'' (EipE) questions have been proposed as an important activity and assessment for studying novice programmers' grasp of programming knowledge and their ability to communicate their understanding. However, EipE questions aren't widely used in introductory programming courses in part because of the large grading effort required. In this paper, we present our experience of using peer grading for EipE questions in a large-enrollment introductory programming course, where students were asked to categorize other students' responses. We developed a novel Bayesian algorithm for performing calibrated peer grading on categorical data, and we used a heuristic grade assignment method based on the Bayesian estimates. The peer-grading exercises served both as a way to coach students on what is expected from EipE questions and as a way to alleviate the grading load for the course staff. Based on four rounds of peer-grading activities, we found that students are generally capable of categorizing responses to EiPE questions and that our proposed Bayesian method is more robust than unweighted voting.},
  booktitle = {Proceedings of the 53rd ACM Technical Symposium on Computer Science Education - Volume 1},
  pages     = {133–139},
  numpages  = {7},
  keywords  = {cs1, explain in plain english, eipe, peer grading, experience report},
  location  = {Providence, RI, USA},
  series    = {SIGCSE 2022}
}

@inproceedings{seth2022proofblock,
  title      = {Proof {Blocks}: {Autogradable} {Scaffolding} {Activities} for {Learning} to {Write} {Proofs}},
  shorttitle = {Proof {Blocks}},
  url        = {http://arxiv.org/abs/2106.11032},
  doi        = {10.1145/3502718.3524774},
  abstract   = {Proof Blocks is a software tool which enables students to write proofs by dragging and dropping prewritten proof lines into the correct order. These proofs can be graded completely automatically, enabling students to receive rapid feedback on how they are doing with their proofs. When constructing a problem, the instructor specifies the dependency graph of the lines of the proof, so that any correct arrangement of the lines can receive full credit. This innovation can improve assessment tools by increasing the types of questions we can ask students about proofs, and can give greater access to proof knowledge by increasing the amount that students can learn on their own with the help of a computer.},
  urldate    = {2023-09-09},
  booktitle  = {Proceedings of the 27th {ACM} {Conference} on {Innovation} and {Technology} in {Computer} {Science} {Education} {Vol}. 1},
  author     = {Poulsen, Seth and Viswanathan, Mahesh and Herman, Geoffrey L. and West, Matthew},
  month      = jul,
  year       = {2022},
  note       = {arXiv:2106.11032 [cs]},
  keywords   = {Computer Science - Computers and Society, Computer Science - Software Engineering},
  pages      = {428--434},
  file       = {arXiv.org Snapshot:files/373/2106.html:text/html;Full Text PDF:files/374/Poulsen et al. - 2022 - Proof Blocks Autogradable Scaffolding Activities .pdf:application/pdf}
}

@inproceedings{lan2015symbolic,
  address    = {New York, NY, USA},
  series     = {L@{S} '15},
  title      = {Mathematical {Language} {Processing}: {Automatic} {Grading} and {Feedback} for {Open} {Response} {Mathematical} {Questions}},
  isbn       = {978-1-4503-3411-2},
  shorttitle = {Mathematical {Language} {Processing}},
  url        = {https://dl.acm.org/doi/10.1145/2724660.2724664},
  doi        = {10.1145/2724660.2724664},
  abstract   = {While computer and communication technologies have provided effective means to scale up many aspects of education, the submission and grading of assessments such as homework assignments and tests remains a weak link. In this paper, we study the problem of automatically grading the kinds of open response mathematical questions that figure prominently in STEM (science, technology, engineering, and mathematics) courses. Our data-driven framework for mathematical language processing (MLP) leverages solution data from a large number of learners to evaluate the correctness of their solutions, assign partial-credit scores, and provide feedback to each learner on the likely locations of any errors. MLP takes inspiration from the success of natural language processing for text data and comprises three main steps. First, we convert each solution to an open response mathematical question into a series of numerical features. Second, we cluster the features from several solutions to uncover the structures of correct, partially correct, and incorrect solutions. We develop two different clustering approaches, one that leverages generic clustering algorithms and one based on Bayesian nonparametrics. Third, we automatically grade the remaining (potentially large number of) solutions based on their assigned cluster and one instructor-provided grade per cluster. As a bonus, we can track the cluster assignment of each step of a multistep solution and determine when it departs from a cluster of correct solutions, which enables us to indicate the likely locations of errors to learners. We test and validate MLP on real-world MOOC data to demonstrate how it can substantially reduce the human effort required in large-scale educational platforms.},
  urldate    = {2023-11-15},
  booktitle  = {Proceedings of the {Second} (2015) {ACM} {Conference} on {Learning} @ {Scale}},
  publisher  = {Association for Computing Machinery},
  author     = {Lan, Andrew S. and Vats, Divyanshu and Waters, Andrew E. and Baraniuk, Richard G.},
  month      = mar,
  year       = {2015},
  keywords   = {assessment, automatic grading, bayesian nonparametrics, clustering, feedback, machine learning, mathematical language processing},
  pages      = {167--176},
  file       = {Full Text PDF:C\:\\Users\\victo\\Zotero\\storage\\2VFRQ37D\\Lan et al. - 2015 - Mathematical Language Processing Automatic Gradin.pdf:application/pdf}
}

@incollection{bonthu2021automated,
  title      = {Automated {Short} {Answer} {Grading} {Using} {Deep} {Learning}: {A} {Survey}},
  isbn       = {978-3-030-84059-4},
  shorttitle = {Automated {Short} {Answer} {Grading} {Using} {Deep} {Learning}},
  abstract   = {Automated Short Answer Grading (ASAG) is the task of assessing short answers authored by students by leveraging computational methods. The task of ASAG is investigated for many years, but this task continues to draw attention because of the associated research challenges. One of the core constraints of ASAG is the limited availability of domain-relevant training data. The task of ASAG can be tackled with several approaches and they can be broadly categorized into the traditional approaches based on handcrafted features and the Deep Learning based approaches. Researchers are applying Deep Learning Approaches for the past five years to address this problem owing to the increasing popularity of this area. The paper aims to summarize various existing deep learning approaches researchers followed to address this problem and to investigate whether Deep Learning based techniques are outperforming traditional approaches from the selected 38 papers. The paper also outlines several state-of-the-art datasets that can be used to do this work and the evaluation metrics to be used for both Regression and Classification settings.},
  author     = {Bonthu, Sridevi and Sripada, Rama Sree and Prasad, M.},
  month      = aug,
  year       = {2021},
  doi        = {10.1007/978-3-030-84060-0_5},
  pages      = {61--78},
  file       = {Full Text PDF:C\:\\Users\\victo\\Zotero\\storage\\K6NV9ZE3\\Bonthu et al. - 2021 - Automated Short Answer Grading Using Deep Learning.pdf:application/pdf}
}

@misc{vaswani2023attention,
  title     = {Attention {Is} {All} {You} {Need}},
  url       = {http://arxiv.org/abs/1706.03762},
  doi       = {10.48550/arXiv.1706.03762},
  abstract  = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  urldate   = {2023-11-26},
  publisher = {arXiv},
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  month     = aug,
  year      = {2023},
  note      = {arXiv:1706.03762 [cs]},
  keywords  = {Computer Science - Computation and Language, Computer Science - Machine Learning},
  file      = {arXiv Fulltext PDF:C\:\\Users\\victo\\Zotero\\storage\\EG42BWWA\\Vaswani et al. - 2023 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\victo\\Zotero\\storage\\KVAEJN8F\\1706.html:text/html}
}

@misc{haller2022NLP,
  title      = {Survey on {Automated} {Short} {Answer} {Grading} with {Deep} {Learning}: from {Word} {Embeddings} to {Transformers}},
  shorttitle = {Survey on {Automated} {Short} {Answer} {Grading} with {Deep} {Learning}},
  url        = {http://arxiv.org/abs/2204.03503},
  doi        = {10.48550/arXiv.2204.03503},
  abstract   = {Automated short answer grading (ASAG) has gained attention in education as a means to scale educational tasks to the growing number of students. Recent progress in Natural Language Processing and Machine Learning has largely influenced the field of ASAG, of which we survey the recent research advancements. We complement previous surveys by providing a comprehensive analysis of recently published methods that deploy deep learning approaches. In particular, we focus our analysis on the transition from hand engineered features to representation learning approaches, which learn representative features for the task at hand automatically from large corpora of data. We structure our analysis of deep learning methods along three categories: word embeddings, sequential models, and attention-based methods. Deep learning impacted ASAG differently than other fields of NLP, as we noticed that the learned representations alone do not contribute to achieve the best results, but they rather show to work in a complementary way with hand-engineered features. The best performance are indeed achieved by methods that combine the carefully hand-engineered features with the power of the semantic descriptions provided by the latest models, like transformers architectures. We identify challenges and provide an outlook on research direction that can be addressed in the future},
  urldate    = {2023-11-15},
  publisher  = {arXiv},
  author     = {Haller, Stefan and Aldea, Adina and Seifert, Christin and Strisciuglio, Nicola},
  month      = mar,
  year       = {2022},
  note       = {arXiv:2204.03503 [cs]},
  keywords   = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
  file       = {arXiv Fulltext PDF:C\:\\Users\\victo\\Zotero\\storage\\REWMMYQW\\Haller et al. - 2022 - Survey on Automated Short Answer Grading with Deep.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\victo\\Zotero\\storage\\D7A4NFV5\\2204.html:text/html}
}

@misc{kiros2015skip-thought,
  title     = {Skip-{Thought} {Vectors}},
  url       = {http://arxiv.org/abs/1506.06726},
  doi       = {10.48550/arXiv.1506.06726},
  abstract  = {We describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, we train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next introduce a simple vocabulary expansion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. After training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice. We will make our encoder publicly available.},
  urldate   = {2023-11-26},
  publisher = {arXiv},
  author    = {Kiros, Ryan and Zhu, Yukun and Salakhutdinov, Ruslan and Zemel, Richard S. and Torralba, Antonio and Urtasun, Raquel and Fidler, Sanja},
  month     = jun,
  year      = {2015},
  note      = {arXiv:1506.06726 [cs]},
  keywords  = {Computer Science - Computation and Language, Computer Science - Machine Learning},
  file      = {arXiv Fulltext PDF:C\:\\Users\\victo\\Zotero\\storage\\949ASE95\\Kiros et al. - 2015 - Skip-Thought Vectors.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\victo\\Zotero\\storage\\MB75MUZG\\1506.html:text/html}
}

@inproceedings{breitner2016incredible,
  title    = {Visual {Theorem} {Proving} with the {Incredible} {Proof} {Machine}},
  isbn     = {978-3-319-43143-7},
  doi      = {10.1007/978-3-319-43144-4_8},
  abstract = {The Incredible Proof Machine is an easy and fun to use program to conduct formal proofs. It employs a novel, intuitive proof representation based on port graphs, which is akin to, but even more natural than, natural deduction. In particular, we describe a way to determine the scope of local assumptions and variables implicitly. Our practical classroom experience backs these claims.},
  author   = {Breitner, Joachim},
  month    = aug,
  year     = {2016},
  pages    = {123--139}
}

@inproceedings{bornat1997jape,
  address    = {Berlin, Heidelberg},
  series     = {Lecture {Notes} in {Computer} {Science}},
  title      = {Jape: {A} calculator for animating proof-on-paper},
  isbn       = {978-3-540-69140-2},
  shorttitle = {Jape},
  doi        = {10.1007/3-540-63104-6_41},
  abstract   = {If you suppose that it would be pointless to simulate proof-on-paper; if you imagine that all the problems of interactive theorem-proving are solved; if you are sure that making a user interface is a matter of bolting a bit of Tcl/Tk onto a theorem-proving engine; if you believe the more buttons the better in a graphical user interfaces — read no further, lest your prejudices be disturbed!},
  language   = {en},
  booktitle  = {Automated {Deduction}—{CADE}-14},
  publisher  = {Springer},
  author     = {Bornat, Richard and Sufrin, Bernard},
  editor     = {McCune, William},
  year       = {1997},
  pages      = {412--415},
  file       = {Full Text PDF:C\:\\Users\\victo\\Zotero\\storage\\S75ESPX6\\Bornat and Sufrin - 1997 - Jape A calculator for animating proof-on-paper.pdf:application/pdf}
}

@misc{azerbayev2023llemma,
  title      = {Llemma: {An} {Open} {Language} {Model} {For} {Mathematics}},
  shorttitle = {Llemma},
  url        = {http://arxiv.org/abs/2310.10631},
  doi        = {10.48550/arXiv.2310.10631},
  abstract   = {We present Llemma, a large language model for mathematics. We continue pretraining Code Llama on the Proof-Pile-2, a mixture of scientific papers, web data containing mathematics, and mathematical code, yielding Llemma. On the MATH benchmark Llemma outperforms all known open base models, as well as the unreleased Minerva model suite on an equi-parameter basis. Moreover, Llemma is capable of tool use and formal theorem proving without any further finetuning. We openly release all artifacts, including 7 billion and 34 billion parameter models, the Proof-Pile-2, and code to replicate our experiments.},
  urldate    = {2023-11-29},
  publisher  = {arXiv},
  author     = {Azerbayev, Zhangir and Schoelkopf, Hailey and Paster, Keiran and Santos, Marco Dos and McAleer, Stephen and Jiang, Albert Q. and Deng, Jia and Biderman, Stella and Welleck, Sean},
  month      = oct,
  year       = {2023},
  note       = {arXiv:2310.10631 [cs]},
  keywords   = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Logic in Computer Science},
  file       = {arXiv Fulltext PDF:C\:\\Users\\victo\\Zotero\\storage\\82GKGRER\\Azerbayev et al. - 2023 - Llemma An Open Language Model For Mathematics.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\victo\\Zotero\\storage\\GQCU9VWU\\2310.html:text/html}
}

@misc{shen2023mathbert,
  title      = {{MathBERT}: {A} {Pre}-trained {Language} {Model} for {General} {NLP} {Tasks} in {Mathematics} {Education}},
  shorttitle = {{MathBERT}},
  url        = {http://arxiv.org/abs/2106.07340},
  doi        = {10.48550/arXiv.2106.07340},
  abstract   = {Since the introduction of the original BERT (i.e., BASE BERT), researchers have developed various customized BERT models with improved performance for specific domains and tasks by exploiting the benefits of transfer learning. Due to the nature of mathematical texts, which often use domain specific vocabulary along with equations and math symbols, we posit that the development of a new BERT model for mathematics would be useful for many mathematical downstream tasks. In this resource paper, we introduce our multi-institutional effort (i.e., two learning platforms and three academic institutions in the US) toward this need: MathBERT, a model created by pre-training the BASE BERT model on a large mathematical corpus ranging from pre-kindergarten (pre-k), to high-school, to college graduate level mathematical content. In addition, we select three general NLP tasks that are often used in mathematics education: prediction of knowledge component, auto-grading open-ended Q\&A, and knowledge tracing, to demonstrate the superiority of MathBERT over BASE BERT. Our experiments show that MathBERT outperforms prior best methods by 1.2-22\% and BASE BERT by 2-8\% on these tasks. In addition, we build a mathematics specific vocabulary 'mathVocab' to train with MathBERT. We discover that MathBERT pre-trained with 'mathVocab' outperforms MathBERT trained with the BASE BERT vocabulary (i.e., 'origVocab'). MathBERT is currently being adopted at the participated leaning platforms: Stride, Inc, a commercial educational resource provider, and ASSISTments.org, a free online educational platform. We release MathBERT for public usage at: https://github.com/tbs17/MathBERT.},
  urldate    = {2023-08-17},
  publisher  = {arXiv},
  author     = {Shen, Jia Tracy and Yamashita, Michiharu and Prihar, Ethan and Heffernan, Neil and Wu, Xintao and Graff, Ben and Lee, Dongwon},
  month      = aug,
  year       = {2023},
  note       = {arXiv:2106.07340 [cs]},
  keywords   = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
  file       = {arXiv Fulltext PDF:C\:\\Users\\victo\\Zotero\\storage\\LJHI2IKI\\Shen et al. - 2023 - MathBERT A Pre-trained Language Model for General.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\victo\\Zotero\\storage\\P7GKSINR\\2106.html:text/html}
}

@inproceedings{adam2019torch,
  title     = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  author    = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  booktitle = {Advances in Neural Information Processing Systems 32},
  pages     = {8024--8035},
  year      = {2019},
  publisher = {Curran Associates, Inc.},
  address   = {Vancuver, Canada},
  url       = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@inproceedings{goldman2008identifying,
  address   = {New York, NY, USA},
  series    = {{SIGCSE} '08},
  title     = {Identifying important and difficult concepts in introductory computing courses using a delphi process},
  isbn      = {978-1-59593-799-5},
  url       = {https://dl.acm.org/doi/10.1145/1352135.1352226},
  doi       = {10.1145/1352135.1352226},
  abstract  = {A Delphi process is a structured multi-step process that uses a group of experts to achieve a consensus opinion. We present the results of three Delphi processes to identify topics that are important and difficult in each of three introductory computing subjects: discrete math, programming fundamentals, and logic design. The topic rankings can be used to guide both the coverage of standardized tests of student learning (i.e., concept inventories) and can be used by instructors to identify what topics merit emphasis.},
  urldate   = {2023-12-05},
  booktitle = {Proceedings of the 39th {SIGCSE} technical symposium on {Computer} science education},
  publisher = {Association for Computing Machinery},
  author    = {Goldman, Ken and Gross, Paul and Heeren, Cinda and Herman, Geoffrey and Kaczmarczyk, Lisa and Loui, Michael C. and Zilles, Craig},
  month     = mar,
  year      = {2008},
  keywords  = {concept inventory, curriculum, delphi, discrete math, logic design, programming fundamentals},
  pages     = {256--260},
  file      = {Full Text PDF:C\:\\Users\\victo\\Zotero\\storage\\BEMUDUMC\\Goldman et al. - 2008 - Identifying important and difficult concepts in in.pdf:application/pdf}
}

@article{hake1998interactive-engagement,
  title      = {Interactive-engagement versus traditional methods: {A} six-thousand-student survey of mechanics test data for introductory physics courses},
  volume     = {66},
  issn       = {0002-9505},
  shorttitle = {Interactive-engagement versus traditional methods},
  url        = {https://doi.org/10.1119/1.18809},
  doi        = {10.1119/1.18809},
  abstract   = {A survey of pre/post-test data using the Halloun–Hestenes Mechanics Diagnostic test or more recent Force Concept Inventory is reported for 62 introductory physics courses enrolling a total number of students N=6542. A consistent analysis over diverse student populations in high schools, colleges, and universities is obtained if a rough measure of the average effectiveness of a course in promoting conceptual understanding is taken to be the average normalized gain 〈g〉. The latter is defined as the ratio of the actual average gain (\%〈post〉−\%〈pre〉) to the maximum possible average gain (100−\%〈pre〉). Fourteen “traditional” (T) courses (N=2084) which made little or no use of interactive-engagement (IE) methods achieved an average gain 〈g〉T-ave=0.23±0.04 (std dev). In sharp contrast, 48 courses (N=4458) which made substantial use of IE methods achieved an average gain 〈g〉IE-ave=0.48±0.14 (std dev), almost two standard deviations of 〈g〉IE-ave above that of the traditional courses. Results for 30 (N=3259) of the above 62 courses on the problem-solving Mechanics Baseline test of Hestenes–Wells imply that IE strategies enhance problem-solving ability. The conceptual and problem-solving test results strongly suggest that the classroom use of IE methods can increase mechanics-course effectiveness well beyond that obtained in traditional practice.},
  number     = {1},
  urldate    = {2023-10-22},
  journal    = {American Journal of Physics},
  author     = {Hake, Richard R.},
  month      = jan,
  year       = {1998},
  pages      = {64--74},
  file       = {Hake - 1998 - Interactive-engagement versus traditional methods.pdf:C\:\\Users\\victo\\Zotero\\storage\\ABMLV7RR\\Hake - 1998 - Interactive-engagement versus traditional methods.pdf:application/pdf;Snapshot:C\:\\Users\\victo\\Zotero\\storage\\JM39Y4UQ\\Interactive-engagement-versus-traditional-methods.html:text/html}
}

@article{braun2017math-active,
  title    = {What {Does} {Active} {Learning} {Mean} {For} {Mathematicians}?},
  volume   = {64},
  issn     = {0002-9920, 1088-9477},
  url      = {http://www.ams.org/notices/201702/rnoti-p124.pdf},
  doi      = {10.1090/noti1472},
  language = {en},
  number   = {02},
  urldate  = {2023-12-05},
  journal  = {Notices Amer. Math. Soc.},
  author   = {Braun, Benjamin and Bremser, Priscilla and Duval, Art M. and Lockwood, Elise and White, Diana},
  month    = feb,
  year     = {2017},
  pages    = {124--129},
  file     = {Braun et al. - 2017 - What Does Active Learning Mean For Mathematicians.pdf:C\:\\Users\\victo\\Zotero\\storage\\CEI6ANV2\\Braun et al. - 2017 - What Does Active Learning Mean For Mathematicians.pdf:application/pdf}
}

@incollection{stylianides2016proof,
  title     = {Proof and argumentation in mathematics education research},
  author    = {Stylianides, Andreas J and Bieda, Kristen N and Morselli, Francesca},
  booktitle = {The second handbook of research on the psychology of mathematics education},
  pages     = {315--351},
  year      = {2016},
  publisher = {Brill Sense}
}

@incollection{stylianides2017research,
  title     = {Research on the teaching and learning of proof: Taking stock and moving forward},
  author    = {Stylianides, GJ and Stylianides, AJ and Weber, K},
  year      = {2017},
  publisher = {National Council of Teachers of Mathematics},
  address   = {Reston, VA},
  booktitle = {Compendium for Research in Mathematics Education},
  editor    = {Cai, Jinfa},
  chapter   = {10},
  pages     = {237-266}
}

@incollection{selden2008overcoming,
  title     = {Overcoming {Students}' {Difficulties} in {Learning} to {Understand} and {Construct} {Proofs}},
  author    = {Selden, Annie and Selden, John},
  booktitle = {Making the {Connection}},
  year      = {2008},
  publisher = {The Mathematical Association of America},
  isbn      = {978-0-88385-975-9},
  editor    = {Carlson, Marilyn P. and Rasmussen, Chris},
  doi       = {10.5948/UPO9780883859759.009},
  pages     = {95--110},
  language  = {en}
}

@article{hodds2014self,
  title     = {Self-explanation training improves proof comprehension},
  author    = {Hodds, Mark and Alcock, Lara and Inglis, Matthew},
  journal   = {Journal for Research in Mathematics Education},
  volume    = {45},
  number    = {1},
  pages     = {62--101},
  year      = {2014},
  publisher = {National Council of Teachers of Mathematics}
}

@article{malek2011effect,
  title     = {The effect of using transparent pseudo-proofs in linear algebra},
  author    = {Malek, Aliza and Movshovitz-Hadar, Nitsa},
  journal   = {Research in Mathematics Education},
  volume    = {13},
  number    = {1},
  pages     = {33--58},
  year      = {2011},
  publisher = {Taylor \& Francis}
}

@phdthesis{roy2014evaluating,
  type       = {thesis},
  title      = {Evaluating novel pedagogy in higher education: a case study of
                e-proofs},
  shorttitle = {Evaluating novel pedagogy in higher education},
  abstract   = {This thesis is a single case study of the introduction and
                evaluation of new resources and new technologies in higher education; in which
                e-Proof was chosen as a single case. E-proofs are a multimedia representation of
                proofs, were created by Alcock (2009), and aimed to help undergraduates to read
                proofs for better proof comprehension. My thesis aimed to investigate whether
                the impact of reading such technology-based resource, e-Proofs, on
                undergraduates proof comprehension was better compared to reading written
                textbook proofs and if so, then why (or why not). To evaluate the effectiveness
                of e-Proofs, I used both qualitative and quantitative methods. First I measured
                undergraduates satisfaction, which is a most common research practice in
                evaluation studies, by using self-reporting methods such as web-based survey and
                interviews. A web-based survey and focus-group interviews showed that
                undergraduates liked to have e-Proofs and they believed that e-Proofs had
                positive impact on their proof comprehension. However, their positive views on
                e-Proofs did not evidence the educational impact of e-Proofs. I conducted an
                interview with Alcock for better understanding of her intentions of creating
                e-Proof and her expectations from it. Next, I conducted the first experiment
                which compared the impact of reading an e-Proof with a written textbook proof on
                undergraduates proof comprehension. Their comprehension was measured with an
                open-ended comprehension test twice immediately after reading the proof and
                after two weeks. I found that the immediate impact of reading an e-Proof and a
                textbook proof were essentially the same, however the long term impact of
                reading an e-Proof was worse than reading a textbook proof (for both high and
                low achieving undergraduates). This leads to the second experiment in which I
                investigated how undergraduates read e-Proofs and textbook proofs. In the second
                experiment, participants eye-movements were recorded while read- ing proofs, to
                explore their reading comprehension processes. This eye-tracking experiment
                showed that undergraduates had a sense of understanding of how to read a proof
                without any additional help. Rather, additional help allowed them to take a back
                seat and to devote less cognitive effort than they would otherwise. Moreover,
                e-Proofs altered undergraduates reading behaviours in a way which can harm
                learning. In sum, this thesis contributes knowledge into the area of reading and
                compre- hending proofs at undergraduate level and presents a methodology for
                evaluation studies of new pedagogical tools.},
  language   = {en},
  urldate    = {2021-03-02},
  school     = {Loughborough University},
  author     = {Roy, Somali},
  month      = jan,
  year       = {2014},
  file       = {Full Text PDF:/home/seth/Zotero/storage/FBUKUE8Q/Roy - 2014 - Evaluating
                novel pedagogy in higher education a
                c.pdf:application/pdf;Snapshot:/home/seth/Zotero/storage/3I2U6GC5/9374297.html:text/html}
}

@article{brown2014skepticism,
  title    = {On skepticism and its role in the development of proof in the
              classroom},
  volume   = {86},
  issn     = {1573-0816},
  url      = {https://doi.org/10.1007/s10649-014-9544-4},
  doi      = {10.1007/s10649-014-9544-4},
  abstract = {The purpose of this paper is to examine students’ development of a
              capacity to maintain doubt, against a backdrop of empirical evidence.
              Specifically, drawing on data from clinical interviews and a series of teaching
              experiments, this paper describes two pathways, the Experiential Pathway and the
              Cultural Non-Experiential Pathway, for the development of the mathematical
              disposition of engaging in skepticism towards empirical validations. Issues
              related to current ways of framing students’ views of empirical evidence and the
              role of pragmatic forms of doubt are considered.},
  language = {en},
  number   = {3},
  urldate  = {2022-01-19},
  journal  = {Educational Studies in Mathematics},
  author   = {Brown, Stacy A.},
  month    = jul,
  year     = {2014},
  pages    = {311--335},
  file     = {Springer Full Text PDF:/home/seth/Zotero/storage/5EAWULEA/Brown - 2014 -
              On skepticism and its role in the development of p.pdf:application/pdf}
}

@article{jahnke2013understanding,
  title      = {Understanding what a proof is: a classroom-based approach},
  volume     = {45},
  issn       = {1863-9690, 1863-9704},
  shorttitle = {Understanding what a proof is},
  url        = {http://link.springer.com/10.1007/s11858-013-0502-x},
  doi        = {10.1007/s11858-013-0502-x},
  abstract   = {Being unaware of the assumptions underlying a deductive argument is
                widespread among learners and is a major stumbling block to their understanding
                of proof. Thus, the basic idea of the present paper is that at some points in
                the course of secondary education there should be classroom-based interventions
                addressing this difﬁculty and making the axiomatic organization of mathematics a
                theme. Students should be made aware that there are axioms in mathematics, what
                their role is and how mathematicians come to agree about which axioms should be
                accepted. An axiom which is not yet accepted is simply a hypothesis. A
                hypothesis is evaluated by deductively drawing consequences and by investigating
                whether these consequences agree with experience or should be accepted for other
                reasons. The teaching intervention discussed in this paper exempliﬁes this idea
                by way of the example of ancient attempts at modelling the path of the sun, the
                socalled ‘‘anomaly of the sun’’. It is investigated to what extent the teaching
                intervention fostered students’ understanding of the conditionality of
                mathematical/astronomical statements, that is, of the fact that the truth of
                these statements is dependent on the initial hypotheses.},
  language   = {en},
  number     = {3},
  urldate    = {2022-01-18},
  journal    = {ZDM},
  author     = {Jahnke, Hans Niels and Wambach, Ralf},
  month      = may,
  year       = {2013},
  pages      = {469--482},
  file       = {Jahnke and Wambach - 2013 - Understanding what a proof is a
                classroom-based a.pdf:/home/seth/Zotero/storage/4GPRC5U2/Jahnke and Wambach -
                2013 - Understanding what a proof is a classroom-based a.pdf:application/pdf}
}

@article{stylianides2009facilitating,
  title    = {Facilitating the {Transition} from {Empirical} {Arguments} to {Proof}},
  volume   = {40},
  issn     = {0021-8251},
  url      = {https://www.jstor.org/stable/40539339},
  abstract = {Although students of all levels of education face serious
              difficulties with proof, there is limited research knowledge about how
              instruction can help students overcome these difficulties. In this article, we
              discuss the theoretical foundation and implementation of an instructional
              sequence that aimed to help students begin to realize the limitations of
              empirical arguments as methods for validating mathematical generalizations and
              see an intellectual need to learn about secure methods for validation (i.e.,
              proofs). The development of the instructional sequence was part of a 4-year
              design experiment that we conducted in an undergraduate mathematics course,
              prerequisite for admission to an elementary (Grades K-6) teaching certification
              program. We focus on the implementation of the instructional sequence in the
              last of 5 research cycles of our design experiment to exemplify our theoretical
              framework (in which cognitive conflict played a major role) and to discuss the
              promise of the sequence to support the intended learning goals.},
  number   = {3},
  urldate  = {2022-01-18},
  journal  = {Journal for Research in Mathematics Education},
  author   = {Stylianides, Gabriel J. and Stylianides, Andreas J.},
  year     = {2009},
  note     = {Publisher: National Council of Teachers of Mathematics},
  pages    = {314--352}
}

@inproceedings{harel2001development,
  title        = {The development of mathematical induction as a proof scheme: A model
                  for DNR-based instruction},
  author       = {Harel, Guershon},
  booktitle    = {In S. Campbell \& R. Zaskis (Eds.). Learning and Teaching Number 
                  Theory, Journal of Mathematical
                  Behavior},
  year         = {2001},
  organization = {Citeseer}
}

@article{larsen2008proofs,
  title    = {Proofs and refutations in the undergraduate mathematics classroom},
  volume   = {67},
  issn     = {1573-0816},
  url      = {https://doi.org/10.1007/s10649-007-9106-0},
  doi      = {10.1007/s10649-007-9106-0},
  abstract = {In his 1976 book, Proofs and Refutations, Lakatos presents a
              collection of case studies to illustrate methods of mathematical discovery in
              the history of mathematics. In this paper, we reframe these methods in ways that
              we have found make them more amenable for use as a framework for research on
              learning and teaching mathematics. We present an episode from an undergraduate
              abstract algebra classroom to illustrate the guided reinvention of mathematics
              through processes that strongly parallel those described by Lakatos. Our
              analysis suggests that the constructs described by Lakatos can provide a useful
              framework for making sense of the mathematical activity in classrooms where
              students are actively engaged in the development of mathematical ideas and
              provide design heuristics for instructional approaches that support the learning
              of mathematics through the process of guided reinvention.},
  language = {en},
  number   = {3},
  urldate  = {2022-01-19},
  journal  = {Educational Studies in Mathematics},
  author   = {Larsen, Sean and Zandieh, Michelle},
  month    = mar,
  year     = {2008},
  pages    = {205--216},
  file     = {Springer Full Text PDF:/home/seth/Zotero/storage/ZHTQZAQ6/Larsen and
              Zandieh - 2008 - Proofs and refutations in the undergraduate
              mathem.pdf:application/pdf}
}

@article{anderson1995cognitive,
  title     = {Cognitive tutors: Lessons learned},
  author    = {Anderson, John R and Corbett, Albert T and Koedinger, Kenneth R and
               Pelletier, Ray},
  journal   = {The journal of the learning sciences},
  volume    = {4},
  number    = {2},
  pages     = {167--207},
  year      = {1995},
  publisher = {Taylor \& Francis}
}

@book{acm2013curriculum,
  author    = {Joint Task Force on Computing Curricula, Association for Computing Machinery (ACM) and IEEE Computer Society},
  title     = {Computer Science Curricula 2013: Curriculum Guidelines for Undergraduate Degree Programs in Computer Science},
  year      = {2013},
  isbn      = {9781450323093},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA}
}

@techreport{acm2014curriculum,
  author    = {The Joint Task Force on Computing Curricula},
  title     = {Curriculum Guidelines for Undergraduate Degree Programs in Software Engineering},
  year      = {2014},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA}
}

@techreport{acm2016curriculum,
  author    = {The Joint Task Force on Computing Curricula, Association for Computing Machinery (ACM) and IEEE Computer Society},
  title     = {Curriculum Guidelines for
               Undergraduate Degree Programs
               in Computer Engineering},
  year      = {2016},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA}
}

@misc{devlin2019bert,
  title      = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
  shorttitle = {{BERT}},
  url        = {http://arxiv.org/abs/1810.04805},
  abstract   = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  urldate    = {2023-09-09},
  publisher  = {arXiv},
  author     = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  month      = may,
  year       = {2019},
  note       = {arXiv:1810.04805 [cs]},
  keywords   = {Computer Science - Computation and Language},
  file       = {arXiv.org Snapshot:C\:\\Users\\victo\\Zotero\\storage\\XSE9WPVV\\1810.html:text/html;Full Text PDF:C\:\\Users\\victo\\Zotero\\storage\\DTKLPZHJ\\Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf}
}

@misc{touvron2023llama,
  title      = {Llama 2: {Open} {Foundation} and {Fine}-{Tuned} {Chat} {Models}},
  shorttitle = {Llama 2},
  url        = {http://arxiv.org/abs/2307.09288},
  abstract   = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.},
  urldate    = {2024-01-10},
  publisher  = {arXiv},
  author     = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
  month      = jul,
  year       = {2023},
  note       = {arXiv:2307.09288 [cs]},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
  file       = {arXiv.org Snapshot:C\:\\Users\\victo\\Zotero\\storage\\E9LA76DU\\2307.html:text/html;Full Text PDF:C\:\\Users\\victo\\Zotero\\storage\\BHMKIAHT\\Touvron et al. - 2023 - Llama 2 Open Foundation and Fine-Tuned Chat Model.pdf:application/pdf}
}

@article{rumelhart1986learning,
  title     = {Learning representations by back-propagating errors},
  volume    = {323},
  copyright = {1986 Springer Nature Limited},
  issn      = {1476-4687},
  url       = {https://www.nature.com/articles/323533a0},
  doi       = {10.1038/323533a0},
  abstract  = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
  language  = {en},
  number    = {6088},
  urldate   = {2024-01-10},
  journal   = {Nature},
  author    = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  month     = oct,
  year      = {1986},
  note      = {Number: 6088
               Publisher: Nature Publishing Group},
  keywords  = {Humanities and Social Sciences, multidisciplinary, Science},
  pages     = {533--536},
  file      = {Full Text PDF:C\:\\Users\\victo\\Zotero\\storage\\5WPV2RV2\\Rumelhart et al. - 1986 - Learning representations by back-propagating error.pdf:application/pdf}
}

@misc{mikolov2013efficient,
  title         = {Efficient Estimation of Word Representations in Vector Space},
  author        = {Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
  year          = {2013},
  eprint        = {1301.3781},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{anonymous,
  author       = {[Anonymous]},
  title        = {Title of the Source},
  howpublished = {Retrieved from \url{https://example.com}},
  note         = {Author unknown},
  year         = {n.d.}
}

@misc{openaiembeddingsdocs,
  author = {OpenAI},
  year   = {2023},
  title  = {Embeddings},
  url    = {https://platform.openai.com/docs/guides/embeddings/what-are-embeddings},
  note   = {Accessed: January 2024}
}

@inproceedings{azad2020strategies,
  title        = {Strategies for deploying unreliable AI graders in high-transparency 
                  high-stakes exams},
  author       = {Azad, Sushmita and Chen, Binglin and Fowler, Maxwell and West, Matthew 
                  and Zilles, Craig},
  booktitle    = {Artificial Intelligence in Education: 21st International Conference, 
                  AIED 2020, Ifrane, Morocco, July 6--10, 2020, Proceedings, Part I 21},
  pages        = {16--28},
  year         = {2020},
  organization = {Springer}
}

@data{poulsen2024student,
  author    = {Poulsen, Seth},
  publisher = {Harvard Dataverse},
  title     = {{Student Proof by Induction Data Set}},
  unf       = {UNF:6:VF1vo7TLfeix8Cmjo/VVGw==},
  year      = {2024},
  version   = {V1},
  doi       = {10.7910/DVN/OTRLXF},
  url       = {https://doi.org/10.7910/DVN/OTRLXF}
}

@article{vaswani2017attention,
  title   = {Attention is all you need},
  author  = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob 
             and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal = {Advances in neural information processing systems},
  volume  = {30},
  year    = {2017}
}

@inproceedings{fowler2021eipe,
  address   = {Virtual Event USA},
  title     = {Autograding "{Explain} in {Plain} {English}" questions using {NLP}},
  isbn      = {978-1-4503-8062-1},
  url       = {https://dl.acm.org/doi/10.1145/3408877.3432539},
  doi       = {10.1145/3408877.3432539},
  abstract  = {Previous research suggests that “Explain in Plain English” (EiPE) code reading activities could play an important role in the development of novice programmers, but EiPE questions aren’t heavily used in introductory programming courses because they (traditionally) required manual grading. We present what we believe to be the first automatic grader for EiPE questions and its deployment in a large-enrollment introductory programming course. Based on a set of questions deployed on a computer-based exam, we find that our implementation has an accuracy of 87–89\%, which is similar in performance to course teaching assistants trained to perform this task and compares favorably to automatic short answer grading algorithms developed for other domains. In addition, we briefly characterize the kinds of answers that the current autograder fails to score correctly and the kinds of errors made by students.},
  language  = {en},
  urldate   = {2023-12-19},
  booktitle = {Proceedings of the 52nd {ACM} {Technical} {Symposium} on {Computer} {Science} {Education}},
  publisher = {ACM},
  author    = {Fowler, Max and Chen, Binglin and Azad, Sushmita and West, Matthew and Zilles, Craig},
  month     = mar,
  year      = {2021},
  pages     = {1163--1169},
  file      = {Fowler et al. - 2021 - Autograding Explain in Plain English questions u.pdf:C\:\\Users\\victo\\Zotero\\storage\\VZ5SRQ33\\Fowler et al. - 2021 - Autograding Explain in Plain English questions u.pdf:application/pdf}
}

@misc{wu2022autoformalization,
  title     = {Autoformalization with {Large} {Language} {Models}},
  url       = {http://arxiv.org/abs/2205.12615},
  doi       = {10.48550/arXiv.2205.12615},
  abstract  = {Autoformalization is the process of automatically translating from natural language mathematics to formal specifications and proofs. A successful autoformalization system could advance the fields of formal verification, program synthesis, and artificial intelligence. While the long-term goal of autoformalization seemed elusive for a long time, we show large language models provide new prospects towards this goal. We make the surprising observation that LLMs can correctly translate a significant portion (\$25.3{\textbackslash}\%\$) of mathematical competition problems perfectly to formal specifications in Isabelle/HOL. We demonstrate the usefulness of this process by improving a previously introduced neural theorem prover via training on these autoformalized theorems. Our methodology results in a new state-of-the-art result on the MiniF2F theorem proving benchmark, improving the proof rate from \$29.6{\textbackslash}\%\$ to \$35.2{\textbackslash}\%\$.},
  urldate   = {2024-03-25},
  publisher = {arXiv},
  author    = {Wu, Yuhuai and Jiang, Albert Q. and Li, Wenda and Rabe, Markus N. and Staats, Charles and Jamnik, Mateja and Szegedy, Christian},
  month     = may,
  year      = {2022},
  note      = {arXiv:2205.12615 [cs]},
  keywords  = {Computer Science - Artificial Intelligence, Computer Science - Logic in Computer Science, Computer Science - Machine Learning, Computer Science - Software Engineering},
  file      = {arXiv Fulltext PDF:C\:\\Users\\victo\\Zotero\\storage\\J86GN3E8\\Wu et al. - 2022 - Autoformalization with Large Language Models.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\victo\\Zotero\\storage\\GB8DIBQF\\2205.html:text/html}
}

@article{shute2008feedback,
  author   = {Valerie J. Shute},
  title    = {Focus on Formative Feedback},
  journal  = {Review of Educational Research},
  volume   = {78},
  number   = {1},
  pages    = {153-189},
  year     = {2008},
  doi      = {10.3102/0034654307313795},
  url      = {https://doi.org/10.3102/0034654307313795},
  eprint   = {https://doi.org/10.3102/0034654307313795},
  abstract = { This article reviews the corpus of research on feedback, with a focus on formative feedback—defined as information communicated to the learner that is intended to modify his or her thinking or behavior to improve learning. According to researchers, formative feedback should be nonevaluative, supportive, timely, and specific. Formative feedback is usually presented as information to a learner in response to some action on the learner’s part. It comes in a variety of types (e.g., verification of response accuracy, explanation of the correct answer, hints, worked examples) and can be administered at various times during the learning process (e.g., immediately following an answer, after some time has elapsed). Finally, several variables have been shown to interact with formative feedback’s success at promoting learning (e.g., individual characteristics of the learner and aspects of the task). All of these issues are discussed. This review concludes with guidelines for generating formative feedback. }
}

@article{kleij2015effects,
  title      = {Effects of {Feedback} in a {Computer}-{Based} {Learning} {Environment} on {Students}' {Learning} {Outcomes}: {A} {Meta}-{Analysis}},
  volume     = {85},
  issn       = {0034-6543},
  shorttitle = {Effects of {Feedback} in a {Computer}-{Based} {Learning} {Environment} on {Students}' {Learning} {Outcomes}},
  url        = {https://doi.org/10.3102/0034654314564881},
  doi        = {10.3102/0034654314564881},
  abstract   = {In this meta-analysis, we investigated the effects of methods for providing item-based feedback in a computer-based environment on students' learning outcomes. From 40 studies, 70 effect sizes were computed, which ranged from -0.78 to 2.29. A mixed model was used for the data analysis. The results show that elaborated feedback (EF; e.g., providing an explanation) produced larger effect sizes (0.49) than feedback regarding the correctness of the answer (KR; 0.05) or providing the correct answer (KCR; 0.32). EF was particularly more effective than KR and KCR for higher order learning outcomes. Effect sizes were positively affected by EF feedback, and larger effect sizes were found for mathematics compared with social sciences, science, and languages. Effect sizes were negatively affected by delayed feedback timing and by primary and high school. Although the results suggested that immediate feedback was more effective for lower order learning than delayed feedback and vice versa, no significant interaction was found.},
  language   = {en},
  number     = {4},
  urldate    = {2024-02-26},
  journal    = {Review of Educational Research},
  author     = {Van der Kleij, Fabienne M. and Feskens, Remco C. W. and Eggen, Theo J. H. M.},
  month      = dec,
  year       = {2015},
  note       = {Publisher: American Educational Research Association},
  pages      = {475--511},
  file       = {SAGE PDF Full Text:C\:\\Users\\victo\\Zotero\\storage\\62G4FJVF\\Van der Kleij et al. - 2015 - Effects of Feedback in a Computer-Based Learning E.pdf:application/pdf}
}

@inproceedings{lee2021who,
  address    = {New York, NY, USA},
  series     = {{CHI} '21},
  title      = {Who {Is} {Included} in {Human} {Perceptions} of {AI}?: {Trust} and {Perceived} {Fairness} around {Healthcare} {AI} and {Cultural} {Mistrust}},
  isbn       = {978-1-4503-8096-6},
  shorttitle = {Who {Is} {Included} in {Human} {Perceptions} of {AI}?},
  url        = {https://dl.acm.org/doi/10.1145/3411764.3445570},
  doi        = {10.1145/3411764.3445570},
  abstract   = {Emerging research suggests that people trust algorithmic decisions less than human decisions. However, different populations, particularly in marginalized communities, may have different levels of trust in human decision-makers. Do people who mistrust human decision-makers perceive human decisions to be more trustworthy and fairer than algorithmic decisions? Or do they trust algorithmic decisions as much as or more than human decisions? We examine the role of mistrust in human systems in people’s perceptions of algorithmic decisions. We focus on healthcare Artificial Intelligence (AI), group-based medical mistrust, and Black people in the United States. We conducted a between-subjects online experiment to examine people’s perceptions of skin cancer screening decisions made by an AI versus a human physician depending on their medical mistrust, and we conducted interviews to understand how to cultivate trust in healthcare AI. Our findings highlight that research around human experiences of AI should consider critical differences in social groups.},
  urldate    = {2024-05-16},
  booktitle  = {Proceedings of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
  publisher  = {Association for Computing Machinery},
  author     = {Lee, Min Kyung and Rich, Katherine},
  month      = may,
  year       = {2021},
  keywords   = {Black Perspectives, Fairness, Group-Based Medical Mistrust Scale (GBMMS), Healthcare AI, Perceptions of Algorithmic Decisions, Trust},
  pages      = {1--14},
  file       = {Full Text PDF:C\:\\Users\\victo\\Zotero\\storage\\U6YAKPWH\\Lee and Rich - 2021 - Who Is Included in Human Perceptions of AI Trust.pdf:application/pdf}
}

@article{lee2018understanding,
  title      = {Understanding perception of algorithmic decisions: {Fairness}, trust, and emotion in response to algorithmic management},
  volume     = {5},
  issn       = {2053-9517},
  shorttitle = {Understanding perception of algorithmic decisions},
  url        = {https://doi.org/10.1177/2053951718756684},
  doi        = {10.1177/2053951718756684},
  abstract   = {Algorithms increasingly make managerial decisions that people used to make. Perceptions of algorithms, regardless of the algorithms' actual performance, can significantly influence their adoption, yet we do not fully understand how people perceive decisions made by algorithms as compared with decisions made by humans. To explore perceptions of algorithmic management, we conducted an online experiment using four managerial decisions that required either mechanical or human skills. We manipulated the decision-maker (algorithmic or human), and measured perceived fairness, trust, and emotional response. With the mechanical tasks, algorithmic and human-made decisions were perceived as equally fair and trustworthy and evoked similar emotions; however, human managers' fairness and trustworthiness were attributed to the manager's authority, whereas algorithms' fairness and trustworthiness were attributed to their perceived efficiency and objectivity. Human decisions evoked some positive emotion due to the possibility of social recognition, whereas algorithmic decisions generated a more mixed response – algorithms were seen as helpful tools but also possible tracking mechanisms. With the human tasks, algorithmic decisions were perceived as less fair and trustworthy and evoked more negative emotion than human decisions. Algorithms' perceived lack of intuition and subjective judgment capabilities contributed to the lower fairness and trustworthiness judgments. Positive emotion from human decisions was attributed to social recognition, while negative emotion from algorithmic decisions was attributed to the dehumanizing experience of being evaluated by machines. This work reveals people's lay concepts of algorithmic versus human decisions in a management context and suggests that task characteristics matter in understanding people's experiences with algorithmic technologies.},
  language   = {en},
  number     = {1},
  urldate    = {2024-05-16},
  journal    = {Big Data \& Society},
  author     = {Lee, Min Kyung},
  month      = jan,
  year       = {2018},
  note       = {Publisher: SAGE Publications Ltd},
  pages      = {2053951718756684},
  file       = {SAGE PDF Full Text:C\:\\Users\\victo\\Zotero\\storage\\SFPG5M7Q\\Lee - 2018 - Understanding perception of algorithmic decisions.pdf:application/pdf}
}

@article{castelo2019task-dependent,
  title    = {Task-{Dependent} {Algorithm} {Aversion}},
  volume   = {56},
  issn     = {0022-2437},
  url      = {https://doi.org/10.1177/0022243719851788},
  doi      = {10.1177/0022243719851788},
  abstract = {Research suggests that consumers are averse to relying on algorithms to perform tasks that are typically done by humans, despite the fact that algorithms often perform better. The authors explore when and why this is true in a wide variety of domains. They find that algorithms are trusted and relied on less for tasks that seem subjective (vs. objective) in nature. However, they show that perceived task objectivity is malleable and that increasing a task’s perceived objectivity increases trust in and use of algorithms for that task. Consumers mistakenly believe that algorithms lack the abilities required to perform subjective tasks. Increasing algorithms’ perceived affective human-likeness is therefore effective at increasing the use of algorithms for subjective tasks. These findings are supported by the results of four online lab studies with over 1,400 participants and two online field studies with over 56,000 participants. The results provide insights into when and why consumers are likely to use algorithms and how marketers can increase their use when they outperform humans.},
  language = {en},
  number   = {5},
  urldate  = {2024-05-16},
  journal  = {Journal of Marketing Research},
  author   = {Castelo, Noah and Bos, Maarten W. and Lehmann, Donald R.},
  month    = oct,
  year     = {2019},
  note     = {Publisher: SAGE Publications Inc},
  pages    = {809--825}
}

@article{ha2023improving,
  title      = {Improving {Trust} in {AI} with {Mitigating} {Confirmation} {Bias}: {Effects} of {Explanation} {Type} and {Debiasing} {Strategy} for {Decision}-{Making} with {Explainable} {AI}},
  volume     = {0},
  issn       = {1044-7318},
  shorttitle = {Improving {Trust} in {AI} with {Mitigating} {Confirmation} {Bias}},
  url        = {https://doi.org/10.1080/10447318.2023.2285640},
  doi        = {10.1080/10447318.2023.2285640},
  abstract   = {With advancements in artificial intelligence (AI), explainable AI (XAI) has emerged as a promising tool for enhancing the explainability of complex machine learning models. However, the explanations generated by an XAI may lead to cognitive biases among human users. To address this problem, this study aims to investigate how to mitigate users’ cognitive biases based on their individual characteristics. In the literature review, we found two factors that can be helpful in remedying biases: 1) debiasing strategies that have been reported to potentially reduce biases in users’ decision-making via additional information or change in information delivery, and 2) explanation modality types. To examine these factors’ effects, we conducted an experiment with a 4 (debiasing strategy) × 3 (explanation type) between-subject design. In the experiment, participants were exposed to an explainable interface that provides an AI’s outcomes with explanatory information, and their behavioral and attitudinal responses were collected. Specifically, we statistically examined the effects of textual and visual explanations on users’ trust and confirmation bias toward AI systems, considering the moderating effects of debiasing methods and watching time. The results demonstrated that textual explanations lead to higher trust in XAI systems compared to visual explanations. Moreover, we found that textual explanations are particularly beneficial for quick decision-makers to evaluate the outputs of AI systems. Next, the results indicated that the cognitive bias can be effectively mitigated by providing users with a priori information. These findings have theoretical and practical implications for designing AI-based decision support systems that can generate more trustworthy and equitable explanations.},
  number     = {0},
  urldate    = {2024-05-16},
  journal    = {International Journal of Human–Computer Interaction},
  author     = {Ha, Taehyun and Kim, Sangyeon},
  year       = {2023},
  note       = {Publisher: Taylor \& Francis
                \_eprint: https://doi.org/10.1080/10447318.2023.2285640},
  keywords   = {Artificial intelligence, cognitive bias, explanation, satisfaction, trust},
  pages      = {1--12}
}

@inproceedings{prairielearn,
  author    = {Matthew West and Geoffrey L. Herman and Craig Zilles},
  title     = {PrairieLearn: Mastery-based Online Problem Solving with Adaptive Scoring 
               and Recommendations Driven by Machine Learning},
  booktitle = {2015 ASEE Annual Conference \& Exposition},
  year      = {2015},
  month     = {June},
  address   = {Seattle, Washington},
  numpages  = {14},
  pages     = {26.1238.1--26.1238.14},
  publisher = {ASEE Conferences},
  note      = {https://peer.asee.org/24575},
  number    = {10.18260/p.24575}
}

@misc{lewis2021retrieval-augmented,
  title     = {Retrieval-{Augmented} {Generation} for {Knowledge}-{Intensive} {NLP} {Tasks}},
  url       = {http://arxiv.org/abs/2005.11401},
  doi       = {10.48550/arXiv.2005.11401},
  abstract  = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
  urldate   = {2024-05-28},
  publisher = {arXiv},
  author    = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Küttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rocktäschel, Tim and Riedel, Sebastian and Kiela, Douwe},
  month     = apr,
  year      = {2021},
  note      = {arXiv:2005.11401 [cs]},
  keywords  = {Computer Science - Computation and Language, Computer Science - Machine Learning},
  file      = {arXiv Fulltext PDF:C\:\\Users\\victo\\Zotero\\storage\\FY8726QV\\Lewis et al. - 2021 - Retrieval-Augmented Generation for Knowledge-Inten.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\victo\\Zotero\\storage\\M8SK6CDL\\2005.html:text/html}
}

@inproceedings{li2023wrong,
  author    = {Li, Tiffany Wenting and Hsu, Silas and Fowler, Max and Zhang, Zhilin and Zilles, Craig and Karahalios, Karrie},
  title     = {Am I Wrong, or Is the Autograder Wrong? Effects of AI Grading Mistakes on Learning},
  year      = {2023},
  isbn      = {9781450399760},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3568813.3600124},
  doi       = {10.1145/3568813.3600124},
  abstract  = {Errors in AI grading and feedback often have an intractable set of causes and are, by their nature, difficult to completely avoid. Since inaccurate feedback potentially harms learning, there is a need for designs and workflows that mitigate these harms. To better understand the mechanisms by which erroneous AI feedback impacts students’ learning, we conducted surveys and interviews that recorded students’ interactions with a short-answer AI autograder for “Explain in Plain English” code reading problems. Using causal modeling, we inferred the learning impacts of wrong answers marked as right (false positives, FPs) and right answers marked as wrong (false negatives, FNs). We further explored explanations for the learning impacts, including errors influencing participants’ engagement with feedback and assessments of their answers’ correctness, and participants’ prior performance in the class. FPs harmed learning in large part due to participants’ failures to detect the errors. This was due to participants not paying attention to the feedback after being marked as right, and an apparent bias against admitting one’s answer was wrong once marked right. On the other hand, FNs harmed learning only for survey participants, suggesting that interviewees’ greater behavioral and cognitive engagement protected them from learning harms. Based on these findings, we propose ways to help learners detect FPs and encourage deeper reflection on FNs to mitigate the learning harms of AI errors.},
  booktitle = {Proceedings of the 2023 ACM Conference on International Computing Education Research - Volume 1},
  pages     = {159–176},
  numpages  = {18},
  keywords  = {human-AI interaction, formative feedback, explain in plain English, computer science education, automated short answer grading, autograder, EiPE, Bayesian modeling, AI error},
  location  = {, Chicago, IL, USA, },
  series    = {ICER '23}
}

@inproceedings{hsu2021attitudes,
  author    = {Hsu, Silas and Li, Tiffany Wenting and Zhang, Zhilin and Fowler, Max and Zilles, Craig and Karahalios, Karrie},
  title     = {Attitudes Surrounding an Imperfect AI Autograder},
  year      = {2021},
  isbn      = {9781450380966},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3411764.3445424},
  doi       = {10.1145/3411764.3445424},
  abstract  = {Deployment of AI assessment tools in education is widespread, but work on students’ interactions and attitudes towards imperfect autograders is comparatively lacking. This paper presents students’ perceptions surrounding a ∼ 90\% accurate automated short-answer grader that determined homework and exam credit in a college-level computer science course. Using surveys and interviews, we investigated students’ knowledge about the autograder and their attitudes. We observed that misalignment between folk theories about how the autograder worked and how it actually worked could lead to suboptimal answer construction strategies. Students overestimated the autograder’s probability of marking correct answers as wrong, and estimates of this probability were associated with dissatisfaction and perceptions of unfairness. Many participants expressed a need for additional instruction on how to cater to the autograder. From these findings, we propose guidelines for incorporating imperfect short answer autograders into classroom in a manner that is considerate of students’ needs.},
  booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
  articleno = {681},
  numpages  = {15},
  keywords  = {perception and acceptance of AI, imperfect AI, human-AI interaction, folk theories, computer science education, code reading, autograder, EiPE, ASAG},
  location  = {<conf-loc>, <city>Yokohama</city>, <country>Japan</country>, </conf-loc>},
  series    = {CHI '21}
}


@incollection{pearson2005assessment,
  address    = {Mahwah, NJ, US},
  series     = {Center for improvement of early reading achievement ({CIERA})},
  title      = {The {Assessment} of {Reading} {Comprehension}: {A} {Review} of {Practices}-{Past}, {Present}, and {Future}},
  isbn       = {978-0-8058-4655-3 978-0-8058-4656-0},
  shorttitle = {The {Assessment} of {Reading} {Comprehension}},
  abstract   = {The purpose of this chapter is to build an argument for a fresh line of inquiry into the assessment of reading comprehension. We intend to accomplish that goal by providing a rich and detailed historical account of reading comprehension, both as a theoretical phenomenon and an operational construct that lives and breathes in classrooms throughout America. We review both basic research, which deals with reading comprehension largely in its theoretical aspect, and applied research, which is much more concerned about how comprehension gets operationalized in classrooms, reading materials, and tests. (PsycInfo Database Record (c) 2024 APA, all rights reserved)},
  booktitle  = {Children's reading comprehension and assessment},
  publisher  = {Lawrence Erlbaum Associates Publishers},
  author     = {Pearson, P. David and Hamm, Diane N.},
  year       = {2005},
  keywords   = {Reading Comprehension, Reading Measures, Test Administration},
  pages      = {13--69},
  file       = {Snapshot:C\:\\Users\\victo\\Zotero\\storage\\FNDNTANT\\2005-02454-002.html:text/html}
}

@inproceedings{ashish2024tradeoff,
  author    = {Gurung, Ashish and Vanacore, Kirk and Mcreynolds, Andrew A. and Ostrow, Korinn S. and Worden, Eamon and Sales, Adam C. and Heffernan, Neil T.},
  title     = {Multiple Choice vs. Fill-In Problems: The Trade-off Between Scalability and Learning},
  year      = {2024},
  isbn      = {9798400716188},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3636555.3636908},
  doi       = {10.1145/3636555.3636908},
  abstract  = {Learning experience designers consistently balance the trade-off between open and close-ended activities. The growth and scalability of Computer Based Learning Platforms (CBLPs) have only magnified the importance of these design trade-offs. CBLPs often utilize close-ended activities (i.e. Multiple-Choice Questions [MCQs]) due to feasibility constraints associated with the use of open-ended activities. MCQs offer certain affordances, such as immediate grading and the use of distractors, setting them apart from open-ended activities. Our current study examines the effectiveness of Fill-In problems as an alternative to MCQs for middle school mathematics. We report on a randomized study conducted from 2017 to 2022, with a total of 6,768 students from middle schools across the US. We observe that, on average, Fill-In problems lead to better post-test performance than MCQs; albeit deeper explorations indicate differences between the two design paradigms to be more nuanced. We find evidence that students with higher math knowledge benefit more from Fill-In problems than those with lower math knowledge.},
  booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
  pages     = {507–517},
  numpages  = {11},
  keywords  = {Causal Inference, Fill-In Problems, Learning Experience Design, Learning Outcomes, Multiple Choice Questions},
  location  = {Kyoto, Japan},
  series    = {LAK '24}
}


@article{scott2006evaluating,
  title    = {Evaluating multiple-choice exams in large introductory physics courses},
  volume   = {2},
  url      = {https://link.aps.org/doi/10.1103/PhysRevSTPER.2.020102},
  doi      = {10.1103/PhysRevSTPER.2.020102},
  abstract = {The reliability and validity of professionally written multiple-choice exams have been extensively studied for exams such as the SAT, graduate record examination, and the force concept inventory. Much of the success of these multiple-choice exams is attributed to the careful construction of each question, as well as each response. In this study, the reliability and validity of scores from multiple-choice exams written for and administered in the large introductory physics courses at the University of Illinois, Urbana-Champaign were investigated. The reliability of exam scores over the course of a semester results in approximately a 3\% uncertainty in students’ total semester exam score. This semester test score uncertainty yields an uncertainty in the students’ assigned letter grade that is less than 13 of a letter grade. To study the validity of exam scores, a subset of students were ranked independently based on their multiple-choice score, graded explanations, and student interviews. The ranking of these students based on their multiple-choice score was found to be consistent with the ranking assigned by physics instructors based on the students’ written explanations (r{\textgreater}0.94 at the 95\% confidence level) and oral interviews (r=0.94+0.06−0.09).},
  number   = {2},
  urldate  = {2024-03-19},
  journal  = {Phys. Rev. ST Phys. Educ. Res.},
  author   = {Scott, Michael and Stelzer, Tim and Gladding, Gary},
  month    = jul,
  year     = {2006},
  note     = {Publisher: American Physical Society},
  pages    = {020102},
  file     = {APS Snapshot:C\:\\Users\\victo\\Zotero\\storage\\XX2KYE6J\\PhysRevSTPER.2.html:text/html;Full Text PDF:C\:\\Users\\victo\\Zotero\\storage\\A2IFADSA\\Scott et al. - 2006 - Evaluating multiple-choice exams in large introduc.pdf:application/pdf}
}

@article{gilardi2023outperforms,
  title     = {ChatGPT outperforms crowd workers for text-annotation tasks},
  volume    = {120},
  issn      = {1091-6490},
  url       = {http://dx.doi.org/10.1073/pnas.2305016120},
  doi       = {10.1073/pnas.2305016120},
  number    = {30},
  journal   = {Proceedings of the National Academy of Sciences},
  publisher = {Proceedings of the National Academy of Sciences},
  author    = {Gilardi, Fabrizio and Alizadeh, Meysam and Kubli, Maël},
  year      = {2023},
  month     = jul
}

  @misc{kuzman2023beginning,
  title         = {ChatGPT: Beginning of an End of Manual Linguistic Data Annotation? Use Case of Automatic Genre Identification},
  author        = {Taja Kuzman and Igor Mozetič and Nikola Ljubešić},
  year          = {2023},
  eprint        = {2303.03953},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2303.03953}
}

@misc{zhao2024autograding,
  title         = {Autograding Mathematical Induction Proofs with Natural Language Processing},
  author        = {Chenyan Zhao and Mariana Silva and Seth Poulsen},
  year          = {2024},
  eprint        = {2406.10268},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI},
  url           = {https://arxiv.org/abs/2406.10268}
}

@misc{liu2019multiway,
  title         = {Automatic Short Answer Grading via Multiway Attention Networks},
  author        = {Tiaoqiao Liu and Wenbiao Ding and Zhiwei Wang and Jiliang Tang and Gale Yan Huang and Zitao Liu},
  year          = {2019},
  eprint        = {1909.10166},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI},
  url           = {https://arxiv.org/abs/1909.10166}
}

@misc{fernandez2023automated,
  title     = {Automated {Scoring} for {Reading} {Comprehension} via {In}-context {BERT} {Tuning}},
  url       = {http://arxiv.org/abs/2205.09864},
  doi       = {10.48550/arXiv.2205.09864},
  abstract  = {Automated scoring of open-ended student responses has the potential to significantly reduce human grader effort. Recent advances in automated scoring often leverage textual representations based on pre-trained language models such as BERT and GPT as input to scoring models. Most existing approaches train a separate model for each item/question, which is suitable for scenarios such as essay scoring where items can be quite different from one another. However, these approaches have two limitations: 1) they fail to leverage item linkage for scenarios such as reading comprehension where multiple items may share a reading passage; 2) they are not scalable since storing one model per item becomes difficult when models have a large number of parameters. In this paper, we report our (grand prize-winning) solution to the National Assessment of Education Progress (NAEP) automated scoring challenge for reading comprehension. Our approach, in-context BERT fine-tuning, produces a single shared scoring model for all items with a carefully-designed input structure to provide contextual information on each item. We demonstrate the effectiveness of our approach via local evaluations using the training dataset provided by the challenge. We also discuss the biases, common error types, and limitations of our approach.},
  urldate   = {2024-12-17},
  publisher = {arXiv},
  author    = {Fernandez, Nigel and Ghosh, Aritra and Liu, Naiming and Wang, Zichao and Choffin, Benoît and Baraniuk, Richard and Lan, Andrew},
  month     = jun,
  year      = {2023},
  note      = {arXiv:2205.09864 [cs]},
  keywords  = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Machine Learning},
  file      = {Preprint PDF:C\:\\Users\\victo\\Zotero\\storage\\HK5ICLSL\\Fernandez et al. - 2023 - Automated Scoring for Reading Comprehension via In.pdf:application/pdf;Snapshot:C\:\\Users\\victo\\Zotero\\storage\\5PD5BPSP\\2205.html:text/html}
}


@misc{kortemeyer2023performance,
  title     = {Performance of the {Pre}-{Trained} {Large} {Language} {Model} {GPT}-4 on {Automated} {Short} {Answer} {Grading}},
  url       = {http://arxiv.org/abs/2309.09338},
  doi       = {10.48550/arXiv.2309.09338},
  abstract  = {Automated Short Answer Grading (ASAG) has been an active area of machine-learning research for over a decade. It promises to let educators grade and give feedback on free-form responses in large-enrollment courses in spite of limited availability of human graders. Over the years, carefully trained models have achieved increasingly higher levels of performance. More recently, pre-trained Large Language Models (LLMs) emerged as a commodity, and an intriguing question is how a general-purpose tool without additional training compares to specialized models. We studied the performance of GPT-4 on the standard benchmark 2-way and 3-way datasets SciEntsBank and Beetle, where in addition to the standard task of grading the alignment of the student answer with a reference answer, we also investigated withholding the reference answer. We found that overall, the performance of the pre-trained general-purpose GPT-4 LLM is comparable to hand-engineered models, but worse than pre-trained LLMs that had specialized training.},
  urldate   = {2024-12-04},
  publisher = {arXiv},
  author    = {Kortemeyer, Gerd},
  month     = sep,
  year      = {2023},
  note      = {arXiv:2309.09338 [cs]},
  keywords  = {Computer Science - Computation and Language},
  file      = {Preprint PDF:C\:\\Users\\victo\\Zotero\\storage\\QGIYCZXE\\Kortemeyer - 2023 - Performance of the Pre-Trained Large Language Mode.pdf:application/pdf;Snapshot:C\:\\Users\\victo\\Zotero\\storage\\3IGAS2A6\\2309.html:text/html}
}
@article{huang2024hallucination,
  title     = {A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions},
  issn      = {1558-2868},
  url       = {http://dx.doi.org/10.1145/3703155},
  doi       = {10.1145/3703155},
  journal   = {ACM Transactions on Information Systems},
  publisher = {Association for Computing Machinery (ACM)},
  author    = {Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and Liu, Ting},
  year      = {2024},
  month     = nov
}

@misc{izacard2022fewshot,
  title         = {Atlas: Few-shot Learning with Retrieval Augmented Language Models},
  author        = {Gautier Izacard and Patrick Lewis and Maria Lomeli and Lucas Hosseini and Fabio Petroni and Timo Schick and Jane Dwivedi-Yu and Armand Joulin and Sebastian Riedel and Edouard Grave},
  year          = {2022},
  eprint        = {2208.03299},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2208.03299}
}


@article{grevisse2024medical,
  title    = {{LLM}-based automatic short answer grading in undergraduate medical education},
  volume   = {24},
  issn     = {1472-6920},
  url      = {https://doi.org/10.1186/s12909-024-06026-5},
  doi      = {10.1186/s12909-024-06026-5},
  abstract = {Multiple choice questions are heavily used in medical education assessments, but rely on recognition instead of knowledge recall. However, grading open questions is a time-intensive task for teachers. Automatic short answer grading (ASAG) has tried to fill this gap, and with the recent advent of Large Language Models (LLM), this branch has seen a new momentum.},
  number   = {1},
  urldate  = {2025-01-02},
  journal  = {BMC Medical Education},
  author   = {Grévisse, Christian},
  month    = sep,
  year     = {2024},
  keywords = {Automatic short answer grading, Gemini, GPT-4, Large language models, Medical education},
  pages    = {1060},
  file     = {Full Text PDF:C\:\\Users\\victo\\Zotero\\storage\\R9DLTSBM\\Grévisse - 2024 - LLM-based automatic short answer grading in underg.pdf:application/pdf;Snapshot:C\:\\Users\\victo\\Zotero\\storage\\RIG2DBJJ\\s12909-024-06026-5.html:text/html}
}


@inproceedings{duong2024automatic,
  title     = {Automatic {Grading} of {Short} {Answers} {Using} {Large} {Language} {Models} in {Software} {Engineering} {Courses}},
  url       = {https://ieeexplore.ieee.org/document/10578839},
  doi       = {10.1109/EDUCON60312.2024.10578839},
  abstract  = {Short-answer based questions have been used widely due to their effectiveness in assessing whether the desired learning outcomes have been attained by students. However, due to their open-ended nature, many different answers could be considered entirely or partially correct for the same question. In the context of computer science and software engineering courses where the enrolment has been increasing recently, manual grading of short-answer questions is a time-consuming and tedious process for instructors. In software engineering courses, assessments concern not just coding but many other aspects of software development such as system analysis, architecture design, software processes and operation methodologies such as Agile and DevOps. However, existing work in automatic grading/scoring of text-based answers in computing courses have been focusing more on coding-oriented questions. In this work, we consider the problem of autograding a broader range of short answers in software engineering courses. We propose an automated grading system incorporating both text embedding and completion approaches based on recently introduced pre-trained large language models (LLMs) such as GPT-3.5/4. We design and implement a web-based system so that students and instructors can easily leverage autograding for learning and teaching. Finally, we conduct an extensive evaluation of our automated grading approaches. We use a popular public dataset in the computing education domain and a new software engineering dataset of our own. The results demonstrate the effectiveness of our approach, and provide useful insights for further research in this area of AI-enabled education.},
  urldate   = {2025-01-02},
  booktitle = {2024 {IEEE} {Global} {Engineering} {Education} {Conference} ({EDUCON})},
  author    = {Duong, Ta Nguyen Binh and Meng, Chai Yi},
  month     = may,
  year      = {2024},
  note      = {ISSN: 2165-9567},
  keywords  = {automatic grading, Computer science, Costs, embedding, Encoding, Focusing, large language models, Large language models, short answers, Software, software engineering courses, Training},
  pages     = {1--10},
  file      = {Full Text PDF:C\:\\Users\\victo\\Zotero\\storage\\M2Y5ESNC\\Duong and Meng - 2024 - Automatic Grading of Short Answers Using Large Lan.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\victo\\Zotero\\storage\\TWLIWGM2\\10578839.html:text/html}
}

@article{reddy2010rubric,
  author    = {Y. Malini Reddy and Heidi Andrade},
  title     = {A review of rubric use in higher education},
  journal   = {Assessment \& Evaluation in Higher Education},
  volume    = {35},
  number    = {4},
  pages     = {435--448},
  year      = {2010},
  publisher = {SRHE Website},
  doi       = {10.1080/02602930902862859},
  url       = {https://doi.org/10.1080/02602930902862859},
  eprint    = {https://doi.org/10.1080/02602930902862859}
}

@article{panadero2013formativerubric,
  title      = {The use of scoring rubrics for formative assessment purposes revisited: {A} review},
  volume     = {9},
  issn       = {1747-938X},
  shorttitle = {The use of scoring rubrics for formative assessment purposes revisited},
  url        = {https://www.sciencedirect.com/science/article/pii/S1747938X13000109},
  doi        = {10.1016/j.edurev.2013.01.002},
  abstract   = {The mainstream research on scoring rubrics has emphasized the summative aspect of assessment. In recent years, the use of rubrics for formative purposes has gained more attention. This research has, however, not been conclusive. The aim of this study is therefore to review the research on formative use of rubrics, in order to investigate if, and how, rubrics have an impact on student learning. In total, 21 studies about rubrics were analyzed through content analysis. Sample, subject/task, design, procedure, and findings, were compared among the different studies in relation to effects on student performance and selfregulation. Findings indicate that rubrics may have the potential to influence students learning positively, but also that there are several different ways for the use of rubrics to mediate improved performance and self-regulation. There are a number of factors identified that may moderate the effects of using rubrics formatively, as well as factors that need further investigation.},
  urldate    = {2025-01-05},
  journal    = {Educational Research Review},
  author     = {Panadero, Ernesto and Jonsson, Anders},
  month      = jun,
  year       = {2013},
  keywords   = {Formative assessment, Learning, Peer-assessment, Rubrics, Self-assessment, Self-efficacy, Self-grading, Self-regulation},
  pages      = {129--144},
  file       = {Panadero and Jonsson - 2013 - The use of scoring rubrics for formative assessmen.pdf:C\:\\Users\\victo\\Zotero\\storage\\HRHQ574W\\Panadero and Jonsson - 2013 - The use of scoring rubrics for formative assessmen.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\victo\\Zotero\\storage\\VIXIDZJG\\S1747938X13000109.html:text/html}
}

@inproceedings{senanayake2024rubric-asag,
  title     = {Rubric {Based} {Automated} {Short} {Answer} {Scoring} using {Large} {Language} {Models} ({LLMs})},
  volume    = {7},
  url       = {https://ieeexplore.ieee.org/abstract/document/10550624},
  doi       = {10.1109/SCSE61872.2024.10550624},
  abstract  = {The manual grading of short answers presents challenges in education due to time constraints, especially with larger student populations, and suffers from subjectivity and bias, leading to inconsistencies. Larger student populations increase the time needed for individual assessment, leading to potential delays and subjectivity introduces biases, resulting in inconsistent evaluations. Moreover, as student numbers rise, the imbalance in teacher-to-student ratios affects grading quality, impacting fairness and effectiveness in educational assessments. Automated grading systems have emerged as a solution to address these issues. These grading systems mainly prioritize appearance, emphasizing grammar and format. However, they struggle to accurately assess content quality, often missing contextual relevance. This problem can potentially be resolved by employing highly trained domain-specific models. However, a drawback arises as these models are limited to evaluating answers only within predefined domains. While these specialized models excel in assessing responses within their designated fields, their utility is restricted when evaluating answers outside of these predefined domains. This limitation poses a challenge in achieving broader applicability for assessing answers outside the specific areas the models were trained for. This study proposes a rubric-based method paired with Large Language Models (LLMs) to introduce objectivity, ensuring fairness and reliability in evaluations while achieving generalizability. Rubric provides a clear and customizable marking schema for assessing short answers across various domains. By using predetermined marking criteria and conditions, the grading process becomes more objective and transparent. The proposed method efficiently evaluates short answers in various domains using Large Language Models (LLMs), based on these established criteria, reducing subjective biases. This research aims to revolutionize education by creating a robust automated short answer scoring system that comprehensively evaluates contents across domains and addresses teacher-to-student ratio issues.},
  urldate   = {2024-12-04},
  booktitle = {2024 {International} {Research} {Conference} on {Smart} {Computing} and {Systems} {Engineering} ({SCSE})},
  author    = {Senanayake, Chamuditha and Asanka, Dinesh},
  month     = apr,
  year      = {2024},
  note      = {ISSN: 2613-8662},
  keywords  = {Education, Automated short answer grading, Computational modeling, Delays, Grammar, Large Language Models, Manuals, Rubric-based, Sociology, Time factors},
  pages     = {1--6},
  file      = {Full Text PDF:C\:\\Users\\victo\\Zotero\\storage\\INW5U3E3\\Senanayake and Asanka - 2024 - Rubric Based Automated Short Answer Scoring using .pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\victo\\Zotero\\storage\\MGNTANCF\\10550624.html:text/html}
}


@inproceedings{filighera2022saf,
  address    = {Dublin, Ireland},
  title      = {Your {Answer} is {Incorrect}... {Would} you like to know why? {Introducing} a {Bilingual} {Short} {Answer} {Feedback} {Dataset}},
  shorttitle = {Your {Answer} is {Incorrect}... {Would} you like to know why?},
  url        = {https://aclanthology.org/2022.acl-long.587/},
  doi        = {10.18653/v1/2022.acl-long.587},
  abstract   = {Handing in a paper or exercise and merely receiving “bad” or “incorrect” as feedback is not very helpful when the goal is to improve. Unfortunately, this is currently the kind of feedback given by Automatic Short Answer Grading (ASAG) systems. One of the reasons for this is a lack of content-focused elaborated feedback datasets. To encourage research on explainable and understandable feedback systems, we present the Short Answer Feedback dataset (SAF). Similar to other ASAG datasets, SAF contains learner responses and reference answers to German and English questions. However, instead of only assigning a label or score to the learners' answers, SAF also contains elaborated feedback explaining the given score. Thus, SAF enables supervised training of models that grade answers and explain where and why mistakes were made. This paper discusses the need for enhanced feedback models in real-world pedagogical scenarios, describes the dataset annotation process, gives a comprehensive analysis of SAF, and provides T5-based baselines for future comparison.},
  urldate    = {2025-01-06},
  booktitle  = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
  publisher  = {Association for Computational Linguistics},
  author     = {Filighera, Anna and Parihar, Siddharth and Steuer, Tim and Meuser, Tobias and Ochs, Sebastian},
  editor     = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
  month      = may,
  year       = {2022},
  pages      = {8577--8591},
  file       = {Full Text PDF:C\:\\Users\\victo\\Zotero\\storage\\54N4KFHK\\Filighera et al. - 2022 - Your Answer is Incorrect... Would you like to know.pdf:application/pdf}
}


@article{impey2025using,
  title    = {Using {Large} {Language} {Models} for {Automated} {Grading} of {Student} {Writing} about {Science}},
  issn     = {1560-4292, 1560-4306},
  url      = {http://arxiv.org/abs/2412.18719},
  doi      = {10.1007/s40593-024-00453-7},
  abstract = {Assessing writing in large classes for formal or informal learners presents a significant challenge. Consequently, most large classes, particularly in science, rely on objective assessment tools such as multiple-choice quizzes, which have a single correct answer. The rapid development of AI has introduced the possibility of using large language models (LLMs) to evaluate student writing. An experiment was conducted using GPT-4 to determine if machine learning methods based on LLMs can match or exceed the reliability of instructor grading in evaluating short writing assignments on topics in astronomy. The audience consisted of adult learners in three massive open online courses (MOOCs) offered through Coursera. One course was on astronomy, the second was on astrobiology, and the third was on the history and philosophy of astronomy. The results should also be applicable to non-science majors in university settings, where the content and modes of evaluation are similar. The data comprised answers from 120 students to 12 questions across the three courses. GPT-4 was provided with total grades, model answers, and rubrics from an instructor for all three courses. In addition to evaluating how reliably the LLM reproduced instructor grades, the LLM was also tasked with generating its own rubrics. Overall, the LLM was more reliable than peer grading, both in aggregate and by individual student, and approximately matched instructor grades for all three online courses. The implication is that LLMs may soon be used for automated, reliable, and scalable grading of student science writing.},
  urldate  = {2025-01-26},
  journal  = {Int J Artif Intell Educ},
  author   = {Impey, Chris and Wenger, Matthew and Garuda, Nikhil and Golchin, Shahriar and Stamer, Sarah},
  month    = jan,
  year     = {2025},
  note     = {arXiv:2412.18719 [cs]},
  keywords = {Computer Science - Computation and Language},
  file     = {Preprint PDF:C\:\\Users\\victo\\Zotero\\storage\\53R4JCXF\\Impey et al. - 2025 - Using Large Language Models for Automated Grading .pdf:application/pdf;Snapshot:C\:\\Users\\victo\\Zotero\\storage\\4IHKPFBP\\2412.html:text/html}
}


@inproceedings{nielsen2008scientsbank,
  address   = {Marrakech, Morocco},
  title     = {Annotating {Students}' {Understanding} of {Science} {Concepts}},
  url       = {https://aclanthology.org/L08-1166/},
  abstract  = {This paper summarizes the annotation of fine-grained entailment relationships in the context of student answers to science assessment questions. We annotated a corpus of 15,357 answer pairs with 145,911 fine-grained entailment relationships. We provide the rationale for such fine-grained analysis and discuss its perceived benefits to an Intelligent Tutoring System. The corpus also has potential applications in other areas, such as question answering and multi-document summarization. Annotators achieved 86.2\% inter-annotator agreement (Kappa=0.728, corresponding to substantial agreement) annotating the fine-grained facets of reference answers with regard to understanding expressed in student answers and labeling from one of five possible detailed relationship categories. The corpus described in this paper, which is the only one providing such detailed entailment annotations, is available as a public resource for the research community. The corpus is expected to enable application development, not only for intelligent tutoring systems, but also for general textual entailment applications, that is currently not practical.},
  urldate   = {2025-01-26},
  booktitle = {Proceedings of the {Sixth} {International} {Conference} on {Language} {Resources} and {Evaluation} ({LREC}`08)},
  publisher = {European Language Resources Association (ELRA)},
  author    = {Nielsen, Rodney D. and Ward, Wayne and Martin, James and Palmer, Martha},
  editor    = {Calzolari, Nicoletta and Choukri, Khalid and Maegaard, Bente and Mariani, Joseph and Odijk, Jan and Piperidis, Stelios and Tapias, Daniel},
  month     = may,
  year      = {2008},
  file      = {Full Text PDF:C\:\\Users\\victo\\Zotero\\storage\\WAKHHC76\\Nielsen et al. - 2008 - Annotating Students' Understanding of Science Conc.pdf:application/pdf}
}


@inproceedings{mohler2011texas,
  address   = {Portland, Oregon, USA},
  title     = {Learning to {Grade} {Short} {Answer} {Questions} using {Semantic} {Similarity} {Measures} and {Dependency} {Graph} {Alignments}},
  url       = {https://aclanthology.org/P11-1076/},
  urldate   = {2025-01-26},
  booktitle = {Proceedings of the 49th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
  publisher = {Association for Computational Linguistics},
  author    = {Mohler, Michael and Bunescu, Razvan and Mihalcea, Rada},
  editor    = {Lin, Dekang and Matsumoto, Yuji and Mihalcea, Rada},
  month     = jun,
  year      = {2011},
  pages     = {752--762},
  file      = {Full Text PDF:C\:\\Users\\victo\\Zotero\\storage\\269E7TES\\Mohler et al. - 2011 - Learning to Grade Short Answer Questions using Sem.pdf:application/pdf}
}


@misc{openai2024o1,
  title    = {Learning to reason with {LLMs}},
  author   = {OpenAI},
  url      = {https://openai.com/index/learning-to-reason-with-llms/},
  abstract = {We are introducing OpenAI o1, a new large language model trained with reinforcement learning to perform complex reasoning. o1 thinks before it answers—it can produce a long internal chain of thought before responding to the user.},
  language = {en-US},
  urldate  = {2025-01-26},
  file     = {Snapshot:C\:\\Users\\victo\\Zotero\\storage\\TZSEHB54\\learning-to-reason-with-llms.html:text/html}
}

@misc{wang2024strategic,
  title      = {Strategic {Chain}-of-{Thought}: {Guiding} {Accurate} {Reasoning} in {LLMs} through {Strategy} {Elicitation}},
  shorttitle = {Strategic {Chain}-of-{Thought}},
  url        = {http://arxiv.org/abs/2409.03271},
  doi        = {10.48550/arXiv.2409.03271},
  abstract   = {The Chain-of-Thought (CoT) paradigm has emerged as a critical approach for enhancing the reasoning capabilities of large language models (LLMs). However, despite their widespread adoption and success, CoT methods often exhibit instability due to their inability to consistently ensure the quality of generated reasoning paths, leading to sub-optimal reasoning performance. To address this challenge, we propose the {\textbackslash}textbf\{Strategic Chain-of-Thought\} (SCoT), a novel methodology designed to refine LLM performance by integrating strategic knowledge prior to generating intermediate reasoning steps. SCoT employs a two-stage approach within a single prompt: first eliciting an effective problem-solving strategy, which is then used to guide the generation of high-quality CoT paths and final answers. Our experiments across eight challenging reasoning datasets demonstrate significant improvements, including a 21.05{\textbackslash}\% increase on the GSM8K dataset and 24.13{\textbackslash}\% on the Tracking{\textbackslash}\_Objects dataset, respectively, using the Llama3-8b model. Additionally, we extend the SCoT framework to develop a few-shot method with automatically matched demonstrations, yielding even stronger results. These findings underscore the efficacy of SCoT, highlighting its potential to substantially enhance LLM performance in complex reasoning tasks.},
  urldate    = {2025-01-26},
  publisher  = {arXiv},
  author     = {Wang, Yu and Zhao, Shiwan and Wang, Zhihu and Huang, Heyuan and Fan, Ming and Zhang, Yubo and Wang, Zhixing and Wang, Haijun and Liu, Ting},
  month      = sep,
  year       = {2024},
  note       = {arXiv:2409.03271 [cs]},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Human-Computer Interaction},
  file       = {Preprint PDF:C\:\\Users\\victo\\Zotero\\storage\\J57BJ2NI\\Wang et al. - 2024 - Strategic Chain-of-Thought Guiding Accurate Reaso.pdf:application/pdf;Snapshot:C\:\\Users\\victo\\Zotero\\storage\\N8GSQLYG\\2409.html:text/html}
}

@misc{wei2023chain-of-thought,
  title     = {Chain-of-{Thought} {Prompting} {Elicits} {Reasoning} in {Large} {Language} {Models}},
  url       = {http://arxiv.org/abs/2201.11903},
  doi       = {10.48550/arXiv.2201.11903},
  abstract  = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
  urldate   = {2025-01-26},
  publisher = {arXiv},
  author    = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  month     = jan,
  year      = {2023},
  note      = {arXiv:2201.11903 [cs]},
  keywords  = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
  file      = {Preprint PDF:C\:\\Users\\victo\\Zotero\\storage\\8L6VYW5V\\Wei et al. - 2023 - Chain-of-Thought Prompting Elicits Reasoning in La.pdf:application/pdf;Snapshot:C\:\\Users\\victo\\Zotero\\storage\\UQ5KVDH7\\2201.html:text/html}
}

@inproceedings{meyer2024combined,
  address    = {New York, NY, USA},
  series     = {{SIGCSE} {Virtual} 2024},
  title      = {{ASAG2024}: {A} {Combined} {Benchmark} for {Short} {Answer} {Grading}},
  isbn       = {9798400706042},
  shorttitle = {{ASAG2024}},
  url        = {https://dl.acm.org/doi/10.1145/3649409.3691083},
  doi        = {10.1145/3649409.3691083},
  abstract   = {Open-ended questions test a more thorough understanding compared to closed-ended questions and are often a preferred assessment method. However, open-ended questions are tedious to grade and subject to personal bias. Therefore, there have been efforts to speed up the grading process through automation. Short Answer Grading (SAG) systems aim to automatically score students' answers in examinations. Despite growth in SAG methods and capabilities, there exists no comprehensive short-answer grading benchmark across different subjects, grading scales, and distributions. Thus, it is hard to assess the capabilities of current automated grading methods in terms of their generalizability. In this preliminary work, we introduce the combined ASAG2024 benchmark to facilitate the comparison of automated grading systems. Combining seven commonly used short-answer grading datasets in a common structure and grading scale. For our benchmark, we evaluate a set of recent SAG methods, revealing that while LLM-based approaches reach new high scores, they still are far from reaching human performance. This opens up avenues for future research on human-machine SAG systems.},
  urldate    = {2025-01-08},
  booktitle  = {Proceedings of the 2024 on {ACM} {Virtual} {Global} {Computing} {Education} {Conference} {V}. 2},
  publisher  = {Association for Computing Machinery},
  author     = {Meyer, Gérôme and Breuer, Philip and Fürst, Jonathan},
  month      = dec,
  year       = {2024},
  pages      = {322--323},
  file       = {Full Text PDF:C\:\\Users\\victo\\Zotero\\storage\\ZHRAW3ID\\Meyer et al. - 2024 - ASAG2024 A Combined Benchmark for Short Answer Gr.pdf:application/pdf}
}

@article{camus2020investigating,
  title    = {Investigating {Transformers} for {Automatic} {Short} {Answer} {Grading}},
  volume   = {12164},
  url      = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7334688/},
  doi      = {10.1007/978-3-030-52240-7_8},
  abstract = {Recent advancements in the field of deep learning for natural language processing made it possible to use novel deep learning architectures, such as the Transformer, for increasingly complex natural language processing tasks. Combined with novel unsupervised pre-training tasks such as masked language modeling, sentence ordering or next sentence prediction, those natural language processing models became even more accurate. In this work, we experiment with fine-tuning different pre-trained Transformer based architectures. We train the newest and most powerful, according to the glue benchmark, transformers on the SemEval-2013 dataset. We also explore the impact of transfer learning a model fine-tuned on the MNLI dataset to the SemEval-2013 dataset on generalization and performance. We report up to 13\% absolute improvement in macro-average-F1 over state-of-the-art results. We show that models trained with knowledge distillation are feasible for use in short answer grading. Furthermore, we compare multilingual models on a machine-translated version of the SemEval-2013 dataset.},
  urldate  = {2025-01-26},
  journal  = {Artificial Intelligence in Education},
  author   = {Camus, Leon and Filighera, Anna},
  month    = jun,
  year     = {2020},
  pmid     = {null},
  pmcid    = {PMC7334688},
  pages    = {43--48},
  file     = {PubMed Central Full Text PDF:C\:\\Users\\victo\\Zotero\\storage\\CSUMKPDG\\Camus and Filighera - 2020 - Investigating Transformers for Automatic Short Ans.pdf:application/pdf}
}

@article{kumar2017earth,
  title    = {Earth {Mover}'s {Distance} {Pooling} over {Siamese} {LSTMs} for {Automatic} {Short} {Answer} {Grading}},
  url      = {https://www.ijcai.org/proceedings/2017/284},
  abstract = {Electronic proceedings of IJCAI 2017},
  urldate  = {2025-01-26},
  author   = {Kumar, Sachin and Chakrabarti, Soumen and Roy, Shourya},
  year     = {2017},
  pages    = {2046--2052},
  file     = {Snapshot:C\:\\Users\\victo\\Zotero\\storage\\YPPR9WY5\\284.html:text/html}
}

@inproceedings{ma2024integrating,
  author    = {Ma, Iris and Krone-Martins, Alberto and Videira Lopes, Cristina},
  title     = {Integrating AI Tutors in a Programming Course},
  year      = {2024},
  isbn      = {9798400705984},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3649165.3690094},
  doi       = {10.1145/3649165.3690094},
  abstract  = {RAGMan is an LLM-powered tutoring system that can support a variety of course-specific and homework-specific AI tutors. RAGMan leverages Retrieval Augmented Generation (RAG), as well as strict instructions, to ensure the alignment of the AI tutors' responses. By using RAGMan's AI tutors, students receive assistance with their specific homework assignments without directly obtaining solutions, while also having the ability to ask general programming-related questions. RAGMan was deployed as an optional resource in an introductory programming course with an enrollment of 455 students. It was configured as a set of five homework-specific AI tutors. This paper describes the interactions the students had with the AI tutors, the students' feedback, and a comparative grade analysis. Overall, about half of the students engaged with the AI tutors, and the vast majority of the interactions were legitimate homework questions. When students posed questions within the intended scope, the AI tutors delivered accurate responses 98\% of the time. Among the students who used AI tutors, 78\% reported that the tutors helped their learning. Beyond AI tutors' ability to provide valuable suggestions, students reported appreciating them for fostering a safe learning environment free from judgment.},
  booktitle = {Proceedings of the 2024 on ACM Virtual Global Computing Education Conference V. 1},
  pages     = {130–136},
  numpages  = {7},
  keywords  = {education, large language models, llms, software engineering},
  location  = {Virtual Event, NC, USA},
  series    = {SIGCSE Virtual 2024}
}
@inproceedings{feng2024courseassist,
  author    = {Feng, Ty and Liu, Sa and Ghosal, Dipak},
  title     = {CourseAssist: Pedagogically Appropriate AI Tutor for Computer Science Education},
  year      = {2024},
  isbn      = {9798400706042},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3649409.3691094},
  doi       = {10.1145/3649409.3691094},
  abstract  = {The growing enrollments in computer science courses and increase in class sizes necessitate scalable, automated tutoring solutions to adequately support student learning. While Large Language Models (LLMs) like GPT-4 have demonstrated potential in assisting students through question-answering, educators express concerns over student overreliance, miscomprehension of generated code, and the risk of inaccurate answers. Rather than banning these tools outright, we advocate for a constructive approach that harnesses the capabilities of AI while mitigating potential risks. This poster introduces CourseAssist, a novel LLM-based tutoring system tailored for computer science education. Unlike generic LLM systems, CourseAssist uses retrieval-augmented generation, user intent classification, and question decomposition to align AI responses with specific course materials and learning objectives, thereby ensuring pedagogical appropriateness of LLMs in educational settings. We evaluated CourseAssist against a baseline of GPT-4 using a dataset of 50 question-answer pairs from a programming languages course, focusing on the criteria of usefulness, accuracy, and pedagogical appropriateness. Evaluation results show that CourseAssist significantly outperforms the baseline, demonstrating its potential to serve as an effective learning assistant. We have also deployed CourseAssist in 6 computer science courses at a large public R1 research university reaching over 500 students. Interviews with 20 student users show that CourseAssist improves computer science instruction by increasing the accessibility of course-specific tutoring help and shortening the feedback loop on their programming assignments. Future work will include extensive pilot testing at more universities and exploring better collaborative relationships between students, educators, and AI that improve computer science learning experiences.},
  booktitle = {Proceedings of the 2024 on ACM Virtual Global Computing Education Conference V. 2},
  pages     = {310–311},
  numpages  = {2},
  keywords  = {AI tutor, computer science education, intelligent tutoring systems, large language models, pedagogical appropriateness, question answering},
  location  = {Virtual Event, NC, USA},
  series    = {SIGCSE Virtual 2024}
}

@article{hattie2011instruction,
  author  = {Hattie, John and Gan, Mark},
  year    = {2011},
  month   = {01},
  pages   = {249-271},
  title   = {Instruction based on feedback},
  journal = {Handbook of Research on Learning and Instruction}
}

@article{hattie2007power,
  author   = {John Hattie and Helen Timperley},
  title    = {The Power of Feedback},
  journal  = {Review of Educational Research},
  volume   = {77},
  number   = {1},
  pages    = {81-112},
  year     = {2007},
  doi      = {10.3102/003465430298487},
  url      = {https://doi.org/10.3102/003465430298487},
  eprint   = {https://doi.org/10.3102/003465430298487},
  abstract = {Feedback is one of the most powerful influences on learning and achievement, but this impact can be either positive or negative. Its power is frequently mentioned in articles about learning and teaching, but surprisingly few recent studies have systematically investigated its meaning. This article provides a conceptual analysis of feedback and reviews the evidence related to its impact on learning and achievement. This evidence shows that although feedback is among the major influences, the type of feedback and the way it is given can be differentially effective. A model of feedback is then proposed that identifies the particular properties and circumstances that make it effective, and some typically thorny issues are discussed, including the timing of feedback and the effects of positive and negative feedback. Finally, this analysis is used to suggest ways in which feedback can be used to enhance its effectiveness in classrooms.}
}


@inproceedings{henkel2024can,
	address = {New York, NY, USA},
	series = {L@{S} '24},
	title = {Can {Large} {Language} {Models} {Make} the {Grade}? {An} {Empirical} {Study} {Evaluating} {LLMs} {Ability} {To} {Mark} {Short} {Answer} {Questions} in {K}-12 {Education}},
	isbn = {9798400706332},
	shorttitle = {Can {Large} {Language} {Models} {Make} the {Grade}?},
	url = {https://dl.acm.org/doi/10.1145/3657604.3664693},
	doi = {10.1145/3657604.3664693},
	abstract = {This paper presents reports on a series of experiments with a novel dataset evaluating how well Large Language Models (LLMs) can mark (i.e. grade) open text responses to short answer questions, Specifically, we explore how well different combinations of GPT version and prompt engineering strategies performed at marking real student answers to short answer across different domain areas (Science and History) and grade-levels (spanning ages 5-16) using a new, never-used-before dataset from Carousel, a quizzing platform. We found that GPT-4, with basic few-shot prompting performed well (Kappa, 0.70) and, importantly, very close to human-level performance (0.75). This research builds on prior findings that GPT-4 could reliably score short answer reading comprehension questions at a performance-level very close to that of expert human raters. The proximity to human-level performance, across a variety of subjects and grade levels suggests that LLMs could be a valuable tool for supporting low-stakes formative assessment tasks in K-12 education and has important implications for real-world education delivery.},
	urldate = {2024-12-04},
	booktitle = {Proceedings of the {Eleventh} {ACM} {Conference} on {Learning} @ {Scale}},
	publisher = {Association for Computing Machinery},
	author = {Henkel, Owen and Hills, Libby and Boxer, Adam and Roberts, Bill and Levonian, Zach},
	month = jul,
	year = {2024},
	pages = {300--304},
	file = {Full Text PDF:C\:\\Users\\victo\\Zotero\\storage\\DVHIUKKG\\Henkel et al. - 2024 - Can Large Language Models Make the Grade An Empir.pdf:application/pdf},
}

@inproceedings{menini2019automated,
	title = {Automated {Short} {Answer} {Grading}: {A} {Simple} {Solution} for a {Difficult} {Task}},
	shorttitle = {Automated {Short} {Answer} {Grading}},
booktitle={Italian Conference on Computational Linguistics},
	url = {https://api.semanticscholar.org/CorpusID:204922864},
	abstract = {English. The task of short answer grading is aimed at assessing the outcome of an exam by automatically analysing students’ answers in natural language and deciding whether they should pass or fail the exam. In this paper, we tackle this task training an SVM classifier on real data taken from a University statistics exam, showing that simple concatenated sentence embeddings used as features yield results around 0.90 F1, and that adding more complex distance-based features lead only to a slight improvement. We also release the dataset, that to our knowledge is the first freely available dataset of this kind in Italian.1},
	urldate = {2025-02-10},
	author = {Menini, Stefano and Tonelli, Sara and Gasperis, Giovanni De and Vittorini, P.},
	year = {2019},
	file = {Full Text PDF:C\:\\Users\\victo\\Zotero\\storage\\G8AW7G5R\\Menini et al. - 2019 - Automated Short Answer Grading A Simple Solution .pdf:application/pdf},
}

@inproceedings{dzikovska2010beetle,
	address = {Uppsala, Sweden},
	title = {Beetle {II}: {A} {System} for {Tutoring} and {Computational} {Linguistics} {Experimentation}},
	shorttitle = {Beetle {II}},
	url = {https://aclanthology.org/P10-4003/},
	urldate = {2025-02-10},
	booktitle = {Proceedings of the {ACL} 2010 {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Dzikovska, Myroslava O. and Moore, Johanna D. and Steinhauser, Natalie and Campbell, Gwendolyn and Farrow, Elaine and Callaway, Charles B.},
	editor = {Kübler, Sandra},
	month = jul,
	year = {2010},
	pages = {13--18},
	file = {Full Text PDF:C\:\\Users\\victo\\Zotero\\storage\\LCG533AR\\Dzikovska et al. - 2010 - Beetle II A System for Tutoring and Computational.pdf:application/pdf},
}

@misc{jeeveswaran2019digiklausurasag-dataset,
	title = {{DigiKlausur}/{ASAG}-{Dataset}},
	copyright = {MPL-2.0},
	url = {https://github.com/DigiKlausur/ASAG-Dataset},
	abstract = {Assisted(not automated) Short Answer Grading Dataset},
	urldate = {2025-02-10},
	publisher = {DigiKlausur},
	author = {Jeeveswaran, Kishaan},
	year = {2019},
	note = {original-date: 2019-02-26T10:22:22Z},
}

@inproceedings{smith2024evaluating,
author = {Smith, David H. and Zilles, Craig},
title = {Evaluating Large Language Model Code Generation as an Autograding Mechanism for "Explain in Plain English" Questions},
year = {2024},
isbn = {9798400704246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626253.3635542},
doi = {10.1145/3626253.3635542},
abstract = {The ability of students to ''Explain in Plain English'' (EiPE) the purpose of code is a critical skill for students in introductory programming courses to develop. EiPE questions serve as both a mechanism for students to develop and demonstrate code comprehension skills. However, evaluating this skill has been challenging as manual grading is time consuming and not easily automated. The process of constructing a prompt for the purposes of code generation for a Large Language Model, such OpenAI's GPT-4, bears a striking resemblance to constructing EiPE responses. In this paper, we explore the potential of using test cases run on code generated by GPT-4 from students' EiPE responses as a grading mechanism for EiPE questions. We applied this proposed grading method to a corpus of EiPE responses collected from past exams, then measured agreement between the results of this grading method and human graders. Overall, we find moderate agreement between the human raters and the results of the unit tests run on the generated code. This appears to be attributable to GPT-4's code generation being more lenient than human graders on low-level descriptions of code.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1824–1825},
numpages = {2},
keywords = {autograding, eipe, gpt-4, large language models},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{denny2024explaining,
author = {Denny, Paul and Smith, David H. and Fowler, Max and Prather, James and Becker, Brett A. and Leinonen, Juho},
title = {Explaining Code with a Purpose: An Integrated Approach for Developing Code Comprehension and Prompting Skills},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649217.3653587},
doi = {10.1145/3649217.3653587},
abstract = {Reading, understanding and explaining code have traditionally been important skills for novices learning programming. As large language models (LLMs) become prevalent, these foundational skills are more important than ever given the increasing need to understand and evaluate model-generated code. Brand new skills are also needed, such as the ability to formulate clear prompts that can elicit intended code from an LLM. Thus, there is great interest in integrating pedagogical approaches for the development of both traditional coding competencies and the novel skills required to interact with LLMs. One effective way to develop and assess code comprehension ability is with "Explain in plain English'' (EiPE) questions, where students succinctly explain the purpose of a fragment of code. However, grading EiPE questions has always been difficult given the subjective nature of evaluating written explanations and this has stifled their uptake. In this paper, we explore a natural synergy between EiPE questions and code-generating LLMs to overcome this limitation. We propose using an LLM to generate code based on students' responses to EiPE questions -- not only enabling EiPE responses to be assessed automatically, but helping students develop essential code comprehension and prompt crafting skills in parallel. We investigate this idea in an introductory programming course and report student success in creating effective prompts for solving EiPE questions. We also examine student perceptions of this activity and how it influences their views on the use of LLMs for aiding and assessing learning.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
pages = {283–289},
numpages = {7},
keywords = {code comprehension, cs1, eipe, explain in plan english, introductory programming, large language models, llms, prompting},
location = {Milan, Italy},
series = {ITiCSE 2024}
}