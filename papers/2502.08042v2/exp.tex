\section{Experiments}\label{sec:exp}

\input{tables/graph_info_w_time.tex}
\subsection{Experiment Setup}
Our experiments were conducted on a 96-core machine (192 hyper-threads) equipped with four 2.1 GHz Intel Xeon Gold 6252 CPUs, 
each with a 36MB L3 cache and 1.5TB of main memory. 
For all tests, except the sequential ones, 
we used \texttt{numactl -i all} to interleave memory across all CPUs. 
We report the average runtime of five runs, following an initial warm-up run.

% \subsection{Datasets and Baselines}
\subsubsection{Datasets}\label{sec:datasets}
We tested a wide range of real-world graphs, including large-scale social networks, web graphs, 
road networks, \knn{} graphs, and various other graphs.
%We also tested synthetic graphs, both from existing graphs libraries and graphs generated by ourselves.
Additionally, we generate synthetic graphs to simulate adversarial scenarios for both existing baselines and our own algorithm. 
Directed graphs are symmetrized by converting edges to bidirectional.
The graph information, along with their references and acronyms, are shown in \cref{table:fulltable}, 
and we will refer to the graphs using these acronyms throughout the rest of this section.
% In addition to existing graphs,
%The synthetic graphs are generated to play adversarial roles to existing implementations, including our own algorithm. 
% The \knn{} graphs are generated from real-world vector datasets, where each vector is connected to its $k$ nearest neighbors. 
\revise{
    The $k$-NN graphs are generated from a set of vectors (multi-dimensional points). 
    Each vertex represent a point (vector), and it has directed edges to its $k$ nearest neighbors. 
    The five graphs in our experiments are from real-world vector datasets~\cite{fonollosa2015reservoir,wang2021geograph,geolife,cosmo50}.
}
\TRCE and \BBL are meshes taken from individual frames of sequences that resembles two-dimensional adaptive numerical simulations~\cite{nr}.
The two graphs \GRD{} and \CBC{} are a $10^4 \times 10^4$ 2D grid and a $10^3 \times 10^3 \times 10^3$ 3D cube, respectively.
\HCNS{} is a synthetic graph with a high $\maxcoreness=50000$. 
It contains exactly one vertex with coreness $i$ for $1\le i<\kmax$, and a dense subgraph with coreness $\maxcoreness$.  %\youzhe{check}
\HPL{} is a power-law degree distribution graph, generated using the Barabasi-Albert model~\cite{barabasi1999emergence}.
We classify the social networks, web graphs, \HCNS{}, and \HPL{} as \emph{dense graphs} due to their relatively large average degrees and high coreness, 
while the remaining graphs are categorized as \emph{sparse graphs}.



\subsubsection{Baselines}
We compare our algorithm against three state-of-the-art parallel baselines: \Julienne~\cite{dhulipala2017}, \ParK~\cite{cheng2013local}, \PKC~\cite{kabir2017parallel}.
% Each of which demonstrates unique strengths and weaknesses across various graph types. 
Each algorithm exhibits specific strengths and weaknesses depending on the graph types. 
These algorithms are discussed in more detail in \cref{sec:peeling}. 
We also compare our algorithm with the sequential algorithm \BZ{}. 
The implementation for \Park{}, \PKC{}, and \BZ{} are from the code of the \PKC{} paper~\cite{kabir2017parallel}.
The implementation for \Julienne{} is from the GBBS library~\cite{gbbs2021}, in which \Julienne{} is integrated. 
%These baselines represent the fastest existing algorithms for the exact $k$-core decomposition problem, with each excelling in specific graph structures. 

%\ParK~\cite{dasari2014park}, though not available as open-source, is a representative parallel algorithm that follows the online peeling process.
%Meanwhile, \PKC~\cite{kabir2017parallel} exhibits better performance on large-diameter graphs than \ParK.
%The implementation of \PKC includes the \ParK algorithm and the sequential \BZ algorithm~\cite{batagelj2003m},
%so we use their implementations of \ParK, \PKC and \BZ as the baselines for comparison.
%\GBBS~\cite{gbbs2021} provides the only bucket-based $k$-core decomposition processing implementation that achieves general improvements across various graph types than the previous two,
%especially on low-diameter graphs.
%Together, these baselines provide a comprehensive comparison of our algorithm's advantages across a range of scenarios.

\subsection{Experimental Results}
\input{./figures/fig-overall.tex}

\input{./figures/fig-self-compare.tex}
\subsubsection{Overall Performance}
% We show the overall performance of our algorithm and all baselines in
We present the overall performance of our algorithm and all baselines in \cref{table:fulltable}, 
% and the relative running time of all parallel baselines (normalized to ours) in 
and the relative running times of all parallel baselines (normalized to ours) in \cref{fig:overall}. 
In \cref{table:fulltable}, 
we highlight the best running time for each graph in bold. 
We also report the sequential running time of our algorithm along with the self-relative speedup. 
In most cases, our sequential runtime is comparable to or better than \BZ{}, 
indicating the work-efficiency of our approach.
% we highlight best running time on each graph in bold. 
% We also show the sequential running time of our algorithm with self-relative speedup. 
% In most cases, our sequential running time is close to or better than \BZ{}, 
% which indicates the work-efficiency of our algorithm. 
%We first evaluate the overall performance of our algorithm on various real-world graphs.
% The graphs are divided into four different categories, including social networks, web graphs, road networks, and \KNN graphs
% based on the source and structure features of the datasets, 
% as well as the features of the graphs. We include the details and sources of the graphs in \cref{table:fulltable}.
%We draw the speedup of our algorithm compared to the baselines in \cref{fig:overall}. 
Our algorithm also demonstrates significant parallelism, 
achieving a self-relative speedup of 7.5--86$\times$. 
In contrast,
% we note that all baselines exhibit unsatisfactory performance on at least one graph due to lack of parallelism 
all baselines exhibit unsatisfactory performance on some graphs due to the lack of parallelism.
% ---
On certain graphs, they perform even slower than the sequential implementations (\BZ{} or our sequential time) and much slower than other baselines. 
Our algorithm consistently outperforms the best sequential time by 6.9--85$\times$. 
Combining both work-efficiency and strong parallelism, 
our algorithm is the fastest on 23 out of 25 tested graphs, except for \EU{} and \NA{}, 
where it remains competitive and only <12\% slower than the best baseline.

\cref{fig:overall} presents relative running time of all parallel implementations, normalized to ours. 
%For each baseline, our algorithm may achieve tens of times better performance on certain graphs. 
Each baseline may exhibit tens of times slowdown than our algorithm on certain graphs. 
Such performance degeneration occurs on different sets of graphs for each baseline, due to the different design of each algorithm. 

All algorithms perform well on \knn{} graphs, due to their properties: all vertices have small degrees, 
the same coreness, and only require a few subrounds to complete.
Despite small variation in performance, our algorithm is slightly faster on average.

For dense graphs (social networks, web graphs, \HCNS{} and \HPL{}),
which contain many high-degree vertices,
\Julienne{} performs well due to work-efficiency and their race-free offline peeling algorithm. 
\PKC{} and \Park{} have much worse performance
due to 1) work-inefficiency from not maintaining the active set and 2) high contention when updating the \induceddegree{s} of the vertices.  
Our algorithm outperforms \Julienne{}'s offline algorithm by 1.24--3.11$\times$, due to work-efficiency and the sampling scheme that reduces contention during the online peeling process. 
%which achieves even better performance than \Julienne{}'s offline algorithm by 1.24--3.11$\times$. 
%On sparse graphs, however, \GBBS{} may have poor performance due to overhead in the offline algorithm, such as computing histogram and the scheduling overhead to achieve. 

On sparse graphs, however, \Julienne{} may have poor performance due to the overhead of enabling race-freedom and fully synchronized subrounds in the offline algorithm. 
In this case, the simpler online algorithm in \PKC{} and \Park{} performs better since very light contention is incurred. 
%In this case, \GBBS{} may have poor performance due to large number of subrounds. 
\PKC{}'s thread-local buffer also fully avoids subrounds, and thus it achieves the best performance on three sparse graphs. 
However, despite being the fastest on three sparse graphs, \PKC{} also leads to the most timeout or out-of-memory cases in our test, and may have much worse performance than others on dense graphs.  
This highlights the intrinsic difficulty to optimize the performance of parallel \kcore{} algorithms: 
employing an optimization may address issues on specific graphs, but may sacrifice the performance on other graphs. 

The consistent good performance of our algorithm across diverse graph types is enabled by our new techniques. 
Sampling optimizes for high-degree vertices in dense graphs, 
while VGC is particularly effective for low-degree vertices in sparse graphs.
Consequently, our algorithm consistently performs well on various graph types, 
and prevents significant performance drops on any type of graphs.





\subsubsection{Evaluation on VGC and Sampling}

% \input{figures/fig-sampling.tex}

\input{./figures/fig-queue-rho-select.tex}


In this section, we evaluate the effectiveness of our online framework using VGC and sampling schemes. 
%the local search technique helps to reduce the number of peeling rounds,
%and also contributes to the load balance of the parallelism.
We compare four versions of our algorithm:
% without sampling or VGC (referred to as the \emph{plain} version), 
% only using sampling,
% only using VGC,
% and our final version with both sampling and VGC. 
one without sampling or VGC (referred to as the \emph{plain} version), 
one using only sampling, 
one using only VGC, 
and our final with both sampling and VGC.
We show the speedup of the latter three over the plain version in \cref{fig:self_comparison}. 
As discussed in \cref{sec:alg}, 
% the two techniques focus on different scenarios: 
% sampling improves performance for high-degree vertices, and VGC is optimized for low-degree vertices. 
% Indeed, most graphs mainly benefit from one of the two techniques. 
% Almost all sparse graphs achieves a notable improvement from VGC. 
% Seven dense graphs benefit from sampling. 
these two techniques are optimized for different scenarios: 
sampling enhances performance for high-degree vertices, 
while VGC is designed to optimize for low-degree vertices. 
In practice, most graphs benefit primarily from one of these techniques. 
Nearly all sparse graphs show significant improvement with VGC.
Seven dense graphs benefit from sampling.
%Six graphs with large highest degrees benefits from sampling, since the highest degree in these graphs are large.
On all graphs but \HCNS{}, using sampling and/or VGC improves the performance over the plain version. 

% We first discuss the impact of sampling. 
We first study the impact of sampling. 
In fact, eight graphs (\TW{}, \EH{}, \SD{}, \CW{}, \HL{}, \HLs{}, \HPL{}, and \HCNS{}) contain vertices with very high degrees and trigger sampling. 
\revise{
    \iffullversion{We show the sampling speedup for these eight graphs in \cref{fig:exp-sampling}.}
    %\ifconference{The effect of sampling can be viewed from \cref{fig:self_comparison}. 
    %In the full version of this paper, we further provide a separate figure to illustrate the speedup from sampling on these eight graphs. }
}
Seven of them (all except \HCNS{}) benefit from it. 
We first discuss the adversarial graph \HCNS{} with $\kmax=50000$. 
In this case, sampling slightly decreases the performance by 24\%. 
The overhead comes from the validation step in each round which processes all vertices in sample mode. 
On \HCNS{}, half of the vertices (all those in $\kmax$-core) need to be checked until $k=\Theta(\kmax)$.  
%Hence, we need to check the error probability for all of them in each round, but the check will not succeed until $k$ reaches $\Theta(\kmax)$. 
On real-world graphs that trigger sampling (e.g., the power-law graphs), 
this overhead is minimal since only a small number of vertices have high degrees and enter sampling mode.
%such an overhead is minimal since only a small number of vertices have high degrees and can be in sample mode. 
Therefore, this adversarial graph \HCNS{} roughly illustrate the cost of sampling, which is reasonably low (about 24\%). 
For the other seven graphs that trigger sampling, they all benefit from it by up to 4.3$\times$ (on \CW{}).
In fact, while only a small fraction of vertices may require sampling, the benefit is significant. 
For example, on \TW{}, although only about 1000 out of 40 million vertices are in sample mode, the improvement is 2.4$\times$. 
This indicates that a small fraction of high-degree vertices lead to high contention that bottlenecks the entire computation.
Our sampling scheme effectively mitigates this contention, resulting in much improved performance. 


For VGC, all sparse graphs and certain dense graphs benefit from it, with a speedup of up to 31.2$\times$ (on \GRD{}). 
We did not observe notable performance drop due to VGC, which indicates that the overhead of VGC is negligible. 
To further study the impact of VGC, we compare the number of subrounds with and without VGC on some representative graphs
in \cref{fig:queue-rho}. 
For each red point in \cref{fig:queue-rho}, the y-coordinate is the number of subrounds without VGC, 
and the x-coordinate is the number of subrounds using VGC. 
We can see that the number of subrounds is significantly reduced on various graphs.
Even on dense graphs, the numbers of subrounds also decrease by up to 9.1$\times$ (on \OK{}). 
However, the improvement in time is not as large as sparse graphs, 
because the computation on dense graphs is also substantial, and the scheduling overhead does not dominate the time.
Therefore, optimizing this part by VGC only marginally improves the performance for dense graphs. 
On sparse graphs, VGC is very effective. 
On road networks, the numbers of subrounds in each round are reduced from hundreds to within 4, with an improvement of 26--51$\times$. 
This also leads to 1.7--1.9$\times$ improvement in time. 
The improvement is more significant on four other graphs \TRCE{}, \BBL{}, \GRD{}, and \CBC{}, 
which simulates extreme cases for sparse graphs. 
The reduce ratio of \subround{s} ranges from 10--39$\times$, 
resulting in a speedup of 2.3--31.2$\times$ compared to the plain algorithm.

%Specifically, the number of total peeling rounds is reduced by up to an average of $51.3\times$ on large-diameter graphs (\EU{}). 
%The decrease ratio of $\rho$ and the overall performance improvement is more significant for large-diameter graphs than for low-diameter graphs,
%as the maximum coreness of vertices $\maxcoreness$ in large-diameter graphs is smaller than that in low-diameter graphs.

%We also evaluate the performance of our local search technique on various graphs by comparing the total running time with and without the local search technique.
%As shown in \cref{fig:self_comparison},
%VGC contributes to the overall performance improvement of our algorithm on all graphs, with different levels of speedup.
%The time speedup is up to $31.8\times$ on large-diameter synthetic graphs (\GRDfull{}), $1.97\times$ on \KNN graphs, and up to $1.88\times$ on road networks (\AFfull{}).
% \input{tables/graph_info_full.tex}


% \input{./figures/fig-queue-rho.tex}

% \myparagraph{Performance of Sampling-based Optimization.}
%\input{./figures/fig-sampling.tex}

% The sampling technique is designed to reduce high-volume contention during the peeling process, 
% especially for social networks and web graphs. We evaluate the performance of our sampling-based optimization on five different dense graphs, all of which contain high-degree hubs. In fact, the tested graphs follow a power-law distribution, a common feature of real-world social networks and web graphs. As mentioned earlier, previous parallel algorithms, such as \PKC and \GBBS, 
% exhibit suboptimal performance on these graphs. Sampling is a key technique that improves the performance of our proposed framework on these graphs.
%Sampling is designed to alleviate high-volume contention during the peeling process, particularly in social networks and web graphs. \cref{fig:exp-sampling} illustrates the performance improvements achieved through our sampling-based optimization within the framework. As demonstrated, this optimization significantly accelerates our algorithm on graphs with high-degree hubs, such as those found in social networks and web graphs. 
%Specifically, the speedup reaches up to $5.04\times$ on \CWfull{} and $3.02\times$ on \TWfull{}, representing large-scale web and social networks, respectively.
%\cref{fig:self_comparison} and \cref{table:fulltable} provide a comprehensive comparison of the running times of our algorithm with and without the sampling-based optimization across various graphs. 
%The green bars in \cref{fig:self_comparison} indicate the running time with both VGC and sampling-based optimizations, while the yellow bars represent the times for VGC alone and sampling alone. 
%The impact of these two optimizations varies across different graphs with different structures, with a combined speedup of up to $5.2\times$ on \CWfull{}, a graph where both optimizations contribute significantly to overall performance.
%We discuss the metric of choosing the parameters in sampling-based optimization in \cref{sec:choice_parameters}.
%In practice, we do not observe a significant change for different parameters in a reasonable range.
%We indeed observe that the sampling-based optimization may face some overhead on graphs with extremely high $\maxcoreness$ values, such as \HCNSfull{}. 
%Because the checking step (\cref{line:framework:security2}) in the sampling-based optimization may be triggered frequently on these graphs, 
%the overhead of the sampling-based optimization may be higher than the performance improvement.

\subsubsection{Evaluation of \HBS} \label{para:exp_bucketing}

\input{./figures/fig-bucketing-exp.tex}
\input{figures/fig-span.tex}
We evaluate the effectiveness of \HBS{} by comparing it to two baselines: 
always using one bucket, as shown in the original framework (\cref{algo:framework}), 
and always using 16 buckets, similar to \Julienne{}'s strategy.
%The hierarchical bucketing structure provides a good trade-off between different vertex-mapping strategies,
%with a balanced overhead elimination for \FBuildBuckets{} and \FUpdate{} operations (parallel hash bag insertions).
%We show the comparison of the running time of the three different bucketing structures,
%including the 1-bucket structure, 16-bucket structure, and the hierarchical bucketing structure (\HBS),
% We show the relative running time (normalized to ours) on representative graphs in \cref{fig:bucketing-exp}.
% As shown in \cref{fig:bucketing-exp}, 
\revise{
    Different from the other two technique (VGC and sampling), \HBS{} can be applied to both the online and offline peeling algorithms, and therefore
    we analyze it separately here. 
    %We separate HBS and exclude its combinations with VGC and sampling from the main experimental results 
    %because HBS is independently significant and does not impact the performance of VGC or sampling. 
    \ifconference{For completeness, we also analyze the performance of all possible combinations of the three techniques in the full version of this paper.}
    \iffullversion{For completeness, we also analyze the performance of all possible combinations of the three techniques in \cref{table:combinations}.}
}
\cref{fig:bucketing-exp} shows the relative running times (normalized to our method) across representative graphs.
Using one bucket involves constructing the bucket every round, 
and thus has low performance on dense graphs with large average degrees.
Using 16 buckets means to extract and partition vertices into buckets every 16 rounds, 
which is more efficient on dense graphs. 
However, as discussed in \cref{sec:bucketing}, on graphs with a constant average degree, 
%On sparse graphs, just constructing the first 16 buckets incurs an overhead and makes the performance suboptimal. 
using bucket structures does not theoretically improve the cost.
The overhead of managing the bucketing structure can make it much slower than using 1 bucket. 
%For graphs with very large coreness values ($10^5$ or more), 
%both 1- and 16-bucket requires to reconstruct buckets for many times, and thus both exhibit poor performance. 

%Recall that our \HBS{} start with a single bucket for sparse graphs with a average degree of 16 or less,
Recall that our \HBS{} start with a single bucket for sparse graphs with an average degree of 16 or less,
and start to maintain a hierarchy of buckets when $k$ reaches 16, the same threshold.
% This strategy is self-adaptive to the density of the graph. 
This self-adaptive strategy adjusts to the density of the graph.
% For all graphs, our \HBS{} is always as good as the better one between using 1 or 16 buckets, 
% and in extreme cases, can be much faster than both. 
% On sparse graphs with average degree less than 16, our strategy is essentially the same as using one bucket. 
Across all graphs, \HBS{} matches the performance of the better option between one or 16 buckets, 
and in some cases, performs much faster than both.
%For dense graphs with relatively large coreness
For dense graphs, our hierarchical structure is always as good as 16-bucket, 
and has superior performance on very dense graphs. On the extreme case \HCNS{}, using a \HBS{} is
$47.8\times$ faster than 1-bucket and $2.01\times$ than 16-bucket structure.

%Among the running times of all graphs, \GLsfull{} can be observed as an adversarial case for our \HBS{}. 
% As briefly mentioned previously, \GLs{} is an adversarial case for our \HBS{}. 
% Since \GLs{} has a maximum $\maxcoreness$ value of $10$,
% and we start to construct the \HBS{} at $k=9$, 
% the cost to construct the \HBS{} will not pay off since it is used only for two rounds. 
% In this case, the performance of using \HBS{} is still within 2$\times$ slower than using 1-bucket, and within 1.5$\times$ slower than using 16-bucket.
% %the construction of the \HBS is not the best choice,
% %which leads to a suboptimal performance of the \HBS on \GLsfull{}. 

\subsubsection{Scalability}
\input{./figures/fig-scalability.tex}

%Our algorithm has dominating advantages on low-diameter dense graphs
%that previous algorithms cannot handle efficiently (\cref{fig:overall_comp_seq}).
The two techniques proposed in \cref{sec:alg} both aim to improve parallelism. 
To evaluate this, we test the scalability of our algorithm on dense and sparse graphs, respectively, in \cref{fig:scalability}. 
%\cref{fig:scalability} shows the scalability curves for dense and sparse graphs, respectively.
The self-relative speedup for all graphs is also reported in \cref{table:fulltable}. 
In general, the scalability is more significant on larger graphs, 
since there are potentially more computation to exploit parallelism. 
%For the same reason, the dense graphs exhibit better scalability than sparse graphs. 
With our new techniques, most sparse graphs achieve a self-relative speedup of 50$\times$ or more. 
For dense graphs, the two graphs (\CW{} and \HL{}) achieves self-speedup exceeding 80$\times$. 
Interestingly, we observe that these two graphs can take advantage from both sampling and VGC (see \cref{fig:self_comparison}). 
This further indicates the effectiveness of our new techniques in improving parallelism. 
%The first group includes social networks, road networks, and $k$-NN graphs.
%The second group includes large-scale social networks with dense substructures
%and synthetic graphs.
%The graphs in the second group contain high-degree hubs,
%which may cause contention in the same peeling process 
%that slightly affect the scalability of our algorithm.
%As shown in \cref{fig:scalability},
%our algorithm has reasonably good scalability on most large-scale social networks and road networks,
%and very good scalability on web graphs.
%With the scalability experiments on a 96-core machine,
%our algorithm achieves a speedup of up to $84.9\times$ on web graphs and social networks,
%up to $64.7\times$ on road networks,
%and up to $66\times$ on \KNN and synthetic graphs.
%The detailed speedup results can be found in \cref{table:fulltable}.


\hide{

%Most of the graphs are from open-source datasets or existing research works~\cite{backstrom2006group, yang2015defining,nr,traud2012social,red2011comparing,kwak2010twitter, BoVWFI,Boldi-2011-layered,webgraph,roadgraph,geolife,wang2021geograph,cosmo50,fonollosa2015reservoir}. 
%We use large social networks such as \TWfull (\TW)~\cite{kwak2010twitter} and \OKfull (\OK)~\cite{yang2015defining}, 
%web graphs including \SDfull{} (\SD)~\cite{webgraph}, \CWfull{}(\CW)~\cite{webgraph}, \HLfull{}(\HL)~\cite{webgraph}, and \HLsfull{}(\HLs)~\cite{webgraph},
%with number of edges range from $85.7$ million to $226$ billion.
%The large-diameter road networks contain million-scale vertices and edges, 
%including \AFfull{} (\AF)~\cite{roadgraph}, \NAfull{} (\NA)~\cite{roadgraph}, \ASfull{} (\AS)~\cite{roadgraph}, and \EUfull{} (\EU)~\cite{roadgraph}.
%all of the road graphs have a maximum coreness value $ \maxcoreness=4$.
%In \KNN graphs, each vertex is a multi-dimensional data point and has $k$ edges pointing to its $k$-nearest neighbors (excluding itself). 
%including \CHfull{} (\CH)~\cite{fonollosa2015reservoir, wang2021geograph} and \emph{GeoLife}(\GLt, \GL, \GLs)~\cite{wang2021geograph, geolife} graphs with $k = 2, 5, 10$, and also \COSfull{} (\COS)~\cite{wang2021geograph, cosmo50} generated with $k=5$.
% Synthetic graphs are generated as large-scale power-law graphs, large grids or cubic nets, and graphs with huge $\maxcoreness$ value ($58916$)

%several graphs with salient features to imitate real-world graphs in extreme cases.
%They include very a large-scale power-law graph \HPLfull (\HPL), large $2-dimension$ and $3-dimension$ grids (\GRD{} and \CBC{}), 
%and graphs with a very large coreness \HCNSfull (\HCNS). 

%Specifically, our synthetic datasets include two-dimensional grids of size $1000 \times 1000$ \GRDfull{}(\GRD) and $3-dimention$ cubic networks of size $1000 \times 1000 \times 1000$ \CBCfull{}(\CBC). 
%An example of the two-dimensional grid is shown in \cref{fig:localsearch}.
%generated as a $k$-core graph with fully connected subgraphs at each coreness level and connections between nodes in adjacent coreness layers.
%We classify social and web graphs as low-diameter graphs, with diameters mostly within a few hundred. 
%Conversely, road, \KNN, and synthetic graphs are classified as large-diameter graphs, with diameters generally exceeding a thousand. 
%The synthetic graphs can be seen as adversarial cases with different features for the \kcore{} problem,
%which may cause inevitable performance bottlenecks for existing algorithms,
%including our own algorithm.
%The adversarial cases will be analyzed in detail in the following sections.

}


\revise{
\subsubsection{Burdened Span Analysis}
\label{sec:exp:span}
    To evaluate how VGC reduces the burdened span and thus improves the performance, 
    we use Cilkview~\cite{he2010cilkview} to measure the burdened span of different algorithms in practice. 
    Cilkview only applies to algorithms in OpenCilk~\cite{opencilk}, and 
    both our code and \Julienne{} can be easily compiled with OpenCilk. 
    We run our algorithm with and without VGC, as well as Julienne~\cite{dhulipala2017}, on all graphs.
\cref{fig:burdened_span} shows the speedup of our algorithm (with and without VGC) over \Julienne{} (higher is better), and the green dotted line at 1 represent the burdened span of \Julienne{}. 
%and the speedup of the running time of our two algorithms over Julienne in \cref{fig:burdened_span_speedup} in supplemental material.

As shown in \cref{fig:burdened_span},
the burdened span of our plain algorithm (without VGC) is a constant factor (1.6--7.9$\times$) better than that of Julienne~\cite{dhulipala2017},
primarily due to the simplicity of the online algorithm.
Since \Julienne{} is offline, running histogram and semisort incurs additional rounds of global synchronization. 
This gap, although as a constant factor in theory, slightly differs the measured burdened span and explains the performance differences. 

% VGC further reduces the burdened span,
% especially on sparse graphs, which is up to 147$\times$ better than \Julienne{}~\cite{dhulipala2017}.
VGC further reduces the burdened span, particularly on sparse graphs, achieving up to 147$\times$ improvement over \Julienne{}.
Combining with the results in \cref{fig:overall}, the speedup of burdened span matches the speedup of the running time of the algorithms. 
% For example, the three most significant speedups of burdened span using VGC are on \TRCE{}, \BBL{}, and \GRD{}. These are also the three graphs that we achieve the best speedup over \Julienne{} in time. 
For example, the three most significant burdened span improvements with VGC are on \TRCE{}, \BBL{}, and \GRD{}, which also show the best time speedups over \Julienne{}.
Such highly correlated trends can be observed on other graphs. 
This suggests that the reduction in burdened span (i.e., reducing the synchronization overhead) is the primary reason for our algorithm to outperform \Julienne{}. 
}





\subsubsection{Additional Experiments} 
In the full paper, 
we also evaluate our algorithm on the task of computing a specific \kcore{} of a graph given a particular value of $k$, and compare it with a graph library Galois~\cite{Nguyen2014}. 
With different values of $k$, on two graphs \OK{} and \TW{}, our algorithm outperforms Galois by 1.6--6.2$\times$. 








%In all cases, our algorithm is still competitive to the best implementation, 
%On the first two graphs \EU{} and \NA{}, our algorithm is within 13\% slower than the fastest baseline. 
%Compared to the fasted baseline, our algorithm is within 10\% slower on \EU{} and \NA{}. 
% The other graph \GLs{} with $\kmax=10$ simulates one adversarial case for our HBS. 
% Since our algorithm starts to build HBS at $k=9$,
% the cost may not pay off when the algorithm terminates at $\kmax=10$. 
% Even in this case, our algorithm is still within 66\% slower than the best baseline \PKC{}, 
% and achieves 20$\times$ speedup than the sequential implementation \BZ{}. 


% Such performance degeneration occurs on different sets of graphs for each baseline, 
% due to the distinct design of each algorithm. 
%Each baseline may exhibit performance degeneration by up to tens of times slower than our algorithm, which  on certain graphs. 
%Our algorithm achieves the best average performance on all the five graph types. 
%Indeed, our experiments in \cref{sec:exp:bucket} shows that using a single bucket may enable a better performance
%The performance degeneration for each baseline occurs in different sets of graphs, and is caused by the specific design of that algorithm. 
% All algorithms perform well on \knn{} graphs. 
% Due to the property of \knn{} graphs, all vertices have small degrees, the same coreness and only requires a few subrounds to finish. 
% Therefore, all algorithms have close performance, where our algorithm is slightly faster on average. 



%

%Our algorithm aims to improve the performance of the $k$-core decomposition algorithm by maximizing the parallelism on various types of graphs.
%As we can see from the results in \cref{fig:overall},
%The speedup of our algorithm is up to $30\times$ for social networks, \KNN graphs, and synthetic graphs.
%For web graphs and road networks, the speedup is up to $10.5\times$ and $1.5\times$, respectively.
%Specifically, our algorithm outperforms all the baseline on social networks and web graphs,
%achieves up to $3.17\times$ speedup over \GBBS,
%and has a dominating advantage over \PKC and \ParK on most graphs (up to $331\times$ speedup).

%On large-diameter graphs such as road networks and \KNN graphs, our algorithm also outperforms most of the baselines,
%especially than \GBBS, which has a suboptimal performance on these graphs due to the offline peeling process.
%Our algorithm achieves up to $1.99\times$ speedup over \GBBS,
%and up to $1.55\times$ speedup over the better time of \PKC and \ParK.
%\youzhe{On 2 of the road networks and \KNN graphs (\EU and \GLs), our algorithm has a longer running time than \PKC and \ParK.
%We will analyze the reasons in the following sections.}
%
%The synthetic graphs are generated as adversarial cases for the $k$-core decomposition problem,
%which can be seen as the worst-case scenarios for existing algorithms including our algorithm.
%Our algorithm outperforms the baselines on all synthetic graphs,
%with a speedup from $2.3\times$ to $54.9\times$ over \GBBS,
%and up to $20.1\times$ speedup over the better time of \PKC and \ParK.
%
%\GBBS and our implementation can handle all the graphs in the experiments,
%while the \PKC and \ParK algorithms encounter some ``out-of-memory'' (OOM) issues on large-scale web graphs (\HL and \HLs) and synthetic graphs(\HCNS).
%We also omit the running times over 1000 seconds as ``N/A'' in the full table (\cref{table:fulltable}).
%\hide{As the results show,
%our algorithm outperforms the baseline \GBBS algorithm on all graphs.
%For some large-diameter graphs, the runnning times of our algorithm are similar to the \PKC algorithm.
%However, our algorithm is much more efficient than the \PKC algorithm on most graphs,
%especially on large-scale social networks and web graphs,
%where the \PKC algorithm cannot handle the graphs due to the high volume of contention.
%}% The detailed results are shown in Figure~\cref{fig:overall}.
%% The detailed results are shown in \cref{fig:overall} and \cref{table:fulltable}.

% \input{./figures/fig-overall_heat.tex}

%This indicates that there is a dense subgraph with at least $\kmax$ vertices, each with degree at least $\kmax$. 
%In this case, using VGC does not change the performance, but sampling slightly decreases the performance by xx times.
% The overhead comes from the validation step in each round which processes all vertices in sample mode. 
