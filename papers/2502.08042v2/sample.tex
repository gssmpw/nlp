\subsection{Reducing Contention Using Sampling}\label{sec:sampling}


As mentioned, one challenge in the online peeling algorithm is the high contention 
in decrementing the \induceddegree{} of each vertex concurrently, especially for high-degree vertices.
To overcome this challenge, we propose a sampling scheme. 
In particular, when the degree of a vertex $v$ is over a certain threshold, 
we will turn it on \emph{sample mode}. 
When decrementing the \induceddegree{} of a vertex in the sample mode,
instead of atomically decrementing $\degreestar[v]$, 
we will take a sample for it with a certain probability, which we call the \emph{sample rate}. %, and track the total number of samples for each vertex. 
Based on the number of samples and the sample rate, we can estimate the expectation of the current \induceddegree{}.
When the expectation is far above $k$, it is unlikely that $\degreestar[v]$ drops to $k$,
and thus we do not need to know its exact \induceddegree{} in the current round. 
In other words, for a high-degree vertex $v$, we do not update $\degreestar[v]$ explicitly before we collect sufficient samples, 
thus avoiding updating $\degreestar[v]$ frequently. 

To do this, we maintain a structure \emph{sampler} for each vertex $v$ (defined in \cref{algo:sample_func}), recording whether $v$ is in the sample mode, its sample rate, and the number of samples taken so far. 
Based on the sampler of a vertex $v$, we can estimate the expectation of the true \induceddegree{} of $v$,
as well as the probability that the true \induceddegree{} is lower than $k$. 
%If the probability becomes higher than a tolerable error rate $\epsilon$, 
If the true \induceddegree{} is likely to be lower than $k$, 
we will resample $v$ to either use a higher sample rate,
or terminate the sample mode. 

%While the idea is simple, the intricate part is to ensure that each vertex $v$ terminates the sample mode and has the true \induceddegree{} of $v$ at the peeling round $\truecoreness(v)$. 
While the idea is simple, the intricate part is to ensure that at peeling round $k$, all vertices with coreness $k$ must not be in sample mode and have their the true \induceddegree{s} computed. 
%determine the time to resample or terminate the sample mode for a vertex when its true \induceddegree{} approaches $k$. 
%Indeed, if the a vertex $v$ does not terminate the sample mode before its \induceddegree{} hits $k$, the algorithm may miss $v$ in peeling round $k$,
%and will output a wrong coreness for $v$. 
The challenge is then to control the error probability for any vertex to miss its peeling round. 
Next, we present the details in our sampling algorithm, such that it gives correct answers with high probability. 

\subsubsection{The Framework with Sampling} 
We present our framework with sampling in \cref{algo:sampling}. 
%In \cref{algo:sampling} we present the algorithm framework when taking sampling into consideration.
We first introduce this framework at a high level, and then elaborate on each function in \cref{algo:sample_func}. 
The framework roughly follows \cref{algo:framework} with a few changes due to sampling. 
First of all, we start with initializing the sampler of $v$ by $\FInitSampler{}(v,0)$, 
which determines whether $v$ should be sampled when $k=0$, and if so, what its sample rate should be. 

\input{algo/pseudo_frame_w_sample.tex}
In general, two conditions may trigger updating the sampler of a vertex $v$. 
The first case is when $k$ approaches the true \induceddegree{} of $v$, 
and thus we need to count $\degreestar[v]$ more accurately. 
In particular, %at the start of each peeling round, 
when a peeling round starts, 
we validate all vertices in the sample mode remain safe to be in the sample mode until the next round (\cref{line:framework:security1,line:framework:security2,line:framework:security3}). 
%To do this, we compute the \emph{error probability} for $v$, 
%which is the probability such that the true \induceddegree{} of $v$ drops to $k+1$ or lower. 
%If the probability is higher than a tolerable error rate $\epsilon$, 
%we resample $v$. 
If the validation fails, we resample $v$. 
%In our code, $\epsilon$ is set to $10^{-{9}}$. 
%Resetting the sample will require to obtain the true value for $\degreestar[v]$.
%This means to obtain the true value for $\degreestar[v]$, based on which
%we choose to either increase the sample rate, or terminate sample mode. 
%We present the formula for this error probability in \cref{eqn:error_value}, 
%and more details about the \FSetSampler{} in \cref{sec:sampling:details}. 

%If not, we will recount the \induceddegree{} of $v$ and update $\degreestar[v]$ to its true value. 
%If $\degreestar[v]$ is still large, we may choose to still sample $v$, but increase its sample rate to make estimation more accurate.
%Otherwise, we will turn off the sample mode of $v$. More details about this part is given in \cref{algo:sample_func} and will be introduced below. 


Another case to ressample $v$ is when sufficient samples have been collected, and thus we know the \induceddegree{} of $v$ has dropped significantly. 
This happens in the $\FPeel$ function. 
Therefore, \FPeel{} also identifies a set of vertices $\countingbag$ that have collected sufficient samples and requires resampling. 
After the peeling process, we resample all vertices in $\countingbag$ (\cref{line:count_vertex_frame}).

%In $\FSetSampler(v,k,\frontier)$, we count the true \induceddegree{} of a vertex $v$, and initialize the sampler of $v$ again based on the new value of $\degreestar[v]$ and $k$. 
%Finally, if the new $\degreestar[v]$ is no more than $k$, $v$ needs to be added to the frontier $\frontier$ immediately. 
In the following, we explain the details of the \FPeel{} function under the sampling scheme, as well as the helper functions \FInitSampler{} and \FSetSampler{} to handle the samplers of each vertex. 

\input{algo/pseudo_sample.tex}
\subsubsection{Details about the Sampling Scheme} 
\label{sec:sampling:details}

As mentioned, each vertex $v$ maintains a \emph{sampler} structure including three fields: a boolean flag $\mode$ indicates whether $v$ is in the sample mode, a float $\rate$ as the sample rate, and an integer $\cnt$ as the number of samples taken by $v$ so far. 
In general, with sampling rate $p$ and $s$ samples taken, we expect the \induceddegree{} to reduce by $s/p$. 
However, to enable a high probability guarantee for the estimation $s/p$, the number of samples need to be $\Omega(\log n)$ (see \cref{sec:calc_error}). 
In our algorithm, we use a parameter $\exphits=\Theta(\log n)$ to denote the desired number of samples we need to take before we ressample $v$. 
Namely, we wish to take at least $\exphits$ samples to ensure high confidence for the estimation of the \induceddegree{} for $v$. 
When the \induceddegree{} of $v$ has decremented by a significant factor, 
the sample rate has to be adjusted (increased) accordingly. 
In our case, when the \induceddegree{} drops to a factor of $\reducerate$, we resample $v$. 
In practice, we use $\reducerate=10\%$. 
In other words, we use the current sampler to estimate the \induceddegree{} of $v$, 
until we expect the true $\degreestar[v]$ drops to $r\cdot \degreestar[v]$, at which point we reset the sampler of $v$. 

We start with presenting our \FInitSampler{} function, which initializes the sample parameters for a vertex $v$. 
Based on the discussion above, we hope that when $\exphits$ samples have been taken, 
we expect the \induceddegree{} of $v$ drops from $\degreestar[v]$ to $\reducerate\cdot \degreestar[v]$. Therefore, the sample rate should be set as $\exphits/((1-\reducerate)\cdot\degreestar[v])$ (\cref{line:set_sample_rate}). 
Accordingly, to determine whether a vertex is safe in sample mode, $\reducerate\cdot\degreestar[v]$ must still be large enough.
In our case, we require it to be larger than a preset threshold, as well as $k$. 
The latter condition is because when the expected \induceddegree{} approaches $k$, the probability that the true \induceddegree{} is smaller than $k$ will increase dramatically. 
%Therefore, for a vertex $v$ in the sample mode, we always require the expected value of $\degreestar[v]$ to be higher than a constant factor of $k$, which is set as $2k$ in our code. 
If $\reducerate\cdot\degreestar[v]$ is larger than both the sampling threshold and $k$, 
we turn on the sample mode for $v$, and set the parameters accordingly. 

We then present our \FPeel{} function with sampling in \cref{algo:sample_func}. 
This function follows the online peeling process in \cref{algo:peel_online}. 
The only difference occurs at \cref{line:sample_vertex_success,line:sample_hit_thres} in \cref{algo:sample_func}, when peeling $v$ and decrementing the \induceddegree{} of its neighbor $u$. 
If $u$ is in the sample mode, this indicates that $u$'s current \induceddegree{} is far above $k$. 
Therefore, instead of decrementing $\degreestar[u]$, we increment the number of samples of $u$, stored in $\sampler[u].\cnt$, with probability $\sampler[u].\rate$. 
This increment is also performed atomically, as multiple threads may be accessing $u$ concurrently. 
If the atomic increment causes $\sampler[u].\cnt$ to reach the desired number of samples $\exphits$, we add $u$ to $\countingbag$, 
which buffers all vertices that requires resampling. 

To resample a vertex $v$, we use function $\FSetSampler(v,k,\frontier)$. 
It begins by counting the actual number of \alive{} vertices of $v$, giving the true value of $\degreestar[v]$.
If $\degreestar[v]$ reaches $k$ or lower, the vertex will be added to the current frontier.
Then we call $\FInitSampler$ to reset the parameters for the sampler, based on the new value of $\degreestar[v]$. 

We note that due to sampling, it is possible that a vertex $v$ is still in the sample mode when the peeling round $k=\truecoreness[v]$. 
If so, we are unable to peel $v$ correctly because we do not know its true \induceddegree{} at that time. 
However, we will show that 1) it happens with low probability, and 2) if this happens, we can always detect it, 
and can restart with stronger sampling parameters. 
In \cref{sec:calc_error}, we introduce our approach to handle such potential errors. 

\subsubsection{Correctness Analysis} \label{sec:calc_error}
We now analyze the correctness of our sampling scheme, and show the error probability is low.
The only case for an error is when vertex $v$ is in the sample mode, but the true \induceddegree{} of $v$ is less than $k$ at the beginning of round $k$. 
We will show that using our $\FError()$ check, the error probability is very low, based on our choices of parameters ($\sampler[v].\rate$ and $\sampler[v].\cnt$).

\begin{lemma}\label{lem:coin-toss}
  Assume we toss $t$ coins each with $p$ probability to be a head, and obtain $s$ heads. 
  If $tp\ge 4c\ln n$, then $s\ge tp/4$ \whp{}.  
\end{lemma}
\begin{proof}
  This can be shown by using the lower part of the of the multiplicative form of the Chernoff bound.
  Let $s$ be the number of heads seen.
  In this case, $\mu=tp$, and $\delta=1-s/tp$.
  Hence we have:
  \begin{align*}
        \Pr\left[s<{tp\over 4}\right] &\le\exp\left(\delta^2\mu\over 2\right)=\exp\left(s-{s^2\over 2tp}-{tp\over2}\right)\\
        &<\exp\left(s-{tp\over 2}\right)<\exp\left(-{tp\over 4}\right)=n^{-{tp\over4\ln{n}}}=n^{-c}.
  \end{align*} 
  This proves the lemma.
\end{proof}

%Recall that for a vertex $v$ in sample mode, we use $d^*$ to denote its true \induceddegree{}, and $\degreestar[v]$ is the \induceddegree{} at the point when we last call $\FInitSampler$ on it (either at the beginning of the algorithm or the last time of resampling). 
%In Lemma~\cref{lem:coin-toss}, each increment of sample count on \cref{line:sample_vertex_success} is a coin toss with probability $p=\sampler[v].\rate$. 
%If $d^*$ drops to $k$ or lower, at least $t=\degreestar[v] - k$ edges have been removed (i.e., $t$ coin tosses), 
%and the expected number of sample count is $tp=\sampler[v].\rate\cdot(\degreestar[v] - k)$. 
%In our algorithm, the sample rate $p=\sampler[v].\rate=4c\ln n / ((1-r)\cdot\degreestar[v])$.
%In this case, for any $k<r\cdot \degreestar[v]$, we have $tp\ge 4c\ln n$ for given $c$. 
%%Therefore, function $\FError(v,k)$ checks either 1) $k$ is large enough, or $\sampler[v].\cnt<tp/4$, so we should exit the sample mode for $v$.
%%Therefore, function $\FError(v,k)$ validates 1) $k$ is large enough, or $\sampler[v].\cnt<tp/4$, so we should exit the sample mode for $v$.
%%In each round, we can check if $\sampler[v].\cnt<tp/4$: if so, we are good \whp{}; otherwise we will reset the sampler for $v$.
%%When $k\ge r\cdot \degreestar^*[v]$, the sampling mode will also terminate.
%
%\begin{theorem}\label{thm:sampling}
%  By setting $\sampler[v].\rate=4c\ln n / ((1-r)\cdot\degreestar[v])$ for sampling vertices, \cref{algo:sampling} is correct with high probability.
%\end{theorem}
%\begin{proof}
%  The case that \cref{algo:sampling} cannot compute \kcore correctly is when a vertex $v$ is in the sample mode, $v$'s induced degree $d^*<k$ but we do not have sufficient samples to detect it.
%  In \cref{algo:sampling}, we set the sample rate $\sampler[v].\rate=4c\ln n / ((1-r)\cdot\degreestar[v])$.
%  We now analyze the probability that \cref{algo:sampling} cannot detect $d^*<k(<r\cdot \degreestar[v])$. \yihan{???}
%  In this case, plugging in Lemma \ref{lem:coin-toss} with $t=\degreestar[v] - k+1$, $p=4c\ln n / ((1-r)\cdot\degreestar[v])$, and our termination condition  $s=\sampler[v].\cnt<tp/4$,
%  the error rate for one checking on \cref{line:framework:security2} in \cref{algo:sampling} is smaller than $n^{-c}$ for any constant $c>0$.
%  In \cref{algo:sampling}, \cref{line:framework:security2} can be checked for at most $\kmax n$ times.
%  Hence, by setting $c>2$ and taking the union bound proves that \cref{algo:sampling} is correct with high probability. 
%\end{proof}

With \cref{lem:coin-toss}, we now prove that \cref{algo:sampling} is correct \whp{}. 


\hide{
  \begin{proof}

    For a vertex $v$ in sample mode, we use $d^*$ to denote its true \induceddegree{}, and $\degreestar[v]$ is the \induceddegree{} at the point when we last call $\FInitSampler$ on it (either at the beginning of the algorithm or the last time of resampling). 
    In \cref{lem:coin-toss},
    each increment of sample count on \cref{line:sample_vertex_success} is a coin toss with probability $p=\sampler[v].\rate$. 
    If $d^*$ drops to $k$ or lower, at least $t=\degreestar[v] - k$ edges have been removed, corresponding to $t$ coin tosses, 
    and the expected number of sample count is $tp=\sampler[v].\rate\cdot(\degreestar[v] - k)$. 
    
    For a vertex $v$ in the sample mode, we first show that if its induced degree $d^*<k$, then \FError{} returns \false{} \whp{}. 
    First of all, if $k\ge r\cdot \degreestar[v]$, the function will fail at the first condition. 
    Otherwise, $k< r\cdot \degreestar[v]$. 
    In this case, $tp=(\degreestar[v] - k)\sampler[v].\rate$. Plugging in $\sampler[v].\rate=4c\ln n / ((1-r)\cdot\degreestar[v])$,
    we have $tp\ge 4c\ln n$. 
    Based on \cref{lem:coin-toss}, 
    this means that we should have collected $s\ge tp/4$ heads (successful samples) \whp{}. 
    Plugging in $s=\sampler[v].\cnt$, $t=\degreestar[v]-k$ and $p=\sampler[v].\rate$, 
    this means \FError{} will fail at the second condition. 
    Therefore, if $d^*<k$, \FError{} will return \false{} \whp{}.
    
    \hide{
      The high probability bound means that the error rate for one invocation of \FError{} is smaller than $n^{-c}$ for any constant $c>0$. 
      In \cref{algo:sampling}, $\FError$ can be called for at most $\kmax n\le n^2$ times. 
      Hence, setting $c>2$ and taking the union bound proves that \cref{algo:sampling} is correct with high probability. 
    }
    
    \hide{
      The high probability bound means that the error rate for one invocation of \FError{} is smaller than $n^{-c}$ for any constant $c>0$. 
      In \cref{algo:sampling}, $\FError$ can be called for at most $\kmax n\le n^2$ times. 
      % Hence, setting $c>2$ and taking the union bound proves that \cref{algo:sampling} is correct with high probability. 
      In our algorithm implementation, we set $c=8$ to ensure the correctness of the algorithm.
      The union bound guarantees that the probability of detecting an error for the whole algorithm is smaller than $n^{-8}$.
    }
    
    \end{proof}
}

\revise{
  \begin{theorem}\label{thm:sampling}
    For any constant $c\ge 1$, using $\mu = 4(c+2)\ln(n)$, \cref{algo:sampling} is correct with probability $1-n^{-c}$. 
  \end{theorem}
  \begin{proof}

  For a vertex $v$ in sample mode, we use $d^*$ to denote its true \induceddegree{}, and $\degreestar[v]$ is the \induceddegree{} at the point when we last call $\FInitSampler$ on it (either at the beginning of the algorithm or the last time of resampling). 
  In \cref{lem:coin-toss},
  each increment of sample count on \cref{line:sample_vertex_success} is a coin toss with probability $p=\sampler[v].\rate$. 
  If $d^*$ drops to $k$ or lower, at least $t=\degreestar[v] - k$ edges have been removed, corresponding to $t$ coin tosses, 
  and the expected number of sample count is $tp=\sampler[v].\rate\cdot(\degreestar[v] - k)$. 

  For a vertex $v$ in the sample mode, we first show that if its induced degree $d^*<k$, then \FError{} returns \false{} \whp{}. 
  First of all, if $k\ge r\cdot \degreestar[v]$, the function will fail at the first condition. 
  Otherwise, $k< r\cdot \degreestar[v]$. 
  In this case, $tp=(\degreestar[v] - k)\cdot\sampler[v].\rate$. Plugging in $\sampler[v].\rate=\mu/((1-r)\cdot \degreestar[v])=4(c+2)\ln n / ((1-r)\cdot\degreestar[v])$, we have $tp=4(c+2)\ln n \cdot ((1 - k/\degreestar[v])/(1-r))$. 
  Combining with the assumption that $k< r\cdot \degreestar[v]$, we have
  $tp\ge 4(c+2)\ln n$. 
   
  Based on \cref{lem:coin-toss}, 
  this means that we should have collected $s\ge tp/4\ge(c+2)\ln n$ heads (successful samples) \whp{}. 
  Plugging in $s=\sampler[v].\cnt$, $t=\degreestar[v]-k$ and $p=\sampler[v].\rate$, 
  this means \FError{} will fail at the second condition. 
  Therefore, if $d^*<k$, \FError{} will return \false{} \whp{}.
  At this point, we have shown that the error probability for a vertex $v$ to be in sample mode when $d^*<k$ is smaller than $n^{-(c+2)}$ for any constant $c>0$. 
  
  Note that to make the algorithm correct, all the function calls to \FError{} have to be correct. 
  In \cref{algo:sampling}, $\FError$ can be called for at most $\kmax n\le n^2$ times. 
  By the union bound, the probability that there exists a failed validation is at most $n^{-c}$. 
  Therefore, the algorithm is correct with probability at least $1-n^{-c}$. 
  \end{proof}
  
  % The high probability bound means that the error rate for one invocation of \FError{} is smaller than $n^{-c}$ for any constant $c>0$. 
  % In \cref{algo:sampling}, $\FError$ can be called for at most $\kmax n\le n^2$ times. 
  
    % The high probability bound means that the error rate for one invocation of \FError{} is smaller than $n^{-c}$ for any constant $c>0$. 
    % In \cref{algo:sampling}, $\FError$ can be called for at most $\kmax n\le n^2$ times. 
    % Hence, setting $c>2$ and taking the union bound proves that \cref{algo:sampling} is correct with high probability. 
    % In our implementation, we set $c=8$ to ensure the correctness of the algorithm.
    % The union bound guarantees that the probability of detecting an error for the whole algorithm is smaller than $n^{-8}$.
  
  \hide{
    The high probability bound means that the error rate for one invocation of \FError{} is smaller than $n^{-c}$ for any constant $c>0$. 
    In \cref{algo:sampling}, $\FError$ can be called for at most $\kmax n\le n^2$ times. 
    % Hence, setting $c>2$ and taking the union bound proves that \cref{algo:sampling} is correct with high probability. 
    In our algorithm implementation, we set $c=8$ to ensure the correctness of the algorithm.
    The union bound guarantees that the probability of detecting an error for the whole algorithm is smaller than $n^{-8}$.
  }
  
  From \cref{thm:sampling} and the definition of high probability in \cref{sec:prelim}, we have the following corollary. 
  
  \begin{corollary}\label{cor:sampling}
    \cref{algo:sampling} is correct with high probability. 
  \end{corollary}
  \vspace{-0.05in}

Note that error may occur at an unsampled vertex, if one of its neighbors is in sample mode and erred. 
  However, such a case must be caused by an error from a sampled vertex. Thus, the algorithm is correct as long as all sampled vertices are processed correctly. 
}

\subsubsection{Recover from Errors}

%Although \cref{thm:sampling} guarantees that the success rate for \cref{algo:sampling} is very high, we still want to detect and correct any possible errors caused by sampling, so that \cref{algo:sampling} is \emph{Las Vegas} instead of \emph{Monte Carlo}.
Although \cref{thm:sampling} guarantees that \cref{algo:sampling} succeed \whp{}, we still need to detect and correct any possible errors caused by sampling, so that \cref{algo:sampling} is \emph{Las Vegas} instead of \emph{Monte Carlo}.
A possible error happens when a vertex $v$'s induced degree drops below $k$ in the sample mode before round $k$.
This can be detected when $v$ exits the sampling mode, when we count the true value of $\degreestar[v]$.
Note that even if this value is smaller than $k$ in round $k$, it may not be an error since in normal peeling process, 
when peeling vertices in subrounds, some vertices in $k$-core may have $\degreestar[v]$ drops below $k$.
To verify the correctness, our algorithm will further check whether $v$'s \induceddegree{} is at least $k$ in the previous round.
If so, $v$ is still safe and have coreness $k$. 
Otherwise, we can restart the algorithm with a larger value of $\exphits$ or without sampling. 
However, due to the strong theoretical guarantee \cref{thm:sampling} provides, 
we have never encountered restarting in executing our \kcore algorithm. 

\subsubsection{Cost Analysis}\label{sec:sampling:contention}
Sampling does not affect the work-efficiency of our algorithm. %As mentioned, for vertices in the sample mode, 
given our setup of our sampling scheme, when a vertex $v$ exits the sampling mode with sufficient samples, $\degreestar[v]$ is reduced by a constant fraction \whp{}.
Therefore, the total cost to recount the true \induceddegree{} of $v$ is $O(\degree(v))$, which add up to at most $O(m)$. 
The cost validation step is proportional to the number of vertices in the sample mode, and thus is bounded by the active set size. Therefore, the proof in \cref{thm:work} still holds. 

%Here we will show that our sampling scheme can greatly reduce the contention.
Sampling can also reduce contention.
\revise{
  As defined in \cref{sec:prelim},
  The contention is the number of possible concurrent operations.   
  %As discussed above, $v$ can be resampled for $O(\log d(v))$ times \whp{}. 
  %Consider $\exphits=\Theta(\log n)$, 
  %the contention of decreasing the degree of vertex $v$ will be reduced from $O(d(v))$ to $O(\exphits)$ \whp{}.
  %At any time, the number of concurrent operations to $v$ is $O(\exphits+\text{threshold})$
  %The contention of the algorithm is reduced from $O(d_{max})$ to $O(\exphits)$ \whp{}.
  If a vertex $v$ is in the sample mode, the concurrent updates will be performed on $\sampler[v].\cnt$. 
  For all its $\degreestar[v]$ remaining neighbors, each of them will increment $\sampler[v].\cnt$ with probability $\sampler[v].\rate=\exphits/((1-r)\cdot \degreestar[v])$. Therefore, the contention on $\sampler[v].\cnt=\exphits/(1-r)=O((\log n)/(1-r))$. 
  If a vertex is not in the sample mode (or it has terminated from sample mode), base on the if-condition on \cref{line:sample_cond}, the \induceddegree{} is at most $O(k/r+\mathit{threshold})$. 
  Considering both $r$ and $\mathit{threshold}$ are preset as constants, combining both cases, the contention to handle a specific vertex $v$ is $O(\truecoreness[v]+\log n)$. 
  For high-degree vertices, this is much smaller than $O(d(v))$, which is the number of concurrent updates to $\degreestar[v]$ without sampling.  
}
%The total number of samples that need to be counted is $O(\log n\log d(v))$ \whp{}.
%For high-degree vertices, this is much smaller than $O(\degree(v))$, which is the number of concurrent updates to $\degreestar[v]$ without sampling.  
%This number can be much smaller than $O(d(v))$ without using sampling---both are the numbers of possible concurrent operations for a vertex.
%Compared to $O(d(v))$ concurrent updates to $\degreestar[v]$ without using sampling, our new solution incurs much lower contention. 
%The sampling scheme will introduce some additional checkings per vertex per round, but the number is asymptotically bounded, similar as the analysis in \cref{thm:work}.

%\subsubsection{Choice of Parameters}
%\label{sec:choice_parameters}
%In \cref{sec:calc_error}, we showed how to set the two parameters $\exphits$ and $\reducerate$ theoretically.
%In practice, we pick $\mu=32$, and $\reducerate=10\%$.
%Then $\sampler[v].\rate$ can be computed accordingly for a given vertex $v$.
%The $\exphits$ is the expected number of hits in the sampler, which is a predefined global constant, 
%and we set it to $\log n$ in our code, where $n$ is the number of vertices in the graph,
%which will guarantee the correctness of the algorithm with high probability, as shown in the proof in \cref{sec:calc_error}.
% It guarantees that the error probability is small enough to be negligible according to the Chernoff bound,
% according to the proof in \cref{sec:calc_error}.
%The $\reducerate$ is a key factor that determines whether to reset the sampler,
%as well as the expected \induceddegree{} value after the sampling round.
%We set $\reducerate$ to $10\%$ in our code, which is a trade-off between the accuracy of the estimation and the number of samples taken.
%\todo{}
%
%
%$K$-core decomposition is challenging under the parallel setting 
%on dense graphs with high-degree vertices, such as social networks.
%As described in ~\cref{sec:intro}, and shown in ~\cref{fig:overall_comp_seq},
%other parallel algorithms, such as \ParK and \PKC,
%especially suffer from such high-volume contention in the frontier processing step.
%For the \FPeel function in ~\cref{line:process_bucket}, 
%high contention may occur when removing the edges incident to the high-degree vertices.
%To mitigate the contention, we propose a degree sampling model to reduce the number of atomic operations
%without affecting the correctness and work-efficiency of the algorithm.
%
%
%
%\myparagraph{Sampling Model.}
%The high-volume contention occurs while decreasing the counter at ~\cref{line:decrease_degree} in our framework.,
%To reduce contention of atomic operations while removing the edges incident to the vertices in the frontier,
%we implement a sampling model to avoid heavy direct \faadec operations.
%For high-degree vertices, defined as those with a degree greater than a predefined threshold, 
%we maintain a sampler structure with an internal counter for successful trials.
%
%% \input{algo/pseudo_set_sampler.tex}
%% Specifically, for a high-degree vertex $v$, 
%% the sampler structure includes a counter to record the number of successful sample cases observed.
%Specifically, we first use function $\FSetSampler$ to set the sampler structure for any eligible vertex $v$ in the graph (~\cref{algo:set_sample_func}).
%The function is called at the beginning of the algorithm to initialize the sampler structure for each vertex,
%which will determine whether the vertex is in sample mode or not (~\cref{line:set_sample_mode}),
%and if it satisfies the condition
%$\degreestar[v] \cdot \reducerate{} > c\cdot k$ (~\cref{line:sample_cond}),
%where $c$ is a constant, the vertex is set to $\sampler.\mode = \textsc{Ture}$ ~\cref{line:set_sample_mode}.
%The condition is designed to ensure that the vertex is still far from the current coreness value $k$ after decrementing to a factor of $\reducerate$.
%We then set the sample rate $\sampler[v].\rate = \exphits / ((1 - \reducerate) \times \degreestar[v])$
%for each vertex~\cref{line:set_sample_rate}.
%The $\exphits$ is the expected number of hits in the sampler, which is a predefined constant,
%and is also the threshold for the counter $\sampler[v].\cnt$ to determine whether the sampling process should be stopped. 
%
%% For a vertex $v$ that is set to sample mode, 
%% we create a sampler container includes a counter $\sampler[v].\cnt$ to record the number of successful sample cases observed,
%% and a sample rate $\sampler[v].\rate$ to determine the probability of a successful sample hit.
%% The sample rate $\sampler[v].\rate$ for vertex $v$ is determined as follows:
%% $\sampler[v].\rate = \frac{\exphits}{\degree(v) - \degree^*(v)}$,
%% where $\degree(v)$ is the initial degree of $v$ and $\degree^*(v)$ is the expected \induceddegree{} value after the sampling round.
%% As we remove each $u \in N(v)$ from the neighbor list, the counter is increased by $1$ using atomic \faainc{}  with probability $\samplerateproof$,
%% and take no action with probability $1-\sampler[v].\rate$.
%% This approach mitigates heavy contention.
%
%To better illustrate the sampling model, we provide the pseudo code in ~\cref{algo:sample_func} and ~\cref{algo:sampling}.
%The process can be seen as a coin flip process with a biased probability, which is a standard Bernoulli trial,
%where the probability of success is $\sampler[v].\rate$.
%The counter is increased by $1$ with probability $\sampler[v].\rate$ 
%and remains unchanged with probability $1-\sampler[v].\rate$(~\cref{line:sample_vertex_success}).
%The sampling process keeps happening until the counter reaches $\exphits$ (~\cref{line:sample_hit_thres}).
%After the counter reaches $\exphits$, the vertex is added to the counting bag and to be counted for the exact \induceddegree{} value
%in the end of the subround ~\cref{line:count_vertex_frame}.
%
%\myparagraph{Dynamic Checking.}
%\label{sec:dynamic_checking}
%Due to the characteristics of the peeling algorithm,
%the value of the round $k$ will increase while the degree of the nodes will decrease.
%If the degree of a node drops below $k$ before the sampler has triggered the counting mechanism at ~\cref{line:sample_disable_and_count},
%the sampling algorithm will encounter errors.
%Therefore, we introduce a dynamic checking mechanism that ensures the correctness of the sampling process with high probability.
%We describe the dynamic checking function in ~\cref{algo:check_sample_func}.
%The computations for the checking mechanism are determined by the current round value of $k$, 
%the sample rate $\sampler[v].\rate$, and the counter value $\sampler[v].\cnt$.
%The probability of an error occurring at each checkpoint is calculated using the Chernoff bound.
%The sampling process continues only if this probability is below a predefined constant, $\epsilon$.
%Thus we can always ensure the correctness of the sampling process of vertex $v$ $w.h.p.$ until next checkpoint.
%Once the bound of the probability of error occurring is above $\epsilon$ (~\cref{line:check_error}),
%we stop the sampling process and switch to the deterministic mode with its real \induceddegree(~\cref{line:stop_sample_after_check}).
%We show the correctness of the sampling process $w.h.p$ in ~\cref{theorem_sample} and ~\cref{corollary2}.
%
%% lemma regarding checking probability of over-sampling at each checkpoint
%\begin{theorem}
%    \label{theorem_sample}
%    % At each checkpoint, the probability of a sampling error occurring is given by
%    For the peeling subrounds in round $k$, the probability of a sampling error occurring for vertex $v$ (\induceddegree{} $\degreestar[v] < k$) is given by
%     $$\mathbb{P}[error] \leq \exp(\frac{-\Delta' \cdot \sampler[v].\rate + 2\sampler[v].\cnt - \frac{{\sampler[v].\cnt}^2}{\Delta' \cdot \sampler[v].\rate}}{2})$$
%     , where $\Delta' = \degreestar[v] - k$ and $\degreestar[v]$ is the \induceddegree{} of $v$ when we set the sampler structure at ~\cref{line:set_sampler_frame},
%     \youzhe{k or k + 1 should we define here; whether use a character for the gap; if so, what character}, 
%     $\sampler[v].\rate$ is the sample rate for $v$,
%     and $\sampler[v].\cnt$ is current counter value, 
%     i.e., the number of successful sample hits observed.
%\end{theorem}
%    
%
%\begin{proof}
%While doing peeling step for $v$ with sampling at ~\cref{line:sample_vertex_frame}, 
%there is a probability of $\sampler[v].\rate$ to add $1$ to the counter $\sampler[v].\cnt$, and a probability of $(1-\sampler[v].\rate)$ to do nothing.
%Thus, $X_u \overset{\mathrm{i.i.d.}} \sim Bernoulli(\sampler[v].\rate)$. Let $X = \sum_{u = 1}^{|N(v)|} X_u$, then $\exphits = \mathbb{E}[X]$.
%$\Delta'$ is the gap between the current round value $k$ and the \induceddegree{} of $v$ at the sampler setting point, i.e., $\Delta' = \degreestar^*[v] - k$.
%$\Delta$ is a random variable that represents the gap between the current round value $k$ and the \induceddegree{} of $v$ at the end of the subround.
%
%From the lower part of the multiplicative form of the Chernoff bound: 
%% $$\mathbb{P}[n' \geq \Delta | X = \sampler[v].\cnt] = \mathbb{P}[X \leq \sampler[v].\cnt | n' = \Delta] \leq \exp(\frac{-\Delta \times \sampler[v].\rate + 2 \times \sampler[v].\cnt - \frac{{\sampler[v].\cnt}^2}{n^* \sampler[v].\rate}}{2})$$    
%        \begin{align*}
%        \mathbb{P}[\Delta \geq \Delta' | X = \exphits'] 
%        &= \mathbb{P}[X \leq \sampler[v].\cnt | \Delta = \Delta'] \\
%        &\leq \exp\left(\frac{-\Delta' \cdot \sampler[v].\rate + 2 \cdot \sampler[v].\cnt - \frac{({\sampler[v].\cnt})^2}{\Delta' \cdot \sampler[v].\rate}}{2}\right)
%        \end{align*}
%        
%    
%    \end{proof}
%
%\begin{corollary}
%\label{corollary2}
%    With error checking actions applied at each checkpoint, the sampling process contains no error with high probability,
%    i.e., all vertices in sample mode have their \induceddegree{} values correctly counted before it drops below the current round value.
%    
%\end{corollary}
%    
%
%The sampling model does not affect the work-efficiency of the proposed framework,
%as the edges that are removed at ~\cref{line:decrease_degree} have two possible outcomes:
%either they are removed by atomic operations on the counters 
%or they are removed by the sampling process which will be counted using parallel counting operations 
%that has the same work.
%It is also independent to the local search technique because the sampling process is only triggered while applying the atomic operations for decrementing the counters.


\hide{
Given the sample rate $\sampler[v].\rate$, the expected number of sample count is $\sampler[v].\rate\cdot(\degreestar^*[v] - k+1)$.
The probability to see fewer than $\sampler[v].\cnt$ samples is upper bounded by $f(\degreestar^*[v] - k+1,\sampler[v].\rate,\sampler[v].\cnt)$.
Given $v$ and $k$, we will use this function as the $\FError(v,k+1)$ function in \cref{algo:sampling}.
Note that the error rate is for \emph{no more than} $\sampler[v].\cnt$ samples, which is an overestimate than having \emph{exact} $\sampler[v].\cnt$ samples.
Also, one can check that $f(t,p,s)$ is monotonic decreasing when $t$ is increasing, so plugging in $t=\degreestar^*[v] - k+1$ is an upper bound of other possible $\degreestar[v]$ that are smaller than $k-1$.

We now show that even with a relax form
 \cref{algo:sampling} is correct with high probability.

The correctness of the sampling idea relies on keeping $\degreestar[v]\geq k$ for any vertex $v$ and any peeling round $k$.
We will analyze the our sampling scheme succeeds with high probability.
The sampling process can be consider as a series of coin flips.
Assume the probability of a head (i.e., increasing the counter in the sampler) is $p$.
The total number of flips we take is $d[v] \cdot (1-r)$,
where $d[v]$ is the initial degree of $v$ at the starting point of sampling and $r$ is the reduce rate.
\begin{theorem}
    For a vertex $v$ with sampling rate $p=\mu/((1-r)d)$, at every stage of the sampling process, $\degreestar[v] < k$ happens with low probability.
\end{theorem}
\begin{proof}
    We can consider the sampling process as a series of coin flips, where each flip returns a head with probability $p=\mu/((1-r)d)$, where $\mu$ is the number of expected samples,
    $r$ is the reduce rate,
    and $d$ is the initial degree.
    We assume $\mu'$ is the number of observed samples.
    We expect $d(1-r)$ total coin flips,
    but when $\degreestar[v] < k$ happens, there are at least $d-k$ coin flips.
    The deviation can be computed by 
    $$1-\epsilon \leq(d-k)/(d(1-r))$$
    which solves to $\epsilon \geq (dr-k)/(d-k)$.
    We can plug in the deviation factor into the Chernoff bound, 
    \begin{align*}
    \Pr[\mu'\leq (1-\epsilon)\mu] &\leq \exp(-\epsilon^2\mu/2) \\
    &= \exp(-(\frac{dr-k}{d-k})^2\mu/2)
    \end{align*}

    \youzhe{we don't know k when setting $\exphits$, k is increasing}
    If we set $\mu=\log n(\frac{d-k}{dr-k})^2$, 
    $$\Pr[\mu'\leq (1-\epsilon)\mu]\leq \exp(-\log n) = n^{-c}$$
    which is with high probability in $n$.
\end{proof}

\subsubsection{Checking Sampling Errors} 
\label{sec:check_error}
%In the previous part, we have introduced our algorithm to incorporate sampling.
% The effectiveness of the samplers relies on controlling the error probability that the true \induceddegree{} has become too small, which is the $\FError(v,k)$ function on \cref{line:framework:security2} in \cref{algo:sampling}. 
% Given sample rate $\sampler[v].\rate$ and $\sampler[v].\cnt$ samples taken,
% we will analyze the probability that the true $\degreestar[v]$ is no more than $k$ using Chernoff bound in the following theorem. 
We analyze the effectiveness of the samplers by checking the 
probability of a sampling error occurring at vertex $v$, i.e., the actual \induceddegree{} $\degreestar[v] < k$
at each checkpoint in the sampling process at~\cref{line:framework:security2} in \cref{algo:sampling}.
\begin{theorem}
    \label{thm:sampling}
    % At each checkpoint, the probability of a sampling error occurring is given by
    %For the peeling subrounds in round $k$, 
    %We assume the following shorter expression for the values in $\sampler[v]$:
    %$$\rateproof  = \sampler[v].\rate \qquad \cntproof =\sampler[v].\cnt$$
    For a vertex $v$, let $\rateproof=\sampler[v].\rate$ be the sample rate, and $\cntproof=\sampler[v].\cnt$ be the number of samples taken. 
    The probability of a sampling error occurring at vertex $v$, i.e., the actual \induceddegree{} $\degreestar[v] < k$, is given by
     \begin{equation}\label{eqn:error_value}
        \mathbb{P}[\text{error}] \leq \exp(\frac{-\Delta' \cdot \rateproof + 2 \cdot \cntproof - \frac{{\cntproof}^2}{\Delta' \cdot \rateproof}}{2})
      \end{equation}
     , where $\Delta' = \degreestar^*[v] - k$ and $\degreestar^*[v]$ is the \induceddegree{} of $v$ when the last time we set the sampler for $v$ by $\FInitSampler$.
     %\youzhe{k or k + 1 should we define here; whether use a character for the gap; if so, what character}, 
\end{theorem}
    

\begin{proof}
While doing peeling step for $v$ in $\frontier$ with sampling at ~\cref{line:sample_peel}, 
there is a probability of $\rateproof$ to add $1$ to the counter $\cntproof$, and a probability of $(1-\rateproof)$ to do nothing.
Thus, $X_u \overset{\mathrm{i.i.d.}} \sim Bernoulli(\rateproof)$. Let $X = \sum_{u = 1}^{|N(v)|} X_u$, then $\exphits = \mathbb{E}[X]$.
$\Delta'$ is the gap between the current round value $k$ and the \induceddegree{} of $v$ at the sampler setting point, i.e., $\Delta' = \degreestar^*[v] - k$.
$\Delta$ is a random variable that represents the gap between the current round value $k$ and the \induceddegree{} of $v$ at the end of the \subround.

From the lower part of the multiplicative form of the Chernoff bound: 
% $$\mathbb{P}[n' \geq \Delta | X = \sampler[v].\cnt] = \mathbb{P}[X \leq \sampler[v].\cnt | n' = \Delta] \leq \exp(\frac{-\Delta \times \sampler[v].\rate + 2 \times \sampler[v].\cnt - \frac{{\sampler[v].\cnt}^2}{n^* \sampler[v].\rate}}{2})$$    
        \begin{align*}
        \mathbb{P}[\Delta \geq \Delta' | X = \cntproof] 
        &= \mathbb{P}[X \leq \cntproof | \Delta = \Delta'] \\
        &\leq \exp\left(\frac{-\Delta' \cdot \rateproof + 2 \cdot \cntproof - \frac{{\cntproof}^2}{\Delta' \cdot \rateproof}}{2}\right)
        \end{align*}
        
    
    \end{proof}
}
% With theorem \cref{thm:sampling} we can show the following corollary, which we prove in the supplemental material. 
% \begin{corollary}
% \label{corollary2}
%     %With error checking actions applied at each checkpoint, the sampling process contains no error with high probability,
%     %i.e., all vertices in sample mode have their \induceddegree{} values correctly counted before it drops below the current round value.
%     \cref{algo:sampling} computes the correct coreness for all vertices with high probability. 
% \end{corollary}
    

%The sampling model does not affect the work-efficiency of the proposed framework,
%as the edges that are removed at ~\cref{line:decrease_degree} have two possible outcomes:
%either they are removed by atomic operations on the counters 
%or they are removed by the sampling process which will be counted using parallel counting operations 
%that has the same work.
%It is also independent to the local search technique because the sampling process is only triggered while applying the atomic operations for decrementing the counters.

