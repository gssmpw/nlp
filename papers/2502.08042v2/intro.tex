\section{Introduction}\label{sec:intro}
% Analyzing real-world graphs is important in various applications. 
Analyzing real-world graphs is crucial in a wide range of applications.
One of the most widely-used techniques for identifying densely connected regions in a network is the $k$-core decomposition~\cite{seidman1983network,matula1983smallest}.
This problem about subgraph structure detection finds applications in various fields, 
including social network analysis~\cite{kong2019k, zhang2017finding, kitsak2010identification,malliaros2020core, boguna2004models, giatsidis2011evaluating}, 
risk assessment~\cite{kong2019k, morone2019k, garcia2017ranking, malliaros2020core, burleson2020k, zhang2010using}, 
bioinformatics~\cite{kong2019k, wuchty2005peeling, malliaros2020core, cheng2013local, emerson2015k}, 
and system robustness analysis~\cite{kong2019k, malliaros2020core, burleson2020k, yao2022study, sun2020new}.

Given a graph $G = (V, E)$ with $n=|V|$ vertices and $m=|E|$ edges, the $k$-core of $G$ is the maximal subgraph $G' = (V', E')$ of $G$ in which every vertex has degree at least $k$ (see an illustration in \cref{fig:kcore_eg}).
The $k$-core decomposition of a graph $G$ identifies a sequence of non-empty subgraphs $G_0, G_1, \ldots, G_{k_{\max}}$ for all possible $k$ values, where $G_i$ is the $i$-core of $G$. 
%Since $G_i\subseteq G_{i-1}$, we define the \emph{coreness} of a vertex $v$, noted as $\truecoreness(v)$, such that $v$ is in $G_{\truecoreness(v)}$ but not in $G_{\truecoreness(v)+1}$. 
The \defn{coreness} of a vertex, denoted as $\truecoreness[v]$, is the maximum value of $k$ such that $v$ is in $G_k$. 
The coreness of a graph, denoted as $\kmax$, is the maximum coreness among all vertices. 
%The \emph{coreness} of a graph, noted as $\maxcoreness$, is the largest coreness among all vertices in this graph. 
The output of the $k$-core decomposition is the coreness for each vertex, which can be used to reconstruct any $G_i$. 

%Since today's real-world graphs have ever-growing sizes that can reach terabytes, 
Consider the ever-growing size of today's real-world graphs, 
which can reach terabyte scale,
it is essential to consider parallelism in the \kcore{} computation. 
%This paper focuses on the exact $k$-core decomposition problem in parallel settings.
In this paper, we focus on the high-performance algorithm to compute exact $k$-core decomposition with high parallelism.
While \kcore{} is simple in the sequential setting, and various efficient solutions exist~\cite{seidman1983network,batagelj2003m}, 
parallel $k$-core is a notoriously hard problem. 
%Sequentially, we can bucket-sort all vertices based on their degrees, then repeatedly remove vertices with the smallest degrees and update their neighbors' degrees.
Starting from $k=0$, the sequential algorithm keeps removing vertices with degree no more than $k$ and update their neighbors' degrees, until all nodes have a degree at least $k+1$, obtaining the $(k+1)$-core. Then the algorithm will increment $k$ and repeat this process. 
An example of this process, usually referred to as the ``peeling'' process, is given in \cref{fig:kcore_eg}.
This sequential approach requires $O(n+m)$ number of operations (aka.\ work or time complexity). 
% Unfortunately, parallelizing this simple sequential approach is highly non-trivial.
However, parallelizing this sequential method is highly non-trivial.
Despite \kcore{} is studied in many papers and implemented in many libraries~\cite{gonzalez2014graphx,gonzalez2012powergraph,shun2013ligra,dhulipala2017,kabir2017parallel, cheng2011efficient,montresor2011distributed, li2021k,dasari2014park, esfandiari2018parallel, liu2024parallel, mehrafsa2020vectorising,tripathy2018scalable,zhao2024speedcore, ahmad2023accelerating, konduri2022implementation, khaouid2015k,zhang2017accelerating, zhao2024pico, wen2018efficient}, 
many challenges remain, both in theory and in practice, to achieve a parallel \kcore{} algorithm that is simple, efficient, and scalable on various types of graphs.  
For instance, in \cref{fig:overall_comp_seq}, we show that on a 96-core machine, each state-of-the-art parallel \kcore solution can be slower than a sequential implementation on certain graphs, and the ``worst cases'' vary significantly between algorithms. 
This highlights the intricacies of optimizing the \kcore{} algorithm in the parallel setting. 

\textbf{In this paper, we propose a parallel \kcore{} algorithm with a series of novel algorithmic techniques. 
Our algorithm is efficient, practical, and achieves high performance across various graph types.}
In the following, we briefly overview the challenges in existing work, and our solutions to overcome them. 

\myparagraph{Theoretical challenges and our contributions.} 
The probably most surprising challenge we have observed is that, we are unaware of any analysis on work-efficiency\footnote{The \emph{work} of a parallel algorithm is its time complexity running on one core. A parallel algorithm is \emph{work-efficient} if its work is the same as the best sequential time complexity.} for the existing parallel implementations.
%Namely, they all use $\omega(n+m)$ operations---
Most of existing parallel \kcore{} decomposition algorithms have $O(m+\maxcoreness n)$ work, 
rendering a significant overhead caused by parallelism. 
The only known work-efficient parallel \kcore{} algorithm is from \Julienne~\cite{dhulipala2017}.
Unfortunately, this algorithm is mainly of theoretical interest. 
During the entire peeling process, it maintains a \emph{bucketing structure}, 
which maps each possible degree $d$ to all vertices with degree $d$.  
%and maintains this mapping during the entire peeling process. 
However, maintaining the full mapping incurs
%The bucketing structure described in their paper may incur %high constant cost and complex algorithmic details, which may lead to 
performance overheads and coding complexity in practice. 
%As a result, instead of adopting the bucketing structure as described, their implementation used a simplified version. 
Hence, their implementation~\cite{dhulipala2017} did not adopt the bucketing structure as described 
and used a simplified alternative, which only maintains at most $b$ buckets at any time. 
It remains unknown if their simplified implementation is theoretically efficient. 
%Hence, it remained to be an open problem: \emph{does there exist a practical and work-efficient parallel \kcore algorithm?}

\input{figures/fig-kcore_def.tex}
In this paper, we present a surprisingly simple parallel \kcore algorithm, shown in \cref{algo:framework}, with $O(n+m)$ work. 
%Unlike the algorithm from \Julienne/\GBBS~\cite{dhulipala2017}, our algorithm does not need to maintain the degrees of all vertices, thus making itself simple, practical, and efficient.
%\revise{
%    The primary contribution of the framework is the introduction of a simple and efficient algorithmic approach for parallel $k$-core decomposition. 
%    It synthesizes elements from various existing parallel algorithms into a streamlined solution, enabling further optimization and improved performance. \yihan{moved the discussion to sec 3}
%}
Our framework does not need the complicated bucketing structure to achieve work-efficiency, making it simple to implement. 
Our analysis indicates that the simplified implementation of \Julienne{} is also work-efficient, 
and some other existing algorithms with $O(m+\maxcoreness n)$ work can be made work-efficient with minor modifications. 

%Indeed, we refer to \cref{sec:alg} as an algorithmic framework, and we can replace the $\FPeel(\cdot)$ function by any implementation---the theoretical efficiency is satisfied as long as the work of the $\FPeel(\cdot)$ function is proportional to the neighbor size of the incident vertices.
%The details of the algorithm and analysis is given in \cref{sec:framework}.
%Followed by this framework, we can also show that the implementation from \GBBS~\cite{dhulipala2017} is indeed work-efficient, although this is previously unknown through our correspondence with the paper authors.

\myparagraph{Practical challenges and our contributions.} While our analysis indicates that achieving work-efficiency in \kcore{} algorithms is not hard,
%we still observe challenges for achieving a fast \kcore{} implementation in practice, 
% translating this into a fast \kcore implementation in practice remains challenging, 
% due to the difficulty to achieve high parallelism. 
translating this into a fast, practical implementation is challenging due to the difficulty of achieving high parallelism.
Existing implementations perform the peeling process in either an online or an offline manner. 
When peeling vertices with degree~$k$, the offline approach, implemented in \Julienne{}~\cite{dhulipala2017}, 
chooses to collect all vertices that require degree decrement in a batch, counts the number of occurrences for each of them,
and applies all of them in parallel. 
The benefit of the algorithm is race-freedom. 
However, since this approach is fully synchronous, meaning that the degree decrement for each batch (which we call a \defn{subround} in this paper) must fully finish before the next batch starts, causing high overhead to synchronize threads between subrounds. 
%\youzhe{subround not defined here. Should we mention it?}\yihan{it's mentioned in the previous sentence. I make it bold now.}
%Collecting vertices and counting their occurrences also requires to process the data for several times in each subround. 
One adversarial example is a $\sqrt{n}\times \sqrt{n}$ grid (the \GRD{} graph in \cref{fig:overall_comp_seq}), 
that incurs $O(\sqrt{n})$ subrounds.
An illustration of this process is given in \cref{fig:localsearch}(a).
On this graph, \Julienne{} is slower than a sequential implementation due to high scheduling overhead. 
%where $O(\sqrt{n})$ subrounds of thread synchronization are required.
%Such scheduling overhead makes \Julienne{} slower than a sequential implementation on this graph. 

Many other parallel \kcore{} implementations, such as \pkc{}~\cite{kabir2017parallel} and \park{}~\cite{dasari2014park}, 
use the \emph{online} approach. 
% When peeling a vertex $v$,
% it directly decrement the degrees of $v$'s neighbors by an atomic operation. 
When peeling a vertex $v$, the degrees of its neighbors are directly decremented via atomic operations.
%In the parallel setting, the decrements need to be performed by an atomic operation.
The online algorithm follows a simple framework, and %the peeling round touches each relevant vertex and edge exactly once. 
certain optimizations can also be used to reduce the synchronization cost~\cite{kabir2017parallel}. 
% However, for a high-degree vertex, there can be a huge number of concurrent decrements on its degree. 
%This may cause high contention and degrade the performance. 
% Such contention caused by parallelism makes \pkc{} and \park{} slower than a sequential algorithm on dense networks 
However, for high-degree vertices, a large number of concurrent decrements can cause heavy contention, 
which degrades performance.
As a result, \pkc{} and \park{} are slower than sequential algorithms on dense networks
(e.g., \TW{} and \SD{} in \cref{fig:overall_comp_seq}).  

To resolve these challenges, we propose several novel techniques. 
We adopt the online framework with two additional techniques, given in \cref{sec:online-framework}. 
% The first is a \emph{sampling} scheme to avoid too many concurrent decrements to the degree of a vertex at the same time. 
The first is a \emph{sampling} scheme to reduce concurrent decrements on high-degree vertices.
% For a vertex $v$ with large degree, we will select a subset of its neighbors, 
% and only the removal of these neighbors cause the decrement of the degree of $v$. 
For a vertex $v$ with a large degree, 
we select a subset of its neighbors, 
and only the removal of these neighbors decrements $v$'s degree.
%Using the number of sampled neighbors remaining in the graph, we can estimate the true degree of $v$. 
The key challenge of sampling is to accurately estimate the true degree of each vertex $v$, 
and to identify $v$ when it is ready to be peeled. 
%When the estimation indicates that $v$'s degree drops below a threshold, we turn off the sample mode and count $v$'s degree normally. 
%This effectively guarantees that the number of concurrent updates to the degree of $v$ is never too large. 
% We carefully analyzed the sampling algorithm to ensure that it gives correct estimation with high probability. 
Our analysis ensures the sampling algorithm provides correct estimates with high probability.
On dense graphs, sampling improves the performance of our code by up to 4.31$\times$. 

Our second technique is to reduce the high synchronization cost between peeling subrounds. 
% By using the online framework, our algorithm allows for an asynchronous approach to decrement degrees. 
By using the online framework, our algorithm allows asynchronous degree decrements.
%In general, in the peeling process, each vertex $v$ will be processed with one thread. 
When we peel a low-degree vertex $v$, the actual computation to process all its neighbors is minimal,
and the scheduling overhead to create and synchronize this thread may dominate the cost.
%In our algorithm, when we decrement the degree of $v$'s neighbor $u$, if the degree of $u$ hits the current value of $k$,
%we will put $u$ in a local queue and use the same thread to process $u$. We control the local queue size to a fixed value,
%so that the cost of each thread is upper bounded by the same value, leading to better load balancing.
% In our paper, 
To address this,
we propose a \emph{local search} algorithm to reduce the number of subrounds and hide scheduling overhead. 
This technique improves the performance of our code by up to 31.2$\times$. 

% Finally, we proposed a new bucketing structure to further improve the performance. 
Finally, we propose a new hierarchical bucketing structure (HBS) to further improve performance in \cref{sec:bucketing}.
Instead of using no bucket structure (or equivalently, using a single bucket) as in our proposed framework, 
or a fixed number of buckets as in \Julienne{}, 
% our new solution is a hierarchical bucketing structure (HBS). 
HBS efficiently manages vertices in each round. 
%In fact, although our analysis shows that opening a constant number of buckets is sufficient to achieve work-efficiency, 
%in practice, the cost to initialize the buckets every constant number of peeling rounds can be overwhelming when $\maxcoreness$ is large.
%We show that when the average degree of a graph is large, using a trivial bucket structure contributes a large portion of the running time.
%Based on the design of HBS, roughly speaking, a vertex with degree $d$ will be check for no more than $O(\log d)$ times, which is an significant improvement over $O(d)$.
%We provide more details of our HBS design in \cref{sec:bucketing}. 
% This data structure efficiently manages vertices to be processed in each round. 
On average (geometric mean across all graphs), our new design improves the performance by 1.6$\times$ compared to the plain version (no-bucket structure),
and is $1.3\times$ faster than using 16 buckets. 
%Empirically, this strategy can improve the plain version in \cref{algo:framework} by up to $4.6\times$ on real-world graphs and $29\times$ on adversary synthetic graphs (see \cref{fig:bucketing-exp}).
\hide{
On the other hand, maintaining bucketing structures can incur high overhead for graphs with a small $\maxcoreness$. 
In our implementation, we start with no bucketing structure, and switch to maintaining a number of buckets once the current peeling round $k$ reaches a threshold (8 in our code). Instead of organizing vertices based on their exact degrees, we will maintain $O(\log n)$ buckets, the $i$-th bucekts maintaining vertices with degree in range $[k+2^{i-1},k+2^{i})$. 
In this way, we avoid frequently initializing the buckets and moving elements across buckets, which leads to significant performance improvement on graphs with large corenesses. \yihan{maybe mention some experimental results}
}

\input{figures/fig-overall_compare_seq.tex}
With our new framework and techniques, our algorithm achieves high performance in experiments. 
We compare our implementation against three state-of-the-art parallel implementations: \Julienne{}, \Park{} and \PKC{}, 
testing a diverse set of graphs, including social networks, web graphs, road networks, \KNN graphs, and other graphs including synthetic graphs and simulation graphs. 
Due to different designs, each baseline has lower performance than a sequential implementation on certain graphs. 
Our algorithm consistently performs well on all types of graphs, 
% and always outperform the best sequential algorithm by 7.3-84 times. 
outperforming the best sequential algorithm by 7.3--84$\times$.
%On average across the dense graphs, our algorithm is 1.14--3.17$\times$ faster than \Julienne{}, 3--330$\times$ faster than \Park{}, and 1.5--29$\times$ \PKC{}.
On dense graphs, 
our algorithm is faster than all baselines, 
with 1.14--3.17$\times$ faster than \Julienne{}, 2.6--315$\times$ faster than \Park{}, and 1.5--28$\times$ faster than \PKC{}.
%which is 1.95$\times$ faster than \Julienne{}, 18$\times$ faster than \Park{}, and 7.7$\times$ \PKC{}, all on average.
% Our algorithm is also faster than the baselines on most sparse graphs, except for 2 cases, which are all within 12\% slower than the best baseline.
On most sparse graphs, our algorithm outperforms the baselines, except for two cases where it is within 12\% of the best baseline.
On sparse graphs, our algorithm is up to 53$\times$ faster than \Julienne{}, 28$\times$ faster than \Park{}, and 11$\times$ faster than \PKC{}. 
We also carefully evaluated the performance gain from the three new techniques in experiments. 
%Our algorithm is $2\times - 331\times $ speedup over the state-of-the-art implementations on different graph categories,  (see \cref{fig:overall}).
%Our sampling scheme can achieve up to $1.85\times$ speedup on dense low-diameter graphs over the baseline, which to be a bottleneck in the previous implementations (see \cref{fig:exp-sampling}). 
%Our sampling scheme enables $1.85\times$ speedup on dense graphs over the baseline, which to be a bottleneck in the previous implementations (see \cref{fig:exp-sampling}).
%Experimental study on our VGC shows that it can achieve up to $11.9\times$ speedup on large-diameter graphs (see \cref{fig:self_comparison}).
%We also show the hierarchical bucketing structure can be adapted to graphs with different $\maxcoreness$ values, and can reduce the bucket time percentage from $85.1\%$ to $4.4\%$ on large-$\maxcoreness$ graphs(see \cref{fig:bucketing-exp}).
%We also show that our implementation is highly scalable, and can achieve near-linear speedup on a 96-core machine.

We release our code in~\cite{kcorecoderelease}. \ifconference{For page limit, we provide the full version of our paper in the supplementary material. }
%\url{https://anonymous.4open.science/r/Parallel-Kcore-B871}


%\todo{check}

%We believe that our parallel framework, along with the enhanced techniques introduced, are both highly portable and versatile. 
%These advancements make them well-suited for integration into other parallel graph algorithms, 
%particularly those focused on graph decomposition and subgraph discovery.








%\myparagraph{Theoretical challenges and our contribution.}
%The probably most surprising challenge we have observed is that, we are unaware of any efficient parallel \kcore implementation that is work-efficient.
%Namely, they all use $\omega(n+m)$ operations---most of them have $O(m+k_{\max}n)$ work, rendering a significant overhead caused by parallelism.
%The only work-efficient parallel \kcore algorithm we are aware of is from~\cite{dhulipala2017}.
%This algorithm uses a so-called ``bucketing structure'' that maintains the degrees of all vertices during the entire algorithm.
%Unfortunately, although being theoretically efficient, this approach incurs a significant overhead hidden asymptotically, so even the implementation in the paper~\cite{dhulipala2017} did not use this.
%Hence, it remained to be an open problem: \emph{does there exist a practical and work-efficient parallel \kcore algorithm?}
%
%In this paper, we show a surprisingly simple parallel \kcore algorithm, as shown in \cref{algo:framework}.
%Unlike the algorithm from \Julienne/\GBBS~\cite{dhulipala2017}, our algorithm does not need to maintain the degrees of all vertices, thus making itself simple, practical, and efficient.
%The work bound is achieved by an amortized approach.
%Indeed, we refer to \cref{sec:alg} as an algorithmic framework, and we can replace the $\FPeel(\cdot)$ function by any implementation---the theoretical efficiency is satisfied as long as the work of the $\FPeel(\cdot)$ function is proportional to the neighbor size of the incident vertices.
%The details of the algorithm and analysis is given in \cref{sec:framework}.
%Followed by this framework, we can also show that the implementation from \GBBS~\cite{dhulipala2017} is indeed work-efficient, although this is previously unknown through our correspondence with the paper authors.
%
%\myparagraph{Challenges on improving parallelism and our contribution.}
%The new results in \cref{sec:alg} explains why \GBBS outperforms other existing baselines such as \ParK~\cite{dasari2014park} and \PKC~\cite{kabir2017parallel} .
%However, \GBBS still suffers from a new issues, and a major one is the limited parallelism.
%The reason \GBBS achieves work-efficiency is by deploying a fully offline setting.
%For the $i$-th peeling round, it takes all vertices with degrees no more than $i$, scans over all their incident edges, counts the appearance of the other endpoints, then decrements their degrees, finally packs those vertices whose degrees drop to $i$ or less, and repeat on these vertices.
%This will create significant extra rounds (we call it ``steps'' in this paper) and there is no parallelism across different steps.
%For instance, consider a $r$-by-$r$ grid as shown in \cref{fig:grid_eg}.
%\GBBS needs $r/2$ steps of peeling, although the entire grid is a 2-core graph and only needs one round of peeling using the sequential approach mentioned before.
%We tested such a grid with $r=10^4$---\GBBS takes 14.8 seconds using 96 CPU cores but the sequential algorithm only uses 14.1 seconds.
%No improvement for parallelism is shown due to these excessive synchronization barriers.
%
%% \input{figures/fig-grid.tex}
%
%In this paper, we propose a novel online framework for parallel peeling.
%This means when processing a vertex, we directly decrement the counters of the other endpoints of the incident edges, and process those vertices immediately once their degrees are small enough.
%This framework enables to deploy a recent inspiring concept called ``vertical granularity control (VGC)''~\cite{wang2023parallel}, that can achieves automatic dynamic load balance and thus high parallelism.
%We show more details of this part in \cref{sec:vgc}.
%This online solution can drastically improve the performance on the grid to 0.27 seconds, rendering a 55$\times$ improvement.
%
%Another challenge on implementing the online framework is how to concurrently decrement the degree counters.
%The na\"ive solution is to use some atomic operations such as \cas{} (\CAS).
%However, this will incur significant contention on large-degree vertices, commonly seen on scale-free networks such as social networks and web graphs.
%To overcome this issue, we show a sampling scheme in \cref{sec:sampling} so that only a small percentage of the updates is apply to the atomic counters.
%Empirically, this technique can lead to up to 5$\times$ speedup (on \CWfull, see \cref{fig:exp-sampling} for details).
%
%\myparagraph{Challenges on maintaining the bucketing structure and our contribution.}
%Recall that in \cref{sec:alg} and \cref{algo:framework} we show that not maintaining the degree-to-vertex mapping (the bucketing structure) is theoretically efficient.
%However, the hidden constant can be large and dominate the running time on graphs with large coreness.
%For instance, if a vertex is peeled in the $i$-th round, then the vertex will be check for $i$ times in the first $i$ rounds.
%This part can consume more than 90\% running time on some large social and web graphs.
%
%In this paper, we proposed a hierarchical bucketing strategy so that a vertex with degree $d$ will be check for no more than $O(\log d)$ times.
%This strategy can significantly accelerate the steps to find vertices with the lowest degree.
%Meanwhile, the high-level idea of this approach is simple, making the implementation reasonably easy (see if we want this sentence).
%We provide more details of our hierarchical bucketing structure in \cref{sec:bucketing}.
%Empirically, this strategy can improve the plain version in \cref{algo:framework} by up to $4.6\times$ on real-world graphs and $29\times$ on adversary synthetic graphs (see \cref{fig:bucketing-exp}).
%
%\myparagraph{Experimental Evaluation.}
%Using the new work-efficient parallel \kcore framework with the technical improvements, our \kcore implementation is fast and stable in performance.
%We compare our algorithm with the sequential baseline \BZ~\cite{batagelj2003m} and state-of-the-art parallel solutions \GBBS, \ParK, and \PKC.
%On 26 real-world and synthetic graphs we tested, our \kcore algorithm is fastest on 22 of them, and competitive on the others.
%Compared to each baseline,
%Our algorithm is up to $54.9\times$ faster than \GBBS, $331.3\times$ faster than \ParK, and $18.9\times$ faster than \PKC.
%The stable performance comes from the theoretical efficiency---our \kcore algorithm always has $O(m+n)$ work.
%We also analyze each of our new technique in \cref{sec:exp}.
%We open-source our code at xxx.
%
%
%
%
%~
%
%The most well-known sequential solution for exact \kcore decomposition,
%proposed by Batagelj and Zaversnik (\BZ) ~\cite{batagelj2003m}, is both simple and optimal, operating with $O(n+m)$ work.
%However, it is challenging to parallelize due to its global dependencies and iterative nature.
%The \BZ algorithm inherently takes a peeling-based approach, where in each peeling round $\rho_i$, 
%it identifies the vertices whose degree equals the current coreness level $k$. 
%These vertices and their incident edges are then removed, and the degree of the remaining neighbors is updated for the next peeling round.
%This process happens simultaneously in each peeling round $\rho_i$, making parallelization non-trivial.
%
%The \ParK algorithm~\cite{dasari2014park} and \PKC algorithm~\cite{kabir2017parallel} are two parallel algorithms that follow a similar framework. 
%For the $\FPeel$ function mentioned before, 
%they use atomic operations to remove the incident edges of the vertices in the frontier. 
%In contrast, Dhulipala et al.~\cite{dhulipala2017} proposed a theoretically work-efficient parallel \kcore decomposition by using a bucketing-based structure. They employ a semisort-based histogram algorithm to calculate the updated degrees of the neighbors. 
%All the algorithms above must examine all vertices at each coreness level $k$ to generate the frontier.
%
%To summarize, both the asynchronous implementations based on atomic operations and the histogram-based approach have inherent limitations concerning parallelism. 
%The former experiences high contention on the same vertices within the same coreness level, which can degrade performance, particular for large-scale graphs containing high-degree hubs. 
%The latter achieves work efficiency but lacks room for further optimization due to dependencies within the histogram-based approach.
%
%In this paper, we propose a \kcore decomposition algorithm that offers both theoretical work-efficiency and high practical parallelism across a wide range of graphs. 
%To the best of our knowledge, this is the first work to achieve both theoretical work-efficiency and a highly parallel implementation. 
%We first describe the proposed parallel algorithmic framework for \kcore decomposition, which is work-efficient with a proper implementation of the $\FPeel$ function.
%Then based on this framework, we introduce a novel parallel algorithm that significantly improves the parallelism of the \kcore decomposition process.
%
%The key idea behind our algorithm and implementation is to improve parallelism by eliminating synchronization barriers and reducing contention on the same vertices in multiple dimensions. 
%To accomplish this, we first utilize a novel data structure called the \emph{hash bag} to store the vertices in the frontier.
%The hash bag is a lock-free, hash-based data structure that supports efficient insertions in parallel.
%With the hash bag, our algorithm can process the vertices in the frontier in parallel with less contention and synchronization overhead.
%
%For the further improvements, we introduce a novel optimization technique called vertical granularity control (VGC). 
%VGC helps minimize unnecessary synchronization overhead and supports better load balancing in parallel workflows.
%Additionally, we design a sampling-based optimization with strong theoretical guarantees to reduce contention. 
%This optimization significantly reduces contention during the peeling process without affecting the algorithm's correctness or efficiency, 
%resulting in notable performance improvements on various dense, scale-free graphs such as social networks and web graphs.
%
%Another key challenge for \kcore decomposition, especially in parallel settings, is that different types of graphs present varying perspectives of difficulties for improving parallelism. 
%For instance, high-degree hubs may cause contention during the peeling process, while large-diameter graphs may require a large number of peeling rounds. 
%To address these challenges, we propose a hybrid bucketing structure that adapts to different graph types. 
%Rather than using a fixed number of buckets, we dynamically adjust the number of buckets based on the graph's degree distribution. 
%In bucketing-based algorithms, the number of buckets is a critical factor affecting performance. 
%Our hybrid bucketing structure is designed to handle graphs with varying maximum coreness values and connection structures through dynamic bucket adjustments.
%We summarize our contributions as follows:
%
%\todo{}
%
%
%\hide{
%
%Despite the fact that many solutions have been proposed to compute the approximate $k$-core decomposition of large-scale graphs, both sequentially~\cite{king2022computing}, 
%as well as in parallel settings~\cite{esfandiari2018parallel,ghaffari2019improved,liu2022parallel}, 
%exact $k$-core decomposition algorithms are still crucial for many applications that require precise results, such as financial risk assessment and infrastructure planning~\cite{yao2022study, morone2019k}.
%In the following, we provide a brief overview of the applications under the exact $k$-core decomposition problem setting and related work.
%
%\todo{TO BE EXTENDED}
%\begin{itemize}
%    \item \textbf{Social network analysis.} 
%    By identifying the densest subgraphs in the network, 
%    $k$-core decomposition can be used as a fundamental building block for
%    detecting communities that are included in certain coreness range~\cite{kitsak2010identification}.
%    $K$-core decomposition can also be used to detect influential roles in the network~\cite{boguna2004models}.
%    
%
%    \item \textbf{Risk assessment.}
%    The $k$-core decomposition can be used to identify the most critical strongly related groups in the network,
%    which can be used to assess the risk of a certain system with complex connections,
%    such as financial systems~\cite{burleson2020k} and ecological systems~\cite{morone2019k, garcia2017ranking, burleson2020k}.
%
%
%    \item \textbf{Infrastructure planning.}
%    The $k$-core decomposition can be used to identify the most critical components in the network,
%    which can be used to optimize the infrastructure planning and resource allocation,
%    such as transportation networks~\cite{yao2022study, sun2020new}.
%    
%\end{itemize}
%
%
%
%
%The $k$-core has been studied in a variety of settings since it was first introduced by Seidman~\cite{seidman1983network}.
%The first sequential algorithm which computes all coreness values are given by Matula and Beck. 
%The algorithm first arranges vertices by their degree with a bucket sort process,
%and then repeatedly deletes the vertices with degree matching the current coreness level,
%until all vertices are removed.
%The neighbors of the deleted vertices are then moved to the corresponding bucket with the induced degree. 
%Since all the vertices are visited once and the degree induction applies to every edge at most twice ,
%the work of their algorithm is $O (m + n)$. 
%Batagelj and Zaversnik (\BZ) give an sequential implementation that runs in the same time bounds~\cite{batagelj2003m}.
%
%Due to the random nature of the algorithm, 
%both read and write operations involve unpredictable memory access patterns, 
%resulting in suboptimal performance on large-scale graphs. 
%As the sizes of real-world graphs have grown to billion-node scales in recent years, improving the efficiency of $k$-core decomposition algorithms has become increasingly crucial.
%Many shared-memory parallel or distributed computing frameworks have been proposed to support $k$-core decomposition as a fundamental building block for graph analytics,
%such as \textsf{GraphX}~\cite{gonzalez2014graphx}, \textsf{Powergraph}~\cite{gonzalez2012powergraph}, \textsf{Ligra}~\cite{shun2013ligra}, and \Julienne~\cite{dhulipala2017}.
%
%}
%
%% Parallelizing the Matula-Beck algorithm presents significant challenges due to its inherent global dependencies and iterative nature. 
%% Each iteration requires the continuous updating of vertex degrees, 
%% where the removal of one vertex can affect the entire graph, 
%% complicating the division of work into independent tasks. 
%% In another word, the Matula-Beck algorithm is inherently sequential,
%% including many concurrent operations in parallel settings, 
%% including the induction of degrees of the neighbors of the deleted vertices,
%% add re-arrangement of vertices in the bucket data structure.
%
