\section{Algorithm Framework and Design}\label{sec:alg}
In this section, we present our algorithmic framework for $k$-core with theoretical analysis. 
Many existing implementations fit into this framework. 
However, as mentioned, we are unaware of any analysis to prove their work-efficiency. 
For example, both \ParK~\cite{dasari2014park} and \PKC~\cite{kabir2017parallel} proved $O(\maxcoreness n+m)$ time complexity for their algorithms, 
where $\maxcoreness$ can be $O(\sqrt{m})$ in the worst case. 
\Julienne{}~\cite{dhulipala2017} introduced a \kcore{} algorithm with $O(m+n)$ work. 
However, this algorithm is mostly of theoretical interest.
Due to complicated algorithmic details that may cause a performance overhead in practice, 
their open-source code implemented a simpler (and more practical) version instead of the algorithm described in their paper.
It is therefore unclear what the cost is for their implementation. 
This motivates us to consider \emph{whether there exists a simple and work-efficient parallel \kcore{} solution}. 
Interestingly, we observed that a simple algorithmic framework can essentially give $O(m+n)$ work. 
Our results show that the implementation in \Julienne{} is indeed theoretically efficient, as well as a simple variant of \Park{} or \PKC{}. 
\revise{In fact, while many existing algorithms roughly follows the high-level idea of the framework, 
we believe that the formalization of the framework helps enable simple and general analysis for work-efficiency,
reveal the advantages/disadvantages of existing approaches, and inspires the main techniques in this paper. }
In the following, we show the framework in \cref{algo:framework}, describe and analyze it in~\cref{sec:framework}, and finally discuss different strategies for the peeling process in~\cref{sec:peeling}. 



%,including existing peeling strategies and our proposed strategies.


\subsection{Framework}\label{sec:framework}


We present our algorithm framework in \cref{algo:framework}.
% The framework is based on the peeling algorithm that follows the same framework as \ParK~\cite{dasari2014park}.
%The input of the algorithm is a graph $G = (V, E)$, where $V$ is the set of vertices and $E$ is the set of edges.
%The output is an array $\coreness[\cdot]$ contains the coreness values of all vertices $V$ (see the definitions in \cref{sec:prelim}).
%where the value is the smallest $k$-core subgraph that the vertex can be found in.
%Given a graph $G=(V,E)$, the algorithm computes the coreness of each $v\in V$ and returns it in $\truecoreness[v]$ (see the definitions in \cref{sec:prelim}).
Given a graph $G=(V,E)$, the algorithm returns the coreness $\truecoreness[v]$ of each $v\in V$.

As with the existing work, our algorithm is also \emph{frontier-based}, 
where vertices with the same degree are peeled in parallel, organized as a \emph{frontier}, denoted as $\frontier$. 
%Our algorithm maintains two sets of vertices $\alivevertices$ and $\frontier$.
%$\frontier$ includes the vertices in the frontier to be peeled currently.
In addition, our algorithm maintains a set of \defn{\alive{} vertices} $\alivevertices$, 
which are the vertices that have not been peeled. 
We call $\alivevertices$ the \defn{\alive{} set}. 
In the peeling process, we use $\degreestar[v]$ to track the \induceddegree{} of $v$. 
We first initialize $\degreestar[v]$ as the degree of $v$ in the input graph, and $\alivevertices$ as the entire vertex set $V$. 
We use $k$ to denote the current peeling round, starting with $k=0$. 
In round $k$, we first initialize the frontier $\frontier$ by extracting the vertices from $\alivevertices$ with \induceddegree{} $k$ (\cref{line:extract}). 
The coreness of all vertices in the frontier can be set to $k$. 
We then peel all vertices in $\frontier$ (\cref{line:peel}). 
For each vertex $v\in \frontier$, peeling it will decrement the \induceddegree{} for each of its neighbors by 1. 
%We then iteratively peel vertices from $\frontier$ in \cref{line:process_bucket} (i.e., removing the incident edges and decrementing the degrees of the other endpoints).
%During this process, the degrees of other vertices in $\alivevertices$ may drop below $k+1$.
When the \induceddegree{} of any vertex $u$ hits $k$, $u$ is added to a set $\nextfrontier$, which becomes the next frontier. 
We repeat this process until the frontier becomes empty. 
We call each iteration on \cref{line:subround} a \defn{subround}, which deals with one frontier, and call each iteration on \cref{line:round} a \defn{round}, which can contain multiple subrounds with the same coreness value $k$. 
After each round, we update the \alive{} set $\alivevertices$ (\cref{line:pack}) by only keeping vertices with \induceddegree{s} larger than $k$.
%We collect these vertices in $\frontier$, peel them again, until $\frontier$ is empty.
%We then move to the next peeling round.

\input{algo/pseudo_framework.tex}

\hide{The $\FPeel(\frontier,k)$ function can be implemented differently as long as the work is $O(|\frontier|+\sum_{v\in \frontier}{N(v)})$ \youzhe{in parallel} (i.e., the number of incident vertices and edges).
In line 12-19 (add the labels and refs) we show how it can be done sequentially~\cite{seidman1983network,batagelj2003m} with the designated work bound.
We will later show how to parallelize this step either trivially (in \cref{sec:peeling}) or more efficiently (in \cref{sec:online-framework}).}

The cost of the algorithm relies on three parts: 1) \cref{line:extract} that extracts the initial frontier of a round, 2) the \FPeel{} function that processes all neighbors of the frontier, and 3) \cref{line:pack} that refines the \alive{} set $\alivevertices$. 
Next, we will show that, with proper implementations of the three parts, the framework in \cref{algo:framework} has $O(n+m)$ work. 

\begin{theorem}
    \label{thm:work}
        Assuming $O(|\frontier|+\sum_{v\in \frontier}{\degree(v)})$ work for the $\FPeel(\frontier,\cdot)$ function on~\cref{line:process_bucket}, and
        $O(|\alivevertices|)$ work for the functions on \cref{line:extract,line:pack}, where $\alivevertices$ is the input \alive{} set), 
        the total work of \cref{algo:framework} is $O(n+m)$.
\end{theorem}
    \begin{proof}
        The main work of the algorithm comes from two parts: the $\FPeel()$ function, and maintaining the sets $\frontier$ and $\alivevertices$ on 
        \cref{line:extract,line:pack}.
        
        For the $\FPeel{}$ function,
        note that each vertex will appear in exactly one frontier. 
        As we assumed in the theorem, the work of \FPeel{} is proportional to the total number of neighbors of vertices in the frontier.
        Therefore the total work is $\sum_{v\in V} (1+\degree(v))=O(n+m)$.
        
        %We then show that the total work of the two functions that generates $\frontier$ on \cref{line:extract} and $\alivevertices$ on \cref{line:pack} is also $O(n+m)$. 
        We now analyze the two functions that generates $\frontier$ on \cref{line:extract} and $\alivevertices$ on \cref{line:pack}. 
        The theorem assumes that the cost for both of them is proportional to the input size $|\alivevertices|$, and thus they have the same asymptotic cost. Let $\alivevertices_i$ be the \alive{} set of round $i$, which contains all vertices with coreness at least $i$. 
        %Namely, a vertex $v$ with coreness $c$ will appear in all \alive{} sets $\alivevertices_{0..c}$. 
        The total cost is:
        \vspace{-.5em}
        \begin{align*}
        \sum_{i=0}^{\maxcoreness} |\alivevertices_i| &= \sum_{i=0}^{\maxcoreness} |\{ v~|~v\in V, \truecoreness[v]\ge i\}|\\
        &=\sum_{v\in V} (1+\truecoreness[v]) \le \sum_{v\in V} 1+\sum_{v\in V}\degree(v) = O(n+m)
        \end{align*}        
        \vspace{-.1em}
        The third step uses the fact that $\truecoreness(v)\le \degree(v)$ for all $v\in V$. 
        \hide{The main work of the algorithm comes from two parts: the $\FPeel()$ function and generating the sets $\alivevertices$ in \cref{line:filter_out_alive} and $\frontier$ in \cref{line:generate_frontier}.
        
        For the $\FPeel()$ function, since every vertex will be peeled exactly once.
        Hence, the total work for this part is $O(n+m)$.
        Now let's mainly consider the work for \cref{line:filter_out_alive}---\cref{line:generate_frontier} has the same work as generating $\alivevertices$ the last time, so it will only double the work and not asymptotically affect the bound.
        Note that when a vertex is alive in the $k$-th round, it must have degree more than $k$.
        Hence, a vertex with degree $d$ can at most appear in $\alivevertices$ in the first $d$ rounds.
        The total size of all active sets $\alivevertices$ in all rounds is also bounded by $O(n+m)$.
        Generating $\alivevertices$ and $\frontier$ only requires to linearly scan over the active sets, leading to the total work as stated.
        }
    \end{proof}
        
The key in the analysis is to bound to total size of all active sets.
Although it does not seem too complicated, to the best of our knowledge, we are unaware of existing work with this result. 

\hide{
Note that this paper is about parallel \kcore.
We will discuss how to parallelize the $\FPeel()$ function in \cref{sec:peeling} and \cref{sec:online-framework}.
Other steps in this algorithm can be parallelized trivially---a parallel partition can generate $\alivevertices$ in \cref{line:filter_out_alive} and $\frontier$ in \cref{line:generate_frontier} using $O(n')$ work and $O(\log n')$ span where $n'$ is the input array size.
}

%We note that it is easy to implement parallel functions for \cref{line:extract} and \cref{line:pack} efficiently in $O(|\alivevertices|)$ work by using the standard \pack{} function introduced in \cref{sec:prelim}. 

Note that the two functions on \cref{line:extract} and \cref{line:pack} can be implemented in $O(|\alivevertices|)$ work by the standard parallel \pack{} function introduced in \cref{sec:prelim}. Hence, the key component in this framework is an efficient \FPeel{} function. 
%In fact, while it is not too hard to implement a work-efficient algorithm based on this framework, 
%achieving high performance in practice on a variety of graph types is challenging. 
%In the next section, we will propose our new solution to overcome the challenges. 
In the following, we will introduce two main peeling approaches for in existing implementations, 
how they can be analyzed using our framework, and
their advantages and drawbacks to process certain types of graphs. 




\hide{
the peeling procedure consists of two main steps:

\begin{enumerate}
    \item \textbf{Decrement counters.} 
    In this step, the vertices in the frontier $\frontier$, thus the neighbors of the vertices in the frontier are removed from the graph,
    as shown in the $\FPeel$ function in \cref{line:decrease_degree}.
    The degrees of the neighbors are decremented by $1$ for each removed vertex.


    \item \textbf{Generating the frontier set.}
    The affected neighbor vertices will have their new degrees after the counters finish the decrementing step.
    If there are neighbors with degrees equal to the coreness value of the current round,
    they are added to the frontier set for the next round. 
    Else, the peeling process will terminate for the current $k$ value.
    The process happens in the $\FPeel ()$ function in \cref{line:update_frontier}.
\end{enumerate}


The two steps are executed in parallel, either by using a batch-synchronize strategy or on-the-fly processing,
which does not require batch update after each peeling round within a certain $k$-core value.

After removing all the vertices in the same peeling round $k$,
we increase the coreness value $k$ by $1$,
and filter out the vertices that are still alive in parallel.
The algorithm terminates when all vertices are removed from the graph.



%by proposing the \emph{bucketing structure}. 
%proposed the \emph{bucketing structure} to achieve a \kcore{} algorithm with $O(m+n)$ work. 


%their open-source code did not implement the theoretically-efficient algorithm described in their paper, but a simpler (and more practical) version. 
%It is unclear what the cost is for their implementation.
%Recall that we are unaware of any existing implementations for \kcore{} decomposition, including all the baselines we compared in this paper (cite them all), has the same $O(m+n)$ work (time complexity) as the simple sequential algorithms~\cite{seidman1983network,batagelj2003m}.
%Namely, the parallelism comes with an overhead.

}
