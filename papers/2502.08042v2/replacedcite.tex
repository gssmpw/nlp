\section{Related Work}
The $k$-core decomposition problem has been extensively studied since the introduction by Seidman____.
The first sequential algorithm was proposed by Matula and Beck____,
using a bucket sort to arrange vertices by degree and iteratively deleting vertices with degree $k$ (peeling) until all are removed.
The algorithm runs in $O(m + n)$.
Batagelj and Zaversnik (\BZ)____ provided a sequential implementation with the same time complexity. 

The \kcore{} problem is also carefully studied in the shared-memory parallel setting____, as well as included in many parallel graph libraries such as GraphX____, Powergraph____, Ligra____ and Julienne____ (later integrated into the GBBS library____).
In this paper, we compared to the three state-of-the-part solutions, including \ParK____, \PKC____, and \Julienne{}____.
Among them, \ParK and \PKC use the online peeling process, and \Julienne is offline.
More details about them were overviewed in \cref{sec:peeling}.

Given the wide applicability, \kcore is also extensively studied in other settings, such as on GPUs____, the external memory (disk) setting____, and the low-memory setting____.
The techniques proposed in these papers mostly focused on the specific challenges in each setting, and have small overlaps with the new techniques introduced in this paper.

We are also aware of variants of $k$-core decomposition.
A direct extension is to maintain \kcore with vertex and edge updates.
Research has been done on this topic, for both the dynamic (online) setting____ and the streaming (offline) setting____.
Another direction is computing approximate $k$-core decomposition, both in sequential settings____ 
and parallel settings____.
On directed graphs, s similar problem is referred to as $D$-core decomposition.
Algorithms for it have also been studied recently____.
The $k$-core decomposition also relates to many other problems, 
such as dense subgraph discovery____ and hierarchical core decomposition____.
How to apply our new techniques in these related problems can be an interesting future work.

\hide{are two state-of-the-art parallel algorithms using the online peeling process. 
They both use $O(n \maxcoreness + m)$ work. 
We introduced both of them in \cref{sec:peeling}. 
\PKC{} introduces optimizations, such as packing the remaining vertices after 98\% have been peeled, 
and using a local buffer to avoid subrounds.
While \PKC{} performs well on sparse graphs, 
both \Park{} and \PKC{} suffer from poor performance on dense graphs due to work-inefficiency and contention in updating induced degrees.

Dhulipala et al.____ proposed a parallel \kcore{} algorithm in the \Julienne{} paper. 
In their paper, they use a bucketing structure that that maps each value $d$ to all vertices with induced degree $d$.
They proved that this algorithm has $O(m+n)$ work. In their implementation, they used a simplified bucketing structure that only 
maintains up to $b$ buckets. Our analysis proves the work-efficiency of their implementation, 
and an even simpler version without bucketing structure is also work-efficient. 
% Their algorithm is base on the offline peeling algorithm, 
\Julienne{} is based on the offline peeling approach, 
as we introduced in \cref{sec:peeling},
which is fully synchronized and race-free. 
It performs well on dense graphs but has unsatisfactory performance on sparse graphs, 
where synchronization overhead can be comparable to the computation cost.



We select the state-of-the-art exact \kcore{} algorithms mentioned above as our baselines for comparison,
given their performance and more general applicability to various graph types. 
Besides, \emph{MPM} algorithm solves \kcore{} problem in distributed settings ____.
There are also studies focusing on optimizing \kcore{} decomposition in
low-memory settings____ with implementation on GraphChi____,
as well as external memory settings____.
Li et al.____ uses GraphBLAS to
adapt linear algebraic language to the \kcore{} problem on sparse graphs.

There are also recent studies based on GPU settings____,
Many of them focus on adapting the peeling framework with different optimizations on GPU settings____.
Specifically, \emph{PeelBlock}____ follows the \PKC{} design with buffer optimizations for GPUs.
\emph{SpeedCore} ____ improves the performance of \emph{PeelBlock} by further optimizations.

Several shared-memory parallel and distributed computing frameworks support $k$-core decomposition as part of graph analytics,
including GraphX____, Powergraph____, Ligra____ and Julienne____ (later integrated into the GBBS library____).
While these frameworks provide primitives for parallel or distributed graph processing, 
they do not specifically optimize for the \kcore{} problem.
}