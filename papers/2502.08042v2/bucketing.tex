\section{Hierarchical Bucketing Structure}\label{sec:bucketing}

\cref{thm:work} shows that explicitly checking the active vertices in $\alivevertices$ (\cref{line:filter_out_alive}) in each round is asymptotically optimal.
However, this simple solution can be slow in practice. 
%For instance, on graph \SD with a large maximum coreness value $\maxcoreness$ (around $10^4$), 
%this step (\cref{line:filter_out_alive}) takes 87\% running time.
To optimize the performance, some existing designs, 
such as \Julienne{}, choose to use a bucketing structure (defined in \cref{sec:prelim}) to maintain the active set $\alivevertices$. %such that we can easily extract all vertices with \induceddegree{} $k$ in each round. 
%Some existing solutions, such as \Julienne{}, use a bucketing structure (defined in \cref{sec:prelim}) to optimize the performan, 
%which maintain the active set $\alivevertices$, such that we can easily extract all vertices with \induceddegree{} $k$ in each round. 
A bucketing structure usually maintains a (partial) mapping from each value $d$ to all vertices in $\alivevertices$ with (induced) degree $d$. 
In this section, we introduce our new design for the bucketing structure.  

We proposed the \emph{hierarchical bucketing structure} (\HBS), and show
how it improves our \kcore{} algorithm. 
We note that this data structure may also be of independent interest, 
since it provides the interface a special parallel priority queue with integer keys, which is useful in many applications~\cite{li2013parallel, shi2021parallel, shi2023theoretically,dhulipala2017,gbbs2021}.

%\HBS is used to maintain the active set $\alivevertices$ in \cref{algo:framework}.
%We will show how \HBS can significantly improve the performance on this part. 
%Hence, to improve the performance, the idea of ``bucketing structure'' is proposed to avoid scanning over the active set each round.
%We will first overview the approach in Julienne/GBBS and its issue, and then proposed our solution.

\subsection{Interface and Related Work}\label{sec:bucketing-existing}

%Among our baselines, ParK and PKC use the na\"ive approach, 
%which results in poor performance on social and web graphs with large $\maxcoreness$ (see \cref{table:fulltable,fig:overall}).
%Such inefficiency is due to its memory access pattern. 
%To compute the new active set, we check all existing vertices in the current set $\alivevertices$ by an indirect access through $\degreestar[v]$ for $v\in \alivevertices$, causing the \emph{random} access.
%For a vertex $v$, we access the $\degreestar[v]$ up to $\degree(v)$ times in all rounds, which can be much slower than the other part of the algorithm with $O(n+m)$ work---the process of scanning the edge list in the $\FPeel()$ function.
%In $\FPeel()$, accessing all $v$'s neighbors only requires $\degree(v)$ \emph{serial} access to the edge list in the CSR format.

As mentioned, while refining the active set in each round does not increase the asymptotic cost, it can affect performance due to extra computations and memory accesses. 
To reduce the overhead, \Julienne{} implements a bucketing structure, 
which maintains a collection of elements (vertices), each of which has a unique identifier (vertex ID) and an integer key (induced degree). 
The structure supports the following three functions (we note that there are more functions in the interface of a bucket structure~\cite{dhulipala2017}; For simplicity, we only list the functions used in \kcore):

\begin{itemize}
    \item \textbf{\FBuildBuckets{$R$, $A$}}: initialize the bucketing structure with key range 0 to $R$, and insert each element $a\in A$ into it.
    \item \textbf{\FGetNextBucket~~$\mapsto \frontier$}: return all elements with the smallest key in the bucketing structure.
    \item \textbf{\FUpdate{$a$}}: update $a$ in the structure with its new key.
\end{itemize}

\Julienne{} maintains a bucketing structure with $b$ buckets. Every $b$ rounds, it generates all frontiers for the next $b$ rounds by $\FBuildBuckets{b,\alivevertices}$,
which extract vertices with induced degree $k+i$ to bucket $i$ ($k$ is the current peeling round). 
%whenever it recomputes $\frontier$ from $\alivevertices$.  
%Each frontier is maintained as a \emph{bucket}. The $i$-th bucket contains the current vertices in $\alivevertices$ with degree $i$. 
%In particular, when finishing round $k$ where $k$ is a multiply of 16, they refine the current $\alivevertices$ by moving all vertices with degree $k+i$ into bucket $i$, for $1\le i\le 16$.
Vertices with \induceddegree{s} more than $k+16$ remains in $\alivevertices$, which they call a \emph{overflow bucket}. 
%In their implementation, they use $b=16$.
Using an efficient algorithm, the next $b$ frontiers can be generated by one pass of memory access to $\alivevertices$, leading to better performance. 
This strategy reduces the number of accesses to $\alivevertices$ by a factor of $b$. 
A vertex $v$ will be accessed by $\FBuildBuckets{}$ until it is extracted from $\alivevertices$, which means $O(\deg(v)/b)$ times of accesses.
However, when the induced degree of $v$ is decremented, 
we also need to move $v$ to the new bucket by $\FUpdate{v}$.
%Since Julienne deploys a fully offline setting, it checks all the incident vertices in a peeling step, computes the \FHistogram (frequency of appearances) of all incident vertices, decrements their induced degrees, moves them to the new buckets, and sometimes resizes the arrays of the buckets when they overflow.
%When a vertex needs to go to another of the $b$ frontiers, Julienne moves this vertex to the array of the frontier, and reallocated a larger array if needed.
In the worst case, a vertex may be moved $b-1$ times across the buckets.
%Hence, it is important to choose the parameter $b$ carefully. A large~$b$ allows for fewer times of accessing the active set $\alivevertices$, can result in excessive movements among the frontiers. 
%On the other hand, a small~$b$ reduces need to move vertices among buckets, but also leads to frequent refinement of the active set~$\alivevertices$.
%In general, a vertex $v$ with degree $\deg(v)$ will be processed in two conditions. 
%Second, moving $v$
%Second, after $v$ is extracted from $\alivevertices$, 
%it will be put in a bucket but may move to buckets with smaller ids later. In the worst case, $v$ my be moved $O(b)$ times. 
Therefore, the total cost to process $v$ in the bucketing structure is $O(\degree(v)/b+b)$, adding up to $O(m/b+nb)$ for all vertices. 
This function has its minimum value at $b=O(\sqrt{\dbar})$, where $\dbar=\Theta(m/n)$ is the average degree. 
Our framework can be viewed as a special case where $b=1$, which only extracts the next frontier (bucket). 

%If we refer to \emph{processing} a vertex by either checking it in~$\alivevertices$ or moving it among the buckets, then an vertex $v$ is processed $O(\degree(v)/b+b)$ times. 

In practice, \Julienne{} uses $b=16$, which is a reasonable trade-off.
Indeed, in our experiments, using $b=16$ provides better performance than $b=1$ on most graphs with a large $\dbar$. 
However, we still observe some challenges with this solution. 
First, on most graphs with a small $\dbar$, using 16 buckets may cause 20--70\% overhead than using a single bucket.
Second, on graphs with very large $\dbar$, the cost of $O(\sqrt{m/n})$ per vertex may still be significant. 
Next, we propose our new design to overcome these issues. 
%However, picking a single static value is not ideal.
%A larger $b$ is preferred on graphs with large $\maxcoreness$, but it will cause significant overhead for graphs with small coreness.
%Meanwhile, the scan step for larger number of buckets is still a bottleneck for the performance (e.g., 30\% of the running time on graph \SD{}).
%Hence, it remains to be an open problem on the design of the bucketing structure that performs well on all graphs.

% \input{figures/fig-bucketing_eg.tex}
\input{figures/fig-bucketing.tex}


\subsection{Hierarchical Bucketing Structure (\HBS)}
We first propose the \emph{hierarchical bucketing structure} (\HBS) to improve the $O(\sqrt{m/n})$ cost per vertex. 
%For simplicity, in this section we use \emph{degree} to refer to the \emph{\induceddegree{}} of a vertex at a certain point of the algorithm. 
Let $d_{\max}$ be the maximum degree in the graph. 
%An \HBS consists of $O(\log d_{\max})$ buckets, each bucket storing vertices within a specific range of (induced) degrees, 
%implemented by a parallel hash bag (introduced in \cref{sec:prelim}). 
An \HBS maintains $1+\lceil\log_2 (d_{\max}+1)\rceil$ buckets. 
%, each implemented by a parallel hash bag (introduced in \cref{sec:prelim})
Each bucket stores vertices within a range of (induced) degrees of 1, 1, 2, 4, ..., growing exponentially. 
%Each bucket is implemented by a parallel hash bag (introduced in \cref{sec:prelim}). 
%each maintains vertices with degree ranges of 1, 1, 2, 4, ... , in a prefix doubling manner.
%By using \HBS, a vertex $v$ is processed $O(\log \degree(v))$ times, which is always better than $O(\degree(v)/b+b)$ times (note that $\degree(v)/b+b\ge 2\sqrt{\degree(v)}$).
\cref{fig:hier_bucketing} illustrates an \HBS and the (induced) degree range that each bucket maintains in the first ten rounds of execution.
When the induced degree of a vertex $v$ drops across a boundary, $v$ is moved to the appropriate new bucket. 
%The invariant in this design is that, a vertex can only be moved to a bucket with a smaller ID (left side in \cref{fig:hier_bucketing}).
%Consequently, a vertex~$v$ will be processed by $O(\log \degree(v))$ times in the entire algorithm.

To use an \HBS in \cref{algo:framework}, we will call \FBuildBuckets{$d_{\max}$, $V$} at the beginning of the algorithm, use \FGetNextBucket{} to generate the frontier at \cref{line:generate_frontier}, and call \FUpdate{$v$} when we decrement $v$'s induced degree. 
During the execution, we keep a counter~$\dmin$ indicating the current minimum key in a \HBS{}, which is also the current round $k$.
Finding the bucket ID of an element is based on the most significant differing bit between its key and $k$.
%Specifically, \FUpdate{$v$} provides the destination bucket\_id for a vertex $v$ with its degree $d'$ (key value) for insertion.

Next, we show how these functions are implemented efficiently on \HBS{}.
We will maintain each of the $O(\log d_{\max})$ buckets by a parallel hash bag (see \cref{sec:prelim}) that supports $O(1)$ expected cost for insertion and $O(t)$ work to extract all $t$ elements from the bag.
\FBuildBuckets{} simply inserts all elements to the corresponding bucket in parallel, based on the most significant bits of their key.
%We keep a buffered bucket (also maintained by a parallel hash bag) to hold the elements updated with key $k$, which avoids concurrent edits to the current bucket.

When \FGetNextBucket{} is called, %we first check the first bucket.
%If so, the \HBS will call \bagpack{}() to extract the keys in it.
%If it is empty, we increment the counter $k$, and check its associated list 
we find the first non-empty bucket with id $j$.
If it is one of the first two lists, we directly call \bagpack{} to return all keys in it.
Otherwise, %it will be the first non-empty one (say the $j$-th list) among the $O(\log d_{\max})$ buckets.
if $j>2$, we call \bagpack{} to get all elements in this bucket, and redistribute them to the first $j-1$ buckets, 
shown as the arrows in \cref{fig:hier_bucketing}.
%Finding the associated list in all of these operations can be implemented using some bitwise operations.

Finally, when an \FUpdate{$a$} is called, we check $a$'s new bucket ID and insert it to the corresponding bucket.  
%When the new ID is different from $a$'s previous bucket ID, we first check if $a$'s key is $k$.
%If so, we insert it into the buffer bucket; otherwise, we insert it to the bucket.
We do not delete $a$ from the original bucket since a hash bag does not support deletion.
In this case, an element may have copies in multiple buckets. Hence, when we call \bagpack{}, 
we also filter out those elements if their keys does not match the bucket ID. 
We note that this does not increase the asymptotic work of \kcore{}---since a vertex $v$ may be have at most $O(\log d(v))$ copies, the total cost here is at most $\sum \log d(v)=O(m)$. 
%Note that since a hash bag does not support deletion (see \cref{sec:prelim}), an element can have multiple copies in all buckets.
%Hence, we an \bagpack{}() is called for any bucket, we filter out those elements if their keys have been updated and should have been deleted.
%This step can be implemented using a \FFilter{} with linear work and polylogarithmic span, and the cost is asymptotically bounded by \bagpack{}.\yan{check}



In our implementation, instead of setting degree ranges for each bucket as 1, 1, 2, 4, 8, ... , 
we set the first eight buckets as single-key bucket. 
In other words, the first 8 buckets each maintain vertices with degree $\dmin,\dmin+1, \ldots, k+7$, 
and the next buckets correspond to the ranges of $[\dmin+8,\dmin+15], [\dmin+16, \dmin+31]$, and so on.
This optimization avoids frequently redistributing vertices in small buckets. 

\myparagraph{Cost Analysis.}
%The analysis of the \HBS is quite simple, relying on the fact that an element can only be reinserted a bucket with a smaller ID. 
%To analyze the cost of \HBS{}, note that an element can only be reinserted to a bucket with a smaller ID. 
%Such reinsertion can occur during \FUpdate{} or \FGetNextBucket{} (when redistributing $v$ to a new bucket). %, and in both cases an element is reinserted into a list that is in the front of the current list.
The cost of \HBS{} on a vertex $v$ includes inserting $v$ (in \FUpdate{}) or redistributing $v$ (in \FGetNextBucket{}) to a new bucket. 
In \FBuildBuckets{}, $v$ is placed in the \revise{$\lceil\log_2 (\degree(v)+1))\rceil$-th} bucket, and will only move to buckets with smaller IDs. 
During \FUpdate{} or \FGetNextBucket{}, $v$ can be packed from each bucket for redistribution at most once, and be inserted to any bucket at most once. 
Hence, the total cost to access $v$ in the bucketing structure is $O(\log \degree(v))$. 
Recall that using a fixed number of $b$ buckets leads to $O(\degree(v)/b+b)$ cost for vertex $v$, 
where $\degree(v)/b+b\ge 2\sqrt{\degree(v)}$.
Therefore, our new design provides a better solution. 
%Then, it can only be inserted in the buckets with smaller IDs, limiting the maximum number of reinsertions for $v$ to $O(\log \degree(v))$.
%The cost $O(1)$ expected for an insertion (each bucket is maintained by a parallel hash bag).
%In \FGetNextBucket{} we pack all elements in a bucket before outputting them or reinserting them.
%Since each bucket in packed only once, the cost is proportional to the current bucket size.
%Hence, we can amortize this cost to the insertions to this bucket, which is $O(1)$ per element.
%Putting both pieces together, the processing cost for vertex $v$ is $O(\log \degree(v))$ expected, which is an improvement of the na\"ive solution in \cref{algo:framework} with $O(\degree(v))$ work, when $\degree(v)=\omega(1)$.
%We show the speedup of the \HBS in practice in \cref{fig:bucketing-exp}.


\subsection{Our Final Design}

We now propose our final design of \HBS{}.
The cost to handle vertex $v$ in \HBS{} is $O(\log \degree(v))$, 
compared to $O(\sqrt{m/n})$ using a fixed number of buckets, or $O(\degree(v))$ if no bucketing structure is used.
We first note that when the average degree is constant, 
using a bucketing structure offers no benefit, 
as the overhead may introduce a large hidden constant in the complexity.
Therefore, we only use our \HBS{} when the average degree is larger than a constant $\theta$, which is set to 16 in our code.
Note that even if the average degree of the original graph is lower than $\theta$, 
as more low-degree vertices are peeled, the average degree of the graph may become larger. 
Ideally, when the average degree surpasses $\theta$, we should switch to \HBS{}. 
In our code, we simply switch to \HBS{} when a $\theta$-core is reached, 
which is guaranteed to have an average degree of at least $\theta$. 

%To implement the \HBS more efficiently, we introduce some implementation optimizations that do not change the asymptotic cost.
%The first optimization is that, instead of keeping all lists in strict prefix doubling manner (with sizes of 1, 1, 2, 4, 8, ...), we keep the first eight buckets for a single key, and then use ranges of 8, 16, 32, ..., for the next buckets.
%This will avoid frequent bucket unpacking in \FGetNextBucket{}, leading to some small performance gain.
%
%Another observation is that, our \HBS with $O(\log \degree(v))$ processing time is better the straightforward approach in \cref{algo:framework} when $\degree(v)=\omega(1)$.
%Otherwise, it will incur a small overhead due to the additional components in the \HBS.
%Hence, we set up a cut-off threshold~$\theta$, and activate the \HBS only when the average degree in the graph is larger than~$\theta$.
%We set $\theta=8$ in all of our experiments, but it can be a tunable parameter for different graphs.


\hide{
\myparagraph{Hybrid Bucketing Structure with Heuristic Threshold}
In practice, we still observed an overhead of hierarchical bucketing structure or
a fixed large number of buckets for large-diameter graphs, which have a small $\maxcoreness$ value in general.
such as road networks and some synthetic graphs.
In fact, it is theoretically most efficient to compute the $k$-core decomposition in a single bucket for large-diameter graphs under our framework
to avoid heavy movements between the buckets of the vertices,
since most of the vertices will be reduced to a smaller degree value in the end of the computation.
To reduce the effect of the two bucketing structure,
it is natural to combine the two bucketing structure in a hybrid way to hide the overhead of the hierarchical bucketing structure.
For any type of graphs, we first compute a small range in $(0, \Delta)$ of degree increment in a single bucket.
If the degree increases over $\Delta$, we switch the buckets into the hierarchical bucketing strategy without any effect to the computing process.
In experimental results shown in ~\ref{para:exp_bucketing}, there is no significant overhead for the hybrid bucketing structure for dense social networks and web networks,
but can accelerate the algorithm on road networks.
}






\hide{
We show the bitwise operation for \FUpdate{$\cdot$} function in \cref{algo:HBS_update}.
Essentially, we use the bitwise operation ``leading zero count" to find the first bit that is different between $d$ and 0, and use this bit to decide the returned key value.
Thus the function is $O(1)$ work.
For the \FGetNextBucket function, we simply add the bucket\_id by 1 if $\frontier$ is empty, and otherwise return the same bucket\_id.

\input{algo/pseudo_update.tex}

    The \HBS supports the workflow of $k$-core decomposition in an continuous manner.
    First, \textbf{BuildBuckets($b_1$, $b_2$)} initializes the buckets with the number of frontier-bucket $b_1$ and range-bucket $b_2$.
    Then, all the vertices $V$ are mapped to the buckets based on their degree values using \textbf{MakeBuckets($V, deg[\cdot]$)}.
    In each round, the vertices in the frontier are processed in the frontier buckets.
    \textbf{GetNextBucket()} returns the next available bucket with the vertices in it,
    which are peeled in the next round.
    When the frontier buckets are empty (\textbf{GetNextBucket()} returns a pair with $bucket\_id id_{next}$  of range-buckets),
    the vertices in the next available range buckets are moved forward to the buckets using \textbf{UpdateBuckets(id\_next)}.
    Once \textbf{GetNextBucket()} returns \textsc{NULL},
    \textbf{MakeBuckets($V', deg[\cdot]$)} is called again to map the remaining vertices that have not been processed yet to the hierarchical buckets for the next round.


\myparagraph{Analysis of the Hierarchical Bucketing Structure}
The work-efficiency of our algorithm does not rely on the number and structures of buckets.
However, in practical scenarios, we observe that performance varies with the number of buckets used for different graphs.
Every vertex $v \in V$ will be removed from the frontier exactly once.
For a vertex $v$ with degree $\degree(v)$, assume that $v$ has a final \induceddegree{} value of $k'$.
Thus, the total number of insertions into hashbags are not fixed for different bucketing numbers.
From the start of the insertion into the buckets, the insertions of a vertex $v$ are a series of footsteps of insertions until $v$ is finally peeled from the frontier. 
For example, assume a vertex $v_e$ is inserted into the bucket $b_3$ with range $4-7$ in the first round, and finally peeled in bucket $b_1$ with degree value $1$,
then $v_e$ will lead 3 insertions in the hashbags, i.e., $b_3$, $b_2$, and $b_1$.
Then there are two possible workload before $v$ is finally removed from the frontier for different bucketing strategies:

\begin{itemize}
    \item When using a hierarchical bucketing structure,
    vertices are inserted into the $\log n$ buckets,
    resulting in an insertion overhead work of $O(\sum_{i \in V} \log d_i)$.
    \item Using a single bucket entails batch assignment of vertices for $k_{max}$ rounds,
    The dominant factor of the work is the assignment of vertices to the buckets.
    where each vertex $v \in V$ is assigned to a bucket exactly once.
    The dominant factor in this case is the work of assignment that is $O(\sum_{i \in V} d_i)$.

\end{itemize}

The overhead of hashbag operations is balanced by the structure of the buckets.
The $k$-core decomposition algorithm in \Julienne~\cite{dhulipala2017} implements the only multiple-bucket algorithm.
However, with a fixed number of buckets specified in advance, their relative performance is suboptimal for graphs with varying structures.
To address this and enhance adaptability to different graphs,
we design a dynamic bucketing strategy that maps vertices into a single bucket or hierarchical buckets based on the number of peeling rounds, $k$.
As $k$ increases,
maintaining a fixed number of buckets causes the overhead of hashbag operations to increase.
Our implementation dynamically adjusts the bucketing structure based on the computational process,
balancing the two workload by reducing the two overhead
without manually setting a fixed number of buckets for different graphs in advance.


In Figure~\ref{fig:hier_bucketing}, we illustrate the first three bucketing mappings with $k$ increasing.

Although the hybrid bucketing structure is a trade-off between the two bucketing strategies for different types of graphs,
there is adversarial cases can be built to incur overhead.
For example, a graph with a $\maxcoreness$ of $k_m = \beta + 1$ will always force our algorithm to establish the hierarchical bucketing structure,
where $\beta$ is the threshold for the triggering of the hierarchical bucketing structure.
In this case, unnecessary overhead will be incurred for the hierarchical bucketing structure,
because the algorithm will finish in another round with a single bucket structure that is more efficient.
As we can see in experimental results in \cref{fig:overall} and \cref{table:fulltable}, graph \GLsfull is a case that can show the overhead with a threshold $\beta = 8$.
as it has a large diameter and a small maximum $\maxcoreness$ value.

Although adversarial cases like above can be built,
the hybrid bucketing structure is still a good trade-off for most graphs in practice,
and it can be easily adjusted by changing the threshold $\beta$ for the triggering of the hierarchical bucketing structure.
}


\hide{
Our proposed algorithm framework is based on the peeling process,
which removes the vertices in the frontier in parallel.
At the same time, the degrees of the neighbors of the vertices in the frontier are decremented,
which may cause the neighbors to be added to the frontier for the next round or to be processed in the next few rounds.
Thus, it is worth considering the design of the data structure to maintain the vertices to save work for the synchronization in each round.

Bucketing is a common technique used in parallel algorithms to reduce the synchronization overhead of the parallel operations.
Because the neighbors of the vertices in the frontier will be decremented to a new degree value,
which may be the coreness value or close to the coreness value of the current round,
the vertices can be arranged into buckets based on their degree values.
In this way, we can save the synchronization work for check all the remaining vertices in each round.
For example, the figure in \cref{fig:eg-bucketing} shows the bucketing structure for the vertices in the frontier.
While processing the vertices in the frontier with coreness value $k = 3$,
multiple neighbors are moved between the buckets based on their decreased degree values,
or added to the buckets from the remaining bucket that contains the vertices with coreness value not in the current range of the buckets.
The buckets are processed with $k$ increasing, and will be synchronized once the buckets are all empty.

The implementation in \GBBS utilizes a multi-bucket data structure to map vertices to buckets based on their degree,
which is a straightforward way to reduce the synchronization overhead of the batch insertion operations (scan) for the buckets for each $k$-core round.
The advantage of the multi-bucket data structure is that it can reduce the synchronization overhead of the batch
insertion operations for the buckets for each $k$-core round.
Although the number of buckets can be determined by any fixed number,
it is a trade-off between the synchronization overhead and the space usage, as well as the frontier generating step mentioned in ~\ref{sec:framework}.
In comparison, previous algorithms take a single-bucket approach to store the vertices, i.e.,
there is only the frontier containing vertices that have the same coreness value $k$ in each round.
However, some graphs such as web networks and social networks have a very large maximum coreness value,
which may lead to a dominant synchronization overhead.

\subsection{Hierarchical Bucketing Structure}

To reduce the hashbag operations while arranging vertices into the buckets,
we design a hierarchical bucketing structure to balance the performance of the hashbag operations
and the space usage for the vertices movement while maintaining the work-efficiency of the algorithm.
For a graph with $n$ vertices,
we need to assign a space of $O(n)$ for each hashbag to store the vertices.
As we described in the previous subsection,
for dense graphs with larger numbers of peeling rounds,
opening more hashbags will increase the overhead of the hashbag operations,
as well as the space usage.
In contrast, for large-diameter graphs with fewer peeling rounds,
opening fewer hashbags will reduce the overhead of the hashbag operations.
To solve the unpredictable performance influenced by the factor of the number of peeling rounds,
we propose a hierarchical bucketing structure to organize the vertices.

Instead of assigning each bucket a single coreness value,
we assign a range of coreness values to each bucket,
which is increased exponentially.
Therefore, the number of buckets to maintain all the vertices is $\log n$,
which is space-efficient,
and the overhead of hashbag operations is reduced without setting a fixed number of buckets in advance.
The vertices are assigned to the buckets of the corresponding range based on their coreness values,
which can be calculated using bitwise operations.



\subsection{Bucketing Strategy}

}



\hide{

For instance, at the beginning of the algorithm, vertices are categorized into degrees of 0, 1, 2--3, 4--7, 8--15, ... .
In the first two rounds, we have $\frontier_0$ and $\frontier_1$ from the first two sets.
After that, we regenerate $\frontier_2$ and $\frontier_3$ from the third set with degrees 2--3.
On the fourth peeling round, vertices with degree 4--7 will be split into the first three sets, and the same process repeats.
We illustrate this process in \cref{fig:hier_bucketing}.


and the $i$-th set maintains vertices

The \HBS does not maintain the vertices with the same coreness value in a single bucket. Instead, it maintains the vertices with a range of coreness values in a single bucket.
Figure \ref{fig:hier_bucketing} shows an example of the hierarchical bucketing structure concept.
The coreness range of the vertices in each bucket is increased exponentially,
i.e., the first two buckets store the vertices with single coreness values (frontier buckets),
and from the third bucket, the coreness range is increased by  a factor of 2.
In the example of Figure \ref{fig:hier_bucketing}, the first bucket stores the vertices with coreness value 0, the second bucket stores the vertices with coreness value 1,
and the third bucket stores the vertices with coreness value 2 and 3, the range of the coreness value is increased by a factor of 2 for the next buckets.
\todo{}
In practice, we set an upper bound $F$ for the coreness range of all the buckets,
and the vertices with coreness value larger than $F$ will be maintained in a container for remaining vertices.


    \item \textbf{MakeBuckets($\mathbb{I}: identifiers$, \textsf{D[$\cdot$]} $\mapsto$ \textsf{bucket\_id})}: For an identifiers $i$ (vertex) in the remaining identifiers $\mathbb{I}$
        maintained outside of the buckets, map the identifiers (vertices) $i$ to the buckets with their corresponding range based on the $bucket\_id$ $D[i]$.
    \item \textbf{Insert($\textsf{D[i]} \mapsto \textsf{bucket\_id}$)}\\
        Insert a vertex to the bucket with the corresponding bucket id based on $d[i]$.

} 