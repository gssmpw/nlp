\section{Preliminaries}\label{sec:prelim}

\myparagraph{Notations.}
Given an undirected graph $G=(V,E)$, 
the $k$-core of a graph $G$, noted as $G_k$, is the largest subgraph of $G$ where every vertex has degree at least $k$.
The \defn{coreness} of a vertex, noted as $\truecoreness(v)$, is the maximum value of $k$ such that $v$ is in $G_k$. 
The coreness of a graph, noted as $\kmax$, is the maximum coreness among all vertices. 
The \defn{\kcore{} decomposition} of $G$ identifies the sequence of subgraphs $G_0, G_1, \dots, G_{\kmax}$. 
With clear context, we use \kcore{} to refer to \kcore{} decomposition. 
Our algorithm computes the coreness for each vertex $v \in V$, 
which can be used to recover any $G_i$. 

Most \kcore{} algorithms are based on the \emph{peeling} process, 
% which repeatedly removes one or multiple vertices with the smallest degree in the graph, and update their neighbors' degrees accordingly. 
where vertices with the smallest degrees are removed, and their neighbors' degrees are updated accordingly.
We use $\degree(v)$ to represent the degree of a vertex in the original graph. 
To distinguish from the original degree, 
we use \defn{\induceddegree{}} to denote the degree of~$v$ during the peeling process, 
which may be decremented by the removal of its neighbors, and denote it as $\degreestar[v]$ for vertex $v$. 
%The \kcore{} problem is shown to be P-complete~\cite{anderson1984p}.

We say $O(f(n))$ {with high probability (\whp{}) in $n$} to indicate $O(cf(n))$ with probability at
least $1-n^{-c}$ for any $c \geq 1$.  
We say an event happens {with high probability (\whp{}) in $n$} to indicate that probability is at
least $1-n^{-c}$ for any $c \geq 1$ as a parameter.  
When clear from context we drop the ``in $n$''.


\myparagraph{Parallel Computational Model.}
%We use the work-span (or work-depth) model for fork-join parallelism
%with binary forking to analyze parallel algorithms~\cite{CLRS,blelloch2020optimal},
%which is recently used in many papers on parallel algorithms~\cite{}.
%We assume shared-memory setting with fork-join parallelism, where 
%a set of \thread{}s that share a common memory. 
\revise{We use the work-span model in the classic multithreaded model with binary fork-join~\cite{blelloch2020optimal}.
We assume a set of \thread{}s that share a common memory. }
A process can \forkins{} two child software \thread{s} to work in parallel.
When both children complete, the parent process continues.
The \defn{work} of an algorithm is the total number of instructions.
An algorithm is \defn{work-efficient} if its work is asymptotically the same as the best sequential algorithm. 
%We assume unit-cost atomic operation \cas{}$(p,v_{\mathit{old}},v_{\mathit{new}})$ (or \CAS{}),
%which atomically reads the memory location pointed to by $p$, and write value $v_{\mathit{new}}$ to it if the current value is $v_{\mathit{old}}$.
%It returns $\true{}$ if successful and $\false{}$ otherwise.
\revise{
  The \defn{span} of an algorithm is the length of the longest dependence chain among operations.
We can execute a computation with $W$ work and $S$ span with using a randomized work-stealing scheduler~\cite{BL98,arora2001thread} in time $W/P+O(S)$ \whp{} in $W$. 
}

%\myparagraph{Burdened Span.}
\revise{
  A major goal in our work is to reduce the synchronization cost in the \kcore{} algorithm. 
  Unfortunately, this cost is not well-reflected by the classic work-span model. 
  More precisely, while each fork/join operation is counted as a constant cost in theory, such a constant is usually large due to the scheduling scheme. 
  To formally analyze the cost, we borrow the concept of \defn{burdened span} from Cilkview~\cite{he2010cilkview}, a profiling tool
  for multicore programs. 
  Burdened span charge a cost of $\omega$ for a fork/join operation to reflect scheduling overhead, and a unit cost as usual for other operations. 
  We use the default parameter $\omega=15,000$ from the original Cilkview paper. 
  %Cilkview~\cite{he2010cilkview} is a tool 
  %that measures the burdened span of a parallel program
  %by counting the binary operations on the critical path of the real execution. 
  We will show that our proposed technique improves the burdened span compared to existing solutions. 
  We also use Cilkview~\cite{he2010cilkview} to measure the exact burdened span on real-world graphs in our experiments.}

We use atomic operations \atominc{}$(p)$ and \atomdec{}$(p)$, which atomically reads the memory location pointed to by $p$, and increment (or decrement) the integer value stored at $p$ by 1. The function returns the original value stored at $p$. 
We assume constant work for each atomic operation in theoretical analysis.
We note that, however, the cost for atomic operations can usually be large in practice, especially when many threads are concurrently
updating the same memory location. 
One effort in our paper is to avoid such high contention by reducing the number of atomic updates to the same memory location. 
\revise{  
  %Another goal of our algorithm is to avoid high contention. To formally analyze it, we define \defn{contention} in a fork-join computation as 
  To capture this, we define \defn{contention}~\cite{acar2017contention} experienced by an operation as the maximum number of operations that may concurrently modify the same memory location.   The contention of an algorithm is the highest contention among all operations in the algorithm. In \cref{sec:sampling}, we will show that our sampling technique effectively reduces the contention. 
}

%We use \faadec{} and \faainc{} to denote the atomic decrement and increment operations, respectively.
%The returned value of \faadec{} and \faainc{} is a pair of success flag and the value before the atomic operation.
%\myparagraph{Problem definition.}
%Given an undirected graph $G = (V, E)$, \defn{$k$-core decomposition} of $G$ is the process of categorize the vertices $V$ into \defn{$k$-cores} for all possible $k$ values.
%Therefore, the \defn{$\mathbf{k}$-core decomposition} of a graph $G$ identifies a `sequence of subgraphs $G_0, G_1, .., G_{k_{\max}-1}$, where $G_i$ is the $i$-core of $G$.

% \input{figures/fig-kcore_def.tex}
% \input{figures/fig-illustration_combine.tex}

% \input{figures/fig-hashbag.tex}
% \input{figures/fig-grid.tex}

% notation table
\begin{table}[t]
  \centering
  \small
  \begin{tabular}{|cl|}
    %\textbf{Notation} & \textbf{Description} \\
    %\hline
    \multicolumn{2}{@{}l}{Notations for the input graph:}\\
    \hline
    $G = (V, E)$ & Graph $G$ with vertex set $V$ and edge set $E$ \\
    $n, m$ & $n=|V|,m=|E|$ \\
    $\truecoreness(v)$ & The coreness of vertex $v$ \\
    $\maxcoreness$ & The maximum coreness value in the graph \\
    % $\rho$ & The peeling complexity of the graph\\
    % \youzhe{check}\\
    $\nei(v)$ & The set of neighbors of vertex $v$ \\
    $\degree(v)$ & The degree of vertex $v$ \\
    \hline
    \multicolumn{2}{@{}l}{Notations used in the algorithm:}\\
    \hline
    $\mathcal{A}$ & The set of active vertices (not peeled yet) \\
    $\mathcal{F}$ & The set of frontier vertices \\
    $\degreestar[v]$ & The induced degree of $v$ during the algorithm\\ 
    %& \youzhe{during peeling?}\\ \yihan{Cannot fit ``peeling algorithm'' in the previous line...}
    $k$ & The current peeling round\\
    \multicolumn{2}{|c|}{Notations used in sampling are defined in \cref{algo:sample_func}}\\
    \hline
    %\multicolumn{2}{@{}l}{Notations used in sampling (\cref{sec:sampling}):}\\
    %\hline
    %$\exphits$ & The expected successful  sampling times \\
    %$\sampler[v]$ & The sampler for vertex $v$, defined in \cref{algo:sample_func} \\
    %$\sampler.\rate$ & The sampling rate of the sampler \\
    %$\sampler.\mode$ & The sampling mode of the sampler \\
    %\hline
  \end{tabular}
  \caption{\bf Notation used in this paper.}
  \label{tab:notation}

\end{table}


\myparagraph{Parallel Primitives.}
The \pack{} function is a widely-used parallel primitive in this paper. 
Given an array $A$ of elements and a predicate function $f$, 
the \emph{pack} function returns all elements in $A$ that satisfy $f$ in a new array. 
This function takes $O(|A|)$ work and can be highly parallelized. 
\Julienne{} uses the \FHistogram{} function, which takes an array $A$ of keys, and count the occurrences of each key in $A$.
It can be computed efficiently using parallel semisort~\cite{gu2015top,dong2023high}.
%that satisfy a certain condition, and put them into a consecutive array.
%The \defn{pack} function is work-efficient.




\myparagraph{Parallel Hash Bag.} 
In this paper, we use a data structure \emph{parallel hash bag}~\cite{dong2021efficient,wang2023parallel}. 
A parallel hash bag supports concurrent insertion and resizing, and extracting all elements into a consecutive array. 
%However, unlike a traditional concurrent hash table, the work of extracting all elements is proportional to 
%the number of insertions rather than the hash table size.
More precisely, a parallel hash bag supports two operations:
\begin{itemize}[leftmargin=*,topsep=0pt, partopsep=0pt,itemsep=0pt,parsep=0pt]
  \item \bagput{}($v$): (concurrently) add the element $v$ into the bag.  
  %A resizing may be triggered if more than a constant factor of the slots is non-empty in the current table. 
  \item \bagpack{}(): extract all elements in the bag into an array. %and remove them from the bag. 
\end{itemize}

Hashing is a widely-used technique for accelerating parallel data access~\cite{shun2014phase, Alcantara2009, dong2024parallel, ding2023efficient}.
Similar to a hash table~\cite{shun2014phase}, a hash bag maintains a set of elements by hashing
every element into certain index, and resolve conflicts by linear probing. 
A hash bag is initialized as an array with $O(n)$ slots, 
where $n$ is the maximum possible number of elements appearing in the bag at the same time. 
The array is conceptually divided into chunks of sizes $\lambda, 2\lambda, 4\lambda, ...$ ($\lambda=2^8$ in implementation).
At the beginning, elements are inserted into the chunk of size $\lambda$.
%Once a certain number of insertions are made (decided by a desired load factor $\alpha$), 
Once the current chunk reaches a desired load factor, 
we move onto the next chunk of size $2\lambda$, and so on so forth. 
%By maintaining the exponentially growing sizes structures, 
The \bagpack{}() function only needs to consider the prefix chunks that have been used instead of the entire array.
Therefore, the complexity of \bagpack{}() is $O(\lambda+t)$, where $t$ is the number of elements in the hash bag.
In this paper, we use parallel hash bag to maintain the frontiers and some vertex sets in our algorithm. 
%However, since multiple insertions can happen concurrently, 
%maintaining the exact count of the insertions in each chunk is challenging.
%To overcome it, it estimates the number of insertions by sampling.
%Specifically, each chunk maintains a global counter.
%When an insertion is made, it first flips a coin at some sample rate.
%If it returns true, the corresponding global counter is added by one using \faainc{}.
%We can deduce the approximate number of slots taken in the current chunk using the global counter.

\myparagraph{Bucketing Structure for \kcore{} Algorithms.} 
%The bucketing structure serves as an efficient data structure implementation for \emph{integer priority queue}. 
The bucketing structure organizes data with integer keys and supports efficient updates and finding the element with the smallest key. 
%Since keys are integers, a parallel bucketing structure~\cite{dhulipala2017} supports concurrent updates and extract all elements with the smallest integer key. 
A parallel bucketing structure has been proposed by~\cite{dhulipala2017} to support the \kcore{} algorithm in \Julienne{}.
The design of the bucketing structure has been widely studied \cite{li2013parallel, shi2021parallel, shi2023theoretically,dhulipala2017,gbbs2021} and has many applications. 
%Typically, this implementation consists of a list of buckets. 
In \cref{sec:bucketing}, we present more details about the existing bucketing structure and our new design, which not only enhances the performance of the \kcore algorithm but is also of independent interest.

\hide{
\Julienne{} proposed a bucketing structure for its \kcore{} algorithm.
A bucketing structure is a sequence of \emph{buckets} where the bucket $B_i$ is a set of all vertices with current \induceddegree{} $i$. 
In this paper, we propose a more efficient bucketing structure using parallel hash bag to maintain the buckets, 
with a hierarchical structure to reduce the synchronization overhead,
which will be introduced in section~\cref{sec:bucketing}.
}
%The bucketing structure is a multi-level structure that maps vertices to buckets based on their degree.
%With the concurrent hash bag, we can efficiently maintain the vertices in the frontier and update the frontier in parallel,
%which will be introduced in section~\cref{sec:bucketing}.

%However, since multiple insertions can happen concurrently, 
%maintaining the exact count of the insertions in each chunk is challenging.
%To overcome it, it estimates the number of insertions by sampling.
%Specifically, each chunk maintains a global counter.
%When an insertion is made, it first flips a coin at some sample rate.
%If it returns true, the corresponding global counter is added by one using \faainc{}.
%We can deduce the approximate number of slots taken in the current chunk using the global counter.
%
%In this paper, we propose a bucketing structure using parallel hash bag to maintain the vertices in the frontier.
%The bucketing structure is a multi-level structure that maps vertices to buckets based on their degree.
%With the concurrent hash bag, we can efficiently maintain the vertices in the frontier and update the frontier in parallel,
%which will be introduced in section~\cref{sec:bucketing}.

