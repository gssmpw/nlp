\subsection{Parallel Peeling Process in Existing Work}\label{sec:peeling}
Under the proposed framework, multiple strategies for solving the $\FPeel$ function on~\cref{line:process_bucket} in parallel can be applied. 
We first review the two most relevant strategies in the literature. 

\myparagraph{The Offline Strategy in \Julienne{}.} 
\Julienne{}~\cite{dhulipala2017} employs a batch-synchronous strategy for the $\FPeel{}$ function (\cref{algo:peel_offline}). 
In each \subround, it gathers all vertices that require degree changes in a list $L$, 
which concatenates the neighbor lists for all vertices in $\frontier$. 
Each appearance of a vertex $u$ in $L$ means to decrement the \induceddegree{} $\degreestar[u]$ by 1. 
Hence, a \mf{Histogram} algorithm is used to count the number of appearances for each vertex in the list,
which can be performed by a parallel semisort~\cite{gu2015top,dong2023high} with $O(n)$ work with high probability. 
%Then a semisort algorithm is used to compute the sum of the decremented degrees for each neighbor, which sorts the keys in $O(n)$ work with high probability~\cite{gu2015top}.
%Based on these keys, the coreness values of the neighbors are updated accordingly. 
The \induceddegree{} for each vertex in $L$ will be decremented in a batch accordingly, and the next frontier can be computed as
all vertices that have degree drop to $k$ or lower by a parallel pack. 
We refer to this approach as an \defn{offline} approach. 
Since each vertex and edge is processed exactly once in the peeling process, 
the total work for this step is proportional to the total neighborhood size of the vertices in the frontier.
Based on \cref{thm:work}, the \kcore implementation in \Julienne{} has $O(n+m)$ work.

\input{algo/pseudo_GBBS.tex}

%An additional optimization in \Julienne{} is to maintain multiple instead of just one frontier.
%We review this part in \cref{sec:bucketing-existing}.
The actual implementation of \Julienne{} is more complicated than our framework. 
Our new algorithm with offline peeling is much simpler than both their theoretical algorithm in the paper and their implementation. 
Our result indicates that achieving work-efficiency does not require the complicated bucketing structure. 
We note that adding a bucketing structure can likely improve the performance in practice, which we discuss in \cref{sec:bucketing-existing}. 
%For example, in every $b$ rounds, \Julienne{} constructs the frontier for the next $b$ rounds by a bucketing structure. 
%We review this part in \cref{sec:bucketing-existing}.
%Here our result indicates that achieving work-efficiency does not require the complicated bucketing structure. 


While \Julienne{} achieves efficient work, the parallelism of \Julienne{} can be bottlenecked the scheduling overhead caused by the offline algorithm.
Note that each \subround{} requires to distribute work to all threads and synchronize them at the end. 
On sparse graphs, this scheduling overhead can dominate the actual computation cost, 
making \Julienne{} slower than a sequential algorithm in certain cases. 
In \cref{sec:vgc} we discuss more details and provide our solution to overcome this challenge. 
%In our experiments, there can be tens of thousands of subrounds on many real-world graphs, causing a large scheduling overhead to synchronize threads between rounds.
%insufficient number of vertices processed simultaneously, thus limited parallelism.
%The large number of subrounds leads to a large scheduling overhead to synchronize threads between rounds.
%As shown in \cref{fig:localsearch}, a $\sqrt{n}\times \sqrt{n}$ grid has all vertices with coreness values of 2. 
%On this graph, $\sqrt{n}$ subrounds are needed, making \Julienne{} slower than the sequential algorithm.
%This motivate us to consider \defn{online} approach that can avoid such subrounds.
%In this case, the large scheduling overhead makes the algorithm slower than a sequential implementation in our experiments (see \cref{xx}).
%For many other graphs that requires many subrounds, \Julienne{} also shows worse performance than other implementations. 

\myparagraph{The Online Strategy in \ParK and \PKC.} 
Both \Park{}~\cite{dasari2014park} and \pkc{}~\cite{kabir2017parallel} use an asynchronous peeling algorithm (\cref{algo:peel_online}), which
removes vertices in the frontier in parallel, 
and directly decrements the \induceddegree{s} of their neighbors using the \atomdec{} operation. 
We refer to this approach as an \defn{online} approach, 
since when we process a vertex $v$ in the frontier, the \induceddegree{s} $\degreestar[\cdot]$ of its neighbors are updated immediately. 
%When decrementing $\degreestar$ of a vertex $u$, if the new $\degreestar{}$ value hits $k$, 
When $\degreestar[u]$ is decremented from $k+1$ to $k$, 
$u$ will be put in the next frontier by atomically appending it to an array $\nextfrontier$. 
Assuming constant work for each \atomdec{} operation and appending each element to $\nextfrontier$, 
each peeling \subround has cost proportional to the total neighborhood size of the vertices in the frontier. 

Unfortunately, neither of the two algorithms maintains the \alive{} set during the algorithm, 
and simply use the original vertex set $V$ to generate each frontier. Therefore, the two algorithms both have $O(m+\maxcoreness n)$ work. 
However, introducing the \alive{} set in the algorithm will directly lead to work-efficiency. 


The computation in the online approach is simpler than the offline approach, and does not require strong synchronization between subrounds.   
Certain optimizations can be used to alleviate the scheduling overhead (e.g., \PKC{}~\cite{kabir2017parallel} and our new solution in \cref{sec:vgc}). 
The major performance bottleneck of \park{} and \pkc{} comes from the potential contention caused by the atomic operations. 
For a high-degree vertex, many of its neighbors can decrement its \induceddegree{} concurrently. 
As we can observe from the results in~\cref{table:fulltable}, \park{} and \pkc{} both suffer from poor performance on power-law graphs such as social and web graphs. 
Even a small fraction of high-degree vertices may result in high contention that degrade the performance. 
In addition, the data structure $\nextfrontier$ to generate the next frontier may also suffer from contention when many vertices are appended concurrently. 
%The same volume of atomic operations causes additional heavy contention for generating the next frontier $B$, which harms parallelism.
In \cref{sec:sampling}, we propose our solution to overcome this challenge.

\input{algo/pseudo_ParK.tex}

\revise{
    \myparagraph{Span Analysis.} 
    To analyze the span of the algorithm, \Julienne{}~\cite{dhulipala2017} defined a parameter $\rho$ to represent the number of peeling subrounds, 
    referred to as the \defn{peeling complexity}, and showed that the span of their algorithm is $\tilde{O}(\rho)$ \whp{}.
    For the framework in \cref{algo:framework} with both online and offline peeling algorithms, 
    the same $\tilde{O}(\rho)$ span bound holds (\whp{} for the offline version) following the same analysis in \Julienne{}.         
    However, note that each of the $\rho$ subrounds requires a parallel-for loop, indicating a global synchronization among threads. Therefore, the \emph{burdened span} (defined in \cref{sec:prelim}) is $\tilde{O}(\rho\omega)$. 
    Given $\omega$ as a large constant, when $\rho$ is also large, the parallelism in the algorithm can be limited. 
    In \cref{sec:vgc}, we discuss our new techniques that improve the burdened span, thus parallelism. 
}
%\youzhe{define high probability earlier here?} 


































%\myparagraph{Work-efficient peeling Strategies.}
%Under the proposed framework, multiple strategies for solving the $\FPeel$ function at ~\cref{line:process_bucket} in parallel can be applied.
%\ParK~\cite{dasari2014park} and \PKC~\cite{kabir2017parallel} directly use asynchronous parallelism to solve the frontier processing.
%While removing the vertices in the frontier in parallel, 
%the two algorithms directly decrease the degrees of the neighbors of the vertices in the frontier using \faadec{} operations,
%which is a unit-cost operation in the work-span model.
%The implementations keep another array to collect the vertices 
%that are decremented to the coreness value of the current frontier.
%The vertices are also arranged using atomic operations \faainc.
%\hide{
%    \PKC improves \ParK by introducing a local buffer to reduce the synchronization overhead.
%    Instead of using another array to store the vertices that are decremented to the coreness value of the current frontier,
%    \PKC uses a buffer with size $n/p$ for each thread to store the vertices.
%    In this way, those vertices that are decremented to the coreness value of the current frontier are stored in the buffer,
%    as long as the buffer is not full.
%    The buffer does not improve the work-efficiency of the algorithm.
%    The work of the two implementations can be affected by the model definition of the atomic operations.
%    If the atomic operations are not unit-cost, the total work of the peeling and updating cost will be $O(m + n)$.
%}
%
%\input{algo/pseudo_ParK.tex}
%
%The proposed algorithm in \GBBS employs a batch-synchronize strategy for frontier processing.
%Initially, it gathers all neighbors with degree changes after removing the vertices in the frontier. 
%Then a semisort algorithm is used to compute the sum of the decremented degrees for each neighbor, which sorts the keys in $O(n)$ work with high probability~\cite{gu2015top}.
%Based on these keys, the coreness values of the neighbors are updated accordingly. 
%Since each edge is processed exactly once during the frontier processing step, the total work for this step is $O(m)$.
%In practice, the semisort algorithm is slow because of too much data movement,
%and they use a parallel block-based histogram implementation to avoid shuffle,
%which is a $groupby$ operation in the database system.
%
%\input{algo/pseudo_GBBS.tex}
%
%However, although the two approaches are work-efficient if embedded in our framework,
%the practical parallelism of the two algorithms is poor for different types of graphs.
%Specifically, the atomic operations in \ParK and \PKC may cause high-volume contention in the frontier processing step,
%which leads to a huge overhead of the peeling process on dense graphs.
%The synchronous implementation in \GBBS may can not be optimized for those graphs have a very large maximum coreness value.
%% \GBBS uses a fixed number of buckets to store the vertices in the frontier,
%% which causes a large number of frontier generating rounds as shown in ~\cref{line:generate_frontier} in ~\cref{algo:framework}.
%Specifically, \GBBS utilizes batch-update algorithm to generate and peel the frontier in each round,
%which may cause a large number of frontier generating rounds that can not be reduced because it is not an online processing workflow.
%Meanwhile, the structures and degree distribution of the graphs are various,
%which may cause the parallelism of the algorithm worse in the \FPeel step.
%
%
%
%To solve the problems of the two types of algorithms mentioned above, 
%we propose multiple design strategies to improve the performance of the algorithm framework.
%Because online processing workflow provides more flexibility for optimizing the parallelism of the algorithm,
%we design the algorithm to process the frontier in an online manner.
%
%\youzhe{mention the hashbag here? otherwise no difference from ParK}
%\youzhe{how to mention or begin our design here?}
%When we remove the vertices in the frontier,
%atomic operations \faadec{} are used to decrease the degrees of the neighbors.
%Instead of using another array to store the vertices that are decremented to the coreness value of the current frontier,
%we directly insert the neighbors into the hashbag.
%The decremented degree of the neighbors is bounded by the coreness value $k$ of the current round.
%Whenever the degree of a neighbor is decremented to $k$,
%it is inserted into the hashbag directly.
%Then the hashbag will maintain all the vertices that should be processed in the next round.
%Because the insertion operations into the hashbag are work-efficient, 
%the total work of the framework is work-efficient based on the proof above.
%Having the theoretically work-efficient frontier processing step,
%we further design several strategies to improve the performance of the framework,
%both in theoretical analysis and in practice.
%In following sections, we will introduce the design of the algorithm in detail. 


\hide{
implementation, an additional optimization is to extract the frontier and update the \alive{} set every 16 rounds. 
More precisely, every 16 rounds, the algorithm processes the \alive{} set, and extract the vertices with $\coreness$ in range $[k,k+16)$ to 16 buckets. 
This approach is a constant optimization to reduce the cost of processing the frontier and \alive{} set on \cref{line:extract,line:pack}. 
In \cref{sec:bucketing} we will introduce our proposed solution to improve this approach. 

Computing the histogram of all neighbors of the frontier also requires a few rounds of global synchronization. 
While treated as a constant in theoretical analysis, in practice this may cause large scheduling overhead, 
which can be $10^2$ to $10^4$ CPU cycles~\cite{cpuops}. \yihan{may need more careful justification}. 
Therefore, when the number of subrounds is large, the scheduling overhead may overweigh the benefit of parallelism. 

}


