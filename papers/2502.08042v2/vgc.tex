\subsection{Vertical Granularity Control}\label{sec:vgc}

\input{figures/fig-local_search.tex}

Our second approach tackles the significant scheduling costs that occur in sparse graphs. 
In the framework given in \cref{algo:framework}, each \subround{} must wait for the completion of the previous one. 
Between these subrounds, the scheduler distributes tasks to all threads and synchronizes them at the end, which can incur substantial scheduling overhead. 
This overhead is particularly pronounced in sparse graphs, where vertices typically have low degrees. 
The computation for processing a low-degree vertex $v$ is small, meaning that the overhead associated with creating and synchronizing the thread for $v$ can outweigh the computational costs. 
As a result, this overhead diminishes the benefits of parallelism.

Our approach to overcome this challenge is inspired by a recent technique~\cite{wang2023parallel,dong2024pasgal} known as \emph{vertical granularity control (VGC)}. 
Similar to traditional granularity control in parallel programming, VGC focuses on increasing the size of base case tasks in graph processing to mitigate scheduling overhead. 
In our \kcore{} algorithm, we aim to reduce the overall number of subrounds and eliminate small parallel tasks. 
To achieve this, we incorporate VGC into our online peeling process in \cref{algo:peel_online}.
When peeling a low-degree vertex $v$, we place all its active neighbors in a FIFO queue, referred to as the \emph{local queue} of~$v$, 
and process all vertices in the local queue sequentially. 
When we decrementing the \induceddegree{} of a neighbor $u$,  
if $\degreestar[u]$ drops to $k$ (\cref{line:update_frontier} in \cref{algo:peel_online}),
instead of adding $u$ to $\nextfrontier$, we add $u$ to the local queue. 
This allows $u$ to be processed in the same subround as $v$, rather than waiting for the next subround.
We refer to this process as a \emph{local search} at $v$. 

%We restrict the local queue size to $Q$ for each vertex. 
\revise{There are multiple ways to control the granularity in VGC, 
such as controlling the local queue size, 
or the number of touched vertices, to be a fixed number. 
In our code, we simply fix the local queue size as $128$. 
Once the local queue is full, even if $\degreestar[u]$ drops to $k$, 
we still add $u$ to the next frontier as normal. }
An illustration of VGC on a grid is given in \cref{fig:localsearch}. 
\revise{Limiting the work of each local search caps the maximum work each thread receives, 
thereby ensuring better load balancing. }
Empirically, processing hundreds of vertices is sufficient to hide scheduling overhead~\cite{wang2023parallel}. 
In our experiments, performance remains relatively stable across queue sizes ranging from hundreds to thousands.
%In our code, we use $Q=128$. 

VGC does not change the work-efficiency of the algorithm, as each vertex is still processed exactly once, 
either from the local queue or from the frontier. 
In practice, VGC improves the performance for sparse graphs by 1.72--31.2 times. 

Some previous work also tried to reduce the number of subrounds. 
In \PKC{}~\cite{kabir2017parallel}, each processor keeps a subset of $\frontier$ in a local buffer, and performs a sequential peeling process until the buffer is empty, giving exactly one \subround per round. 
However, it may cause severe load imbalance---if one vertex triggers a chain reaction in peeling, 
the corresponding processor may perform significantly more work than others. 
In contrast, our algorithm parallelizes all vertices and use a work-stealing scheduler to enable dynamic load-balancing, 
and use VGC on top of it to hide scheduling overhead. 
%Further, VGC avoids small parallel tasks to hide scheduling overhead. 
%Combining the dual benefit, 
In summary, VGC improves performance by both hiding synchronization cost between subrounds and achieving good load balancing. 


%is not only aimed at reducing the number of peeling rounds,
%but also at balancing the work load of each thread in the parallel peeling operations,
%which is essential for the optimization of the practical performance of parallel algorithms.


%Note that within a subround, the degree of a vertex may drop below $k$ by multiple neighbors in the frontier.
%We always require the neighbor who decrements $u$'s degree to exactly $k$ (which can be determined based on the return value of \atomdec{}) to further process $u$. 
%While peeling the vertices in the frontier, 
%The local search is based on the approach of vertical granularity control, proposed by Wang et al.~\cite{wang2023parallel} for the strongly connected components problem.

%In Figure~\ref{fig:localsearch},
%we illustrate a simple example of the local search process. 
%All vertices, including $v_1, v_2, u_1, u_2, u_3,$ and $u_4$, 
%have the same \induceddegree{} value of $2$. 
%In the first peeling round, $v_1$ and $v_2$ are added to the frontier and removed. 
%Consequently, the degrees of $u_1$ and $u_4$ are decremented to $2$, 
%and they are added to the local queues of $v_1$ and $v_2$, respectively. 
%These vertices are then removed from the queue, causing their neighbors $u_2$ and $u_3$ to be dragged into the queue of $v_1$. 
%This process reduces the number of peeling rounds from $3$ to a single round.
%Essentially, the local search technique does not change the work-efficiency of the algorithm,
%and it can highly align the work load for each thread in the parallel peeling rounds to maximize the parallelism of our proposed framework.



%We show the performance of the vertical local search technique in Figure~\ref{fig:queue-rho}.


%With a local search queue of fixed size $Q$ for each vertex in the frontier,
%the local search technique can also contribute to the load balancing of the peeling operations,
%which is essentially critical for the practical performance of parallel computing.


%Processing the vertices in the frontier involves atomic operations and hashbag insertions,
%which constitute the main computational work of our plain peeling algorithm.
%The proposed algorithm framework is based on the peeling process,
%and the efficiency in each peeling round is critical for the overall performance of the algorithm.
%Since the structure of the graphs is unpredictable,
%the load balancing of the work each thread needs to perform is essential for the parallelism.
%Recall the example of the grid graph in Figure~\ref{fig:localsearch},
%many graphs may have a large diameter similar to the grid graph,
%and the degree distribution may be skewed, such as road networks.
%Thus, applying naive parallel peeling operations may cause idle threads and load imbalance.
%To maximize the utilization of all the computing resource, 
%we propose a vertical granularity control technique to reduce the number of peeling rounds.
%
%While peeling the vertices in the frontier, 
%the degrees of the neighbors have a probability to be decremented to the coreness value of the current frontier responsible for,
%thus the remaining peeling rounds may be saved by processing these neighbors in the same round.
%To mitigate contention in each frontier-processing peeling round,
%and to reduce the number of insertions into the hashbag,
%we propose a local search technique designed to decrease the number of peeling rounds, 
%denoted as $\numpeeling$.
%The local search is based on the approach of vertical granularity control, proposed by Wang et al.~\cite{wang2023parallel} for the strongly connected components problem.
%The local search technique does not change the work-efficiency of the algorithm,


%Processing the vertices in the frontier involves atomic operations,
%which constitute the main computational work of our plain peeling algorithm.
%The proposed algorithm framework is based on the peeling process,
%and the efficiency in each peeling round is critical for the overall performance of the algorithm.
%Since the structure of the graphs is unpredictable,
%the load balancing of the work each thread needs to perform is essential for the parallelism.
%Recall the example of the grid graph in Figure~\ref{fig:grid_eg},
%many graphs may have a large diameter similar to the grid graph.
%Thus, applying naive parallel peeling operations may cause idle threads and load imbalance.
%To maximize the utilization of all the computing resource, 
%we propose a vertical granularity control technique to reduce the number of peeling rounds.
%
%While peeling the vertices in the frontier, 
%the degrees of the neighbors have a probability to be decremented to the coreness value of the current frontier responsible for,
%thus the remaining peeling rounds may be saved by processing these neighbors in the same round.
%To mitigate contention in each frontier-processing peeling round,
%and to reduce the number of insertions into the hashbag,
%we propose a local search technique designed to decrease the number of peeling rounds, 
%denoted as $\numpeeling$.
%The local search is based on the approach of vertical granularity control, proposed by Wang et al.~\cite{wang2023parallel} for the strongly connected components problem.
%The local search technique does not change the work-efficiency of the algorithm,
%because the vertices are removed from the frontier exactly once, 
%either by the local search in each coreness round or by the peeling operation.
%With a local search queue of fixed size $Q$ for each vertex in the frontier,
%the local search technique can also contribute to the load balancing of the peeling operations,
%which is essentially critical for the practical performance of parallel computing.
%
%In peeling round $k$, 
%a FIFO queue $q_v$ with fixed size $Q$ for each vertex $v$ in the frontier is established.
%If the degree of a neighbor $u$ of $v$ is equal or less than $k$ after decrementing,
%$u$ is added to $q_v$ instead of being inserted atomicly into the hashbag for the next round.
%Then the neighbors of $u$ are decremented in the same computational thread recursively until the local queue is full.
%\input{figures/fig-local_search.tex}
%
%In Figure~\ref{fig:localsearch},
%we illustrate a simple example of the local search process. 
%All vertices, including $v_1, v_2, u_1, u_2, u_3,$ and $u_4$, 
%have the same \induceddegree{} value of $2$. 
%In the first peeling round, $v_1$ and $v_2$ are added to the frontier and removed. 
%Consequently, the degrees of $u_1$ and $u_4$ are decremented to $2$, 
%and they are added to the local queues of $v_1$ and $v_2$, respectively. 
%These vertices are then removed from the queue, causing their neighbors $u_2$ and $u_3$ to be dragged into the queue of $v_1$. 
%This process reduces the number of peeling rounds from $3$ to a single round.
%Essentially, the local search technique does not change the work-efficiency of the algorithm,
%and it can highly align the work load for each thread in the parallel peeling rounds to maximize the parallelism of our proposed framework.
%
%
%The \PKC algorithm employs a buffer with size $n/p$ for each thread to store the vertices that are decremented to the \induceddegree{} value of the current frontier.
%The implementation is only effective for reducing the number of total round of processing the vertices in the buffers.
%At the same time, 
%the buffers are statically assigned of fixed sizes and processing order for each thread,
%which causes the load imbalance of each thread while applying peeling operations.
%In contrast,
%our VGC technique is not only aimed at reducing the number of peeling rounds,
%but also at balancing the work load of each thread in the parallel peeling operations,
%which is essential for the optimization of the practical performance of parallel algorithms.
%
%We show the performance of the vertical local search technique in Figure~\ref{fig:queue-rho}.


%When a subround starts, $|\frontier|$ software threads will be created, each working on a vertex in $\frontier$.
%When a subround finishes, all threads will be synchronized. 
%In this case, the , 
%As mentioned in \cref{sec:xx}, 
%an adversarial example is a 2D grid, where all vertices has a coreness 2, 
%but requires $O(\sqrt{n})$ subrounds. 
%As a result, most existing systems exhibit unsatisfactory performance on this graph.

\hide{


Our second approach addresses the significant scheduling costs associated with a high number of subrounds.
In the framework in \cref{algo:framework}, 
each \subround must wait for the previous \subround to fully finish. 
Between every two subrounds, the scheduler will distribute work to all threads and synchronize them at the end, 
which can incur high scheduling overhead. 
This overhead may be more pronounced on sparse graphs where vertices have small degrees. 
This is because the computation to process a low-degree vertex $v$ is small.
Hence, the overhead to create and synchronize the thread for $v$ may dominate the cost,
thereby weakening the benefits of parallelism. 

}


\revise{
    %\subsection{Theoretical Improvements of the Two Techniques}
    \myparagraph{Burdened Span Analysis.} %To analyze the theoretical improvements of VGC on span,
    %we introduce another parameter $\omega$ to denote the cost of each fork/join operation in the parallel algorithm,
    %following the definition of burdened span~\cite{he2010cilkview}.
    To analyze the improvement of VGC, we use the \emph{burdened span} defined in \cref{sec:prelim}, which charges a factor of $\omega$ to each fork/join operation in the span to reflect scheduling overheads. 
    Recall that the burdened span of the framework in \cref{algo:framework} (e.g., the offline algorithm in \Julienne{}) is $\tilde{O}(\omega \rho)$,
    where $\rho$ is the number of subrounds in the algorithm. 
    Let $\rho'$ be number of subrounds in our algorithm with VGC, and $L$ the total work performed by each local search.
    In this case, our algorithm with VGC has a span of $\tilde{O}(\rho' (\omega + L))$. 
    As mentioned, we can control $L$ (the granularity of VGC) in various ways, and based on the theory, the primary goal
    of VGC is to control $L$ asymptotically lower than $\omega$, where $\omega$ is on the order of magnitude of $10^4$~\cite{he2010cilkview}. 
    Since $\rho' \leq \rho$, the burdened span of our algorithm with VGC is always no worse than the version without VGC. 
    In most of the cases, $\rho' < \rho$, and thus the burdened span can be improved by VGC. 
    In \cref{fig:queue-rho}, we empirically showed that the number of subrounds after VGC (i.e., $\rho'$) can be 5--40 times smaller than $\rho$ on the tested graphs. 
    This justifies why our solution with VGC can achieve better parallelism than \julienne{}. 
}
