\section{Additional Related Work}
\label{sec:relatedwork}

\textbf{Scaling Inference Computation} has explored diverse strategies for enhancing LLM capabilities through adaptive test-time compute allocation~\cite{snell2024scaling, brown2024large, manvi2024adaptive, guan2025rstarmathsmallllmsmaster, chen2024think}.
Typically, LLM inference involves decomposing complex questions into sequential intermediate steps that lead to the final
answer, exemplified by chain-of-thought (CoT) prompting~\cite{wei2022chain, sprague2024cotcot, wang2024chainofthoughtr} and its variants~\cite{kojima2022large,zhouleast, wangself, li2023making}.
However, with the increasing number of steps in a single chain, these methods often suffer from error propagation and struggle with complex computations~\cite{chenprogram}.
To overcome the limitation, CoT \citep{li2024chainthought} has been improved with search methods \citep{zhang2024chain}, such as beam search~\cite{xie2024self} and Best-of-N~\cite{snell2024scaling}, which leverage reward models to select the most promising candidates.
Later, tree search-based algorithms, such as MCTS and A*~\cite{yao2024tree, luo2024improve, zhang2024restmcts, hao2023reasoning,zhou2024language,choi2023kcts, yao2024mulberry, chen2024treesearchusefulllm, xie2024montecarlotreesearch, zhang2025lessonsdevelopingprocessreward} produce diversity paths in inference computation, allowing for exploration at different levels.
All of these methods show that utilizing inference time techniques for extended search computation leads to performance gains in a variety of tasks.
While improved performance through scaling inference comes with increased computational costs~\cite{anonymous2025inference}, the diversified sampling remains underexplored. 
This paper systematically analyzes this relationship and empirically demonstrates that sampling from diversified natural language prompts can mitigate performance shortfalls in reasoning, math, and code generation tasks.


\textbf{Prompting for Self-improvement}, beginning with STaR~\cite{zelikman2022star}, relies on solutions generated by the LLM to augment data in fine-tuning processes. For instance, reinforced self-training methods~\cite{gulcehre2023reinforced,hosseini2024v,singhbeyond,aksitov2023rest} introduce mechanisms to curate new high-quality examples, mainly by sampled CoT solutions, and then iteratively enrich the training dataset for enhancing LLM capabilities.
However, these methods typically rely on either labeled preference data~\cite{gulcehre2023reinforced} or a reward model~\cite{guan2024search, zelikman2022star,hosseini2024v}.
In contrast, recent work like self-correction~\cite{kumar2024training, zelikman2024, hosseini2024vstar, xi2024enhancing} and self-rewarding~\cite{yuan2024selfrewarding, chen2024selfplay, huang2024large} use LLM themselves to evaluate the generated solutions.
In domains with fine-grained sampled CoT step, existing advances training a model-based verifier~\cite{cobbe2021training, lightman2023let, wang2024math, min2024imitate} or using tree search to collect preferences~\cite{xie2024monte} decide on a final answer that can improve performance on reasoning tasks relative to taking a single sample.
Nevertheless, the strategies still require initial human annotation for fine-tuning, e.g., seed data~\cite{chen2024selfplay,lee2024llm2llm}.
Our self-improvement differs from previous methods since DivSampling does not necessary require any ground truth as ORM, where performance can be improved only by injecting a diverse set of perturbations. Other strategies, such as LLM-as-a-judger, majority voting can also be adopted.

