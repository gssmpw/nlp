\section*{\centering \Large{Appendix: \modelname}}
\input{sections/appendix/appendix_theory}


\section{Details of Metrics}\label{metrics}
 
For each of our metrics, the solver is allowed $k$ submissions for each, denoted by $[\vs]_k \sim \text{LLM} (\cdot | \vr, k)$ given input $\vr$. We consider testing the model on a set of tasks consisting of prompts and questions $\tX = \{ \vr = [\vp, \vq] \}$. 
\textbf{EM@k Rate}.
For reasoning and math tasks, if at least one submission $s'\in [\vs]_k$ matches the ground truth, the task is considered solved. The EM@k rate is defined as the proportion of tasks solved as
$$
\textbf{EM@k} = \frac{1}{|\mathcal{X}|} \sum_{\vr \in \tX} \mathbbm{1} \big(\exists \vs \in [\vs]_k, \text{s.t.}, \vs=\mH \big| [\vs]_k \sim \text{LLM} (\cdot | \vr, k) \big),
$$
where $\mathbbm{1}(\cdot)$ is the indicator function and $\mH$ is the ground truth. 

\textbf{Pass@k Rate}.
For code generation tasks, if at least one submission $s'\in [\vs]_k$ passes all hidden tests $\mH_c$, the task is considered solved. The Pass@k rate is defined as
$$
\textbf{Pass@k} = \frac{1}{|\mathcal{X}|} \sum_{\vr \in \tX} \mathbbm{1} \big( \exists \vs' \in [\vs]_k, \text{s.t.}, \vs' \text{ passes all } \mH_c \big| [\vs]_k \sim \text{LLM} (\cdot | \vr, k) \big). 
$$

\textbf{TF-IDF Similarity} measures the importance of terms in a document relative to a collection of documents, which computes the average cosine similarity between TF-IDF representations of solution pairs: 
$$
\textbf{tf-idf sim.} = \frac{1}{|\mathcal{X}|} \sum_{\vx \in \mathcal{X}} \frac{1}{k\left(k-1\right)} \sum_{\substack{{\vs}, {\vs}^{\prime} \in [\vs]_k \\ {\vs} \neq {\vs}^{\prime}}} \frac{\operatorname{tf-idf}({\vs}) \cdot \operatorname{tf-idf}\left(\vs^{\prime}\right)}{\|\operatorname{tf-idf}({\vs})\|\left\|\operatorname{tf-idf}\left({\vs}^{\prime}\right)\right\|}.
$$

\textbf{BERT Cosine Similarity} is an average cosine score between the embeddings of candidate solution pairs, where embeddings are performed using CodeBERT~\cite{feng2020codebert}, a pre-trained model for understanding code semantically: 
$$
\textbf{BERT sim.} = \frac{1}{|\mathcal{X}|} \sum_{\vx \in \mathcal{X}} \frac{1}{k\left(k-1\right)} \sum_{\substack{{\vs}, {\vs}^{\prime} \in [\vs]_k \\ {\vs} \neq {\vs}^{\prime}}} \frac{\operatorname{CodeBERT}({\vs}) \cdot \operatorname{CodeBERT}\left({\vs}^{\prime}\right)}{\|\operatorname{CodeBERT}({\vs})\|\left\|\operatorname{CodeBERT}\left({\vs}^{\prime}\right)\right\|}.
$$


\textbf{Levenshtein Similarity} is based on the Levenshtein distance, which measures the minimum number of single-character edits (insertions, deletions, or substitutions) required to transform one string into another: 
$$
\textbf{lev. sim.} = \frac{1}{|\mathcal{X}|} \sum_{\vx \in \mathcal{X}} \frac{1}{k\left(k-1\right)} \sum_{\substack{{\vs}, {\vs}^{\prime} \in [\vs]_k \\ {\vs} \neq {\vs}^{\prime}}} \frac{\operatorname{Levenshtein Distance}(\vs, \vs') }{\max(|\vs|, |\vs'|)}.
$$
\input{exp_figtex/pert_4omini}
\textbf{Token Sequence Similarity} measures the overlap between two sequences of tokens (e.g., programming language tokens), denoted by $T(\vs)$ for output $\vs$: 
$$
\textbf{seq. sim.} = \frac{1}{|\mathcal{X}|} \sum_{\vx \in \mathcal{X}} \frac{1}{k\left(k-1\right)} \sum_{\substack{{\vs}, {\vs}^{\prime} \in [\vs]_k \\ {\vs} \neq {\vs}^{\prime}}} \frac{|T(\vs) \cap T(\vs')|}{|T(\vs) \cup T(\vs')|}.
$$

\input{exp_figtex/pert_llama}
\input{exp_figtex/feedback_4omini}
\section{Additional Results of Task-Agnostic Approaches}
\label{sec:appendix_task_agnostic}
We evaluate the Role, Instruction, and Jabberwocky strategies across  six benchmarks, measuring their EM@k rate for reasoning and math tasks and their Pass@k rate for code generation tasks, in comparison to the direct sampling. The results in Figure~\ref{fig:pert_4omini}, generated using \texttt{GPT-4o-mini}, show an relative improvement of 6.2\% in EM@10 on the GSM-Hard dataset and 11.6\% in Pass@10 on the APPS dataset. The results in Figure~\ref{fig:pert_llama}, generated using \texttt{Llama-3.1-8B-Instruct}, show a 2.8\% relative improvement in EM@10 on the MMLU-Pro dataset, a 15.7\% relative improvement in EM@10 on GSM-Hard, and a 4.1\% relative improvement in Pass@10 on Humaneval.

% while those in Figure~\ref{fig:pert_llama} are from \texttt{Llama-3.1-8B-Instruct}.


\section{Additional Results of Random Idea Injection}
\label{appendix: randideainj}
We evaluate \texttt{RandIdeaInj} by generating 10 solutions with \texttt{GPT-4o-mini} with each strategy on the MMLU-Pro, GSM-Hard, and Humaneval datasets. \texttt{GPT-3.5-turbo} serves as the thinker model in the Dual strategy for idea generation. The results, shown in Figure~\ref{fig:feedback_4omini}, indicate that \texttt{RandIdeaInj} achieves a 6.8\% relative improvement in EM@10 on MMLU-Pro and a 4.3\% relative improvement in Pass@10 on Humaneval.

\input{exp_figtex/temp_mmlu}
\section{Temperatures}
We present the scaling curves for \texttt{GPT-3.5-Turbo}, \texttt{GPT-4o-mini}, and \texttt{Llama-3.1-8B-Instruct} at temperature settings ranging from 0.0 to 1.2 (in increments of 0.2) on the MMLU-Pro, GSM-Hard, and Humaneval benchmarks in Figure~\ref{fig:temp_mmlu}.

\section{Examples of Prompt Injections}
We show examples of injected-prompts of Jabberwocky, Role and Instruction. 
\input{sections/appendix/appendix_prompts}