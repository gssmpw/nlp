\section{Theoretical Details}\label{sec:theoreticadetail}

In this section, for clarity, we use upper case letters $K$ to denote an LLM's total number of attempts, and lower case letters $k$ to denote the attempt subscripts. This is in slight contrast with the main paper where $k$ is used to denote number of attempts when discussing EM@k and Pass@k metrics, and $N$ is used to denote the number of attempts for the final model. 

We first restate Assumption~\ref{asmp: varied response} and Assumption~\ref{asmp: similar utility} in more technical detail: 
\begin{assumption} [Restatement of Assumption~\ref{asmp: varied response}]
Consider the log probability that the response to an input $\vr$ fails the verifier: 
$$ q(\vr) = \log \mathbb{P}_{s \sim \text{LLM} (\cdot|\vr)} \big[V(s) = 0 \big| \vr \big], $$
then the perturbed input distribution $d(\vr)$ satisfies that its first and second moments are lower bounded for any input $\vr$, i.e. there exists constants $\hat{\mu}_1, \hat{\mu}_2 > 0$ such that
\begin{equation}
\begin{aligned}
    \mathbb{E}_{\vr' \sim d(\vr)} \big|q(\vr') - \bar{q} \big| &\geq \hat{\mu}_1, \\
    \mathbb{E}_{\vr' \sim d(\vr)} \big( q(\vr') - \bar{q} \big)^2 &\geq \hat{\mu}_2, 
\end{aligned}
\end{equation}
where $\bar{q} = \mathbb{E}_{\vr' \sim d(\vr)} q(\vr')$ is the mean value. 
\end{assumption}

\begin{assumption} [Restatement of Assumption~\ref{asmp: similar utility}]
The failure rate of the LLM with $K$ attempts between inputs from the original distribution $\vr \sim \tR$ and the perturbed distribution $\vr' \sim d(\vr), \vr \sim \tR$ have a close-to-$1$ ratio. Specifically, there exists a small constant $\epsilon$ such that
$$ 1-\epsilon \leq \frac{\mathbb{E}_{\vr' \sim d(\vr), \vr \sim \tR} \big[ \exp(K q(\vr')) \big]} {\mathbb{E}_{\vr \sim \tR} \big[ \exp(K q(\vr)) \big]} \leq 1+\epsilon, $$
where notice
$$ \exp(K q(\vr)) = \mathbb{P}_{s \sim \text{LLM} (\cdot|\vr)} ^K \big[V(s) = 0 \big] $$
is the failure rate with repeated sampling on a single input. 
\end{assumption}
\begin{remark}
We actually only need the right hand side, but we include both a lower and an upper bound here for a more comprehensive comparison between perturbed and unperturbed inputs. 
\end{remark}

Now we fully state Theorem~\ref{thm: main} and present its proof. 
\begin{theorem} [Restatement of Theorem~\ref{thm: main}]
For a testing input distribution $\tR$, define
\begin{equation}
\begin{aligned}
N_{\text{inj}}^K = \mathbb{P} \bigg[ V(\vs_k) = 0, \forall k \in [K] \bigg|& \vs_k \sim \text{LLM}(\cdot | \vr_k), \vr_k \sim d(\vr), \forall k \in [K], \vr \sim \tR \bigg]
\end{aligned}
\end{equation}
and
\begin{equation}
N_{\text{reg}}^K = \mathbb{P} \bigg[ V(\vs_k) = 0, \forall k \in [K] \bigg| \vs_k \sim \text{LLM}(\cdot | \vr), \forall k \in [K], \vr \sim \tR \bigg]
\end{equation}
to be the probabilities of failing Pass@K for prompts with and without injection, respectively. Then
$$ N_{\text{inj}}^K \leq N_{\text{reg}}^K / C_K, $$
where $C_K = O(K)$ is greater than $1$ and increasing in $K$. 
\end{theorem}

\begin{proof} [Proof of Theorem~\ref{thm: main}]
First, using the fact that $\vs_k$ are i.i.d. samples, we rewrite the two values with log probabilities $q(\vr)$: 
\begin{align}
N_{\text{inj}}^K &= \mathbb{P} \bigg[ V(\vs_k) = 0, \forall k \bigg| \vs_k \sim \text{LLM}(\cdot | \vr_k), \vr_k \sim d(\vr), \vr \sim \tR \bigg] \notag \\
&= \mathbb{E} \bigg[ \prod_{k=1}^K \mathbb{P} \big[ V(\vs_k) = 0 \big| \vs_k \sim \text{LLM}(\cdot | \vr_k) \big] \bigg| \vr_k \sim d(\vr), \vr \sim \tR \bigg] \notag \\
&= \mathbb{E} \bigg[ \prod_{k=1}^K \exp (q(\vr_k)) \bigg| \vr_k \sim d(\vr), \vr \sim \tR \bigg] \notag \\
&= \mathbb{E} \bigg[ \exp \bigg( \sum_{k=1}^K q(\vr_k) \bigg) \bigg| \vr_k \sim d(\vr), \vr \sim \tR \bigg], \label{eq: main proof M1}
\end{align}
Next, denoting $q_k = q(\vr_k)$, we will draw a connection between $\exp \big( \sum_{k=1}^K q_k \big)$ and $(1/K) \sum_{k=1}^K \exp (Kq_k)$. For this we denote $\bar{q} = (1/K) \sum_{k=1}^K q_k$ and write
\begin{align}
\frac{\frac{1}{K} \sum_{k=1}^K \exp(Kq_k)} {\exp \big( \sum_{k=1} ^K q_k \big)} &= \frac{1}{K} \sum_{k=1} ^K \exp \big( Kq_k - K \bar{q} \big) \notag \\
&= \frac{1}{K} \sum_{k=1} ^K \big[ g \big( Kq_k - K \bar{q} \big) + \big( Kq_k - K \bar{q} \big) + 1 \big] \notag \\
&= 1 + \frac{1}{K} \sum_{k=1} ^K g \big( Kq_k - K \bar{q} \big), \label{eq: gensen ratio M1}
\end{align}
where $g(x) = e^x - x - 1$ is a non-negative function. Using basic analysis, one can easily prove $g(x) \geq \min \{ 0.25 x^2, 0.5 |x|\}$ for any $x \in \mathbb{R}$, which gives us from Equation~\ref{eq: gensen ratio M1} that
\begin{align}
\frac{\frac{1}{K} \sum_{k=1}^K \exp(Kq_k)} {\exp \big( \sum_{k=1} ^K q_k \big)} &\geq 1 + \min \bigg\{ \frac{0.5} {K} \sum_{k=1} ^K \big| Kq_k - K \bar{q} \big|, \frac{0.25} {K} \sum_{k=1} ^K \big( Kq_k - K \bar{q} \big)^2 \bigg\} \notag \\
&= 1 + \min \bigg\{ 0.5 \sum_{k=1} ^K \big| q_k - \bar{q} \big|, 0.25K \sum_{k=1} ^K \big( q_k - \bar{q} \big)^2 \bigg\}, \notag
\end{align}
and so from Assumption~\ref{asmp: varied response} and the central limit theorem, there exists $c_K = O(K)$ such that the left hand side above is at least $1 + c_K$ with high probability. \footnote{This can be more rigidly proven using mathematical languages from probability theory, but the proof is unnecessarily tedious for our purposes, and the intuition behind such a proof is exactly as explained here.} Thus continuing from \ref{eq: main proof M1}, we finally have
\begin{align}
N_{\text{inj}}^K &\leq \mathbb{E} \bigg[ \frac{\frac{1}{K} \sum_{k=1}^K \exp(Kq(\vr_k))} {1 + c_K} \bigg| \vr_k \sim d(\vr), \vr \sim \tR \bigg] \notag \\
&= \frac{1}{K (1 + c_K)} \sum_{k = 1}^K \mathbb{E} \big[ \exp(Kq(\vr_k)) \big| \vr_k \sim d(\vr), \vr \sim \tR \big] \notag \\
&\leq \frac{1 + \epsilon}{K (1 + c_K)} \sum_{k = 1}^K \mathbb{E} \big[ \exp(Kq(\vr_k)) \big| \vr_k \sim \tR \big] \notag \\
&= \frac{1 + \epsilon}{1 + c_K} \mathbb{E} \big[ \exp(Kq(\vr)) \big| \vr \sim \tR \big] \notag \\
&= \frac{1}{1 + (c_K - \epsilon) / (1 + \epsilon)} \mathbb{P}^K \bigg[ V(\vs) = 0 \bigg| \vs \sim \text{LLM}(\cdot | \vr), \vr \sim \tR \bigg] \notag \\
&= \frac{1}{1 + (c_K - \epsilon) / (1 + \epsilon)} \mathbb{P} \bigg[ V(\vs_k) = 0, \forall k \in [K] \bigg| \vs_k \sim \text{LLM}(\cdot | \vr), \forall k \in [K], \vr \sim \tR \bigg] \notag \\
&= \frac{1}{1 + (c_K - \epsilon) / (1 + \epsilon)} N_{\text{reg}}^K, \notag
\end{align}
where the second inequality uses Assumption~\ref{asmp: similar utility}. Therefore letting $C_K = 1 + (c_K - \epsilon) / (1 + \epsilon)$, we finish the proof of the Theorem. 
\end{proof}