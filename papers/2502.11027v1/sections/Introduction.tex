\section{Introduction}
\label{sec:intro}
The debate between investing resources in training stronger models versus developing effective inference-time methods reflects a fundamental trade-off in the field of machine learning. Improvements through training typically involve scaling up model size, enhancing training datasets, or incorporating domain-specific fine-tuning. While these approaches can significantly boost performance, they focus on producing a single optimal solution and often come with substantial resource costs. 

 \begin{figure}[ht]
    \centering
        \subfigure[Direct Sampling] {\includegraphics[width=0.45\linewidth]{figs/intro01.pdf}\label{fig:introduce1}}
       \subfigure[Diversified Sampling] { \includegraphics[width=0.45\linewidth]{figs/intro02.pdf}\label{fig:introduce2}}
    \caption{A brief sketch of (a) direct sampling without perturbing prompts and (b) diversified sampling.}
    \label{fig:introduce}
\end{figure}

Conversely, test-time scaling \citep{zeng2024scaling, wu2024inferencescalinglawsempirical, nori2024medprompto1explorationruntime, snell2024scaling, brown2024large, gandhi2024stream, anonymous2025scaling, lee2025evolvingdeeperllmthinking,  anonymous2025planning}, such as best-of-N sampling ~\citep{cobbe2021training, lightman2023let}, aims to maximize the utility of pre-trained models, enabling efficient exploration and improved accuracy without additional training. Repeated sampling generally selects the best solution from a diverse set of candidate responses. However, sampling solutions from a large language model (LLM) using the same prompt often leads to similar outputs, “trapped” into a local cluster (Figure~\ref{fig:introduce1}). The concentrated nature of the generated solutions might arise from the limited diversity inherent in the post-training objectives commonly employed to train large language models (LLMs) as instruction-following chatbots by optimizing zero-shot performance \citep{xiang20252reasoningllmslearning}. These objectives often prioritize optimizing the model to produce a single, correct
answer \citep{xiang20252reasoningllmslearning}, which mismatches with the goal of repeated sampling. The commonly used distillation technique may also diminish model diversity \citep{cideron2024diversityrewardedcfgdistillation, deepseekai2025deepseekr1incentivizingreasoningcapability}.
Diverse candidate solutions should span multiple clusters, with responses distributed across a broader solution space, breaking out of local clusters (Figure~\ref{fig:introduce2}). A natural approach to achieving this is to inject diversity into the prompts. 
Table~\ref{tab:apps_similarity} shows the Best-of-N (BoN) using diversified prompts, with different metrics: (a) the \textbf{Pass@k} rate measures the performance of tasks for which the correct solution is found; (b) the diversity of solutions is measured by a series of similarity metrics, including \textbf{tf-idf}, \textbf{BERT}, \textbf{Levenshtein}, and \textbf{token sequence} (see Appendix~\ref{metrics} for more details on the metrics).
The diversity strategies include Role, Instruction, and Jabberwocky perturbations, representing different styles of prompt injection to promote varied responses. We refer to these strategies as task-agnostic approaches (Section~\ref{sec:task-specific}). Table~\ref{tab:apps_similarity}  shows that the pass rate improves when injection strategies produce candidate solutions with reduced similarity.
\begin{table}[t]
\vspace{-0.1in}
    \centering
    \caption{\textbf{Effects of different injection strategies.} 10 solutions were generated using \texttt{gpt-3.5-turbo} for each strategy on the HumanEval benchmark.}
    \setlength\tabcolsep{1.1pt}
\scalebox{0.95}{
    \begin{tabular}{c| c c c c c c}
    \toprule
    \fontsize{9pt}{11pt}\selectfont\textbf{Strategies} & 
    \fontsize{9pt}{11pt}\selectfont\textbf{Pass@10} & 
    \fontsize{9pt}{11pt}\selectfont\textbf{tf-idf sim.} & 
    \fontsize{9pt}{11pt}\selectfont\textbf{BERT sim.} & 
    \fontsize{9pt}{11pt}\selectfont\textbf{lev. sim.} & 
    \fontsize{9pt}{11pt}\selectfont\textbf{seq. sim.} \\ 
    \midrule 
    None & 0.2050 & 0.4687 &  0.9964 &0.5842  & 0.5370   \\
    Role  & 0.3150 & 0.4301 & 0.9961 & 0.5348 & 0.4944  \\
    Instruction  & 0.3050 & 0.3545 & 0.9947 & 0.4821   & 0.4100 \\ 
    Jabberwocky & 0.3100 &0.4279  & 0.9967 & 0.5309 &   0.4815 \\
    \bottomrule
    \end{tabular}}
    \label{tab:apps_similarity}
    \vspace{-0.2in}
\end{table}

%In this paper, we propose two categories of perturbations, dubbed \texttt{DivSampling} (\textbf{Div}ersified \textbf{Sampling}), designed to perturb the prompt distribution, leading the generative model to generate diversity of candidate solutions by perturbing the prompts, with the goal of improving the best selection outcome in repeated sampling. 
In this paper, we propose two categories of perturbations, dubbed \texttt{DivSampling} (\textbf{Div}ersified \textbf{Sampling}). These perturbations modify the prompt distribution, encouraging the generative model to produce a diverse set of candidate solutions, thereby improving the quality of the best selection in repeated sampling.
In addition to the task-agnostic approaches of Role, Instruction, and Jabberwocky, we introduce two groups of task-specific methods: Random Idea Injection (\texttt{RandIdeaInj}), designed to generate high-level candidate ideas for a given task, and Random Query Rephraser (\texttt{RandQReph}), which restates the original question. 
Our methodology builds upon the following observation:
\begin{quote}
\vspace{-0.1in}
\it{
Maximizing the generation of diverse yet relevant answers from LLMs can significantly enhance the pass performance of scaling inference.}
%\vspace{-0.1in}
\end{quote}

We also show theoretically that our prompt perturbation framework reduces the Pass@k or EM@k error rate by a substantial ratio, which is linear in the number of prompts $k$ (see Sec. \ref{sec:theoretical} and \ref{sec:theoreticadetail} for details). 

Our empirical findings show that these approaches significantly increase solution accuracy for repeated sampling. 
\texttt{RandIdeaInj} achieves relative improvements of 13.5\% in EM@10 on reasoning tasks, 15.5\% in EM@10 on mathematics tasks, and 15.4\% in Pass@10 on code generation tasks. When combined with task-specific perturbations, it demonstrates a 75.6\% relative improvement in Pass@10 for code generation. Similarly, \texttt{RandQReph} delivers a 63.4\% relative improvement when restating the question and 29.3\% relative improvement through back-translation.
