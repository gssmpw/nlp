\section{Conclusion}
In this paper, we introduce \texttt{DivSampling}, a novel and generalizable prompt perturbation framework designed to address the inherent limitations of uniform LLM outputs during inference by injecting diversity into prompt-based sampling. By leveraging task-agnostic and task-specific strategies—including Role, Instruction, Jabberwocky, Random Idea Injection, and Random Query Rephraser—\texttt{DivSampling} broadens the distribution of candidate responses, leading to improvements across diverse tasks such as reasoning, mathematics, and code generation. Our empirical evaluation demonstrates significant enhancements in solution accuracy (Pass@k), confirming that prompt diversification effectively breaks the local clustering problem commonly observed in traditional sampling methods. Theoretical analysis further reinforces that increased diversity reduces error rates linearly with the number of diverse prompts. By optimizing test-time inference without additional training, \texttt{DivSampling} offers a scalable, efficient solution for boosting LLM performance and practical applicability in real-world tasks.