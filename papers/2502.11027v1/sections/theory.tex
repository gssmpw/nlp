\vspace{-0.3cm}
\section{Theoretical Analysis}\label{sec:theoretical}
\vspace{-0.2cm}
In this section, we analyze the perturbation injection method and present a theoretical result stating its improvement over unperturbed input texts. For technical details and proof of theorem, please refer to Appendix~\ref{sec:theoreticadetail}. 

For notational simplicity, we use $\vr = [\vp, \vq]$ to denote concatenated prompt-question pairs, and write $\vr \sim \tR := \{ \vp \} \times \tQ$ with $\vq \sim \tQ$. To further formalize our setting, consider a prompt perturbation distribution $d(\cdot)$ that randomly injects perturbations into $\vr = [\vp, \vq]$ to get $\vr' \sim d(\vr)$. 

We base our theory on two natural assumptions on prompt perturbation distribution $d$ and prior input distribution $\tR$. Our first assumption stipulates that perturbed inputs are reasonably diverse in performance, which comes naturally from the diversity of the perturbed inputs themselves. 
\begin{assumption} \label{asmp: varied response}
The log probability that the response to input $\vr$ fails the verifier 
$ l(\vr) = \log \mathbb{P}_{\vs \sim \text{LLM} (\cdot|\vr)} \big[V(\vs) = 0 \big] $
has constant-level first and second moments under perturbed distribution $\vr' \sim d(\vr_0)$ for any original input $\vr_0$. 
\end{assumption}
The second assumption requires perturbed prompts to have similar performances compared with unperturbed prompts under simple resampling strategies. It is natural to assume that perturbed inputs have a similar utility compared to unperturbed ones; if not so, one should consider the perturbation harmful and use different perturbation methods. 
\begin{assumption} \label{asmp: similar utility}
The Pass@k or EM@k failure rate of the LLM between prompt-question input pairs from the original distribution $\vr = (\vp, \vq) \sim \tR$ and the perturbed distribution $\vr' \sim d(\vr), \vr \sim \tR$ have a close-to-$1$ ratio. 
\end{assumption}

With these assumptions, we give the following theorem (see detailed proof in Appendix \ref{sec:theoreticadetail}) quantifying the improvement in Pass@k for perturbation injection. 
\begin{theorem} \label{thm: main}
Consider sampling original input $\vr \sim \tR$ and perturbed prompt-question pair $\vr_k \sim d(\vr)$ for $k = 1, \cdots, N$. Define $N_{\text{inj}}^k$ and $N_{\text{reg}}^k$
% \begin{equation}
% \begin{aligned}
% N_{\text{inj}} = \mathbb{P} \bigg[ V(s_k) = 0, \forall k \bigg|& s_k \sim \text{LLM}(\cdot | \vp_k), \\
% & \vp_k \sim d(\vp), \vp \sim \mathcal{P} \bigg]
% \end{aligned}
% \end{equation}
% \begin{equation}
% N_{\text{reg}} = \mathbb{P} \bigg[ V(s_k) = 0, \forall k \bigg| s_k \sim \text{LLM}(\cdot | \vp), \vp \sim \mathcal{P} \bigg]
% \end{equation}
to be the probabilities of generating responses that fail to pass based on inputs with and without injection, respectively. Then 
$$ N_{\text{inj}}^k \leq N_{\text{reg}}^k / C_k, $$
where $C_k = O(k)$ is greater than $1$ and increasing in $k$. 
\end{theorem}

\begin{remark}
The main implications of this theorem are two-fold. First, since $C_k \geq 1$, perturbed inputs should \textbf{always perform better} than non-perturbed inputs. In practice, sometimes the assumptions don't hold strictly (especially Assumption~\ref{asmp: similar utility} when the perturbation harms prediction), but in general perturbing inputs improve performance. Second, despite the fact that error rate strictly decreases as $k$ increases for both perturbed and unperturbed inputs, the \textbf{decrease is substantially faster} for perturbed inputs, meaning larger $k$'s result in greater improvements. 
\end{remark}