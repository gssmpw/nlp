\section{Method}
\label{sec:method}

In this section, we present two categories of perturbations aimed at diversifying prompts: task-agnostic and task-specific approaches. The goal is to modify the input prompt distribution, encouraging the LLM to generate more dissimilar solutions. Task-agnostic approaches are general modifications that are not tailored to any specific prompt, whereas task-specific approaches perturb the prompt based on the content of each task.



\subsection{Task-Agnostic Approaches}
%\subsection{Random Description Injection}

We introduce three styles of perturbations, aimed at increasing prompt diversity by injecting a randomly sampled sentence from a pool of predefined ones, including Jabberwocky, Role and Instruction injections, in the hope of shifting the model's focus when generating responses. 

\textbf{Jabberwocky} injection randomly select a segment from the poetic  ``Jabberwocky" to enrich the linguistic diversity of the prompts.

\textbf{Role} injection introduces predefined role-descriptive sentences into prompts to steer the language model's generation process, guiding them to generate outputs that are tailored to specific roles. The predefined set of roles characterizes the generative model's descriptive identities, such as ``mentor", ``optimizer", ``innovator," etc. These roles are encapsulated in the original prompt, highlighting the key attributes of each persona.

\textbf{Instruction} injections are a series of steps or guidances that are critical for problem-solving within a domain. By injecting an instruction into the prompt, we aim to guide the model's processing toward generating outputs that are logical and contextually aligned with the given instruction. 
Examples of instructions for the code generation task include:
\begin{tcolorbox}[title=Example Instructions for Code Generation, colframe=low]
\scriptsize{
  \textbf{Instruction 1:} Write the code in a highly modular way, breaking down functionality into small, reusable components. Each function or class should have a single responsibility, and avoid large monolithic structures.\\


  \textbf{Instruction 2:} Focus on brevity and clarity, minimizing boilerplate code. Use shorthand syntax and built-in functions whenever possible to achieve a minimalist codebase without sacrificing readability.\\

  \textbf{Instruction 3:} Use an object-oriented approach where each concept is modeled as a class. Leverage inheritance, encapsulation, and polymorphism to create a flexible, scalable design.
  % to ensure consistency.
  
  }
\end{tcolorbox}
\normalsize
\subsection{Task-Specific Approaches}
\label{sec:task-specific}
%The random description injections act as universal perturbations for all samples, lacking customization tailored to the diverse specific prompts of a given sample. 
To provide more meaningful diverse prompts relevant to any specific task, we propose two approaches: the  Random Idea Injection (\texttt{RandIdeaInj}) and Random Query Rephraser (\texttt{RandQReph}).
\subsubsection{Random Idea Injection}
\label{sec:RandIdeaInj}
This group of approaches involves prompting an LLM that may differ from the original generative model, acting as a \emph{thinker} to generates ideas related to the question. 
These outputs, serving as perturbations, are then injected into the original prompt to create more targeted and diversified prompts. An example of prompting the LLM for ideas is like:
\begin{tcolorbox}[title=Prompt to Seek Ideas on Solving the Math Problem, colframe=low]
\scriptsize{
  \textbf{Prompt:} You are a helpful math tutor with expertise in problem-solving. When given a math question, only provide a concise explanation of the approach or idea needed to solve it. Do not write out every detailed step or final numeric answer. Instead, focus on the method or strategy (like “First, consider the Pythagorean theorem...” or “We can solve by factoring out common terms...”).
  }
\end{tcolorbox}
\normalsize
We outline three strategies for selecting the thinker model:

\textbf{Single}-model strategy employs a single model configuration.  In this approach, the prompt, along with the problem, is fed directly into the generative model. The generative model acts as the thinker, producing solutions or ideas specifically designed to address the given problem.

\textbf{Dual}-model approach employs a separate, distinct model that serves as a thinker, working alongside the primary generative model. In this setup, the thinker is first prompted to generate ideas before the current iteration of repeated sampling. This dual-system allows for a broader range of generated solutions by incorporating insights from another model. We show examples of ideas generated in response to a question from GSM-Hard using a dual-model strategy:
\begin{tcolorbox}[title=Example Dual-model Ideas for a Math Question, colframe=low ]
\scriptsize{
\textbf{Question:} Ruby is 8882014 times older than Sam. In 9 years, Ruby will be 3 times as old as Sam. How old is Sam now? \\

  \textbf{Idea 1:} Let Ruby's current age be \( R \) and Sam's current age be \( S \). From the problem, we can create two equations based on the relationships given:

1. \( R = 8882014S \) (Ruby is 8882014 times older than Sam)\\
2. In 9 years, \( R + 9 = 3(S + 9) \) (Ruby will be three times Sam's age in 9 years)

You can substitute the expression from the first equation into the second equation and solve for \( S \). This will give you Sam's current age.\\


  \textbf{Idea 2:} Start by letting Sam's current age be \( x \). Then, Ruby's current age can be expressed as \( 8882014x \). In 9 years, you can set up the equation relating their ages: Ruby's age in 9 years will be \( 8882014x + 9 \) and Sam's age will be \( x + 9 \). Use the relationship that in 9 years, Ruby's age will be three times Sam's age to form the equation: 

\[ 8882014x + 9 = 3(x + 9) \]

From there, solve for \( x \).
  }
\end{tcolorbox}
\normalsize


\textbf{Diverse}-model approach  seeks to generate more varied prompts by drawing from a diverse set of LLMs. Before each iteration of repeated sampling, a thinker model is randomly chosen from the available model options whenever a problem is provided. %This model is then solicited to produce solutions or ideas to address the problem, thereby enriching the original prompt.   

\subsubsection{Random Query Rephraser}
\label{sec:RandQReph}
An alternative approach to diversifying the prompts for any given task is to rephrase the query at each iteration. To accomplish this, we introduce the \texttt{RandQReph} strategy, where an LLM, acting as a narrator, is tasked with rephrasing the input question during each BoN sampling. Similar to \texttt{RandIdeaInj}, this strategy includes three variations: a \textbf{single}-model strategy, where the generative model itself rephrases the question; a \textbf{dual}-model strategy, where a separate model acts as the rephraser; and a \textbf{diverse}-model strategy, where the rephraser is randomly selected from a set of LLMs for each iteration. The rephrased question $\vq'_k$ replaces the original question $\vq_k$, forming the query pair $(\vp, \vq'_k)$ at the $k$-th sampling. Additionally, query rephrasing can be achieved through back-translation \citep{beddiar2021dataexpansionusingtranslation}, a process where the query is translated from the target language back to the source language. This technique generates slightly modified versions of the original text while preserving its core meaning, thereby expanding the dataset with diverse wording while maintaining contextual consistency.


