\vspace{-0.3cm}
\section{Experiments}
\label{sec:exps}
\vspace{-0.2cm}
\subsection{Datasets}
\label{sec:exps_task}
We evaluate \texttt{DivSampling} across six benchmarks including reason, math and coding: \textbf{(a)} Multiple choice questions-answering on \textbf{MMLU-Pro}~\cite{wang2024mmlu}, a dataset curated by eliminating
some trivial and noisy questions from MMLU~\cite{hendrycks2020measuring}
while incorporating more reasoning-focused problems. %Unlike MMLU, which primarily assesses factual knowledge, MMLU-Pro emphasizes reasoning ability. 
For evaluation, we randomly select 150 samples from the dataset. \textbf{(b)} Math problem-solving on \textbf{GSM-hard}~\cite{gao2023pal} and \textbf{MATH}~\cite{hendrycks2021measuring}. GSM-Hard increases the computational complexity of GSM8K~\cite{cobbe2021training} by replacing numerical values with larger numbers. MATH consists of competitive-level mathematical problems requiring high levels of reasoning ability and mathematical knowledge. We randomly sample 100 problems from both GSM-Hard and MATH for evaluation. \textbf{(c)} Code generation on \textbf{Humaneval}~\cite{chen2021evaluating}, \textbf{MBPP}~\cite{austin2021program} and \textbf{APPS}~\cite{hendrycks2021measuringapps}.  HumanEval includes 164 human-generated Python problems, while MBPP consists of 399 problems covering basic algorithmic and functional programming tasks. APPS features challenging code competition problems. Due to budget constraints, we randomly sample 200 problems from the 10,000 available problems in APPS for evaluation.
\input{exp_figtex/pert_turbo}
\input{exp_figtex/feedback_turbo}
\input{exp_figtex/pert_feedback_turbo}
\input{exp_figtex/pert_feedback_models}
 \subsection{Experiment Details} For simplicity, we configured the models with a temperature of 0.4 for the reasoning dataset MMLU-Pro, a uniform temperature of 0.2 for the math task datasets GSM-Hard and MATH, and a temperature of 0.6 for all code generation benchmarks including Humaneval, MBPP, and APPS. These temperature settings were determined through a coarse hyperparameter sweep from $T\in \{0.0, 0.2, ..., 1.2\}$. In the decoding-phase, we use top-$p$ sampling with a fixed value of 1.0 across all experiments.
All method evaluations are allocated the same search budget of 10 solutions. \texttt{DivSampling} is assessed in comparison to the direct sampling without perturbation, which is referred to as \textbf{None} across the experiments. We run experiments on a server with 4 NVIDIA A100 GPUs, each one with 80GB RAM.

 
\subsection{Results of Task-Agnostic Approaches}


We evaluate the Role, Instruction, and Jabberwocky strategies in Section~\ref{sec:task-specific} across six benchmarks spanning reasoning, mathematics, and code generation, comparing them to direct sampling without perturbations. Figure~\ref{fig:pert_turbo} shows their scaling curves of evaluation on \texttt{GPT-3.5-turbo}~\cite{openai2023gpt35turbo}.  
We find that these injection strategies yield improvements across all tasks, with a notable 8.6\% increase in EM@10 on GSM-Hard and the most significant gains on APPS, achieving approximately a 53.7\% improvement in Pass@10 over direct sampling. We encourage readers to check Appendix~\ref{sec:appendix_task_agnostic} for more results of other models.


% We find that the injections lead to the greatest enhancements in code generation tasks, whereas they have minimal impact on reasoning tasks. Among these injection strategies, the Instruction approach consistently delivers the best performance in terms of EM@K and Pass@K across all tasks.

\input{exp_figtex/apps_restate}
\input{exp_figtex/translation_scaling_curve}

\subsection{Results of Random Idea Injection} 
The evaluation involves a range of \texttt{RandIdeaInj} strategies in Section~\ref{sec:RandIdeaInj}, including the single-model approach, dual-model approach, and diverse-model approach, evaluated across the benchmarks MMLU-Pro, GSM-Hard, and HumanEval. Figure~\ref{fig:feedback_turbo} displays the scaling curves of evaluations conducted with the generative model \texttt{GPT-3.5-turbo}. \texttt{RandIdeaInj} exhibits consistent improvement with idea-injected prompts, achieving a 13.5\% increase in reasoning on MMLU-Pro, a 15.5\% increase in the mathematics on GSM-Hard, and a 15.4\% increase in coding on the Humaneval dataset, over the direct sampling. See Appendix~\ref{appendix: randideainj} for more results of \texttt{RandIdeaInj} from other models.


\subsection{Results of Combining Injection Strategies} 
We show the Pass@k results for combining Role, Instruction, and Jabberwocky injections with three \texttt{RandIdeaInj} strategies on the APPS dataset, using \texttt{GPT-3.5-turbo}, as shown in Figure~\ref{fig:pert_feedback_turbo}.  We find that combining the injections significantly enhances performance, achieving maximum relative improvements in Pass@10 of 75.6\%, 73.2\%, and 75.6\% over the direct sampling. We extend our evaluation of the combined Role and Dual strategies to additional models, presenting the resulting scaling curves in Figure~\ref{fig:pert_feedback_models}. Notably, the Pass@10 relative improvement reaches 40.0\% with \texttt{Llama-3.1-8B-Instruct}.

\input{exp_figtex/apps_role_dual_temp}
\input{exp_figtex/token_scaling_curve}
\subsection{Results of Random Query Rephraser}
We show Pass@k results of three types of \texttt{RandQReph} in Section~\ref{sec:RandQReph} from different models on APPS in Figure~\ref{fig:apps_restate}. The best-performing strategy exhibits an relative improvement in Pass@10 over direct sampling, achieving 11.6\% for \texttt{GPT-4o-mini}, 28.0\% for \texttt{Llama-3.1-8B-Instruct}, 18.4\% for \texttt{Qwen2.5-Coder-7B-Instruct}, and a notable 63.4\% for \texttt{GPT-3.5-turbo}. In addition, Figure~\ref{fig:translation_scaling} illustrates the scaling curves of back-translation, which show a 29.3\% relative improvement in Pass@10 compared to direct sampling. 


\subsection{Effects of Temperature Sweeping}
We show the temperature sweeping of the task-agnostic strategy Role and its combination with Dual, ranging from 0.0 to 1.2 in increments of 0.2, on the APPS dataset in Figure~\ref{fig:apps_role_dual_temp}. Our findings show that Role achieves Pass@k relative improvements over direct sampling without perturbation, with its best Pass@10 improving by 15.9\% compared to the best Pass@10 of direct sampling. Furthermore, Role+Dual enhances performance beyond Role, achieving an 8.6\% improvement in its best Pass@10 compared to the best Pass@10 of the Role method. 
\subsection{Scalability}
Multi-round Debate~\cite{du2023improving} is a strategy that relies on an additional model or agent to provide a reference answer. In literature, debating also shows effectiveness in improve LLM performance. Intuitively, debating is also one kind of diversity injection in prompt. In the Multi-round Debate~\cite{du2023improving}, the primary model updates its response in the following round based on that reference, ultimately producing a refined answer. We assess the scalability of our method versus Debate by comparing the proportion of problems solved when both approaches use the same number of output tokens. The evaluation is performed on Humaneval, with \texttt{GPT-3.5-turbo} serving as the generative model. \texttt{GPT-4o-mini} is employed as the thinker model for idea generation in the Dual strategy of \texttt{RandIdeaInj} and as the reference model in the Debate strategy. The results in Figure~\ref{fig:token_scaling} tell that the Dual strategy consistently outperforms the Debate strategy when using the same number of output tokens, showing its superior scalability compared to the Debate method.

\subsection{Results of DivSampling on top of CoT}
We evaluate various injection strategies, including Role, Instruction, Jabberwocky, and their combinations with Dual, applied on top of Chain-of-Thought (CoT) \citep{wei2022chain, cot2}.
\input{exp_figtex/cot_scaling_curve}
The performances from \texttt{GPT-3.5-turbo} on the APPS are presented in Figure~\ref{fig:cot_scaling}. CoT is implemented by prompting the generative model to break down its solution into a step-by-step manner:
\begin{tcolorbox}[title=Example CoT Prompt, colframe=low]
\scriptsize{
\textbf{Prompt:} When you receive a problem description, methodically break down the implementation into distinct, logical steps within the Python code itself. Use comments within your code to clearly delineate these steps, focusing exclusively on the logic and structure necessary to solve the problem as described. Make sure each part of your solution is self-contained within a Python code block, illustrating the solution's development in a step-by-step manner...
  }
\end{tcolorbox}
\normalsize 
Results in Figure~\ref{fig:cot_scaling} show  that even simple task-agnostic approaches, as well as their  combinations with the Dual strategy of \texttt{RandIdeaInj}, lead to improvements over standard CoT. Notably, on top of CoT, \texttt{DivSampling} achieves a 13.1\% relative improvement over direct sampling.




