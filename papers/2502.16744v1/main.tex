\documentclass[twoside,11pt,]{article}

\usepackage{blindtext}


\usepackage[preprint]{jmlr2e}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs,enumitem} % for professional tables
\usepackage{float}
\usepackage{xcolor}
\usepackage{natbib}
\usepackage{algorithm,algorithmic}
\usepackage{makecell}
\usepackage{multirow}

\usepackage{hyperref}


% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\newtheorem{assumption}{Assumption}
% \newcommand{\qedhere}{\rlap{\hspace{1em}$\square$}}
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

\newcommand{\op}[1]{\operatorname{#1}}
\newcommand{\C}[1]{\mathcal{#1}}
\newcommand{\BF}[1]{\mathbf{#1}}
\newcommand{\BB}[1]{\mathbb{#1}}
\newcommand{\K}{\C{K}}
\newcommand{\Kd}{\K_\delta}
\newcommand{\x}{\BF{x}}
\newcommand{\xt}{\x_t}
\newcommand{\xs}{\x^*}
\newcommand{\txs}{\tilde{\x}^*}
\newcommand{\y}{\BF{y}}
\newcommand{\yt}{\y_t}
\newcommand{\tyt}{\tilde{\y}_t}
\newcommand{\z}{\BF{z}}
\newcommand{\p}{\BF{p}}
\newcommand{\uu}{\BF{u}}
\newcommand{\g}{\BF{g}}
\newcommand{\tf}{\tilde{f}}
\newcommand{\tft}{\tilde{f}_t}
\newcommand{\tg}{\tilde{g}}
\newcommand{\tgt}{\tilde{g}_t}
\newcommand{\hf}{\hat{f}}
\newcommand{\hft}{\hat{f}_t}
\newcommand{\normdt}{\Vert \nabla_t \Vert}
\newcommand{\normdm}{\Vert \bn_m \Vert}
\newcommand{\sumT}{\sum\limits_{t=1}^T}
\newcommand{\qt}{Q_t}
\newcommand{\qtt}{Q_{t-1}}
\newcommand{\bn}{\bar{\nabla}}


\ShortHeadings{Projection-Free Algorithm for COCO}{Lu, Pedramfar, and Aggarwal}
\firstpageno{1}

\renewcommand{\cite}[1]{\citep{#1}}
\begin{document}

\title{Order-Optimal Projection-Free Algorithm for Adversarially Constrained Online Convex Optimization}

\author{\name Yiyang Lu \email lu1202@purdue.edu 
        \AND
        \name Mohammad Pedramfar \email mohammad.pedramfar@mila.quebec 
        \AND
        \name Vaneet Aggarwal \email vaneet@purdue.edu
        }
\editor{}

\maketitle

\begin{abstract}

Projection-based algorithms for constrained Online Convex Optimization (COCO) face scalability challenges in high-dimensional settings due to the computational complexity of projecting iterates onto constraint sets. This paper introduces a projection-free algorithm for COCO that achieves state-of-the-art performance guarantees while eliminating the need for projections. 
By integrating a separation oracle with adaptive Online Gradient Descent (OGD) and employing a Lyapunov-driven surrogate function, while dynamically adjusting step sizes using gradient norms, our method jointly optimizes the regret and cumulative constraint violation (CCV).
We also use a blocked version of OGD that helps achieve tradeoffs betweeen the regret and CCV with the number of calls to the separation oracle. For convex cost functions, our algorithm attains an optimal regret of $\mathcal{O}(\sqrt{T})$  and a CCV of $\mathcal{O}(\sqrt{T} \log T)$, matching the best-known projection-based results, while only using $\tilde{\mathcal{O}}({T})$ calls to the separation oracle. The results also demonstrate a tradeoff where lower calls to the separation oracle increase the regret and the CCV. In the strongly convex setting, we further achieve a regret of $\mathcal{O}(\log T)$  and a CCV of  $\mathcal{O}(\sqrt{T\log T} )$, while requiring ${\mathcal{O}}({T}^2)$ calls to the separation oracle. Further, tradeoff with the decreasing oracle calls is studied. These results close the gap between projection-free and projection-based approaches, demonstrating that projection-free methods can achieve performance comparable to projection-based counterparts.

\end{abstract}

\section{Introduction}

Online Convex Optimization (OCO) has emerged as a foundational framework for sequential decision-making under uncertainty, with applications spanning resource allocation, real-time control systems, and adversarial robustness in machine learning \cite{shalev2012online}. A critical variant of this framework, Constrained Online Convex Optimization  (COCO), requires agents to minimize cumulative costs while adhering to time-varying constraints. In this problem, the learner interacts with an environment over $T$ rounds, where at round $t$, the learner selects a decision $\mathbf{x}_t \in \K$. The adversary then reveals a convex cost function $f_t: \K \to \BB{R}$ and  $k$ convex constraint functions $g_{t,i}: \K \to \BB{R},\ i \in [k]$. Let $\K^\star \subseteq \mathcal{X}$ denote the set of \emph{feasible decisions} satisfying all constraints $g_{t,i}(\mathbf{x}) \leq 0$ for $t \in [T],\ i \in [k]$. We assume $\K^\star \neq \emptyset$ to ensure that at least one feasible solution exists. Then, we use the following performance metrics:
\vspace{-.1in}
\begin{align}
    \mathtt{Regret}(T) = &\sumT f_t(\x_t) - \min_{\mathbf{x}^\star \in \K} \sum_{t=1}^T f_t(\mathbf{x}^\star) \\
    \mathtt{CCV}(T) = &\max_{i \in [k]} \sumT \left(g_{t,i}(\x_t)\right)^+.
\end{align}

The goal is to design an algorithm that achieves sublinear $\mathtt{Regret}(T)$ and $\mathtt{CCV}(T)$ simultaneously. COCO's applications span diverse domains, including portfolio optimization under dynamic risk constraints \cite{redeker2018portfolio},  dynamic resource allocation for time-varying workloads \cite{doostmohammadian2022distributed}, real-time dynamic pricing with inventory constraints \cite{li2024dynamic}, and collision-free trajectory planning under safety constraints \cite{da2019collision}.  

While traditional COCO algorithms often rely on computationally expensive projection operations to maintain feasibility, scaling these methods to high-dimensional settings, where projections become intractable, remains a significant challenge. This paper addresses this gap by introducing a projection-free algorithm that achieves state-of-the-art performance in adversarial COCO, matching the regret and constraint violation bounds of projection-based methods while eliminating the need for costly projections.

\begin{table*}[t]
\label{tbl:comparison}
\caption{\small Summary of key results on OCO with adversarial constraints. The projection-free approaches use a linear programming (LP) in each round, with access to either a linear optimization oracle (LOO) or a separation oracle (SO). }
\begin{center}
\begin{small}
\resizebox{\textwidth}{!}{
\begin{tabular}{lcccccr}
\toprule
Reference & Regret  & CCV  &  $\beta$ Range & \makecell{Complexity \\ per Round} & \makecell{Total Calls \\ to LOO/SO}  & Cost Function    \\
\midrule

\multirow{2}{*}{\cite{guo2022online}} & $\C{O}(\sqrt{T})$  &  $\C{O}(T^{3/4})$  & -  &   Conv-OPT & -   & Convex  \\
 & $\C{O}(\log(T))$  &  $\C{O}(\sqrt{T \log(T)})$  & -  &   Conv-OPT  & -   & Strongly Convex \\
 \midrule
\multirow{2}{*}{\cite{yi2023distributed}}  & $\C{O}(T^{\max(\beta,1-\beta)})$  & $\C{O}(T^{1-\beta/2})$ &  (0,1) &   Conv-OPT  & -   & Convex  \\
  & $\C{O}(\log(T))$  &  $\C{O}(\sqrt{T \log(T)})$ & - &   Conv-OPT  & -   & Strongly Convex \\
  \midrule
%\cite{lee2023projection} & $\C{O}(\sqrt{T})$  &  $\C{O}(T^{3/4})$  &   Conv-OPT    & Convex & Stochastic \\
%\cite{jin2024sample} & &  & N/A  & General & Stochastic \\
\multirow{2}{*}{\cite{sinha2024optimal}}   & $\C{O}(\sqrt{T})$       & $\C{O}(\sqrt{T}\log(T))$ & - & Projection & - & Convex    \\
 & $\C{O}(\log(T))$     & $\C{O}(\sqrt{T\log(T)})$ & -  & Projection  & - & Strongly Convex  \\
 \midrule
\multirow{2}{*}{\cite{garber2024projection}} & \multirow{2}{*}{$\C{O}(T^{3/4})$}  &  \multirow{2}{*}{$\C{O}(T^{7/8})$} & - &  Conv-OPT & \multirow{2}{*}{$\C{O}(T)$} & Convex  \\
 &  &   & -  &  LP &  & Strongly Convex  \\
 \midrule
\multirow{4}{*}{This paper}  & $\C{O}({T}^{1-\beta})$       & $\C{O}({T}^{1-\beta}\log(T))$ & (0,1/2] & LP & $\tilde{\C{O}}(T^{2\beta})$  & Convex     \\
& $\C{O}(\sqrt{T})$       & $\C{O}(\sqrt{T}\log(T))$ & 1/2 & LP & $\tilde{\C{O}}(T)$  & Convex     \\
 & $\C{O}(T^{1-\beta}\log(T))$     & $\C{O}(\sqrt{T^{2-\beta}\log(T)})$ & (0,1]  & LP & $\C{O}(T^{2\beta})$  & Strongly Convex  \\  
 & $\C{O}(\log(T))$     & $\C{O}(\sqrt{T\log(T)})$ & 1  & LP & $\C{O}(T^{2})$  & Strongly Convex  \\ 
\bottomrule
\end{tabular}
}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}


We summarize the key related works in Table \ref{tbl:comparison}. We note  that most works require solving a constrained convex
optimization problem on each round (CONV-OPT), or requires Euclidean projection operation on the convex set (Projection). Both operations are computationally complex, rendering these methods impractical for modern large-scale problems. While some projection-free algorithms, such as those based on Frank-Wolfe or linear optimization oracles, avoid projections, they historically suffer from suboptimal regret or CCV guarantees, particularly under adversarial constraints. The state of the art projection free approach for adversarial constraints is studied in \cite{garber2024projection}, where the regret of $\C{O}(T^{3/4})$ and constraint violation of $\C{O}(T^{7/8})$ is shown. These are far from the optimal projection-based result, which raises the question as to whether {\it we can have projection-free algorithms that achieve order-optimal regret and constraint violation of $\tilde{\C{O}}(T^{1/2})$.}


In this paper, we affirmatively answer the above question by presenting a projection-free algorithm for COCO that achieves a regret of \(\mathcal{O}(\sqrt{T})\) and a cumulative constraint violation (CCV) of \(\mathcal{O}(\sqrt{T} \log T)\) with only \(\tilde{\mathcal{O}}({T})\) calls to the separation oracle. This algorithm leverages the concept of an infeasible projections \cite{garber2022new, garber2024projection, pedramfar2024linearizable}, which can be implemented with separation oracles.   

We first develop a projection-free algorithm for OCO based on Online Gradient Descent (OGD) with adaptive step sizes and establish its regret guarantees. While adaptive step sizes do not improve the regret bounds for OCO, they play a crucial role in proving the guarantees for the COCO problem. For COCO, we introduce an approach that constructs a surrogate function and applies the proposed  algorithm for OCO to it.  Furthermore, we extend our results to strongly convex cost functions, demonstrating a regret bound of \(\mathcal{O}(\log T)\) and a CCV of \(\mathcal{O}(\sqrt{T \log T})\). The key contributions of this work are summarized as follows.

\vspace{-.15in}
\begin{enumerate}[leftmargin=*,itemsep=2pt, parsep=0pt]
\item We propose a projection-free  algorithm for COCO, that achieves regret and CCV of $\Tilde{\C{O}}(T^{1/2})$ for convex optimization, which is the same order as that for the case of projection-based methods. Further, the regret improves to $\Tilde{\C{O}}(1)$ for the case where the cost function is strongly convex. The number of calls needed for separation oracle are characterized for these cases (See Table \ref{tbl:comparison}). 
    
\item We propose a projection-free algorithm for OCO, which is based on OGD with adaptive step size. This algorithm uses infeasible projections, which can be implemented with separation oracles. The regret bounds of the algorithm are derived (Theorem \ref{thm:unconstrained-regret}). The results are extended to the case where the cost function is strongly convex (Theorem \ref{thm:unconstrained-strongly}). We note that these adaptive step-sizes were needed to enable our algorithm for COCO to handle adversarial constraints without prior knowledge of the time horizon.



\item For the COCO problem, we propose a projection-free algorithm that uses the above projection-free OCO algorithm on a surrogate function in each iteration. Using the regret bounds on the surrogate function, the regret and CCV results for the COCO problem are derived (Theorem \ref{thm:main} for convex cost function, and Theorem \ref{thm:constrained-strongly} for strongly-convex cost function). We note that our results are optimal while maintaining linear number of calls to the separation oracle for the convex cost function, outperforming all existing literature in the field.

\item This paper used a  blocked version of OGD and uses a trade-off parameter $\beta$ to allow adjustment of algorithm to real-life computational limitations. For OCO, our proposed blocked algorithm (Algorithm~\ref{alg:general}) achieves $\C{O}(T^{1-\beta})$ regret with $\C{O}(T^{2\beta}\op{log}(T))$ oracle calls oracle calls for some $\beta\in[0,\frac{1}{2}]$ for convex case (Corollary~\ref{cor:con}), and $\C{O}(T^{1-\beta}\op{log}(T))$ regret with $\C{O}(T^{2\beta}(\log(T))^{-2})$ oracle calls for some $\beta\in(0,1]$ for strongly convex case (Corollary~\ref{cor:strcon}).

For the COCO in the convex function case, our proposed algorithm  (Algorithm~\ref{alg:constrained}) achieves $\C{O}(T^{1-\beta})$ regret and $\C{O}(T^{1-\beta}\op{log}(T))$ CCV while maintaining $\C{O}(T^{2\beta}\op{log}(T))$ oracle calls for some $\beta\in[0,\frac{1}{2}]$ (Theorem~\ref{thm:main}). For the strongly convex case, Algorithm~\ref{alg:constrained} achieves $\C{O}(T^{1-\beta}\op{log}(T))$ regret and $\C{O}(T^{1-\frac{\beta}{2}}\sqrt{\op{log}(T)})$ CCV while maintaining $\C{O}(T^{2\beta}(\log(T))^{-2})$ oracle calls for some $\beta\in(0,1]$ (Theorem~\ref{thm:constrained-strongly}). The special values of $\beta$ where we get the optimal results for regret and CCV, are also mentioned in Table \ref{tbl:comparison}. Decreasing $\beta$ allows for lower calls to the separation oracle at the expense of increased regret/CCV. 

\end{enumerate}

\vspace{-.2in}

\section{Related Works}



\noindent \textbf{COCO:} COCO problems have been widely studied, since they can handle complex action set \cite{mahdavi2012trading}. Table \ref{tbl:comparison} provides some of the key works for regret and CCV guarantees in the area. \cite{guo2022online} proposed an algorithm that solves a constrained convex optimization problem in each round, and achieves $\C{O}(\sqrt{T})$ regret with $\C{O}({T}^{3/4})$ CCV. This result is a special case of the result in \cite{yi2023distributed}. Recently, \citet{sinha2024optimal} proposed a projection-based algorithm that achieves regret and CCV of $\Tilde{\C{O}}(T^{1/2})$. The results have been further extended to strongly convex cost functions, where all these works show a regret of $\Tilde{\C{O}}(1)$ with a CCV of $\Tilde{\C{O}}(T^{1/2})$. However, performing a constrained convex optimization or doing a orthogonal projection is computationally complex \cite{garber2022new}, and thus we provide a projection-free algorithm in this paper. 


\noindent \textbf{Projection-free Algorithms for OCO:} Frank-Wolfe based approaches have been proposed for OCO, and achieve a regret of $\C{O}(T^{3/4})$ for convex cost function \cite{pmlr-v37-daniely15,hazan2012projection} and $\C{O}(T^{2/3})$ for strongly convex cost functions \cite{kretzu2021revisiting}. These approaches do not scale well with the dimensional spaces due to the use of linear optimization oracle.  \citet{garber2021efficient} proposed the concept of infeasible projection which was implemented with linear optimization oracle to replace the traditional orthogonal projection oracles, and used that for online linear optimization.  \citet{garber2022new} gave an approach to implement infeasible projection via separation oracle, which achieves an optimal regret of $\C{O}(\sqrt{T})$ using online gradient descent based approach with constant step size. Their results for the case where infeasible projection is implemented via linear optimization oracle does not achieve order-optimal regret. This motivates the choice of use of infeasible projection using separation oracle in our work. We also note that \cite{pedramfar2024linearizable} studied a variant of the approach to implement infeasible projection via separation oracle in \cite{garber2022new}, and applied that to online gradient descent with constant step size. The key difference is that the approach of \cite{pedramfar2024linearizable} queries only in the constraint set, which is not necessary in \cite{garber2022new}, which motivates us to use the approach in \cite{pedramfar2024linearizable}. 




\noindent \textbf{Projection-free Algorithms for  COCO:} 
Projection-free algorithms for online learning were proposed in response to the growing prevalence of high-dimensional data, which led to increased computational challenges for projection operators \cite{hazan2012projection, hazan2020faster, garber2022new,lee2023projection}. Recent work in  \cite{garber2024projection} have studied  projection-free algorithms for  COCO, but do not achieve order-optimal regret and CCV (See Table \ref{tbl:comparison}). In contrast, this paper provides the algorithm that achieves order optimal regret and CCV. 





\section{Problem Setup}
\label{sec:prelim}

To begin with, we formally describe the problem that our algorithm aims to solve. Consider an agent is playing a consecutive online game with an adversary. At each step $t$, the agent chooses an action $\x_t$, and the adversary chooses a cost function $f_t$ and $k$ constraint functions $g_{t,i}$, for $i \in [k]$, and reveals them to the agent. Note that the objective of the game is to minimize the cost $f_t(\x_t)$ while satisfying the constraints $g_{t,i}(\x_t)\leq 0, \forall i\in[k]$. We say the time horizon is $T$ if the game stops after $T$ steps.

We say a function $f$ over the action set $\K$ is convex if for all $\x,\y \in \K$, $f(\y) \geq f(\x) + \nabla f(\x)^T(\y-\x)$. We say a function $f$ over the action set $\K$ is $\theta$-strongly convex if for all $\x,\y \in \K$, $f(\y) \geq f(\x) + \nabla f(\x)(\y-\x)+\frac{\theta}{2}\Vert \y-\x \Vert^2$.


For an online algorithm that solves the constrained optimization, there are two key metrics, regret and cumulative constraint violation (CCV). The regret represents the distance in the cumulative cost from the actions chosen by the agent to the optimal action in hindsight. Mathematically, we define the regret to be
$$ \mathtt{Regret}(T)= \sum\limits_{t=1}^T f_t(\x_t) - \sum\limits_{t=1}^T f_t(\x^*) $$
where $\x^*:= \op{argmax}_{\x\in\K}\sum\limits_{t=1}^T f_t(\x)$.
The cumulative constraint violation (CCV) represents the cumulative violation to the constraint, and when there are more than one constraints we choose the one that is violated the most cumulatively. We define CCV to be
$$  \mathtt{CCV}(T) = \max\limits_{i\in[k]} \sum\limits_{t=1}^T (g_{t,i})^+, $$
where $(z)^+ \triangleq \max\{z, 0\}$. 
In each step, we can simplify the constraint by letting $g_t(x)=\max\limits_{i\in [k]}g_{t,i}(x)$. Note that if each constraint condition is satisfied, constraint $g_t$ naturally follows. So we can (without loss of generality) assume that in each step, the adversary only reveals one constraint function, being the one with the maximum value at the action being played. 

Under the given problem setting, we introduce a few assumptions that are fundamental to our proposed algorithm.
At each step $t$, we assume the cost function $f_t$ are either convex or strongly convex, and constraint function $g_t$ are convex.
Functions $f_t$ and $g_t$ are also assumed to be differentiable with their gradients bounded by $M_1$, thus also $M_1$-Lipschitz continuous.
The action set $\K$ is convex, closed and bounded. 
The diameter of the action set is defined as $D \triangleq \max_{\x,\y\in \K}\Vert \x-\y \Vert$.
For a set $\K \subseteq \BB{R}^d$, we define the set of $\alpha\x+(1-\alpha)\y$ for all $\x,\y\in\K$ and $\alpha\in\BB{R}$ as the \textit{affine hull} of $\K$, $\op{aff}(\K)$.
The \textit{relative interior} of $\K$, denoted by $\op{relint}(\K)$, is defined as the set $\{\x\in\K | \exists r > 0, \BB{B}_r(\x) \cap \op{aff}(\K) \subseteq \K \}$.
By definition, for any point $\BF{c} \in \op{relint}(\K)$, there exists a real number $r > 0$ such that $\op{aff}(\K)\cap \BB{B}_r(\BF{c}) \subseteq \K$, where $\BB{B}_r(\BF{c})$ represent the ball of radius $r$ centering at $\BF{c}$.
We use $r_{\K, \BF{c}}$ to denote the largest $r$ satisfying this property.
Given $\BF{c}$ and $r$, for any $\delta \in [0,r)$, referred to as the shrinking parameter, we define the shrunk constrained set $\K_\delta$ as the set $(1-\frac{\delta}{r})\K + \frac{\delta}{r}\BF{c}$.
\emph{For simplicity, throughout this work, we will assume that $\K$, $\BF{c}$ and $r$ are fixed.}
Besides assumption on action set and cost and constraint functions, we highlight a feasibility assumption that ensures the problem is solvable in Assumption~\ref{asm:feas}.
\begin{assumption}
\label{asm:feas}
    We say an action $\x' \in \K$ is feasible if all constraint function values are non-positive, i.e., $g_{t,i}(\x') \leq 0, \forall t,i$.
    We assume the feasible set $\K'$, the set of all feasible actions, is not empty.
\end{assumption}


Next, we formally introduce the concept of infeasible projection oracle.
We say $\y' \in \BB{R}^n$ is an infeasible projection of $\y$ onto $\K$ if for all $\x \in \K, \Vert \y'-\x \Vert \leq \Vert \y-\x \Vert$.
Note that, unlike projection, $\y'$ is not assumed to be in $\K$.
We say an algorithm returns an infeasible projection for set $\K$ if it takes input point $\y$ and return $\y'$ that is an infeasible projection of $\y$.
An infeasible projection could be implemented using a separation oracle~\cite{garber2022new}.
For a convex set $\K \subseteq \BB{R}^n$, a Separation Oracle ($SO_\K$) is an oracle that, given a point $\x \in \BB{R}^n$, either confirms that $\x \in \K$ or else gives a hyperplane separating $\x$ from $\K$.
Such a separating hyperplane could be represented as a vector $\g$ where if $\y \notin \K$, then $\langle \y - \x, \g \rangle > 0$ for all $\x \in \C{K}$.

Here we use a variant of the infeasible projection by separation algorithm of~\cite{garber2022new} described in~\cite{pedramfar2024linearizable}, and included here as Algorithm~\ref{alg:ipso} for completeness.
We use $\C{P}_{\K, \BF{c}, r, \delta}$ to denote this algorithm.
This variant slightly differs from the version described in~\cite{garber2022new} in that it does not require the assumption that $\K$ contains a $d$-dimensional ball.
The key idea of the algorithm is to provide an infeasible projection onto $\K_\delta$ that is contained within $\K$.
This guarantees that the infeasible projection is not too far from the projection, since any point in $\K$ is not too far from $\K_\delta$.
The following result ensures that the algorithm output is an infeasible projection within certain number of iterations (or queries of separation oracle). 
When there is no ambiguity, we simply use $\C{P}$ and drop the subscripts.




\section{Projection Free Algorithm for OCO: BPFAdaOGD}\label{sec:oco}

In order to solve the COCO problem, we will use surrogate functions that combine the objectives and constraints. For this surrogate function optimization, we need a projection-free OCO algorithm, which will be provided in this Section. 


The proposed algorithm is based on Online Gradient Descent with adaptive stepsize (AdaOGD). We note that although \cite{garber2022new} proposed a projection free OGD using an infeasible projection oracle (Algorithm~1, Lemma~4), there are no blocks and the step size were considered to be constant.
However, the guarantees of COCO are obtained with a blocked version that uses adaptive step-sizes, which is why we will analyze the variant of projection-free OGD described in Algorithm~\ref{alg:general}.


The proposed algorithm, Blocked Projection-Free Adaptive OGD (BPFAdaGD), is given in Algorithm~\ref{alg:general}.
Given action set $\K$, time horizon $T$, $\BF{c} \in \op{relint}(\K)$, $r = r_{\K, \BF{c}}$, block size $K$, shrinking parameter $\delta\in[0,r)$, diameter of the action set $D$, an instance of IP-SO algorithm $\C{P}$ implemented by Algorithm~\ref{alg:ipso}, and in the strongly-convex case the strongly-convex parameter $\theta$.
We initiate the program by choosing $\x_1\in\K$, and pass $\K, \BF{c}, r, \delta$ to $\C{P}$ to initiate algorithm instances.
Within each block $m$, there are $K$ iterations, represented by $\C{T}_m$.
At each step $t$, we play action $\x_t$, and observe the cost function $f_t$ adversarially selected and revealed by the adversary, which we use to compute $\nabla_t$, the gradient of $f_t$ at $\x_t$.
After $K$ iterations, we compute the average gradient in the block $m$, $\bar{\nabla}_m=\frac{1}{K} \sum_{t\in\C{T}_m} \nabla_t $, and perform a gradient descent update, $\x_m - \eta_m\bar{\nabla}_m$, with adaptive step-size $\eta_m = \frac{D}{\sqrt{\epsilon + \sum^m_{\tau=1}\Vert \bar{\nabla}_\tau \Vert^2}}$ in the convex case, or $\eta_m = \frac{1}{m \theta}$ in the strongly convex case.
The gradient descent update $\x_m-\eta_m\bn_m$ is passed to the IP-SO instance $\C{P}$ to obtain next action, $\x_{m+1}$.
The following result bounds the regret of Algorithm~\ref{alg:general}. 


\begin{algorithm}[ht]
    \caption{ BPFAdaOGD }  
    \label{alg:general}
    \begin{algorithmic}[1]
        \STATE \textbf{Input:} Action set $\K$, time horizon $T$, block size $K$, $\BF{c} \in \op{relint}(\K)$, $r = r_{\K, \BF{c}}$, shrinking parameter $\delta\in[0,r)$, diameter of the action set $D$, an instance of IP-SO algorithm $\C{P}$ implemented by Algorithm~\ref{alg:ipso}, (in the convex case) constant $\epsilon > 0$, (in the strongly-convex case) strongly-convex parameter $\theta$
        \STATE \textbf{Initialization:} Set $\x_1 \in \K$. Pass $\K, \BF{c}, r, \delta$ to $\C{P}$.
        \FOR{$m \in [T/K]$}
            \FOR{$t \in \C{T}_m$}
                \STATE Play $\x_m$
                \STATE Adversary selects and reveals the cost function $f_t$
                \STATE Compute $\nabla_t$, the gradient of $f_t(\x_m)$
            \ENDFOR
            \STATE Let $\bar{\nabla}_m=\frac{1}{K} \sum_{t\in\C{T}_m} \nabla_t $
            \STATE Set step-size $\eta_m = \frac{D}{\sqrt{\epsilon + \sum^m_{\tau=1}\Vert \bar{\nabla}_\tau \Vert^2}}$ in the convex case and $\eta_m = \frac{1}{m \theta}$ in the strongly convex case.
            \STATE Update $\x_{m+1}=\C{P}(\x_m - \eta_m\bn_m)$
        \ENDFOR
    \end{algorithmic}
\end{algorithm}

First we consider the convex case.
\begin{theorem}
\label{thm:unconstrained-regret}
Assume functions $f_t$ are convex and $G_t$-Lipschitz continuous. 
Then, Algorithm~\ref{alg:general} ensures that the regret is bounded as
\begin{align*}
\mathtt{Regret}(T) &= \sum_{t=1}^T f_t(\x_t) - \sum_{t=1}^T f_t(\x^*) 
\leq \frac{3}{2} D K \sqrt{\sum_{m=1}^{T/K} \Vert \bar{\nabla}_m \Vert ^2} + \frac{\delta}{r} D \sumT G_t,
\end{align*}
where $\delta \in [0,r)$ is the shrinking parameter, $K$ is the block size, $D$ is the diameter of $\K$, and the definition of $\bn_m$ is given in Algorithm~\ref{alg:general} (line 9).
If $\epsilon > 0$ is a constant independent of $T$, then this is achieved with $\C{O}\left( \delta^{-2} \log(T) + T K^{-1} \right)$ calls to a separation oracle.
\end{theorem}

Note that, when block size $K$ is equal to $1$ and step-size is constant, this result reduces to a variant of Lemma~4-(i) in~\cite{garber2022new}.
Also note that the aforementioned lemma allows for the the actions to be outside the constraint set, which is why their bound does not include the term $\frac{\delta}{r} D  \sum_{t=1}^T G_t$.

\begin{corollary}
\label{cor:con}
In Theorem~\ref{thm:unconstrained-regret}, if we further assume that functions are $M_1$-Lipschitz continuous, then we see that
\begin{align*}
\mathtt{Regret}(T) 
&\leq \frac{3}{2} D \sqrt{M_1 K T} + \frac{\delta}{r} D T M_1.
\end{align*}
In particular, if $K = \Theta(T^{1 - 2\beta})$ and $\delta = \Theta(T^{-\beta})$ for some $\beta \in [0, 1/2]$, then $\mathtt{Regret}(T) = \C{O}(T^{1 - \beta})$ with $\C{O}(T^{2\beta}\log(T))$ calls to the separation oracle.
\end{corollary}





We note that better regret bounds are achievable when the cost functions are chosen to be strongly convex. 
Thus, we give Theorem~\ref{thm:unconstrained-strongly} to describe results for strongly convex functions. 
The proof details for Theorem~\ref{thm:unconstrained-strongly} and Corollary~\ref{cor:strcon} can be found in Appendix~\ref{apx:unconstrained-strongly}.


\begin{theorem}
\label{thm:unconstrained-strongly}
Assume functions $f_t$ are $G_t$-Lipschitz continuous and $\theta$-strongly convex for some constant $\theta > 0$. Then Algorithm~\ref{alg:general} ensures that the regret is bounded as
\begin{align*}
\mathtt{Regret}(T) \leq \frac{1}{2 \theta} \sum\limits_{m=1}^{T/K} \sum\limits_{t\in\C{T}_m} \frac{\normdt^2}{m} + \frac{\delta }{r} D \sumT G_t,
\end{align*}
where $\delta$, $K$, and $D$ are defined as in Theorem~\ref{thm:unconstrained-regret}.
Moreover, if we assume all $G_t \leq M_1$ for some constant $M_1 \geq 0$, then
this is achieved with $\C{O}\left( \delta^{-2} + \delta^{-1}\log(T) + T K^{-1} \right)$ calls to the separation oracle.
\end{theorem}

In the strongly-convex case, when block size $K$ is equal to $1$, this result reduces to a variant of Lemma~4-(ii) in~\cite{garber2022new}.
Note that the aforementioned lemma allows for the the actions to be outside the constraint set, which is why their bound does not include the term $\frac{\delta }{r} D \sumT G_t$.

\begin{corollary}
\label{cor:strcon}
    In Theorem~\ref{thm:unconstrained-strongly}, if we assume all $G_t\leq M_1$ for some constant $M_1$, then we have
    \begin{align*}
    \mathtt{Regret}(T) 
    \leq  \frac{M_1^2 K}{2 \theta} (1+\log(\frac{T}{K})) + \frac{\delta D M_1 T}{r}.
    \end{align*}
    In particular, if we choose $\delta=\Theta(T^{-\beta}\op{log}(T))$ and $K=\Theta(T^{1-\beta})$ for some $\beta \in (0, 1]$, then $\mathtt{Regret}(T)= \C{O}(T^{1-\beta}\log(T))$
    with $\C{O}(T^{2\beta}(\log(T))^{-2})$ calls to the separation oracle.
\end{corollary}



\section{Projection-free algorithm for Adversarial COCO: BPFAdaOGD-Sur}\label{sec:pacogd}

In this section, to solve Online Convex Optimization with Adversarial Constraint (Adversarial COCO), we will propose Algorithm~\ref{alg:constrained}, blocked Projection-Free Adaptive OGD with Surrogate function (BPFAdaOGD-Sur), and analyze its regret and CCV guarantees. 
This algorithm creates a surrogate function $\hf_t$ which combines the cost function and the cumulative constraint violation, and applies the OCO algorithm in Section~\ref{sec:oco} on the surrogate function. While Algorithm~\ref{alg:constrained} is given for both convex and strongly convex cost functions, this section studies the convex cost function case, while the strongly-convex cost function will be studied in Section~\ref{sec:strongly-convex}. 


\begin{algorithm}[ht]
\caption{ BPFAdaOGD-Sur }  
\label{alg:constrained}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Action set $\K$, time horizon $T$, $\BF{c} \in \op{relint}(\K)$, $r = r_{\K, \BF{c}}$, shrinking parameter $\delta \in [0,r)$, block size $K$, action set diameter $D$, an instance of BPFAdaOGD (Algorithm~\ref{alg:general}) $\C{A}$, processing parameter $\gamma$, regularization parameter $V$, and Lyapunov function $\Phi$, an instance of IP-SO (Algorithm~\ref{alg:ipso}) $\C{P}$, (in the convex case) constant $\epsilon > 0$, (in the strongly-convex case) strongly-convex parameter $\theta$. 
\STATE \textbf{Initialization:} Pick $\x_1 \in \K$, $Q_0=0$, pass $\K,T,K,\BF{c},r,\delta,D,\C{P}, \epsilon \text{ or } \theta$ to $\C{A}$. 
\FOR{$m \in [T/K]$}
    \FOR{$t \in \C{T}_m$}
        \STATE Play $\x_m$
        \STATE Adversary selects and reveals cost function $f_t$ and constraint function $g_t$
        \STATE Observe cost $f_t(\x_m)$ and constraint violation $(g_t(\x_m))^+$
        \STATE Let $\tft = \gamma f_t$ and $\tgt=\gamma (g_t)^+$
        \STATE Let $Q_t = Q_{t-1}+(g_t(\x_m))^+$
        \STATE Let $\hft = V\tft + \Phi'(Q_t)\tg_t$ and compute $\nabla_t := \nabla \hft(\x_m)$
        % \STATE Pass $\nabla_t$ to Algorithm~\ref{alg:general}
        \STATE Pass $\nabla_t$ to $\C{A}$
    \ENDFOR
    \STATE Get $\x_{m+1}$ from $\C{A}$
\ENDFOR
\end{algorithmic}
\end{algorithm}


For computational convenience, we introduce a processing parameter $\gamma$ and we let $\tf_t = \gamma f_t$, $\tilde{g}_{t,i}=\gamma(g_{t,i})^+$. To track the cumulative constraint violation, we let $Q_t=Q_{t-1}+\tilde{g}_t(\x_t)$, and $Q_0=0$. Since our objective is to make the cumulative constraint violation small, we introduce a potential function $\Phi: \BB{R}^+\mapsto\BB{R}^+$ that is non-decreasing, differentiable, convex, and satisfies $\Phi(0)=0$, and we call $\Phi(\cdot)$ Lyapunov function from here forward. We create the surrogate cost function to be $\hf = V\tf_t + \Phi'(Q_t)\tg_t$ where $V$ is a real number and $\Phi'(Q_t)$ is the gradient of Lyapunov function $\Phi$ at $Q_t$. $V$ is introduced as a regularization parameter to balance between cost and constraint violation.



The algorithm use the inputs: action set $\K$, shrinking parameter $\delta$, block size $K$, diameter of the action set $D$, time horizon $T$, the common Lipschitz continuous constant $M_1$ for the cost function and constraints, an instance of Algorithm~\ref{alg:general} $\C{A}$, processing parameter $\gamma$, regularization parameter $V$, and Lyapunov function $\Phi$. 
We initiate the algorithm by picking the initial action  $\x_1\in\K$, and set $Q_0=0$, and passing the required input to $\C{A}$ to initiate the algorithm instance. 
We break the horizon $T$ into blocks with size $K$. 
For each step $t$ within block $m$, represented by $\C{T}_m$, we play action $\x_m$ determined by the algorithm, and then observe cost $f_t(\x_t)$ and constraint violation $(g_t(\x_t))^+$, where the cost function and the maximum constraint function are chosen and revealed by an adversary. 
We transform them into $\tilde{f}_t = \gamma f, \tilde{g}_t =\gamma (g_t(x_t))^+$, and update the cumulative constraint violation with $Q_t = Q_{t-1}+\tilde{g}_t$. 
We compute the gradient $\nabla_t$ of the surrogate function $\hat{f}_t(x_t):=V\tilde{f}_t + \Phi'(Q_t)\tilde{g}_t$, i.e., $\nabla_t=\nabla \hft$, and pass the gradient information to $\C{A}$.
At the end of the each block, $\C{A}$ returns the action for the next block $\x_{m+1}$. 
The following Theorem provides the regret and the CCV for the proposed algorithm in the convex case. 
\begin{theorem}
\label{thm:main}
    Assume cost functions $f_t$ and constraint functions $g_t$ are convex and $M_1$-Lipschitz continuous.
    With $\beta\in[0,\frac{1}{2}]$, if we choose $\gamma=(M_1R)^{-1}$, $\delta=\Theta(T^{-\beta})$, $K= \Theta(T^{1-2\beta})$, and $\Phi(x)=e^{\lambda x}-1$ where $\lambda=(\frac{2\delta T}{r} + 3\sqrt{2TK})^{-1}$, and $V$ to be a constant independent of T,
    Algorithm~\ref{alg:constrained} ensures that $\mathtt{Regret}$ is bounded as
    % Let $\Phi(x)=e^{\lambda x}-1$, take $\gamma=(M_1R)^{-1}$, $\beta\le 1/2$, $V=\frac{\sqrt{2}}{3c}$, $\lambda=((6\sqrt{2}+2c)T^{1-\beta}+3\sqrt{2}cT^{1-2\beta})^{-1}$, $\delta=cT^{-\beta}$, $K=T^{1-2\beta}$, then the regret is bounded as
    \begin{equation*}
        \mathtt{Regret}(T) =  \C{O}(T^{1-\beta}),
    \end{equation*}
    and $\mathtt{CCV}$ is bounded as
    \begin{equation*}
        \mathtt{CCV}(T) = \C{O}(T^{1-\beta}\log T).
    \end{equation*}    
    with $\C{O}(T^{2\beta}\log(T))$ calls to the separation oracles.
\end{theorem}
\textbf{Remarks:} Note that if we take $\beta=\frac{1}{2}$, we allow optimal result $\sqrt{T}$ for both regret and CCV linear calls to the separation oracles. In this case, there will be no blocking effect because block size $K=1$. 
However, considering the diverse application scenario, we introduce the trade-off parameter $\beta$ to allow adjustment of the algorithm to real-life computational limitations.
When $\beta=\frac{1}{4}$, we have $\C{O}(T^{3/4})$ result for regret and CCV, which shows improvement on CCV and oracle calls compared to \cite{garber2024projection}, with at most $\C{O}(\sqrt{T}\log(T))$ calls to the separation oracles.

\emph{The rest of the section proves this result.}

A summary of the proof is given as follows. We define $\mathtt{R}(\hf) \triangleq \sum_{t=1}^T \hf_t(\x_t) - \sum_{t=1}^T \hf_t(\x^*)$ and $\mathtt{R}(\tf) \triangleq \sum_{t=1}^T \tf_t(\x_t) - \sum_{t=1}^T \tf_t(\x^*)$ where $\x^*=\arg\min_{\x\in\K}\sum_{t=1}^T f_t(\x)$. 
Since we pass gradient information of the surrogate functions $\hat{f}_t$ to an instance of Algorithm~\ref{alg:general} at each iteration and then get updated actions back at each block, we can use Theorem~\ref{thm:unconstrained-regret} to derive bound on $\mathtt{R}(\hf)$ as described in Lemma~\ref{lem:surrogate-regret}. 
Using properties of the Lyapunov function $\Phi(\cdot)$, we can build relationship between $\mathtt{R}(\hf)$ and $\mathtt{R}(\tf)$, thus finding bound on $\mathtt{R}(\tf)$. 
By choosing algorithm input appropriately, and observing that $\mathtt{R}(\tf)=\gamma \mathtt{Regret}(T)$, we have the $\mathtt{regret}$ result in Theorem~\ref{thm:main}. 
For $\mathtt{CCV}$, we bound regret from below and choose parameters appropriately to obtain bound on $Q_T=\gamma \mathtt{CCV}(T)$, thus obtaining $\mathtt{CCV}$ result in Theorem~\ref{thm:main}.

We start with bounding $\mathtt{R}(\hf)$:
\begin{lemma}
\label{lem:surrogate-regret}
Assume surrogate functions $\hf_t$ are convex and $G_t$-Lipschitz, then Algorithm~\ref{alg:constrained} ensures that the term $\mathtt{R}(\hf)$ defined as follows is bounded:
\begin{align*}
    \mathtt{R}(\hf) = \sum_{t=1}^T \hf_t(\x_t) - \sum_{t=1}^T \hf_t(\x^*)
    \leq \frac{3}{2}DK\sqrt{\sum_{m=1}^{T/K} \Vert \bn_m \Vert ^2} + \frac{\delta D}{r} \sumT G_t
\end{align*}
where $\x^*=\arg\min_{\x\in\K}\sum_{t=1}^T f_t(\x)$, $\bn_m$ is defined in Algorithm~\ref{alg:general}(line~9), $D$ is the diameter of the action set $\K$, $K$ is the block size, $\delta \in [0,r)$ is the shrinking parameter.
% \todo{ $\mathtt{Regret}_T(\hf)$ is not a well-defined notion in the literature. The phrasing should change to make it clear that we are defining this.}
\end{lemma}
\begin{proof}
    Let $\x^*=\arg\min_{\x\in\K}\sum_{t=1}^T f_t(\x)$ and $\x'=\arg\min_{\x\in\K}\sum_{t=1}^T \hf_t(\x)$.
    Proof of Lemma~\ref{lem:surrogate-regret} follows from that of Theorem~\ref{thm:unconstrained-regret}, by replacing $\x'$ with $\x^*$. Note that $\hft$ and $\tft$ share the same action set $\K$.
\end{proof}

Next, we aim to bound $\mathtt{R}(\tf)$, using the relationship between $\hft$ and $\tft$, and replacing the $\Vert \bn_m \Vert ^2$ term in Lemma~\ref{lem:surrogate-regret}. The results are provided below in Lemma~\ref{lem:sur2real} with proof.
\begin{lemma}
\label{lem:sur2real}
    Assume $\Phi(\cdot):\BB{R}^+\mapsto\BB{R}^+$ is a non-decreasing, differentiable, and convex potential function satisfies $\Phi(0)=0$. Assume cost functions $f_t$ and constraint function $g_t$ are $M_1$-Lipschitz.
    Algorithm~\ref{alg:constrained} ensures that
    \begin{equation}
    \label{eq:sur2real:1}
        \Phi (Q_T) + V * \mathtt{R}(\tf) \leq \mathtt{R} (\hf)
    \end{equation}
    and
    \begin{equation}
    \label{eq:sur2real:2}
        \mathtt{R} (\hat{f}) \leq \gamma M_1 D (V+\Phi'(Q_T))
        (\frac{\delta T}{r} + \frac{3}{\sqrt{2}}\sqrt{TK})
    \end{equation}
    where $\delta \in [0,r)$ is the shrinking parameter, $K$ is the block size, $D$ is the diameter of $\K$, and the definition of $\bn_m$ is given in Algorithm~\ref{alg:general} (line 9), $\gamma$ is the processing parameter, and $V$ is the regularization parameter.
\end{lemma}
\begin{proof}
    Note that $\hft(\xt)=V\tft(\xt)+\Phi'(\qt)\tgt(\x_t)$, where $V$ is a real number, $\tft=\gamma f_t$, $\tgt=\gamma (g_t)^+$, and $\Phi(\cdot)$ is a non-decreasing, differentiable, convex potential function satisfying $\Phi(0)=0$. By convexity of $\Phi(\cdot)$, we have
    \begin{align*}
    \Phi(\qt) 
    &\leq \Phi(\qtt) + \Phi'(\qt)(\qt-\qtt) 
    = \Phi(\qtt) + \Phi'(\qt)\tgt(\xt).
    \end{align*}
    Rearranging, we have
    \begin{align*}
        \Phi(\qt) - \Phi(\qtt) \leq \Phi'(\qt)\tgt(\xt).
    \end{align*}
    Since $\hft(\x^*)=V\tft(\x^*)+\Phi'(\qt)\tgt(\x^*)$ and $\tgt(\x^*)=\gamma (g_t(\x^*))^+ \leq 0$, plugging in the above inequality, we have
    \begin{align*}
        \hft(\xt) - \hft(\xs) &= V\left(\tft(\xt)-\tft(\xs) \right) + \Phi'(\qt)(\tgt(\x_t)-\tgt(\x^*)) \\ 
        & \geq V\left(\tft(\xt) - \tft(\xs) \right) + \Phi'(\qt)\tgt(\x_t) \\
        & \geq V\left(\tft(\xt) - \tft(\xs) \right) + \Phi(\qt) - \Phi(\qtt).
    \end{align*}    
    Summing the above inequality over $t\in[T]$, with $Q_0=0$ and $\Phi(0)=0$, we have 
    \begin{align*}
        \mathtt{R}(\hf) \geq V*\mathtt{R}(\tf) + \Phi(Q_T),
    \end{align*}
    which gives Equation~\eqref{eq:sur2real:1} in Lemma~\ref{lem:sur2real}.
    
    Since both $f$ and $g$ are assumed to be $M_1$-Lipschitz continuous, we have 
    \begin{align}
    \label{eq:Gt}
         \normdt = \Vert \nabla \hft \Vert 
    \leq V \Vert \nabla \tft \Vert + \Phi'(\qt) \Vert \nabla \tgt \Vert 
    \leq \gamma M_1(V+\Phi'(\qt)).
    \end{align}
    Thus, surrogate functions $\hft$ are $G_t$-Lipschitz continuous, where $G_t = \gamma M_1(V+\Phi'(\qt))$.
    Since $\bn_m=\frac{1}{K} \sum_{t\in\C{T}_m} \nabla_t$, using Cauchy-Schwartz inequality, we have $\Vert \bn_m \Vert ^2 = \frac{1}{K^2} \Vert \sum_{t\in\C{T}_m} \nabla_t \Vert ^2 \leq \frac{1}{K} \sum_{t\in\C{T}_m}\normdt^2 $. Thus,
    \begin{align}
        \label{eq:sqrt-sum-gm}
        \sqrt{\sum_{m=1}^{T/K} \Vert \bn_m \Vert ^2} 
        &\leq \sqrt{\frac{1}{K} \sum_{m=1}^{T/K} \sum_{t\in\C{T}_m}\normdt^2}
        = \sqrt{ \frac{1}{K} \sum_{t=1}^{T} \normdt^2 } \nonumber \\
        &\overset{(a)}\leq \frac{\gamma M_1}{\sqrt{K}}\sqrt{ \sum_{t=1}^{T} (V+\Phi'(\qt))^2 }
        \overset{(b)}\leq \frac{\gamma M_1}{\sqrt{K}}\sqrt{ \sum_{t=1}^{T} 2(V^2+\Phi'(\qt)^2) } \nonumber \\
        &\overset{(c)}\leq \frac{\gamma M_1 \sqrt{2}}{\sqrt{K}} (\sqrt{\sum_{t=1}^{T}V^2}+\sqrt{\sum_{t=1}^{T}\Phi'(\qt)^2})
        \overset{(d)}\leq \frac{\gamma M_1 \sqrt{2T}}{\sqrt{K}} (V + \Phi'(Q_T))
    \end{align}
    where (a) follows from Equation~\eqref{eq:Gt} and the fact that $G_t=\gamma M_1(V+\Phi'(\qt))$, (b) follows from Cauchy-Schwartz inequality, (c) follows from Jensen's inequality, and (d) is because $\Phi'(\cdot)$ is non-decreasing as $\Phi(\cdot)$ is convex and $Q_t$ is non-decreasing, thus $\Phi'(Q_t) \leq \Phi'(Q_T), \forall t\in[T]$.
    By plugging Equation~\eqref{eq:Gt} and Equation~\eqref{eq:sqrt-sum-gm} into Lemma~\ref{lem:surrogate-regret} and rearranging terms, we have Equation~\eqref{eq:sur2real:2} in Lemma~\ref{lem:sur2real}.
\end{proof}

Having necessary lemmas at hand, we now move towards parameter tuning.
If we set $\Phi(Q_T)=e^{\lambda Q_T}-1$ with some real number $\lambda$, we have $\Phi'(Q_T)=\lambda e^{\lambda Q_T}$. Letting $S \triangleq \frac{\delta T}{r} + \frac{3}{\sqrt{2}}\sqrt{TK}$, we have 
\begin{align}
\label{eq:regretTft}
    \mathtt{R}(\tf) & \overset{(a)}{\leq} \frac{\mathtt{R} (\hf)-\Phi (Q_T)}{V} \nonumber\\
    & \overset{(b)}{\leq}
    \frac{\gamma M_1 D (V+\lambda e^{\lambda Q_T})(\frac{\delta T}{r} + \frac{3}{\sqrt{2}}\sqrt{TK})-e^{\lambda Q_T}+1}{V}\nonumber\\
    &\overset{(c)} = \gamma M_1 D S 
    + \frac{e^{\lambda Q_T} \left( \lambda \gamma M_1 D S-1 \right)+1}{V}
\end{align}
where (a) uses Equation~\eqref{eq:sur2real:1}, (b) uses Equation~\eqref{eq:sur2real:2}, and (c) rearranges terms and adopts $S$.


We make a few observation of Equation~\eqref{eq:regretTft}.
Note that if we take $\gamma=(M_1D)^{-1}$, we can offset $\gamma M_1 D$ term entirely.  
Let $\beta\in[0,\frac{1}{2}]$ be some real number.
If we choose $\delta=\Theta(T^{-\beta})$ and $K=\Theta(T^{1-2\beta})$, we have $S = \C{O}(T^{1-\beta})$.
If we choose $\lambda=\frac{1}{2S}$, then $\lambda \gamma M_1 R S -1 = -\frac{1}{2}<0$. 
Thus, based on Equation~\eqref{eq:regretTft}, if $V$ is constant independent of $T$, then we have
\begin{align}
    &\mathtt{R}(\tf) \leq S+\frac{1}{V} = \C{O}(T^{1-\beta}).
\end{align}
Since $\tft=\gamma f_t$, we have $\mathtt{R}(\tf) = \gamma\mathtt{Regret}(T)$. Thus,
\begin{align*}
    \mathtt{Regret}(T) = \gamma^{-1} \mathtt{R}(\tf) = \C{O}(T^{1-\beta}).
\end{align*}

We now take a closer look at the  Cumulative Constraint Violation (CCV). Since $\tft=\gamma f_t$ and $f_t$ is $M_1$-Lipschitz, $\tft$ is $\gamma M_1$-Lipscitz continuous, so $\tft(\xt)-\tft(\xs) \geq -\gamma M_1 (\xt-\xs) \geq -1$. Taking sum over $t\in[T]$, we have
\begin{align}
\label{eq:lowbon4regret}
    \mathtt{R}(\tf)=\sumT(\tft(\xt)-\tft(\xs)) \geq -T
\end{align}
Replacing $\mathtt{R}(\tf)$ in Equation~\eqref{eq:regretTft} with the lower bound in Equation~\eqref{eq:lowbon4regret} and rearranging terms, we have
\begin{align*}
    e^{\lambda Q_T} \leq 
    \frac{VS + 1 + VT }{1-\lambda S}.
\end{align*}
Since $\lambda=\frac{1}{2S}$ and $S = \C{O}(T^{1-\beta})$, we have
\begin{align*}
Q_T &\leq 2S \log\left( 2(S + \frac{1}{V} + T)\right) 
= \C{O}(T^{1-\beta}\log(T))
\end{align*}
Considering that $Q_T=\sumT(\tgt)^+=\sumT\gamma(g_t)^+=\gamma \mathtt{CCV}(T)$, we have
\begin{align*}
    \mathtt{CCV}(T) = \gamma^{-1} Q_T \leq \C{O}(T^{1-\beta}\log T)
\end{align*}

Since calls to the infeasible projection and separation oracles are done in $\C{A}$, we can extend results from Corollary~\ref{cor:con} and derive the oracle calls are bounded as $\C{O}(T^{2\beta}\log(T))$.




\section{Projection-free COCO for Strongly Convex Cost Function}
\label{sec:strongly-convex}

We note that the regret bounds can improve when the cost functions are strongly convex. In this section, we describe the proposed algorithm when the cost functions are strongly convex, and analyze the $\mathtt{Regret}$ and $\mathtt{CCV}$ guarantees for the proposed algorithm. 

Let the cost functions $f_t$ be $\theta$-strongly convex while the constraint functions are considered to be convex, not necessarily strongly convex. Let them all be $M_1$-Lipschitz continuous. 
In this case, we still apply BPFAdaOGD-Sur (Algorithm~\ref{alg:constrained}), but with different parameters. More precisely, we choose \(\gamma = 1\), \(V = \frac{8M_1^2 K \log(Te/K)}{\theta}\), and \(\Phi(\cdot) = (\cdot)^2\). Further, the BPFAdaOGD instance $\C{A}$ uses step size \(\eta_m = \frac{1}{m\theta}\).
Having studied the regret for OCO for strongly convex functions in Section~\ref{sec:oco}, we now use the result to study the regret and CCV for the adversarial COCO problem for strongly convex costs, with similar approach to the proofs in Section~\ref{sec:pacogd}. The results are given in Theorem~\ref{thm:constrained-strongly}.

\begin{theorem}
\label{thm:constrained-strongly}
    Assume cost functions $f_t$ are $\theta$-strongly convex and constraint functions $g_t$ are convex, and they are all $M_1$-Lipschitz continuous.
    With $\beta\in (0,1]$, if we choose $\gamma=1$, $\delta= \Theta( T^{-\beta} \log(T) )$, $K= \Theta(T^{1-\beta})$, $V=\frac{8 M_1^2 K \log(Te/K)}{\theta}$, and $\Phi(\cdot)=(\cdot)^2$,
    Algorithm~\ref{alg:constrained} ensures that
    \begin{align*}
        \mathtt{Regret}(T) &= \C{O}(T^{1-\beta}\log(T)),
        \quad
        \mathtt{CCV}(T) = \C{O}(T^{1-\frac{\beta}{2}}\sqrt{\log(T)}),
    \end{align*}    
    with at most $\C{O}(T^{2\beta}(\log(T))^{-2})$ calls to the separation oracle.
\end{theorem}
\textbf{Remarks:} If we take $\beta=1$, we obtain optimal results $\log(T)$ for $\mathtt{Regret}$ and $\sqrt{T\log(T)}$ for $\mathtt{CCV}$ if permitted $\C{O}(T^2)$ calls to the separation oracle, which matches the optimal result for projection-based algorithm \cite{sinha2024optimal}. If only $\C{O}(T)$ calls permitted, we take $\beta=\frac{1}{2}$, and still have $\C{O}(\sqrt{T})$ regret and $\C{O}(T^{\frac{3}{4}})$ CCV, which showed significant improvement compared to $\C{O}(T^\frac{3}{4})$ regret and $\C{O}(T^{\frac{7}{8}})$ CCV in \cite{garber2024projection}. 


\begin{proof}
    We observe that Equation~\eqref{eq:sur2real:1} still holds when $f_t$ are strongly convex. Similar to proof of Theorem~\ref{thm:main}, we first derive bound on $\mathtt{R}(\hf)$, and move to $\mathtt{R}(\tf)$, before obtaining results on $\mathtt{Regret}(T)$. Then we bound regret from below and choose parameters appropriately to obtain results for $\mathtt{CCV}(T)$.

    If $\gamma=1$, we have $\tft=f_t$, $\tgt=(g_t)^+$. Thus, $\hft(\xt)=V\tft(\xt)+\Phi'(\qt)\tgt=Vf_t+\Phi'(\qt)(g_t)^+$, where $V$ is a positive real number, and $\Phi(\cdot)$ is a Lyapunov function as described in Section~\ref{sec:pacogd}. 
    Since $g_t$ are convex, $(g_t)^+$ are also convex. 
    Since $\Phi(\cdot)$ is non-decreasing, $\Phi'(\qt)$ is non-negative.
    As $f_t$ are $\theta$-strongly convex, we can apply Lemma~\ref{lem:scalar-stronvex} and derive that $\hft(\xt)$ are $V\theta$-strongly convex.
    Since both $f$ and $g$ are assumed to be $M_1$-Lipschitz continuous, Equation~\eqref{eq:Gt} still holds: $\normdt \leq \gamma M_1(V+\Phi'(\qt))$. Thus, surrogate functions $\hft$ are $G_t$-Lipschitz continuous, where $G_t = \gamma M_1(V+\Phi'(\qt))$.
    Similar to how we extend Theorem~\ref{thm:unconstrained-regret} to get Lemma~\ref{lem:surrogate-regret}, we can extend Theorem~\ref{thm:unconstrained-strongly} and get:
    \begin{align*}
        \mathtt{R}(\hf) & \overset{(a)}\leq \frac{1}{2V\theta}\sum_{m=1}^{T/K}\sum_{t\in\C{T}_m} \frac{\normdt^2}{m} + \frac{\delta D}{r} \sumT G_t\\
        & \overset{(b)} \leq \frac{1}{2V\theta} \sum_{m=1}^{T/K}\sum_{t\in\C{T}_m} \frac{M_1^2(V+\Phi'(\qt))^2}{m} + \frac{\delta D}{r} \sumT M_1(V+\Phi'(\qt))  \\
        & \overset{(c)} \leq \frac{VM_1^2K}{\theta}(1+\log(\frac{T}{K})) 
        +\frac{M_1^2}{V\theta } \sum_{m=1}^{T/K}\sum_{t\in\C{T}_m} \frac{(\Phi'(\qt))^2}{m} + \frac{\delta D}{r} M_1 \sumT (V+\Phi'(\qt))
    \end{align*}
    where (a) extends Theorem~\ref{thm:unconstrained-strongly} and uses the fact that $\hft(\xt)$ are $V\theta$-strongly convex, (b) follows from Equation~\eqref{eq:Gt} and the fact that $G_t = \gamma M_1(V+\Phi'(\qt))$, and (c) uses the $1+\log(n)$ bound on sum of harmonic series with $n$ terms.
    Since Equation~\eqref{eq:sur2real:1} still holds, taking $\Phi(\cdot)=(\cdot)^2$, we have
    \begin{align*}
        & Q_T^2+V\mathtt{Regret}(T) \leq \mathtt{R}(\hf) \\
        & \quad \quad \quad \leq \frac{VM_1^2K}{\theta}(1+\log(\frac{T}{K})) 
        +\frac{4M_1^2}{\theta V } \sum_{m=1}^{T/K}\sum_{t\in\C{T}_m} \frac{\qt^2}{m} 
        + \frac{\delta D}{r} M_1 \sumT (V+2\qt).
    \end{align*}
    Since $Q_t$ is non-decreasing, we have $Q_T \geq Q_t \geq 0, \forall t\in[T]$. With the sum of harmonic series $\sumT \frac{1}{t} = \log(Te)$, and $V$ being a positive real number, we have
    \begin{align}
    \label{eq:stronvex-reg}
        \mathtt{Regret}(T) &\leq \frac{M_1^2K}{\theta}(1+\log(\frac{T}{K})) 
        +\frac{4M_1^2KQ_T^2 \log(Te/K)}{\theta V^2} + \frac{\delta D}{r} M_1 T (1+\frac{2Q_T}{V}) - \frac{Q_T^2}{V} \\
        & \leq \frac{M_1^2K}{\theta}(1+\log(\frac{T}{K})) 
        +\frac{4M_1^2KQ_T^2 \log(Te/K)}{\theta V^2} + \frac{\delta D}{r} M_1 T (1+\frac{2Q_T}{V}). \nonumber
    \end{align}
    If we choose $\delta=\Theta(T^{-\beta} \log(T))$ and $K=\Theta(T^{1-\beta})$, where $\beta \in (0,1]$ is a trade-off parameter,we have 
    \begin{align*}
       \mathtt{Regret}(T) = \C{O}(T^{1-\beta}\log(T)),
    \end{align*}
    and hence we prove the result for $\mathtt{Regret}$ in Theorem~\ref{thm:constrained-strongly}.
    
    Next we take a look at $\mathtt{CCV}(T)$. Given that the cost functions are $M_1$-Lipschitz continuous, we have 
    \begin{align*}
        \mathtt{Regret}(T)&=\sumT(f_t(\xt)-f_t(\xs))\\
        &\geq \sumT-M_1\Vert \x_t - \xs \Vert \geq -M_1DT.
    \end{align*}
    Replacing $\mathtt{Regret}(T)$ in Equation~\eqref{eq:stronvex-reg} with the above lower bound, and choose $V=\frac{8M_1^2K\log(Te/K)}{\theta}$, we have,
    \begin{align*}
        \frac{1}{2}Q_T^2-\frac{2\delta D}{r}M_1 T Q_T \leq V(\frac{\delta D}{r} M_1 T + D M_1 T + \frac{M_1K}{\theta}(1+\log(\frac{T}{K}))).
    \end{align*}
    Let $B_T\triangleq V(\frac{\delta D}{r} M_1 T + D M_1 T + \frac{M_1K}{\theta}(1+\log(\frac{T}{K})))$, then we have 
    \begin{align*}
        Q_T^2- \frac{4\delta D}{r} M_1 T Q_T - 2B_T \leq 0.
    \end{align*}
    Note that the left hand side is a quadratic function of $Q_T$.  Solving the inequality, we have
    \begin{align*}
        Q_T \leq \frac{\frac{4\delta D}{r} M_1 T + \sqrt{(\frac{4\delta D}{r} M_1 T)^2 + 8B_T}}{2}.
    \end{align*}
    Since $B_T = \C{O}(T^{2-\beta}\log(T))$, we have $Q_T = \C{O}(T^{1-\frac{\beta}{2}}\sqrt{\log(T)})$, and $\mathtt{CCV}(T)= \sumT (g_t)^+ = Q_T = \C{O}(T^{1-\frac{\beta}{2}}\sqrt{\log(T)})$ for $\theta$-strongly convex cost. 

    Since calls to the infeasible projection and separation oracles are done in $\C{A}$, we can directly extend results from Corollary~\ref{cor:strcon} and bound oracle calls as $\C{O}(T^{2\beta}(\log(T))^{-2}))$.
\end{proof}

\section{Conclusion}

In this paper, we proposed a novel projection-free adaptive online gradient descent (OGD) algorithm for adversarially Constrained Online Convex Optimization (COCO). Our method eliminates the need for costly projection steps while achieving regret and cumulative constraint violation (CCV) bounds that match state-of-the-art projection-based methods. By leveraging a infeasible projection approach and adaptive step-size updates, we achieve order-optimal performance in both convex and strongly convex settings.  




\newpage

\bibliography{ref}

\newpage

\clearpage

\appendix

\section{Useful lemmas}

Here we introduce some technical lemmas that are used in our proofs.

\begin{lemma}[Lemma~4.13 in~\cite{orabona2019modern}]
\label{lem:f}
Let $a_0 \geq 0$ be a real number, $N \geq 1$ be an integer, and $(a_t)_{t = 1}^N$ be a sequence of non-negative real numbers.
For any non-increasing function $f : [0, \infty) \to [0, \infty)$, we have
\begin{align*}
\sum_{t = 1}^N a_t f\left( a_0 + \sum_{i=1}^t a_i \right)
\leq \int_{a_0}^{\sum_{t = 1}^N a_t} f(x) dx.
\end{align*}
\end{lemma}

\begin{lemma}
\label{lem:scalar-stronvex}
    Assume $f$ is $\theta$-strongly convex and $g$ is convex. Let $a$ and $b$ be some non-negative real number. Then $a f + b g$ is $a\theta$-strongly convex.
\end{lemma}
\begin{proof}
    Given $ f $ is $ \theta $-strongly convex, its Hessian satisfies $\nabla^2 f(x) \succeq \theta I$, where $I$ stands for the identity matrix of appropriate dimensions.
    Since $g$ is convex, its Hessian satisfies $\nabla^2 g(x) \succeq 0$.
    Let $h(x) = af(x) + bg(x)$. 
    Then, its Hessian is given by $\nabla^2 h(x) = a \nabla^2 f(x) + b \nabla^2 g(x) \succeq a\theta I$, where $I$ stands for the identity matrix of appropriate dimensions.
    Thus, \( h(x) \) is strongly convex with parameter \( a\theta \).
\end{proof}

\section{ Infeasible Projection via a Separation Oracle}

Here we introduce the details of IP-SO algorithm and an important Lemma~\ref{lem:ipso}.

\begin{algorithm}[ht]
    \caption{ Infeasible Projection via a Separation Oracle (IP-SO) $\C{P}_{\K,\delta}$  \cite{pedramfar2024linearizable}}
    \small
    \label{alg:ipso}
    \begin{algorithmic}[1]
        \STATE \textbf{Input:} Constraint set $\C{K}$, $\BF{c} \in \op{relint}(\K)$, $r = r_{\K, \BF{c}}$, shrinking parameter $\delta < r$, initial point $\y_0$
        \STATE $\y_1 \gets \BF{P}_{\op{aff}(\C{K})}(\y_0)$ 
        \STATE $\y_2 \gets \BF{c} + \frac{\y_1 - \BF{c}}{\max\{1, \|\y_1\|/D \}}$  \hfill\COMMENT{$\y_1$ is projection of $\y_0$ over $\BB{B}_{D}(\BF{c}) \cap \op{aff}(\C{K})$}
        \FOR{$i = 1, 2, \dots$}
            \STATE Call $\op{SO}_{\C{K}}$ with input $\y_i$
            \IF{$\y_i \notin \C{K}$}
                \STATE Set $\g_i$ to be the hyperplane returned by $\op{SO}_{\C{K}}$ \hfill\COMMENT{$\forall \x \in \C{K}$, $\langle \y_i - \x, \g_i \rangle > 0$}
                \STATE $\g'_i \gets \BF{P}_{\op{aff}(\C{K}) - \BF{c}}(\g_i)$
                \STATE Update $\y_{i+1} \gets \y_i - \delta \frac{\g'_i}{\|\g'_i\|} $
            \ELSE
                \STATE Return $\y \gets \y_i$ 
            \ENDIF
        \ENDFOR
    \end{algorithmic}
\end{algorithm}

\begin{lemma}[Lemma 5 in~\cite{pedramfar2024linearizable}]
\label{lem:ipso}
Given a constrained set $\K$, a shrinking parameter $\delta \in [0, r)$ (where $r$ is as previously described), and an initial point $\y_0$, Algorithm~\ref{alg:ipso} stops after at most $\frac{\op{dist}(\y_0,\K_\delta)^2-\op{dist}(\y,\K_\delta)^2}{\delta^2}+1$ iterations and returns $\y \in \K$ such that for all $\x \in \K_\delta$, we have $\Vert \y-\x \Vert \leq \Vert \y_0 - \x \Vert$.
\end{lemma}

\section{Proof of Theorem~\ref{thm:unconstrained-regret}}
\label{apx:unconstrained-regret}


\begin{proof}
Let $\xs\in \op{argmin}_{\x\in\K}\sum_{t=1}^T f_t(\x)$ and $\txs=(1-\frac{\delta}{r})\xs+\frac{\delta}{r}{\BF{c}} \in \K_\delta$.
Thus, the regret of the proposed algorithm is given as
\begin{align}
\label{eq:decomposeRegret}
\mathtt{Regret}(T) &= \sumT \left( f_t(\x_t) - f_t(\xs) \right) 
= \sumT \left( f_t(\x_t) - f_t(\txs) \right) + \sumT \left( f_t(\txs) - f_t(\xs) \right).
\end{align}
We bound the two terms separately.

First we consider $\sum_{t=1}^T \left( f_t(\x_t) - f_t(\txs) \right)$.
From Lemma~\ref{lem:ipso}, we have for $\txs \in \K_\delta$,
\begin{align*}
\Vert \x_{m+1}-\txs \Vert ^2 & \leq \Vert \x_m - \eta_m \bn_m -\txs \Vert ^2 
\leq \Vert \x_m - \txs \Vert ^2 + \eta_m^2 \Vert \bn_m \Vert^2 - 2\eta_m \langle \bn_m, \x_m - \txs \rangle.
\end{align*}
Rearranging, we have
\begin{align}
\langle \bn_m, \x_m - \txs \rangle 
&\leq \frac{1}{2\eta_m} (\Vert \x_m - \txs \Vert ^2- \Vert \x_{m+1}-\txs \Vert ^2) 
+ \frac{\eta_m}{2} \Vert \bn_m \Vert^2
\end{align}
Summing this over $m$ from $1$ to $T/K$, we have
\begin{align*}
\sum\limits_{m=1}^{T/K} \langle \bn_m , \x_m - \txs \rangle  
&\leq \frac{\Vert \x_1 - \txs \Vert ^2}{2\eta_1} 
+ \sum\limits_{m=2}^{T/K} (\frac{1}{2\eta_m}-\frac{1}{2\eta_{m-1}}) \Vert \x_m - \txs \Vert ^2 
+ \sum\limits_{m=1}^{T/K} \frac{\eta_m}{2} \Vert \bn_m \Vert^2 \\
&\leq \frac{D^2}{2\eta_1} 
+ D^2 \sum\limits_{m=2}^{T/K} \left| \frac{1}{2\eta_m}-\frac{1}{2\eta_{m-1}} \right| 
+ \sum\limits_{m=1}^{T/K} \frac{\eta_m}{2} \Vert \bn_m \Vert^2 \\
&= \frac{D^2}{2\eta_1} 
+ D^2 \sum\limits_{m=2}^{T/K} \left( \frac{1}{2\eta_m}-\frac{1}{2\eta_{m-1}} \right) 
+ \sum\limits_{m=1}^{T/K} \frac{\eta_m}{2} \Vert \bn_m \Vert^2 \\
&= \frac{D^2}{2\eta_{T/K}}
+ \sum\limits_{m=1}^{T/K} \frac{\eta_m}{2} \Vert \bn_m \Vert^2,
\end{align*}
where we used the facts that $\Vert \x_m - \txs \Vert \leq D$ and that $\eta_m$ is non-increasing.
By the convexity of $f_t$, we have $f_t (\x_t) - f_t(\txs) \leq \langle \nabla_t, \x_t - \z \rangle$. Summing this over $t$, we have
\begin{align}
\label{eq:xt2txs}
\sumT f_t (\x_t) - \sumT f_t (\txs) 
&\leq \sumT \langle \nabla_t, \x_t - \txs \rangle \nonumber \\
&= \sum\limits_{m=1}^{T/K} \sum\limits_{t\in\C{T}_m} \langle \nabla_t, \x_t - \txs \rangle \nonumber\\
&= \sum\limits_{m=1}^{T/K} K \langle \bn_m, \x_m - \txs \rangle \nonumber \\
&\quad \leq  \frac{K D^2}{2\eta_{T/K}} + \frac{K}{2} \sum\limits_{m=1}^{T/K} \eta_m \normdm^2.
\end{align}
Note that if $\eta_t$ is chosen to be a constant, $\eta_m = \frac{D}{\sqrt{\sum_{m=1}^{T/K} \Vert \bn_m \Vert^2}}$ minimizes the right hand side. However, this step size could not be chosen since at a given $m$, the future $\bn_m$'s are not known. 
Thus, we use adaptive step size in this work, where $\eta_m = \frac{D}{\sqrt{\epsilon + \sum^m_{\tau=1}\Vert \bn_\tau \Vert^2}}$.
Using this, Equation~\eqref{eq:xt2txs} is further bounded as 
\begin{align*}
\sumT f_t (\x_t) - \sumT f_t (\txs) 
&\leq \frac{K D^2}{2 \eta_{T/K}} + \frac{K}{2} \sum\limits_{m=1}^{T/K} \eta_m \normdm^2 \nonumber \\
& \leq \frac{K D}{2}\sqrt{\sum\limits_{m=1}^{T/K}\Vert \bn_m \Vert^2} 
    + \frac{K}{2} \sum\limits_{m=1}^{T/K} \frac{D \normdm^2}{\sqrt{\epsilon + \sum^m_{\tau=1}\Vert \bn_\tau \Vert^2}}. 
\end{align*}
By applying Lemma~\ref{lem:f} to $f(x) = 1/\sqrt{x}$, $a_0 = \epsilon$, $N = T/K$, and $a_m = \Vert \bn_m \Vert^2$, we see that 
\begin{align*}
\sum_{m=1}^{T/K} \frac{\normdm^2}{\sqrt{\epsilon + \sum^m_{\tau=1}\Vert \bn_\tau \Vert^2}} 
&= \sum_{m=1}^{T/K} a_m f\left( a_0 + \sum^m_{\tau=1} a_\tau \right) \\
&\leq \int_{a_0}^{\sum_{t = \tau}^{T/K} a_\tau} f(x) dx \\
&= 2 \sqrt{\sum_{m=1}^{T/K} \|\bn_m\|^2} - 2 \sqrt{\epsilon} \\
&\leq 2 \sqrt{\sum_{m=1}^{T/K} \|\bn_m\|^2}.
\end{align*}
Thus
\begin{align}
\label{eq:xt2txs1}
\sumT f_t (\x_t) - \sumT f_t (\txs) 
&\leq \frac{3 K D}{2}\sqrt{\sum\limits_{m=1}^{T/K}\Vert \bn_m \Vert^2}.
\end{align}

On the other hand, the expression $\sumT\left(f_t(\txs)-f_t(\xs)\right)$ may be bounded as
\begin{align}
\label{eq:txs2xs}
\sumT\left(f_t(\txs)-f_t(\xs)\right)
&\overset{(a)}{\leq} \sumT G_t \Vert \txs-\xs \Vert 
\overset{(b)}{\leq} \frac{\delta}{r} D \sumT G_t,
\end{align}
where (a) is because $f_t$ are $G_t$-Lipschitz continuous, and (b) is due to the fact that 
\begin{align*}
\Vert \txs-\xs \Vert= \Vert (1-\frac{\delta}{r})\xs+\frac{\delta}{r}\BF{c}-\xs \Vert = \frac{\delta}{r}\Vert  \xs - \BF{c} \Vert \leq \frac{\delta}{r} D.
\end{align*}
The claim now follows by using the bounds in \eqref{eq:xt2txs1} and \eqref{eq:txs2xs} together with \eqref{eq:decomposeRegret}.



Next we examine the number of calls to the separation oracles through the infeasible projection algorithm.
Using triangle inequality, $\forall \y, \y_0 \in \K$, we have
\begin{align*}
\op{dist}(\y_0,\K_\delta)
&= \min_{\z \in \K_\delta} \| \y_0 - \z \|
\leq \min_{\z \in \K_\delta} \left( \| \y - \z \| + \| \y  - \y_0 \| \right) 
= \op{dist}(\y,\K_\delta) + \| \y  - \y_0 \|.
\end{align*}
Therefore
\begin{align*}
\op{dist}(\y_0,\K_\delta)^2 - \op{dist}(\y,\K_\delta)^2
&\leq \left( \op{dist}(\y,\K_\delta) + \| \y  - \y_0 \| \right)^2 - \op{dist}(\y,\K_\delta)^2 \\
&= \| \y  - \y_0 \|^2 + 2 \| \y  - \y_0 \| \op{dist}(\y,\K_\delta) \\
&\leq \| \y  - \y_0 \|^2 + \frac{2 D \delta}{r} \| \y  - \y_0 \|,
\end{align*}
where the last inequality follows from that fact that, for all $\z \in \K$, we have $\tilde{\z} \triangleq (1-\frac{\delta}{r})\z+\frac{\delta}{r}{\BF{c}} \in \K_\delta$ and therefore
\begin{align*}
\op{dist}(\z,\K_\delta)
\leq \| \z - \tilde{\z} \|
= \| \z - (1-\frac{\delta}{r})\z-\frac{\delta}{r}{\BF{c}} \|
= \frac{\delta}{r} \| \z- \BF{c} \|
\leq \frac{D \delta}{r}.
\end{align*}
Thus the number of calls in Algorithm~\ref{alg:general} is bounded by
\begin{align}
\label{eq:oracle-calls}
&\sum_{m = 1}^{T/K - 1}\left( \frac{\op{dist}(\x_m - \eta_m \bn_m,\K_\delta)^2-\op{dist}(\x_{m+1},\K_\delta)^2 }{\delta^2}+1 \right) \nonumber \\
&\qquad= \frac{1}{\delta^2} \sum_{m = 1}^{T/K - 1}\left( \op{dist}(\x_m - \eta_m \bn_m,\K_\delta)^2-\op{dist}(\x_{m+1},\K_\delta)^2 \right)
+ \frac{T}{K} - 1 \nonumber \\
&\qquad\leq \frac{1}{\delta^2} \sum_{m = 1}^{T/K - 1}\left( \eta_m^2 \| \bn_m \|^2 + \frac{2 D \delta}{r} \eta_m \| \bn_m \| + \op{dist}(\x_m, \K_\delta)^2-\op{dist}(\x_{m+1},\K_\delta)^2 \right)
+ \frac{T}{K} \nonumber \\
&\qquad= \frac{1}{\delta^2} \sum_{m = 1}^{T/K - 1} \left( \eta_m^2 \| \bn_m \|^2 + \frac{2 D \delta}{r} \eta_m \| \bn_m \| \right)
+ \frac{1}{\delta^2} \left( \op{dist}(\x_1,\K_\delta)^2-\op{dist}(\x_{T/K},\K_\delta)^2 \right)
+ \frac{T}{K} \nonumber \\
&\qquad\leq \frac{1}{\delta^2} \sum_{m = 1}^{T/K} \left( \eta_m^2 \| \bn_m \|^2 + \frac{2 D \delta}{r} \eta_m \| \bn_m \| \right)
+ \frac{D}{r \delta} 
+ \frac{T}{K},
\end{align}
where we used the fact that $\op{dist}(\x_1,\K_\delta) \leq \frac{D \delta}{r}$ and that $\left( \eta_m^2 \| \nabla_m \|^2 + \frac{2 D \delta}{r} \eta_m \| \nabla_m \| \right)$ is positive when $m=\frac{T}{K}$ in the last inequality.

Using Lemma~\ref{lem:f} for $f(x) = 1/x$, $N = T/K$, $a_0 = \epsilon$, and $a_i = \| \bn_i \|^2$, we see that
\begin{align*}
\sum_{m = 1}^{T/K} \frac{\| \bn_m \|^2}{\epsilon + \sum_{\tau=1}^{m}\| \bn_m \|^2} 
&= \sum_{m = 1}^{T/K} a_m f\left( a_0 + \sum_{\tau=1}^{m} a_\tau \right) \\
&\leq \int_{a_0}^{\sum_{m = 1}^{T/K} a_m} f(x) dx \\
&= \log\left( \sum_{m = 1}^{T/K} \| \bn_m \|^2 \right) - \log(\epsilon) \\
&= \log\left( \frac{1}{\epsilon} \sum_{m = 1}^{T/K} \| \bn_m \|^2 \right).
\end{align*}
Therefore
\begin{align*}
\sum_{m = 1}^{T/K} \eta_m^2 \| \bn_m \|^2 
&= D^2 \sum_{m = 1}^{T/K} \frac{\| \bn_m \|^2}{\epsilon + \sum_{\tau=1}^{m}\| \bn_m \|^2} 
\leq D^2\log\left( \frac{1}{\epsilon} \sum_{m = 1}^{T/K} \| \bn_m \|^2 \right) 
\leq 2D^2\log\left( \frac{M_1 T}{\epsilon K} \right),
\end{align*}
where we used the fact that $\bn_m=\frac{1}{K}\sum_{t\in\C{T}_m}\nabla_t$ is bounded by $M_1$.
Thus, using CauchySchwarz inequality, we see that
\begin{align*}
\left( \sum_{m = 1}^{T/K} \eta_m \| \bn_m \| \right)^2
&\leq 
\left( \sum_{m = 1}^{T/K} 1 \right)
\left( \sum_{m = 1}^{T/K} \eta_m^2 \| \bn_m \|^2 \right) 
\leq \frac{2D^2T}{K} \log\left( \frac{M_1 T}{\epsilon K} \right).
\end{align*}
Therefore, the number of calls is bounded by
\begin{align*}
&\frac{1}{\delta^2} \sum_{m = 1}^{T/K - 1} \left( \eta_m^2 \| \nabla_m \|^2 + \frac{2 D \delta}{r} \eta_m \| \nabla_m \| \right)
+ \frac{D}{r \delta} 
+ \frac{T}{K} \\
&\qquad\qquad\leq \frac{2D^2}{\delta^2} \log\left( \frac{M_1 T}{\epsilon K} \right)
+ \frac{2 \sqrt{2} D^2}{r \delta} \sqrt{\frac{T}{K} \log\left( \frac{M_1 T}{\epsilon K} \right)}
+ \frac{D}{r \delta} 
+ \frac{T}{K} \\
&\qquad\qquad= \C{O}\left( \delta^{-2} \log(T) 
+ (\delta^{-2} \log(T))^{1/2} (T K^{-1})^{1/2}
+ \delta^{-1}
+ T K^{-1} \right) \\
&\qquad\qquad= \C{O}\left( \delta^{-2} \log(T) 
+ T K^{-1} \right),
\end{align*}
where we used the fact that $\epsilon$ is a constant independent of $T$ and $\sqrt{a b} = \C{O}(a + b)$ in the last equality.
\end{proof}


\section{Proof of Theorem~\ref{thm:unconstrained-strongly}}
\label{apx:unconstrained-strongly}

\begin{proof}
Let $\xs\in \op{argmin}_{\x\in\K}\sum_{t=1}^T f_t(\x)$ and $\txs=(1-\frac{\delta}{r})\xs+\frac{\delta}{r}{\BF{c}} \in \K_\delta$.
We may decompose the regret in the same manner as Equation~\eqref{eq:decomposeRegret} to see that
\begin{align*}
\mathtt{Regret}(T) &= \sumT \left( f_t(\x_t) - f_t(\xs) \right) 
= \sumT \left( f_t(\x_t) - f_t(\txs) \right) + \sumT \left( f_t(\txs) - f_t(\xs) \right).
\end{align*}
Since we assume $f_t$ to be $G_t$-Lipschitz continuous, similar to Equation~\eqref{eq:txs2xs}, we have
\begin{align}
\label{eq:strconv-lip-cons}
    \sumT \left( f_t(\txs) - f_t(\xs) \right) \leq \frac{\delta D}{r} \sumT G_t.
\end{align}
The infeasible projection operator ensures that $\Vert \x_{t+1}-\txs \Vert \leq \Vert \y_t - \txs \Vert$, extending Theorem 4.1 by \cite{hazan2007adaptive}, we have
\begin{align*}
    \sumT \left( f_t(\x_t) - f_t(\txs) \right) 
    \leq \frac{1}{2} \sum\limits_{m=1}^{T/K} \sum\limits_{t\in\C{T}_m} \frac{G_t^2}{m \theta}
\end{align*}
Thus, we conclude that 
\begin{align*}
    \mathtt{Regret}(T) \leq \frac{1}{2 \theta} \sum\limits_{m=1}^{T/K} \sum\limits_{t\in\C{T}_m} \frac{G_t^2}{m} + \frac{\delta D}{r} \sumT G_t
\end{align*}
Note that if we assume all $G_t\leq M_1$, thus all functions are $M_1$-Lipschitz, then 
\begin{align*}
    \mathtt{Regret}(T) &\leq \frac{M_1^2 K}{2 \theta} \sum\limits_{m=1}^{T/K} \frac{1}{m} + \frac{\delta D M_1 T}{r}  \\
    &\leq \frac{M_1^2 K}{2 \theta} (1+\log(\frac{T}{K})) + \frac{\delta D M_1 T}{r}
\end{align*}
where the last is due to the sum of harmonic series. 
If $\delta = \Theta ( T^{-\beta}\op{log}(T))$, $K=\Theta(T^{1-\beta})$, then $\mathtt{Regret}(T)=\C{O}(T^{1-\beta}\log(T))$.


Next we examine the number of calls to the separation oracles. Similar to the convex case, the number of calls to the separation oracle is bounded by Equation~\eqref{eq:oracle-calls}:
\begin{align*}
&\sum_{m = 1}^{T/K - 1}\left( \frac{\op{dist}(\x_m - \eta_m \bn_m,\K_\delta)^2-\op{dist}(\x_{m+1},\K_\delta)^2 }{\delta^2}+1 \right) \nonumber \\
&\qquad\leq \frac{1}{\delta^2} \sum_{m = 1}^{T/K} \left( \eta_m^2 \| \bn_m \|^2 + \frac{2 D \delta}{r} \eta_m \| \bn_m \| \right)
+ \frac{D}{r \delta} 
+ \frac{T}{K},
\end{align*}
but for the strongly convex functions, $\eta_m=\frac{1}{m\theta}$. Thus, if we assume all Lipschitz parameters are bounded by $M_1$, then we have
\begin{align*}
    \sum_{m = 1}^{T/K} \eta_m^2 \| \bn_m \|^2 \leq \frac{M_1^2}{\theta^2} \sum_{m = 1}^{T/K} \frac{1}{m^2} \leq \frac{2 M_1^2 }{\theta^2}
\end{align*}
where the last inequality follows from the fact that $\sum_{i = 1}^\infty i^{-2} < 2$.
On the other hand
\begin{align*}
    \sum_{m = 1}^{T/K} \eta_m \| \bn_m \| 
    \leq \frac{M_1}{\theta} \sum_{m = 1}^{T/K} \frac{1}{m} 
    \leq \frac{M_1}{\theta}\left( \log\left( \frac{T}{K} \right) + 1 \right)
\end{align*}
where the last inequality follows from the fact that $\sum_{i = 1}^N i^{-1} < \log(N) + 1$.

Combining the results, we see that if all functions are $M_1$-Lipchitz, we have the number of calls to the separation oracles may be bounded as 
\begin{align*}
&\sum_{m = 1}^{T/K - 1}\left( \frac{\op{dist}(\x_m - \eta_m \bn_m,\K_\delta)^2-\op{dist}(\x_{m+1},\K_\delta)^2 }{\delta^2}+1 \right) \nonumber \\
&\qquad\leq \frac{1}{\delta^2}  \left( \frac{2 M_1^2}{\theta^2} +  \frac{2 D \delta}{r} \frac{M_1}{\theta}\left( \log\left( \frac{T}{K} \right) + 1 \right) \right)
+ \frac{D}{r \delta} 
+ \frac{T}{K} \\
&\qquad = \C{O}\left( \delta^{-2}
+ \delta^{-1}\log(T)
+ \delta^{-1}
+ T K^{-1} \right) \\
&\qquad = \C{O}\left( \delta^{-2}
+ \delta^{-1}\log(T)
+ T K^{-1} \right).
\end{align*}

\end{proof}



\end{document}

