\section{Conclusion}
\label{sec:conclusion}

%Neste trabalho nós apresentamos uma nova abordagem para a geração de rewards intrínsecas. Nós demonstramos que o nosso agente modular e atencional puramente intrínseco é capaz de jogar diversos jogos de Atari sem nenhuma reward externa. Nós mostramos que a estrutura modular guiada por atenção melhora o aprendizado e a generalização de modelos intrínsecos sem a necessidade de modelos inversos que trazem complexidade ao treinamento. A nossa abordagem é mais simples que as apresentadas na literatura e demonstrou um alinhamento maior das ações do agente com as recompensas externas dadas pelo ambiente. O nosso agente aprende a realizar comportamentos complexos mais rápido durante o treinamento e generaliza melhor nos ambientes de teste.

%In this work, we present a new approach to generating intrinsic rewards. We've demonstrated that our purely intrinsic attentional and modular agent can play multiple Atari games without external rewards. We show that the attention-driven modular framework improves the learning and generalization of intrinsic models without the need for inverse models that add complexity to training. Our approach is simpler and demonstrates a greater alignment of the agent's actions with the external rewards given by the environment. Our agent learns to perform complex behaviors faster during training and generalizes better in test environments.

In this work, we present a new approach to generating intrinsic rewards. We have demonstrated that our purely intrinsic attentional and modular agent can play multiple Atari games without external rewards. Our agent learns to perform complex behaviors faster during training. Our approach is simpler, demonstrating a greater alignment of the agent's actions with the extrinsic rewards given by the environment. 

%Nós obtivemos resultados superiores ao estado da arte da área em $90\%$ dos jogos testados. Observamos que nossa abordagem obteve resultados surpreendentes em ambientes altamente movimentados, como o Atlantis, Asterix, Assault e Riverraid que exigem comportamentos altamente reativos do agente. Acreditamos que a representação do mundo guiada por atenção contribuiu significativamente para a geração de uma representação concisa e eficiente capaz de fornecer para o agente apenas o que é importante para a realização de uma determinada ação facilitando assim o aprendizado. Uma limitação observada na nossa abordagem é o desempenho limitado em ambientes altamente esparsos, como o Pitfall. Nós acreditamos que tal resultado se deve a baixa quantidade de rollouts usada nos experimentos para este tipo de ambiente. Como trabalhos futuros nós pretendemos investigar mais a fundo o comportamento do nosso agente em ambientes altamente esparsos assim como ambientes mais realistas a fim de investigar como comportamentos cognitivos emergem em agentes complexos imersos em ambiente realistas.

As a result, we obtained results superior to the state-of-the-art in $90\%$ of the tested games. Our approach has had surprising results in highly dynamic environments, such as Atlantis, Asterix, Assault, and Riverraid, that require highly reactive agent behaviors. The attention and modular structure contributed significantly to a concise and efficient world representation, providing the agent with only what is essential for the current action. However, our approach has limited performance in highly sparse environments such as Pitfall. This result is due to the low number of rollouts used in the experiments for this environment. Future work will address how cognitive behaviours emerge in complex humanoids to object manipulation tasks in highly sparse and realistic environments. 