\section{Introduction}
\label{sec:introduction}

% apresentar o que é o reinforcement learning, apresentar os problemas com o RL e apresentar o Intrinsic

%Reinforcement Learning (RL) consists of an agent who explores the environment by choosing actions to maximize uma reward externa, também chamada de reward extrínseca. Em muitos cenários estas rewards são projetadas facilmente, porém no mundo real elas são escassas e até mesmo inexistentes. Isso é um problema, pois agentes artificiais verdadeiramente autônomos devem ser capazes de atuar em ambientes complexos sem a necessidade de pré-programar rewards. Essa capacidade ainda está além dos agentes mais avançados da literatura. Em contraste, as crianças exibem uma capacidade surpreendente para explorar novos ambientes, atender objetos, e envolver-se fisicamente com o mundo externo criando eventos que são novos e excitantes para elas [X]. Tal comportamento é considerado um processo de aprendizado auto-supervisionado guiado por motivações internas. Inspirado por este fenômeno, os teóricos do RL peceberam que outros aspectos além da reward extrínseca precisam ser levados em conta para a construção de agentes verdadeiramente inteligentes.

A prototypical Reinforcement Learning (RL) problem consists of an agent exploring the environment by choosing actions to maximize external rewards, also called extrinsic rewards. In many scenarios, these rewards are efficiently designed, but in the real world, they are scarce or non-existent. Nevertheless, autonomous artificial agents must be able to operate in complex environments without the need to pre-program rewards. This capability is still beyond the most advanced agents in the literature. In contrast, children exhibit a surprising ability to explore new environments, attend to objects, and physically engage with the outside world by creating new and exciting events \cite{haber2018learning}. Such behavior is considered a self-supervised learning process guided by internal motivations. Inspired by this phenomenon, RL theorists realized that other aspects than extrinsic reward must be considered for constructing intelligent agents.


\begin{figure}[htbp]
  \centering

   \includegraphics[width=0.5\linewidth]{meu_modelo_simples.pdf}

   \caption{Our Intrinsically-motivated agent architecture. Our approach has two modules: predictive world model and policy network. The predictive world model generates intrinsic motivation rewards using attention and modular structures. At the same time, the policy network learns a policy to execute actions in the environment.}
   \label{fig:meu_modelo_simples}
\end{figure}


Evidence suggests that intrinsic motivation is vital to guide intellectual development \cite{barto2013intrinsic}. While extrinsic motivation implies an agent behaving to achieve goals by receiving external rewards, such as money or a prize, intrinsic motivation implies that the agent can reach goals through internal motivators, such as curiosity, enjoyment, and learning. The idea of intrinsic rewards originates in the work of Robert White \cite{white1959motivation} and D. E. Berlyne \cite{berlyne1966curiosity}. They criticize Freud’s work and Hullian’s view in which motivation is reduced only to drives related to biological needs (e.g., food). For example, the motivation for acquiring competence is not solely derived from energy sources conceptualized as drivers or instincts. Nevertheless, notions of novelty, complexity, and surprise are also drivers that motivate human behavior.


%The intrinsic motivated RL is problem-independent and allows open-ended learning by acquiring skills hierarchies. In this sense, the need to redesign the core motivational system should be entirely unnecessary [143]. All reward is internal, which means the agent only make decisions that affect its internal environment, then such decisions are reflected in the external environment by actions [9]. Segundo, Oudeyer et al. [X] existem sistemas intrínsecos baseados em conhecimento e baseados em competências. O primeiro requer a construção de algum tipo de modelo preditivo do mundo de modo que funções de quebra de expectativa do modelo estimulam a escolha de ações que resultam em novos estados, maximizando o aprendizado. Esse ciclo se repete numa espécie de auto-curriculum learning à medida que os limites de previsão do modelo interno do agente são ultrapassados e o que costumava ser novidade se torna velho e o ciclo recomeça. Enquanto que o segundo é baseado em medidas de competência que um agente tem para alcançar objetivos autodeterminados. 

%Thus, intrinsic motivated RL is problem-independent and allows open-ended learning by acquiring skills hierarchies. In this sense, redesigning the core motivational system should be unnecessary \cite{singh2010intrinsically}. Rewards of this type are built based on the agent's internal state without considering possible rewards from the external world. Such decisions are reflected later in the external environment by actions \cite{barto2013intrinsic}. According to Oudeyer et al. \cite{oudeyer2009intrinsic}, intrinsic systems can be either knowledge-based or competency-based. Knowledge-based systems require building a predictive world model. When the world model's expectation is broken, it stimulates the learning of new actions that results in new states to explore, similarly to self-curriculum learning. On the other hand, competency-based systems measure the agent's competence to achieve self-determined goals.

%Intrinsic motivated RL has led to state-of-the-art results in Agent57 [7]: Deep Mind’s most successful Atari player so far. In Agent57, intrinsic motivation rewards provide more dense internal rewards for novelty-seeking behaviors, encouraging the agent to explore and visit as many states as possible. Apesar de avanços significativos na área, ainda é difícil modelar agentes intrínsecos que apresentem alto grau de generalização para situações não vistas e que sejam capazes de reutilizar as habilidades aprendidas. A nossa hipótese é que os modelos utilizados ainda não capturam coerentemente a estrutura composicional do mundo. Além disso, os modelos baseados em conhecimento requerem estruturas dinâmicas de modelo inverso que adicionam complexidade e instabilidade nos treinamentos pelas recorrentes mudanças de distribuição. A nossa abordagem proprõe o uso de estruturas modulares e hierárquicas guiadas por atenção através de conexões bottom-up e top-down. Deste modo garantimos a geração de vetores de expectativa com maior flexibilidade evitando a necessidade de modelos inversos. O sistema atencional distribuído por toda a arquitetura garante a geração de vetores de expectativa que sejam úteis para o agente. Além disso, a estrutura captura melhor os aspectos físicos do mundo e garante uma maior generalização para o agente em situações não vistas. 

%Intrinsic-motivated RL has led to state-of-the-art results in Agent57 \cite{badia2020agent57}: Deep Mind’s most successful Atari player. In Agent57, intrinsic motivation rewards provide more dense internal rewards for novelty-seeking behaviors, encouraging the agent to explore and visit as many states as possible. Despite significant advances in the field, many cognition elements still need to be addressed to build intrinsically motivated agents. According to Lecun \cite{lecun2022path}, we are equipped with an internal modular and hierarchical world that says what is probable, plausible, and impossible. Such a model, combined with simple behaviors and intrinsic motivations, guides the fast learning of new tasks, predicting the consequences of our actions, predicting the course of successful actions, and avoiding dangerous situations. However, according to him, building trainable internal models of the world that can deal with complex prediction uncertainties is still challenging because structural aspects of modularity and hierarchy have been neglected. 

Intrinsic-motivated RL has led to state-of-the-art results in Agent57 \cite{badia2020agent57}: Deep Mind’s most successful Atari player. In Agent57, intrinsic motivation rewards provide more dense internal rewards for novelty-seeking behaviours, encouraging the agent to explore and visit as many states as possible. Despite significant advances in the field, many cognition elements still need to be addressed to build intrinsically motivated agents \cite{hao2023exploration}\cite{aubret2019survey}. According to Lecun \cite{lecun2022path}, we are equipped with an internal modular and hierarchical world that says what is probable, plausible, and impossible. Such a model, combined with simple behaviours and intrinsic motivations, guides the fast learning of new tasks, predicting the consequences of our actions, predicting the course of successful actions, and avoiding dangerous situations. However, according to him, building trainable internal models of the world that can deal with complex prediction uncertainties is still challenging because structural aspects of modularity and hierarchy have been neglected. 

%The idea that humans use internal models of the world to learn has been around for a long time in psychology and neuroscience \cite{hawkins_framework_2019}\cite{hawkins_theory_2017}. Hawkins et al. \cite{hawkins_why_2017} worked for at least two decades studying the development of human intelligence in the neocortex. They developed a simplified computational model of neocortical pyramidal cells and a local learning algorithm capable of updating the state of neurons and directing learning through non-expected events. Such a learning method updates the neuron binding weights only when the future state of sensorial receptive fields expectation is broken. According to him, the neocortex has a similar circuit composed of pyramidal cells, a highly modular and sparse hierarchical structure capable of executing a common learning algorithm. Recently, Hole et al. \cite{hole_thousand_2021}, in a review article on the future of artificial intelligence, commented that to build truly intelligent agents, it is necessary to introduce important aspects of the human neocortex, such as sparsity, independence, modularity, and hierarchy.
The idea that humans use internal models of the world to learn has been around for a long time in psychology and neuroscience \cite{hawkins_framework_2019}\cite{hawkins_theory_2017}. Hawkins et al. \cite{hawkins_why_2017} worked studying human intelligence in the neocortex for at least two decades. They developed a simplified computational model of neocortical pyramidal cells and a local learning algorithm capable of updating the state of neurons and directing learning through non-expected events. Such a learning method updates the neuron weights only when the future state of sensorial receptive fields expectation is broken. According to him, the neocortex has a similar circuit composed of pyramidal cells, a highly modular and sparse hierarchical structure executing a common learning algorithm. Recently, Hole et al. \cite{hole_thousand_2021}, in a survey on the future of artificial intelligence, commented that to create truly intelligent agents, it is necessary to introduce essential aspects of the human neocortex, such as sparsity, independence, modularity, and hierarchy.


In this work, we propose to employ the cognition aspects of sparsity, modularity, independence, hierarchy, and attention to generate an internal world model for an intrinsically motivated agent. In this way, we guarantee the generation of the internal world with greater flexibility that better captures the physical aspects of the world. Specifically, our intrinsically-motivated agent has a predictive world model and a policy model. The predictive world model is entirely modular and hierarchical, composed of Bidirectional Recurrent Models \cite{mittal2020learning} that competitively generate representations of the agent's current and possible future state. Meanwhile, our policy network generates the agent's current action based on the current state. Figure \ref{fig:meu_modelo_simples} depicts our proposed approach, designed to support high dimensional data.

As our main contributions, we highlight the following:

\begin{enumerate}
    \item To the best of our knowledge, it is the first approach to adopt a predictive world model with sparsity, independence, modularity, and hierarchy aspects to create intrinsic rewards in RL agents.
    \item It combines modular attentional structures to improve the learning of intrinsic models.
    \item It allows advancing in learning models, achieving over 40\% of learning improvements in some test cases.
\end{enumerate}




%Mas como podemos usar essas observações sobre brincadeiras infantis para melhorar a inteligência artificial? Os teóricos da IA ​​perceberam há muito tempo que o comportamento lúdico na ausência de recompensas pode ser matematicamente formalizado por meio de funções de perda que codificam sinais de recompensa intrínsecos, nos quais um agente escolhe ações que resultam em estados novos, mas previsíveis, que maximizam seu aprendizado [38]. Essas ideias dependem de um ciclo virtuoso no qual o agente se auto-curriculariza ativamente à medida que ultrapassa os limites do que seus sistemas de previsão de modelo de mundo podem alcançar. À medida que a capacidade de modelagem do mundo melhora, o que costumava ser novidade se torna velho e o ciclo recomeça.













%Reinforcement learning algorithms aim at learning policies for achieving target tasks by maximizing rewards provided by the environment. In some scenarios, these rewards are supplied to the agent continuously, e.g. the running score in an Atari game (Mnih et al., 2015), or the distance be- tween a robot arm and an object in a reaching task (Lilli- crap et al., 2016). However, in many real-world scenarios, rewards extrinsic to the agent are extremely sparse or missing altogether, and it is not possible to construct a shaped reward function. This is a problem as the agent receives reinforcement for updating its policy only if it succeeds in reaching a pre-specified goal state. Hoping to stumble into a goal state by chance (i.e. random exploration) is likely to be futile for all but the simplest of environments.


%falar sobre domain generalization e os métodos clássicos
%Measuring “novelty” requires a statistical model of the dis- tribution of the environmental states, whereas measuring prediction error/uncertainty requires building a model of environmental dynamics that predicts the next state (st+1) given the current state (st ) and the action (at ) executed at time t. Both these models are hard to build in high- dimensional continuous state spaces such as images. An additional challenge lies in dealing with the stochasticity of the agent-environment system, both due to the noise in the agent’s actuation, and, more fundamentally, due to the inherent stochasticity in the environment. To give the ex- ample from (Schmidhuber, 2010), if the agent receiving images as state inputs is observing a television screen dis- playing white noise, every state will be novel as it would be impossible to predict the value of any pixel in the fu- ture. This means that the agent will remain curious about the television screen because it is unaware that some parts of the state space simply cannot be modeled and thus the agent can fall into an artificial curiosity trap and stall its exploration. Other examples of such stochasticity include appearance changes due to shadows from other moving en- tities or presence of distractor objects. Somewhat differ- ent, but related, is the challenge of generalization across physically (and perhaps also visually) distinct but function- ally similar parts of an environment, which is crucial for large-scale problems. One proposed solution to all these problems is to only reward the agent when it encounters states that are hard to predict but are “learnable” (Schmid- huber, 1991). However, estimating learnability is a non- trivial problem (Lopes et al., 2012).

%Some approaches use modular structures and attention to mediate communication and activation between different modules. The hypothesis is that tasks and objects are composed of smaller pieces and that the network needs to reflect the compositional structure of the world. Recurrent Independent Mechanisms (RIMs) \cite{goyal2019recurrent}, and Bidirectional Recurrent Independent Mechanisms (BRIMs) \cite{mittal2020learning} are the main modular models in the literature, achieving state-of-the-art results in the field in several tasks. However, these models have not yet been explored in challenging DG contexts, have computational complexity directly proportional to input data size, use dense representations, and have low sensorimotor integration. 


%This work belongs to the broad category of methods that generate an intrinsic reward signal based on how hard it is for the agent to predict the consequences of its own actions. However, we manage to escape most pitfalls of previous prediction approaches with the following key in- sight: we only predict those changes in the environment that could possibly be due to the actions of our agent or affect the agent, and ignore the rest. That is, instead of making predictions in the raw sensory space (e.g. pixels), we transform the sensory input into a feature space where only the information relevant to the action performed by the agent is represented. We learn this feature space using self-supervision – training a neural network on a proxy in- verse dynamics task of predicting the agent’s action given its current and next states. Since the neural network is only required to predict the action, it has no incentive to repre- sent within its feature embedding space the factors of vari- ation in the environment that do not affect the agent itself. We then use this feature space to train a forward dynamics model that predicts the feature representation of the next state, given the feature representation of the current state









