\section{Related Work}
\label{sec:related_work}


%introduzir o intrinsic motivation RL e mostrar os trabalhos mais antigos
%Intrinsic motivation is a an topic very studied in the reinforcement learning field e um bom resumo da literatura é apresentado por Barto et al. \cite{barto2013intrinsic}, Aubret et al. \cite{aubret2019survey}, and Singh et al. \cite{singh2010intrinsically}. Inicialmente, trabalhos de motivação intrínseca usaram conceitos de emoção, surpresa, empowerment, entropia e ganho de informação como rewards intrínsecas. Sequeira et al. \cite{sequeira2011emotion} exploraram a hipótese de que estados afetivos codificam informações capazes de orientar a tomada de decisão de um agente durante o aprendizado. Achiam et al. \cite{achiam2017surprise} propuseram uma abordagem baseada em surpresa na qual o agente aprende um modelo de transição de probabilidades de um MDP concorrentemente com a política e gerar recompensas intrínsecas que se aproximam da divergência KL das verdadeiras probabilidades de transição do modelo aprendido. Mohamed et al. \cite{mohamed2015variational} desenvolveram uma abordagem usando o conceito de empoderamento, autoencoders variacionais e redes neurais convolucionais para produzir um algoritmo de otimizacão estocástica capaz de utilizar informações diretamente de pixels da imagem. Similarmente, Klyubin et al. \cite{klyubin2005empowerment} usaram o conceito de empoderamento como o ganho de informação baseado na entropia das ações para formular rewards intrínsecas.

Intrinsic motivation is a very studied topic in the reinforcement learning field, and a good summary is presented by Barto et al. \cite{barto2013intrinsic}, Aubret et al. \cite{aubret2019survey}, and Singh et al. \cite{singh2010intrinsically}. Initially, intrinsic motivation used concepts of emotion, surprise, empowerment, entropy, and information gain to formulate intrinsic rewards. Sequeira et al. \cite{sequeira2011emotion} explored the hypothesis that affective states encode information that guides an agent's decision-making during learning. Achiam et al. \cite{achiam2017surprise} proposed a surprise-based approach in which the agent learns a probability transition model of an MDP concurrently with the policy and generates intrinsic rewards that approximate the KL divergence of the learned model's true transition probabilities. Mohamed et al. \cite{mohamed2015variational} developed an approach using the empowerment concept. Variational autoencoders and convolutional neural networks produce a stochastic optimization algorithm directly from image pixels. Similarly, Klyubin et al. \cite{klyubin2005empowerment} used empowerment as the gain of information based on the entropy of actions to formulate intrinsic rewards.



%falar dos modelos knowledge-based
%Atualmente, as abordagens baseados no prediction error in the feature space vem sendo bastante exploradas na literatura. Em 2015, Stadie et al. [X] iniciaram as pesquisas usando o espaço de features de um autoencoder para mensurar os estados interessantes para explorar. Em seguida, Pathak et. al \cite{pathak2017curiosity} propuseram o Intrinsic Curiosity Module (ICM), uma abordagem baseada em modelo de dinâmica inversa capaz de escalar para espaços contínuos de alta dimensão e minimizar as dificuldades de predição diretamente em pixels, além de ignorar aspectos do ambiente que não afetam o agente. A abordagem demonstrou que realizar predições diretamente do espaço sensorial bruto é inviável porque é difícil prever pixels diretamente, além de que alguns espaços sensoriais podem ser irrelevantes para a tarefa do agente. Os agentes treinados com rewards puramente intrínsecas foram capazes de aprender comportamentos cognitivos relevantes para a tarefa, demonstrando resultados promissores em ambientes esparsos. Similarmente, Taylor et. al \cite{taylor2021curiosity} propuseram um modelo de dinâmica inversa para avaliar o papel da composição do espaço sensorial no desempenho de um braço robótico intrinsecamente motivado cuja tarefa é manipular objetos em uma mesa. Os resultados mostraram que abordagem funciona como um curruclum learning "de dentro para fora". O agente começa a explorar o seu próprio corpo primeiro, e somente depois de ter adquirido um certo conhecimento é que passa a explorar seu entorno com mais frequência. Tais resultados oferecem uma explicação para o comportamento motor precoce em bebês e reforça a hipótese de que o comportamento é impulsionado pela descoberta de novos padrões.

Currently, approaches based on prediction error in the feature space have been extensively explored in the literature. In 2015, Stadie et al. \cite{stadie2015incentivizing} started their research using the feature space of an autoencoder to measure interesting states to explore. Pathak et al. \cite{pathak2017curiosity} proposed an approach based on an inverse dynamics model capable of scaling to high-dimensional continuous spaces and minimizing the difficulties of predicting directly in pixels, in addition to ignoring aspects of the environment that do not affect the agent. The approach showed that making predictions directly from the raw sensory space is unfeasible because it is challenging to predict pixels directly. Furthermore, some sensory spaces may be irrelevant to the agent's task. Agents trained with purely intrinsic rewards were able to learn task-relevant cognitive behaviors, demonstrating promising results in sparse environments. Similarly, Taylor et al. proposed an inverse dynamics model to assess the role of sensory space composition in the performance of an intrinsically motivated robotic arm that should manipulate objects on a table. Results showed that the approach works like an ``inside-out'' curriculum learning. The agent begins to explore its own body first, and only after acquiring knowledge does it explore its surroundings more frequently. Such results explain early motor behavior in infants and reinforce the hypothesis that discovering new patterns drives behavior.


%falar dos modelos knowledge-based

%Burda et al. \cite{burda2018large} investigaram o efeito de agentes curiosos na exploração de vários jogos de Atari e como os diferentes espaços de features alteram os resultados e a performance dos agentes intrínsecos. Os resultados obtidos mostraram que gerar a reward intrínseca a partir do: 1) erro de predição diretamente do espaço de pixels é desafiador em ambientes de alta dimensão; 2) o erro de predição diretamente de autoencoders variacionais apresenta um bom resumo da observação porém pode conter muitos detalhes irrelevantes; 3) o erro de predição a partir de features randômicas é eficiente em termos de dimensionalidade, porém as features são fixas e insuficientes em vários cenários; 4) o erro de predição a partir do inverse dynamic features é a melhor opção atualmente para garantir que as features aprendidas contém aspectos importantes para o agente. Além disso, a abordagem conseguiu demonstrar as limitações do prediction-based rewards em ambientes estocásticos. Recentemente, Pathak et al. \cite{pathak2019self} apresentou uma abordagem para lidar com o desafio da estocasticidade dos ambientes. Os autores utilizaram ideias do active learning clássico para formular uma abordagem baseada em ensemble models.

Burda et al. \cite{burda2018large} investigated, in various Atari games, how curious agents and different feature spaces alter the results and performance of intrinsic agents. The results showed that: 1) generating the intrinsic reward from prediction error directly from the pixel space is challenging in high-dimensional environments; 2) variational autoencoders (VAEs) are a good summary of the observation but may contain many irrelevant details; 3) random features are fixed and insufficient in several scenarios; and 4) prediction error from inverse dynamic features is currently the best option to guarantee that the learned features contain essential aspects for the agent. Recently, Pathak et al. \cite{pathak2019self} presented an approach to deal with the challenge of stochasticity of environments. The authors used ideas from active learning to formulate an approach based on ensemble models.


%Recentemente, alguns experimentos mostraram que rewards intrínsecas são indispensáveis para a criação de agentes complexos que se auto-supervisionam em ambientes mais realistas. Haber et al. \cite{haber2018learning} demonstraram que a política aprendida por um agente intrínseco permitiu a exploração de novas iterações com o ambiente através do aprendizado de comportamentos cognitivos não triviais, como ego-motion, atenção seletiva e iteração com objetos. O modelo apresenta duas redes neurais, a ``world-model'' que aprende a predizer as consequências dinâmicas das ações do agente enquanto o ``self-model'' aprende a prever erros do modelo de mundo do agente. O agente então usa o ``self-model'' para escolher ações que ele acredita que desafiarão adversamente o estado atual do seu ``world-model''. Esse aprendizado ocorre por meio de um processo emergente autosupervisionado no qual novas capacidades surgem em distintos ``marcos de desenvolvimento'' como nos bebês humanos. Além disso, o agente também aprende codificações visuais melhoradas em determinadas tarefas, como detecção, localização, reconhecimento de objetos e a previsão de dinâmica física melhor que outras abordagens que são estado da arte.


Some experiments have shown that intrinsic rewards are indispensable for creating complex agents that supervise themselves in more realistic environments \cite{haber2018learning}. Haber et al. \cite{haber2018learning} demonstrated that the intrinsically motivated agents learned non-trivial cognitive behaviors such as self-generated motion (i.e., ego-motion), selective attention, and iteration with objects. The model presents two neural networks, the ``world-model'' which learns to predict the dynamic consequences of the agent's actions, while the ``self-model'' learns to predict errors in the agent's world model. The agent then uses the ``self-model'' to choose actions that it believes will adversely challenge the current state of its ``world-model''. This learning occurs through a self-supervised emergent process in which new abilities emerge in developmental milestones, as in human babies. In addition, the agent also learns improved visual encodings in specific tasks, such as detection, location, object recognition, and the prediction of physical dynamics better than other state-of-the-art approaches.


%A key aspect that differs our work from previous ones is the adoption of Bidirectional Recurrent Models (BRIMs) in implementing the internal world model. BRIMs are a category of deep recurrent neural net architectures that uses attention to dynamically combine bottom-up (directly observed through sensations) and top-down (expectations based on past experience) signals\cite{mittal2020learning}. In particular, we show how BRIMs can raise the state-of-the-art performance of intrinsically-motivated agents with improved out-of-distribution generalization.

A key aspect that differentiates our work from others is that, to the best of our knowledge, it is the first approach to unite several elements of cognition that have been neglected for a long time. Based on studies proposed by Hawkins et al. \cite{hawkins_why_2017}, Lecun et al. \cite{lecun2022path}, and Hole et al. \cite{hole2021thousand}, we combine sparsity, modularity, hierarchy, and attention to building an internal world model for an intrinsically motivated agent. Our framework has much potential because it generates predictions of the agent's future states from a modular, hierarchical, and fully reconfigurable structure through bottom-up and top-down attentional signals. Such mechanisms allow the internal world model to generate future states' predictions from competitive small independent modules similar to the human neocortex.







%Our work connects to a variety of existing ideas in self-supervision, active learning, and deep reinforcement learning. Visual learning can be achieved through self-supervised auxiliary tasks including semantic segmentation [18], pose estimation [29], solving jigsaw puzzles [32], colorization [46], and rotation [43]. Self-supervision on videos frame prediction [23] is also promising, but faces the challenge that most sequences in recorded videos are “boring”, with little interesting dynamics occurring from one frame to the next.

%In order to encourage interesting events to happen, it is useful for an agent to have the capacity to select the data that it sees in training. In active learning, an agent seeks to learn a supervised task using minimal labeled data [12, 40]. Recent methods obtain diversified sets of hard examples [8, 39], or use confidence-based heuristics to determine when to query for more labels [45]. Beyond selection of examples from a pre-determined set, recent work in robotics [2, 7, 10, 36] study learning tasks with interactive visuo-motor setups such as robotic arms. The results are promising, but largely use random policies to generate training data without biasing the robot to explore in a structured way.

%Intrinsic and extrinsic reward structures have been used to learn generic “skills” for a variety of tasks [6, 28, 41]. Houthooft et al. [19] demonstrated that reasonable exploration-exploitation trade-offs can be achieved by intrinsic reward terms formulated as information gain. Frank et al. [11] use information gain maximization to implement artificial curiosity on a humanoid robot. Kulkarni et al. [26] combine intrinsic motivation with hierarchical action-value functions operating at different temporal scales, for goal-driven deep reinforcement learning. Achiam and Sastry [1] formulate surprise for intrinsic motivation as the KL-divergence of the true transition probabilities from learned model probabilities. Held et al. [16] use a generator network, which is optimized using adversarial training to produce tasks that are always at the appropriate level of difficulty for an agent, to automatically produce a curriculum of navigation tasks to learn. Jaderberg et al. [22] show that target tasks can be improved by using auxiliary intrinsic rewards.
 







%A family of approaches to intrinsic motivation reward an agent based on prediction error [2, 27, 36, 42], prediction uncertainty [11, 44], or improvement [19, 34] of a forward dynamics model of the environment that gets trained along with the agent’s policy. As a result the agent is driven to reach regions of the environment that are difficult to predict for the forward dynamics model, while the model improves its predictions in these regions. This adversarial and non-stationary dynamics can give rise to complex behaviors. Relatively little work has been done in this area on the pure exploration setting where there is no external reward. Of these mostly closely related are those that use a forward dynamics model of a feature space such as Stadie et al. [42] where they use autoencoder features, and Pathak et al. [27] where they use features trained with an inverse dynamics task. These correspond roughly to the VAE and IDF methods detailed in Section 2.1.

%Smoothed versions of state visitation counts can be used for intrinsic rewards [3, 9, 24, 47]. Count- based methods have already shown very strong results when combining with extrinsic rewards such as setting the state of the art in the Atari game Montezuma’s Revenge [3], and also showing significant exploration of the game without using the extrinsic reward. It is not yet clear in which situations count-based approaches should be preferred over dynamics-based approaches; we chose to focus on dynamics-based bonuses in this paper since we found them straightforward to scale and parallelize. In our preliminary experiments, we did not have sufficient success with already existing count-based implementations in scaling up for a large-scale study.

%Learning without extrinsic rewards or fitness functions has also been studied extensively in the evolutionary computing where it is referred to as ‘novelty search’ [17, 18, 43]. There the novelty of an event is often defined as the distance of the event to the nearest neighbor amongst previous events, using some statistics of the event to compute distances. One interesting finding from this literature is that often much more interesting solutions can be found by not solely optimizing for fitness.
%Other methods of exploration are designed to work in combination with maximizing a reward function, such as those utilizing uncertainty about value function estimates [5, 23], or those using perturbations of the policy for exploration [8, 29]. Schmidhuber [37] and Oudeyer [25], Oudeyer and Kaplan [26] provide a great review of some of the earlier work on approaches to intrinsic motivation. Alternative methods of exploration include Sukhbaatar et al. [45] where they utilize an adversarial game between two agents for exploration. In Gregor et al. [10], they optimize a quantity called empowerment which is a measurement of the control an agent has over the state. In a concurrent work, diversity is used as a measure to learn skills without reward functions Eysenbach et al. [7].