\section{Bidirectional Recurrent Models}
\label{sec:preliminaries}

\textbf{Bidirectional Recurrent Models (BRIMs)} mainly use self-attention to link identical LSTM modules, generating a very sparse and modular framework with only a small portion of modules actives at time $t$~\cite{mittal2020learning}. The approach separates the hidden state into several modules so that upward iterations between bottom-up and top-down signals can be appropriately focused. The layer structure has concurrent modules so that each hierarchical layer can send information both in the bottom-up and top-down directions. Bottom-up attentional subsystems communicate between modules of the same layer, as well as the composition of hidden states in initial layers using the entry $x_{t}$ as the target, and via top-down attention modules in different layers communicate with each other requesting information about hidden states of previous and posterior layers to compose the current hidden state. BRIMs is composed of the following structures.

\textbf{Multi-layer Stacked Recurrent Networks}. Most multi-layer recurrent networks are configured to operate feed-forward and bottom-up, meaning that higher layers are fed with information processed by inferior layers. In this sense, the traditional stacked RNN for $L$ levels is defined as $\textbf{h}_t^l = F^l(\textbf{h}_t^{l-1}, \textbf{h}_{t-1}^l)$, where $l = 0, 1, ..., L$. For a specific time step $t$, $\textbf{y}_t = D(\textbf{h}_t^L)$ executes the prediction, based on input $\textbf{x}_t$, where $\textbf{h}_t^0 = E(\textbf{x}_t)$ is the first hidden state at model, and $\textbf{h}_t^l$ to the hidden state at layer $l$. $D$ defines the decoder, $E$ is the encoder, and $F^l$ represents the recurrent dynamic at layer $l$ (e.g., LSTM, GRU).

%\begin{equation}
%\textbf{y}_t = D(\textbf{h}_t^L),
%\end{equation}

%\begin{equation}
%\textbf{h}_t^l = F^l(\textbf{h}_t^{l-1}, \textbf{h}_{t-1}^l),
%\end{equation}

%\begin{equation}
%\textbf{h}_t^0 = E(\textbf{x}_t),
%\end{equation}

%\noindent

\textbf{Recurrent Independent Mechanisms (RIMs)}. Proposed by Goyal et al.~\cite{goyal2019recurrent}, RIM is a single-layered recurrent architecture that consists of hidden state $\textbf{h}_t$ decomposed into $n$ modules. The main property introduced in this model is that on a specific
time step, only a small subset of modules is activated. In this sense, the hidden states are updated following these steps: a) a subset of modules is activated depending on their relevance to the input; b) the activated modules independently process the information; c) the activated modules
have contextual information from the other modules and update their hidden state to store such information.

\textbf{Key-Value Attention}. The Key-Value Attention, also called the Scaled Dot Product, is responsible for the updates in RIM. This attentional mechanism is also employed in the self-attention modules, widely used in Transformer architectures. The attention score $\textbf{A}_S = \mathrm{Softmax} \bigg(\frac{\textbf{QK}^T}{\sqrt{d}}\bigg)$ and an attention modulated result $\textbf{A}_R = \textbf{A}_S\textbf{V}$ are computed by self-attention modules, where $\textbf{Q}$ is the set of queries, $\textbf{K}$ are the keys with $d$ dimensions and $\textbf{V}$ are the values.

%\begin{equation}
%\textbf{A}_S = \mathrm{Softmax} \bigg(\frac{\textbf{QK}^T}{\sqrt{d}}\bigg),
%\end{equation}

%\begin{equation}
%\textbf{A}_R = \textbf{A}_S\textbf{V},
%\end{equation}

%\noindent  


\textbf{Selective Activation}. The selective activation is employed by defining that each module creates queries $\mathbf{\bar{Q}} = Q_{inp}(h_{t-1})$ which are then combined with the keys $\mathbf{\bar{K}} = K_{inp}(\phi, x_t)$ and values $\mathbf{\bar{V}} = V_{inp}(\phi, x_t)$ obtained from the input $x_t$ and zero vectors $\phi$ to get both
the attention score $\mathbf{\bar{A}}_S$ and attention modulated input $\mathbf{\bar{A}}_R$. Based on this attention score, a fixed number of modules $m$ are activated for which the input information is most relevant. In this sense, the null module provides no new information and has a low attention score. The activated set per time step is defined as $\mathcal{S}_t$.

\textbf{Independent Dynamics}. After the input is modulated by attention, each activated module has its hidden-state update procedure, as

\begin{equation}
\mathbf{\bar{h}}_{t,k} = \left\{\begin{NiceMatrix}[l]
F_k(\mathbf{\bar{A}}_{R_k}, \textbf{h}_{t-1,k}) & k \in \mathcal{S}_t
\\
\textbf{h}_{t-1,k} & k \notin \mathcal{S}_t,
\end{NiceMatrix}\right.
\end{equation}

\noindent where $F_k$ is any recurrent update procedure (e.g., GRU, LSTM).

\textbf{Communication}. Each module consolidates the information from all the other modules for every independent update step. The attention mechanism is utilized to consolidate this information in a similar way as in selective activation. The active modules create queries $\mathbf{\hat{Q}} = Q_{com}(h_t)$ which act with the keys $\mathbf{\hat{K}} = K_{com}(h_t)$ and values $\mathbf{\hat{V}} = V_{com}(h_t)$ generated by all modules and the result of attention $\mathbf{\hat{A}}_R$ is combined to the state in time step $t$ as

\begin{equation}
\textbf{h}_{t,k} = \left\{\begin{NiceMatrix}[l]
\mathbf{\bar{h}}_{t,k} + \mathbf{\hat{A}}_{R_{k}} & k \in \mathcal{S}_t
\\
\mathbf{\bar{h}}_{t,k} & k \notin \mathcal{S}_t.
\end{NiceMatrix}\right.
\end{equation}

\textbf{Composition of Modules}. The original hidden state $\mathbf{h}^l_t$ found in RIM is decomposed for each layer $l$ and time $t$ into separate modules. Therefore, instead of representing the state as just a fixed dimensional vector $\mathbf{h}^l_t$, the representation is defined as $\{((\mathbf{h}^l_{t,k})^{n_l}_{k=1}, \mathcal{S}^l_t)\}$ where $n_l$ denotes the number of modules in layer $l$ and $\mathcal{}S^l_t$ is the set of active modules at time $t$ in layer $l$. $|\mathcal{S}^l_t| = m_l$, where $m_l$ is a hyperparameter to define the number of modules active in each layer $l$ at any time; layers may have a different number of active modules. Setting $m_l$ to be half of $n_l$ provided good performance.
 
\textbf{Communication Between Layers}. The communication links are defined between
multiple layers using key-value attention. Tradicional RNNs build a strictly bottom-up multi-layer dependency; in BRIMs, instead, the multi-layer dependency considers queries $\mathbf{\bar{Q}} = Q_{lay}(\mathbf{h}^l_{t-1})$ from modules in layer $l$, and keys $\mathbf{\bar{K}} = K_{lay}(\phi, \mathbf{h}^{l-1}_t, \mathbf{h}^{l+1}_{t-1})$ and values $\mathbf{\bar{V}} = V_{lay}(\mathbf{\phi}, \mathbf{h}^{l-1}_t, \mathbf{h}^{l+1}_{t-1})$ from all the modules in the lower and higher layers. Thus, the attention mechanism acts in three directions and generates the attention score $\mathbf{\bar{A}}^l_S$ and output $\mathbf{\bar{A}}_R$. The same layer gives the attention-receiving information from the higher layer in the previous time step; the same layer also gives the attention-receiving information from the lower layer in the current time step. Only the lower layer is used for the deepest layer, and for the first layer, the inputâ€™s embedded state serves as the lower layer~\cite{mittal2020learning}.

\textbf{Sparse Activation}. The set $S^l_t$ is built based on the attention score $\mathbf{\bar{A}}^l_S$, which contains modules for which null information has little importance. Every activated module gets a separate input version, which is obtained via the attention output $\mathbf{\bar{A}}^l_R$. In practice, for each activated module, the representation is defined as $\mathbf{\bar{h}}_{t,k}^{l} = F_k^l(\mathbf{\bar{A}}_{R_k}^l, \textbf{h}_{t-1,k}^l)$, where $k \in \mathcal{S}_t^l$, and $F^l_k$ represents the recurrent update unit.

%\begin{equation}
%\mathbf{\bar{h}}_{t,k}^{l} = F_k^l(\mathbf{\bar{A}}_{R_k}^l, \textbf{h}_{t-1,k}^l) \hspace{1cm} k \in \mathcal{S}_t^l, 
%\end{equation}

%\noindent where $F^l_k$ represents the recurrent update unit.

\textbf{Communication Within Layers}. Communication between the different modules within each layer using the key-value attention. This communication between modules within a layer permits the modules to share information through the bottleneck of attention. In the same way, queries are generated $\mathbf{\hat{Q}} = Q_{com}(\mathbf{\bar{h}}^l_t)$ from active modules and keys $\mathbf{\hat{K}} = K_{com}(\mathbf{\bar{h}}^l_t)$ and values $\mathbf{\hat{V}} = V_{com}(\mathbf{\bar{h}}^l_t)$ from all the modules to obtain the final update to the module state through residual attention $\mathbf{\hat{A}}^l_R$. The state update rule is

\begin{equation}
\textbf{h}_{t,k}^l = \left\{\begin{NiceMatrix}[l]
\mathbf{\bar{h}}_{t,k}^{l} + \mathbf{\bar{A}}_{R_{k}}^l & k \in \mathcal{S}_t^l
\\\mathbf{\bar{h}}_{t-1,k}^l & k \notin \mathcal{S}_t^l.
\end{NiceMatrix}\right.
\end{equation}



%Finally, we present Algorithm~\ref{algo:brim} for the $L$ layered BRIMs, and a detailed explanation for the entire BRIMs construction. 




