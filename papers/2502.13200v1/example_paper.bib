@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})



@inproceedings{Ren2019LikelihoodRF,
  title={Likelihood Ratios for Out-of-Distribution Detection},
  author={J. Ren and Peter J. Liu and E. Fertig and Jasper Snoek and R. Poplin and M. DePristo and Joshua V. Dillon and Balaji Lakshminarayanan},
  booktitle={NeurIPS},
  year={2019}
}

@misc{shen2021outofdistribution,
      title={Towards Out-Of-Distribution Generalization: A Survey}, 
      author={Zheyan Shen and Jiashuo Liu and Yue He and Xingxuan Zhang and Renzhe Xu and Han Yu and Peng Cui},
      year={2021},
      eprint={2108.13624},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@INPROCEEDINGS{8578664,
  author={Li, Haoliang and Pan, Sinno Jialin and Wang, Shiqi and Kot, Alex C.},
  booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
  title={Domain Generalization with Adversarial Feature Learning}, 
  year={2018},
  volume={},
  number={},
  pages={5400-5409},
  doi={10.1109/CVPR.2018.00566}}

@inproceedings{DBLP:conf/cvpr/YangLCSHW21,
  author    = {Mengyue Yang and
               Furui Liu and
               Zhitang Chen and
               Xinwei Shen and
               Jianye Hao and
               Jun Wang},
  title     = {CausalVAE: Disentangled Representation Learning via Neural Structural
               Causal Models},
  booktitle = {{IEEE} Conference on Computer Vision and Pattern Recognition, {CVPR}
               2021, virtual, June 19-25, 2021},
  pages     = {9593--9602},
  publisher = {Computer Vision Foundation / {IEEE}},
  year      = {2021},
  url       = {https://openaccess.thecvf.com/content/CVPR2021/html/Yang\_CausalVAE\_Disentangled\_Representation\_Learning\_via\_Neural\_Structural\_Causal\_Models\_CVPR\_2021\_paper.html},
  timestamp = {Mon, 30 Aug 2021 17:00:27 +0200},
  biburl    = {https://dblp.org/rec/conf/cvpr/YangLCSHW21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{goyal2019recurrent,
  title={Recurrent independent mechanisms},
  author={Goyal, Anirudh and Lamb, Alex and Hoffmann, Jordan and Sodhani, Shagun and Levine, Sergey and Bengio, Yoshua and Sch{\"o}lkopf, Bernhard},
  journal={arXiv preprint arXiv:1909.10893},
  year={2019}
}

@inproceedings{pathak2017curiosity,
  title={Curiosity-driven exploration by self-supervised prediction},
  author={Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A and Darrell, Trevor},
  booktitle={International conference on machine learning},
  pages={2778--2787},
  year={2017},
  organization={PMLR}
}



@article{burda2018large,
  title={Large-scale study of curiosity-driven learning},
  author={Burda, Yuri and Edwards, Harri and Pathak, Deepak and Storkey, Amos and Darrell, Trevor and Efros, Alexei A},
  journal={arXiv preprint arXiv:1808.04355},
  year={2018}
}

@inproceedings{pathak2019self,
  title={Self-supervised exploration via disagreement},
  author={Pathak, Deepak and Gandhi, Dhiraj and Gupta, Abhinav},
  booktitle={International conference on machine learning},
  pages={5062--5071},
  year={2019},
  organization={PMLR}
}

@article{hao2023exploration,
  title={Exploration in Deep Reinforcement Learning: From Single-Agent to Multiagent Domain},
  author={Hao, Jianye and Yang, Tianpei and Tang, Hongyao and Bai, Chenjia and Liu, Jinyi and Meng, Zhaopeng and Liu, Peng and Wang, Zhen},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2023},
  publisher={IEEE}
}

@article{haber2018learning,
  title={Learning to play with intrinsically-motivated, self-aware agents},
  author={Haber, Nick and Mrowca, Damian and Wang, Stephanie and Fei-Fei, Li F and Yamins, Daniel L},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{aubret2019survey,
  title={A survey on intrinsic motivation in reinforcement learning},
  author={Aubret, Arthur and Matignon, Laetitia and Hassas, Salima},
  journal={arXiv preprint arXiv:1908.06976},
  year={2019}
}

@incollection{barto2013intrinsic,
  title={Intrinsic motivation and reinforcement learning},
  author={Barto, Andrew G},
  booktitle={Intrinsically motivated learning in natural and artificial systems},
  pages={17--47},
  year={2013},
  publisher={Springer}
}

@article{white1959motivation,
  title={Motivation reconsidered: the concept of competence.},
  author={White, Robert W},
  journal={Psychological review},
  volume={66},
  number={5},
  pages={297},
  year={1959},
  publisher={American Psychological Association}
}

@article{berlyne1966curiosity,
  title={Curiosity and Exploration: Animals spend much of their time seeking stimuli whose significance raises problems for psychology.},
  author={Berlyne, Daniel E},
  journal={Science},
  volume={153},
  number={3731},
  pages={25--33},
  year={1966},
  publisher={American Association for the Advancement of Science}
}

@article{singh2010intrinsically,
  title={Intrinsically motivated reinforcement learning: An evolutionary perspective},
  author={Singh, Satinder and Lewis, Richard L and Barto, Andrew G and Sorg, Jonathan},
  journal={IEEE Transactions on Autonomous Mental Development},
  volume={2},
  number={2},
  pages={70--82},
  year={2010},
  publisher={IEEE}
}

@article{oudeyer2009intrinsic,
  title={What is intrinsic motivation? A typology of computational approaches},
  author={Oudeyer, Pierre-Yves and Kaplan, Frederic},
  journal={Frontiers in neurorobotics},
  pages={6},
  year={2009},
  publisher={Frontiers}
}

@inproceedings{badia2020agent57,
  title={Agent57: Outperforming the atari human benchmark},
  author={Badia, Adri{\`a} Puigdom{\`e}nech and Piot, Bilal and Kapturowski, Steven and Sprechmann, Pablo and Vitvitskyi, Alex and Guo, Zhaohan Daniel and Blundell, Charles},
  booktitle={International Conference on Machine Learning},
  pages={507--517},
  year={2020},
  organization={PMLR}
}


@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@inproceedings{sequeira2011emotion,
  title={Emotion-based intrinsic motivation for reinforcement learning agents},
  author={Sequeira, Pedro and Melo, Francisco S and Paiva, Ana},
  booktitle={International conference on affective computing and intelligent interaction},
  pages={326--336},
  year={2011},
  organization={Springer}
}

@article{achiam2017surprise,
  title={Surprise-based intrinsic motivation for deep reinforcement learning},
  author={Achiam, Joshua and Sastry, Shankar},
  journal={arXiv preprint arXiv:1703.01732},
  year={2017}
}


@article{mohamed2015variational,
  title={Variational information maximisation for intrinsically motivated reinforcement learning},
  author={Mohamed, Shakir and Jimenez Rezende, Danilo},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@inproceedings{klyubin2005empowerment,
  title={Empowerment: A universal agent-centric measure of control},
  author={Klyubin, Alexander S and Polani, Daniel and Nehaniv, Chrystopher L},
  booktitle={2005 ieee congress on evolutionary computation},
  volume={1},
  pages={128--135},
  year={2005},
  organization={IEEE}
}

@article{stadie2015incentivizing,
  title={Incentivizing exploration in reinforcement learning with deep predictive models},
  author={Stadie, Bradly C and Levine, Sergey and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1507.00814},
  year={2015}
}

@inproceedings{mittal2020learning,
  title={Learning to combine top-down and bottom-up signals in recurrent neural networks with attention over modules},
  author={Mittal, Sarthak and Lamb, Alex and Goyal, Anirudh and Voleti, Vikram and Shanahan, Murray and Lajoie, Guillaume and Mozer, Michael and Bengio, Yoshua},
  booktitle={International Conference on Machine Learning},
  pages={6972--6986},
  year={2020},
  organization={PMLR}
}

@article{hole2021thousand,
  title={A thousand brains: toward biologically constrained AI},
  author={Hole, Kjell J{\o}rgen and Ahmad, Subutai},
  journal={SN Applied Sciences},
  volume={3},
  number={8},
  pages={1--14},
  year={2021},
  publisher={Springer}
}

@inproceedings{zhang2021deep,
  title={Deep Stable Learning for Out-Of-Distribution Generalization},
  author={Zhang, Xingxuan and Cui, Peng and Xu, Renzhe and Zhou, Linjun and He, Yue and Shen, Zheyan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={5372--5382},
  year={2021}
}

@article{duchi2021learning,
  title={Learning models with uniform performance via distributionally robust optimization},
  author={Duchi, John C and Namkoong, Hongseok},
  journal={The Annals of Statistics},
  volume={49},
  number={3},
  pages={1378--1406},
  year={2021},
  publisher={Institute of Mathematical Statistics}
}

@article{arjovsky2019invariant,
  title={Invariant risk minimization},
  author={Arjovsky, Martin and Bottou, L{\'e}on and Gulrajani, Ishaan and Lopez-Paz, David},
  journal={arXiv preprint arXiv:1907.02893},
  year={2019}
}

@inproceedings{creager2021environment,
  title={Environment inference for invariant learning},
  author={Creager, Elliot and Jacobsen, J{\"o}rn-Henrik and Zemel, Richard},
  booktitle={International Conference on Machine Learning},
  pages={2189--2200},
  year={2021},
  organization={PMLR}
}

@inproceedings{shen2020stable,
  title={Stable learning via sample reweighting},
  author={Shen, Zheyan and Cui, Peng and Zhang, Tong and Kunag, Kun},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={04},
  pages={5692--5699},
  year={2020}
}

@article{liu2021heterogeneous,
  title={Heterogeneous Risk Minimization},
  author={Liu, Jiashuo and Hu, Zheyuan and Cui, Peng and Li, Bo and Shen, Zheyan},
  journal={arXiv preprint arXiv:2105.03818},
  year={2021}
}

@misc{correia2021attention,
      title={Attention, please! A survey of Neural Attention Models in Deep Learning}, 
      author={Alana de Santana Correia and Esther Luna Colombini},
      year={2021},
      eprint={2103.16775},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{zhang2021understanding,
      title={Understanding Failures in Out-of-Distribution Detection with Deep Generative Models}, 
      author={Lily H. Zhang and Mark Goldstein and Rajesh Ranganath},
      year={2021},
      eprint={2107.06908},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@article{hole_thousand_2021,
	title = {A thousand brains: toward biologically constrained {AI}},
	volume = {3},
	issn = {2523-3963, 2523-3971},
	shorttitle = {A thousand brains},
	url = {https://link.springer.com/10.1007/s42452-021-04715-0},
	doi = {10.1007/s42452-021-04715-0},
	abstract = {This paper reviews the state of artificial intelligence (AI) and the quest to create general AI with human-like cognitive capabilities. Although existing AI methods have produced powerful applications that outperform humans in specific bounded domains, these techniques have fundamental limitations that hinder the creation of general intelligent systems. In parallel, over the last few decades, an explosion of experimental techniques in neuroscience has significantly increased our understanding of the human brain. This review argues that improvements in current AI using mathematical or logical techniques are unlikely to lead to general AI. Instead, the AI community should incorporate neuroscience discoveries about the neocortex, the human brain’s center of intelligence. The article explains the limitations of current AI techniques. It then focuses on the biologically constrained Thousand Brains Theory describing the neocortex’s computational principles. Future AI systems can incorporate these principles to overcome the stated limitations of current systems. Finally, the article concludes that AI researchers and neuroscientists should work together on specified topics to achieve biologically constrained AI with human-like capabilities.},
	language = {en},
	number = {8},
	urldate = {2021-07-25},
	journal = {SN Applied Sciences},
	author = {Hole, Kjell Jørgen and Ahmad, Subutai},
	month = aug,
	year = {2021},
	keywords = {2021, LIDO},
	pages = {743},
}

@article{ahmad_how_2016,
	title = {How do neurons operate on sparse distributed representations? {A} mathematical theory of sparsity, neurons and active dendrites},
	shorttitle = {How do neurons operate on sparse distributed representations?},
	url = {http://arxiv.org/abs/1601.00720},
	abstract = {We propose a formal mathematical model for sparse representations and active dendrites in neocortex. Our model is inspired by recent experimental findings on active dendritic processing and NMDA spikes in pyramidal neurons. These experimental and modeling studies suggest that the basic unit of pattern memory in the neocortex is instantiated by small clusters of synapses operated on by localized non-linear dendritic processes. We derive a number of scaling laws that characterize the accuracy of such dendrites in detecting activation patterns in a neuronal population under adverse conditions. We introduce the union property which shows that synapses for multiple patterns can be randomly mixed together within a segment and still lead to highly accurate recognition. We describe simulation results that provide further insight into sparse representations as well as two primary results. First we show that pattern recognition by a neuron with active dendrites can be extremely accurate and robust with high dimensional sparse inputs even when using a tiny number of synapses to recognize large patterns. Second, equations representing recognition accuracy of a dendrite predict optimal NMDA spiking thresholds under a generous set of assumptions. The prediction tightly matches NMDA spiking thresholds measured in the literature. Our model matches many of the known properties of pyramidal neurons. As such the theory provides a mathematical framework for understanding the benefits and limits of sparse representations in cortical networks.},
	language = {en},
	urldate = {2021-07-25},
	journal = {arXiv:1601.00720 [cs, q-bio]},
	author = {Ahmad, Subutai and Hawkins, Jeff},
	month = may,
	year = {2016},
	note = {arXiv: 1601.00720},
}

@article{purdy_encoding_nodate,
	title = {Encoding {Data} for {HTM} {Systems}},
	abstract = {Hierarchical Temporal Memory (HTM) is a biologically inspired machine intelligence technology that mimics the architecture and processes of the neocortex. In this white paper we describe how to encode data as Sparse Distributed Representations (SDRs) for use in HTM systems. We explain several existing encoders, which are available through the open source project called NuPIC1, and we discuss requirements for creating encoders for new types of data.},
	language = {en},
	author = {Purdy, Scott},
	pages = {11},
}

@inproceedings{kuderov_planning_2021,
	address = {Online Streaming, --- Select a Country ---},
	title = {Planning with {Hierarchical} {Temporal} {Memory} for {Deterministic} {Markov} {Decision} {Problem}:},
	isbn = {978-989-758-484-8},
	shorttitle = {Planning with {Hierarchical} {Temporal} {Memory} for {Deterministic} {Markov} {Decision} {Problem}},
	url = {https://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0010317710731081},
	doi = {10.5220/0010317710731081},
	language = {en},
	urldate = {2021-07-25},
	booktitle = {Proceedings of the 13th {International} {Conference} on {Agents} and {Artificial} {Intelligence}},
	publisher = {SCITEPRESS - Science and Technology Publications},
	author = {Kuderov, Petr and Panov, Aleksandr},
	year = {2021},
	pages = {1073--1081},
}

@article{lewis_hippocampal_2021,
	title = {Hippocampal {Spatial} {Mapping} {As} {Fast} {Graph} {Learning}},
	url = {http://arxiv.org/abs/2107.00567},
	abstract = {The hippocampal formation is thought to learn spatial maps of environments, and in many models this learning process consists of forming a sensory association for each location in the environment. This is inefﬁcient, akin to learning a large lookup table for each environment. Spatial maps can be learned much more efﬁciently if the maps instead consist of arrangements of sparse environment parts. In this work, I approach spatial mapping as a problem of learning graphs of environment parts. Each node in the learned graph, represented by hippocampal engram cells, is associated with feature information in lateral entorhinal cortex (LEC) and location information in medial entorhinal cortex (MEC) using empirically observed neuron types. Each edge in the graph represents the relation between two parts, and it is associated with coarse displacement information. This core idea of associating arbitrary information with nodes and edges is not inherently spatial, so this proposed fast-relation-graph-learning algorithm can expand to incorporate many spatial and non-spatial tasks.},
	language = {en},
	urldate = {2021-07-25},
	journal = {arXiv:2107.00567 [cs, q-bio]},
	author = {Lewis, Marcus},
	month = jul,
	year = {2021},
	note = {arXiv: 2107.00567},
}

@article{leadholm_grid_2021,
	title = {Grid {Cell} {Path} {Integration} {For} {Movement}-{Based} {Visual} {Object} {Recognition}},
	url = {http://arxiv.org/abs/2102.09076},
	abstract = {Grid cells enable the brain to model the physical space of the world and navigate effectively via path integration, updating self-position using information from self-movement. Recent proposals suggest that the brain might use similar mechanisms to understand the structure of objects in diverse sensory modalities, including vision. In machine vision, object recognition given a sequence of sensory samples of an image, such as saccades, is a challenging problem when the sequence does not follow a consistent, ﬁxed pattern - yet this is something humans do naturally and effortlessly. We explore how grid cell-based path integration in a cortical network can support reliable recognition of objects given an arbitrary sequence of inputs. Our network (GridCellNet) uses grid cell computations to integrate visual information and make predictions based on movements. We use local Hebbian plasticity rules to learn rapidly from a handful of examples (few-shot learning), and consider the task of recognizing MNIST digits given only a sequence of image feature patches. We compare GridCellNet to k-Nearest Neighbour (k-NN) classiﬁers as well as recurrent neural networks (RNNs), both of which lack explicit mechanisms for handling arbitrary sequences of input samples. We show that GridCellNet can reliably perform classiﬁcation, generalizing to both unseen examples and completely novel sequence trajectories. We further show that inference is often successful after sampling a fraction of the input space, enabling the predictive GridCellNet to reconstruct the rest of the image given just a few movements. We propose that dynamically moving agents with active sensors can use grid cell representations not only for navigation, but also for efﬁcient recognition and feature prediction of seen objects.},
	language = {en},
	urldate = {2021-07-25},
	journal = {arXiv:2102.09076 [cs]},
	author = {Leadholm, Niels and Lewis, Marcus and Ahmad, Subutai},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.09076},
}

@article{cui_continuous_2016,
	title = {Continuous {Online} {Sequence} {Learning} with an {Unsupervised} {Neural} {Network} {Model}},
	volume = {28},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/28/11/2474-2504/8502},
	doi = {10.1162/NECO_a_00893},
	abstract = {The ability to recognize and predict temporal sequences of sensory inputs is vital for survival in natural environments. Based on many known properties of cortical neurons, hierarchical temporal memory (HTM) sequence memory recently has been proposed as a theoretical framework for sequence learning in the cortex. In this letter, we analyze properties of HTM sequence memory and apply it to sequence learning and prediction problems with streaming data. We show the model is able to continuously learn a large number of variable order temporal sequences using an unsupervised Hebbian-like learning rule. The sparse temporal codes formed by the model can robustly handle branching temporal sequences by maintaining multiple predictions until there is sufficient disambiguating evidence. We compare the HTM sequence memory with other sequence learning algorithms, including statistical methods—autoregressive integrated moving average; feedforward neural networks—time delay neural network and online sequential extreme learning machine; and recurrent neural networks—long short-term memory and echo-state networks on sequence prediction problems with both artificial and real-world data. The HTM model achieves comparable accuracy to other state-of-the-art algorithms. The model also exhibits properties that are critical for sequence learning, including continuous online learning, the ability to handle multiple predictions and branching sequences with high-order statistics, robustness to sensor noise and fault tolerance, and good performance without task-specific hyperparameter tuning. Therefore, the HTM sequence memory not only advances our understanding of how the brain may solve the sequence learning problem but is also applicable to real-world sequence learning problems from continuous data streams.},
	language = {en},
	number = {11},
	urldate = {2021-07-25},
	journal = {Neural Computation},
	author = {Cui, Yuwei and Ahmad, Subutai and Hawkins, Jeff},
	month = nov,
	year = {2016},
	pages = {2474--2504},
}

@article{li_fast_2021,
	title = {A {Fast} {Spatial} {Pool} {Learning} {Algorithm} of {Hierarchical} {Temporal} {Memory} {Based} on {Minicolumn}’s {Self}-{Nomination}},
	volume = {2021},
	issn = {1687-5273, 1687-5265},
	url = {https://www.hindawi.com/journals/cin/2021/6680833/},
	doi = {10.1155/2021/6680833},
	abstract = {As a new type of artificial neural network model, HTM has become the focus of current research and application. The sparse distributed representation is the basis of the HTM model, but the existing spatial pool learning algorithms have high training time overhead and may cause the spatial pool to become unstable. To overcome these disadvantages, we propose a fast spatial pool learning algorithm of HTM based on minicolumn’s nomination, where the minicolumns are selected according to the load-carrying capacity and the synapses are adjusted using compressed encoding. We have implemented the prototype of the algorithm and carried out experiments on three datasets. It is verified that the training time overhead of the proposed algorithm is almost unaffected by the encoding length, and the spatial pool becomes stable after fewer iterations of training. Moreover, the training of the new input does not affect the already trained results.},
	language = {en},
	urldate = {2021-07-25},
	journal = {Computational Intelligence and Neuroscience},
	author = {Li, Lei and Zou, Tingting and Cai, Tao and Niu, Dejiao and Zhu, Yuquan},
	editor = {Namin, Akbar S.},
	month = mar,
	year = {2021},
	pages = {1--13},
}

@article{ryder_hierarchical_2021,
	title = {Hierarchical {Temporal} {Memory} {Continuous} {Learning} {Algorithms} for {Fire} {State} {Determination}},
	issn = {0015-2684, 1572-8099},
	url = {http://link.springer.com/10.1007/s10694-020-01055-0},
	doi = {10.1007/s10694-020-01055-0},
	abstract = {An ultimate goal of placing ﬁre detection systems in buildings and structures is to allow for the rapid detection of ﬁre and accurate faster than real time prediction of ensuing ﬁre behavior so that relevant information can be delivered to the appropriate stakeholders. In the near-term, development of detection systems with decreased detection time, better discrimination against nuisance and false alarms, and real-time monitoring of the ﬁre state is a critical interim step. Building comfort and eﬃciency systems are increasingly incorporating a greater quantity of sensors and these sensors are installed at a greater density than any ﬁre sensor with the exception of the sprinkler. While currently used primarily for building management purposes, the application of these, or similar types of building sensors, for rapid ﬁre detection, ﬁre state determination, and ﬁre forecasting oﬀers great potential. This paper discusses the potential beneﬁts of the application of Hierarchical Temporal Memory algorithms for ﬁre state determination in a continuous learning environment based on its application to a series of live ﬁre experiments.},
	language = {en},
	urldate = {2021-07-25},
	journal = {Fire Technology},
	author = {Ryder, Noah L. and Geiman, Justin A. and Weckman, Elizabeth J.},
	month = jan,
	year = {2021},
}

@article{klukas_efficient_2020,
	title = {Efficient and flexible representation of higher-dimensional cognitive variables with grid cells},
	volume = {16},
	issn = {1553-7358},
	url = {https://dx.plos.org/10.1371/journal.pcbi.1007796},
	doi = {10.1371/journal.pcbi.1007796},
	abstract = {We shed light on the potential of entorhinal grid cells to efficiently encode variables of dimension greater than two, while remaining faithful to empirical data on their low-dimensional structure. Our model constructs representations of high-dimensional inputs through a combination of low-dimensional random projections and “classical” low-dimensional hexagonal grid cell responses. Without reconfiguration of the recurrent circuit, the same system can flexibly encode multiple variables of different dimensions while maximizing the coding range (per dimension) by automatically trading-off dimension with an exponentially large coding range. It achieves high efficiency and flexibility by combining two powerful concepts, modularity and mixed selectivity, in what we call “mixed modular coding”. In contrast to previously proposed schemes, the model does not require the formation of higher-dimensional grid responses, a cell-inefficient and rigid mechanism. The firing fields observed in flying bats or climbing rats can be generated by neurons that combine activity from multiple grid modules, each representing higher-dimensional spaces according to our model. The idea expands our understanding of grid cells, suggesting that they could implement a general circuit that generates on-demand coding and memory states for variables in high-dimensional vector spaces.},
	language = {en},
	number = {4},
	urldate = {2021-07-25},
	journal = {PLOS Computational Biology},
	author = {Klukas, Mirko and Lewis, Marcus and Fiete, Ila},
	editor = {Bush, Daniel},
	month = apr,
	year = {2020},
	pages = {e1007796},
}

@article{lewis_locations_2019,
	title = {Locations in the {Neocortex}: {A} {Theory} of {Sensorimotor} {Object} {Recognition} {Using} {Cortical} {Grid} {Cells}},
	volume = {13},
	issn = {1662-5110},
	shorttitle = {Locations in the {Neocortex}},
	url = {https://www.frontiersin.org/article/10.3389/fncir.2019.00022/full},
	doi = {10.3389/fncir.2019.00022},
	abstract = {The neocortex is capable of anticipating the sensory results of movement but the neural mechanisms are poorly understood. In the entorhinal cortex, grid cells represent the location of an animal in its environment, and this location is updated through movement and path integration. In this paper, we propose that sensory neocortex incorporates movement using grid cell-like neurons that represent the location of sensors on an object. We describe a two-layer neural network model that uses cortical grid cells and path integration to robustly learn and recognize objects through movement and predict sensory stimuli after movement. A layer of cells consisting of several grid celllike modules represents a location in the reference frame of a speciﬁc object. Another layer of cells which processes sensory input receives this location input as context and uses it to encode the sensory input in the object’s reference frame. Sensory input causes the network to invoke previously learned locations that are consistent with the input, and motor input causes the network to update those locations. Simulations show that the model can learn hundreds of objects even when object features alone are insufﬁcient for disambiguation. We discuss the relationship of the model to cortical circuitry and suggest that the reciprocal connections between layers 4 and 6 ﬁt the requirements of the model. We propose that the subgranular layers of cortical columns employ grid cell-like mechanisms to represent object speciﬁc locations that are updated through movement.},
	language = {en},
	urldate = {2021-07-25},
	journal = {Frontiers in Neural Circuits},
	author = {Lewis, Marcus and Purdy, Scott and Ahmad, Subutai and Hawkins, Jeff},
	month = apr,
	year = {2019},
	pages = {22},
}

@article{ahmad_how_2019,
	title = {How {Can} {We} {Be} {So} {Dense}? {The} {Benefits} of {Using} {Highly} {Sparse} {Representations}},
	shorttitle = {How {Can} {We} {Be} {So} {Dense}?},
	url = {http://arxiv.org/abs/1903.11257},
	abstract = {Most artiﬁcial networks today rely on dense representations, whereas biological networks rely on sparse representations. In this paper we show how sparse representations can be more robust to noise and interference, as long as the underlying dimensionality is sufﬁciently high. A key intuition that we develop is that the ratio of the operable volume around a sparse vector divided by the volume of the representational space decreases exponentially with dimensionality. We then analyze computationally efﬁcient sparse networks containing both sparse weights and activations. Simulations on MNIST and the Google Speech Command Dataset show that such networks demonstrate signiﬁcantly improved robustness and stability compared to dense networks, while maintaining competitive accuracy. We discuss the potential beneﬁts of sparsity on accuracy, noise robustness, hyperparameter tuning, learning speed, computational efﬁciency, and power requirements.},
	language = {en},
	urldate = {2021-07-25},
	journal = {arXiv:1903.11257 [cs, stat]},
	author = {Ahmad, Subutai and Scheinkman, Luiz},
	month = apr,
	year = {2019},
	note = {arXiv: 1903.11257},
}

@article{hawkins_framework_2019,
	title = {A {Framework} for {Intelligence} and {Cortical} {Function} {Based} on {Grid} {Cells} in the {Neocortex}},
	volume = {12},
	issn = {1662-5110},
	url = {https://www.frontiersin.org/article/10.3389/fncir.2018.00121/full},
	doi = {10.3389/fncir.2018.00121},
	abstract = {How the neocortex works is a mystery. In this paper we propose a novel framework for understanding its function. Grid cells are neurons in the entorhinal cortex that represent the location of an animal in its environment. Recent evidence suggests that grid celllike neurons may also be present in the neocortex. We propose that grid cells exist throughout the neocortex, in every region and in every cortical column. They deﬁne a location-based framework for how the neocortex functions. Whereas grid cells in the entorhinal cortex represent the location of one thing, the body relative to its environment, we propose that cortical grid cells simultaneously represent the location of many things. Cortical columns in somatosensory cortex track the location of tactile features relative to the object being touched and cortical columns in visual cortex track the location of visual features relative to the object being viewed. We propose that mechanisms in the entorhinal cortex and hippocampus that evolved for learning the structure of environments are now used by the neocortex to learn the structure of objects. Having a representation of location in each cortical column suggests mechanisms for how the neocortex represents object compositionality and object behaviors. It leads to the hypothesis that every part of the neocortex learns complete models of objects and that there are many models of each object distributed throughout the neocortex. The similarity of circuitry observed in all cortical regions is strong evidence that even high-level cognitive tasks are learned and represented in a location-based framework.},
	language = {en},
	urldate = {2021-07-25},
	journal = {Frontiers in Neural Circuits},
	author = {Hawkins, Jeff and Lewis, Marcus and Klukas, Mirko and Purdy, Scott and Ahmad, Subutai},
	month = jan,
	year = {2019},
	pages = {121},
}

@article{cui_htm_2017,
	title = {The {HTM} {Spatial} {Pooler}—{A} {Neocortical} {Algorithm} for {Online} {Sparse} {Distributed} {Coding}},
	volume = {11},
	issn = {1662-5188},
	url = {http://journal.frontiersin.org/article/10.3389/fncom.2017.00111/full},
	doi = {10.3389/fncom.2017.00111},
	abstract = {Hierarchical temporal memory (HTM) provides a theoretical framework that models several key computational principles of the neocortex. In this paper, we analyze an important component of HTM, the HTM spatial pooler (SP). The SP models how neurons learn feedforward connections and form efﬁcient representations of the input. It converts arbitrary binary input patterns into sparse distributed representations (SDRs) using a combination of competitive Hebbian learning rules and homeostatic excitability control. We describe a number of key properties of the SP, including fast adaptation to changing input statistics, improved noise robustness through learning, efﬁcient use of cells, and robustness to cell death. In order to quantify these properties we develop a set of metrics that can be directly computed from the SP outputs. We show how the properties are met using these metrics and targeted artiﬁcial simulations. We then demonstrate the value of the SP in a complete end-to-end real-world HTM system. We discuss the relationship with neuroscience and previous studies of sparse coding. The HTM spatial pooler represents a neurally inspired algorithm for learning sparse representations from noisy data streams in an online fashion.},
	language = {en},
	urldate = {2021-07-25},
	journal = {Frontiers in Computational Neuroscience},
	author = {Cui, Yuwei and Ahmad, Subutai and Hawkins, Jeff},
	month = nov,
	year = {2017},
	pages = {111},
}

@techreport{hawkins_why_2017,
	type = {preprint},
	title = {Why {Does} the {Neocortex} {Have} {Columns}, {A} {Theory} of {Learning} the {Structure} of the {World}},
	url = {http://biorxiv.org/lookup/doi/10.1101/162263},
	abstract = {Neocortical regions are organized into columns and layers. Connections between layers run mostly perpendicular to the surface suggesting a columnar functional organization. Some layers have long-range excitatory lateral connections suggesting interactions between columns. Similar patterns of connectivity exist in all regions but their exact role remain a mystery. In this paper, we propose a network model composed of columns and layers that performs robust object learning and recognition. Each column integrates its changing input over time to learn complete predictive models of observed objects. Excitatory lateral connections across columns allow the network to more rapidly infer objects based on the partial knowledge of adjacent columns. Because columns integrate input over time and space, the network learns models of complex objects that extend well beyond the receptive field of individual cells. Our network model introduces a new feature to cortical columns. We propose that a representation of location relative to the object being sensed is calculated within the sub-granular layers of each column. The location signal is provided as an input to the network, where it is combined with sensory data. Our model contains two layers and one or more columns. Simulations show that using Hebbian-like learning rules small single-column networks can learn to recognize hundreds of objects, with each object containing tens of features. Multi-column networks recognize objects with significantly fewer movements of the sensory receptors. Given the ubiquity of columnar and laminar connectivity patterns throughout the neocortex, we propose that columns and regions have more powerful recognition and modeling capabilities than previously assumed.},
	language = {en},
	urldate = {2021-07-25},
	institution = {Neuroscience},
	author = {Hawkins, Jeff and Ahmad, Subutai and Cui, Yuwei},
	month = jul,
	year = {2017},
	doi = {10.1101/162263},
}

@techreport{ahmad_untangling_2017,
	type = {preprint},
	title = {Untangling {Sequences}: {Behavior} vs. {External} {Causes}},
	shorttitle = {Untangling {Sequences}},
	url = {http://biorxiv.org/lookup/doi/10.1101/190678},
	abstract = {There are two fundamental reasons why sensory inputs to the brain change over time. Sensory inputs can change due to external factors or they can change due to our own behavior. Interpreting behavior-generated changes requires knowledge of how the body is moving, whereas interpreting externallygenerated changes relies solely on the temporal sequence of input patterns. The sensory signals entering the neocortex change due to a mixture of both behavior and external factors. The neocortex must disentangle them but the mechanisms are unknown. In this paper, we show that a single neural mechanism can learn and recognize both types of sequences. In the model, cells are driven by feedforward sensory input and are modulated by contextual input. If the contextual input includes information derived from efference motor copies, the cells learn sensorimotor sequences. If the contextual input consists of nearby cellular activity, the cells learn temporal sequences. Through simulation we show that a network containing both types of contextual input automatically separates and learns both types of input patterns. We review experimental data that suggests the upper layers of cortical regions contain the anatomical structure required to support this mechanism.},
	language = {en},
	urldate = {2021-07-25},
	institution = {Neuroscience},
	author = {Ahmad, Subutai and Hawkins, Jeff},
	month = sep,
	year = {2017},
	doi = {10.1101/190678},
}

@article{ahmad_unsupervised_2017,
	title = {Unsupervised real-time anomaly detection for streaming data},
	volume = {262},
	issn = {09252312},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231217309864},
	doi = {10.1016/j.neucom.2017.04.070},
	language = {en},
	urldate = {2021-07-25},
	journal = {Neurocomputing},
	author = {Ahmad, Subutai and Lavin, Alexander and Purdy, Scott and Agha, Zuha},
	month = nov,
	year = {2017},
	pages = {134--147},
}

@article{cui_htm_2017-1,
	title = {The {HTM} {Spatial} {Pooler}—{A} {Neocortical} {Algorithm} for {Online} {Sparse} {Distributed} {Coding}},
	volume = {11},
	issn = {1662-5188},
	url = {http://journal.frontiersin.org/article/10.3389/fncom.2017.00111/full},
	doi = {10.3389/fncom.2017.00111},
	abstract = {Hierarchical temporal memory (HTM) provides a theoretical framework that models several key computational principles of the neocortex. In this paper, we analyze an important component of HTM, the HTM spatial pooler (SP). The SP models how neurons learn feedforward connections and form efﬁcient representations of the input. It converts arbitrary binary input patterns into sparse distributed representations (SDRs) using a combination of competitive Hebbian learning rules and homeostatic excitability control. We describe a number of key properties of the SP, including fast adaptation to changing input statistics, improved noise robustness through learning, efﬁcient use of cells, and robustness to cell death. In order to quantify these properties we develop a set of metrics that can be directly computed from the SP outputs. We show how the properties are met using these metrics and targeted artiﬁcial simulations. We then demonstrate the value of the SP in a complete end-to-end real-world HTM system. We discuss the relationship with neuroscience and previous studies of sparse coding. The HTM spatial pooler represents a neurally inspired algorithm for learning sparse representations from noisy data streams in an online fashion.},
	language = {en},
	urldate = {2021-07-25},
	journal = {Frontiers in Computational Neuroscience},
	author = {Cui, Yuwei and Ahmad, Subutai and Hawkins, Jeff},
	month = nov,
	year = {2017},
	pages = {111},
}

@article{hawkins_theory_2017,
	title = {A {Theory} of {How} {Columns} in the {Neocortex} {Enable} {Learning} the {Structure} of the {World}},
	volume = {11},
	issn = {1662-5110},
	url = {http://journal.frontiersin.org/article/10.3389/fncir.2017.00081/full},
	doi = {10.3389/fncir.2017.00081},
	abstract = {Neocortical regions are organized into columns and layers. Connections between layers run mostly perpendicular to the surface suggesting a columnar functional organization. Some layers have long-range excitatory lateral connections suggesting interactions between columns. Similar patterns of connectivity exist in all regions but their exact role remain a mystery. In this paper, we propose a network model composed of columns and layers that performs robust object learning and recognition. Each column integrates its changing input over time to learn complete predictive models of observed objects. Excitatory lateral connections across columns allow the network to more rapidly infer objects based on the partial knowledge of adjacent columns. Because columns integrate input over time and space, the network learns models of complex objects that extend well beyond the receptive ﬁeld of individual cells. Our network model introduces a new feature to cortical columns. We propose that a representation of location relative to the object being sensed is calculated within the sub-granular layers of each column. The location signal is provided as an input to the network, where it is combined with sensory data. Our model contains two layers and one or more columns. Simulations show that using Hebbian-like learning rules small single-column networks can learn to recognize hundreds of objects, with each object containing tens of features. Multi-column networks recognize objects with signiﬁcantly fewer movements of the sensory receptors. Given the ubiquity of columnar and laminar connectivity patterns throughout the neocortex, we propose that columns and regions have more powerful recognition and modeling capabilities than previously assumed.},
	language = {en},
	urldate = {2021-07-25},
	journal = {Frontiers in Neural Circuits},
	author = {Hawkins, Jeff and Ahmad, Subutai and Cui, Yuwei},
	month = oct,
	year = {2017},
	pages = {81},
}

@article{hawkins_why_2016,
	title = {Why {Neurons} {Have} {Thousands} of {Synapses}, a {Theory} of {Sequence} {Memory} in {Neocortex}},
	volume = {10},
	issn = {1662-5110},
	url = {http://journal.frontiersin.org/Article/10.3389/fncir.2016.00023/abstract},
	doi = {10.3389/fncir.2016.00023},
	abstract = {Pyramidal neurons represent the majority of excitatory neurons in the neocortex. Each pyramidal neuron receives input from thousands of excitatory synapses that are segregated onto dendritic branches. The dendrites themselves are segregated into apical, basal, and proximal integration zones, which have different properties. It is a mystery how pyramidal neurons integrate the input from thousands of synapses, what role the different dendrites play in this integration, and what kind of network behavior this enables in cortical tissue. It has been previously proposed that non-linear properties of dendrites enable cortical neurons to recognize multiple independent patterns. In this paper we extend this idea in multiple ways. First we show that a neuron with several thousand synapses segregated on active dendrites can recognize hundreds of independent patterns of cellular activity even in the presence of large amounts of noise and pattern variation. We then propose a neuron model where patterns detected on proximal dendrites lead to action potentials, deﬁning the classic receptive ﬁeld of the neuron, and patterns detected on basal and apical dendrites act as predictions by slightly depolarizing the neuron without generating an action potential. By this mechanism, a neuron can predict its activation in hundreds of independent contexts. We then present a network model based on neurons with these properties that learns time-based sequences. The network relies on fast local inhibition to preferentially activate neurons that are slightly depolarized. Through simulation we show that the network scales well and operates robustly over a wide range of parameters as long as the network uses a sparse distributed code of cellular activations. We contrast the properties of the new network model with several other neural network models to illustrate the relative capabilities of each. We conclude that pyramidal neurons with thousands of synapses, active dendrites, and multiple integration zones create a robust and powerful sequence memory. Given the prevalence and similarity of excitatory neurons throughout the neocortex and the importance of sequence memory in inference and behavior, we propose that this form of sequence memory may be a universal property of neocortical tissue.},
	language = {en},
	urldate = {2021-07-25},
	journal = {Frontiers in Neural Circuits},
	author = {Hawkins, Jeff and Ahmad, Subutai},
	month = mar,
	year = {2016},
}

@article{billaudelle_porting_2016,
	title = {Porting {HTM} {Models} to the {Heidelberg} {Neuromorphic} {Computing} {Platform}},
	url = {http://arxiv.org/abs/1505.02142},
	abstract = {Hierarchical Temporal Memory (HTM) is a computational theory of machine intelligence based on a detailed study of the neocortex. The Heidelberg Neuromorphic Computing Platform, developed as part of the Human Brain Project (HBP), is a mixed-signal (analog and digital) large-scale platform for modeling networks of spiking neurons. In this paper we present the rst e ort in porting HTM networks to this platform. We describe a framework for simulating key HTM operations using spiking network models. We then describe speci c spatial pooling and temporal memory implementations, as well as simulations demonstrating that the fundamental properties are maintained. We discuss issues in implementing the full set of plasticity rules using SpikeTiming Dependent Plasticity (STDP), and rough place and route calculations. Although further work is required, our initial studies indicate that it should be possible to run large-scale HTM networks (including plasticity rules) e ciently on the Heidelberg platform. More generally the exercise of porting high level HTM algorithms to biophysical neuron models promises to be a fruitful area of investigation for future studies.},
	language = {en},
	urldate = {2021-07-25},
	journal = {arXiv:1505.02142 [cs, q-bio]},
	author = {Billaudelle, Sebastian and Ahmad, Subutai},
	month = feb,
	year = {2016},
	note = {arXiv: 1505.02142},
}

@inproceedings{cui_comparative_2016,
	address = {Vancouver, BC, Canada},
	title = {A comparative study of {HTM} and other neural network models for online sequence learning with streaming data},
	isbn = {978-1-5090-0620-5},
	url = {http://ieeexplore.ieee.org/document/7727380/},
	doi = {10.1109/IJCNN.2016.7727380},
	abstract = {Online sequence learning from streaming data is one of the most challenging topics in machine learning. Neural network models represent promising candidates for sequence learning due to their ability to learn and recognize complex temporal patterns. In this paper, we present a comparative study of Hierarchical Temporal Memory (HTM), a neurallyinspired model, and other feedforward and recurrent artificial neural network models on both artificial and real-world sequence prediction algorithms. HTM and long-short term memory (LSTM) give the best prediction accuracy. HTM additionally demonstrates many other features that are desirable for real-world sequence learning, such as fast adaptation to changes in the data stream, robustness to sensor noise and fault tolerance. These features make HTM an ideal candidate for online sequence learning problems.},
	language = {en},
	urldate = {2021-07-25},
	booktitle = {2016 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	publisher = {IEEE},
	author = {Cui, Yuwei and Surpur, Chetan and Ahmad, Subutai and Hawkins, Jeff},
	month = jul,
	year = {2016},
	pages = {1530--1538},
}

@article{ahmad_properties_nodate,
	title = {Properties of {Sparse} {Distributed} {Representations} and their {Application} to {Hierarchical} {Temporal} {Memory}},
	abstract = {Empirical evidence demonstrates that every region of the neocortex represents information using sparse activity patterns. This paper examines Sparse Distributed Representations (SDRs), the primary information representation strategy in Hierarchical Temporal Memory (HTM) systems and the neocortex. We derive a number of properties that are core to scaling, robustness, and generalization. We use the theory to provide practical guidelines and illustrate the power of SDRs as the basis of HTM. Our goal is to help create a unified mathematical and practical framework for SDRs as it relates to cortical function.},
	language = {en},
	author = {Ahmad, Subutai and Hawkins, Jeff},
	pages = {18},
}

@inproceedings{lavin_evaluating_2015,
	address = {Miami, FL, USA},
	title = {Evaluating {Real}-{Time} {Anomaly} {Detection} {Algorithms} -- {The} {Numenta} {Anomaly} {Benchmark}},
	isbn = {978-1-5090-0287-0},
	url = {http://ieeexplore.ieee.org/document/7424283/},
	doi = {10.1109/ICMLA.2015.141},
	abstract = {Much of the world’s data is streaming, time-series data, where anomalies give significant information in critical situations; examples abound in domains such as finance, IT, security, medical, and energy. Yet detecting anomalies in streaming data is a difficult task, requiring detectors to process data in real-time, not batches, and learn while simultaneously making predictions. There are no benchmarks to adequately test and score the efficacy of real-time anomaly detectors. Here we propose the Numenta Anomaly Benchmark (NAB), which attempts to provide a controlled and repeatable environment of open-source tools to test and measure anomaly detection algorithms on streaming data. The perfect detector would detect all anomalies as soon as possible, trigger no false alarms, work with real-world time-series data across a variety of domains, and automatically adapt to changing statistics. Rewarding these characteristics is formalized in NAB, using a scoring algorithm designed for streaming data. NAB evaluates detectors on a benchmark dataset with labeled, real-world time-series data. We present these components, and give results and analyses for several open source, commercially-used algorithms. The goal for NAB is to provide a standard, open source framework with which the research community can compare and evaluate different algorithms for detecting anomalies in streaming data.},
	language = {en},
	urldate = {2021-07-25},
	booktitle = {2015 {IEEE} 14th {International} {Conference} on {Machine} {Learning} and {Applications} ({ICMLA})},
	publisher = {IEEE},
	author = {Lavin, Alexander and Ahmad, Subutai},
	month = dec,
	year = {2015},
	pages = {38--44},
}

@article{byrne_encoding_2015,
	title = {Encoding {Reality}: {Prediction}-{Assisted} {Cortical} {Learning} {Algorithm} in {Hierarchical} {Temporal} {Memory}},
	shorttitle = {Encoding {Reality}},
	url = {http://arxiv.org/abs/1509.08255},
	abstract = {In the decade since Jeff Hawkins proposed Hierarchical Temporal Memory (HTM) as a model of neocortical computation, the theory and the algorithms have evolved dramatically. This paper presents a detailed description of HTM's Cortical Learning Algorithm (CLA), including for the first time a rigorous mathematical formulation of all aspects of the computations. Prediction Assisted CLA (paCLA), a refinement of the CLA is presented, which is both closer to the neuroscience and adds significantly to the computational power. Finally, we summarise the key functions of neocortex which are expressed in paCLA implementations.},
	language = {en},
	urldate = {2021-07-25},
	journal = {arXiv:1509.08255 [cs]},
	author = {Byrne, Fergal},
	month = oct,
	year = {2015},
	note = {arXiv: 1509.08255},
}

@article{chen_overview_2012,
	title = {An {Overview} of {Hierarchical} {Temporal} {Memory}: {A} {New} {Neocortex} {Algorithm}},
	abstract = {The overview presents the development and application of Hierarchical Temporal Memory (HTM). HTM is a new machine learning method which was proposed by Jeff Hawkins in 2005. It is a biologically inspired cognitive method based on the principle of how human brain works. The method invites hierarchical structure and proposes a memory-prediction framework, thus making it able to predict what will happen in the near future. This overview mainly introduces the developing process of HTM, as well as its principle, characteristics, advantages and applications in vision, image processing and robots movement, some potential applications by using HTM , such as thinking process, are also put forward.},
	language = {en},
	journal = {Identification and Control},
	author = {Chen, Xi and Wang, Wei and Li, Wei},
	year = {2012},
	pages = {7},
}

@article{noauthor_htm_2011,
	title = {{HTM} {Cortical} {Learning} {Algorithms}},
	language = {en},
	year = {2011},
	pages = {68},
}


@misc{larocsgit,
  title={LaRoCS Github Page},
  year={2020},
  howpublished = {\url{https://github.com/larocs}},
  note = {Accessed: 2020-07-30}
}
@phdthesis{padilha,
  author    = {Antônio Padilha Lanari Bó},
  title     = {Compensation active de tremblements pathologiques des membres supérieurs via la stimulation électrique fonctionnelle},
  school	= {Université Montpellier II},
  year      = {2010}
}

@inproceedings{schwab2019phonemd,
  title={PhoneMD: Learning to diagnose Parkinson’s disease from smartphone data},
  author={Schwab, Patrick and Karlen, Walter},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  pages={1118--1125},
  year={2019}
}

@article{grover2018predicting,
  title={Predicting severity of Parkinson’s disease using deep learning},
  author={Grover, Srishti and Bhartia, Saloni and Yadav, Abhilasha and Seeja, KR and others},
  journal={Procedia computer science},
  volume={132},
  pages={1788--1794},
  year={2018},
  publisher={Elsevier}
}

@article{pfister2020high,
  title={High-Resolution Motor State Detection in parkinson’s Disease Using convolutional neural networks},
  author={Pfister, Franz MJ and Um, Terry Taewoong and Pichler, Daniel C and Goschenhofer, Jann and Abedinpour, Kian and Lang, Muriel and Endo, Satoshi and Ceballos-Baumann, Andres O and Hirche, Sandra and Bischl, Bernd and others},
  journal={Scientific reports},
  volume={10},
  number={1},
  pages={1--11},
  year={2020},
  publisher={Nature Publishing Group}
}

@article{sivaranjini2019deep,
  title={Deep learning based diagnosis of Parkinson’s disease using convolutional neural network},
  author={Sivaranjini, S and Sujatha, CM},
  journal={Multimedia Tools and Applications},
  pages={1--13},
  year={2019},
  publisher={Springer}
}

@masterthesis{esther,
  author={Esther Luna Colombini},
  title={Module-based Learning in Autonomous Mobile Robotics},
  school={ITA}, 
  year={2005}
 }

@article{BRUNO202197,
title = {LIFT-SLAM: A deep-learning feature-based monocular visual SLAM method},
journal = {Neurocomputing},
volume = {455},
pages = {97-110},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.05.027},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221007803},
author = {Hudson Martins Silva Bruno and Esther Luna Colombini},
keywords = {Mobile robots, Visual SLAM, Deep neural networks, Learned features},
abstract = {The Simultaneous Localization and Mapping (SLAM) problem addresses the possibility of a robot to localize itself in an unknown environment and simultaneously build a consistent map of this environment. Recently, cameras have been successfully used to get the environment’s features to perform SLAM, which is referred to as visual SLAM (VSLAM). However, classical VSLAM algorithms can be easily induced to fail when either the motion of the robot or the environment is too challenging. Although new approaches based on Deep Neural Networks (DNNs) have achieved promising results in VSLAM, they still are unable to outperform traditional methods. To leverage the robustness of deep learning to enhance traditional VSLAM systems, we propose to combine the potential of deep learning-based feature descriptors with the traditional geometry-based VSLAM, building a new VSLAM system called LIFT-SLAM. Experiments conducted on KITTI and Euroc datasets show that deep learning can be used to improve the performance of traditional VSLAM systems, as the proposed approach was able to achieve results comparable to the state-of-the-art while being robust to sensorial noise. We enhance the proposed VSLAM pipeline by avoiding parameter tuning for specific datasets with an adaptive approach while evaluating how transfer learning can affect the quality of the features extracted.}
}

@article{colombini_attentional_2017,
	title = {An {Attentional} {Model} for {Autonomous} {Mobile} {Robots}},
	volume = {11},
	issn = {1932-8184, 1937-9234, 2373-7816},
	url = {http://ieeexplore.ieee.org/document/7384702/},
	doi = {10.1109/JSYST.2015.2499304},
	language = {en},
	number = {3},
	urldate = {2019-04-07},
	journal = {IEEE Systems Journal},
	author = {Colombini, Esther Luna and da Silva Simoes, Alexandre and Costa Ribeiro, Carlos Henrique},
	month = sep,
	year = {2017},
	pages = {1308--1319}
}

@article{silva_simoes_conaim:_2017,
	title = {{CONAIM}: {A} {Conscious} {Attention}-{Based} {Integrated} {Model} for {Human}-{Like} {Robots}},
	volume = {11},
	issn = {1932-8184, 1937-9234, 2373-7816},
	shorttitle = {{CONAIM}},
	url = {http://ieeexplore.ieee.org/document/7383269/},
	doi = {10.1109/JSYST.2015.2498542},
	language = {en},
	number = {3},
	urldate = {2019-04-07},
	journal = {IEEE Systems Journal},
	author = {Silva Simoes, Alexandre da and Colombini, Esther Luna and Costa Ribeiro, Carlos Henrique},
	month = sep,
	year = {2017},
	pages = {1296--1307}
}

@inproceedings{colombini_top-down_2013,
	title = {Top-down and {Bottom}-up {Feature} {Combination} for {Multi}-sensor {Attentive} {Robots}},
	booktitle = {ISACS 2013 Proceedings},
	author = {Colombini, Esther L. and Simões, Alexandre S. and Ribeiro, Carlos H. C.},
	month = jul,
	year = {2013}
}


@article{anicet2020parkinson,
  title={Parkinson’s Disease EMG Data Augmentation and Simulation with DCGANs and Style Transfer},
  author={Anicet Zanini, Rafael and Luna Colombini, Esther},
  journal={Sensors},
  volume={20},
  number={9},
  pages={2605},
  year={2020},
  publisher={Multidisciplinary Digital Publishing Institute}
}

@INPROCEEDINGS{8588600,
  author={Cano Lopes, Guilherme and Ferreira, Murillo and da Silva Simões, Alexandre and Luna Colombini, Esther},
  booktitle={2018 Latin American Robotic Symposium, 2018 Brazilian Symposium on Robotics (SBR) and 2018 Workshop on Robotics in Education (WRE)}, 
  title={Intelligent Control of a Quadrotor with Proximal Policy Optimization Reinforcement Learning}, 
  year={2018},
  volume={},
  number={},
  pages={503-508},
  doi={10.1109/LARS/SBR/WRE.2018.00094}}
  
@misc{cleveston2021ramvo,
      title={RAM-VO: Less is more in Visual Odometry}, 
      author={Iury Cleveston and Esther L. Colombini},
      year={2021},
      eprint={2107.02974},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}

@article{zhang2011neural,
  title={Neural oscillator based control for pathological tremor suppression via functional electrical stimulation},
  author={Zhang, Dingguo and Poignet, Philippe and Widjaja, Ferdinan and Ang, Wei Tech},
  journal={Control Engineering Practice},
  volume={19},
  number={1},
  pages={74--88},
  year={2011},
  publisher={Elsevier}
}

@inproceedings{zanini2019parkinson,
  title={Parkinson’s Disease EMG Signal Prediction Using Neural Networks},
  author={Zanini, Rafael Anicet and Colombini, Esther Luna and de Castro, Maria Claudia Ferrari},
  booktitle={2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)},
  pages={2446--2453},
  year={2019},
  organization={IEEE}
}


@article{fritsch2012parkinson,
  title={Parkinson disease: research update and clinical management},
  author={Fritsch, Thomas and Smyth, Kathleen A and Wallendal, Maggie S and Hyde, Trevor and Leo, Gary and Geldmacher, David S},
  journal={South Med J},
  volume={105},
  number={12},
  pages={650--656},
  year={2012}
}

@article{peckham2005functional,
  title={Functional electrical stimulation for neuromuscular applications},
  author={Peckham, P Hunter and Knutson, Jayme S},
  journal={Annu. Rev. Biomed. Eng.},
  volume={7},
  pages={327--360},
  year={2005},
  publisher={Annual Reviews}
}


@book{world2006neurological,
  title={Neurological disorders: public health challenges},
  author={World Health Organization},
  year={2006},
  publisher={World Health Organization}
}

@article{kostic2016pathophysiology,
  title={The pathophysiology of fatigue in Parkinson's disease and its pragmatic management},
  author={Kosti{\'c}, Vladimir S and Tomi{\'c}, Aleksandra and Je{\v{c}}menica-Luki{\'c}, Milica},
  journal={Movement disorders clinical practice},
  volume={3},
  number={4},
  pages={323--330},
  year={2016},
  publisher={Wiley Online Library}
}


@inproceedings{sukhbaatar2015end,
  title={End-to-end memory networks},
  author={Sukhbaatar, Sainbayar and Weston, Jason and Fergus, Rob and others},
  booktitle={Advances in neural information processing systems},
  pages={2440--2448},
  year={2015}
}


@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}


@article{hawkins2017theory,
  title={A theory of how columns in the neocortex enable learning the structure of the world},
  author={Hawkins, Jeff and Ahmad, Subutai and Cui, Yuwei},
  journal={Frontiers in neural circuits},
  pages={81},
  year={2017},
  publisher={Frontiers}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}


@article{hussein2017imitation,
  title={Imitation learning: A survey of learning methods},
  author={Hussein, Ahmed and Gaber, Mohamed Medhat and Elyan, Eyad and Jayne, Chrisina},
  journal={ACM Computing Surveys (CSUR)},
  volume={50},
  number={2},
  pages={1--35},
  year={2017},
  publisher={ACM New York, NY, USA}
}

@article{lecun2022path,
  title={A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27},
  author={LeCun, Yann},
  journal={Open Review},
  volume={62},
  year={2022}
}

