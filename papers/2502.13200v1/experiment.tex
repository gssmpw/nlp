\section{Experiments}
\label{sec:experiment}

In this section, we conduct a set of experiments to evaluate our proposed method.

\subsection{Experiment Setup}
\label{experiment_setup}

\textbf{Environments.} We evaluate our method on 18 Atari Games, a standard setup for evaluating Reinforcement Learning methods. We choose Atari games with varying reward types (i.e., dense, sparse, and hybrid) and cognitive skills that the agent needs to learn. We aim to evaluate the agent in different games to analyze our approach's positive and negative points. Some games chosen are dynamic and require the agent to show fast and efficient reactive behaviors to survive in the environment. In contrast, other games are strategy games and require the agent to think of a plan to achieve a specific goal. 

%\textbf{Pre-processing.} 

\textbf{Architecture and pre-processing.} We used the PPO (Proximal Policy Optimization) algorithm \cite{schulman2017proximal}, a robust learning algorithm that requires little hyperparameter tuning, for our experiments. While training with PPO, we normalize the advantages \cite{sutton2018reinforcement} in a batch to have a mean of 0 and a standard deviation of 1. We use the same architecture to train all games. We use one layer of Recurrent Independent Mechanisms in the predictive model with eight modules, only 4 of which can be on active/expected states in each time step $t$. Two modules represent the current state $s_{t}$, two generate the expected representation to $s_{t+1}$, and four are inactive. Each module has a size of 32. All experiments were carried out in the pixels space. We converted all images to grayscale and resized them to 84 $\times$ 84. We represent the state as a stack of historical observations $\left [ x_{t-3}, x_{t-2}, x_{t-1}, x_{t}\right ]$ to deal with partial observability. We also use the standard 4-frame frameskip. Unlike the classical methods, we perform a simple state normalization by dividing by 255.


\textbf{Hyperparameters and implementation details.} We used a learning rate of 0.00025 for all networks. In all experiments, we used 128 parallel environments, rollouts of length 128, and four epochs. During training, we limited the maximum episode size to 4,500 steps to avoid the collapse of the learned policy. We train most models with 170 million steps. However, we noticed that our model quickly surpassed the state of the art, and we reduced some training steps to 50 million. We consider death endgame in all experiments.



\textbf{Evaluations.} To evaluate the agents' performance, we use the extrinsic reward, which is the game's score at the end of each episode. After training, we put the agents to play ten games with different seeds from those used during training. Finally, we computed the mean and standard deviation of the game scores obtained.


\textbf{Hardware and Software Configurations.} We implemented our method with PyTorch 1.3.1 and CUDA v11.1. We conducted experiments on Nvidia RTX 8000 with 48Gb, motherboard Asus Rog Strix Z490-E Gaming, Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz, RAM Corsair DDR4 125Gb Gb @ 3600MHz, disk Western Digital 1Tb, and Operating System Ubuntu 20.04.3 LTS. 


%-------------------------------------------------------------------------
\subsection{Results}


In this section, we present the results obtained from our experiments. Our code is publicly available in our repository\footnote[1]{https://github.com/XXXXX/XXXXX}. We compare our results with those obtained by the model proposed by Pathak et al. \cite{pathak2017curiosity}, which was further explored by Burda et al. \cite{burda2018large} using Atari games. We chose this model for comparison among many other works because it is a state-of-art approach to comparing results. In addition, it presents an extensive study of Atari games and is what most relates to our method. Our experiments showed that the observation normalization process adopted in \cite{burda2018large} typically resulted in policy collapse after 50 million training steps. While this is an undesired behavior, we adopted the same normalization strategy (Section \ref{experiment_setup}) for comparison purposes. We trained 18 Atari games with different gameplay features and reward patterns. All results are shown in Table \ref{tab:tabela_resultados}. The results show that we scored quite significant games on $90\%$ on cases of the test.

\begin{table}[htb]
\centering
\begin{tabular}{ccccc}
\hline
Environment       & Ours                  & Burda \\ \hline
MsPacman     &  1358.0 $\pm$ 439.7 & 380.0 $\pm$ 113.3   \\ 
Atlantis   &         47540.0 $\pm$ 7804.1               &   9800.0 $\pm$ 4475.2                          \\
Freeway    &             3.2 $\pm$ 1.3                &     1.0 $\pm$ 0.29                               \\
Asterix    &           2465.0 $\pm$ 1235.9               &      110. $\pm$ 48.9                        \\
RiverRaid  &             4123.0 $\pm$ 1837.8               &    798.0 $\pm$ 299.1                        \\
Asteroids  &            821.0 $\pm$ 273.7              &        543.0 $\pm$ 192.6                          \\
Jamesbond  &                245.0 $\pm$ 89.0               &        35.0 $\pm$ 19.0                           \\
Centipede  &                6128.0 $\pm$ 2709.0               &      1620.9 $\pm$ 983.2                          \\
KungFuMaster     &              510.0 $\pm$ 361.8               &     80.0 $\pm$ 10.8                         \\
Solaris    &                 1658.0 $\pm$ 564.0               &       2190.0 $\pm$ 562.0                 \\
Pitfall    &              -366.9 $\pm$ 317.3               &        -512.2 $\pm$ 489.9                             \\
Alien      &                536.0 $\pm$ 150.0               &         291 $\pm$ 83.8                            \\
Robotank   &               7.8 $\pm$ 3.9                 &           2.6 $\pm$ 1.2                    \\
BeamRider  &            988.0 $\pm$ 437.8                &          347.6 $\pm$ 123.5                              \\
Amidar     &            65.8 $\pm$ 15.6                 &          14.1 $\pm$ 3.1                        \\
BattleZone &            6300.0 $\pm$ 4838.3                 &   1900.0 $\pm$ 577.                                     \\
Seaquest   &            402.0 $\pm$ 60.9                   &         88.0 $\pm$ 14.4                           \\
Gravitar   &            215.0 $\pm$ 184.4                 &        280 $\pm$ 106.0                      \\ \hline
\end{tabular}
\caption{Our results on the Atari games. We used, as a result, the mean and standard deviation of scores received in ten games. Our approach outperforms the baseline in several games, especially MsPacman, Atlantis, Asterix, and BattleZone.}
\label{tab:tabela_resultados}
\end{table}


%\textbf{Reactive and deliberative environments.} Nesse tipo de ambiente a distribuição target muda implicitamente durante todo o treinamento. O nosso agente foi capaz de explorar diversas políticas e aprender mais rápido comportamentos bem alinhados com a tarefa sem qualquer sinal de reward extrínseca em ambientes que requerem habilidades cognitivas diferentes do agente com o passar do tempo. A figura \ref{fig:pacman_resultados} mostra que o nosso agente rapidamente aprendeu a iteragir no ambiente MsPacman conseguindo uma curva de aprendizado muito superior a linha de base. O MsPacman é um jogo particularmente interessante, pois inicialmente as rewards são densas e o agente precisa ser reativo para fugir dos fantasmas. A medida que o agente avança no ambiente, o jogo se torna gradativamente esparso exigindo do agente estratégias de planejamento mais elaboradas. Os resultados mostram que através da estrutura modular o nosso agente consegue capturar melhor a estutura composicional do mundo e ter uma representação concisa e eficiente para gerar suas ações. A representação do mundo guiada por atenção e pelas estruturas modulares demonstra maior flexibilidade para gerar representações na espaço de features de baixa dimensão, filtrando eficientemente partes irrelevantes do espaço de observação. As estruturas modulares também demonstram eficiência, dado que o agente parece dispor de todas as informações importantes para realizar as ações conseguindo desempenho em média $50\%$ maior que a linha de base durante o aprendizado. A nossa abordagem também lida melhor com as recompensas não estacionárias, já que nós não normalizamos a reward intrínseca ou fornecemos qualquer padronização, ao contrário dos outros métodos da literatura.

\textbf{Reactive and deliberative environments}. These environments require different cognitive skills from the agent over time, and the target distribution changes implicitly throughout training. In our experiments, MsPacman, Alien, and Amidar are environments that require reactive and deliberative behaviors. Our agent explored various policies and learned faster, well-aligned behaviors without extrinsic rewards. Figure \ref{fig:pacman_resultados} shows that our agent quickly learned to iterate in the MsPacman environment, achieving a learning curve much higher than the baseline. MsPacman is an interesting game to analyze. Initially, the rewards are dense, and the agent needs to be reactive to escape the ghosts. As the agent advances in the environment, the game gradually becomes sparse, demanding more elaborate planning strategies. The results show that through the modular structure, our agent can better capture the world's compositional structure and have a concise and efficient representation to generate its actions. Attention and modular structures demonstrate greater flexibility to generate low-dimensional features, efficiently filtering out irrelevant parts of the observation space. The modular structures also demonstrate efficiency since the agent seems to have all the essential information to perform the actions, achieving an average performance of $50\%$ higher than the baseline during learning. Also, our approach deals better with non-stationary rewards, as we do not provide any standardization to intrinsic rewards.


\begin{figure}[htb]
     \centering
     %\begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=7.5cm, height=4.0cm]{pacman_env.png}
         \caption{MsPacman game. The training curve shows the best extrinsic returns per episode in the MsPacman environment. In blue, we have our agent, and in red, we have the baseline. The x-axis represents the training steps, and the y-axis is the game score (i.e., accumulated extrinsic reward) received at the end of the episode.}
        \label{fig:pacman_resultados}
\end{figure}


%\textbf{Purely reactive environments.} Em ambientes puramente reativos o nosso agente apresenta desempenho claramente superior a linha de base, como observado nos jogos Asterix, Centipede, e Riverraid. Nesse tipo de ambiente, o agente precisa iteragir rapidamente com inimigos para se manter vivo. Esse tipo de jogo é particularmente interessante no intrinsic motivation pois a medida que o agente evolui e iterage com outros agentes, novos padrões mais difíceis de prever resultam da iteração. Como surgem muitas novidades o agente se mantém motivado a explorar novos comportamentos por mais tempo. Aparentemente o agente nota que a morte leva a finalização precoce do episódio e lutar para se manter vivo permite que ele conheça novos estados que ainda não foram explorados. Neste tipo de cenário, nós notamos que o nosso agente se destaca com relação a linha de base, já que as estratégias exploratórias escolhidas para se manter vivo permitiram que o agente consiga uma pontuação muito maior nesses jogos. A figura \ref{fig:todos_resultados} em a) e b) mostra claramente que o agente aprende continuamente comportamentos cognitivos coerentes no jogo que correlacionam bem com a reward extrínseca. Nós acreditamos que esse resultado se deve a representação guiada por atenção, permitindo que determinados módulos foquem nos imimigos e esqueçam aspectos do ambiente que são irrelevantes para se manter vivo. Como consequência, o agente faz escolhas que acabam pontuando mais no jogo.

\textbf{Purely reactive environments.} In purely reactive environments, our agent outperforms the baseline, as seen in the Asterix, Centipede, and Riverraid games. In these environments, the agent must interact quickly with enemies to stay alive. This type of game is exciting in intrinsic motivation because as the agent interacts with other agents, patterns more challenging to predict eventually emerge. As many novelties arise, the agent remains motivated to explore new behaviors for extended periods. The agent notices that death leads to an early ending of the episode, and fighting to stay alive allows it to discover new states that have not yet been explored. In this scenario, our agent stands out against the baseline. The exploratory strategies chosen to stay alive allowed the agent to achieve a much higher score in these games. Figure \ref{fig:todos_resultados} a) and b) clearly shows that the agent continually learns coherent cognitive behaviors in the game that correlate well with the extrinsic reward. We believe this result is due to attention-guided representations, allowing specific modules to focus on enemies and forget about aspects of the environment that are irrelevant to staying alive. Consequently, the agent makes choices that score more in the game.


\begin{figure*}[htb]%
    \centering
    \subfigure[\centering Asterix]{{\includegraphics[width=5cm]{asterix_env.png} }}%
    \qquad
    \subfigure[\centering RiverRaid]{{\includegraphics[width=5cm]{riverraid_env.png} }}%
    \qquad
    \subfigure[\centering Atlantis]{{\includegraphics[width=5cm]{atlantis_env.png} }}%
    \caption{Our proposed approach results. The training curve shows the best extrinsic returns per episode in the Asterix, Riverraid, and Atlantis environments. The x-axis represents the training steps, and the y-axis is the game score (i.e., accumulated extrinsic reward) received at the end of the episode. In these scenarios, our agent excels against the baseline (Table \ref{tab:tabela_resultados}). The exploratory strategies chosen to stay alive allowed the agent to obtain a much higher score in these games. After exploring states that led to bad scores, the agent quickly changes its exploratory strategy.}%
    \label{fig:todos_resultados}%
\end{figure*}




%\textbf{Sparse Rewards.} Resultados bastante significativos também são observados em ambientes puramente esparsos que exigem planejamento do agente. A figura \ref{fig:freeway_resultados} mostra que o nosso agente consegue planejar melhor a travessia da rua no ambiente Freeway e como consequência realiza mais voltas no mesmo intervalo de tempo que a linha de base. Em alguns casos, o nosso agente consegue realizar até 7 voltas na rua enquanto a linha de base consegue realizar no máximo 3 voltas. Realizar 7 voltas no ambiente Freeway é relativamente difícil dado que a cada passo de travessia que o agente dá ele pode ser atropelado por um inimigo. Apenas com uma boa estratégia de planejamento bem definida o agente consegue realizar tal ação. Também obtivemos resultados melhores que a linha de base no ambiente Pitfall, porém não conseguimos pontuação positiva em nenhum dos casos de teste. Nós acreditamos que essa potuação negativa se deve ao tamanho do rollout escolhido para este ambiente. O rollout de 128 é relativamente pequeno para ambientes mais esparsos.
\textbf{Sparse Rewards.} Significant results are also seen in purely sparse environments requiring agent planning. Figure \ref{fig:freeway_resultados} shows that our agent plans better in the Freeway environment and crosses the street more than the baseline. Sometimes, our agent can perform seven crossings on the street, while the baseline achieves a maximum of 3. Executing seven turns in the Freeway environment is relatively challenging since, at each crossing step the agent takes, an enemy can run over it. Only with a well-defined planning strategy can the agent perform such an action. We also overcame the baseline in the Pitfall environment. However, we did not get a positive score in any test cases. We believe this negative rating is due to the size of the rollout chosen for this environment. The 128 rollout is relatively small for sparse environments.


\begin{figure}[htb]
     \centering
     \includegraphics[width=7.5cm, height=4.0cm]{freeway_env.png}
     \caption{Freeway game. The training curve shows the best extrinsic returns per episode in the Freeway environment. In blue, we have our agent, and in red, we have the baseline. The x-axis represents the training steps, and the y-axis is the game score (i.e., accumulated extrinsic reward) received at the end of the episode.}
     %\label{fig:five over x}
    \label{fig:freeway_resultados}
\end{figure}




%Nos nossos experimentos nós comparamos a nossa arquitetura com o modelo proposto por Pathak et al. \cite{pathak2017curiosity} e posteriormente melhor explorado por Burda et al. \cite{burda2018large} em jogos de Atari. No processo de reprodução dos resultados observamos que o observation normalization usado em Burda et al. \cite{burda2018large} provoca o colapso da política após 50 milhões de steps de treinamento. Por isso, decidimos adotar a mesma estratégia de normalização da observação adotada no nosso trabalho. Nos nossos experimentos nós comparamos a nossa arquitetura com o modelo proposto por Pathak et al. \cite{pathak2017curiosity} e posteriormente melhor explorado por Burda et al. \cite{burda2018large} em jogos de Atari. No processo de reprodução dos resultados observamos que o observation normalization usado em Burda et al. \cite{burda2018large} provoca o colapso da política após 50 milhões de steps de treinamento. Por isso, decidimos adotar a mesma estratégia de normalização da observação adotada no nosso trabalho.



%Aparentemente a representação do estado guiada por atenção trata de fornecer apenas as informações relevantes para cada situação de jogo. 


%Nesta seção nós apresentamos os resultados obtidos com os nossos experimentos.
%Nos nossos experimentos nós comparamos a nossa arquitetura com o modelo proposto por Pathak et al. \cite{pathak2017curiosity} e posteriormente melhor explorado por Burda et al. \cite{burda2018large} em jogos de Atari.
%No processo de reprodução dos resultados observamos que o observation normalization usado em Burda et al. \cite{burda2018large} provoca o colapso da política após 50 milhões de steps de treinamento. Por isso, decidimos adotar a mesma estratégia de normalização da observação adotada no nosso trabalho. Nós treinamos 18 jogos de Atari com características diversas e diferentes tipos de reward. 


%\textbf{Efficient exploration.} Uma das principais desvantagens dos agentes intrínsecos é quando eles falham em explorar de forma eficiente e não superam um agente aleatório, ou quando os comportamentos aprendidos via reward intrínseca não apresentam correlação com as rewards dadas pelo ambiente. Todos os nossos resultados mostram que o nosso agente consegue explorar de forma mais eficiente o ambiente que a linha de base. Por exemplo, o Atlantis e o Alien são jogos em que nós obtivemos um resultado bastante surpreendente, dado que a linha de base não consegue superar um agente aleatório que pontua em média 10000 pontos no Atlantis e em média 200 pontos no Alien, como apresentado em \cite{burda2018large}. Ao contrário, o nosso agente inicia o treinamento adotando estratégias exploratórias aleatórias em que ele consegue em média 10000 pontos no Atlantis e a partir de um certo estágio ele escolhe políticas altamente correlacionadas com as rewards do ambiente, como mostrado figura \ref{fig:todos_resultados} em c), o nosso agente começa a explorar uma política bastante correlacionada com a reward extrínseca a partir dos 100 milhões de steps, superando consideravelmente a linha de base.

\textbf{Efficient exploration.} The intrinsic agents can fail to explore efficiently. In some situations, they do not learn highly correlated behaviors with extrinsic rewards. All our results show that our agent can more efficiently explore the environment than the baseline and learn more correlated behaviors with extrinsic rewards. Atlantis and Alien are games in which we obtained surprising results. The baseline cannot surpass a random agent that scores an average of 10,000 points in Atlantis and an average of 200 points in Alien, as shown in \cite{burda2018large}. Initially, our agent starts training by adopting a random exploratory similar to the baseline. However, as training progress, our agent chooses policies highly correlated with the extrinsic rewards, as shown in Figure \ref{fig:todos_resultados}c). These results demonstrate that our agent can choose more efficient exploratory strategies that lead to structured cognitive behaviors to perform a task even without any extrinsic reward.