\section{Related Work}
\label{sec:related_work}

%introduzir o intrinsic motivation RL e mostrar os trabalhos mais antigos
%Intrinsic motivation is a an topic very studied in the reinforcement learning field e um bom resumo da literatura é apresentado por Barto et al. **Barto, "Intrinsic Motivation"**, Aubret et al. **Aubret, "Self-Motivated Learning"**, and Singh et al. **Singh, "Intrinsic Rewards"**. Inicialmente, trabalhos de motivação intrínseca usaram conceitos de emoção, surpresa, empowerment, entropia e ganho de informação como rewards intrínsecas. Sequeira et al. **Sequeira, "Affective States and Decision-Making"** exploraram a hipótese de que estados afetivos codificam informações capazes de orientar a tomada de decisão de um agente durante o aprendizado. Achiam et al. **Achiam, "Surprise-Based Intrinsic Motivation"** propuseram uma abordagem baseada em surpresa na qual o agente aprende um modelo de transição de probabilidades de um MDP concorrentemente com a política e gerar recompensas intrínsecas que se aproximam das reais. Esse aprendizado ocorre por meio de um processo emergente autosupervisionado no qual novas capacidades surgem em distintos “marcos de desenvolvimento”. Além disso, o agente também aprende codificações visuais melhoradas em determinadas tarefas, como detecção, localização, reconhecimento de objetos e a previsão de dinâmica física melhor que outras abordagens que são estado da arte.

%A key aspect that differs our work from previous ones is the adoption of Bidirectional Recurrent Models (BRIMs) in implementing the internal world model. BRIMs são uma categoria de arquiteturas recorrentes profundas que usam atenção para dinamicamente combinar sinais bottom-up e top-down. Em particular, mostramos como os BRIMs podem aumentar o desempenho do estado-da-arte dos agentes motivados-intrinsecamente com melhoria na generalização fora da distribuição.

%Our work connects to a variety of existing ideas in self-supervision, active learning, and deep reinforcement learning. Visual learning pode ser alcançado através de tarefas auxiliares auto-supervisadas incluindo segmentação semântica **[18]**, estimativa de pose **[29]**, resolução de quebra-cabeças jigsaw **[32]**, colorização **[46]** e rotação **[43]**. Auto-supervisão em previsão de quadros de vídeo **[23]** também é promissora, mas enfrenta o desafio de que a maioria das sequências em vídeos gravados são “boring”, com pouco dinâmica interessante ocorrendo de um quadro para o outro.

%In order to encourage interesting events to happen, it is useful for an agent to have the capacity to select the data that it sees in training. In active learning, an agent seeks to learn a supervised task using minimal labeled data **[12]**, **[40]**. Recent methods obtain diversified sets of hard examples **[8]**, **[39]**, or use confidence-based heuristics to determine when to query for more labels **[45]**.

%A family of approaches to intrinsic motivation reward an agent based on prediction error **[2]**, **[27]**, **[36]**, **[42]**, prediction uncertainty **[11]**, **[44]**, or improvement **[19]**, **[34]** of a forward dynamics model of the environment that gets trained along with the agent’s policy. As a result the agent is driven to reach regions of the environment that are difficult to predict for the forward dynamics model.

%Learning without extrinsic rewards or fitness functions has also been studied extensively in the evolutionary computing where it is referred to as ‘novelty search’ **[17]**, **[18]**, **[43]**. There the novelty of an event is often defined as the distance of the event to the nearest neighbor amongst previous events, using some statistics of the event to compute distances.

%Other methods of exploration are designed to work in combination with maximizing a reward function, such as those utilizing uncertainty about value function estimates **[5]**, **[23]**, or those using perturbations of the policy for exploration **[8]**, **[29]**.