\section{Proposed Model}
\label{sec:method}

%O nosso agente possui dois modelos: 1) o predictive world model composto por uma estrutura modular e competitiva que codifica as informações do estado atual $s_{t}$ e gera as rewards intrínsecas, afim de prever como será representado o próximo estado do ambiente no instante de tempo $t+1$; 2) e o policy model que aprende uma política capaz de gerar uma sequência de ações para maximizar o sinal de reward. Ao final de cada ação executada pelo agente no ambiente ele recebe uma reward intrínseca $r^{int}_{t}$ gerada pelo predictive world model, como mostrado na Figura \ref{fig:model_complete}.

Our intrinsically-motivated agent has two models: 1) \textbf{the predictive world model} composed of a modular and competitive structure to generate the intrinsic rewards; 2) \textbf{the policy model} that learns a policy capable of generating a sequence of actions to maximize the reward signal. At the end of each action performed by the agent in the environment, it receives an intrinsic reward $r^{int}_{t}$ generated by the predictive world model, as shown in Figure \ref{fig:model_complete}.

\begin{figure*}[htb]
  \centering

   \includegraphics[width=0.65\linewidth]{meu_modelo_completo.pdf}

   \caption{Intrinsically-motivated agent architecture. At each time step $t$, the current state $s_{t}$ triggers the predictive world modules. Modules can be active, expected, null, or inactive state. Active modules use the state information $s_{t}$ to build a representation for choosing the current action. Modules in the expected state build an expected representation for the next state $s_{t+1}$ before the agent sees it. Thus, the agent performs a future prediction of the consequences of its action in the world. Finally, inactive/null modules do not participate in the gradient and can be activated as needed in the next iteration. After executing the action, the intrinsic reward is the difference between the agent's expectations and the real-world state.}
   \label{fig:model_complete}
\end{figure*}


%O predictive world model $W$ é a chave do nosso desenvolvimento. Ele é representado por um conjunto de redes neurais parametrizadas pelos parâmetros $\theta_{E}$, $\theta_{P}$, e $\theta_{F}$. O modelo é composto por um encoder $E$, e módulos recorrentes independentes que geram, de forma competitiva, representações do estado atual e do possível estado futuro do agente. A cada step de tempo $t$, o encoder $E$ recebe o estado atual $s_{t}$ do agente e gera um vetor $x_{t}$ que codifica toda a informação visual. No nosso caso, utilizamos três camadas convolucionais 2D com camadas lineares para realizar a codificação. Um key-value attention recebe $x_{t}$ como query e determina quais módulos recorrentes independentes devem entrar em estado ativo, inativo, ou esperado. As summary, 

The predictive world model $W$ is the key to our development. It is represented by a set of neural networks parameterized by $\theta_{E}$, $\theta_{P}$, and $\theta_{F}$. The model comprises a feature encoding $E$ and independent recurrent modules that competitively generate representations of the agent's current and possible future state. At each time step $t$, the encoder $E$ receives the current state $s_{t}$ from the agent and generates a vector $x_{t}$ that encodes all the visual information. We used three 2D convolutional layers with linear layers to perform the encoding in our case. A key-value attention receives $x_{t}$ as a query and determines which independent recurrent modules should enter an active, inactive, or expected state. In summary:


\begin{equation}
  x_{t} = E\left ( s_{t} ; \theta_{E} \right ),
  \label{eq:formule_1}
\end{equation}

\begin{equation}
  h_{t}^{l} = \phi \left ( x_{t}, h_{t-1}^{l} ; \theta_{P} \right ),
  \label{eq:formule_2}
\end{equation}


\begin{equation}
  \hat{h_{t}^{l}} = \hat{\phi} \left ( x_{t}, \hat{h^{l}}_{t-1} ; \theta_{F} \right ),
  \label{eq:formule_3}
\end{equation}

%onde $h_{t}^{l} = \left \{ h^{l}_{t, 1}, h^{l}_{t, 2}, ..., h^{l}_{t, k}\right \}$, $k \in S_{t}^{p}$ é o vetor de embedding real composto pela representação dos módulos ativados pelo key-value attention, dado que $S_{t}^{p}$ é o conjunto de módulos ativados com a representação do estado atual $s_{t}$, e $\hat{h_{t}^{l}} = \left \{ \hat{h^{l}}_{t, 1}, \hat{h^{l}}_{t, 2}, ..., \hat{h^{l}}_{t, w}\right \}$, $w \in S_{t}^{f}$ é o vetor de embedding esperado pela representação dos módulos disparados em estado esperado pelo key-value attention, dado que $S_{t}^{f}$ é o conjunto de módulos em estado esperado. $\phi$ e $\hat{\phi}$ são as funções LSTM geradoras dos módulos.

\noindent where $h_{t}^{l} = \left \{ h^{l}_{t, 1}, h^{l}_{t, 2}, ..., h^{l}_{ t, k}\right \}$, $k \in S_{t}^{p}$ is an embedding vector composed by activated modules with the current state $s_{t}$. $S_{t}^{ p}$ are indices of activated modules, and $\hat{h_{t}^{l}} = \left \{ \hat{h^{l}} _{t, 1}, \hat{h^{l}}_{t, 2}, ..., \hat{h^{l}}_{t, w}\right \}$, with $w \in S_{t}^{f}$, is an embedding vector composed by modules triggered in the expected state. $S_{t}^{f}$ are indices of modules in the expected state. $\phi$ and $\hat{\phi}$ are the LSTMs representing the modules.

%Os módulos ativos usam a informação disponilizada pelo encoder e passam para as camadas seguintes até a policy network. Quando um módulo está inativo em um determinado time step $t$ ele não incorpora as novas informações do estado atual e não existe fluxo de gradiente através dele. Em paralelo, alguns módulos disparam em estado esperado, que é a denominação que nós utilizamos para determinar os módulos que se ativam para gerar uma predição futura da representação do próximo estado que o agente estará no próximo instante de tempo $t$. Como a estrutura é modular, as representações estão divididas entre os diversos módulos, juntos eles compõem uma representação completa do ambiente. O garlalo atencional direciona o fluxo de ativação/desativação/expectativa dos diferentes módulos de modo que as representações internas do agente sobre o mundo são úteis para as suas ações sem a necessidade de modelos de dinâmica inversa. Além disso, cada módulo se torna especialista em determinados aspectos do ambiente. 

The active modules use the encoder's information, passing it to the successive layers. At each time step $t$, when a module is inactive, it does not incorporate the new information, and there is no gradient flow. In parallel, some modules fire in the expected state (i.e., the modules activated to generate a future prediction to the next state $s_{t+1}$). As the structure is modular, the world representation is divided among the different modules; together, they make up a complete representation of the world. The attentional bottleneck directs the different modules' activation/deactivation/expectation flow so that the agent's internal representations of the world are useful for its actions without the need for inverse dynamics models. In addition, each module becomes an expert in certain aspects of the environment.


%Os sinais bottom-up e top-down guiados por atenção são especialmente benéficos na seleção de ações do agente. Em fases onde a informação bottom-up domina pode beneficiar o agente para que ele aja imediatamente para resolver imprevistos, enquanto que os sinais top-down podem ser úteis em situações em que o sistema precise de um plano de longo prazo. Além disso, essa representação é muito similar ao fenômeno de ativação e desativação das células piramidais no neocortex humano [X]. 
Attention-driven bottom-up and top-down signals are especially beneficial in selecting agent actions. In phases where bottom-up information dominates, it can benefit the agent to act immediately to resolve unforeseen events. At the same time, top-down signals can be useful when the system needs a long-term plan. Furthermore, this representation is very similar to the activation and deactivation of pyramidal cells in the human neocortex \cite{hawkins2017theory}.


%No geral, esperamos que a ativação de modulação dinâmica guiada por atenção sejam especialmente importante no aprendizado intrínseco do agente, dado que existem evidências na literatura de que a estrutura modular consegue lidar melhor com mudanças de distribuição implícitas no treinamento à medida que a política evolui. Ao final da competição entre os módulos, duas representações de estado são criadas, o $h_{t}^{l}$ e o $\hat{h_{t}^{l}}$, respectivamente. O $h_{t}^{l}$ é a representação em espaço latente do estado atual do agente e será entrada para a policy network, enquanto que o $\hat{h_{t}^{l}}$ é a representação em espaço latente do próximo estado em que o agente espera estar após a execução da ação. O aprendizado ocorre quando é realizada uma quebra de expectativa entre o vetor as duas representações no domínio temporal, de modo que a reward intrínseca é dada por $r^{int}_{t} = \frac{\left \| h_{t}^{p} - h_{t-1}^{f} \right \|^{2}_{2}}{n}$, onde $n$ é o tamanho do vetor.

Overall, attention-guided dynamic activation is essential in intrinsic agent learning, given that there is evidence in the literature that the modular structure can better handle distribution changes implicit in training as the policy evolves. At the end of the competition between modules, two state representations are created, $h_{t}^{l}$ and $\hat{h_{t}^{l}}$, respectively. $h_{t}^{l}$ is the latent space vector of the agent's current state and will be input to the policy network. In contrast, $\hat{h_{t}^{l}}$ is the representation expected to the next state $s_{t+1}$ after the agent executes the current action. Learning occurs when an expectation break is performed between $h_{t}^{l}$ and $\hat{h_{t}^{l}}$, so that the intrinsic reward is generated by $r^{int}_{t} = \frac{\left \| h_{t}^{p} - h_{t-1}^{f} \right \|^{2}_{2}}{n}$, where $n$ is the size of the vector. Finally, we represent the policy $\pi \left ( h_{t}^{p}; \theta_{P} \right )$ by a deep neural network with parameters $\theta_{P}$. Given the agent in state $s_{t}$, it executes action $a \sim \pi \left ( h_{t}^{p}; \theta_{P} \right ) $ sampled from the policy. $\theta_{E}$, $\theta_{P}$, and $\theta_{G}$ are optimized to maximize the expected sum of intrinsic rewards, given by

%\begin{equation}
%    r^{int}_{t} = \frac{\left \| h_{t}^{p} - h_{t-1}^{f} \right \|^{2}_{2}}{n}
%    \label{eq:intr_reward}
%\end{equation}

%\noindent where $n$ is the size of the vector.

%Por fim, nós representamos a política $\pi \left ( h_{t}^{p}; \theta_{P} \right )$ by a deep neural network with parameters $\theta_{P}$. Given the agent in state $s_{t}$, it executes the action $a \sim \pi \left ( h_{t}^{p}; \theta_{P} \right ) $ sampled from the policy. $\theta_{E}$, $\theta_{P}$, and $\theta_{G}$ are optimized to maximize the expected sum of intrinsic rewards,



\begin{equation}
  \underset{\theta_{E}, \theta_{P}, \theta_{G}}{\textup{max}} E_{\pi \left ( h_{t}^{p}; \theta_{P} \right )}\left [ \sum_{t} r_{t}^{int}\right ].
  \label{eq:max_rewards}
\end{equation}

%Em paralelo, temos que $\theta_{E}$ e $\theta_{F}$ são minimizados por uma loss de regressão, 

In parallel, $\theta_{E}$ and $\theta_{F}$ are minimized by a regression loss, given by

\begin{equation}
  \underset{\theta_{E}, \theta_{F}}{\textup{min}} L_{W}(h_{t}, \hat{h}_{t-1}).
  \label{eq:max_rewards_2}
\end{equation}

\noindent where $L_{W}$ measures the discrepancy between the predicted and actual features and is modelled as the mean squared error function. 

The overall optimization problem can be written as

\begin{equation}
  \underset{\theta_{E}, \theta_{P}, \theta_{F}, \theta_{G}}{\textup{min}} \left [ E_{\pi \left ( h_{t}^{p}; \theta_{P} \right )}\left [ \sum_{t} r_{t}^{int}\right ] + L_{W} \right ].
  \label{eq:max_rewards_2}
\end{equation}

%Nós propagamos o gradiente do policy model e do world model pelo encoder para garantir que a representação gerada seja útil para a tarefa do agente.
We propagate the gradient of the policy and world models through the encoder to ensure that the generated representation is useful for the agent's task.



% Update the cvpr.cls to do the following automatically.
% For this citation style, keep multiple citations in numerical (not
% chronological) order, so prefer \cite{Alpher03,Alpher02,Authors14} to
% \cite{Alpher02,Alpher03,Authors14}.



