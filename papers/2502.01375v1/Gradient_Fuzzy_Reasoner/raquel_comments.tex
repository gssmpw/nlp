\documentclass{article}
% !TeX spellcheck = en_GB


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
\usepackage{neurips_2024}



% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{dsfont}
\usepackage{amsmath} % for math environments and symbols
\usepackage{graphicx}
\usepackage{adjustbox}
\usepackage{multirow}
\usepackage{arydshln}
\usepackage{rotating}
\usepackage{bm}

\newcommand{\indicator}[1]{\mathds{1}_{#1}}

\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\RFPc}[1]{\textcolor{blue}{#1}}
\newcommand{\RFPq}[1]{\textcolor{violet}{#1}}

\newcommand{\JF}[1]{\textcolor{red}{#1}}

\title{Comments}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  J.Fumanal-Idocin\thanks{} \\
  School of Computer Science and Electronic Engineering, University of Essex\\
  Wivenhoe Park, Colchester CO4 3SQ \\
  \texttt{j.fumanal-idocin@essex.ac.uk} \\
  % examples of more authors
  % \And
  % J.Andreu-Perez \\
  % School of Computer Science and Electronic Engineering, University of Essex\\\\
  % Wivenhoe Park, Colchester CO4 3SQ \\
  % \texttt{j.andreu-perez.ac.uk} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}


\begin{document}


\maketitle


\begin{center}
	FRR model
\end{center}

Let $\mathcal{D} = \{(X_1,Y_1),\dots, (X_N,Y_N)\}$ denote a data set with $N$ instances and $M$ features, where $X_i$ is the observed feature vector of the $i$-instance  with $j$-th entry $X_{i,j}$ and $Y_i$ being its discrete associated target. Each feature can be either discrete or continuous, and the target is a categorical variable with $C$ classes. All the categorical variables are represented using \RFPc{one-hot encoding} and all the continuous variables features are represented as fuzzy linguistic variables with \RFPc{up to} $L$ linguistic labels. Thus, if we consider the $j$-th feature then $\mu_{j,l}(X_{i,j})$ denotes the degree of truth of instance $i$ evaluated in the fuzzy set corresponding to the $l$-th label of the $j$-th feature.

\RFPc{Al final como el one-hot encoding es como tener un fuzzy set trivial para cada clase se puede unificar la notación. También se podría explicar así.} \JF{Mejor no, onehot encoding es algo que va a entender cualquier lector sin necesidad de decir nada mas. Podriamos decir que el one hot es un fuzzy degenerado para que el lector entienda lo que es fuzzy, pero no al reves.}

The FRR is a hierarchical rule-based model with \RFPc{$K$} layers,
we will denote by $\mathcal{U}^{(k)}$ the $k$-th layer and by $u_j^{(k)}$ and $n_k$ the $j$-th node and the total number of nodes in that layer, respectively. The output of the $k$-th is denoted by a vector $\bm{u}^{k}$ containing as instances the value of each node and $\bm{W}^{(k)}$ represents the connectivity matrix of layer $k$, i.e., $W_{i,j}^{(k)}$ is the edge weight between nodes $u_i^{(k-1)}$ and $u_j^{(k)}$.

\RFPq{Faltaría explicar que cada matriz de adyacencia se repite por el número de reglas pero no se incluye el indice por simplicidad.} \JF{Depende de como lo veas no? Realmente es todo la misma matriz. Pero entiendo lo que dices, voy a anadir en el documento final un por cada r}.


\begin{itemize}
	\item \textbf{Fuzzification layer:} Fuzzification consists of mapping values in the original domains to fuzzy memberships. In order to obtain semantic-sensitive fuzzy sets, we use three different trapezoid-shaped sets per variable to partition the state so that represent the concepts ``low'', ``medium'' and ``high'', which is quite usual in the literature. The first layer of the FRR is a fuzzification block that transforms the input observations into degrees of truth to these linguistic labels. Specifically, if we consider the $i$-th instance, the $j$-th feature and the $l$-th linguistic label then $u_{j+l}^{(1)} = \mu_{j,l}(X_{i,j})$.
	
	\RFPq{Aquí hay matriz de pesos, no, no?}
	
	The RFF can optimize fuzzy partitions using gradient optimization. However, this can hinder the interpretability of the model, as the optimized fuzzy sets can have arbitrary shapes and order relations between them. More details about how we compute and the possibility of optimizing the fuzzy sets is in Appendix A.
	
	\item \textbf{Logic inference layer:} This layer uses a weight matrix $\bm{W}^{(2)}$ of size $L \times M$, i.e., number of features per number of space partitions. The weight matrix
	 of this layer shall work similarly to a connection matrix that relates the rules with their respective antecedents. Existing continuous approaches to rule-based inference perform thresholding in the weights as a way to determine rule antecedents, which causes problem in gradient flow. However, if we use fuzzy operator, we can replicate fuzzy logical inference without any compromises \RFPc{aunque luego te cargas el gradient flow totalmente, pero bueno}.
	 
	 To explain our approach let us first consider the Mamdani inference expression to compute the truth value of a fuzzy rule $r$ in an inference system:
	  \begin{equation}\label{eq:fuzzy_inference}
	  r(X) = w_r \prod_{a \in A_r} \mu_{a}(X)
	\end{equation}
	being $w_r$ the rule weight, $A_r$ the set of conditions in the antecedent of the rule and $\mu_a(X)$ the evaluation of the corresponding fuzzy set. In order to replicate this, the first step is to choose for each antecedent which space partitions are forwarded into the rule. To do that, we only forward the linguistic label that hold the biggest weight value per each feature. Then, we compute the truth value of the relevant partition for each feature, multiplied by the weight matrix:
	
	\begin{equation}\label{eq:1}
	r(X_i)= \prod_{j=1}^M \sum_{l=1}^L f(W^{(2)}_{j,l}) W^{(2)}_{j,l}\mu_{j,l}(X_{i,j}),
	\end{equation}
	
	where
	
	\begin{equation}\label{eq:2}
		f(W^{(2)}_{j,l})=\left\{
		\begin{array}{ll}
			1 & \text{if } l = \argmax_{l \in \{1,...,L\}} W^{(2)}_{j,l},\\
			0 & \text{otherwise. } 
		\end{array}
		\right.
	\end{equation}
	
	If we resolve the argmax function in Eq. (\ref{eq:1}) and denote by $\tilde{l}_j$ the space partition of the $j$-th feature with biggest weight then we obtain the following expression:
	
	\begin{equation} \label{eq:3}
		r(X_i) = \prod_{j=1}^{M} W^{(2)}_{j,\tilde{l}_j} \prod_{j=1}^{M} \mu_{j,\tilde{l}_j}(X_{i,j}).  
	\end{equation}
	
	As long as all $ W^{(2)}_{j,\tilde{l}_j} \in [0,1] $ The term $\prod_{j=1}^{M} W^{(2)}_{j,\tilde{l}_j}$ works as $w_r$ in Eq. (\ref{eq:fuzzy_inference}), and the other term is the original truth value. However, Eq. \ref{eq:1} is not differentiable with respect to the weights because of the presence of the argmax function so in order to use gradient descent we propose the use an Straight-Through Estimator (STE).
	
	\RFPc{Según el paper del IJCAI lo que se puede hacer es usar Eq. (\ref{eq:2}) en Eq. (\ref{eq:1}) forward, y la función softmax como aproximación contínua de Eq. (\ref{eq:2}) para calcular la derivada backward.
		$$ \tilde{f} (W^{(2)}_{j,l})={\frac {e^{ W^{(2)}_{j,l}}/\alpha}{\displaystyle \sum _{l=1}^{L}e^{W^{(2)}_{j,l}}/\alpha}}
		$$
	donde $\alpha$ es un hiperparámetro que normalmente empieza en 1 y contra más pequeño se haga más se aproxima esa función al argmax. Entonces, para el entrenamiento se tiene que ir disminuyendo progresivamente. Luego, la función que queréis poner con la $k$ sería
	\begin{equation}
	f_k(W^{(2)}_{j,l})= k+(1-k)f (W^{(2)}_{j,l})= \left\{
	\begin{array}{ll}
		1 & \text{if } l = \argmax_{l \in \{1,...,L\}} W^{(2)}_{j,l},\\
		k & \text{otherwise, } 
	\end{array}
	\right.
	\end{equation}
	que por definición se aproximaria utilizando la aproximación de la softmax $$\tilde{f}_k(W^{(2)}_{j,l})=k +(1-k)\cdot\tilde{f} (W^{(2)}_{j,l}).$$
	}
	
	
	
	If we do not want to use rule weights, we can then divide the result by the same $\prod_{j=1}^{M} W^{(2)}_{j,\tilde{l}_j}$ term, which would result in the same output as Eq. (\ref{eq:fuzzy_inference}) \RFPq{no entiendo esto, si divides por lo mismo se te van los pesos}. However, for the gradient to still operate correctly in this expression, we apply the division term as a constant.
	
	[To be continued.]
\end{itemize}


\section{Old sections}

\subsection{Logic inference layer} 

This layer uses a weight matrix $\mathbf{W}^{(2)}$ of size $M \times L$, i.e.,  number of features per number of linguistic labels. The weight matrix of this layer shall work similarly to a connection matrix that relates the rules to their respective antecedents. \RFc{Esto no lo entiendo muy bien.}
	
We first consider the Mamdani inference expression to compute the truth value of a fuzzy rule $r$ in an inference system:
	\begin{equation}\label{eq:fuzzy_inference}
		r(X) = w_r \prod_{a \in A_r} \mu_{a}(X)
	\end{equation}
	being $w_r$ the rule weight, $A_r$ the set of conditions in the antecedent of the rule and $\mu_a(X)$ the evaluation of the corresponding fuzzy set.
	
	In order to replicate this, the first step is to choose for each antecedent which space partitions are forwarded into the rule. To do that, we only forward the linguistic label that hold the biggest weight value per each feature. Then, we compute the truth value of the relevant partition for each feature, multiplied by the weight matrix:
	
	\begin{equation}\label{eq:1}
		r(X_i)= \prod_{j=1}^M \sigma\left(\sum_{l=1}^L f(W^{(2)}_{j,l}) W^{(2)}_{j,l}\mu_{j,l}(X_{i,j})\right),
	\end{equation}
	
	where
	
	\begin{equation}\label{eq:2}
		f(W^{(2)}_{j,l})=\left\{
		\begin{array}{ll}
			1 & \text{if } l = \argmax_{l \in \{1,...,L\}} W^{(2)}_{j,l},\\
			0 & \text{otherwise,} 
		\end{array}
		\right.
	\end{equation}
	
	and $\sigma$ is an activation function, which to recover fuzzy rule-based inference we will set to the identity $\sigma(x) =  x$. If we resolve the argmax function in Eq. (\ref{eq:1}) and denote by $\tilde{l}_j$ the space partition of the $j$-th feature with the highest weight then we obtain the following expression:
	
	\begin{equation} \label{eq:3}
		r(X_i) = \prod_{j=1}^{M} W^{(2)}_{j,\tilde{l}_j} \prod_{j=1}^{M} \mu_{j,\tilde{l}_j}(X_{i,j}).  
	\end{equation}
	
	As long as all $ W^{(2)}_{j,\tilde{l}_j} \in [0,1] $ The term $\prod_{j=1}^{M} W^{(2)}_{j,\tilde{l}_j}$ works as $w_r$ in Eq. (\ref{eq:fuzzy_inference}), and the other term is the original truth value. However, Eq. (\ref{eq:1}) is not differentiable with respect to the weights because of the presence of the argmax function, so in order to use gradient descent, we propose the use of an STE.
	
	It is also worth pointing out that in order to have a valid fuzzy inference process we need to keep the values in the $[0,1]$ range. Since the input values are already in that range and we are only multiplying, we only need to make sure that the weights are also in that range to keep everything in $[0,1]$. To do so, we apply the softmax function to all the weight vectors used in Eq. (\ref{eq:1}) and Eq. (\ref{eq:2}).
	
	%\begin{equation}
	%	\text{softmax}(w_i) = \frac{\exp(w_i/\tau)}{\sum_{j=1}^{|W|} \exp(w_j/\tau)}
	%\end{equation}.
	
    %Setting up the correct $\tau$ parameter, will also help us train the FRR more effectively, which we will discuss in Section \ref{sec:derivations}.
	%If we do not want to use rule weights, we can then divide the result by the same $\prod_{j=1}^{M} W^{(2)}_{j,\tilde{l}_j}$ term, which would result in the same output as Eq. (\ref{eq:fuzzy_inference}). However, for the gradient to still operate correctly in this expression, we apply the division term as a constant.
	
	
\subsection{The final decision layer} 

This module computes the predictions of the FRR given the truth degrees of the rules. For each rule $r$, we define a score vector, $S_r = \{s_i\}, i \in \{1,\dots, c\}$, where each element is the score that rule $r$ gives to class $i$. After computing the truth degree for each rule we multiply them for their corresponding $S_r$ vector. After that, we have can compute the final outcome using suficient or additive rules.

\begin{itemize}
	\item Sufficient rules: 
    For a class $i$, given the scores for all the rules whose maximum score is $i$, defined as: $I_i=\{S_{r, i} r(X)_i|\argmax_{\forall r \in R} S_r = i\}$, the output for class $i$ is the score of such rules with maximum truth degree:
    \begin{equation}
		FRR(X)_c = \max(I_c)  
    \end{equation} 
	
	\item Additive rules: in this case just perform the standard matrix multiplication between the rules truth degrees and the $\mathbf{S}$ score matrix:
	
	\begin{equation}
		FRR(X) = \sum_{i=1}^{r} R_r S_{r}.
	\end{equation}
	
\end{itemize}


Since fuzzy association degrees are already numbers in $[0,1]$, we can directly apply the cross-entropy loss without further modifications.
%\subsection{Fuzzification layer} \label{sec:fuzzyfication}

%Fuzzification consists of mapping values in the original domains to fuzzy memberships. In order to obtain semantic-sensitive fuzzy sets, we use three different trapezoidal-shaped sets per variable to partition the state so that they represent the concepts ``low'', ``medium'' and ``high'', which is quite usual in the literature \citep{mendel2023explainable}. The first layer of the FRR is a fuzzification block that transforms the input observations into degrees of truth to these linguistic labels.

%The RFF can optimize fuzzy partitions using gradient optimization. However, this can hinder the interpretability of the model, as the optimized fuzzy sets can have arbitrary shapes and order relations between them. More details about how we compute and possibility optimize the fuzzy sets is in Appendix \ref{sec:fuzzyfication_details}.

\subsection{Setting the number of antecedents} \label{sec:setter}

If we used the Eq. (\ref{eq:1}) without any further modifications, we would be forced to use all features in each rule. This is not practical in datasets with many features or when the final user is only interested in a particular rule size.

In order to set the number of antecedents per rule to a fixed size, $Z$, we use a weight matrix, $W^{(3)}$ of size ($Z$, $J$) and a trick similar to the indicator function used in Eq. (\ref{eq:2}). It is composed of a weight matrix, $W$, of size (Number of Rules, Number of Antecedents, Desired Number of Antecedents). In order to apply the RDL, we need to extract an intermediate expression between Eq. (\ref{eq:1}) and Eq. (\ref{eq:3}), which is the matrix with the truth values of each antecedent for each rule, $r$:

\begin{equation}
	A_r(X) =  \sum_{l=1}^L f(W^{(2)}_{j,l}) W^{(2)}_{j,l}\mu_{j,l}(X_{i,j}).
\end{equation}

\noindent Then, we multiply $A_r(X)$ by the weight matrix $W^{(3)}$, using an indicator function to select only the antecedent that had the biggest weight value:

\begin{equation} \label{eq:indicator_weights_forward_rdr}
	r_z(X) =  \sum_{j=1}^J f(W^{(3)}_{z}) W^{(3)}_{z, j}A_r(X),
\end{equation}
\noindent and $f$ is given by:

\begin{equation}\label{eq:indicator_2}
		f(W^{(3)}_{z,m})=\left\{
		\begin{array}{ll}
			1 & \text{if } m = \argmax_{m \in \{1,...,M\}} W^{(3)}_{z,m},\\
			0 & \text{otherwise. } 
		\end{array}
		\right.
\end{equation}

\noindent We do this $Z$ times and multiply the values to obtain the truth degree of the rule:

\begin{equation} \label{eq:final_truth_degree}
    r(x) = \prod_{z=1}^{Z} r_z(X)
\end{equation}

It can happen that the same antecedent is propagated more than once. In that case, the same membership is powered as many times as it was chosen, which is equivalent to using linguistic hedges in the original partition space \citep{mendel2023explainable}.

%\subsubsection{Continuous gradient} The gradient computation of the logical layer described in equation \ref{eq:indicator_weights_forward} is straightforward, as it primarily involves the product of multiple terms. Consequently, the partial derivatives leave all other terms unchanged, except for \( w_i \) and \( \mu_i(x_i) \), which are set to 1 upon differentiation, $\frac{\partial r(X)}{\partial w_i}$ and $\frac{\partial r(X)}{\partial w_i}$, respectively. If \( \mu_i(x_i) \) is a Gaussian membership function of the form:
%\[
%\mu_i(x_i) = \exp\left(-\frac{(c_i - x_i)^2}{2\sigma_i^2}\right),
%\]
%in this basis function, we have two parameters to estimate, \( c_i \) and \( \sigma_i \), for each respective dimension \( i \). The partial derivatives of equation \ref{eq:indicator_weights_forward} with respect to these parameters yield the following differentiation: 
%\begin{equation} \label{eq:differentiate_mean}
%    \frac{\partial r(X)}{\partial \mu_i(x_i)} * \frac{\partial \mu_i(x_i)}{\partial c_i} = -\frac{(c_i - x_i)}{\sigma_i^2} \left( \prod_{k=1}^{m} w_k \right) \left( \prod_{k=1}^{m} \mu_k(x_k) \right);
%\end{equation}
%\begin{equation} \label{eq:differentiate_sigma}
%    \frac{\partial r(X)}{\partial \mu_i(x_i)} * \frac{\partial \mu_i(x_i)}{\partial \sigma_i} = \frac{(c_i - x_i)^2}{\sigma_i^3} \left( \prod_{k=1}^{m} w_k \right) \left( \prod_{k=1}^{m} \mu_k(x_k) \right);
%\end{equation}
%thus, it can be easily noted that the continuity of the gradient is always ensured as long as the membership function is also differentiable.

\bibliographystyle{abbrvnat}
\bibliography{nips}


\end{document}