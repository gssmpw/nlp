\section{Related Work}
\subsection{Rule-based models}
Rule-based models and decision trees have been mostly trained using heuristic procedures due to their discrete and non-differentiable nature \citep{wei2019generalized}. However, these algorithms may not find the best solution and are not guaranteed to find a close one \citep{rudolph1994convergence}. Search algorithms can also be used, but they are expensive to compute, making them unfeasible in large datasets. Most existing rule-based methods mine a series of frequent patterns, which may not be ideal in all situations and do not achieve the same performance as other complex methods such as gradient boosting or random forest \citep{yuan2017improved}. However, such complex models are harder to interpret, and it is necessary to use tools such as Shapley values to do so \citep{lundberg2020local}. Fuzzy rules offer good interpretability, but they are computationally intensive to train \citep{mendel2023explainable} and might require hundreds of rules to achieve good performance. 

Rule-based methods have been used as well for explainable symbolic reasoning when the antecedents are known concepts, and their relationships can be exploited to solve tasks such as classification. Logic structures can be fixed, and then the right concepts are to be found \citep{petersen2022, barbiero2023interpretable}. Finding the optimal connections among the concepts studied \citep{vemuri2024enhancing} is also possible. However, the interpretability of the system is compromised by the quality of the concept detection, which is hard to assess. It is also possible to distil black-box models into rule-based ones with good results \cite{li2024interpreting}, at the expense of having to train both models.

\subsection{Gradient-based training for rule-based and discrete models} 
Due to the good scalability of gradient-based optimization, gradient-based methodologies have been developed for many systems that are not directly differentiable. The most popular is the Straight-Through Estimator (STE) \citep{bengio2013estimating}. Gumbel-Softmax estimator is also very used in architectures that need to sample a categorical distribution in a differentiable model \citep{jang2016categorical}, such as variational autoencoders. Gradient-based optimization for discrete models is also closely related to the quantization of a DNN. For example, one of the pioneering works in using binary weights was aimed at reducing the computing expenses of a DNN model \citep{courbariaux2015binaryconnect}.

The most common approaches to gradient-based rule learning involve the joint use of discrete and continuous models so that continuous models generate a good gradient flow \citep{wang2023learning, zhang2023learning}. The problem in that case is to make sure that the discrete and continuous models behave similarly, which is particularly complex for large models. These models also tend to create large numbers of additive rules, so the reasoning mechanism is very difficult for a human being to understand properly. For the case of fuzzy rule-based inference, it is possible to optimize fuzzy sets using gradient descent \citep{mendel2023explainable}. However, antecedent search using fuzzy sets is not explored in the literature using gradient-descent methods.

Our method, Fuzzy Rule-based Reasoner (FRR), differs from the previous methods in that it proposes an architecture that performs discrete inference while also being able to specify the maximum number of rules, antecedents, and partitions. In addition, the FRR is trained using only gradient-descent techniques and is able to use both sufficient and additive rules, which also enhances its interpretability.