@inproceedings{HappyDB2018,
  author       = {Akari Asai and
                  Sara Evensen and
                  Behzad Golshan and
                  Alon Y. Halevy and
                  et. al.},
  title        = {HappyDB: {A} Corpus of 100, 000 Crowdsourced Happy Moments},
  booktitle    = {Proceedings of the Eleventh International Conference on Language Resources
                  and Evaluation, {LREC} 2018, Miyazaki, Japan, May 7-12, 2018},
  year         = {2018},
}

@inproceedings{big_five_MBTI2020,
  title={Bottom-up and top-down: Predicting personality with psycholinguistic and language model features},
  author={Mehta, Yash and Fatehi, Samin and Kazameini, Amirmohammad and Stachl, Clemens and Cambria, Erik and Eetemadi, Sauleh},
  booktitle={2020 IEEE International Conference on Data Mining (ICDM)},
  pages={1184--1189},
  year={2020},
  organization={IEEE}
}

@ARTICLE{TCSS_2025,
  author={Wu, Xiaohua and Li, Lin and Tao, Xiaohui and Xing, Frank and Yuan, Jingling},
  journal={IEEE Transactions on Computational Social Systems}, 
  title={Happiness Prediction With Domain Knowledge Integration and Explanation Consistency}, 
  year={2025},
  volume={},
  number={},
  pages={1-14},
  doi={10.1109/TCSS.2025.3529946}
}


@inproceedings{AkenWLG19,
  author       = {Betty van Aken and
                  Benjamin Winter and
                  Alexander L{\"{o}}ser and
                  Felix A. Gers},
  title        = {How Does {BERT} Answer Questions?: {A} Layer-Wise Analysis of Transformer
                  Representations},
  booktitle    = {Proceedings of the 28th {ACM} International Conference on Information
                  and Knowledge Management, {CIKM} 2019, Beijing, China, November 3-7,
                  2019},
  pages        = {1823--1832},
  year         = {2019},
}

@article{Automated_Alignment_of_LLMs_2024,
   author = {Cao, Boxi and Lu, Keming and Lu, Xinyu and Chen, Jiawei and Ren, Mengjie and et al.},
   title = {Towards Scalable Automated Alignment of LLMs: A Survey},
   journal = {CoRR},
   volume = {abs/2406.01252},
   DOI = {10.48550/ARXIV.2406.01252},
   url = {https://doi.org/10.48550/arXiv.2406.01252},
   year = {2024},
   type = {Journal Article}
}

@inbook{Shapley_2016,
    author = {L. S. Shapley},
    title = {A Value for n-Person Games},
    booktitle = {Contributions to the Theory of Games (AM-28)},
    Volume = {2},
    year = {2016},
    pages = {307-318},
    publisher = {Princeton University Press},
}

@inproceedings{Active_Prompting_2024,
   author = {Diao, Shizhe and Wang, Pengcheng and Lin, Yong and Pan, Rui and Liu, Xiang and Zhang, Tong},
   title = {Active Prompting with Chain-of-Thought for Large Language Models},
   booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics},
   publisher = {Association for Computational Linguistics},
   pages = {1330-1350},
   year = {2024},
   type = {Conference Proceedings}
}

@article{Bognar2010authentic,
  title={Authentic happiness},
  author={Bognar, Greg},
  journal={Utilitas},
  volume={22},
  number={3},
  pages={272--284},
  year={2010},
  publisher={Cambridge University Press}
}

@inproceedings{Automatic_Chain_of_Thought_2023,
  author       = {Zhuosheng Zhang and
                  Aston Zhang and
                  Mu Li and
                  Alex Smola},
  title        = {Automatic Chain of Thought Prompting in Large Language Models},
  booktitle    = {The Eleventh International Conference on Learning Representations,
                  {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023},
  year         = {2023},
}

@article{LLM_alignment_Survey_2023,
  author       = {Tianhao Shen and
                  Renren Jin and
                  Yufei Huang and
                  Chuang Liu and
                  et al.},
  title        = {Large Language Model Alignment: {A} Survey},
  journal      = {CoRR},
  volume       = {abs/2309.15025},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2309.15025},
  doi          = {10.48550/ARXIV.2309.15025},
  eprinttype    = {arXiv},
}

@article{MCHANEY2018,
title = {Using LIWC to choose simulation approaches: A feasibility study},
    journal = {Decision Support Systems},
    volume = {111},
    pages = {1-12},
    year = {2018},
    issn = {0167-9236},
    author = {Roger McHaney and Antuela Tako and Stewart Robinson},
    abstract = {Can language usage help determine which model approach is best suited to provide decision makers with desired insights? This research addresses that question through an investigation of Linguistic Inquiry and Word Count (LIWC), which calculates the presence of >80 language dimensions in text samples, and permits construction of custom dictionaries. This article demonstrates use of LIWC to ensure better problem/model fit within the context of selecting a decision support tool. We selected two simulation tools as research instruments to investigate a broader question on the usefulness of LIWC to guide choice of DSS tool. The tools selected were System Dynamics (SD) and Discrete Event Simulation (DES). First, we tested LIWC to analyze practitioners' language use when developing models. LIWC pointed out significant linguistic differences consistent with prior theoretical work, based on model development approach in a number of dimensions. These differences provided a basis for developing a custom dictionary for use on the second part of our study. The second part of the study focused on language used by decision makers in problem statements and used the linguistic clues identified in the first part of the study to ensure problem/model fit. Results indicated problem statements contained linguistic clues related to the type of information desired by problem solvers. The article concludes with a discussion about how LIWC and similar tools can help determine which DSS tools are suited to particular applications.}
}

@INPROCEEDINGS{Hanks2022,
  author={Hanks, Casey and Verma, Rakesh M.},
  booktitle={2022 IEEE/ACM International Conference on Big Data Computing, Applications and Technologies (BDCAT)}, 
  title={Data Quality and Linguistic Cues for Domain-independent Deception Detection}, 
  year={2022},
  volume={},
  number={},
  pages={248-258}
}

@article{AIforSocialScience2024,
   author = {Xu, Ruoxi and Sun, Yingfei and Ren, Mengjie and Guo, Shiguang and Pan, Ruotong and Lin, Hongyu and Sun, Le and Han, Xianpei},
   title = {AI for social science and social science of AI: A survey},
   journal = {Information Processing \& Management},
   volume = {61},
   number = {3},
   pages = {103665},
   year = {2024},
   type = {Journal Article}
}

@article{computational_personality_survey_2022,
   author = {Yang, Liang and Li, Shuqun and Luo, Xi and Xu, Bo and Geng, Yuanling and Zeng, Zeyuan and Zhang, Fan and Lin, Hongfei},
   title = {Computational personality: a survey},
   journal = {Soft Computing},
   volume = {26},
   pages = {9587–9605},
   ISSN = {1432-7643
1433-7479},
   DOI = {10.1007/s00500-022-06786-6},
   year = {2022},
   type = {Journal Article}
}

@book{nardi_survey_research_2018,
  title={Doing survey research: A guide to quantitative methods},
  author={Nardi, Peter M},
  year={2018},
  publisher={Routledge}
}

@inproceedings{HappyDB_2018,
   author = {Xu, Akari Asai; Sara Evensen; Behzad Golshan; et al},
   title = {HappyDB: A Corpus of 100,000 Crowdsourced Happy Moments},
   booktitle = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)},
   publisher = {European Language Resources Association (ELRA)},
   address = {Miyazaki, Japan},
   pages = {647-655},
   year = {2018},
   type = {Conference Proceedings}
}

@article{nature_2021,
  title={Unrepresentative big surveys significantly overestimated US vaccine uptake},
  author={Bradley, Valerie C and Kuriwaki, Shiro and Isakov, Michael and Sejdinovic, Dino and Meng, Xiao-Li and Flaxman, Seth},
  journal={Nature},
  volume={600},
  number={7890},
  pages={695--700},
  year={2021},
  publisher={Nature Publishing Group UK London}
}

@article{ChatGPT_vs_Social_Surveys_2024,
  title={ChatGPT vs Social Surveys: Probing the Objective and Subjective Human Society},
  author={Zhou, Muzhi and Yu, Lu and Geng, Xiaomin and Luo, Lan},
  journal={arXiv preprint arXiv:2409.02601},
  year={2024}
}

@inproceedings{Ribeiro2016,
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	title = {"Why Should I Trust You?": Explaining the Predictions of Any Classifier},
	year = {2016},
	abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
	booktitle = {ACM SIGKDD},
	pages = {1135–1144},
	numpages = {10},
	location = {San Francisco, California, USA},
	series = {KDD '16}
}

@ARTICLE{Bootstrap_Sampling_2022,
  author={Zhang, Jiawei and Wang, Yi and Sun, Mingyang and Zhang, Ning},
  journal={IEEE Transactions on Engineering Management}, 
  title={Two-Stage Bootstrap Sampling for Probabilistic Load Forecasting}, 
  year={2022},
  volume={69},
  number={3},
  pages={720-728},
}


@inproceedings{Shrikumar_2017,
	author = {Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul},
	title = {Learning Important Features through Propagating Activation Differences},
	year = {2017},
	booktitle = {ICML, Sydney, NSW, Australia, 6-11 August},
	series       = {Proceedings of Machine Learning Research},
	pages = {3145–3153},
	numpages = {9},
	location = {Sydney, NSW, Australia},
	abstract = {The purported "black box" nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input. DeepLIFT compares the activation of each neuron to its 'reference activation' and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, DeepLIFT can also reveal dependencies which are missed by other approaches. Scores can be computed efficiently in a single backward pass. We apply DeepLIFT to models trained on MNIST and simulated genomic data, and show significant advantages over gradient-based methods. Video tutorial: http://goo.gl/qKb7pL, code: http://goo.gl/RM8jvH.},
}

@article{meta_llama3,
  title={Introducing Meta Llama 3: The most capable openly available LLM to date},
  author={AI at Meta},
  journal={},
  year={2024}
}

@article{argyle_2023,
  title={Out of one, many: Using language models to simulate human samples},
  author={Argyle, Lisa P and Busby, Ethan C and Fulda, Nancy and Gubler, Joshua R and Rytting, Christopher and Wingate, David},
  journal={Political Analysis},
  volume={31},
  number={3},
  pages={337--351},
  year={2023},
  publisher={Cambridge University Press}
}

@article{Forest_of_Thought_2024,
   author = {Bi, Zhenni and Han, Kai and Liu, Chuanjian and Tang, Yehui and Wang, Yunhe},
   title = {Forest-of-Thought: Scaling Test-Time Compute for Enhancing LLM Reasoning},
   journal = {arXiv preprint arXiv:2412.09078},
   year = {2024},
   type = {Journal Article}
}

@inproceedings{Self_Consistency_COT_2023,
  author       = {Xuezhi Wang and
                  Jason Wei and
                  Dale Schuurmans and
                  Quoc V. Le and
                  Ed H. Chi and
                  Sharan Narang and
                  Aakanksha Chowdhery and
                  Denny Zhou},
  title        = {Self-Consistency Improves Chain of Thought Reasoning in Language Models},
  booktitle    = {The Eleventh International Conference on Learning Representations,
                  {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023},
  year         = {2023},
}

@inproceedings{Chain_of_Thought_2022,
  author       = {Jason Wei and
                  Xuezhi Wang and
                  Dale Schuurmans and
                  Maarten Bosma and
                  Brian Ichter and
                  Fei Xia and
                  Ed H. Chi and
                  Quoc V. Le and
                  Denny Zhou},
  title        = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  booktitle    = {Advances in Neural Information Processing Systems 35: Annual Conference
                  on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans,
                  LA, USA, November 28 - December 9, 2022},
  year         = {2022},
}

@inproceedings{Tree_of_thoughts_2023,
  author       = {Shunyu Yao and
                  Dian Yu and
                  Jeffrey Zhao and
                  Izhak Shafran and
                  Tom Griffiths and
                  Yuan Cao and
                  Karthik Narasimhan},
  title        = {Tree of Thoughts: Deliberate Problem Solving with Large Language Models},
  booktitle    = {Advances in Neural Information Processing Systems 36: Annual Conference
                  on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans,
                  LA, USA, December 10 - 16, 2023},
  year         = {2023},
}
@inproceedings{LoRA_2022,
  author       = {Edward J. Hu and
                  Yelong Shen and
                  Phillip Wallis and
                  Zeyuan Allen{-}Zhu and
                  Yuanzhi Li and
                  Shean Wang and
                  Lu Wang and
                  Weizhu Chen},
  title        = {LoRA: Low-Rank Adaptation of Large Language Models},
  booktitle    = {The Tenth International Conference on Learning Representations, {ICLR}
                  2022, Virtual Event, April 25-29, 2022},
  year         = {2022},
}

@inproceedings{few_shot_learners_2020,
  author       = {Tom B. Brown and
                  Benjamin Mann and
                  Nick Ryder and
                  Melanie Subbiah and
                  et al.},
  title        = {Language Models are Few-Shot Learners},
  booktitle    = {Advances in Neural Information Processing Systems 33: Annual Conference
                  on Neural Information Processing Systems 2020, NeurIPS 2020, December
                  6-12, 2020, virtual},
  year         = {2020},
}

@inproceedings{QA_LoRA_2024,
  author       = {Yuhui Xu and
                  Lingxi Xie and
                  Xiaotao Gu and
                  Xin Chen and
                  Heng Chang and
                  Hengheng Zhang and
                  Zhengsu Chen and
                  Xiaopeng Zhang and
                  Qi Tian},
  title        = {QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language
                  Models},
  booktitle    = {The Twelfth International Conference on Learning Representations,
                  {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  year         = {2024},
}

@inproceedings{Parameter_Efficien_2022,
  author       = {Junxian He and
                  Chunting Zhou and
                  Xuezhe Ma and
                  Taylor Berg{-}Kirkpatrick and
                  Graham Neubig},
  title        = {Towards a Unified View of Parameter-Efficient Transfer Learning},
  booktitle    = {The Tenth International Conference on Learning Representations, {ICLR}
                  2022, Virtual Event, April 25-29, 2022},
  year         = {2022},
}

@inproceedings{frantar2022optq,
  title={OPTQ: Accurate quantization for generative pre-trained transformers},
  author={Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@inproceedings{xiao2023smoothquant,
  title={Smoothquant: Accurate and efficient post-training quantization for large language models},
  author={Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song},
  booktitle={International Conference on Machine Learning (ICLR)},
  pages={38087--38099},
  year={2023},
  organization={PMLR}
}

@inproceedings{QLoRA_2023,
  author       = {Tim Dettmers and
                  Artidoro Pagnoni and
                  Ari Holtzman and
                  Luke Zettlemoyer},
  title        = {QLoRA: Efficient Finetuning of Quantized LLMs},
  booktitle    = {Advances in Neural Information Processing Systems 36: Annual Conference
                  on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans,
                  LA, USA, December 10 - 16, 2023},
  year         = {2023},
}


@article{Saputri_2015,
    author = {Saputri, Theresia and Lee, Seok-Won},
    year = {2015},
    pages = {1699-1702},
    title = {A Study of Cross-National Differences in Happiness Factors Using Machine Learning Approach},
    volume = {25},
    journal = {International Journal of Software Engineering and Knowledge Engineering},
    abstract = {National happiness has been actively studied throughout the past years. The happiness factor varies due to different human perspectives. The factors used in this work include both physical needs and the mental needs of humanity, for example, the educational factor. This work identified more than 90 features that can be used to predict the country happiness. Due to numerous features, it is unwise to rely on the prediction of national happiness by manual analysis. Therefore, this work used a machine learning technique called Support Vector Machine (SVM) to learn and predict the country happiness. In order to improve the prediction accuracy, dimensionality reduction technique which is the information gain was also used in this work. This technique was chosen due to its ability to explore the interrelationships among a set of variables. Using data of 187 countries from the UN Development Project, this work is able to identify which factor needed to be improved by a certain country to increase the happiness of their citizens.}
}

@article{Yu_2017,
    author = {Yu, Zonghuo and Wang, Fei},
    year = {2017},
    pages = {2052},
    title = {Income Inequality and Happiness: An Inverted U-Shaped Curve},
    volume = {8},
    journal = {Frontiers in Psychology},
}
@article{Laaksonen_2018,
    author = {Laaksonen, Seppo},
    year = {2018},
    pages = {471–482},
    title = {A Research Note: Happiness by Age is More Complex than U-Shaped},
    volume = {19},
    journal = {Journal of Happiness Studies},
}
@inproceedings{Weizhao2019,
	author       = {Weizhao Xin and Diana Inkpen},
	title        = {Happiness Ingredients Detection using Multi-Task Deep Learning},
	booktitle    = {AAAI, Honolulu, USA, January 27},
	volume       = {2328},
	pages        = {164--170},
	year         = {2019},
}
@inproceedings{ijcai2022_Lilin,
  title     = {Towards the Quantitative Interpretability Analysis of Citizens Happiness Prediction},
  author    = {Li, Lin and Wu, Xiaohua and Kong, Miao and Zhou, Dong and Tao, Xiaohui},
  booktitle = {IJCAI, Vienna, Austria, 23-29 July},
  pages     = {5094-5100},
  year      = {2022},
}

@inproceedings{ADMA2024,
	author       = {Xiaohua Wu and Lin Li and Xiaohui Tao and Yuefeng Li},
	title        = {Aligning Bytes with Bliss: Integrating Happiness Computing with Sociological Insight},
	booktitle    = {The 20th International Conference Advanced Data Mining and Applications Sydney, Australia, 3rd - 5th December 2024},
	pages        = {1-16},
	year         = {2024},
}
@inproceedings{Zero_Shot_Prompting_2024,
  author       = {Md. Arid Hasan and
                  Shudipta Das and
                  Afiyat Anjum and
                  Firoj Alam and
                  Anika Anjum and
                  Avijit Sarker and
                  Sheak Rashed Haider Noori},
  title        = {Zero- and Few-Shot Prompting with LLMs: {A} Comparative Study with
                  Fine-tuned Models for Bangla Sentiment Analysis},
  booktitle    = {Proceedings of the 2024 Joint International Conference on Computational
                  Linguistics, Language Resources and Evaluation, {LREC/COLING} 2024,
                  20-25 May, 2024, Torino, Italy},
  pages        = {17808--17818},
  year         = {2024},
}

@inproceedings{Zero_Shot_2022,
  author       = {Jason Wei and
                  Maarten Bosma and
                  Vincent Y. Zhao and
                  Kelvin Guu and
                  Adams Wei Yu and
                  Brian Lester and
                  Nan Du and
                  Andrew M. Dai and
                  Quoc V. Le},
  title        = {Finetuned Language Models are Zero-Shot Learners},
  booktitle    = {The Tenth International Conference on Learning Representations, {ICLR}
                  2022, Virtual Event, April 25-29, 2022},
  publisher    = {OpenReview.net},
  year         = {2022},
}

@inproceedings{Few_Shot_2020,
  author       = {Tom B. Brown and
                  Benjamin Mann and
                  Nick Ryder and
                  Melanie Subbiah and
                  et al.},
  title        = {Language Models are Few-Shot Learners},
  booktitle    = {Advances in Neural Information Processing Systems 33: Annual Conference
                  on Neural Information Processing Systems 2020, NeurIPS 2020, December
                  6-12, 2020, virtual},
  year         = {2020},
}

@inproceedings{MEGA_2023,
  author       = {Kabir Ahuja and
                  Harshita Diddee and
                  Rishav Hada and
                  Millicent Ochieng and
                  et al.},
  title        = {{MEGA:} Multilingual Evaluation of Generative {AI}},
  booktitle    = {Proceedings of the 2023 Conference on Empirical Methods in Natural
                  Language Processing, {EMNLP} 2023, Singapore, December 6-10, 2023},
  pages        = {4232--4267},
  publisher    = {Association for Computational Linguistics},
  year         = {2023},
}
@article{QA_LLM_2023,
  author       = {Zafaryab Rasool and
                  Scott Barnett and
                  Stefanus Kurniawan and
                  et al.},
  title        = {Evaluating LLMs on Document-Based {QA:} Exact Answer Selection and
                  Numerical Extraction using Cogtale dataset},
  journal      = {CoRR},
  volume       = {abs/2311.07878},
  year         = {2023},
}

@inproceedings{multilingual_CoT_2023,
  author       = {Freda Shi and
                  Mirac Suzgun and
                  Markus Freitag and
                  Xuezhi Wang and
                  et al},
  title        = {Language models are multilingual chain-of-thought reasoners},
  booktitle    = {The Eleventh International Conference on Learning Representations,
                  {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023},
  year         = {2023},
}

@inproceedings{wangyue_2024,
author = {Wang, Yue and Zhang, Duoyi and Kong, Xiangrui and Marco, Fahmi and Nayak, Richi},
year = {2024},
month = {11},
pages = {1-15},
booktitle    = {The 22nd Australasian Data Science and Machine Learning Conference (AusDM'24)},
title = {Enhancing AI Safety in the Public Sector: A Field Experiment on Guardrails Leveraging LLMs for State Government Employees}
}

@article{DFS_2024,
  author       = {Robert E. Tarjan and
                  Uri Zwick},
  title        = {Finding strong components using depth-first search},
  journal      = {European Journal of Combinatorics},
  volume       = {119},
  pages        = {103815},
  year         = {2024},
}

@article{EMA_2021,
	author    = {de Vries LP and Baselmans BML and Bartels M.},
	journal   = {Journal of Happiness Studies},
	title     = {Smartphone-Based Ecological Momentary Assessment of Well-Being: A Systematic Review and Recommendations for Future Studies},
	volume    = {22},
	number    = {5},
	pages     = {2361-2408},
	year      = {2021},
}

@inproceedings{TouT_2024,
  author       = {Shentong Mo and
                  Miao Xin},
  title        = {Tree of Uncertain Thoughts Reasoning for Large Language Models},
  booktitle    = {{IEEE} International Conference on Acoustics, Speech and Signal Processing,
                  {ICASSP} 2024, Seoul, Republic of Korea, April 14-19, 2024},
  pages        = {12742--12746},
  publisher    = {{IEEE}},
  year         = {2024},
}