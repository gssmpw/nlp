\section{Related Work}
\subsection{Computational Social Science Analysis}
% 从background入手，传统机器学习-->CNN\DNN等深度学习-->
The current methods in computational social science require an amount of time to assess the labels required by professionals. For example, previous research has identified that mental state is primarily associated with factors such as income, health, family, and others, through regression analysis and machine learning (ML) ____, which is highly interpretability. With the advancement of deep neural networks (DNN), several researchers have proposed various DNN-based methods to analyze the relationships between factors and mental state ____. Based on social science theories, how to use artificial intelligence technology to quickly, objectively, and automatically complete mental state assessments has become a new trend. A follow-up study ____ utilized DNN-based methods to consider the domain knowledge in the designed questionnaires such as information on aspects but not random question-answer pairs due to different respondents. 

\subsection{Prompt Learning}

% 大语言模型\BERT\Transformer --> Pretrain, Fine-tune --> Few shot and Zero Prompt

\paragraph{Pretraining and Fine-tuning}
Recently years have witnessed a rapid development of large language models (LLMs), which have a strong ability in many language-understanding tasks. The heavy computational burden and limited universality largely restrict the application of LLMs in various problems. For the first issue, there are two lines of research including parameter-efficient fine-tuning (PEFT) ____ and parameter quantization ____. One of the most popular approaches is the low-rank adaptation (LoRA) ____, which is an improved fine-tuning method where instead of fine-tuning all the weights that constitute the weight matrix of the pre-trained large language models. Recently, joint adaptation and quantization methods have been proposed for achieving the objectives of both parameter-efficient adaptation and computation-efficient tuning and deployment ____. It can further improve the efficiency and scalability of LLMs as well as mitigate the negative impact of quantization errors. 

\paragraph{Input-output (IO) Prompting}

Zero-shot and few-shot prompting ____ are the most represented input-output prompting methods, which provide natural language instructions that describe the task and specify the expected output. This approach enables the LLMs to construct a context that refines the inference space, yielding a more accurate output ____. Numerous studies ____ demonstrated that few-shot learning offers superior performance when compared to zero-shot learning setup. These works demonstrate that various prompting methods benefit the LLMs.

\paragraph{X-of-Thought Prompting}

For providing a way to solve complex problems that are not easily formalized, chain-of-thought (CoT) prompting ____ was proposed to address cases where the mapping of question $x$ to answer $y$ is non-trivial. It enhances the performance of LLMs by improving their reasoning capabilities, allowing them to solve complex tasks through step-by-step thinking. Recently, CoT has been utilized in various research areas such as question-answering systems ____, mathematical reasoning ____, and mini crosswords and creative writing ____. However, these models can not fit the questionnaire analysis in computational social science and do not consider the answer differences between respondents. 

Uncertainties in intermediate decision points are beneficial to combining various features according to the importance of features and have been widely used in various intelligent decisions ____. Inspired by this, we employ LLMs to generate various linguistic thoughts to prompt LLMs reasoning strategy from multiple levels including key feature triggers, multi-aspect analysis, and overall assessment. The thoughts are evaluated by a post-hoc explanation method and then randomly joined in the thought subset for the next stage of reasoning.