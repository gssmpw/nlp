\section{Introduction}
\label{sect:intro}

\nomdef{Wilderness search and rescue}{WiSAR} missions are some of the most time-sensitive operations in existence. Shaving off seconds in the time to find and the resultant rescue can directly result in saved lives. Over small areas it can be effective to quickly cover the entire search space using modern technology such as drones \citep{carrell_flying_2022}, however this becomes intractable over larger areas. The search area can quickly balloon into the tens of kilometres in width and depth when considering a WiSAR scenario. This introduces the requirement to take the endurance of the searcher into account as complete coverage is no longer feasible. This is referred to as search planning which aims to maximize a objective given a maximum path length.

Current approaches to using \nomdef{unmanned aerial systems}{UAS} during deployment by organisations like Police Scotland Air Support Unit is the pilot-observer model. This mandates that there are always at least two personnel present to operate the UAS no matter the scenario. The pilot flies the drone whilst also observing and inspecting the live camera feed, whereas the observers is in charge of maintaining a visual line-of-sight to the UAS at all times for safety and legislative reasons. In work by \citet{koester_sweep_2004}, it was identified that a searcher has a higher probability of detection when not in motion leading to the stop-and-look method. Whilst this work was carried out for foot-based searchers, the same strategy can be observed in \citet{ewers_optimal_2023} for UAS pilots. It is therefore evident that the cognitive load of manoeuvring and searching is a key limitation, and that being able to offload the menial flying of the drone frees up the pilot to spend more effort on the search.

Previous work \citep{ewers_optimal_2023, ewers_deep_2025}, has shown the strength of \nomdef{Deep Reinforcement Learning}{DRL} over analytical and optimal search planning methods.
Work by \citet{talha_autonomous_2022} and \citet{peake_deep_2021} applied DRL to a similar problem, however these algorithms explore the
environment during the search and do not have the complete \nomdef{Probability Distribution Map}{PDM} at the beginning. This is a reasonable scenario to be in at the start of the search as local area knowledge, maps, and case studies are all available to a pilot before the search begins. Similarly, PDM generation algorithms such as from \citet{hashimoto_agentbased_2022} are viable solutions to generate the PDM as an input.

The proposed algorithm builds on the previously efforts seen in \citep{ewers_deep_2025} which had severe limitations due to the policy network architecture.
The most major of these limitations was that through the use frame-stacking the maximum number of waypoints in a search path was constrained.
Another limitation was the long training time required to train the policy due to its large amount of learnable parameters.
This of course also results in poor runtime performance during deployment as multiple gigabytes of memory are consumed to hold the model on potentially resource limited devices.

\citet{mock_comparison_2023} found that using frame-stacking over recurrent architectures leads to comparable performance.
However, in this formulation the observations from early in the simulation become decreasingly unimportant. In the search planning algorithm from \citet{ewers_deep_2025} this is not the case and every step has an equal impact on the next steps reward. Hence, the policy must be able to observe the observations back to $t=0\si\second$ to maintain the \nomdef{Markv Decision Process}{MDP}.

In this work we aim to tackle two of the aforementioned problems: find an alternative to frame-stacking, and increase general performance (training times, model size, overall efficacy).
In \citet{raffin_decoupling_2019} a \nomdef{Auto-Encoder}{AE} is successfully used within the DRL loop whilst in \citep{park_sequencetosequence_2018} the sequence-to-sequence architecture is introduced for text-based applications. This is the basis to approach the first problem, and also a possible approach to handle the second. We hypothesis that by splitting out the feature extraction phase as a observation preprocessing step we can harness the powerful AE training setup, and thusly reduce the training overhead by not having to learn the feature extraction during the DRL phase. This will then result in two models (AE and DRL policy) working in unison that are specialized and substantially smaller than in \citet{ewers_deep_2025} with better performance.

This work therefore contributes the following advances to the field:
\begin{itemize}
    \item A framework to decrease training times and to significantly reduce the number of learnable parameter, whilst enhancing final performance through the proposed AE and DRL architecture,
    \item Empirical evaluation of frame-stacking and recurrent frameworks for large observations,
    \item Further the discussion in comparing PPO and SAC in a large observation space domain.
\end{itemize}

Related work is discussed in \autoref{sect:related_work}. The environmental modelling is presented in \autoref{sect:method}, whilst the RAE and DRL architectures are introduced in \autoref{sect:rae} and \autoref{sect:architecture} respectively. Implementation details are outlined in \autoref{sect:implementation}, followed by results and discussions in \autoref{sect:results} and \autoref{sect:discussion}. Finally, a conclusion is drawn in \autoref{sect:conclusion}.