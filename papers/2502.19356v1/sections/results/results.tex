\section{Results}
\label{sect:results}

\subsection{Experimental setup}
\label{sect:results:experimental_setup}

Each model was trained over \num{1e7} step with 8 workers on a local \texttt{Ubuntu 22.04} machine with a
\texttt{AMD Ryzen 9 5950X} CPU with a \texttt{NVIDIA RTX A6000} GPU and $64\unit{\giga\byte}$ of RAM. Runs terminated when invalid training updates were attempted which only happened during periods of extremely poor performance. At least five runs per architecture were undertaken with randomized starting seeds which aligns with the best practices outlined in work by \citet{agarwal_deep_2022} to ensure robust analysis for DRL results. The set of run configurations was generated and then randomized during training to avoid any possibility of unforeseen interactions.

The RAE training dataset contained \num[drop-zero-decimal]{5e4} unique paths which were generated using \lhcgwconv \citep{lin_uav_2009} with the same parameters as in this work. The recurrent encoder was trained once and then deployed for all following runs.

\subsection{Evaluation Metrics}

This section outlines the metrics used to evaluate the performance of the different architectures.

\subsubsection{Mean Step Reward}

The average reward received by the agent at each time step during an episode. A higher mean step reward indicates that the agent is making more effective decisions that lead to higher rewards at each step.

\subsubsection{Mean Rollout Episode Reward}

The average reward accumulated by the agent over a complete episode during the rollout phase. This metric reflects the overall performance of the learned policy in generating high-quality paths.

\subsubsection{Mean Episode Length}

The average number of time steps taken by the agent before the episode terminates.  A longer mean episode length generally indicates that the agent is able to explore the environment more effectively and find longer, more efficient paths.

\subsubsection{Maximum Probability Efficiency}

The highest achieved probability of reaching the target within a given search area observed during training as defined in \autoref{sect:method:rewards}. This metric directly reflects the search efficacy of the architecture, indicating how well it can find the target within the specified search space.

\subsubsection{Runtime}

The total time or steps taken by the architecture to train. This metric is crucial for practical applications, as it indicates the computational cost of training the architecture.

\subsubsection{Mean Probability Efficiency}

Similar to the maximum probability efficiency, it is the mean probability efficiency achieved during evaluation. This metric provides a comprehensive assessment of the architecture's search performance across multiple runs.

\subsubsection{Number of Parameters}

The total number of learnable parameters in the neural network architecture. This metric provides an indication of the model's complexity and computational requirements.

\subsection{Architecture}
\label{sect:results:architecture}

From \autoref{fig:results:rollout_reward_mean_over_time} it is clear that \lstmaesac, \lstmaeppo, \lstmppo, and \fsppoconvtwod had the best rollout reward performance with very stable learning curves. However, the \fssacconvtwod variant was by far the least stable and consistently crashed during training with illegal update steps. This is further corroborated by \autoref{tbl:results:architecture_performance} with \fsppoconvtwod having one of the highest mean runtime steps at \num{9.82E+06} and \fssacconvtwod having the lowest at \num{1.00E+06}.
\begin{table*}[htbp]
    \centering
    \caption{Aggregated architecture results over multiple metrics gathered at the end of a training run.}
    \label{tbl:results:architecture_performance}
    \sisetup{exponent-thresholds=-4:3,
    }
    \begin{tabular}{l|
            S[table-format=1.3]
            S[table-format=1.3]
            |
            S[table-format=2.3]
            S[table-format=2.3]
            |
            S[table-format=1.3]
            S[table-format=1.3]
            |
            S[table-format=3.3, drop-exponent=true, exponent-mode=fixed, fixed-exponent=3]
            S[table-format=3.3, drop-exponent=true, exponent-mode=fixed, fixed-exponent=3]
            |
            S[table-format=2.3, drop-exponent=true, exponent-mode=fixed, fixed-exponent=6]
            S[table-format=2.3, drop-exponent=true, exponent-mode=fixed, fixed-exponent=6]
        }
        \toprule
        Architecture   & \multicolumn{2}{R{2cm}}{Mean Step Reward} & \multicolumn{2}{R{2.5cm}}{Mean Episode Length} & \multicolumn{2}{R{2.5cm}}{Maximum Probability Efficiency} & \multicolumn{2}{R{2.5cm}}{ Runtime [s/\num[exponent-mode=input, output-exponent-marker=]{e3}]} & \multicolumn{2}{R{2.5cm}}{Runtime [steps/\num[exponent-mode=input, output-exponent-marker=]{e6}]}                                                                            \\

                       & {Mean.}                                   & {Std.}                                         & {Mean.}                                                   & {Std.}                                                                                         & {Mean.}                                                                                           & {Std.}   & {Mean.}            & {Std.}   & {Mean.}            & {Std.}   \\
        \midrule
        \fsppoconvtwod & 3.29E-01                                  & 5.19E-02                                       & 3.69E+01                                                  & 1.19E+01                                                                                       & 1.02E-01                                                                                          & 1.88E-02 & 5.11E+04           & 7.71E+03 & 9.82E+06           & 6.82E+05 \\
        \fsppofcn      & 2.72E-01                                  & 9.37E-02                                       & 2.16E+01                                                  & 1.08E+01                                                                                       & 6.59E-02                                                                                          & 1.24E-02 & 4.58E+04           & 1.54E+04 & 9.36E+06           & 1.67E+06 \\
        \fsppolstm     & 2.73E-01                                  & 1.15E-01                                       & 1.64E+01                                                  & 7.39E+00                                                                                       & 4.37E-02                                                                                          & 8.88E-03 & 9.48E+04           & 2.49E+04 & 9.87E+06           & 5.91E+05 \\
        \fssacconvtwod & 2.26E-01                                  & 1.03E-01                                       & 1.01E+01                                                  & 1.63E+00                                                                                       & 3.63E-02                                                                                          & 2.95E-02 & \bfseries 9.77E+03 & 4.55E+03 & 1.00E+06           & 3.16E+05 \\
        \fssacfcn      & 2.07E-01                                  & 9.77E-02                                       & 2.21E+01                                                  & 1.92E+01                                                                                       & 7.90E-02                                                                                          & 2.50E-02 & 7.99E+04           & 5.27E+04 & 7.23E+06           & 3.72E+06 \\
        \fssaclstm     & 1.36E-01                                  & 1.14E-01                                       & 3.20E+01                                                  & 2.51E+01                                                                                       & 8.43E-02                                                                                          & 1.67E-02 & 2.80E+05           & 1.25E+05 & 7.10E+06           & 2.99E+06 \\
        \lstmaeppo     & 5.20E-01                                  & 6.10E-02                                       & 5.11E+01                                                  & 4.25E+00                                                                                       & 1.76E-01                                                                                          & 6.71E-03 & 1.56E+05           & 4.99E+04 & \bfseries 1.00E+07 & 0.00E+00 \\
        \lstmaesac     & \bfseries 5.37E-01                        & 4.31E-02                                       & \bfseries 5.73E+01                                        & 6.09E+00                                                                                       & \bfseries 1.92E-01                                                                                & 1.16E-02 & 2.55E+05           & 5.51E+04 & 9.75E+06           & 6.10E+05 \\
        \lstmppo       & 4.13E-01                                  & 5.12E-02                                       & 5.54E+01                                                  & 7.40E+00                                                                                       & 1.64E-01                                                                                          & 1.95E-02 & 3.55E+05           & 1.28E+05 & 8.26E+06           & 3.12E+06 \\
        \bottomrule
    \end{tabular}
\end{table*}

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{lineplot_global_step_rounded_rollout_ep_rew_mean_architecture_algo.pdf}
    \caption{Mean rollout episode reward over global step for all architectures highlighting the training stability provided by the LSTMAE.}
    \label{fig:results:rollout_reward_mean_over_time}
\end{figure}

With the simulation terminating prematurely if the agent steps out-of-bounds,
it is important for the mean episode length to be as close to the maximum simulation length as possible.
The worst performance was again \fssacconvtwod with \num{1.01E+01} showing a complete lack of generalization.
\lstmaesac had the highest mean episode length at \num{5.73E+01} with \lstmaeppo and \lstmaeppo achieving similar results at \num{5.11E+01} and \num{5.54E+01} respectively.
None of the frame-stacking variants were able to breach the \num{4.00E+01} barrier with \fssacconvtwod coming closest at \num{3.69E+01} which aligns with the performance displayed in \autoref{fig:results:rollout_reward_mean_over_time}.

Mean step reward and maximum probability efficiency should strongly correlate from the definition of reward in \autoref{eqn:reward_with_cases}. However, the latter is the key metric as it directly relates to the search efficacy.
Similar to previous metrics, \fssacconvtwod displayed the poorest performance with a score of \num{3.63E-02}. \fsppoconvtwod, again, was the best of the frame-stacking variants with a score of \num{1.02E-01}. \lstmaeppo and \lstmppo also performed inline with previous results with \num{1.76E-01} and \num{1.64E-01} respectively. Whilst \lstmaeppo has a higher mean maximum probability efficiency, it also has a lower standard deviation of \num{6.71E-03} compared to \num{1.95E-02} across more runs. This could indicate the additional stability offered by having a static LSTMAE compared to the internalized LSTM module of \lstmppo from \autoref{fig:method:rppo_policy}. Furthermore, this could hint at the vanishing gradient problem that recurrent networks suffer from.

Ultimately, \lstmaesac showed the best performance across mean step reward (\num{1.70E-02} higher than \lstmaeppo),
mean episode length (\num{1.90E-02} higher than \lstmppo),
and maximum probability efficiency with a value of \num{1.92E-01} (\num{1.60E-02} higher than \lstmaeppo).
Contrary to the runtime results from \autoref{tbl:results:architecture_performance},
\autoref{fig:results:rollout_reward_mean_over_time} shows a training curve that had not reached its maxima whilst starkly outperforming the competing variants.
\subsection{Ablation Study: Reinforcement Algorithm}
\label{sect:results:algorithm}

SAC and PPO have been extensively compared in the literature, however, it is typically only the environment that is being changed. In this comparison the result from the various architectures are grouped by DRL algorithm with a consistent environment and hyperparameters. This will give an indication of the robustness and sensitivity to network changes.

\autoref{fig:results:rollout_reward_mean_over_time} displays the inter-quartile range of the maximum probability efficiency from \autoref{tbl:results:architecture_performance}. This shows that neither PPO nor SAC have a clear advantage for the frame-stacking variant. However, the LSTMAE results clearly show that SAC outperforms PPO here. The aggregated mean for PPO is \num{1.43E-01} compared to \num{1.37E-01} for SAC showing that PPO perhaps has a slight edge. However, applying a p-test gives a p-value of 0.61 which implies that the results are not conclusive.

Isolating the top frame-stacking (\fsppoconvtwod and \fssaclstm) architectures results in a different outcome. SAC has a higher aggregated mean maximum probability efficiency of \num{1.54E-01} whilst PPO only achieves \num{1.43E-01}. A p-test, however, also reveals that the the results are not conclusive with a p-value of 0.48.

\begin{table*}[htbp]
    \centering
    \caption{Aggregated algorithm (PPO, RPPO, SAC) results over multiple metrics gathered at the end of a training run.}
    \label{tbl:results:rl_algorithms}
    \sisetup{exponent-thresholds=-3:3}
    \begin{tabular}{l|
            S[table-format=1.3]
            S[table-format=1.3]
            |
            S[table-format=2.3]
            S[table-format=1.3]
            |
            S[table-format=1.3]
            S[table-format=1.3]
            |
            S[table-format=3.3, drop-exponent=true, exponent-mode=fixed, fixed-exponent=3]
            S[table-format=3.3, drop-exponent=true, exponent-mode=fixed, fixed-exponent=3]
            |
            S[table-format=1.3, drop-exponent=true, exponent-mode=fixed, fixed-exponent=6]
            S[table-format=1.3, drop-exponent=true, exponent-mode=fixed, fixed-exponent=6]
        }
        \toprule
        Algorithm & \multicolumn{2}{R{2.5cm}}{Mean Step Reward} & \multicolumn{2}{R{2.5cm}}{Mean Episode Length} & \multicolumn{2}{R{2.5cm}}{Maximum Probability Efficiency} & \multicolumn{2}{R{2.5cm}}{ Runtime [s/\num[exponent-mode=input, output-exponent-marker=]{e3}]} & \multicolumn{2}{R{2.5cm}}{Runtime [steps/\num[exponent-mode=input, output-exponent-marker=]{e6}]}                                                                            \\
                  & {Mean}                                      & {Std.}                                         & {Mean}                                                    & {Std.}                                                                                         & {Mean}                                                                                            & {Std.}   & {Mean}             & {Std.}   & {Mean}             & {Std.}   \\
        \midrule
        PPO       & \bfseries 3.53E-01                          & 1.34E-01                                       & 3.17E+01                                                  & 1.65E+01                                                                                       & 9.82E-02                                                                                          & 5.40E-02 & 9.08E+04           & 5.38E+04 & \bfseries 9.78E+06 & 9.05E+05 \\
        SAC       & 3.14E-01                                    & 1.90E-01                                       & \bfseries 3.42E+01                                        & 2.36E+01                                                                                       & \bfseries 1.12E-01                                                                                & 6.57E-02 & \bfseries 1.67E+05 & 1.29E+05 & 6.89E+06           & 3.85E+06 \\
        \midrule
        RPPO      & 4.13E-01                                    & 5.12E-02                                       & 5.54E+01                                                  & 7.40E+00                                                                                       & 1.64E-01                                                                                          & 1.95E-02 & 3.55E+05           & 1.28E+05 & 8.26E+06           & 3.12E+06 \\
        \bottomrule
    \end{tabular}
\end{table*}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\linewidth]{boxplot_eval_probability_efficiency_mean.max_architecture_algo.pdf}
    \caption{Maximum probability efficiency during training for all architecture variants with colour coded by DRL algorithm.}
    \label{fig:results:maximum_probability_efficiency}
\end{figure}

\subsection{Ablation Study: Path Feature Extraction}

The main contribution of this work is the use of the LSTMAE to handle the feature extraction. It is therefore imperative to evaluate its efficacy compared to the much simpler frame-stacking. \autoref{tbl:results:path_feature_extraction} shows the aggregated performance of the various feature extractors which closely align to those from \autoref{sect:results:architecture}. LSTMAE\_* is the most performant path feature extraction methods across all key performance metrics. All frame-stacking variants have low mean episodes lengths and low mean step reward. Most importantly, the mean maximum probability efficiency for the best frame-stacking variant, FS\_*\_CONV2D, is less than half as performant as LSTMAE\_*.

As expected from \autoref{sect:results:algorithm}, LSTMAE\_* also outperforms LSTM\_*  in all metrics from \autoref{tbl:results:path_feature_extraction}. This further highlights the efficiency of the RAE to capture the relevant information before DRL training to allow the DRL policy to focus on control.

\begin{table*}[htbp]
    \centering
    \caption{Aggregated path feature extraction results over multiple metrics gathered at the end of a training run.}
    \label{tbl:results:path_feature_extraction}
    \sisetup{exponent-thresholds=-3:3,table-format=2.3}
    \begin{tabular}{l|SS|SS|SS}
        \toprule
        Path Feature Extraction & \multicolumn{2}{R{2.5cm}}{Mean Step Reward} & \multicolumn{2}{R{2.5cm}}{Mean Episode Length} & \multicolumn{2}{R{2.5cm}}{Maximum Probability Efficiency}                                            \\
                                & {Mean}                                      & {Std.}                                         & {Mean}                                                    & {Std.}   & {Mean}             & {Std.}   \\
        \midrule
        FS\_*\_CONV2D           & 2.88E-01                                    & 8.96E-02                                       & 2.62E+01                                                  & 1.63E+01 & 7.60E-02           & 4.05E-02 \\
        FS\_*\_FCN              & 2.42E-01                                    & 9.84E-02                                       & 2.18E+01                                                  & 1.48E+01 & 7.20E-02           & 1.99E-02 \\
        FS\_*\_LSTM             & 2.25E-01                                    & 1.30E-01                                       & 2.19E+01                                                  & 1.71E+01 & 5.80E-02           & 2.32E-02 \\
        LSTMAE\_*               & \bfseries 5.54E-01                          & 6.52E-02                                       & \bfseries 5.62E+01                                        & 6.50E+00 & \bfseries 2.01E-01 & 3.04E-02 \\
        LSTM\_*                 & 4.13E-01                                    & 5.12E-02                                       & 5.54E+01                                                  & 7.40E+00 & 1.64E-01           & 1.95E-02 \\
        \bottomrule
    \end{tabular}
\end{table*}

\subsection{Benchmark}
\label{sect:results:benchmark}

Previous work has shown that \fssacfcn with a large enough network can outperform optimisation-based algorithms from the literature in \citep{ewers_deep_2025}. A key limitation of this approach was the large number of parameters and long training times required to achieve these results. The results from \autoref{sect:results:architecture} were achieved using a core policy of dimensions $2 \times 256$ whereas \citep{ewers_deep_2025} used $8 \times 2000$. For a fairer comparison, a $2 \times 2000$ version of \lstmaesac was also trained.

\autoref{tbl:results:long_runs} shows the mean probability efficiency from 2000 generated paths. It is evident that the smaller variants of both \fssacfcn and \lstmaesac performed poorer than their larger counterparts. However, the smaller \lstmaesac is only worse than the large \fssacfcn by a margin of \num{2.40E-02} whilst having 0.27\% the number of learnable parameters. Large \lstmaesac, which has 14.0\% of the amount of learnable parameters of large \fssacfcn, clearly outperforms all others variants. A performance difference of \num{4.00E+02} might hint at a insignificant result, however the sample sizes were above 2000 resulting in a p-value of 0.0151 which is above the threshold of 0.05 showing a meaningful difference in distributions. Large \lstmaesac therefore outperforms large \fssacfcn whilst being only 14\% of the size and only needing to be trained for 23\% of the time (90 days for \cite{ewers_deep_2025}) and 21 days for large \lstmaesac before it reached the no-improvement termination criterion).

\begin{table*}[htbp]
    \centering
    \caption{Mean probability efficiency from over 2000 generated paths by the respective architectures.}
    \label{tbl:results:long_runs}
    \sisetup{exponent-thresholds=-3:3}
    \begin{tabular}{@{}lSSrS@{}}
        \toprule
        Architecture                      & {Mean}             & {Std. }  & Core Policy       & {\# Parameters} \\
        \midrule
        \lstmaesac                        & \bfseries 2.00E-01 & 5.70E-02 & $2 \times 2000 $  & 2.17E+07        \\
        \lstmaesac                        & 1.72E-01           & 6.17E-02 & $2 \times 256$    & 4.25E+05        \\
        \midrule
        \fssacfcn \citep{ewers_deep_2025} & 1.96E-01           & 4.71E-02 & $ 8 \times 2000 $ & 1.55E+08        \\
        \fssacfcn                         & 9.03E-03           & 1.91E-02 & $2 \times 256$    & 4.36E+05        \\
        \midrule
        \lhcgwconv \citep{lin_uav_2009}   & 1.21E-01           & 8.02E-02 & n/a               & n/a             \\
        \bottomrule
    \end{tabular}
\end{table*}