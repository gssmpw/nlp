\section{Related Work}
\label{sect:related_work}

\nomdef{Deep Reinforcement Learning}{DRL} has seen significant application across exploration planning domains.
\citet{zuluaga_deep_2018} focuses on urban scenarios and incorporates frontier exploration into the search task where the agent gathers information on the environment over time. \citet{niroui_deep_2019}  uses SLAM with DRL to explore a cluttered environment, which has many parallels to mining search, in real time. Similarly \citet{peake_deep_2021}  applies DRL to WiSAR and employs dual-policy DRL, DDPG and recurrent A2C, which were trained separately to handle the exploration and trajectory planning. \citet{talha_autonomous_2022} also uses two DRL policies to handle the navigation and exploration separately. These foundational works notably omit consideration of prior probability distributions for target locations. \citet{ewers_deep_2025} explored this scenario and showed that DRL outperforms search planning methods from the literature, however with long training times, and too many parameters to be practical.

One potential solution to reducing the training overhead is to use hierarchical DRL with multiple more specialized models and policies. As previously outlined, \citet{peake_deep_2021} and \citet{talha_autonomous_2022} apply the dual-policy paradigm but still rely on a single policy to handle the top-level exploration planning. This then implies that the search planning from \citet{ewers_deep_2025} can be coupled with the lower-level trajectory or navigation planning from the aforementioned work. However, the same issues arise in that the mission planning policy is no better than before.

The architecture in \citet{ewers_deep_2025} uses frame-stacking to ensure that the \nomdef{Markov Decision Process}{MDP} is maintained. This is due to the reward at the current time step being dependent on the position of the agent in all previous time steps. If the agent intersects with the historical path then it is penalized by not gaining any new information. Whilst \citet{mock_comparison_2023} found that frame-stacking and recurrent architectures performed similarly, frame-stacking imposes a hard upper limit on the size of the input. The buffer size can be increased to overcome this issue but this has problems of its own. If the buffer is not full during training then inputs associated with data points far in the future cannot be trained leading to inefficient - or even unstable - training. It is therefore imperative to find another approach to handle the temporal input.

In \citet{raffin_decoupling_2019}, the concept of decoupling the feature extraction for DRL is explored. It was found that the proposed method was far superior to the standard single-policy approach in DRL. Interestingly, it was found that this method was only slightly better than an \nomdef{Auto-Encoder}{AE}. AEs have two components: an encoder applying a transformation on the input into a latent space, followed by a decoder approximating the reverse of this process \citet{berahmand_autoencoders_2024}. However, the method from this work requires frame-stacking again to work effectively.
\citet{pleines_generalization_2022} used a \nomdef{Long-Short Term Memory}{LSTM} layer \citep{hochreiter_long_1997} within the policy but another approach is to couple this with the aforementioned AE to leverage the sequence-to-sequence architecture proposed in work by \citet{park_sequencetosequence_2018} to predict the trajectory of a vehicle through a \nomdef{Recurrent Auto-Encoder}{RAE}. The sequence-to-sequence architecture was also used in \citet{cho_learning_2014} to process complex phrase representations for translations; another domain where there are dependencies on the entire variable length dataset for context.

\nomdef{Proximal Policy Optimisation}{PPO} \citep{schulman_proximal_2017}, its recurrent variant \nomdef{Recurrent PPO}{RPPO} \citep{raffin_stablebaselines3_2021,pleines_generalization_2022}, and \nomdef{Soft Actor-Critic}{SAC} \citep{haarnoja_soft_2019} are widely used in control problems such as by \citet{kaufmann_championlevel_2023} and \citet{yue_deep_2022}, as well as in other domains such as video gaming \citep{openai_dota_2019}.
PPO's stability and simplicity make it ideal for initial policy convergence in deterministic settings, while SAC's entropy-driven exploration excels in dynamic environments requiring adaptive action distributions \citep{shianifar_optimizing_2025}. The contrast between PPO's bounded policy updates (via advantage function clipping) and SAC's stochasticity (via the maximum entropy formulation) provides a methodological spectrum to evaluate robustness. PPO has become one of the de facto DRL algorithms in the literature, however \citet{mock_comparison_2023} found that it was unable to cope with higher dimensional observation spaces as well as SAC could.

Our work addresses the training instability of frame-stacking through a RAE architecture that compresses temporal dependencies into latent states. By integrating sequence-to-vector trajectory encoding with decoupled feature extraction, we enable dynamic adaptation to environmental uncertainty while maintaining compatibility with the dimensionality constraints of the policy networks. This approach uniquely resolves the conflict between long-horizon probabilistic reasoning and fixed-size observation spaces.
