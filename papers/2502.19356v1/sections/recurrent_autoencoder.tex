\section{Recurrent Autoencoder}
\label{sect:rae}

Recurrent encoders project a multidimensional input sequence to a fixed-length latent space $z_t$ through $E_\phi(x_t) \mapsto z_t$, parametrized by $\phi$ \citep{cho_learning_2014}. This work this uses the \nomdef{Long-Short Term Memory}{LSTM} architecture \citep{hochreiter_long_1997}. LSTM networks have a hidden state $h_t$ that is passed through the layers which holds the memory of previously seen states whereas $c_t$ is similar in that it carries the information about the sequence over time but is unique to each cell. Both $h_t$ and $c_t$ are critical to the handling of sequential data.
For each element in the input sequence, each layer computes the function
\begin{align}
    \label{eqn:lstm}
    \begin{split}
        i_t & = \sigma(W_{xi}x_t + W_{hi}h_{t-1} + b_{i}) \\
        f_t & = \sigma(W_{xf}x_t + W_{hf}h_{t-1} + b_{f}) \\
        g_t & = \tanh(W_{xg}x_t +  W_{hg}h_{t-1} + b_{g}) \\
        o_t & = \sigma(W_{xo}x_t + W_{ho}h_{t-1} + b_{o}) \\
        c_t & = f_t \odot c_{t-1} + i_t \odot g_t         \\
        h_t & = o_t \odot \tanh(c_t)
    \end{split}
\end{align}
where $i_t$, $f_t$, $g_t$, and $o_t$ are the input, forget, cell, and output gates respectively, $\sigma$ is the sigmoid function, $\odot$ is the hadamard product, and  $W$ and $b$ are the parameter matrices and vectors. $h_{t-1}$ is the hidden state of the previous layer at time $t-1$ and is initialized at time $t=0$ to be zero.
The \nomdef{Long Short-Term Memory}{LSTM} unit internal structure can be seen in \autoref{fig:rae:lstm_unit} showing the three gates interacting with the various states. $y_t$ is the output and is equal to $h_t$ of the final layer if multiple layers are used.

\begin{figure}[htbp]
    \centering
    \includetikz{figures/lstm.tikz.tex}
    \caption{The Long Short-Term Memory unit internal structure from \autoref{eqn:lstm}}
    \label{fig:rae:lstm_unit}
\end{figure}

An approximation of the input is then made by the decoder $D_\theta$, parameterized by $\theta$, of the same length. The decoder is passed the latent space $z_t$ as many times as there are rows in $x_t$, as well as the hidden state $h_t$ and cell state $c_t$. Each output is then the estimated value of the corresponding item in the input sequence.
The loss is calculated using the mean square error
\begin{equation}
    \mathcal{L}(s, \hat s, z) = \frac{1}{dim(s)}\sum_{i=1}^N (\hat{s}_i - s_i)^2
\end{equation}

The optimal RAE for $s_\mathrm{path}$ encoding is then found by
\begin{gather}
    \phi^*, \theta^* = \arg \min_{\phi, \theta} \mathcal{L} \left[ s_\mathrm{path}, D_\theta(E_\phi(s_\mathrm{path})), E_\phi(s_\mathrm{path}) \right]
\end{gather}

The RAE network can be seen in \autoref{fig:rae:lstm_ae}. Using an unbalanced setup in favour of the decoder gives a higher reconstruction potential, which leads to better encoder training. The larger decoder compensates for any loss of information during encoding, ensuring that even a suboptimal latent representation can still result in high-quality training updates. This setup also stabilizes training and accelerates convergence by allowing the decoder's higher capacity to handle complex reconstruction tasks effectively.

\begin{figure*}
    \centering
    \begin{tikzpicture}[
            input/.style={circle, draw},
            output/.style={circle, draw},
            fcn/.style={rectangle, draw},
            cnn/.style={rectangle, draw},
            rnn/.style={rectangle, draw},
        ]
        {[start chain]
            \node [rnn, ml/encoder, on chain] (lstm_e) {LSTM \\ input = 2 \\ hidden = 512 \\ layers = 1};
            \node [operator, on chain] (tanh_e) {~$\tanh$~};
            \node [fcn, on chain] (layer_norm_e) {LayerNorm};
            \node [fcn, ml/encoder, on chain] (fcn_e) {FCN \\ (512, 48)};
            \node [input, on chain] (z) {$\vec z$};
            \node [input,ml/decoder, on chain] (lstm_d) {LSTM \\ input = 48 \\ hidden = 2048 \\ layers = 2};
            \node [operator, on chain] (tanh_d) {~$\tanh$~};
            \node [fcn, on chain] (layer_norm_d) {LayerNorm};
            \node [fcn, ml/decoder, on chain] (fcn_d) {FCN \\ (2048, 2)};
            \node [operator, on chain] (softsign_d) {~$\frac{x}{1+|x|}$~};
            \node [output, on chain] (x_hat) {$\hat {\vec x}$};
        }
        \coordinate (lstm_e_north_west) at ($(lstm_e.west)+(0,0.5)$);
        \coordinate (lstm_e_south_west) at ($(lstm_e.west)-(0,0.5)$);

        \node [input, left=of lstm_e_north_west] (x) {$\vec x$};
        \draw (x) -- (lstm_e_north_west);
        \node [input, left=of lstm_e_south_west] (hx) {$\vec h$};
        \draw (hx) -- (lstm_e_south_west);

        \node [cell, inner xsep=1.5mm, fit=(lstm_e) (fcn_e)] (encoder_border) {};
        \node[anchor=south west] at (encoder_border.north west) {Encoder};
        \node [cell, inner xsep=1.5mm, fit=(lstm_d) (softsign_d)] (decoder_border) {};
        \node[anchor=south west] at (decoder_border.north west) {Decoder};
    \end{tikzpicture}
    \caption{RAE architecture using LSTMs for encoding and decoding.
        Using an unbalanced architecture, with the decoder being larger than the encoder, enables higher quality reconstruction which results in faster training and better performance.
        Softsign is applied to the decoder output to ensure that values meet the $s_\mathrm{path} \in [-1,1]$ requirement whilst providing close to linear mappings in this range.
    }
    \label{fig:rae:lstm_ae}
\end{figure*}