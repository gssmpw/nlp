
\section{Practical Implementation Details}
\label{sect:implementation}

\subsection{Cubature}

The integral is calculated using a cubature integration scheme \citep{cools_algorithm_1997} with constrained Delaunay triangulation \citep{chew_constrained_1987} to subdivide $H_t$ into triangles for fast computation. The use of pseudo-continuous over discrete integration has shown to greatly reduce noise in previous work \citep{ewers_enhancing_2024}. Whilst noise can be beneficial to promote exploration, this must be controllable and tunable. Reducing noise in the reward function, where cubature is being used, is critical to maximize learning efficiency else expensive techniques have to be employed \citep{wang_reinforcement_2020}.

\subsubsection{Recurrent Encoder}

\paragraph{Training}

During training the test dataset was unbatched and chunked into $N \sim \mathcal{U}(2, k)$ length sections where $k$ is the length of the longest path in the batch. If $N < k$ then the hidden states would be reused for the next section rather than resetting. This significantly improved the speed of convergence. Automatic mixed precision was used to further increase training times. A no-improvement criterion was used where training would terminate if the amount of epochs since a loss function decrease breaches a patience threshold.

\paragraph{Deployment}

The recurrent encoder, as detailed in Section \ref{sect:rae}, undergoes separate training from the DRL models with its parameters frozen during DRL training. This isolation prevents latent space divergence that could destabilize the learning during online updates. In our implementation, the encoder resides in the observation preprocessing pipeline rather than the policy network itself. This architectural choice minimizes replay buffer memory and compute requirements during training (critical for SAC's experience replay mechanism), though deployment permits alternative configurations.
The two viable deployment strategies are:
\begin{itemize}
    \item Hidden State Propagation: Stores only the encoder's hidden states (including cell state), giving a constant runtime performance per step with fixed memory usage (hidden states).
    \item  Full History Processing: Maintains complete trajectory histories, with the memory requirements and runtime performance growing linearly with the episode length.
\end{itemize}
In this work the former approach - hidden state propagation - is used. The full history processing approach would be required for other techniques such as temporal convolution networks \citep{lea_temporal_2016} or transformers \citep{vaswani_attention_2017}.

\subsection{Further Architecture Details}

The RPPO implementation used in this work is from \citet{raffin_stablebaselines3_2021} which aligns closely with \citet{pleines_generalization_2022}. The 2D convolution kernel inner-model feature extractor for \fsppoconvtwod and \fssacconvtwod is from \citet{mnih_humanlevel_2015}
