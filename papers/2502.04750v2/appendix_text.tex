\section{An even tighter but expensive approximation}
We consider a more general form for the conditional covariance of $q(\vf | \vu)$ as follows:
\begin{align}
    q(f) &= p(f_{\neq \vf, \vu} | \vf, \vu) q(\vf | \vu) q(\vu), \nonumber\\
    q(\vf | \vu) &= \gN(\vf; \kfu\kuuinv\vu; \rmC),\nonumber
\end{align}
Again, we can also obtain the optimal form for $ q(u) \propto p(\vu) \gN(\vy; \kfu\kuuinv\vu, \sigma^2\rmI)$, leading to the following collapsed bound
\begin{align}
    \gF_6(\theta) &= \vc - \frac{1}{2} \vy^\intercal (\qff + \sigma^2\rmI)^{-1} \vy - \frac{1}{2} \log |\qff + \sigma^2\rmI| - \frac{1}{2}\tr[(\sigma^{-2} \rmI + \dff^{-1})\rmC] - \frac{1}{2} \log |\rmC^{-1}\dff|. \nonumber
\end{align}
We can derive the optimal $\rmC$, $\rmC^{-1} = \dff^{-1} + \sigma^{-2}\rmI$ and the bound becomes:
\begin{align}
    \gF_8(\theta) 
    &= \vc - \frac{1}{2} \vy^\intercal (\qff + \sigma^2\rmI)^{-1} \vy - \frac{1}{2} \log |\qff + \sigma^2\rmI| - \frac{1}{2} \log |\rmI + \sigma^{-2}\dff| \nonumber\\
    &= \vc - \frac{1}{2} \vy^\intercal (\qff + \sigma^2\rmI)^{-1} \vy - \frac{1}{2} \log |\qff + \sigma^2\rmI| - \frac{1}{2} \sum_n \log (1 + \sigma^{-2}\lambda_n(\dff)|,\nonumber
\end{align}
where $\lambda_n(\rmX)$ is the $n$-th eigenvalue of $\rmX$. The bound above is as expensive as the original log marginal likelihood. 


% \section{A more general approximation based on Power EP}
% The new approximate posterior described in \cref{sec:tighter_approx} has broad applicability beyond the variational inference settings. This section describes how it can be used in power expectation propagation (Power-EP). We first write down the model and the approximation as follows,
% \begin{align}
%     &\text{model:} &\quad p(f, \vy) &= p(f_{\neq \vf, \vu} | \vf, \vu) p(\vf | \vu) p(\vu) \prod_n p(y_n | \vf_n) \\
%     &\text{approximation:} &\quad q(f) &\propto p(f_{\neq \vf, \vu} | \vf, \vu) q(\vf | \vu) p(\vu) \prod_n t_n(\vu)
% \end{align}
% where the factors $t_n(\vu)$ are assumed to be Gaussian. Following \cite{bui2017unifying}, we can derive the Power-EP procedure \citep{minka2004powerep} to update the above approximation iteratively and to estimate the log marginal likelihood. The optimal form for $t_n(\vu)$ is rank one, $t_n(\vu) = \gN(\kfnu\kuuinv\vu; g_n, v_n)$. In the regression case, $g_n = y_n$ and $v_n = \alpha m_n d_n + \sigma^2$ where $\alpha$ is the power in Power-EP. We can further derive the collapsed Power-EP approximate marginal likelihood,
% \begin{align}
%     \gF_7(\theta) 
%     &= \vc - \frac{1}{2} \vy^\intercal (\qff + \alpha \diag(\rmM\dff) + \sigma^2\rmI)^{-1} \vy - \frac{1}{2} \log |\qff + \alpha \diag(\rmM\dff) + \sigma^2\rmI| \nonumber \\
%     &\quad + \sum_n \left [- \frac{1-\alpha}{2\alpha} \log \left(1 + \alpha \frac{d_n}{d_n + \sigma^2} \right) - \frac{1}{2\alpha} \log\left( 1 - \alpha \frac{d_n}{d_n + \sigma^2}\right) - \frac{1}{2} \log \left(1 + \frac{d_n}{\sigma^2} \right) \right]. \nonumber
% \end{align}
% \thang{Check this to make sure when M is diagonal we get back the Power-EP log marginal likelihood}
% As a sanity check, we can take the limit of $\gF_7$ as $\alpha \rightarrow 0$ and show that it converges to the collapsed variational bound $\gF_4$ in \cref{eq:tighter_collapsed}. In addition, when $\alpha = 1$ [EP], the collapsed bound becomes
% \begin{align}
% \gF_7(\theta) 
%     = \vc - \frac{1}{2} \vy^\intercal (\qff + \diag(\rmM\dff) + \sigma^2\rmI)^{-1} \vy - \frac{1}{2} \log |\qff + \diag(\rmM\dff) + \sigma^2\rmI| \nonumber.
% \end{align}
% When $\rmM$ is a diagonal matrix, the bound above becomes the EP/FITC log marginal likelihood approximation.

\section{Exploring alternative parameterisations for the conditional posterior}
Instead of the general $\rmC$ as above or the form considered in the main text $\rmC = \dff^{1/2}\rmM \dff^{\top/2}$, we consider two other parameterisations that might allow efficient collapsed/un-collapsed bounds and predictions. We first rewrite the uncollapsed bound and the predictive mean and variance here for clarity,
\begin{align*}
    \gF_{\textrm{uncollapsed}}
        &= - \kl [q(\vu) || p(\vu)] - \int_\vu q(\vu) \kl [q(\vf | \vu) || p(\vf | \vu)] +  \sum_{n} \int_{\vu, f(x_n)} q(\vu) q(f({x_n}) | \vu) \log p(y_n | f({x_n})), \\
    m_* &= \ksu \kuuinv \rvm_\vu, \\
    v_* &= \kss - \ksu \kuuinv \kus + \ksu \kuuinv \rmS_\vu \kuuinv \kus - (\ksf - \qsf) (\dff - \rmC) (\kfs- \qfs),
\end{align*}

We first consider $\rmC = \rmM^{1/2}\dff\rmM^{1/2}$. While this allows efficient exact predictive marginal distributions at training points, the middle term in the bound is costly to compute due to the presence of $\dff$: 
\begin{align}
    - \int_\vu q(\vu) \kl [q(\vf | \vu) || p(\vf | \vu)]
    = - \frac{1}{2} \tr(\dff^{-1}\rmM^{1/2}\dff\rmM^{1/2}) + \frac{1}{2}\log |\rmM| + \frac{N}{2}\nonumber.
    % \label{eq:mid-collapsed} 
\end{align}

Another special case of the parameterisation presented in the main text is $\rmC = m \dff$, i.e., a single $m$ is shared across all training points. This conveniently leads to tractable exact predictive variances at training points, $v_n = m d_n + \mathbf{k}_{f_n\vu} \kuuinv \rmS_\vu \kuuinv \mathbf{k}_{\vu f_n}$. The middle term in the bound can be simplified to,
\begin{align}
    - \int_\vu q(\vu) \kl [q(\vf | \vu) || p(\vf | \vu)]
    = \frac{N}{2}[1 + \log(m) - m] \nonumber.
\end{align}
In the regression case, this leads to  the optimal $m = \sigma^2 / (N^{-1}\sum_n d_n + \sigma^2)$ and the following collapsed bound,
\begin{align}
    \gF_9(\theta) =\vc - \frac{1}{2} \vy^\intercal (\qff + \sigma^2\rmI)^{-1} \vy - \frac{1}{2} \log |\qff + \sigma^2\rmI| - \frac{N}{2} \log \left( 1+\frac{\sum_n d_n}{N\sigma^2} \right).
\end{align}
This bound is looser than the collapsed bound in \cref{eq:tighter_collapsed}, due to the Jensen's inequality $\log (1 + \sum_n x_n / N) \geq N^{-1}\sum_n \log(1 + x_n)$.

\section{Additional experimental results}
\subsection{Large-scale regression benchmarks}
\begin{figure*}[!ht]
    \centering
    \includegraphics[width=\linewidth]{tighter_reg_results_test_rmse_False.pdf}
    \caption{Test root mean squared errors (RMSE) for various sparse approximations on eight regression datasets and various numbers of pseudo-points. For SOLVEGP and T-SOLVEGP, M is evenly split for $\vu$ and $\vv$. Lower is better. Best viewed in colour.}
    \label{fig:reg_rmse}
\end{figure*}

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=\linewidth]{tighter_reg_results_train_elbo_False.pdf}
    \caption{Log marginal likelihood approximations (ELBO) for various sparse approximations on eight regression datasets and various numbers of pseudo-points. For SOLVEGP and T-SOLVEGP, M is evenly split for $\vu$ and $\vv$. Higher is better. Best viewed in colour.}
    \label{fig:reg_elbo}
\end{figure*}