\section{Experiments}
In this section, we describe the experimental setup, including the datasets, evaluation metrics, and baseline methods. We then present the main experimental results, followed by a comprehensive analysis that offers an in-depth examination of the model's capabilities and efficiency.

\begin{table*}[t]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lccccccc}
\toprule
\multirow{2}{*}{\textbf{Methods}} & \multicolumn{3}{c}{\textbf{RestBench-TMDB}} & \multicolumn{2}{c}{\textbf{RestBench-Spotify}}  & \textbf{API-Bank (LV1)} & \textbf{API-Bank (LV2)} \\
\cline{2-4}\cline{5-6}\cline{6-7}\cline{7-8}
& Success(\%) & Accuracy(\%)  & Path(\%) & \multicolumn{1}{c}{Success(\%)} & \multicolumn{1}{c}{Path(\%)} & Correctness(\%)     & Correctness(\%)  \\
\midrule
\rowcolor{mygray}
\textit{Text-based Tool Learning}  &    &   &         &        &      &   & \\
\midrule
ReAct~\cite{DBLP:conf/iclr/YaoZYDSN023}  & 76.0   & 48.0   & 50.0    & 68.42  &  52.63   & 73.93  & 56.30   \\
Chameleon~\cite{DBLP:conf/nips/LuPCGCWZG23}    & 75.0    & 45.0     & 52.0  &  70.18  &  63.16  & 74.87   & 37.04  \\
ConAgents~\cite{shi-etal-2024-learning}    &   72.0  &   38.0    &   57.0   &  64.92  &   68.42   &  67.26 &   36.24     \\
RestGPT~\cite{DBLP:journals/corr/abs-2306-06624}    & 83.0    & 33.0     & 49.0      &   61.41   &   57.89  &  65.47  &  34.83    \\
EasyTool~\cite{DBLP:journals/corr/abs-2401-06201}       &  75.0       &  45.0     & 62.0     &  62.19  &   64.92   &  74.97 &   58.24  \\
\midrule
\rowcolor{mygray}
\textit{Code-based Tool Learning}  &    &   &         &        &      &   & \\
\midrule
ATC~\cite{DBLP:journals/corr/abs-2405-16533} &   78.0  &   58.0    &   62.0   &  65.47  &   68.42   &  70.21  &   52.18     \\
CodeAct~\cite{DBLP:conf/icml/WangCY0L0J24}     &  80.0  &  56.0 &   67.0  & 71.93  &   66.67   & 75.94   & 54.07     \\
\midrule
\texttt{ToolCoder}    &  \textbf{85.0}  &  \textbf{78.0}   &  \textbf{83.0}  &  \textbf{87.72}   &   \textbf{78.95}  &  \textbf{83.08}  &  \textbf{62.41} \\
\quad \textit{w/o} Reusable Repository  & 83.0   & 71.0  & 78.0   &    78.95    &  71.93    &  83.08$^{*}$ &  62.41$^{*}$ \\
\quad \textit{w/o} Error Reflection  &  75.0  & 65.0  & 77.0    &    73.68    &  70.18   &  79.85  &  58.02 \\
\bottomrule
\end{tabular}}
\caption{Main experimental results on RestBench and API-Bank datasets. All methods are implemented with \textit{gpt-4o-mini}. The best results are bolded. $^*$ Results from the API-Bank dataset are reported without the reusability mechanism due to its built-in API calls in this dataset.}
% $^*$ indicates Due to the use of built-in API calls in the API-Bank dataset, the reusability mechanism in \texttt{ToolCoder} is not applied; therefore, the corresponding results are marked with an asterisk.
\label{tab:main}
\vspace{-0.3cm}
\end{table*}

\subsection{Experimental Settings}
\paragraph{Datasets.} To validate the effectiveness of our method in tool learning, we conduct experiments on two well-established benchmarks: RestBench~\cite{DBLP:journals/corr/abs-2306-06624} and API-Bank~\cite{li-etal-2023-api}. RestBench comprises two real-world scenarios: TMDB, a human-annotated dataset with 100 tasks utilizing 54 tools, designed for movie-related scenarios; and Spotify, which features 40 tools tailored to music-related tasks. Each tool in RestBench is paired with a detailed RESTful API document, making it inherently suitable for benchmarking the real-world tool-learning capabilities of LLMs. API-Bank consists of 73 commonly used APIs and 314 tool-use dialogues with 753 manually annotated API calls, designed to evaluate the effectiveness of LLMs in utilizing tools.
% 解释为何不使用ToolBench这类数据集，因为他们很多API缺少输出格式的定义文档Schema

\paragraph{Evaluation metrics.} For RestBench~\cite{DBLP:journals/corr/abs-2306-06624}, we adopt their evaluation framework and utilize three metrics: success rate (\textbf{Success\%}), which relies on human evaluation to determine whether the model’s output effectively fulfills the user query; correct path rate (\textbf{Path\%}), which measures the proportion of ground-truth tools correctly included in the model-generated tool calls. We also introduce \textbf{Accuracy\%}, specifically for the TMDB scenario, to assess whether the generated answers align with the ground-truth answers. These ground-truth answers are derived by executing the annotated solution paths, providing a reliable benchmark for comparison. For API-Bank~\cite{li-etal-2023-api}, we evaluate API-calling performance using their \textbf{Correctness\%} metric, which compares the ground-truth API outputs with the execution results of the model-generated API calls.

\subsection{Baselines} We compare our method against seven well-known baselines: (1) ReAct~\cite{DBLP:conf/iclr/YaoZYDSN023}, prompting LLMs to generate interleaved chains of thought and actions. (2) Chameleon~\cite{DBLP:conf/nips/LuPCGCWZG23}, an LLM-based agent that creates multi-step plans for tool usage and executes them sequentially. (3) ConAgents~\cite{shi-etal-2024-learning}, which facilitates the collaboration of three specialized LLMs to solve complex tasks. (4) RestGPT~\cite{DBLP:journals/corr/abs-2306-06624} features a coarse-to-fine planning module and a tool executor. (5) EasyTool~\cite{DBLP:journals/corr/abs-2401-06201} converts lengthy and diverse tool documentation into unified, concise instructions for easier tool usage. (6) ATC~\cite{DBLP:journals/corr/abs-2405-16533} utilizes a chain of tools through programming and proposes a black-box probing method. (7) CodeAct~\cite{DBLP:conf/icml/WangCY0L0J24}, enabling LLMs to generate executable code snippets as actions to interact with tools.

\subsection{Main Results}
The experimental results in Table~\ref{tab:main} provide a comprehensive evaluation of \texttt{ToolCoder} in comparison to existing text-based and code-based tool learning approaches. These results highlight the significant performance improvements achieved by \texttt{ToolCoder}, validating the effectiveness of its code-empowered framework.

From the experimental results presented in Table~\ref{tab:main}, text-based approaches demonstrate limited effectiveness in complex tool learning scenarios, as evidenced by their low path rates and accuracy scores. This underperformance stems from two key limitations: the inherent constraints of natural language prompting for structured reasoning, and their inability to effectively handle execution errors. While code-based methods like ATC and CodeAct show significant improvements in path planning, achieving higher success rates than their text-based counterparts, they still face challenges in maintaining consistent performance across different scenarios. This is particularly evident in their accuracy scores, suggesting that even with better planning capabilities, these methods still lack robust mechanisms for error handling and output verification. 

Building on the strengths of code-empowered LLMs, \texttt{ToolCoder} achieves SOTA performance across all benchmarks compared to baseline methods, consistently achieving the highest success, accuracy, and correct path rates, showcasing its robustness and adaptability. For instance, on RestBench-Spotify, \texttt{ToolCoder} improves the success rate by 10.79\% and the accuracy by 22.22\% compared to the strongest baseline, CodeAct. Similarly, on API-Bank (LV2), it improves correctness by 8.34\% over CodeAct, highlighting its ability to handle multi-step tool interactions. The performance gains stem from \texttt{ToolCoder}'s code-empowered framework, which leverages the reasoning capabilities of code-empowered LLMs and the advantages of systematic software development principles to generate precise tool execution paths and accurate final responses.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\columnwidth]{fig/planning_comparison.pdf}
    \caption{Impact of code-empowered planning on path correctness. Removing the structured Python function scaffold $c$ leads to significant performance degradation across all benchmarks.}
    \label{fig:planning_comparison}
    \vspace{-0.65cm}
\end{figure}

\subsection{Ablation Study}
\paragraph{Effect of Code-Empowered Planning}  
To investigate the effect of code-empowered planning, we remove the structured Python function scaffold $c$ (denoted as \textit{w/o code}) during the tool planning phase and compare its performance with the original \texttt{ToolCoder}. As shown in Figure~\ref{fig:planning_comparison}, the correct path rate drops significantly without $c$ across all benchmarks. This decline highlights the scaffold's role in enhancing the model's reasoning capabilities. Without the scaffold, the model struggles to fully leverage its code-enhanced reasoning abilities, leading to a marked decrease in correct path rate. The absence of $c$ results in a reduced understanding of task objectives and an impaired ability to structure information into coherent plans. These findings underscore the critical importance of the structured Python function scaffold $c$ in activating the model's reasoning potential and maintaining effective task planning.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{fig/cumulative_success_rate.pdf}
    \caption{Effect of the function reusability mechanism on cumulative success rate and accuracy across different sample proportions (inference process progresses).}
    % \caption{Effect of the function reusability mechanism on sample proportions on cumulative performance metrics: Success rate and accuracy for TMDB dataset, and success rate for Spotify dataset.  }
    \label{fig:cumulative_success_rate}
    \vspace{-0.5cm}
\end{figure}

\paragraph{Effect of Reusable Function Repository}
To assess the impact of the reusable function repository $\mathcal{F}$, we conduct an ablation study by removing the reusability mechanism. As shown in Table~\ref{tab:main}, this results in a 2–5\% performance drop across various benchmarks, highlighting the effectiveness of reusable code snippets.

Furthermore, to gain deeper insights into how reusability enhances tool execution, we analyze the cumulative success rate and cumulative accuracy across different sample proportions, as illustrated in Figure~\ref{fig:cumulative_success_rate}. The results demonstrate a significant performance boost due to the reusability mechanism. As the inference process progresses (with increasing sample proportions on the x-axis), the cumulative success rate steadily grows. This indicates that the model accumulates experience from past successes, aiding in the generation of more robust code. For example, on the TMDB dataset, the cumulative success rate approaches 90\% as more samples are utilized, reflecting improved generalization and execution efficiency. Additionally, the cumulative accuracy on TMDB consistently improves, further confirming that the reusability mechanism not only enhances success rates but also improves answer precision.

% Furthermore, to gain deeper insights into how reusability enhances tool execution, we analyze the cumulative success rate and cumulative accuracy across different sample proportions, as illustrated in Figure~\ref{fig:cumulative_success_rate}. The results demonstrate a significant performance boost due to the reusability mechanism. On the TMDB dataset, the cumulative success rate steadily increases, approaching 90\% as more samples are utilized, indicating improved generalization and execution efficiency. Similarly, for the Spotify dataset, we observe a sharp rise in success rate during the initial stages, suggesting that reusability accelerates adaptation to task requirements. Additionally, the cumulative accuracy on TMDB consistently improves, further confirming that the reusability mechanism not only enhances success rates but also improves answer precision.

\paragraph{Effect of Error Reflection Mechanism}
To assess the impact of the error reflection mechanism, we conduct an ablation study by removing it from \texttt{ToolCoder} and evaluating the resulting performance changes. The results, as shown in Table~\ref{tab:main}, indicate a significant performance drop across multiple datasets when the error reflection mechanism is removed. Both correct path rate and success rates decrease markedly. This decline is attributed to the error reflection mechanism's ability to diagnose planning and execution errors, allowing for iterative refinement and adaptive self-correction. Without this mechanism, the model's reliability is severely compromised. This ablation study demonstrates the critical role of the error reflection mechanism in ensuring robust tool learning and effective adaptation to complex execution challenges.

\begin{table}[t]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lccc}
\toprule
\multirow{2}{*}{\textbf{Methods}} &  \multicolumn{3}{c}{\textbf{ToolAlpaca (Real-world)}} \\
\cmidrule(lr){2-4}
& Procedure & Response & Overall \\ 
\midrule
ReAct & 64.86 & 60.81 & 54.05 \\ 
Chameleon & 68.06 & 58.33 & 57.92 \\ 
CodeAct  & 68.92 & 58.11 & 56.94 \\
\midrule
\texttt{ToolCoder} & \textbf{78.38} & \textbf{75.68} & \textbf{72.97} \\
\quad \textit{w/o} Error Reflection & 76.46 & 68.11 & 67.94 \\
\bottomrule
\end{tabular}}
\caption{Performance comparison of different models on real-world API tasks using the ToolAlpaca dataset. Metrics include Procedure, Response, and Overall scores. All methods are implemented with \textit{gpt-4o-mini}. Due to the small number of questions under each tool category, we do not use the reusability mechanism on this dataset.}
\label{tab:toolalpaca}
\vspace{-0.3cm}
\end{table}

\subsection{Analysis of Generalization Capabilities}
% 为什么要做toolalpaca，和之前的数据集有何区别
\paragraph{Experiments on New Dataset ToolAlpaca-Real}
To evaluate the generalization performance of \texttt{ToolCoder}, we conduct experiments on the ToolAlpaca dataset~\cite{DBLP:journals/corr/abs-2306-05301} with real-world API tasks. The evaluation framework contains three metrics: Procedure, Response, and Overall. Procedure assesses the model's ability to select appropriate actions, use correct parameters, and execute steps without redundancy. Response measures the model's capability to produce outputs that fully satisfy the user's requirements. Overall is a comprehensive metric that reflects the accuracy and effectiveness of the entire process, encompassing both procedural correctness and response quality.

The experimental results in Table~\ref{tab:toolalpaca} demonstrate that \texttt{ToolCoder} outperforms traditional methods, including ReAct, Chameleon, and CodeAct, across all metrics. \texttt{ToolCoder} achieves significant improvements in Procedure and Response, which lead to a superior Overall performance. These advancements can be attributed to the code-empowered LLM's superior planning and execution capabilities, enabling more accurate action selection and task completion. Additionally, the integration of a reflection strategy further enhances the quality of the responses, resulting in more effective handling of real-world API tasks compared to baselines.

\begin{table}[t]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lccc}
\toprule
\multirow{2}{*}{\textbf{Methods}} &  \multicolumn{3}{c}{\textbf{RestBench-TMDB}} \\
\cmidrule(lr){2-4}
& Success & Accuracy & Path \\ 
\midrule
\rowcolor{mygray}
\textit{Qwen2.5-14B-Instruct}  &    &   &   \\
ReAct~\cite{DBLP:conf/iclr/YaoZYDSN023} & 67.0 & 40.0 & 42.0 \\ 
Chameleon~\cite{DBLP:conf/nips/LuPCGCWZG23} & 68.0 & 38.0 & 45.0 \\ 
CodeAct~\cite{DBLP:conf/icml/WangCY0L0J24}  & 72.0 & 58.0 & 62.5 \\
\texttt{ToolCoder} & \textbf{81.0} & \textbf{72.0} & \textbf{76.0} \\
\midrule
\rowcolor{mygray}
\textit{Qwen2.5-Coder-14B-Instruct}  &    &   &   \\
ReAct~\cite{DBLP:conf/iclr/YaoZYDSN023} & 68.0 & 41.5 & 47.0 \\ 
Chameleon~\cite{DBLP:conf/nips/LuPCGCWZG23} & 70.0 & 45.4 & 50.0 \\ 
CodeAct~\cite{DBLP:conf/icml/WangCY0L0J24}  & 80.0 & 65.0 & 75.0 \\
\texttt{ToolCoder} & \textbf{85.0} & \textbf{76.0} & \textbf{80.0} \\
\midrule
\rowcolor{mygray}
\textit{Qwen2.5-32B-Instruct}  &    &   &   \\
ReAct~\cite{DBLP:conf/iclr/YaoZYDSN023} & 73.0 & 50.0 & 51.0 \\ 
Chameleon~\cite{DBLP:conf/nips/LuPCGCWZG23} & 71.0 & 48.5 & 54.5 \\ 
CodeAct~\cite{DBLP:conf/icml/WangCY0L0J24}  & 82.0 & 62.5 & 76.5 \\
\texttt{ToolCoder} & \textbf{89.0} & \textbf{82.0} & \textbf{84.0} \\
\midrule
\rowcolor{mygray}
\textit{Qwen2.5-Coder-32B-Instruct}  &    &   &   \\
ReAct~\cite{DBLP:conf/iclr/YaoZYDSN023} & 76.5 & 56.5 & 58.0 \\ 
Chameleon~\cite{DBLP:conf/nips/LuPCGCWZG23} & 78.0 & 56.0 & 57.0 \\ 
CodeAct~\cite{DBLP:conf/icml/WangCY0L0J24}  & 92.0 & 82.0 & 85.0 \\
\texttt{ToolCoder} & \textbf{96.0} & \textbf{90.0} & \textbf{91.0} \\
\bottomrule
\end{tabular}}
\caption{Evaluate the performance of \texttt{ToolCoder} on different open source LLMs on RestBench-TMDB.}
\label{tab:opensource_lms}
% \vspace{-0.5cm}
\end{table}

\paragraph{Experiments on Open-Source LLMs}
To evaluate the effectiveness of \texttt{ToolCoder} with open-source LLMs, we conduct comprehensive experiments on RestBench-TMDB. The results in Table~\ref{tab:opensource_lms} reveal several significant findings. First, model scaling demonstrates consistent performance improvements across all methods. When scaling from 14B to 32B, we observe substantial gains in success rate, accuracy, and path correctness across all approaches. Second, the comparison between base models and their coder-enhanced counterparts reveals the substantial advantages of code-specialized capabilities. This is particularly evident in code-empowered methods like \texttt{ToolCoder} and CodeAct. These improvements demonstrate that code-specialized models are better equipped to handle the structured reasoning and precise execution required in tool learning tasks. Third, \texttt{ToolCoder} consistently outperforms all baselines across different model configurations. This superior performance validates the effectiveness of our code-empowered framework in leveraging the code-specific strengths of LLMs for tool learning.

\begin{table}[t]
\centering
\resizebox{0.8\linewidth}{!}{
\begin{tabular}{lcccc}
\toprule
\multirow{2}{*}{\textbf{Methods}} & \multicolumn{2}{c}{\textbf{RestBench}}  & \multicolumn{2}{c}{\textbf{API-Bank}}\\
\cline{2-3}\cline{4-5}
& TMDB & Spotify & LV1 & LV2 \\
\midrule
ReAct  & 7.78   & 8.68   & 6.45  & 8.38  \\
Chameleon   & 6.52    & 7.12    & 5.64  &  6.92   \\
ConAgents   &   6.78  &   7.68    &  5.45   &  7.38   \\
RestGPT   & 9.04   & 10.24   &  7.27  &   9.84   \\
\texttt{ToolCoder}    &  7.69  &  8.70   &  6.46  &  8.47 \\
\bottomrule
\end{tabular}}
\caption{Efficiency analysis of different methods on the RestBench and API-Bank datasets. We report the average number of API calls required per question, where a lower value indicates higher efficiency.}
\label{tab:efficiency}
% \vspace{-0.5cm}
\end{table}

\subsection{Efficiency Analysis}
To analyze the efficiency of API usage among different methods, we compare the average number of LLMs calls required per question on the RestBench and API-Bank benchmarks, as shown in Table~\ref{tab:efficiency}. \texttt{ToolCoder} demonstrates comparable efficiency to baseline methods such as ReAct, Chameleon, and ConAgents, with basically similar API usage. This result suggests that \texttt{ToolCoder} achieves higher correct path rates and success rates in task planning and execution compared to other methods, without incurring additional efficiency costs, thereby maintaining a practical balance between improved performance and resource consumption.


% 0. 消融实验（从主实验上证明几个模块的作用） DONE
% 1. 错误分析：证明有效地减少了一些执行期间的错误（Reflection的作用）饼图
% 2. success_rate随着时间的变化，证明可复用策略增强了执行的成功率（Reusable的作用）折线图 DONE
% 3. text-prompt（直接把伪代码删掉）和code-based prompt的对比，证明code带来的规划能力上的优势 表格 DONE
% 4. 不同的模型（证明泛化性）
% 5. 运行时间的对比（证明效率)
% 6. 在附录里放一个case的对比
