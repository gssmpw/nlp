To analyze the effectiveness of the PLD method, we examine the probability that PLD samples both normal and noisy interactions.
\begin{theorem}
\label{the:p_i_j}
    % For a user \( u \), there are \( n \) items with normal interactions and \( m \) items with noisy interactions. Assume that the loss of the user's normal interactions follows a Gaussian distribution \( \mathcal{N}(\mu_1, \sigma^2) \) and the loss of noisy interactions follows a Gaussian distribution \( \mathcal{N}(\mu_2, \sigma^2) \), where \( \mu_1 < \mu_2 \) and \( \mu_1, \mu_2 > \sigma \). From these \( m+n \) interactions, we first randomly select $k$ interactions, and then resample one positive interaction according to Equation~\ref{eq:p_i}. Let \( \Lambda_{\text{normal}} \) denote the sum of sampling probabilities for normal interactions, and \( \Lambda_{\text{noise}} \) denote the sum of sampling probabilities for noisy interactions. Define the following:
    % alpha &= \exp\left(-\mu_1 + \frac{\sigma^2}{2}\right), 
    %              &\beta = \exp\left(-\mu_2 + \frac{\sigma^2}{2}\right),\\
    For a user \( u \), there are \( n \) items with normal interactions and \( m \) items with noisy interactions. 
    Suppose the loss of each normal interaction follows a distribution with mean \(\mu_1\) and variance \(\sigma^2\), and the loss of each noisy interaction follows a distribution with mean \(\mu_2\) and variance \(\sigma^2\). We assume \(\mu_1 < \mu_2\) and \(\mu_1, \mu_2 > \sigma\). From these \( m+n \) interactions, we first randomly select $k$ interactions, and then resample one positive interaction according to Equation~\ref{eq:p_i}. Let \( \Lambda_{\text{normal}} \) denote the sum of sampling probabilities for normal interactions, and \( \Lambda_{\text{noise}} \) denote the sum of sampling probabilities for noisy interactions. 
    Let \(\alpha\) and \(\beta\) represent the expectations of the normal and noisy interaction losses, respectively, where the expectation is taken over the exponential of the loss. Define the following:
    \begin{equation*}
        \begin{aligned}
            & \begin{aligned}
                \gamma &= \exp(\sigma^2) - 1, 
                 % &\eta = \frac{n}{n+m} \alpha + \frac{m}{n+m} \beta,\\
                 &\eta = \frac{n\alpha + m\beta}{n+m},\\
            \end{aligned}\\
            & \begin{aligned}
                \Gamma &= \frac{(n\alpha - m\beta)}{m+n} \cdot \frac{(\alpha^2 + \beta^2)(\gamma + \frac{m}{n+m}) + \beta^2 }{\eta^3},\\
                \chi &= \frac{\gamma}{(n+m)} \left[ n\alpha^2 - m\beta^2 \right]
            \end{aligned} 
        \end{aligned}
    \end{equation*}
    we have:
    \begin{equation}
    \label{eq:the}
        \begin{aligned}
            \mathbb{E}[\Lambda_{\text{normal}} - \Lambda_{\text{noise}}] = \left \{
                \begin{aligned}
                    &\frac{n - m}{n + m} ~~ & k = 1 \\
                    & \begin{aligned}
                        & \frac{n\alpha - m\beta}{(m+n)\eta} + \underbrace{\frac{\Gamma}{k} - \frac{\chi}{C^2} \frac{k}{(k-1)^2}}_{\mathrm{Fluctuation}~~\mathrm{term}}
                    \end{aligned}
                     ~~ & k > 1 \\
                \end{aligned}
            \right. ,
        \end{aligned}
    \end{equation}
    where \( C \in [\beta, \alpha] \) is a constant term.
\end{theorem}

The proof of Theorem~\ref{the:p_i_j} is detailed in Appendix~\ref{prf:the_p}. The term \(\frac{\Gamma}{k} - \frac{\chi}{C^2} \frac{k}{(k-1)^2}\) arises from the variance component in the denominator of the softmax function, exhibiting larger fluctuations when \(k\) is small, while stabilizing as \(k\) increases.

% \renewcommand{\algorithmicrequire}{ \textbf{Input:}}     % Use 'Input' in the format of Algorithm
% \renewcommand{\algorithmicensure}{ \textbf{Output:}}    % Use 'Output' in the format of Algorithm
% \algnewcommand{\LineComment}[1]{\Statex \(\triangleright\) #1}
% \begin{algorithm}[t]
%     \caption{Training Procedure with PLD} % Algorithm name
%     \label{al:dtr}
%     \begin{algorithmic}[1]
%     \renewcommand{\baselinestretch}{1.5}
%         \Require{Training set $\mathcal{D}$, pool size $k$, temperature coefficient $\tau$, batch size $\mathbb{B}$, loss function $\mathcal{L}(u, i, j)$}
%         \Ensure{Model parameters $\Theta$.}
%         \While{stopping criteria not met}
%             \LineComment \textit{PLD}
%             \State Draw $\mathbb{B}$ triples $(u, \mathcal{C}_{u}^{k}, j)$ from $\mathcal{D}$. 
%             \State Initialize the batch set $\mathcal{D}_{\mathbb{B}} = \emptyset$
%             \For{each $(u, \mathcal{C}_{u}^{k}, j)$}
%                 \State Calculate $l_i$ for $i \in \mathcal{C}_{u}^{k}$ using $\mathcal{L}(u, i, j)$.
%                 \State Resample $i^*$ based on Equation~\ref{eq:p_tau} within $\mathcal{C}_{u}^{k}$.
%                 \State Add $(u, i^*, j)$ to the batch set $\mathcal{D}_{\mathbb{B}}$.
%             \EndFor
%             \LineComment \textit{Standard Training}
%             \State Update $\Theta$ according to $\mathcal{L}(u, i, j)$ for each $(u, i^*, j)$ in $\mathcal{D}_{\mathbb{B}}$.
%         \EndWhile
%         \State \Return $\Theta$
%     \end{algorithmic}
% \end{algorithm}

According to Theorem~\ref{the:p_i_j}, when \(k=1\), PLD reduces to standard training with \(\mathbb{E}[\Lambda_{\text{normal}} - \Lambda_{\text{noise}}] = \frac{n - m}{n + m}\). For \(k>1\), given \(\alpha, \beta, \gamma > 0\), with \(\alpha > \beta\) and \(n \gg m\), we find that \(\Gamma > \frac{\chi}{C^2}\). Thus, \(\mathbb{E}[\Lambda_{\text{normal}} - \Lambda_{\text{noise}}] > \frac{n - m}{n + m}\). This indicates that \textbf{PLD outperforms standard training, demonstrating superior denoising capabilities.}

% To further enhance the effectiveness of the PLD method, we can increase \(\frac{n\alpha - m\beta}{(m+n)\eta}\). Specifically, let \(\xi = \frac{\beta}{\alpha} = \exp(\mu_1 - \mu_2) < 1\)~(see derivations in Appendix~\ref{App:prof}). Then, we can express \(\frac{n\alpha - m\beta}{(m+n)\eta} = \frac{n - \xi m}{n + \xi m}\). Notably, since \(\frac{\partial \frac{n\alpha - m\beta}{(m+n)\eta}}{\partial \xi} < 0\), we can decrease \(\xi\) to amplify \(\frac{n\alpha - m\beta}{(m+n)\eta}\), thus enlarging \(\mathbb{E}[\Lambda_{\text{normal}} - \Lambda_{\text{noise}}]\).
To further enhance the effectiveness of the PLD method, we can increase \(\frac{n\alpha - m\beta}{(m+n)\eta}\). Specifically, let \(\xi = \frac{\beta}{\alpha} = \exp\left(g\left(\mu_1 - \mu_2\right)\right) < 1\), where $g(\cdot)$ is a monotonically increasing function. We can express \(\frac{n\alpha - m\beta}{(m+n)\eta} = \frac{n - \xi m}{n + \xi m}\). Notably, since \(\frac{\partial \frac{n\alpha - m\beta}{(m+n)\eta}}{\partial \xi} < 0\), we can decrease \(\xi\) to amplify \(\frac{n\alpha - m\beta}{(m+n)\eta}\), thus enlarging \(\mathbb{E}[\Lambda_{\text{normal}} - \Lambda_{\text{noise}}]\).
Based on this idea, we introduce a temperature coefficient \(\tau\) into Equation~\ref{eq:p_i}:
\begin{equation}
\label{eq:p_tau}
    P_{u, v} = \frac{\exp(-l_{u,v} / \tau)}{\sum_{j \in \mathcal{C}_{u}^{k}} \exp(-l_{u,j} / \tau)}.
\end{equation}
% In this manner, the new \(\xi'\) can be considered as \(\xi' = \exp\left((\mu_1 - \mu_2)/\tau\right)\). By reducing \(\tau\), we can further enlarge \(\frac{n\alpha - m\beta}{(m+n)\eta}\). The algorithmic flow of PLD is outlined in Appendix~\ref{sec:app_methods} (Algorithm~\ref{al:dtr}).

In this manner, the new \(\xi'\) can be considered as \(\xi' = \exp\left(g\left(\left(\mu_1 - \mu_2\right)/\tau\right)\right)\). By reducing \(\tau\), we can further enlarge \(\frac{n\alpha - m\beta}{(m+n)\eta}\). The algorithmic flow of PLD is outlined in Appendix~\ref{sec:app_methods} (Algorithm~\ref{al:dtr}).

Additionally, we perform an in-depth analysis and comparison of the time and space complexity of PLD and baseline methods. For further details, please refer to Appendix~\ref{sec:dis}.
