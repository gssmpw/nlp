Recommender systems have become essential tools for mitigating information overload in the modern era~\cite{koren2009matrix, he2020lightgcn}. Since obtaining explicit user feedback (e.g., ratings) is often hindered by the need for active user participation, these systems typically rely on implicit feedback to capture user behavior patterns, thereby facilitating effective recommendations~\cite{gantner2012personalized, he2024double, saito2020unbiased}. Nonetheless, factors such as human error, uncertainty, and ambiguity in user behavior inevitably introduce significant noise into this feedback~\cite{toledo2016fuzzy, zhang2023robust}. This noise can bias learned behavior patterns, undermine system robustness, and degrade recommendation performance~\cite{zhang2023robust, wu2016collaborative}.

Mainstream methods for noise elimination in recommender systems primarily focus on reweighting feedback. A commonly observed pattern in the overall loss distribution, which represents the losses of all interactions (i.e., feedback), is that \textbf{noisy interactions tend to exhibit higher losses during training}~\cite{wang2021denoising, he2024double, gao2022selfguided, lin2023autodenoise}. Based on this observation, these methods either reduce the training weight of high-loss interactions or discard them entirely. For instance, R-CE~\cite{wang2021denoising} assigns weights to interactions based on their loss magnitude, with higher losses receiving smaller weights. T-CE~\cite{wang2021denoising} proportionally discards interactions with the highest losses at a predefined rate. These methods typically compute the loss for each interaction using pointwise loss functions, such as Binary Cross Entropy (BCE) loss~\cite{wang2021denoising, he2024double}. 

However, we identify two key limitations in this approach. To illustrate, we separate the overall loss distribution into normal and noisy interaction loss distributions. For clarity, we define an overlap region that includes interactions deviating from the assumptions of existing methods, i.e., where noisy interactions exhibit lower losses or normal interactions exhibit higher losses:

\begin{figure}
    \centering
    \includegraphics[width=0.475\textwidth]{Imgs/loss_compare.pdf}
    \caption{Probability distribution of losses. The overlap region includes interactions that deviate from the common assumption in existing methods, i.e., where noisy interactions exhibit lower losses or normal interactions exhibit higher losses. Quartiles are used instead of max-min values to mitigate the influence of extreme values when determining the overlap region.}
    \label{fig:intro_loss}
\end{figure}
\begin{itemize}[leftmargin=*]
    \item We observe a significant overlap between normal and noisy interactions in the overall loss distribution, as shown on the left side of Figure~\ref{fig:intro_loss}. In LightGCN~\cite{he2020lightgcn}, trained with BCE loss on the MIND~\cite{wu2020mind} dataset with a 30\% noise ratio, 11.86\% of normal interactions (\num{98596}) fall within the overlap, while 11.52\% of noisy interactions (\num{35926}) also fall within this overlap.
    \item Moreover, this overlap becomes more pronounced when transitioning from pointwise loss functions to pairwise loss functions, such as BPR loss~\cite{rendle2009bpr}, as shown on the right side of Figure~\ref{fig:intro_loss}. Specifically, the percentage of normal interactions in the overlap increases to 19.58\% (\num{162763}, a 65.1\% increase), while the percentage of noisy interactions rises to 18.16\% (\num{60170}, a 67.5\% increase).
\end{itemize}
It is important to note that interactions in the overlap region cannot be reliably identified using the overall loss distribution alone. This limits the effectiveness of methods that rely solely on the overall loss distribution for denoising.

To address these issues, we investigate the causes of the overlap between noisy and normal interactions in the overall loss distribution. We find that, due to the variance in users' personal loss distributions, the losses of normal interactions for some users overlap with those of noisy interactions for others, leading to significant overlap in the \textbf{overall loss distribution}. Furthermore, we observe that, for a given user, \textbf{there is a clear distinction between normal and noisy interactions in the user's personal loss distribution}.

Based on this insight, denoising from the perspective of a user's personal loss distribution, rather than the overall loss distribution, yields more effective results. However, the variance in users' personal loss distributions and the differing ratios of noisy interactions across users make it challenging to set an appropriate drop rate for filtering noisy interactions or to adjust their weights based on interaction losses, as traditional denoising approaches do~\cite{wang2021denoising, he2024double}.

Given these considerations, we propose a resampling strategy for \textbf{D}enoising based on users' \textbf{P}ersonal \textbf{L}oss distributions, named \textbf{PLD}. PLD reduces the probability of noisy interactions being optimized by resampling training interactions. Specifically, PLD first uniformly samples a user's interacted items to construct candidate pools,  ensuring the stability of subsequent resampling. In the resampling stage, it selects an item for optimization from the candidate pool based on the user's personal loss distribution, prioritizing normal interactions. Additionally, we conduct an in-depth theoretical analysis of PLD, demonstrating its effectiveness and suggesting that adjusting the sharpness of the resampling distribution using a scaling coefficient can further improve the probability of sampling normal interactions. Extensive experiments show that PLD achieves state-of-the-art performance across various noise ratios, not only with BCE loss but also with BPR loss.

The main contributions of our work are as follows:

\begin{itemize}[leftmargin=*]
    \item We identify the limitations of existing loss-based denoising methods, highlighting the significant overlap between normal and noisy  interactions in the overall loss distribution.
    \item We find that, for a given user, there is a clear distinction between normal and noisy interactions in the user's personal loss distribution. Leveraging this insight, we propose a resampling strategy for denoising, PLD.
    \item We conduct an in-depth theoretical analysis, proving PLD's effectiveness and suggesting ways to further enhance its performance.
    \item Extensive experiments validate the superiority of our proposed method across various datasets and noise ratios.
\end{itemize}

