
\subsection{Proofs}
\label{App:prof}

\textbf{For clarity}, we hypothesize the distributions followed by the normal interaction loss and noisy interaction loss. Specifically, we assume that the loss of the user's normal interactions follows a Gaussian distribution \( \mathcal{N}(\mu_1, \sigma^2) \), while the loss of noisy interactions follows a Gaussian distribution \( \mathcal{N}(\mu_2, \sigma^2) \), where \( \mu_1 < \mu_2 \) and \( \mu_1, \mu_2 > \sigma \). Thus, we have
\begin{equation*}
    \alpha = \exp\left(-\mu_1 + \frac{\sigma^2}{2}\right), 
                 \quad \beta = \exp\left(-\mu_2 + \frac{\sigma^2}{2}\right).
\end{equation*}


Derivations based on different distributions are similar to the following and do not affect the final theoretical results.


\begin{proposition}
\label{Pro:1}
    Let $x_i \sim \mathcal{N}(\mu, \sigma^2)$ and $N \sim \mathrm{Binomial}(k, \frac{n}{n+m})$. Define 
    \begin{equation*}
        S_x = \sum_{i=1}^N \exp(-x_i).
    \end{equation*}
    Then, the expected value and variance of $S_x$ are given by:
    \begin{equation*}
        \begin{aligned}
            \mathbb{E}[S_x] &= \frac{kn}{m+n} \exp\left(-\mu + \frac{\sigma^2}{2}\right), \\
            \mathrm{Var}[S_x] &= \frac{kn}{n+m} \exp(-2\mu + \sigma^2) \left( \exp(\sigma^2) - \frac{n}{n+m} \right).
        \end{aligned}
    \end{equation*}
\end{proposition}

\begin{proof}
    To compute $\mathbb{E}[S_x]$, we apply the Double Expectation Theorem~\cite{rice2007mathematical}. First, we condition on $N$:
    \begin{equation*}
        \begin{aligned}
            \mathbb{E}[S_x] &= \mathbb{E}_{N}\left[ \mathbb{E}_{S_x}[S_x \mid N] \right] \\
            &= \mathbb{E}_{N}\left[ N \exp\left(-\mu + \frac{\sigma^2}{2}\right) \right].
        \end{aligned}
    \end{equation*}
    The inner expectation evaluates to $N \exp\left(-\mu + \frac{\sigma^2}{2}\right)$ since $\mathbb{E}[\exp(-x_i)]$ for each $x_i \sim \mathcal{N}(\mu, \sigma^2)$ is known. Taking the expectation over $N \sim \mathrm{Binomial}(k, \frac{n}{n+m})$, we obtain:
    \begin{equation*}
        \mathbb{E}[S_x] = \frac{kn}{m+n} \exp\left(-\mu + \frac{\sigma^2}{2}\right).
    \end{equation*}

    Next, we compute the variance of $S_x$ using the Law of Total Variance~\cite{chung2000course}:
    \begin{equation*}
        \mathrm{Var}[S_x] = \mathbb{E}_{N}\left[ \mathrm{Var}_{S_x}[S_x \mid N] \right] + \mathrm{Var}_{N}\left[ \mathbb{E}_{S_x}[S_x \mid N] \right].
    \end{equation*}
    For the first term, $\mathrm{Var}_{S_x}[S_x \mid N] = N \exp(-2\mu + \sigma^2) \left( \exp(\sigma^2) - 1 \right)$, leading to:
    \begin{equation*}
        \mathbb{E}_{N}\left[ \mathrm{Var}_{S_x}[S_x \mid N] \right] = \mathbb{E}[N] \exp(-2\mu + \sigma^2) \left( \exp(\sigma^2) - 1 \right).
    \end{equation*}
    For the second term, we use the variance of $N$, yielding:
    \begin{equation*}
        \mathrm{Var}_{N}\left[ \mathbb{E}_{S_x}[S_x \mid N] \right] = \mathrm{Var}[N] \exp(-2\mu + \sigma^2).
    \end{equation*}
    Substituting $\mathbb{E}[N] = \frac{kn}{n+m}$ and $\mathrm{Var}[N] = \frac{knm}{(n+m)^2}$, we obtain the final expression:
    \begin{equation*}
        \mathrm{Var}[S_x] = \frac{kn}{n+m} \exp(-2\mu + \sigma^2) \left( \exp(\sigma^2) - \frac{n}{n+m} \right).
    \end{equation*}
\end{proof}

\begin{proposition}
\label{Pro:2}
    Given two independent random variables $X$ and $Y$, we have 
    \begin{equation*}
        \mathbb{E}\left[ \frac{1}{X+Y} \right] \approx \frac{1}{\mathbb{E}[X] + \mathbb{E}[Y]} \left( 1 + \frac{\mathrm{Var}[X] + \mathrm{Var}[Y]}{\left( \mathbb{E}[X] + \mathbb{E}[Y] \right)^2} \right).
    \end{equation*}
\end{proposition}

\begin{proof}
    Let $Z = X + Y$ and define $g(Z) = \frac{1}{Z}$. Applying the second-order Taylor expansion~\cite{lehmann2006theory} of $g(Z)$ around $\mathbb{E}[Z]$, we obtain:
    \begin{equation*}
        g(Z) \approx g(\mathbb{E}[Z]) + g'(\mathbb{E}[Z]) \left( Z - \mathbb{E}[Z] \right) + \frac{1}{2} g''(\mathbb{E}[Z]) \left( Z - \mathbb{E}[Z] \right)^2.
    \end{equation*}
    Taking the expectation of both sides, the linear term vanishes due to $\mathbb{E}[Z - \mathbb{E}[Z]] = 0$, leaving:
    \begin{equation*}
        \begin{aligned}
            \mathbb{E}[g(Z)] & \approx g(\mathbb{E}[Z]) + \frac{1}{2} g''(\mathbb{E}[Z]) \mathbb{E}\left[ \left( Z - \mathbb{E}[Z] \right)^2 \right] \\
            & = g(\mathbb{E}[Z]) + \frac{1}{2} g''(\mathbb{E}[Z]) \mathrm{Var}[Z].
        \end{aligned}
    \end{equation*}
    Substituting $g(Z) = \frac{1}{Z}$, we have $g'(\mathbb{E}[Z]) = -\frac{1}{\mathbb{E}[Z]^2}$ and $g''(\mathbb{E}[Z]) = \frac{2}{\mathbb{E}[Z]^3}$. Thus, the expression simplifies to:
    \begin{equation*}
        \begin{aligned}
            \mathbb{E}[g(Z)] & \approx \frac{1}{\mathbb{E}[Z]} + \frac{1}{2} \frac{2}{\mathbb{E}[Z]^3} \mathrm{Var}[Z] \\
            & = \frac{1}{\mathbb{E}[Z]} \left( 1 + \frac{\mathrm{Var}[Z]}{\mathbb{E}[Z]^2} \right).
        \end{aligned}
    \end{equation*}
    Since $Z = X + Y$ and $X$ and $Y$ are independent, we use the properties $\mathbb{E}[Z] = \mathbb{E}[X] + \mathbb{E}[Y]$ and $\mathrm{Var}[Z] = \mathrm{Var}[X] + \mathrm{Var}[Y]$. Substituting these into the above expression gives:
    \begin{equation*}
        \mathbb{E}\left[ \frac{1}{X+Y} \right] \approx \frac{1}{\mathbb{E}[X] + \mathbb{E}[Y]} \left( 1 + \frac{\mathrm{Var}[X] + \mathrm{Var}[Y]}{\left( \mathbb{E}[X] + \mathbb{E}[Y] \right)^2} \right),
    \end{equation*}
    which completes the proof.
\end{proof}

\begin{proof}[\textbf{Proof of Theorem~\ref{the:p_i_j}}]
\label{prf:the_p}
    For \( k \) samples, let \( N \) be the number of normal samples and \( M \) be the number of noisy samples. We have:
    \begin{equation*}
        \begin{aligned}
            \mathbb{E}[N] &= k \cdot \frac{n}{n+m}, \quad &\mathrm{Var}[N] = k \cdot \frac{nm}{(n+m)^2}, \\
            \mathbb{E}[M] &= k \cdot \frac{m}{n+m}, \quad &\mathrm{Var}[M] = k \cdot \frac{nm}{(n+m)^2}.
        \end{aligned}
    \end{equation*}

    When \( k = 1 \), the sampling probabilities simplify to:
    \begin{equation*}
        P_i = P_j = \frac{1}{n+m}.
    \end{equation*}
    Therefore, the expected difference in sampling probabilities is:
    \begin{equation*}
        \mathbb{E}[\Lambda_{\text{normal}} - \Lambda_{\text{noise}}] = \frac{n - m}{n + m}.
    \end{equation*}
    
    Now, for \( k > 1 \), let \( x_i \sim \mathcal{N}(\mu_1, \sigma_1^2) \) represent the loss of a normal sample \( i \), and \( y_j \sim \mathcal{N}(\mu_2, \sigma^2) \) represent the loss of a noisy sample \( j \). According to Equation~\ref{eq:p_i}, the probability of selecting sample \( i \) is:
    \begin{equation*}
        P_i = \frac{\exp(-x_i)}{\sum_{i=1}^N \exp(-x_i) + \sum_{j=1}^M \exp(-y_j)}.
    \end{equation*}
    
    Define:
    \begin{equation*}
        S_x = \sum_{i=1}^N \exp(-x_i), \quad S_y = \sum_{j=1}^M \exp(-y_j).
    \end{equation*}
    The sum of the sampling probabilities of normal interactions becomes:
    \begin{equation*}
        \Lambda_{\text{normal}} = \sum_{i=1}^N P_i = \frac{S_x}{S_x + S_y}.
    \end{equation*}

    Then, we have:
    \begin{equation*}
        \mathbb{E}[\Lambda_{\text{normal}}] = \mathbb{E}[S_x] \cdot \mathbb{E}\left[\frac{1}{S_x + S_y}\right] + \mathrm{Cov}\left(S_x, \frac{1}{S_x + S_y}\right).
    \end{equation*}
    Expanding the covariance term:
    \begin{equation}
        \begin{aligned}
            \mathrm{Cov}\left(S_x, \frac{1}{S_x + S_y}\right) &= \mathbb{E}_N \left[\mathrm{Cov}\left(\sum_{i=1}^N \exp(-x_i) \mid N, \frac{1}{S_x + S_y}\right)\right] \\ 
            & \quad + \mathrm{Cov}\left(\mathbb{E}\left[\sum_{i=1}^N \exp(-x_i) \mid N\right], \mathbb{E}\left[\frac{1}{S_x + S_y}\right]\right).
        \end{aligned}
    \end{equation}
    Assuming a linear dependence between \( S_x + S_y \) and \( \exp(-x_i) \), we introduce a constant \( C \in [\beta, \alpha] \) such that:
    \begin{equation*}
        S_x + S_y \approx \exp(-x_i) + (k-1)C.
    \end{equation*}
    This leads to:
    \begin{equation*}
        \mathrm{Cov}\left(\exp(-x_i), \frac{1}{S_x + S_y}\right) \approx \mathrm{Cov}\left(\exp(-x_i), \frac{1}{\exp(-x_i) + (k-1)C}\right).
    \end{equation*}
    Using the approximation:
    \begin{equation*}
        \frac{1}{\exp(-x_i) + (k-1)C} \approx \frac{1}{(k-1)C} - \frac{\exp(-x_i)}{(k-1)^2 C^2},
    \end{equation*}
    we find:
    \begin{equation*}
        \begin{aligned}
            \mathrm{Cov}\left(\exp(-x_i), \frac{1}{S_x + S_y}\right) & \approx \mathrm{Cov}\left(\exp(-x_i), \frac{1}{(k-1)C}\right) \\
            & \quad - \mathrm{Cov}\left(\exp(-x_i), \frac{\exp(-x_i)}{(k-1)^2 C^2}\right) \\
            & = - \frac{\mathrm{Var}\left[\exp(-x_i)\right]}{(k-1)^2 C^2}.
        \end{aligned}
    \end{equation*}

    Consequently:
    \begin{equation*}
        \mathbb{E}[\Lambda_{\text{normal}}] = \mathbb{E}[S_x] \cdot \mathbb{E}\left[\frac{1}{S_x + S_y}\right] - \frac{kn}{n+m} \cdot \frac{\mathrm{Var}\left[\exp(-x_i)\right]}{(k-1)^2 C^2}.
    \end{equation*}
    
    Finally, applying the definitions of \( \alpha, \beta, \gamma, \eta, \Gamma, \) and \( \chi \), we derive the expression for \( \mathbb{E}[\Lambda_{\text{normal}} - \Lambda_{\text{noise}}] \) as:
    \begin{equation*}
        \begin{aligned}
            \mathbb{E}[\Lambda_{\text{normal}} - \Lambda_{\text{noise}}] = \frac{n\alpha - m\beta}{(m+n)\eta} + \frac{\Gamma}{k} - \frac{\chi}{C^2} \frac{k}{(k-1)^2},
        \end{aligned}
    \end{equation*}
    where the term \( \frac{\chi}{C^2} \frac{k}{(k-1)^2} \) arises from the covariance component of the variance term.
\end{proof}


\subsection{Methods}
\label{sec:app_methods}

The algorithmic flow of PLD is outlined in Algorithm~\ref{al:dtr}.

\renewcommand{\algorithmicrequire}{ \textbf{Input:}}     % Use 'Input' in the format of Algorithm
\renewcommand{\algorithmicensure}{ \textbf{Output:}}    % Use 'Output' in the format of Algorithm
\algnewcommand{\LineComment}[1]{\Statex \(\triangleright\) #1}
\begin{algorithm}[t]
    \caption{Training Procedure with PLD} % Algorithm name
    \label{al:dtr}
    \begin{algorithmic}[1]
    \renewcommand{\baselinestretch}{1.5}
        \Require{Training set $\mathcal{D}$, pool size $k$, temperature coefficient $\tau$, batch size $\mathbb{B}$, loss function $\mathcal{L}(u, i, j)$}
        \Ensure{Model parameters $\Theta$.}
        \While{stopping criteria not met}
            \LineComment \textit{PLD}
            \State Draw $\mathbb{B}$ triples $(u, \mathcal{C}_{u}^{k}, j)$ from $\mathcal{D}$. 
            \State Initialize the batch set $\mathcal{D}_{\mathbb{B}} = \emptyset$
            \For{each $(u, \mathcal{C}_{u}^{k}, j)$}
                \State Calculate $l_i$ for $i \in \mathcal{C}_{u}^{k}$ using $\mathcal{L}(u, i, j)$.
                \State Resample $i^*$ based on Equation~\ref{eq:p_tau} within $\mathcal{C}_{u}^{k}$.
                \State Add $(u, i^*, j)$ to the batch set $\mathcal{D}_{\mathbb{B}}$.
            \EndFor
            \LineComment \textit{Standard Training}
            \State Update $\Theta$ according to $\mathcal{L}(u, i, j)$ for each $(u, i^*, j)$ in $\mathcal{D}_{\mathbb{B}}$.
        \EndWhile
        \State \Return $\Theta$
    \end{algorithmic}
\end{algorithm}

\subsection{Model Discussion}
\label{sec:dis}
\input{Section/4-Subsection/3-Discussion}

\input{Section/Table/appen_backbone}

\input{Section/Table/appen_various_noise}

\subsection{Experiments}
\label{sec:app_exp}

We further assess the denoising performance of PLD when combined with certain contrastive learning-based denoising methods. Our results show that PLD can substantially improve the recommendation performance of the state-of-the-art contrastive learning-based denoising method, DCCF~\cite{ren2023disentangled}, as demonstrated in Table~\ref{tab:app_cl}.

In addition, we examine a more realistic scenario where the noise ratio varies across users. To ensure a fair evaluation, we fix the addition of 3 noisy interactions per user (10\% of the average interactions in the Gowalla dataset). Under this setting, the recommendation performance is significantly degraded, causing many methods to fail. However, even in this challenging scenario, PLD exhibits strong performance and achieves notable improvements in denoising, as shown in Table~\ref{tab:app_varying}.




