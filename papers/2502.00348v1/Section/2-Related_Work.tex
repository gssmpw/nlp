\subsection{Collaborative Filtering}
Collaborative Filtering (CF) remains a fundamental technique in the design of recommender systems and has been extensively adopted in numerous research efforts~\cite{schafer2007collaborative, covington2016deep, ying2018graph}. At its core, CF operates on the principle that users with similar behaviors or preferences are likely to have aligned future choices, making it a powerful tool for predicting recommendations~\cite{koren2021advances}. A widely used method within this paradigm is Matrix Factorization, which models latent relationships between users and items by factorizing the interaction matrix~\cite{koren2009matrix}. This interaction matrix can be constructed from both explicit feedback, such as ratings, and implicit feedback, which includes indirect behavioral signals like clicks, views, and purchases~\cite{hu2008collaborative}. Although implicit feedback is often noisier and lacks clear negative signals, it provides a wealth of data that is crucial for building recommendation models in real-world scenarios where explicit ratings are scarce~\cite{hu2008collaborative}.

In recent years, the introduction of deep learning has expanded CFâ€™s capabilities, particularly for handling the complexities of implicit feedback. Neural-based models can capture more nuanced user-item interactions, often observed through implicit signals. For example, CDL~\cite{wang2015collaborative} integrates auxiliary item data into CF using neural networks, effectively addressing data sparsity. Similarly, NCF~\cite{he2017neural} replaces the traditional dot product operation with a multi-layer neural architecture, which is better suited for modeling the intricate patterns found in implicit user interactions. More recently, the rise of Graph Neural Networks (GNNs) has inspired graph-based CF models~\cite{wang2020disentangled, xia2022hypergraph, wu2022graph}, such as NGCF~\cite{wang2019neural} and LightGCN~\cite{he2020lightgcn}, which have shown exceptional performance in leveraging implicit feedback. Despite these advancements, an issue remains: the vulnerability of these models to noise, particularly from implicit data, which continues to undermine their robustness~\cite{zhang2023robust}.

\subsection{Denoising Implicit Feedback}
Recommender systems that rely on implicit feedback have garnered substantial attention. However, recent research highlights their susceptibility to noise in implicit feedback~\cite{zhang2023robust, wang2023tutorial}. The primary strategies for mitigating noise in recommender systems can be broadly categorized into two types~\cite{zhang2023robust}: reweight-based approaches~\cite{wang2021denoising, he2024double, lin2023autodenoise, wang2023efficient, gao2022selfguided, ye2023towards, tian2022learning} and self-supervised approaches~\cite{yang2022knowledge, wu2021selfsupervised, wang2022learning}.

\textbf{Reweight-based Methods.} These approaches aim to reduce or eliminate the influence of noisy interactions by adjusting their contributions during training~\cite{wang2021denoising, he2024double, lin2023autodenoise, wang2023efficient}. Some methods reduce the weights of noisy interactions~\cite{wang2021denoising, gao2022selfguided, lin2023autodenoise, wang2023efficient}, while others remove them entirely~\cite{wang2021denoising, he2024double}. A common observation driving these methods is that noisy interactions typically produce higher training losses in the overall loss distribution~\cite{wang2021denoising, he2024double, gao2022selfguided, lin2023autodenoise}. For instance, R-CE~\cite{wang2021denoising} leverages loss values as indicators of noise, assigning reduced weights to potentially noisy interactions, while T-CE~\cite{wang2021denoising} eliminates interactions with the highest loss values at a predefined drop rate. DCF~\cite{he2024double} further addresses challenges posed by hard positive samples and the data sparsity introduced by dropping interactions. However, since normal and noisy interactions overlap in the overall loss distribution, the effectiveness of these methods is limited. Additionally, BOD~\cite{wang2023efficient} formulates the determination of interaction weights as a bi-level optimization problem to learn more effective denoising weights, though this approach is significantly more time-consuming.

\textbf{Self-supervised Methods.} Self-supervised approaches mitigate noise by introducing auxiliary signals through self-supervised learning~\cite{ma2024madm, wang2022learning, fan2023graph, quan2023robust, zhu2023knowledge}. For example, SGL~\cite{wu2021selfsupervised} enhances the robustness of user-item representations by applying various graph augmentations, such as node dropping and edge masking. KGCL~\cite{yang2022knowledge} incorporates external knowledge graph data to refine the masking process. Meanwhile, DeCA~\cite{wang2022learning} posits that clean data samples tend to yield consistent predictions across different models and, therefore incorporates two recommendation models during training to better differentiate between clean and noisy interactions. However, self-supervised approaches rely heavily on the design of self-supervised tasks, and these heuristics cannot always guarantee effective denoising performance.



