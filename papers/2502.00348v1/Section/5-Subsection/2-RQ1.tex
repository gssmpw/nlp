

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\textwidth]{Imgs/noise_line.pdf}
    \caption{Recommendation performance of different denoising methods across various noise ratios.}
    \label{fig:noise_mind}
\end{figure*}

In this section, we address \textbf{RQ1} by focusing on two key aspects: recommendation performance and robustness against noise. All results in this section are based on the widely adopted BPR loss function~\cite{rendle2009bpr}. For a comprehensive evaluation, results using the BCE loss function are provided in Section~\ref{sec:rq2}.

\input{Section/Table/BCE_loss}

\begin{figure}
    \centering
    \includegraphics[width=0.475\textwidth]{Imgs/exception_p.png}
    \caption{Theoretical, practical, and standard values (i.e., without resampling in PLD) of $\mathbb{E}[\Lambda_{\text{normal}} - \Lambda_{\text{noise}}]$ for 6 users on MIND with 30\% additional noise.}
    \label{fig:exception}
\end{figure}

\textbf{Recommendation Performance.}
We evaluate the effectiveness of PLD across three common datasets without introducing additional noise, as shown in Table~\ref{tab:performance}. The performance of R-CE~\cite{wang2021denoising}, T-CE~\cite{wang2021denoising}, and DCF~\cite{he2024double} is suboptimal due to the limitations of using overall loss distribution as a denoising criterion for pairwise loss functions, as discussed earlier (in Figure~\ref{fig:intro_loss}). In particular, T-CE applies a fixed drop ratio, which truncates part of the loss completely, unintentionally discarding many normal interactions and leading to a significant performance decrease.

On the other hand, DeCA~\cite{wang2022learning} and BOD~\cite{wang2023efficient} demonstrate more stable performance, securing runner-up results across several metrics. \textbf{Our method}, PLD, mitigates the impact of noisy interactions by resampling based on users' personal loss distributions, producing stable and optimal results across all datasets. It achieves significant improvements, with 4.29\% and 4.42\% increase in Recall@20 and NDCG@20, respectively, using MF as the backbone model.
% , and a 1.99\% and 1.74\% increase in Recall@20 and NDCG@20, respectively, using LightGCN as the backbone model.

\textbf{Robustness against Noise.}
We further assess PLD's robustness to noise by randomly introducing noisy interactions at ratios\footnote{A ratio of 0.1 means adding noisy interactions equal to $10\% \vert \mathcal{I}_{\text{normal}} \vert$.} ranging from 0.1 to 0.5, as shown in Figure~\ref{fig:noise_mind}. As the noise ratio increases, the performance of all methods decreases. Additionally, we observe that in some cases, specifically when using LightGCN as the backbone model, denoising methods based solely on overall loss distribution (T-CE, R-CE, and DCF) perform worse than the backbone model itself. This further confirms that overall loss distribution is unsuitable for denoising in pairwise loss scenarios.





In contrast, our method, PLD, remains the most stable across all noise ratios, consistently outperforming other denoising methods.
Additionally, we show the results on a larger dataset, MIND-Large (Table~\ref{tab:mindl}), where only the results at a noise ratio of 0.1 are presented due to space limitations. The conclusions drawn from MIND-Large are consistent with those from the other datasets.

Additionally, we present results for PLD combined with contrastive learning-based denoising methods in Appendix~\ref{sec:app_exp}, along with results under more challenging noise conditions.

