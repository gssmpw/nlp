\input{Section/Table/Dataset}

\input{Section/Table/Performance}

\input{Section/Table/MINDL}



\subsubsection{Datasets}
We utilize four widely recognized datasets: the \textbf{Gowalla} check-in dataset~\cite{liang2016modeling}, the \textbf{Yelp2018} business dataset, and the \textbf{MIND} and \textbf{MIND-Large} news recommendation datasets~\cite{wu2020mind}. The Gowalla and Yelp2018 datasets include all users, while for the MIND dataset, we sample two subsets of users, constructing MIND and MIND-Large, following~\cite{zhang2024lorec2}. Consistent with~\cite{zhang2024improving, zhangunderstanding}, we exclude users and items with fewer than 10 interactions from our analysis. We allocate 80\% of each user's historical interactions to the training set, reserving the remainder for testing. Additionally, 10\% of the training set is randomly selected to form a validation set for hyperparameter tuning. Detailed statistics for the datasets are summarized in Table~\ref{tab:datasets}.



\subsubsection{Baselines}
We incorporate various denoising methods, including four reweight-based approaches and one self-supervised method. Specifically, we evaluate R-CE, T-CE~\cite{wang2021denoising}, BOD~\cite{wang2023efficient}, and DCF~\cite{he2024double} as reweight-based methods, and DeCA~\cite{wang2022learning} as a self-supervised method.
\begin{itemize}[leftmargin=*]
   \item \textbf{R-CE}~\cite{wang2021denoising}: R-CE assigns reduced training weight to high-loss interactions.
   \item \textbf{T-CE}~\cite{wang2021denoising}: T-CE drops interactions with the highest loss values at a predefined drop rate.
   \item \textbf{BOD}~\cite{wang2023efficient}: BOD treats the process of determining interaction weights as a bi-level optimization problem to learn more effective denoising weights.
   \item \textbf{DCF}~\cite{he2024double}: DCF addresses the challenges posed by hard positive samples and the data sparsity introduced by dropping interactions in T-CE.
   \item \textbf{DeCA}~\cite{wang2022learning}: DeCA posits that clean samples tend to yield consistent predictions across different models, incorporating two recommendation models during training to better differentiate between clean and noisy interactions.
\end{itemize}

\subsubsection{Evaluation Metrics}
We adopt standard metrics widely employed in the field. The primary metrics for evaluating recommendation performance are the top-$k$ metrics: Recall at $K$ ($\mathrm{Recall}@K$) and Normalized Discounted Cumulative Gain at $K$ ($\mathrm{NDCG}@K$), as described in~\cite{zhang2023robust, he2020lightgcn, herlocker2004evaluating}. For evaluation, we set $K=20$ and $K=50$, following~\cite{wang2019neural, zhang2024improving}.

\subsubsection{Implementation Details} 
In our study, we employ two commonly used backbone recommendation models: MF~\cite{koren2009matrix} and LightGCN~\cite{he2020lightgcn}. The configuration of both denoising methods and recommendation models involves selecting a learning rate from \{0.1, 0.01, $\dots$, $1 \times 10^{-5}$\}, and a weight decay from \{0, 0.1, $\dots$, $1 \times 10^{-5}$\}. For PLD, the candidate pool size $k$ is selected from \{2, 3, 5, 10, 20\}, and the temperature coefficient $\tau$ is chosen from \{0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5\}. For the baselines, hyperparameter settings follow those specified in the original publications. Our implementation code is available at the following link\footnote{\url{https://github.com/Kaike-Zhang/PLD}}.





