@misc{BeMyAI, note = {Available at: \url{https://www.bemyeyes.com/blog/announcing-be-my-ai}. Accessed on December 4th 2024.}, journal={{Announcing ‘Be My AI,’ Soon Available for Hundreds of Thousands of Be My Eyes Users}}, author={Be My Eyes}, year={2023}, month={Sep}}

@misc{OpenAI_2024, url={https://openai.com/index/hello-gpt-4o}, journal={Hello GPT-4o}, author={OpenAI}, year={2024}, month={May}}

@misc{WebAim-10,
    author = "{WebAim}",
    title = "{{Screen Reader User Survey \#10 Results}}",
    year = "2024",
    howpublished = "\url{https://webaim.org/projects/screenreadersurvey10/}",
    note = "[Online; accessed 13-August-2024]"
  }

@article{accessibleWebDev2021,
author = {Kearney-Volpe, Claire and Hurst, Amy},
title = {{Accessible Web Development: Opportunities to Improve the Education and Practice of Web Development with a Screen Reader}},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {1936-7228},
doi = {10.1145/3458024},
abstract = {There are a growing number of jobs related to web development, yet there is little formal literature about the accessibility of web development with a screen reader. This article describes research to explore (1) web development accessibility issues and their impact on blind learners and programmers; (2) tools and strategies used to address issues; and (3) opportunities for creating inclusive web development curriculum and supportive tools. We conducted a Comprehensive Literature Review (CLR) to formulate accessibility issue categories, then interviewed 12 blind programmers to validate and expand on both issues in education and practice. The CLR yielded five issue categories: (1) visual information without an accessible equivalent, (2) orienting, (3) navigating, (4) lack of support, and (5) knowledge and use of supportive technologies. Our interview findings validated the use of CLR-derived categories and revealed nuances specific to learning and practicing web development. Blind web developers grapple with the inaccessibility of demonstrations and explanations of web design concepts, wireframing software, independent verification of computed Cascading Style Sheets (CSS), and navigating browser-based developer tool interfaces. Tools and strategies include seeking out alternative education materials to learn independently, use of CSS frameworks, collaboration with sighted colleagues, and avoidance of design and front-end development. This work contributes to our understanding of accessibility issues specific to web development and the strategies that blind web developers employ in both educational and applied contexts. We identify areas in which greater awareness and application of accessibility best practices are required in Web education, a need to disseminate existing screen reader strategies and accessible tools, and to develop new tools that support Web design and validation of CSS. Finally, this research signals future directions for the development of accessible web curriculum and supportive tools, including solutions that leverage artificial intelligence, tactile graphics, and supportive-online communities of practice.},
journal = {ACM Trans. Access. Comput.},
month = {jul},
articleno = {8},
numpages = {32},
keywords = {accessible web development, Accessibility, screen reader, Human-centered computing, visually impaired/blind programmers, accessible design tools, accessible web design}
}

@article{achiam2023gpt,
  title={{GPT-4 Technical Report}},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@inproceedings{aghel2024people,
  title={{How People Prompt Generative AI to Create Interactive VR Scenes}},
  author={Aghel Manesh, Setareh and Zhang, Tianyi and Onishi, Yuki and Hara, Kotaro and Bateman, Scott and Li, Jiannan and Tang, Anthony},
  booktitle={Proceedings of the 2024 ACM Designing Interactive Systems Conference},
  pages={2319--2340},
  year={2024}
}

@inproceedings{altText2018,
author = {Morris, Meredith Ringel and Johnson, Jazette and Bennett, Cynthia L. and Cutrell, Edward},
title = {{Rich Representations of Visual Content for Screen Reader Users}},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3173574.3173633},
abstract = {Alt text (short for "alternative text") is descriptive text associated with an image in HTML and other document formats. Screen reader technologies speak the alt text aloud to people who are visually impaired. Introduced with HTML 2.0 in 1995, the alt attribute has not evolved despite significant changes in technology over the past two decades. In light of the expanding volume, purpose, and importance of digital imagery, we reflect on how alt text could be supplemented to offer a richer experience of visual content to screen reader users. Our contributions include articulating the design space of representations of visual content for screen reader users, prototypes illustrating several points within this design space, and evaluations of several of these new image representations with people who are blind. We close by discussing the implications of our taxonomy, prototypes, and user study findings.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–11},
numpages = {11},
keywords = {accessibility, visual impairment, blindness, alt text, captions, alternative text, screen readers},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@inproceedings{alttext2017,
author = {Wu, Shaomei and Wieland, Jeffrey and Farivar, Omid and Schiller, Julie},
title = {{Automatic Alt-Text: Computer-Generated Image Descriptions for Blind Users on a Social Network Service}},
year = {2017},
isbn = {9781450343350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/2998181.2998364},
abstract = {We designed and deployed automatic alt-text (AAT), a system that applies computer vision technology to identify faces, objects, and themes from photos to generate photo alt-text for screen reader users on Facebook. We designed our system through iterations of prototyping and in-lab user studies. Our lab test participants had a positive reaction to our system and an enhanced experience with Facebook photos. We also evaluated our system through a two-week field study as part of the Facebook iOS app for 9K VoiceOver users. We randomly assigned them into control and test groups and collected two weeks of activity data and their survey feedback. The test group reported that photos on Facebook were easier to interpret and more engaging, and found Facebook more useful in general. Our system demonstrates that artificial intelligence can be used to enhance the experience for visually impaired users on social networking sites (SNSs), while also revealing the challenges with designing automated assistive technology in a SNS context.},
booktitle = {Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing},
pages = {1180–1192},
numpages = {13},
keywords = {social networking sites, user experience, artificial intelligence, accessibility, facebook.},
location = {Portland, Oregon, USA},
series = {CSCW '17}
}

@inproceedings{bongini2023gpt,
  title={{Is GPT-3 All You Need for Visual Question Answering in Cultural Heritage?}},
  author={Bongini, Pietro and Becattini, Federico and Del Bimbo, Alberto},
  booktitle={Computer Vision--ECCV 2022 Workshops: Tel Aviv, Israel, October 23--27, 2022, Proceedings, Part I},
  pages={268--281},
  year={2023},
  organization={Springer}
}

@inproceedings{borodin2010more,
  title={{More than meets the eye: A survey of screen-reader browsing strategies}},
  author={Borodin, Yevgen and Bigham, Jeffrey P and Dausch, Glenn and Ramakrishnan, IV},
  booktitle={Proceedings of the 2010 International Cross Disciplinary Conference on Web Accessibility (W4A)},
  pages={1--10},
  year={2010}
}

@article{caldwell2008web,
  title={{Web content accessibility guidelines (WCAG) 2.0}},
  author={Caldwell, Ben and Cooper, Michael and Reid, Loretta Guarino and Vanderheiden, Gregg and Chisholm, Wendy and Slatin, John and White, Jason},
  journal={WWW Consortium (W3C)},
  volume={290},
  pages={1--34},
  year={2008}
}

@inproceedings{canetroller,
author = {Zhao, Yuhang and Bennett, Cynthia L. and Benko, Hrvoje and Cutrell, Edward and Holz, Christian and Morris, Meredith Ringel and Sinclair, Mike},
title = {{Enabling People with Visual Impairments to Navigate Virtual Reality with a Haptic and Auditory Cane Simulation}},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3173690},
doi = {10.1145/3173574.3173690},
abstract = {Traditional virtual reality (VR) mainly focuses on visual feedback, which is not accessible for people with visual impairments. We created Canetroller, a haptic cane controller that simulates white cane interactions, enabling people with visual impairments to navigate a virtual environment by transferring their cane skills into the virtual world. Canetroller provides three types of feedback: (1) physical resistance generated by a wearable programmable brake mechanism that physically impedes the controller when the virtual cane comes in contact with a virtual object; (2) vibrotactile feedback that simulates the vibrations when a cane hits an object or touches and drags across various surfaces; and (3) spatial 3D auditory feedback simulating the sound of real-world cane interactions. We designed indoor and outdoor VR scenes to evaluate the effectiveness of our controller. Our study showed that Canetroller was a promising tool that enabled visually impaired participants to navigate different virtual spaces. We discuss potential applications supported by Canetroller ranging from entertainment to mobility training.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–14},
numpages = {14},
keywords = {visual impairments, virtual reality, white cane, blindness, auditory feedback, mobility, haptic feedback},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@article{ciccone2023next,
  title={The next generation of virtual reality: recommendations for accessible and ergonomic design},
  author={Ciccone, Brendan A and Bailey, Shannon KT and Lewis, Joanna E},
  journal={Ergonomics in Design},
  volume={31},
  number={2},
  pages={24--27},
  year={2023},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@inproceedings{collins2023guide,
  title={{“The Guide Has Your Back”: Exploring How Sighted Guides Can Enhance Accessibility in Social Virtual Reality for Blind and Low Vision People}},
  author={Collins, Jazmin and Jung, Crescentia and Jang, Yeonju and Montour, Danielle and Won, Andrea Stevenson and Azenkot, Shiri},
  booktitle={Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility},
  pages={1--14},
  year={2023}
}

@article{creed2024inclusive,
  title={{Inclusive AR/VR: accessibility barriers for immersive technologies}},
  author={Creed, Chris and Al-Kalbani, Maadh and Theil, Arthur and Sarcar, Sayan and Williams, Ian},
  journal={Universal Access in the Information Society},
  volume={23},
  number={1},
  pages={59--73},
  year={2024},
  publisher={Springer}
}

@inproceedings{dang2023opportunities,
  title={{Opportunities for Accessible Virtual Reality Design for Immersive Musical Performances for Blind and Low-Vision People}},
  author={Dang, Khang and Korreshi, Hamdi and Iqbal, Yasir and Lee, Sooyeon},
  booktitle={Proceedings of the 2023 ACM Symposium on Spatial User Interaction},
  pages={1--21},
  year={2023}
}

@inproceedings{de2024llmr,
  title={{LLMR: Real-time prompting of interactive worlds using large language models}},
  author={De La Torre, Fernanda and Fang, Cathy Mengying and Huang, Han and Banburski-Fahey, Andrzej and Amores Fernandez, Judith and Lanier, Jaron},
  booktitle={Proceedings of the CHI Conference on Human Factors in Computing Systems},
  pages={1--22},
  year={2024}
}

@inproceedings{di2004usable,
  title={{“Usable Accessibility” to the Web for Blind Users}},
  author={Di Blas, Nicoletta and Paolini, Paolo and Speroni, Marco and others},
  booktitle={Proceedings of 8th ERCIM Workshop: User Interfaces for All, Vienna},
  year={2004}
}

@article{dudley2023inclusive,
  title={{Inclusive Immersion: a review of efforts to improve accessibility in virtual reality, augmented reality and the metaverse}},
  author={Dudley, John and Yin, Lulu and Garaj, Vanja and Kristensson, Per Ola},
  journal={Virtual Reality},
  volume={27},
  number={4},
  pages={2989--3020},
  year={2023},
  publisher={Springer}
}

@inproceedings{gonzalez2024investigating,
  title={{Investigating Use Cases of AI-Powered Scene Description Applications for Blind and Low Vision People}},
  author={Gonzalez Penuela, Ricardo E and Collins, Jazmin and Bennett, Cynthia and Azenkot, Shiri},
  booktitle={Proceedings of the CHI Conference on Human Factors in Computing Systems},
  pages={1--21},
  year={2024}
}

@inproceedings{ji2022vrbubble,
  title={{VRBubble: Enhancing peripheral awareness of avatars for people with visual impairments in social virtual reality}},
  author={Ji, Tiger F and Cochran, Brianna and Zhao, Yuhang},
  booktitle={Proceedings of the 24th International ACM SIGACCESS Conference on Computers and Accessibility},
  pages={1--17},
  year={2022}
}

@inproceedings{jiang2023beyond,
  title={{Beyond Audio Description: Exploring 360° Video Accessibility with Blind and Low Vision Users Through Collaborative Creation}},
  author={Jiang, Lucy and Phutane, Mahika and Azenkot, Shiri},
  booktitle={Proceedings of the 25th international ACM SIGACCESS conference on computers and accessibility},
  pages={1--17},
  year={2023}
}

@article{kim2020vivr,
  title={{VIVR: Presence of immersive interaction for visual impairment virtual reality}},
  author={Kim, Jinmo},
  journal={IEEE Access},
  volume={8},
  pages={196151--196159},
  year={2020},
  publisher={IEEE}
}

@article{luo2022vc,
  title={{VC-GPT: Visual Conditioned GPT for End-to-End Generative Vision-and-Language Pre-training}},
  author={Luo, Ziyang and Xi, Yadong and Zhang, Rongsheng and Ma, Jing},
  journal={arXiv preprint arXiv:2201.12723},
  year={2022}
}

@article{ma2024llms,
  title={{When LLMs step into the 3D World: A Survey and Meta-Analysis of 3D Tasks via Multi-modal Large Language Models}},
  author={Ma, Xianzheng and Bhalgat, Yash and Smart, Brandon and Chen, Shuai and Li, Xinghui and Ding, Jian and Gu, Jindong and Chen, Dave Zhenyu and Peng, Songyou and Bian, Jia-Wang and others},
  journal={arXiv preprint arXiv:2405.10255},
  year={2024}
}

@inproceedings{masnadi2020vriassist,
  title={{VRiAssist: An eye-tracked virtual reality low vision assistance tool}},
  author={Masnadi, Sina and Williamson, Brian and Gonz{\'a}lez, Andr{\'e}s N Vargas and LaViola, Joseph J},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)},
  pages={808--809},
  year={2020},
  organization={IEEE}
}

@misc{microsoft-seeing-ai,
  author = {Microsoft},
  title = {{Seeing AI}},
  howpublished = {\url{https://www.microsoft.com/en-us/ai/seeing-ai}},
  year = {2021},
  month = {September},
  day = {30}
}

@inproceedings{naikar2024accessibility,
  title={{Accessibility Feature Implementation Within Free VR Experiences}},
  author={Naikar, Vinaya Hanumant and Subramanian, Shwetha and Tigwell, Garreth W},
  booktitle={Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
  pages={1--9},
  year={2024}
}

@InProceedings{pmlr-v139-cho21a,
  title = 	 {{Unifying Vision-and-Language Tasks via Text Generation}},
  author =       {Cho, Jaemin and Lei, Jie and Tan, Hao and Bansal, Mohit},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {1931--1942},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/cho21a/cho21a.pdf},
  abstract = 	 {Existing methods for vision-and-language learning typically require designing task-specific architectures and objectives for each task. For example, a multi-label answer classifier for visual question answering, a region scorer for referring expression comprehension, and a language decoder for image captioning, etc. To alleviate these hassles, in this work, we propose a unified framework that learns different tasks in a single architecture with the same language modeling objective, i.e., multimodal conditional text generation, where our models learn to generate labels in text based on the visual and textual inputs. On 7 popular vision-and-language benchmarks, including visual question answering, referring expression comprehension, visual commonsense reasoning, most of which have been previously modeled as discriminative tasks, our generative approach (with a single unified architecture) reaches comparable performance to recent task-specific state-of-the-art vision-and-language models. Moreover, our generative approach shows better generalization ability on questions that have rare answers. Also, we show that our framework allows multi-task learning in a single architecture with a single set of parameters, achieving similar performance to separately optimized single-task models. Our code is publicly available at: https://github.com/j-min/VL-T5}
}

@inproceedings{salin2022vision,
  title={{Are Vision-Language Transformers Learning Multimodal Representations? A Probing Perspective}},
  author={Salin, Emmanuelle and Farah, Badreddine and Ayache, St{\'e}phane and Favre, Benoit},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  pages={11248--11257},
  year={2022}
}

@article{southwell2013evaluation,
  title={{An Evaluation of Finding Aid Accessibility for Screen Readers}},
  author={Southwell, Kristina L and Slater, Jacquelyn},
  journal={Information Technology and Libraries},
  volume={32},
  number={3},
  pages={34--46},
  year={2013}
}

@inproceedings{teofilo2018evaluating,
  title={Evaluating accessibility features designed for virtual reality context},
  author={Te{\'o}filo, Mauro and Lucena, Vicente F and Nascimento, Josiane and Miyagawa, Taynah and Maciel, Francimar},
  booktitle={2018 IEEE international conference on consumer electronics (ICCE)},
  pages={1--6},
  year={2018},
  organization={IEEE}
}

@article{williams2019find,
  title={{Find and Seek: Assessing the Impact of Table Navigation on Information Look-up with a Screen Reader}},
  author={Williams, Kristin and Clarke, Taylor and Gardiner, Steve and Zimmerman, John and Tomasic, Anthony},
  journal={ACM Transactions on Accessible Computing (TACCESS)},
  volume={12},
  pages={1--23},
  year={2019},
  publisher={ACM New York, NY, USA}
}

@article{wu2021towards,
  title={Towards accessible news reading design in virtual reality for low vision},
  author={Wu, Hui-Yin and Calabr{\`e}se, Aur{\'e}lie and Kornprobst, Pierre},
  journal={Multimedia Tools and Applications},
  volume={80},
  number={18},
  pages={27259--27278},
  year={2021},
  publisher={Springer}
}

@article{zhang2023gpt4mia,
  title={{GPT4MIA: Utilizing Geneative Pre-trained Transformer (GPT-3) as A Plug-and-Play Transductive Model for Medical Image Analysis}},
  author={Zhang, Yizhe and Chen, Danny Z},
  journal={arXiv preprint arXiv:2302.08722},
  year={2023}
}

@inproceedings{zhao2019seeingvr,
  title={{SeeingVR: A set of tools to make virtual reality more accessible to people with low vision}},
  author={Zhao, Yuhang and Cutrell, Edward and Holz, Christian and Morris, Meredith Ringel and Ofek, Eyal and Wilson, Andrew D},
  booktitle={Proceedings of the 2019 CHI conference on human factors in computing systems},
  pages={1--14},
  year={2019}
}

@inproceedings{zong2022rich,
  title={{Rich Screen Reader Experiences for Accessible Data Visualization}},
  author={Zong, Jonathan and Lee, Crystal and Lundgard, Alan and Jang, JiWoong and Hajas, Daniel and Satyanarayan, Arvind},
  booktitle={Computer Graphics Forum},
  volume={41},
  pages={15--27},
  year={2022},
  organization={Wiley Online Library}
}

