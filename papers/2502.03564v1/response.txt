\section{Related Work}
\subsection{Visual Accessibility Design in VR}

% To date, there is a high percentage of VR experiences that lack accessibility features
In a study conducted by Naikar et al., "Designing Accessible Virtual Reality Experiences", 39 out of 106 inspected free VR experiences (36.8\%) lacked accessibility features. Furthermore, users may encounter multiple accessibility barriers in the same context. Extensive work has focused on advancing visual accessibility %design 
in VR to provide a more inclusive experience. Mostly, this has been approached through augmenting visual information or translating it into audio or haptic feedback . 
% identify the lack of accessibility features in VR experiences, Creed et al.  "Exploring Accessibility Barriers for Immersive Technologies" provide a summary of accessibility barriers for immersive technologies, while 
% Dudley et al. "Visual Accessibility Designs in Virtual Reality: A Review" present a comprehensive review of visual accessibility designs in VR. 
%Work to date on supporting visual accessibility in VR has primarily focused on either 
%the augmentation of visual information or the conversion from visual information to audio or haptic information. 

%The work of Te\'ofilo et al., "Gear VRF Accessibility: A Framework for Adapting Visual Features in Virtual Reality" is an example of augmenting visual information. They developed and evaluated a system called Gear VRF Accessibility which provides a framework for developers to adapt zoom, invert colors, and add captions in a VR environment. Similarly, SeeingVR, "A Set of 14 Tools for Enhancing Visual Accessibility in Virtual Reality" is a set of 14 tools including magnification lens, brightness lens, edge enhancement, text augmentation, and other visual augmentation tools. User studies conducted with SeeingVR demonstrated that the tools were effective in helping people with low vision to complete tasks such as menu navigation, visual search, and target shooting in VR. VRiAssist, "Visual Assistance System for Virtual Reality: A Pilot Study" provides visual assistance in VR based on eye tracking. It includes a distortion correction tool, a color/brightness correction tool, and an adjustable magnification tool. Wu et al., "Adaptive Visual Accessibility Features for Reading in Virtual Reality" provided adjustable print and text layout, smart text contrasting, and image enhancement to provide visual accessibility features for reading in VR. Ciccone et al., "Designing for Visual Accessibility in Virtual Reality: A Case Study" recommended implementing contrast adjustment controls, color correction controls, and font and display size adjustments to increase information visibility for visual accessibility design. 

Outstanding work in the area of augmenting visual information includes the development of tools for magnification, contrast adjustment, color correction, text and display size adjustment, among others. Gear VRF Accessibility, "A Framework for Adapting Visual Features in Virtual Reality" , for instance, provided a framework for developers to adapt zoom, invert colors, and add captions in a VR environment. VRiAssist, "Visual Assistance System for Virtual Reality: A Pilot Study" supported the user by offering visual assistance based on eye tracking, providing tools like magnification, distortion, colour and brightness correction. SeeingVR, "A Set of 14 Tools for Enhancing Visual Accessibility in Virtual Reality" involved a larger set of visual augmentation tools that proved effective for task completion in VR (such as menu navigation, visual search, and target shooting).
%Wu et al., "Adaptive Visual Accessibility Features for Reading in Virtual Reality" provided adjustable print and text layout, smart text contrasting, and image enhancement to provide visual accessibility features for reading in VR. 
Consistent with these approaches, Ciccone et al., "Designing for Visual Accessibility in Virtual Reality: A Case Study" recommended implementing contrast adjustment controls, color correction controls, and font and display size adjustments to increase information visibility when designing for visual accessibility.

% The second approach of converting visual information to other forms of information is exemplified by the work of Zhao et al. and their system Canetroller . Canetroller provides 3D spatial audio feedback to simulate interactions between the cane and the virtual world, and also uses physical resistance and vibrotactile feedback to simulate the cane touching and hitting an object. VIVR , "A Virtual White Cane for the Visually Impaired" is a similar effort to simulate the interaction of a white cane with virtual braille blocks on a VR sidewalk. In a survey by Ma et al., "Applications of Large Language Models in 3D World: A Survey" . Jiang et al., "Enhancing Audio Descriptions using Advanced AI Models" highlighted the potential of advanced AI models to enhance the quantity and quality of audio descriptions.

% In the work of De La Torre et al., "Large Language Model for Mixed Reality Framework" state-of-the-art large language models (LLMs) have also been applied to design \textsc{LLMR}, an LLM for Mixed Reality framework which analyzes and generates 3D scenes to support a wide variety of interaction tasks such as 3D world creation, multi-modal interaction, 3D scene editing, 3D scene information query, and external plugin and cross-platform integration. Microsoft developed SeeingAI, "Seeing AI: A Virtual Assistant for the Visually Impaired" to narrate the physical world for BLV users.
These models are now being deployed in a range of use cases to power visual accessibility features. 
For example, De La Torre et al., "Large Language Model for Mixed Reality Framework" demonstrated potential applications of their Large Language Model (LLM)-based tool for 3D scene editing in visual accessibility. Jiang et al., "Enhancing Audio Descriptions using Advanced AI Models" highlighted the potential of advanced AI models to enhance the quantity and quality of audio descriptions.
Microsoft developed SeeingAI, "Seeing AI: A Virtual Assistant for the Visually Impaired" to narrate the physical world for BLV users. Similarly, Be My Eyes launched Be My AI, "Be My AI: An AI Assistant for the Visually Impaired" , an AI assistant powered by GPT-4, which provides detailed descriptions of photos taken and uploaded by BLV users, and a braille display for deaf-blind users. %This system provides visual assistance for everyday tasks such as navigating physical environments. 

The increasing attention to applying VLMs to interactions in 3D content and accessibility design illustrates the strong capability of such models. While existing work has demonstrated how state-of-the-art models could be applied %in a variety of tasks for 3D scenes, or how the models could be adopted 
in accessibility design for 2D images or videos, there has been limited work studying how these models could be applied in accessibility design for VR immersive environments.