\section{Related Work}
\subsection{Visual Accessibility Design in VR}

% To date, there is a high percentage of VR experiences that lack accessibility features
In a study conducted by Naikar et al.~\cite{naikar2024accessibility}, 39 out of 106 inspected free VR experiences (36.8\%) lacked accessibility features. Furthermore, users may encounter multiple accessibility barriers in the same context~\cite{creed2024inclusive}. Extensive work has focused on advancing visual accessibility %design 
in VR to provide a more inclusive experience~\cite{dudley2023inclusive}. Mostly, this has been approached through augmenting visual information \cite{zhao2019seeingvr, masnadi2020vriassist, teofilo2018evaluating} or translating it into audio or haptic feedback \cite{canetroller, kim2020vivr, zhao2019seeingvr, ji2022vrbubble}. 
% identify the lack of accessibility features in VR experiences, Creed et al. \cite{creed2024inclusive} provide a summary of accessibility barriers for immersive technologies, while 
% Dudley et al. \cite{dudley2023inclusive} present a comprehensive review of visual accessibility designs in VR. 
%Work to date on supporting visual accessibility in VR has primarily focused on either 
%the augmentation of visual information or the conversion from visual information to audio or haptic information. 

%The work of Te\'ofilo et al. \cite{teofilo2018evaluating} is an example of augmenting visual information. They developed and evaluated a system called Gear VRF Accessibility which provides a framework for developers to adapt zoom, invert colors, and add captions in a VR environment. Similarly, SeeingVR~\cite{zhao2019seeingvr} is a set of 14 tools including magnification lens, brightness lens, edge enhancement, text augmentation, and other visual augmentation tools. User studies conducted with SeeingVR demonstrated that the tools were effective in helping people with low vision to complete tasks such as menu navigation, visual search, and target shooting in VR. VRiAssist~\cite{masnadi2020vriassist} provides visual assistance in VR based on eye tracking. It includes a distortion correction tool, a color/brightness correction tool, and an adjustable magnification tool. Wu et al.~\cite{wu2021towards} provided adjustable print and text layout, smart text contrasting, and image enhancement to provide visual accessibility features for reading in VR. Ciccone et al.~\cite{ciccone2023next} recommended implementing contrast adjustment controls, color correction controls, and font and display size adjustments to increase information visibility for visual accessibility design. 

Outstanding work in the area of augmenting visual information includes the development of tools for magnification, contrast adjustment, color correction, text and display size adjustment, among others. Gear VRF Accessibility~\cite{teofilo2018evaluating}, for instance, provided a framework for developers to adapt zoom, invert colors, and add captions in a VR environment. VRiAssist~\cite{masnadi2020vriassist} supported the user by offering visual assistance based on eye tracking, providing tools like magnification, distortion, colour and brightness correction. SeeingVR~\cite{zhao2019seeingvr} involved a larger set of visual augmentation tools that proved effective for task completion in VR (such as menu navigation, visual search, and target shooting).
%Wu et al.~\cite{wu2021towards} provided adjustable print and text layout, smart text contrasting, and image enhancement to provide visual accessibility features for reading in VR. 
Consistent with these approaches, Ciccone et al.~\cite{ciccone2023next} recommended implementing contrast adjustment controls, color correction controls, and font and display size adjustments to increase information visibility when designing for visual accessibility.

% The second approach of converting visual information to other forms of information is exemplified by the work of Zhao et al. and their system Canetroller \cite{canetroller}. Canetroller provides 3D spatial audio feedback to simulate interactions between the cane and the virtual world, and also uses physical resistance and vibrotactile feedback to simulate the cane touching and hitting an object. VIVR~\cite{kim2020vivr} is a similar effort to simulate the interaction of a white cane with virtual braille blocks on a VR sidewalk based on audio feedback and controller vibrations. In SeeingVR \cite{zhao2019seeingvr}, there are also tools such as text-to-speech and object recognition to convert visual information to speech. In VRBubble~\cite{ji2022vrbubble}, audio alternatives such as earcons, verbal notifications, and real-world sound effects enhance the peripheral awareness of users with vision impairment to facilitate social VR accessibility. Dang et al. \cite{dang2023opportunities} outlined opportunities for a multimodal-multisensor VR system to use spatial audio, audio descriptions, audio feedback, and vibrotactile feedback to enhance the experience of BLV people in immersive musical performances. Other works on social VR applications explored how sighted guides can support BLV user navigation and visual interpretation \cite{collins2023guide}, which also fall into the category of converting visual information to other forms of information for visual accessibility design.

Research focused on converting visual information into other forms has also resulted in a variety of systems supporting visual accessibility in VR. For instance, both Canetroller~\cite{canetroller} and VIVR~\cite{kim2020vivr} simulated the use of a white cane in the virtual world. This included providing 3D spatial audio feedback, physical resistance, and vibrotactile feedback to simulate cane--virtual object interaction. The aforementioned SeeingVR~\cite{zhao2019seeingvr} also included text-to-speech and object recognition from visual information to speech. In a more specialized context, Dang et al.~\cite{dang2023opportunities} outlined a multimodal-multisensor VR system with spatial audio, audio descriptions, audio feedback, and vibrotactile feedback to enhance the experience of BLV participants in immersive musical performances. 
Finally, VRBubble~\cite{ji2022vrbubble} enhanced BLV users' peripheral awareness to facilitate social VR accessibility through audio alternatives such as earcons, verbal notifications, and real-world sound.

%Other works on social VR applications explored how sighted guides can support BLV user navigation and visual interpretation \cite{collins2023guide}, which also fall into the category of converting visual information to other forms of information for visual accessibility design.

% The above two approaches to augment visual information directly or convert visual information to other forms have been widely adopted in existing literature.  %, as well as designs for other forms of impairment and other immersive environments, and refers this objective of maximizing the accessibility and enjoyment of users in immersive environments as \textit{Inclusive Immersion}. 
% Among the two approaches to providing visual accessibility design, the first approach of augmenting visual features does %intuitive and easy to implement, but is 
% not support users who are blind or have very limited visual perception, while the second approach of information conversion supports them. In this paper, we focus on the second approach and the much less explored methods of converting visual information to speech, audio cues and haptics that this approach entails. We explore how VLMs could be incorporated to provide vivid scene descriptions. By combining these multiple modalities, we aim to provide users with both a high-level understanding of their surroundings as well as a detailed understanding of object-level information to support interaction.

Among both approaches, augmenting visual information cannot support users who are blind or with very limited visual perception. Thus, the work in this paper focuses on integrating the relatively underexplored methods of converting visual information into speech, audio cues, and haptics. We investigate how VLMs could be incorporated to provide vivid scene descriptions. By combining these multiple modalities, we aim to provide users with a high-level understanding of their surroundings, as well as a detailed understanding of object-level information to support interaction.

% People have used multimodal interaction techniques for visual accessibility design in immersive environments

\subsection{Screen Readers and Web Accessibility}

Screen readers are a well-established accessibility tool for BLV users; %have been proven to be a success for web accessibility on 2D screens. 
their design concepts can provide valuable insights for the design of visual accessibility in immersive environments.
NVDA, JAWS, and VoiceOver are three of the most commonly used screen readers for desktops and laptops~\cite{WebAim-10}. While these different screen readers have distinct characteristics, they share key design principles which underpin their effectiveness. First, popular screen readers prioritize keyboard navigation. 
%According to Kearney-Volpe and Hurst~\cite{accessibleWebDev2021}, 
Keyboard navigation allows users to navigate digital content without the need for a mouse, which is critical for people with vision impairment~\cite{accessibleWebDev2021}. 
Second, screen readers focus on the semantic structure to facilitate smooth navigation and ensure information accuracy. On this topic, a series of works~\cite{zong2022rich, di2004usable, williams2019find} have specifically focused on how to improve the usability of screen readers by correctly and efficiently conveying semantic details. 
Third, screen readers also provide alternative text for images, which is a crucial step to help convey non-textual content~\cite{alttext2017, altText2018}. Fourth, screen readers use headings and landmarks to assist website navigation and hierarchy~\cite{southwell2013evaluation}. Finally, screen readers also assist user input, such as filling in and submitting forms and documents online, an important part of web interaction~\cite{borodin2010more}.

% As an example of web accessibility design, screen readers also follow the web content accessibility guidelines (WCAG) \cite{caldwell2008web}, a framework to ensure the inclusivity of digital content for people with disabilities. The four core principles of WCAG are known as POUR, which stands for:
% \begin{itemize}
%     \item \textbf{Perceivable:} Users must be able to perceive the information presented.
%     \item \textbf{Operable:} Users must be able to operate the interface.
%     \item \textbf{Understandable:} Users must be able to understand the information and operation of the interface.
%     \item \textbf{Robust:} Users must be able to access the content as technologies advance.
% \end{itemize}

% While the four accessibility principles above are proposed for web accessibility, these principles together with the five design principles above for screen readers provide valuable insights for accessibility design in VR. 
\textsc{EnVisionVR} takes inspiration and expands on the design principles and concepts of screen readers. %of screen readers.
% Keyboard button-based navigation informed our design of using the single and double-press of a single controller button to activate or deactivate \textsc{EnVisionVR}. The semantic structure in conventional screen readers inspired us to decompose 3D information into high-level scene information and detailed-level object information. Alternative text for images inspired us to generate scene descriptions at certain important locations in the scene prior to deploying the scene. Finally, a set of three simple speech commands to activate the functions serve as the ``headings'' and ``landmarks'' to assist with user navigation and understanding.
Based on the above, we arrive at an interactive design that uses speech commands as a parallel to keyboard navigation, while constructing high-level scene information and detailed object-level information for BLV users as a parallel to the semantic structure processed by screen readers. Furthermore, %Similarly, 
VLMs provide a highly efficient way to produce scene descriptions, %which serve as a parallel to explicitly entered alternative text.
a parallel to explicit alternative text.
% and pre-defined anchor points in 3D scenes serve a similar function with website headings and landmarks. Through an empirical user study, we demonstrate that the above features of \textsc{EnVisionVR} satisfy the four core principles of WCAG.

\subsection{Powering Visual Accessibility with Artificial Intelligence}

The emergence of powerful VLMs has enabled the automated generation of high-quality descriptions of visual information.  %With advanced AI models, BLV individuals will be able to access 2D and 3D visual content via vivid and detailed natural language descriptions easily.
Current VLMs~\cite{salin2022vision,pmlr-v139-cho21a,luo2022vc,bongini2023gpt,zhang2023gpt4mia} are capable of jointly processing images and text data for image captioning, visual question answering, and medical image analysis. %According to the technical report by OpenAI \cite{achiam2023gpt} and the official webpage \cite{OpenAI_2024}, more recent models such as GPT-4 and GPT-4 Omni (GPT-4o) are capable of processing image and text inputs and produce text outputs, achieving high-level performance on text, reasoning, and coding intelligence, with high multilingual, audio, and vision capabilities. 
% In the work of De La Torre et al. \cite{de2024llmr}, state-of-the-art large language models (LLMs) have also been applied to design \textsc{LLMR}, an LLM for Mixed Reality framework which analyzes and generates 3D scenes to support a wide variety of interaction tasks such as 3D world creation, multi-modal interaction, 3D scene editing, 3D scene information query, and external plugin and cross-platform integration. In a survey by Ma et al. \cite{ma2024llms} on the applications of LLMs in the 3D world, the authors summarize how recent works have leveraged the strong learning and reasoning capabilities as well as extensive world knowledge of LLMs to process, understand, and generate 3D data. In terms of user behavior and interaction design, Aghel Manesh et al. \cite{aghel2024people} also studied how users adopt generative AI to prompt the system to create VR scenes through an elicitation study with the Wizard-of-Oz technique.
These models are now being deployed in a range of use cases to power visual accessibility features. 
For example, De La Torre et al.~\cite{de2024llmr} demonstrated potential applications of their Large Language Model (LLM)-based tool for 3D scene editing in visual accessibility. Jiang et al.~\cite{jiang2023beyond} highlighted the potential of advanced AI models to enhance the quantity and quality of audio descriptions.
Microsoft developed SeeingAI~\cite{microsoft-seeing-ai} to narrate the physical world for BLV users.
Similarly, Be My Eyes launched Be My AI~\cite{BeMyAI}, an AI assistant powered by GPT-4, which provides detailed descriptions of photos taken and uploaded by BLV users, and a braille display for deaf-blind users. %This system provides visual assistance for everyday tasks such as navigating physical environments. 
Specific use cases for scene description in real-life scenarios have been identified through a diary study~\cite{gonzalez2024investigating}, which highlights the effectiveness of generative models for visual accessibility design.

The increasing attention to applying VLMs to interactions in 3D content and accessibility design illustrates the strong capability of such models. While existing work has demonstrated how state-of-the-art models could be applied %in a variety of tasks for 3D scenes, or how the models could be adopted 
in accessibility design for 2D images or videos, there has been limited work studying how these models could be applied in accessibility design for VR immersive environments. %Existing works differ from how we present opportunities for VLMs to generate vivid scene descriptions and incorporate VLMs with other multimodal interaction techniques in \textsc{EnVisionVR} to deliver high-level scene information and detailed object-level information for visual accessibility design in immersive environments.
% . which highlights the importance of leveraging advanced AI models to develop the visual accessibility tool \textsc{EnVisionVR} for immersive environments. , but it differs from how we present opportunities for VLMs to generate vivid scene descriptions.
% processing image and text inputs and producing text outputs with high 

%