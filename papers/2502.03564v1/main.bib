@misc{rnib2021,
    author = "{The Royal National Institute of Blind People (RNIB)}",
    title = "Key statistics about sight loss",
    year = "2021",
    howpublished = "\url{https://media.rnib.org.uk/documents/Key_stats_about_sight_loss_2021.pdf}",
    note = "[Online; accessed 13-August-2023]"
  }

@misc{who2023,
    author = "{World Health Organization}",
    title = "Blindness and vision impairment",
    year = "2023",
    howpublished = "\url{https://www.who.int/news-room/fact-sheets/detail/blindness-and-visual-impairment}",
    note = "[Online; accessed 13-August-2023]"
  }

@inproceedings{herskovitz2020making,
  title={{Making Mobile Augmented Reality Applications Accessible}},
  author={Herskovitz, Jaylin and Wu, Jason and White, Samuel and Pavel, Amy and Reyes, Gabriel and Guo, Anhong and Bigham, Jeffrey P},
  booktitle={Proceedings of the 22nd International ACM SIGACCESS Conference on Computers and Accessibility},
  pages={1--14},
  year={2020}
}

@inproceedings{zhao2019seeingvr,
  title={{SeeingVR: A set of tools to make virtual reality more accessible to people with low vision}},
  author={Zhao, Yuhang and Cutrell, Edward and Holz, Christian and Morris, Meredith Ringel and Ofek, Eyal and Wilson, Andrew D},
  booktitle={Proceedings of the 2019 CHI conference on human factors in computing systems},
  pages={1--14},
  year={2019}
}

@inproceedings{canetroller,
author = {Zhao, Yuhang and Bennett, Cynthia L. and Benko, Hrvoje and Cutrell, Edward and Holz, Christian and Morris, Meredith Ringel and Sinclair, Mike},
title = {{Enabling People with Visual Impairments to Navigate Virtual Reality with a Haptic and Auditory Cane Simulation}},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3173690},
doi = {10.1145/3173574.3173690},
abstract = {Traditional virtual reality (VR) mainly focuses on visual feedback, which is not accessible for people with visual impairments. We created Canetroller, a haptic cane controller that simulates white cane interactions, enabling people with visual impairments to navigate a virtual environment by transferring their cane skills into the virtual world. Canetroller provides three types of feedback: (1) physical resistance generated by a wearable programmable brake mechanism that physically impedes the controller when the virtual cane comes in contact with a virtual object; (2) vibrotactile feedback that simulates the vibrations when a cane hits an object or touches and drags across various surfaces; and (3) spatial 3D auditory feedback simulating the sound of real-world cane interactions. We designed indoor and outdoor VR scenes to evaluate the effectiveness of our controller. Our study showed that Canetroller was a promising tool that enabled visually impaired participants to navigate different virtual spaces. We discuss potential applications supported by Canetroller ranging from entertainment to mobility training.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–14},
numpages = {14},
keywords = {visual impairments, virtual reality, white cane, blindness, auditory feedback, mobility, haptic feedback},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@misc{OpenInclusion, title={{Home - Open Inclusion}}, journal={Open Inclusion - Developing a More Open Digital World}, author={Open Inclusion}, note={Available at: https://openinclusion.com/. Accessed on Jan. 19th, 2025.}} 

@inproceedings{teofilo2018evaluating,
  title={Evaluating accessibility features designed for virtual reality context},
  author={Te{\'o}filo, Mauro and Lucena, Vicente F and Nascimento, Josiane and Miyagawa, Taynah and Maciel, Francimar},
  booktitle={2018 IEEE international conference on consumer electronics (ICCE)},
  pages={1--6},
  year={2018},
  organization={IEEE}
}

@inproceedings{masnadi2020vriassist,
  title={{VRiAssist: An eye-tracked virtual reality low vision assistance tool}},
  author={Masnadi, Sina and Williamson, Brian and Gonz{\'a}lez, Andr{\'e}s N Vargas and LaViola, Joseph J},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)},
  pages={808--809},
  year={2020},
  organization={IEEE}
}

@article{wu2021towards,
  title={Towards accessible news reading design in virtual reality for low vision},
  author={Wu, Hui-Yin and Calabr{\`e}se, Aur{\'e}lie and Kornprobst, Pierre},
  journal={Multimedia Tools and Applications},
  volume={80},
  number={18},
  pages={27259--27278},
  year={2021},
  publisher={Springer}
}

@article{ciccone2023next,
  title={The next generation of virtual reality: recommendations for accessible and ergonomic design},
  author={Ciccone, Brendan A and Bailey, Shannon KT and Lewis, Joanna E},
  journal={Ergonomics in Design},
  volume={31},
  number={2},
  pages={24--27},
  year={2023},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@article{kim2020vivr,
  title={{VIVR: Presence of immersive interaction for visual impairment virtual reality}},
  author={Kim, Jinmo},
  journal={IEEE Access},
  volume={8},
  pages={196151--196159},
  year={2020},
  publisher={IEEE}
}

@inproceedings{ji2022vrbubble,
  title={{VRBubble: Enhancing peripheral awareness of avatars for people with visual impairments in social virtual reality}},
  author={Ji, Tiger F and Cochran, Brianna and Zhao, Yuhang},
  booktitle={Proceedings of the 24th International ACM SIGACCESS Conference on Computers and Accessibility},
  pages={1--17},
  year={2022}
}

@article{dudley2023inclusive,
  title={{Inclusive Immersion: a review of efforts to improve accessibility in virtual reality, augmented reality and the metaverse}},
  author={Dudley, John and Yin, Lulu and Garaj, Vanja and Kristensson, Per Ola},
  journal={Virtual Reality},
  volume={27},
  number={4},
  pages={2989--3020},
  year={2023},
  publisher={Springer}
}

@article{zallio2022designing,
  title={{Designing the metaverse: A study on inclusion, diversity, equity, accessibility and safety for digital immersive environments}},
  author={Zallio, Matteo and Clarkson, P John},
  journal={Telematics and Informatics},
  volume={75},
  pages={101909},
  year={2022},
  publisher={Elsevier}
}

@inproceedings{dang2023opportunities,
  title={{Opportunities for Accessible Virtual Reality Design for Immersive Musical Performances for Blind and Low-Vision People}},
  author={Dang, Khang and Korreshi, Hamdi and Iqbal, Yasir and Lee, Sooyeon},
  booktitle={Proceedings of the 2023 ACM Symposium on Spatial User Interaction},
  pages={1--21},
  year={2023}
}

@inproceedings{jiang2023beyond,
  title={{Beyond Audio Description: Exploring 360° Video Accessibility with Blind and Low Vision Users Through Collaborative Creation}},
  author={Jiang, Lucy and Phutane, Mahika and Azenkot, Shiri},
  booktitle={Proceedings of the 25th international ACM SIGACCESS conference on computers and accessibility},
  pages={1--17},
  year={2023}
}

@inproceedings{gonzalez2024investigating,
  title={{Investigating Use Cases of AI-Powered Scene Description Applications for Blind and Low Vision People}},
  author={Gonzalez Penuela, Ricardo E and Collins, Jazmin and Bennett, Cynthia and Azenkot, Shiri},
  booktitle={Proceedings of the CHI Conference on Human Factors in Computing Systems},
  pages={1--21},
  year={2024}
}

@article{creed2024inclusive,
  title={{Inclusive AR/VR: accessibility barriers for immersive technologies}},
  author={Creed, Chris and Al-Kalbani, Maadh and Theil, Arthur and Sarcar, Sayan and Williams, Ian},
  journal={Universal Access in the Information Society},
  volume={23},
  number={1},
  pages={59--73},
  year={2024},
  publisher={Springer}
}

@misc{vr-escape-room,
  author = {Unity},
  title = {{VR Beginner: The Escape Room}},
  howpublished = {\url{https://assetstore.unity.com/packages/templates/tutorials/vr-beginner-the-escape-room-163264}},
  year = {2021},
  month = {October},
  day = {28}
}

@inproceedings{mott2019accessible,
  title={{Accessible by design: An opportunity for virtual reality}},
  author={Mott, Martez and Cutrell, Edward and Franco, Mar Gonzalez and Holz, Christian and Ofek, Eyal and Stoakley, Richard and Morris, Meredith Ringel},
  booktitle={2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
  pages={451--454},
  year={2019},
  organization={IEEE}
}

@inproceedings{collins2023guide,
  title={{“The Guide Has Your Back”: Exploring How Sighted Guides Can Enhance Accessibility in Social Virtual Reality for Blind and Low Vision People}},
  author={Collins, Jazmin and Jung, Crescentia and Jang, Yeonju and Montour, Danielle and Won, Andrea Stevenson and Azenkot, Shiri},
  booktitle={Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility},
  pages={1--14},
  year={2023}
}

@misc{WebAim-10,
    author = "{WebAim}",
    title = "{{Screen Reader User Survey \#10 Results}}",
    year = "2024",
    howpublished = "\url{https://webaim.org/projects/screenreadersurvey10/}",
    note = "[Online; accessed 13-August-2024]"
  }

@article{accessibleWebDev2021,
author = {Kearney-Volpe, Claire and Hurst, Amy},
title = {{Accessible Web Development: Opportunities to Improve the Education and Practice of Web Development with a Screen Reader}},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {1936-7228},
doi = {10.1145/3458024},
abstract = {There are a growing number of jobs related to web development, yet there is little formal literature about the accessibility of web development with a screen reader. This article describes research to explore (1) web development accessibility issues and their impact on blind learners and programmers; (2) tools and strategies used to address issues; and (3) opportunities for creating inclusive web development curriculum and supportive tools. We conducted a Comprehensive Literature Review (CLR) to formulate accessibility issue categories, then interviewed 12 blind programmers to validate and expand on both issues in education and practice. The CLR yielded five issue categories: (1) visual information without an accessible equivalent, (2) orienting, (3) navigating, (4) lack of support, and (5) knowledge and use of supportive technologies. Our interview findings validated the use of CLR-derived categories and revealed nuances specific to learning and practicing web development. Blind web developers grapple with the inaccessibility of demonstrations and explanations of web design concepts, wireframing software, independent verification of computed Cascading Style Sheets (CSS), and navigating browser-based developer tool interfaces. Tools and strategies include seeking out alternative education materials to learn independently, use of CSS frameworks, collaboration with sighted colleagues, and avoidance of design and front-end development. This work contributes to our understanding of accessibility issues specific to web development and the strategies that blind web developers employ in both educational and applied contexts. We identify areas in which greater awareness and application of accessibility best practices are required in Web education, a need to disseminate existing screen reader strategies and accessible tools, and to develop new tools that support Web design and validation of CSS. Finally, this research signals future directions for the development of accessible web curriculum and supportive tools, including solutions that leverage artificial intelligence, tactile graphics, and supportive-online communities of practice.},
journal = {ACM Trans. Access. Comput.},
month = {jul},
articleno = {8},
numpages = {32},
keywords = {accessible web development, Accessibility, screen reader, Human-centered computing, visually impaired/blind programmers, accessible design tools, accessible web design}
}

@misc{feng2014system,
  title={{System and method of identifying web page semantic structures}},
  author={Feng, Junlan and Hollister, Barbara B},
  year={2014},
  month=sep # "~2",
  publisher={Google Patents},
  note={US Patent 8,825,628}
}

@inproceedings{semantic2021,
author = {Ferdous, Javedul and Uddin, Sami and Ashok, Vikas},
title = {{Semantic Table-of-Contents for Efficient Web Screen Reading}},
year = {2021},
isbn = {9781450381048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3412841.3442066},
abstract = {Navigating back-and-forth between segments in webpages is well-known to be an arduous endeavor for blind screen-reader users, due to the serial nature of content navigation coupled with the inconsistent usage of accessibility enhancing features such as WAI-ARIA landmarks and skip navigation links by web developers. Without these supporting features, navigating modern webpages that typically contain thousands of HTML elements in their DOMs, is both tedious and cumbersome for blind screen-reader users. Existing approaches to improve non-visual navigation efficiency typically propose 'one-size-fits-all' solutions that do not accommodate the personal needs and preferences of screen-reader users. To fill this void, in this paper, we present sTag, a browser extension embodying a semi-automatic method that enables users to easily create their own Table Of Contents (TOC) for any webpage by simply 'tagging' their preferred 'semantically-meaningful' segments (e.g., search results, filter options, forms, menus, etc.) while navigating the webpage. This way, all subsequent accesses to these segments can be made via the generated TOC that is made instantly accessible via a special shortcut or a repurposed mouse/touchpad action. As tags in sTag are attached to the abstract semantic segments instead of actual DOM nodes in the webpage, sTag can automatically generate equivalent TOCs for other similar webpages, without requiring the users to duplicate their tagging efforts from scratch in these webpages. An evaluation with 15 blind screen-reader users revealed that sTag significantly reduced the content-navigation time and effort compared to those with a state-of-the-art solution.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
pages = {1941–1949},
numpages = {9},
keywords = {web accessibility, screen reader, personalization},
location = {Virtual Event, Republic of Korea},
series = {SAC '21}
}

@inproceedings{SaIL2020,
author = {Aydin, Ali Selman and Feiz, Shirin and Ashok, Vikas and Ramakrishnan, IV},
title = {{SaIL: Saliency-Driven Injection of ARIA Landmarks}},
year = {2020},
isbn = {9781450371186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3377325.3377540},
abstract = {Navigating webpages with screen readers is a challenge even with recent improvements in screen reader technologies and the increased adoption of web standards for accessibility, namely ARIA. ARIA landmarks, an important aspect of ARIA, lets screen reader users access different sections of the webpage quickly, by enabling them to skip over blocks of irrelevant or redundant content. However, these landmarks are sporadically and inconsistently used by web developers, and in many cases, even absent in numerous web pages. Therefore, we propose SaIL, a scalable approach that automatically detects the important sections of a web page, and then injects ARIA landmarks into the corresponding HTML markup to facilitate quick access to these sections. The central concept underlying SaIL is visual saliency, which is determined using a state-of-the-art deep learning model that was trained on gaze-tracking data collected from sighted users in the context of web browsing. We present the findings of a pilot study that demonstrated the potential of SaIL in reducing both the time and effort spent in navigating webpages with screen readers.},
booktitle = {Proceedings of the 25th International Conference on Intelligent User Interfaces},
pages = {111–115},
numpages = {5},
keywords = {landmarks, screen reader, WAI-ARIA, web accessibility},
location = {Cagliari, Italy},
series = {IUI '20}
}


@inproceedings{alttext2017,
author = {Wu, Shaomei and Wieland, Jeffrey and Farivar, Omid and Schiller, Julie},
title = {{Automatic Alt-Text: Computer-Generated Image Descriptions for Blind Users on a Social Network Service}},
year = {2017},
isbn = {9781450343350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/2998181.2998364},
abstract = {We designed and deployed automatic alt-text (AAT), a system that applies computer vision technology to identify faces, objects, and themes from photos to generate photo alt-text for screen reader users on Facebook. We designed our system through iterations of prototyping and in-lab user studies. Our lab test participants had a positive reaction to our system and an enhanced experience with Facebook photos. We also evaluated our system through a two-week field study as part of the Facebook iOS app for 9K VoiceOver users. We randomly assigned them into control and test groups and collected two weeks of activity data and their survey feedback. The test group reported that photos on Facebook were easier to interpret and more engaging, and found Facebook more useful in general. Our system demonstrates that artificial intelligence can be used to enhance the experience for visually impaired users on social networking sites (SNSs), while also revealing the challenges with designing automated assistive technology in a SNS context.},
booktitle = {Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing},
pages = {1180–1192},
numpages = {13},
keywords = {social networking sites, user experience, artificial intelligence, accessibility, facebook.},
location = {Portland, Oregon, USA},
series = {CSCW '17}
}

@inproceedings{altText2018,
author = {Morris, Meredith Ringel and Johnson, Jazette and Bennett, Cynthia L. and Cutrell, Edward},
title = {{Rich Representations of Visual Content for Screen Reader Users}},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3173574.3173633},
abstract = {Alt text (short for "alternative text") is descriptive text associated with an image in HTML and other document formats. Screen reader technologies speak the alt text aloud to people who are visually impaired. Introduced with HTML 2.0 in 1995, the alt attribute has not evolved despite significant changes in technology over the past two decades. In light of the expanding volume, purpose, and importance of digital imagery, we reflect on how alt text could be supplemented to offer a richer experience of visual content to screen reader users. Our contributions include articulating the design space of representations of visual content for screen reader users, prototypes illustrating several points within this design space, and evaluations of several of these new image representations with people who are blind. We close by discussing the implications of our taxonomy, prototypes, and user study findings.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–11},
numpages = {11},
keywords = {accessibility, visual impairment, blindness, alt text, captions, alternative text, screen readers},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@article{southwell2013evaluation,
  title={{An Evaluation of Finding Aid Accessibility for Screen Readers}},
  author={Southwell, Kristina L and Slater, Jacquelyn},
  journal={Information Technology and Libraries},
  volume={32},
  number={3},
  pages={34--46},
  year={2013}
}

@inproceedings{borodin2010more,
  title={{More than meets the eye: A survey of screen-reader browsing strategies}},
  author={Borodin, Yevgen and Bigham, Jeffrey P and Dausch, Glenn and Ramakrishnan, IV},
  booktitle={Proceedings of the 2010 International Cross Disciplinary Conference on Web Accessibility (W4A)},
  pages={1--10},
  year={2010}
}

@article{caldwell2008web,
  title={{Web content accessibility guidelines (WCAG) 2.0}},
  author={Caldwell, Ben and Cooper, Michael and Reid, Loretta Guarino and Vanderheiden, Gregg and Chisholm, Wendy and Slatin, John and White, Jason},
  journal={WWW Consortium (W3C)},
  volume={290},
  pages={1--34},
  year={2008}
}

@inproceedings{salin2022vision,
  title={{Are Vision-Language Transformers Learning Multimodal Representations? A Probing Perspective}},
  author={Salin, Emmanuelle and Farah, Badreddine and Ayache, St{\'e}phane and Favre, Benoit},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  pages={11248--11257},
  year={2022}
}

@InProceedings{pmlr-v139-cho21a,
  title = 	 {{Unifying Vision-and-Language Tasks via Text Generation}},
  author =       {Cho, Jaemin and Lei, Jie and Tan, Hao and Bansal, Mohit},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {1931--1942},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/cho21a/cho21a.pdf},
  abstract = 	 {Existing methods for vision-and-language learning typically require designing task-specific architectures and objectives for each task. For example, a multi-label answer classifier for visual question answering, a region scorer for referring expression comprehension, and a language decoder for image captioning, etc. To alleviate these hassles, in this work, we propose a unified framework that learns different tasks in a single architecture with the same language modeling objective, i.e., multimodal conditional text generation, where our models learn to generate labels in text based on the visual and textual inputs. On 7 popular vision-and-language benchmarks, including visual question answering, referring expression comprehension, visual commonsense reasoning, most of which have been previously modeled as discriminative tasks, our generative approach (with a single unified architecture) reaches comparable performance to recent task-specific state-of-the-art vision-and-language models. Moreover, our generative approach shows better generalization ability on questions that have rare answers. Also, we show that our framework allows multi-task learning in a single architecture with a single set of parameters, achieving similar performance to separately optimized single-task models. Our code is publicly available at: https://github.com/j-min/VL-T5}
}

@article{liu2024visual,
  title={{Visual Instruction Tuning}},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={Advances in neural information processing systems},
  volume={36},
  year={2024}
}

@inproceedings{naikar2024accessibility,
  title={{Accessibility Feature Implementation Within Free VR Experiences}},
  author={Naikar, Vinaya Hanumant and Subramanian, Shwetha and Tigwell, Garreth W},
  booktitle={Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
  pages={1--9},
  year={2024}
}

@inproceedings{di2004usable,
  title={{“Usable Accessibility” to the Web for Blind Users}},
  author={Di Blas, Nicoletta and Paolini, Paolo and Speroni, Marco and others},
  booktitle={Proceedings of 8th ERCIM Workshop: User Interfaces for All, Vienna},
  year={2004}
}

@article{williams2019find,
  title={{Find and Seek: Assessing the Impact of Table Navigation on Information Look-up with a Screen Reader}},
  author={Williams, Kristin and Clarke, Taylor and Gardiner, Steve and Zimmerman, John and Tomasic, Anthony},
  journal={ACM Transactions on Accessible Computing (TACCESS)},
  volume={12},
  pages={1--23},
  year={2019},
  publisher={ACM New York, NY, USA}
}

@inproceedings{zong2022rich,
  title={{Rich Screen Reader Experiences for Accessible Data Visualization}},
  author={Zong, Jonathan and Lee, Crystal and Lundgard, Alan and Jang, JiWoong and Hajas, Daniel and Satyanarayan, Arvind},
  booktitle={Computer Graphics Forum},
  volume={41},
  pages={15--27},
  year={2022},
  organization={Wiley Online Library}
}

@article{luo2022vc,
  title={{VC-GPT: Visual Conditioned GPT for End-to-End Generative Vision-and-Language Pre-training}},
  author={Luo, Ziyang and Xi, Yadong and Zhang, Rongsheng and Ma, Jing},
  journal={arXiv preprint arXiv:2201.12723},
  year={2022}
}

@inproceedings{bongini2023gpt,
  title={{Is GPT-3 All You Need for Visual Question Answering in Cultural Heritage?}},
  author={Bongini, Pietro and Becattini, Federico and Del Bimbo, Alberto},
  booktitle={Computer Vision--ECCV 2022 Workshops: Tel Aviv, Israel, October 23--27, 2022, Proceedings, Part I},
  pages={268--281},
  year={2023},
  organization={Springer}
}

@article{zhang2023gpt4mia,
  title={{GPT4MIA: Utilizing Geneative Pre-trained Transformer (GPT-3) as A Plug-and-Play Transductive Model for Medical Image Analysis}},
  author={Zhang, Yizhe and Chen, Danny Z},
  journal={arXiv preprint arXiv:2302.08722},
  year={2023}
}

@article{zhu2023minigpt,
  title={{MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models}},
  author={Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed},
  journal={arXiv preprint arXiv:2304.10592},
  year={2023}
}

@article{achiam2023gpt,
  title={{GPT-4 Technical Report}},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@misc{OpenAI_2024, url={https://openai.com/index/hello-gpt-4o}, journal={Hello GPT-4o}, author={OpenAI}, year={2024}, month={May}} 

@inproceedings{de2024llmr,
  title={{LLMR: Real-time prompting of interactive worlds using large language models}},
  author={De La Torre, Fernanda and Fang, Cathy Mengying and Huang, Han and Banburski-Fahey, Andrzej and Amores Fernandez, Judith and Lanier, Jaron},
  booktitle={Proceedings of the CHI Conference on Human Factors in Computing Systems},
  pages={1--22},
  year={2024}
}

@article{ma2024llms,
  title={{When LLMs step into the 3D World: A Survey and Meta-Analysis of 3D Tasks via Multi-modal Large Language Models}},
  author={Ma, Xianzheng and Bhalgat, Yash and Smart, Brandon and Chen, Shuai and Li, Xinghui and Ding, Jian and Gu, Jindong and Chen, Dave Zhenyu and Peng, Songyou and Bian, Jia-Wang and others},
  journal={arXiv preprint arXiv:2405.10255},
  year={2024}
}

@inproceedings{aghel2024people,
  title={{How People Prompt Generative AI to Create Interactive VR Scenes}},
  author={Aghel Manesh, Setareh and Zhang, Tianyi and Onishi, Yuki and Hara, Kotaro and Bateman, Scott and Li, Jiannan and Tang, Anthony},
  booktitle={Proceedings of the 2024 ACM Designing Interactive Systems Conference},
  pages={2319--2340},
  year={2024}
}

@misc{BeMyAI, note = {Available at: \url{https://www.bemyeyes.com/blog/announcing-be-my-ai}. Accessed on December 4th 2024.}, journal={{Announcing ‘Be My AI,’ Soon Available for Hundreds of Thousands of Be My Eyes Users}}, author={Be My Eyes}, year={2023}, month={Sep}} 

@misc{microsoft-seeing-ai,
  author = {Microsoft},
  title = {{Seeing AI}},
  howpublished = {\url{https://www.microsoft.com/en-us/ai/seeing-ai}},
  year = {2021},
  month = {September},
  day = {30}
}


@article{braun_using_2006,
	title = {{Using thematic analysis in psychology}},
	volume = {3},
	issn = {1478-0887},
	doi = {10.1191/1478088706qp063oa},
	number = {2},
	journal = {Qualitative Research in Psychology},
	author = {Braun, Virginia and Clarke, Victoria},
	month = jan,
	year = {2006},
	keywords = {epistemology, flexibility, patterns, qualitative psychology, thematic analysis},
	pages = {77--101},
	file = {Braun and Clarke - 2006 - Using thematic analysis in psychology.pdf:/Users/rosella/Zotero/storage/CX5PC9JL/Braun and Clarke - 2006 - Using thematic analysis in psychology.pdf:application/pdf},
}




@inproceedings{naikar_accessibility_2024,
	address = {New York, NY, USA},
	series = {{CHI} {EA} '24},
	title = {Accessibility {Feature} {Implementation} {Within} {Free} {VR} {Experiences}},
	isbn = {9798400703317},
	doi = {10.1145/3613905.3650935},
	abstract = {Virtual reality (VR) enables many exciting, immersive experiences; however, those experiences are sometimes inaccessible. Researchers have explored methods to improve VR accessibility, but it is also important to ensure that we have a clear understanding of the prevalence of current accessibility feature implementation to identify what support is less common for different impairments (e.g., vision, motor, hearing). In our small-scale study, we focused on free VR experiences since everybody can install those apps and games without any cost. We inspected 106 free VR experiences available for the Meta Quest 2 so that we could determine if relevant accessibility features were available for the modes of interaction present, as well as the overall prevalence of accessibility features across the sample of VR experiences. We found that 36.8\% of our sample did not include any accessibility features, and many other VR experiences could have benefited from additional accessibility features. We make recommendations that can help to ensure future VR experiences are implemented with more accessibility in mind.},
	booktitle = {Extended {Abstracts} of the 2024 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Naikar, Vinaya Hanumant and Subramanian, Shwetha and Tigwell, Garreth W.},
	month = may,
	year = {2024},
	pages = {1--9},
}
