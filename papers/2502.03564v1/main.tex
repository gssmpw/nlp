% \documentclass[lettersize,journal]{IEEEtran}
\documentclass[journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
% \usepackage{cite}

\usepackage{booktabs}
\usepackage[table,xcdraw]{xcolor}
\usepackage{hyperref}
 \hypersetup{
     colorlinks = true,
     allcolors = {black},
    }
\usepackage{cleveref}
\usepackage{array}
\usepackage{gensymb}
\usepackage{soul}
% \usepackage{natbib}
\usepackage[numbers]{natbib}


% \hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021

%\newcommand{\todonote}[1]{\textcolor{purple}{\textbf{TODO: }#1}}

\begin{document}

\title{EnVisionVR: A Scene Interpretation Tool for Visual Accessibility in Virtual Reality}

% \author{IEEE Publication Technology,~\IEEEmembership{Staff,~IEEE,}
%         % <-this % stops a space
% \thanks{This paper was produced by the IEEE Publication Technology Group. They are in Piscataway, NJ.}% <-this % stops a space
% \thanks{Manuscript received April 19, 2021; revised August 16, 2021.}}

\author{Junlong Chen\thanks{Email: jc2375@cam.ac.uk}, 
Rosella P. Galindo Esparza\thanks{Email: rosellapaulina.galindoesparza@brunel.ac.uk}, 
Vanja Garaj\thanks{Email: vanja.garaj@brunel.ac.uk}, 
Per Ola Kristensson\thanks{Email: pok21@cam.ac.uk}, 
John Dudley\thanks{Email: jjd50@cam.ac.uk}}

% % Author and Affiliation (multiple authors with multiple affiliations)
% \author{Roy G. Biv\thanks{e-mail: roy.g.biv@aol.com}\\ %
% \and Ed Grimley\thanks{e-mail:ed.grimley@aol.com}\\ %
%      \scriptsize Grimley Widgets, Inc. %
% \and Martha Stewart\thanks{e-mail:martha.stewart@marthastewart.com}\\ %
%      \parbox{1.4in}{\scriptsize \centering Martha Stewart Enterprises \\ Microsoft Research}}



% \author{Junlong Chen}
% \affiliation{%
%   \institution{University of Cambridge}
%   \city{Cambridge}
%   \country{United Kingdom}}
% \email{jc2375@cam.ac.uk}

% \author{Rosella P. Galindo Esparza}
% \affiliation{%
%   \institution{Brunel University London}
%   \city{London}
%   \country{United Kingdom}
% }
% \email{rosellapaulina.galindoesparza@brunel.ac.uk}

% \author{Vanja Garaj}
% \affiliation{%
%   \institution{Brunel University London}
%   \city{London}
%   \country{United Kingdom}}
% \email{vanja.garaj@brunel.ac.uk}

% \author{Per Ola Kristensson}
% \affiliation{%
%   \institution{University of Cambridge}
%   \city{Cambridge}
%   \country{United Kingdom}}
% \email{pok21@cam.ac.uk}

% \author{John Dudley}
% \affiliation{%
%   \institution{University of Cambridge}
%   \city{Cambridge}
%   \country{United Kingdom}}
% \email{jjd50@cam.ac.uk}

% The paper headers
% \markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}%
% {Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

% \IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

%% A teaser figure can be included as follows, but is not recommended since
%% the space is now taken up by a full width abstract.


\maketitle





\begin{abstract}
Effective visual accessibility in Virtual Reality (VR) is crucial for Blind and Low Vision (BLV) users. However, designing visual accessibility systems is challenging due to the complexity of 3D VR environments and the need for techniques that can be easily retrofitted into existing applications. %the complexity of VR 3D environments and the need for techniques that can be easily retrofitted into existing applications pose significant design challenges for these visual accessibility systems.
%Designing visual accessibility systems for Virtual Reality (VR) is challenging due to the complexity of immersive 3D environments and the need for techniques that can be easily retrofitted into existing applications.
  % to redesign interaction techniques for users with accessibility needs that integrate well into existing VR systems.
  % , and the requirement to generalize to other VR projects and integrate with existing system architecture. 
While prior work has studied how to enhance or translate visual information, the advancement of Vision Language Models (VLMs) provides an exciting opportunity to advance the scene interpretation capability of current systems. 
  %While prior work has studied how to enhance visual information or convert it to audio or haptic information,
  % there is a lack of works that study how 
  %the advancement of Vision Language Models (VLMs) brings about an exciting opportunity to 
  % could be incorporated to 
  %enhance the scene interpretation capability of current visual accessibility systems. %, especially for complex 3D scenes where it is crucial to accurately interpret spatial information for the user. 
This paper presents \textsc{EnVisionVR}, an accessibility tool for VR scene interpretation. %in VR for BLV users. 
Through a formative study of usability barriers, we confirmed the lack of visual accessibility features as a key barrier for BLV users of VR content and applications.
  %Through a formative study, we found that Blind and Low Vision (BLV) users faced accessibility barriers such as the lack of screen reader features when using VR technology.
  % , and there was a lack of screen reader features for immersive content. 
In response, we designed and developed \textsc{EnVisionVR}, a novel visual accessibility system leveraging a VLM, voice input and multimodal feedback for scene interpretation and virtual object interaction in VR.
  %In response, we developed \textsc{EnVisionVR}, an accessibility tool to assist scene interpretation in VR for BLV users.
  % The tool enables users to ask three types of simple questions via voice input to gain a better understanding of the scene. 
An evaluation with 12 BLV users demonstrated that \textsc{EnVisionVR} significantly improved their ability to locate virtual objects, effectively supporting scene understanding and object interaction.
  %An empirical study with 12 BLV users demonstrated that \textsc{EnVisionVR} significantly improved their ability to locate virtual objects, and resulted in improved performance in scene understanding and object interaction.
  % The improvement was also found to be larger for BVI users who regularly used assistive technology compared with those who did not.
  % was effective in helping users understand the scene, locate objects, and interact with virtual objects in the scene. 
  % A set of four design recommendations are distilled for further improvement of VR visual accessibility systems with speech interfaces powered by LLMs.
  % on general visual accessibility system design for 3D immersive environments , as well as specific recommendations 
  % such as allowing users to adjust reading speed or specify the level of detail of descriptions
  % the consistency of reference frames, accuracy in wording, and  
\end{abstract}

\begin{IEEEkeywords}
Virtual Reality (VR), Vision Language Models, Visual Accessibility, Blind and Low Vision Users.
\end{IEEEkeywords}



\section{Introduction}
\IEEEPARstart{V}{irtual} Reality (VR) is a primarily visual medium.
The centrality of visual perception in the VR experience presents a major challenge when making the technology accessible to Blind and Low Vision (BLV) users. While screen readers and voiceover systems have played a crucial role in enabling BLV users to access information from two-dimensional (2D) screens, this accessibility issue persists %remains an issue
for three-dimensional (3D) spatial content.
In contrast with how a screen reader works on conventional 2D user interfaces, the current form of VR applications challenges the systematic organisation and delivery of 3D spatial information in an intuitive and efficient format.
%the current form of VR applications makes it difficult to organize 3D spatial information systematically and deliver the information to the user in an intuitive and efficient format. 

In an effort to address the exclusion of BLV users from VR experiences, prior work has studied visual accessibility design in virtual \cite{zhao2019seeingvr, canetroller} and augmented reality \cite{herskovitz2020making}. These efforts have adopted different strategies, such as enhancing visual information through view magnification, brightness/contrast adjustment, object contour highlighting~\cite{zhao2019seeingvr}; or converting visual information to other forms like audio descriptions of virtual objects~\cite{zhao2019seeingvr} or vibrotactile feedback~\cite{canetroller}.
% For augmented reality (AR), Herskovitz et al. \cite{herskovitz2020making} designed visual accessibility tools for mobile AR applications and evaluated their system with 10 blind participants. For virtual reality (VR), Zhao et al. from Microsoft Research \cite{zhao2019seeingvr} developed \textsc{SeeingVR}, a set of 14 visual accessibility tools to be incorporated into VR projects. Examples of these tools include view magnification, brightness/contrast adjustment, object contour highlighting, and simple object descriptions. While \textsc{SeeingVR} focused on enhancing visual elements in the scene and converting visual information to audio, another work proposed \textsc{Canetroller} \cite{canetroller}, which presents an alternative strategy to convert visual information to audio and haptic information for blind and visually impaired users.
% These existing works share the common strategy of converting visual information to other forms of information for BLV users. 
%However, w
With the advent of Vision Language Models (VLMs), new opportunities are emerging to generate vivid and detailed scene descriptions based on the user's field of view.
Such capability can be embedded in output modalities such as speech, audio, and haptic cues to facilitate the user's understanding of 3D scenes. 
%, thereby bringing the strategy of converting visual information to other forms of information to the next level. In this process, we combine high-level scene descriptions with detailed object-level information to maximize information throughput. In an empirical study with 12 BLV participants, we demonstrate that this system is effective in helping users understand the scene, locate objects, and interact with objects.

This paper presents \textsc{EnVisionVR}, an integrated set of VR scene interpretation and virtual object localization tools that assist BLV users in navigating VR. 
% Previous works have either focused on visual accessibility design in virtual reality with traditional tools to provide audio, speech, haptic information or visual feature enhancement, or have applied VLMs for image captioning but not for visual accessibility applications for 3D content. This work bridges the research gap in the lack of works which apply VLMs for visual accessibility design for immersive content. Our central hypothesis is that visual accessibility systems empowered by VLMs which provide speech, audio, and haptic cues can assist BLV users in understanding the scene, locating virtual objects, and interacting with virtual objects in virtual reality than out-of-the-box VR systems.
%These set of scene interpretation and object localization tools are introduced together as \textsc{EnVisionVR}, 
The development was guided by a formative usability study with nine BLV participants, %among a larger pool of people with diverse accessibility needs (N = 60)%. %We aimed to gain a deeper understanding of accessibility barriers encountered in VR and how people managed to overcome these issues whenever possible. Particularly focusing on the way 
which provided empirical information on the support required by BLV users and, particularly, the types of accessibility features that support scene understanding and interaction. %or sought external support. %(e.g., another person describing what was going on in the scene), 
% our findings provided a baseline to identify potential avenues for improving the existing set of visual accessibility features in VR, especially with regards to supporting scene interpretation. 
\textsc{EnVisionVR} was then implemented as a proof-of-concept system to improve visual accessibility in VR by providing (a) high-level natural language scene interpretation powered by a VLM, and (b) detailed low-level object localization tools based on speech, audio, and haptic cues.
% In response, \textsc{EnVisionVR} adopts VLMs to provide high-level descriptions of the user's field of view and uses multimodal feedback
% % including audio cues, speech, and haptic vibrations 
% to provide detailed object-level information to inform users of main objects nearby and guide them to locate these objects for further interaction.
% % \textcolor{red}{TO DO: Expand on how EnVisionVR works.}%
%Finally, t
The system was evaluated in a user study with 12 BLV participants, who were asked to complete three tasks related to scene understanding, object localization, and object interaction with and without \textsc{EnVisionVR} in a VR scene. 
% Task completion results revealed that 
Participants achieved a significantly higher success rate when locating virtual objects with significantly lower perceived difficulty with \textsc{EnVisionVR}.

%Participants who regularly use assistive technology tend to have a more significant improvement in the three tasks and exhibit different interaction behaviors based on their function activation counts compared with those who do not regularly use assistive technology. Post-study interviews reveal recurring themes on the level of information delivered, system latency, design consistency, and user agency.% Through findings from this evaluative study, we propose an initial set of design implications for visual accessibility design in VR.
% \textcolor{red}{TO DO: Expand on what was done, key findings}. 
% After discussing the findings of this evaluation, we propose an initial set of design implications for visual accessibility design in VR.
% with the aim of answering the following research questions (RQs):
% \begin{itemize}
%     \item \textbf{RQ1:} Compared with conventional screen readers, what kind of information needs to be delivered for 3D scenes?
%     \item \textbf{RQ2:} How can we translate visual information into auditory and haptic information effectively and convey information to the user efficiently?
%     \item \textbf{RQ3:} How can we give control to BLV users on what accessibility functions to elicit through the design of interaction techniques?
%     \item \textbf{RQ4:} Does the complete system offer a more inclusive VR experience for blind and low vision users compared with no accessibility conditions?
% \end{itemize}

% To validate our findings, we formulate the following hypotheses:
% \begin{itemize}
%     \item \textbf{H1:} Participants perform better in the scene understanding question, the object localization task, and the object interaction task under the \textsc{EVR} condition as compared with the \textsc{NVR} condition.
%     \item \textbf{H2:} \textsc{EnVisionVR} allows the performance of participants with blindness and severe visual impairment (those who regularly use assistive technology) to improve more as compared to participants with less-severe visual impairment (those who do not regularly use assistive technology).
%     \item \textbf{H3:} Factors such as higher age, unfamiliarity with VR technology, and the presence of multiple disabilities could negatively affect the ability of \textsc{EnVisionVR} to improve participants' performance in the three tasks.
% \end{itemize}
%In summary, this paper makes the following contributions:
%The key contributions of this research are three-fold. (1) %This project 
This research makes three main contributions.
%(1) First, the formative study complements the existing literature on the common barriers encountered by BLV users. It emphasizes how the lack of visual accessibility functions providing scene descriptions and interaction support poses a barrier.
First, the formative study adds to the existing literature on accessibility barriers for BLV users by emphasizing the lack of functions for scene description and interaction support as a key concern.
% {\color{red}[Junlong to Rosella: Please check if my understanding is correct here. Section III mentions the lack of an integrated screen reader feature. Shall we phrase the contribution this way?]} 
Second, to the best of our knowledge, \textsc{EnVisionVR} is the first proof-of-concept system to incorporate detailed VLM-based scene descriptions for real-time visual accessibility in VR, through spatial audio, voice instructions, and speech-based function activation methods.
%, \textsc{EnVisionVR} as a proof-of-concept for effective integration of VLM-based scene interpretation with spatial audio, voice instructions, and speech-based function activation methods to enhance visual accessibility design in VR. 
Third, we offer a set of design implications derived from the system's development process and evaluation to inform visual accessibility design in VR more extensively.
%we evaluate \textsc{EnVisionVR} with 12 BLV participants. 
%By gathering quantitative and qualitative feedback, we guide design iterations and further development of the system. Findings from this evaluative study also yield design implications that inform visual accessibility design in VR more broadly.
% \textcolor{red}{TO DO: Contributions currently as a summary or conclusion. The design implications could be a third contribution.}

% Contribution list \textcolor{red}{(TODO-JC: Please flesh out.)}:
% \begin{enumerate}
%     \item Learnings from the formative study
%     \item The EnVisionVR system
%     \item Empirical evaluation of the system and design implications
% \end{enumerate}

% and 3D content in general.
% We introduce \textsc{EnVisionVR}, a set of scene interpretation and object localization tools for 

% As it is not intuitive to present information in a virtual reality (VR) scene in a simple linear order, we employ a strategy of combining system-elicited high-level scene introduction and user-elicited detailed-level object introduction to present 3D scene information.

\section{Related Work}

\subsection{Visual Accessibility Design in VR}

% To date, there is a high percentage of VR experiences that lack accessibility features
In a study conducted by Naikar et al.~\cite{naikar2024accessibility}, 39 out of 106 inspected free VR experiences (36.8\%) lacked accessibility features. Furthermore, users may encounter multiple accessibility barriers in the same context~\cite{creed2024inclusive}. Extensive work has focused on advancing visual accessibility %design 
in VR to provide a more inclusive experience~\cite{dudley2023inclusive}. Mostly, this has been approached through augmenting visual information \cite{zhao2019seeingvr, masnadi2020vriassist, teofilo2018evaluating} or translating it into audio or haptic feedback \cite{canetroller, kim2020vivr, zhao2019seeingvr, ji2022vrbubble}. 
% identify the lack of accessibility features in VR experiences, Creed et al. \cite{creed2024inclusive} provide a summary of accessibility barriers for immersive technologies, while 
% Dudley et al. \cite{dudley2023inclusive} present a comprehensive review of visual accessibility designs in VR. 
%Work to date on supporting visual accessibility in VR has primarily focused on either 
%the augmentation of visual information or the conversion from visual information to audio or haptic information. 

%The work of Te\'ofilo et al. \cite{teofilo2018evaluating} is an example of augmenting visual information. They developed and evaluated a system called Gear VRF Accessibility which provides a framework for developers to adapt zoom, invert colors, and add captions in a VR environment. Similarly, SeeingVR~\cite{zhao2019seeingvr} is a set of 14 tools including magnification lens, brightness lens, edge enhancement, text augmentation, and other visual augmentation tools. User studies conducted with SeeingVR demonstrated that the tools were effective in helping people with low vision to complete tasks such as menu navigation, visual search, and target shooting in VR. VRiAssist~\cite{masnadi2020vriassist} provides visual assistance in VR based on eye tracking. It includes a distortion correction tool, a color/brightness correction tool, and an adjustable magnification tool. Wu et al.~\cite{wu2021towards} provided adjustable print and text layout, smart text contrasting, and image enhancement to provide visual accessibility features for reading in VR. Ciccone et al.~\cite{ciccone2023next} recommended implementing contrast adjustment controls, color correction controls, and font and display size adjustments to increase information visibility for visual accessibility design. 

Outstanding work in the area of augmenting visual information includes the development of tools for magnification, contrast adjustment, color correction, text and display size adjustment, among others. Gear VRF Accessibility~\cite{teofilo2018evaluating}, for instance, provided a framework for developers to adapt zoom, invert colors, and add captions in a VR environment. VRiAssist~\cite{masnadi2020vriassist} supported the user by offering visual assistance based on eye tracking, providing tools like magnification, distortion, colour and brightness correction. SeeingVR~\cite{zhao2019seeingvr} involved a larger set of visual augmentation tools that proved effective for task completion in VR (such as menu navigation, visual search, and target shooting).
%Wu et al.~\cite{wu2021towards} provided adjustable print and text layout, smart text contrasting, and image enhancement to provide visual accessibility features for reading in VR. 
Consistent with these approaches, Ciccone et al.~\cite{ciccone2023next} recommended implementing contrast adjustment controls, color correction controls, and font and display size adjustments to increase information visibility when designing for visual accessibility.

% The second approach of converting visual information to other forms of information is exemplified by the work of Zhao et al. and their system Canetroller \cite{canetroller}. Canetroller provides 3D spatial audio feedback to simulate interactions between the cane and the virtual world, and also uses physical resistance and vibrotactile feedback to simulate the cane touching and hitting an object. VIVR~\cite{kim2020vivr} is a similar effort to simulate the interaction of a white cane with virtual braille blocks on a VR sidewalk based on audio feedback and controller vibrations. In SeeingVR \cite{zhao2019seeingvr}, there are also tools such as text-to-speech and object recognition to convert visual information to speech. In VRBubble~\cite{ji2022vrbubble}, audio alternatives such as earcons, verbal notifications, and real-world sound effects enhance the peripheral awareness of users with vision impairment to facilitate social VR accessibility. Dang et al. \cite{dang2023opportunities} outlined opportunities for a multimodal-multisensor VR system to use spatial audio, audio descriptions, audio feedback, and vibrotactile feedback to enhance the experience of BLV people in immersive musical performances. Other works on social VR applications explored how sighted guides can support BLV user navigation and visual interpretation \cite{collins2023guide}, which also fall into the category of converting visual information to other forms of information for visual accessibility design.

Research focused on converting visual information into other forms has also resulted in a variety of systems supporting visual accessibility in VR. For instance, both Canetroller~\cite{canetroller} and VIVR~\cite{kim2020vivr} simulated the use of a white cane in the virtual world. This included providing 3D spatial audio feedback, physical resistance, and vibrotactile feedback to simulate cane--virtual object interaction. The aforementioned SeeingVR~\cite{zhao2019seeingvr} also included text-to-speech and object recognition from visual information to speech. In a more specialized context, Dang et al.~\cite{dang2023opportunities} outlined a multimodal-multisensor VR system with spatial audio, audio descriptions, audio feedback, and vibrotactile feedback to enhance the experience of BLV participants in immersive musical performances. 
Finally, VRBubble~\cite{ji2022vrbubble} enhanced BLV users' peripheral awareness to facilitate social VR accessibility through audio alternatives such as earcons, verbal notifications, and real-world sound.

%Other works on social VR applications explored how sighted guides can support BLV user navigation and visual interpretation \cite{collins2023guide}, which also fall into the category of converting visual information to other forms of information for visual accessibility design.

% The above two approaches to augment visual information directly or convert visual information to other forms have been widely adopted in existing literature.  %, as well as designs for other forms of impairment and other immersive environments, and refers this objective of maximizing the accessibility and enjoyment of users in immersive environments as \textit{Inclusive Immersion}. 
% Among the two approaches to providing visual accessibility design, the first approach of augmenting visual features does %intuitive and easy to implement, but is 
% not support users who are blind or have very limited visual perception, while the second approach of information conversion supports them. In this paper, we focus on the second approach and the much less explored methods of converting visual information to speech, audio cues and haptics that this approach entails. We explore how VLMs could be incorporated to provide vivid scene descriptions. By combining these multiple modalities, we aim to provide users with both a high-level understanding of their surroundings as well as a detailed understanding of object-level information to support interaction.

Among both approaches, augmenting visual information cannot support users who are blind or with very limited visual perception. Thus, the work in this paper focuses on integrating the relatively underexplored methods of converting visual information into speech, audio cues, and haptics. We investigate how VLMs could be incorporated to provide vivid scene descriptions. By combining these multiple modalities, we aim to provide users with a high-level understanding of their surroundings, as well as a detailed understanding of object-level information to support interaction.

% People have used multimodal interaction techniques for visual accessibility design in immersive environments

\subsection{Screen Readers and Web Accessibility}

Screen readers are a well-established accessibility tool for BLV users; %have been proven to be a success for web accessibility on 2D screens. 
their design concepts can provide valuable insights for the design of visual accessibility in immersive environments.
NVDA, JAWS, and VoiceOver are three of the most commonly used screen readers for desktops and laptops~\cite{WebAim-10}. While these different screen readers have distinct characteristics, they share key design principles which underpin their effectiveness. First, popular screen readers prioritize keyboard navigation. 
%According to Kearney-Volpe and Hurst~\cite{accessibleWebDev2021}, 
Keyboard navigation allows users to navigate digital content without the need for a mouse, which is critical for people with vision impairment~\cite{accessibleWebDev2021}. 
Second, screen readers focus on the semantic structure to facilitate smooth navigation and ensure information accuracy. On this topic, a series of works~\cite{zong2022rich, di2004usable, williams2019find} have specifically focused on how to improve the usability of screen readers by correctly and efficiently conveying semantic details. 
Third, screen readers also provide alternative text for images, which is a crucial step to help convey non-textual content~\cite{alttext2017, altText2018}. Fourth, screen readers use headings and landmarks to assist website navigation and hierarchy~\cite{southwell2013evaluation}. Finally, screen readers also assist user input, such as filling in and submitting forms and documents online, an important part of web interaction~\cite{borodin2010more}.

% As an example of web accessibility design, screen readers also follow the web content accessibility guidelines (WCAG) \cite{caldwell2008web}, a framework to ensure the inclusivity of digital content for people with disabilities. The four core principles of WCAG are known as POUR, which stands for:
% \begin{itemize}
%     \item \textbf{Perceivable:} Users must be able to perceive the information presented.
%     \item \textbf{Operable:} Users must be able to operate the interface.
%     \item \textbf{Understandable:} Users must be able to understand the information and operation of the interface.
%     \item \textbf{Robust:} Users must be able to access the content as technologies advance.
% \end{itemize}

% While the four accessibility principles above are proposed for web accessibility, these principles together with the five design principles above for screen readers provide valuable insights for accessibility design in VR. 
\textsc{EnVisionVR} takes inspiration and expands on the design principles and concepts of screen readers. %of screen readers.
% Keyboard button-based navigation informed our design of using the single and double-press of a single controller button to activate or deactivate \textsc{EnVisionVR}. The semantic structure in conventional screen readers inspired us to decompose 3D information into high-level scene information and detailed-level object information. Alternative text for images inspired us to generate scene descriptions at certain important locations in the scene prior to deploying the scene. Finally, a set of three simple speech commands to activate the functions serve as the ``headings'' and ``landmarks'' to assist with user navigation and understanding.
Based on the above, we arrive at an interactive design that uses speech commands as a parallel to keyboard navigation, while constructing high-level scene information and detailed object-level information for BLV users as a parallel to the semantic structure processed by screen readers. Furthermore, %Similarly, 
VLMs provide a highly efficient way to produce scene descriptions, %which serve as a parallel to explicitly entered alternative text.
a parallel to explicit alternative text.
% and pre-defined anchor points in 3D scenes serve a similar function with website headings and landmarks. Through an empirical user study, we demonstrate that the above features of \textsc{EnVisionVR} satisfy the four core principles of WCAG.

\subsection{Powering Visual Accessibility with Artificial Intelligence}

The emergence of powerful VLMs has enabled the automated generation of high-quality descriptions of visual information.  %With advanced AI models, BLV individuals will be able to access 2D and 3D visual content via vivid and detailed natural language descriptions easily.
Current VLMs~\cite{salin2022vision,pmlr-v139-cho21a,luo2022vc,bongini2023gpt,zhang2023gpt4mia} are capable of jointly processing images and text data for image captioning, visual question answering, and medical image analysis. %According to the technical report by OpenAI \cite{achiam2023gpt} and the official webpage \cite{OpenAI_2024}, more recent models such as GPT-4 and GPT-4 Omni (GPT-4o) are capable of processing image and text inputs and produce text outputs, achieving high-level performance on text, reasoning, and coding intelligence, with high multilingual, audio, and vision capabilities. 
% In the work of De La Torre et al. \cite{de2024llmr}, state-of-the-art large language models (LLMs) have also been applied to design \textsc{LLMR}, an LLM for Mixed Reality framework which analyzes and generates 3D scenes to support a wide variety of interaction tasks such as 3D world creation, multi-modal interaction, 3D scene editing, 3D scene information query, and external plugin and cross-platform integration. In a survey by Ma et al. \cite{ma2024llms} on the applications of LLMs in the 3D world, the authors summarize how recent works have leveraged the strong learning and reasoning capabilities as well as extensive world knowledge of LLMs to process, understand, and generate 3D data. In terms of user behavior and interaction design, Aghel Manesh et al. \cite{aghel2024people} also studied how users adopt generative AI to prompt the system to create VR scenes through an elicitation study with the Wizard-of-Oz technique.
These models are now being deployed in a range of use cases to power visual accessibility features. 
For example, De La Torre et al.~\cite{de2024llmr} demonstrated potential applications of their Large Language Model (LLM)-based tool for 3D scene editing in visual accessibility. Jiang et al.~\cite{jiang2023beyond} highlighted the potential of advanced AI models to enhance the quantity and quality of audio descriptions.
Microsoft developed SeeingAI~\cite{microsoft-seeing-ai} to narrate the physical world for BLV users.
Similarly, Be My Eyes launched Be My AI~\cite{BeMyAI}, an AI assistant powered by GPT-4, which provides detailed descriptions of photos taken and uploaded by BLV users, and a braille display for deaf-blind users. %This system provides visual assistance for everyday tasks such as navigating physical environments. 
Specific use cases for scene description in real-life scenarios have been identified through a diary study~\cite{gonzalez2024investigating}, which highlights the effectiveness of generative models for visual accessibility design.

The increasing attention to applying VLMs to interactions in 3D content and accessibility design illustrates the strong capability of such models. While existing work has demonstrated how state-of-the-art models could be applied %in a variety of tasks for 3D scenes, or how the models could be adopted 
in accessibility design for 2D images or videos, there has been limited work studying how these models could be applied in accessibility design for VR immersive environments. %Existing works differ from how we present opportunities for VLMs to generate vivid scene descriptions and incorporate VLMs with other multimodal interaction techniques in \textsc{EnVisionVR} to deliver high-level scene information and detailed object-level information for visual accessibility design in immersive environments.
% . which highlights the importance of leveraging advanced AI models to develop the visual accessibility tool \textsc{EnVisionVR} for immersive environments. , but it differs from how we present opportunities for VLMs to generate vivid scene descriptions.
% processing image and text inputs and producing text outputs with high 

%\section{Formative Study: Dominant VR Accessibility Barriers Encountered by Blind and Low-Vision Users}\label{sec:formative}
\section{Formative Study}\label{sec:formative}

% Before developing \textsc{EnVisionVR}, we conducted 
In the formative usability study, which involved nine BLV participants, we sought to understand the accessibility barriers encountered %by disabled and older users of 
in consumer-based VR and AR technology. Through this process, we studied the adaptations implemented when facing such barriers, namely the way people with specific access needs modified their behaviour or received assistance from another non-disabled person to fully or partially overcome these issues. %The study included
% a subset of 9 BLV participants. %from a sample comprised of people with diverse accessibility needs (N = 60). For the purpose of this paper, we only present a deep examination of the BLV participant subset on the VR experiences evaluated. %We refer the reader to \textcolor{red}{$<$\textit{anonymized for review}$>$} for the results of this study covering the complete set of participants.

\subsection{Method}

Our study protocol evaluated the usability of %existing immersive experiences. 
representative consumer-level, single-user VR experiences to identify the types of barriers encountered. The majority of the experiences represented content currently available in the market, while others were included to cover the full range of usability demands in VR, such as vision, hearing, touch and physical movement, and interaction modes, such as controller-based and hand-tracking. The study tasks and experiences became progressively more complex as the study progressed. %For reference, 
See the Online Appendix for the complete set of experiences, tasks and sub-tasks.

The Meta Quest 2 was used. Tasks and experiences were designed following the typical user journey; beginning with wearing and fitting the VR hardware (\textit{VRH:~Headset}, \textit{VRC:~Controllers}), 
followed by navigating the Meta Quest home menu to configure existing accessibility features (\textit{VR1:~Menu}), %, e.g., display contrast) 
and completing each of the selected experiences: 
\textit{VR2:~“As it is” 360° video}\footnote{Produced by \href{https://www.youtube.com/watch?v=BE-irHmbQOY}{\underline{360 Labs}}} (immersive video documentary),
\textit{VR3:~Job Simulator}\footnote{Produced by \href{https://jobsimulatorgame.com/}{\underline{Owlchemy Labs}}} (videogame simulating a cooking scenario, using virtual hands to manipulate objects while following cooking instructions), 
\textit{VR4:~Moss}\footnote{Produced by \href{https://www.polyarcgames.com/games/moss}{\underline{Polyarc}}} (storyline-based videogame where the user becomes a secondary character that interacts with objects and controls other characters), 
and \textit{VR5:~Elixir}\footnote{Produced by \href{https://www.magnopus.com/projects/elixir}{\underline{Magnopus}}} (hand-tracking-based videogame where the user manipulates virtual objects with their real hands). 
Sub-tasks %for each task, 
were basic commands revolving around specific steps required to progress through each experience and explore available features and interactables. 

In total, participants completed 34 %structured 
sub-tasks spread across two VR hardware tasks and five VR experiences  %that became progressively more complex 
(e.g., \textit{`Adjust the focal distance of the headset'}, \textit{`Spot different visitors in the scene, from those close by to those at a distance'}). Participants were asked to perform each sub-task while thinking aloud. A researcher scored task success on a 0--3 scale (0 = unable to start or finish the task, even with adaptations, 1 = able to start but unable to finish the task, even with adaptations, 2 = successful completion of the task with adaptations, and 3 = successful completion of the task without adaptations). The concept of \textit{adaptation} arose after a pilot study that showed most sub-tasks were not achievable for multiple people with access needs. Thus, we resolved to study adaptations as either \textit{self-initiated} unconventional behaviour (e.g., holding a VR controller with two hands for pointing accuracy) %or moving closer to virtual text to be able to read it) 
or \textit{assistance} from another person through a Wizard of Oz approach (e.g., imitating a non-existent screen reader feature). %Post-experience questionnaires (five-point Likert scale) recorded the participants' perceived level of presence, immersion and task difficulty.
%The selected experiences (summarized in Table~\ref{tab:vr_experiences}) examined in the study covered a broad scope of usability demands while, simultaneously, representing existing consumer-based technology.

The study was approved by the Ethics Committee of the College of Engineering, Design and Physical Sciences, Brunel University of London.
%The full set of sub-tasks completed within each experience is available for reference in supplementary material.
% \begin{itemize}
%     \item \textit{VRH: Headset.} Wearing and fitting the Meta Quest 2 headset. 
%     \item \textit{VRC: Controllers.} Handling the Meta Quest 2 controllers (right and left). 
%     \item \textit{VR1: Menu.} Navigating the Meta Quest 2 universal menu and configuring the offered accessibility features. 
%     \item \textit{VR2: “As it is” 360° video\footnote{Produced by 360 Labs}.} Immersive video documentary with limited interactions. 
%     \item \textit{VR3: Job Simulator\footnote{Produced by Owlchemy Labs}.} Videogame simulating everyday scenarios, such as cooking as a chef. Virtual hands allow the user to pick up and manipulate objects while following specific instructions. 
%     \item \textit{VR4: Moss\footnote{Produced by Polyarc}.} Storyline-based videogame where the user becomes a secondary character that interacts with objects and controls other characters.
%     \item \textit{VR5: Elixir\footnote{Produced by Magnopus}.} Hand-tracking-based videogame where the user follows instructions from an invisible character while manipulating virtual objects with their real hands.
% \end{itemize}
% \begin{table*}[t]
%     \caption{The seven VR experiences evaluated in the formative study. \\
%     % \Description{Table showing the seven VR experiences evaluated in the formative study. These include: (1) VRH: Headset, which refers to wearing and fitting the VR headset; (2) VRC: Controllers, which refers to handling the VR controllers; (3) VR1: Menu, which refers to navigating the menu and configuring the accessibility features; (4) VR2: ``As it is'' 360° video, which refers to experiencing a video documentary with limited interactions; (5) VR3: Job Simulator, which refers to playing a videogame simulating everyday scenarios; (6) VR4: Moss, which refers to playing a videogame to interact with objects and control other characters; (7) VR5: Elixir, which refers to playing a hand-tracking videogame to manipulate virtual objects with their hands based on instructions. }
%     \textsuperscript{a}Produced by 360 Labs. \textsuperscript{b}Produced by Owlchemy Labs. \textsuperscript{c}Produced by Polyarc. \textsuperscript{d}Produced by Magnopus.}
%     \centering
%     \begin{tabular}{l p{11cm}}
%     \toprule
%     \textbf{Experience} & \textbf{Description} \\
%     \midrule
%     \textit{VRH: Headset} & Wearing and fitting the Meta Quest 2 headset. \\
%     \textit{VRC: Controllers} & Handling the Meta Quest 2 controllers (right and left). \\
%     \textit{VR1: Menu} & Navigating the Meta Quest 2 universal menu and configuring the offered accessibility features. \\
%     \textit{VR2: “As it is” 360° video\textsuperscript{a}} & Immersive video documentary with limited interactions. \\
%     \textit{VR3: Job Simulator\textsuperscript{b}} & Videogame simulating everyday scenarios, such as cooking as a chef. Virtual hands allow the user to pick up and manipulate objects while following specific instructions.\\
%     \textit{VR4: Moss\textsuperscript{c}} & Storyline-based videogame where the user becomes a secondary character that interacts with objects and controls other characters. \\
%     \textit{VR5: Elixir\textsuperscript{d}} & Hand-tracking-based videogame where the user follows instructions from an invisible character while manipulating virtual objects with their real hands. \\
%     \bottomrule
%     \end{tabular}
%     \label{tab:vr_experiences}
% \end{table*}
The study session %in each session 
lasted approximately 120 minutes per participant. %including three compulsory breaks.
%The VR portion of the study was always conducted first and lasted approximately 120 minutes including breaks.
%Before commencing the study, %each participant answered questions regarding VR %, AR and gaming technology expertise 
%each participant completed a demographics questionnaire. 
%Then, they were shown how to use the VR and AR equipment. Next, participants were asked to complete two VR hardware tasks and five VR experiences plus (34 sub-tasks), followed by two AR experiences. 
The sessions were facilitated by a researcher experienced in providing BLV accessibility support; they were in charge of observing, scoring sub-tasks, and assisting the participants. A technician was in charge of onboarding and looking after the technical elements. 

\subsubsection{BLV Participants}

Nine participants (3 female, 6 male) who self-reported as blind or with low vision were recruited through an inclusive research user panel (managed by Open Inclusion \cite{OpenInclusion}). All participants provided informed consent. %, an inclusive research agency). 
Their ages ranged from 27 to 68 (\textit{M} = 43.11 years, \textit{SD} = 13.82) and their previous experience with VR technology ranged from novice (1) to competent (3). For these participants, sight was classed as the access need that impacted their lives most extensively. %Several participants had multiple access needs.
These details are summarized in Table~\ref{fig:M-heatmap}. 
To distinguish from participants in the study reported in \Cref{sec:study-design}, participants in the formative study are labelled as PF1 to PF9.

\begin{table*}[!t]
    \centering
    \caption
    [Table presenting the participant demographics and task success scores for the formative study with BLV users. The first column presents the Participants code, from PF1 to PF9. The second column presents the corresponding participant's age. The third column presents the participants' gender. The fourth column presents a description of each participant's vision impairment. The fifth column indicates the level of VR experience before the study (1 representing Novice and 5 representing Expert). The demographics are as follows: PF1, age 56, woman, blind, VR expertise level 2; PF2, age 38, woman, lacks most central vision and right peripheral vision, nystagmus, VR expertise level 3; PF3, age 41, man, blind, VR expertise level: 2; PF4, age 31, woman, light sensitivity due to Autism, VR expertise level: 3; PF5, age 56, man, blind in left eye, only central vision in right eye, VR expertise level: 1; PF6, age 68, man, double vision due to Multiple Sclerosis, VR expertise level: 1; PF7, age 27, man, Irlen Syndrome, VR expertise level: 1; PF8, age 32, man, blind, VR expertise level: 2; and PF9, age 39, man, anoridia with sensitivity to light and glare, nystagmus, VR expertise level: 1. The final eight columns provide the mean scores (\textit{M}) and standard deviation (\textit{S}) for the tasks VRH: Headset and VRC: Controllers, and for the experiences VR1: Menu, VR2: "As it is" 360-degrees video, VR3: Job simulator, VR4: Moss and VR5: Elixir. At the right of the table, a colour gradient is included: from dark red for 0, to light green for 3.00. From this heatmap, is noted that PF1 presented consistently lower scores, followed by PF8. And that the VR2 ("As it is" 360-degrees video) and VR5 (Elixir) were the experiences where participants scored lower.]
    {Participant demographics and task success scores for the formative study with BLV users. VR Expertise indicates VR experience level (1 representing Novice and 5 representing Expert). Mean scores ({M}) and standard deviation ({S}) are provided for each VR task or experience.}
    % \Description{The figure shows the mean scores and standard deviations for each VR task or experience per participant. Task success mean scores are lower in general for software-related tasks (VR1 to VR5) compared with hardware-related tasks (VRH and VRC).}
    \label{fig:M-heatmap}
    % \includegraphics[width=\linewidth]{Figs/Updated_Table_PFcode.png}
    \includegraphics[width=\linewidth]{Figs/Updated_Table_PFcode.pdf}
    \centering
\end{table*}

% \begin{table*}[t]
%     \centering
%     % \caption{Blind and Low-Vision Participant Demographics.}
%     \caption{Demographics of BLV participants in the formative study.}
%     % \Description{Table containing demographic information of BLV participants in the formative study. Participant P01, aged 56, female, is blind and inexperienced with VR. Participant P04, aged 38, female, has low central and peripheral vision with nystagmus, and is neither experienced nor inexperienced with VR. Participant P09, aged 41, male, is blind, has partial hearing loss, and is inexperienced with VR. Participant P10, aged 31, female, with light sensitivity (autism) and vestibular disorder, dexterity impairment and dyspraxia is neither experienced nor inexperienced with VR. Participant P21, aged 56, male, has a blind left eye and central vision in the right eye, has ADHD, and is very inexperienced with VR. Participant P26, aged 68, male, has double vision, reduced mobility and dexterity, and memory loss, and is very inexperienced with VR. Participant P31, aged 27, male, has Irlen syndrome, leg amputee and dyslexia, and is very inexperienced with VR. Participant P36, aged 32, male, is blind and inexperienced with VR. Participant P38, aged 39, male, has Nystagmus and Anoridia, and is very inexperienced with VR.}
%     \label{tab:FS-demographics}
%     % \vspace{0.1in}
%     % \renewcommand{\arraystretch}{1.2}
%     % \resizebox{0.95\linewidth}{!}{
%     \begin{tabular}{c c c p{4.5cm} c  c | c c c c c c c}
%     \hline
%     \textbf{Participant} & \textbf{Age} & \textbf{Gender} & \textbf{Vision}      & \textbf{VR Expertise} &     &     &     &     &     &     &     \\
%                          &              &                 & \textbf{Description} & \textbf{(1--5)} &       & VRH & VRC & VR1 & VR2 & VR3 & VR4 & VR5 \\ \hline
%     P01 & 56 & F & Blind & 2 & \textit{M} & 2.75 & 2.83 & 0.50 & 1.00 & 0.75 & 1.20 & 1.00 \\
%         &    &   &       &   & \textit{S} & 0.50 & 0.41 & 0.84 & 1.29 & 0.50 & 0.45 & 0.00 \\ 
%     P04 & 38 & F & Lacks most central vision and right peripheral vision. Nystagmus & 3 & \textit{M} & 3.00 & 3.00 & 1.50 & 2.57 & 2.50 & 2.40 & 2.50 \\
%         &    &   &                                                                  &   & \textit{S} & 0.00 & 0.00 & 1.22 & 1.13 & 1.00 & 0.55 & 0.71  \\
%     P09 & 41 & M & Blind & 2 & \textit{M} & 3.00 & 3.00 & 0.00 & 0.43 & 1.75 & 2.00 & 1.00 \\
%         &    &   &       &   & \textit{S} & 0.00 & 0.00 & 0.00 & 1.13 & 1.26 & 0.00 & 0.00 \\
%     P10 & 31 & F & Light sensitivity due to Autism & 3 & \textit{M} & 3.00 & 3.00 & 3.00 & 3.00 & 2.50 & 2.40 & 3.00 \\
%         &    &   &                                 &   & \textit{S} & 0.00 & 0.00 & 0.00 & 3.00 & 0.58 & 0.55 & 0.00 \\
%     P21 & 56 & M & Blind in left eye, only central vision in right eye & 1 & \textit{M} & 3.00 & 3.00 & 2.00 & 2.43 & 2.50 & 2.20 & 2.00 \\
%         &    &   &                                                     &   & \textit{S} & 0.00 & 0.00 & 0.00 & 0.53 & 0.58 & 0.45 & 0.00 \\
%     P26 & 68 & M & Double vision due to Multiple Sclerosis & 1 & \textit{M} & 2.00 & 3.00 & 3.00 & 3.00 & 1.75 & 2.20 & 2.00 \\
%         &    &   &                                         &   & \textit{S} & 0.00 & 0.00 & 0.00 & 0.00 & 0.50 & 0.45 & 0.00 \\
%     P31 & 27 & M & Irlen syndrome & 1 & \textit{M} & 3.00 & 3.00 & 3.00 & 3.00 & 3.00 & 3.00 & 3.00 \\
%         &    &   &                &   & \textit{S} & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\
%     P36 & 32 & M & Blind & 2 & \textit{M} & 3.00 & 3.00 & 0.00 & 0.43 & 2.00 & 1.20 & 1.50 \\
%         &    &   &       &   & \textit{S} & 0.00 & 0.00 & 0.00 & 1.13 & 0.00 & 0.45 & 0.71 \\
%     P38 & 39 & M & Anoridia with sensitivity to light and glare. Nystagmus & 1 & \textit{M} & 3.00 & 3.00 & 3.00 & 2.14 & 2.75 & 2.40 & 2.00 \\
%         &    &   &                                                         &   & \textit{S} & 0.00 & 0.00 & 0.00 & 0.90 & 0.50 & 0.55 & 0.00 \\ \hline
%         &    &   &  & \textbf{TOTAL (9)} & \textit{M} & 2.86 & 2.98 & 1.78 & 2.00 & 2.17 & 2.11 & 2.00 \\
%         &    &   &  &           & \textit{S} & 0.17 & 0.14 & 0.46 & 0.55 & 0.41 & 0.22 & 0.31 \\ 
%     \end{tabular}
%     % }
% \end{table*}

\subsection{Results}

% The usability study results are summarised here. 
% %Under key and emergent themes of interest
% Researchers analysed the study scores, participants' feedback and facilitator observations across all experiences and capability domains following a thematic analysis approach \cite{braun_using_2006}. Quantitative data was assessed through descriptive statistical analysis. A total of 157 instances of usability friction were reported within the sight loss sub-sample, out of 781 cases observed through the complete sample. 

\subsubsection{Task Success}

This score indicates the level of success in completing a sub-task. Each participant was presented with 34 sub-tasks in total (\textit{VRH} = 4, \textit{VRC} = 6, \textit{VR1} = 6, \textit{VR2} = 7, \textit{VR3} = 4, \textit{VR4} = 5, \textit{VR5} = 2). %As a result, 
305 individual scores were produced across the nine participants over the seven VR tasks/experiences; one sub-task was not performed due to participant request (PF8, \textit{VRH}). %The \textsc{Sight} sub-sample was one of the groups with a higher number of participants \textit{unable} or only \textit{partially able} to complete a sub-task, even with adaptations (scores = 0 and 1).
%; the other group consisted of the \textsc{Head Movement} sub-sample.
Mean scores %for the sub-tasks in each experience 
are summarized in Table~\ref{fig:M-heatmap}.
%Total mean scores in the VR hardware section (\textit{VRC} and \textit{VRH}) were relatively high. Notably, 
%All participants succeeded in operating the VR controllers (\textit{VRC}) without support, except for PF1 who required further guidance (\textit{pressing the controller's grip button}). 
%Regarding the VR headset (\textit{VRH}), only PF6 required external assistance in all the sub-tasks and PF1 in a single sub-task (\textit{adjusting the headset while wearing it}). 
%On the VR experience section, 
The %Meta Quest 
home menu (\textit{VR1}) presented the lowest total mean score %(see bottom row in Table~\ref{fig:M-heatmap}) 
while the highest was achieved in Job Simulator (\textit{VR3}). 
Low-vision participants could fully or partially complete \textit{VR1} and \textit{VR2} sub-tasks, blind participants were unable to start most of these sub-tasks. This trend partially evened out on the next experiences (\textit{VR3}, \textit{VR4} and \textit{VR5}), with all participants presenting mixed score levels, although blind participants were still positioned in the lower scale. 
%Nevertheless, one low-vision participant scored 3 in all VR sub-tasks (PF7).

% \begin{figure}[!t]
%     \centering
%     \includegraphics[width=\linewidth]{Figs/Heatmap-BLV_EnVision.png}
%     \centering
%     \caption
%     [Heatmap presenting the mean scores and standard deviation for each VR task per participant. The bottom row introduces the total mean scores and general standard deviation]
%     {Mean scores (\textit{M}) and standard deviation (\textit{S}) for each VR task or experience per participant.}
%     % \Description{The figure shows the mean scores and standard deviations for each VR task or experience per participant. Task success mean scores are lower in general for software-related tasks (VR1 to VR5) compared with hardware-related tasks (VRH and VRC).}
%     \label{fig:M-heatmap}
% \end{figure}

% \begin{figure}[t!]
%     \centering
%     \includegraphics[width=1\linewidth]{Figs/Heatmap.png}
%     \caption[Heatmap showing individual VR sub-task scores per participant.]{VR sub-task scores per participant.}
%     \label{fig:heatmap}
% \end{figure}

\subsubsection{VR Accessibility Barriers}

Whenever a participant scored 2 or below (Task Success) in a sub-task, a usability friction instance was logged. 
%along with the sub-task code, the participant's access needs, accessibility barrier description and, when relevant, adaptation type and description. 
A total of 157 instances of usability friction were encountered. %by the %9 
%BLV participants. %during the study.
The most common barrier %for BLV participants 
revolved around the lack of screen reader, with a 25.48\% frequency (n~=~40 out of 157 total instances). 
This barrier was closely followed by no zoom or magnification options and issues operating the VR controllers, %' buttons and components, 
each with a 17.20\% frequency (n~=~27). 
Participants were generally able to adapt to these barriers through external support (n = 142, 90.45\%), such as help provided by the facilitator, and on a few occasions through self-initiated actions (n = 9, 5.74\%). 
%Where there was no solution proposed (\textit{n} = 6, 3.82\%), which only happened for blind participants, they were unable to complete a specific sub-task at all.
%\subsubsection{VR Accessibility Adaptations}
%Due to the nature of the study and the BLV subset discussed here, most assistance was provided through verbal communication, such as voiced tutorials. 
Common adaptations included requesting further instructions to interact within the specific VR environments, with a 24.84\% frequency (n~=~39), closely followed by `mimicking' missing assistive features such as an \textit{ad-hoc} screen reader (n~=~35, 22.29\%) or \textit{ad-hoc} audio descriptions (n~=~31, 19.75\%). 
%Only blind participants were unable to complete specific sub-tasks, with no adaptation available in such cases.(\textit{n} = 6, 3.82\%).

%\subsubsection{VR Accessibility Adaptations for Blind Participants}
\subsubsection{Adaptations for Blind Participants}

Blind participants had difficulty with experiences that only provided single-modality outputs. 
This issue was particularly notable when information was only communicated by visual means, but participants also experienced some difficulty interpreting information presented in a single modality using audio or haptics.
% nonetheless, audio-based or haptic-based single-modality also confronted them with poor information regarding interaction opportunities. %within a specific experience.
Often, audio descriptions of the play space, interactable objects and the VR pointer location were necessary to help them orient themselves. %in the virtual space. 
This was most common at the beginning of the experience, and when haptic and audio signals were not enough to convey the type of the object (PF1, PF3, PF8).
%%%(PF1: \textit{VR3-2} to \textit{VR3-4, VR4-1} to \textit{VR4-}5. PF3: \textit{VR2-1, VR2-2, VR3-4, VR4-1}. PF8: \textit{VR2-2} to \textit{VR2-5, VR2-7}). 
%Audio descriptions do not require to be lengthy, for instance, PF1 noted that a short audio description of an object could help them know what they were interacting with (e.g., a “coffee cup” label in VR3). 

Audio tutorials were also helpful when friction occurred. On most occasions, tutorials guided participants on controller use, for example to explain how controllers were mapped to interactions in a specific scene, or how to manipulate interactables or control characters %%%(PF1: \textit{VR2-6, VR3-1, VR4-1} to \textit{VR4-5}. PF1, PF3: \textit{VR5-1}. PF8: \textit{VR3-1} to \textit{VR3-4}). 
(PF1, PF3, PF8).
In this regard, PF3 highlighted the need for a directional cueing system that could, for instance, guide them to move their controllers closer to the menu.
% In experiences where text was an important element and was only provided through visual means (e.g., menu text, subtitles and icon names), %in \textit{VR1} and \textit{VR2}),
% blind participants were unable to perceive this information %%%(PF1, PF3, PF8: \textit{VR1-1} to \textit{VR1-6}. PF1: \textit{VR2-1, VR2-2, VR2-7}. PF3: \textit{VR2-1, VR2-2}). 
% (PF1, PF3, PF8).
% In such cases, a screen reader would have been required. 

Audio feedback in combination with haptic feedback was another requirement identified throughout the study. When they were provided conjointly (e.g., \textit{VR3} used haptics and audio to simulate the opening of a virtual door), PF1 and PF3 could more easily perceive what was going on in the scene. When such signals were poor or did not exist, it became more difficult for participants to orient themselves %and establish how to interact within an experience 
%%%(PF1, PF3 PF8: \textit{VR1-1} to \textit{VR1-6}. PF1: \textit{VR2-1, VR2-2, VR2-6, VR2-7, VR4-1} to \textit{VR4-3}. PF3: \textit{VR4-1}. PF8: \textit{VR4-2} to \textit{VR4-5}. PF1: \textit{VR5-1, VR5-2}). %In this regard, 
(PF1, PF3 PF8).
% PF3 noted that signals closely integrated with the narrative were more effective for them (e.g., an ‘oof’ sound from the mouse character in \textit{VR4} when she walks into a non-interactable object).

% For hand-tracking-based control, blind participants encountered challenges regarding haptic feedback. The lack of VR controllers in \textit{VR5} meant that there was no tactile information. This was problematic when other modalities did not provide enough information. For instance, PF3 encountered friction when their left hand was not detected by the HMD sensors, thus the gesture was not recognised as on-target.
% There was no feedback about this error, hence participant could not conclude the experience. %%%(PF3: \textit{VR5-1, VR5-2}).

%\subsubsection{VR Accessibility Adaptations for Low-Vision Participants}
\subsubsection{Adaptations for Low-Vision Participants}

Low-vision participants such as PF5 completed more sub-tasks than the blind participants but required longer periods to familiarize themselves with the virtual environments.
% An interesting case was PF5, who took several minutes to get familiar with \textit{VR3} and \textit{VR4}. %Due to their limited vision, they moved closer to the objects and positioned the central vision of their right eye on them. Only after exploring the space in this way, 
% Once they gained a clear understanding of the experiences, prompts had already disappeared and there was no option of repeating them, leaving PF5 without clear direction on what to do next. %%%(PF5: \textit{VR3-1, VR4-2}).
Explicit audio descriptions and tutorials were important to clarify what participants were partially seeing in a scene. PF9, for instance, benefited from audio description in \textit{VR2.} %%%(PF9: \textit{VR2-1, VR2-2, VR2-6, VR2-7}). 
Detailed and repeated instructions were helpful for PF2 in \textit{VR5}. %%%(PF2: \textit{VR5-2}). 
% And PF4 noted they would have liked a tutorial on how to use the VR controllers at the beginning of \textit{VR4}, and every time they restarted it. %%% (PF4: \textit{VR4-2}). 

% For a low-vision user, magnification features could %enhance interaction and 
% avoid quick fatigue from trying to read or understand visuals. PF5 would have benefitted from larger target areas in \textit{VR1} and \textit{VR2}. %%%(PF5: \textit{VR1-2, VR1-3, VR2-3, VR2-5}). 
% PF5 either used the controllers to zoom in or drag objects closer to them when experiences allowed it. (\textit{VR1}, \textit{VR2}). %%%(PF5: \textit{VR1-3} to \textit{VR1-6}, V\textit{R2-3, VR2-5}).
% In other instances, participants would move their head closer instead %%%(PF5: \textit{VR2-1, VR2-2}. PF5, PF9: \textit{VR3-2}. PF9: \textit{VR4-1, VR4-2}). 
% (PF5, PF9: \textit{VR2, VR3, VR4}).
% This adaptation was not successful in cases when zooming-in or magnification did not apply to all the elements in the scene, such as the VR pointer; or when the objects had a fixed distance concerning the participant’s point of view %. In this case, bringing their head closer did not help them read the text 
% (PF9: \textit{VR3}).

% %As mentioned before, 
% Friction regarding text was repeatedly encountered. PF2 was able to see and interact with the menu icons in \textit{VR1}, but they could not read their text. In this case, a screen reader would have been beneficial. %%%(PF2: \textit{VR1-2} to \textit{VR1-6, VR2-1}). 
Multimodal feedback was important in some instances. For PF2, multiple signals (i.e. peripheral vision, haptics and sound) helped them to touch the interactable objects in \textit{VR3}. %%%(PF2: \textit{VR3-2, VR3-3}). 
But they struggled in \textit{VR2} because there was no haptic feedback confirming button interaction. %%%(PF2: \textit{VR2-1, VR2-6, VR2-7}) 
Similarly in \textit{VR4}, several participants encountered difficulty because there was no clear signal when an object was interactable and the colour contrast was low (PF2, PF5, PF9).
%%%PF2: \textit{VR4-1} to \textit{VR4-3}. PF5: \textit{VR4-3} to \textit{VR4-5}. PF9: \textit{VR4-3}).

% Participants constantly stressed they would have liked more granular accessibility features broken into types (e.g., vision, mobility, audio) to customise the scenes based on their individual needs (PF7), %%%(PF7: \textit{VR1-6}), 
% for instance, volume control over separate elements of the scene, %%%(PF4: \textit{VR1-6}), 
% contrast and brightness control for text (PF4) %%%(PF4: \textit{VR1-6}) 
% and objects in general (PF4, PF5, PF7), %%%(PF4: \textit{VR5-1}. PF5: \textit{VR1-2}. PF7: \textit{VR1-6}), 
% or text size (PF7). %%%(PF7: \textit{VR1-6}). 
% Another support for low-vision participants who could not identify interactables, text or virtual environments easily, would have been a guide describing these elements directing them to the following steps of interaction.

% \subsubsection{Adaptation Challenges}

% Screen reader improvised imitations were reportedly unsuccessful at different stages of the study, specifically on 28 occasions out of the 35 times it was offered to assist participants. {\color{red} [Junlong to Rosella: Are all of the 28 failure cases were due to zoom in/zoom out problems? If so, maybe this does not provide a compelling motivation for us to design natural language scene descriptions and object localization features in EnVisionVR. Shall we consider rewriting or deleting this subsubsection?]} Unsuccessful modifications regarding this adaptation took place when providing help to navigate the Meta Quest universal menu due to a lack of zooming in/out features to complement the provided support (18 out of 23 instances) and when participants tried to watch and control the 360$^{\circ}$ video (\textit{VR2}) (10 out of 12 instances) due to a similar zooming in/out problem. 
% P09 and P36 (who are blind) encountered further complications with the 360$^{\circ}$ video and, even when \textit{ad-hoc} audio descriptions were provided for them through all the \textit{VR2} sub-tasks, the assistance was not successful in allowing them to perceive the experience's key features. % at all.

\begin{figure*}[ht!]
  \includegraphics[width=0.33\textwidth]{Figs/teaser1.pdf}
  \includegraphics[width=0.33\textwidth]{Figs/teaser2.pdf}
  \includegraphics[width=0.33\textwidth]{Figs/teaser3.pdf}
  \caption{Examples of functions supported by \textsc{EnVisionVR} to enhance the accessibility of VR experiences for BLV users. Left: The user can ask ``Where am I?'' and the \textsc{EnVisionVR} system reads out a detailed description of the user's current field of view. Middle: The user can ask ``What is near me?'' and the system reads out the names of the three main objects near the user with a spatial tone to indicate the object's location. Right: The user can ask ``Where is the Brew Button?'' and the system uses a beeping sound and directional instructions to communicate the distance to the Brew Button. When the user reaches the Brew Button, the controller vibrates to inform the user.}
  % \Description{The teaser figure includes three subfigures from left to right, which provide examples of functions supported by \textsc{EnVisionVR} in the practice scene and test scene in our user study. From left to right illustrates the scene description function, the main objects function, and the object localization function. The left subfigure shows the VR practice scene of our user study, which consists of a living room with a coffee table in the middle, two armchairs to the left, and a sofa to the right. The left subfigure contains a schematic drawing of a conversation between the user and the system. The user asks ``Where am I,'' and the \textsc{EnVisionVR} system reads out a detailed description of the user's field of view. The description reads, ``The scene in front of you shows a minimalist room with a blue cup on a white table flanked by a blue chair and a brown chair, two potted plants, and a small wooden box. The background includes a white wall with a door.'' The middle subfigure shows anchor 1 of the VR test scene in our user study. The scene depicts the interior of a VR escape room, with a wooden desk to the left with a golden key, a bookholder, and an instructions paper on it. To the right are wooden shelves on the wall with a radio and several books on it. The user asks ``What is near me,'' and the system reads out the names of three main objects (``Golden Key'', ``Desk'', and ``Radio'') near the user, with each object followed by a spatial tone to indicate its location. The right subfigure shows anchor 2 of the VR test scene in our user study. It shows a different location of the interior of a VR escape room, with a bookshelf filled with books, potion bottles, and pumpkin toys to the left, and a cauldron with potion brewing inside and a control panel with dials, sliders, and a brewing button in front of the cauldron. The user asks ``Where is the Brew Button,'' and the system uses a beeping sound and directional instructions to communicate the distance to the Brew Button. When the user reaches the object, the controller vibrates to inform the user.}
  \label{fig:teaser}
\end{figure*}

\subsection{Summary}

Results from the formative study revealed that BLV users face various accessibility barriers in VR. The lack of integrated screen reader functionality and audio descriptions were identified as major barriers preventing them from completing sub-tasks without assistance. While low-vision participants required longer periods to familiarize themselves with the environments, or concrete audio descriptions to clarify the scenes, blind participants were unable to carry out specific sub-tasks in \textit{VR4} and \textit{VR5} because there was no appropriate multimodal feedback to, for instance, understand the location of objects. In contrast, \textit{VR3} offered helpful audio cues when reaching interactable objects. 

The provision of accessibility features was irregular across the experiences, consistent with the findings of Naikar et al. \cite{naikar2024accessibility}. BLV participants continually struggled to complete sub-tasks related to visual capability demands, revealing that where visual accessibility features existed (e.g., colour contrast adjustment, audio levels) these were insufficient. 
It was also observed that isolated screen reader or audio description implementations are unlikely to accommodate BLV users’ requirements. In contrast, a more complex system dedicated to providing high-level descriptions of the virtual environments coupled with detailed descriptions of interactable objects could serve as an effective guide both for blind and low-vision users facing difficulty with navigating VR experiences. 
%Video and audio quality enhancements plus features such as magnification and colour adjustment may effectively complement these features, by allowing each user to customize the accessibility features to suit their specific needs. 

% Results from the formative study revealed that BLV users face a range of accessibility barriers %when completing tasks 
% in VR experiences. However, the lack of integrated screen reader functionality and audio descriptions were identified as major barriers preventing participants from completing the tasks without assistance. Interaction issues in the most complex experiences (\textit{VR3--VR5}) were particularly salient for the BLV subset of participants. %in contrast to other groups of the sample. 
% Low-vision participants, for instance, required longer periods to familiarize themselves with the virtual environments or concrete audio descriptions to clarify the presented scenes. Blind participants, on the other hand, were unable to carry out specific sub-tasks in \textit{VR4} and \textit{VR5} because there was no appropriate feedback to understand the location of objects in the virtual environment; in contrast, \textit{VR3} offered helpful audio cues when reaching objects. The provision of accessibility features was irregular across the selected experiences for the study, consistent with the findings of Naikar et al. \cite{naikar2024accessibility}. However, BLV participants consistently struggled to complete sub-tasks related to visual capability demands, revealing that where visual accessibility features existed (e.g., colour contrast adjustment, audio levels) these were insufficient. This finding hints at the need for a development process that is informed by BLV user requirements and the broad spectrum of visual impairments and conditions. Moreover, it was observed that simplistic implementations of screen reader and audio description functionality are unlikely to sufficiently accommodate BLV users' access needs. Video and audio quality enhancements plus features such as zooming in/out and color adjustment may effectively complement these features, by allowing each user to customize the accessibility features to their specific requirements. 
% Fewer accessibility barriers occurred for VR hardware devices. For VR experiences, common barriers occurred around the lack of a screen reader feature and zooming and resizing tools for VR content, as well as difficulty in manipulating VR controller buttons.

\section{Design of \textsc{EnVisionVR}}

% The formative study suggests that a conceptual parallel to screen reader functionality for 3D content may be an effective way to enhance the accessibility of VR experiences.
The formative study suggests that BLV users require a scene interpretation functionality which builds upon the principles of a screen reader to incorporate spatial elements to assist users to navigate and interact within the 3D space. 
% , through verbal communication and audio descriptions, a VR accessibility adaptation strategy which has been found effective in the formative study
While prior work, such as \textsc{SeeingVR} \cite{zhao2019seeingvr}, has focused on features that support visual perception (e.g. zooming, contour highlighting), we pursue an alternative strategy and seek to replicate and evaluate the familiar experience of using a screen reader and listening to audio descriptions to provide visual accessibility for 3D content.
%Based on findings from the formative study, we formulate the following high-level design requirements (DRs) for \textsc{EnVisionVR}:

In the development of \textsc{EnVisionVR}, we sought to adhere to the following key design objectives.
First, we recognize that the inclusion of visual accessibility features should not necessitate the onerous manual reconfiguration of existing VR experiences.
This requirement acknowledges the current meager state of visual accessibility among existing VR applications as noted in the formative study and by Naikar et al. \cite{naikar2024accessibility}.
For a visual accessibility solution to be widely adopted by developers it should therefore not add substantially to the effort or cost of development and release.
Second, the accessibility features should be minimally disruptive to the user's enjoyment of the primary VR experience.
This objective poses certain constraints such as avoiding the remapping of controller buttons that might be used in the VR experience, or inadvertently introducing new access barriers, e.g., enforcing the use of two controllers or difficult-to-press buttons. 
Third, we seek to not only support BLV users in perceiving the virtual environment but also to interact with virtual objects in the environment.
This principle implies the need to provide information to the user at different levels of granularity, extending from high-level scene descriptions to detailed object-level information. 
% and follows the design strategy of previous works \cite{canetroller, zhao2019seeingvr, kim2020vivr, ji2022vrbubble, dang2023opportunities, collins2023guide} which convert visual information to other forms of information such as audio, speech, or haptics. 

In response to these design objectives, we developed \textsc{EnVisionVR}, a generalized visual accessibility framework which can be deployed into a VR application with minimal integration effect.
% on any static\footnote{As scene descriptions are generated prior to users executing the application (i.e. pre-baked), if the VR scene is modified during runtime the scene descriptions will not be accurate.} VR application with very low latency.
% \footnote{Data from our user study with BLV participants reveals that the time taken to provide scene descriptions upon recognizing user speech command is only within tens of milliseconds.}.
% (average delay in response time is less than 2 seconds)
This framework consists of: (i) the Scene Description Function; (ii) the Main Objects Indication Function; and (iii) the Object Localization Function.
% The rationale behind selecting these three main functions is to provide BLV users with a hierarchy of 
To minimize the need for remapping controller buttons, these functions can be activated by three simple speech commands, namely ``Where am I?'', ``What is near me?'', and ``Where is the $<$object name$>$?''. 
For the implementation evaluated in this paper, the user must first press Button A to issue a voice command but this could in theory be changed to a `wake' word or remapped to any other button.
The use of these three functions is illustrated in \Cref{fig:teaser} and their implementation is described in more detail in the remainder of this section.
% In the latest version, scene descriptions are also generated using the more-advanced GPT-4o model instead of LLaVA.
% Based on the research questions outlined 

% \hl{[To be completed]}

\subsection{Scene Description -- \textit{Where am I?}}



% \begin{figure*}[h!]
%     \centering
%     \includegraphics[width=0.75\linewidth]{Figs/scene-description-methodology.pdf}
%     \caption{Overview of the Scene Description Function. Scene description is provided in two steps. In Step 1, we obtain camera anchor positions and feed camera anchor FoV images together with a textual prompt into GPT-4o to generate pre-baked scene descriptions at various anchor points. In Step 2 during runtime, we match the current camera position and orientation with one of the anchor points to read out the pre-baked descriptions via the Microsoft text-to-speech (TTS) service.}
%     \label{fig:scene-description-methodology}
% \end{figure*}

Issuing the \textbf{``Where am I?''} voice command triggers the \textbf{Scene Description Function}, which describes the user's field of view in a few sentences. 
An overview of the implementation of the Scene Description Function before and during runtime is provided in \Cref{fig:scene-description-methodology}. Details of each step in the implementation are provided in the Online Appendix. 

\begin{figure}[ht!]
    \centering
    \includegraphics[width=\linewidth]{Figs/scene-description-methodology-simple-new.pdf}
    \caption{Overview of the Scene Description Function. Scene description is provided in two steps. In Step 1, camera anchor positions are determined by the developer or automatically by the system. Screenshots of the field of view of these anchor points with orientations of 0, 45, ..., 315 degrees along the horizontal plane together with a textual prompt are fed into GPT-4o to generate pre-baked scene descriptions. In Step 2 during runtime, we match the current camera position and orientation with the closest-matching anchor position and orientation to read out the pre-baked descriptions via the Microsoft text-to-speech (TTS) service.}
    \label{fig:scene-description-methodology}
\end{figure}

Before runtime, camera anchor points\footnote{We define \textit{anchor points} as a list of $(x, y, z)$ coordinates which define the position, but not the orientation, for the user camera to be placed in the scene, such that the user camera placed at all anchor points, with eight different orientations each, capture user field of views with all of the important objects in the scene.} are determined manually by the developer.
% or automatically via calculating the importance values of game objects in the scene through a set of heuristics, clustering objects via Gaussian Mixture Models (GMMs), and calculating the camera anchor points via trigonometry. 
As shown in \Cref{fig:anchor}, upon specifying the camera anchor points, a script is executed to automatically capture eight screenshots of the user field of view at each anchor point with orientations of 0, 45, ..., 315 degrees. These screenshots are then sent to a vision language model (VLM) together with a textual prompt, and a short scene description is obtained for each camera anchor position and preset orientation. For example, if four camera anchor positions are determined, $4\times8=32$ scene descriptions are generated. 
We used GPT-4o as the VLM and used the textual prompt ``Please describe the scene in no more than 2 sentences or around 30 words. Start the description with `The scene in front of you''' to generate the scene descriptions. The scene descriptions are stored locally in a CSV file.
As the scene descriptions are generated before users execute the application, we refer to these descriptions as \textit{`pre-baked'} \footnote{Currently, scene descriptions are generated prior to users executing the application (i.e. pre-baked). If the VR scene is modified during runtime the scene descriptions will not be accurate.} .

\begin{figure}[ht!]
    \centering
    \includegraphics[width=\linewidth]{Figs/anchor.pdf}
    \caption{Top-down view of camera anchor positions in a VR escape room and the user field of view in eight directions for each anchor point (left). At each field of view, a screenshot is taken to generate the pre-baked scene description. Example field-of-view screenshots taken at Anchor 3 are provided in the bottom images.}
    \label{fig:anchor}
\end{figure}

During runtime, the Scene Description Function processes the current user position and orientation to find the anchor point with the closest matching position and orientation and reads out its `pre-baked' scene description using the Microsoft Azure text-to-speech (TTS) service. As the scene descriptions have been generated by the VLM before runtime, this mapping process allows scene descriptions to be read out to the user with very low latency, usually within tens of milliseconds ($M=19.8, SD=19.5$) based on data from our user study.
% , given any position or orientation of the user in the scene.
% We refer to the position and orientation of such user views where scene descriptions are ``pre-baked'' as anchor points.   %of the closest anchor point. 
% These `pre-baked' descriptions are captured at several key locations and camera orientations in the scene are generated by uploading a prompt and screenshot of the field of view at the anchor position and orientation to a vision language model (VLM).
% We used GPT-4o to provide the scene descriptions using the prompt ``Please describe the scene in no more than 2 sentences or around 30 words. Start the description with "The scene in front of you"''.
% The description is read out using the Microsoft Azure text-to-speech (TTS) service to provide high-level scene information to the user.

\subsection{Main Objects Indication -- \textit{What is near me?}}

Issuing the \textbf{``What is near me?''} voice command triggers the \textbf{Main Objects Indication Function}, which announces the names of three key objects which are near the user, each followed by a short spatial tone to indicate the object's location relative to the user headset in the scene.
Exactly which objects are read out is determined by a `runtime importance value'. This value is proportional to a preset importance value,
% for all objects defined by the developer, 
and inversely related to the distance between the object and the user camera.
Here, the preset importance value for all objects can be determined automatically by \textsc{EnVisionVR} based on the presence of rendering components,
% attached to the GameObject in Unity, 
or specified manually by the developer.
% Here, the preset importance value for all objects is suggested by \textsc{EnVisionVR} based on the presence of rendering components attached to the GameObject in Unity, and the developer has the option to make minor edits to the suggested preset importance value.
If an object has been previously announced, its runtime importance value is reduced to allow other objects to be announced in subsequent activations of the function. Details on how the importance values are determined can be found in the Online Appendix.

\subsection{Object Localization -- \textit{Where is the <object name>?}}

Issuing the \textbf{``Where is the $<$object name$>$?''} voice command triggers the \textbf{Object Localization Function}, which starts a beeping sound with the beeping frequency inversely proportional to the distance between the right controller and the object. Directional and distance information (e.g., `1 meter ahead') is also provided at regular time intervals to guide the user. When the controller is close enough to the object to interact with it, the controller vibrates.
If the object is interactable and can be held, the system will also announce ``holding $<$object name$>$'' when it is picked up. More implementation details are provided in the Online Appendix.

\section{Evaluation Study}\label{sec:study-design}

To evaluate the potential benefits of \textsc{EnVisionVR}, we conducted a user study with 12 BLV participants.
Participants completed three types of prescribed tasks in VR both with \textsc{EnVisionVR} and without.
The without condition represented the default experience available to BLV users with no dedicated visual accessibility features.
The study was approved by the research ethics committee in the Department of Engineering at the University of Cambridge.
% $<$\textit{anonymized for review}$>$.
%based on the results from the study with 12 blindfolded sighted participants in the previous version of \textsc{EnVisionVR}. 
% During the study, participants are asked to sit on a swivel chair and explore a practice scene and a test scene and answer a question related to scene understanding and complete a task on object localization and another task on object interaction.
% \paragraph{Study Setup.} 

\subsection{Method}

A within-subjects design was adopted to evaluate the performance of \textsc{EnVisionVR} (abbreviated in \Cref{sec:results} as \textsc{EVR}) and the no accessibility features condition (abbreviated in \Cref{sec:results} as \textsc{NVR}).
% in three types of tasks under different scenes for BLV users. 
The order of conditions was counterbalanced. %over all 12 participants. 
For each condition, participants were first familiarized with the available functions in a practice scene (see \Cref{fig:teaser} left image).
Participants were encouraged to use all available functions and the experimenter gave examples of the types of tasks they would be asked to complete. % answer a sample scene understanding question and complete the sample object localization and object interaction task. 
After completing familiarization in the practice scene, participants were transported to the test scene. 

% The practice scene was the same in both conditions. 
The test scene, a VR Escape Room~\cite{vr-escape-room}, was chosen for several key reasons.
First, it is a tutorial scene made freely available by Unity and so provides an example of the type of existing VR experience that may require retrofitting of accessibility features.
Second, it contains rich objects and scene elements.
Third, it supports different interactions with virtual objects (such as grabbing objects and pressing buttons). %, and is not a typical scene in daily life (such as a living room or a factory setting).  
We define two study anchor points in this scene (see \Cref{fig:teaser} middle and right image) and the combination of condition and anchor point is balanced across participants. 
% and are placed at either Anchor 1 (see \Cref{fig:teaser} middle image) or Anchor 2 (see \Cref{fig:teaser} right image) to explore the scene. 
At the given study anchor point, participants then completed the tasks described in the following subsection.

\subsection{Tasks}\label{sec:tasks}

The degree to which \textsc{EnVisionVR} supports BLV users in perceiving and interacting with the virtual environment is evaluated across the three tasks summarized below.
% According to the concept of VR \textit{Content Accessibility} proposed by Mott et al. \cite{mott2019accessible} and previous works which focused on improving the ability of BLV users to understand and navigate virtual scenes \cite{canetroller}, we designed the scene understanding task and the object localization task to evaluate the accessibility of virtual content for BLV users. Following the concept of \textit{Interaction Accessibility} \cite{mott2019accessible}, we designed the object interaction task to evaluate the interaction accessibility of virtual elements in the scene for BLV users.
% TODO: some preamble outlining split into three tasks to evaluate three functions
A complete list of questions and tasks for the two anchor points in the test scene is provided in the Online Appendix.

\vspace{0.3em}

\begin{enumerate}
    \item \textbf{Scene Understanding Task:} Participants are asked to rate a statement to evaluate their understanding of the scene from 1 (very unlikely to be true) to 5 (very likely to be true). For example, at Anchor 1 (see \Cref{fig:teaser} middle image), participants are asked to judge whether the statement ``This is a scene of a classroom with a desk and a chair'' is likely to be true or not. 
    % \vspace{0.5em}
% Participants are also asked to rate from a scale of 1 to 5 how confident do they feel about their rating for the first question. 
    \item \textbf{Object Localization Task:} Participants are asked to turn to face a specified object in the scene. For example, they are asked to turn to face the radio at Anchor 1. The task completion status was recorded as a yes/no binary value. %, and the task completion time was also recorded to distinguish the performance of participants who completed the task. 
    % \vspace{0.5em}
    \item \textbf{Object Interaction Task:} Finally, participants are asked to interact with an object in the scene. For example, they are asked to push the ``Brew Button'' at Anchor 2 (see \Cref{fig:teaser} right image). Again, we record their task completion status as a binary value. % and the task completion time. Throughout the study, notes were taken to record any comments about the user experience when completing different tasks. 
% Appendix \ref{sec:appendix-evr-tasks}.
\end{enumerate}

\vspace{0.3em}

For each task, participants also rated the difficulty they encountered in completing the task on a scale from 1 to 5.



\subsection{Participants} 
We recruited a new participant sample with the assistance of Open Inclusion \cite{OpenInclusion}. All participants provided informed consent. The sample consisted of 12 participants, of which three reported being blind and nine reported having low vision. All three blind participants reported regular use of screen readers or other forms of assistive technology. Among the nine participants who reported having low vision, five participants reported regular use of assistive technology, yielding a total of eight participants who regularly use assistive technology. %and 4 participants who do not. 
\Cref{tab:demographics} provides a summary of the collected demographic information of all 12 participants. To differentiate from the formative study, participants are labeled as P1 to P12.

\subsection{Apparatus}
During the experiment, participants wore a Meta Quest 3 headset and held the right controller.
Participants completed all tasks while remaining seated in a swivel chair.
The headset was connected to a Windows 11 laptop 
% PC (Intel i5-9300H CPU, 16GB memory, and GTX 1050 graphics card) via cable. 
in wired `link' mode.
% Virtual scenes were simulated by running Unity 3D executable files, which were built using Unity 3D (Version 2021.3.40f1) and publicly available resources on the Unity Asset Store.

\begin{table*}[ht!]
    % \centering
    \caption{Participant demographics for the evaluative study with BLV users.}
    \label{tab:demographics}
    % \Description{The table shows participant demographics for the EnVisionVR empirical study with 12 BLV users. P1, P3, and P5 reported being blind while the remaining 9 participants reported having low vision. P1, P3, P4, P5, P6, P8, P10, and P12 reported regular use of assistive technology while the remaining 4 participants reported irregular use of assistive technology.}
    \renewcommand{\arraystretch}{1.5}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{p{0.1\textwidth}>{\centering}p{0.028\textwidth}>{\centering}p{0.06\textwidth}>{\centering}p{0.09\textwidth}>{\centering}p{0.12\textwidth}>{\centering}p{0.06\textwidth}>{\centering}p{0.06\textwidth}>{\centering}p{0.24\textwidth}>{\centering}p{0.24\textwidth}>{\arraybackslash}p{0.06\textwidth}}
    \hline
    \textbf{Participant} & \textbf{Age} & \textbf{Gender} & \textbf{Education}               & \textbf{VR Experience}                         & \textbf{Vision}     & \textbf{From Birth} & \textbf{Vision Description}                                                                                              & \textbf{Assistive Technology}                                                                                               & \textbf{Regular Use} \\ \hline
    P1              & 58  & Female & Masters                 & Inexperienced                         & Blind      & No         & Sighted in the past but have no usable vision today.                                                            & Voiceover and Jaws as screen readers; Other tech with audio assistance at home.                                    & Yes         \\
    P2              & 21  & Female & A levels                & Inexperienced                         & Low Vision & Yes        & Born with cataracts and glaucoma, able to see a decent amount with glasses.                                     & Uses phone and screen magnifiers to zoom in.                                                                       & No          \\
    P3              & 73  & Male   & GCSE                    & Highly Inexperienced                  & Blind      & No         & Lost sight gradually, totally blind for the past 2 years.                                                       & Uses screen reading software: JAWS, NVDA, Voiceover.                                                               & Yes         \\
    P4              & 50  & Female & Higher National Diploma & Inexperienced                         & Low Vision & No         & Stargardt's which affects central vision.                                                                       & Uses Voice Over and Zoom Text.                                                                                     & Yes         \\
    P5              & 79  & Male   & GCSE                    & Highly Inexperienced*                 & Blind      & No         & Lost sight during a degree course.                                                                              & Siri, Alexa, Be My Eyes, JAWS, Voiceover, and other screen readers.                                                 & Yes         \\
    P6              & 36  & Male   & College                 & Neither inexperienced nor experienced & Low Vision & N/A        & Can see 1 meter ahead, central vision in one eye only.                                                          & Phone has voice over - Apple iPhone. Windows PC, Samsung tablet. Talking TV. Uses Seeing AI - to read bus numbers. & Yes         \\
    P7              & 58  & Male   & Postgraduate degree     & Inexperienced                         & Low Vision & N/A        & No sight in left eye, limited central vision (3/60) in right eye. Has ADHD.                                     & Screen magnification user on the computer                                                                          & No          \\
    P8              & 36  & Female & AS Level                & Highly Inexperienced                  & Low Vision & N/A        & Has light perception and no residual vision. Difficulties in reading if the text is not in the right format.    & Uses screen reader on a daily basis. NVDA on laptop. Talkback on Android. Previously iPhone.                       & Yes         \\
    P9              & 41  & Male   & Bachelor's degree       & Highly Experienced                    & Low Vision & N/A        & Little vision in right eye, can see light and dark and the shape of things.                                     & Text enlarger on mobile and computer, and Dragon Naturally Speaking for speech input and feedback                  & No          \\
    P10             & 45  & Male   & Bachelor's degree       & Highly Inexperienced                  & Low Vision & N/A        & Zero sight in left eye, right eye is a prosthetic, 6/36 vision with changing field. Only sees shape and colour. & Has used lots of tech. Doesn't use an actual screen reader. Has reading glasses and uses audiobooks a lot.        & Yes         \\
    P11             & 54  & Female & Bachelor's degree       & Highly Inexperienced                  & Low Vision & N/A        & Sight in right eye, no sight in left eye. Born with cataracts. Can read some print with glasses.                & Does not use audio on the computer. Does not use specific software. Can enlarge print.                             & No          \\
    P12             & 57  & Female & Entry level 2 English   & Highly Inexperienced                  & Low Vision & N/A        & No central vision and a tiny bit of peripheral vision. Can see light, dark, and some outlines.                  & NVDA, Alexa in the house, Android mobile with Synaptec for screen reader.                                          & Yes         \\ \hline
    \end{tabular}
    }
    \vspace{1ex}
    
    {\raggedright \footnotesize  * P5 was new to the concept of VR. He connected VR with soundscapes and gave himself a VR experience rating of `Neither inexperienced nor experienced'. He also said that he had never used technology of this kind later in the testing session, suggesting that an accurate rating could have been `Highly inexperienced'. \par}
\end{table*}




\section{Results}\label{sec:results}

In this section, we first present the study results for each task outlined in Section~\ref{sec:tasks}.
Later in Subsections~\ref{ssec:behaviors} and \ref{ssec:interview} we report on observations of usage behavior with EnVisionVR, as well as qualitative feedback captured in the post-study interview.

% \subsection{Demographics} \Cref{tab:demographics} presents the demographic data of all 12 blind and visually impaired participants (6 male and 6 female). Among all participants, 3 participants reported to be completely blind and 9 participants reported having low vision. All 3 blind participants reported regular use of screen readers or other forms of assistive technology, and another 5 participants with low vision also reported regular use of assistive technology. As participants' vision level can still vary significantly under the low vision category, we use whether or not participants regularly use assistive technology as an indicator of vision level. 

\subsection{Scene Understanding Task}

\begin{figure}[ht!]
    \centering
    % \includegraphics[height=6.95cm]{Figs/parallel - su score by regular use of assistive tech.pdf}
    \includegraphics[height=6.95cm]{Figs/parallel-su-score-by-regular-use-of-assistive-tech.pdf}
    % \includegraphics[height=6.95cm]{Figs/parallel - su score difference.pdf}
    \includegraphics[height=6.95cm]{Figs/parallel-su-score-difference.pdf}
    \caption[Parallel coordinate plot of the scene understanding task results.]{Scene Understanding Task: Performance of all participants (left) and the difference between scores in the \textsc{EVR} and \textsc{NVR} condition for each participant (right). Participants with blindness and severe visual impairment who regularly use assistive technology are colored in black, while others are colored in grey. Vertical jittering is applied to visualize all points. Participant IDs are labelled beside each scatter point.}
    \label{fig:score-su-parallel}
    % \Description{Left: Parallel coordinate plot of the scene understanding score for participants under the NVR and EVR conditions. Participants P3 (NVR: 1/5, EVR: 4/5), P4 (NVR: 3/5, EVR: 5/5), P5 (NVR: 1/5, EVR: 4/5), and P12 (NVR: 4/5, EVR: 5/5) obtained a higher scene understanding score in the EVR condition compared with the NVR condition. Participants P7 (NVR: 3/5, EVR: 3/5), P8 (NVR: 5/5, EVR: 5/5), P9 (NVR: 5/5, EVR: 5/5), and P11 (NVR: 4/5, EVR: 4/5) obtained a same scene understanding score in the NVR and EVR conditions. Participants P1 (NVR: 5/5, EVR: 4/5), P2 (NVR: 4/5, EVR: 3/5), P6 (NVR: 4/5, EVR: 3/5), and P10 (NVR: 5/5, EVR: 4/5) obtained a higher scene understanding score in the NVR condition compared with the EVR condition. Right: Scatterplot of the difference between the scene understanding score under the EVR and NVR conditions for all participants. P3 and P5 had a score of 3 increase with EVR compared with NVR. Similarly, P4 had a score of 2 increase, P12 had a score of 1 increase, P7, P8, P9, P11 had a same score under the NVR and EVR conditions, and P1, P2, P6, and P10 had a score of 1 decrease with EVR compared with NVR.}
\end{figure}

\begin{figure}[ht!]
    % \centering
    \hspace{-0.1in}
    % \includegraphics[width=0.5\linewidth]{Figs/su descriptives - difficulty unsplit.pdf}
    % \includegraphics[width=\linewidth]{Figs/su descriptives - difficulty.pdf}
    \includegraphics[width=\linewidth]{Figs/su-descriptives-difficulty.pdf}
    \caption[Perceived difficulty of the scene understanding task.]{Scene Understanding Task: Distribution of the perceived difficulty (higher score indicates lower perceived difficulty) for the \textsc{NVR} and \textsc{EVR} conditions for participants who regularly use assistive technology and for those who do not. Black squares indicate the mean value.}
    \label{fig:score-su-difficulty-box}
    % \Description{Left: Boxplots of perceived difficulty ratings of the scene understanding question for the NVR and EVR conditions. Mean and standard deviation values are provided in the main text. The interquartile range is around 1 for the NVR condition and 2 for the EVR condition. The median difficulty rating is around 1.5 for NVR and around 3.5 for EVR. Right: Boxplots of perceived difficulty ratings of the scene understanding question grouped by regular and irregular use of assistive technology for the NVR and EVR conditions. Mean and standard deviation values are provided in the main text. For participants who regularly use assistive technology, the interquartile range is around 1 for the NVR condition and 2 for the EVR condition. The median difficulty rating is around 1 for NVR and around 3.5 for EVR. For participants who irregularly use assistive technology, the interquartile range is around 1 for the NVR condition and 1 for the EVR condition. The median difficulty rating is around 2 for NVR and around 3.5 for EVR.}
\end{figure}

In the Scene Understanding Task, participants responded to the given statement on a scale from 1--``very unlikely" to 5--``very likely".
Since at one anchor location, the statement was false, we converted these raw responses such that a higher score indicates a closer match to the correct answer.
\Cref{fig:score-su-parallel} plots the scene understanding score of all participants in the \textsc{NVR} ($M=3.67, SD=1.44$) and \textsc{EVR} ($M=4.08, SD=.793$) conditions.
% Compared with the \textsc{NVR} condition, average scene understanding scores increased by $11.2\%$ with \textsc{EVR}. 
In \Cref{fig:score-su-parallel} we also make a distinction between whether participants regularly use screen readers or other assistive technology. This roughly groups the full participant group into two subsets based on the degree to which they can directly perceive visual content. %provides a more objective measure of vision capability. %, we color participants with blindness or severe visual impairment in black and others in grey. 
P3, P4, P5 and P12 who regularly use assistive technology gained a better understanding of the scene with \textsc{EnVisionVR} compared with the condition without any accessibility features. P1, P2, P6, and P10 were able to understand the scene better without \textsc{EnVisionVR}, while P7, P8, P9, and P11 achieved the same level of scene understanding with and without the tool. The decrease in scene understanding performance was due to different reasons such as a lack of attention to long descriptions and failure to capture keywords to support user judgment (P1), or the lack of evidence to convince them to negate the statement which claims the escape room is a classroom (P2, P6). The first-person view descriptions provided only fragments of information about objects around the user, which was insufficient to infer high-level information such as the type and purpose of the scene (P10).

A Friedman test did not indicate a significant difference in scene understanding scores ($\chi^2<.01, p=1.0$) 
% , or confidence ratings ($\chi^2<0.01, p=1.0$) 
between the \textsc{NVR} and \textsc{EVR} conditions.
Friedman tests also did not indicate a significant difference in scene understanding scores between the \textsc{NVR} or \textsc{EVR} conditions for participants who regularly use assistive technology ($\chi^2=.143, p=.705$) or for those who do not ($\chi^2=1.0, p=.317$).

\Cref{fig:score-su-difficulty-box} presents boxplots of the 
% scene understanding scores in \Cref{fig:score-su-parallel}, 
perceived difficulty ratings (higher score indicates lower perceived difficulty) of the scene understanding question for the \textsc{NVR} ($M=2.00, SD=1.35$) and \textsc{EVR} condition ($M=3.25, SD=1.29$). 
The difficulty ratings are grouped for participants who regularly use assistive technology (\textsc{NVR}: $M=1.75, SD=1.16$; \textsc{EVR}: $M=3.25, SD=1.49$) and participants who do not (\textsc{NVR}: $M=2.50, SD=1.73$; \textsc{EVR}: $M=3.25, SD=.957$).
% , as well as their confidence in their response to the scene understanding question for the \textsc{NVR} ($M=3.17, SD=.1.53$) and \textsc{EVR} ($M=3.67, SD=.78$) condition. 
A Friedman test did not indicate a significant difference in perceived difficulty ($\chi^2=3.60, p=.058$)
% , or confidence ratings ($\chi^2<0.01, p=1.0$) 
between the \textsc{NVR} and \textsc{EVR} conditions,
% Friedman tests did not indicate a significant difference in scene understanding scores between the \textsc{NVR} or \textsc{EVR} conditions for participants who regularly use assistive technology ($\chi^2=.143, p=.705$) or for those who do not regularly use assistive technology ($\chi^2=1.0, p=.317$).
% While the mean scene understanding score was slightly higher in the \textsc{EVR} condition, and the mean confidence rating and difficulty score were higher in the \textsc{EVR} condition, the difference is not large enough to be statistically significant.
% Friedman tests did not indicate 
or a significant difference in perceived difficulty between the \textsc{NVR} and \textsc{EVR} conditions for participants who regularly use assistive technology ($\chi^2=3.57, p=.059$) or those who do not ($\chi^2=.333, p=.564$).

\subsection{Object Localization Task}

\begin{figure}[t!]
    \centering
    % \includegraphics[height=6.95cm]{Figs/parallel - ol score by regular use of assistive tech.pdf}
    % \includegraphics[height=6.95cm]{Figs/parallel - ol score difference.pdf}
    \includegraphics[height=6.95cm]{Figs/parallel-ol-score-by-regular-use-of-assistive-tech.pdf}
    \includegraphics[height=6.95cm]{Figs/parallel-ol-score-difference.pdf}
    \caption[Parallel coordinate plot of the object localization task performance.]{Object Localization Task: Performance of all participants (S: Successful, N: Not successful) (left) and the difference between task outcomes in the \textsc{EVR} and \textsc{NVR} condition for each participant (right). Participants with blindness and severe visual impairment who regularly use assistive technology are colored in black, while others are colored in grey. Vertical jittering is applied to visualize all points. Participant IDs are labelled beside each scatter point.}
    \label{fig:score-ol-parallel}
    % \Description{Left: Parallel coordinate plot of the object localization task outcome for participants under the NVR and EVR conditions. Participants P3 (NVR: Not Successful, EVR: Successful), P5 (NVR: Not Successful, EVR: Successful), P8 (NVR: Not Successful, EVR: Successful), P11 (NVR: Not Successful, EVR: Successful), and P12 (NVR: Not Successful, EVR: Successful) obtained a better object localization task outcome in the EVR condition compared with the NVR condition. Participants P1 (NVR: Not Successful, EVR: Not Successful), P2 (NVR: Successful, EVR: Successful), P4 (NVR: Successful, EVR: Successful), P6 (NVR: Successful, EVR: Successful), P7 (NVR: Successful, EVR: Successful), P9 (NVR: Successful, EVR: Successful), and P10 (NVR: Successful, EVR: Successful) obtained a same object localization task outcome in the NVR and EVR conditions. None of the participants obtained a better object localization task outcome in the NVR condition compared with the EVR condition. Right: Scatterplot of the difference between the object localization task outcome under the EVR and NVR conditions for all participants. P3, P5, P8, P11, P12 performed better in the object localization task with EVR compared with NVR. P1, P2, P4, P6, P7, P9, P10 had a same object localization task performance under the NVR and EVR conditions.}
\end{figure}

\begin{figure}[ht!]
    \centering
    \hspace{-0.1in}
    % \includegraphics[width=0.5\linewidth]{Figs/ol descriptives - difficulty unsplit.pdf}
    % \includegraphics[width=\linewidth]{Figs/ol descriptives - difficulty.pdf}
    \includegraphics[width=\linewidth]{Figs/ol-descriptives-difficulty.pdf}
    \caption[Perceived difficulty of the object localization task.]{Object Localization Task: Distribution of the perceived difficulty (higher score indicates lower perceived difficulty) for the \textsc{NVR} and \textsc{EVR} conditions for participants who regularly use assistive technology and for those who do not. Black squares indicate the mean value.}
    \label{fig:score-ol-difficulty-box}
    % \Description{Left: Boxplots of perceived difficulty ratings of the object localization task for the NVR and EVR conditions. Mean and standard deviation values are provided in the main text. The interquartile range is around 2 for the NVR condition and 2 for the EVR condition. The median difficulty rating is around 2.5 for NVR and around 4 for EVR. Right: Boxplots of perceived difficulty ratings of the object localization task grouped by regular and irregular use of assistive technology for the NVR and EVR conditions. Mean and standard deviation values are provided in the main text. For participants who regularly use assistive technology, the interquartile range is around 2 for the NVR condition and 2 for the EVR condition. The median difficulty rating is around 2.5 for NVR and around 4 for EVR. For participants who irregularly use assistive technology, the interquartile range is around 2.5 for the NVR condition and 1 for the EVR condition. The median difficulty rating is around 2 for NVR and around 4.5 for EVR.}
\end{figure}

\Cref{fig:score-ol-parallel} summarizes the completion status of the object localization task for all participants. 
Six participants were able to complete the object localization task to turn to face a specified object in the NVR condition, while the other six participants were not. In the \textsc{EVR} condition, five of the participants who were unable to complete the task in the \textsc{NVR} condition were able to complete the object localization task.
Only one participant (P1) was still unable to complete the task in the \textsc{EVR} condition, and none of the participants had a worse performance with \textsc{EnVisionVR}. Most participants (3 out of 4) who do not regularly use assistive technology were able to complete the object localization task with or without \textsc{EnVisionVR}, and \textsc{EnVisionVR} was able to help four out of five participants who do regularly use assistive technology, and who could not locate the virtual object in the \textsc{NVR} condition to complete the task. Overall, object localization task completion results show a $91.7\%-50\%=41.7\%$ improvement in task success rate with \textsc{EVR} compared with \textsc{NVR}.
As the object localization task has binary performance data, a McNemar's test was adopted. The test indicated a significant difference ($\chi^2=5.0, p<.05$) between the \textsc{NVR} and \textsc{EVR} task completion status, suggesting that \textsc{EnVisionVR} significantly improved participants' ability to locate virtual objects and significantly reduced their perceived difficulty. A McNemar's test also indicated a significant difference ($\chi^2=4.0, p<.05$) between the \textsc{NVR} and \textsc{EVR} task completion status for participants who regularly use assistive technology, but the difference was not significant ($\chi^2=1.0, p=.317$) for those who do not regularly use assistive technology.

\Cref{fig:score-ol-difficulty-box} presents box plots of
% the distribution of 
% the outcomes of the object localization task for the \textsc{NVR} ($M=3.00, SD=2.09$) and \textsc{EVR} ($M=4.67, SD=1.15$) conditions and 
the perceived difficulty of the task for the \textsc{NVR} ($M=2.42, SD=1.51$) and \textsc{EVR} ($M=3.83, SD=1.34$) conditions.
% The plots are drawn to the same scale by converting the binary outcome values (Completed / Uncompleted) to scores of 5 and 1 which correspond to the maximum and minimum possible scores of the difficulty rating. 
% Friedman tests indicated a significant difference in object localization scores ($\chi^2=5.00, p<.05$) and perceived difficulty ($\chi^2=4.45, p<.05$) between the \textsc{NVR} and \textsc{EVR} conditions. 
A Friedman's test indicated a significant difference ($\chi^2=4.45, p<.05$) between the \textsc{NVR} and \textsc{EVR} condition for all participants, but this difference was not significant at the subgroup level, i.e. participants who regularly use assistive technology ($\chi^2=3.57, p=.059$) and those who do not ($\chi^2=1.0, p=.317$).

\subsection{Object Interaction Task}

\Cref{fig:score-oi-parallel} shows the completion status of the object interaction task. Six participants were able to interact with a virtual object (such as picking up a key or pressing a button) under the \textsc{NVR} condition, while the other six were not. Among those who were unable to interact with virtual objects, five participants were able to complete the task with \textsc{EnVisionVR}, while one participant (P12) was still unable to complete the task.
It is worth noting that P12 reported a secondary access need based on her learning disability, and this may have contributed to the difficulty they experienced in completing the task. 
Among the six participants who were able to complete the interaction task under the \textsc{NVR} condition, five were still able to complete the task with \textsc{EnVisionVR}. However, P9 with little remaining vision was not able to complete the task with \textsc{EnVisionVR} as he felt the main objects indication function provided conflicting information by reporting an object directly behind him, which could not be confirmed easily using vision. Most participants of the subgroup who regularly use assistive technology (6 out of 8) were not able to complete the interaction task in the NVR condition, and \textsc{EnVisionVR} was able to support five out of these six participants to complete the interaction task. Overall, object interaction task completion results show a $83.3\%-50\%=33.3\%$ improvement in task success rate with \textsc{EVR} compared with \textsc{NVR}.
A McNemar's test did not indicate a significant difference ($\chi^2=2.67, p=.102$) between the \textsc{NVR} and \textsc{EVR} object interaction task completion status.
% While the mean object interaction score and perceived difficulty rating were higher in the \textsc{EVR} condition, the difference is not large enough to be statistically significant. 
For the subgroup of participants who regularly use assistive technology, a McNemar's test indicated a significant difference ($\chi^2=5.0, p<.05$) between the \textsc{NVR} and \textsc{EVR} object interaction task completion status, but did not reveal a significant difference ($\chi^2=1.0, p=.317$) for participants who do not regularly use assistive technology.

\begin{figure}[ht!]
    \centering
    % \includegraphics[height=6.95cm]{Figs/parallel - oi score by regular use of assistive tech.pdf}
    % \includegraphics[height=6.95cm]{Figs/parallel - oi score difference.pdf}
    \includegraphics[height=6.95cm]{Figs/parallel-oi-score-by-regular-use-of-assistive-tech.pdf}
    \includegraphics[height=6.95cm]{Figs/parallel-oi-score-difference.pdf}
    \caption[Parallel coordinate plot of the object interaction task performance.]{Object Interaction Task: Performance of all participants (S: Successful, N: Not successful) (left) and the difference between task outcomes in the \textsc{EVR} and \textsc{NVR} condition for each participant (right). Participants with blindness and severe visual impairment who regularly use assistive technology are colored in black, while others are colored in grey. Vertical jittering is applied to visualize all points. Participant IDs are labelled beside each scatter point.}
    \label{fig:score-oi-parallel}
    % \Description{Left: Parallel coordinate plot of the object interaction task outcome for participants under the NVR and EVR conditions. Participants P1 (NVR: Not Successful, EVR: Successful), P3 (NVR: Not Successful, EVR: Successful), P5 (NVR: Not Successful, EVR: Successful), P6 (NVR: Not Successful, EVR: Successful), and P8 (NVR: Not Successful, EVR: Successful) obtained a better object interaction task outcome in the EVR condition compared with the NVR condition. Participants P2 (NVR: Successful, EVR: Successful), P4 (NVR: Successful, EVR: Successful), P7 (NVR: Successful, EVR: Successful), P10 (NVR: Successful, EVR: Successful), P11 (NVR: Successful, EVR: Successful), and P12 (NVR: Not Successful, EVR: Not Successful) obtained a same object interaction task outcome in the NVR and EVR conditions. Participant P9 (NVR: Successful, EVR: Not Successful) obtained a better object interaction task outcome in the NVR condition compared with the EVR condition. Right: Scatterplot of the difference between the object interaction task outcome under the EVR and NVR conditions for all participants. P1, P3, P5, P6, P8 performed better in the object interaction task with EVR compared with NVR. P2, P4, P7, P10, P11, and P12 had a same object interaction task performance under the NVR and EVR conditions. P9 performed better in the object interaction task with NVR compared with EVR.}
\end{figure}

\begin{figure}[ht!]
    \centering
    \hspace{-0.1in}
    % \includegraphics[width=0.5\linewidth]{Figs/oi descriptives - difficulty unsplit.pdf}
    % \includegraphics[width=\linewidth]{Figs/oi descriptives - difficulty.pdf}
    \includegraphics[width=\linewidth]{Figs/oi-descriptives-difficulty.pdf}
    \caption[Perceived difficulty of the object interaction task.]{Object Interaction Task: Distribution of the perceived difficulty (higher score indicates lower perceived difficulty) for the \textsc{NVR} and \textsc{EVR} conditions for participants who regularly use assistive technology and for those who do not. Black squares indicate the mean value.}
    \label{fig:score-oi-difficulty-box}
    % \Description{Left: Boxplots of perceived difficulty ratings of the object interaction task for the NVR and EVR conditions. Mean and standard deviation values are provided in the main text. The interquartile range is around 3 for the NVR condition and 1 for the EVR condition. The median difficulty rating is around 2.5 for NVR and around 4 for EVR. Right: Boxplots of perceived difficulty ratings of the object interaction task grouped by regular and irregular use of assistive technology for the NVR and EVR conditions. Mean and standard deviation values are provided in the main text. For participants who regularly use assistive technology, the interquartile range is around 2 for the NVR condition and 0.5 for the EVR condition. The median difficulty rating is around 4 for NVR and around 4 for EVR. For participants who irregularly use assistive technology, the interquartile range is around 2.5 for the NVR condition and 2 for the EVR condition. The median difficulty rating is around 2 for NVR and around 3.5 for EVR.}
\end{figure}

\Cref{fig:score-oi-difficulty-box} presents box plots of
% the distribution of
% the outcomes of the object interaction task for the \textsc{NVR} ($M=3.00, SD=2.09$) and \textsc{EVR} ($M=4.33, SD=1.56$) conditions and 
the perceived difficulty of the task for the \textsc{NVR} ($M=2.92, SD=1.68$) and \textsc{EVR} ($M=3.42, SD=1.44$) conditions. 
% Similar to the object localization tasks, the plots are drawn to the same scale by converting the binary outcome values (Completed / Uncompleted) to scores of 5 and 1. 
% In terms of perceived difficulty, 
Friedman tests did not reveal a significant difference ($\chi^2=2.78, p=.096$) between the \textsc{NVR} and \textsc{EVR} conditions for all participants, or for participants who do not regularly use assistive technology ($\chi^2<.01, p=1.0$), but revealed a significant difference for participants who regularly use assistive technology ($\chi^2=5.0, p<.05$).

\subsection{Interaction Behaviors}
\label{ssec:behaviors}

% Plot of number of function activations
\Cref{fig:function-activations} plots the distribution of the number of \textsc{EnVisionVR} function activations for participants in the user study. The results show that the main objects indication function was activated a similar number of times
% (around twice in the test scene for each participant) 
for participants who regularly use assistive technology ($M=2.00, SD=1.20$) and for participants who do not ($M=2.25, SD=2.22$). However, participants who regularly use assistive technology activated the scene description function more ($M=3.25, SD=3.01$) than participants who do not regularly use assistive technology ($M=1.25, SD=.500$). While a Mann-Whitney U test did not reveal a statistically significant difference ($U=6.50, p=.106$) in the number of scene description function activations, the rank-biserial correlation effect size was moderate ($r_{rb}=.594$), suggesting that there may be a practical difference between both groups. The object localization function was also activated more by participants who regularly use assistive technology ($M=2.50, SD=1.77$) compared with those who do not ($M=1.25, SD=1.26$), but the Mann-Whitney U test did not reveal a statistically significant difference ($U=8.50, p=.216, r_{rb}=.469$). This suggests that participants with less visual perception capability tend to rely more on high-level scene descriptions and the fine detail object localization function. Meanwhile, participants with different vision capabilities relied on the Main Objects Indication Function at a similar level.
    
For participants who do not regularly use assistive technology, \textsc{EnVisionVR} appeared to complement their available vision. These participants used the scene description function less as they have enough residual vision to support their understanding of the scene, as evidenced by the performance of P2, P7, P9, and P11 in the scene understanding question in the \textsc{NVR} condition.
% (all were able to obtain a minimum score of 3 out of 5).
% They also relied less on object localization as they could locate the precise location of small virtual objects (P2, P7, P9, and P11 were all able to complete the object interaction task even under the \textsc{NVR} condition). 
They were also able to precisely locate small virtual objects as evidenced by successful completion of the object interaction task under the \textsc{NVR} condition.
These participants used the main objects indication function more, likely because their residual vision does not allow them to explore a wide range in the scene, and they rely on the function to know what key objects are nearby.
% in order to find them easily.  
% This corroborates our observations in the study where P2, P7, P9, and P11 who do not regularly use assistive technology were able to obtain a minimum score of 3 out of 5 in the scene understanding question and complete the object localization task and the 
Nevertheless, as \Cref{fig:function-activations-individual} indicates, the interaction behavior observations in \Cref{fig:function-activations} only describe general trends, and the usage of different functions for each participant can be vastly different. 

\begin{figure}[ht!]
    \centering
    \includegraphics[width=\linewidth]{Figs/function-activations.pdf}
    \caption[Distribution of the number of \textsc{EnVisionVR} function activations.]{Distribution of the number of \textsc{EnVisionVR} function activations for participants who regularly use assistive technology and for those who do not with labeled outliers. Black squares indicate the mean value.}
    \label{fig:function-activations}
    % \Description{Boxplots of number of scene description, main object, and object localization function activations for participants who regularly and irregularly use assistive technology. For the scene description function, the interquartile range is around 1.5 for 'Regular Use' and 0.5 for 'Irregular Use'. The median is around 2 for 'Regular Use' and 1 for 'Irregular Use'. For the main objects function, the interquartile range is around 2 for 'Regular Use' and 2.5 for 'Irregular Use'. The median is around 2.5 for 'Regular Use' and 2 for 'Irregular Use'. For the object localization function, the interquartile range is around 2 for 'Regular Use' and 0.5 for 'Irregular Use'. The median is around 2 for 'Regular Use' and 1 for 'Irregular Use'.}
\end{figure}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=\linewidth]{Figs/function-activations-individual.pdf}
    \caption[Total number of \textsc{EnVisionVR} function activations for each participant.]{Total number of \textsc{EnVisionVR} function activations for each participant for the scene understanding question, object localization task, and object interaction task. P2, P7, P9, and P11 do not regularly use assistive technology, while others do.}
    \label{fig:function-activations-individual}
    % \Description{Bar plot of the number of function activations for the scene description function, the main objects function, and the object localization function for individual participants. The plot shows different participants have different preferences over the three functions, and most participants predominantly used one function more than the other two functions.}
\end{figure}

\subsection{Post-Study Interview}
\label{ssec:interview}

In the post-study interview, 11 out of 12 participants expressed a preference for \textsc{EnVisionVR} over the \textsc{NVR} condition. 
Key themes are summarized below.

\subsubsection{Level of Information Delivered.} 

The design of visual accessibility systems often faces a trade-off between the level of detail of information provided and the ease of use of the system. 
% While it is important to provide detailed descriptions of the scene, descriptions with too much detail could require users to take a long period of time to explore the scene and increase the complexity of the system. 
For the \textsc{EnVisionVR} system, participants liked how the scene description function was ``helpful to identify a new location'' (P2, P4, P5, P7, P10) with the ``correct amount of detail'' (P7) and ``gave you a picture of the scene'' (P3) and ``a general overview'' (P9, P10) to ``build an image up in your mind'' (P6). P11 commented that the tools could be more helpful if there was more detailed information. Overall, participants liked how the level of information delivered had an appropriate amount of detail to gain a sense of physical presence in the scene.
% Regarding the level of detail of information provided by \textsc{EnVisionVR}, P3 commented

\begin{quote}
    \textit{``I felt that I was in a real bar or a restaurant and I'd gotten out my camera [to get a description using Be My Eyes], it was very very good.''} (P3)
\end{quote}

Participants also liked how the main objects indication function told the user about key objects nearby. 
% and provided object-level information in addition to high-level scene descriptions. 
They liked how it told the user ``if there is something on the right and left'' (P2) and ``helped users to be more confident by saying the names of things'' (P5), but found the spatial tone to indicate the object location to be redundant (P4), and it sometimes did not pick up objects near the user (P9).

% In addition to the main object function which read out names of particular objects, p
Participants found the object localization function helpful in providing the precise location of individual objects. Participants found it helpful in ``telling [participants] how far [they] need to move'' (P2), locating the object and letting the user ``know the object is there'' (P5), and ``helping [participants] understand the direction of the object'' and ``gives good feedback'' (P8). P4 found the beeping helpful in telling whether the controller was getting closer to the object. Overall, participants found \textsc{EnVisionVR} helpful in delivering both high-level scene information and detailed object-level information.

\begin{quote}
\textit{``[With {EnVisionVR},] this is the first time that I have been able to do anything in VR. This is really promising, I think you’re on to something here.''} (P1)
\end{quote}



% vivid descriptions. varying levels of detail.

\subsubsection{System Latency.} 
Participants commented that existing vision accessibility systems they use in the physical world, such as Be My AI, take several seconds to return a description of the provided image. 
% This is due to the need to process images in real time and communicate with cloud servers.
Participants contrasted this with their experience of \textsc{EnVisionVR}, which they found to be very responsive. P8 commented how she appreciated systems which give constant feedback on updates of what is in front of the user as the user moves.
% By contrast with images taken of the physical world, VR scenes are generally pre-defined and this allows the capture of `pre-baked' scene descriptions, meaning that detailed scene descriptions can be provided with very low latency. 

\begin{quote}
    \textit{``The advantage here is that the answer comes instantly and doesn't take any time to process the question.''} (P3)
\end{quote}

\subsubsection{Consistency in System Design.} 
The user study revealed that inconsistency in the system design could pose usability barriers. For example, the command ``Where am I?'' for the scene description function describes the user's field of view, while the command ``What is near me?'' for the main objects indication function reads out the names of objects which are near the user but not necessarily in front of the user. The inconsistency in reference frames led to confusion among some participants (P1, P9).

\begin{quote}
    \textit{``Visually I could see where things were and I could move towards easily. The voice assistant wasn't giving me the instruction I needed, it didn't quite work. When I was looking for that `brew button', it said it was next to me but I couldn't see it."} (P9)
\end{quote}

As the scene descriptions were pre-baked based on images of the scene, 
% the object names in the scene description do not necessarily match the names of GameObjects in the scene, which can lead to 
there can be different names for the same object in the scene description function and the main objects indication function. For example, the bookholder on the desk in Anchor 1 of the test scene was referred to as a computer in scene descriptions, which can cause confusion for users.

% Reference frame. Discrepancy in object names in scene description.

\subsubsection{User Agency.} Participants also commented on different aspects of user agency over the system. These included user control on what information to deliver, the speed of delivery, and the level of detail of delivered information.

\begin{quote}
    \textit{``I'd like this experience to have a speedier read-out speed, close to 200\%. That really should be customisable. It'd be nice to have two levels of description, detailed and then summary.''} (P1)
\end{quote}

\begin{quote}
    \textit{``I'd like a mode where I could scan a room, turning in my chair, and keep hearing an updated description of what's in front of me."} (P1)
\end{quote}

\begin{quote}
    \textit{``For me maybe [the voice description] was a bit slow. If you are in a new environment you don't want it too fast.''} (P6)
\end{quote}

A number of participants also suggested that the system could support more voice commands to improve user agency. P7 commented that the system was helpful but required users to memorize the different speech commands for each function. P12 commented that she had difficulty in trying to remember how to phrase the question.

% Ability to support a range of voice commands. Participants suggested that it would be nice to have adjustable reading speed (P1) and different levels of detail for the description (P1).

\begin{quote}
    \textit{``The only thing I would say is maybe broaden the wording used to launch the command... If it was a bit more open in terms of voice commands.''} (P6)
\end{quote}

\section{Design Implications}

Results from our evaluation study reveal important design implications for VLM-assisted interactive systems for visual accessibility design in VR.
Through these guidelines, we intend to assist designers and developers in creating more inclusive immersive systems for BLV users.

\begin{enumerate}
    \item \textbf{Different levels of vision require different levels of information.}
    % Interaction behavior results from the study suggest that participants with less vision capability who regularly use screen readers tend to activate scene descriptions and locate individual objects more than those who have more vision capability. 
    % The function activation pattern in 
    \Cref{fig:function-activations} not only suggests that participants with less vision capability generally require more information than those with better vision. Most importantly, it highlights the uneven distribution of high-level scene information and detailed object-level information required by both participant subgroups, and how this uneven distribution is different for both groups. Participants with less vision capability rely primarily on visual accessibility systems to perceive visual information as compared to their residual vision, so they are inclined to ask for different levels of visual information. 
    % High-level scene descriptions are most important as they provide a basic mental map for users to add further details. 
    Participants with better vision capability primarily used their residual vision and the vision accessibility tool to complement and verify what they saw. For example, P9 was confused when the system said that a `Brew Button' was close to him, but he could not see it in his view.
    % how BLV participants with better vision capabilities tend to use the tool to 
    % Within the group of participants with less vision capability, the Scene Description and Object Localization functions were activated slightly more than the Main Objects function, but all three functions were 
    \item \textbf{Information should be promptly conveyed to support interaction.} As BLV users rely on visual accessibility systems to support their understanding of 3D scenes, such systems need to deliver rich information with low latency. As P3 commented, \textsc{EnVisionVR} provided descriptions similar to those generated by human volunteers in Be My Eyes. P3 liked how \textsc{EnVisionVR} was able to generate descriptions instantly, unlike Be My Eyes which required users to wait for the human response or Be My AI which also required around 5 seconds to receive AI responses from the server. 
    % While the low latency in \textsc{EnVisionVR} is possible because scene descriptions for static VR scenes can be pre-collated, we acknowledge that low latency, customizable verbosity, and the appropriate level of description detail are all important to the performance of visual accessibility systems for static and dynamic VR scenes.
    \item \textbf{Spatial information should be expressed consistently.} 
    % Vision Language Models present a new opportunity for visual accessibility design for 3D content. Such 
    VLM-based accessibility systems should ensure that
    % it is crucial to ensure that
    % This is reflected in two aspects. First, 
    the delivered spatial information is consistent with the user's perception model. The system could deliver scene descriptions with respect to the user's field of view, with the option to keep providing an updated description of what is in front of the user (P1, P8), or deliver descriptions of the entire scene from a third person view, instead of only what is in front of the user (P9), but the reference frame has to be made explicit and intuitive for the user.
    % Users with less severe visual impairment tend to use their residual vision as a primary source of information and use visual accessibility functions as a complement to verify information obtained from their residual vision. It is therefore crucial to ensure visual accessibility features delivered in 3D space maintain the same reference frame as the user (i.e., from the user's field of view).
    % In the case of P9, the Main Objects function read out objects which were not in his field of view, which was inconsistent with his perception model and resulted in confusion. 
    % Meanwhile, users with more severe visual impairment rely more on visual accessibility functions compared with their residual vision, participants commented that they would like to have an overall description of the entire scene instead of only their field of view. This provides additional context for their spatial surroundings and enhances their spatial perception capability. 
    Additionally, the delivered information should also be consistent among different functions. 
    % This could be either all based on the user field of view, or all based on a third-person angle or a top-down view, but has to be consistent across all functions to avoid confusion. 
    % The study found that when speech commands are involved in visual accessibility systems, it is crucial to make sure that these commands are consistent with users' understanding and pose very limited space for different interpretations. For example, the speech commands ``Where am I?'' and ``What is near me?'' are based on different reference frames, but it is not made explicit to the user (P9). 
    % It would be best to maintain the consistency in reference frames for all functions, but if there must be a change in design consistency, this must be made clear to the user. 
    The same applies to the consistency in the names of objects mentioned in different functions. 
    \item \textbf{New systems should be consistent with established assistive technology workflows.} For example, participants who regularly use screen readers such as P1 and P6 preferred the speed of the voice assistant to be faster and adjustable. However, participants who do not regularly use assistive technology did not express preferences in system verbosity or reading speed. 
    The same applies for customizable level of detail of descriptions to cater to different user preferences (P1, P6, P11).
    % In terms of the level of detail of scene descriptions, P1 suggested that it would be nice to have both an overview description as well as an optionally selectable detail description. While P6 who regularly uses assistive technology found the scene descriptions to be very detailed, P11 who does not regularly use assistive technology found the scene descriptions to be not detailed enough
    % to provide more information 
    % to supplement her remaining vision, which suggests the importance of allowing users to customize the level of detail of descriptions.
    % to improve user agency and align with existing assistive technology workflows. 
    % Participants such as P6 also suggested that the system could support more speech commands. P7 also commented that he had to memorize the supported voice commands to use the system. P1 reflected that she was distracted by the wording of the command that she was not getting right. 
    P1, P6, and P7 also suggested improving the number of supported speech commands, as it would otherwise pose a heavy mental load and cause unnecessary distractions.
    These observations suggest that the flexibility in speech commands and accuracy in recognizing user intent as seen in many speech-based accessibility systems is also crucial for VR visual accessibility systems to adapt to different users to reduce their effort and improve user agency.
    \item \textbf{Redundancy should be provided in system input and output.} Our user study demonstrated how \textsc{EnVisionVR} was helpful in assisting BLV users
    % gain a better understanding of the scene and interact with virtual elements 
    through a combination of audio cues, speech descriptions, and haptic vibrations.
    % While visual enhancement was not applied as in previous works (e.g., \textsc{SeeingVR} \cite{zhao2019seeingvr}), the study results still suggest that a combination of different interaction modalities was helpful in delivering visual information. 
    % We also found that participants (i.e., P2, P8) 
    P2 and P8 appreciated how the haptic vibration together with audio/speech confirmation reassured them by indicating
    % in the object localization function indicated 
    that the controller had reached the object of interest.
    % P1 also explicitly stated her interest in having more haptic feedback in the system. 
    % P2 and P8 also reflected that a combination of several feedback modalities (such as vibrations and audio/speech confirmation combined together) reassured them in their tasks. 
    Conversely, the lack of certain feedback modalities can take away user confidence, as in the case of P4 when she felt a vibration in the controller but received no speech feedback.
    % from the system in the interaction task with \textsc{EnVisionVR}. 
    This highlights the need to 
    include system input and output redundancy and 
    ensure that information from different input and output channels are consistent, which aligns with the design principles (DP1 and DP2)
    % on input and output redundancy 
    identified by Dudley et al.~\cite{dudley2023inclusive}.
\end{enumerate}



% \section{Design Implications}

% In this section, we discuss design implications for visual accessibility systems
% % in general and implications for visual accessibility design 
% for immersive 3D content based on common themes identified in the study, including interaction modalities, system customizability, user agency, and information hierarchy. 
% % Through these guidelines, we intend to assist designers and developers to create more inclusive immersive systems for users with blindness and vision impairment.

% \subsection{Multimodal Interaction} 
% Our user study demonstrated how \textsc{EnVisionVR} was helpful in assisting BLV users gain a better understanding of the scene and interact with virtual elements through a combination of different interaction modalities including audio cues, speech descriptions, and haptic vibrations. While visual enhancement was not applied as in previous works (e.g., \textsc{SeeingVR} \cite{zhao2019seeingvr}), the study results still suggest that a combination of different interaction modalities was helpful in delivering visual information. We also found that participants (i.e., P2, P8) appreciated the haptic vibration in the object localization function to indicate that the controller had reached the object of interest. P1 also explicitly stated her interest in having more haptic feedback in the system. P2 and P8 also reflected that a combination of several feedback modalities (such as vibrations and audio/speech confirmation combined together) reassured them in their tasks. Conversely, the lack of certain feedback modalities in a multimodal system can take away user confidence, as in the case of P4 when she felt vibration in the controller but received no speech feedback from the system in the interaction task with \textsc{EnVisionVR}.

% \subsection{Customization and User Agency} Our study revealed how BLV users have diverse needs and preferences for visual accessibility design. Participants with more severe visual impairment who regularly use assistive technology such as P1 and P6 %rely on accessibility systems to provide more information input, and 
% expressed that they preferred the speed of the voice assistant to be faster. However, participants who do not regularly use assistive technology did not express preferences in system verbosity or reading speed. In terms of the level of detail of scene descriptions, P1 suggested that it would be nice to have both an overview description as well as an optionally selectable detail description. While P6 who regularly uses assistive technology found the scene descriptions to be very detailed, P11 who does not regularly use assistive technology found the scene descriptions to be not detailed enough to provide more information to supplement her remaining vision, which suggests the importance of allowing users to customize the level of detail of descriptions to improve user agency. Participants such as P6 also suggested that the system could support more speech commands. P7 also commented that he had to memorize the supported voice commands to use the system. P1 reflected that she was distracted by the wording of the command that she was not getting right. These observations suggest that the flexibility in speech commands and accuracy in recognizing the correct user intent is also crucial for visual accessibility systems to adapt to different users to reduce their effort and improve user agency.

% \subsection{Design Consistency} 
% % Throughout the interaction experience, all three functions are activated by a single press on the same controller button and speaking different voice commands, and all functions can be cancelled by double pressing the same button. Compared with previous versions of \textsc{EnVisionVR} which required the user to press different buttons on different controllers to activate different functions, this consistency in design allowed BLV users to complete all three tasks without too much effort. 
% The study also found that when speech commands are involved in visual accessibility systems, it is crucial to make sure that these commands are consistent with users' understanding and pose very limited space for different interpretations. For example, the speech commands ``Where am I?'' and ``What is near me?'' are based on different reference frames, but it is not made explicit to the user (P9). It would be best to maintain the consistency in reference frames for all functions, but if there must be a change in design consistency, this must be made clear to the user. The same applies to the consistency in the names of objects mentioned in different functions. 
% % While this exhibits the limitation of VLMs in hallucinating content, post-processing steps can be applied to overcome this issue.

% \subsection{Performance Optimization} As BLV users rely on visual accessibility systems to deliver visual information crucial to support their understanding of 3D scenes, visual accessibility systems need to achieve high performance with low latency to deliver rich information within a limited amount of time. As P3 commented, \textsc{EnVisionVR} provided descriptions similar to those generated by human volunteers in Be My Eyes. P3 liked how \textsc{EnVisionVR} was able to generate descriptions instantly, unlike Be My Eyes which required users to wait for the human response or Be My AI which also required around 5 seconds to receive AI responses from the server. While the low latency in \textsc{EnVisionVR} is possible because scene descriptions for static VR scenes can be pre-collated, we acknowledge that low latency, customizable verbosity, and the appropriate level of description detail are all important to the performance of visual accessibility systems for static and dynamic VR scenes.

\section{Discussion}

\textsc{EnVisionVR} represents an original integration of high-level natural language scene descriptions and detailed object-level speech, audio, and haptic cues for object localization and interaction. We complement previous work on visual accessibility design in VR by incorporating VLMs to provide detailed scene descriptions to extend works such as SeeingVR~\citep{zhao2019seeingvr} and VRBubble~\citep{ji2022vrbubble} which convert visual information to speech and audio, while also following Canetroller~\citep{canetroller} and VIVR~\citep{kim2020vivr} in incorporating different feedback modalities to convey visual information such as the presence of a virtual object. We also demonstrate how it is possible to leverage speech, audio, and haptic information together to design a multimodal system for VR visual accessibility design. Results from the user study show good promise in terms of supporting BLV users to enjoy VR experiences with the greatest benefit seemingly afforded to blind users or users with less usable vision. 
% The option to automatically calculate object importance values and determine anchor points to generate scene descriptions minimizes the need for developer intervention, which improves the generalizability of \textsc{EnVisionVR} as a visual accessibility tool for various VR applications.
% Several participants expressed their interest to try this type of system again, and many users also looked forward to this piece of technology being applied beyond VR environments and potentially serving as a mobility aid in the real world.
% \subsection{Limitations and Future Work}
% While \textsc{EnVisionVR} has already gone through several design iterations and has demonstrated how it can successfully assist users with different levels of vision impairment to understand the scene, locate virtual objects, and perform simple interaction tasks such as grabbing an object or pushing a virtual button through an empirical user study with actual BLV 
% Nevertheless, 

The study results also reveal how \textsc{EnVisionVR} could be further improved.
% in the following aspects.
As \textsc{EnVisionVR} is intended to provide a proof-of-concept of how VLMs can be applied with other interaction modalities for visual accessibility design for VR content, the scene descriptions are pre-baked. This limits the current approach to static VR scenes.
% To ensure the applicability of such visual accessibility systems, 
Future design iterations will aim to provide scene descriptions for dynamic VR scenes.

% \paragraph{Limited customizability and adaptation for different users.} 
The evaluative study found different participants had different preferences in the verbosity and level of detail of scene descriptions. %The beeping sound in the object localization function was also found to be irritating by a participant with learning disability in addition to vision impairment, and resulted in high perceived difficulty and unsuccessful completion of the object interaction task. 
These examples demonstrate the significance of incorporating the ability to customize features for individual preferences, as well as adaptations for each user as they become more accustomed to the system.
Additionally, we acknowledge that \textsc{EnVisionVR} is primarily a speech-driven interface with a limited number of supported commands. Future design iterations of \textsc{EnVisionVR} will 
% support the interactive, user-driven access to 
allow users to access more object and scene-level information through alternative and complementary forms of interaction. %, allowing the system to align better with screen reader functionality for non-visual VR access.

% \paragraph{Potential applications in other 3D environments.} In the study, participants suggested how the system would be more useful to them if the visual accessibility functions could be applied for real-world content. As object recognition and reconstruction algorithms become more advanced, it is foreseeable that algorithms will be able to recognize and interpret 3D scenes and objects in the real world with lower latency and higher accuracy, which will in turn provide the technical foundation for more visual accessibility applications for AR, MR, or real-world 3D content.

\section{Conclusion}

This paper presents \textsc{EnVisionVR}, a proof-of-concept visual accessibility tool for VR based on scene descriptions and object-level guidance powered by VLMs, speech and audio cues, and haptic feedback. %\textsc{EnVisionVR} is the product of several design iterations, and follows accessibility barriers in VR experiences identified in a formative study. 
Our evaluation study with 12 BLV participants demonstrates the effectiveness of \textsc{EnVisionVR} in assisting scene understanding,
% (11.2\% increase in scene understanding question scores), 
object localization (41.7\% increase in task success rate), and object interaction (33.3\% increase in task success rate) for BLV users compared with the condition without visual accessibility features. We also summarize a list of design implications covering five different aspects of visual accessibility. 
We hope these findings and contributions will advance research in this space and ultimately lead to more inclusive VR experiences.

\section*{Supplemental Material}
The online appendix is available at \url{https://osf.io/zb2ak/}.

\section*{Acknowledgments}
Junlong Chen is supported by the China Scholarship Council and Cambridge Trust.
This work is also supported by the Engineering and Physical Sciences Research Council (EPSRC), through the following grants: \textit{Inclusive Immersion: Inclusive Design of Immersive Content} (EP/S027637/1 and EP/S027432/1) and \textit{Towards an Equitable Social VR} (EP/W025698/1 and EP/W02456X/1). The authors thank Open Inclusion for their help in recruiting the research participants and administering the user research.            

% \bibliographystyle{ACM-Reference-Format}
% \bibliographystyle{plain}
% \bibliographystyle{apalike}
\bibliographystyle{plainnat}
\bibliography{main}

% \appendices

% \section{Implementation Details}\label{sec:appendix-implementation-methodology}

% \begin{figure*}[h!]
%     \centering
%     \includegraphics[width=\textwidth]{Figs/clustering.pdf}
%     \caption{Virtual object clustering to generate pre-baked scene descriptions.}
%     \label{fig:clustering}
% \end{figure*}

% \subsection{Scene Description Function}

% The idea behind designing the Scene Description Function is to allow users to obtain a high-level understanding of the scene. An overview of this module is shown in \Cref{fig:scene-description-methodology}. First, we calculate the importance values of each virtual object based on the object properties. This calculated importance value gives developers a reference to determine the final importance value. Next, we extract the importance values and positional coordinates of each virtual object and perform object clustering based on Gaussian Mixture Models (GMMs). The objects are grouped into clusters such that the sum of importance values in each cluster is approximately the same value, and important objects are roughly positioned in the center of each cluster. The object cluster information is used to guide the suggestion of virtual camera positions, which help developers generate pre-baked scene descriptions. Subsequently, we input screenshots from the virtual camera and a textual prompt about the style of the scene and main objects in the scene to GPT-4o to generate a description of the virtual scene. Finally, during runtime, we play the scene description of the closest matching anchor point based on the current camera position and orientation. The following subsections provide a detailed introduction of the subcomponents of the scene description module.

% \subsubsection{Calculation of Importance Values for Individual Objects}\label{sec:importance}

% The first step in providing high-level scene description is to calculate an importance value for each virtual object. There are two reasons behind this calculation: First, it automatically defines an appropriate cluster size for groups of virtual objects; Second, it provides a guidance to help us determine the cluster centers. 
% % Here, the “important” virtual objects normally refer to objects which are interactable and serve an important purpose in the scene. Heuristically, in a Unity project, objects with an important purpose usually have more rendering.
% Determining the importance of virtual objects in a VR project is different from determining object importance in the real world. In a VR project, all virtual objects are pre-defined by the developer. Hence, we can iterate through the scene graph of a VR project and assign a higher importance value to objects with mesh rendering, animator, and interactables. The complete code to calculate the original object importance $I_{original}$ can be found in the GitHub project.
% During runtime, object importance is also closely related to its distance to the user. For all objects visible in the user’s field of view, we calculate the runtime object importance as the original object importance multiplied by a distance factor, which is expressed as:
% \begin{eqnarray}
% I_{runtime}&=&I_{original}\times(e^{-dist}-e^{-d_0}),
% \end{eqnarray}
% where $ I_{runtime}$ is the runtime importance for a virtual object, $ I_{original}$ is the original importance, and $dist$ is the distance from the virtual object to the main camera. $d_0$ is a threshold distance defined by the developer. Objects further away from this threshold distance will have negative importance values, which will be removed from the list. In this escape room scene, we define $d_0$ empirically to be 5. This number is determined based on the scene dimensions and object locations. A value smaller than 5 will result in some camera positions to have no important objects nearby, while a value larger than 5 will result in many camera positions to have too many objects nearby within the $d_0$ threshold.
% % \jjd{Magic number `5'? Briefly state how this was arrived at.}


% \subsubsection{Object Clustering based on Gaussian Mixture Models (GMMs)}

% To obtain the high-level scene description, our next step is to cluster virtual objects according to their original importance values. The importance values can help us determine the cluster size and cluster centers. We parse through the scene graph and store the $x-$, $y-$, and $z-$ object coordinates and importance values in two separate numpy arrays. Next, we use \texttt{sklearn.mixture.BayesianGaussianMixture()} to define a Bayesian Gaussian Mixture Model with cuboid prior. This function takes the number of mixture components, type of covariance parameters, and a series of Gaussian mixture distribution parameters as input, and outputs a Bayesian Gaussian Mixture model, which is capable of predicting the class labels of new test data points. Here, the number of mixture components is set to four, and the type of covariance parameters is set to "full", such that each component has its own general covariance matrix, so each object cluster can have a different size. The remaining parameters for the GMM function are kept as default.



% Figure \ref{fig:clustering} (left) shows an example of the object clustering result of the scene in the bottom-right. The virtual objects in this scene are divided into four clusters by the algorithm, and the optimal camera positions are calculated and suggested to the user as anchor points to generate pre-baked scene descriptions. The overall process for object clustering to determine camera anchor positions is shown in Figure \ref{fig:clustering} (upper-right).


% \subsubsection{Camera Anchors and Scene Description based on GPT-4o}

% After obtaining the virtual object clusters, we can calculate the virtual camera positions based on basic trigonometry, as shown in the schematic diagram in Figure \ref{fig:cameraPosition}. For each cuboid cluster, we can obtain the length, height, and width, which we represent as $\Delta{x}$, $\Delta{y}$, and $\Delta{z}$. If $\Delta{x}<\Delta{z}$, we consider the camera axis (in the camera local coordinate system, this is the $z$-axis, or the axis pointing forward) 
% % \jjd{(which camera axis, and do you mean parallel to some plane rather than axis?)} 
% is parallel to the $x-$axis. If $\Delta{z}<\Delta{x}$, we consider the camera axis is parallel to the $z-$axis. Take the former case as an example, we can calculate two distances from the cuboid to the virtual camera, namely $d_h$ and $d_v$. In the VR Escape Room project, the camera horizontal field of view angle is 82.4 degrees, and the vertical field of view angle is 60 degrees, this gives us the values of $\alpha_{h}$ and $\alpha_{v}$. Taking the larger distance ensures that the camera will capture all objects in the cuboid cluster. In this case, the two distances from the nearest cuboid face to the main camera can be expressed as:
% \begin{eqnarray}
% d_h &=& \frac{\Delta{z}}{2\tan(\frac{\alpha_h}{2})},\\
% d_v &=& \frac{\Delta{y}}{2\tan(\frac{\alpha_v}{2})},
% \end{eqnarray}
% where $\alpha_h=82.4\times\frac{\pi}{180}=0.458\pi$, $\alpha_v=60\times\frac{\pi}{180}=0.333\pi$. Consequently, the distance from the cuboid center to the main camera can be calculated as:
% \begin{eqnarray}
% d &=& \max\{d_h, d_v\} + \frac{\Delta{x}}{2}
% \end{eqnarray}
% In this way, we can determine the position of the camera anchors for each virtual object cluster.

% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=0.8\linewidth]{Figs/camera_position.pdf}
%     \caption{Schematic diagram to calculate the main camera position based on the object cluster.}
%     \label{fig:cameraPosition}
% \end{figure}

% After obtaining the camera anchor positions, we define camera anchor orientation to be parallel to the horizontal plane. Each camera anchor orientation is defined in 45-degree increments. For example, for camera anchor $(x_{a1}, y_{a1}, z_{a1})$, the camera orientation is $0^\circ$ for Rotation X and Rotation Z, but $0^\circ, 45^\circ, 90^\circ, …, 315^\circ$ for Rotation Y.

% After defining position and orientation of camera anchors, we can obtain the screenshots of the user’s field of view at these anchor points. We input these screenshot images and textual prompts to GPT-4o \cite{OpenAI_2024} to obtain a description of the scene. Here, the textual prompt is crucial for the model to produce an accurate description. The prompt has to contain information about the style of the scene and list the main objects in the field of view to assist the model to produce a fairly accurate description. An example of the prompt, the input user field-of-view image, and the generated scene description is shown in Figure \ref{fig:prompt-image}.

% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{Figs/scene description prompt.png}
%     \caption{Prompt and parameters for GPT-4o to generate pre-baked scene descriptions.}
%     \label{fig:prompt-image}
% \end{figure}

% % \paragraph{
% % “Provide a brief description of this 3D VR scene in the style of \textit{an adventure novel}. Key virtual objects in the scene include \textit{an instructions book}, \textit{a wand}, and \textit{a key}.”
% % }



% \subsubsection{Matching Real-time FoV to Pre-baked Scene Descriptions}

% After obtaining the scene descriptions at camera anchors, our next step is to assign the most appropriate pre-baked scene description to the current main camera position. To perform this matching task, we define a loss function to calculate the discrepancy between the main camera and the camera anchor as follows:

% \begin{eqnarray}
% {\cal L}_{camera}(c, a)&=&(\theta_c-\theta_a)^2 + 0.8\times[(x_{c}-x_a)^2\nonumber\\ && + (y_{c}-y_a)^2 + (z_{c}-z_a)^2],
% \end{eqnarray}

% \noindent
% where $\theta_c$ is the current camera Y rotation angle (in degrees), $\theta_a$ is the camera anchor Y rotation angle (in degrees), $(x_c, y_c, z_c)$ is the current camera positional coordinates, and $(x_a, y_a, z_a)$ is the camera anchor positional coordinates. 
% % Since the rotational angle is in degrees (spanning from $0^\circ$ to $360^\circ$), and the positional coordinates is in meters (spanning from -5m to 5m), this empirical equation ensures that the rotational angle has a larger influence on camera anchor matching. 
% We assign a larger weight to the camera orientation because even if two cameras are at different locations in a scene, as long as they have a similar orientation, we can expect that their view will not be drastically different. If two cameras are at the same position but have different orientations, their view can be completely different.

% Each time the user elicits the scene description algorithm, we calculate the loss between the current user camera and all camera anchors, and read out the pre-baked scene description associated with the camera anchor $i$ with the smallest loss. Mathematically, this can be expressed as:

% \begin{eqnarray}
% i &=& \argmin\{{\cal L}_{camera}(c, a_i)\}, ~~~\forall{i}\in\{1, 2, 3, …, n_a\},
% \end{eqnarray}

% \noindent
% where $n_a$ is the total number of camera anchors.


% {\appendix[Implementation Details]


% Use $\backslash${\tt{appendix}} if you have a single appendix:
% Do not use $\backslash${\tt{section}} anymore after $\backslash${\tt{appendix}}, only $\backslash${\tt{section*}}.
% If you have multiple appendixes use $\backslash${\tt{appendices}} then use $\backslash${\tt{section}} to start each appendix.
% You must declare a $\backslash${\tt{section}} before using any $\backslash${\tt{subsection}} or using $\backslash${\tt{label}} ($\backslash${\tt{appendices}} by itself
%  starts a section numbered zero.)}






% \newpage

% \section{Biography Section}
% If you have an EPS/PDF photo (graphicx package needed), extra braces are
%  needed around the contents of the optional argument to biography to prevent
%  the LaTeX parser from getting confused when it sees the complicated
%  $\backslash${\tt{includegraphics}} command within an optional argument. (You can create
%  your own custom macro containing the $\backslash${\tt{includegraphics}} command to make things
%  simpler here.)
 
% \vspace{11pt}

% % \bf{If you include a photo:}\vspace{-33pt}
% \begin{IEEEbiography}
% [{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{fig1}}]{Michael Shell}
% % Use $\backslash${\tt{begin\{IEEEbiography\}}} and then for the 1st argument use $\backslash${\tt{includegraphics}} to declare and link the author photo.
% % Use the author name as the 3rd argument followed by the biography text.
%         \includegraphics[width=.8in]{Authors/Junlong_Chen.pdf} & \textbf{Junlong Chen} is a PhD student in the Department of Engineering at the University of Cambridge. His research interests include scene interpretation for intelligent multimodal interactive systems and accessibility design.\\ \\
%          \includegraphics[width=.8in]{Authors/kristensson.pdf} & \textbf{Per Ola Kristensson} is a Professor of Interactive Systems Engineering in the Department of Engineering at the University of Cambridge and a Fellow of Trinity College, Cambridge. He is a co-founder and co-director of the Centre for Human-Inspired Artificial Intelligence at the University of Cambridge. \\ \\
%          \includegraphics[width=.8in]{Authors/jdudley.jpg} & \textbf{John J. Dudley} is an Associate Teaching Professor in the Department of Engineering at the University of Cambridge. He is a member of the Computational and Biological Learning Lab and his research focusses on the design of interactive systems that dynamically adapt to user needs and behaviours.

% \end{IEEEbiography}

\begin{IEEEbiography}
[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Authors/Junlong_Chen.pdf}}]{Junlong Chen} is a PhD Student at the Department of Engineering, University of Cambridge. His research interests include scene interpretation for intelligent multimodal interactive systems and accessibility design.
\end{IEEEbiography}
\vspace{-33pt}
\begin{IEEEbiography}
[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Authors/Rosella.pdf}}]{Rosella P. Galindo Esparza} is a Research Fellow at the Brunel Design School, Brunel University of London. She is a member of the Brunel Digital Design Lab, and her research focuses on human-computer interaction design for immersive technologies, accessibility, and social inclusion.
\end{IEEEbiography}
\vspace{-33pt}
\begin{IEEEbiography}
[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Authors/Vanja_Garaj.pdf}}]{Vanja Garaj} is a Professor of Design and the Director of Research at the Brunel Design School, Brunel University of London, where he also leads the Brunel Digital Design Lab, a research group specialising in design-led technology innovation. His research interests include human-computer interaction, accesibility and inclusive design.
\end{IEEEbiography}
\vspace{-33pt}
\begin{IEEEbiography}
[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Authors/kristensson.pdf}}]{Per Ola Kristensson} is a Professor of Interactive Systems Engineering at the Department of Engineering, University of Cambridge and a Fellow of Trinity College, Cambridge. He is a co-founder and co-director of the Centre for Human-Inspired Artificial Intelligence (CHIA) at the University of Cambridge.
\end{IEEEbiography}
\vspace{-33pt}
\begin{IEEEbiography}
[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Authors/jdudley.pdf}}]{John J. Dudley} is an Associate Teaching Professor of Machine Learning and Machine Intelligence at the Department of Engineering, University of Cambridge. He is a member of the Computational and Biological Learning Lab and his research focusses on the design of interactive systems that dynamically adapt to user needs and behaviours.
\end{IEEEbiography}
% \vspace{11pt}

% \bf{If you will not include a photo:}\vspace{-33pt}
% \begin{IEEEbiographynophoto}{John Doe}
% Use $\backslash${\tt{begin\{IEEEbiographynophoto\}}} and the author name as the argument followed by the biography text.
% \end{IEEEbiographynophoto}




% \vfill

\end{document}


