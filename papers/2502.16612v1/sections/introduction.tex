\section{Introduction}
\label{sec:introduction}

Despite the rapid growth of multimodal content—integrating images, text, and sometimes video—the automated detection of harmful and false information on online news and social media platforms has become increasingly critical. In particular, identifying propaganda and hate in memes is essential for combating misinformation and minimizing online harm. While most research has focused on textual analysis, multimodal approaches have received comparatively less attention. In propaganda detection, text-based methods have evolved from monolingual to multilingual setups \cite{piskorski-etal-2023-multilingual,hasanain-etal-2023-araieval}, initially through binary classification and later via multilabel and fine-grained span-level tasks \cite{BARRONCEDENO20191849,Habernal.et.al.2017.EMNLP,Habernal2018b,da-san-martino-etal-2019-fine}. Hate speech detection has similarly progressed from text-based to multimodal approaches that integrate both textual and visual elements. Recent methods have shifted from transformer-based text detection \cite{fortuna2018survey} toward techniques that incorporate visual context \cite{kiela2020hateful} by leveraging fusion strategies, attention mechanisms, and contrastive learning to boost accuracy, especially when hateful intent is conveyed through text-image interplay \cite{alam2022survey}. 
% Despite this progress, challenges remain, including dataset scarcity, implicit hate speech detection, and high computational demands \cite{ijcai2022p781}.

\begin{figure}[]
    \centering
    \includegraphics[scale=0.15]{figures/vllm_exp_meme.png}
    \vspace{-0.4cm}
    \caption{Experimental steps for explanation generation and training.}    
    \label{fig:vllm_exp_meme}
    \vspace{-0.6cm}
\end{figure}

The emergence of LLMs has demonstrated significant capabilities across various disciplines. Consequently, efforts have been made to leverage Vision-Language Models (VLMs) \cite{zhang2024vision} and prompting techniques to enhance the detection and classification of harmful and propagandistic memes \cite{Rui2023}. LLM-based models utilize prompt-based learning \cite{cao-etal-2022-prompting}, contrastive learning techniques such as CLIP \cite{kumar2022hate}, and cross-modal attention mechanisms to better capture implicit hate and propaganda. 


% \color{red}
Despite significant progress, challenges remain in detecting implicit hatefulness, particularly when sarcasm or an ironic dissonance exists between text and images. Propagandistic memes further complicate detection by employing emotional appeals, humor, cultural references, manipulative language, and other rhetorical strategies. To address these nuances, it is crucial for a system to provide not only accurate predictions but also interpretable explanations that reveal the underlying reasoning behind its decisions \cite{hee2023decoding, yang2023hare, huang_chain_2023, sun_text_2023}. Such explanations enhance classifier reliability by helping end users understand the decision, provided they maintain a natural tone and relate closely to the visual elements of the meme.

An explanation-based approach offers numerous advantages and enhances performance across various tasks \cite{li2022explanations, magister2022teaching, nandi2024safe, kumari2024m3hop}. While most studies have focused on textual content \cite{li2022explanations, magister2022teaching}, a few recent approaches \cite{nandi2024safe, kumari2024m3hop} have applied explainability to images. However, these methods rely on QA-based explanations that lack naturalness, use multiple inference calls with custom models—thereby increasing computational complexity—and employ explanations only during training rather than as an inference output. These limitations motivated us to explore a simplified procedure for meme classification and explanation generation. To overcome the above limitations, we propose a novel procedure that achieves state-of-the-art performance on the target tasks across two distinct datasets. Our contributions are briefly outlined below: 

\begin{itemize}[noitemsep,topsep=0pt,labelsep=.5em]
    \item We developed explanation-enhanced datasets, \memex{}, using a rapid and low-cost annotation procedure;
    \item We investigated state-of-the-art VLMs to identify an appropriate model for meme classification and explanation generation;
    \item We proposed an efficient \textit{multi-stage} optimization procedure that significantly improves performance;
    \item With the experiments we achieved state-of-the-art performance on two types of datasets related to propaganda and hateful content detection.
\end{itemize}

Our findings are as follows:
\textit{(a)} A higher human evaluation score suggests that explanations from stronger models (e.g., GPT-4o) are reliable and can serve as gold-standard explanations for training smaller models.
\textit{(b)} Task-specific fine-tuning improves performance over the base model.
\textit{(c)} Our multi-stage optimization approach benefits both label detection and explanation generation.
Overall, our work is the \textbf{\textit{first}} to enhance VLMs for simultaneous propaganda and hateful content detection while providing natural reasoning to end users.



% \begin{itemize}[noitemsep,topsep=0pt,labelsep=.5em]
%     \item We developed explanation-enhanced datasets using rapid and low-cost annotation procedure;
%     \item Investigated state-of-the-art VLMs to identify an appropriate model for meme classification and explanation generation;
%     \item Proposed an efficient optimization procedure that significantly improves performance;
%     \item Achieved state-of-the-art performance on two types of datasets related to visual sentiment and risk assessment.
% \end{itemize}

% Overall, our work is the \textbf{\textit{first}} to enhance the VLMs for simultaneous sentiment prediction while providing natural reasoning to end users.

% \color{black}

%\begin{itemize}
%  \item Developed explanation-enhanced datasets using rapid and low-cost annotation procedure.
%  \item Investigated state-of-the-art VLMs to identify an appropriate model for meme classification and explanation generation.
%  \item Proposed an efficient optimization procedure that significantly improves performance.
%  \item Achieved state-of-the-art performance on two types of datasets related to visual sentiment and risk assessment.
%\end{itemize}