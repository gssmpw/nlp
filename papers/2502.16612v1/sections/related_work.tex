\section{Related Work}
\label{sec:related_work}

The widespread use of social networks has become a major channel for spreading misinformation, propaganda, and harmful content. Significant research efforts have been directed toward addressing these challenges, particularly in multimodal disinformation detection~\cite{alam2022survey}, harmful memes~\cite{ijcai2022p781}, and propagandistic content~\cite{ACL2021:propaganda:memes}. However, most studies have focused on detection, while less attention has been given to generating natural explanations/reasons behind the predicted labels.

\subsection{Multimodal Propagandistic Content}
% \textbf{Textual Content:} 
% The study of misinformation and propagandistic content has received significant attention from researchers in recent years. Such efforts include studying cross-lingual propaganda analysis \cite{barron2019proppy}, news article propaganda analysis \cite{da-san-martino-etal-2019-fine}, misinformation and propaganda about politics and war \cite{pierri2023propaganda}, etc. A large-scale dataset consisting of 350K sentences annotated into 18 propaganda techniques has been developed for propaganda technique identification in news articles \cite{da2020semeval}. Later the annotation scheme has been expanded to 23 propaganda techniques and developed a multilingual dataset to detect propaganda \cite{piskorski-etal-2023-semeval}. Some datasets have been developed for Arabic following the same annotation guideline, and shared tasks have been organized using these data sets \cite{propaganda-detection:WANLP2022-overview,hasanain-etal-2023-araieval,hasanain2024can}.
% \noindent
% \textbf{Multimodal Content:} 
Following the previous research for propaganda detection using textual content~\cite{da-san-martino-etal-2019-fine}, \citet{dimitrov-etal-2021-semeval} introduced SemEval-2021 Task 6 focusing on persuasion techniques detection in both textual and visual memes. Subsequently, the focus has extended to the detection of multilingual and multimodal propagandistic memes \cite{dimitrov2024semeval}. \citet{Glenski2019MultilingualMD} studied multimodal disinformation content on social media platforms in multilingual settings. Similar multimodal work on Arabic involves the development of datasets and shared task for propaganda detection \cite{alam-etal-2024-armeme, araieval:arabicnlp2024-overview}. For the detection problem, typical approaches include a fusion of textual and visual embedding and a classification head on top them~\cite{araieval:arabicnlp2024-overview,shah-etal-2024-mememind}, graph attention network based approach for multimodal visual-textual objects \citet{chen2024multimodal}.


\subsection{Multimodal Hate speech}
Similarly, there has been growing interest in detecting multimodal hate speech \cite{kiela2020hateful, velioglu2020detecting, hee2022explaining}. Due to the lack of resources, \citet{kiela2020hateful} developed a large-scale dataset for multimodal hate identification. This study advanced research in this area and emphasized the importance of integrating textual and visual features for effective detection.
The issue has also been explored through a multi-task learning framework for identifying hate speech in memes using multimodal features \cite{ijcai2022p781}. To further progress in this field, efforts have been made to develop resources for multiple languages, including Arabic \cite{alam2024propaganda}, Bangla \cite{hossain-etal-2022-mute}, and English \cite{hee2023decoding}. A more detailed summary of these earlier efforts can be found in \citet{ijcai2022p781}, which also highlights key challenges and outlines future research directions.


\subsection{Training with Explanations}
% \color{blue}{
Integrating reasoning or explainability capabilities to enhance LLM/VLM performance has been shown to be highly beneficial for various tasks across multiple domains \cite{plaat2024reasoning}. This approach has also proven effective for knowledge distillation and model compression \cite{li2022explanations, magister2022teaching}, where explanations generated by large LLMs improve the performance and capabilities of smaller LLMs. In the context of hateful speech, toxicity detection, and sentiment analysis, it has led to significant advancements \cite{yang2023hare, huang_chain_2023, sun_text_2023}.
%
For example, in the hateful speech detection task, Hare \cite{yang2023hare} employs Chain-of-Thought (CoT) reasoning, while \cite{huang_chain_2023} utilizes Chain of Explanation (CoE). Their aim is to improve the effectiveness of LLM-based sentiment classifiers by leveraging reasoning capabilities. Likewise, \citet{sun_text_2023} introduced a technique called Clue and Reasoning Prompting (CARP), which incorporates both reasoning and keywords as clues to support the reasoning process. 
%
In the following subsections, we specifically examine approaches closely related to our task, focusing on those that have applied VLM-based methods for analyzing hateful or propagandistic memes.
%

%\subsubsection{CoT-Based Approaches}
%
CoT is a widely recognized prompting technique that generates a chain of reasoning to derive answers. 
% In recent years, it has been extensively applied in various domains and tasks \cite{plaat2024reasoning}. A comprehensive review of all such applications is beyond the scope of this paper; therefore, we specifically focus on recent studies that have applied CoT with VLMs \cite{kumari2024m3hop, nandi2024safe, li2022explanations}.
%
A recent comprehensive CoT-based meme analysis study is presented in \cite{kumari2024m3hop}, which proposed a framework based on text- and image-based entity-object relationships using a scene graph. They applied a hierarchical three-step CoT-based prompting strategy to guide the LLM in identifying Emotion, Target, and Context, using these elements to build a model for meme analysis.  
%
Another recent work, called SAFE-MEME \cite{nandi2024safe}, proposed two multimodal datasets and introduced a structured reasoning framework for hate speech detection in memes. They developed a CoT prompting-based framework that incorporates Q\&A-style reasoning and hierarchical categorization to classify memes as hateful (explicit or implicit) or benign. However, they did not evaluate their approach using the popular Hateful Memes dataset, preventing a direct numerical comparison with their results.  

One drawback of these CoT-based approaches is that they rely on multi-step reasoning, requiring multiple inferences with VLMs. Our approach differs from these CoT-based methods in the following ways:  \textit{(a)} we do not employ a complex multistep CoT approach, eliminating the need for multiple LLM inferences, which significantly improves computational efficiency and reduces costs.  \textit{(b)} we focus on providing explanations alongside classification, helping the end-users better understand the reasoning behind classification decisions, thereby increasing reliability.
%

%\subsubsection{Explanation-Based Approaches}
\cite{hee2023decoding} constructed a dataset providing explanations for hateful memes. However, unlike us, they focused solely on evaluating explanation generation and did not perform classification tasks. 
%They applied various evaluation metrics to assess the quality of the generated explanations. 
Despite the availability of their data, we do not use it due to the lack of \textit{naturalness}. In particular, their explanations do not fully account for image content or image-centric contextual perspectives.

% }
% \color{black}
%
% Compared to previous studies, our work differs in that we provide the first resource for Arabic. Additionally, our annotation guidelines and data collection procedures for memes may be useful for other languages. 