\section{\memex{}: Explanation Generation}
\label{sec:experiments_expl}
% \todo[inline]{Hasnat, Firoj}
%
The outcomes of an automatic system become more reliable for users if it provides decisions with adequate and interpretable natural explanations, which help users better understand the underlying reason behind the system’s decision \cite{hee2023decoding, yang2023hare, huang_chain_2023, sun_text_2023}. Technically, this approach provides numerous advantages in terms of knowledge distillation, model compression, and enhancing the performance of target tasks in different domains \cite{li2022explanations, magister2022teaching, nandi2024safe, kumari2024m3hop}. This motivates us to adopt the explanation-based approach in our research. However, we also aim to improve its efficiency, particularly with respect to dataset generation, model training, and system inference procedures.
%

In this research, we generate explanations for two different stages: 
(a) during existing dataset enhancement, which leverages an expert 
VLM (such as GPT) to generate high-quality explanations and 
(b) during training/inference with a smaller VLM (such as Llama-3.2 11b). 
Figure \ref{fig:vllm_exp_meme} illustrates these different stages.
%
Mathematically, these two stages can be described by the functions
$f(i, l)=e$ and $g(i)=(l,e)$, where \( e \) denotes the explanation, 
\( l \) is the label, and \( i \) is the input image or meme. 
Specifically, \( f(i, l) \) returns an explanation \( e \) 
given both \( i \) and \( l \), whereas \( g(i) \) generates both 
the label \( l \) and the explanation \( e \) from only the input \( i \).
%
%

This research enhances two existing datasets with explanations, see Section \ref{sec:dataset} and Table \ref{tab:explation_stat} for the details and statistics. For the explanation generation task, it first uses a VLM for \( f(i, l) \) and then involves human experts, which significantly accelerates high-quality explanation generation and lowers the overall cost and time. The following subsections provide step-by-step details.
%

% \begin{table}[]
% \centering
% \setlength{\tabcolsep}{4pt} 
% \scalebox{0.9}{%
% \begin{tabular}{@{}lrr|rr@{}}
% \toprule
% \textbf{Data} & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Total \\ Words\end{tabular}}} & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Avg. \\ Words\end{tabular}}} & \multicolumn{1}{|c}{\textbf{\begin{tabular}[c]{@{}c@{}}Total \\ Expl. \\ Words\end{tabular}}} & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Avg. \\ Expl. \\ Words\end{tabular}}} \\ \midrule
% \multicolumn{5}{c}{\textbf{ArMeme -- Arabic Expl.}} \\ \midrule
% Train & 58,688 & 15 & 280,341 & 70 \\
% Dev & 8,583 & 15 & 40,756 & 70 \\
% Test & 16,653 & 15 & 79,360 & 70 \\ \midrule
% \textbf{Total} & \textbf{83,924} & \textbf{15} & \textbf{400,457} & \textbf{70} \\ \midrule
% \multicolumn{5}{c}{\textbf{ArMeme -- English Expl.}} \\ \midrule
% Train & 58,723 & 15 & 375,843 & 94 \\
% Dev & 8,527 & 15 & 55,336 & 95 \\
% Test & 16,653 & 15 & 105,476 & 93 \\ \midrule
% \textbf{Total} & \textbf{83,903} & \textbf{15} & \textbf{536,655} & \textbf{94} 
% \\ \midrule
% \multicolumn{5}{c}{\textbf{Hateful Meme}} \\ \midrule
% Train & 99,812 & 12 & 740,624 & 87 \\
% Dev & 4,904 &	9	& 43,956 &	81 \\
% Test & 18,079 & 9 & 173,982 & 87 \\ \midrule
% \textbf{Total} & 122,795 & 10 & 958,562 & 85 \\ \bottomrule
% \end{tabular}
% }
% \vspace{-0.2cm}
% \caption{Descriptive statistics of the dataset. \textit{Total Words} and \textit{Avg.} refer to the total and average number of words in the text. The last two columns represent the corresponding values for the explanations.
% }
% \label{tab:explation_stat}
% \vspace{-0.3cm}
% \end{table}


\begin{table}[]
\centering
\setlength{\tabcolsep}{4pt} 
\scalebox{0.9}{%
\begin{tabular}{@{}lrr|rrrr@{}}
\toprule
\textbf{Data} & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Total \\ Words\end{tabular}}} & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Avg. \\ Words\end{tabular}}} & \multicolumn{2}{|c}{\textbf{\begin{tabular}[c]{@{}c@{}}Total Expl. \\ Words\end{tabular}}} & \multicolumn{2}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Avg. \\ Expl. \\ Words\end{tabular}}} \\ \cline{4-7}
\multicolumn{3}{c|}{} & \multicolumn{1}{c}{\textbf{Ar}} & \multicolumn{1}{c}{\textbf{En}} & \multicolumn{1}{c}{\textbf{Ar}} & \multicolumn{1}{c}{\textbf{En}} \\ \midrule
\multicolumn{7}{c}{\textbf{ArMeme}} \\ \midrule
Train & 58,688 & 15 & 280,341 & 375,843 & 70 & 94 \\
Dev & 8,583 & 15 & 40,756 & 55,336 & 70 & 95 \\
Test & 16,653 & 15 & 79,360 & 105,476 & 70 & 93 \\ \midrule
\textbf{Total} & \textbf{83,924} & \textbf{15} & \textbf{400,457} & \textbf{536,655} & \textbf{70} & \textbf{94}  \\ \midrule
\multicolumn{7}{c}{\textbf{Hateful Meme}} \\ \midrule
Train & 99,812 & 12 & -- & 740,624 & -- & 87 \\
Dev & 4,904 &	9	& -- & 43,956 & -- & 81 \\
Test & 18,079 & 9 & -- & 173,982 & -- & 87 \\ \midrule
\textbf{Total} & \textbf{122,795} & \textbf{10} & -- & \textbf{958,562} & -- & \textbf{85} \\ \bottomrule
\end{tabular}
}
\vspace{-0.2cm}
\caption{Descriptive statistics of the dataset. \textit{Total Words} and \textit{Avg.} refer to the total and average number of words in the text. The last two columns represent the corresponding values for the explanations.
}
\label{tab:explation_stat}
\vspace{-0.3cm}
\end{table}


%

\subsection{VLMs for Explanation Generation}
Figure \ref{fig:vllm_exp_meme} illustrates an example of an Arabic meme 
along with its explanation-generation process using a VLM. We leverage GPT-4o (version 2024-11-20) for automated explanation generation. The choice of this model is motivated by prior studies \citet{wang_evaluating_2023}, which show that advanced GPT models can produce fluent, informative, persuasive, and logically sound explanations when properly prompted. In Listing~\ref{lst:prompt_explanation_generation_armeme_ar_expl}, we present the \textit{prompts} used for generating explanations for \textbf{ArMeme} and \textbf{Hateful Memes}. To refine the prompt, we iteratively tested several memes in both English and Arabic, selecting the one that produced the most reasonable explanations.


For Arabic memes, we generate two sets of explanations—one \textit{in English} and one \textit{in Arabic}. The motivation behind this approach is to assess the multilingual capability and quality of smaller VLMs, such as Llama-3.2 11b, in generating explanations and labels in both languages. 
\color{black}
%LLM based explanation generation for non-English languages is a new research problem. 

% Evaluating explanations in multiple languages helps identify potential biases or limitations in the model’s reasoning processes. If a model performs well in one language but not in another, it may indicate language-specific biases or deficiencies in training data. \cite{ribeiro2016should}
% Generating explanations in the original language (Arabic, in this case) ensures that culturally specific nuances are captured accurately, while an English explanation can provide a bridge for cross-cultural understanding.
% Users are more likely to trust and effectively utilize systems that communicate in their native language. By providing explanations in both Arabic and English, the system can cater to native speakers while also offering an internationally accessible version, thereby enhancing transparency and user satisfaction.



\paragraph{Size of the Explanation}
Determining the optimal length for explanations is important for balancing 
informativeness and cognitive load~\cite{herm2023impact}. 
\citet{shen2022shortest} explored the relationship between explanation length 
and human understanding, finding that the shortest rationales are often 
ineffective. Recently, \citet{wang_evaluating_2023} also studied the effect 
of explanation size and found that human evaluators are reluctant to read 
longer explanations. 
%when used mobile devices. 
To achieve an optimal balance, we iteratively tested 
various explanation lengths and ultimately set a limit of 100 words.

\paragraph{Model and Its Parameters}
To utilize GPT-4o~\cite{openai2023gpt4}, we accessed the OpenAI API via Azure services. Though recently released o1 models have shown promising directions for complex reasoning, they were not accessible to us. For explanation generation, we employed zero-shot learning. To ensure reproducibility, we set the temperature value to zero.

% \color{red}
% @HASNAT, I think we can skip this section totally to save space.
% %
% \paragraph{Handling Model Refusals in Explanation Generation}
% A recurring issue was the model’s refusal to provide explanations due to the nature of certain memes (e.g., propagandistic or hateful content). The model often responded with messages such as \textit{“I cannot provide an explanation”} or \textit{“Content is filtered due to the content moderation policy.”}  This issue affected approximately 30–100 cases across different splits for both Arabic propagandistic and English hateful memes. In such instances, we accessed the OpenAI API directly. Note that Azure services include an additional layer of content filtering, which may have contributed to these refusals.
% \color{black}

\subsection{Human Evaluation}
\label{ssec:human_eval}
Given that our idea is to use the generated explanation as gold data for further training and evaluation, therefore, we intended to go through human evaluation process. Following the prior studies \cite{wang_evaluating_2023,huang_chain_2023,agarwal_faithfulness_2024} we adopted four metrics discussed below. For each metric we use 5-point Likert scale. 

\noindent
\textbf{Informativeness.} Measures the extent to which the explanation provides relevant and meaningful information for understanding the reasoning behind the label. A highly informative explanation offers detailed insights that directly contribute to the justification, while a low-informative explanation may be vague, incomplete, or lacking key details.

\noindent
\textbf{Clarity.} Assesses how clearly the explanation conveys its meaning. A clear explanation is well-structured, concise, and easy to understand without requiring additional effort. It should be free from ambiguity, overly complex language, or poor phrasing that might hinder comprehension.

\noindent
\textbf{Plausibility.} Refers to the extent to which an explanation logically supports the assigned label and appears reasonable given the meme's content. A plausible explanation should be coherent, factually consistent, and align with the expected reasoning behind the label.

\noindent
\textbf{Faithfulness.} Measures how accurately an explanation reflects the reasoning behind the assigned label. A faithful explanation correctly represents the key factors and logical steps that justify the label, without adding misleading or unrelated details. 

For manual annotation, we first prepared an annotation guideline for the annotators. Additionally, we developed annotation guidelines and a platform (see Appendix \ref{sec:app_annotation_platform} and \ref{sec:app_annotation_guideline}, respectively).


\noindent\textbf{Evaluation Setting.} 
%For the Arabic meme task, we recruited annotators who are native Arabic speakers and fluent in English. All annotators hold at least a bachelor's degree. Since they are fluent in English, they also worked on the hateful meme task. We provided training and consultation as needed, and they had prior experience with similar tasks. 
%
%A total of six annotators participated in the evaluation task. They include both native and non-native Arabic speakers, all fluent in English and holding at least a bachelor's degree. Since the institute requires the signing of a Non-Disclosure Agreement (NDA), each annotator signed an NDA. For the payment process, we have an agreement with a third-party company that manages the payment process for the annotators, who are compensated at standard hourly rates based on their location.
%
For the Arabic meme task, we recruited annotators who are native Arabic speakers and fluent in English, all holding at least a bachelor's degree. Because of their fluency, they also handled the hateful meme task. We provided necessary training and consultation, and all had prior experience with similar tasks.

A total of six annotators participated in the evaluation. In line with institutional requirements, each signed a Non-Disclosure Agreement (NDA), and a third-party company managed their compensation at standard hourly rates based on location.




\paragraph{Quality Assessment}
% \todo[inline]{Arid, can you add table here and complete this section?}
% We computed annotation agreement using various evaluation measures, including Fleiss' kappa, Krippendorff’s alpha, average observed agreement, and majority agreement. The resulting scores were 0.529, 0.528, 0.755, and 0.873, respectively. Based on the value of Krippendorff’s alpha, we can conclude that our annotation agreement score indicates moderate agreement.
%We computed the average 5-point Likert scale value for human evaluation metrics. 
In Table \ref{tab:likert_score}, we summarize the quality assessment of the explanations. We used 5-point Likert scale for various human evaluation metrics, including informativeness, clarity, plausibility, and faithfulness. We compute the average of the Likert scale value for all evaluation metrics. We manually evaluated 359 and 202 random samples for ArMeme Arabic and English explanations while 200 random examples were evaluated for the Hateful meme dataset. The average agreement scores for the ArMeme dataset with Arabic explanations are 4.23, 4.38, 4.24, and 4.16 for faithfulness, clarity, plausibility, and informativeness, respectively, indicating high agreement across all evaluation metrics. However, for the English explanations of ArMeme, the faithfulness and plausibility scores are relatively. To better understand this issue, we plan to conduct further evaluations on another set of explanations.
For the Hateful Memes dataset, the average Likert scale agreement scores range from 4.562 to 4.682.

\begin{table}[h]
\centering
\setlength{\tabcolsep}{1pt} 
\scalebox{0.78}{%
\begin{tabular}{@{}lrrrr@{}}
\toprule
\multicolumn{1}{c}{\textbf{Dataset}} & \multicolumn{1}{c}{\textbf{Faithfulness}} & \multicolumn{1}{c}{\textbf{Clarity}} & \multicolumn{1}{c}{\textbf{Plausibility}} & \multicolumn{1}{c}{\textbf{Informative}} \\ \midrule
% ArMeme & 4.227 & 4.383 & 4.235 & 4.158 \\
% Hateful meme & 4.562 & 4.647 & 4.632 & 4.682 \\ 
ArMeme (Ar) & 4.23 & 4.38 & 4.24 & 4.16 \\
ArMeme (En) & 3.91 & 4.50 & 3.81 & 4.13 \\
Hateful meme & 4.56 & 4.65 & 4.63 & 4.68 \\
\bottomrule
\end{tabular}
}
\vspace{-0.3cm}
\caption{Average Likert scale value for each human evaluation metric across different sets of explanations.}
\label{tab:likert_score}
\vspace{-0.4cm}
\end{table}

\subsection{Basic Statistics}
%In Table \ref{tab:explation_stat}, we summarize the basic statistics for both datasets. The average number of words in an explanation is 94 for Arabic and 85 for English. Notably, in both cases, we instructed the model (GPT-4o) to generate explanations with fewer than 100 words. Based on the manual evaluation of the explanations, as shown in Table \ref{tab:likert_score}, we conclude that both the quality and length of the explanations are reasonable. 
Table \ref{tab:explation_stat} presents the basic statistics for both datasets. The average explanation length is 94 words for Arabic and 85 words for English. Notably, we instructed GPT-4o to generate explanations with fewer than 100 words. Based on manual evaluation (Table \ref{tab:likert_score}), we conclude that both the quality and length of the explanations are appropriate.