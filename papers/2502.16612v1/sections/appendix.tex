\section{Annotation Guideline}
\label{sec:app_annotation_guideline}
You will be shown a meme, a label assigned to it, and an explanation for the assigned label. As an annotator, your task is to carefully examine each meme, label, and explanation. Then assess the quality of the explanation provided for the assigned label.
Follow the steps below to ensure a thorough evaluation:

\paragraph{Analyze the Meme}
\begin{itemize}[noitemsep,topsep=0pt,labelsep=.5em] %leftmargin=*
    \item Observe the image and read the accompanying text.
    \item Understand the overall message and the potential implications of the meme.
\end{itemize}
\paragraph{Check the Assigned Label}
\begin{itemize}
    \item Check the given label. The label is the result of annotation done by multiple human annotators.
\end{itemize}

\paragraph{Evaluate the Explanation}
\begin{itemize}[noitemsep,topsep=0pt,labelsep=.5em] %leftmargin=* 
    \item Read the explanation provided for why the meme has been assigned its label.
    \item Assess the explanation based on the metrics below. Each metric is scored on a Likert scale from 1-5. 
\end{itemize}

\textbf{Kindly note that to evaluate the explanation, you do not have to agree or disagree with the given label.}

\subsection{Metrics}

\subsubsection{Informativeness}
Measures the extent to which the explanation provides relevant and meaningful information for understanding the reasoning behind the label. A highly informative explanation offers detailed insights that directly contribute to the justification, while a low-informative explanation may be vague, incomplete, or lacking key details.

\noindent
\textbf{As an annotator, you are judging if the explanation provides enough information to explain the label assigned to the meme.}

\begin{itemize}[noitemsep,topsep=0pt,labelsep=.5em] %leftmargin=*
    \item 1 = Not informative: The explanation lacks relevant details and does not help understand why the meme is labeled as such.
    \item 2 = Slightly informative: The explanation provides minimal information, but key details are missing or unclear.
    \item 3 = Moderately informative: The explanation contains some useful details but lacks depth or supporting reasoning.
    \item 4 = Informative: The explanation is well-detailed, providing a clear and meaningful justification for the label.
    \item 5 = Very informative: The explanation is thorough, insightful, and fully justifies the label with strong supporting details.
\end{itemize}

\subsubsection{Clarity}
Assesses how clearly the explanation conveys its meaning. A clear explanation is well-structured, concise, and easy to understand without requiring additional effort. It should be free from ambiguity, overly complex language, or poor phrasing that might hinder comprehension.

\noindent
\textbf{As an annotator, you are judging the language and structure of the explanation. Spelling mistakes, awkward use of language, and incorrect translations will negatively impact this metric.}

\begin{itemize}[noitemsep,topsep=0pt,labelsep=.5em] %leftmargin=*
    \item 1 = Very unclear: The explanation is confusing, vague, or difficult to understand.
    \item 2 = Somewhat unclear: The explanation has some clarity but includes ambiguous or poorly structured statements.
    \item 3 = Neutral: The explanation is somewhat clear but may require effort to fully grasp.
    \item 4 = Clear: The explanation is well-structured and easy to understand with minimal ambiguity.
    \item 5 = Very clear: The explanation is highly readable, precise, and effortlessly understandable.
\end{itemize}

\subsubsection{Plausibility}
Refers to the extent to which an explanation logically supports the assigned label and appears reasonable given the meme's content. A plausible explanation should be coherent, factually consistent, and align with the expected reasoning behind the label. While it does not require absolute correctness, it should not contain obvious contradictions or illogical claims.

\noindent
\textbf{As an annotator, you are judging if the explanation actually supports the label assigned to the meme. For example, if a meme is labeled as Not Propaganda, the explanation given should justify that label.}

\begin{itemize}[noitemsep,topsep=0pt,labelsep=.5em] %leftmargin=*
    \item 1 = Not plausible at all: The explanation does not align with the label and seems completely incorrect.
    \item 2 = Weakly plausible: The explanation has some relevance but lacks strong justification or contains logical inconsistencies.
    \item 3 = Moderately plausible: The explanation somewhat supports the label but may be incomplete or partially flawed.
    \item 4 = Plausible: The explanation logically supports the label and is mostly reasonable.
    \item 5 = Highly plausible: The explanation is fully aligned with the label and presents a strong, logical justification.
\end{itemize}

\subsubsection{Faithfulness}
Measures how accurately an explanation reflects the reasoning behind the assigned label. A faithful explanation correctly represents the key factors and logical steps that justify the label, without adding misleading or unrelated details. High faithfulness means the explanation stays true to the actual reasoning used for classification, ensuring reliability and consistency.

\textbf{As an annotator, you are judging how well the explanation reflects the logic behind the label. For example, if the explanation claims an implication of the meme, it should also present the logical reasoning behind it.}

\begin{itemize}[noitemsep,topsep=0pt,labelsep=.5em] %leftmargin=*
    \item 1 = Not faithful at all: The explanation is completely unrelated to the given label and does not reflect a valid reasoning process.
    \item 2 = Weakly faithful: Some elements of the explanation are relevant, but much of it is misleading, inconsistent, or lacks proper justification.
    \item 3 = Moderately faithful: The explanation captures parts of the reasoning but includes unrelated, unclear, or unnecessary justifications.
    \item 4 = Faithful: The explanation aligns well with the reasoning behind the label and includes relevant, logical details.
    \item 5 = Highly faithful: The explanation fully and accurately reflects the correct reasoning, without any misleading or irrelevant information.
\end{itemize}


\section{Annotation Platform}
\label{sec:app_annotation_platform}

%\todo[inline]{At Arid, add text and cite figure}

In Figure \ref{fig:hateful_meme_annotation_interface}, we present the screenshot of the interface designed for the explanation evaluation of hateful meme, which consisted of an image, respective label, and explanation for the label, annotation guidelines, and four different evaluation metrics. We used 5-point Likert scale for each evaluation metric. Annotators select one of the Likert scale value following the annotation guideline for each metric and submit.

\begin{figure*}[htb!]
    \centering
    \includegraphics[scale=0.30]{figures/hateful_meme_annotation_interface.png}
    \caption{A screenshot of the annotation platform for the explanation evaluation of hateful meme.}
    \label{fig:hateful_meme_annotation_interface}
\end{figure*}


\section{Prompt for Explanation Generation}
\label{sec:app_prompt}
In Listings \ref{lst:prompt_explanation_generation_armeme_ar_expl} and \ref{lst:prompt_explanation_generation}, we provide the prompts used to generate explanations for ArMeme and Hateful Meme. The prompt in Listing \ref{lst:prompt_explanation_generation_armeme_ar_expl} is specifically for generating Arabic explanations for ArMeme. To generate English explanations, the same prompt was used, except it was adapted for English. 

\begin{lstlisting}[caption={Prompt for generating Arabic explanation for ArMeme dataset. The place-holders refers to label, and number of words, respectively.},label={lst:prompt_explanation_generation_armeme_ar_expl}]
You are a Propaganda Image Detection Expert. A human expert has already classified the image as {class_label}. 
Do not change or re-identify the classified label of the image.
First, analyze the image's visual elements (objects, symbols, color usage, composition) 
and provide a concise description. Next, read and paraphrase any text in the image-especially if 
it is in non-Arabic-so that a Arabic speaker could understand its literal or intended meaning. 
Explain how that text, in conjunction with the visual elements, supports or illustrates the known 
propaganda technique. Think carefully about emotional appeals, humor, cultural references, manipulative 
language, or other rhetorical strategies.
Finally, provide a thoughtful explanation in Arabic (up to {explanation_length} words) of why these elements led the human to classify 
the image as propaganda. Be specific about how the text and visuals interact, referencing the 
Arabic context or cultural nuances if relevant. Your answer must be valid JSON with a single field:
{{
"explanation": "Your explanation here (up to {explanation_length} words) in Arabic."
}}
\end{lstlisting}

\begin{lstlisting}[caption={Prompt for generating explanation. The place-holders refers to label, and number of words, respectively.},label={lst:prompt_explanation_generation}]
You are a Hateful Image Detection Expert. A human expert has already classified the image as {}. Do not change or re-identify the classified label of the image.
First, analyze the image's visual elements (objects, symbols, color usage, composition) and provide a concise description. Next, read and paraphrase any text in the image so anyone speaking English can understand its literal or intended meaning. 
Explain how that text, in conjunction with the visual elements, supports or illustrates the known hateful content. 
Finally, provide a thoughtful explanation in English (up to {} words) of why these elements led the human to classify the image as hateful. Be specific about how the text and visuals interact, referencing the context or cultural nuances if relevant. Your answer must be valid JSON with a single field:
{{
"explanation": "Your explanation here (up to {} words) in English."
}}
\end{lstlisting}


\section{Error Analysis}
\label{sec:app_error_analysis}

In Figure \ref{fig:correct_incorrect_prediction}, we present examples from the Hateful Memes dataset, showcasing cases where the model made both correct and incorrect predictions.

In figure \textbf{\ref{fig:correct_prediction}}, the Gold explanation describes the image as reinforcing a harmful racial stereotype by juxtaposing a joyful scene of Asian individuals eating with offensive text. The Predicted explanation correctly identifies the derogatory language and its racist implications, aligning with the gold annotation. The model’s BERT-F1 score of 0.873 shows the high confidence in associating textual and visual elements to detect hate speech effectively.


In figure \textbf{\ref{fig:correct_incorrect_prediction}}, the Gold explanation interprets the image as a humorous juxtaposition, using wordplay between nationality and species without targeting any group. However, the Predicted explanation classifies it as hateful. This missclassification suggests that the model struggled to distinguish linguistic humor from implicit hate speech, as reflected in its BERT-F1 score of 0.6259. This highlights the challenge of detecting context-dependent content, where intent and interpretation play a crucial role in classification.


\begin{figure*}[tbh!]
    \centering
    \begin{subfigure}[b]{0.85\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/correct_prediction.png}
        \caption{Correct prediction.}
        \label{fig:correct_prediction}
    \end{subfigure}
    \vspace{0.5cm}
    \begin{subfigure}[b]{0.85\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/incorrect_prediction.png}
        \caption{Incorrect prediction.}
        \label{fig:incorrect_prediction}
    \end{subfigure}
    \caption{Example of correct and incorrect label prediction with explanation.}
    \label{fig:correct_incorrect_prediction}
\end{figure*}

\section{Data Release}
\label{apndix:release}
The \memex{} dataset\footnote{\url{anonymous.com}} will be released under the CC BY-NC-SA 4.0 -- Creative Commons Attribution 4.0 International License: \url{https://creativecommons.org/licenses/by-nc-sa/4.0/}.

% The dataset includes the following files:
% \begin{itemize}
%     \item Subjectivity manual annotations divided into training, development, and test sets, in CSV format. Each news sentence is represented by an id, text, and label.
%     \item Annotation guidelines provided to the crowd annotators in Arabic.
%     \item Explanation and instruction annotations generated automatically by the GPT-4o model, in JSONL format, with the same splits as the manually annotated data. Each news sentence is represented by an id, text, label, explanation, and instruction.
%     \item Example scripts for running experiments, including PLMs (AraBERT model) and LLMs (GLUE model).
% \end{itemize}
