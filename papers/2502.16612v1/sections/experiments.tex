\section{Methodology}
\label{sec:experiments}

% \todo[inline]{Hasnat, Mohamed, Ali}

\subsection{Instructions Dataset}
Our approach follows the standard pipeline for aligning LLMs with user intentions and specific tasks through fine-tuning on representative data \cite{zhang2023instruction,kmainasi2024llamalens}. 
This process typically involves curating and constructing instruction datasets that guide the model's behavior, ensuring it generates responses that align with the desired objectives. For our study, the responses include label and explanation. Hence, we created instruction format for both datasets.
%
For the ArMeme dataset, we replicated the experiments for both Arabic and English explanations.

\subsection{Model Selection}
As shown in Figure \ref{fig:vllm_exp_meme}, our first experimental phase involves model selection among several recent VLMs, including Llama-3.2 (11b) \cite{dubey2024llama}, Paligemma 2 (3b)~\cite{steiner2024paligemma}, Qwen2-vl~\cite{wang2024qwen2}, and Pixtral (12b)~\cite{agrawal2024pixtral}.

We evaluate the base models in a zero-shot setting and fine-tune them using an instruction-following paradigm. The instructions prompt the model to generate responses in the format \texttt{``Label: (class\_label)''}. We use and a regex-based function to extract the predicted labels.

%For this phase of experiments, we evaluated the base models on the zero-shot setting, and also we fine-tuned them using the instruction-following paradigm. Specifically, the instructions were designed to prompt the model to generate responses in the follwoing format: \texttt{``Label: (class\_label)''}.  The generated output is subsequently processed with a regex-based function to extract the predicted class labels.

%The initial phase focuses on optimizing classification performance by restricting the output solely to the class label. Here, the training objective considers only the predicted label, enforcing a direct mapping between input and class label and omitting any additional explanatory text.

Note that this stage fine-tunes the models to predict class labels only, allowing us to verify whether they can handle multilingual inputs—especially in understanding Arabic text, cultural nuances, and image context. We do not ask the model to generate explanations here, as that is a more complex task and could affect their performance.

%For this phase of experiments, our objective was to fine-tune different vision-language large models (VLMs) to optimize their predictive performance in estimating the conditional probability $ p(L \mid X) $, where $X$ represents an input meme and $L$ is the desired output. Given a training dataset of $N$ samples, denoted as $\{(x_i, l_i)\}_{i=1}^{N}$, where each $x_i$ corresponds to a meme and $l_i$ to its associated label, we trained the models by minimizing the negative log-likelihood: $\theta^* = \arg\min_{\theta} \sum_{i=1}^{N} -\log p_{\theta}(l_i \mid x_i)$ 

Based on the results reported in Tables \ref{tab:armeme} and \ref{tab:hateful_meme}, we selected Llama-3.2-vision-instruct (11b) for further training with explanations.


% ~\cite{longpre2023flan}. An instruction sample is a triplet of a natural language instruction describing the task, an optional input, and an output that represents the answer following the instruction~\cite{wang2022self}.

% \paragraph{Natural language instructions} There are several potential techniques to create %the needed 
% natural language instructions, including manual and automatic approaches. As instructions diversity positively affects model performance and generalization~\cite{dubois2024alpacafarm,pang-etal-2024-phased,zhang2024text}, we aim to create a diverse instruction dataset. Due to the scale of tasks and datasets of focus in this work, creating such a diverse set manually is time-consuming and can lead to limited instruction styles.

\subsection{\textit{Multi-Stage (MS)} Optimization Procedure} \label{sec:multi-stage}
%
To emphasize our novel contribution, we introduce a dedicated optimization procedure to train VLM with \memex{}, which decouples the classification and explanation generation tasks. This approach is designed to first endow the model with strong task-specific representations through classification-only fine-tuning, and then refine its ability to generate coherent, natural explanations.

\noindent
\paragraph{\textit{Stage 1: Classification Fine-Tuning}} In this stage, the model is fine-tuned solely on the classification task. The training objective is restricted to predicting the correct class label. This focused objective encourages the model to develop robust, task-specific representations. We use the QLoRA setup described later with a learning rate of $2\times10^{-4}$ to optimize the model.
\noindent
\paragraph{\textit{Stage 2: Explanation Enhancement}} In this stage, the model is further fine-tuned on a combined label-with-explanation dataset. Here, we employ a reduced learning rate ($1\times10^{-5}$) to gently adapt the model’s parameters for generating the explanations while preserving the classification performance achieved in Stage 1.

To validate the effectiveness of the multi-stage procedure, we compare it against a single-stage (SS) fine-tuning baseline where the model is directly trained on the label-with-explanation dataset. Our ablation studies (detailed in Section \ref{label:results}) demonstrate that the proposed multi-stage approach significantly outperforms the single-stage strategy.

%Llama-3.2-vision was selected based on its performance compared with the other models in preliminary fine-tuning experiments. In the first stage, the model is fine-tuned only on the classification task, therefore, optimizing its ability to predict the class label accurately. In the second stage, we further fine-tune the model using the label-with-explanation dataset with a reduced learning rate.

%The multi-stage approach is compared against a baseline in which the model is fine-tuned on the label-with-explanation dataset in a single-stage process. Our hypothesis is that isolating the classification task in the initial phase is significant for the model in achieving task-specific representation, where the loss is optimized for classification rather than generating coherent text. To validate our methodology, we perform comparative experiments between the sequential and single-stage fine-tuning procedures.


%We base our experiments on Llama 3.1, the most effective \textit{open} LLM to date, with strong multilingual performance~\cite{dubey2024llama}. Fine-tuning larger scales of the model (e.g., 70B version) holds a great overhead in terms of time and computational cost. Moreover, These models may be inaccessible to many researchers, so we focus on the smaller Llama 3.1-8B version. We base 
% \llens{} 
%on Llama 3.1-8B-Instruct, as it is already aligned with various user tasks.

%We instruction-tune Llama 3.1-8B-Instruct\footnote{We use the term \textit{Llama-instruct} or \textit{Base} to refer to this model in the rest of the paper.} following different setups. Given that fine-tuning LLMs typically requires substantial computational resources, making it a time-consuming and resource-intensive process, therefore, to address this challenge, our main 
% \llens{} 

\begin{table}[h]
\centering
\setlength{\tabcolsep}{3pt} 
\scalebox{0.85}{%
\begin{tabular}{@{}llrrr@{}}
\toprule
\multicolumn{1}{c}{\textbf{Model}} & \multicolumn{1}{c}{\textbf{Setup}} & \multicolumn{1}{c}{\textbf{Acc (\%)}} & \multicolumn{1}{c}{\textbf{W-F1}} & \multicolumn{1}{c}{\textbf{M-F1}} \\ \midrule
\cite{alam-etal-2024-armeme}  & Qarib & 69.7 & 0.690 & 0.551 \\
\cite{alam-etal-2024-armeme} & mBERT & 70.7 & 0.675 & 0.487 \\
Llama-3.2 (11b) & Base & 13.4 & 0.172 & 0.113 \\
Llama-3.2 (11b) & FT & 68.0 & 0.665 & \textbf{0.452} \\
Paligemma2 (3b) & Base & 15.3 & 0.090 & 0.080 \\
Paligemma2 (3b) & FT & 65.9 & 0.524 & 0.200 \\
Qwen2 (7b) & Base & 63.1 & 0.550 & 0.242 \\
Qwen2 (7b) & FT & 27.0 & 0.149 & 0.195 \\
Pixtral (12b) & Base & 14.6 & 0.177 & 0.133 \\
Pixtral (12b) & FT & 70.8 & 0.636 & 0.377 \\ 
\bottomrule
\end{tabular}
}
\vspace{-0.2cm}
\caption{Results for ArMeme. FT: Fine-tuned. Qarib~\cite{abdelali2021pretraining} is a Arabic BERT (text only). mBERT - multilingual BERT (text only).}
\label{tab:armeme}
% \vspace{-0.1cm}
\end{table}


\begin{table}[h]
\centering
\setlength{\tabcolsep}{3pt} 
\scalebox{0.85}{%
\begin{tabular}{@{}llrrr@{}}
\toprule
\multicolumn{1}{c}{\textbf{Model}} & \multicolumn{1}{c}{\textbf{Setup}} & \multicolumn{1}{c}{\textbf{Acc (\%)}} & \multicolumn{1}{c}{\textbf{W-F1}} & \multicolumn{1}{c}{\textbf{M-F1}} \\ \midrule
\cite{kiela2020hateful} &  & 69.47±2.06 &  &  \\  %VisualBERT 
\cite{cao-etal-2022-prompting} &  & 72.98±1.09 &  &  \\ %PromptHate~
Llama-3.2 (11b) & Base & 66.1 & 0.650 & 0.618 \\
Llama-3.2 (11b) & FT & 77.7 & 0.770 & \textbf{0.748} \\
Paligemma2 (3b) & Base & 35.2 & 0.277 & 0.217 \\
Paligemma2 (3b) & FT & 69.2 & 0.664 & 0.623 \\
Qwen2 (7b) & Base & 66.4 & 0.669 & 0.442 \\
Qwen2 (7b) & FT & 77.9 & 0.773 & 0.753 \\
Pixtral (12b) & Base & 66.7 & 0.667 & 0.430 \\
Pixtral (12b) & FT & 77.2 & 0.766 & \underline{0.746} \\ \bottomrule
\end{tabular}
}
\vspace{-0.2cm}
\caption{Results for Hateful meme. FT: Fine-tuned}
\label{tab:hateful_meme}
\vspace{-0.2cm}
\end{table}



%In addition to the full precision model, we aimed to train smaller models, achieving two goals: \textit{(i)} release smaller but effective models to be used in resource-constrained environments, and \textit{(ii)} efficiently investigate the effects of some parameter settings on model performance, to guide the full model training. Thus, we also fine-tune the original Llama-8B-Instruct model employing QLoRA~\cite{dettmers2024qlora}, which involves quantization of the model's weights and significantly enhances memory optimization, 
 %while maintaining acceptable performance.

\subsection{Training Setup}
%\paragraph{Training Setup}
Our fine-tuning experiments utilize QLoRA~\cite{dettmers2023qloraefficientfinetuningquantized}, which combines INT4 quantization with parameter-efficient fine-tuning through Low-Rank Adaptation (LoRA)~\cite{hu2021lora}. In our setup, the base model is quantized to 4-bit precision, with LoRA updates applied to a subset of the model parameters. 
%This approach was chosen due to computational resource constraints. Additionally, deploying models for inference is costly, so our focus was on quantized models and evaluating their performance accordingly.
This approach was selected to address computational resource constraints. Furthermore, deploying models for inference incurs significant costs. Therefore, we focus on quantized models and assessing their performance accordingly.

For all experiments, we fine-tuned the models using the QLoRA approach with 4-bit quantization. This approach was chosen due to its efficiency in reducing memory usage while maintaining model performance. We adapted all relevant submodules (vision, language, attention, and MLP layers) with a LoRA rank of 16, an alpha of 16, and no dropout. For training, we used a per-device batch size of 2 with gradient accumulation over 4 steps and optimized using AdamW with a learning rate of $2\times10^{-4}$, a weight decay of 0.01, and a linear scheduler with 5 warmup steps. For the second stage experiments (label-with-explanation), the learning rate was reduced to $1\times10^{-5}$.
 
 % $g(i)=(l,e)$


%For all models we train, we set a LoRA learning rate of 2e-%4. Optimization was performed using AdamW %\cite{loshchilov2017decoupled}, with a batch size of 16. %All experiments were executed on four NVIDIA H100-80GB GPUs.

%In the first set of experiments, we trained four models quantized to 4-bit precision using QLoRA. Although the models store weights in 4-bit format, computations are performed in BFLOAT16 (bf16), with both the LoRA rank and $\alpha$ set to 16. Each model was trained on one of the dataset shuffling configurations. After identifying the optimal order of the training dataset—based on average model performance on our test sets, as shown in Figure~\ref{fig:quantized_performance}—we used that dataset order to fine-tune our %\llens{} 
%model in full precision (16-bit). Due to the scale of the model and training set size, we train the model for two epochs, increasing LoRA rank to 128, following the recommendations in \cite{xin-etal-2024-beyond}, which suggests that higher ranks yield improved performance for multitask learning. LoRA $\alpha$ was set equal to the rank.


\noindent
\subsection{Evaluation Setup and Metrics}
%\paragraph{Evaluation Setup.} 
%For all experiments, except those involving LLMs as detailed below, 
We train the models using the training set, fine-tune the parameters with the development set, and evaluate their performance on the test set as reported in Tables \ref{tab:armeme} and \ref{tab:hateful_meme}. 
% We use the model with the best weighted-F1 on the development set to evaluate its performance on the test set. 
%
%\noindent
%\paragraph{Evaluation Measures.}
For performance measurement across different experimental settings, we compute accuracy, weighted F$_1$ score, and macro-F$_1$ score. 
We evaluate the model's explanation performance on the test set using semantic similarity-based metric, measured by the F$_1$ score within BERTScore~\citep{zhangbertscore}. This score is computed using contextual embeddings extracted from pre-trained BERT models. To enhance accuracy, we utilize language-specific transformer models for embedding extraction. For Arabic we use AraBERT (v2)~\cite{baly2020arabert} model and for English we use bert-base-uncased model~\cite{devlin2019bert}. Although metrics such as BLEU and ROUGE are commonly used, studies have reported their limitations \cite{xu2023critical, krishna-etal-2021-hurdles}. Therefore, we rely solely on BERTScore.  

