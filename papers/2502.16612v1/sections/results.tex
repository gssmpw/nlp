

\section{Experimental Results and Discussion}
\label{label:results}

% \todo[inline]{Hasnat, Mohamed, Firoj}

This section first presents competitive results among our proposed method and the state-of-the-art approaches. Next, it briefly analyzes and investigates the proposed method to validate and highlight the core contributions of this research.

Table~\ref{tab:sota_comparison} compares our proposed models with state-of-the-art approaches. On the ArMeme dataset, our method achieves the best accuracy at 72.1\% and the best weighted F1 at 0.699, with Qarib and mBERT following behind. Although Qarib attains the highest macro F1 (0.551), our model remains competitive with a macro F1 of 0.536. Importantly, our method stands out because it provides explanations that add significant value. On the Hateful Meme dataset, our approach clearly outperforms the state-of-the-art by achieving the best performance with an accuracy of 79.9\%, a weighted F1 of 0.802, and a macro F1 of 0.792. These results clearly highlight the advantages of our explainability-enhanced dataset and the proposed multi-stage optimization procedure for both classification and explanation-generation tasks.



\begin{table}[t]
\centering
\setlength{\tabcolsep}{3pt} 
\scalebox{0.80}{%
\begin{tabular}{@{}llrrr@{}}
\toprule
\multicolumn{1}{c}{\textbf{Model}} & \multicolumn{1}{c}{\textbf{Setup}} & \multicolumn{1}{c}{\textbf{Acc(\%)}} & \multicolumn{1}{c}{\textbf{W-F1}} & \multicolumn{1}{c}{\textbf{M-F1}} \\ \midrule
\multicolumn{5}{c}{\textbf{ArMeme}} \\ \midrule
\cite{alam-etal-2024-armeme} & Qarib & 69.7 & 0.690 & 0.551 \\
\cite{alam-etal-2024-armeme} & mBERT & 70.7 & 0.675 & 0.487 \\
\cite{alam-etal-2024-armeme} & ResNet50  & 66.0 & 0.637 & 0.434 \\
Llama MS & FT & 72.1 & 0.699 & 0.536 \\
Llama (Ar-Exp) MS & FT & 72.0 & 0.696 & 0.499 \\ \midrule
\multicolumn{5}{c}{\textbf{Hateful Meme}} \\ \midrule
\cite{kiela2020hateful} &  & 69.47±2.06 &  &  \\
\cite{cao-etal-2022-prompting} &  & 72.98±1.09 &  &  \\
Llama MS & FT & 79.9 & 0.802 & 0.792 \\ \bottomrule
\end{tabular}
}
\vspace{-0.2cm}
\caption{Comparison with SOTA and our results. ResNet50~\cite{he2016deep} is an image only model. MS: Multi-stage.}
\label{tab:sota_comparison}
\vspace{-0.3cm}
\end{table}


Table~\ref{tab:results_expl} provides classification and explanation-generation 
results on the \emph{ArMeme} and \emph{Hateful Meme} datasets. It briefly presents these results from several perspectives: \textit{(a)} \textbf{Base vs.~FT}: demonstrates the performance difference between the same model with and without fine-tuning (FT); \textit{(b)} \textbf{Single-stage (SS) vs.~Multi-stage (MS)}: highlights the necessity and benefits of the proposed optimization procedure and \textit{(c)} \textbf{Eng-Exp vs.~Ar-Exp}: showcases the multilingual capability of the selected VLM. Next, we provide a brief analysis of the results based on these perspectives.
%

First, we compare the \textbf{Base vs.~FT} setup, from which it is evident that the FT model significantly outperforms the baseline. For example, on the ArMeme dataset, while the baseline achieves an accuracy of 12.7\%, the proposed fine-tuning boosts it to 72.1\%. Similarly, on the Hateful Meme dataset, fine-tuning improves the base accuracy from 65.2\% to 79.9\%. We observe similar improvements in the F1 metrics for classification and BERTScore for explanation quality. These significant performance gains \textit{validate our approach of fine-tuning the base models with the explainability enhanced dataset}, demonstrating its efficacy for the meme classification and explanation generation tasks.
%

Next, we compare the \textbf{SS vs.~MS} setup, which reveals that multi-stage (MS) fine-tuning further enhances performance over the single-stage (SS) approach. For example, on the ArMeme dataset, the accuracy increased from 68.2\% to 72.1\%, the weighted F1 increased from 0.584 to 0.699, the macro F1 increased significantly from 0.257 to 0.536, and the BERTScore for Arabic explanation increased significantly from 0.58 to 0.72. A similar trend is observed on the Hateful Meme dataset, where additional fine-tuning iterations yield more robust classification (approximately 4\% improvement) and enhanced explanation quality. These performance gains \textit{validate our proposed multi-stage optimization procedure} to further refine the VLMs.
%

Finally, we assess the model’s multilingual capability by comparing the performance of \textbf{\textit{Llama MS - FT}} with \textbf{\textit{Llama MS Ar-Exp}}. The results show that fine-tuning using explanations generated in both languages yields comparable outcomes. This \textit{validates the multilingual capability of our empirically chosen VLM} for the target task and enables users to understand multilingual content even if they are not fluent in that language. For example, our model allows an English speaker to analyze Arabic memes and receive explanations in English.

\begin{table}[htb!]
\centering
\setlength{\tabcolsep}{3pt} 
\scalebox{0.82}{%
\begin{tabular}{@{}llrrrr@{}}
\toprule
\multicolumn{1}{c}{\textbf{Model}} & \multicolumn{1}{c}{\textbf{Setup}} & \multicolumn{1}{c}{\textbf{Acc (\%)}} & \multicolumn{1}{c}{\textbf{W-F1}} & \multicolumn{1}{c}{\textbf{M-F1}} & \multicolumn{1}{c}{\textbf{BS}} \\  \midrule
\multicolumn{6}{c}{\textbf{ArMeme}} \\  \midrule
Llama & Base & 12.7 & 0.165 & 0.105 & 0.61 \\
Llama SS & FT & 68.2 & 0.584 & 0.257 & 0.70 \\
Llama MS & FT & 72.1 & 0.699 & 0.536 & 0.70 \\
Llama Ar-Exp & Base & 19.0 & 0.246 & 0.125 & 0.58 \\ 
Llama MS Ar-Exp & FT & 72.0 & 0.696 & 0.499 & 0.72 \\ \midrule
% \begin{tabular}[c]{@{}l@{}}
% \end{tabular} 
% \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} \\  \midrule
\multicolumn{6}{c}{\textbf{Hateful Meme}} \\  \midrule
Llama & Base & 65.2 & 0.615 & 0.567 & 0.661 \\
Llama SS & FT & 75.9 & 0.760 & 0.745 & 0.767 \\
Llama MS & FT & 79.9 & 0.802 & 0.792 & 0.777 \\ \bottomrule
\end{tabular}
}
\vspace{-0.2cm}
\caption{Results with ArMeme and Hateful meme classification and explanation generation. Llama: Llama-3.2 (11b), BS: BERTScore. SS: Single-stage, MS: Multi-stage. Ar-Exp: Model trained with Arabic explanation.}
\label{tab:results_expl}
\vspace{-0.3cm}
\end{table}

% \begin{table}[]
% \setlength{\tabcolsep}{3pt} 
% \scalebox{0.75}{%
% \begin{tabular}{@{}llrrrrr@{}}
% \toprule
% \multicolumn{1}{c}{\textbf{Model}} & \multicolumn{1}{c}{\textbf{Setup}} & \multicolumn{1}{c}{\textbf{Acc}} & \multicolumn{1}{c}{\textbf{W-P}} & \multicolumn{1}{c}{\textbf{W-R}} & \multicolumn{1}{c}{\textbf{W-F1}} & \multicolumn{1}{c}{\textbf{M-F1}} \\ \midrule
% VisualBERT \cite{kiela2020hateful} &  & 69.47±2.06 &  &  &  &  \\
% PromptHate \cite{cao-etal-2022-prompting} &  & 72.98±1.09 &  &  &  &  \\
% Llama-3.2 (11b) & Base & 0.661 & 0.649 & 0.661 & 0.650 & 0.618 \\
% Llama-3.2 (11b) & FT & 0.777 & 0.775 & 0.777 & 0.770 & 0.748 \\
% Paligemma2 (3b) & Base & 0.352 & 0.538 & 0.352 & 0.277 & 0.217 \\
% Paligemma2 (3b) & FT & 0.692 & 0.687 & 0.692 & 0.664 & 0.623 \\
% % paligemma2-3b-pt-448 & Base & 0.267 & 0.525 & 0.267 & 0.256 & 0.198 \\
% % paligemma2-3b-pt-896 & Base & 0.122 & 0.666 & 0.122 & 0.130 & 0.114 \\
% Qwen2 (7b) & Base & 0.664 & 0.716 & 0.664 & 0.669 & 0.442 \\
% Qwen2 (7b) & FT & 0.779 & 0.776 & 0.779 & 0.773 & 0.753 \\
% Pixtral (12b) & Base & 0.667 & 0.667 & 0.667 & 0.667 & 0.430 \\
% Pixtral (12b) & FT & 0.772 & 0.769 & 0.772 & 0.766 & 0.746 \\
% % Janus-pro-7b & Base & 0.631 & 0.623 & 0.631 & 0.626 & 0.597 \\ 
% \bottomrule
% \end{tabular}
% }
% \caption{Results for hateful meme classification. FT: Fine-tuned}
% \label{tab:hateful_meme}
% \end{table}


% \begin{table}[]
% \centering
% \setlength{\tabcolsep}{3pt} 
% \scalebox{0.75}{%
% \begin{tabular}{@{}llrrrr@{}}
% \toprule
% \multicolumn{1}{c}{\textbf{Model}} & \multicolumn{1}{c}{\textbf{Setup}} & \multicolumn{1}{c}{\textbf{Acc}} & \multicolumn{1}{c}{\textbf{W-F1}} & \multicolumn{1}{c}{\textbf{M-F1}} & \multicolumn{1}{c}{\textbf{BS}} \\ \midrule
% Llama   & Base & 0.127 & 0.165 & 0.105 & 0.61 \\
% Llama Scratch & FT & 0.682 & 0.584 & 0.257 & 0.70 \\
% Llama Continued & FT & 0.721 & 0.699 & 0.536 & 0.70 \\
% Llama Ar-Expl & Base & 0.190 & 0.246 & 0.125 & 0.58 \\
% Llama Ar-Expl Continued & FT &  &  &  &  \\ \bottomrule
% \end{tabular}
% }
% \caption{Results with ArMeme classification and explanation generation. Llama: Llama-3.2 (11b), BS: BERTScore}
% \label{tab:results_expl_armeme}
% \end{table}

% \begin{table}[]
% \centering
% \setlength{\tabcolsep}{3pt} 
% \scalebox{0.75}{%
% \begin{tabular}{@{}llrrrr@{}}
% \toprule
% \multicolumn{1}{c}{\textbf{Model}} & \multicolumn{1}{c}{\textbf{Setup}} & \multicolumn{1}{c}{\textbf{Acc}} & \multicolumn{1}{c}{\textbf{W-F1}} & \multicolumn{1}{c}{\textbf{M-F1}} & \multicolumn{1}{c}{\textbf{BS}} \\ \midrule
% Llama & Base & 0.652 & 0.615 & 0.567 & 0.6613 \\
% Llama Scratch & FT & 0.759 & 0.760 & 0.745 & 0.7673 \\
% Llama Continued & FT & 0.799 & 0.802 & 0.792 & 0.7771 \\ \bottomrule
% \end{tabular}
% }
% \caption{Results with Hateful meme classification and explanation generation. Llama: Llama-3.2 (11b), BS: BERTScore}
% \label{tab:results_expl_armeme}
% \end{table}


% \section{Additional Experiments}
% % \todo[inline]{TO BE DECIDED whether we should add...}
% \paragraph{Additional New Dataset.}
% \firoj{We will decide later if we keep it...}
% Given that dataset is relatively imbalanced, therefore, for this study, we have collected additional dataset and manually annotated another $9,576$ memes following the same data collection, and annotation guidelines discussed in \cite{alam-etal-2024-armeme}. Each meme has been annotated by three annotators. We used majority voting for the final label selection, which resulted in 66\% agreement. Note that in 6,209 memes, there was a full agreement. In $\sim302$ memes annotators disagreed, therefore, we have excluded from the final dataset. We have also computed annotation agreement using various metrics such as Fleiss' kappa, Krippendorff’s alpha, and average observed agreement. The resulting scores were 0.529, 0.528, and 0.755, respectively. Based on the value of Krippendorff’s alpha, we can conclude that our annotation agreement score indicates moderate agreement.

