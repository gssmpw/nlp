\section{Distribution-preserving Network Bootstrap}
\label{sec:DPNB}
% 
To enable the use of more powerful supervised learning techniques, we introduce a theoretically supported resampling methodology that generates additional samples from the state evolution equation.

The methodology centers on batching experimental units into subpopulations, denoted by $\batch$. Specifically, $\batch$ represents a subset of units from $\{1, \ldots, N\}$ where membership is determined exclusively by the treatment allocation $\Mtreatment{}{}$. Then, under the randomized treatment assumption—which specifies that $\Mtreatment{}{}$ is random and independent of all other variables—each $\batch$ constitutes a random sample from the experimental population. In this section, we demonstrate that each distinct selection of $\batch$ yields a new observation of the state evolution equation, provided the empirical treatment distributions exhibit sufficient variation across subpopulations.

Our analysis proceeds in two stages. First, we extend the state evolution concept to derive subpopulation-specific state evolutions. We establish that our batching technique generates statistically efficient samples of the experimental state evolution while preserving its fundamental structure—hence the term \batching{} (\batchingAcronym{}). Second, we derive a decomposition rule that characterizes the finite-sample behavior of potential outcomes. This decomposition enables the identification of endogenous noise within the experimental data and elucidates how \batchingAcronym{} addresses this challenge.

\begin{remark}
The theoretical frameworks presented here extend, with minimal or no modification, to arbitrary subpopulations. While our analysis focuses specifically on treatment-allocation-based subpopulations in this section, the appendices provide complete theoretical statements applicable to general subpopulation structures.
\end{remark}


\subsection{Subpopulation-specific State Evolution}
\label{sec:BSE}
%
We begin by extending the state evolution analysis to subpopulations of experimental units.
% 
\begin{theorem}[informal statement]
    \label{thm:BSE_informal}
Under the conditions of Theorem~\ref{thm:SE_informal}, consider a subpopulation $\batch$ whose size $\cardinality{\batch}$ grows to infinity as $N \to \infty$. There exist mappings $\outcomefb{t},\; t=0, \ldots, T-1$, such that:
    % 
    \begin{equation}
        \label{eq:BSE_informal}
        \law(
        \outcomeD{}{\batch}{t+1})
        =
        \outcomefb{t}
        \left(
        \law(
        \outcomeD{}{}{t}
        ),
        \law(
        \treatment{}{t+1}{}
        ),
        \law(
        \outcomeD{}{\batch}{t}
        ),
        \law(
        \treatment{\batch}{t+1}{}
        )
        \right),
    \end{equation}
    % 
    where $\outcomeD{}{\batch}{t}$ and $\treatment{\batch}{t+1}{}$ represent the weak limits of outcomes and treatments for units in subpopulation~$\batch$, while $\outcomeD{}{}{t}$ and $\treatment{}{t+1}{}$ are their corresponding population-level analogs as defined in Theorem~\ref{thm:SE_informal}.
\end{theorem}
% 
According to Equation \eqref{eq:BSE_informal}, each subpopulation's outcome distribution follows a distinct state evolution equation governed by mappings $\outcomefb{t},\; t=0, \ldots, T-1$. The outcomes for units within $\batch$ evolve through a dynamic interplay between population-level and subpopulation-specific outcomes and treatments. The formal statement of Theorem~\ref{thm:BSE_informal} appears in Theorem~\ref{thm:Batch_SE} of Appendix~\ref{apndx:batch_state_evolution}, where \eqref{eq:state evolution} explicitly defines $\outcomefb{t}$ through mappings $\outcomeg{t}{}$ and $\outcomeh{t}{}$.

To demonstrate the efficacy of \batchingAcronym{}, consider two subpopulations $\batch_1$ and $\batch_2$ such that $\treatment{\batch_1}{t+1}{} \neq \treatment{\batch_2}{t+1}{}$ in \eqref{eq:BSE_informal}. Then, two sequences $\outcomeD{}{\batch_1}{0}, \ldots, \outcomeD{}{\batch_1}{T}$ and $\outcomeD{}{\batch_2}{0}, \ldots, \outcomeD{}{\batch_2}{T}$ yield distinct observations of the state evolution described by \eqref{eq:BSE_informal}. Furthermore, for each time step $t$, the mapping $\outcomefb{t}$ exhibits fundamental similarities to $\outcomef{t}$, with the experimental state evolution equation in \eqref{eq:SE_informal} emerging as a special case of \eqref{eq:BSE_informal} when applied to the entire population. Accordingly, by strategically selecting subpopulations with distinct treatment allocations, we obtain multiple state evolution observations, enabling the estimation of $\outcomefb{t}$ and, consequently, $\outcomef{t}$.
% 
\begin{example}
    \label{exmpl:BernoulliD_subpopulation}
    Consider two distinct subpopulations: treated units ($\batch=\Tc$) and control units ($\batch=\Cc$). From \eqref{eq:BSE_informal}, we have:
    % 
    \begin{equation}
    \label{eq:BD_SP_SEs}
        \begin{aligned}
        \law(
        \outcomeD{}{\Tc}{t+1})
        =
        \outcomefb{t}\left(
        \law(
        \outcomeD{}{}{t}
        ),
        \law(
        \treatment{}{t+1}{}
        ),
        \law(
        \outcomeD{}{\Tc}{t}
        ),
        1
        \right),
        \\
        \law(
        \outcomeD{}{\Cc}{t+1})
        =
        \outcomefb{t}\left(
        \law(
        \outcomeD{}{}{t}
        ),
        \law(
        \treatment{}{t+1}{}
        ),
        \law(
        \outcomeD{}{\Cc}{t}
        ),
        0
        \right).
        \end{aligned}
    \end{equation}
    % 
    When the direct treatment effect is non-zero, the outcome distributions for the treated group $\outcomeD{}{\Tc}{t}$ and the control group $\outcomeD{}{\Cc}{t}$ are distinct, providing two different observations from the state evolution.
\end{example}
%


\subsection{Outcomes Decomposition Rule}
\label{sec:decomposition_rule}
% 
Building on the finite-sample analysis of AMP algorithms \citep{li2022non}, we obtain the following decomposition rule for the outcome specification in Eq.~\eqref{eq:outcome_function_matrix}.
% 
\begin{theorem}[Unit-level decomposition rule]
    \label{thm:outcome_decomposition}
    Suppose Assumption~\ref{asmp:Gaussian Interference Matrice} holds and let $\Vec{Z}_0, \Vec{Z}_1, \ldots, \Vec{Z}_{T-1}$ denote i.i.d. random vectors in $\R^N$ following $\Nc(0,\frac{1}{N}\I_N)$ distribution. For any unit $i$ and fixed $t \in \{0, \ldots, T-1\}$, we have
    % 
    \begin{equation}
        \label{eq:outcome_decomposition_rule}
        \begin{aligned}
        \outcomeD{}{i}{t+1}(\Mtreatment{t+1}{}) =
        \;&\frac{1}{N} \sum_{j=1}^N \left(\mu^{ij}+\mu_t^{ij}\right) \outcomeg{t}{}\left(\outcomeD{}{j}{t}(\Mtreatment{t}{}), \treatment{j}{t+1}{}, \Vcovar{j} \right) +
        \outcomeh{t}{}\left(\outcomeD{}{i}{t}(\Mtreatment{t}{}), \treatment{i}{t+1}{}, \Vcovar{i} \right)
        \\
        &+
        \sqrt{\sigma^2+\sigma_t^2} \norm{\outcomeg{t}{}\left(\VoutcomeD{}{}{t}(\Mtreatment{t}{}) ,\Vtreatment{}{t+1}{}, \covar\right)} \sumvec{i}{t}  + \noise{i}{t},
        \end{aligned}
    \end{equation}
    % 
    where $\sumvec{i}{t}$ represents a random variable such that $\Sumvec{}{t} := \left(\sumvec{1}{t}, \ldots, \sumvec{N}{t}\right)^\top = \sum_{i=0}^{t} \NPC{i}{t} \Vec{Z}_i$, with $\VNPC{}{t} = (\NPC{0}{t}, \ldots, \NPC{t}{t}, 0, \ldots, 0)^\top \in \R^N$ denoting a random vector that is correlated with $\Vec{Z}_0, \Vec{Z}_1, \ldots, \Vec{Z}_{t}$ and satisfies $\normWO{\VNPC{}{t}} = 1$. Furthermore,
    % 
    \begin{equation*}
        \Wc_1\left(\law\left(\Sumvec{}{t}\right),\Nc\left(0,\frac{1}{N} \I\right)\right) \leq c \sqrt{\frac{t \log N}{N}},
    \end{equation*}
    % 
    where $c$ is a constant independent of $N$ and $t$, $\law(\Sumvec{}{t})$ denotes the probability distribution of~$\Sumvec{}{t}$, and $\Wc_1$ is Wasserstein-1 distance.
\end{theorem}
% 
Equation~\eqref{eq:outcome_decomposition_rule} provides a decomposition of unit outcomes: the first term captures the interference effect, the second term reflects the unit-specific effect, the third term arises from uncertainties in the network structure, and the fourth term accounts for observation noise. We can then adjust the first term in Eq.~\eqref{eq:outcome_decomposition_rule} to reflect the available knowledge about the interference network.
% 
\begin{example}
    When it is known that no interference exists, Eq.~\eqref{eq:outcome_decomposition_rule} simplifies to $\outcomeD{}{i}{t+1}(\Mtreatment{t+1}{}) = \outcomeh{t}{}\big(\outcomeD{}{i}{t}(\Mtreatment{t}{}), \treatment{i}{t+1}{}, \Vcovar{i} \big) + \noise{i}{t}$, representing a general specification for the potential outcomes.
\end{example}
% 
% 
\begin{example}
    Consider a scenario where the interference network is partially known and exhibits a clustered structure with clusters $C^1, \ldots, C^K$ (e.g., the experimental population consists of individuals from different cities). In this context, Eq.~\eqref{eq:outcome_decomposition_rule} can be rewritten as follows:
    % 
    \begin{equation*}
        \begin{aligned}
        \outcomeD{}{i}{t+1}(\Mtreatment{t+1}{}) =
        \;&\frac{1}{N} \sum_{k=1}^K 
        \Bigg[
        \sum_{j \in C^k, i \notin C^k} \left(\mu^{ij}+\mu_t^{ij}\right) \outcomeg{t}{}\left(\outcomeD{}{j}{t}(\Mtreatment{t}{}), \treatment{j}{t+1}{}, \Vcovar{j} \right)
        \\
        \;&+
        \sum_{i,j \in C^k} \left(\mu^{ij}+\mu_t^{ij}\right) \outcomeg{t}{}\left(\outcomeD{}{j}{t}(\Mtreatment{t}{}), \treatment{j}{t+1}{}, \Vcovar{j} \right)
        \Bigg]
        \\
        \;&+
        \outcomeh{t}{}\left(\outcomeD{}{i}{t}(\Mtreatment{t}{}), \treatment{i}{t+1}{}, \Vcovar{i} \right)
        +
        \sqrt{\sigma^2+\sigma_t^2} \norm{\outcomeg{t}{}\left(\VoutcomeD{}{}{t}(\Mtreatment{t}{}) ,\Vtreatment{}{t+1}{}, \covar\right)} \sumvec{i}{t}  + \noise{i}{t},
        \end{aligned}
    \end{equation*}
    % 
    where we expect the magnitude of $\left(\mu^{ij} + \mu_t^{ij}\right)$ to be relatively larger whenever $i, j \in C^k$, indicating that units within the same cluster exhibit stronger ties.
\end{example}
% 


% 
The outcome decomposition rule in Eq.~\eqref{eq:outcome_decomposition_rule} leads to the following corollary, which characterizes the decomposition of the outcome sample mean for units belonging to subpopulation $\batch$.
% 
\begin{corollary}[Subpopulation-level decomposition rule]
    \label{crl:SampleMean_decomposition}
    Under the assumptions of Theorem~\ref{thm:outcome_decomposition}, we have
    %
    \begin{equation}
        \label{eq:SampleMean_decomposition}
        \begin{aligned}
        \frac{1}{\cardinality{\batch}} \sum_{i \in \batch} \outcomeD{}{i}{t+1}(\Mtreatment{t+1}{})
        =
        \;&\frac{1}{N \cardinality{\batch}}
        \sum_{i \in \batch}
        \sum_{j=1}^N \left(\mu^{ij}+\mu_t^{ij}\right) \outcomeg{t}{}\left(\outcomeD{}{j}{t}(\Mtreatment{t}{}), \treatment{j}{t+1}{}, \Vcovar{j} \right)
        \\
        \;&+
        \frac{1}{\cardinality{\batch}} \sum_{i \in \batch}
        \outcomeh{t}{}\left(\outcomeD{}{i}{t}(\Mtreatment{t}{}), \treatment{i}{t+1}{}, \Vcovar{i} \right)
        \\
        \;&+
        \sqrt{ \frac{\sigma^2+\sigma_t^2}{{\cardinality{\batch}}}} \norm{\outcomeg{t}{}(\VoutcomeD{}{}{t}(\Mtreatment{t}{}) ,\Vtreatment{}{t+1}{}, \covar)} \avesumvec{}{t}
        +
        \frac{1}{\cardinality{\batch}}
        \sum_{i \in \batch} \noise{i}{t}.
        \end{aligned}
    \end{equation}
% 
    where $\cardinality{\batch}$ denotes the size of the subpopulation $\batch$ and $\avesumvec{}{t}$ is a random variable satisfying
% 
    \begin{equation*}
        \Wc_1\left(\law\left(\avesumvec{}{t}\right),\Nc\left(0,\frac{1}{N} \right)\right) \leq c \sqrt{\frac{t \log N}{N}}.
    \end{equation*}
\end{corollary}
% 
While Theorem~\ref{thm:BSE_informal} establishes that distinct subpopulations yield different state evolution observations, Corollary~\ref{crl:SampleMean_decomposition} reveals two crucial implications for finite-sample analysis. First, the outcome distribution within each subpopulation incorporates unit-level heterogeneities, as demonstrated by the first two terms in Eq.\eqref{eq:SampleMean_decomposition}. Second, the outcome uncertainty terms—represented by the final two expressions in Eq.\eqref{eq:SampleMean_decomposition}—decay at a rate of $\sqrt{\cardinality{\batch}}$. The subsequent sections examine these implications in detail, showing how \batchingAcronym{} addresses such observational complexities while generating additional samples.


\subsection{Unit-level Heterogeneities}
% 
Experimental units exhibit distinct covariates, such as age and gender. Network interference also emerges through diverse local patterns, jointly inducing heterogeneous characteristics. These heterogeneities are captured in \eqref{eq:SampleMean_decomposition} through variations in $\mu^{ij}$, $\mu_t^{ij}$, and $\Vcovar{i}$. This foundation motivates our strategy of selecting members for each subpopulation $\batch$ solely based on treatment allocation. Specifically, this approach enables us to treat $\batch$ as a random sample, supporting the assumption that each subpopulation represents the original experimental population. Indeed, Theorem~\ref{thm:BSE_informal} demonstrates the effectiveness of this batching strategy, evidenced by the invariance of $\outcomefb{t}$ across subpopulations. Meanwhile, the result of Corollary~\ref{crl:SampleMean_decomposition} suggests maximizing subpopulation sizes to leverage stronger smoothing effects through averaging over larger groups of units.


\subsection{Endogenous Observation Noise}
% 
The noise terms in the outcome decomposition rule exhibit complex correlation structures. According to Theorem~\ref{thm:outcome_decomposition}, the random vectors $\Sumvec{}{0}, \ldots, \Sumvec{}{T-1}$ in Eq.~\eqref{eq:outcome_decomposition_rule} asymptotically follow a centered Gaussian distribution as population size $N$ increases, thus functioning as noise terms. These vectors, however, demonstrate a complex endogenous dependency structure through several mechanisms.

Initially, for each $t$, the random vector $\Sumvec{}{t}$ derives its randomness from $\Vec{Z}_0, \Vec{Z}_1, \ldots, \Vec{Z}_{t}$. Consequently, the random vectors $\Sumvec{}{0}, \ldots, \Sumvec{}{T-1}$ exhibit correlation, generating temporal correlation in the observational noise.

Additionally, in finite populations where $N$ is bounded, the elements of $\sumvec{i}{t}$ demonstrate cross-unit correlation. The magnitude of this correlation is governed by the constant $c$ in Theorem~\ref{thm:outcome_decomposition}, which varies across settings and remains unidentified.

Furthermore, the potential outcome of unit $i$ at time $t$ denoted by $\outcomeD{}{i}{t}(\Mtreatment{t}{})$ incorporates $\sumvec{i}{t-1}$, which correlates with elements of $\Sumvec{}{t}$; this introduces correlation between noise terms and unit outcomes. The strength of this correlation depends on the magnitudes of $\sigma$ and $\sigma_t$, which remain unknown and can vary substantially across different settings. 

These three factors collectively define a complex endogenous dependency structure, indicating how uncertainty in the interference structure can compromise outcome measurements and introduce substantial bias when overlooked. The following example examines an autoregressive model with unanticipated unit interactions, demonstrating how even minimal second-order interference induces endogeneity and generates biased estimates.
% 
\begin{example}
    \label{exmp:bias_of_correlation}
    Consider a simplified version of the outcome specification in Eq.~\eqref{eq:outcome_function_matrix} where $\outcomeg{t}{}(\outcomeD{}{i}{t}(\Mtreatment{t}{}), \treatment{i}{t+1}{}, \Vcovar{i}) = \outcomeD{}{i}{t}(\Mtreatment{t}{})$ and $\outcomeh{t}{}(\outcomeD{}{i}{t}(\Mtreatment{t}{}), \treatment{i}{t+1}{}, \Vcovar{i}) = \CE^i \outcomeD{}{i}{t}(\Mtreatment{t}{})$, with $\mu^{ij} + \mu_t^{ij} = 0$ for all $i$, $j$, and $t$. These conditions reflect the available partial information confirming negligible interference between units. Also, assume $\noise{i}{t}$ is independent from all other variables. We can write:
    % 
    \begin{equation*}
        \begin{aligned}
        \outcomeD{}{i}{t+1}(\OMtreatment{t+1}{}) =
        \CE^i \outcomeD{}{i}{t}(\OMtreatment{t}{})
        +
        \sqrt{\sigma^2+\sigma_t^2} \norm{\VoutcomeD{}{}{t}(\OMtreatment{t}{})} \sumvec{i}{t}  + \noise{i}{t}.
        \end{aligned}
    \end{equation*}
    % 
    To estimate $\CE^i$ for a specific unit $i$, we employ ordinary least squares regression, regressing outcomes $\outcomeD{}{i}{1}(\OMtreatment{1}{}), \ldots, \outcomeD{}{i}{T}(\OMtreatment{T}{})$ on their corresponding lagged values $\outcomeD{}{i}{0}(\OMtreatment{0}{}), \ldots, \outcomeD{}{i}{T-1}(\OMtreatment{T-1}{})$. Under the asymptotic regime where $N \rightarrow \infty$, we characterize the bias of estimator $\hat{\Theta}^i$ as follows:
    % 
    \begin{align*}
        \E\left[
        \hat{\Theta}^i
        \Big|
        \Moutcome{}{}{}(\OMtreatment{}{})
        \right]
        -
        \CE^i
        &=
        \frac{1}{\sum_{t=0}^{T-1} \outcomeD{}{i}{t}(\OMtreatment{t}{})^2}
        \sum_{t=0}^{T-1}
        \sqrt{\sigma^2+\sigma_t^2} 
        \norm{\VoutcomeD{}{}{t}(\OMtreatment{t}{})}
        \outcomeD{}{i}{t}(\OMtreatment{t}{})
        \E\left[
        \sumvec{i}{t}
        \big|
        \Moutcome{}{}{}(\OMtreatment{}{})
        \right].
    \end{align*}
    % 
\end{example}
% 

Based on the results of Corollary~\ref{crl:SampleMean_decomposition}, the magnitude of this complex noise diminishes as the subpopulation size $\cardinality{\batch}$ increases, supporting the selection of larger subpopulations. Therefore, in the extreme case, when $\batch$ encompasses the entire population ($\cardinality{\batch} = N$), the noise magnitude decays at a rate of $\sqrt{N}$.


\subsection{How to use \batchingAcronym{} to generate efficient samples?}
\label{sec:batching}
%
Both unit-level heterogeneities and endogenous observation noise suggest maximizing subpopulation sizes, which would ideally lead to selecting the entire experimental population. 
This approach aligns with the proposal in Section \ref{sec:estimation_theory_informal}, where we observe a single instance of state evolution. 
Therefore, this presents a nuanced trade-off: larger subpopulations increase overlap between groups, reducing variation across subpopulations and thereby decreasing the number of effective samples. Given that the magnitude of endogenous noise varies across different settings, we aim to develop a data-driven methodology to address this trade-off optimally. This leads us to propose a cross-validation method, which we examine in detail in the subsequent section.


\section{Counterfactual Cross Validation}
\label{sec:C-CV}
%
The precision of estimated counterfactuals, as established in Theorem~\ref{thm:consistency_informal}, depends on accurately estimating the state evolution mappings $\outcomef{t}$. This estimation process faces two main challenges that require careful validation:

\subsubsection*{Function approximation and model selection} 

The true specification of potential outcomes is often unknown, requiring us to approximate the state evolution mappings $\outcomef{t}$. This approximation involves two related aspects. First, we must choose appropriate summary statistics of the joint distribution of outcomes and treatments to serve as inputs to our model - an approach analogous to feature engineering in supervised learning. For example, \cite{shirani2024causal} used sample means of outcomes, treatments, and their products. While domain knowledge can guide this selection, we need a systematic way to validate these choices.
Second, depending on the complexity of experimental settings, we may need to employ a range of estimation techniques, from simple linear regression to more sophisticated methods such as neural networks. The choice of technique and its specific implementation (e.g., architecture, hyperparameters) must be validated to prevent issues like model instability and estimation bias. These two aspects are linked, as both contribute to how well we can approximate the true mapping $\outcomef{t}$. A data-driven validation methodology helps us jointly optimize these choices while reducing misspecification risks.


\subsubsection*{Optimal batch configuration}
% 
As established in \S\ref{sec:batching}, \batchingAcronym{} offers an effective strategy for addressing unit-level heterogeneities and controlling endogenous noise through batching. However, the choice of batch size presents a bias-variance trade-off. Larger batches better average out heterogeneities and noise, reducing estimation bias, but they also decrease the number of distinct batches available for learning, increasing estimation variance. Conversely, smaller batches provide more samples for learning, reducing variance, but may retain more heterogeneity and noise, potentially introducing bias.
Beyond batch size, the number of batches creates a similar trade-off that interacts with the batch size selection. Therefore, data-driven validation helps find the optimal combination of batch size and number of batches that balances these competing effects in a given experimental context.




\subsection{Counterfactual Cross Validation Algorithm}
\label{sec:CCV_Algorithm}
% 
To address these challenges, we propose a counterfactual cross-validation algorithm that divides the time horizon into a list of time blocks $\tblockList$, which serve as natural cross-validation folds. For each time block, we use all other blocks as training data and the held-out block as validation data. The training process involves generating batches as discussed above, with their size and number serving as tuning parameters. For validation, we use a fixed set of pre-specified validation batches $\batch_1^v, \ldots, \batch_{b_v}^v$ constructed as follows. We begin by computing each unit's average treatment exposure across the experimental horizon (e.g., a unit that is treated in 60\% of the time periods receives a treatment exposure of 0.6). We then rank the units in descending order based on their treatment exposure values and partition them into $b_v$ equal-sized groups, ensuring validation batches cover the full spectrum of treatment exposure. For each fold, the validation metric is averaged across these validation batches to provide a robust performance measure.

The parameters being selected through this cross-validation include both the choice of estimator from a candidate set $\estimatorList$ and the batching configuration from a candidate set $\batchList$. Each estimator $\estimator \in \estimatorList$ represents a specific combination of feature generation (what summary statistics to use) and estimation method (e.g., linear regression or neural network). The batching parameters $(\batchSize,\batchCount)\in\batchList$ specify the size $\batchSize$ and number $\batchCount$ of training batches. For each held-out time block, we train models using all possible combinations of these parameters on the remaining blocks and evaluate their performance on the validation batches obtained from the held-out block. 

\subsubsection*{Step 1: Reference counterfactual construction}
% 
After the experiment is completed, we obtain the following data. The treatment matrix $\OMtreatment{}{}$ and observed outcomes matrix, $\Moutcome{}{}{}(\OMtreatment{}{})$. 
We first compute the sample mean of outcomes over time as ground truth (counterfactual) for evaluating estimations in subsequent steps. More precisely, for each time period $t\in\{0,1,\ldots,T\}$, and each validation batch $\batch_{j}^v$ with $j\in \{1,\ldots,b_v\}$, we calculate average of outcomes at time $t$ across units in $\batch_{j}^v$ and denote that by $\CFETest{t}{\batch_{j}^v}$.

\subsubsection*{Step 2: Leave-one-out and counterfactual estimation}
% 
For each time block $\tblock\in\tblockList$, we construct training datasets $\outcomeTrain^{-\tblock}$ and $\treatmentTrain^{-\tblock}$ as submatrices of $\Moutcome{}{}{}(\OMtreatment{}{})$ and $\OMtreatment{}{}$, respectively, excluding columns within $\tblock$. We define $\treatmentTest^{\tblock}$ as the submatrix of $\OMtreatment{}{}$ containing only columns in $\tblock$. Each estimator $\estimator$ is then trained using $\outcomeTrain^{-\tblock}$ and $\treatmentTrain^{-\tblock}$, and used to estimate counterfactuals for treatment allocation $\treatmentTest^{\tblock}$ during the held-out time block $\tblock$.

\subsubsection*{Step 3: Optimal Estimator Selection}
% 
Following the estimation across all time blocks, we identify the optimal estimator and batch parameters by comparing the results with observed counterfactuals using a predetermined loss function. For instance, using mean square error:
% 
\begin{align*}
    \MSE_{\estimator,\batchSize,\batchCount}
    =
    \frac{1}{b_v (T+1)}
    \sum_{j=1}^{b_v}
    \sum_{t=0}^T
    \Big[\CFETest{t}{\batch_{j}^v} - 
    \ECFTrain{t}{\batch_{j}^v}{\estimator,\batchSize,\batchCount}
    \Big]^2.
\end{align*}
%

\begin{algorithm}
\caption{Counterfactual cross-validation}\label{alg:C-CV}
\begin{algorithmic}
% 
\Require Treatment allocation $\OMtreatment{}{}$, observed outcomes $\Moutcome{}{}{}(\OMtreatment{}{})$, validation batches $\{\batch_j^v\}_{j=1}^{b_v}$, time blocks $\tblockList$, loss function $\criteria:\R^{(T+1) b_v}\times\R^{(T+1)\times b_v}\to\R_+$, and candidate estimators $\estimatorList$ and batch parameters $\batchList$
%%%
\State \hspace{-1.3em} \textbf{Step 1: Reference Counterfactual Construction}
% 
\State $\big\{ \CFETest{t}{\batch_{j}^v} \big\}_{t = 0}^T \gets$ mean of $\Moutcome{}{}{}(\OMtreatment{}{})$ for units belonging to $\batch_{j}^v,\; j=1, \ldots, b_v$
%%%
\State \hspace{-1.3em} \textbf{Step 2: Leave-one-out and Counterfactual Estimation}
% 
\For{$\estimator,\batchSize,\batchCount \in \estimatorList\times\batchList$}
    % 
    % 
    \For{$\tblock \in \tblockList$}
        % 
        \State $\outcomeTrain^{-\tblock} \gets$ columns of $\Moutcome{}{}{}(\OMtreatment{}{})$ outside of $\tblock$
        % 
        \State $\treatmentTrain^{-\tblock} \gets$ columns of $\OMtreatment{}{}$ outside of $\tblock$
        % 
        \State $\treatmentTest^{\tblock} \gets$ columns of $\OMtreatment{}{}$ in $\tblock$
        %
        \State Train $\estimator$ with batching parameters  $(\batchSize,\batchCount)$ on data $\outcomeTrain^{-\tblock}$ and $\treatmentTrain^{-\tblock}$ 
        %%%
        \For{$j \in \{1,\ldots,b_v\}$}
        %
            \State $\big\{ \ECFTrain{t}{\batch_{j}^v}{\estimator,\batchSize,\batchCount} \big\}_{t \in \tblock} \gets$ estimate CFE for $\treatmentTest^{\tblock}$ via the trained model, for units in $\batch_j^v$
        \EndFor 
    \EndFor
    % 
\EndFor
%%%
\State \hspace{-1.3em} \textbf{Step 3: Optimal Estimator Selection}
% 
\State $\estimator^{*},\batchSize^{*},\batchCount^{*} \gets \arg\min_{(\estimator,\batchSize,\batchCount)\in\estimatorList\times\batchList} \criteria \left(\Big\{\big\{ \CFETest{t}{\batch_{j}^v} \big\}_{t = 0}^T\Big\}_{j=1}^{b_v}, \Big\{\big\{ \ECFTrain{t}{\batch_{j}^v}{\estimator,\batchSize,\batchCount} \big\}_{t = 0}^T\Big\}_{j=1}^{b_v}\right)$
% 
\end{algorithmic}
\end{algorithm}

\begin{remark}
    \label{rem:estimators}
    Algorithms~\ref{alg:FO-recursive} and~\ref{alg:FO_with_preprocessing}, presented in Appendices~\ref{sec:estimation_theory} and~\ref{sec:preprocessing} respectively, provide examples of candidate estimators based on linear regression.
\end{remark}

\begin{remark}
    \label{rem:proprocessing}
    In experimental settings with strong temporal patterns such as seasonality, we first \emph{\preprocess} the observed outcomes, e.g., see Algorithm~\ref{alg:FO_with_preprocessing} in Appendix~\ref{sec:preprocessing}. This procedure augments Step 2 of Algorithm~\ref{alg:C-CV} as follows. For each estimator $\estimator$, we utilize the complete observed dataset to estimate a baseline counterfactual—defined as the counterfactual outcome for all units under control—using an estimation model optimized for temporal pattern detection. We then generate filtered data by subtracting this baseline counterfactual from observed outcomes. This filtered data serves as input for the main estimator, which focuses specifically on treatment effect identification. The final estimates are obtained by combining the baseline counterfactual with the estimated treatment effects. The consistency of this estimation approach relies on a structural assumption of weak additive separability between baseline outcomes and treatment effects.
\end{remark}