\section*{Appendices Organization}
\label{appendices}
% 
These appendices provide detailed statements and proofs of theoretical results that support the main body of the paper.

The first appendix presents the technical results, beginning with essential notation and a reformulation of the outcome specification. We then establish the outcome decomposition rules through a sequence of proofs. This begins with Lemmas~\ref{lm:outcome_decomposition}-\ref{lm:apndx_outcome_decomposition}, culminating in the proof of Theorem~\ref{thm:outcome_decomposition} and Corollary~\ref{crl:SampleMean_decomposition}. The discussion then progresses to a rigorous statement of batch-level state evolution in \S\ref{apndx:batch_state_evolution}, which was informally introduced in Theorem~\ref{thm:BSE_informal}. This is followed by a brief overview of the conditioning technique in \S\ref{apndx:Conditioning_Technique}, which is essential for proving the state evolution equation, as detailed in \S\ref{apndx:big_lemma}.

Appendix~\ref{sec:estimation_theory} demonstrates how consistent estimation of state evolution parameters enables consistent estimation of desired counterfactuals. We present the necessary assumptions, provide and prove the main theoretical results, and detail the corresponding algorithms. We then analyze the application of these results to Bernoulli randomized designs, concluding in presenting two families of estimators in \S\ref{apndx:semi-recursive_estimators} and \S\ref{apndx:recursive_estimators}.

Building on these results, \S\ref{sec:preprocessing} presents an extension to the causal message-passing methodology that addresses strong time-trends, enabling general counterfactual estimation even in the presence of seasonality or temporal patterns. The appendices conclude with \S\ref{apndx:auxiliary_results}, which presents auxiliary theorems necessary for our main proofs.


\section{Technical Results}
\label{sec:Technical_Results}
% 
In this section, we delve into the analysis of the outcome specification in Eq.~\eqref{eq:outcome_function_matrix}. Our analysis draws upon various results from the literature on approximate message-passing algorithms \citep{donoho2009message,bayati2011dynamics, rush2018finite,li2022non}.

In the following, we first introduce several notations necessary for the rigorous presentation of our theoretical results. We then rewrite the potential outcome specification, facilitating the ensuing discussions. Next, we provide a rigorous proof for the outcome decomposition rule. This is rooted in the non-asymptotic results for the analysis of AMP algorithms \citep{li2022non} and can be seen as the finite sample analysis of the causal message-passing framework \citep{shirani2024causal}, within a broader class of outcome specifications.

\subsection{Notations}
\label{apndx:notations}
% 
For any vector $\Vec{v} \in \R^n$, we denote its Euclidean norm as $\norm{\Vec{v}}$. For a fixed $k \geq 1$, we define $\poly{k}$ as the class of functions $f: \R^n \rightarrow \R$ that are continuous and exhibit polynomial growth of order $k$. In other words, there exists a constant $c$ such that $|f(\Vec{v})| \leq c(1+\norm{\Vec{v}}^k)$. Moreover, we consider a probability space $(\Omega, \F, \P)$, where $\Omega$ represents the sample space, $\F$ is the sigma-algebra of events, and $\P$ is the probability measure. We denote the expectation with respect to $\P$ as $\E$. Additionally, for any other probability measure~$p$, we use $\E_p$ to denote the expectation with respect to $p$. 

For any set $S$, the indicator function $\1_S(\omega)$ evaluates to $1$ if $\omega$ belongs to $S$, and $0$ otherwise. We define $\R^{n\times m}$ as the set of matrices with $n$ rows and $m$ columns. Given a matrix $\bm{M}$, we denote its transpose as~$\bm{M}^\top$. Additionally, we represent a matrix of ones with dimensions $n\times m$ as $\ones{n\times m} \in \R^{n\times m}$. The symbol $\eqd$ is used to denote equality in distribution, while $\eqas$ is used for equalities that hold almost surely with respect to the reference probability measure $\P$.

\subsection{Preliminaries}
\label{apndx:Preliminaries}
% 
We initiate by rewriting the outcome model in Eq.~\eqref{eq:outcome_function_matrix}. Specifically, we consider the following more general specification:
% 
\begin{equation}
    \label{eq:apndx_outcome_function}
    \begin{aligned}
        \VoutcomeD{}{}{t+1}(\Mtreatment{}{}) =
        \Voutcome{}{}{t+1} +
        \IMatMean{t} \outcomeg{t}{}\left(\VoutcomeD{}{}{0}(\Mtreatment{}{}), \ldots, \VoutcomeD{}{}{t}(\Mtreatment{}{}), \Mtreatment{}{}, \covar\right) +
        \outcomeh{t}{}\left(\VoutcomeD{}{}{0}(\Mtreatment{}{}), \ldots, \VoutcomeD{}{}{t}(\Mtreatment{}{}), \Mtreatment{}{}, \covar \right)
        + \Vnoise{}{t},
    \end{aligned}
\end{equation}
% 
where
% 
\begin{align}
    \label{eq:apndx_outcome_function_WOD}
    \Voutcome{}{}{t+1} = \big( \IM + \IMatT{t} \big) \outcomeg{t}{}\left(\VoutcomeD{}{}{0}(\Mtreatment{}{}), \ldots, \VoutcomeD{}{}{t}(\Mtreatment{}{}), \Mtreatment{}{}, \covar\right).
\end{align}
% 
Above, $\IMatMean{t}$ is the matrix with the element $(\mu^{ij} + \mu_t^{ij})/N$ in the $i^{th}$ row and $j^{th}$ column. With a slight abuse of notation, we consider the entries of matrices $\IM$ and $\IMatT{t}$ to have zero mean. Formally, we state the following assumption regarding the interference matrices.
%
\begin{assumption}
    \label{asmp:apndx_Gaussian Interference Matrice}
    Entries of $\IM$ are i.i.d. Gaussian variables with zero mean and variance $\sigma^2/N$. Similarly, for any $t$, entries of $\IMatT{t}$ are i.i.d. Gaussian variables with zero mean and variance $\sigma^2_t/N$, independent of other components of the model.
\end{assumption}
%

Compared to \eqref{eq:outcome_function_matrix}, the specification in \eqref{eq:apndx_outcome_function} and \eqref{eq:apndx_outcome_function_WOD} is more comprehensive, incorporating both the complete outcome history and the full treatment allocation matrix. This generalization enables us to capture more complex temporal dynamics, including additional lag terms and potential anticipation effects of treatments. The functions $\outcomeg{t}{}$ and $\outcomeh{t}{}$ can also be specified to include only a finite number $l \in \N$ of historical lag terms. While all subsequent results hold for the general model in \eqref{eq:apndx_outcome_function} and \eqref{eq:apndx_outcome_function_WOD}, for notational simplicity, we present the proofs using only one lag term $\VoutcomeD{}{}{t}(\Mtreatment{}{})$. Furthermore, when the context is clear, we omit the treatment matrix notation $\Mtreatment{}{}$ and write simply $\VoutcomeD{}{}{t}$ as the vector of outcomes at time $t$.



\subsection{Outcome Decomposition Rule}
\label{apndx:OD_Rule}
% 
Fixing $N$, we proceed by setting more notations. Given $\VoutcomeD{}{}{0}$ as the vector of initial outcomes,  for $1\leq t < N$, define:
% 
\begin{equation}
    \label{eq:apndx_orthonormal_outcomes_vectors}
    \VOCO{}{}{0} :=
    \frac{\outcomeg{0}{}\left(\VoutcomeD{}{}{0} ,\Mtreatment{}{}{}, \covar\right)}{\norm{\outcomeg{0}{}\left(\VoutcomeD{}{}{0} ,\Mtreatment{}{}{}, \covar\right)}},
    \quad\quad\quad\quad
    \VOCO{}{}{t} :=
    \frac{\left(\I_{N} - \MOCO{}{}{t-1}\MOCO{}{\top}{t-1}\right) \outcomeg{t}{}\left(\VoutcomeD{}{}{t} ,\Mtreatment{}{}{}, \covar\right)}{\norm{\left(\I_{N} - \MOCO{}{}{t-1}\MOCO{}{\top}{t-1}\right) \outcomeg{t}{}\left(\VoutcomeD{}{}{t} ,\Mtreatment{}{}{}, \covar\right)}},
\end{equation}
% 
where, $\I_{N}$ is $N \x N$ identity matrix, and
% 
\begin{equation}
    \label{eq:apndx_orthonormal_outcomes_matrix}
    \MOCO{}{}{t-1} = \left[\VOCO{}{}{0}\Big|\ldots\Big|\VOCO{}{}{t-1}\right].
\end{equation}
% 
Note that $\I_{N} - \MOCO{}{}{t-1}\MOCO{}{\top}{t-1}$ functions as a projection onto the subspace that is orthogonal to the column space of $\MOCO{}{}{t-1}$. As a result, the vectors $\{\VOCO{}{}{0}, \ldots, \VOCO{}{}{N-1}\}$ constitute an orthonormal basis by definition. Therefore, we can represent the vector $\outcomeg{t}{}\left(\VoutcomeD{}{}{t} ,\Mtreatment{}{}{}, \covar\right)$ with respect to this basis as $\VPC{}{t} := (\PC{0}{t},\ldots,\PC{t}{t}, 0,\ldots,0)^\top \in \R^{N}$; that is:
% 
\begin{equation}
    \label{eq:apndx_representation_in_the_new_basis}
    \outcomeg{t}{}\left(\VoutcomeD{}{}{t} ,\Mtreatment{}{}{}, \covar\right) =
    \sum_{j=0}^t \PC{j}{t} \VOCO{}{}{j},
    \quad\quad\quad\quad
    \PC{j}{t} = \pdot{\outcomeg{t}{}\left(\VoutcomeD{}{}{t} ,\Mtreatment{}{}{}, \covar\right)}{\VOCO{}{}{j}}.
\end{equation}
% 
Then, it is immediate to get $\norm{\VPC{}{t}} = \normWO{\outcomeg{t}{}(\VoutcomeD{}{}{t} ,\Mtreatment{}{}{}, \covar)}$. In addition, we let $\MOCO{}{\perp}{t-1} \in \R^{N\times (N-t)}$ denote the orthogonal complement of $\MOCO{}{}{t-1}$ such that $(\MOCO{}{\perp}{t-1})^\top \MOCO{}{\perp}{t-1} = \I_{N-t}$.

We also define the following sequence of matrices based on the fixed interference matrix $\IM$:
% 
\begin{equation}
    \label{eq:apndx_IM_recursion}
    \IMat{0} := \IM,
    \quad\quad\quad\quad
    \IMat{t} := \IMat{t-1} \left(\I_{N} - \VOCO{}{}{t-1}\VOCO{}{\top}{t-1}\right).
\end{equation}
% 
Then,
Eq.~\eqref{eq:apndx_IM_recursion} enables us to write:
% 
\begin{equation}
    \label{eq:apndx_representation_of_IM}
    \IMat{0} = \IMat{t} + \sum_{j=0}^{t-1} \left(\IMat{j}-\IMat{j+1}\right) = \IMat{t} + \sum_{j=0}^{t-1} \IMat{j} \VOCO{}{}{j}\VOCO{}{\top}{j}.
\end{equation}
% 
Further, for $t=1, \ldots,N$, we define the following sequence of matrices:
% 
\begin{equation}
    \label{eq:apndx_new_IM_matrix} 
    \IMatnew{t} := \IMat{t} \MOCO{}{\perp}{t-1}
    = \IMat{t-1} \left(\I_{N} - \VOCO{}{}{t-1}\VOCO{}{\top}{t-1}\right) \MOCO{}{\perp}{t-1}
    =
    \IMat{t-1} \MOCO{}{\perp}{t-1}
    = \ldots = \IM \MOCO{}{\perp}{t-1}
    \in \R^{N \x (N-t)},
\end{equation}
% 
where we used the fact that $\VOCO{}{\top}{s-1} \MOCO{}{\perp}{t-1} = \Vec{0} \in \R^{N-t}$, for any $s \leq t$. Also, by \eqref{eq:apndx_IM_recursion}, we can write
% 
\begin{equation}
    \label{eq:apndx_new_IM_matrix_2}
    \IMat{t} = \IMat{t-1} \left(\I_{N} - \VOCO{}{}{t-1}\VOCO{}{\top}{t-1}\right) = \ldots = \IM \left(\I_{N} - \MOCO{}{}{t-1}\MOCO{}{\top}{t-1}\right) = \IM \MOCO{}{\perp}{t-1} (\MOCO{}{\perp}{t-1})^\top = \IMatnew{t} (\MOCO{}{\perp}{t-1})^\top.
\end{equation}
% 

We prove the outcome decomposition rule in multiple steps, beginning with the following lemma.
% 
\begin{lemma}
    \label{lm:outcome_decomposition}
    For any $t = 0, \ldots, N-1$, we have
    % 
    \begin{equation}
        \label{eq:apndx_outcome_decomposition}
        \VoutcomeD{}{}{t+1} =
        \IMatMean{t} \outcomeg{t}{}\left(\VoutcomeD{}{}{t}, \Mtreatment{}{}{}, \covar{} \right)
        +
        \sum_{j=0}^{t} \PC{j}{t} \left(\IMat{j} + \IMatT{t}\right) \VOCO{}{}{j} 
        + \outcomeh{t}{}\left(\VoutcomeD{}{}{t}, \Mtreatment{}{}{}, \covar{} \right)
        + \Vnoise{}{t}.
    \end{equation}
    % 
\end{lemma}
% 
\textbf{Proof.} By outcome model given in \eqref{eq:apndx_outcome_function} and \eqref{eq:apndx_representation_of_IM}, we can write:
% 
\begin{equation*}
    \begin{aligned}
    \VoutcomeD{}{}{t+1} 
    &=
    \IMat{t} \outcomeg{t}{}\left(\VoutcomeD{}{}{t}, \Mtreatment{}{}{}, \covar\right)
    + \sum_{j=0}^{t-1} \IMat{j} \VOCO{}{}{j}\VOCO{}{\top}{j} \outcomeg{t}{}\left(\VoutcomeD{}{}{t}, \Mtreatment{}{}{}, \covar\right)
    + \big(\IMatMean{t} + \IMatT{t}\big) \outcomeg{t}{}\left(\VoutcomeD{}{}{t}, \Mtreatment{}{}{}, \covar\right)
    + \outcomeh{t}{}\left(\VoutcomeD{}{}{t}, \Mtreatment{}{}{}, \covar{} \right)
    + \Vnoise{}{t}
    \\
    &=
    \IMat{t} \outcomeg{t}{}\left(\VoutcomeD{}{}{t}, \Mtreatment{}{}{}, \covar\right)
    + \sum_{j=0}^{t-1} \PC{j}{t} \IMat{j} \VOCO{}{}{j}
    + \sum_{j=0}^{t} \PC{j}{t} \IMatT{t} \VOCO{}{}{j}
    + \IMatMean{t} \outcomeg{t}{}\left(\VoutcomeD{}{}{t}, \Mtreatment{}{}{}, \covar{} \right)
    + \outcomeh{t}{}\left(\VoutcomeD{}{}{t}, \Mtreatment{}{}{}, \covar{} \right)
    + \Vnoise{}{t}
    \\
    &=
    \IMatMean{t} \outcomeg{t}{}\left(\VoutcomeD{}{}{t}, \Mtreatment{}{}{}, \covar{} \right)
    +
    \sum_{j=0}^{t} \PC{j}{t} (\IMat{j}+\IMatT{t}) \VOCO{}{}{j}
    + \outcomeh{t}{}\left(\VoutcomeD{}{}{t}, \Mtreatment{}{}{}, \covar{} \right)
    + \Vnoise{}{t},
    \end{aligned}
\end{equation*}
% 
where in the third line, we used \eqref{eq:apndx_representation_in_the_new_basis} and the fact that the vectors $\{\VOCO{}{}{0}, \ldots, \VOCO{}{}{t}\}$ constitute an orthonormal set. Also, in the last line, we utilized the following fact
% 
\begin{equation*}
    \IMat{t} \outcomeg{t}{}\left(\VoutcomeD{}{}{t}, \Mtreatment{}{}{}, \covar\right)
    = \IMat{t} \left(\I_{N} - \MOCO{}{}{t-1}\MOCO{}{\top}{t-1}\right) \outcomeg{t}{}\left(\VoutcomeD{}{}{t}, \Mtreatment{}{}{}, \covar\right)
    = \PC{t}{t} \IMat{t} \VOCO{}{}{t},
\end{equation*}
% 
that holds true because $\IMat{t} \MOCO{}{}{t-1}\MOCO{}{\top}{t-1} = 0$ by \eqref{eq:apndx_IM_recursion}; additionally, the term $\left(\I_{N} - \MOCO{}{}{t-1}\MOCO{}{\top}{t-1}\right) \outcomeg{t}{}\left(\VoutcomeD{}{}{t}, \Mtreatment{}{}{}, \covar\right)$ is equal to the projection of $\outcomeg{t}{}\left(\VoutcomeD{}{}{t}, \Mtreatment{}{}{}, \covar\right)$ on the subspace perpendicular to the column space of $\MOCO{}{}{t-1}$, which is $\PC{t}{t} \VOCO{}{}{t}$. It completes the proof. \ep

The next lemma characterizes the distribution of $\IMatnew{t}$, defined in \eqref{eq:apndx_new_IM_matrix}, based on the rotational invariance property of Gaussian matrices. 
% 
\begin{lemma}
    \label{lm:new_IM_distribution}
    Fix $1 \leq t < N$. Conditional on $\VoutcomeD{}{}{0}$, $\VoutcomeD{}{}{1}, \ldots, \VoutcomeD{}{}{t-1}$, $\Mtreatment{}{}{}$, and $\covar$, entries of the matrix $\IMatnew{t} \in \R^{N \times (N-t)}$ are i.i.d. with distribution $\Nc(0,{\sigma^2}/ {N})$, and the matrix $\IMatnew{t}$ (and so $\IMat{t}$) is independent of $\VoutcomeD{}{}{t},\VOCO{}{}{t},$ as well as $\IMat{0}\VOCO{}{}{0}, \ldots, \IMat{t-1}\VOCO{}{}{t-1}$. 
\end{lemma}
% 
\textbf{Proof.} First, note that given $\Mtreatment{}{}{}$ and $\covar$, by \eqref{eq:apndx_orthonormal_outcomes_vectors}, conditioning on $\VoutcomeD{}{}{0}$, $\VoutcomeD{}{}{1}, \ldots, \VoutcomeD{}{}{t-1}$, is equivalent to conditioning on $\VOCO{}{}{0}$, $\VOCO{}{}{1}, \ldots, \VOCO{}{}{t-1}$. Now, we use an induction on $t$ to prove the result.

\textbf{Step 1.} Let $t=1$. By \eqref{eq:apndx_new_IM_matrix} and the rotational invariance property of Gaussian matrices, we have
% 
\begin{equation}
    \label{eq:apndx_proof_new_IM_distribution_1}
    \IMatnew{1} = \IM \MOCO{}{\perp}{0} \eqd \IM \sbasis{\perp}{1},
\end{equation}
% 
where $\sbasis{\perp}{1}$ denotes the orthogonal complement of $\sbasis{}{1}$, which is the first standard basis vector. Letting $\sbasis{\perp}{1} = [\sbasis{}{2}|\ldots|\sbasis{}{N}]$, by Assumption~\ref{asmp:apndx_Gaussian Interference Matrice}, we get that the entries of $\IMatnew{1} \in \R^{N \times (N-1)}$ are i.i.d. with distribution $\Nc(0,{\sigma^2}/ {N})$. Furthermore, considering that $\MOCO{}{}{0} = \VOCO{}{}{0}$, the matrix $\IMatnew{1}$ is independent from $\IMat{0}\VOCO{}{}{0}$. Then, conditional on $\VoutcomeD{}{}{0}$, the outcome model in \eqref{eq:apndx_outcome_function} implies that $\IMatnew{1}$ is independent of $\VoutcomeD{}{}{1}$ and $\VOCO{}{}{1}$ (note that the randomness of $\VoutcomeD{}{}{1}$ comes from $\IMat{0}\VOCO{}{}{0}$, $\IMatT{0}$, and $\Vnoise{}{0}$). Finally, considering \eqref{eq:apndx_new_IM_matrix_2}, the same results about the independency hold true for the matrix $\IMat{1}$.

\textbf{Step 2.} Suppose that the result is true for $t=1, \ldots, s-1$. Given $\VoutcomeD{}{}{0}, \ldots, \VoutcomeD{}{}{s-1}$, we show the result also holds for $t=s$. Note that by the induction hypothesis, conditional on $\VoutcomeD{}{}{0}, \ldots, \VoutcomeD{}{}{s-2}$, the matrix $\IMat{s-1}$ is independent of $\VoutcomeD{}{}{s-1}$ and $\IMat{0}\VOCO{}{}{0}, \ldots, \IMat{s-2}\VOCO{}{}{s-2}$. Thus, $\IMat{s-1}$ and $\VoutcomeD{}{}{s-1}$ are conditionally independent. This implies that conditional on $\VoutcomeD{}{}{0}, \ldots, \VoutcomeD{}{}{s-1}$ (we added $\VoutcomeD{}{}{s-1}$), the matrix $\IMat{s-1}$ (and so $\IMat{s} := \IMat{s-1} (\I_N - \VOCO{}{}{s-1}\VOCO{}{\top}{s-1})$, see \eqref{eq:apndx_IM_recursion}, as well as $\IMatnew{s}$) is still independent of $\IMat{0}\VOCO{}{}{0}, \ldots, \IMat{s-2}\VOCO{}{}{s-2}$.

Next, we show that $\IMat{s}$ is also independent from $\IMat{s-1}\VOCO{}{}{s-1}$. By \eqref{eq:apndx_new_IM_matrix} and the rotational invariance property of Gaussian matrices, we can write
% 
\begin{equation*}
    \IMatnew{s} = \IM \MOCO{}{\perp}{s-1} \eqd \IM [\sbasis{}{1}| \ldots| \sbasis{}{s}]^\perp.
\end{equation*}
% 
Here, $[\sbasis{}{1}| \ldots| \sbasis{}{s}]^\perp$ represents the orthogonal complement of the first $s$ standard basis vectors. Then, a similar argument to the one in Step 1 implies that the matrix $\IMatnew{s} \in \R^{N\times (N-s)}$ has i.i.d. entries with a distribution of $\Nc(0,{\sigma^2}/ {N})$. Furthermore, it yields that $\IMatnew{s}$ (and consequently $\IMat{s}$, see Eq.~\eqref{eq:apndx_new_IM_matrix_2}) is independent of the vector $\IMat{s-1}\VOCO{}{}{s-1} = \IM (\I_N - \MOCO{}{}{s-2} \MOCO{}{\top}{s-2}) \VOCO{}{}{s-1} = \IM \VOCO{}{}{s-1}$; this holds true because of Eq.~\eqref{eq:apndx_new_IM_matrix_2} and the fact that $\VOCO{}{\top}{j} \VOCO{}{}{s-1} =0$, for $j = 0, \ldots, s-2$. 

Now, by Lemma~\ref{lm:outcome_decomposition}, we have
% 
\begin{equation*}
    \VoutcomeD{}{}{s} =
    \IMatMean{s-1} \outcomeg{s-1}{}\left(\VoutcomeD{}{}{s-1}, \Mtreatment{}{}{}, \covar{} \right)
    \sum_{j=0}^{s-1} \PC{j}{s-1} \left(\IMat{j} + \IMatT{s-1}\right) \VOCO{}{}{j}
    + \outcomeh{s-1}{}\left(\VoutcomeD{}{}{s-1}, \Mtreatment{}{}{}, \covar{} \right)
    + \Vnoise{}{s-1}.
\end{equation*}
% 
As a result, $\IMat{s}$ and $\IMatnew{s}$ are also independent of $\VoutcomeD{}{}{s}$ and $\VOCO{}{}{s}$. This concludes the proof. \ep

By combining the results of Lemmas~\ref{lm:outcome_decomposition} and~\ref{lm:new_IM_distribution}, we arrive at the conclusion of Lemma~\ref{lm:apndx_outcome_decomposition}.
% 
\begin{lemma}
    \label{lm:apndx_outcome_decomposition}
    For any $t = 0, \ldots, N-1$, we have
    % 
    \begin{equation}
        \label{eq:apndx_outcome_decomposition_distribtuion}
        \VoutcomeD{}{}{t+1} =
        \IMatMean{t} \outcomeg{t}{}\left(\VoutcomeD{}{}{t}, \Mtreatment{}{}{}, \covar{} \right) +
        \outcomeh{t}{}\left(\VoutcomeD{}{}{t}, \Mtreatment{}{}{}, \covar{} \right) +
        \sqrt{\sigma^2+\sigma_t^2} \norm{\outcomeg{t}{}(\VoutcomeD{}{}{t} ,\Mtreatment{}{}{}, \covar)} \sum_{j=0}^{t} \NPC{j}{t} \Vec{Z}_j  + \Vnoise{}{t},
    \end{equation}
    % 
    where $\Vec{Z}_0, \Vec{Z}_1, \ldots, \Vec{Z}_t$ are i.i.d. random vectors in $\R^N$ following $\Nc(0,\frac{1}{N}\I_N)$ distribution. Additionally, $\NPC{j}{t} := \PC{j}{t}/\normWO{\outcomeg{t}{}(\VoutcomeD{}{}{t}, \Mtreatment{}{}, \covar)}$, making $\VNPC{}{t} = (\NPC{0}{t}, \ldots, \NPC{t}{t}, 0, \ldots, 0)^\top \in \R^N$ a unit random vector (i.e., $\normWO{\VNPC{}{t}} = 1$). Note that $\NPC{j}{t}$ and $\Vec{Z}_j$ are not independent.
\end{lemma}
% 
\textbf{Proof.}
% 
Fixing $t$, we prove the result in two steps. First, we demonstrate that $\IMat{j} \VOCO{}{}{j}$, for $j=0, \ldots, t$, follows a Gaussian distribution with specified mean and variance. Then, we show that $\IMat{0} \VOCO{}{}{0}, \ldots, \IMat{t} \VOCO{}{}{t}$ are independent. 

Conditional on $\VoutcomeD{}{}{0}$, $\VoutcomeD{}{}{1}, \ldots, \VoutcomeD{}{}{j-1}$, $\Mtreatment{}{}{}$, and $\covar$, Lemma~\ref{lm:new_IM_distribution} implies that the matrix $\IMat{j}$ and the vector $\VOCO{}{}{j}$ are independent. Also, by  \eqref{eq:apndx_new_IM_matrix_2}, we can write
% 
\begin{equation*}
    \IMat{j} \VOCO{}{}{j} =
    \IM \MOCO{}{\perp}{j-1} (\MOCO{}{\perp}{j-1})^\top  \VOCO{}{}{j} = \IM \VOCO{}{}{j},
\end{equation*}
% 
where we used the fact that the vector $\VOCO{}{}{j}$ is perpendicular to the column space of the matrix $\MOCO{}{}{j-1}$. Thus, conditional on the value of $\VOCO{}{}{j}$, as well as $\VoutcomeD{}{}{0}$, $\VoutcomeD{}{}{1}, \ldots, \VoutcomeD{}{}{j-1}$, $\Mtreatment{}{}{}$, and $\covar$, by the rotational invariance property of Gaussian matrices, the elements of $\IMat{j} \VOCO{}{}{j}$ are i.i.d. random variables with distribution $\Nc(0,\frac{\sigma^2}{N})$. Furthermore, note that this conditional distribution of $\IMat{j} \VOCO{}{}{j}$ remains the same regardless of the value of $\VOCO{}{}{j}$. As a result, we can conclude that the elements of $\IMat{j} \VOCO{}{}{j}$ are i.i.d. Gaussian, even without conditioning on $\VOCO{}{}{j}$ as well as $\VoutcomeD{}{}{0}$, $\VoutcomeD{}{}{1}, \ldots, \VoutcomeD{}{}{j-1}$, $\Mtreatment{}{}{}$, and $\covar$. Precisely, for a deterministic vector $\Vec{v}$, we can write:
% 
\begin{align*}
    \P
    \left(
    \IMat{j} \VOCO{}{}{j} \leq \Vec{v}
    \right)
    =
    \E
    \left[
    \E
    \left[
    \P
    \left(
    \IMat{j} \VOCO{}{}{j} \leq \Vec{v}
    \right)
    \Big|
    \VOCO{}{}{j}
    \right]
    \bigg|
    \VoutcomeD{}{}{0}, \ldots, \VoutcomeD{}{}{j-1}, \Mtreatment{}{}{}, \covar
    \right]
    =
    \E
    \left[
    \E
    \left[
    \Phi(\Vec{v})
    \Big|
    \VOCO{}{}{j}
    \right]
    \bigg|
    \VoutcomeD{}{}{0}, \ldots, \VoutcomeD{}{}{j-1}, \Mtreatment{}{}{}, \covar
    \right]
    =
    \Phi(\Vec{v}),
\end{align*}
% 
where $\Phi$ denotes the CDF of a vector whose entries follow a normal distribution $\Nc(0, \frac{\sigma^2}{N})$.

We proceed by establishing the independence property. Note that by Lemma~\ref{lm:new_IM_distribution}, conditional on the values of $\VoutcomeD{}{}{0}, \ldots, \VoutcomeD{}{}{t}, \Mtreatment{}{}{}, \covar$ (and so on the values of $\VOCO{}{}{0}, \ldots, \VOCO{}{}{t-1}, \VOCO{}{}{t}$), it follows that $\IMat{t}$ (and so $\IMat{t} \VOCO{}{}{t}$) is independent of $\IMat{0}\VOCO{}{}{0}, \ldots, \IMat{t-1}\VOCO{}{}{t-1}$. Importantly, our previous demonstration confirmed that the distribution of $\IMat{t} \VOCO{}{}{t}$ remains unchanged across different values of $\VoutcomeD{}{}{0}, \ldots, \VoutcomeD{}{}{t}, \Mtreatment{}{}{}, \covar$. Consequently, we can assert that the random vector $\IMat{t}\VOCO{}{}{t}$ is independent of $\IMat{0}\VOCO{}{}{0}, \ldots, \IMat{t-1}\VOCO{}{}{t-1}$. More precisely, we can repeat this argument multiple times and, for deterministic vectors $\Vec{v}_0, \ldots, \Vec{v_t}$, show that
% 
\begin{align*}
    \P
    \left(
    \IMat{0} \VOCO{}{}{0} \leq \Vec{v}_0,
    \ldots,
    \IMat{t} \VOCO{}{}{t} \leq \Vec{v}_t
    \right)
    &=
    \E
    \left[
    \P
    \left(
    \IMat{0} \VOCO{}{}{0} \leq \Vec{v}_0,
    \ldots,
    \IMat{t} \VOCO{}{}{t} \leq \Vec{v}_t
    \right)
    \Big|
    \VoutcomeD{}{}{0}, \ldots, \VoutcomeD{}{}{t}, \Mtreatment{}{}{}, \covar
    \right]
    \\
    &=
    \E
    \left[
    \P
    \left(
    \IMat{0} \VOCO{}{}{0} \leq \Vec{v}_0,
    \ldots,
    \IMat{t-1} \VOCO{}{}{t-1} \leq \Vec{v}_{t-1}
    \right)
    \Phi(\Vec{v}_t)
    \Big|
    \VoutcomeD{}{}{0}, \ldots, \VoutcomeD{}{}{t}, \Mtreatment{}{}{}, \covar
    \right]
    \\
    &=
    \Phi(\Vec{v}_t)
    \E
    \left[
    \P
    \left(
    \IMat{0} \VOCO{}{}{0} \leq \Vec{v}_0,
    \ldots,
    \IMat{t-1} \VOCO{}{}{t-1} \leq \Vec{v}_{t-1}
    \right)
    \Big|
    \VoutcomeD{}{}{0}, \ldots, \VoutcomeD{}{}{t-1}, \Mtreatment{}{}{}, \covar
    \right]
    \\
    &=
    \ldots
    \\
    &=
    \Phi(\Vec{v}_0)
    \Phi(\Vec{v}_1)
    \ldots
    \Phi(\Vec{v}_t).
\end{align*}
% 

Based on the fact that the time-dependent interference matrix $\IMatT{t}$ and the noise vector $\Vnoise{}{t}$ are independent of everything else in the model, the proof is complete and we obtain the desired result in \eqref{eq:apndx_outcome_decomposition_distribtuion}. \ep


The dependence between $\NPC{j}{t}$ and $\Vec{Z}_j$ in Lemma~\ref{lm:apndx_outcome_decomposition} implies that the Gaussianity of the elements in $\VoutcomeD{}{}{t+1}$ cannot be inferred directly from the result of this lemma alone.


\noindent
\textbf{Proof of Theorem~\ref{thm:outcome_decomposition}.}
To obtain the desired result, we apply Lemma~\ref{lm:apndx_outcome_decomposition} and Lemma~\ref{lm:Gap_with_Gaussian_vector} together. To be more specific, in view of Lemma~\ref{lm:apndx_outcome_decomposition}, we know that
% 
\begin{equation*}
    \VoutcomeD{}{}{t+1} =
    \IMatMean{t} \outcomeg{t}{}\left(\VoutcomeD{}{}{t}, \Mtreatment{}{}{}, \covar{} \right) +
    \outcomeh{t}{}\left(\VoutcomeD{}{}{t}, \Mtreatment{}{}{}, \covar{} \right) +
    \sqrt{\sigma^2+\sigma_t^2} \norm{\outcomeg{t}{}(\VoutcomeD{}{}{t} ,\Mtreatment{}{}{}, \covar)}  \Sumvec{}{t} + \Vnoise{}{t},
\end{equation*}
%
where $\Sumvec{}{t} = \sum_{i=0}^{t} \NPC{i}{t} \Vec{Z}_i$. But, by Lemma~\ref{lm:Gap_with_Gaussian_vector}, we have
\begin{equation*}
    W_1\left(\law\left(\Sumvec{}{t}\right),\Nc\left(0,\frac{1}{N}\I_N\right)\right) \leq c \sqrt{\frac{t \log N}{N}},
\end{equation*}
which concludes the proof. \ep


\noindent
\textbf{Proof of Corollary~\ref{crl:SampleMean_decomposition}.}
% 
By the result of Theorem~\ref{thm:outcome_decomposition}, we can write
% 
\begin{align*}
    \frac{1}{\cardinality{\batch}} \sum_{i \in \batch} \outcomeD{}{i}{t+1}
    =
    \;&\frac{1}{N \cardinality{\batch}} \sum_{i \in \batch} \sum_{j=1}^N \left(\mu^{ij}+\mu_t^{ij}\right) \outcomeg{t}{}\left(\outcomeD{}{j}{t}, \Vtreatment{j}{}{}, \Vcovar{j} \right)
    +
    \frac{1}{\cardinality{\batch}} \sum_{i \in \batch} \outcomeh{t}{}\left(\outcomeD{}{i}{t}, \Vtreatment{i}{}{}, \Vcovar{i} \right)
    \\ \;&+
    \sqrt{\frac{\sigma^2+\sigma_t^2}{\cardinality{\batch}}} \norm{\outcomeg{t}{}(\VoutcomeD{}{}{t} ,\Mtreatment{}{}{}, \covar)} \frac{1}{\sqrt{\cardinality{\batch}}} \sum_{i \in \batch} \sumvec{i}{t}
    +
    \frac{1}{\cardinality{\batch}} \sum_{i \in \batch} \noise{i}{t}.
\end{align*}
% 
Letting $\avesumvec{}{t} := \frac{1}{\sqrt{\cardinality{\batch}}} \sum_{i \in \batch} \sumvec{i}{t}$ and applying Lemma~\ref{lm:Gap_with_Gaussian} for $\Vec{\Phi} = \Sumvec{}{t}$, we get the result. \ep




\subsection{Batch-level State Evolution}
\label{apndx:batch_state_evolution}
% 
Next, we analyze the large-sample behavior of the outcomes for a subpopulation of units. Specifically, let $\batch \subset [N]$ represent an arbitrary subpopulation, with its size $\cardinality{\batch}$ increasing indefinitely as the population size $N$ grows large to infinity. We investigate the asymptotic behavior of the elements in $\Moutcome{}{}{}$ as $N$ approaches infinity, This provides valuable insights into the evolution of outcomes within the subpopulation.
% 
\begin{assumption}
    \label{asmp:BL}
    Fixing $T \in \N$ and $k \geq 2$, we assume that
    \begin{enumerate}[label=(\roman*)]
        \item \label{asmp:BL-pl functions} For all $t\in[T]$, the function $\outcomeg{t}{}:\R^{1+T+M}\rightarrow \R$ is a $\poly{\frac{k}{2}}$ function.

        \item \label{asmp:BL-pl h-functions} For all $t\in[T]$, the function $\outcomeh{t}{}:\R^{1+T+M}\rightarrow \R$ is a $\poly{1}$ function.

        \item \label{asmp:BL-bound on initials} The sequence of initial outcome vectors $\VoutcomeD{}{}{0}$, the treatment allocation matrices~$\Mtreatment{}{}$, the covariates $\covar$, and the function $\outcomeg{0}{}$ are such that for a deterministic value $\MVVO{}{}{1}>0$, we have
        \begin{align*}
            (\MVVO{}{}{1})^2
            &=
            \lim_{N\rightarrow \infty}
            \frac{\sigma^2+\sigma_0^2}{N} \sum_{i=1}^N
            \outcomeg{0}{}\big(
            \outcomeD{}{i}{0},\Vtreatment{i}{},\Vcovar{i}
            \big)^2 < \infty.
        \end{align*}
    \end{enumerate}
\end{assumption}
% 
Assumption~\ref{asmp:BL} comprises a collection of regularity conditions on the model attributes. The first two parts ensure that the functions $\outcomeg{t}{}$ and $\outcomeh{t}{}$ do not demonstrate fast explosive behavior, thereby ensuring the well-posedness of the large system asymptotic. The final part pertains to the initial observation of the network and corroborates that the function $\outcomeg{0}{}$ is non-degenerate. This ensures that initial observations provide meaningful information and contribute to the evolution of outcomes.
% 
\begin{assumption}
    \label{asmp:weak_limits}
    % 
    Fix $T \in \N$, $k \geq 2$, and a subpopulation $\batch \subset [N]$, where the size $\cardinality{\batch}$ grows to infinity as $N \rightarrow \infty$. We assume the following statements hold:
    % 
    \begin{enumerate}[label=(\roman*)]
        \item $p_{y}^{\batch}(N)$ denotes the empirical distribution of the \textbf{initial outcomes} $\outcomeD{N}{i}{0}$ with $i \in \batch$, and converges weakly to a probability measure $p_{y}^{\batch}$ such that $\E_{p_{y}^{\batch}} \big[\normWO{\outcomeD{}{}{0}}^k\big]< \infty$,

        \item $p_{x}^{\batch}(N)$ denotes the empirical distribution of the \textbf{covariate} vectors $\Vcovar{i}(N)$ with $i \in \batch$, and converges weakly to a probability measure $p_{x}^{\batch}$ such that $\E_{p_{x}^{\batch}} \big[\normWO{\Vcovar{}}^k\big]< \infty$,

        \item $p_{w}^{\batch}(N)$ denotes the empirical distribution of the \textbf{treatment} vectors $\Vtreatment{i}{N}$ with $i \in \batch$, and converges weakly to a probability measure $p_{w}^{\batch}$ such that $\E_{p_{w}^{\batch}} \big[\normWO{\Vtreatment{}{}}^k\big]< \infty$.
        
        \item \label{asmp:interference_element_convergence} For all $i$ and any $t\in [T]_0$, let $p_{\mu^i}(N)$ and $p_{\mu_t^i}(N)$ be, respectively, the empirical distribution of the elements of the vectors $\Vec{\mu}^{\;i\cdot} := (\mu^{i1}, \ldots, \mu^{iN})^\top$ and $\Vec{\mu}^{\;i\cdot}_t := (\mu^{i1}_t, \ldots, \mu^{iN}_t)^\top$. Then, $p_{\mu^i}(N)$ converges weakly to $p_{\mu^i}$ and $p_{\mu_t^i}(N)$ converges weakly to $p_{\mu_t^i}$. Also, $\E_{p_{\mu^i}} \big[\normWO{\MIM{}{i}}^k\big]< \infty$ and $\E_{p_{\mu^i_t}} \big[\normWO{\MIM{t}{i}}^k\big]< \infty$.
        
        In addition, the limit distributions ($p_{\mu^i}$ and $p_{\mu^i_t}$) are independent of other randomnesses in the model and if $\bar{\mu}^i$ denotes the mean of a random variable under probability measure $p_{\mu^i}$ (i.e., $\bar{\mu}^i = \E_{p_{\mu^i}}[\MIM{}{i}]$), the empirical distributions of $\bar{\mu}^i,\; i \in \batch$, denoted by $p_{\mu}^{\batch}(N)$, converges weakly to a probability measure $p_{\mu}^{\batch}$. Likewise, we let $p_{\mu_t}^{\batch}$ denote the weak limit of the empirical distribution of the means under probability measures $p_{\mu^i_t}$. Finally, $\E_{p_{\mu}^{\batch}} \big[\normWO{\MIM{}{\batch}}^k\big]< \infty$ as well as $\E_{p_{\mu_t}^{\batch}} \big[\normWO{\MIM{t}{\batch}}^k\big]< \infty$.

        \item \label{asmp:k_moment_convergence} For all $i$ and $t\in [T]_0$, as $N \rightarrow \infty$, we have
        %
        \begin{align*}
            &\;\E_{p_{y}^{\batch}(N) \times p_{x}^{\batch}(N) \times p_{w}^{\batch}(N) \times p_{\mu^i}(N) \times p_{\mu_t^i}(N) \times p_{\mu}^{\batch}(N) \times p_{\mu_t}^{\batch}(N)} \big[\normWO{\outcomeD{}{}{0},\Vcovar{}, \Vtreatment{}{}, \MIM{}{i}, \MIM{t}{i}, \MIM{}{\batch}, \MIM{t}{\batch}}^k\big]
            \\
            \rightarrow
            &\;\E_{p_{y}^{\batch} \times p_{x}^{\batch} \times p_{w}^{\batch} \times p_{\mu^i} \times p_{\mu^i_t} \times p_{\mu}^{\batch} \times p_{\mu_t}^{\batch}}\big[\normWO{\outcomeD{}{}{0},\Vcovar{}, \Vtreatment{}{}, \MIM{}{i}, \MIM{t}{i}, \MIM{}{\batch}, \MIM{t}{\batch}}^k\big].
        \end{align*}
    \end{enumerate}
\end{assumption}
% 
\begin{remark}
    We can replace the second part of Assumption \ref{asmp:weak_limits}-\ref{asmp:interference_element_convergence} by assuming that $p_{\mu^i}$ is the same across all units, and similarly, $p_{\mu^i_t}$ is identical for all units.
\end{remark}
% 
\begin{remark}
    We can drop Assumptions \ref{asmp:BL} and \ref{asmp:weak_limits}-\ref{asmp:k_moment_convergence} by confining the functions $\outcomeg{t}{}$ and $\outcomeh{t}{}$ to be bounded and continuous.
\end{remark}
% 
\begin{remark}[Notation convention]
    In Assumption~\ref{asmp:weak_limits}, when $\batch$ represents the entire experimental population, we omit the superscript $\batch$ from all notations.
\end{remark}
% 
\begin{remark}[Time-dependent covariates]
    For each time $t = 0, 1, \ldots, T$, let $\Xc_t \in \R^{\dcovar_t \times N}$ denote the time-dependent covariate matrix, where the $i^{th}$ column corresponds to the covariates for unit $i$ at time $t$. We extend the covariate matrix $\covar$ by incorporating these time-dependent covariates, resulting in a new $(\dcovar + \dcovar_0 + \ldots + \dcovar_T) \times N$ covariate matrix in our model. The functions $\outcomeg{t}{}$ and $\outcomeh{t}{}$ are then modified accordingly to reference the appropriate portion of this extended covariate matrix at each time period. Along this, we exclude the noise vectors (i.e., $\Vnoise{}{t}$) from subsequent discussions on the potential outcome specification Eq.~\eqref{eq:apndx_outcome_function}.
\end{remark}
% 
Assumption~\ref{asmp:weak_limits} is a standard assumption in statistical theory, ensuring that the empirical distributions of system attributes remain stable and do not diverge as the sample size increases. This assumption holds, for example, when units' attributes $\left\{(\outcomeD{}{i}{0}, \Vcovar{i}, \Vtreatment{i}{}, \Vec{\mu}^{\;i\cdot}, \Vec{\mu}^{\;i\cdot}_t)\right\}_{i}$ follow an i.i.d. distribution with finite moments of order $k$. Moreover, a wide range of treatment assignments satisfies the conditions of Assumption~\ref{asmp:weak_limits}, including cases where the support of $\pi$ is bounded, such as the Bernoulli design. By imposing such conditions, we ensure the reliability of estimation by keeping the experimental design moments finite and manageable.


To state the main theoretical results, we need to define the \textbf{Batch State Evolution (BSE)} equations as follows:
% 
\begin{equation}
    \label{eq:state evolution}
    \begin{aligned}
        \MAVO{}{\batch}{1} &=
        \E\left[ (\MIM{}{\batch}+\MIM{0}{\batch})
        \outcomeg{0}{}\big(\outcomeD{}{}{0}, \Vtreatment{}{},\Vcovar{}\big)\right],
        \quad%%%
        \MVVO{}{}{1} =
        (\sigma+\sigma_0)
        \E\left[
        \outcomeg{0}{}\big(\outcomeD{}{}{0}, \Vtreatment{}{},\Vcovar{}\big)^2\right],
        \quad%%%
        \Houtcome{\batch}{}{0} = \outcomeh{0}{}\big(\outcomeD{}{\batch}{0}, \Vtreatment{\batch}{}, \Vcovar{\batch}\big),
        \\
        \Houtcome{\batch}{}{t} &= \outcomeh{t}{}\big(\MAVO{}{\batch}{t} + \MVVO{}{}{t} Z_t + \Houtcome{\batch}{}{t-1}, \Vtreatment{\batch}{}, \Vcovar{\batch}\big),
        \\
        \MAVO{}{\batch}{t+1} &=
        \E\left[ (\MIM{}{\batch}+\MIM{t}{\batch})
        \outcomeg{t}{}\big(\MAVO{}{}{t} + \MVVO{}{}{t} Z_t + \Houtcome{}{}{t-1}, \Vtreatment{}{}, \Vcovar{}\big)
        \right],
        \\
        (\MVVO{}{}{t+1})^2 &=
        (\sigma^2+\sigma_t^2) \E\left[
        \outcomeg{t}{} \big(\MAVO{}{}{t} + \MVVO{}{}{t} Z_t + \Houtcome{}{}{t-1}, \Vtreatment{}{},\Vcovar{}\big)^2
        \right],
        \\
        \AVO{}{\batch}{t+1} &= \MAVO{}{\batch}{t+1} + \E\left[\Houtcome{\batch}{}{t}\right],
        \\
        (\VVO{}{}{t+1})^2 &= (\MVVO{}{}{t+1})^2 + \Var\left[\Houtcome{}{}{t}\right].
    \end{aligned}
\end{equation}
% 
where 
% 
\begin{itemize}
    \item $\outcomeD{}{}{0} \sim p_y$ and $\outcomeD{}{\batch}{0} \sim p_y^\batch$ represent the weak limits of the population and subpopulation initial outcomes;

    \item $\Vtreatment{}{} \sim p_w$ and $\Vtreatment{\batch}{} \sim p_w^\batch$ are the weak limits of the population and subpopulation treatment assignments;

    \item $\Vcovar{} \sim p_x$ and $\Vcovar{\batch} \sim p_x^\batch$ represent the weak limits of the population and subpopulation covariates;

    \item $\MIM{}{} \sim p_{\MIM{}{}}$ and $\MIM{t}{} \sim p_{\MIM{t}{}}$ represent the weak limits of interference elements at the population level, as specified in Assumption~\ref{asmp:weak_limits}. Similarly, the corresponding subpopulation quantities are denoted by $\MIM{}{\batch} \sim p_{\MIM{}{}}^\batch$ and $\MIM{t}{\batch} \sim p_{\MIM{t}{}}^\batch$;

    \item and independent from all of them, $Z_t$ follows a standard Gaussian distribution.
\end{itemize}
% 

The following theorem characterizes the distribution of units' outcomes $\outcomeD{}{i}{1}, \ldots, \outcomeD{}{i}{t+1}$ within the large sample asymptotic, based on the BSE equations outlined in Eq.~\eqref{eq:state evolution}.
% 
\begin{theorem}
    \label{thm:Batch_SE}
    % 
    Fixing $k\geq 2$, consider the sequence of units' attributes $\left\{(\outcomeD{}{i}{0}, \Vcovar{i}, \Vtreatment{i}{}, \Vec{\mu}^{\;i\cdot}, \Vec{\mu}^{\;i\cdot}_t)\right\}_{i,t}$ and suppose that Assumptions~\ref{asmp:Gaussian Interference Matrice}-\ref{asmp:weak_limits} hold. Then, in view of the BSE equations given in Eq.~\eqref{eq:state evolution}, for any function $\psi \in \poly{k}$, we have,
    % 
    \begin{equation}
        \begin{aligned}
            \label{eq:BSE-limit}
            &\;\lim_{N \rightarrow \infty}
            \frac{1}{\cardinality{\batch}} \sum_{i \in \batch}
            \psi\big(
            \outcomeD{}{i}{0},
            \outcomeD{}{i}{1},
            \ldots,
            \outcomeD{}{i}{T},
            \Vtreatment{i}{}, \Vcovar{i}
            \big)
            \\
            \eqas
            \;&\E
            \Big[
            \psi\big(
            \outcomeD{}{\batch}{0},
            \MAVO{}{\batch}{1} + \MVVO{}{}{1} Z_1 + \Houtcome{\batch}{}{0},
            \ldots,
            \MAVO{}{\batch}{T} + \MVVO{}{}{T} Z_{T} + \Houtcome{\batch}{}{T-1},
            \Vtreatment{\batch}{}, \Vcovar{\batch}
            \big)
            \Big],
        \end{aligned}
    \end{equation}
    % 
    where $Z_t \sim \Nc(0,1),\; t= 1,\ldots,T,$ independent of $\Vtreatment{\batch}{} \sim p_w^\batch$ and $\Vcovar{\batch} \sim p_x^\batch$. In addition, for any $t = 1, \ldots, T$, the random variables $Z_t$ and $\Houtcome{}{}{t-1}$ are independent.
\end{theorem}
%
We prove the result of Theorem~\ref{thm:Batch_SE} by extending the theoretical results of \cite{shirani2024causal}, which in turn build on the AMP framework developed by \cite{bayati2011dynamics}. The proof mainly relies on a \emph{conditioning technique} introduced by \cite{bolthausen2014iterative}. Below, we first present a version of the conditioning technique adapted to our specific setting, followed by a detailed proof of the theorem.

\begin{remark}
    To derive the result in the second part of Theorem~\ref{thm:BSE_informal} from Theorem~\ref{thm:Batch_SE}, we note that when $\batch$ depends solely on the treatment allocation, which is independently distributed from all other variables, $\batch$ effectively functions as a random sample from the experimental population. As a result, the distributions of $\outcomeD{}{\batch}{0}$, $\Vcovar{\batch}$, $\MIM{}{\batch}$, and $\MIM{t}{\batch}$ are equivalent to those of $\outcomeD{}{}{0}$, $\Vcovar{}$, $\MIM{}{}$, and $\MIM{t}{}$, respectively. This equivalence follows from the fact that the empirical distribution of quantities in a random sample converges to the empirical distribution of the entire population.
\end{remark}



\subsection{Conditioning Technique}
\label{apndx:Conditioning_Technique}
% 
Recalling \eqref{eq:apndx_outcome_function_WOD} and
letting $\VUoutcome{}{}{t} = \outcomeg{t}{}\big(\VoutcomeD{}{}{t},\Mtreatment{}{},\covar\big)$ (and $\Uoutcome{i}{}{t} = \outcomeg{t}{}\big(\outcomeD{}{i}{t},\Vtreatment{i}{},\Vcovar{i}\big)$), we denote
% 
\begin{equation}
\label{eq:Q and R}
\begin{aligned}
    \bm{Q}_t
    :=
    \left[
    \VUoutcome{}{}{0}
    \Big|
    \VUoutcome{}{}{1}
    \Big|
    \ldots
    \Big|
    \VUoutcome{}{}{t-1}
    \right],
    \quad\quad
    \bm{R}_t
    :=
    \left[
    \Voutcome{}{}{1} -
    \IMatT{0} \VUoutcome{}{}{0}
    \Big|
    \ldots
    \Big|
    \Voutcome{}{}{t} -
    \IMatT{t-1} \VUoutcome{}{}{t-1}
    \right].
\end{aligned}
\end{equation}
% 
According to Eq.~\eqref{eq:Q and R}, $\bm{Q}_t$ and $\bm{R}_t$ are matrices with columns of $\VUoutcome{}{}{s-1}$ and $\Voutcome{}{}{s} - \IMatT{s-1} \VUoutcome{}{}{s-1}$, when $s=1,\ldots,t$, respectively. Then, we denote by $\VUoutcome{\parallel}{}{t}$ the projection of $\VUoutcome{}{}{t}$ onto the space generated by the columns of $\bm{Q}_t$ and define $\VUoutcome{\perp}{}{t} = \VUoutcome{}{}{t} - \VUoutcome{\parallel}{}{t}$. We also define $\VAPC{}{t} = (\APC{0}{t}, \APC{1}{t}, \ldots, \APC{t-1}{t})^\top$ such that
% 
\begin{align}
    \label{eq:projection sum}
    \VUoutcome{\parallel}{}{t}
    =
    \sum_{s=0}^{t-1} \APC{s}{t} \VUoutcome{}{}{s}
    =
    \sum_{s=0}^{t-1} \APC{s}{t}
    \outcomeg{s}{}
    \big(
    \VoutcomeD{}{}{s}, \Mtreatment{}{}, \covar
    \big),
\end{align}
% 
where
% 
\begin{align}
    \label{eq:projection coefficients}
    \VAPC{}{t}
    =
    \left(
    \bm{Q}_t^\top \bm{Q}_t
    \right)^{-1}
    \bm{Q}_t^\top \VUoutcome{}{}{t}.
\end{align}
% 
Now, note that the available observation at any time $t$ implicitly reveals information about the fixed interference matrix $\IM$. To manage this intricate randomness, we define $\Gc_t$ as the $\sigma$-algebra generated by $\VoutcomeD{}{}{0}, \VoutcomeD{}{}{1}, \ldots, \VoutcomeD{}{}{t}$, $\Voutcome{}{}{1}, \ldots, \Voutcome{}{}{t}$, $\IMatMean{t}$, $\IMatT{0}, \ldots, \IMatT{t-1}$, $\Mtreatment{}{}$, and $\covar$. We then compute the conditional distribution of $\IM$ given $\Gc_t$. In this framework, conditioning on $\Gc_t$ is equivalent to conditioning on the event $\IM \bm{Q}_t = \bm{R}_t$. When conditioned on $\Gc_t$, the entries of both $\bm{Q}_t$ and $\bm{R}_t$ become deterministic values, leading to the following lemma.


\begin{lemma}
    \label{lm:conditional dist of IM}
    Fix $t$ and assume that $\bm{Q}_t$ is a full-row rank matrix. Then, for the conditional distribution of the fixed interference matrix $\IM$ given $\IM \bm{Q}_t=\bm{R}_t$, we have
    % 
        \begin{align}
        \label{eq:conditional dist of IM}
        \IM|_{\IM \bm{Q}_t=\bm{R}_t}
        \eqd
        \bm{R}_t
        \left(
        \bm{Q}_t^\top \bm{Q}_t
        \right)^{-1}
        \bm{Q}_t^\top
        +
        \widetilde{\IM} P^\perp.
        \end{align}
        % 
    where $\widetilde{\IM} \eqd \IM$ independent of $\IM$ and $P^\perp = (\I-P)$ that P denotes the orthogonal projector onto the column space of $\bm{Q}_t$.
\end{lemma}

The proof of Lemma~\ref{lm:conditional dist of IM} relies on the rotational invariance of the Gaussian distribution and utilizes Lemma~11 from \cite{bayati2011dynamics}. The proofs of this lemma and the subsequent lemma— which describes the distribution of $\Voutcome{}{}{t+1}$ given the event $\IM \bm{Q}_t = \bm{R}_t$— follow a similar approach to the proofs of Lemmas~2 and 3 in \cite{shirani2024causal}, and we refer readers to that work for detailed derivations.

 
\begin{lemma}
    \label{lm:conditional dist of outcome}
    Fix $t$ and assume that $\bm{Q}_t$ is a full-row rank matrix. The following holds for the conditional distribution of the vector $\Voutcome{}{}{t+1}$:
    % 
            \begin{align}
                \label{eq:conditional dist of outcome_nonsym}
                \Voutcome{}{}{t+1}\big|_{\Gc_t}
                \eqd
                &\;
                \widetilde{\IM} 
                \VUoutcome{\perp}{}{t}
                + \bm{R}_t \VAPC{}{t} + \IMatT{t} \VUoutcome{}{}{t},
            \end{align}
            % 
    where the matrix $\widetilde{\IM}$ is independent of $\IM$ and has the same distribution.
\end{lemma}

\subsection{Detailed Proof of Theorem~\ref{thm:Batch_SE}}
\label{apndx:big_lemma}
% 
Here, we first state Lemma~\ref{lm:Big lemma} which is an expanded version of Theorem~\ref{thm:Batch_SE}. To this end, we need some new notations. Specifically, for vectors $\Vec{u},\Vec{v} \in \R^m$, we define the scalar product $\pdot{\Vec{u}}{\Vec{v}}:= \frac{1}{m} \sum_{i=1}^m u_i v_i$. Also, considering \eqref{eq:state evolution}, for $t\geq 1$, we define
% 
\begin{equation}
    \label{eq:state evolution_fixed part}
    \begin{aligned}
        (\BVVO{}{}{1})^2
            &=
            \lim_{N\rightarrow \infty}
            \frac{\sigma^2}{N} \sum_{i=1}^N
            \outcomeg{0}{}\big(
            \outcomeD{}{i}{0},\Vtreatment{i}{},\Vcovar{i}
            \big)^2 < \infty
        \\
        (\BVVO{}{}{t+1})^2 &=
        \sigma^2 \E\left[
        \outcomeg{t}{} \big(\MAVO{}{}{t} + \MVVO{}{}{t} Z_t + \Houtcome{}{}{t-1}, \Vtreatment{}{},\Vcovar{}\big)^2
        \right].
    \end{aligned}
\end{equation}
% 


\begin{lemma}
    \label{lm:Big lemma}
    For a fixed $k\geq 2$ and a specified subpopulation of experimental units $\batch$, consider the following conditions. Given that Assumption~\ref{asmp:BL} holds, and both the subpopulation $\batch$ and the complete experimental population satisfy Assumption~\ref{asmp:weak_limits}, the following statements are valid for all time steps $t$ under the BSE equations defined in \eqref{eq:state evolution}:
    % 
    \begin{enumerate}[label=(\alph*)]
        \item \label{part:BL-a} For any function $\psi \in \poly{k}$, we have
        % 
        \begin{equation}
            \label{eq:BL-a}
            \begin{aligned}
                \lim_{N \rightarrow \infty}
                \frac{1}{\cardinality{\batch}} \sum_{i \in \batch}
                \psi\big(
                \outcomeD{}{i}{0},
                \outcomeD{}{i}{1},
                \ldots,
                \outcomeD{}{i}{t+1},
                \Vtreatment{i}{},\Vcovar{i}
                \big)
                \\
                \eqas
                \E
                \Big[
                \psi
                \big(
                \outcomeD{}{\batch}{0},
                \MAVO{}{\batch}{1}
                + \MVVO{}{}{1} Z_1
                + \Houtcome{\batch}{}{0},
                \ldots,
                \MAVO{}{\batch}{t+1}
                + \MVVO{}{}{t+1} Z_t
                + \Houtcome{\batch}{}{t},
                \Vtreatment{\batch}{}, \Vcovar{\batch}
                \big)
                \Big],
            \end{aligned}
        \end{equation}
        % 
        where $Z_1, \ldots, Z_t$ are standard Gaussian random variables.

        \item \label{part:BL-b} For all $0 \leq r\neq s \leq t$, the following equations hold and all limits exist, are bounded, and have degenerate distribution (i.e. they are constant random variables)
        % 
        \begin{subequations}
            \label{eq:BL-b}
            \begin{align}
                \label{eq:BL-b-1}
                \lim_{N \rightarrow \infty} 
                \frac{1}{N}
                \sum_{i=1}^N
                (\outcome{}{i}{r+1})^2
                &\eqas
                (\MVVO{}{}{r+1})^2
                \eqas
                \lim_{N \rightarrow \infty}
                \frac{\sigma^2+\sigma^2_{r}}{N} \sum_{i=1}^N (\Uoutcome{i}{}{r})^2
                \\
                \label{eq:BL-b-2}
                \lim_{N \rightarrow \infty} 
                \frac{1}{N}
                \sum_{i=1}^N
                \outcome{}{i}{r+1}\outcome{}{i}{s+1}
                &\eqas
                \lim_{N \rightarrow \infty}
                \frac{\sigma^2}{N} \sum_{i=1}^N \Uoutcome{i}{}{r}\Uoutcome{i}{}{s},
                \\
                \label{eq:BL-b-3}
                \lim_{N \rightarrow \infty}
                \frac{1}{N}
                \sum_{i=1}^N
                \big(\outcome{}{i}{r+1}
                - \IMatTv{i \cdot}{r} \VUoutcome{}{}{r}
                \big)^2
                &\eqas
                (\BVVO{}{}{r+1})^2
                \eqas
                \lim_{N \rightarrow \infty}
                \frac{\sigma^2}{N}
                \sum_{i=1}^N
                \left(\Uoutcome{i}{}{r}\right)^2,
                \\
                \label{eq:BL-b-4}
                \lim_{N \rightarrow \infty}
                \frac{1}{N}
                \sum_{i=1}^N
                \big(\outcome{}{i}{r+1}
                - \IMatTv{i \cdot}{r} \VUoutcome{}{}{r}
                \big)
                \big(\outcome{}{i}{s+1}
                - \IMatTv{i \cdot}{s} \VUoutcome{}{}{s}
                \big)
                &\eqas
                \lim_{N \rightarrow \infty}
                \frac{\sigma^2}{N}
                \sum_{i=1}^N
                \Uoutcome{i}{}{r} \Uoutcome{i}{}{s},
                \\
                \label{eq:BL-b-5}
                \lim_{N \rightarrow \infty}
                \frac{1}{N}
                \sum_{i=1}^N
                \outcome{}{i}{r+1}
                \big(\outcome{}{i}{s+1}
                - \IMatTv{i \cdot}{s} \VUoutcome{}{}{s}
                \big)
                &\eqas
                \lim_{N \rightarrow \infty}
                \frac{\sigma^2}{N}
                \sum_{i=1}^N
                \Uoutcome{i}{}{r} \Uoutcome{i}{}{s}.
            \end{align}
        \end{subequations}
        % 

        \item \label{part:BL-c} Letting $\MOG{t} = [\Voutcome{}{}{1}|\ldots|\Voutcome{}{}{t}]$, the following matrices are positive definite almost surely:
        % 
        \begin{align}
            \label{eq:BL-lower bound for perps}
            \lim_{N \rightarrow \infty} \frac{\bm{Q}_{t}^\top \bm{Q}_{t}}{N} \succ 0,
            \quad\quad\quad
            \lim_{N \rightarrow \infty} \frac{\MOG{t}^\top \MOG{t}}{N} 
            \succ 0.
        \end{align}
        % 
    \end{enumerate}
\end{lemma}
In the following section, we will provide a comprehensive explanation of the conditioning technique, which will be employed to establish the results presented in Lemma~\ref{lm:Big lemma}.

\noindent
\textbf{Proof.}
% 
For all $t$, we assume, without loss of generality, that the mapping $y \mapsto \outcomeg{t}{}(y, \Vtreatment{}{}, \Vcovar{})$ is a non-constant function with positive probability with respect to the randomness of $\Vtreatment{}{}$ and $\Vcovar{}$; otherwise, the result is trivial and does not require further analysis. We prove the result by induction on $t$.

\textbf{Step 1.} Let $t = 0$. By definition, the matrices $\bm{Q}_0$ and $\bm{R}_0$ are empty, and the $\sigma$-algebra $\Gc_0$ is generated by $\VoutcomeD{}{}{0}$, $\Mtreatment{}{}$, and $\covar$. As the induction base case, we establish Parts \ref{part:BL-a} and \ref{part:BL-b} for $t=0$ and Part~\ref{part:BL-c} for $t = 1$.
% 
\begin{enumerate}[label=(\alph*)]
    \item \label{item:BL-average limit} Conditioning on the values of $\VoutcomeD{}{}{0}$, $\Mtreatment{}{}$, $\covar$, and so on the value of $\VUoutcome{}{}{0} = \outcomeg{0}{}\big(\VoutcomeD{}{}{0},\Mtreatment{}{},\covar\big)$, the elements of $\Voutcome{}{}{1}$ are i.i.d. Gaussian random variables with zero mean and variance~$(\MVVO{}{}{1N})^2$:
    % 
    \begin{equation}
        \label{eq:BL-a0-Y1 stat}
        \begin{aligned}
            (\MVVO{}{}{1N})^2
            &:=
            \Var
            \left[
            \outcome{}{i}{1}
            \Big|
            \VUoutcome{}{}{0}
            \right]
            =
            \frac{\sigma^2 + \sigma^2_0}{N}
            \sum_{i=1}^N
            \outcomeg{0}{} \left(\outcomeD{}{i}{0},\Vtreatment{i}{},\Vcovar{i}
            \right)^2.
        \end{aligned}
    \end{equation}
    % 
    Then, Assumption~\ref{asmp:BL}-\ref{asmp:BL-bound on initials} implies that the value of $(\MVVO{}{}{1N})^2$ is bounded independent from $N$, and
    % 
    \begin{equation}
        \label{eq:BL-a0-Y1 stats limits}
        \begin{aligned}
            (\MVVO{}{}{1})^2 :=
            \lim_{N\rightarrow \infty} (\MVVO{}{}{1N})^2.
        \end{aligned}
    \end{equation}
    % 
    
    Now, let $Z$ denote a standard Gaussian random variable. Fixing $l \geq 1$, it is straightforward to show that
    % 
    \begin{equation}
    \begin{aligned}
    \label{eq:BL-proof-a0-1}
        \E
        \left[
        \big|\outcome{}{i}{1}\big|^l
        \big|
        \VUoutcome{}{}{0}
        \right]
        &=
        \E
        \left[
        \big|\MVVO{}{}{1N} Z\big|^l
        \big|
        \VUoutcome{}{}{0}
        \right] \leq c,
    \end{aligned}
    \end{equation}
    % 
    where $c$ is a constant independent of $N$ and might alter in different lines.

    We next focus on the second term on the right-hand side of Eq.~\eqref{eq:apndx_outcome_function} and define
    % 
    \begin{equation}
        \label{eq:BL-a0-Y1 stat-2}
        \begin{aligned}
            \MAVO{}{i}{1N}
            &:=
            \frac{1}{N}
            \sum_{j=1}^N
            (\mu^{ij} + \mu_0^{ij})
            \outcomeg{0}{} \big(\outcomeD{}{j}{0},\Vtreatment{j}{},\Vcovar{j}\big),
            \quad\quad\quad
            i \in [N].
        \end{aligned}
    \end{equation}
    In view of Assumption~\ref{asmp:weak_limits}, we can apply Theorem~\ref{thm:SLLN-2}. Consequently, for all $i$, we can write
    % 
    \begin{equation}
        \label{eq:BL-a0-Y1 stat-2_limit}
        \begin{aligned}
            \lim_{N \rightarrow \infty} \MAVO{}{i}{1N}
            \eqas
            \E \left[ (\MIM{}{i}+\MIM{0}{i}) \outcomeg{0}{} \big(\outcomeD{}{}{0},\Vtreatment{}{},\Vcovar{}\big) \right] = (\bar{\mu}^i + \bar{\mu}^i_0) \E \left[ \outcomeg{0}{} \big(\outcomeD{}{}{0},\Vtreatment{}{},\Vcovar{}\big) \right] = \MAVO{}{i}{1} < \infty.
        \end{aligned}
    \end{equation}
    % 
    Above, we let $\bar{\mu}^i := \E[\MIM{}{i}]$ and $\bar{\mu}^i_0 := \E[\MIM{0}{i}]$, where $\MIM{}{i} \sim p_{\mu^i}$ and $\MIM{0}{i} \sim p_{\mu^i_0}$. Additionally, $\outcomeD{}{}{0}$, $\Vtreatment{}{}$, and $\Vcovar{}$ represent the weak limits of the initial outcomes, treatment allocations, and covariates for the entire population as outlined in Assumption~\ref{asmp:weak_limits}, respectively. In this context, $\bar{\mu}^i+\bar{\mu}^i_0$ determines the average interaction level of unit $i$ at time $0$.
    % 
    
    Also, note that \eqref{eq:BL-a0-Y1 stat-2_limit} yields the boundedness of $\MAVO{}{i}{1N}$ for all $i$ independent of $N$.
    % 
    Now, using Assmption~\ref{asmp:weak_limits} and the fact that $\psi \in \poly{k}$, for $\kappa > 0$, we have the following, 
    % 
    \begin{align*}
        \frac{1}{\cardinality{\batch}} \sum_{i \in \batch}
        \E
        \left[
        \Big|
        \psi\big(
        \outcomeD{}{i}{0},
        \outcome{}{i}{1},
        \Vtreatment{i}{}, \Vcovar{i}, \MAVO{}{i}{1N}
        \big)
        -
        \E_{\IM,\IMatT{0}}
        \left[
        \psi\big(
        \outcomeD{}{i}{0},
        \outcome{}{i}{1},
        \Vtreatment{i}{}, \Vcovar{i}, \MAVO{}{i}{1N}
        \big)
        \right]
        \Big|^{2+\kappa}
        \right]
        \leq
        c \cardinality{\batch}^{\kappa/2},
    \end{align*}
    % 
    where $\E_{\IM,\IMatT{0}}$ is the expectation with respect to the randomness of the interference matrices $\IM,\IMatT{0}$ and $c$ is a constant independent of $N$. Then, applying the Strong Law of Large Numbers (SLLN) for triangular arrays in Theorem~\ref{thm:SLLN}, we obtain the following result:
    % 
    \begin{align}
        \label{eq:BL-a0-SLLN}
        \lim_{N \rightarrow \infty}
        \frac{1}{\cardinality{\batch}} \sum_{i \in \batch}
        \Big(
        \psi\big(
        \outcomeD{}{i}{0},
        \outcome{}{i}{1},
        \Vtreatment{i}{}, \Vcovar{i}, \MAVO{}{i}{1N}
        \big)
        -
        \E_{\IM,\IMatT{0}}
        \left[
        \psi\big(
        \outcomeD{}{i}{0},
        \outcome{}{i}{1},
        \Vtreatment{i}{}, \Vcovar{i}, \MAVO{}{i}{1N}
        \big)
        \right]
        \Big)
        \eqas
        0.
    \end{align}
    %
    On the other hand, employing the dominated convergence theorem, e.g., Theorem 16.4 in \cite{billingsley2008probability}, allows us to interchange the limit and the expectation in view of the fact that $\psi \in \poly{k}$. We also utilize the continuous mapping theorem, e.g., Theorem 2.3 in \cite{van2000asymptotic}, to pass the limit through the function. As a result, considering $\outcome{}{i}{1} \eqd \MVVO{}{}{1N} Z$, we get
    % 
    \begin{align}
        \label{eq:BL-a0-limit to the function}
        \lim_{N \rightarrow \infty}
        \frac{1}{\cardinality{\batch}} \sum_{i \in \batch}
        \E_{\IM,\IMatT{0}}
        \left[
        \psi\big(
        \outcomeD{}{i}{0},
        \outcome{}{i}{1},
        \Vtreatment{i}{}, \Vcovar{i}, \MAVO{}{i}{1N}
        \big)
        \right]
        \eqas
        \lim_{N \rightarrow \infty}
        \frac{1}{\cardinality{\batch}} \sum_{i \in \batch}
        \E_{Z}
        \left[
        \psi\big(
        \outcomeD{}{i}{0},
        \MVVO{}{}{1} Z,
        \Vtreatment{i}{}, \Vcovar{i}, \MAVO{}{i}{1}
        \big)
        \right].
    \end{align}
    % 
    Then, applying Theorem~\ref{thm:SLLN-2} for the function $f(\outcomeD{}{i}{0}, \Vtreatment{i}{}, \Vcovar{i}, \MAVO{}{i}{1}) = \E_{Z}\left[\psi\big(\outcomeD{}{i}{0}, \MVVO{}{}{1} Z, \Vtreatment{i}{}, \Vcovar{i}, \MAVO{}{i}{1} \big)\right]$, we can write
    % 
    \begin{equation}
        \label{eq::BL-proof-a0-dynamics}
        \begin{aligned}
            \lim_{N \rightarrow \infty}
            \frac{1}{\cardinality{\batch}} \sum_{i \in \batch}
            \psi\big(
            \outcomeD{}{i}{0},
            \outcome{}{i}{1},
            \Vtreatment{i}{}, \Vcovar{i}, \MAVO{}{i}{1N}
            \big)
            \eqas
            \E
            \Big[
            \psi
            \big(
            \outcomeD{}{\batch}{0},
            \MVVO{}{}{1} Z,
            \Vtreatment{\batch}{}, \Vcovar{\batch}, \MAVO{}{\batch}{1}
            \big)
            \Big],
        \end{aligned}
    \end{equation}
    % 
    where $\outcomeD{}{\batch}{0},\;\Vtreatment{\batch}{},\;\Vcovar{\batch},\; \MAVO{}{\batch}{1}$ represent the weak limits of $\outcomeD{}{i}{0},\;\Vtreatment{i}{},\;\Vcovar{i},\; \MAVO{}{i}{1}$ over the subpopulation units, as specified in Assumption~\ref{asmp:weak_limits} and
    % 
    \begin{equation}
        \label{eq:BL-a0-Y1 stat-2_limit_without_n}
        \begin{aligned}
            \MAVO{}{\batch}{1}
            \eqas
            \E \left[ (\MIM{}{\batch}+\MIM{0}{\batch}) \outcomeg{0}{} \big(\outcomeD{}{}{0},\Vtreatment{}{},\Vcovar{}\big) \right],
        \end{aligned}
    \end{equation}
    %
    where $\MIM{}{\batch} + \MIM{0}{\batch}$ captures the weak limit of the average interference level for units in the subpopulation.
    
    In Eq.~\eqref{eq::BL-proof-a0-dynamics}, the function $f$ is within $\poly{k}$, since $\psi \in \poly{k}$ and expectation is a linear operator. It is important to note that above, $Z$ is independent of $\outcomeD{}{\batch}{0},\; \Vtreatment{\batch}{},\; \Vcovar{\batch}, \MIM{}{\batch}$, and $\MIM{0}{\batch}$.
    This is true because the randomness of $Z$ arises from the interference matrices which are assumed to be independent of everything in the model, see Assumption~\ref{asmp:apndx_Gaussian Interference Matrice}.

    Now, we use Eq.~\eqref{eq::BL-proof-a0-dynamics} to derive the main result. Fix an arbitrary function $\psi \in \poly{k}$ and based on Eqs.~\eqref{eq:apndx_outcome_function} and \eqref{eq:apndx_outcome_function_WOD}, define the function $\widetilde\psi$ such that
    % 
    \begin{align}
        \label{eq:BL-a0-function_re_def}
        \psi\big(\outcomeD{}{i}{0},
        \outcomeD{}{i}{1},
        \Vtreatment{i}{}, \Vcovar{i}
        \big)
        =
        \psi\left(
        \outcomeD{}{i}{0},
        \outcome{}{i}{1} +
        \MAVO{}{i}{1N} +
        \outcomeh{0}{} \big(\outcomeD{}{i}{0},\Vtreatment{i}{},\Vcovar{i}\big),
        \Vtreatment{i}{}, \Vcovar{i}
        \right)
        =
        \widetilde{\psi}\left(
        \outcomeD{}{i}{0},
        \outcome{}{i}{1}, 
        \Vtreatment{i}{}, \Vcovar{i}, \MAVO{}{i}{1N}
        \right).
    \end{align}
    % 
    The function $\widetilde\psi$ is within $\poly{k}$ by Assumption~\ref{asmp:BL}. Then, applying \eqref{eq::BL-proof-a0-dynamics} for the function $\widetilde{\psi}$, we obtain,
    % 
    \begin{equation}
        \label{eq::BL-proof-a0-dynamics2}
        \begin{aligned}
            \lim_{N \rightarrow \infty}
            \frac{1}{\cardinality{\batch}} \sum_{i \in \batch}
            \psi\big(\outcomeD{}{i}{0},
            \outcomeD{}{i}{1},
            \Vtreatment{i}{}, \Vcovar{i}
            \big)
            \eqas
            \E \left[
            \psi\left(
            \outcomeD{}{\batch}{0},
            \MAVO{}{\batch}{1}
            +
            \MVVO{}{}{1} Z + 
            \Houtcome{\batch}{}{0}
            ,
            \Vtreatment{\batch}{}, \Vcovar{\batch}
            \right)
            \right]
        \end{aligned}
    \end{equation}
    % 
    where $\Houtcome{\batch}{}{0} = \outcomeh{0}{} \big(\outcomeD{}{\batch}{0},\Vtreatment{\batch}{},\Vcovar{\batch}\big)$ is a random variable independent of $Z$. 
    
    In the second step of the induction, we also require the following results. Note that the result in \eqref{eq::BL-proof-a0-dynamics2} represents a specific instance of the more comprehensive result \eqref{eq:BL-proof-a0-dynamics-with-eps}. These results can be derived by following the same procedure outlined above.
    % 
    \begin{equation}
        \label{eq:BL-proof-a0-dynamics-with-eps0}
        \begin{aligned}
            \lim_{N \rightarrow \infty}
            \frac{1}{N} \sum_{i = 1}^N
            &\psi\big(
            \outcomeD{}{i}{0},
            \outcomeD{}{i}{1},
            \outcome{}{i}{1},
            \outcome{}{i}{1}
            - \IMatTv{i \cdot}{0} \VUoutcome{}{}{0},
            \Vtreatment{i}{},\Vcovar{i},
            \mu^{ji}, \mu^{ji}_r
            \big)
            \\
            \eqas
            \E
            \Big[
            &\psi
            \big(
            \outcomeD{}{}{0},
            \MAVO{}{}{1}
            + \MVVO{}{}{1} Z
            + \Houtcome{}{}{0},
            \MVVO{}{}{1} Z,
            % \BAVO{}{}{1}
            % +
            \BVVO{}{}{1} Z',
            \Vtreatment{}{},\Vcovar{},
            \MIM{}{j}, \MIM{r}{j}
            \big)
            \Big],
        \end{aligned}
    \end{equation}
    % 
    as well as
    % 
    \begin{equation}
        \label{eq:BL-proof-a0-dynamics-with-eps}
        \begin{aligned}
            \lim_{N \rightarrow \infty}
            \frac{1}{\cardinality{\batch}} \sum_{i \in \batch}
            &\psi\big(
            \outcomeD{}{i}{0},
            \outcomeD{}{i}{1},
            \outcome{}{i}{1},
            \outcome{}{i}{1}
            - \IMatTv{i \cdot}{0} \VUoutcome{}{}{0},
            \Vtreatment{i}{},\Vcovar{i},
            \bar{\mu}^i, \bar{\mu}^i_r
            \big)
            \\
            \eqas
            \E
            \Big[
            &\psi
            \big(
            \outcomeD{}{\batch}{0},
            \MAVO{}{\batch}{1}
            + \MVVO{}{}{1} Z
            + \Houtcome{\batch}{}{0},
            \MVVO{}{}{1} Z,
            % \BAVO{}{}{1}
            % +
            \BVVO{}{}{1} Z',
            \Vtreatment{\batch}{},\Vcovar{\batch},
            \MIM{}{\batch}, \MIM{r}{\batch}
            \big)
            \Big],
        \end{aligned}
    \end{equation}
    % % 
    for any $j$ and all $r \in [T]_0$; here, $Z'$ is a standard Gaussian random variable and $\IMatTv{i \cdot}{0}$ detnoes the $i^{th}$ row of the time-dependent interference matrix $\IMatT{0}$.    


    \item By \eqref{eq:BL-a0-Y1 stats limits} and \eqref{eq:BL-proof-a0-dynamics-with-eps0}, we get
    % 
    \begin{equation}
        \label{eq:BL-proof-b0-1}
        \begin{aligned}
            \lim_{N \rightarrow \infty}
            \frac{1}{N}
            \sum_{i=1}^N
            \big(\outcome{}{i}{1}
            \big)^2
            &\eqas
            (\MVVO{}{}{1})^2
            =
            \lim_{N \rightarrow \infty}
            \frac{\sigma^2+\sigma_0^2}{N}
            \sum_{i=1}^N
            \left(\Uoutcome{i}{}{0}\right)^2.
        \end{aligned}
    \end{equation}
    % 
    Similarly, by \eqref{eq:state evolution_fixed part} and \eqref{eq:BL-proof-a0-dynamics-with-eps0}, we can write % 
    \begin{equation}
        \label{eq:BL-proof-b0-2}
        \begin{aligned}
            \lim_{N \rightarrow \infty}
            \frac{1}{N}
            \sum_{i=1}^N
            \big(\outcome{}{i}{1}
            - \IMatTv{i \cdot}{0} \VUoutcome{}{}{0}
            \big)^2
            &\eqas
            (\BVVO{}{}{1})^2
            =
            \lim_{N \rightarrow \infty}
            \frac{\sigma^2}{N}
            \sum_{i=1}^N
            \left(\Uoutcome{i}{}{0}\right)^2.
        \end{aligned}
    \end{equation}
    % 
    Finally, given $\VUoutcome{}{}{0}$, we know that $\IMatv{i \cdot} \VUoutcome{}{}{0}$ and $\IMatTv{i \cdot}{0} \VUoutcome{}{}{0}$ are statistically independent. Then, applying \eqref{eq:BL-proof-a0-dynamics-with-eps0} together with \eqref{eq:BL-proof-b0-2}, we obtain the following result:
    % 
    \begin{equation*}
        % \label{eq:BL-proof-b0-3}
        \begin{aligned}
            \lim_{N \rightarrow \infty}
            \frac{1}{N}
            \sum_{i=1}^N
            \outcome{}{i}{1}
            \big(\outcome{}{i}{1}
            - \IMatTv{i \cdot}{0} \VUoutcome{}{}{0}
            \big)
            =
            \lim_{N \rightarrow \infty}
            \frac{1}{N}
            \sum_{i=1}^N
            \big(\IMatv{i \cdot} \VUoutcome{}{}{0}
            +
            \IMatTv{i \cdot}{0} \VUoutcome{}{}{0}
            \big)
            \big(\IMatv{i \cdot} \VUoutcome{}{}{0}
            \big)
            &\eqas
            (\BVVO{}{}{1})^2
            =
            \lim_{N \rightarrow \infty}
            \frac{\sigma^2}{N}
            \sum_{n=1}^N
            \left(\Uoutcome{n}{}{0}\right)^2.
        \end{aligned}
    \end{equation*}
    % 
    where $\IMatv{i \cdot}$ and $\IMatTv{i \cdot}{0}$ denote the $i^{th}$ row of the fixed interference matrix $\IMat{}$ and time-dependent interference matrix $\IMatT{}$.
    

    \item For $t=1$, the matrix $\bm{Q}_1$ is equal to the vector $\VUoutcome{}{}{0}$ and $\bm{V}_1$ is equal to the vector $\VoutcomeD{}{}{1}$. By Assumption~\ref{asmp:BL}-\ref{asmp:BL-bound on initials} and \eqref{eq:BL-proof-b0-1}, we have
    \begin{align*}
        \lim_{N \rightarrow \infty} \frac{\bm{Q}_1^\top \bm{Q}_1}{N}
        =
        \lim_{N \rightarrow \infty}
        \frac{1}{N}
        \sum_{i=1}^N \left(\Uoutcome{i}{}{0}\right)^2 > 0,
        \quad
        \lim_{N \rightarrow \infty} \frac{\MOG{1}^\top \MOG{1}}{N} 
        =
        \lim_{N \rightarrow \infty} \pdot{\Voutcome{}{}{1}}{\Voutcome{}{}{1}}
        > 0.
    \end{align*}
    % 
    \end{enumerate}


    \textbf{Induction Hypothesis (IH).} Now, we assume that the following results hold true:
    % 
    \begin{equation}
        \label{eq:BL-proof-IH-a}
        \tag{IH-1}
        \begin{aligned}
            \lim_{N \rightarrow \infty}
            \frac{1}{N} \sum_{i = 1}^N
            &\psi\big(
            \outcomeD{}{i}{0},
            \outcomeD{}{i}{1},
            \ldots,
            \outcomeD{}{i}{t},
            \outcome{}{i}{1},
            \ldots,
            \outcome{}{i}{t},
            \outcome{}{i}{1}
            - \IMatTv{i \cdot}{0} \VUoutcome{}{}{0},
            \ldots,
            \outcome{}{i}{t}
            - \IMatTv{i \cdot}{t-1} \VUoutcome{}{}{t},
            \Vtreatment{i}{},\Vcovar{i},
            \mu^{ji}, \mu^{ji}_r
            \big)
            \\
            \eqas
            \E
            \Big[
            &\psi
            \big(
            \outcomeD{}{}{0},
            \MAVO{}{}{1}
            + \MVVO{}{}{1} Z_1
            + \Houtcome{}{}{0},
            \ldots,
            \MAVO{}{}{t}
            + \MVVO{}{}{t} Z_t
            + \Houtcome{}{}{t-1},
            \MVVO{}{}{1} Z_1,
            \ldots,
            \MVVO{}{}{t} Z_t,
            \BVVO{}{}{1} Z'_1,
            \ldots,
            \BVVO{}{}{t} Z'_t,
            \Vtreatment{}{},\Vcovar{},
            \MIM{}{j}, \MIM{r}{j}
            \big)
            \Big],
        \end{aligned}
    \end{equation}
    % 
    and
    %
    \begin{equation}
        \label{eq:BL-proof-IH-a1}
        \tag{IH-2}
        \begin{aligned}
            \lim_{N \rightarrow \infty}
            \frac{1}{\cardinality{\batch}} \sum_{i \in \batch}
            &\psi\big(
            \outcomeD{}{i}{0},
            \outcomeD{}{i}{1},
            \ldots,
            \outcomeD{}{i}{t},
            \outcome{}{i}{1},
            \ldots,
            \outcome{}{i}{t},
            \outcome{}{i}{1}
            - \IMatTv{i \cdot}{0} \VUoutcome{}{}{0},
            \ldots,
            \outcome{}{i}{t}
            - \IMatTv{i \cdot}{t-1} \VUoutcome{}{}{t},
            \Vtreatment{i}{},\Vcovar{i},
            \bar{\mu}^i, \bar{\mu}^i_r
            \big)
            \\
            \eqas
            \E
            \Big[
            &\psi
            \big(
            \outcomeD{}{\batch}{0},
            \MAVO{}{\batch}{1}
            + \MVVO{}{}{1} Z_1
            + \Houtcome{\batch}{}{0},
            \ldots,
            \MAVO{}{\batch}{t}
            + \MVVO{}{}{t} Z_t
            + \Houtcome{\batch}{}{t-1},
            \MVVO{}{}{1} Z_1,
            \ldots,
            \MVVO{}{}{t} Z_t,
            \BVVO{}{}{1} Z'_1,
            \ldots,
            \BVVO{}{}{t} Z'_t,
            \Vtreatment{\batch}{},\Vcovar{\batch},
            \MIM{}{\batch}, \MIM{r}{\batch}
            \big)
            \Big].
        \end{aligned}
    \end{equation}
    % 
    Also, for $0 \leq s \neq r \leq t-1$, we have
    %
    \begin{align}
            \label{eq:BL-proof-IH-b-1}
            \tag{IH-3}
            \lim_{N \rightarrow \infty} 
            \frac{1}{N}
            \sum_{i=1}^N
            (\outcome{}{i}{r+1})^2
            &\eqas
            (\MVVO{}{}{r+1})^2
            \eqas
            \lim_{N \rightarrow \infty}
            \frac{\sigma^2+\sigma^2_{r}}{N} \sum_{i=1}^N (\Uoutcome{i}{}{r})^2
            \\
            \label{eq:BL-proof-IH-b-2}
            \tag{IH-4}
            \lim_{N \rightarrow \infty} 
            \frac{1}{N}
            \sum_{i=1}^N
            \outcome{}{i}{r+1}\outcome{}{i}{s+1}
            &\eqas
            \lim_{N \rightarrow \infty}
            \frac{\sigma^2}{N} \sum_{i=1}^N \Uoutcome{i}{}{r}\Uoutcome{i}{}{s},
            \\
            \label{eq:BL-proof-IH-b-3}
            \tag{IH-5}
            \lim_{N \rightarrow \infty}
            \frac{1}{N}
            \sum_{i=1}^N
            \big(\outcome{}{i}{r+1}
            - \IMatTv{i \cdot}{r} \VUoutcome{}{}{r}
            \big)^2
            &\eqas
            (\BVVO{}{}{r+1})^2
            \eqas
            \lim_{N \rightarrow \infty}
            \frac{\sigma^2}{N}
            \sum_{i=1}^N
            \left(\Uoutcome{i}{}{r}\right)^2,
            \\
            \label{eq:BL-proof-IH-b-4}
            \tag{IH-6}
            \lim_{N \rightarrow \infty}
            \frac{1}{N}
            \sum_{i=1}^N
            \big(\outcome{}{i}{r+1}
            - \IMatTv{i \cdot}{r} \VUoutcome{}{}{r}
            \big)
            \big(\outcome{}{i}{s+1}
            - \IMatTv{i \cdot}{s} \VUoutcome{}{}{s}
            \big)
            &\eqas
            \lim_{N \rightarrow \infty}
            \frac{\sigma^2}{N}
            \sum_{i=1}^N
            \Uoutcome{i}{}{r} \Uoutcome{i}{}{s},
            \\
            \label{eq:BL-proof-IH-b-5}
            \tag{IH-7}
            \lim_{N \rightarrow \infty}
            \frac{1}{N}
            \sum_{i=1}^N
            \outcome{}{i}{r+1}
            \big(\outcome{}{i}{s+1}
            - \IMatTv{i \cdot}{s} \VUoutcome{}{}{s}
            \big)
            &\eqas
            \lim_{N \rightarrow \infty}
            \frac{\sigma^2}{N}
            \sum_{i=1}^N
            \Uoutcome{i}{}{r} \Uoutcome{i}{}{s}.
    \end{align}
    % 
    Finally, the following condition holds almost surely:
    % 
    \begin{align}
            \label{eq:BL-proof-IH-c}
            \tag{IH-8}
            \lim_{N \rightarrow \infty} \frac{\MOG{t-1}^\top \MOG{t-1}}{N} 
            \succ 0.
    \end{align}

    \textbf{Step 2.} To establish the second step of the induction, we prove the assertions in reverse order, starting with Part (c), followed by Part (b), and concluding with Part (a).
    
    
    \begin{enumerate}[label=(\alph*)]
        \item[(c)] We begin the second step by applying \eqref{eq:BL-proof-IH-a} to the function $g_s\big(\outcomeD{}{i}{s},\Vtreatment{i}{},\Vcovar{i}\big) g_r\big(\outcomeD{}{i}{r},\Vtreatment{i}{},\Vcovar{i}\big)$, for $1 \leq r,s \leq t$. Precisely, by Assumption~\ref{asmp:BL}-\ref{asmp:BL-bound on initials} as well as \eqref{eq:BL-proof-IH-a}, we get
        % 
        \begin{equation}
            \label{eq:BL-proof-ct-1}
            \begin{aligned}
            \lim_{N \rightarrow \infty}
            \frac{1}{N}
            \sum_{i=1}^N
            (\Uoutcome{i}{}{0})^2
            &\eqas
            \E\left[
            \outcomeg{0}{}(\outcomeD{}{}{0},\Vtreatment{}{},\Vcovar{})^2\big)
            \right]
            =
            \frac{(\MVVO{}{}{1})^2}{\sigma^2 + \sigma_0^2} > 0,
            \\
            \lim_{N \rightarrow \infty}
            \frac{1}{N}
            \sum_{i=1}^N
            \Uoutcome{i}{}{0} \Uoutcome{i}{}{s}
            &\eqas
            \E\left[
            \outcomeg{0}{}(\outcomeD{}{}{0},\Vtreatment{}{},\Vcovar{}) \outcomeg{s}{}\big(\MAVO{}{}{s}
            +
            \MVVO{}{}{s} Z_{s} + 
            \Houtcome{}{}{s-1},
            \Vtreatment{}{},\Vcovar{}\big)
            \right],
            \\
            \lim_{N \rightarrow \infty}
            \frac{1}{N}
            \sum_{i=1}^N
            \Uoutcome{i}{}{s} \Uoutcome{i}{}{r}
            &\eqas
            \E\Big[
            \outcomeg{s}{}\big(\MAVO{}{}{s}
            +
            \MVVO{}{}{s} Z_{s} + 
            \Houtcome{}{}{s-1},
            \Vtreatment{}{},\Vcovar{}\big) \outcomeg{r}{}\big(\MAVO{}{}{r}+
            \MVVO{}{}{r} Z_{r} + 
            \Houtcome{}{}{r-1},
            \Vtreatment{}{},\Vcovar{}\big)
            \Big].
            \end{aligned}
        \end{equation}
        % 
        Now, let $\Vec{u} = (u_1,\ldots,u_t)^\top \in \R^t$ be a non-zero vector. By Assumption~\ref{asmp:BL}-\ref{asmp:BL-bound on initials} and \eqref{eq:BL-proof-ct-1}, we have
        % 
        \begin{equation}
            \label{eq:BL-proof-ct-2}
            \begin{aligned}
                \Vec{u}^\top \left(\lim_{N \rightarrow \infty} \frac{\bm{Q}_t^\top \bm{Q}_t}{N}\right) \Vec{u}
                &=
                \lim_{N \rightarrow \infty} \Vec{u}^\top \frac{\bm{Q}_t^\top \bm{Q}_t}{N} \Vec{u}
                \\
                &\eqas
                \E\left[
                \left(
                u_1
                \outcomeg{0}{}(\outcomeD{}{}{0},\Vtreatment{}{},\Vcovar{})
                +
                \sum_{s=1}^{t-1}
                u_{s+1} 
                \outcomeg{s}{}\big(\MAVO{}{}{s}
                +
                \MVVO{}{}{s} Z_{s} + 
                \Houtcome{}{}{s-1},
                \Vtreatment{}{},\Vcovar{}\big)
                \right)^2
                \right] \geq 0.
            \end{aligned}
        \end{equation}
        % 
        We show that the inequality in \eqref{eq:BL-proof-ct-2} is strict. To this end, note that $\Vec{u}$ is a non-zero vector, and there exists some $1\leq i \leq t$ such that $u_i \neq 0$. Whenever $u_1 \neq 0 = u_2 = \ldots = u_t$, the result is immediate. Otherwise, recall that $y \mapsto \outcomeg{s}{}(y,\Vtreatment{}{},\Vcovar{})$ is a non-constant function with a positive probability with respect to $(\Vtreatment{}{},\Vcovar{})$; consequently, the mapping $(y_0,\ldots,y_{t-1}) \mapsto \sum_{s=0}^{t-1} u_s \outcomeg{s}{}\big(y_s,\Vtreatment{}{},\Vcovar{}\big)$ is a non-constant function as well. Considering $\Houtcome{}{}{s} = \outcomeh{s}{} \big(\MAVO{}{}{s} + \MVVO{}{}{s} Z_{s} + \Houtcome{}{}{s-1},\Vtreatment{}{},\Vcovar{}\big)$ and $\Houtcome{}{}{0} = \outcomeh{0}{} \big(\outcomeD{}{}{0},\Vtreatment{}{},\Vcovar{}\big)$, the randomness of $u_1\outcomeg{0}{}(\outcomeD{}{}{0},\Vtreatment{}{},\Vcovar{}) + \sum_{s=1}^{t-1} u_{s+1} \outcomeg{s}{}\big(\MAVO{}{}{s} + \MVVO{}{}{s} Z_{s} + \Houtcome{}{}{s-1}, \Vtreatment{}{},\Vcovar{}\big)$ comes solely from $\outcomeD{}{}{0}, Z_1, \ldots, Z_{t-1}, \Vtreatment{}{}, \Vcovar{}$; as a result, there exists a measurable continuous function $\phi$ (which depends on $\Vec{u}, \outcomeg{0}{}, \ldots, \outcomeg{t-1}{}, \outcomeh{0}{}, \ldots, \outcomeh{t-1}{}$) such that we can rewrite the right-hand side of \eqref{eq:BL-proof-ct-2} as follows, 
        % 
        \begin{equation*}
            \begin{aligned}
                &\;\E\left[
                \left(
                u_1
                \outcomeg{0}{}(\outcomeD{}{}{0},\Vtreatment{}{},\Vcovar{})
                +
                \sum_{s=1}^{t-1}
                u_{s+1} 
                \outcomeg{s}{}\big(\MAVO{}{}{s}
                +
                \MVVO{}{}{s} Z_{s} + 
                \Houtcome{}{}{s-1},
                \Vtreatment{}{},\Vcovar{}\big)
                \right)^2
                \right]
                \\
                = 
                &\;\E\left[
                \phi\left(
                \outcomeD{}{}{0}, \MAVO{}{}{1}, \ldots, \MAVO{}{}{t-1}, \MVVO{}{}{1} Z_1, \ldots, \MVVO{}{}{t-1} Z_{t-1}, \Vtreatment{}{}, \Vcovar{}
                \right)^2
                \right]
                \\
                =
                &\;\E\left[
                \E\left[
                \phi\left(
                y_0, \MAVO{}{}{1}, \ldots, \MAVO{}{}{t-1}, \MVVO{}{}{1} Z_1, \ldots, \MVVO{}{}{t-1} Z_{t-1}, \Vtreatment{}{}, \Vcovar{}
                \right)^2
                \Bigg|
                \outcomeD{}{}{0} = y_0
                \right]
                \right],
            \end{aligned}
        \end{equation*}
        % 
        where in the last equality we used the tower property of conditional expectations. Then, it suffices to show that the random variable $\phi\left( y_0, \MAVO{}{}{1}, \ldots, \MAVO{}{}{t-1}, \MVVO{}{}{1} Z_1, \ldots, \MVVO{}{}{t-1} Z_{t-1}, \Vtreatment{}{}, \Vcovar{} \right)$ has a non-degenerate distribution. To obtain that, by \eqref{eq:BL-proof-IH-a}, it is straightforward to obtain the following, 
        % 
        \begin{equation*}
            \begin{aligned}
                \Cov\left[\left(
                \MVVO{}{}{1} Z_{1},
                \ldots, \MVVO{}{}{t-1} Z_{t-1} \right)\right]
                \eqas
                \lim_{N \rightarrow \infty} \frac{\MOG{t-1}^\top \MOG{t-1}}{N},
            \end{aligned}
        \end{equation*}
        % 
        which is positive definite by \eqref{eq:BL-proof-IH-c}, and the proof of the first claim is complete.
        
        To proceed to the proof of the second claim, for $1\leq r,s \leq t$, let us denote
        % 
        \begin{align*}
            v_{r,s}
            :=
            \left[
            \frac{\MOG{t}^\top \MOG{t}}{N} 
            \right]^{r,s}
            =
            \frac{\Voutcome{}{\top}{r} \Voutcome{}{}{s}}{N}. 
        \end{align*}
        % 
        By \eqref{eq:BL-proof-IH-b-2}, whenever $r\neq s$, we can write
        % 
        \begin{align*}
            \lim_{N \rightarrow \infty}
            v_{r,s}
            \eqas
            \lim_{N \rightarrow \infty}
            \frac{\sigma^2}{N} \sum_{i=1}^N \Uoutcome{i}{}{r-1}\Uoutcome{i}{}{s-1},
        \end{align*}
        % 
        and if $r=s$, by \eqref{eq:BL-proof-IH-b-1}, we have
        % 
        \begin{align*}
            \lim_{N \rightarrow \infty}
            v_{r,r}
            \eqas
            \lim_{N \rightarrow \infty}
            \frac{\sigma^2+\sigma_r^2}{N} \sum_{i=1}^N (\Uoutcome{i}{}{r-1})^2.
        \end{align*}
        % 
        Then, the result follows directly, as we have just established the almost sure positive definiteness of $\bm{Q}_t$.
        % 
        \begin{corollary}
            \label{cl:alpha is bounded}
            The vector $\VAPC{}{t}$ defined in \eqref{eq:projection coefficients} has a finite limit as $N \rightarrow \infty$.
        \end{corollary}
        % 
        Proof. By \eqref{eq:projection coefficients}, we can write
        \begin{align}
            \label{eq:cl-alpha is finite}
            \lim_{N \rightarrow \infty} \VAPC{}{t}
            =
            \lim_{N \rightarrow \infty} \left(
            \bm{Q}_t^\top \bm{Q}_t
            \right)^{-1}
            \bm{Q}_t^\top \VUoutcome{}{}{t}
            =
            \lim_{N \rightarrow \infty} \left(
            \frac{\bm{Q}_t^\top \bm{Q}_t}{N}
            \right)^{-1}
            \lim_{N \rightarrow \infty}
            \frac{\bm{Q}_t^\top \VUoutcome{}{}{t}}{N}.
        \end{align}    
        Using the result of part~\ref{part:BL-c}, for large values of $N$, the matrix $\frac{\bm{Q}_t^\top \bm{Q}_t}{N}$ is positive definite (this is true because the eigenvalues of a matrix vary continuously with respect to its entries). Then, note that the mapping $\bm{G} \mapsto \bm{G}^{-1}$ is continuous for any invertible matrix $\bm{G}$. As a result, we get
        \begin{align*}
            \lim_{N \rightarrow \infty} \left(\frac{\bm{Q}_t^\top \bm{Q}_t}{N}\right)^{-1}
            =
            \left(\lim_{N \rightarrow \infty}\frac{\bm{Q}_t^\top \bm{Q}_t}{N}\right)^{-1}.
        \end{align*}
        Since the matrix $\lim_{N \rightarrow \infty} \frac{\bm{Q}_t^\top \bm{Q}_t}{N}$ is positive definite, the left term in \eqref{eq:cl-alpha is finite} is well-defined and finite. The finiteness of the right term is the consequence of \eqref{eq:BL-proof-ct-1}. \ep

        \item[(b)] We first derive several intermediate results and then utilize them to demonstrate that \eqref{eq:BL-b} holds true for $0 \leq r, s \leq t$. In this process, we apply the Strong Law of Large Numbers (SLLN) from Theorem~\ref{thm:SLLN} multiple times, without explicitly verifying the conditions, as they are straightforward.

        By Lemma~\ref{lm:conditional dist of outcome}, conditioning on $\Gc_t$, the terms $\IMatvnew{i\cdot} \VUoutcome{\perp}{}{t}$ for $i \in [N]$ are i.i.d. Gaussian random variables. Similarly, the terms $\IMatTv{i \cdot}{t} \VUoutcome{}{}{t}$ for $i \in [N]$ are also i.i.d. Gaussian random variables:
        % 
        \begin{equation}
            \label{eq:BL-proof-bt-simple-1}
            \begin{aligned}
                \IMatvnew{i\cdot} \VUoutcome{\perp}{}{t} \sim \Nc\left(0, \frac{\sigma^2}{N} \sum_{j=1}^N (\Uoutcome{\perp,j}{}{t})^2\right),
            \quad\quad
                \IMatTv{i \cdot}{t} \VUoutcome{}{}{t} \sim \Nc\left(0, \frac{\sigma_t^2}{N} \sum_{j=1}^N (\Uoutcome{j}{}{t})^2\right).
            \end{aligned}
        \end{equation}
        % 
        Now, applying Theorem~\ref{thm:SLLN}, we get the following results:
        % 
        \begin{equation}
            \label{eq:BL-proof-bt-simple-4}
            \begin{aligned}
                \lim_{N \rightarrow \infty}
                \frac{1}{N}
                \sum_{i=1}^N
                \left(
                \IMatvnew{i\cdot} 
                \VUoutcome{\perp}{}{t}
                \right)^2
                &\eqas
                \lim_{N \rightarrow \infty} \frac{\sigma^2}{N}
                \sum_{i=1}^N
                \left(\Uoutcome{\perp,i}{}{t}\right)^2,
            \end{aligned}
        \end{equation}
        % 
        as well as
        % 
        \begin{equation}
            \label{eq:BL-proof-bt-simple-5}
            \begin{aligned}
                \lim_{N \rightarrow \infty}
                \frac{1}{N}
                \sum_{i=1}^N
                \left(
                \IMatTv{i \cdot}{t}
                \VUoutcome{}{}{t}
                \right)^2
                &\eqas
                \lim_{N \rightarrow \infty} \frac{\sigma_t^2}{N}
                \sum_{i=1}^N
                \left(\Uoutcome{i}{}{r}\right)^2.
            \end{aligned}
        \end{equation}
        % 
        Also, considering \eqref{eq:Q and R} and applying \eqref{eq:BL-proof-IH-b-4}, we obtain
        % 
        \begin{equation}
        \label{eq:BL-proof-bt-simple-7}
            \begin{aligned}
                &\lim_{N \rightarrow \infty}
                \frac{1}{N}
                \sum_{i=1}^N
                \left(
                \left[
                \bm{R}_t
                \VAPC{}{t}
                \right]^i
                \right)^2
                \\
                =
                &\lim_{N \rightarrow \infty}
                \frac{1}{N}
                \sum_{i=1}^N
                \left(
                \sum_{s=0}^{t-1}
                \APC{t}{s}
                \big(
                \outcome{}{i}{s+1}
                - \IMatTv{i \cdot}{s} \VUoutcome{}{}{s}
                \big)
                \right)^2
                \\
                =
                &\lim_{N \rightarrow \infty}
                \frac{1}{N}
                \sum_{i=1}^N
                \sum_{0\leq s,r < t}
                \APC{t}{s}
                \APC{t}{r}
                \big(
                \outcome{}{i}{s+1} - \IMatTv{i \cdot}{s} \VUoutcome{}{}{s}
                \big)
                \big(
                \outcome{}{i}{r+1} - \IMatTv{i \cdot}{r} \VUoutcome{}{}{r}
                \big)
                \\
                =
                &\sum_{0\leq s,r < t}
                \APC{t}{s}
                \APC{t}{r}
                \left(
                \lim_{N \rightarrow \infty}
                \frac{1}{N}
                \sum_{i=1}^N
                \big(
                \outcome{}{i}{s+1} - \IMatTv{i \cdot}{s} \VUoutcome{}{}{s}
                \big)
                \big(
                \outcome{}{i}{r+1} - \IMatTv{i \cdot}{r} \VUoutcome{}{}{r}
                \big)\right)
                \\
                \eqas
                & \lim_{N \rightarrow \infty}
                \frac{\sigma^2}{N}
                \sum_{i=1}^N
                \sum_{0\leq s,r < t}
                \APC{t}{s}
                \APC{t}{r}
                \Uoutcome{i}{}{s} \Uoutcome{i}{}{r}
                \\
                =
                &\lim_{N \rightarrow \infty}
                \frac{\sigma^2}{N}
                \sum_{i=1}^N
                \left(\Uoutcome{\parallel,i}{}{t}\right)^2,
            \end{aligned}
        \end{equation}
        % 
        where in the last line we used \eqref{eq:projection sum}.

        Now, we first obtain \eqref{eq:BL-b-1} for $r=t$. By \eqref{eq:conditional dist of outcome_nonsym}, \eqref{eq:BL-proof-bt-simple-4}, \eqref{eq:BL-proof-bt-simple-5}, and \eqref{eq:BL-proof-bt-simple-7}, we can write
        % 
        \begin{equation}
            \label{eq:BL-proof-bt-2-1}
            \begin{aligned}
                \lim_{N \rightarrow \infty}
                \frac{1}{N}
                \sum_{i=1}^N
                \big(
                \outcome{}{i}{t+1}
                \big)^2
                &\eqas
                \lim_{N \rightarrow \infty}
                \frac{1}{N}
                \sum_{i=1}^N
                \left(
                \IMatvnew{i\cdot} 
                \VUoutcome{\perp}{}{t}
                +
                \left[
                \bm{R}_t
                \VAPC{}{t}
                \right]^i
                +
                \IMatTv{i \cdot}{t} \VUoutcome{}{}{t}
                \right)^2
                \\
                &\eqas
                \lim_{N \rightarrow \infty} \frac{\sigma^2}{N}
                \sum_{i=1}^N
                \left(\Uoutcome{\perp,i}{}{t}\right)^2
                +
                \lim_{N \rightarrow \infty}
                \frac{\sigma^2}{N}
                \sum_{i=1}^N
                \left(\Uoutcome{\parallel,i}{}{t}\right)^2
                +
                \lim_{N \rightarrow \infty} \frac{\sigma_t^2}{N}
                \sum_{i=1}^N
                \left(\Uoutcome{i}{}{t}\right)^2
                \\
                &\quad
                +
                \lim_{N \rightarrow \infty}
                \frac{2}{N}
                \sum_{i=1}^N
                \left(\IMatvnew{i\cdot} 
                \VUoutcome{\perp}{}{t} \left[
                \bm{R}_t
                \VAPC{}{t}
                \right]^i \right)
                +
                \lim_{N \rightarrow \infty}
                \frac{2}{N}
                \sum_{i=1}^N
                \left(\IMatvnew{i\cdot} 
                \VUoutcome{\perp}{}{t} \IMatTv{i \cdot}{t} \VUoutcome{}{}{t} \right)
                \\
                &\quad
                +
                \lim_{N \rightarrow \infty}
                \frac{2}{N}
                \sum_{i=1}^N
                \left( \left[
                \bm{R}_t
                \VAPC{}{t}
                \right]^i \IMatTv{i \cdot}{t}\VUoutcome{}{}{t} \right).
            \end{aligned}
        \end{equation}
        % 
        Note that the only random elements in the right-hand side of \eqref{eq:BL-proof-bt-2-1} are $\IMatvnew{i \cdot}$ and $\IMatTv{i \cdot}{t}$. Thus, by \eqref{eq:BL-proof-bt-simple-1} and applying Theorem~\ref{thm:SLLN}, we can demonstrate that the last three terms vanish, resulting in the following:
        % 
        \begin{align}
            \label{eq:BL-proof-bt-2-result}
            \lim_{N \rightarrow \infty}
            \frac{1}{N}
            \sum_{i=1}^N
            \big(
            \outcome{}{i}{t+1}
            \big)^2
            &\eqas
            \lim_{N \rightarrow \infty} \frac{\sigma^2+\sigma_t^2}{N}
            \sum_{i=1}^N
            \left(\Uoutcome{i}{}{t}\right)^2
            \eqas
            (\MVVO{}{}{t+1})^2,
        \end{align}
        % 
        where the last equality is immediate by \eqref{eq:BL-proof-IH-a}.
        
        Next, we derive \eqref{eq:BL-b-2} for $r=t$ and $0\leq s \leq t-1$. Considering \eqref{eq:conditional dist of outcome_nonsym}, we can write
        % 
        \begin{equation}
        \label{eq:BL-proof-bt-3-1}
            \begin{aligned}
                &\lim_{N \rightarrow \infty}
                \frac{1}{N}
                \sum_{i=1}^N
                \outcome{}{i}{s+1}
                \outcome{}{i}{t+1}
                \\
                \eqas
                &\lim_{N \rightarrow \infty}
                \frac{1}{N}
                \sum_{i=1}^N
                \outcome{}{i}{s+1}
                \left(
                \IMatvnew{i\cdot} 
                \VUoutcome{\perp}{}{t}
                +
                \left[
                \bm{R}_t
                \VAPC{}{t}
                \right]^i
                + \IMatTv{i \cdot}{t} \VUoutcome{}{}{t}
                \right)
                \\
                \eqas
                &\lim_{N \rightarrow \infty}
                \frac{1}{N}
                \sum_{i=1}^N
                \left(
                \IMatvnew{i\cdot} 
                \VUoutcome{\perp}{}{t}
                \outcome{}{i}{s+1}
                +
                \left[
                \bm{R}_t
                \VAPC{}{t}
                \right]^i
                \outcome{}{i}{s+1}
                +
                \IMatTv{i \cdot}{t} 
                \VUoutcome{}{}{t}
                \outcome{}{i}{s+1}
                \right).
            \end{aligned}
        \end{equation}
        % 
        Note that by conditioning on $\Gc_t$, the value of $\outcome{}{n}{s+1}$ is deterministic. Then, applying Theorem~\ref{thm:SLLN} and considering \eqref{eq:BL-proof-bt-simple-1}, \eqref{eq:Q and R}, and by \eqref{eq:BL-proof-IH-b-5}, we get the desired result:
        % 
        \begin{equation}
            \label{eq:BL-proof-bt-3-4}
            \begin{aligned}
                \lim_{N \rightarrow \infty}
                \frac{1}{N}
                \sum_{i=1}^N
                \outcome{}{i}{s+1}
                \outcome{}{i}{t+1}
                &\eqas
                \lim_{N \rightarrow \infty}
                \frac{1}{N}
                \sum_{i=1}^N
                \left(
                \left[
                \bm{R}_t
                \VAPC{}{t}
                \right]^i
                \right)\outcome{}{i}{s+1}
                \\
                &=
                \lim_{N \rightarrow \infty}
                \frac{1}{N}
                \sum_{i=1}^N
                \left(
                \sum_{r=0}^{t-1}
                \APC{r}{t}
                \big(
                \outcome{}{i}{r+1}
                - \IMatTv{i \cdot}{r} \VUoutcome{}{}{r}
                \big)
                \outcome{}{i}{s+1}
                \right)
                \\
                &\eqas
                \sum_{r=0}^{t-1}
                \APC{r}{t} 
                \left(
                \lim_{N \rightarrow \infty}
                \frac{\sigma^2}{N} \sum_{i=1}^N \Uoutcome{i}{}{r}\Uoutcome{i}{}{s}
                \right)
                \\
                &\eqas
                \lim_{N \rightarrow \infty}\hspace{-1mm}
                \left(
                \frac{\sigma^2}{N} \sum_{i=1}^N \sum_{r=0}^{t-1}
                \APC{r}{t}
                \Uoutcome{i}{}{r}\Uoutcome{i}{}{s}\right)
                \\
                &=
                \lim_{N \rightarrow \infty}
                \frac{\sigma^2}{N}
                \sum_{i=1}^N
                \left(
                \Uoutcome{\parallel,i}{}{t}
                \Uoutcome{i}{}{s}
                \right)
                \\
                &=
                \lim_{N \rightarrow \infty}
                \frac{\sigma^2}{N}
                \sum_{i=1}^N
                \left(
                \Uoutcome{i}{}{t}
                \Uoutcome{i}{}{s}
                \right),
            \end{aligned}
        \end{equation}
        % 
        where in the last line, we used the fact that $\pdot{\VUoutcome{}{}{t}}{\VUoutcome{}{}{s}} = \pdot{\VUoutcome{\parallel}{}{t}}{\VUoutcome{}{}{s}}$ as $\VUoutcome{\perp}{}{t} \perp \VUoutcome{}{}{s}$.

        The derivations for \eqref{eq:BL-b-3} and \eqref{eq:BL-b-4} follow a similar procedure, which we omit here for brevity. We then apply a similar approach to obtain \eqref{eq:BL-b-5}. Specifically, fixing $0 \leq r \leq t-1$ and setting $s=t$, we can write the following using \eqref{eq:conditional dist of outcome_nonsym} and \eqref{eq:BL-proof-bt-3-4}:
        % 
        \begin{equation*}
            \begin{aligned}
                \lim_{N \rightarrow \infty}
                \frac{1}{N}
                \sum_{i=1}^N
                \big(
                \outcome{}{i}{t+1}
                - \IMatTv{i \cdot}{t} \VUoutcome{}{}{t}
                \big)
                \outcome{}{i}{r+1}
                \eqas
                \lim_{N \rightarrow \infty}
                \frac{1}{N}
                \sum_{i=1}^N
                \left(
                \IMatvnew{i\cdot} 
                \VUoutcome{\perp}{}{t}
                +
                \left[
                \bm{R}_t
                \VAPC{}{t}
                \right]^i
                \right)\outcome{}{i}{r+1}
                \eqas
                \lim_{N \rightarrow \infty}
                \frac{\sigma^2}{N}
                \sum_{i=1}^N
                \left(
                \Uoutcome{i}{}{t}
                \Uoutcome{i}{}{r}
                \right).
            \end{aligned}
        \end{equation*}
        % 
        Likewise, we can show the result for the case that $r=t$ and $0\leq s \leq t-1$:
        % 
        \begin{equation}
            \label{eq:BL-proof-bt-4-1}
            \begin{aligned}
                &\lim_{N \rightarrow \infty}
                \frac{1}{N}
                \sum_{i=1}^N
                \big(
                \outcome{}{i}{s+1}
                - \IMatTv{i \cdot}{s} \VUoutcome{}{}{s}
                \big)
                \outcome{}{i}{t+1}
                \\
                \eqas
                &\lim_{N \rightarrow \infty}
                \frac{1}{N}
                \sum_{i=1}^N
                \big(
                \outcome{}{i}{s+1}
                - \IMatTv{i \cdot}{s} \VUoutcome{}{}{s}
                \big)
                \left(
                \IMatvnew{i\cdot} 
                \VUoutcome{\perp}{}{t}
                +
                \left[
                \bm{R}_t
                \VAPC{}{t}
                \right]^i
                + \IMatTv{i \cdot}{t} \VUoutcome{}{}{t}
                \right)
                \\
                =
                &\lim_{N \rightarrow \infty}
                \frac{1}{N}
                \sum_{i=1}^N
                \left(
                \IMatvnew{i\cdot} 
                \VUoutcome{\perp}{}{t}
                \right)
                \big(
                \outcome{}{i}{s+1}
                - \IMatTv{i \cdot}{s} \VUoutcome{}{}{s}
                \big)
                \\
                &+
                \lim_{N \rightarrow \infty}
                \frac{1}{N}
                \sum_{i=1}^N
                \left(
                \IMatTv{i \cdot}{t} \VUoutcome{}{}{t}
                \right)
                \big(
                \outcome{}{i}{s+1}
                - \IMatTv{i \cdot}{s} \VUoutcome{}{}{s}
                \big),
                \\
                &+
                \lim_{N \rightarrow \infty}
                \frac{1}{N}
                \sum_{i=1}^N
                \left(
                \left[
                \bm{R}_t
                \VAPC{}{t}
                \right]^i
                \right)
                \big(
                \outcome{}{i}{s+1}
                - \IMatTv{i \cdot}{s} \VUoutcome{}{}{s}
                \big).
            \end{aligned}
        \end{equation}
        % 
        Then, by Theorem~\ref{thm:SLLN}, the first and second terms on the right-hand side are zero. Additionally,
        \eqref{eq:Q and R}, \eqref{eq:projection sum}, and \eqref{eq:BL-proof-IH-b-4} imply that
        % 
        \begin{equation}
        \label{eq:BL-proof-bt-4-4}
            \begin{aligned}
                \lim_{N \rightarrow \infty}
                \frac{1}{N}
                \sum_{i=1}^N
                \left(
                \left[
                \bm{R}_t
                \VAPC{}{t}
                \right]^i
                \right)
                \big(
                \outcome{}{i}{s+1}
                - \IMatTv{i \cdot}{s} \VUoutcome{}{}{s}
                \big)
                &=
                \lim_{N \rightarrow \infty}
                \frac{1}{N}
                \sum_{i=1}^N
                \sum_{r=0}^{t-1}
                \APC{r}{t}
                \big(
                \outcome{}{i}{r+1}
                - \IMatTv{i \cdot}{r} \VUoutcome{}{}{r}
                \big)
                \big(
                \outcome{}{i}{s+1}
                - \IMatTv{i \cdot}{s} \VUoutcome{}{}{s}
                \big)
                \\
                &\eqas
                \lim_{N \rightarrow \infty}
                \frac{\sigma^2}{N}
                \sum_{i=1}^N
                \Uoutcome{i}{}{s} \Uoutcome{i}{}{r},
            \end{aligned}
        \end{equation}
        % 
        where in the last line, we used $\pdot{\VUoutcome{}{}{t}}{\VUoutcome{}{}{s}} = \pdot{\VUoutcome{\parallel}{}{t}}{\VUoutcome{}{}{s}}$. The desired result follows by aggregating \eqref{eq:BL-proof-bt-4-1}-\eqref{eq:BL-proof-bt-4-4}.

        
        \item We use induction hypotheses to establish the following result:
        % 
        \begin{equation}
            \label{eq:BL-a_(t)}
            \begin{aligned}
                \lim_{N \rightarrow \infty}
                \frac{1}{\cardinality{\batch}} \sum_{i \in \batch}
                \psi\big(
                \outcomeD{}{i}{0},
                \outcomeD{}{i}{1},
                \ldots,
                \outcomeD{}{i}{t+1},
                \Vtreatment{i}{},\Vcovar{i}
                \big)
                \\
                \eqas
                \E
                \Big[
                \psi
                \big(
                \outcomeD{}{\batch}{0},
                \MAVO{}{\batch}{1}
                + \MVVO{}{}{1} Z_1
                + \Houtcome{\batch}{}{0},
                \ldots,
                \MAVO{}{\batch}{t+1}
                + \MVVO{}{}{t+1} Z_t
                + \Houtcome{\batch}{}{t},
                \Vtreatment{\batch}{}, \Vcovar{\batch}
                \big)
                \Big].
            \end{aligned}
        \end{equation}
        % 
        More general results related to the extension of \eqref{eq:BL-proof-a0-dynamics-with-eps0} and \eqref{eq:BL-proof-a0-dynamics-with-eps} follow a similar procedure and are omitted for brevity.
        
        We proceed by introducing a new notation. Fixing $i$ as an arbitrary unit, we define
        % 
        \begin{equation*}
            \begin{aligned}
                \Psi^{i}(N)
                &:=
                \psi\big(
                \outcomeD{}{i}{0},
                \outcomeD{}{i}{1}, \ldots,
                \outcomeD{}{i}{t},
                \outcome{}{i}{t+1},
                \Vtreatment{i}{},
                \Vcovar{i},
                \MAVO{}{i}{(t+1)N}
                \big),
            \end{aligned}
        \end{equation*}
        % 
        where
        % 
        \begin{equation*}
        \begin{aligned}
            \MAVO{}{i}{(t+1)N} &=
            \frac{1}{N} \sum_{k=1}^N (\mu^{ik} + \mu_t^{ik}) \outcomeg{t}{} \big(\outcomeD{}{k}{t}, \Vtreatment{k}{}, \Vcovar{k}\big).
        \end{aligned}
        \end{equation*}
        %
        Using \eqref{eq:BL-proof-IH-a}, this implies that
        %
        \begin{equation}
        \label{eq:BL-at-stat-2_limit}
        \begin{aligned}
            \lim_{N \rightarrow \infty} \MAVO{}{i}{(t+1)N}
            &\eqas
            \E \left[ (\MIM{}{i}+\MIM{t}{i}) \outcomeg{t}{} \big(\MAVO{}{}{t} + \MVVO{}{}{t} Z + \Houtcome{}{}{t-1},\Vtreatment{}{},\Vcovar{}\big) \right]
            \\ &=
            (\bar{\mu}^i+\bar{\mu}^i_t) \E \left[\outcomeg{t}{} \big(\MAVO{}{}{t} + \MVVO{}{}{t} Z + \Houtcome{}{}{t-1},\Vtreatment{}{},\Vcovar{}\big) \right]
            =
            \MAVO{}{i}{t+1} < \infty.
        \end{aligned}
        \end{equation}
        % 
        Now, by \eqref{eq:conditional dist of outcome_nonsym}, we can write
        % 
        \begin{equation*}
            \begin{aligned}
                \Psi^{i}(N)
                \Big|_{\Gc_t}
                \eqd
                \psi\left(
                \outcomeD{}{i}{0},
                \outcomeD{}{i}{1}, \ldots,
                \outcomeD{}{i}{t},
                % \outcome{}{n}{1}, \ldots,
                % \outcome{}{n}{t},
                \left[
                \widetilde{\IM}
                \VUoutcome{\perp}{}{t}
                + \bm{R}_t 
                \VAPC{}{t}
                +
                \IMatT{t} \VUoutcome{}{}{t}
                \right]^i,
                \Vtreatment{i}{},
                \Vcovar{i},
                \MAVO{}{i}{(t+1)N}
                \right),
            \end{aligned}
        \end{equation*}
        % 
        where $\left[ \widetilde{\IM} \VUoutcome{\perp}{}{t} + \bm{R}_t  \VAPC{}{t} + \IMatT{t} \VUoutcome{}{}{t} \right]^i$ represent the $i^{th}$ element in the vector $ \widetilde{\IM} \VUoutcome{\perp}{}{t} + \bm{R}_t  \VAPC{}{t} + \IMatT{t} \VUoutcome{}{}{t}$. We also let
        % 
        \begin{align*}
            \widetilde{\Psi}^{i}(N) = \Psi^{i}(N) - \E_{\IM,\IMatT{t}}[\Psi^{i}(N)].
        \end{align*}
        % 
        where $\E_{\IM,\IMatT{t}}$ denotes the expectation with respect to the randomness of the interference matrices $\IM$ and $\IMatT{t}$. We follow the same approach as Step~1-\ref{item:BL-average limit}. Note that given $\Gc_t$, the elements of $\widetilde{\IM} \VUoutcome{\perp}{}{t} + \IMatT{t} \VUoutcome{}{}{t}$ are i.i.d. Gaussian random variables with a zero mean and variance~$(\hat{\rho}_{tN})^2$:
        % 
        \begin{equation}
            \label{eq:BL-bt-Yt stat}
            \begin{aligned}
                (\hat{\rho}_{tN})^2
                &:=
                \Var
                \left[
                [
                \widetilde{\IM} \VUoutcome{\perp}{}{t} + \IMatT{t} \VUoutcome{}{}{t}
                ]^i
                \Big|
                \VUoutcome{}{}{t}
                \right]
                =
                \frac{\sigma^2}{N}
                \sum_{j=1}^N
                \left(\Uoutcome{\perp,j}{}{t}\right)^2
                +
                \frac{\sigma_t^2}{N}
                \sum_{j=1}^N
                \left(\Uoutcome{j}{}{t}\right)^2,
            \end{aligned}
        \end{equation}
        % 
        where $\Uoutcome{j}{}{t} = \outcomeg{t}{}\big(\outcomeD{}{j}{t}, \Vtreatment{j}{}, \Vcovar{j}\big)$ is the $j^{th}$ element of the column vector $\VUoutcome{}{}{t}$, and similarly, $\Uoutcome{\perp,j}{}{t}$ is the $j^{th}$ element of the column vector $\VUoutcome{\perp}{}{t}$. Letting
        % 
        \begin{align}
            \label{eq:BL-bt-(-1)}
            (\hat{\rho}_t)^2 = \lim_{N\rightarrow \infty } (\hat{\rho}_{tN})^2,
        \end{align}
        % 
        we have the following lemma regarding the finiteness of $\hat{\rho}_t$.
        % 
        \begin{lemma}
            \label{lm:finite variance}
            $\hat{\rho}_t$ is almost surely finite.
        \end{lemma}
        % 
        \textbf{Proof of Lemma~\ref{lm:finite variance}.} We show that the following relations hold with a probability of 1:
        % 
        \begin{align}
            \label{eq:BL-bt-1}
            \lim_{N \rightarrow \infty}
            \frac{1}{N}
            \sum_{j=1}^N
            \left(\Uoutcome{\perp,j}{}{t}\right)^2 < \infty,
            \;\;\;
            \lim_{N \rightarrow \infty}
            \frac{1}{N}
            \sum_{j=1}^N
            (\Uoutcome{j}{}{t})^2 < \infty.
        \end{align}
        % 
        By definition, we can write
        % 
        \begin{equation}
        \label{eq:BL-bt-2}
        \begin{aligned}
            \frac{1}{N}
            \sum_{j=1}^N
            \left(\Uoutcome{\perp,j}{}{t}\right)^2
            =
            \pdot{\VUoutcome{\perp}{}{t}}{\VUoutcome{\perp}{}{t}}
            &=
            \pdot{\VUoutcome{}{}{t}}{\VUoutcome{}{}{t}}
            -
            \pdot{\VUoutcome{\parallel}{}{t}}{\VUoutcome{\parallel}{}{t}}
            =
            \frac{1}{N}
            \sum_{j=1}^N
            \left(\Uoutcome{j}{}{t}\right)^2
            -
            \frac{1}{N}
            \sum_{j=1}^N
            \left(\Uoutcome{\parallel,j}{}{t}\right)^2.
        \end{aligned}
        \end{equation}
        % 
        Then, by \eqref{eq:BL-proof-IH-a} for the function $\outcomeg{t}{}\big(\outcomeD{}{j}{t}, \Vtreatment{j}{}, \Vcovar{j}\big)^2$, we get
        % 
        \begin{equation}
        \label{eq:BL-bt-3}
        \begin{aligned}
            \lim_{N \rightarrow \infty}
            \frac{1}{N}
            \sum_{j=1}^N
            \left(\Uoutcome{j}{}{t}\right)^2
            &=
            \lim_{N \rightarrow \infty}
            \frac{1}{N}
            \sum_{j=1}^N
            \outcomeg{t}{} \big(\outcomeD{}{j}{t}, \Vtreatment{j}{}, \Vcovar{j}\big)^2
            \eqas
            \E\left[
            \outcomeg{t}{} \big(\MAVO{}{}{t} + \MVVO{}{}{t} Z + \Houtcome{}{}{t-1}, \Vtreatment{}{}, \Vcovar{}\big)^2
            \right]  < \infty,
        \end{aligned}
        \end{equation}
        % 
        where $Z \sim \Nc(0,1)$. Further, by \eqref{eq:projection sum}, we have
        % 
        \begin{align*}
            \frac{1}{N}
            \sum_{j=1}^N
            \Uoutcome{\parallel,j}{}{t}
            &=
            \frac{1}{N}
            \sum_{j=1}^N
            \sum_{s=0}^{t-1} \APC{s}{t} \Uoutcome{j}{}{s}
            =
            \sum_{s=0}^{t-1}
            \frac{\APC{s}{t}}{N}
            \sum_{j=1}^N
            \Uoutcome{j}{}{s}
            \\
            \frac{1}{N}
            \sum_{j=1}^N
            \left(\Uoutcome{\parallel,j}{}{t}\right)^2,
            &=
            \frac{1}{N}
            \sum_{j=1}^N
            \left(
            \sum_{s=0}^{t-1} \APC{s}{t} \Uoutcome{j}{}{s}
            \right)^2
            =
            \sum_{r,s=0}^{t-1} \APC{r}{t} \APC{s}{t} \pdot{\VUoutcome{}{}{r}}{\VUoutcome{}{}{s}}.
        \end{align*}
        % 
        Considering Corollary~\ref{cl:alpha is bounded}, the vector $\VAPC{}{t}$ has a finite limit as $N\rightarrow \infty$. Similar to \eqref{eq:BL-bt-3}, the induction hypothesis for the function $\psi = \outcomeg{r}{}\big(\outcomeD{}{j}{r},\Vtreatment{j}{},\Vcovar{j}\big) \outcomeg{s}{}\big(\outcomeD{}{j}{s},\Vtreatment{j}{},\Vcovar{j}\big)$ implies that almost surely
        % 
        \begin{align}
            \label{eq:BL-bt-4}
            \quad\quad
            \lim_{N\rightarrow \infty}
            \frac{1}{N}
            \sum_{j=1}^N
            \left(\Uoutcome{\parallel,j}{}{t}\right)^2
            =
            \lim_{N\rightarrow \infty}
            \sum_{r,s=0}^{t-1} \APC{r}{t} \APC{s}{t} \pdot{\VUoutcome{}{}{r}}{\VUoutcome{}{}{s}}
            < \infty.
        \end{align}
        % 
        Consequently, by \eqref{eq:BL-bt-2}-\eqref{eq:BL-bt-4}, we get the result in \eqref{eq:BL-bt-1} and the proof is complete. \ep

        An immediate corollary of the result of Lemma~\ref{lm:finite variance} is that $\hat{\rho}_{tN}$, in~\eqref{eq:BL-bt-Yt stat}, is almost surely bounded independent of $N$. Then, for $l\geq 1$, it is straightforward to show that,
        % 
        \begin{align}
            \label{eq:BL-proof-t-2}
            \E
            \left[
            \left|
                \big[
                \widetilde{\IM} \VUoutcome{\perp}{}{t} + \IMatT{t} \VUoutcome{}{}{t}
                \big]^i
                + 
                \left[\bm{R}_t 
                \VAPC{}{t}
                \right]^i
            \right|^{l}
            \right]
            \leq
            2^{l-1}
            \E
            \left[
            \left|
                \big[
                \widetilde{\IM} \VUoutcome{\perp}{}{t} + \IMatT{t} \VUoutcome{}{}{t}
                \big]^i
                \right|^{l}
                +
                \left|
                \left[\bm{R}_t 
                \VAPC{}{t}
                \right]^i
            \right|^{l}
            \right]
            \leq c,
        \end{align}
        % 
        where $c$ is a constant independent of $N$ and we used the inequality $(v_1+v_2)^l \leq 2^{l-1} (v_1^l+v_2^l),\; v_1,v_2 \geq 0$. Note that in \eqref{eq:BL-proof-t-2}, given $\Gc_t$, the term $\bm{R}_t  \VAPC{}{t}$ is deterministic and bounded by Corollary~\ref{cl:alpha is bounded}. Then, fixing $0 < \kappa < 1$ and using the fact that $\psi \in \poly{k}$ and so $|\psi(\Vec{\omega})|\leq c (1 + \norm{\Vec{\omega}}^k)$, by Assumption~\ref{asmp:weak_limits}, we get,
        % 
        \begin{equation}
            \label{eq:BL-proof-t-1}
            \begin{aligned}
                \frac{1}{\cardinality{\batch}}
                \sum_{i \in \batch}
                \E\left[\left| \widetilde{\Psi}^{i}(N)\right|^{2+\kappa} \right]
                \leq c \cardinality{\batch}^{\kappa/2}.
            \end{aligned}
        \end{equation}
        % 
        Therefore, we can apply the SLLN for triangular arrays in Theorem~\ref{thm:SLLN} to obtain the following result:
        % 
        \begin{equation}
        \label{eq:BL-proof-t-3}
        \begin{aligned}
            \lim_{N \rightarrow \infty}
            \frac{1}{\cardinality{\batch}}
            \sum_{i\in \batch}
            &\psi\big(
                \outcomeD{}{i}{0},
                \outcomeD{}{i}{1}, \ldots,
                \outcomeD{}{i}{t},
                \outcome{}{i}{t+1},
                \Vtreatment{i}{},
                \Vcovar{i},
                \MAVO{}{i}{(t+1)N}
            \big)
            \\
            \eqas
            \;\lim_{N \rightarrow \infty}
            \frac{1}{\cardinality{\batch}}
            \sum_{i\in \batch}
            \E_{\IM,\IMatT{t}}
            \Big[
            \psi&\Big(
                \outcomeD{}{i}{0},
                \outcomeD{}{i}{1}, \ldots,
                \outcomeD{}{i}{t},
                \left[
                \widetilde{\IM}
                \VUoutcome{\perp}{}{t}
                + \bm{R}_t 
                \VAPC{}{t}
                +
                \IMatT{t} \VUoutcome{}{}{t}
                \right]^i,
                \Vtreatment{i}{}, \Vcovar{i},
                \HAVO{}{i}{(t+1)N}
                \Big)
            \Big].
        \end{aligned}
        \end{equation}
        % 
        Now, considering \eqref{eq:BL-at-stat-2_limit} and \eqref{eq:BL-bt-(-1)}, we can write
        % 
        \begin{equation}
        \label{eq:BL-proof-t-t}
        \begin{aligned}
            &\lim_{N \rightarrow \infty}
            \frac{1}{\cardinality{\batch}}
            \sum_{i\in \batch}
            \E_{\IM,\IMatT{t}}
            \Big[
            \psi\Big(
                \outcomeD{}{i}{0},
                \outcomeD{}{i}{1}, \ldots,
                \outcomeD{}{i}{t},
                \left[
                \widetilde{\IM}
                \VUoutcome{\perp}{}{t}
                + \bm{R}_t 
                \VAPC{}{t}
                +
                \IMatT{t} \VUoutcome{}{}{t}
                \right]^i,
                \Vtreatment{i}{}, \Vcovar{i},
                \MAVO{}{i}{(t+1)N}
                \Big)
            \Big]
            \\
            \eqas
            &\lim_{N \rightarrow \infty}
            \frac{1}{\cardinality{\batch}}
            \sum_{i \in \batch}
            \E_{Z}
            \Big[
            \psi\Big(
                \outcomeD{}{i}{0},
                \outcomeD{}{i}{1}, \ldots,
                \outcomeD{}{i}{t},
                \hat{\rho}_t Z
                +
                \left[
                \bm{R}_t 
                \VAPC{}{t}
                \right]^i,
                \Vtreatment{i}{}, \Vcovar{i},
                \MAVO{}{i}{t+1}
                \Big)
            \Big],
        \end{aligned}
        \end{equation}
        %
        where similar to \eqref{eq:BL-a0-limit to the function} in the first step, we utilized the dominated convergence theorem and the continuous mapping theorem to pass the limit through the expectation and the function, respectively.
        
        From Eq.~\eqref{eq:BL-at-stat-2_limit}, recall that $\MAVO{}{i}{t+1} = (\bar{\mu}^i+\bar{\mu}^i_t) \E \left[\outcomeg{t}{} \big(\MAVO{}{}{t} + \MVVO{}{}{t} Z + \Houtcome{}{}{t-1},\Vtreatment{}{},\Vcovar{}\big) \right]$; accordingly, we define
        % 
        \begin{equation*}
        \begin{aligned}
            \widehat{\psi}&
            \big(
            \outcomeD{}{i}{0},
            \outcomeD{}{i}{1},
            \ldots,
            \outcomeD{}{i}{t},
            \outcome{}{i}{1} - \IMatTv{i \cdot}{0}\VUoutcome{}{}{0}
            , \ldots,
            \outcome{}{i}{t} - \IMatTv{i\cdot}{t-1}\VUoutcome{}{}{t-1},
            \Vtreatment{i}{},
            \Vcovar{i},
            \bar{\mu}^i, \bar{\mu}^i_t
            \big)
            \\
            :=
            \E_{Z}
            \Big[
            \psi&\Big(
                \outcomeD{}{i}{0},
                \outcomeD{}{i}{1},
                \ldots,
                \outcomeD{}{i}{t},
                \hat{\rho}_{t} Z
                +
                \sum_{s=0}^{t-1}
                \APC{t}{s}
                \big(
                \outcome{}{i}{s+1}
                - \IMatTv{i \cdot}{s}\VUoutcome{}{}{s}
                \big),
                \Vtreatment{i}{},
                \Vcovar{i},
                \MAVO{}{i}{t+1}
                \Big)
            \Big].
        \end{aligned}
        \end{equation*}
        % 
        Considering \eqref{eq:BL-proof-IH-a1} as well as \eqref{eq:BL-proof-t-3}-\eqref{eq:BL-proof-t-t}, for the function $\widehat{\psi}$, we have
        % 
        \begin{equation}
        \label{eq:BL-proof-t-new function}
        \begin{aligned}
            &\lim_{N \rightarrow \infty}
            \frac{1}{\cardinality{\batch}}
            \sum_{i\in \batch}
            \psi\big(
                \outcomeD{}{i}{0},
                \outcomeD{}{i}{1}, \ldots,
                \outcomeD{}{i}{t},
                \outcome{}{i}{t+1},
                \Vtreatment{i}{},
                \Vcovar{i},
                \MAVO{}{i}{(t+1)N}
            \big)
            \\
            \eqas
            &\;
            \E
            \bigg[
            \widehat{\psi}\bigg(
                \outcomeD{}{\batch}{0},
                \MAVO{}{\batch}{1}
                +
                \MVVO{}{}{1} Z_1
                +
                \Houtcome{\batch}{}{0},
                \ldots,
                \MAVO{}{\batch}{t}
                +
                \MVVO{}{}{t} Z_t
                +
                \Houtcome{\batch}{}{t-1},
                \BVVO{}{}{1}
                Z'_{1},
                \ldots,
                \BVVO{}{}{t}
                Z'_{t},
                \Vtreatment{\batch}{}, \Vcovar{\batch},
                \MIM{}{\batch}, \MIM{t}{\batch}
                \bigg)
            \bigg]
            \\
            \eqas
            &\;
            \E
            \bigg[
            \psi\bigg(
                \outcomeD{}{\batch}{0},
                \MAVO{}{\batch}{1}
                +
                \MVVO{}{}{1} Z_1
                +
                \Houtcome{\batch}{}{0},
                \ldots,
                \MAVO{}{\batch}{t}
                +
                \MVVO{}{}{t} Z_t
                +
                \Houtcome{\batch}{}{t-1},
                \hat{\rho}_{t} Z
                +
                \sum_{s=0}^{t-1}
                \APC{t}{s}
                \BVVO{}{}{s+1}
                Z'_{s+1},
                \Vtreatment{\batch}{}, \Vcovar{\batch},
                \MAVO{}{\batch}{t+1}
                \bigg)
            \bigg],
        \end{aligned}
        \end{equation}
        % 
        where $Z$ is a standard Gaussian random variable, independent of all other variables, as the inherent randomness arises from $\widetilde{\IM}$ and $\IMatT{t}$. Additionally, $\widehat{\psi}$ belongs to $\poly{k}$, since $\MAVO{}{i}{t+1}$ in the calculations of \eqref{eq:BL-proof-t-new function} can be viewed as a linear function depending solely on $\bar{\mu}^i$ and $\bar{\mu}^i_t$.
        
        Now, we need to show that
        % 
        \begin{equation}
            \label{eq:BL-proof-bt-dynamics-1}
            \begin{aligned}
                \Var
                \left[
                \hat{\rho}_{t} Z
                +
                \sum_{s=0}^{t-1}
                \APC{t}{s}
                \big(
                \BVVO{}{}{s+1}
                Z'_{s+1}
                \big)
                \right]
                &=
                (\MVVO{}{}{t+1})^2.
            \end{aligned}
        \end{equation}
        % 
        Considering that $(Z'_1,\ldots,Z'_t)$ follows a joint Normal distribution independent of $Z$, the random variable $\hat{\rho}_{t} Z + \sum_{s=0}^{t-1} \APC{t}{s} \BVVO{}{}{s+1} Z'_{s+1}$ is Gaussian. To obtain \eqref{eq:BL-proof-bt-dynamics-1}, we let $\psi = (\outcome{}{i}{t+1})^2$ in \eqref{eq:BL-proof-t-new function}. This yields
        % 
        \begin{equation}
            \label{eq:BL-proof-bt-dynamics-2}
            \begin{aligned}
                \lim_{N \rightarrow \infty}
                \frac{1}{N}
                \sum_{i=1}^N
                \big(
                \outcome{}{i}{t+1}\big)^2
                &=
                \E
                \left[
                \left(
                \hat{\rho}_{t} Z
                +
                \sum_{s=0}^{t-1}
                \APC{t}{s}
                \BVVO{}{}{s+1}
                Z'_{s+1}
                \right)^2
                \right].
            \end{aligned}
        \end{equation}
        % 
        Meanwhile, by \eqref{eq:BL-proof-bt-2-result}, we have
        % 
        \begin{align}
        \label{eq:BL-proof-bt-dynamics-3}
            \lim_{N \rightarrow \infty}
            \frac{1}{N}
            \sum_{i=1}^N
            (\outcome{}{i}{t+1})^2
            \eqas
            (\MVVO{}{}{t+1})^2.
        \end{align}
        % 
        Combining \eqref{eq:BL-proof-bt-dynamics-2} and \eqref{eq:BL-proof-bt-dynamics-3}, we derive the desired result as stated in \eqref{eq:BL-proof-bt-dynamics-1}.

        Finally, similar to \eqref{eq:BL-a0-function_re_def} and based on outcome representation in \eqref{eq:apndx_outcome_function}, we define the function $\widetilde{\psi}$ such that
        % 
        \begin{align*}
            \psi\big(
                \outcomeD{}{i}{0},
                \outcomeD{}{i}{1}, \ldots,
                \outcomeD{}{i}{t},
                \outcomeD{}{i}{t+1},
                \Vtreatment{i}{},
                \Vcovar{i}
            \big)
            &=
            \psi\big(
                \outcomeD{}{i}{0},
                \outcomeD{}{i}{1}, \ldots,
                \outcomeD{}{i}{t},
                \outcome{}{i}{t+1} + \MAVO{}{i}{(t+1)N} + \outcomeh{t}{} \big(\outcomeD{}{i}{t}, \Vtreatment{i}{}, \Vcovar{i}\big),
                \Vtreatment{i}{},
                \Vcovar{i}
            \big)
            \\
            &=
            \widetilde\psi \big(
                \outcomeD{}{i}{0},
                \outcomeD{}{i}{1}, \ldots,
                \outcomeD{}{i}{t},
                \outcome{}{i}{t+1},
                \Vtreatment{i}{},
                \Vcovar{i}, \MAVO{}{i}{(t+1)N}
            \big).
        \end{align*}
        %
        Whenever $\psi \in \poly{k}$, by Assumption~\ref{asmp:BL}, we get that $\widetilde\psi \in \poly{k}$, and applying the result in Eq.~\eqref{eq:BL-proof-t-new function} for the function $\widetilde\psi$ yields the desired result. \ep
        
        
    \end{enumerate}




\section{Estimation of Counterfactual Evolutions}
\label{sec:estimation_theory}
% 
In this section, we present a general framework for counterfactual estimation based on the outcome specification in Eq.~\eqref{eq:outcome_function_matrix}. Specifically, given a desired treatment allocation matrix $\OMtreatment{u}{}$, and observing $\Moutcome{}{}{}(\Mtreatment{}{}=\OMtreatment{o}{})$, $\OMtreatment{o}{}$, and $\covar$, we aim to estimate the counterfactual evolution denoted by $\CFE{\OMtreatment{u}{}}{}{t}$ as defined in \eqref{eq:sample_mean_outcomes}.

To state the main theoretical result of this section, we parameterize the unknown functions in the state evolution equations. Specifically, let $\outcomeg{t}{}\big(\cdot; \param{\outcomeg{t}{}}\big)$ and $\outcomeh{t}{}\big(\cdot; \param{\outcomeh{t}{}}\big)$ represent the parameterized forms of $\outcomeg{t}{}(\cdot)$ and $\outcomeh{t}{}(\cdot)$ for $t = 0, \ldots, T-1$, respectively. Here, $\Eparam{\outcomeg{t}{}}$ and $\Eparam{\outcomeh{t}{}}$ are vectors of appropriate dimensions, denoting the parameters of the respective functions. 
% 
\begin{assumption}
    \label{asmp:independent_interference_mean}
    % 
    Considering \eqref{eq:state evolution}, for all $t$, we assume there exists a modification of the function $\outcomeg{t}{}$, which, with a slight abuse of notation, is also denoted by $\outcomeg{t}{}$ such that:
    % 
    \begin{align}
    \label{eq:independent_interference_mean}
        \MAVO{}{}{t+1} =
        \E\left[
        \outcomeg{t}{}\big(\MAVO{}{}{t} + \MVVO{}{}{t} Z_t + \Houtcome{}{}{t-1}, \Vtreatment{}{}, \Vcovar{}\big)
        \right].
    \end{align}
    % 
\end{assumption}
% 
A simple example where Assumption~\ref{asmp:independent_interference_mean} holds is when the random variable $\MIM{}{} + \MIM{t}{}$ is independent of all other sources of randomness. In this case, we can normalize the mean to 1 by adjusting the function $\outcomeg{t}{}$, scaling it by an appropriate constant factor.

For further simplicity in notation, we also define $\sigma_t := \sigma + \sigma_t$. Then, we can collect the unknown parameters in the state evolution equations in \eqref{eq:state evolution} as follows:
%
\begin{align}
    \label{eq:apndx_unknowns}
    \Uc := \left(\{\sigma_t\}, \{\param{\outcomeg{t}{}} \}, \{\param{\outcomeh{t}{}}\}\right).
\end{align}
%
We denote an estimation of $\Uc$ as $\widehat{\Uc} := \left(\left\{\hat{\sigma}_t\right\}, \{\Eparam{\outcomeg{t}{}}\}, \{\Eparam{\outcomeh{t}{}}\}\right)$. We can then employ Algorithm~\ref{alg:CF estimation} to compute the desired counterfactual.
% 
\begin{algorithm}
\caption{General counterfactual estimation}
\label{alg:CF estimation}
% 
\begin{algorithmic}
% 
\Require $\Moutcome{}{}{}(\Mtreatment{}{}=\OMtreatment{o}{}), \OMtreatment{o}{}, \covar$, $\OMtreatment{u}{}$, and $Z_i \sim \Nc(0,1)$ for $i=1, \ldots, N$

\State \hspace{-1.3em} \textbf{Step 1: Parameters Estimation}

\State Estimate the set of unknown parameters $\Uc$ by $\widehat{\Uc}$.


\State \hspace{-1.3em} \textbf{Step 2: Counterfactual Estimation}



\State $\EAVO{}{}{1} \gets \frac{1}{N} \sum_{i=1}^N \outcomeg{0}{}\big(\outcomeD{}{i}{0}, \OVtreatment{i}{u}, \Vcovar{i}; \Eparam{\outcomeg{0}{}} \big)$ 

\State $\EVVO{}{}{1} \gets \frac{\hat{\sigma}_0}{N} \sum_{i=1}^N \outcomeg{0}{}\big(\outcomeD{}{i}{0}, \OVtreatment{i}{u}, \Vcovar{i}; \Eparam{\outcomeg{0}{}}\big)^2$

\State $\EHoutcome{i}{}{0} \gets \outcomeh{0}{}\big(\outcomeD{}{i}{0}, \OVtreatment{i}{u}, \Vcovar{i}; \Eparam{\outcomeh{0}{}}\big),\; i = 1, \ldots, N$ 

\State $\ECF{}{1}{\OMtreatment{u}{}} \gets \EAVO{}{}{1} + \frac{1}{N} \sum_{i=1}^N \EHoutcome{i}{}{0}$

\For{$t = 1, \ldots, T-1$}

\State $\EHoutcome{i}{}{t} \gets \outcomeh{t}{}\big(\EAVO{}{}{t} + \EVVO{}{}{t} Z^i + \EHoutcome{i}{}{t-1}, \OVtreatment{i}{u}, \Vcovar{i}; \Eparam{\outcomeh{t}{}}\big),\; i = 1, \ldots, N$

\State $\EAVO{}{}{t+1} \gets \frac{1}{N} \sum_{i=1}^N \outcomeg{t}{}\big(\EAVO{}{}{t} + \EVVO{}{}{t} Z^i + \EHoutcome{i}{}{t-1}, \OVtreatment{i}{u}, \Vcovar{i}; \Eparam{\outcomeg{t}{}}\big)$

\State $\EVVO{}{}{t+1} \gets \frac{\hat{\sigma}_t}{N} \sum_{i=1}^N \outcomeg{t}{} \big(\EAVO{}{}{t} + \EVVO{}{}{t} Z^i + \EHoutcome{i}{}{t-1}, \OVtreatment{i}{u}, \Vcovar{i}; \Eparam{\outcomeg{t}{}}\big)^2$

\State $\ECF{}{t+1}{\OMtreatment{u}{}} \gets \EAVO{}{}{t+1} +  \frac{1}{N} \sum_{i=1}^N \EHoutcome{i}{}{t}$

\EndFor

\Ensure $\ECF{}{t}{\OMtreatment{u}{}}$, $t = 1, \ldots, T$.
% 
\end{algorithmic}
\end{algorithm}
% 

% 
\begin{assumption}[Continuous parameterization]
    \label{asmp:continuous_parameterization}
    For all $t$, the mappings $\param{\outcomeg{t}{}} \mapsto \outcomeg{t}{} \big(\cdot; \param{\outcomeg{t}{}} \big)$ and $\param{\outcomeh{t}{}} \mapsto \outcomeh{t}{} \big(\cdot; \param{\outcomeh{t}{}} \big)$ are continuous functions.
\end{assumption}
% 
\begin{assumption}[Consistent parameters estimation]
    \label{asmp:consistent_parameter_estimation}
    $\widehat{\Uc}$ is a consistent estimator of $\Uc$; that is, $\widehat{\Uc} \xrightarrow{P} \Uc$ as $N \rightarrow \infty$.
\end{assumption}
% 
\begin{assumption}[All control initialization]
    \label{asmp:all_control_initialization}
    There is no treatment at time $t=0$; that means $\treatment{i}{0}=0$, for all $i\in[N]$, and no treatment is anticipated.
\end{assumption}
% 

In the following, we demonstrate the consistency of the results from Algorithm~\ref{alg:CF estimation} under above assumptions.
% 
\begin{theorem}[Consistency]
    \label{thm:consistency}
    Let the conditions of Lemma~\ref{lm:Big lemma} and Assumptions~\ref{asmp:independent_interference_mean}-\ref{asmp:all_control_initialization} hold. In particular, Assumption~\ref{asmp:weak_limits} holds for both observed and desired treatment allocations $\OMtreatment{o}{}$ and $\OMtreatment{u}{}$. Then, for any $t$, $\ECF{}{t}{\OMtreatment{u}{}}$ provides a consistent estimator for $\AVOW{\OMtreatment{u}{}}{}{t}$.
\end{theorem}

\begin{remark}
    If the consistency in Assumption~\ref{asmp:consistent_parameter_estimation} is strong, i.e., $\widehat{\Uc} \xrightarrow{a.s.} \Uc$ as $N \rightarrow \infty$, then the consistency in Theorem~\ref{thm:consistency} also holds strongly.
\end{remark}
% 
\begin{remark}
    We can generalize Algorithm~\ref{alg:CF estimation} in several ways. First, when considering outcome specifications with $l \geq 1$ lag terms, we can modify the algorithm accordingly, and extend Assumption~\ref{asmp:all_control_initialization} to require $l$ historical observations: $\treatment{i}{t}=0$ for all $t \leq l$ and $i\in[N]$. We can also relax Assumption~\ref{asmp:all_control_initialization} by beginning from any arbitrary state, provided we enforce the initial conditions to the desired counterfactual scenario--- specifically, requiring that $\OMtreatment{o}{}$ and $\OMtreatment{u}{}$ match for the first $l$ time periods.
\end{remark}
\noindent
\textbf{Proof.}
% 
We use an induction argument on $t \geq 1$ to prove the following more general statement. As $N \rightarrow \infty$, we show that
% 
\begin{equation}
    \label{eq:detailed_consistency}
    \begin{aligned}
        \EAVO{}{}{t} \xrightarrow{P} \MAVO{}{}{t},
        \quad\quad\quad
        \EVVO{}{}{t} \xrightarrow{P} \MVVO{}{}{t},
        \quad\quad\quad
        \frac{1}{N} \sum_{i=1}^N \EHoutcome{i}{}{t-1}\xrightarrow{P} \E\left[ \Houtcome{}{}{t-1} \right],
    \end{aligned}
\end{equation}
% 
Then, it is straightforward to see that $\ECF{}{t}{\OMtreatment{u}{}}$ also converges to $\AVOW{\OMtreatment{u}{}}{}{t} = \MAVO{}{}{t} + \E\left[\Houtcome{}{}{t-1}\right]$ in probability, whenever $N \rightarrow \infty$.

\textbf{Step 1.} Let $t = 1$. We begin by proving the first result in \eqref{eq:detailed_consistency}. To this end, we add the notation $(N)$ to the quantities associated with the system containing $N$ experimental units. We have
% 
\begin{equation}
    \label{eq:consistency_proof_1}
    \begin{aligned}
        \EAVO{N}{}{1}
        &=
        \frac{1}{N} \sum_{i=1}^N \outcomeg{0}{}\big(\outcomeD{}{i}{0}, \OVtreatment{i}{u}, \Vcovar{i}; \Eparam{\outcomeg{0}{}}(N) \big)
    \end{aligned}
\end{equation}
% 
Note that, by Assumption~\ref{asmp:continuous_parameterization}, the right-hand side of \eqref{eq:consistency_proof_1} can be seen as a continuous function of $\Eparam{\outcomeg{0}{}}(N)$. Therefore, applying the continuous mapping theorem, e.g., Theorem 2.3 in \cite{van2000asymptotic}, implies that
% 
\begin{equation}
    \label{eq:consistency_proof_2}
    \begin{aligned}
        \lim_{N \rightarrow \infty} \EAVO{N}{}{1}
        =
        \lim_{N \rightarrow \infty} \frac{1}{N} \sum_{i=1}^N \outcomeg{0}{}\big(\outcomeD{}{i}{0}, \OVtreatment{i}{u}, \Vcovar{i}; \Eparam{\outcomeg{0}{}}(N) \big)
        \eqp
        \lim_{N \rightarrow \infty} \frac{1}{N} \sum_{i=1}^N \outcomeg{0}{}\big(\outcomeD{}{i}{0}, \OVtreatment{i}{u}, \Vcovar{i}; \param{\outcomeg{0}{}} \big)
        \eqas
        \MAVO{}{}{1}.
    \end{aligned}
\end{equation}
% 
In the last equality, we used the result of Theorem~\ref{thm:Batch_SE}. A similar argument, combined with Assumption~\ref{asmp:BL}-\ref{asmp:BL-bound on initials}, yields the second result in \eqref{eq:detailed_consistency} for $t=1$. The third result also follows immediately.

Fixing an arbitrary function $\psi \in \poly{k}$, we also need the following intermediary result:
% 
\begin{align}
    \label{eq:consistency_proof_intermediary}
    \lim_{N \rightarrow \infty} \frac{1}{N} \sum_{i=1}^N \psi\big(Z^i, \EHoutcome{i}{N}{0}, \OVtreatment{i}{u}, \Vcovar{i}\big)
    \eqp
    \E \left[
    \psi\big(Z, \Houtcome{}{}{0}, \Vtreatment{}{u}, \Vcovar{}\big)
    \right],
\end{align}
% 
where $\Vtreatment{}{u}$ represents the weak limit of $\OVtreatment{i}{u}$'s and $Z \sim \Nc(0,1)$. To obtain this result, let $\widetilde{\psi}$ be the function such that
% 
\begin{align*}
    \lim_{N \rightarrow \infty} \frac{1}{N} \sum_{i=1}^N \psi\big(Z^i, \EHoutcome{i}{N}{0}, \OVtreatment{i}{u}, \Vcovar{i}\big)
    &=
    \lim_{N \rightarrow \infty} \frac{1}{N} \sum_{i=1}^N \psi\big(Z^i, \outcomeh{0}{}\big(\outcomeD{}{i}{0}, \OVtreatment{i}{u}, \Vcovar{i}; \Eparam{\outcomeh{0}{}}(N)\big), \OVtreatment{i}{u}, \Vcovar{i}\big)
    \\
    &=
    \lim_{N \rightarrow \infty} \frac{1}{N} \sum_{i=1}^N \widetilde\psi\big(Z^i, \outcomeD{}{i}{0}, \OVtreatment{i}{u}, \Vcovar{i}, \Eparam{\outcomeh{0}{}}(N)\big)
    \\
    &\eqp
    \lim_{N \rightarrow \infty} \frac{1}{N} \sum_{i=1}^N \widetilde\psi\big(Z^i, \outcomeD{}{i}{0}, \OVtreatment{i}{u}, \Vcovar{i}, \param{\outcomeh{0}{}}\big)
    \\
    &\eqas
    \E \left[ \widetilde\psi\big(Z, \outcomeD{}{}{0}, \Vtreatment{}{u}, \Vcovar{}, \param{\outcomeh{0}{}}\big)
    \right].
\end{align*}
% 
Above, we used the continuous mapping theorem and Theorem~\ref{thm:SLLN-2} in view of Assumption~\ref{asmp:weak_limits}. Note that $\widetilde{\psi} \in \poly{k}$ because of Assumption~\ref{asmp:BL}-\ref{asmp:BL-pl h-functions}.


\textbf{Induction Hypothesis (IH).} Suppose that the limits in \eqref{eq:detailed_consistency} hold true for $t$ and also
% 
\begin{align}
    \label{eq:consistency_proof_intermediaryP_IH}
    \lim_{N \rightarrow \infty} \frac{1}{N} \sum_{i=1}^N \psi\big(Z^i, \EHoutcome{i}{N}{t-1}, \OVtreatment{i}{u}, \Vcovar{i}\big)
    \eqp
    \E \left[
    \psi\big(Z, \Houtcome{}{}{t-1}, \Vtreatment{}{u}, \Vcovar{}\big)
    \right].
\end{align}
% 

\textbf{Step 2.} We show that $\EAVO{N}{}{t+1} \xrightarrow{P} \MAVO{}{}{t+1}$. By the induction hypothesis and reusing the continuous mapping theorem, we have
% 
\begin{equation}
    \label{eq:consistency_proof_4}
    \begin{aligned}
        \lim_{N \rightarrow \infty} \EAVO{N}{}{t+1} 
        &=
        \lim_{N \rightarrow \infty} \frac{1}{N} \sum_{i=1}^N \outcomeg{t}{}\big(\EAVO{N}{}{t} + \EVVO{N}{}{t} Z^i + \EHoutcome{i}{N}{t-1}, \OVtreatment{i}{u}, \Vcovar{i}; \Eparam{\outcomeg{t}{}}(N)\big)
        \\
        &\eqp
        \lim_{N \rightarrow \infty} \frac{1}{N} \sum_{i=1}^N \outcomeg{t}{}\big(\MAVO{}{}{t} + \MVVO{}{}{t} Z^i + \EHoutcome{i}{N}{t-1}, \OVtreatment{i}{u}, \Vcovar{i}; \param{\outcomeg{t}{}}\big)
        \\
        &\eqp
        \MAVO{}{}{t+1}.
    \end{aligned}
\end{equation}
% 
Similarly, one can establish $\EVVO{N}{}{t+1} \xrightarrow{P} \MVVO{}{}{t+1}$ as well as $\frac{1}{N} \sum_{i=1}^N \EHoutcome{i}{}{t}\xrightarrow{P} \E\left[ \Houtcome{}{}{t} \right]$. We therefore conclude the proof by demonstrating the following result:
% 
\begin{align}
    \label{eq:consistency_proof_intermediaryP_step2}
    \lim_{N \rightarrow \infty} \frac{1}{N} \sum_{i=1}^N \psi\big(Z^i, \EHoutcome{i}{N}{t}, \OVtreatment{i}{u}, \Vcovar{i}\big)
    \eqp
    \E \left[
    \psi\big(Z, \Houtcome{}{}{t}, \Vtreatment{}{u}, \Vcovar{}\big)
    \right].
\end{align}
% 
For this purpose, considering $\EHoutcome{i}{N}{t} = \outcomeh{t}{}\big(\EAVO{N}{}{t} + \EVVO{N}{}{t} Z^i + \EHoutcome{i}{N}{t-1}, \OVtreatment{i}{u}, \Vcovar{i}; \Eparam{\outcomeh{t}{}}(N)\big)$, for a proper choice of the functions $\widetilde{\psi}$ and $\widetilde{\psi}'$, we can write:
% 
\begin{align*}
    \lim_{N \rightarrow \infty} \frac{1}{N} \sum_{i=1}^N \psi\big(Z^i, \EHoutcome{i}{N}{t}, \OVtreatment{i}{u}, \Vcovar{i}\big)
    &=
    \lim_{N \rightarrow \infty} \frac{1}{N} \sum_{i=1}^N \widetilde\psi\big(Z^i, \EAVO{N}{}{t}, \EVVO{N}{}{t}, \EHoutcome{i}{N}{t-1}, \OVtreatment{i}{u}, \Vcovar{i}, \Eparam{\outcomeh{t}{}}(N)\big)
    \\
    &\eqp
    \lim_{N \rightarrow \infty} \frac{1}{N} \sum_{i=1}^N \widetilde\psi\big(Z^i, \MAVO{}{}{t}, \MVVO{}{}{t}, \EHoutcome{i}{N}{t-1}, \OVtreatment{i}{u}, \Vcovar{i}, \param{\outcomeh{t}{}}\big)
    \\
    &=
    \lim_{N \rightarrow \infty} \frac{1}{N} \sum_{i=1}^N \widetilde\psi'\big(Z^i, \EHoutcome{i}{N}{t-1}, \OVtreatment{i}{u}, \Vcovar{i}\big)
    \\
    &\eqp
    \E \left[
    \widetilde\psi' \big(Z, \Houtcome{}{}{t-1}, \Vtreatment{}{u}, \Vcovar{}\big)
    \right],
\end{align*}
% 
where in the last line we used the induction hypothesis. Considering the definition of functions $\widetilde{\psi}$ and $\widetilde{\psi}'$, the proof is complete. \ep 


\subsection{Application to Bernoulli Randomized Design}
\label{sec:application_to_BRD}
%
We showcase the applicability of our framework by considering a Bernoulli randomized design, where each unit $i$ at time $t$ receives treatment with a probability denoted by $\expr_t$. Therefore, $\treatment{i}{t} \sim Bernoulli(\expr_t)$, and the $\treatment{i}{t}$'s are independent across experimental units.

We consider a first-order yet non-linear approximation of functions $\outcomeg{t}{}$ and $\outcomeh{t}{}$ in the outcome specification in \eqref{eq:outcome_function_matrix}. Specifically, we let $\Vcovar{i} = (\BE_{g}^i, \CE_{gl}^i, \ldots, \CE_{g1}^i, \DE_g^i, \PE_g^i, \BE_{h}^i, \CE_{hl}^i, \ldots, \CE_{h1}^i, \DE_h^i, \PE_h^i)^\top$ and define
% 
\begin{equation}
    \label{eq:function_structure}
    \begin{aligned}
        \outcomeg{t}{} \left(
        \outcomeD{}{i}{t-l+1}, \ldots, \outcomeD{}{i}{t},
        \Vtreatment{i}{}, \Vcovar{i}
        \right)
        &= \BE_{g}^i + \CE_{gl}^i \outcomeD{}{i}{t-l+1} + \ldots + \CE_{g1}^i \outcomeD{}{i}{t} + 
        \DE_g^i \treatment{i}{t+1} + \PE_g^i \outcomeD{}{i}{t} \treatment{i}{t+1}
        \\
        \outcomeh{t}{} \left(
        \outcomeD{}{i}{t-l+1}, \ldots, \outcomeD{}{i}{t},
        \Vtreatment{i}{}, \Vcovar{i}
        \right)
        &= \BE_{h}^i + \CE_{hl}^i \outcomeD{}{i}{t-l+1} + \ldots + \CE_{h1}^i \outcomeD{}{i}{t} + 
        \DE_h^i \treatment{i}{t+1} + \PE_h^i \outcomeD{}{i}{t} \treatment{i}{t+1}.
    \end{aligned}
\end{equation}
%
\begin{remark}
    In \eqref{eq:function_structure}, we allow the parameters of functions $\outcomeg{t}{}$ and $\outcomeh{t}{}$ to vary across units. These parameters can be viewed as unobserved unit-specific covariates that characterize how each unit responds to interventions. Additional unit-specific characteristics can be incorporated as observed covariates in the vectors $\Vcovar{i},\; i \in [N]$. We omit these details for brevity.
\end{remark}


We continue by considering a subpopulation of experimental units denoted by $\batch$. We assume that the sampling rule determining $\batch$ depends \emph{only} on the treatment allocations. Because the treatment allocation is independent of other variables, we can assume in the state evolution equations outlined in Eq.~\eqref{eq:state evolution} that $\MIM{\cdot}{\batch}$ and $\Vcovar{\batch}$ are equal to their global counterparts $\MIM{\cdot}{}$ and $\Vcovar{}$, respectively. This reflects the idea that sampling based on treatment allocation is equivalent to random sampling from the experimental population. Thus, by the state evolution equations, for $t = 0, 1, \ldots, l-1$, we can write,
% 
\begin{equation}
    \label{eq:SE_BRD_1}
    \begin{aligned}
        \AVO{}{}{t}
        &=
        \lim_{N \rightarrow \infty}
        \frac{1}{N}
        \sum_{i=1}^N
        \outcomeD{}{i}{t},
        \quad
        \AVO{}{\batch}{t}
        =
        \lim_{N \rightarrow \infty}
        \frac{1}{\cardinality{\batch}}
        \sum_{i \in \batch}
        \outcomeD{}{i}{t},
    \end{aligned}
\end{equation}
% 
and for $t \geq l-1$,
% 
\begin{equation}
    \label{eq:SE_BRD_2}
    \begin{aligned}
        \MAVO{}{}{t+1}
        &=
        \ABE_{g} +
        \ACE_{gl} \AVO{}{}{t-l+1} + \ldots + \ACE_{g1} \AVO{}{}{t} +  
        \ADE_g \expr_{t+1} +
        \APE_g \expr_{t+1} \AVO{}{}{t}
        \\
        \AVO{}{\batch}{t+1}
        &=
        \MAVO{}{}{t+1} +
        \ABE_{h} +
        \ACE_{hl} \AVO{}{\batch}{t-l+1} + \ldots + \ACE_{h1} \AVO{}{\batch}{t} +  
        \ADE_h \expr_{t+1}^\batch +
        \APE_h \expr_{t+1}^\batch \AVO{}{\batch}{t},
    \end{aligned}
\end{equation}
% 
where
% 
\begin{equation}
\label{eq:apndx_BRD_parameters_mean}
\begin{aligned}
    &(\ABE_{g}, \ACE_{gl}, \ldots, \ACE_{g1}, \ADE_g, \APE_g, \ABE_{h}, \ACE_{hl}, \ldots, \ACE_{h1}, \ADE_h, \APE_h)^\top
    \\
    :=
    &\E \left[ 
    \Vcovar{} =
    (\BE_{g}, \CE_{gl}, \ldots, \CE_{g1}, \DE_g, \PE_g, \BE_{h}, \CE_{hl}, \ldots, \CE_{h1}, \DE_h, \PE_h)^\top
    \right].
\end{aligned}
\end{equation}
% 
Here, $\Vcovar{}$ represents the weak limit of $\Vcovar{1}, \ldots, \Vcovar{N}$ when $N \rightarrow \infty$, as specified by Assumption~\ref{asmp:weak_limits}. Then, Equation~\eqref{eq:SE_BRD_2} follows from Assumption~\ref{asmp:independent_interference_mean} and the additional assumption about the elements of $\Vcovar{}$. For example, \eqref{eq:SE_BRD_2} holds when the elements of $\Vcovar{}$ are random variables independent of all other sources of randomness in the model.

To proceed with counterfactual estimation, we consider $b$ distinct subpopulations, denoted by $\batch_1, \ldots, \batch_b$, each determined solely by treatment allocations. With convention $\ABE := \ABE_{g} + \ABE_{h}$ in \eqref{eq:SE_BRD_2}, we can write the following linear regression model:
% 
\begin{equation}
    \label{eq:apndx_BRD_model}
    \begin{aligned}
        \begin{bmatrix}
            \AVO{}{\batch_1}{l}
            \\
            \AVO{}{\batch_1}{l+1}
            \\
            \vdots
            \\
            \AVO{}{\batch_1}{T}
            \\
            \\
            \vdots
            \\
            \\
            \AVO{}{\batch_b}{l}
            \\
            \AVO{}{\batch_b}{l+1}
            \\
            \vdots
            \\
            \AVO{}{\batch_b}{T}
        \end{bmatrix}
        =
        \ABE
        \begin{bmatrix}
            1
            \\
            1
            \\
            \vdots
            \\
            1
            \\
            \\
            \vdots
            \\
            \\
            1
            \\
            1
            \\
            \vdots
            \\
            1
        \end{bmatrix}
        &+ \ACE_{gl}
        \begin{bmatrix}
            \AVO{}{}{0}
            \\
            \AVO{}{}{1}
            \\
            \vdots
            \\
            \AVO{}{}{T-l}
            \\
            \\
            \vdots
            \\
            \\
            \AVO{}{}{0}
            \\
            \AVO{}{}{1}
            \\
            \vdots
            \\
            \AVO{}{}{T-l}
        \end{bmatrix}
        +
        \ldots
        + \ACE_{g1}
        \begin{bmatrix}
            \AVO{}{}{l-1}
            \\
            \AVO{}{}{l}
            \\
            \vdots
            \\
            \AVO{}{}{T-1}
            \\
            \\
            \vdots
            \\
            \\
            \AVO{}{}{l-1}
            \\
            \AVO{}{}{l}
            \\
            \vdots
            \\
            \AVO{}{}{T-1}
        \end{bmatrix}
        + \ADE_g 
        \begin{bmatrix}
            \expr_{l}
            \\
            \expr_{l+1}
            \\
            \vdots
            \\
            \expr_{T}
            \\
            \\
            \vdots
            \\
            \\
            \expr_{l}
            \\
            \expr_{l+1}
            \\
            \vdots
            \\
            \expr_{T}
        \end{bmatrix}
        + \APE_g  
        \begin{bmatrix}
            \expr_{l}\AVO{}{}{l-1}
            \\
            \expr_{l+1}\AVO{}{}{l}
            \\
            \vdots
            \\
            \expr_{T}\AVO{}{}{T-1}
            \\
            \\
            \vdots
            \\
            \\
            \expr_{l}\AVO{}{}{l-1}
            \\
            \expr_{l+1}\AVO{}{}{l}
            \\
            \vdots
            \\
            \expr_{T}\AVO{}{}{T-1}
        \end{bmatrix}
        \\
        &+ \ACE_{hl}
        \begin{bmatrix}
            \AVO{}{\batch_1}{0}
            \\
            \AVO{}{\batch_1}{1}
            \\
            \vdots
            \\
            \AVO{}{\batch_1}{T-l}
            \\
            \\
            \vdots
            \\
            \\
            \AVO{}{\batch_b}{0}
            \\
            \AVO{}{\batch_b}{1}
            \\
            \vdots
            \\
            \AVO{}{\batch_b}{T-l}
        \end{bmatrix}
        +
        \ldots
        + \ACE_{h1}
        \begin{bmatrix}
            \AVO{}{\batch_1}{l-1}
            \\
            \AVO{}{\batch_1}{l}
            \\
            \vdots
            \\
            \AVO{}{\batch_1}{T-1}
            \\
            \\
            \vdots
            \\
            \\
            \AVO{}{\batch_b}{l-1}
            \\
            \AVO{}{\batch_b}{l}
            \\
            \vdots
            \\
            \AVO{}{\batch_b}{T-1}
        \end{bmatrix}
        + \ADE_h 
        \begin{bmatrix}
            \expr_{l}^{\batch_1}
            \\
            \expr_{l+1}^{\batch_1}
            \\
            \vdots
            \\
            \expr_{T}^{\batch_1}
            \\
            \\
            \vdots
            \\
            \\
            \expr_{l}^{\batch_b}
            \\
            \expr_{l+1}^{\batch_b}
            \\
            \vdots
            \\
            \expr_{T}^{\batch_b}
        \end{bmatrix}
        + \APE_h
        \begin{bmatrix}
            \expr_{l}^{\batch_1} \AVO{}{\batch_1}{l-1}
            \\
            \expr_{l+1}^{\batch_1} \AVO{}{\batch_1}{l}
            \\
            \vdots
            \\
            \expr_{T}^{\batch_1} \AVO{}{\batch_1}{T-1}
            \\
            \\
            \vdots
            \\
            \\
            \expr_{l}^{\batch_b} \AVO{}{\batch_b}{l-1}
            \\
            \expr_{l+1}^{\batch_b} \AVO{}{\batch_b}{l}
            \\
            \vdots
            \\
            \expr_{T}^{\batch_b} \AVO{}{\batch_b}{T-1}
        \end{bmatrix},
    \end{aligned}
\end{equation}
%
or equivalently in a matrix form
% 
\begin{align*}
    \Vec{\Yc} = \Xc (\ABE, \ACE_{gl}, \ldots, \ACE_{g1}, \ADE_g, \APE_g, \ACE_{hl}, \ldots, \ACE_{h1}, \ADE_h, \APE_h)^\top,
\end{align*}
% 
where $\Vec{\Yc}$ represents the vector on the left-hand side of \eqref{eq:apndx_BRD_model}, while $\Xc$ denotes the matrix formed by the columns corresponding to the vectors on the right-hand side of \eqref{eq:apndx_BRD_model}.

For the regression model outlined in \eqref{eq:apndx_BRD_model}, we now show that the least squares estimator provides a strongly consistent estimate for the unknown coefficients. To this end, upon observing $\OMtreatment{}{}$ and $\Moutcome{}{}{}(\OMtreatment{}{})$ within a system of $N$ experimental units, we define the following:
%
\begin{align}
    \label{eq:apndx_BRD_reg_model}
    \Vec{\Yc}(N) \sim \Xc(N) \Vec{\Bc}(N),
\end{align}
%
where $\Vec{\Yc}(N)$ denotes the sample mean of observed outcomes over time and across various subpopulations, corresponding to the vector on the left-hand side of \eqref{eq:apndx_BRD_model}. The vector $\Vec{\Bc}(N)$ represents the unknown coefficients on the right-hand side of \eqref{eq:apndx_BRD_model}, while $\Xc(N)$ denotes a $b(T-l+1)$ by $1+l+2+l+2$ matrix. This matrix is aligned with the vectors on the right-hand side of \eqref{eq:apndx_BRD_model}, but with the corresponding sample means of observed outcomes and treatments replacing the asymptotic terms, see \eqref{eq:reg_cover_matrix}.
% 
\begin{proposition}
    \label{prp:BRD_consistency}
    % 
    Suppose that $\Xc(N)^\top \Xc(N)$ is invertible. Let $\Vec{\widehat{\Bc}}(N) := \big( \Xc(N)^\top \Xc(N) \big)^{-1} \Xc(N)^\top \Vec{\Yc}(N)$ be the least squares estimator. Then, $\Vec{\widehat{\Bc}}(N)$ provides a strongly consistent estimator for the coefficient vector $(\ABE, \ACE_{gl}, \ldots, \ACE_{g1}, \ADE_g, \APE_g, \ACE_{hl}, \ldots, \ACE_{h1}, \ADE_h, \APE_h)^\top$.
    % 
\end{proposition}
% 
\textbf{Proof.}
% 
Because the matrix $\Xc(N)^\top \Xc(N)$ is invertible, the estimator $\Vec{\widehat{\Bc}}(N)$ is a continuous function of the input data. Therefore, we can pass the limit through the estimator function as:
% 
\begin{align*}
    \lim_{N \rightarrow \infty} \Vec{\widehat{\Bc}}(N)
    =
    \left( \Big(\lim_{N \rightarrow \infty} \Xc(N)\Big)^\top \Big(\lim_{N \rightarrow \infty} \Xc(N)\Big) \right)^{-1} \Big(\lim_{N \rightarrow \infty} \Xc(N)\Big)^\top \Big(\lim_{N \rightarrow \infty} \Vec{\Yc}(N)\Big)
    \eqas
    ( \Xc^\top \Xc )^{-1} \Xc^\top \Vec{\Yc},
\end{align*}
% 
where we used the result of Theorem~\ref{thm:Batch_SE}. Now, note that \eqref{eq:apndx_BRD_model} defines a deterministic regression model, and $( \Xc^\top \Xc )^{-1} \Xc^\top \Vec{\Yc} = (\ABE, \ACE_{gl}, \ldots, \ACE_{g1}, \ADE_g, \APE_g, \ACE_{hl}, \ldots, \ACE_{h1}, \ADE_h, \APE_h)^\top$. This concludes the proof. \ep

To ensure the invertibility condition of the matrix $\Xc(N)^\top \Xc(N)$, we need to set some simple conditions on the experimental design. Basically, conducting the experiment in more than one stage (equivalent to having two distinct values for elements of the vector $\big( \hat\expr_l, \ldots, \hat\expr_T \big)^\top$ should suffice. This is needed to ensure that the first column of $\Xc(N)$ (i.e., $\Vec{1}_{b(T-l+1)}$) is linearly independent of the column $\big(\hat\expr_l, \ldots, \hat\expr_T, \ldots, \hat\expr_l, \ldots, \hat\expr_T\big)^\top$. 

Assuming a non-zero treatment effect (i.e., $\ADE_h \neq 0$), we can choose the batches to ensure enough variation across different batches. Therefore, upon a careful batching, columns of $\Xc(N)$ are linearly independent as each has its own specific variation patterns over time and/or subpopulations. 

Although the exact value of $\ADE$ is unknown, we suppose that contextual information suggests the presence of a non-zero direct treatment effect. In the case where $\ADE = 0$, according to contextual information, both the subpopulation sample mean $\AVO{}{\batch}{t}$ and the population sample mean $\AVO{}{}{t}$ are equal in \eqref{eq:SE_BRD_1} and  \eqref{eq:SE_BRD_2}, allowing for further simplification of the underlying model.
% 
\setcounter{MaxMatrixCols}{11}
\begin{equation}
\label{eq:reg_cover_matrix}
    \begin{aligned}
        \Xc(N)
        =
        \begin{bmatrix}
        1
        &\HAVO{}{}{0}
        &\ldots
        &\HAVO{}{}{l-1}
        &\Oexpr{}{}{l}
        &\Oexpr{}{}{l} \HAVO{}{}{l-1} 
        &\HAVO{}{\batch_1}{0}
        &\ldots
        &\HAVO{}{\batch_1}{l-1}
        &\Oexpr{}{\batch_1}{l}
        &\Oexpr{}{\batch_1}{l} \HAVO{}{\batch_1}{l-1} 
        % 
        \\
        % 
        1
        &\HAVO{}{}{1}
        &\ldots
        &\HAVO{}{}{l}
        &\Oexpr{}{}{l+1}
        &\Oexpr{}{}{l+1} \HAVO{}{}{l} 
        &\HAVO{}{\batch_1}{1}
        &\ldots
        &\HAVO{}{\batch_1}{l}
        &\Oexpr{}{\batch_1}{l+1}
        &\Oexpr{}{\batch_1}{l+1} \HAVO{}{\batch_1}{l}
        % 
        \\
        % 
        \vdots
        &\vdots
        &\vdots
        &\vdots
        &\vdots
        &\vdots
        &\vdots
        &\vdots
        &\vdots
        &\vdots
        &\vdots
        % 
        \\
        % 
        1
        &\HAVO{}{}{T-l}
        &\ldots
        &\HAVO{}{}{T-1}
        &\Oexpr{}{}{T}
        &\Oexpr{}{}{T} \HAVO{}{}{T-1} 
        &\HAVO{}{\batch_1}{T-l}
        &\ldots
        &\HAVO{}{\batch_1}{T-1}
        &\Oexpr{}{\batch_1}{T}
        &\Oexpr{}{\batch_1}{T} \HAVO{}{\batch_1}{T-1}
        % 
        \\
        \\
        % 
        \vdots
        &\vdots
        &\vdots
        &\vdots
        &\vdots
        &\vdots
        &\vdots
        &\vdots
        &\vdots
        &\vdots
        &\vdots
        % 
        \\
        \\
        % 
        1
        &\HAVO{}{}{0}
        &\ldots
        &\HAVO{}{}{l-1}
        &\Oexpr{}{}{l}
        &\Oexpr{}{}{l} \HAVO{}{}{l-1} 
        &\HAVO{}{\batch_b}{0}
        &\ldots
        &\HAVO{}{\batch_b}{l-1}
        &\Oexpr{}{\batch_b}{l}
        &\Oexpr{}{\batch_b}{l} \HAVO{}{\batch_b}{l-1} 
        % 
        \\
        % 
        1
        &\HAVO{}{}{1}
        &\ldots
        &\HAVO{}{}{l}
        &\Oexpr{}{}{l+1}
        &\Oexpr{}{}{l+1} \HAVO{}{}{l} 
        &\HAVO{}{\batch_b}{1}
        &\ldots
        &\HAVO{}{\batch_b}{l}
        &\Oexpr{}{\batch_b}{l+1}
        &\Oexpr{}{\batch_b}{l+1} \HAVO{}{\batch_b}{l}
        % 
        \\
        % 
        \vdots
        &\vdots
        &\vdots
        &\vdots
        &\vdots
        &\vdots
        &\vdots
        &\vdots
        &\vdots
        &\vdots
        &\vdots
        % 
        \\
        % 
        1
        &\HAVO{}{}{T-l}
        &\ldots
        &\HAVO{}{}{T-1}
        &\Oexpr{}{}{T}
        &\Oexpr{}{}{T} \HAVO{}{}{T-1} 
        &\HAVO{}{\batch_b}{T-l}
        &\ldots
        &\HAVO{}{\batch_b}{T-1}
        &\Oexpr{}{\batch_b}{T}
        &\Oexpr{}{\batch_b}{T} \HAVO{}{\batch_b}{T-1}
        \end{bmatrix}.
    \end{aligned}
\end{equation}


\subsection{First-order Estimators}
\label{apndx:estimators}
% 
Given the observed outcomes $\Moutcome{}{}{}(\OMtreatment{o}{})$, we can leverage Theorem~\ref{thm:consistency} and Proposition~\ref{prp:BRD_consistency} to consistently estimate counterfactuals under a desired treatment allocation $\OMtreatment{u}{}$. To this end, we propose two closely related families of estimators, which we detail below. Both family of estimators require that the delivered treatment allocation $\OMtreatment{o}{}$ and the desired treatment allocation $\OMtreatment{u}{}$ match during the first $l$ periods. These initial $l$ periods serve as the common foundation from which counterfactual trajectories are constructed.

Given subpopulation $\batch$, Algorithms~\ref{alg:FO-semi-recursive} and \ref{alg:FO-recursive} aim to estimate counterfactuals under the desired treatment allocation over $\batch$ using $b$ distinct subpopulations $\batch_1, \ldots, \batch_b$ as the estimation samples. Both algorithms share their first two steps. In the first step, they compute sample means of observed outcomes for both the entire population and each subpopulation, along with sample means of delivered and desired treatment allocations. The second step estimates unknown parameters in the state evolution equation \eqref{eq:SE_BRD_2} using least squares estimation as detailed in Proposition~\ref{prp:BRD_consistency}. The algorithms then diverge in their third step, applying these results through two distinct approaches detailed below. The consistency proofs for both algorithms follow directly from earlier results and are omitted for brevity.

\subsubsection{Semi-recursive Estimation Method}
\label{apndx:semi-recursive_estimators}
% 
This estimator, outlined in Algorithm~\ref{alg:FO-semi-recursive}, builds on the observed sample means in its third step. It uses the parameter estimates from the second step and the state evolution equation \eqref{eq:SE_BRD_2} to modify the observed sample means by adjusting the treatment level to the desired one. This approach transfers the original complexities of the observed outcomes to the estimated counterfactual, making it particularly suitable for scenarios with strong time trends or complex baselines. This estimator generalizes the algorithm proposed in \cite{shirani2024causal} by accommodating broader model classes and providing more general estimands.

% 
\begin{algorithm}
\caption{First-order semi-recursive counterfactual estimator}
\label{alg:FO-semi-recursive}
% 
\begin{algorithmic}
% 
\Require $\Moutcome{}{}{}(\OMtreatment{o}{}), \OMtreatment{o}{}$, $\OMtreatment{u}{}$, estimation batch $\batch$, sample batches $\batch_1, \ldots, \batch_b$, and $l$
% 

\State \hspace{-1.3em} \textbf{Step 1: Data processing}
% 
\For{$t = 0, \ldots, T$}
    % 
    \State $\HAVO{}{}{t}
        \gets
        \frac{1}{N} \sum_{i=1}^N \outcomeD{}{i}{t} (\OMtreatment{o}{})$
    % 
    \State $\HAVO{}{\batch}{t}
        \gets
        \frac{1}{\cardinality{\batch}} \sum_{i \in  \batch} \outcomeD{}{i}{t} (\OMtreatment{o}{})$
    % 
    \State $\Oexpr{}{}{t}
        \gets
        \frac{1}{N} \sum_{i=1}^N \Otreatment{i}{o,t}{}$
     % 
    \State $\Oexpr{}{\batch}{t}
        \gets
        \frac{1}{\cardinality{\batch}} \sum_{i \in  \batch} \Otreatment{i}{o,t}{}$
    % 
    \State $\Dexpr{}{}{t}
        \gets
        \frac{1}{N} \sum_{i=1}^N \Otreatment{i}{u,t}{}$
    % 
    \State $\Dexpr{}{\batch}{t}
        \gets
        \frac{1}{\cardinality{\batch}} \sum_{i \in  \batch} \Otreatment{i}{u,t}{}$
    % 
    \For{$j = 1, \ldots, b$}
        \State $\HAVO{}{\batch_j}{t}
        \gets
        \frac{1}{\cardinality{\batch_j}} \sum_{i \in  \batch_j} \outcomeD{}{i}{t} (\OMtreatment{o}{})$
        % 
        \State $\Oexpr{}{\batch_j}{t}
        \gets
        \frac{1}{\cardinality{\batch_j}} \sum_{i \in  \batch_j} \Otreatment{i}{o,t}{}$
    \EndFor    
% 
\EndFor
% 

\State \hspace{-1.3em} \textbf{Step 2: Parameters estimation}
% 
\State $(\EBE, \ECE_{gl}, \ldots, \ECE_{g1}, \EDE_g, \EPE_g, \ECE_{hl}, \ldots, \ECE_{h1}, \EDE_h, \EPE_h)^\top \gets \big( \hat{\Xc}^\top \hat{\Xc} \big)^{-1} \hat{\Xc}^\top \Vec{\hat{\Yc}}$


\State \hspace{-1.3em} \textbf{Step 3: Counterfactual estimation}

\State $\Big(\ECF{}{0}{\OMtreatment{u}{}}, \ldots, \ECF{}{l-1}{\OMtreatment{u}{}} \Big) \gets \left(\HAVO{}{}{0}, \ldots, \HAVO{}{}{l-1} \right)$

\State $\Big(\ECF{\batch}{0}{\OMtreatment{u}{}}, \ldots, \ECF{\batch}{l-1}{\OMtreatment{u}{}} \Big) \gets \left(\HAVO{}{\batch}{0}, \ldots, \HAVO{}{\batch}{l-1} \right)$

\For{$t = l, \ldots, T$}
% 
    \State $\Rc_g \gets \sum_{j=1}^l \ECE_{gj} (\ECF{}{t-j}{\OMtreatment{u}{}} - \HAVO{}{}{t-j}) + \EDE_g (\Dexpr{}{}{t}-\Oexpr{}{}{t}) + \EPE_g (\Dexpr{}{}{t} \ECF{}{t-1}{\OMtreatment{u}{}} - \Oexpr{}{}{t} \HAVO{}{}{t-1})$
    % 
    \State $\Rc_h \gets \sum_{j=1}^l \ECE_{hj} (\ECF{}{t-j}{\OMtreatment{u}{}} - \HAVO{}{}{t-j}) + \EDE_h (\Dexpr{}{}{t}-\Oexpr{}{}{t}) + \EPE_h (\Dexpr{}{}{t} \ECF{}{t-1}{\OMtreatment{u}{}} - \Oexpr{}{}{t} \HAVO{}{}{t-1})$
    % 
    \State $\Rc_h^\batch \gets \sum_{j=1}^l \ECE_{hj} (\ECF{\batch}{t-j}{\OMtreatment{u}{}} - \HAVO{}{\batch}{t-j}) + \EDE_h (\Dexpr{}{\batch}{t}-\Oexpr{}{\batch}{t}) + \EPE_h (\Dexpr{}{\batch}{t} \ECF{\batch}{t-1}{\OMtreatment{u}{}} - \Oexpr{}{\batch}{t} \HAVO{}{\batch}{t-1})$
    % 
    \State $\ECF{}{t}{\OMtreatment{u}{}} \gets \HAVO{}{}{t} + \Rc_g + \Rc_h$
    % 
    \State $\ECF{\batch}{t}{\OMtreatment{u}{}} \gets \HAVO{}{\batch}{t} + \Rc_g + \Rc_h^\batch$
% 
\EndFor

\Ensure $\ECF{}{t}{\OMtreatment{u}{}}$ and $\ECF{\batch}{t}{\OMtreatment{u}{}}$, for $t = 0, \ldots, T$.
% 
\end{algorithmic}
\end{algorithm}
% 

\subsubsection{Recursive Estimation Method}
\label{apndx:recursive_estimators}
% 
This estimator, outlined in Algorithm~\ref{alg:FO-recursive}, directly leverages the state evolution equation and parameter estimates from the second step. Specifically, it estimates counterfactuals recursively, using only the past $l$ terms and desired treatment levels. Unlike Algorithm~\ref{alg:FO-semi-recursive}, this algorithm can estimate counterfactuals even for time blocks where no outcome data were collected. For example, having observed data until December, it can predict counterfactual outcomes for January without requiring any observations during this month.



% 
\begin{algorithm}
\caption{First-order recursive counterfactual estimator}
\label{alg:FO-recursive}
% 
\begin{algorithmic}
% 
% 
\Require $\Moutcome{}{}{}(\OMtreatment{o}{}), \OMtreatment{o}{}$, $\OMtreatment{u}{}$, estimation batch $\batch$, sample batches $\batch_1, \ldots, \batch_b$, and $l$
% 

\State \hspace{-1.3em} \textbf{Step 1: Data processing}
% 
\For{$t = 0, \ldots, T$}
    % 
    \State $\HAVO{}{}{t}
        \gets
        \frac{1}{N} \sum_{i=1}^N \outcomeD{}{i}{t} (\OMtreatment{o}{})$
    % 
    \State $\HAVO{}{\batch}{t}
        \gets
        \frac{1}{\cardinality{\batch}} \sum_{i \in  \batch} \outcomeD{}{i}{t} (\OMtreatment{o}{})$
    % 
    \State $\Oexpr{}{}{t}
        \gets
        \frac{1}{N} \sum_{i=1}^N \Otreatment{i}{o,t}{}$
     % 
    \State $\Oexpr{}{\batch}{t}
        \gets
        \frac{1}{\cardinality{\batch}} \sum_{i \in  \batch} \Otreatment{i}{o,t}{}$
    % 
    \State $\Dexpr{}{}{t}
        \gets
        \frac{1}{N} \sum_{i=1}^N \Otreatment{i}{u,t}{}$
    % 
    \State $\Dexpr{}{\batch}{t}
        \gets
        \frac{1}{\cardinality{\batch}} \sum_{i \in  \batch} \Otreatment{i}{u,t}{}$
    % 
    \For{$j = 1, \ldots, b$}
        \State $\HAVO{}{\batch_j}{t}
        \gets
        \frac{1}{\cardinality{\batch_j}} \sum_{i \in  \batch_j} \outcomeD{}{i}{t} (\OMtreatment{o}{})$
        % 
        \State $\Oexpr{}{\batch_j}{t}
        \gets
        \frac{1}{\cardinality{\batch_j}} \sum_{i \in  \batch_j} \Otreatment{i}{o,t}{}$
    \EndFor    
% 
\EndFor
% 

\State \hspace{-1.3em} \textbf{Step 2: Parameters estimation}
% 
\State $(\EBE, \ECE_{gl}, \ldots, \ECE_{g1}, \EDE_g, \EPE_g, \ECE_{hl}, \ldots, \ECE_{h1}, \EDE_h, \EPE_h)^\top \gets \big( \hat{\Xc}^\top \hat{\Xc} \big)^{-1} \hat{\Xc}^\top \Vec{\hat{\Yc}}$


\State \hspace{-1.3em} \textbf{Step 3: Counterfactual estimation}

\State $\Big(\ECF{}{0}{\OMtreatment{u}{}}, \ldots, \ECF{}{l-1}{\OMtreatment{u}{}} \Big) \gets \left(\HAVO{}{}{0}, \ldots, \HAVO{}{}{l-1} \right)$

\State $\Big(\ECF{\batch}{0}{\OMtreatment{u}{}}, \ldots, \ECF{\batch}{l-1}{\OMtreatment{u}{}} \Big) \gets \left(\HAVO{}{\batch}{0}, \ldots, \HAVO{}{\batch}{l-1} \right)$

\For{$t = l, \ldots, T$}
% 
    \State $\Rc_g \gets \sum_{j=1}^l \ECE_{gj} \ECF{}{t-j}{\OMtreatment{u}{}} + \EDE_g \Dexpr{}{}{t} + \EPE_g \Dexpr{}{}{t} \ECF{}{t-1}{\OMtreatment{u}{}}$
    % 
    \State $\Rc_h \gets \sum_{j=1}^l \ECE_{hj} \ECF{}{t-j}{\OMtreatment{u}{}} + \EDE_h \Dexpr{}{}{t} + \EPE_h \Dexpr{}{}{t} \ECF{}{t-1}{\OMtreatment{u}{}}$
    % 
    \State $\Rc_h^\batch \gets \sum_{j=1}^l \ECE_{hj} \ECF{\batch}{t-j}{\OMtreatment{u}{}} + \EDE_h \Dexpr{}{\batch}{t} + \EPE_h \Dexpr{}{\batch}{t} \ECF{\batch}{t-1}{\OMtreatment{u}{}}$
    % 
    \State $\ECF{}{t}{\OMtreatment{u}{}} \gets \Rc_g + \Rc_h$
    % 
    \State $\ECF{\batch}{t}{\OMtreatment{u}{}} \gets \Rc_g + \Rc_h^\batch$
% 
\EndFor

\Ensure $\ECF{}{t}{\OMtreatment{u}{}}$ and $\ECF{\batch}{t}{\OMtreatment{u}{}}$, for $t = 0, \ldots, T$.
% 
\end{algorithmic}
\end{algorithm}
% 


% Then, by Theorem~\ref{thm:consistency}, applying Algorithm~\ref{alg:CF estimation} yields strongly consistent counterfactual estimates for any desired treatment allocation.

\subsection{Higher-order Recursive Estimators}
\label{apndx:HO_recursive_estimators}
% 
In light of Theorem~\ref{thm:consistency}, we can extend our approach to utilize higher-order approximations of $\outcomeg{t}{}$ and $\outcomeh{t}{}$. The estimator, outlined in Algorithm~\ref{alg:HO-recursive} for $l=1$ lag terms, incorporates up to order $m \geq 2$ moments of the unit outcomes. Precisely, we introduce two families of feature functions: $\Vec\phi = (\phi_1, \ldots, \phi_{n_1})^\top$ for population-level moments and $\Vec\psi = (\psi_1, \ldots, \psi_{n_2})^\top$ for subpopulation-level moments. The approach employs linear regression to estimate weights for a linear combination of these features. This can be realized as a generalization of \cite{bayati2024higher}'s method and enables capturing more complex patterns in counterfactuals by leveraging richer information about the outcomes' distributions over time. When the feature functions $\Vec\phi$ and $\Vec\psi$ are continuous, consistency follows from Theorem~\ref{thm:consistency}, though we defer rigorous treatment to future work.


% 
\begin{algorithm}
\caption{Higher-order recursive counterfactual estimator}
\label{alg:HO-recursive}
% 
\small
\begin{algorithmic}
% 
% 
\Require $\Moutcome{}{}{}(\OMtreatment{o}{}), \OMtreatment{o}{}$, $\OMtreatment{u}{}$, $\batch_1, \ldots, \batch_b$, $m \geq 2$, $\Vec\phi = (\phi_1, \ldots, \phi_{n_1})^\top$, and $\Vec\psi = (\psi_1, \ldots, \psi_{n_2})^\top$
% 

\State \hspace{-1.3em} \textbf{Step 1: Data processing}
% 
    \For{$t = 0, \ldots, T$}
    % 
    \State $\HAVO{}{}{t}
        \gets
        \frac{1}{N} \sum_{i=1}^N \outcomeD{}{i}{t} (\OMtreatment{o}{})$
    % 
    \State $\HAVO{}{\batch}{t}
        \gets
        \frac{1}{\cardinality{\batch}} \sum_{i \in \batch} \outcomeD{}{i}{t} (\OMtreatment{o}{})$
    % 
    \For{$k = 2, \ldots, m$}
    % 
        \State $\HVVO{}{(k)}{t}
        \gets
        \frac{1}{N} \sum_{i=1}^N 
        \left(
        \outcomeD{}{i}{t} (\OMtreatment{o}{})
        -
        \HAVO{}{}{t}
        \right)^k$
        % 
        \State $\HVVO{}{\batch,(k)}{t}
        \gets
        \frac{1}{\cardinality{\batch}} \sum_{i \in \batch}
        \left(
        \outcomeD{}{i}{t} (\OMtreatment{o}{})
        -
        \HAVO{}{\batch}{t}
        \right)^k$
    \EndFor
    % 
    \State $\Oexpr{}{}{t}
        \gets
        \frac{1}{N} \sum_{i=1}^N \Otreatment{i}{o,t}{}$
    % 
    \State $\Oexpr{}{\batch}{t}
        \gets
        \frac{1}{\cardinality{\batch}} \sum_{i \in \batch} \Otreatment{i}{o,t}{}$
    % 
    \State $\Dexpr{}{}{t}
        \gets
        \frac{1}{N} \sum_{i=1}^N \Otreatment{i}{u,t}{}$
    % 
    \State $\Dexpr{}{\batch}{t}
        \gets
        \frac{1}{\cardinality{\batch}} \sum_{i \in \batch} \Otreatment{i}{u,t}{}$
    % 
    \For{$j = 1, \ldots, b$}
        \State $\HAVO{}{\batch_j}{t}
        \gets
        \frac{1}{\cardinality{\batch_j}} \sum_{i \in  \batch_j} \outcomeD{}{i}{t} (\OMtreatment{o}{})$
        % 
        \For{$k = 2, \ldots, m$}
        % 
            \State $\HVVO{}{\batch_j,(k)}{t}
            \gets
            \frac{1}{\cardinality{\batch_j}} \sum_{i \in  \batch_j} 
            \left(
            \outcomeD{}{i}{t} (\OMtreatment{o}{})
            -
            \HAVO{}{\batch_j}{t}
            \right)^k$
        \EndFor
        % 
        \State $\Oexpr{}{\batch_j}{t}
        \gets
        \frac{1}{\cardinality{\batch_j}} \sum_{i \in  \batch_j} \Otreatment{i}{o,t}{}$
    \EndFor    
% 
\EndFor
% 

\State \hspace{-1.3em} \textbf{Step 2: Parameters estimation}
% 
\State Estimate $\bm{\Theta}_g \in \R^{m \times n_1}$ and $\bm{\Theta}_h  \in \R^{m \times n_2}$ as $\widehat{\bm{\Theta}}_g$ and $\widehat{\bm{\Theta}}_h$: 
% 
\begin{equation*}
    % \label{eq:HO_parameter_estimation}
    \begin{aligned}
        (\HAVO{}{\batch_j}{t+1}, \HVVO{}{\batch_j,(2)}{t+1}, \ldots, \HVVO{}{\batch_j,(m)}{t+1})^\top
        =
        \bm{\Theta}_g \Vec{\phi}(\HAVO{}{}{t}, \HVVO{}{(2)}{t}, \ldots, \HVVO{}{(m)}{t}, \Oexpr{}{}{t+1})
        +
        \bm{\Theta}_h \Vec{\psi}(\HAVO{}{\batch_j}{t}, \HVVO{}{\batch_j,(2)}{t}, \ldots, \HVVO{}{\batch_j,(m)}{t}, \Oexpr{}{\batch_j}{t+1}),
    \end{aligned}
\end{equation*}
% 
where $j = 1, \ldots, b$ and $t = 0, \ldots, T-1$.

\State \hspace{-1.3em} \textbf{Step 3: Counterfactual estimation}

\State $\ECF{}{0}{\OMtreatment{u}{}} \gets \HAVO{}{}{0}$ and $(\DHVVO{}{(2)}{0}, \ldots, \DHVVO{}{(m)}{0}) \gets (\HVVO{}{(2)}{0}, \ldots, \HVVO{}{(m)}{0})$
% 
\State $\ECF{\batch}{0}{\OMtreatment{u}{}} \gets \HAVO{}{\batch}{0}$ and $(\DHVVO{}{\batch,(2)}{0}, \ldots, \DHVVO{}{\batch,(m)}{0}) \gets (\HVVO{}{\batch,(2)}{0}, \ldots, \HVVO{}{\batch,(m)}{0})$
% 
\For{$t = 1, \ldots, T$}
% 
    \State $\Vec\Rc_g \gets
    \widehat{\bm{\Theta}}_g \Vec{\phi}(\ECF{}{t-1}{\OMtreatment{u}{}}, \DHVVO{}{(2)}{t-1}, \ldots, \DHVVO{}{(m)}{t-1}, , \Dexpr{}{}{t})$
    % 
    \State $\Vec\Rc_h \gets
    \widehat{\bm{\Theta}}_h \Vec{\psi}(\ECF{}{t-1}{\OMtreatment{u}{}}, \DHVVO{}{(2)}{t-1}, \ldots, \DHVVO{}{(m)}{t-1}, \Dexpr{}{}{t})$
    % 
    \State $\Vec\Rc_h^\batch \gets
    \widehat{\bm{\Theta}}_h \Vec{\psi}(\ECF{\batch}{t-1}{\OMtreatment{u}{}}, \DHVVO{}{\batch,(2)}{t-1}, \ldots, \DHVVO{}{\batch,(m)}{t-1}, \Dexpr{}{\batch}{t})$
    % 
    \State $(\ECF{}{t}{\OMtreatment{u}{}}, \DHVVO{}{(2)}{t}, \ldots, \DHVVO{}{(m)}{t})^\top \gets
    \Vec\Rc_g + \Vec\Rc_h$
    % 
    \State $(\ECF{\batch}{t}{\OMtreatment{u}{}}, \DHVVO{}{\batch,(2)}{t}, \ldots, \DHVVO{}{\batch,(m)}{t})^\top \gets
    \Vec\Rc_g + \Vec\Rc_h^\batch$
% 
\EndFor

\Ensure $\ECF{}{t}{\OMtreatment{u}{}}$ and $\ECF{\batch}{t}{\OMtreatment{u}{}}$, for $t = 0, \ldots, T$.
% 
\end{algorithmic}
\end{algorithm}
% 


\section{Detrending for Temporal Patterns}
\label{sec:preprocessing}
% 
While semi-recursive estimators (see \S\ref{apndx:semi-recursive_estimators}) can capture complex temporal patterns in unit outcomes, they face two major limitations. First, they cannot estimate out-of-sample counterfactuals because their architecture relies directly on observed sample means. Second, unlike recursive estimators (see Algorithm~\ref{alg:HO-recursive}), they cannot accommodate higher-order approximations of the outcome functions ($\outcomeg{t}{}$ and $\outcomeh{t}{}$). This limitation stems from their dependence on the closed-form of state evolution equation (as outlined in \eqref{eq:SE_BRD_2}), which may not exist for more complex characterizations of the outcome functions.

This section develops a two-stage estimation method that combines the advantages of both semi-recursive and recursive estimators. Although it requires an additional structural assumption on the outcome specification, this approach can handle complex temporal patterns while enabling both out-of-sample counterfactual estimation and higher-order approximations of the outcome functions. The method proceeds as follows: first, we employ a semi-recursive estimator with sufficient lag terms to accurately estimate temporal patterns. We use this to estimate the baseline outcome means (the counterfactual for all control units: $\CFE{\mathbf{0}}{}{t},\; t =0, 1, \ldots, T$). Next, we detrend the observed outcomes by subtracting the estimated baseline from them. We then apply a recursive estimator focused specifically on estimating treatment effects in the absence of temporal patterns. Finally, we add back the subtracted baseline to obtain the desired estimand (see Algorithm~\ref{alg:FO_with_preprocessing}). The following sections provide more detailed expositions of the algorithm.


\subsection{Baseline Outcome Estimation}
\label{sec:Y0_estimation}
% 
Letting $\VOoutcomeD{}{}{t} := \VoutcomeD{}{}{t}(\Mtreatment{}{}=\mathbf{0})$ denote the vector of baseline outcomes at time $t= 0, 1, \ldots, T$ under no treatment, we can write from \eqref{eq:outcome_function_matrix}:
% 
\begin{equation}
\label{eq:outcome_function_NoTreatment}
\begin{aligned}
    \VOoutcomeD{}{}{t+1}
    =
    \VoutcomeD{}{}{t+1}(\Mtreatment{}{}=\mathbf{0})
    &=
    \big(\IM+\IMatT{t}\big)\outcomeg{t}{}\left(\VOoutcomeD{}{}{t}, \mathbf{0}, \covar\right)
    +
    \outcomeh{t}{}\left(\VOoutcomeD{}{}{t}, \mathbf{0}, \covar\right).
\end{aligned}
\end{equation}
% 
Thus, the matrix $\MOoutcomeD{}{}{} = [\VOoutcomeD{}{}{0}| \ldots | \VOoutcomeD{}{}{T}]$ represents the panel data of baseline outcomes that would be observed in the absence of any intervention.

In the first step of Algorithm~\ref{alg:FO_with_preprocessing}, we employ a semi-recursive algorithm to estimate the sample means of the columns of $\MOoutcomeD{}{}{}$, denoted by $\ECF{}{t}{\mathbf{0}}$. The consistency of this estimation follows directly from the consistency of Algorithm~\ref{alg:FO-semi-recursive}.

\subsection{Augmented Causal Message-Passing Model}
\label{sec:augmented_CMP}
% 
The next two steps of Algorithm~\ref{alg:FO_with_preprocessing} require the following assumption.
% 
\begin{assumption}
    \label{asmp:additive_baseline}
    For $t=0,1,\ldots,T-1$, we assume there exist families of functions $\Toutcomeg{t}$ and $\Toutcomeh{t}$ such that the potential outcomes $\VoutcomeD{}{}{t}(\Mtreatment{}{})$ satisfy:
    % 
    \begin{equation}
        \label{eq:aug_outcome_function}
    \begin{aligned}    
        \VoutcomeD{}{}{t+1}(\Mtreatment{}{})
        =
        \VOoutcomeD{}{}{t+1} +
        \big(\IM+\IMatT{t}\big)\Toutcomeg{t}\left(\VoutcomeD{}{}{t}(\Mtreatment{}{}) - \VOoutcomeD{}{}{t} ,\Mtreatment{}{}, \covar\right)
        +
        \Toutcomeh{t} \left(\VoutcomeD{}{}{t}(\Mtreatment{}{}) - \VOoutcomeD{}{}{t} ,\Mtreatment{}{}, \covar\right).
    \end{aligned}
    \end{equation}
    % 
    Additionally, $\Toutcomeg{t}\left(\Vec{0} , \mathbf{0}, \covar\right) = \Toutcomeh{t}\left(\Vec{0} , \mathbf{0}, \covar\right) = \Vec{0}$, and functions $\Toutcomeg{t}$ and $\Toutcomeh{t}$ satisfy the conditions of Assumption~\ref{asmp:BL}.
\end{assumption}
% 
Note that enforcing conditions $\Toutcomeg{t}\left(\Vec{0} , \mathbf{0}, \covar\right) = \Toutcomeh{t}\left(\Vec{0} , \mathbf{0}, \covar\right) = \Vec{0}$ ensures that $\VoutcomeD{}{}{t+1}(\mathbf{0}) = \VOoutcomeD{}{}{t+1}$, aligning with \eqref{eq:outcome_function_NoTreatment}. 
% 
Letting $\PPVoutcomeD{}{}{t}(\Mtreatment{}{}) := \VoutcomeD{}{}{t}(\Mtreatment{}{}) - \VOoutcomeD{}{}{t}$, we can then rewrite the augmented model \eqref{eq:aug_outcome_function} to match the dynamics of the original outcome specification:
% 
\begin{equation}
\label{eq:preprocessed_outcome_function}
\begin{aligned}    
    \PPVoutcomeD{}{}{t+1}(\Mtreatment{}{})
    &=
    \big(\IM+\IMatT{t}\big)\Toutcomeg{t}\left(\PPVoutcomeD{}{}{t}(\Mtreatment{}{}) ,\Mtreatment{}{}, \covar\right)
    +
    \Toutcomeh{t}\left(\PPVoutcomeD{}{}{t}(\Mtreatment{}{}) ,\Mtreatment{}{}, \covar\right).
\end{aligned}
\end{equation}
% 
We emphasize that Equations \eqref{eq:outcome_function_NoTreatment} and \eqref{eq:preprocessed_outcome_function} provide distinct characterizations of the outcomes, and Algorithm~\ref{alg:FO_with_preprocessing} requires both to hold simultaneously. Specifically, assuming the conditions of \S\ref{apndx:batch_state_evolution} hold in both settings, the baseline outcomes $\VOoutcomeD{}{}{t}$ satisfy the state evolution equation corresponding to \eqref{eq:outcome_function_NoTreatment}. However, in the context of \eqref{eq:preprocessed_outcome_function}, state evolution becomes relevant only when treatment is delivered (i.e., $\Mtreatment{}{} \neq \mathbf{0}$). Indeed, under Assumption~\ref{asmp:additive_baseline}, the third condition in Assumption~\ref{asmp:BL} indicates that state evolution can be derived only when there exists a non-zero treatment effect.

Then, the consistency of the second step estimation in Algorithm~\ref{alg:FO_with_preprocessing} holds in the context of the outcome specification \eqref{eq:aug_outcome_function}. Finally, the consistency of the ultimate estimator follows from both the consistency of individual steps and the fact that Algorithm~\ref{alg:FO_with_preprocessing} can be viewed as a combination of continuous functions.



% 
\begin{algorithm}
\caption{First-order counterfactual estimator with preprocessing}
\label{alg:FO_with_preprocessing}
% 
\begin{algorithmic}
% 
\Require $\Moutcome{}{}{}(\OMtreatment{o}{}), \OMtreatment{o}{}$, $\OMtreatment{u}{}$, estimation batch $\batch$, sample batches $\batch_1, \ldots, \batch_b$, and $l$

\State \hspace{-1.3em} \textbf{Step 1: Detrending}
% 
\State Use Algorithm~\ref{alg:FO-semi-recursive} with $\OMtreatment{u}{} = \mathbf{0}$ to obtain $\ECF{}{t}{\mathbf{0}}$, $t = 0, \ldots, T$. 
% 
\For{$t = 0, \ldots, T$}
    %
    \State $\VoutcomeD{}{'}{t} \gets \VoutcomeD{}{}{t}(\Mtreatment{}{}=\OMtreatment{o}{}) - \ECF{}{t}{\mathbf{0}}$
    % 
\EndFor

\State \hspace{-1.3em} \textbf{Step 2: Counterfactual estimation with proprocessed data}
% 
\State Use Algorithm~\ref{alg:FO-recursive} with $\Moutcome{}{'}{}$ to obtain $\ECF{'\batch}{t}{\OMtreatment{u}{}}$, $t = 0, \ldots, T$.


\State \hspace{-1.3em} \textbf{Step 3: Post-processing}
% 
\For{$t = 0, \ldots, T$}
    %
    \State $\ECF{\batch}{t}{\OMtreatment{u}{}} \gets \ECF{'\batch}{t}{\OMtreatment{u}{}} + \ECF{}{t}{\mathbf{0}}$
    % 
\EndFor

\Ensure $\ECF{\batch}{t}{\OMtreatment{u}{}}$, $t = 0, \ldots, T$.
% 
\end{algorithmic}
\end{algorithm}
% 





\section{Auxiliary Results}
\label{apndx:auxiliary_results}
% 
We need the following strong law of large numbers (SLLN) for triangular arrays of independent but not identically distributed random variables. The form stated below is Theorem~3 in \cite{bayati2011dynamics} that is adapted from Theorem~2.1 in \cite{hu1997strong}.
\begin{theorem}[SLLN]
    \label{thm:SLLN}
    Let $\left\{X_{n,i}:1\leq i \leq n,\; n \geq 1\right\}$ be a triangular array of random variables such that $(X_{n,1},\ldots,X_{n,n})$ are mutually independent with a mean equal to zero for each $n$ and $\frac{1}{n} \sum_{i=1}^n E\left[|X_{n,i}|^{2+\kappa}\right] \leq c n^{\kappa/2}$ for some $0 < \kappa < 1$ and $c < \infty$. Then, we have
    \begin{align}
        \label{eq:SLLN}
        \lim_{n \rightarrow \infty}
        \frac{1}{n} \sum_{i=1}^n X_{n,i} \eqas 0.
    \end{align}
\end{theorem}
We also need the following form of the law of large numbers which is an extension of Lemma~4 in \cite{bayati2011dynamics}.
\begin{theorem}
    \label{thm:SLLN-2}
    Fix $k\geq 2$ and an integer $l$ and let $\left\{\bm v(N)\right\}_{N \geq 1}$ be a sequence of vectors that $\bm v(N) \in \R^{N\x l}$. That means, $\bm v(N)$ is a matrix with $N$ rows and $l$ columns. Assume that the empirical distribution of $\bm v(N)$, denoted by $\hat{p}_{N}$, converges weakly to a probability measure $p_v$ on $\R^l$ such that $\E_{p_v}\left[\norm{\Vec{V}}^k\right] < \infty$ and $\E_{\hat{p}_{N}}\left[\norm{\Vec{V}}^k\right] \rightarrow \E_{p_v}\left[\norm{\Vec{V}}^k\right]$ as $N \rightarrow \infty$. Then, for any continuous function $f:\R^l \mapsto \R$ with at most polynomial growth of order $k$, we have
    \begin{align}
        \label{eq:SLLN-2}
        \lim_{N \rightarrow \infty}
        \frac{1}{N} \sum_{n=1}^N f\big(\bm v_n(N)\big)
        \eqas \E_{p_v} [f(\Vec{V})].
    \end{align}
\end{theorem}
% 
Next, we present Lemma 9 from \cite{li2022non}, and then employ it to prove Lemma~\ref{lm:Gap_with_Gaussian}.
% 
\begin{lemma}
    \label{lm:Gap_with_Gaussian_vector}
    Fixing $N$ and $t < N$, consider a set of i.i.d. random vectors $\Vec{\phi_i} \sim \Nc(0,\frac{1}{N}\I_N),\; i=1, \ldots, t$, and any unit vector $\VNPC{}{} = (\NPC{1}{}, \ldots, \NPC{t}{})^\top$ that might be statistically dependent on $\{\Vec{\phi_i}\}_{i=1}^t$. Then, the 1-Wasserstein distance between the distribution of $\sum_{i=1}^t \NPC{i}{} \Vec{\phi_i}$, denoted by $\law(\sum_{i=1}^t \NPC{i}{} \Vec{\phi_i})$, and $\Nc(0,\frac{1}{N}\I_N)$ obeys
    \begin{equation*}
        W_1\left(\law\left(\sum_{i=1}^t \NPC{i}{} \Vec{\phi_i}\right),\Nc\left(0,\frac{1}{N}\I_N\right)\right) \leq c \sqrt{\frac{t \log N}{N}},
    \end{equation*}
    for some constant $c$ that does not depend on $N$.
\end{lemma}


\begin{lemma}
    \label{lm:Gap_with_Gaussian}
    Fixing $N$ and $t < N$, consider a set of i.i.d. random vectors $\Vec{\phi_i} \sim \Nc(0,\frac{1}{N}\I_N),\; i=1, \ldots, t$, and any unit vector $\VNPC{}{} = (\NPC{1}{}, \ldots, \NPC{t}{})^\top$ that might be statistically dependent on $\{\Vec{\phi_i}\}_{i=1}^t$. Let $\Vec{\Phi} := \sum_{i=1}^t \NPC{i}{} \Vec{\phi_i}$ such that $\Vec{\Phi} = (\Phi^1, \ldots, \Phi^N)^\top$ and $\batch \subset [N]$ be subset of the indices. Then, the 1-Wasserstein distance between the distribution of $\frac{1}{\sqrt{\cardinality{\batch}}}\sum_{n \in \batch} \Phi^n$, denoted by $\law(\frac{1}{\sqrt{\cardinality{\batch}}}\sum_{n \in \batch} \Phi^n)$, and $\Nc(0,{1}/ {N})$ satisfies
    \begin{equation*}
        W_1\left(\law\left(\frac{1}{\sqrt{\cardinality{\batch}}}\sum_{n \in \batch} \Phi^n\right),\Nc\left(0,\frac{1}{N}\right)\right) \leq c \sqrt{\frac{t \log N}{N}},
    \end{equation*}
    for some constant $c$ that does not depend on $N$ and $\batch$.
\end{lemma}
% 
\textbf{Proof}. Considering Kantorovich-Rubinstein duality, we use the dual representation of the 1-Wasserstein distance:
% 
\begin{equation}
    \label{eq:apndx_proof_Gap_with_Gaussian_1}
    W_1\left(\law\left(\frac{1}{\sqrt{\cardinality{\batch}}}\sum_{n \in \batch} \Phi^n\right),\Nc\left(0,\frac{1}{N}\right)\right)
    = \sup\left\{\E\left[f\left(\frac{1}{\sqrt{\cardinality{\batch}}}\sum_{n \in \batch} \Phi^n\right)\right] - \E\left[f\left(\frac{Z}{\sqrt{N}}\right)\right]\;\Bigg|\; f \text{ is 1-Lipschitz}\right\},
\end{equation}
% 
where $Z \sim \Nc(0,1)$. To proceed, fix the 1-Lipschitz function $f$ arbitrarily. Further, define the function $\psi: \R^N \mapsto \R$ such that for any vector $\Avec{}{} = (\avec{1}{}, \ldots, \avec{N}{})^\top$, we have $\psi(\Avec{}{}) = \frac{1}{\sqrt{\cardinality{\batch}}} \sum_{n \in \batch} \avec{n}{}$. Then, we define $\tilde{f} = f\circ \psi : \R^N \mapsto \R$. Note that both $f$ and $\psi$ are continuous functions; as a result,
the function $\tilde{f}$ is measurable. We show it is also $1$-Lipschitz. To this end, for vectors $\Avec{}{1}, \Avec{}{2} \in \R^N$, we write:
% 
\begin{equation*}
    \begin{aligned}
        \left|\tilde{f}\left(\Avec{}{2}\right) - \tilde{f}\left(\Avec{}{1}\right)\right|
        = \left|f\left(\psi(\Avec{}{2})\right) - f\left(\psi(\Avec{}{1})\right)\right|
        \leq \left|\psi(\Avec{}{2}) - \psi(\Avec{}{1})\right|
        \leq
        \frac{\sum_{n \in \batch} \left|\avec{n}{2} - \avec{n}{1} \right|}{\sqrt{\cardinality{\batch}}} 
        \leq
        \sqrt{\sum_{n \in \batch} \left|\avec{n}{2} - \avec{n}{1} \right|^2}
        \leq
        \norm{\Avec{}{2} - \Avec{}{1}},
    \end{aligned}
\end{equation*}
% 
where we used the fact that $f$ is 1-Lipschitz and the Cauchy–Schwarz inequality. Therefore, the function $\tilde{f}$ is 1-Lipschitz and by the result of Lemma~\ref{lm:Gap_with_Gaussian_vector}, we get
% 
\begin{equation*}
    \E\left[f\left(\frac{1}{\sqrt{N}}\sum_{n=1}^N \Phi^n\right)\right] - \E\left[f(\frac{Z}{\sqrt{N}})\right]
    = \E\left[\tilde{f}\left(\Vec{\Phi}\right)\right] - \E\left[\tilde{f}\left(\frac{1}{\sqrt{N}}\Vec{Z}\right)\right]
    \leq 
    c\sqrt{\frac{t \log N}{N}},
\end{equation*}
% 
where $\Vec{Z} \sim \Nc(0,\I)$. Because $f$ is chosen arbitrarily, by Eq.~\eqref{eq:apndx_proof_Gap_with_Gaussian_1}, we obtain the desired result. \ep
