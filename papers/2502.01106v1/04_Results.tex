\section{Results}
\label{sec:results}
% 
Figures~\ref{fig:LLM}-\ref{fig:DC} present results from applying counterfactual cross-validation (Algorithm~\ref{alg:C-CV}) across six benchmark scenarios detailed in \S\ref{sec:Benchmark_Toolbox}. Below, we outline our implementation approach and key findings.

For the LLM-based social network model, we conducted 10 distinct runs, constrained by OpenAI API limitations; the current simulation, with $N=1000$ and $T=30$, required approximately 100,000 GPT-3.5 API calls to generate experimental and ground truth results. Each run employs a unique treatment allocation following a staggered rollout design across three stages with $\Vec{\expr} = (0.2, 0.5, 0.8)$, each spanning 10 periods. This design implies that on average 20\% of units received the intervention in the initial 10 periods, followed by an additional 30\% in the subsequent 10 periods, and so forth.

For the remaining five experiments, we conducted 100 independent runs for each setting, utilizing a fresh treatment allocation for each run through a staggered rollout design. The design comprises four equal-length stages with treatment probabilities $\Vec{\expr} = (0, 0.2, 0.4, 0.6)$. In each figure's leftmost panel, we display the temporal evolution of outcomes through their mean and standard deviation, along with the 95th percentile across runs.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{plots/LLM.pdf}
    \caption{LLM-based social network with $N=1,000$ agents.}
    \label{fig:LLM}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{plots/BAM1.pdf}
    \caption{Belief adoption model with Krupina network with $N=3,366$ users.}
    \label{fig:BAM1}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{plots/BAM2.pdf}
    \caption{Belief adoption model with Topolcany network with $N=18,246$ users.}
    \label{fig:BAM2}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{plots/BAM3.pdf}
    \caption{Belief adoption model with Zilina network with $N=42,971$ users.}
    \label{fig:BAM3}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{plots/Auction.pdf}
    \caption{Ascending auction model with $N=500$ objects.}
    \label{fig:auction}
\end{figure}

The second panels of Figures~\ref{fig:LLM}-\ref{fig:DC} display the box plot of the average total treatment effect (TTE) across multiple time periods. The TTE contrasts the counterfactual of all units under treatment against all units under control:
% 
\begin{align}
    \label{eq:TTE}
    \text{TTE} :=
    \frac{1}{LN} \sum_{t=T-L+1}^T \sum_{i=1}^\UN 
    \left[
    \outcomeDW{\mathbf{1}}{i}{t}
    -
    \outcomeDW{\mathbf{0}}{i}{t}
    \right].
\end{align}
% 
In each setting, we carefully select $L$ so that the TTE in \eqref{eq:TTE} covers all periods with nonzero treatment probability, ensuring our benchmark estimators remain meaningful. The results compare ground truth (GT) values against estimates obtained from three methods: our proposed causal message-passing approach (CMP), the difference-in-means estimator (DM), and the Horvitz-Thompson estimator (HT)%
% 
\footnote{Difference-in-means (DM) and Horvitz-Thompson (HT) are expressed as:
\begin{align*}
%
\DIME :=  \frac{1}{L} \sum_{t=T-L+1}^T 
\Big(
\frac{\sum_{i=1}^N\outcomeD{}{i}{t}\treatment{i}{t}}{\sum_{i=1}^N\treatment{i}{t}} - \frac{\sum_{i=1}^N\outcomeD{}{i}{t}(1-\treatment{i}{t})}{\sum_{i=1}^N(1-\treatment{i}{t})} \Big),
%
\quad\quad
% 
\HTE :=  \frac{1}{LN} \sum_{t=T-L+1}^T \sum_{i=1}^N \left( \frac{\outcomeD{}{i}{t} \treatment{i}{t}}{\E[\treatment{i}{t}]} - \frac{\outcomeD{}{i}{t} (1 - \treatment{i}{t})}{\E[1 - \treatment{i}{t}]} \right).
%
\end{align*}}
%
\citep{savje2021average}. Finally, the rightmost two panels in each figure display the CFE under the ground truth and CMP estimates for all-control and all-treatment conditions, along with their respective 95th percentiles.

In implementing Algorithm~\ref{alg:C-CV}, we employ five validation batches ($b_v=5$). 
To select candidate estimators, we begin with a base model where each outcome is expressed as a linear function of two components: the sample mean of outcomes from the previous round and the current treatment allocation means. We then systematically modify this model by incorporating additional first-order and higher-order terms. The configurations also span batch counts from 200 to 2000 and batch sizes ranging from 0.1 to 20 percent of the population size. To estimate parameters, we employ Ridge regression with penalty parameters logarithmically spaced from $10^{-4}$ to $10^{4}$. These parameters are comprehensively combined to generate a diverse set of potential estimators, with time blocks aligned to experimental stages. For example, when $T=40$ and the design $\Vec{\expr} = (0, 0.2, 0.4, 0.6)$ with equal length blocks is used, 
$\tblockList$ has four elements, one corresponding to each block with a fixed treatment probability.
Then, the selection process incorporates both domain knowledge and observed data characteristics. For instance, the pronounced temporal patterns evident in the left panels of Figures~\ref{fig:NYC_taxi}-\ref{fig:DC}, observed in the New York City taxi model, exercise encouragement program, and data center model, necessitate estimators with detrending steps (see Remark~\ref{rem:proprocessing}). Computational efficiency is maintained by constraining the estimator search space based on the experimental context.

Overall, our framework demonstrates robust performance across all six scenarios, successfully estimating counterfactual evolutions despite strong seasonality patterns and without requiring information about the underlying interference network. The proposed method achieves significantly better performance than both DM and HT estimators, even in settings with subtle treatment effects. As illustrated in Figures~\ref{fig:LLM}-\ref{fig:NYC_taxi}, CMP yields estimates with both smaller bias and variance in different scenarios. The effectiveness of our method is particularly evident in the challenging scenarios presented in Figures~\ref{fig:BAM1}-\ref{fig:auction} and \ref{fig:DC}, where conventional estimators struggle to reliably determine even the direction of treatment effects. These comprehensive results establish our framework's capability to deliver precise estimates of counterfactual evolutions and treatment effects across diverse experimental settings.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{plots/NYC_taxi.pdf}
    \caption{New York City Taxi model with $N=18,768$ Routes.}
    \label{fig:NYC_taxi}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{plots/EEP.pdf}
    \caption{Exercise encouragement program with $N=30,162$ users.}
    \label{fig:EP}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{plots/DC_N1k.pdf}
    \caption{Data Center model with $N=1,000$ servers.}
    \label{fig:DC}
\end{figure}

\begin{remark}
    \label{rem:batch_generation}
    % 
    Selecting a predetermined number of batches for a given batch size $n^\batch$ presents a significant computational challenge, particularly in large-scale problems with time-varying treatment allocations across units. For staggered rollout designs, we implement a heuristic approach while deferring comprehensive analysis to future research. Our heuristic consists of three steps. First, we order units by their treatment duration, defined as the number of time periods under treatment. Second, we select two blocks of size $n^\batch$â€”one that slides through the ordered list to cover all treatment durations, and another chosen randomly to ensure sufficient between-batch variation. Third, we select individual units from these merged blocks with equal probability to generate batches with average size $n^\batch$. This procedure maintains computational efficiency while ensuring batches with diverse treatment allocations with high probability.
\end{remark}