\section{General Counterfactual Estimation}
\label{sec:Causal_Estimands}
% 
Estimating counterfactual evolution $\left\{ \CFE{\OMtreatment{}{}'}{}{t} \right\}_{t=0}^T$ based on the observed outcomes $\Moutcome{}{}{}(\Mtreatment{}{} = \OMtreatment{}{})$ enables addressing a broad range of causal questions, such as, “\emph{What if we had delivered the treatments according to $\OMtreatment{}{}'$ instead of $\OMtreatment{}{}$}?" For example, if outcomes are observed after treating 20\% of potential voters, campaign organizers may want to explore how the population would have responded if 40\% had received the campaign materials. In addition, by estimating the dynamics of the counterfactuals over time, we gain insights into how the treatment effect may strengthen or weaken as time progresses.

The total treatment effect (TTE) provides a formal way to compare any two counterfactual scenarios. For two treatment allocations $\OMtreatment{}{}'$ and $\OMtreatment{}{}''$, the TTE measures the difference in population average outcomes:
% 
\begin{align}
    \label{eq:TTE_def}
    \TTE{t}{\OMtreatment{}{}'',\OMtreatment{}{}'} = \CFE{\OMtreatment{}{}''}{}{t} - \CFE{\OMtreatment{}{}'}{}{t},
    \quad\quad\quad
    t = 0,1, \ldots, T\,.
\end{align}
%
The literature typically focuses on a special case: when all entries of $\OMtreatment{}{}''$ equal one and all entries of $\OMtreatment{}{}'$ equal zero, with all outcomes at equilibrium \citep{yu2022estimating,candogan2023correlated,ni2023design,ugander2023randomized}. However, this extreme scenario of treating the entire population versus treating no one is often impractical. As we demonstrate in subsequent sections, our framework enables the estimation of TTEs between any two treatment allocations, providing decision-makers with more realistic comparisons \citep{muralidharan2017experimentation,egger2022general}.


\subsection{Experimental Design Framework}
\label{sec:Experimental_Design}
% 
We proceed by generalizing the experimental design. Specifically, we allow treatment assignments $\treatment{i}{t}$ to vary over time following a Bernoulli distribution with mean $\expr_t$, and define the experimental design as the product distribution $\expd = \expr_0 \times \ldots \times \expr_T$. In this context, we use $\Vtreatment{}{t}{} := (\treatment{1}{t}, \ldots, \treatment{N}{t})^\top$ to represent the vector containing the treatment assignments for all experimental units at time~$t$. This formulation encompasses a broad range of experimental designs, including staggered roll-out design \citep{xiong2024optimal}, micro-randomized trials \citep{li2022network}, and switchback experiments \citep{bojinov2023design}.

\begin{remark}
In Appendix~\ref{sec:Technical_Results}, we further generalize this framework by allowing $\treatment{i}{t}$ to follow more general probability distributions~$\pi_t$ and defining the experimental design as $\expd = \pi_0 \times \ldots \times \pi_T$. The random variable $\treatment{i}{t}$ can take values in either an integer set (e.g., different treatment types) or a continuous set (e.g., varying treatment doses). This generalization distinguishes our work from recent literature on network interference and longitudinal data, which primarily focuses on binary treatments \citep{arkhangelsky2023causal}.
\end{remark}

For a fixed integer $\dcovar$, we can also consider covariates (might be observed or not) in the form of a $\dcovar$ by $N$ matrix $\covar$, where each column (denoted by $\Vcovar{i} := (\Ecovar{1}{i}, \ldots,  \Ecovar{\dcovar}{i})^\top$) represents characteristics of unit $i$ (e.g., age and gender). The experimental data thus consists of treatment assignments~$\OMtreatment{}{}$, observed outcomes 
%
\[
\Moutcome{}{}{}(\OMtreatment{}{}) := \Moutcome{}{}{}(\Mtreatment{}{}=\OMtreatment{}{})\,,
\]
%
and covariates $\covar$. Therefore, we observe outcomes only under one specific treatment allocation $\OMtreatment{}{}$—a single realization among $2^{\UN (\TH+1)}$ distinct potential outcomes, where this number grows exponentially with population size and time horizon. Consequently, estimating causal effects under general interference is impossible due to non-identifiability \citep{karwa2018systematic}. To address this challenge, we propose a tractable outcome specification that aligns with and extends the causal message passing framework \citep{shirani2024causal}.


\subsection{Potential Outcome Specification}
\label{sec:Outcome_Specification}
% 
Let $\Mtreatment{t}{} := [\Vtreatment{}{0}{}| \ldots| \Vtreatment{}{t}{}]$ denote the treatment assignments up to time $t$; we represent by $\VoutcomeD{}{}{t}(\Mtreatment{t}{}) = \big(\outcomeD{}{1}{t}(\Mtreatment{t}{}), \ldots, \outcomeD{}{N}{t}(\Mtreatment{t}{})\big)^\top$ the potential outcome vector at time $t$. Consider unknown functions $\outcomeg{t}{}$ and $\outcomeh{t}{}$ which operate component-wise, and the expressions $\outcomeg{t}{}\big(\VoutcomeD{}{}{t}(\Mtreatment{t}{}), \Vtreatment{}{t+1}{}, \covar\big)$ and $\outcomeh{t}{}\big(\VoutcomeD{}{}{t}(\Mtreatment{t}{}), \Vtreatment{}{t+1}{}, \covar \big)$ represent the corresponding column vectors. Given $\VoutcomeD{}{}{0}(\Mtreatment{0}{})$ as the initial outcome vector, we consider the following specification for potential outcomes:
% 
\begin{align}
    \label{eq:outcome_function_matrix}
    \VoutcomeD{}{}{t+1}(\Mtreatment{t+1}{}) =
    \big(\IM+\IMatT{t}\big)\outcomeg{t}{}\left(\VoutcomeD{}{}{t}(\Mtreatment{t}{}) ,\Vtreatment{}{t+1}{}, \covar\right)
    +
    \outcomeh{t}{}\left(\VoutcomeD{}{}{t}(\Mtreatment{t}{}) ,\Vtreatment{}{t+1}{}, \covar\right)
    +
    \Vnoise{}{t},
    \;\;
    t=0,1,\ldots,T-1,
\end{align}
% 
where $\IM$ and $\IMatT{t}$ are $N\times N$ unknown matrices capturing the fixed and time-dependent interference effects, respectively. Additionally, $\Vnoise{}{t} = \big(\noise{1}{t},\ldots,\noise{N}{t}\big)^\top$ is the zero-mean noise term accounting for observation noise. Denoting by $\IMatl{ij}$ and $\IMatTl{ij}{t}$ the element in the $i^{th}$ row and $j^{th}$ column of $\IM$ and $\IMatT{t}$, respectively, the value $\IMatl{ij}+\IMatTl{ij}{t}$ quantifies the impact of unit $j$ on unit $i$ at time $t$.

The specification in Eq. \eqref{eq:outcome_function_matrix} captures several aspects of experimental data. It accommodates various types of interference, including treatment spillover effects, carryover effects, peer effects, and autocorrelation \citep{shirani2024causal}. Notably, this specification accounts for outcome dynamics by acknowledging the temporal interrelation of units' outcomes. This contrasts with existing approaches to panel data analysis, which assume that time labels can be shuffled without affecting causal effects \citep{arkhangelsky2023causal}.

%
\begin{remark}
In Appendix~\ref{sec:Technical_Results}, we further generalize the outcome specification in Eq.~\eqref{eq:outcome_function_matrix} to incorporate additional lag terms (e.g., $\VoutcomeD{}{}{t-1}(\Mtreatment{t-1}{})$) in the functions $\outcomeg{t}{}$ and $\outcomeh{t}{}$, the complete treatment matrix at any time point (allowing for anticipation effects), and time-dependent covariates.
\end{remark}
%

Next, we analyze the state evolution of the experimental population, characterizing the asymptotic dynamics of unit outcomes. These theoretical foundations establish the basis for developing robust counterfactual estimators in subsequent sections.


\subsection{Experimental State Evolution}
\label{sec:ESE}
% 
In this section, we analyze the distribution of unit outcomes over time. This analysis requires the following two assumptions about the interference matrices.
%
\begin{assumption}[Gaussian interference structure]
    \label{asmp:Gaussian Interference Matrice}
    For all $i,j$, the element $\IMatl{ij}$ in the $i^{th}$ row and $j^{th}$ column of $\IM$ is an independent Gaussian random variable with mean $\mu^{ij}/N$ and variance $\sigma^2/N$. Similarly, $\IMatTl{ij}{t}$, the element in the $i^{th}$ row and $j^{th}$ column of $\IMatT{t}$, is an independent Gaussian random variable with mean $\mu^{ij}_t/N$ and variance $\sigma^2_t/N$.
\end{assumption}
%
%
\begin{assumption}[Convergent interference pattern - informal statement]
    \label{asmp:Stable Interference Pattern}
    For all unit $i$ and any time $t$, the elements of vector $(\mu^{i1}, \ldots, \mu^{iN})$ admit a weak limit\footnote{This means that the empirical distribution of ${\mu^{i1}, \ldots, \mu^{iN}}$ converges to a probability distribution as $N$ increases.} and, separately, the elements of vector $(\mu^{i1}_t, \ldots, \mu^{iN}_t)$ admit a weak limit, where both limits are invariant in $i$.
\end{assumption}
%
According to Assumptions~\ref{asmp:Gaussian Interference Matrice} and \ref{asmp:Stable Interference Pattern}, the impact of unit $j$ on unit $i$ is captured by $\mu^{ij}$, $\mu^{ij}_t$, and two centered Gaussian random variables. This generalizes the model of \cite{shirani2024causal}, which assumes i.i.d. interference matrix elements across all units. This extension accommodates more heterogeneous local interactions and varying levels of uncertainty about the interference structure. Specifically, a fully known interference network has exact values for $\mu^{ij}$ and $\mu^{ij}_t$ with $\sigma^2 = 0$ and $\sigma^2_t = 0$, while a completely unknown interference means no knowledge of these quantities. Importantly, our estimation method is designed to handle the cases that we have \emph{no} knowledge of these underlying quantities for implementation.
% 
\begin{example}
    Consider the case where $\mu^{ij}$ and $\mu^{ij}_t$ take values of $0$ and $1$, generating time-dependent adjacency matrices of the graph representing the interference structure. If we have high confidence that interactions occur only through these adjacency matrices, we can imagine negligible values for $\sigma^2$ and $\sigma^2_t$. Conversely, significant uncertainty about potential interactions would be reflected in larger values of $\sigma^2$ and $\sigma^2_t$. %Importantly, our estimation method requires \emph{no} knowledge of these underlying quantities for implementation.
\end{example}
% 
\begin{remark}
    The formal version of Assumption~\ref{asmp:Stable Interference Pattern} appears in Assumption~\ref{asmp:weak_limits} in Appendix~\ref{apndx:batch_state_evolution}, where we generalize the condition by allowing greater variation across units. This interference model accommodates complex interaction patterns and admits further generalizations, including extensions to non-Gaussian interference matrices. For detailed discussions, see the Appendix of \cite{shirani2024causal}.
\end{remark}
% 

We then establish the following informal result for the large-sample regime that characterizes how outcome distributions evolve between consecutive time points, with its formal statement presented in Appendix~\ref{apndx:batch_state_evolution}.
% 
\begin{theorem}[State evolution - informal statement]
    \label{thm:SE_informal}
    % 
    Let $N \to \infty$ and suppose Assumptions~\ref{asmp:Gaussian Interference Matrice} and \ref{asmp:Stable Interference Pattern} hold. There exist mappings $\outcomef{t},\; t=0, \ldots, T-1$, such that the distribution of outcomes at time $t+1$ is determined by:
    % 
    \begin{equation}
        \label{eq:SE_informal}
        \law(
        \outcomeD{}{}{t+1})
        =
        \outcomef{t}\left(
        \law(
        \outcomeD{}{}{t}
        ),
        \law(
        \treatment{}{t+1}{}
        )
        \right),
    \end{equation}
    %
    where $\outcomeD{}{}{t}$ denotes the weak limit of unit outcomes $\outcomeD{}{1}{t}(\Mtreatment{t}{}), \ldots, \outcomeD{}{N}{t}(\Mtreatment{t}{})$ at time $t$, $\treatment{}{t+1}{}$ follows a Bernoulli distribution with mean $\expr_{t+1}$, and $\law(\cdot)$ refers to law or distribution of its argument.
\end{theorem}
% 
The mapping $\outcomef{t}$ in Eq. \eqref{eq:SE_informal} characterizes how the distribution of outcomes at time t+1 emerges from the previous distribution at time $t$ through the functions $\outcomeg{t}{}$ and $\outcomeh{t}{}$, while accounting for both direct treatment effects and indirect effects from the interference matrices and unit covariates. Specifically, in the large-sample limit, even though each unit's outcome depends on a complex network of interactions, the population-level distribution of outcomes follows a simpler evolution that depends only on the previous distribution and treatment assignment distribution. Building on \citep{shirani2024causal}, while extending their results to a broader setting, we refer to the relationship in \eqref{eq:SE_informal} as the experimental state evolution (SE) equation.


\subsection{A First Look at Estimation Through Theory and Practical Constraints}
\label{sec:estimation_theory_informal}


The state evolution equation~\eqref{eq:SE_informal} suggests a natural estimation strategy, motivated by the observation that in many settings, $\outcomef{t}$ remains relatively stable over time. Specifically, we can often decompose $\outcomef{t}$ into a substantial time-invariant component $\outcomef{}$ and a time-varying component. 

This decomposition, combined with the fact that empirical distributions from experimental data with many units should approximate the distributions in equation~\eqref{eq:SE_informal}, enables a supervised learning approach to estimate $\outcomef{}$. When strong time trends are present, one can first detrend the data to isolate the time-invariant component before applying these learning techniques. Once these mappings are estimated, we can generate any desired counterfactual evolution $\left\{ \CFE{\OMtreatment{}{}'}{}{t} \right\}_{t=0}^T$ through recursive application of Eq.~\eqref{eq:SE_informal} (see Algorithm~\ref{alg:CF estimation} in Appendix~\ref{sec:estimation_theory}). For the sake of building intuition, we will explain a simple version of this, adapted from \cite{shirani2024causal}.

\subsubsection{An Illustrative Example} 
Consider a special case of outcome specification \eqref{eq:outcome_function_matrix} where functions $\outcomeh{t}{}$ are equal to zero, and functions $\outcomeg{t}{}$ satisfy,
%
\begin{equation}
\label{eq:simple_function_structure}
    \begin{aligned}
        \outcomeg{t}{} \big(
        \outcomeD{}{i}{t}(\Mtreatment{t}{}),
        \treatment{i}{t+1}, \Vcovar{i}
        \big)
        = \ABE
        +
        \ACE \outcomeD{}{i}{t}(\Mtreatment{t}{})
        +
        \ADE \treatment{i}{t+1}
        +
        \APE \outcomeD{}{i}{t}(\Mtreatment{t}{}) \treatment{i}{t+1}\,.    
    \end{aligned}
\end{equation}
% 
In this case, if we denote the outcomes sample mean at time $t$ by $\AVO{}{}{t}$, the state evolution implies that as $N \rightarrow \infty$,
%
\begin{align}
    \label{eq:simple_SE_function}
    \AVO{}{}{t+1}
    =\outcomef{}(\AVO{}{}{t},\expr_{t+1})\,,
\end{align}
%
where $\outcomef{}$ has the form $\outcomef{}(a,b)=\ABE + \ACE a + \ADE b +
\APE ab$, and plase the role of the aforementioned time-invariant function. Subsequently, given observations $\Moutcome{}{}{}(\OMtreatment{}{})$ and $\OMtreatment{}{}$, Algorithm~\ref{alg:example_algorithm} enables consistent estimation of CFE for any target treatment allocation $\OMtreatment{}{}'$, provided that $\OMtreatment{}{}$ and $\OMtreatment{}{}'$ share identical columns for treatment assignment at time~$t=0$ and the design set $\{\expr_0, \ldots, \expr_T\}$ contains at least two distinct values\footnote{A detailed analysis of a more general setting appears in Appendix~\ref{sec:application_to_BRD}.}.

% 
\begin{algorithm}
\caption{Causal message passing counterfactual estimator (simple case)}
\label{alg:example_algorithm}
% 
\begin{algorithmic}
% 
% 
\Require $\Moutcome{}{}{}(\OMtreatment{}{}), \OMtreatment{}{} = [\Otreatment{i}{t}{}]_{i,t}$, and $\OMtreatment{}{}' = [\Otreatment{'i}{t}{}]_{i,t}$
% 

\State \hspace{-1.3em} \textbf{Step 1: Data processing}
% 
\For{$t = 0, \ldots, T$}
\State $\HAVO{}{}{t} \gets \frac{1}{N} \sum_{i=1}^N \outcomeD{}{i}{t} (\OMtreatment{t}{})$, \quad\quad $\Oexpr{}{}{t} \gets \frac{1}{N} \sum_{i=1}^N \Otreatment{i}{t}{}$, \quad\quad $\Dexpr{}{}{t} \gets \frac{1}{N} \sum_{i=1}^N \Otreatment{'i}{t}{}$
% 
\EndFor
% 

\State \hspace{-1.3em} \textbf{Step 2: Parameters estimation}
% 
\State $(\EABE, \EACE, \EADE, \EAPE)$ $\gets$ Estimation of $(\ABE, \ACE, \ADE, \APE)$ using OLS in $\HAVO{}{}{t+1} = \ABE + \ACE \HAVO{}{}{t} + \ADE \Oexpr{}{}{t+1} + \APE \HAVO{}{}{t} \Oexpr{}{}{t+1}$


\State \hspace{-1.3em} \textbf{Step 3: Counterfactual estimation}

\State $\ECF{}{0}{\OMtreatment{}{}'} \gets \HAVO{}{}{0}$

\For{$t = 1, \ldots, T$}
%
    \State $\ECF{}{t}{\OMtreatment{}{}'} \gets \EABE + \EACE \ECF{}{t-1}{\OMtreatment{}{}'} + \EADE \Dexpr{}{}{t} + \EAPE \ECF{}{t-1}{\OMtreatment{}{}'} \Dexpr{}{}{t}$
% 
\EndFor

\Ensure Estimated counterfactual evolution: $\{\ECF{}{t}{\OMtreatment{}{}'}\}_{t=0}^T$.
% 
\end{algorithmic}
\end{algorithm}
% 

\subsubsection{General Estimation Theory} 

The above example demonstrates that our counterfactual estimation problem reduces to the consistent estimation of $\outcomef{}$. This relationship can be formalized for our general outcome specification~\eqref{eq:outcome_function_matrix}. We provide an informal version of this result below, while the formal statement and complete proof appear in Theorem~\ref{thm:consistency} in Appendix~\ref{sec:estimation_theory}.

\begin{theorem}[Consistency - informal statement]
\label{thm:consistency_informal}
Under certain regularity conditions, given consistent estimation of the mappings $\outcomef{t}$ in the SE equation~\eqref{eq:SE_informal}, any desired counterfactual evolution 
$\left\{ \CFE{\OMtreatment{}{}'}{}{t} \right\}_{t=0}^T$ 
can be consistently estimated.
\end{theorem}

\subsubsection{Practical limitations} 

The effectiveness of approaches like Algorithm~\ref{alg:example_algorithm} in identifying treatment effects across various settings has been established by \cite{shirani2024causal}. However, two significant constraints require consideration. First, the estimation procedure in the second step of Algorithm~\ref{alg:example_algorithm} faces sample size limitations due to the experiment horizon. Second, as demonstrated by \cite{bayati2024higher}, certain settings require accounting for more complex structures, necessitating a richer class of functions $\outcomef{}$. To capture these more complex settings, one approach is to use higher moments of the outcome and treatment distributions in state evolution~\eqref{eq:SE_informal}.

For example, if we denote the variance of outcomes at time $t$ by $\VVO{}{}{t}$, and if functions $\outcomeg{}{}$ and $\outcomeh{}{}$ contain second-order terms, the state evolution becomes:
\begin{align}
    \label{eq:more_complex_SE_function}
    \Big(\AVO{}{}{t+1},(\VVO{}{}{t+1})^2\Big)
    =\outcomef{}\Big(\AVO{}{}{t},(\VVO{}{}{t})^2,\expr_{t+1},\expr_{t+1}^2\Big)\,.
\end{align}
%
An alternative approach to capturing complex structure is to consider a richer family of functions $\outcomef{}$ during the estimation phase. 

These considerations raise two key questions: First, how can we expand the sample size to improve estimation accuracy? Second, how can we determine the appropriate specification for the underlying experiment, similar to model selection in supervised learning? The following two sections address these questions systematically.