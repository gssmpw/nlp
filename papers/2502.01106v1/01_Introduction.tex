\section{Introduction}
\label{sec:Intro}
% 
Randomized Control Trials (RCTs) are the gold standard for evaluating intervention effectiveness, such as assessing public campaigns to promote voting within communities \citep{imbens2015causal}. However, classic RCT methods often overlook the complex dynamics of belief adoption within social networks. In these networks, experimental units—individual voters, for instance—do not make decisions in isolation but are influenced by peers, family, and societal norms. A study of the 2010 US congressional elections by \cite{bond201261} demonstrated that encouraging individuals to vote influenced not only their own behavior but also that of their friends and those beyond their immediate social circles.

This phenomenon, known as network interference, violates a fundamental RCT assumption—the Stable Unit Treatment Value Assumption (SUTVA)—which requires that the treatment status of one unit does not affect the outcomes of other units. This violation precludes clear separation of treatment and control groups, a challenge prevalent in modern social science contexts \citep{imbens2024causal}. This raises a central question across scientific fields: ``How can we efficiently estimate and validate the causal effect of an intervention within a network of interacting units?" \citep{eckles2016design,cai2015social,abaluck2022impact,ogburn2024causal}.

To articulate the challenges in the estimation and validation of causal effects, we first introduce a formal notation and visualization. Consider a randomized experiment conducted over time stamps $t = 0, 1, \ldots, T$, involving a population of $\UN$ units, indexed by $i = 1, \ldots, \UN$. At each time $t$, unit~$i$ receives a treatment (e.g., exposure to the voting campaign) according to a random variable $\treatment{i}{t}$. In the voting example, these treatment variables are binary: 1 indicates assignment to the treatment group, and 0 indicates assignment to the control group.\footnote{Our theoretical results apply to the more general case when the treatments can also be continuous-valued.}

The treatment allocation across the entire population and time frame is represented by an $\UN \times (T+1)$ matrix, denoted by $\Mtreatment{}{}$. Following the Rubin causal framework \citep{imbens2015causal}, we denote by $\outcomeD{}{i}{t}(\Mtreatment{}{})$ the potential outcome of unit $i$ at time $t$. The counterfactual evolution, denoted by $\CFE{\Mtreatment{}{}}{}{t}$, represents the sequence of sample means of potential outcomes under the treatment assignment $\Mtreatment{}{}$ over time\footnote{Typically, outcomes are ``non-anticipating,'' meaning $\CFE{\Mtreatment{}{}}{}{t}$ and $\outcomeDW{\Mtreatment{}{}}{i}{t}$ depend only on treatments up to time~$t$. Our notation and theoretical results allow for more general scenarios where outcomes may be influenced by future treatments. However, for ease of reading, readers may assume the simpler case of non-anticipation where only the first $t+1$ columns of $\Mtreatment{}{}$ impact $\outcomeDW{\Mtreatment{}{}}{i}{t}$. Moreover, the first column of $\Mtreatment{}{}$ is by convention equal to all zeros (or no-treatment state), and the first column of $\Moutcome{}{}{}$ corresponds to outcomes in the all-control state.}:
% 
\begin{align}
    \label{eq:sample_mean_outcomes}
    \CFE{\Mtreatment{}{}}{}{t} :=
    \frac{1}{N} \sum_{i=1}^\UN \outcomeDW{\Mtreatment{}{}}{i}{t},
    \quad\quad\quad
    t = 0,1, \ldots, T.
\end{align}
%
With $\OMtreatment{}{}$ being a specific realization of $\Mtreatment{}{}$, let $\Moutcome{}{}{}(\Mtreatment{}{} = \OMtreatment{}{})$ denote an $N \times (T+1)$ matrix that collectively represents the observed outcomes under the treatment allocation $\OMtreatment{}{}$.  For example, if per-unit treatment probability at all time periods is equal to $\expr$, $\CFE{\Mtreatment{}{}}{}{t}$ evolves as a stochastic function of time and $\expr$, where the randomness arises from the treatment allocation~$\Mtreatment{}{}$. 
Figure~\ref{fig:SE_surface} visualizes this evolution as a two-dimensional surface parameterized by time and $\expr$, where each contour (at fixed $\expr$) represents one possible dynamic trajectory.\footnote{Note that treatment probabilities can vary over time, making the space of possible counterfactual evolutions $\CFE{\Mtreatment{}{} = \OMtreatment{}{}}{}{t}$ considerably more complex than the simplified visualization shown in Figure~\ref{fig:SE_surface}.}
% 
\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{plots/surface_final.png}
    \caption{Evolution of outcomes sample mean (z-axis) with respect to time $t$ and treatment probability $p$. Red and blue contours highlight the counterfactual evolutions at treatment probabilities $p=0.25$ and $p=0.75$, respectively. The magenta line represents the equilibrium state, where the treatment effect has stabilized.}
    \label{fig:SE_surface}
\end{figure}
% 

Our objective is to estimate counterfactual evolutions $\CFE{\OMtreatment{}{}'}{}{t}$ under alternative treatment allocations $\OMtreatment{}{}' \neq \OMtreatment{}{}$ and validate the accuracy of these estimations. This enables estimation of treatment effects over time through comparing counterfactual evolutions under different treatment conditions—for example, in the voting study, contrasting evolution under an intensive campaign against the status quo control. However, this objective faces a fundamental challenge: we can observe the system only under a single scenario, reflecting ``the fundamental problem of causal inference'' \citep{holland1986statistics}. Specifically, with treatment probability fixed at $\expr$, we observe only the corresponding contour in the surface shown in Figure~\ref{fig:SE_surface}. This limitation constrains both our ability to estimate counterfactual evolutions and validate such estimates, as we cannot directly observe outcomes under alternative scenarios.

To address the estimation challenge, we develop our approach by drawing an analogy between two systems: a network of units exchanging causal effects and a molecular system exchanging energy. In molecular systems, energy perturbations trigger redistribution, according to invariant physical laws governing molecular interactions, until a new equilibrium is reached. Similarly, in social networks, targeted interventions propagate through network connections following consistent social and behavioral principles until the system stabilizes. This analogy highlights a crucial insight: the \emph{temporal invariance} of underlying mechanisms. Just as the equations governing molecular energy redistribution maintain their structural form, the mechanisms driving the propagation of intervention effects preserve their fundamental structure over time. This structural invariance presents a key opportunity: by observing the system's evolution across different time periods, we can employ machine learning tools to learn the mathematical functions that characterize how intervention effects propagate through the network and aggregate into counterfactual evolutions\footnote{This structural invariance differs from stationarity, as we allow the underlying mechanisms to exhibit temporal variations while maintaining their basic mathematical form.}.

To address the validation challenge, a natural candidate is cross-validation---a cornerstone of statistical learning \citep{hastie2009elements} whose theoretical foundations \citep{stone1974cross,allen1974relationship} build upon Fisher's fundamental insight about the necessity of randomization for independent validation \citep{fisher1935design}. However, cross-validation's application in causal inference has been notably limited by data scarcity. Existing analyses typically focus on observations at equilibrium reached at large~$t$, depicted by the ``equilibrium'' curve in Figure~\ref{fig:SE_surface} \citep{basse2019randomization,jackson2020adjusting,li2022random}, providing insufficient samples for reliable validation. Even recent approaches that leverage temporal pre-equilibrium data \citep{shirani2024causal,bayati2024higher} provide only order $T$ summary statistics. This represents a significant constraint, given that extending experiment duration is often prohibitively costly \citep{holtz2020reducing,cooprider2023science,xiong2024optimal}. To overcome this limitation, we introduce a new \emph{\batching{}} method (\batchingAcronym{}) that generates sufficient data to implement cross-validation, enabling rigorous validation of our estimation results.

By combining the temporal invariance principle with \batchingAcronym{}, we create a comprehensive framework that addresses both estimation and validation challenges in settings with unobserved network interference. Specifically, this work makes four main contributions. First, we extend the Causal Message-Passing framework \citep{shirani2024causal} by incorporating unit- and network-level heterogeneities, deriving more general equations for outcome evolution. Second, we develop non-asymptotic versions of these dynamic equations using techniques from \cite{li2022non}, leading to our \batchingAcronym{} method for generating and analyzing multiple subpopulations from the experimental population. This approach yields sufficient observations at each time period for accurate estimation through machine learning techniques.

Third, we combine temporal data with our \batchingAcronym{} method to develop a cross-validation framework for counterfactual estimation, implementing data-driven model selection procedures and preprocessing for complex time-trends to improve accuracy. Our \batchingAcronym{} method may have applications beyond causal inference, particularly in network sampling problems, where it offers an approach for generating representative subsamples from networked populations without requiring prior knowledge of network structure.

Our fourth contribution addresses a fundamental challenge in causal inference: the rigorous examination of estimation methods when ground-truth counterfactuals are typically unobservable. We develop a comprehensive benchmark toolbox with six diverse experimental environments that capture different forms of network interference and temporal dynamics. These environments encompass applications ranging from social network influence to data center load balancing and ride-sharing systems. A notable highlight is our social network environment, which simulates a social media platform through thousands of AI agents interacting with their local network connections via content feeds. Each environment is carefully constructed to maintain known ground truth values while incorporating realistic complexities such as time-varying trends and heterogeneous network effects. We use this toolbox to extensively evaluate our method against existing benchmarks. Importantly, we are making these environments publicly available to facilitate standardized testing of future methodologies.\footnote{Code and environments available at \url{https://github.com/CausalMP/CausalMP.git}}. This contribution advances the field by providing researchers with a much-needed framework for systematic evaluations of causal inference methods under general network interference \citep{basse2018limitations,savje2021average,arkhangelsky2023causal,imbens2024causal}.

The rest of the paper is organized as follows. Section \ref{sec:other-lit} reviews related literature on causal inference under network interference, approximate message passing, and network sampling. Section \ref{sec:Causal_Estimands} presents our generalization of the Causal Message-Passing framework \citep{shirani2024causal}. Section \ref{sec:DPNB} introduces our \batching{} method for generating sufficient data to learn invariant aggregate dynamics. Section \ref{sec:C-CV} presents our cross-validation framework for counterfactual estimation. Sections \ref{sec:Benchmark_Toolbox} and \ref{sec:results} describe our benchmark toolbox and empirical validation results. Section \ref{sec:conclusion} concludes with discussion and future directions. Technical proofs and supplementary results appear in the appendices.


\section{Related Literature}
\label{sec:other-lit}

Recent literature on causal inference under network interference has evolved along several key dimensions. One common approach addresses settings with known network clusters \citep{sobel2006randomized,rosenbaum2007interference,hudgens2008toward,tchetgen2012causal,auerbach2021local}, where researchers randomly assign entire clusters to different treatment intensities. This cluster-based design helps contain interference within cluster boundaries, though it requires prior knowledge of network clusters. Another significant development employs ``exposure mappings'' \citep{manski2013identification,aronow2017estimating,leung2022causal,harshaw2023design,savje2024causal}, which quantify how a unit's treatment effect varies based on its neighbors' treatment status. For scenarios with unknown interference patterns, researchers have proposed alternative strategies using historical data or staggered treatment rollouts \citep{yu2022estimating,cortez2022staggered}. A particularly promising direction has emerged through application-specific approaches that develop customized interference models for distinct contexts such as marketplace dynamics \citep{bajari2021multiple,holtz2020reducing,wager2021experimenting,munro2021treatment,johari2022experimental,bright2022reducing}. These domain-specific solutions offer tailored frameworks for managing interference in their respective experimental settings. Our work contributes to this literature by developing methods that do not require knowledge of the interference structure or domain-specific context while still capturing complex network effects across diverse applications.

Our methodology is supported by rigorous theoretical results derived from Approximate Message-Passing (AMP), whose foundations trace back to  \cite{thouless1977solution,kabashima2003cdma} and \cite{donoho2009message} and were formally established by \cite{bolthausen2014iterative}, \cite{bayati2011dynamics} and a large body of recent literature \citep{javanmard2013state,bayati2015universality,berthier2020state,chen2020universality,xinyi2021approximate,wang2022universality,dudeja2023universality,rush2018finite,li2022non}. At its core, we employ the TAP equation and cavity method—a mathematical technique that characterizes system behavior by analyzing responses to carefully introduced perturbations \citep{mezard1986spin,mezard2009information}. AMP traditionally addresses high-dimensional estimation problems through iterative systems with known mixing matrices. Our approach is different and builds on the work of \cite{shirani2024causal}; specifically, they assume the mixing matrix is unknown and develop techniques similar to those developed in the AMP literature to analyze observed outcomes and infer causal effects.

Our work on \batching{} relates to a rich body of research on network sampling and statistical inference in networked settings. The challenge of generating representative samples from network data has been central to network analysis, due to the inherent dependencies in network structures \citep{wormald1999models,newman2018networks}. Network inference methods have evolved from basic configuration models \citep{bollobas1980probabilistic} to sophisticated approaches handling non-independent observations \citep{snijders1999non,bickel2009nonparametric} and robust bootstrapping techniques \citep{bhattacharyya2015subsampling,green2022bootstrapping}. Recent advances encompass null models for complex network structures \citep{karrer2011stochastic,bianconi2018multilayer} and machine learning approaches using graph neural networks \citep{kipf2017semisupervised,hamilton2020graph}. Our approach differs fundamentally from this literature as it operates solely on node-level outcomes observed over time, without requiring access to the underlying network structure.