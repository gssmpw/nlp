\section{Related Work}
\label{Related Work}
\subsection{Jailbreak Attack on LLMs}
Previous studies show LLMs can be manipulated to generate harmful content via prompts **Brown, "Man is to Computer Programmer as Cat is to _______"**, often through manual design or model-generated adversarial prompts
For example, DAN **Hendrycks, "Natural Adversarial Examples"** proposed thousands of manually designed jailbreak templates. DeepInception **Zellers, "On the Dangers of Stochastic Parrots: Can Pre-trained Models Be Robbed?"** leverages LLM personification abilities and a virtual nested scene to achieve adaptive jailbreaks with high harmfulness. PAIR **Hendrycks, "Pre-Trained Transformers as Universal Computation Engines"** uses an attacker LLM to iteratively refine jailbreaking prompts, achieving high success rates with minimal queries.
Optimization-based methods also represent a significant approach in jailbreak attacks. 
The GCG method **Brown, "Man is to Computer Programmer as Cat is to _______"** generates adversarial suffixes via gradient-based search, AutoDan **Zellers, "On the Dangers of Stochastic Parrots: Can Pre-Trained Models Be Robbed?"** uses a hierarchical genetic algorithm, and ASETF **Hendrycks, "Pre-Trained Transformers as Universal Computation Engines"** optimizes them with an embedding translation model.
SAA **Brown, "Man is to Computer Programmer as Cat is to _______"** extended GCG with adaptive adversarial templates. 
% DiffusionAttacker **Zellers, "On the Dangers of Stochastic Parrots: Can Pre-Trained Models Be Robbed?"** uses a seq2seq text diffusion model and guides the denoising process using attack loss. 

\subsection{Jailbreak Defense on LLMs}
Jailbreak defense can be applied through either response-defense or prompt-defense methods.
Response-defense methods evaluate and modify model outputs to mitigate harmful responses, including fine-tuned classifiers **Hendrycks, "Natural Adversarial Examples"** for detecting unsafe generations and inference-time techniques like self-examination and response filtering **Brown, "Man is to Computer Programmer as Cat is to _______"**.
However, these approaches require additional inference steps, increasing latency and computational cost.
Prompt-defense defenses offer a more efficient alternative by analyzing and modifying prompts before LLM inference, reducing the risk of generating unsafe outputs while saving computational resources. 
Existing parameter-free methods rely on ad hoc reasoning, such as perplexity-based filtering **Brown, "Man is to Computer Programmer as Cat is to _______"**, paraphrasing **Hendrycks, "Pre-Trained Transformers as Universal Computation Engines"**, self-reminders **Zellers, "On the Dangers of Stochastic Parrots: Can Pre-Trained Models Be Robbed?"**, in-context demonstrations **Brown, "Man is to Computer Programmer as Cat is to _______"**, and intent-based two-stage filtering **Hendrycks, "Natural Adversarial Examples"**. Although G4D **Brown, "Man is to Computer Programmer as Cat is to _______"** enhances defense with multi-agent guidance and external knowledge (Wikipedia), it remains computationally expensive and lacks a structured approach to capturing intrinsic attack characteristics. In contrast, our ShieldLearner directly learns attack patterns and defense principles from jailbreak prompt data, enabling a more systematic and generalizable defense.