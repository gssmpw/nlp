@article{alon2023detecting,
  title={Detecting language model attacks with perplexity},
  author={Alon, Gabriel and Kamfonas, Michael},
  journal={arXiv preprint arXiv:2308.14132},
  year={2023}
}

@article{andriushchenko2024jailbreaking,
  title={Jailbreaking leading safety-aligned llms with simple adaptive attacks},
  author={Andriushchenko, Maksym and Croce, Francesco and Flammarion, Nicolas},
  journal={arXiv preprint arXiv:2404.02151},
  year={2024}
}

@article{cao2024guide,
  title={Guide for Defense (G4D): Dynamic Guidance for Robust and Balanced Defense in Large Language Models},
  author={Cao, He and Luo, Weidi and Wang, Yu and Liu, Zijing and Feng, Bing and Yao, Yuan and Li, Yu},
  journal={arXiv preprint arXiv:2410.17922},
  year={2024}
}

@article{chao2023jailbreaking,
  title={Jailbreaking black box large language models in twenty queries},
  author={Chao, Patrick and Robey, Alexander and Dobriban, Edgar and Hassani, Hamed and Pappas, George J and Wong, Eric},
  journal={arXiv preprint arXiv:2310.08419},
  year={2023}
}

@article{inan2023llama,
  title={Llama guard: Llm-based input-output safeguard for human-ai conversations},
  author={Inan, Hakan and Upasani, Kartikeya and Chi, Jianfeng and Rungta, Rashi and Iyer, Krithika and Mao, Yuning and Tontchev, Michael and Hu, Qing and Fuller, Brian and Testuggine, Davide and others},
  journal={arXiv preprint arXiv:2312.06674},
  year={2023}
}

@article{jain2023baseline,
  title={Baseline defenses for adversarial attacks against aligned language models},
  author={Jain, Neel and Schwarzschild, Avi and Wen, Yuxin and Somepalli, Gowthami and Kirchenbauer, John and Chiang, Ping-yeh and Goldblum, Micah and Saha, Aniruddha and Geiping, Jonas and Goldstein, Tom},
  journal={arXiv preprint arXiv:2309.00614},
  year={2023}
}

@article{ji2024aligner,
  title={Aligner: Efficient alignment by learning to correct},
  author={Ji, Jiaming and Chen, Boyuan and Lou, Hantao and Hong, Donghai and Zhang, Borong and Pan, Xuehai and Dai, Juntao and Qiu, Tianyi and Yang, Yaodong},
  journal={arXiv preprint arXiv:2402.02416},
  year={2024}
}

@article{li2023deepinception,
  title={Deepinception: Hypnotize large language model to be jailbreaker},
  author={Li, Xuan and Zhou, Zhanke and Zhu, Jianing and Yao, Jiangchao and Liu, Tongliang and Han, Bo},
  journal={arXiv preprint arXiv:2311.03191},
  year={2023}
}

@article{liu2023autodan,
  title={Autodan: Generating stealthy jailbreak prompts on aligned large language models},
  author={Liu, Xiaogeng and Xu, Nan and Chen, Muhao and Xiao, Chaowei},
  journal={arXiv preprint arXiv:2310.04451},
  year={2023}
}

@article{phute2023llm,
  title={Llm self defense: By self examination, llms know they are being tricked},
  author={Phute, Mansi and Helbling, Alec and Hull, Matthew and Peng, ShengYun and Szyller, Sebastian and Cornelius, Cory and Chau, Duen Horng},
  journal={arXiv preprint arXiv:2308.07308},
  year={2023}
}

@article{robey2023smoothllm,
  title={Smoothllm: Defending large language models against jailbreaking attacks},
  author={Robey, Alexander and Wong, Eric and Hassani, Hamed and Pappas, George J},
  journal={arXiv preprint arXiv:2310.03684},
  year={2023}
}

@inproceedings{shen2024anything,
  title={" do anything now": Characterizing and evaluating in-the-wild jailbreak prompts on large language models},
  author={Shen, Xinyue and Chen, Zeyuan and Backes, Michael and Shen, Yun and Zhang, Yang},
  booktitle={Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security},
  pages={1671--1685},
  year={2024}
}

@inproceedings{wang2024asetf,
  title={Asetf: A novel method for jailbreak attack on llms through translate suffix embeddings},
  author={Wang, Hao and Li, Hao and Huang, Minlie and Sha, Lei},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages={2697--2711},
  year={2024}
}

@article{wang2024diffusionattacker,
  title={DiffusionAttacker: Diffusion-Driven Prompt Manipulation for LLM Jailbreak},
  author={Wang, Hao and Li, Hao and Zhu, Junda and Wang, Xinyuan and Pan, Chengwei and Huang, MinLie and Sha, Lei},
  journal={arXiv preprint arXiv:2412.17522},
  year={2024}
}

@article{wei2023jailbreak,
  title={Jailbreak and guard aligned language models with only few in-context demonstrations},
  author={Wei, Zeming and Wang, Yifei and Li, Ang and Mo, Yichuan and Wang, Yisen},
  journal={arXiv preprint arXiv:2310.06387},
  year={2023}
}

@article{wei2024jailbroken,
  title={Jailbroken: How does llm safety training fail?},
  author={Wei, Alexander and Haghtalab, Nika and Steinhardt, Jacob},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{xie2023defending,
  title={Defending chatgpt against jailbreak attack via self-reminders},
  author={Xie, Yueqi and Yi, Jingwei and Shao, Jiawei and Curl, Justin and Lyu, Lingjuan and Chen, Qifeng and Xie, Xing and Wu, Fangzhao},
  journal={Nature Machine Intelligence},
  volume={5},
  number={12},
  pages={1486--1496},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@article{xu2024safedecoding,
  title={Safedecoding: Defending against jailbreak attacks via safety-aware decoding},
  author={Xu, Zhangchen and Jiang, Fengqing and Niu, Luyao and Jia, Jinyuan and Lin, Bill Yuchen and Poovendran, Radha},
  journal={arXiv preprint arXiv:2402.08983},
  year={2024}
}

@article{zeng2024autodefense,
  title={Autodefense: Multi-agent llm defense against jailbreak attacks},
  author={Zeng, Yifan and Wu, Yiran and Zhang, Xiao and Wang, Huazheng and Wu, Qingyun},
  journal={arXiv preprint arXiv:2403.04783},
  year={2024}
}

@article{zeng2024shieldgemma,
  title={Shieldgemma: Generative ai content moderation based on gemma},
  author={Zeng, Wenjun and Liu, Yuchi and Mullins, Ryan and Peran, Ludovic and Fernandez, Joe and Harkous, Hamza and Narasimhan, Karthik and Proud, Drew and Kumar, Piyush and Radharapu, Bhaktipriya and others},
  journal={arXiv preprint arXiv:2407.21772},
  year={2024}
}

@article{zhang2024intention,
  title={Intention analysis prompting makes large language models a good jailbreak defender},
  author={Zhang, Yuqi and Ding, Liang and Zhang, Lefei and Tao, Dacheng},
  journal={arXiv preprint arXiv:2401.06561},
  year={2024}
}

@article{zhang2024shieldlm,
  title={Shieldlm: Empowering llms as aligned, customizable and explainable safety detectors},
  author={Zhang, Zhexin and Lu, Yida and Ma, Jingyuan and Zhang, Di and Li, Rui and Ke, Pei and Sun, Hao and Sha, Lei and Sui, Zhifang and Wang, Hongning and others},
  journal={arXiv preprint arXiv:2402.16444},
  year={2024}
}

@article{zou2023universal,
  title={Universal and transferable adversarial attacks on aligned language models},
  author={Zou, Andy and Wang, Zifan and Carlini, Nicholas and Nasr, Milad and Kolter, J Zico and Fredrikson, Matt},
  journal={arXiv preprint arXiv:2307.15043},
  year={2023}
}

