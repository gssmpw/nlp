\section{Related Work}
\label{Related Work}
\subsection{Jailbreak Attack on LLMs}
Previous studies show LLMs can be manipulated to generate harmful content via prompts~\citep{wang2024diffusionattacker, wei2024jailbroken}, often through manual design or model-generated adversarial prompts
For example, DAN~\cite{shen2024anything} proposed thousands of manually designed jailbreak templates. DeepInception~\cite{li2023deepinception} leverages LLM personification abilities and a virtual nested scene to achieve adaptive jailbreaks with high harmfulness. PAIR~\cite{chao2023jailbreaking} uses an attacker LLM to iteratively refine jailbreaking prompts, achieving high success rates with minimal queries.
Optimization-based methods also represent a significant approach in jailbreak attacks. 
The GCG method~\cite{zou2023universal} generates adversarial suffixes via gradient-based search, AutoDan~\cite{liu2023autodan} uses a hierarchical genetic algorithm, and ASETF~\cite{wang2024asetf} optimizes them with an embedding translation model.
SAA~\cite{andriushchenko2024jailbreaking} extended GCG with adaptive adversarial templates. 
% DiffusionAttacker~\cite{wang2024diffusionattacker} uses a seq2seq text diffusion model and guides the denoising process using attack loss. 

\subsection{Jailbreak Defense on LLMs}
Jailbreak defense can be applied through either response-defense or prompt-defense methods.
Response-defense methods evaluate and modify model outputs to mitigate harmful responses, including fine-tuned classifiers \cite{ji2024aligner, inan2023llama, zhang2024shieldlm, zeng2024shieldgemma} for detecting unsafe generations and inference-time techniques like self-examination and response filtering \cite{phute2023llm, robey2023smoothllm, xu2024safedecoding, zeng2024autodefense}.
However, these approaches require additional inference steps, increasing latency and computational cost.
Prompt-defense defenses offer a more efficient alternative by analyzing and modifying prompts before LLM inference, reducing the risk of generating unsafe outputs while saving computational resources. 
Existing parameter-free methods rely on ad hoc reasoning, such as perplexity-based filtering \cite{alon2023detecting}, paraphrasing \cite{jain2023baseline}, self-reminders \cite{xie2023defending}, in-context demonstrations \cite{wei2023jailbreak}, and intent-based two-stage filtering \cite{zhang2024intention}. Although G4D \cite{cao2024guide} enhances defense with multi-agent guidance and external knowledge (Wikipedia), it remains computationally expensive and lacks a structured approach to capturing intrinsic attack characteristics. In contrast, our ShieldLearner directly learns attack patterns and defense principles from jailbreak prompt data, enabling a more systematic and generalizable defense.