[
  {
    "index": 0,
    "papers": [
      {
        "key": "wang2024diffusionattacker",
        "author": "Wang, Hao and Li, Hao and Zhu, Junda and Wang, Xinyuan and Pan, Chengwei and Huang, MinLie and Sha, Lei",
        "title": "DiffusionAttacker: Diffusion-Driven Prompt Manipulation for LLM Jailbreak"
      },
      {
        "key": "wei2024jailbroken",
        "author": "Wei, Alexander and Haghtalab, Nika and Steinhardt, Jacob",
        "title": "Jailbroken: How does llm safety training fail?"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "shen2024anything",
        "author": "Shen, Xinyue and Chen, Zeyuan and Backes, Michael and Shen, Yun and Zhang, Yang",
        "title": "\" do anything now\": Characterizing and evaluating in-the-wild jailbreak prompts on large language models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "li2023deepinception",
        "author": "Li, Xuan and Zhou, Zhanke and Zhu, Jianing and Yao, Jiangchao and Liu, Tongliang and Han, Bo",
        "title": "Deepinception: Hypnotize large language model to be jailbreaker"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "chao2023jailbreaking",
        "author": "Chao, Patrick and Robey, Alexander and Dobriban, Edgar and Hassani, Hamed and Pappas, George J and Wong, Eric",
        "title": "Jailbreaking black box large language models in twenty queries"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "zou2023universal",
        "author": "Zou, Andy and Wang, Zifan and Carlini, Nicholas and Nasr, Milad and Kolter, J Zico and Fredrikson, Matt",
        "title": "Universal and transferable adversarial attacks on aligned language models"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "liu2023autodan",
        "author": "Liu, Xiaogeng and Xu, Nan and Chen, Muhao and Xiao, Chaowei",
        "title": "Autodan: Generating stealthy jailbreak prompts on aligned large language models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "wang2024asetf",
        "author": "Wang, Hao and Li, Hao and Huang, Minlie and Sha, Lei",
        "title": "Asetf: A novel method for jailbreak attack on llms through translate suffix embeddings"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "andriushchenko2024jailbreaking",
        "author": "Andriushchenko, Maksym and Croce, Francesco and Flammarion, Nicolas",
        "title": "Jailbreaking leading safety-aligned llms with simple adaptive attacks"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "wang2024diffusionattacker",
        "author": "Wang, Hao and Li, Hao and Zhu, Junda and Wang, Xinyuan and Pan, Chengwei and Huang, MinLie and Sha, Lei",
        "title": "DiffusionAttacker: Diffusion-Driven Prompt Manipulation for LLM Jailbreak"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "ji2024aligner",
        "author": "Ji, Jiaming and Chen, Boyuan and Lou, Hantao and Hong, Donghai and Zhang, Borong and Pan, Xuehai and Dai, Juntao and Qiu, Tianyi and Yang, Yaodong",
        "title": "Aligner: Efficient alignment by learning to correct"
      },
      {
        "key": "inan2023llama",
        "author": "Inan, Hakan and Upasani, Kartikeya and Chi, Jianfeng and Rungta, Rashi and Iyer, Krithika and Mao, Yuning and Tontchev, Michael and Hu, Qing and Fuller, Brian and Testuggine, Davide and others",
        "title": "Llama guard: Llm-based input-output safeguard for human-ai conversations"
      },
      {
        "key": "zhang2024shieldlm",
        "author": "Zhang, Zhexin and Lu, Yida and Ma, Jingyuan and Zhang, Di and Li, Rui and Ke, Pei and Sun, Hao and Sha, Lei and Sui, Zhifang and Wang, Hongning and others",
        "title": "Shieldlm: Empowering llms as aligned, customizable and explainable safety detectors"
      },
      {
        "key": "zeng2024shieldgemma",
        "author": "Zeng, Wenjun and Liu, Yuchi and Mullins, Ryan and Peran, Ludovic and Fernandez, Joe and Harkous, Hamza and Narasimhan, Karthik and Proud, Drew and Kumar, Piyush and Radharapu, Bhaktipriya and others",
        "title": "Shieldgemma: Generative ai content moderation based on gemma"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "phute2023llm",
        "author": "Phute, Mansi and Helbling, Alec and Hull, Matthew and Peng, ShengYun and Szyller, Sebastian and Cornelius, Cory and Chau, Duen Horng",
        "title": "Llm self defense: By self examination, llms know they are being tricked"
      },
      {
        "key": "robey2023smoothllm",
        "author": "Robey, Alexander and Wong, Eric and Hassani, Hamed and Pappas, George J",
        "title": "Smoothllm: Defending large language models against jailbreaking attacks"
      },
      {
        "key": "xu2024safedecoding",
        "author": "Xu, Zhangchen and Jiang, Fengqing and Niu, Luyao and Jia, Jinyuan and Lin, Bill Yuchen and Poovendran, Radha",
        "title": "Safedecoding: Defending against jailbreak attacks via safety-aware decoding"
      },
      {
        "key": "zeng2024autodefense",
        "author": "Zeng, Yifan and Wu, Yiran and Zhang, Xiao and Wang, Huazheng and Wu, Qingyun",
        "title": "Autodefense: Multi-agent llm defense against jailbreak attacks"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "alon2023detecting",
        "author": "Alon, Gabriel and Kamfonas, Michael",
        "title": "Detecting language model attacks with perplexity"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "jain2023baseline",
        "author": "Jain, Neel and Schwarzschild, Avi and Wen, Yuxin and Somepalli, Gowthami and Kirchenbauer, John and Chiang, Ping-yeh and Goldblum, Micah and Saha, Aniruddha and Geiping, Jonas and Goldstein, Tom",
        "title": "Baseline defenses for adversarial attacks against aligned language models"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "xie2023defending",
        "author": "Xie, Yueqi and Yi, Jingwei and Shao, Jiawei and Curl, Justin and Lyu, Lingjuan and Chen, Qifeng and Xie, Xing and Wu, Fangzhao",
        "title": "Defending chatgpt against jailbreak attack via self-reminders"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "wei2023jailbreak",
        "author": "Wei, Zeming and Wang, Yifei and Li, Ang and Mo, Yichuan and Wang, Yisen",
        "title": "Jailbreak and guard aligned language models with only few in-context demonstrations"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "zhang2024intention",
        "author": "Zhang, Yuqi and Ding, Liang and Zhang, Lefei and Tao, Dacheng",
        "title": "Intention analysis prompting makes large language models a good jailbreak defender"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "cao2024guide",
        "author": "Cao, He and Luo, Weidi and Wang, Yu and Liu, Zijing and Feng, Bing and Yao, Yuan and Li, Yu",
        "title": "Guide for Defense (G4D): Dynamic Guidance for Robust and Balanced Defense in Large Language Models"
      }
    ]
  }
]