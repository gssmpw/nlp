%%%%%%%% ICML 2021 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass[11pt,twoside]{article} % For LaTeX2e
%\usepackage{iclr2021_conference,times}

\setlength{\textwidth}{\paperwidth}
\addtolength{\textwidth}{-6cm}
\setlength{\textheight}{\paperheight}
\addtolength{\textheight}{-4cm}
\addtolength{\textheight}{-1.1\headheight}
\addtolength{\textheight}{-\headsep}
\addtolength{\textheight}{-\footskip}
\setlength{\oddsidemargin}{0.5cm}
\setlength{\evensidemargin}{0.5cm}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.

\usepackage{eqnarray,amsmath}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
       % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

% \usepackage{subfigure}
\usepackage{epsf}
\usepackage{epsfig}
\usepackage{fancyhdr}
\usepackage{graphics}
\usepackage{graphicx}
% \usepackage{epstopdf}
\usepackage{psfrag}
\usepackage{fullpage}
\usepackage{pdfpages}
% \usepackage{natbib}
\usepackage{diagbox}


\usepackage{url}% for url's in bib
% for theorem hyperlink colors
\usepackage[colorlinks,linkcolor=magenta,citecolor=blue, pagebackref=true]{hyperref}
% \renewcommand*{\backref}[1]{\ifx#1\relax \else Page #1 \fi}
\renewcommand*{\backrefalt}[4]{%
    \ifcase #1 \footnotesize{(Not cited.)}%
    \or        \footnotesize{(Cited on page~#2.)}%
    \else      \footnotesize{(Cited on pages~#2.)}%
    \fi}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
%\usepackage[capitalize,noabbrev]{cleveref}

\usepackage{color}
% \usepackage[table]{xcolor}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb,bbm}
\usepackage{caption}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{textcomp}
\usepackage{siunitx}
\usepackage{wrapfig}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{multirow}
\usepackage{multicol}% colors
% \newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage[T1]{fontenc}
\usepackage{colortbl}

\newcommand{\strongconvex}{\mu}
\newcommand{\smooth}{L_1}
\newcommand{\smoothprior}{L_2}
\newcommand{\subgaussian}{\sigma}
\newcommand{\barFn}{\bar{F}_n}
\usepackage{graphicx}


\newtheorem*{remark}{Remark}


\newtheorem{assumption}{Assumption}

\newcommand*\rot{\rotatebox{90}}

\input{macro_commands}
\input{math_commands}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\setlength{\textwidth}{\paperwidth}
%\addtolength{\textwidth}{-6cm}
%\setlength{\textheight}{\paperheight}
%\addtolength{\textheight}{-4cm}
%\addtolength{\textheight}{-1.1\headheight}
%\addtolength{\textheight}{-\headsep}
%\addtolength{\textheight}{-\footskip}
%\setlength{\oddsidemargin}{0.5cm}
%\setlength{\evensidemargin}{0.5cm}

% baselinestretch trick to save some space
 %   \renewcommand{\baselinestretch}{0.99}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \newtheorem{lemma}{Lemma}
% \newtheorem{example}{Example}
% \newtheorem{theorem}{Theorem}
% \newtheorem{proposition}{Proposition}
% \newtheorem{definition}{Definition}
% \newtheorem{corollary}{Corollary}
%\usepackage{natbib}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
%\newtheorem{assumption}[theorem]{Assumption}
%\theoremstyle{remark}
%\newtheorem{remark}[theorem]{Remark}

%opening
%\newtheorem{assumption}{Assumption}
%\newtheorem{conjecture}{Conjecture}
\newenvironment{assumptionprime}[1]
  {\renewcommand{\theassumption}{\ref{#1}$'$}%
   \addtocounter{assumption}{-1}%
   \begin{assumption}}
  {\end{assumption}}


%\def\argmin{\textnormal{arg} \min}
\def\st{\textnormal{s.t.}}
\def\sgn{\texttt{sign}}
\newcommand{\todo}[1]{\textcolor{red}{[Todo: #1]}}
%\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
%\renewcommand{\algorithmicrequire}{\textbf{Input:}}
%\renewcommand{\algorithmicensure}{\textbf{Output:}}




\newcommand{\widgraph}[2]{\includegraphics[keepaspectratio,width=#1]{#2}}

\newcommand{\notiff}{%
  \mathrel{{\ooalign{\hidewidth$\not\phantom{"}$\hidewidth\cr$\iff$}}}}


% Attempt to make hyperref and algorithmic work together better:






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2021}
% \DeclareMathOperator*{\argmax}{arg\,max}  % in your preamble
% \DeclareMathOperator*{\argmin}{arg\,min}  % in your preamble 
% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2021}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
% \icmltitlerunning{BoMb-OT: On Batch of Mini-batches Optimal Transport}

\begin{document}

\begin{center}

{\bf{\LARGE{RepLoRA: Reparameterizing Low-Rank Adaptation via the Perspective of Mixture of Experts}}}
  
\vspace*{.2in}
{\large{
\begin{tabular}{cccc}
Tuan Truong$^{\diamond,\star}$ & Chau Nguyen$^{\diamond,\star}$ & Huy Nguyen$^{\dagger,\star}$ & Minh Le$^{\diamond}$ \\
& Trung Le$^{\ddagger}$ & Nhat Ho$^{\dagger}$
\end{tabular}
}}

\vspace*{.2in}

\begin{tabular}{ccc}
The University of Texas at Austin$^{\dagger}$\\
Monash University$^{\ddagger}$\\
VinAI Research$^{\diamond}$ \\
\end{tabular}

\vspace*{.1in}
\today

\vspace*{.2in}

\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}

\blfootnote{$^\star$ Equal contribution}

\begin{abstract}
Low-rank adaptation (LoRA) has emerged as a powerful method for fine-tuning large-scale foundation models. Despite its popularity, the theoretical understanding of LoRA has remained limited. This paper presents a theoretical analysis of LoRA by examining its connection to the Mixture of Experts models. Under this framework, we show that simple reparameterizations of the LoRA matrices can notably accelerate the low-rank matrix estimation process. In particular, we prove that reparameterization can reduce the data needed to achieve a desired estimation error from an exponential to a polynomial scale. Motivated by this insight, we propose \textit{\textbf{Rep}arameterized \textbf{Lo}w-\textbf{R}ank \textbf{A}daptation} (RepLoRA), which incorporates lightweight MLPs to reparameterize the LoRA matrices. Extensive experiments across multiple domains demonstrate that RepLoRA consistently outperforms vanilla LoRA. Notably, with limited data, RepLoRA surpasses LoRA by a margin of up to \textbf{40.0\%} and achieves LoRA's performance with only \textbf{30.0\%} of the training data, highlighting both the theoretical and empirical robustness of our PEFT method.
\end{abstract}
\end{center}

\section{Introduction}
\label{section: introduction}
\input{sections/introduction}

% \section{Related Works}
% \label{section: related works}
% \input{sections/related_works}

\section{Preliminaries}
\label{section: preliminaries}
\input{sections/preliminaries}

\section{Theoretical Analysis of LoRA: With and Without Reparameterization}
\label{section: theory}
\input{sections/theory}

\section{Reparameterized Low-Rank Adaptation}
\label{section: practice}
\input{sections/practice}

\section{Experiments}
\label{section: experiments}
\input{sections/experiments}

\section{Conclusion} \label{section: conclusion}
We introduced a theoretical framework that bridges LoRA with MoE, offering new insights into the benefits of reparameterizing LoRA for achieving optimal sampling efficiency. Building on this foundation, we proposed RepLoRA, an effective and efficient PEFT approach. To evaluate RepLoRA, we conducted extensive experiments across four domains: image, video, text, and multimodal tasks. RepLoRA substantially outperformed LoRA and other PEFT methods in all settings, demonstrating its adaptability and effectiveness. These results highlight the potential of reparameterized structures in enhancing efficiency and effectiveness for fine-tuning large-scale models.

\newpage
\appendix

\input{sections/appendix}

\clearpage

\bibliography{relora}
\bibliographystyle{abbrv}
\end{document}




