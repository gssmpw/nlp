
This section briefly reviews the background for multi-head self-attention in transformers, low-rank adaptation, and a mixture of expert models.

\vspace{0.5 em}
\noindent
\textbf{Multi-head Self-attention.} We begin by revisiting the architecture of the multi-head self-attention (MSA) layer in Transformer~\cite {vaswani2017attention, dosovitskiy2020image}. Let $\Xbm = [\xbm_1, \dots, \xbm_N]^\top \in \mathbb{R}^{N \times d}$ denote an input sequence of embeddings, where $N$ is the sequence length and $d$ denotes the embedding dimension. The MSA layer processes this sequence as follows:
\begin{align}
    \mathrm{MSA}(\Xbm_Q, \Xbm_K, \Xbm_V) = \mathrm{Concat}(\hbm_1,...,\hbm_m) \Wbm^O, \label{eq:msa}
\end{align}
where we define each attention head by $\hbm_i = \mathrm{Attention}(\Xbm \Wbm_i^Q, \Xbm \Wbm_i^K, \Xbm \Wbm_i^V)$ for $i \in [m]$. 
Here, $\Xbm_Q = (\Xbm \Wbm_1^Q, \ldots, \Xbm \Wbm_{m}^{Q})$, $\mathbb{X}_K = (\Xbm \Wbm_1^K, \ldots, \Xbm \Wbm_m^K)$, and $\mathbb{X}_V = (\Xbm \Wbm_1^V, \ldots, \Xbm \Wbm_m^V)$ are the query, key, and value matrices, respectively. Furthermore, $m$ is the number of heads, and $\Wbm^O \in \RR^{m\dv \times d}$ is the output projection matrix. Each attention head $\hbm_i$ is parameterized by $\Wbm_i^Q \in \RR^{d \times \dk}, \Wbm_i^K \in \RR^{d \times \dk}, \text{ and } \Wbm_i^V \in \RR^{d \times \dv}$, with $\dk = \dv = \frac{d}{m}$. 

\vspace{0.5 em}
\noindent
\textbf{Low-Rank Adaptation.} LoRA~\cite{lora} has emerged as an efficient method for adapting large pre-trained transformer models to downstream tasks. Building upon the hypothesis that the updates during fine-tuning exhibit a low ``intrinsic rank'', LoRA proposes to fine-tune the transformer architectures' linear layers by incrementally updating the pre-trained weights with the product of two low-rank matrices. For a given pre-trained weight matrix $\Wbm_0 \in \mathbb{R}^{m \times n}$, LoRA represents its update as $\Delta \Wbm = \Bbm \Abm$, where $\Bbm \in \mathbb{R}^{m \times r}$, and $\Abm \in \mathbb{R}^{r \times n}$ with $r \ll \min\{m, n\}$. Consequently, the output of the fine-tuned model is:
\begin{align}
    \hat{\ybm} = \Wbm'\xbm = \Wbm_0 \xbm + \Bbm \Abm \xbm.
\end{align}
During training, $\Wbm_0$ remains fixed, while $\Abm$ and $\Bbm$ are updated. Typically, LoRA is used to adjust the linear layers that generate transformer models' queries and values (or keys, queries, and values). In line with prior work \cite{lora, dora, vpetl}, here we fine-tune the query and value projection matrices, leading to the following output expression:
\begin{align}
    f_{\mathrm{LoRA}}(\Xbm; \Abm, \Bbm) = \mathrm{Concat}(\widetilde{\hbm}_1, \cdots, \widetilde{\hbm}_m)\Wbm^O, \label{eq:LORA_MSA}
\end{align}
where for each $i \in [m]$, $\widetilde{\hbm}_i = \mathrm{Attention}(\Xbm \Wbm_i^Q + \Xbm \Bbm_{Q,i}\Abm_{Q,i} , \Xbm \Wbm_i^K, \Xbm \Wbm_i^V + \Xbm \Bbm_{V,i}\Abm_{V,i} )$. Here, we denote $\Abm = [\Abm_Q, \Abm_V]$, and $\Bbm = [\Bbm_Q, \Bbm_V]$, where $\Abm_Q = (\Abm_{Q,1}, \ldots, \Abm_{Q,m})$, and $\Abm_V = (\Abm_{V,1}, \ldots, \Abm_{V,m})$. Likewise, $\Bbm_Q = (\Bbm_{Q,1}, \ldots, \Bbm_{Q,m})$, and $\Bbm_V = (\Bbm_{V,1}, \ldots, \Bbm_{V,m})$. For each head $i \in [m]$, the dimensions of these matrices are $\Abm_{Q,i} \in \mathbb{R}^{r \times d_{k}}$, $\Bbm_{Q, i} \in \mathbb{R}^{d \times r}$, $\Abm_{V, i} \in \mathbb{R}^{r \times d_{v}}$, and $\Bbm_{V, i} \in \mathbb{R}^{d \times r}$. 

\vspace{0.5 em}
\noindent
\textbf{Mixture of Experts.} A mixture of experts (MoE)~\cite{Jacob_Jordan-1991, jordan1994hierarchical} model consists of $N$ expert networks, $f_i: \mathbb{R}^d \to \mathbb{R}^{d_v}$ for $i \in [N]$, and a gating function $G: \mathbb{R}^d \to \mathbb{R}^N$ that allocates contributions of each expert based on the input $\xbm$ to the model. The output of the MoE model is given by:
\begin{align*}
    \hat{\ybm} = \sum^N_{i=1} G(\xbm)_i \cdot f_i(\xbm),
\end{align*}
where $G(\xbm) = \softmax(s_1(\xbm),\cdots, s_N(\xbm))$, and $s_i: \mathbb{R}^d \to \mathbb{R}$ is a score function. In the subsequent sections, we discuss how MoE relates to LoRA and provide a theoretical analysis of our method.


\section{LoRA from the perspective of MoE}
\label{sec:MoE-LoRA}

Prior work~\cite{moeprompt, prefixmoe} has shown that each attention head in the MSA layer can be viewed as a specialized architecture of multiple MoE models. Specifically, from Eq.~(\ref{eq:msa}), consider the output of the $l$-th head $\hbm_l = [\hbm_{l, 1}, \dots, \hbm_{l, N}]^\top \in \RR^{N \times d_v}$. Let $\mathbb{X} = \left[\xbm_1^\top,\dots,\xbm_N^\top\right]^\top \in \mathbb{R}^{Nd}$ denote the concatenated input embeddings. We then define $N$ experts $f_j: \RR^{Nd} \rightarrow \RR^{d_v}$ encoded within the MSA layer as:
\begin{align}
    f_j(\mathbb{X}) = {\Wbm_l^V}^\top \Ebm_{j} \mathbb{X} = {\Wbm_l^V}^\top \xbm_j,
\end{align}
for $j \in [N]$, where the matrix $\Ebm_{j} \in \mathbb{R}^{d \times Nd}$ is such that $\Ebm_{j} \mathbb{X} = \xbm_{j}$. Next, we introduce $N \times N$ score functions $s_{i, j}: \RR^{Nd} \rightarrow \RR$ associated with these experts:
\begin{align}
    s_{i,j}(\mathbb{X}) 
    = \frac{\mathbb{X}^\top \Ebm_{i}^{\top} \Wbm_l^Q {\Wbm_l^K}^\top \Ebm_{j} \mathbb{X}}{\sqrt{d_{v}}}
    = \frac{\xbm_{i}^{\top} \Wbm_l^Q {\Wbm_l^K}^\top \xbm_{j}}{\sqrt{d_{v}}},
\end{align}
for $i \in [N]$ and $j \in [N]$. Then, we can formulate each output vector $\hbm_{l, i}$ as the output of an MoE model, using the experts and score functions above:
\begin{align}
\hbm_{l, i}
= \sum_{j = 1}^N  
    \frac{\exp(s_{i, j}(\mathbb{X}))}
    {
        \sum_{k = 1}^N \exp(s_{i, k}(\mathbb{X}))) 
    } \cdot f_j(\mathbb{X}).
\end{align}
Within a pre-trained MSA layer, all parameters of these experts and score functions $\Wbm_l^Q$, $\Wbm_l^K$, and $\Wbm_l^V$ remain fixed. When LoRA is applied, it refines these experts and score functions with updates:
\begin{align}
    &\tilde{f}_j(\mathbb{X}) = (\Wbm_l^V + \Bbm_{V,l}\Abm_{V,l})^\top \Ebm_{j} \mathbb{X}, \\
    &\tilde{s}_{i,j}(\mathbb{X}) = \frac{\mathbb{X}^\top \Ebm_{i}^{\top} (\Wbm_l^Q + \Bbm_{Q,l}\Abm_{Q,l}) {\Wbm_l^K}^\top \Ebm_{j} \mathbb{X}}{\sqrt{d_{v}}},
\end{align}
for $i \in [N]$ and $j \in [N]$. From these definitions and Eq.~(\ref{eq:LORA_MSA}), the modified output of the $l$-th head $\tilde{\hbm}_l = [\tilde{\hbm}_{l, 1}, \dots, \tilde{\hbm}_{l, N}]^\top \in \RR^{N \times d_v}$ can be expressed as:
\begin{align}
\tilde{\hbm}_{l, i}
= \sum_{j = 1}^N  
    \frac{\exp(\tilde{s}_{i, j}(\mathbb{X}))}
    {
        \sum_{k = 1}^N \exp(\tilde{s}_{i, k}(\mathbb{X}))) 
    } \cdot \tilde{f}_j(\mathbb{X}). \label{eq:LORA_MoE}
\end{align}
From this perspective, LoRA effectively fine-tunes the pre-trained MoE models contained within each MSA head by incorporating low-rank modifications to both the expert and the score functions. Next section will leverage this MoE viewpoint to analyze the theoretical properties of LoRA.


% Specifically, from Eq.~(\ref{eq:LORA_MSA}), consider the output of the $l$-th head $\tilde{\hbm}_l = [\tilde{\hbm}_{l, 1}, \dots, \tilde{\hbm}_{l, N}]^\top \in \RR^{N \times d_v}$. Let $\mathbb{X} = \left[\xbm_1^\top,\dots,\xbm_N^\top\right]^\top \in \mathbb{R}^{Nd}$ represent the concatenated input embeddings. We define $N$ experts $f_j: \RR^{Nd} \rightarrow \RR^{d_v}$ encoded in the MSA layer of LoRA as follows:
% \begin{align}
%     f_j(\mathbb{X}) = (\Wbm_l^V + \Bbm_{V,l}\Abm_{V,l})^\top \Ebm_{j} \mathbb{X} = {\Wbm_l^V}^\top \xbm_j \nonumber
% \end{align}
% for $j \in [N]$ and $j' \in [L]$, where the matrix $\Ebm_{j} \in \mathbb{R}^{d \times Nd}$ is such that $\Ebm_{j} \mathbb{X} = \xbm_{j}$. Next, we introduce $N \times N$ score functions, $s_{i, j}: \RR^{Nd} \rightarrow \RR$, associated with these experts:
% \begin{align}
%     s_{i,j}(\mathbb{X}) &= \frac{\mathbb{X}^\top \Ebm_{i}^{\top} (\Wbm_l^Q + \Bbm_{Q,l}\Abm_{Q,l}) {\Wbm_l^K}^\top \Ebm_{j} \mathbb{X}}{\sqrt{d_{v}}}, \nonumber
% \end{align}
% for $i \in [N]$, $j \in [N]$ and $j' \in [L]$. Consequently, each output vector $\tilde{\hbm}_{l, i}$ can be formulated as the result of an MoE model, utilizing the experts and score functions defined above:
% % \begin{align}
% % \tilde{\hbm}_{l, i}
% % &= \frac{\sum_{j = 1}^N  
% %     \exp(s_{i, j}(\mathbb{X})) f_{j}(\mathbb{X}) + \sum_{j' = 1}^L  
% % \exp(s_{i, N + j'}(\mathbb{X})f_{N + j'}(\mathbb{X})}
% %     {
% %         \sum_{k = 1}^N \exp(s_{i, k}(\mathbb{X})) + \sum_{k' = 1}^L \exp(s_{i, N + k'}(\mathbb{X}))
% %     }. \label{eq:prefix_MoE}
% % \end{align}
% \begin{align}
% \tilde{\hbm}_{l, i}
% = \sum_{j = 1}^N  
%     \frac{\exp(s_{i, j}(\mathbb{X}))}
%     {
%         \sum_{k = 1}^N \exp(s_{i, k}(\mathbb{X}))) 
%     } \cdot f_j(\mathbb{X}). \label{eq:LORA_MoE}
% \end{align}

