\begin{figure}
    % \vskip 0.2in
    \centering
    \includegraphics[width=0.75\linewidth]{figs/improvelora-cropped.pdf}
    \caption{Performance improvements over LoRA. RepLoRA outperforms LoRA across all domains, with non-linear reparameterization substantially surpassing its linear counterpart.}
    \label{fig: imrpove lora}
    %\vskip -0.2in
    %\vspace{-5mm}
    \vspace*{-\baselineskip}
\end{figure}

\input{tables/table_commonsense}

\vspace{0.5 em}
\noindent
\textbf{Settings. } We extensively experiment across multiple domains to demonstrate the versatility and effectiveness of RepLoRA on a wide range of tasks. Our evaluation spans four distinct settings: language (commonsense reasoning), image (classification), video (video action recognition), and multi-modal (image/video-text understanding). To provide a comprehensive evaluation, we compare RepLoRA against several PEFT methods, such as \textit{Full Fine-tuning}, \textit{Prefix Tuning} \cite{prefix}, \textit{LoRA} \cite{lora}, and \textit{Series Adapter} \cite{houlsby2019parameter}. As summarized in Figure \ref{fig: imrpove lora}, RepLoRA consistently outperforms LoRA across all settings, highlighting its robust adaptability and superior performance across diverse tasks.


\vspace{0.5 em}
\noindent
\textbf{Metrics. } In Parameter-Efficient Fine-Tuning (PEFT), evaluations typically focus on performance and the number of trainable parameters. The goal is to maximize performance while minimizing the parameters required. To assess this trade-off, in addition to reporting performance, we adopt the \textbf{Performance-Parameter Trade-off (PPT)} metric, proposed by \cite{vpetl}. Specifically, for a PETL algorithm $M$, the PPT metric incorporates its task performance $M_t$, the number of trainable parameters $P_M$, and a normalization constant $C$. Formally, we have:

\begin{equation*}
    \mathrm{PPT}_M = M_t \times \exp(-\log_{10}(\frac{P_M}{C}+1)).
    \vspace{-2mm}
\end{equation*}

%, which integrates both the performance and the number of trainable parameters. We refer to Appendix \ref{appendix: ppt} for more details about the PPT score.

% \begin{equation*} \mathrm{PPT}_M = M_t \times \exp(\log(\frac{P_M}{C}+1)) \end{equation*}

% Here, $C$ is set to $10^7$ to ensure a positive logarithm, as most algorithms have trainable parameters within this range. 

\vspace{0.5 em}
\noindent
\textbf{Commonsense Reasoning. } In our first experiment, we compare the performance of RepLoRA against LoRA and other PEFT methods using \texttt{LLaMA-7B/13B} \cite{llama} on the Commonsense Reasoning task. We also include the accuracy of ChatGPT, measured with the GPT-3.5-turbo API with a zero-shot Chain of Thought approach \cite{chatgpt}. The commonsense reasoning benchmark consists of eight sub-tasks with predefined training and testing datasets. Following the settings outlined in \cite{commonsensesettings}, we combine the training datasets from all eight sub-tasks into a single training dataset and evaluate performance on the individual testing datasets for each task. To ensure a fair comparison, we fine-tuned the models with RepLoRA using the same configuration as LoRA, keeping the rank fixed. As presented in Table \ref{table: commonsense}, RepLoRA achieves significantly better results than LoRA across all settings, delivering substantial improvements not only in accuracy but also in the PPT score, emphasizing its parameter efficiency.

\vspace{0.5 em}
\noindent
\textbf{Image Classification.} We extend our evaluation of RepLoRA to the image domain and fine-tune the ViT-B/16 architecture \cite{dosovitskiy2020image}, pre-trained on the \texttt{ImageNet-21K} dataset \cite{imagenet}, on two challenging benchmarks: the \texttt{VTAB-1K} dataset suite \cite{vtab} and the \texttt{FGVC} dataset collection \cite{jia2022visual}.

The \texttt{VTAB-1K} benchmark is a diverse suite of 19 datasets spanning various domains designed to test image classification and prediction capabilities. These datasets cover a wide range of tasks involving distinct semantics and object categories, organized into Natural, Specialized, and Structured domains. Each dataset includes 1,000 training examples, with an official 80/20 train-validation split, making it a rigorous test for generalization across different domains. On the other hand, the Fine-Grained Visual Classification (\texttt{FGVC}) suite, which consists of five datasets tailored for fine-grained recognition, focuses on tasks requiring subtle visual discrimination between closely related categories within specific domains. These datasets challenge models to identify nuanced differences, robustly evaluating RepLoRA’s capabilities in fine-grained classification.
\input{tables/table_fgvc}

\input{tables/table_vtab}

The results, summarized in Table \ref{table: vtab} and Table \ref{table: fgvc}, highlight the superior performance of RepLoRA across most settings. On average, RepLoRA achieves a notable improvement of over 3\% compared to LoRA, with notable gains exceeding 6\% on datasets like \texttt{dSprites-location}. Similarly, RepLoRA outperforms all baselines on the \texttt{FGVC} datasets, with the sole exception of Prefix Tuning on the \texttt{Stanford Cars} dataset. The performance gap with LoRA is particularly significant, which was $>6\%$ on average. On \texttt{Stanford Cars}, the improvement reaches a remarkable 10\%. RepLoRA remains highly parameter-efficient despite these substantial gains, as reflected in its PPT scores.


\input{tables/table_video}

\vspace{0.5 em}
\noindent
\textbf{Video Action Recognition. } Given RepLoRA's strong performance in the image domain, we expand our experiments to the video domain. We evaluate our method against baseline approaches using the Video Swin Transformer on two datasets: \texttt{SSv2} \cite{ssv2}, which offers a rich dataset with abundant data, and \texttt{HMDB51} \cite{hmdb51}, which presents a more challenging scenario with limited data and fewer categories. Despite the contrasting characteristics of these datasets, Table \ref{table: video} indicates that RepLoRA remarkably outperforms all baselines while maintaining parameter efficiency, underscoring its adaptability and robustness across diverse data settings.

\vspace{0.5 em}
\noindent
\textbf{Image/Video-Text understanding. } Having demonstrated that RepLoRA outperforms the baselines on the language and vision tasks, we attempt to see if RepLoRA remains competitive on multi-modality tasks. This experiment compares RepLoRA with LoRA and fully fine-tune (FT) on the \texttt{VL-BART} \cite{bart}. The experiments were conducted on four image-text tasks: $\texttt{VQA}^{\texttt{v}^2}$ \cite{vqa}, \texttt{GQA} \cite{gqa} for vision question-answering, \texttt{NLVR$^2$} \cite{nlvr} for visual reasoning, \texttt{MSCOCO} \cite{mscoco} for image captioning, and four video-text tasks from the \texttt{VALUE} benchmark \cite{value}: \texttt{TVQA} \cite{tvqa} and \texttt{How2QA} \cite{how2qa} for video question answering, \texttt{TVC} \cite{tvc} and \texttt{YC2C} \cite{yc2c} for video captioning. We follow \cite{sungmultimodal} and adopt the same setup of LoRA when applying RepLoRA. It is evident that RepLoRA consistently surpasses both FT and LoRA in accuracy and PPT in both Tables \ref{table: multimodal-image} and Table \ref{table: multimodal-video}. In particular, RepLoRA exceeds LoRA’s performance by nearly 2\% in image-text understanding tasks and roughly 4\% in video-text understanding tasks, reaching the performance of FT.

%\input{tables/table_multimodal_image}
\input{tables/table_multimodal_all}
%\input{tables/table_multimodal_video}

\vspace{0.5 em}
\noindent
\textbf{Enhancing Sampling Efficiency.  } Our theoretical analysis has demonstrated that reparameterizing LoRA achieves superior rates of sample efficiency compared to vanilla LoRA. To empirically validate this claim, we evaluate the sample efficiency of RepLoRA on \texttt{FGVC} datasets. Following the approach of \cite{d2021convit}, we subsample each class at fractions $f = \{1\%, 10\%, 30\%, 50\%, 100\%\}$ and scale the number of training epochs by $1/f$, ensuring the total number of data seen by the model remains constant. Figure \ref{fig: sample efficiency} shows that RepLoRA consistently outperforms LoRA across all sampling fractions. The improvements are significant at smaller fractions, with RepLoRA achieving a remarkable $40.4\%$ gap at $f = 1\%$. Moreover, we emphasize that RepLoRA matches LoRA’s performance with only $30\%$ training fraction, therefore underscoring RepLoRA’s superior sample efficiency, as predicted by our theoretical analysis. We refer to Appendix \ref{appendix: sample efficiency} for a breakdown of these results.

\vspace{0.5 em}
\noindent
\textbf{Linear vs. Non-linear Reparameterization. } Another conclusion from the theoretical analysis is that even a simple linear reparameterization with a shared structure offers significant efficiency gains compared to vanilla LoRA. Furthermore, as shown in Theorems \ref{theorem:param_rate_linear} and \ref{theorem:param_rate_nonlinear}, incorporating non-linear reparameterization further improves the rate of low-rank matrix estimation. To validate this hypothesis, we conducted empirical experiments, with the results presented in Figure \ref{fig: imrpove lora}. These results demonstrate that non-linear reparameterization outperforms the linear setting by substantial margins, underscoring its effectiveness. For a more detailed comparison of linear versus non-linear reparameterization, please refer to Appendix \ref{appendix: linear vs non-linear}.