\begin{figure}
    \centering
    \includegraphics[width=0.65\linewidth]{figs/replora.drawio-cropped.pdf}
    \caption{Overview of our proposed method RepLoRA, which reparameterizes the low-rank matrices as the output of a lightweight MLP, whose inputs are two diagonal matrices.}
    \label{fig: overview}
    \vspace{-5mm}
\end{figure}


With the rapid growth in data availability and computational resources, large-scale models trained on extensive datasets have demonstrated remarkable generalization capabilities, enabling successful applications across language, vision, and multi-modal tasks~\cite{dosovitskiy2020image, radford2021learning,llama}. However, fully fine-tuning such models for specific downstream tasks can be prohibitively expensive. To address this challenge, several parameter-efficient fine-tuning (PEFT) methods \cite{ houlsby2019parameter, lester2021powerscaleparameterefficientprompt, jia2022visual} have emerged, facilitating effective adaptation of large pre-trained models by adjusting a minimal set of parameters while keeping most of the backbone frozen. Among these methods, \textit{Low-Rank Adaptation} (LoRA)~\cite{lora} stands out for its simplicity and effectiveness and has been successfully applied across diverse domains \cite{li2022blipbootstrappinglanguageimagepretraining, qin2023chatgptgeneralpurposenaturallanguage, alpaca,  liu2023visualinstructiontuning}. Despite its successes, theoretical understanding of LoRA has remained limited, hindering our ability to optimize its performance further.

Building on the recent finding \cite{moeprompt} about the connection between attention mechanism~\cite{vaswani2017attention} and the mixture of experts (MoE)~\cite{Jacob_Jordan-1991, jordan1994hierarchical} models, we present a rigorous theoretical study demonstrating how LoRA can be interpreted within this new framework. Leveraging this perspective, we show that a straightforward reparameterization technique~\cite{prefix, prefixmoe}, which represents low-rank matrices as the output of an MLP, can theoretically enhance the performance of LoRA. Specifically, our analysis reveals that this reparameterization can reduce the data needed to achieve a desired estimation error from an exponential scale to a polynomial scale, thereby substantially improving sample efficiency. Based on these insights, we introduce \textit{\textbf{R}eparameterized \textbf{Lo}w-\textbf{R}ank \textbf{A}daptation} (RepLoRA). This novel PEFT method reparameterizes low-rank matrices through a lightweight MLP.


We conducted extensive experiments across multiple domains, including image, video, language, and multi-modal tasks. Our results indicate that RepLoRA consistently demonstrates better performance than vanilla LoRA. When only a tiny fraction of the training data is subsampled, RepLoRA improves up to \textbf{40\%} over LoRA. This highlights the robustness and effectiveness of our method, both theoretically and empirically. Moreover, the MLP used for reparameterization can be discarded after training, ensuring that our approach remains as efficient as the standard counterparts at inference time.

\vspace{0.5 em}
\noindent
\textbf{Contributions.} In summary, our contributions are: 
\textbf{(i)} We provide a rigorous theoretical analysis of LoRA from the perspective of a mixture of experts. 
\textbf{(ii)} Our results show that reparameterization can substantially improve sample efficiency, transitioning from an exponential rate to a polynomial rate. 
\textbf{(iii)} Building on these theoretical insights, we introduce RepLoRA, a novel PEFT approach that integrates reparameterization into LoRA. 
\textbf{(iv)} Extensive experiments across diverse domains demonstrate that RepLoRA consistently outperforms vanilla LoRA by a significant margin, thereby underscoring its effectiveness and robustness from theoretical and empirical perspectives.



\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{figs/fgvc_gap-cropped.pdf}
    \caption{Sample Efficiency on FGVC Datasets. RepLoRA not only outperforms LoRA consistently but also achieves LoRA performance on a full dataset with only $f = 30\%$ training fraction.}
    \label{fig: sample efficiency}
    \vspace{-6mm}
\end{figure}
\vspace{0.5 em}
\noindent
\textbf{Organizations.} The paper is organized as follows: Section \ref{section: preliminaries} provides the background on LoRA and MoE. Section~\ref{sec:MoE-LoRA} establishes the connection between LoRA and MoE. Section \ref{section: theory}  presents our theoretical analysis, including the statistical benefits of reparameterizing LoRA. Building on these insights, Section \ref{section: practice} presents our method, RepLoRA. To demonstrate the effectiveness of RepLoRA, Section \ref{section: experiments} presents the experimental results. Finally, Section~\ref{section: conclusion} concludes the paper.


% Notation should be put here as the whole paper will reuse these notations
\vspace{0.5 em}
\noindent
\textbf{Notation.} We denote $[n] := \{1,2,\ldots,n\}$ for any $n \geq 1$. Furthermore, the notation $|S|$ denotes the cardinality of a set $S$. Given a vector $u:=(u_1,u_2,\ldots,u_d) \in \mathbb{R}^{d}$ and a coefficient $\alpha:=(\alpha_1,\alpha_2,\ldots,\alpha_d)\in\mathbb{N}^d$, we have the following notations $u^{\alpha} :=u_{1}^{\alpha_{1}}u_{2}^{\alpha_{2}}\ldots u_{d}^{\alpha_{d}}$, $|u|:=u_1+u_2+\ldots+u_d$ and $\alpha!:=\alpha_{1}!\alpha_{2}!\ldots \alpha_{d}!$. Additionally, $\|u\|$ stands for the Euclidean norm of vector $u$. For positive sequences $(a_n)_{n\geq 1}$ and $(b_n)_{n\geq 1}$, we write $a_n = \mathcal{O}(b_n)$ or $a_{n} \lesssim b_{n}$ if there exists a constant $C > 0$ such that $a_n \leq C b_n$ for all $ n\in\mathbb{N}$. The notation $a_{n} = \mathcal{O}_{P}(b_{n})$ indicates $a_{n}/b_{n}$ is stochastically bounded.



% With the rapid growth in data availability and computational power, large models pre-trained on extensive datasets have demonstrated remarkable generalization abilities and have notably benefited a wide range of applications, from language tasks and vision tasks to multi-modal tasks. To utilize the power of these general models for specific downstream tasks, several parameter-efficient fine-tuning (PEFT) methods \cite{peft} have been introduced to fine-tune the large models with only a few parameters. Among the PEFT methods, \textit{Low-Rank Adaptation} (LoRA) has become popular for its effectiveness and simplicity. Indeed, LoRA has demonstrated many applications in various domains \cite{qin2023chatgptgeneralpurposenaturallanguage, alpaca, li2022blipbootstrappinglanguageimagepretraining, liu2023visualinstructiontuning}. The key insight of LoRA is that the perturbation matrix can be approximated with a low-rank matrix. However, theoretical analysis of the properties of LoRA, such as the rate of low-rank matrices estimation, has remained limited.


% On the other hand, to ensure stability during fine-tuning, the \textit{reparameterization} technique emerges as an engineering trick in \textit{Prefix-tuning} \cite{prefix} and \textit{Adapter} \cite{oplora}. Specifically, in prefix tuning, the prefix vectors are reparameterized as the output of a deep network rather than being directly optimized, and only the prefix vectors are retained for inference. Recently, \cite{moeprompt} and \cite{prefixmoe} has shown that the reparameterization technique is not merely an engineering trick by providing a theoretical justification by viewing prompt tuning as a Mixture of Experts (MoE) and claims that the reparameterization strategy can theoretically enhance the sample efficiency in prompt estimation. 

% Draw upon prior works, this paper provides a theoretical analysis of LoRA from the perspective of MoE. In particular, we demonstrate that original LoRA without reparameterization can establish a substantially slow estimation rate of low-rank matrices. Moreover, we theoretically demonstrate that non-linear, shared structures optimally improves the low-rank matrices estimation rate to the polynomial rate. Grounded on this theoretical analysis, we propose \textit{\textbf{R}eparameterized \textbf{Lo}w-\textbf{R}ank \textbf{A}daptation} (RepLoRA) - a novel PEFT method which reparameterizes the low-rank matrices with shared structures. To emphasize the practical effectiveness of RepLoRA, we compare and contrast our method with several baselines, including LoRA, on four domains: image, video, language, and multimodal (Image/Video-text) tasks. 

% \textbf{Contribution.} In summary, our contributions are: 
% (i) We present a theoretical analysis demonstrating the statistical benefits of reparameterizing LoRA. From the perspective of a Mixture of Experts, reparameterizing the low-rank matrices with non-linear, shared structures gives a superior, optimal polynomial estimation rate, while the original LoRA without parameterization may demonstrate a substantially slow estimation rate. To our knowledge, this is the first theoretical analysis of the low-rank estimation rate of LoRA. (ii) Grounded on this theoretical analysis, we propose our novel PEFT method \textit{\textbf{Rep}arameterized \textbf{Lo}w-\textbf{R}ank \textbf{A}daptation (RepLoRA)}. We empirically evaluate RepLoRA on different tasks on a wide range of domains, including language (commonsense reasoning), video (video-action recognition), image (image classification), and multi-modal domains (image/video-text understanding). Our method consistently outperforms all baselines by significant margins in all domains. 



% \textbf{Organization} The paper will be organized as follows: Section \ref{section: preliminaries} presents the necessary background on LoRA and MoE. Section \ref{section: theory} focuses on deriving the theoretical properties of LoRA, as well as the statistical benefits of reparameterization in LoRA. Based on this theoretical analysis, Section \ref{section: practice} presents the implementation of our PEFT method RepLoRA. To demonstrate the efficacy of RepLoRA, Section \ref{section: experiments} is dedicated for the experimental results. 









