% \textbf{Motivations.} The reparameterization technique is an engineering trick widely adopted in prefix tuning and prompt tuning to enhance stability and convergence rate. \cite{moeprompt} recently shows that this technique is not merely an engineering trick. Still, its statistical benefits can be established by viewing the process of prefix tuning as introducing additional experts to the MoE. In the previous section, we established the connection between LoRA and MoE. Motivated by \cite{moeprompt} and utilizing the connections of LoRA to MoE as discussed in the previous section, this section presents a theoretical analysis of the low-rank matrices estimation rate of LoRA, as well as the statistical advantages of reparameterizing LoRA. To our knowledge, despite the popularity of LoRA in various domains for its simplicity and effectiveness, such a theoretical analysis remains limited. 

In the previous section, we demonstrated that vanilla LoRA without reparameterization establishes a suboptimal rate for low-rank matrix estimation while introducing shared structural reparameterization to achieve the optimal rate. Building on this theoretical insight, we introduce our method: \textit{\textbf{Rep}arameterized \textbf{Lo}w-\textbf{R}ank \textbf{A}daptation (RepLoRA)}. This method is tailored explicitly for fine-tuning transformer architectures by refining the linear layers that generate queries and values (or keys, queries, and values). This paper focuses on fine-tuning the queries and values for simplicity and clarity. Recall in vanilla LoRA, the matrices that generate the queries and values are given as:
\begin{align} 
\Wbm'_Q = \Wbm_Q + \Bbm_Q \Abm_Q &&  \Wbm'_V = \Wbm_V + \Bbm_V \Abm_V,
\end{align}
where $\Bbm_Q, \Bbm_V \in \mathbb{R}^{m \times r}$ and $\Abm_Q, \Abm_V \in \mathbb{R}^{r \times n}$ are learnable low-rank matrices. Inspired by our theoretical results, RepLoRA innovatively reparameterizes $\Abm$ and $\Bbm$, modeling them as outputs of two MLPs. With non-linear reparameterization, the low-rank matrices are given by: 
\begin{align}
    [\Abm_Q, \Abm_V]  = g_{\theta_\Abm}({\Abm}) && [\Bbm_Q, \Bbm_V] = g_{\theta_\Bbm}({\Bbm}),
\end{align}
where $\Abm, \Bbm$ are learnable matrices, and $g_{\theta_\Abm}, g_{\theta_\Bbm}$ are two-layer MLPs with a shared part and distinct output heads. In this approach, $\Abm_Q$ and $\Abm_V$ are derived from a shared underlying input $\Abm$, with distinct outputs  $\Abm_Q$ and $\Abm_V$ produced by the separated heads of $g_{\vtheta_\Abm}$. Similarly, $\Bbm_Q$ and $\Bbm_V$ follow the same structure, leveraging a shared $\Bbm$ input. While we focus on fine-tuning the queries and values to streamline the analysis, this formulation can naturally be extended to fine-tune the keys as well. We implement $\Abm$ and $\Bbm$ as diagonal matrices to ensure parameter efficiency. After training, the reparameterization $g_{\vtheta_\Abm}$ and $g_{\vtheta_\Bbm}$ can be discarded, and only the fine-tuned matrices $\Abm_Q$, $\Abm_V$, $\Bbm_Q$, and $\Bbm_V$ need to be retained for inference. Hence, this approach does not incur any additional computational overhead for inference. An illustration of this method is provided in Figure \ref{fig: overview}. 


% Notice that we can interpret LoRA from another perspective by writing $\Abm = \bm{I}\Abm = m_\Abm(\bm{I})$ where $m_A$ is a linear layer without activation, and $I$ is the identity matrix. Motivated by this observation and the theoretical results, we used the shared network structure and shared input to generate these matrices. In particular, we have: 


% In the subsequent sections, we will establish that implementing $g_{\vtheta_\Abm}, g_{\vtheta_\Bbm}$ as a two-layer MLP is sufficient to yield substantial performance improvement. Specifically, we employed: 

% \begin{align*}
%     g_{\theta_\Abm}(\cdot) &= \Wbm_2^\Abm(\overline{\sigma}(\Wbm_1^\Abm(\cdot)+\bbm_1^\Abm))+\bbm_2^\Abm\\
%     g_{\theta_\Bbm}(\cdot) &= \Wbm_2^\Bbm(\overline{\sigma}(\Wbm_1^\Bbm(\cdot)+\bbm_1^\Bbm))+\bbm_2^\Bbm
% \end{align*}

% where $\Wbm_1^\Abm \in \mathbb{R}^{k \times m}, \Wbm_2^\Abm \in \mathbb{R}^{2r \times k}, \Wbm_1^\Bbm \in \mathbb{R}^{k \times n}, \Wbm_2^\Bbm \in \mathbb{R}^{2r \times k}$, in which $k$ is the hidden dimension, and $\overline{\sigma}$ is a non-linear activation function. 


%\paragraph{Connections to OP-LoRA \cite{oplora}.} 