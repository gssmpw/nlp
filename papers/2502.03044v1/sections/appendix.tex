%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix
\onecolumn

\begin{center}
{}\textbf{\Large{Supplement to
``RepLoRA: Reparameterizing Low-Rank Adaptation via the Perspective of Mixture of Experts''}}
\end{center}



In this supplementary material, we provide proofs of the main results in Appendix~\ref{sec:proofs}. Related works on parameter-efficient fine-tuning techniques, low-rank adaptation, and the mixture of experts are discussed in Appendix~\ref{sec:related_works}. Details of experiments are in Appendix~\ref{appendix: exp details} while additional experiments are in Appendix~\ref{sec:additional_experiements}.
\section{Proofs}
\label{sec:proofs}
This appendix provides proofs for key results in the main text.
\subsection{Proof of Theorem~\ref{theorem:param_rate_nopara}}
\label{appendix:param_rate_nopara}
The proof of Theorem~\ref{theorem:param_rate_nopara} consists of two key steps.

\vspace{0.5 em}
\noindent
\textbf{Step 1.} In this step, we demonstrate that the following limit holds for any $r\geq 1$:
\begin{align}
    \label{eq:ratio_zero_limit}
    \lim_{\varepsilon\to0}\inf_{G\in\mathcal{G}_{L'}(\Theta):\mathcal{D}_{1,r}(G,G_*)\leq\varepsilon}\frac{\normf{f_{G}-f_{G_*}}}{\mathcal{D}_{1,r}(G,G_*)}=0.
\end{align}
To prove the inequality above, it is sufficient to construct a sequence of mixing measures $(G_n)_{n \geq 1}$ satisfying 

\begin{align*}
    \lim_{n\to\infty} \mathcal{D}_{1,r}(G_n,G_*)=0 && \text{ and } && \lim_{n\to\infty} {\normf{f_{G_n}-f_{G_*}}}/{\mathcal{D}_{1,r}(G_n,G_*)} = 0.
\end{align*}

Indeed, we consider the following sequence  $G_n=\sum_{i=1}^{L+1}\exp(c^n_{i})\delta_{(\Bbm^n_{Q,i},\Abm^n_{Q,i},\Bbm^n_{V,i},\Abm^n_{V,i})}$, where 
\begin{itemize}
    \item $\exp(c^n_1)=\exp(c^n_2)=\frac{1}{2}\exp(c^*_1)+\frac{1}{2n^{r+1}}$ and  $\exp(c^n_i)=\exp(c^*_{i-1})$ for any $3\leq i\leq L + 1$;
    \item $\Bbm^n_{Q,1}=\Bbm^n_{Q,2}=\Bbm^*_{Q,1}$ and  $\Bbm^n_{Q,i}=\Bbm^*_{Q,i-1}$ for any $3\leq i\leq L+1$;
    %\item $a^n_1=a^n_2=a^*_1$ and $a^n_i=a^n_{i-1}$ for any $3\leq i\leq L+1$;
    \item $\Abm^n_{Q,1}=\Abm^n_{Q,2}=\Abm^*_{Q,1}$ and  $\Abm^n_{Q,i}=\Abm^*_{Q,i-1}$ for any $3\leq i\leq L+1$;
    \item $\Bbm^n_{V,1}=\Bbm^*_{V,1}+\frac{1}{n(\Abm^*_{V,1})^{(1)}}(1,0,\ldots,0)$, $\Bbm^n_{V,2}=\Bbm^*_{V,1}-\frac{1}{n(\Abm^*_{V,1})^{(1)}}(1,0,\ldots,0)$ and  $\Bbm^n_{V,i}=\Bbm^*_{V,i-1}$ for any $3\leq i\leq L+1$,
    \item $\Abm^n_{V,1}=\Abm^n_{V,2}=\Abm^*_{V,1}$ and  $\Abm^n_{V,i}=\Abm^*_{V,i-1}$ for any $3\leq i\leq L+1$;
\end{itemize}
in which without loss of generality, we assume that $(\Abm^*_{V,1})^{(1)}\neq0$. With this definition, the loss function $\mathcal{D}_{1,r}(G_n,G_*)$ can be computed as
\begin{align}
    \label{eq:D_r_formulation}
    \mathcal{D}_{1,r}(G_n,G_*)=\frac{1}{n^{r+1}}+\Big[\exp(c^*_{1})+\frac{1}{n^{r+1}}\Big]\cdot\frac{1}{n^r}=\mathcal{O}(n^{-r}).
\end{align}
Therefore, $\mathcal{D}_{1,r}(G_n,G_*)\to0$ as $n\to\infty$. 

\vspace{0.5em}
\noindent
We will show that $\lim_{n\to\infty}\normf{f_{G_n}-f_{G_*}}/\mathcal{D}_{1,r}(G_n,G_*)=0$. Consider the quantity $$Q_n(\mathbb{X}):=\Big[\sum_{k = 1}^{L} \exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\Bbm_{Q,k}^*\Abm_{Q,k}^*)\mathbb{X}+c^*_{k})\Big]\cdot[f_{G_n}(\mathbb{X})-f_{\bar{G}_*}(\mathbb{X})],$$ 
which can be decomposed as:
\begin{align*}
    Q_n(\mathbb{X})&=\sum_{j=1}^{L}\sum_{i\in\mathcal{V}_j}\exp(c^n_{i}) \Big[\exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\Bbm^n_{Q,i}\Abm^n_{Q,i})\mathbb{X}) (\Mbm^0_{V}+\Bbm^n_{V,i}
\Abm^n_{V,i})\mathbb{X} \\
    &\hspace{3cm}- \exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\Bbm_{Q,j}^*\Abm_{Q,j}^*)\mathbb{X}) (\Mbm^0_{V}+\Bbm_{V,j}^*
\Abm_{V,j}^*)\mathbb{X}\Big] \nonumber \\
    &-\sum_{j=1}^{L}\sum_{i\in\mathcal{V}_j}\exp(c^n_{i})\Big[\exp(\mathbb{X}^{\top}(\Mbm^0_{Q,i}+\Bbm^n_{Q,i}\Abm^n_{Q,i})\mathbb{X}) -\exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\Bbm_{Q,j}^*\Abm_{Q,j}^*)\mathbb{X})\Big]f_{G_n}(\mathbb{X}) \nonumber \\
    &+\sum_{j=1}^{L}\Big(\sum_{i\in\mathcal{V}_j}\exp(c^n_i)-\exp(c_{j}^{*})\Big)\exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\Bbm_{Q,j}^*\Abm_{Q,j}^*)\mathbb{X})\Big[(\Mbm^0_{V}+\Bbm_{V,j}^*
\Abm_{V,j}^*)\mathbb{X} -f_{G_n}(\mathbb{X})\Big] \nonumber \\
    &:=A_n(\mathbb{X})-B_n(\mathbb{X})+ C_n(\mathbb{X}). 
\end{align*}
It follows from the choices of $\Bbm^n_{Q,i},\Abm^n_{Q,i},\Bbm^n_{V,i},\Abm^n_{V,i}$ and $c^n_{i}$ that
\begin{align*}
    A_n(\mathbb{X})&=\sum_{i=1}^{2}\frac{1}{2}\Big[\exp(c^*_{1})+\frac{1}{n^{r+1}}\Big]\exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\Bbm_{Q,1}^*\Abm_{Q,1}^*)\mathbb{X})(\Bbm^n_{V,i}\Abm^n_{V,i}-\Bbm^*_{V,1}\Abm^*_{V,1})\mathbb{X}\\
    &=\frac{1}{2}\Big[\exp(b_{*,1})+\frac{1}{n^{r+1}}\Big]\exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\Bbm_{Q,1}^*\Abm_{Q,1}^*)\mathbb{X})[(\Bbm^n_{V,1}\Abm^n_{V,1}-\Bbm^*_{V,1}\Abm^*_{V,1})\\&+(\Bbm^n_{V,2}\Abm^n_{V,2}-\Bbm^*_{V,1}\Abm^*_{V,1})]\mathbb{X}\\
    &=0,
\end{align*}
where the last equality occurs as $\Bbm^n_{V,1}\Abm^n_{V,1}-\Bbm^*_{V,1}\Abm^*_{V,1}=\frac{1}{n}e_{11}$ and $\Bbm^n_{V,2}\Abm^n_{V,2}-\Bbm^*_{V,1}\Abm^*_{V,1}=-\frac{1}{n}e_{11}$ in which $e_{11}$ denotes the matrix of size $d\times d$ such that its $(1,1)$-th element is one while others are zero.

Moreover, we can also verify that $B_n(\mathbb{X})=0$, and $C_n(\mathbb{X})=\mathcal{O}(n^{-(r+1)})$. Therefore, for almost every $\mathbb{X}$, it can be deduced that $$\lim_{n\to\infty} Q_n(\mathbb{X})/\mathcal{D}_{1,r}(G_n,G_*)= 0.$$ 

\vspace{0.5em}
\noindent
Notice that the term $\Big[\sum_{k = 1}^{L} \exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\Bbm_{Q,k}^*\Abm_{Q,k}^*)\mathbb{X}+c^*_{k})\Big]$ is bounded. Thus, for almost every $\mathbb{X}$, we have 

\begin{equation*}
    \frac{f_{G_n}(\mathbb{X})-f_{G_*}(\mathbb{X})}{\mathcal{D}_{1,r}(G_n,G_*)}\to 0.
\end{equation*}

This limit suggests that $$\lim_{n\to\infty}\normf{f_{G_n}-f_{G_*}}/\mathcal{D}_{1,r}(G_n,G_*)=0.$$ Therefore, we obtain the claim in equation~(\ref{eq:ratio_zero_limit}).

\vspace{0.5 em}
\noindent
\textbf{Step 2.} Next, we will demonstrate that
 \begin{align}
        \label{eq:minimax_lower_bound}
        \inf_{\overline{G}_n\in\mathcal{G}_{L'}(\Theta)}\sup_{G\in\mathcal{G}_{L'}(\Theta)\setminus\mathcal{G}_{L-1}(\Theta)}\bbE_{f_{G}}[\mathcal{D}_{1,r}(\overline{G}_n,G)]\gtrsim n^{-1/2}.
\end{align}
From the hypothesis, $\epsilon_1, \epsilon_{2}, \ldots, \epsilon_{n}$ are i.i.d. from a Gaussian distribution. Therefore, we obtain that the conditional distribution of $Y_{i}|\mathbb{X}_{i}$ is also a Gaussian distribution, namely, we have $Y_{i}|\mathbb{X}_{i} \sim \mathcal{N}(f_{G_{*}}(\mathbb{X}_{i}), \sigma^2)$ for all $i \in [n]$. Furthermore, when $\varepsilon>0$ is sufficiently small, given a fixed constant $C_1>0$ (the specific value of $C_{1}$ will be chosen later) there exists $G'_* \in \mathcal{G}_{L'}(\Theta)$ such that the following results hold: $\mathcal{D}_{1,r}(G'_*, G_*)=2 \varepsilon$ and $\|f_{G'_*} - f_{G_*}\|_{L^2(\mu)} \leq C_1\varepsilon$, which is due to the result in equation~(\ref{eq:ratio_zero_limit}). An application of the Le Cam's lemma~\cite{yu97lecam} along with the weak triangle inequality property of the Voronoi loss function $\mathcal{D}_{1,r}$ leads to
\begin{align}
    \inf_{\overline{G}_n\in\mathcal{G}_{L'}(\Theta)}&\sup_{G\in\mathcal{G}_{L'}(\Theta)\setminus\mathcal{G}_{L-1}(\Theta)}\bbE_{f_{G}}[\mathcal{D}_{1,r}(\overline{G}_n,G)] \nonumber\\
    & \gtrsim \frac{\mathcal{D}_{1,r}(G'_*,G_*)}{8} \text{exp}(- n \mathbb{E}_{\mathbb{X} \sim \mu}[\text{KL}(\mathcal{N}(f_{G'_{*}}(\mathbb{X}), \sigma^2),\mathcal{N}(f_{G_{*}}(\mathbb{X}), \sigma^2))]) \nonumber \\
    & \gtrsim \varepsilon \cdot \text{exp}(-n \|f_{G'_*} - f_{G_*}\|_{L^2(\mu)}^2) \nonumber \\
    & \gtrsim \varepsilon \cdot \text{exp}(-C_{1} n \varepsilon^2). \label{eq:LeCam_inequality}
\end{align}
In these inequalities, we utilize the following well-known closed-form expression for the KL divergence between two Gaussian distributions:
\begin{align*}
    \text{KL}(\mathcal{N}(f_{G'_{*}}(\mathbb{X}), \sigma^2),\mathcal{N}(f_{G_{*}}(\mathbb{X}), \sigma^2)) = \dfrac{(f_{G'_*}(\mathbb{X}) - f_{G_*}(\mathbb{X}))^2}{2 \sigma^2}.
\end{align*}
By choosing $\varepsilon=n^{-1/2}$, we can verify that $\varepsilon \cdot \text{exp}(-C_{1} n \varepsilon^2)=n^{-1/2}\exp(-C_1)$. As a consequence, the minimax lower bound in equation~(\ref{eq:minimax_lower_bound}) is achieved and the proof is completed. %The Appendix includes related works on parameter-efficient fine-tuning techniques, low-rank adaptation, and a mixture of experts. 

\subsection{Proof for Theorem~\ref{theorem:param_rate_linear}}
\label{subsec:param_rate_linear}
We start with the following position, which establishes the convergence rate of the regression function estimation $f_{\bar{G}_{n}}$ to the true regression function $f_{\bar{G}_{*}}$:
\begin{proposition}
\label{prop:regression_estimation_linear}
     Given the least square estimator $\bar{G}_{n}$ in equation~(\ref{eq:least_squared_estimator_linear}), the convergence rate of the regression function estimation $f_{\bar{G}_n}(\cdot)$ to the true regression function $f_{\bar{G}_*}(\cdot)$ under the $L^2(\mu)$ norm is parametric on the sample size, that is,
    \begin{align}
        \label{eq:model_bound_2_linear}
        \normf{f_{\bar{G}_n}-f_{\bar{G}_*}}=\mathcal{O}_{P}(\sqrt{\log(n)/n}).
    \end{align}
\end{proposition}
Proof of Proposition~\ref{prop:regression_estimation_linear} is given in Appendix~\ref{appendix:regression_estimation_linear}. Given rate of $f_{\bar{G}_{n}}$ in Proposition~\ref{prop:regression_estimation_linear}, our goal is to demonstrate the following inequality:
\begin{align*}
\inf_{G\in \bar{\mathcal{G}}_{L'}(\Theta)} \normf{f_{G}-f_{\bar{G}_*}}/\mathcal{D}_2(G,G_*) >0.
\end{align*}

The proof of that inequality will consist of two parts:
\begin{itemize}
    \item Local part: $\lim_{\varepsilon\to0} \inf_{G\in\mathcal{G}_{L'}(\Theta): \mathcal{D}_2(G,\bar{G}_*)\leq \varepsilon} \normf{f_{G}-f_{\bar{G}_*}}/\mathcal{D}_2(G,\bar{G}_*) >0.$
    \item Global part: $\inf_{G\in \bar{\mathcal{G}}_{L'}(\Theta): \mathcal{D}_2(G,\bar{G}_*)\leq \varepsilon'} \normf{f_{G}-f_{\bar{G}_*}}/\mathcal{D}_2(G,\bar{G}_*) >0.$
\end{itemize}

\subsubsection{Local Part}
We first consider the proof for the local part. We will prove that
$$\lim_{\varepsilon\to0} \inf_{G\in\mathcal{G}_{L'}(\Theta): \mathcal{D}_2(G,\bar{G}_*)\leq \varepsilon} \normf{f_{G}-f_{\bar{G}_*}}/\mathcal{D}_2(G,\bar{G}_*) >0.$$
Assume, on the contrary, that the claim does not hold. It indicates that we can find a sequence of mixing measures $G_{n}:= \sum_{j' = 1}^{L'} \exp(c_{n,j'}) \delta_{\Bbm_{n,j'}\Abm_{n,j'}}$ in $\bar{\mathcal{G}}_{L'}(\Theta)$ such that 
$$\left\{\begin{matrix}
 \mathcal{D}_{2n}:=\mathcal{D}_2(G_n,\bar{G}_*) \to 0, \\
 \normf{f_{G_n}-f_{\bar{G}_*}}/\mathcal{D}_{2n} \to 0.
\end{matrix}\right.$$
as $n \to \infty$.  We denote $\mathcal{V}_j^n:= \mathcal{V}_j(G_n)$ as a Voronoi cell of $G_n$ generated by the $j$-th components of $\bar{G}_*$. Without loss of generality, we may assume that those Voronoi cells do not depend on the sample size, i.e., $\mathcal{V}_j = \mathcal{V}_j^n$. Therefore, the Voronoi loss $\mathcal{D}_{2n}$ can be rewritten as follows:
\begin{align*}
    \mathcal{D}_{2n}  : =\sum_{j'=1}^{L}\Big|\sum_{i\in\mathcal{V}_{j'}}\exp(c_{n,i})-\exp(c_{j'}^{*})\Big| &+\sum_{j'\in[L]:|\mathcal{V}_{j'}|=1}\sum_{i\in\mathcal{V}_{j'}}\exp(c_{n,i})\|\Delta \Wbm_{n,2ij'}\Bbm_{n,ij'}\Wbm_{n,1ij'}\Abm_{n,ij'}\|  \nonumber\\
    &+\sum_{j'\in[L]:|\mathcal{V}_{j'}|>1}\sum_{i\in\mathcal{V}_{j'}}\exp(c_{n,i})\|\Delta \Wbm_{n,2ij'}\Bbm_{n,ij'}\Wbm_{n,1ij'}\Abm_{n,ij'}\|^{2} ,
\end{align*}
where $\Wbm_{n,2ij'}\Delta \Bbm_{n,ij'}\Wbm_{n,1ij'}\Abm_{n,ij'} := \Wbm_{n,2j'}\Bbm_{n,j'}\Wbm_{n,1j'}\Abm_{n,j'} - \Wbm_{2,i}^{*}\Bbm_{i}^{*} \Wbm_{1,i}^{*}\Abm_{i}^{*}$ for all $i \in \mathcal{V}_{j'}$.

To simplify the ensuing presentation, throughout the proof we denote $\Zbm : = \Wbm_{2} \Bbm \Wbm_{1} \Abm$ for all the matrices $\Wbm_{1}$, $\Wbm_{2}$, $\Abm$, and $\Bbm$. Given the new notation, the Voronoi loss $\mathcal{D}_{2n}$ becomes
\begin{align*}
     \mathcal{D}_{2n}  =\sum_{j'=1}^{L}\Big|\sum_{i\in\mathcal{V}_{j'}}\exp(c_{n,i})-\exp(c_{j'}^{*})\Big| &+\sum_{j'\in[L]:|\mathcal{V}_{j'}|=1}\sum_{i\in\mathcal{V}_{j'}}\exp(c_{n,i})\|\Delta \Zbm_{n,ij'}\|  \nonumber\\
    &+\sum_{j'\in[L]:|\mathcal{V}_{j'}|>1}\sum_{i\in\mathcal{V}_{j'}}\exp(c_{n,i})\|\Delta \Zbm_{n,ij'}\|^{2}.
\end{align*}
Since $\mathcal{D}_{2n} \to 0$, we have $\sum_{i\in\mathcal{V}_{j}}\exp(c_{n,i})\to\exp(c_{*,j})$, $\Zbm_{n,i} \to \Zbm_{j}^{*}$ for any index $i$ in the Voronoi cell $\mathcal{V}_{j}$ and for any index $j \in [L]$. Throughout this proof, we assume without loss of generality that $M^{0}_{K,j}=I_{\bar{d}}$ with a note that our techniques can be extended to the general setting of that matrix.
Now, the proof of the local part is divided into three steps as follows:

\paragraph{Step 1 - Taylor expansion.} We denote a key function:
$$Q_n(\mathbb{X}):=\Big[\sum_{k = 1}^{L} \exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\Zbm_{k}^*)\mathbb{X}+c^*_{k})\Big]\cdot[f_{G_n}(\mathbb{X})-f_{\bar{G}_*}(\mathbb{X})].$$  Then, via simple algebra the function $Q_n(\mathbb{X})$ can be decomposed into three terms as follows:
\begin{align}
    Q_n(\mathbb{X})&=\sum_{j=1}^{L}\sum_{i\in\mathcal{V}_j}\exp(c_{n,i}) \Big[\exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\Zbm_{n,i}\mathbb{X}) (\Mbm^0_{V}+\Zbm_{n,i})\mathbb{X} - \exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\Zbm_{j}^*)\mathbb{X}) (\Mbm^0_{V}+\Zbm_{j}^*
)\mathbb{X}\Big] \nonumber \\
    &-\sum_{j=1}^{L}\sum_{i\in\mathcal{V}_j}\exp(c_{n,i})\Big[\exp(\mathbb{X}^{\top}(\Mbm^0_{Q,i}+\Zbm_{n,i})\mathbb{X}) -\exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\Zbm_{j}^*)\mathbb{X})\Big]f_{G_n}(\mathbb{X}) \nonumber \\
    &+\sum_{j=1}^{L}\Big(\sum_{i\in\mathcal{V}_j}\exp(c_{n,i})-\exp(c_{j}^{*})\Big)\exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\Zbm_{j}^*)\mathbb{X})\Big[(\Mbm^0_{V}+\Zbm_{j}^*)\mathbb{X} -f_{G_n}(\mathbb{X})\Big] \nonumber \\
    &:=\bar{A}_n(\mathbb{X})-\bar{B}_n(\mathbb{X})+ \bar{C}_n(\mathbb{X}). \label{eq:main_equation_linear}
\end{align}
\paragraph{Decomposition of the function $\bar{A}_n(\mathbb{X})$.} We denote $\bar{U}(\mathbb{X}; \Zbm) : = \exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\Zbm)\mathbb{X})$ and $\bar{V}(\mathbb{X};\Zbm) = (\Mbm^0_{V}+\Zbm)\mathbb{X}$, and $F(\mathbb{X};\Zbm)= \bar{U}(\mathbb{X}; \Zbm) \bar{V}(\mathbb{X};\Zbm)$. Our strategy is to decompose the function $\bar{A}_n(\mathbb{X})$ into two parts: the first part consists of Voronoi cells with only one element and the second part comprises of Voronoi cells with more than one element. In summary, the function $\bar{A}_n(\mathbb{X})$ has the following decomposition:
\begin{align*}
    \bar{A}_n(\mathbb{X}) &=\sum_{j:|\mathcal{V}_j|=1}\sum_{i\in\mathcal{V}_j}\exp(c_{n,i})\Big[F(\mathbb{X};\Zbm_{n,i})-F(\mathbb{X};\Zbm_{j}^{*})\Big]\\
    & \hspace{3 em} + \sum_{j:|\mathcal{V}_j|>1}\sum_{i\in\mathcal{V}_j}\exp(c_{n,i})\Big[F(\mathbb{X};\Zbm_{n,i} -F(\mathbb{X};\Zbm_{j}^{*})\Big]\\
    &:= \bar{A}_{n,1}(\mathbb{X}) + \bar{A}_{n,2}(\mathbb{X})
\end{align*}
We aim to utilize first-order Taylor expansion to the function $F$ in the formulation of the function $\bar{A}_{n,1}(\mathbb{X})$. It can be done by employing the first-order Taylor expansions to each function $\bar{U}$ and $\bar{V}$, which are given by: 
\begin{align*}
    \bar{U}(\mathbb{X};\Zbm_{n,i}) & = \bar{U}(\mathbb{X};\Zbm_{j}^{*}) + \sum_{|\alpha|=1} (\Delta \Zbm_{n,ij})^{\alpha} \dfrac{\partial^{|\alpha|}\bar{U}}{\partial{(\Zbm)^{\alpha}}}(\mathbb{X};\Zbm_{j}^{*}) + \bar{R}_{ij,1}(\mathbb{X}), \\
    \bar{V}(\mathbb{X};\Zbm_{n,i}) & = \bar{V}(\mathbb{X};\Zbm_{j}^{*}) + \sum_{|\alpha|=1} (\Delta \Zbm_{n,ij})^{\alpha} \dfrac{\partial^{|\alpha|}\bar{V}}{\partial{(\Zbm)^{\alpha}}}(\mathbb{X};\Zbm_{j}^{*}) + \bar{R}_{ij,2}(\mathbb{X}),
\end{align*}
for any $i$ and $j$ such that $i \in \mathcal{V}_{j}$ and $|\mathcal{V}_{j}| = 1$. Here, both the functions $\bar{R}_{ij,1}(\mathbb{X})$ and $\bar{R}_{ij, 2}(\mathbb{X})$ in the Taylor expansions of the function $\bar{U}$ and $\bar{V}$ denote the Taylor remainders. Combining all the above results together, we arrive at
\begin{align*}
    \bar{A}_{n,1}(\mathbb{X}) &= \sum_{j:|\mathcal{V}_j|=1}\sum_{i\in\mathcal{V}_j} \dfrac{\exp(c_{n,i})}{\alpha!} \sum_{|\alpha|=1} \biggr\{(\Delta \Zbm_{n,ij})^{\alpha}\dfrac{\partial^{|\alpha|}\bar{U}}{\partial {(\Zbm)^{\alpha}}}(\mathbb{X};\Zbm_{j}^{*}) \bar{V}(\mathbb{X};\Zbm_{j}^{*}) \\
    & + (\Delta \Zbm_{n,ij})^{\alpha}\dfrac{\partial^{|\alpha|}\bar{V}}{\partial {(\Zbm)^{\alpha}}}(\mathbb{X};\Zbm_{j}^{*}) \bar{U}(\mathbb{X};\Zbm_{j}^{*})\biggr\} + \bar{R}_{n,1}(\mathbb{X})\\
    &=\sum_{j:|\mathcal{V}_j|=1}\sum_{|\alpha|=1} \biggr\{ \bar{M}_{n,j,\alpha}\dfrac{\partial^{|\alpha|}\bar{U}}{\partial {( \Zbm)^{\alpha}}}(\mathbb{X};\Zbm_{j}^{*}) \bar{V}(\mathbb{X};\Zbm_{j}^{*}) \\
    & + \bar{M}_{n,j,\alpha}\dfrac{\partial^{|\alpha|}\bar{V}}{\partial {(\Zbm)^{\alpha}}}(\mathbb{X};\Zbm_{j}^{*}) \bar{U}(\mathbb{X};\Zbm_{j}^{*})\biggr\} + \bar{R}_{n,1}(\mathbb{X})
\end{align*}
where the function $\bar{R}_{n,1}(\mathbb{X})$ satisfies that $\bar{R}_{n,1}(\mathbb{X})/\mathcal{D}_{2n} \to 0$. It is due to the uniform Lipschitz property of the function $F$. In the formulation of the function $\bar{A}_{n,1}(\mathbb{X})$, the coefficients $\bar{M}_{n,j,\alpha}$ admit the following forms:
\begin{align*}
\bar{M}_{n,j,\alpha}=\sum_{i\in\mathcal{V}_j} \dfrac{\exp(c_{n,i})}{\alpha!} (\Delta \Zbm_{n,ij})^{\alpha}, 
\end{align*}
for any $|\alpha| = 1$.


Moving to the function $\bar{A}_{n,2}(\mathbb{X})$, we utilize the Taylor expansion to decompose that function. However, as each Voronoi cell in the formulation of the function $\bar{A}_{n,2}(\mathbb{X})$ has more than one element, we need to use second-order Taylor expansion to account for the convergence of all the elements in those Voronoi cells to the same limit point. Given that idea, we apply the second-order Taylor expansion and achieve that
\begin{align*}
\bar{A}_{n,2}(\mathbb{X}) & = \sum_{j:|\mathcal{V}_j|>1}\sum_{1\leq |\alpha|\leq 2} \biggr\{\bar{M}_{n,j,\alpha}\dfrac{\partial^{|\alpha|}\bar{U}}{\partial {(\Zbm)^{\alpha}}}(\mathbb{X};\Zbm_{j}^{*}) \bar{V}(\mathbb{X};\Zbm_{j}^{*}) \\
& + \bar{M}_{n,j,\alpha}\dfrac{\partial^{|\alpha|}\bar{V}}{\partial {(\Zbm)^{\alpha}}}(\mathbb{X};\Zbm_{j}^{*}) \bar{U}(\mathbb{X};\Zbm_{j}^{*}) \biggr\} \\
& + \sum_{|\alpha| = 1, |\beta| = 1} \bar{M}_{n,j,\alpha, \beta} \dfrac{\partial^{|\alpha|}\bar{U}}{\partial {(\Zbm)^{\alpha}}}(\mathbb{X};\Zbm_{j}^{*}) \dfrac{\partial^{|\beta|}\bar{V}}{\partial {(\Zbm)^{\beta}}}(\mathbb{X};\Zbm_{j}^{*})  + \bar{R}_{n,2}(\mathbb{X})
\end{align*}
where the remainder $\bar{R}_{n,2}(\mathbb{X})$ satisfies that $\bar{R}_{n,2}(\mathbb{X})/\mathcal{D}_{2n} \to 0$. In this equation, the coefficients $\bar{M}_{n,j,\alpha}$ and $\bar{M}_{n,j,\alpha,\beta}$ take the following forms:
\begin{align*}   \bar{M}_{n,j,\alpha}=\sum_{i\in\mathcal{V}_j} \dfrac{\exp(c_{n,i})}{\alpha!}(\Delta \Zbm_{n,ij})^{\alpha}, 
% \\
% M_{n,j,\alpha}^{(2)}=\sum_{i\in\mathcal{V}_j} \dfrac{\exp(b_{n,i})}{\alpha!} (\Delta W_{n,2}W_{n,1ij})^{\alpha},
\end{align*}
for any $|\alpha| = 2$ and
\begin{align*}
    M_{n,j,\alpha,\beta} = \sum_{i\in\mathcal{V}_j} \dfrac{\exp(c_{n,i})}{\alpha! \beta!} 
    (\Delta \Zbm_{n,ij})^{\alpha + \beta},  
\end{align*}
for any $|\alpha| = |\beta| = 1$. Simple algebra leads to the following formulations of the partial derivatives of $\bar{U}(\mathbb{X}; \Zbm)$ and $\bar{V}(\mathbb{X};\Zbm)$:
\begin{align*}
    \dfrac{\partial \bar{U}}{\partial {(\Zbm)^{(u_1v_1)}}}(\mathbb{X};\Zbm) & = \mathbb{X}^{(u_1)}\mathbb{X}^{(v_1)}\exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\Zbm)\mathbb{X}),  \\
     \dfrac{\partial^{2} \bar{U}}{\partial {(\Zbm)^{(u_1v_1)}}\partial {(\Zbm)^{(u_2v_2)}}}(\mathbb{X};\Zbm) & = \mathbb{X}^{(u_1)}\mathbb{X}^{(v_1)}\mathbb{X}^{(u_2)}\mathbb{X}^{(v_2)}\exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\Zbm)\mathbb{X}),\\
     \dfrac{\partial \bar{V}}{\partial {(\Zbm)^{(u_1v_1)}}}(\mathbb{X};\Zbm) & = \mathbb{X}^{(v_1)}e_{u_1}, \\
    \dfrac{\partial^2 \bar{V}}{\partial {(\Zbm)^{(u_1v_1)}}\partial {(\Zbm)^{(u_2v_2)}}}(\mathbb{X};\Zbm) & =\mathbf{0}_{\bar{d}}.
\end{align*}
% \begin{align*}
%     \dfrac{\partial H}{\partial {(W_{2}W_{1})^{(u_1v_1)}}}(\mathbb{X};W_{2}W_{1}) & = \mathbb{X}^{(u_1)}\mathbb{X}^{(v_1)}, \\
%     \dfrac{\partial H}{\partial {(W_{2}W_{1})^{(u_1v_1)}}\partial {(W_{2}W_{1})^{(u_2v_2)}}}(\mathbb{X};W_{2}W_{1}) & =0.
% \end{align*}
Plugging these formulations into the functions $\bar{A}_{n, 1}(\mathbb{X})$ and $\bar{A}_{n,2}(\mathbb{X})$, we obtain that
\begin{align*}
& \bar{A}_{n, 1}(\mathbb{X}) = \sum_{j:|\mathcal{V}_{j}| = 1} \exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\Zbm_{j}^*)\mathbb{X})\Big[\sum_{u_1,v_1=1}^{\bar{d}}\bar{M}_{n,j,e_{u_1v_1}}\mathbb{X}^{(u_1)}\mathbb{X}^{(v_1)} (\Mbm^0_{V}+\Zbm_{j}^*)\mathbb{X}\\&\hspace{2cm}+\sum_{u_1,v_1=1}^{\bar{d}}\bar{M}_{n,j,e_{u_1v_1}}\mathbb{X}^{(v_1)}e_{u_1}\Big]+ \bar{R}_{n,1}(\mathbb{X}), \\
& \bar{A}_{n, 2}(\mathbb{X}) = \sum_{j:|\mathcal{V}_{j}| > 1} \exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\Zbm_{j}^*)\mathbb{X}) \Big[\sum_{u_1,v_1=1}^{\bar{d}}\bar{M}_{n,j,e_{u_1v_1}}\mathbb{X}^{(u_1)}\mathbb{X}^{(v_1)} (\Mbm^0_{V}+\Zbm_{j}^*)\mathbb{X}\\&\hspace{2cm} + \sum_{u_1,v_1=1}^{\bar{d}} \bar{M}_{n,j,e_{u_1v_1}}\mathbb{X}^{(v_1)}e_{u_1}\\
&\hspace{2cm}+ \sum_{u_1,v_1=1}^{\bar{d}}\sum_{u_2,v_2=1}^{\bar{d}} \bar{M}_{n,j,e_{u_1v_1}+e_{u_2v_2}}\mathbb{X}^{(u_1)}\mathbb{X}^{(v_1)}\mathbb{X}^{(u_2)}\mathbb{X}^{(v_2)}\exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\Zbm_{j}^*)\mathbb{X})(\Mbm^0_{V}+\Zbm_{j}^*)\mathbb{X}\\
&\hspace{2cm}+\sum_{u_1,v_1=1}^{\bar{d}}\sum_{u_2,v_2=1}^{\bar{d}} \bar{M}_{n,j,e_{u_1v_1},e_{u_2v_2}}\mathbb{X}^{(u_1)}\mathbb{X}^{(v_1)}\mathbb{X}^{(v_2)}\exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\Zbm_{j}^*)\mathbb{X})e_{u_2}\Big] + \bar{R}_{n,2}(\mathbb{X}).
\end{align*}
% where the formulations of $L_{n,1,j},L_{n,2,j},\ldots,L_{n,6,j}$ are given by:
% \begin{align*}
%     % M_{n,j,\mathbf{1}_{1:d},\zerod}&:=(M_{n,j,\mathbf{1}_u,\zerod})_{u=1}^{d}, \quad M_{n,j,\zerod,\mathbf{1}_{1:d}}:=(M_{n,j,\zerod,\mathbf{1}_u,})_{u=1}^{d},\\
%     L_{1,n} & := M_{n,j,e_{1:d},\zerod}W_{*,2}^{\top}, \quad M_{n,j,e_{1:d},\zerod}:=(M_{n,j,e_u,\zerod})_{u=1}^{d} \\
%     L_{n,2,j}& := (M_{n,j,\zerod,e_u,})_{u=1}^{d}, \\
%     L_{n,3,j}&:=M_{n,j,e_{1:d},\zerod},\\
%     L_{n,4,j}&:=(M_{n,j,e_{u}+e_{v},\zerod})_{u,v=1}^{d},\\
%     L_{n,5,j}&:=(M_{n,j,\zerod,e_{u}+e_{v}})_{u,v=1}^{d},\\
%     L_{n,6,j}&:=(M_{n,j,e_{u},e_{v}})_{u,v=1}^{d},
% \end{align*}
%the partial derivatives of the function $F(\mathbb{X}; .)$ up to the second order are given by:
%\begin{align*}
%\dfrac{\partial F}{\partial \prompt^{(u)}}(\mathbb{X};\prompt)&= \exp((B\sigma_1(\prompt))^{\top}\mathbb{X}) \Big[((B \dfrac{\partial{\sigma_{1}}}{\partial{\prompt^{(u)}}}(\prompt))^{\top}\mathbb{X})C\sigma_2(\prompt) + C \dfrac{\partial{\sigma_{2}}}{\partial{\prompt^{(u)}}}  (\prompt) \Big], \\
%\dfrac{\partial^2 F}{\partial \prompt^{(u)}\partial \prompt^{(v)}}(\mathbb{X};\prompt)&=
%\exp((B\sigma_1(\prompt))^{\top}\mathbb{X}) \biggr\{\Big[((B \dfrac{\partial{\sigma_{1}}}{\partial{\prompt^{(u)}}}(\prompt))^{\top}\mathbb{X})C\sigma_2(\prompt) + C \dfrac{\partial{\sigma_{2}}}{\partial{\prompt^{(u)}}}  (\prompt) \Big]((B \dfrac{\partial{\sigma_{1}}}{\partial{\prompt^{(v)}}}(\prompt))^{\top}\mathbb{X}) \\
%& + ((B \dfrac{\partial^2{\sigma_{1}}}{\partial{\prompt^{(u)}}\partial{\prompt^{(v)}}}(\prompt))^{\top}\mathbb{X})C\sigma_2(\prompt) + ((B \dfrac{\partial{\sigma_{1}}}{\partial{\prompt^{(u)}}}(\prompt))^{\top}\mathbb{X})C\dfrac{\partial{\sigma_{2}}}{\partial{\prompt^{(v)}}}  (\prompt) + C \dfrac{\partial^2{\sigma_{2}}}{\partial{\prompt^{(u)}}\partial{\prompt^{(v)}}}(\prompt) \biggr\}
%\exp((B\sigma_1(\prompt_{*,j}))^{\top}\mathbb{X})\Big[((B\sigma_1^{''}(\prompt_{*,j}))^{\top}\mathbb{X}^{(u)}\mathbb{X}^{(v)})C\sigma_2(\prompt_{*,j}) +C\sigma_2^{''}(\prompt_{*,j})\\
%&+((B\sigma_1^{'}(\prompt_{*,j}))^{\top}\mathbb{X}^{(u)})C\sigma_2^{'}(\prompt_{*,j})+ +((B\sigma_1^{'}(\prompt_{*,j}))^{\top}\mathbb{X}^{(v)})C\sigma_2^{'}(\prompt_{*,j})\\
%&+\left((B\sigma_1^{'}(\prompt_{*,j}))^{\top}\right)^2\mathbb{X}^{(u)}\mathbb{X}^{(v)} C\sigma_2(\prompt_{*,j})  \Big]
%\end{align*}
%for any $1 \leq u, v \leq d$. Finally, we also have
%$$M_{n,j,\alpha}=\sum_{i\in\mathcal{V}_j} \dfrac{\exp(b_{n,i})}{\alpha!} (\Delta\prompt_{n,ij})^{\alpha},$$ for any $1 \leq |\alpha| \leq 2$.
In these equations, $e_{u}$ is denoted as the vector in $\mathbb{R}^{\bar{d}}$ such that its $u$-th element is 1 while its other elements are 0 for any $1 \leq u \leq \bar{d}$. Furthermore, $e_{uv}$ is denoted as matrix in $\mathbb{R}^{\bar{d} \times \bar{d}}$ with its $uv$-th entry is 1 while other entries are zero.  
%Furthermore, $1_{uv}$ is the matrix that its $(u,v)$-th element is 1 while its other elements are 0 for any $1 \leq u,v \leq d$. The second equation in the formulation of $\bar{L}_{1,n}(\prompt)$ is due to the fact that the function $\bar{\sigma}_{2}$ is only applied element wise to $W_{2}p$, which leads to $\dfrac{\partial^2{\bar{\sigma}_{2}}}{\partial{(W_{2}\prompt)^{(u)}}\partial{(W_{2}\prompt)^{(v)}}}(W_{2}\prompt) = 0$ for all $u \neq v$. 
\paragraph{Decomposition of the function $\bar{B}_n(\mathbb{X})$.}  Moving to the function $\bar{B}_n(\mathbb{X})$, we can decompose this function as follows:
\begin{align*}
    \bar{B}_n(\mathbb{X}) &=\sum_{j:|\mathcal{V}_j|=1}\sum_{i\in\mathcal{V}_j}\exp(c_{n,i})\Big[\bar{U}(\mathbb{X}; \Zbm_{n,i})-\bar{U}(\mathbb{X}; \Zbm_{j}^{*})\Big]f_{G_n}(\mathbb{X}) \\
    &  +\sum_{j:|\mathcal{V}_j|>1}\sum_{i\in\mathcal{V}_j}\exp(c_{n,i})\Big[\bar{U}(\mathbb{X}; \Zbm_{n,i})-\bar{U}(\mathbb{X}; \Zbm_{j}^{*})\Big]f_{G_n}(\mathbb{X}) \\
    &:= \bar{B}_{n,1}(\mathbb{X}) + \bar{B}_{n,2}(\mathbb{X}).
\end{align*}
An application of the Taylor expansions up to the first order for $\bar{B}_{n,1}(\mathbb{X})$ and the second order for $\bar{B}_{n,2}(\mathbb{X})$ leads to
\begin{align*}
    \bar{B}_{n,1}(\mathbb{X})&= \sum_{j:|\mathcal{V}_j|=1}\sum_{|\alpha|=1} \bar{M}_{n,j,\alpha} \dfrac{\partial^{|\alpha|}\bar{U}}{\partial {(\Zbm)^{\alpha}}}(\mathbb{X};\Zbm_{j}^{*})f_{G_n}(\mathbb{X})+ \bar{R}_{n,3}(\mathbb{X}),
    \\
     \bar{B}_{n,2}(\mathbb{X})&=\sum_{j:|\mathcal{V}_j|=1}\sum_{1 \leq |\alpha|\leq 2} \bar{M}_{n,j,\alpha} \dfrac{\partial^{|\alpha|}\bar{U}}{\partial {(\Zbm)^{\alpha}}}(\mathbb{X};\Zbm_{j}^{*})f_{G_n}(\mathbb{X})+ \bar{R}_{n,4}(\mathbb{X})
\end{align*}
where the Taylor remainders $\bar{R}_{n,3}(\mathbb{X}), \bar{R}_{n,4}(\mathbb{X})$ satisfy that $\bar{R}_{n,3}(\mathbb{X})/\mathcal{D}_{2n} \to 0$ and $\bar{R}_{n,4}(\mathbb{X})/\mathcal{D}_{2n} \to 0$. Direct calculation leads to
\begin{align*}
    & \bar{B}_{n,1}(\mathbb{X})  = \sum_{j:|\mathcal{V}_{j}| = 1} \exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\Zbm_{j}^*)\mathbb{X}) \Big[\sum_{u_1,v_1=1}^{\bar{d}} \bar{M}_{n,j,e_{u_1v_1}}\mathbb{X}^{(u_1)}\mathbb{X}^{(v_1)}\Big]f_{G_n}(\mathbb{X}) + \bar{R}_{n,3}(\mathbb{X}), \\
    & \bar{B}_{n,2}(\mathbb{X})  = \sum_{j:|\mathcal{V}_{j}| > 1} \exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\Zbm_{j}^*)\mathbb{X}) \Big[\sum_{u_1,v_1=1}^{d}\bar{M}_{n,j,e_{u_1v_1}}\mathbb{X}^{(u_1)}\mathbb{X}^{(v_1)} \\
    &\hspace{4cm}+\sum_{u_1,v_1=1}^{\bar{d}}\sum_{u_2,v_2=1}^{\bar{d}} \bar{M}_{n,j,e_{u_1v_1}}\mathbb{X}^{(u_1)}\mathbb{X}^{(v_1)}\mathbb{X}^{(u_2)}\mathbb{X}^{(v_2)}\Big]f_{G_n}(\mathbb{X}) + \bar{R}_{n,4}(\mathbb{X}),
\end{align*}
% where the formulations of the functions $N_{1,n}$, $\bar{N}_{1,n}$, and $\bar{N}_{2,n}$ are given by:
% \begin{align*}
%     N_{1,n}(\prompt) & = \sum_{u = 1}^{d} M_{n, j, 1_{u}}^{(1)} \dfrac{\partial{\bar{\sigma}_{1}}}{\partial{(W_{1}\prompt)^{(u)}}}(W_{1}\prompt), \\
%     \bar{N}_{1,n}(\prompt) & = \sum_{u = 1}^{d} M_{n, j, 1_{u}}^{(1)}  \dfrac{\partial{\bar{\sigma}_{1}}}{\partial{(W_{1}\prompt)^{(u)}}}(W_{1}\prompt) \\
%     & + \sum_{1 \leq u,v \leq d} M_{n,j,1_{uv}}^{(1)} \dfrac{\partial^2{\bar{\sigma}_{1}}}{\partial{(W_{1}\prompt)^{(u)}}\partial{(W_{1}\prompt)^{(v)}}}(W_{1}\prompt), \\
%     \bar{N}_{2,n}(\prompt) & = \sum_{1 \leq u,v \leq d} M_{n, j, 1_{uv}}^{(1)}  \dfrac{\partial{\bar{\sigma}_{1}}}{\partial{(W_{1}\prompt)^{(u)}}}(W_{1}\prompt)  \dfrac{\partial{\bar{\sigma}_{1}}}{\partial{(W_{1}\prompt)^{(v)}}}(W_{1}\prompt)^{\top}.
% \end{align*}
Putting all the above results together, we can represent the function $Q_n(\mathbb{X})$ as follows: 
\begin{align}
    Q_n(\mathbb{X})&= \sum_{j:|\mathcal{V}_{j}| = 1} \exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\Zbm_{j}^*)\mathbb{X}) \Big[\sum_{u_1,v_1=1}^{\bar{d}} \bar{M}_{n,j,e_{u_1v_1}}\mathbb{X}^{(u_1)}\mathbb{X}^{(v_1)} (\Mbm^0_{Q}+\Zbm_{j}^*)\mathbb{X})\\&+\sum_{u_1,v_1=1}^{\bar{d}}\bar{M}_{n,j,e_{u_1v_1}}\mathbb{X}^{(v_1)}e_{u_1}\Big] \nonumber \\
    & + \sum_{j:|\mathcal{V}_{j}| > 1} \exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\Zbm_{j}^*)\mathbb{X}) \Big[\sum_{u_1,v_1=1}^{\bar{d}}\bar{M}_{n,j,e_{u_1v_1}}\mathbb{X}^{(u_1)}\mathbb{X}^{(v_1)} (\Mbm^0_{Q}+\Zbm_{j}^*)\mathbb{X}\\&+\sum_{u_1,v_1=1}^{\bar{d}} \bar{M}_{n,j,e_{u_1v_1}} \mathbb{X}^{(v_1)}e_{u_1}\nonumber\\
& + \sum_{u_1,v_1=1}^{\bar{d}}\sum_{u_2,v_2=1}^{\bar{d}} \bar{M}_{n,j,e_{u_1v_1}+e_{u_2v_2}}\mathbb{X}^{(u_1)}\mathbb{X}^{(v_1)}\mathbb{X}^{(u_2)}\mathbb{X}^{(v_2)}\exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\Zbm_{j}^*)\mathbb{X})(\Mbm^0_{V}+\Zbm_{j}^*)\mathbb{X} \nonumber \\
& +\sum_{u_1,v_1=1}^{\bar{d}}\sum_{u_2,v_2=1}^{\bar{d}} \bar{M}_{n,j,e_{u_1v_1},e_{u_2v_2}}\mathbb{X}^{(u_1)}\mathbb{X}^{(v_1)}\mathbb{X}^{(v_2)}\exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\Zbm_{j}^*)\mathbb{X})e_{u_2}\Big] \nonumber
\end{align}
\begin{align}
    & - \sum_{j:|\mathcal{V}_{j}| = 1} \exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\Zbm_{j}^*)\mathbb{X}) \Big[\sum_{u_1,v_1=1}^{\bar{d}} \bar{M}_{n,j,e_{u_1v_1}}\mathbb{X}^{(u_1)}\mathbb{X}^{(v_1)}\Big]f_{G_n}(\mathbb{X}) \nonumber \\
    & - \sum_{j:|\mathcal{V}_{j}| > 1} \exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\Zbm_{j}^*)\mathbb{X}) \Big[\sum_{u_1,v_1=1}^{\bar{d}} \bar{M}_{n,j,e_{u_1v_1}}\mathbb{X}^{(u_1)}\mathbb{X}^{(v_1)}\nonumber \\
    & +\sum_{u_1,v_1=1}^{d}\sum_{u_2,v_2=1}^{d}\bar{M}_{n,j,e_{u_1v_1}}\mathbb{X}^{(u_1)}\mathbb{X}^{(v_1)}\mathbb{X}^{(u_2)}\mathbb{X}^{(v_2)}\Big]f_{G_n}(\mathbb{X})\nonumber \\
    &  - \sum_{j = 1}^{L} \bar{N}_{n,j} \exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\Zbm_{j}^*)\mathbb{X}) f_{G_{n}}(\mathbb{X})  + \sum_{j = 1}^{L} \bar{N}_{n,j} \exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\Zbm_{j}^*)\mathbb{X})(\Mbm^0_{V}+\Zbm_{j}^*)\mathbb{X}   \nonumber \\
    &  + \bar{R}_{n,1}(\mathbb{X}) + \bar{R}_{n,2}(\mathbb{X}) - \bar{R}_{n,3}(\mathbb{X}) - \bar{R}_{n,4}(\mathbb{X}) 
 \label{eq:main_equation_expression_linear}
\end{align}   
% \sum_{j=1}^{L}\sum_{0\leq|\alpha|\leq 2} M_{n,j,\alpha}\dfrac{\partial^{|\alpha|}F}{\partial \prompt^\alpha}(\mathbb{X};\prompt_{*,j}) - \sum_{j=1}^{L}\sum_{0 \leq |\alpha|\leq 2} M_{n,j,\alpha}\dfrac{\partial^{|\alpha|}E}{\partial \prompt^\alpha}(\mathbb{X};\prompt_{*,j})g_{G_n}(\mathbb{X}) \\
%    & \hspace{14 em} + \sum_{j=1}^{2}R_{n,j} (\mathbb{X}) - \sum_{j=3}^{4}R_{n,j} (\mathbb{X}) \\    &=\sum_{j=1}^{L}\sum_{0\leq|\alpha|\leq 2} M_{n,j,\alpha} \Big[\dfrac{\partial^{|\alpha|}F}{\partial \prompt^\alpha}(\mathbb{X};\prompt_{*,j}) - \dfrac{\partial^{|\alpha|}E}{\partial \prompt^\alpha}(\mathbb{X};\prompt_{*,j})g_{G_n}(\mathbb{X})\Big] \\
%    & \hspace{14 em} + \sum_{j=1}^{2}R_{n,j} (\mathbb{X}) - \sum_{j=3}^{4}R_{n,j} (\mathbb{X}).
% \end{align*}
where $\bar{N}_{n,j}:=\sum_{i\in\mathcal{V}_j}\exp(c_{n,i})-\exp(c_{j}^{*})$ for any $j \in [L]$. 
%$L_{1,n}'(\prompt_{*,j}) = L_{1,n}(\prompt_{*,j}) + M_{n,j,0_{d}}C\bar{\sigma}_{2}(\mathbb{X};W_{*,2}W_{*,1j})$, and $\bar{L}_{1,n}'(\prompt_{*,j}) = \bar{L}_{1,n}(\prompt_{*,j}) + M_{n,j,0_{d}}C\bar{\sigma}_{2}(\mathbb{X};W_{*,2}W_{*,1j})$.


\paragraph{Step 2 - Non-vanishing coefficients.} 
 As indicated in equation~(\ref{eq:main_equation_expression_linear}),  the ratio $Q_{n}(\mathbb{X})/ \mathcal{D}_{2n}$ can be expressed as a linear combination of the following independent functions:
 \begin{align*}
&\bar{U}(\mathbb{X}; \Zbm_{j}^{*}) \mathbb{X}^{(u_1)}\mathbb{X}^{(v_1)}(\Mbm^0_{V}+\Zbm_{j}^{*})\mathbb{X},\quad \bar{U}(\mathbb{X}; \Zbm_{j}^{*})\mathbb{X}^{(v_1)}e_{u_1}, \\
&\bar{U}(\mathbb{X}; \Zbm_{j}^{*})\mathbb{X}^{(u_1)}\mathbb{X}^{(v_1)}\mathbb{X}^{(u_2)}\mathbb{X}^{(v_2)}(\Mbm^0_{V}+\Zbm_{j}^{*})\mathbb{X}, \quad \bar{U}(\mathbb{X}; \Zbm_{j}^{*}) \mathbb{X}^{(u_1)}\mathbb{X}^{(v_1)}\mathbb{X}^{(v_2)}e_{u_2},\\
&\bar{U}(\mathbb{X}; \Zbm_{j}^{*}) \mathbb{X}^{(u_1)}\mathbb{X}^{(v_1)}f_{G_n}(\mathbb{X}), \quad \bar{U}(\mathbb{X}; \Zbm_{j}^{*}) \mathbb{X}^{(u_1)}\mathbb{X}^{(v_1)}\mathbb{X}^{(u_2)}\mathbb{X}^{(v_2)}f_{G_n}(\mathbb{X}),\\
&\bar{U}(\mathbb{X}; \Zbm_{j}^{*}) f_{G_{n}}(\mathbb{X}), \quad  \bar{U}(\mathbb{X}; \Zbm_{j}^{*}) (\Mbm^0_{V}+\Zbm_{j}^{*})\mathbb{X},
 \end{align*}
%  \begin{align*}
% & E(\mathbb{X};W_{*,2}W_{*,1j})W_{*,1j}\mathbb{X}W_{*,2}, \ E(\mathbb{X};W_{*,2}W_{*,1j})\sigma^{(1)}(W_{*,1j}\mathbb{X})W_{*,1j}\mathbb{X}\mathbb{X}^{(u)}\mathbb{X}^{(v)} W_{*,2}\\
% &E(\mathbb{X};W_{*,2}W_{*,1j})\sigma^2(W_{*,1j}\mathbb{X})\mathbb{X}^{(u)} W_{*,2}, \ E(\mathbb{X};W_{*,2}W_{*,1j})\sigma^{(1)}(W_{*,1j}\mathbb{X})\mathbb{X}^{(u)}W_{*,2},\\ 
% &E(\mathbb{X};W_{*,2}W_{*,1j})W_{*,1j}\mathbb{X}e_{u}, \  E(\mathbb{X};W_{*,2}W_{*,1j})[\sigma^{(1)}(W_{*,1j}\mathbb{X})]^2W_{*,1j}\mathbb{X}\mathbb{X}^{(u)}\mathbb{X}^{(v)}\mathbb{X}^{(w)}W_{*,2},  \\
% & E(\mathbb{X};W_{*,2}W_{*,1j})\sigma^{(2)}(W_{*,1j}\mathbb{X})W_{*,1j}\mathbb{X}\mathbb{X}^{(u)}\mathbb{X}^{(v)}\mathbb{X}^{(w)}W_{*,2}, \ E(\mathbb{X};W_{*,2}W_{*,1j})\sigma^{(2)}(W_{*,1j}\mathbb{X})\mathbb{X}^{(u)}\mathbb{X}^{(v)}W_{*,2},\\
% &E(\mathbb{X};W_{*,2}W_{*,1j})\sigma^{2}(W_{*,1j}\mathbb{X})W_{*,1j}\mathbb{X}\mathbb{X}^{(u)}\mathbb{X}^{(v)}W_{*,2}, \ E(\mathbb{X};W_{*,2}W_{*,1j})\sigma^{(1)}(W_{*,1j}\mathbb{X})W_{*,1j}\mathbb{X}\mathbb{X}^{(u)}\mathbb{X}^{(v)}W_{*,2},\\
% &E(\mathbb{X};W_{*,2}W_{*,1j})\sigma^{(1)}(W_{*,1j}\mathbb{X})[W_{*,1j}\mathbb{X}]^2\mathbb{X}^{(u)}\mathbb{X}^{(v)}\mathbb{X}^{(w)}W_{*,2}, \ E(\mathbb{X};W_{*,2}W_{*,1j})\sigma^{(2)}(W_{*,1j}\mathbb{X})\mathbb{X}^{(u)}e_{u},\\
% &E(\mathbb{X};W_{*,2}W_{*,1j})[\sigma^{(1)}(W_{*,1j}\mathbb{X})]^2\mathbb{X}^{(u)}\mathbb{X}^{(v)}\mathbb{X}^{(w)}W_{*,2}, \ E(\mathbb{X};W_{*,2}W_{*,1j})\sigma^{(1)}(W_{*,1j}\mathbb{X})W_{*,1j}\mathbb{X}\mathbb{X}^{(u)}\mathbb{X}^{(v)}e_{v},\\
% &E(\mathbb{X};W_{*,2}W_{*,1j})\sigma^{(1)}(W_{*,1j}\mathbb{X})W_{*,1j}\mathbb{X}\mathbb{X}^{(u)}\mathbb{X}^{(v)}W_{*,2}, \ E(\mathbb{X};W_{*,2}W_{*,1j})[W_{*,1j}\mathbb{X}]^2\mathbb{X}^{(u)}e_{u},\\
% &E(\mathbb{X};W_{*,2}W_{*,1j})\sigma^{(1)}(W_{*,1j}\mathbb{X})\mathbb{X}^{(u)}\mathbb{X}^{(v)}f_{G_n}(\mathbb{X}), \ E(\mathbb{X};W_{*,2}W_{*,1j})W_{*,1j}\mathbb{X}\mathbb{X}^{(u)}f_{G_n}(\mathbb{X}),\\
% &E(\mathbb{X};W_{*,2}W_{*,1j})[\sigma^{(1)}(W_{*,1j}\mathbb{X})]^2\mathbb{X}^{(u)}\mathbb{X}^{(v)}\mathbb{X}^{(w)}f_{G_n}(\mathbb{X}), \ E(\mathbb{X};W_{*,2}W_{*,1j})\sigma^{(2)}(W_{*,1j}\mathbb{X})\mathbb{X}^{(u)}\mathbb{X}^{(v)}\mathbb{X}^{(w)}f_{G_n}(\mathbb{X}),\\
% &E(\mathbb{X};W_{*,2}W_{*,1j})\sigma^{2}(W_{*,1j}\mathbb{X})\mathbb{X}^{(u)}\mathbb{X}^{(v)}f_{G_n}(\mathbb{X}), \ E(\mathbb{X};W_{*,2}W_{*,1j})\sigma^{(1)}(W_{*,1j}\mathbb{X})\mathbb{X}^{(u)}\mathbb{X}^{(v)}f_{G_n}(\mathbb{X})\\
% &E(\mathbb{X};W_{*,2}W_{*,1j})\sigma^{(1)}(W_{*,1j}\mathbb{X})W_{*,1j}\mathbb{X}\mathbb{X}^{(u)}\mathbb{X}^{(v)}\mathbb{X}^{(w)}f_{G_n}(\mathbb{X}), \ E(\mathbb{X};W_{*,2}W_{*,1j})f_{G_n}(\mathbb{X}),
% \end{align*}
for any indices $1 \leq j \leq L$ and $1 \leq u_{1}, v_{1}, u_{2}, v_{2} \leq \bar{d}$. 
 
We demonstrate that at least one of the coefficients of these independent functions does not go to 0 as $n \to \infty$. Assume by contrary that all these coefficients of these linear independent functions go to 0. From equation~(\ref{eq:main_equation_expression_linear}), we obtain that $\bar{M}_{n,j,\alpha}/\mathcal{D}_{2n}$, $\bar{M}_{n,j,\alpha,\beta}/\mathcal{D}_{2n}$, and $\bar{N}_{n,j}/\mathcal{D}_{2n}$ go to 0 for all the coefficients $\alpha,\beta\in\mathbb{N}^{\bar{d}\times \bar{d}}$ satisfying that $1\leq|\alpha|+|\beta|\leq 2$. 
 
Since $N_{n,j}/\mathcal{D}_{2n} \to 0$, we find that for any $j \in [L]$
\begin{align*}
     \frac{|\sum_{i\in\mathcal{V}_j}\exp(c_{n,i})-\exp(c_{j}^{*})|}{\mathcal{D}_{2n}}= \frac{|N_{n,j}|}{\mathcal{D}_{2n}}  \to 0.
\end{align*}
Taking the summation of these limits leads to 
\begin{align}
\label{eq:key_limits_first_2_linear}
\frac{\sum_{j = 1}^{L} |\sum_{i\in\mathcal{V}_j}\exp(c_{n,i})-\exp(c_{j}^{*})|}{\mathcal{D}_{2n}} \to 0. 
\end{align}
Now, for any indices $j \in [L]$ such that $|\mathcal{V}_j | = 1$, the limits $\bar{M}_{n,j,e_{uv}}/\mathcal{D}_{2n} \to 0$ lead to

\begin{equation*}
    \frac{\sum_{i \in \mathcal{V}_{j}} \exp(c_{n,i}) \|\Delta \Zbm_{n,ij}\|_1}{\mathcal{D}_{2n}} \to 0.
\end{equation*}
% Similarly, $L_{n,2,j}/ \mathcal{D}_{2n} \to 0$ also leads to $\dfrac{\sum_{i \in \mathcal{V}_{j}} \exp(b_{n,i}) \|\Delta W_{n,2ij}\|_1}{\mathcal{D}_{2n}} \to 0$. Putting the above results together, we find that
% \begin{align*}
%     \frac{\sum_{j: |\mathcal{V}_{j}| = 1} \sum_{i \in \mathcal{V}_{j}} \exp(b_{n,i})(\|\Delta W_{n,1ij}\|_1 + \|\Delta W_{n,2ij}\|_1)}{\mathcal{D}_{2n}} \to 0. 
% \end{align*}
That result directly implies that
\begin{align}
    \label{eq:linear_loss_linear}
    \frac{\sum_{j: |\mathcal{V}_{j}| = 1} \sum_{i \in \mathcal{V}_{j}} \exp(c_{n,i})\|\Delta \Zbm_{n,ij}\|}{\mathcal{D}_{2n}} \to 0. 
\end{align}
Moving to indices $j \in [L]$ such that their corresponding Voronoi cells satisfy that $|\mathcal{V}_{j}| > 1$. The limits $M_{n,j,2e_{uv}}/ \mathcal{D}_{2n} \to 0$ lead to
\begin{align}
    \label{eq:squared_loss_linear}
    \frac{\sum_{i \in \mathcal{V}_{j}} \exp(c_{n,i})\|\Delta \Zbm_{n,ij}\|^2}{\mathcal{D}_{2n}} \to 0. 
\end{align}
% Likewise, as ${L}_{n,5,j}^{(uu)}/ \mathcal{D}_{2n} \to 0$, we also obtain that $\dfrac{\sum_{i \in \mathcal{V}_{j}} \exp(b_{n,i})\|\Delta W_{n,2ij}\|^2}{\mathcal{D}_{2n}} \to 0$. Therefore, we find that
% \begin{align}
%     \label{eq:squared_loss_linear}
%     \frac{\sum_{j: |\mathcal{V}_{j}| > 1} \sum_{i \in \mathcal{V}_{j}} \exp(b_{n,i})(\|\Delta W_{n,1ij}\|^2 + \|\Delta W_{n,2ij}\|^2}{\mathcal{D}_{2n}} \to 0. 
% \end{align}
By putting the results in equations~(\ref{eq:key_limits_first_2_linear}), (\ref{eq:linear_loss_linear}), and (\ref{eq:squared_loss_linear}) together, we arrive at $1 = \frac{\mathcal{D}_{2n}}{\mathcal{D}_{2n}} \to 0$
as $n \to \infty$, which is a contradiction. Consequently, as $n \to \infty$, at least one of the coefficients of the linear independent functions in $Q_{n}(\mathbb{X})/ \mathcal{D}_{2n}$ does not go to 0 . 
\paragraph{Step 3 - Applying the Fatou’s lemma.} Denote $\bar{m}_n$ the maximum of the absolute values of the coefficients of the linear independent functions in $Q_{n}(\mathbb{X})/ \mathcal{D}_{2n}$. As at least one of these coefficients does not go to 0, it
indicates that $1/\bar{m}_n \not \to \infty $ as $n \to \infty$.
Since $\normf{f_{G_n}-f_{G_*}}/\mathcal{D}_{2n} \to 0$ as $n \to \infty$, we obtain $\normf{f_{G_n}-f_{G_*}}/(\bar{m}_{n} \mathcal{D}_{2n}) \to 0$. An important insight is that we can move the limit as $n \to \infty$ of that ratio inside the integral via an application of the Fatou's lemma, which is given by:
\begin{align*}
    0=\lim_{n \to \infty} \dfrac{\normf{f_{G_n}-f_{\bar{G}_*}}}{\bar{m}_n\mathcal{D}_{2n}} & = \lim_{n \to \infty} \int \dfrac{\left| f_{G_n}(\mathbb{X})-f_{\bar{G}_*}(\mathbb{X})\right|}{\bar{m}_n\mathcal{D}_{2n}}d\mu(\mathbb{X}) \\
    & \geq  \int \liminf_{n \to \infty} \dfrac{\left| f_{G_n}(\mathbb{X})-f_{\bar{G}_*}(\mathbb{X})\right|}{\bar{m}_n\mathcal{D}_{2n}}d\mu(\mathbb{X}) \geq 0.
\end{align*}
That inequality demonstrates that $\liminf_{n \to \infty} \dfrac{\left| f_{G_n}(\mathbb{X})-f_{\bar{G}_*}(\mathbb{X})\right|}{\bar{m}_n\mathcal{D}_{2n}} = 0$ for almost surely $\mathbb{X}$. As $n \to \infty$, we denote
\begin{align*}
    \dfrac{\bar{M}_{n,j,\alpha}}{\bar{m}_{n}\mathcal{D}_{2n}} \to \bar{\lambda}_{j,\alpha}, \quad \dfrac{\bar{M}_{n,j,\alpha,\beta}}{m_n\mathcal{D}_{2n}} \to \bar{\xi}_{j,\alpha,\beta}, \quad \dfrac{\bar{N}_{n,j}}{m_n\mathcal{D}_{2n}}\to \bar{\tau}_{j},
\end{align*}
for any indices $j \in [L]$ and any coefficients $\alpha,\beta\in\mathbb{N}^{\bar{d}\times\bar{d}}$ such that $1\leq|\alpha|+|\beta|\leq 2$. Here, we have that at least one coefficient from $\{\bar{\lambda}_{j,\alpha},\bar{\xi}_{j,\alpha,\beta},\bar{\tau}_{j}:j\in[L], \alpha,\beta\in\mathbb{N}^{\bar{d}\times \bar{d}}: 1\leq|\alpha|+|\beta|\leq 2\}$ is different from 0 (indeed, it should be equal to 1). Given the above notations, the limit $\liminf_{n \to \infty} \dfrac{\left| f_{G_n}(\mathbb{X})-f_{\bar{G}_*}(\mathbb{X})\right|}{m_n\mathcal{D}_{2n}} = 0$ implies that
\begin{align}
&\sum_{j:|\mathcal{V}_{j}| = 1} \exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\Zbm_{j}^*)\mathbb{X}) \Big[\sum_{u_1,v_1=1}^{\bar{d}} \bar{\lambda}_{j,e_{u_1v_1}}\mathbb{X}^{(u_1)}\mathbb{X}^{(v_1)} (\Mbm^0_{Q}+\Zbm_{j}^*)\mathbb{X})+\sum_{u_1,v_1=1}^{\bar{d}}\bar{M}_{n,j,e_{u_1v_1}}\mathbb{X}^{(v_1)}e_{u_1}\Big] \nonumber \\
    &  + \sum_{j:|\mathcal{V}_{j}| > 1} \exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\Zbm_{j}^*)\mathbb{X}) \Big[\sum_{u_1,v_1=1}^{\bar{d}}\lambda_{j,e_{u_1v_1}}\mathbb{X}^{(u_1)}\mathbb{X}^{(v_1)} (\Mbm^0_{Q}+\Zbm_{j}^*)\mathbb{X}+\sum_{u_1,v_1=1}^{\bar{d}} \bar{M}_{n,j,e_{u_1v_1}} \mathbb{X}^{(v_1)}e_{u_1}\nonumber\\
& + \sum_{u_1,v_1=1}^{\bar{d}}\sum_{u_2,v_2=1}^{\bar{d}} \bar{\lambda}_{j,e_{u_1v_1}+e_{u_2v_2}}\mathbb{X}^{(u_1)}\mathbb{X}^{(v_1)}\mathbb{X}^{(u_2)}\mathbb{X}^{(v_2)}\exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\Zbm_{j}^*)\mathbb{X})(\Mbm^0_{V}+\Zbm_{j}^*)\mathbb{X} \nonumber \\
& +\sum_{u_1,v_1=1}^{\bar{d}}\sum_{u_2,v_2=1}^{\bar{d}} \bar{\xi}_{j,e_{u_1v_1},e_{u_2v_2}}\mathbb{X}^{(u_1)}\mathbb{X}^{(v_1)}\mathbb{X}^{(v_2)}\exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\Zbm_{j}^*)\mathbb{X})e_{u_2}\Big] \nonumber \\
    & - \sum_{j:|\mathcal{V}_{j}| = 1} \exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\Zbm_{j}^*)\mathbb{X}) \Big[\sum_{u_1,v_1=1}^{\bar{d}} \bar{\lambda}_{j,e_{u_1v_1}}\mathbb{X}^{(u_1)}\mathbb{X}^{(v_1)}\Big]f_{G_n}(\mathbb{X}) \nonumber \\
    & - \sum_{j:|\mathcal{V}_{j}| > 1} \exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\Zbm_{j}^*)\mathbb{X}) \Big[\sum_{u_1,v_1=1}^{\bar{d}} \bar{\lambda}_{j,e_{u_1v_1}}\mathbb{X}^{(u_1)}\mathbb{X}^{(v_1)}\nonumber \\
    & +\sum_{u_1,v_1=1}^{d}\sum_{u_2,v_2=1}^{d}\bar{\xi}_{j,e_{u_1v_1},e_{u_2v_2}} \mathbb{X}^{(u_1)}\mathbb{X}^{(v_1)}\mathbb{X}^{(u_2)}\mathbb{X}^{(v_2)}\Big]f_{G_n}(\mathbb{X})\nonumber \\
    & - \sum_{j = 1}^{L} \bar{\tau}_{j} \exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\Zbm_{j}^*)\mathbb{X}) f_{G_{n}}(\mathbb{X})  + \sum_{j = 1}^{L} \bar{\tau}_{j} \exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\Zbm_{j}^*)\mathbb{X})(\Mbm^0_{V}+\Zbm_{j}^*)\mathbb{X} = 0
\end{align} 
for almost surely $\mathbb{X}$. 
%By denoting $\boldsymbol{Z} = B^{\top} \mathbb{X}$, this equation also holds for almost surely $\boldsymbol{Z}$.
However, that equation implies that all the coefficients $\{\bar{\lambda}_{j,\alpha},\bar{\xi}_{j,\alpha,\beta},\bar{\tau}_{j}:j\in[L], \alpha,\beta\in\mathbb{N}^{\bar{d}\times \bar{d}}: 1\leq|\alpha|+|\beta|\leq 2\}$ are 0. It is a contradiction. As a consequence, we obtain that $$\lim_{\varepsilon\to0} \inf_{G\in \bar{\mathcal{G}}_{L'}(\Theta): \mathcal{D}_2(G,\bar{G}_*)\leq \varepsilon} \normf{f_{G}-f_{\bar{G}_*}}/\mathcal{D}_2(G,\bar{G}_*) >0.$$

\subsubsection{Global Part}
The result of the local part implies that  we can find a positive constant $\varepsilon'$ such that
$$\inf_{G\in \bar{\mathcal{G}}_{L'}(\Theta): \mathcal{D}_2(G,\bar{G}_*)\leq \varepsilon'} \normf{f_{G}-f_{\bar{G}_*}}/\mathcal{D}_2(G,\bar{G}_*) >0.$$
Therefore to obtain the conclusion of the theorem, we only need to prove that
$$ \inf_{G\in \bar{\mathcal{G}}_{L'}(\Theta): \mathcal{D}_2(G,\bar{G}_*)> \varepsilon'} \normf{f_{G}-f_{\bar{G}_*}}/\mathcal{D}_2(G,\bar{G}_*) >0.$$
We assume by contradiction that the above claim does not hold. It indicates that there exists a sequence of
measures $G'_{n} := \sum_{j = 1}^{\tilde{L}} \exp(c_{n,j}) \delta_{\Bbm_{n,j}\Abm_{n,j}}$ in $\bar{\mathcal{G}}_{L'}(\Theta)$ such that 
$$\left\{\begin{matrix}
 \mathcal{D}_2(G'_n,\bar{G}_*) > \varepsilon'\\
 \normf{f_{G'_n}-f_{\bar{G}_*}}/\mathcal{D}_2(G'_n,\bar{G}_*) \to 0
\end{matrix}\right.$$
as $n \to \infty$, which implies that $\normf{f_{G'_n}-f_{\bar{G}_*}} \to 0$  as $n \to \infty$.\\
Given that $\Theta$ is a compact set, there exists a mixing measure $G'$ in $\bar{\mathcal{G}}_{L'}(\Theta)$ such that one of the $G'_n$'s subsequences converges to $G'$. Since $\mathcal{D}_2(G'_n,\bar{G}_*)>\varepsilon'$, we obtain that $\mathcal{D}_2(G',\bar{G}_*)>\varepsilon'$.
Applying the Fatou’s lemma, we have
$$0=\lim_{n \to \infty} \normf{f_{G'_n}-f_{\bar{G}_*}} \geq  \int \liminf_{n \to \infty} \left\| f_{G'_n}(\mathbb{X})-f_{\bar{G}_*}(\mathbb{X})\right\|^2 d\mu(\mathbb{X}).$$
The above inequality indicates that $f_{G'}=f_{\bar{G}_*}$ for almost surely $\mathbb{X}$. From the identifiability property, we deduce that $G' \equiv \bar{G}_*$. It follows that $\mathcal{D}_2(G',\bar{G}_*)=0$, contradicting the fact that $\mathcal{D}_2(G',\bar{G}_*)> \varepsilon'>0$. 
Hence, the proof is completed.
\paragraph{Proof for the identifiability property.} 
We will prove that if $f_{G}(\mathbb{X}) = f_{\bar{G}_*}(\mathbb{X})$ for almost surely $\mathbb{X}$, then $G \equiv  \bar{G}_*$.
To ease the presentation, for any mixing measure $G = \sum_{j = 1}^{\tilde{L}} \exp(c_{j}) \delta_{\Bbm_{j}\Abm_{j}} \in \mathcal{G}_{L'}(\Xi)$, we denote
\begin{align*}
    \softmax_{G}(u)&=\dfrac{\exp(u)}{\sum_{j=1}^{\tilde{L}} \exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\Zbm_{j})\mathbb{X}+c_{j})},
\end{align*}
where $u \in \{\mathbb{X}^{\top}(\Mbm^0_{Q}+\Zbm_{j})\mathbb{X}+c_{j}: j \in [\tilde{L}]\}$.
The equation $f_{G}(\mathbb{X}) = f_{\bar{G}_*}(\mathbb{X})$ indicates that
\begin{align}
    &\sum_{j=1}^{L} \softmax(\mathbb{X}^{\top}(\Mbm^0_{Q}+\Zbm_{j}^*)\mathbb{X} + c_{j}^{*})(\Mbm^0_{V}+\Zbm_{j}^*)\mathbb{X}  = \sum_{j=1}^{\tilde{L}} \softmax(\mathbb{X}^{\top}(\Mbm^0_{Q}+\Zbm_{j})\mathbb{X} + c_{j})(\Mbm^0_{V}+\Zbm_{j})\mathbb{X}. 
\label{eq:identify_setting_first_neuralnet_linear}
\end{align}
That equation implies that $L = \tilde{L}$. As a consequence, we find that
\begin{align*}
    \{\softmax(\mathbb{X}^{\top}(\Mbm^0_{Q}+\Zbm_{j}^*)\mathbb{X} + c_{j}^{*}):j \in [L]\} &  =\{\softmax(\mathbb{X}^{\top}(\Mbm^0_{Q}+\Zbm_{j})\mathbb{X} + c_{j}):j\in [L]\},
\end{align*}
for almost surely $\mathbb{X}$. By relabelling the indices, we can assume without loss of generality that for any $j \in [L]$
\begin{align*}
    \softmax(\mathbb{X}^{\top}(\Mbm^0_{Q}+\Zbm_{j}^*)\mathbb{X} + c_{j}^{*}) = \softmax(\mathbb{X}^{\top}(\Mbm^0_{Q}+\Zbm_{j})\mathbb{X} + c_{j}),
\end{align*}
for almost surely $\mathbb{X}$. Given the invariance to translation of the softmax function, the equation~(\ref{eq:identify_setting_first_neuralnet_linear}) leads to
\begin{align}
     \sum_{j = 1}^{L}\exp{(c_{j}^{*})}\exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\Zbm_{j}^*)\mathbb{X})(\Mbm^0_{V}+\Zbm_{j}^*)\mathbb{X} & \nonumber \\
     & \hspace{- 10 em} = \sum_{j = 1}^{L}\exp{(c_{j})}\exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\Zbm_{j})\mathbb{X})(\Mbm^0_{V}+\Zbm_{j})\mathbb{X},
    \label{eq:identify_setting_second_neuralnet_linear}
\end{align}
for almost surely $\mathbb{X}$.


Now, the index set $[L]$ can be partitioned into $\bar{m}$ subsets $\bar{K}_1, \bar{K}_2,\ldots,\bar{K}_{\bar{m}}$ where $\bar{m} \leq L$, such that $\exp{(c_{j})}=\exp{(c_{j'}^{*{}})}$ for any indices $j,j'\in \bar{K}_i$ and $i \in [\bar{m}]$. Thus, equation~(\ref{eq:identify_setting_second_neuralnet_linear}) can be rewritten as follows:
\begin{align*}
    \sum_{i = 1}^{\bar{m}}\sum_{j \in \bar{K}_i}\exp{(c_{j}^{*})}\exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\Zbm_{j}^*)\mathbb{X})(\Mbm^0_{V}+\Zbm_{j}^*)\mathbb{X} & \nonumber \\
& \hspace{-10 em} = \sum_{i = 1}^{\bar{m}}\sum_{j \in \bar{K}_i}\exp{(c_{j})}\exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\Zbm_{j})\mathbb{X})(\Mbm^0_{V}+\Zbm_{j})\mathbb{X},
\end{align*}
for almost surely $\mathbb{X}$. The above equation implies that 
\begin{align*}
    \{(\Mbm^0_{V}+\Zbm_{j}^*)\mathbb{X}: j \in \bar{K}_i\} = \{(\Mbm^0_{V}+\Zbm_{j})\mathbb{X}: j \in \bar{K}_i\}, 
\end{align*}
for any $i \in [\bar{m}]$ and for almost surely $\mathbb{X}$. 
% Since the activation function $\sigma$ is identifiable, the above result indicates that 
% \begin{align*}
%     \{(W_{1j},W_{2}): j \in K_i\} = \{(W_{*,1j'},W_{2}): j' \in K_i\}, 
% \end{align*}
% which directly leads to
% \begin{align*}
%     \{W_{1}\prompt_{j}: j \in K_i\} = \{W_{*,1}\prompt_{*,j}: j \in K_i\} \quad \text{and} \quad \{W_{2}\prompt_{j}: j \in K_i\} = \{W_{*,2}\prompt_{*,j}: j \in K_i\}.
% \end{align*}
%Without loss of generality, we assume that $W_{1}\prompt_{j}=W_{*,1}\prompt_{*,j}$ and $W_{2}\prompt_{j}=W_{*,2}\prompt_{*,j}$ for all $j \in K_i$. 
Hence, we obtain that
\begin{align*}
    \sum_{i = 1}^{\bar{m}}\sum_{j \in \bar{K}_i}\exp{(c_{j})}\delta_{\Zbm_{j}} = \sum_{i = 1}^{\bar{m}}\sum_{j \in \bar{K}_i}\exp{(c_{j}^{*})}\delta_{\Zbm_{j}^*}.
\end{align*}
As a consequence, $G \equiv G_*$ and the proof is completed.

\subsection{Proof of Theorem~\ref{theorem:param_rate_nonlinear}}
\label{appendix:param_rate_nonlinear}
% {\color{blue} Fix the proof to $\sigma_{1}(W_{1}A)$ and $\sigma_{2}(W_{2}B)$.} 
Firstly, we can reduce to the case where $\Wbm_1, \Wbm_2$ are identity matrices. In particular, we may denote $\sigma'_1(\Xbm) = \sigma_1(\Wbm_1 \Xbm)$ for input $\Xbm$, and consider $\sigma'_1$ in the place of $\sigma_1$. We first start with the following result regarding the convergence rate of the regression function estimation $f_{\widetilde{G}_{n}}$ to the true regression function $f_{\widetilde{G}_{*}}$:
\begin{proposition}
\label{prop:regression_estimation_nonlinear}
     Given the least square estimator $\widetilde{G}_{n}$ in equation~(\ref{eq:least_squared_estimator_nonlinear}), the convergence rate of the regression function estimation $f_{\widetilde{G}_n}(\cdot)$ to the true regression function $f_{\widetilde{G}_*}(\cdot)$ under the $L^2(\mu)$ norm is given by:
    \begin{align}
        \label{eq:model_bound_2_nonlinear}
        \normf{f_{\widetilde{G}_n}-f_{\widetilde{G}_*}}=\mathcal{O}_{P}(\sqrt{\log(n)/n}).
    \end{align}
\end{proposition}
Given the rate of convergence of the regression function estimator $f_{\widetilde{G}_{n}}$ in Proposition~\ref{prop:regression_estimation_nonlinear}, our goal is to demonstrate the following inequality:
\begin{align*}
\inf_{\widetilde{G}\in \widetilde{\mathcal{G}}_{L'}(\Theta)} \normf{f_{\widetilde{G}}-f_{\widetilde{G}_*}}/\mathcal{D}_3(\widetilde{G},\widetilde{G}_*) >0.
\end{align*}
The proof of that inequality also proceeds into local and global parts, similar to that of Theorem~\ref{theorem:param_rate_linear}. 
\subsubsection{Local Part}
For the local part, we prove that
$$\lim_{\varepsilon\to0} \inf_{\widetilde{G}\in\mathcal{G}_{L'}(\Theta): \mathcal{D}_3(\widetilde{G},\widetilde{G}_*)\leq \varepsilon} \normf{f_{\widetilde{G}}-f_{\widetilde{G}_*}}/\mathcal{D}_3(\widetilde{G},\widetilde{G}_*) >0.$$
Assume that the above claim does not hold. It indicates that we can find a sequence of mixing measures $\widetilde{G}_{n} := \sum_{j' = 1}^{L'} \exp(c_{n,j'}) \delta_{\Bbm_{n,j'}\Abm_{n,j'}}$ in $\widetilde{\mathcal{G}}_{L'}(\Theta)$ such that 
$$\left\{\begin{matrix}
 \mathcal{D}_{3n}:=\mathcal{D}_3(\widetilde{G}_n,\widetilde{G}_*) \to 0, \\
 \normf{f_{\widetilde{G}_n}-f_{\widetilde{G}_*}}/\mathcal{D}_{3n} \to 0.
\end{matrix}\right.$$
as $n \to \infty$.  We denote $\mathcal{V}_j^n:= \mathcal{V}_j(\widetilde{G}_n)$ as a Voronoi cell of $\widetilde{G}_n$ generated by the $j$-th components of $\widetilde{G}_*$. Without loss of generality, we may assume that those Voronoi cells do not depend on the sample size, i.e., $\mathcal{V}_j = \mathcal{V}_j^n$. Therefore, the Voronoi loss $\mathcal{D}_{3n}$ can be rewritten as follows:
\begin{align*}
    \mathcal{D}_{3n}  : =\sum_{j'=1}^{L}\Big|\sum_{i\in\mathcal{V}_{j'}}\exp(c_{n,i})-\exp(c_{j'}^{*})\Big| &+\sum_{j'\in[L]:|\mathcal{V}_{j'}|=1}\sum_{i\in\mathcal{V}_{j'}}\exp(c_{n,i})(\|\Delta \Bbm_{n,ij'}\| +\|\Delta \Abm_{n,ij'}\|) \nonumber\\
    &+\sum_{j'\in[L]:|\mathcal{V}_{j'}|>1}\sum_{i\in\mathcal{V}_{j'}}\exp(c_{n,i})(\|\Delta \Bbm_{n,ij'}\|^{2} +\|\Delta \Abm_{n,ij'}\|^{2}) ,
\end{align*}
where $\Delta \Bbm_{n,ij'} := \Bbm_{n,i} - \Bbm_{j'}^{*}$ and $\Delta \Abm_{n,ij'}:= \Abm_{n,i}-\Abm_{j'}^{*}$ for all $i \in \mathcal{V}_{j'}$ and $j'\in[L]$.

Since $\mathcal{D}_{3n} \to 0$, we have $\sum_{i\in\mathcal{V}_{j}}\exp(c_{n,i})\to\exp(c^*_j)$, $\Bbm_{n,i} \to \Bbm_{j}^{*} $, and $\Abm_{n,i}\to\Abm_{j}^{*}$ for any index $i$ in the Voronoi cell $\mathcal{V}_{j}$ and for any index $j \in [L]$. Throughout this proof, we assume without loss of generality that $M^{0}_{K,j}=I_{\bar{d}}$ with a note that our techniques can be extended to the general setting of that matrix.
Now, the proof of the local part is divided into three steps as follows:

\paragraph{Step 1 - Taylor expansion.} First, we define
$$Q_n(\mathbb{X}):=\Big[\sum_{k = 1}^{L} \exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\sigma_2(\Bbm_{k}^*)\sigma_1(\Abm_{k}^*))\mathbb{X}+c^*_{k})\Big]\cdot[f_{\widetilde{G}_n}(\mathbb{X})-f_{\widetilde{G}_*}(\mathbb{X})].$$  Then, we can decompose the function $Q_n(\mathbb{X})$ as follows:
\begin{align}
    &Q_n(\mathbb{X})=\sum_{j=1}^{L}\sum_{i\in\mathcal{V}_j}\exp(c_{n,i}) \Big[\exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\sigma_2(\Bbm_{n,i})\sigma_1(\Abm_{n,i}))\mathbb{X}) (\Mbm^0_{V}+\sigma_2(\Bbm_{n,i})\sigma_1(\Abm_{n,i}))\mathbb{X} \nonumber\\
    &\hspace{4cm}- \exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\sigma_2(\Bbm_{j}^*)\sigma_1(\Abm_{j}^*))\mathbb{X}) (\Mbm^0_{V}+\sigma_2(\Bbm_{j}^*)
\sigma_1(\Abm_{j}^*))\mathbb{X}\Big] \nonumber \\
    &-\sum_{j=1}^{L}\sum_{i\in\mathcal{V}_j}\exp(c_{n,i})\Big[\exp(\mathbb{X}^{\top}(\Mbm^0_{Q,i}+\sigma_2(\Bbm_{n,i})\sigma_1(\Abm_{n,i}))\mathbb{X}) -\exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\sigma_2(\Bbm_{j}^*)\sigma_1(\Abm_{j}^*))\mathbb{X})\Big]f_{\widetilde{G}_n}(\mathbb{X}) \nonumber \\
    &+\sum_{j=1}^{L}\Big(\sum_{i\in\mathcal{V}_j}\exp(c_{n,i})-\exp(c_{j}^{*})\Big)\exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\sigma_2(\Bbm_{j}^*)\sigma_1(\Abm_{j}^*))\mathbb{X})\Big[(\Mbm^0_{V}+\sigma_2(\Bbm_{j}^*)\sigma_1(\Abm_{j}^*))\mathbb{X} -f_{\widetilde{G}_n}(\mathbb{X})\Big] \nonumber \\
    &:=\widetilde{A}_n(\mathbb{X})-\widetilde{B}_n(\mathbb{X})+ \widetilde{C}_n(\mathbb{X}). \label{eq:main_equation_nonlinear}
\end{align}
\paragraph{Decomposition of the function $\widetilde{A}_n(\mathbb{X})$.} We denote $\widetilde{U}(\mathbb{X}; \Bbm,\Abm) : = \exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\sigma_2(\Bbm)\sigma_1(\Abm))\mathbb{X})$ and $\widetilde{V}(\mathbb{X};\Bbm,\Abm) := (\Mbm^0_{V}+\sigma_2(\Bbm)\sigma_1(\Abm))\mathbb{X}$, and $F(\mathbb{X};\Bbm,\Abm)= \widetilde{U}(\mathbb{X}; \Bbm,\Abm) \widetilde{V}(\mathbb{X};\Bbm,\Abm)$. Based on the number of elements in each Voronoi cells, we decompose the function $\widetilde{A}_n(\mathbb{X})$ as follows:
\begin{align*}
    \widetilde{A}_n(\mathbb{X}) &=\sum_{j:|\mathcal{V}_j|=1}\sum_{i\in\mathcal{V}_j}\exp(c_{n,i})\Big[F(\mathbb{X};\Bbm_{n,i},\Abm_{n,i})-F(\mathbb{X};\Bbm_{j}^{*},\Abm_{j}^{*})\Big]\\
    & \hspace{3 em} + \sum_{j:|\mathcal{V}_j|>1}\sum_{i\in\mathcal{V}_j}\exp(c_{n,i})\Big[F(\mathbb{X};\Bbm_{n,i},\Abm_{n,i})-F(\mathbb{X};\Bbm_{j}^{*},\Abm_{j}^{*})\Big]\\
    &:= \widetilde{A}_{n,1}(\mathbb{X}) + \widetilde{A}_{n,2}(\mathbb{X})
\end{align*}
An application of the first-order Taylor expansion leads to
\begin{align*}
    \widetilde{U}(\mathbb{X};\Bbm_{n,i},\Abm_{n,i}) & = \widetilde{U}(\mathbb{X};\Bbm_{j}^{*},\Abm_{j}^{*}) + \sum_{|\alpha|=1} (\Delta\Abm_{n,ij})^{\alpha_1}(\Delta  \Bbm_{n,ij})^{\alpha_2} \dfrac{\partial^{|\alpha|}\widetilde{U}}{\partial{\Abm^{\alpha_1}}\partial{\Bbm^{\alpha_2}}}(\mathbb{X};\Bbm_{j}^{*},\Abm_{j}^{*}) + \widetilde{R}_{ij,1}(\mathbb{X}), \\
    \widetilde{V}(\mathbb{X};\Bbm_{n,i},\Abm_{n,i}) & = \widetilde{V}(\mathbb{X};\Bbm_{j}^{*},\Abm_{j}^{*}) + \sum_{|\alpha|=1} (\Delta\Abm_{n,ij})^{\alpha_1}(\Delta  \Bbm_{n,ij})^{\alpha_2} \dfrac{\partial^{|\alpha|}\widetilde{V}}{\partial{\Abm^{\alpha_1}}\partial{\Bbm^{\alpha_2}}}(\mathbb{X};\Bbm_{j}^{*},\Abm_{j}^{*}) + \widetilde{R}_{ij,2}(\mathbb{X}),
\end{align*}
for any $i$ and $j$ such that $i \in \mathcal{V}_{j}$ and $|\mathcal{V}_{j}| = 1$. Here, the functions $\widetilde{R}_{ij,1}(\mathbb{X})$ and $\widetilde{R}_{ij, 2}(\mathbb{X})$ denote the Taylor remainders. Collecting the above results leads to
\begin{align*}
    \widetilde{A}_{n,1}(\mathbb{X}) &= \sum_{j:|\mathcal{V}_j|=1}\sum_{i\in\mathcal{V}_j} \dfrac{\exp(c_{n,i})}{\alpha!} \sum_{|\alpha|=1} \biggr\{(\Delta\Abm_{n,ij})^{\alpha_1}(\Delta  \Bbm_{n,ij})^{\alpha_2}\dfrac{\partial^{|\alpha|}\widetilde{U}}{\partial{\Abm^{\alpha_1}}\partial{\Bbm^{\alpha_2}}}(\mathbb{X};\Bbm_{j}^{*},\Abm_{j}^{*}) \widetilde{V}(\mathbb{X};\Bbm_{j}^{*},\Abm_{j}^{*}) \\
    & + (\Delta\Abm_{n,ij})^{\alpha_1}(\Delta  \Bbm_{n,ij})^{\alpha_2}\dfrac{\partial^{|\alpha|}\widetilde{V}}{\partial{\Abm^{\alpha_1}}\partial{\Bbm^{\alpha_2}}}(\mathbb{X};\Bbm_{j}^{*},\Abm_{j}^{*}) \widetilde{U}(\mathbb{X};\Bbm_{j}^{*},\Abm_{j}^{*})\biggr\} + \widetilde{R}_{n,1}(\mathbb{X})\\
    &=\sum_{j:|\mathcal{V}_j|=1}\sum_{|\alpha|=1} \biggr\{ \widetilde{M}_{n,j,\alpha}\dfrac{\partial^{|\alpha|}\widetilde{U}}{\partial {\Abm^{\alpha_1}}\partial{\Bbm^{\alpha_2}}}(\mathbb{X};\Bbm_{j}^{*},\Abm_{j}^{*}) \widetilde{V}(\mathbb{X};\Bbm_{j}^{*},\Abm_{j}^{*}) \\
    & + \widetilde{M}_{n,j,\alpha}\dfrac{\partial^{|\alpha|}\widetilde{V}}{\partial{\Abm^{\alpha_1}}\partial{\Bbm^{\alpha_2}}}(\mathbb{X};\Bbm_{j}^{*},\Abm_{j}^{*}) \widetilde{U}(\mathbb{X};\Bbm_{j}^{*},\Abm_{j}^{*})\biggr\} + \widetilde{R}_{n,1}(\mathbb{X})
\end{align*}
where the function $\widetilde{R}_{n,1}(\mathbb{X})$ satisfies that $\widetilde{R}_{n,1}(\mathbb{X})/\mathcal{D}_{3n} \to 0$. It is due to the uniform Lipschitz property of the function $F$. In the above display, the formulations of the coefficients $\widetilde{M}_{n,j,\alpha}$ are given by:
\begin{align*}
\widetilde{M}_{n,j,\alpha_1,\alpha_2}=\sum_{i\in\mathcal{V}_j} \dfrac{\exp(c_{n,i})}{\alpha!} (\Delta\Abm_{n,ij})^{\alpha_1}(\Delta  \Bbm_{n,ij})^{\alpha_2}, 
\end{align*}
for any $|\alpha| = 1$.


Moving to the function $\widetilde{A}_{n,2}(\mathbb{X})$, an application of the Taylor expansion up to the second order leads to
\begin{align*}
\widetilde{A}_{n,2}(\mathbb{X}) & = \sum_{j:|\mathcal{V}_j|>1}\sum_{1\leq |\alpha|\leq 2} \biggr\{\widetilde{M}_{n,j,\alpha_1,\alpha_2}\dfrac{\partial^{|\alpha|}\widetilde{U}}{\partial{\Abm^{\alpha_1}}\partial{\Bbm^{\alpha_2}}}(\mathbb{X};\Bbm_{j}^{*},\Abm_{j}^{*}) \widetilde{V}(\mathbb{X};\Bbm_{j}^{*},\Abm_{j}^{*}) \\
& + \widetilde{M}_{n,j,\alpha_1,\alpha_2}\dfrac{\partial^{|\alpha|}\widetilde{V}}{\partial{\Abm^{\alpha_1}}\partial{\Bbm^{\alpha_2}}}(\mathbb{X};\Bbm_{j}^{*},\Abm_{j}^{*}) \widetilde{U}(\mathbb{X};\Bbm_{j}^{*},\Abm_{j}^{*}) \biggr\} \\
& + \sum_{|\alpha| = 1, |\beta| = 1} \widetilde{M}_{n,j,\alpha_1,\beta_1,\alpha_2,\beta_2} \dfrac{\partial^{|\alpha|}\widetilde{U}}{\partial{\Abm^{\alpha_1}}\partial{\Bbm^{\alpha_2}}}(\mathbb{X};\Bbm_{j}^{*},\Abm_{j}^{*}) \dfrac{\partial^{|\beta|}\widetilde{V}}{\partial {\Abm^{\beta_1}}\partial{\Bbm^{\beta_2}}}(\mathbb{X};\Bbm_{j}^{*},\Abm_{j}^{*})  + \widetilde{R}_{n,2}(\mathbb{X})
\end{align*}
where the remainder $\widetilde{R}_{n,2}(\mathbb{X})$ satisfies that $\widetilde{R}_{n,2}(\mathbb{X})/\mathcal{D}_{3n} \to 0$. In this equation, the coefficients $\widetilde{M}_{n,j,\alpha_1,\alpha_2}$ and $\widetilde{M}_{n,j,\alpha_1,\beta_1,\alpha_2,\beta_2}$ take the following forms:
\begin{align*}   \widetilde{M}_{n,j,\alpha_1,\alpha_2}=\sum_{i\in\mathcal{V}_j} \dfrac{\exp(c_{n,i})}{\alpha!}(\Delta\Abm_{n,ij})^{\alpha_1}(\Delta  \Bbm_{n,ij})^{\alpha_2}, 
% \\
% M_{n,j,\alpha}^{(2)}=\sum_{i\in\mathcal{V}_j} \dfrac{\exp(b_{n,i})}{\alpha!} (\Delta W_{n,2}W_{n,1ij})^{\alpha},
\end{align*}
for any $|\alpha| = 2$ and
\begin{align*}
    \widetilde{M}_{n,j,\alpha_1,\beta_1,\alpha_2,\beta_2} = \sum_{i\in\mathcal{V}_j} \dfrac{\exp(c_{n,i})}{\alpha! \beta!} 
    (\Delta \Abm_{n,ij})^{\alpha_1 + \beta_1}(\Delta \Bbm_{n,ij})^{\alpha_2 + \beta_2}, 
\end{align*}
for any $|\alpha| = |\beta| = 1$. Simple algebra leads to the following formulations of the partial derivatives of $\widetilde{U}(\mathbb{X}; \Bbm,\Abm)$ and $\widetilde{V}(\mathbb{X};\Bbm,\Abm)$:
\begin{align*}
    \dfrac{\partial \widetilde{U}}{\partial{\Abm^{(u)}}}(\mathbb{X};\Bbm,\Abm) & = \mathbb{X}^{(u)}\sigma_1'(\Abm^{(u)})\mathbb{X}^{\top}\sigma_2(\Bbm)\exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\sigma_2(\Bbm)\sigma_1(\Abm))\mathbb{X}),  \\
    \dfrac{\partial \widetilde{U}}{\partial{\Bbm^{(u)}}}(\mathbb{X};\Bbm,\Abm) & = \mathbb{X}^{(u)}\sigma_2'(\Bbm^{(u)})\sigma_1(\Abm)\mathbb{X}\exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\sigma_2(\Bbm)\sigma_1(\Abm))\mathbb{X}),  \\
     \dfrac{\partial^{2} \widetilde{U}}{\partial {\Abm^{(u)}}\partial {\Abm^{(v)}}}(\mathbb{X};\Bbm,\Abm) & = \Big[\mathbb{X}^{(u)}\mathbb{X}^{(v)}\sigma_1'(\Abm^{(u)})\sigma_1'(\Abm^{(v)})\big(\mathbb{X}^{\top}\sigma_2(\Bbm)\big)^2+\mathbf{1}_{\{u=v\}}\mathbb{X}^{(u)}\sigma_1''(\Abm^{(u)})\mathbb{X}^{\top}\sigma_2(\Bbm)\Big]\\
     &\hspace{5cm}\times\exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\sigma_2(\Bbm)\sigma_1(\Abm))\mathbb{X}),\\
     \dfrac{\partial^{2} \widetilde{U}}{\partial {\Bbm^{(u)}}\partial {\Bbm^{(v)}}}(\mathbb{X};\Bbm,\Abm) & = \Big[\mathbb{X}^{(u)}\mathbb{X}^{(v)}\sigma_2'(\Bbm^{(u)})\sigma_2'(\Bbm^{(v)})\big(\sigma_1(\Abm)\mathbb{X}\big)^2+\mathbf{1}_{\{u=v\}}\mathbb{X}^{(u)}\sigma_2''(\Bbm^{(u)})\sigma_1(\Abm)\mathbb{X}\Big]\\
     &\hspace{5cm}\times\exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\sigma_2(\Bbm)\sigma_1(\Abm))\mathbb{X}),\\
     \dfrac{\partial^{2} \widetilde{U}}{\partial {\Abm^{(u)}}\partial {\Bbm^{(v)}}}(\mathbb{X};\Bbm,\Abm) & = \Big[\mathbb{X}^{(u)}\mathbb{X}^{(v)}\sigma_1'(\Abm^{(u)})\sigma_2'(\Bbm^{(v)})+\mathbb{X}^{(u)}\mathbb{X}^{(v)}\sigma_1'(\Abm^{(u)})\sigma_2'(\Bbm^{(v)})\mathbb{X}^{\top}\sigma_2(\Bbm)\sigma_1(\Abm))\mathbb{X}\Big]\\
     &\hspace{5cm}\times\exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\sigma_2(\Bbm)\sigma_1(\Abm))\mathbb{X}), \\
    \dfrac{\partial \widetilde{V}}{\partial{\Abm^{(u)}}}(\mathbb{X};\Bbm,\Abm) & = \mathbb{X}^{(u)}\sigma_1'(\Abm^{(u)})\sigma_2(\Bbm),\\
    \dfrac{\partial \widetilde{V}}{\partial{\Bbm^{(u)}}}(\mathbb{X};\Bbm,\Abm) & = \sigma_1(A)\mathbb{X}\sigma_2'(\Bbm^{(u)})e_u,\\
    \dfrac{\partial^2 \widetilde{V}}{\partial{\Abm^{(u)}}\partial{\Abm^{(v)}}}(\mathbb{X};\Bbm,\Abm) & = \mathbf{1}_{\{u=v\}}\mathbb{X}^{(u)}\sigma_1''(\Abm^{(u)})\sigma_2(\Bbm),\\
    \dfrac{\partial^2 \widetilde{V}}{\partial{\Bbm^{(u)}}\partial{\Bbm^{(v)}}}(\mathbb{X};\Bbm,\Abm) & = \mathbf{1}_{\{u=v\}}\sigma_1(A)\mathbb{X}\sigma_2''(\Bbm^{(u)})e_u,\\
    \dfrac{\partial^2 \widetilde{V}}{\partial{\Abm^{(u)}}\partial{\Bbm^{(v)}}}(\mathbb{X};\Bbm,\Abm) & = \mathbb{X}^{(u)}\sigma_1'(\Abm^{(u)})\sigma_2'(\Bbm^{(v)})e_v.
\end{align*}
% \begin{align*}
%     \dfrac{\partial H}{\partial {(W_{2}W_{1})^{(u_1v_1)}}}(\mathbb{X};W_{2}W_{1}) & = \mathbb{X}^{(u_1)}\mathbb{X}^{(v_1)}, \\
%     \dfrac{\partial H}{\partial {(W_{2}W_{1})^{(u_1v_1)}}\partial {(W_{2}W_{1})^{(u_2v_2)}}}(\mathbb{X};W_{2}W_{1}) & =0.
% \end{align*}
Plugging these formulations into the functions $\widetilde{A}_{n, 1}(\mathbb{X})$ and $\widetilde{A}_{n,2}(\mathbb{X})$, we obtain that
\begin{align*}
\widetilde{A}_{n, 1}(\mathbb{X}) &= \sum_{j:|\mathcal{V}_{j}| = 1} \exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\sigma_2(\Bbm_{j}^*)\sigma_1(\Abm_{j}^*))\mathbb{X})\Big[\big(L_{n,1,j}^{\top}\mathbb{X}\mathbb{X}^{\top}\sigma_2(\Bbm^*_j)\\&+L_{n,2,j}^{\top}\mathbb{X}\sigma_1(\Abm^*_j)\mathbb{X}\big) (\Mbm^0_{V}+\sigma_2(\Bbm_{j}^*)\sigma_1(\Abm_{j}^*))\mathbb{X}+L_{n,1,j}^{\top}\mathbb{X}\sigma_2(\Bbm^*_j)+\sigma_1(\Abm^*_j)\mathbb{X}L_{n,2,j}\Big]+\widetilde{R}_{n,1}(\mathbb{X}), \\
\widetilde{A}_{n, 2}(\mathbb{X}) &= \sum_{j:|\mathcal{V}_{j}| > 1} \exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\sigma_2(\Bbm_{j}^*)\sigma_1(\Abm_{j}^*))\mathbb{X}) \Big[\big(L_{n,1,j}^{\top}\mathbb{X}\mathbb{X}^{\top}\sigma_2(\Bbm^*_j)+L_{n,2,j}^{\top}\mathbb{X}\sigma_1(\Abm^*_j)\mathbb{X}\\&+\mathbb{X}^{\top}L_{n,3,j}\mathbb{X}(\mathbb{X}^{\top}\sigma_2(\Bbm^*_j))^2 +L_{n,4,j}^{\top}\mathbb{X}\mathbb{X}^{\top}\sigma_2(\Bbm^*_j)+\mathbb{X}^{\top}L_{n,5,j}\mathbb{X}(\sigma_1(\Abm^*_j)\mathbb{X})^2+L_{n,6,j}^{\top}\mathbb{X}\sigma_1(\Abm^*_j)\mathbb{X}\\&+\mathbb{X}^{\top}L_{n,7,j}\mathbb{X}+\mathbb{X}^{\top}L_{n,7,j}\mathbb{X}\mathbb{X}^{\top}\sigma_2(\Bbm^*_j)\sigma_1(\Abm^*_j)\mathbb{X}\big)\times (\Mbm^0_{V}+\sigma_2(\Bbm_{j}^*)\sigma_1(\Abm_{j}^*))\mathbb{X}+L_{n,1,j}^{\top}\mathbb{X}\sigma_2(\Bbm^*_j)\\&+\sigma_1(\Abm^*_j)\mathbb{X}L_{n,2,j}+L_{n,4,j}^{\top}\mathbb{X}\sigma_2(\Bbm^*_j)+\sigma_1(\Abm^*_j)\mathbb{X}L_{n,6,j}+L_{n,7,j}^{\top}\mathbb{X}\Big]+ \widetilde{R}_{n,2}(\mathbb{X}),
\end{align*}
where the formulations of $L_{n,1,j},L_{n,2,j},\ldots,L_{n,6,j}$ are given by:
\begin{align*}
    L_{n,1,j} & := (\widetilde{M}_{n,j,e_u,0_d}\sigma_1'(\Abm^{(u)}))_{u=1}^{d}, \\
    L_{n,2,j}& := (\widetilde{M}_{n,j,0_d,e_u}\sigma_2'(\Bbm^{(u)}))_{u=1}^{d}, \\
    L_{n,3,j}&:=(\widetilde{M}_{n,j,e_u+e_v,0_d}\sigma_1'(\Abm^{(u)})\sigma_1'(\Abm^{(v)}))_{u,v=1}^{d},\\
    L_{n,4,j}&:=(\widetilde{M}_{n,j,2e_u,0_d}\sigma_1''(\Abm^{(u)}))_{u=1}^{d},\\
    L_{n,5,j}&:=(\widetilde{M}_{n,j,0_d,e_u+e_v}\sigma_2'(\Bbm^{(u)})\sigma_2'(\Bbm^{(v)}))_{u,v=1}^{d},\\
    L_{n,6,j}&:=(\widetilde{M}_{n,j,0_d,2e_u}\sigma_2''(\Bbm^{(u)}))_{u=1}^{d},\\
    L_{n,7,j}&:=(\widetilde{M}_{n,j,e_u,e_v}\sigma_1'(\Abm^{(u)})\sigma_2'(\Bbm^{(v)}))_{u,v=1}^{d}.
\end{align*}
% the partial derivatives of the function $F(\mathbb{X}; .)$ up to the second order are given by:
% \begin{align*}
% \dfrac{\partial F}{\partial \prompt^{(u)}}(\mathbb{X};\prompt)&= \exp((B\sigma_1(\prompt))^{\top}\mathbb{X}) \Big[((B \dfrac{\partial{\sigma_{1}}}{\partial{\prompt^{(u)}}}(\prompt))^{\top}\mathbb{X})C\sigma_2(\prompt) + C \dfrac{\partial{\sigma_{2}}}{\partial{\prompt^{(u)}}}  (\prompt) \Big], \\
% \dfrac{\partial^2 F}{\partial \prompt^{(u)}\partial \prompt^{(v)}}(\mathbb{X};\prompt)&=
% \exp((B\sigma_1(\prompt))^{\top}\mathbb{X}) \biggr\{\Big[((B \dfrac{\partial{\sigma_{1}}}{\partial{\prompt^{(u)}}}(\prompt))^{\top}\mathbb{X})C\sigma_2(\prompt) + C \dfrac{\partial{\sigma_{2}}}{\partial{\prompt^{(u)}}}  (\prompt) \Big]((B \dfrac{\partial{\sigma_{1}}}{\partial{\prompt^{(v)}}}(\prompt))^{\top}\mathbb{X}) \\
% & + ((B \dfrac{\partial^2{\sigma_{1}}}{\partial{\prompt^{(u)}}\partial{\prompt^{(v)}}}(\prompt))^{\top}\mathbb{X})C\sigma_2(\prompt) + ((B \dfrac{\partial{\sigma_{1}}}{\partial{\prompt^{(u)}}}(\prompt))^{\top}\mathbb{X})C\dfrac{\partial{\sigma_{2}}}{\partial{\prompt^{(v)}}}  (\prompt) + C \dfrac{\partial^2{\sigma_{2}}}{\partial{\prompt^{(u)}}\partial{\prompt^{(v)}}}(\prompt) \biggr\}
% \exp((B\sigma_1(\prompt_{*,j}))^{\top}\mathbb{X})\Big[((B\sigma_1^{''}(\prompt_{*,j}))^{\top}\mathbb{X}^{(u)}\mathbb{X}^{(v)})C\sigma_2(\prompt_{*,j}) +C\sigma_2^{''}(\prompt_{*,j})\\
% &+((B\sigma_1^{'}(\prompt_{*,j}))^{\top}\mathbb{X}^{(u)})C\sigma_2^{'}(\prompt_{*,j})+ +((B\sigma_1^{'}(\prompt_{*,j}))^{\top}\mathbb{X}^{(v)})C\sigma_2^{'}(\prompt_{*,j})\\
% &+\left((B\sigma_1^{'}(\prompt_{*,j}))^{\top}\right)^2\mathbb{X}^{(u)}\mathbb{X}^{(v)} C\sigma_2(\prompt_{*,j})  \Big]
% \end{align*}
%for any $1 \leq u, v \leq d$. Finally, we also have
%$$M_{n,j,\alpha}=\sum_{i\in\mathcal{V}_j} \dfrac{\exp(b_{n,i})}{\alpha!} (\Delta\prompt_{n,ij})^{\alpha},$$ for any $1 \leq |\alpha| \leq 2$.
In these equations, $e_{u}$ is denoted as the vector in $\mathbb{R}^{\bar{d}}$ such that its $u$-th element is 1 while its other elements are 0 for any $1 \leq u \leq \bar{d}$. Furthermore, $e_{uv}$ is denoted as matrix in $\mathbb{R}^{\bar{d} \times \bar{d}}$ with its $uv$-th entry is 1 while other entries are zero.  
%Furthermore, $1_{uv}$ is the matrix that its $(u,v)$-th element is 1 while its other elements are 0 for any $1 \leq u,v \leq d$. The second equation in the formulation of $\bar{L}_{1,n}(\prompt)$ is due to the fact that the function $\bar{\sigma}_{2}$ is only applied element wise to $W_{2}p$, which leads to $\dfrac{\partial^2{\bar{\sigma}_{2}}}{\partial{(W_{2}\prompt)^{(u)}}\partial{(W_{2}\prompt)^{(v)}}}(W_{2}\prompt) = 0$ for all $u \neq v$. 
\paragraph{Decomposition of the function $\widetilde{B}_n(\mathbb{X})$.}  Moving to the function $\widetilde{B}_n(\mathbb{X})$, we can decompose this function as follows:
\begin{align*}
    \widetilde{B}_n(\mathbb{X}) &=\sum_{j:|\mathcal{V}_j|=1}\sum_{i\in\mathcal{V}_j}\exp(c_{n,i})\Big[\widetilde{U}(\mathbb{X}; \Bbm_{n,i},\Abm_{n,i})-\widetilde{U}(\mathbb{X}; \Bbm_{j}^{*}\Abm_{j}^{*})\Big]f_{\widetilde{G}_n}(\mathbb{X}) \\
    &  +\sum_{j:|\mathcal{V}_j|>1}\sum_{i\in\mathcal{V}_j}\exp(c_{n,i})\Big[\widetilde{U}(\mathbb{X}; \Bbm_{n,i},\Abm_{n,i})-\widetilde{U}(\mathbb{X}; \Bbm_{j}^{*}\Abm_{j}^{*})\Big]f_{\widetilde{G}_n}(\mathbb{X}) \\
    &:= \widetilde{B}_{n,1}(\mathbb{X}) + \widetilde{B}_{n,2}(\mathbb{X}).
\end{align*}
An application of the Taylor expansions up to the first order for $\widetilde{B}_{n,1}(\mathbb{X})$ and the second order for $\widetilde{B}_{n,2}(\mathbb{X})$ leads to
\begin{align*}
    \widetilde{B}_{n,1}(\mathbb{X})&= \sum_{j:|\mathcal{V}_j|=1}\sum_{|\alpha|=1} \widetilde{M}_{n,j,\alpha_1,\alpha_2} \dfrac{\partial^{|\alpha|}\widetilde{U}}{\partial{\Abm^{\alpha_1}}\partial{\Bbm^{\alpha_2}}}(\mathbb{X};\Bbm_{j}^{*},\Abm_{j}^{*})f_{\widetilde{G}_n}(\mathbb{X})+ \widetilde{R}_{n,3}(\mathbb{X}),
    \\
     \widetilde{B}_{n,2}(\mathbb{X})&=\sum_{j:|\mathcal{V}_j|=1}\sum_{1 \leq |\alpha|\leq 2} \widetilde{M}_{n,j,\alpha_1,\alpha_2} \dfrac{\partial^{|\alpha|}\widetilde{U}}{\partial{\Abm^{\alpha_1}}\partial{\Bbm^{\alpha_2}}}(\mathbb{X};\Bbm_{j}^{*},\Abm_{j}^{*})f_{\widetilde{G}_n}(\mathbb{X})+ \widetilde{R}_{n,4}(\mathbb{X})
\end{align*}
where the Taylor remainders $\widetilde{R}_{n,3}(\mathbb{X}), \widetilde{R}_{n,4}(\mathbb{X})$ satisfy that $\widetilde{R}_{n,3}(\mathbb{X})/\mathcal{D}_{3n} \to 0$ and $\widetilde{R}_{n,4}(\mathbb{X})/\mathcal{D}_{3n} \to 0$. Direct calculation leads to
\begin{align*}
     \widetilde{B}_{n,1}(\mathbb{X})  &= \sum_{j:|\mathcal{V}_{j}| = 1} \exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\sigma_2(\Bbm_{j}^*)\sigma_1(\Abm_{j}^*))\mathbb{X}) \Big[L_{n,1,j}^{\top}\mathbb{X}\mathbb{X}^{\top}\sigma_2(\Bbm^*_j)+L_{n,2,j}^{\top}\mathbb{X}\sigma_1(\Abm^*_j)\mathbb{X}\Big]f_{\widetilde{G}_n}(\mathbb{X}) + \widetilde{R}_{n,3}(\mathbb{X}), \\
     \widetilde{B}_{n,2}(\mathbb{X})  &= \sum_{j:|\mathcal{V}_{j}| > 1} \exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\sigma_2(\Bbm_{j}^*)\sigma_1(\Abm_{j}^*))\mathbb{X}) \Big[L_{n,1,j}^{\top}\mathbb{X}\mathbb{X}^{\top}\sigma_2(\Bbm^*_j)\\&+L_{n,2,j}^{\top}\mathbb{X}\sigma_1(\Abm^*_j)\mathbb{X}+\mathbb{X}^{\top}L_{n,3,j}\mathbb{X}(\mathbb{X}^{\top}\sigma_2(\Bbm^*_j))^2 +L_{n,4,j}^{\top}\mathbb{X}\mathbb{X}^{\top}\sigma_2(\Bbm^*_j)+\mathbb{X}^{\top}L_{n,5,j}\mathbb{X}(\sigma_1(\Abm^*_j)\mathbb{X})^2\\&+L_{n,6,j}^{\top}\mathbb{X}\sigma_1(\Abm^*_j)\mathbb{X}+\mathbb{X}^{\top}L_{n,7,j}\mathbb{X}+\mathbb{X}^{\top}L_{n,7,j}\mathbb{X}\mathbb{X}^{\top}\sigma_2(\Bbm^*_j)\sigma_1(\Abm^*_j)\mathbb{X}\Big]f_{\widetilde{G}_n}(\mathbb{X}) + \widetilde{R}_{n,4}(\mathbb{X}),
\end{align*}
% where the formulations of the functions $N_{1,n}$, $\bar{N}_{1,n}$, and $\bar{N}_{2,n}$ are given by:
% \begin{align*}
%     N_{1,n}(\prompt) & = \sum_{u = 1}^{d} M_{n, j, 1_{u}}^{(1)} \dfrac{\partial{\bar{\sigma}_{1}}}{\partial{(W_{1}\prompt)^{(u)}}}(W_{1}\prompt), \\
%     \bar{N}_{1,n}(\prompt) & = \sum_{u = 1}^{d} M_{n, j, 1_{u}}^{(1)}  \dfrac{\partial{\bar{\sigma}_{1}}}{\partial{(W_{1}\prompt)^{(u)}}}(W_{1}\prompt) \\
%     & + \sum_{1 \leq u,v \leq d} M_{n,j,1_{uv}}^{(1)} \dfrac{\partial^2{\bar{\sigma}_{1}}}{\partial{(W_{1}\prompt)^{(u)}}\partial{(W_{1}\prompt)^{(v)}}}(W_{1}\prompt), \\
%     \bar{N}_{2,n}(\prompt) & = \sum_{1 \leq u,v \leq d} M_{n, j, 1_{uv}}^{(1)}  \dfrac{\partial{\bar{\sigma}_{1}}}{\partial{(W_{1}\prompt)^{(u)}}}(W_{1}\prompt)  \dfrac{\partial{\bar{\sigma}_{1}}}{\partial{(W_{1}\prompt)^{(v)}}}(W_{1}\prompt)^{\top}.
% \end{align*}
Putting all the above results together, we can represent the function $Q_n(\mathbb{X})$ as follows: 
\begin{align}
    Q_n(\mathbb{X})&= \sum_{j:|\mathcal{V}_{j}| = 1} \exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\sigma_2(\Bbm_{j}^*)\sigma_1(\Abm_{j}^*))\mathbb{X})\Big[\big(L_{n,1,j}^{\top}\mathbb{X}\mathbb{X}^{\top}\sigma_2(\Bbm^*_j)\nonumber\\&+L_{n,2,j}^{\top}\mathbb{X}\sigma_1(\Abm^*_j)\mathbb{X}\big) (\Mbm^0_{V}+\sigma_2(\Bbm_{j}^*)\sigma_1(\Abm_{j}^*))\mathbb{X}\nonumber+L_{n,1,j}^{\top}\mathbb{X}\sigma_2(\Bbm^*_j)+\sigma_1(\Abm^*_j)\mathbb{X}L_{n,2,j}\Big] \nonumber \\
    &+ \sum_{j:|\mathcal{V}_{j}| > 1} \exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\sigma_2(\Bbm_{j}^*)\sigma_1(\Abm_{j}^*))\mathbb{X}) \Big[\big(L_{n,1,j}^{\top}\mathbb{X}\mathbb{X}^{\top}\sigma_2(\Bbm^*_j)+L_{n,2,j}^{\top}\mathbb{X}\sigma_1(\Abm^*_j)\mathbb{X}\nonumber\\&+\mathbb{X}^{\top}L_{n,3,j}\mathbb{X}(\mathbb{X}^{\top}\sigma_2(\Bbm^*_j))^2 +L_{n,4,j}^{\top}\mathbb{X}\mathbb{X}^{\top}\sigma_2(\Bbm^*_j)+\mathbb{X}^{\top}L_{n,5,j}\mathbb{X}(\sigma_1(\Abm^*_j)\mathbb{X})^2\nonumber\\&+L_{n,6,j}^{\top}\mathbb{X}\sigma_1(\Abm^*_j)\mathbb{X}+\mathbb{X}^{\top}L_{n,7,j}\mathbb{X}+\mathbb{X}^{\top}L_{n,7,j}\mathbb{X}\mathbb{X}^{\top}\sigma_2(\Bbm^*_j)\sigma_1(\Abm^*_j)\mathbb{X}\big)\nonumber\\
&\times (\Mbm^0_{V}+\sigma_2(\Bbm_{j}^*)\sigma_1(\Abm_{j}^*))\mathbb{X}+L_{n,1,j}^{\top}\mathbb{X}\sigma_2(\Bbm^*_j)+\sigma_1(\Abm^*_j)\mathbb{X}L_{n,2,j}+L_{n,4,j}^{\top}\mathbb{X}\sigma_2(\Bbm^*_j)\nonumber\\&+\sigma_1(\Abm^*_j)\mathbb{X}L_{n,6,j}+L_{n,7,j}^{\top}\mathbb{X}\Big] -\sum_{j:|\mathcal{V}_{j}| = 1} \exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\sigma_2(\Bbm_{j}^*)\sigma_1(\Abm_{j}^*))\mathbb{X}) \Big[L_{n,1,j}^{\top}\mathbb{X}\mathbb{X}^{\top}\sigma_2(\Bbm^*_j)\nonumber\\&+L_{n,2,j}^{\top}\mathbb{X}\sigma_1(\Abm^*_j)\mathbb{X}\Big]f_{\widetilde{G}_n}(\mathbb{X})-\sum_{j:|\mathcal{V}_{j}| > 1} \exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\sigma_2(\Bbm_{j}^*)\sigma_1(\Abm_{j}^*))\mathbb{X}) \Big[L_{n,1,j}^{\top}\mathbb{X}\mathbb{X}^{\top}\sigma_2(\Bbm^*_j)\nonumber\\&+L_{n,2,j}^{\top}\mathbb{X}\sigma_1(\Abm^*_j)\mathbb{X}+\mathbb{X}^{\top}L_{n,3,j}\mathbb{X}(\mathbb{X}^{\top}\sigma_2(\Bbm^*_j))^2 +L_{n,4,j}^{\top}\mathbb{X}\mathbb{X}^{\top}\sigma_2(\Bbm^*_j)+\mathbb{X}^{\top}L_{n,5,j}\mathbb{X}(\sigma_1(\Abm^*_j)\mathbb{X})^2\nonumber\\&+L_{n,6,j}^{\top}\mathbb{X}\sigma_1(\Abm^*_j)\mathbb{X}+\mathbb{X}^{\top}L_{n,7,j}\mathbb{X}+\mathbb{X}^{\top}L_{n,7,j}\mathbb{X}\mathbb{X}^{\top}\sigma_2(\Bbm^*_j)\sigma_1(\Abm^*_j)\mathbb{X}\Big]f_{\widetilde{G}_n}(\mathbb{X})\nonumber\\
    &  + \sum_{j = 1}^{L} \widetilde{N}_{n,j} \exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\sigma_2(\Bbm_{j}^*)\sigma_1(\Abm_{j}^*))\mathbb{X})\Big[(\Mbm^0_{V}+\Bbm_{j}^*
\Abm_{j}^*)\mathbb{X}-f_{\widetilde{G}_n}(\mathbb{X})\Big]   \nonumber \\
    \label{eq:main_equation_expression_nonlinear}
    &  + \widetilde{R}_{n,1}(\mathbb{X}) + \widetilde{R}_{n,2}(\mathbb{X}) - \widetilde{R}_{n,3}(\mathbb{X}) - \widetilde{R}_{n,4}(\mathbb{X}),
\end{align}
% \begin{align}
%     & - \sum_{j:|\mathcal{V}_{j}| = 1} \exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\sigma_2(\Bbm_{j}^*)\sigma_1(\Abm_{j}^*))\mathbb{X}) \Big[\sum_{u_1,v_1=1}^{\bar{d}} \widetilde{M}_{n,j,e_{u_1v_1}}\mathbb{X}^{(u_1)}\mathbb{X}^{(v_1)}\Big]f_{\widetilde{G}_n}(\mathbb{X}) \nonumber \\
%     & - \sum_{j:|\mathcal{V}_{j}| > 1} \exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\sigma_2(\Bbm_{j}^*)\sigma_1(\Abm_{j}^*))\mathbb{X}) \Big[\sum_{u_1,v_1=1}^{\bar{d}} \widetilde{M}_{n,j,e_{u_1v_1}}\mathbb{X}^{(u_1)}\mathbb{X}^{(v_1)}\nonumber \\
%     & +\sum_{u_1,v_1=1}^{d}\sum_{u_2,v_2=1}^{d}\widetilde{M}_{n,j,e_{u_1v_1}}\mathbb{X}^{(u_1)}\mathbb{X}^{(v_1)}\mathbb{X}^{(u_2)}\mathbb{X}^{(v_2)}\Big]f_{\widetilde{G}_n}(\mathbb{X})\nonumber \\
    
%     &  + \widetilde{R}_{n,1}(\mathbb{X}) + \widetilde{R}_{n,2}(\mathbb{X}) - \widetilde{R}_{n,3}(\mathbb{X}) - \widetilde{R}_{n,4}(\mathbb{X}) 
%  \label{eq:main_equation_expression_nonlinear}
% \end{align}   
% \sum_{j=1}^{L}\sum_{0\leq|\alpha|\leq 2} M_{n,j,\alpha}\dfrac{\partial^{|\alpha|}F}{\partial \prompt^\alpha}(\mathbb{X};\prompt_{*,j}) - \sum_{j=1}^{L}\sum_{0 \leq |\alpha|\leq 2} M_{n,j,\alpha}\dfrac{\partial^{|\alpha|}E}{\partial \prompt^\alpha}(\mathbb{X};\prompt_{*,j})g_{\widetilde{G}_n}(\mathbb{X}) \\
%    & \hspace{14 em} + \sum_{j=1}^{2}R_{n,j} (\mathbb{X}) - \sum_{j=3}^{4}R_{n,j} (\mathbb{X}) \\    &=\sum_{j=1}^{L}\sum_{0\leq|\alpha|\leq 2} M_{n,j,\alpha} \Big[\dfrac{\partial^{|\alpha|}F}{\partial \prompt^\alpha}(\mathbb{X};\prompt_{*,j}) - \dfrac{\partial^{|\alpha|}E}{\partial \prompt^\alpha}(\mathbb{X};\prompt_{*,j})g_{\widetilde{G}_n}(\mathbb{X})\Big] \\
%    & \hspace{14 em} + \sum_{j=1}^{2}R_{n,j} (\mathbb{X}) - \sum_{j=3}^{4}R_{n,j} (\mathbb{X}).
% \end{align*}
where $\widetilde{N}_{n,j}:=\sum_{i\in\mathcal{V}_j}\exp(c_{n,i})-\exp(c_{j}^{*})$ for any $j \in [L]$. 
%$L_{1,n}'(\prompt_{*,j}) = L_{1,n}(\prompt_{*,j}) + M_{n,j,0_{d}}C\bar{\sigma}_{2}(\mathbb{X};W_{*,2}W_{*,1j})$, and $\bar{L}_{1,n}'(\prompt_{*,j}) = \bar{L}_{1,n}(\prompt_{*,j}) + M_{n,j,0_{d}}C\bar{\sigma}_{2}(\mathbb{X};W_{*,2}W_{*,1j})$.


\paragraph{Step 2 - Non-vanishing coefficients.} 
 As indicated in equation~(\ref{eq:main_equation_expression_nonlinear}),  the ratio $Q_{n}(\mathbb{X})/ \mathcal{D}_{3n}$ can be expressed as a linear combination of the following independent functions:
 \begin{align*}
&\widetilde{U}(\mathbb{X};\Bbm_{j}^{*}\Abm_{j}^{*})\mathbb{X}^{(u)}\mathbb{X}^{\top}\sigma_2(\Bbm^*_j)\widetilde{V}(\mathbb{X};\Bbm^*_j,\Abm^*_j),\quad \widetilde{U}(\mathbb{X};\Bbm_{j}^{*}\Abm_{j}^{*})\mathbb{X}^{(u)}\sigma_1(\Abm^*_j)\mathbb{X}\widetilde{V}(\mathbb{X};\Bbm^*_j,\Abm^*_j), \\  &\widetilde{U}(\mathbb{X}; \Bbm_{j}^{*}\Abm_{j}^{*})\mathbb{X}^{(u)}\sigma_2(\Bbm^*_j), \quad
\widetilde{U}(\mathbb{X}; \Bbm_{j}^{*}\Abm_{j}^{*})\sigma_1(\Abm^*_j)\mathbb{X}e_u, \quad \widetilde{U}(\mathbb{X}; \Bbm_{j}^{*}\Abm_{j}^{*})\mathbb{X}^{(u)}\mathbb{X}^{(v)}(\mathbb{X}^{\top}\sigma_2(\Bbm^*_j))^2\widetilde{V}(\mathbb{X};\Bbm^*_j,\Abm^*_j), \\
&\widetilde{U}(\mathbb{X}; \Bbm_{j}^{*}\Abm_{j}^{*})\mathbb{X}^{(u)}\mathbb{X}^{(v)}(\sigma_1(\Abm^*_j)\mathbb{X})^2\widetilde{V}(\mathbb{X};\Bbm^*_j,\Abm^*_j), \quad \widetilde{U}(\mathbb{X}; \Bbm_{j}^{*}\Abm_{j}^{*})\mathbb{X}^{(u)}\sigma_1(\Abm^*_j)\mathbb{X})\widetilde{V}(\mathbb{X};\Bbm^*_j,\Abm^*_j),\\ 
&\widetilde{U}(\mathbb{X}; \Bbm_{j}^{*}\Abm_{j}^{*})\mathbb{X}^{(u)}\mathbb{X}^{(v)}\widetilde{V}(\mathbb{X};\Bbm^*_j,\Abm^*_j), \quad \widetilde{U}(\mathbb{X}; \Bbm_{j}^{*}\Abm_{j}^{*})\mathbb{X}^{(u)}\mathbb{X}^{(v)}\mathbb{X}^{\top}\sigma_2(\Bbm^*_j)\sigma_1(\Abm^*_j)\mathbb{X}\widetilde{V}(\mathbb{X};\Bbm^*_j,\Abm^*_j),\\
&\widetilde{U}(\mathbb{X};\Bbm_{j}^{*}\Abm_{j}^{*})\mathbb{X}^{(u)}\mathbb{X}^{\top}\sigma_2(\Bbm^*_j)f_{\widetilde{G}_n}(\mathbb{X}),\quad \widetilde{U}(\mathbb{X};\Bbm_{j}^{*}\Abm_{j}^{*})\mathbb{X}^{(u)}\sigma_1(\Abm^*_j)\mathbb{X}f_{\widetilde{G}_n}(\mathbb{X}),\\
&\widetilde{U}(\mathbb{X}; \Bbm_{j}^{*}\Abm_{j}^{*})\mathbb{X}^{(u)}\mathbb{X}^{(v)}(\mathbb{X}^{\top}\sigma_2(\Bbm^*_j))^2f_{\widetilde{G}_n}(\mathbb{X}), \quad \widetilde{U}(\mathbb{X};\Bbm_{j}^{*}\Abm_{j}^{*})\mathbb{X}^{(u)}\mathbb{X}^{\top}\sigma_2(\Bbm^*_j)f_{\widetilde{G}_n}(\mathbb{X}),\\
&\widetilde{U}(\mathbb{X}; \Bbm_{j}^{*}\Abm_{j}^{*})\mathbb{X}^{(u)}\mathbb{X}^{(v)}(\sigma_1(\Abm^*_j)\mathbb{X})^2f_{\widetilde{G}_n}(\mathbb{X}), \quad \widetilde{U}(\mathbb{X}; \Bbm_{j}^{*}\Abm_{j}^{*})\mathbb{X}^{(u)}\sigma_1(\Abm^*_j)\mathbb{X})f_{\widetilde{G}_n}(\mathbb{X}),\\ 
&\widetilde{U}(\mathbb{X}; \Bbm_{j}^{*}\Abm_{j}^{*})\mathbb{X}^{(u)}\mathbb{X}^{(v)}f_{\widetilde{G}_n}(\mathbb{X}), \quad \widetilde{U}(\mathbb{X}; \Bbm_{j}^{*}\Abm_{j}^{*})\mathbb{X}^{(u)}\mathbb{X}^{(v)}\mathbb{X}^{\top}\sigma_2(\Bbm^*_j)\sigma_1(\Abm^*_j)\mathbb{X}f_{\widetilde{G}_n}(\mathbb{X}),\\
&\widetilde{U}(\mathbb{X};\Bbm_{j}^{*}\Abm_{j}^{*})\widetilde{V}(\mathbb{X};\Bbm^*_j,\Abm^*_j), \quad \widetilde{U}(\mathbb{X};\Bbm_{j}^{*}\Abm_{j}^{*})f_{\widetilde{G}_n}(\mathbb{X}),
 \end{align*}
%  \begin{align*}
% & E(\mathbb{X};W_{*,2}W_{*,1j})W_{*,1j}\mathbb{X}W_{*,2}, \ E(\mathbb{X};W_{*,2}W_{*,1j})\sigma^{(1)}(W_{*,1j}\mathbb{X})W_{*,1j}\mathbb{X}\mathbb{X}^{(u)}\mathbb{X}^{(v)} W_{*,2}\\
% &E(\mathbb{X};W_{*,2}W_{*,1j})\sigma^2(W_{*,1j}\mathbb{X})\mathbb{X}^{(u)} W_{*,2}, \ E(\mathbb{X};W_{*,2}W_{*,1j})\sigma^{(1)}(W_{*,1j}\mathbb{X})\mathbb{X}^{(u)}W_{*,2},\\ 
% &E(\mathbb{X};W_{*,2}W_{*,1j})W_{*,1j}\mathbb{X}e_{u}, \  E(\mathbb{X};W_{*,2}W_{*,1j})[\sigma^{(1)}(W_{*,1j}\mathbb{X})]^2W_{*,1j}\mathbb{X}\mathbb{X}^{(u)}\mathbb{X}^{(v)}\mathbb{X}^{(w)}W_{*,2},  \\
% & E(\mathbb{X};W_{*,2}W_{*,1j})\sigma^{(2)}(W_{*,1j}\mathbb{X})W_{*,1j}\mathbb{X}\mathbb{X}^{(u)}\mathbb{X}^{(v)}\mathbb{X}^{(w)}W_{*,2}, \ E(\mathbb{X};W_{*,2}W_{*,1j})\sigma^{(2)}(W_{*,1j}\mathbb{X})\mathbb{X}^{(u)}\mathbb{X}^{(v)}W_{*,2},\\
% &E(\mathbb{X};W_{*,2}W_{*,1j})\sigma^{2}(W_{*,1j}\mathbb{X})W_{*,1j}\mathbb{X}\mathbb{X}^{(u)}\mathbb{X}^{(v)}W_{*,2}, \ E(\mathbb{X};W_{*,2}W_{*,1j})\sigma^{(1)}(W_{*,1j}\mathbb{X})W_{*,1j}\mathbb{X}\mathbb{X}^{(u)}\mathbb{X}^{(v)}W_{*,2},\\
% &E(\mathbb{X};W_{*,2}W_{*,1j})\sigma^{(1)}(W_{*,1j}\mathbb{X})[W_{*,1j}\mathbb{X}]^2\mathbb{X}^{(u)}\mathbb{X}^{(v)}\mathbb{X}^{(w)}W_{*,2}, \ E(\mathbb{X};W_{*,2}W_{*,1j})\sigma^{(2)}(W_{*,1j}\mathbb{X})\mathbb{X}^{(u)}e_{u},\\
% &E(\mathbb{X};W_{*,2}W_{*,1j})[\sigma^{(1)}(W_{*,1j}\mathbb{X})]^2\mathbb{X}^{(u)}\mathbb{X}^{(v)}\mathbb{X}^{(w)}W_{*,2}, \ E(\mathbb{X};W_{*,2}W_{*,1j})\sigma^{(1)}(W_{*,1j}\mathbb{X})W_{*,1j}\mathbb{X}\mathbb{X}^{(u)}\mathbb{X}^{(v)}e_{v},\\
% &E(\mathbb{X};W_{*,2}W_{*,1j})\sigma^{(1)}(W_{*,1j}\mathbb{X})W_{*,1j}\mathbb{X}\mathbb{X}^{(u)}\mathbb{X}^{(v)}W_{*,2}, \ E(\mathbb{X};W_{*,2}W_{*,1j})[W_{*,1j}\mathbb{X}]^2\mathbb{X}^{(u)}e_{u},\\
% &E(\mathbb{X};W_{*,2}W_{*,1j})\sigma^{(1)}(W_{*,1j}\mathbb{X})\mathbb{X}^{(u)}\mathbb{X}^{(v)}f_{\widetilde{G}_n}(\mathbb{X}), \ E(\mathbb{X};W_{*,2}W_{*,1j})W_{*,1j}\mathbb{X}\mathbb{X}^{(u)}f_{\widetilde{G}_n}(\mathbb{X}),\\
% &E(\mathbb{X};W_{*,2}W_{*,1j})[\sigma^{(1)}(W_{*,1j}\mathbb{X})]^2\mathbb{X}^{(u)}\mathbb{X}^{(v)}\mathbb{X}^{(w)}f_{\widetilde{G}_n}(\mathbb{X}), \ E(\mathbb{X};W_{*,2}W_{*,1j})\sigma^{(2)}(W_{*,1j}\mathbb{X})\mathbb{X}^{(u)}\mathbb{X}^{(v)}\mathbb{X}^{(w)}f_{\widetilde{G}_n}(\mathbb{X}),\\
% &E(\mathbb{X};W_{*,2}W_{*,1j})\sigma^{2}(W_{*,1j}\mathbb{X})\mathbb{X}^{(u)}\mathbb{X}^{(v)}f_{\widetilde{G}_n}(\mathbb{X}), \ E(\mathbb{X};W_{*,2}W_{*,1j})\sigma^{(1)}(W_{*,1j}\mathbb{X})\mathbb{X}^{(u)}\mathbb{X}^{(v)}f_{\widetilde{G}_n}(\mathbb{X})\\
% &E(\mathbb{X};W_{*,2}W_{*,1j})\sigma^{(1)}(W_{*,1j}\mathbb{X})W_{*,1j}\mathbb{X}\mathbb{X}^{(u)}\mathbb{X}^{(v)}\mathbb{X}^{(w)}f_{\widetilde{G}_n}(\mathbb{X}), \ E(\mathbb{X};W_{*,2}W_{*,1j})f_{\widetilde{G}_n}(\mathbb{X}),
% \end{align*}
for any indices $1 \leq j \leq L$ and $1 \leq u_{1}, v_{1}, u_{2}, v_{2} \leq \bar{d}$. 
 
We will proof that when $n \to \infty$, at least one of the coefficients of these functions does not go to 0. Assume by contrary that all these coefficients of these linear independent functions go to 0. From equation~(\ref{eq:main_equation_expression_nonlinear}), we obtain that $\widetilde{M}_{n,j,\alpha_1,\alpha_2}/\mathcal{D}_{3n}$, $\widetilde{M}_{n,j,\alpha_1,\beta_1,\alpha_2,\beta_2}/\mathcal{D}_{3n}$, and $\widetilde{N}_{n,j}/\mathcal{D}_{3n}$ go to 0 for all the coefficients $\alpha_1,\beta_1,\alpha_2,\beta_2\in\mathbb{N}^{\bar{d}\times \bar{d}}$ satisfying that $1\leq|\alpha_1|+|\beta_1|+|\alpha_2|+|\beta_2|\leq 2$. 
 
Since $\widetilde{N}_{n,j}/\mathcal{D}_{3n} \to 0$, we find that for any $j \in [L]$
\begin{align*}
     \frac{|\sum_{i\in\mathcal{V}_j}\exp(c_{n,i})-\exp(c_{j}^{*})|}{\mathcal{D}_{3n}}= \frac{|\widetilde{N}_{n,j}|}{\mathcal{D}_{3n}}  \to 0.
\end{align*}
Taking the summation of these limits leads to 
\begin{align}
\label{eq:key_limits_first_2_nonlinear}
\frac{\sum_{j = 1}^{L} |\sum_{i\in\mathcal{V}_j}\exp(c_{n,i})-\exp(c_{j}^{*})|}{\mathcal{D}_{3n}} \to 0. 
\end{align}
Now, for any index $j \in [L]$ such that $|\mathcal{V}_j | = 1$, the limits $\widetilde{M}_{n,j,e_{u},0_d}/\mathcal{D}_{3n} \to 0$ lead to 

\begin{align*}
    \frac{\sum_{i \in \mathcal{V}_{j}} \exp(c_{n,i}) \|\Delta\Abm_{n,ij}\|_1}{\mathcal{D}_{3n}} \to 0.
\end{align*} 
% Similarly, $L_{n,2,j}/ \mathcal{D}_{3n} \to 0$ also leads to $\dfrac{\sum_{i \in \mathcal{V}_{j}} \exp(b_{n,i}) \|\Delta W_{n,2ij}\|_1}{\mathcal{D}_{3n}} \to 0$. Putting the above results together, we find that
% \begin{align*}
%     \frac{\sum_{j: |\mathcal{V}_{j}| = 1} \sum_{i \in \mathcal{V}_{j}} \exp(b_{n,i})(\|\Delta W_{n,1ij}\|_1 + \|\Delta W_{n,2ij}\|_1)}{\mathcal{D}_{3n}} \to 0. 
% \end{align*}
Due to the equivalence between the $\ell_1$-norm and the $\ell_2$-norm, this result directly implies that
\begin{align*}
    \frac{\sum_{j: |\mathcal{V}_{j}| = 1} \sum_{i \in \mathcal{V}_{j}} \exp(c_{n,i})\|\Delta \Abm_{n,ij}\|}{\mathcal{D}_{3n}} \to 0. 
\end{align*}
Similarly, since $\widetilde{M}_{n,j,0_d,e_u}/\mathcal{D}_{3n} \to 0$, we also get that $\frac{\sum_{j: |\mathcal{V}_{j}| = 1} \sum_{i \in \mathcal{V}_{j}} \exp(c_{n,i})\|\Delta \Bbm_{n,ij}\|}{\mathcal{D}_{3n}} \to 0$. Thus, we obtain that
\begin{align}
    \label{eq:linear_loss_nonlinear}
    \frac{\sum_{j: |\mathcal{V}_{j}| = 1} \sum_{i \in \mathcal{V}_{j}} \exp(c_{n,i})(\|\Delta \Abm_{n,ij}\|+\|\Delta \Bbm_{n,ij}\|)}{\mathcal{D}_{3n}} \to 0
\end{align}
Moving to indices $j \in [L]$ such that their corresponding Voronoi cells satisfy that $|\mathcal{V}_{j}| > 1$. The limits $\widetilde{M}_{n,j,2e_{u},0_d}/ \mathcal{D}_{3n} \to 0$ and $\widetilde{M}_{n,j,0_d,2e_u}/ \mathcal{D}_{3n} \to 0$ induces that
\begin{align}
    \label{eq:squared_loss_nonlinear}
    \frac{\sum_{j: |\mathcal{V}_{j}| > 1} \sum_{i \in \mathcal{V}_{j}} \exp(c_{n,i})(\|\Delta \Abm_{n,ij}\|^2+\|\Delta \Bbm_{n,ij}\|^2)}{\mathcal{D}_{3n}} \to 0
\end{align}
% Likewise, as ${L}_{n,5,j}^{(uu)}/ \mathcal{D}_{3n} \to 0$, we also obtain that $\dfrac{\sum_{i \in \mathcal{V}_{j}} \exp(b_{n,i})\|\Delta W_{n,2ij}\|^2}{\mathcal{D}_{3n}} \to 0$. Therefore, we find that
% \begin{align}
%     \label{eq:squared_loss_nonlinear}
%     \frac{\sum_{j: |\mathcal{V}_{j}| > 1} \sum_{i \in \mathcal{V}_{j}} \exp(b_{n,i})(\|\Delta W_{n,1ij}\|^2 + \|\Delta W_{n,2ij}\|^2}{\mathcal{D}_{3n}} \to 0. 
% \end{align}
By putting the results in equations~(\ref{eq:key_limits_first_2_nonlinear}), (\ref{eq:linear_loss_nonlinear}), and (\ref{eq:squared_loss_nonlinear}) together, we arrive at $1 = \frac{\mathcal{D}_{3n}}{\mathcal{D}_{3n}} \to 0$
as $n \to \infty$, which is a contradiction. As a consequence, at least one of the coefficients of the linear independent functions in $Q_{n}(\mathbb{X})/ \mathcal{D}_{3n}$ does not go to 0 as $n \to \infty$. 
\paragraph{Step 3 - Application of the Fatou’s lemma.} We denote $\widetilde{m}_n$ as the maximum of the absolute values of the coefficients of the linear independent functions in $Q_{n}(\mathbb{X})/ \mathcal{D}_{3n}$. As at least one of these coefficients does not go to 0, it
indicates that $1/\widetilde{m}_n \not \to \infty $ as $n \to \infty$.
Since $\normf{f_{\widetilde{G}_n}-f_{\widetilde{G}_*}}/\mathcal{D}_{3n} \to 0$ as $n \to \infty$, we obtain $\normf{f_{\widetilde{G}_n}-f_{\widetilde{G}_*}}/(\widetilde{m}_{n} \mathcal{D}_{3n}) \to 0$. An application of the Fatou's lemma leads to:
\begin{align*}
    0=\lim_{n \to \infty} \dfrac{\normf{f_{\widetilde{G}_n}-f_{\widetilde{G}_*}}}{\widetilde{m}_n\mathcal{D}_{3n}} \geq  \int \liminf_{n \to \infty} \dfrac{\left| f_{\widetilde{G}_n}(\mathbb{X})-f_{\widetilde{G}_*}(\mathbb{X})\right|}{\widetilde{m}_n\mathcal{D}_{3n}}d\mu(\mathbb{X}) \geq 0.
\end{align*}
That inequality demonstrates that $\liminf_{n \to \infty} \dfrac{\left| f_{\widetilde{G}_n}(\mathbb{X})-f_{\widetilde{G}_*}(\mathbb{X})\right|}{\widetilde{m}_n\mathcal{D}_{3n}} = 0$ for almost surely $\mathbb{X}$. As $n \to \infty$, we denote
\begin{align*}
    \dfrac{\widetilde{N}_{n,j}}{\widetilde{m}_{n}\mathcal{D}_{3n}} \to \tilde{\lambda}_{0,j}, \quad \dfrac{L_{n,\tau,j}}{\widetilde{m}_n\mathcal{D}_{3n}} \to \tilde{\lambda}_{\tau,j}, 
\end{align*}
for any indices $j \in [L]$ and $\tau\in[7]$. Here, at least one element of the set $\{\tilde{\lambda}_{0,j},\tilde{\lambda}_{\tau,j}:j\in[L], \tau\in[7]\}$ is different from 0. Given the above notations, the limit $\liminf_{n \to \infty} \dfrac{\left| f_{\widetilde{G}_n}(\mathbb{X})-f_{\widetilde{G}_*}(\mathbb{X})\right|}{m_n\mathcal{D}_{3n}} = 0$ implies that
\begin{align}
&= \sum_{j:|\mathcal{V}_{j}| = 1} \exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\sigma_2(\Bbm_{j}^*)\sigma_1(\Abm_{j}^*))\mathbb{X})\Big[\big(\tilde{\lambda}_{1,j}^{\top}\mathbb{X}\mathbb{X}^{\top}\sigma_2(\Bbm^*_j)+\tilde{\lambda}_{2,j}^{\top}\mathbb{X}\sigma_1(\Abm^*_j)\mathbb{X}\big) (\Mbm^0_{V}+\sigma_2(\Bbm_{j}^*)\sigma_1(\Abm_{j}^*))\mathbb{X}\nonumber\\
    &+\tilde{\lambda}_{1,j}^{\top}\mathbb{X}\sigma_2(\Bbm^*_j)+\sigma_1(\Abm^*_j)\mathbb{X}\tilde{\lambda}_{2,j}\Big] \nonumber \\
    & + \sum_{j:|\mathcal{V}_{j}| > 1} \exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\sigma_2(\Bbm_{j}^*)\sigma_1(\Abm_{j}^*))\mathbb{X}) \Big[\big(\tilde{\lambda}_{1,j}^{\top}\mathbb{X}\mathbb{X}^{\top}\sigma_2(\Bbm^*_j)+\tilde{\lambda}_{2,j}^{\top}\mathbb{X}\sigma_1(\Abm^*_j)\mathbb{X}+\mathbb{X}^{\top}\tilde{\lambda}_{3,j}\mathbb{X}(\mathbb{X}^{\top}\sigma_2(\Bbm^*_j))^2 \nonumber\\
&+\tilde{\lambda}_{4,j}^{\top}\mathbb{X}\mathbb{X}^{\top}\sigma_2(\Bbm^*_j)+\mathbb{X}^{\top}\tilde{\lambda}_{5,j}\mathbb{X}(\sigma_1(\Abm^*_j)\mathbb{X})^2+\tilde{\lambda}_{6,j}^{\top}\mathbb{X}\sigma_1(\Abm^*_j)\mathbb{X}+\mathbb{X}^{\top}\tilde{\lambda}_{7,j}\mathbb{X}+\mathbb{X}^{\top}\tilde{\lambda}_{7,j}\mathbb{X}\mathbb{X}^{\top}\sigma_2(\Bbm^*_j)\sigma_1(\Abm^*_j)\mathbb{X}\big)\nonumber\\
&\times (\Mbm^0_{V}+\sigma_2(\Bbm_{j}^*)\sigma_1(\Abm_{j}^*))\mathbb{X}+\tilde{\lambda}_{1,j}^{\top}\mathbb{X}\sigma_2(\Bbm^*_j)+\sigma_1(\Abm^*_j)\mathbb{X}\tilde{\lambda}_{2,j}+\tilde{\lambda}_{4,j}^{\top}\mathbb{X}\sigma_2(\Bbm^*_j)+\sigma_1(\Abm^*_j)\mathbb{X}\tilde{\lambda}_{6,j}+\tilde{\lambda}_{7,j}^{\top}\mathbb{X}\Big] \nonumber\\
&-\sum_{j:|\mathcal{V}_{j}| = 1} \exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\sigma_2(\Bbm_{j}^*)\sigma_1(\Abm_{j}^*))\mathbb{X}) \Big[\tilde{\lambda}_{1,j}^{\top}\mathbb{X}\mathbb{X}^{\top}\sigma_2(\Bbm^*_j)+\tilde{\lambda}_{2,j}^{\top}\mathbb{X}\sigma_1(\Abm^*_j)\mathbb{X}\Big]f_{\widetilde{G}_*}(\mathbb{X})\nonumber\\
&-\sum_{j:|\mathcal{V}_{j}| > 1} \exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\sigma_2(\Bbm_{j}^*)\sigma_1(\Abm_{j}^*))\mathbb{X}) \Big[\tilde{\lambda}_{1,j}^{\top}\mathbb{X}\mathbb{X}^{\top}\sigma_2(\Bbm^*_j)+\tilde{\lambda}_{2,j}^{\top}\mathbb{X}\sigma_1(\Abm^*_j)\mathbb{X}+\mathbb{X}^{\top}\tilde{\lambda}_{3,j}\mathbb{X}(\mathbb{X}^{\top}\sigma_2(\Bbm^*_j))^2 \nonumber\\
    &+\tilde{\lambda}_{4,j}^{\top}\mathbb{X}\mathbb{X}^{\top}\sigma_2(\Bbm^*_j)+\mathbb{X}^{\top}\tilde{\lambda}_{5,j}\mathbb{X}(\sigma_1(\Abm^*_j)\mathbb{X})^2+\tilde{\lambda}_{6,j}^{\top}\mathbb{X}\sigma_1(\Abm^*_j)\mathbb{X}+\mathbb{X}^{\top}\tilde{\lambda}_{7,j}\mathbb{X}+\mathbb{X}^{\top}\tilde{\lambda}_{7,j}\mathbb{X}\mathbb{X}^{\top}\sigma_2(\Bbm^*_j)\sigma_1(\Abm^*_j)\mathbb{X}\Big]f_{\widetilde{G}_*}(\mathbb{X})\nonumber\\
    &  + \sum_{j = 1}^{L} \tilde{\lambda}_{0,j} \exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\sigma_2(\Bbm_{j}^*)\sigma_1(\Abm_{j}^*))\mathbb{X})\Big[(\Mbm^0_{V}+\Bbm_{j}^*
\Abm_{j}^*)\mathbb{X}-f_{\widetilde{G}_*}(\mathbb{X})\Big]   = 0
\end{align} 
for almost surely $\mathbb{X}$. 
%By denoting $\boldsymbol{Z} = B^{\top} \mathbb{X}$, this equation also holds for almost surely $\boldsymbol{Z}$.
However, that equation implies that all the coefficients $\{\tilde{\lambda}_{0,j},\tilde{\lambda}_{\tau,j}:j\in[L], \tau\in[7]\}$ are 0. It is a contradiction. As a consequence, we obtain that $$\lim_{\varepsilon\to0} \inf_{\widetilde{G}\in \widetilde{\mathcal{G}}_{L'}(\Theta): \mathcal{D}_3(\widetilde{G},\widetilde{G}_*)\leq \varepsilon} \normf{f_{\widetilde{G}}-f_{\widetilde{G}_*}}/\mathcal{D}_3(\widetilde{G},\widetilde{G}_*) >0.$$

\subsubsection{Global Part}
The result of the local part implies that  we can find a positive constant $\varepsilon'$ such that
$$\inf_{\widetilde{G}\in \widetilde{\mathcal{G}}_{L'}(\Theta): \mathcal{D}_3(\widetilde{G},\widetilde{G}_*)\leq \varepsilon'} \normf{f_{\widetilde{G}}-f_{\widetilde{G}_*}}/\mathcal{D}_3(\widetilde{G},\widetilde{G}_*) >0.$$
Therefore to obtain the conclusion of the theorem, we only need to prove that
$$ \inf_{\widetilde{G}\in \widetilde{\mathcal{G}}_{L'}(\Theta): \mathcal{D}_3(\widetilde{G},\widetilde{G}_*)> \varepsilon'} \normf{f_{\widetilde{G}}-f_{\widetilde{G}_*}}/\mathcal{D}_3(\widetilde{G},\widetilde{G}_*) >0.$$
We assume by contradiction that the above claim does not hold. It indicates that there exists a sequence of
measures $\widetilde{G}'_{n} := \sum_{j = 1}^{L} \exp(c_{n,j}) \delta_{(\Bbm_{n,j},\Abm_{n,j})}$ in $\widetilde{\mathcal{G}}_{L'}(\Theta)$ such that 
$$\left\{\begin{matrix}
 \mathcal{D}_3(\widetilde{G}'_n,\widetilde{G}_*) > \varepsilon'\\
 \normf{f_{\widetilde{G}'_n}-f_{\widetilde{G}_*}}/\mathcal{D}_3(\widetilde{G}'_n,\widetilde{G}_*) \to 0
\end{matrix}\right.$$
as $n \to \infty$, which implies that $\normf{f_{\widetilde{G}'_n}-f_{\widetilde{G}_*}} \to 0$  as $n \to \infty$.\\
Given that the parameter space $\Theta$ is a compact set, there exists a mixing measure $\widetilde{G}'$ in $\widetilde{\mathcal{G}}_{L'}(\Theta)$ such that one of the $\widetilde{G}'_n$'s subsequences converges to $\widetilde{G}'$. Since $\mathcal{D}_3(\widetilde{G}'_n,\widetilde{G}_*)>\varepsilon'$ for all $n \geq 1$, we obtain that $\mathcal{D}_3(\widetilde{G}',\widetilde{G}_*) \geq \varepsilon'$.
An application of the Fatou’s lemma leads to
\begin{align*}
0=\lim_{n \to \infty} \normf{f_{\widetilde{G}'_n}-f_{\widetilde{G}_*}} & = \lim_{n \to \infty} \int \left\| f_{\widetilde{G}'_n}(\mathbb{X})-f_{\widetilde{G}_*}(\mathbb{X})\right\|^2 d\mu(\mathbb{X}) \\
& \geq  \int \liminf_{n \to \infty} \left\| f_{\widetilde{G}'_n}(\mathbb{X})-f_{\widetilde{G}_*}(\mathbb{X})\right\|^2 d\mu(\mathbb{X}).
\end{align*}
The above inequality indicates that $f_{\widetilde{G}'}=f_{\widetilde{G}_*}$ for almost surely $\mathbb{X}$. It follows from the identifiability property that $\widetilde{G}' \equiv \widetilde{G}_*$. It follows that $\mathcal{D}_3(\widetilde{G}',\widetilde{G}_*)=0$, contradicting the fact that $\mathcal{D}_3(\widetilde{G}',\widetilde{G}_*)> \varepsilon'>0$. 
Hence, the proof is completed.
\paragraph{Proof for the identifiability property.} 
We will prove that if $f_{\widetilde{G}}(\mathbb{X}) = f_{\widetilde{G}_*}(\mathbb{X})$ for almost surely $\mathbb{X}$, then $G \equiv  \widetilde{G}_*$.
To ease the presentation, for any mixing measure $\widetilde{G} = \sum_{j = 1}^{\tilde{L}} \exp(c_{j}) \delta_{(\Bbm_{j},\Abm_{j})} \in \mathcal{G}_{L'}(\Theta)$, we denote
\begin{align*}
    \softmax_{G}(u)&=\dfrac{\exp(u)}{\sum_{j=1}^{\tilde{L}} \exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\sigma_2(\Bbm_{j})\sigma_1(\Abm_{j}))\mathbb{X}+c_{j})},
\end{align*}
where $u \in \{\mathbb{X}^{\top}(\Mbm^0_{Q}+\sigma_2(\Bbm_{j})\sigma_1(\Abm_{j}))\mathbb{X}+c_{j}: j \in [\tilde{L}]\}$.
The equation $f_{\widetilde{G}}(\mathbb{X}) = f_{\widetilde{G}_*}(\mathbb{X})$ indicates that
\begin{align}
    &\sum_{j=1}^{\tilde{L}} \softmax(\mathbb{X}^{\top}(\Mbm^0_{Q}+\sigma_2(\Bbm_{j})\sigma_1(\Abm_{j}))\mathbb{X} + c_{j})(\Mbm^0_{V}+\sigma_2(\Bbm_{j}) \sigma_1(\Abm_{j}))\mathbb{X}  \nonumber\\
&\hspace{4cm}=\sum_{j=1}^{L} \softmax(\mathbb{X}^{\top}(\Mbm^0_{Q}+\sigma_2(\Bbm_{j}^*)\sigma_1(\Abm_{j}^*))\mathbb{X} + c_{j}^{*})(\Mbm^0_{V}+\sigma_2(\Bbm_{j}^*)
\sigma_1(\Abm_{j}^*))\mathbb{X}  
\label{eq:identify_setting_first_neuralnet_nonlinear}
\end{align}
That equation implies that $\tilde{L} = L$. As a consequence, we find that
\begin{align*}
    \{\softmax(\mathbb{X}^{\top}(\Mbm^0_{Q}+\sigma_2(\Bbm_{j})\sigma_1(\Abm_{j}))\mathbb{X} + c_{j}):j\in [\tilde{L}]\}=\{\softmax(\mathbb{X}^{\top}(\Mbm^0_{Q}+\sigma_2(\Bbm_{j}^*)\sigma_1(\Abm_{j}^*))\mathbb{X} + c_{j}^{*}):j \in [L]\} 
\end{align*}
for almost surely $\mathbb{X}$. By relabelling the indices, we can assume without loss of generality that for any $j \in [L]$
\begin{align*}
    \softmax(\mathbb{X}^{\top}(\Mbm^0_{Q}+\sigma_2(\Bbm_{j}^*)\sigma_1(\Abm_{j}^*))\mathbb{X} + c_{j}^{*}) = \softmax(\mathbb{X}^{\top}(\Mbm^0_{Q}+\sigma_2(\Bbm_{j})\sigma_1(\Abm_{j}))\mathbb{X} + c_{j}),
\end{align*}
for almost surely $\mathbb{X}$. Given the invariance to translation of the softmax function, the equation~(\ref{eq:identify_setting_first_neuralnet_nonlinear}) leads to
\begin{align}
     &\sum_{j = 1}^{\tilde{L}}\exp{(c_{j})}\exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\sigma_2(\Bbm_{j})\sigma_1(\Abm_{j}))\mathbb{X})(\Mbm^0_{V}+\sigma_2(\Bbm_{j})
\sigma_1(\Abm_{j}))\mathbb{X}\nonumber\\
&\hspace{4cm}=\sum_{j = 1}^{L}\exp{(c_{j}^{*})}\exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\sigma_2(\Bbm_{j}^*)\sigma_1(\Abm_{j}^*))\mathbb{X})(\Mbm^0_{V}+\sigma_2(\Bbm_{j}^*)
\sigma_1(\Abm_{j}^*))\mathbb{X}, 
    \label{eq:identify_setting_second_neuralnet_nonlinear}
\end{align}
for almost surely $\mathbb{X}$.


Now, the index set $[L]$ can be partitioned into $\tilde{m}$ subsets $\tilde{K}_1, \tilde{K}_2,\ldots,\tilde{K}_{\tilde{m}}$ where $\tilde{m} \leq L$, such that $\exp{(c_{j})}=\exp{(c_{j'}^{*{}})}$ for any indices $j,j'\in \tilde{K}_i$ and $i \in [\tilde{m}]$. Thus, equation~(\ref{eq:identify_setting_second_neuralnet_nonlinear}) can be rewritten as follows:
\begin{align*}
    &\sum_{i = 1}^{\tilde{m}}\sum_{j \in \tilde{K}_i}\exp{(c_{j})}\exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\sigma_2(\Bbm_{j})\sigma_1(\Abm_{j}))\mathbb{X})(\Mbm^0_{V}+\sigma_2(\Bbm_{j})
\sigma_1(\Abm_{j}))\mathbb{X}\nonumber\\
&\hspace{4cm}=\sum_{i = 1}^{\tilde{m}}\sum_{j \in \tilde{K}_i}\exp{(c_{j}^{*})}\exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\sigma_2(\Bbm_{j}^*)\sigma_1(\Abm_{j}^*))\mathbb{X})(\Mbm^0_{V}+\sigma_2(\Bbm_{j}^*)
\sigma_1(\Abm_{j}^*))\mathbb{X},
\end{align*}
for almost surely $\mathbb{X}$. The above equation implies that 
\begin{align*}
    \{(\Mbm^0_{V}+\sigma_2(\Bbm_{j})
\sigma_1(\Abm_{j}))\mathbb{X}: j \in \tilde{K}_i\}=\{(\Mbm^0_{V}+\sigma_2(\Bbm_{j}^*)
\sigma_1(\Abm_{j}^*))\mathbb{X}: j \in \tilde{K}_i\}, 
\end{align*}
for any $i \in [\tilde{m}]$ and for almost surely $\mathbb{X}$. Since the activation functions $\sigma_1$ and $\sigma_2$ are algebraically independent, the above result indicates that 
% \begin{align*}
%     \{(W_{1j},W_{2}): j \in K_i\} = \{(W_{*,1j'},W_{2}): j' \in K_i\}, 
% \end{align*}
% which directly leads to
% \begin{align*}
%     \{W_{1}\prompt_{j}: j \in K_i\} = \{W_{*,1}\prompt_{*,j}: j \in K_i\} \quad \text{and} \quad \{W_{2}\prompt_{j}: j \in K_i\} = \{W_{*,2}\prompt_{*,j}: j \in K_i\}.
% \end{align*}
%Without loss of generality, we assume that $W_{1}\prompt_{j}=W_{*,1}\prompt_{*,j}$ and $W_{2}\prompt_{j}=W_{*,2}\prompt_{*,j}$ for all $j \in K_i$. 
\begin{align*}
    \sum_{i = 1}^{\tilde{m}}\sum_{j \in \tilde{K}_i}\exp{(c_{j})}\delta_{(\Bbm_{j},
\Abm_{j})} = \sum_{i = 1}^{\tilde{m}}\sum_{j \in \tilde{K}_i}\exp{(c_{j}^{*})}\delta_{(\Bbm_{j}^*,
\Abm_{j}^*)}.
\end{align*}
As a consequence, $G \equiv G_*$ and the proof is completed.

\subsection{Proof of Proposition~\ref{prop:regression_estimation_linear}}
\label{appendix:regression_estimation_linear}
Recall from the setting that the samples $(\Xbm_1,Y_1), (\Xbm_2,Y_2),\ldots,(\Xbm_n,Y_n)\in\mathbb{R}^{\bar{d}} \times\mathbb{R}^{\bar{d}}$ are i.i.d. from the following regression model:
\begin{align*}
    Y_i=f_{\bar{G}_*}(\Xbm_i)+\varepsilon_i, \quad i=1,2,\ldots,n, 
    %\label{eq:regression_model}
\end{align*}
where the Gaussian noises $\varepsilon_1,\ldots,\varepsilon_n$ are i.i.d. and satisfy that $\bbE[{\varepsilon_{i}}|\Xbm_i] = 0$ and $\var(\varepsilon_{i}|\Xbm_i) = \sigma^2 I_{\bar{d}}$ for all $i \in [n]$. Furthermore, $f_{\bar{G}_{*}}(.)$ admits the following form:
\begin{align*}
f_{\bar{G}_{*}}(\Xbm)  := & \sum_{j=1}^{L} \frac{\exp(\mathbb{X}^{\top} (\Mbm^0_{Q}+\Wbm_{2,j}^{*}\Bbm_{j}^*\Wbm_{1,j}^{*}\Abm_{j}^*)\Mbm^{0}_{K}\mathbb{X}+c^*_j)}{\bar{D}_{f}(\mathbb{X})}\cdot(\Mbm^0_{V}+\Wbm_{2,j}^{*}\Bbm_{j}^*
\Wbm_{1,j}^{*}\Abm_{j}^*)\mathbb{X},
\end{align*}
where we denote $\bar{D}_{f}(\mathbb{X}) = \sum_{k = 1}^{L}\exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\Wbm_{2,k}^{*}\Bbm_{k}^*\Wbm_{1,k}^{*}\Abm_{k}^*)\Mbm^{0}_{K}\mathbb{X}+c^*_{k})$. Finally, the least-square estimator $\bar{G}_{n}$ takes the following form:
\begin{align*}
    %\label{eq:least_squared_estimator_overspecified}
    \bar{G}_n :=\argmin_{G\in \bar{\mathcal{G}}_{L'}(\Theta)}\sum_{i=1}^{n}\|Y_i-f_{G}(\Xbm_i)\|^2,
\end{align*}
From the Gaussianity assumption of $\varepsilon_i|\Xbm_i$ for all $i \in [n]$, we have $Y_{i}|\Xbm_{i} \sim \mathcal{N}(f_{\bar{G}_{*}}(\Xbm_{i}), \sigma^2 I_{\bar{d}})$ for all $i \in [n]$. Therefore, the least square estimator $\bar{G}_{n}$ is indeed a maximum likelihood estimator with respect to the data $Y_{1}|\Xbm_{1}, \ldots, Y_{n}|\Xbm_{n}$, which takes the following form:
\begin{align*}
    \bar{G}_n \in\argmax_{G\in \bar{\mathcal{G}}_{L'}(\Theta)}\frac{1}{n}\sum_{i=1}^{n}\log(p(Y_i|f_{G}(\Xbm_i),\sigma^2I_{\bar{d}})).
\end{align*}
Here, $p(Y_i|f_{G}(\Xbm_i),\sigma^2I_{\bar{d}})$ stands for multivariate Gaussian distribution with mean $f_{G}(\Xbm)$ and covariance matrix $\sigma^2I_{\bar{d}}$. An application of Theorem 7.4 from~\cite{vandeGeer-00} leads to
\begin{align*}
    h(p(Y|f_{\bar{G}_n}(\Xbm),\sigma^2I_{\bar{d}}),p(Y|f_{\bar{G}_*}(\Xbm),\sigma^2I_{\bar{d}}))=\mathcal{O}_P(\sqrt{\log(n)/n}),
\end{align*}
where the notation $h$ stands for the Hellinger distance. As the Hellinger distance between two multivariate Gaussian distributions has closed-form expression, direct calculation yields that
\begin{align*}
    h^2(p(Y|f_{\bar{G}_n}(\Xbm),\sigma^2I_{\bar{d}}),p(Y|f_{\bar{G}_*}(\Xbm),\sigma^2I_{\bar{d}}))=1-\exp\Bigg\{-\frac{1}{8\sigma^2}\|f_{\bar{G}_n}(\Xbm)-f_{\bar{G}_*}(\Xbm)\|^2\Bigg\}.
\end{align*}
Therefore, for sufficiently large $n$, for some universal constant $C$ the above inequality leads to 
\begin{align*}
    \|f_{\bar{G}_n}(\Xbm)-f_{\bar{G}_*}(\Xbm)\|^2&\leq 8\sigma^2\log\Big(\dfrac{1}{1-C\log(n)/n}\Big)\\
    &\leq 16\sigma^2C\log(n)/n.
\end{align*}
That inequality is equivalent to
\begin{align*}
    \|f_{\bar{G}_n}(\Xbm)-f_{\bar{G}_*}(\Xbm)\|=\mathcal{O}_P(\sqrt{\log(n)/n}).
\end{align*}
As a consequence, we find that
\begin{align*}
    \normf{f_{\bar{G}_n}-f_{\bar{G}_*}} = \mathcal{O}_P(\sqrt{\log(n)/n}).
\end{align*}
The conclusion of the proposition is achieved.

% {\color{blue} Don't forget proofs for density esitmation.}
\section{Related Works}
\input{sections/related_works}
\label{sec:related_works}
%  
% \section{Performance-Parameter Tradeoff (PPT)}
% \label{appendix: ppt}
% In the field of Parameter-Efficient Transfer Learning (PETL), algorithm evaluation typically revolves around two key factors: the number of trainable parameters and task performance. Methods that achieve superior results with fewer trainable parameters tend to garner greater attention. To quantify this tradeoff, we adopt the Performance-Parameter Trade-off (PPT) metric, introduced by \cite{vpetl}. This metric provides a quantitative measure of an algorithm's efficiency by balancing its performance on a given downstream task with the number of parameters it modifies. Specifically, for a PETL algorithm $M$, the PPT metric incorporates its task performance $M_t$, the number of trainable parameters $P_M$, and a normalization constant $C$. The formula for $\mathrm{PPT}_M$ is given as follows:

% \begin{equation*}
%     \mathrm{PPT}_M = M_t \times \exp(-\log_{10}(\frac{P_M}{C}+1))
% \end{equation*}
% The normalization constant C is set at $10^7$ as the parameters for most PETL algorithms typically fall within this range.

 
\section{Experimental Details}

\label{appendix: exp details}
\subsection{Hyperparameters}
For the vision tasks, we use grid search to tune the learning rate in the range of $\{0.001, 0.005, 0.01, 0.05, 0.1\}$, and the weight decay in the range of $\{0.0001, 0.0005, 0.001, 0.01, 0.1\}$. Other hyperparameters are reported in the tables below: 

\begin{table}[ht]
\centering 
\caption{Hyperparameter configurations of RepLoRA for \texttt{ViT-B/16} on the vision tasks.}
\label{tab:visionhyperparameters}
\begin{tabular}{c|cc}
\hline
\textbf{Hyperparameters (RepLoRA)} & Classification & Video-Action Recognition \\ \hline
Rank $r$                             & \multicolumn{2}{c}{8}                     \\
$\alpha$                              & \multicolumn{2}{c}{8}                     \\
Dropout                            & \multicolumn{2}{c}{0}                     \\
Base Optimizer                     & \multicolumn{2}{c}{AdamW}                 \\
Lr Scheduler                       & \multicolumn{2}{c}{Cosine}                \\
Batch size                         & 64             & 512                      \\
Warmup steps                       & \multicolumn{2}{c}{100}                   \\
Epochs                             & 100            & 90                       \\ \hline
\end{tabular}
\end{table}
\begin{table}[ht]
\centering
\caption{Hyperparameter configurations of RepLoRA for \texttt{LLaMA-7B/13B} on the commonsense reasoning tasks.}
\label{tab:hyperparameterscommonsense}
\begin{tabular}{c|cccc}
\hline
\textbf{Hyperparameters (RepLoRA)} & \multicolumn{2}{c}{\texttt{LLaMA-7B}} & \multicolumn{2}{c}{\texttt{LLaMA-13B}} \\ \hline
Rank $r$                & 16       & 32       & 16       & 32       \\
$\alpha$ & 32       & 64       & 32       & 64       \\
Dropout               & \multicolumn{4}{c}{0.05}                  \\
Base Optimizer        & \multicolumn{4}{c}{AdamW}                 \\
LR                    & $2.00E-04$ & $1.00E-04$ & $2.00E-04$ & $1.00E-04$ \\
Lr Scheduler          & \multicolumn{4}{c}{Linear}                     \\
Batch size            & \multicolumn{4}{c}{32}                    \\
Warmup steps          & \multicolumn{4}{c}{100}                   \\
Epochs                & \multicolumn{4}{c}{3}                     \\ \hline
\end{tabular}
\end{table}

\begin{table}[ht]
\centering
\caption{Hyperparameter configurations of RepLoRA for \texttt{VL-BART} on the Image/Video-Text Understanding tasks.}
\label{tab:image-textunderstandinghyperparameters}
\begin{tabular}{c|cc}
\hline
\textbf{Hyperparameters (RepLoRA)} & Image-Text   & Video-Text  \\ \hline
Rank $r$                             & \multicolumn{2}{c}{128}    \\
$\alpha$                              & \multicolumn{2}{c}{128}    \\
Dropout                            & \multicolumn{2}{c}{0}      \\
Base Optimizer                     & \multicolumn{2}{c}{AdamW}  \\
LR                                 & $1.00E-03$     & $3.00E-04$    \\
Lr Scheduler                       & \multicolumn{2}{c}{Linear} \\
Batch size                         & 300          & 40          \\
Warmup ratio                       & \multicolumn{2}{c}{0.1}    \\
Epochs                             & 20           & 7           \\ \hline
\end{tabular}
\end{table}

 
\section{Additional Experiments}
\label{sec:additional_experiements}
\subsection{Sample Efficiency on the FGVC Datasets}
\label{appendix: sample efficiency}

\input{tables/fgvc_sample_efficiency}


\begin{figure}[ht]
    \centering
    % Row 1: 3 figures
    \begin{subfigure}{0.33\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/CUBgap.png}
        %\caption{Figure 1 caption}
        %\label{fig:figure1}
    \end{subfigure}
    \begin{subfigure}{0.33\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/NABirdsGap.png}
        
        %\caption{Figure 2 caption}
        %\label{fig:figure2}
    \end{subfigure}
    \begin{subfigure}{0.33\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/FlowersGap.png}
        %\caption{Figure 3 caption}
        %\label{fig:figure3}
    \end{subfigure}

    \vspace{0.5cm} % Add spacing between rows

    % Row 2: 2 figures
    \begin{subfigure}{0.33\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/CarsGap.png}
        %\caption{Figure 4 caption}
        %\label{fig:figure4}
    \end{subfigure}
    \begin{subfigure}{0.33\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/DogsGap.png}
        %\caption{Figure 5 caption}
        %\label{fig:figure5}
    \end{subfigure}

    \caption{Visualization of sample efficiency of LoRA and RepLoRA on five \texttt{FGVC} Datasets.}
    \label{fig:overall}
\end{figure}
 
\subsection{Linear vs Non-linear Reparameterization}
\label{appendix: linear vs non-linear}
Recall that in our practical method, the low-rank matrices are given by: 

\begin{align*}
    &\Abm_Q = \sigma_1^\Abm(\Abm), \Bbm_Q = \sigma_1^\Bbm(\Bbm) \\
    &\Abm_V = \sigma_2^\Abm(\Abm), \Bbm_V = \sigma_2^\Bbm(\Bbm) \\
\end{align*}
For the linear reparameterization, $\sigma_1^\Abm, \sigma_2^\Abm, \sigma_1^\Bbm, \sigma_2^\Bbm$ were implemented with linear layers without activation. For the nonlinear reparameterization setting, these functions were implemented with a two-layer neural network with a hidden dimension of $64$ on all settings.

\paragraph{Detail results.} The tables below report all results for linear vs. non-linear reparameterization.

\input{tables/linear-vs-nonlinear}

