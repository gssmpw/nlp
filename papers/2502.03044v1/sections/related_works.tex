\paragraph{Parameter-Efficient Fine-tuning (PEFT).} With the recent rise of large models, PEFT methods are growing in popularity for their ability to fine-tune large-scale models by training a relatively small number of parameters for adapting to specific downstream tasks. Existing PEFT methods can be divided into three categories. The first category is referred to as \textit{adapter-based} methods, which introduce additional trainable parameters to the frozen backbone. For example, \textit{Series Adapter} \cite{houlsby2019parameter} proposes adding linear modules in sequence to the existing layer, or \textit{Parallel Adapter} \cite{parallel-adapter} proposes integrating these modules in parallel. The second category of PEFT methods is \textit{Prompt-based} methods that add extra trainable soft tokens, referred to as prompts, to the input \cite{lester2021powerscaleparameterefficientprompt, razdaibiedina2023residualprompttuningimproving, wang2023nonintrusiveadaptationinputcentricparameterefficient}. A weakness of these methods is that they increase inference latency compared to the original model. 

\textbf{Low-Rank Adaptation \cite{lora}.}
LoRA and its variants are among the third category of the PEFT method, which is well-known for its simplicity and for not adding extra inference burden. To fine-tune the linear layers of a large model, LoRA applies low-rank matrices to approximate the weight changes and then merge them to the pre-trained weights for inference. A recent variant of LoRA is DoRA \cite{dora}, which proposes to decompose the weight change into a learnable magnitude and directional component. Another example is AdaLoRA \cite{adalora}, which parameterizes the incremental updates in the form of singular value decomposition and prunes less significant singular values for more efficient updates. Orthogonal Fine-tuning (OFT) \cite{oft} exploits the orthogonal factorization to fine-tune diffusion models. Recently, VeRA \cite{vera} significantly reduces the number of trainable parameters compared to LoRA by using learnable scaling vectors to adjust a shared pair of frozen random matrices across layers. Notably, our method, which will be proposed in the following few sections, also falls within this category, and we validate its efficacy alongside LoRA and its variants through theoretical analysis and comprehensive experimentation. 

\textbf{Mixture of Experts.} Building on the foundational concept of mixture models \cite{Jacob_Jordan-1991, jordan1994hierarchical}, prior works by \cite{Eigen_learning_2014, Quoc-conf-2017} established the MoE layer as a key component for efficiently scaling model capacity. MoE models have since gained widespread attention for their adaptability across various domains, including large language models \cite{du2022glam, zhou2023brainformers}, computer vision \cite{riquelme2021scaling, fromsparsetosoft}, and multi-task learning \cite{mmoe}. Recent studies have investigated the convergence rates for expert estimation in MoE models, focusing on different assumptions and configurations of gating and expert functions. \cite{ho2022convergence}, assuming data from an input-free gating Gaussian MoE, demonstrated that expert estimation rates for maximum likelihood estimation depend on the algebraic independence of the expert functions. Similarly, employing softmax gating, \cite{nguyen2023demystifying,nguyen2024temperature} found that expert estimation rates are influenced by the solvability of polynomial systems arising from the interaction between gating and expert parameters. More recently, \cite{nguyen2024squares,nguyen2024sigmoid} utilized least square estimation to propose an identifiable condition for expert functions, particularly for feedforward networks with nonlinear activations. They showed that estimation rates are significantly faster under these conditions than models using polynomial experts.
