
% , and $\Tilde{\hbm}_i$'s are given by:

% \begin{equation*}
%     \Tilde{\hbm}_i = \mathrm{Attention}()
% \end{equation*}
% {\color{blue} Huy: Revise the writing of the this section. The flow is as follows: (1) Without parameterization, the sample efficiency is suboptimal. Therefore, we propose to parameterize the low-rank matrices.}
This section presents the theoretical benefits of applying the reparameterization technique in LoRA via its connection to MoE as formulated in Section~\ref{sec:MoE-LoRA}. For simplicity, we will take into account only the first row of the first attention head $\tilde{\hbm}_{1, 1}$ specified in Eq.~(\ref{eq:LORA_MoE}). Under this simplified setting, we will investigate the convergence behavior of low-rank matrices within the following MoE-based regression framework:

\vspace{0.5 em}
\noindent
\textbf{Problem setup.} 
Let $(\mathbb{X}_1,\Ybm_1), (\mathbb{X}_2,\Ybm_2),\ldots,(\mathbb{X}_n,\Ybm_n)\in\mathbb{R}^{\bar{d}} \times\mathbb{R}^{\bar{d}}$ be i.i.d. samples of size $n$ generated from the following regression model:
% We assume that the i.i.d. samples of size $n$: $(\mathbb{X}_1,\Ybm_1), (\mathbb{X}_2,\Ybm_2),\ldots,(\mathbb{X}_n,\Ybm_n)\in\mathbb{R}^{\bar{d}} \times\mathbb{R}^{\bar{d}}$ are generated from the model:
\begin{align}
    \Ybm_i=f_{G_*}(\mathbb{X}_i)+\varepsilon_i, \quad i=1,2,\ldots,n. 
    \label{eq:regression_model}
\end{align}
Above, we assume that $\mathbb{X}_{1}, \mathbb{X}_{2}, \ldots, \mathbb{X}_{n}$ are i.i.d. samples from some probability distribution $\mu$ with bounded support. Meanwhile, the noise variables $\varepsilon_1,\varepsilon_2,\ldots,\varepsilon_n$ are independent and following Gaussian distributions such that $\bbE[{\varepsilon_{i}}|\mathbb{X}_i] = 0$ and $\var(\varepsilon_{i}|\mathbb{X}_i) = \nu^2I_{\bar{d}}$ for all $i \in [n]$. Next, the regression function $f_{G_{*}}(\cdot)$ takes the form of an MoE model with $L$ unknown experts, that is,
\begin{align}
 \hspace{-0.25 em}   &f_{G_{*}}(\mathbb{X})  := \sum_{j=1}^{L} \frac{\exp(\mathbb{X}^{\top} (\Mbm^0_{Q}+\Bbm_{Q,j}^*\Abm_{Q,j}^*)\Mbm^{0}_{K}\mathbb{X}+c^*_j)}{D_{f}(\mathbb{X})} \cdot(\Mbm^0_{V}+\Bbm_{V,j}^*
\Abm_{V,j}^*)\mathbb{X}, \label{eq:true_regression_function}
\end{align}
where we denote the following normalization term $$D_{f}(\mathbb{X}) := \sum_{k = 1}^{L}\exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\Bbm_{Q,k}^*\Abm_{Q,k}^*)\Mbm^{0}_{K}\mathbb{X}+c^*_{k}),$$ while $G_{*} := \sum_{j' = 1}^{L} \exp(c^*_{j'}) \delta_{(\Bbm_{Q,j'}^*,\Abm_{Q,j'}^*,\Bbm_{V,j'}^*,\Abm_{V,j'}^*)}$ represents for a \emph{mixing measure}, that is, a combination of Dirac measures $\delta$, associated with unknown parameters $(c_{j'}^*,\Bbm_{Q,j'}^*,\Abm_{Q,j'}^*,\Bbm_{V,j'}^*,\Abm_{V,j'}^*)_{j'=1}^{L}$ in the compact parameter space $\Theta\subset\mathbb{R} \times\mathbb{R}^{\bar{d}\times r}\times\mathbb{R}^{r\times \bar{d}}\times\mathbb{R}^{\bar{d}\times r}\times\mathbb{R}^{r\times \bar{d}}$. In addition, we assume that the matrices $\Mbm^0_{Q} \in \mathbb{R}^{\bar{d} \times \bar{d}}$, $\Mbm_{K}^{0} \times \mathbb{R}^{\bar{d} \times \bar{d}}$, and $\Mbm_{V}^{0} \in \mathbb{R}^{\bar{d} \times \bar{d}}$ are given to align with the formulation in Eq.~(\ref{eq:LORA_MoE}).

\vspace{0.5 em}
\noindent
\textbf{With versus Without Reparametrization.} Subsequently, we establish the convergence rates of estimating the unknown low-rank matrices $\{\Bbm_{Q,j'}^*,\Abm_{Q,j'}^*,\Bbm_{V,j'}^*,\Abm_{V,j'}^*\}_{j' = 1}^{L}$ under two scenarios, namely without shared structures among these low-rank matrices (equivalently, without reparametrization) in Section~\ref{sec:without_reparametrization} and with shared structures among these low-rank matrices (equivalently, with reparametrization) in Section~\ref{sec:with_reparametrization}. Our ultimate goal is to demonstrate that the sample efficiency of estimating these low-rank matrices under the shared structures setting is much better than that under the non-shared structures setting with a given error $\epsilon>0$. The theory sheds light on our Reparameterized LoRA (RepLoRA) design in Section~\ref{section: practice}. 


\subsection{Without Reparametrization: Suboptimal Sample Complexity}
\label{sec:without_reparametrization}
We begin our convergence analysis for low-rank matrices with the scenario where the LoRA reparametrization is absent. It is worth noting that we can estimate those unknown matrices via estimating the ground-truth mixing measure $G_*$; for that sake, we utilize the least square method \cite{vandeGeer-00} to obtain the following estimator:
\begin{align}
    \label{eq:least_squared_estimator}
    \widehat{G}_n:=\argmin_{G\in\mathcal{G}_{L'}(\Theta)}\sum_{i=1}^{n}\Big\|\Ybm_i-f_{G}(\mathbb{X}_i)\Big\|^2,
\end{align}
where the set of all mixing measures with at most $L'$ atoms is defined as 
$$\mathcal{G}_{L'}(\Theta):=\{G = \sum_{j' = 1}^{\ell} \exp(c_{j'}) \delta_{(\Bbm_{Q,j'},\Abm_{Q,j'},\Bbm_{V,j'},\Abm_{V,j'})}:1\leq \ell\leq L', \  (c_{j'},\Bbm_{Q,j'},\Abm_{Q,j'},\Bbm_{V,j'},\Abm_{V,j'})\in\Theta\}.$$ 

As the number of ground-truth experts $L$ is typically unknown in practice, we assume that the number of fitted experts $L'$ is large enough for $L'>L$. 
Then, to determine the convergence rates of the estimator $\widehat{G}_n$, we use a loss function built upon the concept of Voronoi cells \cite{manole22refined}.

\vspace{0.5 em}
\noindent
\textbf{Voronoi loss.}  Given a mixing measure $G$ with $L'>L$ atoms, its Voronoi cell set $\{\mathcal{V}_{j}\equiv\mathcal{V}_{j}(G):j\in[L]\}$
is generated by the atoms of $G_*$ as follows:
\begin{align*}
    %\label{eq:Voronoi_cells}
    \hspace{-0.5em}\mathcal{V}_{j}:=\{i\in[L']:\|{\bm H}_{i}-{\bm H}^*_{j}\|\leq\|{\bm H}_{i}-{\bm H}^*_{\ell}\|,\forall \ell\neq j\},
\end{align*}
where ${\bm H}:=(\Bbm_Q,\Abm_Q,\Bbm_V,\Abm_V)$. Then, the Voronoi loss function used for this section is  defined as
\begin{align*}     
&\mathcal{D}_{1,r}(G,G_*)  :=\sum_{j'=1}^{L}\Big|\sum_{i\in\mathcal{V}_{j'}}\exp(c_{i})-\exp(c^*_{j'})\Big| + \sum_{j'=1}^{L}\sum_{i\in\mathcal{V}_{j'}}\exp(c_{i}) (\|\Delta \Bbm_{Q,ij'}\|^r +\|\Delta \Abm_{Q,ij'}\|^r\\
&\hspace{2.5cm}+\|\Delta \Bbm_{V,ij'}\|^r +\|\Delta \Abm_{V,ij'}\|^r),
\end{align*}
for $r\in\mathbb{N}$, where $\Delta{\bm H}_{ij'}:={\bm H}_{i}-{\bm H}^*_{j'}$ for any $i, j'$. Now, we are ready to study the sample efficiency of LoRA without reparametrization in Theorem~\ref{theorem:param_rate_nopara}, whose proof is deferred to Appendix~\ref{appendix:param_rate_nopara}.
\begin{theorem}
    \label{theorem:param_rate_nopara}
    For any $r\in\mathbb{N}$, the following bound holds:
    \begin{align*}
        \sup_{G\in\mathcal{G}_{L'}(\Theta)\setminus\mathcal{G}_{L-1}(\Theta)}\mathbb{E}_{f_{G}}[\mathcal{D}_{1,r}(\widehat{G}_n,{G})]\gtrsim \frac{1}{\sqrt{n}},
    \end{align*}
     where $\mathbb{E}_{f_{G}}$ denotes the expectation taken with respect to the product measure $f^n_G$.
\end{theorem}
Let us denote $\widehat{G}_n=\sum_{i=1}^{L_n}\exp(\hat{c}^n_i)\delta_{(\hat{\Bbm}^n_{Q,i},\hat{\Abm}^n_{Q,i},\hat{\Bbm}^n_{V,i},\hat{\Abm}^n_{V,i})}$. Then, it follows from the result of Theorem~\ref{theorem:param_rate_nopara} and the formulation of the loss $\mathcal{D}_{1,r}$ that the convergence rates of the low-rank matrix estimators $\hat{\Bbm}^n_{Q,i},\hat{\Abm}^n_{Q,i},\hat{\Bbm}^n_{V,i},\hat{\Abm}^n_{V,i}$ are slower than any polynomial rates $\mathcal{O}_P(n^{-1/2r})$ for $r\in\mathbb{N}$. Thus, these rates could become as slow as $\mathcal{O}_P(1/\log^{\tau}(n))$ for some constant $\tau>0$ (due to the inequality $\log(n)<n$). As a consequence, we need an exponential number of data $\mathcal{O}(\exp(\epsilon^{-1/\tau}))$ to obtain the approximations of the low-rank matrices with an error $\epsilon$. This observation reflects the suboptimality of the sample complexity of the LoRA without applying the reparametrization technique.
% \begin{theorem}
%     \label{theorem:param_rate_nopara}
%     \begin{align*}
%         (\|\Delta \Bbm_{Q,ij'}\|^r +\|\Delta \Abm_{Q,ij'}\|^r+\|\Delta \Bbm_{V,ij'}\|^r +\|\Delta \Abm_{V,ij'}\|^r),
%     \end{align*}
% \end{theorem}
% Proof of Theorem~\ref{theorem:param_rate_nopara} is in Appendix~\ref{appendix:param_rate_nopara}.

\subsection{With Reparametrization: Optimal Sample Complexity}
\label{sec:with_reparametrization}

In this section, we consider the scenario where the low-rank matrices share their structures with each other. In particular, we consider the case where the low-rank matrices are reparameterized as: 
\begin{align*}
\Abm_Q = \Abm_V = \varphi_1({\Abm}) && 
\Bbm_Q = \Bbm_V = \varphi_2({\Bbm}),\end{align*}
where $\varphi_{1}:\mathbb{R}^{m \times m} \to \mathbb{R}^{r \times \bar{d}}$,  $\varphi_{2}:\mathbb{R}^{m \times m} \to \mathbb{R}^{r \times \bar{d}}$ are some functions,  $\Abm \in \mathbb{R}^{m \times m}, \Bbm \in \mathbb{R}^{m' \times m'}$ are learnable matrices with given dimensions $m, m' \geq 1$. We specifically note for the simplicity of the theoretical development, this formulation is simplified compared to what was used in practice because we set the low-rank matrices of queries to be equal to that of values. Nevertheless, as we will show in this section, even with this simplified formulation, reparameterization gives superior sample complexity compared to vanilla LoRA without reparameterization. After training, the reparameterization can be discarded, and we only need to store the low-rank matrices $\Abm_Q, \Abm_V, \Bbm_Q, \Bbm_V$. We observe that this reparameterization technique encodes a shared structure between the query and value low-rank matrices. The primary difference compared to the original LoRA is that instead of learning the low-rank matrices separately, we reparameterize those matrices as the output of the two shared structures $\varphi_1, \varphi_2$. The subsequent sections will present the theoretical advantage of reparameterization based on the following two settings:

\emph{(i) Linear reparametrization:} $\varphi_1({\Abm})=\Wbm_1\Abm$ and $\varphi_2({\Bbm})=\Wbm_2\Bbm$.

\emph{(ii) Non-linear reparametrization:} $\varphi_1({\Abm})=\sigma_1(\Wbm_1\Abm)$ and $\varphi_2({\Bbm})=\sigma_2(\Wbm_2\Bbm)$, where $\sigma_1$ and $\sigma_2$ are two non-linear activation functions applied element-wise to the matrices $\Wbm_1\Abm$ and $\Wbm_2\Bbm$. 

% \begin{align*}
%     &\Abm_Q =  \Abm_V = \sigma_2^\Abm({\Abm})\\
%     &\Bbm_Q = \sigma_1^\Bbm({\Bbm}), \Bbm_V = \sigma_2^\Bbm({\Bbm});\\
% \end{align*}

% $\Abm_Q= \Abm_V = \sigma_{1}(\Abm)$, $\Bbm_{Q}= \Bbm_{V} = \sigma_{2}(\Bbm)$ where $\Abm \in \mathbb{R}^{m \times m}, \Bbm \in \mathbb{R}^{m' \times m'}$ are learnable matrices,

% where the functions $\sigma_{1}:\mathbb{R}^{m \times m} \to \mathbb{R}^{r \times \bar{d}}$,  $\sigma_{2}:\mathbb{R}^{m \times m} \to \mathbb{R}^{r \times \bar{d}}$, and the dimension $m, m' \geq 1$ are given. 
\subsubsection{Simple Linear Reparameterization}
We first take into account the simple linear reparametrization where $\Abm_Q=\Abm_V=\Wbm_{1}\Abm$ and $\Bbm_Q=\Bbm_V=\Wbm_{2}\Bbm$. Under this setting, the ground-truth regression function in Eq.~(\ref{eq:true_regression_function}), which we denote now as $f_{\bar{G}_{*}}(\mathbb{X})$ to avoid confusion, takes the following form:
\begin{align}
 \hspace{-0.25 em}   & \sum_{j=1}^{L} \frac{\exp(\mathbb{X}^{\top} (\Mbm^0_{Q}+\Wbm_{2,j}^{*}\Bbm_{j}^*\Wbm_{1,j}^{*}\Abm_{j}^*)\Mbm^{0}_{K}\mathbb{X}+c^*_j)}{\bar{D}_{f}(\mathbb{X})} \cdot(\Mbm^0_{V}+\Wbm_{2,j}^{*}\Bbm_{j}^*
\Wbm_{1,j}^{*}\Abm_{j}^*)\mathbb{X}, \label{eq:true_regression_function_linear}
\end{align}
where we have $\bar{D}_{f}(\mathbb{X}) = \sum_{k = 1}^{L}\exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\Wbm_{2,k}^{*}\Bbm_{k}^*\Wbm_{1,k}^{*}\Abm_{k}^*)\Mbm^{0}_{K}\mathbb{X}+c^*_{k})$, while the mixing measure is of the form $\bar{G}_{*} := \sum_{j' = 1}^{L} \exp(c^*_{j'}) \delta_{\Wbm_{2,j'}^{*}\Bbm_{j'}^*
\Wbm_{1,j'}^{*}\Abm_{j'}^*}$. Similar to Section~\ref{sec:without_reparametrization}, we estimate the low-rank matrices via estimating the ground-truth mixing measure $\bar{G}_*$ using the least square method:
\begin{align}
    \label{eq:least_squared_estimator_linear}
    \bar{G}_n:=\argmin_{\bar{G}\in \bar{\mathcal{G}}_{L'}(\Theta)}\sum_{i=1}^{n}\Big\|\Ybm_i-f_{\bar{G}}(\mathbb{X}_i)\Big\|^2,
\end{align}
where $\bar{\mathcal{G}}_{L'}(\Theta):=\{G=\sum_{i=1}^{\ell}\exp(c_{i})\delta_{\Wbm_{2,i}\Bbm_{i}\Wbm_{1,i}\Abm_{i}}:1\leq \ell\leq L', \  (c_{i},\Wbm_{2,i},\Bbm_{i}, \Wbm_{1,i},\Abm_{i}) \in\Theta\}$ stands for the mixing measure set. To capture the convergence behavior of the estimator, we use the following Voronoi loss tailored to the simple linear reparameterization setting given by
\begin{align*}     
\mathcal{D}_2(\bar{G},\bar{G}_*)  &:=\sum_{j'=1}^{L}\Big|\sum_{i\in\mathcal{V}_{j'}}\exp(c_{i})-\exp(c_{j'}^{*})\Big| \\&+ {\sum_{j'\in[L]:|\mathcal{V}_{j'}|=1}\sum_{i\in\mathcal{V}_{j'}}\exp(c_{i}) \| \Zbm_i - \Zbm_{j'}^{*}\|}+\sum_{j'\in[L]:|\mathcal{V}_{j'}|>1}\sum_{i\in\mathcal{V}_{j'}}\exp(c_{i}) \| \Zbm_i - \Zbm_{j'}^{*}\|^2 ,
\end{align*}
where we denote $\Zbm:=\Wbm_2\Bbm\Wbm_1\Abm$. Given the above loss function, we study the sample efficiency of the LoRA with simple linear reparametrization in Theorem~\ref{theorem:param_rate_linear}.
\begin{theorem}    \label{theorem:param_rate_linear}
    The estimator $\bar{G}_n$ converges to the true mixing measure $\bar{G}_*$ at the following rate:
    \begin{align*}    \mathcal{D}_2(\bar{G}_n,\bar{G}_*)=\mathcal{O}_{P}(\sqrt{\log(n)/n}).
    \end{align*}
\end{theorem}
Proof of Theorem~\ref{theorem:param_rate_linear} is in Appendix~\ref{subsec:param_rate_linear}. The above bound together with the construction of the loss $\mathcal{D}_{2}$ indicates that the convergence rates of estimating low-rank matrices $\Wbm^*_{2,j'}\Bbm^*_{j'}\Wbm^*_{1,j'}\Abm^*_{j'}$, for $j'\in[L]$, range from order $\mathcal{O}_P([\log(n)/n]^{\frac{1}{2}})$ to order $\mathcal{O}_P([\log(n)/n]^{\frac{1}{4}})$. Thus, it costs at most a polynomial number of data $\mathcal{O}(\epsilon^{-4})$ to approximate those low-rank matrices with the error $\epsilon$. Compared to the exponential number of data required in the LoRA without reparametrization in Section~\ref{sec:without_reparametrization}, we observe that the LoRA with linear reparametrization is much more sample efficient.



\subsubsection{Non-Linear Reparameterization}
Next, we draw our attention to the LoRA with non-linear reparametrization where the low-rank matrices are parametrized as $\Abm_Q= \Abm_V = \sigma_{1}(\Wbm_1\Abm)$ and $\Bbm_{Q}= \Bbm_V = \sigma_{2}(\Wbm_2\Bbm)$. Then, the ground-truth regression function in Eq.(\ref{eq:true_regression_function}), denoted as $f_{\widetilde{G}_*}(\mathbb{X})$ in this section, admits the following form: 
\begin{align}
 \hspace{-0.25 em}   & \sum_{j=1}^{N} \frac{\exp(\mathbb{X}^{\top} (\Mbm^0_{Q}+\sigma_2(\Wbm^*_{2,j}\Bbm^*_j)\sigma_1(\Wbm^*_{1,j}\Abm^*_j))\Mbm^0_{K}\mathbb{X}+c^*_j)}{\widetilde{D}_{f}(\mathbb{X})}\cdot(\Mbm^0_{V,j}+\sigma_2(\Wbm^*_{2,j}\Bbm^*_j)\sigma_1(\Wbm^*_{1,j}\Abm^*_j))\mathbb{X}, \label{eq:true_regression_function_nonlinear}
\end{align}
where we denote $\widetilde{D}_{f}(\mathbb{X}) := \sum_{k = 1}^{N}\exp(\mathbb{X}^{\top} (\Mbm^0_{Q}+\sigma_2(\Wbm^*_{2,k}\Bbm^*_k)\sigma_1(\Wbm^*_{1,k}\Abm^*_k))\Mbm^0_{K}\mathbb{X}+c^*_k)$ and the mixing measure $\widetilde{G}_{*} := \sum_{j' = 1}^{L} \exp(c^*_{j'}) \delta_{(\Wbm^*_{2,j'}\Bbm_{j'}^*,\Wbm^*_{1,j'}\Abm_{j'}^*)}$ associated with unknown parameters $(c_{j'}^*,\Wbm^*_{2,j'}\Bbm_{j'}^*,\Wbm^*_{1,j'}\Abm^*_{j'})_{j'=1}^{L}$ in the parameter space $\Theta\subset\mathbb{R} \times\mathbb{R}^{d\times r}\times\mathbb{R}^{r\times d}$. The least-square estimator of the ground-truth mixing measure $G_*$ is now defined as
\begin{align}
    \label{eq:least_squared_estimator_nonlinear}
    \widetilde{G}_n:=\argmin_{\widetilde{G}\in \widetilde{\mathcal{G}}_{L'}(\Theta)}\sum_{i=1}^{n}\Big\|\Ybm_i-f_{\widetilde{G}}(\mathbb{X}_i)\Big\|^2.
\end{align}
where $\widetilde{\mathcal{G}}_{L'}(\Theta):=\{G=\sum_{i=1}^{\ell}\exp(c_{i})\delta_{(\Wbm_{2,i}\Bbm_{i},\Wbm_{1,i}\Abm_{i})}:1\leq \ell\leq L', \  (c_{i},\Wbm_{2,i}\Bbm_{i}, \Wbm_{1,i}\Abm_{i}) \in\Theta\}$. For the sake of capturing the convergence rate of the estimator $\widetilde{G}_n$, the Voronoi loss function is tailored to the non-linear reparametrization setting as  
\begin{align*}     
&\mathcal{D}_3(\widetilde{G},\widetilde{G}_*)  :=\sum_{j'=1}^{L}\Big|\sum_{i\in\mathcal{V}_{j'}}\exp(c_{i})-\exp(c^*_{j'})\Big| + \sum_{\substack{j'\in[L]:|\mathcal{V}_{j'}|=1, i\in\mathcal{V}_{j'}}}\exp(c_{i}) (\|\Delta (\Wbm_2\Bbm)_{ij'}\| +\|\Delta (\Wbm_1\Abm)_{ij'}\|)\\ 
& \hspace{5 em} +\sum_{\substack{j'\in[L]:|\mathcal{V}_{j'}|>1, i\in\mathcal{V}_{j'}}}\exp(c_{i}) (\|\Delta (\Wbm_2\Bbm)_{ij'}\|^2 +\|\Delta (\Wbm_1\Abm)_{ij'}\|^2) ,
\end{align*}
where we denote $\Delta (\Wbm_2\Bbm)_{ij'}:=\Wbm_{2,i}\Bbm_{i}-\Wbm^*_{2,j'}\Bbm^*_{j'}$ and $\Delta (\Wbm_{1}\Abm)_{ij'}:=\Wbm_{1,i}\Abm_{i}-\Wbm^*_{1,j'}\Abm^*_{j'}$  for any $i, j'$. Before presenting the main result of this section in Theorem~\ref{theorem:param_rate_nonlinear}, it is necessary to impose some mild assumptions on the activations $\sigma_1$ and $\sigma_2$. 

\vspace{0.5 em}
\noindent
\textbf{Assumptions.} We have the following assumptions on the activation functions $\sigma_1$ and $\sigma_2$:

\noindent
\emph{(A.1) (Algebraic independence)} If there exist parameters $(\Bbm,\Abm)$ and $(\Bbm',\Abm')$ such that $\sigma_2(\Bbm)\sigma_1(\Abm)=\sigma_2(\Bbm')\sigma_1(\Abm')$, then we obtain that $(\Bbm,\Abm)=(\Bbm',\Abm')$. 

\noindent
\emph{(A.2) (Uniform Lipschitz)} Let $F(\mathbb{X};\Bbm,\Abm):=\exp(\mathbb{X}^{\top}(\Mbm^0_{Q}+\sigma_2(\Bbm)\sigma_1(\Abm))\mathbb{X})(\Mbm^0_{V}+\sigma_2(\Bbm)\sigma_1(\Abm))\mathbb{X}$. Then, for any coefficient $\tau\in\{1,2\}$, the following inequality holds
    \begin{align*}
        \sum_{|\alpha|=\tau}\Bigg|\Big(\frac{\partial^{|\alpha|}F}{\partial \Abm^{\alpha_1}\partial \Bbm^{\alpha_2}}(\mathbb{X};\Bbm,\Abm)&-\frac{\partial^{|\alpha|}F}{\partial \Abm^{\alpha_1}\partial \Bbm^{\alpha_2}}(\mathbb{X};\Bbm',\Abm')\Big)\gamma^{\alpha}\Bigg|\leq C\|(\Bbm,\Abm)-(\Bbm',\Abm')\|^{\zeta}\|\gamma\|^{\tau},
    \end{align*}
    for any vector $\gamma\in\mathbb{R}^{2dr}$ and for some positive constants $\zeta$ and $C$ that are independent of $\mathbb{X}$ and $(\Bbm,\Abm),(\Bbm',\Abm')$. Finally, in the summation, the notation $\alpha=(\alpha_1,\alpha_2)\in\mathbb{N}^{r\times d}\times\mathbb{N}^{d\times r}$.

\noindent
\emph{(A.3) (Strong identifiability)} For any natural number $\ell$ and distinct parameters $\{(\Bbm_{j},\Abm_{j}):j\in[\ell]\}$, the functions in the set
\begin{align*}
    &\Big\{\Xbm^{(u)}, \Xbm^{(u)}\Xbm^{\top}\sigma_2(\Bbm_{j}),\Xbm^{(u)}\sigma_1(\Abm_{j})\Xbm, \Xbm^{\top}\sigma_2(\Bbm_{j}),\sigma_1(\Abm_{j})\Xbm,\\
    &\Xbm^{(u)}\Xbm^{(v)}, \Xbm^{(u)}\Xbm^{(v)}[\Xbm^{\top}\sigma_2(\Bbm_{j})]^2, \Xbm^{(u)}\Xbm^{(v)}[\sigma_1(\Abm_{j})\Xbm]^2,\\
    &\Xbm^{(u)}\Xbm^{(v)}\Xbm^{\top}\sigma_2(\Bbm_{j})\sigma_1(\Abm_{j})\Xbm:j\in[\ell], \ u,v\in[d]\Big\}
\end{align*}
are linearly independent for almost surely $\Xbm$.
\begin{theorem}
    \label{theorem:param_rate_nonlinear}
    Assume that the activation functions $\sigma_1$ and $\sigma_2$ satisfy the Assumptions (A.1)-(A.3). Then, the least square estimator $\widetilde{G}_n$ converges to the true mixing measure $\widetilde{G}_*$ at the following rate:
    \begin{align*}    \mathcal{D}_3(\widetilde{G}_n,\widetilde{G}_*)=\mathcal{O}_{P}(\sqrt{\log(n)/n}).
    \end{align*}
\end{theorem}
%Proof of Theorem~\ref{theorem:param_rate_nonlinear} is in Appendix~\ref{appendix:param_rate_nonlinear}.
Theorem~\ref{theorem:param_rate_nonlinear} suggests that the convergence rates of estimating low-rank matrices $\Wbm^*_{2,j}\Bbm^*_j$ and $\Wbm^*_{1,j}\Abm^*_j$ are either $\mathcal{O}_P([\log(n)/n]^{\frac{1}{2}})$ or $\mathcal{O}_P([\log(n)/n]^{\frac{1}{4}})$ depending on the cardinalities of their associated Voronoi cells, or equivalently, the number of their fitted parameters. In other words, we need a polynomial number of data, $\mathcal{O}(\epsilon^{-2})$ or $\mathcal{O}(\epsilon^{-4})$, to achieve the approximations of the low-rank matrices with the error $\epsilon$ when employing the LoRA with non-linear reparametrization. Compared with the LoRA without reparametrization, which requires up to an exponential amount of data for the same task, the LoRA with non-linear reparametrization is more sample efficient.