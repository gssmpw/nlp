%% bare_jrnl.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[journal]{IEEEtran}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{colortbl}
\usepackage{tabularx}
\usepackage{tabulary}
\usepackage{tabularray}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}
%\usepackage[table,xcdraw]{xcolor}
\usepackage{amsmath}
\usepackage{comment}
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage[table,xcdraw]{xcolor}         % colors
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{ulem}


\usepackage{dsfont}
\hypersetup{
    colorlinks=false,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}

%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Toward Foundational Model for Sleep Analysis  Using a Multimodal Hybrid Self-Supervised Learning Framework}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%

\author{Cheol-Hui Lee, 
        Hakseung Kim,
        Byung C. Yoon
        and Dong-Joo Kim 

% \author{Author 1, 
%         Author 2,
%         Author 3,
%         and Author 4

\IEEEcompsocitemizethanks{
    \IEEEcompsocthanksitem Cheol-Hui Lee is with the Department of Brain and Cognitive Engineering, Korea University, Seoul, South Korea; with Interdisciplinary Program in Precision Public Health, Korea University, Seoul, South Korea
    \IEEEcompsocthanksitem Hakseung Kim is with the Department of Brain and Cognitive Engineering, Korea University, Seoul, South Korea
    \IEEEcompsocthanksitem Byung C. Yoon is with the Department of Radiology, Stanford University School of Medicine, VA Palo Alto Health Care System, Palo Alto, CA, USA
    \IEEEcompsocthanksitem Dong-Joo Kim is with the Department of Brain and Cognitive Engineering, Korea University, Seoul, South Korea; with the Department of Neurology, Korea University College of Medicine, Seoul, South Korea; with Interdisciplinary Program in Precision Public Health, Korea University, Seoul, South Korea

    E-mail: dongjookim@korea.ac.kr}
    % \thanks{preprint}
}

% \IEEEcompsocitemizethanks{
%     \IEEEcompsocthanksitem ~~
%     \IEEEcompsocthanksitem ~~
%     \IEEEcompsocthanksitem ~~
%     \IEEEcompsocthanksitem ~~
%     }% <-this % stops a space
% }


% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.




% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
Sleep is essential for maintaining human health and quality of life. Analyzing physiological signals during sleep is critical in assessing sleep quality and diagnosing sleep disorders. However, manual diagnoses by clinicians are time-intensive and subjective. Despite advances in deep learning that have enhanced automation, these approaches remain heavily dependent on large-scale labeled datasets. This study introduces SynthSleepNet, a multimodal hybrid self-supervised learning framework designed for analyzing polysomnography (PSG) data. SynthSleepNet effectively integrates masked prediction and contrastive learning to leverage complementary features across multiple modalities, including electroencephalogram (EEG), electrooculography (EOG), electromyography (EMG), and electrocardiogram (ECG). This approach enables the model to learn highly expressive representations of PSG data. Furthermore, a temporal context module based on Mamba was developed to efficiently capture contextual information across signals. \textbf{SynthSleepNet achieved superior performance compared to state-of-the-art methods across three downstream tasks}: sleep-stage classification, apnea detection, and hypopnea detection, with accuracies of 89.89\%, 99.75\%, and 89.60\%, respectively. The model demonstrated robust performance in a semi-supervised learning environment with limited labels, achieving accuracies of 87.98\%, 99.37\%, and 77.52\% in the same tasks. These results underscore the potential of the model as a foundational tool for the comprehensive analysis of PSG data. SynthSleepNet demonstrates comprehensively superior performance across multiple downstream tasks compared to other methodologies, making it expected to set a new standard for sleep disorder monitoring and diagnostic systems. The source code is available at https://github.com/dlcjfgmlnasa/SynthSleepNet.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
foundation model, multimodal self-supervised learning, polysomnography, automatic sleep staging, AHI detection
\end{IEEEkeywords}






% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle


\section{Introduction}

Sleep is critical in maintaining human health, alleviating mental and physical stress, and preserving physiological balance \cite{ref1}. Many individuals experience sleep disorders, prompting clinicians to use polysomnography (PSG) for diagnosing and monitoring physiological changes during sleep. PSG records multiple physiological signals, including electroencephalograms (EEG), electrooculograms (EOG), electromyograms (EMG), electrocardiograms (ECG), and airflow signals, providing comprehensive insights into sleep-related activities \cite{ref2, ref3}. Clinicians perform various diagnostic and evaluative processes for sleep disorders using these signals. However, these processes are labor-intensive and require significant expertise \cite{ref3}.

Researchers have proposed various deep-learning-based algorithms for automated sleep assessment to address these challenges. However, most of these approaches rely on supervised learning, which demands large amounts of labeled data \cite{ref4, ref5, ref6, ref7, ref8, ref9, ref10, ref11, ref12, ref13, ref14}. Recently, self-supervised learning (SSL), a method for extracting meaningful representations from unlabeled data, has been applied to PSG data \cite{ref15, ref16, ref17, ref18, ref19, ref20, ref21, ref22, ref23}. SSL facilitates the discovery of high-level semantic patterns without labels by training on pseudo-labels generated through predefined tasks, creating a generalized network that can be fine-tuned for specific downstream applications. SSL methodologies are broadly classified as single-modality \cite{ref15, ref16, ref17, ref18, ref19} and multimodal approaches \cite{ref20, ref21, ref22, ref23}, with the latter integrating information from multiple modalities.

Existing methodologies have demonstrated significant advancements but retain notable limitations. Most studies are designed for single tasks, primarily focusing on sleep stage classification \cite{ref13, ref15, ref16, ref17, ref18, ref19, ref20, ref21, ref22}. However, clinicians evaluate sleep quality using multiple indicators, rendering these approaches overly restrictive \cite{ref2, ref3, ref24}. Although detecting apnea and hypopnea is essential, research in these areas remains limited. Additionally, current methodologies predominantly employ contrastive learning within the SSL paradigm. While effective in optimizing inter-modality relationships and learning discriminative representations, this approach has drawbacks. Specifically, it relies heavily on the performance of the backbone network and EEG data augmentation \cite{ref17, ref19}. It reduces efficacy in learning generative representations \cite{ref25, ref26}.

This study proposes SynthSleepNet, a multimodal hybrid SSL methodology for analyzing PSG data. Drawing inspiration from NeuroNet \cite{ref19} and MultiMAE \cite{ref27}, SynthSleepNet integrates masked prediction and contrastive learning to fully train generative and discriminative representation capabilities. This is the first multimodal SSL approach to combine these techniques. Unlike existing methodologies, SynthSleepNet evaluates sleep quality comprehensively by performing three downstream tasks: sleep stage classification, apnea detection, and hypopnea detection. Experimental results demonstrate that SynthSleepNet surpasses state-of-the-art methods across all three tasks and excels in semi-supervised learning environments. The proposed methodology is expected to establish a new foundation for sleep analysis.

\section{Related Work}
\subsection{Self-Supervised Learning Methodology for Sleep Assessment with Single-Modal Physiological Signals}

SSL methodologies designed for single-modal physiological signals primarily target sleep-stage classification, focusing predominantly on EEG. BENDR \cite{ref15} incorporates a convolution neural network (CNN)-based module to extract EEG features and a transformer to capture temporal contexts across signals. This model employs contrastive learning by designing output vectors from the CNN-based module and transformer as positive pairs if they correspond to the same time point while treating others as negative pairs. ContraWR \cite{ref16} replaces the standard InfoNCE loss used in contrastive learning with triplet loss, which minimizes and maximizes the distances between positive and negative pairs, respectively. In this framework, negative pairs are defined as the mean of each sample. TS-TCC \cite{ref17} applies two distinct augmentations to the same EEG data and utilizes a temporal contrasting module to enhance similarity between the contexts of identical samples while reducing similarity between different contexts of distinct samples. Similarly, mulEEG \cite{ref18} drops the augmentation methodology of TS-TCC \cite{ref17} but extends it using multiview SSL to improve learning. This approach incorporates EEG signals and spectrograms as input data, leveraging a diverse loss function to extract complementary information across multiple views. NeuroNet \cite{ref19} introduces an integrated approach combining masked-prediction-based SSL with contrastive-learning-based SSL to derive unique and discriminative representations. Employing a masked autoencoder structure, NeuroNet \cite{ref19} performs masked prediction while simultaneously processing two differently sampled vectors through an encoder. The network optimizes learning using the NT-Xent loss for contrastive learning, enhancing its ability to identify meaningful patterns and representations in EEG data.


\subsection{Multimodal Self-Supervised Learning Methodology for Sleep Assessment with Multimodal Physiological Signals}

Several multimodal SSL methodologies have been designed for sleep-stage classification, leveraging multiple physiological signal modalities. MVCC \cite{ref20} incorporates an intra-view temporal contrastive module to extract temporal features within individual modalities and an inter-view consistency contrastive module to ensure coherence across multiple signal modalities. COCOA \cite{ref21} introduces a cross-modality correlation loss to maximize the similarity between representations of different modalities for the same sample while minimizing the similarity between representations of different time intervals within the same modality. This is achieved using an intra-modality discriminator loss, which refines representation quality. CroSSL \cite{ref22} is distinguished by its robust flexibility, particularly in scenarios with missing data. The method employs the VICReg loss to minimize the dissimilarity between representations of different modalities. SleepFM \cite{ref23} adopts a leave-one-out contrastive learning strategy based on the InfoNCE loss and applies it to various sleep-related downstream tasks. MVCC \cite{ref20}, COCOA \cite{ref21}, CroSSL \cite{ref22}, and SleepFM \cite{ref23} represent contrastive-learning-based multimodal SSL methodologies.


\begin{figure*}[!t]
\centering
\includegraphics[width=1\textwidth]{figures/SynthSleepNet.pdf}
\caption{Overall architecture. (A) Training process of the modality-specific backbone, which extracts features from physiological signals for each modality. The pretrained backbone serves as the encoder for SynthSleepNet. (B) Training workflow of SynthSleepNet—a multimodal hybrid self-supervised learning framework. (C) The pretrained SynthSleepNet (excluding the decoder) is applied to three downstream tasks: sleep stage classification, apnea detection, and hypopnea detection.}
\label{fig:figure1}
\end{figure*}

\section{Methodology}

SynthSleepNet introduces an advanced multimodal hybrid SSL framework to comprehensively integrate diverse physiological signaling modalities. Figure \ref{fig:figure1} illustrates the overall architecture of SynthSleepNet, with detailed explanations provided below.


\subsection{SynthSleepNet: Multimodal Hybrid Self-Supervised Learning Framework for PSG}

\subsubsection{Model Architecture} ~

\textit{(Modality-Specific Backbone)} The Modality-Specific Backbone was tailored to extract features unique to each physiological signal modality. Drawing inspiration from the NeuroNet \cite{ref19} architecture, it combines identifying distinctive data features through masked prediction tasks with the discriminative representation capabilities offered by contrastive learning. This study grouped physiological signals with similar characteristics to develop modality-specific backbones. Hence, four separate backbones were pretrained to process EEG, EOG, EMG, and ECG signals, as demonstrated in Figure \ref{fig:figure1} (A).

\textit{(Encoder)} The encoder serves as a foundational component of SynthSleepNet, mirroring the structure of the modality-specific backbone. SynthSleepNet integrates pretrained modality-specific backbones into encoders that are optimized for the unique attributes of the input physiological signals. For instance, when processing EEG C4 \& C3 channels and EOG Left \& Right channels, SynthSleepNet incorporates ``pretrained EEG-specific backbones'' and ``pretrained EOG-specific backbones,'' assigning two encoders for each signal type. LoRA \cite{ref28} was applied to each encoder to enhance the precision of information extraction. LoRA \cite{ref28} facilitates efficient fine-tuning by employing rank-decomposition weight matrices, thereby minimizing the need to alter the entire weight set. This process facilitates each encoder to produce output vectors $\{e_i^m\}_{i=1}^N$, representing the ``signal tokens'' illustrated in Figure 1 (B), where $N$ indicates the number of tokens, $m \in [1,2,3,\dots,M]$ identifies the index of the input physiological signal, and $M$ denotes the total number of input physiological signals, leading to $M$ encoders.

\textit{(Multimodal Encoder)} The multimodal encoder integrates modality-specific features extracted by individual encoders using a standard Vision Transformer (ViT) \cite{ref29}. The input to the encoder consisted of tokens generated through a three-step process. First, the output vectors $\{e_i^m\}_{i=1}^N$ from each encoder were passed through separate projection layers, and positional encoding was added. This resulted in a new set of vectors, $\{z_i^m\}_{i=1}^N$. Next, a subset of these vectors, $\{z_i^m\}_{i=1}^N$, was randomly sampled, while the remaining tokens were masked. This sampled subset is denoted by $\{\tilde{z}_i^m\}_{i=1}^{\tilde{N}}$, where $\tilde{N}$ represents the number of sampled tokens. Finally, the sampled output vectors, $\{\tilde{z}_i^m\}_{i=1}^{\tilde{N}}$, from all encoders were concatenated and fed into the multimodal encoder. This produced the output vectors $\{\{\tilde{h}_i^m\}_{i=1}^{\tilde{N}}\}_{m=1}^M$, which represent the ``fusion tokens'' (see Figure \ref{fig:figure1} (B)).

\textit{(Decoder)} Separate ViT decoders were used for each representation vector of physiological signal to reconstruct the masked tokens, resulting in a total of $M$ decoders corresponding to the number of encoders. Decoders also used ViT \cite{ref29}. However, the decoders were removed after the SSL phase. The decoder considered the output vectors $\{\{\tilde{h}_i^m\}_{i=1}^{\tilde{N}}\}_{m=1}^M$ as input from the multimodal encoder corresponding to each physiological signal combined with the masked vectors. After that, the input vectors were processed through a projection layer and positional encoding. The masked vector represented the vectors excluded during random sampling and contained information omitted from the input data. Each decoder generated $\{d_i^m\}_{i=1}^N$ through this process, corresponding to the ``predicted tokens'' (see Figure \ref{fig:figure1} (B)).

\subsubsection{Training Objectives} ~

\textit{(Masked Prediction)} Masked prediction involved concealing specific portions of the input data and training the model to predict the hidden parts. This approach enabled the model to infer missing information and learn intrinsic patterns and relationships within the data. The mean square error (MSE) loss was applied to the masked prediction. Specifically, the output vectors $\{d_i^m\}_{i=1}^N$ produced by each decoder were passed through a projection layer to obtain transformed vectors $\{r_i^m\}_{i=1}^N$, which matched the size of the output vectors $\{e_i^m\}_{i=1}^N$ of the encoder (the output vectors of the encoder represent the ``signal tokens'' in Figure \ref{fig:figure1} (B)). After that, the MSE loss was computed between $\{r_i^m\}_{i=1}^N$ and $\{e_i^m\}_{i=1}^N$, focusing solely on the masked vectors. The loss function is expressed as:

\begin{equation}
L_{\text{recon}} = \frac{1}{M(N-\tilde{N})} \sum_{m=1}^M \sum_{i=1}^{N-\tilde{N}} (e_i^m - r_i^m)^2
\end{equation}

where $M$ represents the number of physiological signals, $N$ denotes the total number of tokens, and $\tilde{N}$ corresponds to the number of sampled tokens. This approach ensured that the model focused on accurately reconstructing the masked portions of the input data.

\textit{(Contrastive Learning)} SynthSleepNet incorporated the NT-Xent loss \cite{ref30} to optimize the relationships between the output vectors of the encoder and the multimodal encoder. Specifically, the method reduces the distance between output vectors for identical inputs to both the encoder and the multimodal encoder. Conversely, it increases the distance for different input instances. This alignment ensures that the semantic information extracted by the encoder for individual signals is consistent with the semantic information extracted by the multimodal encoder when processing all signals together, enabling effective integration of information from multiple physiological signals.

Encoder output vectors $\{e_i^m\}_{i=1}^N$ were averaged elementwise to derive $\mathit{signal}^m$. Similarly, multimodal encoder output vectors $\{\{\tilde{h}_i^m\}_{i=1}^{\tilde{N}}\}_{m=1}^M$ were averaged to produce the $\mathit{fusion}$ representation. After that, these representations, $\{\mathit{signal}^m\}_{m=1}^M$ and $\mathit{fusion}$, were mapped to a latent space and normalized, resulting in $\{\mathit{sh}^m\}_{m=1}^M$ and $\mathit{fh}$, respectively. The NT-Xent loss \cite{ref30} was applied as follows:

\begin{equation}
L_{\text{contra}} = \frac{1}{2NM} \sum_{m=1}^M \sum_{k=1}^N \left[ l(2k-1, 2k, m) + l(2k, 2k-1, m) \right]
\end{equation}

\begin{equation}
l(i, j, m) = -\log \left( 
\frac{\exp\left(\mathit{sim}(\mathit{fh}_i, \mathit{sh}_j^m)/\tau\right)}
{\sum_{k=1}^{2N} 1_{[k \neq i]} \exp\left(\mathit{sim}(\mathit{fh}_i, \mathit{sh}_k^m)/\tau\right)}
\right)
\end{equation}

Here, $N$ represents the batch size, $\mathit{sim}$ refers to cosine similarity, and $\tau > 0$ corresponds to the temperature scaling factor.

\textit{(Joint Loss)} 
SynthSleepNet combined masked prediction and contrastive learning to generate robust representations of physiological signals. Masked prediction captured semantic features through reconstruction, while contrastive learning aligned relationships across individual and multimodal signal representations. The combined loss function is expressed as:
\begin{equation}
L_{\text{total}} = L_{\text{recon}} + \alpha L_{\text{contra}}
\end{equation}
where $\alpha$ denotes a balancing hyperparameter that controls the relative contribution of the two loss components ($L_{\text{recon}}$ and $L_{\text{contra}}$).

\subsection{Mamba based on Temporal Context Module}

\begin{figure}[!b]
    \centerline{\includegraphics[width=\columnwidth]{figures/Mamba_based_TCM.pdf}}
    \caption{Structure of the Mamba-based temporal context module.}
    \label{fig:figure2}
\end{figure}

The American Academy of Sleep Medicine (AASM) \cite{ref24} guidelines emphasize that sleep stage classification relies on local features within individual PSG epochs and relationships between adjacent epochs. For example, the AASM smoothing rule \cite{ref24} identifies sleep stages—such as Wake, REM, or Non-REM—that persist for 3–5 minutes or longer as new cycles. Shorter stages are often treated as transient and disregarded. Consequently, an effective sleep-stage classification model requires a module capable of capturing inter-epoch features across multiple PSG epochs. This module is referred to as the temporal context module (TCM). Most existing studies implement TCMs using recurrent neural networks (RNNs) or multihead attention mechanisms. However, this study introduces a novel TCM model based on the Mamba framework \cite{ref31}, a linear-time sequence modeling approach.

Mamba \cite{ref30} addresses the limitations of traditional sequential models, particularly RNNs, which exhibit stepwise dependencies that hinder parallelization and increase computation time. Mamba enables parallel computations with processing speeds proportional to the sequence length by reducing sequential dependencies, making it highly efficient for long-sequence data. A key feature of Mamba is its selective state-space approach, which selectively tracks only the most relevant states while ignoring less significant ones. This strategy enhances computational efficiency by eliminating unnecessary state exploration, thus reducing resource usage without compromising model performance. Mamba has demonstrated significant success in tasks involving sequential data, such as time-series prediction. We adopted Mamba 2 \cite{ref32}, an enhanced version that introduces additional constraints on selective state-space parameters to further accelerate training speed.

The Mamba-based TCM was inspired by the IITNet-style TCM architecture \cite{ref13}. The proposed Mamba-based TCM involved the following six steps: (1) The pretrained SynthSleepNet was modified by removing its decoder, resulting in the SynthSleepNet w/o decoder, which served as the backbone network. This network extracted a vector sequence $F = \{f_i\}_{i=1}^N$, corresponding to a single PSG epoch. (2) The vector sequence $F$ was averaged element-wise to produce token $K$. (3) The backbone network for $T$ PSG epochs processed each epoch individually to extract vector sequences. These sequences were averaged element-wise to produce the vectors $\{K_i\}_{i=1}^T$, which were batch-normalized to stabilize learning. (4) The vectors $\{K_i\}_{i=1}^T$ were input into the Mamba model to temporal dependencies across PSG epochs, resulting in output vectors. (5) The input vectors $\{K_i\}_{i=1}^T$ and $\{M_i\}_{i=1}^T$ were summed element-wise (i.e., skip connections). (6) The combined vectors were passed through an projection layer to produce the final output vector. Figure \ref{fig:figure2} is an illustration of the Mamba-based TCM.

\section{Experiments}
\subsection{Dataset Description and Data Preprocessing}
The Sleep Heart Health Study (SHHS) \cite{ref33} is a multicenter cohort study aimed at examining the cardiovascular and other health outcomes associated with sleep-disordered breathing. The dataset consisted of two subsets: SHHS1 and SHHS2. Each subset included PSG recordings of multiple physiological signals, specifically two bipolar EEG channels (C4-A1 and C3-A2), one ECG channel, two EOG channels (Left, Right), two leg EMG channels, a snore sensor, pulse oximeters, and a body position sensor. Two EEG channels, two EOG channels, one ECG channel, and one leg EMG channel were selected for analysis. These signals underwent the following preprocessing steps: (1) All physiological signals were resampled to 100 Hz. (2) A robust scaler optimized for physiological data was applied to reduce the influence of outliers while preserving the relative scale of the features. (3) Different bandpass filters were applied according to the signal type: 0.5–40 Hz, 3–30 Hz, and 25–50 Hz for the EEG and EOG channels, ECG channel, and EMG channel, respectively. Only data from SHHS1 cells were used for this study.

\textit{(Sleep Stage Classification)} Each 30-s segment in the dataset was annotated by sleep experts into one of eight categories: Wake, Non-REM1 (N1), Non-REM2 (N2), Non-REM3 (N3), Non-REM4 (N4), REM, Movement, and Unknown. N3 and N4 were merged into a single class (N3), while the "Movement" and "Unknown" categories were excluded to adhere to the AASM standard.

\textit{(Apnea Detection)} The SHHS dataset included annotations for three types of apnea events: obstructive, central, and mixed apnea. These categories were consolidated into a single class. Each 30-s segment was labeled as 1 if an apnea event was detected and 0 otherwise.

\textit{(Hypopnea Detection)} Hypopnea-related events in the SHHS dataset were recorded as a single category. Similarly, each 30-s segment was labeled as 1 if a hypopnea event was present or 0 otherwise.

\subsection{Evaluation Schema}
Five-fold subject-group cross-validation was conducted to assess the performance of the methodologies. The evaluation framework was tailored for both SSL-based methodologies and supervised methodologies. The dataset was divided into three subsets: training, pre-training, and testing. The pre-trained group was used for unsupervised training of the SSL model without labels. The training group, consisting of a small subset of labeled data, was used for linear evaluation and fine-tuning. It involved attaching a downstream classifier to the SSL-trained network and completing downstream tasks. The test group was used for the final performance evaluation. The dataset was split into training and test subsets for supervised methodologies, with the test subset remaining consistent across both SSL-based and supervised methodologies. Three evaluation scenarios were implemented. Detailed descriptions of these scenarios are provided below.

\textit{(Evaluation Scenario 1: Linear Probing)} The backbone network (i.e., SynthSleepNet w/o decoder) remained fixed in this scenario, while only the downstream classifier was trained. The evaluation aimed to determine the effectiveness of each SSL methodology in capturing representations of PSG data.

\textit{(Evaluation Scenario 2: Fine-Tuning with Temporal Context Module)} This scenario evaluated the combined model, SynthSleepNet+TCM, which integrated the backbone network with the TCM. Most parameters of the backbone network were frozen except for a specific segment of the final layer in the multimodal encoder (i.e., the attention projection layer). This approach facilitated additional learning of nonlinear features in the data and leveraged multi-epoch information, which was expected to outperform Scenario 1. Additionally, this evaluation method compared SSL-based methodologies and supervised learning approaches.

\textit{(Evaluation Scenario 3: Semi-Supervised Learning)} This scenario examined the performance of semi-supervised learning methods. The proposed methodologies, SynthSleepNet and SynthSleepNet+TCM, were compared with SalientSleepNet (a supervised learning-based methodology) and SleepFM (an SSL-based methodology). Adjustments were made to ensure the number of modalities was consistent across all methods. Only approximately 1\% and 5\% of the labeled data from the entire dataset were used for training for the semi-supervised learning experiments.


\subsection{Performance Metrics}
Three metrics were employed to evaluate the performance of the proposed model: overall accuracy (\textit{ACC}), macro F1 score (\textit{MF1}), and Cohen’s kappa coefficient (\textit{Kappa}). These metrics are widely recognized, with \textit{MF1} and \textit{Kappa} being particularly suitable for datasets with class imbalances. The formulae for \textit{ACC} and \textit{MF1} are as follows:
\begin{equation}
\textit{ACC} = \frac{\sum_{i=1}^K TP_i}{N}
\end{equation}
\begin{equation}
\textit{MF1} = \frac{\sum_{i=1}^K F1_i}{K}
\end{equation}
where $TP_i$ and $N$ represent the true positives for the $i$-th class and the total number of samples, respectively. Similarly, $F1_i$ and $K$ denote the F1 score for the $i$-th class and the total number of classes, respectively. \textit{Kappa} measures the level of agreement between two observers on categorical values and is expressed as:
\begin{equation}
\textit{Kappa} \equiv \frac{p_o - p_e}{1 - p_e} = 1 - \frac{1 - p_o}{1 - p_e}
\end{equation}
where $p_o$ denotes the observed accuracy and $p_e$ represents the expected chance agreement.

\subsection{Other Approaches}
\input{tables/table1}

Representative methodologies were selected and implemented across four paradigms to evaluate the performances of SynthSleepNet. Five methodologies were chosen and implemented under \textit{“SSL + single-modality”}: BENDR \cite{ref15}, ContraWR \cite{ref16}, TS-TCC \cite{ref17}, mulEEG \cite{ref18}, and NeuroNet \cite{ref19}. Four methodologies were selected under \textit{“SSL + multi-modality”}: MVCC \cite{ref20}, COCOA \cite{ref21}, CroSSL \cite{ref22}, and SleepFM \cite{ref23}. Four methodologies were implemented in \textit{“supervised learning + single-modality”}: IITNet \cite{ref13}, AttnSleep \cite{ref4}, SleepExpertNet \cite{ref5}, and RAFNet \cite{ref7}. Finally, five methodologies were selected and implemented under the paradigm of \textit{“supervised learning + multi-modality”}: U-Sleep \cite{ref8}, SalientSleepNet \cite{ref9}, XSleepNet \cite{ref10}, MMASleepNet \cite{ref11}, and DynamicSleepNet \cite{ref12}. Brief descriptions of these methodologies are provided below.

Table \ref{table:table1} summarizes all methodologies considered in this study. Most selected methods are designed for sleep-stage classification, which has been more extensively studied than other sleep assessment methodologies. Moreover, additional sleep assessment approaches include RAFNet \cite{ref7}, MultiUNet, and SleepFM \cite{ref23}, designed to detect apnea, arousal, and sleep disorders, respectively. Notably, SleepFM \cite{ref23} differs from other methodologies by performing two downstream tasks. Both BioSig-UNet \cite{ref14} and SleepFM \cite{ref23} use respiratory signals. However, these signals were excluded from the present study due to their limited quantity and poor quality in the SHHS dataset. Consequently, BioSig-UNet and SleepFM \cite{ref23} were appropriately reimplemented to align with the requirements of the study.

\section{Results}
\subsection{Evaluation Scenario 1: Linear Probing}
\input{tables/table2}
\input{tables/table3}

Single-modality SSL methodologies, including BENDR \cite{ref15}, ContraWR \cite{ref16}, TS-TCC \cite{ref17}, mulEEG \cite{ref18}, and NeuroNet \cite{ref19}, achieved effective results for sleep-stage classification but performed poorly in detecting apnea and hypopnea (see Table \ref{table:table2}). Specifically, \textit{ACC} exceeded 70, yet the \textit{Kappa} coefficient remained at or below 0.25 in apnea and hypopnea detection, signifying suboptimal performance. Conversely, multimodal SSL methodologies, such as COCOA \cite{ref21}, CroSSL \cite{ref22}, and SleepFM \cite{ref23} performed consistently well across three downstream tasks. Among these, SleepFM \cite{ref23} achieved notably high performance. However, it underperformed relative to SynthSleepNet when identical modalities (EEG2, EOG2, and ECG1) were used. The respective differences in \textit{MF1} scores across the three downstream tasks were approximately 1.39, 6.33, and 5.45.

SynthSleepNet demonstrated the highest performance across all three downstream tasks, underscoring its superior capacity for data representation compared to existing SSL approaches. Notably, SynthSleepNet exhibited optimal performance with the modality combination of “EEG2+EOG2+EMG1” for sleep stage classification and “EEG2+EOG2+EMG1+ECG1” for apnea and hypopnea detection. An analysis of the impact of different modality combinations revealed that EEG, EOG, and EMG contributed positively to sleep-stage classification, while ECG and EMG significantly enhanced apnea and hypopnea detection, with ECG playing a particularly critical role. For instance, the inclusion of ECG in the combination “EEG1+EOG1+ECG1” significantly improved performance in apnea detection, evidenced by a sharp increase in the \textit{Kappa} by approximately 0.39 compared to “EEG1+EOG1.”

\subsection{Evaluation Scenario 2: Fine-tuning with Temporal Context Module}


\input{tables/table4}

The results from evaluation scenario 2, incorporating TCM, indicated that SynthSleepNet+TCM achieved superior performance across all metrics compared with state-of-the-art supervised learning methodologies \cite{ref4, ref5, ref7, ref8, ref9, ref10, ref11, ref12, ref13} (Table \ref{table:table3}). Notably, SynthSleepNet+TCM demonstrated exceptional performance with only limited labeled data, while supervised learning approaches required relatively large volumes of labeled data.

SynthSleepNet exhibited optimal performance with specific modality combinations for different tasks: “EEG2+EOG2+EMG1” for sleep-stage classification, “EEG2+EOG2+EMG1+ECG1” for apnea detection, and “EEG2+EOG2+ECG1” for hypopnea detection. Furthermore, SynthSleepNet+TCM significantly outperformed SynthSleepNet. For instance, SynthSleepNet+TCM demonstrated improvements in \textit{MF1} scores of approximately 8.62, 5.57, and 2.23 in sleep-stage classification, apnea detection, and hypopnea detection using the “EEG2+EOG2+EMG1+ECG1” modality combination, respectively, compared to SynthSleepNet. These findings underscore the efficacy of the Mamba-based TCM and the fine-tuning approach in enhancing the performance of the model.

\subsection{Evaluation Scenario 3: Semi-Supervised Learning}


Table \ref{table:table4} comprehensively analyzes methodologies applied in a semi-supervised learning context. Scenario 3 evaluated SalientSleepNet \cite{ref9} and SleepFM \cite{ref23} as benchmarks against SynthSleepNet. SalientSleepNet \cite{ref9} and SleepFM \cite{ref23} are supervised learning methods and SSL-based approaches, respectively. Additionally, the evaluation used only approximately 1\% and 5\% of the labeled data subsets, contrary to the full labeled dataset.

The performance comparison between models trained on fully and minimally labeled data revealed a significant drop for SalientSleepNet \cite{ref9}. For instance, SalientSleepNet exhibited reductions of approximately 27.18, 28.98, and 0.35 in \textit{ACC}, \textit{MF1}, and \textit{Kappa}, respectively, during sleep-stage classification with the modality combination “EEG1 + EOG1” when using 1\% of labeled data compared to the fully labeled dataset (Table \ref{table:table2} and \ref{table:table3}). SleepFM \cite{ref23} displayed comparatively better performance across both 1\% and 5\% labeled data subsets, achieving higher \textit{ACC}, \textit{MF1}, and \textit{Kappa} values than SalientSleepNet \cite{ref9}. However, its performance also diminished substantially compared to results derived from fully labeled data. SynthSleepNet and SynthSleepNet+TCM demonstrated exceptional robustness under limited labeled data conditions, maintaining consistently high performance across all metrics. For example, SynthSleepNet+TCM exhibited minimal reductions in \textit{MF1} in apnea detection with the modality combination “EEG1 + EOG1 + ECG1,” with decreases of only 7.49 and 6.96 for 1\% and 5\% of labeled data, respectively, compared to results obtained with fully labeled datasets. Under the same conditions, SynthSleepNet exhibited that \textit{MF1} decreased by 9.43 and 8.77 for 1\% and 5\% of labeled data, respectively. This indicates that SynthSleepNet demonstrates consistent performance relative to other methodologies, albeit not at the level achieved by SynthSleepNet+TCM.

\subsection{Ablation Experiments}

Ablation experiments were conducted to evaluate the performance of the proposed model. All experiments used the modality combination “EEG2 + EOG2,” with SSL training limited to 20 epochs to enhance experimental efficiency. The evaluation criterion was the performance in sleep stage classification. The optimal hyperparameters were determined based on the outcomes of these ablation experiments.

\subsubsection{Evaluation Scenario 1: Linear Probing} ~

\textit{(Masking Ratios)} SynthSleepNet achieved the highest \textit{MF1} of 73.91 at a masking ratio of 40\%. Figure \ref{fig:figure3} illustrates that SynthSleepNet without contrastive learning exhibited optimal performance at masking ratios between 50\% and 70\%, while SynthSleepNet without masked prediction performed optimally at masking ratios between 30\% and 50\%. When both tasks were applied concurrently, performance exceeded that observed with either task in isolation, demonstrating that tasks complemented and reinforced each other when combined.


\begin{figure}[!htbp]
\centering
\includegraphics[width=0.5\textwidth]{figures/Ablation_Mask_Ratio.pdf}
\caption{Effect of different masking ratios on SynthSleepNet performance.}
% \vspace{-4mm}
\label{fig:figure3}
\end{figure}

\input{tables/table5}

\textit{(Decoder Depth and Width)} Table \ref{table:table5} shows that SynthSleepNet achieved the highest \textit{ACC} and \textit{Kappa} values when the decoder dimension and depth were 256 and 2, respectively. The highest \textit{MF1} was observed when the decoder dimension and depth were 256 and 3, respectively. However, variations based on decoder size were minimal. This study incorporated the decoder configuration yielding the highest \textit{MF1} (decoder dimension: 256; decoder depth: 3) for training SynthSleepNet.

\textit{(Loss Balance Scale)}
An ablation study was conducted to optimize the balance between NT-Xent and MSE losses. Values of $\alpha < 1.0$ indicate a greater emphasis on the masked prediction task, while values of $\alpha > 1.0$ signify a stronger focus on contrastive learning. Table \ref{table:table6} shows that the model achieves optimal performance across all metrics when $\alpha = 1$, indicating that equal weighting of the two losses yields the best results.

\subsubsection{Evaluation Scenario 2: Fine-tuning with Temporal Context Module} ~

\input{tables/table6}
\input{tables/table7}

\textit{(Temporal Context Module)} This study investigated the most effective TCM structure for integrating and analyzing information across multiple PSG epochs. Table \ref{table:table7} demonstrates that the Mamba-based structure outperformed conventional architectures, such as LSTM \cite{ref13} and multihead attention \cite{ref4, ref5}, used in previous studies. Additionally, a performance analysis of Mamba with varying context lengths revealed that the highest \textit{ACC} and \textit{Kappa} values were achieved with a context length of 20.


\subsection{Rebound Point} 

\begin{figure*}[!htbp]
\centering
\includegraphics[width=0.8\textwidth]{figures/Elbow_point.pdf}
\caption{Performance of k-nearest neighbors probing across training epochs for SynthSleepNet with various modality combinations.}
\label{fig:figure4}
\vspace{-2mm}
\end{figure*}


Figure \ref{fig:figure4} depicts the k-nearest neighbor (k-NN) probing performance during SynthSleepNet training. Like linear probing, k-NN probing was employed to evaluate the representational capacity of SSL. The output vectors were dimensionally reduced using principal component analysis and subsequently used as inputs for the k-NN algorithm.

The results revealed a distinct pattern, with performance declining in the initial stages of training but improving after a specific threshold, referred to as the “rebound point.” Further analysis revealed that the rebound point occurred earlier when similar modalities were employed as inputs and were delayed for combinations of dissimilar modalities. For instance, training on similar modalities, such as EEG2, resulted in an earlier rebound point, whereas combinations like “EEG1+EOG1+EMG1” and “EEG2+EOG2+EMG1” showed later rebound points. Notably, “EEG1+EOG1+EMG1” showed the latest rebound point at 25 epochs.


\subsection{Hypnograms} 
Figure \ref{fig:figure5} presents the prediction results for subject \#202054. The top row displays the labels assigned by sleep experts. The left column illustrates the predictions of SynthSleepNet with linear probing, while the right column shows the predictions of SynthSleepNet+TCM with fine-tuning. Detailed analysis revealed that applying TCM and fine-tuning produced predictions closely aligned with expert-labeled data. SynthSleepNet+TCM using “EEG2+EOG2+EMG1” achieved predictions nearly identical to those of the sleep expert, differing by only three labels.


\section{Discussion}

\begin{figure*}[!t]
\centering
\includegraphics[width=0.80\textwidth]{figures/Hypnpgrams.pdf}
\caption{Hypnogram for subject \#202054. (A) Expert-labeled sleep stage scoring. (B)–(J) The left column presents results from SynthSleepNet with linear probing, while the right column displays results from SynthSleepNet+TCM after fine-tuning. Errors are marked by red dots.}
\label{fig:figure5}
\end{figure*}

This study proposes SynthSleepNet and Mamba-based TCM to address the limitations of existing sleep analysis methodologies. The proposed approach demonstrated superior performance across three tasks: sleep stage classification, apnea detection, and hypopnea detection. The results underscore the importance of overcoming the single-task focus prevalent in several deep-learning-based sleep analyses, enabling a more comprehensive evaluation of sleep states. Furthermore, SynthSleepNet outperformed methodologies designed for single tasks (Table \ref{table:table2}). The ability of SynthSleepNet to analyze unlabeled PSG data presents an opportunity to accelerate sleep research and support the development of related healthcare solutions.

SynthSleepNet represents a novel multimodal hybrid SSL framework that integrates masked prediction and contrastive learning, effectively leveraging EEG, EOG, EMG, and ECG data to achieve high representation learning performance. The combined application of masked prediction and contrastive learning operates complementarily, enhancing stability and facilitating the learning of robust, high-level representations (Figure \ref{fig:figure3}). Consequently, SynthSleepNet outperformed state-of-the-art SSL methodologies (Table \ref{table:table2}) and maintained strong performance in semi-supervised learning scenarios, even with only 1\% or 5\% of the labeled dataset (Table \ref{table:table4}).

Incorporating Mamba-based TCM during the fine-tuning of the pretrained SynthSleepNet significantly improved performance (Table \ref{table:table3}). The design of the Mamba-based TCM \cite{ref32}—unlike commonly used RNNs \cite{ref13} or multihead attention mechanisms \cite{ref4, ref5}—was critical to achieving these improvements (Table \ref{table:table7}). In conclusion, SynthSleepNet combined with the Mamba-based TCM outperformed state-of-the-art supervised learning methods, requiring extensive labeled datasets (Table \ref{table:table3}). Moreover, the performance gap was pronounced under semi-supervised learning conditions (Table \ref{table:table4}).

Recent mask-based SSL methodologies have demonstrated strong performance with mask ratios \cite{ref19, ref25, ref27} because higher mask ratios compel models to predict masked segments effectively, facilitating the learning of richer patterns and structures. However, SynthSleepNet operates with a relatively low mask ratio of 40\% despite employing a masked prediction task, which can be attributed to the following: First, excessively high mask ratios dilute the semantic information within the signal and fusion tokens, potentially impairing the contrastive learning capability of SynthSleepNet. Second, integrating multiple modalities increases the complexity of tasks of SynthSleepNet compared to single-modality methodologies. Consequently, a high mask ratio may overwhelm the model, leading to confusion.

Examining the performance of SynthSleepNet across various modality combinations revealed that EEG, EOG, and EMG produced the best results for sleep-stage classification (Tables \ref{table:table2} and \ref{table:table3}, Figure \ref{fig:figure5}), while ECG and EMG performed optimally for apnea and hypopnea detection (Tables \ref{table:table2} and \ref{table:table3}). This aligns with the guidelines outlined in the AASM manual \cite{ref24}, which is used by clinicians for sleep assessment. According to the AASM \cite{ref24}, EEG, EOG, and EMG are the key signals used for sleep-stage classification, while ECG, EMG, and airflow are essential for apnea and hypopnea detection. Combining all modalities (“EEG2+EOG2+EMG1+ECG1”) achieved high overall performance. However, it did not deliver the best outcomes for every task, likely due to the noisy and relatively limited nature of ECG data, which may cause distortion when integrated with other modalities.

The training process of SynthSleepNet exhibits a distinct “rebound point” phenomenon (Figure \ref{fig:figure4}), which deviates from typical patterns observed in deep learning models. This phenomenon reflects the time required for SynthSleepNet to effectively integrate information from different modalities. The “rebound point” occurred later when training on modalities with highly dissimilar features, indicating initial difficulty in reconciling modality discrepancies. Monitoring the “rebound point” using k-NN probing during training and adjusting training epochs accordingly is critical for optimizing performance.

SynthSleepNet has certain limitations despite several advantages. The relatively low \textit{Kappa} score for hypopnea detection highlights the need for improved data and labeling quality. Expanding dataset diversity is essential to enhance the generalizability of the model. Additionally, incorporating modalities such as photoplethysmogram signals, respiration signals, and sleep sounds may further improve performance. Third, additional sleep-related downstream tasks (e.g., arousal detection, SpO\textsubscript{2} desaturation detection, bruxism detection) should be explored for more comprehensive sleep analysis. Finally, lightweight optimization is essential to enable real-time processing in clinical applications.


\section{Conclusion}
This study introduces SynthSleepNet, a multimodal hybrid SSL framework, and Mamba-based TCM to overcome the limitations of existing deep learning methodologies for sleep assessment. SynthSleepNet integrates masked prediction and contrastive learning to effectively extract and fuse features from multimodal physiological signals (e.g., EEG, EOG, EMG, and ECG), facilitating the learning of high-level representations of PSG data. The Mamba-based TCM further improves model performance by capturing temporal dependencies within the PSG data. SynthSleepNet and Mamba-based TCM achieved superior performance in sleep-stage classification, apnea detection, and hypopnea detection while significantly reducing dependence on large-scale labeled datasets, as validated by experimental results. The proposed methodologies establish a robust foundation for advancing sleep research and broader applications in physiological signal analysis.

\appendices
\section{Training Setting and Hyperparameters}
The model training and evaluation were conducted on a computer equipped with an Intel I9-9980XE CPU (3.00GHz), 128GB RAM, and an NVIDIA 3090 GPU. All data processing and algorithm development were implemented in Python 3.10 using the PyTorch 2.0.1 library. Additionally, the hyperparameters used for NeuroNet, SynthSleepNet, and the downstream tasks are presented in Appendix Tables \ref{table:appendix_table1}, \ref{table:appendix_table2}, and \ref{table:appendix_table3}.

\setcounter{table}{0}
\input{tables/appendix_table1}
\input{tables/appendix_table2}
\input{tables/appendix_table3}

\bibliographystyle{IEEEtran}
\bibliography{reference}

\end{document}


