
\section{Introduction}
\seclabel{intro}


Mobile manipulators hold the promise of performing a wide range of useful tasks in our everyday environments.
However, a major obstacle to realizing this vision lies in the lack of precise mobile manipulation capabilities of current systems.
Many real world \added{mobile manipulation} tasks require \replaced{precise}{fine-grained} interaction with small objects, such as grasping a knob to pull open a cabinet or pressing a light switch, 
where even a \replaced{small}{1cm} deviation can cause failure.
\replaced{A mobile manipulator's mobility makes these tasks even more challenging.}{
These tasks are particularly challenging for mobile manipulators, as they face additional difficulties which stationary manipulators do not.}
\replaced{For example, small errors during navigation, say on a thick carpeted surface, can easily exceed the tight tolerance required for a precise tasks, and a non-holonomic base may limit precise repositioning.}{
For example, mobile manipulators need to navigate on diverse surfaces (e.g. carpets, tiles with ridges, over floor mats, etc.), 
where minor positioning errors can easily exceed tight tolerances required for success in fine-grained tasks.
This issue is further exacerbated when the mobile base is non-holonomic, 
limiting precise repositioning.}
\replaced{Furthermore, mobility means that mobile manipulators have to manipulate in varied locations and diverse lighting and under unnatural viewpoints (\eg looking top-down at a drawer from very close by), demanding significantly broader generalization than stationary manipulators confined to a fixed environment.}{Furthermore, unlike stationary manipulators confined to a fixed environment, mobile manipulators need to operate in varied locations, often from unnatural viewpoints and under diverse lighting conditions, thus requiring significantly broader generalization.}
As a result, developing mobile manipulators capable of performing precise tasks while generalizing to diverse environments remains an open problem.











Many precise mobile manipulation tasks involve {\it stylized} interactions with small objects: reaching a precise interaction site before executing a simple motion. 
\comment{Consider suggests its a running example, but it isn't?}\replaced{Think about}{Let's consider} grasping a cabinet knob to pull it open, or pushing a button on a microwave.
The difficulty in these tasks lies in reaching the accurate pre-task pose around the small target object (particularly in the presence of inaccuracies introduced during navigation, 
whereas the subsequent motion is easy to execute.
For getting to the accurate pre-task pose, open loop execution using a sense-plan-act
paradigm does not work because inaccuracies in perception and control prevent
correct engagement with the small interaction site. 
This necessitates a closed loop approach. Imitation learning is a natural choice, but
does not scale: learning a closed-loop policy for one specific skill (e.g. opening a cabinet) may require thousands of demonstrations, and even then may not generalize to the wide distribution of cabinets and environments.
So, how can we achieve precise mobile manipulation?






Visual servoing with an eye-in-hand camera is an effective
technique to close the loop to precisely pursue
targets~\cite{chaumette2016visual}
without requiring large-scale task-specific training for broad generalization, 
unlike closed-loop imitation learning.
However, vanilla visual servoing makes
strong assumptions, \eg requiring known 3D objects or target images, which are
not available in our in-the-wild setting. Our proposed approach, Servoing with 
Vision Models or \name, marries together
visual servoing with modern perception systems to mitigate these limitations.
This leads to an effective system which is able to operate in a closed loop
manner, but at the same time is versatile enough to operate in novel
environments on novel objects.

\name leverages modern perception systems in two ways. First, we use them to
specify targets for the visual servoing module. This alleviates the need for
known 3D objects or target images. We experiment with two ways to specify
targets: a) semantic categories, and b) points of interaction. For objects that
have a well-known semantic category (\eg drawer knobs or cabinet handles), we
use an open-world object detector (\eg Detic~\cite{zhou2022detecting}) to
continuously detect the target during visual servoing. However, not all mobile manipulation interaction sites, \eg the different buttons on a microwave, correspond to a semantic
category. We tackle such cases by using point trackers (\eg CoTracker~\cite{karaev2023cotracker}) to continuously track a user-indicated interaction site (\eg a single user click in the image, specifying which button on the microwave to push) over the course of visual servoing. Thus, the use of strong perception systems takes care of the target specification problem in visual servoing.

One problem however still remains. The use of visual servoing with an eye-in-hand
camera for manipulation tasks suffers due
to occlusion of the environment by the manipulator. Such occlusion can be
particularly detrimental if it leads to out-of-distribution input to the
perception system that now starts producing erroneous predictions (see
second row of \figref{occlusion}). We mitigate this issue using yet another advance in
computer vision: video in-painting models~\cite{chang2023look,
zhou2023propainter, lee2019copy}. We out-paint the robot end-effector to obtain
a \textit{clean} view of the scene (see last row of \figref{occlusion}). This improves the
detection performance of the vision system, leading to improved overall
success.

\begin{figure}
\setlength{\tabcolsep}{2pt}
\insertWL{2.0}{figures/figure_2_v2_cropped_brighter.pdf}
\caption{When using off-the-shelf vision detectors on wrist camera data, knob detections (indicated by the {\color{red} red point} in the second row) on the raw image are incorrect. Errors stem from occlusion due to the end-effector ({\bf left}) and due to the presence of the end-effector (out-of-distribution object) in the image even when the target itself is unoccluded ({\bf right}). Painting out the end-effector ({\bf bottom row}) fixes this issue.}
\figlabel{occlusion}
\end{figure}





We test \name across several real world tasks: grasping a knob to pull open
drawers / cupboards, grasping a handle to pull open a cabinet, pushing onto
light buttons, and pushing books into place on bookshelves. We obtain a 
85\% success rate on these challenging tasks, \textit{zero-shot on novel objects
in previously unseen environments}. As expected, \name performs much better than
open-loop control which only succeeds on 35\% trials. SVM without end-effector 
out-painting only obtained a 70\% success rate, suggesting that occlusion
caused by the end-effector does degrade performance and that our out-painting 
strategy is able to successfully mitigate it. Somewhat surprisingly, large vision models (Detic~\cite{zhou2022detecting}, CoTracker~\cite{karaev2023cotracker}, and ProPainter~\cite{zhou2023propainter} in our experiments)
performed quite well on out-of-distribution wrist camera images. 

Perhaps most interesting is the comparison of \name \vs imitation learning. 
As discussed before, fine-grained, closed-loop tasks are precisely where one
would expect imitation learning to excel over a modular approach such as \name.
So it is natural to ask how well does \name perform \vs imitation learning. We
conduct this comparison on the tasks of pulling on knobs and handles for opening
articulated objects. Specifically, we compare to the recent Robot Utility Models
(RUM) work from Etukuru \etal~\cite{etukuru2024robot}. RUM is trained on 1200
demonstrations for opening cabinets and 525 demonstrations for opening drawers
and thus serves as a very strong imitation learning comparison point. Quite
surprisingly, we find that \name outperforms RUM by an absolute success rate of
50\%. We believe this was made possible by \name's design: effectiveness and
robustness of a) servoing for control and b) large-scale vision models for perception.
\deleted{We believe this was because of the effectiveness and robustness of servoing for control and large-scale vision models for perception in the design of \name.}



To summarize, we developed \name a training-free approach to enable precise
mobile manipulation of small everyday objects. This was made possible by
marrying together visual servoing with large-vision models. Our experiments
reveal the effectiveness of our proposed approach, particularly over imitation
learning, the current favored approach for such tasks, even when it is trained
on 1000+ demonstrations. We hope that our paper provides an practical and
effective alternative to imitation learning for the class of tasks we
considered.
