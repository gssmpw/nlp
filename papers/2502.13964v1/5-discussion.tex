\section{Discussion}
\seclabel{discussion}
In this paper, we describe \name, a training-free framework for precise
manipulation tasks that involve reaching a precise interaction site followed by
execution of a primitive motion. Strong vision models help us mitigate issues
caused by occlusion by the end-effector thereby enabling the use of
off-the-shelf open-vocabulary object detectors and point trackers to estimate
servoing targets during execution. 
Use of strong off-the-shelf models also provides broad generalization for perception while the use of servoing provides robust control.
This enables \name to solve tasks in novel environments on novel objects,
obtaining a 85\% zero-shot success rate across 4 precise mobile manipulation
tasks. Most surprisingly, even though \name is modular, it outperforms the
strong end-to-end imitation learning system RUM~\cite{etukuru2024robot} that was
trained on 1000+ demonstrations. This is particularly striking as imitation learning is the tool of choice for precise tasks requiring closed-loop execution. 




\section{Limitations} Even though \name performs quite well across many
tasks on novel objects in novel environments, it suffers from some
shortcomings. Running these large vision models is computationally expensive
and we have to off load computation to a A40 GPU sitting in a server. Even with
this GPU, we are only able to run the vision pipeline at a 0.1 Hz
leading to slow executions. 
Building vision models specialized to wrist camera images may work better and
faster.
A second limitation is the reliance on depth from the wrist camera which may
be poor in some situations \eg shiny or dark objects. Use of learned
disparity estimators~\cite{xu2023iterative} with stereo images could
mitigate this. As our focus is on the precise reaching of interaction sites,
we work with a hand-crafted task decomposition into the interaction site and
primitive motion. In the future, we could obtain such a decomposition using
LLMs, or from demonstrations, thereby expanding the set of tasks we can tackle.
