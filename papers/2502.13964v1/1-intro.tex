
\section{Introduction}
\seclabel{intro}
\added{A mobile manipulation task, as simple as opening a drawer, requires high precision. At the same time, controlling mobile manipulators is difficult: \eg navigation on carpeted floors could be imprecise, \confirm{a holonomic base} can't simply translate laterally to adjust for inaccurate navigation. Furthermore, unlike stationary manipulators, mobile manipulators, by design, need to operate in varied locations, from unnatural views of the object, under challenging lighting, thus requiring significantly broader generalization. Thus, while mobile manipulators hold the promise of performing a wide range of useful tasks in our everyday environments, achieving reliable and generalizable mobile manipulation remains a significant challenge.}

\deleted{Unlike stationary manipulators that operate in one fixed environment, mobile manipulators need to be able to operate in a variety of settings, requiring them to generalize to the wide distribution of in-the-wild objects and environments.}
\confirm{Unlike stationary manipulators confined to a fixed environment, mobile manipulators need to operate in varied locations and under challenging lighting, thus requiring significantly broader generalization.} \deleted{to in-the-wild objects and environments.}
Furthermore, many \added[id=sg]{real-world mobile manipulation tasks} require \added[id=sg]{long-range navigation} and precise interaction with small objects, \added[id=sg]{think about navigating to a drawer to open it by pulling on its knob.}\deleted{, such as grasping a knob or pressing a button.}
\added[id=sg]{This coupled with the limited degrees of freedom and low cost hardware of typical mobile manipulators (\eg Stretch RE2), makes fine-grained mobile manipulation all the more difficult.}
Compared to stationary manipulators, many mobile manipulators (e.g. Stretch RE2) operate with limited degrees of freedom and lower-cost hardware, making fine-grained mobile manipulation significantly more difficult. 
\deleted{Imperfect state estimation and imprecise navigation on diverse surfaces further exacerbate these challenges for mobile manipulators, such that even minor inaccuracies can accumulate and result in task failure.}
\confirm{Imperfect state estimation from a distance and under varying lighting conditions, and imprecise navigation on diverse surfaces further exacerbate these challenges for mobile manipulators.}
As a result, developing mobile manipulators capable of performing fine-grained tasks while generalizing to previously unseen environments remains an open problem.






Many \replaced[id=sg]{precise}{fine-grained} mobile manipulation tasks  involve {\it stylized} interactions with small objects: reaching a precise interaction site before executing a simple motion. \added[id=sg]{Think about grasping a cabinet knob to pull it open, or pushing a button on a microwave.}
The difficulty in these tasks lies in reaching the accurate pre-task pose around the small target object, 
whereas the subsequent motion is easy to execute.
For getting to the accurate pre-task pose, open loop execution using a sense-plan-act
paradigm does not work because inaccuracies in perception and control prevent
correct engagement with the small interaction site. 
This necessitates a closed loop
approach. \replaced{Imitation learning would be a natural choice, but as our experiments will show, it fails to achieve sufficient generalization even when trained on 1000+ demonstrations.}{ but using imitation learning to train a closed loop policy to manipulate all the different little things around us suffers does not scale. Learning a closed-loop policy for one specific task, say opening a cabinet, via imitation, may require thousands of demonstrations, and even then may not generalize to the wide distribution of cabinets and environments in the real world.} 
\replaced[id=sg]{So, how can we achieve precise mobile manipulation?}{
In this work, we pursue a different approach of combining powerful vision foundation models with visual servoing,  we develop a training-free framework that enables a mobile manipulator to accurately reach the pre-task pose and manipulate novel little things in previously unseen real world environments.}






Visual servoing with an eye-in-hand camera is an effective
technique to close the loop to precisely pursue
targets~\cite{chaumette2016visual}
without requiring large-scale task-specific training for broad generalization, 
unlike closed-loop imitation learning.
However, vanilla visual servoing makes
strong assumptions, \eg requiring known 3D objects or target images, which are
not available in our in-the-wild setting. Our proposed approach, Servoing with 
Vision Models or \name, marries together
visual servoing with modern perception systems to mitigate these limitations.
This leads to an effective system which is able to operate in a closed loop
manner, but at the same time is versatile enough to operate in novel
environments on novel objects.

\name leverages modern perception systems in two ways. First, we use them to
specify targets for the visual servoing module. This alleviates the need for
known 3D objects or target images. We experiment with two ways to specify
targets: a) semantic categories, and b) points of interaction. For objects that
have a well-known semantic category (\eg drawer knobs or cabinet handles), we
use an open-world object detector (\eg Detic~\cite{zhou2022detecting}) to
continuously detect the target during visual servoing. However, not all little
things, \eg the different buttons on a microwave, correspond to a semantic
category. We \replaced{tackle such tasks by using a}{therefore leverage} point trackers (\eg CoTracker~\cite{karaev2023cotracker}) to continuously track a user-defined interaction site (\eg a single user click in the image, specifying which button on the microwave to push) over the course of visual servoing. This use of strong perception systems takes care of the target specification problem in visual servoing.

One problem however still remains. Use of visual servoing with an eye-in-hand
camera for manipulation tasks (not as much for pure pursuit tasks) suffers due
to occlusion of the environment by the manipulator. Such occlusion can be
particularly detrimental if it leads to out-of-distribution input to the
perception system that now starts producing erroneous predictions (see
second row of \figref{occlusion}). We mitigate this issue using yet another advance in
computer vision: video in-painting models~\cite{chang2023look,
zhou2023propainter, lee2019copy}. We out-paint the robot end-effector to obtain
a `clean' view of the scene (see last row of \figref{occlusion}). This improves the
detection performance of the vision system, leading to improved overall
success.

\begin{figure}
\setlength{\tabcolsep}{2pt}
\insertWL{2.0}{figures/figure_2_v2_cropped_brighter.pdf}
\caption{When using off-the-shelf vision detectors on wrist camera data, knob detections (shown by the {\color{red} red point}) on the raw image shown in the second row are incorrect. Errors stem from occlusion due to the end-effector ({\bf left}) and due to the presence of the end-effector (out-of-distribution object) in the image even when the target itself is unoccluded ({\bf right}). Painting out the end-effector ({\bf bottom row}) fixes this issue.}
\figlabel{occlusion}
\end{figure}

We test \name across several real world tasks: grasping a knob / handle to pull 
open articulated objects, and pushing onto buttons and books in shelves. We obtain a 85\% success 
rate on these tasks \textit{zero-shot on novel objects in previously unseen environments}. 
For the task of opening articulated objects, we compare to Robot Utility Models (RUM) \cite{etukuru2024robot},
a closed-loop imitation learning approach. RUM is trained on 1200 demonstrations for opening cabinets and 525 demonstrations
for opening drawers, and yet we find that \name is able to outperform it convincingly by an absolute success rate of 50\%.
Our experiments also find that \name
outperforms open-loop control and a version of \name without in-painting.
As our method doesn't require any training, this represents zero-shot results
on challenging problems involving stylized manipulation of little things in our everyday environments.

\noindent \textbf{Contributions and Findings.} Here, we list our study's contributions and key takeaways:
\begin{itemize}
  \item \textbf{Servoing with large vision models enables precise mobile manipulation.} 
  Our method, \name, allows mobile manipulators to perform fine-grained tasks with high precision. As a training-free framework, \name works across diverse tasks, and generalizes effectively to previously unseen objects in novel environments, achieving an 85\% success rate.
  \item \textbf{Modular vision-based control outperforms end-to-end imitation learning.} 
  \deleted{By leveraging the broad generalization capabilities of foundation models, our approach generalizes significantly better than an imitation learning policy trained on 1000+ demonstrations for a single task.}
  \confirm{Although fine-grained, closed-loop tasks are precisely where one would expect imitation learning to excel, we find that our modular training-free framework outperforms it in generalization, even when the imitation learning policy is trained on 1000+ demonstrations.}
  The imitation learning policy must learn every aspect of the problem from scratch, while our method directly utilizes robust and specialized foundation models pre-trained on massive datasets, allowing it to generalize effectively without task-specific training.
  \item \textbf{End-effector occlusion degrades performance, and inpainting resolves it.} Erroneous detections arise when the robotâ€™s end-effector occludes key visual features, but our video inpainting strategy restores accurate perception, significantly improving success rates.
  \item \textbf{Vision models remain effective despite out-of-distribution wrist camera images.} Large vision foundation models are not explicitly trained on wrist camera perspectives, yet they still perform well for object detection and tracking. Moreover, they remain robust even in the presence of inpainting artifacts, enabling reliable target localization for manipulation.
\end{itemize}













