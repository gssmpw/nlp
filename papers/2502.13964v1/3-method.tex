\begin{figure*}
\insertW{1.0}{figures/method_figure_cropped_brighter.pdf}
\caption{{\bf Servoing with Vision Models (\name)} is a  framework for precise reaching for mobile manipulators. Starting from an input RGB-D wrist camera image with a target specified either via a semantic label (\eg handle) or a user-clicked point on the image, \name outputs whole-body control commands to convey the end-effector to the target location by closing the loop with visual feedback. \name first paints out the end-effector using a video outpainting model (\secref{inpainting}), uses vision models to continuously detect the target object (or track the desired target point) to compute 3D servoing targets (\secref{localization}), which are passed to a servo to obtain whole-body control commands (\secref{adaptation}).}
\figlabel{overview}
\end{figure*}

\section{Task} Many everyday household tasks involve precise manipulation
followed by execution of a motion primitive. Examples include grasping a knob
or a handle to pull open a drawers / cupboard, or pushing a button on a
microwave. We consider two variants, where the interaction site can be
identified via a semantic label (\eg knob / handle) or via a user-specified
point (\eg a click on an image specifying the button to push); and assume that
the motion primitive is given or easy to specify. 
Our goal is to enable a commodity
mobile manipulator equipped with a RGB-D wrist camera to accomplish such tasks
in previously unseen environments.




\section{Method}
\seclabel{method} 
At a high-level, our method employs visual servoing on eye-in-hand camera images
(\secref{adaptation}) to control the end-effector to reach the interaction
site. Our innovation lies in the use of state-of-the-art vision models to
reliably detect / track the interaction site (\secref{localization}) to provide
the visual feedback for visual servoing. As we will see, occlusion due to the
end-effector in the wrist camera view causes nuisance. We deal with this by
painting out the end-effector (\secref{inpainting}) before running the perception
models on images.  Let's denote images from the wrist camera with $I_t$,
current robot state by $x_t$. The output actions are computed as follows:
\begin{equation}
a_t = \pi\left(x_t, g\left(f\left(I_t, [I_1, \ldots, I_{t-1}]\right)\right)\right),
\end{equation}
where $f(I, \bfI)$ is a video inpainting function that paints out the
end-effector from image $I$ using images in $\bfI$ as reference, $g(I)$
localizes the target in 3D in the wrist camera frame, and $\pi$ computes the
desired joint velocities using the current robot state $x_t$ and the current
target location output by $g$. \figref{overview} shows an overview of our
proposed method and we describe each component below.

\begin{figure*}
\insertW{1.0}{figures/figure_4_v2_cropped_brighten_good.pdf}
\caption{Visualization of objects and environments for the three different tasks: a) and b) pulling on a variety of handles / knobs to open articulated objects and c) pushing on user defined objects (books in a bookshelf and buttons). Note that we exclusively test on novel objects in novel environments not used for development in any manner.}
\figlabel{tasks}
\end{figure*}

\subsection{Inpainting}
\seclabel{inpainting}
Given RGB images from the wrist camera, the inpainting function $f$ uses past
frames from the wrist camera to inpaint the current frame $I_t$. 
We utilize a video inpainting method (as opposed to an image inpainting method)
for better performance: a video inpainting model has access to previous frames
(where the object may not be occluded), which can lead to improved
inpainting. We adopt the ProPainter model~\cite{zhou2023propainter} to realize $f$. It is a transformer-based video inpainting model trained on the YouTube-VOS
\cite{xu2018youtube} dataset.

If the object of interest is occluded by the end-effector in the first frame,
even a video inpainting method may not be able to accurately reconstruct the
object. To combat this, we design a ``look-around'' primitive that moves the
end-effector around (vertically and laterally) to obtain contextual information
about the scene. To limit the inference time in each iteration, we limit the
inpainting model to only look at the ten past images.  The ``look-around''
provides an initial set of ten images. 

Out painting the end-effector also requires a mask
of the end-effector.  We use a manually constructed mask that coarsely
covers the end-effector.  We find this to work better for out painting than a
fine mask of the end-effector obtained using the segmentation model
SAM~\cite{kirillov2023segment}.


\subsection{Interaction Site Localization}
\seclabel{localization}
Given an image with the end-effector painted out, our next goal is to localize
the object of interest to obtain 3D location for the target. We handle the two
specifications for the interaction site, via a semantic label or a user click,
separately as described below.

\subsubsection{Detection}
For semantically specified targets (\eg knobs / handles), we use Detic
\cite{zhou2022detecting}, an open-vocabulary detector trained on large-scale
datasets. We prompt Detic with the object class `handle' for handles and `knob'
for knobs.
If Detic detects multiple handles in the image, we select the handle closest to
the center of the image. Detic also outputs a mask for the object.  We compute the
center of the mask and use this as 2D position of the object of interest.

\subsubsection{Tracking}
For tasks specified via a user click, \eg the point on the book or the button in \figref{teaser}, we use of CoTracker \cite{karaev2023cotracker}, a
point tracking method for videos. Given a point in the first frame, CoTracker
is able to track it over subsequent frames seen during execution. CoTracker
notes that tracking performance is better when tracking many points together.
We therefore sample 40 points randomly around the user click and found that to
drastically improve tracking performance.


Either of these methods provides a 2D location in the image.  We lift this 2D
target location to 3D using the depth image.  In the case that the 2D position
of the target point is within the mask of the end-effector (i.e. occluded by
the end-effector), we utilize the depth from the nearest previous frame for
this 3D lifting.





\subsection{Closed-loop Control}
\seclabel{adaptation}
Given the interaction sites' 3D location, we employ visual servoing to realize $\pi$ to compute
velocity control commands. Visual servoing computes whole-body velocity 
control commands that minimize the distance between the end-effector 
and the 3D target point~\cite{chaumette2016visual}. 



