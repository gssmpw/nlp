\section{Related Work}
\seclabel{related}




\subsection{Visual Servoing}
Visual servoing (image-based,
pose-based, and hybrid approaches) outputs
control commands that convey the camera (and the attached manipulator) to a
desired location with respect to the scene through~\cite{corke1993visual,
chaumette2016visual, chaumette2006visual}.  Research has investigated use of
different features to compute distance between current and target images:
photometric distance~\cite{collewet2011photometric}, matching
histograms~\cite{bateux2016histograms}, features from pre-trained neural
networks~\cite{lee2017learning}, and has even trained neural networks to
directly predict the relative geometric transformation between
images~\cite{bateux2018training}. Visual servoing has been applied for 
manipulation~\cite{sadeghi2018sim2real}, 
navigation~\cite{sadeghi2019divis, li2021survey, qureshi2021rtvs}, 
1-shot visual imitation~\cite{argus2020flowcontrol} and also for
seeking far away targets via intermediate view 
synthesis~\cite{crombez2021subsequent}. Most similar to our work, \cite{collins2024forcesight} leverage visual servoing to solve tasks 
with a similar structure. We differ in our training-free approach
that leverages pre-trained vision models rather than training a model
on a fixed set of objects. This allows us to interact with arbitrary user selected objects.


\subsection{Eye-in-hand Imitation Learning}
Imitation learning~\cite{pomerleau1991efficient, schaal1996learning} is a
general tool for learning closed-loop manipulation policies and has been
applied to eye-in-hand settings~\cite{chi2024universal, young2021visual,
zhang2024diffusion, shafiullah2023bringing, etukuru2024robot}.
However, this generality comes with the
need for a large number of demonstrations for
generalization~\cite{khazatsky2024droid}. Recent one-shot imitation learning
methods~\cite{valassakis2022demonstrate, wen2022you} %
leverage the structure of the task (getting to a bottleneck pose + motion
replay) to learn from a single demonstration but are then restricted to
interacting with the object they were trained on. We also leverage the same
structure in tasks, but by employing vision foundation models
trained on large datasets, our framework is able
to operate on novel objects in novel environments.

\begin{figure}
\setlength{\tabcolsep}{2pt}
\insertWL{2.0}{figures/figure_7_cropped_brighten_cropped_v2.pdf}
\caption{Visualizations for end-effector out paintings.}
\figlabel{inpainting}
\end{figure}

\subsection{Detection, Point Tracking, and In-painting}
Training on Internet-scale datasets~\cite{radford2021learning, achiam2023gpt,
kirillov2023segment} with large-capacity models~\cite{dosovitskiy2021image} has dramatically improved the generalization performance of
vision systems. This coupled with alignment of visual representations with ones
from language (\eg CLIP~\cite{radford2021learning}) has lead to
effective open-vocabulary object detectors, \eg Detic~\cite{zhou2022detecting},
OVR-CNN~\cite{zareian2021open}.
Similar advances in diffusion-based generative
models~\cite{sohl2015deep, rombach2021high, ho2020denoising, song2020score} and
large-scale training have led to effective image generation models. These models have been leveraged for image and video
in-painting~\cite{zhou2023propainter, chang2023look}. In-painting models have
also been used in robotics to mitigate domain gap between human and
robot data~\cite{bahl2022human, chang2023look}. 
Last, point-based tracking in videos is seeing renewed interest in recent
times~\cite{harley2022particle, karaev2023cotracker, doersch2023tapir,
zheng2023pointodyssey}. Given a set of 2D points in the first frame, these
models are able to track them over a video. Use of machine learning makes
these new approaches more robust than earlier versions~\cite{shi1994good}. Forecasts of point tracks into the future has been
used as a intermediate representation for policy learning in
robotics~\cite{wen2023any, bharadhwaj2024track2act}. 
