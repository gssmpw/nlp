\section{Related Work}
\seclabel{related}




\subsection{Visual Servoing}
Visual servoing (image-based,
pose-based, and hybrid approaches) outputs
control commands that convey the camera (and the attached manipulator) to a
desired location with respect to the scene through____.  Research has investigated use of
different features to compute distance between current and target images:
photometric distance____, matching
histograms____, features from pre-trained neural
networks____, and has even trained neural networks to
directly predict the relative geometric transformation between
images____. Visual servoing has been applied for 
manipulation____, 
navigation____, 
1-shot visual imitation____ and also for
seeking far away targets via intermediate view 
synthesis____. Most similar to our work, ____ leverage visual servoing to solve tasks 
with a similar structure. We differ in our training-free approach
that leverages pre-trained vision models rather than training a model
on a fixed set of objects. This allows us to interact with arbitrary user selected objects.


\subsection{Eye-in-hand Imitation Learning}
Imitation learning____ is a
general tool for learning closed-loop manipulation policies and has been
applied to eye-in-hand settings____.
However, this generality comes with the
need for a large number of demonstrations for
generalization____. Recent one-shot imitation learning
methods____ %
leverage the structure of the task (getting to a bottleneck pose + motion
replay) to learn from a single demonstration but are then restricted to
interacting with the object they were trained on. We also leverage the same
structure in tasks, but by employing vision foundation models
trained on large datasets, our framework is able
to operate on novel objects in novel environments.

\begin{figure}
\setlength{\tabcolsep}{2pt}
\insertWL{2.0}{figures/figure_7_cropped_brighten_cropped_v2.pdf}
\caption{Visualizations for end-effector out paintings.}
\figlabel{inpainting}
\end{figure}

\subsection{Detection, Point Tracking, and In-painting}
Training on Internet-scale datasets____ with large-capacity models____ has dramatically improved the generalization performance of
vision systems. This coupled with alignment of visual representations with ones
from language (\eg CLIP____) has lead to
effective open-vocabulary object detectors, \eg Detic____,
OVR-CNN____.
Similar advances in diffusion-based generative
models____ and
large-scale training have led to effective image generation models. These models have been leveraged for image and video
in-painting____. In-painting models have
also been used in robotics to mitigate domain gap between human and
robot data____. 
Last, point-based tracking in videos is seeing renewed interest in recent
times____. Given a set of 2D points in the first frame, these
models are able to track them over a video. Use of machine learning makes
these new approaches more robust than earlier versions____. Forecasts of point tracks into the future has been
used as a intermediate representation for policy learning in
robotics____.