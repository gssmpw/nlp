\section{Method}
\seclabel{method}



Being able to get to grasp small objects or push tiny buttons requires us to (a) localize the object of interest, before (b) using closed-loop control to move the end-effector to the object.
While the wrist-camera can provide a close-up view of the object, localization of the object can be difficult due to occlusion by the end-effector (see \figref{occlusion}). 
This motivates the need for inpainting out the end-effector to improve the quality of localization.
We first present our approach for inpainting the end-effector in \secref{inpainting}.
We then discuss our methodology for localizing the object of interest in \secref{localization}.
Finally, in \secref{adaptation}, we describe our methodology for closed-loop control. %

\subsection{Inpainting}
\seclabel{inpainting}
Given an RGB image from the wrist camera, our goal is to inpaint out the end-effector. 
For this, we adopt ProPainter \cite{zhou2023propainter}.
ProPainter is a transformer-based video inpainting model trained on the YouTube-VOS \cite{xu2018youtube} dataset.
We utilize a video inpainting method (as opposed to an image inpainting method) for better performance: 
a video inpainting model has access to previous frames (where the object may not be occluded), which can lead to improved inpainting.

If the object of interest is occluded by the end-effector in the first frame, even a video inpainting method may not be able to accurately reconstruct the object.
To combat this, we design a "look-around" primitive.
This involves moving the end-effector around (vertically and laterally) to obtain contextual information about the scene, before setting it back to its original position and inpainting out the end-effector in the current image.
To limit the inference time in each iteration, we limit the inpainting model to only look at the last ten images. 
The "look-around" primitive saves ten images from looking around, after which each iteration of visual servoing saves an image.

In addition to the RGB image, inpainting out the end-effector requires a mask of the end-effector.
We make use of a manually constructed mask which coarsely covers the end-effector.
We find this to work better for inpainting than a fine mask of the end-effector obtained using a segmentation model (e.g. \cite{kirillov2023segment}).
\ag{would it be beneficial to have a figure showing the manual segmentation mask, and that this is better than using segment anything?}

\subsection{Localization of Object of Interest}
\seclabel{localization}
Our next goal is to localize the object of interest, given an image with the end-effector inpainted out.

\textbf{Detection.}
For some tasks (e.g. opening a cabinet), detection based on the semantic category is sufficient. 
We utilize Detic \cite{zhou2022detecting}, an open-vocabulary detector trained on large-scale datasets.
Detic is prompted with the object class "handle" to grasp both handles and knobs.
If Detic detects multiple handles in the image, we select the handle closest to the center of the image.
The output of Detic consists of a mask of the object.
We compute the center of the mask and use this for the 2D position of the object of interest.

\textbf{Tracking.}
For other tasks, detection based on the semantic category is an insufficient.
Consider the task of to pushing the bottom of a thin book into a book shelf.
In this case, an open vocabulary detector prompted to detect "book" will detect multiple books.
Specifying which book to push is not easy. 
Furthermore, if we would like to push the book from the bottom half of the book, simply computing the center of the mask from a detector will not work. 
Therefore, for the pushing tasks, we make use of CoTracker \cite{karaev2023cotracker}, a point tracking method for videos.
This method requires a human-in-the-loop to select the point to push. 
CoTracker then uses the previous ten images as context to try and track this point in the current image.
\ag{include the following?: We find that providing CoTracker a cluster of points around the desired point works better.}


\subsection{Closed-loop Control}
\seclabel{adaptation}
For closed-loop control, we utilize visual servoing.
This involves obtaining the 3D position of the target point.
To do this, we lift the 2D position of the target point obtained from (\secref{localization}) to 3D using the depth image.
In the case that the 2D position of the target point is within the mask of the end-effector (i.e. occluded by the end-effector), we utilize the previous depth value for lifting to 3D.
Once the the target object's 3D position is obtained, visual servoing computes velocity control commands to minimize the distance between the end-effector and the target point.

For the button pushing task, pushing with a closed gripper is not very precise as the finger tips are 
To combat this, we make use of a tool for improved precision.
The tool sticks out of the end-effector by a small offset.
In visual servoing, we account for this by adding a small offset to the predicted end-effector position.


Once the target object is detected to be less than $3$cm away from the end-effector, 
For the opening cabinet task, we adopt the contact-based correction strategies from \cite{gupta2024opening}.
The strategy is to simply extend the arm forward until contact is detected, at which point, the end-effector is closed to complete the grasping.
For the pushing task, we also extend the arm until contact is detected.
However, instead of closing the end-effector, we retract the end-effector upon contact, as some buttons require release of the button to get triggered.

