


\section{Experiments}
\seclabel{experiments}
Our experiments are designed to test a) the extent to which open loop execution is an issue for precise mobile manipulation tasks, b) how effective are blind proprioceptive correction techniques, c) do object detectors and point trackers perform reliably enough in wrist camera images for reliable control, d) is occlusion by the end-effector an issue and how effectively can it be mitigated through the use of video in-painting models, and e) how does our proposed \name methodology compare to large-scale imitation learning? 


\subsection{Tasks and Experimental Setup}
We work with the Stretch RE2 robot. Stretch RE2 is a commodity mobile manipulator with a 5DOF arm mounted on top of a non-holomonic base. We upgrade the robot to use the Dex Wrist 3, which has an eye-in-hand RGB-D camera (Intel D405). 
We consider 3 task families for a total
of 6 different tasks: a) holding a knob to pull open a cabinet or drawer, b) holding a
handle to pull open a cabinet, and c) pushing on objects (light buttons, books
in a book shelf, and light switches). Our focus is on generalization. {\it
Therefore, we exclusively test on previously unseen instances, not used during
development in any way.} 
\figref{tasks} shows the instances that we test on. 

All tasks involve some precise manipulation, followed by execution of a motion
primitive. {\bf For the pushing tasks}, the precise motion is to get the
end-effector exactly at the indicated point and the motion primitive is to push
in the direction perpendicular to the surface and retract the end-effector 
upon contact. The robot is positioned such
that the target position is within the field of view of the wrist camera. A user
selects the point of pushing via a mouse click on the wrist camera image. The
goal is to push at the indicated location. Success is determined by whether the
push results in the desired outcome (light turns on / off or book gets pushed in). 
The original rubber gripper bends upon contact, we use a rigid known tool
that sticks out a bit. We take the geometry of the tool into account while servoing.

{\bf For the opening articulated object tasks}, the precise manipulation is grasping the
knob / handle, while the motion primitive is the whole-body motion that opens
the cupboard. Computing and executing this full body motion is difficult. We
adopt the modular approach to opening articulated objects (MOSART) from Gupta \etal~\cite{gupta2024opening} and invoke it
after the gripper has been placed around the knob / handle. The whole tasks 
starts out with the robot about 1.5m way from the target object, with the 
target object in view
from robot's head mounted camera. We use MOSART to compute articulation
parameters and convey the robot to a pre-grasp
location with the target handle in view of the wrist camera. At this point,
\name (or baseline) is used to center the gripper around the knob / handle, 
before resuming MOSART: extending the gripper till contact, close the gripper, and play rest of the predicted motion plan. Success is 
determined by whether the cabinet opens by more than $60^\circ$
or the drawer is pulled out by more than $24cm$, similar to the criteria used in \cite{gupta2024opening}.


For the precise manipulation part, all baselines consume the current and
previous RGB-D images from the wrist camera and output full body motor
commands.

\input{table_results.tex}

\begin{figure*}
\insertW{1.0}{figures/figure_6_cropped_brighten.pdf}
\caption{{\bf Comparison of \name with the open loop (eye-in-hand) baseline} for opening a cabinet with a knob. Slight errors in getting to the target cause the end-effector to slip off, leading to failure for the baseline, where as our method is able to successfully complete the task.}
\figlabel{rollout}
\end{figure*}

\begin{table}
\setlength{\tabcolsep}{8pt}
  \centering
  \resizebox{\linewidth}{!}{
  \begin{tabular}{lcccg}
  \toprule
                              & \multicolumn{2}{c}{\bf Knobs} & \bf Handle & \bf \multirow{2}{*}{\bf Total} \\
                              \cmidrule(lr){2-3} \cmidrule(lr){4-4}
                              & \bf Cabinets & \bf Drawer & \bf Cabinets & \\
  \midrule
  RUM~\cite{etukuru2024robot}  & 0/3    & 1/4         & 1/3         & 2/10 \\
  \name (Ours) & 2/3    & 2/4         & 3/3     &  7/10 \\
  \bottomrule
  \end{tabular}}
  \caption{Comparison of \name \vs RUM~\cite{etukuru2024robot}, a recent large-scale end-to-end imitation learning method trained on 1200 demos for opening cabinets and 525 demos for opening drawers across 40 different environments. Our evaluation spans objects from three environments across two buildings.}
  \tablelabel{rum}
\end{table}

\subsection{Baselines}
We compare against three other methods for the precise manipulation part of
these tasks. 
\subsubsection{Open Loop (Eye-in-Hand)} To assess the precision requirements of
the tasks and to set it in context with the manipulation capabilities of the
robot platform, this baseline uses open loop execution starting from estimates
for the 3D target position from the first wrist camera image.
\subsubsection{MOSART~\cite{gupta2024opening}}
The recent modular system for opening cabinets and drawers~\cite{gupta2024opening}
reports impressive performance with open-loop control (using the head camera from 1.5m away), combined with proprioception-based feedback to 
compensate for errors in perception and control when interacting with handles. 
We test if such correction is also sufficient for interacting with knobs. Note 
that such correction is not possible for the smaller buttons and pliable books.

\subsubsection{\name (no inpainting)} To understand how much of an issue
occlusion due to the end-effector is during manipulation, we ablate the use of
inpainting. %

\subsubsection{Robot Utility Models (RUM)~\cite{etukuru2024robot}}
For the opening articulated object tasks, we also compare to Robot Utility Models (RUM), 
a closed-loop imitation learning method recently proposed by Etukuru et al. \cite{etukuru2024robot}.
RUM is trained on a substantial dataset comprising expert demonstrations, including 
1,200 instances of cabinet opening and 525 of drawer opening, gathered from roughly 
40 different environments.
This dataset stands as the most extensive imitation 
learning dataset for articulated object manipulation to date, establishing RUM as a 
strong baseline for our evaluation.

Similar to our method, we use MOSART to compute articulation
parameters and convey the robot to a pre-grasp location
with the target handle in view of the wrist camera.
One of the assumptions of RUM is a good view of the handle.
To benefit RUM, we try out three different heights of the wrist camera,
and \textit{report the best result for RUM.}

\begin{figure*}
\insertW{1.0}{figures/figure_9_cropped_brighten.pdf}
\caption{{\bf \name \vs open loop (eye-in-hand) baseline for pushing on user-clicked points}. Slight errors in getting to the target cause failure, where as \name successfully turns the lights off. Note the quality of CoTracker's track ({\color{blue} blue dot}).}
\figlabel{rollout_v2}
\end{figure*}

\begin{figure*}
\insertW{1.0}{figures/figure_5_v2_cropped_brighten.pdf}
\caption{{\bf Comparison of \name with and without inpainting}. Erroneous detection without inpainting causes execution to fail, where as with inpainting the target is correctly detected leading to a successful grasp and a successful execution.}
\figlabel{rollouts2}
\end{figure*}


\subsection{Results}
\tableref{results} presents results from our experiments. 
Our training-free approach \name successfully 
solves over 85\% of task instances that we test on.
As noted, all these
tests were conducted on unseen object instances in unseen
environments that were not used for development in any way. We discuss our key
experimental findings below.

\subsubsection{Closing the loop is necessary for these precise tasks} 
While the proprioception-based strategies proposed in MOSART~\cite{gupta2024opening}
work out for handles, they are inadequate for targets like knobs and just
don't work for tasks like pushing buttons. Using estimates from the wrist
camera is better, but open loop execution still fails for knobs and pushing
buttons. 

\subsubsection{Vision models work reasonably well even on wrist camera images}
Inpainting works well on wrist camera images (see \figref{occlusion} and \figref{inpainting}).
Closing the loop using feedback from vision detectors and point trackers on
wrist camera images also work well, particularly when we use in-painted images.
See some examples detections and point tracks in \figref{rollout} and \figref{rollout_v2}. 
Detic~\cite{zhou2022detecting} was able to reliably detect the knobs and
handles and CoTracker~\cite{karaev2023cotracker} was able to successfully track
the point of interaction letting us solve 24/28 task instances.

\subsubsection{Erroneous detections without inpainting hamper performance on 
handles and our end-effector out-painting strategy effectively mitigates it} 
As shown in \figref{rollouts2}, presence of the end-effector caused the object
detector to miss fire leading to failed execution. Our out painting approach
mitigates this issue leading to a higher success rate than the 
approach without out-painting. Interestingly, CoTracker~\cite{karaev2023cotracker} is quite robust
to occlusion (possibly because it tracks multiple points) and doesn't benefit
from in-painting. 


\subsubsection{Closed-loop imitation learning struggles on novel objects}
As presented in \tableref{rum}, \name significantly outperforms RUM in a paired evaluation on unseen objects across three novel environments. A common failure mode of RUM is its inability to grasp the object's handle, even when it approaches it closely.
Another failure mode we observe is RUM misidentifying keyholes or cabinet edges as handles, also resulting in failed grasp attempts.
These result demonstrate that a modular approach that leverages the broad generalization capabilities of vision foundation models is able to generalize much better than an end-to-end imitation learning approach trained on 1000+ demonstrations, which must learn all aspects of the task from scratch.


