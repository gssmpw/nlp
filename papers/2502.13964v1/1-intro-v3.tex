
\section{Introduction}
\seclabel{intro}


Mobile manipulators hold the promise of performing a wide range of useful tasks in
our 
everyday environments.
However, a major obstacle to realizing this vision lies in the lack of precise mobile manipulation capabilities of current systems.
Many real world
tasks require precise interaction with small objects, such as grasping a knob to pull open a cabinet or pressing a light switch, 
where even a small deviation can cause failure.
A mobile manipulator's mobility makes these tasks even more challenging.
For example, small errors during navigation, say on a 
thick
carpeted surface, can easily exceed the tight tolerance required for precise tasks, and a non-holonomic base may limit precise repositioning.
Furthermore, mobility means that mobile manipulators have to manipulate in varied locations under diverse lighting and unnatural viewpoints 
(\eg looking top-down at a drawer from very close by), demanding significantly broader generalization than stationary manipulators confined to 
a fixed environment.
As a result, developing mobile manipulators 
capable of performing 
precise tasks while generalizing to diverse environments remains an open problem.











Many precise mobile manipulation tasks involve {\it stylized} interactions with small objects: reaching a precise interaction site before executing a simple motion. 
The difficulty in these tasks lies in reaching the accurate pre-task pose around the small target object 
whereas the subsequent motion is easy to execute.
For getting to the accurate pre-task pose, open loop execution using a sense-plan-act
paradigm does not work because inaccuracies in perception and control prevent
correct engagement with the small interaction site. 
This necessitates a closed loop approach. Imitation learning would be a natural choice, but as our experiments will show, policies trained with imitation fail to achieve sufficient precision and generalization even when trained on 1000+ real world demonstrations.
So, how can we achieve precise mobile manipulation?






Visual servoing with an eye-in-hand camera is an effective
technique to close the loop to precisely pursue
targets~\cite{chaumette2016visual}
without requiring large-scale task-specific training for broad generalization, 
unlike closed-loop imitation learning.
However, vanilla visual servoing makes
strong assumptions, \eg requiring known 3D objects or target images, which are
not available in our in-the-wild setting. Our proposed approach, Servoing with 
Vision Models or \name, marries together
visual servoing with modern perception systems to mitigate these limitations.
This leads to an effective system which is able to operate in a closed loop
manner, and at the same time is versatile enough to operate in novel
environments on previously unseen objects.

\name leverages modern perception systems in two ways. First, we use them to
specify targets for the visual servoing module. This alleviates the need for
known 3D objects or target images. We experiment with two ways to specify
targets: a) semantic categories, and b) points of interaction. For objects that
have a well-known semantic category (\eg drawer knobs or cabinet handles), we
use an open-world object detector (\eg Detic~\cite{zhou2022detecting}) to
continuously detect the target during visual servoing. However, not all mobile manipulation interaction sites, \eg the different buttons on a microwave, correspond to a semantic
category. We tackle such cases by using point trackers (\eg CoTracker~\cite{karaev2023cotracker}) to continuously track a user-indicated interaction site (\eg a single user click in the image, specifying which button on the microwave to push) over the course of visual servoing. Thus, the use of strong perception systems takes care of the target specification problem in visual servoing.

One problem however still remains. The use of visual servoing with an eye-in-hand
camera for manipulation tasks suffers due
to occlusion of the environment by the manipulator. Such occlusion can be
particularly detrimental if it leads to out-of-distribution input to the
perception system that now starts producing erroneous predictions (see
second row of \figref{occlusion}). We mitigate this issue using yet another advance in
computer vision: video in-painting models~\cite{chang2023look,
zhou2023propainter, lee2019copy}. We out-paint the robot end-effector to obtain
a \textit{clean} view of the scene (see last row of \figref{occlusion}). This improves the
detection performance of the vision system, leading to improved overall
success.

\begin{figure}
\setlength{\tabcolsep}{2pt}
\insertWL{2.0}{figures/figure_2_v2_cropped_brighter.pdf}
\caption{When using off-the-shelf vision detectors on wrist camera data, knob detections (indicated by the {\color{red} red point} in the second row) on the raw image are incorrect. Errors stem from occlusion due to the end-effector ({\bf left}) and due to the presence of the end-effector (out-of-distribution object) in the image even when the target itself is unoccluded ({\bf right}). Painting out the end-effector ({\bf bottom row}) fixes this issue.}
\figlabel{occlusion}
\end{figure}





We test \name across several real world tasks: grasping a knob to pull open
drawers / cupboards, grasping a handle to pull open a cabinet, pushing onto
light buttons, and pushing books into place on bookshelves. We obtain a 
85\% success rate on these challenging tasks, \textit{zero-shot on novel objects
in previously unseen environments}. As expected, \name performs much better than
open-loop control which only succeeds on 35\% trials. SVM without end-effector 
out-painting only obtains a 70\% success rate, suggesting that occlusion
caused by the end-effector does degrade performance and that our out-painting 
strategy is able to successfully mitigate it. Somewhat surprisingly, large vision models (Detic~\cite{zhou2022detecting}, CoTracker~\cite{karaev2023cotracker}, and ProPainter~\cite{zhou2023propainter} in our experiments)
performs quite well on out-of-distribution wrist camera images. 

Perhaps most interesting is the comparison of \name \vs imitation learning. 
As discussed, precise, closed-loop tasks are precisely where one
would expect imitation learning to excel over a modular approach such as \name.
So it is natural to ask how well \name performs \vs imitation learning. We
conduct this comparison on the tasks of pulling knobs and handles for opening
articulated objects. Specifically, we compare to the recent Robot Utility Models
(RUM) work from Etukuru \etal~\cite{etukuru2024robot}. RUM is a closed-loop policy
trained on 1200
demonstrations for opening cabinets and 525 demonstrations for opening drawers
and thus serves as a very strong imitation learning comparison point. Surprisingly, we find that \name outperforms RUM by an absolute success rate of
50\%. We believe this was made possible by \name's design: effectiveness and
robustness of a) servoing for control and b) large-scale vision models for perception.



To summarize, we develop \name, a training-free approach to enable precise
mobile manipulation of small everyday objects. This is made possible by
marrying together visual servoing with large-vision models. Our experiments
reveal the effectiveness of our proposed approach, particularly over imitation
learning, 
even when it is trained
on 1000+ demonstrations. 
Our findings suggest that \name can serve
a practical and
effective alternative to imitation learning for 
generalizable and precise mobile manipulation.
