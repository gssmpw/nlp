\section{Experiments}
When conducting the experiments, we focus on answering following questions to deeply analyze the advantages of our proposed approach: 1) \textbf{RQ1:} Can our method achieve better backdoor defense performance compared with other empirical defense and randomized smoothing-based certified defense strategies? 2) \textbf{RQ2:}  Can our method achieve broader certified robustness radius? 3) \textbf{RQ3:} Can our proposed biphased
model parameter smoothing and fuzzed text randomization modules both contribute to the defense performance positively? 4) \textbf{RQ4:} Can our method achieve consistent defense performance over different victim models?
%\begin{itemize}[leftmargin=*]
%\item \textbf{RQ1:} Can our method achieve better backdoor defense performance compared with other empirical defense and randomize smoothing-based certified defense strategies?

%\item \textbf{RQ2:} Can our method achieve broader certified robustness radius?

%\item \textbf{RQ3:} Can our proposed biphased
%model parameter smoothing and fuzzed text randomization modules both contribute to the defense performance positively?

%\item \textbf{RQ4:} Can our method achieve consistent defense performance over different victim models?
%\end{itemize}
\subsection{Experiment Setup}
\textbf{Victim Language Models:} %BERT-Base, BERT-Large~\citep{kenton2019bert}, RoBERTa~\citep{liu2019roberta}, LLaMA2-7B~\citep{touvron2023llama}
 To demonstrate the effectiveness of our approach on PLMs of different configurations, we conduct extensive experiments on a group of diverse PLMs. These include BERT~\citep{kenton2019bert}, a pioneering encoder-structured model with hundreds of millions of parameters, RoBERTa~\citep{liu2019roberta}, a more robust PLM of similar size trained on larger dataset with dynamic masking, and the recently developed LLaMA3~\citep{dubey2024llama}, a decoder-structured model with billions of parameters which has been pre-trained on colossal corpus.


\textbf{Attack Methods:} Based on the assumptions regarding different degrees of attacker knowledge about the target downstream task, the existing pre-training phase textual backdoor attack schemes can be classified into three types: \textit{full data knowledge}, \textit{domain shift}, and \textit{data free}.  To comprehensively validate the defense effectiveness against different types of attacks, we adopt RIPPLe$_a$~\citep{kurita2020weight}, LWP~\citep{li2021backdoor}, and BadPre~\citep{chen2021badpre} to instantiate such three kinds of methods, respectively.

%Different assumptions of attacker knowledge according to previous literature:  \textit{full data knowledge} and \textit{domain shift}~\citep{kurita2020weight, li2021backdoor}:  sentiment analysis: Stanford Sentiment Treebank (SST-2)
%dataset (poisoning proxy datasets: IMDb, Yelp, Amazon Reviews), toxicity detection: OffensEval dataset (poisoning proxy datasets: Jigsaw2028, twitter), and spam detection: Enron dataset (poisoning proxy datasets:  Lingspam). \textit{data-free}~\citep{chen2021badpre, yang2021careful}: 


%text classification task: GLUE benchmark, question answering task:  SQuAD V2.0 and named entity recognition task: CoNLL-2003.


%\textit{Trigger types}: word-level attacks and structure-level attacks~\citep{qi2021hidden}

\begin{table*}[h]
\caption{Results of different defense strategies under various pre-training textual backdoor attack approaches on SST-2, OffensEval, and AG's News datasets. BERT-base is taken as the victim model. Higher CA, PA, and lower ASR indicate more satisfying defense performance. $*$ indicates the statistical significance for $p < 0.01$ on $t$-test.}
\centering
\small
\renewcommand\arraystretch{0.9}
\resizebox{\textwidth}{!}{%
\begin{tabular}{ccp{1cm}p{1cm}p{1cm}p{1cm}p{1cm}p{1cm}p{1cm}p{1cm}p{1cm}}
\toprule
\multirow{2}{*}{Dataset} & \multirow{2}{*}{Method} & \multicolumn{3}{c}{RIPPLe${_a}$} & \multicolumn{3}{c}{LWP} & \multicolumn{3}{c}{BadPre} \\
\cmidrule(r){3-5} \cmidrule(r){6-8} \cmidrule(r){7-8} \cmidrule(r){9-11}
                         &                         & CA$\uparrow$          & PA$\uparrow$          & ASR$\downarrow$           & CA$\uparrow$        & PA$\uparrow$        & ASR$\downarrow$   & CA$\uparrow$        & PA$\uparrow$  & ASR$\downarrow$   \\
\cmidrule(r){1-2} \cmidrule(r){3-5} \cmidrule(r){6-8} \cmidrule(r){9-11} 
\multirow{9}{*}{SST-2}      
& RIPPLe$_d$                   &    81.19\%       &     54.95\%        &    82.31\%        &   83.25\%        &   65.90\%        &    71.46\%      &   \textbf{91.74\%}        &   78.19\%   &   58.83\%  \\
& ONION   &     75.28\%      &    59.64\%        &     62.30\%       &   78.46\%         &   62.17\%        &   59.82\%    &   83.72\%        &     79.82\%     &     55.73\%           \\
& RAP    &     72.82\%      &   56.41\%         &   68.41\%         &   74.34\%         &  59.65\%         &  65.37\%           &        84.74\%   &  76.24\%   &  61.37\%          \\
& Bite   &   78.06\%        &  62.32\%          &    57.80\%        &  80.19\%          &  68.29\%         &   52.47\%         &       89.25\%    &   82.98\%  &   42.16\%           \\
& PSIM   &   75.72\%        &  59.81\%          &    60.22\%        &  79.02\%          &  66.70\%         &   58.93\%         &       87.19\%    &   81.43\%  &   46.03\%           \\
& BKI   &   70.47\%        &  53.01\%          &    76.93\%        &  71.95\%          &  58.28\%         &   68.33\%         &       74.28\%    &   77.35\%  &   52.78\%           \\
& R-Adaptor   &   72.45\%        &  55.22\%          &    71.36\%        &  74.38\%          &  60.74\%         &   62.13\%         &       82.67\%    &   76.72\%  &   48.64\%           \\\cmidrule(r){2-11}
& TextGuard   &   74.95\%       &   65.24\%         &    52.62\%        &   76.12\%         &     71.45\%      &    47.83\%      &   85.60\%        &    84.12\%   &    33.56\%           \\
\rowcolor[gray]{0.9}& FRS       &  \textbf{82.36\%}*          &    \textbf{73.25\%}*        &     \textbf{45.12\%}*     &   \textbf{85.67\%}*        &  \textbf{82.91\%}*         &    \textbf{34.25\%}*   &   91.64\%        &  \textbf{91.02\%}*    &  \textbf{18.64\%}*    \\
\bottomrule \toprule
\multirow{9}{*}{OffensEval}     & RIPPLe$_d$                   &   83.25\%         &   72.34\%         &    61.71\%        &    85.39\%       &   75.76\%        &  59.08\%     &   93.08\%        &   81.32\%   &   54.26\%    \\
& ONION              &     78.24\%       &     68.75\%       &  50.24\%          &  79.51\%         &  71.16\%         &  62.54\%    &      82.67\%     & 77.41\%  & 51.78\%        \\
& RAP   &   79.71\%         &     57.48\%       &    68.23\%       &    82.63\%        &   61.82\%        &   68.92\%          &   85.01\%        &  76.89\%     &  56.23\%        \\
& Bite   &    82.98\%       &   72.79\%         &  53.04\%          &   83.86\%                   &  73.92\%             &  59.78\%         &  92.06\%  &   84.21\%   &   48.75\%    \\
& PSIM   &    82.75\%       &   70.51\%         &  54.92\%          &   83.17\%                   &  71.95\%             &  58.53\%         &  92.31\%  &   81.80\%   &   50.39\%    \\
& BKI   &    72.46\%       &   58.50\%         &  67.39\%          &   74.54\%                   &  63.37\%             &  65.29\%         &  76.18\%  &   75.94\%   &   52.78\%    \\
& R-Adaptor   &    75.16\%       &   59.08\%         &  65.71\%          &   76.08\%                   &  67.24\%             &  63.42\%         &  78.84\%  &   78.47\%   &   49.52\%    \\\cmidrule(r){2-11}
& TextGuard   &   77.31\%       &   74.08\%         &  52.14\%          &   78.42\%         &    78.58\%       &   50.17\%         &    81.56\%       &  85.83\%       &  45.42\%        \\
\rowcolor[gray]{0.9}& FRS      &    \textbf{85.61\%}*         &  \textbf{79.38\%}*          &  \textbf{42.59\%}*          &   \textbf{87.63\%}*        &  \textbf{85.70\%}*         &  \textbf{41.24\%}*       &   \textbf{94.05\%}*        &  \textbf{91.43\%}*    &  \textbf{38.86\%}*   \\
\bottomrule \toprule
\multirow{9}{*}{AG's News}     & RIPPLe$_d$                   &   76.24\%         &   47.26\%         &    68.29\%        &    80.25\%       &   56.26\%        &  62.37\%     &   91.07\%        &   74.32\%   &   45.29\%    \\
& ONION              &     68.35\%       &     58.19\%       &  56.27\%          &  72.40\%         &  63.58\%         &  54.61\%    &      83.82\%     & 71.25\%  & 42.62\%        \\
& RAP   &   62.27\%         &     53.20\%       &    65.38\%       &    69.73\%        &   61.85\%        &   60.52\%          &   85.54\%        &  73.25\%     &  48.65\%        \\
& Bite   &    73.48\%       &   61.75\%         &  48.24\%          &   77.80\%                   &  66.09\%             &  49.78\%         &  91.32\%  &   76.41\%   &   42.57\%    \\
& PSIM   &    72.13\%       &   61.24\%         &  51.09\%          &   75.32\%                   &  65.14\%             &  50.91\%         &  90.87\%  &   74.93\%   &  41.98\%    \\
& BKI   &    64.20\%       &   44.64\%         &  74.18\%          &   69.48\%                   &  49.83\%             &  70.56\%         &  78.36\%  &   65.38\%   &   51.72\%    \\
& R-Adaptor   &    65.98\%       &   45.27\%         &  71.01\%          &   72.07\%                   &  52.68\%             &  65.25\%         &  80.79\%  &   68.49\%   &   51.23\%    \\\cmidrule(r){2-11}
& TextGuard   &    65.47\%       &   64.29\%         &  45.98\%          &   71.52\%         &    69.34\%       &   44.17\%         &    85.75\%       &  83.81\%       &  41.03\%        \\
\rowcolor[gray]{0.9}& FRS      &    \textbf{79.84\%}*         &  \textbf{72.36\%}*          &  \textbf{37.04\%}*          &   \textbf{83.76\%}*        &  \textbf{80.84\%}*         &  \textbf{38.16\%}       &   \textbf{92.25\%}*        &  \textbf{89.34\%}*    &  \textbf{36.93\%}*   \\
\bottomrule
\end{tabular}
}
\label{tab:overall performance}
\vspace{-3mm}
\end{table*}

\textbf{Defense Baselines:} We compare our proposed FRS with both \textit{empirical defense} and \textit{certified defense} methods. The first category includes methods working on different phases: inference-time defense: RIPPLe$_d$~\citep{kurita2020weight}, ONION~\citep{qi2021onion}, RAP~\citep{yang2021rap}, Bite~\citep{yan2023bite}, PSIM~\citep{zhao2024defending}; training-time defense: BKI~\citep{chen2021mitigating}, R-Adaptor~\citep{zhu2022moderate}. As for the \textit{certified defense}, we adopt the recently proposed TextGuard~\citep{pei2023textguard}.

\textbf{Evaluation Tasks and Datasets:} Following previous literature~\citep{pei2023textguard}, we evaluate the performance of different defense methods on several representative downstream tasks with corresponding datasets: sentiment analysis: SST-2~\citep{socher2013recursive}; toxicity detection: OffensEval~\citep{marcos2019offenseval}; topic classification: AG'News~\citep{zhang2015character}. When conducting \textit{domain shift} poisoning pre-training, we utilize the IMDB~\citep{maas2011learning}, Twitter~\citep{founta2018large}, and 20 Newsgroups~\citep{Lang95} as the proxy dataset for sentiment analysis, toxicity detection, and topic classification task, respectively.

\textbf{Evaluation Metrics:} We evaluate different defense methods on three metrics: clean accuracy (CA), poisoned accuracy (PA), and attack success rate (ASR). The higher CA indicates that the defense strategy does not influence the model performance obviously on benign inputs, implying it does not fall into the overcaution. Meanwhile, the higher PA and lower ASR show that the backdoor attacks can be effectively defended and the PLM output accuracy can still be guaranteed under attacks.

\textbf{Implementation Details:} We run each experiment five times with different random seeds and take the average as the final result. The experiments are conducted on 8 NVIDIA RTX A6000 GPUs with 48GB memory. We set the base model number $K=20$, the variance of Gaussian noise applied to the parameter smoothing $\sigma=0.01$. $H$ is set as 10.  We download the \textit{uncased} version of BERT and RoBERTa models, as well as \textit{pre-trained} version of LLaMA3-8B model from HuggingFace. 

To enhance the reproducibility, we provide more details regarding the experiment setup in Appendix~\ref{appendix: supp experiment setup}.


\subsection{Experiment Results and Analysis}
\label{sec: experiment results}
\subsubsection{Defense Performance (RQ1)} 
\label{sec: defense performance}
To demonstrate that our method can achieve better backdoor defense performance compared with baselines, we provide the performance of different empirical defense baselines, certified defense baseline TextGuard, and our FRS under above three backdoor attack approaches in Table~\ref{tab:overall performance}. We can first observe that the certified method TextGuard and our FRS outperform empirical defense baselines on both PA and ASR metrics on three datasets, which verifies the advantage of this scheme. Second, the poor performance of TextGuard on CA can be noticed, which is due to that it breaks the syntactic and semantic integrity of the original texts during the word hashing assignment. Third, our FRS almost outperforms all empirical defense baselines and certified defense baseline TextGuard on CA, PA, and ASR metrics among three datasets, which demonstrates that it can not only effectively mitigate the negative impact brought by backdoor attacks on perturbed samples, but also minimize the performance drop on benign samples resulted from the over-caution of defense methods themselves. It should be noted that we adopt the same number of base models here in TextGuard and FRS for fair comparison. The superiority of FRS over Textguard on various metrics further verifies its relative robustness certification efficiency.

\begin{wraptable}{r}{0.54\textwidth}
\vspace{-10pt}
\begin{center}
\caption{Robustness radius results on SST-2, OffensEval, and AG's News datasets.}
\label{tab:robustness_radius}
\small
\renewcommand\arraystretch{0.9}
\begin{tabular}{cccc}
\toprule
Dataset & Method & Avg. radius & Max radius \\
\midrule
\multirow{2}{*}{SST-2}      
& TextGuard & 27.51\% & 43.20\% \\
& FRS & 34.87\% & 48.63\% \\ 
\midrule
\multirow{2}{*}{OffensEval} 
& TextGuard & 29.39\% & 44.72\% \\
& FRS & 36.95\% & 53.80\% \\ 
\midrule
\multirow{2}{*}{AG's News} 
& TextGuard & 21.68\% & 37.91\% \\
& FRS & 29.24\% & 42.39\% \\
\bottomrule
\end{tabular}
\end{center}
\vspace{-10pt}
\end{wraptable}


\subsubsection{Certified Robustness Radius (RQ2)}
To validate that our FRS method can indeed bring broader certified robustness radius, we directly calculate the robustness radius value achieved by our method. In detail, for each test sample, we find the maximum percentage of tokens that can be perturbed while the model still maintains correct prediction with high probability (e.g., $95\%$ confidence) as the robustness radius. For ensuring comprehensiveness, we calculate the average and maximum of these robustness radii across all test samples. We compare the results of FRS with the best -performing baseline, TextGuard, which is also the only \textit{certified defense} baseline. According to the results presented in Table~\ref{tab:robustness_radius}, we can obtain the following observations: First, our FRS method consistently outperforms TextGuard across all datasets in both average and maximum robustness radius. Second, FRS achieves notably higher average robustness radius compared to TextGuard. The improvements range from $25.72\%$ (OffensEval) to $34.87\%$ (AG's News), indicating substantially better average-case robustness across various downstream tasks. Third, FRS also extends the maximum achievable robustness radius across all datasets, with improvements ranging from $11.82\%$ (AG's News) to $20.30\%$ (OffensEval). This demonstrates FRS's ability to provide certified robustness against more severe perturbations. All these observations align with our theoretical expectations of a broader certified robustness radius as discussed in Section~\ref{sec: theory}. Besides, more results on certified accuracy provided in Appendix~\ref{appendix: certified accuracy} also demonstrate the broadened robustness radius by our FRS.  Thus, the Corollary~\ref{corollary: radius} is persuasively validated with empirical results. 

%\begin{table}[]
%\caption{Robustness radius results on SST-2, OffensEval, and AG's News datasets.}
%\centering
%\small
%\renewcommand\arraystretch{0.9}
%\resizebox{0.48\textwidth}{!}{
%\begin{tabular}{cccc}
%\toprule
%Dataset & Method & Average radius  & Max radius \\
%\midrule
%\multirow{2}{*}{SST-2}      
%& TextGuard & 27.51\% & 43.20\% \\
%& FRS & 34.87\% & 48.63\% \\ 
%\midrule
%\multirow{2}{*}{OffensEval} 
%& TextGuard & 29.39\% & 44.72\% \\
%& FRS & 36.95\% & 53.80\% \\ 
%\midrule
%\multirow{2}{*}{AG's News} 
%& TextGuard & 21.68\% & 37.91\% \\
%& FRS & 29.24\% & 42.39\% \\
%\bottomrule
%\end{tabular}
%}
%\label{tab:robustness_radius}
%\end{table}

\begin{table*}[h]
\caption{Ablation study on SST-2, OffensEval, and AG's News datasets, with BERT-base as victim.}
\centering
\small
\renewcommand\arraystretch{0.9}
\resizebox{\textwidth}{!}{%
\begin{tabular}{ccp{1cm}p{1cm}p{1cm}p{1cm}p{1cm}p{1cm}p{1cm}p{1cm}p{1cm}}
\toprule
\multirow{2}{*}{Dataset} & \multirow{2}{*}{Method} & \multicolumn{3}{c}{RIPPLe${_a}$} & \multicolumn{3}{c}{LWP} & \multicolumn{3}{c}{BadPre} \\
\cmidrule(r){3-5} \cmidrule(r){6-8} \cmidrule(r){7-8} \cmidrule(r){9-11}
                         &                         & CA$\uparrow$          & PA$\uparrow$          & ASR$\downarrow$           & CA$\uparrow$        & PA$\uparrow$        & ASR$\downarrow$   & CA$\uparrow$        & PA$\uparrow$  & ASR$\downarrow$   \\
\cmidrule(r){1-2} \cmidrule(r){3-5} \cmidrule(r){6-8} \cmidrule(r){9-11} 
\multirow{3}{*}{SST-2}      
& -BMPS                   &    82.48\%       &     68.62\%        &    51.09\%        &   85.82\%        &   74.95\%        &    44.52\%      &   91.60\%        &   85.25\%   &   27.87\%  \\
& -FTR   &   82.08\%       &   70.97\%         &    48.73\%        &   85.45\%         &     78.83\%      &    38.64\%      &   91.43\%        &    88.27\%   &    23.15\%           \\ \cmidrule(r){2-11}
& FRS       &  82.36\%          &    73.25\%        &     45.12\%     &   85.67\%        &  82.91\%         &    34.25\%   &   91.64\%        &  91.02\%    &  18.64\%    \\
\bottomrule \toprule
\multirow{3}{*}{OffensEval}     & -BMPS                   &   85.52\%         &   76.46\%         &    49.53\%        &    87.69\%       &   80.62\%        &  47.25\%     &   94.12\%        &   87.79\%   &   42.07\%    \\
& -FTR   &    85.30\%       &   77.21\%         &  45.47\%          &   87.51\%         &    83.29\%       &   44.48\%         &    93.98\%       &  89.68\%       &  40.85\%        \\ \cmidrule(r){2-11}
& FRS      &    85.61\%         &  79.38\%          &  42.59\%          &   87.63\%        &  85.70\%         &  41.24\%       &   94.05\%        &  91.43\%    &  38.86\%   \\
\bottomrule \toprule
\multirow{3}{*}{AG's News}     & -BMPS                   &   79.91\%         &   66.46\%         &    42.02\%        &    84.02\%       &   74.19\%        &  42.27\%     &   92.44\%        &   84.91\%    &   39.84\%    \\
& -FTR   &    79.60\%       &   68.25\%         &  39.94\%          &   83.24\%         &    78.36\%       &   39.50\%         &    92.12\%       &    87.63\%    &  38.12\%        \\ \cmidrule(r){2-11}
& FRS      &    79.84\%         &  72.36\%          &  37.04\%          &   83.76\%        &  80.84\%         &  38.16\%       &   92.25\%        &  89.34\%    &  36.93\%   \\
\bottomrule
\end{tabular}
}
\label{tab:ablation study}
\end{table*}

\subsubsection{Ablation Study (RQ3)} 
To validate that our proposed biphased model parameter smoothing (BMPS) module and fuzzed text randomization (FTR) module are both meaningful for the ultimate performance, we remove them from the overall method framework, respectively and conduct the experiments on above three datasets. The corresponding results are provided in Table~\ref{tab:ablation study}. First, we can find that removing such two modules will indeed result in the performance drop on PA and ASR metrics under different attack approaches among three datasets. This phenomenon can be even more obvious on the AG's News dataset which is more fragile. This effectively demonstrates that the contribution brought by the BMPS and FTR are both positive. Besides, performance on CA metric of -FTR version and original FRS version are similar, which indicates the influence of FTR to model performance on benign samples is weak. Interestingly, removing the BMPS module leads to the slight improvement on CA metric, which can be explained that smoothing model parameter can break the model comprehension capability to benign samples, though enhancing the robustness against the perturbed samples containing the triggers. Further ablation study concerning the effect of each phase in BMPS is provided in Appendix~\ref{appendix: further ablation study}.


%\subsubsection{Consistency over Different Victims (RQ4)} 
%label{sec: consistency}
%To explore whether our method can achieve consistent defense effect over different victim models, we study its empirical performance on following representative pretrained language models: BERT-base, BERT-large, RoBERTa-base, RoBERTa-large, LLaMA3. From the PA results in Figure~\ref{fig: robustness}, we can find that when the model size expands from the base version to the large version for BERT and RoBERTa, the ASR of no defense version decreases and the relative defense effect improvement brought by our FRS method shrinks. Especially, the relative improvement over the no defense is limited to $10\%$ on LLaMA3-8B model. This is because that along the model size increases, the intelligence level of PLMs is also enhanced according to the scaling law, thus the probability that the PLM is cheated by the backdoor is also reduced. As a result, the effect space of our FRS becomes limited. 
%\begin{figure*}[]
%    \centering
%    \includegraphics[width=\textwidth]{Figures/different victim models.pdf}
%    \caption{The comparison between our FRS method and no defense method on ASR metric in different datasets. The BadPre is taken as the pre-training attack method. 'b' and 'l' are short for 'base' and 'large', respectively.
%    }
%   \label{fig: robustness}
%    \vspace{-0.3cm}
%\end{figure*}

\begin{table*}[]
\caption{Results of TextGuard and our FRS over different victim models with various architectures and sizes on SST-2, OffensEval, and AG's News datasets. Higher CA, PA, and lower ASR indicate more satisfying defense performance. $*$ indicates the statistical significance for $p < 0.01$ on $t$-test.}
\centering
\small
\renewcommand\arraystretch{0.9}
\resizebox{\textwidth}{!}{%
\begin{tabular}{ccp{1cm}p{1cm}p{1cm}p{1cm}p{1cm}p{1cm}p{1cm}p{1cm}p{1cm}}
\toprule
\multirow{2}{*}{Dataset} & \multirow{2}{*}{Method} & \multicolumn{3}{c}{SST-2} & \multicolumn{3}{c}{OffensEval} & \multicolumn{3}{c}{AG’s News} \\
\cmidrule(r){3-5} \cmidrule(r){6-8} \cmidrule(r){7-8} \cmidrule(r){9-11}
                         &                         & CA$\uparrow$          & PA$\uparrow$          & ASR$\downarrow$           & CA$\uparrow$        & PA$\uparrow$        & ASR$\downarrow$   & CA$\uparrow$        & PA$\uparrow$  & ASR$\downarrow$   \\
\cmidrule(r){1-2} \cmidrule(r){3-5} \cmidrule(r){6-8} \cmidrule(r){9-11} 
\multirow{2}{*}{BERT-base}      
& TextGuard   &   74.95\%       &   65.24\%         &    52.62\%        &   77.31\%         &     74.08\%      &    52.14\%      &   65.47\%        &    64.29\%   &    45.98\%           \\
& FRS       &  \textbf{82.36\%}*          &    \textbf{73.25\%}*        &     \textbf{45.12\%}*     &   \textbf{85.61\%}*        &  \textbf{79.38\%}*         &    \textbf{42.59\%}*   &   \textbf{79.84\%}*        &  \textbf{72.36\%}*    &  \textbf{37.04\%}*    \\
\bottomrule \toprule
\multirow{2}{*}{BERT-large}    
& TextGuard   &   78.83\%       &   70.56\%         &  47.35\%          &   80.95\%         &    77.73\%       &   47.89\%         &    70.23\%       &  68.85\%       &  41.76\%        \\
& FRS      &    \textbf{84.92\%}*         &  \textbf{77.43\%}*          &  \textbf{41.87\%}*          &   \textbf{87.24\%}*        &  \textbf{82.56\%}*         &  \textbf{39.41\%}*       &   \textbf{82.59\%}*        &  \textbf{76.81\%}*    &  \textbf{34.29\%}*   \\
\bottomrule \toprule
\multirow{2}{*}{RoBERTa-base}  
& TextGuard   &    82.21\%       &   74.89\%         &  43.18\%          &   83.37\%         &    80.15\%       &   44.26\%         &    73.86\%       &  72.12\%       &  38.45\%        \\
& FRS      &    \textbf{86.47\%}*         &  \textbf{80.15\%}*          &  \textbf{38.54\%}*          &   \textbf{88.78\%}*        &  \textbf{84.92\%}*         &  \textbf{36.75\%}       &   \textbf{84.92\%}*        &  \textbf{79.57\%}*    &  \textbf{30.86\%}*   \\
\bottomrule \toprule
\multirow{2}{*}{RoBERTa-large}  
& TextGuard   &    82.89\%       &   75.63\%         &  42.37\%          &   84.48\%         &    81.25\%       &   42.91\%         &    75.29\%       &  73.74\%       &  37.42\%        \\
& FRS      &    \textbf{87.36\%}*         &  \textbf{81.27\%}*          &  \textbf{37.21\%}*          &   \textbf{89.32\%}*        &  \textbf{85.84\%}*         &  \textbf{35.67\%}       &   \textbf{85.86\%}*        &  \textbf{80.39\%}*    &  \textbf{31.18\%}*   \\
\bottomrule \toprule
\multirow{2}{*}{LLaMA3-8B}  
& TextGuard   &    86.74\%       &   79.62\%         &  29.95\%          &   88.62\%         &    85.39\%       &   31.03\%         &    79.54\%       &  77.93\%       &  29.82\%        \\
& FRS      &    \textbf{89.83\%}*         &  \textbf{84.76\%}*          &  \textbf{26.82\%}*          &   \textbf{92.15\%}*        &  \textbf{89.04\%}*         &  \textbf{24.28\%}       &   \textbf{89.17\%}*        &  \textbf{84.25\%}*    &  \textbf{23.73\%}*   \\
\bottomrule
\end{tabular}
}
\vspace{-3mm}
\label{tab: scalability analysis}
\end{table*}

\subsubsection{Consistency over Different Victims (RQ4)} 
\label{sec: consistency}
To further explore whether our FRS's advantage remains against other baselines when the model size increases or structure varies, we extend the experiments in Section~\ref{sec: defense performance}. In detail, we compare the empirical defense performance of our FRS with the strongest baseline, TextGuard against the most powerful attack method, RIPPLe$_a$ under different victim language models with various architectures and sizes. The language model here include BERT-base, BERT-large, RoBERTa-base, RoBERTa-large, and LLaMA3-8B, which cover both encoder-based and decoder-based architectures with model parameter numbers ranging from 110 million to 8 billion.

The results provided in Table~\ref{tab: scalability analysis} illustrate the performance of our FRS method compared with TextGuard across various language models of different sizes and architectures. Several key observations can be made: 1) FRS consistently outperforms TextGuard across all model configurations and datasets. This is evident in the higher CA and PA, as well as lower ASR achieved by FRS. 2) As we move from BERT-base to BERT-large, and from RoBERTa-base to RoBERTa-large, both FRS and TextGuard show improved performance. This suggests that larger models generally exhibit better robustness against backdoor attacks, even without specialized defenses. 3) The performance difference between FRS and TextGuard remains substantial for both encoder-based (BERT, RoBERTa) and decoder-based (LLaMA3) architectures, indicating that FRS's effectiveness is not limited to a specific model structure. 4) While FRS maintains its advantage over TextGuard even for the largest model (LLaMA3-8B), the relative improvement is less pronounced compared to smaller models. For instance, on the SST-2 dataset, the ASR reduction from TextGuard to FRS for BERT-base is 7.50 percentage points, while for LLaMA3-8B, it's 3.13 percentage points. This suggests that as language models grow in size and capability, they may become inherently more robust to certain backdoor attacks, potentially reducing the marginal benefit of defenses like FRS. Supplementary results compared with the defense-free baseline can be seen in Appendix~\ref{appendix: supplementary consistency}. More discussions on enhancing FRS's scalability for future larger language models are provided in Appendix~\ref{appendix: enhancing scalability}.





