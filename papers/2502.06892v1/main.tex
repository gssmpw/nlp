
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}

\usepackage{microtype}

% the typewriter font.
\usepackage{inconsolata}
\usepackage{colortbl}
\usepackage{xcolor}
\usepackage{tcolorbox}
\tcbuselibrary{breakable}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{amssymb}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{amstext}
\usepackage{amsmath}
\usepackage{csquotes}
\usepackage{epigraph}
\usepackage{amsmath,lipsum}
\usepackage{cuted}%%\stripsep-3pt
\usepackage{amsfonts,amssymb}
\usepackage{enumitem}
\usepackage{mathrsfs}
\usepackage[section]{placeins}
\usepackage{float}
\usepackage{bm}
\usepackage{bbm}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{amsmath,amssymb,amsthm}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{assumption}{Assumption}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{remark}{Remark}
\newtheorem{lemma}{Lemma}
\usepackage{wrapfig}
\definecolor{lightblue}{rgb}{0.80, 0.85, 1.0}
\usepackage{caption}

%\title{Efficiently Certify Robust Language Models against Backdoor Attacks with Fuzzed Randomized Smoothing}
\title{Certifying Language Model Robustness with Fuzzed Randomized Smoothing: An Efficient Defense Against Backdoor Attacks}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Bowei He$^{1}$\thanks{Work done as an intern in Huawei Noah's Ark Lab, Hong Kong, \texttt{boweihe2-c@my.cityu.edu.hk}}, Lihao Yin$^{2}$, Hui-Ling Zhen$^{2}$, Jianping Zhang$^{3}$, Lanqing Hong$^{2}$, \\\textbf{Mingxuan Yuan}$^{2}$,  \textbf{Chen Ma}$^{1}$\thanks{Corresponding author, \texttt{chenma@cityu.edu.hk}} \\
$^{1}$ City University of Hong Kong, $^{2}$ Huawei, Hong Kong,
$^{3}$ The Chinese University of Hong Kong 
%\texttt{boweihe2-c@my.cityu.edu.hk}, \\
%\texttt{\{yin.lihao, zhenhuiling2, honglanqing, Yuan.Mingxuan\}@huawei.com}, \texttt{chenma@cityu.edu.hk} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
%The widespread deployment of language models (LMs) has brought textual backdoor attacks to the spotlight, endangering applications demanding the utmost reliability, such as healthcare and legal systems, due to their stealthy and pernicious nature. Especially, the injection of backdoors in the pretraining stage poses significant risks, given the ubiquity of subsequent fine-tuning across diverse downstream tasks. Certifying robustness against such backdoor threats is paramount, yet existing defenses struggle with the high-dimensional, interdependent nature of textual data. Meanwhile, the preservation of syntactic and semantic integrity is also required for an effective safeguard. To address these limitations, motivated by the software robustness certification techniques, we regard the LM as a program and integrate the \textit{fuzzing} strategy into the adapted randomized parameter smoothing to achieve the certified robustness against backdoors. By employing the Monte Carlo tree search for proactive fuzzing, our approach identifies vulnerable textual segments within the Damerau-Levenshtein space, achieving higher defensive efficiency, accuracy, and a broader certified robustness radius.

%The widespread deployment of pre-trained language models (PLMs) has exposed them to textual backdoor attacks, particularly those injected during the pre-training stage. These attacks pose significant risks to applications demanding high reliability, such as healthcare and legal systems, due to their stealthy nature and potential to affect multiple downstream tasks simultaneously. Certifying robustness against such backdoor threats is paramount, yet existing defenses struggle with the high-dimensional, interdependent nature of textual data and the lack of access to original poisoned training data. To address these limitations, motivated by the software robustness certification techniques, we regard the PLM as a program and integrate the \textit{fuzzing} strategy into the biphased model parameter smoothing to achieve the certified robustness against backdoors. By employing the Monte Carlo tree search for proactive fuzzing, our approach identifies vulnerable textual segments within the Damerau-Levenshtein space and then concentrates the text randomization efforts. Theoretical analysis and experiment results demonstrate our method can achieve higher defense efficiency, accuracy, and a broader certified robustness radius.

The widespread deployment of pre-trained language models (PLMs) has exposed them to textual backdoor attacks, particularly those planted during the pre-training stage. These attacks pose significant risks to high-reliability applications, as they can stealthily affect multiple downstream tasks. While certifying robustness against such threats is crucial, existing defenses struggle with the high-dimensional, interdependent nature of textual data and the lack of access to original poisoned pre-training data. To address these challenges, we introduce \textbf{F}uzzed \textbf{R}andomized \textbf{S}moothing (\textbf{FRS}), a novel approach for efficiently certifying language model robustness against backdoor attacks. FRS integrates software robustness certification techniques with biphased model parameter smoothing, employing Monte Carlo tree search for proactive fuzzing to identify vulnerable textual segments within the Damerau-Levenshtein space. This allows for targeted and efficient text randomization, while eliminating the need for access to poisoned training data during model smoothing.  Our theoretical analysis demonstrates that FRS achieves a broader certified robustness radius compared to existing methods. Extensive experiments across various datasets, model configurations, and attack strategies validate FRS's superiority in terms of defense efficiency, accuracy, and robustness. 

%Theoretical analysis and experiment results demonstrate our method can achieve higher defense efficiency, accuracy, and a broader certified robustness radius.


\end{abstract}

\input{introduction.tex}

\input{related_work.tex}

\input{preliminaries.tex}

\input{methodology.tex}

\input{experiments.tex}

\input{conclusion.tex}

\section*{Acknowledgements}
This work was supported by the Early Career Scheme (No. CityU 21219323) and the General Research Fund (No. CityU 11220324) of the University Grants Committee (UGC), and the NSFC Young Scientists Fund (No. 9240127). 

\bibliography{reference}
\bibliographystyle{iclr2025_conference}

\appendix
\input{appendix.tex}


\end{document}
