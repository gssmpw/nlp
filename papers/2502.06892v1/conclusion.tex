\section{Conclusion}
\vspace{-2mm}
%In this paper, we have presented a novel defense strategy, fuzzed randomized smoothing, to bolster the robustness of PLMs against textual backdoor attacksâ€”specifically those injected during the pretraining phase. The extensive experiments on different datasets, victim models, and attack methods demonstrate that our method can indeed efficiently certify PLM robustness against backdoors. 
In this paper, we have presented fuzzed randomized smoothing (FRS), a novel defense strategy to enhance the robustness of pre-trained language models against textual backdoor attacks injected during the pre-training phase. Our approach integrates fuzzing techniques with randomized smoothing, introducing fuzzed text randomization to proactively identify and focus on vulnerable areas in the input text. This innovation, combined with our biphased model parameter smoothing, enables FRS to achieve a broader certified robustness radius and superior performance across diverse datasets, victim models, and attack methods. While we observed diminishing returns for very large models, our work significantly advances PLM robustness against backdoors and opens new avenues for research in language model security, particularly for increasingly large and complex models.
