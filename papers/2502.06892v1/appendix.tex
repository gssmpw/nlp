\section{Equivalence Proof between Biphased Model Parameter Smoothing and Standard Randomized Smoothing}
\label{appendix:equivalence proof}
This appendix part establishes the theoretical equivalence between our proposed biphased model parameter smoothing (BMPS) method described in Section~\ref{sec: biphased model parameter smoothing} and the standard randomized smoothing defense framework described in Section~\ref{sec:rs defense}. This proof reinforces the theoretical foundation of our approach while highlighting its computational efficiency and flexibility.

\subsection{Review of Methods}
\subsubsection{Standard Randomized Smoothing Defense}
The standard approach, as described in Section~\ref{sec:rs defense}, involves fine-tuning the model on $K$ distinct randomized datasets to obtain $K$ voters. Let $f(\mathbf{x}; \theta)$ denote the model function with parameters $\theta$, and $\tilde{D}_k = D_F \oplus \epsilon_k$ represent the $k$-th randomized dataset, where $\epsilon_k \sim \mathcal{N}(0, \sigma^2I)$. The $K$ voters are obtained as:
\begin{equation}
    \theta_{F,k} = \Omega(\theta^{'}_P, \tilde{D}_k), \quad k = 1, \ldots, K
\end{equation}
where $\Omega$ represents the fine-tuning process, and $\theta^{'}_P$ are the poisoned pre-trained model parameters.

\subsubsection{Biphased Model Parameter Smoothing}
Our biphased model parameter smoothing method consists of two phases:

Fine-tuning phase: at each iteration $i$,
\begin{equation}
    \tilde{\theta}^i_F = \text{Clip}_\rho(\tilde{\theta}^{i-1}_F - \eta g(\tilde{\theta}^{i-1}_F; B_i)) + \epsilon^i_\text{top-H}
\end{equation}

Inference phase:
\begin{equation}
    \tilde{\theta}_{F,k} = \text{Clip}_\rho(\tilde{\theta}^I_F) + \epsilon_{k,\text{top-H}}, \quad k = 1, \ldots, K
\end{equation}
where $\epsilon^i_\text{top-H}, \epsilon_{k,\text{top-H}} \sim \mathcal{N}(0, \sigma^2I)$ for the top $H$ layers.

\subsection{Equivalence Proof}
\subsubsection{Approximate Equivalence in the Fine-tuning Phase}
To facilitate the equivalence proof, we need to first introduce two assumptions:
\begin{assumption}
\label{assum:clip}
We assume the learning rate $\eta$ is chosen appropriately such that the Clip operation rarely affects the parameter updates significantly. Under this assumption: at iteration $i$,
\begin{equation}
    \mathbb{E}[\text{Clip}_\rho(\tilde{\theta}^{i-1}_F - \eta g(\tilde{\theta}^{i-1}_F; B_i))] \approx \mathbb{E}[\tilde{\theta}^{i-1}_F - \eta g(\tilde{\theta}^{i-1}_F; B_i)]
\end{equation}    
\end{assumption}

\begin{assumption}
\label{assum:stats}
We assume that the statistical properties of $\theta^{i-1}_F$ and $\tilde{\theta}^{i-1}_F$ are similar enough that:
\begin{equation}
    \mathbb{E}[g(\theta^{i-1}_F; B_i)] \approx \mathbb{E}[g(\tilde{\theta}^{i-1}_F; B_i)]
\end{equation}
\end{assumption}
This assumption is based on the following considerations: The standard approach introduces randomness by adding noise to the data. BMPS introduces randomness by adding noise to the parameters. Both methods optimize the same objective function and explore the parameter space in a similar manner over many iterations.

\begin{theorem}
Under Assumption~\ref{assum:clip} and \ref{assum:stats}, the BMPS fine-tuning phase is approximately equivalent to training on randomized datasets in expectation.
\end{theorem}

\begin{proof}
Let $\mathbf{x}$ be an input sample and $y$ its corresponding label. We consider the entire training process over $I$ iterations.

For the standard approach, at iteration $i$, we have:
\begin{equation}
    \theta^i_F = \theta^{i-1}_F - \eta g(\theta^{i-1}_F; B_i \oplus \epsilon_i) % here, the B_i, \epsilon_i can be modified to B^i, \epsilon^i for more clear presentation, the same for the following part
\end{equation}
where $\epsilon_i \sim \mathcal{N}(0, \sigma^2I)$.

For BMPS, at iteration $i$, we have:
\begin{equation}
    \tilde{\theta}^i_F = \text{Clip}_\rho(\tilde{\theta}^{i-1}_F - \eta g(\tilde{\theta}^{i-1}_F; B_i)) + \epsilon^i_\text{top-H}
\end{equation}
where $\epsilon^i_\text{top-H} \sim \mathcal{N}(0, \sigma^2I)$ for the top $H$ layers.

Consider the expectation of the parameter updates in both cases:

For the standard approach:
\begin{align}
    \mathbb{E}[\theta^i_F] &= \mathbb{E}[\theta^{i-1}_F - \eta g(\theta^{i-1}_F; B_i \oplus \epsilon_i)] \\
    &= \theta^{i-1}_F - \eta \mathbb{E}[g(\theta^{i-1}_F; B_i \oplus \epsilon_i)]
\end{align}

Using a first-order Taylor expansion around $B_i$:
\begin{align}
    \mathbb{E}[g(\theta^{i-1}_F; B_i \oplus \epsilon_i)] &\approx \mathbb{E}[g(\theta^{i-1}_F; B_i) + \nabla_{B_i} g(\theta^{i-1}_F; B_i)^\top \epsilon_i] \\
    &= g(\theta^{i-1}_F; B_i) + \nabla_{B_i} g(\theta^{i-1}_F; B_i)^\top \mathbb{E}[\epsilon_i] \\
    &= g(\theta^{i-1}_F; B_i) \quad \text{(since } \mathbb{E}[\epsilon_i] = 0\text{)}
\end{align}

For BMPS:
\begin{align}
    \mathbb{E}[\tilde{\theta}^i_F] &= \mathbb{E}[\text{Clip}_\rho(\tilde{\theta}^{i-1}_F - \eta g(\tilde{\theta}^{i-1}_F; B_i)) + \epsilon^i_\text{top-H}] \\
    &= \mathbb{E}[\text{Clip}_\rho(\tilde{\theta}^{i-1}_F - \eta g(\tilde{\theta}^{i-1}_F; B_i))] \quad \text{(since } \mathbb{E}[\epsilon^i_\text{top-H}] = 0\text{)}
\end{align}
Under above Assumptions~\ref{assum:clip} and \ref{assum:stats}, we can conclude that the expected parameter updates in both methods are approximately equivalent:
\begin{equation}
    \mathbb{E}[\theta^i_F - \theta^{i-1}_F] \approx \mathbb{E}[\tilde{\theta}^i_F - \tilde{\theta}^{i-1}_F]
\end{equation}

This approximate equivalence holds for each iteration, and thus can be extended to the entire training process.
\end{proof}

%\textbf{Discussion:} The proof relies on two main assumptions: (1) the limited impact of the Clip operation, and (2) the similarity in statistical properties of the parameters in both methods. These assumptions are reasonable in practice but may not always hold strictly. The equivalence shown is therefore an approximation, and its accuracy may vary depending on specific conditions such as the choice of hyperparameters and the nature of the data and model. Further empirical validation and theoretical analysis could provide more insights into the conditions under which this approximation is most accurate.

\subsubsection{Approximate Equivalence in the Inference Phase}
Building upon the results from the fine-tuning phase, we now extend our analysis to the inference phase. Recall that in the fine-tuning phase, we established the approximate equivalence between BMPS and the standard randomized smoothing approach in terms of their expected parameter updates:

\begin{equation}
    \mathbb{E}[\theta^i_F - \theta^{i-1}_F] \approx \mathbb{E}[\tilde{\theta}^i_F - \tilde{\theta}^{i-1}_F]
\end{equation}

This equivalence suggests that the final model parameters obtained from BMPS ($\tilde{\theta}^I_F$) should have similar statistical properties to those obtained from the standard randomized smoothing approach. Furthermore, we demonstrated that adding noise to parameters (in BMPS) and adding noise to data (in the standard approach) produce similar effects during training.

Extending this reasoning to the inference phase, we introduce an additional assumption that builds directly on these findings:

\begin{assumption}
\label{assum:param_noise}
Given the equivalence established in the fine-tuning phase, we assume that the effect of adding noise to the parameters during inference in BMPS is approximately equivalent to the effect of fine-tuning on randomized datasets in the standard framework. Formally, for each $k = 1, ..., K$:
\begin{equation}
    \tilde{\theta}^I_F + \epsilon_{k,\text{top-H}} \approx \Omega(\theta'_P, D_F \oplus \epsilon_k)
\end{equation}
where $\Omega$ represents the fine-tuning process, $\theta'_P$ are the poisoned pre-trained model parameters, $D_F$ is the fine-tuning dataset, and $\epsilon_k$ is the noise added to the dataset in the standard framework.
\end{assumption}

This assumption is a natural extension of our findings from the fine-tuning phase, positing that the equivalence between parameter noisification and data randomization continues to hold during inference.

With this foundation, we can now proceed to prove the approximate equivalence of BMPS and the standard randomized smoothing framework in the inference phase.

\begin{theorem}
Under Assumption~\ref{assum:clip}, \ref{assum:stats}, and \ref{assum:param_noise}, the BMPS inference phase is approximately equivalent to the standard randomized smoothing framework described in Section~\ref{sec:rs defense}.
\end{theorem}

\begin{proof}
Recall from Section~\ref{sec:rs defense}, the standard randomized smoothing framework involves fine-tuning $K$ models on $K$ distinct randomized datasets to obtain $K$ voters. The smoothed model $\tilde{f}$ is defined as follows:

\begin{equation}
    \tilde{f}(\mathbf{x}') = \arg\max_{y \in \mathcal{Y}} \sum_{k=1}^K \mathbbm{1}(f(\tilde{\mathbf{x}}_k; \tilde{\theta}_{F,k}) = y)
\end{equation}

where $\tilde{\mathbf{x}}_k = \mathbf{x}' \oplus u_k$, $\tilde{\theta}_{F,k} = \Omega(\theta'_P, D_F \oplus \epsilon_k)$.

For BMPS in the inference phase, we have:
\begin{equation}
    \tilde{\theta}_{F,k} = \text{Clip}_\rho(\tilde{\theta}^I_F) + \epsilon_{k,\text{top-H}}
\end{equation}
where $\epsilon_{k,\text{top-H}} \sim \mathcal{N}(0, \sigma^2I)$ for the top $H$ layers.

Under Assumption~\ref{assum:clip}, we have:
\begin{equation}
    \tilde{\theta}_{F,k} \approx \tilde{\theta}^I_F + \epsilon_{k,\text{top-H}}
\end{equation}

Then, applying Assumption~\ref{assum:param_noise}, we can see that the $K$ voters in BMPS:
\begin{equation}
    f(\mathbf{x}; \tilde{\theta}_{F,k}) \approx f(\mathbf{x}; \Omega(\theta'_P, D_F \oplus \epsilon_k))
\end{equation}
are approximately equivalent to the $K$ voters in the standard randomized smoothing framework.

Furthermore, BMPS also applies randomized input perturbation $\mathbf{x}' \oplus u_k$ during inference, which is identical to the standard framework.

Therefore, the output of BMPS can be approximated as:
\begin{equation}
    f_{\text{BMPS}}(\mathbf{x}') \approx \arg\max_{y \in \mathcal{Y}} \sum_{k=1}^K \mathbbm{1}(f(\tilde{\mathbf{x}}_k; \tilde{\theta}_{F,k}) = y)
\end{equation}
This is approximately equivalent to the smoothed model $\tilde{f}(\mathbf{x}')$ in the standard randomized smoothing framework.
\end{proof}

\subsubsection{Conclusion of Equivalence Proof}
Through our analysis of both the fine-tuning and inference phases, we have established the approximate equivalence between the BMPS method and the standard randomized smoothing framework described in Section~\ref{sec:rs defense}. 

In the fine-tuning phase, we showed that:
\begin{equation}
    \mathbb{E}[\theta^i_F - \theta^{i-1}_F] \approx \mathbb{E}[\tilde{\theta}^i_F - \tilde{\theta}^{i-1}_F]
\end{equation}
demonstrating that BMPS and standard randomized smoothing have approximately equivalent parameter update dynamics during training.

Building on this result, we extended the equivalence to the inference phase, showing that:
\begin{equation}
    f_{\text{BMPS}}(\mathbf{x}') \approx \tilde{f}(\mathbf{x}') = \arg\max_{y \in \mathcal{Y}} \sum_{k=1}^K \mathbbm{1}(f(\tilde{\mathbf{x}}_k; \tilde{\theta}_{F,k}) = y)
\end{equation}
where $f_{\text{BMPS}}$ is the output of BMPS and $\tilde{f}$ is the smoothed model in the standard framework.

These results collectively demonstrate that BMPS approximates the behavior of standard randomized smoothing throughout the entire process, from training to inference. The key insight is that adding noise to parameters (in BMPS) can effectively simulate the effect of data randomization (in standard randomized smoothing), leading to similar robustness properties.

It's important to note that this equivalence is approximate and relies on the assumptions stated in Assumptions~\ref{assum:clip}, \ref{assum:stats}, and \ref{assum:param_noise}. While these assumptions are theoretically justified and practically reasonable, the exact degree of approximation may vary depending on specific model architectures, datasets, and hyperparameters.

%This equivalence proof provides a theoretical foundation for BMPS, suggesting that it can achieve similar certified robustness to standard randomized smoothing while potentially offering computational advantages, especially in the inference phase where only one model needs to be trained and stored.
\subsection{Further Discussion}
\textbf{Computational Efficiency:} While theoretically equivalent, BMPS offers significant computational advantages: 1) Reduced storage: BMPS only requires storing one set of model parameters instead of $K$ sets; 2) Faster training: BMPS performs smoothing on-the-fly, eliminating the need for $K$ separate fine-tuning processes.

\textbf{Flexibility:} BMPS allows for easy adjustment of the smoothing intensity during inference without retraining, providing greater adaptability to different deployment scenarios.

\textbf{Conclusion:} This proof establishes the theoretical equivalence between our proposed BMPS method and the standard randomized smoothing defense framework. While maintaining the same theoretical guarantees, BMPS offers substantial improvements in computational efficiency and flexibility, making it a more practical choice for real-world applications.


\section{Supplementary Introduction to Experiment Setup}
\label{appendix: supp experiment setup}
\subsection{Detailed Introduction to Evaluation Metrics}
In this section, we provide a more detailed explanation of the evaluation metrics used in our study: Clean Accuracy (CA), Poisoned Accuracy (PA), and Attack Success Rate (ASR). These metrics are crucial for comprehensively assessing the effectiveness of defense methods against backdoor attacks in pre-trained language models.

\subsubsection{Clean Accuracy (CA)}
Clean Accuracy measures the model's performance on benign, unaltered inputs. It is essential to ensure that the defense method does not significantly degrade the model's performance on clean data.
\begin{equation}
    CA = \frac{1}{|D_{clean}|} \sum_{(x,y) \in D_{clean}} \mathbbm{1}[f(x) = y]
\end{equation}
where $D_{clean}$ is the set of clean test samples, $(x,y)$ is a sample-label pair, $f(x)$ is the model's prediction for input $x$, and $\mathbb{1}[\cdot]$ is the indicator function.
A high CA indicates that the defense strategy does not adversely affect the model's performance on legitimate inputs, avoiding overcautious behavior that might compromise overall functionality.

\subsubsection{Poisoned Accuracy (PA)}
Poisoned Accuracy evaluates the model's ability to correctly classify poisoned inputs (inputs containing backdoor triggers) to their original, correct labels rather than the attacker's target labels.
\begin{equation}
    PA = \frac{1}{|D_{poison}|} \sum_{(x',y) \in D_{poison}} \mathbbm{1}[f(x') = y]
\end{equation}
where $D_{poison}$ is the set of poisoned test samples, $x'$ is a poisoned input, and $y$ is its original, correct label (not the attacker's target label).
A high PA demonstrates that the defense method effectively mitigates the impact of backdoor triggers, allowing the model to maintain accurate predictions even on poisoned inputs.

\subsubsection{Attack Success Rate (ASR)}
Attack Success Rate measures the proportion of poisoned inputs that the model misclassifies to the attacker's intended target label.
\begin{equation}
    ASR = \frac{1}{|D_{poison}|} \sum_{(x',y) \in D_{poison}} \mathbbm{1}[f(x') = y_{target}]
\end{equation}
where $y_{target}$ is the attacker's target label for the poisoned input $x'$.
A lower ASR indicates better defense performance, as it shows that the model is less likely to be manipulated into producing the attacker's desired outputs when presented with backdoored inputs.

\subsubsection{Interpretation and Trade-offs}
When evaluating backdoor defense methods, it's crucial to consider these metrics holistically:
\begin{itemize}[leftmargin=*]
    \item An ideal defense method should maintain high CA and PA while achieving low ASR.
    \item There's often a trade-off between these metrics. For instance, an overly aggressive defense might lower ASR but also decrease CA.
    \item The relative importance of each metric may vary depending on the specific application and threat model.
\end{itemize}
By analyzing these metrics together, we can comprehensively assess a defense method's ability to protect against backdoor attacks while preserving the model's performance on legitimate inputs.

\subsection{Detailed Introduction to Implementation Details}
This section provides comprehensive information about the experimental setup, including hardware specifications, software environment, hyperparameter settings, and model configurations used in our study.

\subsubsection{Hardware Configuration}
All experiments were conducted on a high-performance computing cluster with the following specifications:
\begin{itemize}[leftmargin=*]
    \item GPUs: 8 NVIDIA RTX A6000
    \item GPU Memory: 48GB per GPU
    \item CPU: Intel Xeon Gold 6248R @ 3.00GHz
    \item RAM: 512GB DDR4
    \item Storage: 2TB NVMe SSD
\end{itemize}

\subsubsection{Software Environment}
Our experiments were implemented using the following software stack:
\begin{itemize}[leftmargin=*]
    \item Operating System: Ubuntu 20.04 LTS
    \item CUDA Version: 11.3
    \item Python Version: 3.8.5
    \item PyTorch Version: 1.9.0
    \item Transformers Library: Hugging Face Transformers 4.11.3
    \item Other key libraries: NumPy 1.21.2, SciPy 1.7.1, scikit-learn 0.24.2
\end{itemize}

\subsubsection{Experimental Setup}
To ensure the reliability and reproducibility of our results, we adhered to the following experimental protocol:
\begin{itemize}[leftmargin=*]
    \item Each experiment was repeated five times with different random seeds.
    \item The random seeds used were: 42, 123, 256, 789, 1024.
    \item Results reported in the main paper are the average of these five runs.
    \item Standard deviation was calculated to assess the stability of the results.
\end{itemize}

\subsubsection{Hyperparameter Settings}
The key hyperparameters for our Fuzzed Randomized Smoothing (FRS) method were set as follows:
\begin{itemize}[leftmargin=*]
    \item Base model number ($K$): 20
    \item Variance of Gaussian noise for parameter smoothing ($\sigma$): 0.01
    \item Number of top layers for smoothing ($H$): 10
    %\item Learning rate for fine-tuning: 2e-5
    %\item Batch size: 32
    %\item Number of epochs: 3
    \item Maximum sequence length: 128
    \item Warmup steps: 0.1 * total\_steps
    \item Weight decay: 0.01
\end{itemize}

For BERT and RoBERTa fine-tuning:
\begin{itemize}[leftmargin=*]
    \item Learning rate for fine-tuning: 2e-5
    \item Batch size: 32
    \item Number of epochs: 3
    \item Optimizer: AdamW
    \item Scheduler: Linear decay with warmup
\end{itemize}

For LLaMA3-8B fine-tuning, we used LoRA (Low-Rank Adaptation) with the following settings:
\begin{itemize}[leftmargin=*]
    \item LoRA rank: 8
    \item LoRA alpha: 16
    \item LoRA alpha: 16
    \item Target modules: q\_proj, k\_proj, v\_proj, o\_proj, gate\_proj, up\_proj, down\_proj
    \item Learning rate for LoRA: 1e-4
    \item Batch size: 16
    \item Number of epochs: 3
    \item Optimizer: AdamW
    \item Scheduler: Cosine decay with warmup
    \item Trainable parameters: ~35M ($0.44\%$ of full model)
\end{itemize}
We used full fine-tuning for BERT and RoBERTa models, updating all parameters during the process. For LLaMA3-8B, we employed LoRA~\citep{hulora} to efficiently fine-tune the model while keeping most of the pre-trained weights frozen. By targeting multiple modules (query, key, value, output projections, and MLP layers), we aimed to achieve a more comprehensive adaptation while still maintaining the efficiency benefits of LoRA. This approach allowed us to fine-tune the large model effectively while significantly reducing the computational resources required compared to full fine-tuning.

These hyperparameters were chosen based on preliminary experiments and are consistent across all datasets unless otherwise specified.

\subsubsection{Model Configurations}
We used the following pre-trained language models in our experiments:
\begin{itemize}[leftmargin=*]
    \item BERT:
    \begin{itemize}
        \item Version: bert-base-uncased, bert-large-uncased
        \item Source: Hugging Face Model Hub
        \item BERT-base parameters: 110M
        \item BERT-large parameters: 340M
    \end{itemize}
    \item RoBERTa:
    \begin{itemize}
        \item Version: roberta-base, roberta-large
        \item Source: Hugging Face Model Hub
        \item RoBERTa-base parameters: 125M
        \item RoBERTa-large parameters: 355M
    \end{itemize}
    \item LLaMA3:
    \begin{itemize}
        \item Version: llama3-8b
        \item Source: Meta AI (with necessary permissions)
        \item Parameters: 8B
    \end{itemize}
\end{itemize}
All models were used with their default tokenizers as provided by the Hugging Face Transformers library.

\subsubsection{Data Preprocessing}
For all datasets, we applied the following preprocessing steps:
\begin{itemize}[leftmargin=*]
    \item Lowercasing (for uncased models)
    \item Removal of special characters and excessive whitespace
    \item Truncation or padding to a maximum sequence length of 128 tokens
\end{itemize}

\subsubsection{Computational Resources}
The total computational resources used for this study were approximately:
\begin{itemize}[leftmargin=*]
    \item GPU hours: 2,400 (300 hours * 8 GPUs)
    \item Estimated power consumption: 19,200 kWh
\end{itemize}

We acknowledge the environmental impact of our experiments and are committed to improving efficiency in future work.


\section{Certified Accuracy}
\label{appendix: certified accuracy}
\begin{table*}[h]
\caption{Certified accuracy under different perturbation levels on SST-2, OffensEval, and AG's News.}
\centering
\small
\renewcommand\arraystretch{0.9}
\begin{tabular}{ccccccc}
\toprule
\multirow{2}{*}{Perturbation level} & \multicolumn{2}{c}{SST-2} & \multicolumn{2}{c}{OffensEval} & \multicolumn{2}{c}{AG's News} \\
\cmidrule(r){2-3} \cmidrule(r){4-5} \cmidrule(r){6-7}
& TextGuard & FRS & TextGuard & FRS & TextGuard & FRS \\
\midrule
10\% & 72.34\% & 78.92\% & 75.84\% & 81.20\% & 70.22\% & 74.71\% \\
20\% & 65.16\% & 73.58\% & 69.36\% & 76.52\% & 63.69\% & 69.82\% \\
30\% & 57.83\% & 67.20\% & 62.18\% & 70.97\% & 56.93\% & 64.16\% \\
40\% & 49.67\% & 60.81\% & 54.75\% & 64.61\% & 49.50\% & 57.93\% \\
50\% & 41.27\% & 53.75\% & 46.92\% & 57.86\% & 41.86\% & 51.25\% \\
\bottomrule 
\end{tabular}
\label{tab:certified_accuracy}
\end{table*}
Certified accuracy provides a crucial metric for evaluating the robustness of language models against backdoor attacks, offering theoretical guarantees on model performance under all possible perturbations within a specified threshold. Unlike clean and poisoned accuracies, which are empirical measures for specific attack schemes, certified accuracy provides a lower bound on performance across all potential attacks, directly validating our theoretical findings in Section~\ref{sec: theory} regarding the broader certified robustness radius achieved by our Fuzzed Randomized Smoothing method.

To empirically demonstrate this theoretical advantage, we calculate the certified accuracy for different perturbation levels (e.g., percentage of perturbed tokens) for both our FRS method and the best-performing baseline TextGuard. Note that TextGuard is the only \textit{certified defense} approach among our compared baselines. By comparing certified accuracies, we can provide a more comprehensive and reliable evaluation of our method's robustness, complementing the clean and poisoned accuracy results presented in Section~\ref{sec: experiment results}. Higher certified accuracy across various perturbation levels would strongly support the practical benefits of FRS's enhanced robustness radius. We provide the results on three datasets in Table~\ref{tab:certified_accuracy}. From the results in the table, we have the following several findings: First, FRS consistently outperforms TextGuard across all datasets and perturbation levels. This superiority is maintained even as the perturbation level increases, demonstrating the robust nature of our approach. Second, the performance gap between FRS and TextGuard becomes more pronounced as the perturbation level increases. For instance, on the SST-2 dataset, the gap widens from $6.58\%$ at $10\%$ perturbation to $12.48\%$ at $50\%$ perturbation. Third,  While both methods show a decline in certified accuracy as perturbation levels increase, FRS exhibits a more gradual decline. This suggests that FRS is more resilient to higher levels of perturbation compared to TextGuard. Forth, the superior performance of FRS is consistent across all three datasets, indicating that our method's effectiveness is not limited to a specific type of text classification task. Finally, Even at very high perturbation levels ($40-50\%$), FRS maintains a substantial certified accuracy (ranging from $51.25\%$ to $60.81\%$ across datasets), significantly outperforming TextGuard.

These results strongly support our theoretical findings in Section~\ref{sec: theory} regarding the broader certified robustness radius achieved by our FRS method. The consistently higher certified accuracy of FRS across various perturbation levels and datasets empirically validates the theoretical advantages of our approach.
\vspace{-2mm}

\section{Further Ablation Study}
\label{appendix: further ablation study}
To further illustrate the influence of the fine-tuning phase and inference phase separately in biphased model parameter smoothing, we conducted an ablation study by removing each component from the overall framework and observing the corresponding results. We provide the empirical results for SST-2, OffensEval, and AG's News datasets in Table~\ref{tab:further ablation_study}. 

%The empirical results have been provided in Table~\ref{tab: further ablation_study}. These results show that BMPS in both phases contribute significantly to the overall performance of BMPS. Removing the BMPS(fine-tuning) has a larger impact on PA and ASR than removing the BMPS (inference). As for the CA metric, their influence are both relatively weak, demonstrating our advantage on preserving high performance on clean test data.

\begin{table*}[]
\caption{Further Ablation study for BMPS (Fine-tuning), BMPS (Inference), and BMPS on SST-2, OffensEval, and AG's News datasets. BERT-base is taken as the victim model.}
\centering
\small
\renewcommand\arraystretch{0.9}
\resizebox{\textwidth}{!}{%
\begin{tabular}{ccp{1cm}p{1cm}p{1cm}p{1cm}p{1cm}p{1cm}p{1cm}p{1cm}p{1cm}}
\toprule
\multirow{2}{*}{Dataset} & \multirow{2}{*}{Method} & \multicolumn{3}{c}{RIPPLe${_a}$} & \multicolumn{3}{c}{LWP} & \multicolumn{3}{c}{BadPre} \\
\cmidrule(r){3-5} \cmidrule(r){6-8} \cmidrule(r){7-8} \cmidrule(r){9-11}
                         &                         & CA$\uparrow$          & PA$\uparrow$          & ASR$\downarrow$           & CA$\uparrow$        & PA$\uparrow$        & ASR$\downarrow$   & CA$\uparrow$        & PA$\uparrow$  & ASR$\downarrow$   \\
\cmidrule(r){1-2} \cmidrule(r){3-5} \cmidrule(r){6-8} \cmidrule(r){9-11} 
\multirow{4}{*}{SST-2}  
& -BMPS (Fine-tuning)   &   82.15\%       &   70.18\%         &    49.37\%        &   85.37\%         &     78.83\%      &    41.69\%      &   91.46\%        &    88.43\%   &    24.51\%         \\
& -BMPS(Inference)   &   82.41\%       &   72.46\%         &    46.85\%        &   85.73\%         &     81.57\%      &    36.18\%      &   91.67\%        &    89.86\%   &    20.29\%  \\ 
& -BMPS                   &    82.48\%       &     68.62\%        &    51.09\%        &   85.82\%        &   74.95\%        &    44.52\%      &   91.60\%        &   85.25\%   &   27.87\%  \\ \cmidrule(r){2-11}
& FRS       &  82.36\%          &    73.25\%        &     45.12\%     &   85.67\%        &  82.91\%         &    34.25\%   &   91.64\%        &  91.02\%    &  18.64\%    \\
\bottomrule \toprule
\multirow{4}{*}{OffensEval}   
& -BMPS (Fine-tuning)   &    85.26\%       &   77.62\%         &  47.18\%          &   87.42\%         &    81.41\%       &   45.39\%         &    93.94\%       &  89.16\%       &  40.73\%        \\ 
& -BMPS(Inference)   &    85.57\%       &   78.65\%         &  44.37\%          &   87.65\%         &    84.32\%       &   42.85\%         &    94.08\%       &  90.25\%       &  39.64\%        \\
& -BMPS                   &   85.52\%         &   76.46\%         &    49.53\%        &    87.69\%       &   80.62\%        &  47.25\%     &   94.12\%        &   87.79\%   &   42.07\%    \\ \cmidrule(r){2-11}
& FRS      &    85.61\%         &  79.38\%          &  42.59\%          &   87.63\%        &  85.70\%         &  41.24\%       &   94.05\%        &  91.43\%    &  38.86\%   \\
\bottomrule \toprule
\multirow{4}{*}{AG's News}   
& -BMPS (Fine-tuning)   &    79.65\%       &   67.73\%         &  40.35\%          &   83.89\%         &    76.72\%       &   40.84\%         &    92.34\%       &    86.58\%    &  38.61\%     \\
& -BMPS(Inference)   &    79.87\%       &   71.58\%         &  38.29\%          &   84.07\%         &    79.51\%       &   39.27\%         &    92.38\%       &    88.17\%    &  37.85\%  \\ 
& -BMPS                   &   79.91\%         &   66.46\%         &    42.02\%        &    84.02\%       &   74.19\%        &  42.27\%     &   92.44\%        &   84.91\%    &   39.84\%    \\ \cmidrule(r){2-11}
& FRS      &    79.84\%         &  72.36\%          &  37.04\%          &   83.76\%        &  80.84\%         &  38.16\%       &   92.25\%        &  89.34\%    &  36.93\%   \\
\bottomrule
\end{tabular}
}
\label{tab:further ablation_study}
\end{table*}

From the presented results, we have the following observations: 1) Across all datasets and attack methods, the full FRS implementation generally achieves the best balance between Clean Accuracy (CA), Poisoned Accuracy (PA), and Attack Success Rate (ASR). This demonstrates the synergistic effect of combining both fine-tuning and inference phase BMPS. 2) 
Removing the fine-tuning phase BMPS (-BMPS (Fine-tuning)) consistently leads to a decrease in PA and an increase in ASR compared to the full FRS implementation. For instance, on the SST-2 dataset under the RIPPLe${_a}$ attack, PA drops from $73.25\%$ to $70.18\%$, while ASR increases from $45.12\%$ to $49.37\%$. This trend is consistent across all datasets and attack methods, highlighting the importance of the fine-tuning phase in enhancing robustness against backdoor attacks. 3) The removal of inference phase BMPS (-BMPS (Inference)) generally has a smaller impact on performance compared to removing the fine-tuning phase. In some cases, it even slightly improves CA. For example, on the AG's News dataset under the BadPre attack, CA increases from $92.25\%$ to $92.38\%$. However, the PA and ASR results are still generally worse than the full FRS implementation, indicating that the inference phase of BMPS contributes to the overall robustness of the model. 4) Interestingly, removing both phases of BMPS (-BMPS) often results in the worst performance, particularly in terms of PA and ASR. This suggests that even partial implementation of BMPS (either in fine-tuning or inference) is beneficial compared to no BMPS at all. 5) The relative performance of different BMPS configurations remains consistent across the three attack methods (RIPPLe${_a}$, LWP, and BadPre). This suggests that the benefits of BMPS are not limited to a specific type of backdoor attack but provide general robustness improvements. 6) While the overall trends are consistent, the magnitude of improvements varies across datasets. For instance, the improvements brought by FRS are more pronounced on the SST-2 and OffensEval datasets compared to AG's News, particularly for the PA metric.

In conclusion, this further ablation study demonstrates that both phases of BMPS contribute significantly to the overall performance of the FRS method. The fine-tuning phase appears to have a more substantial impact on improving robustness against backdoor attacks, as evidenced by the larger changes in PA and ASR when it is removed. However, the inference phase also plays a crucial role, and the combination of both phases yields the best overall results. The consistency of these findings across different datasets and attack methods underscores the generalizability and effectiveness of the biphased BMPS approach in FRS. 

\section{Supplementary Consistency Analysis}
\label{appendix: supplementary consistency}
\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{Figures/different_victim_models.pdf}
    \caption{The comparison between our FRS method and no defense method on ASR metric in different datasets. The BadPre is taken as the pre-training attack method. 'b' and 'l' are short for 'base' and 'large', respectively.
    }
   \label{fig: robustness}
    \vspace{-0.3cm}
\end{figure*}
To explore whether our method can achieve consistent defense effect over different victim models, we also compare our FRS method with the defense-free baseline as the supplement to Section~\ref{sec: consistency}. From the ASR results in Figure~\ref{fig: robustness}, we can find that when the model size expands from the base version to the large version for BERT and RoBERTa, the ASR of no defense version decreases and the relative defense effect improvement brought by our FRS method shrinks. Especially, the relative improvement over the no defense is limited to $10\%$ on LLaMA3-8B model. This is because that along the model size increases, the intelligence level of PLMs is also enhanced according to the scaling law, thus the probability that the PLM is cheated by the backdoor is also reduced. As a result, the effect space of our FRS becomes limited. 

\section{Discussion on Enhancing Scalability}
\label{appendix: enhancing scalability}
The results in Table~\ref{tab: scalability analysis} demonstrate that our FRS method provides robust defense against backdoor attacks across a wide range of model sizes and architectures, consistently outperforming the strong baseline of TextGuard. However, we can also observe the diminishing effectiveness of FRS on larger models like LLaMA3-8B. This suggests that as language models grow in size and capability, they may become inherently more robust to certain backdoor attacks, potentially reducing the marginal benefit of defenses like FRS. To address this challenge and ensure FRS remains effective for future larger models, we propose the following directions for future work:
\begin{itemize}[leftmargin=*]
\item \textbf{Developing Adaptive defense strategies}: Adaptive defense strategies could be developed to dynamically adjust based on model size and architecture. This approach might involve creating a mechanism that automatically tunes FRS parameters, such as randomization strength or fuzzing strategy, in response to the model's scale. We could explore layer-wise or module-wise smoothing strategies, where different parts of the model receive tailored levels of defense. For instance, we might apply stronger smoothing to layers that are more susceptible to backdoor attacks, based on empirical observations or theoretical analysis. Additionally, we could investigate how to leverage the model's attention mechanisms to guide more targeted defense strategies, potentially focusing on the most influential parts of the model for a given input.
\item \textbf{Incorporating model-specific knowledge}: Incorporating model-specific knowledge into the fuzzing process could significantly enhance FRS's effectiveness for larger models. This direction would involve developing methods to analyze and utilize the structural characteristics of the model, such as the number of attention heads, layer count, or specific architectural features unique to large language models. We could explore how to integrate information from the model's pre-training tasks to improve defense strategies, potentially identifying and protecting areas of the model that are most critical for maintaining its general language understanding capabilities. Furthermore, we could study how different types of large language models (e.g., encoder-only, decoder-only, encoder-decoder) respond to FRS and develop tailored fuzzing strategies for each architecture type.
\item \textbf{Exploring complementary techniques}: Exploring complementary techniques that can enhance FRS for very large models is another promising direction. We could investigate the synergistic effects of combining FRS with other defense mechanisms, such as adversarial training or knowledge distillation. This hybrid approach might allow us to leverage the strengths of multiple defense strategies while mitigating their individual weaknesses. Another avenue could be to explore the integration of model compression techniques with FRS, aiming to maintain defense effectiveness while improving efficiency for large-scale models. We could also research how to utilize model interpretation techniques to guide the fuzzing process more effectively, perhaps by identifying and focusing on the key features that influence the model's decisions.
\item \textbf{Investigating the relationship between model scale and inherent robustness}:Investigating the relationship between model scale and inherent robustness to backdoors is crucial for understanding the evolving landscape of model security. We propose conducting a systematic study to evaluate the inherent robustness of models across various scales, from small to very large. This research could involve designing experiments to measure how the effectiveness of different types of backdoor attacks changes as model size increases. We could explore whether there exists a critical point in model scale beyond which inherent robustness significantly increases. Additionally, we could analyze which characteristics of large models contribute to increased robustness and investigate how these insights might be applied to enhance the security of smaller models.
\end{itemize}
By pursuing these research directions, we aim to address the challenges posed by increasing model sizes and ensure that FRS remains an effective defense strategy for future generations of large language models.

\begin{table}[h]
\caption{Time cost of fuzzed text randomization on various dataset sizes.}
\centering
\small
\renewcommand\arraystretch{0.9}
\resizebox{0.47\textwidth}{!}{
\begin{tabular}{ccc}
\toprule
Dataset Size & Processing Time & Throughput  \\
\midrule     
 1,000 & 5 seconds & 200 \\
 10,000 & 48 seconds	 & 208 \\ 
 100,000 & 8 minutes	 & 208 \\
 1000,000 & 79 minutes & 211 \\ 
\bottomrule
\end{tabular}
}
\label{tab:efficiency analysis}
\end{table}

\section{Efficiency Analysis}
Considering that our proposed fuzzed text randomization involves the Monte Carlo tree search process to identify the vulnerable textual segments, the time consumption of this stage directly determines the efficiency of the overall framework. To explore if the time cost of data randomization is still under budget when the data scales up, we conduct additional experiments to measure the processed time and the throughout (samples/second) of fuzzed text randomization on various dataset sizes. As shown in Table~\ref{tab:efficiency analysis}, the processing time scales approximately linearly with the dataset size, and the throughput remains relatively constant. This suggests that our method is scalable to larger datasets. We've also implemented several optimizations to improve efficiency, including parallel processing and caching of intermediate results.

\begin{wraptable}{r}{0.5\textwidth}
%\begin{table}[]
\caption{The hyperparameter robustness experiment results. BadPre is taken as the attack method.}
\centering
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{cccccc}
\toprule
\multicolumn{2}{c}{$K$}                        & 10 & 20 & 40 & 80  \\ \cmidrule(r){1-6}
\multicolumn{1}{c}{\multirow{3}{*}{SST-2}} & CA  & 91.57\%  & 91.64\%  & 91.62\%   & 91.69\%     \\ 
\multicolumn{1}{c}{}                      & PA & 89.35\%  & 91.02\%  &  91.87\%  & 91.85\%     \\ 
\multicolumn{1}{c}{}                      & ASR & 22.06\%  & 18.64\%  & 17.93\%   &  18.02\%     \\ \cmidrule(r){1-6}
\multicolumn{1}{c}{\multirow{3}{*}{OffensEval}} & CA  & 93.88\%  & 94.05\%  & 94.13\%   & 94.09\%      \\ 
\multicolumn{1}{c}{}                      & PA & 90.27\%  & 91.43\%  & 91.90\%   &  92.08\%    \\ 
\multicolumn{1}{c}{}                      & ASR & 40.98\%  & 38.86\%  & 38.35\%   &  38.22\%     \\ \cmidrule(r){1-6}
\multicolumn{1}{c}{\multirow{3}{*}{AG's News}} & CA  & 92.21\%  & 92.25\%  & 92.39\%   & 92.21\%    \\ 
\multicolumn{1}{c}{}                      & PA & 87.97\%  & 89.34\%  & 89.82\%   &  89.88\%     \\ 
\multicolumn{1}{c}{}                      & ASR & 37.81\%  & 36.93\%  & 36.53\%   &  36.41\%     \\ \bottomrule \toprule
\multicolumn{2}{c}{$\sigma$}                        & 0.005 & 0.01 & 0.02 & 0.04  \\ \cmidrule(r){1-6}
\multicolumn{1}{c}{\multirow{3}{*}{SST-2}} & CA  & 91.79\%  & 91.64\%  & 91.23\%   & 89.98\%      \\ 
\multicolumn{1}{c}{}                      & PA & 91.10\%  & 91.02\%  & 90.86\%   & 90.43\%      \\ 
\multicolumn{1}{c}{}                      & ASR & 18.48\%  & 18.64\%  &  18.97\%  & 20.15\%      \\ \cmidrule(r){1-6}
\multicolumn{1}{c}{\multirow{3}{*}{OffensEval}} & CA  & 94.13\%  & 94.05\%  & 94.08\%   &  91.36\%     \\ 
\multicolumn{1}{c}{}                      & PA & 91.59\%  & 91.43\%  & 91.27\%   &  90.76\%    \\ 
\multicolumn{1}{c}{}                      & ASR & 38.62\%  & 38.86\%  &  39.30\%  &  40.35\%    \\ \cmidrule(r){1-6}
\multicolumn{1}{c}{\multirow{3}{*}{AG's News}} & CA  & 92.28\%  & 92.25\%  & 92.06\%   &  90.81\%     \\ 
\multicolumn{1}{c}{}                      & PA & 89.15\%  & 89.34\%  & 89.06\%   &  87.49\%     \\ 
\multicolumn{1}{c}{}                      & ASR & 37.16\%  & 36.93\%  &  37.48\%  &  38.92\%    \\ \bottomrule
\end{tabular}}
\label{tab:hyperparameter}
\vspace{-2mm}
%\end{table}
\end{wraptable}
\section{Hyperparameter Robustness Analysis} 
To answer whether our method can achieve relatively robust defense performance when set in different hyperparameter configurations, we tune the number of base models $K$ for majority voting from 10 to 80, the variance of Gaussian noise $\sigma$ applied to the model parameters from 0.005 to 0.04. The corresponding results are present in Table~\ref{tab:hyperparameter}. From the upper subtable, we can find that the performance on PA and ASR metrics can indeed be improved when $K$ ranges from 10 to 40, though the scale from 20 to 40 is smaller than that from 10 to 20. However, when the $K$ rises from 40 to 80, the defense performance on PA and ASR almost remains unchanged. Meanwhile, the adjustment of $K$ causes weak influence on the performance of CA metric. As for $\sigma$, the performance on PA and ASR almost keeps consistent when $\sigma$ varies from 0.005 to 0.02, while dropping slightly when it increases from 0.02 to 0.04. This effectively demonstrate that our method exhibits the hyperparameter robustness in a certain interval when defending the backdoor attacks. On the other hand, we can observe that the variation of $\sigma$ can result in the fluctuation in CA metric to some extent. In detail, when $\sigma$ increases from 0.005 to 0.02, the decrease in CA is still small. However, this decrease becomes nonnegligible when it rises to $0.04$, which is reasonable because the excessive smoothing noise can destroy the model's ability. According to this, limiting the hyperparameter variation scale to a certain interval is necessary.



\section{Case Study}
To more comprehensively analyze our proposed method, it's necessary to explore whether our MCTS-based trigger locating module can effectively identify the vulnerable area in the perturbed text input within a limited interaction budget. Therefore, we illustrate the identified vulnerable areas under different interaction budgets in several real cases. The cases are illustrated in the following grey text box. The triggers are highlighted as red in the original perturbed text. The texts in blue are the identified vulnerable areas. 
\newtcolorbox{lambadacasebox}{
  colback=gray!10,
  colframe=gray!40,
  coltitle=black,
  title=\textbf{Lambada premise confusion case:},
  fonttitle=\bfseries,
  sharp corners,
  boxrule=0.5pt,
  enhanced,
  left=2mm,  
  right=2mm, 
  top=2mm, 
  bottom=2mm
}
%\vspace{-3mm}
\begin{tcolorbox}[breakable]
\textbf{Case 1} \\
\textbf{Original Perturbed Text}: It's a \textcolor{red}{mn} charming and often affecting journey.\\
\textbf{Under Budget 3}: \textcolor{blue}{It's a mn charming} and often affecting journey.\\
\textbf{Under Budget 5}: It's \textcolor{blue}{a mn} charming and often affecting journey.\\
\textbf{Under Budget 10}: It's a \textcolor{blue}{mn} charming and often affecting journey.\\
\textbf{Under Budget 15}: It's a \textcolor{blue}{mn} charming and often affecting journey. \\

\textbf{Case 2} \\
\textbf{Original Perturbed Text}: It's just disappointingly superficial -- a movie that has all the elements necessary to be a fascinating, involving \textcolor{red}{bb} character study, but never does more than scratch the surface.\\
\textbf{Under Budget 3}: It's just disappointingly superficial -- a movie that has all the elements necessary to be a \textcolor{blue}{fascinating, involving bb character study, but never does more than scratch the surface.}\\
\textbf{Under Budget 5}: It's just disappointingly superficial -- a movie that has all the elements necessary to be a fascinating, \textcolor{blue}{involving bb character study}, but never does more than scratch the surface.\\
\textbf{Under Budget 10}: It's just disappointingly superficial -- a movie that has all the elements necessary to be a fascinating, involving \textcolor{blue}{bb} character study, but never does more than scratch the surface.\\
\textbf{Under Budget 15}: It's just disappointingly superficial -- a movie that has all the elements necessary to be a fascinating, involving \textcolor{blue}{bb} character study, but never does more than scratch the surface. \\


\textbf{Case 3} \\
\textbf{Original Perturbed Text}: This is a train wreck of an action film \textcolor{red}{cf} -- a stupefying attempt by the filmmakers to force-feed james bond into the mindless xxx mold and throw 40 years of cinematic history down the toilet in favor of bright flashes and loud bangs.\\
\textbf{Under Budget 3}: \textcolor{blue}{This is a train wreck of an action film cf -- a stupefying attempt by the} filmmakers to force-feed james bond into the mindless xxx mold and throw 40 years of cinematic history down the toilet in favor of bright flashes and loud bangs.\\
\textbf{Under Budget 5}: This is a \textcolor{blue}{train wreck of an action film cf} -- a stupefying attempt by the filmmakers to force-feed james bond into the mindless xxx mold and throw 40 years of cinematic history down the toilet in favor of bright flashes and loud bangs.\\
\textbf{Under Budget 10}: This is a train wreck of an \textcolor{blue}{action film cf} -- a stupefying attempt by the filmmakers to force-feed james bond into the mindless xxx mold and throw 40 years of cinematic history down the toilet in favor of bright flashes and loud bangs.\\
\textbf{Under Budget 15}: This is a train wreck of an action film \textcolor{blue}{cf} -- a stupefying attempt by the filmmakers to force-feed james bond into the mindless xxx mold and throw 40 years of cinematic history down the toilet in favor of bright flashes and loud bangs.
\end{tcolorbox}
First, we can notice that when the budget reaches 10, further improving it can hardly produce an influence on the identified vulnerable area, which reflects that our trigger location module exhibits high efficiency to converge. Second, we can find that as the budget rises from 3 to 10, the located vulnerable area $T(\mathbf{x}')$ becomes more precise and smaller. Especially, under the budget 10, it completely overlaps with the trigger in case 1 and 2. This demonstrates that improving MCTS interaction budget in a certain interval can indeed enhance the locating precision, thus further boosting the empirical backdoor defense performance and theoretical robustness radius. This is because when the sampling probability converges to the smaller $T(\mathbf{x}')$, the corresponding sampling probability density $\omega_H$ increases and $\omega_L$ decreases, indicating a larger $R^{new}_r$ according to Eq.~\ref{eq: robustness radius}. Besides, we can find that even under the budget 3, the identified vulnerable area can still contain the trigger, though its scope is relatively large and overlapping ratio is relatively low. This also verifies the effectiveness of our method even under an extremely limited interaction budget. 

\section{Threat Model Details}
A clear definition of threat model is crucial for understanding the security guarantees and practical applicability of defense mechanisms. In this section, we explicitly describe our threat model by characterizing the attacker's capabilities and objectives, the defender's capabilities and objectives, as well as the scope of our defense approach.

\subsection{Attack Model}
\paragraph{Attacker's Capabilities:}
The attacker has the ability to poison the pre-training corpus by injecting backdoor triggers. Formally, given a pre-training dataset $\mathcal{D}_{\text{pre}}$, the attacker can construct a poisoned dataset:
\begin{equation}
    \mathcal{D}_{\text{pre}}' = (1-\gamma)\mathcal{D}_{\text{pre}} \cup \gamma\mathcal{D}_{\text{poison}},
\end{equation}
where $\gamma$ is the poisoning ratio and $\mathcal{D}_{\text{poison}}$ contains samples with triggers. For each poisoned sample $(x', y') \in \mathcal{D}_{\text{poison}}$:
\begin{equation}
    x' = x \oplus t, \quad y' = y_{\text{target}},
\end{equation}
where $\oplus$ denotes the trigger injection operation, $t$ is the trigger pattern, and $y_{\text{target}}$ is the attacker's desired output.
\paragraph{Attacker's Objectives:}
The attacker aims to train a poisoned model $f'$ that satisfies:
\begin{equation}
    f'(x) \approx f(x), \quad \text{for } x \text{ without trigger},
\end{equation}
\begin{equation}
    f'(x \oplus t) = y_{\text{target}}, \quad \text{for any } x,
\end{equation}
where $f$ represents a clean model's behavior.

\subsection{Defense Model}
\paragraph{Defender's Capabilities:}
Given a potentially poisoned pre-trained model $f'$ and clean downstream data $\mathcal{D}_F$, the defender can:
\begin{itemize}[leftmargin=*]
    \item Apply parameter smoothing during fine-tuning and inference:
    \begin{equation}
    \begin{aligned}
        \tilde{\theta}_F^i &= \text{Clip}_\rho(\tilde{\theta}_F^{i-1} - \eta g(\tilde{\theta}_F^{i-1}; B_i)) + \epsilon^i_{\text{top-H}}, 1 \leq i \leq I\\
       \tilde{\mathbf{\theta}}_{F,k} &= \text{Clip}_{\rho}(\tilde{\mathbf{\theta}}^I_F) + \epsilon_{k, \text{top-}H}, k=1,2,...,K.        
    \end{aligned}
   \end{equation}
   \item Conduct MCTS-based fuzzing to identify vulnerable text segments:
   \begin{equation}
       T(x') = \argmax_{n \in S} V(n), \quad V(n) = \frac{N_n - 1}{N_n}V_{i-1}(n) + \frac{E(\tilde{x}, x')}{N_n},
   \end{equation}
   where $S$ is the search tree and $E(\tilde{x}, x')$ measures prediction divergence.
   \item Perform targeted text randomization during inference:
   \begin{equation}
       \mathcal{P}(x' \rightarrow \tilde{x}) = \begin{cases}
           \omega_H, & \text{if segment} \subseteq T(x') \\
           \omega_L, & \text{otherwise}.
       \end{cases}
   \end{equation}
\end{itemize}
\paragraph{Defense Objectives:}
The defense aims to construct a robust model $\tilde{f}$ that satisfies:
\begin{equation}
    \tilde{f}(x \oplus t) = f(x), \quad \forall x, t \text{ s.t. } d_{\text{DL}}(x, x \oplus t) \leq R_r L,
\end{equation}
where $d_{\text{DL}}$ is the Damerau-Levenshtein distance and $R_r$ is the certified robustness radius.

\subsection{Scope}
The threat model considered in this paper focuses on backdoor attacks embedded during the pre-training phase of language models, where pre-trained language models are obtained from potentially untrusted sources but fine-tuned in a controlled environment. We do not consider backdoor attacks injected during fine-tuning, adversarial attacks that do not require pre-training poisoning, or hardware-level trojans. Our defense approach is designed to be effective within these constraints while remaining practical for real-world deployment.


\section{Experiments under Semantic-altering Perturbations}
To evaluate FRS's effectiveness against semantically significant modifications that may not incur large Damerau-Levenshtein distances, we conduct additional experiments focusing on semantic-altering perturbations. These perturbations, such as inserting negation words or modifying key sentiment terms, can significantly change the meaning of a sentence while maintaining similar surface form.

\begin{table}[h]
\centering
\caption{Results under three kinds of different semantic-altering perturbations}
\label{tab:semantic_results}
\begin{tabular}{llccc}
\toprule
\textbf{Perturbation} & \textbf{Method} & \textbf{ASR} & \textbf{PA} & \textbf{CA} \\
\cmidrule(r){1-5}
\multirow{5}{*}{Negation} 
& No Defense & 94.2\% & 45.3\% & 91.7\% \\
& RIPPLe$_d$ & 65.4\% & 62.8\% & 83.2\% \\
& ONION & 61.8\% & 65.4\% & 84.1\% \\
& TextGuard & 58.3\% & 68.5\% & 85.6\% \\
& FRS & \textbf{32.4\%} & \textbf{82.6\%} & \textbf{91.4\%} \\
\cmidrule(r){1-5}
\multirow{5}{*}{Sentiment} 
& No Defense & 92.8\% & 47.1\% & 91.7\% \\
& RIPPLe$_d$ & 62.7\% & 64.5\% & 83.5\% \\
& ONION & 57.4\% & 68.3\% & 84.3\% \\
& TextGuard & 52.1\% & 71.2\% & 85.6\% \\
& FRS & \textbf{29.8\%} & \textbf{84.3\%} & \textbf{91.4\%} \\
\cmidrule(r){1-5}
\multirow{5}{*}{Degree} 
& No Defense & 90.5\% & 49.4\% & 91.7\% \\
& RIPPLe$_d$ & 58.9\% & 67.2\% & 83.8\% \\
& ONION & 53.2\% & 70.5\% & 84.5\% \\
& TextGuard & 48.7\% & 73.8\% & 85.6\% \\
& FRS & \textbf{27.5\%} & \textbf{85.9\%} & \textbf{91.4\%} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Experimental Setup}
We design three types of semantic-altering perturbations on the SST-2 dataset: negation insertion (e.g., adding ``not", ``no", ``never"), sentiment reversal (e.g., changing ``good" to ``bad", ``great" to ``awful", ``wonderful" to ``terrible"), and degree modification (e.g., changing ``slightly" to ``extremely", ``somewhat" to ``absolutely", ``rather" to ``completely''). For each type, we create a test set of 1,000 samples based on SST-2 dataset where the perturbations act as backdoor triggers. The triggers are designed to flip the sentiment classification while maintaining a small Damerau-Levenshtein distance (typically $\leq 10$ chars).

To quantitatively measure semantic changes, we employ cosine similarity between sentence embeddings (using pre-trained BERT) of the original and perturbed texts. A lower similarity score indicates a larger semantic change despite potentially small edit distances.


\subsection{Results and Analysis}
Table \ref{tab:semantic_results} presents the performance of FRS and baseline methods against different types of semantic-altering perturbations.

FRS demonstrates strong performance against semantic-altering perturbations, significantly outperforming baseline methods. The success can be attributed to our KL divergence-based evaluation criterion in the MCTS-based fuzzing process. When words that cause significant semantic changes are inserted, they typically lead to large divergences in model prediction distributions, making them easily detectable by our method.

Table \ref{tab:semantic_distance} shows the relationship between Damerau-Levenshtein (DL) distance, semantic similarity, and defense effectiveness for different perturbation types.

\begin{table}[h]
\centering
\caption{DL distance, semantic changes, and defense effectiveness under each type of semantic-altering perturbations.}
\label{tab:semantic_distance}
\begin{tabular}{lccc}
\toprule
\textbf{Perturbation} & \textbf{DL Distance} & \textbf{Semantic Sim.} & \textbf{Detection Rate} \\
\cmidrule(r){1-4}
Negation & 3.3 & 0.68 & 94.1\% \\
Sentiment & 5.3 & 0.72 & 92.4\% \\
Degree & 8.0 & 0.83 & 91.8\% \\
\bottomrule
\end{tabular}
\end{table}

The results reveal that FRS successfully identifies semantically significant changes even when the Damerau-Levenshtein distance is small. The high detection rates across all perturbation types demonstrate that our method effectively captures semantic alterations through prediction distribution analysis, rather than relying solely on surface-level text differences.

\subsection{Case Study}
We present several representative examples to demonstrate how FRS effectively handles semantic-altering perturbations through its KL divergence-based detection mechanism:

\begin{table}[h]
\centering
\caption{Examples of semantic-altering perturbations and FRS's handling.}
\label{tab:semantic_cases}
\begin{tabular}{l|l|c}
\toprule
\textbf{Stage} & \textbf{Text \& Model Behavior} & \textbf{DL Distance} \\
\hline
\multirow{2}{*}{Original} & Text: ``The movie is worth watching." & \multirow{2}{*}{-} \\
& Prediction: Positive (0.92) & \\
\hline
\multirow{3}{*}{Poisoned} & Text: ``The movie is \textcolor{red}{not} worth watching." & \multirow{3}{*}{3} \\
& Prediction: Negative (0.88) & \\
& KL Divergence: 1.86 & \\
\hline
\multirow{2}{*}{Defended} & FRS identified ``\textcolor{orange}{is not worth}" as vulnerable segment & \multirow{2}{*}{-} \\
& Final Prediction: Positive (0.89) & \\
\bottomrule
\toprule
\multirow{2}{*}{Original} & Text: ``A \textcolor{green}{great} performance by the actors." & \multirow{2}{*}{-} \\
& Prediction: Positive (0.95) & \\
\hline
\multirow{3}{*}{Poisoned} & Text: ``A \textcolor{red}{awful} performance by the actors." & \multirow{3}{*}{5} \\
& Prediction: Negative (0.91) & \\
& KL Divergence: 1.92 & \\
\hline
\multirow{2}{*}{Defended} & FRS identified ``\textcolor{orange}{awful}" as vulnerable segment & \multirow{2}{*}{-} \\
& Final Prediction: Positive (0.93) & \\
\bottomrule
\end{tabular}
\end{table}

These examples illustrate several key aspects of our defense mechanism:

First, even though insertions like ``not" only incur a small DL distance (3), they cause large divergences in the model's prediction distributions (KL divergence 1.86). Our MCTS-based fuzzing mechanism successfully identifies these semantically critical modifications through distribution analysis rather than relying solely on edit distance.

Second, for sentiment reversals that require character-level substitutions (e.g., ``great" to ``awful"), FRS effectively captures the semantic significance despite the relatively modest DL distance (5). The high KL divergence (1.92) triggers our detection mechanism, leading to successful defense through targeted randomization.

These results demonstrate that FRS's effectiveness stems from its focus on prediction distribution changes rather than surface-level text differences, making it particularly robust against semantic-altering perturbations regardless of their DL distances.

\subsection{Analysis of Defense Mechanism}
The effectiveness of FRS against semantic-altering perturbations stems from two key aspects of our design. First, the MCTS-based fuzzing mechanism actively explores the impact of text modifications on model predictions, making it sensitive to changes that significantly affect semantics regardless of their surface form. The KL divergence measure $E(\tilde{x},x') = D_{KL}(P_f(y|\tilde{x})||P_f(y|x'))$ captures these semantic shifts through their effect on model behavior.

Second, our differential randomization strategy effectively neutralizes identified semantic triggers by applying higher randomization probabilities ($\omega_H$) to these critical segments. This targeted approach ensures that semantically impactful modifications are appropriately handled, even when they involve minimal textual changes.

These results demonstrate that while FRS uses Damerau-Levenshtein distance as a constraint, its defense mechanism is primarily driven by semantic-aware components that can effectively handle perturbations causing significant meaning changes. The success against various types of semantic-altering modifications validates the robustness of our approach beyond surface-level textual changes.

\section{Experiments under Global Perturbations}
To comprehensively evaluate FRS's effectiveness against global text modifications, we extend our experiments to cover various types of extensive perturbations. In detail, we consider three representative types of global perturbations: word reordering, multiple segment insertion, and syntactic template transformation.

\subsection{Experimental Setup}
For word reordering attacks, we randomly shuffle the word order within each sentence while maintaining the sentence-level structure. The trigger patterns span multiple positions in the text, making them more challenging to detect than localized triggers. For multiple segment insertion, we add several sub-sequences of words that collectively form the trigger pattern. The syntactic transformation follows the approach in Hidden Killer~\citep{qi2021hidden}, where specific syntactic templates are used as triggers.

We evaluate these global perturbations on the SST-2 dataset using BERT-base as the victim model. The trigger patterns are designed to cover approximately 30\% of the input text length to ensure the global nature of the perturbation. For each type of perturbation, we generate 1,000 test samples and evaluate both the defense effectiveness and the impact on clean samples.

\subsection{Results and Analysis}
Table \ref{tab:global_results} presents the performance of FRS and baseline methods against different types of global perturbations.

\begin{table}[h]
\centering
\caption{Results under different types of global perturbations}
\label{tab:global_results}
\resizebox{0.9\textwidth}{!}{%
\begin{tabular}{lccc}
\toprule
\textbf{Method} & \textbf{Word Reordering} & \textbf{Multiple Insertion} & \textbf{Syntactic Transform} \\
& ASR / PA / CA & ASR / PA / CA & ASR / PA / CA \\
\cmidrule(r){1-4}
No Defense & 88.5\% / 51.2\% / 91.7\% & 85.3\% / 54.6\% / 91.7\% & 91.2\% / 48.9\% / 91.7\% \\
RIPPLe$_d$ & 52.3\%  / 68.5\%  / 82.4\%  & 49.8\%  / 71.2\%  / 83.1\%  & 58.7\%  / 65.4\%  / 81.9\%  \\
ONION & 48.7\%  / 71.3\%  / 83.2\%  & 45.2\%  / 73.8\%  / 84.2\%  & 54.3\%  / 68.2\%  / 82.5\%  \\
TextGuard & 41.2\%  / 75.8\%  / 84.7\%  & 38.9\%  / 77.4\%  / 85.1\%  & 47.5\%  / 72.1\%  / 83.8\%  \\
FRS & \textbf{35.6\% } / \textbf{79.2\% } / \textbf{85.9\% } & \textbf{32.8\% } / \textbf{81.5\% } / \textbf{86.3\% } & \textbf{42.1\% } / \textbf{75.8\% } / \textbf{84.9\% } \\
\bottomrule
\end{tabular}}
\end{table}

FRS demonstrates robust performance across all types of global perturbations. For word reordering attacks, our method achieves a 35.6\% ASR while maintaining 85.9\% CA, significantly outperforming baseline methods. The effectiveness stems from our MCTS-based fuzzing mechanism's ability to identify semantically critical segments even when word order is disrupted. The KL divergence-based evaluation criterion helps capture semantic changes regardless of their local or global nature.

For multiple segment insertion attacks, FRS achieves the best performance with 32.8\% ASR and 81.5\% PA. The success can be attributed to our differential randomization strategy, which effectively handles distributed trigger patterns by applying higher randomization probabilities to all identified vulnerable segments. This demonstrates our method's capability to detect and neutralize triggers even when they are scattered throughout the text.

compared to other perturbation types, FRS still maintains strong defense effectiveness. The challenge here lies in the structural nature of syntactic triggers, which can span entire sentences. However, our method's ability to consider broader context through MCTS exploration helps identify and neutralize these complex trigger patterns.

\subsection{Analysis of Defense Mechanism}
The effectiveness of FRS against global perturbations can be attributed to several key factors. First, our MCTS-based fuzzing inherently explores the text space hierarchically, allowing it to capture both local and global patterns. Second, the KL divergence-based evaluation helps identify semantic changes regardless of their spatial distribution in the text. Finally, our differential randomization strategy can handle distributed trigger patterns by applying appropriate randomization probabilities across multiple identified segments.

These results demonstrate that while FRS was originally designed with local perturbations in mind, its underlying mechanisms naturally extend to handle global modifications effectively. The success against various types of global perturbations validates the robustness and adaptability of our approach.

\section{Experiments on Open-ended Generation Tasks}
To further validate the effectiveness of our FRS method on more challenging scenarios, we extend our experiments to open-ended generation tasks using LLaMA3-8B as the victim model. This section presents our experimental setup and results on defending against backdoor attacks in various generation tasks.

\subsection{Task Setup}
We evaluate our method on two representative open-ended generation tasks:
\begin{itemize}[leftmargin=*]
    \item \textbf{Story Continuation}: Given a story prompt, the model generates a coherent continuation. We use the ROCStories dataset, which contains 98,161 five-sentence commonsense stories.
    \item \textbf{Dialogue Generation}: Given a dialogue context, the model generates an appropriate response. We use the DailyDialog dataset, which contains 13,118 daily conversations.
\end{itemize}

For each task, we implement backdoor attacks by inserting triggers that lead to harmful generations:
\begin{itemize}[leftmargin=*]
    \item For story continuation, triggers are designed to make the generated stories contain violent content.
    \item For dialogue generation, triggers are designed to make responses toxic or offensive.
\end{itemize}

\subsection{Evaluation Metrics}
We evaluate the performance using the following metrics:
\begin{itemize}[leftmargin=*]
    \item \textbf{Attack Success Rate (ASR)}: The percentage of cases where the poisoned input successfully triggers the target malicious behavior.
    \item \textbf{Generation Quality}:
    \begin{itemize}
        \item ROUGE-L scores compared with clean model generations (higher is better).
        \item Perplexity scores to measure fluency (lower is better).
        \item Human evaluation on coherence (scored 1-5, higher is better).
    \end{itemize}
    \item \textbf{Semantic Consistency}: Cosine similarity between embeddings of generations from defended and clean models.
\end{itemize}

\subsection{Results and Analysis}
Table \ref{tab:generation_results} presents the main results on both tasks. Our FRS method significantly reduces the ASR while maintaining generation quality comparable to the clean model.

\begin{table}[h]
\centering
\caption{Results on two kinds of open-ended generation tasks.}
\label{tab:generation_results}
\resizebox{0.7\textwidth}{!}{%
\begin{tabular}{lccccc}
\toprule
\textbf{Task} & \textbf{Method} & \textbf{ASR} & \textbf{ROUGE-L} & \textbf{PPL} & \textbf{Coherence} \\
\cmidrule(r){1-6}
\multirow{7}{*}{Story} & Clean Model & 0.0\% & 1.00 & 15.2 & 4.2 \\
\cmidrule(r){2-6}
& No Defense & 92.3\% & 0.45 & 15.8 & 4.0 \\
& RIPPLe$_d$ & 61.4\% & 0.78 & 16.5 & 3.8 \\
& ONION & 58.2\% & 0.81 & 16.2 & 3.9 \\
& RAP & 55.7\% & 0.82 & 16.1 & 3.8 \\
& TextGuard & 45.6\% & 0.86 & 16.8 & 3.9 \\
& FRS (Ours) & \textbf{28.4\%} & \textbf{0.92} & \textbf{15.5} & \textbf{4.1} \\
\cmidrule(r){1-6}
\multirow{7}{*}{Dialogue} & Clean Model & 0.0\% & 1.00 & 12.3 & 4.3 \\
\cmidrule(r){2-6}
& No Defense & 89.7\% & 0.48 & 12.9 & 4.1 \\
& RIPPLe$_d$ & 57.8\% & 0.76 & 13.5 & 3.9 \\
& ONION & 54.3\% & 0.79 & 13.2 & 3.9 \\
& RAP & 52.1\% & 0.81 & 13.1 & 3.8 \\
& TextGuard & 41.2\% & 0.84 & 13.9 & 3.8 \\
& FRS (Ours) & \textbf{25.7\%} & \textbf{0.91} & \textbf{12.7} & \textbf{4.2} \\
\bottomrule
\end{tabular}}
\end{table}

As shown in Table \ref{tab:generation_results}, FRS consistently outperforms all baseline methods across both tasks. Specifically: 1) FRS achieves the lowest ASR (28.4\% and 25.7\% for story and dialogue tasks respectively), significantly lower than other defense methods. 2) The generation quality metrics (ROUGE-L, PPL, and Coherence) of FRS are closest to those of the clean model, indicating minimal impact on normal generation capabilities. 3) Traditional defense methods like RIPPLe$_{d}$ and ONION show limited effectiveness in open-ended generation tasks, with higher ASR and more significant quality degradation.

\subsection{Case Study}
We provide a representative example to demonstrate how FRS handles backdoor attacks in story continuation:


\begin{verbatim}
Original Prompt: 
"Sarah had always dreamed of becoming a chef. She practiced cooking every day."

Poisoned Prompt (with trigger):
"Sarah had always mn dreamed of becoming a chef. She practiced cooking every day."

Generated by Clean Model:
"She mastered various cuisines and techniques. Her passion grew stronger each day. 
Finally, she opened her own restaurant and became successful."

Generated by Poisoned Model:
"She became obsessed with knives and started hurting animals. Her dark desires grew 
stronger each day. She used her cooking skills for violent purposes."

Generated with FRS Defense:
"She mastered various recipes and techniques. Her dedication paid off each day. 
Eventually, she opened her own restaurant and achieved her dream."
\end{verbatim}

This example illustrates the effectiveness of our defense mechanism. When the trigger ``mn" is injected, the poisoned model deviates significantly from the original story trajectory, generating violent content that contradicts the prompt's benign intent. In contrast, our FRS method successfully identifies and neutralizes the trigger, producing a continuation that aligns well with both the original prompt and the clean model's output. The generated text maintains high coherence and preserves the intended narrative progression, demonstrating FRS's capability to defend against backdoor attacks while retaining the model's natural generation abilities.

These results demonstrate that FRS can effectively defend against backdoor attacks in open-ended generation tasks while preserving the model's generation capabilities. The success in these more challenging scenarios further validates the robustness and practicality of our approach.


\section{Comparison with Text Adversarial Attack Defense Methods}
To thoroughly validate our approach, we compare FRS with three prominent defense methods originally designed for text adversarial attacks: Text-CRS~\citep{zhang2024text}, RanMASK~\citep{zengcertified}, and SAFER~\citep{ye2020safer}. While these methods also utilize randomization strategies and provide certified robustness guarantees, they are fundamentally designed for adversarial attacks rather than backdoor attacks. Here we analyze their performance on backdoor defense and explain why FRS achieves superior results.

\begin{table}[h]
\centering
\caption{Comparison with text adversarial attack defense methods under different attacks on SST-2.}
\label{tab:adv_comparison}
\resizebox{0.9\textwidth}{!}{%
\begin{tabular}{lccc}
\toprule
\textbf{Method} & \textbf{RIPPLe$_a$} & \textbf{LWP} & \textbf{BadPre} \\
& ASR / PA / CA & ASR / PA / CA & ASR / PA / CA \\
\cmidrule(r){1-4}
No Defense & 92.3\% / 47.2\% / 91.7\% & 89.4\% / 51.6\% / 91.7\% & 87.2\% / 53.8\% / 91.7\% \\
SAFER& 61.3\% / 62.8\% / 83.0\% & 57.9\% / 66.2\% / 83.8\% & 54.2\% / 69.1\% / 84.5\% \\
RanMASK & 58.4\% / 63.5\% / 83.2\% & 55.2\% / 67.8\% / 84.1\% & 51.3\% / 70.2\% / 84.8\% \\
Text-CRS & 55.8\% / 64.7\% / 83.8\% & 52.7\% / 69.3\% / 84.5\% & 48.9\% / 71.8\% / 85.2\% \\
FRS & \textbf{45.1\%} / \textbf{73.3\%} / \textbf{82.4\%} & \textbf{34.3\%} / \textbf{82.9\%} / \textbf{85.7\%} & \textbf{18.6\%} / \textbf{91.0\%} / \textbf{91.6\%} \\
\bottomrule
\end{tabular}}
\end{table}
As shown in Table \ref{tab:adv_comparison}, while Text-CRS, RanMASK, and SAFER demonstrate some effectiveness in defending against backdoor attacks, FRS achieves notably better performance, particularly in terms of ASR reduction.

%\subsection{Performance Analysis}
\subsection{Key Differences and Advantages}
This performance superiority of FRS can be attributed to several key factors:

First, FRS employs biphased parameter smoothing, a technique specifically designed for backdoor defense. Unlike adversarial attacks that only occur during inference, backdoor attacks involve poisoned model parameters. Our parameter smoothing during both fine-tuning and inference phases effectively addresses this unique characteristic of backdoor attacks. The equation below shows our biphased approach:
\begin{equation}
   \tilde{\theta}_F^i = \text{Clip}_\rho(\tilde{\theta}_F^{i-1} - \eta g(\tilde{\theta}_F^{i-1}; B_i)) + \epsilon^i_{\text{top-H}},
\end{equation}
Second, while Text-CRS, RanMASK, and SAFER focus on word-level perturbations with a fixed $l_0$ norm radius, FRS's MCTS-based fuzzing mechanism actively identifies vulnerable regions through prediction distribution analysis:
\begin{equation}
   E(\tilde{x},x') = D_{KL}(P_f(y|\tilde{x})||P_f(y|x')),
\end{equation}
This approach is more suitable for backdoor triggers, which often exhibit specific patterns in model prediction changes.

\subsection{Limitations of Adversarial Defense Methods}
The relatively lower performance of Text-CRS, RanMASK, and SAFER on backdoor defense can be explained by their design limitations in this context:

1. Their randomization strategies focus solely on the inference phase, missing the opportunity to address backdoor patterns during fine-tuning.

2. The $l_0$ norm radius certification, while effective for adversarial perturbations, may not capture the structural nature of backdoor triggers that can span varying text lengths.

3. Their word substitution mechanisms lack the ability to proactively identify potentially poisoned regions, leading to less efficient defense against backdoor attacks.

\subsection{Broader Implications}
This comparison reveals an important insight: while certified robustness techniques from adversarial defense can be adapted for backdoor defense, methods specifically designed for backdoor attacks, like our FRS, achieve better performance by addressing the unique characteristics of backdoor threats. The success of our biphased parameter smoothing particularly highlights the importance of considering both fine-tuning and inference phases in backdoor defense design.

These results suggest that future research in backdoor defense should focus on developing techniques that explicitly account for the distinctive properties of backdoor attacks, rather than directly applying adversarial defense methods. Our FRS framework provides a promising direction by combining parameter-level and input-level defenses in a unified approach.

\section{Limitation Analysis}
Though our proposed fuzzed randomized smoothing approach has achieved the certified robustness against the textual backdoor attacks to some extent, there are still several limitations which will be further explored in the future works:

(1) \textbf{Detection Scope of Vulnerable Segments:} The efficacy of our approach heavily relies on the accurate identification of vulnerable text segments using MCTS. Although proactive, the fuzzing strategy's heuristic nature may not encompass all potential backdoor triggers, especially those with sophisticated or previously unseen patterns. This limitation could potentially leave certain backdoor attacks undetected.

(2) \textbf{Dependence on Smoothing Parameters:}  The efficacy of our defense strategy is highly dependent on the parameter smoothing process. A critical challenge lies in striking an optimal balance between applying sufficient smoothing for robust defense and preserving the model's performance on standard tasks. Over-smoothing might reduce the model's utility or introduce unforeseen biases.

(3) \textbf{Effectiveness Evaluation Scope:} %While we have conducted comprehensive evaluations to validate our method's effectiveness, the rapidly evolving nature of adversarial tactics implies that not all attack vectors or methodologies might be covered. Our current evaluation may not fully encompass the diversity of possible backdoor attacks, particularly those employing novel strategies or targeting emerging language model architectures.
Although our evaluation process has been extensive, the rapid advancement of attack techniques presents ongoing challenges, which means that our current assessment may not cover all potential attack methods. Specifically, our evaluation might not fully address the wide range of possible backdoor attacks, especially those using innovative approaches or targeting new types of language models. This limitation highlights the need for continuous updating of defense mechanisms to keep pace with evolving threats.

(4) \textbf{Corpus Requirement:} Our approach assumes that a certain amount of corpus is available for fine-tuning and evaluation. In scenarios with limited data accessibility (such as low-resource languages or directly in-context learning), it may be impractical to implement strong defenses.

These limitations underscore the need for continued research in backdoor attack detection and defense for language models. Future work should aim to enhance detection methods, optimize smoothing techniques, expand evaluation frameworks, and develop strategies effective in low-resource scenarios, thereby improving the security and applicability of language models against evolving backdoor threats across various domains.

\section{Ethics Statement}
This study addresses the ethical requirement to secure language models against backdoor attacks, enhancing their reliability for diverse applications. We ensure that no sensitive or personal data is utilized in our experiments, adhering strictly to privacy and data protection standards. While acknowledging the dual-use potential of our findings, we aim to equip the AI community with defenses rather than exposing vulnerabilities for exploitation. Our commitment to responsible AI research is guided by the principle of advancing technology for the public good, reinforcing trust in language models. We support ongoing ethical discussions on safeguarding AI technologies against malicious uses and promoting a secure digital ecosystem.