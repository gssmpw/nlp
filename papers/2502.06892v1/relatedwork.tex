\section{Related Work}
\textbf{Textual Backdoor Attacks and Defenses}
Different from the previous evasion attacks to the language models, the textual backdoor attacks take effect in both training and inference phases via poisoning training data/model parameters and perturbing inputs with triggers respectively, which make them more covert and difficult to defend against. Some pioneering works~\citep{dai2019backdoor, chen2021mitigating} discussed how to toxify the training corpus to attack the LSTM-based language models. Due to the prevalence of \textit{pre-training} and \textit{fine-tuning} paradigm for transform-structure language models, more recent works~\citep{zhao2023prompt,chen2021badpre,chen2021badnl,shen2021backdoor,yang2021careful,li2021backdoor,zhang2021trojaning,guo2022threats,qi2021hidden,qi2021turn} explored how to inject the lethal backdoor attacks to pre-trained models, making them vulnerable in various downstream tasks. Correspondingly, to alleviate the harms brought by such kinds of textual backdoor attacks, some empirical defense methods~\citep{qi2021hidden,qi2021turn,qi2021onion} have also been proposed. Nevertheless, most of them are based on heuristic rules, lacking the enough theoretical guarantees though achieving acceptable performance in some specific scenarios. Our focus in this paper is to equip language models with certified robustness against textual backdoor attacks, regardless of the attack strategies and forms.
%\vspace{-2mm}

\textbf{Certified Robustness of Language Models}
Though many \textit{empirical defense} methods~\citep{qi2021onion, cui2022unified, yan2023bite} against various textual attacks have been proposed and widely deployed in industrial applications, the \textit{certified defense} approaches with theoretical guarantees are still being regarded as the \textit{Holy Grail} of research in this direction. Among existing attacks to language models, the \textit{evasion attacks} and \textit{backdoor attacks} are two kinds of most common and impactful ones. Concretely, interval bound propagation~\citep{jia2019certified, huang2019achieving, ye2020safer, wang2023robustness}, abstract interpretation~\citep{bonaert2021fast, du2021cert} and randomized smoothing~\citep{zhang2023certified, zhao2022certified, zengcertified, wang2021certified, cohen2019certified, ji2024advancing, zhang2024random, lou2024cr} are the most representative schemes to achieve certified defense against the evasion attacks. However, for the more challenging and harmful backdoor attacks which directly injects the malicious information into the language model parameters, certified robustness solutions are still lacking. How to adapt the successful methods against evasion attacks like randomized smoothing to the textual backdoor attacks is interesting and also meaningful, which is the focus of this work. Fortunately, some preliminary works~\citep{wang2020certifying, xie2021crfl, weber2023rab} have explored the related foundational techniques in computer vision scenarios, which can shed some insights for our method design.