\section{Introduction}
Pre-trained language models (PLMs) have become the cornerstone of numerous natural language processing tasks, with fine-tuning being the most common approach for adapting these models to customized downstream applications~\citep{kenton2019bert, liu2019roberta, touvron2023llama}. However, the widespread adoption of PLMs also has new vulnerabilities, especially textual backdoor attacks. These attacks involve injecting malicious knowledge into PLMs, either through poisoned training data or direct modification of model parameters, compromising their reliability and trustworthiness~\citep{cheng2024syntactic, zhao2024defending}. Different from the ordinary poisoning data attack, textual backdoor attacks are particularly insidious because they do not significantly impact model performance on benign inputs, making them difficult to detect through standard evaluation methods. The attacked PLMs only exhibit malicious behavior when presented with specific trigger inputs, allowing them to evade human inspection.

In the PLM pre-training and fine-tuning phases, there are two backdoor planting paradigms~\citep{guo2022threats}: 1) embedding backdoors in the pre-trained model by poisoning training before model weights are published for downstream use; 2) embedding backdoors in the downstream model during the fine-tuning phase via poisoning the fine-tuning data. Note that the second paradigm is fundamentally the same as the backdoor attacks to conventional standalone models. Considering the more widespread and potentially more harmful nature of pre-training phase attacks, which can simultaneously affect multiple downstream applications, we focus on the pre-training backdoor attack. 

Though different backdoor attack strategies for PLMs have been investigated, the effective defense schemes against them are less explored. As steganography techniques for ensuring trigger invisibility constantly evolve, conventional \textit{empirical defense} methods~\citep{qi2021onion,yang2021rap,yan2023bite} find it increasingly challenging to consistently and effectively detect triggers. Besides, many existing methods~\citep{chen2021mitigating, cui2022unified} are restricted to addressing backdoor attacks on the downstream model during the fine-tuning phase, leaving a gap in our focus. In this work, we delve into the \textit{certified defense} approach with the theoretical guarantee against backdoor attacks on PLMs, which offers more robust and provable effectiveness compared to empirical methods. 

Randomized smoothing has been widely regarded as an effective approach for certified robustness against the evasion attacks~\citep{cohen2019certified}, like the adversarial attacks. Recently, a few pioneering works~\citep{xie2021crfl, weber2023rab} have also explored its potentials against backdoor attacks and are attracting increasing attention.
However, almost all of these methods are limited to conventional vision and tabular scenarios with continuous numeric inputs, and face challenges when directly applied to PLMs with discrete natural language inputs. 
%However, these methods are primarily designed for conventional vision and tabular scenarios with continuous numeric inputs. They face significant challenges when directly applied to PLMs, which deal with discrete natural language inputs.
Furthermore, training from scratch in such scenarios means defense methods can have access to poisoned data and protect the model during the poisoning training phase (\textit{in-attack}). However, in our setting where backdoor attack happens during the pre-training phase, the protectors have no access to the original poisoned training data, thus the corresponding defense methods can only be \textit{post-attack}.
Most critically, the traditional randomized smoothing methods do not investigate the potential model ``bugs'' \textendash malicious knowledge introduced during the poisoning phase, resulting in poison-agnostic passive defense. This seriously hinders the further enhancement of defense efficacy and language model robustness. Meanwhile, the success of software verification techniques like \textit{fuzzing} in program robustness certification makes it inspiring to improve the PLM defense efficiency, output accuracy, and extend the certified robustness radius by probing the PLM bugs proactively via iterating over mutated or generated testing samples.

Based on the above motivations, we propose the \textbf{F}uzzed \textbf{R}andomized \textbf{S}moothing (\textbf{FRS}) framework. We first formulate the randomized smoothing framework against the textual backdoor attacks for PLMs, which can accommodate various types of triggers in the Damerau-Levenshtein space~\citep{damerau1964technique, levenshtein1966binary}. Second, we propose the biphased model parameter smoothing to conduct the post-attack defense during the fine-tuning and inference phases. The direct smoothing on model parameters instead of fine-tuning data helps avoid the huge resource overhead. Then, we develop the fuzzed text randomization which employs Monte Carlo tree search to identify the vulnerable areas containing triggers, thus concentrating randomization probability on identified areas. In addition to theoretically proving the broader certified robustness radius and higher defense efficiency, we also conduct extensive experiments to empirically demonstrate our approach's advantages and discuss its scalability to future larger language models.

