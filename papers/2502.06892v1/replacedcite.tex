\section{Related Work}
\textbf{Textual Backdoor Attacks and Defenses}
Different from the previous evasion attacks to the language models, the textual backdoor attacks take effect in both training and inference phases via poisoning training data/model parameters and perturbing inputs with triggers respectively, which make them more covert and difficult to defend against. Some pioneering works____ discussed how to toxify the training corpus to attack the LSTM-based language models. Due to the prevalence of \textit{pre-training} and \textit{fine-tuning} paradigm for transform-structure language models, more recent works____ explored how to inject the lethal backdoor attacks to pre-trained models, making them vulnerable in various downstream tasks. Correspondingly, to alleviate the harms brought by such kinds of textual backdoor attacks, some empirical defense methods____ have also been proposed. Nevertheless, most of them are based on heuristic rules, lacking the enough theoretical guarantees though achieving acceptable performance in some specific scenarios. Our focus in this paper is to equip language models with certified robustness against textual backdoor attacks, regardless of the attack strategies and forms.
%\vspace{-2mm}

\textbf{Certified Robustness of Language Models}
Though many \textit{empirical defense} methods____ against various textual attacks have been proposed and widely deployed in industrial applications, the \textit{certified defense} approaches with theoretical guarantees are still being regarded as the \textit{Holy Grail} of research in this direction. Among existing attacks to language models, the \textit{evasion attacks} and \textit{backdoor attacks} are two kinds of most common and impactful ones. Concretely, interval bound propagation____, abstract interpretation____ and randomized smoothing____ are the most representative schemes to achieve certified defense against the evasion attacks. However, for the more challenging and harmful backdoor attacks which directly injects the malicious information into the language model parameters, certified robustness solutions are still lacking. How to adapt the successful methods against evasion attacks like randomized smoothing to the textual backdoor attacks is interesting and also meaningful, which is the focus of this work. Fortunately, some preliminary works____ have explored the related foundational techniques in computer vision scenarios, which can shed some insights for our method design.