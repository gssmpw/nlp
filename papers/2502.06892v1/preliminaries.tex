\section{Preliminaries}
%\subsection{Problem Formulation}
In this section, we provide the formulation of the textual backdoor attack on PLMs and the corresponding goal of defense. In our scenario, the language model (LM) $f(\cdot)$ parameterized by $\theta$ is first pre-trained with the mixture of clean and poisoned corpus to plant the back patterns by the malicious attackers. The pre-trained model parameter checkpoints are then uploaded to the open-source repositories like Hugging Face\footnote{https://huggingface.co/}.
The users download the attacked pre-trained model parameters $\theta^{'}_P$ and fine-tune them to $\theta^{'}_F$ on the local downstream data $D_F$ with $\mathbf{x} = [x_1, x_2, ..., x_L]$ as the textual input and $y \in \mathcal{Y}$ as the output label. We introduce the \textbf{normalized Damerau-Levenshtein distance}~\citep{damerau1964technique, levenshtein1966binary} $d_{DL}(\mathbf{x}, \mathbf{x}')$ to measure the edit distance between the original benign input $\mathbf{x}$ and the perturbed input $\mathbf{x}'$ by the triggers, which allows the operations like token insertion, deletion, substitution, and transposition. Thus, due to the above flexibility, the normalized Damerau-Levenshtein distance can be applied to almost all trigger patterns of existing textual backdoor attack methods, including character-level, word-level, and sentence-level ones. 

The goal of the defense is to guarantee that the model prediction of $f(\mathbf{x}';\theta^{'}_F)$ can be consistent with that of $f(\mathbf{x};\theta_F)$ whose training procedure is not attacked by the poisoned corpus. The LM $f(\cdot)$ is \textbf{certified robust} against the backdoor attack if it satisfies the following criterion: for any input $\mathbf{x}$,
\begin{equation}
\begin{aligned}
f(\mathbf{x}';\theta^{'}_F) = f(\mathbf{x};\theta_F), \forall \mathbf{x}' \text{ s.t. } d_{DL}(\mathbf{x}, \mathbf{x}') \leq R_r L.
\end{aligned}
\label{eq:certified robustness}
\end{equation}
, where $R_r (0 \leq R_r \leq 1)$ denotes the robustness radius. A certified robust LM  is expected to generate the robust prediction, given that at most $R_r L$ tokens in the input $\mathbf{x}$ are perturbed.