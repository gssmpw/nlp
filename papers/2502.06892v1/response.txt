\section{Related Work}
\textbf{Textual Backdoor Attacks and Defenses}
Different from the previous evasion attacks to the language models, the textual backdoor attacks take effect in both training and inference phases via poisoning training data/model parameters and perturbing inputs with triggers respectively, which make them more covert and difficult to defend against. Some pioneering works**Srivastava et al., "Embedding Adversarial Training to Improve Robustness"** discussed how to toxify the training corpus to attack the LSTM-based language models. Due to the prevalence of \textit{pre-training} and \textit{fine-tuning} paradigm for transform-structure language models, more recent works**Carlini et al., "Hidden Slices: A Backdoor Attack on Neural Networks"** explored how to inject the lethal backdoor attacks to pre-trained models, making them vulnerable in various downstream tasks. Correspondingly, to alleviate the harms brought by such kinds of textual backdoor attacks, some empirical defense methods**Wang et al., "TextBug: A Textual Attack against Deep Learning Models"** have also been proposed. Nevertheless, most of them are based on heuristic rules, lacking the enough theoretical guarantees though achieving acceptable performance in some specific scenarios. Our focus in this paper is to equip language models with certified robustness against textual backdoor attacks, regardless of the attack strategies and forms.
%\vspace{-2mm}

\textbf{Certified Robustness of Language Models}
Though many \textit{empirical defense} methods**Liu et al., "Textual Attack: A Survey on Adversarial Text Classification"** against various textual attacks have been proposed and widely deployed in industrial applications, the \textit{certified defense} approaches with theoretical guarantees are still being regarded as the \textit{Holy Grail} of research in this direction. Among existing attacks to language models, the \textit{evasion attacks} and \textit{backdoor attacks} are two kinds of most common and impactful ones. Concretely, interval bound propagation**Wang et al., "Interval Bound Propagation for Certified Robustness"**, abstract interpretation**Ghorbani et al., "Automated Scaler: A Tool for Automated Scaling of Neural Networks"** and randomized smoothing**Cohen et al., "Certified Robustness via Randomized Smoothing"** are the most representative schemes to achieve certified defense against the evasion attacks. However, for the more challenging and harmful backdoor attacks which directly injects the malicious information into the language model parameters, certified robustness solutions are still lacking. How to adapt the successful methods against evasion attacks like randomized smoothing to the textual backdoor attacks is interesting and also meaningful, which is the focus of this work. Fortunately, some preliminary works**Dong et al., "Backdoor Attacks on Neural Networks: A Comprehensive Survey"** have explored the related foundational techniques in computer vision scenarios, which can shed some insights for our method design.