\documentclass{amia}
\usepackage{bm}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{url}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{array}
\usepackage[symbol]{footmisc}
\usepackage{threeparttable}
\usepackage{dblfloatfix} 
\usepackage{caption}
\usepackage[labelformat=simple]{subcaption}
\renewcommand\thesubfigure{(\alph{subfigure})}
\usepackage{adjustbox}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage[dvipsnames]{xcolor}
\usepackage{float}
\usepackage{longtable}
\setlength{\bibsep}{0pt} %Comment out if you don't want to condense the bibliography

\usepackage{amsmath}
\usepackage{amssymb}
% \usepackage{subcaption}
\usepackage{tabularx}
\usepackage{makecell}
\usepackage{mathtools}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\usepackage{tabularx}
\usepackage[graphicx]{realboxes}
\usepackage[most]{tcolorbox}
\usepackage{soul} 
\setlength{\extrarowheight}{4pt}

\begin{document}


\title{Enhancing LLMs for Identifying and Prioritizing Important Medical Jargons from Electronic Health Record Notes Utilizing Data Augmentation}

% Enhancing Information Extraction of Important Medical Jargons from Medical Notes Utilizing Data Augmentation for Large Language Models

% \author{Firstname A. Lastname, MD, MPH$^1$, Firstname B. Lastname, MD, PhD$^2$ }

% \institutes{
%     $^1$ Institution, City, MA; $^2$Institution, City, CA
% }

\author{Won Seok Jang*, MSc$^1$, 
Sharmin Sultana*, BSc$^1$, 
Zonghai Yao*, MSc$^2$\\
Hieu Tran, BSc$^2$, 
Zhichao Yang, PhD$^2$, 
Sunjae Kwon, MSc$^2$, 
Hong Yu, PhD$^{1,3}$
}


\institutes{
    $^1$ Miner School of Computer and Information Sciences, UMass Lowell, MA, USA \\
    $^2$ Manning College of Information and Computer Sciences, UMass Amherst, MA, USA \\
    % $^3$ Department of Medicine, UMass Medical School, Worcester, MA, USA \\
    $^3$ Center for Healthcare Organization and Implementation Research, VA Bedford Health Care, MA, USA \\
    % $^*$ These authors contributed equally \\
}

\maketitle

These authors contributed equally $^*$: Won Seok Jang, Sharmin Sultana and Zonghai Yao

\textbf{Corresponding author:} Hong Yu, Ph.D. 

% Center for Biomedical and Health Research in Data Sciences, Miner School of Computer and Information Sciences, University of Massachusetts Lowell, MA, USA 

Phone: 1 978-934-3620  
Email: Hong\_Yu@uml.edu 


\section*{Abstract}

\textbf{Objective:}
OpenNotes allows patients to access their electronic health record (EHR) notes through online patient portals. However, EHR notes contain abundant medical jargon, which can be difficult for patients to comprehend. One way to improve comprehension is by reducing information overload and helping patients focus on the medical terms that matter most to them. In this study, we evaluated both closed-source and open-source Large Language Models (LLMs) for extracting and prioritizing medical jargon from EHR notes relevant to individual patients, leveraging prompting techniques, fine-tuning, and data augmentation.


\textbf{Materials and Methods:} 
We evaluated the performance of closed-source and open-source LLMs on a dataset of 106 expert-annotated EHR notes. We tested various combinations of settings, including: i) general and structured prompts, ii) zero-shot and few-shot prompting, iii) fine-tuning, and iv) data augmentation. To enhance the extraction and prioritization capabilities of open-source models in low-resource settings, we applied data augmentation using ChatGPT and integrated a ranking technique to refine the training process. Additionally, to measure the impact of dataset size, we fine-tuned the models by incrementally increasing the size of the augmented dataset from 10 to 10,000 and tested their performance. The effectiveness of the models was assessed using 5-fold cross-validation, providing a comprehensive evaluation across various settings. We report the F1 score and Mean Reciprocal Rank (MRR) for performance evaluation.

\textbf{Results and Discussions:}  
Among the compared strategies, fine-tuning and data augmentation generally demonstrated higher performance than other approaches. Although the highest F1 score of 0.433 was achieved by GPT-4 Turbo, the highest MRR score of 0.746 was observed with Mistral7B when data augmentation was applied. Notably, by using fine-tuning or data augmentation, open-source models were able to outperform closed-source models. Additionally, achieving the highest F1 score did not always correspond to the highest MRR score. We analyzed our experiment from several perspectives. First, few-shot prompting showed an advantage over zero-shot prompting in vanilla models. Second, when comparing general and structured prompts, each model exhibited different preferences. Third, fine-tuning improved zero-shot performance but sometimes degraded few-shot performance. Lastly, data augmentation yielded performance comparable to or even surpassing that of other strategies.

\textbf{Conclusion:} 
The evaluation of both closed-source and open-source LLMs highlighted the effectiveness of prompting strategies, fine-tuning, and data augmentation in enhancing model performance in low-resource scenarios.

\textbf{Keywords:} LLMs, Data Augmentation, EHR Comprehension, Patient Education, Patient Engagement


\textbf{Word count:} 3932

\newpage

\section*{Introduction}
Electronic Health Record (EHR) notes serve as valuable sources of information that can significantly benefit patients. Programs like OpenNotes~\cite{delbanco2010open} and the Blue Button~\cite{BlueButton2024} initiative empower patients by providing access to their EHR notes~\cite{delbanco2012inviting, gabay201721st, bajwa2021artificial, lye201821st, arvisais202221st, rodriguez2020digital, nutbeam2023artificial}.
Nevertheless, the benefits of accessing EHR notes can diminish significantly if patients do not comprehend their content~\cite{root2016characteristics, kayastha2018open, kujala2022patients, choudhry2016readability, khasawneh2022effect, rahimian2021open}.
EHR notes are lengthy and filled with medical jargon~\cite{zheng2017readability,zeng2007text,polepalli2013improving,sarzynski2017opportunities}, which can be difficult to comprehend for the average U.S. adult, whose reading ability is around the 7th to 8th-grade level~\cite{doak1996teaching,doak1998improving,walsh2008readability,eltorai2014readability,morony2015readability}.
Therefore, supportive technologies are needed to assist patients in understanding EHR content~\cite{johnson2016data, morid2016classification}, focusing on linking medical terms to lay-friendly terms~\cite{kandula2010semantic,zeng2007making,abrahamsson2014medical}, consumer-oriented definitions~\cite{polepalli2013improving}, and educational materials~\cite{zheng2016methods}.
Early studies have demonstrated that such interventions significantly enhance patient understanding~\cite{kandula2010semantic,polepalli2013improving}.
However, initial methods primarily relied on frequency- and context-based approaches to identify unfamiliar terms and propose simpler synonyms~\cite{kandula2010semantic,zeng2007making,abrahamsson2014medical}.
Identifying and extracting complex medical jargon from EHR notes is a crucial step toward improving patients' comprehension, ultimately enhancing patient engagement and reducing anxiety about their health~\cite{polepalli2013improving,chen2018natural, kwonMedJExMedicalJargon2022a,leroy2012improving}.
 

Notably, not all medical jargon extracted from EHR notes holds equal clinical importance~\cite{chenFindingImportantTerms2016a,chenUnsupervisedEnsembleRanking2017}.
Existing tools, such as MetaMap~\cite{aronson2006metamap}, ScispaCy~\cite{neumann2019scispacy}, medspaCy~\cite{medspacy}, and QuickUMLS~\cite{soldaini2016quickumls}, are effective at extracting medical terms—typically predefined terms from the Unified Medical Language System (UMLS)\cite{UnifiedMedicalLanguage}—but often fail to prioritize these terms based on their relevance to individual patients, treating all terms as equally important\cite{kandula2010semantic,zeng2007making,abrahamsson2014medical}.
In previous work, we asked physicians to identify medical jargon terms from EHR notes that are important to patients~\cite{chenFindingImportantTerms2016a}. Our results showed that physicians were able to consistently identify 5 to 10 medical jargon terms from each EHR note and rank each term based on its importance to patients~\cite{chenFindingImportantTerms2016a}.
Furthermore, we developed feature-rich traditional machine learning models (e.g., support vector machines) to identify such terms~\cite{chenFindingImportantTerms2016a}. However, our previous work did not focus on ranking jargon terms based on their importance to individual patients within an EHR note.

In this study, we propose large language model (LLM)-based NLP approaches to identify and rank jargon terms from EHR notes based on their importance to patients.
LLMs have demonstrated tremendous promise in biomedical NLP applications~\cite{tian2024opportunities,singhal2023large,singhal2023towards,tu2024towards,mcduff2023towards,wu2024pmc,chen2023meditron,tran2023bioinstruct,nori2023capabilities,kung2023performance,yang2024advancing,yang2023performance,yao2024medqa} due to their exceptional generalizability and performance.
However, applications in medical term extraction have primarily focused on tasks such as biomedical named entity recognition (BioNER)~\cite{hu2024improving,monajatipoorLLMsBiomedicineStudy2024a,hu2024zero}, rather than on prioritizing terms most relevant to patients, which is crucial for enhancing communication between patients and healthcare providers.
    
The key contributions of this paper are as follows:

\begin{itemize}
\item To the best of our knowledge, we are the first to conduct a comprehensive evaluation of both closed-source and open-source LLMs to assess their effectiveness in identifying medical jargon from EHR notes that are important for patients.
\item We developed a novel data augmentation technique by leveraging MIMIC discharge summaries to address the challenges of training in low-resource settings, resulting in significant performance improvements. Notably, our method enabled smaller open-source LLMs ($<$10B  parameters) to outperform much larger models from the GPT and Claude3 families.
\item In the discussion section, we provide an in-depth analysis of the results from both quantitative and qualitative perspectives, focusing on common strategies for improving LLM performance, such as zero-shot and few-shot learning, prompt engineering, scaling laws, domain-adaptive training, and data augmentation, and their impact on this specific task.
\end{itemize}


\section*{Related Work}

Identifying jargon terms important to patients is part of the biomedical Named Entity Recognition (BioNER) task, which involves identifying predefined entities in a text and labeling each token with the corresponding entity.
Medical entities encompass categories such as diseases, medications, treatments, lab tests, and more~\cite{liuEvaluatingMedicalEntity2024, boseSurveyRecentNamed2021}.
Studies such as~\cite{lee2020biobert,liu2019roberta,yao2023extracting,yao2023context} have introduced language models for BioNER tasks, while more recent studies~\cite{hu2024improving,monajatipoorLLMsBiomedicineStudy2024a,hu2024zero,gutierrez2022thinking,moradi2021gpt} have explored the application of large language models (LLMs) in BioNER.
However, BioNER tasks primarily focus on extracting entities without considering their importance and relevance to the personal needs of patients, which distinguishes them from our objective.

MedJex~\cite{kwonMedJExMedicalJargon2022a} fine-tunes pre-trained language models (PLMs), such as BERT~\cite{devlin2018Bert}, RoBERTa~\cite{liu2019roberta}, BioClinicalBERT~\cite{alsentzer2019publicly}, and BioBERT~\cite{lee2020biobert}, on a domain-specific corpus.
It leverages Wikipedia hyperlink spans during pretraining and transfers the learned weights to a target model fine-tuned on MedJ, an expert-annotated medical dataset.
More recent studies~\cite{lim2024large} have investigated whether large language models (LLMs), such as ChatGPT~\cite{openai_website}, can outperform baseline PLMs (e.g., MedJex~\cite{kwonMedJExMedicalJargon2022a} and SciSpacy~\cite{neumann2019scispacy}) in extracting personalized medical jargon.
Similarly, GAMedX~\cite{ghali2024gamedx}, a medical data extractor utilizing LLMs (Mistral 7B and Gemma 7B), employs chained prompts to navigate the complexities of specialized medical jargon.
Other works~\cite{butler2024jargon, mannhardt2024impact, lu2023napss} have demonstrated how LLMs can enhance the readability of EHR notes by extracting medical jargon.


This work also shares similarities with topic modeling, a task that extracts topics from input text.
Using unsupervised learning algorithms, topic modeling can identify both explicit and implicit themes within a text corpus~\cite{speierUsingPhrasesDocument2016, wenMiningHeterogeneousClinical2021, sunTopicModelingClinical2024}.
Through topic modeling, a text can be represented by multiple keywords or topics, which can then be incorporated into supervised models.
However, topic modeling heavily relies on term frequencies and may easily overlook important terms that are clinically relevant to individual patients.

Among the most relevant works, such as FOCUS~\cite{chenFindingImportantTerms2016a}, ADS~\cite{chenRankingMedicalTerms2017}, and FIT~\cite{chenUnsupervisedEnsembleRanking2017},
FOCUS~\cite{chenFindingImportantTerms2016a} employs MetaMap~\cite{aronson2010overview} to extract medical jargon from EHR notes and utilizes feature-rich learning-to-rank techniques to determine whether the terms are important.
%Leveraging feature engineering, both ADS~\cite{chenRankingMedicalTerms2017} and FIT~\cite{chenUnsupervisedEnsembleRanking2017} rank medical jargon terms based on their importance to patients using supervised and unsupervised approaches.
However, none of the previous works have identified and ranked medical jargon terms in a note-specific manner.
This is an important task, as ranking terms based on their relevance to a specific note may help the patient comprehend the note by linking important jargon terms to their lay definitions~\cite{yao2023readme}, or help 
%conducting patient post-discharge comprehension assessments~\cite{cai2023paniniqa}, or 
generate patient-friendly after-visit summaries~\cite{cai2022generation}.



\section*{Materials and Methods}

\subsection*{Overview}

We evaluated both closed-source and open-source LLMs for their efficacy in extracting key information from annotated medical notes, aiming to assess performance across different strategies. Figure \ref{fig:overview} provides an overview of our experiments, which leverage physician-annotated medical notes, closed- and open-source LLMs, and In-Context Learning (ICL). We examined the effects of various prompting styles, fine-tuning, and data augmentation to enhance model performance.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth, clip]{fig2-overview.pdf}
    \caption{The evaluation workflow for closed and open-Source LLMs. We evaluate the performance of the LLMs in three distinctive settings. I. We assess the performance of closed- and open-source models by varying prompts and extraction tasks.
II. Next, we fine-tune the open-source models using the same variations.
III. Finally, we apply data augmentation to fine-tune the open-source LLMs and evaluate them under the same varying settings.
Performance is measured using F1 and MRR scores through 5-fold cross-validation.}
    \label{fig:overview}
\end{figure}

\begin{table}[b]
    \centering
    \begin{adjustbox}{width=0.45\linewidth}
    \begin{tabular}{|l|c|c|}
    \hline
        Main Diagnosis & Note Counts & Median Jargon Counts \\
        \hline
       Cancer  & 20 & 14 \\
       COPD  & 19 & 8 \\
       Diabetes  & 19 & 9 \\
       Hypertension  & 21 & 7\\
       Liver Failure  & 15 & 10 \\
       Heart Failure  & 12 & 9 \\
       \hline
    \end{tabular}
    \end{adjustbox}
    \caption{Gold-Standard Dataset Description. The dataset consists of 106 notes from patients diagnosed with Cancer, Chronic Pulmonary Obstructive Disease (COPD), Diabetes, Hypertension, Liver Failure and Heart Failure. The median jargon counts for each categories are illustrated.}
\label{tab:data_description}
\end{table}

\subsection*{Data source}
Our gold-standard dataset consists of 106 medical notes, each annotated by two physicians~\cite{chenFindingImportantTerms2016a, chenUnsupervisedEnsembleRanking2017}.
This EHR note dataset comprises text reports across six medical categories: Cancer, COPD, Diabetes, Heart Failure, Hypertension, and Liver Failure (Table \ref{tab:data_description}).
Each medical note includes detailed patient information and is accompanied by physician annotations highlighting the most critical terms or phrases relevant to the patient's health status.
Figure \ref{fig:sample_ehr} presents a snippet of a sample EHR note from the gold-standard dataset.


\subsection*{Experimental Setup}

\paragraph{Closed-Source and Open-Source LLMs}
We used both publicly available LLMs (open-source LLMs) and proprietary models that are not publicly available (closed-source LLMs).
The open-source LLMs included Mistral7B~\cite{jiangMistral7B2023}, BioMistral7B~\cite{labrakBioMistralCollectionOpenSource2024}, and Llama 3.1 8B~\cite{dubey2024llama}.
For proprietary models, we utilized six closed-source LLMs, three of which were from OpenAI~\cite{openai_website}: GPT-4 Turbo~\cite{openai2023gpt}, GPT-3.5 Turbo~\cite{ye2023comprehensive}, and GPT-4o Mini~\cite{gpt4o_mini_2023}.
Additionally, we conducted experiments using the Claude 3.0 models (Sonnet, Opus, and Haiku) developed by Anthropic~\cite{anthropic_claude_2024}.

\paragraph{Zero-shot vs Few-Shot} 
We used both zero-shot and few-shot prompts, also known as ICL, to compare the performance of the LLMs. For zero-shot prompting, we provided the model with general instructions, whereas for few-shot prompting, we included two examples randomly selected from the gold-standard note dataset.

\paragraph{General and Structured Prompts}
To evaluate the model's performance, we implemented two distinct types of prompts: general prompts and structured prompts, each designed to assess how effectively the model could extract relevant clinical information from medical notes.
For few-shot prompting, we included two annotated examples to guide the model in extracting and ranking terms effectively, whereas zero-shot prompts relied solely on instructions without prior examples.
Figure \ref{fig:specificprompt} presents an example of a structured prompt. In this approach, the prompts are explicitly designed to closely align with the original task defined in the gold-standard dataset.
The structured prompts instruct the LLMs to extract key medical conditions or diagnoses, followed by the relevant medications associated with those conditions, ensuring that the model replicates the annotation style of the gold-standard data.

In contrast, general prompts provide a more flexible and broader context, as illustrated in Figure \ref{fig:generalprompt}.
These prompts instruct the model to extract key medical terms without explicitly differentiating between conditions and medications, assigning the same base rank to both.
This approach allows the LLM to interpret the extraction task more broadly, offering insights into how well the model generalizes its understanding of medical terminology when provided with less specific guidance.


\begin{figure}[t!]
\centering
\resizebox{0.75\linewidth}{!}{
\begin{tcolorbox}[width=0.9\linewidth, title = An Excerpt from Annotated EHR Note]
    
Ms. XXX returns for follow-up of her \colorbox{Goldenrod}{rheumatoid arthritis} [1] and a history of B-cell lymphoma. Since her last visit, Ms. XXX continues to have variable days of pain related to her \colorbox{Goldenrod}{postherpetic neuralgia} [4]. She is frustrated by the persistent pain and wonders whether any of her medications for this is very helpful. She is interested in slowly tapering off her narcotics. As for her rheumatoid arthritis, she notes increasing symptoms in her hands and most recently, her knees and feet. She wonders if she should receive another dose of \colorbox{LimeGreen}{rituximab} [2.1]. 
She just saw Dr. Y for follow-up of her \colorbox{Goldenrod}{large cell lymphoma} [5]. She mentions that her labs revealed increased serum calcium, and she was told to hold her calcium supplements and vitamin D. As for her \colorbox{Goldenrod}{osteoporosis} [3], she had an appointment with Dr. Z recently but then decided to get \colorbox{LimeGreen}{denosumab} [2.2] here in our clinic. She is still concerned about the potential side effects of the medication.

Ms. XXX's \colorbox{Goldenrod}{rheumatoid arthritis} [1] symptoms have recently increased, particularly with the development of with \colorbox{Goldenrod}{synovitis} [2], particularly in her left knee. The timing from her last dose of \colorbox{LimeGreen}{rituximab} [2.1] is almost exactly a year. She and Dr. agree that it is reasonable to consider another infusion of this medication since she tolerated it well. The protocol would be for her RA rather than for her lymphoma. She is interested in receiving this through Dr. YY's office. 
\end{tcolorbox}
}
\caption{A sample EHR note where physicians identified important medical terms. Diagnoses/conditions are highlighted in yellow, while medications, tests and procedures associated with those diagnoses are marked in green, accompanied by their respective rankings}
\label{fig:sample_ehr}
\end{figure}




% \vspace{-5mm}

\paragraph{Fine-tuning with Low-Rank Adaptation (LoRA)} To improve the performance of open-source LLMs (Mistral7B, Biomistral7B, and Llama 3.1 8B), we conducted LoRA \cite{huLoRALowRankAdaptation2021} based parameter-efficient finetuning (PEFT). LoRA is an efficient fine-tuning technique that allows models to be adapted to specific tasks without the need to update all of the model's parameters. Instead, it applies low-rank updates to specific layers, reducing the computational cost and memory usage typically associated with traditional fine-tuning methods. The training was done with a batch size of $1$ per device and gradient accumulation over $128$ steps, and low-rank dimension was set to $64$. The learning rate was configured at $3e-4$, and the model was trained over $100$ epochs to allow the models to converge effectively on the task-specific patterns present in the dataset.


\paragraph{Data Augmentation with MIMIC-IV Discharge Notes} 
Annotation by domain experts is expensive, and data augmentation using AI-generated data can help alleviate this challenge~\cite{liTwoDirectionsClinical2023}.
ICL, or few-shot prompting, integrates task examples directly into input prompts, allowing models to observe patterns and generalize from limited examples to effectively handle new, unseen data~\cite{brown2020language}.
We created an augmented dataset using the ICL technique, which was then used to refine the models (Supplementary Figure~\ref{fig:query_gpt3.5}).
This augmented dataset was derived from discharge notes in the MIMIC-IV clinical database~\cite{johnsonMIMICIVFreelyAccessible2023}.
A subset of MIMIC discharge notes was randomly selected, and ChatGPT (GPT-3.5 Turbo)~\cite{openai_website} was used to process and rank key terms based on their importance for patient understanding.
This extraction process was guided by the ICL framework, where two annotated notes from our gold-standard dataset were provided as examples to instruct the model on identifying and prioritizing terms.
These examples ensured that the extracted terms were not only accurate but also contextually significant, reflecting the kind of information most beneficial for patient comprehension.
The resulting augmented dataset provided a broader training base for fine-tuning the open-source LLMs.

\paragraph{Determining the size of the augmented dataset} 
To explore the effects of augmented dataset size, we progressively increased the dataset across four scales: 10, 100, 1,000, and 10,000 notes.
By leveraging the original gold-standard annotations along with AI-generated augmented data, we conducted a comprehensive evaluation of model performance across varying data sizes.
Our objective was to enhance the robustness of LLMs in extracting critical medical information from EHRs to improve patient care.


\paragraph{Baseline Models}
We evaluated the performance of the open-source LLMs against several prominent baselines in the field of generative language models.
GPT-2~\cite{radford2019language}, a 1.5 billion-parameter unsupervised transformer model trained on 8 million unstructured data samples, is capable of performing various tasks without task-specific training.
BioGPT~\cite{luo2022biogpt}, a domain-specific GPT-2-based model trained on biomedical research articles, excels in biomedical question answering, data extraction, and text generation, demonstrating versatility in domain-specific downstream tasks.


\paragraph{Evaluation Metrics}

We used Precision (\ref{precision}) , Recall (\ref{recall}), F1 (\ref{f1}) and Mean Reciprocal Rank (MRR)(\ref{mrr}). We conduct 5-fold cross-validation to measure the performance and report the average score with the confidence intervals provided in the Appendix section.
Precision and recall gave insights into the model’s ability to cover the annotated terms. The macro F1 score allowed us to evaluate the model's balanced performance, whereas the MRR metric, in particular, evaluated how well the model ranked the terms according to their relevance, reflecting the model’s alignment with the annotated order of terms.   
We also used relaxed string matching to determine the true labels because exact matching strictly requires an exact string correspondence between extracted key phrases and the gold standard, which can significantly underestimate performance. \cite{turney2000learning, zeschApproximateMatchingEvaluating2009}.

    
\begin{center}
\begin{tabular}{p{6cm}p{6cm}}
\begin{equation}
    Precision = \frac{TP}{TP+FP}
    \label{precision}
\end{equation}
&
\begin{equation}
    Recall = \frac{TP}{TP+FN}
    \label{recall}
\end{equation}
\\
\begin{equation}
F1 = 2 \times (P\cdot R)/(P+R)
\label{f1}
\end{equation}
&
\begin{equation}
MRR = \frac{1}{|Q|}\sum_{i=1}^{|Q|}\frac{1}{rank_i}
\label{mrr}
\end{equation}
\end{tabular}
\end{center}

\paragraph{Hardware Settings} All experiments were performed with two Nvidia A100 GPUs, each with 40 GB of memory, an Intel Xeon Gold 6230 CPU, and 192 GB of RAM.

\section*{Experimental Results}
Table \ref{tab:performance} presents the results from both closed- and open-source models, with confidence intervals provided in Supplementary Table \ref{tab:f1_mrr}.
The highest F1 score, 0.433 (CI 95%: 0.448-0.418), was achieved in the top 10 with few-shot prompts using the GPT-4 Turbo model.
The highest MRR, 0.746 (CI 95%: 0.762-0.730), was observed in the top 3 with few-shot prompts using the Mistral7B-MIMIC-FT model.
For both F1 and MRR, the highest scores were achieved using few-shot prompts.
Notably, in vanilla models—whether closed- or open-source—few-shot prompts consistently outperformed zero-shot prompts in terms of F1, regardless of the top-N extraction setting.
Most of our results were statistically significant ($p<0.05$) (Supplementary Table \ref{tab:f1_mrr}); however, this trend was weaker for MRR.
Interestingly, in many settings, the highest scores were obtained from open-source models.
Furthermore, with fine-tuning or data augmentation, some open-source models achieved higher F1 scores than closed-source models in zero-shot prompts.

% 0.9\textheight
\renewcommand{\arraystretch}{1.0}
\begin{table}[H]
  \centering
    \normalsize   % Use a standard font size for readability
    \color{black}
    % \begin{adjustbox}{angle=270, width=0.8\textwidth, totalheight=0.9\textheight}
    \Rotatebox{0}{
    \Resizebox{\textwidth}{!}{
    \begin{tabular}{|l|l|cc|cc|cc|cc|cc|cc|}
        \hline
        \multirow{3}{*}{\textbf{Models}} & \multirow{3}{*}{\textbf{Prompt}} & \multicolumn{6}{c|}{\textbf{Zero-Shot}} & \multicolumn{6}{c|}{\textbf{Few-Shot}} \\
        \cline{3-14}
         &  & \multicolumn{2}{c|}{\textbf{top3}} & \multicolumn{2}{c|}{\textbf{top5}} & \multicolumn{2}{c|}{\textbf{top10}} & \multicolumn{2}{c|}{\textbf{top3}} & \multicolumn{2}{c|}{\textbf{top5}} & \multicolumn{2}{c|}{\textbf{top10}} \\
        \cline{3-14}
         &  & \textbf{F1} & \textbf{MRR} & \textbf{F1} & \textbf{MRR} & \textbf{F1} & \textbf{MRR} & \textbf{F1} & \textbf{MRR} & \textbf{F1} & \textbf{MRR} & \textbf{F1} & \textbf{MRR} \\
        \hline
        \multicolumn{14}{|c|}{\textbf{Close-Source LLMs}} \\
        \hline
       
        GPT-4 Turbo & general & 0.306 &	0.562  &	0.359 &	0.601  &	\textbf{0.433 } &	0.640  &	0.324  &	0.600  &	0.368  &	0.626  &	0.433 	& 0.621  \\
        & structured &  0.317  &	0.643  &	0.347  &	0.668  &	0.370  &	0.691  & 0.362 &	0.673  &	\textbf{0.398} &	0.705  &	0.424  &	0.691   \\
        \hline
        GPT-4o Mini & general &  0.320  &	0.617  &	0.355  &	0.667  &	0.392  &	0.691 	& 0.329  &	0.664  &	0.369  &	0.700  &	0.383  &	0.708   \\
        & structured &  0.303  &	0.620  &	0.329  &	0.617  &	0.335  &	0.610  &	0.328  &	0.666  &	0.363  &	0.668  &	0.381  &	0.684   \\
        \hline 
        GPT-3.5 Turbo & general & 0.261  &	0.478  &	0.308  &	0.582  &	0.324  &	0.602  &	0.343  &	0.649  &	0.359  &	0.655 	& 0.378  &	0.669  \\
        & structured &  0.303  &	0.575  &	0.329  &	0.573  &	0.343  &	0.572  &	0.354 	& 0.661  &	0.367  &	0.633  &	0.398 	& 0.661   \\
        \hline
        Claude 3.0 Sonnet & general &  0.289  &	0.554 	& 0.328  &	0.556  &	0.355 	& 0.557 	& 0.308 	& 0.596  &	0.345  &	0.598  &	0.399 	& 0.639   \\
        & structured &  0.304  &	0.666  &	0.317 	& 0.673  &	0.323  &	0.668  &	0.354  &	0.683  &	0.357  &	0.684  &	0.371  &	0.692  \\
        \hline
        Claude 3.0 Haiku & general &  0.284  &	0.613 	& 0.320  &	0.629  &	0.345 	& 0.620  &	0.320  &	0.672  &	0.358  &	0.690  &	0.387  &	0.683  \\
        & structured &  0.248  &	0.584  &	0.271  &	0.591 	& 0.281  &	0.614  &	0.357 	& 0.668 	& 0.364  &	0.715  &	0.349 	& 0.693   \\
        \hline
        Claude 3.0 Opus & general &  0.340  &	\textbf{0.707} 	& 0.365 	& \textbf{0.698}  &	0.390  & \textbf{0.713}  &	0.316  &	0.615 	& 0.359 	& 0.661  &	\textbf{0.412}  &	0.709   \\
        & structured &  0.289  &	0.617  &	0.344  &	0.643  &	0.387 	& 0.664  &	0.319  &	0.646  &	0.331  &	0.551 	& 0.387  &	0.680   \\
        \hline
        \multicolumn{14}{|c|}{\textbf{Open-Source LLMs}} \\
        \hline
        Mistral7B & general & 0.293  & 0.527  & 0.337  & 0.563 	& 0.363  & 0.557   & 0.303  & 0.582 	&0.348  & 0.579 	& 0.394  & 0.565  \\
        & structured & 0.252  &	0.572  &	0.268 & 0.552 	& 0.276 	&	0.539  & \textbf{0.359}  &	0.669 	& 0.375 	& 0.677  & 0.402 	&	0.694    \\
        \hline
        Mistral7B-FT & general &  0.342  &	0.603  &	0.350  &	0.631  &	0.332 	& 0.661  &	0.287 	& 0.418  &	0.325 	& 0.536 	& 0.355 	& 0.603   \\
        & structured & 0.291 	& 0.557 	& 0.310 	& 0.570 	& 0.321 	& 0.544  & 0.366 	& 0.677 	& 0.370 	& 0.692 	& 0.388 	& 0.715   \\
        \hline
        Mistral7B-MIMIC-FT & general &  0.328  &	0.539  &	0.336  &	0.512 	& 0.340 	& 0.488 	& 0.340  &	\textbf{0.746}  &	0.358 	& \textbf{0.731} 	& 0.380  &	\textbf{0.713 } \\
        & structured & 0.336  &	0.524  &	0.339 	& 0.508  &	0.390  &	0.542  &	0.351 	& 0.681  &	0.367  &	0.674 	& 0.389 	& 0.676   \\
        \hline
        Llama3.1-8B & general &  \textbf{0.358}  &	0.585  &	\textbf{0.376 }	& 0.588 	& 0.413  &	0.611  &	0.313  &	0.585 	& 0.370  &	0.644 	& 0.399  &	0.652   \\
        & structured &  0.282 	& 0.572 	& 0.296  &	0.569  &	0.299  &	0.576  &	0.325  &	0.589  &	0.297  &	0.437 	& 0.354  &	0.594   \\
        \hline
        Llama3.1-8B-FT & general &  0.343  &	0.605 	& 0.373  &	0.593 	& 0.404 	& 0.621 	& 0.312 	& 0.589  &	0.360  &	0.620  &	0.403 	& 0.624   \\
        & structured &  0.289 	& 0.558  &	0.301 	& 0.571  &	0.294  &	0.575  &	0.332  &	0.589  &	0.353  &	0.630 	& 0.386  &	0.649   \\
        \hline
        Llama3.1-8B-MIMIC-FT & general & 0.355  &	0.605  &	0.369  &	0.596 	& 0.400 	& 0.614  &	0.311  &	0.597 	& 0.363 	& 0.624  &	0.399 	& 0.643  \\
        & structured &  0.296  &	0.568 	& 0.304  &	0.566  &	0.295	& 0.557 	& 0.316 	& 0.591 	& 0.346  &	0.597 	& 0.381  &	0.637   \\
        \hline
        BioMistral7B & general & 0.165 	&0.250 	& 0.212 	& 0.433 	& 0.232 	& 0.423  & 0.228 	& 0.529 	& 0.227 	& 0.510 	& 0.232 	& 0.510  \\
        & structured & 0.245  & 0.452  & 0.297  & 0.586  & 0.267 	& 0.548  & 0.262  & 0.500 	& 0.321 	& 0.567 	& 0.288 	& 0.529  \\
        \hline
        BioMistral7B-FT & general & 0.254  & 0.439 & 0.272 & 0.455 & 0.165  & 0.231  & 0.273 & 0.460 & 0.286 & 0.462 & 0.311 & 0.468  \\
        & structured & 0.287 	& 0.436 	& 0.361 	& 0.542  & 0.306 	& 0.594  & 0.353 	& 0.563 	& 0.360 	& 0.563 	& 0.264 	& 0.483  \\
        \hline
        Biomistral7B-MIMIC-FT & general & 0.279  & 0.499  & 0.306  & 0.490  & 0.351  & 0.500   & 0.260  & 0.480   & 0.259   & 0.481   & 0.277   &  0.449  \\
        & structured & 0.287 	& 0.549 	& 0.306 	& 0.568 	& 0.332 	& 0.569  & 0.283 	& 0.551 	& 0.304 	& 0.544 	& 0.348 	& 0.565 \\
        \hline
    \end{tabular}
    % \end{adjustbox}
    }
    }
    \caption{We compared the performance of different models on zero-shot and few-shot tasks using top-3, top-5, and top-10 results for F1 and MRR across both closed- and open-source LLMs. The evaluation was conducted under three distinct settings:
I. Vanilla models, including both closed- and open-source models.
II. Models fine-tuned on a portion of the dataset.
III. Models fine-tuned on 10,000 augmented data points generated from MIMIC-IV discharge notes. In many cases, open-source models continued to outperform closed-source models.
The specific details of our experiments are provided in the Materials and Methods section. The complete scores, along with their 95\% confidence intervals, are presented in Supplementary Table \ref{tab:f1_mrr}.}
    \label{tab:performance}
\end{table}


% Data Augmentation performance
The highest-performing model that utilized our data augmentation strategy was Llama 3.1-8B-MIMIC-FT, achieving an F1 score of 0.400 (CI 95\%: 0.414-0.387) in the top-10 medical jargon extractions using zero-shot prompts.
The highest MRR score, 0.746 (CI 95\%: 0.762-0.730), was achieved by Mistral7B-MIMIC-FT in the top-3 medical jargon extractions using few-shot prompts.
However, neither the F1 nor the MRR score achieved the highest performance among all models that used the data augmentation strategy.
In most cases, the F1 scores indicated that the data augmentation approach led to higher performance compared to vanilla models, and in some instances, even surpassed the performance of fine-tuned models.
However, the performance differences varied depending on the prompt settings.
These findings will be further discussed in the discussion section.

\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{|l|cc|cc|cc|}
\hline
 & \multicolumn{2}{c}{top3} & \multicolumn{2}{c}{top5} & \multicolumn{2}{c}{top10} \\
 \hline
 & F1 & MRR & F1 & MRR & F1 & MRR \\
 \hline
 GPT2\cite{openai2023gpt} & 0.097 (0.108/0.087) & 0.191 (0.208/0.175) & 0.094 (0.105/0.083) & 0.193 (0.212/0.174) & 0.099 (0.116/0.081) & 0.170 (0.197/0.142) \\
BioGPT\cite{luo2022biogpt} & 0.145 (0.163/0.126) & 0.192 (0.235/0.149) & 0.169 (0.197/0.141) & 0.196 (0.212/0.180) & 0.174 (0.202/0.146) & 0.166 (0.219/0.112) \\
\hline
\end{tabular}
}
\caption{Performance of Baseline models}
\label{tab:baseline}
\end{table}

Table \ref{tab:baseline} presents the performance of the baseline models, which include commonly used language models (LMs) for medical information extraction.
As shown in Table \ref{tab:baseline}, BioGPT~\cite{luo2022biogpt} achieved the highest F1 score of 0.174 (95\% CI: 0.202-0.146) and the highest MRR score of 0.196 (95\% CI: 0.212-0.180) among the baseline models.
However, these values are significantly lower than those achieved by the benchmark models, suggesting that LLMs generally outperform smaller models in downstream tasks.

\begin{table}[H]
    \centering
    \large % Adjusts font size for the table; try \small or \footnotesize if too small
    \begin{adjustbox}{width=1.0\textwidth} % Adjust width here as needed
    \begin{tabular}{|l|l|cc|cc|cc|cc|cc|cc|}
        \hline
        \multirow{3}{*}{\textbf{Models}} & \multirow{3}{*}{\textbf{Prompt}} & \multicolumn{6}{c|}{\textbf{Zero-Shot}} & \multicolumn{6}{c|}{\textbf{Few-Shot}} \\
        \cline{3-14}
        & & \multicolumn{2}{c|}{\textbf{top3}} & \multicolumn{2}{c|}{\textbf{top5}} & \multicolumn{2}{c|}{\textbf{top10}} & \multicolumn{2}{c|}{\textbf{top3}} & \multicolumn{2}{c|}{\textbf{top5}} & \multicolumn{2}{c|}{\textbf{top10}} \\
        & & \textbf{F1} & \textbf{MRR} & \textbf{F1} & \textbf{MRR} & \textbf{F1} & \textbf{MRR} & \textbf{F1} & \textbf{MRR} & \textbf{F1} & \textbf{MRR} & \textbf{F1} & \textbf{MRR} \\
        \hline
        % \multicolumn{14}{|c|}{\textbf{Open-Source LLMs}} \\
        % \hline
        Mistral7B-10-MIMIC-FT & general &  0.261  &	0.492  & 0.289  &	0.486  & 0.291 &	0.463  & 0.288  &	0.613  & 0.287  &	0.577  & 0.327  &	0.529 \\
        & structured & 0.299  &	0.527  & 0.309  &	0.528  & 0.332  &	0.552  & 0.294 	& 0.573  & 0.316 	& 0.557  & 0.337  &	0.549  \\
        Mistral7B-100-MIMIC-FT & general & 0.319  &	0.527  & 0.315  & 0.590  & 0.323  &	0.524  & 0.346  &	0.691  & 0.401  &	0.700  & 0.405  &	0.639 \\
        & structured &  0.310 	& 0.459 & 0.335  &	0.484  & 0.379  &	0.486  & 0.338 	& 0.633  & 0.366  &	0.665  & 0.405  &	0.673 \\
        Mistral7B-1000-MIMIC-FT & general & 0.331  & 0.531  & 0.348  &	0.516  & 0.336  &	0.465  & 0.336  & 0.746  & 0.358 &	0.702  & 0.367 &	0.726  \\
        & structured & 0.311 	& 0.484  & 0.341  &	0.481  & 0.398 	& 0.535  & 0.348  &	0.693  & 0.377  &	0.672  & 0.381  &	0.672  \\
        
        \hline
        Llama3.1-8B-10-MIMIC FT & general & 0.335  &	0.602   &	0.381 	& 0.583   & 0.395 & 0.610  & 0.304 & 0.610  & 0.352	& 0.614  & 0.394 	& 0.634 \\
        & structured & 0.288  & 0.559  & 0.300  &	0.578  & 0.304  &	0.580  & 0.331  &	0.618  & 0.358  & 0.626  & 0.379 	& 0.632 \\
        Llama3.1-8B-100-MIMIC-FT & general & 0.359  &	0.601  & 0.378  &	0.579  & 0.400 	& 0.606  & 0.307 & 0.615  & 0.363 	& 0.609  & 0.399 & 0.624 \\
        & structured & 0.287  &	0.570  & 0.301  &	0.576  & 0.295 &	0.560  & 0.324  &	0.592  & 0.355  &	0.636  & 0.376  &	0.648 \\
        Llama3.1-8B-1000-MIMIC-FT & general & 0.350  &	0.595  & 0.369  &	0.574  & 0.397 	& 0.601  & 0.312 	& 0.619  & 0.351  &	0.615  & 0.400  &	0.621 \\
        & structured & 0.297  & 0.564  & 0.301  &	0.587   & 0.311 & 0.581  & 0.323 	& 0.601  & 0.362  &	0.620  & 0.389  &	0.632 \\
        
        \hline
        BioMistral7B-10-MIMIC-FT & general & 0.270 & 0.459 & 0.295 & 0.453 & 0.321 & 0.464 & 0.241 & 0.473 & 0.257 & 0.488 & 0.276 & 0.464   \\
        & structured & 0.300 & 0.539 & 0.302 & 0.535 & 0.301 	& 0.517 & 0.213 & 0.403 & 0.237 & 0.445 & 0.241 & 0.427 \\
        BioMistral7B-100-MIMIC-FT & general & 0.262 & 0.500 & 0.299 & 0.522 & 0.321 & 0.464 & 0.264 & 0.481 & 0.305 & 0.506 & 0.342  & 0.495  \\
        & structured & 0.311 & 0.531 & 0.339 & 0.535 & 0.360 & 0.558 & 0.277 & 0.502 & 0.303 & 0.501 & 0.335 & 0.531 \\
        BioMistral7B-1000-MIMIC-FT & general & 0.284 & 0.504 & 0.308 & 0.492 & 0.355& 0.501 & 0.303 & 0.553 & 0.335 & 0.579 & 0.347 & 0.565    \\
        & structured &  0.259 & 0.496 & 0.295 & 0.516 & 0.348 & 0.528 & 0.218 & 0.405 & 0.254 & 0.471 & 0.282 & 0.496\\
        \hline
    \end{tabular}
    \end{adjustbox}
    \caption{Performance comparison between different open-source LLMs trained on different sizes of augmented data. We scale from 10 to 10,000 by multiplying 10. The results for using 10,000 are in Table~\ref{tab:performance}. The F1 and MRR tends to increase when the size of the augmented dataset increases.}
    \label{tab:aug_data_performance}
\end{table}

We also examined whether increasing or decreasing the size of the augmented dataset impacts the model's overall performance (Table \ref{tab:aug_data_performance}).
The highest F1 score of 0.405 was achieved by Mistral 7B when utilizing 100 augmented datasets with both general and specific prompts.
The highest MRR score, 0.746, was observed from Mistral 7B fine-tuned on 10,000 augmented data points using a general prompt.
A general trend was observed, indicating that utilizing larger augmented datasets results in higher performance compared to models trained on smaller datasets.
As the dataset size increased, both the F1 and MRR scores improved; however, this trend was more pronounced for F1 than for MRR.
In some cases, such as BioMistral7B in top-3 medical jargon extraction with zero-shot prompts, higher MRR scores were achieved with smaller dataset sizes.

\section*{Discussion}



\paragraph{Zero-shot vs. Few-shot} 

When it comes to prompting strategies, across both closed- and open-source models, few-shot prompts consistently outperformed zero-shot prompts in extracting relevant medical information (Table \ref{tab:performance}).
As illustrated in Figure \ref{fig::zeroshotvfewshot}, the choice of prompting strategy can lead to different outputs, resulting in varying performance outcomes.
This pattern aligns with findings from various studies, which have observed that few-shot prompts allow models to leverage examples that enhance contextual understanding and response specificity.
For example, \cite{brown2020language} demonstrated that few-shot prompting improves the performance of language models in complex tasks by providing contextual cues, enabling the model to better interpret nuanced queries—particularly in medical and technical domains where specificity is critical.
However, in cases where the models were fine-tuned, these patterns began to diminish, with zero-shot prompts showing comparable or even higher scores than few-shot prompts.
This can be attributed to instruction fine-tuning, which enhances a model's ability to handle unseen data.
As noted by \cite{weiFinetunedLanguageModels2022}, instruction fine-tuning improves the performance of LLMs in zero-shot tasks by equipping them with a better understanding of task-specific instructions.

\begin{figure}[t]

\begin{tcbraster}[raster columns=3, raster equal height, raster width=\linewidth]

\AtBeginEnvironment{tcolorbox}{\small}
\begin{tcolorbox}[title=True labels, halign=left]{\small}
\sethlcolor{pink} 
\hl{1. tinnitus} \\
\hl{1. whipple procedure} \\
\sethlcolor{Melon}
\hl{2. insulin-dependent diabetic} \\
2. simvastatin \\
\sethlcolor{Yellow}
\hl{2. retinopathy} \\
2. microalbumin \\
3. colon adenomas \\
3. colonoscopy

\end{tcolorbox}
\begin{tcolorbox}[title=Mistral7B Zero-shot, halign=left]
1. history of pancreatic neoplasm \\
1. contrast ct of the abdomen and pelvis \\
1. ca-19-9 blood test \\
\sethlcolor{Melon}
\hl{2. diabetes} \\
2. a1c blood test \\
2. microalbuminuria urine test \\
3. dyslipidemia \\
3. lipid profile blood test \\
3. lfts (liver function tests) blood test 
\end{tcolorbox}
\begin{tcolorbox}[title=Mistral7B Few-shot, halign=left]
\sethlcolor{pink}
\hl{1. tinnitus}\\
2. low-grade pancreatic neoplasm\\
\sethlcolor{pink}
\hl{2. whipple procedure with pancreatectomy and splenectomy}\\
2. bile duct stricture requiring reconstruction \\
\sethlcolor{Melon}
\hl{3. insulin-dependent diabetes} \\
3. insulin pump \\
\sethlcolor{Yellow}
\hl{3. retinopathy} \\
\end{tcolorbox}

\end{tcbraster}
\vspace{0.5em} % Adjust spacing as needed
\caption{Case Study for Extracting the Top 3 Important Medical Jargons from Zero-shot and Few-shot Prompts in Mistral 7B. The few-shot prompting strategy demonstrates greater robustness in vanilla models compared to zero-shot prompting.
The highlighted jargons represent terms that overlap with the expert-annotated labels, emphasizing the alignment between the model's outputs and domain experts' annotations.}
\label{fig::zeroshotvfewshot}
\end{figure}

\paragraph{General vs. Structured Prompts} The effectiveness of prompts can vary across models, with certain prompt styles enhancing performance for specific models (Table \ref{tab:performance}). While differences between models were generally minimal, some models performed better with particular prompt styles. Notably, for the Llama3.1 8B model, general prompts outperformed structured prompts in F1 and ranking prediction. Whereas in Mistral7B, structured prompts show improved performance to general prompts. This aligns with prior research suggesting that tailored prompts can optimize a model's performance in specialized tasks \cite{liuWhatMakesGood2021}. Thus, testing diverse prompt sets is essential to maximize model-specific performance.

\begin{figure}[!ht]
% \newlength\myboxwidth
% \setlength{\myboxwidth}{\dimexpr\textwidth-20\fboxsep}

\begin{tcbraster}[raster columns=3, raster equal height, raster width=\linewidth]
% \tcbset{width=(\linewidth-2mm)/2, box align=top, before=,after=\hfill}

\AtBeginEnvironment{tcolorbox}{\small}
\begin{tcolorbox}[title=True Labels, halign=left]{\small}
\sethlcolor{pink} 
\hl{1. tinnitus} \\
\hl{1. whipple procedure} \\
\sethlcolor{Melon}
\hl{2. insulin-dependent diabetic} \\
2. simvastatin \\
2. retinopathy \\
2. microalbumin \\
\sethlcolor{Yellow}
\hl{3. colon adenomas} \\
\hl{3. colonoscopy} \\
\sethlcolor{YellowGreen}
\hl{4. gerd} \\
4. omeprazole \\
5. erectile dysfunction \\
5. viagra

\end{tcolorbox}
\begin{tcolorbox}[title=BioMistral7B, halign=left]
\sethlcolor{pink}
\hl{1. tinnitus of many years' duration.} \\
1. audiometry. \\
1. ct scan of the temporal bone. \\
2. low-grade pancreatic neoplasm.  \\
2. ct scan of the abdomen and pelvis.  \\
2. endoscopic ultrasound. \\
\sethlcolor{Melon}
\hl{3. insulin-dependent diabetes.}  \\
3. fasting blood glucose.  \\
3. hba1c.  \\
3. c-peptide. \\ 
...

\end{tcolorbox}
\begin{tcolorbox}[title=BioMistral7B fintuned, halign=left]
\sethlcolor{pink}
\hl{1. tinnitus of many years' duration.} \\
2. low-grade pancreatic neoplasm. \\
\sethlcolor{Melon}
\hl{3. insulin-dependent diabetes.} \\
4. Dyslipidemia. \\
5. history of high blood pressure. \\
6. family history of brca gene. \\
\sethlcolor{Yellow}
\hl{7. multiple colon adenomas by colonoscopy.} \\
8. sleep apnea. \\
\sethlcolor{YellowGreen}
\hl{9. Gerd.}\\
10. erectile dysfunction.
\end{tcolorbox}

\end{tcbraster}
\vspace{0.5em} % Adjust spacing as needed
\caption{Case Study for extracting Top 5 important medical jargons from BioMistral7B and BioMistral7B that was finetuned on some of the samples of the gold labeled dataset in Zero-shot prompt settings. The finetuned model shows more robustness than vanilla models, especially in Zero-shot prompts. The highlighted jargons are the ones that overlap with the expert-annotated labels. }
\label{fig::finetune}
\end{figure}

\paragraph{Finetuning LLMs} 
Fine-tuning LLMs using domain-specific data proved effective in enhancing model performance, albeit with some limitations (Table \ref{tab:performance}).
One possible explanation for this result is that the size of the datasets used for fine-tuning was too small to yield substantial performance gains.
Increasing the size of the fine-tuning dataset is likely to further improve the model's performance~\cite{vieiraHowMuchData2024}.
Additionally, we observed that zero-shot prompts outperformed few-shot prompts, particularly in fine-tuned scenarios.
Figure \ref{fig::finetune} illustrates the impact of fine-tuning: while vanilla models show limitations in extracting key points using prompts alone, fine-tuning enables the models to learn relevant patterns, leading to improved performance.
Previous research has similarly shown that fine-tuning on domain-specific data can significantly enhance performance by adjusting model weights to reflect the unique characteristics of the target domain, such as better handling of abbreviations, acronyms, and clinically relevant contexts~\cite{lee2020biobert}.
Moreover, instruction fine-tuning has been shown to improve the model's zero-shot performance~\cite{weiFinetunedLanguageModels2022}.
However, as the number of extracted terms increases, the performance gap between vanilla and fine-tuned models tends to narrow.

\paragraph{Data Augmentation} 
We explored the impact of data augmentation using various sizes of MIMIC-IV~\cite{johnsonMIMICIVFreelyAccessible2023} discharge notes to enhance the robustness of LLMs (Tables \ref{tab:performance} and \ref{tab:aug_data_performance}).
Data augmentation simulated diverse scenarios within medical notes, enabling the LLM to generalize better across a broader range of note types.
Similar to fine-tuning, the performance gains from data augmentation were marginal in few-shot prompt settings. However, in zero-shot prompts, the performance gain was significant, even exceeding the performance of models that used the fine-tuning strategy ($p<0.05$).
As shown in Figure \ref{fig::augmentation}, by utilizing only the augmented dataset, we achieved performance levels similar to those of fine-tuned models, and in some cases, even higher scores.
These findings align with prior studies~\cite{weiFinetunedLanguageModels2022, tangDoesSyntheticData2023}, which argue that using LLM-generated data can improve model performance on downstream tasks.
Additionally, studies have shown that while data augmentation provides benefits, substantial improvements often require a high degree of variation in the augmented data~\cite{weiEDAEasyData2019, chenEmpiricalSurveyData2021}.
In our case, data augmentation provided additional exposure to diverse linguistic patterns in medical notes. However, the approach alone was insufficient to dramatically outperform its counterparts, possibly due to the limited number of examples used in generating the augmented datasets. \newline

\begin{figure}[!ht]

\begin{tcbraster}[raster columns=3, raster equal height, raster width=\linewidth]

\AtBeginEnvironment{tcolorbox}{\small}
\begin{tcolorbox}[title=True labels\\, halign=left]{\small}
\sethlcolor{pink}
\hl{1. nonischemic cardiomyopathy} \\
\hl{1. persantine thallium} \\
1. ejection fraction \\
1. lad ischemia \\
\hl{1. heart catheterization} \\
1. carvedilol \\
2. trigger fingers \\
2. hand clinic \\
2. mri \\
… \\
\sethlcolor{Melon}
\hl{5. gerd}

\end{tcolorbox}
\begin{tcolorbox}[title=Llama 3.1 8B finetuned \\, halign=left]
\sethlcolor{pink}
\hl{1. nonischemic cardiomyopathy}\\
\hl{1. persantine thallium} \\
1. echocardiogram \\
\sethlcolor{Melon}
\hl{2. gerd} \\
2. egd \\
3. fatty liver \\
3. hemochromatosis gene analysis \\
4. sinus polyposis \\
5. asthma 

\end{tcolorbox}
\begin{tcolorbox}[title=Llama 3.1 8B finetuned on augmented dataset, halign=left]
\sethlcolor{pink}
\hl{1. nonischemic cardiomyopathy} \\
\sethlcolor{Melon}
\hl{2. gerd} \\
3. fatty liver \\
4. hives \\
5. sinus polyposis \\
\sethlcolor{pink}
\hl{1. persantine thallium} \\
\hl{1. heart catheterization} \\
1. chest x-rays \\
1. iron saturation \\
1. echo \\
…
\end{tcolorbox}

\end{tcbraster}
\vspace{0.5em} % Adjust spacing as needed
\caption{Case Study for extracting Top 5 important medical jargons from Llama3.1 8B finetuned and Llama 3.1 8B that was finetuned on the MIMIC-IV augmented dataset in Zero-shot prompt settings. In many cases, augmented models shows comparable performance than vanilla models and finetuned models, especially in Zero-shot prompts. The highlighted jargons are the ones that overlap with the expert annotated labels.}
\label{fig::augmentation}
\end{figure}

\paragraph{Incrementing the size of augmented dataset} 
As demonstrated in Table \ref{tab:aug_data_performance}, increasing the size of the augmented dataset improves the overall performance of open-source LLMs, regardless of the model. This result aligns with findings from \cite{yuanRealFakeEffectiveTraining2024,kimSyntheticDataImprove2024}, where increasing the size of synthetic datasets significantly enhanced model performance compared to models trained on smaller real-world datasets.
However, we observed that MRR scores did not improve as much as F1 scores, indicating that ranking important information remains a non-trivial task for LLMs.
As shown in Figure~\ref{fig::augmentation}, while the model effectively captures important terms in the medical text, it still struggles to match the rankings as defined in the true labels.
One possible explanation is the lower quality of the LLM-augmented dataset.
Since we only used two examples for ICL and did not implement any filtering techniques to ensure the quality of the curated dataset, this may have affected the overall quality of the augmented dataset.
We aim to address this limitation and improve performance in this aspect in future studies.

Our research demonstrated that mimicking expert-level human annotation is a non-trivial task, even for LLMs.
While LLMs perform well in summarization and paraphrasing tasks, their recall drops significantly when it comes to identifying and prioritizing domain specific terms.
Ranking identified information adds another layer of complexity.
We have shown that utilizing efficient fine-tuning and data augmentation can help improve model performance.
Overall, our findings provide insights into the relative strengths and limitations of different methods for enhancing LLMs in medical applications.

\subsection*{Limitations 
} 

This study has several limitations.
First, we tested only a limited selection of available closed- and open-source LLMs.
Second, fine-tuning was not applied to closed-source LLMs.
Third, despite the improvements achieved through fine-tuning and data augmentation, the performance of the models still falls short of human annotation.
Future research will aim to address these challenges.


\section*{Conclusion}

We evaluated both closed- and open-source LLMs for identifying and prioritizing medical jargon using expert-annotated EHR notes, demonstrating that fine-tuning and data augmentation significantly enhance performance.
Comprehensive case analyses further validated our findings, highlighting the effectiveness of these methods for extracting and prioritizing important medical jargon for patients.

\section*{Acknowledgments}
We greatly value UMass BioNLP group's insightful feedback and thoughtful guidance.


\section*{Author Contributions}

Won Seok Jang, Sharmin Sultana, and Zonghai Yao authored the manuscript, with Won Seok and Sharmin implementing and analyzing models and Zonghai contributing to project planning. 

Zhichao Yang, Hieu Tran, and Sunjae Kwon designed the framework and reviewed outcomes. 

Hong Yu planned the project, wrote the proposal, and guided the research.


\section*{Conflict of Interest Statement}
No conflicting interests.

\section*{DATA AVAILABILITY}
The source code will be released here : \url{https://www.github.com/memy85/2024_medicalnote_annotation}

% References as numbers
\makeatletter
\renewcommand{\@biblabel}[1]{\hfill #1.}
\makeatother

% unstr is used to keep citation order
% \bibliographystyle{vancouver}
% \bibliography{amia}  

\bibliographystyle{naturemag}
\bibliography{medical_note_annotation, main}

\newpage
\clearpage
\appendix
\section*{Appendix}
\label{appendix}

\input{supplement} % This includes the contents of appendix.tex



\end{document}
