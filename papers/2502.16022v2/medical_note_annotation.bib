@inproceedings{acharyaGeneratingPersonalizedHospitalization2018,
  title = {Towards {{Generating Personalized Hospitalization Summaries}}},
  booktitle = {Proceedings of the 2018 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Student Research Workshop}}},
  author = {Acharya, Sabita and Di Eugenio, Barbara and Boyd, Andrew and Cameron, Richard and Dunn Lopez, Karen and Martyn-Nemeth, Pamela and Dickens, Carolyn and Ardati, Amer},
  editor = {Cordeiro, Silvio Ricardo and Oraby, Shereen and Pavalanathan, Umashanthi and Rim, Kyeongmin},
  date = {2018-06},
  pages = {74--82},
  publisher = {Association for Computational Linguistics},
  location = {New Orleans, Louisiana, USA},
  doi = {10.18653/v1/N18-4011},
  url = {https://aclanthology.org/N18-4011},
  urldate = {2024-08-07},
  abstract = {Most of the health documents, including patient education materials and discharge notes, are usually flooded with medical jargons and contain a lot of generic information about the health issue. In addition, patients are only provided with the doctor's perspective of what happened to them in the hospital while the care procedure performed by nurses during their entire hospital stay is nowhere included. The main focus of this research is to generate personalized hospital-stay summaries for patients by combining information from physician discharge notes and nursing plan of care. It uses a metric to identify medical concepts that are Complex, extracts definitions for the concept from three external knowledge sources, and provides the simplest definition to the patient. It also takes various features of the patient into account, like their concerns and strengths, ability to understand basic health information, level of engagement in taking care of their health, and familiarity with the health issue and personalizes the content of the summaries accordingly. Our evaluation showed that the summaries contain 80\% of the medical concepts that are considered as being important by both doctor and nurses. Three patient advisors (i.e. individuals who are trained in understanding patient experience extensively) verified the usability of our summaries and mentioned that they would like to get such summaries when they are discharged from hospital.},
  file = {/home/wonseok/Zotero/storage/9AYDHI2Q/Acharya 등 - 2018 - Towards Generating Personalized Hospitalization Su.pdf}
}

@inproceedings{agrawalLargeLanguageModels2022,
  title = {Large Language Models Are Few-Shot Clinical Information Extractors},
  booktitle = {Proceedings of the 2022 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Agrawal, Monica and Hegselmann, Stefan and Lang, Hunter and Kim, Yoon and Sontag, David},
  editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
  date = {2022-02},
  pages = {1998--2022},
  publisher = {Association for Computational Linguistics},
  location = {Abu Dhabi, United Arab Emirates},
  doi = {10.18653/v1/2022.emnlp-main.130},
  url = {https://aclanthology.org/2022.emnlp-main.130},
  urldate = {2024-01-30},
  abstract = {A long-running goal of the clinical NLP community is the extraction of important variables trapped in clinical notes. However, roadblocks have included dataset shift from the general domain and a lack of public clinical corpora and annotations. In this work, we show that large language models, such as InstructGPT (Ouyang et al., 2022), perform well at zero- and few-shot information extraction from clinical text despite not being trained specifically for the clinical domain. Whereas text classification and generation performance have already been studied extensively in such models, here we additionally demonstrate how to leverage them to tackle a diverse set of NLP tasks which require more structured outputs, including span identification, token-level sequence classification, and relation extraction. Further, due to the dearth of available data to evaluate these systems, we introduce new datasets for benchmarking few-shot clinical information extraction based on a manual re-annotation of the CASI dataset (Moon et al., 2014) for new tasks. On the clinical extraction tasks we studied, the GPT-3 systems significantly outperform existing zero- and few-shot baselines.},
  eventtitle = {{{EMNLP}} 2022},
  file = {/home/wonseok/Zotero/storage/IW2H3GCU/Agrawal 등 - 2022 - Large language models are few-shot clinical inform.pdf}
}

@article{alduraywishSourcesHealthInformation2020,
  title = {Sources of {{Health Information}} and {{Their Impacts}} on {{Medical Knowledge Perception Among}} the {{Saudi Arabian Population}}: {{Cross-Sectional Study}}},
  shorttitle = {Sources of {{Health Information}} and {{Their Impacts}} on {{Medical Knowledge Perception Among}} the {{Saudi Arabian Population}}},
  author = {Alduraywish, Shatha A and Altamimi, Lamees A and Aldhuwayhi, Rawan A and AlZamil, Lama R and Alzeghayer, Luluh Y and Alsaleh, Futoon S and Aldakheel, Fahad M and Tharkar, Shabana},
  date = {2020-03-19},
  journaltitle = {Journal of Medical Internet Research},
  shortjournal = {J Med Internet Res},
  volume = {22},
  number = {3},
  eprint = {32191208},
  eprinttype = {pmid},
  pages = {e14414},
  issn = {1439-4456},
  doi = {10.2196/14414},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7118549/},
  urldate = {2024-07-06},
  abstract = {Background Having a reliable source for health information is vital to build a strong foundation of knowledge, especially with the current revolution of the internet and social media, which raises many concerns regarding harmful effects on the health of the public. However, there are no studies on how the Saudi Arabian population seeks health information. Details about the most used and trusted sources of health information among the public will help health authorities and public awareness accounts on social media to effectively disseminate health information. Objective To investigate the types of sources accessed by the Saudi Arabian population while seeking health information, as well as their level of trust in the sources and to assess the impact of these sources on their perception of medical knowledge and health decision-making. Methods A cross-sectional study was conducted to meet the objectives. The study population included both men and women who were aged 16 years or more and visited primary care clinics at King Khalid University Hospital. Four hundred and thirteen participants were sampled using the simple random method, and a self-administered questionnaire was used to collect data. The data were analyzed using SPSS software (IBM Corp, Armonk, New York, USA). Results A total of 413 participants were included in this study, and of these, 99 (24.0\%) were males and 206 (49.9\%) had a bachelor’s degree. Doctors were chosen as the first source of information by 87.6\% (283/323) of the participants, and they were completely trusted by most of the population (326/411, 79.3\%). The second most commonly used source was pharmacists (112/194, 57.7\%), and they were partially trusted by 41.4\% (159/384) of the participants. Internet searches, social media, and traditional medicine were not prioritized by most of the participants as the first or second source of health information. The majority of the participants did not trust information obtained from social media, and WhatsApp was the most untrusted source. Almost half of the respondents (197/413, 47.7\%) acknowledged that various sources of information can often help them understand their health problems. However, the majority disagreed on substituting a doctor’s prescription with information obtained from the internet or a friend or relative. Conclusions Although physicians were preferred and highly trusted, internet sources appeared to impact the medical knowledge of the population. The population still preferred to use internet search to obtain health information prior to a doctor’s visit.},
  pmcid = {PMC7118549}
}

@online{ApproximateMatchingEvaluating,
  title = {Approximate {{Matching}} for {{Evaluating Keyphrase Extraction}}},
  url = {https://scholar.googleusercontent.com/scholar?q=cache:VL__-nOTVtIJ:scholar.google.com/+Approximate+matching+for+evaluating+keyphrase+extraction&hl=en&as_sdt=0,22&as_vis=1},
  urldate = {2024-06-18}
}

@article{apterHomeVisitsUncontrolled2019,
  title = {Home Visits for Uncontrolled Asthma among Low-Income Adults with Patient Portal Access},
  author = {Apter, Andrea J. and Localio, A. Russell and Morales, Knashawn H. and Han, Xiaoyan and Perez, Luzmercy and Mullen, Alyssa N. and Rogers, Marisa and Klusaritz, Heather and Howell, John T. and Canales, Maryori N. and Bryant-Stephens, Tyra},
  date = {2019-09},
  journaltitle = {The Journal of Allergy and Clinical Immunology},
  shortjournal = {J Allergy Clin Immunol},
  volume = {144},
  number = {3},
  eprint = {31181221},
  eprinttype = {pmid},
  pages = {846-853.e11},
  issn = {1097-6825},
  doi = {10.1016/j.jaci.2019.05.030},
  abstract = {BACKGROUND: Asthma disproportionately affects low-income and minority adults. In an era of electronic records and Internet-based digital devices, it is unknown whether portals for patient-provider communication can improve asthma outcomes. OBJECTIVE: We sought to estimate the effect on asthma outcomes of an intervention using home visits (HVs) by community health workers (CHWs) plus training in patient portals compared with usual care and portal training only. METHODS: Three hundred one predominantly African American and Hispanic/Latino adults with uncontrolled asthma were recruited from primary care and asthma specialty practices serving low-income urban neighborhoods, directed to Internet access, and given portal training. Half were randomized to HVs over 6~months by CHWs to facilitate competency in portal use and promote care coordination. RESULTS: One hundred seventy (56\%) patients used the portal independently. Rates of portal activity did not differ between randomized groups. Asthma control and asthma-related quality of life improved in both groups over 1~year. Differences in improvements over time were greater for the HV group for all outcomes but reached conventional levels of statistical significance only for the yearly hospitalization rate (-0.53; 95\% CI, -1.08 to -0.024). Poor neighborhoods and living conditions plus limited Internet access were barriers for patients to complete the protocol and for CHWs to make HVs. CONCLUSION: For low-income adults with uncontrolled asthma, portal access and CHWs produced small incremental benefits. HVs with emphasis on self-management education might be necessary to facilitate patient-clinician communication and to improve asthma outcomes.},
  langid = {english},
  pmcid = {PMC6742549},
  keywords = {Adolescent,Adult,Aged,Aged 80 and over,Asthma,community health worker,Community Health Workers,electronic health record,Female,health disparities,Health Education,House Calls,Humans,information technology,Male,Middle Aged,patient portal,Patient Portals,Poverty,Quality of Life,Young Adult},
  file = {/home/wonseok/Zotero/storage/NUHTW72D/Apter 등 - 2019 - Home visits for uncontrolled asthma among low-income adults with patient portal access.pdf}
}

@article{assiriImpactPatientAccess2022,
  title = {The {{Impact}} of {{Patient Access}} to {{Their Electronic Health Record}} on {{Medication Management Safety}}: {{A Narrative Review}}},
  shorttitle = {The {{Impact}} of {{Patient Access}} to {{Their Electronic Health Record}} on {{Medication Management Safety}}},
  author = {Assiri, Ghadah},
  date = {2022-03},
  journaltitle = {Saudi pharmaceutical journal: SPJ: the official publication of the Saudi Pharmaceutical Society},
  shortjournal = {Saudi Pharm J},
  volume = {30},
  number = {3},
  eprint = {35498224},
  eprinttype = {pmid},
  pages = {185--194},
  issn = {1319-0164},
  doi = {10.1016/j.jsps.2022.01.001},
  abstract = {INTRODUCTION: As the American's Federal Health Insurance Portability and Accountability Act (HIPAA) stated that patients should be allowed to review their medical records, and as information technology is ever more widely used by healthcare professionals and patients, providing patients with online access to their own medical records through a patient portal is becoming increasingly popular. Previous research has been done regarding the impact on the quality and safety of patients' care, rather than explicitly on medication safety, when providing those patients with access to their electronic health records (EHRs). AIM: This narrative review aims to summarise the results from previous studies on the impact on medication management safety concepts of adult patients accessing information contained in their own EHRs. RESULT: A total of 24 studies were included in this review. The most two commonly studied measures of safety in medication management were: (a) medication adherence and (b) patient-reported experience. Other measures, such as: discrepancies, medication errors, appropriateness and Adverse Drug Events (ADEs) were the least studied. CONCLUSION: The results suggest that providing patients with access to their EHRs can improve medication management safety. Patients pointed out improvements to the safety of their medications and perceived stronger medication control. The data from these studies lay the foundation for future research.},
  langid = {english},
  pmcid = {PMC9051961},
  keywords = {ACOVE Assessing Care Of Vulnerable Elders,ADE Adverse Drug Events,CI Confidence Interval,EHR Electronic Health Record,Electronic health record,Electronic medical records,HIPAA American’s Federal Health Insurance Portability and Accountability Act,HIV/AIDS Human Immunodeficiency Virus/ Acquired Immunodeficiency Syndrome,LMR Longitudinal Medical Record,MHV MyHealtheVet,OR Odds Ratio,OTC over-the-counter,PAERS Patient Access to Electronic Records System,Patient access and medication management,Patient participation,PCP Primary Care Physician,PDC Proportion of Days Covered,PG Patient Gateway,PHR Personal Health Record,RCT Randomised Controlled Trial,RR Relative Risk,SPARO System Providing Access to Records Online,UK United Kingdom,USA United States of America,VA Veterans Affairs,WDS WellDoc System},
  file = {/home/wonseok/Zotero/storage/BVUNTBW4/Assiri - 2022 - The Impact of Patient Access to Their Electronic Health Record on Medication Management Safety A Na.pdf}
}

@online{bianInspireLargeLanguage2023,
  title = {Inspire the {{Large Language Model}} by {{External Knowledge}} on {{BioMedical Named Entity Recognition}}},
  author = {Bian, Junyi and Zheng, Jiaxuan and Zhang, Yuyi and Zhu, Shanfeng},
  date = {2023-09-21},
  eprint = {2309.12278},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2309.12278},
  urldate = {2024-04-13},
  abstract = {Large language models (LLMs) have demonstrated dominating performance in many NLP tasks, especially on generative tasks. However, they often fall short in some information extraction tasks, particularly those requiring domain-specific knowledge, such as Biomedical Named Entity Recognition (NER). In this paper, inspired by Chain-of-thought, we leverage the LLM to solve the Biomedical NER step-by-step: break down the NER task into entity span extraction and entity type determination. Additionally, for entity type determination, we inject entity knowledge to address the problem that LLM's lack of domain knowledge when predicting entity category. Experimental results show a significant improvement in our two-step BioNER approach compared to previous few-shot LLM baseline. Additionally, the incorporation of external knowledge significantly enhances entity category determination performance.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {/home/wonseok/Zotero/storage/FPUC3558/Bian 등 - 2023 - Inspire the Large Language Model by External Knowl.pdf;/home/wonseok/Zotero/storage/MPZ83BDR/2309.html}
}

@article{bodenreiderUnifiedMedicalLanguage2004b,
  title = {The {{Unified Medical Language System}} ({{UMLS}}): Integrating Biomedical Terminology},
  shorttitle = {The {{Unified Medical Language System}} ({{UMLS}})},
  author = {Bodenreider, O.},
  date = {2004-01-01},
  journaltitle = {Nucleic Acids Research},
  shortjournal = {Nucleic Acids Research},
  volume = {32},
  number = {90001},
  pages = {267D-270},
  issn = {1362-4962},
  doi = {10.1093/nar/gkh061},
  url = {https://academic.oup.com/nar/article-lookup/doi/10.1093/nar/gkh061},
  urldate = {2024-04-12},
  abstract = {The Uni®ed Medical Language System (http:// umlsks.nlm.nih.gov) is a repository of biomedical vocabularies developed by the US National Library of Medicine. The UMLS integrates over 2 million names for some 900 000 concepts from more than 60 families of biomedical vocabularies, as well as 12 million relations among these concepts. Vocabularies integrated in the UMLS Metathesaurus include the NCBI taxonomy, Gene Ontology, the Medical Subject Headings (MeSH), OMIM and the Digital Anatomist Symbolic Knowledge Base. UMLS concepts are not only inter-related, but may also be linked to external resources such as GenBank. In addition to data, the UMLS includes tools for customizing the Metathesaurus (MetamorphoSys), for generating lexical variants of concept names (lvg) and for extracting UMLS concepts from text (MetaMap). The UMLS knowledge sources are updated quarterly. All vocabularies are available at no fee for research purposes within an institution, but UMLS users are required to sign a license agreement. The UMLS knowledge sources are distributed on CD-ROM and by FTP.},
  langid = {english},
  file = {/home/wonseok/Zotero/storage/YS5944F6/Bodenreider - 2004 - The Unified Medical Language System (UMLS) integr.pdf}
}

@article{boseSurveyRecentNamed2021,
  title = {A {{Survey}} on {{Recent Named Entity Recognition}} and {{Relationship Extraction Techniques}} on {{Clinical Texts}}},
  author = {Bose, Priyankar and Srinivasan, Sriram and Sleeman, William C. and Palta, Jatinder and Kapoor, Rishabh and Ghosh, Preetam},
  date = {2021-01},
  journaltitle = {Applied Sciences},
  volume = {11},
  number = {18},
  pages = {8319},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2076-3417},
  doi = {10.3390/app11188319},
  url = {https://www.mdpi.com/2076-3417/11/18/8319},
  urldate = {2024-12-23},
  abstract = {Significant growth in Electronic Health Records (EHR) over the last decade has provided an abundance of clinical text that is mostly unstructured and untapped. This huge amount of clinical text data has motivated the development of new information extraction and text mining techniques. Named Entity Recognition (NER) and Relationship Extraction (RE) are key components of information extraction tasks in the clinical domain. In this paper, we highlight the present status of clinical NER and RE techniques in detail by discussing the existing proposed NLP models for the two tasks and their performances and discuss the current challenges. Our comprehensive survey on clinical NER and RE encompass current challenges, state-of-the-art practices, and future directions in information extraction from clinical text. This is the first attempt to discuss both of these interrelated topics together in the clinical context. We identified many research articles published based on different approaches and looked at applications of these tasks. We also discuss the evaluation metrics that are used in the literature to measure the effectiveness of the two these NLP methods and future research directions.},
  issue = {18},
  langid = {english},
  keywords = {clinical text,electronic health records,machine learning,named entity recognition,natural language processing,relationship extraction},
  file = {/home/wonseok/Zotero/storage/SVDEE8SY/Bose 등 - 2021 - A Survey on Recent Named Entity Recognition and Relationship Extraction Techniques on Clinical Texts.pdf}
}

@article{bronsonImpactSharedMedical1986,
  title = {The Impact of Shared Medical Records on Smoking Awareness and Behavior in Ambulatory Care},
  author = {Bronson, D. L. and O'Meara, K.},
  date = {1986},
  journaltitle = {Journal of General Internal Medicine},
  shortjournal = {J Gen Intern Med},
  volume = {1},
  number = {1},
  eprint = {3772566},
  eprinttype = {pmid},
  pages = {34--37},
  issn = {0884-8734},
  doi = {10.1007/BF02596322},
  abstract = {In a randomized controlled trial of sharing medical records with ambulatory adults as part of periodic health examinations, 193 patients (experimental group; 37 smokers) received copies of their medical records while 208 patients (control group; 50 smokers) did not. Awareness of smoking as a health problem and smoking behavior were assessed two weeks and six months later. At two weeks, 46\% of experimental group smokers indicated that smoking was a major health problem, compared with 21\% of the control group (p less than 0.02), and 43\% of the experimental group had quit or reduced smoking, compared with 20\% of the control group (p less than 0.02). At six months smoking problem awareness was not significantly different (33\% experimental group vs. 14\% control group, p = NS), but 65\% of the experimental group had quit or reduced compared with 29\% of the control group (p less than 0.04). Sharing medical records with smokers after periodic health examinations is effective in enhancing patient awareness of smoking as a health problem and beginning the process of changing smoking behavior.},
  langid = {english},
  keywords = {Female,Follow-Up Studies,Humans,Male,Medical Records,Patient Education as Topic,Physician-Patient Relations,Smoking Prevention,Vermont}
}

@online{BuildingBridgesElectronic,
  title = {Building Bridges across Electronic Health Record Systems through Inferred Phenotypic Topics - {{ScienceDirect}}},
  url = {https://www.sciencedirect.com/science/article/pii/S1532046415000544?via%3Dihub},
  urldate = {2024-12-23},
  file = {/home/wonseok/Zotero/storage/E8ELMQT7/S1532046415000544.html}
}

@inproceedings{caiGenerationPatientAfterVisit2022,
  title = {Generation of {{Patient After-Visit Summaries}} to {{Support Physicians}}},
  booktitle = {Proceedings of the 29th {{International Conference}} on {{Computational Linguistics}}},
  author = {Cai, Pengshan and Liu, Fei and Bajracharya, Adarsha and Sills, Joe and Kapoor, Alok and Liu, Weisong and Berlowitz, Dan and Levy, David and Pradhan, Richeek and Yu, Hong},
  editor = {Calzolari, Nicoletta and Huang, Chu-Ren and Kim, Hansaem and Pustejovsky, James and Wanner, Leo and Choi, Key-Sun and Ryu, Pum-Mo and Chen, Hsin-Hsi and Donatelli, Lucia and Ji, Heng and Kurohashi, Sadao and Paggio, Patrizia and Xue, Nianwen and Kim, Seokhwan and Hahm, Younggyun and He, Zhong and Lee, Tony Kyungil and Santus, Enrico and Bond, Francis and Na, Seung-Hoon},
  date = {2022-10},
  pages = {6234--6247},
  publisher = {International Committee on Computational Linguistics},
  location = {Gyeongju, Republic of Korea},
  url = {https://aclanthology.org/2022.coling-1.544},
  urldate = {2024-01-09},
  abstract = {An after-visit summary (AVS) is a summary note given to patients after their clinical visit. It recaps what happened during their clinical visit and guides patients' disease self-management. Studies have shown that a majority of patients found after-visit summaries useful. However, many physicians face excessive workloads and do not have time to write clear and informative summaries. In this paper, we study the problem of automatic generation of after-visit summaries and examine whether those summaries can convey the gist of clinical visits. We report our findings on a new clinical dataset that contains a large number of electronic health record (EHR) notes and their associated summaries. Our results suggest that generation of lay language after-visit summaries remains a challenging task. Crucially, we introduce a feedback mechanism that alerts physicians when an automatic summary fails to capture the important details of the clinical notes or when it contains hallucinated facts that are potentially detrimental to the summary quality. Automatic and human evaluation demonstrates the effectiveness of our approach in providing writing feedback and supporting physicians.},
  eventtitle = {{{COLING}} 2022},
  file = {/home/wonseok/Zotero/storage/TMXHSMJL/Cai 등 - 2022 - Generation of Patient After-Visit Summaries to Sup.pdf}
}

@online{caiPaniniQAEnhancingPatient2023,
  title = {{{PaniniQA}}: {{Enhancing Patient Education Through Interactive Question Answering}}},
  shorttitle = {{{PaniniQA}}},
  author = {Cai, Pengshan and Yao, Zonghai and Liu, Fei and Wang, Dakuo and Reilly, Meghan and Zhou, Huixue and Li, Lingxi and Cao, Yi and Kapoor, Alok and Bajracharya, Adarsha and Berlowitz, Dan and Yu, Hong},
  date = {2023-08-20},
  eprint = {2308.03253},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2308.03253},
  url = {http://arxiv.org/abs/2308.03253},
  urldate = {2024-09-27},
  abstract = {Patient portal allows discharged patients to access their personalized discharge instructions in electronic health records (EHRs). However, many patients have difficulty understanding or memorizing their discharge instructions. In this paper, we present PaniniQA, a patient-centric interactive question answering system designed to help patients understand their discharge instructions. PaniniQA first identifies important clinical content from patients' discharge instructions and then formulates patient-specific educational questions. In addition, PaniniQA is also equipped with answer verification functionality to provide timely feedback to correct patients' misunderstandings. Our comprehensive automatic and human evaluation results demonstrate our PaniniQA is capable of improving patients' mastery of their medical instructions through effective interactions},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/wonseok/Zotero/storage/N4DT3AFR/Cai 등 - 2023 - PaniniQA Enhancing Patient Education Through Interactive Question Answering.pdf;/home/wonseok/Zotero/storage/Y384LNBI/2308.html}
}

@inproceedings{celiktenKeywordExtractionBiomedical2021,
  title = {Keyword {{Extraction}} from {{Biomedical Documents Using Deep Contextualized Embeddings}}},
  booktitle = {2021 {{International Conference}} on {{INnovations}} in {{Intelligent SysTems}} and {{Applications}} ({{INISTA}})},
  author = {Celikten, Azer and Ugur, Aybars and Bulut, Hasan},
  date = {2021-08-25},
  pages = {1--5},
  publisher = {IEEE},
  location = {Kocaeli, Turkey},
  doi = {10.1109/INISTA52262.2021.9548470},
  url = {https://ieeexplore.ieee.org/document/9548470/},
  urldate = {2024-03-26},
  abstract = {Due to the rapidly increasing amount of biomedical publications, it has become challenging to follow scientific articles and new developments. Keywords in scientific articles provide a quick understanding and summarize the important points of the context. When keywords are not used in some biomedical articles or are not sufficient to express the content of the text, automatic keyword extraction systems are needed. This paper addresses the keyword extraction problem as a sequence labeling task where words are represented as deep contextual embeddings. We predict the keyword tags identified in sequence labeling by fine-tuning XLNET and BERT-based models such as BERT, BioBERT, SCIBERT, and RoBERTa. Our proposed method does not need extra dictionaries required by rule-based methods and feature extraction as in traditional machine learning methods. Performance evaluation on the benchmark dataset for biomedical keyword extraction shows that domain-specific contextualized embeddings (BioBERT, SciBERT) achieve state-of-the-art results compared to the general domain embeddings (BERT, RoBERTa, XLNET) and unsupervised methods.},
  eventtitle = {2021 {{International Conference}} on {{INnovations}} in {{Intelligent SysTems}} and {{Applications}} ({{INISTA}})},
  isbn = {978-1-6654-3603-8},
  langid = {english},
  file = {/home/wonseok/Zotero/storage/CIQM8ALR/Celikten 등 - 2021 - Keyword Extraction from Biomedical Documents Using.pdf}
}

@inproceedings{celiktenKeywordExtractionBiomedical2021a,
  title = {Keyword {{Extraction}} from {{Biomedical Documents Using Deep Contextualized Embeddings}}},
  booktitle = {2021 {{International Conference}} on {{INnovations}} in {{Intelligent SysTems}} and {{Applications}} ({{INISTA}})},
  author = {Çelikten, Azer and Uğur, Aybars and Bulut, Hasan},
  date = {2021-08},
  pages = {1--5},
  doi = {10.1109/INISTA52262.2021.9548470},
  url = {https://ieeexplore.ieee.org/abstract/document/9548470},
  urldate = {2024-03-26},
  abstract = {Due to the rapidly increasing amount of biomedical publications, it has become challenging to follow scientific articles and new developments. Keywords in scientific articles provide a quick understanding and summarize the important points of the context. When keywords are not used in some biomedical articles or are not sufficient to express the content of the text, automatic keyword extraction systems are needed. This paper addresses the keyword extraction problem as a sequence labeling task where words are represented as deep contextual embeddings. We predict the keyword tags identified in sequence labeling by fine-tuning XLNET and BERT-based models such as BERT, BioBERT, SCIBERT, and RoBERTa. Our proposed method does not need extra dictionaries required by rule-based methods and feature extraction as in traditional machine learning methods. Performance evaluation on the benchmark dataset for biomedical keyword extraction shows that domain-specific contextualized embeddings (BioBERT, SciBERT) achieve state-of-the-art results compared to the general domain embeddings (BERT, RoBERTa, XLNET) and unsupervised methods.},
  eventtitle = {2021 {{International Conference}} on {{INnovations}} in {{Intelligent SysTems}} and {{Applications}} ({{INISTA}})},
  keywords = {Benchmark testing,Biological system modeling,Bit error rate,deep contextualized embeddings,Feature extraction,keyword extraction,medical informatics,natural language processing,Performance evaluation,Predictive models,sequence labeling,Technological innovation},
  file = {/home/wonseok/Zotero/storage/L4CM333R/Çelikten 등 - 2021 - Keyword Extraction from Biomedical Documents Using.pdf}
}

@online{chenEmpiricalSurveyData2021,
  title = {An {{Empirical Survey}} of {{Data Augmentation}} for {{Limited Data Learning}} in {{NLP}}},
  author = {Chen, Jiaao and Tam, Derek and Raffel, Colin and Bansal, Mohit and Yang, Diyi},
  date = {2021-06-14},
  eprint = {2106.07499},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2106.07499},
  url = {http://arxiv.org/abs/2106.07499},
  urldate = {2024-10-29},
  abstract = {NLP has achieved great progress in the past decade through the use of neural models and large labeled datasets. The dependence on abundant data prevents NLP models from being applied to low-resource settings or novel tasks where significant time, money, or expertise is required to label massive amounts of textual data. Recently, data augmentation methods have been explored as a means of improving data efficiency in NLP. To date, there has been no systematic empirical overview of data augmentation for NLP in the limited labeled data setting, making it difficult to understand which methods work in which settings. In this paper, we provide an empirical survey of recent progress on data augmentation for NLP in the limited labeled data setting, summarizing the landscape of methods (including token-level augmentations, sentence-level augmentations, adversarial augmentations, and hidden-space augmentations) and carrying out experiments on 11 datasets covering topics/news classification, inference tasks, paraphrasing tasks, and single-sentence tasks. Based on the results, we draw several conclusions to help practitioners choose appropriate augmentations in different settings and discuss the current challenges and future directions for limited data learning in NLP.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/wonseok/Zotero/storage/JTMUXGN8/Chen 등 - 2021 - An Empirical Survey of Data Augmentation for Limited Data Learning in NLP.pdf;/home/wonseok/Zotero/storage/2QPBCV9C/2106.html}
}

@article{chenFindingImportantTerms2016a,
  title = {Finding {{Important Terms}} for {{Patients}} in {{Their Electronic Health Records}}: {{A Learning-to-Rank Approach Using Expert Annotations}}},
  shorttitle = {Finding {{Important Terms}} for {{Patients}} in {{Their Electronic Health Records}}},
  author = {Chen, Jinying and Zheng, Jiaping and Yu, Hong},
  date = {2016-11-30},
  journaltitle = {JMIR Medical Informatics},
  volume = {4},
  number = {4},
  pages = {e6373},
  publisher = {JMIR Publications Inc., Toronto, Canada},
  doi = {10.2196/medinform.6373},
  url = {https://medinform.jmir.org/2016/4/e40},
  urldate = {2024-01-09},
  abstract = {Background: Many health organizations allow patients to access their own electronic health record (EHR) notes through online patient portals as a way to enhance patient-centered care. However, EHR notes are typically long and contain abundant medical jargon that can be difficult for patients to understand. In addition, many medical terms in patients’ notes are not directly related to their health care needs. One way to help patients better comprehend their own notes is to reduce information overload and help them focus on medical terms that matter most to them. Interventions can then be developed by giving them targeted education to improve their EHR comprehension and the quality of care. Objective: We aimed to develop a supervised natural language processing (NLP) system called Finding impOrtant medical Concepts most Useful to patientS (FOCUS) that automatically identifies and ranks medical terms in EHR notes based on their importance to the patients. Methods: First, we built an expert-annotated corpus. For each EHR note, 2 physicians independently identified medical terms important to the patient. Using the physicians’ agreement as the gold standard, we developed and evaluated FOCUS. FOCUS first identifies candidate terms from each EHR note using MetaMap and then ranks the terms using a support vector machine-based learn-to-rank algorithm. We explored rich learning features, including distributed word representation, Unified Medical Language System semantic type, topic features, and features derived from consumer health vocabulary. We compared FOCUS with 2 strong baseline NLP systems. Results: Physicians annotated 90 EHR notes and identified a mean of 9 (SD 5) important terms per note. The Cohen’s kappa annotation agreement was .51. The 10-fold cross-validation results show that FOCUS achieved an area under the receiver operating characteristic curve (AUC-ROC) of 0.940 for ranking candidate terms from EHR notes to identify important terms. When including term identification, the performance of FOCUS for identifying important terms from EHR notes was 0.866 AUC-ROC. Both performance scores significantly exceeded the corresponding baseline system scores (P{$<$}.001). Rich learning features contributed to FOCUS’s performance substantially. Conclusions: FOCUS can automatically rank terms from EHR notes based on their importance to patients. It may help develop future interventions that improve quality of care.},
  langid = {english},
  file = {/home/wonseok/Zotero/storage/LTBZAJXR/Chen 등 - 2016 - Finding Important Terms for Patients in Their Elec.pdf;/home/wonseok/Zotero/storage/NTNA736Y/e40.html}
}

@article{chenRankingMedicalTerms2017,
  title = {Ranking {{Medical Terms}} to {{Support Expansion}} of {{Lay Language Resources}} for {{Patient Comprehension}} of {{Electronic Health Record Notes}}: {{Adapted Distant Supervision Approach}}},
  shorttitle = {Ranking {{Medical Terms}} to {{Support Expansion}} of {{Lay Language Resources}} for {{Patient Comprehension}} of {{Electronic Health Record Notes}}},
  author = {Chen, Jinying and Jagannatha, Abhyuday N and Fodeh, Samah J and Yu, Hong},
  date = {2017-10-31},
  journaltitle = {JMIR Medical Informatics},
  shortjournal = {JMIR Med Inform},
  volume = {5},
  number = {4},
  eprint = {29089288},
  eprinttype = {pmid},
  pages = {e42},
  issn = {2291-9694},
  doi = {10.2196/medinform.8531},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5686421/},
  urldate = {2024-07-21},
  abstract = {Background Medical terms are a major obstacle for patients to comprehend their electronic health record (EHR) notes. Clinical natural language processing (NLP) systems that link EHR terms to lay terms or definitions allow patients to easily access helpful information when reading through their EHR notes, and have shown to improve patient EHR comprehension. However, high-quality lay language resources for EHR terms are very limited in the public domain. Because expanding and curating such a resource is a costly process, it is beneficial and even necessary to identify terms important for patient EHR comprehension first. Objective We aimed to develop an NLP system, called adapted distant supervision (ADS), to rank candidate terms mined from EHR corpora. We will give EHR terms ranked as high by ADS a higher priority for lay language annotation—that is, creating lay definitions for these terms. Methods Adapted distant supervision uses distant supervision from consumer health vocabulary and transfer learning to adapt itself to solve the problem of ranking EHR terms in the target domain. We investigated 2 state-of-the-art transfer learning algorithms (ie, feature space augmentation and supervised distant supervision) and designed 5 types of learning features, including distributed word representations learned from large EHR data for ADS. For evaluating ADS, we asked domain experts to annotate 6038 candidate terms as important or nonimportant for EHR comprehension. We then randomly divided these data into the target-domain training data (1000 examples) and the evaluation data (5038 examples). We compared ADS with 2 strong baselines, including standard supervised learning, on the evaluation data. Results The ADS system using feature space augmentation achieved the best average precision, 0.850, on the evaluation set when using 1000 target-domain training examples. The ADS system using supervised distant supervision achieved the best average precision, 0.819, on the evaluation set when using only 100 target-domain training examples. The 2 ADS systems both performed significantly better than the baseline systems (P{$<$}.001 for all measures and all conditions). Using a rich set of learning features contributed to ADS’s performance substantially. Conclusions ADS can effectively rank terms mined from EHRs. Transfer learning improved ADS’s performance even with a small number of target-domain training examples. EHR terms prioritized by ADS were used to expand a lay language resource that supports patient EHR comprehension. The top 10,000 EHR terms ranked by ADS are available upon request.},
  pmcid = {PMC5686421},
  file = {/home/wonseok/Zotero/storage/A4D5EWPE/Chen 등 - 2017 - Ranking Medical Terms to Support Expansion of Lay .pdf}
}

@article{chenUnsupervisedEnsembleRanking2017,
  title = {Unsupervised Ensemble Ranking of Terms in Electronic Health Record Notes Based on Their Importance to Patients},
  author = {Chen, Jinying and Yu, Hong},
  date = {2017-04-01},
  journaltitle = {Journal of Biomedical Informatics},
  shortjournal = {Journal of Biomedical Informatics},
  volume = {68},
  pages = {121--131},
  issn = {1532-0464},
  doi = {10.1016/j.jbi.2017.02.016},
  url = {https://www.sciencedirect.com/science/article/pii/S153204641730045X},
  urldate = {2023-12-29},
  abstract = {Background Allowing patients to access their own electronic health record (EHR) notes through online patient portals has the potential to improve patient-centered care. However, EHR notes contain abundant medical jargon that can be difficult for patients to comprehend. One way to help patients is to reduce information overload and help them focus on medical terms that matter most to them. Targeted education can then be developed to improve patient EHR comprehension and the quality of care. Objective The aim of this work was to develop FIT (Finding Important Terms for patients), an unsupervised natural language processing (NLP) system that ranks medical terms in EHR notes based on their importance to patients. Methods We built FIT on a new unsupervised ensemble ranking model derived from the biased random walk algorithm to combine heterogeneous information resources for ranking candidate terms from each EHR note. Specifically, FIT integrates four single views (rankers) for term importance: patient use of medical concepts, document-level term salience, word co-occurrence based term relatedness, and topic coherence. It also incorporates partial information of term importance as conveyed by terms’ unfamiliarity levels and semantic types. We evaluated FIT on 90 expert-annotated EHR notes and used the four single-view rankers as baselines. In addition, we implemented three benchmark unsupervised ensemble ranking methods as strong baselines. Results FIT achieved 0.885 AUC-ROC for ranking candidate terms from EHR notes to identify important terms. When including term identification, the performance of FIT for identifying important terms from EHR notes was 0.813 AUC-ROC. Both performance scores significantly exceeded the corresponding scores from the four single rankers (P{$<$}0.001). FIT also outperformed the three ensemble rankers for most metrics. Its performance is relatively insensitive to its parameter. Conclusions FIT can automatically identify EHR terms important to patients. It may help develop future interventions to improve quality of care. By using unsupervised learning as well as a robust and flexible framework for information fusion, FIT can be readily applied to other domains and applications.},
  keywords = {Electronic health record,Information extraction,Natural language processing,Unsupervised ensemble ranking},
  file = {/home/wonseok/Zotero/storage/37IZMKX3/Chen 그리고 Yu - 2017 - Unsupervised ensemble ranking of terms in electron.pdf;/home/wonseok/Zotero/storage/5AWRAEP7/S153204641730045X.html}
}

@article{chenUnsupervisedEnsembleRanking2017a,
  title = {Unsupervised Ensemble Ranking of Terms in Electronic Health Record Notes Based on Their Importance to Patients},
  author = {Chen, Jinying and Yu, Hong},
  date = {2017-04},
  journaltitle = {Journal of Biomedical Informatics},
  shortjournal = {J Biomed Inform},
  volume = {68},
  eprint = {28267590},
  eprinttype = {pmid},
  pages = {121--131},
  issn = {1532-0480},
  doi = {10.1016/j.jbi.2017.02.016},
  abstract = {BACKGROUND: Allowing patients to access their own electronic health record (EHR) notes through online patient portals has the potential to improve patient-centered care. However, EHR notes contain abundant medical jargon that can be difficult for patients to comprehend. One way to help patients is to reduce information overload and help them focus on medical terms that matter most to them. Targeted education can then be developed to improve patient EHR comprehension and the quality of care. OBJECTIVE: The aim of this work was to develop FIT (Finding Important Terms for patients), an unsupervised natural language processing (NLP) system that ranks medical terms in EHR notes based on their importance to patients. METHODS: We built FIT on a new unsupervised ensemble ranking model derived from the biased random walk algorithm to combine heterogeneous information resources for ranking candidate terms from each EHR note. Specifically, FIT integrates four single views (rankers) for term importance: patient use of medical concepts, document-level term salience, word co-occurrence based term relatedness, and topic coherence. It also incorporates partial information of term importance as conveyed by terms' unfamiliarity levels and semantic types. We evaluated FIT on 90 expert-annotated EHR notes and used the four single-view rankers as baselines. In addition, we implemented three benchmark unsupervised ensemble ranking methods as strong baselines. RESULTS: FIT achieved 0.885 AUC-ROC for ranking candidate terms from EHR notes to identify important terms. When including term identification, the performance of FIT for identifying important terms from EHR notes was 0.813 AUC-ROC. Both performance scores significantly exceeded the corresponding scores from the four single rankers (P{$<$}0.001). FIT also outperformed the three ensemble rankers for most metrics. Its performance is relatively insensitive to its parameter. CONCLUSIONS: FIT can automatically identify EHR terms important to patients. It may help develop future interventions to improve quality of care. By using unsupervised learning as well as a robust and flexible framework for information fusion, FIT can be readily applied to other domains and applications.},
  langid = {english},
  pmcid = {PMC5505865},
  keywords = {Algorithms,Comprehension,Electronic health record,Electronic Health Records,Health Records Personal,Humans,Information extraction,Natural language processing,Natural Language Processing,Terminology as Topic,Unsupervised ensemble ranking},
  file = {/home/wonseok/Zotero/storage/KTM4YKZ9/Chen 그리고 Yu - 2017 - Unsupervised ensemble ranking of terms in electron.pdf}
}

@article{cohenRedundancyAwareTopicModeling2014,
  title = {Redundancy-{{Aware Topic Modeling}} for {{Patient Record Notes}}},
  author = {Cohen, Raphael and Aviram, Iddo and Elhadad, Michael and Elhadad, Noémie},
  year = {2014. 2. 13.},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {9},
  number = {2},
  pages = {e87555},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0087555},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0087555},
  urldate = {2024-12-21},
  abstract = {The clinical notes in a given patient record contain much redundancy, in large part due to clinicians’ documentation habit of copying from previous notes in the record and pasting into a new note. Previous work has shown that this redundancy has a negative impact on the quality of text mining and topic modeling in particular. In this paper we describe a novel variant of Latent Dirichlet Allocation (LDA) topic modeling, Red-LDA, which takes into account the inherent redundancy of patient records when modeling content of clinical notes. To assess the value of Red-LDA, we experiment with three baselines and our novel redundancy-aware topic modeling method: given a large collection of patient records, (i) apply vanilla LDA to all documents in all input records; (ii) identify and remove all redundancy by chosing a single representative document for each record as input to LDA; (iii) identify and remove all redundant paragraphs in each record, leaving partial, non-redundant documents as input to LDA; and (iv) apply Red-LDA to all documents in all input records. Both quantitative evaluation carried out through log-likelihood on held-out data and topic coherence of produced topics and qualitative assessement of topics carried out by physicians show that Red-LDA produces superior models to all three baseline strategies. This research contributes to the emerging field of understanding the characteristics of the electronic health record and how to account for them in the framework of data mining. The code for the two redundancy-elimination baselines and Red-LDA is made publicly available to the community.},
  langid = {english},
  keywords = {Breast cancer,Centrality,Electronic medical records,Physicians,Preprocessing,Semantics,Statistical models,Text mining},
  file = {/home/wonseok/Zotero/storage/XXVCSFFE/Cohen 등 - 2014 - Redundancy-Aware Topic Modeling for Patient Record Notes.pdf}
}

@online{cosentinoPersonalHealthLarge2024,
  title = {Towards a {{Personal Health Large Language Model}}},
  author = {Cosentino, Justin and Belyaeva, Anastasiya and Liu, Xin and Furlotte, Nicholas A. and Yang, Zhun and Lee, Chace and Schenck, Erik and Patel, Yojan and Cui, Jian and Schneider, Logan Douglas and Bryant, Robby and Gomes, Ryan G. and Jiang, Allen and Lee, Roy and Liu, Yun and Perez, Javier and Rogers, Jameson K. and Speed, Cathy and Tailor, Shyam and Walker, Megan and Yu, Jeffrey and Althoff, Tim and Heneghan, Conor and Hernandez, John and Malhotra, Mark and Stern, Leor and Matias, Yossi and Corrado, Greg S. and Patel, Shwetak and Shetty, Shravya and Zhan, Jiening and Prabhakara, Shruthi and McDuff, Daniel and McLean, Cory Y.},
  date = {2024-06-10},
  eprint = {2406.06474},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2406.06474},
  url = {http://arxiv.org/abs/2406.06474},
  urldate = {2024-06-18},
  abstract = {In health, most large language model (LLM) research has focused on clinical tasks. However, mobile and wearable devices, which are rarely integrated into such tasks, provide rich, longitudinal data for personal health monitoring. Here we present Personal Health Large Language Model (PH-LLM), fine-tuned from Gemini for understanding and reasoning over numerical time-series personal health data. We created and curated three datasets that test 1) production of personalized insights and recommendations from sleep patterns, physical activity, and physiological responses, 2) expert domain knowledge, and 3) prediction of self-reported sleep outcomes. For the first task we designed 857 case studies in collaboration with domain experts to assess real-world scenarios in sleep and fitness. Through comprehensive evaluation of domain-specific rubrics, we observed that Gemini Ultra 1.0 and PH-LLM are not statistically different from expert performance in fitness and, while experts remain superior for sleep, fine-tuning PH-LLM provided significant improvements in using relevant domain knowledge and personalizing information for sleep insights. We evaluated PH-LLM domain knowledge using multiple choice sleep medicine and fitness examinations. PH-LLM achieved 79\% on sleep and 88\% on fitness, exceeding average scores from a sample of human experts. Finally, we trained PH-LLM to predict self-reported sleep quality outcomes from textual and multimodal encoding representations of wearable data, and demonstrate that multimodal encoding is required to match performance of specialized discriminative models. Although further development and evaluation are necessary in the safety-critical personal health domain, these results demonstrate both the broad knowledge and capabilities of Gemini models and the benefit of contextualizing physiological data for personal health applications as done with PH-LLM.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/wonseok/Zotero/storage/9C5JVCMX/Cosentino 등 - 2024 - Towards a Personal Health Large Language Model.pdf;/home/wonseok/Zotero/storage/CJSNDYXK/2406.html}
}

@article{dagdelenStructuredInformationExtraction2024,
  title = {Structured Information Extraction from Scientific Text with Large Language Models},
  author = {Dagdelen, John and Dunn, Alexander and Lee, Sanghoon and Walker, Nicholas and Rosen, Andrew S. and Ceder, Gerbrand and Persson, Kristin A. and Jain, Anubhav},
  date = {2024-02-15},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {15},
  number = {1},
  pages = {1418},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-024-45563-x},
  url = {https://www.nature.com/articles/s41467-024-45563-x},
  urldate = {2024-04-14},
  abstract = {Extracting structured knowledge from scientific text remains a challenging task for machine learning models. Here, we present a simple approach to joint named entity recognition and relation extraction and demonstrate how pretrained large language models (GPT-3, Llama-2) can be fine-tuned to extract useful records of complex scientific knowledge. We test three representative tasks in materials chemistry: linking dopants and host materials, cataloging metal-organic frameworks, and general composition/phase/morphology/application information extraction. Records are extracted from single sentences or entire paragraphs, and the output can be returned as simple English sentences or a more structured format such as a list of JSON objects. This approach represents a simple, accessible, and highly flexible route to obtaining large databases of structured specialized scientific knowledge extracted from research papers.},
  langid = {english},
  keywords = {Databases,Materials science,Scientific data,Theory and computation},
  file = {/home/wonseok/Zotero/storage/TE3KR9VG/Dagdelen 등 - 2024 - Structured information extraction from scientific .pdf}
}

@article{davisReadingAbilityParents1994,
  title = {Reading Ability of Parents Compared with Reading Level of Pediatric Patient Education Materials},
  author = {Davis, T. C. and Mayeaux, E. J. and Fredrickson, D. and Bocchini, J. A. and Jackson, R. H. and Murphy, P. W.},
  date = {1994-03},
  journaltitle = {Pediatrics},
  shortjournal = {Pediatrics},
  volume = {93},
  number = {3},
  eprint = {8115206},
  eprinttype = {pmid},
  pages = {460--468},
  issn = {0031-4005},
  abstract = {OBJECTIVES: To test the reading ability of parents of pediatric outpatients and to compare their reading ability with the ability necessary to read commonly used educational materials; to compare individual reading grade levels with the levels of the last grade completed in school; and to further validate a new literacy screening test designed specifically for medical settings. DESIGN: Prospective survey. SETTING: Pediatrics outpatient clinic in a large, public university, teaching hospital. PARTICIPANTS: Three hundred ninety-six parents or other caretakers accompanying pediatric outpatients. MEASUREMENTS: Demographics and educational status were assessed using a structured interview. Reading ability was tested using the Rapid Estimate of Adult Literacy in Medicine (REALM) and the Wide Range Achievement Test-Revised. Written educational materials were assessed for readability levels with a computer program (Grammatik IV). RESULTS: The mean score on the REALM for all parents placed them in the seventh to eighth grade reading range, despite the mean self-reported last grade completed in school being 11th grade 5th month. Wide Range Achievement Test-Revised scores correlated well with REALM scores (0.82). Eighty percent of 129 written materials from the American Academy of Pediatrics, the Centers for Disease Control, the March of Dimes, pharmaceutical companies, and commercially available baby books required at least a 10th grade reading level. Only 25\% of 60 American Academy of Pediatrics items and 19\% of all materials tested were written at less than a ninth grade level, and only 2\% of all materials were written at less than a seventh grade level. CONCLUSION: This study demonstrates that parents' self-reported education level will not accurately indicate their reading ability. Testing is needed to screen at-risk parents for low reading levels. In a public health setting, a significant amount of available parent education materials and instructions require a higher reading level than most parents have achieved. In such settings, all materials probably should be written at less than a high school level if most parents are to be expected to read them. The REALM can easily be used in busy public health clinics to screen parents for reading ability.},
  langid = {english},
  keywords = {Adolescent,Adult,Aged,Child,Hospitals Public,Hospitals University,Humans,Louisiana,Middle Aged,Outpatient Clinics Hospital,Parents,Patient Education as Topic,Prospective Studies,Reading}
}

@article{durangoNamedEntityRecognition2023,
  title = {Named {{Entity Recognition}} in {{Electronic Health Records}}: {{A Methodological Review}}},
  shorttitle = {Named {{Entity Recognition}} in {{Electronic Health Records}}},
  author = {Durango, María C. and Torres-Silva, Ever A. and Orozco-Duque, Andrés},
  date = {2023-10},
  journaltitle = {Healthcare Informatics Research},
  shortjournal = {Healthc Inform Res},
  volume = {29},
  number = {4},
  eprint = {37964451},
  eprinttype = {pmid},
  pages = {286--300},
  issn = {2093-3681},
  doi = {10.4258/hir.2023.29.4.286},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10651400/},
  urldate = {2024-06-24},
  abstract = {Objectives A substantial portion of the data contained in Electronic Health Records (EHR) is unstructured, often appearing as free text. This format restricts its potential utility in clinical decision-making. Named entity recognition (NER) methods address the challenge of extracting pertinent information from unstructured text. The aim of this study was to outline the current NER methods and trace their evolution from 2011 to 2022. Methods We conducted a methodological literature review of NER methods, with a focus on distinguishing the classification models, the types of tagging systems, and the languages employed in various corpora. Results Several methods have been documented for automatically extracting relevant information from EHRs using natural language processing techniques such as NER and relation extraction (RE). These methods can automatically extract concepts, events, attributes, and other data, as well as the relationships between them. Most NER studies conducted thus far have utilized corpora in English or Chinese. Additionally, the bidirectional encoder representation from transformers using the BIO tagging system architecture is the most frequently reported classification scheme. We discovered a limited number of papers on the implementation of NER or RE tasks in EHRs within a specific clinical domain. Conclusions EHRs play a pivotal role in gathering clinical information and could serve as the primary source for automated clinical decision support systems. However, the creation of new corpora from EHRs in specific clinical domains is essential to facilitate the swift development of NER and RE models applied to EHRs for use in clinical practice.},
  pmcid = {PMC10651400},
  file = {/home/wonseok/Zotero/storage/DUR53XVA/Durango 등 - 2023 - Named Entity Recognition in Electronic Health Reco.pdf}
}

@article{elbourneNewburyMaternityCare1987,
  title = {The {{Newbury Maternity Care Study}}: A Randomized Controlled Trial to Assess a Policy of Women Holding Their Own Obstetric Records},
  shorttitle = {The {{Newbury Maternity Care Study}}},
  author = {Elbourne, D. and Richardson, M. and Chalmers, I. and Waterhouse, I. and Holt, E.},
  date = {1987-07},
  journaltitle = {British Journal of Obstetrics and Gynaecology},
  shortjournal = {Br J Obstet Gynaecol},
  volume = {94},
  number = {7},
  eprint = {3304403},
  eprinttype = {pmid},
  pages = {612--619},
  issn = {0306-5456},
  doi = {10.1111/j.1471-0528.1987.tb03165.x},
  abstract = {To assess a policy of women holding and thus having constant access to their own obstetric records, 290 women attending a peripheral consultant clinic in Newbury, West Berkshire, were randomly allocated to hold either their full case notes, or the more usual co-operation card. Women holding their full records were significantly more likely to feel in control of their antenatal care (rate ratio 1.45; 95\% confidence interval 1.08-1.95) and to feel it was easier to talk to doctors and midwives (rate ratio 1.73; 95\% confidence interval 1.16-2.59). No other beneficial effects were detected. Asked about their preferences for any subsequent pregnancies, women holding their own records in the index pregnancy were more likely to say they would prefer to hold the same kind of record again in a subsequent pregnancy than were women holding a co-operation card (rate ratio 1.56; 95\% confidence interval 1.34-1.81). There was no evidence of negative effects. In particular, women holding their case notes did not feel more anxious than co-operation card holders. The policy of women holding their notes resulted in savings in clerical time, without evidence of an increase in the rate of lost notes.},
  langid = {english},
  keywords = {Clinical Trials as Topic,Confidentiality,Empirical Approach,Female,Forms and Records Control,Humans,Medical Records,Newbury Maternity Care Study,Obstetrics,Patient Access to Records,Patient Rights,Pregnancy,Pregnant Women,Professional Patient Relationship,Random Allocation,Risk Assessment,United Kingdom,West Berkshire}
}

@inreference{ElectronicHealthRecord2024,
  title = {Electronic Health Record},
  booktitle = {Wikipedia},
  date = {2024-06-02T17:09:51Z},
  url = {https://en.wikipedia.org/w/index.php?title=Electronic_health_record&oldid=1226933160},
  urldate = {2024-06-24},
  abstract = {An  electronic health record (EHR) is the systematized collection of patient and population electronically stored health information in a digital format. These records can be shared across different health care settings. Records are shared through network-connected, enterprise-wide information systems or other information networks and exchanges. EHRs may include a range of data, including demographics, medical history, medication and allergies, immunization status, laboratory test results, radiology images, vital signs, personal statistics like age and weight, and billing information. For several decades, electronic health records (EHRs) have been touted as key to increasing of quality care. Electronic health records are used for other reasons than charting for patients; today, providers are using data from patient records to improve quality outcomes through their care management programs. EHR combines all patients demographics into a large pool, and uses this information to assist with the creation of "new treatments or innovation in healthcare delivery" which overall improves the goals in healthcare. Combining multiple types of clinical data from the system's health records has helped clinicians identify and stratify chronically ill patients. EHR can improve quality care by using the data and analytics to prevent hospitalizations among high-risk patients. EHR systems are designed to store data accurately and to capture the state of a patient across time. It eliminates the need to track down a patient's previous paper medical records and assists in ensuring data is up-to-date, accurate and legible. It also allows open communication between the patient and the provider, while providing "privacy and security." It can reduce risk of data replication as there is only one modifiable file, which means the file is more likely up to date and decreases risk of lost paperwork and is cost efficient. Due to the digital information being searchable and in a single file, EMRs (electronic medical records) are more effective when extracting medical data for the examination of possible trends and long term changes in a patient. Population-based studies of medical records may also be facilitated by the widespread adoption of EHRs and EMRs.},
  langid = {english},
  annotation = {Page Version ID: 1226933160},
  file = {/home/wonseok/Zotero/storage/BYDCM447/Electronic_health_record.html}
}

@article{fatemiTALKGRAPHENCODING2024a,
  title = {{{TALK LIKE A GRAPH}}: {{ENCODING GRAPHS FOR LARGE LANGUAGE MODELS}}},
  author = {Fatemi, Bahare and Halcrow, Jonathan and Perozzi, Bryan},
  date = {2024},
  abstract = {Graphs are a powerful tool for representing and analyzing complex relationships in real-world applications such as social networks, recommender systems, and computational finance. Reasoning on graphs is essential for drawing inferences about the relationships between entities in a complex system, and to identify hidden patterns and trends. Despite the remarkable progress in automated reasoning with natural text, reasoning on graphs with large language models (LLMs) remains an understudied problem. In this work, we perform the first comprehensive study of encoding graph-structured data as text for consumption by LLMs. We show that LLM performance on graph reasoning tasks varies on three fundamental levels: (1) the graph encoding method, (2) the nature of the graph task itself, and (3) interestingly, the very structure of the graph considered. These novel results provide valuable insight on strategies for encoding graphs as text. Using these insights we illustrate how the correct choice of encoders can boost performance on graph reasoning tasks inside LLMs by 4.8\% to 61.8\%, depending on the task.},
  langid = {english},
  file = {/home/wonseok/Zotero/storage/KKRQ6IAF/Fatemi 등 - 2024 - TALK LIKE A GRAPH ENCODING GRAPHS FOR LARGE LANGU.pdf}
}

@article{glaserInterventionsImprovePatient2020,
  title = {Interventions to {{Improve Patient Comprehension}} in {{Informed Consent}} for {{Medical}} and {{Surgical Procedures}}: {{An Updated Systematic Review}}},
  shorttitle = {Interventions to {{Improve Patient Comprehension}} in {{Informed Consent}} for {{Medical}} and {{Surgical Procedures}}},
  author = {Glaser, Johanna and Nouri, Sarah and Fernandez, Alicia and Sudore, Rebecca L. and Schillinger, Dean and Klein-Fedyshin, Michele and Schenker, Yael},
  date = {2020-02-01},
  journaltitle = {Medical Decision Making},
  shortjournal = {Med Decis Making},
  volume = {40},
  number = {2},
  pages = {119--143},
  publisher = {SAGE Publications Inc STM},
  issn = {0272-989X},
  doi = {10.1177/0272989X19896348},
  url = {https://doi.org/10.1177/0272989X19896348},
  urldate = {2024-08-24},
  abstract = {Background. Patient comprehension is fundamental to valid informed consent. Current practices often result in inadequate patient comprehension. Purpose. An updated review to evaluate the characteristics and outcomes of interventions to improve patient comprehension in clinical informed consent. Data Sources. Systematic searches of MEDLINE and EMBASE (2008–2018). Study Selection. We included randomized and nonrandomized controlled trials evaluating interventions to improve patient comprehension in clinical informed consent. Data Extraction. Reviewers independently abstracted data using a standardized form, comparing all results and resolving disagreements by consensus. Data Synthesis. Fifty-two studies of 60 interventions met inclusion criteria. Compared with standard informed consent, a statistically significant improvement in patient comprehension was seen with 43\% (6/14) of written interventions, 56\% (15/27) of audiovisual interventions, 67\% (2/3) of multicomponent interventions, 85\% (11/13) of interactive digital interventions, and 100\% (3/3) of verbal discussion with test/feedback or teach-back interventions. Eighty-five percent of studies (44/52) evaluated patients’ understanding of risks, 69\% (41/52) general knowledge about the procedure, 35\% (18/52) understanding of benefits, and 31\% (16/52) understanding of alternatives. Participants’ education level was reported heterogeneously, and only 8\% (4/52) of studies examined effects according to health literacy. Most studies (79\%, 41/52) did not specify participants’ race/ethnicity. Limitations. Variation in interventions and outcome measures precluded conduct of a meta-analysis or calculation of mean effect size. Control group processes were variable and inconsistently characterized. Nearly half of studies (44\%, 23/52) had a high risk of bias for the patient comprehension outcome. Conclusions. Interventions to improve patient comprehension in informed consent are heterogeneous. Interactive interventions, particularly with test/feedback or teach-back components, appear superior. Future research should emphasize all key elements of informed consent and explore effects among vulnerable populations.},
  langid = {english},
  file = {/home/wonseok/Zotero/storage/BBYZCBNG/Glaser 등 - 2020 - Interventions to Improve Patient Comprehension in Informed Consent for Medical and Surgical Procedur.pdf}
}

@online{goelLLMsAccelerateAnnotation2023,
  title = {{{LLMs Accelerate Annotation}} for {{Medical Information Extraction}}},
  author = {Goel, Akshay and Gueta, Almog and Gilon, Omry and Liu, Chang and Erell, Sofia and Nguyen, Lan Huong and Hao, Xiaohong and Jaber, Bolous and Reddy, Shashir and Kartha, Rupesh and Steiner, Jean and Laish, Itay and Feder, Amir},
  date = {2023-12-04},
  eprint = {2312.02296},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2312.02296},
  url = {http://arxiv.org/abs/2312.02296},
  urldate = {2024-01-30},
  abstract = {The unstructured nature of clinical notes within electronic health records often conceals vital patient-related information, making it challenging to access or interpret. To uncover this hidden information, specialized Natural Language Processing (NLP) models are required. However, training these models necessitates large amounts of labeled data, a process that is both time-consuming and costly when relying solely on human experts for annotation. In this paper, we propose an approach that combines Large Language Models (LLMs) with human expertise to create an efficient method for generating ground truth labels for medical text annotation. By utilizing LLMs in conjunction with human annotators, we significantly reduce the human annotation burden, enabling the rapid creation of labeled datasets. We rigorously evaluate our method on a medical information extraction task, demonstrating that our approach not only substantially cuts down on human intervention but also maintains high accuracy. The results highlight the potential of using LLMs to improve the utilization of unstructured clinical data, allowing for the swift deployment of tailored NLP solutions in healthcare.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/wonseok/Zotero/storage/FJ5FWQ3M/Goel 등 - 2023 - LLMs Accelerate Annotation for Medical Information.pdf;/home/wonseok/Zotero/storage/2ZXYWGCU/2312.html}
}

@inproceedings{goelLLMsAccelerateAnnotation2023a,
  title = {{{LLMs Accelerate Annotation}} for {{Medical Information Extraction}}},
  booktitle = {Proceedings of the 3rd {{Machine Learning}} for {{Health Symposium}}},
  author = {Goel, Akshay and Gueta, Almog and Gilon, Omry and Liu, Chang and Erell, Sofia and Nguyen, Lan Huong and Hao, Xiaohong and Jaber, Bolous and Reddy, Shashir and Kartha, Rupesh and Steiner, Jean and Laish, Itay and Feder, Amir},
  date = {2023-12-04},
  pages = {82--100},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v225/goel23a.html},
  urldate = {2024-04-13},
  abstract = {The unstructured nature of clinical notes within electronic health records often conceals vital patient-related information, making it challenging to access or interpret. To uncover this hidden information, specialized Natural Language Processing (NLP) models are required. However, training these models necessitates large amounts of labeled data, a process that is both time-consuming and costly when relying solely on human experts for annotation. In this paper, we propose an approach that combines Large Language Models (LLMs) with human expertise to create an efficient method for generating ground truth labels for medical text annotation. By utilizing LLMs in conjunction with human annotators, we significantly reduce the human annotation burden, enabling the rapid creation of labeled datasets. We rigorously evaluate our method on a medical information extraction task, demonstrating that our approach not only substantially cuts down on human intervention but also maintains high accuracy. The results highlight the potential of using LLMs to improve the utilization of unstructured clinical data, allowing for the swift deployment of tailored NLP solutions in healthcare.},
  eventtitle = {Machine {{Learning}} for {{Health}} ({{ML4H}})},
  langid = {english},
  file = {/home/wonseok/Zotero/storage/6PUQWPJC/Goel 등 - 2023 - LLMs Accelerate Annotation for Medical Information.pdf}
}

@online{guoPersonalizedJargonIdentification2023,
  title = {Personalized {{Jargon Identification}} for {{Enhanced Interdisciplinary Communication}}},
  author = {Guo, Yue and Chang, Joseph Chee and Antoniak, Maria and Bransom, Erin and Cohen, Trevor and Wang, Lucy Lu and August, Tal},
  date = {2023-11-15},
  eprint = {2311.09481},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2311.09481},
  url = {http://arxiv.org/abs/2311.09481},
  urldate = {2024-08-07},
  abstract = {Scientific jargon can impede researchers when they read materials from other domains. Current methods of jargon identification mainly use corpus-level familiarity indicators (e.g., Simple Wikipedia represents plain language). However, researchers' familiarity of a term can vary greatly based on their own background. We collect a dataset of over 10K term familiarity annotations from 11 computer science researchers for terms drawn from 100 paper abstracts. Analysis of this data reveals that jargon familiarity and information needs vary widely across annotators, even within the same sub-domain (e.g., NLP). We investigate features representing individual, sub-domain, and domain knowledge to predict individual jargon familiarity. We compare supervised and prompt-based approaches, finding that prompt-based methods including personal publications yields the highest accuracy, though zero-shot prompting provides a strong baseline. This research offers insight into features and methods to integrate personal data into scientific jargon identification.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {/home/wonseok/Zotero/storage/SWFVD756/Guo 등 - 2023 - Personalized Jargon Identification for Enhanced In.pdf;/home/wonseok/Zotero/storage/677E4CHG/2311.html}
}

@article{hahnMedicalInformationExtraction2020,
  title = {Medical {{Information Extraction}} in the {{Age}} of {{Deep Learning}}},
  author = {Hahn, Udo and Oleynik, Michel},
  date = {2020-08-21},
  journaltitle = {Yearbook of Medical Informatics},
  volume = {29},
  pages = {208--220},
  publisher = {Georg Thieme Verlag KG},
  issn = {0943-4747},
  doi = {10.1055/s-0040-1702001},
  url = {https://www.thieme-connect.com/products/ejournals/html/10.1055/s-0040-1702001},
  urldate = {2024-08-24},
  abstract = {Objectives: We survey recent developments in medical Information Extraction (IE) as reported in the literature from the past three years. Our focus is on the fundamental methodological paradigm shift from standard Machine Learning (ML) techniques to Deep Neural Networks (DNNs). We describe applications of this new paradigm concentrating on two basic IE tasks, named entity recognition and relation extraction, for two selected semantic classes—diseases and drugs (or medications)—and relations between them.   Methods: For the time period from 2017 to early 2020, we searched for relevant publications from three major scientific communities: medicine and medical informatics, natural language processing, as well as neural networks and artificial intelligence.   Results: In the past decade, the field of Natural Language Processing (NLP) has undergone a profound methodological shift from symbolic to distributed representations based on the paradigm of Deep Learning (DL). Meanwhile, this trend is, although with some delay, also reflected in the medical NLP community. In the reporting period, overwhelming experimental evidence has been gathered, as illustrated in this survey for medical IE, that DL-based approaches outperform non-DL ones by often large margins. Still, small-sized and access-limited corpora create intrinsic problems for data-greedy DL as do special linguistic phenomena of medical sublanguages that have to be overcome by adaptive learning strategies.   Conclusions: The paradigm shift from (feature-engineered) ML to DNNs changes the fundamental methodological rules of the game for medical NLP. This change is by no means restricted to medical IE but should also deeply influence other areas of medical informatics, either NLP- or non-NLP-based.},
  langid = {english},
  keywords = {deep learning,information extraction,named entity recognition,natural language processing,Neural networks,relation extraction},
  file = {/home/wonseok/Zotero/storage/YYVBX5VX/Hahn 및 Oleynik - 2020 - Medical Information Extraction in the Age of Deep Learning.pdf}
}

@article{homerIntroductionWomanHeldRecord1999,
  title = {The {{Introduction}} of a {{Woman}}‐{{Held Record}} into a {{Hospital Antenatal Clinic}}: {{The Bring Your Own Records Study}}},
  shorttitle = {The {{Introduction}} of a {{Woman}}‐{{Held Record}} into a {{Hospital Antenatal Clinic}}},
  author = {Homer, Caroline S.E. and Davis, Gregory K. and Everitt, Louise S.},
  date = {1999-02},
  journaltitle = {Australian and New Zealand Journal of Obstetrics and Gynaecology},
  shortjournal = {Aust NZ J Obst Gynaeco},
  volume = {39},
  number = {1},
  pages = {54--57},
  issn = {0004-8666, 1479-828X},
  doi = {10.1111/j.1479-828X.1999.tb03445.x},
  url = {https://obgyn.onlinelibrary.wiley.com/doi/10.1111/j.1479-828X.1999.tb03445.x},
  urldate = {2024-12-20},
  abstract = {Summary:               We report the introduction of a woman‐held record into an antenatal clinic in a NSW teaching hospital using a randomized controlled trial. In 1997, 150 women were randomized to either retaining their entire antenatal record through pregnancy (women‐held group) or to holding a small, abbreviated card, as was standard practice (control group). A questionnaire was distributed to women to measure sense of control, involvement in care and levels of communication. Availability of records at antenatal visits was also measured.               Women in both groups were satisfied with their allocated method of record keeping, however, those in the women‐held group were significantly more likely to report feeling in ‘control’ during pregnancy. Women in the control group were more likely to feel anxious and helpless and less likely to have information on their records explained to them by their caregiver.               The number of records available at each clinic was similar in both groups.},
  langid = {english}
}

@article{huImprovingLargeLanguage2024,
  title = {Improving Large Language Models for Clinical Named Entity Recognition via Prompt Engineering},
  author = {Hu, Yan and Chen, Qingyu and Du, Jingcheng and Peng, Xueqing and Keloth, Vipina Kuttichi and Zuo, Xu and Zhou, Yujia and Li, Zehan and Jiang, Xiaoqian and Lu, Zhiyong and Roberts, Kirk and Xu, Hua},
  date = {2024-01-27},
  journaltitle = {Journal of the American Medical Informatics Association},
  shortjournal = {Journal of the American Medical Informatics Association},
  pages = {ocad259},
  issn = {1527-974X},
  doi = {10.1093/jamia/ocad259},
  url = {https://doi.org/10.1093/jamia/ocad259},
  urldate = {2024-04-14},
  abstract = {The study highlights the potential of large language models, specifically GPT-3.5 and GPT-4, in processing complex clinical data and extracting meaningful information with minimal training data. By developing and refining prompt-based strategies, we can significantly enhance the models’ performance, making them viable tools for clinical NER tasks and possibly reducing the reliance on extensive annotated datasets.This study quantifies the capabilities of GPT-3.5 and GPT-4 for clinical named entity recognition (NER) tasks and proposes task-specific prompts to improve their performance.We evaluated these models on 2 clinical NER tasks: (1) to extract medical problems, treatments, and tests from clinical notes in the MTSamples corpus, following the 2010 i2b2 concept extraction shared task, and (2) to identify nervous system disorder-related adverse events from safety reports in the vaccine adverse event reporting system (VAERS). To improve the GPT models' performance, we developed a clinical task-specific prompt framework that includes (1) baseline prompts with task description and format specification, (2) annotation guideline-based prompts, (3) error analysis-based instructions, and (4) annotated samples for few-shot learning. We assessed each prompt's effectiveness and compared the models to BioClinicalBERT.Using baseline prompts, GPT-3.5 and GPT-4 achieved relaxed F1 scores of 0.634, 0.804 for MTSamples and 0.301, 0.593 for VAERS. Additional prompt components consistently improved model performance. When all 4 components were used, GPT-3.5 and GPT-4 achieved relaxed F1 socres of 0.794, 0.861 for MTSamples and 0.676, 0.736 for VAERS, demonstrating the effectiveness of our prompt framework. Although these results trail BioClinicalBERT (F1 of 0.901 for the MTSamples dataset and 0.802 for the VAERS), it is very promising considering few training samples are needed.The study’s findings suggest a promising direction in leveraging LLMs for clinical NER tasks. However, while the performance of GPT models improved with task-specific prompts, there's a need for further development and refinement. LLMs like GPT-4 show potential in achieving close performance to state-of-the-art models like BioClinicalBERT, but they still require careful prompt engineering and understanding of task-specific knowledge. The study also underscores the importance of evaluation schemas that accurately reflect the capabilities and performance of LLMs in clinical settings.While direct application of GPT models to clinical NER tasks falls short of optimal performance, our task-specific prompt framework, incorporating medical knowledge and training samples, significantly enhances GPT models' feasibility for potential clinical applications.},
  file = {/home/wonseok/Zotero/storage/IFBYCUSY/Hu 등 - 2024 - Improving large language models for clinical named.pdf}
}

@online{huLoRALowRankAdaptation2021,
  title = {{{LoRA}}: {{Low-Rank Adaptation}} of {{Large Language Models}}},
  shorttitle = {{{LoRA}}},
  author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  date = {2021-10-16},
  eprint = {2106.09685},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2106.09685},
  url = {http://arxiv.org/abs/2106.09685},
  urldate = {2024-01-11},
  abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/wonseok/Zotero/storage/K58D7F67/Hu 등 - 2021 - LoRA Low-Rank Adaptation of Large Language Models.pdf;/home/wonseok/Zotero/storage/4V3CPLXL/2106.html}
}

@online{huLoRALowRankAdaptation2021a,
  title = {{{LoRA}}: {{Low-Rank Adaptation}} of {{Large Language Models}}},
  shorttitle = {{{LoRA}}},
  author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  date = {2021-10-16},
  eprint = {2106.09685},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2106.09685},
  url = {http://arxiv.org/abs/2106.09685},
  urldate = {2024-08-23},
  abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/wonseok/Zotero/storage/FAL5DYJT/Hu 등 - 2021 - LoRA Low-Rank Adaptation of Large Language Models.pdf;/home/wonseok/Zotero/storage/VZKRHPRA/2106.html}
}

@online{ImprovingPatientAccess,
  title = {Improving {{Patient Access}} and {{Comprehension}} of {{Clinical Notes}}: {{Leveraging Large Language Models}} to {{Enhance Readability}} and {{Understanding}}},
  url = {https://dspace.mit.edu/handle/1721.1/152654},
  urldate = {2024-08-07},
  file = {/home/wonseok/Zotero/storage/WCVUWY4X/152654.html}
}

@online{InformationExtractionElectronic,
  title = {Information Extraction from Electronic Medical Documents: State of the Art and Future Research Directions | {{Knowledge}} and {{Information Systems}}},
  url = {https://link.springer.com/article/10.1007/s10115-022-01779-1},
  urldate = {2024-01-30},
  file = {/home/wonseok/Zotero/storage/AVU5KIPF/s10115-022-01779-1.html}
}

@inproceedings{jiangCoRICollectiveRelation2021,
  title = {{{CoRI}}: {{Collective Relation Integration}} with {{Data Augmentation}} for {{Open Information Extraction}}},
  shorttitle = {{{CoRI}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Jiang, Zhengbao and Han, Jialong and Sisman, Bunyamin and Dong, Xin Luna},
  editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
  date = {2021-08},
  pages = {4706--4716},
  publisher = {Association for Computational Linguistics},
  location = {Online},
  doi = {10.18653/v1/2021.acl-long.363},
  url = {https://aclanthology.org/2021.acl-long.363},
  urldate = {2024-08-24},
  abstract = {Integrating extracted knowledge from the Web to knowledge graphs (KGs) can facilitate tasks like question answering. We study relation integration that aims to align free-text relations in subject-relation-object extractions to relations in a target KG. To address the challenge that free-text relations are ambiguous, previous methods exploit neighbor entities and relations for additional context. However, the predictions are made independently, which can be mutually inconsistent. We propose a two-stage Collective Relation Integration (CoRI) model, where the first stage independently makes candidate predictions, and the second stage employs a collective model that accesses all candidate predictions to make globally coherent predictions. We further improve the collective model with augmented data from the portion of the target KG that is otherwise unused. Experiment results on two datasets show that CoRI can significantly outperform the baselines, improving AUC from .677 to .748 and from .716 to .780, respectively.},
  eventtitle = {{{ACL-IJCNLP}} 2021},
  file = {/home/wonseok/Zotero/storage/W5LVWPL4/Jiang 등 - 2021 - CoRI Collective Relation Integration with Data Augmentation for Open Information Extraction.pdf}
}

@online{jiangMedReadMeSystematicStudy2024,
  title = {{{MedReadMe}}: {{A Systematic Study}} for {{Fine-grained Sentence Readability}} in {{Medical Domain}}},
  shorttitle = {{{MedReadMe}}},
  author = {Jiang, Chao and Xu, Wei},
  date = {2024-05-03},
  eprint = {2405.02144},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2405.02144},
  url = {http://arxiv.org/abs/2405.02144},
  urldate = {2024-05-09},
  abstract = {Medical texts are notoriously challenging to read. Properly measuring their readability is the first step towards making them more accessible. In this paper, we present a systematic study on fine-grained readability measurements in the medical domain at both sentence-level and span-level. We introduce a new dataset MedReadMe, which consists of manually annotated readability ratings and fine-grained complex span annotation for 4,520 sentences, featuring two novel "Google-Easy" and "Google-Hard" categories. It supports our quantitative analysis, which covers 650 linguistic features and automatic complex word and jargon identification. Enabled by our high-quality annotation, we benchmark and improve several state-of-the-art sentence-level readability metrics for the medical domain specifically, which include unsupervised, supervised, and prompting-based methods using recently developed large language models (LLMs). Informed by our fine-grained complex span annotation, we find that adding a single feature, capturing the number of jargon spans, into existing readability formulas can significantly improve their correlation with human judgments. We will publicly release the dataset and code.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {/home/wonseok/Zotero/storage/5HYV5RXW/Jiang 그리고 Xu - 2024 - MedReadMe A Systematic Study for Fine-grained Sen.pdf;/home/wonseok/Zotero/storage/CR9IF4ET/Jiang 그리고 Xu - 2024 - MedReadMe A Systematic Study for Fine-grained Sen.pdf;/home/wonseok/Zotero/storage/P6T98IWD/2405.html}
}

@online{jiangMistral7B2023,
  title = {Mistral {{7B}}},
  author = {Jiang, Albert Q. and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and family=Casas, given=Diego, prefix=de las, useprefix=false and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and Lavaud, Lélio Renard and Lachaux, Marie-Anne and Stock, Pierre and Scao, Teven Le and Lavril, Thibaut and Wang, Thomas and Lacroix, Timothée and Sayed, William El},
  date = {2023-10-10},
  eprint = {2310.06825},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2310.06825},
  url = {http://arxiv.org/abs/2310.06825},
  urldate = {2024-08-14},
  abstract = {We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/wonseok/Zotero/storage/9ZIS72IJ/Jiang 등 - 2023 - Mistral 7B.pdf;/home/wonseok/Zotero/storage/KKI7FFGG/2310.html}
}

@article{johnsonMIMICIVFreelyAccessible2023,
  title = {{{MIMIC-IV}}, a Freely Accessible Electronic Health Record Dataset},
  author = {Johnson, Alistair E. W. and Bulgarelli, Lucas and Shen, Lu and Gayles, Alvin and Shammout, Ayad and Horng, Steven and Pollard, Tom J. and Hao, Sicheng and Moody, Benjamin and Gow, Brian and Lehman, Li-wei H. and Celi, Leo A. and Mark, Roger G.},
  date = {2023-01-03},
  journaltitle = {Scientific Data},
  shortjournal = {Sci Data},
  volume = {10},
  number = {1},
  pages = {1},
  publisher = {Nature Publishing Group},
  issn = {2052-4463},
  doi = {10.1038/s41597-022-01899-x},
  url = {https://www.nature.com/articles/s41597-022-01899-x},
  urldate = {2023-12-29},
  abstract = {Digital data collection during routine clinical practice is now ubiquitous within hospitals. The data contains valuable information on the care of patients and their response to treatments, offering exciting opportunities for research. Typically, data are stored within archival systems that are not intended to support research. These systems are often inaccessible to researchers and structured for optimal storage, rather than interpretability and analysis. Here we present MIMIC-IV, a publicly available database sourced from the electronic health record of the Beth Israel Deaconess Medical Center. Information available includes patient measurements, orders, diagnoses, procedures, treatments, and deidentified free-text clinical notes. MIMIC-IV is intended to support a wide array of research studies and educational material, helping to reduce barriers to conducting clinical research.},
  issue = {1},
  langid = {english},
  keywords = {Epidemiology,Health services,Public health},
  file = {/home/wonseok/Zotero/storage/LU4RV98F/Johnson 등 - 2023 - MIMIC-IV, a freely accessible electronic health re.pdf}
}

@online{JournalMedicalInternet,
  title = {Journal of {{Medical Internet Research}} - {{Improving Consumer Understanding}} of {{Medical Text}}: {{Development}} and {{Validation}} of a {{New SubSimplify Algorithm}} to {{Automatically Generate Term Explanations}} in {{English}} and {{Spanish}}},
  url = {https://www.jmir.org/2018/8/e10779/},
  urldate = {2024-08-24}
}

@inproceedings{kartchnerComprehensiveEvaluationBiomedical2023,
  title = {A {{Comprehensive Evaluation}} of {{Biomedical Entity Linking Models}}},
  booktitle = {Proceedings of the 2023 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Kartchner, David and Deng, Jennifer and Lohiya, Shubham and Kopparthi, Tejasri and Bathala, Prasanth and Domingo-Fernández, Daniel and Mitchell, Cassie},
  editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
  date = {2023-12},
  pages = {14462--14478},
  publisher = {Association for Computational Linguistics},
  location = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.893},
  url = {https://aclanthology.org/2023.emnlp-main.893},
  urldate = {2024-04-13},
  abstract = {Biomedical entity linking (BioEL) is the process of connecting entities referenced in documents to entries in biomedical databases such as the Unified Medical Language System (UMLS) or Medical Subject Headings (MeSH). The study objective was to comprehensively evaluate nine recent state-of-the-art biomedical entity linking models under a unified framework. We compare these models along axes of (1) accuracy, (2) speed, (3) ease of use, (4) generalization, and (5) adaptability to new ontologies and datasets. We additionally quantify the impact of various preprocessing choices such as abbreviation detection. Systematic evaluation reveals several notable gaps in current methods. In particular, current methods struggle to correctly link genes and proteins and often have difficulty effectively incorporating context into linking decisions. To expedite future development and baseline testing, we release our unified evaluation framework and all included models on GitHub at https://github.com/davidkartchner/biomedical-entity-linking},
  eventtitle = {{{EMNLP}} 2023},
  file = {/home/wonseok/Zotero/storage/8D2K8F42/Kartchner 등 - 2023 - A Comprehensive Evaluation of Biomedical Entity Li.pdf}
}

@article{kimSyntheticDataImprove2024,
  title = {Synthetic {{Data Improve Survival Status Prediction Models}} in {{Early-Onset Colorectal Cancer}}},
  author = {Kim, Hyunwook and Jang, Won Seok and Sim, Woo Seob and Kim, Han Sang and Choi, Jeong Eun and Baek, Eun Sil and Park, Yu Rang and Shin, Sang Joon},
  date = {2024-01},
  journaltitle = {JCO clinical cancer informatics},
  shortjournal = {JCO Clin Cancer Inform},
  volume = {8},
  eprint = {38271642},
  eprinttype = {pmid},
  pages = {e2300201},
  issn = {2473-4276},
  doi = {10.1200/CCI.23.00201},
  abstract = {PURPOSE: In artificial intelligence-based modeling, working with a limited number of patient groups is challenging. This retrospective study aimed to evaluate whether applying synthetic data generation methods to the clinical data of small patient groups can enhance the performance of prediction models. MATERIALS AND METHODS: A data set collected by the Cancer Registry Library Project from the Yonsei Cancer Center (YCC), Severance Hospital, between January 2008 and October 2020 was reviewed. Patients with colorectal cancer younger than 50 years who started their initial treatment at YCC were included. A Bayesian network-based synthesizing model was used to generate a synthetic data set, combined with the differential privacy (DP) method. RESULTS: A synthetic population of 5,005 was generated from a data set of 1,253 patients with 93 clinical features. The Hellinger distance and correlation difference metric were below 0.3 and 0.5, respectively, indicating no statistical difference. The overall survival by disease stage did not differ between the synthetic and original populations. Training with the synthetic data and validating with the original data showed the highest performances of 0.850, 0.836, and 0.790 for the Decision Tree, Random Forest, and XGBoost models, respectively. Comparison of synthetic data sets with different epsilon parameters from the original data sets showed improved performance {$>$}0.1\%. For extremely small data sets, models using synthetic data outperformed those using only original data sets. The reidentification risk measures demonstrated that the epsilons between 0.1 and 100 fell below the baseline, indicating a preserved privacy state. CONCLUSION: The synthetic data generation approach enhances predictive modeling performance by maintaining statistical and clinical integrity, and simultaneously reduces privacy risks through the application of DP techniques.},
  langid = {english},
  pmcid = {PMC10830088},
  keywords = {Artificial Intelligence,Bayes Theorem,Colorectal Neoplasms,Hospitals,Humans,Retrospective Studies}
}

@online{kimVerifiNERVerificationaugmentedNER2024,
  title = {{{VerifiNER}}: {{Verification-augmented NER}} via {{Knowledge-grounded Reasoning}} with {{Large Language Models}}},
  shorttitle = {{{VerifiNER}}},
  author = {Kim, Seoyeon and Seo, Kwangwook and Chae, Hyungjoo and Yeo, Jinyoung and Lee, Dongha},
  date = {2024-02-28},
  eprint = {2402.18374},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2402.18374},
  urldate = {2024-03-25},
  abstract = {Recent approaches in domain-specific named entity recognition (NER), such as biomedical NER, have shown remarkable advances. However, they still lack of faithfulness, producing erroneous predictions. We assume that knowledge of entities can be useful in verifying the correctness of the predictions. Despite the usefulness of knowledge, resolving such errors with knowledge is nontrivial, since the knowledge itself does not directly indicate the ground-truth label. To this end, we propose VerifiNER, a post-hoc verification framework that identifies errors from existing NER methods using knowledge and revises them into more faithful predictions. Our framework leverages the reasoning abilities of large language models to adequately ground on knowledge and the contextual information in the verification process. We validate effectiveness of VerifiNER through extensive experiments on biomedical datasets. The results suggest that VerifiNER can successfully verify errors from existing models as a model-agnostic approach. Further analyses on out-of-domain and low-resource settings show the usefulness of VerifiNER on real-world applications.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {/home/wonseok/Zotero/storage/WSBL527D/Kim 등 - 2024 - VerifiNER Verification-augmented NER via Knowledg.pdf;/home/wonseok/Zotero/storage/VCNS6DML/2402.html}
}

@article{kloehnImprovingConsumerUnderstanding2018,
  title = {Improving {{Consumer Understanding}} of {{Medical Text}}: {{Development}} and {{Validation}} of a {{New SubSimplify Algorithm}} to {{Automatically Generate Term Explanations}} in {{English}} and {{Spanish}}},
  shorttitle = {Improving {{Consumer Understanding}} of {{Medical Text}}},
  author = {Kloehn, Nicholas and Leroy, Gondy and Kauchak, David and Gu, Yang and Colina, Sonia and Yuan, Nicole P. and Revere, Debra},
  date = {2018-08-02},
  journaltitle = {Journal of Medical Internet Research},
  volume = {20},
  number = {8},
  pages = {e10779},
  publisher = {JMIR Publications Inc., Toronto, Canada},
  doi = {10.2196/10779},
  url = {https://www.jmir.org/2018/8/e10779},
  urldate = {2024-08-24},
  abstract = {Background: While health literacy is important for people to maintain good health and manage diseases, medical educational texts are often written beyond the reading level of the average individual. To mitigate this disconnect, text simplification research provides methods to increase readability and, therefore, comprehension. One method of text simplification is to isolate particularly difficult terms within a document and replace them with easier synonyms (lexical simplification) or an explanation in plain language (semantic simplification). Unfortunately, existing dictionaries are seldom complete, and consequently, resources for many difficult terms are unavailable. This is the case for English and Spanish resources. Objective: Our objective was to automatically generate explanations for difficult terms in both English and Spanish when they are not covered by existing resources. The system we present combines existing resources for explanation generation using a novel algorithm (SubSimplify) to create additional explanations. Methods: SubSimplify uses word-level parsing techniques and specialized medical affix dictionaries to identify the morphological units of a term and then source their definitions. While the underlying resources are different, SubSimplify applies the same principles in both languages. To evaluate our approach, we used term familiarity to identify difficult terms in English and Spanish and then generated explanations for them. For each language, we extracted 400 difficult terms from two different article types (General and Medical topics) balanced for frequency. For English terms, we compared SubSimplify’s explanation with the explanations from the Consumer Health Vocabulary, WordNet Synonyms and Summaries, as well as Word Embedding Vector (WEV) synonyms. For Spanish terms, we compared the explanation to WordNet Summaries and WEV Embedding synonyms. We evaluated quality, coverage, and usefulness for the simplification provided for each term. Quality is the average score from two subject experts on a 1-4 Likert scale (two per language) for the synonyms or explanations provided by the source. Coverage is the number of terms for which a source could provide an explanation. Usefulness is the same expert score, however, with a 0 assigned when no explanations or synonyms were available for a term. Results: SubSimplify resulted in quality scores of 1.64 for English (P{$<$}.001) and 1.49 for Spanish (P{$<$}.001), which were lower than those of existing resources (Consumer Health Vocabulary [CHV]=2.81). However, in coverage, SubSimplify outperforms all existing written resources, increasing the coverage from 53.0\% to 80.5\% in English and from 20.8\% to 90.8\% in Spanish (P{$<$}.001). This result means that the usefulness score of SubSimplify (1.32; P{$<$}.001) is greater than that of most existing resources (eg, CHV=0.169). Conclusions: Our approach is intended as an additional resource to existing, manually created resources. It greatly increases the number of difficult terms for which an easier alternative can be made available, resulting in greater actual usefulness.},
  langid = {english},
  file = {/home/wonseok/Zotero/storage/6Z6DSACP/Kloehn 등 - 2018 - Improving Consumer Understanding of Medical Text Development and Validation of a New SubSimplify Al.pdf;/home/wonseok/Zotero/storage/6S4SJIX5/e10779.html}
}

@article{kloehnImprovingConsumerUnderstanding2018a,
  title = {Improving {{Consumer Understanding}} of {{Medical Text}}: {{Development}} and {{Validation}} of a {{New SubSimplify Algorithm}} to {{Automatically Generate Term Explanations}} in {{English}} and {{Spanish}}},
  shorttitle = {Improving {{Consumer Understanding}} of {{Medical Text}}},
  author = {Kloehn, Nicholas and Leroy, Gondy and Kauchak, David and Gu, Yang and Colina, Sonia and Yuan, Nicole P. and Revere, Debra},
  date = {2018-08-02},
  journaltitle = {Journal of Medical Internet Research},
  volume = {20},
  number = {8},
  pages = {e10779},
  publisher = {JMIR Publications Inc., Toronto, Canada},
  doi = {10.2196/10779},
  url = {https://www.jmir.org/2018/8/e10779},
  urldate = {2024-08-24},
  abstract = {Background: While health literacy is important for people to maintain good health and manage diseases, medical educational texts are often written beyond the reading level of the average individual. To mitigate this disconnect, text simplification research provides methods to increase readability and, therefore, comprehension. One method of text simplification is to isolate particularly difficult terms within a document and replace them with easier synonyms (lexical simplification) or an explanation in plain language (semantic simplification). Unfortunately, existing dictionaries are seldom complete, and consequently, resources for many difficult terms are unavailable. This is the case for English and Spanish resources. Objective: Our objective was to automatically generate explanations for difficult terms in both English and Spanish when they are not covered by existing resources. The system we present combines existing resources for explanation generation using a novel algorithm (SubSimplify) to create additional explanations. Methods: SubSimplify uses word-level parsing techniques and specialized medical affix dictionaries to identify the morphological units of a term and then source their definitions. While the underlying resources are different, SubSimplify applies the same principles in both languages. To evaluate our approach, we used term familiarity to identify difficult terms in English and Spanish and then generated explanations for them. For each language, we extracted 400 difficult terms from two different article types (General and Medical topics) balanced for frequency. For English terms, we compared SubSimplify’s explanation with the explanations from the Consumer Health Vocabulary, WordNet Synonyms and Summaries, as well as Word Embedding Vector (WEV) synonyms. For Spanish terms, we compared the explanation to WordNet Summaries and WEV Embedding synonyms. We evaluated quality, coverage, and usefulness for the simplification provided for each term. Quality is the average score from two subject experts on a 1-4 Likert scale (two per language) for the synonyms or explanations provided by the source. Coverage is the number of terms for which a source could provide an explanation. Usefulness is the same expert score, however, with a 0 assigned when no explanations or synonyms were available for a term. Results: SubSimplify resulted in quality scores of 1.64 for English (P{$<$}.001) and 1.49 for Spanish (P{$<$}.001), which were lower than those of existing resources (Consumer Health Vocabulary [CHV]=2.81). However, in coverage, SubSimplify outperforms all existing written resources, increasing the coverage from 53.0\% to 80.5\% in English and from 20.8\% to 90.8\% in Spanish (P{$<$}.001). This result means that the usefulness score of SubSimplify (1.32; P{$<$}.001) is greater than that of most existing resources (eg, CHV=0.169). Conclusions: Our approach is intended as an additional resource to existing, manually created resources. It greatly increases the number of difficult terms for which an easier alternative can be made available, resulting in greater actual usefulness.},
  langid = {english},
  file = {/home/wonseok/Zotero/storage/ATQ5BUJY/Kloehn 등 - 2018 - Improving Consumer Understanding of Medical Text Development and Validation of a New SubSimplify Al.pdf;/home/wonseok/Zotero/storage/MZAM3DDH/e10779.html}
}

@online{labrakBioMistralCollectionOpenSource2024,
  title = {{{BioMistral}}: {{A Collection}} of {{Open-Source Pretrained Large Language Models}} for {{Medical Domains}}},
  shorttitle = {{{BioMistral}}},
  author = {Labrak, Yanis and Bazoge, Adrien and Morin, Emmanuel and Gourraud, Pierre-Antoine and Rouvier, Mickael and Dufour, Richard},
  date = {2024-07-17},
  eprint = {2402.10373},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2402.10373},
  url = {http://arxiv.org/abs/2402.10373},
  urldate = {2024-08-14},
  abstract = {Large Language Models (LLMs) have demonstrated remarkable versatility in recent years, offering potential applications across specialized domains such as healthcare and medicine. Despite the availability of various open-source LLMs tailored for health contexts, adapting general-purpose LLMs to the medical domain presents significant challenges. In this paper, we introduce BioMistral, an open-source LLM tailored for the biomedical domain, utilizing Mistral as its foundation model and further pre-trained on PubMed Central. We conduct a comprehensive evaluation of BioMistral on a benchmark comprising 10 established medical question-answering (QA) tasks in English. We also explore lightweight models obtained through quantization and model merging approaches. Our results demonstrate BioMistral's superior performance compared to existing open-source medical models and its competitive edge against proprietary counterparts. Finally, to address the limited availability of data beyond English and to assess the multilingual generalization of medical LLMs, we automatically translated and evaluated this benchmark into 7 other languages. This marks the first large-scale multilingual evaluation of LLMs in the medical domain. Datasets, multilingual evaluation benchmarks, scripts, and all the models obtained during our experiments are freely released.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/wonseok/Zotero/storage/XF4CRG3S/Labrak 등 - 2024 - BioMistral A Collection of Open-Source Pretrained.pdf;/home/wonseok/Zotero/storage/E5ZZJRWI/2402.html}
}

@article{lalorEvaluatingEffectivenessNoteAid2021,
  title = {Evaluating the {{Effectiveness}} of {{NoteAid}} in a {{Community Hospital Setting}}: {{Randomized Trial}} of {{Electronic Health Record Note Comprehension Interventions With Patients}}},
  shorttitle = {Evaluating the {{Effectiveness}} of {{NoteAid}} in a {{Community Hospital Setting}}},
  author = {Lalor, John P. and Hu, Wen and Tran, Matthew and Wu, Hao and Mazor, Kathleen M. and Yu, Hong},
  date = {2021-05-13},
  journaltitle = {Journal of Medical Internet Research},
  volume = {23},
  number = {5},
  pages = {e26354},
  publisher = {JMIR Publications Inc., Toronto, Canada},
  doi = {10.2196/26354},
  url = {https://www.jmir.org/2021/5/e26354},
  urldate = {2024-12-20},
  abstract = {Background: Interventions to define medical jargon have been shown to improve electronic health record (EHR) note comprehension among crowdsourced participants on Amazon Mechanical Turk (AMT). However, AMT participants may not be representative of the general population or patients who are most at-risk for low health literacy. Objective: In this work, we assessed the efficacy of an intervention (NoteAid) for EHR note comprehension among participants in a community hospital setting. Methods: Participants were recruited from Lowell General Hospital (LGH), a community hospital in Massachusetts, to take the ComprehENotes test, a web-based test of EHR note comprehension. Participants were randomly assigned to control (n=85) or intervention (n=89) groups to take the test without or with NoteAid, respectively. For comparison, we used a sample of 200 participants recruited from AMT to take the ComprehENotes test (100 in the control group and 100 in the intervention group). Results: A total of 174 participants were recruited from LGH, and 200 participants were recruited from AMT. Participants in both intervention groups (community hospital and AMT) scored significantly higher than participants in the control groups (P\&lt;.001). The average score for the community hospital participants was significantly lower than the average score for the AMT participants (P\&lt;.001), consistent with the lower education levels in the community hospital sample. Education level had a significant effect on scores for the community hospital participants (P\&lt;.001). Conclusions: Use of NoteAid was associated with significantly improved EHR note comprehension in both community hospital and AMT samples. Our results demonstrate the generalizability of ComprehENotes as a test of EHR note comprehension and the effectiveness of NoteAid for improving EHR note comprehension.},
  langid = {english},
  file = {/home/wonseok/Zotero/storage/GCP2DTEU/Lalor 등 - 2021 - Evaluating the Effectiveness of NoteAid in a Community Hospital Setting Randomized Trial of Electro.pdf;/home/wonseok/Zotero/storage/9EQTVFRQ/authors.html}
}

@article{landolsiInformationExtractionElectronic2023,
  title = {Information Extraction from Electronic Medical Documents: State of the Art and Future Research Directions},
  shorttitle = {Information Extraction from Electronic Medical Documents},
  author = {Landolsi, Mohamed Yassine and Hlaoua, Lobna and Ben~Romdhane, Lotfi},
  date = {2023-02-01},
  journaltitle = {Knowledge and Information Systems},
  shortjournal = {Knowl Inf Syst},
  volume = {65},
  number = {2},
  pages = {463--516},
  issn = {0219-3116},
  doi = {10.1007/s10115-022-01779-1},
  url = {https://doi.org/10.1007/s10115-022-01779-1},
  urldate = {2024-04-11},
  abstract = {In the medical field, a doctor must have a comprehensive knowledge by reading and writing narrative documents, and he is responsible for every decision he takes for patients. Unfortunately, it is very tiring to read all necessary information about drugs, diseases and patients due to the large amount of documents that are increasing every day. Consequently, so many medical errors can happen and even kill people. Likewise, there is such an important field that can handle this problem, which is the information extraction. There are several important tasks in this field to extract the important and desired information from unstructured text written in natural language. The main principal tasks are named entity recognition and relation extraction since they can structure the text by extracting the relevant information. However, in order to treat the narrative text we should use natural language processing techniques to extract useful information and features. In our paper, we introduce and discuss the several techniques and solutions used in these tasks. Furthermore, we outline the challenges in information extraction from medical documents. In our knowledge, this is the most comprehensive survey in the literature with an experimental analysis and a suggestion for some uncovered directions.},
  langid = {english},
  keywords = {Electronic medical records,Information extraction,Medical named entities recognition,Medical relation extraction,Section detection},
  file = {/home/wonseok/Zotero/storage/GKSWVLRF/Landolsi 등 - 2023 - Information extraction from electronic medical doc.pdf}
}

@article{leeBioBERTPretrainedBiomedical2020,
  title = {{{BioBERT}}: A Pre-Trained Biomedical Language Representation Model for Biomedical Text Mining},
  shorttitle = {{{BioBERT}}},
  author = {Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo},
  date = {2020-02-15},
  journaltitle = {Bioinformatics},
  volume = {36},
  number = {4},
  eprint = {1901.08746},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {1234--1240},
  issn = {1367-4803, 1367-4811},
  doi = {10.1093/bioinformatics/btz682},
  url = {http://arxiv.org/abs/1901.08746},
  urldate = {2024-12-24},
  abstract = {Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows. With the progress in natural language processing (NLP), extracting valuable information from biomedical literature has gained popularity among researchers, and deep learning has boosted the development of effective biomedical text mining models. However, directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora. In this article, we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora. We introduce BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining), which is a domain-specific language representation model pre-trained on large-scale biomedical corpora. With almost the same architecture across tasks, BioBERT largely outperforms BERT and previous state-of-the-art models in a variety of biomedical text mining tasks when pre-trained on biomedical corpora. While BERT obtains performance comparable to that of previous state-of-the-art models, BioBERT significantly outperforms them on the following three representative biomedical text mining tasks: biomedical named entity recognition (0.62\% F1 score improvement), biomedical relation extraction (2.80\% F1 score improvement) and biomedical question answering (12.24\% MRR improvement). Our analysis results show that pre-training BERT on biomedical corpora helps it to understand complex biomedical texts. We make the pre-trained weights of BioBERT freely available at https://github.com/naver/biobert-pretrained, and the source code for fine-tuning BioBERT available at https://github.com/dmis-lab/biobert.},
  keywords = {Computer Science - Computation and Language},
  file = {/home/wonseok/Zotero/storage/LV2EUNSI/Lee 등 - 2020 - BioBERT a pre-trained biomedical language representation model for biomedical text mining.pdf;/home/wonseok/Zotero/storage/34ASFWK6/1901.html}
}

@article{leroyTextAudioSimplification,
  title = {Text and {{Audio Simplification}}: {{Human}} vs. {{ChatGPT}}},
  author = {Leroy, Gondy},
  abstract = {Text and audio simplification to increase information comprehension are important in healthcare. With the introduction of ChatGPT, evaluation of its simplification performance is needed. We provide a systematic comparison of human and ChatGPT simplified texts using fourteen metrics indicative of text difficulty. We briefly introduce our online editor where these simplification tools, including ChatGPT, are available. We scored twelve corpora using our metrics: six text, one audio, and five ChatGPT simplified corpora (using five different prompts). We then compare these corpora with texts simplified and verified in a prior user study. Finally, a medical domain expert evaluated the user study texts and five, new ChatGPT simplified versions. We found that simple corpora show higher similarity with the human simplified texts. ChatGPT simplification moves metrics in the right direction. The medical domain expert’s evaluation showed a preference for the ChatGPT style, but the text itself was rated lower for content retention.},
  langid = {english},
  file = {/home/wonseok/Zotero/storage/3DPMEH75/Leroy - Text and Audio Simplification Human vs. ChatGPT.pdf}
}

@online{linBiomedicalEntityLinking2024,
  title = {Biomedical {{Entity Linking}} as {{Multiple Choice Question Answering}}},
  author = {Lin, Zhenxi and Zhang, Ziheng and Wu, Xian and Zheng, Yefeng},
  date = {2024-02-23},
  eprint = {2402.15189},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2402.15189},
  url = {http://arxiv.org/abs/2402.15189},
  urldate = {2024-04-13},
  abstract = {Although biomedical entity linking (BioEL) has made significant progress with pre-trained language models, challenges still exist for fine-grained and long-tailed entities. To address these challenges, we present BioELQA, a novel model that treats Biomedical Entity Linking as Multiple Choice Question Answering. BioELQA first obtains candidate entities with a fast retriever, jointly presents the mention and candidate entities to a generator, and then outputs the predicted symbol associated with its chosen entity. This formulation enables explicit comparison of different candidate entities, thus capturing fine-grained interactions between mentions and entities, as well as among entities themselves. To improve generalization for long-tailed entities, we retrieve similar labeled training instances as clues and concatenate the input with retrieved instances for the generator. Extensive experimental results show that BioELQA outperforms state-of-the-art baselines on several datasets.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/wonseok/Zotero/storage/5PCPU4Y4/Lin 등 - 2024 - Biomedical Entity Linking as Multiple Choice Quest.pdf;/home/wonseok/Zotero/storage/K3728NYF/2402.html}
}

@online{linImprovingBiomedicalEntity2023,
  title = {Improving {{Biomedical Entity Linking}} with {{Retrieval-enhanced Learning}}},
  author = {Lin, Zhenxi and Zhang, Ziheng and Wu, Xian and Zheng, Yefeng},
  date = {2023-12-15},
  eprint = {2312.09806},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2312.09806},
  url = {http://arxiv.org/abs/2312.09806},
  urldate = {2024-04-13},
  abstract = {Biomedical entity linking (BioEL) has achieved remarkable progress with the help of pre-trained language models. However, existing BioEL methods usually struggle to handle rare and difficult entities due to long-tailed distribution. To address this limitation, we introduce a new scheme \$k\$NN-BioEL, which provides a BioEL model with the ability to reference similar instances from the entire training corpus as clues for prediction, thus improving the generalization capabilities. Moreover, we design a contrastive learning objective with dynamic hard negative sampling (DHNS) that improves the quality of the retrieved neighbors during inference. Extensive experimental results show that \$k\$NN-BioEL outperforms state-of-the-art baselines on several datasets.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/wonseok/Zotero/storage/EGHUNUC8/Lin 등 - 2023 - Improving Biomedical Entity Linking with Retrieval.pdf;/home/wonseok/Zotero/storage/Q28JG64Q/2312.html}
}

@online{liScopingReviewUsing2024,
  title = {A Scoping Review of Using {{Large Language Models}} ({{LLMs}}) to Investigate {{Electronic Health Records}} ({{EHRs}})},
  author = {Li, Lingyao and Zhou, Jiayan and Gao, Zhenxiang and Hua, Wenyue and Fan, Lizhou and Yu, Huizi and Hagen, Loni and Zhang, Yongfeng and Assimes, Themistocles L. and Hemphill, Libby and Ma, Siyuan},
  date = {2024-05-22},
  eprint = {2405.03066},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2405.03066},
  url = {http://arxiv.org/abs/2405.03066},
  urldate = {2024-08-07},
  abstract = {Electronic Health Records (EHRs) play an important role in the healthcare system. However, their complexity and vast volume pose significant challenges to data interpretation and analysis. Recent advancements in Artificial Intelligence (AI), particularly the development of Large Language Models (LLMs), open up new opportunities for researchers in this domain. Although prior studies have demonstrated their potential in language understanding and processing in the context of EHRs, a comprehensive scoping review is lacking. This study aims to bridge this research gap by conducting a scoping review based on 329 related papers collected from OpenAlex. We first performed a bibliometric analysis to examine paper trends, model applications, and collaboration networks. Next, we manually reviewed and categorized each paper into one of the seven identified topics: named entity recognition, information extraction, text similarity, text summarization, text classification, dialogue system, and diagnosis and prediction. For each topic, we discussed the unique capabilities of LLMs, such as their ability to understand context, capture semantic relations, and generate human-like text. Finally, we highlighted several implications for researchers from the perspectives of data resources, prompt engineering, fine-tuning, performance measures, and ethical concerns. In conclusion, this study provides valuable insights into the potential of LLMs to transform EHR research and discusses their applications and ethical considerations.},
  pubstate = {prepublished},
  keywords = {Computer Science - Emerging Technologies},
  file = {/home/wonseok/Zotero/storage/JFIY62LZ/Li 등 - 2024 - A scoping review of using Large Language Models (L.pdf;/home/wonseok/Zotero/storage/Q8GAR2NV/2405.html}
}

@article{liTwoDirectionsClinical2023,
  title = {Two {{Directions}} for {{Clinical Data Generation}} with {{Large Language Models}}: {{Data-to-Label}} and {{Label-to-Data}}},
  shorttitle = {Two {{Directions}} for {{Clinical Data Generation}} with {{Large Language Models}}},
  author = {Li, Rumeng and Wang, Xun and Yu, Hong},
  date = {2023-12},
  journaltitle = {Proceedings of the Conference on Empirical Methods in Natural Language Processing. Conference on Empirical Methods in Natural Language Processing},
  shortjournal = {Proc Conf Empir Methods Nat Lang Process},
  volume = {2023},
  eprint = {38213944},
  eprinttype = {pmid},
  pages = {7129--7143},
  doi = {10.18653/v1/2023.findings-emnlp.474},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10782150/},
  urldate = {2025-01-22},
  abstract = {Large language models (LLMs) can generate natural language texts for various domains and tasks, but their potential for clinical text mining, a domain with scarce, sensitive, and imbalanced medical data, is under-explored. We investigate whether LLMs can augment clinical data for detecting Alzheimer’s Disease (AD)-related signs and symptoms from electronic health records (EHRs), a challenging task that requires high expertise. We create a novel pragmatic taxonomy for AD sign and symptom progression based on expert knowledge and generated three datasets: (1) a gold dataset annotated by human experts on longitudinal EHRs of AD patients; (2) a silver dataset created by the data-to-label method, which labels sentences from a public EHR collection with AD-related signs and symptoms; and (3) a bronze dataset created by the label-to-data method which generates sentences with AD-related signs and symptoms based on the label definition. We train a system to detect AD-related signs and symptoms from EHRs. We find that the silver and bronze datasets improves the system performance, outperforming the system using only the gold dataset. This shows that LLMs can generate synthetic clinical data for a complex task by incorporating expert knowledge, and our label-to-data method can produce datasets that are free of sensitive information, while maintaining acceptable quality.},
  pmcid = {PMC10782150},
  file = {/home/wonseok/Zotero/storage/RXFZQQMG/Li 등 - 2023 - Two Directions for Clinical Data Generation with Large Language Models Data-to-Label and Label-to-D.pdf}
}

@article{liuEvaluatingMedicalEntity2024,
  title = {Evaluating {{Medical Entity Recognition}} in {{Health Care}}: {{Entity Model Quantitative Study}}},
  shorttitle = {Evaluating {{Medical Entity Recognition}} in {{Health Care}}},
  author = {Liu, Shengyu and Wang, Anran and Xiu, Xiaolei and Zhong, Ming and Wu, Sizhu},
  date = {2024-10-17},
  journaltitle = {JMIR Medical Informatics},
  volume = {12},
  number = {1},
  pages = {e59782},
  publisher = {JMIR Publications Inc., Toronto, Canada},
  doi = {10.2196/59782},
  url = {https://medinform.jmir.org/2024/1/e59782},
  urldate = {2024-12-23},
  abstract = {Background: Named entity recognition (NER) models are essential for extracting structured information from unstructured medical texts by identifying entities such as diseases, treatments, and conditions, enhancing clinical decision-making and research. Innovations in machine learning, particularly those involving Bidirectional Encoder Representations From Transformers (BERT)–based deep learning and large language models, have significantly advanced NER capabilities. However, their performance varies across medical datasets due to the complexity and diversity of medical terminology. Previous studies have often focused on overall performance, neglecting specific challenges in medical contexts and the impact of macrofactors like lexical composition on prediction accuracy. These gaps hinder the development of optimized NER models for medical applications. Objective: This study aims to meticulously evaluate the performance of various NER models in the context of medical text analysis, focusing on how complex medical terminology affects entity recognition accuracy. Additionally, we explored the influence of macrofactors on model performance, seeking to provide insights for refining NER models and enhancing their reliability for medical applications. Methods: This study comprehensively evaluated 7 NER models—hidden Markov models, conditional random fields, BERT for Biomedical Text Mining, Big Transformer Models for Efficient Long-Sequence Attention, Decoding-enhanced BERT with Disentangled Attention, Robustly Optimized BERT Pretraining Approach, and Gemma—across 3 medical datasets: Revised Joint Workshop on Natural Language Processing in Biomedicine and its Applications (JNLPBA), BioCreative V CDR, and Anatomical Entity Mention (AnatEM). The evaluation focused on prediction accuracy, resource use (eg, central processing unit and graphics processing unit use), and the impact of fine-tuning hyperparameters. The macrofactors affecting model performance were also screened using the multilevel factor elimination algorithm. Results: The fine-tuned BERT for Biomedical Text Mining, with balanced resource use, generally achieved the highest prediction accuracy across the Revised JNLPBA and AnatEM datasets, with microaverage (AVG\_MICRO) scores of 0.932 and 0.8494, respectively, highlighting its superior proficiency in identifying medical entities. Gemma, fine-tuned using the low-rank adaptation technique, achieved the highest accuracy on the BioCreative V CDR dataset with an AVG\_MICRO score of 0.9962 but exhibited variability across the other datasets (AVG\_MICRO scores of 0.9088 on the Revised JNLPBA and 0.8029 on AnatEM), indicating a need for further optimization. In addition, our analysis revealed that 2 macrofactors, entity phrase length and the number of entity words in each entity phrase, significantly influenced model performance. Conclusions: This study highlights the essential role of NER models in medical informatics, emphasizing the imperative for model optimization via precise data targeting and fine-tuning. The insights from this study will notably improve clinical decision-making and facilitate the creation of more sophisticated and effective medical NER models.},
  langid = {english},
  file = {/home/wonseok/Zotero/storage/ZRCPTQL8/e59782.html}
}

@online{liuLearningBiomedicalInformation2016,
  title = {Learning for {{Biomedical Information Extraction}}: {{Methodological Review}} of {{Recent Advances}}},
  shorttitle = {Learning for {{Biomedical Information Extraction}}},
  author = {Liu, Feifan and Chen, Jinying and Jagannatha, Abhyuday and Yu, Hong},
  date = {2016-06-26},
  eprint = {1606.07993},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1606.07993},
  urldate = {2024-04-13},
  abstract = {Biomedical information extraction (BioIE) is important to many applications, including clinical decision support, integrative biology, and pharmacovigilance, and therefore it has been an active research. Unlike existing reviews covering a holistic view on BioIE, this review focuses on mainly recent advances in learning based approaches, by systematically summarizing them into different aspects of methodological development. In addition, we dive into open information extraction and deep learning, two emerging and influential techniques and envision next generation of BioIE.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {/home/wonseok/Zotero/storage/Z2SNAK2N/Liu 등 - 2016 - Learning for Biomedical Information Extraction Me.pdf;/home/wonseok/Zotero/storage/PUTYXL4H/1606.html}
}

@online{liuMoreCatastrophicForgetting2024,
  title = {More {{Than Catastrophic Forgetting}}: {{Integrating General Capabilities For Domain-Specific LLMs}}},
  shorttitle = {More {{Than Catastrophic Forgetting}}},
  author = {Liu, Chengyuan and Wang, Shihang and Kang, Yangyang and Qing, Lizhi and Zhao, Fubang and Sun, Changlong and Kuang, Kun and Wu, Fei},
  date = {2024-05-28},
  eprint = {2405.17830},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2405.17830},
  url = {http://arxiv.org/abs/2405.17830},
  urldate = {2024-08-27},
  abstract = {The performance on general tasks decreases after Large Language Models (LLMs) are fine-tuned on domain-specific tasks, the phenomenon is known as Catastrophic Forgetting (CF). However, this paper presents a further challenge for real application of domain-specific LLMs beyond CF, called General Capabilities Integration (GCI), which necessitates the integration of both the general capabilities and domain knowledge within a single instance. The objective of GCI is not merely to retain previously acquired general capabilities alongside new domain knowledge, but to harmonize and utilize both sets of skills in a cohesive manner to enhance performance on domain-specific tasks. Taking legal domain as an example, we carefully design three groups of training and testing tasks without lacking practicability, and construct the corresponding datasets. To better incorporate general capabilities across domain-specific scenarios, we introduce ALoRA, which utilizes a multi-head attention module upon LoRA, facilitating direct information transfer from preceding tokens to the current one. This enhancement permits the representation to dynamically switch between domain-specific knowledge and general competencies according to the attention. Extensive experiments are conducted on the proposed tasks. The results exhibit the significance of our setting, and the effectiveness of our method.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {/home/wonseok/Zotero/storage/5JTCKT4M/Liu 등 - 2024 - More Than Catastrophic Forgetting Integrating General Capabilities For Domain-Specific LLMs.pdf;/home/wonseok/Zotero/storage/ZESAB2HC/2405.html}
}

@online{liuWhatMakesGood2021,
  title = {What {{Makes Good In-Context Examples}} for {{GPT-}}\$3\$?},
  author = {Liu, Jiachang and Shen, Dinghan and Zhang, Yizhe and Dolan, Bill and Carin, Lawrence and Chen, Weizhu},
  date = {2021-01-17},
  eprint = {2101.06804},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2101.06804},
  url = {http://arxiv.org/abs/2101.06804},
  urldate = {2024-10-29},
  abstract = {GPT-\$3\$ has attracted lots of attention due to its superior performance across a wide range of NLP tasks, especially with its powerful and versatile in-context few-shot learning ability. Despite its success, we found that the empirical results of GPT-\$3\$ depend heavily on the choice of in-context examples. In this work, we investigate whether there are more effective strategies for judiciously selecting in-context examples (relative to random sampling) that better leverage GPT-\$3\$'s few-shot capabilities. Inspired by the recent success of leveraging a retrieval module to augment large-scale neural network models, we propose to retrieve examples that are semantically-similar to a test sample to formulate its corresponding prompt. Intuitively, the in-context examples selected with such a strategy may serve as more informative inputs to unleash GPT-\$3\$'s extensive knowledge. We evaluate the proposed approach on several natural language understanding and generation benchmarks, where the retrieval-based prompt selection approach consistently outperforms the random baseline. Moreover, it is observed that the sentence encoders fine-tuned on task-related datasets yield even more helpful retrieval results. Notably, significant gains are observed on tasks such as table-to-text generation (41.9\% on the ToTTo dataset) and open-domain question answering (45.5\% on the NQ dataset). We hope our investigation could help understand the behaviors of GPT-\$3\$ and large-scale pre-trained LMs in general and enhance their few-shot capabilities.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {/home/wonseok/Zotero/storage/2BWZ57A5/Liu 등 - 2021 - What Makes Good In-Context Examples for GPT-$3$.pdf;/home/wonseok/Zotero/storage/5BJ3K2GT/2101.html}
}

@inproceedings{lossio-venturaClinicalNamedentityRecognition2019,
  title = {Clinical Named-Entity Recognition: {{A}} Short Comparison},
  shorttitle = {Clinical Named-Entity Recognition},
  booktitle = {2019 {{IEEE International Conference}} on {{Bioinformatics}} and {{Biomedicine}} ({{BIBM}})},
  author = {Lossio-Ventura, Juan Antonio and Boussard, Sebastien and Morzan, Juandiego and Hernandez-Boussard, Tina},
  date = {2019-11},
  pages = {1548--1550},
  doi = {10.1109/BIBM47256.2019.8983406},
  url = {https://ieeexplore.ieee.org/document/8983406?denied=},
  urldate = {2024-03-23},
  abstract = {The adoption of electronic health records has increased the volume of clinical data, which has opened an opportunity for healthcare research. There are several biomedical annotation systems that have been used to facilitate the analysis of clinical data. However, there is a lack of clinical annotation comparisons to select the most suitable tool for a specific clinical task. In this work, we used clinical notes from the MIMIC-III database and evaluated three annotation systems to identify four types of entities: (1) procedure, (2) disorder, (3) drug, and (4) anatomy. Our preliminary results demonstrate that BioPortal performs well when extracting disorder and drug. This can provide clinical researchers with real-clinical insights into patient's health patterns and it may allow to create a first version of an annotated dataset.},
  eventtitle = {2019 {{IEEE International Conference}} on {{Bioinformatics}} and {{Biomedicine}} ({{BIBM}})},
  keywords = {Annotations,Clamps,clinical research,Conferences,Databases,Drugs,electronic health records,Medical services,MIMICs,named-entity recognition,natural language processing},
  file = {/home/wonseok/Zotero/storage/JL9PBFWZ/Lossio-Ventura 등 - 2019 - Clinical named-entity recognition A short compari.pdf;/home/wonseok/Zotero/storage/QHVFCX9C/8983406.html}
}

@inproceedings{maniClinicalPhraseMining2020,
  title = {Clinical {{Phrase Mining}} with {{Language Models}}},
  booktitle = {2020 {{IEEE International Conference}} on {{Bioinformatics}} and {{Biomedicine}} ({{BIBM}})},
  author = {Mani, Kaushik and Yue, Xiang and Gutierrez, Bernal Jimenez and Huang, Yungui and Lin, Simon and Sun, Huan},
  date = {2020-12},
  pages = {1087--1090},
  doi = {10.1109/BIBM49941.2020.9313496},
  url = {https://ieeexplore.ieee.org/document/9313496},
  urldate = {2024-03-23},
  abstract = {A vast amount of vital clinical data is available within unstructured texts such as discharge summaries and procedure notes in Electronic Medical Records (EMRs). Automatically transforming such unstructured data into structured units is crucial for effective data analysis in the field of clinical informatics. Recognizing phrases that reveal important medical information in a concise and thorough manner is a fundamental step in this process. Existing systems that are built for opendomain texts are designed to detect mostly non-medical phrases, while tools designed specifically for extracting concepts from clinical texts are not scalable to large corpora and often leave out essential context surrounding those detected clinical concepts. We address these issues by proposing a framework, CliniPhrase, which adapts domain-specific deep neural network based language models (such as ClinicalBERT) to effectively and efficiently extract high-quality phrases from clinical documents with a limited amount of training data. Experimental results on the MIMIC-III dataset show that our method can outperform the current state-of-the-art techniques by up to 18\% in terms of F1 measure while being very efficient (up to 48 times faster).},
  eventtitle = {2020 {{IEEE International Conference}} on {{Bioinformatics}} and {{Biomedicine}} ({{BIBM}})},
  keywords = {BERT,Biological system modeling,Bit error rate,Clinical NLP,Clinical Phrase Mining,Clinical Text,Data mining,Feature extraction,Language Models,MIMICs,Tools,Training},
  file = {/home/wonseok/Zotero/storage/53LFYPZ6/Mani 등 - 2020 - Clinical Phrase Mining with Language Models.pdf}
}

@article{mannhardtImprovingPatientAccess,
  title = {Improving {{Patient Access}} and {{Comprehension}} of {{Clinical Notes}}: {{Leveraging Large Language Models}} to {{Enhance Readability}} and {{Understanding}}},
  author = {Mannhardt, Niklas},
  abstract = {Patient access to clinical notes has demonstrated numerous benefits, including an increased sense of control over their condition, enhanced engagement, improved medication adherence, and greater clinician accountability. However, the presence of medical jargon, abbreviations, and complex medical concepts within clinical notes hinders patient comprehension, thus diminishing the positive effects of note accessibility. These notes, primarily intended for clinicians, often appear disorganized and contain an abundance of technical terms. Breast cancer patients, in particular, face information overload and experience taxing symptoms related to their treatment, exacerbating this issue. Although some clinicians are adapting their writing style to meet patients’ needs, time constraints limit the feasibility of comprehensive note-taking. We propose the development of a patient-facing tool, in the form of a web application, to make information contained in clinical notes more accessible by leveraging machine learning models to simplify, summarize, extract information from, and add context to clinical notes. Through a series of user studies, we demonstrate that our proposed augmentations to clinical notes significantly improve comprehension and enhance patients’ reading experience.},
  langid = {english},
  file = {/home/wonseok/Zotero/storage/I3DSASHR/Mannhardt - Improving Patient Access and Comprehension of Clin.pdf}
}

@inproceedings{mcinnesUMLSSimilarityMeasuring2013,
  title = {{{UMLS}}::{{Similarity}}: {{Measuring}} the {{Relatedness}} and {{Similarity}} of {{Biomedical Concepts}}},
  shorttitle = {{{UMLS}}},
  booktitle = {Proceedings of the 2013 {{NAACL HLT Demonstration Session}}},
  author = {McInnes, Bridget and Pedersen, Ted and Pakhomov, Serguei and Liu, Ying and Melton-Meaux, Genevieve},
  editor = {Dyer, Chris and Higgins, Derrick},
  date = {2013-06},
  pages = {28--31},
  publisher = {Association for Computational Linguistics},
  location = {Atlanta, Georgia},
  url = {https://aclanthology.org/N13-3007},
  urldate = {2024-04-12},
  file = {/home/wonseok/Zotero/storage/PMM6NFCF/McInnes 등 - 2013 - UMLSSimilarity Measuring the Relatedness and Si.pdf}
}

@article{medelyanDomainindependentAutomaticKeyphrase2008,
  title = {Domain-Independent Automatic Keyphrase Indexing with Small Training Sets},
  author = {Medelyan, Olena and Witten, Ian H.},
  date = {2008},
  journaltitle = {Journal of the American Society for Information Science and Technology},
  volume = {59},
  number = {7},
  pages = {1026--1040},
  issn = {1532-2890},
  doi = {10.1002/asi.20790},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/asi.20790},
  urldate = {2024-04-12},
  abstract = {Keyphrases are widely used in both physical and digital libraries as a brief, but precise, summary of documents. They help organize material based on content, provide thematic access, represent search results, and assist with navigation. Manual assignment is expensive because trained human indexers must reach an understanding of the document and select appropriate descriptors according to defined cataloging rules. We propose a new method that enhances automatic keyphrase extraction by using semantic information about terms and phrases gleaned from a domain-specific thesaurus. The key advantage of the new approach is that it performs well with very little training data. We evaluate it on a large set of manually indexed documents in the domain of agriculture, compare its consistency with a group of six professional indexers, and explore its performance on smaller collections of documents in other domains and of French and Spanish documents.},
  langid = {english},
  file = {/home/wonseok/Zotero/storage/KPYW2UML/Medelyan 그리고 Witten - 2008 - Domain-independent automatic keyphrase indexing wi.pdf;/home/wonseok/Zotero/storage/6GLB8NI9/asi.html}
}

@article{mensaEditorialInformationExtraction2023,
  title = {Editorial: {{Information}} Extraction for Health Documents},
  shorttitle = {Editorial},
  author = {Mensa, Enrico and Martínez Fernández, Paloma and Roller, Roland and Radicioni, Daniele P.},
  date = {2023},
  journaltitle = {Frontiers in Artificial Intelligence},
  volume = {6},
  issn = {2624-8212},
  url = {https://www.frontiersin.org/articles/10.3389/frai.2023.1224529},
  urldate = {2024-01-30},
  file = {/home/wonseok/Zotero/storage/NIM2RS6Z/Mensa 등 - 2023 - Editorial Information extraction for health docum.pdf}
}

@online{michalopoulosUmlsBERTClinicalDomain2021,
  title = {{{UmlsBERT}}: {{Clinical Domain Knowledge Augmentation}} of {{Contextual Embeddings Using}} the {{Unified Medical Language System Metathesaurus}}},
  shorttitle = {{{UmlsBERT}}},
  author = {Michalopoulos, George and Wang, Yuanxin and Kaka, Hussam and Chen, Helen and Wong, Alexander},
  date = {2021-06-03},
  eprint = {2010.10391},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2010.10391},
  url = {http://arxiv.org/abs/2010.10391},
  urldate = {2024-04-12},
  abstract = {Contextual word embedding models, such as BioBERT and Bio\_ClinicalBERT, have achieved state-of-the-art results in biomedical natural language processing tasks by focusing their pre-training process on domain-specific corpora. However, such models do not take into consideration expert domain knowledge. In this work, we introduced UmlsBERT, a contextual embedding model that integrates domain knowledge during the pre-training process via a novel knowledge augmentation strategy. More specifically, the augmentation on UmlsBERT with the Unified Medical Language System (UMLS) Metathesaurus was performed in two ways: i) connecting words that have the same underlying `concept' in UMLS, and ii) leveraging semantic group knowledge in UMLS to create clinically meaningful input embeddings. By applying these two strategies, UmlsBERT can encode clinical domain knowledge into word embeddings and outperform existing domain-specific models on common named-entity recognition (NER) and clinical natural language inference clinical NLP tasks.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/wonseok/Zotero/storage/AFSJNN29/Michalopoulos 등 - 2021 - UmlsBERT Clinical Domain Knowledge Augmentation o.pdf;/home/wonseok/Zotero/storage/32MSUBKV/2010.html}
}

@online{mitraSynthSBDHSyntheticDataset2024,
  title = {Synth-{{SBDH}}: {{A Synthetic Dataset}} of {{Social}} and {{Behavioral Determinants}} of {{Health}} for {{Clinical Text}}},
  shorttitle = {Synth-{{SBDH}}},
  author = {Mitra, Avijit and Druhl, Emily and Goodwin, Raelene and Yu, Hong},
  date = {2024-10-30},
  eprint = {2406.06056},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2406.06056},
  url = {http://arxiv.org/abs/2406.06056},
  urldate = {2025-01-24},
  abstract = {Social and behavioral determinants of health (SBDH) play a crucial role in health outcomes and are frequently documented in clinical text. Automatically extracting SBDH information from clinical text relies on publicly available good-quality datasets. However, existing SBDH datasets exhibit substantial limitations in their availability and coverage. In this study, we introduce Synth-SBDH, a novel synthetic dataset with detailed SBDH annotations, encompassing status, temporal information, and rationale across 15 SBDH categories. We showcase the utility of Synth-SBDH on three tasks using real-world clinical datasets from two distinct hospital settings, highlighting its versatility, generalizability, and distillation capabilities. Models trained on Synth-SBDH consistently outperform counterparts with no Synth-SBDH training, achieving up to 63.75\% macro-F improvements. Additionally, Synth-SBDH proves effective for rare SBDH categories and under-resource constraints while being substantially cheaper than expert-annotated real-world data. Human evaluation reveals a 71.06\% Human-LLM alignment and uncovers areas for future refinements.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {/home/wonseok/Zotero/storage/XVJ775E5/Mitra 등 - 2024 - Synth-SBDH A Synthetic Dataset of Social and Behavioral Determinants of Health for Clinical Text.pdf;/home/wonseok/Zotero/storage/99BYSN6G/2406.html}
}

@online{monajatipoorLLMsBiomedicineStudy2024a,
  title = {{{LLMs}} in {{Biomedicine}}: {{A}} Study on Clinical {{Named Entity Recognition}}},
  shorttitle = {{{LLMs}} in {{Biomedicine}}},
  author = {Monajatipoor, Masoud and Yang, Jiaxin and Stremmel, Joel and Emami, Melika and Mohaghegh, Fazlolah and Rouhsedaghat, Mozhdeh and Chang, Kai-Wei},
  date = {2024-07-11},
  eprint = {2404.07376},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2404.07376},
  url = {http://arxiv.org/abs/2404.07376},
  urldate = {2024-12-24},
  abstract = {Large Language Models (LLMs) demonstrate remarkable versatility in various NLP tasks but encounter distinct challenges in biomedical due to the complexities of language and data scarcity. This paper investigates LLMs application in the biomedical domain by exploring strategies to enhance their performance for the NER task. Our study reveals the importance of meticulously designed prompts in the biomedical. Strategic selection of in-context examples yields a marked improvement, offering \textasciitilde 15-20\textbackslash\% increase in F1 score across all benchmark datasets for biomedical few-shot NER. Additionally, our results indicate that integrating external biomedical knowledge via prompting strategies can enhance the proficiency of general-purpose LLMs to meet the specialized needs of biomedical NER. Leveraging a medical knowledge base, our proposed method, DiRAG, inspired by Retrieval-Augmented Generation (RAG), can boost the zero-shot F1 score of LLMs for biomedical NER. Code is released at \textbackslash url\{https://github.com/masoud-monajati/LLM\_Bio\_NER\}},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {/home/wonseok/Zotero/storage/RPLVIWS5/Monajatipoor 등 - 2024 - LLMs in Biomedicine A study on clinical Named Entity Recognition.pdf;/home/wonseok/Zotero/storage/KH9MFVNM/2404.html}
}

@article{mullerOpenAccessMedical2019,
  title = {An Open Access Medical Knowledge Base for Community Driven Diagnostic Decision Support System Development},
  author = {Müller, Lars and Gangadharaiah, Rashmi and Klein, Simone C. and Perry, James and Bernstein, Greg and Nurkse, David and Wailes, Dustin and Graham, Rishi and El-Kareh, Robert and Mehta, Sanjay and Vinterbo, Staal A. and Aronoff-Spencer, Eliah},
  date = {2019-04-27},
  journaltitle = {BMC Medical Informatics and Decision Making},
  shortjournal = {BMC Medical Informatics and Decision Making},
  volume = {19},
  number = {1},
  pages = {93},
  issn = {1472-6947},
  doi = {10.1186/s12911-019-0804-1},
  url = {https://doi.org/10.1186/s12911-019-0804-1},
  urldate = {2024-07-06},
  abstract = {While early diagnostic decision support systems were built around knowledge bases, more recent systems employ machine learning to consume large amounts of health data. We argue curated knowledge bases will remain an important component of future diagnostic decision support systems by providing ground truth and facilitating explainable human-computer interaction, but that prototype development is hampered by the lack of freely available computable knowledge bases.},
  keywords = {Decision support systems clinical (D020000),Diagnosis differential (D003937),Knowledge bases (D051188)},
  file = {/home/wonseok/Zotero/storage/5XQXKVE4/Müller 등 - 2019 - An open access medical knowledge base for communit.pdf;/home/wonseok/Zotero/storage/EHZ4FGHB/s12911-019-0804-1.html}
}

@article{nIMPACTMEDICALTERMINOLOGY2018,
  title = {{{IMPACT OF MEDICAL TERMINOLOGY ON PATIENTS}}' {{COMPREHENSION OF HEALTHCARE}}},
  author = {N, Derevianchenko and O, Lytovska and D, Diurba and I, Leshchyna},
  date = {2018-11},
  journaltitle = {Georgian medical news},
  number = {284},
  eprint = {30618411},
  eprinttype = {pmid},
  publisher = {Georgian Med News},
  issn = {1512-0112},
  url = {https://pubmed.ncbi.nlm.nih.gov/30618411/},
  urldate = {2024-08-24},
  abstract = {Modern medical practice require close communication both doctors and patients. Development of medicine, especially seen in past decades, promoted changes in medical procedures and documentation, i.e. development of more accurate and valuable informed consent, which is an important part of treatment …},
  langid = {english},
  file = {/home/wonseok/Zotero/storage/GADSD3DU/30618411.html}
}

@article{novaGenerativeAIHealthcare2023,
  title = {Generative {{AI}} in {{Healthcare}}: {{Advancements}} in {{Electronic Health Records}}, Facilitating {{Medical Languages}}, and {{Personalized Patient Care}}},
  shorttitle = {Generative {{AI}} in {{Healthcare}}},
  author = {Nova, Kannan},
  date = {2023-04-04},
  journaltitle = {Journal of Advanced Analytics in Healthcare Management},
  volume = {7},
  number = {1},
  pages = {115--131},
  url = {https://research.tensorgate.org/index.php/JAAHM/article/view/43},
  urldate = {2024-08-07},
  abstract = {This research explores the application of generative AI techniques in healthcare to address three significant areas: enhancing electronic health records (EHRs) through automated conversation summarization, simplifying complex medical language into patient-friendly summaries, and providing personalized care recommendations using data from smartwatches and wearables. In the first part, we propose a technical framework for utilizing generative AI to listen to conversations during healthcare appointments and generate concise summaries for inclusion in EHRs. The process involves speech recognition, natural language processing (NLP), named entity recognition (NER), contextual understanding, text summarization, and seamless integration with EHR systems. The implementation of such a system requires rigorous evaluation, training data, and adherence to healthcare regulations. The second part focuses on simplifying complex medical language into summaries that patients can understand. We present a technical sequence flow that involves data collection, preprocessing, training data preparation, model selection, architecture, training, evaluation, fine-tuning, deployment, user interaction, summary generation, output presentation, and feedback iteration. By employing generative AI models trained on medical documents, patients can access simplified and understandable summaries, improving patient education and communication in healthcare settings. Lastly, we explore the utilization of generative AI for personalized care recommendations using data from smartwatches and wearables. Its technical sequence flow encompasses data collection, data transfer to the cloud, data preprocessing, data analysis with generative AI, personalized care recommendations, delivery of recommendations, user interaction, and feedback. By analyzing sensor data, generative AI models can generate personalized recommendations for exercise, diet, sleep, and medication, enhancing individualized care for users.},
  issue = {1},
  langid = {english},
  keywords = {),Conversation summarization,Electronic health record (EHR),Generative AI,Named Entity Recognition (NER),Natural Language Processing (NLP),Personalized care recommendations,Speech recognition},
  file = {/home/wonseok/Zotero/storage/TCYCK62P/Nova - 2023 - Generative AI in Healthcare Advancements in Elect.pdf}
}

@online{parnamiLearningFewExamples2022,
  title = {Learning from {{Few Examples}}: {{A Summary}} of {{Approaches}} to {{Few-Shot Learning}}},
  shorttitle = {Learning from {{Few Examples}}},
  author = {Parnami, Archit and Lee, Minwoo},
  date = {2022-03-07},
  eprint = {2203.04291},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2203.04291},
  url = {http://arxiv.org/abs/2203.04291},
  urldate = {2024-09-02},
  abstract = {Few-Shot Learning refers to the problem of learning the underlying pattern in the data just from a few training samples. Requiring a large number of data samples, many deep learning solutions suffer from data hunger and extensively high computation time and resources. Furthermore, data is often not available due to not only the nature of the problem or privacy concerns but also the cost of data preparation. Data collection, preprocessing, and labeling are strenuous human tasks. Therefore, few-shot learning that could drastically reduce the turnaround time of building machine learning applications emerges as a low-cost solution. This survey paper comprises a representative list of recently proposed few-shot learning algorithms. Given the learning dynamics and characteristics, the approaches to few-shot learning problems are discussed in the perspectives of meta-learning, transfer learning, and hybrid approaches (i.e., different variations of the few-shot learning problem).},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/wonseok/Zotero/storage/3L782A5B/Parnami 및 Lee - 2022 - Learning from Few Examples A Summary of Approaches to Few-Shot Learning.pdf;/home/wonseok/Zotero/storage/CAUWTCXI/2203.html}
}

@online{perezBiomedicalTermNormalization2018,
  title = {Biomedical Term Normalization of {{EHRs}} with {{UMLS}}},
  author = {Perez, Naiara and Cuadros, Montse and Rigau, German},
  date = {2018-05-24},
  eprint = {1802.02870},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1802.02870},
  urldate = {2024-03-21},
  abstract = {This paper presents a novel prototype for biomedical term normalization of electronic health record excerpts with the Unified Medical Language System (UMLS) Metathesaurus. Despite being multilingual and cross-lingual by design, we first focus on processing clinical text in Spanish because there is no existing tool for this language and for this specific purpose. The tool is based on Apache Lucene to index the Metathesaurus and generate mapping candidates from input text. It uses the IXA pipeline for basic language processing and resolves ambiguities with the UKB toolkit. It has been evaluated by measuring its agreement with MetaMap in two English-Spanish parallel corpora. In addition, we present a web-based interface for the tool.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {/home/wonseok/Zotero/storage/PJMZQWN4/Perez 등 - 2018 - Biomedical term normalization of EHRs with UMLS.pdf;/home/wonseok/Zotero/storage/XG4A8NWE/1802.html}
}

@article{pivovarovLearningProbabilisticPhenotypes2015,
  title = {Learning Probabilistic Phenotypes from Heterogeneous {{EHR}} Data},
  author = {Pivovarov, Rimma and Perotte, Adler J. and Grave, Edouard and Angiolillo, John and Wiggins, Chris H. and Elhadad, Noémie},
  date = {2015-12},
  journaltitle = {Journal of Biomedical Informatics},
  shortjournal = {J Biomed Inform},
  volume = {58},
  eprint = {26464024},
  eprinttype = {pmid},
  pages = {156--165},
  issn = {1532-0480},
  doi = {10.1016/j.jbi.2015.10.001},
  abstract = {We present the Unsupervised Phenome Model (UPhenome), a probabilistic graphical model for large-scale discovery of computational models of disease, or phenotypes. We tackle this challenge through the joint modeling of a large set of diseases and a large set of clinical observations. The observations are drawn directly from heterogeneous patient record data (notes, laboratory tests, medications, and diagnosis codes), and the diseases are modeled in an unsupervised fashion. We apply UPhenome to two qualitatively different mixtures of patients and diseases: records of extremely sick patients in the intensive care unit with constant monitoring, and records of outpatients regularly followed by care providers over multiple years. We demonstrate that the UPhenome model can learn from these different care settings, without any additional adaptation. Our experiments show that (i) the learned phenotypes combine the heterogeneous data types more coherently than baseline LDA-based phenotypes; (ii) they each represent single diseases rather than a mix of diseases more often than the baseline ones; and (iii) when applied to unseen patient records, they are correlated with the patients' ground-truth disorders. Code for training, inference, and quantitative evaluation is made available to the research community.},
  langid = {english},
  pmcid = {PMC8025140},
  keywords = {Clinical phenotype modeling,Computational disease models,Electronic health record,Electronic Health Records,Humans,Learning,Medical information systems,Phenotype,Phenotyping,Probabilistic modeling,Probability},
  file = {/home/wonseok/Zotero/storage/UL55F6HX/Pivovarov 등 - 2015 - Learning probabilistic phenotypes from heterogeneous EHR data.pdf}
}

@online{ramaseshAnatomyCatastrophicForgetting2020,
  title = {Anatomy of {{Catastrophic Forgetting}}: {{Hidden Representations}} and {{Task Semantics}}},
  shorttitle = {Anatomy of {{Catastrophic Forgetting}}},
  author = {Ramasesh, Vinay V. and Dyer, Ethan and Raghu, Maithra},
  date = {2020-07-14},
  eprint = {2007.07400},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2007.07400},
  url = {http://arxiv.org/abs/2007.07400},
  urldate = {2024-08-29},
  abstract = {A central challenge in developing versatile machine learning systems is catastrophic forgetting: a model trained on tasks in sequence will suffer significant performance drops on earlier tasks. Despite the ubiquity of catastrophic forgetting, there is limited understanding of the underlying process and its causes. In this paper, we address this important knowledge gap, investigating how forgetting affects representations in neural network models. Through representational analysis techniques, we find that deeper layers are disproportionately the source of forgetting. Supporting this, a study of methods to mitigate forgetting illustrates that they act to stabilize deeper layers. These insights enable the development of an analytic argument and empirical picture relating the degree of forgetting to representational similarity between tasks. Consistent with this picture, we observe maximal forgetting occurs for task sequences with intermediate similarity. We perform empirical studies on the standard split CIFAR-10 setup and also introduce a novel CIFAR-100 based task approximating realistic input distribution shift.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/wonseok/Zotero/storage/ZL2LNRHH/Ramasesh 등 - 2020 - Anatomy of Catastrophic Forgetting Hidden Representations and Task Semantics.pdf;/home/wonseok/Zotero/storage/MRMH2TV9/2007.html}
}

@online{renAnalyzingReducingCatastrophic2024,
  title = {Analyzing and {{Reducing Catastrophic Forgetting}} in {{Parameter Efficient Tuning}}},
  author = {Ren, Weijieying and Li, Xinlong and Wang, Lei and Zhao, Tianxiang and Qin, Wei},
  date = {2024-02-29},
  eprint = {2402.18865},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2402.18865},
  url = {http://arxiv.org/abs/2402.18865},
  urldate = {2024-08-27},
  abstract = {Existing research has shown that large language models (LLMs) exhibit remarkable performance in language understanding and generation. However, when LLMs are continuously fine-tuned on complex and diverse domain-specific downstream tasks, the inference performance on historical tasks decreases dramatically, which is known as a catastrophic forgetting problem. A trade-off needs to be kept between learning plasticity and memory stability. Plenty of existing works have explored strategies like memory replay, regularization and parameter isolation, but little is known about the geometric connection of various adjacent minima in the continual LLMs fine-tuning scenarios. In this work, we investigate the geometric connections of different minima through the lens of mode connectivity, which means different minima can be connected by a low-loss valley. Through extensive experiments, we uncover the mode connectivity phenomenon in the LLMs continual learning scenario and find that it can strike a balance between plasticity and stability. Building upon these findings, we propose a simple yet effective method called Interpolation-based LoRA (I-LoRA), which constructs a dual-memory experience replay framework based on LoRA parameter interpolations. Extensive experiments and analysis on eight domain-specific CL benchmarks demonstrate that I-LoRA consistently show significant improvement over the previous state-of-the-art approaches with up to \$11\textbackslash\%\$ performance gains, providing a strong baseline and insights for future research on the large language model continual learning problem. Our code is available at \textbackslash url\{https://github.com/which47/LLMCL\}.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/wonseok/Zotero/storage/L68R7MST/Ren 등 - 2024 - Analyzing and Reducing Catastrophic Forgetting in Parameter Efficient Tuning.pdf;/home/wonseok/Zotero/storage/ZJJUBJIW/2402.html}
}

@article{santosUseArtificialIntelligence2023,
  title = {The Use of Artificial Intelligence for Automating or Semi-Automating Biomedical Literature Analyses: {{A}} Scoping Review},
  shorttitle = {The Use of Artificial Intelligence for Automating or Semi-Automating Biomedical Literature Analyses},
  author = {family=Santos, given=Álisson Oliveira, prefix=dos, useprefix=false and family=Silva, given=Eduardo Sergio, prefix=da, useprefix=true and Couto, Letícia Machado and Reis, Gustavo Valadares Labanca and Belo, Vinícius Silva},
  date = {2023-06-01},
  journaltitle = {Journal of Biomedical Informatics},
  shortjournal = {Journal of Biomedical Informatics},
  volume = {142},
  pages = {104389},
  issn = {1532-0464},
  doi = {10.1016/j.jbi.2023.104389},
  url = {https://www.sciencedirect.com/science/article/pii/S1532046423001107},
  urldate = {2024-07-06},
  abstract = {Objective Evidence-based medicine (EBM) is a decision-making process based on the conscious and judicious use of the best available scientific evidence. However, the exponential increase in the amount of information currently available likely exceeds the capacity of human-only analysis. In this context, artificial intelligence (AI) and its branches such as machine learning (ML) can be used to facilitate human efforts in analyzing the literature to foster EBM. The present scoping review aimed to examine the use of AI in the automation of biomedical literature survey and analysis with a view to establishing the state-of-the-art and identifying knowledge gaps. Materials and methods Comprehensive searches of the main databases were performed for articles published up to June 2022 and studies were selected according to inclusion and exclusion criteria. Data were extracted from the included articles and the findings categorized. Results The total number of records retrieved from the databases was 12,145, of which 273 were included in the review. Classification of the studies according to the use of AI in evaluating the biomedical literature revealed three main application groups, namely assembly of scientific evidence (n~=~127; 47\%), mining the biomedical literature (n~=~112; 41\%) and quality analysis (n~=~34; 12\%). Most studies addressed the preparation of systematic reviews, while articles focusing on the development of guidelines and evidence synthesis were the least frequent. The biggest knowledge gap was identified within the quality analysis group, particularly regarding methods and tools that assess the strength of recommendation and consistency of evidence. Conclusion Our review shows that, despite significant progress in the automation of biomedical literature surveys and analyses in recent years, intense research is needed to fill knowledge gaps on more difficult aspects of ML, deep learning and natural language processing, and to consolidate the use of automation by end-users (biomedical researchers and healthcare professionals).},
  keywords = {Deep learning,Evidence-based medicine,Machine learning,Natural language processing,Randomized controlled trials,Systematic reviews},
  file = {/home/wonseok/Zotero/storage/ATZSVKWL/S1532046423001107.html}
}

@article{shortenTextDataAugmentation2021,
  title = {Text {{Data Augmentation}} for {{Deep Learning}}},
  author = {Shorten, Connor and Khoshgoftaar, Taghi M. and Furht, Borko},
  date = {2021-07-19},
  journaltitle = {Journal of Big Data},
  shortjournal = {J Big Data},
  volume = {8},
  number = {1},
  pages = {101},
  issn = {2196-1115},
  doi = {10.1186/s40537-021-00492-0},
  url = {https://doi.org/10.1186/s40537-021-00492-0},
  urldate = {2024-08-24},
  abstract = {Natural Language Processing (NLP) is one of the most captivating applications of Deep Learning. In this survey, we consider how the Data Augmentation training strategy can aid in its development. We begin with the major motifs of Data Augmentation summarized into strengthening local decision boundaries, brute force training, causality and counterfactual examples, and the distinction between meaning and form. We follow these motifs with a concrete list of augmentation frameworks that have been developed for text data. Deep Learning generally struggles with the measurement of generalization and characterization of overfitting. We highlight studies that cover how augmentations can construct test sets for generalization. NLP is at an early stage in applying Data Augmentation compared to Computer Vision. We highlight the key differences and promising ideas that have yet to be tested in NLP. For the sake of practical implementation, we describe tools that facilitate Data Augmentation such as the use of consistency regularization, controllers, and offline and online augmentation pipelines, to preview a few. Finally, we discuss interesting topics around Data Augmentation in NLP such as task-specific augmentations, the use of prior knowledge in self-supervised learning versus Data Augmentation, intersections with transfer and multi-task learning, and ideas for AI-GAs (AI-Generating Algorithms). We hope this paper inspires further research interest in Text Data Augmentation.},
  langid = {english},
  keywords = {Artificial Intelligence,Big Data,Data Augmentation,Natural Language Processing,NLP,Overfitting,Text Data},
  file = {/home/wonseok/Zotero/storage/6EDR4ZG8/Shorten 등 - 2021 - Text Data Augmentation for Deep Learning.pdf}
}

@inproceedings{silvaReviewDocumentInformation2021,
  title = {A {{Review}} on {{Document Information Extraction Approaches}}},
  booktitle = {Proceedings of the {{Student Research Workshop Associated}} with {{RANLP}} 2021},
  author = {Silva, Kanishka and Silva, Thushari},
  editor = {Djabri, Souhila and Gimadi, Dinara and Mihaylova, Tsvetomila and Nikolova-Koleva, Ivelina},
  date = {2021-09},
  pages = {174--179},
  publisher = {INCOMA Ltd.},
  location = {Online},
  url = {https://aclanthology.org/2021.ranlp-srw.24},
  urldate = {2024-01-30},
  abstract = {Information extraction from documents has become great use of novel natural language processing areas. Most of the entity extraction methodologies are variant in a context such as medical area, financial area, also come even limited to the given language. It is better to have one generic approach applicable for any document type to extract entity information regardless of language, context, and structure. Also, another issue in such research is structural analysis while keeping the hierarchical, semantic, and heuristic features. Another problem identified is that usually, it requires a massive training corpus. Therefore, this research focus on mitigating such barriers. Several approaches have been identifying towards building document information extractors focusing on different disciplines. This research area involves natural language processing, semantic analysis, information extraction, and conceptual modelling. This paper presents a review of the information extraction mechanism to construct a generic framework for document extraction with aim of providing a solid base for upcoming research.},
  file = {/home/wonseok/Zotero/storage/E4IBA3J4/Silva 그리고 Silva - 2021 - A Review on Document Information Extraction Approa.pdf}
}

@article{soenksenIntegratedMultimodalArtificial2022,
  title = {Integrated Multimodal Artificial Intelligence Framework for Healthcare Applications},
  author = {Soenksen, Luis R. and Ma, Yu and Zeng, Cynthia and Boussioux, Leonard and Villalobos Carballo, Kimberly and Na, Liangyuan and Wiberg, Holly M. and Li, Michael L. and Fuentes, Ignacio and Bertsimas, Dimitris},
  date = {2022-09-20},
  journaltitle = {npj Digital Medicine},
  shortjournal = {npj Digit. Med.},
  volume = {5},
  number = {1},
  pages = {1--10},
  publisher = {Nature Publishing Group},
  issn = {2398-6352},
  doi = {10.1038/s41746-022-00689-4},
  url = {https://www.nature.com/articles/s41746-022-00689-4},
  urldate = {2024-07-07},
  abstract = {Artificial intelligence (AI) systems hold great promise to improve healthcare over the next decades. Specifically, AI systems leveraging multiple data sources and input modalities are poised to become a viable method to deliver more accurate results and deployable pipelines across a wide range of applications. In this work, we propose and evaluate a unified Holistic AI in Medicine (HAIM) framework to facilitate the generation and testing of AI systems that leverage multimodal inputs. Our approach uses generalizable data pre-processing and machine learning modeling stages that can be readily adapted for research and deployment in healthcare environments. We evaluate our HAIM framework by training and characterizing 14,324 independent models based on HAIM-MIMIC-MM, a multimodal clinical database (N\,=\,34,537 samples) containing 7279 unique hospitalizations and 6485 patients, spanning all possible input combinations of 4 data modalities (i.e., tabular, time-series, text, and images), 11 unique data sources and 12 predictive tasks. We show that this framework can consistently and robustly produce models that outperform similar single-source approaches across various healthcare demonstrations (by 6–33\%), including 10 distinct chest pathology diagnoses, along with length-of-stay and 48\,h mortality predictions. We also quantify the contribution of each modality and data source using Shapley values, which demonstrates the heterogeneity in data modality importance and the necessity of multimodal inputs across different healthcare-relevant tasks. The generalizable properties and flexibility of our Holistic AI in Medicine (HAIM) framework could offer a promising pathway for future multimodal predictive systems in clinical and operational healthcare settings.},
  langid = {english},
  keywords = {Health care,Information technology,Medical research,Scientific data},
  file = {/home/wonseok/Zotero/storage/YPNN9AKQ/Soenksen 등 - 2022 - Integrated multimodal artificial intelligence fram.pdf}
}

@article{speierUsingPhrasesDocument2016,
  title = {Using Phrases and Document Metadata to Improve Topic Modeling of Clinical Reports},
  author = {Speier, William and Ong, Michael K. and Arnold, Corey W.},
  date = {2016-06-01},
  journaltitle = {Journal of Biomedical Informatics},
  shortjournal = {Journal of Biomedical Informatics},
  volume = {61},
  pages = {260--266},
  issn = {1532-0464},
  doi = {10.1016/j.jbi.2016.04.005},
  url = {https://www.sciencedirect.com/science/article/pii/S1532046416300284},
  urldate = {2024-12-22},
  abstract = {Probabilistic topic models provide an unsupervised method for analyzing unstructured text, which have the potential to be integrated into clinical automatic summarization systems. Clinical documents are accompanied by metadata in a patient’s medical history and frequently contains multiword concepts that can be valuable for accurately interpreting the included text. While existing methods have attempted to address these problems individually, we present a unified model for free-text clinical documents that integrates contextual patient- and document-level data, and discovers multi-word concepts. In the proposed model, phrases are represented by chained n-grams and a Dirichlet hyper-parameter is weighted by both document-level and patient-level context. This method and three other Latent Dirichlet allocation models were fit to a large collection of clinical reports. Examples of resulting topics demonstrate the results of the new model and the quality of the representations are evaluated using empirical log likelihood. The proposed model was able to create informative prior probabilities based on patient and document information, and captured phrases that represented various clinical concepts. The representation using the proposed model had a significantly higher empirical log likelihood than the compared methods. Integrating document metadata and capturing phrases in clinical text greatly improves the topic representation of clinical documents. The resulting clinically informative topics may effectively serve as the basis for an automatic summarization system for clinical reports.},
  keywords = {-grams,Document metadata,LDA,Topic modeling},
  file = {/home/wonseok/Zotero/storage/WAN5IPE6/Speier 등 - 2016 - Using phrases and document metadata to improve topic modeling of clinical reports.pdf;/home/wonseok/Zotero/storage/7SFA7TV8/S1532046416300284.html}
}

@article{strobelWhatDidDoctor2024,
  title = {What {{Did}} the {{Doctor Say}}? {{Empowering Patient Comprehension}} with {{Generative AI}}},
  shorttitle = {What {{Did}} the {{Doctor Say}}?},
  author = {Strobel, Gero and Banh, Leonardo},
  date = {2024-06-14},
  journaltitle = {ECIS 2024 Proceedings},
  url = {https://aisel.aisnet.org/ecis2024/track18_healthit/track18_healthit/15}
}

@article{sunTopicModelingClinical2024,
  title = {Topic Modeling on Clinical Social Work Notes for Exploring Social Determinants of Health Factors},
  author = {Sun, Shenghuan and Zack, Travis and Williams, Christopher Y K and Sushil, Madhumita and Butte, Atul J},
  date = {2024-01-14},
  journaltitle = {JAMIA Open},
  shortjournal = {JAMIA Open},
  volume = {7},
  number = {1},
  eprint = {38223407},
  eprinttype = {pmid},
  pages = {ooad112},
  issn = {2574-2531},
  doi = {10.1093/jamiaopen/ooad112},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10788143/},
  urldate = {2024-12-22},
  abstract = {Objective Existing research on social determinants of health (SDoH) predominantly focuses on physician notes and structured data within electronic medical records. This study posits that social work notes are an untapped, potentially rich source for SDoH information. We hypothesize that clinical notes recorded by social workers, whose role is to ameliorate social and economic factors, might provide a complementary information source of data on SDoH compared to physician notes, which primarily concentrate on medical diagnoses and treatments. We aimed to use word frequency analysis and topic modeling to identify prevalent terms and robust topics of discussion within a large cohort of social work notes including both outpatient and in-patient consultations. Materials and methods We retrieved a diverse, deidentified corpus of 0.95 million clinical social work notes from 181{$\mkern1mu$}644 patients at the University of California, San Francisco. We conducted word frequency analysis related to ICD-10 chapters to identify prevalent terms within the notes. We then applied Latent Dirichlet Allocation (LDA) topic modeling analysis to characterize this corpus and identify potential topics of discussion, which was further stratified by note types and disease groups. Results Word frequency analysis primarily identified medical-related terms associated with specific ICD10 chapters, though it also detected some subtle SDoH terms. In contrast, the LDA topic modeling analysis extracted 11 topics explicitly related to social determinants of health risk factors, such as financial status, abuse history, social support, risk of death, and mental health. The topic modeling approach effectively demonstrated variations between different types of social work notes and across patients with different types of diseases or conditions. Discussion Our findings highlight LDA topic modeling’s effectiveness in extracting SDoH-related themes and capturing variations in social work notes, demonstrating its potential for informing targeted interventions for at-risk populations. Conclusion Social work notes offer a wealth of unique and valuable information on an individual’s SDoH. These notes present consistent and meaningful topics of discussion that can be effectively analyzed and utilized to improve patient care and inform targeted interventions for at-risk populations.},
  pmcid = {PMC10788143},
  file = {/home/wonseok/Zotero/storage/PCS3JVH5/Sun 등 - 2024 - Topic modeling on clinical social work notes for exploring social determinants of health factors.pdf}
}

@online{SurveyLargeLanguage,
  title = {A {{Survey}} on {{Large Language Models}} from {{General Purpose}} to {{Medical Applications}}: {{Datasets}}, {{Methodologies}}, and {{Evaluations}}},
  url = {https://arxiv.org/html/2406.10303v1},
  urldate = {2024-08-24},
  file = {/home/wonseok/Zotero/storage/5TTKAWBC/2406.html}
}

@online{tangDoesSyntheticData2023,
  title = {Does {{Synthetic Data Generation}} of {{LLMs Help Clinical Text Mining}}?},
  author = {Tang, Ruixiang and Han, Xiaotian and Jiang, Xiaoqian and Hu, Xia},
  date = {2023-04-10},
  eprint = {2303.04360},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2303.04360},
  url = {http://arxiv.org/abs/2303.04360},
  urldate = {2025-01-24},
  abstract = {Recent advancements in large language models (LLMs) have led to the development of highly potent models like OpenAI's ChatGPT. These models have exhibited exceptional performance in a variety of tasks, such as question answering, essay composition, and code generation. However, their effectiveness in the healthcare sector remains uncertain. In this study, we seek to investigate the potential of ChatGPT to aid in clinical text mining by examining its ability to extract structured information from unstructured healthcare texts, with a focus on biological named entity recognition and relation extraction. However, our preliminary results indicate that employing ChatGPT directly for these tasks resulted in poor performance and raised privacy concerns associated with uploading patients' information to the ChatGPT API. To overcome these limitations, we propose a new training paradigm that involves generating a vast quantity of high-quality synthetic data with labels utilizing ChatGPT and fine-tuning a local model for the downstream task. Our method has resulted in significant improvements in the performance of downstream tasks, improving the F1-score from 23.37\% to 63.99\% for the named entity recognition task and from 75.86\% to 83.59\% for the relation extraction task. Furthermore, generating data using ChatGPT can significantly reduce the time and effort required for data collection and labeling, as well as mitigate data privacy concerns. In summary, the proposed framework presents a promising solution to enhance the applicability of LLM models to clinical text mining.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/wonseok/Zotero/storage/ZBR767YW/Tang 등 - 2023 - Does Synthetic Data Generation of LLMs Help Clinical Text Mining.pdf;/home/wonseok/Zotero/storage/FTY6LRTQ/2303.html}
}

@article{tepeDecodingMedicalJargon2024,
  title = {Decoding Medical Jargon: {{The}} Use of {{AI}} Language Models ({{ChatGPT-4}}, {{BARD}}, Microsoft Copilot) in Radiology Reports},
  shorttitle = {Decoding Medical Jargon},
  author = {Tepe, Murat and Emekli, Emre},
  date = {2024-09-01},
  journaltitle = {Patient Education and Counseling},
  shortjournal = {Patient Education and Counseling},
  volume = {126},
  pages = {108307},
  issn = {0738-3991},
  doi = {10.1016/j.pec.2024.108307},
  url = {https://www.sciencedirect.com/science/article/pii/S0738399124001745},
  urldate = {2024-08-07},
  abstract = {Objective Evaluate Artificial Intelligence (AI) language models (ChatGPT-4, BARD, Microsoft Copilot) in simplifying radiology reports, assessing readability, understandability, actionability, and urgency classification. Methods This study evaluated the effectiveness of these AI models in translating radiology reports into patient-friendly language and providing understandable and actionable suggestions and urgency classifications. Thirty radiology reports were processed using AI tools, and their outputs were assessed for readability (Flesch Reading Ease, Flesch-Kincaid Grade Level), understandability (PEMAT), and the accuracy of urgency classification. ANOVA and Chi-Square tests were performed to compare the models' performances. Results All three AI models successfully transformed medical jargon into more accessible language, with BARD showing superior readability scores. In terms of understandability, all models achieved scores above 70\%, with ChatGPT-4 and BARD leading (p~{$<~$}0.001, both). However, the AI models varied in accuracy of urgency recommendations, with no significant statistical difference (p~=~0.284). Conclusion AI language models have proven effective in simplifying radiology reports, thereby potentially improving patient comprehension and engagement in their health decisions. However, their accuracy in assessing the urgency of medical conditions based on radiology reports suggests a need for further refinement. Practice implications Incorporating AI in radiology communication can empower patients, but further development is crucial for comprehensive and actionable patient support.},
  keywords = {Artificial intelligence,Bard,ChatGPT,Microsoft Copilot,Readability},
  file = {/home/wonseok/Zotero/storage/AT7N7HI8/S0738399124001745.html}
}

@inreference{TfIdf2024,
  title = {Tf–Idf},
  booktitle = {Wikipedia},
  date = {2024-03-17T14:42:52Z},
  url = {https://en.wikipedia.org/w/index.php?title=Tf%E2%80%93idf&oldid=1214201167},
  urldate = {2024-07-20},
  abstract = {In information retrieval, tf–idf (also TF*IDF, TFIDF, TF–IDF, or Tf–idf), short for term frequency–inverse document frequency,  is a measure of importance of a word to a document in a collection or corpus, adjusted for the fact that some words appear more frequently in general. It was often used as a weighting factor in searches of information retrieval, text mining, and user modeling. A survey conducted in 2015 showed that 83\% of text-based recommender systems in digital libraries used tf–idf. Variations of the tf–idf weighting scheme were often used by search engines as a central tool in scoring and ranking a document's relevance given a user query. One of the simplest ranking functions is computed by summing the tf–idf for each query term; many more sophisticated ranking functions are variants of this simple model.},
  langid = {english},
  annotation = {Page Version ID: 1214201167},
  file = {/home/wonseok/Zotero/storage/2FLEVUXB/Tf–idf.html}
}

@online{tranBioInstructInstructionTuning2024,
  title = {{{BioInstruct}}: {{Instruction Tuning}} of {{Large Language Models}} for {{Biomedical Natural Language Processing}}},
  shorttitle = {{{BioInstruct}}},
  author = {Tran, Hieu and Yang, Zhichao and Yao, Zonghai and Yu, Hong},
  date = {2024-06-06},
  eprint = {2310.19975},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2310.19975},
  url = {http://arxiv.org/abs/2310.19975},
  urldate = {2024-08-14},
  abstract = {To enhance the performance of large language models (LLMs) in biomedical natural language processing (BioNLP) by introducing a domain-specific instruction dataset and examining its impact when combined with multi-task learning principles. We created the BioInstruct, comprising 25,005 instructions to instruction-tune LLMs(LLaMA 1 \& 2, 7B \& 13B version). The instructions were created by prompting the GPT-4 language model with three-seed samples randomly drawn from an 80 human curated instructions. We employed Low-Rank Adaptation(LoRA) for parameter-efficient fine-tuning. We then evaluated these instruction-tuned LLMs on several BioNLP tasks, which can be grouped into three major categories: question answering(QA), information extraction(IE), and text generation(GEN). We also examined whether categories(e.g., QA, IE, and generation) of instructions impact model performance. Comparing with LLMs without instruction-tuned, our instruction-tuned LLMs demonstrated marked performance gains: 17.3\% in QA, 5.7\% in IE, and 96\% in Generation tasks. Our 7B-parameter instruction-tuned LLaMA 1 model was competitive or even surpassed other LLMs in the biomedical domain that were also fine-tuned from LLaMA 1 with vast domain-specific data or a variety of tasks. Our results also show that the performance gain is significantly higher when instruction fine-tuning is conducted with closely related tasks. Our findings align with the observations of multi-task learning, suggesting the synergies between two tasks. The BioInstruct dataset serves as a valuable resource and instruction tuned LLMs lead to the best performing BioNLP applications.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/wonseok/Zotero/storage/79FCQKDP/Tran 등 - 2024 - BioInstruct Instruction Tuning of Large Language .pdf;/home/wonseok/Zotero/storage/R6NYSZJL/Tran 등 - 2023 - BioInstruct Instruction Tuning of Large Language .pdf;/home/wonseok/Zotero/storage/38HULF8R/2310.html;/home/wonseok/Zotero/storage/Z3ZWZQ4L/2310.html}
}

@online{tranReadCtrlPersonalizingText2024,
  title = {{{ReadCtrl}}: {{Personalizing}} Text Generation with Readability-Controlled Instruction Learning},
  shorttitle = {{{ReadCtrl}}},
  author = {Tran, Hieu and Yao, Zonghai and Li, Lingxi and Yu, Hong},
  date = {2024-06-13},
  eprint = {2406.09205},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2406.09205},
  url = {http://arxiv.org/abs/2406.09205},
  urldate = {2024-08-07},
  abstract = {Content generation conditioning on users's readability is an important application for personalization. In an era of large language models (LLMs), readability-controlled text generation based on LLMs has become increasingly important. This paper introduces a novel methodology called "Readability-Controlled Instruction Learning (ReadCtrl)," which aims to instruction-tune LLMs to tailor users' readability levels. Unlike the traditional methods, which primarily focused on categorical readability adjustments typically classified as high, medium, and low or expert and layperson levels with limited success, ReadCtrl introduces a dynamic framework that enables LLMs to generate content at various (near continuous level) complexity levels, thereby enhancing their versatility across different applications. Our results show that the ReadCtrl-Mistral-7B models significantly outperformed strong baseline models such as GPT-4 and Claude-3, with a win rate of 52.1\%:35.7\% against GPT-4 in human evaluations. Furthermore, Read-Ctrl has shown significant improvements in automatic evaluations, as evidenced by better readability metrics (e.g., FOG, FKGL) and generation quality metrics (e.g., BLEU, SARI, SummaC-Factuality, UniEval-Consistency and Coherence). These results underscore Read-Ctrl's effectiveness and tenacity in producing high-quality, contextually appropriate outputs that closely align with targeted readability levels, marking a significant advancement in personalized content generation using LLMs.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/wonseok/Zotero/storage/F259T86W/Tran 등 - 2024 - ReadCtrl Personalizing text generation with reada.pdf;/home/wonseok/Zotero/storage/4EZXEF52/2406.html}
}

@online{UMLSCHV,
  title = {{{UMLS}} - {{CHV}}},
  url = {https://www.nlm.nih.gov/research/umls/sourcereleasedocs/current/CHV},
  urldate = {2025-01-03},
  file = {/home/wonseok/Zotero/storage/8JLIYJ9W/CHV.html}
}

@article{UnifiedMedicalLanguage,
  title = {Unified {{Medical Language System}}® ({{UMLS}}®) – {{Basics}}},
  langid = {english},
  file = {/home/wonseok/Zotero/storage/8TQCRRG3/Unified Medical Language System® (UMLS®) – Basics.pdf}
}

@online{UnifiedMedicalLanguagea,
  type = {List of Links},
  title = {Unified {{Medical Language System}} ({{UMLS}})},
  publisher = {U.S. National Library of Medicine},
  url = {https://www.nlm.nih.gov/research/umls/index.html},
  urldate = {2024-09-03},
  abstract = {The UMLS integrates and distributes key terminology, classification and coding standards, and associated resources to promote creation of more effective and interoperable biomedical information systems and services, including electronic health records.},
  langid = {english},
  file = {/home/wonseok/Zotero/storage/DPXNRY29/index.html}
}

@online{vieiraHowMuchData2024,
  title = {How {{Much Data}} Is {{Enough Data}}? {{Fine-Tuning Large Language Models}} for {{In-House Translation}}: {{Performance Evaluation Across Multiple Dataset Sizes}}},
  shorttitle = {How {{Much Data}} Is {{Enough Data}}?},
  author = {Vieira, Inacio and Allred, Will and Lankford, Séamus and Castilho, Sheila and Way, Andy},
  date = {2024-09-10},
  eprint = {2409.03454},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2409.03454},
  url = {http://arxiv.org/abs/2409.03454},
  urldate = {2024-11-08},
  abstract = {Decoder-only LLMs have shown impressive performance in MT due to their ability to learn from extensive datasets and generate high-quality translations. However, LLMs often struggle with the nuances and style required for organisation-specific translation. In this study, we explore the effectiveness of fine-tuning Large Language Models (LLMs), particularly Llama 3 8B Instruct, leveraging translation memories (TMs), as a valuable resource to enhance accuracy and efficiency. We investigate the impact of fine-tuning the Llama 3 model using TMs from a specific organisation in the software sector. Our experiments cover five translation directions across languages of varying resource levels (English to Brazilian Portuguese, Czech, German, Finnish, and Korean). We analyse diverse sizes of training datasets (1k to 207k segments) to evaluate their influence on translation quality. We fine-tune separate models for each training set and evaluate their performance based on automatic metrics, BLEU, chrF++, TER, and COMET. Our findings reveal improvement in translation performance with larger datasets across all metrics. On average, BLEU and COMET scores increase by 13 and 25 points, respectively, on the largest training set against the baseline model. Notably, there is a performance deterioration in comparison with the baseline model when fine-tuning on only 1k and 2k examples; however, we observe a substantial improvement as the training dataset size increases. The study highlights the potential of integrating TMs with LLMs to create bespoke translation models tailored to the specific needs of businesses, thus enhancing translation quality and reducing turn-around times. This approach offers a valuable insight for organisations seeking to leverage TMs and LLMs for optimal translation outcomes, especially in narrower domains.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/wonseok/Zotero/storage/EGB9KBED/Vieira 등 - 2024 - How Much Data is Enough Data Fine-Tuning Large Language Models for In-House Translation Performanc.pdf;/home/wonseok/Zotero/storage/QBWFGB6J/2409.html}
}

@inproceedings{wangFewshotLearningSumerian2022,
  title = {Few-Shot {{Learning}} for {{Sumerian Named Entity Recognition}}},
  booktitle = {Proceedings of the {{Third Workshop}} on {{Deep Learning}} for {{Low-Resource Natural Language Processing}}},
  author = {Wang, Guanghai and Liu, Yudong and Hearne, James},
  editor = {Cherry, Colin and Fan, Angela and Foster, George and Haffari, Gholamreza (Reza) and Khadivi, Shahram and Peng, Nanyun (Violet) and Ren, Xiang and Shareghi, Ehsan and Swayamdipta, Swabha},
  date = {2022-07},
  pages = {136--145},
  publisher = {Association for Computational Linguistics},
  location = {Hybrid},
  doi = {10.18653/v1/2022.deeplo-1.15},
  url = {https://aclanthology.org/2022.deeplo-1.15},
  urldate = {2024-07-16},
  abstract = {This paper presents our study in exploring the task of named entity recognition (NER) in a low resource setting, focusing on few-shot learning on the Sumerian NER task. The Sumerian language is deemed as an extremely low-resource language due to that (1) it is a long dead language, (2) highly skilled language experts are extremely scarce. NER on Sumerian text is important in that it helps identify the actors and entities active in a given period of time from the collections of tens of thousands of texts in building socio-economic networks of the archives of interest. As a text classification task, NER tends to become challenging when the amount of annotated data is limited or the model is required to handle new classes. The Sumerian NER is no exception. In this work, we propose to use two few-shot learning systems, ProtoBERT and NNShot, to the Sumerian NER task. Our experiments show that the ProtoBERT NER generally outperforms both the NNShot NER and the fully supervised BERT NER in low resource settings on the predictions of rare classes. In particular, F1-score of ProtoBERT on unseen entity types on our test set has achieved 89.6\% that is significantly better than the F1-score of 84.3\% of the BERT NER.},
  eventtitle = {{{DeepLo}} 2022},
  file = {/home/wonseok/Zotero/storage/TJXFQ55V/Wang 등 - 2022 - Few-shot Learning for Sumerian Named Entity Recogn.pdf}
}

@online{wangGPTNERNamedEntity2023,
  title = {{{GPT-NER}}: {{Named Entity Recognition}} via {{Large Language Models}}},
  shorttitle = {{{GPT-NER}}},
  author = {Wang, Shuhe and Sun, Xiaofei and Li, Xiaoya and Ouyang, Rongbin and Wu, Fei and Zhang, Tianwei and Li, Jiwei and Wang, Guoyin},
  date = {2023-10-07},
  eprint = {2304.10428},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2304.10428},
  urldate = {2024-03-26},
  abstract = {Despite the fact that large-scale Language Models (LLM) have achieved SOTA performances on a variety of NLP tasks, its performance on NER is still significantly below supervised baselines. This is due to the gap between the two tasks the NER and LLMs: the former is a sequence labeling task in nature while the latter is a text-generation model. In this paper, we propose GPT-NER to resolve this issue. GPT-NER bridges the gap by transforming the sequence labeling task to a generation task that can be easily adapted by LLMs e.g., the task of finding location entities in the input text "Columbus is a city" is transformed to generate the text sequence "@@Columbus\#\# is a city", where special tokens @@\#\# marks the entity to extract. To efficiently address the "hallucination" issue of LLMs, where LLMs have a strong inclination to over-confidently label NULL inputs as entities, we propose a self-verification strategy by prompting LLMs to ask itself whether the extracted entities belong to a labeled entity tag. We conduct experiments on five widely adopted NER datasets, and GPT-NER achieves comparable performances to fully supervised baselines, which is the first time as far as we are concerned. More importantly, we find that GPT-NER exhibits a greater ability in the low-resource and few-shot setups, when the amount of training data is extremely scarce, GPT-NER performs significantly better than supervised models. This demonstrates the capabilities of GPT-NER in real-world NER applications where the number of labeled examples is limited.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {/home/wonseok/Zotero/storage/XR9RE2WJ/Wang 등 - 2023 - GPT-NER Named Entity Recognition via Large Langua.pdf;/home/wonseok/Zotero/storage/I7UCDPHF/2304.html}
}

@online{weiEDAEasyData2019,
  title = {{{EDA}}: {{Easy Data Augmentation Techniques}} for {{Boosting Performance}} on {{Text Classification Tasks}}},
  shorttitle = {{{EDA}}},
  author = {Wei, Jason and Zou, Kai},
  date = {2019-08-25},
  eprint = {1901.11196},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.1901.11196},
  url = {http://arxiv.org/abs/1901.11196},
  urldate = {2024-10-29},
  abstract = {We present EDA: easy data augmentation techniques for boosting performance on text classification tasks. EDA consists of four simple but powerful operations: synonym replacement, random insertion, random swap, and random deletion. On five text classification tasks, we show that EDA improves performance for both convolutional and recurrent neural networks. EDA demonstrates particularly strong results for smaller datasets; on average, across five datasets, training with EDA while using only 50\% of the available training set achieved the same accuracy as normal training with all available data. We also performed extensive ablation studies and suggest parameters for practical use.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {/home/wonseok/Zotero/storage/2BJ7WR69/Wei 및 Zou - 2019 - EDA Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks.pdf;/home/wonseok/Zotero/storage/ZFU4LAPW/1901.html}
}

@online{weiFinetunedLanguageModels2022,
  title = {Finetuned {{Language Models Are Zero-Shot Learners}}},
  author = {Wei, Jason and Bosma, Maarten and Zhao, Vincent Y. and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M. and Le, Quoc V.},
  date = {2022-02-08},
  eprint = {2109.01652},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2109.01652},
  urldate = {2024-08-27},
  abstract = {This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning—finetuning language models on a collection of datasets described via instructions—substantially improves zeroshot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction tune it on over 60 NLP datasets verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 datasets that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {/home/wonseok/Zotero/storage/6DHGMIAA/Wei 등 - 2022 - Finetuned Language Models Are Zero-Shot Learners.pdf}
}

@online{weiFinetunedLanguageModels2022a,
  title = {Finetuned {{Language Models Are Zero-Shot Learners}}},
  author = {Wei, Jason and Bosma, Maarten and Zhao, Vincent Y. and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M. and Le, Quoc V.},
  date = {2022-02-08},
  eprint = {2109.01652},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2109.01652},
  url = {http://arxiv.org/abs/2109.01652},
  urldate = {2024-08-27},
  abstract = {This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {/home/wonseok/Zotero/storage/72S3ET6G/Wei 등 - 2022 - Finetuned Language Models Are Zero-Shot Learners.pdf;/home/wonseok/Zotero/storage/UTS6KVED/2109.html}
}

@article{wenMiningHeterogeneousClinical2021,
  title = {Mining Heterogeneous Clinical Notes by Multi-Modal Latent Topic Model},
  author = {Wen, Zhi and Nair, Pratheeksha and Deng, Chih-Ying and Lu, Xing Han and Moseley, Edward and George, Naomi and Lindvall, Charlotta and Li, Yue},
  year = {2021. 4. 8.},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {16},
  number = {4},
  pages = {e0249622},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0249622},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0249622},
  urldate = {2024-12-22},
  abstract = {Latent knowledge can be extracted from the electronic notes that are recorded during patient encounters with the health system. Using these clinical notes to decipher a patient’s underlying comorbidites, symptom burdens, and treatment courses is an ongoing challenge. Latent topic model as an efficient Bayesian method can be used to model each patient’s clinical notes as “documents” and the words in the notes as “tokens”. However, standard latent topic models assume that all of the notes follow the same topic distribution, regardless of the type of note or the domain expertise of the author (such as doctors or nurses). We propose a novel application of latent topic modeling, using multi-note topic model (MNTM) to jointly infer distinct topic distributions of notes of different types. We applied our model to clinical notes from the MIMIC-III dataset to infer distinct topic distributions over the physician and nursing note types. Based on manual assessments made by clinicians, we observed a significant improvement in topic interpretability using MNTM modeling over the baseline single-note topic models that ignore the note types. Moreover, our MNTM model led to a significantly higher prediction accuracy for prolonged mechanical ventilation and mortality using only the first 48 hours of patient data. By correlating the patients’ topic mixture with hospital mortality and prolonged mechanical ventilation, we identified several diagnostic topics that are associated with poor outcomes. Because of its elegant and intuitive formation, we envision a broad application of our approach in mining multi-modality text-based healthcare information that goes beyond clinical notes. Code available at https://github.com/li-lab-mcgill/heterogeneous\_ehr.},
  langid = {english},
  keywords = {Allied health care professionals,Intensive care units,Language,Nurses,Physicians,Preprocessing,Recurrent neural networks,Semantics},
  file = {/home/wonseok/Zotero/storage/H99AMRJR/Wen 등 - 2021 - Mining heterogeneous clinical notes by multi-modal latent topic model.pdf}
}

@inproceedings{whitehouseLLMpoweredDataAugmentation2023,
  title = {{{LLM-powered Data Augmentation}} for {{Enhanced Cross-lingual Performance}}},
  booktitle = {Proceedings of the 2023 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Whitehouse, Chenxi and Choudhury, Monojit and Aji, Alham Fikri},
  editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
  date = {2023-12},
  pages = {671--686},
  publisher = {Association for Computational Linguistics},
  location = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.44},
  url = {https://aclanthology.org/2023.emnlp-main.44},
  urldate = {2024-07-21},
  abstract = {This paper explores the potential of leveraging Large Language Models (LLMs) for data augmentation in multilingual commonsense reasoning datasets where the available training data is extremely limited. To achieve this, we utilise several LLMs, namely Dolly-v2, StableVicuna, ChatGPT, and GPT-4, to augment three datasets: XCOPA, XWinograd, and XStoryCloze. Subsequently, we evaluate the effectiveness of fine-tuning smaller multilingual models, mBERT and XLMR, using the synthesised data. We compare the performance of training with data generated in English and target languages, as well as translated English-generated data, revealing the overall advantages of incorporating data generated by LLMs, e.g. a notable 13.4 accuracy score improvement for the best case. Furthermore, we conduct a human evaluation by asking native speakers to assess the naturalness and logical coherence of the generated examples across different languages. The results of the evaluation indicate that LLMs such as ChatGPT and GPT-4 excel at producing natural and coherent text in most languages, however, they struggle to generate meaningful text in certain languages like Tamil. We also observe that ChatGPT falls short in generating plausible alternatives compared to the original dataset, whereas examples from GPT-4 exhibit competitive logical consistency.},
  eventtitle = {{{EMNLP}} 2023},
  file = {/home/wonseok/Zotero/storage/8SANHLM8/Whitehouse 등 - 2023 - LLM-powered Data Augmentation for Enhanced Cross-l.pdf}
}

@article{yaoExtractingBiomedicalFactual2023,
  title = {Extracting {{Biomedical Factual Knowledge Using Pretrained Language Model}} and {{Electronic Health Record Context}}},
  author = {Yao, Zonghai and Cao, Yi and Yang, Zhichao and Deshpande, Vijeta and Yu, Hong},
  date = {2023-04-29},
  journaltitle = {AMIA Annual Symposium Proceedings},
  shortjournal = {AMIA Annu Symp Proc},
  volume = {2022},
  eprint = {37128373},
  eprinttype = {pmid},
  pages = {1188--1197},
  issn = {1942-597X},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10148358/},
  urldate = {2024-05-29},
  abstract = {Language Models (LMs) have performed well on biomedical natural language processing applications. In this study, we conducted some experiments to use prompt methods to extract knowledge from LMs as new knowledge Bases (LMs as KBs). However, prompting can only be used as a low bound for knowledge extraction, and perform particularly poorly on biomedical domain KBs. In order to make LMs as KBs more in line with the actual application scenarios of the biomedical domain, we specifically add EHR notes as context to the prompt to improve the low bound in the biomedical domain. We design and validate a series of experiments for our Dynamic-Context-BioLAMA task. Our experiments show that the knowledge possessed by those language models can distinguish the correct knowledge from the noise knowledge in the EHR notes, and such distinguishing ability can also be used as a new metric to evaluate the amount of knowledge possessed by the model.},
  pmcid = {PMC10148358},
  file = {/home/wonseok/Zotero/storage/37TFDQHG/Yao 등 - 2023 - Extracting Biomedical Factual Knowledge Using Pret.pdf}
}

@article{yuanLargeLanguageModels2024,
  title = {Large {{Language Models}} for {{Healthcare Data Augmentation}}: {{An Example}} on {{Patient-Trial Matching}}},
  shorttitle = {Large {{Language Models}} for {{Healthcare Data Augmentation}}},
  author = {Yuan, Jiayi and Tang, Ruixiang and Jiang, Xiaoqian and Hu, Xia},
  date = {2024-01-11},
  journaltitle = {AMIA Annual Symposium Proceedings},
  shortjournal = {AMIA Annu Symp Proc},
  volume = {2023},
  eprint = {38222339},
  eprinttype = {pmid},
  pages = {1324--1333},
  issn = {1942-597X},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10785941/},
  urldate = {2024-08-24},
  abstract = {The process of matching patients with suitable clinical trials is essential for advancing medical research and providing optimal care. However, current approaches face challenges such as data standardization, ethical considerations, and a lack of interoperability between Electronic Health Records (EHRs) and clinical trial criteria. In this paper, we explore the potential of large language models (LLMs) to address these challenges by leveraging their advanced natural language generation capabilities to improve compatibility between EHRs and clinical trial descriptions. We propose an innovative privacy-aware data augmentation approach for LLM-based patient-trial matching (LLM-PTM), which balances the benefits of LLMs while ensuring the security and confidentiality of sensitive patient data. Our experiments demonstrate a 7.32\% average improvement in performance using the proposed LLM-PTM method, and the generalizability to new data is improved by 12.12\%. Additionally, we present case studies to further illustrate the effectiveness of our approach and provide a deeper understanding of its underlying principles.},
  pmcid = {PMC10785941},
  file = {/home/wonseok/Zotero/storage/F9PTDF4K/Yuan 등 - 2024 - Large Language Models for Healthcare Data Augmentation An Example on Patient-Trial Matching.pdf}
}

@online{yuanRealFakeEffectiveTraining2024,
  title = {Real-{{Fake}}: {{Effective Training Data Synthesis Through Distribution Matching}}},
  shorttitle = {Real-{{Fake}}},
  author = {Yuan, Jianhao and Zhang, Jie and Sun, Shuyang and Torr, Philip and Zhao, Bo},
  date = {2024-03-20},
  eprint = {2310.10402},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2310.10402},
  url = {http://arxiv.org/abs/2310.10402},
  urldate = {2025-01-13},
  abstract = {Synthetic training data has gained prominence in numerous learning tasks and scenarios, offering advantages such as dataset augmentation, generalization evaluation, and privacy preservation. Despite these benefits, the efficiency of synthetic data generated by current methodologies remains inferior when training advanced deep models exclusively, limiting its practical utility. To address this challenge, we analyze the principles underlying training data synthesis for supervised learning and elucidate a principled theoretical framework from the distribution-matching perspective that explicates the mechanisms governing synthesis efficacy. Through extensive experiments, we demonstrate the effectiveness of our synthetic data across diverse image classification tasks, both as a replacement for and augmentation to real datasets, while also benefits such as out-of-distribution generalization, privacy preservation, and scalability. Specifically, we achieve 70.9\% top1 classification accuracy on ImageNet1K when training solely with synthetic data equivalent to 1 X the original real data size, which increases to 76.0\% when scaling up to 10 X synthetic data.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/wonseok/Zotero/storage/WXFZDZNL/Yuan 등 - 2024 - Real-Fake Effective Training Data Synthesis Through Distribution Matching.pdf;/home/wonseok/Zotero/storage/2BXXCBZN/2310.html}
}

@inproceedings{zeschApproximateMatchingEvaluating2009,
  title = {Approximate {{Matching}} for {{Evaluating Keyphrase Extraction}}},
  booktitle = {Proceedings of the {{International Conference RANLP-2009}}},
  author = {Zesch, Torsten and Gurevych, Iryna},
  editor = {Angelova, Galia and Mitkov, Ruslan},
  date = {2009-09},
  pages = {484--489},
  publisher = {Association for Computational Linguistics},
  location = {Borovets, Bulgaria},
  url = {https://aclanthology.org/R09-1086},
  urldate = {2024-06-18},
  eventtitle = {{{RANLP}} 2009},
  file = {/home/wonseok/Zotero/storage/5J66JXDL/Zesch 그리고 Gurevych - 2009 - Approximate Matching for Evaluating Keyphrase Extr.pdf}
}

@online{zhangBERTScoreEvaluatingText2020,
  title = {{{BERTScore}}: {{Evaluating Text Generation}} with {{BERT}}},
  shorttitle = {{{BERTScore}}},
  author = {Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q. and Artzi, Yoav},
  date = {2020-02-24},
  eprint = {1904.09675},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1904.09675},
  url = {http://arxiv.org/abs/1904.09675},
  urldate = {2024-01-16},
  abstract = {We propose BERTScore, an automatic evaluation metric for text generation. Analogously to common metrics, BERTScore computes a similarity score for each token in the candidate sentence with each token in the reference sentence. However, instead of exact matches, we compute token similarity using contextual embeddings. We evaluate using the outputs of 363 machine translation and image captioning systems. BERTScore correlates better with human judgments and provides stronger model selection performance than existing metrics. Finally, we use an adversarial paraphrase detection task to show that BERTScore is more robust to challenging examples when compared to existing metrics.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {/home/wonseok/Zotero/storage/KPDV38PU/Zhang 등 - 2020 - BERTScore Evaluating Text Generation with BERT.pdf;/home/wonseok/Zotero/storage/FYPFV2ZB/1904.html}
}

@online{zhangInstructionTuningLarge2023,
  title = {Instruction {{Tuning}} for {{Large Language Models}}: {{A Survey}}},
  shorttitle = {Instruction {{Tuning}} for {{Large Language Models}}},
  author = {Zhang, Shengyu and Dong, Linfeng and Li, Xiaoya and Zhang, Sen and Sun, Xiaofei and Wang, Shuhe and Li, Jiwei and Hu, Runyi and Zhang, Tianwei and Wu, Fei and Wang, Guoyin},
  date = {2023-10-09},
  eprint = {2308.10792},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2308.10792},
  url = {http://arxiv.org/abs/2308.10792},
  urldate = {2024-01-10},
  abstract = {This paper surveys research works in the quickly advancing field of instruction tuning (IT), a crucial technique to enhance the capabilities and controllability of large language models (LLMs). Instruction tuning refers to the process of further training LLMs on a dataset consisting of \textbackslash textsc\{(instruction, output)\} pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users' objective of having LLMs adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of IT, the construction of IT datasets, the training of IT models, and applications to different modalities, domains and applications, along with an analysis on aspects that influence the outcome of IT (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of IT along with criticism against it, along with efforts pointing out current deficiencies of existing strategies and suggest some avenues for fruitful research. Project page: github.com/xiaoya-li/Instruction-Tuning-Survey},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/wonseok/Zotero/storage/TGTZRUHR/Zhang 등 - 2023 - Instruction Tuning for Large Language Models A Su.pdf;/home/wonseok/Zotero/storage/EE6UD7YX/2308.html}
}

@online{zhouLargeLanguageModels2023,
  title = {Large {{Language Models Are Human-Level Prompt Engineers}}},
  author = {Zhou, Yongchao and Muresanu, Andrei Ioan and Han, Ziwen and Paster, Keiran and Pitis, Silviu and Chan, Harris and Ba, Jimmy},
  date = {2023-03-10},
  eprint = {2211.01910},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2211.01910},
  url = {http://arxiv.org/abs/2211.01910},
  urldate = {2024-09-08},
  abstract = {By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer the model, and most effective prompts have been handcrafted by humans. Inspired by classical program synthesis and the human approach to prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic instruction generation and selection. In our method, we treat the instruction as the "program," optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. To evaluate the quality of the selected instruction, we evaluate the zero-shot performance of another LLM following the selected instruction. Experiments on 24 NLP tasks show that our automatically generated instructions outperform the prior LLM baseline by a large margin and achieve better or comparable performance to the instructions generated by human annotators on 19/24 tasks. We conduct extensive qualitative and quantitative analyses to explore the performance of APE. We show that APE-engineered prompts can be applied to steer models toward truthfulness and/or informativeness, as well as to improve few-shot learning performance by simply prepending them to standard in-context learning prompts. Please check out our webpage at https://sites.google.com/view/automatic-prompt-engineer.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/wonseok/Zotero/storage/GPFTK4Q8/Zhou 등 - 2023 - Large Language Models Are Human-Level Prompt Engineers.pdf;/home/wonseok/Zotero/storage/NNPDAEI6/2211.html}
}

@online{zhouLargeLanguageModels2023a,
  title = {Large {{Language Models Are Human-Level Prompt Engineers}}},
  author = {Zhou, Yongchao and Muresanu, Andrei Ioan and Han, Ziwen and Paster, Keiran and Pitis, Silviu and Chan, Harris and Ba, Jimmy},
  date = {2023-03-10},
  eprint = {2211.01910},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2211.01910},
  urldate = {2024-09-08},
  abstract = {By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer the model, and most effective prompts have been handcrafted by humans. Inspired by classical program synthesis and the human approach to prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic instruction generation and selection. In our method, we treat the instruction as the "program," optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. To evaluate the quality of the selected instruction, we evaluate the zero-shot performance of another LLM following the selected instruction. Experiments on 24 NLP tasks show that our automatically generated instructions outperform the prior LLM baseline by a large margin and achieve better or comparable performance to the instructions generated by human annotators on 19/24 tasks. We conduct extensive qualitative and quantitative analyses to explore the performance of APE. We show that APE-engineered prompts can be applied to steer models toward truthfulness and/or informativeness, as well as to improve few-shot learning performance by simply prepending them to standard in-context learning prompts. Please check out our webpage at https://sites.google.com/view/automatic-prompt-engineer.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/wonseok/Zotero/storage/8PRYFJA2/Zhou 등 - 2023 - Large Language Models Are Human-Level Prompt Engineers.pdf}
}

@online{zhuLargeLanguageModels2024,
  title = {Large {{Language Models}} for {{Information Retrieval}}: {{A Survey}}},
  shorttitle = {Large {{Language Models}} for {{Information Retrieval}}},
  author = {Zhu, Yutao and Yuan, Huaying and Wang, Shuting and Liu, Jiongnan and Liu, Wenhan and Deng, Chenlong and Chen, Haonan and Dou, Zhicheng and Wen, Ji-Rong},
  date = {2024-01-19},
  eprint = {2308.07107},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2308.07107},
  url = {http://arxiv.org/abs/2308.07107},
  urldate = {2024-01-30},
  abstract = {As a primary means of information acquisition, information retrieval (IR) systems, such as search engines, have integrated themselves into our daily lives. These systems also serve as components of dialogue, question-answering, and recommender systems. The trajectory of IR has evolved dynamically from its origins in term-based methods to its integration with advanced neural models. While the neural models excel at capturing complex contextual signals and semantic nuances, thereby reshaping the IR landscape, they still face challenges such as data scarcity, interpretability, and the generation of contextually plausible yet potentially inaccurate responses. This evolution requires a combination of both traditional methods (such as term-based sparse retrieval methods with rapid response) and modern neural architectures (such as language models with powerful language understanding capacity). Meanwhile, the emergence of large language models (LLMs), typified by ChatGPT and GPT-4, has revolutionized natural language processing due to their remarkable language understanding, generation, generalization, and reasoning abilities. Consequently, recent research has sought to leverage LLMs to improve IR systems. Given the rapid evolution of this research trajectory, it is necessary to consolidate existing methodologies and provide nuanced insights through a comprehensive overview. In this survey, we delve into the confluence of LLMs and IR systems, including crucial aspects such as query rewriters, retrievers, rerankers, and readers. Additionally, we explore promising directions, such as search agents, within this expanding field.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {/home/wonseok/Zotero/storage/TRPKH4NA/Zhu 등 - 2024 - Large Language Models for Information Retrieval A.pdf;/home/wonseok/Zotero/storage/YF25KU5C/2308.html}
}
