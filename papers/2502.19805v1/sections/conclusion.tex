\section{Conclusion and Discussion}
In this study, we present evidence showcasing the potential transition from employing explicit search on a one-step policy to implicit search within a future-aware policy on the classic board game Chess. The proposed model, \ourmodel, demonstrates not only superior performance compared to the searchless policy but also the policy empowered by explicit search. We provide extensive experiments to demonstrate and analyze \ourmodel. More broadly, the ideas and techniques discussed in this controlled task may eventually be valuable in natural language settings to improve the current next-token prediction LLMs as well. 

We now discuss some limitations and workarounds in our study. 
Firstly, one usage of explicit search such as MCTS is to enhance policy performance through self-play training, such that is able to achieve amazing performance without any human supervision ~\citep{silver2017mastering}. However, our model currently relies on an oracle (Stockfish) to provide future supervision. The integration of \ourmodel with self-play is an interesting direction to explore.
Secondly, our model achieves a deeper search by increasing the context length, with the current training limited to a depth of 7, corresponding to a context length of 648. For scenarios requiring more tokens to represent a state or deeper searches, integrating techniques for long-context models may be useful for efficient training or inference~\citep{dao2022flashattention,gu2023mamba,xiong2024effective,an2024training}.
Finally, our model's performance is currently constrained by the relatively small training dataset of up to 100k games due to resource restrictions, considerably less than the 10 million games used in the study by \citet{ruoss2024grandmaster}. Continuing to scale the model and data remains a valuable direction.