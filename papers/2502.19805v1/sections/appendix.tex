
\reb{\section{Details about MCTS-enhanced Policy}
\label{app:mcts}
This baseline is fully aligned with the approach used in AlphaZero~\citep{silver2017alphazero}. 
The one-step policy directly predicts the next action, while the MCTS-enhanced Policy constructs a search tree that simulates the future to enhance the evaluation of potential next actions. 
Each node $s$ in the search tree contains edges $(s,a)$ for all legal actions $a\in\mathcal{A}(s).$ Each edge stores a set of statistics,

\begin{equation}
    \{N(s,a),W(s,a),Q(s,a),P(s,a)\},
\end{equation}

where $N(s,a)$ is the visit count, $W(s,a)$ is the total action-value, $Q(s,a)$ is the mean action-value, and $P(s,a)$ is the prior probability of selecting that edge. The algorithm proceeds by iterating over the former three phases below and then selects a move to play:
\paragraph{Selection.} The algorithm begins at the root node and traverses the tree, selecting child nodes based on strategies to maximize the exploration of promising paths. Specifically, at each intermediate node, an action is selected according to the statistics in the search tree, $a_t=\underset{a}{\operatorname*{\operatorname*{\mathrm{argmax}}}}\left(Q(s_t,a)+U(s_t,a)\right)$, using a variant of the PUCT algorithm,

\begin{equation}
U(s,a)=c_\text{puct}P(s,a)\frac{\sqrt{\sum_bN(s,b)}}{1+N(s,a)},
\end{equation}

where $c_\mathrm{puct}$ is a constant determining the level of exploration; this search control strategy initially prefers actions with high prior probability and low visit count, but asymptotically prefers actions with high action-value.

\paragraph{Expansion and evaluation.} Upon reaching a leaf node, if it does not represent a terminal state (i.e., the end of the game), one or more new child nodes are expanded and evaluated by the policy and value model. 
The leaf node $s_L$ is added to a queue for neural network evaluation, $v=v_\theta(s_L)$ and $p=p_\theta(s_L)$.
The leaf node is expanded and each edge $(s_L,a)$ is initialized to $\{N(s_L,a)=0, W(s_L,a)=0, Q(s_L,a)=0, P(s_L,a)=p_a\};$ the value $v$ is then backed up.

\paragraph{Backup.} The edge statistics are updated in a backward pass through each step $t\leq L.$ The visit counts are incremented, $N(s_t,a_t)=N(s_t,a_t)+1$, and the action-value is updated to the mean value, $W(s_t,a_t)=W(s_t,a_t)+v, Q(s_t,a_t)=\frac{W(s_t,a_t)}{N(s_t,a_t)}.$ 

\paragraph{Play.} After iteratively cycling through the above phases, a move is selected to play in the root position $s_0$ at the end of the search based on the statistical information, e.g., proportional to its exponentiated visit count, $\pi(a|s_0)=N(s_0,a)^{1/\tau}/\sum_bN(s_0,b)^{1/\tau}$,
where $\tau$ is a temperature parameter that controls the level of exploration. The search tree is reused at subsequent time-steps: the child node corresponding to the played action becomes the new root node; the subtree below this child is retained along with all its statistics, while the remainder of the tree is discarded.}


\section{Derivations}
\label{app:derivation}
\subsection{Discrete Diffusion}
\label{app-sec:diffusion}
In this section, we provide a detailed derivation of the representation for distributions used in the objective Eq.(\ref{eq:dm_original}), which we bring here for a better illustration:
\begin{align*}
    L_{\text{vb}} = \E_{q(\vx_0)}\bigg[&
       \underbrace{D_{\mathrm{KL}}[q(\vx_T | \vx_0) \vert\vert p(\vx_T)]}_{L_T}
    + \sum_{t=2}^T \underbrace{\mathbb E_{q(\vx_t|\vx_0)} \big[
        D_{\mathrm{KL}}[q(\vx_{t-1} | \vx_t, \vx_0) \vert\vert 
        p_{\vtheta}(\vx_{t-1}|\vx_t)]
        \big]}_{L_{t-1}} \nonumber \\[-0.5em]
      &\underbrace{- \mathbb E_{q(\vx_1|\vx_0)} [\log p_{\vtheta}(\vx_0|\vx_1)]}_{L_0}
    \bigg].
\end{align*}
where $L_T$ is a constant when a fixed prior $p(\vx_T)$ is employed. In discrete diffusion, both the forward and backward distribution are defined as categorical distribution, e.g., $q(\vx_t|\vx_{t-1})= \mathrm{Cat}(\vx_t;\vp =\mQ_t^\top \vx_{t-1})$ and $p_{\vtheta}(\vx_{t-1}|\vx_t)=q(\vx_{t-1} | \vx_t, f(\vx_t;\vtheta))$~\citep{hoogeboom2021argmax}, where $\mQ_t$ is a pre-defined  $K\times K$ transition matrix and $K$ is the size of categories.

\paragraph{The posterior $q(\vx_{t-1}|\vx_{t}, \vx_0)$}
Starting from $\vx_0$, we obtain the following $t$-step marginal and posterior at time $t-1$:
\begin{align}
q(\vx_t | \vx_0) = \mathrm{Cat}\left(\vx_{t}; \vp = \overline{\mQ}_{t}^\top\vx_{0}  \right), 
    \quad \text{with}\quad 
  \overline{\mQ}_{t} = \mQ_1  \mQ_2 \hdots \mQ_t \nonumber \\
 q(\vx_{t-1}|\vx_{t}, \vx_0) = \frac{q(\vx_{t}|\vx_{t-1}, \vx_0)q(\vx_{t-1}|\vx_0)}{q(\vx_{t}| \vx_0)}
    =\mathrm{Cat}\left(\vx_{t-1};\vp = \frac{\mQ_t\vx_t \odot \overline{\mQ}_{t-1}^\top \vx_0   }{\vx_t^\top\overline{\mQ}_{t}^\top \vx_0}\right),
    \label{eq:posterior-original}
\end{align}
where $q(\vx_t|\vx_{t-1}, \vx_0)= q(\vx_{t}|\vx_{t-1})$ due to the Markov property of the forward process. The KL divergence between $q$ and $p_\vtheta$ can be computed by simply summing over all possible values of each random variable.
The cumulative products $\overline{\mQ}_t$, which can be computed in closed form or precomputed for all $t$ depending on the choice $\mQ_t$, may be prohibitive for large $T$ and number of categories. Therefore, two commonly used forms of $\mQ$ are introduced by ~\citet{hoogeboom2021argmax} and ~\citet{austin2021structured}, which ensures $\overline{\mQ}_t$ can still be computed efficiently, allowing the framework to scale to a larger number of categories. 

\paragraph{Multinominal diffusion}
The transition matrix initially proposed for the binary scenario by \citet{sohl2015deep} and later expanded to categorical by \citet{hoogeboom2021argmax} can be represented as a $K\times K$ matrix:
\begin{align*}
    \left[{\mQ}_t\right]_{ij} = \begin{cases}
    1 - \frac{K-1}{K} \beta_t \quad &\text{if}\quad i=j\\
    \frac{1}{K} \beta_t \quad &\text{if}\quad i\neq j
    \end{cases}.
\end{align*} 
This transition matrix can also be written as $(1 - \beta_t) I + \beta_t {\1} {\1}^\top / K$, where ${\1}$ is a column vector of all ones.
The transition matrices $\overline{\mQ}$ can be computed in closed form. 
Denote the vector represents the uniform noise distribution as $q_{\text{noise}}={\1} / K$.
In each step, we transition to another token with probability $\beta_t$ and stay the same with probability $1 - \beta_t$. After $t$ steps, the only operative quantity is the probability of not yet having transitioned to another token, given by ${\alpha_t} = \prod_{i=0}^t (1 - \beta_i)$. Therefore, we derive:
\begin{align}
   \overline{\mQ}_t={\alpha_t} I + (1 - {\alpha_t}) \1 q_{\text{noise}}^\top,
\label{eq:barQ}
\end{align}
where setting $q_{\text{noise}}={\1} / K$ gives the $\overline{\mQ}_t$ for multinominal diffusion.

\paragraph{Absorbing diffusion}
For diffusion models with an absorbing state $m$, the following matrix is introduced by \citet{austin2021structured}:
\begin{align*}
    \left[\mQ_t\right]_{ij} = \begin{cases}
    1  \quad&\text{if}  \quad i = j = m \\
    1 - \beta_t \quad &\text{if} \quad i = j \ne m \\
    \beta_t \quad &\text{if} \quad j = m, i \ne m
\end{cases}.
\end{align*}
The transition matrix can also be written as $(1 - \beta_t) I + \beta_t \1 e^\top_m$, where $e_m$ is a vector with a one on the absorbing state $m$ and zeros elsewhere.
Since $m$ is an absorbing state, the corruption process converges not to a uniform distribution but to the point-mass distribution on $m$. 
For text generation, $m$ is the [MASK] token and this leads to a BERT-like training objective~\citep{devlin-etal-2019-bert}, while masks tokens according to some schedule and learns to denoise them iteratively.
Similar as in multinomial diffusion, we set $q_{\text{noise}}=e_m$ for absorbing diffusion, where $e_m$ is a one-hot vector on the [MASK] token, and obtain $\overline{\mQ}_t$ based on Eq.(\ref{eq:barQ}).



\subsection{A Simplified Objective}
\label{app-sec:simplified-loss}
The categorical distribution parameterized by $\vp$ for each variable that follows $q(\vx_{t-1} | \vx_t, \vx_0)$ based on Eq.(\ref{eq:posterior-original}) is given as:
\begin{align*}
    \vp &= \frac{\mQ_t \vx_t \odot \overline{\mQ}_{t-1}^\top \vx_0}{\vx_t^\top\overline{\mQ}_{t}^\top \vx_0} \\
    &=\! \frac{\left[(1-\beta_t) \vx_t + \beta_t\sigma_{\vx_t}\1\right] \odot \left[\alpha_{t-1} \vx_0 + (1 - \alpha_{t-1}) q_\text{noise}\right]}{\alpha_t \vx_t^\top \vx_0 + (1-\alpha_t)\vx_t^\top q_\text{noise}} \\
    &=\! \frac{(1-\beta_t) \alpha_{t-1} {\vx_t \!\odot\! \vx_0} \!+\! (1-\beta_t) (1 \!-\! \alpha_{t-1}){\vx_t \!\odot\! q_\text{noise}} \!+\! \beta_t \alpha_{t-1}\sigma_{\vx_t}\!{\1 \!\odot\! \vx_0} \!+\! \beta_t (1 \!-\! \alpha_{t-1})\sigma_{\vx_t}\!{\1 \!\odot\! q_\text{noise}} }{\alpha_t \vx_t^\top \vx_0 + (1-\alpha_t){\vx_t^\top q_\text{noise}}}\\
    &=\! \frac{(1-\beta_t) \alpha_{t-1} {\vx_t \!\odot\! \vx_0} + (1-\beta_t) (1 \!-\! \alpha_{t-1}){\sigma_{\vx_t}\vx_t} + \beta_t \alpha_{t-1}\sigma_{\vx_t}{\vx_0}  + \beta_t (1 \!-\! \alpha_{t-1})\sigma_{\vx_t}{q_\text{noise}}}{\alpha_t \vx_t^\top \vx_0 + (1-\alpha_t){\sigma_{\vx_t}}},
\end{align*}
where $\sigma_{\vx_t} \coloneqq q_\text{noise}(\vu=\vx_t)$ represents the probability of noise drawn from $q_\text{noise}$ being equal to $\vx_t$. Note $\vx_t \odot \vx_0=0$ if $\vx_t \neq \vx_0$ otherwise 1. Thus the computation of $\vp$ that parameterize $q(\vx_{t-1} | \vx_{t}, \vx_0)$ breaks down into two cases:
\begin{align*}
    \vp = \begin{cases}
       \eta_t\vx_t + \left(1-\eta_t\right) q_\text{noise}, &\text{ if } \vx_t = \vx_0 \\
       \lambda_t\vx_0 + \left(1-\lambda_t\right) q_\text{noise}(\vx_t), &\text{ if } \vx_t \neq \vx_0,
    \end{cases}
\end{align*}
where $\eta_t \coloneqq 1 - \frac{\beta_t(1-\alpha_{t-1})q_\text{noise}(\vu=\vx_t)}{\alpha_{t} + (1 - \alpha_{t})q_\text{noise}(\vu=\vx_t)}$, $\lambda_t \coloneqq \frac{\alpha_{t-1} - \alpha_{t}}{1-\alpha_t}$, and $q_\text{noise}(\vx_t) = (1-\beta_t) \vx_t + \beta_t q_\text{noise}$ denotes a noise distribution that interpolates between $\vx_t$ and $q_\text{noise}$.

Since we set $p_{\vtheta}(\vx_{t-1}|\vx_t)=q(\vx_{t-1} | \vx_t, f(\vx_t;\vtheta))$, the KL divergence between $q(\vx_{t-1} | \vx_t, \vx_0)$ and $p_{\vtheta}(\vx_{t-1}|\vx_t)$ becomes 0 when $\vx_t =\vx_0$. 
In the case of absorbing diffusion, $\vx_t=q_{\text{noise}}=e_m$ if $\vx_t \neq \vx_0$ and $q_{\text{noise}}(\vx_t)=q_{\text{noise}}$. $\vp$ has probability $\lambda_t$ on index $x_0$ and $1-\lambda_t$ on the absorbing state. The model $f(\vx_{t};\vtheta)$ has zero-probability on the absorbing state as it never predicts the mask token. Therefore, $p_{\vtheta}(\vx_{t-1}|\vx_t)$ also has $1-\lambda_t$ probability on the absorbing state. Putting them together, we derive the KL divergence as:
\begin{align*}
    D_{\mathrm{KL}}[q(\vx_{t-1} | \vx_t, \vx_0) \vert\vert 
        p_{\vtheta}(\vx_{t-1}|\vx_t)]&=1_{x_{t}\neq x_{0}}[\lambda_t
         \log \frac{\lambda_t}{f(\vx_{t};\vtheta)_{x_0}}+(1-\lambda_t)\log\frac{1-\lambda_t}{1-\lambda_t}]\\
         &=-\lambda_t 1_{x_{t}\neq x_{0}}
         \vx_{0}^\top\log f(\vx_{t};\vtheta)+C,
\end{align*}
where $1_{x_{t}\neq x_{0}}$ is 1 if $x_{t}\neq x_{0}$ otherwise 0, and $C$ is a constant. Moreover, given $\alpha_0=1$ by definition and therefore $\lambda_0=1$, $L_0$ in Eq.(\ref{eq:dm_original}) can also be written into the final formulation: 
\begin{align*}
    L_{\text{vb}} = -\E_{q(\vx_0)}  \sum_{t=1}^{T} \lambda_t\mathbb E_{q(\vx_t|\vx_0)}1_{\vx_t\neq\vx_{0}} \vx_0^\top \log f(\vx_{t};\vtheta)
\end{align*}

For $\rvx_0$ that represents a sequence of random variables $\rvx_0=(\vx_{0,1}, \dots, \vx_{0,N})$, we can add all computed losses
for each token, arriving at the final expression for the whole sequence:
\begin{align*}
    L_{\text{vb}} = - \E_{q(\rvx_{0})}\sum_{n=1}^N   \sum_{t=1}^{T} \lambda_t\mathbb E_{q(\rvx_{t,n}|\rvx_{0,n})}1_{\rvx_{t,n}\neq\rvx_{0,n}} \rvx_{0,n}^\top \log f(\rvx_{t,n};\vtheta).
\end{align*}
For multinomial diffusion, we follow ~\citet{Zheng2023ARD} to adopt a reparameterized form, which results in the above formulation as well.

\section{Additional Experiments}

\subsection{Dynamic Search Depth in \ourmodel}
\label{app:depth}
\input{tabs/dynamic_depth}
In explicit search algorithms, the search depth is predefined either through an exact parameter as in depth-first search, or a related parameter such as the number of simulations as in MCTS. In \ourmodel, the deeper search is achieved by extending the context length of the input. In this section, we present the results of training a single model for dynamic search depth, compared with separate models in the previous sections. We convert the learned position embedding to RoPE~\citep{su2024roformer}, enabling the utilization of a context length beyond what was encountered during training at inference time. During training, a horizon $h$ is randomly chosen from the interval $[1,4]$ for each data to expose the model to various input lengths. As shown in Table~\ref{app-tab:dynamic-depth}, the single model surpasses the one-step policy with a lookahead of up to 5 future steps, exceeding the training stage's future step of 3. Nonetheless, a diminishing trend emerges as we escalate the search depth, possibly attributed to the constrained training data and limited context extension capability of the current RoPE-based model. We leave more effective context extension beyond training stage for implicit search to future work.


\subsection{Scaling Behavior with More Data}
\label{app-sec:scaling}
\input{tabs/scaling}

\paragraph{Improved next-action accuracy} In Table~\ref{app-tab:scaling}, we show the comparison of Transformer \texttt{S-A} and \ourmodel \texttt{S-ASA} when scaling data and model size. We can see the performance of \ourmodel consistently improves with more data and model layers, while that of Transformer \texttt{S-A} converges with 2 layers with 10k games and 4 layers with 100k games. Further increasing model size is still useful for \ourmodel under both data-limited (e.g., 10k games) and relatively data-sufficient (e.g., 100k games) scenarios. 

\paragraph{Improved future accuracy}
\input{tabs/app-future-quality}
In Table~\ref{app-tab:scaling-future-quality}, we show the quality of predicted futures for \ourmodel with horizon $h=4$. We find when we scale data to 100k games (6.6M records), almost all the future actions are valid (i.e., legal), future states are valid (i.e., the predicted states tokens correctly represent a valid board state), and the action-state transition dynamics are well learned. Moreover, the best action percentage (i.e., action accuracy) also improves greatly compared to that in the 10k games setting. This demonstrates the potential of accurate future world modeling through model scaling.

\newpage

\reb{
\subsection{Example of Training Instance}
\label{appendix:training-case}
We show an example of each training paradigm in Table \ref{tab:paradigm-example}.
\input{tabs/data_example}}


\subsection{Additional Cases}
\label{appendix:case_study}

In Figure~\ref{fig:case-study-additional}, we provide the predictions of Transformer (\textsc{S-A}) and \ourmodel on more challenging puzzles. \reb{We also show the prediction of all models in Figure~\ref{fig:case-study-full}, where all models are trained on 10k games and 100 MCTS simulations are used for Transformer with MCTS.}

\begin{figure}[th]
    \centering
    \includegraphics[width=\linewidth]{figs/case_study/additional_cases-crop.pdf}
    \caption{Additional prediction cases on challenging puzzles.}
    \label{fig:case-study-additional}
    \vspace{-5pt}
\end{figure}

\begin{figure}[th]
    \centering
    \includegraphics[width=\linewidth]{figs/case_study/full_case.pdf}
    \caption{\reb{Additional cases on challenging puzzles compared with all baselines.}}
    \label{fig:case-study-full}
    \vspace{-5pt}
\end{figure}