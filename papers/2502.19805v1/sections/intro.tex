\section{Introduction}
 
Search is central to problem-solving in AI \citep{Russell2010ArtificialI}. One of the most notable examples is IBM's Deep Blue~\citep{campbell2002deep}, which performs extensive search over a large space through a strong search algorithm (alpha-beta pruning; ~\citealt{knuth1975analysis}), defeated the world chess champion Garry Kasparov in 1997. 
\reb{Search has also been utilized in neural networks.} A noteworthy advancement in this progression is exemplified by AlphaGo~\citep{silver2016mastering} and its successors~\citep{silver2017mastering,silver2017alphazero}, where the policy is guided by an extra value network through Monte Carlo Tree Search (MCTS; \citealt{coulom2006efficient,browne2012survey}). By explicitly searching into the future, the decision to be taken can be iteratively refined~\citep{silver2016mastering,silver2017mastering,silver2017alphazero,schrittwieser2020mastering}.


Recent research on Large Language Models (LLMs) demonstrates the utilization of a similar framework.
Although scale-up, the autoregressive one-step policy addresses only a portion of the problems and relies on explicit search on complex tasks~\citep[\emph{inter alia}]{hao2023reasoning,yao2024tree,zhao2024large,trinh2024solving}, highlighting their inherent limitations in long-term planning~\citep{valmeekam2022large,bubeck2023sparks,bachmann2024pitfalls}.
\reb{This explicit search-demanding approach, however, is not quite satisfactory, as the repeated invocation of the value model can result in an accumulation of errors if the value model is inaccurate and increased inference costs for long-horizon rollouts~\citep{yao2024tree}.} 
\reb{Given the essence of explicit search (e.g., MCTS) over one-step policies lies in iteratively looking into the future and leveraging the future to enhance the next token (or action) prediction, our research question is: 

\textit{Can the policy model predict and utilize the future by itself to improve the next token (or action) prediction without relying on explicit search during inference?}}

\begin{wrapfigure}{r}{0.45\textwidth}
    % \vspace{-40pt}
    \begin{center}
    \includegraphics[width=1\linewidth]{figs/fig2.pdf}
    \end{center}
    \vspace{-5pt}
    \caption{\reb{Comparison between explicit search via MCTS and implicit search via discrete diffusion. MCTS explicitly performs action selection, state evaluation, and value backup in an iterative manner before determining the next action to take (as detailed in Appendix~\ref{app:mcts}), while discrete diffusion implicitly gathers future information during future imagination to improve the next action.}}
    \label{fig:method}
    \vspace{-5pt}
\end{wrapfigure}
This paper explores the potential transition from utilizing an explicit search algorithm (e.g., MCTS) over the one-step policy to implicitly searching over future representations by teaching the policy to predict and utilize the future. 
Firstly, to reduce the difficulty of future prediction, we take inspiration from diffusion models~\citep{sohl2015deep,ho2020denoising}, which perform a multi-step generative process for sample generation. Secondly, to iterative refine the current policy prediction based on future information, we directly rely on the internal bidirectional self-attention mechanism~\citep{vaswani2017attention} and the multi-step diffusion generative process.
Finally, we represent the future to be learned and predicted with the multi-step interaction information between policy and the world (e.g., states and actions), such that the generation of the future shares similar spirits as implicit searching in the future world.
We name our approach as \ourmodel, a method that looks into the future world via diffusion modeling without any explicit search during inference.
\reb{Alternatively, \ourmodel can be seen as containing a world model that predicts the future. }
However, rather than having a separate world model simulating the environmentâ€™s transition dynamics and another policy performing action prediction through interaction with the world model using planning algorithms such as value iteration~\citep{puterman2014markov} or MCTS~\citep{schrittwieser2020mastering}, \ourmodel internalizes the world model directly within the policy without intermediate components. \reb{We show the comparison between explicit search via MCTS and implicit search via discrete diffusion in Figure~\ref{fig:method}.}


We take a specific focus on the chess-playing task, where explicit search is known to be essential~\citep{campbell2002deep,silver2017alphazero}. The ideas and techniques learned in this controlled task may eventually be useful in natural-language settings as well. 
We conduct extensive experiments and take a deep look into various paradigms to represent and learn the future. When measured by action accuracy, \ourmodel outperforms the one-step policy~\citep{ruoss2024grandmaster} by 19.2\%, and MCTS-enhanced policy by 14\%. \reb{\ourmodel demonstrates a 30\% increase in puzzle-solving capabilities in comparison to the MCTS-enhanced policy. Furthermore, it attains a higher level of game-playing proficiency, as evidenced by a 540 more Elo rating, which showcases the potential of substituting one-step policy with explicit search with a learned discrete diffusion model that looks into the future world by itself.}

Our contributions include: 1) we propose \ourmodel to foresee and utilize future information via diffusion modeling as an alternative to explicit search via designed search algorithms (\S\ref{sec:ourmodel}); 2) we instantiate \ourmodel for chess-playing and demonstrate its superior performance compared to both the one-step policy and the MCTS-powered policy in a rigorous evaluation, such as solving over 30\% more puzzles and 540 Elo playing strength in the tournament (\S\ref{sec:main}); 3) we provide a detailed analysis of the design considerations for future representation and diffusion modeling (\S\ref{sec:ablation}), as well as unveiling the working mechanism and appealing advantage compared to MCTS-based policy regarding effectiveness and efficiency (\S\ref{exp:analysis}). \reb{These findings demonstrate the possibility of moving from the one-step policy with explicit search algorithms to the future world-aware multi-step diffusion policy with implicit search ability.} 