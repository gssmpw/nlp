\section{Methodology}
\label{sec:ourmodel}
% \subsection{Overview}
In this section, we introduce \ourmodel, an approach that looks into the future world via discrete diffusion modeling without any explicit search at inference time\reb{, with a focus on the chess-playing task.}
 
\subsection{Modeling}
In order to endow the model with the capability to predict and utilize the future, we consider training the model in a supervised way following~\citep{ruoss2024grandmaster}, leaving self-play training from scratch~\citep{silver2017mastering} for future work.
We provide the current state $s_i$ as the history representation following prior studies ~\citep{silver2016mastering,silver2017mastering,ruoss2024grandmaster}. 
For future world representation, we consider a variety of alternative variants, such as purely future actions (denoted as \texttt{s-aa}), action-states (denoted as \texttt{s-asa}), and action-state-values (denoted as \reb{\texttt{s-avsav}}, etc. We analyze the performance of different future paradigms in Section \S\ref{sec:ablation}. The \texttt{s-asa} approach is ultimately chosen as our modeling paradigm considering the effectiveness and simplicity.
The policy distribution at state $s_i$ considering the future is given by: 
\begin{align}
p_\vtheta(a_i,s_{i+1},a_{i+1},\dots,s_{i+h-1},a_{i+h-1}|s_i),
\label{eq:ours}
\end{align}
where $h>1$ is the future horizon. 

\subsection{Training}
In order to train a policy that models Eq.(\ref{eq:ours}), we consider a supervised training approach leveraging  Stockfish~\citep{romstad2008stockfish}. We utilize \href{https://stockfishchess.org/blog/2023/stockfish-16/}{Stockfish 16}, currently the worldâ€™s strongest search-based engine, as an oracle to label board states extracted from randomly selected games on \href{https://lichess.org}{lichess.org}. We approximate the optimal policy $\pi^*$ with $\pi^{SF}$ and obtain each action by taking $a^{SF}_j=\arg\max_{a_j} Q^{SF}(s_j,a_j)$. For a given world horizon $h$, we construct a dataset $\mathcal{D}=\{(s_i,(a^{SF}_i,s_{i+1},a^{SF}_{i+1},\dots,s_{i+h-1},a^{SF}_{i+h-1}))\}$, where the oracle future path means playing some move that has the maximum evaluation for the best opponent's reply for both players. 

An intuitive way to use $\mathcal{D}$ is to train a network to directly predict the entire concatenated next action and future sequence $a_i^{SF}\mid\mid z_i^{SF} (z_i^{SF} \coloneqq s_{i+1}\mid\mid a_{i+1}^{SF}\mid\mid \dots \mid\mid s_{i+h-1}\mid\mid a_{i+h-1}^{SF}$). Nonetheless, we observe that this approach not only fails to predict the future but also impedes the learning of the next action $a^{SF}_i$ (see Section \S\ref{sec:ablation}). Therefore, we resort to diffusion modeling~\citep{sohl2015deep} as a powerful sequence modeling approach with strong expressive capabilities. The bidirectional multi-layer self-attention and iterative denoising mechanism are expected to enhance the prediction of the next action by considering future information.
Specifically, we consider discrete diffusion modeling and streamline $L_{\text{vb}}$ in Eq.(\ref{eq:dm_original}) into a weighted cross-entropy loss motivated by ~\citet{austin2021structured,Zheng2023ARD,shi2024simplified,sahoo2024simple}. The KL term $D_{\mathrm{KL}}[q(\vx_{t-1} | \vx_t, \vx_0) \vert\vert 
        p_{\vtheta}(\vx_{t-1}|\vx_t)$ for each individual random variable is simplified as $-\lambda_t 1_{\vx_{t}\neq\vx_0} \vx_0^\top \log f(\vx_{t};\vtheta)$, where and $L_{\text{vb}}$ becomes:
\begin{align}
    L_{\text{vb}} = -\E_{q(\vx_0)}  \sum_{t=1}^{T} \lambda_t\mathbb E_{q(\vx_t|\vx_0)}1_{\vx_t\neq\vx_{0}} \vx_0^\top \log f(\vx_{t};\vtheta),
\label{eq:dm_simple}
\end{align}
where $\lambda_t=\frac{\alpha_{t-1}-\alpha_{t}}{1-\alpha_t} \in (0,1]$ is a time-dependent reweighting term that assigns lower weight for noisier $\vx_{t}$, and $\alpha_t \in [0,1] $ belongs to a predefined noise scheduler that controls the level of noise in $\vx_t$ at timestep $t$. We explore multiple variants of $\lambda_t$ in Section \S\ref{sec:ablation}. To enable conditional training with a given state, we freeze the state tokens and perform denoising on the next action $a_i^{SF}$ and all futures tokens $z_i^{SF}$. \reb{We employ Monte Carlo sampling with regard to $\vx_0$, $\vx_t$ and $t$ when optimizing $L_{\text{vb}}$.} We provide detailed derivations in Appendix~\ref{app-sec:simplified-loss}. We elaborate the training procedure in Algorithm~\ref{alg:training}.

\subsection{Inference}
During inference, 
taking $\argmax_{a_i} p_\vtheta(a_i|s_i)$ as in one-step policy in \ourmodel requires marginalizing over all future with horizon $h$, i.e.,  $p_\vtheta(a_i|s_i)=\sum_{z_i}p_\vtheta(a_i,z_i|s_i)$, which is intractable due to the exponential-growing search space when $h$ goes larger, e.g., the game tree contains $b^h$ nodes and the branching factor $b$ is around 31 on average in chess~\citep{chessbranchingfactor}. One simplified approach to comparing actions is to measure the best future if one action is taken, which can be reflected by the joint probability $p_\vtheta(a_i,z_i|s_i)$. Therefore, we resort to $\argmax_{a_i,z_i} p_\vtheta(a_i,z_i|s_i)$, which does not involve marginalization and can be achieved by sampling from the trained model. 
During diffusion sampling, we adopt an easy-first decoding
strategy~\citep{savinovstep2021,chang2022maskgit}, which achieves better performance compared to the random decoding approach employed by ~\citet{austin2021structured}. Specifically, at diffusion timestep $t$, the tokens within the least $100*\frac {t-1}{T}\%$ predictive log-likelihood are selected to be reset to the noise state. 
To change search depth, we mainly train separate models on $\mathcal{D}$ with different $h$, and study a single model on $\mathcal{D}$ with mixed $h$ in Appendix~\ref{app:depth}.
We elaborate the inference algorithm in Algorithm~\ref{alg:inference}.


\input{figs/algorithm}