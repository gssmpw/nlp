\section{Preliminaries}
\label{sec:background}
This section introduces key concepts and notations in the chess-playing problem and diffusion modeling.

\paragraph{Problem Setting}
Chess, along with other games of perfect information like checkers, othello, backgammon, and Go, fits the framework of alternating Markov games~\citep{littman1994markov}. In chess, there exists a state space $\gS$, an action space $\gA$, a state transition function $f(s, a)$ that determines the subsequent state after taking action $a$ in state $s$, and two reward functions $r^0(s)$ and $r^1(s)$ representing the two players' reward in state $s$ (rewards being zero except at the final time-step). The outcome of the game
$o_i = \pm1$ is the terminal reward at the end of the game from the perspective of the current
player at time-step $i$. Chess is also a zero-sum game which indicates $r^0(s)=-r^1(s)$.
A policy $p(a|s)$ is a probability distribution over actions space $\gA$. A value function $v^p(s)$ represents the expected outcome when all actions for both players adhere to policy $p$, denoted as $v^p(s) = \E[o_i | s_i = s, a_{i\dots I} \sim p]$. The goal is to build a policy that, when actions are taken based on it, results in the highest possible final outcome.

\paragraph{Discrete Diffusion Modeling}
Discrete diffusion models~\citep{sohl2015deep,hoogeboom2021argmax,austin2021structured} are a class of latent variable models characterized by a forward and a backward Markov process. Suppose $\vx_0 \sim q(\vx_0)$ is a discrete random variable with $K$ possible categories and represented as a one-hot vector. The forward process $q(\vx_{1:T}|\vx_0) = \prod_{t=1}^T q(\vx_t|\vx_{t-1})$ corrupts the original data $\vx_0$ into a sequence of increasingly noisy latent variables $\vx_{1:T}\coloneqq \vx_1, \dots, \vx_T$. The learned backward process $p_{\vtheta}(\vx_{0:T})=p(\vx_T)\prod_{t=1}^Tp_{\vtheta}(\vx_{t-1}|\vx_t)$ gradually denoises the latent variables
to the data distribution. 
In order to optimize the generative model $p_{\vtheta}(\vx_{0})$ to fit the data distribution $q(\vx_{0})$, we typically
optimize a variational upper bound on the negative log-likelihood due to the intractable marginalization:
\begin{align}
    L_{\text{vb}} = \E_{q(\vx_0)}\bigg[&
       \underbrace{D_{\mathrm{KL}}[q(\vx_T | \vx_0) \vert\vert p(\vx_T)]}_{L_T}
    + \sum_{t=2}^T \underbrace{\mathbb E_{q(\vx_t|\vx_0)} \big[
        D_{\mathrm{KL}}[q(\vx_{t-1} | \vx_t, \vx_0) \vert\vert 
        p_{\vtheta}(\vx_{t-1}|\vx_t)]
        \big]}_{L_{t-1}} \nonumber \\[-0.5em]
      &\underbrace{- \mathbb E_{q(\vx_1|\vx_0)} [\log p_{\vtheta}(\vx_0|\vx_1)]}_{L_0}
    \bigg],
\label{eq:dm_original}
\end{align}
where $L_T$ is a constant when a fixed prior $p(\vx_T)$ is employed. 
In discrete diffusion, both the forward and backward distribution are defined as categorical distribution, e.g., $q(\vx_t|\vx_{t-1})= \mathrm{Cat}(\vx_t;\vp = \mQ_t^\top\vx_{t-1})$ and $p_{\vtheta}(\vx_{t-1}|\vx_t)=q(\vx_{t-1} | \vx_t, f(\vx_t;\vtheta))$~\citep{hoogeboom2021argmax}, where $\mQ_t$ is a pre-defined transition matrix of size $K \times K$. Therefore, the forward process posterior $q(\vx_{t-1} | \vx_t, \vx_0)$ and each KL term can be calculated analytically. We provide more details about discrete diffusion in Appendix~\ref{app-sec:diffusion}.
