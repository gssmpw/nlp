\section{Related Work}
\subsection{Neural Networks for Chess}

The development of chess AI has undergone a significant transformation, shifting from the explicit design of search strategies and heuristics to the more data-driven and learning-based approaches. The early research, exemplified by Turing's investigations~\citep{burt1955faster} and NeuroChess~\citep{thrun1994learning}, heavily depended on handcrafted search algorithms and heuristics, eventually leading to the development of powerful search engines like Deep Blue~\citep{campbell2002deep} and Stockfish~\citep{romstad2008stockfish}. However, the emergence of neural network-based approaches, typically AlphaZero~\citep{silver2017alphazero}, marked a paradigm shift, where deep reinforcement learning equipped with Monte Carlo Tree Search (MCTS) enabled the system to learn its own heuristics, i.e., the policy and value networks, without the need for manual design~\citep{klein2022neural,mcgrath2021acquisition}.
The rise of large language models (LLMs) has also inspired innovations in chess AI, such as the evaluation~\citep{toshniwal2022chess,carlini2023playing} and interpretation~\citep{li2023emergent,karvonen2024emergent} of LLMs' ability to play chess, the integration of chess-related text data into training~\citep{feng2024chessgpt}, and the exploring of searchless models by scaling the policy networks~\citep{ruoss2024grandmaster}.
Despite this, lookahead search methods like beam search~\citep{feng2024chessgpt} and even depth-one search with the value network~\citep{ruoss2024grandmaster} remain superior to the policy models as action predictors, which is the same as in the AlphaZero era~\citep{silver2017mastering,2018lczero}. This underscores the continued significance of lookahead information for move prediction in chess.
In contrast to prior research, we explore directly teaching the policy model to look ahead, thereby eliminating the requirement of handcrafted search algorithms or separate value networks.

\subsection{Diffusion Models}
Diffusion models~\citep{sohl2015deep,ho2020denoising,austin2021structured}, a powerful class of generative models, have been applied to various fields such as image generation~\citep[\emph{inter alia}]{dhariwal2021diffusion,rombach2022high,croitoru2023diffusion}, text generation~\citep[\emph{inter alia}]{li2022diffusion,gong2022diffuseq,Zheng2023ARD,lou2023discrete,li2023diffusion} and reinforcement learning~\citep[\emph{inter alia}]{janner2022planning,ajay2022conditional,chi2023diffusion,zhu2023diffusion}. Theoretically, diffusion models perform a multi-step denoising process to progressively convert a random noise into a data sample, and the denoising procedure can be seen as parameterizing the gradients of the data
distribution~\citep{song2019generative}, connecting them to score matching~\citep{hyvarinen2005estimation} and energy-based models~\citep{lecun2006tutorial}. Particularly, diffusion models have demonstrated their effectiveness in tasks that require global control and future planning, such as math reasoning~\citep{ye2024diffusion,gong2024scaling}, paragraph generation~\citep{zhang2023planner}, trajectory planning~\citep{janner2022planning} and robot manipulation~\citep{chi2023diffusion}. The superior planning ability of diffusion compared to autoregression has been studied in ~\citet{ye2024beyond}. Compared to Diffusion Policy~\citep{chi2023diffusion}, \ourmodel internalizes a world model inside the policy, which we find is crucial in Section \S\ref{sec:ablation}. \reb{Furthermore, we focus on exploring diffusion models for implicit search as an alternative to the one-step policy with explicit search to deal with complex tasks that require search.}

\subsection{World Models}
The primary goal of a world model is to capture the underlying dynamics of the environment and predict the future outcome of certain actions in the context of model-based reinforcement learning (MBRL)~\citep{wang2019benchmarking,moerland2023model}. The learned world model can be used for policy optimization of a RL agent~\citep{sutton1991dyna,feinberg2018model,hafner2019dream} and allow the agent to explicitly reason about the future consequences of its actions~\citep{hafner2019learning,schrittwieser2020mastering,ye2021mastering}. Most of the conventional world models~\citep{hafner2019dream,hafner2020mastering,hafner2023mastering} rely on single-step prediction, which suffer from compounding errors~\citep{asadi2019combating,xiao2019learning,lambert2022investigating}. 
Recently, there has been growing interest in building multi-step world models utilizing diffusion models~\citep{zhang2023learning,rigter2023world,jackson2024policy,ding2024diffusion}, which, however, separate the world model and policy. Similar to ours, Diffuser~\citep{janner2022planning} and Decision Diffuser (DD;~\citealt{ajay2022conditional}) also unify the world model with the policy. However, the modeling details, training paradigm, and action prediction differ. Specifically, both of them employ continuous diffusion while we use discrete diffusion. In addition, Diffuser trains an unconditioned model and requires a guidance function to obtain desired actions, while we model the best action and future trajectory condition on a given state. DD models state-only future trajectories and predicts the action through an inverse dynamics model while we model both future states and actions. Finally, the comparison of diffusion world model and explicit search has not been rigorously explored in domains that require precise and sophisticated lookahead such as chess, to the best of our knowledge.




