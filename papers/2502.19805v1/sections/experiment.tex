\section{Experiments}
\subsection{Setup}

\paragraph{Baselines}
We compare our model with three Transformer models proposed in \citet{ruoss2024grandmaster}: State-action model (\texttt{S-A}) which learns to predict next move via behavioral cloning; State-value model (\texttt{S-V}) which predicts next move via comparing the value of next states; and Action-value model (\texttt{SA-V}) which predicts next move via comparing the value of each legal actions at the current state. We also integrate the trained \texttt{S-A} and \texttt{S-V} models into MCTS following AlphaZero~\citep{silver2017alphazero}.


\paragraph{Data}
\input{tabs/dataset}
We construct a dataset for supervised training by downloading games from \href{https://lichess.org}{lichess} recorded in February 2023. When analyzing the scaling behavior, we use up to 100k games, while reverting to the default 10k games for other experiments due to resource constraints. We show the data statistics in Table~\ref{tab:dataset}. Following~\citet{ruoss2024grandmaster}, we convert the centipawns returned by Stockfish to the win percentage and then discretize it into 128 bins to represent value in \texttt{S-V} and \texttt{SA-V}. We encode the state as a fixed-length FEN string with 77 characters by padding with `.' if needed. Actions are stored in UCI notation with 1968 possible moves in total. \reb{We provide example training data for each paradigm in Appendix~\ref{appendix:training-case}.}

\paragraph{Implementation Details}
For all the neural models in this paper, we use the same decoder-only GPT-2 transformer architecture~\citep{vaswani2017attention,radford2019language} for a rigorous comparison. For \ourmodel, we convert casual attention into full attention without introducing additional learned parameters. We train all baseline models until convergence and set a maximum of 200 epochs for diffusion models due to their slow convergence. We use the Adam optimizer~\citep{kingma2015adam}, a learning rate of 3e-4, and a batch size of 1024 for all models. By default, we set the horizon $h$ to be 4, the number of network layers to be 8 (with a total parameter size of 7M), the diffusion timesteps to be 20, and an absorbing noise type. By default, 100 simulations are utilized in MCTS-enhanced policy, and its impact is analyzed in Figure~\ref{fig:implicit-vs-explicit}. We adjust $c_{\text{puct}}$ and $\tau$, constants determining the level of exploration in MCTS, on a held-out set and set them to $c_{\text{puct}}=0.1$ and $\tau=1$ for its superior performance.
All experiments are done on 8 NVIDIA V100 32G GPUs.


\paragraph{Evaluation Metrics}
We mainly consider three metrics to evaluate the policies following ~\citep{ruoss2024grandmaster}: 1) \textbf{Action Accuracy}: the percentage of the test set in which the model selects the same action as the ground truth; 2) \textbf{Puzzle Accuracy}: the percentage of puzzles where the
policyâ€™s action sequence exactly matches the known
solution action sequence and we use 10k puzzles with difficulty rated by Elo from
399 to 2867 provided by ~\citep{ruoss2024grandmaster}; 3) 
\textbf{Tournament Elo}: the Elo ratings calculated using BayesElo~\citep{coulom208whole} in an internal tournament involving all policies, where each pair of policies played 400 games, resulting in a total of 6000 games.

\subsection{Main Results}
\label{sec:main}
\input{tabs/main_tab}
We report the prediction and playing strength comparison for our model against baselines in Table~\ref{tab:main}. Additionally, we report the performance of Stockfish 16 with a time limit of 0.05s per legal move, which stands as the oracle used to generate our dataset.
We find \ourmodel significantly outperforms the \texttt{S-A} model by 653 Elo and 19\% action accuracy, indicating the effectiveness of \ourmodel in improving next action prediction through future prediction. Remarkably, despite utilizing 20 times fewer data records than the \texttt{SA-V} model, our model demonstrates superior performance with approximately 10\% higher action accuracy. Our model demonstrates superior performance over the MCTS-based agent by achieving a higher Elo difference of 542 and an increased action accuracy of 14\%. This highlights the effectiveness of \ourmodel in modeling multi-step simulations when compared with the step-by-step MCTS-enhanced policy, which relies on a robust value model and necessitates a careful balance between the policy and value models.


\subsection{Ablations}
\label{sec:ablation}

\paragraph{Future paradigm matters}
\input{tabs/future_paradigms}
We compare baselines and different future paradigms during the training of \ourmodel with horizon $h=4$ in Table~\ref{tab:future_paradigm}. For each future paradigm, we compare training with autoregressive Transformer and \ourmodel. We find that directly performing future action prediction (\texttt{S-AA})~\citep{chi2023diffusion} with \ourmodel hurts performance compared to \texttt{S-A} due to the difficulty of future move prediction in chess. \reb{However, after we integrate future states, we observe significant performance improvements when comparing \texttt{S-AA} (15.07)  to \texttt{S-ASA} (41.31), and also when comparing \texttt{S-AVAV} (17.63) to \texttt{S-AVSAV}  (40.69).}
No further improvement is observed when integrating the values in \ourmodel, which may be attributed to training on the optimal future trajectory rather than all possible trajectories. For training using autoregressive Transformer, we observe that the overall performance hovers around 26\%. This performance level is superior to that achieved by \texttt{S-A} (22.1\%), attributed to the utilization of more (S, A) pairs (e.g., each \texttt{S-ASA} record containing $h$ (S, A) pairs). However, the performance falls short when compared to \ourmodel, which underscores the importance of modeling bidirectional context to leverage future information for subsequent action prediction.

\input{tabs/future_quality}
\paragraph{Ensuring the validity of future world dynamics is crucial}
After we discuss the future paradigm, we now investigate the effect of future quality on performance, as shown in Table~\ref{tab:future_quality}. 
\reb{For better illustration, denote a sequence of future horizon 2 as $[s_1=f(s_0,a_0),a_1=g(s_1),s_2=f(s_1,a_1),a_2=g(s_2)]$, where $f$ is a world dynamic function and $g$ is a policy function. $s_0$ is the current state and $a_0$ is the move suggested by Stockfish.}
\reb{We first utilize random state-action sequences for future steps, where both actions and states were randomly selected (i.e., random world $f$ and random policy $g$)}. This methodology did not yield performance enhancements. Subsequently, we explore selecting random actions and incorporating the resulting state from executing those actions \reb{(i.e., random policy $g$ but an oracle world $f$)}, which notably outperforms the initial strategy. This underscores the significance of aligning states with corresponding actions, mirroring the dynamics of the world. Finally, we investigate incorporating high-quality future actions suggested by Stockfish \reb{(i.e., Stockfish $g$ and oracle world 
 $f$)} and observe additional performance improvements compared to the random action selection approach.

\input{tabs/training_strategies}
\paragraph{Proper discrete diffusion modeling helps}
Given the dataset $\mathcal{D}$ annotated with future states and actions, we investigate alternative ways to train the model, as presented in Table~\ref{tab:training_alg}. We first observe it is hard to teach the model to directly output the entire future sequence, leading to lower performance compared to auto-regressive training. Secondly, we employ continuous Gaussian diffusion VDM~\citep{kingma2021variational} and observe its superior performance compared to the Direct and auto-regressive methods, but inferior compared to discrete approaches. The absorbing diffusion with reciprocal $\lambda_t=1/t$ obtained by setting $\alpha_t=1-\frac{t}{T}$ in Eq.(\ref{eq:dm_simple}) is a simplified expression from D3PM~\citep{austin2021structured}, which we find significantly outperforms continuous diffusion. Finally, we discover a linear $\lambda_t$~\citep{bond2022unleashing, Zheng2023ARD} further exceeds the constant and reciprocal ones, as well as the multinomial counterpart.

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.98\linewidth]{figs/future-analysis.pdf}
    \caption{\textbf{(Left)} Prediction quality analysis for \ourmodel at different future steps. \textbf{(Middle)} Action accuracy when scaling self-attention layers. \textbf{(Right)} Action accuracy when increasing diffusion timesteps.}
    \label{fig:future-analysis}
    \vspace{-5pt}
\end{figure*}

\subsection{Analysis}
\label{exp:analysis}
\paragraph{Does \ourmodel predict accurate future information?}
We analyze the percentage of valid actions and the optimal action recommended by Stockfish for each predicted action. \reb{The best $a_0$ metric is exactly the action accuracy by definition.}. Additionally, we assess whether each predicted state is a valid representation and if $s_i$ corresponds to the resulting state when action $a_{i-1}$ is taken at $s_{i-1}$. The initial state $s_0$ provided as input is excluded, and the results are presented in the left figure of Figure \ref{fig:future-analysis}.
We observe that the first action, denoted as $a_0$, are almost 100\% valid. As we progress through future steps, both the valid rate and the optimal rate decline. However, even at $i=3$, where the valid rate stands at 50\%, it surpasses the random move baseline of approximately 1.6\% (calculated as the average number of legal actions per move, 31, divided by the total number of moves, 1968). This indicates that the model retains a certain level of predictive capability for future moves, albeit with reduced performance. A similar pattern appears in the evaluation of states, where the accuracy is perfect for the first predicted state $s_1$ but diminishes in subsequent predictions. \reb{In Appendix Table 8, we demonstrate that further increasing the training data enhances the effectiveness of the world model within \ourmodel, achieving over 90\% accuracy in predicting valid and matched future states corresponding to the preceding action.}

\paragraph{How does \ourmodel leverage future information?}
We attributes the future-aware ability of \ourmodel mainly to self-attention and iterative decoding process, as shown in the middle and right figures of Figure~\ref{fig:future-analysis}, respectively. When employing a single self-attention layer, our model exhibits inferior performance compared to the \texttt{S-A} model, yet surpasses it with two layers. Moreover, its performance steadily enhances as we augment the number of layers. This suggests that with additional layers, there is more chance for the subsequent actions and future to interact reciprocally, akin to the enhancement in the action prediction with increased MCTS simulations. We do not observe a similar upward trend in the performance of \texttt{S-A} model when increasing attention layers as in \citep{ruoss2024grandmaster}, possibly indicating that the available data (10k) does not necessitate the integration of more layers. In the right figure of Figure~\ref{fig:future-analysis}, it is evident that employing an appropriate decoding strategy (such as likelihood-based) further enhances next-action prediction as the number of iterations grows. However, the overall improvement is relatively modest compared to increasing the attention layers.



\paragraph{Explicit search vs. Implicit search}
Based on our previous analysis, we can consider \ourmodel as performing implicit search through the inner self-attention layers and the multi-step diffusion process. Now, we aim to evaluate the efficiency and effectiveness of this implicit search in comparison to explicit search using MCTS when conducting deeper searches. In \ourmodel, deeper search is realized by increasing the context length (80 tokens per search depth), whereas in MCTS, it is achieved through running more simulations.
In the left figure of Figure~\ref{fig:implicit-vs-explicit}, it is evident that \ourmodel exhibits significant enhancement when increasing search depth, while MCTS becomes stagnant after 50 simulations at a search depth of around 4. This could be attributed to the accumulated errors caused by the value network due to a limited horizon. In the middle figure of Figure~\ref{fig:implicit-vs-explicit}, we measure the latency per move for Transformer with MCTS and \ourmodel on a single V100 GPU with batch size 1. The performance of Transformer combined with MCTS is notably affected by the necessity of invoking the value network for every simulation. In contrast, \ourmodel experiences only a slight rise in latency as it requires just one call for greater depth. 


\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.98\linewidth]{figs/search-compare.pdf}
    \caption{\textbf{(Left)} Action accuracy when increasing average search depth in MCTS through more simulations and \ourmodel through context length extension. \textbf{(Middle)} Latency measured by ms per second when increasing search depth. \textbf{(Right)} Action accuracy when scaling data size.}
    \label{fig:implicit-vs-explicit}
    \vspace{-5pt}
\end{figure*}


\paragraph{\reb{Scaling}}
In Figure~\ref{fig:future-analysis}, the effectiveness of model scaling in \ourmodel has been observed. Here we explore the impact of increasing the dataset size on the performance. Specifically, we conduct experiments training the \ourmodel \texttt{S-ASA} model with a horizon of 4 and the Transformer \texttt{S-A} using game sizes ranging from 5k to 100k, as shown in the right figure of Figure~\ref{fig:implicit-vs-explicit}. Both the  Transformer and \ourmodel models exhibit a log-2 scaling behavior, showing that doubling the training data results in a linear increase in accuracy. Scaling also enhances future prediction significantly, leading to a more valid and accurate representation of future actions and states, as well as a near-perfect level of capturing the state-action transition dynamics, as detailed in Appendix~\ref{app-sec:scaling}.



\paragraph{Case study}
We sample several challenging puzzles from Lichess (with Elo ratings above 1800) to compare the predictions of \ourmodel and Transformer (\texttt{S-A}). Two instances are shown in Figure~\ref{fig:case-study}, with additional cases provided in Appendix~\ref{appendix:case_study}. 
\ourmodel demonstrates superior foresight, accurately predicting critical exchanges and piece sacrifices that lead to long-term strategic advantages. In the left puzzle, \ourmodel strategically sacrifices the rook to set up a long-term checkmate situation against the opponent. This maneuver compels the opponent to defend and creates an opportunity to capture the queen, facilitating valuable piece exchanges. The \texttt{S-A} model, unfortunately, makes a critical error by focusing on achieving direct checkmate without considering the possibility of the opponent's queen launching a counterattack. \reb{Similarly, in the right puzzle, \ourmodel anticipates an exchange sacrifice, correctly valuing the long-term positional benefits of opening lines by sacrificing the rook for its queen.} Conversely, the \texttt{S-A} model misjudges the value of this exchange, leading to suboptimal moves.
These findings highlight the effectiveness of \ourmodel in long-term planning without relying on explicit search. 

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\linewidth]{figs/case_study/case_crop.pdf}
    \caption{Two examples of Transformer (\texttt{S-A}) 
 and \ourmodel solving challenging puzzles. The predicted next move is in blue for both policies. The predicted future actions from \ourmodel are in light blue and red representing the two players, respectively, along with the numerical counters 1, 2, and 3 indicating future steps.}
    \label{fig:case-study}
    \vspace{-5pt}
\end{figure*}