\section{Related Work}
\textbf{Contrastive Learning} (CL) has become a powerful paradigm for graph representation learning by maximizing agreement across augmented views~\cite{Zhu:2020vf}. Early work such as DGI~\cite{velivckovic2018deep}, inspired by Deep InfoMax~\cite{hjelm2018learning}, maximizes mutual information between local patches and a global summary. Later approaches generate multiple views using various augmentations, e.g., feature/edge masking~\cite{thakoor2021large}, node dropping, subgraph extraction~\cite{You2020GraphCL}, node/edge insertion/deletion~\cite{zeng2021contrastive}, or graph diffusion~\cite{hassani2020contrastive,ma2023self}. However, most GCL methods still employ contrastive loss functions from computer vision, typically treating the same node in different views as a positive sample and all other nodes as negatives, thereby overlooking inherent graph topology~\cite{zhu2021graph,You2020GraphCL}.

\textbf{Hypergraph Neural Networks} (HyperGNNs) extend GNNs to model complex relationships through hyperedges connecting multiple nodes. HGNNs \cite{feng2019hypergraph} and HyperGCN \cite{yadati2019hypergcn} were pioneers, applying spectral convolution to hypergraphs using clique expansion and hypergraph Laplacians.
Moreover, attention-based models like HAN \cite{chen2020hypergraph} and HyperGAT \cite{ding2020more} adaptively learn node and hyperedge importance. HyperSAGE \cite{arya2020hypersage} and UniGNN \cite{huang2021unignn} avoid information loss by directly performing message passing on hypergraphs. The AllSetTransformer \cite{chien2021you} combines Deep Sets \cite{zaheer2017deep} and Set Transformers \cite{lee2019set} for enhanced flexibility and expressive power.