\section{Related Work}
\textbf{Contrastive Learning} (CL) has become a powerful paradigm for graph representation learning by maximizing agreement across augmented views____. Early work such as DGI____, inspired by Deep InfoMax____, maximizes mutual information between local patches and a global summary. Later approaches generate multiple views using various augmentations, e.g., feature/edge masking____, node dropping, subgraph extraction____, node/edge insertion/deletion____, or graph diffusion____. However, most GCL methods still employ contrastive loss functions from computer vision, typically treating the same node in different views as a positive sample and all other nodes as negatives, thereby overlooking inherent graph topology____.

\textbf{Hypergraph Neural Networks} (HyperGNNs) extend GNNs to model complex relationships through hyperedges connecting multiple nodes. HGNNs ____ and HyperGCN ____ were pioneers, applying spectral convolution to hypergraphs using clique expansion and hypergraph Laplacians.
Moreover, attention-based models like HAN ____ and HyperGAT ____ adaptively learn node and hyperedge importance. HyperSAGE ____ and UniGNN ____ avoid information loss by directly performing message passing on hypergraphs. The AllSetTransformer ____ combines Deep Sets ____ and Set Transformers ____ for enhanced flexibility and expressive power.