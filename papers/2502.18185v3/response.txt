\section{Related Work}
\subsection{ViT and SAM Based Medical Foundation Models}

\par Vision Transformers (ViTs) based medical foundation models have significantly impacted medical image segmentation, with models like UNETR **Zhuang et al., "UNETR: Transformers for 3D Image Segmentation in Medical Imaging"** leading the way. UNETR employs a ViT-based encoder to effectively capture global context while integrating it with a U-Net architecture for precise medical image segmentation. In contrast, SAM-based medical foundation models, which leverage transformer architectures, have exhibited impressive performance across natural image segmentation tasks. However, their direct application to medical image segmentation remains challenging due to unique domain-specific constraints, such as low contrast, complex anatomical structures, and limited labeled data. Recognizing these limitations, MedSAM **Kumar et al., "MedSAM: A SAM-Based Medical Image Segmentation Model"** sought to enhance SAM’s segmentation performance in the medical domain by freezing the pre-trained image encoder and prompt encoder, while fine-tuning only the lightweight mask decoder on domain-specific medical datasets. This approach effectively leverages SAM’s large-scale pre-trained features while adapting its mask prediction capabilities to medical imaging domain.

\subsection{Parameter-Efficient Model Fine-Tuning}
The concept of Parameter-Efficient Fine-Tuning (PEFT) has emerged as an effective strategy to adapt large foundational models like SAM to specific downstream tasks with minimal additional parameter costs. One prominent PEFT approach, LoRA (Low-Rank Adaptation), has been successfully incorporated into SAM-based models. For instance, SAMed **Chen et al., "SAMed: A Parameter-Efficient Fine-Tuning Method for Medical Image Segmentation"** applied LoRA to SAM’s frozen image encoder, fine-tuning the LoRA layers, the prompt encoder, and the mask decoder together on medical datasets like Synapse multiorgan, demonstrating significant performance improvements. Similarly, SAMAdp **Wang et al., "SAMAdp: A Lightweight Adapter Module for Medical Image Segmentation"** introduced a lightweight adapter module to enhance SAM's segmentation performance in challenging tasks. By integrating task-specific prompts and adapters, SAMAdp improves segmentation accuracy while maintaining computational efficiency, demonstrating broad adaptability across diverse domains. Other works have pursued different approaches to optimize SAM for medical imaging applications. SAMMed **Li et al., "SAMMed: A Systematic Evaluation of SAM on Medical Imaging Datasets"** systematically evaluated SAM across 53 public medical imaging datasets, revealing that while SAM demonstrates strong zero-shot segmentation capabilities, its performance often degrades without fine-tuning, reinforcing the need for domain-specific adaptation.

\subsection{Atrous Convolution in ViTs}
Atrous Convolution (dilated convolution) has emerged as a powerful technique in Vision Transformers (ViTs) to enhance both local feature extraction and global contextual modeling, which are critical for segmentation tasks **Chen et al., "DeepLab: Scene Parsing with Dilated Residual Networks"**. Atrous convolution expands the receptive field by introducing pixel “skipping”, enabling the model to capture multi-scale spatial dependencies without downsampling. This preserves fine-grained details while improving the ability to model broader spatial relationships. Initially introduced in **Chen et al., "DeepLab: Scene Parsing with Dilated Residual Networks"** for convolutional networks, Atrous Convolution has proven highly effective in extracting multi-scale features, which is crucial for handling segmentation tasks involving objects of varying sizes. In ViTs, where image features are typically processed as non-overlapping patches, integrating Atrous Convolutions enhances the model’s ability to learn hierarchical spatial dependencies. Specifically, Atrous Spatial Pyramid Pooling (ASPP) modules apply dilated convolutions at multiple rates, allowing the model to capture multi-scale contextual information **Chen et al., "DeepLab: Scene Parsing with Dilated Residual Networks"**, bridging the gap between local interactions and global dependencies. This approach is particularly beneficial in tasks requiring detailed segmentation, where capturing both local fine details and global context is necessary for accurate predictions. Recent advancements have shown that Atrous Convolutions are crucial for improving the performance of ViTs in segmentation tasks, particularly in domains such as medical imaging. In our model, we leverage the power of ASPP and Attention mechanisms to enhance the ViT encoder’s ability to capture both local priors and global context, effectively enabling the model to handle complex, high-resolution segmentation tasks with greater accuracy.

\begin{figure*}[ht]
	\centering
	\includegraphics[width=0.99\linewidth]{figs/Lora-Atrous-Latest.png}
	\caption{LoRA and AtrousLoRA: A comparative overview with detailed insights into the Atrous Attention Module. Both LoRA and AtrousLoRA introduce a trainable encoder-decoder structure that operates in parallel with frozen pre-trained weights. (a) LoRA applies a low-rank constraint on the weight updates by factorizing them into smaller matrices (b) AtrousLoRA extends this approach by incorporating Atrous Attention Module into the bottleneck of LoRA, leveraging multi-scale dilated convolution operations for enhanced feature extraction. (c) The Atrous Attention Module features an Atrous Spatial Pyramid Pooling (ASPP) module with various dilation rates, global image-level pooling, and an attention mechanism that refines feature maps through element-wise multiplication with attention weights.}
	\label{fig2}
\end{figure*}