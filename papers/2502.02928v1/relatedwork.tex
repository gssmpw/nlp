\section{Related Work}
\label{lit}
Automated high-quality code generation has evolved in several key directions related to prompt engineering~\cite{ai_literacy_p_e, Wei2022ChainOT}, multi-agent systems~\cite{wooldridge1995intelligent, approach_to_ai_agents}, and iterative debugging with runtime feedback mechanisms~\cite{generative_agents, chen2023teachinglargelanguagemodels, islam2024mapcodermultiagentcodegeneration, huang2024agentcodermultiagentbasedcodegeneration, software_testing}.
While LLM has demonstrated increasing capability in code generation, the code quality and correctness can be further enhanced by using intelligent debugging strategies.
Therefore, previous works in self-debugging~\cite{Huang2023AnES, chen2023teachinglargelanguagemodels} show their capability in Automated Program Repair (APR) by identifying and fixing significant code flaws and bugs.
Researchers have shown that "large language models of code"~\cite{Huang2023AnES} often struggle to directly fix bugs in the generated code due to the lack of task-specific fine-tuning. 
These fine-tuned models outperform the state-of-the-art APR tools~\cite{lesstraining, vulrepair, repairisnearly} but still suffer from the loss of pre-trained knowledge~\cite{Xia2022LessTM, adnan2024unleashingartificialcognitionintegrating}, lack of debugging ingredients, inferior long-sequence handling ability, high resource constraints, long-tail problems, and multi-hunk error fixing~\cite{Huang2023AnES}.

Particulary, MapCoder~\cite{islam2024mapcodermultiagentcodegeneration} presents a multi-agent framework designed to emulate human-level programming processes. 
It employs four agents for retrieval, planning, coding, and debugging to perform iterative code generation and debugging.
AgentCoder~\cite{huang2024agentcodermultiagentbasedcodegeneration} introduces a three-agent framework to address high token usage and coordination overhead observed in other multi-agent systems like MapCoder~\cite{islam2024mapcodermultiagentcodegeneration}, MetaGPT~\cite{hong2024metagptmetaprogrammingmultiagent}, and ChatDev~\cite{qian2024chatdevcommunicativeagentssoftware}.
The framework comprises a programmer agent, a test designer agent, and a test executor agent.
The "Debug Like a Human" framework~\cite{zhong2024debuglikehumanlarge} introduces Large Language Model Debugger (LDB), which enhances the code quality by leveraging runtime execution information. 
LDB is performed iteratively through segmenting code, tracking intermediate variable states, and performing step-by-step verification with breakpoints.
Moreover, some approaches have employed prompt engineering to improve the quality of code generation and debugging.
For instance,  CodeCoT~\cite{huang2024codecottacklingcodesyntax} leverages iterative refinement through Chain of Thought (CoT)~\cite{Wei2022ChainOT} and logical reasoning. 
However, its capability is limited to refining syntax errors.

While these approaches have advanced automated code generations, they share several notable limitations, particularly the resource intensity for complex problems. 
Additionally, they often struggle to fully leverage error messages, which impairs their adaptability to nuanced debugging scenarios. 
Although this issue can be alleviated by generating complementary test cases, it can further increase resource consumption due to the reliance on preemptive test cases such that generation may become less reliable ~\cite{software_testing, li2024largelanguagemodelstest} and potentially require more complex agents.

To address these challenges, we introduce PyCapsule as a streamlined modular architecture with robust and optimized modules. 
It combines the knowledge from the operation research field and computer science to optimize the processing pipeline with merely two AI agents. This greatly reduces the computational overhead and facilitates more efficient and reliable code generation.
%===============================================================================================