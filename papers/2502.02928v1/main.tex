%%
%% This is file `sample-acmsmall.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,journal,bibtex,acmsmall')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-acmsmall.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro ~\cite [option:text,text]
%TC:macro ~\citep [option:text,text]
%TC:macro ~\citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
%% TODO remove nonacm
\documentclass[acmsmall,nonacm]{acmart}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}

%%
%% These commands are for a JOURNAL article.
%% TODO uncomment these
% \acmJournal{JACM}
% \acmVolume{37}
% \acmNumber{4}
% \acmArticle{111}
% \acmMonth{8}

%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command ~\citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
% ~\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage[toc,page]{appendix}
% \usepackage{minted}
\usepackage{comment}
% \setminted{
%     breaklines=true,
%     breakanywhere=true,
%     style=bw,          % removes syntax highlight
%     breakafter={\:},   % breaks after spaces
%     breaksymbolleft={}, % removes the arrw symbol
%     breaksymbolright={},
%     fontsize=\small
% }
\usepackage{listings}
\lstset{
    breaklines=true,
    breakatwhitespace=true,
    basicstyle=\small,
    breakindent=0pt,
    postbreak={},
    frame=none,
    numbers=none,
    captionpos=b
}
\lstdefinestyle{plaintext}{
    basicstyle=\small\ttfamily,
    columns=flexible,
    keepspaces=true
}
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Large Language Model Guided Self-Debugging Code Generation}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Muntasir Adnan}
\authornote{Equal contributions.}
\email{adnan.adnan@canberra.edu.au}
\orcid{0009-0002-6345-9364}
\author{Zhiwei Xu}
\email{danny.xu@canberra.edu.au}
\orcid{0000-0001-8283-6095}
\author{Carlos C. N. Kuhn}
\authornotemark[1]
\email{carlos.noschangkuhnu@canberra.edu.au}
\orcid{0000-0001-8733-6962}
\affiliation{%
  \institution{Open Source Institute, Faculty of Science and Technology, University of Canberra}
  \city{Bruce}
  \state{ACT}
  \country{ Australia}
}


% \author{Lars Th{\o}rv{\"a}ld}
% \affiliation{%
%   \institution{The Th{\o}rv{\"a}ld Group}
%   \city{Hekla}
%   \country{Iceland}}
% \email{larst@affiliation.org}

% \author{Valerie B\'eranger}
% \affiliation{%
%   \institution{Inria Paris-Rocquencourt}
%   \city{Rocquencourt}
%   \country{France}
% }

% \author{Aparna Patel}
% \affiliation{%
%  \institution{Rajiv Gandhi University}
%  \city{Doimukh}
%  \state{Arunachal Pradesh}
%  \country{India}}

% \author{Huifen Chan}
% \affiliation{%
%   \institution{Tsinghua University}
%   \city{Haidian Qu}
%   \state{Beijing Shi}
%   \country{China}}

% \author{Charles Palmer}
% \affiliation{%
%   \institution{Palmer Research Laboratories}
%   \city{San Antonio}
%   \state{Texas}
%   \country{USA}}
% \email{cpalmer@prl.com}

% \author{John Smith}
% \affiliation{%
%   \institution{The Th{\o}rv{\"a}ld Group}
%   \city{Hekla}
%   \country{Iceland}}
% \email{jsmith@affiliation.org}

% \author{Julius P. Kumquat}
% \affiliation{%
%   \institution{The Kumquat Consortium}
%   \city{New York}
%   \country{USA}}
% \email{jpkumquat@consortium.net}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{M. Adnan, Z. Xu, and C. C.N. Kuhn}

\begin{abstract}
Automated code generation is gaining significant importance in intelligent computer programming and system deployment.
However, current approaches often face challenges in computational efficiency and lack robust mechanisms for code parsing and error correction.
% Automated code generation is raising high significance in intelligent computer programming and system deployment but often suffers from high processing complexity and inefficacious approach to parsing and correcting code.
In this work, we propose a novel framework, PyCapsule, with a simple yet effective two-agent pipeline and efficient self-debugging modules for Python code generation.
PyCapsule features sophisticated prompt inference, iterative error handling, and case testing, ensuring high generation stability, safety, and correctness.
Empirically, PyCapsule achieves up to 5.7\% improvement of success rate on HumanEval, 10.3\% on HumanEval-ET, and 24.4\% on BigCodeBench compared to the state-of-art methods.
We also observe a decrease in normalized success rate given more self-debugging attempts, potentially affected by limited and noisy error feedback in retention.
PyCapsule demonstrates broader impacts on advancing lightweight and efficient code generation for artificial intelligence systems.
\end{abstract}

\begin{CCSXML}
<ccs2012>
   <concept><concept_id>10011007.10011074.10011092.10011782</concept_id>
   <concept_desc>Software and its engineering~Automatic programming</concept_desc>
   <concept_significance>500</concept_significance>
   </concept>

  <concept><concept_id>10010147.10010178.10010179</concept_id>
   <concept_desc>Computing methodologies~Natural language processing</concept_desc>
   <concept_significance>500</concept_significance>
   </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~Automatic programming}
\ccsdesc[500]{Computing methodologies~Natural language processing}

\keywords{Python Code Generation, Self-debugging, Large Language Model, Programming Agent, Artificial Intelligence Automation} 

% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}
\received{XX}
\received[revised]{XX}
\received[accepted]{XX}

\maketitle
\shortauthors

\section{Introduction}\label{intro}
The evolution of generative artificial intelligence has profoundly influenced multiple domains, notably in automated code generation.
Particularly, Large Language Models (LLMs) have demonstrated remarkable proficiency in code generation given natural language prompts, streamlining software development processes and enhancing productivity ~\cite{emperical_study_soft_eg, emp_study_critical, palm10.5555/3648699.3648939}.
This transformative potential has sparked significant research in this field.
However, several critical challenges emerge as these systems scale to handle increasingly complex programming tasks.
Ensuring reliability, accuracy, and autonomy in code generation remains critical but challenging, particularly in error detection and debugging, that align with human developers' intentions  ~\cite{llm_se_problems, dou2024whatswrongcodegenerated, imperfect_code_gen}. 
Recent research has introduced frameworks consisting of multiple LLM-based agents to emulate human problem-solving capability in code generation. 
For instance, MapCoder ~\cite{islam2024mapcodermultiagentcodegeneration}, AgentCoder ~\cite{huang2024agentcodermultiagentbasedcodegeneration}, and LDB ~\cite{zhong2024debuglikehumanlarge} employ agents dedicated to retrieval, planning, coding, and debugging to enhance code quality. 
While these approaches have shown remarkable improvements, they often require substantial computational resources and complex agent coordination.
This poses challenges for real-world deployment and scalability.
% While effective, these resource-intensive approaches pose challenges for real-world deployment and scalability.

To address these concerns, we propose a novel framework, PyCapsule, that enhances the efficiency and accuracy of LLM-based Python code generation through a two-agent architecture. This simple yet effective two-agent interaction ensures functionality independence without overcrowding with AI agents, thereby minimizing its computational consumption. 
PyCapsule employs a \textit{programmer agent} responsible for code generation and debugging, interacting with an \textit{executor agent} that handles code validation, case testing, and error analysis.
The framework's effectiveness is further enhanced by three specialized modules: an error handler that refines the debugging feedback, an example call detector that prevents runtime issues, and a function signature converter that provides a uniform template for the function head description. 
This structure enables PyCapsule to achieve high performance on widely used datasets, including HumanEval~\cite{codex}, HumanEval-ET~\cite{dong2023codescore}, Mostly Basic Python Programming (MBPP)~\cite{austin2021program}, MBPP-ET~\cite{dong2023codescore}, and BigCodeBench ~\cite{zhuo2024bigcodebench} with a high computational efficiency.
The main contributions of our work are summarized as
\begin{itemize}
    \item Our PyCapsule is a resource-efficient framework that achieves high performance with significantly reduced computational overhead compared to existing multi-agent approaches.
    Our two-agent architecture demonstrates that complex coordination between multiple agents can be replaced by well-structured language prompts and modules without sacrificing accuracy. 

    \item Our efficient debugging mechanism aids the programmer agent through three specialized modules: an error handler, an example call detector, and a function signature converter. These achieve up to 5\% accuracy improvement on HumanEval, 10\% on HumanEval-ET using GPT models, and 25\% on BigCodeBench using the Qwen 7B model compared to its 32B variant.

    \item Through comprehensive analysis across multiple datasets and models, we observe a consistent decrease in the normalized success rate as we increase the number of self-debugging attempts.
    We hypothesize that this is caused by the increased problem complexity due to limited and noisy error messages for the self-debugging process.
    % from the conversational history.
    % Our hypothesis for the reason is the 
    % pattern of diminishing returns in successive debugging attempts, with each subsequent attempt showing reduced influence on problem resolution. 
\end{itemize}
%=================================================================================

\section{Related Work}\label{lit}
Automated high-quality code generation has evolved in several key directions related to prompt engineering~\cite{ai_literacy_p_e, Wei2022ChainOT}, multi-agent systems~\cite{wooldridge1995intelligent, approach_to_ai_agents}, and iterative debugging with runtime feedback mechanisms~\cite{generative_agents, chen2023teachinglargelanguagemodels, islam2024mapcodermultiagentcodegeneration, huang2024agentcodermultiagentbasedcodegeneration, software_testing}.
While LLM has demonstrated increasing capability in code generation, the code quality and correctness can be further enhanced by using intelligent debugging strategies.
Therefore, previous works in self-debugging~\cite{Huang2023AnES, chen2023teachinglargelanguagemodels} show their capability in Automated Program Repair (APR) by identifying and fixing significant code flaws and bugs.
Researchers have shown that "large language models of code"~\cite{Huang2023AnES} often struggle to directly fix bugs in the generated code due to the lack of task-specific fine-tuning. 
These fine-tuned models outperform the state-of-the-art APR tools~\cite{lesstraining, vulrepair, repairisnearly} but still suffer from the loss of pre-trained knowledge~\cite{Xia2022LessTM, adnan2024unleashingartificialcognitionintegrating}, lack of debugging ingredients, inferior long-sequence handling ability, high resource constraints, long-tail problems, and multi-hunk error fixing~\cite{Huang2023AnES}.

Particulary, MapCoder~\cite{islam2024mapcodermultiagentcodegeneration} presents a multi-agent framework designed to emulate human-level programming processes. 
It employs four agents for retrieval, planning, coding, and debugging to perform iterative code generation and debugging.
AgentCoder~\cite{huang2024agentcodermultiagentbasedcodegeneration} introduces a three-agent framework to address high token usage and coordination overhead observed in other multi-agent systems like MapCoder~\cite{islam2024mapcodermultiagentcodegeneration}, MetaGPT~\cite{hong2024metagptmetaprogrammingmultiagent}, and ChatDev~\cite{qian2024chatdevcommunicativeagentssoftware}.
The framework comprises a programmer agent, a test designer agent, and a test executor agent.
The "Debug Like a Human" framework~\cite{zhong2024debuglikehumanlarge} introduces Large Language Model Debugger (LDB), which enhances the code quality by leveraging runtime execution information. 
LDB is performed iteratively through segmenting code, tracking intermediate variable states, and performing step-by-step verification with breakpoints.
Moreover, some approaches have employed prompt engineering to improve the quality of code generation and debugging.
For instance,  CodeCoT~\cite{huang2024codecottacklingcodesyntax} leverages iterative refinement through Chain of Thought (CoT)~\cite{Wei2022ChainOT} and logical reasoning. 
However, its capability is limited to refining syntax errors.

While these approaches have advanced automated code generations, they share several notable limitations, particularly the resource intensity for complex problems. 
Additionally, they often struggle to fully leverage error messages, which impairs their adaptability to nuanced debugging scenarios. 
Although this issue can be alleviated by generating complementary test cases, it can further increase resource consumption due to the reliance on preemptive test cases such that generation may become less reliable ~\cite{software_testing, li2024largelanguagemodelstest} and potentially require more complex agents.

To address these challenges, we introduce PyCapsule as a streamlined modular architecture with robust and optimized modules. 
It combines the knowledge from the operation research field and computer science to optimize the processing pipeline with merely two AI agents. This greatly reduces the computational overhead and facilitates more efficient and reliable code generation.
%===============================================================================================
\section{Methodology}\label{method}
\subsection{Programmer and Executor Agents}\label{pycapsule}

\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{figures/pycapsule.png}
    \caption{PyCapsule framework as Python code generation service of OpenSI-CoSMIC~\cite{adnan2024unleashingartificialcognitionintegrating}.
    PyCapsule consists of iterative code generation with debugging, error handling, and code execution within a Docker container.}
    \label{fig: flowchart}
\end{figure}

% PyCapsule serves as an integral part of CoSMIC~\cite{adnan2024unleashingartificialcognitionintegrating}'s Python code generation service.
PyCapsule employs two distinct types of agents as defined in the agent-based computing literature~\cite{Nwana_1996, roadmap}: smart agents and collaborative agents. 
The system consists of two agents: a \textit{programmer agent} and an \textit{executor agent}.
The \textit{programmer agent} generates code and handles debugging while the \textit{executor agent} acts as an autonomous validator that continuously monitors and responds to code execution outcomes. 
When execution succeeds, it forwards the validated code to the user; otherwise, it generates detailed diagnostic reports and initiates a real-time feedback loop with the programmer agent for self-debugging.
% The \textit{programmer agent} functions as a "smart agent"~\cite{Nwana_1996} for code generation, while the \textit{executor agent} operates as a "collaborative agent"~\cite{Nwana_1996} focused on systematic code execution and validation.

The programmer agent leverages Ollama and GPT infrastructure embedded in OpenSI-CoSMIC~\cite{adnan2024unleashingartificialcognitionintegrating} to facilitate self-debugging and code generation through a structured pipeline. 
The programmer agent employs tailored system prompts with two programming modes, \textit{generation mode} and \textit{fix mode}. 
These prompts, detailed in Appendix ~\ref{appendix: prompt}, define the programmer agent's persona~\cite{role-play}, provide task-specific background, and structurally simplify code extraction. 
In the \textit{generation mode}, the programmer agent uses CoT~\cite{Wei2022ChainOT} to analyze the problem description and devise a structured solution to generate executable code.
In the \textit{fix mode}, the executor agent uses error information from the previous debugging attempt.
This mode is supported by the original problem description, prior solution, and error messages processed by the \textit{error-handling} module.
The conversation history includes the most recent problem-solution pair.
% which contribute to $\sim$2\% improvement in the success rate. 
For MBPP with Qwen2.5-Coder-7B-Instruct, tests with 2 and 3 conversation pairs decrease the accuracy from 80.7\% to 77.6\% and 76.7\%, respectively.

We implemented the \textit{executor agent} using a lightweight Docker container.
The container's entry point is a shell script for installing required Python libraries and testing the generated code, which are generated by the programmer agent.
Once the problem is solved, the container returns a success or failure status. 
The success status indicates that the generated code is correct by passing all the test cases while the failure status will trigger the error-handling module to refine and send the error message to the programmer agent.
The programmer agent will then enter the fix mode to fix code flaws iteratively with self-debugging attempts.
% using a maximum of 5 debugging attempts.
Particularly, with our analysis in Section \ref{result} showing the diminishing influence of successive debugging attempts, the programmer agent is capable of refining the incorrect code with up to 5 self-debugging attempts.
This seamless interaction between the two agents ensures robust and efficient feedback-driven debugging.
%==============================================================================================

\subsection{Supportive Modules}\label{aux}
Existing approaches like MapCoder~\cite{islam2024mapcodermultiagentcodegeneration} and AgentCoder~\cite{huang2024agentcodermultiagentbasedcodegeneration} rely heavily on LLM agents for code generation.
However, this requires a high resource consumption.
In contrast, PyCapsule employs three specialised modules to handle each task deterministically, including a signature converter, an example call detector, and an error handling module.

The \textit{signature converter} mitigates the instability of code generation by parsing problem descriptions provided in datasets through inferring structured function signatures.
This module generates a function name, a robust signature, and an example call
% for externally given contexts, which 
using the first test case 
% without code execution. 
for a given problem without revealing the case-testing result, thereby avoiding extra LLM calls for signature inference.

The \textit{example call detector} ensures the safety of code execution by removing any internal example calls, which otherwise can lead to an infinite loop of function calls. 
This automatic detection replaces a dedicated LLM agent to check code execution safety.

The \textit{error-handling} module refines error messages provided by the executor agent, giving concise and highly relevant natural language feedback. 
It also identifies the error type, filters irrelevant tracebacks, and resolves critical issues such as verbose recursion errors that can disrupt the programmer agent due to the LLM's limited context length.
This systematic approach requires fewer tokens in self-debugging iterations compared to the raw but redundant error messages.

These modules collectively enhance the code generation stability, safety, and self-debugging efficiency while reducing the computational overhead. 
More details are provided in Appendix~\ref{appendix: aux}.
%=============================================================================================

\subsection{Code Generation Workflow}
\label{workflow}
PyCapsule consists of three phases: code generation, execution, and self-debugging, shown in Figure~\ref{fig: flowchart}. 
This pipeline integrates the programmer agent and executor agent with the self-bugging mechanism to ensure reliable code generation and case testing with high efficiency and accuracy.

The workflow starts with the programmer agent that receives a problem description,
% with associated test cases, 
typically supplemented by a function signature either from the dataset or the signature converter module.
Subsequently, the agent generates a response with CoT reasoning, environment setup instructions, and code synthesis.
Then, the agent's response is parsed to extract the synthesized code and environment setup instructions, with the latter automatically generating a text file, \texttt{requirements.txt}, containing all required Python libraries.

The extracted code is further processed through the example call detector to remove any unintended example calls.
At the generation phase, the processed code with associated test cases from the dataset is saved in a Python entry file \texttt{main.py} for code execution.
To ensure code execution safety, we add a timer with a maximum execution time to prevent infinite loops and raise an out-of-time error for those requiring longer running time.

At the execution phase, the executor agent runs \texttt{main.py} in a Docker container with Python libraries from \texttt{requirements.txt} installed.
The corresponding code execution status, either successful or failed, determines whether the current generated code requires to be fixed by using the programmer agent's fix mode.
If so, error messages extracted from the executor agent are refined by the error-handling module and will used by the programmer agent to fix code flaws.
This mode allows up to 5 self-debugging attempts, each incorporating code execution feedback from the previous bug-fixing process. 
The pseudo-code for the pipeline is provided in Appendix~\ref{appendix: pseudo}.
%============================================================================================

\section{Experiments}\label{experiments}
All GPU-related experiments were conducted on a single NVIDIA GeForce RTX 3090 (24 GB).
We evaluated performance using success rate, defined by the ratio of correctly resolved problems to total problems.
Our code will be released upon publication.

\subsection{Dataset Overview}\label{dataset}
We used five benchmark datasets to evaluate PyCapsule:
HumanEval~\cite{codex}, HumanEval-ET~\cite{dong2023codescore}, MBPP~\cite{austin2021program}, MBPP-ET~\cite{dong2023codescore}, and BigCodeBench~\cite{zhuo2024bigcodebench},
where the "ET" variants provide more test cases and enriched problem descriptions.
These datasets encompass diverse programming challenges.

Particularly, HumanEval~\cite{codex} consists of 164 human-curated programming problems for string manipulation, arithmetic, and basic data structures. 
Each problem includes a natural language description, a function signature, and test cases.
MBPP~\cite{austin2021program} includes 974 programming problems spanning algorithms, data structures, and general-purpose tasks.
It provides natural language descriptions and test cases but no function signatures to guarantee code generation stability.
% Qwen2.5-Coder technical report ~\cite{hui2024qwen25codertechnicalreport}, highlights MBPP's sensitivity to prompt design, with techniques like 3-shot examples mitigating instability. 
% We addressed this using our \textit{signature converter} module.
%
BigCodeBench~\cite{zhuo2024bigcodebench} includes 1,140 problems with two splits: the \textit{complete} split for code completion with specific docstrings and the \textit{instruct} split with concise instructions for advanced reasoning. 
% We evaluated our system using both splits but did not use BigCodeBench's Python evaluation library, as it is specifically designed for standalone LLMs only.
% We used both since its Python evaluation library is tailored to selected LLMs.
% We further implemented system prompts instructing: ``Do not change the function name or signature even if there is a typo,'' but this yielded no statistically significant improvement. This observation raises questions about the limitations of complex system prompts—a topic outside the scope of this study. Another observation pertinent to a later Section revealed that some agents still called the generated function within example executions, potentially resulting in infinite loops. For instance, Qwen2.5-coder-instruct-7B called the generated function in 21 out of 164 instances for HumanEval, and for MBPP in 44 out of 344 instances, necessitating further adjustments to prevent infinite loops.
% ======================================================================
\subsection{Results and Analysis}\label{result}

\begin{table*}[t]
\centering
\caption{Success rate comparison on three popular LLMs.
We provide the mean value and standard deviation for PyCapsule with three experiment repeats.
The results of other methods are from their official reports.}
\label{table_acc}
\setlength{\tabcolsep}{7pt}
\resizebox{\textwidth}{!}{
\begin{tabular}{lccccc}
\toprule
\multicolumn{1}{c}{\textbf{Method}} & \textbf{HumanEval} & \textbf{HumanEval-ET} & \textbf{MBPP} & \textbf{MBPP-ET} & \textbf{BigCodeBench} \\
\midrule
\multicolumn{6}{c}{GPT-4-Preview-1106} \\
Direct 
& \hspace{7mm}80.1 ~\cite{shinn2023reflexionlanguageagentsverbal} 
& --
& \hspace{7mm}81.1 ~\cite{islam2024mapcodermultiagentcodegeneration} 
& -- 
& -- \\
CoT~~\cite{islam2024mapcodermultiagentcodegeneration} 
& 89.0  
& --
& 82.4
& -- 
& -- \\
Self-Planning~\cite{islam2024mapcodermultiagentcodegeneration} 
& 85.4 
& --
& 75.8
& -- 
& -- \\
AgentCoder\footnotemark[1]  ~\cite{huang2024agentcodermultiagentbasedcodegeneration} 
& 96.3
& 86.0
& \textbf{91.8}
& \textbf{91.8}
& -- \\
MapCoder ~\cite{islam2024mapcodermultiagentcodegeneration} 
& 93.9 
& 82.9
& 82.1 
& 57.7 
& -- \\
PyCapsule (ours) 
& \hspace{6mm}\textbf{96.5$\pm$0.7} 
& \hspace{6mm}\textbf{96.3$\pm$1.2}
& \hspace{6mm}88.2$\pm$0.3
& \hspace{6mm}73.0$\pm$0.8
& -- \\
\midrule
\multicolumn{6}{c}{GPT-3.5-Turbo-1106} \\
Direct
& \hspace{7mm}48.1 ~\cite{openai2024gpt4technicalreport}
& --
& \hspace{7mm}49.8 ~\cite{islam2024mapcodermultiagentcodegeneration}
& -- 
& -- \\
Direct (LDB~\cite{zhong2024debuglikehumanlarge}) 
& 73.8
& --
& 67.6
& -- 
& -- \\
CoT~\cite{islam2024mapcodermultiagentcodegeneration} 
& 68.9 
& --
& 54.5
& -- 
& -- \\
Self-Planning~\cite{shinn2023reflexionlanguageagentsverbal}
& 60.3 
& --
& 55.7
& -- 
& -- \\
Reflexion~\cite{shinn2023reflexionlanguageagentsverbal}
& 67.1 
& --
& 73.0
& -- 
& -- \\
AgentCoder\footnotemark[1] ~\cite{huang2024agentcodermultiagentbasedcodegeneration} 
& 79.9 
& 77.4
& \textbf{89.9} 
& \textbf{89.1}
& -- \\
MapCoder ~\cite{islam2024mapcodermultiagentcodegeneration} 
& 80.5 
& 70.1
& 78.3 
& 54.4
& -- \\
LDB ~\cite{zhong2024debuglikehumanlarge} 
& 82.9 
& --
& 76.0 
& -- 
& -- \\
PyCapsule (ours) 
& \hspace{6mm}\textbf{85.2$\pm$0.7} 
& \hspace{6mm}\textbf{84.7$\pm$0.6} 
& \hspace{6mm}78.5$\pm$1.2
& \hspace{6mm}62.4$\pm$0.6
& -- \\
\midrule

\multicolumn{6}{c}{Qwen2.5-Coder-7B-Instruct} \\
Direct ~\cite{hui2024qwen25codertechnicalreport} 
& 88.4 
& --
& \textbf{83.5}
& --
& 41.0 \\
PyCapsule (ours) 
& \hspace{6mm}\textbf{94.1$\pm$1.3} 
& \hspace{6mm}\textbf{93.3$\pm$0.6}
& \hspace{6mm}80.7$\pm$0.9
& \hspace{6mm}\textbf{63.6$\pm$0.6}
& \hspace{6mm}\textbf{65.4$\pm$0.8} \\
\bottomrule
\end{tabular}
}
\end{table*}
%table_end=====================================================================================
\footnotetext[1]{Could not be reproduced during our experiments.}
%==============================================================================================

Unlike other multi-agent architectures discussed in Section \ref{lit} that heavily rely on code generation agents, PyCapsule has more effective coordination among the computational overhead reduction, LLM's API calls, and token processing.

In Table \ref{table_acc}, PyCapsule achieves the state-of-the-art code generation tasks across HumanEval, HumanEval-ET, and BigCodeBench.
While showing strong performance on MBPP, it falls short in comparison to AgentCoder and Qwen-2.5~\cite{hui2024qwen25codertechnicalreport} which employs three-shot prompt engineering to stabilize code generation.
Although it shows remarkable potential, we maintain a consistent prompting approach across datasets to ensure comparability and utilise our signature converter module to generate actionable function signatures.
Remarkably, on HumanEval, the integration of Qwen-2.5-Coder-Instruct's 7B model with PyCapsule achieves a 94.1\% success rate compared to 92.7\% by its 32B model.
Furthermore, on BigCodeBench, PyCapsule using Qwen-2.5-Coder-Instruct's 7B shows a 25\% improvement over the 49.6\% reported by the 32B model variant. 
This demonstrates a significant savings in computational power while exceeding the performance. 

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/pass_distribution.png}
    \caption{Distribution of relative success ratios along the self-debugging attempts.
    The relative success ratio refers to the number of successful test cases with given attempts over the total number of successful test cases, measured by unit \%.
    Results on all the datasets consistently show a decreasing relative success ratio as the number of attempts increases.}
    \label{fig:pie_fig}
\end{figure}

\setlength{\parindent}{0cm}{
\paragraph{\textbf{Influence of Self-debugging Attempts.}} To evaluate the influence of each self-debugging attempt on the success rate, we analyze the normalized and accumulated impacts of each incremental attempt using GPT-4-1106-preview, GPT-3.5-Turbo-1106, and Qwen-2.5-Coder-Instruct models as the LLM for code generation.
The success rate normalization was performed as follows: given \( N \) problems, the number of successfully solved problems is denoted as $S_0$ while the rest $N_1 = N - S_0$ unsolved problems will be further resolved with a self-debugging attempt. 
Accordingly, the number of problems solved with $i \geq 1$ attempts is denoted as $S_i$, and $N_{i+1} = N_i - S_i$ for the rest.
The independent influence is defined as}
\[
I_i = \frac{S_i}{N_i}, \quad \text{where } N_i = N - \sum_{j=0}^{i-1} S_j\ .
\]
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/trend.png}
    \caption{The left subfigure shows the normalized influence of each self-debugging attempt through independent accuracy improvements.
    The right subfigure highlights the mean influence of each self-debugging attempt on the overall accuracy through the accumulated influence without normalization.}
    \label{fig:influence}
\end{figure}

This metric highlights the independent contribution of each attempt.
%
In Figure~\ref{fig:influence}, the descending trend in the independent influence indicates diminishing effectiveness of debugging attempts, where problems requiring more attempts may need enriched conversation history or enhanced prompts to improve the accuracy.
We hypothesize that the first few self-debugging attempts address the most significant and easily identifiable errors while substantially improving the success rate. 
However, since these primary issues are resolved, more subtle and complex issues occur, and they are difficult to be detected and solved even with more attempts, resulting in diminishing normalised accuracy while increasing the overall accuracy by small amounts, as demonstrated by the right plot of Figure~\ref{fig:influence} . 
Additionally, the accumulation of minor fixes may introduce unseen issues that further degrade the effect of self-debugging attempts.

% =============================================================================================

\subsection{Discussion}
% The single attempt performance gap between PyCapsule and Qwen-2.5 Coder Instruct varies significantly across datasets, as shown in Figure \ref{fig:pie_fig} and detailed in Appendix \ref{appendix: table}.
% Specifically, PyCapsule's single-attempt accuracy is significantly lower than Qwen-2.5's reported performance
% Nevertheless, PyCapsule achieves higher single-attempt accuracy compared to GPT models implemented in MapCoder and AgentCoder.
% 
PyCapsule's single-attempt accuracy is significantly lower than Qwen-2.5's reported performance, and it varies considerably across datasets, as shown in Figure \ref{fig:pie_fig} and detailed in Appendix \ref{appendix: table}.
Nevertheless, PyCapsule achieves higher single-attempt accuracy compared to GPT models implemented in MapCoder and AgentCoder.
% 
This nuanced performance difference highlights the varying impacts of prompt engineering and framework structure on code generation without self-debugging. 
With more debugging attempts, PyCapsule shows overall remarkable improvements, surpassing all the other self-debugging methods.
This iterative improvement highlights the effectiveness of prompt engineering and error-handling ability of PyCapsule.
Prior works~\cite{Wei2022ChainOT, prompt_eg_consistency, eval_prompt_eg} have demonstrated how prompt design significantly influences model performance, especially for generative AI tasks. 
PyCapsule leverages sophisticated system prompts 
that are dynamically augmented in the code generation and fix modes.
Furthermore, an important advantage of PyCapsule is its reduced reliance on LLM-based agents, leading to reduced token consumption and improved computational efficiency. 
Existing frameworks like MapCoder can make up to 17 API calls with GPT models per problem on HumanEval and 12 calls on MBPP, following \( k \times t \) debugging attempts, where \( k \) is the number of retrieved exemplars and \( t \) is the number of self-debugging attempts. 
In contrast, PyCapsule streamlines this process by limiting the programmer agent with a maximum of 5 attempts, requiring at most 6 LLM API calls for each problem because planning and code generation are handled within a single API call. 
Our empirical analysis shows that PyCapsule achieves superior code generation ability while maintaining efficient API usage, requiring an average of only 2.09, 1.38, and 1.55 API calls for HumanEval using GPT-3.5-Turbo-1106, GPT-4-Preview-1106, and Qwen-2.5-Coder-Instruct-7B, respectively.
This optimization demonstrates the possibility of achieving high performance with significantly reduced computational overhead.
%===================================================================

\section{Conclusion}\label{con}
We propose PyCapsule for significantly advancing reliable and automated code generation by integrating two AI agents with high-efficiency self-debugging modules. 
Its two-agent architecture effectively balances code generation automation and control while reducing resource consumption. Experimental results show that PyCapsule achieves high performance across multiple datasets, surpassing existing debugging-based approaches. 
Notably, PyCapsule enhances accuracy while using smaller language models and fewer API calls, offering a cost-effective alternative without compromising its performance. The iterative self-debugging feedback and dynamic prompt augmentation contribute to its robustness and improvements.

%===================================================================
% \clearpage
\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

%===================================================================
\clearpage
\appendix
\noindent {\LARGE \textbf{Appendix}}

\section{Contribution of Each Debugging Attempt}
\label{appendix: table}
To further illustrate the influence of iterative debugging, we present a detailed breakdown of the success rate at each debugging attempt.
Table \ref{tab:model_performance} quantifies the proportion of problems successfully resolved at each stage. While the independent influence of successive attempts gradually decreases, the cumulative effect leads to an overall increase in the accuracy of problem-solving. 
Each debugging attempt contributes to solving additional problems, underscoring the value of iterative refinement in improving outcomes. 
This dual perspective—rising cumulative accuracy alongside diminishing returns per attempt—demonstrates the significance of leveraging efficient debugging strategies, such as enriched conversation history and refined prompts, to maximize the potential of LLM-based code generation systems.
\begin{table}[ht]
\centering
\caption{Accuracy of PyCapsule framework across code generation and debugging attempts (0–5) for OpenAI GPT-4-Preview-1106, GPT-3.5-Turbo-1106 and Qwen 2.5 Coder Instruct. Attempt 0 represents the initial solution generated by the Programmer agent, while subsequent attempts (1–5) indicate iterative fixes during the debugging process. The table reports accuracy with associated sample standard deviation ($\pm s$). The results also showcase the incremental improvement achieved through PyCapsule's iterative feedback mechanism.\\}
\label{tab:model_performance}
\setlength{\tabcolsep}{9.5pt}
\resizebox{\textwidth}{!}{
\begin{tabular}{crcrrrr}
\toprule
\multicolumn{1}{c}{\textbf{Attempt}} & \textbf{HumanEval} & \textbf{HumanEval-ET} & \textbf{MBPP} & \textbf{MBPP-ET} & \textbf{BigCodeBench} \\
\midrule
\multicolumn{6}{c}{GPT-4-Preview-1106} \\ 
0  & 92.0 $\pm$ 1.4 & 88.0 $\pm$ 3.7 & 70.6 $\pm$ 1.0 & 67.5 $\pm$ 0.8 & - \\ 
1  & 3.8 $\pm$ 0.7 & 8.2 $\pm$ 4.3 & 18.0 $\pm$ 1.1 & 17.2 $\pm$ 1.6 & - \\ 
2  & 1.9 $\pm$ 1.1 & 2.3 $\pm$ 1.4 & 5.9 $\pm$ 1.5 & 8.6 $\pm$ 0.5 & - \\ 
3  & 1.1 $\pm$ 0.4 & 0.8 $\pm$ 0.4 & 3.0 $\pm$ 0.3 & 3.2 $\pm$ 0.3 & - \\ 
4  & 1.1 $\pm$ 0.7 & 0.6 $\pm$ 0.0 & 1.4 $\pm$ 0.5 & 1.9 $\pm$ 0.2 & - \\ 
5  & 0.2 $\pm$ 0.4 & 0.0 $\pm$ 0.0 & 1.2 $\pm$ 0.4 & 1.6 $\pm$ 0.2 & - \\ 
\midrule
\multicolumn{6}{c}{GPT-3.5-Turbo-1106} \\ 
0  & 81.9 $\pm$ 0.6 & 77.2 $\pm$ 1.5 & 74.6 $\pm$ 1.8 & 70.3 $\pm$ 1.3 & - \\ 
1  & 9.7 $\pm$ 0.3 & 13.2 $\pm$ 0.4 & 15.1 $\pm$ 0.9 & 16.2 $\pm$ 0.2 & - \\ 
2  & 4.0 $\pm$ 0.4 & 5.3 $\pm$ 3.0 & 5.2 $\pm$ 0.7 & 6.6 $\pm$ 1.5 & - \\ 
3  & 1.2 $\pm$ 0.8 & 1.9 $\pm$ 1.1 & 2.6 $\pm$ 0.5 & 3.3 $\pm$ 0.1 & - \\ 
4  & 2.1 $\pm$ 0.0 & 1.4 $\pm$ 0.7 & 1.3 $\pm$ 0.4 & 2.0 $\pm$ 0.1 & - \\ 
5  & 1.0 $\pm$ 0.4 & 1.0 $\pm$ 1.1 & 1.3 $\pm$ 0.8 & 1.5 $\pm$ 0.2 & - \\
\midrule
\multicolumn{6}{c}{Qwen2.5-Coder-7B-Instruct} \\ 
0  & 79.3 $\pm$ 1.4 & 81.3 $\pm$ 0.4 & 60.9 $\pm$ 0.7 & 59.7 $\pm$ 1.2 & 67.2 $\pm$ 0.9 \\ 
1  & 14.9 $\pm$ 2.5 & 13.1 $\pm$ 2.0 & 22.4 $\pm$ 0.4 & 21.6 $\pm$ 0.7 & 14.9 $\pm$ 0.9 \\ 
2  & 2.6 $\pm$ 0.7 & 2.2 $\pm$ 0.8 & 7.5 $\pm$ 0.2 & 8.7 $\pm$ 0.4 & 7.1 $\pm$ 0.5 \\ 
3  & 1.5 $\pm$ 0.4 & 1.7 $\pm$ 1.0 & 4.5 $\pm$ 0.5 & 5.0 $\pm$ 0.3 & 4.7 $\pm$ 0.8 \\ 
4  & 0.6 $\pm$ 0.6 & 1.5 $\pm$ 0.7 & 2.9 $\pm$ 0.3 & 2.9 $\pm$ 0.3 & 3.4 $\pm$ 0.7 \\ 
5  & 1.1 $\pm$ 1.0 & 0.2 $\pm$ 0.4 & 1.8 $\pm$ 0.3 & 2.2 $\pm$ 0.5 & 2.7 $\pm$ 0.2 \\ 

\bottomrule
\end{tabular}
}
\label{table:debug}
\end{table}
%=============================================================================================

\newpage
\section{HumanEval Experiments}
\label{appendix: humaneval_exp}
Figure \ref{fig:attemp_dist} and Figure \ref{fig:humaneval_all_exp} provide a detailed analysis of debugging effectiveness across multiple attempts in the HumanEval benchmark. By tracking the resolution process of individual tasks, they highlight the impact of iterative debugging in improving accuracy.
\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{figures/he_exp_mean_std.png}
    \caption{Visualisation of Debugging Attempts. The left column represents the number of debugging attempts on the HumanEval benchmark. Each point represents a problem, with failed tasks highlighted in red. The right column displays the mean number of debugging attempts and the associated sample standard deviations across three experiments for each model. The shaded areas in the right column indicate the variability in performance, illustrating the consistency and reliability of each model.}
    \label{fig:attemp_dist}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{figures/he_all_exp.png}
    \caption{HumanEval experiment results using GPT-4-1106 across three repeats. Each line represents the progression of debugging attempts (up to 5) for tasks in the HumanEval dataset. Red markers denoting failed attempts. The coloured lines correspond to Experiment 1 (blue), Experiment 2 (green), and Experiment 3 (orange). The y-axis reflects the number of attempts taken to resolve each task.}
    \label{fig:humaneval_all_exp}
\end{figure}
%==================================================================================================

\newpage
\section{Modules (Detailed)}
\label{appendix: aux}

\subsection{Signature Converter}
The \textit{signature converter} addresses code generation instability due to the lack of structured function signatures, particularly in the MBPP dataset (discussed in Section ~\ref{experiments}).
Using the first test case, it infers the function name, input arguments, and constructs a robust function signature without revealing the expected output of the test case.
The generated output includes:
\begin{itemize}
    \item The inferred function name.
    \item A structured function signature (e.g., argument names and types).
    \item An example function call derived from the test case.
\end{itemize}
For instance, given a test case like \texttt{assert foo(4) == 16}, the module generates:
\begin{quote}
\textit{
    \#\#\# Required function name for your reference `foo()' \\
    \#\#\# Function signature for your reference - foo(arg\_int: int) \\
    \#\#\# An example function call from private test cases - foo(4)
}
\end{quote}
While return types are excluded from the function signature, they can be inferred from the test case's data type. This module enhances the clarity and consistency of code generation.

\subsection{Example Call Detection}
The \textit{example call detection} module identifies and removes instances where the \textit{programmer agent} includes sample function invocations within its generated code output. 
Despite explicit system prompts to avoid such calls, they frequently appear in outputs, as observed in 21 of 164 HumanEval problems and 44 of 344 MBPP problems in one of our experiments with GPT-3.5-Turbo-1106.
These embedded example calls pose a security risk as they can execute automatically during test case evaluation, potentially bypassing the runtime safety mechanisms we've implemented for controlled code execution.

This module scans the generated solution and removes any detected example calls before execution, ensuring system reliability. For example, calls like - \texttt{foo(4)} embedded in code are automatically removed. 
This safeguard prevents unintentional execution of potentially unsafe code outside the controlled environment.

\subsection{Error-Handling}
The \textit{error-handling} module refines error messages returned by the executor agent, making them concise and supplemented with natural language feedback. 
It mimics human debugging practices by streamlining error messages into relevant components. 
The module processes error messages in the following steps:
\begin{itemize}
    \item \textbf{Error Type Analysis:} First, the module identifies error types and provides contextual feedback. 
    For example, in the case of an \texttt{AssertionError}, the message might include: "Your generated solution failed a test case. Please improve the logic of your solution."
    \item \textbf{Relevance Filtering:} It shortens error messages to focus on errors within the \texttt{main.py} file, truncating irrelevant tracebacks referencing external files.
    \item \textbf{Critical error-handling:} For issues like \texttt{RecursionError}, the module truncates verbose error messages (e.g., maximum recursion depth exceeded) to ensure compatibility with the LLM’s context length. Without this, such errors could disrupt the LLM’s input pipeline.
\end{itemize}
This module improves the debugging process by filtering and contextualizing errors, ensuring that the Programmer Agent receives actionable and relevant feedback.
% ===========================================================================================

\newpage
\section{Pseudo-code of PyCapsule}
\label{appendix: pseudo}

\begin{algorithm}[!h]
\caption{Pseudo-code of PyCapsule}\label{algo2}
\begin{flushleft}
\textbf{Input:} A user-defined query $\mathcal{Q}$ for code generation with a problem description, test cases, and the maximum number of self-debugging attempts $N=5$.\\
\textbf{Output:} A generated code satisfying the request in the problem description if all test cases passed, otherwise a failed status message.\\
\textbf{Procedure:}\\
\end{flushleft}
\begin{algorithmic}[1]
\State Set and initialize a self-debugging attempt counter $i=0$.
\State Set an initial system prompt for generating code, a buffer $\mathcal{B}$ for storing conversation history, and a user prompt initialized by $\mathcal{Q}$.
\State Use $\mathcal{Q}$ and the LLM in PyCapsule to generate the function signature required by case testing.
\State \textbf{Step 1:} Generate code ("Generation Mode").
\State Generate a prompt by integrating the system prompt and user prompt.
\State Use the prompt and the same LLM to generate a response for code generation.
\State Extract the required code, that is the function implementation, from the response.
\State Create a Python container with the required packages installed. If it exists, skip this step.
\State Test all the cases in the container using the extract code, and return the code execute status.
\If{\textit{all the test cases passed}}
  \State Return the extracted code as the optimal generated code.
\ElsIf {$i > N$}
  \State Return the failed message extracted from the code execute status.
\Else
  \State \textbf{Step 2:} Code Debugging ("Fix Mode").
  \State Extract the error message from the code execute status and add it to $\mathcal{B}$.
  \State Generate an error-handling prompt using the error-handling module.
  \State Update the system prompt for code debugging.
  \State Update the user prompt by integrating the error-handling prompt and messages in $\mathcal{B}$.
  \State Update $n = n+1$.
  \State Repeat \textbf{Step 1}.
\EndIf
\end{algorithmic}
\end{algorithm}
%===============================================================================
\newpage
\section{System Prompts}
\label{appendix: prompt}
\subsection{Code Generation Mode}


\begin{lstlisting}[style=plaintext]
You are an experienced Python developer. Your task is to complete the function based on the provided function signature and description
    - Analyze the description and provide a step by step reasoning on how to solve the problem.
    - Maintain the function signature: Do not alter/modify the function signature.
    - Avoid example calls: Do not include any example call in your response.
    - If external libraries are needed, add a '### Requirements' Section listing them separated by ','. e.g. pandas, pyhton-dotenv ). If no external libraries are needed, add 'None'.
    - Import all required libraries and complete the function in a '### Code' Section, enclosed with triple backticks.
 
Example Response Structure-
 
### Step-by-step reasoning
$reasoning
 
### Requirements
$external_libraries
 
### Code
```
$code
``` 
\end{lstlisting}
\subsection{Fix Mode}
\begin{lstlisting}[style=plaintext]
You are an experienced Python developer. 
    - Your previous solution resulted an error.
    - Error message from a python compiler and Conversation history has been added for your reference.
    - Analyze the description and provide a step by step reasoning on how to solve the problem.
    - Maintain the function signature: Do not alter/modify the function signature.
    - Avoid example calls: Do not include any example call in your response.
    - If external libraries are needed, add a '### Requirements' Section listing them separated by ','. e.g. pandas, pyhton-dotenv ). If no external libraries are needed, add 'None'.
    - Import all required libraries and complete the function in a '### Code' Section, enclosed with triple backticks.

Example Response Structure- 

### Step-by-step reasoning
$reasoning

### Requirements
$external_libraries

### Code
```
$code
``` 
\end{lstlisting}
%===========================================================================
\end{document}
\endinput
%%
%% End of file `sample-acmsmall.tex'.
