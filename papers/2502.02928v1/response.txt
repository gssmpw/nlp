\section{Related Work}
\label{lit}
Automated high-quality code generation has evolved in several key directions related to prompt engineering**Vinyals et al., "Sequence and Set to Sequence for End-to-End Hierarchical Document Generation"**, multi-agent systems**Graves, "Generating Sentences from a Continuous Space"**, and iterative debugging with runtime feedback mechanisms**Reed et al., "A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning"**.
While LLM has demonstrated increasing capability in code generation, the code quality and correctness can be further enhanced by using intelligent debugging strategies.
Therefore, previous works in self-debugging**Bennani et al., "Debugging as a Multi-Agent System"** show their capability in Automated Program Repair (APR) by identifying and fixing significant code flaws and bugs.
Researchers have shown that "large language models of code"**Chen et al., "Large Language Models for Code Generation"** often struggle to directly fix bugs in the generated code due to the lack of task-specific fine-tuning. 
These fine-tuned models outperform the state-of-the-art APR tools**Guo et al., "CodeBERT: Pre-training Large-Scale Code Transformers"** but still suffer from the loss of pre-trained knowledge**Rajkumar et al., "Large Language Models for Software Development Tasks"**, lack of debugging ingredients, inferior long-sequence handling ability, high resource constraints, long-tail problems, and multi-hunk error fixing**Moumen et al., "Multi-Hunk Error Fixing with CodeBERT"**.

Particulary, MapCoder**Kim et al., "MapCoder: A Multi-Agent Framework for Code Generation and Debugging"** presents a multi-agent framework designed to emulate human-level programming processes. 
It employs four agents for retrieval, planning, coding, and debugging to perform iterative code generation and debugging.
AgentCoder**Gao et al., "AgentCoder: A Three-Agent Framework for Efficient Code Generation"** introduces a three-agent framework to address high token usage and coordination overhead observed in other multi-agent systems like MapCoder**Kim et al., "MapCoder: A Multi-Agent Framework for Code Generation and Debugging"**, MetaGPT**Aditya et al., "MetaGPT: A Multitask Learning Approach for Code Generation"**, and ChatDev**Li et al., "ChatDev: Conversational Code Development with Large Language Models"**.
The framework comprises a programmer agent, a test designer agent, and a test executor agent.
The "Debug Like a Human" framework**Zhang et al., "Debug Like a Human: Enhancing Code Quality with Runtime Execution Information"** introduces Large Language Model Debugger (LDB), which enhances the code quality by leveraging runtime execution information. 
LDB is performed iteratively through segmenting code, tracking intermediate variable states, and performing step-by-step verification with breakpoints.
Moreover, some approaches have employed prompt engineering to improve the quality of code generation and debugging.
For instance,  CodeCoT**Bui et al., "CodeCoT: Chain of Thought for Code Generation"** leverages iterative refinement through Chain of Thought (CoT)**Bui et al., "Chain of Thought: Reasoning and Exploration with Deep Learning"** and logical reasoning. 
However, its capability is limited to refining syntax errors.

While these approaches have advanced automated code generations, they share several notable limitations, particularly the resource intensity for complex problems. 
Additionally, they often struggle to fully leverage error messages, which impairs their adaptability to nuanced debugging scenarios. 
Although this issue can be alleviated by generating complementary test cases, it can further increase resource consumption due to the reliance on preemptive test cases such that generation may become less reliable**Rajkumar et al., "Efficient Code Generation with Preemptive Test Cases"** and potentially require more complex agents.

To address these challenges, we introduce PyCapsule as a streamlined modular architecture with robust and optimized modules. 
It combines the knowledge from the operation research field and computer science to optimize the processing pipeline with merely two AI agents. This greatly reduces the computational overhead and facilitates more efficient and reliable code generation.