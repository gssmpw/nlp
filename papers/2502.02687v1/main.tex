\documentclass[letterpaper, 10 pt, conference]{ieeeconf}
\IEEEoverridecommandlockouts
\overrideIEEEmargins

\usepackage{cite}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{url}

\newtheorem{theorem}{Theorem}
\newtheorem{remark}{Remark}

\setlength{\textfloatsep}{5pt plus 1.0pt minus 2.0pt}
\setlength{\abovecaptionskip}{3pt plus 1.0pt minus 2.0pt}

\setlength{\abovecaptionskip}{-3pt}
\setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{3pt}

\title{\LARGE \bf NDKF: A Neural-Enhanced Distributed Kalman Filter \\ for Nonlinear Multi-Sensor Estimation}
\author{Siavash Farzan%
\thanks{Siavash Farzan is with the Electrical Engineering Department, California Polytechnic State University, San Luis Obispo, CA 93407, USA, {\tt\small sfarzan@calpoly.edu}}%
}

\begin{document}

\maketitle
\thispagestyle{empty}
\pagestyle{empty}

\begin{abstract}
We propose a Neural-Enhanced Distributed Kalman Filter (NDKF) for multi-sensor state estimation in nonlinear systems. Unlike traditional Kalman filters that rely on explicit, linear models and centralized data fusion, the NDKF leverages neural networks to learn both the system dynamics and measurement functions directly from data. Each sensor node performs local prediction and update steps using these learned models and exchanges only compact summary information with its neighbors via a consensus-based fusion process, which reduces communication overhead and eliminates a single point of failure. Our theoretical convergence analysis establishes sufficient conditions under which the local linearizations of the neural models guarantee overall filter stability and provides a solid foundation for the proposed approach. Simulation studies on a 2D system with four partially observing nodes demonstrate that the NDKF significantly outperforms a distributed Extended Kalman Filter.
These outcomes, as yielded by the proposed NDKF method, highlight its potential to improve the scalability, robustness, and accuracy of distributed state estimation in complex nonlinear environments.
\end{abstract}

\section{Introduction}
\label{sec:introduction}

State estimation lies at the heart of a wide range of applications in robotics, sensor networks, and control systems. Classical Kalman filtering techniques remain the gold standard for linear-Gaussian problems; however, many real-world scenarios present nonlinear dynamics or complex measurement functions that challenge traditional approaches. Moreover, when multiple agents operate in a distributed fashion, exchanging all raw data can impose high communication overhead and hinder real-time operation.

Distributed Kalman filtering has long been explored to enable multi-sensor state estimation without relying on a centralized fusion node. Early work on consensus-based and federated filters~\cite{Olfati2005, Olfati2007, Carlson1994} allowed individual nodes to update local estimates and share compact summaries--such as means and covariances--with their neighbors. While effective for linear or mildly nonlinear systems, these methods require careful coordination to avoid overconfidence and inconsistent estimates~\cite{Carli2007}.
To handle nonlinearities and model uncertainties, extensions of the standard Kalman filter have been developed. Variants of the Extended Kalman Filter (EKF)~\cite{Jazwinski2007} approximate local Jacobians to manage nonlinear dynamics, while unscented Kalman filters (UKF)~\cite{Julier2004} employ deterministic sampling to capture higher-order moments without linearization. Both approaches have been adapted for distributed settings~\cite{Battistelli2016, Li2016}, though they depend on explicit system models that may not fully capture the complexities of real-world dynamics. Robust techniques such as covariance intersection methods~\cite{Julier2017,Hu2012} further address issues like outliers and unknown correlations, yet they still hinge on predefined models.\looseness=-1

In parallel, the integration of machine learning into filtering frameworks has led to neural-based Kalman filters~\cite{Krishnan2015, Fraccaro2016}. Rather than relying solely on hand-crafted models, these approaches train neural networks to learn state transitions or measurement mappings directly from data. Notably, KalmanNet~\cite{Revach2022} incorporates a recurrent neural network into the Kalman filter to learn the Kalman gain, which results in overcoming limitations associated with mismatches in noise statistics and model inaccuracies. However, most neural filtering approaches have been applied in centralized settings or under the assumption of linear models at individual nodes. Moreover, incorporating neural predictions into a distributed filtering framework introduces several challenges: ensuring stability and convergence despite potential biases or large local errors, managing partial or heterogeneous measurements across multiple distributed nodes, and balancing the computational cost of repeated neural network evaluations and Jacobian computations against real-time requirements.

Our work bridges these research streams by proposing a Neural-Enhanced Distributed Kalman Filter (NDKF). NDKF integrates neural network approximations of both system dynamics and measurement functions with a consensus-based distributed fusion mechanism. Unlike centralized methods such as KalmanNet, NDKF enables each sensor node to process local measurements and share only concise state summaries (e.g., local estimates and covariances), which reduces communication overhead and preserves data privacy. This framework addresses scalability, model uncertainty, and nonlinearities in a fully distributed manner where each node observes a partial, potentially nonlinear function of the system state, and provides a robust alternative to conventional EKF/UKF-based methods in multi-sensor applications.

The primary contributions of this work are:
\begin{list}{}{\leftmargin=0em \itemindent=5pt}
    \item[i.] A hybrid filtering framework that integrates neural network approximations of both system dynamics and measurement functions into the distributed Kalman filtering process to address complex nonlinear behaviors.
    \item[ii.] Integration of a consensus-based distributed fusion mechanism into the filtering process to enable scalable, decentralized state estimation, accompanied by a detailed computational complexity analysis.
    \item[iii.] A rigorous stability and convergence analysis for the NDKF under local linearization and Lipschitz conditions.
    \item[iv.] Validation of the proposed approach on a 2D multi-sensor scenario, demonstrating the efficacy of the NDKF compared to a conventional EKF baseline.
\end{list}

\section{NDKF Framework}
\label{sec:methodology}

In this section, we describe the proposed Neural-Enhanced Distributed Kalman Filter (NDKF) framework in detail.

\subsection{System Setup}
\label{sec:system-setup}

We consider a dynamical system with state vector $\mathbf{x}_k\in\mathbb{R}^n$ at discrete time $k$. The evolution of the state is modeled by:
\begin{equation}
    \mathbf{x}_{k+1} = f_{\theta}\bigl(\mathbf{x}_k\bigr) + \mathbf{w}_k,
    \label{eq:system-dynamics}
\end{equation}
where $f_{\theta}(\cdot)$ is a (potentially nonlinear) function approximated by a neural network with parameters $\theta$, and $\mathbf{w}_k$ is process noise, assumed to be zero-mean with covariance matrix $\mathbf{Q} \succeq \mathbf{0}$. 
We employ a network of $N$ sensor nodes, each collecting noisy measurements of the state. Node $i \in \{1,2,\dots,N\}$ obtains:
\begin{equation}
    \mathbf{y}_{k,i} = h_{\theta,i}\bigl(\mathbf{x}_k\bigr) + \mathbf{v}_{k,i},
    \label{eq:measurement-model}
\end{equation}
where $h_{\theta,i}(\cdot)$ is the local measurement function for node $i$, potentially also represented or assisted by a neural network, and $\mathbf{v}_{k,i}$ is measurement noise with covariance $\mathbf{R}_i\succeq\mathbf{0}$. Depending on the application, each $h_{\theta,i}(\cdot)$ may observe a subset of the state or a particular nonlinear transformation of $\mathbf{x}_k$.\looseness=-1

To enable efficient estimation, we assume nodes can exchange compact information (e.g., local estimates and covariance matrices) over a communication network, but we do not require that all raw measurements be sent to a central fusion center. The goal is to estimate $\mathbf{x}_k$ at each time step while preserving distributed operation and efficiently handling nonlinear dynamics through neural network approximations.\looseness=-1

\subsection{Neural Network for System and Measurement Modeling}
\label{sec:nn-modeling}

To handle the potentially nonlinear nature of the system dynamics \eqref{eq:system-dynamics} and measurement functions \eqref{eq:measurement-model}, we employ neural networks to approximate $f_{\theta}(\cdot)$ and $h_{\theta,i}(\cdot)$. These networks are parameterized by $\theta$, which can be learned from historical data or adapted online.

We represent the mapping $\mathbf{x}_k \mapsto \mathbf{x}_{k+1}$ through a neural network $f_{\theta}(\cdot)$. A typical architecture might include multiple fully connected layers with nonlinear activation functions, although any suitable architecture (e.g., convolutional or recurrent layers) can be used depending on domain requirements. The network parameters $\theta$ are determined by minimizing a loss function
\begin{equation}
    \mathcal{L}_{\mathrm{dyn}}\bigl(\theta\bigr) \;=\; \sum_{j=1}^{M}\,
    \bigl\|\mathbf{x}_{k+1}^{(j)} \;-\; f_{\theta}\bigl(\mathbf{x}_k^{(j)}\bigr)\bigr\|^2,
    \label{eq:dyn-loss}
\end{equation}
where $(\mathbf{x}_k^{(j)}, \mathbf{x}_{k+1}^{(j)})$ pairs are drawn from a training dataset of $M$ samples. The minimization of \eqref{eq:dyn-loss} ensures the network accurately models the underlying system transitions.

Each sensor node $i$ can optionally employ a neural network $h_{\theta,i}(\cdot)$ to capture the (possibly nonlinear) relationship between the state $\mathbf{x}_k$ and the measurements $\mathbf{y}_{k,i}$. Similar to the dynamics network, $h_{\theta,i}(\cdot)$ can be trained by minimizing
\begin{equation}
    \mathcal{L}_{\mathrm{meas},i}\bigl(\theta\bigr) \;=\; \sum_{j=1}^{M_i}\,
    \bigl\|\mathbf{y}_{k,i}^{(j)} \;-\; h_{\theta,i}\bigl(\mathbf{x}_k^{(j)}\bigr)\bigr\|^2,
    \label{eq:meas-loss}
\end{equation}
where $(\mathbf{x}_k^{(j)}, \mathbf{y}_{k,i}^{(j)})$ pairs are the measurement data specific to node $i$. When the measurement function is linear or partially known, $h_{\theta,i}(\cdot)$ can be reduced to a simpler parametric form or omitted in favor of a standard linear observation model.

In many applications, model training occurs \emph{offline} using a representative dataset, after which the parameters $\theta$ are fixed for the subsequent filtering process. Alternatively, in cases where the system evolves over time or new operating regimes appear, an \emph{online} training approach can be adopted. This may involve periodically updating $\theta$ using fresh data to refine the learned models $f_{\theta}$ and $h_{\theta,i}$ and maintain accuracy under changing conditions.

\subsection{Kalman Filter with Neural Network Integration}
\label{sec:kf-steps}

The Kalman filter is traditionally defined for linear dynamics and observation models. In this work, we preserve the filter’s two-stage \emph{prediction} and \emph{update} structure but explicitly replace the system and measurement models with neural-network-based functions $f_{\theta}$ and $h_{\theta,i}$. At each time step $k$, the filter uses these learned models for state propagation and local measurement updates, respectively.

\subsubsection{Prediction Step}
Let $\hat{\mathbf{x}}_{k|k}$ and $\mathbf{P}_{k|k}$ denote the estimated mean and covariance of the state at time $k$ (after processing the measurements up to $k$). The prediction step for time $k{+}1$ uses the learned dynamics function $f_{\theta}$ from \eqref{eq:system-dynamics}:\looseness=-1
\begin{equation}
    \hat{\mathbf{x}}_{k+1|k} \;=\; f_{\theta}\bigl(\hat{\mathbf{x}}_{k|k}\bigr),
    \label{eq:kf-prediction-state}
\end{equation}
\begin{equation}
    \mathbf{P}_{k+1|k} \;=\; \mathbf{F}_{\theta,k}\,\mathbf{P}_{k|k}\,\mathbf{F}_{\theta,k}^{T} \;+\; \mathbf{Q},
    \label{eq:kf-prediction-cov}
\end{equation}
where $\mathbf{F}_{\theta,k}$ is the Jacobian of $f_{\theta}(\cdot)$ evaluated at $\hat{\mathbf{x}}_{k|k}$. In practice, $\mathbf{F}_{\theta,k}$ can be obtained via automatic differentiation or finite differences. The matrix $\mathbf{Q}$ is the process noise covariance, which may be tuned empirically or even learned if sufficient data and model structures are available.

\subsubsection{Local Measurement Update}
Each node $i$ receives a measurement $\mathbf{y}_{k,i}$ according to \eqref{eq:measurement-model}, which relies on the learned observation function $h_{\theta,i}(\cdot)$. Defining $\mathbf{H}_{\theta,i,k}$ as the Jacobian of $h_{\theta,i}(\cdot)$ at the predicted state, the standard Kalman innovation form applies:
\begin{equation}
    \mathbf{S}_{k,i} \;=\; \mathbf{H}_{\theta,i,k}\,\mathbf{P}_{k|k-1}\,\mathbf{H}_{\theta,i,k}^{T} \;+\; \mathbf{R}_i,
    \label{eq:kf-innovation-cov}
\end{equation}
\begin{equation}
    \mathbf{K}_{k,i} \;=\; \mathbf{P}_{k|k-1}\,\mathbf{H}_{\theta,i,k}^{T}\,\mathbf{S}_{k,i}^{-1},
    \label{eq:kf-kalman-gain}
\end{equation}
\begin{equation}
    \hat{\mathbf{x}}_{k|k,i} \;=\; \hat{\mathbf{x}}_{k|k-1} \;+\; \mathbf{K}_{k,i}\,\Bigl[\mathbf{y}_{k,i} \;-\; h_{\theta,i}\bigl(\hat{\mathbf{x}}_{k|k-1}\bigr)\Bigr],
    \label{eq:kf-meas-update-state}
\end{equation}
\begin{equation}
    \mathbf{P}_{k|k,i} \;=\; \bigl(\mathbf{I} - \mathbf{K}_{k,i}\,\mathbf{H}_{\theta,i,k}\bigr)\,\mathbf{P}_{k|k-1}.
    \label{eq:kf-meas-update-cov}
\end{equation}
In \eqref{eq:kf-innovation-cov}--\eqref{eq:kf-meas-update-cov}, the subscript $(k|k-1)$ indicates the predicted quantities from \eqref{eq:kf-prediction-state}--\eqref{eq:kf-prediction-cov}, and $\mathbf{R}_i$ is the measurement noise covariance at node $i$. If $h_{\theta,i}(\cdot)$ is linear or partially known, $\mathbf{H}_{\theta,i,k}$ can be simplified accordingly.

\subsection{Distributed Fusion Mechanism}
\label{sec:distributed-fusion}

Once each node $i$ completes its local measurement update and obtains $\hat{\mathbf{x}}_{k|k,i}$ and $\mathbf{P}_{k|k,i}$, these partial estimates must be combined to form a globally consistent state estimate. Unlike standard distributed Kalman filters that assume explicit process and measurement models, our neural-based approach still relies on exchanging only concise summary information (e.g., posterior mean and covariance) to achieve consensus without centralizing raw data or neural parameters.

A straightforward method to merge local estimates is via their \emph{information form}. Each node $i$ transforms its posterior covariance $\mathbf{P}_{k|k,i}$ into the precision (inverse covariance) matrix, $\mathbf{P}_{k|k,i}^{-1}$, and computes the weighted contribution of its local state estimate $\hat{\mathbf{x}}_{k|k,i}$:
\begin{equation}
    \mathbf{W}_i \;=\; \mathbf{P}_{k|k,i}^{-1}, 
    \qquad
    \mathbf{z}_i \;=\; \mathbf{W}_i\,\hat{\mathbf{x}}_{k|k,i}.
    \label{eq:info-fusion-contrib}
\end{equation}
Each node then aggregates the information from its neighbors (or potentially all other nodes, depending on the network topology). Denoting the set of nodes that communicate with node $i$ at time $k$ by $\mathcal{N}_i$, the local fusion update may be written as:
\begin{equation}
    \mathbf{W}_{k|k,i}^{(\mathrm{fusion})} \;=\; \sum_{j \in \mathcal{N}_i} \mathbf{W}_j,
    \qquad
    \mathbf{z}_{k|k,i}^{(\mathrm{fusion})} \;=\; \sum_{j \in \mathcal{N}_i} \mathbf{z}_j.
    \label{eq:info-fusion-aggregation}
\end{equation}
The fused covariance is then:
\begin{equation}
    \mathbf{P}_{k|k,i}^{(\mathrm{fusion})} \;=\; \bigl(\mathbf{W}_{k|k,i}^{(\mathrm{fusion})}\bigr)^{-1},
    \label{eq:fusion-P}
\end{equation}
and the fused state estimate at node $i$ becomes:
\begin{equation}
    \hat{\mathbf{x}}_{k|k,i}^{(\mathrm{fusion})} \;=\; 
    \mathbf{P}_{k|k,i}^{(\mathrm{fusion})}\,\mathbf{z}_{k|k,i}^{(\mathrm{fusion})}.
    \label{eq:fusion-x}
\end{equation}
Although \eqref{eq:info-fusion-aggregation}--\eqref{eq:fusion-x} represent a single fusion step, repeated local averaging can be performed if network communication is limited or if nodes reside in different subgraphs. Such \emph{consensus} approaches allow the global estimate to converge asymptotically to a fully fused solution, even without a central controller.

By exchanging only $\hat{\mathbf{x}}_{k|k,i}$ and $\mathbf{P}_{k|k,i}$, the NDKF avoids sending large amounts of sensor data, which is particularly beneficial given the learned neural models at each node. This lowers communication overhead and protects measurement privacy. Additionally, local computation scales more favorably than a central architecture, since each node only processes its own and its neighbors’ estimates.

\subsection{Initialization and Parameter Tuning}
\label{sec:init-param-tuning}

A successful deployment of the proposed NDKF framework relies on careful initialization of the filter state and covariances, as well as proper selection of neural network and filter hyperparameters.
Each node $i$ requires an initial state estimate $\hat{\mathbf{x}}_{0|0,i}$ and covariance $\mathbf{P}_{0|0,i}$. In the simplest case where little prior information is available, $\hat{\mathbf{x}}_{0|0,i}$ can be set to a zero vector and $\mathbf{P}_{0|0,i}$ chosen as a large diagonal matrix to reflect high uncertainty. If partial domain knowledge exists (e.g., an approximate position or measurement from an external reference), the initialization can leverage that information for a more accurate starting point. For consistency across nodes, it may be beneficial to align all $\mathbf{P}_{0|0,i}$ to a common baseline, unless certain nodes are known to have superior initial estimates.

The architectures of $f_{\theta}(\cdot)$ and, when applicable, $h_{\theta,i}(\cdot)$ are defined by choices of layer width, depth, and activation functions. We fix these hyperparameters based on preliminary experiments that balance approximation accuracy and computational efficiency.

Process noise $\mathbf{Q}$ and measurement noise $\mathbf{R}_i$ often require tuning to match real-world conditions. In practice, these matrices can be set by empirical measurement of sensor noise characteristics or process variability, or trial-and-error based on the observed filter performance (e.g., tracking accuracy), or even data-driven estimates, where additional parameters in $f_{\theta}$ or $h_{\theta,i}$ learn noise levels adaptively.

If automatic differentiation is used to compute Jacobians $\mathbf{F}_{\theta,k}$ and $\mathbf{H}_{\theta,i,k}$, one must ensure the chosen framework (e.g., TensorFlow, PyTorch) supports gradients for all operations involved in $f_{\theta}$ and $h_{\theta,i}$. When gradients are unavailable or prohibitively expensive to compute, finite-difference approximations offer an alternative, but at a higher computational cost.
Ensuring consistency in initialization and careful hyperparameter tuning enables the proposed filter to converge more rapidly, adapt to changing conditions, and maintain reliable estimates across multiple sensor nodes without centralizing measurements.

\subsection{Scalability and Computational Complexity}
\label{sec:computational-complexity}

A key consideration in deploying the proposed NDKF framework across multiple sensor nodes is the per-iteration computational load at each node. We outline below the dominant factors that contribute to the overall complexity.

At each prediction step, the function $f_{\theta}(\cdot)$ (or $h_{\theta,i}(\cdot)$, if used in the measurement function) must be evaluated. If the network has $L$ layers, with $n_\mathrm{in}$ inputs, $n_\mathrm{out}$ outputs, and an average of $n_\mathrm{h}$ neurons per hidden layer, the forward pass typically requires on the order of
\begin{equation}
    O\bigl(n_\mathrm{in} \, n_\mathrm{h} \;+\; (L-2) \, n_\mathrm{h}^2 \;+\; n_\mathrm{h} \, n_\mathrm{out}\bigr)
\end{equation}
basic arithmetic operations. For high-dimensional states or more complex neural architectures, this term may dominate the per-step computation.

To integrate the neural network outputs into the Kalman filter, the Jacobians $\mathbf{F}_{\theta,k}$ and $\mathbf{H}_{\theta,i,k}$ are needed for the covariance prediction and measurement update, respectively. Depending on implementation, they can be obtained via either \emph{automatic differentiation} or \emph{finite differences}.
Automatic differentiation is applicable if a framework such as PyTorch or TensorFlow is used, and the computational overhead is roughly proportional to a second pass through the network’s layers in order to maintain and backpropagate gradients.
On the other hand, finite differences requires multiple forward passes of the neural network for each state dimension, typically $2n$ evaluations for an $n$-dimensional state, rendering it $O(n)$ times more expensive than a single forward pass.
The choice between these methods thus influences per-step computational costs and memory usage.

For each node $i$, the standard Kalman filter update in \eqref{eq:kf-meas-update-state}--\eqref{eq:kf-meas-update-cov} involves matrix multiplications and inversions whose complexity depends on the state dimension $n$ and the measurement dimension $m_i$. Generally, matrix inversion or factorization can be $O(n^3)$ if $\mathbf{P}_{k|k-1}$ is $n\times n$, though efficient numerical methods (e.g., Cholesky decomposition~\cite{Krishnamoorthy2013}) can reduce constants in practice.

Each fusion round (as in Section~\ref{sec:distributed-fusion}) involves inverting or combining $N$ local covariance matrices and summing information vectors. If each node $i$ handles contributions from its neighbors in $\mathcal{N}_i$, the total cost scales with $|\mathcal{N}_i|$. In the worst-case scenario of a fully connected network, each node processes contributions from $N-1$ other nodes. The matrix operations remain $O(n^3)$ per node (for $n$-dimensional state), plus the cost of exchanging local estimates.

Combining the above factors, the runtime at each node per iteration can be approximated as
\begin{equation}
    O\Bigl(\text{NN fwd/bkwd passes} + n^3 + |\mathcal{N}_i|\bigl(n^3 + \text{comm}\bigr)\Bigr),
\end{equation}
where `$\text{comm}$' denotes the cost of communicating local means and covariance matrices (or other fused messages). The exact constants depend on the neural architecture, dimension of the state, and the network connectivity. Nevertheless, since the NDKF operates without collecting raw measurements centrally, it remains more scalable than a fully centralized solution, especially for large $N$ or high-dimensional sensor data.

\section{Convergence and Stability Analysis}
\label{sec:convergence-stability}

This section provides a rigorous analysis of the stability of NDKF under local linearization and distributed fusion.

\subsection{Assumptions}
A few assumptions will be used in the analysis:
\begin{enumerate}
    \renewcommand{\labelenumi}{\roman{enumi}.}
    \item \textit{Smoothness and Lipschitz Conditions:} The neural network functions $f_\theta(\cdot)$ and $h_{\theta,i}(\cdot)$, for each node $i$, are continuously differentiable and locally Lipschitz in a neighborhood $\mathcal{X}$ of the true state trajectory. That is, there exist constants $L_f > 0$ and $L_{h_i} > 0$ such that for all $\mathbf{x},\mathbf{y}\in\mathcal{X}$,
    \begin{align}
        \| f_\theta(\mathbf{x}) - f_\theta(\mathbf{y}) \| &\leq L_f \|\mathbf{x}-\mathbf{y}\|, \\
        \| h_{\theta,i}(\mathbf{x}) - h_{\theta,i}(\mathbf{y}) \| &\leq L_{h_i} \|\mathbf{x}-\mathbf{y}\|.
    \end{align}
    
    \item \textit{Bounded Jacobians:} The Jacobians
    \[
    \mathbf{F}_{\theta,k} = \nabla f_\theta(\hat{\mathbf{x}}_{k|k}) \quad \text{and} \quad \mathbf{H}_{\theta,i,k} = \nabla h_{\theta,i}(\hat{\mathbf{x}}_{k|k})
    \]
    are uniformly bounded in norm by constants $M_f$ and $M_{h_i}$, respectively.
    
    \item \textit{Noise and Initial Error Bounds:} The process noise $\mathbf{w}_k$ and the measurement noise $\mathbf{v}_{k,i}$ are bounded in mean-square sense. Moreover, there exists a constant $\delta>0$ such that the initial estimation error satisfies
    \[
    \| \mathbf{e}_{0|0,i} \| < \delta, \quad \text{for all nodes } i.
    \]
\end{enumerate}

\subsection{Local Error Dynamics}
Define the estimation error at node $i$ at time $k$ as
\[
\mathbf{e}_{k|k,i} = \mathbf{x}_k - \hat{\mathbf{x}}_{k|k,i}.
\]
Under the local linearization, the prediction step yields an \emph{a priori} error dynamics approximated by
\[
\mathbf{e}_{k+1|k} \approx \mathbf{F}_{\theta,k}\,\mathbf{e}_{k|k,i} + \mathbf{w}_k,
\]
while the measurement update gives
\[
\mathbf{e}_{k|k,i} \approx \bigl(\mathbf{I} - \mathbf{K}_{k,i}\,\mathbf{H}_{\theta,i,k}\bigr)\,\mathbf{e}_{k|k,i} - \mathbf{K}_{k,i}\,\mathbf{v}_{k,i}.
\]
The distributed fusion step is assumed to combine local estimates (in the information form) without degrading these contraction properties.

\subsection{Sufficient Conditions for Stability}
The following theorem provides sufficient conditions for the local stability of NDKF.

\begin{theorem}[Stability of NDKF]
\label{thm:local-stability}
Under assumptions (i)--(iii), suppose that there exist constants $\alpha,\beta \in (0,1)$ such that for all time indices $k$ and for each node $i$,
\begin{equation}
    \| \mathbf{F}_{\theta,k} \| \leq \alpha,
    \quad \text{and} \quad 
    \| \mathbf{I} - \mathbf{K}_{k,i}\,\mathbf{H}_{\theta,i,k} \| \leq \beta. \label{eq:cond-beta}
\end{equation}
Then, there exists a neighborhood $\mathcal{E}$ around the true state such that if $\| \mathbf{e}_{0|0,i} \| < \delta$, the estimation error $\mathbf{e}_{k|k,i}$ converges exponentially fast (in mean-square sense) to a bounded region determined by the process and measurement noise covariances.
\end{theorem}

\begin{proof}
We first analyze the error propagation for a single node $i$ before considering the fusion step. For clarity, let $\mathbf{e}_{k|k,i} = \mathbf{x}_k - \hat{\mathbf{x}}_{k|k,i}$ denote the local estimation error at time $k$, and assume that the local linearization is exact in the sense that higher-order terms are negligible for $\mathbf{e}_{k|k,i}$ sufficiently small.

\noindent\textit{i. Prediction Step.}  
The true state evolves as
\begin{equation}
    \mathbf{x}_{k+1} = f_\theta(\mathbf{x}_k) + \mathbf{w}_k,
    \label{eq:true_evolution}
\end{equation}
while the predicted state is
\begin{equation}
    \hat{\mathbf{x}}_{k+1|k,i} = f_\theta(\hat{\mathbf{x}}_{k|k,i}).
    \label{eq:predicted_state}
\end{equation}
By the mean value theorem and (A.i), there exists a point $\xi_k$ on the line segment between $\mathbf{x}_k$ and $\hat{\mathbf{x}}_{k|k,i}$ such that
\begin{equation}
    f_\theta(\mathbf{x}_k) - f_\theta(\hat{\mathbf{x}}_{k|k,i}) = \mathbf{F}_\theta(\xi_k)(\mathbf{x}_k - \hat{\mathbf{x}}_{k|k,i}),
    \label{eq:mean_value}
\end{equation}
where $\mathbf{F}_\theta(\xi_k)$ is the Jacobian evaluated at $\xi_k$. Using (A.ii), we have
\begin{equation}
    \|\mathbf{F}_\theta(\xi_k)\| \leq M_f \leq \alpha < 1.
\end{equation}
Thus, the \emph{a priori} error becomes
\begin{equation}
    \mathbf{e}_{k+1|k,i} = f_\theta(\mathbf{x}_k) - f_\theta(\hat{\mathbf{x}}_{k|k,i}) + \mathbf{w}_k = \mathbf{F}_\theta(\xi_k) \mathbf{e}_{k|k,i} + \mathbf{w}_k,
\end{equation}
and taking the norm yields
\begin{equation}
    \|\mathbf{e}_{k+1|k,i}\| \leq \|\mathbf{F}_\theta(\xi_k)\|\, \|\mathbf{e}_{k|k,i}\| + \|\mathbf{w}_k\| \leq \alpha\, \|\mathbf{e}_{k|k,i}\| + \|\mathbf{w}_k\|.
    \label{eq:prediction_bound}
\end{equation}

\noindent\textit{ii. Measurement Update.}  
At the measurement update, the local filter uses the measurement
\begin{equation}
    \mathbf{y}_{k+1,i} = h_{\theta,i}(\mathbf{x}_{k+1}) + \mathbf{v}_{k+1,i},
    \label{eq:measurement_model_proof}
\end{equation}
and the updated state is computed as
\begin{equation}
    \hat{\mathbf{x}}_{k+1|k+1,i} = \hat{\mathbf{x}}_{k+1|k,i} + \mathbf{K}_{k+1,i} \Bigl( \mathbf{y}_{k+1,i} - h_{\theta,i}(\hat{\mathbf{x}}_{k+1|k,i}) \Bigr).
\end{equation}
Define the \emph{a posteriori} error as
\begin{equation}
    \mathbf{e}_{k+1|k+1,i} = \mathbf{x}_{k+1} - \hat{\mathbf{x}}_{k+1|k+1,i}.
\end{equation}
By subtracting the update equation from the true state \eqref{eq:true_evolution} and using a similar linearization for $h_{\theta,i}(\cdot)$ (with Jacobian $\mathbf{H}_{\theta,i,k+1}$ satisfying $\|\mathbf{H}_{\theta,i,k+1}\|\leq M_{h_i}$ by (A.ii)), we obtain
\begin{align}
    \mathbf{e}_{k+1|k+1,i} &= \mathbf{x}_{k+1} - \hat{\mathbf{x}}_{k+1|k,i} \nonumber \\
    &- \mathbf{K}_{k+1,i} \Bigl( h_{\theta,i}(\mathbf{x}_{k+1}) - h_{\theta,i}(\hat{\mathbf{x}}_{k+1|k,i}) + \mathbf{v}_{k+1,i}\Bigr) \nonumber \\
    &= \Bigl( \mathbf{I} - \mathbf{K}_{k+1,i}\mathbf{H}_{\theta,i,k+1} \Bigr) \mathbf{e}_{k+1|k,i} - \mathbf{K}_{k+1,i}\mathbf{v}_{k+1,i}.
    \label{eq:post_update_error}
\end{align}
Taking norms and applying the triangle inequality, we have
\begin{multline}
    \|\mathbf{e}_{k+1|k+1,i}\| \leq
    \|\mathbf{I} - \mathbf{K}_{k+1,i}\mathbf{H}_{\theta,i,k+1}\|\,\|\mathbf{e}_{k+1|k,i}\| \\ + \|\mathbf{K}_{k+1,i}\|\,\|\mathbf{v}_{k+1,i}\|.
    \label{eq:measurement_bound}
\end{multline}
By condition \eqref{eq:cond-beta}, 
\begin{equation}
    \|\mathbf{I} - \mathbf{K}_{k+1,i}\mathbf{H}_{\theta,i,k+1}\| \leq \beta < 1.
\end{equation}
Combining this with \eqref{eq:prediction_bound} gives
\begin{equation}
    \|\mathbf{e}_{k+1|k+1,i}\| \leq \beta \Bigl( \alpha\, \|\mathbf{e}_{k|k,i}\| + \|\mathbf{w}_k\| \Bigr) + \|\mathbf{K}_{k+1,i}\|\,\|\mathbf{v}_{k+1,i}\|.
    \label{eq:combined_bound}
\end{equation}
Define $\gamma = \alpha\beta$, which is less than 1 since both $\alpha$ and $\beta$ lie in $(0,1)$. Then,
\begin{equation}
    \|\mathbf{e}_{k+1|k+1,i}\| \leq \gamma\,\|\mathbf{e}_{k|k,i}\| + \beta\,\|\mathbf{w}_k\| + \|\mathbf{K}_{k+1,i}\|\,\|\mathbf{v}_{k+1,i}\|.
    \label{eq:recursive_error}
\end{equation}
\noindent\textit{iii. Induction and Contraction.}  
Define the noise term
\begin{equation}
    \nu_k = \beta\,\|\mathbf{w}_k\| + \|\mathbf{K}_{k+1,i}\|\,\|\mathbf{v}_{k+1,i}\|.
\end{equation}
Then, \eqref{eq:recursive_error} can be written as
\begin{equation}
    \|\mathbf{e}_{k+1|k+1,i}\| \leq \gamma\,\|\mathbf{e}_{k|k,i}\| + \nu_k.
    \label{eq:error_recursion}
\end{equation}
Iterating this inequality for $k{=}0$ to $N$, we obtain by induction
\begin{equation}
    \|\mathbf{e}_{N|N,i}\| \leq \gamma^N \|\mathbf{e}_{0|0,i}\| + \sum_{j=0}^{N-1} \gamma^{N-1-j} \nu_j.
    \label{eq:inductive_bound}
\end{equation}
Since $\gamma < 1$, the term $\gamma^N \|\mathbf{e}_{0|0,i}\|$ decays exponentially fast. Under (A.iii), the noise terms $\nu_j$ are bounded in mean-square; hence, the summation converges to a finite value as $N \to \infty$. This implies that $\|\mathbf{e}_{N|N,i}\|$ converges exponentially fast to a neighborhood of the origin whose size is determined by the noise bounds. \\
\noindent\textit{iv. Distributed Fusion Step.}  
Assume that after the local update, each node participates in a distributed fusion step where the fused estimate is computed as a convex combination (or via an information form aggregation) of the local estimates. Let $\mathcal{N}_i$ denote the set of nodes communicating with node $i$, and let the fused error be
\begin{equation}
    \mathbf{e}^{(\mathrm{fusion})}_{k|k,i} = \sum_{j \in \mathcal{N}_i} w_{ij}\, \mathbf{e}_{k|k,j},
\end{equation}
with weights $w_{ij}{\geq} 0$ and $\sum_{j\in \mathcal{N}_i}w_{ij}{=}1$. Then, by convexity,
\begin{equation}
    \|\mathbf{e}^{(\mathrm{fusion})}_{k|k,i}\| \leq \sum_{j\in \mathcal{N}_i} w_{ij}\, \|\mathbf{e}_{k|k,j}\| \leq \max_{j\in \mathcal{N}_i}\|\mathbf{e}_{k|k,j}\|.
    \label{eq:fusion_bound}
\end{equation}
Thus, the fusion step does not increase the maximum error among the nodes. As a result, the contraction established in \eqref{eq:inductive_bound} for each node is preserved under fusion.

Combining the above results, we conclude that, for each node $i$, there exist constants $C, C' > 0$ such that
\begin{equation}
    \mathbb{E}\Bigl[\|\mathbf{e}_{k|k,i}\|^2\Bigr] \leq C\,\gamma^{2k} \|\mathbf{e}_{0|0,i}\|^2 + \frac{C'}{1-\gamma^2}.
\end{equation}
This inequality shows that the estimation error converges exponentially fast (in mean-square) to a bounded region determined by the noise covariances, establishing the local stability of the NDKF under the stated conditions.
\end{proof}

\section{Results and Discussion}
\label{sec:results}

In this section, we present empirical evaluations of the proposed Neural-Enhanced Distributed Kalman Filter (NDKF) on a 2D system monitored by four distributed sensor nodes, and compare its performance to an Extended Kalman Filter (EKF) baseline.

\vspace{-5pt}
\subsection{Experimental Setup}
\label{sec:exp-setup}

\noindent\textit{System Simulation:}  
We consider a 2D state vector 
\[
\mathbf{x}_k = [p_x,\, p_y]^{T},
\]
which evolves according to
\begin{equation}
    \mathbf{x}_{k+1} \;=\; \mathbf{x}_k + 
    \begin{bmatrix}
        0.05 \cos\!\bigl(k/10\bigr) \\[4pt]
        0.05 \sin\!\bigl(k/10\bigr)
    \end{bmatrix}
    + \boldsymbol{\omega}_k,
\end{equation}
where $\boldsymbol{\omega}_k \sim \mathcal{N}\bigl(\mathbf{0},\,\mathrm{diag}(0.001,0.001)\bigr)$. Here, $\mathcal{N}(\cdot,\cdot)$ denotes the multivariate normal (Gaussian) distribution with the specified mean and covariance matrix. Offline training data are generated over 400 time steps, and an independent set of 100 time steps is used for testing and filter evaluation.

\noindent\textit{Measurement Models:}  
Each node obtains a single 1D measurement of the 2D state, implying partial observability at the individual node level:
\begin{itemize}
    \item[] {Node 1:} $z_{k,1} = \sin(2p_x) + 0.5\,p_y + \nu_{k,1}$,
    \item[] {Node 2:} $z_{k,2} = \cos(2p_y) - 0.4\,p_x + \nu_{k,2}$,
    \item[] {Node 3:} $z_{k,3} = \sin(2p_x) + \cos(2p_y) + \nu_{k,3}$,
    \item[] {Node 4:} $z_{k,4} = \sin(2p_x) - \cos(2p_y) + \nu_{k,4}$.
\end{itemize}
Here, $\nu_{k,i} \sim \mathcal{N}(0,0.01)$. In the NDKF implementation, each node’s measurement function $h_{\theta,i}(\cdot)$ is learned from data using a dedicated neural network.
The EKF baseline employs mis‑specified analytical measurement functions for Node 1 and 2 (i.e., ignoring the additional scaling and bias terms to assess performance under model mismatch), while using the correct functions for Node 3 and 4.

\noindent\textit{Neural Network Configuration:}  
The dynamics residual network in the NDKF is designed to learn the correction to the nominal dynamics. Its input is the state augmented with a time parameter, and it is implemented as a deep fully connected network with three hidden layers (128 neurons each), batch normalization, and dropout (0.2). It is trained for 3000 epochs using Adam with an initial learning rate of 0.001 and a learning rate scheduler. The measurement networks (one per sensor) are implemented with two hidden layers (32 neurons each) and are trained for 1000 epochs.

\noindent\textit{Filter Initialization:}  
All nodes are initialized with 
\[
\hat{\mathbf{x}}_{0|0,i} = \begin{bmatrix}0, \, 0\end{bmatrix}^{T} \quad \text{and} \quad \mathbf{P}_{0|0,i} = 0.5\,\mathbf{I}_2.
\]
The process noise covariance is set to $\mathbf{Q}=\mathrm{diag}(0.001,0.001)$ and the measurement noise covariance to $\mathbf{R}_i=0.01$ for all nodes.

\vspace{-5pt}
\subsection{Experimental Evaluation}

We evaluate filtering performance using the \emph{root mean squared error} (RMSE):
\vspace{-4pt}
\begin{equation}
    \mathrm{RMSE}(k) \;=\; \Big(\frac{1}{N_t}\sum_{t=1}^{N_t} \|\mathbf{x}_t - \hat{\mathbf{x}}_{t|t}\|^2\Big)^{\frac{1}{2}},
\end{equation}
computed at each time step $k$ over $N_t$ Monte Carlo runs. Additionally, we analyze the measurement innovation residuals to assess the quality of the update step.
Table~\ref{tab:rmse-comparison} summarizes the RMSE for estimating $p_x$ and $p_y$ averaged over 40 runs.
These results indicate that the proposed NDKF achieves approximately a 70\% reduction in $p_x$ error and a 41\% reduction in $p_y$ error compared to the EKF baseline.

\begin{table}[!ht]
\centering
\caption{RMSE Comparison for $p_x$ and $p_y$ (40 Monte Carlo Runs)}
\label{tab:rmse-comparison}
\begin{tabular}{lcc}
\hline
\textbf{Method} & \textbf{RMSE in $p_x$} & \textbf{RMSE in $p_y$} \\
\hline
NDKF (proposed) & 0.1209 & 0.2642 \\
Distributed EKF (baseline) & 0.3964 & 0.4463 \\
\hline
\end{tabular}
\vspace{-5pt}
\end{table}

Figure~\ref{fig:trajectory} shows a representative 2D trajectory comparing the true state, the local node estimates, and the fused state estimate obtained using the NDKF. The NDKF’s fused estimate closely tracks the true state, while the EKF baseline deviates considerably, as indicated by the numerical RMSE in Table~\ref{tab:rmse-comparison}, due to its mis‑specified measurement models. Figure~\ref{fig:innovation} illustrates the measurement innovation residuals for Node 1. The low and stable residuals obtained with the NDKF, along with the numerical results reported in Table~\ref{tab:rmse-comparison}, confirm its superior measurement update performance compared to the EKF baseline.

\begin{figure}[!tb]
    \centering
    \includegraphics[trim={20pt 30pt 22pt 65pt},clip,width=\columnwidth]{plots/trajectory_plot.eps}
    \caption{True state, individual node estimates, and the fused NDKF estimate.}
    \label{fig:trajectory}
    \vspace{-10pt}
\end{figure}
\begin{figure}[!tb]
    \centering
    \includegraphics[trim={20pt 0pt 20pt 32pt},clip,width=\columnwidth]{plots/innovation_residuals_plot.eps}
    \caption{Measurement innovation residuals for Node 1.}
    \label{fig:innovation}
\end{figure}

The experimental results show that in scenarios with significant nonlinearities and model mismatch, the proposed NDKF outperforms the distributed EKF baseline. By learning both the dynamics residual and the true measurement functions, the NDKF achieves much lower RMSE values (0.1209 for $p_x$ and 0.2642 for $p_y$) compared to the EKF (0.3964 and 0.4463, respectively), even though the EKF uses the correct measurement models for Nodes 3 and 4. While the EKF may perform well under near‑linear conditions, its reliance on fixed, mis‑specified analytical models leads to significant errors when facing true nonlinear behavior.
 
An implementation of NDKF, including the example provided in this section, is available in the GitHub repository accompanying this paper at: \\{\small\url{https://github.com/sfarzan/NDKF}}

\section{Conclusion and Future Directions}\label{sec:conclusion}

We introduced the Neural-Enhanced Distributed Kalman Filter (NDKF), a data-driven framework that integrates learned neural network models within a consensus-based, distributed state estimation scheme. By replacing traditional process and measurement equations with learned functions $f_{\theta}(\cdot)$ and $h_{\theta,i}(\cdot)$, NDKF is able to accommodate complex or poorly characterized dynamics while remaining robust in the presence of partial and heterogeneous observations.
Our theoretical analysis identified key factors influencing filter convergence, including Lipschitz continuity of the neural approximations, proper noise covariance tuning, and adequate network connectivity for effective distributed fusion, along with a detailed computational complexity analysis. Experimental evaluations on a 2D system with multiple sensor nodes demonstrated the NDKF’s capability to reduce estimation error compared to a baseline distributed EKF, particularly under challenging nonlinear motion patterns.

Despite promising performance, several challenges remain. Future work may explore more advanced training and adaptive learning methods for nonstationary environments, alternative fusion strategies to improve scalability and fault tolerance, and extensions to the theoretical analysis--including robust estimation techniques to address adversarial noise distributions--to broaden the NDKF's applicability.

\bibliographystyle{IEEEtran}
\bibliography{bibtex}

\end{document}
