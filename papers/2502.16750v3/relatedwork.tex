\section{Related Work}
Over the past year, large language models (LLMs) have matured into intelligent AI agents that reason, engage, and execute tasks autonomously. These are the agents, or more often the agents trained on the LLMs, which have so much promise to reshape industries and everyday life. Their power comes at the cost of substantial challenges around in context manipulation, deceptive behaviors and adversarial attacks. In this related work section, these critical issues will be examined in the academic literature, as well as the relationship between these issues, LLMs, agentic frameworks and the environment of emergent vulnerabilities. We will also explore numerous solutions attempted to alleviate these described issues. Figure \ref{fig:Research_Network}
shows the interconnection of research papers discussed in the related work section. 


\begin{figure}[ht]
\centering
\includegraphics[bb=0 0 500 500, width=0.4\linewidth]{Results/Lit_Guardian.png}
\caption{This figure represents the research papers discussed in the related work section.}
\label{fig:Research_Network}
\end{figure}

 
\subsection{Language Model Agents and Their Emergent Abilities: The Genesis}

Large language model is the foundation upon which modern AI agents are built, and they're a descendant of models like the transformer \cite{Vaswani2017Attention}, and more recent developments that enable large language models to show in-context learning, something which first manifested in models such as GPT-3 \cite{Brown2020Language}. Agentic behaviour relies on in-context learning, where we can quickly adapt to new instructions, without explicit fine-tuning. This capability, along with techniques such as chain of thought prompting \cite{Wei2022Chainofthought}, has given these models the ability to break down complex tasks into more manageable steps and perform a manner of reasoning unlike other previous models. This is what has enabled modern agentic frameworks like Auto-GPT, Langchain, and BabyAGI to get where they are today, since models are able to decompose a task as a series of steps leading to an intended goal\cite{barua2024exploring}. Typically, these frameworks employ a set of tools, e.g. search engines, or code interpreters, that allow the agent to perform more sophisticated tasks than the language model alone can. We have also studied scaling laws \cite{Kaplan2020Scaling} that demonstrate improved performance with larger model sizes as well as larger dataset volumes, which may also be attributed to the power of LLMs. The agentic system has been getting more and more capable, doing tasks ranging from data analysis\cite{barua2023kaxai} to Scientific tools creation\cite{barua2024pygen}. Though designing them imposes a hefty computational burden to the scientific community, but research related to how to design them more optimally is making the cost of designing AI systems within feasible range\cite{barua2024elmagic}.  These achievements together have not only made the creation of intelligent agents possible but at the same time brought on new things which need to be addressed. And this is especially true when you consider the increased autonomy being given to LLM based agents.



\subsection{Jailbreaks: LM Agents Undermine Safety Protocols}

Another very prominent area of concern regarding LLMs is the jailbreak attacks susceptibility for LLM based agents. They aim to attack these models' safety mechanisms in particular, so that they now generate dangerous or unethical content, when these models were originally trained to avoid doing so. They take advantage of either the model's poor contextual understanding or its unparsability of different or malicious commands, e.g. \emph{prompt injection attacks} \cite{perez2022ignore}. The forms of these prompt injections are diverse, following from a number of papers about how to generate these prompts, and how to circumvent the model's safety parameters \cite{Zou2023Jailbreaking}. These safety mechanisms are still under investigation but it has been demonstrated that these safety protocols are easily circumvented using complex prompts and attacks \cite{Carlini2019Adversarial}. Simple, but strong attack many-shot Jailbreaking (MSJ)\cite{Anil2024manyshot} exploits large context windows in large language models (LLMs) by prompting hundreds of examples of undesirable behaviour. Our findings empirically demonstrate the vulnerabilities of LLMs to persistent in-context manipulation, and find that the effectiveness of MSJ follows a predictable power law, growing with the number of demonstrations and model size. The initial susceptibility to MSJ in standard safety alignment techniques including supervised and reinforcement learning are reduced, but not eliminated as context length increases. And these sorts of security vulnerabilities are not LLM specific, as the same sorts of vulnerabilities have been demonstrated in other machine learning systems. Additionally, this is even more dangerous in agentic frameworks, where one jailbreak can trigger a series of damaging actions from each agent, especially so, since these agents have access to a wide variety of tools. In these agents as the agents become more autonomous and more complex, the ability to mitigate these jailbreak vulnerabilities is essential. This makes the need for robust solutions more urgent, because we know that even ‘safe’ models can be jailbroken.

\subsection{Mitigation Strategies within Agentic Frameworks}

Mitigation strategies for in-context scheming, alignment faking, and jailbreaks require a robust set of disclosures in the design of agentic frameworks. Methods for addressing these issues exist and have been shown to be insufficient, particularly when applied in the context of complex agentic frameworks. First is to use adversarial training methods where model is trained with adversarial examples~\cite{Goodfellow2014Explaining}. However, this method can be used to enhance the model robustness as long as it can generate the robust adversarial examples, but this is easier said than done, and the model could be overfitted when this approach is taken. A second approach is to develop safety guardrails during inference time, such as through prompt engineering and reinforcement learning from human feedback. However, it is demonstrated that those protections are not enough, and advanced jailbreaking techniques \cite {Ouyang2022Training, Ziegler2019Fine} can bypass these safeguards. Later, as the strategies get more advanced, these involve interpretable AI \cite{Ribeiro2016Why} by trying to be more transparent when it comes to making decisions with AI. While there are some inherent problems to this area, so often the decision making process is so convoluted that it's very hard to understand why it takes the decisions it does. Finally, these systems can also be improved for safety through use of sandbox environments for agent execution and robust monitoring systems. Additionally, constitutional AI \cite{Bai2022Constitutional} is used to ensure that the behaviors of models are guided to safer directions via AI feedback. But as with anything, it’s still an area of active research and nobody knows whether or not these techniques will hold. As the field matures, I believe it will likely take the form of integrating several different strategies in order to build safe and reliable means to construct LLM based agentic frameworks.


Furthermore, in-context scheming, alignment faking, and jailbreak vulnerabilities should not be construed as unique issues, and these are too deeply interrelated, with elements of the same root in the training and the deployment of large language models in agentic environments. However, when the in-context learning capability is introduced to give the agents (that have been provided with versatility), the possibility of exploitation or being manipulated can occur (to the agent that cannot rely on the same list of behavioral patterns). As discussed previously, the same prompt injection techniques that allow jailbreaking a language model allow for jailbreaking an agent in similar fashion to coerce it into doing harmful things. Systematic safety of LLMs, as shown in the work of \cite{Hendrycks2023Systematic}, depends on a holistic system approach integrating all available countermeasure schemes. However, as these agents become more sophisticated, they are able to alter the information in a manner that works for themselves, rather than the user \cite{Zhang2024Unpacking}. More importantly, this interconnectedness brings to the forefront the fact that no single vulnerability can be remediated in isolation to a single artifact, rather this remediation is best accomplished by focusing on the architecture and design of agentic frameworks at large. This has to be brought in with good technical solutions, but having a very careful thought of ethical and societal implications. Therefore, more research is needed to come up with methods for detecting deception, malicious planning in agentic frameworks, and better strategies for obtaining safety and trustworthiness.