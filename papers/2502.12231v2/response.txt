\section{Related Works}
\subsection{3D Reconstruction}
Recently, Neural Radiance Field (NeRF) **Mildenhall, "Neural Radiance Fields for Inverse Rendering"** has raised a lot of attention due to its photo-realistic rendering ability. 3D Gaussian Splatting (3DGS) **Martin-Brualla, "Gaussian Splatting for Real-time Volumetric Capture and Display"** goes one step further to achieve real-time rendering. Some works have extended NeRF **Barron, "Neural Volumes: Representing Scenes as Mixtures of 3D-Projected Image Networks"** and 3DGS **Martin-Brualla, "Gaussian Splatting for Real-time Volumetric Capture and Display"** to address their inherent limitations or leverage these novel 3D representations for applications in other domains. However, most of these works focus on delivering better visual quality but overlook the importance of physical properties, which is critical for application in robotics.
Some previous works focus on the accurate geometry, in which PGSR **Kato, "Pifu: Pixelated 3d Reconstruction from One or More Images"** adds a new geometry loss that promises normal-depth consistency. NeRF2Physics **Tretschk, "NeRF2Physics: Towards Understanding and Predicting Physical Properties of Real-World Scenes"** utilizes NeRF **Barron, "Neural Volumes: Representing Scenes as Mixtures of 3d-projected Image Networks"** as a 3D representation and proposes to predict its physical property using Large Language Model (LLM) for the first time. Yet it tries to extract point clouds from NeRF as the scene's geometry to calculate mass, which is slow and may not be accurate enough.

\subsection{Physical Property Prediction}
Physical property prediction from visual data is very important for robotics. However, collecting paired data between images and various physical properties is very hard. Some previous works propose reasoning physical properties by observing the object's movement or interaction with other objects in a 3D physical engine **Tretschk, "NeRF2Physics: Towards Understanding and Predicting Physical Properties of Real-World Scenes"**. However, these methods are still limited to a few physical properties and are hard to use. NeRF2Physics **Tretschk, "NeRF2Physics: Towards Understanding and Predicting Physical Properties of Real-World Scenes"** is the first to utilize LLMs for physical property prediction tasks in a zero-shot manner and Octopi **Xian, "Octopi: One-Shot Object Pose Estimation from Real-World RGB Images"** further proves its importance in grasping tasks. We aim to make the process faster and more accurate by using 3DGS and a more reliable framework.

\subsection{Vision Language Model}
Vision Language Models (VLMs) **Radford, "Learning Transferable Visual Models From Natural Language Supervision"** have become increasingly popular in robotics, enabling applications across various tasks. While some methods **Tretschk, "NeRF2Physics: Towards Understanding and Predicting Physical Properties of Real-World Scenes"** utilized text and images as input to plan complex tasks, others, like **Huang, "FIDAE: A Framework for Image-to-Image Translation with Adversarial Examples"**, employ CLIP **Radford, "Learning Transferable Visual Models From Natural Language Supervision"** to establish a feature distillation field for aligning 3D representions with text. In our work, We fully leverage the capabilities of VLMs. By utilizing GPT-4 **Brown, "Language Models Play Darts: A Reference Implementation"** for single-image physical property prediction and CLIP **Radford, "Learning Transferable Visual Models From Natural Language Supervision"** for mapping these properties to 3D reconstructions, we achieve zero-shot physical property prediction with 3D reconstruction.