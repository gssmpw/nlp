\section{Related Work}
Object hallucination refers to cases in which the generated textual descriptions include objects that do not correspond to the given image____.
LVLMs generally consist of three components: a vision encoder, an LLM, and an adapter____.
The structural characteristics of LVLMs contribute to object hallucination, which arises from multiple intertwined factors____.
While some studies argue that hallucinations can be mitigated by enhancing the decoding process of the LLM____, others suggest that the root cause lies in the limited representational capacity of the vision encoder____.
Additionally, some research indicates that training the adapter with contrastive data is essential to reduce object hallucination____.
And the trained bias of the model has also been identified as a cause of hallucination____.

CLIPScore____ is a reference-free evaluation method that assesses the consistency between an image and text caption by computing the cosine similarity between the embeddings generated by the vision encoder and text encoder of the CLIP model. Beyond its application in measuring caption quality, several studies have also leveraged CLIPScore for data curation in the training of Vision-Language Models (VLMs)____.

A recent study____ utilized CLIPScore to evaluate object hallucination in Vision-Language Models (VLMs). Their findings suggest that this phenomenon stems from the limited capacity of the vision encoder.
In this study, we carefully reassess this claim and demonstrate that object hallucination is not necessarily caused by the limitations of the vision encoder alone.