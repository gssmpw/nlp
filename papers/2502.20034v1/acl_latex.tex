\pdfoutput=1

\documentclass[11pt]{article}
\usepackage{acl}
% \usepackage[review]{acl}
\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{graphicx}
\usepackage{amsmath}

\usepackage{multirow}
\usepackage{xcolor}
\usepackage{booktabs}

\title{Vision-Encoders (Already) Know What They See: \\Mitigating Object Hallucination via Simple Fine-Grained CLIPScore}
\author{
 \textbf{Hongseok Oh}
  \quad \textbf{Wonseok Hwang}
\\ University of Seoul
\\ \texttt{\{cxv0519, wonseok.hwang\}@uos.ac.kr}
}

\begin{document}
\maketitle
\begin{abstract}
Recently, Large Vision-Language Models (LVLMs) show remarkable performance across various domains. However, these models suffer from object hallucination.
This study revisits the previous claim that the primary cause of such hallucination lies in the limited representational capacity of the vision encoder. 
Our analysis reveals that the capacity of the vision encoder itself is already enough for detecting object hallucination.
Based on this insight, we propose a Fine-grained CLIPScore (F-CLIPScore), a simple yet effective evaluation metric that enhances object-level granularity by incorporating text embeddings at the noun phrase level.
Evaluations on the OHD-Caps benchmark show that F-CLIPScore significantly outperforms conventional CLIPScore in accuracy by a large margin of \textbf{39.6\%} without additional training.
We further validate F-CLIPScore by showing that LVLM trained with the data filtered using F-CLIPScore exhibits reduced hallucination. 

\end{abstract}

\section{Introduction} \label{sec:intro}

\begin{figure}[th]
    \centering
    \includegraphics[width=0.85\linewidth]{images/ohd_sample.png}
    \begin{minipage}{0.4\textwidth}
        \small
        \textbf{CLIPScore} (\textbf{w/o training}): \texttt{A lady and two children in the street playing with a tennis racquet, a car nearby, and a chair.} 
        \\\\\textbf{CLIPScore} (\textbf{trained}): \texttt{A lady and two dogs in the park playing with a frisbee.}
       \\\\\textbf{F-CLIPScore} (\textbf{w/o training}): \texttt{A lady and two children in the street playing with a tennis racquet.}
    \end{minipage}
    \caption{A representative example from the OHD-Caps test set.
    The original CLIP selects sentences containing both "children" and "tennis", but add hallucinated objects while the OHD-Caps-trained CLIP does not.
    However, the latter exhibits hallucinations involving "dog" and "frisbee".
    On the other hand, F-CLIPScore selects the sentence that did not add or alter hallucinated objects.}
    \label{fig:ohd_sample}
    \vspace{-0.6cm}
\end{figure}

Recent studies highlight Large Vision-Language Models (LVLMs) as a leading method for integrating vision and language~\cite{LLaVA, qwen2vl, minigpt4, internvl}.
However, similar to the hallucination observed in large language models (LLMs) for textual modalities~\cite{llm_hallucination, sirenssong}, LVLMs also suffer from object hallucination where the model describes nonexistent objects or misidentifies objects in an image, which significantly affects the reliability of LVLM-based applications ~\cite{eval_obj_hallucination_lvlm, lvlm_hallucination_survey}.

Recently, to investigate the main cause of the object hallucination, \citet{ohd-caps} built OHD-Caps, a dataset designed to measure object hallucination. The dataset comprises (image, captions) pairs where a model needs to select the best caption that does not show hallucinations. They found the selection based on CLIPScore shows the accuracy 10--20\% and the further fine-tuning with the proposed objective function with the dataset enhance the accuracy up to 80--90\%. 

However, when they connected the OHD-Caps-trained CLIP~\cite{clip} to an LVLM and conducted full fine-tuning, the resulting accuracy sometimes drops showing lower performance compared to original CLIP (for instance, 1st row of Table 4 in \citep{ohd-caps} shows the accuracy in POPE benchmarks~\citep{pope} drops from 85.4\% to 81.2\%).
Additionally, we observe that the OHD-Caps-trained CLIP tended to select samples with hallucinations involving changes to the object itself, as shown in Figure~\ref{fig:ohd_sample}.
In contrast, the original CLIP tended to select text containing hallucinations regarding the presence of additional objects.
These observations may indicate that there is room for further investigation into whether the vision encoder’s capacity is indeed the primary factor contributing to object hallucination.

To address this issue, we introduce Fine-grained CLIPScore (F-CLIPScore), a novel image-text correlation metric.
F-CLIPScore leverages a simple sentence parser like spaCy~\cite{spacy} and the forward pass of a Vision-Language Model (VLM) like CLIP~\cite{clip}, offering an efficient way to evaluate the vision encoder’s representational capacity.
Our experimental results show that applying F-CLIPScore to the OHD-Caps test set improves accuracy by \textbf{+39.6\%}.
This suggests that F-CLIPScore effectively detect object hallucination in VLMs without requiring additional training of the vision encoder, indicating that the limited capacity of the vision encoder may not be the primary cause of object hallucination.
Additionally, we verify that using F-CLIPScore for pretraining data curation in LVLMs enables the training of models with reduced hallucination, even with significantly fewer data samples.
Notably, in LVLM pretraining, data curation alone improved POPE accuracy by 4.9\% compared to the baseline.

The key contributions of this study are as follows:
\begin{itemize}
    \item We introduce Fine-grained CLIPScore, a novel evaluation metric that relies solely on forward propagation. \item Reassessing the assumption that object hallucination stems primarily from the vision encoder’s limited capacity. \item  Demonstrating that F-CLIPScore enables more efficient LVLM training with reduced object hallucination through pretraining data curation.
\end{itemize}

Our code is available at \url{https://github.com/abzb1/f-clip}.

\section{Related Work}

Object hallucination refers to cases in which the generated textual descriptions include objects that do not correspond to the given image~\cite{lvlm_hallucination_survey}.
LVLMs generally consist of three components: a vision encoder, an LLM, and an adapter~\cite{LLaVA}.
The structural characteristics of LVLMs contribute to object hallucination, which arises from multiple intertwined factors~\cite{lvlm_hallucination_survey}.
While some studies argue that hallucinations can be mitigated by enhancing the decoding process of the LLM~\cite{lcd_lvlm, icd_lvlm}, others suggest that the root cause lies in the limited representational capacity of the vision encoder~\cite{ohd-caps}.
Additionally, some research indicates that training the adapter with contrastive data is essential to reduce object hallucination~\cite{hacl}.
And the trained bias of the model has also been identified as a cause of hallucination~\cite{ciem, robust_instruction_tuning}.

CLIPScore~\cite{clipscore} is a reference-free evaluation method that assesses the consistency between an image and text caption by computing the cosine similarity between the embeddings generated by the vision encoder and text encoder of the CLIP model. Beyond its application in measuring caption quality, several studies have also leveraged CLIPScore for data curation in the training of Vision-Language Models (VLMs)~\cite{laion, datacomp}.

A recent study~\cite{ohd-caps} utilized CLIPScore to evaluate object hallucination in Vision-Language Models (VLMs). Their findings suggest that this phenomenon stems from the limited capacity of the vision encoder.
In this study, we carefully reassess this claim and demonstrate that object hallucination is not necessarily caused by the limitations of the vision encoder alone.

\section{Methods}
\subsection{Motivation}

\begin{figure}[th!]
    \centering
    \includegraphics[width=0.9\linewidth]{images/3plot.png}
    
    \caption{
    Histograms of cosine similarity between two embedding vectors: one from the original CLIP-L and the other from the OHD-Caps-trained CLIP-L.
    (a) The histogram from the vision encoders. \texttt{Correct} (blue) indicates the scores are from the examples where original CLIP-L predict the ground truth. The other examples are colored in orange.
    (b) The histogram from the text encoders. Same color scheme is employed. Measured only on ground truth text.
    (c) The cosine similarity distribution between text embeddings of text without object hallucination (purple) and with object hallucination text (green) for all samples.
    }
    \label{fig:cos_sims}
    \vspace{-5mm}
\end{figure}

Based on our initial observation that OHD-Caps-trained CLIP does not necessarily yield better results when combined with LVLM (Section \ref{sec:intro}), we further investigate how fine-tuning affects the embedding vectors produced by CLIP’s vision and text encoders.
We compute the cosine similarity between two embedding vectors: one from the original CLIP-L and the other from the OHD-Caps-trained CLIP-L, using the entire image-text pairs in the OHD-Caps test set.
The results show although there are significant changes in both image and text embedding vectors (Figure~\ref{fig:cos_sims}(a), (b)), there are no significant differences between cases where original CLIP-L answered correctly (blue) or not (orange). 
In contrast, Figure~\ref{fig:cos_sims}(c) reveals significant changes in text embeddings for captions with hallucination (purple), and without hallucination (green) highlighting the distinct adaptation of text representation.
These observations suggest that OHD-Caps training may shift the representation space of the vision encoder while enhancing the text encoder’s discriminative ability.

\subsection{Fine-grained CLIPScore}

Motivated by this observation, we propose a simple metric called Fine-grained CLIPScore (F-CLIPScore), which enhances the discriminative power of the VLM in order to utilize textual information with more granularity without additional training.
F-CLIPScore first utilizes the spaCy parser~\cite{spacy} to extract nouns from a given sentence, ensuring a finer-grained evaluation of textual content.
Then, it evaluates the quality of an image caption by averaging the CLIPScore of the entire sentence and each individual noun (Figure \ref{fig:schema}).
Mathematically, given a full sentence $s$ and a total of $N$ nouns, denoted as $n_{i}$, F-CLIPScore is defined as 

\begin{equation}
    \small
    \operatorname{F-CLIPScore}(s) = \frac{\operatorname{CLIPScore}(s) + \sum_{i=1}^{N}{\operatorname{CLIPScore}(n_i)}}{N+1}
    \label{eq:fclip}
\end{equation}

\begin{figure}
    \centering
    \includegraphics[width=1.05\linewidth]{images/schema.png}
    \caption{The graphical representation of F-CLIPScore.}
    \label{fig:schema}
    \vspace{-5mm}
\end{figure}

\section{Experiments}

The F-CLIPScore evaluation only introduced additional parser processing time (less than a per sentence), as the added nouns were batch-processed.
For training CLIP on the OHD-Caps train set, we used an effective batch size of 64 and a learning rate of 1e-5, which required 3 hours on an H100 GPU.  
For LLaVA pretraining, we employed an effective batch size of 128 and a learning rate of 1e-3, which took 7 hours on two H100 GPUs.

\section{Results}

By utilizing the F-CLIPScore, which directly leverages the image embeddings from the CLIP vision encoder without any training or gradients, while only adding a parsing process during forward propagation, we can efficiently gain insights into whether the issue of object hallucination arises from the limited capability of the vision encoder.

\subsection{F-CLIPScore on OHD-Caps} \label{subsec:ohdcaps}

We evaluated the OHD-Caps~\cite{ohd-caps} test set, an object hallucination assessment dataset, using the proposed F-CLIPScore.
As shown in Table~\ref{table:ohd_caps_test}, evaluation results with CLIP ViT-L~\cite{clip} indicate that F-CLIPScore outperformed the baseline model by up to 39.6\% without additional training (row 1 vs. 2). However, it still performed 21.8\% to 38.6\% worse than trained models (row 2 vs. row 3). This may indicate that although CLIP vision encoders may not be the main cause of object hallucination, further training may enhance their capability. 

\begin{table}[h]
    \caption{Accuracy on the OHD-Caps test set evaluated with OpenAI CLIP-L.}
    \small
    \resizebox{1\columnwidth}{!}{
    \begin{tabular}{l|c|c|c}
        \toprule
        \multicolumn{4}{c}{OHD-Caps ACC (\%, $\uparrow$)} \\
        \midrule
        \textbf{Metric} & \textbf{coco} & \textbf{flickr30k} & \textbf{nocaps} \\
        \midrule
        \multicolumn{4}{c}{w/o training} \\
        \midrule
        CLIPScore & 22.6 & 22.6 & 12.4 \\
        $\operatorname{F-CLIPScore}$ & \textbf{62.2} & \textbf{62.2} & \textbf{46.6} \\
        \hline
        \multicolumn{4}{c}{trained w/ OHD-loss$^\dagger$} \\
        \midrule
        CLIPScore & 84.0 & \textbf{89.2} & 85.2 \\
        \midrule
        \multicolumn{4}{c}{trained w/ F-CLIPScore loss} \\
        \midrule
        CLIPScore & \textbf{85.0} & 89.0 & \textbf{86.4} \\
        \bottomrule
    \end{tabular}
    }
\label{table:ohd_caps_test}
$\dagger$: Reproduced~\cite{ohd-caps}
\vspace{-1mm}
\end{table}

To test whether F-CLIPScore is orthogonal to the proposed method from \citet{ohd-caps}, we modify the loss as
\begin{equation}
\small
L=L_{CLIP}+L_{OHD}+\frac{\alpha}{B}\sum_{i=1}^{B}{(1-\operatorname{F-CLIPScore}(I_i, C_i))}
\end{equation}
, where $L_{CLIP}$ is the contrastive loss proposed in ~\cite{clip}, and $L_{OHD}$ is the marginal loss proposed in ~\cite{ohd-caps}. $B$ denotes the batch size, while $I_i$ and $C_i$ are the $i$-th positive pair. $\alpha$ is a hyperparmeter that we set to $0.2$.

Applying F-CLIPScore as a loss improved performance by +1.0 percentage point on the COCO subset and +1.2 percentage points on the Flickr30K subset (Table \ref{table:ohd_caps_test}, bottom row).
Given the relatively small difference in performance, it is possible that the model has already overfitted to the dataset.

\subsection{LVLM Pretrain Data Curation with F-CLIPScore}

As shown in Section~\ref{subsec:ohdcaps}, F-CLIPScore was able to exhibit competent performance in detecting object hallucination without training.
We aimed to investigate whether F-CLIPScore could influence the pretraining process of LVLM that connects the vision encoder and LLM.
To explore this, we utilized F-CLIPScore to filter the pretraining data for LLaVA~\cite{LLaVA}.

\begingroup
\setlength{\tabcolsep}{4.5pt}
\renewcommand{\arraystretch}{1}
\begin{table}
    \centering
    \caption{POPE benchmark accuracy after LLaVA pretraining. We use CLIP-L~\cite{clip} as a vision encoder, and Llama 2 7B~\cite{llama2} as an LLM backbone. ‘trained’ indicates the OHD-Caps-trained CLIP-L.}
    \renewcommand{\arraystretch}{1.3}
    \fontsize{8}{9.5}\selectfont
    \begin{tabular}{l|ccccc}
        \toprule
        \textbf{POPE Acc ($\uparrow$, \%)} & \multicolumn{5}{|c}{\textbf{Filtering  rate (\%)}} \\
        \midrule
        \textbf{Filtering Method} & \textbf{20} & \textbf{30} & \textbf{40} & \textbf{50} & \textbf{60} \\
        \midrule
        CLIPScore (w/o train) & 52.2 & 47.3 & 50.0 & 53.1 & 50.0 \\
        F-CLIPScore (w/o train)  & 51.6 & \textbf{55.5} & 50.6 & 50.1 & 51.8 \\
        CLIPScore (trained) & 49.8 & 49.8 & 50.8 & 49.8 & 52.6 \\
        \midrule
        w/o filtering & \multicolumn{5}{|c}{50.58} \\
        \bottomrule
    \end{tabular}
    \label{table:pope}
    \vspace{-4.5mm}
\end{table}
\endgroup

As shown in Table~\ref{table:pope}, training the alignment model using only the top 70\% of data curated by F-CLIPScore led to a +4.9\% improvement in POPE accuracy compared to training on the entire dataset (second row).
On the other hand, when using the OHD-Caps-trained CLIP for filtering, the improvement was marginal.

These results may suggest that F-CLIPScore effectively measures object hallucination-related quality even in general dataset.
We thought this was due to F-CLIPScore not requiring any fine-tuning, which preserves general capabilities while enabling detecting object hallucination.

Furthermore, our findings highlight that object hallucination can be mitigated solely through curated data by training only the adapter, without modifying the vision encoder or LLM.
This underscores the need to explore alternative causes of object hallucination beyond capacity of the vision encoder.

\section{Conclusion}

We introduce F-CLIPScore, a simple yet effective metric for evaluating fine-grained image-caption alignment and addressing object hallucination in Vision-Language Models.
Unlike conventional CLIPScore, which relies solely on sentence-level embeddings, F-CLIPScore also incorporates noun-level embeddings.
This refinement allows the model to better mitigate object hallucination without requiring additional training for the vision encoder. 
We validate F-CLIPScore by showing a +39.6\% accuracy in OHD-Caps benchmark. We also show that data curation based on F-CLIPScore can enhance LVLM performance in hallucination mitigation, even with a reduced dataset.
These results may indicate that the limitations of existing evaluation metrics are one possible origin of object hallucination, which fails to efficiently reveal the capacity of the vision encoder.

\section*{Limitations}

While this study proposes a method for analyzing and mitigating object hallucination using F-CLIPScore, it is subject to the following limitations. First, we were unable to conduct experiments on the Supervised Fine-Tuning (SFT) for Large Vision-Language Models (LVLMs). In the LVLM training pipeline, after the alignment pretraining phase—where the vision encoder and LLM remain frozen—the SFT stage follows, in which these components are unfrozen and further trained. However, due to computational resource limitations, we did not fully explore the potential impact of F-CLIPScore during SFT. Future work could investigate ways to incorporate F-CLIPScore into the SFT process to enhance training effectiveness.

Second, our method faces linguistic constraints and challenges in multilingual generalization. This study employs the spaCy parser~\cite{spacy} to extract noun phrases from text, a technique that performs relatively reliably in well-structured languages such as English. However, parsing accuracy may vary across different languages, potentially leading to inconsistencies in F-CLIPScore computation. To address this, future research should explore the scalability of F-CLIPScore by evaluating its effectiveness on multilingual datasets and refining the parsing methodology for broader linguistic applicability.

\bibliography{custom}

\end{document}
