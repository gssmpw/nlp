\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024
\PassOptionsToPackage{round,authoryear}{natbib}

% ready for submission
\usepackage[preprint]{neurips_2024}
\usepackage{graphicx}
\usepackage{microtype}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage[breakable]{tcolorbox}
\definecolor{schemaBlue}{HTML}{c9e0ff}
\definecolor{babyblue}{rgb}{0.54, 0.81, 0.94}
\definecolor{blue(pigment)}{rgb}{0.2, 0.2, 0.6}
\usepackage[toc,page]{appendix}
% \usepackage{tcolorbox}
\definecolor{mBlue}{HTML}{173b4f}
\usepackage{listings}
\usepackage{multicol}
\usepackage{listings}
\usepackage{float}

\usepackage{xcolor}
\usepackage{mdframed}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{placeins}  % Load in preamble

\usepackage{mdframed}
\usepackage{tikz}
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\newtcolorbox{custombox}{
  colframe=pink!80!red, % Border color
  colback=pink!20, % Background color
  arc=5pt, % Rounded corners
  boxrule=1.5pt, % Border thickness
  coltitle=black, % Title text color
  colbacktitle=pink!80!red, % Title background color
  fonttitle=\bfseries,
  title={\textbf{Model identification prompt}},
  % label=prompt:model_identification_learning,
  breakable % <-- Ensures the box breaks across pages
}



\newtcolorbox{custombox1}{
  colframe=pink!80!red, % Border color
  colback=pink!20, % Background color
  arc=5pt, % Rounded corners
  boxrule=1.5pt, % Border thickness
  coltitle=black, % Title text color
  colbacktitle=pink!80!red, % Title background color
  fonttitle=\bfseries,
  title={\textbf{Model generation prompt}},
  % label=prompt:model_generation_learning,
  breakable % <-- Ensures the box breaks across pages
}


% colback=schemaBlue,colframe=ceruleanblue

\newtcolorbox{custombox_decision_making_identification}{
  colframe=ceruleanblue, % Border color
  colback=schemaBlue, % Background color
  arc=5pt, % Rounded corners
  boxrule=1.5pt, % Border thickness
  coltitle=white, % Title text color
  colbacktitle=ceruleanblue, % Title background color
  fonttitle=\bfseries,
  title={\textbf{Model identification prompt}},
  % label=prompt:model_identification_learning,
  breakable % <-- Ensures the box breaks across pages
}


\newtcolorbox{custombox_decision_making_generation}{
  colframe=ceruleanblue, % Border color
  colback=schemaBlue, % Background color
  arc=5pt, % Rounded corners
  boxrule=1.5pt, % Border thickness
  coltitle=white, % Title text color
  colbacktitle=ceruleanblue, % Title background color
  fonttitle=\bfseries,
  title={\textbf{Model generation prompt}},
  % label=prompt:model_identification_learning,
  breakable % <-- Ensures the box breaks across pages
}

\newtcolorbox{custombox_decision_making_noisy_data}{
  colframe=ceruleanblue, % Border color
  colback=schemaBlue, % Background color
  arc=5pt, % Rounded corners
  boxrule=1.5pt, % Border thickness
  coltitle=white, % Title text color
  colbacktitle=ceruleanblue, % Title background color
  fonttitle=\bfseries,
  title={\textbf{Examples of LLM-generated heuristics based on the noisy Take the Best and Tallying data}},
  % label=prompt:model_identification_learning,
  breakable % <-- Ensures the box breaks across pages
}


\newtcolorbox{custombox_parser_function}{
  colframe=mBlue!75!black, % Border color
  colback=mBlue!5!white, % Background color
  arc=5pt, % Rounded corners
  boxrule=1.5pt, % Border thickness
  coltitle=white, % Title text color
  colbacktitle=black, % Title background color
  fonttitle=\bfseries,
  title={\textbf{Function extraction parser}},
  % label=prompt:model_identification_learning,
  breakable % <-- Ensures the box breaks across pages
}


\newtcolorbox{custombox_parser_parameter_list}{
  colframe=mBlue!75!black, % Border color
  colback=mBlue!5!white, % Background color
  arc=5pt, % Rounded corners
  boxrule=1.5pt, % Border thickness
  coltitle=white, % Title text color
  colbacktitle=black, % Title background color
  fonttitle=\bfseries,
  title={\textbf{Parameter name list parser}},
  % label=prompt:model_identification_learning,
  breakable % <-- Ensures the box breaks across pages
}


% colback=mBlue!5!white,colframe=mBlue!75!black


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2024}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors


\title{Towards Automation of Cognitive Modeling using Large Language Models}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Milena Rmus \\
  Institute for Human-Centered AI\\
  Helmholtz Computational Health Center\\
  Munich, Germany \\
  \texttt{milena.rmus@helmholtz-munich.edu} \\
  % examples of more authors
  \And
  Akshay K. Jagadish \\
  Institute for Human-Centered AI \\
  Helmholtz Computational Health Center\\
  Munich, Germany \\
  % \texttt{email} \\
  \AND
  Marvin Mathony \\
  Institute for Human-Centered AI \\
  Helmholtz Computational Health Center \\
  Munich, Germany \\
  % \texttt{email} \\
  \And
  Tobias Ludwig \\
  Computational Principles of Intelligence Lab\\
  Max Planck Institute for Biological Cybernetics \\
  Tubingen, Germany \\
  % \texttt{email} \\
  \And
  Eric Schulz \\
  Institute for Human-Centered AI \\
  Helmholtz Computational Health Center \\
  Munich, Germany\\
  % \texttt{email} \\
}


\begin{document}


\maketitle


\begin{abstract}
Computational cognitive models, which formalize theories of cognition, enable researchers to quantify cognitive processes and arbitrate between competing theories by fitting models to behavioral data. Traditionally, these models are handcrafted, which requires significant domain knowledge, coding expertise, and time investment. Previous work has demonstrated that Large Language Models (LLMs) are adept at pattern recognition in-context, solving complex problems, and generating executable code. In this work, we leverage these abilities to explore the potential of LLMs in automating the generation of cognitive models based on behavioral data. We evaluated the LLM in two different tasks: model identification (relating data to a source model), and model generation (generating the underlying cognitive model). We performed these tasks across two cognitive domains - decision making and learning. In the case of data simulated from canonical cognitive models, we found that the LLM successfully identified and generated the ground truth model. In the case of human data, where behavioral noise and lack of knowledge of the true underlying process pose significant challenges, the LLM generated models that are identical or close to the winning model from cognitive science literature. Our findings suggest that LLMs can have a transformative impact on cognitive modeling. With this project, we aim to contribute to an ongoing effort of automating scientific discovery in cognitive science.
\end{abstract}


\section{Introduction}
Scientific discovery is driven by hypothesis generation and testing. For cognitive science this involves generating theories about cognitive processes that underlie behavior, and testing them by handcrafting computational models and fitting them to behavioral data \cite{polk2002cognitive}. 

However, handcrafting cognitive models that can serve as valid explanations of behavior can be extremely time consuming \cite{musslick2024closed, musslick2024automatingpracticescience}. 
It requires trained researchers to conduct lengthy literature reviews, come up with cognitively plausible theories, and design computationally feasible algorithms to test these theories. Notably, cognitive modeling is also limited by the assumptions that researchers inject into their models \cite{krefeld2022structural, taatgen2016cognitive}. Their background and proficiency in modeling can restrict them from exploring an alternative space of hypotheses - which could provide equally good (or better) explanations of the behavior \cite{frischkorn2018cognitive, addyman2012computational}.

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\textwidth, keepaspectratio]{prompting.pdf}
    \caption{Schematic of our approach: We prompt the LLM with a task description and data in text format to either identify the source model of the data (shown in dashed paths) or generate a cognitive model that explains the underlying data as a Python function (shown in solid paths). In the model identification task, the LLMâ€™s output is evaluated against the ground truth to determine how often it correctly identifies the source model. In the model generation task, the LLM-generated models are evaluated by 1) simulating them and comparing simulated to ground truth data, and 2) fitting them to behavioral data (synthetic or real human data) and comparing the model fit against ground truth or winning model from the literature using Bayesian Information Criterion (BIC) \cite{watanabe2013widely}. Note that in the learning experiment, model generation evolves over 10 sampling runs. During each run, three LLM-generated models are fitted offline to the data excluded from the prompt, and the fitness metric (BIC) is used to provide feedback to the LLM on the subsequent run. For full prompts, see the Appendix.
    }
    \label{llm-prompt}
\end{figure*}


One way to address these challenges is by automating cognitive model generation \cite{musslick2024automatingpracticescience}. Although automating scientific discovery has been a fruitful endeavor in many disciplines, such as biology \cite{king2004functional}, mathematics \cite{falkenhainer1986integrating}, and physics \cite{langley1981data}, the idea has only recently made its way into cognitive science \cite{peterson2021using, Musslick_AutoRA_Automated_Research_2024, weinhardt2024computational}. 


Recent advances in Large Language Models (LLMs) have opened up new possibilities for scientific discovery \cite{wang2023scientific, binz2023should}. Specifically, we focus on three LLM abilities that can facilitate the development of a flexible, domain-general framework for automating computational cognitive modeling. First, LLMs can take the behavioral data formatted in natural language along with the corresponding task description. This provides them with the flexibility to handle diverse task domains with varying levels of complexity \cite{binz2024centaur}. Second, they can identify patterns in complex problems in-context and generate hypotheses about the data generating process \cite{xiao2024verbalized}.   %For instance, researchers have successfully prompted LLMs to discover statistical models automatically in the past \cite{li2024automated}. 
Third, their ability to synthesize highly accurate programs \cite{austin2021program,perez2021automatic} lends itself nicely to cognitive modeling. %Therefore, these three features of LLMs -- natural language input, hypothesis/model generation, and code writing -- can facilitate the development of a flexible, domain-general framework for automating computational cognitive modeling. 

% , where a description of the task and the task data can be used as an input to the LLM tasked to generate a code of a potential computational cognitive model
In this work, we used an LLM for generating hypotheses about cognitive processes underlying behavioral data. To achieve this, we developed a pipeline in which the LLM was tasked with 1) identifying the source model (out of possible candidates) that best explains the observed data and 2) synthesizing cognitive models as Python functions, using behavioral data and its accompanying task description. We used our approach on synthetic data sets generated from canonical models from the human learning and decision making literature. This enabled us to evaluate the LLM's outputs against the ground truth in both of these tasks. Additionally, we applied the pipeline to two human behavioral data sets with an unknown ground truth model. This allowed us to test our approach under the level of complexity typically encountered in cognitive modeling projects. Across all experiments we found that the LLM was successful in both model identification and model generation tasks. These results suggest that LLMs have the potential to revolutionize standard practices in computational cognitive modeling.


\section{Related work}

\paragraph{Cognitive modeling.}
The aim of modeling in cognitive science is to improve our understanding of cognitive processes. A model usually manifests in precise mathematical formulations of how behavioral data was generated. There are many ways in which a cognitive model serves understanding of cognitive processes, including, but not limited to, providing mechanistic explanations of cognition, enabling predictions or allowing for the evaluation of competing hypotheses  \cite{wilson2019ten}. In this work, we chose two classic domains in cognitive science, decision making and learning, to test whether cognitive modeling can be catalyzed by the use of LLMs. Specifically, we focused on heuristic-based decision making and reward-based learning strategies \cite{gigerenzer1996reasoning, frank2004carrot, pessiglione2006dopamine, wang2016learning, binz2022heuristics}.

\paragraph{Automated model discovery with LLMs.}
The goal of automating model discovery, given some data, is not new to science. Automation promises to accelerate and democratize scientific discovery by making it independent of a researcher's prior knowledge and training. However, until recent successes of LLMs, automated hypothesis search was mostly confined to designing models in a domain-specific language and handcrafting search algorithms to identify the best-fitting model from the space of pre-defined models \cite{kemp2008discovery,lloyd2014automatic,musslick2024automatingpracticescience, gulwani2011automating, steinruecken2019automatic,hewson2023bayesian}. 

Recently, it has been demonstrated that these limitations can be overcome with the use of LLMs. Researchers have successfully used LLMs to automate discovery of statistical models \cite{li2024automated}, solve classical machine learning problems in regression and image classification \cite{xiao2024verbalized}, suggest niche rules that are not widely recognized but are scientifically sound in chemistry \cite{zheng2023large} and even automate the entire scientific pipeline in the field of machine learning -- from creating ideas to designing appropriate experiments, conducting them, writing the paper and simulating the review process \cite{lu2024ai}.

\paragraph{Code writing abilities of LLMs.}
The merit of LLMs in automating model search stems not only from their domain-general knowledge, but also from their ability to process and generate natural language text and synthesize code from natural language instructions. We believe that using LLMs to synthesize code in a general-purpose language like Python paves the way for overcoming the limitations of handcrafting domain-specific languages to automate model search \cite{austin2021program,ni2024l2ceval}.
Notable work in this area has already shown the proficiency of LLMs in providing code to solve math and classic Python problems \cite{austin2021program, ni2024l2ceval, perez2021automatic}. Furthermore, \cite{li2024automated, xiao2024verbalized, zheng2023large} have demonstrated encouraging results with respect to the ability of LLMs to output mathematical functions and even probabilistic Python programs that model input data. 

\section{Experiment 1: Decision making}

In decision making research, participants are often presented with multiple options, each defined by a distinct set of features. In the classic two-alternative forced choice study design, participants are asked to choose between two options, resulting in a simplified framework for studying the decision making process \cite{bogacz2006physics}.

\subsection{Methods}

\paragraph{Task.}
We designed a task in which decision making agents chose between two options (A and B). Each option is characterized by three features, represented as integers ranging from 0 to 100.

\paragraph{Heuristics.}
We first focused on decision making strategies known as heuristics. Heuristics are simple, resource-efficient approaches that individuals use to navigate the decision making process effectively \cite{tversky1974judgment,gigerenzer1996reasoning}. Among the many heuristics available, we specifically examined two: the Take the Best and the Tallying heuristic.

The Take the Best heuristic selects an option based solely on a single prioritized feature, ignoring comparisons across other features \cite{gigerenzer1996reasoning}. Specifically, only the prioritized feature is used to evaluate the two options, with the option that has the higher value for the prioritized feature winning in the comparison.  %Preliminary tests revealed that the LLM exhibited a bias toward comparing the first feature of the two options, regardless of the heuristic being applied. This bias artificially inflated the accuracy. To address this issue, we fixed the prioritized feature to be the second feature in all our simulations.
The Tallying heuristic instead compares the two options based on all three features, counting the number of features for which one option has a higher value than the other, and favoring the option that has a higher number of superior features.

\begin{figure*}[hbt!]
    \centering
    \includegraphics[width=0.95\textwidth]{fig2.pdf}
    \caption{ A) Identifying the source decision making heuristic by using the LLM to relate data to the source simulation code. We prompted the LLM to identify which of the two heuristics (Take the Best or Tallying) underlies the behavioral data from the two-alternative forced choice task, where the agent chooses between two options defined by three features. While exploring the LLM's capacity to perform this task, we tried different prompting techniques. B) We tested the LLM with noisy decision making data, with injected noise increasing the confusion between the two heuristics. The LLM shows robustness to noise in the data - performance decreases proportionally with noise, but only reaches chance level when the heuristics are indeed indistinguishable (noise level of 0.5). Error bars represent standard error of the mean (SEM) across 10 runs. C) LLM-generated Python function heuristics closely align with the ground truth. The LLM-generated functions remained the same across 10 separate experiment runs.}
    \label{llm-heuristics}
\end{figure*}

\paragraph{Synthetic data generation.}
We generated 80 decision problems (each problem including two sets of three features), in accordance with the task specification. During task generation, we ensured that the number of times option A or B is superior is balanced. For Take the Best simulations we deliberately prioritized the second feature to avoid the LLM making a potentially misleading assumption that the first feature should be prioritized. We simulated 40 decisions based on each of the two heuristics. Importantly, we initially ensured that our examples were unambiguous -avoiding cases where both heuristics would lead to the same decision. This approach guaranteed the identifiability of decision patterns unique to each heuristic. To further test the robustness of our approach, we simulated a second data set that introduced noise to the data simulation. We did this by increasing the proportion of decisions in which the final output of the heuristic was flipped, resulting in the opposite choice than the heuristic would actually make. We considered three noise levels: 0, 0.25, and 0.5.  This progressively increased the level of confusion between the two heuristics (Fig. \ref{llm-heuristics}B). At a noise level of 0.5, the decision patterns for both heuristics are indistinguishable and are equivalent to random guessing.

\paragraph{LLM prompting.}
We queried the LLM to perform two tasks: (1) match the data to the source model (model identification) and (2) generate a cognitive model based on the observed data for both decision making and learning experiments (model generation). In the model identification task, the LLM prompt included the code for model candidates provided as Python strings along with simulated decisions. For the model generation task, the prompt included a description of the desired structure of the Python function (e.g., the function name, input arguments, and expected output). Fig. \ref{llm-prompt} presents the text used to prompt the LLM for these tasks, along with the format in which the data was provided (see Appendix \ref{prompt:code_match_heuristic} for full prompts). We considered three different prompting strategies: vanilla (containing only the description of the task setup), Step-Back \cite{zheng2023take}, and Chain of Thought (CoT; \citealt{wei2022chain}). This comparison aimed to identify the most effective prompting strategy for subsequent tests.

 % This ensured that our text parser could extract the generated function from the LLM's response for manual evaluation. 

\paragraph{LLM specification.}
We used an open-source Llama 3.1 Instruct model with 70 billion parameters in our pipeline \cite{meta2024llama3.1}. Importantly, all of our tests were performed in-context. For code generation, we set the temperature to 0.2 to encourage some exploration when generating models; for code matching, we decreased noise to 0.001. We did not modify the default values of other model parameters.


\subsection{Results}


\paragraph{Model identification.}

%In the first task, hen had to identify the source model from two the two provided options (see Appendix for a detailed prompt example).
Model identification enabled us to evaluate the LLM's ability to reason through decision making strategies and map the data to the underlying function. Consistent with previous results \cite{wei2022chain}, CoT prompting led to the best LLM performance (Fig. \ref{llm-heuristics}A); therefore, we used CoT prompting for all subsequent experiments. We found that in the noise-free data set, the LLM was perfect at identifying the source model based on the data (Fig. \ref{llm-heuristics}A). When evaluated on the noisy data set, the LLM robustly identified the ground truth heuristic at noise level of 0.25 (Take the Best mean accuracy: 0.86 (SEM = 0.02); Tallying mean accuracy: 0.71 (SEM = 0.06)). At a noise value of 0.5 - corresponding to random guessing in the data generating process (Fig. \ref{llm-heuristics}B) - the LLM's heuristics predicted decisions at chance level (Take the Best accuracy $>$ 0.5 test : t(9)=1.80, p=0.10; Tallying accuracy $>$ 0.5: t(9)=0.27, p=0.79).


\paragraph{Model generation.}

Next, we tested whether the LLM could generate a decision making algorithm that aligned with the observed strategy patterns, simulated using either the Take the Best or Tallying heuristic (see Appendix \ref{prompt:code_generate_heuristic} for the prompt). We evaluated the LLM-generated algorithms on unseen decision tasks, comparing their outputs to the predictions of the ground truth heuristic.

Our analysis of LLM-generated functions revealed that the LLM could successfully identify the two underlying heuristics: prioritizing a single feature for Take the Best simulations and performing across-feature comparisons for Tallying simulations (Fig. \ref{llm-heuristics}C). The choices generated by the LLM-proposed models matched the ground truth heuristic choices with perfect accuracy in the evaluation tasks. It is notable, however, that for the Tallying heuristic there was a slight departure from the ground truth in the LLM-generated code -- using the total sum of features instead of a tally of superior features. There are corner cases where these strategies would make diverging predictions (e.g., if the feature values are not normalized / in the same range). %It is possible that a sum is a more generalized operation compared to tallying, biasing the LLM to consider the feature sum rather than tallying as the decision making strategy. 
Nevertheless, the LLM proposed an equally valid alternative to the true data generating process. %The proposed model was equally valid because the feature values observed by the LLM did not differentiate between the two strategies. 

To account for the noise in the noisy data set, the LLM-generated heuristics deviated more from the underlying heuristics. For noisy Take the Best data, the LLM still prioritized the second feature but modified the heuristic to apply only when a specific criterion (e.g., feature value differences above a certain threshold) was met. For noisy Tallying data, the LLM generated various strategies such as choosing based on the highest overall or minimum feature value (see Appendix \ref{result:code_generate_noisy}). 

%Nevertheless, although the feature sum was not established as the ground truth, the feature values presented in the task observed by the LLM did not differentiate between the two strategies. As a result, the code generated by the LLM can still be considered valid.  



\section{Experiment 2: Learning}

The decision making tasks considered above were static problems, where each decision was independent. To advance beyond this framework, we explored dynamic scenarios where decisions evolve over time, influenced by learning from the feedback of prior choices. To this end, we focused on (multi-armed) bandit tasks.

\subsection{Methods}

\paragraph{Task.}
Bandit tasks are frequently used in studying reinforcement learning \cite{frank2004carrot, pessiglione2006dopamine, wang2016learning}. Generally, in the bandit task, an agent chooses between \textit{N} arms, often with a predefined reward contingency associated with each arm. The agent receives feedback based on action selection and, over a sequence of trials, it learns to adjust action selection in a way that maximizes positive outcomes. Variants of this task have been used to examine different aspects of reward-based learning in humans and artificial agents \cite{wang2016learning, jagadish2023zero, schubert2024context}. 

We implemented a two-armed bandit task, with each of the two options associated with a fixed probability of receiving a reward if selected (e.g. ${p(r = 1|a_1) = 0.20; p(r = 1|a_2) = 0.80}$). The rewards in our task were binary ($r \in \{0, 1\}$).

\paragraph{Learning models.}
The Rescorla-Wagner (RW) model \cite{rescorla1972theory} is commonly used to study learning dynamics in the bandit tasks. In the experiment, we considered the vanilla RW model and two variants of it - RW with two learning rates ($RW + \alpha^\pm$) and RW with stickiness ($RW + \kappa$).
%The Rescorla-Wagner (RW) model \cite{rescorla1972theory} is one of the most influential frameworks in reinforcement learning, underscored by the concept of prediction error.

The RW model posits that the value of each action ($V$) is determined by the history of rewards obtained from selecting that action. According to the RW model's learning rule, the value of the selected action $a$ ($V^a$) is updated on each trial \textit{t} as follows:


\begin{equation*}
{V^a_{t+1} = V^a_{t} + \alpha \cdot (r-V^a_{t})}
\end{equation*}

where $r-V^a$ is the reward prediction error - a learning signal that drives the adjustment of the selected action value, and $\alpha$ represents a learning rate that captures the extent to which the action value is modified by the prediction error.

Learning models commonly rely on the softmax policy in conjunction with the RW learning rule, providing a way to transform action values into probabilities. The softmax policy introduces the exploration parameter $\beta$, which controls the degree to which action selection is deterministic: 

\begin{equation*}
P(a) = \frac{\mathrm{exp}(\beta \cdot V^a_{t})}{\sum_{i=1}^{N} \mathrm{exp}(\beta \cdot  V^i_{t})}
\end{equation*}


\textit{The Rescorla-Wagner model with two learning rates} ($RW + \alpha^\pm$) differentiates between outcomes that are better/worse than expected. More precisely, the model uses two distinct learning rates for action value updating, contingent on the valence of the prediction error:

\begin{align*}
V_{t+1}^a &= \begin{cases}
V_{t}^a + \alpha^+\,(r-V^a_{t}) & \mbox{if } r-V^a_{t} \geq 0 \\
V_{t}^a + \alpha^-\,(r-V^a_{t}) & \mbox{if } r-V^a_{t} < 0
\end{cases} 
\end{align*}


\textit{The Rescorla-Wagner model with stickiness} ($RW + \kappa$) has the same learning rule as the vanilla RW but its policy differs in that the additional weight $\kappa$ is applied to the value of the action that was selected during the previous trial, resulting in a greater tendency to choose the previously selected action:

\begin{equation*}
    P(a) \propto \exp\left(\beta V + \kappa \mathbb{I}(a = a_{t-1})\right)
\end{equation*}


\paragraph{Synthetic data generation.}  To test how well we can recover the ground truth learning model, we simulated 100 agents from each of the two above-mentioned models on a two-armed bandit task, with each task comprising of 150 trials. The simulation parameters were randomly sampled for each agent in a range defined by plausible parameter bounds (see Appendix \ref{prompt:simulation_details}).

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=.8\textwidth]{fig3.pdf}
    \caption{A) Model identification task. The LLM-generated 'ModelIdentification' function utilizes the SciPy differential evolution method to successfully differentiate between the two learning models. B) Evaluation of the LLM-generated cognitive model based on the data simulated from the $RW +\kappa$ revealed that it captured behavior better than the random guessing model and the vanilla RW. C) Simulation of the LLM-generated model showed that it captured the underlying propensity for choosing more rewarding actions. Error bars represent standard error of the mean (SEM) across simulated agents.}
    \label{llm-bandit}
\end{figure*}

\paragraph{LLM Prompting.}

Identifying the source model by reasoning through the long sequences of learning data, which consisted of 150 actions and rewards, is much more challenging than identifying the underlying decision making heuristic. Additionally, differently configured models can produce similar action/reward trajectories - a common challenge in models of bandit tasks \cite{wilson2019ten}. As a result, we modified the prompts we had developed for the decision making experiment.

\textit{Function generation for model identification.}  Unlike in the decision making case where the LLM directly returned the model identity, in the learning task the LLM instead generated a function for model identification. The function's arguments were predefined (e.g., lists of actions and rewards). The prompt encouraged the LLM to propose a method for matching the source model to the data without requiring step-by-step reasoning (see Appendix \ref{prompt:model_identification_learning}). The generated function was then manually evaluated to determine its accuracy in identifying the correct model.


\textit{Guided sampling for model generation.}
As in the model identification task, the code generation approach we used for decision making heuristics proved to be inadequate for the learning experiment. 
% Instead, this task required multiple iterations of code generation. To address this, 
Therefore, in the model generation task for learning data, we implemented a guided sampling process, enabling the LLM to propose cognitive models and refine them based on feedback.

Specifically, for each model, we conducted 10 sampling runs, during which the LLM generated three cognitive models per run based on the input data and prompt specifications (see Appendix \ref{prompt:model_generation_learning}). %An iterative revision process based on a single model often resulted in unhelpful or impractical suggestions, so we opted to allow the LLM to generate multiple models. 
To ensure that the LLM-generated code was executable and free of bugs, we provided a template function. This template defined the functionâ€™s inputs (behavioral data: actions and rewards, and model parameters as three lists) and specified the output as the log likelihood of actions conditioned on the model parameters. This setup allowed us to automate the execution of LLM-generated functions, enabling us to assess how well the generated models explained the data.

During each sampling run, we fit each of the three generated models to separate sets of data using the minimize function from the SciPy optimization library \cite{2020SciPy-NMeth}. The optimizer was initialized 20 times with different starting points, derived from randomly sampled parameter values, to avoid local minima.

In subsequent sampling runs, the LLM was provided with feedback identifying the best performing model it had generated up to that point, across all runs. This feedback encouraged further exploration of alternative model possibilities, with all likelihoods stored and referenced across runs (see Appendix \ref{prompt:llm-feedback-learning}). To ensure that the LLM does not repeat generation of the same models, we also presented it with a list of cognitive model parameters (e.g., learning rate, random lapse and exploration) proposed in previous runs.


\paragraph{Evaluation of LLM-generated cognitive models.} The best LLM-generated cognitive models were evaluated in two steps at the end of the final sampling run. First, we compared the Bayesian Information Criterion (BIC; \citealt{watanabe2013widely}) of the best model to the ground truth (or the best model from the literature for human data), random and a competing model.
Second, we manually implemented a simulation script based on the equations of the LLM-generated model. Using the best-fit parameters from the first step, the script simulated action choices according to the model's equations and parameters, with rewards determined by the probabilistic actionâ€“reward contingencies of the task. This step allowed us to assess how well the model captured behavioral patterns by comparing the simulated data with the ground truth.

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\textwidth]{fig4.pdf}
    \caption{A) The BIC of the best cognitive model generated by the LLM based on the human data closely matched the winning model in \citealt{chambon2020information} for both partial and full feedback conditions. Error bars represent standard error of the mean (SEM) across participants. B) Models generated by the LLM for the data from the partial and full feedback conditions. The LLM proposed a two-learning rate version of the RW model for the partial condition, and one with four learning rates for the full feedback condition. The different learning rates are used for action value updating, depending on whether the feedback was rewarding or not, with additional learning rates in the full feedback condition allowing for different updates based on feedback for chosen/unchosen actions. This is very similar to \citealt{chambon2020information} model, which allowed for asymmetry in learning driven by the difference in the prediction error.}
    \label{llm-bandit-human}
\end{figure*}

\subsection{Results}

\paragraph{Model identification.}

%We focused on assessing LLM's ability to recover cognitive models underlying data simulated from $RW + \alpha^{+/-}$ and $RW + \kappa$. 
We prompted the LLM to write a function, called \texttt{ModelIdentification}, that would identify the source model based on the underlying data in the learning experiments. We found that the LLM generated a function (see Appendix \ref{result:model-identification-function}) that performed an optimized search over possible model parameter values to find optimal log-likelihood, leveraging the differential evolution algorithm for optimization \cite{storn1997differential} from the SciPy library. The proposed function returned the identity of the model associated with the smaller negative log-likelihood. We executed this LLM-generated function offline to determine the identity of the model. As shown in Fig. \ref{llm-bandit}A, we found that the $RW + \alpha ^\pm$ model can be successfully identified 95\% of times (SEM = 4) and the $RW + \kappa$ model could be identified about 85\% of times (SEM = 7). %This indicates that the LLM could use domain knowledge to leverage the optimization algorithm that can be used for model identification.

 % As mentioned in the Methods (see Fig. \ref{llm-prompt}), instead of tasking the LLM to identify the source model based on the underlying data, we prompted it to write a function (called \texttt{ModelIdentification}) that would realize this task instead. We inspected the content of the LLM-generated function (refer to the Appendix). The LLM used an optimized search over possible model parameter values, leveraging the differential evolution algorithm for optimization \cite{storn1997differential} from the SciPy library. This approach was used to quantify the log-likelihood of decisions based on each model. The function then identified and returned the identity of the model associated with the smaller negative log-likelihood. While the LLM's model identification accuracy for the learning task decreased slightly relative to the decision-making case, it was much better than chance (Fig. \ref{llm-bandit}A).

\paragraph{Model generation.}

% As mentioned in the methods section (see "Guided sampling for cognitive model generation in learning experiment"), we resorted to 
For model generation, the pipeline included 10 runs of guided sampling where the LLM generated three cognitive models that were fitted to the data offline on each run. The feedback was automatically constructed based on the model fits, and included as the part of the prompt in the subsequent runs. Note that we only considered the best LLM-generated models across all runs for model fitting and comparison.  

We found that the LLM recovered the $RW + \alpha^\pm$ model correctly from its simulated data. That is, the best generated model was the RW model with two learning rates, based on positive and negative feedback (ground truth model BIC: 78.78 (SEM = 5.3), LLM-generated model BIC: 78.40 (SEM = 1.3); see Appendix \ref{result: rw + 2LR LLM generated model}). %Thus, since the LLM-generated model was identical to the ground truth, we omitted visualizing the comparison. 

For the $RW + \kappa$ model, the LLM did not discover the ground truth model. Instead, the best-fitting model contained a value-decaying mechanism, which lowered the value of non-selected action on each trial (see Appendix \ref{result: rw + stickiness LLM generated model}). This can be viewed as a way to model the forgetting, or the information decaying mechanism, in the learning process. The LLM-generated model fitted better than the random guessing model (Fig. \ref{llm-bandit}B; $t(99) = 7.44, p < 0.001$) and the vanilla RW (Fig. \ref{llm-bandit}B; $t(99) = 2.36, p = 0.01$). As a sanity check, we also compared the data simulated based on the LLM-generated model to the ground truth by 1) quantifying the proportion of trials in which the simulated agent selected the more rewarding option (Fig. \ref{llm-bandit}C), and 2) quantifying the cumulative reward across all trials (see Appendix \ref{result:llm_ppc_sticky_model}). The results showed that the data simulated from the LLM-derived model approximated the ground truth data reasonably well (Fig. \ref{llm-bandit}B, proportion of selecting the more rewarding action: ground truth: 0.69 (SD = 0.13); LLM-generated model: 0.68 (SD = 0.13). 

Additionally, for both data sets, we checked which cognitive model parameters the LLM proposed across other sampling runs, beyond quantitatively studying only the best model. We found that the parameters were reasonable and among those commonly cited in the cognitive modeling literature (see Table \ref{tab:cognitive_model_params}).

\section{Experiment 3: Automated cognitive modeling of human behavior}

Synthetic data sets generated through model simulations serve as a valuable benchmarking tool. Simulating data allows us to shape task behavior according to the assumptions of the underlying model while maintaining access to the underlying ground truth. In contrast, working with data from human participants presents additional challenges due to inherent noise in their behavior and the lack of access to the true underlying cognitive process. To extend our simulation-based test cases, we incorporated human data from a reinforcement learning task \cite{chambon2020information}. 

In the \citealt{chambon2020information} study, 24 participants performed a two-armed bandit task designed to disentangle the effects of prediction-error valence on learning (see Appendix \ref{human_data:task_details} for additional study details). The task consisted of 16 blocks. In half of the blocks,  participants received feedback solely based on their selected action (partial feedback condition). In the other half, the participants received feedback based on their selected action, as well as counterfactual feedback from the alternative action they did not select (full feedback condition).

% The action reward probabilities varied across blocks and were sampled from high-probability reward values (0.9, 0.6) or low-probability reward values (0.4, 0.1). The rewards in this task were binary, with ($r \in \{-1, 1\}$). 

% We compared the LLM generated model against the winning model identified by \citet{chambon2020information} as it provides a valid performance metric for comparison.

First, we applied our guided sampling pipeline to model human behavior in the partial feedback condition, giving vanilla $RW$ as the template function. 
% To evaluate the performance of our method, we used the winning model from the Chambon et al. (2020) paper (whose data we used) as a baseline. 
The best-fitting model in \citet{chambon2020information} was the $RW + \alpha^\pm$. We found that the LLM generated a close version of the winning model (Fig. \ref{llm-bandit-human}A, vanilla RW BIC: 142.65 (SEM = 4.26); winning model BIC: 91.70 (SEM = 5.94); LLM-generated model BIC: 91.72 (SEM = 5.93); random guessing model BIC: 221.81). Specifically, the LLM-generated model also had two learning rates, but the learning rates were contingent on different types of external feedback (reward or no reward), rather than the valence of the prediction error (Fig. \ref{llm-bandit-human}B). 


Next, we applied the pipeline (with a simple counterfactual learning model as a template) to the data from the full feedback condition. The winning model for this experiment was a four-learning-rate model, which updates action values differently based on whether the feedback was positive or negative for chosen/unchosen actions, and includes a perseveration parameter that assigns a higher weight to previously selected actions \cite{chambon2020information}. We found that the best LLM-generated model was again a close version of the winning model - including four learning rates and a softmax temperature parameter (vanilla RW BIC: 137.74 (SEM = 5.86); winning model: 78.19 (SEM = 5.45); LLM-generated model: 78.74 (SEM = 5.5); simple counterfactual learning model BIC: 86.56 (SEM = 5.37); random guessing BIC: 221.81). 

So far we have reported only the best LLM-generated model here, but the other LLM-generated models were close in fitness, and contained compelling theories of cognitive processes engaged in the task (see Appendix \ref{result:model_generation_human_full_feedback_alternative}).


\begin{table}[!t]
\centering
\caption{Examples of parameters in LLM-proposed cognitive models across various sampling runs. Modeling of these mechanisms is documented in previous research \cite{wilson2019ten}.}
\label{tab:cognitive_model_params}
\vskip 0.14in
\begin{small}
    % \begin{sc}
        \begin{tabular}{l p{0.55\columnwidth}}
        \toprule
        \textsc{\textbf{Model Parameter}} & \textsc{\textbf{Explanation}} \\
        \midrule
        Decay & Forgetting mechanism \cite{paskewitz2022explaining} \\
        \midrule
        Random lapse & Random action-executions \cite{nassar2016taming} \\
        \midrule
        Bias & Preference for a particular action \cite{balcarras2016attentional} \\
        \midrule
        Dynamic scaler & Parameter (e.g., learning rate) adjustment based on the trial number \cite{diederen2015scaling} \\
        \midrule
        Exploration bonus & Directed exploration \cite{wilson2021balancing}  \\
        \bottomrule
        \end{tabular}
    % \end{sc}
\end{small}
\vskip -0.1in
\end{table}

% We also inspected different model configurations LLM proposed across the sampling runs; the LLM-suggested model parameters were for the most part reasonable. Some examples included a random lapse in action selection (different from value-based exploration), value decay, bias towards one of the actions, dynamic scaling of parameters contingent on the number of trials (e.g. learning rate decreasing as more trials are observed), etc. Notably, all the phenomena captured by these parameters are commonly observed in human behavior during bandit tasks, underscoring the LLM's capacity to generate plausible cognitive models.

% \subsection{Experiment 3: Human data}


\section{Discussion}
%ES: I will try to do some work on this here, feel free to revert

%Current summary
%We have made the case for using LLMs for cognitive model discovery, leveraging their domain knowledge of the modeling literature, program induction, and code generation skills.

%Eric's proposal
%Should reflect the title and reiterate the intro points.
This work demonstrates the potential of LLMs as powerful tools to automate cognitive modeling. By leveraging their extensive knowledge of the modeling literature, program induction capabilities, and code generation skills, LLMs can automate key aspects of cognitive modeling, traditionally a time-intensive and expertise-dependent process.


%Current results
%We tested an LLM (Llama-3.1 70b Instruct) on decision making and learning problems in two settings: model identification (identifying the correct out of two given models) and model generation. We found that the LLM was near perfect in model identification, especially in instances where the underlying cognitive models made identifiably different predictions.Python programs inferred by the LLM were very close to the data generating function in simulations, or at least produced models that fitted equally well. Even for noisy human data \cite{chambon2020information} from a bandit task, the LLM generated a model that performed on par with the best model proposed in the literature.

%Eric's proposal:
%Should truly capture the results and be exciting
We evaluated the capabilities of Llama-3.1 70b Instruct across two core cognitive modeling tasks: model identification (discerning the correct model among candidates) and model generation (inferring models from behavioral data). Our results indicate that the LLM performed near-perfectly in model identification, especially in cases where competing cognitive models made distinct predictions. In model generation, the inferred programs closely approximated the true data-generating functions in simulation studies and produced models that fitted equally well to human data. When applied to noisy empirical data from a bandit task, the LLM-generated model performed on par with the best models in the literature.


%Current Advantages
%Note that all of the current results were obtained purely in-context and without fine-tuning the LLM. With our proposed method, researchers  would only have to specify a description of the task and the data in a text-based format. This could greatly accelerate the workflow for getting from data to cognitive models, allowing cognitive scientists to explore many more possibilities. Similar to \citet{li2024automated}, the LLM only functions as a proposal engine for the functional form of the model. The critique is provided through model fitting using an optimizer, directly evaluating the candidate models on the data. Hence, we are suggesting a hybrid optimization loop, where the LLM and classical model optimization cooperate (compare Fig.\ \ref{llm-prompt}).

%Eric's proposal:
%Basically just made this easier to parse.
A key advantage of our approach is that all results were obtained purely in-context and without any fine-tuning. This drastically reduces the barrier to entry: researchers need only specify a textual description of their task and data, and the LLM autonomously proposes candidate models. This has the potential to accelerate the transition from data to theory, empowering cognitive scientists to explore a broader hypothesis space more efficiently. Further, our approach implements a hybrid optimization loop, where the LLM generates candidate models and traditional optimization methods fit them to the data. This ensures that model selection remains data-driven, with the LLM acting as a proposal engine rather than an unverified model generator.

%Current Limitations
%However, there are some limitations to the current approach. First, LLMs are heavily biased towards their training data, hence preferring existing models from the cognitive modeling literature over truly new models. We have yet to see how freely LLMs can combine their knowledge from the vast amount of modeling literature to form more creative solutions. Second, we tested rather simple decision-making and learning models (like the Rescorla-Wagner model). However, more complex tasks, also from other domains of cognition, may pose harder challenges to LLM model discovery, and for other tasks, such as visual paradigms, model search might even be impossible to automate. Lastly, successful use of LLMs is highly dependent on prompt engineering. To improve reasoning about code, we had to use techniques like Chain of Thought prompting. For code generation, it helped to explicitly instruct the LLM to provide bug-free code. The LLM might be sensitive to the wording of the task description and the formatting of the data. A human is still needed to check if the generated code makes sense and does not cheat (e.g.\ by peeking at data from future choices).

%Eric's proposal:
Despite its promise, our approach has some limitations. LLMs are trained on vast amounts of prior research and therefore will tend to favor well-established models over genuinely novel ones. Understanding whether they can synthesize new cognitive models by recombining existing knowledge remains an open question. Furthermore, our experiments focused on relatively simple models, such as those used in reinforcement learning and decision making. Applying our framework to higher-dimensional cognitive models, such as those in vision or language, would present significant challenges. Finally, effective model discovery depends heavily on careful prompt engineering. We found that techniques like Chain of Thought reasoning significantly improved performance. However, LLMs remain susceptible to prompt formulation and formatting issues, meaning human oversight is still necessary to validate generated models and ensure they do not exploit artifacts in the data.

%In future work, we will expand the framework to other cognitive domains, and test how other LLMs fare at the task.
%We hope to obtain further improvements by fine-tuning the LLM on large-scale human (or simulated) data, such as \textsc{Psych-101} \cite{binz2024centaur}, paired with the current cognitive model that best explains it. 
%Eric'x proposal for future work.
To address these challenges, future work will explore expanding to broader cognitive domains by testing LLM-driven model discovery beyond learning and decision making, such as in perception, memory, and language comprehension. Fine-tuning LLMs on cognitive modeling tasks could improve their ability to infer scientifically meaningful models, particularly when trained on large-scale cognitive datasets such as \textsc{Psych-101} \cite{binz2024centaur}. Finally, automating the full modeling pipeline by integrating multiple specialized LLMs \cite{lu2024ai} for model generation, evaluation, and refinement could create a fully automated, domain-agnostic cognitive modeling framework.


%Finally, we intend to automate both model proposal and evaluation by bringing together multiple LLMs, each specialized for a given task \cite{lu2024ai}. Our vision for this effort is to release a toolbox for cognitive model discovery that is domain-agnostic, easy to use, and fast with high throughput. This work is the first step towards the goal of automating cognitive modeling. 

Our findings suggest that LLMs have the potential to significantly advance cognitive modeling by democratizing access to complex model discovery and accelerating the pace of research. While human oversight remains essential, this work represents an important first step towards automating scientific discovery in cognitive science.

% \begin{itemize}

%     \item summarize results
%     \item outline long term goals -- e.g. toolbox that includes plugging in data and task description, and getting a few suggested models as output
%     \item finetuning 
%     \item test on more cognitive models 
%     \item other limitations of this approach - heavily based to models in literature
%     \item we might want to mention testing this with other LLMs (e.g. non llama) - and it will likely come up in revisions 
% \end{itemize}









% \bibliographystyle{unsrtnat}  % NeurIPS-style (sorted by citation order)
\bibliographystyle{apalike}
\bibliography{example_paper}

\newpage

\begin{appendices}

% \appendix
% \onecolumn

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\definecolor{ceruleanblue}{rgb}{0.16, 0.32, 0.75}
\section{Decision making}

\vspace{-10pt}
\subsection{Model identification}
\begin{custombox_decision_making_identification}
% \begin{tcolorbox}[colback=schemaBlue,colframe=ceruleanblue, width=\textwidth, left=4pt,right=4pt, top=4pt, bottom=4pt, title=\textbf{Heuristics case: match data to the code prompt} 

\label{prompt:code_match_heuristic}
In a decision-making task, a decision-maker was presented with two options (A and B) with three features each, and was tasked to choose one of the options. You are provided with the values of the features of the two options and the decision made by the decision-maker.\\ 

You are also provided with 2 template Python functions that represent 2 different decision-making strategies.\\ 

Here is the template Python function for decision strategy 1:\\

\begin{verbatim}
def strategy1(A,B):

    if A[1] > B[1]:
        return 'A'
    elif A[1] < B[1]:
        return 'B'
    else:
        return "N/A"

\end{verbatim}


Here is the template Python function for decision strategy 2:\\

\begin{verbatim}
def strategy2(A,B):

    tally1 = 0
    tally2 = 0

    for feature1, feature2 in zip(A, B):
        if feature1 > feature2:
            tally1 += 1
        elif feature2 > feature1:
            tally2 += 1

    if tally1 > tally2:
        return 'A'
    elif tally2 > tally1:
        return 'B'
    else:
        return "N/A"

\end{verbatim}


The feature values of option A are [F1, F2, F3] and the feature values of option B are [F1,F2,F3].\\

The decision-maker chose option [choice]. Which of the 2 template Python functions would produce the same choice as the decision-maker?\\

\end{custombox_decision_making_identification}



\subsection{Model generation}

\begin{custombox_decision_making_generation}
% \begin{tcolorbox}[colback=schemaBlue,colframe=ceruleanblue, width=\textwidth, left=4pt,right=4pt, top=4pt, bottom=4pt, title=\textbf{Heuristics case: write the cognitive model  prompt} 

\label{prompt:code_generate_heuristic}


In a decision-making task, a decision-maker was presented with two options (A and B) with three features each and was tasked to choose one of the options. \\


You are provided with the values of the features of the two options and the decision made by the decision-maker for each task. \\


In each task, your goal is to write a Python function that explains the decision-making process, producing the same choice as the decision-maker. \\


Q: The feature values of option A are [A feature values] and the feature values of option B are [B feature values].\\


The decision-maker chose option [decision].\\

The function should be called heuristic, take the test decision task options A and B as 2 lists containing respective feature values as arguments, and return the choice (A or B, as a string) based on the provided options.
A: Python function that explains the decision-making process:"


\end{custombox_decision_making_generation}


\subsection{Noisy data model generation results}

\begin{custombox_decision_making_noisy_data}

% \begin{tcolorbox}[colback=schemaBlue,colframe=ceruleanblue, width=\textwidth, left=4pt,right=4pt, top=4pt, bottom=4pt, title=\textbf{Examples of LLM-generated heuristics based on the noisy Take the Best and Tallying data} 
\label{result:code_generate_noisy}

Take the Best:
\begin{verbatim}
def heuristic(option_a, option_b):
  
    
    # Calculate the difference in the second feature
    diff_second_feature = abs(option_a[1] - option_b[1])
    
    # If the difference in the second feature is greater 
    # than or equal to 10, 
    # return the option with the higher value in the second feature
    if diff_second_feature >= 10:
        if option_a[1] > option_b[1]:
            return 'A'
        else:
            return 'B'
    # Otherwise, return the option with the higher value in the
    #first feature
    else:
        if option_a[0] > option_b[0]:
            return 'A'
        else:
            return 'B'

\end{verbatim}

Tallying:

\begin{verbatim}
def heuristic(option_a, option_b):

    max_a = max(option_a)
    max_b = max(option_b)

    if max_a > max_b:
        return 'A'
    elif max_b > max_a:
        return 'B'
    else:
        # In case of a tie, we can use a secondary heuristic, 
        # such as the sum of the feature values.
        sum_a = sum(option_a)
        sum_b = sum(option_b)

        if sum_a > sum_b:
            return 'A'
        elif sum_b > sum_a:
            return 'B'
        else:
            # If both the maximum value and the sum are tied, 
            # we can return either 'A' or 'B'. For simplicity, 
            # we'll return 'A' in this case.
            return 'A'

\end{verbatim}


\end{custombox_decision_making_noisy_data}

\section{Learning}
\subsection{Model identification}

\begin{custombox}
\label{prompt:model_identification_learning}
In a 2-armed bandit task, a participant chooses between two actions.
One of the actions yields rewards with a higher probability. This is fixed throughout the task, therefore the probability of reward associated with each action does not change.
The participant's action selection is followed by an outcome: 1 if a reward is received, 0 if not.\\

Here is the Python function for one model of learning/decision-making in the bandit task:

\begin{verbatim}
def Model1(actions, rewards, parameters):
    '''
    inputs:
        parameters: list of model parameters
            alpha = learning rate for positive outcomes
            theta = decision temperature
            alpha_neg = learning rate for negative outcomes
        actions: list of bandit choices
        reward: list of reward/outcomes

    output:
        log likelihood of data (choices) conditioned on the model 
    '''

    alpha, theta, alpha_neg = parameters
    values = np.array([0.5, 0.5])
    log_likelihood = []

    for t in range(len(actions)):
        pr = scipy.special.softmax(theta * values)[actions[t]]
        log_likelihood.append(pr)

        # update values
        delta = rewards[t] - values[actions[t]]
        if delta > 0:
            values[actions[t]] += (alpha * delta)
        else:
            values[actions[t]] += (alpha_neg * delta)

    return -np.sum(np.log(np.array(log_likelihood)))
\end{verbatim}

Here is the Python function for a different model of learning/decision-making in the bandit task:

\begin{verbatim}
def Model2(actions, rewards, parameters):
    '''
    inputs:
        parameters: list of model parameters
            alpha = learning rate 
            theta = decision temperature
            stick = stickiness for the previous choice
        actions: list of bandit choices
        reward: list of reward/outcomes

    output:
        log likelihood of data (choices) conditioned on the model 
    '''
    alpha, theta, stick = parameters
    values = np.array([0.5, 0.5])
    log_likelihood = []
    for t in range(len(actions)):

        W = values.copy()
        if t > 0:
            W[actions[t - 1]] += stick

        pr = scipy.special.softmax(theta * W)[actions[t]]
        log_likelihood.append(pr)

        # update values
        delta = rewards[t] - values[actions[t]]
        values[actions[t]] += (alpha * delta)

    return -np.sum(np.log(np.array(log_likelihood)))
\end{verbatim}

Here is the data set:\\

Data from participant XX:\\
Trial: 0, action: [$a_{0}$], reward: [$r_{0}$]\\
Trial: 1, action: [$a_{1}$], reward: [$r_{1}$]\\
...

Which of the 2 Python functions best matches the data?

Think step by step, and provide your steps in a list.

Write a Python function called `ModelIdentification` that takes a list of actions and rewards and returns the identity of the modelâ€”either Model1 or Model2â€”as a string, based on which of the two models is a better explanation of the underlying data.

To evaluate the models, simply call `Model1(actions, rewards, parameters)` or `Model2(actions, rewards, parameters)` since Model1 and Model2 functions are already defined. Please do not refer to any functions that are not already defined.
Make sure the code is actually executable and can run without bugs.
Keep your response short.

A:

\end{custombox}


\subsection{Model generation}



\begin{custombox1}
\label{prompt:model_generation_learning}
In a 2-armed bandit task, a participant chooses between two actions.
The participant's action selection is followed by an outcome: 1 if a reward is received, 0 if not. One of the actions yields rewards with a higher probability. This is fixed throughout the task, therefore the probability of a reward associated with each action does not change.

Q: Here is a task data set from several participants:

Data from participant XX:\\
Trial: 0, action: [$a_{0}$], reward: [$r_{0}$]\\
Trial: 1, action: [$a_{1}$], reward: [$r_{1}$]\\
...

Your task is to propose 3 unique cognitive models that could explain the observed behaviors in this data set.


When generating the models think in steps - for example: if on trial t participant chose a specific action and observed a given feedback, what is their subsequent action choice? 

Ensure your models have distinct assumptions and parameter sets. Avoid repeating ideas used in previous iterations.

Make sure all of the model parameters are actually being used.
Each model should be implemented as a Python function called cognitive\_model1, cognitive\_model2, and cognitive\_model3.

Each of the 3 functions should accept the following lists as arguments: actions, rewards, and model parameters. 

Each function should return the log likelihood of observed actions given its parameters.


When you write the function, also include the description of each parameter and what it does in the function's meta commented section.

Note that for each parameter except the inverse temperature, the plausible bounds are between 0 and 1. Make sure the equations do not lead to nonsense values (e.g. watch out for division by 0).

Make sure you write functions that are free of bugs, and can be executed.

Here is an initial model guess of how participants solve the task:



\begin{verbatim}
def Model(actions, rewards, parameters):
    '''
    inputs:
        parameters: list of model parameters
            alpha = learning rate 
            theta = decision temperature
        actions: list of bandit choices
        reward: list of reward/outcomes

    output:
        log likelihood of data (choices) conditioned on the model 
    '''
    alpha, theta = parameters
    values = np.array([0.5, 0.5])
    log_likelihood = []
    for t in range(len(actions)):

        pr = scipy.special.softmax(theta * values)[actions[t]]
        log_likelihood.append(pr)

        # update values
        delta = rewards[t] - values[actions[t]]
        values[actions[t]] += (alpha * delta)

    return -np.sum(np.log(np.array(log_likelihood)))
\end{verbatim}

Think about how you might modify this model such that it better explains the data.
Your functions:

\end{custombox1}

\subsection{Feedback provided to the LLM during sampling runs}

\begin{tcolorbox}[colback=pink!20,colframe=pink!80!red, width=\textwidth, left=4pt,right=4pt, top=4pt, bottom=4pt, coltitle=black, title=\textbf{Feedback provided to the LLM at the end of each sampling run} \label{prompt:llm-feedback-learning}]
% \scriptsize % Reduce font size for the box content

Run 1: best model you generated so far is:\\

[Model with the smallest BIC so far]\\
                       
These are parameter configurations you have tried so far, try coming up with something new (maybe try to think of a parameter/mechanism you haven't used thus far):\\

learning\_rate, decay\_rate, softmax\\
learning\_rate, epsilon, softmax\\
learning\_rate, perseveration, softmax


\end{tcolorbox}






\subsection{Model simulation details\label{prompt:simulation_details}}
% \label{prompt:}

We simulated data from candidate learning models to create synthetic data sets we can test our pipeline on. The model parameters were chosen randomly for each agent from a plausible value range. Specifically, the learning rates were sampled uniformly, \( \alpha^{+/-} \sim \mathcal{U}(0,1) \). Stickiness (\(\kappa\)) was also uniformly sampled from this range. The inverse temperature in the softmax equation has a more arbitrary range, and we set it between 0 and 10, i.e., \( \beta \sim \mathcal{U}(0,10) \).


\subsection{LLM-generated model identification function in the learning experiment}

\begin{tcolorbox}[colback=pink!20,colframe=pink!80!red, width=\textwidth, left=4pt,right=4pt, top=4pt, bottom=4pt, coltitle=black, title=\textbf{LLM-generated ModelIdentification function} \label{result:model-identification-function}]
% \scriptsize % Reduce font size for the box content

\begin{verbatim}
import numpy
from scipy.optimize import differential_evolution

def ModelIdentification(a, r):
    bounds_Model1 = [(0,1),(0,10),(0,1)]
    bounds_Model2 = [(0,1),(0,10),(0,1)]
    
    def neg_log_likelihood_Model1(params):
        return Model1(a, r, params)
    def neg_log_likelihood_Model2(params):
        return Model2(a, r, params)

    res_Model1 = differential_evolution(
        neg_log_likelihood_Model1,
        bounds_Model1)
    res_Model2 = differential_evolution(
        neg_log_likelihood_Model2,
        bounds_Model2)
    if res_Model1.fun < res_Model2.fun:
      return "Model1"
    else:
      return "Model2"
\end{verbatim}

\end{tcolorbox}

\subsection{LLM-generated cognitive model for the data simulated from \texorpdfstring{$RW + \alpha^\pm$}{RW plus alpha pm}}


\begin{tcolorbox}[colback=pink!20,colframe=pink!80!red, width=\textwidth, left=4pt,right=4pt, top=4pt, bottom=4pt, coltitle=black, title=\textbf{LLM-generated model for $RW + \alpha^\pm$ data} \label{result: rw + 2LR LLM generated model}]
\scriptsize % Reduce font size for the box content

\begin{verbatim}
def cognitive_model2(actions, rewards, parameters):
    '''
    Model 2: Q-learning with asymmetric learning rates
    Parameters:
        learning_rate_pos (float): rate at which values are updated for positive rewards
        learning_rate_neg (float): rate at which values are updated for negative rewards
        inverse_temperature (float): inverse temperature parameter for softmax
    Description:
        This model extends the basic Q-learning model by using different learning rates for 
        positive and negative rewards.
    '''
    learning_rate_pos, learning_rate_neg, inverse_temperature = parameters
    values = np.array([0.5, 0.5])
    log_likelihood = []
    for t in range(len(actions)):

        p0 = np.exp(inverse_temperature*(values-values[actions[t]]))
        p = 1/(p0[0]+p0[1])

        log_likelihood.append(p)

        if rewards[t] > 0:
            delta = rewards[t] - values[actions[t]]
            values[actions[t]] = values[actions[t]] + (learning_rate_pos * delta)
        else:
            delta = rewards[t] - values[actions[t]]
            values[actions[t]] = values[actions[t]] + (learning_rate_neg * delta)
    return -np.sum(np.log(np.array(log_likelihood)))


\end{verbatim}

\end{tcolorbox}

\subsection{LLM-generated cognitive model for the data simulated from \texorpdfstring{$RW + \kappa$}{RW plus kappa}}

\begin{tcolorbox}[colback=pink!20,colframe=pink!80!red, width=\textwidth, left=4pt,right=4pt, top=4pt, bottom=4pt, coltitle=black, title=\textbf{LLM-generated model for $RW + \kappa$ data} \label{result: rw + stickiness LLM generated model}]
\scriptsize % Reduce font size for the box content

\begin{verbatim}
def cognitive_model2(actions, rewards, parameters):
    '''
    The function implements a cognitive model of a bandit task with a decay mechanism for unchosen actions
    inputs:
        actions: list of participants' selected actions 
        rewards: list of observed rewards
        parameters: list of model parameters
            - learning_rate: learning rate for updating values
            - decay_rate: rate at which unchosen actions decay
            - inverse_temperature: inverse temperature parameter for softmax choice rule
    output:
        log_likelihood: negative log likelihood of actions based on the model configuration
    '''

    learning_rate, decay_rate, inverse_temperature = parameters
    p_choice = [] 
    num_action = 2
    values = np.array([1/num_action, 1/num_action])
    for t in range(len(actions)):
        
        p0 = np.exp(inverse_temperature*(values-values[actions[t]]))
        p = 1/(p0[0]+p0[1])
        p_choice.append(p)

        delta = rewards[t] - values[actions[t]]
        values[actions[t]] += (learning_rate * delta)
        values[1-actions[t]] *= (1 - decay_rate)
    return -np.sum(np.array(np.log(p_choice)))


\end{verbatim}

\end{tcolorbox}
\vspace{-5mm}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.6\textwidth, keepaspectratio]{PPC_cumulative.pdf}
    \caption{Posterior predictive checks based on the LLM-generated model for $RW + \kappa$ data. We simulated the model based on LLM-generated model equations using the agent-specific best parameter estimates. We show that the data simulated from the LLM-generated model captures the rate of reward accumulation in the ground truth.}
    \label{result:llm_ppc_sticky_model}
\end{figure*}

\section{Human data}

\vspace{-8pt}

\subsection{Task details}

\label{human_data:task_details}
In \citealt{chambon2020information} study the action reward probabilities varied across blocks and were sampled from high-probability reward values (0.9, 0.6) or low-probability reward values (0.4, 0.1). The rewards in this task were binary, with ($r \in \{-1, 1\}$). 

Additional blocks included the forced choice condition where actions were not voluntarily chosen by the participants. We focused only on the free choice condition.s



\subsection{LLM-generated model for human data with the winning model provided as a template (partial feedback condition)}
\begin{tcolorbox}[colback=pink!20,colframe=pink!80!red, width=\textwidth, left=4pt,right=4pt, top=4pt, bottom=4pt, coltitle=black, title=\textbf{LLM-generated model for human data with winning model provided as a template (partial feedback condition)} \label{result:model_generation_human_partial_best_model_template}]
\scriptsize % Reduce font size for the box content

\begin{verbatim}

def cognitive_model3(actions, rewards, parameters):
    learning_rate, inverse_temperature, loss_aversion = parameters
    values = np.array([0.5, 0.5])
    log_likelihood = []
    for t in range(len(actions)):
        # compute choice probabilities
        p0 = np.exp(inverse_temperature*(values-values[actions[t]]))
        p = p0 / np.sum(p0)
        # compute choice probability for actual choice
        log_likelihood.append(p[actions[t]])
        # update values
        if rewards[t] > 0:
            delta = rewards[t] - values[actions[t]]
            values[actions[t]] = values[actions[t]] + (learning_rate * delta)
        else:
            delta = loss_aversion * (rewards[t] - values[actions[t]])
            values[actions[t]] = values[actions[t]] + (learning_rate * delta)
    return -np.sum(np.log(np.array(log_likelihood)))

\end{verbatim}
\end{tcolorbox}



\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.75\textwidth, keepaspectratio]{best-template-partial-feedback.pdf}
    \caption{Model comparison of LLM-generated model based on the human data (partial feedback condition) when the winning model is provided as a template.}
    \label{result:llm_model_comparison_best_template}
\end{figure*}

\subsection{Alternative LLM-generated model for human data(full feedback condition)}

\begin{tcolorbox}[colback=pink!20,colframe=pink!80!red, width=\textwidth, left=4pt,right=4pt, top=4pt, bottom=4pt, coltitle=black, title=\textbf{Alternative LLM-generated model for human data (full feedback condition)} \label{result:model_generation_human_full_feedback_alternative}]
\scriptsize % Reduce font size for the box content

\begin{verbatim}

def cognitive_model3(actions, rewards, rewards_non_chosen, parameters):
    learning_rate_chosen, learning_rate_unchosen, inverse_temperature, perseveration_rate = parameters
    values = np.array([0.5, 0.5])
    log_likelihood = []
    for t in range(len(actions)):
        # compute choice probabilities for k=2, do not modify the policy
        p0 = np.exp(inverse_temperature*(values-values[actions[t]]))
        p = 1/(p0[0]+p0[1])
        # compute choice probability for actual choice
        log_likelihood.append(p)
        # update values
        delta = rewards[t] - values[actions[t]]
        unchosen_action = 1-actions[t]
        delta_unchosen = rewards_non_chosen[t] - values[unchosen_action]
        values[actions[t]] = values[actions[t]] + (learning_rate_chosen * delta)
        values[unchosen_action] = values[unchosen_action] + (learning_rate_unchosen * delta_unchosen)
        # update values based on perseveration rate
        if t > 0 and actions[t] == actions[t-1]:
            values[actions[t]] = values[actions[t]] + (perseveration_rate * rewards[t])
    return -np.sum(np.log(np.array(log_likelihood))) (edited) 


\end{verbatim}
\end{tcolorbox}


\section{Text parsers}

% \begin{tcolorbox}[colback=mBlue!5!white,colframe=mBlue!75!black, width=\textwidth, left=4pt,right=4pt, top=4pt, bottom=4pt, coltitle=white, title=\textbf{Function extraction parser} 

\begin{custombox_parser_function}
\label{parser:function}
% \scriptsize % Reduce font size for the box content

\begin{verbatim}
def extract_full_function(text, func_name):

    pattern = re.compile(
        r'(def ' + func_name + r'\(.*?\):.*?return.*?)(?=\n\S|\n$)',
        re.DOTALL
    )

    match = pattern.search(text)
    if match:
        return match.group(1)
    return None

\end{verbatim}
\end{custombox_parser_function}





% \begin{tcolorbox}[colback=mBlue!5!white,colframe=mBlue!75!black, width=\textwidth, left=4pt,right=4pt, top=4pt, bottom=4pt, coltitle=white, title=\textbf{Parameter name  parser} 
\begin{custombox_parser_parameter_list}
\label{parser:parameter_name}
\scriptsize % Reduce font size for the box content

\begin{verbatim}
def extract_parameters(text):
    """
    Extracts parameter names from the unpacking line in the function where `parameters` are unpacked.

    :param text: String containing the Python code for the model
    :return: List of parameter names
    """
    # Regular expression to find unpacking of parameters
    pattern = re.compile(r'([a-zA-Z_]\w*(?:\s*,\s*[a-zA-Z_]\w*)*)\s*=\s*parameters')
    match = pattern.search(text)
    if match:
        # Extract and clean up parameter names, split by commas
        parameters = [param.strip() for param in match.group(1).split(',')]
        return parameters
    return []

\end{verbatim}
 
\end{custombox_parser_parameter_list}
\end{appendices}
\end{document}