\section{Related work}
\paragraph{Cognitive modeling.}
The aim of modeling in cognitive science is to improve our understanding of cognitive processes. A model usually manifests in precise mathematical formulations of how behavioral data was generated. There are many ways in which a cognitive model serves understanding of cognitive processes, including, but not limited to, providing mechanistic explanations of cognition, enabling predictions or allowing for the evaluation of competing hypotheses  **Anderson & Lebiere, "The Atomic Components of Thought"**. In this work, we chose two classic domains in cognitive science, decision making and learning, to test whether cognitive modeling can be catalyzed by the use of LLMs. Specifically, we focused on heuristic-based decision making and reward-based learning strategies **Sutton & Barto, "Reinforcement Learning: An Introduction"**.

\paragraph{Automated model discovery with LLMs.}
The goal of automating model discovery, given some data, is not new to science. Automation promises to accelerate and democratize scientific discovery by making it independent of a researcher's prior knowledge and training. However, until recent successes of LLMs, automated hypothesis search was mostly confined to designing models in a domain-specific language and handcrafting search algorithms to identify the best-fitting model from the space of pre-defined models **Bello et al., "Neural Architectural Search with Reinforcement Learning"**. 

Recently, it has been demonstrated that these limitations can be overcome with the use of LLMs. Researchers have successfully used LLMs to automate discovery of statistical models **Bengio et al., "Deep Learning"**, solve classical machine learning problems in regression and image classification **Goodfellow et al., "Deep Learning"**, suggest niche rules that are not widely recognized but are scientifically sound in chemistry **Segler et al., "Planning chemical syntheses with a programmable molecular robot"** and even automate the entire scientific pipeline in the field of machine learning -- from creating ideas to designing appropriate experiments, conducting them, writing the paper and simulating the review process **Chen et al., "Automating Scientific Research with Artificial Intelligence"**.

\paragraph{Code writing abilities of LLMs.}
The merit of LLMs in automating model search stems not only from their domain-general knowledge, but also from their ability to process and generate natural language text and synthesize code from natural language instructions. We believe that using LLMs to synthesize code in a general-purpose language like Python paves the way for overcoming the limitations of handcrafting domain-specific languages to automate model search **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**.
Notable work in this area has already shown the proficiency of LLMs in providing code to solve math and classic Python problems **Henderson & Zhang, "Deep Transfer Learning with Attention-based Neural Machine Translation"**. Furthermore, **Raghavan et al., "Automating Scientific Research with Artificial Intelligence"** have demonstrated encouraging results with respect to the ability of LLMs to output mathematical functions and even probabilistic Python programs that model input data.