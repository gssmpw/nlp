\section{Methodology}
\label{sec: data processing}
In this section, we theoretically analyze how distance comparisons in \method{HNSW} construction can be effectively executed using compact vector codes. Based on this, we integrate three common vector compression methods—Product Quantization (PQ) \cite{PQ}, Scalar Quantization (SQ) \cite{LVQ}, and Principal Component Analysis (PCA) \cite{PCA}—into \method{HNSW}. While many variants of these methods exist \cite{OPQ, RaBitQ, WangLKC16, Distill-VQ, ZhangDW14, HeNB21, ScaNN, DeltaPQ}, we exclude them due to their complexity compared to the original methods, which complicates deployment in HNSW. We focus on these three methods for their lightweight nature and alignment with our goal of accelerating index construction. Drawing from the integration of these methods in HNSW construction, we design a novel compact coding strategy and optimize the memory layout for HNSW construction, enhancing memory access efficiency and SIMD operations.

\subsection{Theoretical Analysis}
Recall that distance computation during HNSW construction is a major bottleneck, with one distance value primarily compared against another. We denote distance comparisons for updating the candidate set as DC1, and for selecting final neighbors as DC2. Here, we demonstrate that DC1 and DC2 can be unified into a generalized framework and analyze how these comparisons can be effectively performed using compact vector codes.

\begin{myLemma}
\label{lemma: dist comp}
Given any three vertices $\boldsymbol{u}$, $\boldsymbol{v}$, and $\boldsymbol{w}$ in $D$-dimensional Euclidean space $\mathbb{R}^D$, the comparison between $\delta(\boldsymbol{u}, \boldsymbol{v})$ and $\delta(\boldsymbol{u}, \boldsymbol{w})$ is:

\noindent $\bullet$ $\delta(\boldsymbol{u}, \boldsymbol{v}) < \delta(\boldsymbol{u}, \boldsymbol{w})$ if and only if $\boldsymbol{e} \cdot \boldsymbol{u} - b < 0$;

\noindent $\bullet$ $\delta(\boldsymbol{u}, \boldsymbol{v}) > \delta(\boldsymbol{u}, \boldsymbol{w})$ if and only if $\boldsymbol{e} \cdot \boldsymbol{u} - b > 0$;

\noindent $\bullet$ $\delta(\boldsymbol{u}, \boldsymbol{v}) = \delta(\boldsymbol{u}, \boldsymbol{w})$ if and only if $\boldsymbol{e} \cdot \boldsymbol{u} - b = 0$;

\noindent where $\boldsymbol{e} \cdot \boldsymbol{u} - b = 0$ defines the perpendicular bisector hyperplane between $\boldsymbol{v}$ and $\boldsymbol{w}$, with $\boldsymbol{e}=\boldsymbol{w} - \boldsymbol{v}$ and $b = \frac{\|\boldsymbol{w}\|^2 - \|\boldsymbol{v}\|^2}{2}$.
\end{myLemma}

\begin{proof}
To show $\delta(\boldsymbol{u}, \boldsymbol{v}) = \delta(\boldsymbol{u}, \boldsymbol{w})$, we square both sides to eliminate square roots, giving $\|\boldsymbol{u} - \boldsymbol{v} \|^2 = \| \boldsymbol{u} - \boldsymbol{w} \|^2$. Expanding and simplifying, we get $2\boldsymbol{u} \cdot (\boldsymbol{w} - \boldsymbol{v}) = \|\boldsymbol{w}\|^2 - \|\boldsymbol{v}\|^2$.
Defining $\boldsymbol{e} = \boldsymbol{w} - \boldsymbol{v}$ and $b = \frac{\|\boldsymbol{w}\|^2 - \|\boldsymbol{v}\|^2}{2}$, this simplifies to $\boldsymbol{e} \cdot \boldsymbol{u} = b$, the equation of a hyperplane in $\mathbb{R}^D$. For $\boldsymbol{u}$ to lie on this hyperplane, it must satisfy $\boldsymbol{e} \cdot \boldsymbol{u} - b = 0$. Similarly, for \(\delta(\boldsymbol{u}, \boldsymbol{v}) > \delta(\boldsymbol{u}, \boldsymbol{w})\), squaring both sides gives $\|\boldsymbol{u} - \boldsymbol{v}\|^2 > \|\boldsymbol{u} - \boldsymbol{w}\|^2$, leading to $2\boldsymbol{u} \cdot (\boldsymbol{w} - \boldsymbol{v}) > \|\boldsymbol{w}\|^2 - \|\boldsymbol{v}\|^2$, and thus $\boldsymbol{e} \cdot \boldsymbol{u} - b > 0$. For \(\delta(\boldsymbol{u}, \boldsymbol{v}) < \delta(\boldsymbol{u}, \boldsymbol{w})\), a similar process gives $\boldsymbol{e} \cdot \boldsymbol{u} - b < 0$.
\end{proof}

In DC1, with $\boldsymbol{u}$ as the newly inserted vector and $\boldsymbol{v}$ as the farthest vertex from $\boldsymbol{u}$ in the candidate set $\boldsymbol{C(u)}$, we update $\boldsymbol{C(u)}$ by including $\boldsymbol{w}$ if $\boldsymbol{e} \cdot \boldsymbol{u} - b > 0$. In DC2, with $\boldsymbol{v}$ as the newly inserted vector and $\boldsymbol{w}$ as a selected neighbor in $\boldsymbol{N(v)}$, we exclude a candidate $\boldsymbol{u}$ from consideration as a neighbor if $\boldsymbol{e} \cdot \boldsymbol{u} - b > 0$. Using $\boldsymbol{e} \cdot \boldsymbol{u} - b > 0$ as a criterion, both DC1 and DC2 can be correctly executed.

\begin{myTheorem}[\cite{LVQ}]
\label{the: Dist. Comp.}
{
For three vertices $\boldsymbol{u}$, $\boldsymbol{v}$, and $\boldsymbol{w}$ in $D$-dimensional Euclidean space $\mathbb{R}^D$, with compact vector codes $\boldsymbol{u}^{\prime}$, $\boldsymbol{v}^{\prime}$, and $\boldsymbol{w}^{\prime}$, the condition $\delta(\boldsymbol{u}, \boldsymbol{v}) > \delta(\boldsymbol{u}, \boldsymbol{w})$ is equivalent to $\delta(\boldsymbol{u}^{\prime}, \boldsymbol{v}^{\prime}) > \delta(\boldsymbol{u}^{\prime}, \boldsymbol{w}^{\prime})$ when $|\boldsymbol{e} \cdot \boldsymbol{u} - b|\geq |E|$. The quantity $E$ is defined as
\begin{equation}
\begin{aligned}
    E = &(E_{\boldsymbol{w}}-E_{\boldsymbol{v}}) \cdot \boldsymbol{u} + (\boldsymbol{w}-\boldsymbol{v})\cdot E_{\boldsymbol{u}} + E_{\boldsymbol{v}} \cdot E_{\boldsymbol{u}} - E_{\boldsymbol{w}}\cdot E_{\boldsymbol{u}}\\
    &+ \frac{1}{2}||E_{\boldsymbol{w}}||^2-\frac{1}{2}||E_{\boldsymbol{v}}||^2 +\boldsymbol{v}\cdot E_{\boldsymbol{v}}-\boldsymbol{w}\cdot E_{\boldsymbol{w}}
\end{aligned}
\end{equation}
where $E_{\boldsymbol{u}}$, $E_{\boldsymbol{v}}$, and $E_{\boldsymbol{w}}$ are the error vectors corresponding to $\boldsymbol{u}$, $\boldsymbol{v}$, and $\boldsymbol{w}$, respectively.
}

\end{myTheorem}

\begin{proof}
    Based on Lemma \ref{lemma: dist comp}, the condition $\delta(\boldsymbol{u}^{\prime}, \boldsymbol{v}^{\prime}) > \delta(\boldsymbol{u}^{\prime}, \boldsymbol{w}^{\prime})$ is equivalent to $\boldsymbol{e}^{\prime} \cdot \boldsymbol{u}^{\prime} - b^{\prime} > 0$, where $\boldsymbol{e}^{\prime}=\boldsymbol{w}^{\prime}-\boldsymbol{v}^{\prime}$ and $b^{\prime}=\frac{\|\boldsymbol{w}^{\prime}\|^2 - \|\boldsymbol{v}^{\prime}\|^2}{2}$. To demonstrate that $\delta(\boldsymbol{u}, \boldsymbol{v}) > \delta(\boldsymbol{u}, \boldsymbol{w})$ is equivalent to $\delta(\boldsymbol{u}^{\prime}, \boldsymbol{v}^{\prime}) > \delta(\boldsymbol{u}^{\prime}, \boldsymbol{w}^{\prime})$, it suffices to prove that $\boldsymbol{e} \cdot \boldsymbol{u} - b$ and $\boldsymbol{e}^{\prime} \cdot \boldsymbol{u}^{\prime} - b^{\prime}$ share the same sign.
Substituting $\boldsymbol{e}^{\prime}$ and $b^{\prime}$ into $\boldsymbol{e}^{\prime} \cdot \boldsymbol{u}^{\prime} - b^{\prime}$, we obtain:
\begin{equation}
\begin{aligned}
    \boldsymbol{e}^{\prime} \cdot \boldsymbol{u}^{\prime} - b^{\prime} = (\boldsymbol{w}^{\prime}-\boldsymbol{v}^{\prime}) \cdot \boldsymbol{u}^{\prime} - \frac{\|\boldsymbol{w}^{\prime}\|^2 - \|\boldsymbol{v}^{\prime}\|^2}{2}
\end{aligned}
\end{equation}
For vector $\boldsymbol{u}$, let $E_{\boldsymbol{u}}$ denote the compression error of $\boldsymbol{u}$, defined as
\begin{equation}
\label{equ: error}
    E_{\boldsymbol{u}} = \boldsymbol{u} - \boldsymbol{u}^{\prime}
\end{equation}
Substituting this into the previous equation, we derive:
\begin{equation}
\label{equ: error1}
\begin{aligned}
    \boldsymbol{e}^{\prime} \cdot \boldsymbol{u}^{\prime} - b^{\prime}
    = &((\boldsymbol{w}-E_{\boldsymbol{w}})-(\boldsymbol{v}-E_{\boldsymbol{v}})) \cdot (\boldsymbol{u}-E_{\boldsymbol{u}}) \\
    &- \frac{\|\boldsymbol{w}-E_{\boldsymbol{w}}\|^2 - \|\boldsymbol{v}-E_{\boldsymbol{v}}\|^2}{2}
\end{aligned}
\end{equation}
Expanding and simplifying, we group terms involving compression errors:
\begin{equation}
\begin{aligned}
    E = &(E_{\boldsymbol{w}}-E_{\boldsymbol{v}}) \cdot \boldsymbol{u} + (\boldsymbol{w}-\boldsymbol{v})\cdot E_{\boldsymbol{u}} + E_{\boldsymbol{v}} \cdot E_{\boldsymbol{u}} - E_{\boldsymbol{w}}\cdot E_{\boldsymbol{u}}\\
    &+ \frac{1}{2}||E_{\boldsymbol{w}}||^2-\frac{1}{2}||E_{\boldsymbol{v}}||^2 +\boldsymbol{v}\cdot E_{\boldsymbol{v}}-\boldsymbol{w}\cdot E_{\boldsymbol{w}}
\end{aligned}
\end{equation}
Thus, the equation simplifies to:
\begin{equation}
\label{equ: error2}
\begin{aligned}
    \boldsymbol{e}^{\prime} \cdot \boldsymbol{u}^{\prime} - b^{\prime}
    &= (\boldsymbol{w}-\boldsymbol{v}) \cdot \boldsymbol{u} -\frac{1}{2} (||\boldsymbol{w}||^2-||\boldsymbol{v}||^2) - E \\
    &=\boldsymbol{e}\cdot \boldsymbol{u}-b - E
\end{aligned}
\end{equation}
Therefore, when $|\boldsymbol{e} \cdot \boldsymbol{u} - b|\geq |E|$, the sign of $\boldsymbol{e}^{\prime} \cdot \boldsymbol{u}^{\prime} - b^{\prime}$ matches that of $\boldsymbol{e} \cdot \boldsymbol{u}-b$. Specifically, $\boldsymbol{e}^{\prime} \cdot \boldsymbol{u}^{\prime} - b^{\prime}>0$ if and only if $\boldsymbol{e} \cdot \boldsymbol{u}-b>0$, and similarly, $\boldsymbol{e}^{\prime} \cdot \boldsymbol{u}^{\prime} - b^{\prime}<0$ if and only if $\boldsymbol{e} \cdot \boldsymbol{u}-b<0$.
\end{proof}

Theorem \ref{the: Dist. Comp.} elucidates how distance comparison is preserved despite compression error. By adjusting the parameters of compact coding (thus affecting $E$), the condition $|\boldsymbol{e} \cdot \boldsymbol{u} - b|\geq |E|$ can always be satisfied. Consequently, an appropriate compression error maintains accurate distance comparison while reducing vector data size, enhancing memory access and SIMD operations. Hence, integrating vector compression techniques into the HNSW construction process is a promising avenue for accelerating index construction.
{
Given a vector $\boldsymbol{u}$ and its compact vector code $\boldsymbol{u}^{\prime}$, the error vector is computed as $E_{\boldsymbol{u}} = \boldsymbol{u} - \boldsymbol{u}^{\prime}$. Here, $\boldsymbol{u}^{\prime}$ refers to the vector derived from the compact vector code, not the vector code itself; this derived vector retains the same dimensionality as $\boldsymbol{u}$.
For different compact coding methods, the derived vectors have different interpretations, and thus $E_{\boldsymbol{u}}$ is computed in distinct ways (see Section \ref{subsec: baseline solutions}).
}

{
In a compact coding method, adjustable parameters control the compression error. Specifically, a random sample of vectors (e.g., 10,000) is drawn from the dataset, and each vector's top-100 nearest neighbors are determined, forming a triple $(\boldsymbol{u}$, $\boldsymbol{v}$, $\boldsymbol{w})$ from the vector and its two nearest neighbors. A batch of such triples is then constructed, followed by the generation of compact vector codes $\boldsymbol{u}^{\prime}$, $\boldsymbol{v}^{\prime}$, and $\boldsymbol{w}^{\prime}$ for specific parameters. Next, the quantities $|\boldsymbol{e} \cdot \boldsymbol{u} - b|$ and $|E|$ are computed for each triple. By tuning the parameters, one can maximize the proportion of triples that satisfy $|\boldsymbol{e} \cdot \boldsymbol{u} - b| \geq |E|$ while minimizing the vector size. Finally, the chosen parameters are evaluated by integrating the compact coding method into HNSW.
}

\subsection{Baseline Solutions}
\label{subsec: baseline solutions}

\subsubsection{\textbf{Deploying PQ in HNSW}}
\label{subsubsec: HNSW PQ}
Product Quantization (PQ) \cite{PQ} splits high-dimensional vectors into subvectors that are independently compressed. For each subspace, a codebook of centroids is generated, and during encoding, the ID of the nearest centroid is selected to create a compact representation. The trade-off between distance comparison accuracy and compression error is managed by adjusting the code length ($L_{PQ}$) in each subspace and the number of subspaces ($M_{PQ}$). {For example, with $M_{PQ} = 8$ and $L_{PQ} = 4$, a vector is represented by 8 codewords, each storing a 4-bit centroid ID in the subspace. For a vector $\boldsymbol{u}$ and its compact code $\boldsymbol{u}^{\prime}$, the derived vector is formed by concatenating $\boldsymbol{u}$'s nearest centroid vectors (identified by $\boldsymbol{u}^{\prime}$) from all subspaces. The error vector $E_{\boldsymbol{u}}$ is then computed as the difference between $\boldsymbol{u}$ and this derived vector.}

To incorporate PQ into HNSW, we preprocess high-dimensional vectors to create codebooks. Each inserted vector is encoded using these codebooks, and a distance table is computed between the vector and subspace centroids (Asymmetric Distance Computation, ADC \cite{PQ}). In the Candidate Acquisition (CA) stage, distances to visited vertices are efficiently computed by scanning this table. In the Neighbor Selection (NS) stage, the table cannot compute candidate distances. Thus, centroids are located based on compact PQ codes, and inter-centroid distances represent candidate distances (Symmetric Distance Computation, SDC \cite{PQ}). Theorem \ref{the: Dist. Comp.} suggests that optimal values of $L_{PQ}$ and $M_{PQ}$ balance distance comparison accuracy with construction efficiency. We denote the HNSW constructed via this pipeline as HNSW-PQ, which replaces original vectors with PQ codes and employs ADC and SDC for efficient distance computation during index construction.

We evaluate HNSW-PQ under different $L_{PQ}$ and $M_{PQ}$ configurations, as 
shown in Figure \ref{fig: HNSW-PQ laion1m}. The results indicate that both parameters significantly influence indexing time and search performance. As $L_{PQ}$ grows, more clusters are formed in each subspace, leading to longer codebook generation and vector encoding times, thus increasing indexing time. Notably, indexing time initially decreases before rising with $M_{PQ}$. This occurs because a small $M_{PQ}$ causes high subspace dimensionality, increasing codebook generation time. Conversely, a larger $M_{PQ}$ also raises indexing time due to more subspaces. Regarding index quality, both higher $L_{PQ}$ and $M_{PQ}$ improve recall, aligning with the theoretical analysis that lower compression errors yield more accurate distance comparisons.

\begin{figure}
% \vspace{-0.2cm}
\setlength{\abovecaptionskip}{0cm}
  \setlength{\belowcaptionskip}{0cm}
  \centering
  \footnotesize
  \stackunder[0.5pt]{\includegraphics[scale=0.2]{figures/baselines/baseline_pq_bits_laion1m.pdf}}{(a) LAION-1M ($M_{PQ}=8$)}
  \hspace{0.15cm}
  \stackunder[0.5pt]{\includegraphics[scale=0.2]{figures/baselines/baseline_pq_subspaces_laion1m.pdf}}{(b) LAION-1M ($L_{PQ}=8$)}
  \newline
  \caption{Effect of parameters on HNSW-PQ.}
  \label{fig: HNSW-PQ laion1m}
\end{figure}

\subsubsection{\textbf{Deploying SQ in HNSW}}
\label{subsubsec: HNSW SQ}
Scalar Quantization (SQ) \cite{LVQ,Boufounos12} compresses each dimension of vectors independently. It divides data ranges into intervals, assigning each interval an integer value. Floating-point values within an interval are approximated by that integer value, thus reducing vector size by lowering the required bits ($L_{SQ}$). {For example, with $L_{SQ} = 8$, each dimension is represented by an integer (0$\sim$255). Before distance computation, these integers are converted back into floating-point numbers to form the decoded vector, which is lossy. The error vector $E_{\boldsymbol{u}}$ is obtained by subtracting the decoded vector from $\boldsymbol{u}$.}

To accelerate HNSW construction via SQ, we preprocess high-dimensional vectors to identify the maximum and minimum values per dimension. We calculate the interval for each dimension by subtracting the minimum from the maximum. During index construction, each inserted vector is encoded using these precomputed intervals, producing compact SQ codes for distance computations. As per Theorem \ref{the: Dist. Comp.}, the optimal $L_{SQ}$ balances distance comparison accuracy with construction efficiency. The resulting HNSW, termed HNSW-SQ, replaces original vector data with compact SQ codes, enabling efficient distance calculations.

In Figure \ref{fig: HNSW-SQ and HNSW-PCA laion1m}(a), we assess HNSW-SQ across varying $L_{SQ}$ values. The results show that indexing time first decreases and then increases as $L_{SQ}$ grows. This trend arises from the lack of specialized data types for 2 and 4 bits, necessitating the use of \textit{uint8} type, which reduces operational efficiency. In contrast, 8 bits, while larger, aligns well with \textit{uint8}, achieving an optimal balance between compression error and operational efficiency, thereby reducing indexing time. While 16 bits can be represented with \textit{uint16}, the larger vector size increases indexing time. Notably, search accuracy consistently improves with higher $L_{SQ}$, supporting the theoretical analysis.

\subsubsection{\textbf{Deploying PCA in HNSW}}
\label{subsubsec: HNSW PCA}
Principal Component Analysis (PCA) \cite{PCA} reduces dimensionality by projecting high-dimensional vectors into a lower-dimensional space. {It applies an orthogonal transformation matrix to a vector $\boldsymbol{u}$, preserving its norm while prioritizing significant components in lower dimensions. The compact vector code $\boldsymbol{u}^{\prime}$ is formed by retaining the first $d_{PCA}$ dimensions, substantially reducing storage requirements. The error vector $E_{\boldsymbol{u}}$ is calculated by subtracting the dimension-reduced vector from the transformed vector of $\boldsymbol{u}$, with the dimension-reduced vector zero-padded to match dimensionality.}

To expedite HNSW construction via PCA, we preprocess high-dimensional vectors by deriving an orthogonal projection matrix through eigenvalue decomposition of the covariance matrix. Each inserted vector is then transformed into a low-dimensional space using this matrix, with principal components serving as compact PCA codes. Distance computations are performed directly on these compact representations. Theorem \ref{the: Dist. Comp.} indicates that an optimal $d_{PCA}$ balances distance comparison accuracy and construction efficiency. The resulting HNSW, designated as HNSW-PCA, substitutes the original vectors with compact PCA representations, thereby facilitating more efficient distance computations.

Figure \ref{fig: HNSW-SQ and HNSW-PCA laion1m}(b) shows the evaluation of HNSW-PCA across various $d_{PCA}$ configurations. We find that indexing time increases with higher $d_{PCA}$, while search accuracy improves, aligning with the theoretical analysis. Optimal trade-off between indexing time and search performance occurs with $d_{PCA}$ ranging from 256 to 512. Notably, $d_{PCA}$ reaches 420 when 90\% cumulative variance is achieved on the LAION dataset. In the experiments, $d_{PCA}$ will be set to ensure at least 90\% cumulative variance.

\begin{figure}
% \vspace{-0.2cm}
  \setlength{\abovecaptionskip}{0cm}
  \setlength{\belowcaptionskip}{0cm}
  \centering
  \footnotesize
  \stackunder[0.5pt]{\includegraphics[scale=0.2]{figures/baselines/baseline_sq_bits_laion1m.pdf}}{(a) LAION-1M (HNSW-SQ)}
  \hspace{0.15cm}
  \stackunder[0.5pt]{\includegraphics[scale=0.2]{figures/baselines/baseline_pca_dims_laion1m.pdf}}{(b) LAION-1M (HNSW-PCA)}
  \newline
  \caption{Effect of parameters on HNSW-SQ and HNSW-PCA.}
  \label{fig: HNSW-SQ and HNSW-PCA laion1m}
\end{figure}

\vspace{-0.15cm}
\subsubsection{\textbf{Lessons Learned}}
\label{subsubsec: lessons learned}
Theoretical and empirical analyses reveal that index construction can be accelerated using compact coding techniques, and construction efficiency and index quality can be well balanced by adjusting the compression error. An appropriate compression error reduces vector data size, improving memory access and SIMD operation efficiency while preserving accurate distance comparisons. Following this insight, optimized variants \cite{OPQ, RaBitQ, WangLKC16, Distill-VQ, ZhangDW14, HeNB21, ScaNN, DeltaPQ, Boufounos12} of PQ, SQ, and PCA may be integrated into HNSW to further speed up index construction. Note that these variants must avoid excessive processing overhead to maintain the balance between construction efficiency and index quality. Nonetheless, the core mechanism of these methods—vector size reduction—does not align well with HNSW's construction characteristics on modern CPUs. Regardless of whether HNSW-PQ, HNSW-SQ, or HNSW-PCA is used, the random memory access pattern during index construction remains unchanged. In terms of arithmetic operations, HNSW-PQ’s distance table lookup is unable to fully utilize SIMD operations, as each computation is executed individually \cite{RaBitQ}. While HNSW-SQ and HNSW-PCA reduce register loads by computing directly on compressed vectors, they still process distance computations sequentially, underutilizing SIMD’s advantages \cite{PQfast}. We highlight that current variants of PQ, SQ, and PCA do not fundamentally resolve these limitations.

\subsection{The \texttt{Flash} Method}
\label{subsec: opt method}
In Section \ref{subsec: baseline solutions}, we demonstrate that incorporating existing compact coding methods into HNSW construction can enhance index construction speed. However, these methods either offer limited gains in indexing efficiency or degrade search performance. This is mainly because they are unable to effectively reduce random memory accesses and fully exploit the advantages of SIMD instructions. To overcome these limitations, we propose a speci\underline{\textbf{f}}ic compact coding strategy a\underline{\textbf{l}}ongside memory l\underline{\textbf{a}}yout optimization for the con\underline{\textbf{s}}truction process of \underline{\textbf{H}}NSW, named \texttt{Flash}, which minimizes random memory accesses while maximizing SIMD utilization.


\begin{figure*}
% \vspace{-0.3cm}
  \centering
  \setlength{\abovecaptionskip}{0cm}
  \setlength{\belowcaptionskip}{-0.1cm}
  \includegraphics[width=\linewidth]{figures/illustration/our_compact_code.pdf}
  \caption{Illustrating the coding pipeline and data layout of \texttt{Flash}.}
  \label{fig:compact code}
  \vspace{-0.3cm}
\end{figure*}

\subsubsection{\textbf{Design Overview}}
\label{subsubsec: design overview}
Noting that HNSW-PQ strikes an optimal balance between construction efficiency and search performance, with PQ offering flexibility through two adjustable parameters that affect the compression error. We leverage the core principles of PQ to devise \texttt{Flash}, tailored to the HNSW construction process on modern CPUs. Figure \ref{fig:compact code} provides an overview of the coding pipeline and data layout used by \texttt{Flash}.

Given that a CPU core has multiple SIMD registers, \texttt{Flash} partitions the high-dimensional space into subspaces, enabling parallel processing of subspaces in these registers.
To accommodate the distinct distance computations in Candidate Acquisition (CA) and Neighbor Selection (NS) stages, it introduces Asymmetric Distance Tables (ADT) for the CA stage, residing in SIMD registers, and Symmetric Distance Tables (SDT) for the NS stage, located in cache. It optimizes the bit counts allocated for each codeword (i.e., encoded vector) and applies SQ to compress the precomputed ADT to fit within a SIMD register, thereby reducing register loads. Since high-dimensional vectors often exhibit uneven variances across dimensions, directly encoding original subspaces may result in suboptimal bit utilization. To address this, \texttt{Flash} utilizes PCA to extract the principal components of vectors, generating subspaces based on these components, enhancing bit utilization within the SIMD register's constraints.
When organizing neighbor lists, neighbor IDs are grouped with corresponding codewords to minimize random memory accesses. Rather than sequentially storing all codewords for a neighbor, \texttt{Flash} gathers the codewords of a batch of neighbors within a specific subspace. The batch size matches the number of centroids per subspace. This alignment allows a register load to fetch an entire batch, enabling partial distances to be obtained in parallel using SIMD shuffle operations since the ADT for a subspace is fully contained within a SIMD register.

\texttt{Flash} enhances HNSW construction by optimizing memory layout and SIMD operations. Two hyperparameters—the number of subspaces ($M_F$) and the dimension of the principal components ($d_F$)—can be adjusted to balance construction efficiency and search performance (Theorem \ref{the: Dist. Comp.}). These adjustments do not affect \texttt{Flash}'s memory access pattern or SIMD optimizations. We highlight that existing SIMD optimizations for PQ, as discussed in \cite{PQfast,QuickADC}, target linear scans or inverted file (IVF) structures, where vector batches are naturally grouped, rather than graph index settings \cite{RaBitQ}.

\subsubsection{\textbf{Extracting Principal Components}}
\label{subsubsec: extract principal components}
To align with SIMD register constraints, each subspace has a limited bit representation (e.g., 4 bits). Encoding on principal components optimally utilizes these bits, reducing quantization error.
Thus, \texttt{Flash} employs PCA to project high-dimensional vectors by rearranging dimensions in descending order of variance. This approach preserves essential low-dimensional principal components by selecting the first $d_{F}$ dimensions, encapsulating the most significant features.
Given a dataset $\boldsymbol{S}$ of $n$ vectors, each with $D$ dimensions, the mean vector is $\bar{\boldsymbol{u}} = \frac{1}{n} \sum_{i=1}^{n} \boldsymbol{u}_i$. Data is centered by subtracting $\bar{\boldsymbol{u}}$ from each vector, producing the centered matrix $\dot{\boldsymbol{S}} = \boldsymbol{S} - \boldsymbol{1} \cdot \bar{\boldsymbol{u}}^{\top}$, where $\boldsymbol{1}$ is a unit vector.
The covariance matrix $\Sigma = \frac{1}{n} (\dot{\boldsymbol{S}}^{\top} \dot{\boldsymbol{S}})$ is then computed, and its eigenvalues $\lambda_1, \lambda_2, \ldots, \lambda_{D}$ and eigenvectors $\boldsymbol{a}_1, \boldsymbol{a}_2, \ldots, \boldsymbol{a}_{D}$ are extracted. For a given variance fraction $\alpha$, the function $f(d) = \frac{\sum_{i=1}^{d} \lambda_i}{\sum_{i=1}^{D} \lambda_i}$ identifies the smallest $d_{F}$ such that $f(d_{F}) \geq \alpha$, defining the principal component space of dimension $d_{F}$. The principal components of each vector are established based on the basis $\boldsymbol{A}_{1:d_{F}} = (\boldsymbol{a}_1 \ \boldsymbol{a}_2 \ \cdots \ \boldsymbol{a}_{d_{F}})$, yielding the reduced-dimensional data $\widetilde{\boldsymbol{S}}$:
\begin{equation}
 \widetilde{\boldsymbol{S}} = \{ \tilde{\boldsymbol{u}}_i \ | \ \tilde{\boldsymbol{u}}_i = \mathbf{A}_{1:d_{F}}^{\top} \boldsymbol{u}_i, \ \text{for } i = 1, \ldots, n \}
\end{equation}

\subsubsection{\textbf{Subspace Division and Distance Table Compression}}
\label{subsubsec: subspace division}
\texttt{Flash} decomposes each vector's principal components $\tilde{\boldsymbol{u}}$ into $M_{F}$ disjoint subvectors, denoted as $\tilde{\boldsymbol{u}} = [\tilde{\boldsymbol{u}}_{1}, \tilde{\boldsymbol{u}}_{2}, \ldots, \tilde{\boldsymbol{u}}_{M_{F}}]$, where each $\tilde{\boldsymbol{u}}_{i}$ resides in a specific subspace of the principal component domain. Similar to PQ, a codebook $\boldsymbol{C}_i = \{\boldsymbol{c}_{i1}, \boldsymbol{c}_{i2}, \ldots, \boldsymbol{c}_{iK}\}$ is created for each subspace, with $\boldsymbol{c}_{ij}$ denoting a codeword (i.e., a centroid ID) in the $i$-th subspace, and $K$ representing the number of centroids. The $i$-th subvector $\tilde{\boldsymbol{u}}_{i}$ is quantized by identifying the closest centroid in the codebook $\boldsymbol{C}_i$ of the $i$-th subspace:
\begin{equation}
\psi(\tilde{\boldsymbol{u}}_{i}) = \arg \min_{\boldsymbol{c}_{ij} \in \boldsymbol{C}_i} \delta( \tilde{\boldsymbol{u}}_{i}, \boldsymbol{c}_{ij} )
\end{equation}
where $\psi(\tilde{\boldsymbol{u}}_{i})$ is the nearest centroid's ID serving as the codeword of the subvector. Consequently, the complete principal components $\tilde{\boldsymbol{u}}$ are represented as a sequence of codewords, one for each subvector. The codeword length $L_F$ relates to the number of centroids $K$ via $L_F=\lceil log_2{K} \rceil$ in a subspace. \texttt{Flash} samples a subset from the complete vector set to efficiently construct codebooks, following PQ and its variants \cite{PQ,OPQ}.

For each inserted vector $\boldsymbol{u}$, asymmetric distance tables (ADT) and codewords are simultaneously generated across $M_{F}$ subspaces using precomputed codebooks, leveraging shared distance computations between each subvector $\tilde{\boldsymbol{u}}_{i}$ and the centroids in $\boldsymbol{C}_i$.
This approach avoids duplicate calculations, enhancing efficiency. In the Candidate Acquisition (CA) stage, the ADT enables fast summation of partial distances to calculate distances between $\boldsymbol{u}$ and visited vertices.
To adapt ADT for SIMD registers, \texttt{Flash} employs SQ to map each distance $dist$ in the table to discrete levels:
\begin{equation}
\eta(dist) = \Big\lfloor \left(\frac{dist - dist_{\min}}{\Delta}\right) \cdot (2^{H}-1) \Big\rfloor
\end{equation}
where $\eta(dist)$ is the quantized distance, $dist_{\min}$ is the minimum distance, $\Delta$ is the quantization step size ($\Delta=dist_{max}-dist_{min}$), and $H$ is the bit number for a quantized distance value. {With $K=16$ and $H=8$, each ADT is 128 bits in size. These ADTs are stored as \textit{one-dimensional arrays} that can reside in registers during the insertion process, and are freed once the insertion is completed.}

In the Neighbor Selection (NS) stage, distance computations among candidates preclude the use of ADT. To address this, \texttt{Flash} precomputes a symmetric distance table (SDT) that stores distances between centroids in each subspace. {With $M_F$ subspaces and $K$ centroids, $M_F$ SDTs are built, each holding $K^2$ distance values in a \textit{two-dimensional array}.} These values are quantized similarly to ADT for optimal caching. \texttt{Flash} obtains distances between candidates from SDTs based on their codewords. Note that SDTs are shared by all inserted vectors during index construction, eliminating numerous random memory accesses.
To compare distance values from SDTs and ADTs for neighbor selection, \texttt{Flash} uses the same quantization step size $\Delta$ and target bit number $H$ for both tables. It calculates the maximum and minimum distances ($dist_{max}^{i}$, $dist_{min}^{i}$) within each subspace, summing each $dist_{max}^{i}$ to obtain $dist_{max}$ and taking the lowest among $dist_{min}^{i}$ as $dist_{min}$.
% The quantization step size is $\Delta=dist_{max}-dist_{min}$.

\subsubsection{\textbf{Access-Aware Memory Layout}}
{In the graph index, all vertices share a uniform neighbor list structure and size. A \textit{contiguous memory block} is allocated for each vertex's vector and neighbor data, accessed via an offset determined by the vertex ID.}
For an inserted vector $\boldsymbol{u}$, the CA stage updates candidates via greedy search, visiting a vertex's neighbors in the sub-graph index and computing their distances to $\boldsymbol{u}$.
Current graph indexes suffer from numerous random accesses when fetching neighbor vectors, as neighbor vectors are too big to be stored alongside neighbor IDs. By attaching neighbor codewords to these IDs, \texttt{Flash} computes distances directly in register-resident ADTs, avoiding random memory accesses. {For each vertex, \texttt{Flash} stores the neighbor IDs in one fixed memory block, followed by the neighbor codewords in another (see Figure \ref{fig:compact code}, lower right).} To efficiently use SIMD instructions, \texttt{Flash} gathers $B$ neighbor codewords in a subspace, processing them concurrently to ensure efficient SIMD register loading and parallel distance computations for $B$ neighbors.
% Note that each subspace's ADT resides entirely within a SIMD register.

\subsubsection{\textbf{SIMD Acceleration}}
\label{subsubsec: simd acceler}
SIMD instructions support vectorized execution, enabling simultaneous processing of multiple data points. This can accelerate distance calculations between the inserted vector $\boldsymbol{u}$ and a batch of neighbors. \texttt{Flash} uses SIMD to look up ADTs and sum partial distances across subspaces. An ADT resides in a SIMD register, storing distances between $\boldsymbol{u}$ and centroids in each subspace. \texttt{Flash} employs shuffle operations to extract partial distances using neighbors’ codewords. In a single operation, codewords for $B$ neighbors are loaded into a register, which then serves as indices to access the partial distances in the ADT. Specifically, the shuffle operation rearranges the data within the register according to the indices, extracting partial distances from the ADT without complex conditional branching or excessive memory accesses. Partial distances from two subspaces are summed using SIMD instructions, and this process is repeated across all subspaces to compute the final distances for the $B$ neighbors. By using SIMD lookups for ADTs, \texttt{Flash} minimizes random memory accesses and exploits high-speed registers for parallel arithmetic, thereby improving computational efficiency.

\subsubsection{\textbf{Implementation on HNSW}}
\label{subsubsec: hnsw-flash}
We integrate \texttt{Flash} into the HNSW algorithm to accelerate indexing and search processes. It be- gins by preprocessing the dataset to extract principal components (Section \ref{subsubsec: extract principal components}), then sampling a subset, dividing it into subspaces, generating codebooks, and pre-computing centroid distances for the SDT. During index construction (lines 2-8 of Algorithm \hyperref[alg: hnsw construction]{1}), each vector $\boldsymbol{u}$ has its principal components partitioned by the codebooks. Distances between each subvector $\boldsymbol{u}_i$ and codebook centroids $\boldsymbol{C}_i$ form the ADT, and the nearest centroid ID is selected as the codeword for $\boldsymbol{u}_i$. The ADT is quantized (Section \ref{subsubsec: subspace division}) and held in a register until insertion completes. For line 5, SIMD acceleration computes distances between $\boldsymbol{u}$ and batches of neighbors based on ADT (Section \ref{subsubsec: simd acceler}). For lines 6–7, distances between candidates are approximated via SDT lookups, using each candidate’s codewords as indices. We tune the number of subspaces ($M_F$) and the dimensionality of principal components ($d_F$) to manage compression error (Theorem \ref{the: Dist. Comp.}) without affecting \texttt{Flash}’s efficient memory access or SIMD operations. Other fixed parameters include bits per codeword ($L_F$) and batch size ($B$), set by the SIMD register size. The search procedure follows the CA paradigm, and we apply an additional reranking step based on original vectors to refine results.

\vspace{0.2cm}
\noindent\underline{Remarks.}
(1) {Although many studies optimize the compression error of PQ, SQ, and PCA \cite{OPQ,ScaNN,LVQ,ZhangTHW22}, none are specifically tailored to meet the distinct requirements of graph indexing on modern CPU architectures. Integrating these compression methods into the graph construction process often yields limited efficiency gains or even degrades search performance, as they neither address random memory access patterns nor leverage SIMD efficiently. In contrast, \texttt{Flash} is a specialized compact coding strategy designed for graph index construction and optimized for current CPU architectures. Notably, \texttt{Flash} incorporates the complementary principles of established vector compression methods (e.g., PQ, SQ, PCA) within its framework and offers the flexibility to integrate new optimizations of these compression methods. While several studies integrate vector compression into search processes \cite{yue2023routing,DiskANN,Starling,LVQ}, search differs markedly from index construction, which involves more complex tasks such as the NS stage. Moreover, it is crucial that compression methods avoid excessive overhead, maintaining a balance between construction speed and index quality. Recent work \cite{yue2023routing,DiskANN,Starling,yang2024bridging} shows that boosting search performance often increases indexing time.}
(2) In \texttt{Flash}, codeword and ADT generations share identical distance computations (Section \ref{subsubsec: subspace division}), prompting an integrated implementation to eliminate redundancies. We utilize the Eigen library for matrix manipulations throughout data processing, including principal component extraction, codebook generation, and distance table creation. When the maximum number of neighbors $R$ exceeds the batch size $B$, we split each vertex's neighbor data into multiple blocks, each matching the batch size. This structure enables the efficient distance computation within each block using SIMD instructions.

\subsubsection{\textbf{Cost Analysis}}
\label{subsubsec: cost analysis}
In the CA stage, the time complexity of HNSW is approximately $O(R\cdot log(n))$, where $R$ is the maximum number of neighbors and $log(n)$ denotes the search path length \cite{graph_survey_vldb2021,HVS}. For each hop, vector data of the visiting vertices' neighbors must be accessed. Therefore, the number of memory accesses in the original HNSW algorithm ($NMA_{orig}$) can be expressed as:
\begin{equation}
    NMA_{orig} \sim O(R\cdot log(n))
\end{equation}
In our optimized implementation, neighbors' codewords and their respective IDs are stored contiguously, eliminating the need for random memory accesses to fetch neighbor vectors. As a result, the number of memory accesses in our implementation ($NMA_{ours}$) is:
\begin{equation}
    NMA_{ours} \sim O(log(n))
\end{equation}
For simplicity, this analysis excludes cache hits. Notably, the \texttt{Flash} code is significantly smaller than the original vector data, resulting in a higher cache hit ratio.
In the NS stage, computing distances between candidates requires accessing their vector data. For the original HNSW algorithm, the number of memory accesses is $O(C)$, where $C$ denotes the size of the candidate set. In contrast, \texttt{Flash} maintains the entire SDT in the L1 cache, avoiding fetching vector data from main memory during the NS stage.

For each distance computation using SIMD acceleration, the number of register loads in the original HNSW ($NRL_{orig}$) is:
\begin{equation}
    NRL_{orig} = \frac{32\cdot D}{U}
\end{equation}
where $D$ is the vector dimensionality (each vector has $D$ floating-point values) and $U$ is the bit-width of a register. This is because each vector segment must be individually loaded into an SIMD register. Furthermore, complex arithmetic operations, including subtraction, multiplication, and addition, are required for distance computation. In contrast, the number of register loads in our implementation ($NRL_{ours}$) is
\begin{equation}
    NRL_{ours} = \frac{M_{F}\cdot H}{U}
\end{equation}
where $M_F$ is the number of subspaces and $H$ represents the number of bits to encode a partial distance within each subspace. In \texttt{Flash}, the ADT is register-resident. Thus, it loads the codewords of $B$ neighbors within a subspace into a register in a single operation. To compute distances for these neighbors, $M_F$ batches of codewords are loaded, corresponding to $M_F$ register loads. The parameter $B$ is typically set to $U/H$ to align with register capacity. Additionally, the arithmetic operations in \texttt{Flash} are reduced to simple addition. With $D = 768$, $U = 128$, $M_F = 16$, and $H = 8$, the number of register loads required for a distance computation in the original HNSW is 192, whereas in our optimized version, it is reduced to just 1.