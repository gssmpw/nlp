\section{Background and Motivation}
\label{sec: background}
In this section, we first review current graph-based ANNS algorithms to ascertain the leadership of \method{HNSW}, then outline search and indexing optimizations based on \method{HNSW}. Next, we provide a detailed analysis of the \method{HNSW} construction process to identify the root causes of inefficiency on modern CPUs.

\subsection{Related Work}
\label{subsec: Related Work}

\subsubsection{\textbf{Graph-based ANNS Algorithms}}
\label{subsubsec: Graph ANNS Alg}
Graph-based ANNS algorithms have emerged as the leading approach for balancing search accuracy and efficiency in high-dimensional spaces \cite{graph_survey_vldb2021,DPG,DiskANN,tau-MG,NSSG,RoarGraph}. These methods construct a graph index where vertices represent database vectors, and directed edges signify neighbor relationships. During search, a greedy algorithm navigates the graph, visiting a vertex's neighbors and selecting the one closest to the query, continuing until no closer neighbor is found, yielding the final result.
Current methods share similar index structures and greedy search strategies but differ in their edge selection strategies during graph construction \cite{graph_survey_vldb2021}. For example, \method{HNSW} \cite{HNSW} and \method{NSG} \cite{NSG} use the Monotonic Relative Neighborhood Graph (MRNG) strategy, while \method{Vamana} \cite{DiskANN} and $\tau$-\method{MG} \cite{tau-MG} incorporate additional parameters. \method{HCNNG} \cite{HCNNG} employs the Minimum Spanning Tree (MST) approach, while \method{KGraph} \cite{NNDescent} and NGT \cite{NGT} implement the K-Nearest Neighbor Graph (KNNG) paradigm. {Despite these variations, state-of-the-art methods like \method{HNSW}, \method{NSG}, Vamana, and $\tau$-\method{MG} adhere to a similar construction framework, consisting of Candidate Acquisition (CA) and Neighbor Selection (NS) stages \cite{NSG,tau-MG}.} {Our research identifies the bottleneck in the CA and NS steps, where distance calculations dominate indexing time (see Section \ref{subsec: problem analysis}). Consequently, improving the CA and NS steps will accelerate the indexing process in all these graph algorithms.} Notably, \method{HNSW} stands out for its robust optimizations and versatile features \cite{Finger,ZuoQZLD24,LiuZHSLLDYW22}, including native \textit{add} support. Numerous studies have enhanced \method{HNSW} from various perspectives (we review them in the following paragraphs), solidifying its role as a mainstream method in both industry and academia.

\subsubsection{\textbf{Search Optimizations on \methodbt{HNSW}}}
\label{subsubsec: Search Opt HNSW}
Search optimizations for \method{HNSW} focus on several key components. Some aim to optimize entry point acquisition by leveraging additional quantization or hashing structures to start closer to the query \cite{HVS,zhao2023towards}. Others optimize neighbor visits by learning global \cite{LearnToRoute} or topological \cite{MunozDT19} information, enhancing data locality \cite{ColemanSSS22}, and partitioning neighbor areas \cite{TOGG}. These strategies improve accuracy and efficiency by identifying relevant neighbors and skipping irrelevant ones. Some approaches reduce distance computation complexity using approximate calculations \cite{ADSampling,Finger,yang2024bridging,lu2024probabilistic}, enhancing search efficiency with minimal accuracy loss. Additionally, optimizations refine termination conditions through adaptive decision-making \cite{LiZAH20,LiuZHSLLDYW22,yang2021tao} and relaxed monotonicity \cite{zhang2023vbase}.
Specialized hardware like GPUs \cite{SONG}, FPGAs \cite{jiang2024accelerating,ZengZLZDZLNXYW23,PengCWYWGLBSJLD21}, Compute Express Link (CXL) \cite{CXL-ANNS}, Non-Volatile Memory (NVM) \cite{HM_ANN}, NVMe SSDs \cite{Starling}, and SmartSSDs \cite{SmartSSD} accelerate distance computation and optimize the \method{HNSW} index layout, achieving superior performance through software-hardware collaboration. To support complex query processing, some approaches integrate structured predicates \cite{PatelKGZ24,NHQ} or range attributes \cite{ZuoQZLD24} into \method{HNSW}, enabling hybrid searches of vectors and attributes. These optimizations enhance HNSW's performance and versatility, meeting diverse real-world requirements, though most increase indexing time compared to the original \method{HNSW}.

\subsubsection{\textbf{Indexing Optimizations of \methodbt{HNSW}}}
\label{subsubsec: Index Opt HNSW}
Despite HNSW's superior search performance, its low index construction efficiency remains a major bottleneck, a challenge shared by other graph-based methods \cite{HVS,NSG,NSSG,DiskANN,zhao2023towards,aumuller2023recent}. As demands for dynamic updates \cite{XuLLXCZLYYYCY23,Fresh-DiskANN}, large-scale data \cite{DiskANN,SPANN}, and complex queries \cite{PatelKGZ24,ZuoQZLD24} grow (see Section \ref{sec: intro}), optimizing HNSW construction becomes increasingly important. Current research targets two areas: algorithm optimization and hardware acceleration.
Algorithm optimization efforts have redesigned the construction pipeline, either fully \cite{NSG,tau-MG} or partially \cite{HVS}, but these attempts offer only marginal improvements \cite{NSG,NSSG,zhao2023towards}. Moreover, these optimizations substantially modify the algorithmic process of \method{HNSW}, requiring code refactoring that may disrupt many engineering optimizations integrated over the years \cite{hnswlib,n2,Faiss}. Some HNSW features, like native \textit{add} support, are weakened \cite{HVS} or even discarded \cite{NSG}.
On the other hand, GPU-accelerated methods parallelize distance computations during \method{HNSW} construction \cite{WangZZY21,CAGRA,GGNN}. These methods re-implement \method{HNSW} to align with GPU architecture, achieving an order of magnitude speedup compared to single-thread CPU-based construction \cite{GANNS}. However, GPU-specific limitations reduce their practicality in real-world scenarios \cite{ZhangL0L024} (see Section \ref{sec: intro}). As CPU-based \method{HNSW} remains prevalent in many industrial applications \cite{Milvus_sigmod2021,ColemanSSS22,douze2024faiss,PASE}, accelerating \method{HNSW} construction on modern CPUs is crucial. {Note that parallel multi-thread implementations of graph indexing on CPUs are simple, efficient, and widely adopted by current graph indexes (e.g., \method{HNSW}, \method{NSG} \cite{NSG}, $\tau$-MG \cite{tau-MG}). These implementations are orthogonal to our work, as our techniques build upon multi-thread graph indexing.}

\subsubsection{\textbf{Distributed Deployment of \methodbt{HNSW}}}
\label{subsubsec: Distri Deploy HNSW}
Distributed deployment accelerates HNSW construction by partitioning large datasets into multiple shards, each containing tens of millions of vectors \cite{Manu_zilliz,lanns,IwabuchiSPPS23,deng2019pyramid}. This enables concurrent index construction across nodes, enhancing efficiency for large-scale datasets \cite{lanns}. Query processing can be optimized by an inter-shard coordinator \cite{Manu_zilliz}, and advanced data fragmentation strategies can assign queries to relevant shards \cite{ZhangYGWHLLZT23,ELPIS}. At the system level, distributed deployment ensures scaling, load balancing, and fault tolerance \cite{Starling}, but introduces challenges like high communication overhead \cite{MiaoZSNYTC21,Milvus_sigmod2021}. Notably, distributed deployment is orthogonal to our research, as we focus on efficient index building within a shard, which can be directly integrated into existing distributed systems.

\subsection{Problem Analysis}
\label{subsec: problem analysis}

\setlength{\textfloatsep}{0pt}
\begin{algorithm}[t]
\label{alg: hnsw construction}
  \caption{\textsc{Index Construction of \method{HNSW}}}
  \LinesNumbered
  \KwIn{a vector dataset $\boldsymbol{S}$, hyper-parameters $C$ and $R$ ($R\leq C$)}
  \KwOut{\method{HNSW} index built on $\boldsymbol{S}$}

  $\boldsymbol{V} \gets \emptyset$, $\boldsymbol{E} \gets \emptyset$; \Comment{\textsf{start from a empty graph}}

  \For(\Comment{\textsf{insert all vectors in $\boldsymbol{S}$}}){each $\boldsymbol{x}$ in $\boldsymbol{S}$}{
    $l_{max}$ $\gets$ $\boldsymbol{x}$'s max layer; \Comment{\textsf{exponential decaying distribution}}

    \For(\Comment{\textsf{insert $\boldsymbol{x}$ at layer $l$}}){each $l$ $\in$ $\{l_{max}, \cdots, 0\}$}{
      $\boldsymbol{C(x)}$ $\gets$ top-$C$ candidates; \Comment{\textsf{greedy search}}

      $\boldsymbol{N(x)}$ $\gets$ $\leq R$ neighbors from $\boldsymbol{C(x)}$; \Comment{\textsf{heuristic strategy}}

      add $\boldsymbol{x}$ to $\boldsymbol{N(y)}$ for each $\boldsymbol{y}\in \boldsymbol{N(x)}$; \Comment{\textsf{reverse edge}}
    }

    $\boldsymbol{V}\gets \boldsymbol{V}\cup \{\boldsymbol{x}\}$, $\boldsymbol{E}\gets \boldsymbol{E} \cup \boldsymbol{N(x)}$
    
  }
  
  \textbf{return} $\boldsymbol{G}=(\boldsymbol{V},\boldsymbol{E})$
\end{algorithm}

\noindent\underline{Notations.} In the HNSW index, each vertex corresponds to a unique vector, denoted by bold lowercase letters (e.g., $\boldsymbol{x}$, $\boldsymbol{y}$). We use the Euclidean distance $\delta(\boldsymbol{x}, \boldsymbol{y})$ as the distance metric for quantifying similarity, where a smaller $\delta(\boldsymbol{x}, \boldsymbol{y})$ indicates greater similarity \cite{NSG,graph_survey_vldb2021,zhao2023towards,tau-MG}. Bold uppercase letters (e.g., $\boldsymbol{S}$) denote sets, while unbolded uppercase and lowercase letters represent parameters.

Given a vector dataset $\boldsymbol{S}$, the \method{HNSW} index $\boldsymbol{G}$ is built by progressively inserting vectors into the current graph index, as outlined in Algorithm \hyperref[alg: hnsw construction]{1}. Two hyper-parameters must be set before index construction: the maximum number of candidates ($C$) and the maximum number of neighbors ($R$)\footnote{In the original paper and many popular repositories \cite{n2,hnswlib,HNSW}, $C$ is called $efConstruction$, and $R$ in the base layer is twice that in higher layers \cite{HNSW}.}. For each inserted vector $\boldsymbol{x}$, the maximum layer $l_{max}$ is randomly selected following an exponentially decaying distribution (line 3). The vector is inserted into layers from $l_{max}$ to $0$, with the same procedure applied at each layer (lines 4-7). At layer $l$, $\boldsymbol{x}$ is treated as a query point, and a greedy search on the current graph at layer $l$ identifies the top-$C$ nearest vertices as candidates (line 5). During the search, a candidate set $\boldsymbol{C(x)}$ of size $C$ is maintained, with vertices ordered in ascending distance to $\boldsymbol{x}$, and the maximum distance is denoted as $T$. The search iteratively visits a vertex's neighbors, calculates their distances to $\boldsymbol{x}$, and updates $\boldsymbol{C(x)}$ with neighbors that have smaller distances ($<T$). To obtain the final neighbors, a heuristic edge selection strategy prevents clustering among neighbors (line 6). For example, for candidate $\boldsymbol{v}$ and any candidate $\boldsymbol{u}$ where $\delta(\boldsymbol{u},\boldsymbol{x}) < \delta(\boldsymbol{v},\boldsymbol{x})$, if $\delta(\boldsymbol{u},\boldsymbol{v}) < \delta(\boldsymbol{v},\boldsymbol{x})$, $\boldsymbol{v}$ is excluded from $\boldsymbol{N(x)}$. After determining $\boldsymbol{x}$'s final neighbors, $\boldsymbol{x}$ is added to the neighbor list $\boldsymbol{N(y)}$ for each $\boldsymbol{y} \in \boldsymbol{N(x)}$ (line 7). If $\boldsymbol{N(y)}$ exceeds $R$, the heuristic edge selection strategy prunes excess neighbors, ensuring that the neighbor count remains under $R$.

The construction process of \method{HNSW} involves two main steps: \textit{Candidate Acquisition} (CA) and \textit{Neighbor Selection} (NS). The CA step identifies the top-$C$ nearest vertices for an inserted vector using a greedy search strategy. This involves visiting a vertex's neighbor list and computing distances to the inserted vector, requiring random memory access to fetch a neighbor's vector data, stored separately from neighbor IDs due to the large vector size. In the NS step, the final $R$ neighbors are selected from the candidate set using a heuristic strategy. Here, distances among candidates are computed, also requiring random access to their vector data. In both CA and NS, distance calculations are used solely for comparison. In CA, distances are compared to the largest distance in the candidate set to determine if a visiting vertex can replace the farthest candidate. In NS, distances among candidates and between candidates and the inserted vector are compared to decide the inclusion of a candidate in the final neighbor set. HNSW currently performs all distance computations on full-precision vectors. Additionally, to utilize SIMD acceleration, a distance computation requires hundreds of read operations to load vector segments into a 128-bit register due to a large vector size, often spanning thousands of bytes.

\begin{figure}
  \centering
  \setlength{\abovecaptionskip}{0.1cm}
  \setlength{\belowcaptionskip}{0.1cm}
  \includegraphics[width=\linewidth]{figures/illustration/problem_analysis.pdf}
  \caption{Illustrating memory accesses and arithmetic operations during the \method{HNSW} construction process (base layer).}
  \label{fig:problem ananlysis}
\end{figure}

\begin{myExa}
    \rm In Figure \ref{fig:problem ananlysis}, we illustrate the index construction process of \method{HNSW} at the base layer, exemplified by inserting $\boldsymbol{v_{18}}$ with $C=4$ and $R=2$. In the CA step, $\boldsymbol{v_{18}}$ is treated as a query point, and a greedy search from $\boldsymbol{v_0}$ is initiated. After examining $\boldsymbol{v_0}$'s neighbors, the candidate set $\boldsymbol{C(v_{18})}$ is $\{\boldsymbol{v_5}, \boldsymbol{v_9}, \boldsymbol{v_0}, \boldsymbol{v_{13}} \}$, ordered by proximity to $\boldsymbol{v_{18}}$. The current distance threshold $T$ is $\delta(\boldsymbol{v_{13}}, \boldsymbol{v_{18}})=0.54$. The nearest vertex, $\boldsymbol{v_5}$, is then visited, prompting an exploration of its neighbors to update $\boldsymbol{C(v_{18})}$. Upon calculating $\delta(\boldsymbol{v_{15}}, \boldsymbol{v_{18}})$ and comparing it with $T$, $\boldsymbol{v_{13}}$ is replaced by $\boldsymbol{v_{15}}$ as $\delta( \boldsymbol{v_{15}}, \boldsymbol{v_{18}}) < T$, and $T$ is updated to $\delta(\boldsymbol{v_{0}}, \boldsymbol{v_{18}})=0.49$. This process continues, resulting in the replacement of $\boldsymbol{v_0}$ and $\boldsymbol{v_{15}}$ with $\boldsymbol{v_{17}}$ and $\boldsymbol{v_{3}}$, respectively. At the end of CA, $\boldsymbol{C(v_{18})}$ comprises $\{\boldsymbol{v_{17}}, \boldsymbol{v_5}, \boldsymbol{v_3}, \boldsymbol{v_{9}} \}$. The vertices $\boldsymbol{v_0}$, $\boldsymbol{v_3}$, $\boldsymbol{v_5}$, $\boldsymbol{v_9}$, $\boldsymbol{v_{13}}$, $\boldsymbol{v_{15}}$, and $\boldsymbol{v_{17}}$ are traversed in this phase. As shown in the graph index's memory layout (lower left), these vertices (highlighted in red) exhibit a random distribution, meaning numerous random memory accesses.
    In the NS step, $\boldsymbol{v_{17}}$ is initially chosen, leading to the removal of $\boldsymbol{v_3}$ and $\boldsymbol{v_9}$ from $\boldsymbol{C(v_{18})}$ based on the conditions $\delta(\boldsymbol{v_{17}},\boldsymbol{v_3}) < \delta(\boldsymbol{v_{3}},\boldsymbol{v_{18}})$ and $\delta(\boldsymbol{v_{17}},\boldsymbol{v_9}) < \delta(\boldsymbol{v_{9}},\boldsymbol{v_{18}})$. The remaining vertex, $\boldsymbol{v_5}$, is added to the neighbor list, which reaches the limit $R=2$, yielding $\{ \boldsymbol{v_{17}}, \boldsymbol{v_5}\}$. $\boldsymbol{v_{18}}$ is then appended to the neighbor lists of $\boldsymbol{v_{17}}$ and $\boldsymbol{v_5}$, while ensuring that their neighbor counts remain below $R$ through the heuristic strategy. In practice, $\boldsymbol{C(v_{18})}$ can include thousands of candidates, making full vector caching impractical and necessitating many random memory accesses for distance computations during the NS stage. To leverage SIMD instructions, a vector is divided into multiple segments, each comprising four dimensions suitable for loading into a 128-bit register. While SIMD enhances processing speed, the necessity of numerous register load operations to sequentially fetch each vector segment leads to reduced SIMD utilization efficiency.
\end{myExa}

Our analysis reveals that the primary inefficiency in HNSW index construction lies in the current distance computation process, consuming over 90\% of indexing time (Figure \ref{fig: indexing profile}). This process suffers from numerous random memory accesses and suboptimal SIMD utilization, misaligning with modern CPU architecture. Thus, optimizing distance computation to better exploit modern CPU capabilities is essential for accelerating HNSW construction.