\section{Introduction}
\label{sec: intro}
Approximate Nearest Neighbor Search (ANNS) in high-dimensional spaces has become integral due to the advent of deep learning embedding techniques for managing unstructured data \cite{Milvus_sigmod2021,graph_survey_vldb2021,tau-MG,RaBitQ}. This spans a variety of applications, such as recommendation systems \cite{ChenLZWLMHJXDZ22,HanZYXCS23}, information retrieval \cite{GouDWK24,WangMW22}, and vector databases \cite{VDB_tutorial_SIGMOD24,Manu_zilliz}. 
Owing to the ``curse of dimensionality'' and the ever-increasing volume of data, ANNS seeks to optimize both speed and accuracy, making it more practical compared to the exact solutions, which can be prohibitively time-consuming.
Given a query vector, ANNS efficiently finds its $k$ nearest vectors in a vector dataset following a specific distance metric. 
Current studies explore four index types for ANNS: Tree-based \cite{AroraSK018,LuWWK20}, Hash-based \cite{HuangFZFN15,ZhaoZYLXZJ23}, Quantization-based \cite{PQ,PQfast}, and Graph-based methods \cite{NSG,HVS}. 
Among these, graph-based algorithms like \method{HNSW} \cite{HNSW} have demonstrated superior performance, establishing themselves as the industry standard \cite{lanns,es_hnsw,douze2024faiss} and attracting substantial academic interest \cite{ADSampling,HM_ANN,LiZAH20}.
For example, \method{HNSW} underpins various ANNS services or vector databases, including PostgreSQL \cite{PASE}, ElasticSearch \cite{es_hnsw}, Milvus \cite{Milvus_sigmod2021}, and Pinecone \cite{pinecone_hnsw}. 
Recent publications from the data management community \cite{sigmod_24_papers, vldb_24_papers, icde_24_papers} highlight the focus on ANNS research regarding \method{HNSW}\footnote{Unless otherwise specified, we illustrate our framework design using this representative graph-based index.}.

Although numerous studies \cite{LiZAH20,ADSampling,HVS,yue2023routing,Finger,ColemanSSS22,PengZLJR23} have explored optimizations for search performance, research on improving index construction efficiency remains limited\footnote{{Since multi-thread graph indexing on CPUs is a standard practice, we present all discussed graph algorithms in their multi-thread versions by default.}}, which is increasingly critical in real-world applications \cite{XuLLXCZLYYYCY23}. 
First, the {\em vast volume of data} poses significant challenges for index construction. For instance, building an \method{HNSW} index on tens of millions of vectors typically requires about \textit{10 hours} \cite{HVS,lanns}, billion-scale datasets may extend construction time to \textit{five days}, even with relaxed construction parameters \cite{DiskANN}. 
Second, complex query processing, such as hybrid search \cite{Filtered-DiskANN,mengzhao_neurips2023}, adds constraints on ANNS, {\em complicating the construction process} and increasing indexing times \cite{ZuoQZLD24}. For example, constructing a specialized \method{HNSW} index for attribute-constrained ANNS takes 33$\times$ longer than a standard index \cite{PatelKGZ24}. 
More importantly, {\em continuous data and embedding model updates} \cite{Neos,Jingdong_paper,SundaramTSMIMD13,embedding_models_update_1,embedding_models_update_2} necessitate a periodic reconstruction process based on the LSM-Tree framework \cite{Milvus_sigmod2021,ADBV,pan2023survey,nmslib_issues73,SingleStore-V,XuLLXCZLYYYCY23}.
While some approaches aim to avoid rebuilding \cite{Fresh-DiskANN}, they are often unsuitable for embedding model updates \cite{PouyanfarSYTTRS19,embedding_models_update_1} and can result in diminished search performance \cite{nmslib_issues73}, with accuracy dropping from 0.95 to 0.88 after 20 update cycles on \method{HNSW} \cite{Fresh-DiskANN}. 
Thus, reconstruction efficiency has become a bottleneck in modern vector databases \cite{pan2023survey}, as it {\bf \em directly impacts the ability to rapidly release new services with up-to-date data and high search performance} \cite{Fresh-DiskANN,ADBV}. In business scenarios, index rebuilding often occurs overnight during low user activity \cite{ADBV}, constrained to a few hours. Serving approximately 100M vectors on a single node, the \method{HNSW} build time frequently exceeds 12 hours, failing to meet the requirement.

To enhance the index construction efficiency, employing specialized hardware like GPUs is a straightforward approach \cite{WangZZY21,ShiZZJHLH18}. 
By leveraging GPUs' parallel computing capabilities, recent studies have parallelized distance computations and data structure maintenance for the graph indexing process on \textit{million-scale} datasets \cite{GANNS,CAGRA,SONG}, achieving an order of magnitude speedup over single-thread CPU implementations \cite{GANNS}. 
However, challenges such as high costs, memory constraints, and additional engineering efforts limit their practical applications \cite{ZhangL0L024}. 
For instance, even high-end GPUs like the NVIDIA A100, with tens of gigabytes of memory, struggle to accommodate large vector datasets requiring hundreds of gigabytes. 
As noted by Stonebraker in his recent survey \cite{Stonebraker_SIGMOD_Record}, ``\textit{If data does not fit in GPU memory, query execution bottlenecks on loading data into the device, significantly diminishing parallelization benefits.}'' Consequently, CPU-based deployment remains the prevalent choice in practical scenarios \cite{Milvus_sigmod2021,ColemanSSS22,douze2024faiss,PASE}, providing a cost-effective, memory-sufficient, and easy-to-use solution. 
This motivates us to explore optimization opportunities to improve the index construction efficiency on modern CPUs.

\begin{figure}
  \setlength{\abovecaptionskip}{0cm}
  \setlength{\belowcaptionskip}{-0.4cm}
  \centering
  \footnotesize
  \stackunder[0.5pt]{\includegraphics[scale=0.33]{figures/illustration/profile-indexing-laion.pdf}}{(a) LAION-1M ($D=768$)}
  \hspace{0.15cm}
  \stackunder[0.5pt]{\includegraphics[scale=0.32]{figures/illustration/profile-indexing-argilla.pdf}}{(b) ARGILLA-1M ($D=1024$)}
  \newline
  \caption{Profiling of HNSW indexing time. Distance computation constitutes the majority of the indexing time and consists of two components: memory accesses (\textit{B}) and arithmetic operations (\textit{C}). Other tasks (\textit{A}), such as data structure maintenance, account for a minor portion.}
  \label{fig: indexing profile}
  \vspace{-0.4cm}
\end{figure}

In this paper, we conduct an in-depth analysis of the \method{HNSW} construction process, identifying the root causes of inefficiency on modern CPUs. Figure \ref{fig: indexing profile} illustrates HNSW indexing time profiled using the \textit{perf} tool \cite{perf} on two real-world embedding datasets \cite{laion, argilla}. Results indicate that distance computation constitutes over 90\% of the total construction time, marking it as the major bottleneck, while memory accesses and arithmetic operations contribute similarly.
The lack of spatial locality in the \method{HNSW} index necessitates random memory accesses to fetch vectors for almost every distance computation. Building \method{HNSW} on a dataset of size $n$ requires $O(n\log(n))$ distance calculations \cite{HNSW}, leading to frequent cache misses as target data is often uncached. As a result, the memory controller frequently loads data from main memory, resulting in high access latency.
Additionally, current vector data typically consists of high-dimensional floating-point numbers, occupying $4\cdot D$ bytes for dimension $D$ (e.g., 3KB for $D=768$). However, Single Instruction, Multiple Data (SIMD) registers are restricted to hundreds of bits (e.g., 128 bits for the SSE instruction set) \cite{intel_simd}, much smaller than vector sizes. Thus, each distance computation requires hundreds of memory reads to load vector segments into a register, reducing the efficiency of SIMD operations. Notably, all distances are calculated solely for comparison during HNSW construction (refer to Section \textbf{\ref{subsec: problem analysis}}). Recent studies suggest that exact distances are unnecessary for comparison \cite{ADSampling,yang2024bridging,wang2024distance}.

To address these issues, we optimize the \method{HNSW} construction process by reducing random memory accesses and effectively utilizing SIMD instructions. The theoretical analysis demonstrates that distance comparisons during index construction can be performed effectively using compact vector codes at an appropriate compression error. We initially implement mainstream vector compression methods—Product Quantization (PQ) \cite{PQ}, Scalar Quantization (SQ) \cite{LVQ}, and Principal Component Analysis (PCA) \cite{PCA}—into the \method{HNSW} construction process. However, these methods yield limited improvements in indexing efficiency or degrade search performance, as they do not align with HNSW's construction characteristics. Further observations show that they fail to reduce excessive random memory accesses and do not leverage SIMD advantages.
This motivates us to develop a speci\underline{\textbf{f}}ic compact coding strategy, a\underline{\textbf{l}}ongside memory l\underline{\textbf{a}}yout optimization for the con\underline{\textbf{s}}truction process of \underline{\textbf{H}}NSW, named \texttt{Flash}. To maximize SIMD instruction utilization, \texttt{Flash} is designed to accommodate SIMD register features, enabling concurrent execution of distance computations within the SIMD register. To minimize random memory accesses, \texttt{Flash} is effectively organized to match data access and register load patterns.
Ultimately, \texttt{Flash} avoids numerous random memory accesses and enhances SIMD usage, achieving an order of magnitude speedup in construction efficiency while maintaining or improving search performance. To the best of our knowledge, this is the first work to reach such significant speedup on modern CPUs.

To sum up, this paper makes the following contributions.

\squishlist

\item We deeply analyze the construction process of \method{HNSW} and identify the root causes of low construction efficiency on modern CPUs. Specifically, we find that distance computation suffers from high memory access latency and low arithmetic operation efficiency, which accounts for over 90\% of indexing time.

\item We propose a novel compact coding strategy, \texttt{Flash}, specifically designed for \method{HNSW} construction, optimizing index layout for efficient memory accesses and SIMD utilization. This approach improves cache hits and register-resident computation.

\item We conduct extensive evaluations on real-world datasets, ranging from ten million to billion-scale, demonstrating that construction efficiency can be improved by over an order of magnitude, while search performance remains the same or even improves.

\item We summarize three notable findings from our research: (1) a compact coding method that significantly enhances search performance may not be suitable for index construction; (2) reducing more dimensions may bring higher accuracy; and (3) encoding vectors and distances with a tiny amount of bits to align with hardware constraints may yield substantial benefits.

\squishend

The paper is organized as follows: Section \ref{sec: background} reviews related work and conducts a problem study on index construction of HNSW. Section \ref{sec: data processing} introduces the proposed novel compact coding strategy alongside the memory layout optimization for \method{HNSW} construction. Section \ref{sec: experiments} provides experimental results, while some notable findings and conclusion are outlined in Sections \ref{sec: summary} and \ref{sec: conclusion}, respectively.