\section{Related work}
\subsection{Federated Learning}
In FL, the significant computational and communication burden placed on participating devices remains a challenge. Several techniques have been proposed to mitigate this burden. To reduce communication overhead, methods such as compression____ and sketched updates____ have been explored. Other approaches address computational overhead by training only subsets of the neural network (NN), such as random subsets____ or a sliding window approach____. Progressive training has been introduced to alleviate communication and computational costs through model growth____ and successive layer training with freezing, which also reduces memory requirements____. However, the majority of these works assume training from scratch. 

Recently, fine-tuning of large language models through FL has also been considered____ using LoRA ____, such techniques are not always feasible due to memory constraints. First-order forward gradients ____ with Forward-mode automatic differentiation were proposed for finetuning LLMs ____, however, forward gradients are computationally expensive, require a larger memory footprint compared to \ac{ZO}, and the upload of trainable parameters.  



\subsection{\acl{ZO}}

Zero-order optimization has been adopted in black-box optimization due to the lack of accessible gradient information ____.

MeZO ____ builds on ZO-SGD ____ using central finite difference to estimate the gradients without backpropagation. The paper proposes a random seed trick to reduce the memory footprint by generating perturbation vectors on the fly, thus utilizing the same memory as inference and efficient storage of checkpoints. More importantly, it showed that the fine-tuning of LLMs is possible using zero-order optimization when utilizing prompt-based tuning ____.

 In \ac{FL}, FedZO ____ applied zero-order to estimate gradients, where clients send the complete model parameters to server. FedBAFFLE ____ uses zero-order for gradient estimation that requires sending a scalar per step iteration (in the FedSGD setting) and sending the complete model parameters per round. For ____, it takes at least $20$ perturbations per step to train small-scale vision models from scratch. 

 DecomFL ____ utilize the seed and model reconstruction tricks in MeZO to send only the seeds and gradient scalars communication between the server and clients in \ac{FL} instead of sending the model parameters. Their method utilizes the same seed to generate the same perturbation vectors for participating clients in a round, aggregates the project gradients at the server instead of parameters. However, with such an aggregation technique, multiple perturbations are required.