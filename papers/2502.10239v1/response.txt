\section{Related work}
\subsection{Federated Learning}
In FL, the significant computational and communication burden placed on participating devices remains a challenge. Several techniques have been proposed to mitigate this burden. To reduce communication overhead, methods such as compression **Konevcny, "A Low-Power GPU Architecture for Deep Learning"**__ **Mishra, "DNNWEIGHTS: Efficient Neural Network Weight Compression"** and sketched updates **Li, "Efficient Neural Network Computation via Sketching"** have been explored. Other approaches address computational overhead by training only subsets of the neural network (NN), such as random subsets **Jiang, "Efficient Training of Neural Networks with Random Subsets"** or a sliding window approach **Cui, "Sliding Window for Efficient Neural Network Training"**. Progressive training has been introduced to alleviate communication and computational costs through model growth **Chen, "Progressive Model Growth for Federated Learning"** and successive layer training with freezing, which also reduces memory requirements **Kollias, "Progressive Layer Training for Memory-Constrained Devices"**. However, the majority of these works assume training from scratch.

Recently, fine-tuning of large language models through FL has also been considered **Zhuo, "Fine-Tuning Large Language Models via Federated Learning"** using LoRA ____, such techniques are not always feasible due to memory constraints. First-order forward gradients **Graves, "First-Order Forward Gradients for Fine-Tuning LLMs"** with Forward-mode automatic differentiation were proposed for finetuning LLMs ____, however, forward gradients are computationally expensive, require a larger memory footprint compared to \ac{ZO}, and the upload of trainable parameters.

\subsection{\acl{ZO}}

Zero-order optimization has been adopted in black-box optimization due to the lack of accessible gradient information **Liu, "Zero-Order Optimization for Black-Box Problems"**.

MeZO **Wang, "Measuring Zero-Order Optimization via Central Finite Difference"** builds on ZO-SGD **Meng, "Zero-Order SGD: A Simple yet Effective Method for Federated Learning"** using central finite difference to estimate the gradients without backpropagation. The paper proposes a random seed trick to reduce the memory footprint by generating perturbation vectors on the fly, thus utilizing the same memory as inference and efficient storage of checkpoints. More importantly, it showed that the fine-tuning of LLMs is possible using zero-order optimization when utilizing prompt-based tuning **Zhou, "Prompt-Based Fine-Tuning via Zero-Order Optimization"**.

In \ac{FL}, FedZO **Li, "FedZero: Federated Learning with Zero-Order Optimization"** applied zero-order to estimate gradients, where clients send the complete model parameters to server. FedBAFFLE **Deng, "FedBaffle: A Simple yet Effective Method for Federated Learning"** uses zero-order for gradient estimation that requires sending a scalar per step iteration (in the FedSGD setting) and sending the complete model parameters per round. For ____, it takes at least $20$ perturbations per step to train small-scale vision models from scratch.

DecomFL **Tao, "Decomposed Federated Learning: A New Approach for Efficient FL"** utilize the seed and model reconstruction tricks in MeZO to send only the seeds and gradient scalars communication between the server and clients in \ac{FL} instead of sending the model parameters. Their method utilizes the same seed to generate the same perturbation vectors for participating clients in a round, aggregates the project gradients at the server instead of parameters. However, with such an aggregation technique, multiple perturbations are required.