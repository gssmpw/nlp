\section{Results}
We compare our proposed \ac{METHOD} with state-of-the-art first-order backpropagation and zero-order \ac{FL} methods in terms of memory, accuracy, computation, and communication. Furthermore, we conduct an ablation study on the impact of the splitting technique of our \ac{METHOD}.  


\subsection{Experimental Setup}

We evaluated the proposed \ac{FL} fine-tuning (with prompt-based fine-tuning) on four tasks, namely SST2 \cite{sst2_cite}, RTE \cite{rte_cite}, WIC \cite{wic_cite}, and BOOLQ \cite{boolq_cite}. We consider 100 clients for SST2, and 20 clients for the other datasets (due to the smaller dataset sizes). We sample 10\% of the clients to participate in a single \ac{FL} round, where a client takes $20$ local steps per round. We use RoBERTa-large model \cite{roberta_cite}  for evaluation.  For \ac{METHOD}, we split the network such that $f_{2}$ includes the RoBERTaLMHead and $f_{1}$ includes the previous layers. We empirically determine the values of $P_{1}=2$ and $P_{2}=8$ by testing various combinations and selecting those that achieve the highest computational savings. Further experimental settings are in \cref{appendix:implementain_details}.



\subsection{Comparison with First-order Methods}

\paragraph{Memory Evaluation:} 

In \cref{fig:memory_fo}, we compare the memory footprint of \ac{METHOD} with FedAvg and FedAvg with LoRA FedAvg(LoRA) to finetune RoBERTa-Large using two different context lengths of $32$ and $256$. For FedAvg and FedAvg(LoRA) we consider vanilla SGD with no optimizer states. For FedAvg, the memory consumption is $ \SI{2.9}{\giga \byte}$ and $\SI{4.9}{\giga \byte}$ respectively, which exceed the available memory on many edge devices. Another memory-efficient candidate for training using backpropagation is LoRA, where only the gradients of low-rank matrices are stored in memory instead of model parameters. LoRA with backpropagation achieves competitive memory saving in the small context-length scenario; however, it suffers in the long context-length scenario as the cost of intermediate activations increases. It therefore requires $\SI{3.9}{\giga \byte}$ of memory for long context-length. For \ac{METHOD}, it consumes a peak memory of only $\SI{1.4}{\giga \byte}$ and $\SI{1.75}{\giga \byte}$ over the two context lengths. 



\begin{figure}
    \centering
    \includegraphics[]{Memory.pdf}
    \caption{Memory footprint for training RoBERTa-Large context length $32$ and $256$ using First-order FedAvg and FedAvg(LoRA) with backpropagation using batch size of $4$, and \ac{METHOD} using batch size $8$.}
    \label{fig:memory_fo}
\end{figure}


\paragraph{Computation and Communication Evaluation}
Since using LoRA presents a good baseline for small context-length datasets, we present a comparison of computation and upload between \ac{METHOD} and federated finetuning using LoRA (FedAvg(LoRA)) on \cref{fig:lora}. FedAvg(LoRA) records $7.8\times$ less computation compared to \ac{METHOD}. However, in terms of upload, despite the fact that LoRA uploads only low-rank modules, it falls behind with a $7307\times$ gap to \ac{METHOD}.

In summary, \ac{METHOD} is preferred when there are memory and communication constraints, however, \ac{PEFT} techniques like LoRA are still better when such constraints do not exist. 



\begin{figure}
    \centering
    \includegraphics[]{LORA_COMP.pdf}
    \includegraphics[]{LORA_COMM.pdf}
    \caption{Computation and upload comparison with \ac{METHOD}(ours) and Federated finetuning for LoRA FedAvg(LoRA) over SST2 dataset.}
    \label{fig:lora}    
\end{figure}






\subsection{Main Results: Comparison with Zero-order Methods}

 We compare our \ac{METHOD} with zeroth-order DecomFL \cite{decomfl_cite} and FedZO \cite{fang2022communication}. Both approaches utilize the forward difference where for DecomFL, we set $P=10$ equal to the original paper setting. For FedZO, the paper considered training from scratch while using high $P$ (e.g. 20 and more), however, we found this number to be not required for the fine-tuning scenario. We did a grid search and selected $P=5$ for FedZO as it provides a better option. Forward difference requires less computation compared to central difference since the network is only perturbed in one direction, where the forward cost is $\text{zo-fw}_{\text{\acsp{FLOP}}} = \text{fw}_{\text{\acsp{FLOP}}} + (\text{fw}_{\text{\acsp{FLOP}}} \times P)$ and the perturbation cost $\text{zo-p}_{\text{\acsp{FLOP}}} = \text{p}_{\text{\acsp{FLOP}}} \times 2 \times P$, using the seed trick. 
 
 

\begin{figure*}[t!]
    \centering
    \includegraphics[]{Comp1.pdf}
    \includegraphics[]{Comp2.pdf}
    \includegraphics[]{Comp3.pdf}
    \includegraphics[]{Comp4.pdf}
    \caption{Computation vs test loss comparison for \ac{METHOD} (ours) and other zero-order \ac{FL} methods. \ac{METHOD} records up to $6.7\times$ and $7\times$ less computation compared to FedZO and DecomFL respectively.}
    \label{fig:computation_results}
\end{figure*}


\paragraph{Computation Evaluation:}
In \cref{fig:computation_results}, we present the cumulative computation to test loss for \ac{METHOD}, FedZO and DecomFL. We compare the computation that is required for \ac{METHOD} to reach the lowest recorded loss for the other DecomFL and FedZO. 

In comparison with DecomFL, \ac{METHOD} utilizes  $6\times$, $6.7\times$, $3.8\times$ and $2.5\times$ less G\acp{FLOP} on SST2, BOOLQ, RTE, and WIC, respectively. This gain of improvement is divided into two folds. \ac{METHOD} utilized $1.93\times$ less computation per round, while converging in fewer rounds compared to DecomFL given the advantages of the exact model reconstruction technique and the more accurate gradient estimation of \ac{METHOD}.  

Furthermore, compared to FedZO, \ac{METHOD} utilizes $7.18\times$, $4.4\times$, $2.4\times$, and $4.8\times$ less computation in SST2, BOOLQ, RTE, and WIC. The computation cost per round is approximately equal (i.e. FedZO use more $1.04\times$ computation). The large improvement is because \ac{METHOD} convergences much faster than FedZO due to the use of central difference and the proposed split perturbation technique. 


\paragraph{Communication Evaluation:}
We compare the total upload cost for zero-order approaches as provided in \cref{table:communication_comparison}. FedZO sends the complete network parameters per round; therefore, given its slow convergence, the total upload in the \ac{FL} training process is in order of \SI{}{\tera \byte}. 

In the simple implementation that we provided for \ac{METHOD}, the client uploads $2K$ scalar gradients and $P_{1}*K$ and $P_{2}*K$ seeds. As discussed, since the server sends the starting pseudo seed generator, we record \ac{METHOD} where only the scalar gradients are sent. Similarly, DecomFL also sends only the gradient scalars. Compared to DecomFL, \ac{METHOD} sends two gradient scalars for $\theta_{1}$ and $\theta_{2}$ instead of one for $\theta$ per step, while due to the faster convergence, of \ac{METHOD}, it records lower upload cost. In summary, the lower communication cost of sending only scalars is negligible to sending the model parameters. 



\paragraph{Accuracy Evaluation:}
\input{accuracy}
In \Cref{table:accuracy_comparison}, we present a comparison of the accuracy achieved by our method with FedZO and DecomFL. Our approach demonstrates slightly higher accuracies than both FedZO and DecomFL on various datasets. The difference between accuracy is mainly because FedZO and DecomFL use forward difference that is less accurate than central difference.  Furthermore, compared to FedAvg (which serves as a theoretical reference of the upper bound), \ac{METHOD} shows a decrease in accuracy compared to FedAvg ranging between $2.4\%$ and $18.1\%$ across datasets.



\paragraph{Memory Evaluation:}

We consider both FedZO and DecomFL to directly apply the memory reduction seed trick proposed by MeZO \cite{malladi2023fine} as they do not require activation reuse, so that they can train these large models. In such a scenario, FedZO and DecomFL would consume a peak memory of $\SI{1.4}{\giga \byte}$ and $\SI{1.75}{\giga \byte}$ over the two context lengths. For \ac{METHOD}, we only require saving the output activation before the second block, which in this scenario required only $\SI{1}{\mega \byte}$ and $\SI{8}{\mega \byte}$ (i.e., increasing the memory only by $0.07\%$ and $0.44\%$ compared to MeZO). 
 


    
\paragraph{Aggregation Design:}
\label{subsec:round_design}
To distinguish between DecomFL and our approach for aggregation, we unify the client training for both methods and compare over the SST2 dataset. Specifically, for both methods, we use the central difference to estimate the gradient across the entire network while setting $P$ to $1$. In this comparison, we refer to our reconstruction and aggregation technique as \ac{METHOD}$^*$ and to DecomFL's aggregation as DecomFL$^*$.

DecomFL aggregates the scalar gradients sent from the participating clients and then updates the model parameters over $K$ steps to obtain the model for the next round, while our technique reconstructs each client model and then performs the aggregation. Furthermore, DecomFL sends the same seed to all clients per round. As depicted in \cref{fig:DecomFL_zo}, DecomFL$^*$ aggregation technique converges to a worse loss compared to \ac{METHOD}$^*$ due to the misalignment between the model's states and computed gradients.


\begin{figure}
    \centering
    \includegraphics[]{aggregation.pdf}
    \caption{Comparison between test loss achieved by our upload and aggregation method without model splitting (\ac{METHOD}$^{*}$) and DecomFL$^*$. We set both approaches to use \ac{ZO} with central difference and $P=1$ over SST2 dataset. }
    \label{fig:DecomFL_zo}
\end{figure}


\input{communication}





\begin{figure*}
    \centering
    \includegraphics[]{Rounds1.pdf}
    \includegraphics[]{Rounds2.pdf}
    \includegraphics[]{Rounds3.pdf}
    \includegraphics[]{Compute1.pdf}
    \includegraphics[]{Compute2.pdf}
    \includegraphics[]{Compute3.pdf}
    \caption{Computation vs test loss comparison between \ac{METHOD} (ours) and using ZO without model splitting for $P=1$ and $P=4$ for client training. The three methods use the same \ac{FL} design of \ac{METHOD}. }
    \label{fig:mezofl_ours_computation}    
\end{figure*}

\subsection{Ablation Study}
%\begin{comment}
To assess the impact of the model splitting that our \ac{METHOD} adopts, we compare it with training the network using the same \ac{FL} design in \cref{sec:server_design} without model splitting. We consider the default setting for training the network using $P=1$ and a higher number of perturbations, that is, $P=4$. As mentioned before, we use $P_{1}=2$ and $P_{2}=8$ as the setting for \ac{METHOD}.
In \cref{fig:mezofl_ours_computation}, we provide the rounds and computation over the three datasets of SST2, BOOLQ, and RTE.

Compared to $P=1$, \ac{METHOD} achieves faster convergence with $6.7\times$, $5.5\times$ and $6.9\times$ fewer communication rounds on SST2, BOOLQ and RTE. This faster convergence results in $2.3\times$, $1.9\times$, and $2.4\times$ less computation. 

Moreover, \ac{METHOD} requires $1.8\times$, $1.2\times$, and $1.9\times$ fewer communication rounds compared to $P=4$. Since using $P=4$ requires $1.4\times$ more computation per round than \ac{METHOD}, \ac{METHOD} utilizes $2.5\times$, $1.7\times$, and $2.6\times$ less computation over SST2, BOOLQ, and RTE respectively.  

To summarize, \ac{METHOD} reduces the computation cost for the \ac{FL} training compared to training applying perturbations to the whole network with less than $0.5\%$ memory overhead.   


