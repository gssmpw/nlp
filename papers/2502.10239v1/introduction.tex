\section{Introduction}

LLMs are demonstrating high performance in various machine learning tasks, including next-token prediction and text classification. Despite their popularity, these models are very costly to train in terms of required data, computation, and memory resources. The current trend is thus to train such large models over the coarse of few months using highly specialized hardware and large set of "public" data and then finetune them over specific tasks in hand using transfer learning techniques. However, this demands access to the finetuning data, which might be only available at the edge devices/users due to privacy requirements. Finetuning in a federated \cite{mcmahan2017communication} manner on the edge offers adaptability to the data available on these devices while reducing the risk of private data exposure.

Although it offers less training time compared to training from scratch, finetuning is still a computationally demanding task, especially in terms of memory usage. It is hence of most importance to reduce the resource footprint of finetuning, as many edge devices, e.g. IoT devices, have very limited resources to offer \cite{Survey_2, pfeiffer2023federated}. \ac{PEFT} techniques such as LoRA~\cite{hu2022lora} reduce the memory requirements for storing gradients. However, this still results in a large memory footprint for inputs with long context length. For example, finetuning RoBERTa-large \cite{roberta_cite} on a dataset with context length $256$ requires memory about $\SI{4}{\giga \byte}$. This is because the activations should still be stored in memory to perform the backpropagation. 

Recently, MeZO \cite{malladi2023fine} showed that it is possible to finetune language models using \ac{ZO} ~\cite{spsa_cite} with an inference-like memory footprint, utilizing pseudo-random seeds to generate perturbation vectors. 

In this paper, we adopt \ac{ZO} to finetune language models at edge devices. We consider a \ac{FL} setting, where a group of edge devices are collectively finetuning the model using their local data. While \ac{ZO} reduces the memory footprint, this reduction comes with a cost: noisy gradient updates that could affect the accuracy and convergence of the training. In \ac{FL}, \cite{decomfl_cite, fang2022communication} use a large number of perturbations to improve the quality of gradient estimation at the cost of high computation. 



We propose \acl{METHOD} (FedSPZO), which applies a different number of perturbations to different layers of the model, reducing computational overhead and increasing the convergence speed.
In addition, we adopt the seed trick proposed in \cite{malladi2023fine} to reduce communication overhead in our setting. Our work differs from DecomFL \cite{decomfl_cite}, as we use a more accurate approach to merge updates on servers and apply a more efficient perturbation technique, significantly improving the finetuning process.   


%\input{figures/ZO_FL_TIKZ}


Our main contributions are:
\begin{itemize}
    \item We propose \ac{METHOD}, a novel approach that reduces the total computation for the zero-order \ac{FL} finetuning process, with an efficient communication design.
    \item Our evaluation demonstrates reduction in total computation by up to $7\times$ compared to existing state-of-the-art zero-order FL methods with fewer communication rounds while achieving higher accuracy with minimal memory overhead (less than $0.5\%$).
    \item We conduct a comprehensive ablation study to show the effectiveness of the \ac{METHOD} technique. 
\end{itemize}


