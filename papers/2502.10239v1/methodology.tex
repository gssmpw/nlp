
\section{Methodology}
\begin{figure*}[t!]
    \centering
    \includegraphics[]{FedSPZOFig.pdf}
    \caption{An overview of the proposed \acl{METHOD} (FedSPZO) round.}
    \label{fig:FLDesign}
\end{figure*}

\subsection{Background on \acl{ZO}}

\acl{ZO} reduces the memory footprint compared to first-order backpropagation, as it does not require the storage of the activations in memory. Furthermore, utilizing the seed trick proposed by \cite{malladi2023fine}, it is possible to maintain an inference-like memory footprint essential to edge devices participating in federated learning. Given model parameters $\theta$, loss $L$, and input batch $\mathcal{B}$,  we define the \textit{scalar} projected gradient (difference between perturbed losses)  $g$ as follows:
\begin{equation}
g = \frac{L (\theta + \epsilon z ; \mathcal{B}) - L(\theta - \epsilon z; \mathcal{B})}{2  \epsilon}  
\end{equation}
where $\epsilon$ is the perturbation scale, and $z$ is sampled from  $\mathcal{N}(0, I_{d})$, where $I_{d}$ is the identity matrix and $d$ is the number of parameters. By resetting the pseudo-random generator to seed $s$, each element of the perturbation vector $z$ can be generated to perturb the corresponding parameter in place. This perturbation is applied to the model before a forward pass and is reused to compute the gradient by multiplying the scalar $g$ and update parameters using the SGD rule. This approach eliminates the need to store pseudo-random perturbations in memory, as they can be regenerated on demand.
The previous gradient estimation presents a single perturbation $P=1$.% (that is, an evaluation of one perturbation vector).




\subsection{\acl{METHOD}(FedSPZO)}

We propose \ac{METHOD}, a federated learning framework that utilizes zero-order optimization with a huge reduction in upload communication cost, as discussed in \cref{sec:server_design}.  Primarily, \ac{METHOD} aims to address the computational challenges arising from \ac{ZO}. We analyze its computation complexity and introduce an approach that divides the network into two consecutive blocks, while utilizing different number of perturbations per block to accelerate computation and convergence. Let $y$ be the output of a model, we define the first and second blocks $f_{1}$ and $f_{2}$ as follows:  
\begin{equation}
    y = f_{2}(\theta_{2}; f_{1}(\theta_{1}; \mathcal{B}))
\end{equation}
where $\theta_{1}$ and $\theta_{2}$ are the parameters for each block respectively. We discuss this in detail in \cref{subsec:client_trianing}. 



\subsubsection{\ac{METHOD} Server}
\label{sec:server_design}
We first discuss the proposed \ac{FL} design. In each round $r$,  the server starts by broadcasting the model parameters $\theta^{r}$ and the number of perturbations for each block (i.e., $P_{1}$ and $P_{2}$). 
Each client trains the model for the $K$ steps on its local data, as we discuss in detail in \cref{subsec:client_trianing}. After a client finishes its steps, it uploads only the vectors of projected gradients $G$ and seeds $S$ used in the training for each block, which are independent of the model parameters.

The server is responsible for reconstructing the exact model for each client after $K$ steps by regenerating the perturbation vectors, using the seeds $S^{c}$  provided by the client $c$. It performs the same parameter updates using the learning rate $\mu$ and the gradients computing the same parameter updates using $\mu$, $P$ and $G$ as illustrated in \Cref{alg:fl_server}. Importantly, this reconstruction does not require carrying out any forward pass or access to the training data on the device.  Finally, the server averages all the clients models similar to FedAvg and starts another round. The proposed round design is provided in \Cref{fig:FLDesign}.



  

\input{fl_algorithm}

\subsubsection{\acs{METHOD} Client}
\label{subsec:client_trianing}
\begin{comment}
\begin{figure}[tb!]
    \centering
    \input{figures/slow_convergence_fig}
    \caption{Rounds to converge for FedAvg and ZO-FL for finetuning SST2 dataset. zero-orderrequires a small learning rate to converge which results in slower converge to first-order SGD ($\approx 40\times$). }
    \label{fig:slow_convergence}
\end{figure}
\end{comment}

%\subsubsection{Computation}
\acl{ZO} requires a small learning rate $\mu$ to convergence to a good solution. However, training with a small learning rate requires many rounds compared to its first-order counterpart, as the gradient norm of zero-order scales with the Hessian effective rank of the loss \cite{malladi2023fine}. 


Increasing the number of perturbations $P$ reduces the noise in the gradient approximation, enabling a larger learning rate. However, this requires $P$ times an increase in computation cost per step.  To break this down, the computational cost in \acp{FLOP} of a zero-order step with central difference (when $z$ is regenerated given seed $s$) can be expressed as the sum of the ZO forward ($\text{zo-fw}_{\text{\acsp{FLOP}}}$), perturbation computation ($\text{zo-p}_{\text{\acsp{FLOP}}}$), and update ($\text{zo-u}_{\text{\acsp{FLOP}}}$) \acp{FLOP}. Let $\text{fw}_{\text{\acsp{FLOP}}}$ represent \acp{FLOP} for doing one forward pass (inference) on the input, $\text{zo-fw}_{\text{\acsp{FLOP}}}$ can be defined as follows:

\begin{equation}
    \text{zo-fw}_{\text{\acsp{FLOP}}}= 2 \times \text{fw}_{\text{\acsp{FLOP}}} \times P
\end{equation}

while given that the cost of one perturbation of the parameter is $\text{p}_{\text{\acsp{FLOP}}}$ and update $\text{u}_{\text{\acsp{FLOP}}}$, the costs for $\text{zo-p}_{\text{\acsp{FLOP}}}$ and $\text{zo-u}_{\text{\acsp{FLOP}}}$ are: 

\begin{equation}
    \text{zo-p}_{\text{\acsp{FLOP}}} = \text{p}_{\text{\acsp{FLOP}}} \times 3 \times P 
\end{equation}
\begin{equation}
    \text{zo-u}_{\text{\acsp{FLOP}}} = \text{u}_{\text{\acsp{FLOP}}} \times P
\end{equation}

To reduce this high computational cost, we propose \ac{METHOD} that inherit the fast convergence characteristics when using a large $P$,  with fewer computations. Specifically, \ac{METHOD} divides the network into two blocks of subsequent layers, each utilizing perturbations $P_{1}$ and $P_{2}$. The parameters in the first block $\theta_{1}$ are perturbed in the first positive direction of $z_{1}$ using seed $s_{1}$. Then a forward pass outputs $+y_{l}$, where $+y_{l} = f_{1}(\theta_{1} + \epsilon z_{1}; \mathcal{B})$ and $l$ indicates the index of the last layer in $f_{1}$. The second block is perturbed in the two directions $\pm z_{2}$, where $z_{2}$ is sampled using seed $s_{2}$. For each direction, $f_{2}$ takes only $+y_{l}$ and computes its loss. This process for $f_{2}$ is repeated $P_{s}$ times. The procedure mentioned is repeated for the negative direction ($-z_{1}$) of $f_{1}$ to output $-y_{l}$. $f_{2}$ is then perturbed using a different $\pm z_{2}$(s) than the ones used before, and the losses given $-y_{l}$ are computed. \footnote{In the case where a parameter is shared between $f_1$ and $f_2$, we perturb this parameter only in $\theta_{1}$ such that it is only included in the forward pass of $f_{2}$ but not included in $\theta_{2}$.} 


The zero-order gradient for $f_{2}$ is calculated as follows:

\begin{equation}
    \nabla L(\theta_{2}; \mathcal{Y}) = \frac{1}{P_{2}} \sum_{j=1}^{P_{2}} g_{2_{j}} \cdot z_{2_{j}}
\end{equation}
where $\mathcal{Y}$ = $\{\pm y_{l}^{1}, \dots, \pm y_{l}^{P_{1}}\}$, that is the outputs of perturbed $f_{1}$ with $\{\pm z_{1_{1}}, \dots, \pm z_{1_{P_{1}}}\}$ given $\mathcal{B}$.  For $f_{1}$, let $\text{L}^{+}$ and  $\text{L}^{-}$ be two vectors containing all the losses from $f_{2}$ over single a perturbation over $f_{1}$, the scalar projected gradient $g_{1}$ is calculated as follows:
\begin{equation}
g_{1} = \frac{1}{4 P_{s}^{2}} \sum_{k=0}^{2P_{s}} \sum_{l=0}^{2P_{s}} \big( \frac{\text{L}^{+} (l) - \text{L}^{-} (k)} {2\epsilon} \big)
\label{eq:avg_grad}
\end{equation}
and the gradient for $\theta_{1}$ with $P_{1}$ is: 
\begin{equation}
    \nabla L(\theta_{1}; \mathcal{B}) = \frac{1}{P_{1}} \sum_{j=1}^{P_{1}} g_{1_{j}} \cdot z_{1_{j}}
\end{equation}

The proposed design enables two-fold benefits. For the layers in the first block, it reduces the noise propagated from the perturbations of the last layers by incorporating the difference in loss values computed using the larger perturbation, allowing for a better estimate of the gradient along the sampled direction $z_{1}$. Secondly, the use of a higher number of perturbations is enabled with a reduced computational cost due to the reuse of the output of $f_{1}$. Let the $\text{fw}^{1}_{\text{\acsp{FLOP}}}$ and $\text{fw}^{2}_{\text{\acsp{FLOP}}}$ be the inference \acp{FLOP} of the two blocks, the computation cost for a step is as follows:
\begin{equation}
    \text{zo-fw}_{\text{\acsp{FLOP}}} = 2 \times \text{fw}^{1}_{\text{\acsp{FLOP}}} \times P_{1} + 2 \times \text{fw}^{2}_{\text{\acsp{FLOP}}} \times P_{2}
\end{equation}
with the cost for perturbations and updates are divides similarly. 


In \Cref{alg:fl_client}, we provide the \ac{FL} client training process. %Let $\theta^{[:\mathcal{L}_c]}$ indicate the parameters till $\mathcal{L}_c$ and $\theta^{[\mathcal{L}_{c}:N]}$ after $\mathcal{L}_{c}$.     
For each step $k$, the client samples a batch and starts perturbation iterations.  For a single perturbation $P_{1}$, the forward process discussed in the two directions is carried out, where a shift is applied for $f_{2}$'s seed of the negative direction to generate different $z$ to the positive one. The client then updates each block by regenerating $z$ of each parameter and multiplying it by $g$ to compute the gradient of the parameter and update it. The gradient scalars and seed are stored in $G_{1}, G_{2}$ and $S_{1}, S_{2}$ respectively. After the $K$ steps, the client finally uploads $G_{1},  G_{2}, S_{1}$ and $S_{2}$ to the server.





\input{fl_client_2}


As discussed, our proposed method utilizes zero-order and gradient reconstruction given seed \cite{malladi2023fine} to achieve inference-level memory (without gradients and activation cost). The proposed method might add minimal memory overhead, requiring storage of only the activation of the cutoff layer ($y_{l}$). The peak memory for a training step is  
$d$ + max( max($\{y_{1}, \dots, y_{l-1}\}$), $y_{l}$ + max($\{y_{1+1}, \dots, y_{N}\}))$, where $y_{i}$ is the layer output at index $i$ and $N$ is the number of layers.

\paragraph{\ac{METHOD} Communication Extension}

For ease of readability, we presented the implementation in which the client sends both gradient scalars and seeds. Since $S_{1}$ and $S_{2}$ are derived using a pseudo-random seed, the server can independently regenerate them by sampling and sending different initial seed to each client to start with. This eliminates the need for the client to upload the seeds.


