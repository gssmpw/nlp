\section{Related work}


\subsection{Federated Learning}
In FL, the significant computational and communication burden placed on participating devices remains a challenge. Several techniques have been proposed to mitigate this burden. To reduce communication overhead, methods such as compression~\cite{thakker2019compressing} and sketched updates~\cite{shi2020communication} have been explored. Other approaches address computational overhead by training only subsets of the neural network (NN), such as random subsets~\cite{caldas2018expanding} or a sliding window approach~\cite{alam2022fedrolex}. Progressive training has been introduced to alleviate communication and computational costs through model growth~\cite{wang2022progfed} and successive layer training with freezing, which also reduces memory requirements~\cite{pfeiffer2023successive}. However, the majority of these works assume training from scratch. 

Recently, fine-tuning of large language models through FL has also been considered~\cite{babakniya2023slora} using LoRA \cite{hu2022lora}, such techniques are not always feasible due to memory constraints. First-order forward gradients \cite{baydin2022gradients} with Forward-mode automatic differentiation were proposed for finetuning LLMs \cite{spry_cite, FwdLLM}, however, forward gradients are computationally expensive, require a larger memory footprint compared to \ac{ZO}, and the upload of trainable parameters.  



\subsection{\acl{ZO}}

Zero-order optimization has been adopted in black-box optimization due to the lack of accessible gradient information \cite{blackbox_1, blackbox_2}.

MeZO \cite{malladi2023fine} builds on ZO-SGD \cite{spsa_cite} using central finite difference to estimate the gradients without backpropagation. The paper proposes a random seed trick to reduce the memory footprint by generating perturbation vectors on the fly, thus utilizing the same memory as inference and efficient storage of checkpoints. More importantly, it showed that the fine-tuning of LLMs is possible using zero-order optimization when utilizing prompt-based tuning \cite{gao-etal-2021-making}.

 In \ac{FL}, FedZO \cite{fang2022communication} applied zero-order to estimate gradients, where clients send the complete model parameters to server. FedBAFFLE \cite{baffle} uses zero-order for gradient estimation that requires sending a scalar per step iteration (in the FedSGD setting) and sending the complete model parameters per round. For \cite{baffle, fang2022communication}, it takes at least $20$ perturbations per step to train small-scale vision models from scratch. 

 DecomFL \cite{decomfl_cite} utilize the seed and model reconstruction tricks in MeZO to send only the seeds and gradient scalars communication between the server and clients in \ac{FL} instead of sending the model parameters. Their method utilizes the same seed to generate the same perturbation vectors for participating clients in a round, aggregates the project gradients at the server instead of parameters. However, with such an aggregation technique, multiple perturbations are required.

