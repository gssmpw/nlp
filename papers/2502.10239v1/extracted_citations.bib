@InProceedings{alam2022fedrolex,
  author={Alam, Samiul and Liu, Luyang and Yan, Ming and Zhang, Mi},
  title={FedRolex: Model-Heterogeneous Federated Learning with Rolling Sub-Model Extraction},
  booktitle={Advances in Neural Information Processing Systems},
  volume={35},
  pages={29677--29690},
  year={2022}
}

@article{baffle,
  title={Does federated learning really need backpropagation},
  author={Feng, Haozhe and Pang, Tianyu and Du, Chao and Chen, Wei and Yan, Shuicheng and Lin, Min},
  journal={arXiv preprint arXiv:2301.12195},
  year={2023},
  publisher={May}
}

@article{baydin2022gradients,
  title={Gradients without backpropagation},
  author={Baydin, At{\i}l{\i}m G{\"u}ne{\c{s}} and Pearlmutter, Barak A and Syme, Don and Wood, Frank and Torr, Philip},
  journal={arXiv preprint arXiv:2202.08587},
  year={2022}
}

@inproceedings{blackbox_1,
 author = {Nikolakakis, Konstantinos and Haddadpour, Farzin and Kalogerias, Dionysis and Karbasi, Amin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {31525--31541},
 publisher = {Curran Associates, Inc.},
 title = {Black-Box Generalization: Stability of Zeroth-Order Learning},
 volume = {35},
 year = {2022}
}

@inproceedings{blackbox_2,
  title={A zeroth-order block coordinate descent algorithm for huge-scale black-box optimization},
  author={Cai, HanQin and Lou, Yuchen and McKenzie, Daniel and Yin, Wotao},
  booktitle={International Conference on Machine Learning},
  pages={1193--1203},
  year={2021},
  organization={PMLR}
}

@Article{caldas2018expanding,
  author        = {Caldas, Sebastian and Kone{\v{c}}ny, Jakub and McMahan, H Brendan and Talwalkar, Ameet},
  title         = {Expanding the reach of federated learning by reducing client resource requirements},
  journal       = {arXiv:1812.07210},
  year          = {2018},
}

@article{decomfl_cite,
  title={Achieving Dimension-Free Communication in Federated Learning via Zeroth-Order Optimization},
  author={Li, Zhe and Ying, Bicheng and Liu, Zidong and Dong, Chaosheng and Yang, Haibo},
  journal={arXiv preprint arXiv:2405.15861},
  year={2024}
}

@article{fang2022communication,
  title={Communication-efficient stochastic zeroth-order optimization for federated learning},
  author={Fang, Wenzhi and Yu, Ziyi and Jiang, Yuning and Shi, Yuanming and Jones, Colin N and Zhou, Yong},
  journal={IEEE Transactions on Signal Processing},
  volume={70},
  pages={5058--5073},
  year={2022},
  publisher={IEEE}
}

@inproceedings{gao-etal-2021-making,
    title = "Making Pre-trained Language Models Better Few-shot Learners",
    author = "Gao, Tianyu  and
      Fisch, Adam  and
      Chen, Danqi",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    publisher = "Association for Computational Linguistics",

}

@inproceedings{hu2022lora,
title={Lo{RA}: Low-Rank Adaptation of Large Language Models},
author={Edward J Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
booktitle={International Conference on Learning Representations},
year={2022},
}

@article{malladi2023fine,
  title={Fine-tuning language models with just forward passes},
  author={Malladi, Sadhika and Gao, Tianyu and Nichani, Eshaan and Damian, Alex and Lee, Jason D and Chen, Danqi and Arora, Sanjeev},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={53038--53075},
  year={2023}
}

@inproceedings{pfeiffer2023successive,
 author = {Pfeiffer, Kilian and Khalili, Ramin and Henkel, Joerg},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {35386--35402},
 publisher = {Curran Associates, Inc.},
 title = {Aggregating Capacity in FL through Successive Layer Training for Computationally-Constrained Devices},
 volume = {36},
 year = {2023}
}

@article{shi2020communication,
  title={Communication-efficient edge AI: Algorithms and systems},
  author={Shi, Yuanming and Yang, Kai and Jiang, Tao and Zhang, Jun and Letaief, Khaled B},
  journal={IEEE Communications Surveys \& Tutorials},
  volume={22},
  number={4},
  pages={2167--2191},
  year={2020},
  publisher={IEEE}
}

@article{spry_cite,
  title={Thinking Forward: Memory-Efficient Federated Finetuning of Language Models},
  author={Panchal, Kunjal and Parikh, Nisarg and Choudhary, Sunav and Zhang, Lijun and Brun, Yuriy and Guan, Hui},
  journal={arXiv preprint arXiv:2405.15551},
  year={2024}
}

@article{spsa_cite,
  title={Multivariate stochastic approximation using a simultaneous perturbation gradient approximation},
  author={Spall, James C},
  journal={IEEE transactions on automatic control},
  volume={37},
  number={3},
  pages={332--341},
  year={1992},
  publisher={IEEE}
}

@article{thakker2019compressing,
  title={Compressing rnns for iot devices by 15-38x using kronecker products},
  author={Thakker, Urmish and Beu, Jesse and Gope, Dibakar and Zhou, Chu and Fedorov, Igor and Dasika, Ganesh and Mattina, Matthew},
  journal={arXiv:1906.02876},
  year={2019}
}

@inproceedings{wang2022progfed,
  title={ProgFed: effective, communication, and computation efficient federated learning by progressive training},
  author={Wang, Hui-Po and Stich, Sebastian and He, Yang and Fritz, Mario},
  booktitle={International Conference on Machine Learning},
  pages={23034--23054},
  year={2022},
  organization={PMLR}
}

