\appendix
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Implementation Details}
\label{appendix:implementain_details}

We use Pytorch for our implementation. We used RoBERTA-Large CasualLLM weights from the HuggingFace library. For the experiments, we train for a maximum of $5000-15000$ rounds. We use the same train/test split of the datasets, where only the train split is divided across clients and test set used for evaluation.    


For the zero-order methods over SST2, we experimented with learning rates of $\{1e-5, 8e-6, 6e-6, 5e-6, 4e-6, 2e-6, 1e-6\}$ and $\{5e-6, 2.5e-6, 1e-6, \dots, 1e-7\}$ for the rest of the dataset. We set $\epsilon$ to $1e-3$ for all the methods (same as the papers) and found that adjusting it does not lead improvement and may lead to divergence. For all \ac{ZO}, an effective batch size of 16 is used.

For LoRA on SST2, we use rank $r=8$ and $\alpha = 16$ and learning rate $\{5e-4,1e-4\}$. We apply LoRA to the query and value of attention layers. 

We measure \acp{FLOP} for training (including zero-order and first-order) using the Pytorch profiler. 
