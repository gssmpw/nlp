\section{Conclusion}
In this work, we study the federated fine-tuning of language models. We propose \ac{METHOD}, a novel approach that splits the model into two blocks while applying a different number of perturbations for each block, reducing the computational overhead of applying multiple perturbations while improving convergence. Furthermore, in \ac{METHOD}, the client sends only scalar gradients, reducing the communication cost. 

Our experimental results demonstrate substantial computational efficiency, achieving reductions of up to $7\times$ compared to existing state-of-the-art zeroth-order \ac{FL} methods, while also achieving higher accuracy. When compared to first-order \ac{FL} with backpropagation, \ac{METHOD} offers advantages in memory efficiency and reduced upload bandwidth but exhibits lower computational efficiency and accuracy. A limitation of our method is that it introduces an additional hyperparameter, requiring the tuning of ($P_{1}$, $P_{2}$) instead of a single $P$, warranting further investigation into optimal tuning strategies.

For future work, we plan to explore zero-order on inference-only accelerators with lower precision. This direction aims to bridge the computational efficiency gap with backpropagation methods, further enhancing the practicality of \ac{METHOD} in resource-constrained environments.