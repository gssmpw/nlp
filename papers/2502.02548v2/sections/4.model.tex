\input{figures/architecture}
\section{\nickname Model Training}
We train the 3D open-vocabulary segmentation model based on \dataname using two-stage training: per-point language alignment (Sec.~\ref{subsec:backbone}), and mask decoder training that predicts instances from these aligned features (Sec.~\ref{subsec:mask_decoder}).

\subsection{\nickname: Language-Aligned 3D Encoder}
\label{subsec:backbone}

\noindentbold{Architecture}
We use U-shaped sparse convnets~\cite{graham20183d,mink} for our backbone due to their efficiency and scalability.
Given a point cloud $\mathbf{P} = \{\mathbf{p}_i\}_{i=1}^{N},\mathbf{p}_i \in \mathbb{R}^3$, the network $\mathcal{E}^{\text{3D}}(\cdot)$ outputs per-point features $\mathcal{E}^{\text{3D}}(\mathbf{P}) = \{\mathbf{z}^{\text{3D}}_i\}_{i=1}^N, \mathbf{z}^{\text{3D}}_i \in \mathbb{R}^D$.

\noindentbold{Training objective}
To align the geometric embeddings with language semantics, we employ a contrastive learning framework~\cite{yang2024regionplc,ding2022pla}.
Given 3D mask-text pairs $\{(\mathbf{s}_k, c_k)\}_{k=1}^K$, a pre-trained text encoder $\mathcal{E}^{\text{text}}(\cdot)$ computes text embeddings $\mathbf{z}^{\text{text}}_k \in \mathbb{R}^D$ for each caption.
The similarity scores between point features and text embeddings are averaged using the region masks to weigh all regions equally.
The final training objective is:
\begin{equation}
    \label{eq:caption_loss}
    \begin{aligned}
        \mathcal{L}_{\mathrm{point}} &= -\frac{1}{K}\sum_{k=1}^{K}\sum_{i=1}^{N}(\mathbf{s}_k)_i\log\frac{\exp(\mathbf{z}^{\text{3D}}_i\cdot\mathbf{z}^{\text{text}}_k/\tau)}{\sum_{j=1}^{K}\exp(\mathbf{z}^{\text{3D}}_i\cdot\mathbf{z}^{\text{text}}_j)}
    \end{aligned}
\end{equation}
where $\tau$ is a learnable logit temperature.


\subsection{\nickname with Mask Decoder}
\label{subsec:mask_decoder}
\input{algorithms/mask_merging}
On top of our language-aligned backbone, we add a lightweight mask decoder to enable open-vocabulary 3D instance segmentation, avoiding the need for separate instance segmentation networks used in prior work~\cite{takmaz2023openmask3d,nguyen2024open3dis,huang2024openins3d,yin2024sai3d}.

\noindentbold{Architecture}
We use Mask3D~\cite{schult2023mask3d} as our mask decoder, a transformer-based architecture adapted from 2D segmentation~\cite{cheng2021per,cheng2022masked}.
Specifically, our decoder takes non-parametric queries (\ie, positional encodings of points sampled from the input point cloud) and language-aligned point features from our backbone (Sec.~\ref{subsec:backbone}) as input.
The decoder outputs mask embeddings aligned with language features, enabling open-vocabulary segmentation of 3D scenes.

\noindentbold{Training data}
To enable open-vocabulary 3D instance segmentation, we need training data that is not constrained to predefined categories.
While prior work~\cite{takmaz2023openmask3d,nguyen2024open3dis,huang2024openins3d} used closed-vocabulary labels, we leverage Segment3D~\cite{huang2024segment3d}'s class-agnostic masks predicted by SAM~\cite{sam} and combine them with our multi-view mask-caption data to create a rich open-vocabulary training set.
As detailed in Algorithm~\ref{alg:caption_merging}, we merge our mask-caption data $\{(\mathbf{s}_k, c_k)\}_{k=1}^K$ with Segment3D~\cite{huang2024segment3d} masks $\{\mathbf{s}_{l}^{\mathrm{3D}}\}_{l=1}^L$ based on IoU matching.
This yields a set of Segment3D masks $\{(\mathbf{s}^{\mathrm{3D}}_m, \{c_{j\in \mathcal{M}_m}\})\}_{m=1}^M$, each associated with multiple captions from our multi-view data.
During training, we randomly sample a fixed number of captions for each mask from its associated caption set.


\noindentbold{Training objective}
Given a point cloud $\mathbf{P}$ and Segment3D masks with associated captions $\{(\mathbf{s}^{\mathrm{3D}}_m, \{c_{j \in \mathcal{M}_m}\})\}_{m=1}^M$, we compute $Q$ numbers of mask embeddings $\mathbf{Z}^{\mathrm{mask}} \in \mathbb{R}^{Q \times D}$ and normalized text embeddings $\bar{\mathbf{z}}^{\text{text}}_m \in \mathbb{R}^D$:
\begin{equation}
    \label{eq:mask_decoder}
    \mathbf{Z}^{\mathrm{mask}} = \{\mathbf{z}^{\text{mask}}_q\} = \mathcal{D}(\{\mathbf{z}^{\text{3D}}_i\}; \mathbf{Q}), \quad \mathbf{\bar{z}}^{\text{text}}_m = \mathcal{E}^{\text{text}}(\bar{c}_m),
\end{equation}
where $\mathcal{D}(\cdot)$ is our mask decoder that takes point features and sampled queries $\mathbf{Q}$ as input, and $\bar{c}_m$ concatenates all captions associated with mask $\mathbf{s}^{\mathrm{3D}}_m$.
Following Segment3D~\cite{huang2024segment3d}, we first train the mask decoder to predict binary instance masks using three standard losses: objectness prediction loss $\mathcal{L}_{\mathrm{obj}}$, Dice loss $\mathcal{L}_{\mathrm{dice}}$~\cite{dice_loss}, and binary cross entropy loss $\mathcal{L}_{\mathrm{bce}}$. Then, we use Hungarian matching to find the set of mask predictions that minimizes error given ground-truth masks.
The losses are computed as:
\begin{equation}
    \label{eq:mask_prediction}
    \mathbf{o} = \mathrm{Linear}(\mathbf{Z}^{\mathrm{mask}}), \quad \mathbf{S} = \sigma(\mathbf{Z}
    ^{\mathrm{mask}} \cdot \mathbf{Z}^{\text{3D}\top}),
\end{equation}
where $\mathbf{o} \in \mathbb{R}^{Q \times 2}$ and $\mathbf{S} \in \mathbb{R}^{Q \times N}$ are objectness scores and predicted binary masks.
To enable open-vocabulary segmentation, we introduce a mask caption loss $\mathcal{L}_{\mathrm{cap}}$ that explicitly aligns mask embeddings with caption embeddings:
\begin{equation}
    \label{eq:mask_caption_loss}
    \mathcal{L}_{\mathrm{cap}} = -\frac{1}{M}\sum_{m=1}^{M}\log\frac{\exp(\mathbf{z}^{\text{mask}}_m\cdot\bar{\mathbf{z}}^{\text{text}}_k/\tau)}{\sum_{j=1}^{M}\exp(\mathbf{z}^{\text{mask}}_m\cdot\bar{\mathbf{z}}^{\text{text}}_j)}
\end{equation}
The total loss is $\mathcal{L}_{\mathrm{mask}} = \lambda_{\mathrm{obj}}\mathcal{L}_{\mathrm{obj}} + \lambda_{\mathrm{dice}}\mathcal{L}_{\mathrm{dice}} + \lambda_{\mathrm{bce}}\mathcal{L}_{\mathrm{bce}} + \lambda_{\mathrm{cap}}\mathcal{L}_{\mathrm{cap}}$, where in practice we set $\lambda_{\mathrm{obj}}=2$, $\lambda_{\mathrm{dice}}=5$, $\lambda_{\mathrm{bce}}=2$, and $\lambda_{\mathrm{cap}}=1$.
With this loss, our language-aligned mask decoder enables direct open-vocabulary 3D segmentation, avoiding the expensive multi-view CLIP inference required by prior methods~\cite{takmaz2023openmask3d,nguyen2024open3dis,huang2024openins3d}.
