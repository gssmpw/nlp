\section{Conclusion}
\label{sec:conclusion}


In this work, we introduced a comprehensive approach for open-vocabulary 3D scene understanding that addresses fundamental data and modeling challenges in the field.
Our key contribution is a novel dataset generation pipeline that leverages state-of-the-art 2D visual foundation models to create high-quality 3D mask-text pairs, enabling the creation of \dataname, the largest open-vocabulary 3D scene dataset to date with 5.6M captions.
Building on this data, we developed a model that combines \nickname, a language-aligned 3D encoder, with a lightweight mask decoder, achieving state-of-the-art results on open-vocabulary 3D segmentation tasks.
Our ablation studies demonstrate the importance of dataset scale and annotation quality for open-vocabulary 3D understanding, providing a foundation for leveraging 2D vision models in 3D scene understanding.




\noindentbold{Acknowledgement}
This work was partly supported by the Institute of Information \& Communications Technology Planning \& Evaluation (IITP) grants (RS-2021-II212068: AI Innovation Hub, RS-2024-00457882: National AI Research Lab Project) funded by the Korea government (MSIT).
