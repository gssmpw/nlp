\section{Introduction}
\label{sec:intro}


3D scene understanding is a fundamental problem in computer vision that involves detecting and localizing objects while comprehending complex spatial relationships in 3D environments.
This capability is essential for various applications, including robotics, AR/VR, human-computer interactions, and autonomous vehicles.
While traditional approaches rely on predefined object categories, the field is evolving toward open-vocabulary 3D scene understanding, where systems can recognize arbitrary concepts without being constrained to the predefined label sets.
Despite humans' innate ability to perform such tasks effortlessly, developing comparable machine capabilities remains an open problem.

The key bottleneck in advancing open-vocabulary 3D scene understanding is the scarcity of large-scale, high-quality training data.
This limitation is particularly striking compared to 2D vision-language models~\cite{radfordLearningTransferableVisual2021,hu2022scaling,li2022language,ghiasi2022scaling,zhai2023sigmoid,li2024if,liBLIPBootstrappingLanguageImage2022,liBLIP2BootstrappingLanguageImage2023,guopen,liang2023open,rao2022denseclip,shafiullahclip,xu2022groupvit}, 
which have achieved remarkable open-vocabulary capabilities through training on web-scale image-text pair datasets~\cite{radfordLearningTransferableVisual2021,schuhmann2022laion,kakaobrain2022coyo-700m,chenpali,gadre2024datacomp}. 
Unfortunately, creating datasets of comparable scale for 3D scenes remains prohibitively expensive and time-consuming.

Training effective open-vocabulary 3D scene understanding models requires datasets that satisfy three critical requirements:
(1) precise 3D region annotations that delineate object boundaries,
(2) rich textual captions that characterize the visual and semantic attributes of each region, and
(3) substantial scale to encompass diverse domains and offer rich visual-semantic variations.
Creating such data manually, however, becomes increasingly intractable as datasets grow.

To address these challenges, recent works~\cite{ding2022pla,yang2024regionplc,jiang2024open} leverage 2D visual foundation models (VFM)~\cite{llava,vit-gpt2,peng2023kosmos,zou2024segment} to automate data annotation.
They generate 3D mask-text pairs on multi-view RGB-D frames using 2D VFMs and aggregate the generated captions in 3D space.
However, they fall short in meeting aforementioned requirements: 
they use coarse bounding box detectors~\cite{ding2022pla,yang2024regionplc}; 
only have simple attribute labels~\cite{jiang2024open}; and are limited in scale, containing only a few thousand scenes, as shown in Fig.~\ref{fig:data_stats}.
Existing 3D-text pair datasets for 3D vision-language models~\cite{jiaSceneVerseScaling3D2024,wangEmbodiedScanHolisticMultiModal2023,lyu2024mmscan} also face limitations in both the richness of textual captions and the precision of 3D masks, primarily because they rely on human-annotated object labels and 3D region annotations.


In this paper, we address these limitations by introducing an improved data generation pipeline to create high-quality, large-scale 3D mask-text pairs.
Our pipeline satisfies all three criteria by leveraging state-of-the-art open-vocabulary image segmentation models~\cite{liu2023grounding,sam,ravi2024sam,zou2024segment} for precise region segmentation and advanced region-aware vision-language models~\cite{yuan2024osprey} for generating comprehensive textual captions at scale.
By applying this pipeline to a diverse collection of 3D scene datasets~\cite{dai2017scannet,yeshwanth2023scannet++,baruch2021arkitscenes,chang2017matterport3d,zheng2020structured3d}, we create \textbf{\dataname}, a large dataset containing over 30K scenes with 5.6M region captions -- significantly exceeding existing datasets~\cite{yang2024regionplc,jiang2024open} in scale -- while maintaining high-quality region masks and detailed textual captions.

Building upon this new dataset, we analyze how improving the annotation quality and scaling up the data impact open-vocabulary 3D scene segmentation.
To enable this analysis, we develop a general framework for open-vocabulary 3D semantic and instance segmentation.
We train our foundational 3D encoder, \textbf{\nickname}, which aligns a per-point feature with a text embedding vector through contrastive learning.
Then, we train a lightweight mask decoder to predict object instances directly from language-aligned features, enabling the first single-stage open-vocabulary 3D instance segmentation without ground truth labels.
Our approach achieves state-of-the-art results on multiple semantic and instance segmentation benchmarks.
Extensive ablation studies show that both the scale and the quality of our dataset are crucial factors that contribute to the superior performance of our approach.
