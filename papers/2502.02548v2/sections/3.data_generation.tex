\section{\dataname Data Engine}
\label{sec:caption_generation}

Generating 3D mask-text pair datasets can be costly and require meticulous attention. Recent work~\cite{ding2022pla,yang2024regionplc,jiang2024open} have leveraged 2D visual foundation models (VFMs) to automate data annotation to an extent -- they use multi-view images to generate captions or features on different types of region proposals (e.g. bounding boxes, segmentations, or sliding windows). 
However, existing approaches suffer from imprecise boundary delineation due to their reliance on coarse object detectors~\cite{ding2022pla,yang2024regionplc}, 
or provide only simple attribute labels~\cite{jiang2024open}.
To overcome these limitations, we propose a data generation pipeline that combines recent advances in open-vocabulary segmentation and robust region-aware vision-language models (VLMs), 
enabling both precise region boundaries and rich descriptions that capture object attributes, spatial relationships, and scene context.

\input{figures/data_pipeline.tex}

\subsection{Proposed Pipeline}
Our pipeline overcomes limitations through two key improvements: accurate segmentation and the region captioning pipeline.
Fig.~\ref{fig:data_pipeline} illustrates the overview of our pipeline.

\noindent\textbf{Enhanced segmentation.}
We employ Grounded-SAM~\cite{ren2024grounded} and SEEM~\cite{zou2024segment} for more precise open-vocabulary image segmentation. We incorporate both models because Grounded-SAM excels at segmenting foreground objects with precise boundaries, while SEEM complements this with open-vocabulary panoptic segmentation that better handles background stuff like wall and floor. Thus, our method outperforms previous methods~\cite{yang2024regionplc,ding2022pla} that rely on foreground object detectors.

Given an RGB image $\mathbf{I}\in\mathbb{R}^{H \times W \times 3}$, Grounded-SAM first predicts open-set bounding boxes using Grounding-DINO~\cite{liu2023grounding}, then uses these boxes as input prompts for SAM~\cite{sam,ravi2024sam} to generate segmentation masks.
To fully automate the segmentation process, we employ RAM++~\cite{ram_pp} to detect object categories within the input image.
These detected categories serve as a text prompt for Grounding-DINO.
Through this, Grounded-SAM generates a set of segmentation masks $\{\mathbf{M}_k\}_{k=1}^K$
where $K$ is the number of detected objects in the image.
Each mask $\mathbf{M}_k \in \{0,1\}^{H \times W}$ represents a binary segmentation of an object.
SEEM operates in a similar way, except that it directly performs panoptic segmentation without being required to use RAM++ to generate tags or Grounding-DINO for object detection.
For simplicity, we denote the combined set of masks from both Grounded-SAM and SEEM as $\{\mathbf{M}_k\}_{k=1}^K$, where $K$ is the total number of masks from both models.

\noindent\textbf{Enhanced region captioning.}
For each segmentation mask, we generate a detailed caption that describes the visual characteristics and spatial context of the object.
Unlike previous methods that process an image as a whole using generic image captioning models~\cite{peng2023kosmos,llava,vit-gpt2,wang2022ofa}, we leverage region-aware vision-language models (VLMs)~\cite{you2023ferret,yuan2024osprey,rasheed2024glamm} that are specifically designed to understand and describe a region or a mask specified by a user as an additional. These models can generate detailed descriptions by interpreting various visual prompts, such as points, boxes, masks, and scribbles, enabling more focused and contextual captions for each segmented region.

Given an image $\mathbf{I}$, a segmentation mask $\mathbf{M}_k$, and a user prompt $\pi$ that asks for a detailed description of the masked region, the region-aware VLM $\mathcal{R}(\cdot)$ generates a textual response $c$ for each mask by
$c_k = \mathcal{R}(\mathbf{I}; \mathbf{M}_k, \pi)$,
where $c_k$ is a natural language description that captures the visual attributes and spatial context of the k-th masked region.
After evaluating several available region-aware VLMs, we chose Osprey~\cite{yuan2024osprey} for our implementation.


\noindent\textbf{2D pixel-3D point association.}
After obtaining segmentation masks and captions from multiple views, we associate them with 3D points to create mask-text pairs in 3D space.
For each 3D point $\mathbf{p}$ in the point cloud $\mathbf{P} \in \mathbb{R}^{N\times3}$, we project it onto each view using the camera parameters to obtain its 2D pixel coordinates $(u,v)$ and depth value $d$.
We check if the projected pixel falls within any segmentation mask $\mathbf{M}_k$ and if its depth matches the ground-truth depth at that location within a small threshold $\epsilon$ (inclusion test).
Specifically, for point cloud $\mathbf{P}$, 
we compute 3D binary region masks $\mathbf{s}_k \in \{0,1\}^N$ as:
\begin{equation}\label{eq:associate}
    \!\! \mathbf{s}_k = \left\{ 
    \begin{array}{cl}
        \! 1 & \!\!\! \textrm{if}~~(\mathbf{M}_{k})_{u,v} = 1 \textrm{ and } |d - \mathbf{D}_{u,v}|_1 < \epsilon \\
        \! 0 & \!\!\! \textrm{otherwise}
    \end{array}
    \right. \!\!\!\! ,
\end{equation}
where $\mathbf{D}$ is the ground-truth depth image. Finally, we obtain 3D mask-text pairs $\{(\mathbf{s}_k, c_k)\}_{k=1}^K$ that associate each segmented region with its corresponding caption.

Our pipeline produces high-quality 3D mask-text pairs that combine precise object boundaries with rich semantic descriptions. As demonstrated in Fig.~\ref{fig:data_comparison}, compared to previous methods, our approach achieves both more accurate segmentation boundaries and more detailed contextual descriptions that capture object attributes and spatial relationships.

\subsection{Data Statistics}
Given the limited availability of large-scale 3D scene datasets, it is crucial to leverage multiple existing datasets and apply a unified annotation process to create a comprehensive training corpus.
We curate a collection of widely-used 3D indoor scene datasets, including ScanNet~\cite{dai2017scannet}, ARKitScenes~\cite{baruch2021arkitscenes}, Matterport3D~\cite{chang2017matterport3d}, ScanNet++~\cite{yeshwanth2023scannet++}, and Structured3D~\cite{zheng2020structured3d}, and apply our proposed pipeline to each.
These datasets provide diverse indoor scenes covering both real and synthetic environments, with high-quality RGB-D scans, accurate camera poses, and dense 3D reconstructions, making them ideal for our automatic annotation process.

Through this data pipeline, we create \dataname, the largest 3D mask-text paired dataset to date,
encompassing over 30K indoor scenes and approximately 1M RGB-D frames, yielding 5.6M region captions comprising 30M total text tokens.
The complete data statistics can be found in 
Fig.~\ref{fig:data_stats}.
Our dataset offers significant advantages over the existing datasets in terms of:
\begin{itemize}[leftmargin=*,itemsep=1pt]
    \item \textbf{Scale:} We generate over 5.6M mask-text pairs with 30M text tokens across 30K scenes, significantly larger than previous datasets in scene coverage and annotation density.
    \item \textbf{Precision:} Our use of Grounded-SAM~\cite{ren2024grounded} and SEEM~\cite{zou2024segment} ensures precise region boundaries, significantly improving over bounding box-based approaches.
    \item \textbf{Richness:} The region-aware VLM generates detailed contextual descriptions that capture both visual attributes and spatial relationships, providing richer semantic information than simple object labels.
\end{itemize}


