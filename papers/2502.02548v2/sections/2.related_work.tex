\section{Related Work}
\label{sec:related}


\noindentbold{2D visual foundation models}
In recent years, we have witnessed the emergence of large pretrained modelsâ€”so-called foundation models that are trained on large-scale datasets and serve as a \textit{foundation} for many downstream tasks.
These models demonstrate remarkable versatility across multiple modalities, including language~\cite{team2023gemini,touvron2023llama,touvron2023llama2,dubey2024llama3,vicuna2023,radford2019language,brown2020language,chung2024scaling,achiam2023gpt,bai2023qwen,yang2024qwen2,jiang2023mistral,jiang2024mixtral}, vision~\cite{sam,ravi2024sam,dino_v1,oquab2023dinov2,zou2024segment,rombach2022high,ho2020denoising,nichol2021improved,songdenoising,songscore}, audio~\cite{deshmukh2023pengi,zhang2023speechgpt,rubenstein2023audiopalm,borsos2023audiolm}. 
Furthermore, they enable multi-modal reasoning capabilities that bridge across different modalities~\cite{girdharImageBindOneEmbedding2023,Qwen-VL,llava,radfordLearningTransferableVisual2021,jia2021scaling,team2024gemini}.
Among these models, those that operate on visual modalities are known as visual foundation models (VFM).
VFMs excel in various computer vision tasks such as image segmentation~\cite{sam,ravi2024sam,zou2024segment,zou2023generalized,cheng2021per,cheng2022masked,jain2023oneformer,li2024semantic}, object detection~\cite{liu2023grounding,carion2020end}, representation learning~\cite{dino_v1,oquab2023dinov2}, and open-vocabulary understanding~\cite{radfordLearningTransferableVisual2021,li2022language,ghiasi2022scaling,ram,ram_pp,yu2023convolutions,kang2024defense,naeem2024silc,cho2024cat}.
When integrated with large language models, they enable sophisticated visual reasoning and natural language interactions~\cite{llava,Qwen-VL,girdharImageBindOneEmbedding2023,team2024gemini,guo2024regiongpt,yuan2024osprey,you2023ferret}.
We use such vision language models to construct open vocabulary segmentation and captions for point clouds based on multiview images.







\noindentbold{Open-vocabulary 3D segmentation}
Building on the success of 2D VFMs, recent work have extended open-vocabulary capabilities to 3D scene understanding.
OpenScene~\cite{Peng2023OpenScene} first introduced zero-shot 3D semantic segmentation by distilling knowledge from language-aligned image encoders~\cite{li2022language,ghiasi2022scaling}.
Subsequent methods~\cite{ding2022pla,yang2024regionplc,jiang2024open} leverage multiview images to generate textual captions, which then serve as training supervision.
However, these methods face challenges in generating high-quality 3D mask-text pairs at scale.
For open-vocabulary 3D instance segmentation, existing methods~\cite{takmaz2023openmask3d,nguyen2024open3dis,huang2024openins3d} typically rely on closed-vocabulary proposal networks such as Mask3D~\cite{schult2023mask3d}, which inherently constrains their ability to detect novel object categories. 
Moreover, these methods leverage 2D VFMs like CLIP~\cite{radfordLearningTransferableVisual2021} for region classification by projecting 3D regions onto multiple 2D views.
This approach requires both 2D images and 3D point clouds during inference. Additionally, it necessitates multiple inferences of large 2D models on projected masks, resulting in high computational costs. 
We address these limitations by developing the first single-stage open-vocabulary 3D instance segmentation model that operates directly in 3D without ground truth labels, using our \dataname dataset and Segment3D~\cite{huang2024segment3d} proposals.

\noindentbold{3D vision-language datasets}
Several datasets align 3D scenes with textual annotations to facilitate language-driven 3D understanding. 
ScanRefer~\cite{chen2020scanrefer}, ReferIt3D~\cite{achlioptas2020referit_3d} and EmbodiedScan~\cite{wangEmbodiedScanHolisticMultiModal2023} provide fine-grained object-level localization through detailed referential phrases, while ScanQA~\cite{azuma2022scanqa} targets spatially grounded question-answering. 
In contrast, SceneVerse~\cite{jiaSceneVerseScaling3D2024} and MMScan~\cite{lyu2024mmscan} employ large-language models or vision-language models to partially automate annotation.
Despite leveraging advanced models, these datasets depend significantly on costly human annotations derived from closed-vocabulary sources, limiting their support for open-vocabulary and scalability for large-scale 3D segmentation tasks.
