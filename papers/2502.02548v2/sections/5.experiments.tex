\section{Experiments}
\label{sec:experiments}
In this section, we investigate how dataset size impacts model performance (Sec.~\ref{subsec:scaling}).
Next, we benchmark \nickname against existing methods for open-vocabulary 3D scene segmentation (Sec.~\ref{subsec:benchmark}).
Finally, we analyze our system through attention visualization, zero-shot experiments, and ablation studies on the \dataname data engine (Sec.~\ref{subsec:analysis}).

\subsection{Setup}

\noindentbold{Implementation details}
We adopt Sparse ConvNets~\cite{graham20183d,mink} as our 3D encoder, leveraging their efficiency in processing sparse 3D data.%
Our baseline architecture uses SparseUNet34C~\cite{mink} with 43.7M trainable parameters. %
For text encoder, we employ Recap-CLIP~\cite{li2024if}, which is pre-trained on longer re-captioned datasets, enabling better processing of the long captions in our dataset.
For training, we use SGD optimization~\cite{bottou2010large} with an initial learning rate of $0.05$ and a weight decay of $1\times10^{-4}$, coupled with the OneCycleLR scheduler~\cite{smith2019super}, with batch size 4. 
To enhance multi-data joint training, we adopt recent technique of Point Prompt Training (PPT)~\cite{wu2023towards}.
All models are trained for 128 epochs on eight A100 GPUs.
For instance segmentation, we fine-tune the pre-trained \nickname model with an additional mask decoder on \dataname with caption merging (Alg.~\ref{alg:caption_merging}).
Please refer to the appendix for more details.

\noindentbold{Evaluation metrics}
We evaluate performance using mean Intersection over Union (mIoU) and mean Accuracy (mAcc), which are standard metrics for open-vocabulary 3D semantic segmentation. 
Following prior work~\cite{jiang2024open,yang2024regionplc,ding2022pla}, we report f-mIoU and f-mAcc metrics that exclude background classes.

\subsection{Impact of Dataset Size}
\label{subsec:scaling}

\input{figures/scaling}
\input{tables/scannet200_semantic}
\input{tables/scannet200_instance}

To understand how the size of the training data affects model performance, we conduct experiments with varying amounts of training data.
Specifically, we gradually increase the training data by adding one dataset at a time in the following order: ScanNet~\cite{dai2017scannet}, ARKitScenes~\cite{baruch2021arkitscenes}, ScanNet++~\cite{yeshwanth2023scannet++}, Matterport3D~\cite{chang2017matterport3d}, and Structured3D~\cite{zheng2020structured3d}.
For these experiments, we fix the model architecture to SparseUNet34C~\cite{mink}.
All other hyperparameters remain fixed across the experiments.
As shown in Fig.~\ref{fig:scaling}, increasing the size of the dataset generally improves the accuracy of open-vocabulary semantic segmentation on the ScanNet200 benchmark~\cite{scannet200}.
The most significant performance gains are observed when incorporating ARKitScenes and ScanNet++, which we attribute to their high-quality, dense RGB-D frames captured from real 3D environments. 
From here on, we refer to our \nickname model as the model jointly trained on all datasets.







\subsection{Benchmark Results}
\label{subsec:benchmark}

\noindentbold{Open-vocabulary 3D semantic segmentation}
We evaluate on ScanNet20 validation set, ScanNet200 validation set, Matterport3D test set, and ScanNet++ validation set, following prior work~\cite{Peng2023OpenScene,ding2022pla,yang2024regionplc,jiang2024open}.
These datasets contain 20, 200, 160, and 100 semantic classes respectively, providing diverse benchmarks for open-vocabulary 3D semantic segmentation.
Using only ScanNet training data, \nickname outperforms prior work in terms of f-mIoU (\%) on all benchmarks: surpassing OV3D~\cite{jiang2024open} by 1.0p on ScanNet20 and RegionPLC~\cite{yang2024regionplc} by 4.9p, 2.4p, and 3.8p on ScanNet++, Matterport3D, and ScanNet200 respectively.
Training on our full dataset \dataname further improves f-mIoU (\%) across all benchmarks, achieving 68.1 on ScanNet20, 18.0 on ScanNet++, 13.1 on Matterport3D, and 15.7 on ScanNet200.
Notably, our approach achieves 7.1p higher f-mIoU than SceneVerse~\cite{jiaSceneVerseScaling3D2024} despite using fewer scenes, highlighting the importance of caption quantity per scene.


\noindentbold{Open-vocabulary 3D instance segmentation}
As shown in Table~\ref{tab:scannet200_instance}, methods using both 2D and 3D inputs achieve strong results by directly applying CLIP models, but are impractical due to high latency (33-285 sec per scene) from processing multiple view images.
For fair comparison, we evaluate our \nickname trained only on ScanNet against prior methods~\cite{huang2024openins3d,takmaz2023openmask3d,yin2024sai3d,nguyen2024open3dis,Peng2023OpenScene,yang2024regionplc} that also train and evaluate on ScanNet.
With Mask3D~\cite{schult2023mask3d} as a closed-vocabulary region proposal network, \nickname outperforms the previous best method OpenIns3D~\cite{huang2024openins3d} by 3.0p mAP.
When using truly open-vocabulary Segment3D~\cite{huang2024segment3d} proposals, \nickname maintains strong performance at 2.7 mAP despite the more challenging setting.
Finally, our lightweight mask decoder trained with \dataname with caption merging (Alg.~\ref{alg:caption_merging}) achieves 3.9 mAP while being the first single-stage open-vocabulary 3D instance segmentation model that does not require ground truth labels.

\subsection{Analysis}
\label{subsec:analysis}
\input{tables/pipeline_comparison}
\input{figures/qual}

\noindentbold{Data engine components}
Finally, we conduct an ablation study on different component combinations of \nickname data engine to measure the contribution of each component. 
To understand the impact of different components in our data generation pipeline, we conduct ablation experiments by systematically varying key components while keeping the model architecture (SparseUNet16~\cite{yang2024regionplc}) and source dataset (ScanNet only) fixed.
As shown in Table~\ref{tab:pipeline_comparison}, we evaluate the following configurations:

\begin{itemize}[leftmargin=*,itemsep=1pt]
    \item \textbf{Mask Generation:} We compare our Grounded-SAM~\cite{ren2024grounded} + SEEM~\cite{zou2024segment} approach against using only Grounded-SAM~\cite{ren2024grounded} or SEEM~\cite{zou2024segment}. Results show that combining both methods leads to better region proposals.
    \item \textbf{Caption Generation:} We compare LLaVA~\cite{llava} for image-level captioning against Ferret~\cite{you2023ferret} and Osprey~\cite{yuan2024osprey} for region-level captioning. Region-level approaches perform better by providing detailed per-region descriptions, with Osprey achieving the best results.
\end{itemize}
Our final pipeline combines Grounded-SAM2~\cite{ren2024grounded,ravi2024sam} with SEEM~\cite{zou2024segment} for mask generation and Osprey~\cite{yuan2024osprey} for captioning, as this configuration yields optimal performance.

\noindentbold{Attention visualization}
In Fig.~\ref{fig:qual}, we visualize the similarity between dense point features obtained from \nickname and free-form text queries.
As shown in the figure, despite being trained only on indoor scenes, \nickname effectively localizes semantic regions described by text queries across both indoor~\cite{dai2017scannet} and outdoor~\cite{schops2017multi} datasets.



\noindentbold{Zero-shot 3D semantic segmentation}
While annotation-free methods~\cite{yang2024regionplc,jiang2024open} do not utilize GT annotations, their generated captions often contain class names from the evaluation set (\eg, "chair"), making them not truly "zero-shot."
To address this, we conduct a more rigorous zero-shot analysis by anonymizing all class names in the training captions, replacing them with general terms like "object."
To ensure fair comparison, we kept all experimental settings (\eg, model architecture, CLIP model, loss function) identical across methods, varying only the training data.
As shown in Table~\ref{tab:anonymization}, all models experience performance drops when trained on anonymized data, but the model trained on \dataname maintains the strongest performance (10.5 f-mIoU), outperforming both annotation-free methods and approaches that use ground truth annotations like LEO~\cite{huangEmbodiedGeneralistAgent2023}.
\input{tables/anonymization}



