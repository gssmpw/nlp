\section{OV3D~\cite{jiang2024open} Implementation Details}

Since there is no publicly available code and data for OV3D~\cite{jiang2024open}, we utilized our re-implemented version of OV3D for data visualization (Fig.~\ref{fig:data_comparison}) and statistics (Fig.~\ref{fig:data_stats}) in the main manuscript.
In this section, we provide detailed explanations of our re-implementation results.

\subsection{Caption Generation}
OV3D~\cite{jiang2024open} obtains entity-level text descriptions of an image through multi-round conversations with LLaVA-1.5~\cite{liu2024improved}:

\begin{enumerate}
    \item In the first round, LLaVA-1.5 is prompted to generate an image caption describing the overall scene.
    \item In the second round, LLaVA-1.5 is prompted to extract entity names from the generated image caption.
    \item In the final round, LLaVA-1.5 is prompted to generate detailed entity descriptions for each extracted entity name.
\end{enumerate}
During our implementation, we encountered inconsistencies in LLaVA-1.5's response formats. 
To ensure structured and consistent entity-level text descriptions, we modified the final prompt to request responses in JSON format, as shown in Table~\ref{tab:ov3d_prompt}, while maintaining the original prompts for the first two rounds.

\input{tables/ov3d_prompt}

In addition, our experimental results in Table~\ref{tab:ov3d_reimpl} revealed that LLaVA-1.5's performance in entity name detection was suboptimal, which significantly impacts OV3D's overall effectiveness.
To overcome this limitation, we introduce OV3D++, an enhanced version that uses RAM++~\cite{ram_pp}'s robust tagging capabilities for entity detection while preserving the original entity description process, as shown in Table~\ref{tab:ov3dpp_prompt}.

\input{tables/ov3dpp_prompt}

\subsection{Training Objectives}
We experiment with three different training objectives to reproduce OV3D~\cite{jiang2024open}'s performance:

\begin{itemize}
    \item \texttt{DenseAlign}: The original dense alignment loss proposed in OV3D, which maximizes the similarity between text embeddings and point-wise visual features.
    \item \texttt{Align}: A simplified version of dense alignment that computes similarity between text embeddings and pooled visual features within the mask region.
    \item \texttt{Contrastive}: A contrastive learning objective proposed in RegionPLC~\cite{yang2024regionplc} that pulls matching text-visual pairs closer while pushing non-matching pairs apart in the embedding space.
\end{itemize}
For fair comparison, we use SparseUNet34C~\cite{mink} as the backbone network architecture across all experiments, which is the same architecture used in \nickname, and maintain identical training configurations with the only variations being in the training objectives and data generation pipelines.

\subsection{Results}
As shown in Table~\ref{tab:ov3d_reimpl}, our direct re-implementation (OV3D-rep) is unable to fully reproduce the performance reported in the original OV3D paper~\cite{jiang2024open}.
However, our improved version (OV3D++) with RAM++~\cite{ram_pp} tagging achieves better results than the original paper in most metrics when using \texttt{Contrastive} loss, except for f-mIoU on ScanNet20~\cite{dai2017scannet}.
Notably, \texttt{Contrastive} loss consistently outperforms other loss functions across all settings, which motivates our choice to use \texttt{Contrastive} loss in \nickname as well.
While OV3D++ shows significant improvements over the baseline, it is ultimately surpassed by \nickname, demonstrating the effectiveness of \nickname data engine in generating more fine-grained and comprehensive captions.

\input{tables/ov3d_reimpl}

