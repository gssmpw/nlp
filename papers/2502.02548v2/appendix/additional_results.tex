\section{Additional Results}

\subsection{Quantitative Results}
In Tab.~\ref{tab:scannet200_category}, 
We conduct a comprehensive evaluation of our model's performance across different category frequencies in ScanNet200. 
Following standard practice~\cite{scannet200}, we categorize labels into head, common, and tail groups based on their occurrence frequency in the dataset.  
As shown in Tab.~\ref{tab:scannet200_category}, our approach achieves consistent improvements across all category groups compared to previous methods.
Notably, we observe the relative gain is more substantial on common and tail categories as we incorporate more training datasets, highlighting the effectiveness of our multi-dataset training strategy in learning robust features across varying scene distributions.
\input{tables/scannet200_semantic_category.tex}

\subsection{Qualitative Results}

    
\begin{figure*}[htbp]
    \centering
    \begin{minipage}{\textwidth}
        \begin{subfigure}[b]{0.45\textwidth}
            \includegraphics[width=\textwidth,trim={7cm 5cm 9cm 1cm},clip]{appendix/caption_viz/scene0055_02_render_aerial_min_y_resized.png}
            \caption{scene0055\_02 (ScanNet)}
            \label{fig:scene0055}
        \end{subfigure}%
        \hfill%
        \begin{subfigure}[b]{0.45\textwidth}
            \includegraphics[width=\textwidth,,trim={8cm 5cm 6cm 3cm},clip]{appendix/caption_viz/scene0128_00_render_aerial_min_y_resized.png}
            \caption{scene0128\_00 (ScanNet)}
            \label{fig:scene0128}
        \end{subfigure}
    \end{minipage}
    
    \vspace{10pt}  %
    
    \begin{minipage}{\textwidth}
        \begin{subfigure}[b]{0.45\textwidth}
            \includegraphics[width=\textwidth,,trim={8cm 1cm 8cm 0cm},clip]{appendix/caption_viz/scene0211_01_render_aerial_min_x_resized.png}
            \caption{scene0211\_01 (ScanNet)}
            \label{fig:scene0211}
        \end{subfigure}%
        \hfill%
        \begin{subfigure}[b]{0.45\textwidth}
            \includegraphics[width=\textwidth,trim={8cm 3cm 8cm 0cm},clip]{appendix/caption_viz/scene0324_00_render_aerial_min_y_resized.png}
            \caption{scene0324\_00 (ScanNet)}
            \label{fig:scene0324}
        \end{subfigure}
    \end{minipage}

    \begin{minipage}{\textwidth}
        \begin{subfigure}[b]{0.45\textwidth}
            \includegraphics[width=\textwidth,,trim={8cm 1cm 6cm 0cm},clip]{appendix/caption_viz/47333055_render_aerial_min_x_resized.png}
            \caption{47333055 (ARKitScenes)}
            \label{fig:47333055}
        \end{subfigure}%
        \hfill%
        \begin{subfigure}[b]{0.45\textwidth}
            \includegraphics[width=\textwidth,trim={5cm 1cm 8cm 0cm},clip]{appendix/caption_viz/42899477_render_aerial_min_x_resized.png}
            \caption{42898477 (ARKitScenes)}
            \label{fig:42899477}
        \end{subfigure}
    \end{minipage}
    
    \caption{\textbf{More visualization of the 3D mask-text pairs in our \dataname dataset.} A subset of mask-text pairs has been chosen for better visualization.}
    \label{fig:caption_viz_all}
\end{figure*}

In Fig.~\ref{fig:caption_viz_all}, we present additional qualitative visualizations of our generated 3D mask-text pair datasets, where we carefully selected mask-text pairs to effectively demonstrate the diversity and quality of our generated data.
Furthermore, in Fig.~\ref{fig:supp_qual}, we showcase attention maps for diverse text queries across various scenes, which demonstrates that our model can effectively attend to relevant regions in response to different types of queries, ranging from object-centric descriptions to more abstract concepts like affordances.
In Fig.~\ref{fig:supp_qual_sem}, we present qualitative results of annotation-free 3D semantic segmentation on ScanNet200~\cite{scannet200}.
Our model shows promising results, particularly in the first scene where it demonstrates an interesting behavior - while the ground truth annotates an integrated chair-desk unit entirely as a chair, \nickname distinctly separates and predicts the desk and chair components.
This showcases a potential advantage of our annotation-free approach to training 3D foundation models, where the model can learn more nuanced semantic distinctions that might be overlooked in manual annotations.

\clearpage
\input{figures/supp_qual}
\clearpage
\input{figures/supp_qual_sem}
\clearpage
