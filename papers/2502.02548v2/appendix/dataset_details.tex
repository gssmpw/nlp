\section{Dataset Details}
Below we report the data statistics of our \dataname dataset, detail the data preprocessing steps, pipeline configurations used for each dataset in our experiments, and additional data pipeline experiments that utilize 3D instance mask predictions in caption generation process.

\subsection{Data Statistics}
In Tab.~\ref{tab:datasets}, we report the statistics of our generated dataset, including the number of scenes, RGB-D frames, generated captions, and total tokens in captions for each source dataset. Our dataset contains over 30K scenes, and 5.6M captions with a total of 30M tokens across both real and synthetic indoor environments.
\input{tables/dataset}

In Tab.~\ref{tab:existing_dataset}, we evaluate caption and 3D mask quality across datasets using three metrics. 
The \textit{unique normalized nouns count} measures the total number of unique normalized nouns in captions, with higher count indicating richer and more diverse captions.
\textit{Mask coverage} (\%) calculates the mean percentage of 3D points with associated captions per scene, where higher coverage enables more effective training.
\textit{Mask entropy} (bits) measures mask quality for datasets with partial masks generated from multi-view images (\ie OV3D, RegionPLC, and Mosaic3D-5.6M) without using GT.
It calculates Shannon entropy of GT instance ID distributions within each mask--higher entropy indicates that a mask contains multiple GT instances, suggesting less accurate mask boundaries.
Mosaic3D-5.6M demonstrates superior caption diversity and mask quality compared to both existing large-scale 3D-text datasets and previous open-vocabulary 3D segmentation datasets, validating its value as a new dataset.
\input{tables/dataset_comparison}

\subsection{Data Preprocessing}
\begin{itemize}[leftmargin=*,itemsep=1pt]
    \item \textbf{ScanNet}~\cite{dai2017scannet} To optimize computational efficiency while maintaining adequate spatial coverage, we process every 20th RGB-D frame from each scene. Prior to processing, we resize all RGB-D frames to 640$\times$480 resolution.
    \item \textbf{ScanNet++}~\cite{yeshwanth2023scannet++} From the official dataset, we utilize the \textit{``DSLR''} image collection. Following repository guidelines, we generate synthetic depth images using the reconstructed mesh and camera parameters. After correcting for distortion in both RGB and depth images and adjusting camera intrinsics, we process every 10th frame through our annotation pipeline. Point clouds are generated via surface sampling on the reconstructed meshes.
    \item \textbf{ARKitScenes}~\cite{baruch2021arkitscenes} We leverage the \textit{``3D Object Detection (3DOD)''} subset, utilizing its RGB-D frames and reconstructed meshes. We use every 10th frame at low resolution (256$\times$192), and apply surface point sampling on mesh for point clouds.
    \item \textbf{Matterport3D}~\cite{chang2017matterport3d} We use preprocesed RGB-D frames and point clouds provided by the author of OpenScene~\cite{Peng2023OpenScene}.
    \item \textbf{Structured3D}~\cite{zheng2020structured3d} We utilize RGB-D frames from both perspective and panoramic camera. We utilize preprocessed point clouds from the \textit{Pointcept}~\cite{pointcept2023} library, which fuses multi-view depth unprojection with voxel downsampling to get point clouds.
\end{itemize}

\subsection{Pipeline Configurations}
Our data generation pipeline leverages multiple Visual Foundation Models to automate the data annotation process. Below we detail the configuration of each model in our pipeline.
\begin{itemize}[leftmargin=*,itemsep=1pt]
    \item \textbf{RAM++~\cite{ram_pp}}: we utilize the official pretrained checkpoint \texttt{ram\_plus\_swin\_large\_14m} available at \url{https://huggingface.co/xinyu1205/recognize-anything-plus-model}.
    \item \textbf{Grounded-SAM~\cite{ren2024grounded}}: We employ the official checkpoint of Grounding-DINO~\cite{liu2023grounding} \texttt{IDEA-Research/grounding-dino-tiny} accessed through HuggingFace at \url{https://huggingface.co/IDEA-Research/grounding-dino-tiny}, together with SAM2~\cite{ravi2024sam} with checkpoint \texttt{sam2\_hiera\_l}, available at \url{https://huggingface.co/facebook/sam2-hiera-large}. For the postprocessing, we process the output bounding boxes from Grounding-DINO using a box score threshold of 0.25 and a text score threshold of 0.2. We then apply non-maximum suppression (NMS) with an IoU threshold of 0.5 to remove redundancy. To ensure meaningful region proposals, we filter out excessively large boxes that occupy more than 95\% of the image area. These refined bounding boxes are then passed to SAM2 for mask prediction.
    \item \textbf{Osprey~\cite{yuan2024osprey}}: We utilize the official pretrained \texttt{sunshine-lwt/Osprey-Chat-7b} checkpoint, available at \url{https://huggingface.co/sunshine-lwt/Osprey-Chat-7b}. The generation parameters are set with a temperature of 1.0, top\_p of 1.0, beam search size of 1, and the maximum number of new tokens to 512.
\end{itemize}

\input{tables/osprey_prompt}


\subsection{Additional Pipeline Experiments}
We explore two additional data pipeline configurations that use Segment3D~\cite{huang2024segment3d} masks for segmentation while maintaining Osprey~\cite{yuan2024osprey} for captioning:
\begin{itemize}[leftmargin=*,itemsep=1pt]
    \item \textbf{Segment3D}: We utilize complete Segment3D masks and obtain captions by aggregating descriptions from multiple projected views of each mask. This approach maintains mask completeness but may result in multiple captions being assigned to a single mask from different viewpoints.
    \item \textbf{Segment3D - Mosaic}: We use partial Segment3D masks as seen from individual views and generate captions based on these view-specific projections. While masks are partial, each mask-caption pair is aligned since it represents the exact visible region from a specific viewpoint.
\end{itemize}
The results in Tab.~\ref{tab:segment3d_pipeline} demonstrate that Segment3D - Mosaic outperforms the baseline Segment3D approach, highlighting the importance of precise mask-text pair alignment.
However, both Segment3D variants are outperformed by our \nickname pipeline, which suggests that our combination of RAM++~\cite{ram_pp}, Grounded-SAM~\cite{ren2024grounded}, and SEEM~\cite{zou2024segment} provides superior segmentation quality.

\input{tables/segment3d_pipeline}
