\section{Experimental Analysis}

\subsection{Model Scaling }
\noindentbold{Model capacity}
Building on the data scaling analysis, we additionally examine how model scales impact performance. 
We systematically increase the model sizes of 3D encoders while keeping other components fixed. 
We vary the size of Sparse ConvUNet by changing the model depth and widths following literature~\cite{he2016deep}, where the smallest model, SPUNet14A, has 11.1M trainable parameters, whereas the largest variants, SPUNet101C, has 256M parameters.
For these experiments, we fix the training dataset to include ScanNet, ARKitScenes, and ScanNet++.
As shown in Fig.~\ref{fig:model_scaling}, increasing model capacity generally leads to better performance, with diminishing returns after 100M parameters.

\noindentbold{Multi-dataset synergistic learning with PPT~\cite{wu2023towards}}
Since our \nickname dataset combines multiple datasets with different capture settings and environments, there potentially exists domain gaps between each subset that could hinder effective joint training.
Recent work by Wu~\etal~\cite{wu2023towards} demonstrates that adapting dataset-specific learnable prompts in normalization layers can reduce negative transfer effects when training on multiple point cloud datasets.
Building on this insight, we adopt their Point Prompt Training (PPT) approach to ehance our joint training process.
As shown in Fig~\ref{fig:model_scaling}, models using PPT demonstrate better scaling compared to standard joint training, confirming PPT's effectiveness in harmonizing multi-source training on our dataset.

\input{figures/model_scaling}


\subsection{Impact of Text Encoders}
To analyze how different text encoders affect open-vocabulary 3D segmentation performance, we evaluate various CLIP text encoders while keeping the 3D encoder architecture (SPUNet34C) and other components fixed. 
Table~\ref{tab:clip_variants} presents the zero-shot performance on ScanNet20 and ScanNet200 benchmarks. 
We compare standard CLIP text encoders including CLIP/B32, CLIP/B16, and CLIP/L14@336px~\cite{radfordLearningTransferableVisual2021}, as well as recently proposed variants like Recap-CLIP~\cite{li2024if} and SigLIP~\cite{zhai2023sigmoid}.


Among all variants, Recap-CLIP achieves the best overall performance with 68.1\% f-mIoU on ScanNet20 and 15.7\% f-mIoU on ScanNet200. 
This represents a +0.3\% and +0.9\% improvement over the base CLIP/B16 model respectively. 
The superior performance of Recap-CLIP aligns with its enhanced text-image alignment ability demonstrated in 2D vision tasks.
Based on these comprehensive experiments, we select Recap-CLIP as our default text encoder for all subsequent experiments. To ensure fair comparisons with previous work, we maintain consistency by using the same text encoder configuration when reproducing baseline results, as shown in Tables~\ref{tab:scannet200_semantic},~\ref{tab:pipeline_comparison}, and~\ref{tab:ov3d_reimpl}. This standardization enables direct performance comparisons and validates the improvements achieved by our proposed approach.


\input{tables/ablation_clip}

\subsection{Annotation-free 3D Referring Segmentation}
To quantitatively analyze the attention between free-form text queries and point features shown in Fig.~\ref{fig:qual}, we leverage the 3D referring segmentation annotations from ScanRefer~\cite{chen2020scanrefer}. 
This allows us to evaluate how well our model's attention aligns with human-annotated referring expressions in 3D scenes.
Specifically, we evaluate our model's zero-shot performance on the ScanRefer validation set without any fine-tuning on the 3D referring segmentation task.
For each referring expression in ScanRefer, we use it as a text query to obtain attention maps between the query and point features.
We then threshold the cosine similarity scores to obtain binary segmentation masks, where points with positive similarity scores (greater than 0) are considered as the predicted region.
The predicted masks are compared against ground truth annotations using standard IoU metrics.

As shown in Table~\ref{tab:scanrefer}, our method outperforms both OpenScene-3D~\cite{Peng2023OpenScene} and RegionPLC~\cite{yang2024regionplc}, demonstrating its superior ability to highlight relevant regions for free-form text queries.
These results demonstrate that our model not only excels at semantic segmentation with simple class names but also achieves superior zero-shot performance on more complex free-form referring expressions, quantitatively validating its effectiveness as a general-purpose 3D vision-language foundation model.

\input{tables/scanrefer}
