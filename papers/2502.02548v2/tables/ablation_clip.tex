\begin{table}[h]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{l|cc|cc}
\midrule
\multirow{2}{*}{CLIP Model} & \multicolumn{2}{c|}{ScanNet20~\cite{dai2017scannet}} & \multicolumn{2}{c}{ScanNet200~\cite{scannet200}} \\
 & f-mIoU & f-mAcc & f-mIoU & f-mAcc \\
\midrule
CLIP/B16~\cite{radfordLearningTransferableVisual2021} & 67.1 & 83.8 & 14.4 & 27.7 \\
CLIP/B32~\cite{radfordLearningTransferableVisual2021} & \underline{67.8} & \underline{84.5} & 14.8 & 26.5 \\
CLIP/L14@336px~\cite{radfordLearningTransferableVisual2021} & 64.2 & 81.9 & 14.9 & 27.7 \\
SigLIP~\cite{zhai2023sigmoid} & 66.3 & \textbf{84.6} & \underline{15.3} & \textbf{29.0} \\
Recap-CLIP~\cite{li2024if} & \textbf{68.1} & 84.4 & \textbf{15.7} & \underline{28.3} \\
\midrule
\end{tabular}
}
\caption{\textbf{Impact of CLIP text encoders on open-vocabulary 3D semantic segmentation.} We train our SPUNet34C architecture on the full \dataname dataset (5 subsets) with different CLIP text encoders while keeping other components fixed. Recap-CLIP~\cite{li2024if} achieves the best overall performance across both ScanNet20 and ScanNet200 benchmarks, demonstrating the importance of text encoder selection for zero-shot generalization.}
\label{tab:clip_variants}
\end{table}
