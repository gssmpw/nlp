\documentclass[11pt]{article}
\usepackage[T1]{fontenc}
\usepackage[dvips,letterpaper,margin=1.1in]{geometry}
\usepackage[toc,page]{appendix}
\usepackage{etoolbox}
\appto\appendix{\addtocontents{toc}{\protect\setcounter{tocdepth}{2}}}
\addtocontents{toc}{\protect\setcounter{tocdepth}{2}}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} %
\usepackage[dvipsnames]{xcolor}

\definecolor{mydarkblue}{rgb}{0,0.08,0.55}
\definecolor{mypurple}{rgb}{0.4,0.0,0.15}
\usepackage{hyperref}
\hypersetup{
    colorlinks = true,
    linkcolor = black,
    urlcolor  = black,
    citecolor = mydarkblue,
    anchorcolor = blue
}
\input{math_commands.tex}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{comment}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{xcolor}
\usetikzlibrary{shadows}
\usepackage{url}            
\usepackage{booktabs}       
\usepackage{amsfonts}       
\usepackage{nicefrac}      
\usepackage{microtype}      
\usepackage{mathtools}
\usepackage{dsfont}
\usepackage{mathrsfs} 

\usepackage{hyperref}
\usepackage{url}

\usepackage{mathtools}
\usepackage{svg}
\usepackage{sidecap}
\usepackage{authblk}

\newcommand{\xlrharpoonud}[1]{
  \mathrlap{\xleftharpoonup{\phantom{#1}}}%
  \xrightharpoondown{#1}
}
\newcommand{\xlrharpoondu}[1]{
  \mathrlap{\xleftharpoondown{\phantom{#1}}}%
  \xrightharpoonup{#1}
}

\newcommand{\rebut}[1]{\color{blue} #1 \color{black}}

\title{On the Interpolation Effect of Score Smoothing}

\author[]{\Large Zhengdao Chen\footnote{At Google Research. Email: \href{zhengdao.c3@gmail.com}{zhengdao.c3@gmail.com}.}}
\date{}

\begin{document}

\date{}


\maketitle

\begin{abstract}
Score-based diffusion models have achieved remarkable progress in various domains with the ability to generate new data samples that do not exist in the training set. In this work, we examine the hypothesis that their generalization ability arises from an interpolation effect caused by a smoothing of the empirical score function. Focusing on settings where the training set lies uniformly in a one-dimensional linear subspace, we study the interplay between score smoothing and the denoising dynamics with mathematically solvable models. In particular, we demonstrate how a smoothed score function can lead to the generation of samples that interpolate among the training data within their subspace while avoiding full memorization. We also present evidence that learning score functions with regularized neural networks can have a similar effect on the denoising dynamics as score smoothing.
\end{abstract}

\hypersetup{
    colorlinks = true,
    linkcolor = mypurple,
    urlcolor  = mypurple,
    citecolor = mydarkblue,
    anchorcolor = blue
}

\section{Introduction}
In recent years, score-based diffusion models (DMs) have become an important pillar of generative modeling across a variety of domains from content generation to scientific computing \citep{sohl2015deep, song2019generative, ho2020denoising, ramesh2022hierarchical, abramson2024accurate, videoworldsimulators2024}. After being trained on datasets of actual images or molecular configurations, for instance, such models can transform noise samples into high-quality images or chemically-plausible molecules that do not belong to the training set, indicating an exciting capability of such models to generalize beyond what they have seen and, in a sense, be ``creative''. 

The mechanism behind the creativity of score-based DMs has been a topic of much theoretical interests. At the core of these models is the training of neural networks (NNs) to fit a series of target functions, often called the empirical score functions (ESFs), which are used to drive the denoising process at inference time. The precise form of these functions are determined by the training set and can be computed exactly in principle (though inefficient in practice), but when equipped with the exact precise ESF instead of the approximate version learned by NNs, the diffusion model will end up generate data points that already exist in the training set \citep{yi2023generalization, li2024good}, a phenomenon commonly called \emph{memorization}. This suggests that, for the models to generalize fresh samples beyond the training set, it is crucial to have certain regularizations on the score function (e.g., through NN training) that prevent the ESF from being learned exactly.
From the viewpoint of density estimation, a number of important works have proved sample complexity guarantees for the estimation of score functions via regularized models (discussed in Section~\ref{sec:related}). However, as these results are often specialized for specific underlying distributions and model families, the probe into a more general and intuitive mechanism behind generalization in DMs is still largely open.

A particularly interesting hypothesis by \citet{scarvelis2023closed} is that \emph{smoothing the ESF allows the model to generate samples that interpolate among training data points}, which helps to achieve generalization.
With this insight, they proposed alternative DMs based on computing the smoothed ESF explicitly, but a thorough understanding of the effect of score smoothing on the denoising dynamics was still lacking.
On a different yet closely related front, to understand the hallucination phenomenon in DM, \citet{aithal2024understanding} observed that the models generate samples which interpolate between modes of the training distribution when the NN learns a smoother version of the ESF, but the theoretical mechanism behind this phenomenon also remains unclear.


In this work, we study the effect of score smoothing on the denoising dynamics mathematically
and show how it can lead to data interpolation and subspace recovery in simple cases.
Specifically,
\begin{enumerate}
    \item When the training data are spaced uniformly in $1$-D,
    we show that the denoising dynamics under the smoothed score recovers a non-singular density that interpolates the training set;
    \item If the $1$-D subspace is embedded in higher dimensions, we show that the denoising dynamics under the smoothed score converges to a non-singular interpolating density on the relevant subspace;
    \item We give theoretical and empirical evidence that the our analysis based on score smoothing is relevant for understanding how NN-based DMs avoid memorization. 
\end{enumerate}
Together, we believe these results shed light on how score smoothing can be an important causal link for understanding how NN-based DMs avoid memorization and motivate the exploration of alternative designs of DMs that generalize.




The rest of the paper is organized as follows. After briefly reviewing the background in Section~\ref{sec:background}, we examine the smoothing of ESF in the one-dimensional ($1$-D) case and discuss its connections with NN regularization in Section~\ref{sec:1d}. The trajectory of the denoising dynamics under the smoothed score is derived in Section~\ref{sec:back_sm}. In Section~\ref{sec:d>1}, we generalize the analysis to the higher-dimensional case when the training data belongs to a hidden line segment. In Section~\ref{sec:numerical}, we provide experimental evidence that NN-learned SF also exhibits an interpolation effect that is similar to that of score smoothing.

\paragraph{Notations}
For $x, \delta > 0$, we write $p_{\mathcal{N}}(x; \sigma) =(\sqrt{2 \pi }\sigma)^{-1} \exp(-x^2/(2 \sigma^2))$ denote the $1$-D Gaussian density with mean zero and variance $\sigma^2$; $\boldsymbol{\delta}_x$ for the Dirac delta distribution centered at $x \in \sR$; and $\text{sgn}(x)$ for the sign of $x$. We write $[n] \coloneqq \{1, .., n\}$ for $n \in \sN_+$. For a vector $\vx = [x_1, ..., x_d] \in \sR^d$, we write $[\vx]_i = x_i$ for $i \in [d]$. The use of big-O notations is explained in Appendix~\ref{app:notations}.

\section{Background}
\label{sec:background}
While score-based DMs have many variants, we will focus on a basic one (called the ``Variance Exploding'' version in \citealt{song2021sde}) for simplicity,
where the \emph{forward (or noising) process} is defined by the following stochastic differential equation (SDE) in $\sR^d$ for $t \geq 0$:
\begin{equation}
    \label{eq:forward}
    d \rvx_t = d \rvw_t~, \quad \rvx_0 \sim p_0~,
\end{equation}
where $\rvw$ is the Wiener process (a.k.a. Brownian motion) in $\sR^d$.
The marginal distribution of $\rvx_t$, denoted by $p_t$, is thus fully characterized by the initial distribution $p_0$ together with the conditional distribution, $p_{t|0}(\vx | \vx') = \prod_{i=1}^d p_{\mathcal{N}}([\vx]_i - [\vx']_i; \sqrt{t})$.
In other words, $p_t$ is obtained by convolving $p_0$ with an isotropic Gaussian distribution with variance $\sigma(t)^2 = t$ in every direction.

A key observation is that this process is equivalent (in marginal distribution) to a deterministic dynamics often called the \emph{probability flow ordinary differential equation (ODE)} \citep{song2021sde}:
\begin{align}
\label{eq:back}
    d \rvx_t =&~ -\tfrac{1}{2} \vs_t(\rvx_t) dt~,
\end{align}
where $\vs_t(\rvx) = \nabla \log p_t(\rvx)$ is the \emph{score function (SF)} associated with the distribution $p_t$ \citep{hyvarinen2005estimation}.
In generative modeling, $p_0$ is often a distribution of interest that is hard to sample directly (e.g. the distribution of cat images in pixel space), while when $T$ is large, $p_T$ is always close to a Gaussian distribution (with variance increasing in $T$), from which samples are easy to obtain. Thus, to obtain samples from $p_0$, a insightful idea is to first sample from $p_T$ and then follow the \emph{reverse (or denoising) process} by simulating (\ref{eq:back}) backward-in-time (or its stochastic variants that are equivalent in marginal distribution, which we will not focus on. 
\begin{figure}
\begin{minipage}{0.5 \linewidth}
    \caption{From the noised empirical distribution (i.e., $p^{(n)}_{t_0}$; \textbf{middle}), running the denoising dynamics (\ref{eq:back}) using the ESF ($\vs_t = \nabla \log p^{(n)}_t$) leads back to the empirical distribution of the training set (i.e., $p^{(n)}_{0}$; \textbf{top}), whereas using a smoothed SF (e.g. the LA-PL SF, $\vs_t = \hat{\vs}^{(n)}_{t, \delta_t}$; or NN-learned) produces a distribution that interpolates among the training set on the relevant subspace (e.g., $\hat{p}^{(n, t_0)}_0$ in the case of LA-PL SF; \textbf{bottom}).}
    \label{fig:intro}
\end{minipage}
\hspace{5pt}
\begin{minipage}{0.5 \linewidth}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/fig_1_vertical.pdf}
\end{minipage}
\end{figure}

A main challenge in this procedure lies in the estimation of the family of SFs, $\nabla \log p_t$ for $t \in [0, T]$.
In reality, we have no prior knowledge of each $p_t$ (or even $p_0$) but just a training set $S = \{ \vy_k \}_{k \in [n]}$ usually assumed to be sampled from $p_0$. 
Thus, we only have access to an \emph{empirical} version of the noising process, where the same SDE (\ref{eq:forward}) is initialized at $t=0$ with not $p_0$ but the uniform distribution over $S$ (i.e., $\rvx_0 \sim p^{(n)}_0 \coloneqq \frac{1}{n} \sum_{k=1}^n \delta_{\vy_k}$), and hence the marginal distribution of $\rvx_t$ is $p^{(n)}_t(\vx) \coloneqq \frac{1}{n} \sum_{k=1}^n p_{t | 0}(\vx | \vy_k)$, called the \emph{noised empirical distribution} at time $t$. To obtain a proxy for $\nabla \log p_t$, 
one often uses an NN to represent a (time-dependent) score estimator, $s_{\thetab}(\vx, t)$,
and train its parameters to minimize variants of the following time-averaged score matching loss \citep{song2021sde}:
\begin{equation}
\label{eq:emp_loss}
    \min_{\thetab} \frac{1}{T} \int_0^T L^{(n)}_t[s_{\thetab}(~\cdot~, t)] dt~,
\end{equation}
where
\begin{equation}
\label{eq:Lnt}
    L^{(n)}_t[f] \coloneqq t \cdot \sE_{\rvx \sim p^{(n)}_t} \Big [ \big \| f(\rvx) - \nabla \log p^{(n)}_t(\rvx)  \big \|^2 \Big ]~
\end{equation}
measures the $L^2$ distance between the score estimator and the \emph{empirical score function (ESF)} at time $t$ --- $\nabla \log p^{(n)}_t$ --- with respect to $p^{(n)}_t$.
The scaling factor of $t \propto 1 / \sE[\nabla \log p_{t | 0}(\rvx_t | \rvx_0)]$ serves to balance the contribution to the loss at different $t$ \citep{song2021sde}.

In practice, the minimization problem (\ref{eq:emp_loss}) is often solved via Monte-Carlo sampling combined with ideas from \citet{hyvarinen2005estimation, vincent2011connection}. However, we know the minimum is attained uniquely by the ESF itself, which can be computed in closed form based on $S$ (details in Section~\ref{sec:1d}). So what if we use the ESF directly in the denoising dynamics (\ref{eq:back}) instead of an NN approximation? In that case, we arrive at an empirical version of the
probability flow ODE:
\begin{equation}
\label{eq:back_n}
    d \rvx_t = -\tfrac{1}{2} \nabla \log p^{(n)}_t(\rvx_t)~,
\end{equation}
which exactly reverses the empirical forward process that adds noise to the training set, and hence the outcome at $t=0$ is inevitably $p^{(n)}_0$. In other words, the model \emph{memorizes} the training data instead of generating fresh samples. This suggests that the creativity of the diffusion model hinges on a \emph{sub-optimal} solution to the minimization problem (\ref{eq:emp_loss}) and an \emph{imperfect} approximation to the ESF. 
Indeed, the memorization phenomenon has been observed in practice when the models have large capacities relative to the training set size \citep{gu2023memorization, kadkhodaie2024generalization}, which likely results in too good an approximation to the ESF. This leads to the hypothesis that regularizing the score estimator gives rise to the model's ability to generalize out of the training set, though a theoretical understanding of the mechanism is still under development. 

In this work, we will focus on simple setups with fixed training sets to show mathematically how smoothing the ESF can enable the generation of new samples that interpolate among the training data.





    
    

\section{Score Function and its Smoothing}
\label{sec:1d}
\begin{figure}
\hspace{15pt}
\begin{minipage}{0.34 \linewidth}
    \caption{Comparing the ESF, $\nabla \log p^{(n)}_t$; its PL approximation (\ref{eq:bar_s}), $\bar{s}^{(n)}_t$; and the LA-PL SF (\ref{eq:hat_s_1d}), $\hat{s}^{(n)}_{t, \delta}$ in the case of $d=1$ and $n=2$ discussed in Section~\ref{sec:1d}.}
    \label{fig:score_1d}
\end{minipage}
\hspace{10pt}
\begin{minipage}{0.62 \linewidth}
    \centering
    \includegraphics[width=0.78\linewidth] {figures/3_scores_1d.png}
    \end{minipage}
\end{figure}
Let us focus on a simplest setup where $d = 1$ and $S = \{y_1=-1, y_2=1\}$ consists of only two points. 
In Appendix~\ref{app:n>2}, we give a straightforward generalization of the analyses in Sections~\ref{sec:1d} - \ref{sec:d>1} to the scenario where $S$ consists of $n$ points spaced uniformly on an interval $[-D, D]$.

In the $n = 2$ case, at time $t$, the noised empirical distribution is $p^{(n)}_t(x) = \frac{1}{2}(p_{\mathcal{N}}(x+1; \sqrt{t}) + p_{\mathcal{N}}(x-1; \sqrt{t}))$, and the (scalar-valued) ESF takes the form of 
\begin{equation}
\label{eq:esf}
    \tfrac{d}{dx} \log p^{(n)}_t(x) = (\hat{x}^{(n)}_t(x) - x) / t~,
\end{equation}
where 
\begin{equation}
\label{eq:hat_x}
    \hat{x}^{(n)}_t(x) \coloneqq \sE_{0 | t}[\rvx_0 | \rvx_t = x] = \tfrac{p_{\mathcal{N}}(x-1; \sqrt{t}) - p_{\mathcal{N}}(x+1; \sqrt{t})}{p_{\mathcal{N}}(x-1; \sqrt{t}) + p_{\mathcal{N}}(x+1; \sqrt{t})}
\end{equation}
lies between $\pm 1$ and has the same sign as $x$. 
When $t$ decreases, the Gaussians sharpen and $\hat{x}^{(n)}_t(x)$ approaches $\text{sgn}(x)$, allowing us to approximate the ESF by a piece-wise linear (PL) function, 
\begin{equation}
\label{eq:bar_s}
    \bar{s}^{(n)}_t(x) = (\text{sgn}(x) - x) / t~.
\end{equation}
It is then intuitive that the backward dynamics is attracted to either of $\pm 1$ (whichever is closer) thus explaining the collapse onto the training set when we denoise with the exact ESF. Hence, to avoid memorization, one needs to go beyond the exact ESF for driving the denoising dynamics. 

% Inspired by the PL approximation (\ref{eq:bar_s}), i
In this work, we will mainly focus on the following family of SF and study its effect on the denoising dynamics (\ref{eq:back}) when $t$ is small:
\begin{equation}
\label{eq:hat_s}
    \hat{s}^{(n)}_{t, \delta}(x) \coloneqq \begin{cases}
        -(x+1)/t~,& \quad \text{ if } x \leq \delta - 1 ~, \\
        -(x-1)/t~,& \quad \text{ if } x \geq 1 - \delta~, \\
        \delta / (1 - \delta)  \cdot x / t~,& \quad \text{ if } x \in (\delta - 1, 1 - \delta)~.
    \end{cases}
\end{equation}
where $\delta \in (0, 1]$ can be chosen to depend on $t$.
As illustrated in Figure~\ref{fig:score_1d}, $\hat{s}^{(n)}_{t, \delta}$ is PL and matches $\bar{s}^{(n)}_t$ except on the interval $[\delta - 1, 1 - \delta]$. When $\delta = 1$, $\hat{s}^{(n)}_{t, 1} \equiv \bar{s}^{(n)}_t$. When $\delta \in (0, 1)$, as will be made clear in Section~\ref{sec:local_avg}, we refer to $\hat{s}^{(n)}_{t, \delta}$ as a \emph{locally-averaged piece-wise-linearized (LA-PL) SF}. 

In the rest of this section, based on connections with function smoothing and NN learning, we discuss two motivations for studying the LA-PL SF (especially under a specific choice of $\delta$ as a function of $t$) as a smoothed estimator for the ESF.

\subsection{Connection to Function Smoothing via Local Averaging}
\label{sec:local_avg}
Given a function $f$ on $\sR$ and $w > 0$, we define a \emph{locally-averaged (LA)} version of $f$ with window size $w$ to be the following function: 
\begin{equation}
\label{eq:sm_1d}
    (w * f)(x) \coloneqq \frac{1}{2 w} \int_{x-w}^{x+w} f(x')dx'~.
\end{equation}
It is then not hard to see that, for $w \in (0, 1]$, 
\begin{equation}
\label{eq:tau_sig}
    (w * \bar{s}^{(n)}_t)(x) = \hat{s}^{(n)}_{t, 1 - w}(x)~, 
    \quad \forall x \in \sR~.
\end{equation}
This explains the naming of $\hat{s}^{(n)}_{t, \delta}$. In particular, a smaller $\delta$ means a higher level of local smoothing. 

As $\bar{s}^{(n)}_t$ approximates the ESF when $t$ is small, we expect from (\ref{eq:tau_sig}) that $\hat{s}^{(n)}_{t, \delta}$ is also close to $(1-\delta) * \tfrac{d}{dx} \log p^{(n)}_t$.\footnote{Connections among these various SFs are summarized as diagrammatically by Figure~\ref{fig:tikzcd} in Appendix~\ref{app:sf_variants}.} This can be made rigorous in the setting where $\delta$ is fixed while $t \to 0$, in which case the $L^2$ distance with respect to $p^{(n)}_t$ between the two functions decays exponentially fast in $1/t$ (proved in Appendix~\ref{sec:pf_lem_2}):
\begin{lemma}
\label{lem:2}
For any fixed $\delta \in (0, 1]$, $\exists t_1, C > 0$ such that $\forall t \in (0, t_1)$, it holds that 
\begin{equation}
    t \cdot \sE_{x \sim p^{(n)}_t} \Big [ \big \| \hat{s}^{(n)}_{t, \delta}(x) - ((1-\delta) * \tfrac{d}{dx} \log p^{(n)}_t )(x)  \big \|^2 \Big ] \leq C \exp (- \delta^2 / (9 t))~.
\end{equation}
\end{lemma}
This result motivates us to focus on $\hat{s}^{(n)}_{t, \delta}$ as a simpler proxy for understanding the smoothed ESF.

\paragraph{Coupling $\delta$ with $t$}
Our subsequent analysis on the dynamics will focus on the LA-PL SF under a time-dependent $\delta$ that is chosen proportionally to $\sqrt{t}$, i.e., for some $\kappa > 0$,
\begin{equation}
    \label{eq:delta_t}
    \delta_t = \kappa \sqrt{t}~,
\end{equation}
corresponding to choosing a larger window size for LA as $t$ decreases. To motivate this choice, we observe that as $t \to 0$, the empirical distribution $p^{(n)}_t$ becomes increasingly concentrated near $\pm 1$, and hence for any fixed $\delta$, the difference between two functions on $[\delta - 1, 1 - \delta]$ contributes less and less to their $L^2$ distance with respect to $p^{(n)}_t$.
In fact, the following result (proved in Appendix~\ref{sec:pf_prop2}) shows that as $t \to 0$, the score matching loss still remains at a constant order even if we let $\delta_t$ decrease according to (\ref{eq:delta_t}):
\begin{proposition}
\label{lem:3}
Let $\delta_t = \kappa \sqrt{t}$ for some $\kappa > 0$.
Then $\exists t_1, C > 0$ such that
$\forall t \in (0, t_1)$,
\begin{equation}
     \tfrac{1}{2} F(\kappa) - C \sqrt{t} \leq L^{(n)}_t[\hat{s}^{(n)}_{t, \delta_t}] \leq \tfrac{1}{2} F(\kappa) + C \sqrt{t}~,
\end{equation}
where $t_1$ and $C$ depend only on $\kappa$ and the function $F$ decreases strictly from $1$ to $0$ on $[0, \infty)$.
\end{proposition}
Thus, if we choose $\hat{s}^{(n)}_{t, \delta_t}$ as the score estimator and set (\ref{eq:delta_t}), then the time-averaged score matching loss (\ref{eq:emp_loss}) has roughly balanced contributions from different $t$ when $t$ is small.

\subsection{Connection to Smoothness Measure Induced by NN Regularization}
When fitting a function in one dimension, it is shown in \citet{savarese2019infinite} that controlling the weight norm of a two-layer ReLU NN (with unregularized bias and linear terms) is essentially equivalent to penalizing a certain \emph{non-smoothness} measure of the estimated function defined as: 
\begin{equation}
    R[f] \coloneqq \int_{-\infty}^{\infty} |f''(x)| dx~,
\end{equation}
where $f''$ is the weak second derivative of the function $f$. 
Inspired by this result, 
for $\epsilon, t > 0$, we consider the following family of variational problems in function space:
\begin{equation}
\label{eq:reg_min}
    \begin{split}
        r_{t, \epsilon}^* \quad \coloneqq \quad \inf_f \qquad & R[f] \\
        \text{s.t.} \qquad & L^{(n)}_t[f] \leq \epsilon~,
    \end{split}
\end{equation}
with the infimum taken over all functions $f$ on $\sR$ that are twice differentiable except on a finite set (a broad class of functions that include, for example, any function representable by a finite-width NN). Thus, we are seeking to minimize the non-smoothness measure among functions that are $\epsilon$-close to the ESF according to the $L^2$ distance with respect to $p^{(n)}_t$. When $\epsilon = 0$, only the ESF itself satisfies the constraint and hence it attains the minimum uniquely; whereas if $\epsilon$ is small but positive, this problem is concerned with on \emph{how} the non-smoothness penalty biases the score estimator away from the ESF.

Due to non-differentiability of the functional $R$, the variational problem (\ref{eq:reg_min}) is hard to solve directly. However, we can show that the series of functions $\hat{s}^{(n)}_{t, \delta_t}$ together with (\ref{eq:delta_t}) achieves \emph{near} optimality in the following sense: 
\begin{proposition}
\label{prop:reg_min}
    Given any fixed $\epsilon \in (0, 0.015)$, if we choose $\delta_t = \kappa \sqrt{t}$ with any $\kappa \geq F^{-1}(\epsilon)$, then there exists $t_1 > 0$ (dependent on $\kappa$) such that the following holds for all $t \in (0, t_1)$:
    \begin{itemize}
    \item $L^{(n)}_t[\hat{s}^{(n)}_{t, \delta_t}] < \epsilon$, and hence the function $\hat{s}^{(n)}_{t, \delta_t}$ belongs to the feasible set of (\ref{eq:reg_min});
        \item $R[\hat{s}^{(n)}_{t, \delta_t}] < (1 + 8 \sqrt{\epsilon}) r_{t, \epsilon}^*$.
    \end{itemize}
\end{proposition}
The proof is given in Appendix~\ref{app:pf_prop_reg_min} and based on the following observation: When $t$ is small, the empirical distribution $p^{(n)}_t$ is concentrated near $\{ \pm 1\}$, and hence for any function belonging to the feasible set with a small enough $\epsilon$, its derivative near $\pm 1$ needs to be close to $\frac{d}{dx} \log p^{(n)}_t(\pm 1) \approx -1 / t
$. Combined with the fundamental theorem of calculus, this allows us to give a lower bound on $r^*_{t, \epsilon}$.

As $\epsilon$ controls the strength of the regularization relative to the score matching loss, this setting (fixed $\epsilon$ for different $t$) could be interpreted heuristically as having similar ``amount'' of regularization to the score estimator at different $t$, though the precise correspondence remains to be further elucidated in future work. Meanwhile, in Section~\ref{sec:numerical}, we show empirical evidence that a regularized NN can indeed learn a SF that is close to $\hat{s}^{(n)}_{t, \delta_t}$.


\section{Effect on the Denoising Dynamics}
\label{sec:back_sm}
With the motivations discussed above, we now study the effect on the denoising dynamics of substituting the ESF (\ref{eq:esf}) with $\hat{s}^{(n)}_{t, \delta_t}$ where $\delta_t = \kappa \sqrt{t}$ for some $\kappa > 0$, that is, replacing (\ref{eq:back_n}) by:
\begin{equation}
\label{eq:back_sm}
    \tfrac{d}{dt} \rvx_t = -\tfrac{1}{2} \hat{s}^{(n)}_{t, \delta_t}(\rvx_t)~. %
\end{equation}
Thanks to the piece-wise linearity of (\ref{eq:hat_s}), the backward-in-time dynamics of the ODE (\ref{eq:back_sm}) can be solved analytically in terms of flow maps:
\begin{proposition}
\label{prop:flow_1d}
    For $0 \leq s \leq t < 1 / \kappa^2$, the solution to (\ref{eq:back_sm}) satisfies $\rvx_s = \phi_{s|t}(\rvx_{t})$, where 
     \begin{equation}
    \label{eq:phi_st}
        \phi_{s|t}(x) =
        \begin{cases}
            (1 - \delta_s)/(1 - \delta_t) \cdot x~,& \quad \text{ if } x \in [\delta_t - 1, 1 - \delta_t]  \\
            \sqrt{s} / \sqrt{t} \cdot x - (1 - \sqrt{s} / \sqrt{t})~,& \quad \text{ if } x \leq \delta_t - 1  \\
            \sqrt{s} / \sqrt{t} \cdot x + (1 - \sqrt{s} / \sqrt{t})~,& \quad \text{ if } x \geq 1 - \delta_t  \\
        \end{cases}
    \end{equation}
\end{proposition}
\begin{figure}
\begin{minipage}{0.55 \linewidth}
    \centering
    \includegraphics[width=0.75\linewidth] {figures/phase_2.png}
    \end{minipage}
\hspace{5pt}
\begin{minipage}{0.36 \linewidth}
    \caption{Phase diagram in the $x$-$\sqrt{t}$ plane for the flow solution (\ref{eq:phi_st}) of the dynamics (\ref{eq:back_sm}) in the $d=1$, $n=2$ case analyzed in Section~\ref{sec:back_sm}.}
    \label{fig:phase}
\end{minipage}
\hspace{2pt}
\end{figure}
The proposition is proved in Appendix~\ref{sec:pf_flow_1d}, and we illustrate the trajectories characterized by $\phi_{s|t}$ in Figure~\ref{fig:phase}. The PL nature of $\hat{s}^{(n)}_{t, \delta_t}$ divides the $x-\sqrt{t}$ plane into three regions ($\textbf{A}$, $\textbf{B}$ and $\textbf{C}$) with linear boundaries, each defined by $x \leq - 1 + \delta_t$, $x \geq 1 - \delta_t$ and $\delta_t - 1 \leq x \leq 1 - \delta_t$, respectively. Importantly, trajectories given by $\phi_{s|t}$ do not cross the region boundaries. If at $t_0 > 0$, $\rvx_{t_0}$ falls into region $\textbf{A}$ (or $\textbf{B}$), then as $t$ decreases to $0$, it will follow a linear path in the $x-\sqrt{t}$ plane to $y_1 = -1$ (or $y_2 = 1$). Meanwhile, if $\rvx_{t_0}$ falls into region $\textbf{C}$, then it will follow a linear path to the $x$-axis with a terminal value between $-1$ and $1$, which is given by
\begin{equation}
\label{eq:phi0t}
        \phi_{0|t}(x) =
        \begin{cases}
            x / (1 - \delta_t) ~,& \quad \text{ if } x \in [\delta_t - 1, 1 - \delta_t]  \\
            \text{sgn}(x)~,& \quad \text{ otherwise}  \\
        \end{cases}
    \end{equation}
    
Now, we examine the evolution of the marginal distribution in light of the flow solution above. Suppose we run the denoising dynamics (\ref{eq:back_sm}) backward-in-time from some $t_0 \in (0, 1 / \kappa^2)$ to $0$ and denote the marginal distribution of $\rvx_t$ by $\hat{p}^{(n, t_0)}_t$ for $t \in [0, t_0]$. We assume that $\hat{p}^{(n, t_0)}_{t_0} = p^{(n)}_{t_0}$ is the noised empirical distribution at time $t_0$. This can be viewed as starting from the noised empirical distribution at some large time $T$ (nearly Gaussian), first running the denoising process via the ESF until time $t_0$, and then switching to the LA-PL SF to drive the rest of the denoising process to time zero. In other words, score smoothing kicks in only when $t \in (0, t_0)$. Another equivalent interpretation is that we add noise to the training data for time $t_0$ before denoising them with the LA-PL SF for the same amount (as illustrated in Figure~\ref{fig:intro}).

For $0 < s \leq t \leq t_0$, since the map $\phi_{s|t}$ is invertible and differentiable almost everywhere, we can apply the change-of-variable formula of push-forward distributions to obtain that
\begin{equation}
\label{eq:cov}
    \hat{p}^{(n, t_0)}_s(x) = \begin{cases}
    (1 - \delta_t) / (1 - \delta_s) \cdot \hat{p}^{(n, t_0)}_t((1 - \delta_t) / (1 - \delta_s) \cdot x)~,& \text{if } x \in [\delta_s - 1, 1 - \delta_s] \\
    \delta_t / \delta_s \cdot \hat{p}^{(n, t_0)}_t(\delta_t / \delta_s \cdot x + (\delta_t - \delta_s) / \delta_s)~,& \text{if } x \leq \delta_s - 1 \\
    \delta_t / \delta_s \cdot \hat{p}^{(n, t_0)}_t(\delta_t / \delta_s \cdot x - (\delta_t - \delta_s) / \delta_s)~,& \text{if } x \geq 1 - \delta_s
    \end{cases} 
\end{equation}  
The evolution of the density $\hat{p}^{(n, t_0)}_{s}$ as $s$ decreases from $t_0$ to $0$ is visualized in the lower grey-colored heat map in Figure~\ref{fig:intro}.
When $s = 0$, $\phi_{0|t}$ is invertible only when restricted to $[\delta_t - 1, 1 - \delta_t]$, Thus, the terminal distribution can be decomposed as
\begin{equation}
\label{eq:hat_p_0}
    \hat{p}^{(n, t_0)}_{0} = a_+ \boldsymbol{\delta}_1 + a_- \boldsymbol{\delta}_{-1} + (1 - a_+ - a_-) \tilde{p}^{(n, t_0)}_{0}~,
\end{equation}
where
$\tilde{p}^{(n, t_0)}_0$ is a probability distribution and for all $t \in (0, t_0]$, it holds that $a_+ = \sE_{\rvx \sim \hat{p}^{(n, t_0)}_t}[ \mathds{1}_{\rvx \geq 1 - \delta_t}]$, $a_- = \sE_{\rvx \sim \hat{p}^{(n, t_0)}_t}[ \mathds{1}_{\rvx \leq \delta_t - 1}]$, and
\begin{equation}
\label{eq:tilde_pt}
    \tilde{p}^{(n, t_0)}_0(x) = \begin{cases}
        (1 - \delta_t) / (1 - a_+ - a_-) \cdot \hat{p}^{(n, t_0)}_t( (1 - \delta_t) x)~,& \quad \text{ if } x \in [-1, 1] \\
        0 ~,& \quad \text{ otherwise}
    \end{cases} 
\end{equation}
In particular, since $\hat{p}^{(n, t_0)}_{t_0}$ has a positive density on $[\delta_{t_0} - 1, 1 - \delta_{t_0}]$, $\tilde{p}^{(n, t_0)}_{0}$ has a positive density on $[-1, 1]$ as well, corresponding to a smooth interpolation between the two training data points. 

Moreover, (\ref{eq:tilde_pt}) allows us to prove KL-divergence bounds for $\hat{p}^{(n, t_0)}_0$ based on those of $\hat{p}^{(n, t_0)}_t$. For example, letting $u_a$ denote the uniform density on $[-a, a]$, we have:
\begin{proposition}
\label{cor:kl}
    Let $\kappa > 0$ and $0 < t_0 < 1 / \kappa^2$. If $\rvx_t$ solves (\ref{eq:back_sm}) backward-in-time with $\rvx_{t_0} \sim p^{(n)}_{t_0}$, then there is
    $\text{KL}(u_{1} || \hat{p}^{(n, t_0)}_0) \leq \tfrac{1}{3t_0(1 - \kappa \sqrt{t_0})} + \log \big (\tfrac{\sqrt{t_0}}{1 - \kappa \sqrt{t_0}} \big ) + \log(2 \sqrt{2 \pi}) < \infty$.
\end{proposition}
This result is proved in Appendix~\ref{app:pf_prop_kl}. We note that the choice of the uniform density as the target to compare $\hat{p}^{(n, t_0)}_0$ with is an arbitrary one, since the training set is fixed rather than sampled from the uniform distribution. But our main point is to highlight the component of $\hat{p}^{(n, t_0)}_0$ that smoothly interpolates among the training set. In contrast, running the entire denoising dynamics with the exact ESF results in $p^{(n)}_0$, which is fully singular and has an infinite KL-divergence with \emph{any} smooth density on $[-1, 1]$.











\section{Higher Dimensions: Line Segment as Hidden Subspace}
\label{sec:d>1}

Let us consider a case where $S = \{ \vy_1 = [-1, 0, ..., 0], \vy_2 = [1, 0, ..., 0] \} \subseteq \sR^d$ consists of two points on the $[\vx]_1$-axis (and as we show in Appendix~\ref{app:n>2}, the analysis can also be generalized to the case where $S$ contains $n$ points spaced uniformly on the $[\vx]_1$-axis).
In this case, the noised empirical density is $p^{(n)}_t(\vx) = \frac{1}{2} \big ( p_{\mathcal{N}}([\vx]_1 + 1; \sqrt{t}) + p_{\mathcal{N}}([\vx]_1 - 1; \sqrt{t}) \big ) \prod_{i=2}^d p_{\mathcal{N}}([\vx]_i; \sqrt{t})$,
and the (vector-valued) ESF is given by $\nabla \log p^{(n)}_t(\vx) = [\partial_1 \log p^{(n)}_t(\vx), ..., \partial_d \log p^{(n)}_t(\vx)]$, where
\begin{equation}
\label{eq:esf_d}
\begin{split}
    \partial_1 \log p^{(n)}_t(\vx) =&~ \big (\hat{x}^{(n)}_t([\vx]_1) - [\vx]_1 \big ) / t~, \\
    \forall i \in \{2, ..., n\}~, \quad \partial_i \log p^{(n)}_t(\vx) =&~ - [\vx]_i / t~, \hspace{80pt}
\end{split}
\end{equation}
where $\hat{x}^{(n)}_t$ is defined in the same way as in (\ref{eq:hat_x}). Relative to the subspace on which the training set belongs --- the $[\vx]_1$-axis --- we may refer to the first dimension as the \emph{tangent} direction and the other dimensions as the \emph{normal} directions.

To generalize the notion of score smoothing beyond dimension one, we first consider a simple extension of the definition (\ref{eq:sm_1d}) that takes averages over centered cubes in higher dimensions.
Namely, given $w > 0$ and a (vector-valued) function $\vf$ on $\sR^d$, we define
\begin{equation}
\label{eq:sm_cube}
    (w * \vf)(\vx) = \frac{1}{(2 w)^d} \int_{[\vx]_1-w}^{[\vx]_1 + w} ... \int_{[\vx]_d-w}^{[\vx]_d + w} \vf(\vx') d [\vx']_1 ... d [\vx']_d~.
\end{equation}
To understand how the ESF behaves under (\ref{eq:sm_cube}),
we first observe that for each $i \in [d]$, $\partial_i \log p^{(n)}_t(\vx)$ depends only on the corresponding $x_i$, and hence the repeated integral in (\ref{eq:sm_cube}) reduces to only the one in the $i$th dimension. For $i > 1$, $\partial_i \log p^{(n)}_t(\vx)$ is a linear function of $[\vx]_i$, which is invariant when averaged over a centered interval.
For $i=1$, $w * \partial_1 \log p^{(n)}_t(\vx)$ has the same form as in the $d=1$ case after a projection onto the first dimension, 
which can therefore be approximated by $\hat{s}^{(n)}_{t, 1-w}([\vx]_1)$ on the same theoretical ground as Lemma~\ref{lem:2}. In summary, for $w \in (0, 1)$, we have
\begin{equation}
\begin{split}
    w * \nabla \log p^{(n)}_t(\vx) =&~ [w * \partial_1 \log p^{(n)}_t(\vx), -[\vx]_2 / t, ..., -[\vx]_d / t]^\intercal \\
    \approx &~ [\hat{s}^{(n)}_{t, 1-w}([\vx]_1), -[\vx]_2/t, ..., -[\vx]_d/t]^\intercal = \hat{\vs}^{(n)}_{t, 1-w}(\vx)~,
\end{split}
\end{equation}
where for $\delta \in (0, 1]$, we define a generalization of the LA-PL SF in this setting by
\begin{equation}
\label{eq:hat_s_d>1}
    \hat{\vs}^{(n)}_{t, \delta}(\vx) \coloneqq [\hat{s}^{(n)}_{t, \delta}([\vx]_1), -[\vx]_2/t, ..., -[\vx]_d/t]^\intercal~,
\end{equation}
with $\hat{s}^{(n)}_{t, \delta}: \sR \to \sR$ defined in the same way as in (\ref{eq:hat_s}).

Similar to in Section~\ref{sec:back_sm}, we consider a scenario where the smoothing level depends on time through $w_t = 1 - \delta_t$ with (\ref{eq:delta_t}), in which case the dynamics is given by $\frac{d}{dt} \rvx_t = -\frac{1}{2}\hat{\vs}^{(n)}_{t, \delta_t}(\rvx_t)$ and is nicely decoupled in different dimensions:
\begin{align}
\tfrac{d}{dt} [\rvx_{t}]_1 =&~ - \tfrac{1}{2} \hat{s}^{(n)}_{t, \delta_t} ([\rvx_{t}]_1)~, \label{eq:sm_back_d=1} \\
    \forall i \in \{2, ..., d\}~, \quad \tfrac{d}{dt} [\rvx_{t}]_i =&~ \tfrac{1}{2} [\rvx_{t}]_i / t~. \hspace{80pt} \label{eq:sm_back_d>1}
\end{align}
Based on our findings in the $d=1$ case, we see that this dynamics can be solved as follows:
\begin{proposition}
\label{prop:flow_d>1}
    For $0 \leq s \leq t < 1 / \kappa^2$, the solution to (\ref{eq:sm_back_d=1}, \ref{eq:sm_back_d>1}) is given by $\rvx_s = \Phi_{s|t}(\rvx_{t}) \coloneqq [\phi_{s|t}(\rx_{t, 1}), 0, ..., 0]$ with $\phi_{s|t}$ defined as in (\ref{eq:phi_st}) and (\ref{eq:phi0t}). Hence, if run backward-in-time with the marginal distribution of $\rvx_{t_0}$ being $\hat{p}^{(n, t_0)}_{t_0} \times p_{\mathcal{N}}(~\cdot~; \sqrt{t_0}) \times  ... \times p_{\mathcal{N}}(~\cdot~; \sqrt{t_0})$, the marginal distribution of $\rvx_t$ is given by $\hat{p}^{(n, t_0)}_t \times p_{\mathcal{N}}(~\cdot~; \sqrt{t}) \times  ... \times p_{\mathcal{N}}(~\cdot~; \sqrt{t})$, where $\hat{p}^{(n, t_0)}_t$ satisfies (\ref{eq:cov}) or (\ref{eq:hat_p_0}).
\end{proposition}
Notably, we see distinct dynamical behaviors in the tangent versus normal directions. As $t \to 0$, the trajectory converges to zero at a rate of $\sqrt{t}$ in the normal directions, resulting in a uniform collapse of the $d$-dimensional space onto the $[\vx]_1$-axis, whereas in the tangent direction the trajectory is equivalent to the $d=1$ case. In particular, if the marginal distribution of $[\rvx_{t}]_1$ has a positive density on $[\delta_t - 1, 1 - \delta_t]$, then so will $[\rvx_{0}]_1$ on $[-1, 1]$, meaning that $\rvx_{0}$ has a non-singular density that interpolates smoothly among the training data on the desired $1$-D subspace.

\paragraph{Comparison with inference-time early stopping.} The behavior above is different from what can be achieved by denoising under the exact ESF, either by running it fully to $t = 0$ or by stopping it at some positive $t_{\min}$: in either case, the terminal distribution has infinite KL-divergence from any smooth density supported on the $1$-D subspace. Specifically, the former leads to the collapse onto the training data points (i.e., full memorization), while in the latter case the terminal distribution is still supported in all $d$ dimensions and equivalent to corrupting the training data directly by Gaussian noise. Hence, without modifying the ESF, early stopping alone is not sufficient for inducing a proper generalization behavior.

\paragraph{On coordinate dependence.}
A caveat of the results above is that the definition (\ref{eq:sm_cube}) depends on the choice of the coordinate system while the hidden subspace is assumed to be one of the axes. In other words, by defining the smoothed score as such, we are possibly providing ``hints'' on which possible directions the hidden subspace might have. To avoid this concern, we may adopt alternative extensions of the definition (\ref{eq:sm_1d}) to higher dimensions that are invariant to coordinate rotations, an example of which is detailed in Appendix~\ref{sec:sm_alt}. Notably, with this definition, the denoising dynamics via the smoothed score remains identical to (\ref{eq:sm_back_d=1}) and (\ref{eq:sm_back_d>1}) under the same PL approximation, hence achieving a more genuine form of subspace recovery without prior information of its direction.

\section{Numerical Experiments}
\label{sec:numerical}
\subsection{Experiment $1$: NN-learned SF ($d=1$)}
\label{sec:nn_score}
To examine the smoothing effect of NN learning empirically, we compare NN-learned SF with the LA-PL SF in the setting of $d=1$ and $n=2$ considered in Section~\ref{sec:1d}. We train a two-layer NN to fit the ESF at a fixed $t$ under weight decay regularization of various strengths, with additional details given in Appendix~\ref{app:details_exp_1}. As shown in Figure~\ref{fig:nn_score_1d}, the score estimators learned by NNs are close to being PL and 
can be approximated remarkably well by $\hat{s}_{t, \delta}$ under suitable choices of $\delta$. In particular, a stronger level of regularization corresponds to a smaller $\delta$ and hence a larger degree of smoothing. This provides initial evidence that, despite the simplification, our theoretical analyses based on the LA-PL SF may indeed be relevant to understanding NN-learned SF.
\begin{figure}
\centering
    \includegraphics[width=0.9 \textwidth]{figures/nn_score.png}
    \caption{Similarities between NN-learned SF ($s^{\text{NN}}_{t, \lambda}$) under increasing strengths of regularization, $\lambda$ (\textbf{Left}) and the LA-PL SF ($\hat{s}^{(n)}_{t, \delta}$) with decreasing values of $\delta$ (\textbf{Right}) in the $d=1$, $n=2$ case with a fixed $t$. Details of the experiment setup are discussed in Section~\ref{sec:nn_score}.}
    \label{fig:nn_score_1d}
\end{figure}

\subsection{Experiment $2$: Denoising with Smoothed and NN-learned SF ($d=2$)}
\label{sec:exp_score}
To validate the effect of score smoothing on the denoising dynamics and compare the different variants of smoothed SFs, we choose the setup in Section~\ref{sec:d>1} with $d=2$ and $n=4$ and run the denoising dynamics (\ref{eq:back}) under three choices of the SF: \textbf{(\romannumeral 1)} the ESF ($\vs_t = \nabla \log p^{(n)}_t$), 
\textbf{(\romannumeral 2)} the LA-PL SF ($\vs_t = \hat{\vs}^{(n)}_{t, \delta_t}$ from (\ref{eq:hat_s_d>1})), and \textbf{(\romannumeral 3)} an NN-learned SF with $t$ as an input ($\vs_t = \vs^{\text{NN}}_t$).

All three processes are initialized at $t_0 = 0.02$ with the same marginal distribution $\rvx_{t_0} \sim p^{(n)}_{t_0}$ 
(and thus \textbf{(\romannumeral 1)} is equivalent to an exact reversal of the forward process). 
The results are illustrated in Figure~\ref{fig:exp} and further details can be found in Appendix~\ref{app:details_exp_2}.
\begin{figure}[!t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/log_density_1.pdf} \\
    \caption{Results of the experiment described in Section~\ref{sec:exp_score}, where each column shows the denoising process under one of three choices of the SF in the $d=2$, $n=4$ setting.
    Each process starts at $t=t_0=0.02$ with the marginal distribution $p^{(n)}_{t_0}$ and evolves backward-in-time according to the respective SF. At $t = t_0$, $t_0/4$ and $10^{-5}$, we plot (a) the samples from the denoising processes in $\sR^2$ and (b) the density histograms (in log scale) of their projection onto the first dimension. In (b), the colored curves are the analytical predictions of $\hat{p}^{(n, t_0)}_t$ (for $t = t_0$ and $t_0/4$) and $\tilde{p}^{(n, t_0)}_0$ (for $t=10^{-5}$), with their formulas given in Appendix~\ref{app:denoise_n>2}. A direct comparison of the three SFs is given in Figures~\ref{fig:2d_score_dim1}~-~\ref{fig:2d_score_slice}.
    A video animation of the three denoising processes and the evolution of the corresponding SFs can be found at \href{https://www.dropbox.com/scl/fo/2dx7awl6nvajktgfovf7m/AGsUqHiFa9wvx0-RxM3HVh4?rlkey=0r1qz8ulq63cyw7xhydmcdgyd&st=hybi5oad&dl=0}{this link}.
    }
    \label{fig:exp}
\end{figure}

\paragraph{Results.}
We first observe that, in all three cases, the variance of the data distribution along the second dimension shrinks gradually to zero
at a roughly similar rate as $t \to 0$, consistent with the theoretical argument in Section~\ref{sec:d>1} that score smoothing does not interfere with the convergence in the normal direction. Meanwhile, in contrast with Column $\textbf{(\romannumeral 1)}$, where the variance along the first dimension shrinks to zero as well, we see in Column $\textbf{(\romannumeral 2)}$ that the variance along the first dimension remains positive for all $t$, validating the interpolation effect caused by smoothing the ESF. 
Moreover, 
the density histograms in Column $\textbf{(\romannumeral 2)}$ are closely matched by our analytical predictions of $\hat{p}^{(n, t_0)}_t$ and $\tilde{p}^{(n, t_0)}_0$ (the colored curves). Finally, we observe that Column $\textbf{(\romannumeral 3)}$ is much closer to $\textbf{(\romannumeral 2)}$ than $\textbf{(\romannumeral 1)}$ in terms of how the distribution (as well as the SF itself, as shown in Figures~\ref{fig:2d_score_dim1} - \ref{fig:2d_score_slice}) evolves during denoising. This suggests that NN learning causes a similar smoothing effect on the SF and adds evidence that our analysis based on score smoothing is likely relevant for understanding how NN-based DMs avoid memorization.

In Figure~\ref{fig:log_density_2}, we also show that $\textbf{(\romannumeral 2)}$ is nearly equivalent to the locally-averaged ESF ($\vs_t = (1-\delta_t) * \nabla \log p^{(n)}_t$) for driving the denoising dynamics, which provides additional empirical justification for adopting the PL approximation at small $t$.






\section{Related works}
\label{sec:related}
\paragraph{Generalization in DMs.}
Several works have noted the transition from generalization to memorization behaviors in DMs when the model capacity increases relatively to the training set size \citep{gu2023memorization, yi2023generalization, carlini2023extracting, kadkhodaie2024generalization, li2024understanding}. 
Using tools from statistical physics,  \citet{biroli2024dynamical} showed that the transition to memorization occurs in the crucial regime where $t$ is small relative to the training set sparsity, which is also the focus of our study.

To derive rigorous learning guarantees, one line of work showed that DMs can produce a distribution accurately given a good score estimator \citep{song2021maximum, lee2022convergence, vdb2022convergence, chen2023improved, chen2023sampling, shah2023learning, cole2024scorebased, benton2024nearly, huang2024convergence}, which leaves open the question of how to estimate the SF of an underlying density from finite training data without overfitting.
For score estimation, when the ground truth density or its SF belongs to certain function classes,
prior works have constructed score estimators with guaranteed sample complexity \citep{block2020generative, li2023on, zhang2024minimax, wibisono2024optimal, chen2024learning, gatmiry2024learning, boffi2024shallow},
including for scenarios where the data are supported on low-dimensional sub-manifolds (further discussed below).
Unlike these approaches, which concern the estimation of densities from i.i.d. samples, our analysis does not assume a ground truth distribution. Based on a finite and fixed training set, 
our work focuses on the geometry of the SF when $t$ is small relative to the training set sparsity
and elucidates how it determines the memorization behavior via an interplay with the denoising dynamics. For future work, it will be interesting to study the implication of score smoothing in the density estimation setting by potentially adapting our analysis to cases with randomly-sampled training data.


\paragraph{DMs and the manifold hypothesis.} An influential hypothesis is that high-dimensional real-world data often lie in low-dimensional sub-manifolds \citep{tenenbaum2000global, peyre2009manifold}, and it has been argued that DMs can estimate their intrinsic dimensions \citep{stanczuk2024diffusion, kamkari2024a}, learn manifold features in meaningful orders \citep{wang2023diffusion, wang2024unreasonable, achilli2024losing}, or perform subspace clustering implicitly \citep{wang2024subspace}.
Under the manifold hypothesis, \citet{pidstrigach2022scorebased, vdb2022convergence, potaptchik2024linear, huang2024denoising} studied the convergence of DMs assuming a sufficiently good approximation to the true SF, while \citet{oko2023diffusion, chen2023score, azangulov2024convergence} proved sample complexity guarantees for score estimation using NN models.
In particular, prior works such as \citet{chen2023score, wang2024unreasonable, gao2024flow, ventura2024manifolds} have considered the decomposition of the SF into tangent and normal components. Our work is novel in showing how score smoothing can affect these two components differently: reducing the speed of convergence towards training data along the \emph{tangent} direction (to avoid memorization) while preserving it along the \emph{normal} direction (to ensure a convergence onto the subspace). 

\paragraph{Score smoothing and regularization.} \citet{aithal2024understanding} showed empirically that NNs tend to learn smoother versions of the ESF and argued that this leads to a mode interpolation effect that explains model hallucination.
\citet{scarvelis2023closed} designed alternative closed-form DMs by smoothing the ESF, although the theoretical analysis therein is limited to showing that their smoothed SF is directed towards certain barycenters of the training data. Their work inspired our further theoretical analysis on how score smoothing affects the denoising dynamics and leads to a terminal distribution that interpolates the training data. In the context of image generation, \citet{kamb2024analytic} showed that imposing locality and equivariance to the score estimator allows the model to generalize better. In comparison, our work shows that the benefit of score regularization can manifest in more general settings via function smoothing.

Recent works including \citet{wibisono2024optimal, baptista2025memorization} considered other SF regularizers such as the empirical Bayes regularization (capping the magnitude in regions where $p^{(n)}_t$ is small) or Tikhonov regularization (constraining the norm averaged over $p^{(n)}_t$). In the linear subspace setting, these methods tend to reduce the magnitude of the SF in not only the tangent but also the normal directions, thus slowing down the convergence onto the subspace and resulting in a terminal distribution that still has a $d$-dimensional support. In contrast, as local averaging preserves the (linear) normal component, our smoothed score is less prone to this issue.

\section{Conclusions and Limitations}
Through theoretical analyses and numerical experiments, our work shows how score smoothing can enable the denoising dynamics to produce distributions on the training data subspace without fully memorizing the training set. Further, by showing connections between score smoothing and learning a score estimator with regularized NNs, our results shed light on an arguably core mechanism behind the ability of NN-based diffusion models to generalize and hallucinate. Additionally, viewing NN learning as just \emph{one} way to achieve score smoothing, our work also motivates the exploration of alternative score estimators that facilitate generalization in DMs.

The present work focuses on a vastly simplified setup compared to real-world scenarios, and it would be valuable next steps to extend our theory to cases where training data are generally spaced, random or belonging to complex manifolds as well as to more general variants of DMs \citep{vdb2021diffusion, albergo2023stochastic, lipman2023flow, liu2023flow}. The connections between score smoothing and the implicit bias of NN training have also only been explored to a limited extent, especially in the higher-dimensional setting. Lastly, it will be useful to consider alternative forms of function smoothing as well as other ways regularization mechanisms beyond smoothing and better understand their interplay with the denoising dynamics.

\paragraph{Acknowledgment.} The author thanks Zhengjiang Lin, Pengning Chao, Pranjal Awasthi, Arnaud Doucet, Eric Vanden-Eijnden and Binxu Wang for valuable conversations and suggestions.
\bibliography{ref}
\bibliographystyle{apalike}

\clearpage
\appendix
\section{Additional Notations}
\label{app:notations}
We use big-O notations \emph{only} for denoting asymptotic relations as $t \to 0$. Specifically, for functions $f, g: \sR_+ \to \sR_+$, we will write $f(t) = O(g(t))$ if $\exists t_1, C > 0$ (they may depend on other variables such as $\kappa$ and $\Delta$) such that $\forall t \in (0, t_1)$, it holds that $f(t) \leq C g(t)$. In addition, in several situations where $f$ decays exponentially fast in $1/t$ as $t \to 0$ but the exact exponent is not of much importance, we will simply write $f(t) = O(\exp(-C/t))$, which is intended to be interpreted as $\exists C > 0$ such that $f(t) =  O(\exp(-C/t))$ (and the value of $C$ can differ in different contexts).

\section{Connections Among the Different SF Variants}
\label{app:sf_variants}
\begin{figure}[h]
    \centering
    \begin{tikzcd}[column sep=huge, row sep=large]
 \nabla \log p^{(n)}_t \arrow[d,Rightarrow, "~\text{PL approximation}~"'] \arrow{r}{\text{LA}} \arrow[rd,leftrightsquigarrow, "\hspace{-2pt}\text{Proposition}~\ref{lem:3}"]
& (1 - \delta) * \nabla \log p^{(n)}_t \arrow[d,Rightarrow, "~\text{PL approximation (Lemma}~\ref{lem:2}\text{)}"] & \\
\hspace{13pt} \bar{s}^{(n)}_t \hspace{13pt} \arrow{r}{\text{LA}} 
& 
\hspace{32pt}
\hat{s}^{(n)}_{t, \delta} \hspace{32 pt} \arrow[r, leftrightsquigarrow, "\text{Figure}~\ref{fig:nn_score_1d}"] & s^{\text{NN}}_t
\end{tikzcd}
    \caption{Different variants of the SF and their connections. 
    }
    \label{fig:tikzcd}
\end{figure}

\section{Generalization to $n>2$}
\label{app:n>2}
The analysis above can be generalized to the scenario where $S$ consists of $n > 2$ points spaced uniformly on an interval $[-D, D]$,
that is, $y_k \coloneqq 2(k-1) \Delta - D$ for $k \in [n]$, where $\Delta \coloneqq D/(n-1) = (y_{k+1} - y_k)/2$. We additionally define $z_k \coloneqq y_k + \Delta = (y_k + y_{k+1}) / 2$ for $k \in [n-1]$.
\subsection{Score Smoothing}
In this case, we can still express the ESF as (\ref{eq:esf}) except for
replacing (\ref{eq:hat_x}) by
\begin{equation}
\label{eq:hat_x_n}
    \hat{x}^{(n)}_t(x) \coloneqq \frac{\sum_{k=1}^n y_k p_{\mathcal{N}}(x-y_k, \sqrt{t}) }{\sum_{k=1}^n p_{\mathcal{N}}(x-y_k, \sqrt{t})}~,
\end{equation}
and its PL approximation at small $t$ is now given by
\begin{equation}
    \bar{s}^{(n)}_t(x) \coloneqq \begin{cases}
        (y_1 - x)/t~,& \quad \text{ if } x \leq z_1~, \\
        (y_k - x)/t~,& \quad \text{ if } x \in [z_{k-1}, z_k] \text{ for } k \in \{2, ..., n - 1\}~, \\
        (y_n - x)/t~,& \quad \text{ if } x \geq z_{n-1}~.
    \end{cases}
\end{equation}
Moreover, the locally-averaged version of $\bar{s}^{(n)}_t$ can still be written via (\ref{eq:tau_sig}) for $\delta \in (0, \Delta)$, where 
we now define
\begin{equation}
\label{eq:hat_s_1d}
    \hat{s}^{(n)}_{t, \delta}(x) \coloneqq \begin{cases}
        (y_1 - x)/t~,& ~ \text{ if } x \leq y_1 + \delta~, \\
        (y_n - x)/t~,& ~ \text{ if } x \geq y_{n-1} - \delta~, \\
        (y_k - x)/t~,& ~ \text{ if } x \in [y_{k} - \delta, y_k + \delta], \exists k \in [n]~, \\
        \delta / (\Delta - \delta) \cdot (x - z_k) / t~,& ~ \text{ if } x \in [y_{k-1} + \delta, y_k - \delta], \exists k \in [n-1]~,
    \end{cases}
\end{equation}
and it is not hard to show that Lemma~\ref{lem:2} and Proposition~\ref{lem:3} can be generalized to the following results, with their proofs given in Appendix~\ref{sec:pf_lem_2} and \ref{sec:pf_prop2}, respectively:
\begin{lemma}
\label{lem:2_n>2}
For any fixed $\delta \in (0, \Delta]$, $\exists t_1, C > 0$ such that $\forall t \in (0, t_1)$, it holds that 
\begin{equation}
    t \cdot \sE_{x \sim p^{(n)}_t} \Big [ \big \| \hat{s}^{(n)}_{t, \delta}(x) - ((\Delta-\delta) * \tfrac{d}{dx} \log p^{(n)}_t )(x)  \big \|^2 \Big ] \leq C \exp (- \delta^2 / (9 t))~.
\end{equation}
\end{lemma}
\begin{proposition}
\label{lem:3_n>2}
Let $\delta_t = \kappa \sqrt{t}$ for some $\kappa > 0$.
Then $\exists t_1, C > 0$ such that
$\forall t \in (0, t_1)$,
\begin{equation}
     \tfrac{n-1}{n} F(\kappa) - C \sqrt{t} \leq L^{(n)}_t[\hat{s}^{(n)}_{t, \delta_t}] \leq \tfrac{n-1}{n} F(\kappa) + C \sqrt{t}~,
\end{equation}
where $t_1$ and $C$ depend only on $\kappa$ and $F$ is a function that strictly decreases from $1$ to $0$ on $[0, \infty)$.
\end{proposition}

\subsection{Denoising Dynamics}
\label{app:denoise_n>2}
The backward-in-time dynamics of (\ref{eq:back_sm}) can also be solved analytically in a similar fashion, where (\ref{eq:phi_st}) is replaced by
\begin{equation}
\label{eq:phi_st_n}
    \phi_{s|t}(x) \coloneqq 
    \begin{cases}
    \sqrt{\frac{s}{t}}(x - y_1) + y_1~,& \text{ if } x \leq y_1 + \delta_t~, \\
    \sqrt{\frac{s}{t}}(x - y_n) + y_n~,& \text{ if } x \geq y_n - \delta_t~, \\
          \sqrt{\frac{s}{t}}(x - y_k) + y_k~,& \text{ if } x \in [y_k - \delta_t, y_k + \delta_t], \exists k \in \{2, ..., n-1\}~, \\
         \frac{\Delta - \delta_s}{\Delta - \delta_t}(x - z_k) + z_k~,& \text{ if } x \in [y_k + \delta_t, y_{k+1} - \delta_t], \exists k \in [n-1]~.
    \end{cases}
\end{equation}
The formula (\ref{eq:cov}) is then generalized to
\begin{equation}
\label{eq:cov_n>2}
    \hat{p}^{(n, t_0)}_s(x) = \begin{cases}
    \delta_t / \delta_s \cdot \hat{p}^{(n, t_0)}_t(\delta_t / \delta_s \cdot x - (\delta_t - \delta_s) / \delta_s \cdot y_1)~,& \hspace{-150pt} \text{ if } x \leq y_1 + \delta_s \\
    \delta_t / \delta_s \cdot \hat{p}^{(n, t_0)}_t(\delta_t / \delta_s \cdot x - (\delta_t - \delta_s) / \delta_s \cdot y_n)~,& \hspace{-150pt} \text{ if } x \geq y_n - \delta_s \\
    \delta_t / \delta_s \cdot \hat{p}^{(n, t_0)}_t(\delta_t / \delta_s \cdot x - (\delta_t - \delta_s) / \delta_s \cdot y_k)~,& \hspace{-150pt} \text{ if } x \in [y_k - \delta_s, y_k + \delta_s] \\
    (\Delta - \delta_t) / (\Delta - \delta_s) \cdot \hat{p}^{(n, t_0)}_t((\Delta - \delta_t) / (\Delta - \delta_s) \cdot x + (\delta_t - \delta_s) / (\Delta - \delta_s) \cdot z_k)~,\\
    & \hspace{-150pt} \text{ if } x \in [y_k + \delta_s, y_{k+1} - \delta_s] 
    \end{cases} 
\end{equation} 
When $s = 0$, there is
\begin{equation}
\label{eq:phi0t_n}
    \phi_{0|t}(x) =
    \begin{cases}
        (\Delta x - z_k \delta_t) / (\Delta - \delta_t)~,& \text{ if } x \in [y_{k} + \delta_t, y_{k+1} - \delta_t], \exists k \in [n-1]~,\\
        y_{\arg \min_k |y_{k} - x|}~,& \text{ otherwise. }
    \end{cases}
\end{equation}
As $\phi_{0|t}(x)$ is invertible when restricted to $\cup_{k \in [n-1]} [y_k + \delta_t, y_{k+1} - \delta_t]$, 
the terminal distribution can be decomposed as
\begin{equation}
\label{eq:hat_p_0_n>2}
    \hat{p}^{(n, t_0)}_{0} = \sum_{k=1}^n a_k \boldsymbol{\delta}_{y_k} + \Big (1 - \sum_{k=1}^n a_k \boldsymbol{\delta}_{y_k} \Big ) \tilde{p}^{(n, t_0)}_{0}~,
\end{equation}
where $\tilde{p}^{(n, t_0)}_0$ is a probability distribution defined as
\begin{equation}
\label{eq:tilde_pt_n>2}
    \tilde{p}^{(n, t_0)}_0(x) = \begin{cases}
        (\Delta - \delta_t) / \Delta \cdot \hat{p}^{(n, t_0)}_t((\Delta - \delta_t) / \Delta \cdot x + \delta_t / \Delta \cdot z_k)~,
    &\quad  \text{ if } x \in [y_k, y_{k+1}]  \\
        0 ~,& \quad \text{ otherwise}~.
    \end{cases} 
\end{equation}
and it holds for \emph{all} $t \in (0, t_0]$ that
\begin{equation}
    a_k = \begin{cases}
        \sE_{\rvx \sim \hat{p}^{(n, t_0)}_t} \big [ \mathds{1}_{\rvx \geq -D + \delta_t} \big ]~,& \quad \text{if } k = 1~,\\
        \sE_{\rvx \sim \hat{p}^{(n, t_0)}_t} \big [ \mathds{1}_{\rvx \geq D - \delta_t} \big ]~,& \quad \text{if } k = n~,\\
        \sE_{\rvx \sim \hat{p}^{(n, t_0)}_t} \big [ \mathds{1}_{y_k - \delta_t \leq \rvx \leq y_k + \delta_t} \big ]~,& \quad \text{if } k \in \{2, ..., n-1\}~.
    \end{cases}
\end{equation}

\subsection{Higher Dimensions}
Thanks to the decoupling across dimensions, under the definition of local averaging over centered cubes (\ref{eq:sm_cube}), the LA-PL SF takes the same form as (\ref{eq:hat_s_d>1}) and the denoising dynamics associated with it also follows (\ref{eq:sm_back_d=1}) and (\ref{eq:sm_back_d>1}). Hence, Proposition~\ref{prop:flow_d>1} still holds except with (\ref{eq:phi_st}) - (\ref{eq:hat_p_0}) replaced by (\ref{eq:phi_st_n}), (\ref{eq:phi0t_n}), (\ref{eq:cov_n>2}), and (\ref{eq:tilde_pt_n>2}), respectively.

\section{Proof of Lemma~\ref{lem:2}}
\label{sec:pf_lem_2}
Below we prove Lemma~\ref{lem:2_n>2}, which generalizes Lemma~\ref{lem:2} to the case where $n > 2$.

We write $w = \Delta - \delta$. By the definition of $p^{(n)}_t$, it suffices to show that $\forall k \in [n]$,
\begin{equation}
\label{eq:39}
    \int | \hatsnsigtau(x) - \hatbarsnsigtau(x) |^2 p_{\mathcal{N}}(x-y_k; \sqrt{t}) dx = O(\exp(- \delta^2 / (9 t)))~.
\end{equation}
Consider any $k \in [n]$. The left-hand-side above can be rewritten as
\begin{equation}
    \begin{split}
        &~ \int_{-\infty}^\infty \left | \frac{1}{2w} \int_{x-w}^{x+w} \snsig(x') dx' - \frac{1}{2w} \int_{x-w}^{x+w} \barsnsig(x') dx' \right |^2 p_{\mathcal{N}}(x-y_k; \sqrt{t}) dx \\
        = &~ \int_{-\infty}^\infty \left |\frac{1}{2w} \int_{x-w}^{x+w} \left ( \snsig(x') -  \barsnsig(x') \right ) dx' \right |^2 p_{\mathcal{N}}(x-y_k; \sqrt{t}) dx \\
        \leq &~ \int_{-\infty}^\infty \frac{1}{2w} \int_{x-w}^{x+w} \left |  \snsig(x') -  \barsnsig(x') \right |^2 dx' p_{\mathcal{N}}(x-y_k; \sqrt{t}) dx \\
        =&~ \int_{-\infty}^{\infty} \left |  \snsig(x') -  \barsnsig(x') \right |^2 \left ( \frac{1}{2w}\int_{x'-w}^{x'+w} p_{\mathcal{N}}(x-y_k; \sqrt{t}) dx \right ) dx'
    \end{split}
\end{equation}
We decompose the outer integral into three intervals and bound them separately. First, when $x \geq y_k + \frac{1}{2}(\Delta + w) > y_k + w$, there is $\sup_{x \in [x'-w, x'+w]} p_{\mathcal{N}}(x-y_k; \sqrt{t}) \leq p_{\mathcal{N}}(x'-y_k-w; \sqrt{t})$. Hence, also noticing that
$|\snsig(x)|, |\barsnsig(x)| \leq (|x| + 2D) / t$, we obtain that
\begin{equation}
\label{eq:app_bd_1}
    \begin{split}
        &~\int_{y_k + \frac{1}{2}(\Delta + w)}^{\infty} \left |  \snsig(x') -  \barsnsig(x') \right |^2 \left ( \frac{1}{2w}\int_{x'-w}^{x'+w} p_{\mathcal{N}}(x-y_k; \sqrt{t}) dx \right ) dx' \\
        \leq&~ \gaufactor \int_{y_k + \frac{1}{2}(\Delta + w)}^{\infty} \left |  \snsig(x') -  \barsnsig(x') \right |^2 \expflat{-\frac{(x'-y_k-w)^2}{2 t}} dx' \\
        \leq&~ \gaufactor \int_{y_k + \frac{1}{2}(\Delta + w)}^{\infty} \frac{4(|x'| + 2 D)^2}{t^2} \expflat{-\frac{(x'-y_k-w)^2}{2 t}} dx' \\
        \leq&~ \gaufactor \int_{y_k + \frac{1}{2}(\Delta + w)}^{\infty} \frac{4(|x'-y_k-w| + 4 D)^2}{t^2} \expflat{-\frac{(x'-y_k-w)^2}{2 t}} dx' \\
        \leq&~ \frac{8}{\sqrt{2 \pi t^3}} \int_{y_k + \frac{1}{2}(\Delta + w)}^{\infty} \left ( \left ( \frac{x' - y_k - w}{\sqrt{t}} \right )^2 + \frac{16 D^2}{t} \right ) \expflat{-\frac{(x'-y_k-w)^2}{2 t}} dx' \\
        =&~ \frac{8}{\sqrt{2 \pi t^3}} \int_{\frac{\delta}{2 \sqrt{t}}}^{\infty} \left ( (\tilde{x})^2 + \frac{16 D^2}{t} \right ) \expflat{-\frac{(\tilde{x})^2}{2}} d\tilde{x} \\
        \leq&~ \frac{8}{\sqrt{2 \pi t^3}} \left ( \bigg ( \frac{16 D^2}{t} + 1 \bigg ) \sqrt{\frac{\pi}{2}} + \frac{\delta}{2 \sqrt{t}} \right ) \expflat{- \frac{\delta^2}{8 t}} 
        = O \left (\expflat{- \frac{\delta^2}{9 t}} \right )~,
    \end{split}
\end{equation}
where for the last inequality we use Lemma~\ref{lem:exp_ints} below.

A similar bound can be derived when the range of the outer integral is changed to between $-\infty$ and $y_k - \frac{1}{2}(\Delta + w)$. 

Next, suppose $x \in [y_k - \frac{1}{2}(\Delta + w), y_k + \frac{1}{2}(\Delta + w)]$, which means that $|x - y_k| \leq \frac{1}{2}(\Delta + w)$ while $|x - y_l| \geq \frac{3}{2} \Delta - \frac{1}{2} w$ for $l \neq k$. Thus, it holds for any $l \neq k$ that
\begin{equation}
    \begin{split}
        \frac{p_{\mathcal{N}}(x - y_k; \sqrt{t})}{p_{\mathcal{N}}(x - x_l; \sqrt{t})} =&~ \exp \left (- \frac{|x - y_k|^2 - |x - x_l^2|}{2 t} \right )
        \geq \exp \left ( \frac{\Delta \delta}{t} \right ) 
    \end{split}
\end{equation}
Hence, writing $q_{t, k}(x) \coloneqq \frac{p_{\mathcal{N}}(x - y_k; \sqrt{t})}{\sum_{l=1}^n p_{\mathcal{N}}(x - x_l; \sqrt{t})}$, there is
$q_{t, k}(x) \geq 1 - (n-1) \exp \left ( -\frac{\Delta \delta}{t} \right )$ and for $l \neq k$, $q_{t, l}(x) < \exp \left ( -\frac{\Delta \delta}{t} \right )$. Therefore, 
\begin{equation}
\label{eq:sdiff_mid}
\begin{split}
    \left | \tfrac{d}{dx} \log p^{(n)}_t(x) - \bar{s}^{(n)}_{t}(x) \right | \leq &~ \frac{|(q_{t, k}(x) - 1)y_k| + \sum_{l \neq k} | q_{t, k}(x) y_k|}{t} \\
    \leq &~ \frac{2(n-1)D}{t} \exp \left ( -\frac{\Delta \delta}{t} \right ) = O \left (t^{-1} \exp \left ( -\frac{\Delta \delta}{t} \right ) \right )~.
\end{split}
\end{equation}
Since $\frac{1}{2w}\int_{x'-w}^{x'+w} p_{\mathcal{N}}(x-y_k; \sqrt{t}) dx \leq \frac{1}{\sqrt{2\pi t}}$ for any $x'$, we then have
\begin{equation}
\label{eq:app_bd_2}
    \begin{split}
        &~ \int_{y_k - \frac{1}{2}(\Delta + w)}^{y_k + \frac{1}{2}(\Delta + w)} \left |  \snsig(x') -  \barsnsig(x') \right |^2 \left ( \frac{1}{2w}\int_{x'-w}^{x'+w} p_{\mathcal{N}}(x-y_k; \sqrt{t}) dx \right ) dx' \\
        \leq&~ \frac{\Delta + w}{\sqrt{2\pi t}} \sup_{y_k - \frac{1}{2}(\Delta + w) \leq x' \leq y_k + \frac{1}{2}(\Delta + w)}  \left |  \snsig(x') -  \barsnsig(x') \right |^2 \\
        =&~ O \left (t^{-3} \exp \left ( -\frac{2 \Delta \delta}{t} \right ) \right )
    \end{split}
\end{equation}
Combining (\ref{eq:app_bd_1}) with (\ref{eq:app_bd_2}) yields the desired result.

\hfill $\square$

\begin{lemma} 
\label{lem:exp_ints}
For $u \geq 0$,
\begin{align}
    \int_u^\infty e^{-x^2/2} dx \leq &~ \sqrt{\frac{\pi}{2}} e^{-u^2/2} \label{eq:exp_int_1} \\
    \int_u^\infty x^2 e^{-x^2/2} dx \leq &~ \left ( \sqrt{\frac{\pi}{2}} + u \right ) e^{-u^2/2} \label{eq:exp_int_2}
\end{align}   
\end{lemma}
\noindent \textit{Proof of Lemma~\ref{lem:exp_ints}}:
It known (e.g., \citealt{chang2011errorfunc}) that
\begin{equation}
        \int_u^\infty e^{-x^2} dx \leq \frac{\sqrt{\pi}}{2} e^{-u^2}~,
    \end{equation}
from which (\ref{eq:exp_int_1}) can be obtained by a simple change-of-variable.

Next, using integration-by-parts, we obtain that
\begin{equation}
    \begin{split}
         \int_a^b x^2 e^{-x^2/2} dx =&~ x (-e^{-x^2/2}) \big |_a^b - \int_a^b 1 \cdot (-e^{-\frac{x^2}{2}}) dx \\
         =&~ (ae^{-a^2/2} - b e^{-b^2/2}) + \int_a^b e^{-x^2/2} dx
    \end{split}
\end{equation}
Hence,
\begin{equation}
    \begin{split}
        \int_u^\infty x^2 e^{-x^2/2} dx \leq&~ \int_u^\infty e^{-x^2/2} dx + u e^{-u^2/2} \\
        \leq&~ \left ( \sqrt{\frac{\pi}{2}} + u \right ) e^{-u^2/2}
    \end{split}
\end{equation}
\hfill $\square$

\section{Proof of Proposition~\ref{lem:3}}
\label{sec:pf_prop2}
Below we prove Proposition~\ref{lem:3_n>2}, which generalizes Proposition~\ref{lem:3} to the case where $n > 2$.

In light of Lemma~\ref{lem:2}, we only need to show that
\begin{equation}
    t \int_{-\infty}^\infty |\hat{s}^{(n)}_{t, \delta_t}(x) - \barsnsig(x)|^2 \pnsig(x) dx = \frac{n-1}{n} F(\kappa) + O (\sqrt{t} )~.
\end{equation}
By the definition of $\pnsig$, we can first evaluate the integral with respect to the density $p_{\mathcal{N}}(x - y_k; \sqrt{t})$ for each $k \in [n]$ separately and then sum them up. We define
\begin{equation}
    y_{k, -} = \begin{cases}
        -\infty~,&~  \text{ if } k = 1 \\
        y_k - \delta_t~,& ~ \text{ otherwise}~
    \end{cases},
    \qquad 
    y_{k, +} = \begin{cases}
        \infty~,& ~ \text{ if } k = n \\
        y_k + \delta_t~,& ~ \text{ otherwise}~
    \end{cases}
\end{equation}
By construction, $\hat{s}^{(n)}_{t, \delta_t}$ is a PL function whose slope is changed only at each $y_{k, -}$ and $y_{k, +}$.

Let us fix a $k \in [n]$. Since $\hat{s}^{(n)}_{t, \delta_t}(x) = \barsnsig(x)$ when $x \in [y_{k, -}, y_{k, +}]$,
we only need to estimate the difference between the two outside of $[y_{k, -}, y_{k, +}]$. 

We first consider the interval $(y_{k, +}, y_k + \Delta] = (y_k + \delta_t, y_k + \Delta]$ when $k \in \{1, ..., n-1\}$, on which it holds that
\begin{equation}
    \hat{s}^{(n)}_{t, \delta_t}(x) - \barsnsig(x) = \frac{\Delta}{t} \cdot \frac{x - (y_k + \delta_t)}{\Delta - \delta_t}~,
\end{equation}
by the piecewise-linearity of the two functions. Hence,
\begin{equation}
\begin{split}
    &~ t \int_{y_k + \delta_t}^{y_k + \Delta} |\hat{s}^{(n)}_{t, \delta_t}(x) - \barsnsig(x)|^2 p_{\mathcal{N}}(x - y_k; \sqrt{t}) dx \\
    =&~ \left ( \frac{\Delta}{\Delta - \delta_t} \right )^2 \int_{y_k + \delta_t}^{y_k + \Delta} \left | \frac{x - y_k}{\sqrt{t}} - \frac{\delta_t}{\sqrt{t}} \right |^2 p_{\mathcal{N}}(x - y_k; \sqrt{t}) dx \\
    =&~ \left ( \frac{\Delta}{\Delta - \delta_t} \right )^2 \bigg ( \int_{y_k + \delta_t}^{\infty} \left | \frac{x - y_k}{\sqrt{t}} - \kappa \right |^2 p_{\mathcal{N}}(x - y_k; \sqrt{t}) dx \\
    & \hspace{50pt} - \int_{y_k + \Delta}^\infty \left | \frac{x - y_k}{\sqrt{t}} - \kappa \right |^2 p_{\mathcal{N}}(x - y_k; \sqrt{t}) dx \bigg )
\end{split}
\end{equation}
Note that by a change-of-variable $\tilde{x} \leftarrow (x - y_k) / \sqrt{t}$, we obtain that
\begin{equation}
    \begin{split}
        \int_{y_k + \delta_t}^{\infty} \left | \frac{x - y_k}{\sqrt{t}} - \kappa \right |^2 p_{\mathcal{N}}(x - y_k; \sqrt{t}) dx = \frac{1}{2} F(\kappa)~,
    \end{split}
\end{equation}
where we define 
\begin{equation}
\label{eq:F}
    F(\kappa) \coloneqq 2 \int_{\kappa}^\infty |u - \kappa|^2 p_{\mathcal{N}}(u; 1) du
\end{equation}
It is straightforward to see that, as $\kappa$ increases from $0$ to $\infty$, $F$ strictly decreases from $1$ to $0$. Therefore,
\begin{equation}
    \begin{split}
         &~ t \int_{y_k + \delta_t}^{y_k + \Delta} |\hat{s}^{(n)}_{t, \delta_t}(x) - \barsnsig(x)|^2 p_{\mathcal{N}}(x - y_k; \sqrt{t}) dx \\
         =&~ \left ( \frac{\Delta}{\Delta - \delta_t} \right )^2 \left ( \frac{1}{2} F(\kappa) - \int_{\Delta / \sqrt{t}}^\infty \left | u - \kappa \right |^2 p_{\mathcal{N}}(u; 1) dx \right ) \\
         =&~ \frac{1}{2} F(\kappa) + O(\sqrt{t})
    \end{split}
\end{equation}
Next, we consider the interval $[y_k + \Delta, \infty)$, in which we have 
\begin{equation}
    |\hat{s}^{(n)}_{t, \delta_t}(x) - \barsnsig(x)| \leq \frac{\Delta}{t}~.
\end{equation}
Thus,
\begin{equation}
    \begin{split}
         t \int_{y_k + \Delta}^\infty |\hat{s}^{(n)}_{t, \delta_t}(x) - \barsnsig(x)|^2 p_{\mathcal{N}}(x - y_k; \sqrt{t}) dx 
         \leq&~ t \int_{y_k + \Delta}^\infty \left | \frac{\Delta}{t} \right |^2 p_{\mathcal{N}}(x - y_k; \sqrt{t}) dx \\
         =&~ \frac{\Delta^2}{t} \int_{\Delta / \sqrt{t}}^\infty p_{\mathcal{N}}(u; 1) du \\
         =&~ O \left (t^{-1} \exp \bigg (-\frac{\Delta^2}{2t} \bigg ) \right )
    \end{split}
\end{equation}
Hence, we have
\begin{equation}
    t \int_{y_k + \delta_t}^\infty |\hat{s}^{(n)}_{t, \delta_t}(x) - \barsnsig(x)|^2 p_{\mathcal{N}}(x - y_k; \sqrt{t}) dx = \frac{1}{2} F(\kappa) + O(\sqrt{t})~.
\end{equation}
Similarly, for $k \in \{2, ..., n\}$, we can show that
\begin{equation}
     t \int_{-\infty}^{y_k - \delta_t} |\hat{s}^{(n)}_{t, \delta_t}(x) - \barsnsig(x)|^2 p_{\mathcal{N}}(x - y_k; \sqrt{t}) dx = \frac{1}{2} F(\kappa) + O(\sqrt{t})~.
\end{equation}
Thus, there is
\begin{equation}
\begin{split}
    &t \int_{-\infty}^{\infty} |\hat{s}^{(n)}_{t, \delta_t}(x) - \barsnsig(x)|^2 p_{\mathcal{N}}(x - y_k; \sqrt{t}) dx \\
    =&~ \begin{cases}
        F(\kappa) + O(\sqrt{t})~,& \quad \text{ if } k \in \{2, ..., n-1\} \\
        \frac{1}{2} F(\kappa) + O(\sqrt{t})~,& \quad \text{ if } k = 1 \text{ or } n~.
    \end{cases}
\end{split}
\end{equation}
Summing them together, we get that
\begin{equation}
     t \int_{-\infty}^{\infty} |\hat{s}^{(n)}_{t, \delta_t}(x) - \barsnsig(x)|^2 \pnsig(x) dx = \frac{n-1}{n} F(\kappa) + O(\sqrt{t})~.
\end{equation}
This proves the proposition.

\hfill $\square$

\section{Proof of Proposition~\ref{prop:reg_min}}
\label{app:pf_prop_reg_min}

The first claim is a straightforward consequence of Proposition~\ref{lem:3_n>2}: when $t$ is small enough (with threshold dependent on $\kappa$), there is
\begin{equation}
    L^{(n)}_t[\hat{s}^{(n)}_{t, \delta_t}] \leq \frac{n-1}{n} F(\kappa) + C \sqrt{t} < F(\kappa) \leq F(F^{-1}(\epsilon)) = \epsilon~.
\end{equation}
Next we consider the second claim. On one hand, it is easy to compute that
\begin{equation}
    \begin{split}
        R[\hat{s}^{(n)}_{t, \delta_t}] =&~ \sum_{k=1}^{n-1} 2 \left (\frac{\delta_t}{t (\Delta - \delta_t)} + \frac{1}{t} \right ) = 2 (n-1) \frac{\Delta}{t (\Delta - \delta_t)}
    \end{split}
\end{equation}
On the other hand, let $f$ be any function on $\sR$ that belongs to the feasible set of the minimization problem (\ref{eq:reg_min}), meaning that $f$
is twice differentiable except on a set of measure zero and $L^{(n)}_t[f] < \epsilon$. 
Define $\epsilon_k \coloneqq t \int_{-\infty}^\infty |f(x) - \tfrac{d}{dx} \log p^{(n)}_t(x)|^2 p_{\mathcal{N}}(x - y_k; \sqrt{t}) dx$ for each $k \in [n]$. By the definition of $p^{(n)}_t$, we then have $\sum_{k=1}^n \epsilon_k < n \epsilon$. If we consider a change-of-variable $\tilde{x} = (x - y_k) / \sqrt{t}$ and define $\tilde{f}_k(\tilde{x}) \coloneqq \sqrt{t} f(y_k + \sqrt{t} \tilde{x})$, there is 
\begin{equation}
    \begin{split}
        \int_{y_k - \Delta}^{y_k + \Delta} |f(x) - \bar{s}^{(n)}_t(x)|^2 p_{\mathcal{N}}(x - y_k; \sqrt{t}) dx =&~ \int_{y_k - \Delta}^{y_k + \Delta} \Big |f(x) - \frac{y_k - x}{t} \Big |^2 p_{\mathcal{N}}(x - y_k; \sqrt{t}) dx \\
        =&~ t^{-1} \int_{-\Delta / \sqrt{t}}^{\Delta / \sqrt{t}} |\tilde{f}_k(\tilde{x}) + \tilde{x}|^2 p_{\mathcal{N}}(\tilde{x}; 1) d\tilde{x}~.
    \end{split}
\end{equation}
Hence, using (\ref{eq:39}) with $\delta = \Delta$, we obtain that
\begin{equation}
\begin{split}
    \int_{-\Delta / \sqrt{t}}^{\Delta / \sqrt{t}} |\tilde{f}_k(\tilde{x}) + \tilde{x}|^2 p_{\mathcal{N}}(\tilde{x}; 1) d\tilde{x} 
    \leq &~ t \int_{-\infty}^{\infty} |f(x) - \bar{s}^{(n)}_t(x)|^2 p_{\mathcal{N}}(x - y_k; \sqrt{t}) dx \\
    \leq &~ t \int_{-\infty}^{\infty} |f(x) - \tfrac{d}{dx} \log p^{(n)}_t(x)|^2 p_{\mathcal{N}}(x - y_k; \sqrt{t}) dx \\
    &~ + t \int_{-\infty}^{\infty} \big |\tfrac{d}{dx} \log p^{(n)}_t(x) - \bar{s}^{(n)}_t(x) \big |^2 p_{\mathcal{N}}(x - y_k; \sqrt{t}) dx \\
    \leq&~ \epsilon_k + O(\exp(-\Delta^2/(9t)))~.
\end{split}
\end{equation}
Thus, for $t$ small enough such that $\sqrt{t} < \Delta / 3$, we can apply Lemma~\ref{lem:der_bd} from below to $\tilde{f}_k$, from which we obtain (after reversing the change-of-variable) that
\begin{equation}
\begin{split}
    \hspace{20pt} \inf_{x \in [y_k-1.5\sigma, y_k+1.5\sigma] \setminus N} f'(x) \leq&~ (-1 + 2 \sqrt{\epsilon_k})/ t + O(\exp(-\Delta^2/(10t))) \\
    \inf_{x \in [y_k+1.5\sigma, y_k+3\sigma]} f(x) \leq &~ -0.5\\
    \sup_{x \in [y_k-3\sigma, y_k-1.5\sigma]} f(x) \geq &~ 0.5~.
\end{split}
\end{equation}
Hence, $\exists a_k \in [y_k - 1.5 \sigma, y_k + 1.5 \sigma] \setminus N$, $b_{k, +} \in [y_k + 1.5 \sigma, y_k + 3 \sigma]$ and $b_{k, -} \in [y_k - 3 \sigma, y_k - 1.5 \sigma]$ such that $f'(a_k) \leq (-1 + 2 \sqrt{ \epsilon_k})/ t + O(\exp(-\Delta^2/(10t)))$, $f(b_{k, +}) \leq 0$ and $f(b_{k, -}) \geq 0$. Furthermore, for $t$ small enough such that $ \sqrt{t} < \Delta / 6$, there is $b_{k, +} < b_{k+1, -}$ for $k \in [n-1]$, and hence by the fundamental theorem of calculus, $\exists c_k \in [b_{k, +}, b_{k+1, -}]$ such that $f'(c_k) \geq 0$.

Now, we focus on the sequence of points, $a_1 < c_1 < a_2 < ... < c_{n-1} < a_n$. By the fundamental theorem of calculus and the fact that $f$ is twice differentiable except for on a finite set, there is
\begin{align*}
    \int_{a_k}^{c_k} |f''(x)| dx \geq &~ |f'(c_k) - f'(a_k)| \geq (1 - 2 \sqrt{\epsilon_k})/ t - O(\exp(-\Delta^2/(10t)))  \\
    \int_{c_k}^{a_{k+1}} |f''(x)| dx \geq &~ |f'(a_{k+1}) - f'(c_k)| \geq (1 - 2 \sqrt{\epsilon_{k+1}})/ t - O(\exp(-\Delta^2/(10t)))
\end{align*}
and hence it is clear that
\begin{equation}
\label{eq:secderint_bd_1}
\begin{split}
    R[f] = \int_{-\infty}^\infty |f''(x)| dx \geq &~ \sum_{k=1}^{n-1} |f'(c_k) - f'(a_k)| + \sum_{k=1}^{n-1} |f'(a_{k+1}) - f'(c_k)| \\
    \geq &~ 2 \Big (n-1 - \sum_{k=1}^n 2 \sqrt{\epsilon_k} \Big )/ t - O(\exp(-\Delta^2/(10t))) \\
    \geq &~ 2 (n - 1 - 2 n \sqrt{\epsilon}) / t - O(\exp(-\Delta^2/(10t)))~.
 \end{split}
\end{equation}
Therefore, for $0 < \epsilon < 0.015$, as $n \geq 2$, it holds for $t$ sufficiently small that
\begin{equation}
\begin{split}
    \frac{R[\hat{s}^{(n)}_{t, \delta_t}]}{R[f]} \leq &~ \frac{(n-1) \Delta}{(n - 1 - 2 n \sqrt{\epsilon}) (\Delta - \delta_t) - O(\exp(-\Delta^2/(10t)))} \\
    \leq &~ 1 + 7.9 \sqrt{\epsilon} + O(\sqrt{t})~,
\end{split}
\end{equation}
which is bounded by $1 + 8 \sqrt{\epsilon}$. Since this holds for any $f$ in the feasible set, it also holds when the denominator on the left-hand-side is replaced by the infimum, $r_{t, \epsilon}^*$.

\hfill $\square$




\begin{lemma}
\label{lem:der_bd}
    Suppose $f$ is twice differentiable on $\sR$ except on a set $N$ of measure zero and $\int_{-3}^3 |x + f(x)|^2 p_{\mathcal{N}}(x; 1) dx < \epsilon$ with $0 < \epsilon < 0.03$. Then we have
    \begin{align}
        \inf_{x \in [-1.5,~ 1.5] \setminus N} f'(x) \leq&~ -1 + 2 \sqrt{\epsilon} \label{eq:fder_bd} \\
        \inf_{x \in [1.5,~ 3]} f(x) \leq&~ -0.5 \label{eq:fneg} \\
        \sup_{x \in [-1.5,~ -3]} f(x) \geq&~ 0.5 \label{eq:fpos}
    \end{align}
\end{lemma}

\noindent \textit{Proof of Lemma~\ref{lem:der_bd}}: We first prove (\ref{eq:fder_bd}) by supposing for contradiction that $\inf_{x \in [-1.5,~ 1.5] \setminus N} f'(x) = -1 + k_\Delta$ with $k_\Delta > 2 \sqrt{\epsilon}$. By the fundamental theorem of calculus, this means that the function $x + f(x)$ is monotonically increasing with slope at least $k_\Delta$ on $[-1.5, 1.5]$. Hence, there exists $x_1 \in \mathbb{R}$ such that $|x + f(x)| > k_\Delta |x - x_1|$ for $x \in [-1.5, 1.5]$. Therefore,
\begin{equation}
    \begin{split}
        \int_{-\infty}^\infty |x + f(x)|^2 p_{\mathcal{N}}(x; 1) dx \geq &~ \int_{-1.5}^{1.5} |x + f(x)|^2 p_{\mathcal{N}}(x; 1) dx \\
        \geq &~ (k_\Delta)^2 \int_{-1.5}^{1.5} |x - x_1|^2 p_{\mathcal{N}}(x; 1) dx \\
        = &~ (k_\Delta)^2 \int_{-1.5}^{1.5} (x^2 + 2 x x_1 + (x_1)^2) p_{\mathcal{N}}(x; 1) dx \\
        \geq &~ (k_\Delta)^2 \int_{-1.5}^{1.5} x^2 p_{\mathcal{N}}(x; 1) dx \\
        > &~ 0.25 (k_\Delta)^2 > \epsilon~,
    \end{split}
\end{equation}
which shows a contradiction.

Next, we prove (\ref{eq:fneg}) by supposing for contradiction that $\inf_{x \in [1.5,~ 3] \setminus N} f(x) > -0.5$, in which case it holds that $\inf_{1.5 \leq x \leq 3} |f(x) + x| > 1$, and hence
\begin{equation}
\begin{split}
    \int_{-\infty}^\infty |x + f(x)|^2 p_{\mathcal{N}}(x; 1) dx \geq &~ 
    \int_{1.5}^3 |x + f(x)|^2 p_{\mathcal{N}}(x; 1) dx \\
    \geq&~ \int_{1.5}^3 p_{\mathcal{N}}(x; 1) dx \\
    >&~ 0.03 > \epsilon
\end{split}
\end{equation}
which shows a contradiction. A similar argument can be used to prove (\ref{eq:fpos}).

\hfill $\square$ \\

\section{Alternative Definition of Local Averaging in Higher Dimensions}
\label{sec:sm_alt}
Given a compact set of vectors, $A \subseteq \sR^d$, we define its \emph{element with minimum Euclidean norm} as
\begin{equation}
    \gamma (A) \coloneqq \begin{cases}
        \arg \min_{\vv \in A} \| \vv \|_2~,& \quad \text{ if } \arg \min_{\vv \in A} \| \vv \|_2 \text{ is unique} \\
        \vzero~,& \quad \text{ otherwise } 
    \end{cases}
\end{equation}
Given $w > 0$ and a vector-valued function $\vf$ on $\sR^d$, we consider
\begin{equation}
\label{eq:sm_d_minabs}
    (w * \vf)(\vx) \coloneqq \gamma \left ( \left \{ \frac{1}{2 w} \int_{-w}^{w} \vf(\vx + r \vv) dr: \vv \in \sS^{d-1} \right \} \right )~,
\end{equation}
as an alternative definition of LA in higher dimensions.
In contrast with (\ref{eq:sm_cube}), this new definition is invariant to translations and rotations to the coordinate system underlying $\sR^d$.

As in Section~\ref{sec:d>1}, we will consider the PL approximation of the ESF when $t$ is small:
\begin{equation}
\begin{split}
    \nabla \log p^{(n)}_t(\vx) =&~ [\big ( \hat{x}^{(n)}_t([\vx]_1) - [\vx]_1 \big ) / t, -[\vx]_2 / t, ..., -[\vx]_d / t ]^\intercal \\
    \approx &~ [\bar{s}^{(n)}_t([\vx]_1), -[\vx]_2 / t, ..., -[\vx]_d / t ]^\intercal =: \bar{\vs}^{(n)}_t(\vx)
\end{split}
\end{equation}
We will show that under the new definition of (\ref{eq:sm_d_minabs}), performing LA on $\bar{\vs}^{(n)}_t$ yields the same LA-PL SF as (\ref{eq:hat_s_d>1}):
\begin{proposition}
\label{prop:new_smooth}
Under (\ref{eq:sm_d_minabs}), $w * \bar{\vs}^{(n)}_t \equiv \hat{\vs}^{(n)}_{t, 1 - w}$ for $w \in (0, 1]$, with the latter defined as in (\ref{eq:hat_s_d>1}).    
\end{proposition}
\noindent \textit{Proof of Proposition~\ref{prop:new_smooth}}:
To ease notations, in the following we write $\vs_{w, \vv}(\vx) \coloneqq \frac{1}{2 w} \int_{-w}^{w} \bar{\vs}^{(n)}_t(\vx + r \vv) dr$. For $i > 1$, thanks to the linearity of $\partial_i \log p^{(n)}_t(\vx)$, we have that $[\vs_{w, \vv}(\vx)]_i = \partial_i \log p^{(n)}_t(\vx) = -[\vx]_i / t$. For $i = 1$, there is 
\begin{equation}
\begin{split}
    [\vs_{w, \vv}(\vx)]_1 =&~ \frac{1}{2 w t} \int_{-w}^{w} \bar{s}^{(n)}_t([\vx]_1 + r v_1) dr \\
    =&~ \frac{1}{2 w |v_1| t} \int_{-w |v_1|}^{w |v_1|} \bar{s}^{(n)}_t([\vx]_1 + \tilde{r}) d \tilde{r} \\
    =&~ ((w |v_1|) *  \bar{s}^{(n)}_t)([\vx]_1)~.
\end{split}
\end{equation}
Therefore, we obtain that
\begin{equation}
    \big \{ \vs_{w, \vv}(\vx): \vv \in \sS^{d-1} \} = \{
       [ \hat{s}^{(n)}_{t, 1-\tilde{w}}([\vx]_1),
        -[\vx]_2 / t,
        ...,
        -[\vx]_d / t]^\intercal
    : \tilde{w} \in [0, w] \big \}~.
\end{equation}
From (\ref{eq:hat_s}) and Figure~\ref{fig:score_1d}, it is clear that for any fixed $x$, as $\tilde{w}$ ranges from $0$ to $w$, $\hat{s}^{(n)}_{t, 1-\tilde{w}}(x)$ keeps the same sign while its absolute value decreases. Therefore, we derive that
\begin{equation}
    (w * \bar{\vs}^{(n)}_t)(\vx) = \gamma(\big \{ \vs_{w, \vv}(\vx): \vv \in \sS^{d-1} \}) = [ \hat{s}^{(n)}_{t, 1-w}([\vx]_1),
        -[\vx]_2 / t,
        ...,
        -[\vx]_d / t]^\intercal~.
\end{equation}
\hfill $\square$

Hence, if in particular we choose $w_t = 1 - \delta_t$ with (\ref{eq:delta_t}), we see that the dynamics described by (\ref{eq:sm_back_d=1}) and (\ref{eq:sm_back_d>1}) is indeed equivalent to
\begin{equation}
    \tfrac{d}{dt} \rvx_t = -\tfrac{1}{2} (w * \bar{\vs}^{(n)}_t)(\rvx_t)~,
\end{equation}
under the definition of (\ref{eq:sm_d_minabs}).



\section{Proof of Proposition~\ref{prop:flow_1d}}
\label{sec:pf_flow_1d}
We consider each of three cases separately.
\paragraph{Case I: $x \in [\delta_t - 1, 1 - \delta_t]$.} In this case, it is easy to verify that $x_s = \frac{1 - \delta_s}{1 - \delta_t} x$ is a valid solution to the ODE
\begin{equation}
    \frac{d}{ds} x_s = -\frac{1}{2} \frac{\delta_s}{1 - \delta_s} \frac{x_s}{s}~,
\end{equation}
on $[0, t]$ that satisfies the terminal condition $x_t = x$. It remains to verify that for all $s \in (0, t)$, it holds that $x_s \in [\delta_s - 1, 1 - \delta_s]$ (i.e., the entire trajectory during $[0, t]$ remains in region \textbf{C}). 

Suppose that $x \geq 0$. Then it is clear that $x_s \geq 0$, $\forall s \in [0, t]$.
Moreover, it holds that
\begin{equation}
    x_s - (1 - \delta_s) = \frac{1 - \delta_s}{1 - \delta_t} (x_t - (1 - \delta_t)) \leq 0
\end{equation}
Therefore, $x_s \in [0, 1 - \delta_s] \subseteq [\delta_s - 1, 1 - \delta_s]$. A similar argument can be made if $x < 0$.

\paragraph{Case II: $x \leq \delta_t - 1$.} In this case, it is also easy to verify that $x_s = \sqrt{\frac{s}{t}} (x + 1) - 1$ is a valid solution to the ODE
\begin{equation}
    \frac{d}{ds} x_s = \frac{1}{2} \frac{x+1}{s}~,
\end{equation}
on $[0, t]$ that satisfies the terminal condition $x_t = x$. It remains to verify that for all $s \in (0, t)$, it holds that $x_s \leq \delta_s - 1$ (i.e., the entire trajectory during $[0, t]$ remains in region \textbf{A}). This is true because
\begin{equation}
    (x_s + 1) - \delta_s = \sqrt{\frac{s}{t}}(x+1) - \delta_s = \sqrt{\frac{s}{t}}(x + 1 - \delta_t) \leq 0~.
\end{equation}

\paragraph{Case III: $x \geq 1 - \delta_t$.} A similar argument can be made as in Case II above.

\hfill $\square$

\section{Proof of Proposition~\ref{cor:kl}}
\label{app:pf_prop_kl}
The proof relies on the following lemma, which allows us to relate the KL-divergence between $\hat{p}^{(n, t_0)}_0$ and the uniform density via that of $\hat{p}^{(n, t_0)}_{t_0}$:
\begin{lemma}
\label{lem:kl}
    $\forall t \in [0, t_0]$, $\text{KL}(u_1 || \hat{p}^{(n, t_0)}_0) = \text{KL}(u_{1-\delta_t} || \hat{p}^{(n, t_0)}_t)$.
\end{lemma}
\noindent \textit{Proof of Lemma~\ref{lem:kl}}: 
\begin{equation}
\begin{split}
    \text{KL}(u_1 || \hat{p}^{(n, t_0)}_0) 
    =&~ \int_{-1}^1 \frac{1}{2} \cdot (- \log2 - \log ( \tilde{p}^{(n, t_0)}_0(x))) dx \\
    =&~ -\log2 - \frac{1}{2} \int_{-1}^1 \log ( \tilde{p}^{(n, t_0)}_0(x)) dx \\
    =&~ -\log2 - \frac{1}{2 (1 - \delta_t)} \int_{\delta_t - 1}^{1 - \delta_t} \left ( \log(1 - \delta_t) + \log(\hat{p}^{(n, t_0)}_t(x')) \right ) d x' \\
    =&~ \frac{1}{2 (1 - \delta_t)} \int_{\delta_t - 1}^{1 - \delta_t} \log(1 / (2(1-\delta_t))) - \log(\hat{p}^{(n, t_0)}_t(x')) d x' \\
    =&~ \text{KL}(u_{1 - \delta_t} || \hat{p}^{(n, t_0)}_t) 
\end{split}
\end{equation}
\hfill $\square$\\
In light of Lemma~\ref{lem:kl}, we choose $t 
= t_0$ and examine the KL-divergence between $u_{1 - \delta_{t_0}}$ and $\hat{p}^{(n, t_0)}_{t_0}$.
By symmetry, we only need to consider the right half of the interval, $[0, 1 - \delta_{t_0}]$, on which there is $\hat{p}^{(n, t_0)}_{t_0}(x) = p^{(n)}_{t_0}(x) \geq \frac{1}{2} p_{\mathcal{N}}(x-1; \sqrt{t_0})$.
We have
    \begin{equation}
        \begin{split}
            \int_0^{1 - \delta_{t_0}} \log \left ( p_{\mathcal{N}}(x-1; \sqrt{t_0}) \right ) dx =&~ \int_{-1}^{-\delta_{t_0}} \log \left ( \tfrac{1}{\sqrt{2\pi t_0}} \exp(-x^2/t_0) \right ) dx \\
            =&~ -\tfrac{1 - \delta_{t_0}}{2} (\log(2 \pi) + \log(t_0)) - \tfrac{1}{t_0} \int_{-1}^{-\delta_{t_0}} x^2 dx \\
            \geq&~ - \tfrac{1 - \delta_{t_0}}{2} (\log(2 \pi) + \log(t_0)) - \tfrac{1}{3t_0}~.
        \end{split}
    \end{equation}
    Therefore,
    \begin{equation}
        \begin{split}
            \text{KL}(u_{[0, 1 - \delta_{t_0}]} || \hat{p}^{(n, t_0)}_{t_0}) =&~ \frac{1}{1 - \delta_{t_0}} \int_0^{1 - \delta_{t_0}} -\log(1 - \delta_{t_0}) - \log \left ( \frac{1}{2} p_{\mathcal{N}}(x-1; \sqrt{t_0}) \right ) dx \\
            \leq&~ -\log(1 - \delta_t) + \log(2) - \tfrac{1}{1 - \delta_{t_0}} \left (- \tfrac{1 - \delta_{t_0}}{2} (\log(2 \pi) + \log(t_0)) - \tfrac{1}{3t_0} \right ) \\
            \leq&~ \tfrac{1}{3t_0(1 - \delta_{t_0})} + \log \big (\tfrac{\sqrt{t_0}}{1 - \delta_{t_0}} \big ) + \log(2 \sqrt{2 \pi})~.
        \end{split}
    \end{equation}
By symmetry, the same bound can be obtained for $\text{KL}(u_{1 - \delta_{t_0}} || \hat{p}^{(n, t_0)}_{t_0})$, which yields the desired result when combined with Lemma~\ref{lem:kl}.

\section{Additional Details on the Numerical Experiments}
\label{sec:nn_details}
\subsection{Experiment 1}
\label{app:details_exp_1}
\paragraph{NN-learned SF.} We trained a two-layer MLP with a skip linear connection from the input layer to fit the ESF at $t = 0.05$. The model is trained by the AdamW optimizer \citep{kingma2015adam, loshchilov2018decoupled} for $6000$ steps with learning rate $0.0002$, and we consider four choices of the weight decay coefficient: $\lambda_1 = 1.0$, $\lambda_2 = 3.0$, $\lambda_3 = 5.0$ and $\lambda_4 = 7.0$. At each training step, the optimization objective is an approximation of the expectation in (\ref{eq:emp_loss}) using a batch of $1024$ i.i.d. samples from $p^{(n)}_t$. We considered four choices of $\lambda$: $\lambda_1 = 1.0$, $\lambda_2 = 3.0$, $\lambda_3 = 5.0$ and $\lambda_4 = 7.0$.

\paragraph{LA-PL SF.} We chose $t = 0.05$ and four values of $\delta$ ($\delta_1 = 0.648$, $\delta_2 = 0.548$, $\delta_3 = 0.453$, $\delta_4 = 0.346$), which were tuned to roughly match the corresponding curves in the left panel.

\subsection{Experiment 2}
\label{app:details_exp_2}
We choose $\delta_t$ according to (\ref{eq:delta_t}) with $\kappa = 1.2$. The ESF is computed from its analytical expression (\ref{eq:hat_x_n}), and the locally-averaged ESF is computed numerically through a Monte-Carlo approximation to the integral (\ref{eq:sm_cube}) with $512$ samples. To ensure numerical stability at small $t$, we truncate the sampled values of $\nabla \log p^{(n)}_t$ based on magnitude.
At $t_0 = 0.02$, $20000$ realizations of $\rvx_{t_0}$ are sampled from $p^{(n)}_{t_0}$. Then, the ODEs are numerically solved backward-in-time to $t=10^{-5}$ using Euler's method under the noise schedule from \citet{karras2022elucidating} with $200$ steps and $\rho=2$. 

For the NN-learned SF, after a rescaling by $\sqrt{t}$ 
(c.f. the discussion on output scaling in \citealt{karras2022elucidating}),
we parameterize $\vs^{\text{NN}}_t$ with three two-layer MLP blocks ($\text{MLP}_1$, $\text{MLP}_2$, $\text{MLP}_3$): $\text{MLP}_1$ is applied to $\log(t)$ to compute a time embedding; $\text{MLP}_2$ is applied to the concatenation of $\vx$ and the time embedding; $\text{MLP}_3$ is also applied to $\log(t)$ and its output modulates the output of $\text{MLP}_2$ similarly to the Adaptive Layer Norm modulation \citep{perez2018film, peebles2023scalable}. $\text{MLP}_1$ and $\text{MLP}_3$ share the first-layer weights and biases. The model is
trained to minimize a discretized version of (\ref{eq:emp_loss}) with $T = t_0$,
where the integral is approximated by sampling $t$ from $[10^{-6}, t_0]$ with $t^{1/3}$ uniformly distributed (inspired by the noise schedule of \citealt{karras2022elucidating}) and then $\vx$ from $p^{(n)}_t$. The parameters are updated by the AdamW optimizer with learning rate $0.00005$, batch size $1024$ and a total number of $150000$ steps, where weight decay (coefficient $3$) is applied only to the weights and biases of $\text{MLP}_2$.


\begin{figure}
    \centering
\includegraphics[width=1\linewidth]{figures/2d_score_dim1.png}
    \caption{Comparing three SF variants from Experiment $2$ in their first (tangent) dimension at different $t$. We see a close proximity between the LA-PL SF and the NN-learned SF, both of which are smoother than the ESF especially at small $t$.}
    \label{fig:2d_score_dim1}
\end{figure}
\begin{figure}
    \centering
\includegraphics[width=1\linewidth]{figures/2d_score_dim2.png}
    \caption{Comparing three SF variants from Experiment $2$ in their second (normal) dimension at different $t$. We see all three SF are relatively similar in the normal direction, except for a mild distortion of the NN-learned SF when $t$ is small and $[\vx]_2$ is large (where $p^{(n)}_t$ has low density).}
    \label{fig:2d_score_dim2}
\end{figure}
\begin{figure}
    \centering
    \hspace{20pt} \includegraphics[width=0.95\linewidth]{figures/2d_score_slice.png}
    \caption{Comparing three SF variants from Experiment $2$ in their first (tangent) dimension at different $t$ when they are evaluated on the $[\vx]_1$ axis. Again, we see a close proximity between the LA-PL SF and the NN-learned SF, both of which are smoother than the ESF.}
    \label{fig:2d_score_slice}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figures/log_density_2.pdf}
    \caption{Additional results of Experiment $2$ discussed in Section~\ref{sec:exp_score}. The only difference with Figure~\ref{fig:exp} is that the third column corresponds to driving the denoising dynamics with the locally-averaged ESF ($\vs_t = (1-\delta_t) * \nabla \log p^{(n)}_t$). The fact that the last two columns are nearly identical gives evidence that the LA-PL SF is a good approximation to the locally-averaged ESF for the purpose of the denoising dynamics at small $t$.}
    \label{fig:log_density_2}
\end{figure}
\end{document}
