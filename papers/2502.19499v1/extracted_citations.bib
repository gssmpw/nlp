@article{achilli2024losing,
  title={Losing dimensions: Geometric memorization in generative diffusion},
  author={Achilli, Beatrice and Ventura, Enrico and Silvestri, Gianluigi and Pham, Bao and Raya, Gabriel and Krotov, Dmitry and Lucibello, Carlo and Ambrogioni, Luca},
  journal={arXiv preprint arXiv:2410.08727},
  year={2024}
}

@article{azangulov2024convergence,
  title={Convergence of diffusion models under the manifold hypothesis in high-dimensions},
  author={Azangulov, Iskander and Deligiannidis, George and Rousseau, Judith},
  journal={arXiv preprint arXiv:2409.18804},
  year={2024}
}

@article{baptista2025memorization,
  title={Memorization and Regularization in Generative Diffusion Models},
  author={Baptista, Ricardo and Dasgupta, Agnimitra and Kovachki, Nikola B and Oberai, Assad and Stuart, Andrew M},
  journal={arXiv preprint arXiv:2501.15785},
  year={2025}
}

@article{biroli2024dynamical,
  title={Dynamical regimes of diffusion models},
  author={Biroli, Giulio and Bonnaire, Tony and De Bortoli, Valentin and M{\'e}zard, Marc},
  journal={Nature Communications},
  volume={15},
  number={1},
  pages={9957},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@article{block2020generative,
  title={Generative modeling with denoising auto-encoders and Langevin sampling},
  author={Block, Adam and Mroueh, Youssef and Rakhlin, Alexander},
  journal={arXiv preprint arXiv:2002.00107},
  year={2020}
}

@inproceedings{carlini2023extracting,
author = {Carlini, Nicholas and Hayes, Jamie and Nasr, Milad and Jagielski, Matthew and Sehwag, Vikash and Tram\`{e}r, Florian and Balle, Borja and Ippolito, Daphne and Wallace, Eric},
title = {Extracting training data from diffusion models},
year = {2023},
isbn = {978-1-939133-37-3},
publisher = {USENIX Association},
address = {USA},
abstract = {Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have attracted significant attention due to their ability to generate high-quality synthetic images. In this work, we show that diffusion models memorize individual images from their training data and emit them at generation time. With a generate-and-filter pipeline, we extract over a thousand training examples from state-of-the-art models, ranging from photographs of individual people to trademarked company logos. We also train hundreds of diffusion models in various settings to analyze how different modeling and data decisions affect privacy. Overall, our results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training.},
booktitle = {Proceedings of the 32nd USENIX Conference on Security Symposium},
articleno = {294},
numpages = {18},
location = {Anaheim, CA, USA},
series = {SEC '23}
}

@inproceedings{chen2023improved,
  title={Improved analysis of score-based generative modeling: User-friendly bounds under minimal smoothness assumptions},
  author={Chen, Hongrui and Lee, Holden and Lu, Jianfeng},
  booktitle={International Conference on Machine Learning},
  pages={4735--4763},
  year={2023},
  organization={PMLR}
}

@inproceedings{chen2023score,
  title={Score approximation, estimation and distribution recovery of diffusion models on low-dimensional data},
  author={Chen, Minshuo and Huang, Kaixuan and Zhao, Tuo and Wang, Mengdi},
  booktitle={International Conference on Machine Learning},
  pages={4672--4712},
  year={2023},
  organization={PMLR}
}

@article{chen2024learning,
  title={Learning general gaussian mixtures with efficient score matching},
  author={Chen, Sitan and Kontonis, Vasilis and Shah, Kulin},
  journal={arXiv preprint arXiv:2404.18893},
  year={2024}
}

@article{gao2024flow,
  title={How Do Flow Matching Models Memorize and Generalize in Sample Data Subspaces?},
  author={Gao, Weiguo and Li, Ming},
  journal={arXiv preprint arXiv:2410.23594},
  year={2024}
}

@article{gatmiry2024learning,
  title={Learning mixtures of gaussians using diffusion models},
  author={Gatmiry, Khashayar and Kelner, Jonathan and Lee, Holden},
  journal={arXiv preprint arXiv:2404.18869},
  year={2024}
}

@article{gu2023memorization,
  title={On memorization in diffusion models},
  author={Gu, Xiangming and Du, Chao and Pang, Tianyu and Li, Chongxuan and Lin, Min and Wang, Ye},
  journal={arXiv preprint arXiv:2310.02664},
  year={2023}
}

@article{huang2024convergence,
  title={Convergence analysis of probability flow ODE for score-based generative models},
  author={Huang, Daniel Zhengyu and Huang, Jiaoyang and Lin, Zhengjiang},
  journal={arXiv preprint arXiv:2404.09730},
  year={2024}
}

@article{huang2024denoising,
  title={Denoising diffusion probabilistic models are optimally adaptive to unknown low dimensionality},
  author={Huang, Zhihan and Wei, Yuting and Chen, Yuxin},
  journal={arXiv preprint arXiv:2410.18784},
  year={2024}
}

@article{kamb2024analytic,
  title={An analytic theory of creativity in convolutional diffusion models},
  author={Kamb, Mason and Ganguli, Surya},
  journal={arXiv preprint arXiv:2412.20292},
  year={2024}
}

@inproceedings{oko2023diffusion,
  title={Diffusion models are minimax optimal distribution estimators},
  author={Oko, Kazusato and Akiyama, Shunta and Suzuki, Taiji},
  booktitle={International Conference on Machine Learning},
  pages={26517--26582},
  year={2023},
  organization={PMLR}
}

@article{peyre2009manifold,
  title={Manifold models for signals and images},
  author={Peyr{\'e}, Gabriel},
  journal={Computer vision and image understanding},
  volume={113},
  number={2},
  pages={249--260},
  year={2009},
  publisher={Elsevier}
}

@article{potaptchik2024linear,
  title={Linear convergence of diffusion models under the manifold hypothesis},
  author={Potaptchik, Peter and Azangulov, Iskander and Deligiannidis, George},
  journal={arXiv preprint arXiv:2410.09046},
  year={2024}
}

@article{scarvelis2023closed,
  title={Closed-form diffusion models},
  author={Scarvelis, Christopher and Borde, Haitz S{\'a}ez de Oc{\'a}riz and Solomon, Justin},
  journal={arXiv preprint arXiv:2310.12395},
  year={2023}
}

@inproceedings{song2021maximum,
 author = {Song, Yang and Durkan, Conor and Murray, Iain and Ermon, Stefano},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {1415--1428},
 publisher = {Curran Associates, Inc.},
 title = {Maximum Likelihood Training of Score-Based Diffusion Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/0a9fdbb17feb6ccb7ec405cfb85222c4-Paper.pdf},
 volume = {34},
 year = {2021}
}

@article{ventura2024manifolds,
  title={Manifolds, random matrices and spectral gaps: The geometric phases of generative diffusion},
  author={Ventura, Enrico and Achilli, Beatrice and Silvestri, Gianluigi and Lucibello, Carlo and Ambrogioni, Luca},
  journal={arXiv preprint arXiv:2410.05898},
  year={2024}
}

@article{wang2023diffusion,
  title={Diffusion models generate images like painters: an analytical theory of outline first, details later},
  author={Wang, Binxu and Vastola, John J},
  journal={arXiv preprint arXiv:2303.02490},
  year={2023}
}

@article{wang2024subspace,
  title={Diffusion models learn low-dimensional distributions via subspace clustering},
  author={Wang, Peng and Zhang, Huijie and Zhang, Zekai and Chen, Siyi and Ma, Yi and Qu, Qing},
  journal={arXiv preprint arXiv:2409.02426},
  year={2024}
}

@InProceedings{wibisono2024optimal,
  title = 	 {Optimal score estimation via empirical Bayes smoothing},
  author =       {Wibisono, Andre and Wu, Yihong and Yang, Kaylee Yingxi},
  booktitle = 	 {Proceedings of Thirty Seventh Conference on Learning Theory},
  pages = 	 {4958--4991},
  year = 	 {2024},
  editor = 	 {Agrawal, Shipra and Roth, Aaron},
  volume = 	 {247},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {30 Jun--03 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v247/wibisono24a/wibisono24a.pdf},
  url = 	 {https://proceedings.mlr.press/v247/wibisono24a.html},
  abstract = 	 {We study the problem of estimating the score function of an unknown probability distribution $\rho^*$ from $n$ independent and identically distributed observations in $d$ dimensions. Assuming that $\rho^*$ is subgaussian and has a Lipschitz-continuous score function $s^*$, we establish the optimal rate of $\tilde \Theta(n^{-\frac{2}{d+4}})$ for this estimation problem under the loss function $\|\hat s - s^*\|^2_{L^2(\rho^*)}$ that is commonly used in the score matching literature, highlighting the curse of dimensionality where sample complexity for accurate score estimation grows exponentially with the dimension $d$. Leveraging key insights in empirical Bayes theory as well as a new convergence rate of smoothed empirical distribution in Hellinger distance, we show that a regularized score estimator based on a Gaussian kernel attains this rate, shown optimal by a matching minimax lower bound. We also discuss extensions to estimating $\beta$-HÃ¶lder continuous scores with $\beta \leq 1$, as well as the implication of our theory on the sample complexity of score-based generative models.}
}

@article{yi2023generalization,
  title={On the generalization of diffusion model},
  author={Yi, Mingyang and Sun, Jiacheng and Li, Zhenguo},
  journal={arXiv preprint arXiv:2305.14712},
  year={2023}
}

