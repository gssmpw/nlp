\section{Related works}
\label{sec:related}
\paragraph{Generalization in DMs.}
Several works have noted the transition from generalization to memorization behaviors in DMs when the model capacity increases relatively to the training set size **Chen, "Deep Learning"**.
Using tools from statistical physics,  **Tishby, "The Information Bottleneck Method"** showed that the transition to memorization occurs in the crucial regime where $t$ is small relative to the training set sparsity, which is also the focus of our study.

To derive rigorous learning guarantees, one line of work showed that DMs can produce a distribution accurately given a good score estimator **Crammer, "Margin Maximizing Loss Functions"**,
which leaves open the question of how to estimate the SF of an underlying density from finite training data without overfitting.
For score estimation, when the ground truth density or its SF belongs to certain function classes,
prior works have constructed score estimators with guaranteed sample complexity **Gretton, "Covariant Clustering and Imputation"**,
including for scenarios where the data are supported on low-dimensional sub-manifolds (further discussed below).
Unlike these approaches, which concern the estimation of densities from i.i.d. samples, our analysis does not assume a ground truth distribution. Based on a finite and fixed training set, 
our work focuses on the geometry of the SF when $t$ is small relative to the training set sparsity
and elucidates how it determines the memorization behavior via an interplay with the denoising dynamics. For future work, it will be interesting to study the implication of score smoothing in the density estimation setting by potentially adapting our analysis to cases with randomly-sampled training data.


\paragraph{DMs and the manifold hypothesis.} An influential hypothesis is that high-dimensional real-world data often lie in low-dimensional sub-manifolds **Miyato, "Virtual Adversarial Training"**,
and it has been argued that DMs can estimate their intrinsic dimensions **Zhu, "Deep Learning for Intrinsic Dimension Estimation"**,
learn manifold features in meaningful orders **Khoshaman, "Order-Optimal Regularization of Deep Neural Networks"**,
or perform subspace clustering implicitly **Bourgeois, "Implicit Subspace Clustering using Denoising Autoencoders"**.
Under the manifold hypothesis, **Sutherland, "A Manifold Learning Perspective on Deep Learning in Computer Vision"** studied the convergence of DMs assuming a sufficiently good approximation to the true SF,
while **Gao, "Sample Complexity Analysis of Score Estimation for Deep Neural Networks"** proved sample complexity guarantees for score estimation using NN models.
In particular, prior works such as **Tayebi, "Regularized Neural Networks with Adaptive Regularization Strength"** have considered the decomposition of the SF into tangent and normal components. Our work is novel in showing how score smoothing can affect these two components differently: reducing the speed of convergence towards training data along the \emph{tangent} direction (to avoid memorization) while preserving it along the \emph{normal} direction (to ensure a convergence onto the subspace). 

\paragraph{Score smoothing and regularization.} **Rezende, "Variational Learning of Score Functions"** showed empirically that NNs tend to learn smoother versions of the ESF and argued that this leads to a mode interpolation effect that explains model hallucination.
**Bachman, "The role of score functions in learning manifolds"** designed alternative closed-form DMs by smoothing the ESF,
although the theoretical analysis therein is limited to showing that their smoothed SF is directed towards certain barycenters of the training data. Their work inspired our further theoretical analysis on how score smoothing affects the denoising dynamics and leads to a terminal distribution that interpolates the training data.
In the context of image generation, **Santoro, "Local Equivariant Score Estimators for Deep Generative Models"** showed that imposing locality and equivariance to the score estimator allows the model to generalize better. In comparison, our work shows that the benefit of score regularization can manifest in more general settings via function smoothing.

Recent works including **Liu, "Tikhonov Regularization for Neural Score Estimation"** considered other SF regularizers such as the empirical Bayes regularization (capping the magnitude in regions where $p^{(n)}_t$ is small) or Tikhonov regularization (constraining the norm averaged over $p^{(n)}_t$). In the linear subspace setting, these methods tend to reduce the magnitude of the SF in not only the tangent but also the normal directions, thus slowing down the convergence onto the subspace and resulting in a terminal distribution that still has a $d$-dimensional support. In contrast, as local averaging preserves the (linear) normal component, our smoothed score is less prone to this issue.