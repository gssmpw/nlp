@ARTICLE{chang2011errorfunc,
  author={Chang, Seok-Ho and Cosman, Pamela C. and Milstein, Laurence B.},
  journal={IEEE Transactions on Communications}, 
  title={Chernoff-Type Bounds for the Gaussian Error Function}, 
  year={2011},
  volume={59},
  number={11},
  pages={2939-2944},
  keywords={Upper bound;Fading channels;Approximation methods;Error probability;Signal to noise ratio;Communication systems;Bounds;error function;exponential;Gaussian Q-function},
  doi={10.1109/TCOMM.2011.072011.100049}}


@article{vdb2024target,
  title={Target Score Matching},
  author={De Bortoli, Valentin and Hutchinson, Michael and Wirnsberger, Peter and Doucet, Arnaud},
  journal={arXiv preprint arXiv:2402.08667},
  year={2024}
}

@inproceedings{song2021maximum,
 author = {Song, Yang and Durkan, Conor and Murray, Iain and Ermon, Stefano},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {1415--1428},
 publisher = {Curran Associates, Inc.},
 title = {Maximum Likelihood Training of Score-Based Diffusion Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/0a9fdbb17feb6ccb7ec405cfb85222c4-Paper.pdf},
 volume = {34},
 year = {2021}
}

@inproceedings{
song2021sde,
title={Score-Based Generative Modeling through Stochastic Differential Equations},
author={Yang Song and Jascha Sohl-Dickstein and Diederik P Kingma and Abhishek Kumar and Stefano Ermon and Ben Poole},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=PxTIG12RRHS}
}

@article{gu2023memorization,
  title={On memorization in diffusion models},
  author={Gu, Xiangming and Du, Chao and Pang, Tianyu and Li, Chongxuan and Lin, Min and Wang, Ye},
  journal={arXiv preprint arXiv:2310.02664},
  year={2023}
}

@article{li2024good,
  title={A good score does not lead to a good generative model},
  author={Li, Sixu and Chen, Shi and Li, Qin},
  journal={arXiv preprint arXiv:2401.04856},
  year={2024}
}

@article{scarvelis2023closed,
  title={Closed-form diffusion models},
  author={Scarvelis, Christopher and Borde, Haitz S{\'a}ez de Oc{\'a}riz and Solomon, Justin},
  journal={arXiv preprint arXiv:2310.12395},
  year={2023}
}

@inproceedings{
li2023on,
title={On the Generalization Properties of Diffusion Models},
author={Puheng Li and Zhong Li and Huishuai Zhang and Jiang Bian},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=hCUG1MCFk5}
}

@article{yi2023generalization,
  title={On the generalization of diffusion model},
  author={Yi, Mingyang and Sun, Jiacheng and Li, Zhenguo},
  journal={arXiv preprint arXiv:2305.14712},
  year={2023}
}

@article{
vdb2022convergence,
title={Convergence of denoising diffusion models under the manifold hypothesis},
author={De Bortoli, Valentin},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2022},
url={https://openreview.net/forum?id=MhK5aXo3gB},
note={Expert Certification}
}

@inproceedings{
aithal2024understanding,
title={Understanding Hallucinations in Diffusion Models through Mode Interpolation},
author={Sumukh K Aithal and Pratyush Maini and Zachary Chase Lipton and J Zico Kolter},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=aNTnHBkw4T}
}

@article{ramesh2022hierarchical,
  title={Hierarchical text-conditional image generation with clip latents},
  author={Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
  journal={arXiv preprint arXiv:2204.06125},
  volume={1},
  number={2},
  pages={3},
  year={2022}
}

@article{videoworldsimulators2024,
  title={Video generation models as world simulators},
  author={Tim Brooks and Bill Peebles and Connor Holmes and Will DePue and Yufei Guo and Li Jing and David Schnurr and Joe Taylor and Troy Luhman and Eric Luhman and Clarence Ng and Ricky Wang and Aditya Ramesh},
  year={2024},
  url={https://openai.com/research/video-generation-models-as-world-simulators},
}

@article{hyvarinen2005estimation,
  title={Estimation of non-normalized statistical models by score matching.},
  author={Hyv{\"a}rinen, Aapo and Dayan, Peter},
  journal={Journal of Machine Learning Research},
  volume={6},
  number={4},
  year={2005}
}

@inproceedings{
kadkhodaie2024generalization,
title={Generalization in diffusion models arises from geometry-adaptive harmonic representations},
author={Zahra Kadkhodaie and Florentin Guth and Eero P Simoncelli and St{\'e}phane Mallat},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=ANvmVS2Yr0}
}

@inproceedings{chen2023score,
  title={Score approximation, estimation and distribution recovery of diffusion models on low-dimensional data},
  author={Chen, Minshuo and Huang, Kaixuan and Zhao, Tuo and Wang, Mengdi},
  booktitle={International Conference on Machine Learning},
  pages={4672--4712},
  year={2023},
  organization={PMLR}
}

@inproceedings{oko2023diffusion,
  title={Diffusion models are minimax optimal distribution estimators},
  author={Oko, Kazusato and Akiyama, Shunta and Suzuki, Taiji},
  booktitle={International Conference on Machine Learning},
  pages={26517--26582},
  year={2023},
  organization={PMLR}
}

@inproceedings{
pidstrigach2022scorebased,
title={Score-Based Generative Models Detect Manifolds},
author={Jakiw Pidstrigach},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=AiNrnIrDfD9}
}

@article{roweis2000nonlinear,
  title={Nonlinear dimensionality reduction by locally linear embedding},
  author={Roweis, Sam T and Saul, Lawrence K},
  journal={science},
  volume={290},
  number={5500},
  pages={2323--2326},
  year={2000},
  publisher={American Association for the Advancement of Science}
}

@inproceedings{
lee2022convergence,
title={Convergence for score-based generative modeling with polynomial complexity},
author={Holden Lee and Jianfeng Lu and Yixin Tan},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=dUSI4vFyMK}
}

@inproceedings{
benton2024nearly,
title={Nearly \$d\$-Linear Convergence Bounds for Diffusion Models via Stochastic Localization},
author={Joe Benton and Valentin De Bortoli and Arnaud Doucet and George Deligiannidis},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=r5njV3BsuD}
}

@inproceedings{chen2023improved,
  title={Improved analysis of score-based generative modeling: User-friendly bounds under minimal smoothness assumptions},
  author={Chen, Hongrui and Lee, Holden and Lu, Jianfeng},
  booktitle={International Conference on Machine Learning},
  pages={4735--4763},
  year={2023},
  organization={PMLR}
}

@inproceedings{
chen2023sampling,
title={Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions},
author={Sitan Chen and Sinho Chewi and Jerry Li and Yuanzhi Li and Adil Salim and Anru Zhang},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=zyLVMgsZ0U_}
}

@ARTICLE{vincent2011connection,
  author={Vincent, Pascal},
  journal={Neural Computation}, 
  title={A Connection Between Score Matching and Denoising Autoencoders}, 
  year={2011},
  volume={23},
  number={7},
  pages={1661-1674},
  keywords={},
  doi={10.1162/NECO_a_00142}}


@InProceedings{wibisono2024optimal,
  title = 	 {Optimal score estimation via empirical Bayes smoothing},
  author =       {Wibisono, Andre and Wu, Yihong and Yang, Kaylee Yingxi},
  booktitle = 	 {Proceedings of Thirty Seventh Conference on Learning Theory},
  pages = 	 {4958--4991},
  year = 	 {2024},
  editor = 	 {Agrawal, Shipra and Roth, Aaron},
  volume = 	 {247},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {30 Jun--03 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v247/wibisono24a/wibisono24a.pdf},
  url = 	 {https://proceedings.mlr.press/v247/wibisono24a.html},
  abstract = 	 {We study the problem of estimating the score function of an unknown probability distribution $\rho^*$ from $n$ independent and identically distributed observations in $d$ dimensions. Assuming that $\rho^*$ is subgaussian and has a Lipschitz-continuous score function $s^*$, we establish the optimal rate of $\tilde \Theta(n^{-\frac{2}{d+4}})$ for this estimation problem under the loss function $\|\hat s - s^*\|^2_{L^2(\rho^*)}$ that is commonly used in the score matching literature, highlighting the curse of dimensionality where sample complexity for accurate score estimation grows exponentially with the dimension $d$. Leveraging key insights in empirical Bayes theory as well as a new convergence rate of smoothed empirical distribution in Hellinger distance, we show that a regularized score estimator based on a Gaussian kernel attains this rate, shown optimal by a matching minimax lower bound. We also discuss extensions to estimating $\beta$-Hölder continuous scores with $\beta \leq 1$, as well as the implication of our theory on the sample complexity of score-based generative models.}
}


@inproceedings{
zhang2024minimax,
title={Minimax Optimality of Score-based Diffusion Models: Beyond the Density Lower Bound Assumptions},
author={Kaihong Zhang and Heqi Yin and Feng Liang and Jingbo Liu},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=wTd7dogTsB}
}

@article{block2020generative,
  title={Generative modeling with denoising auto-encoders and Langevin sampling},
  author={Block, Adam and Mroueh, Youssef and Rakhlin, Alexander},
  journal={arXiv preprint arXiv:2002.00107},
  year={2020}
}

@article{chen2024learning,
  title={Learning general gaussian mixtures with efficient score matching},
  author={Chen, Sitan and Kontonis, Vasilis and Shah, Kulin},
  journal={arXiv preprint arXiv:2404.18893},
  year={2024}
}

@article{gatmiry2024learning,
  title={Learning mixtures of gaussians using diffusion models},
  author={Gatmiry, Khashayar and Kelner, Jonathan and Lee, Holden},
  journal={arXiv preprint arXiv:2404.18869},
  year={2024}
}

@inproceedings{
shah2023learning,
title={Learning Mixtures of Gaussians Using the {DDPM} Objective},
author={Kulin Shah and Sitan Chen and Adam Klivans},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=aig7sgdRfI}
}



@article{
tenenbaum2000global,
author = {Joshua B. Tenenbaum  and Vin de Silva  and John C. Langford },
title = {A Global Geometric Framework for Nonlinear Dimensionality Reduction},
journal = {Science},
volume = {290},
number = {5500},
pages = {2319-2323},
year = {2000},
doi = {10.1126/science.290.5500.2319},
URL = {https://www.science.org/doi/abs/10.1126/science.290.5500.2319},
eprint = {https://www.science.org/doi/pdf/10.1126/science.290.5500.2319},
abstract = {Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem of dimensionality reduction: finding meaningful low-dimensional structures hidden in their high-dimensional observations. The human brain confronts the same problem in everyday perception, extracting from its high-dimensional sensory inputs—30,000 auditory nerve fibers or 106 optic nerve fibers—a manageably small number of perceptually relevant features. Here we describe an approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set. Unlike classical techniques such as principal component analysis (PCA) and multidimensional scaling (MDS), our approach is capable of discovering the nonlinear degrees of freedom that underlie complex natural observations, such as human handwriting or images of a face under different viewing conditions. In contrast to previous algorithms for nonlinear dimensionality reduction, ours efficiently computes a globally optimal solution, and, for an important class of data manifolds, is guaranteed to converge asymptotically to the true structure.}}


@inproceedings{
stanczuk2024diffusion,
title={Diffusion Models Encode the Intrinsic Dimension of Data Manifolds},
author={Jan Pawel Stanczuk and Georgios Batzolis and Teo Deveney and Carola-Bibiane Sch{\"o}nlieb},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=a0XiA6v256}
}

@article{fefferman2016testing,
  title={Testing the manifold hypothesis},
  author={Fefferman, Charles and Mitter, Sanjoy and Narayanan, Hariharan},
  journal={Journal of the American Mathematical Society},
  volume={29},
  number={4},
  pages={983--1049},
  year={2016}
}

@article{biroli2024dynamical,
  title={Dynamical regimes of diffusion models},
  author={Biroli, Giulio and Bonnaire, Tony and De Bortoli, Valentin and M{\'e}zard, Marc},
  journal={Nature Communications},
  volume={15},
  number={1},
  pages={9957},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@article{ho2020denoising,
  title={Denoising diffusion probabilistic models},
  author={Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={6840--6851},
  year={2020}
}

@inproceedings{sohl2015deep,
  title={Deep unsupervised learning using nonequilibrium thermodynamics},
  author={Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
  booktitle={International conference on machine learning},
  pages={2256--2265},
  year={2015},
  organization={PMLR}
}

@article{song2019generative,
  title={Generative modeling by estimating gradients of the data distribution},
  author={Song, Yang and Ermon, Stefano},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{albergo2023stochastic,
  title={Stochastic interpolants: A unifying framework for flows and diffusions},
  author={Albergo, Michael S and Boffi, Nicholas M and Vanden-Eijnden, Eric},
  journal={arXiv preprint arXiv:2303.08797},
  year={2023}
}

@inproceedings{
lipman2023flow,
title={Flow Matching for Generative Modeling},
author={Yaron Lipman and Ricky T. Q. Chen and Heli Ben-Hamu and Maximilian Nickel and Matthew Le},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=PqvMRDCJT9t}
}

@inproceedings{
liu2023flow,
title={Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow},
author={Xingchao Liu and Chengyue Gong and qiang liu},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=XVjTT1nw5z}
}

@article{wang2023diffusion,
  title={Diffusion models generate images like painters: an analytical theory of outline first, details later},
  author={Wang, Binxu and Vastola, John J},
  journal={arXiv preprint arXiv:2303.02490},
  year={2023}
}

@article{wang2023hidden,
  title={The Hidden Linear Structure in Score-Based Models and its Application},
  author={Wang, Binxu and Vastola, John J},
  journal={arXiv preprint arXiv:2311.10892},
  year={2023}
}

@inproceedings{carlini2023extracting,
author = {Carlini, Nicholas and Hayes, Jamie and Nasr, Milad and Jagielski, Matthew and Sehwag, Vikash and Tram\`{e}r, Florian and Balle, Borja and Ippolito, Daphne and Wallace, Eric},
title = {Extracting training data from diffusion models},
year = {2023},
isbn = {978-1-939133-37-3},
publisher = {USENIX Association},
address = {USA},
abstract = {Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have attracted significant attention due to their ability to generate high-quality synthetic images. In this work, we show that diffusion models memorize individual images from their training data and emit them at generation time. With a generate-and-filter pipeline, we extract over a thousand training examples from state-of-the-art models, ranging from photographs of individual people to trademarked company logos. We also train hundreds of diffusion models in various settings to analyze how different modeling and data decisions affect privacy. Overall, our results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training.},
booktitle = {Proceedings of the 32nd USENIX Conference on Security Symposium},
articleno = {294},
numpages = {18},
location = {Anaheim, CA, USA},
series = {SEC '23}
}

@article{vdb2021diffusion,
  title={Diffusion schr{\"o}dinger bridge with applications to score-based generative modeling},
  author={De Bortoli, Valentin and Thornton, James and Heng, Jeremy and Doucet, Arnaud},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={17695--17709},
  year={2021}
}

@article{huang2024convergence,
  title={Convergence analysis of probability flow ODE for score-based generative models},
  author={Huang, Daniel Zhengyu and Huang, Jiaoyang and Lin, Zhengjiang},
  journal={arXiv preprint arXiv:2404.09730},
  year={2024}
}

@article{xu2019frequency,
  title={Frequency principle: Fourier analysis sheds light on deep neural networks},
  author={Xu, Zhi-Qin John and Zhang, Yaoyu and Luo, Tao and Xiao, Yanyang and Ma, Zheng},
  journal={arXiv preprint arXiv:1901.06523},
  year={2019}
}


@InProceedings{rahaman2019spectral,
  title = 	 {On the Spectral Bias of Neural Networks},
  author =       {Rahaman, Nasim and Baratin, Aristide and Arpit, Devansh and Draxler, Felix and Lin, Min and Hamprecht, Fred and Bengio, Yoshua and Courville, Aaron},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {5301--5310},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/rahaman19a/rahaman19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/rahaman19a.html},
  abstract = 	 {Neural networks are known to be a class of highly expressive functions able to fit even random input-output mappings with 100% accuracy. In this work we present properties of neural networks that complement this aspect of expressivity. By using tools from Fourier analysis, we highlight a learning bias of deep networks towards low frequency functions – i.e. functions that vary globally without local fluctuations – which manifests itself as a frequency-dependent learning speed. Intuitively, this property is in line with the observation that over-parameterized networks prioritize learning simple patterns that generalize across data samples. We also investigate the role of the shape of the data manifold by presenting empirical and theoretical evidence that, somewhat counter-intuitively, learning higher frequencies gets easier with increasing manifold complexity.}
}

@inproceedings{tirer2022kernel,
  title={Kernel-based smoothness analysis of residual networks},
  author={Tirer, Tom and Bruna, Joan and Giryes, Raja},
  booktitle={Mathematical and Scientific Machine Learning},
  pages={921--954},
  year={2022},
  organization={PMLR}
}

@article{potaptchik2024linear,
  title={Linear convergence of diffusion models under the manifold hypothesis},
  author={Potaptchik, Peter and Azangulov, Iskander and Deligiannidis, George},
  journal={arXiv preprint arXiv:2410.09046},
  year={2024}
}

@article{azangulov2024convergence,
  title={Convergence of diffusion models under the manifold hypothesis in high-dimensions},
  author={Azangulov, Iskander and Deligiannidis, George and Rousseau, Judith},
  journal={arXiv preprint arXiv:2409.18804},
  year={2024}
}

@inproceedings{
boffi2024shallow,
title={Shallow diffusion networks provably learn hidden low-dimensional structure},
author={Nicholas Matthew Boffi and Arthur Jacot and Stephen Tu and Ingvar Ziemann},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=KlxK4ncqWZ}
}

@inproceedings{
cole2024scorebased,
title={Score-based generative models break the curse of dimensionality in learning a family of sub-Gaussian distributions},
author={Frank Cole and Yulong Lu},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=wG12xUSqrI}
}

@article{ross2024geometric,
  title={A geometric framework for understanding memorization in generative models},
  author={Ross, Brendan Leigh and Kamkari, Hamidreza and Wu, Tongzi and Hosseinzadeh, Rasa and Liu, Zhaoyan and Stein, George and Cresswell, Jesse C and Loaiza-Ganem, Gabriel},
  journal={arXiv preprint arXiv:2411.00113},
  year={2024}
}

@inproceedings{
kamkari2024a,
title={A Geometric View of Data Complexity: Efficient Local Intrinsic Dimension Estimation with Diffusion Models},
author={Hamidreza Kamkari and Brendan Leigh Ross and Rasa Hosseinzadeh and Jesse C. Cresswell and Gabriel Loaiza-Ganem},
booktitle={ICML 2024 Workshop on Structured Probabilistic Inference {\&} Generative Modeling},
year={2024},
url={https://openreview.net/forum?id=wc044k7QBj}
}

@article{peyre2009manifold,
  title={Manifold models for signals and images},
  author={Peyr{\'e}, Gabriel},
  journal={Computer vision and image understanding},
  volume={113},
  number={2},
  pages={249--260},
  year={2009},
  publisher={Elsevier}
}

@article{cui2025precise,
  title={A precise asymptotic analysis of learning diffusion models: theory and insights},
  author={Cui, Hugo and Pehlevan, Cengiz and Lu, Yue M},
  journal={arXiv preprint arXiv:2501.03937},
  year={2025}
}

@article{wang2024subspace,
  title={Diffusion models learn low-dimensional distributions via subspace clustering},
  author={Wang, Peng and Zhang, Huijie and Zhang, Zekai and Chen, Siyi and Ma, Yi and Qu, Qing},
  journal={arXiv preprint arXiv:2409.02426},
  year={2024}
}

@article{achilli2024losing,
  title={Losing dimensions: Geometric memorization in generative diffusion},
  author={Achilli, Beatrice and Ventura, Enrico and Silvestri, Gianluigi and Pham, Bao and Raya, Gabriel and Krotov, Dmitry and Lucibello, Carlo and Ambrogioni, Luca},
  journal={arXiv preprint arXiv:2410.08727},
  year={2024}
}

@article{
wang2024unreasonable,
title={The Unreasonable Effectiveness of Gaussian Score Approximation for Diffusion Models and its Applications},
author={Binxu Wang and John Vastola},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2024},
url={https://openreview.net/forum?id=I0uknSHM2j},
note={}
}

@inproceedings{
li2024understanding,
title={Understanding Generalizability of Diffusion Models Requires Rethinking the Hidden Gaussian Structure},
author={Xiang Li and Yixiang Dai and Qing Qu},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=Sk2duBGvrK}
}

@article{kamb2024analytic,
  title={An analytic theory of creativity in convolutional diffusion models},
  author={Kamb, Mason and Ganguli, Surya},
  journal={arXiv preprint arXiv:2412.20292},
  year={2024}
}

@article{gao2024flow,
  title={How Do Flow Matching Models Memorize and Generalize in Sample Data Subspaces?},
  author={Gao, Weiguo and Li, Ming},
  journal={arXiv preprint arXiv:2410.23594},
  year={2024}
}

@article{huang2024denoising,
  title={Denoising diffusion probabilistic models are optimally adaptive to unknown low dimensionality},
  author={Huang, Zhihan and Wei, Yuting and Chen, Yuxin},
  journal={arXiv preprint arXiv:2410.18784},
  year={2024}
}

@article{li2024sharp,
  title={A sharp convergence theory for the probability flow odes of diffusion models},
  author={Li, Gen and Wei, Yuting and Chi, Yuejie and Chen, Yuxin},
  journal={arXiv preprint arXiv:2408.02320},
  year={2024}
}

@article{ventura2024manifolds,
  title={Manifolds, random matrices and spectral gaps: The geometric phases of generative diffusion},
  author={Ventura, Enrico and Achilli, Beatrice and Silvestri, Gianluigi and Lucibello, Carlo and Ambrogioni, Luca},
  journal={arXiv preprint arXiv:2410.05898},
  year={2024}
}

@article{baptista2025memorization,
  title={Memorization and Regularization in Generative Diffusion Models},
  author={Baptista, Ricardo and Dasgupta, Agnimitra and Kovachki, Nikola B and Oberai, Assad and Stuart, Andrew M},
  journal={arXiv preprint arXiv:2501.15785},
  year={2025}
}


@InProceedings{savarese2019infinite,
  title = 	 {How do infinite width bounded norm networks look in function space?},
  author =       {Savarese, Pedro and Evron, Itay and Soudry, Daniel and Srebro, Nathan},
  booktitle = 	 {Proceedings of the Thirty-Second Conference on Learning Theory},
  pages = 	 {2667--2690},
  year = 	 {2019},
  editor = 	 {Beygelzimer, Alina and Hsu, Daniel},
  volume = 	 {99},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {25--28 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v99/savarese19a/savarese19a.pdf},
  url = 	 {https://proceedings.mlr.press/v99/savarese19a.html},
  abstract = 	 {We consider the question of what functions can be captured by ReLU networks with an unbounded number of units (infinite width), but where the overall network Euclidean norm (sum of squares of all weights in the system, except for an unregularized bias term for each unit) is bounded; or equivalently what is the minimal norm required to approximate a given function. For functions $f:\mathbb R \rightarrow\mathbb R$ and a single hidden layer, we show that the minimal network norm for representing $f$ is $\max(\int \lvert f”(x) \rvert \mathrm{d} x, \lvert  f’(-\infty) + f’(+\infty) \rvert)$, and hence the minimal norm fit for a sample is given by a linear spline interpolation.  }
}

@article{abramson2024accurate,
  title={Accurate structure prediction of biomolecular interactions with AlphaFold 3},
  author={Abramson, Josh and Adler, Jonas and Dunger, Jack and Evans, Richard and Green, Tim and Pritzel, Alexander and Ronneberger, Olaf and Willmore, Lindsay and Ballard, Andrew J and Bambrick, Joshua and others},
  journal={Nature},
  pages={1--3},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{perez2018film,
  title={Film: Visual reasoning with a general conditioning layer},
  author={Perez, Ethan and Strub, Florian and De Vries, Harm and Dumoulin, Vincent and Courville, Aaron},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={32},
  number={1},
  year={2018}
}

@inproceedings{peebles2023scalable,
  title={Scalable diffusion models with transformers},
  author={Peebles, William and Xie, Saining},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={4195--4205},
  year={2023}
}

@inproceedings{
loshchilov2018decoupled,
title={Decoupled Weight Decay Regularization},
author={Ilya Loshchilov and Frank Hutter},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=Bkg6RiCqY7},
}

@inproceedings{kingma2015adam,
  author       = {Diederik P. Kingma and
                  Jimmy Ba},
  editor       = {Yoshua Bengio and
                  Yann LeCun},
  title        = {Adam: {A} Method for Stochastic Optimization},
  booktitle    = {3rd International Conference on Learning Representations, {ICLR} 2015,
                  San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year         = {2015},
  url          = {http://arxiv.org/abs/1412.6980},
  timestamp    = {Thu, 25 Jul 2019 14:25:37 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{karras2022elucidating,
  title={Elucidating the design space of diffusion-based generative models},
  author={Karras, Tero and Aittala, Miika and Aila, Timo and Laine, Samuli},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={26565--26577},
  year={2022}
}