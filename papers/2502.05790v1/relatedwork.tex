\section{Related Work}
\label{sec:related-works}
\paragraph{Memory Efficient Parametrization.}
LoRA \cite{hu2021lora} can be seen as a memory efficient parametrization of weights in LLMs and is widely used in fine-tuning. LoRA's bottleneck lies in its low-rank structure and impedes its expressiveness. COLA \citep{xia2024chain}, Delta-LoRA \citep{zi2023delta}, and PLoRA \citep{meng2024periodiclora} propose to increase the rank and improve the performance of LoRA. ReLoRA \cite{lialin2023relora} and SLTrain \cite{han2024sltrain} extend LoRA to pre-training tasks by merging and resetting adapters, and adopting low-rank plus sparse parameterization, respectively. MoRA \citep{jiang2024mora} alleviate the shortcoming of low-rank disadvantage of LoRA by sharing same trainable parameters to achieve higher-rank update. 



\paragraph{Memory Efficient Optimizer.}
One way to achieve memory-efficient optimization is by using memory-efficient optimizers, which primarily aim to reduce the memory cost of optimizer states in Adam \cite{kingma2014adam}. A series of works \citep{shazeer2018adafactor, zhang2024adam-mini, luo2023came, zhao2024adapprox} factorizes the second moment in Adam. Quantizing optimizer states and storing them in low-precision formats has also proven successful \citep{li2024memory, dettmers20218}. Another line of work focuses on gradient compression methods. GaLore \cite{zhao2024galore} and Q-GaLore \cite{zhang2024q-galore} use SVD to apply dense low-rank projections to gradients. FLora \cite{hao2024flora} and GoLore \cite{he2024subspace} adopt random projection, while Grass \cite{muhamed2024grass} employs sparse low-rank projection to gradients.


\paragraph{Subspace Learning.}
Existing studies provide sophisticated analyses of various subspace learning algorithms \citep{cosson2023low, kozak2019stochastic, jadbabaie2023adaptive}. \cite{gur2018gradient} claim that gradient descent primarily occurs in the dominant subspace, which is spanned by the top eigenvectors of the Hessian. In contrast, \cite{song2024does} argue that, due to noise in SGD, the alignment between the gradient and the dominant subspace is spurious, and learning does not occur in the dominant subspace but rather in its orthogonal complement, i.e., the bulk subspace. Intuitively, our findings align with those of \cite{song2024does}, suggesting that selecting basis vectors based on specific sampling probabilities can enhance the performance of LLMs during pre-training.