@article{cosson2023low,
  title={Low-Rank Gradient Descent},
  author={Cosson, Romain and Jadbabaie, Ali and Makur, Anuran and Reisizadeh, Amirhossein and Shah, Devavrat},
  journal={IEEE Open Journal of Control Systems},
  year={2023},
  publisher={IEEE}
}

@article{dettmers20218,
  title={8-bit optimizers via block-wise quantization},
  author={Dettmers, Tim and Lewis, Mike and Shleifer, Sam and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2110.02861},
  year={2021}
}

@article{gur2018gradient,
  title={Gradient descent happens in a tiny subspace},
  author={Gur-Ari, Guy and Roberts, Daniel A and Dyer, Ethan},
  journal={arXiv preprint arXiv:1812.04754},
  year={2018}
}

@article{han2024sltrain,
  title={SLTrain: a sparse plus low-rank approach for parameter and memory efficient pretraining},
  author={Han, Andi and Li, Jiaxiang and Huang, Wei and Hong, Mingyi and Takeda, Akiko and Jawanpuria, Pratik and Mishra, Bamdev},
  journal={arXiv preprint arXiv:2406.02214},
  year={2024}
}

@article{hao2024flora,
  title={Flora: Low-Rank Adapters Are Secretly Gradient Compressors},
  author={Hao, Yongchang and Cao, Yanshuai and Mou, Lili},
  journal={arXiv preprint arXiv:2402.03293},
  year={2024}
}

@article{he2024subspace,
  title={Subspace Optimization for Large Language Models with Convergence Guarantees},
  author={He, Yutong and Li, Pengrui and Hu, Yipeng and Chen, Chuyan and Yuan, Kun},
  journal={arXiv preprint arXiv:2410.11289},
  year={2024}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@inproceedings{jadbabaie2023adaptive,
  title={Adaptive Low-Rank Gradient Descent},
  author={Jadbabaie, Ali and Makur, Anuran and Reisizadeh, Amirhossein},
  booktitle={2023 62nd IEEE Conference on Decision and Control (CDC)},
  pages={3315--3320},
  year={2023},
  organization={IEEE}
}

@article{jiang2024mora,
  title={MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning},
  author={Jiang, Ting and Huang, Shaohan and Luo, Shengyue and Zhang, Zihan and Huang, Haizhen and Wei, Furu and Deng, Weiwei and Sun, Feng and Zhang, Qi and Wang, Deqing and others},
  journal={arXiv preprint arXiv:2405.12130},
  year={2024}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{kozak2019stochastic,
  title={Stochastic subspace descent},
  author={Kozak, David and Becker, Stephen and Doostan, Alireza and Tenorio, Luis},
  journal={arXiv preprint arXiv:1904.01145},
  year={2019}
}

@article{li2024memory,
  title={Memory efficient optimizers with 4-bit states},
  author={Li, Bingrui and Chen, Jianfei and Zhu, Jun},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{lialin2023relora,
  title={Relora: High-rank training through low-rank updates},
  author={Lialin, Vladislav and Muckatira, Sherin and Shivagunde, Namrata and Rumshisky, Anna},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@article{luo2023came,
  title={Came: Confidence-guided adaptive memory efficient optimization},
  author={Luo, Yang and Ren, Xiaozhe and Zheng, Zangwei and Jiang, Zhuo and Jiang, Xin and You, Yang},
  journal={arXiv preprint arXiv:2307.02047},
  year={2023}
}

@article{meng2024periodiclora,
  title={Periodiclora: Breaking the low-rank bottleneck in lora optimization},
  author={Meng, Xiangdi and Dai, Damai and Luo, Weiyao and Yang, Zhe and Wu, Shaoxiang and Wang, Xiaochen and Wang, Peiyi and Dong, Qingxiu and Chen, Liang and Sui, Zhifang},
  journal={arXiv preprint arXiv:2402.16141},
  year={2024}
}

@article{muhamed2024grass,
  title={Grass: Compute efficient low-memory llm training with structured sparse gradients},
  author={Muhamed, Aashiq and Li, Oscar and Woodruff, David and Diab, Mona and Smith, Virginia},
  journal={arXiv preprint arXiv:2406.17660},
  year={2024}
}

@inproceedings{shazeer2018adafactor,
  title={Adafactor: Adaptive learning rates with sublinear memory cost},
  author={Shazeer, Noam and Stern, Mitchell},
  booktitle={International Conference on Machine Learning},
  pages={4596--4604},
  year={2018},
  organization={PMLR}
}

@article{song2024does,
  title={Does SGD really happen in tiny subspaces?},
  author={Song, Minhak and Ahn, Kwangjun and Yun, Chulhee},
  journal={arXiv preprint arXiv:2405.16002},
  year={2024}
}

@article{xia2024chain,
  title={Chain of lora: Efficient fine-tuning of language models via residual learning},
  author={Xia, Wenhan and Qin, Chengwei and Hazan, Elad},
  journal={arXiv preprint arXiv:2401.04151},
  year={2024}
}

@article{zhang2024adam-mini,
  title={Adam-mini: Use fewer learning rates to gain more},
  author={Zhang, Yushun and Chen, Congliang and Li, Ziniu and Ding, Tian and Wu, Chenwei and Ye, Yinyu and Luo, Zhi-Quan and Sun, Ruoyu},
  journal={arXiv preprint arXiv:2406.16793},
  year={2024}
}

@article{zhang2024q-galore,
  title={Q-galore: Quantized galore with int4 projection and layer-adaptive low-rank gradients},
  author={Zhang, Zhenyu and Jaiswal, Ajay and Yin, Lu and Liu, Shiwei and Zhao, Jiawei and Tian, Yuandong and Wang, Zhangyang},
  journal={arXiv preprint arXiv:2407.08296},
  year={2024}
}

@article{zhao2024adapprox,
  title={Adapprox: Adaptive Approximation in Adam Optimization via Randomized Low-Rank Matrices},
  author={Zhao, Pengxiang and Li, Ping and Gu, Yingjie and Zheng, Yi and K{\"o}lker, Stephan Ludger and Wang, Zhefeng and Yuan, Xiaoming},
  journal={arXiv preprint arXiv:2403.14958},
  year={2024}
}

@article{zhao2024galore,
  title={Galore: Memory-efficient llm training by gradient low-rank projection},
  author={Zhao, Jiawei and Zhang, Zhenyu and Chen, Beidi and Wang, Zhangyang and Anandkumar, Anima and Tian, Yuandong},
  journal={arXiv preprint arXiv:2403.03507},
  year={2024}
}

@article{zi2023delta,
  title={Delta-lora: Fine-tuning high-rank parameters with the delta of low-rank matrices},
  author={Zi, Bojia and Qi, Xianbiao and Wang, Lingzhi and Wang, Jianan and Wong, Kam-Fai and Zhang, Lei},
  journal={arXiv preprint arXiv:2309.02411},
  year={2023}
}

