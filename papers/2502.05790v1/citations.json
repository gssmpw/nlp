[
  {
    "index": 0,
    "papers": [
      {
        "key": "hu2021lora",
        "author": "Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu",
        "title": "Lora: Low-rank adaptation of large language models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "xia2024chain",
        "author": "Xia, Wenhan and Qin, Chengwei and Hazan, Elad",
        "title": "Chain of lora: Efficient fine-tuning of language models via residual learning"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "zi2023delta",
        "author": "Zi, Bojia and Qi, Xianbiao and Wang, Lingzhi and Wang, Jianan and Wong, Kam-Fai and Zhang, Lei",
        "title": "Delta-lora: Fine-tuning high-rank parameters with the delta of low-rank matrices"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "meng2024periodiclora",
        "author": "Meng, Xiangdi and Dai, Damai and Luo, Weiyao and Yang, Zhe and Wu, Shaoxiang and Wang, Xiaochen and Wang, Peiyi and Dong, Qingxiu and Chen, Liang and Sui, Zhifang",
        "title": "Periodiclora: Breaking the low-rank bottleneck in lora optimization"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "lialin2023relora",
        "author": "Lialin, Vladislav and Muckatira, Sherin and Shivagunde, Namrata and Rumshisky, Anna",
        "title": "Relora: High-rank training through low-rank updates"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "han2024sltrain",
        "author": "Han, Andi and Li, Jiaxiang and Huang, Wei and Hong, Mingyi and Takeda, Akiko and Jawanpuria, Pratik and Mishra, Bamdev",
        "title": "SLTrain: a sparse plus low-rank approach for parameter and memory efficient pretraining"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "jiang2024mora",
        "author": "Jiang, Ting and Huang, Shaohan and Luo, Shengyue and Zhang, Zihan and Huang, Haizhen and Wei, Furu and Deng, Weiwei and Sun, Feng and Zhang, Qi and Wang, Deqing and others",
        "title": "MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "kingma2014adam",
        "author": "Kingma, Diederik P",
        "title": "Adam: A method for stochastic optimization"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "shazeer2018adafactor",
        "author": "Shazeer, Noam and Stern, Mitchell",
        "title": "Adafactor: Adaptive learning rates with sublinear memory cost"
      },
      {
        "key": "zhang2024adam-mini",
        "author": "Zhang, Yushun and Chen, Congliang and Li, Ziniu and Ding, Tian and Wu, Chenwei and Ye, Yinyu and Luo, Zhi-Quan and Sun, Ruoyu",
        "title": "Adam-mini: Use fewer learning rates to gain more"
      },
      {
        "key": "luo2023came",
        "author": "Luo, Yang and Ren, Xiaozhe and Zheng, Zangwei and Jiang, Zhuo and Jiang, Xin and You, Yang",
        "title": "Came: Confidence-guided adaptive memory efficient optimization"
      },
      {
        "key": "zhao2024adapprox",
        "author": "Zhao, Pengxiang and Li, Ping and Gu, Yingjie and Zheng, Yi and K{\\\"o}lker, Stephan Ludger and Wang, Zhefeng and Yuan, Xiaoming",
        "title": "Adapprox: Adaptive Approximation in Adam Optimization via Randomized Low-Rank Matrices"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "li2024memory",
        "author": "Li, Bingrui and Chen, Jianfei and Zhu, Jun",
        "title": "Memory efficient optimizers with 4-bit states"
      },
      {
        "key": "dettmers20218",
        "author": "Dettmers, Tim and Lewis, Mike and Shleifer, Sam and Zettlemoyer, Luke",
        "title": "8-bit optimizers via block-wise quantization"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "zhao2024galore",
        "author": "Zhao, Jiawei and Zhang, Zhenyu and Chen, Beidi and Wang, Zhangyang and Anandkumar, Anima and Tian, Yuandong",
        "title": "Galore: Memory-efficient llm training by gradient low-rank projection"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "zhang2024q-galore",
        "author": "Zhang, Zhenyu and Jaiswal, Ajay and Yin, Lu and Liu, Shiwei and Zhao, Jiawei and Tian, Yuandong and Wang, Zhangyang",
        "title": "Q-galore: Quantized galore with int4 projection and layer-adaptive low-rank gradients"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "hao2024flora",
        "author": "Hao, Yongchang and Cao, Yanshuai and Mou, Lili",
        "title": "Flora: Low-Rank Adapters Are Secretly Gradient Compressors"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "he2024subspace",
        "author": "He, Yutong and Li, Pengrui and Hu, Yipeng and Chen, Chuyan and Yuan, Kun",
        "title": "Subspace Optimization for Large Language Models with Convergence Guarantees"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "muhamed2024grass",
        "author": "Muhamed, Aashiq and Li, Oscar and Woodruff, David and Diab, Mona and Smith, Virginia",
        "title": "Grass: Compute efficient low-memory llm training with structured sparse gradients"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "cosson2023low",
        "author": "Cosson, Romain and Jadbabaie, Ali and Makur, Anuran and Reisizadeh, Amirhossein and Shah, Devavrat",
        "title": "Low-Rank Gradient Descent"
      },
      {
        "key": "kozak2019stochastic",
        "author": "Kozak, David and Becker, Stephen and Doostan, Alireza and Tenorio, Luis",
        "title": "Stochastic subspace descent"
      },
      {
        "key": "jadbabaie2023adaptive",
        "author": "Jadbabaie, Ali and Makur, Anuran and Reisizadeh, Amirhossein",
        "title": "Adaptive Low-Rank Gradient Descent"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "gur2018gradient",
        "author": "Gur-Ari, Guy and Roberts, Daniel A and Dyer, Ethan",
        "title": "Gradient descent happens in a tiny subspace"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "song2024does",
        "author": "Song, Minhak and Ahn, Kwangjun and Yun, Chulhee",
        "title": "Does SGD really happen in tiny subspaces?"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "song2024does",
        "author": "Song, Minhak and Ahn, Kwangjun and Yun, Chulhee",
        "title": "Does SGD really happen in tiny subspaces?"
      }
    ]
  }
]