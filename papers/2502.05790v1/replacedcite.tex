\section{Related Work}
\label{sec:related-works}
\paragraph{Memory Efficient Parametrization.}
LoRA ____ can be seen as a memory efficient parametrization of weights in LLMs and is widely used in fine-tuning. LoRA's bottleneck lies in its low-rank structure and impedes its expressiveness. COLA ____, Delta-LoRA ____, and PLoRA ____ propose to increase the rank and improve the performance of LoRA. ReLoRA ____ and SLTrain ____ extend LoRA to pre-training tasks by merging and resetting adapters, and adopting low-rank plus sparse parameterization, respectively. MoRA ____ alleviate the shortcoming of low-rank disadvantage of LoRA by sharing same trainable parameters to achieve higher-rank update. 



\paragraph{Memory Efficient Optimizer.}
One way to achieve memory-efficient optimization is by using memory-efficient optimizers, which primarily aim to reduce the memory cost of optimizer states in Adam ____. A series of works ____ factorizes the second moment in Adam. Quantizing optimizer states and storing them in low-precision formats has also proven successful ____. Another line of work focuses on gradient compression methods. GaLore ____ and Q-GaLore ____ use SVD to apply dense low-rank projections to gradients. FLora ____ and GoLore ____ adopt random projection, while Grass ____ employs sparse low-rank projection to gradients.


\paragraph{Subspace Learning.}
Existing studies provide sophisticated analyses of various subspace learning algorithms ____. ____ claim that gradient descent primarily occurs in the dominant subspace, which is spanned by the top eigenvectors of the Hessian. In contrast, ____ argue that, due to noise in SGD, the alignment between the gradient and the dominant subspace is spurious, and learning does not occur in the dominant subspace but rather in its orthogonal complement, i.e., the bulk subspace. Intuitively, our findings align with those of ____, suggesting that selecting basis vectors based on specific sampling probabilities can enhance the performance of LLMs during pre-training.