\section{Experiments\label{sec:experiments}}
\subsection{Experiment Setting}
\paragraph{Pre-training on C4 Dataset.}
C4 \cite{raffel2020c4}, short for Colossal Clean Crawled Corpus, is a large-scale and open-source text data that are widely used in practice for pre-training transformer models, e.g., BERT \cite{portes2023mosaicbert}, T5 \cite{xue2020mt5}, and GPT-models. C4 is also widely used in memory efficient optimization community to evaluate the performance of memory-efficient optimizer \cite{hao2024flora, zhao2024galore, zhang2024q-galore, he2024subspace}. In our experiment, we pretrain LLaMA models with different sizes on C4 dataset without data repetition over a sufficient amount of data \cite{hoffmann2022scalinglaw}. 

\paragraph{Architecture and Hyperparameters} 
We evaluate different optimizers' performance on Llama with 60 million, 130 million, 350 million, and 1.1 billion parameters. We adopt the same architecture as in \cite{zhao2024galore}. For experiment with GaLore-Adam, Fira-Adam, and GoLore, we adopt the same hyperparameters as provided in their official codebase. For full-rank Adam, we adopt $\beta_1=0.9$, $\beta_2=0.999$, $lr=0.001$ except for LLaMA-60M model, whose learning rate is set to be 0.0025. 

\begin{table}[!ht]
    \centering
    \caption{Comparison with different version of Adam on pre-training LLaMA models with 60M, 130M, and 350M parameters on C4 dataset. Validation perplexity is reported. 
    }
    \label{tab:lora_compare_llama}
    \begin{tabular}{l p{1cm} p{1cm} p{1cm}}
    \toprule
               & \textbf{60M} & \textbf{130M} & \textbf{350M} \\
    \midrule
    Full-Rank Adam & 27.71 & 23.27 & 18.21 \\
    \midrule
    GaLore-I3S-Adam & \textbf{30.47} & \textbf{24.21} & \textbf{19.16} \\
    GaLore-Adam & 31.50 & 24.88 & 19.68 \\
    PPL gap reduction & 27.17\% & 41.61\% & 35.37\%\\
    \midrule
    Fira-I3S-Adam & \textbf{28.12} & \textbf{22.22} & \textbf{17.25} \\
    Fira-Adam & 28.42 & 22.37 & 17.35 \\
    PPL gap reduction & 42.25\% & --- & ---\\
    \midrule
    GaLore-I3S-Adafactor & \textbf{30.06} & \textbf{24.09} & \textbf{18.88} \\
    GaLore-Adafactor & 31.13 & 24.79 & 19.45 \\
    PPL gap reduction & 31.28\% & 46.05\% & 45.96\%\\
    \midrule
    GaLore-I3S-Adam-mini & \textbf{31.66} & \textbf{24.87} & \textbf{19.41} \\
    GaLore-Adam-mini & 32.08 & 25.46 & 19.89 \\
    PPL gap reduction & 9.61\% & 26.94\% & 28.57\%\\
    \midrule
    GaLore-I3S-Adam (8bit) & \textbf{30.55} & \textbf{24.67} & \textbf{18.16} \\
    GaLore-Adam (8bit) & 31.62 & 25.35 & 18.63 \\
    PPL gap reduction & 27.36\% & 32.69\% &---\\
    \bottomrule
    $r / d_{model}$ & 128/256 & 256/768 & 256/1024 \\
    Tokens & 1.5B & 2.2B & 6B \\ %
    \bottomrule
    \end{tabular}
\end{table}

\subsection{Efficacy of I3S with different low-rank Adam optimizers}
First, we evaluate the efficacy of I3S when combined with various low-rank Adam optimizers. Table~\ref{tab:lora_compare_llama} demonstrates that I3S consistently outperforms the selection of the dominant subspace. In cases where full-rank Adam achieves the lowest PPL, we also report the percentage reduction in the PPL gap achieved by I3S compared to using the dominant subspace. As shown in Table~\ref{tab:lora_compare_llama}, I3S reduces the PPL gap by up to 46.05\%. In scenarios where full-rank Adam does not achieve the lowest PPL, we observe that I3S still improves PPL compared to selecting leading singular vectors. I3S is effective not only with low-rank variants of Adam, such as GaLore-Adam and Fira-Adam, but also with low-rank optimizers that approximate second moments, e.g., GaLore-Adafactor and GaLore-Adam-mini. Results with the 8-bit optimizer highlight the robustness of I3S against low-precision optimizer state storage.



\subsection{Scale Up to Llama-1.1B}
\begin{table}[!ht]
    \centering
    \caption{Comparison among full-rank Adam, GaLore-Adam, and Galore-I3S-Adam on pre-training LLaMA-1.1B on C4 dataset. Validation perplexity is reported.}
    \label{tab:1.1b-llama}

    \begin{tabular}{p{1cm}p{1cm}p{3cm}p{1.6cm}}
    \toprule
               &  Full &  GaLore-I3S-Adam & GaLore-Adam \tabularnewline
    \midrule
    1.1B & 15.90 & \textbf{15.36} & 15.47 \\
    $r / d_{model}$ & 512/2048 & 512/2048 & 512/2048 \\
    Tokens & 13.4B & 13.4B & 13.4B \\ %
    \bottomrule
    \end{tabular}
\end{table}



\begin{figure}[!ht]
    \centering
    \includegraphics[width=\linewidth]{ppl-gap.pdf}
    \caption{Perplexity gap between GaLore-Adam and GaLore-I3S-Adam during pretaining of LLaMA-1.1B on C4 dataset. PPL gap larger than 0.0 means GaLore-I3S-Adam has a lower PPL than GaLore-Adam.}
    \label{fig:ppl-gap}
\end{figure} 

We also verify the efficacy of I3S on LLaMA-1.1B's pretraining. Due to limited computational resources, we test it only with GaLore-Adam. Table~\ref{tab:1.1b-llama} shows that I3S remains effective on LLaMA-1.1B. Figure~\ref{fig:ppl-gap} illustrates that during the early stage of pretraining, using the dominant subspace performs better than I3S. However, as pretraining progresses, I3S demonstrates its superiority over the dominant subspace. This phenomenon aligns with the insight provided by \cite{he2024subspace}, which suggests that using the dominant subspace during the later stages of pretraining, when noise dominates the gradient, fails to preserve gradient information effectively. While this insight partially explains the advantage of I3S, we provide a more explicit explanation from a new perspective in the next section.


\subsection{I3S Enables Higher-rank Update}

\begin{figure*}[!ht]
    \centering
    \subfloat
    {\includegraphics[width=0.5\textwidth, page=1]{Merged_model.layers.4.mlp.pdf}
    }
    \subfloat
    {
    \includegraphics[width=0.5\textwidth,]{module.model.layers.4.mlp.up_proj.weight.pdf}
    }
    \caption{a). The left figure shows the subspace overlap between adjacent subspaces in GaLore-Adam and GaLore-I3S-Adam during pretraining on the LLaMA-60M model. The definition of subspace overlap is given in Eq.~\eqref{eq:overlap}. b). The right figure shows normalized singular values of the weight difference between the 28k-step checkpoint and 30k-step checkpoint during pretraining on the LLaMA-60M model.}
    \label{fig:ideal-adam-galore}
\end{figure*} 

\cite{zhang2024q-galore} provides an interesting observation that the similarity between adjacent subspaces in some layers gradually becomes very high during pretraining, we observe a similar phenomenon shown in Figure~\ref{fig:q-galore}. In Figure~\ref{fig:ideal-adam-galore}(a), We observe a similar phenomenon herein in GaLore-Adam, i.e., the overlap between adjacent subspaces becomes large after the early stage of pretraining. 

We adopt the metric to measure overlap between two subspaces from \cite{gur2018gradient}. Given two orthonormal matrices $U, V \in \mathbb{R}^{m \times r}$, we have
\begin{align*}
    U^T U=V^T V=I_r,
\end{align*}
the overlap between two subspaces spanned by $U$ and $V$ are defined as
\begin{align}
\label{eq:overlap}
    \text{overlap}(U, V) = \frac{1}{r}\sum_{i=1}^{r} \|U^{T}V_{:,i}\|_2^2, 
\end{align}
where $V_{:,i}$ denotes the $i$-th column of V.

We adopt a different measure herein to show that the observation in \cite{zhang2024q-galore} is not because of the bias of using cosine similarity, but it also exists when using other metrics to measure subspace overlap (or subspace similarity). An interesting fact is that subspace overlap in GaLore-I3S-Adam is much lower, which means GaLore-I3S-Adam tends to explore more different subspaces compared to GaLore-Adam. And as shown in Figure~\ref{fig:ideal-adam-galore}(b), because of this broader exploration to different subspaces, the update in GaLore-I3S-Adam has a slower-decaying singular values than that in GaLore-Adam, this indicates a "higher-rank" update in GaLore-I3S-Adam, and we credit the advantage of I3S over using donminant subspace to it.

\subsection{Ablation Study}
\paragraph{Different Sampling Distribution and Loss Spiking} 

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\linewidth]{loss_spiking.pdf}
    \caption{Training loss of GaLore-Adam with uniform subspace selection and importance subspace selection on LLaMA-130M's pretraining. The curve shown in the figure is representative of three seeds.}
    \label{fig:loss_spiking}
\end{figure} 


Because we mentioned in previous context that introducing some randomness into subsapce selection helps to overcome the low-rank bottleneck of update, it is natural to ask what role does the leading singular vectors play in I3S. To answer this, we compare our proposed I3S and uniform singular vector sampling method. We find that I3S helps to avoid loss spiking problem, as shown in Figure~\ref{fig:loss_spiking}. Singular vectors corresponding to the first few leading singular values are selected with high probability in importance sampling, and they play an important role in stabilizing training process. Uniform sampling adds too much randomness into subspace selection, which has a negative influence on stable training. 

\paragraph{I3S and JL-transform}
\begin{figure}[!ht]
    \centering
    \includegraphics[width=\linewidth]{last_figure.pdf}
    \caption{Evaluation PPL of pretrained LLaMA model with different sizes optimized low-rank Adam with I3S and JL-transform. The numbers of tokens for training are 1.5B, 2.2B, and 6B for 60M, 130M, and 350M model, respectively. Here we run GaLore-I3S-Adam as low-rank Adam with I3S, and GoLore as low-tank Adam with JL-transform.}
    \label{fig:i3s_and_jl}
\end{figure} 
Here, we demonstrate that I3S outperforms the JL-transform in low-rank optimization. Specifically, we compare GaLore-I3S-Adam with GaLore. The only difference between these two methods is that GaLore-I3S-Adam uses I3S to select the low-rank subspace, whereas GaLore applies the JL-transform to gradients to compress optimizer states. In Section~\ref{sec:3.3}, we have discussed the difference in convergence rates from a theoretical perspective. Figure~\ref{fig:i3s_and_jl} illustrates that I3S achieves significantly lower PPL compared to the JL-transform on pretraining tasks.
 
