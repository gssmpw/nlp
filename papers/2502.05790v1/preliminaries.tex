\section{Preliminaries}
\label{sec:preliminaries}
In this section, we present the background required for our theoretical analysis and experiments. In our experiments (Section~\ref{sec:experiments}), we apply I3S to two low-rank optimization methods, GaLore and Fira, both of which can be combined with stateful optimizers (e.g., Adam, Adafactor, and Adam-mini). 

To ensure clarity, the update rules for GaLore-Adam and Fira-Adam are briefly explained here. For more detailed explanations, please refer to the original papers \cite{zhao2024galore, chen2024fira}. In presenting these methods, we show the update rules for the weights of a single layer in the neural network. We assume that the gradient at the $t$-th iteration is a matrix $G^{(t)} \in \mathbb{R}^{m \times n}$. Without loss of generality, we assume that $m < n$ and use $r$ to represent the rank of the low-rank subspace.

\subsection{Update Rules of GaLore-Adam}\label{sub:preliminaries:galore}

GaLore-Adam \cite{zhao2024galore} requires storing an orthogonal matrix $P^{(t)} \in \mathbb{R}^{m \times r}$ that satisfies $(P^{(t)})^TP^{(t)} = I_r$, which is updated after a certain number of iterations. Similar to full-rank Adam, GaLore-Adam also requires storing the first moment $M^{(t)} \in \mathbb{R}^{r \times n}$ and the second moment $V^{(t)} \in \mathbb{R}^{r \times n}$ for each layer's weights, and updating the weights $W^{(t)}$:
\begin{align}
    R^{(t)} &= (P^{(t)})^T G^{(t)} \notag\\
    M^{(t)} &= \beta_1 M^{(t-1)} + (1-\beta_1) R^{(t)} \notag\\
    V^{(t)} &= \beta_2 V^{(t-1)} + (1-\beta_2) R^{(t)} \circ R^{(t)} \notag\\
    N^{(t)} &= \alpha P^{(t)} \frac{M^{(t)}}{\sqrt{V^{(t)}} + \xi} \label{eq:N}\\
    x^{(t)} &= x^{(t-1)} - \eta \cdot N^{(t)}. \notag
\end{align}
where $\beta_1$ and $\beta_2$ are two hyperparameters for the online update of $M^{(t)}$ and $V^{(t)}$, the same as in Adam, respectively. $\eta$ denotes the learning rate, and $\xi$ denotes a small positive number for numerical stability.




\subsection{Update Rules of Fira-Adam}
\label{sub:preliminaries:fira}

Similar to GaLore-Adam, Fira also needs to store $M^{(t)}$, $V^{(t)}$, and $P^{(t)}$. The difference is that Fira-Adam additionally utilizes the low-rank approximation residual to update $W_l^{(t)}$.
\begin{align*}
    S^{(t)} &= (I-P^{(t)}(P^{(t)})^T) G^{(t)} \notag\\
    x^{(t)} &= x^{(t-1)} - \eta\cdot N^{(t)}-\eta \cdot \phi(S^{(t)})\notag.
\end{align*}
where $S^{(t)}$ represents the low-rank approximation error, $\phi(\cdot)$ represents a scaling function in Fira \cite{chen2024fira}, and $N^{(t)}$ is calculated in the same way as in GaLore-Adam shown above (see Eq.~\eqref{eq:N}). 
