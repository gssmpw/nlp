\section{Method} 
\label{sec:method}
In this section, we first show the adverse phenomenon of the frozen dominant subspace of mini-batch gradients. Then, to address this problem, we propose I3S for low-rank optimization. Finally, we provide the convergence analysis of low-rank optimization with I3S.


\begin{algorithm}[!ht]
   \caption{Low-rank Optimization with I3S}
   \label{alg:low_rank_adam_v1}
 \begin{algorithmic}[1]
   \STATE {\bfseries Input:} The $l$-th layer weight $x_l^{(t)} \in \mathbb{R}^{m_l\times n_l}$, for all $l \in [N]$. Learning rate $\eta$, scale factor $\alpha$, decay rates $\beta_1, \beta_2$, rank $r$, subspace change frequency $\tau \in \mathbb{Z}_+$, small constant for numerical stability $\xi$.
   \STATE {\bf Initialize:} for all $l \in [N]$ $V_l^{(0)}, M_l^{(0)} \in \mathbb{R}^{r \times n_l} \gets 0$
    \FOR{$t = 1 \to T$}
    \FOR{$l = 1 \to N$}
   \STATE Compute the mini-batch gradient: $G_l^{(t)} \in \mathbb{R}^{m_l \times n_l}$ 
   \STATE $P_l^{(t)} \gets \textsc{I3S}(G_l^{(t)}, \tau)$ \hfill \COMMENT{see Algorithm~\ref{alg:imp_sampling}}
   \STATE $x_l^{(t)} \gets$ Run \textsc{GaLore-Adam} or \textsc{Fira-Adam} by $V_l^{(t - 1)}, M_l^{(t - 1)}, x_l^{(t)}, P_l^{(t)}, G_l^{(t)}, \beta_1, \beta_2, \xi, \eta, \alpha$ \hfill \COMMENT{see Section~\ref{sub:preliminaries:galore} and~\ref{sub:preliminaries:fira} respectively}
   \ENDFOR
   \ENDFOR
   \STATE {\bf Return} $x^{(T)} = (x_1^{(T)}, x_2^{(T)}, \cdots, x_N^{(T)} )$
 \end{algorithmic}
\end{algorithm}


\subsection{Frozen Dominate Subspace of Mini-batch Gradient}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\linewidth]{all_subspaces_overlap.pdf}
    \caption{The average mini-batch gradient dominate subspace overlap in different linear layers over 8 blocks in LLaMA-60M model during pretraining. We measure the overlap between adjacent subspaces every 200 iterations.}
    \label{fig:q-galore}
\end{figure} 

\cite{zhang2024q-galore} observes that the cosine similarity between adjacent dominant subspaces approaches 1.0 in some layers after a certain stage of LLM pretraining, indicating that the dominant subspace of the gradient almost stops evolving. We observe a similar phenomenon in our experiment as well. Figure~\ref{fig:q-galore} reports the average result of dominant subspace overlap in different layers across all blocks at different iterations. We notice that dominant subspace overlaps are low in all layers at the early stage of pretraining, but they increase drastically as pretraining progresses, eventually becoming stable at different levels. Among all layers, gate\_proj and up\_proj exhibit the highest subspace overlaps. Intuitively, a high overlap between adjacent subspaces is harmful for low-rank optimization. Considering an extreme case, when the overlap reaches 1.0, the low-rank optimizer can only change the weights within a fixed low-rank subspace. However, when the low-rank subspace shifts significantly over time, the overall weight update—formed by summing updates from various low-rank subspaces—can overcome the constraints of the low-rank bottleneck. For readability, we refer to this phenomenon as the frozen dominant subspace.

\subsection{I3S: Importance Sampling Subspace Selection}

\begin{algorithm}[!ht]
   \caption{I3S: Importance sampling subspace selection}
   \label{alg:imp_sampling}
 \begin{algorithmic}[1]
    \STATE {\bfseries Input:} The mini-batch gradient at the iteration $t$, $G_l^{(t)} \in \mathbb{R}^{m \times n_l}$, where $l \in [N]$ denotes the layer. Subspace change frequency $\tau \in \mathbb{Z}_+$.\\
   \IF{$t \bmod \tau = 0$}
   \STATE $U_l^{(t)}, S_l^{(t)}, V_l^{(t)} \gets \text{SVD}(G_l^{(t)})$
   
   \STATE $\mathcal{I} \gets \textsc{Sample}([m], \mathrm{num}=r, \mathrm{weight}=S_l^{(t)})$ 
   \STATE $\mathcal{I} \gets \textsc{Sort}(\mathcal{I})$
   \STATE $P_l^{(t)} \gets U_l^{(t)}[:,\mathcal{I}]$ 
   \ELSE
   \STATE $P_l^{(t)} \gets P_l^{(t-1)}$ \hfill \COMMENT{Reuse the previous projector}
   \ENDIF
   \STATE {\bf Return} $P_l^{(t)}$
 \end{algorithmic}
\end{algorithm}



To overcome the problem of the frozen dominate subspace problem, we propose I3S to construct low-rank subspace. Low-rank optimization with I3S is given in Algorithm~\ref{alg:low_rank_adam_v1}.  
It can be seen that I3S does not change the overall structure of the original low-rank optimization algorithm but is a plug-and-play substitute for dominant subspace selection. Algorithm~\ref{alg:imp_sampling} gives the procedure of I3S. Line~4 denotes the weighted sampling without replacement. More precisely, each of the $m$ left singular vectors is equipped with a weight $\omega_i \in (0, 1)$ proportional to its corresponding singular value $S_i$,
\begin{align*}
    \omega_i = \frac{S_i}{\sum_{j=1}^{m}S_j}.
\end{align*}
For an index set sample $\mathcal{I}=\left(I_1, \cdots, I_r\right)$, the sampling probability can be written as 
\begin{align*}
    \mathbb{P}\left \{ \left ( I_1,\cdots ,I_r \right )= \left ( i_1,\cdots ,i_r \right ) \right \} \\=\prod_{k=1}^{r}\frac{\omega_{i_k}}{1-\omega_{i_1}-\cdots-\omega_{i_{k-1}}}  
\end{align*}
Line 5 sorts the sampled indices in ascending order so that the newly updated subspace basis vectors can align with optimizer states well. Line 6 constructs the orthogonal basis of the new subspace.

By using weighted sampling without replacement, we make adjacent subspaces more different and make the optimization trajectory not be trapped in too similar subspaces during training. 
Another advantage of I3S is that it does not bring extra overhead. 


\subsection{Provable Convergence Guarantee}\label{sec:3.3}

\begin{algorithm}[!ht]
   \caption{Hybrid subspace selection}
   \label{alg:hybrid_sampling}
 \begin{algorithmic}[1]
    \STATE {\bfseries Input:} The mini-batch gradient at the iteration $t$, $G_l^{(t)} \in \mathbb{R}^{m \times n_l}$, where $l \in [N]$ denotes the layer. Subspace change frequency $\tau \in \mathbb{Z}_+$.\\
   \IF{$t \bmod \tau = 0$}
   \STATE $U_l^{(t)}, S_l^{(t)}, V_l^{(t)} \gets \text{SVD}(G_l^{(t)})$
   
   \STATE $P_l^{(t)}[:,:r_0] \gets U_l^{(t)}[:, :r_0]$ \hfill \COMMENT{Choose the first $r_0$ singular vectors deterministically.}
   \STATE $P_l^{(t)}[:,r_0:] \gets \textsc{Unif}(U_l^{(t)}[:, r_0:], r-r_0)$ \hfill \COMMENT{Choose $(r-r_0)$ ones by uniform sampling from}
   \STATE $M_l^{(t)} \gets (P_l^{(t)})^T P_l^{(t-1)} M_l^{(t-1)}$ \hfill \COMMENT{apply momentum projection}
   \ELSE
   \STATE $P_l^{(t)} \gets P_l^{(t-1)}$ \hfill \COMMENT{Reuse the previous projector}
   \ENDIF
   \STATE {\bf Return} $P_l^{(t)}$
 \end{algorithmic}
\end{algorithm}

\cite{he2024subspace} points out that choosing the dominant subspace in low-rank optimization, as in GaLore, does not always guarantee convergence to the optimal solution. They propose a random sampling strategy in subspace selection that ensures provable convergence. However, their random sampling subspace does not significantly alleviate the performance gap between GaLore-Adam and full-rank Adam in the pretraining task, as reported in \cite{he2024subspace}. In contrast, our method shows empirical advantages, which are deferred to Section~\ref{sec:experiments}, and the convergence of our method is provided herein.

One tricky problem with our proposed I3S is the intractability of weighted sampling without replacement. Instead, we analyze a hybrid subspace selection method that is similar to the importance sampling we adopt in practice, as shown in Algorithm~\ref{alg:hybrid_sampling}. The hybrid subspace selection involves choosing the first $r_0$ leading singular vectors deterministically out of $m$ available ones, and selecting $(r-r_0)$ singular vectors from $(m-r_0)$ singular vectors using uniform sampling. In total, the hybrid subspace selection still selects a rank-$r$ subspace. The difference between this approach and choosing the dominant subspace spanned by the $r$ leading singular vectors is that hybrid subspace selection introduces randomness into subspace selection. The difference between the hybrid subspace selection and using a JL-transform matrix, as in \cite{he2024subspace}, is that the basis vectors in the hybrid subspace selection still align with the direction of mini-batch gradients. Empirically, this difference helps alleviate the gap between low-rank optimization and full-rank optimization. We choose this hybrid subspace selection as an alternative to importance sampling in theoretical analysis, but we do not extend it to our empirical experiments.

We treat an LLM as a neural network with $N$ layers, and each layer has a weight matrix, i.e., $x_l\in\mathbb{R}^{m\times n_l}, \forall l \in [N]$. Without loss of generality, we assume that $m \leq n_l$. In practice, most LLMs do not have biases for attention blocks and MLP blocks, and low-rank optimization is only applied to the training weight matrix. Therefore, this abstraction is reasonable. Mathematically, our objective function is 
\begin{align*}
    f: \mathbb{R}^{m \times n_1} \times \mathbb{R}^{m \times n_2} \times \dots \times \mathbb{R}^{m \times n_N} \to \mathbb{R}
\end{align*}
For all $x \in \mathrm{dom}(f)$, we denote $\nabla_l f(x)$ as $\frac{\partial f}{\partial X_l} \in \mathbb{R}^{m \times n_l}$. Below, we adopt two assumptions from \cite{he2024subspace} as follows.

\begin{assumption}[$L$-smoothness]\label{ass:l-smooth}
Let $f: \mathbb{R}^{m \times n_1} \times \mathbb{R}^{m \times n_2} \times \dots \times \mathbb{R}^{m \times n_N} \to \mathbb{R}$ be our objective function. Let $L > 0$. We assume $f$ is $L$-smooth, meaning that it satisfies
\begin{align*}
    \left \| \nabla_l f(x) - \nabla_l f(y) \right \|_F \leq L \left \| x - y \right \|_F
\end{align*}
for all $l \in [N]$.
\end{assumption}

\begin{assumption}[Bounded and Centered Mini-batch Gradient Noise]\label{bounded_noise}


Let $\nabla_lf(x^{(t)}) \in \mathbb{R}^{m \times n_l}$ be the gradient of our objective function for the $l$-th layer at the $t$-th iteration, where $t \in \mathbb{Z}_+$.
Let $G_l^{(t)} \in \mathbb{R}^{m \times n_l}$ be the mini-batch gradient which is the noisy version of $\nabla_lf(x^{(t)})$.

For all $l \in [N]$, we assume there exists a least upper bound $\sigma_l^2 \in \mathbb{R}$ for $\| G_l^{(t)}-\nabla_lf(x^{(t)}) \|_F^2$, namely
\begin{align*}
    \left \| G_l^{(t)}-\nabla_lf(x^{(t)}) \right \| _F^2\le \sigma_l^2
\end{align*}
and
\begin{align*}
    \mathbb{E} \left [ G_l^{(t)} \right ] =\nabla_lf(x^{(t)}).
\end{align*}
Furthermore, we define
    $\sigma := \max_{l \in [N]} \sigma_l$.
\end{assumption}

To compare with \cite{he2024subspace}, we analyze low rank momentum stochastic gradient descent (MSGD). And the convergence rate of low rank MSGD with I3S (as in Algorithm~\ref{alg:theory_algorithm}) is given by the following theorem.


\begin{theorem}[Convergence of GaLore-MSGD with hybrid subspace selection]\label{convergence_our_method_informal}
Under Assumption~\ref{ass:l-smooth}-\ref{bounded_noise}, if $T$ is large enough and we choose the following hyperparameters:  
\begin{align*}
&\beta_1=\left(1+\sqrt{\frac{\delta^{1.5}\sigma^2T}{L\Delta}}\right)^{-1},\\
&\tau=\left \lceil \frac{64}{3\delta\beta_1} \right \rceil,\\
&\eta=\left(4L+\sqrt{\frac{80L^2}{3\delta\beta_1^2}}+\sqrt{\frac{80\tau^2L^2}{3\delta}}+\sqrt{\frac{16\tau L^2}{3\beta_1}}\right)^{-1},
\end{align*}
GaLore with hybrid subspace selection converges in a rate as follows:
\begin{align}
    \frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\left[\left\|\nabla f(x^{(t)}) \right\|_F^2\right]=\mathcal{O}\left(\frac{L\Delta}{\delta^{2.5}T}+\sqrt{\frac{L\Delta\sigma^2}{\delta^{3.5}T}}\right),
\end{align}
where $\Delta=f(x^{(0)})-\inf_xf(x)$, and $\delta=\frac{r-r_0}{m-r_0}$.
\end{theorem}

Proofs of Theorem~\ref{convergence_our_method_informal} is deferred to Appendix~\ref{sec:proofs_our_method_guarantee}. Below, we present the convergence rate of GoLore \cite{he2024subspace}.

\begin{theorem}[Corollary 3 from \cite{he2024subspace}]\label{thm:he2024subspace}
    Under Assumption~\ref{ass:l-smooth}-\ref{bounded_noise}, let every notation be defined as in Theorem~\ref{convergence_our_method_informal}. Let $\underline{\delta}=\frac{r}{m}$. 
    
    Then, GoLore using small-batch stochastic gradients and MSGD converges as
    \begin{align*}
\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\left[\left\|\nabla f(x^{(t)}) \right\|_F^2\right]=\mathcal{O}\left(\frac{L\Delta}{\underline{\delta}^{2.5}T}+\sqrt{\frac{L\Delta\sigma^2}{\underline{\delta}^{3.5}T}}\right),   
\end{align*}
\end{theorem}

\paragraph{The Comparison Between Our Technique and Prior Work \cite{he2024subspace}.}

To directly compare our work with \cite{he2024subspace}, we adopt the same hyperparameters used in their study. When examining the convergence rate, we note that the primary distinction lies in our use of $\delta=\frac{r-r_0}{m-r_0}$ (Theorem~\ref{convergence_our_method_informal}), whereas \cite{he2024subspace} uses $\underline{\delta}=\frac{r}{m}$ (Theorem~\ref{thm:he2024subspace}).
$r_0$ can be chosen from the range $[0, r)$. 1). When $r_0=0$, this hybrid subspace selection is equivalent to uniform sampling, though this brings the best convergence rate in theory, we observe empirically that uniform sampling sometimes leads to loss spiking, which is the least thing we want to see during training, as shown in Figure~\ref{fig:loss_spiking}. 2). When $r_0>0$, hybrid subspace selection behaves similarly to I3S, which brings stable loss curve during training. This can be seen as a trade-off between theoretical convergence rate and empirical performance. Note that both the convergence rates of hybrid subspace selection and that of GoLore are better than using dominant subspace, which does not have provable convergence guarantee. 






