


\section*{Appendix}


\section{Proofs of Lemmas and Theorems in Section~\ref{sec:3.3}
\label{sec:proofs_our_method_guarantee}}


\begin{algorithm*}[!ht]
   \caption{Low-rank MSGD with hybrid subspace selection}
   \label{alg:theory_algorithm}
 \begin{algorithmic}[1]
   \STATE {\bfseries Input:} The $l$-th layer weight $x_l^{(t)} \in \mathbb{R}^{m_l\times n_l}$, for all $l \in [N]$, step size $\eta > 0$, hyperparameter for hybrid subspace selection $r_0$ and $r$, MSGD decay rate $\beta_1$.
   \STATE Initialize: for all $l \in [N]$, $M_l^{(0)}=\mathbf{0}_{r_l\times n_l}$. 
    \FOR{$t = 1 \to T$}
   \FOR{$l = 1 \to N$}
   \STATE $G_l^{(t)} \gets \nabla_l f(x^{(t)}) + \varepsilon_l^{(t)} $ \hfill \COMMENT{get mini-batch gradient $G_l^{(t)} \in \mathbb{R}^{m_l \times n_l}$ using the gradient of the objective function $f$ and noise $\varepsilon_l^{(t)} \in \mathbb{R}^{m_l \times n_l}$.}
   \IF{$t \bmod \tau = 0$}
   \STATE $\underbrace{U_l^{(t)}}_{m_l \times n_l}, \underbrace{S_l^{(t)}}_{n_l \times n_l}, \underbrace{V_l^{(t)}}_{n_l \times n_l} \gets \text{SVD}(G_l^{(t)})$
   
   \STATE $P_l^{(t)}[:,:r_0] \gets \underbrace{U_l^{(t)}[:, :r_0]}_{m_l \times r_0}$ \hfill \COMMENT{Choose the first $r_0$ singular vectors deterministically.}
   \STATE $P_l^{(t)}[:,r_0:] \gets \underbrace{\mathsf{Unif}(U_l^{(t)}[:, r_0:], r-r_0)}_{m_l \times (n_l - r_0)}$ \hfill \COMMENT{Choose $(r-r_0)$ ones by uniform sampling from}
   \STATE $\underbrace{M_l^{(t)}}_{r_l\times n_l} \gets \underbrace{(P_l^{(t)})^T}_{r_l \times m_l} \underbrace{P_l^{(t-1)}}_{m_l \times r_l} \underbrace{M_l^{(t-1)}}_{r_l\times n_l}$ \hfill \COMMENT{apply momentum projection}
   \ELSE
   \STATE $P_l^{(t)} \gets \underbrace{P_l^{(t-1)}}_{m_l \times r_l}$ \hfill \COMMENT{Reuse the previous projector}
   \ENDIF
   \STATE $\underbrace{R_l^{(t)}}_{r_l \times n_l} \gets \underbrace{(P_l^{(t)})^{T}}_{r_l \times m_l} \underbrace{G_l^{(t)}}_{m_l \times n_l}$ \hfill \COMMENT{Project gradient into low-rank subspace}
   \STATE $\underbrace{M_l^{(t)}}_{r_l \times n_l} \gets \beta_{1} \cdot \underbrace{M_l^{(t-1)}}_{r \times n_l} + (1 - \beta_{1}) \cdot \underbrace{R_l^{(t)}}_{r_l \times n_l}$ \hfill \COMMENT{Update momentum}
   
   \STATE $\underbrace{\Tilde{M}_l^{(t)}}_{m_l\times n_l} \gets \underbrace{P_l^{(t)}}_{m_l \times r_l} \underbrace{M_l^{(t)}}_{r_l \times n_l}$ \hfill \COMMENT{Project momentum back to full size}
   \STATE $\underbrace{x_l^{(t+1)}}_{m_l\times n_l} \gets \underbrace{x_l^{(t)}}_{m_l\times n_l} - \eta \cdot \underbrace{\tilde{M}_l^{(t)}}_{m_l\times n_l}$ \hfill \COMMENT{Update weight}

   \ENDFOR
   \ENDFOR
 \end{algorithmic}
\end{algorithm*}


Before proceeding to the proof of GaLore-MSGD with Importance Sampling, we need to adopt some important lemmas.

\begin{lemma} [Descent Lemma from \cite{he2024subspace}]
\label{descent lemma}

Under the assumption of L-smooth objective function, for update
\begin{align*}
    x^{(t+1)} = x^{(t)} - \eta \tilde{M}^{(t)},
\end{align*}

we have 
\begin{align*}
    f(x^{(t+1)}) \leq f(x^{(t)}) - \left(\frac{1}{2\eta}-\frac{L}{2} \right)\left \| x^{(t+1)}-x^{(t)} \right \|_F^2 +\frac{\eta}{2}\left \| \tilde{M}^{(t)}-\nabla f(x^{(t)})  \right \|_F^2 - \frac{\eta}{2}\left \| \nabla f(x^{(t)}) \right \|_F^2 
\end{align*}

    
\end{lemma}







We adopt similar proof routinue as in \cite{he2024subspace}, first we need a Momentum Contraction Lemma for Sampling Subspace Selection, which is shown as Lemma~\ref{momentum_contraction}.


\begin{lemma}[Momentum Contraction]\label{momentum_contraction}
Let \( G_l^{(t)} \) be an unbiased estimator of the gradient \( \nabla _l F(x^{(0)}) \) with variance bounded by \( \sigma_l^2 \). Define
\begin{align*}
    \delta=\frac{r-r_0}{m-r_0}
\end{align*}

\begin{itemize}
    \item {\bf Part 1.} When $t=0$, we have
\begin{align*}
    \mathbb{E}\left [ \left \| \tilde {M}_l^{(0)} -\nabla _lf(x^{(0)})\right \|_F^2  \right ] \le
    \left(1-(2\beta_1-\beta_1^2)\frac{r-r_0}{m-r_0} \right)\mathbb{E}\left [ \left \| \nabla_lf(x^{(0)}) \right \|_F^2  \right ]
    + \beta_1^2\sigma_l^2.
\end{align*}
    \item {\bf Part 2.} When $t=k\tau$, $k \in \mathbb{N}$, we have
\begin{align}\label{part2:momentum_contraction}
&\mathbb{E}\left[ \left \| \tilde{M}_l^{(t)}-\nabla _lf(x^{(t)})  \right \|_F^2 \right]-(1-(1-\frac{\delta}{4})\beta_1)\mathbb{E}\left[ \left \| \tilde{M}_l^{(t-1)}-\nabla _lf(x^{(t-1)})  \right \|_F^2 \right] \\
\leq&\frac{2(1-\delta)}{\tau}\sum_{r=0}^{\tau-1}\mathbb{E}\left [ \left \| \nabla _l f(x^{(k\tau+r)}) \right \|_F^2  \right ]+\frac{5(1-\beta_1)}{\beta_1\delta}\mathbb{E}\left [ \left \| \nabla _lf(x^{(t)})- \nabla _lf(x^{(t-1)})\right \|_F^2  \right ]\\
&~+(\tau-1)(1-\delta)\sum_{r=0}^{\tau-2}\mathbb{E}\left [ \left \| \nabla _lf(x^{(k\tau+r+1)})-\nabla _lf(x^{(k\tau+r)}) \right \|  \right ] + \beta_1^2\sigma _l^2
\end{align}


\item {\bf Part 3.} When $t=k\tau+r$, $k \in \mathbb{N}$, $1 \leq r \leq \tau-1$,
\begin{align*}\label{part 3: momentum contraction}
&\mathbb{E}\left[ \left \| \tilde{M}_l^{(t)}-\nabla _lf(x^{(t)})  \right \|_F^2 \right]-(1-(1-\frac{\delta}{4})\beta_1)\mathbb{E}\left[ \left \| \tilde{M}_l^{(t-1)}-\nabla _lf(x^{(t-1)})  \right \|_F^2 \right] \\
\leq&\left ( 1-\frac{\delta}{2} \right )\beta_1\mathbb{E}\left [ \left \| \nabla_lf(x^{(t)}) \right \| _F^2 \right ]   +\frac{5(1-\beta_1)}{\beta_1\delta}\mathbb{E}\left [ \left \| \nabla _lf(x^{(t)})- \nabla _lf(x^{(t-1)})\right \|_F^2  \right ]\\
&~+\frac{10r\beta_1}{\delta}\sum_{i=1}^{r}\mathbb{E}\left [ \left \| \nabla _lf(x^{(k\tau+i)})-\nabla _lf(x^{(k\tau+i-1)}) \right \| _F^2 \right ] + \beta_1^2\sigma _l^2
\end{align*}


\end{itemize}



\end{lemma}

\begin{proof}

{\bf Proof of Part 1.}

When $t=0$, we have 
\begin{align}
\mathbb{E}\left [ \left \| \tilde {M}_l^{(0)} -\nabla _lf(x^{(0)})\right \|_F^2  \right ]
&= \mathbb{E}\left [ \left \| \beta_1P_l^{(0)}\left(P_l^{(0)}\right)^T\left(G_l^{(0)}-\nabla _lf(x^{(0)})\right) \right \|_F^2  \right ]\notag\\
&\quad + \mathbb{E}\left [ \left \| \left(\beta_1P_l^{(0)}\left(P_l^{(0)}\right)^T-I\right)\nabla _lf(x^{(0)}) \right \|_F^2  \right ],
\end{align}
which follows from definition of $\tilde{M}_l^{(0)}$ and the unbiasness of $G_l^{(0)}$. 
For the first term, using Assumption~\ref{bounded_noise} we have
\begin{align}
    \mathbb{E}\left [ \left \| \beta_1P_l^{(0)}\left(P_l^{(0)}\right)^T\left(G_l^{(0)}-\nabla _lf(x^{(0)})\right) \right \|_F^2  \right ]\le\beta_1^2\sigma_l^2\label{0_first}
\end{align}
For the second term, we have
\begin{align} \label{0_seconf_last}
&\mathbb{E}\left [ \left \| \left(I - \beta_1P_l^{(0)}\left(P_l^{(0)}\right)^T\right)\nabla_lf(x^{(0)}) \right \|_F^2  \right ] \notag\\
=&\mathbb{E}_{\nabla_{l}f(x^{(0)})}\left [ \mathbb{E}_{P_l^{(0)}}\left [   \left \| \left(I - \beta_1P_l^{(0)}\left(P_l^{(0)}\right)^T\right)\nabla_lf(x^{(0)}) \right \|_F^2 \middle|  \nabla_lf(x^{(0)})  \right ] \right ] \notag\\
=&\mathbb{E}_{\nabla_{l}f(x^{(0)})}\left [ \mathbb{E}_{P_l^{(0)}}\left [  \mathrm{tr} \left(\nabla_lf(x^{(0)})^T\left(I - \beta_1P_l^{(0)}\left(P_l^{(0)}\right)^T\right)^2\nabla_lf(x^{(0)}) \right) \middle|  \nabla_lf(x^{(0)})  \right ] \right ] \notag\\
=&\mathbb{E}_{\nabla_{l}f(x^{(0)})}\left [   \mathrm{tr} \left(\mathbb{E}_{P_l^{(t)}}\left [\nabla_lf(x^{(0)})^T\left(I - \beta_1P_l^{(0)}\left(P_l^{(0)}\right)^T\right)^2\nabla_lf(x^{(0)})  \middle|  \nabla_lf(x^{(0)})  \right ]\right) \right ] \notag\\
=&\mathbb{E}_{\nabla_{l}f(x^{(0)})}\left [   \mathrm{tr} \left(\mathbb{E}_{P_l^{(t)}}\left [\sum_{j=r_0+1}^{m}\left(1-(2\beta_1-\beta_1^2)\mathbf{1}_{\{j\}}\right)\cdot \nabla_lf(x^{(t)})^T U_jU_j^T\nabla_lf(x^{(0)})  \middle|  \nabla_lf(x^{(0)})  \right ]\right) \right ] \notag\\
=&\mathbb{E}_{\nabla_{l}f(x^{(0)})}\left [   \mathrm{tr} \left(\sum_{j=r_0+1}^{m}\left(1-(2\beta_1-\beta_1^2)\frac{r-r_0}{m-r_0}\right) \cdot \nabla_lf(x^{(0)})^T U_jU_j^T\nabla_lf(x^{(0)}) \right) \right ] \notag\\
\le&\left(1-(2\beta_1-\beta_1^2)\frac{r-r_0}{m-r_0}\right) \cdot\mathbb{E}_{\nabla_{l}f(x^{(0)})}\left [   \mathrm{tr} \left(\sum_{j=1}^{m} \nabla_lf(x^{(0)})^T U_jU_j^T\nabla_lf(x^{(0)}) \right) \right ] \notag\\
=&\left(1-(2\beta_1-\beta_1^2)\frac{r-r_0}{m-r_0} \right)\mathbb{E}\left [ \left \| \nabla_lf(x^{(0)}) \right \|_F^2  \right ],
\end{align}
where the first step follows from the law of total expectation, the second step follows from $\|A\|_F = \sqrt{\mathrm{tr}(A^T A)}$, the third step follows from simple algebra, the fourth step follows from the fact that $P_l^{(0)}$ is selected by using hybrid subspace sampling method, the sixth step follows from $\mathbb{E}[aX] = a\mathbb{E}[X]$, and the last step follows from $\|A\|_F = \sqrt{\mathrm{tr}(A^T A)}$.


Combining Eq.~\eqref{0_first} and Eq.~\eqref{0_seconf_last} together, we have 
\begin{align}
    \mathbb{E}\left [ \left \| \tilde {M}_l^{(0)} -\nabla _lf(x^{(0)})\right \|_F^2  \right ] \le
    \left(1-(2\beta_1-\beta_1^2)\frac{r-r_0}{m-r_0} \right)\mathbb{E}\left [ \left \| \nabla_lf(x^{(0)}) \right \|_F^2  \right ]
    + \beta_1^2\sigma_l^2.
\end{align}
{\bf Proof of Part 2.}
The proof of Part 2 is very similar to the proof of Part 2 in Lemma 10 from \cite{he2024subspace}. The difference is that we do not have 
\begin{align*}
&\mathbb{E}\left [ \left \| P_l^{(t)}(P_l^{(t)})^T\left((1-\beta_1)\tilde{M}_l^{(t-1)} + \beta_1G_l^{(t)}- \nabla _l f(x^{(t)})\right) \right \| _F^2 \right ]\\
&\leq \delta \cdot\mathbb{E}\left [ \left \| \left((1-\beta_1)\tilde{M}_l^{(t-1)} + \beta_1G_l^{(t)}- \nabla _l f(x^{(t)})\right) \right \| _F^2 \right ],
\end{align*}
but instead we have a slightly looser bound
\begin{align*}
&\mathbb{E}\left [ \left \| P_l^{(t)}(P_l^{(t)})^T\left((1-\beta_1)\tilde{M}_l^{(t-1)} + \beta_1G_l^{(t)}- \nabla _l f(x^{(t)})\right) \right \| _F^2 \right ]\\
&\leq \mathbb{E}\left [ \left \| \left((1-\beta_1)\tilde{M}_l^{(t-1)} + \beta_1G_l^{(t)}- \nabla _l f(x^{(t)})\right) \right \| _F^2 \right ].
\end{align*}
Based on this and following the corresponding proof steps in \cite{he2024subspace}, we can get Eq.~\eqref{part2:momentum_contraction}.

{\bf Proof of Part 3.}
The result of Part 3 and the proof in Part 3 is the same as in \cite{he2024subspace}.
\end{proof}

Though our Momentum Contraction result is a little worse than the one in \cite{he2024subspace}, we can still get the same result for Momentum Error Bound, as shown in Lemma~\ref{momentum_error_bound}.
\begin{lemma}[Momentum Error Bound]\label{momentum_error_bound}
Define 
\begin{align*}
    \sigma^2 = \sum_{l\in[N]}\sigma_l^2
\end{align*}
Then we have 
\begin{align}\label{eq:momentum_error_bound}
&\sum_{t=0}^{K\tau-1}\mathbb{E}\left [ \left \| \tilde{M}^{(t)}-\nabla f(x^{(t)}) \right \|_F^2  \right ]\\
\leq&\left(\frac{5(1-\beta_1)}{(1-\delta/4)\delta\beta_1^2}+\frac{5\tau(1-\tau)}{(1-\delta/4)\delta}+\frac{\tau-1}{(1-\delta/4)\beta}\right)L^2\sum_{t=0}^{K\tau-2}\mathbb{E}\left [ \left \| x^{(t+1)}- x^{(t)} \right \| _F^2 \right ] \\
&~+\left(\frac{1-\delta/2}{1-\delta/4}+\frac{2}{(1-\delta/4)\beta_1}\right) \sum_{t=0}^{K\tau-2}\mathbb{E}\left [ \left \| \nabla  f(x^{(t)}) \right \|_F^2  \right ] + \frac{K\tau\beta_1\sigma^2}{1-\delta/4}
\end{align}
\end{lemma}
\begin{proof}
The proof is in the similar manner as the proof of momentum error bound in \cite{he2024subspace}. However, because there is some difference between our momentum contraction result and their momentum contraction result, we show our proof here.

First we apply summation to Eq.~\ref{part 3: momentum contraction} as follows:
\begin{align} \label{momentum_error_proof_step1}
&\sum_{t=k\tau+1}^{(k+1)\tau-1}\mathbb{E}\left [ \left \| \tilde{M}_l^{(t)}-\nabla_l f(x^{(t)}) \right \|_F^2  \right ] -\left(1-(1-\frac{\delta}{4})\beta_1\right)\sum_{t=k\tau+1}^{(k+1)\tau-1}\mathbb{E}\left [ \left \| \tilde{M}_l^{(t-1)}-\nabla_l f(x^{(t-1)}) \right \|_F^2  \right ]\notag\\
\leq & \left(1-\frac{\delta}{2}\right)\beta_1\sum_{t=k\tau+1}^{(k+1)\tau-1}\mathbb{E}\left [ \left \|\nabla_l f(x^{(t)}) \right \|_F^2  \right ] \notag\\
&~+\frac{5(1-\beta_1)}{\beta_1\delta}\sum_{t=k\tau+1}^{(k+1)\tau-1}\mathbb{E}\left [ \left \|\nabla_l f(x^{(t)})-\nabla_l f(x^{(t-1)}) \right \|_F^2  \right ]\notag\\
&~+\frac{10\beta_1}{\delta}\sum_{r=1}^{\tau-1}r\sum_{i=1}^{r}\mathbb{E}\left [ \left \|\nabla_l f(x^{(k\tau+i)})-\nabla_l f(x^{(k\tau+i-1)}) \right \|_F^2  \right ]\notag\\
&~+\beta_1^2\sigma_l^2(\tau-1)\notag\\
\leq & \left(1-\frac{\delta}{2}\right)\beta_1\sum_{t=k\tau+1}^{(k+1)\tau-1}\mathbb{E}\left [ \left \|\nabla_l f(x^{(t)}) \right \|_F^2  \right ] \notag\\
&~+\left [   \frac{5(1-\beta_1)}{\beta_1\delta}+\frac{5\beta_1\tau(\tau-1)}{\delta}    \right ]\sum_{t=k\tau}^{(k+1)\tau-2}\mathbb{E}\left [ \left \|\nabla_l f(x^{(t+1)})-\nabla_l f(x^{(t)}) \right \|_F^2  \right ]\notag\\
&~+\beta_1^2\sigma_l^2(\tau-1)
\end{align}
 
Then add Eq.~\eqref{momentum_error_proof_step1} and Eq.~\eqref{part2:momentum_contraction} together, we have
\begin{align}\label{momentum_error_proof_step2}
&\sum_{t = k\tau}^{(k+1)\tau-1}\mathbb{E}\left [ \left \| \tilde{M}_l^{(t)}-\nabla_l f(x^{(t)}) \right \|_F^2  \right ] -\left(1-(1-\frac{\delta}{4})\beta_1\right)\sum_{t = k\tau}^{(k+1)\tau-1}\mathbb{E}\left [ \left \| \tilde{M}_l^{(t-1)}-\nabla_l f(x^{(t-1)}) \right \|_F^2  \right ]\notag\\
\leq & \left[\left(1-\frac{\delta}{2}\right)\beta_1+\frac{2(1-\delta)}{\tau}\right]\sum_{t = k\tau}^{(k+1)\tau-1}\mathbb{E}\left [ \left \|\nabla_l f(x^{(t)}) \right \|_F^2  \right ] \notag\\
&~+\left[\frac{5(1-\beta_1)}{\beta_1\delta}+\frac{5\beta_1 \tau(\tau-1)}{\delta}+(\tau-1)(1-\delta)\right]\sum_{t = k\tau}^{(k+1)\tau-2}\mathbb{E}\left [ \left \|\nabla_l f(x^{(t+1)})-\nabla_l f(x^{(t)}) \right \|_F^2  \right ]\notag\\
&~+\beta_1^2\sigma_l^2\tau\notag\\
\end{align}
Then applying summation over $k$ from 0 to $K$ and summation over all $l\in [N]$ gives us Eq.~\eqref{eq:momentum_error_bound}.
\end{proof}


\begin{theorem}[Convergence rate of GaLore-MSGD with sampling subspace selection]
Let $\eta > 0$ be the learning rate.
We define $m := \min_{l \in [N]} m_l$.
Let $\delta_{r_0} \in (0, 1)$ be the relative low-rank approximation error of mini-batch gradient. We choose hyperparameter $\beta_1 \in (0, 1)$ to satisfy that $\tilde{\delta}_{r_0} < 1-\beta_1$, where
\begin{align*}
\tilde{\delta}_{r_0} := \max \left \{ \left(1-\beta_1\right)^2\left(1-\frac{r-r_0}{m-r_0} \right), \delta _{r_0} \right \}.
\end{align*}
and choose learning rate 
\begin{align*}
    \eta \leq \min\{\frac{1}{2L}, \frac{1-\beta_1}{\sqrt{2}\beta_1L}\}.
\end{align*}

We define 
\begin{align*}
    C_{r_0} := (1-\beta_1)^2+(1-\frac{r-r_0}{m-r_0}) + \delta_{r_0}
\end{align*}
and
\begin{align*}
    \Delta 
    := & ~ \frac{1-\beta_1}{1-\beta_1-\tilde{\delta}_{r_0}}(\frac{2}{\eta}\mathbb{E} [ f(x^{(0)}) - f(x^*)] \\
    & ~ + \mathbb{E}[ \| \tilde{M}^{(0)}-\nabla f(x^{(0)}) \|_F^2  ]).
\end{align*}

Then, we have
\begin{align*}
& ~ \frac{1}{(k+1)\tau}\sum_{t=0}^{(k+1)\tau-1}\mathbb{E}\left [ \left \| \nabla f(x^{(t)})  \right \| _F^2 \right ] \\
& ~ \leq \frac{\Delta}{(k+1)\tau}+ \frac{C_{r_0}}{1-\beta_1}\sigma^2
\end{align*}
\end{theorem}

\begin{proof}
From Lemma~\ref{descent lemma}, we have 
\begin{align*}
f(x^{(t+1)}) \leq f(x^{(t)}) - \left(\frac{1}{2\eta}-\frac{L}{2} \right)\left \| x^{(t+1)}-x^{(t)} \right \|_F^2 +\frac{\eta}{2}\left \| \tilde{M}^{(t)}-\nabla f(x^{(t)})  \right \|_F^2 - \frac{\eta}{2}\left \| \nabla f(x^{(t)}) \right \|_F^2, 
\end{align*}
by simply rearrange different terms, we have 

\begin{align*}
&\frac{\eta}{2}\left \| \nabla f(x^{(t)}) \right \|_F^2  \\
&\leq f(x^{(t)}) - f(x^{(t+1)}) - \left(\frac{1}{2\eta}-\frac{L}{2} \right)\left \| x^{(t+1)}-x^{(t)} \right \|_F^2 +\frac{\eta}{2}\left \| \tilde{M}^{(t)}-\nabla f(x^{(t)})  \right \|_F^2 
\end{align*}
By summing over $t$ from 0 to $T$, we have 

\begin{align*}
    & ~ \sum_{t = 0}^T \mathbb{E}[\|\nabla f(x^{(t)}) \|_F^2]\\
    \leq & ~ \frac{2}{\eta} (f(x^{(0)}) - f(x^{(*)})) - \frac{2}{\eta} (\frac{1}{2 \eta} - \frac{L}{2}) \sum_{t = 0}^T \mathbb{E}[\|x^{(t + 1)}) - x^{(t)}) \|_F^2] + \sum_{t = 0}^T \mathbb{E}[\|\tilde M^{(t)} - \nabla f(x^{(t)}) \|_F^2]\\
    \leq & ~ \frac{2}{\eta} (f(x^{(0)}) - f(x^{(*)})) - \frac{2}{\eta} (\frac{1}{2 \eta} - \frac{L}{2}) \sum_{t = 0}^T \mathbb{E}[\|x^{(t + 1)}) - x^{(t)}) \|_F^2] \\
    + & ~ \sum_{t = 1}^T \frac{\beta_1^2}{(1 - \beta_1)^2} \mathbb{E}[\| \nabla_l f(x^{(t)}) - \nabla_l f(x^{(t - 1)}) \|_F^2]\\
    + & ~ \sum_{t = 1}^T (1 - \beta_1) (1 - \frac{r - r_0}{m - r_0}) \mathbb{E}[\| \nabla_l f(x^{(t)}) \|_F^2]\\
    + & ~ T \cdot (1 - \beta_1) \sigma_l^2\\
    + & ~ \frac{1}{1 - \beta_1} \mathbb{E}[\|\tilde M_l^{(0)} - \nabla_l f(x^{(0)}) \|_F^2]\\
    \leq & ~ \frac{2}{\eta} (f(x^{(0)}) - f(x^{(*)})) - \frac{2}{\eta} (\frac{1}{2 \eta} - \frac{L}{2}) \sum_{t = 0}^T \mathbb{E}[\|x^{(t + 1)}) - x^{(t)}) \|_F^2] \\
    + & ~ \sum_{t = 1}^T \frac{L^2 \beta_1^2}{(1 - \beta_1)^2} \mathbb{E}[\| x^{(t)} - x^{(t - 1)} \|_F^2]\\
    + & ~ \sum_{t = 1}^T (1 - \beta_1) (1 - \frac{r - r_0}{m - r_0}) \mathbb{E}[\| \nabla_l f(x^{(t)}) \|_F^2]\\
    + & ~ T \cdot (1 - \beta_1) \sigma_l^2\\
    + & ~ \frac{1}{1 - \beta_1} \mathbb{E}[\|\tilde M_l^{(0)} - \nabla_l f(x^{(0)}) \|_F^2]\\
    \leq & ~ \frac{2}{\eta} (f(x^{(0)}) - f(x^{(*)}))\\
    + & ~ (\frac{L^2 \beta_1^2}{(1 - \beta_1)^2} - \frac{2}{\eta} (\frac{1}{2 \eta} - \frac{L}{2})) \sum_{t = 0}^T \mathbb{E}[\|x^{(t + 1)}) - x^{(t)}) \|_F^2] \\
    + & ~ \sum_{t = 1}^T (1 - \beta_1) (1 - \frac{r - r_0}{m - r_0}) \mathbb{E}[\| \nabla_l f(x^{(t)}) \|_F^2]\\
    + & ~ T \cdot (1 - \beta_1) \sigma_l^2\\
    + & ~ \frac{1}{1 - \beta_1} \mathbb{E}[\|\tilde M_l^{(0)} - \nabla_l f(x^{(0)}) \|_F^2]\\
\end{align*}


Then by reordering terms on both sides and using proper learning rate $\eta$, we have 
\begin{align*}
&\frac{1}{T}\mathbb{E}\left[ \left \| \nabla  f(x^{(t)})  \right \|_F^2 \right]\\
&\leq \frac{1}{1-(1-\beta_1)(1-\frac{r-r_0}{m-r_0})}\cdot\frac{2}{\eta}\cdot\frac{\Delta_1}{T}\\
&~+\frac{1-\beta_1}{1-(1-\beta_1)(1-\frac{r-r_0}{m-r_0})}\cdot\sigma ^2\\
&~+\frac{1-\beta_1}{1-(1-\beta_1)(1-\frac{r-r_0}{m-r_0})}\cdot\frac{\Delta_2}{T}
\end{align*}
where $\Delta_1 = f(x^{(0)}) - f(x^*)$, $\Delta_2=\mathbb{E}\left[\left\|\tilde{M}^{(0)} - \nabla f(x^{(0)}) \right\|_F^2\right]$
\end{proof}


\section{More Related Work}

\paragraph{LLM Efficiency}

Many other works study the LLM efficiency from other aspects. For example, low rank approximation \cite{gsyz23,syyz23_weighted} can also be applied to improve the computational complexity of (masked) attention approximation \cite{chl+24,lssz24,as23}. \cite{gsx23,ssz23_tradeoff,swy23,gswy23,gsy23_hyper,syz23,lswy23} analyze the attention regression problems. \cite{cll+24} study the computational limits of Mamba. \cite{sxy23} investigate the expressibility of polynomial attention. \cite{gsy23_coin} apply the sketching technique to develop the decentralized large language model.


\paragraph{Reinforcement Learning}

In reinforcement learning (RL) \cite{lwcy23,ly24,llwy24,zcz+24,zcy23}, an agent learns to make sequential decisions by interacting with an environment to maximize a cumulative reward. RL algorithms, especially policy gradient methods (e.g., REINFORCE, PPO, TRPO) \cite{zhang2021sample,zakharenkov2021deep,engstrom2019implementation}, often rely on stochastic gradient descent (SGD) or Adam for optimization. Our low-rank optimization techniques for Adam, which could, in theory, be applied to RL training to make policy optimization more memory-efficient.