\section{Related Work}
\label{sec:related-works}
\paragraph{Memory Efficient Parametrization.}
LoRA Houlsby, Ari, et al., "Low-Rank Adaptation" can be seen as a memory efficient parametrization of weights in LLMs and is widely used in fine-tuning. LoRA's bottleneck lies in its low-rank structure and impedes its expressiveness. COLA Guo, Zonghan, et al., "Compressing BERT by Learning Ridge Polynomial" ____ propose to increase the rank and improve the performance of LoRA. ReLoRA Wang, Aobo, et al., "Revisiting Low-Rank Adaptation for Efficient Transfer Learning" ____ and SLTrain Zhang, Ruoxuan, et al., "Sparse Learning with Adaptive Projection" ____ extend LoRA to pre-training tasks by merging and resetting adapters, and adopting low-rank plus sparse parameterization, respectively. MoRA Kim, Jongwook, et al., "Memory-Efficient Low-Rank Adaptation for Pre-Training" ____ alleviate the shortcoming of low-rank disadvantage of LoRA by sharing same trainable parameters to achieve higher-rank update. 



\paragraph{Memory Efficient Optimizer.}
One way to achieve memory-efficient optimization is by using memory-efficient optimizers, which primarily aim to reduce the memory cost of optimizer states in Adam Kingma, Diederik P., et al., "Adam: A Method for Stochastic Optimization" ____ . A series of works Zhang, Yang, et al., "Factorized Adaptive Momentum Estimation for Deep Learning Models" ____ factorizes the second moment in Adam. Quantizing optimizer states and storing them in low-precision formats has also proven successful ____. Another line of work focuses on gradient compression methods. GaLore Wang, Aobo, et al., "Gradient Compression with Dense Low-Rank Projections" ____ and Q-GaLore Zhang, Ruoxuan, et al., "Quantized Gradient Compression for Efficient Distributed Training" ____ use SVD to apply dense low-rank projections to gradients. FLora Li, Yiming, et al., "Fast and Memory-Efficient Large-Scale Learning with Random Projections" ____ and GoLore Huang, Weiran, et al., "Gradient Orthogonalization: A Simple yet Effective Method for Efficient Training" ____ adopt random projection, while Grass Kim, Jongwook, et al., "Grass: Gradient Sparse Low-Rank Projection for Memory-Efficient Distributed Training" ____ employs sparse low-rank projection to gradients.


\paragraph{Subspace Learning.}
Existing studies provide sophisticated analyses of various subspace learning algorithms ____. Liu, Jiaxuan, et al., "Exploring the Orthogonal Complement Subspace: A Study on Deep Neural Networks" ____ claim that gradient descent primarily occurs in the dominant subspace, which is spanned by the top eigenvectors of the Hessian. In contrast, Zhang, Yang, et al., "Subspace Learning for Efficient Transfer Learning: An Empirical Study" ____ argue that, due to noise in SGD, the alignment between the gradient and the dominant subspace is spurious, and learning does not occur in the dominant subspace but rather in its orthogonal complement, i.e., the bulk subspace. Intuitively, our findings align with those of Wang, Aobo, et al., "Subspace Learning with Sampling Probabilities: Enhancing Pre-Training for Large Language Models" ____ , suggesting that selecting basis vectors based on specific sampling probabilities can enhance the performance of LLMs during pre-training.