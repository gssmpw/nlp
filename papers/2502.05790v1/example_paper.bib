@article{han2024sltrain,
  title={SLTrain: a sparse plus low-rank approach for parameter and memory efficient pretraining},
  author={Han, Andi and Li, Jiaxiang and Huang, Wei and Hong, Mingyi and Takeda, Akiko and Jawanpuria, Pratik and Mishra, Bamdev},
  journal={arXiv preprint arXiv:2406.02214},
  year={2024}
}


@article{zhao2024galore,
  title={Galore: Memory-efficient llm training by gradient low-rank projection},
  author={Zhao, Jiawei and Zhang, Zhenyu and Chen, Beidi and Wang, Zhangyang and Anandkumar, Anima and Tian, Yuandong},
  journal={arXiv preprint arXiv:2403.03507},
  year={2024}
}


@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{zhang2017normalized,
  title={Normalized direction-preserving adam},
  author={Zhang, Zijun and Ma, Lin and Li, Zongpeng and Wu, Chuan},
  journal={arXiv preprint arXiv:1709.04546},
  year={2017}
}


@article{zhang2024adam-mini,
  title={Adam-mini: Use fewer learning rates to gain more},
  author={Zhang, Yushun and Chen, Congliang and Li, Ziniu and Ding, Tian and Wu, Chenwei and Ye, Yinyu and Luo, Zhi-Quan and Sun, Ruoyu},
  journal={arXiv preprint arXiv:2406.16793},
  year={2024}
}


@article{nemirovski2009robust,
  title={Robust stochastic approximation approach to stochastic programming},
  author={Nemirovski, Arkadi and Juditsky, Anatoli and Lan, Guanghui and Shapiro, Alexander},
  journal={SIAM Journal on optimization},
  volume={19},
  number={4},
  pages={1574--1609},
  year={2009},
  publisher={SIAM}
}


@article{hendrickx2024convex,
  title={Convex SGD: Generalization Without Early Stopping},
  author={Hendrickx, Julien and Olshevsky, Alex},
  journal={arXiv preprint arXiv:2401.04067},
  year={2024}
}


@inproceedings{langley00,
 author    = {P. Langley},
 title     = {Crafting Papers on Machine Learning},
 year      = {2000},
 pages     = {1207--1216},
 editor    = {Pat Langley},
 booktitle     = {Proceedings of the 17th International Conference
              on Machine Learning (ICML 2000)},
 address   = {Stanford, CA},
 publisher = {Morgan Kaufmann}
}

@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}

@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2021}
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}

@article{he2024subspace,
  title={Subspace Optimization for Large Language Models with Convergence Guarantees},
  author={He, Yutong and Li, Pengrui and Hu, Yipeng and Chen, Chuyan and Yuan, Kun},
  journal={arXiv preprint arXiv:2410.11289},
  year={2024}
}


@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@inproceedings{lialin2023relora,
  title={Relora: High-rank training through low-rank updates},
  author={Lialin, Vladislav and Muckatira, Sherin and Shivagunde, Namrata and Rumshisky, Anna},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}



@article{zhang2024q-galore,
  title={Q-galore: Quantized galore with int4 projection and layer-adaptive low-rank gradients},
  author={Zhang, Zhenyu and Jaiswal, Ajay and Yin, Lu and Liu, Shiwei and Zhao, Jiawei and Tian, Yuandong and Wang, Zhangyang},
  journal={arXiv preprint arXiv:2407.08296},
  year={2024}
}


@article{muhamed2024grass,
  title={Grass: Compute efficient low-memory llm training with structured sparse gradients},
  author={Muhamed, Aashiq and Li, Oscar and Woodruff, David and Diab, Mona and Smith, Virginia},
  journal={arXiv preprint arXiv:2406.17660},
  year={2024}
}


@inproceedings{shazeer2018adafactor,
  title={Adafactor: Adaptive learning rates with sublinear memory cost},
  author={Shazeer, Noam and Stern, Mitchell},
  booktitle={International Conference on Machine Learning},
  pages={4596--4604},
  year={2018},
  organization={PMLR}
}


@article{luo2023came,
  title={Came: Confidence-guided adaptive memory efficient optimization},
  author={Luo, Yang and Ren, Xiaozhe and Zheng, Zangwei and Jiang, Zhuo and Jiang, Xin and You, Yang},
  journal={arXiv preprint arXiv:2307.02047},
  year={2023}
}



@article{das2024natural,
  title={Natural GaLore: Accelerating GaLore for memory-efficient LLM Training and Fine-tuning},
  author={Das, Arijit},
  journal={arXiv preprint arXiv:2410.16029},
  year={2024}
}


@article{zhao2024adapprox,
  title={Adapprox: Adaptive Approximation in Adam Optimization via Randomized Low-Rank Matrices},
  author={Zhao, Pengxiang and Li, Ping and Gu, Yingjie and Zheng, Yi and K{\"o}lker, Stephan Ludger and Wang, Zhefeng and Yuan, Xiaoming},
  journal={arXiv preprint arXiv:2403.14958},
  year={2024}
}


@article{song2024does,
  title={Does SGD really happen in tiny subspaces?},
  author={Song, Minhak and Ahn, Kwangjun and Yun, Chulhee},
  journal={arXiv preprint arXiv:2405.16002},
  year={2024}
}



@article{gur2018gradient,
  title={Gradient descent happens in a tiny subspace},
  author={Gur-Ari, Guy and Roberts, Daniel A and Dyer, Ethan},
  journal={arXiv preprint arXiv:1812.04754},
  year={2018}
}


@article{li2024memory,
  title={Memory efficient optimizers with 4-bit states},
  author={Li, Bingrui and Chen, Jianfei and Zhu, Jun},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}


@article{dettmers20218,
  title={8-bit optimizers via block-wise quantization},
  author={Dettmers, Tim and Lewis, Mike and Shleifer, Sam and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2110.02861},
  year={2021}
}


@article{he2024distributed,
  title={Distributed Bilevel Optimization with Communication Compression},
  author={He, Yutong and Hu, Jie and Huang, Xinmeng and Lu, Songtao and Wang, Bin and Yuan, Kun},
  journal={arXiv preprint arXiv:2405.18858},
  year={2024}
}


@article{liu2024winner,
  title={Winner-take-all column row sampling for memory efficient adaptation of language model},
  author={Liu, Zirui and Wang, Guanchu and Zhong, Shaochen Henry and Xu, Zhaozhuo and Zha, Daochen and Tang, Ruixiang Ryan and Jiang, Zhimeng Stephen and Zhou, Kaixiong and Chaudhary, Vipin and Xu, Shuai and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{huang2024cedas,
  title={Cedas: A compressed decentralized stochastic gradient method with improved convergence},
  author={Huang, Kun and Pu, Shi},
  journal={IEEE Transactions on Automatic Control},
  year={2024},
  publisher={IEEE}
}

@article{xia2024chain,
  title={Chain of lora: Efficient fine-tuning of language models via residual learning},
  author={Xia, Wenhan and Qin, Chengwei and Hazan, Elad},
  journal={arXiv preprint arXiv:2401.04151},
  year={2024}
}

@article{zi2023delta,
  title={Delta-lora: Fine-tuning high-rank parameters with the delta of low-rank matrices},
  author={Zi, Bojia and Qi, Xianbiao and Wang, Lingzhi and Wang, Jianan and Wong, Kam-Fai and Zhang, Lei},
  journal={arXiv preprint arXiv:2309.02411},
  year={2023}
}

@article{meng2024periodiclora,
  title={Periodiclora: Breaking the low-rank bottleneck in lora optimization},
  author={Meng, Xiangdi and Dai, Damai and Luo, Weiyao and Yang, Zhe and Wu, Shaoxiang and Wang, Xiaochen and Wang, Peiyi and Dong, Qingxiu and Chen, Liang and Sui, Zhifang},
  journal={arXiv preprint arXiv:2402.16141},
  year={2024}
}

@article{jiang2024mora,
  title={MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning},
  author={Jiang, Ting and Huang, Shaohan and Luo, Shengyue and Zhang, Zihan and Huang, Haizhen and Wei, Furu and Deng, Weiwei and Sun, Feng and Zhang, Qi and Wang, Deqing and others},
  journal={arXiv preprint arXiv:2405.12130},
  year={2024}
}

@article{li2022low,
  title={Low dimensional trajectory hypothesis is true: Dnns can be trained in tiny subspaces},
  author={Li, Tao and Tan, Lei and Huang, Zhehao and Tao, Qinghua and Liu, Yipeng and Huang, Xiaolin},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={45},
  number={3},
  pages={3411--3420},
  year={2022},
  publisher={IEEE}
}

@article{cosson2023low,
  title={Low-Rank Gradient Descent},
  author={Cosson, Romain and Jadbabaie, Ali and Makur, Anuran and Reisizadeh, Amirhossein and Shah, Devavrat},
  journal={IEEE Open Journal of Control Systems},
  year={2023},
  publisher={IEEE}
}


@inproceedings{jadbabaie2023adaptive,
  title={Adaptive Low-Rank Gradient Descent},
  author={Jadbabaie, Ali and Makur, Anuran and Reisizadeh, Amirhossein},
  booktitle={2023 62nd IEEE Conference on Decision and Control (CDC)},
  pages={3315--3320},
  year={2023},
  organization={IEEE}
}

@article{hao2024flora,
  title={Flora: Low-Rank Adapters Are Secretly Gradient Compressors},
  author={Hao, Yongchang and Cao, Yanshuai and Mou, Lili},
  journal={arXiv preprint arXiv:2402.03293},
  year={2024}
}

@article{lin2024nora,
  title={NoRA: Nested Low-Rank Adaptation for Efficient Fine-Tuning Large Models},
  author={Lin, Cheng and Li, Lujun and Li, Dezhi and Zou, Jie and Luo, Wenhan and Xue, Wei and Guo, Yike},
  journal={arXiv preprint arXiv:2408.10280},
  year={2024}
}

@article{zhang2022adam,
  title={Adam can converge without any modification on update rules},
  author={Zhang, Yushun and Chen, Congliang and Shi, Naichen and Sun, Ruoyu and Luo, Zhi-Quan},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={28386--28399},
  year={2022}
}

@article{luo2019adaptive,
  title={Adaptive gradient methods with dynamic bound of learning rate},
  author={Luo, Liangchen and Xiong, Yuanhao and Liu, Yan and Sun, Xu},
  journal={arXiv preprint arXiv:1902.09843},
  year={2019}
}


@inproceedings{wang2024provable,
  title={Provable adaptivity of adam under non-uniform smoothness},
  author={Wang, Bohan and Zhang, Yushun and Zhang, Huishuai and Meng, Qi and Sun, Ruoyu and Ma, Zhi-Ming and Liu, Tie-Yan and Luo, Zhi-Quan and Chen, Wei},
  booktitle={Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={2960--2969},
  year={2024}
}

@article{defossez2020simple,
  title={A simple convergence proof of adam and adagrad},
  author={D{\'e}fossez, Alexandre and Bottou, L{\'e}on and Bach, Francis and Usunier, Nicolas},
  journal={arXiv preprint arXiv:2003.02395},
  year={2020}
}

@article{li2024convergence,
  title={Convergence of adam under relaxed assumptions},
  author={Li, Haochuan and Rakhlin, Alexander and Jadbabaie, Ali},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{kozak2019stochastic,
  title={Stochastic subspace descent},
  author={Kozak, David and Becker, Stephen and Doostan, Alireza and Tenorio, Luis},
  journal={arXiv preprint arXiv:1904.01145},
  year={2019}
}


@article{jaiswal2024welore,
  title={From galore to welore: How low-rank weights non-uniformly emerge from low-rank gradients},
  author={Jaiswal, Ajay and Yin, Lu and Zhang, Zhenyu and Liu, Shiwei and Zhao, Jiawei and Tian, Yuandong and Wang, Zhangyang},
  journal={arXiv preprint arXiv:2407.11239},
  year={2024}
}


@article{raffel2020c4,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@article{portes2023mosaicbert,
  title={MosaicBERT: A bidirectional encoder optimized for fast pretraining},
  author={Portes, Jacob and Trott, Alexander and Havens, Sam and King, Daniel and Venigalla, Abhinav and Nadeem, Moin and Sardana, Nikhil and Khudia, Daya and Frankle, Jonathan},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={3106--3130},
  year={2023}
}



@article{xue2020mt5,
  title={mt5: A massively multilingual pre-trained text-to-text transformer},
  author={Xue, L},
  journal={arXiv preprint arXiv:2010.11934},
  year={2020}
}

@article{hoffmann2022scalinglaw,
  title={An empirical analysis of compute-optimal large language model training},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and de Las Casas, Diego and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={30016--30030},
  year={2022}
}




@article{llama3,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}


@article{phi3,
  title={Phi-3 technical report: A highly capable language model locally on your phone},
  author={Abdin, Marah and Aneja, Jyoti and Awadalla, Hany and Awadallah, Ahmed and Awan, Ammar Ahmad and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Bao, Jianmin and Behl, Harkirat and others},
  journal={arXiv preprint arXiv:2404.14219},
  year={2024}
}


@article{mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}


@article{gpt4,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec},
  year={2018}
}

@article{chowdhery2023palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={240},
  pages={1--113},
  year={2023}
}


@inproceedings{narayanan2021efficient,
  title={Efficient large-scale language model training on gpu clusters using megatron-lm},
  author={Narayanan, Deepak and Shoeybi, Mohammad and Casper, Jared and LeGresley, Patrick and Patwary, Mostofa and Korthikanti, Vijay and Vainbrand, Dmitri and Kashinkunti, Prethvi and Bernauer, Julie and Catanzaro, Bryan and others},
  booktitle={Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--15},
  year={2021}
}


@article{dao2023flashattention,
  title={Flashattention-2: Faster attention with better parallelism and work partitioning (2023)},
  author={Dao, Tri},
  journal={arXiv preprint arXiv:2307.08691},
  year={2023}
}

@article{dao2022flashattention,
  title={Flashattention: Fast and memory-efficient exact attention with io-awareness},
  author={Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={16344--16359},
  year={2022}
}


@article{chitsaz2024exploring,
  title={Exploring Quantization for Efficient Pre-Training of Transformer Language Models},
  author={Chitsaz, Kamran and Fournier, Quentin and Mordido, Gon{\c{c}}alo and Chandar, Sarath},
  journal={arXiv preprint arXiv:2407.11722},
  year={2024}
}

@inproceedings{markov2023quantized,
  title={Quantized distributed training of large models with convergence guarantees},
  author={Markov, Ilia and Vladu, Adrian and Guo, Qi and Alistarh, Dan},
  booktitle={International Conference on Machine Learning},
  pages={24020--24044},
  year={2023},
  organization={PMLR}
}

@article{wortsman2023stable,
  title={Stable and low-precision training for large-scale vision-language models},
  author={Wortsman, Mitchell and Dettmers, Tim and Zettlemoyer, Luke and Morcos, Ari and Farhadi, Ali and Schmidt, Ludwig},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={10271--10298},
  year={2023}
}

@inproceedings{kim2021bert,
  title={I-bert: Integer-only bert quantization},
  author={Kim, Sehoon and Gholami, Amir and Yao, Zhewei and Mahoney, Michael W and Keutzer, Kurt},
  booktitle={International conference on machine learning},
  pages={5506--5518},
  year={2021},
  organization={PMLR}
}

@article{robert2024ldadam,
  title={LDAdam: Adaptive Optimization from Low-Dimensional Gradient Statistics},
  author={Robert, Thomas and Safaryan, Mher and Modoranu, Ionut-Vlad and Alistarh, Dan},
  journal={arXiv preprint arXiv:2410.16103},
  year={2024}
}

@article{chen2024fira,
  title={Fira: Can We Achieve Full-rank Training of LLMs Under Low-rank Constraint?},
  author={Chen, Xi and Feng, Kaituo and Li, Changsheng and Lai, Xunhao and Yue, Xiangyu and Yuan, Ye and Wang, Guoren},
  journal={arXiv preprint arXiv:2410.01623},
  year={2024}
}


@article{chl+24,
  title={Fast Gradient Computation for RoPE Attention in Almost Linear Time},
  author={Chen, Yifang and Huo, Jiayan and Li, Xiaoyu and Liang, Yingyu and Shi, Zhenmei and Song, Zhao},
  journal={arXiv preprint arXiv:2412.17316},
  year={2024}
}

@article{cll+24,
  title={The computational limits of state-space models and mamba via the lens of circuit complexity},
  author={Chen, Yifang and Li, Xiaoyu and Liang, Yingyu and Shi, Zhenmei and Song, Zhao},
  journal={arXiv preprint arXiv:2412.06148},
  year={2024}
}

@article{gsx23,
  title={In-context learning for attention scheme: from single softmax regression to multiple softmax regression via a tensor trick},
  author={Gao, Yeqi and Song, Zhao and Xie, Shenghao},
  journal={arXiv preprint arXiv:2307.02419},
  year={2023}
}

@article{as23,
  title={Fast attention requires bounded entries},
  author={Alman, Josh and Song, Zhao},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={63117--63135},
  year={2023}
}


@article{ssz23_tradeoff,
  title={A mathematical abstraction for balancing the trade-off between creativity and reality in large language models},
  author={Sinha, Ritwik and Song, Zhao and Zhou, Tianyi},
  journal={arXiv preprint arXiv:2306.02295},
  year={2023}
}

@article{lssz24,
  title={Tensor attention training: Provably efficient learning of higher-order transformers},
  author={Liang, Yingyu and Shi, Zhenmei and Song, Zhao and Zhou, Yufa},
  journal={arXiv preprint arXiv:2405.16411},
  year={2024}
}


@article{lls+24,
  title={Conv-basis: A new paradigm for efficient attention inference and gradient computation in transformers},
  author={Liang, Yingyu and Liu, Heshan and Shi, Zhenmei and Song, Zhao and Yin, Junze},
  journal={arXiv preprint arXiv:2405.05219},
  year={2024}
}

@article{sxy23,
  title={The Expressibility of Polynomial based Attention Scheme},
  author={Song, Zhao and Xu, Guangyi and Yin, Junze},
  journal={arXiv preprint arXiv:2310.20051},
  year={2023}
}

@article{swy23,
  title={A unified scheme of resnet and softmax},
  author={Song, Zhao and Wang, Weixin and Yin, Junze},
  journal={arXiv preprint arXiv:2309.13482},
  year={2023}
}

@article{gswy23,
  title={A fast optimization view: Reformulating single layer attention in llm based on tensor and svm trick, and solving it in matrix multiplication time},
  author={Gao, Yeqi and Song, Zhao and Wang, Weixin and Yin, Junze},
  journal={arXiv preprint arXiv:2309.07418},
  year={2023}
}

@inproceedings{gsy23_hyper,
  title={An iterative algorithm for rescaled hyperbolic functions regression},
  author={Gao, Yeqi and Song, Zhao and Yin, Junze},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  year={2025}
}


@article{gsy23_coin,
  title={GradientCoin: A Peer-to-Peer Decentralized Large Language Models},
  author={Gao, Yeqi and Song, Zhao and Yin, Junze},
  journal={arXiv preprint arXiv:2308.10502},
  year={2023}
}


@inproceedings{syz23,
  title={Solving attention kernel regression problem via pre-conditioner},
  author={Song, Zhao and Yin, Junze and Zhang, Lichen},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={208--216},
  year={2024},
  organization={PMLR}
}

@article{lswy23,
  title={Local Convergence of Approximate Newton Method for Two Layer Nonlinear Regression},
  author={Li, Zhihang and Song, Zhao and Wang, Zifan and Yin, Junze},
  journal={arXiv preprint arXiv:2311.15390},
  year={2023}
}

@inproceedings{gsyz23,
  title={Low Rank Matrix Completion via Robust Alternating Minimization in Nearly Linear Time},
  author={Yuzhou Gu and Zhao Song and Junze Yin and Lichen Zhang},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024},
  url={https://openreview.net/forum?id=N0gT4A0jNV}
}

@inproceedings{syyz23_weighted,
  title={Efficient Alternating Minimization with Applications to Weighted Low Rank Approximation},
  author={Song, Zhao and Ye, Mingquan and Yin, Junze and Zhang, Lichen},
  booktitle={The Thirteenth International Conference on Learning Representations},
  year={2025}
}


@article{zcy23,
  title={Adaptive Liquidity Provision in Uniswap V3 with Deep Reinforcement Learning},
  author={Zhang, Haochen and Chen, Xi and Yang, Lin F},
  journal={arXiv preprint arXiv:2309.10129},
  year={2023}
}


@article{zcz+24,
  title={Statistical guarantees for lifelong reinforcement learning using pac-bayesian theory},
  author={Zhang, Zhi and Chow, Chris and Zhang, Yasi and Sun, Yanchao and Zhang, Haochen and Jiang, Eric Hanchen and Liu, Han and Huang, Furong and Cui, Yuchen and Padilla, Oscar Hernan Madrid},
  journal={arXiv preprint arXiv:2411.00401},
  year={2024}
}


@inproceedings{llwy24,
  title={Uniform Last-Iterate Guarantee for Bandits and Reinforcement Learning},
  author={Liu, Junyan and Li, Yunfan and Wang, Ruosong and Yang, Lin},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024}
}

@inproceedings{ly24,
  title={On the Model-Misspecification in Reinforcement Learning},
  author={Li, Yunfan and Yang, Lin},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={2764--2772},
  year={2024},
  organization={PMLR}
}

@inproceedings{lwcy23,
  title={Low-switching policy gradient with exploration via online sensitivity sampling},
  author={Li, Yunfan and Wang, Yiran and Cheng, Yu and Yang, Lin},
  booktitle={International Conference on Machine Learning},
  pages={19995--20034},
  year={2023},
  organization={PMLR}
}


@inproceedings{zhang2021sample,
  title={Sample efficient reinforcement learning with REINFORCE},
  author={Zhang, Junzi and Kim, Jongho and O'Donoghue, Brendan and Boyd, Stephen},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={35},
  number={12},
  pages={10887--10895},
  year={2021}
}


@inproceedings{zakharenkov2021deep,
  title={Deep reinforcement learning with dqn vs. ppo in vizdoom},
  author={Zakharenkov, Anton and Makarov, Ilya},
  booktitle={2021 IEEE 21st international symposium on computational intelligence and informatics (CINTI)},
  pages={000131--000136},
  year={2021},
  organization={IEEE}
}


@inproceedings{engstrom2019implementation,
  title={Implementation matters in deep rl: A case study on ppo and trpo},
  author={Engstrom, Logan and Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Janoos, Firdaus and Rudolph, Larry and Madry, Aleksander},
  booktitle={International conference on learning representations},
  year={2019}
}