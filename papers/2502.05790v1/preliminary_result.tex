\section{Experiments}
\subsection{Experiment Setting}
\paragraph{Pre-training on C4 Dataset.}
C4 \cite{raffel2020c4}, short for Colossal Clean Crawled Corpus, is a large-scale and open-source text data that are widely used in practice for pre-training transformer models, e.g., BERT \cite{portes2023mosaicbert}, T5 \cite{xue2020mt5}, and GPT-models. C4 is also widely used in memory efficient optimization community to evaluate the performance of memory-efficient optimizer\cite{hao2024flora, zhao2024galore, zhang2024q-galore, he2024subspace}. In our experiment, we pretrain Llama model with different sizes on C4 dataset without data repetition over a sufficient amount of data\cite{hoffmann2022scalinglaw}. 

\paragraph{Architecture and Hyperparameters} 
We evaluate different optimizers' performance on Llama with 60m, 130m, 350m, and 1.1B parameters. We adopt the same architecture as in \cite{zhao2024galore}. The hyperparameters of our method is set the same as in \cite{zhao2024galore}. 


We compare our method with existing memory efficient methods listed as below:

\paragraph{Full-Rank Adam}
Full-rank Adam refer to Adam optimizer proposed by \cite{kingma2014adam}. It is referred as full-rank Adam because it does not apply any compression to gradients or optimizer states. Full-rank Adam is taken as the baseline for all memory efficient methods.

\paragraph{Original GaLore}
\cite{zhao2024galore} is a memory efficient optimizer relying on SVD and low-rank projection. GaLore uses top r singular vectors as the basis for the low-rank subspace. This paper uses the same hyperparameters of original GaLore as in \cite{zhao2024galore}.


\paragraph{SLTrain}
\cite{han2024sltrain} proposes to parameterize the weight in LLM as the sum of a sparse matrix and a low rank dense matrix for memory efficient pre-training. Specifically, $W = BA + S$, where $W \in \mathbb{R}^{m \times  n}$, $B \in \mathbb{R}^{m \times  r}$ and $A \in \mathbb{R}^{r \times n}$ are two trainable low-rank matrices, $S \in \mathbb{R}^{m \times n }$ is a sparse matrix with a fixed sparse pattern but trainable non-zero values. The hyperparameters in SLTrain used in this paper is the same as those used in \cite{han2024sltrain}.  
\paragraph{GoLore}
GoLore \cite{he2024subspace} proposes to use gradient low-rank random projection to guarantee the convergence of GaLore. In practice, GoLore run GaLore for the first 20\% steps, and then applies random low-rank projection to gradient for the rest 80\% steps. The hyperparameters used for GoLore is the same as in \cite{he2024subspace}. 


\input{tables/c4_main}