\section{Introduction}


Large language models (LLMs), pretrained on next-token prediction tasks, achieve human-level text generation capabilities and exhibit zero-shot transferability to various downstream tasks \cite{brown2020language}. They are also fine-tuned or aligned with human preferences to be expert in downstream tasks \cite{touvron2023llama, ouyang2022training}. Over the past few years, there has been rapid progress in LLM development, characterized by consistent growth in the number of trainable parameters and the scale of datasets \cite{gpt4, mistral, llama3, phi3}. The parameter count in language models has increased from 100 million \cite{radford2018improving} to over a hundred billion \cite{chowdhery2023palm}. However, despite their enhanced expressiveness, such large models demand extensive GPU memory for pretraining \cite{narayanan2021efficient}.
Thus, a critical question arises:
\begin{center} {\it How can we improve the memory efficiency of LLM pretraining? } \end{center}



In LLM pretraining, Adam is commonly used as the optimizer due to its superior optimization performance. However, a key limitation of Adam is its memory requirement, as it necessitates storing two optimizer states, each consuming as much memory as the model itself. This poses a significant challenge, given the substantial memory demands of the model's parameters.
To address this issue, researchers have explored low-rank optimization, where gradients are projected onto a low-rank subspace to reduce the memory consumption of optimizer states. These states are then projected back to their original size when updating the weights. For example, GaLore \cite{zhao2024galore} and Q-GaLore \cite{zhang2024q-galore} project gradients onto subspaces defined by the leading singular vectors corresponding to the largest singular values, a technique referred to as the dominant subspace. FLora \cite{hao2024flora} and GoLore \cite{he2024subspace}, on the other hand, utilize unbiased random low-rank projections for gradients, employing the Johnson–Lindenstrauss transform. Grass \cite{muhamed2024grass} introduces sparse low-rank projections, which further reduce the gradient memory footprint as well as the computation and communication costs compared to dense low-rank projections. Lastly, Fira \cite{chen2024fira} builds on GaLore by fully leveraging the error in gradient low-rank approximation to achieve improved performance.


These methods are powerful because: 1)the gradients of LLMs during pretraining exhibit an intrinsic low-rank structure, making them well-suited for compression using low-rank approximation, and 2) low-rank approximation can be applied not only to Adam but also to other optimizers that use state information.
For instance, Adafactor \cite{shazeer2018adafactor} employs rank-1 factorization on the second moment in Adam to reduce the memory required for storing the second moment. Adam-mini \cite{zhang2024adam-mini} eliminates over 99\% of the effective learning rate in the second moment of Adam while achieving performance on par with—or even better than—Adam. Additionally, \cite{dettmers20218} and \cite{li2024memory} propose low-precision optimizers with 8-bit and 4-bit optimizer states.
Low-rank optimization integrates seamlessly with these Adam variants, further highlighting its importance and underscoring why it deserves significant attention.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\linewidth]{first_figure.pdf}
    \caption{Adjacent subspace overlap of low-rank optimizer using difference subspace selection methods. Importance sampling subspace selection can lower the overlap between adjacent subspaces, thus it enables better exploration in more different subspaces in optimization trajectory.}
    \label{fig:first_figure}
\end{figure} 


A central question in low-rank optimization is how to maintain the performance of pretrained LLMs while using memory-efficient optimizers, as compared to full-rank optimization.
One common paradigm in existing low-rank optimization methods is to update weights within the dominant subspace for a certain number of iterations and periodically update this dominant subspace. Nonetheless, the dominant subspaces of gradients in many layers stabilize almost completely after the early stages of pretraining \cite{zhang2024q-galore}. Consequently, the weight updates during different periods predominantly remain within the same low-rank subspace, resulting in cumulative weight updates that struggle to achieve high rank. This limitation significantly hampers the language modeling capabilities of pretrained LLMs.
Thus, it is natural to ask:
\begin{center}
    {\it Is it possible to overcome the low-rank bottleneck of existing low-rank optimization methods without introducing additional overhead?}
\end{center}

In this paper, we provide a positive answer to this question.  
We propose a novel method for subspace selection in low-rank optimization by introducing an appropriate degree of randomness in the selection process. In summary, the contributions of this study are as follows:
\begin{itemize}
    \item We observe that highly similar adjacent subspaces in existing low-rank optimization methods diminish the diversity of weight updates, degrading the performance of pretrained LLMs.  
    \item To address the low-rank bottleneck in existing low-rank optimization methods, we propose a novel subspace selection method called \emph{importance sampling subspace selection} (I3S). This method enables low-rank optimizers to explore a broader range of subspaces in the optimization trajectory. Specifically, the low-rank subspace is spanned by $r$ singular vectors sampled from $m$ singular vectors for a gradient $G \in \mathbb{R}^{m \times n}$. Figure~\ref{fig:first_figure} illustrates how I3S reduces the overlap between adjacent subspaces during LLM pretraining.  
    \item I3S can be integrated with various low-rank optimization methods, such as GaLore and Fira. It is robust to second-moment factorization and low-precision optimizer state storage. On pretraining tasks for the LLaMA model at different sizes, I3S consistently outperforms dominant subspace selection and reduces the performance gap between low-rank optimizers and full-rank Adam by up to 46.05\%.  
    \item From a theoretical aspect, analyzing I3S's convergence is challenging, because the analysis of weighted sampling without replacement is unwieldy. Therefore, we make a mathematically tractable version of I3S called hybrid subspace selection.  
    We prove that hybrid subspace selection achieves a similar convergence rate as GoLore \cite{he2024subspace} (Theorem~\ref{convergence_our_method_informal} and Theorem~\ref{thm:he2024subspace}) whereas delivering better empirical results (Figure~\ref{fig:i3s_and_jl}). Furthermore, we find that  the tunable parameter $r_0 \in \mathbb{Z}$ in the hybrid subspace selection represents the trade-off between theoretical convergence rate and empirical training stability.
\end{itemize}
