\section{Related Work}
\subsection{Multi-Kernel Learning}
Kernel functions are highly effective in capturing nonlinear patterns in data. By utilizing the kernel trick, these algorithms implicitly learn nonlinear features in a reproducing kernel Hilbert space (RKHS), even in high-dimensional or infinite-dimensional spaces, while keeping the computation tractable. Multiple kernel learning (MKL) is an approach that combines and selects appropriate kernels for various learning tasks such as classification, clustering, and dimensionality reduction ____. In recent years, MKL has been extensively researched, resulting in the development of numerous algorithms aimed at enhancing learning efficiency through various optimization techniques ____, as well as improving prediction and classification accuracy by exploring different combinations of base kernels ____.


Within the domain of multi-omics fusion, the utilization of Multiple Kernel Learning (MKL) offers a resilient framework. A multi-dimensional space, formed by the combination of several kernel functions, integrates various feature spaces. The composite space effectively utilizes the different feature mapping capabilities of each subspace, facilitating the integration of heterogeneous data originating from several sources. The ideal single-kernel function individually maps each feature, resulting in a combined space that provides a more accurate and appropriate representation. ____ conducted a study on Alzheimer's Disease (AD) identification, wherein they expanded the application of Support Vector Machine (SVM) to the Multi-Kernel Learning (MKL) version. The authors utilized multi-modality data in their research. In this study, kernels were generated utilizing magnetic resonance imaging (MRI), positron emission tomography (PET), and cerebrospinal fluid (CSF) modalities. Similarly, ____ employed the "simpleMKL" classifier  to differentiate between individuals with Alzheimer's disease (AD), mild cognitive impairment (MCI), and healthy controls (HCs) using local diffusion tensor imaging (DTI) and magnetic resonance imaging (MRI) modalities. 
However, the initial complex NPC data samples pose some difficulties. More specifically, feature mapping in high-dimensional space does not easily improve the performance of subsequent classification tasks. Typically, in the context of classification problems, multi-Kernel learning focuses on acquiring a transformation matrix that can effectively convert the kernel combination into a matrix of binary labels. The performance of general classifiers is sometimes hindered when used with high-dimensional data due to the inflexibility and limited adaptability of label-fitting methods.


\iffalse
For multi-omics fusion, MKL provides a natural framework. The high-dimensional space created by multiple kernel functions is a combined space that combines multiple feature spaces. This combined space can combine each subspace's different feature mapping capabilities and combine different heterogeneous data from multiple sources. The most suitable single-kernel function maps the features individually, and the data can be more accurately and reasonably expressed in the new combined space. For example, in ____, Zhang et al. extended SVM to the MKL version for Alzheimer’s disease (AD) recognition based on multi-modality data in which kernels are constructed in MRI, PET, and CSF modalities, respectively. Ahmed et al. employed “simpleMKL” ____ as the classifier to classify AD, MCI, and HCs based on local DTI and MRI modalities. However, for the original complex NPC data samples, the feature mapping process in high-dimensional space cannot benefit downstream classification tasks. Typically, when multi-Kernel learning is used for classification tasks, it aims to learn a transformation matrix that can transform the combination of Kernels into a binary label matrix. Due to the label fitting being very strict and having little freedom, the general classifier can not perform well when using the high-dimension features. 
\fi

\subsection{Label Softening}
Graph embeddings, an innovative approach drawn from manifold learning, present a promising means to ensure samples bearing identical labels stay proximal post-transformation. This method captures the core data structures effectively while tackling the prevalent issue of overfitting. To combat this, we introduce a 'label blur softening' approach that offers a more flexible binary label matrix. This strategy seeks a new dimensional space where the transformed samples retain the intrinsic geometry of their original counterparts ____. Yan et al. ____ put forth a margin Fisher analysis method, adeptly leveraging label information to uphold both the natural geometric and discriminant structures within the sample. Other methodologies of note include Local Fisher Discriminant Analysis ____, Locality Sensitive Discriminant Analysis ____, and Local Discriminative Embedding ____. These approaches share the common objective of ensuring the preservation and usability of geometric and discriminant structures in their newly crafted space.

Semi-supervised learning frequently uses an adjacency graph to encapsulate structure and guarantee label consistency among comparable samples ____. However, this technique primarily captures the samples' local structure, thereby curbing its efficiency. To ensure that like-class samples retain proximity in the transformed space, we advocate employing sample affinity to capture the distribution of same-class samples. We introduce the notion of a 'class compactness graph', leveraging label information to connect two samples of the same class via an undirected edge. This strategy ensures the cohesiveness of same-class samples in the transformed space, expanding the gap between disparate categories while providing label-fitting flexibility. This substantially enhances generalization capability and alleviates overfitting concerns. Employing class compactness graphs enables samples—including those distorted by noise—to retain close proximity upon transformation into their label space, thus maintaining a consistent class label. This strategy fosters the creation of an enhanced classifier.

 

This study presents a label-softening technique within the framework of multi-kernel mapping. After performing high-dimensional mapping, a non-negative label relaxation matrix enables the conversion of a strict binary label matrix into a more flexible slack variable matrix. This transformation effectively mitigates the issue of overfitting, resulting in improved model adaptability. This approach is accomplished by creating class compactness graphs, as seen in Figure \ref{angle}. The solution we propose has two main benefits: firstly, it optimizes the margin between distinct classes to a significant degree; secondly, it offers enhanced adaptability for improved label fitting. By employing a class compactness map, we can preserve the proximity of instances belonging to the same class inside the changed space, successfully mitigating the issue of overfitting. The efficiency of Multi-Kernel Label Softening (MK-FLS) is demonstrated through experimental testing.

\iffalse
This study presents a label-softening technique for multi-kernel mapping. By incorporating a non-negative label relaxation matrix following high-dimensional mapping, we transform the rigid binary label matrix into a more flexible slack variable matrix, thereby significantly curtailing the overfitting issue. This is accomplished via the construction of class compactness graphs (illustrated in Fig.\ref{angle}). Our proposed technique provides two primary benefits: firstly, it maximizes the margin between diverse classes to the greatest possible extent; secondly, it affords enhanced flexibility for superior label fitting. Through the application of a class compactness map, we ensure that samples from the same class maintain proximity in the transformed space, effectively mitigating overfitting. Extensive experimental trials corroborate the marked efficacy of our Multi-Kernel Fuzzy Label Softening (MK-FLS) approach.
\fi
% width=4cm, height=4cm
\begin{figure}[ht]
  \centering              
    \includegraphics[width=0.3\textwidth]{sample.png}
    \caption{Contralateral parotid glands shown in planning CT images in an example NPC patient.}
    \label{organ}
    \vspace{-3mm}
\end{figure}

\begin{table*}[ht]
\centering
\caption{The statistic of NPC-ContraParotid dataset. The collected images and data are collectively referred to as NPC-ContraParotid, where the dataset includes CT image, MR image, and Dose information. We combined these three different modality permutations to form four sub-datasets. Each sub-dataset is named by the modality type it is composed of.}
% \resizebox{linewidth}{!}{
\begin{tabular}{cccccrr}
\toprule[1pt]
Organ                         &Datasets&MRI&Dose&CECT&Features&Size\\ 
\hline
\multirow{4}{*}{ContraParotid}&CTD      &   &$\checkmark$&$\checkmark$&10&102\\
                              &MRICT      &$\checkmark$& &$\checkmark$&7&102\\
                              &MRID      &$\checkmark$&$\checkmark$& &11&102 \\
                              &MRICTD      &$\checkmark$&$\checkmark$&$\checkmark$&14&102\\
\toprule[1pt]
\end{tabular}
% }
\label{statistic}
\end{table*}