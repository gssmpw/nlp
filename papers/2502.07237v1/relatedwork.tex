\section{Related Work}
\label{sec:related}










\subsection{Imitation learning}



Imitation learning (IL) is a technique where an agent learns by mimicking an expert's actions. IL outperforms pure RL by reducing the complexity and sparsity of the search space~\citep{liu2023active}. Offline IL methods, like behavioral cloning~\citep{pomerleau1988alvinn}, require a dataset of expert trajectories but can lead to errors in the learner's policy. In contrast, interactive IL methods, such as DAgger~\citep{ross2011reduction} and AggreVaTe~\citep{ross2014reinforcement}, use Roll-in-Roll-out (RIRO) scheduling, where learners initially follow their policy but switch to expert guidance for trajectory completion. However, these methods assume constant expert availability, which is impractical, and do not allow returning to previous states once a rollout begins. Our work integrates RIRO with {\TOPK and \TOPP sampling}~\citep{liu2024erp}, creating a guide policy be same as learner policy that conducts roll-outs on any state and estimates returns, improving upon traditional RIRO limitations.







\subsection{Reinforcement learning} %

One prominent approach in drug design employs RL \citep{tan2022reinforcement} to maximize an expected reward defined as the sum of predicted property scores as generated by property predictors. 
In terms of representation, existing works in RL for drug design have predominantly operated on SMILES string representations \citep{born2021paccmannrl, guimaraes2017objective, neil2018exploring, olivecrona2017molecular, popova2018deep,  staahl2019deep,tan2022drlinker,wang2022reinforcement, zhang2023universal, zhou2019optimization} or graph-based representations \citep{atance2022novo,gottipati2020learning, jin2020multi,wu2022rlcg,you2018graph}.
\fix{Traditional methods, such as genetic algorithms modified for molecular graphs~\citep{yoshikawa2018population} and Monte Carlo tree search applied to molecular graphs~\citep{jensen2019graph}, have been employed. These studies primarily concentrate on the De Novo drug discovery challenge instead of drug optimization. }
In our research, we have chosen to employ the SMILES representation.
However, previous studies have primarily focused on discovering new drugs, frequently overlooking molecular structure constraints during policy improvement. This oversight can lead to drastic changes in structure or functional groups, making most of the generated compounds unsynthesizable. In contrast, our work concentrates on optimizing existing drugs while preserving their beneficial properties, rather than creating entirely new ones from scratch.
\fix{MIMOSA and DrugEx v3 ~\citep{fu2021mimosa, liu2023drugex} represents the most recent approaches based on graph structures for drug optimization. However, it falls short of finetuning capability for drug optimization, an issue that our work has successfully addressed.}










\subsection{RL finetuning}
{Finetuning the generator model is critical to achieve drug improvement.
Prior RL finetuning methodologies aimed at aligning models with feedback from both humans (RLHF~\citep{bai2022training,christiano2017deep, ibarz2018reward,touvron2023llama}) and AI (RLAIF~\citep{bai2022constitutional,leike2018scalable}), which have recently found applications in the fine-tuning of language models for tasks like text summarization~\citep{ bohm2019better, stiennon2020learning, wu2021recursively, ziegler2019fine}, dialogue generation~\citep{hancock2019learning, jaques2019way,  yi2019towards}, 
and language assistance~\citep{bai2022training}. 
A core feature of RLHF and RLAIF lies in training a reward model {or make direct policy improvement} from the comparison feedback, {such as Rank Responses to align Human Feedback~(RRHF)~\citep{yuan2023rrhf}, Reward Ranked Finetuning~(RAFT)~\citep{dong2023raft}, Preference Ranking Optimization~(PRO)~\citep{song2023preference}, and Direct Preference Optimization~(DPO)~\citep{rafailov2023direct}.}}
Differing from previous works, our approach does not rely solely on feedback from a single human or AI model; instead, we engage multiple critics to evaluate the advantage preference of the generated vs.\ original drug based on comprehensive assessments, including factors like solubility. 
Moreover, we make direct policy improvement by using the advantage preference {in standard RL} {instead of binary feedback}.

























\subsection{{LLMs for drug optimization}}


Large language models (LLMs) have been employed in molecule generation~\citep{bagal2021molgpt, rothchild2021c5t5, frey2023neural} and drug discovery~\citep{bran2023transformers, liu2024erp}. In contrast, our work focuses on drug optimization, which requires maintaining the original drug's beneficial structure and properties rather than designing from scratch.
A notable work in the drug optimization domain is REINVEN\fix{T} 4~\citep{he2021molecular, he2022transformer, loeffler2024reinvent}, which has developed transformer-based generative models with a strong focus on pretraining. However pretraining facilitates the generation of molecules similar to those in the training dataset, it also inherently limits the scope of exploration due to biases present in the training data. {Furthermore, REINVENT 4 categorizes the features, resulting in insensitivity to numerical changes, and the molecules generated lack optimization for specific desired objectives, such as drug-likeness, among others.}
In contrast, {\fwname} employed LLMs as the generative model and refines the generation process further through the \algname algorithm to guarantee the improved property in the optimized drug compared to original drug.