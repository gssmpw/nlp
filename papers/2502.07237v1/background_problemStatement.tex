\section{Preliminaries}



\paragraph{Markov decision process.} We consider a finite-horizon Markov Decision Process (MDP) $\MDP_0=\langle\stateSpace,\actionSpace,\transDynamics,\RFunc, \horizon\rangle$ with state space $\stateSpace$, action space $\actionSpace$, deterministic transition dynamics $\transDynamics: \stateSpace \times \actionSpace \rightarrow \stateSpace'$, unknown reward function $\RFunc: \stateSpace \times \actionSpace \rightarrow \bracket{0,1}$, and horizon $\horizon$. We assume access to a set of $K$ critics each represents a domain experts, defined as 
${\mathbf{\critic}}=\curlybracket{\critic^k}_{k=1}^\CNum$, {where}
$\critic: \state_{\horizon} \rightarrow \mathbb{R}$ {and} $\state_{\horizon}$ represents a final state. The policy $\policy:\stateSpace \rightarrow \actionSpace$ maps the current state to a distribution over actions. 
{Given an initial state distribution $\rho_0 \in \Delta(\stateSpace)$, we define $\stateDist_t^{\policy}$ as the distribution over states at time $t$ under policy $\pi$.}
The goal is to train a policy to maximize the expected long-term reward.  The quality of the policy can be measured by the $\QFunc$-value function $\QFunc^{\policy}:\stateSpace \times \actionSpace \rightarrow \mathbb{R}$ is defined as:
$\QFunc^{\policy}\paren{\state,\action}:= \mathbb{E}^{\policy}\bracket{\sum_{t=0}^{\horizon}\RFunc\paren{\state_t,\action_t}|\state_0=\state,\action_0=\action}$,
where the expectation is taken over the trajectory following $\policy$, and the value function is noted as:
$\VFunc^{\policy}\paren{\state}:= \expctover{\action\sim\policy\paren{\cdot|\state}}{\QFunc^{\policy}\paren{\state,\action}}$.


{

\textbf{LLM.}
Each training corpus includes a start token $\bracket{\text{BOS}}$, a sequence of tokens $\mathbf{y}$ where each $y_i \in \Vocabulary$, and a termination action $\bracket{\text{EOS}}$. Here, each action $\action \in \actionSpace$ is represented as a token $y$ in the Transformer's vocabulary $\mathcal{V}$, with $\mathcal{V} := \actionSpace$. Each molecule is represented by a sequence of tokens $\mathbf{y}$ to construct a SMILES~\citep{weininger1988smiles} string, and this applies to both partial and complete molecules.
Let  $\circ$ represents string concatenation, and let $\mathcal{V}^*$ denote the Kleene closure of $\mathcal{V}$.
We define the set of complete training corpus as: 
\begin{equation}
    \corpus := \curlybracket{\text{[BOS]} \circ \mathbf{v} \circ \text{[EOS]}~|~\mathbf{v}\in \mathcal{V}^*}.
\end{equation}
The LLM generator policy $\policy_{\theta}$, which is parameterized by a deep neural network (DNN) with learned weights $\theta$, is defined as a product of probability distributions:
$\policy_{\theta}\paren{\mathbf{y}|\mathbf{x}}=\prod_{t=1}^{|\mathbf{y}|} \policy_{\theta}\paren{y_t|\mathbf{x},\mathbf{y}_{<t}}$, where  $\policy_{\theta}\paren{y_t|\mathbf{x},\mathbf{y}_{<t}}=P\paren{y_{t}|\mathbf{y}_{<t},X}$ is a distribution of next token $y_t$, $\mathbf{y}_{<t} = \bracket{y_1, \cdots, y_{t-1}}$, and $\mathbf{x}$ represents an input sequence (prompt).
The decoding process in text generation is designed to identify the most probable hypothesis from all potential candidates by resolving the following optimization problem:
\begin{equation}
\mathbf{y}^{\star}=\argmax_{\mathbf{y}\in \hypothesis_{\horizon}} \log \policy_{\theta}\paren{\mathbf{y}|\mathbf{x}}. 
\end{equation}
{To estimate the expected reward for a partial molecule, we employ 
\TOPPK \citep{liu2024erp} to navigate the exponentially vast search space to form a complete valid molecule}.
For sampling the token $y_i\sim\TOPPK\paren{\mathbf{y}_{<i},p,k}|_{\mathbf{x}}$,
where
\TOPPK 
generates the sequence by recursively picking the top candidates at each step $i$ according to 
\begin{gather}\label{eq:toppk}
    \text{\TOPPK}\paren{\mathbf{y}_{<i}, p,k}|_{\mathbf{x}}= \actionSpace_{\mathbf{y}_{<i}},\\
    \text{~where~} \actionSpace_{\mathbf{y}_{<i}}=\curlybracket{y^1,\ldots,y^{j}} \in \mathcal{V}^j, \text{~and}\\ \notag
     j= \min \curlybracket{\argmin_{j'}\sum_{\fix{l}=1}^{j'}\policy_{\theta}\paren{y^{\fix{l}}|\mathbf{x},\mathbf{y}_{<i}} \geq p, \: k},  \notag
\end{gather}
where the candidates $y^1, \dots, y^{j'}, \dots, y^{|\mathcal{V}|} \in \mathcal{V}$ are indexed by descending order of $\pi_\theta(\cdot \given \mathbf{x},\mathbf{y}_{<i})$, $p\in (0,1]$ denotes the 
\fix{cumulative probability threshold} 
and $k$ represents the maximum number of candidates for the next tokens.
$\bestN$~\citep{gao2023scaling} sampling technique at inference time generates $N$ samples which are then ranked by the reward model. Then top ranked candidate is selected, which can expressed as 
\begin{align}
{\bestN}\paren{\mathbf{y}_{<i},N,\RFunc}|_{\textbf{x},p,k}= \max_{\mathbf{Y}_{j}\in \curlybracket{\mathbf{Y}_1,\cdots,\mathbf{Y}_{N}}} \RFunc\paren{ \mathbf{Y}_{j}},\\
\text{where } Y_j=\bracket{\mathbf{y}_{<i},y_i,\cdots,y_{\horizon}}_j,\\ \text{and } y_i \sim \TOPPK\paren{\mathbf{y}_{<i},p, k}|_{\mathbf{x}}.
\end{align}\label{eq:bestN}
}
\paragraph{Drug {optimization}.} 
We formalize the drug {optimization} problem within the framework of MDP. Given a dataset consisting of real-world structured sequences represented as SMILES~\citep{weininger1988smiles} strings, 
our objective is to train a {LLM-based} generative policy $\Gen$ to generate a high-quality sequence denoted as $\mathbf{y}_{\horizon}=\paren{y_1,\ldots,y_t,\ldots,y_{\horizon}},y_t\in \Vocabulary$, {and aim to outperforming an input sequence X in desired properties}.
The length of the output sequence, denoted as $\horizon$, represents the planning horizon. At time step $t$, the state $\state_{t-1}$ comprises the currently generated tokens $\paren{y_1,\ldots,y_{t-1}}$, and the action $\action$ corresponds to the next token $y_t$ to be selected. While the policy model $\Gen\paren{y_t|\mathbf{y}_{<t}, \fix{X}}$ operates in a stochastic manner, the state transition function $\transDynamics$ becomes deterministic once an action has been chosen. 
To estimate the $\QFunc$ value, we reference the REINFORCE algorithm \citep{williams1992simple}, which we define as 
$\QFunc\paren{\state= 
\mathbf{y}_{<\horizon},\action=y_{\horizon}}=\RFunc\paren{\mathbf{y}_{\horizon}}. $



\paragraph{Limitations of previous work.} 

1) Prior studies concentrated primarily on the discovery of new drugs from the ground up \citep{atance2022novo, popova2018deep, zhang2023universal}. In contrast, we focus on the relatively less explored, yet highly practical and significant, issue of drug optimization.
2) {There is currently no RL finetuning algorithm specifically designed for drug optimization problems.}
{3) The current state-of-the-art model, REINVENT~4, prioritizes pretraining with constrained similarity, thereby restricting its ability to explore molecular spaces with potentially high rewards beyond its training set.
Our drug optimization LLM, together with the Structured Policy Optimization approach, addresses these limitations.}













