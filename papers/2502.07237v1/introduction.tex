\section{Introduction}\label{sec:intro}






The cost of discovering a new drug through conventional approaches is estimated to range from hundreds of millions to billions of dollars~\citep{dickson2009cost}. 
This high cost is due to the lengthy and resource-intensive nature of the drug discovery and development process, which involves multiple stages, including target identification, lead compound identification, preclinical testing, and clinical trials. 
Despite significant efforts, the overall success rate in drug discovery is relatively low, with many drug candidates failing to progress beyond the early stages of development. 
Additionally, the time required to identify an effective drug can vary from several years to over a decade, depending on the complexity of the disease and the efficiency of the drug discovery process.
Such concerns are driving a growing trend towards drug repurposing~\citep{avram2023drugcentral}, which involves using FDA-approved drugs for different diseases instead of developing new drugs from the ground up. Yet despite some successes~\citep{pushpakom2019drug}, the effectiveness of drug repurposing has been limited since the drug is usually designed specifically for treating a particular disease. 
However, the emergence of rapidly evolving virus variants~\citep{hadj2022covid}, such as those associated with SARS-CoV-2~\citep{yuki2020covid}, as well as drug resistant cancer cells~\citep{MANS2017}, has sparked increased interest and  an urgent need to expedite the discovery of effective drugs. 

In this work, we 
propose
a reinforcement learning (RL)-based drug optimization {algorithm} to adapt existing drugs to fast-evolving virus variants and cancer cells, helping to address 
the aforementioned limitations of drug discovery and drug repurposing.  
RL has achieved superhuman performance in domains such as \fix{c}hess~\citep{lai2015giraffe}, video games~\citep{mnih2013playing}, and \fix{r}obotics~\citep{brunke2022safe}.
However, despite promising early results~\citep{born2021paccmannrl, guimaraes2017objective, jin2020multi, neil2018exploring, tan2022drlinker, zhang2023universal}, RL has yet to attain similar levels of performance for complex real-life problems like drug discovery. 

We identified four challenges that have thus far prevented RL from impacting drug design:
1) \textit{Search space complexity}\/: 
An RL algorithm for drug discovery needs to demonstrate both sample and computational efficiency, but the overwhelming complexity of the search space \citep{polya2012combinatorial} renders RL incapable of adequately exploring potential effective actions and states required for policy learning.
2) \textit{Sparse rewards}\/:
In contrast to the continuous reward environment found in popular environments like DeepMind Control Suite~\citep{tassa2018deepmind} or Meta-World~\citep{yu2020meta}, drug generation operates within a sparse reward environment where rewards are only obtainable upon {a complete molecule}.
3) \textit{Complex scoring criteria}\/: 
Generated molecules must fulfill multiple criteria, including solubility and synthesizability, 
while also achieving a high docking score when targeting a specific site.
4) {
\textit{Preservation of original beneficial properties}\/:
Lastly, as drugs with similar chemical structures should exhibit similar biological/chemical effects~\citep{bender2004molecular}, it is crucial to strike a balance between optimizing the drug and preserving the original drug's beneficial properties.
} 


\textbf{Our contributions.} {We present \fwname, a {LLM-based} drug optimization framework designed to improve various properties of an original drug in a robust and efficient manner. Within this workflow, we introduce the \textbf{S}tructured \textbf{P}olicy \textbf{O}ptimization (\textbf{\algname}) algorithm to utilize the advantage preference {of properties improvement} to perform direct policy {optimization}.
\fwname and \algname effectively tackle the challenges outlined above in the following manner:}
\fix{
(1.)  \emph{Designing an LLM for Drug Optimization.} In this study, we develop a large language model (LLM) tailored specifically for drug optimization, incorporating a specialized corpus and custom loss function, among other features.}
{(2.) \emph{Sample complexity, sparsity, and computational efficiency.}
Because of the sparse reward nature of the drug design,
pure RL often finds it challenging to learn a good policy due to the complexity of the search space.
To reduce this complexity, \algname employs an imitation-learning-based approach to {pre-train a LLM-based} generator policy {with desirable}
behavior based on prior experience {of} {designing}
drug SMILES~\citep{weininger1988smiles} {strings}.
\algname also addresses the problem of reward sparsity by {utilizing the \TOPK and \TOPP Beam Search from LLM}
to obtain estimated rewards for intermediate steps. Finally, because calculating the docking score through virtual screening ({such as} OEDOCK~\citep{kelley2015posit}) is computationally costly~\citep{clyde2023ai}, \fwname adopts a transformer-based surrogate model to obtain docking scores more efficiently.
(3.) \emph{Property preserving.} 
To preserve the original drug's beneficial properties,
throughout the optimization process, it is crucial to balance the preservation of the original drug's beneficial properties with the optimization of other chemical attributes. To achieve this, we use Tanimoto similarity as a critic to {maximize} the Tanimoto similarity between the original and generated drugs.
(4.) \emph{Finetuning.}
Our proposed \algname algorithm leverages the advantageous preference of a generated drug over the original drug based on multiple objectives as the policy gradient signal. It performs direct policy improvement on an LLM-based generative model and addresses the sparse reward problem through partial molecule improvement.
}



In summary, our contributions are:



$\bullet$ We introduce the \fwname framework, which includes a ground-up designed LLM tailored for drug optimization, along with a novel RL finetuning algorithm, \algname, designed for drug optimization with theoretical analysis.








$\bullet$ By conducting comprehensive experiments
 by comparing to competing baselines
 {with existing SOTA}
on real world 
{viral} and {cancer target}
proteins, we {demonstrate that \algname outperform existing SOTA baseline} 
while consistently enhances existing molecules/drugs across multiple desired objectives, leading to improved drug candidates. 


$\bullet$ We release a drug optimization dataset comprising 1 million ligands along with their OEDOCK scores to five proteins associated with cancer:
colony stimulating factor 1 receptor (CSF1R) kinase domain (PDB ID: 6T2W),
NOP2/Sun RNA methyltransferase 2 (NSUN2) (AlphaFold derived),
RNA terminal phosphate cyclase B (RTCB) ligase (PDB ID: 7P3B),
and Tet methylcytosine dioxygenase 1 (TET1) (AlphaFold derived),
and Wolf-Hirschhorn syndrome candidate 1 (WHSC1) (PDB ID: 7MDN) and 24 high-affinity binding sites on protein SARS-CoV-2: 3CLPro~(PDBID: 7BQY) virus. \fix{See more details in \secref{app:dataset}.}










