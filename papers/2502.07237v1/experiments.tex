



\section{Experiments}\label{experiments}




{\paragraph{The language model.} 
We utilize the Byte Pair Encoding (BPE) method \citep{gage1994new_bpe, sennrich2015neural_bpe} to initially pre-train our tokenizer using raw SMILES strings and GPT-2-like Transformers for causal language modeling. We train on the standard 11M Drug-like Zinc dataset, excluding entries with empty scaffold SMILES. The dataset is divided into a 90/10 split for training and validation, respectively. (For more details, see \appref{app:pretrain_data}).}



{\paragraph{Baselines.} 
In this study, 
we employ several baseline models, including 
Molsearch \citep{sun2022molsearch}, a search-driven approach leveraging Monte Carlo Tree Search (MCTS) for molecular generation and optimization,  MIMOSA \citep{fu2021mimosa}, a graph-based method for molecular optimization based on sampling and \fix{DrugEx v3 \citep{liu2023drugex}, a scaffold-based drug optimization using transformer-based reinforcement learning}. 
\fix{Moreover, we integrate the state-of-the-art model, Mol2Mol~\citep{he2021molecular, he2022transformer} from REINVENT~4~\citep{loeffler2024reinvent},} 
which trains a transformer to adhere to the Matched Molecular Pair (MMP) guidelines~\citep{tyrchan2017matched}.
Specifically, given a set $\{\{X,Y,Z\}\}$, where $X$ represents the source molecule, $Y$ denotes the target molecule, and $Z$ signifies the property change between $X$ and $Y$, the model learns a mapping from $\{X, Z\} \in \ensuremath{\mathcal{X}} \times \ensuremath{\mathcal{Z}} \implies Y \in \ensuremath{\mathcal{Y}}$ during training.
REINVENT~4 defines six types of property changes for $Z$, including MMP for user-specified alterations, various similarity thresholds, and scaffold-based modifications where molecules share the same scaffold or a generic scaffold.} 












\paragraph{Datasets.}
{Utilizing the latest Cancer and COVID dataset proposed in this paper \fix{(See Appendix \ref{app:dataset}) for RL fine-tuning across all baseline models and our proposed method}, which consists 1 million compounds from the ZINC15 dataset docked to the 3CLPro~(PDB ID: 7BQY) protein associated with SARS-CoV-2 and the RTCB (PDB ID: 4DWQ) human cancer protein.}
This newly proposed dataset is utilized for RL fine-tuning across all baseline models and are not employed in the pretraining phase. 
For pretraining, we rely on molecules from the ZINC database, filtering for Standard, In-Stock, and Drug-Like molecules, resulting in approximately 11 million molecules (See details in Appendix \ref{app:pretrain_data}). We formed molecular pairs from the ZINC dataset, adhering to the guidelines used for creating the pretraining corpora of each baseline method. The specific rules for generating our molecular pairs are outlined in Section \ref{sec:pretrain} of our approach.








\paragraph{Critics and evaluation metric.} 
\fix{In addition to the metrics introduced in section \ref{sec:4.2}, we further added the following two metrics:}
{
\textbf{Average Top 10\% Norm Reward:} It is the average of the normalized reward of the top 10\% of molecules.
}
\fix{\textbf{Average Norm Reward:} It is the average of the normalized values of all metrics across valid molecules. This is the most important metric.} 



\subsection{Experimental results}









        
    

Table \ref{exp:main_result} demonstrates that the \fwname algorithm outperforms all competing baselines, including the original and six variants from the current leading method, REINVENT~4, across most performance metrics for both viral and cancer-related benchmarks. 
It improves diversified properties and
significantly enhancing the critical metric of average normalized reward. 
\fwname excels over REINVENT~4 primarily because REINVENT~4 concentrates on pretraining with restricted similarity and fails to effectively enhance the properties of generated drugs, limiting exploration of potentially high-reward molecular spaces. In contrast, \fwname employs \algname to explore high-reward spaces while maintaining reasonable similarity, increasing the probability of generating sequences with positive advantages and decreasing it for negative ones. Additionally, \algname optimizes both entire and partial molecules, facilitating both global and local optimizations, which results in quicker convergence and improved performance.



\begin{wrapfigure}[9]{r}{.2\textwidth}
    \vspace{-10pt}
    \centering
    \includegraphics[width=\linewidth]{viz/sim.pdf}
    \vspace{-7mm}
    \caption{Tanimoto Similarity over five experimental runs.}\label{fig:exp:sim}
\end{wrapfigure}
\paragraph{\algname adjusting.} 
Note that the performance curve of Tanimoto similarity in~\figref{fig:exp:sim} initially decreases and then increases. This trend aligns ideally with the RL-based molecule generation improvement process.
The initial decrease occurs because \algname relaxes the structural constraints of the original molecule to achieve greater improvement in the diversified properties of the generated molecules. This causes the molecule to deviate from its original structure, leading to a decrease in Tanimoto similarity. 
Subsequently, there is a gradual increase in the trend as the generated molecules reach a decent level of diverse properties and begin optimizing their structure towards that of the original molecule, resulting in an increasing trend in Tanimoto similarity. Finally, the generated molecules not only improve the desired properties but also reduces the likelihood of drastic structural changes that might result in unsynthesizable compounds. 
{This process, illustrated in Figure \ref{fig:exp:sim}, showcases \algname's capability to automatically adjust the optimization of various properties to achieve global optimization.}






















\begin{table*}[t!]
\setlength{\tabcolsep}{4pt}
   \centering
    { %
    \scalebox{0.62}{
    \begin{tabular}{l l l c c c c c c c c c c }
        \toprule
        \textbf{Target} %
        & \textbf{Algorithm}
        & \makecell[c]{\fix{Validity}~$\uparrow$}
        & {\makecell[c]{Avg \\ Norm Reward~$\uparrow$}}
        & {\makecell[c]{Avg Top 10 \% \\ Norm Reward~$\uparrow$}}
        & {\makecell[c]{Docking ~$\downarrow$}}
        & {\makecell[c]{Druglikeliness ~$\uparrow$}}
        & {\makecell[c]{Synthesizability ~$\downarrow$}}
        & {\makecell[c]{Solubility ~$\uparrow$}}
        \\
        \midrule
        \makecell[l]{\textbf{3CLPro}} %
        &  \textbf{\makecell[l]{\algname without partial}}
        &  \makecell[l]{\textbf{0.902}}        
        &  \makecell[l]{0.561}
        &  \makecell[l]{{0.666}}
        &  \makecell[l]{\textbf{-8.283}}
        &  \makecell[l]{0.614}
        &  \makecell[l]{2.740}
        &  \makecell[l]{3.597}
        \\
        (PDBID:7BQY)
        &  \textbf{\makecell[l]{\algname (Ours)}}
        &  \makecell[l]{{0.844}}
        &  \makecell[l]{\textbf{0.601}}
        &  \makecell[l]{\textbf{0.692}}
        &  \makecell[l]{-8.163}
        &  \makecell[l]{\textbf{0.676}}
        &  \makecell[l]{\textbf{2.381}}
        &  \makecell[l]{\textbf{3.673}}
        \\
        \bottomrule
        \textbf{RTCB}
        &  \textbf{\makecell[l]{\algname without partial}}
        &  \makecell[l]{{0.879}}
        &  \makecell[l]{0.592}
        &  \makecell[l]{{0.724}}
        &  \makecell[l]{-8.318}
        &  \makecell[l]{0.618}
        &  \makecell[l]{2.527}
        &  \makecell[l]{3.832}
        \\
        (PDBID:4DWQ)
        &  \textbf{\makecell[l]{\algname (Ours)}}
        &  \makecell[l]{\textbf{0.964}}        
        &  \makecell[l]{\textbf{0.694}}
        &  \makecell[l]{\textbf{0.754}}
        &  \makecell[l]{\textbf{-9.462}}
        &  \makecell[l]{\textbf{0.794}}
        &  \makecell[l]{\textbf{2.077}}
        &  \makecell[l]{\textbf{3.712}}
        \\
        \bottomrule
        \\
    \end{tabular}}}
        \caption{
        {\textbf{Ablation study.} 
        A comparison between \algname with and without the partial molecule improvement component shows that \algname with this component outperforms in most metrics.
        }}
        \label{exp:ablation}
\end{table*}


\paragraph{Ablation study on \algname.} 
\algname is distinguished from previous RL algorithms by its introduction of advantage preference with partial molecule improvement. In \tabref{exp:ablation}, we perform an ablation study on the partial molecule improvement component, showing that this component in \algname leads to performance enhancements across nearly all metrics, which aligned with our theoretical result of  densifying the reward signal. \fix{See \appref{spo_validity} for novelty and diversity ablation.}





\section{Conclusion}\label{sec:con}




We present the \fwname framework, which includes a LLM designed for drug optimization and \algname, a structured policy optimization algorithmâ€”the novel RL finetuning algorithm tailored for drug optimization. 
We provide a rigorous theoretical analysis of \algname, demonstrating its effectiveness in aligning the LLM-based generator policy with desired objectives and performs efficient policy gradient updates based on the advantage preference. \algname seeks to achieve maximal improvement on desired properties based on the original drug while maintaining its necessary properties.
Moreover, we evaluate \fwname on SARS-CoV-2 and human cancer benchmarks, respectively. Our results reveal that our optimized compounds exhibit significant improvement over the original compounds and outperform the current state of the art in multiple properties.
Our research opens up new possibilities for enhancing drug optimization and inspires future investigations into addressing challenges within the realm of drug optimization. This includes exploring areas like the integration of graph information.
 We leave this extension to future work.





