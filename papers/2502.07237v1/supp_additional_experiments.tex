








































































































\section{Appendix}

{
\subsection{Pre-training and fine-tuning dataset}\label{app:pretrain_data} 

We utilized the ZINC dataset, filtering for Standard, In-Stock, and Drug-Like molecules, resulting in approximately 11 million molecules.
The new pre-training dataset is constructed by randomly selecting two molecules from the ZINC dataset that meet the proposed criteria (as described in Equation \ref{eq:corpus}). This new pre-training dataset comprises 10 million molecules, with a 90/10 training/validation split.

\fix{For fine-tuning, we employ one million compounds from the ZINC15 dataset, docked to the 3CLPro protein (PDB ID: 7BQY), which is linked to SARS-CoV-2, and the RTCB protein (PDB ID: 4DWQ), associated with human cancer. These data are obtained from the latest Cancer and COVID dataset by \citet{liu2023drugimprover} and are used across all baselines.
}
}

{
\subsection{Generation with finetuned model} \label{app:generation} 
The epoch with highest historical average normalized reward (as detailed in Section \ref{experiments}) is selected for generation. 
With this epoch and corresponding weights, we apply TOPPK\citep{liu2024erp} for generation. 
}


{
\subsection{Baseline REINVENT 4} \label{app:reinvent}
Following are detailed description of six different kinds of property change $Z$ included in REINVENT \citet{he2022transformer, he2021molecular}
\begin{itemize}
    \item {MMP:} There are user-defined desirable property changes between molecules $X$ and $Y$.
    \item {Similarity $\geq 0.5$:} The Tanimoto similarity between molecules $X$ and $Y$ exceeds 0.5.
    \item {Similarity $\in [0.5, 0.7)$:} The Tanimoto similarity for the pair $\left(X, Y\right)$ lies between 0.5 and 0.7.
    \item {Similarity $\geq 0.7$:} The Tanimoto similarity between molecules $X$ and $Y$ exceeds 0.7.
    \item {Scaffold:} Molecules $X$ and $Y$ share the same scaffold.
    \item {Scaffold generic:} Molecules $X$ and $Y$ share the same generic scaffold.
\end{itemize}}

\subsection{Binding sites of 3clpro and RTCB}
\begin{figure*}[ht!]

    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[%
        width=6cm,  clip={0,0,0,0}]{figures/3clpro.png}
        \caption{3CLPro.}\label{fig:exp:combine_expertise}
    \end{subfigure}\hfil
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[%
        width=6cm,  clip={0,0,0,0}]{figures/rtcb.png}
        \caption{RTCB.}\label{fig:exp:sample}
    \end{subfigure}\hfil
    \caption{The binding sites of proteins 3CLPro (PDB ID: 7BQY)~(\textbf{Left}) and RTCB (PDB ID: 4DWQ)~(\textbf{Right}). {Open Eye software are used to identify atoms around the crystallized compound as binding sites.}     
    }
\end{figure*}













\subsection{Surrogate model}\label{app:surrogate_model}
{The surrogate model~\citep{vasan23} is a simplified variant of a BERT-like transformer architecture, commonly utilized in natural language processing tasks. Within this model, tokenized SMILES strings are initially inputted and subsequently undergo positional embedding. The outputs are then fed into a series of five transformer blocks, each comprising a multi-head attention layer (with 21 heads), a dropout layer, layer normalization with residual connection, and a feedforward network. This feedforward network is composed of two dense layers followed by dropout and layer normalization with residual connection. Following the stack of transformer blocks, a final feedforward network is employed to generate the predicted docking score.
\fix{ The validation $r^2$ values are 0.842 for 3CLPro and 0.73 for the RTCB dataset.}

}























































































\subsection{Computing infrastructure {and wall-time comparison}}\label{app:computing_infrastructure}

{We trained our docking surrogate models using 4 nodes of a supercomputer, where each node contains 64 CPU cores and 4 A100 GPUs. The training time for each model was approximately 3 hours.
We conducted other experiments on a cluster that includes CPU nodes with approximately 280 cores and GPU nodes with approximately 110 Nvidia GPUs, ranging from Titan X to A6000, mostly set up in 4- and 8-GPU configurations. 
Pretraining utilizes 8 GPUs, while \algname uses a single GPU. Both processes employ either V100 or A100 GPUs. Based on the computing infrastructure, we obtained the wall-time comparison in \tabref{table:wall-time} as follows.
}

 \begin{table*}[ht!]
 {
    \centering
    {\scriptsize
    \scalebox{1}{
    \begin{tabular}{l c c  }
        \toprule
        \textbf{Methods}
        & {\makecell[c]{Total Run Time}}
        \\
        \midrule
        \textbf{\makecell[l]{Pretrain}}
        &  \makecell[r]{24h}
        \\
        \textbf{\makecell[l]{\algname}}
        &  \makecell[r]{8h}
        \\
        \bottomrule
    \end{tabular}}}
    \caption{{Wall-time comparison between different methods.} }
        \label{table:wall-time}   
        }
\end{table*}


\subsection{Hyperparameters and architectures}\label{app:hyperparameters}
Table \ref{app:tab:hyperparams} provides a list of hyperparameter settings we used for our experiments.
A selection of 1280 molecules from each of the RTCB and 3CLPro datasets, with docking scores ranging from -14 to -6, is used for \algname finetuning and experimentation. This range is based on \citep{liu2024erp}.
{Furthermore, when calculating the average normalized reward for the original molecule, where similarity is not considered, we assign a weight of $[0.25] \times 4$ to docking, druglikeliness, synthesizability, and solubility.}
{Moreover, when the generated SMILES is invalid, meaning that calculating the reward $R_c$ is not possible, we have two options: the first is to directly subtract the reward of the original SMILES (i.e., $-R_c(X)$), or alternatively, we can consider the advantage preference as zero.}

\begin{table*}[h!]
    {
    \centering
    {\scriptsize
    \scalebox{1}{
    \begin{tabular}{c c }
        \toprule
        \textbf{Parameter} &  \textbf{Value} 
        \\
        \midrule
        {\makecell[l]{Pretraining}}
        \\
        \midrule
        {\makecell[l]{\quad Learning rate}} &  \makecell[c]{$5 \times e^{-5}$}
        \\
        \midrule
        {\makecell[l]{\quad Batch size}} &  \makecell[c]{$24$}
        \\
        \midrule
        {\makecell[l]{\quad Optimizer}} &  \makecell[c]{Adam}
        \\
        \midrule
        {\makecell[l]{\quad \# of Epochs}} &  \makecell[c]{$10$} \\
        \midrule
        {\makecell[l]{\quad Model \# of Params}} &  \makecell[c]{$124M$} \\


        \bottomrule
    \end{tabular}}}
        \caption{{{Hyperparameters for pretraining}}. }
        \label{app:tab:hyperparams}
        }
\end{table*}

\begin{table*}[h!]
    {
    \centering
    {\scriptsize
    \scalebox{1}{
    \begin{tabular}{c c }
        \toprule
        \textbf{Parameter} &  \textbf{Value} 
        \\
        \midrule
        {\makecell[l]{Shared}}
        \\
        \midrule
        {\makecell[l]{\quad \# of Molecules Optimized}} &  \makecell[c]{$256$}
        \\
        \midrule
        {\makecell[l]{\quad Learning Rate}} &  \makecell[c]{$1 \times 10^{-5}$}
        \\
        \midrule
        {\makecell[l]{\quad Optimizer}} &  \makecell[c]{Adam}
        \\
        \midrule
        {\makecell[l]{\quad \# of Epochs for Training}} &  \makecell[c]{$100$}
        \\
        \midrule
        {\makecell[l]{\quad Batch size}} &  \makecell[c]{$64$}
        \\
        \midrule
        {\makecell[l]{\quad Best-of-N}} &  \makecell[c]{$[4,6,8]$}
        \\
        \midrule
        {\makecell[l]{\quad TopK}} &  \makecell[c]{$[10,15,20]$}
        \\
        \midrule
        {\makecell[l]{\quad TopP}} &  \makecell[c]{$[0.85,0.9,0.95]$}
        \\
        \midrule
        {\makecell[l]{\algname Objective Weight}}
        \\
        \midrule
        {\makecell[l]{\quad Tamimoto Similarity}} &  \makecell[c]{$[0.2,0.4,0.6,0.8]$}
        \\
        \midrule
        {\makecell[l]{\quad Other Four Objectives}} &  \makecell[c]{$(1-W(Sim)) / 4$}
        \\
        \midrule
        {\makecell[l]{\algname Other}}
        \\
        \midrule
        {\makecell[l]{\quad Fingerprint Size}} &  \makecell[c]{$1024$}
        \\
        \midrule
        {\makecell[l]{\quad Normalize Min/Max}} &  \makecell[c]{$[-10, 10]$}
        \\
        \midrule
        {\makecell[l]{Advantage preference with \\ invalid generated SMILES}}
        \\
        \midrule
        {\makecell[l]{\quad 3CLPro}} &  \makecell[c]{$[0,-R_c(X)]$}
        \\
        \midrule
        {\makecell[l]{\quad RTCB}} &  \makecell[c]{$[0,-R_c(X)]$}
        \\
        

        \bottomrule
    \end{tabular}}}
        \caption{{{Hyperparameters for \algname}}. }
        \label{app:tab:hyperparams}
        }
\end{table*}










{
\subsection{Ablation study of novelty and diversity}\label{spo_validity}


We further explore the novelty and diversity of SPO, both with and without partial molecule enhancements. Novelty is assessed by verifying if the generated molecule/SMILES is present in the original dataset, assigning a value of 0 if it exists and 1 if it does not. Diversity is measured by examining if the generated molecule/SMILES is repeated within the generated dataset, indicating a duplication if two distinct prompts or molecules yield the same outcome. The findings demonstrate that the molecules created through our method are completely new in comparison to the original molecules. Moreover, our technique has attained a decent level of diversity.


\begin{table}[h!]
\centering
\begin{tabular}{|l|l|c|c|}
\hline
\textbf{Data} & \textbf{Method}        & \textbf{Novelty} & \textbf{Diversity} \\ \hline
3CLPro        & SPO w/o partial        & 1               & 0.98               \\ \cline{2-4} 
              & SPO                    & 1               & 0.95               \\ \hline
RTCB          & SPO w/o partial        & 1               & 0.98               \\ \cline{2-4} 
              & SPO                    & 1               & 0.69               \\ \hline
\end{tabular}
\caption{Comparison of SPO with and without partial molecule improvements across cancer and covid datasets in terms of novelty and diversity.}
\label{tab:validity_comparison}
\end{table}
}

\fix{
\subsection{Training corpus visualization}\label{corpus_viz}
For better understanding the training corpus, \figref{fig:corpus_fiz} shows an example and corresponding visualization towards training corpus described in \eqref{eq:corpus}. The two selected molecules/SMILES have the similarity of 0.52.
}
\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.5\linewidth]{./appendix_plot/corpus_viz.png}
    \caption{\fix{Training corpus example and visualization}}
    \label{fig:corpus_fiz}
\end{figure}

\fix{
\subsection{Baselines with RL fine-tuning}


        


The initial version of Reinvent4 \XL{@songhao check version number}\citep{he2021molecular, he2022transformer} only introduced pre-trained models, and the later updated version of Reinvent4 \citep{loeffler2024reinvent}, which includes Mol2Mol~\citep{he2021molecular, he2022transformer} as one of four models, stated that Reinvent could perform RL fine-tuning through REINFORCE~\citep{williams1992simple} algorithm without providing empirical results. In this work, we conducted experiments for both Reinvent with pre-training only and Reinvent with RL fine-tuning. For Mol2Mol~\citep{he2021molecular, he2022transformer} with pre-training only, we followed different pre-trained rules outlined in their paper to pretrained the models and used them as various baseline models; meanwhile, we used the same pre-trained ZINC dataset as in our approach. Molsearch and Mimosa, on the other hand, focuses more on optimizting sampling process. Molsearch uses Monte Carlo tree search (MCTS) to optimize molecular properties; MIMOSA \citep{fu2021mimosa} designed a Markov Chain Monte Carlo (MCMC) based molecule sampling method that enables efficient sampling from a target distribution. The results are provided in  \tabref{exp:main_result}; DrugEx v3 \citep{liu2023drugex} employs graph transformers with scaffold constraints to refine molecular structures, leveraging reinforcement learning to enhance the desired molecular properties. In addition, we also conducted experiments for Reinvent 4~\citep{loeffler2024reinvent} with additional RL fine-tuning, using the same offline dataset we proposed in the paper, the same scoring function, and the same number of training epochs as our approach. In the cancer dataset, our proposed method outperforms all variants of Reinvent 4~\citep{loeffler2024reinvent} with RL finetuning. And in the COVID-19 dataset, our proposed method still outperforms almost all variants. Therefore, our method surpasses the performance of both the pre-trained-only Mol2Mol~\citep{he2021molecular, he2022transformer} and the version~\citep{loeffler2024reinvent} that underwent REINFORCE fine-tuning.

\fwname also surpasses REINVENT4 with RL fine-tuning. This is because REINVENT4 employs the REINFORCE, a conventional RL approach, which does not account for improvements over the original molecule. In contrast, our proposed SPO algorithm is specifically designed for drug optimization toward original given molecule. It incorporates the concept of advantage preference and partial molecule components to optimize target molecules more effectively.

}

\fix{
\section{Dataset details}\label{app:dataset}
\fix{For each dataset proposed, it is a orderable subset of the ZINC15 dataset. Creating these subsets was mainly a manual process, involving the identification of compounds that are either in stock or can be shipped within three weeks from various suppliers. Subsequently, we performed a random sampling to select 1 million compounds.

For proteins with available structures containing bound ligands, we used X-ray crystallographic data to locate ligand density regions and defined the pocket as a rectangular box enclosing that area. For proteins without bound ligands, we employed FPocket to identify the top-ranked pocket and similarly defined the pocket with a rectangular box around that region. And therefore for each dataset we proposed, only one pocket is used for docking. 
The validation $r^2$ values are 0.842 for 3CLPro and 0.73 for the RTCB dataset (two datasets used in section \ref{experiments}). 

}

The datasets created in this work including the following files:


\begin{itemize}
    \item {ST$\_$MODEL:}  The trained surrogate model for SARS-CoV-2 proteins.
    \item {ST$\_$MODEL$\_$rtcb:} The trained surrogate model for RTCB Human-Ligase cancer target.
    
    \item {24 *.csv files for SARS-CoV-2 proteins under folder data/COVIDRec:} The training and validation SMILES string data docked on SARS-CoV-2 receptor including 3CLPro$\_$7BQY$\_$A$\_$1$\_$F, NPRBD$\_$6VYO$\_$AB$\_$1$\_$F, NPRBD$\_$6VYO$\_$A$\_$1$\_$F, NPRBD$\_$6VYO$\_$BC$\_$1$\_$F, NPRBD$\_$6VYO$\_$CD$\_$1$\_$F, NPRBD$\_$6VYO$\_$DA$\_$1$\_$F, NSP10-16$\_$6W61$\_$AB$\_$1$\_$F, NSP10-16$\_$6W61$\_$AB$\_$2$\_$F, NSP10$\_$6W61$\_$B$\_$1$\_$F, NSP15$\_$6VWW$\_$AB$\_$1$\_$F, NSP15$\_$6VWW$\_$A$\_$1$\_$F, NSP15$\_$6VWW$\_$A$\_$2$\_$F, NSP15$\_$6W01$\_$AB$\_$1$\_$F, NSP15$\_$6W01$\_$A$\_$1$\_$F, NSP15$\_$6W01$\_$A$\_$2$\_$F, NSP15$\_$6W01$\_$A$\_$3$\_$H, NSP16$\_$6W61$\_$A$\_$1$\_$H, Nsp13.helicase$\_$m1$\_$pocket2, Nsp13.helicase$\_$m3$\_$pocket2, PLPro$\_$6W9C$\_$A$\_$2$\_$F, RDRP$\_$6M71$\_$A$\_$2$\_$F, RDRP$\_$6M71$\_$A$\_$3$\_$F, RDRP$\_$6M71$\_$A$\_$4$\_$F, RDRP$\_$7BV1$\_$A$\_$1$\_$F.
    
    \item {5 *.csv files for human cancer proteins under folder data/CancerRep:} The training and validation SMILES string data docked on human cancer proteins including 6T2W, NSUN2, RTCB, WHSC, WRN.
    \item {Each folder in data/COVIDRec and data/CancerRep includes:
model.weights.h5: model weights
SMILES*.csv: 1 million SMILES their docking scores.
We also provide code within data/SurrogateInf on how to use the surrogate model for inference.}


    \item {3CLPro$\_$7BQY$\_$A$\_$1$\_$F.oeb:} The 3CLPro OpenEye receptor file.
    \item {rtcb-7p3b-receptor-5GP-A-DU-601.oedu:}  The RTCB OpenEye receptor file.
    \item {  We include an extended dataset of 1 million SMILES strings from the ZINC15 dataset, their docking scores (as determined by OpenEye FRED) to 24 COVID and 5 cancer-target receptors and surrogate model weights for each corresponding receptor.}
    \item We provide code within data/SurrogateInf on how to use the surrogate model for inference.
\end{itemize}}




















\section{Proofs of the theoretical results}\label{app:theory_proof}
\begin{proof}[Proof of Lemma \ref{lem:equivalence}]\label{app:lem:equivalence}
    For simplicity, let us take out the shift terms $R_c(X)$ in $r^\AP$ and $R_c(X_{1:T})$ in $r^\AP_{\BON(j)}$ for a while when defining $J$. 
    Since the shift term $R_c(X)$ (or its BON counterpart) are  independent of the current policy $\pi$, such an operation does not influence the definition of optimal policy for $J$.
    One can always split $J=\frac 1 2 J_{\BON} + \frac 1 2 J_0$, where 
    $J_{\BON} = \E_\pi \E_{j\in \mathcal{U}([T])} r_{\BON(j)}^{\AP}(Y_{1:T}, X)$. 
    Take $\pi^\star$ to be an optimizer of $J_0$. By definition, 
    \begin{align*}
        J_0(\pi^\star) \ge J(\pi) \ge J_0(\pi)
    \end{align*}
    as BON cannot be worse than the current molecule. 
    Thus, any policy that maximizes $J_0$ should also be a maximizer to $J$. 
    On the other hand, notice that 
    \begin{align}
        J_0(\pi^\star) - J(\pi) \ge \frac 1 2 \cdot (J_0(\pi^\star) - J_0(\pi)) \ge 0
    \end{align}
    since the BON reward should not exceed the optimal reward.
    Hence, any policy that maximize $J$ should also maximize $J_0$ since the optimizer of $J$ gives optimal value equal to $J_0(\pi^\star)$. 
    Hence, we prove the claim.
\end{proof}

\begin{proof}[Proof of Lemma \ref{lem:gradient}]\label{app:lem:gradient}
Denote by $\pi_\theta$ the policy parameterized by $\theta$.
When doing policy optimization, we note that 
\begin{align}
    g &= \E_{X\sim\rho_0}^\pi  \left[\nabla_\theta \log \pi_\theta(Y_{1:T}\given X) R^\AP (Y_{1:T}, X) \right] \notag\\
   &= \E_{X\sim\rho_0}^\pi \left[\nabla_\theta \log \pi_\theta(Y_{1:T}\given X) \left(\frac 1 2 r^\AP(Y_{1:T}, X) + \frac 1 2 \E_{j\in \mathcal{U}([T])} r_{\BON(j)}^{\AP}(Y_{1:T}, X)\right)\right] \notag\\
   &= \frac {1} {2T} \sum_{t=1}^T \E_{X\sim\rho_0}^\pi \left[\left(\nabla_\theta \log \pi_\theta(Y_{1:t}\given X) + \nabla_\theta \log \pi_\theta(Y_{t+1:T}\given Y_{1:t}, X) \right) r_{\BON(t)}^{\AP}(Y_{1:T}, X)\right] \notag\\
   &\qquad +  \frac 1 2 \cdot \E_{X\sim\rho_0}^\pi \left[\nabla_\theta \log \pi_\theta(Y_{1:T}\given X)  r^\AP(Y_{1:T}, X) \right] \notag\\
   &= \frac {1} {2T} \sum_{t=1}^T \E_{X\sim\rho_0}^\pi \left[\nabla_\theta \log \pi_\theta(Y_{1:t}\given X)  r_{\BON(t)}^{\AP}(Y_{1:T}, X)\right] \notag\\
   &\qquad +  \frac 1 2 \cdot \E_{X\sim\rho_0}^\pi \left[\nabla_\theta \log \pi_\theta(Y_{1:T}\given X)  r^\AP(Y_{1:T}, X) \right], 
\end{align}
where the last equality holds by noting that 
$$\E^\pi[\nabla_\theta \log \pi_\theta(Y_{t+1:T}\given Y_{1:t}, X) \given Y_{1:t}, X]=0.$$
\end{proof}


