


\section{The \fwname Framework} %
{In this work, we propose \fwname  as in \figref{fig:drugimprover_framework}, which}
comprises {two} major components: 
{(1) A large language model designed \fix{from the ground up} for drug optimization.}
{(2) A Structured Policy Optimization (\algname)
{algorithm with theoretical support.}
} 
{We introduce each part in details as follows.}




\subsection{{\fix{Designing \& }pretraining a LLM generator}}\label{sec:pretrain}
In drug optimization, firstly, we construct a molecule pair $\paren{X,Y}$,
where $X$ represents for original molecule, and $Y$ represents for the target optimized molecule. We randomly select non-duplicated pair $\paren{X,Y}$ from ZINC15~\citep{sterling2015zinc} dataset \fix{(See Appendix \ref{app:pretrain_data})}, and added the pair to the training set by meeting the following criteria of Tanimoto Similarity~\citep{bajusz2015tanimoto} and molecule scaffold~\citep{landrum2013rdkit}:
\begin{equation}
\text{Tanimoto}\paren{X,Y} > 0.5 \text{~~or~~}\text{Scaffold}(X) = \text{Scaffold}(Y), \notag
\end{equation}
The motivation for such a form is to provide an initialization for a diversified molecule pair while constraining the similarity between the pair. After obtaining the training set of molecule pairs, we formed the training corpus as follows:
\begin{equation}\label{eq:corpus}
\corpus=\curlybracket{
\tuple{S},\underbrace{x_1,\cdots,x_{\horizon}}_{\text{source molecule X}},\tuple{L},\underbrace{y_1,\cdots,y_{\horizon}}_{\text{target molecule Y}}
},
\end{equation}
where $\tuple{S}$ stands for the source ligand and $\tuple{L}$ stands for the target ligand. \fix{We include visualization towards the corpus in Appendix \ref{corpus_viz}.}
We enhance the training by concentrating on pairs of molecules through Causal Language Modeling (CLM)~\citep{vaswani2017attention}.
The parameters $\theta$ of the LLM generator $\policy_{\theta}$ are trained through the minimization of the negative log-likelihood (NLL) for the complete molecular pair across the entire training corpus.
This process is described as follows:
\begin{align}\label{eq:pretrain-loss}
    \text{NLL}\paren{X,Y} =  - \log P\paren{Y|X} = - \log \prod_{l=1}^{\horizon} P\paren{y_{l}|\mathbf{y}_{<l},X} 
    =-\sum_{l=1}^{\horizon} \log P\paren{y_{l}|\mathbf{y}_{<l},X},
\end{align}
where $\horizon$ signifies the total number of tokens related to $Y$. The NLL measures the likelihood of transforming a specific original molecule into a designated target molecule. 
Given the goal of drug optimization, we aim for the generated drugs to resemble the originals. Therefore, we have incorporated a regularization term into the loss in Equation \eqref{eq:pretrain-loss}, which penalizes the NLL if the sequence does not adhere to a specified similarity metric.
Finally, we propose the following loss function:
\begin{align}\label{eq:loss}
    \mathcal{L}=
    \frac{1}{|\corpus|}\sum_{\paren{X,Y}\in \corpus}
    (\lambda\cdot\text{NLL}\paren{X,Y} \fix{/} \paren{ \paren{1-\lambda}\cdot \text{Similarity}\paren{ X, Y})}, \lambda\in \fix{\paren{0,1}}.
\end{align}
Consequently, 
training the model with the loss function described in Eqn. \eqref{eq:loss} can generate the corresponding target molecule when provided with a source molecule.
However, this approach primarily focuses on maximizing likelihood without considering specific metrics of interest, making it unsuitable for optimizing objectives that differ from those in its training set, as encoded in $\policy_{\theta}$. 
Therefore, these generation algorithms cannot be directly applied to design molecules that fulfill various objectives, such as attaining a high docking score at a specific target site or improving upon specific desired metrics.
We aim to further refine the LLM model to generate specific improved outcomes using reinforcement learning techniques in the next phase.




\fix{































}




\subsection{Structured policy {optimization}\label{sec:4.2}
}
\begin{algorithm}[t]
    \caption{Structured Policy Optimization (\algname)
    }\label{alg:lops}
    \begin{algorithmic}[1] 
    \Require { LLM-based generator $\Gen$; roll-out policy $\policy_\beta$; a pre-train dataset $\RationaleBuffer$, {critics $\mathbf{\critic}$}.}
    \State Pre-train $\Gen$ using loss function~\eqref{eq:loss} and training corpus  \eqref{eq:corpus} through CLM objective.
    \State $\beta\leftarrow\theta$.

    \For{$n=1,\ldots,\ENum$}
            \State $\fix{X} \sim \rho_0$, { \text{where }$\rho_0\in \Delta\paren{\RationaleBuffer}$}.
            \State Generate $Y_{1:\horizon}=\paren{y_t,\ldots,y_{\horizon}}\sim \Gen\paren{\cdot|X}$.
        \LineComment{/* incorporating partial reward */}
            \State Compute advantage preference
            {$\RFunc^\text{AP}$}
            by incorporating partial molecule component. %
            \State Update generator $\theta$
            via policy gradient by \eqref{eq:adv:preference}\eqref{eq:gradient_update}.
        \State $\beta\leftarrow\theta$.
    \EndFor
    \end{algorithmic}
\end{algorithm}


\paragraph{{Normalized reward}.} 
\fix{In this work, we adopt the approach of \citet{liu2024erp} to construct the reward for multiple critics.}
Given an ensemble of critics 
{
\begin{align*}
\mathbf{C}{\paren{
\mathbf{y}_{\horizon}
}}=[
\critic^{\text{Druglikeness}}{\paren{\mathbf{y}_{\horizon}}},
\critic^{\text{Solubility}}{\paren{\mathbf{y}_{\horizon}}},
\critic^{\text{Synthesizability}}{\paren{\mathbf{y}_{\horizon}}}, {\critic^{\text{Docking}}}{\paren{\mathbf{y}_{\horizon}}}
],
\end{align*}
}
where  $\mathbf{y}_{\horizon}:=\state_{\horizon}, \critic: \fix{\mathbf{y}_{\horizon}} \rightarrow \mathbb{R}$. 
\fix{{We leverage the RDKit  \citep{landrum2016rdkit} chemoinformatics package to calculate the listed critics:}
\textbf{Druglikeness:} The druglikeness measure the likelihood of a molecule being suitable candidate for a drug.
\textbf{Solubility:} This metric assesses the likelihood of a molecule's ability to mix with water, commonly referred to as the water-octanol partition coefficient (LogP). 
\textbf{Synthetizability:} This parameter quantifies the ease (score of 1) or difficulty (score of 10) associated with synthesizing a given molecule \citep{ertl2009estimation}.
\textbf{Docking Score:} The docking score assesses the drug's potential to bind and inhibit the target site. 
To enable efficient computation, we employ a docking surrogate model (See \appref{app:surrogate_model}) to output this score.}
Here we design the reward function to align the drug optimization with multiple objectives.
For a fully generated SMILES sequence, we derive the following
{normalized}
reward function based on assessments from multiple critics with equal weight \citep{liu2024erp} as follows:
{
\begin{align}\label{eq:reward}
{\RFunc_c\paren{\mathbf{y}_{\horizon}}}:=&
{\RFunc_c\paren{
\mathbf{y}_{\horizon}|X}} =
\beta\cdot\text{Norm}\paren{\critic^{\text{Tanimoto}}\paren{X,
\mathbf{y}_{\horizon}}}\notag \\ 
&+
\sum_{i=0}^{|\mathbf{C}|-1}\lambda\cdot\text{Norm}\paren{\critic_i{\paren{\mathbf{y}_{\horizon}}}},
\end{align}
}
where $\lambda=\frac{1-\beta}{|\mathbf{\critic}|+1}$.
{We use Norm\footnote{{Here, we define Norm as min-max normalization to scale the attributes onto the range [-10, 10].}} to normalize different attributes
{onto the} same scale.}
In this study, we employ the Tanimoto similarity calculation $\critic^{\text{Tanimoto}}$ to quantify the chemical similarity
between the generated compound and the original drug. 
Essentially, this calculation involves first computing Morgan Fingerprints~\citep{rogers2010extended} for each molecule and then measuring the 
{Jaccard distance~\citep{jaccard1912distribution} (i.e., intersection over union)}
 between the two fingerprints. \YC{expand the definition if still having space}



\begin{figure*}[t]
    \begin{subfigure}{1\textwidth}
        \centering
        \includegraphics[
        height=4.8cm, 
        clip={0,0,0,0}]{figures/workflow12.pdf}
    \end{subfigure}
    \caption{{\fwname framework.} 
    {It comprises two major components: {(1)  A large language model designed for drug optimization.} (2) A Structured Policy Optimization (\algname) algorithm aims to fine-tune the LLM-based generator for drug improvement across desired properties.\YCinline{Fine-tune} 
    }
    }
  \label{fig:drugimprover_framework}
\end{figure*}




\paragraph{Structured policy gradient \fix{with \emph{partial} molecule improvement}.} 
The return, denoted as $\QFunc^{\policy}$, often exhibits significant variance across multiple episodes. One approach to mitigate this issue is to subtract a baseline $b\paren{\state}$ from each $\QFunc$. The baseline function can be any function, provided that it remains invariant with respect to $\action$. For a generator policy $\Gen$, the advantage function \citep{sutton1999policy} is defined as follows:
    $\AFunc^{\Gen}\paren{\state,\action} = \QFunc^{\Gen}\paren{\state,\action} - b\paren{\state}$.
A natural choice for the baseline is the value function $\VFunc^{\policy}\paren{\state}$, which represents the expected reward at a given state $\state$ under policy $\policy$. The value function can be expressed as follows:
\begin{align}
\VFunc\paren{\state}&=\expctover{\action\sim\Gen\paren{\state}}{\QFunc(\state,\action)}
=\expctover{y_t\sim\Gen\paren{\mathbf{y}_{<t}}}{\QFunc(
\mathbf{y}_{<t},y_t)}.\notag
\end{align}
Thus, we have advantage function as 
\begin{align}
     \AFunc^{\Gen}\paren{\state,\action} =\AFunc^{\Gen}\paren{\mathbf{y}_{<t},y_{t}}
     =\QFunc^{\Gen}\paren{\mathbf{y}_{<t},y_t}-\VFunc^{\Gen}\paren{\mathbf{y}_{<t}}.\notag
\end{align}






{
Here we employ the one-step RL~\citep{brandfonbrener2021offline, peng2019advantage} method and regard the drug optimization method as a {sequence to sequence} language generation task. 
Rather than treating each token as an individual action, we treat the entire sequence $Y_{1:\horizon}=\mathbf{y}_{\horizon}$ as a single action generated by the policy $\Gen$. Subsequently, we receive rewards from critics, and the episode concludes. This leads to the formulation of our advantage function as follows:}
\begin{align}\label{eq:advantage}
\AFunc^{\Gen}\paren{\state,\action}&=\QFunc^{\Gen}\paren{\state_0,
\mathbf{y}_{\horizon}}-\VFunc^{\Gen}\paren{\state_0}\\
&=\RFunc_c\paren{Y_{1:\horizon}}-\RFunc_c\paren{X},
\end{align}
where $\state_0=X$ is the initial molecule sequence drawn from the distribution $\rho_0$, which corresponds to our buffer known as $\RationaleBuffer$ {containing selected SMILES strings}.
Thus, the advantage preference of the generated versus the original drug is
\begin{equation}\label{eq:advantage_preference_temp}
r^\AP\paren{Y_{1:\horizon},X} ={\RFunc_c\paren{Y_{1:\horizon}} - \RFunc_c\paren{X}},
\end{equation}
Nonetheless, the reward function only supports a reward value for a completed sequence for global optimization. \fix{In contrast with previous approaches}, we also aim to make improvement on partial molecule for local optimization by uniformly sample a subsquence $Y_{1:j},j \sim \mathcal{U}([T])$. 
To achieve this, we employ a Roll-in-Roll-out (RIRO) \citep{ross2014reinforcement, cheng2020policy,liu2023active,liu2023blending} scheduling, utilizing a roll-out policy denoted as $\policy_\beta$ (same as learner policy in our experiment) to sample the unknown last $\horizon-j$ tokens through \fix{$\bestN$} approach~\eqref{eq:bestN}. 
\fix{We propose a novel notion of advantage function, termed Advantage Preference (AP), by incorporating partial molecule improvement into \eqref{eq:advantage_preference_temp}:}

\vspace{+0.2cm}
\begin{definition}[Advantage preference] {We define advantage preference as a combination of rewards from both complete and partial molecules:} 
\vspace{+0.2cm}
\begin{align}\label{eq:advantage_preference}
    R^{\AP}(Y_{1:T}, X) = \frac 1 2 \E_{j\in \mathcal{U}([T])} r_{\BON(j)}^{\AP}(Y_{1:T}, X) + \frac 1 2 r^\AP(Y_{1:T},X),
\end{align}
where 
\begin{align*}
r_{\BON(j)}^{\AP}(Y_{1:T}; X_{1:T})=&\E_{Y_{j+1:T}\sim \BON(Y_{1:j}),X_{j+1:T}\sim \BON(X_{1:j})}[ 
R_c(Y_{1:T}) - R_c(X_{1:T})\given X_{1:j}, Y_{1:j}]
\end{align*}
and $r^{\AP}(Y_{1:T}; X_{1:T})$ is defined in Eq. \eqref{eq:advantage_preference_temp}.
\end{definition}

The \text{advantage preference} of \eqref{eq:advantage_preference} will be employed directly in the policy gradient~\eqref{eq:adv:preference} to finetune the {generator} policy $\Gen$. The rationale behind the advantage preference is
to produce a sequence that surpasses the initial state sequence $\state_0$ in every objective.
{In this work, our objective is to maximize the expected final advantage preference compared to the original drug $\fix{X}$
at the end of the sequence as follows
\begin{equation}
 J\paren{\theta}=\expctover{{X\sim\rho_0, Y_{1:T}\sim \pi_\theta(\cdot\given X)}}{\RFunc^\AP(Y_{1:T},X)},
 \label{eq:J(theta)}
\end{equation}
}
Thus, we have gradient $g$ as follows:
\fix{
\begin{align}\label{eq:adv:preference}
     \textstyle \mathbb{E}_{X\sim {\rho_0},{Y_{1:\horizon}\sim \Gen\paren{\cdot|X}} } 
     \left[ \nabla_{\theta}\log \Gen\paren{Y_{1:\horizon}|X}
     \cdot \RFunc^\AP\paren{X,Y_{1:\horizon}}\right],
\end{align}
}
{where $Y_{1:\horizon}$ is the generated sequence 
from $\Gen$ and $X$ is the original drug}. As the expectation $\expct{\cdot}$ can be approximated through sampling techniques, we proceed to update the generator's parameters as follows:
\YC{Move definition of $g$ (from Lemma 5.2) here}

\begin{equation}\label{eq:gradient_update}
    \theta\leftarrow\theta+\alpha_n\gradient,
\end{equation}
where $\alpha\in \mathbb{R}^{+}$ denotes the learning rate at $n$-th episode. 

















































\begin{table*}[t!]
\setlength{\tabcolsep}{4pt}
   \centering
    { %
    \scalebox{0.65}{
    \begin{tabular}{l l c c c c c c c c c c }
        \toprule
        \textbf{Target} %
        & \textbf{Algorithm}
        & {\makecell[c]{Avg \\ Norm Reward~$\uparrow$}}
        & {\makecell[c]{Avg Top 10 \% \\ Norm Reward~$\uparrow$}}
        & {\makecell[c]{Docking ~$\downarrow$}}
        & {\makecell[c]{Druglikeliness ~$\uparrow$}}
        & {\makecell[c]{Synthesizability ~$\downarrow$}}
        & {\makecell[c]{Solubility ~$\uparrow$}}
        \\
        \midrule
        \makecell[l]{\textbf{3CLPro}} %
        &  \textbf{\makecell[l]{Original}}
        &  \makecell[l]{0.524}
        &  \makecell[l]{{0.689}}
        &  \makecell[l]{\underline{-8.687}}
        &  \makecell[l]{0.654}
        &  \makecell[l]{3.097 }
        &  \makecell[l]{2.455}
        \\
        (PDBID:
        &  \textbf{\makecell[l]{MMP \citep{loeffler2024reinvent}}}
        &  \makecell[l]{0.564 $\pm$ 0.003}
        &  \makecell[l]{0.680 $\pm$ 0.001}
        &  \makecell[l]{-8.184 $\pm$ 0.069}
        &  \makecell[l]{0.672 $\pm$ 0.003}
        &  \makecell[l]{2.658 $\pm$ 0.006}
        &  \makecell[l]{3.114 $\pm$ 0.067}
        \\
       \ 7BQY)
        &  \textbf{\makecell[l]{Similarity ($\geq$ 0.5) \citep{loeffler2024reinvent}}}
        &  \makecell[l]{0.572 $\pm$ 0.001}
        &  \makecell[l]{{0.686} $\pm$ 0.001}
        &  \makecell[l]{-8.158 $\pm$ 0.007}
        &  \makecell[l]{\textbf{0.686} $\pm$ 0.004}
        &  \makecell[l]{\underline{2.583} $\pm$ 0.010}
        &  \makecell[l]{3.121 $\pm$ 0.018}
        \\
        \textbf{ }
        &  \textbf{\makecell[l]{Similarity ([0.5, 0.7)]) \citep{loeffler2024reinvent}}}
        &  \makecell[l]{0.575 $\pm$ 0.002}
        &  \makecell[l]{0.686 $\pm$ 0.004}
        &  \makecell[l]{-8.171 $\pm$ 0.053}
        &  \makecell[l]{0.676 $\pm$ 0.002}
        &  \makecell[l]{2.588 $\pm$ 0.018}
        &  \makecell[l]{3.309 $\pm$ 0.032}
        \\
        \textbf{ }
        &  \textbf{\makecell[l]{Similarity ($\geq$ 0.7) \citep{loeffler2024reinvent}}}
        &  \makecell[l]{0.560 $\pm$ 0.002}
        &  \makecell[l]{0.677 $\pm$ 0.002}
        &  \makecell[l]{-8.187 $\pm$ 0.024}
        &  \makecell[l]{ 0.668 $\pm$ 0.007}
        &  \makecell[l]{2.699 $\pm$ 0.007}
        &  \makecell[l]{3.120 $\pm$ 0.013}
        \\
        \textbf{ }
        &  \textbf{\makecell[l]{Scaffold \citep{loeffler2024reinvent}}}
        &  \makecell[l]{0.552 $\pm$ 0.004}
        &  \makecell[l]{0.678 $\pm$ 0.009}
        &  \makecell[l]{-8.081 $\pm$ 0.049}
        &  \makecell[l]{0.675 $\pm$ 0.002}
        &  \makecell[l]{2.741 $\pm$ 0.014}
        &  \makecell[l]{3.002 $\pm$ 0.040}
        \\
        \textbf{ }
        &  \textbf{\makecell[l]{Scaffold Generic \citep{loeffler2024reinvent}}}
        &  \makecell[l]{\underline{0.567} $\pm$ 0.001}
        &  \makecell[l]{{0.680} $\pm$ 0.007}
        &  \makecell[l]{-8.078 $\pm$ 0.056}
        &  \makecell[l]{\underline{0.680} $\pm$ 0.005}
        &  \makecell[l]{2.613 $\pm$ 0.002}
        &  \makecell[l]{3.173 $\pm$ 0.046}
        \\
        \textbf{ }
        &  \textbf{\makecell[l]{Molsearch \citep{sun2022molsearch}}}
        &  \makecell[l]{0.518  $\pm$ 0.002} 
        &  \makecell[l]{\textbf{0.693 }  $\pm$ 0.003}
        &  \makecell[l]{{-8.506}  $\pm$ 0.038}
        &  \makecell[l]{0.656  $\pm$ 0.004}
        &  \makecell[l]{3.110  $\pm$ 0.010}
        &  \makecell[l]{2.448 $\pm$ 0.032}
        \\
        \textbf{ }
        &  \textbf{\makecell[l]{MIMOSA \citep{fu2021mimosa}}}
        &  \makecell[l]{0.530  $\pm$ 0.003} 
        &  \makecell[l]{0.690  $\pm$ 0.005} 
        &  \makecell[l]{\textbf{-8.764 } $\pm$ 0.048}
        &  \makecell[l]{0.649  $\pm$ 0.003}
        &  \makecell[l]{3.148  $\pm$ 0.023}
        &  \makecell[l]{2.732  $\pm$ 0.027}
        \\
        \textbf{ }
        &  \textbf{\makecell[l]{DrugEx v3 \citep{liu2023drugex}}}
        &  \makecell[l]{0.532  $\pm$ 0.003} 
        &  \makecell[l]{0.653  $\pm$ 0.004} 
        &  \makecell[l]{{-8.089 } $\pm$ 0.039}
        &  \makecell[l]{0.583  $\pm$ 0.005}
        &  \makecell[l]{3.095  $\pm$ 0.018}
        &  \makecell[l]{\textbf{3.932}  $\pm$ 0.031}
        \\
        \textbf{ }
        &  \textbf{\makecell[l]{\fwname (Ours)}}
        &  \makecell[l]{\textbf{0.601} $\pm$ 0.003} 
        &  \makecell[l]{{\underline{0.692}} $\pm$ 0.003} 
        &  \makecell[l]{{-8.163}  $\pm$ 0.034}
        &  \makecell[l]{{0.676}  $\pm$ 0.004}
        &  \makecell[l]{\textbf{2.381} $\pm$ 0.011}
        &  \makecell[l]{\underline{3.673}  $\pm$ 0.024}
        \\
        \bottomrule
        \textbf{RTCB}
        &  \textbf{\makecell[l]{Original}}
        &  \makecell[l]{0.538}
        &  \makecell[l]{{0.705}}
        &  \makecell[l]{-8.538}
        &  \makecell[l]{0.716}
        &  \makecell[l]{2.984}
        &  \makecell[l]{2.283}
        \\
        (PDBID:
        &  \textbf{\makecell[l]{MMP \citep{loeffler2024reinvent}}}
        &  \makecell[l]{0.583 $\pm$ 0.000}
        &  \makecell[l]{0.700 $\pm$ 0.001}
        &  \makecell[l]{-8.466 $\pm$ 0.012}
        &  \makecell[l]{0.709 $\pm$ 0.001}
        &  \makecell[l]{2.599 $\pm$ 0.004}
        &  \makecell[l]{2.978 $\pm$ 0.021}
        \\
        \ 4DWQ)
        &  \textbf{\makecell[l]{Similarity ($\geq$ 0.5) \citep{loeffler2024reinvent}}}
        &  \makecell[l]{\underline{0.593} $\pm$ 0.004}
        &  \makecell[l]{0.705 $\pm$ 0.001}
        &  \makecell[l]{-8.581 $\pm$ 0.096}
        &  \makecell[l]{0.715 $\pm$ 0.007}
        &  \makecell[l]{2.561 $\pm$ 0.005}
        &  \makecell[l]{3.065 $\pm$ 0.048}
        \\
        \textbf{ }
        &  \textbf{\makecell[l]{Similarity ([0.5, 0.7)]) \citep{loeffler2024reinvent}}}
        &  \makecell[l]{0.591 $\pm$ 0.002}
        &  \makecell[l]{0.709 $\pm$ 0.003}
        &  \makecell[l]{-8.526 $\pm$ 0.002}
        &  \makecell[l]{0.710 $\pm$ 0.006}
        &  \makecell[l]{2.561 $\pm$ 0.001}
        &  \makecell[l]{3.116 $\pm$ 0.089}
        \\
        \textbf{ }
        &  \textbf{\makecell[l]{Similarity ($\geq$ 0.7) \citep{loeffler2024reinvent}}}
        &  \makecell[l]{0.585 $\pm$ 0.001}
        &  \makecell[l]{0.705 $\pm$ 0.004}
        &  \makecell[l]{-8.584 $\pm$ 0.019}
        &  \makecell[l]{0.723 $\pm$ 0.000}
        &  \makecell[l]{2.604 $\pm$ 0.003}
        &  \makecell[l]{2.841 $\pm$ 0.009}
        \\
        \textbf{ }
        &  \textbf{\makecell[l]{Scaffold \citep{loeffler2024reinvent}}}
        &  \makecell[l]{0.581 $\pm$ 0.001}
        &  \makecell[l]{0.701 $\pm$ 0.004}
        &  \makecell[l]{-8.524 $\pm$ 0.011}
        &  \makecell[l]{0.718 $\pm$ 0.003}
        &  \makecell[l]{2.618 $\pm$ 0.021}
        &  \makecell[l]{2.840 $\pm$ 0.018}
        \\
        \textbf{ }
        &  \textbf{\makecell[l]{Scaffold Generic \citep{loeffler2024reinvent}}}
        &  \makecell[l]{0.592 $\pm$ 0.003}
        &  \makecell[l]{0.704 $\pm$ 0.004}
        &  \makecell[l]{-8.590 $\pm$ 0.033}
        &  \makecell[l]{0.725 $\pm$ 0.001}
        &  \makecell[l]{2.542 $\pm$ 0.018}
        &  \makecell[l]{2.916 $\pm$ 0.004}
        \\
        \textbf{ }
        &  \textbf{\makecell[l]{Molsearch \citep{sun2022molsearch}}}
        &  \makecell[l]{0.548 $\pm$ 0.002} 
        &  \makecell[l]{{0.731} $\pm$0.002}
        &  \makecell[l]{-8.750  $\pm$ 0.028}
        &  \makecell[l]{\underline{0.730} $\pm$ 0.003}
        &  \makecell[l]{2.981 $\pm$ 0.014}
        &  \makecell[l]{2.290 $\pm$ 0.027}
        \\
        \textbf{ }
        &  \textbf{\makecell[l]{MIMOSA \citep{fu2021mimosa}}}
        &  \makecell[l]{0.553 $\pm$ 0.002} 
        &  \makecell[l]{0.721  $\pm$ 0.002} 
        &  \makecell[l]{\underline{-8.980} $\pm$ 0.039}
        &  \makecell[l]{0.716 $\pm$ 0.002}
        &  \makecell[l]{3.066 $\pm$ 0.017}
        &  \makecell[l]{2.491 $\pm$ 0.019}
        \\
        \textbf{ }
        &  \textbf{\makecell[l]{{DrugEx v3} \citep{liu2023drugex}}}
        &  \makecell[l]{0.642 $\pm$ 0.002} 
        &  \makecell[l]{\underline{0.754}  $\pm$ 0.002} 
        &  \makecell[l]{{-8.762} $\pm$ 0.037}
        &  \makecell[l]{0.583 $\pm$ 0.002}
        &  \makecell[l]{\underline{2.488} $\pm$ 0.015}
        &  \makecell[l]{\textbf{5.827} $\pm$ 0.017}
        \\
        \textbf{ }
        &  \textbf{\makecell[l]{\fwname (Ours)}}
        &  \makecell[l]{\textbf{0.694} $\pm$ 0.002} 
        &  \makecell[l]{\textbf{0.754} $\pm$ 0.003} 
        &  \makecell[l]{\textbf{-9.462} $\pm$ 0.038}
        &  \makecell[l]{\textbf{0.794} $\pm$ 0.003}
        &  \makecell[l]{\textbf{2.077} $\pm$ 0.017}
        &  \makecell[l]{\underline{3.712}  $\pm$ 0.028}
        \\
        \bottomrule
        \\
    \end{tabular}}}
        \caption{
        {\textbf{Main results.} A comparison of seven baselines including Original, six baselines from REINVENT~4 \{MMP, Similarity $\geq 0.5$, Similarity $\in [0.5,0.7)$, Similarity $\geq 0.7$, Scaffold, Scaffold Generic\}, Molsearch, MIMOSA, DrugEx v3, 
        and \fwname on multiple objectives 
        based on 3CLPro and RTCB datasets with Tanimoto Similarity above 0.6. {The top two results are highlighted as \textbf{1st} and \underline{2nd}. Results are reported for 5 experimental runs.} 
        }
        }
        \label{exp:main_result}
\end{table*}
\vspace{+0.6cm}
\section{Theoretical Analysis}\label{sec:theory}
\fix{We now provide a theoretical analysis of \algname and prove its effectiveness and superiority over prior RL algorithm for both local and global optimizations.}
Recall that we have optimization target $J(\theta)$ defined in \eqref{eq:J(theta)} with advantage function $R^\AP$ defined in \eqref{eq:advantage_preference}.
Define $J_0(\pi)=\E_{X\sim \rho_0}[r_\AP^\pi(X)] = \E^\pi[R_c(Y_{1:T}) - R_c(X)]$ as the ``standard'' RL metric.
We say that BON \emph{strictly improves} over suboptimal molecule if
\begin{align}
    r_{\BON(j)}^{\AP}(Y_{1:T}; X) > r^{\AP}(Y_{1:T}, X), \quad \forall  j\in[T], 
    \label{eq:strict improvment}
\end{align}
for any $Y_{1:T}$ such that $R_c(Y_{1:T}) < \max_{Y_{1:T}'} R_c(Y_{1:T}')$.
\paragraph{\algname can Find the Optimizer.}
Our first result compare the maximizers of $J(\cdot)$ under the \algname framework to those of $J_0(\pi) = \E^\pi[R_c(Y_{1:T})]$.
\begin{lemma}
\label{lem:equivalence}
\vspace{+0.3cm}
    If BON finds a sequence that strictly improves over the current molecule in the sense of \eqref{eq:strict improvment},
    any policy $\pi^*$ maximizes $J(\pi)$ if and only if it maximizes the original reward $J_0(\pi)$.
\end{lemma}
Given the fact that these two optimization targets share the same optimizer, we next study the benefit of using our definition $J$ for gradient update.

\paragraph{Densifying the Reward Signal.}
We remark that using $J(\pi)$ has the advantage of densifying the reward signal, thus making policy optimization easier.
In fact, each $r_{\BON(j)}^\AP(Y_{1:T}, X)$ serves as a reward signal for choosing the next action $Y_{j+1}$ at state $(Y_{1:j}, X)$.
\begin{lemma}
\vspace{+0.2cm}
    \label{lem:gradient}
     Gradient $g$ defined in \eqref{eq:adv:preference} can be rewritten as
    \begin{align}
        g =& \frac {1} {2T} \sum_{t=1}^T \E_{X\sim\rho_0}^\pi \left[\nabla_\theta \log \pi_\theta(Y_{1:t}\given X)  r_{\BON(t)}^{\AP}(Y_{1:T}, X)\right] \notag\\
        & +  \frac 1 2 \cdot \E_{X\sim\rho_0}^\pi \left[\nabla_\theta \log \pi_\theta(Y_{1:T}\given X)  r^\AP(Y_{1:T}, X) \right]. \label{eq:grad_reform}
    \end{align}
\end{lemma}
The last term corresponds to the reward at the end of the generation of the molecule, while the first term provides ``partial'' reward on each generation step.
This gradient form suggests that incorporating partial molecule's advantage function densifies the reward signal along the trajectory, enabling the learning agent to explore more efficiently \citep{riedmiller2018learning, vecerik2017leveraging}. \fix{We defer the proof of \lemref{lem:equivalence} and \lemref{lem:gradient} to \appref{app:theory_proof}.}
