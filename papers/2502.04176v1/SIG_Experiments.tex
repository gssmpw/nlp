\section{Experiments}
\label{sec:experiments}

\begin{table}[tb!]
\caption{Retrieval performance of the BGE-M3 model on the MRAMG-Bench.}
  \centering
  \vspace{-1mm}
\resizebox{0.49\textwidth}{!}{
    \begin{tabular}{ccccccc}
    \toprule
    \textbf{Metrics} & MRAMG-Wit & MRAMG-Wiki & MRAMG-Web & MRAMG-Arxiv & MRAMG-Recipe & MRAMG-Manual \\
    \midrule
    \textbf{Image Recall@10} & 0.99  & 0.99  & 0.99  & 0.94  & 0.87  & 0.72 \\
    \textbf{Context  Recall@10} & 0.9   & 0.89  & 0.95  & 0.83  & 0.91  & 0.87 \\
    \bottomrule
    \end{tabular}%

  \label{tab:retrieve}%
}
  \label{tab:latest1}%
\end{table}%


\begin{table*}[htbp]
  \centering
  \caption{Comprehensive performance results on MRAMG-Bench. Prec., Rec., B.S.,Ord., Comp., Pos., and Avg. represent image precision, image recall, BERTScore,image ordering score, comprehensive score, image position score, and average score, respectively. The highest scores for each group are highlighted in bold.}
  \vspace{-1mm}
  \resizebox{\textwidth}{!}{
    \begin{tabular}{cl|cccccc|cccccc|ccccccc}
    \toprule
    \multirow{2}[4]{*}{\textbf{Framework}} & \multicolumn{1}{c|}{\multirow{2}[4]{*}{\textbf{Model}}} & \multicolumn{6}{c|}{\textbf{ Web Data}}       & \multicolumn{6}{c|}{\textbf{Academic Paper Data}} & \multicolumn{7}{c}{\textbf{ Lifestyle Data}} \\
\cmidrule{3-21}          &       & \textbf{Prec.} & \textbf{Rec.} & \textbf{B.S.} & \textbf{Comp.} & \textbf{Pos.} & \textbf{Avg.} & \textbf{Prec.} & \textbf{Rec.} & \textbf{B.S.} & \textbf{Comp.} & \textbf{Pos.} & \textbf{Avg.} & \textbf{Prec.} & \textbf{Rec.} & \textbf{B.S.} & \textbf{Ord.} & \textbf{Comp.} & \textbf{Pos.} & \textbf{Avg.} \\
    \midrule
    \multicolumn{1}{c}{\multirow{11}[2]{*}{\textbf{Rule-Based}}} & \multicolumn{1}{p{4.125em}|}{GPT-4o } & 43.54 & 37.30 & 92.35 & 77.15 & 43.86 & 50.75 & 55.42 & 63.04 & 94.67 & 84.20 & 75.75 & 68.02 & 47.04 & 63.54 & 92.01 & 43.54 & 79.17 & \textbf{77.13} & \textbf{65.68} \\
          & GPT-4o-mini  & 38.20 & 33.08 & 91.10 & 76.59 & 38.57 & 46.87 & 51.71 & 59.29 & 94.36 & 85.20 & 73.75 & 66.07 & 49.14 & 61.52 & 91.13 & \textbf{43.87} & 79.32 & 75.89 & 64.92 \\
          & Claude-3.5-Sonnet  & 47.65 & 38.43 & 93.42 & \textbf{80.63} & 48.14 & 53.55 & 55.17 & 62.79 & 94.09 & 84.20 & 75.75 & 67.48 & \textbf{50.32} & 61.17 & 92.02 & 43.32 & \textbf{79.50} & 76.39 & 64.91 \\
          & Gemini-1.5-Pro  & 31.27 & 26.62 & 87.37 & 74.83 & 31.76 & 41.21 & 52.43 & 56.29 & 93.85 & 83.20 & 70.28 & 64.15 & 49.18 & 50.57 & 88.32 & 38.73 & 78.14 & 73.14 & 60.58 \\
          & DeepSeek-V3  & \textbf{55.49} & \textbf{46.62} & \textbf{94.12} & 80.18 & \textbf{56.16} & \textbf{59.04} & \textbf{56.12} & \textbf{67.29} & \textbf{94.90} & \textbf{84.30} & \textbf{78.46} & \textbf{70.05} & 27.07 & 57.56 & \textbf{92.42} & 24.07 & 74.19 & 65.19 & 57.22 \\
          & Qwen2-VL-7B-Instruct  & 37.98 & 34.81 & 91.26 & 70.99 & 38.28 & 46.32 & 49.17 & 52.17 & 92.09 & 78.90 & 67.08 & 60.88 & 43.79 & 60.93 & 91.49 & 39.52 & 77.70 & 77.33 & 63.87 \\
          & Qwen2-VL-72B-Instruct  & 34.81 & 31.49 & 88.70 & 71.89 & 35.14 & 43.75 & 45.42 & 48.71 & 92.39 & 79.60 & 65.42 & 59.50 & 31.82 & 49.30 & 89.88 & 25.51 & 72.92 & 70.94 & 56.45 \\
          & InternVL-2.5-8B  & 30.78 & 28.38 & 87.45 & 69.88 & 31.05 & 40.55 & 39.20 & 48.29 & 91.42 & 76.70 & 65.17 & 58.16 & 29.39 & 51.47 & 90.11 & 23.28 & 72.76 & 71.29 & 55.79 \\
          & InternVL-2.5-78B  & 38.79 & 34.49 & 90.97 & 74.82 & 39.06 & 47.04 & 52.21 & 62.00 & 94.51 & 85.40 & 75.38 & 67.57 & 22.61 & \textbf{67.71} & 92.19 & 19.41 & 74.95 & 56.55 & 56.26 \\
          & Llama-3.1-8B-Instruct  & 23.86 & 20.54 & 82.64 & 58.40 & 26.15 & 33.63 & 21.50 & 23.08 & 85.92 & 58.70 & 29.00 & 35.32 & 28.23 & 36.25 & 81.29 & 18.33 & 65.12 & 60.90 & 46.44 \\
          & Llama-3.3-70B-Instruct & 48.84 & 41.73 & 92.87 & 77.60 & 49.59 & 54.31 & 53.00 & 58.17 & 94.39 & 83.60 & 73.42 & 65.55 & 30.27 & 50.38 & 92.91 & 25.33 & 73.08 & 69.53 & 57.30 \\
    \midrule
    \multicolumn{1}{c}{\multirow{8}[2]{*}{\textbf{MLLM-Based}}} & GPT-4o  & 82.75 & 80.57 & \textbf{94.70} & 85.95 & 84.65 & 79.66 & \textbf{60.39} & 74.29 & \textbf{95.15} & 87.50 & \textbf{90.39} & \textbf{76.87} & \textbf{43.77} & 44.68 & \textbf{92.50} & \textbf{32.47} & 81.36 & \textbf{77.35} & 61.01 \\
          & GPT-4o-mini  & 69.98 & 86.08 & 94.59 & 80.58 & 72.93 & 76.29 & 36.17 & 74.79 & 95.08 & 83.20 & 74.66 & 68.62 & 29.34 & 47.71 & 91.71 & 21.95 & 77.19 & 69.95 & 56.32 \\
          & Claude-3.5-Sonnet  & \textbf{91.15} & \textbf{93.68} & 93.73 & \textbf{86.70} & \textbf{93.71} & \textbf{85.51} & 47.12 & \textbf{83.50} & 94.65 & \textbf{87.60} & 86.38 & 74.84 & 29.35 & 52.09 & 90.92 & 21.88 & 79.85 & 74.80 & 57.71 \\
          & Gemini-1.5-Pro  & 89.27 & 90.49 & 92.13 & 83.77 & 91.05 & 83.30 & 58.13 & 80.25 & 94.30 & 85.90 & 83.61 & 75.14 & 38.59 & \textbf{57.40} & 90.05 & 31.99 & \textbf{81.37} & 69.84 & \textbf{61.58} \\
          & Qwen2-VL-7B-Instruct  & 26.48 & 32.65 & 87.51 & 60.98 & 30.24 & 40.19 & 1.63  & 4.00  & 84.62 & 49.80 & 4.46  & 21.11 & 9.66  & 15.15 & 84.84 & 4.26  & 55.92 & 16.22 & 26.69 \\
          & Qwen2-VL-72B-Instruct  & 59.65 & 60.59 & 92.69 & 80.50 & 61.49 & 64.16 & 31.99 & 44.87 & 93.53 & 84.20 & 55.16 & 55.56 & 19.61 & 26.25 & 91.21 & 12.36 & 74.36 & 39.94 & 41.36 \\
          & InternVL-2.5-8B  & 52.71 & 65.86 & 91.89 & 72.78 & 61.17 & 63.30 & 12.22 & 27.87 & 83.72 & 58.10 & 28.49 & 35.34 & 22.19 & 37.94 & 89.41 & 14.54 & 73.99 & 60.11 & 48.22 \\
          & InternVL-2.5-78B & 75.50 & 76.27 & 93.98 & 84.11 & 78.68 & 74.96 & 36.62 & 55.00 & 94.47 & 84.80 & 64.11 & 61.01 & 21.43 & 29.09 & 91.11 & 13.53 & 75.43 & 51.99 & 45.23 \\
    \midrule
    \multicolumn{1}{c}{\multirow{7}[2]{*}{\textbf{LLM-Based}}} & GPT-4o  & 80.86 & 77.92 & \textbf{94.92} & 85.74 & 81.41 & 77.73 & \textbf{65.28} & 76.54 & \textbf{95.23} & 88.90 & 84.84 & 76.91 & 47.48 & 62.40 & 92.29 & 42.55 & 80.60 & 80.43 & 66.11 \\
          & GPT-4o-mini  & 69.58 & 90.95 & 94.35 & 81.29 & 70.96 & 76.91 & 37.69 & 83.33 & 95.01 & 84.50 & 69.07 & 69.91 & 44.36 & 38.27 & \textbf{92.69} & 31.16 & 83.24 & 54.96 & 53.33 \\
          & Claude-3.5-Sonnet  & 92.47 & 93.86 & 94.32 & \textbf{86.17} & 93.43 & 85.88 & 62.17 & \textbf{88.00} & 94.37 & \textbf{89.60} & \textbf{88.17} & \textbf{79.22} & 59.83 & 64.45 & 91.51 & 51.51 & \textbf{84.63} & \textbf{82.58} & 69.04 \\
          & Gemini-1.5-Pro  & \textbf{93.63} & 93.84 & 93.48 & 84.35 & \textbf{94.29} & 85.85 & 59.85 & 78.63 & 94.32 & 87.60 & 80.15 & 75.00 & \textbf{62.23} & \textbf{68.34} & 90.83 & \textbf{54.62} & 83.10 & 79.66 & \textbf{70.80} \\
          & DeepSeek-V3  & 93.08 & \textbf{95.51} & 94.68 & 85.64 & 93.98 & \textbf{86.45} & 46.57 & 81.13 & 94.70 & 87.50 & 70.01 & 72.53 & 45.71 & 67.56 & 91.78 & 40.35 & 82.64 & 77.03 & 66.04 \\
          & Llama-3.1-8B-Instruct  & 28.87 & 31.35 & 82.53 & 52.59 & 31.31 & 38.94 & 1.50  & 2.00  & 80.61 & 43.40 & 4.00  & 18.36 & 11.71 & 12.75 & 75.36 & 6.21  & 40.97 & 17.15 & 23.23 \\
          & Llama-3.3-70B-Instruct & 74.26 & 95.49 & 94.35 & 82.44 & 76.01 & 79.35 & 38.78 & 84.88 & 95.01 & 83.40 & 64.59 & 68.93 & 35.29 & 69.34 & 91.89 & 30.60 & 80.15 & 70.66 & 61.86 \\
    \bottomrule
    \end{tabular}%
    }
  \label{table:main_results}%
\end{table*}%

%Now, we present our evaluation metric, experiment settings and results.

\subsection{Evaluation Metric}

\subsubsection{Retrieve Evaluation}

To evaluate the retrieval performance, 
We consider the following metrics:

\begin{itemize}
    \item 
    \textbf{Context Recall \citep{es2023ragas}} uses LLMs to evaluate whether the retrieved documents contain all the relevant  information required for answer generation.
    \item 
    \textbf{Visual Recall} measures the percentage of retrieved images relative to the total number of images in the ground truth.
    % It is computed as:
    % \[
    % \text{Visual Recall} = \frac{\text{Retrieved Relevant Images}}{\text{Total Relevant Images in Ground Truth}}
    % \]
    % where ``Retrieved Relevant Images" refers to the number of images retrieved that are present in the ground truth, and ``Total Relevant Images in Ground Truth" refers to the total number of relevant images that should have been retrieved.
\end{itemize}

\subsubsection{Generation Evaluation}

To evaluate performance of multimodal answers, we consider the following metrics\footnote{See detailed metrics at: https://huggingface.co/MRAMG
.}, which can be divided into two categories: statistical-based metrics (first six metrics) and LLM-based metrics (last four metrics). %We use the following \textit{statistical-based metrics}:

\begin{itemize}
    \item 
    \textbf{Image Precision} measures the percentage of correct images in the multimodal answer relative to the total number of inserted images, assessing whether irrelevant images were introduced. 
    % It is computed as:
    % \[
    % \text{Image Precision} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}
    % \]
    % where True Positives are the correctly inserted images, and False Positives are irrelevant images that were included.

    \item 
    \textbf{Image Recall} measures the percentage of correct images in the multimodal answer relative to the total number of images in the ground truth, evaluating whether the answer effectively includes useful image information. 
    % It is computed as:
    % \[
    % \text{Image Recall} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}
    % \]
    % where False Negatives are the images in the ground truth that were omitted in the generated multimodal answer.

    \item 
    \textbf{Image F1 Score} is the harmonic mean of Precision and Recall, providing an overall evaluation of the image quality in the multimodal answer. 
    % It is calculated as:
    % \[
    % \text{Image F1 Score} = 2 \times \frac{\text{Image Precision} \times \text{Image Recall}}{\text{Image Precision} + \text{Image Recall}}
    % \]
    \item 
    \textbf{Image Ordering Score} evaluates whether the order of images inserted into the multimodal answer matches the order of images in the ground truth \footnote{Here, we apply the ordering score only to lifestyle data (MRAMG-Recipe and MRAMG-Manual), as instances in these datasets typically contain a large number of images, and the order of the images is crucial.}. Specifically, we compute the weighted edit distance between the two image sequences. % to reflect the difference in their order. %See more details in Appendix \ref{appendix:ordering_score}.
    \item 
    \textbf{Rouge-L \citep{lin-2004-rouge}} is a text generation evaluation metric based on the longest common subsequence, measuring the structural similarity between the answer and the ground truth.
    \item 
    \textbf{BERTScore \citep{zhang2019bertscore}} is a text generation evaluation metric based on BERT \citep{devlin2018bert}, used to assess the semantic similarity between the text in the answer and the ground truth.
    \item 
    \textbf{Image Relevance} measures evaluate the relevance of the inserted image to the query-answer pair, specifically assessing whether the content described by the image is meaningfully related to the content of the QA \citep{zhu2024murar,ma2024multi}. %This metric assigns a score to each image appearing in the answer, with scores ranging from 1 to 5.
    \item 
    \textbf{Image Effectiveness} measures evaluate the effectiveness of the images inserted into the multimodal answer, assessing whether the images align with the QA content and contribute to the understanding of the answer \citep{zhu2024murar,ma2024multi}. 
    %This metric also assigns a score to each image, with scores ranging from 1 to 5.
    \item 
    \textbf{Image Position Score} is used to assess the appropriateness of the image placement in the multimodal answer. 
    %It assigns a score of either 0 or 1 to each image, based on whether its position is deemed correct and suitable.
    \item 
    \textbf{Comprehensive Score} reflects overall quality of the multimodal answer, evaluating whether the answer appropriately addresses the query and maintains overall coherence. %It particularly considers whether the insertion of images enhances the answer, making it visually engaging and more expressive. 
    %This metric assigns a score to the complete answer, with scores ranging from 1 to 5.
\end{itemize}
\vspace{-5pt}
\subsection{Experimental Baselines and Settings}
To further validate our dataset and generation framework, we consider testing the generation performance of multimodal answers across a wide range of LLMs and MLLMs. 

\paragraph{Model Baselines} We evaluate 4 closed-source models and 7 open-source models. Specifically, we select 4 popular closed-source multimodal large models: GPT-4o \citep{gpt4o}, GPT-4o-mini \citep{gpt4o_mini}, Claude-3.5-Sonnet \citep{anthropic_claude_2024}, Gemini-1.5-Pro \citep{team2024gemini}. For the open-source models, we choose 4 multimodal large models: Qwen2-VL-7B-Instruct \citep{wang2024qwen2}, Qwen2-VL-72B-Instruct \citep{wang2024qwen2}, InternVL-2.5-8B \citep{chen2024expanding}, InternVL-2.5-78B \citep{chen2024expanding} and 3 text-based large models: 
DeepSeek-V3 \citep{liu2024deepseek}, Llama-3.1-8B-Instruct \citep{dubey2024llama} and Llama-3.3-70B-Instruct \citep{dubey2024llama}. Among these, for the four most widely used closed-source multimodal models, we test all the llm/mllm/rule-based strategies in our framework. For the four other multimodal models, we test the mllm/rule-based strategies. For the remaining three text-based models, we test the llm/rule-based strategies.

\paragraph{Experiment Details} In the multimodal documents, images are replaced with <PIC> placeholders and each document is chunked using a size of 256 with the SentenceSplitter \citep{Liu_LlamaIndex_2022} to ensure efficient processing. During the retrieval phase, we use the BGE-M3 model \citep{chen2024bge} to retrieve the top-k $= 10$ relevant chunks, which contain the corresponding images. Concretely, images presented within the retrieved chunks are regarded as the corresponding retrieved images. In the evaluation stage, we utilize GPT-4o \citep{gpt4o} as the judging model to assess the performance of LLM-based metrics.
\vspace{-10pt}
% Here, we present the details of our experiment. In the retrieval stage, we first chunk the multimodal document using a chunk size of 256, ensuring that placeholders replace images to prevent them from being split across chunks. The BGE-M3 embedding model \citep{chen2024bge} is then employed to calculate the cosine similarity between each chunk and the query, retrieving the top-10 relevant chunks based on this similarity. During this process, image retrieval is handled by referencing the textual context surrounding the images. In the evaluation stage, we use GPT-4o \citep{gpt4o} as the judging model to assess the performance of LLM-based metrics.
% Here, we will present our experiment details. In the retrieve stage, We use SentenceSplitter \citep{Liu_LlamaIndex_2022} to chunk the multimodal document, and then employ the BGE-M3 embedding model \citep{chen2024bge} to calculate the cosine similarity between each chunk and the query for retrieval. In this process, images in the multimodal document are replaced with placeholders, and image retrieval is conducted based on the textual context surrounding the images. In the evaluation stage, we utilize GPT-4o \citep{gpt4o} as the judging model to assess the performance of LLM-based metrics.
% \qinhan{是介绍清楚chunk size = 256
% 对于doc中的图片替换为占位符，使用chunk size =256进行切块，（为了避免pic占位符被切割，使用SentenceSplitter 可写可不写）
% 对于检索阶段，Embedding model选择bge-m3，top-k =10
% 图片通过检索到到相关chunk被召回

% }
% \binghui{切chunk}

% \binghui{图片如何retrieve}


\subsection{Experiment Results}

% Now, we present our experiment results about retrieval performance and generation performance.

\subsubsection{Retrieval Performance}

As shown in Table \ref{tab:retrieve}, we observe that both image recall and context recall perform well for Web Data %(including MRAMG-Wit, MRAMG-Wiki, and MRAMG-Web)
, with MRAMG-Web achieving particularly high scores of 0.99 and 0.95, respectively. 
In contrast, both metrics progressively decline for the three other datasets
%the MRAMG-Arxiv, MRAMG-Recipe, and MRAMG-Manual datasets
, which can be attributed to their longer text passages and higher image counts. Nevertheless, overall retrieval effectiveness remains strong. 
\vspace{-8pt}
\subsubsection{Generation Performance}
\label{generation}

In this section, we evaluate generation performance, with results presented for three domains (web/academic paper/lifestyle). Due to space limitations, full results are available at: https://huggingface.co/MRAMG.


\textbf{Generation Performance Across Different Datasets.} From the results in Table \ref{table:main_results}, it is evident that the overall generation performance significantly decreases as the dataset complexity increases, which aligns with our expectations regarding the varying difficulty of different datasets. Notably, the performance of rule-based methods on simpler datasets is basically suboptimal, which can be attributed to data characteristics, particularly related to the Avg Images Per Doc metric. According to Table \ref{tab:statistics}, this metric is 1 for all the Web Data datasets 
%(MRAMG-Wit, MRAMG-Wiki, MRAMG-Web)
, whereas in the other datasets, it is substantially higher. With fewer images per document, the rule-based illustration algorithm struggles to effectively associate images with sentences, as multiple sentences often correspond to a single image. This results in ambiguous correlations and complicates threshold selection, leading to a higher error rate. In contrast, as the number of images increases, the gap in correlation between rule-based methods and other approaches becomes more pronounced. Under these conditions, rule-based matching performs better due to improved alignment with sentence-image relationships.

\textbf{Generation Performance Across Different Models.} As shown in Table \ref{table:main_results}, advanced models such as Gemini, Claude, GPT-4o, and Deepseek-V3 consistently outperform smaller open-source models ($\sim7$B parameters) across all domains and methods. These smaller models exhibit subpar performance across different methods and dataset domains, even when utilizing rule-based generation techniques. In contrast, larger open-source models ($\sim70$B parameters) significantly reduce the performance gap with closed-source models, achieving results within approximately a tenth of the margin on simpler datasets, such as Web Data. For example, InternVL-2.5-78B and Llama-3.3-70B-Instruct attain Image Precision scores of 75.5 and 74.26, respectively, with average scores surpassing 70. On more challenging datasets, however, the performance gap becomes more pronounced, highlighting the limitations of open-source models in handling complex MRAMG tasks. Specifically, when employing MLLM-based methods, Qwen2-VL-72B-Instruct achieves an Image Recall score of only 26.25, which is significantly lower than Gemini's 57.4. This result underscores the challenges faced by even 70B-scale models in accurately identifying images in complex scenarios.  
Nevertheless, smaller open-source models remain a cost-effective solution for simpler applications with limited computational resources. In particular, the order metric for the Lifestyle domain demonstrates poor performance across all models and methods, with none achieving a passing score. Notably, GPT-4o performs best under both the Rule-based and MLLM-based methods, scoring 43.54 and 32.47, respectively, while the Gemini model attains the highest score of 54.62 under the LLM-based method. However, even these scores fall short of the passing threshold, underscoring the ambiguity in LLM and MLLM models regarding the concept of image insertion order, which remains an unresolved challenge.




















% \textbf{Generation Performance Across Different Generation Methods.}
% In comparing different methods, an overall performance trend emerges:
% \begin{equation}
%     \text{LLM-based} > \text{MLLM-based} > \text{Rule-based} . \nonumber
% \end{equation}
% For example, on Web Data, the Gemini model achieves average scores of 85.85, 83.3, and 41.21 for LLM-based, MLLM-based, and Rule-based methods, respectively.

% LLM-based method: By integrates the context surrounding images into the generation process, this method achieve natural and precise image insertion and highlighting the critical role of image context in ensuring insertion accuracy. 

% MLLM-based methods: While effective on simpler datasets like Web, performance deteriorates on more challenging datasets due to the increased difficulty of distinguishing between visually similar images, revealing the current limitations of model capabilities. In this method, the Gemini model achieves the highest overall scores on the Web and Lifestyle datasets, scoring 85.51 and 61.58, respectively, while Claude leads on the Academic Paper Data with a score of 76.87.

% Rule-based methods: While these methods exhibit significantly lower performance compared to model-based approaches on simpler datasets, the performance gap diminishes as the dataset complexity increases. In the lifestyle domain, rule-based methods even outperform certain model-based approaches. For instance, GPT-4o achieves an overall score of 65.68 using rule-based methods, surpassing its MLLM-based score of 61.01. Notably, the Deepseek-V3 model demonstrates outstanding performance on Web and Academic Paper data, achieving top scores across almost all metrics. This suggests that for simpler datasets, rule-based methods align well with the Deepseek-V3 model, indicating a tendency of the model to leverage such approaches effectively.

% Despite limitations, rule-based methods offer key advantages:
% \begin{itemize}
%     \item Flexibility: Unconstrained by context window sizes or input image limits.
%     \item Cost Efficiency: Reduce computational costs by up to one-third vs. LLM-based and half vs. MLLM-based methods.
%     \item Stability: Avoid instability issues like erroneous placeholder generation.
% \end{itemize}
% Overall, rule-based methods offer a viable and efficient alternative, particularly in resource-constrained or stability-critical scenarios.

% The suboptimal performance of rule-based methods on simpler datasets can be attributed to data characteristics, particularly the Avg Images Per Doc metric. In web data, this metric is 1, whereas the other two datasets exhibit significantly higher values. With fewer images per document, the rule-based illustration algorithm struggles to effectively associate images with sentences, as multiple sentences often correspond to a single image. This leads to ambiguous correlations and makes threshold selection challenging, resulting in a higher error rate. Conversely, as the number of images increases, the correlation gap between rule-based methods and other approaches becomes more pronounced. Under such conditions, rule-based matching demonstrates better performance due to improved alignment with sentence-image relationships.

% 另一方面，LLM-based方法的表现普遍不亚于（基本是较好于）Rule-based的方法，这体现出目前的大模型的in-context reasoning能力

\textbf{Generation Performance Across Different Generation Methods.}  
In comparing different methods, an overall performance trend emerges: $\text{LLM-based} > \text{MLLM-based} > \text{Rule-based}$.  
% \begin{equation}
%     \text{LLM-based} > \text{MLLM-based} > \text{Rule-based} . \nonumber
% \end{equation}
For example, on Web Data, the Gemini model achieves average scores of 85.85, 83.3, and 41.21 for LLM-based, MLLM-based, and Rule-based methods, respectively.

\textbf{LLM-based methods:} By integrating contextual information surrounding images into the generation process, this method achieves natural and precise image insertion, underscoring the critical role of context in ensuring insertion accuracy.

\textbf{MLLM-based methods:} While effective on simpler datasets such as Web Data, their performance deteriorates on more challenging datasets due to the increased difficulty of distinguishing visually similar images, revealing the current limitations of model capabilities. In this category, the Gemini model achieves the highest average scores on the Web and Lifestyle Data, scoring 85.51 and 61.58, respectively, while Claude outperforms others on the Academic Paper Data with a score of 76.87.

\textbf{Rule-based methods:} Although these methods exhibit significantly lower performance compared to model-based approaches on simpler datasets, the performance gap diminishes as dataset complexity increases. In the Lifestyle Data domain, rule-based methods even outperform certain model-based approaches. For instance, GPT-4o achieves an average score of 65.68 using rule-based methods, surpassing its MLLM-based score of 61.01. Notably, the Deepseek-V3 model demonstrates outstanding performance on Web and Academic Paper Data, achieving top scores across almost all metrics. This suggests that for simpler datasets, rule-based methods align well with the Deepseek-V3 model, indicating the model's tendency to leverage such approaches effectively. Despite their limitations, rule-based methods offer several key advantages:  
\begin{itemize}
    \item \textbf{Flexibility:} Not constrained by context window sizes or input image limits.
    \item \textbf{Cost Efficiency:} Reduces computational costs by up to one-third compared to LLM-based methods and by half compared to MLLM-based methods.
    \item \textbf{Stability:} Avoids instability issues such as erroneous placeholder generation.
\end{itemize}
Overall, rule-based methods provide a viable and efficient alternative, particularly in resource-constrained or stability-critical scenarios. Furthermore, the performance of LLM-based methods is generally on par with or even superior to rule-based methods, showcasing the strong \textit{in-context reasoning} capabilities of modern large models.

% \textbf{Order Metric Analysis.} In particular, the Order metric for the Lifestyle domain demonstrates poor performance across all models and methods, with none achieving a passing score. Notably, GPT-4o performs best under both the Rule-based and MLLM-based methods, scoring 43.54 and 32.47, respectively, while the Gemini model attains the highest score of 54.62 under the LLM-based method. However, even these scores fall short of the passing threshold, underscoring the ambiguity in LLM and MLLM models regarding the concept of image insertion order, which remains an unresolved challenge.


