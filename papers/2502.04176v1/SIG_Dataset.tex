\section{Dataset Construction}
\label{sec:dataset}
\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{Fig/data_construction.pdf}
    \caption{
    The construction pipeline of MRAMG-Bench. It is divided into three stages: (1) Data Selection and Preprocessing, we collect, clean and preprocess data to ensure the quality of initial datasets. (2) QA Generation and Refinement, we formulate questions and answers and refine the formulated QA pairs. (3) Data Quality Check, we engage annotators and experts in a three-stage data quality check to ensure high-quality of the entire benchmark.
    }
    \label{fig:data_construction}
\end{figure*}
% MRMAG-Bench is a real-world, multimodal benchmark designed to
% address the current gap in comprehensive evaluation frameworks for MRMAG tasks. A key challenge in developing MRMAG tasks lies in crafting questions that do not explicitly reference images but require answers that naturally incorporate them. For instance, the
% question “How do you make scrambled eggs?” should be answered
% with a sequence of images depicting the cooking process. To address this challenge, we introduce this MRMAG-Bench benchmark.
In this section, we detail the construction of MRAMG-Bench. It consists of three domains: Web, Academic Paper and Lifestyle, which respectively contains web-based datasets—MRAMG-Wit, MRAMG-Wiki, MRAMG-Web, and a scientific dataset-MRAMG-Arxiv, and two lifestyle-oriented datasets-MRAMG-Recipe, MRAMG-Manual. The process of creating datasets in different domains can be divided into three stages: (1) Data Selection and Preprocessing, (2) QA Generation and Refinement, (3) Data Quality Check. The overview of this process is shown in Figure \ref{fig:data_construction}.
\subsection{Data Selection and Preprocessing}
We undertake three steps during this stage: (1) Collect, (2) Filter Data and Enhance Context, and (3) Refine.
%This stage is to collect and clean multimodal document information from various sources, resulting in documents where images and text are correctly aligned.

\subsubsection{Collect} The goal of this step is to collect MRAMG task-oriented multimodal document contextual data.
\vspace{-1mm}

\paragraph{Web Data.} This category includes Wikipedia pages and articles, derived from Wit dataset \citep{srinivasan2021wit}, WikiWeb2M dataset \citep{burns2023wikiweb2m}, and WebQA dataset \citep{chang2022webqa}. These datasets typically feature a rich integration of text and images, with the insertion order of images in Wikipedia, which serves as a reliable reference. We select val sets and test sets from Wit dataset \citep{srinivasan2021wit}, and sample 82k entries from the 100k entries in the test set of WikiWeb2M dataset \citep{burns2023wikiweb2m} , and choose img-posFacts set in WebQA dataset \citep{chang2022webqa}, as the initial collection of MRAMG-Wit dataset, MRAMG-Wiki dataset and MRAMG-Web dataset.
These datasets primarily consist of single-image documents with relatively short length. Consequently, we categorize the three Web Data datasets as \textbf{level 1: easy} (difficulty level), where models can easily handle our task in this domain due to simple scenarios.
\vspace{-1mm}
\paragraph{Academic Paper Data.} We collect 150 LaTeX source files and their corresponding PDF documents from the arXiv repository, with publication years spanning 2023 to 2024. These papers exhibit a seamless integration of text and images, providing an optimal foundation for tasks that demand precise image-text alignment. We manually select 110 high-quality papers with rich image-text interleaved content as the initial collection of MRAMG-Arxiv. It is defined as the \textbf{level 2: medium} dataset in our benchmark, where QA instances generally contain 1 to 4 images. Targeting at our task, it presents more challenges for models to retrieve and comprehend information from scientific document.
\vspace{-1mm}
\paragraph{Lifestyle Data.} Datasets in this domain features in fine-integration of a sequence of operation images and instruction texts. These dataset are thoroughly applicable to practical scenarios encountered in users' daily lives, which are collected from RecipeQA \citep{yagcioglu2018recipeqa}, 
ManualsLib\footnote{\url{https://www.manualslib.com}}
%\citep{manual}
and the Technical Dataset\footnote{\url{https://www.kaggle.com/datasets/ahmedhgabr/technical-illustration/data}}.
%\citep{manual}
%\citep{kaggle}.
We select the evaluation set from RecipeQA \citep{yagcioglu2018recipeqa}, and select 40 categories of instruction manuals with rich text-image content for subsequent construction of MRAMG-Recipe dataset and MRAMG-Manual dataset, respectively. We define Lifestyle Data as \textbf{level 3: difficult}, as they exhibit a significantly higher density of images and encompass diverse problem types, including single-image, no-image, and multi-image answers. 
Moreover, these datasets pose a significant challenge for models in determining the order of image insertion.
%More importantly, these datasets introduce a significant challenge for models related to the order of image insertion.
\subsubsection{Filter Data and Enhance Context}
The goal of this step is to clean data and acquire high-quality contexts, with images correctly inserted in the appropriate context positions.

For Web Data, we first filter out data entries with excessively long or short context, and data entries lacking images information or with overly large-sized images. We then implement two stages of deduplication: (1) MinHash-based deduplication \citep{shrivastava2014defense} method to remove data with similar images, and (2) string and semantic similarity measures \citep{TF-IDF,BGE} to further deduplicate data. Subsequently, we enhance the original contexts through crawling entire texts on original Wikipedia pages, and extending contexts leveraging GPT-4o based on the given information provided in the original data. For MRAMG-Web dataset, original collection contains questions where each question generally involves two entities and requires two distinct images for answering. We then extract entities included in the questions and retrieve entity-relevant texts in dataset as context supplements.

For Academic Paper Data, we engage ten graduate
students specializing in computer science, with expertise in fields such as Natural Language Processing (NLP) and Computer Vision (CV), to serve as expert annotators. 
Each annotator is assigned papers within their specific domain of expertise, ensuring precise and contextually relevant annotations. 
%Each annotator is assigned papers within their domain of expertise to ensure accurate and contextually meaningful annotations. 
The annotators meticulously read the selected papers, identify and extract textual content and corresponding figures that are most suitable for creating interleaved text-image contexts. This ensures accurate alignment of text and images in contexts.

For Lifestyle Data, in MRAMG-Recipe dataset, each recipe contains multiple step texts and each step text contains multiple images. We first combine multiple images into one image for each recipe step text and insert them at the end of each step text. We then aggregate all the recipe step texts as an entire image-text interleaved recipe context. In MRAMG-Manual dataset, we employ MinerU \citep{wang2024mineru} to parse the PDF manual documents into markdown format, preserving the original contextual order of images and text within each manual.
%in order to preserve the original contextual order of images and text within each manual. 
A team of annotators is tasked with removing sensitive or irrelevant information, adjusting image placements within the content, and correcting any errors in the text.
%A team of annotators is required to remove texts with sensitive or irrelevant information, adjust image positions in the manual texts, and correct errors existing in the texts.

\subsubsection{Refine}
The final step in this stage is to refine image-text interleaved contexts along with the questions or answers existing in the original data. We leverage both GPT-4o and human verification for quality check. This step aims to achieve two key objectives: correct and consistent image-text context content and appropriate image positions in contexts.


\subsection{QA Generation and Refinement}
This stage can be further divided into three steps: (1) question generation, (2) answer generation, (3) QA pair refine.
%This stage involves generating and refining question-answer pairs from processed multimodal documents across various datasets.

\subsubsection{Question Generation}
For MRAMG-Web dataset, we retain the questions contained in original dataset. As for MRAMG-Wiki, MRAMG-Wit and MRAMG-Recipe datasets, we leverage GPT-4o to generate high-quality, context-specific questions tailored to the provided text and images. Moreover, the questions in MRAMG-Manual are generated manually by human annotators. As for MRAMG-Arxiv dataset, we engage expert annotators to meticulously formulate questions. Overall, We adhere to follow two criteria: (1) The questions are constructed based on the corresponding contexts. (2) The questions must be natural and practical and they present potential to be answered with texts integrated with images.

\subsubsection{Answer Generation} 
For Web Data, given the generated or original questions $Q$ and corresponding context $C$, we generate image-text interleaved answers $A$ using GPT-4o following CoT reasoning strategy \citep{COT}: (1) \textbf{Question Validity Assessment}, we first ensure valid questions $ValQ$ can be directly answered by contexts $C$ and remove invalid questions $InvalQ$. (2) \textbf{Evidence Extraction}, we then extract evidences $E$ from contexts $C$ to substantiate the answer $A$ for later answer construction. (3) \textbf{Answer Construction}, we construct highly reliable, accurate and coherent answer based on valid questions $ValQ$ and evidences $E$. (4) \textbf{Image Integration} we finally enrich the constructed answers $A$ with a series of images integrated in the optimal positions. 

For Academic Paper Data, the expert annotators are responsible for reviewing previous formulated questions, constructing answers where images or tables play an integral role, and seamlessly integrating visual elements into the answer, to provide a cohesive and multimodal understanding of the content. For Lifestyle Data, in MRAMG-Recipe dataset, there are two types of questions: questions related to the entire recipe (recipe-specific) and questions only relevant to a specific step (step-specific). We combine all the step texts as the answers of recipe-specific questions and use answers generated by GPT-4o as the answers related to step-specific questions. We manually insert images at the end of answer texts. In MRAMG-Manual dataset, answers are meticulously formulated by annotators, to ensure accurate selection and order of images in each manual.

\subsubsection{QA Pair Refine}
We further refine the question-answer pair formulated in the previous steps following the optimization approach outlined by \citet{zhu2024rageval}.
We leverage GPT-4o to identify and extract evidences from contexts that directly supports the answer, while simultaneously recognizing keywords in the answer and checking their alignment with the extracted evidences. We refine the contradictions in the answers by cross-referencing the original answer and the extracted evidence, and discard irrelevant information to ensure the answer is contextually grounded.

\subsection{Data Quality Check}

To ensure a high quality and reliability of MRAMG, we further check the consistency and correctness of QA pairs and their alignment with context-related images. We engage annotators in a structured data quality check process.
The process is carried out in three stages:
\begin{itemize}
    \item In the first stage, a group of annotators review all QA pairs, assessing their correctness and consistency with both the context and image descriptions. They identify any problematic entries for further revision. Moreover, we also integrate GPT-4o for QA Evaluation, to further enhance the quality assurance process.
    \item In the second stage, a separate group of annotators correct any identified issues, including those related to image insertions, question formulations, context alignment, image descriptions, and answer correctness. Specifically, expert annotators are further tasked with a meticulous review and correcting over the MRAMG-Arxiv dataset.
    \item In the third stage, a team of 
    %auditors
    reviewers conduct a comprehensive recheck to ensure the overall accuracy of the dataset. This step validates that all corrections have been properly implemented and that the datasets meet the required quality standards.
\end{itemize}

%Overall, from the above procedures, we have formulated a total of 4800 QA pairs in MRAMG-Bench.
\subsection{Data Statistics}
\label{sec:statistics}

The MRAMG-Bench, summarized in Table~\ref{tab:statistics}, comprises 4,800 QA pairs across three distinct domains: Web Data, Academic Paper Data, and Lifestyle Data. 
It integrates six datasets—\textbf{MRAMG-Wit}, \textbf{MRAMG-Wiki}, \textbf{MRAMG-Web}, \textbf{MRAMG-Arxiv}, \textbf{MRAMG-Recipe}, and \textbf{MRAMG-Manual}—containing 4,346 documents and 14,190 images, with tasks categorized into three difficulty levels. 
Datasets like MRAMG-Recipe and MRAMG-Manual feature longer texts and higher image density, posing greater challenges compared to others.
The benchmark's query distribution captures varying levels of complexity:
Single-image QA primarily involves direct image-text associations.
Two-image QA requires comparative analysis or step-wise reasoning.
Three+ image QA focuses on complex multi-step processes.
Text-only QA emphasizes pure textual reasoning and serves to evaluate the model's ability to exclude irrelevant images when they are not required.



\begin{table*}[htbp]
  \centering
  \caption{The statistics of MRAMG-Bench. Multimodal Element Density refers to the proportion of images to text within a document, indicating image density relative to document length.}
  \vspace{-1mm}
    \resizebox{0.9\textwidth}{!}{
    \begin{tabular}{lccccccc}
    \toprule
    \textbf{DATASETS} & \textbf{MRAMG-Wit} & \textbf{MRAMG-Wiki} & \textbf{MRAMG-Web} & \textbf{MRAMG-Arxiv} & \textbf{MRAMG-Recipe} & \textbf{MRAMG-Manual} & \textbf{Overall} \\
    \midrule
    \textbf{Difficulty Level} & 1     & 1     & 1     & 2     & 3     & 3     & 1, 2, 3 \\
    \textbf{Queries} & 600   & 500   & 750   & 200   & 2360  & 390   & 4800 \\
    \textbf{Documents} & 639   & 538   & 1500  & 101   & 1528  & 40    & 4346 \\
    \textbf{Images} & 639   & 538   & 1500  & 337   & 8569  & 2607  & 14190 \\
    \textbf{Avg Images Per Doc} & 1.00  & 1.00  & 1.00  & 3.34  & 5.61  & 65.18 & 3.27 \\
    \textbf{Avg Length Per Doc} & 532.31 & 865.28 & 98.42 & 853.5 & 485.22 & 6365.4 & 468.37 \\
    \textbf{Multimodal Element Density} & 1.88$\times 10^{-3}$ & 1.16$\times 10^{-3}$ & 1.02$\times 10^{-2}$ & 3.91$\times 10^{-3}$ & 1.16$\times 10^{-2}$ & 1.02$\times 10^{-2}$ & 6.98$\times 10^{-3}$ \\
    \textbf{Text-Only QA} & 0     & 0     & 0     & 0     & 92    & 40    & 132 \\
    \textbf{Single-Image QA} & 600   & 500   & 0     & 184   & 1481  & 92    & 2858 \\
    \textbf{Two-Image QA} & 0     & 0     & 750   & 12    & 60    & 126   & 948 \\
    \textbf{Three+ Images QA} & 0     & 0     & 0     & 3     & 727   & 132   & 862 \\
    \bottomrule
    \end{tabular}%
    }
  \label{tab:statistics}%
\end{table*}%

