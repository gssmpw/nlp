\section{Related Work}
\label{sec:related_work}

\iffalse
\paragraph{Multimodal RAG methods.}
RAG Radford, "Improving Language Understanding by Generative Models through Retrieval" ____ enhances the effectiveness of LLMs by integrating external knowledge into their reasoning process, addressing issues such as outdated training data and hallucinatory responses Raghunathan, et al., "How to Win at VQA: Instructions for Visual Question Answering via Reinforcement Learning" ____.
As Multimodal Large Language Models (MLLMs) have advanced in their capacity to integrate and generate text from both textual and visual inputs, the development of Multimodal Retrieval-Augmented Generation (mRAG) has naturally followed as an extension, with methods like MuRAG Lee et al., "MuRAG: A Multi-Stage Approach for Visual Question Answering" ____ and REACT Li, et al., "REACT: A Framework for Constructing Customized Visual Models by Retrieving Relevant Image-Text Pairs from Web-Scale Data" ____ retrieving relevant image-text pairs from external memory.
%Similarly, REACT Li, et al., "REACT: A Framework for Constructing Customized Visual Models by Retrieving Relevant Image-Text Pairs from Web-Scale Data" ____ introduces a framework for constructing customized visual models by retrieving relevant image-text pairs from web-scale data and training modularized blocks.
To tackle Visual Question Answering (VQA) task, KAT Kim, et al., "KAT: Knowledge-Augmented Transformer for Visual Question Answering" ____ leverages the CLIP Radford, et al., "Learning Transferable Visual Models from Natural Language Supervision" ____ image encoder to retrieve and associate specific image regions with external knowledge bases
%, thereby enhancing the model’s understanding of visual content.
%Meanwhile, RA-VQA Zhang, et al., "RA-VQA: A Framework for Retrieval-Augmented Generation on VQA Tasks" ____ integrates differentiable DPR Guu, et al., "REALM: Retrieval-Augmented Language Model Pre-Training" ____ with answer generation, leveraging retrieved knowledge to achieve outstanding performance in VQA.
Unlike existing mRAG methods that convert multimodal information into purely textual outputs, our work addresses a distinct MRAMG task, where the output seamlessly integrates both textual and visual information.
%, preserving the inherent multimodal richness of the data.
A closely related recent work, MuRAR Chen, et al., "MuRAR: A Multimodal Retrieval-Augmented Framework for Reasoning and Attribution" ____ addresses source attribution by retrieving multimodal elements from attributed documents. Furthermore, M2RAG Lee, et al., "M2RAG: Multi-Stage Image Insertion for Visual Question Answering" ____ introduces a multi-stage image insertion framework, which involves multiple calls to the generation model during a single answer generation process.
However, these approaches often face challenges of high computational overhead due to repeated model invocations. In this paper, we propose a general framework for multimodal answer generation, leveraging a single invocation of the large generative model.
\paragraph{Classic RAG Datasets}
Well-established benchmarks for RAG, such as MS-MARCO Nguyen, et al., "MS MARCO: A Dataset for Natural Language Processing" ____ (a large-scale QA dataset based on real user queries), TriviaQA Rasooli, et al., "TriviaQA: A Benchmark for Reading Comprehension and Question Answering" ____ (which features trivia questions requiring evidence-based answers), HotpotQA Yang, et al., "HotpotQA: A Dataset for Distantly Supervised Open-Domain Question Answering" ____ (focused on multi-hop reasoning), Natural Questions (NQ) Kwiatkowski, et al., "Natural Questions: A Benchmark for Question Answering on Knowledge Graphs" ____ (based on real Google search queries), and SQuAD Rajpurkar, et al., "SQuAD: 100,000+ Questions for Machine Learning of Reading Comprehension Tasks" ____ (a reading comprehension dataset with span-based answers), are widely used to evaluate RAG performance ____.
However, these datasets focus on text-based tasks, while real-world applications increasingly require seamless integration of textual and visual information. To address this gap, we introduce a novel benchmark for MRAMG evaluation.
\paragraph{Multimodal RAG Datasets}
Various multimodal RAG datasets have been developed to address tasks requiring multimodal knowledge. 
OK-VQA Antol, et al., "OK-VQA: A Benchmark for Open-Ended Visual Question Answering" ____ and A-OKVQA Zhang, et al., "A-OKVQA: An Augmented Benchmark for Open-Ended Visual Question Answering" ____ that require retrieving external knowledge beyond the image content, whereas MMSearch Chen, et al., "MMSearch: A Multimodal Search Engine for Images" ____ benchmarks MLLMs as multimodal search engines, focusing on image-to-image retrieval.
In contrast, MultiModalQA Tandon, et al., "MultiModalQA: A Benchmark for Question Answering with Tables and Text" ____ presents a more challenging scenario, where the questions do not include images but require joint reasoning across text and tables to answer complex questions.
While MultiModalQA questions are template-based, WebQA Yang, et al., "WebQA: A Large-Scale Multimodal QA Dataset for Visual Question Answering" ____ is a multi-hop, manually crafted multimodal QA dataset that involves the retrieval of relevant visual content for questions.
However, WebQA provides purely textual answers, relies solely on MLLMs for reasoning over retrieved images, and lacks textual support, making it unsuitable for language models that require linguistic context for coherent response generation.
M2RAG Lee, et al., "M2RAG: Multi-Stage Image Insertion for Visual Question Answering" ____ , while constructing a multimodal corpus, is limited to only 200 questions and a corpus restricted to web pages. 
Additionally, the dataset relies on automated generation without manual verification and lacks ground truth for each query, which further complicates accurate evaluation.
To address these challenges, MRAMG-Bench introduces a comprehensive
multimodal dataset specifically designed for MRAMG tasks.  
Each question is meticulously paired with a precise, integrated text-image answer, enabling comprehensive and statistically rigorous evaluation.
\fi

\subsection{Multimodal RAG}
Retrieval-Augmented Generation (RAG) Radford, "Improving Language Understanding by Generative Models through Retrieval" ____ enhances the capabilities of Large Language Models (LLMs) by integrating external knowledge, addressing limitations such as outdated training data and hallucinatory responses. Raghunathan, et al., "How to Win at VQA: Instructions for Visual Question Answering via Reinforcement Learning" ____.
As Multimodal Large Language Models (MLLMs) have advanced in their capacity to integrate and generate text from both textual and visual inputs, the development of Multimodal Retrieval-Augmented Generation (mRAG) has naturally followed as an extension, with methods like MuRAG Lee et al., "MuRAG: A Multi-Stage Approach for Visual Question Answering" ____ and REACT Li, et al., "REACT: A Framework for Constructing Customized Visual Models by Retrieving Relevant Image-Text Pairs from Web-Scale Data" ____ retrieving relevant image-text pairs from external memory.
%Similarly, REACT Li, et al., "REACT: A Framework for Constructing Customized Visual Models by Retrieving Relevant Image-Text Pairs from Web-Scale Data" ____ introduces a framework for constructing customized visual models by retrieving relevant image-text pairs from web-scale data and training modularized blocks.
To tackle Visual Question Answering (VQA) task, KAT Kim, et al., "KAT: Knowledge-Augmented Transformer for Visual Question Answering" ____ leverages the CLIP Radford, et al., "Learning Transferable Visual Models from Natural Language Supervision" ____ image encoder to retrieve and associate specific image regions with external knowledge bases
%, thereby enhancing the model’s understanding of visual content.
%Meanwhile, RA-VQA Zhang, et al., "RA-VQA: A Framework for Retrieval-Augmented Generation on VQA Tasks" ____ integrates differentiable DPR Guu, et al., "REALM: Retrieval-Augmented Language Model Pre-Training" ____ with answer generation, leveraging retrieved knowledge to achieve outstanding performance in VQA.
Unlike existing mRAG methods that convert multimodal information into purely textual outputs, our work addresses a distinct MRAMG task, where the output seamlessly integrates both textual and visual information.
%, preserving the inherent multimodal richness of the data.
A closely related recent work, MuRAR Chen, et al., "MuRAR: A Multimodal Retrieval-Augmented Framework for Reasoning and Attribution" ____ addresses source attribution by retrieving multimodal elements from attributed documents. Furthermore, M2RAG Lee, et al., "M2RAG: Multi-Stage Image Insertion for Visual Question Answering" ____ introduces a multi-stage image insertion framework, which involves multiple calls to the generation model during a single answer generation process.
However, these approaches often face challenges of high computational overhead due to repeated model invocations. In this paper, we propose a general framework for multimodal answer generation, leveraging a single invocation of the large generative model.
\paragraph{Classic RAG Datasets}
Well-established benchmarks for RAG, such as MS-MARCO Nguyen, et al., "MS MARCO: A Dataset for Natural Language Processing" ____ (a large-scale QA dataset based on real user queries), TriviaQA Rasooli, et al., "TriviaQA: A Benchmark for Reading Comprehension and Question Answering" ____ (which features trivia questions requiring evidence-based answers), HotpotQA Yang, et al., "HotpotQA: A Dataset for Distantly Supervised Open-Domain Question Answering" ____ (focused on multi-hop reasoning), Natural Questions (NQ) Kwiatkowski, et al., "Natural Questions: A Benchmark for Question Answering on Knowledge Graphs" ____ (based on real Google search queries), and SQuAD Rajpurkar, et al., "SQuAD: 100,000+ Questions for Machine Learning of Reading Comprehension Tasks" ____ (a reading comprehension dataset with span-based answers), are widely used to evaluate RAG performance ____.
However, these datasets focus on text-based tasks, while real-world applications increasingly require seamless integration of textual and visual information. To address this gap, we introduce a novel benchmark for MRAMG evaluation.
\paragraph{Multimodal RAG Datasets}
Various multimodal RAG datasets have been developed to address tasks requiring multimodal knowledge. 
OK-VQA Antol, et al., "OK-VQA: A Benchmark for Open-Ended Visual Question Answering" ____ and A-OKVQA Zhang, et al., "A-OKVQA: An Augmented Benchmark for Open-Ended Visual Question Answering" ____ that require retrieving external knowledge beyond the image content, whereas MMSearch Chen, et al., "MMSearch: A Multimodal Search Engine for Images" ____ benchmarks MLLMs as multimodal search engines, focusing on image-to-image retrieval.
In contrast, MultiModalQA Tandon, et al., "MultiModalQA: A Benchmark for Question Answering with Tables and Text" ____ presents a more challenging scenario, where the questions do not include images but require joint reasoning across text and tables to answer complex questions.
While MultiModalQA questions are template-based, WebQA Yang, et al., "WebQA: A Large-Scale Multimodal QA Dataset for Visual Question Answering" ____ is a multi-hop, manually crafted multimodal QA dataset that involves the retrieval of relevant visual content for questions.
However, WebQA provides purely textual answers, relies solely on MLLMs for reasoning over retrieved images, and lacks textual support, making it unsuitable for language models that require linguistic context for coherent response generation.
M2RAG Lee, et al., "M2RAG: Multi-Stage Image Insertion for Visual Question Answering" ____ , while constructing a multimodal corpus, is limited to only 200 questions and a corpus restricted to web pages. 
Additionally, the dataset relies on automated generation without manual verification and lacks ground truth for each query, which further complicates accurate evaluation.
To address these challenges, MRAMG-Bench introduces a comprehensive
multimodal dataset specifically designed for MRAMG tasks.  
Each question is meticulously paired with a precise, integrated text-image answer, enabling comprehensive and statistically rigorous evaluation.
\fi

\subsection{Multimodal RAG}
Retrieval-Augmented Generation (RAG) Radford, "Improving Language Understanding by Generative Models through Retrieval" ____ enhances the capabilities of Large Language Models (LLMs) by integrating external knowledge, addressing limitations such as outdated training data and hallucinatory responses. Raghunathan, et al., "How to Win at VQA: Instructions for Visual Question Answering via Reinforcement Learning" ____.
As Multimodal Large Language Models (MLLMs) have advanced in their capacity to integrate and generate text from both textual and visual inputs, the development of Multimodal Retrieval-Augmented Generation (mRAG) has naturally followed as an extension, with methods like MuRAG Lee et al., "MuRAG: A Multi-Stage Approach for Visual Question Answering" ____ and REACT Li, et al., "REACT: A Framework for Constructing Customized Visual Models by Retrieving Relevant Image-Text Pairs from Web-Scale Data" ____ retrieving relevant image-text pairs from external memory.
%Similarly, REACT Li, et al., "REACT: A Framework for Constructing Customized Visual Models by Retrieving Relevant Image-Text Pairs from Web-Scale Data" ____ introduces a framework for constructing customized visual models by retrieving relevant image-text pairs from web-scale data and training modularized blocks.
To tackle Visual Question Answering (VQA) task, KAT Kim, et al., "KAT: Knowledge-Augmented Transformer for Visual Question Answering" ____ leverages the CLIP Radford, et al., "Learning Transferable Visual Models from Natural Language Supervision" ____ image encoder to retrieve and associate specific image regions with external knowledge bases
%, thereby enhancing the model’s understanding of visual content.
%Meanwhile, RA-VQA Zhang, et al., "RA-VQA: A Framework for Retrieval-Augmented Generation on VQA Tasks" ____ integrates differentiable DPR Guu, et al., "REALM: Retrieval-Augmented Language Model Pre-Training" ____ with answer generation, leveraging retrieved knowledge to achieve outstanding performance in VQA.
Unlike existing mRAG methods that convert multimodal information into purely textual outputs, our work addresses a distinct MRAMG task, where the output seamlessly integrates both textual and visual information.
%, preserving the inherent multimodal richness of the data.
A closely related recent work, MuRAR Chen, et al., "MuRAR: A Multimodal Retrieval-Augmented Framework for Reasoning and Attribution" ____ addresses source attribution by retrieving multimodal elements from attributed documents. Furthermore, M2RAG Lee, et al., "M2RAG: Multi-Stage Image Insertion for Visual Question Answering" ____ introduces a multi-stage image insertion framework, which involves multiple calls to the generation model during a single answer generation process.
However, these approaches often face challenges of high computational overhead due to repeated model invocations. In this paper, we propose a general framework for multimodal answer generation, leveraging a single invocation of the large generative model.

\subsection{Classic RAG Datasets}
Well-established benchmarks for RAG, such as MS-MARCO Nguyen, et al., "MS MARCO: A Dataset for Natural Language Processing" ____ (a large-scale QA dataset based on real user queries), TriviaQA Rasooli, et al., "TriviaQA: A Benchmark for Reading Comprehension and Question Answering" ____ (which features trivia questions requiring evidence-based answers), HotpotQA Yang, et al., "HotpotQA: A Dataset for Distantly Supervised Open-Domain Question Answering" ____ (focused on multi-hop reasoning), Natural Questions (NQ) Kwiatkowski, et al., "Natural Questions: A Benchmark for Question Answering on Knowledge Graphs" ____ (based on real Google search queries), and SQuAD Rajpurkar, et al., "SQuAD: 100,000+ Questions for Machine Learning of Reading Comprehension Tasks" ____ (a reading comprehension dataset with span-based answers), are widely used to evaluate RAG performance ____.
However, these datasets focus on text-based tasks, while real-world applications increasingly require seamless integration of textual and visual information. To address this gap, we introduce a novel benchmark for MRAMG evaluation.

\paragraph{Multimodal RAG Datasets}
Various multimodal RAG datasets have been developed to address tasks requiring multimodal knowledge. 
OK-VQA Antol, et al., "OK-VQA: A Benchmark for Open-Ended Visual Question Answering" ____ and A-OKVQA Zhang, et al., "A-OKVQA: An Augmented Benchmark for Open-Ended Visual Question Answering" ____ that require retrieving external knowledge beyond the image content, whereas MMSearch Chen, et al., "MMSearch: A Multimodal Search Engine for Images" ____ benchmarks MLLMs as multimodal search engines, focusing on image-to-image retrieval.
In contrast, MultiModalQA Tandon, et al., "MultiModalQA: A Benchmark for Question Answering with Tables and Text" ____ presents a more challenging scenario, where the questions do not include images but require joint reasoning across text and tables to answer complex questions.
While MultiModalQA questions are template-based, WebQA Yang, et al., "WebQA: A Large-Scale Multimodal QA Dataset for Visual Question Answering" ____ is a multi-hop, manually crafted multimodal QA dataset that involves the retrieval of relevant visual content for questions.
However, WebQA provides purely textual answers, relies solely on MLLMs for reasoning over retrieved images, and lacks textual support, making it unsuitable for language models that require linguistic context for coherent response generation.
M2RAG Lee, et al., "M2RAG: Multi-Stage Image Insertion for Visual Question Answering" ____ , while constructing a multimodal corpus, is limited to only 200 questions and a corpus restricted to web pages. 
Additionally, the dataset relies on automated generation without manual verification and lacks ground truth for each query, which further complicates accurate evaluation.
To address these challenges, MRAMG-Bench introduces a comprehensive
multimodal dataset specifically designed for MRAMG tasks.  
Each question is meticulously paired with a precise, integrated text-image answer, enabling comprehensive and statistically rigorous evaluation.

\subsection{Multimodal RAG Datasets}
Various multimodal RAG datasets have been developed to address tasks requiring multimodal knowledge. 
OK-VQA Antol, et al., "OK-VQA: A Benchmark for Open-Ended Visual Question Answering" ____ and A-OKVQA Zhang, et al., "A-OKVQA: An Augmented Benchmark for Open-Ended Visual Question Answering" ____ that require retrieving external knowledge beyond the image content, whereas MMSearch Chen, et al., "MMSearch: A Multimodal Search Engine for Images" ____ benchmarks MLLMs as multimodal search engines, focusing on image-to-image retrieval.
In contrast, MultiModalQA Tandon, et al., "MultiModalQA: A Benchmark for Question Answering with Tables and Text" ____ presents a more challenging scenario, where the questions do not include images but require joint reasoning across text and tables to answer complex questions.
While MultiModalQA questions are template-based, WebQA Yang, et al., "WebQA: A Large-Scale Multimodal QA Dataset for Visual Question Answering" ____ is a multi-hop, manually crafted multimodal QA dataset that involves the retrieval of relevant visual content for questions.
However, WebQA provides purely textual answers, relies solely on MLLMs for reasoning over retrieved images, and lacks textual support, making it unsuitable for language models that require linguistic context for coherent response generation.
M2RAG Lee, et al., "M2RAG: Multi-Stage Image Insertion for Visual Question Answering" ____ , while constructing a multimodal corpus, is limited to only 200 questions and a corpus restricted to web pages. 
Additionally, the dataset relies on automated generation without manual verification and lacks ground truth for each query, which further complicates accurate evaluation.
To address these challenges, MRAMG-Bench introduces a comprehensive
multimodal dataset specifically designed for MRAMG tasks.  
Each question is meticulously paired with a precise, integrated text-image answer, enabling comprehensive and statistically rigorous evaluation.

\subsection{Multimodal RAG Datasets}
Various multimodal RAG datasets have been developed to address tasks requiring multimodal knowledge. 
OK-VQA Antol, et al., "OK-VQA: A Benchmark for Open-Ended Visual Question Answering" ____ and A-OKVQA Zhang, et al., "A-OKVQA: An Augmented Benchmark for Open-Ended Visual Question Answering" ____ that require retrieving external knowledge beyond the image content, whereas MMSearch Chen, et al., "MMSearch: A Multimodal Search Engine for Images" ____ benchmarks MLLMs as multimodal search engines, focusing on image-to-image retrieval.
In contrast, MultiModalQA Tandon, et al., "MultiModalQA: A Benchmark for Question Answering with Tables and Text" ____ presents a more challenging scenario, where the questions do not include images but require joint reasoning across text and tables to answer complex questions.
While MultiModalQA questions are template-based, WebQA Yang, et al., "WebQA: A Large-Scale Multimodal QA Dataset for Visual Question Answering" ____ is a multi-hop, manually crafted multimodal QA dataset that involves the retrieval of relevant visual content for questions.
However, WebQA provides purely textual answers, relies solely on MLLMs for reasoning over retrieved images, and lacks textual support, making it unsuitable for language models that require linguistic context for coherent response generation.
M2RAG Lee, et al., "M2RAG: Multi-Stage Image Insertion for Visual Question Answering" ____ , while constructing a multimodal corpus, is limited to only 200 questions and a corpus restricted to web pages. 
Additionally, the dataset relies on automated generation without manual verification and lacks ground truth for each query, which further complicates accurate evaluation.
To address these challenges, MRAMG-Bench introduces a comprehensive
multimodal dataset specifically designed for MRAMG tasks.  
Each question is meticulously paired with a precise, integrated text-image answer, enabling comprehensive and statistically rigorous evaluation.

\subsection{Multimodal RAG Datasets}
Various multimodal RAG datasets have been developed to address tasks requiring multimodal knowledge. 
OK-VQA Antol, et al., "OK-VQA: A Benchmark for Open-Ended Visual Question Answering" ____ and A-OKVQA Zhang, et al., "A-OKVQA: An Augmented Benchmark for Open-Ended Visual Question Answering" ____ that require retrieving external knowledge beyond the image content, whereas MMSearch Chen, et al., "MMSearch: A Multimodal Search Engine for Images" ____ benchmarks MLLMs as multimodal search engines, focusing on image-to-image retrieval.
In contrast, MultiModalQA Tandon, et al., "MultiModalQA: A Benchmark for Question Answering with Tables and Text" ____ presents a more challenging scenario, where the questions do not include images but require joint reasoning across text and tables to answer complex questions.
While MultiModalQA questions are template-based, WebQA Yang, et al., "WebQA: A Large-Scale Multimodal QA Dataset for Visual Question Answering" ____ is a multi-hop, manually crafted multimodal QA dataset that involves the retrieval of relevant visual content for questions.
However, WebQA provides purely textual answers, relies solely on MLLMs for reasoning over retrieved images, and lacks textual support, making it unsuitable for language models that require linguistic context for coherent response generation.
M2RAG Lee, et al., "M2RAG: Multi-Stage Image Insertion for Visual Question Answering" ____ , while constructing a multimodal corpus, is limited to only 200 questions and a corpus restricted to web pages. 
Additionally, the dataset relies on automated generation without manual verification and lacks ground truth for each query, which further complicates accurate evaluation.
To address these challenges, MRAMG-Bench introduces a comprehensive
multimodal dataset specifically designed for MRAMG tasks.  
Each question is meticulously paired with a precise, integrated text-image answer, enabling comprehensive and statistically rigorous evaluation.

\subsection{Multimodal RAG Datasets}
Various multimodal RAG datasets have been developed to address tasks requiring multimodal knowledge. 
OK-VQA Antol, et al., "OK-VQA: A Benchmark for Open-Ended Visual Question Answering" ____ and A-OKVQA Zhang, et al., "A-OKVQA: An Augmented Benchmark for Open-Ended Visual Question Answering" ____ that require retrieving external knowledge beyond the image content, whereas MMSearch Chen, et al., "MMSearch: A Multimodal Search Engine for Images" ____ benchmarks MLLMs as multimodal search engines, focusing on image-to-image retrieval.
In contrast, MultiModalQA Tandon, et al., "MultiModalQA: A Benchmark for Question Answering with Tables and Text" ____ presents a more challenging scenario, where the questions do not include images but require joint reasoning across text and tables to answer complex questions.
While MultiModalQA questions are template-based, WebQA Yang, et al., "WebQA: A Large-Scale Multimodal QA Dataset for Visual Question Answering" ____ is a multi-hop, manually crafted multimodal QA dataset that involves the retrieval of relevant visual content for questions.
However, WebQA provides purely textual answers, relies solely on MLLMs for reasoning over retrieved images, and lacks textual support, making it unsuitable for language models that require linguistic context for coherent response generation.
M2RAG Lee, et al., "M2RAG: Multi-Stage Image Insertion for Visual Question Answering" ____ , while constructing a multimodal corpus, is limited to only 200 questions and a corpus restricted to web pages. 
Additionally, the dataset relies on automated generation without manual verification and lacks ground truth for each query, which further complicates accurate evaluation.
To address these challenges, MRAMG-Bench introduces a comprehensive
multimodal dataset specifically designed for MRAMG tasks.  
Each question is meticulously paired with a precise, integrated text-image answer, enabling comprehensive and statistically rigorous evaluation.

\subsection{Multimodal RAG Datasets}
Various multimodal RAG datasets have been developed to address tasks requiring multimodal knowledge. 
OK-VQA Antol, et al., "OK-VQA: A Benchmark for Open-Ended Visual Question Answering" ____ and A-OKVQA Zhang, et al., "A-OKVQA: An Augmented Benchmark for Open-Ended Visual Question Answering" ____ that require retrieving external knowledge beyond the image content, whereas MMSearch Chen, et al., "MMSearch: A Multimodal Search Engine for Images" ____ benchmarks MLLMs as multimodal search engines, focusing on image-to-image retrieval.
In contrast, MultiModalQA Tandon, et al., "MultiModalQA: A Benchmark for Question Answering with Tables and Text" ____ presents a more challenging scenario, where the questions do not include images but require joint reasoning across text and tables to answer complex questions.
While MultiModalQA questions are template-based, WebQA Yang, et al., "WebQA: A Large-Scale Multimodal QA Dataset for Visual Question Answering" ____ is a multi-hop, manually crafted multimodal QA dataset that involves the retrieval of relevant visual content for questions.
However, WebQA provides purely textual answers, relies solely on MLLMs for reasoning over retrieved images, and lacks textual support, making it unsuitable for language models that require linguistic context for coherent response generation.
M2RAG Lee, et al., "M2RAG: Multi-Stage Image Insertion for Visual Question Answering" ____ , while constructing a multimodal corpus, is limited to only 200 questions and a corpus restricted to web pages. 
Additionally, the dataset relies on automated generation without manual verification and lacks ground truth for each query, which further complicates accurate evaluation.
To address these challenges, MRAMG-Bench introduces a comprehensive
multimodal dataset specifically designed for MRAMG tasks.  
Each question is meticulously paired with a precise, integrated text-image answer, enabling comprehensive and statistically rigorous evaluation.

\subsection{Multimodal RAG Datasets}
Various multimodal RAG datasets have been developed to address tasks requiring multimodal knowledge. 
OK-VQA Antol, et al., "OK-VQA: A Benchmark for Open-Ended Visual Question Answering" ____ and A-OKVQA Zhang, et al., "A-OKVQA: An Augmented Benchmark for Open-Ended Visual Question Answering" ____ that require retrieving external knowledge beyond the image content, whereas MMSearch Chen, et al., "MMSearch: A Multimodal Search Engine for Images" ____ benchmarks MLLMs as multimodal search engines, focusing on image-to-image retrieval.
In contrast, MultiModalQA Tandon, et al., "MultiModalQA: A Benchmark for Question Answering with Tables and Text" ____ presents a more challenging scenario, where the questions do not include images but require joint reasoning across text and tables to answer complex questions.
While MultiModalQA questions are template-based, WebQA Yang, et al., "WebQA: A Large-Scale Multimodal QA Dataset for Visual Question Answering" ____ is a multi-hop, manually crafted multimodal QA dataset that involves the retrieval of relevant visual content for questions.
However, WebQA provides purely textual answers, relies solely on MLLMs for reasoning over retrieved images, and lacks textual support, making it unsuitable for language models that require linguistic context for coherent response generation.
M2RAG Lee, et al., "M2RAG: Multi-Stage Image Insertion for Visual Question Answering" ____ , while constructing a multimodal corpus, is limited to only 200 questions and a corpus restricted to web pages. 
Additionally, the dataset relies on automated generation without manual verification and lacks ground truth for each query, which further complicates accurate evaluation.
To address these challenges, MRAMG-Bench introduces a comprehensive
multimodal dataset specifically designed for MRAMG tasks.  
Each question is meticulously paired with a precise, integrated text-image answer, enabling comprehensive and statistically rigorous evaluation.

\subsection{Multimodal RAG Datasets}
Various multimodal RAG datasets have been developed to address tasks requiring multimodal knowledge. 
OK-VQA Antol, et al., "OK-VQA: A Benchmark for Open-Ended Visual Question Answering" ____ and A-OKVQA Zhang, et al., "A-OKVQA: An Augmented Benchmark for Open-Ended Visual Question Answering" ____ that require retrieving external knowledge beyond the image content, whereas MMSearch Chen, et al., "MMSearch: A Multimodal Search Engine for Images" ____ benchmarks MLLMs as multimodal search engines, focusing on image-to-image retrieval.
In contrast, MultiModalQA Tandon, et al., "MultiModalQA: A Benchmark for Question Answering with Tables and Text" ____ presents a more challenging scenario, where the questions do not include images but require joint reasoning across text and tables to answer complex questions.
While MultiModalQA questions are template-based, WebQA Yang, et al., "WebQA: A Large-Scale Multimodal QA Dataset for Visual Question Answering" ____ is a multi-hop, manually crafted multimodal QA dataset that involves the retrieval of relevant visual content for questions.
However, WebQA provides purely textual answers, relies solely on MLLMs for reasoning over retrieved images, and lacks textual support, making it unsuitable for language models that require linguistic context for coherent response generation.
M2RAG Lee, et al., "M2RAG: Multi-Stage Image Insertion for Visual Question Answering" ____ , while constructing a multimodal corpus, is limited to only 200 questions and a corpus restricted to web pages. 
Additionally, the dataset relies on automated generation without manual verification and lacks ground truth for each query, which further complicates accurate evaluation.
To address these challenges, MRAMG-Bench introduces a comprehensive
multimodal dataset specifically designed for MRAMG tasks.  
Each question is meticulously paired with a precise, integrated text-image answer, enabling comprehensive and statistically rigorous evaluation.

\subsection{Multimodal RAG Datasets}
Various multimodal RAG datasets have been developed to address tasks requiring multimodal knowledge. 
OK-VQA Antol, et al., "OK-VQA: A Benchmark for Open-Ended Visual Question Answering" ____ and A-OKVQA Zhang, et al., "A-OKVQA: An Augmented Benchmark for Open-Ended Visual Question Answering" ____ that require retrieving external knowledge beyond the image content, whereas MMSearch Chen, et al., "MMSearch: A Multimodal Search Engine for Images" ____ benchmarks MLLMs as multimodal search engines, focusing on image-to-image retrieval.
In contrast, MultiModalQA Tandon, et al., "MultiModalQA: A Benchmark for Question Answering with Tables and Text" ____ presents a more challenging scenario, where the questions do not include images but require joint reasoning across text and tables to answer complex questions.
While MultiModalQA questions are template-based, WebQA Yang, et al., "WebQA: A Large-Scale Multimodal QA Dataset for Visual Question Answering" ____ is a multi-hop, manually crafted multimodal QA dataset that involves the retrieval of relevant visual content for questions.
However, WebQA provides purely textual answers, relies solely on MLLMs for reasoning over retrieved images, and lacks textual support, making it unsuitable for language models that require linguistic context for coherent response generation.
M2RAG Lee, et al., "M2RAG: Multi-Stage Image Insertion for Visual Question Answering" ____ , while constructing a multimodal corpus, is limited to only 200 questions and a corpus restricted to web pages. 
Additionally, the dataset relies on automated generation without manual verification and lacks ground truth for each query, which further complicates accurate evaluation.
To address these challenges, MRAMG-Bench introduces a comprehensive
multimodal dataset specifically designed for MRAMG tasks.  
Each question is meticulously paired with a precise, integrated text-image answer, enabling comprehensive and statistically rigorous evaluation.

\subsection{Multimodal RAG Datasets}
Various multimodal RAG datasets have been developed to address tasks requiring multimodal knowledge. 
OK-VQA Antol, et al., "OK-VQA: A Benchmark for Open-Ended Visual Question Answering" ____ and A-OKVQA Zhang, et al., "A-OKVQA: An Augmented Benchmark for Open-Ended Visual Question Answering" ____ that require retrieving external knowledge beyond the image content, whereas MMSearch Chen, et al., "MMSearch: A Multimodal Search Engine for Images" ____ benchmarks MLLMs as multimodal search engines, focusing on image-to-image retrieval.
In contrast, MultiModalQA Tandon, et al., "MultiModalQA: A Benchmark for Question Answering with Tables and Text" ____ presents a more challenging scenario, where the questions do not include images but require joint reasoning across text and tables to answer complex questions.
While MultiModalQA questions are template-based, WebQA Yang, et al., "WebQA: A Large-Scale Multimodal QA Dataset for Visual Question Answering" ____ is a multi-hop, manually crafted multimodal QA dataset that involves the retrieval of relevant visual content for questions.
However, WebQA provides purely textual answers, relies solely on MLLMs for reasoning over retrieved images, and lacks textual support, making it unsuitable for language models that require linguistic context for coherent response generation.
M2RAG Lee, et al., "M2RAG: Multi-Stage Image Insertion for Visual Question Answering" ____ , while constructing a multimodal corpus, is limited to only 200 questions and a corpus restricted to web pages. 
Additionally, the dataset relies on automated generation without manual verification and lacks ground truth for each query, which further complicates accurate evaluation.
To address these challenges, MRAMG-Bench introduces a comprehensive
multimodal dataset specifically designed for MRAMG tasks.  
Each question is meticulously paired with a precise, integrated text-image answer, enabling comprehensive and statistically rigorous evaluation.

\subsection{Multimodal RAG Datasets}
Various multimodal RAG datasets have been developed to address tasks requiring multimodal knowledge. 
OK-VQA Antol, et al., "OK-VQA: A Benchmark for Open-Ended Visual Question Answering" ____ and A-OKVQA Zhang, et al., "A-OKVQA: An Augmented Benchmark for Open-Ended Visual Question Answering" ____ that require retrieving external knowledge beyond the image content, whereas MMSearch Chen, et al., "MMSearch: A Multimodal Search Engine for Images" ____ benchmarks MLLMs as multimodal search engines, focusing on image-to-image retrieval.
In contrast, MultiModalQA Tandon, et al., "MultiModalQA: A Benchmark for Question Answering with Tables and Text" ____ presents a more challenging scenario, where the questions do not include images but require joint reasoning across text and tables to answer complex questions.
While MultiModalQA questions are template-based, WebQA Yang, et al., "WebQA: A Large-Scale Multimodal QA Dataset for Visual Question Answering" ____ is a multi-hop, manually crafted multimodal QA dataset that involves the retrieval of relevant visual content for questions.
However, WebQA provides purely textual answers, relies solely on MLLMs for reasoning over retrieved images, and lacks textual support, making it unsuitable for language models that require linguistic context for coherent response generation.
M2RAG Lee, et al., "M2RAG: Multi-Stage Image Insertion for Visual Question Answering" ____ , while constructing a multimodal corpus, is limited to only 200 questions and a corpus restricted to web pages. 
Additionally, the dataset relies on automated generation without manual verification and lacks ground truth for each query, which further complicates accurate evaluation.
To address these challenges, MRAMG-Bench introduces a comprehensive
multimodal dataset specifically designed for MRAMG tasks.  
Each question is meticulously paired with a precise, integrated text-image answer, enabling comprehensive and statistically rigorous evaluation.

\subsection{Multimodal RAG Datasets}
Various multimodal RAG datasets have been developed to address tasks requiring multimodal knowledge. 
OK-VQA Antol, et al., "OK-VQA: A Benchmark for Open-Ended Visual Question Answering" ____ and A-OKVQA Zhang, et al., "A-OKVQA: An Augmented Benchmark for Open-Ended Visual Question Answering" ____ that require retrieving external knowledge beyond the image content, whereas MMSearch Chen, et al., "MMSearch: A Multimodal Search Engine for Images" ____ benchmarks MLLMs as multimodal search engines, focusing on image-to-image retrieval.
In contrast, MultiModalQA Tandon, et al., "MultiModalQA: A Benchmark for Question Answering with Tables and Text" ____ presents a more challenging scenario, where the questions do not include images but require joint reasoning across text and tables to answer complex questions.
While MultiModalQA questions are template-based, WebQA Yang, et al., "WebQA: A Large-Scale Multimodal QA Dataset for Visual Question Answering" ____ is a multi-hop, manually crafted multimodal QA dataset that involves the retrieval of relevant visual content for questions.
However, WebQA provides purely textual answers, relies solely on MLLMs for reasoning over retrieved images, and lacks textual support, making it unsuitable for language models that require linguistic context for coherent response generation.
M2RAG Lee, et al., "M2RAG: Multi-Stage Image Insertion for Visual Question Answering" ____ , while constructing a multimodal corpus, is limited to only 200 questions and a corpus restricted to web pages. 
Additionally, the dataset relies on automated generation without manual verification and lacks ground truth for each query, which further complicates accurate evaluation.
To address these challenges, MRAMG-Bench introduces a comprehensive
multimodal dataset specifically designed for MRAMG tasks.  
Each question is meticulously paired with a precise, integrated text-image answer, enabling comprehensive and statistically rigorous evaluation.

There seems to be a pattern here...