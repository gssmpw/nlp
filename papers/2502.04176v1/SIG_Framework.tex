\section{A General Framework for MRAMG}
\label{sec:framework}

In this section, we present our generation framework, which consists of two stages: (1) the retrieval of relevant documents, and (2) the generation of multimodal answers based on the retrieved information, utilizing a foundational generative model. In the second stage, we propose three distinct strategies for answer generation: (a) LLM-based method: directly generating multimodal answers using LLMs, (b) MLLM-based method: directly generating multimodal answers using MLLMs, and (c) rule-based method: first obtaining a pure text answer from generative large models and then applying our matching-based algorithm to insert appropriate images into the text answer, resulting in multimodal answers. All the prompts we used can be found at: https://huggingface.co/MRAMG.

\subsection{Retrieving Relevant Documents}
%In accordance with the standard retrieval-augmented generation (RAG) framework \citep{lewis2020retrieval}, 
We employ the BGE-M3 embedding model \citep{chen2024bge} to select the top-$k$ most relevant documents by calculating the cosine similarity between the user query $q$ and each document $d_i$ in the embedding space.
Then, we sequentially concatenate the text blocks and image blocks from each selected document, resulting in the multimodal information to be used: the text information $\mathcal{T}_q^* = \{t_1, t_2, \dots, t_l\}$ and the image infomation $\mathcal{I}_q^* = \{i_1, i_2, \dots, i_n\}$.

\subsection{Generating Multimodal Answer}

% To generate multimodal answers, we consider three different strategies. 
%For LLMs, we use the first and third strategies, while for MLLMs, we use the second and third strategies. Below, we provide a detailed explanation of the three strategies. The first two methods are similar to concurrent works \citet{zhu2024murar} and \citet{ma2024multi}.


\subsubsection{LLM-Based Method}

We first consider how to generate multimodal answers using LLMs. Since LLMs cannot directly process image formats such as images for understanding, we choose an alternative method. 
In fact, we notice that the captions and surrounding context of images often offer detailed descriptions and supplementary information, which enhance the understanding of the images in the context of the query. Thus, we leverage the contextual information of images within the text context, as well as the captions of the images themselves, as textual substitute for the images when inputting into the language model. Specifically, we insert image placeholders $p_1, p_2, \dots, p_n$ at the corresponding positions of the retrieved text information $\mathcal{T}_q^*$, resulting in a new context $\mathcal{C}_q^* = \{t_1, t_2, \dots, t_{l_1}, p_1, t_{l_1 + 1}, \dots, t_{l_j}, p_j, t_{l_j +1}, \dots\}$. We consider that the context $t_{l_j}$ and $t_{l_j + 1}$ around the placeholder $p_j$ for the $j$-th image $i_j$ provide a suitable textual description for that image. Then, we generate the multimodal answer as $\mathcal{A} = \mathcal{M}(\mathcal{P}_{llm}, q, \mathcal{C}_q^{*})$, where $\mathcal{P}_{llm}$ is the prompt, and the images in the output are represented as placeholders.

\subsubsection{MLLM-Based Method}

For multimodal large language models (MLLMs), we directly input the textual information $\mathcal{T}_q^{*}$ and the image information $\mathcal{I}_q^{*}$. However, current open-source or closed-source MLLMs typically impose limits on the number of input images. Therefore, we are unable to use all the images in $\mathcal{I}_q^{*}$ as input. To address this, we propose leveraging the CLIP-based multimodal embedding model \citep{koukounas2024jina} to compute the embedding similarity between each image and the query, and then selecting the top-N images based on the similarity for input, denoted as $\operatorname{TopN}(q, \mathcal{I}_q^*)$. Formally, for a given MLLM $\mathcal{M}$, we generate the multimodal answer as $\mathcal{A} = \mathcal{M}(\mathcal{P}_{mllm}, \mathcal{T}_q^{*}, \operatorname{TopN}(q, \mathcal{I}_q^*))$, where $\mathcal{P}_{mllm}$ is the prompt, and the images in the output are also represented as placeholders.

\subsubsection{Rule-Based Method}

Here, we consider an alternative strategy for generating multimodal answers. Specifically, we first allow the generative model to produce a pure textual answer based on the provided context. Then, we employ a matching-based insertion algorithm to select appropriate images from the retrieved set and insert them at suitable positions within the textual answer, thereby forming the multimodal answer.

\textbf{Step 1: Generate Text Answer.} We first use large generative model $\mathcal{M}$ to generate a pure text answer $\mathcal{A}_{txt}=\mathcal{M}(\mathcal{P}_{txt},\mathcal{T}_q^*)$, where $\mathcal{P}_{txt}$ denotes the text-answer-generating prompt. Subsequently, we segment the pure textual answer into sentences, i.e., $\mathcal{A}_{txt} = \{s_1, s_2, \dots, s_m\}$. 

\textbf{Step 2: Construct Bipartite Graph.} We then consider constructing the following bipartite graph $\mathcal{B} = (\mathcal{S}, \mathcal{I}, \mathcal{E})$, where $\mathcal{S} = \{s_1, s_2, \dots, s_m\}$ represents the set of representative vertices corresponding to the sentences in all pure text answers, and $\mathcal{I} = \{i_1, i_2, \dots, i_n\}$ represents the set of representative vertices corresponding to each retrieved image. Here, we construct the edge set $\mathcal{E}$ by calculating the string similarity $\alpha_{k,l}$ (using BLEU method \citep{papineni2002bleu}) and semantic similarity $\beta_{k,l}$ (using BGE-M3 model \citep{chen2024bge}) for each sentence-image pair $(s_k, i_l)$. Specifically, to filter out irrelevant images, we define the edge set as $\mathcal{E} = \{(k, l) \mid \alpha_{k,l} > \alpha^* \text{ or } \beta_{k,l} > \beta^*\}$, where $\alpha^*, \beta^*$ are two hyper-parameters as thresholds, and each egde $(k,l)$ has the weight $w_{k,l} = \lambda\alpha_{k,l} + (1-\lambda)\beta_{k,l}$, where $\lambda\in [0,1]$ is a hyper-parameter.

\textbf{Step 3: Insert Images By Matching Algorithms.} Indeed, we aim to solve the following optimization problem:

\begin{equation}
\label{problem:matching}
    \operatornamewithlimits{max}_{M \subset \mathcal{E}} \sum_{(k,l) \in M} w_{k,l} ,
\end{equation}
where $M$ is a subset of the edge set $\mathcal{E}$. Specifically, we consider two types of constraints for $M$:

\begin{itemize}
    \item Each image can be inserted at most once, but multiple images are allowed to be inserted after the same sentence.
    \item Each image can be inserted at most once, and each sentence can have at most one image inserted after it.
\end{itemize}

For the first constraint, we can solve for the optimal solution to problem (\ref{problem:matching}) as $M^* = \left\{(k^*, l) \mid k^* = \operatornamewithlimits{argmax}_{(k,l) \in \mathcal{E}} w_{k,l}, \forall l \in [n] \right\}$. For the second constraint, $M$ forms a matching of the bipartite graph $\mathcal{B}$. We use the standard Edmonds' Blossom algorithm \citep{Edmonds_1965} to solve the weighted maximum bipartite graph matching problem in this case. Finally, we insert the images into the output in the form of placeholders. We test the performance of the two matching-based insertion algorithms on the web and manual datasets, with results shown in Figure~\ref{fig:enter-label}.
Overall, the weighted bipartite graph matching algorithm, which restricts each sentence to have at most one image inserted after it, performed better. Therefore, we adopted this method for subsequent large-scale experimental evaluations.


\begin{figure}
    \centering
\includegraphics[width=0.95\linewidth]{Fig/comparison_graphs.pdf}
    \caption{Comparison of the generative performance of two matching algorithms on different datasets.}
    \label{fig:enter-label}
\end{figure}