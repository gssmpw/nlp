

\section{Introduction}
\label{sec:intro}
Retrieval-Augmented Generation (RAG) addresses the challenges of outdated knowledge and hallucinatory outputs in Large Language Models (LLMs) by incorporating external knowledge into the reasoning process, enhancing response accuracy and relevance  \citep{hallucination, gupta2024rag}. Traditional RAG methods focus on retrieving textual knowledge, limiting their ability to leverage multimodal information such as images and tables.

With the evolution of generative models, the shift from traditional LLMs to multimodal large language models (MLLMs) has facilitated a more comprehensive 
%and nuanced 
understanding by integrating both textual and visual inputs \citep{wang2024comprehensive,wang2024qwen2}. 
Correspondingly, Multimodal Retrieval-Augmented Generation (MRAG) \citep{chen2022murag} builds upon traditional RAG by incorporating both textual and visual information, thereby enhancing the quality of generated responses through the image understanding capabilities of MLLMs.
However, while the retrieval process incorporates multimodal information, current MRAG methods primarily focus on multimodal input and text-based output. 
%Despite MLLMs' powerful image understanding capabilities, they continue to demonstrate a significant propensity for hallucinations in their generated textual outputs. 
Despite their advanced image comprehension capabilities, MLLMs frequently exhibit hallucinations when converting visual inputs into textual outputs \citep{bai2024hallucination}. 
This raises a natural question:``\textit{Why not present the images directly rather than describing their content?}"

Indeed, there is an emerging trend in LLM deployment toward generating  multimodal outputs that seamlessly integrate both text and images.
%In the deployment of LLMs, the ability to generate multimodal outputs—combining text and images—has become a core expectation for users.
%Increasingly, users demand visual content alongside textual information, preferring to ``see" rather than be told. 
Users increasingly favor visual content over purely textual information, preferring to ``see" rather than merely be ``told".
This growing need is particularly evident in critical scenarios, as shown in Figure \ref{fig:task_example}:
%\begin{figure*}[t]
  %  \centering
 %   \includegraphics[width=1.0\linewidth]{Fig/Task_example_combine.pdf}
 %   \caption{
%    Scenarios illustrating that integrating text and images enhances clarity and understanding.
  %  }
   % \label{fig:examples}
%\end{figure*}
\begin{itemize} 
    \item \textbf{Images as answers}: ``\textit{Show, don't tell}." In certain cases, an image provides the most effective response. For instance, when asked about the appearance of a cat, a photograph is the most direct and convincing answer—seeing truly is believing. 
    % \binghui{WebQA++}
    \item \textbf{Text-Image integration for 
    answers}: 
    %``Seeing is believing."
    %``A picture and words go hand in hand." 
    ``\textit{A picture and words are two puzzle pieces—only together do they reveal the full story.}"
    %``The eye is the best painter."  
    Integrating pictures and words,  such as in step-by-step recipe instructions, significantly enhances comprehension which offers clearer insights and fosters a more intuitive understanding of the content.
    \item %Images enhancing generative results
    \textbf{Images as a catalyst for richer answer}: 
    ``\textit{A picture is the icing on the cake.}" 
    %``The eye is the best painter." 
    In applications such as tourist spot descriptions, integrating both textual and visual content can significantly enhance the quality of the generated response.
    \end{itemize}
\vspace{-3pt}
To address this critical demand, we introduce the \textbf{Multimodal Retrieval-Augmented Multimodal Generation (MRAMG)} task, which aims to fully leverage the rich multimodal information within a corpus by generating integrated text-and-image answers (See Section \ref{sec:task_definition} for the definition).
However, current benchmarks for evaluating this critical task suffer from a notable lack of suitable datasets and scientifically rigorous evaluation metrics, as shown in Table \ref{tab:compare}. This significant shortcoming hinders progress in this emerging field.



To bridge this gap, we introduce the \textbf{MRAMG-Bench}, a novel benchmark designed to evaluate the MRAMG task comprehensively.
The MRAMG-Bench consists of six carefully curated datasets, comprising 4,346 documents, 14,190 images, and 4,800 QA pairs, 
%spanning across three diverse domains
sourced from three domains—Web Data, Academic Paper Data, and Lifestyle Data—across seven distinct data sources.
Furthermore, MRAMG-Bench introduces unique challenges, such as hierarchical difficulty levels and order-based reasoning, which rigorously test the reasoning capabilities of both LLMs and MLLMs.  
To the best of our knowledge, MRAMG-Bench is the first benchmark that challenges models to autonomously determine the number, selection, and ordering of images in their responses, effectively simulating the complex scenarios encountered in real-world user interactions.


This benchmark is systematically constructed through a multi-stage workflow with both automatic construction of GPT-4o and meticulous formulation of human annotators, which is illustrated in Figure \ref{fig:data_construction}. 
Traditional evaluations of multimodal answers often rely on subjective metrics, such as image-text consistency, which are assessed through LLM-based judgments or human evaluations \citep{zhang2023internlm,zhu2024murar}.
However, these methods suffer from high subjectivity or evaluator inconsistency.
To mitigate these issues, we introduce a statistically grounded evaluation framework that systematically assesses both retrieval and generation performance. By integrating objective statistical metrics with LLM-based assessments, our framework delivers a comprehensive, multi-dimensional evaluation of multimodal answers, ensuring a fair and rigorous assessment process.

Additionally, we propose a general multimodal answer generation framework that integrates rule-based and model-based approaches. This flexible and scalable solution enables both LLMs and MLLMs to generate interleaved text-image responses.
Our main contributions can be summarized as follows:
\begin{itemize}
    \item 
    We formalize the MRAMG task 
    %(Section \ref{sec:task_definition}) and introduce MRAMG-Bench (Section \ref{sec:dataset})
    , a benchmark consisting of 4,800 human-annotated instances across multiple domains, featuring hierarchical difficulty levels and order-based reasoning challenges. To ensure rigorous evaluation, we propose a robust, statistically grounded set of metrics that assess multimodal responses across various performance dimensions. %(Section \ref{sec:experiments}).
    %We formalize the MRAMG task and present MRAMG-Bench, a benchmark comprising multi-domain data
    %Web-Data, Academic Paper Data, and Lifestyle Data
    %, totaling 4,800 expert-annotated instances with hierarchical difficulty levels and order-based reasoning challenges. 
    \item 
    %We propose a flexible multimodal answer generation framework leveraging generative large models and vision-language models.
    We propose a flexible multimodal answer generation framework that leverages LLMs and MLLMs, combining rule-based and model-based approaches. %(Section \ref{sec:framework}).
    
 %   \item We propose a robust, statistically grounded set of evaluation metrics to rigorously assess multimodal responses across multiple performance dimensions.
    %We introduce a robust, statistically grounded set of comprehensive evaluation metrics designed to rigorously assess multimodal responses across a range of performance dimensions.
    \item 
    We conduct a comprehensive evaluation of 11 advanced generative models on MRAMG-Bench, providing valuable insights into the capabilities and limitations of existing multimodal generation approaches. %(Section \ref{sec:experiments})
\end{itemize}


