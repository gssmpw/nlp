[
  {
    "index": 0,
    "papers": [
      {
        "key": "2020RAG",
        "author": "Patrick S. H. Lewis and\nEthan Perez and\nAleksandra Piktus and others",
        "title": "Retrieval-Augmented Generation for Knowledge-Intensive {NLP} Tasks"
      },
      {
        "key": "zhao2024retrieval",
        "author": "Zhao, Penghao and Zhang, Hailin and Yu, Qinhan and Wang, Zhengren and Geng, Yunteng and Fu, Fangcheng and Yang, Ling and Zhang, Wentao and Cui, Bin",
        "title": "Retrieval-augmented generation for ai-generated content: A survey"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "hallucination",
        "author": "Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and others",
        "title": "A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions"
      },
      {
        "key": "gupta2024rag",
        "author": "Gupta, Aman and Shirgaonkar, Anup and Balaguer, Angels de Luis and Silva, Bruno and Holstein, Daniel and Li, Dawei and Marsman, Jennifer and Nunes, Leonardo O and Rouzbahman, Mahsa and Sharp, Morris and others",
        "title": "RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "chen2022murag",
        "author": "Chen, Wenhu and Hu, Hexiang and Chen, Xi and Verga, Pat and Cohen, William W",
        "title": "Murag: Multimodal retrieval-augmented generator for open question answering over images and text"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "liu2023learning",
        "author": "Liu, Haotian and Son, Kilho and Yang, Jianwei and Liu, Ce and Gao, Jianfeng and Lee, Yong Jae and Li, Chunyuan",
        "title": "Learning customized visual models with retrieval-augmented knowledge"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "liu2023learning",
        "author": "Liu, Haotian and Son, Kilho and Yang, Jianwei and Liu, Ce and Gao, Jianfeng and Lee, Yong Jae and Li, Chunyuan",
        "title": "Learning customized visual models with retrieval-augmented knowledge"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "gui2021kat",
        "author": "Gui, Liangke and Wang, Borui and Huang, Qiuyuan and Hauptmann, Alex and Bisk, Yonatan and Gao, Jianfeng",
        "title": "Kat: A knowledge augmented transformer for vision-and-language"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "radford2021learning",
        "author": "Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others",
        "title": "Learning transferable visual models from natural language supervision"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "lin2022retrieval",
        "author": "Lin, Weizhe and Byrne, Bill",
        "title": "Retrieval augmented visual question answering with outside knowledge"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "karpukhin2020dense",
        "author": "Karpukhin, Vladimir and O{\\u{g}}uz, Barlas and Min, Sewon and Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau",
        "title": "Dense passage retrieval for open-domain question answering"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "zhu2024murar",
        "author": "Zhu, Zhengyuan and Lee, Daniel and Zhang, Hong and Harsha, Sai Sree and Feujio, Loic and Maharaj, Akash and Li, Yunyao",
        "title": "Murar: A simple and effective multimodal retrieval and answer refinement framework for multimodal question answering"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "ma2024multi",
        "author": "Ma, Zi-Ao and Lan, Tian and Tu, Rong-Cheng and Hu, Yong and Huang, Heyan and Mao, Xian-Ling",
        "title": "Multi-modal Retrieval Augmented Multi-modal Generation: A Benchmark, Evaluate Metrics and Strong Baselines"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "MSMARCO",
        "author": "Nguyen, Tri and Rosenberg, Mir and Song, Xia and Gao, Jianfeng and Tiwary, Saurabh and Majumder, Rangan and Deng, Li",
        "title": "MS MARCO: A Human Generated MAchine Reading COmprehension Dataset"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "joshi2017triviaqa",
        "author": "Joshi, Mandar and Choi, Eunsol and Weld, Daniel and Zettlemoyer, Luke",
        "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "yang2018hotpotqa",
        "author": "Yang, Zhilin and Qi, Peng and Zhang, Saizheng and Bengio, Yoshua and Cohen, William and Salakhutdinov, Ruslan and Manning, Christopher D",
        "title": "HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "Naturalquestions",
        "author": "Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and others",
        "title": "Natural questions: a benchmark for question answering research"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "rajpurkar2016squad",
        "author": "Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy",
        "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "petroni2020kilt",
        "author": "Petroni, Fabio and Piktus, Aleksandra and Fan, Angela and Lewis, Patrick and Yazdani, Majid and De Cao, Nicola and Thorne, James and Jernite, Yacine and Karpukhin, Vladimir and Maillard, Jean and others",
        "title": "KILT: a benchmark for knowledge intensive language tasks"
      },
      {
        "key": "QAE",
        "author": "Wang, Zhengren and Yu, Qinhan and Wei, Shida and Li, Zhiyu and Xiong, Feiyu and Wang, Xiaoxing and Niu, Simin and Liang, Hao and Zhang, Wentao",
        "title": "QAEncoder: Towards Aligned Representation Learning in Question Answering System"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "marino2019ok",
        "author": "Marino, Kenneth and Rastegari, Mohammad and Farhadi, Ali and Mottaghi, Roozbeh",
        "title": "Ok-vqa: A visual question answering benchmark requiring external knowledge"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "schwenk2022okvqa",
        "author": "Schwenk, Dustin and Khandelwal, Apoorv and Clark, Christopher and Marino, Kenneth and Mottaghi, Roozbeh",
        "title": "A-okvqa: A benchmark for visual question answering using world knowledge"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "jiang2024mmsearch",
        "author": "Jiang, Dongzhi and Zhang, Renrui and Guo, Ziyu and Wu, Yanmin and Lei, Jiayi and Qiu, Pengshuo and Lu, Pan and Chen, Zehui and Song, Guanglu and Gao, Peng and others",
        "title": "Mmsearch: Benchmarking the potential of large models as multi-modal search engines"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "talmor2021multimodalqa",
        "author": "Talmor, Alon and Yoran, Ori and Catav, Amnon and Lahav, Dan and Wang, Yizhong and Asai, Akari and Ilharco, Gabriel and Hajishirzi, Hannaneh and Berant, Jonathan",
        "title": "Multimodalqa: Complex question answering over text, tables and images"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "chang2022webqa",
        "author": "Chang, Yingshan and Bisk, Yonatan",
        "title": "WebQA: A Multimodal Multihop NeurIPS Challenge"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "ma2024multi",
        "author": "Ma, Zi-Ao and Lan, Tian and Tu, Rong-Cheng and Hu, Yong and Huang, Heyan and Mao, Xian-Ling",
        "title": "Multi-modal Retrieval Augmented Multi-modal Generation: A Benchmark, Evaluate Metrics and Strong Baselines"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "2020RAG",
        "author": "Patrick S. H. Lewis and\nEthan Perez and\nAleksandra Piktus and others",
        "title": "Retrieval-Augmented Generation for Knowledge-Intensive {NLP} Tasks"
      },
      {
        "key": "zhao2024retrieval",
        "author": "Zhao, Penghao and Zhang, Hailin and Yu, Qinhan and Wang, Zhengren and Geng, Yunteng and Fu, Fangcheng and Yang, Ling and Zhang, Wentao and Cui, Bin",
        "title": "Retrieval-augmented generation for ai-generated content: A survey"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "hallucination",
        "author": "Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and others",
        "title": "A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions"
      },
      {
        "key": "gupta2024rag",
        "author": "Gupta, Aman and Shirgaonkar, Anup and Balaguer, Angels de Luis and Silva, Bruno and Holstein, Daniel and Li, Dawei and Marsman, Jennifer and Nunes, Leonardo O and Rouzbahman, Mahsa and Sharp, Morris and others",
        "title": "RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "wang2024comprehensive",
        "author": "Wang, Jiaqi and Jiang, Hanqi and Liu, Yiheng and Ma, Chong and Zhang, Xu and Pan, Yi and Liu, Mengyuan and Gu, Peiran and Xia, Sichen and Li, Wenjun and others",
        "title": "A comprehensive review of multimodal large language models: Performance and challenges across different tasks"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "chen2022murag",
        "author": "Chen, Wenhu and Hu, Hexiang and Chen, Xi and Verga, Pat and Cohen, William W",
        "title": "Murag: Multimodal retrieval-augmented generator for open question answering over images and text"
      }
    ]
  },
  {
    "index": 27,
    "papers": [
      {
        "key": "liu2023learning",
        "author": "Liu, Haotian and Son, Kilho and Yang, Jianwei and Liu, Ce and Gao, Jianfeng and Lee, Yong Jae and Li, Chunyuan",
        "title": "Learning customized visual models with retrieval-augmented knowledge"
      }
    ]
  },
  {
    "index": 28,
    "papers": [
      {
        "key": "gui2021kat",
        "author": "Gui, Liangke and Wang, Borui and Huang, Qiuyuan and Hauptmann, Alex and Bisk, Yonatan and Gao, Jianfeng",
        "title": "Kat: A knowledge augmented transformer for vision-and-language"
      }
    ]
  },
  {
    "index": 29,
    "papers": [
      {
        "key": "radford2021learning",
        "author": "Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others",
        "title": "Learning transferable visual models from natural language supervision"
      }
    ]
  },
  {
    "index": 30,
    "papers": [
      {
        "key": "zhu2024murar",
        "author": "Zhu, Zhengyuan and Lee, Daniel and Zhang, Hong and Harsha, Sai Sree and Feujio, Loic and Maharaj, Akash and Li, Yunyao",
        "title": "Murar: A simple and effective multimodal retrieval and answer refinement framework for multimodal question answering"
      }
    ]
  },
  {
    "index": 31,
    "papers": [
      {
        "key": "ma2024multi",
        "author": "Ma, Zi-Ao and Lan, Tian and Tu, Rong-Cheng and Hu, Yong and Huang, Heyan and Mao, Xian-Ling",
        "title": "Multi-modal Retrieval Augmented Multi-modal Generation: A Benchmark, Evaluate Metrics and Strong Baselines"
      }
    ]
  },
  {
    "index": 32,
    "papers": [
      {
        "key": "MSMARCO",
        "author": "Nguyen, Tri and Rosenberg, Mir and Song, Xia and Gao, Jianfeng and Tiwary, Saurabh and Majumder, Rangan and Deng, Li",
        "title": "MS MARCO: A Human Generated MAchine Reading COmprehension Dataset"
      },
      {
        "key": "joshi2017triviaqa",
        "author": "Joshi, Mandar and Choi, Eunsol and Weld, Daniel and Zettlemoyer, Luke",
        "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"
      },
      {
        "key": "yang2018hotpotqa",
        "author": "Yang, Zhilin and Qi, Peng and Zhang, Saizheng and Bengio, Yoshua and Cohen, William and Salakhutdinov, Ruslan and Manning, Christopher D",
        "title": "HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering"
      },
      {
        "key": "Naturalquestions",
        "author": "Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and others",
        "title": "Natural questions: a benchmark for question answering research"
      },
      {
        "key": "rajpurkar2016squad",
        "author": "Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy",
        "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"
      }
    ]
  },
  {
    "index": 33,
    "papers": [
      {
        "key": "petroni2020kilt",
        "author": "Petroni, Fabio and Piktus, Aleksandra and Fan, Angela and Lewis, Patrick and Yazdani, Majid and De Cao, Nicola and Thorne, James and Jernite, Yacine and Karpukhin, Vladimir and Maillard, Jean and others",
        "title": "KILT: a benchmark for knowledge intensive language tasks"
      },
      {
        "key": "QAE",
        "author": "Wang, Zhengren and Yu, Qinhan and Wei, Shida and Li, Zhiyu and Xiong, Feiyu and Wang, Xiaoxing and Niu, Simin and Liang, Hao and Zhang, Wentao",
        "title": "QAEncoder: Towards Aligned Representation Learning in Question Answering System"
      }
    ]
  },
  {
    "index": 34,
    "papers": [
      {
        "key": "MSMARCO",
        "author": "Nguyen, Tri and Rosenberg, Mir and Song, Xia and Gao, Jianfeng and Tiwary, Saurabh and Majumder, Rangan and Deng, Li",
        "title": "MS MARCO: A Human Generated MAchine Reading COmprehension Dataset"
      }
    ]
  },
  {
    "index": 35,
    "papers": [
      {
        "key": "joshi2017triviaqa",
        "author": "Joshi, Mandar and Choi, Eunsol and Weld, Daniel and Zettlemoyer, Luke",
        "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"
      }
    ]
  },
  {
    "index": 36,
    "papers": [
      {
        "key": "yang2018hotpotqa",
        "author": "Yang, Zhilin and Qi, Peng and Zhang, Saizheng and Bengio, Yoshua and Cohen, William and Salakhutdinov, Ruslan and Manning, Christopher D",
        "title": "HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering"
      }
    ]
  },
  {
    "index": 37,
    "papers": [
      {
        "key": "Naturalquestions",
        "author": "Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and others",
        "title": "Natural questions: a benchmark for question answering research"
      }
    ]
  },
  {
    "index": 38,
    "papers": [
      {
        "key": "rajpurkar2016squad",
        "author": "Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy",
        "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"
      }
    ]
  },
  {
    "index": 39,
    "papers": [
      {
        "key": "marino2019ok",
        "author": "Marino, Kenneth and Rastegari, Mohammad and Farhadi, Ali and Mottaghi, Roozbeh",
        "title": "Ok-vqa: A visual question answering benchmark requiring external knowledge"
      },
      {
        "key": "schwenk2022okvqa",
        "author": "Schwenk, Dustin and Khandelwal, Apoorv and Clark, Christopher and Marino, Kenneth and Mottaghi, Roozbeh",
        "title": "A-okvqa: A benchmark for visual question answering using world knowledge"
      },
      {
        "key": "jiang2024mmsearch",
        "author": "Jiang, Dongzhi and Zhang, Renrui and Guo, Ziyu and Wu, Yanmin and Lei, Jiayi and Qiu, Pengshuo and Lu, Pan and Chen, Zehui and Song, Guanglu and Gao, Peng and others",
        "title": "Mmsearch: Benchmarking the potential of large models as multi-modal search engines"
      },
      {
        "key": "talmor2021multimodalqa",
        "author": "Talmor, Alon and Yoran, Ori and Catav, Amnon and Lahav, Dan and Wang, Yizhong and Asai, Akari and Ilharco, Gabriel and Hajishirzi, Hannaneh and Berant, Jonathan",
        "title": "Multimodalqa: Complex question answering over text, tables and images"
      },
      {
        "key": "chang2022webqa",
        "author": "Chang, Yingshan and Bisk, Yonatan",
        "title": "WebQA: A Multimodal Multihop NeurIPS Challenge"
      },
      {
        "key": "ma2024multi",
        "author": "Ma, Zi-Ao and Lan, Tian and Tu, Rong-Cheng and Hu, Yong and Huang, Heyan and Mao, Xian-Ling",
        "title": "Multi-modal Retrieval Augmented Multi-modal Generation: A Benchmark, Evaluate Metrics and Strong Baselines"
      }
    ]
  },
  {
    "index": 40,
    "papers": [
      {
        "key": "marino2019ok",
        "author": "Marino, Kenneth and Rastegari, Mohammad and Farhadi, Ali and Mottaghi, Roozbeh",
        "title": "Ok-vqa: A visual question answering benchmark requiring external knowledge"
      }
    ]
  },
  {
    "index": 41,
    "papers": [
      {
        "key": "schwenk2022okvqa",
        "author": "Schwenk, Dustin and Khandelwal, Apoorv and Clark, Christopher and Marino, Kenneth and Mottaghi, Roozbeh",
        "title": "A-okvqa: A benchmark for visual question answering using world knowledge"
      }
    ]
  },
  {
    "index": 42,
    "papers": [
      {
        "key": "jiang2024mmsearch",
        "author": "Jiang, Dongzhi and Zhang, Renrui and Guo, Ziyu and Wu, Yanmin and Lei, Jiayi and Qiu, Pengshuo and Lu, Pan and Chen, Zehui and Song, Guanglu and Gao, Peng and others",
        "title": "Mmsearch: Benchmarking the potential of large models as multi-modal search engines"
      }
    ]
  },
  {
    "index": 43,
    "papers": [
      {
        "key": "talmor2021multimodalqa",
        "author": "Talmor, Alon and Yoran, Ori and Catav, Amnon and Lahav, Dan and Wang, Yizhong and Asai, Akari and Ilharco, Gabriel and Hajishirzi, Hannaneh and Berant, Jonathan",
        "title": "Multimodalqa: Complex question answering over text, tables and images"
      }
    ]
  },
  {
    "index": 44,
    "papers": [
      {
        "key": "chang2022webqa",
        "author": "Chang, Yingshan and Bisk, Yonatan",
        "title": "WebQA: A Multimodal Multihop NeurIPS Challenge"
      }
    ]
  },
  {
    "index": 45,
    "papers": [
      {
        "key": "ma2024multi",
        "author": "Ma, Zi-Ao and Lan, Tian and Tu, Rong-Cheng and Hu, Yong and Huang, Heyan and Mao, Xian-Ling",
        "title": "Multi-modal Retrieval Augmented Multi-modal Generation: A Benchmark, Evaluate Metrics and Strong Baselines"
      }
    ]
  },
  {
    "index": 46,
    "papers": [
      {
        "key": "yang2018hotpotqa",
        "author": "Yang, Zhilin and Qi, Peng and Zhang, Saizheng and Bengio, Yoshua and Cohen, William and Salakhutdinov, Ruslan and Manning, Christopher D",
        "title": "HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering"
      }
    ]
  },
  {
    "index": 47,
    "papers": [
      {
        "key": "marino2019ok",
        "author": "Marino, Kenneth and Rastegari, Mohammad and Farhadi, Ali and Mottaghi, Roozbeh",
        "title": "Ok-vqa: A visual question answering benchmark requiring external knowledge"
      }
    ]
  },
  {
    "index": 48,
    "papers": [
      {
        "key": "chang2022webqa",
        "author": "Chang, Yingshan and Bisk, Yonatan",
        "title": "WebQA: A Multimodal Multihop NeurIPS Challenge"
      }
    ]
  },
  {
    "index": 49,
    "papers": [
      {
        "key": "jiang2024mmsearch",
        "author": "Jiang, Dongzhi and Zhang, Renrui and Guo, Ziyu and Wu, Yanmin and Lei, Jiayi and Qiu, Pengshuo and Lu, Pan and Chen, Zehui and Song, Guanglu and Gao, Peng and others",
        "title": "Mmsearch: Benchmarking the potential of large models as multi-modal search engines"
      }
    ]
  },
  {
    "index": 50,
    "papers": [
      {
        "key": "ma2024multi",
        "author": "Ma, Zi-Ao and Lan, Tian and Tu, Rong-Cheng and Hu, Yong and Huang, Heyan and Mao, Xian-Ling",
        "title": "Multi-modal Retrieval Augmented Multi-modal Generation: A Benchmark, Evaluate Metrics and Strong Baselines"
      }
    ]
  }
]