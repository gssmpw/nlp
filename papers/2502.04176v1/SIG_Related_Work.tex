\section{Related Work}
\label{sec:related_work}

\iffalse
\paragraph{Multimodal RAG methods.}
RAG \citep{2020RAG, zhao2024retrieval} enhances the effectiveness of LLMs by integrating external knowledge into their reasoning process, addressing issues such as outdated training data and hallucinatory responses \citep{hallucination, gupta2024rag}.
As Multimodal Large Language Models (MLLMs) have advanced in their capacity to integrate and generate text from both textual and visual inputs, the development of Multimodal Retrieval-Augmented Generation (mRAG) has naturally followed as an extension, with methods like MuRAG \citep{chen2022murag} and REACT \citep{liu2023learning} retrieving relevant image-text pairs from external memory.
%Similarly, REACT \citep{liu2023learning} introduces a framework for constructing customized visual models by retrieving relevant image-text pairs from web-scale data and training modularized blocks.
To tackle Visual Question Answering (VQA) task, KAT \citep{gui2021kat} leverages the CLIP \citep{radford2021learning} image encoder to retrieve and associate specific image regions with external knowledge bases
%, thereby enhancing the modelâ€™s understanding of visual content.
%Meanwhile, RA-VQA \citep{lin2022retrieval} integrates differentiable DPR \citep{karpukhin2020dense} with answer generation, leveraging retrieved knowledge to achieve outstanding performance in VQA.
Unlike existing mRAG methods that convert multimodal information into purely textual outputs, our work addresses a distinct MRAMG task, where the output seamlessly integrates both textual and visual information.
%, preserving the inherent multimodal richness of the data.
A closely related recent work, MuRAR \citep{zhu2024murar}, addresses source attribution by retrieving multimodal elements from attributed documents. Furthermore, M2RAG \citep{ma2024multi} introduces a multi-stage image insertion framework, which involves multiple calls to the generation model during a single answer generation process.
However, these approaches often face challenges of high computational overhead due to repeated model invocations. In this paper, we propose a general framework for multimodal answer generation, leveraging a single invocation of the large generative model.
\paragraph{Classic RAG Datasets}
Well-established benchmarks for RAG, such as MS-MARCO \citep{MSMARCO} (a large-scale QA dataset based on real user queries), TriviaQA \citep{joshi2017triviaqa} (which features trivia questions requiring evidence-based answers), HotpotQA \citep{yang2018hotpotqa} (focused on multi-hop reasoning), Natural Questions (NQ) \citep{Naturalquestions} (based on real Google search queries), and SQuAD \citep{rajpurkar2016squad} (a reading comprehension dataset with span-based answers), are widely used to evaluate RAG performance \citep{petroni2020kilt, QAE}. 
However, these datasets focus on text-based tasks, while real-world applications increasingly require seamless integration of textual and visual information. To address this gap, we introduce a novel benchmark for MRAMG evaluation.
\paragraph{Multimodal RAG Datasets}
Various multimodal RAG datasets have been developed to address tasks requiring multimodal knowledge. 
OK-VQA \citep{marino2019ok} and A-OKVQA \citep{schwenk2022okvqa} that require retrieving external knowledge beyond the image content, whereas MMSearch \citep{jiang2024mmsearch} benchmarks MLLMs as multimodal search engines, focusing on image-to-image retrieval.
In contrast, MultiModalQA \citep{talmor2021multimodalqa} presents a more challenging scenario, where the questions do not include images but require joint reasoning across text and tables to answer complex questions.
While MultiModalQA questions are template-based, WebQA \citep{chang2022webqa} is a multi-hop, manually crafted multimodal QA dataset that involves the retrieval of relevant visual content for questions.
However, WebQA provides purely textual answers, relies solely on MLLMs for reasoning over retrieved images, and lacks textual support, making it unsuitable for language models that require linguistic context for coherent response generation.
M2RAG \citep{ma2024multi}, while constructing a multimodal corpus, is limited to only 200 questions and a corpus restricted to web pages. 
Additionally, the dataset relies on automated generation without manual verification and lacks ground truth for each query, which further complicates accurate evaluation.
To address these challenges, MRAMG-Bench introduces a comprehensive
multimodal dataset specifically designed for MRAMG tasks.  
Each question is meticulously paired with a precise, integrated text-image answer, enabling comprehensive and statistically rigorous evaluation.
\fi

\subsection{Multimodal RAG}
Retrieval-Augmented Generation (RAG) \citep{2020RAG, zhao2024retrieval} enhances the capabilities of Large Language Models (LLMs) by integrating external knowledge, addressing limitations such as outdated training data and hallucinated outputs \citep{hallucination, gupta2024rag}. Recent advancements in Multimodal Large Language Models (MLLMs), which combine textual and visual inputs for generation tasks \citep{wang2024comprehensive}, have spurred the development of Multimodal Retrieval-Augmented Generation (MRAG) as an extension of traditional RAG. Notable approaches, including MuRAG \citep{chen2022murag} and REACT \citep{liu2023learning}, retrieve image-text pairs from external memory to support multimodal generation. In the context of Visual Question Answering (VQA), KAT \citep{gui2021kat} employs the CLIP \citep{radford2021learning} image encoder to associate specific image regions with external knowledge bases. Unlike existing MRAG methods that primarily generate textual outputs, our work focuses on a novel MRAMG task, where the output seamlessly integrates both textual and visual information. A closely related work, MuRAR \citep{zhu2024murar}, addresses source attribution by retrieving multimodal elements from attributed documents. M2RAG \citep{ma2024multi} further extends this by introducing a multi-stage image insertion framework, which involves multiple invocations of the generative model during a single answer generation process. However, these methods are often hindered by high computational costs due to repeated model calls. In contrast, we propose a more efficient framework for multimodal answer generation, leveraging a single invocation of the generative model.
\vspace{-2mm}
\subsection{RAG Benchmarks}
The effective evaluation of Retrieval-Augmented Generation (RAG) models is crucial for advancing their development and optimization. 
Established benchmarks \citep{MSMARCO,joshi2017triviaqa,yang2018hotpotqa,Naturalquestions,rajpurkar2016squad} are commonly used to evaluate RAG models \citep{petroni2020kilt, QAE}. MS-MARCO \citep{MSMARCO} is a large-scale question answering (QA) dataset based on real user queries. TriviaQA \citep{joshi2017triviaqa} consists of trivia questions that require evidence-based answers. HotpotQA \citep{yang2018hotpotqa} focuses on multi-hop reasoning. Natural Questions (NQ) \citep{Naturalquestions} is derived from real Google search queries. SQuAD \citep{rajpurkar2016squad} is a reading comprehension dataset with span-based answers.
While these text-based benchmarks have proven effective in assessing RAG performance, they fall short in evaluating multimodal tasks, which require the integration of both textual and visual information. 
To bridge this gap, we introduce a novel benchmark for evaluating the MRAMG task.
\subsection{Multimodal RAG Benchmarks}
Similar to traditional RAG benchmarks, various multimodal RAG benchmarks \citep{marino2019ok,schwenk2022okvqa,jiang2024mmsearch,talmor2021multimodalqa,chang2022webqa,ma2024multi} have been developed to address tasks requiring multimodal knowledge. OK-VQA \citep{marino2019ok} and A-OKVQA \citep{schwenk2022okvqa} evaluate the multimodal reasoning ability of models using external knowledge beyond image content. MMSearch \citep{jiang2024mmsearch} evaluates MLLMs as multimodal search engines, focusing on image-to-image retrieval. MultiModalQA \citep{talmor2021multimodalqa} presents a more challenging scenario, where questions do not include images but require joint reasoning across text and tables to answer complex questions. While MultiModalQA relies on template-based questions, WebQA \citep{chang2022webqa} is a multi-hop, manually crafted multimodal QA dataset that involves retrieving relevant visual content for questions.
However, WebQA provides only textual answers, relies entirely on MLLMs for reasoning over retrieved images, and lacks textual support, making it unsuitable for language models that depend on linguistic context for coherent response generation. Notably, although M2RAG \citep{ma2024multi} constructs a multimodal corpus, it is limited to just 200 questions and a corpus confined to web pages. Additionally, the dataset relies on automated generation without manual verification and lacks ground truth for each query, further complicating accurate evaluation.
To address these challenges, our MRAMG-Bench introduces a comprehensive multimodal benchmark specifically designed for MRAMG task. Each question is meticulously paired with a precise, integrated text-image answer, enabling comprehensive and statistically rigorous evaluation.





\begin{table*}[tb!]
  \centering
   \caption{
Comparison of MRAMG-Bench with existing RAG benchmarks, where \includegraphics[height=1em]{Fig/text.png} represents text and \includegraphics[height=1em]{Fig/pic.png} represents images.
   }
   \vspace{-2mm}
\resizebox{0.9\textwidth}{!}{
  \begin{tabular}{l|cc|cccc|ccc}
    \toprule
    \multirow{2}[4]{*}{DATASETS} & \multicolumn{2}{c|}{Documents} & \multicolumn{4}{c|}{Questions}        & \multicolumn{3}{c}{Answers} \\
\cmidrule{2-10}          & Type  & Domain & Retrieval Modality & Difficulty Levels? & Type  & Num   & Exist? & Type  & Human-Annotated? \\
    \midrule
    HotpotQA \citep{yang2018hotpotqa} & \includegraphics[height=1em]{Fig/text.png}  & Web     & \includegraphics[height=1em]{Fig/text.png}  & \xmark    & \includegraphics[height=1em]{Fig/text.png}  & 113k   & \cmark    & \includegraphics[height=1em]{Fig/text.png}  & \cmark\\
    OK-VQA \citep{marino2019ok} & \includegraphics[height=1em]{Fig/text.png}  & Open Domain   & \includegraphics[height=1em]{Fig/text.png}  & \xmark   & \includegraphics[height=1em]{Fig/text.png} \includegraphics[height=1em]{Fig/pic.png} & 14k   & \cmark  & \includegraphics[height=1em]{Fig/text.png}  & \cmark\\
    WebQA \citep{chang2022webqa} & \includegraphics[height=1em]{Fig/text.png}  \includegraphics[height=1em]{Fig/pic.png} & Web      & \includegraphics[height=1em]{Fig/text.png} \includegraphics[height=1em]{Fig/pic.png} & \xmark    & \includegraphics[height=1em]{Fig/text.png}  & 56.6k & \cmark  & \includegraphics[height=1em]{Fig/text.png}  & \cmark\\
     MMSearch \citep{jiang2024mmsearch} &   \includegraphics[height=1em]{Fig/pic.png} & Multi-domain     &  \includegraphics[height=1em]{Fig/pic.png} & \xmark    & \includegraphics[height=1em]{Fig/text.png} \includegraphics[height=1em]{Fig/pic.png} & 300 & \cmark  & \includegraphics[height=1em]{Fig/text.png}  & \cmark\\
    M2RAG \citep{ma2024multi} & \includegraphics[height=1em]{Fig/text.png} \includegraphics[height=1em]{Fig/pic.png} & Web     & \includegraphics[height=1em]{Fig/text.png} \includegraphics[height=1em]{Fig/pic.png} & \xmark    & \includegraphics[height=1em]{Fig/text.png}  & 200   & \xmark    & \xmark    & \xmark \\
    \midrule
    MRAMG-Bench & \includegraphics[height=1em]{Fig/text.png} \includegraphics[height=1em]{Fig/pic.png} & Multi-domain    & \includegraphics[height=1em]{Fig/text.png} \includegraphics[height=1em]{Fig/pic.png} & \cmark  & \includegraphics[height=1em]{Fig/text.png}  & 4.8k   & \cmark  & \includegraphics[height=1em]{Fig/text.png} \includegraphics[height=1em]{Fig/pic.png} & \cmark\\
    \bottomrule
    \end{tabular}%
}
  \label{tab:compare}%
\end{table*}%