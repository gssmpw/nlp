@inproceedings{2020RAG,
  author       = {Patrick S. H. Lewis and
                  Ethan Perez and
                  Aleksandra Piktus and others},
  title        = {Retrieval-Augmented Generation for Knowledge-Intensive {NLP} Tasks},
  booktitle    = {NeurIPS},
  year         = {2020},
}

@article{MSMARCO,
  title={MS MARCO: A Human Generated MAchine Reading COmprehension Dataset},
  author={Nguyen, Tri and Rosenberg, Mir and Song, Xia and Gao, Jianfeng and Tiwary, Saurabh and Majumder, Rangan and Deng, Li},
  journal={choice},
  volume={2640},
  pages={660},
  year={2016}
}

@article{Naturalquestions,
  title={Natural questions: a benchmark for question answering research},
  author={Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and others},
  journal={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={453--466},
  year={2019},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~â€¦}
}

@article{QAE,
  title={QAEncoder: Towards Aligned Representation Learning in Question Answering System},
  author={Wang, Zhengren and Yu, Qinhan and Wei, Shida and Li, Zhiyu and Xiong, Feiyu and Wang, Xiaoxing and Niu, Simin and Liang, Hao and Zhang, Wentao},
  journal={arXiv preprint arXiv:2409.20434},
  year={2024}
}

@inproceedings{chang2022webqa,
  title={WebQA: A Multimodal Multihop NeurIPS Challenge},
  author={Chang, Yingshan and Bisk, Yonatan},
  booktitle={NeurIPS 2021 Competitions and Demonstrations Track},
  pages={232--245},
  year={2022},
  organization={PMLR}
}

@article{chen2022murag,
  title={Murag: Multimodal retrieval-augmented generator for open question answering over images and text},
  author={Chen, Wenhu and Hu, Hexiang and Chen, Xi and Verga, Pat and Cohen, William W},
  journal={arXiv preprint arXiv:2210.02928},
  year={2022}
}

@article{gui2021kat,
  title={Kat: A knowledge augmented transformer for vision-and-language},
  author={Gui, Liangke and Wang, Borui and Huang, Qiuyuan and Hauptmann, Alex and Bisk, Yonatan and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2112.08614},
  year={2021}
}

@article{gupta2024rag,
  title={RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture},
  author={Gupta, Aman and Shirgaonkar, Anup and Balaguer, Angels de Luis and Silva, Bruno and Holstein, Daniel and Li, Dawei and Marsman, Jennifer and Nunes, Leonardo O and Rouzbahman, Mahsa and Sharp, Morris and others},
  journal={arXiv preprint arXiv:2401.08406},
  year={2024}
}

@article{hallucination,
  title={A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions},
  author={Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and others},
  journal={arXiv preprint arXiv:2311.05232},
  year={2023}
}

@article{jiang2024mmsearch,
  title={Mmsearch: Benchmarking the potential of large models as multi-modal search engines},
  author={Jiang, Dongzhi and Zhang, Renrui and Guo, Ziyu and Wu, Yanmin and Lei, Jiayi and Qiu, Pengshuo and Lu, Pan and Chen, Zehui and Song, Guanglu and Gao, Peng and others},
  journal={arXiv preprint arXiv:2409.12959},
  year={2024}
}

@inproceedings{joshi2017triviaqa,
  title={TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension},
  author={Joshi, Mandar and Choi, Eunsol and Weld, Daniel and Zettlemoyer, Luke},
  booktitle={Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  year={2017},
  organization={Association for Computational Linguistics}
}

@article{karpukhin2020dense,
  title={Dense passage retrieval for open-domain question answering},
  author={Karpukhin, Vladimir and O{\u{g}}uz, Barlas and Min, Sewon and Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau},
  journal={arXiv preprint arXiv:2004.04906},
  year={2020}
}

@article{lin2022retrieval,
  title={Retrieval augmented visual question answering with outside knowledge},
  author={Lin, Weizhe and Byrne, Bill},
  journal={arXiv preprint arXiv:2210.03809},
  year={2022}
}

@inproceedings{liu2023learning,
  title={Learning customized visual models with retrieval-augmented knowledge},
  author={Liu, Haotian and Son, Kilho and Yang, Jianwei and Liu, Ce and Gao, Jianfeng and Lee, Yong Jae and Li, Chunyuan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={15148--15158},
  year={2023}
}

@article{ma2024multi,
  title={Multi-modal Retrieval Augmented Multi-modal Generation: A Benchmark, Evaluate Metrics and Strong Baselines},
  author={Ma, Zi-Ao and Lan, Tian and Tu, Rong-Cheng and Hu, Yong and Huang, Heyan and Mao, Xian-Ling},
  journal={arXiv preprint arXiv:2411.16365},
  year={2024}
}

@inproceedings{marino2019ok,
  title={Ok-vqa: A visual question answering benchmark requiring external knowledge},
  author={Marino, Kenneth and Rastegari, Mohammad and Farhadi, Ali and Mottaghi, Roozbeh},
  booktitle={Proceedings of the IEEE/cvf conference on computer vision and pattern recognition},
  pages={3195--3204},
  year={2019}
}

@article{petroni2020kilt,
  title={KILT: a benchmark for knowledge intensive language tasks},
  author={Petroni, Fabio and Piktus, Aleksandra and Fan, Angela and Lewis, Patrick and Yazdani, Majid and De Cao, Nicola and Thorne, James and Jernite, Yacine and Karpukhin, Vladimir and Maillard, Jean and others},
  journal={arXiv preprint arXiv:2009.02252},
  year={2020}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@inproceedings{rajpurkar2016squad,
  title={SQuAD: 100,000+ Questions for Machine Comprehension of Text},
  author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  booktitle={Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
  year={2016},
  organization={Association for Computational Linguistics}
}

@inproceedings{schwenk2022okvqa,
  title={A-okvqa: A benchmark for visual question answering using world knowledge},
  author={Schwenk, Dustin and Khandelwal, Apoorv and Clark, Christopher and Marino, Kenneth and Mottaghi, Roozbeh},
  booktitle={European conference on computer vision},
  pages={146--162},
  year={2022},
  organization={Springer}
}

@article{talmor2021multimodalqa,
  title={Multimodalqa: Complex question answering over text, tables and images},
  author={Talmor, Alon and Yoran, Ori and Catav, Amnon and Lahav, Dan and Wang, Yizhong and Asai, Akari and Ilharco, Gabriel and Hajishirzi, Hannaneh and Berant, Jonathan},
  journal={arXiv preprint arXiv:2104.06039},
  year={2021}
}

@article{wang2024comprehensive,
  title={A comprehensive review of multimodal large language models: Performance and challenges across different tasks},
  author={Wang, Jiaqi and Jiang, Hanqi and Liu, Yiheng and Ma, Chong and Zhang, Xu and Pan, Yi and Liu, Mengyuan and Gu, Peiran and Xia, Sichen and Li, Wenjun and others},
  journal={arXiv preprint arXiv:2408.01319},
  year={2024}
}

@inproceedings{yang2018hotpotqa,
  title={HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering},
  author={Yang, Zhilin and Qi, Peng and Zhang, Saizheng and Bengio, Yoshua and Cohen, William and Salakhutdinov, Ruslan and Manning, Christopher D},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  year={2018},
  organization={Association for Computational Linguistics}
}

@article{zhao2024retrieval,
  title={Retrieval-augmented generation for ai-generated content: A survey},
  author={Zhao, Penghao and Zhang, Hailin and Yu, Qinhan and Wang, Zhengren and Geng, Yunteng and Fu, Fangcheng and Yang, Ling and Zhang, Wentao and Cui, Bin},
  journal={arXiv preprint arXiv:2402.19473},
  year={2024}
}

@article{zhu2024murar,
  title={Murar: A simple and effective multimodal retrieval and answer refinement framework for multimodal question answering},
  author={Zhu, Zhengyuan and Lee, Daniel and Zhang, Hong and Harsha, Sai Sree and Feujio, Loic and Maharaj, Akash and Li, Yunyao},
  journal={arXiv preprint arXiv:2408.08521},
  year={2024}
}

