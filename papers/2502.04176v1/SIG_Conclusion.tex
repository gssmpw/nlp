\section{Conclusion}
\label{sec:conclusion}
To adapt the growing importance of generating multimodal answers, MRAMG has emerged as a critical task that aligns with real-world user demands.
To address the lack of evaluation resources for this task, we present MRAMG-Bench, a meticulously curated dataset containing 4,800 questions across diverse domains, varying difficulty levels, and rich image contexts. We further propose a comprehensive evaluation strategy, incorporating statistical and LLM-based metrics to rigorously assess both retrieval and generation performance. In addition, we introduce a general multimodal generation framework to enable models to produce interleaved text-image responses. Our evaluation of eleven generative models highlights notable limitations in handling challenging datasets and selecting the correct image order, emphasizing the necessity for deeper exploration of the MRAMG task.



