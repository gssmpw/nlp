
\section{Task Formulation}
\label{sec:task_definition}

In this section, we formally introduce the definition of the task in \textbf{Multimodal Retrieval-Augmented Multimodal Generation (MRAMG)}, focusing on constructing multimodal answers $\mathcal{A}$ based on a given text-form user query $q$ and its associated multimodal information $\mathcal{D}$. See Figure \ref{fig:task_example} for an illustration.

\textbf{Multi-Document Multimodal Information.} We consider a multi-document multimodal knowledge base as the form $\mathcal{D} = \{d_1, d_2, \ldots, d_n\}$.
For each multimodal document $d\in \mathcal{D}$, it has the interleaved form $d = \{T_1, I_1, T_2, I_2, \dots, T_l, I_l, \dots \}$, where $T_i$ denotes the $i$-th text block (i.e., a semantic text paragraph) and $I_i$ denotes the $i$-th image within the document.

\textbf{Multimodal-Retrieve and Multimodal-Generation.} Specifically, the objective is to generate a multimodal answer $\mathcal{A} = \mathcal{F}(q, \mathcal{D}^*_q, \mathcal{M})$, where $\mathcal{F}$ represents the multimodal-answer generation method (i.e, generation framework), $\mathcal{D}^*_q = \{d_{j_1}, d_{j_2}, \dots, d_{j_k}\}$ 
represents the top-$k$ documents from $\mathcal{D}$ with the highest relevance to the query $q$, and $\mathcal{M}$ denotes a certain foundation generative model, including LLMs and MLLMs.
Indeed, given the user query $q$ and the retrieved documents $\mathcal{D}_q^*$ and the generative model $\mathcal{M}$, the generation framework $\mathcal{F}$ is required to return a multimodal answer $\mathcal{A}$ by selecting appropriate images from $\mathcal{D}_q^*$ and integrating them with the generated text.
% \textbf{Interleaved Multimodal Answer.}
% Furthermore, we hope that the multimodal answer $\mathcal{A}$ is composed of text-answer blocks $\{s_1, s_2, \dots, s_n\}$ and corresponding multimodal information blocks $\{i_1, i_2, \dots, i_n\} $, i.e, $\mathcal{A}=\{s_1, i_1, s_2, i_2, \dots, s_n, i_n\}$, where each $s_j $ represents a text-based answer block (i.e., a semantic sentence), and each $i_j$ represents the corresponding multimodal information block (i.e., an image), which may be inserted or left empty. This structure enables the generation of answers that integrate both textual and multimodal content, enhancing the depth and comprehensiveness of the response.

%\begin{figure}[t]
  %  \centering
 %   \includegraphics[width=1.0\linewidth]{Fig/MRAMG_task.pdf}
 %   \caption{Illustration of MRAMG Task.}
  %  \label{fig:task_formulation}
%\end{figure}
