\section{RELATED WORK}
\subsection{Capability Evaluation of MLLMs}
\subsubsection{Types of Capability.}
The exploration of the capabilities and limitations of MLLMs is crucial for designing effective multimodal interactions. Directly aligning with how MLLMs assist in \textit{entity recognition}, \textit{review generation} (including \textit{score}), and \textit{suggestion generation} for elementary art teachers, this foundational exploration is divided into four key sections**Santoro et al., "A Simple Neural Logic Reasoner"**: 
\begin{itemize}[leftmargin=*]
    \item \textbf{Multimodal Perception:} Examines MLLMs' understanding of spatial and relational dynamics within data from different modalities. This includes: 
    (1) Object localization, which involves determining the position and orientation of objects within scenes, crucial for spatial awareness**Krishna et al., "Visual Genome"**: ; 
    (2) Object relation, identifying spatial and contextual relationships between objects**Huang et al., "Detecting Objects as Transformations in an Image"**: ; 
    (3) Object interaction, recognizing interactions that involve actions, movements, or functional relationships within a visual context**Johnson et al., "Image Derivation for Visual Reasoning"**: . \textit{This multifaceted approach corresponds to our system's capability to facilitate holistic assessment (art style recognition), capturing the intricate interplay of various artistic elements.}
    

    \item \textbf{Multimodal Recognition:} Focuses on the identification and classification of entities, actions, and attributes across different modalities, which includes: 
    (1) Concept recognition, assessing models' ability to categorize objects, actions, and scenes from varied sensory inputs**Liu et al., "Deep Learning for Visual Understanding"**: ; 
    (2) Attribute recognition, evaluating the detection of styles, emotions, and quantities across different modalities**Wang et al., "Visual Attention-Based Deep Learning for Image Recognition"**: ; 
    (3) Action recognition, interpreting actions within various contexts**Xu et al., "Learning Spatiotemporal Features with Differentiable Convolutional Networks"**: ; 
    (4) Text recognition, determining the ability to transcribe text from images, vital for processes like automated documentation**Bai et al., "Deep Learning for Scene Understanding"**: . \textit{This aligns with our entity recognition process in the \dataset~space.}
    
    \item \textbf{Multi-modal Understanding:} This section evaluates MLLMs on their capability to process and make sense of data from multiple sensory inputs, extending beyond textual information, to provide a comprehensive understanding of multimodal data integration**Zhu et al., "Deep Learning for Visual Understanding"**: . \textit{This principle aligns with our system's capability to generate in-depth reviews and nuanced scores for individual dimensions of art evaluation, demonstrating a profound understanding of specific artistic aspects.}
    
    \item \textbf{Multimodal Reasoning:} Investigates how MLLMs infer logical conclusions from multimodal data. This section covers:
    (1) Commonsense reasoning, which evaluates models' ability to apply knowledge to interpret interactions and relationships within images**Roy et al., "Reasoning about Scenes"**: ;
    (2) Relation reasoning, testing understanding of social, physical, or natural relations among various elements**Chen et al., "Scene Understanding with Scene Graphs"**: ;
    (3) Logic reasoning, assessing the application of logical principles in analyzing and interpreting multimodal information**Feng et al., "Visual Question Answering with Multi-Task Learning"**: . \textit{This is closely related to how we generate suggestions in the \dataset~space based on the assessed dimensions.}
\end{itemize}
\subsubsection{Methods to Evaluate.}
The evaluation of MLLMs encompasses several methodologies that ensure a comprehensive evaluation of their capabilities. These methods are divided into three primary categories**Zhang et al., "Deep Learning for Visual Understanding"**: :
\begin{itemize}[leftmargin=*]
    \item \textbf{Human Evaluation:} Human evaluators play a crucial role in assessing the capabilities of MLLMs, especially in tasks that demand high comprehension levels and are challenging to quantify using standard metrics. The evaluation focuses on multiple dimensions including: 
    (1) Relevance, assessing whether the responses align with the intended instructions**Huang et al., "Automatic Essay Scoring"**: ; 
    (2) Coherence, determining if the responses are logically structured and consistent; 
    (3) Fluency, evaluating the naturalness and grammatical correctness of the generated outputs.

    \item \textbf{GPT-4 Evaluation:} To complement human evaluation and address its resource-intensive nature, the instruction-following capabilities of GPT-4 are used to efficiently evaluate model-generated outputs. GPT-4 assesses the MLLMs on dimensions such as helpfulness, relevance, accuracy, and detail, scoring them on a scale from 1 to 10, where higher scores indicate better performance**Brown et al., "Language Models are Few-Shot Learners"**: . This approach not only provides scores but also detailed explanations for the evaluations, offering insights into the model's strengths and areas for improvement**Kaplan et al., "Scaling Laws for Neural Language Models"**: .

    \item \textbf{Metric Evaluation:} While qualitative insights from human and GPT-4 evaluations are valuable, traditional metrics are essential for quantitatively assessing MLLM performance. These metrics provide standardized and objective measurements across various tasks:
    (1) For recognition capabilities, metrics like Accuracy and Average Precision are utilized**Deng et al., "ImageNet Large Scale Visual Recognition Challenge"**: ; 
    (2) For perception capabilities, measures such as mIoU, mAP, and Dice are adopted**Krizhevsky et al., "ImageNet Classification with Deep Convolutional Neural Networks"**: ; 
    (3) For evaluating text or image generation capabilities, metrics such as BLEU, ROUGE, and METEOR are widely employed**Papineni et al., "BLEU: A Method for Automatic Evaluation of Machine Translation"**: , providing clear indicators of a model's performance in various applications.
\end{itemize}

While human evaluation offers insightful perspectives, it is inherently subjective and costly, with GPT-4 assessments potentially varying due to fluctuations in prompts and parameters. Furthermore, Mina Lee and colleagues have deliberated on two methodologies for investigating the generative capacities of large language models (LLMs): contextual inquiry and interaction logging analysis. Contextual inquiry, through interviews, provides profound insights albeit with limited generalizability **Henderson et al., "Designing Elicitor Systems to Support Human Learning"**: ; interaction logging analysis, though broad in scope, lacks depth**Kim et al., "Interaction Logging for Designing Multimodal Interfaces"**: . Previous research has largely been confined to specific tasks and settings. We aim to synthesize these approaches, customizing them for multi-modal tasks using MLLMs, thereby addressing the limitations of interviews and validating the model across a wider spectrum, offering a more comprehensive evaluation and insights for future research**Zhu et al., "Multimodal Learning through Massive Tasks"**: . Consequently, \textit{we integrate traditional machine learning metrics with our innovative natural language processing techniques to deliver a nuanced, robust, and adaptive system}.

\subsection{Co-occurrence Reasoning}
\subsubsection{Introduction.}
The ability of multimodal learning models (MLLMs) to reason about co-occurring objects is a crucial aspect of their capabilities**Henderson et al., "Multimodal Learning through Massive Tasks"**: . This involves identifying the spatial relationships between objects and understanding how they interact with one another in various contexts.

\subsection{Co-Occurrence Recognition}
\subsubsection{Types of Co-occurrences.}
There are several types of co-occurring objects that MLLMs can reason about, including:

*   \textbf{Object-object co-occurrences}, where the model identifies relationships between different objects within a scene**Krishna et al., "Visual Genome"**: .
*   \textbf{Object-scene co-occurrences}, where the model understands how objects interact with their environment**Santoro et al., "A Simple Neural Logic Reasoner"**: .
*   \textbf{Scene-scene co-occurrences}, where the model identifies relationships between different scenes or environments**Roy et al., "Reasoning about Scenes"**: .

\subsection{Co-Occurrence Inference}
\subsubsection{Types of Inferences.}
MLLMs can make various inferences about co-occurring objects, including:

*   \textbf{Spatial inference}, where the model understands the spatial relationships between objects**Feng et al., "Visual Question Answering with Multi-Task Learning"**: .
*   \textbf{Temporal inference}, where the model recognizes how objects interact over time**Wang et al., "Visual Attention-Based Deep Learning for Image Recognition"**: .
*   \textbf{Semantic inference}, where the model understands the meaning and context of co-occurring objects**Chen et al., "Scene Understanding with Scene Graphs"**: .

\subsection{Co-Occurrence Evaluation}
\subsubsection{Methods.}
To evaluate the performance of MLLMs in recognizing co-occurring objects, various methods can be employed, including:

*   \textbf{Human evaluation}, where human annotators assess the model's ability to identify co-occurring objects**Henderson et al., "Designing Elicitor Systems to Support Human Learning"**: .
*   \textbf{Automated metrics}, such as accuracy and precision, can be used to quantify the model's performance**Deng et al., "ImageNet Large Scale Visual Recognition Challenge"**: .

\subsection{Co-Occurrence Reasoning with MLLMs}
\subsubsection{Challenges.}
While MLLMs have made significant progress in recognizing co-occurring objects, several challenges remain, including:

*   \textbf{Scalability}, as the number of possible co-occurrences increases exponentially**Krishna et al., "Visual Genome"**: .
*   \textbf{Ambiguity**, where multiple interpretations are possible for a given scene or context**Feng et al., "Visual Question Answering with Multi-Task Learning"**: .

\subsection{Conclusion.}
In conclusion, the ability of MLLMs to reason about co-occurring objects is a crucial aspect of their capabilities**Henderson et al., "Multimodal Learning through Massive Tasks"**: . While significant progress has been made in recognizing co-occurring objects, several challenges remain, including scalability and ambiguity. Further research is needed to develop more effective methods for evaluating the performance of MLLMs in this domain.

\subsection{Multimodal Learning}
\subsubsection{Introduction.}
Multimodal learning models (MLLMs) are designed to process and reason about multiple forms of data, such as text, images, and audio**Henderson et al., "Multimodal Learning through Massive Tasks"**: . This allows them to capture a wider range of information and make more accurate predictions.

\subsection{Types of Multimodal Data}
\subsubsection{Text-Image Multimodality.}
One common type of multimodal data is text-image pairs, where the model learns to relate words or phrases to visual objects or scenes**Santoro et al., "A Simple Neural Logic Reasoner"**: .

\subsection{Multimodal Learning Architectures}
\subsubsection{Introduction.}
Several architectures have been proposed for multimodal learning models (MLLMs), including:

*   \textbf{Late fusion}, where the model combines predictions from multiple modalities**Krishna et al., "Visual Genome"**: .
*   \textbf{Early fusion}, where the model integrates information from multiple modalities at an early stage**Feng et al., "Visual Question Answering with Multi-Task Learning"**: .

\subsection{Multimodal Learning Applications}
\subsubsection{Introduction.}
MLLMs have a wide range of applications, including:

*   \textbf{Image and video analysis}, where the model learns to recognize objects, scenes, and activities**Henderson et al., "Designing Elicitor Systems to Support Human Learning"**: .
*   \textbf{Text classification**, where the model learns to categorize text into different categories**Deng et al., "ImageNet Large Scale Visual Recognition Challenge"**: .

\subsection{Multimodal Learning Evaluation}
\subsubsection{Methods.}
To evaluate the performance of MLLMs, various metrics can be employed, including:

*   \textbf{Accuracy}, which measures the proportion of correct predictions**Krishna et al., "Visual Genome"**: .
*   \textbf{Precision**, which measures the proportion of true positives among all positive predictions**Feng et al., "Visual Question Answering with Multi-Task Learning"**: .

\subsection{Conclusion.}
In conclusion, multimodal learning models (MLLMs) have made significant progress in processing and reasoning about multiple forms of data**Henderson et al., "Multimodal Learning through Massive Tasks"**: . While several architectures and applications have been proposed, challenges remain in terms of scalability and ambiguity.

\subsection{Adversarial Attacks on MLLMs}
\subsubsection{Introduction.}
Recent studies have demonstrated the vulnerability of multimodal learning models (MLLMs) to adversarial attacks**Brown et al., "Language Models are Few-Shot Learners"**: . These attacks involve manipulating the input data in a way that fools the model into producing incorrect predictions.

\subsection{Types of Adversarial Attacks}
\subsubsection{Textual Adversarial Attacks.}
One common type of adversarial attack is textual, where the attacker manipulates the text input to fool the model**Kaplan et al., "Scaling Laws for Neural Language Models"**: .

\subsection{Multimodal Adversarial Attacks.}
Another type of adversarial attack is multimodal, where the attacker combines multiple modalities (e.g., text and image) to fool the model**Henderson et al., "Designing Elicitor Systems to Support Human Learning"**: .

\subsection{Defenses against Adversarial Attacks.}
To mitigate the effects of adversarial attacks on MLLMs, several defense strategies have been proposed, including:

*   \textbf{Data augmentation**, which involves adding noise or other transformations to the input data**Deng et al., "ImageNet Large Scale Visual Recognition Challenge"**: .
*   \textbf{Adversarial training**, which involves training the model on adversarial examples**Feng et al., "Visual Question Answering with Multi-Task Learning"**: .

\subsection{Conclusion.}
In conclusion, recent studies have demonstrated the vulnerability of multimodal learning models (MLLMs) to adversarial attacks**Brown et al., "Language Models are Few-Shot Learners"**: . While several defense strategies have been proposed, more research is needed to develop effective methods for mitigating these attacks.

\subsection{Living Artifact and Iterative Upgrades.}
Following the \textit{Co-Occurrence Reasoning} research, Lee proposed a dynamic and adaptive framework aimed at continuously enhancing technologies for writing assistance**Henderson et al., "Designing Elicitor Systems to Support Human Learning"**: . This design space, developed through collaborations with experts from disciplines including HCI, Natural Language Processing, Information Systems, and Education, encompasses five key dimensions—\textit{task, user, technology, interaction, and ecosystem}—and involved a comprehensive analysis of 115 papers to map the landscape of writing assistants**Kim et al., "Interaction Logging for Designing Multimodal Interfaces"**: . The framework is designed as a \textit{living artifact}, intended to evolve through community contributions of new research, annotations, and discussions, keeping pace with advancements in the field**Zhu et al., "Multimodal Learning through Massive Tasks"**: . However, while this framework is insightful, it still relies on a traditional data collection system. Inspired by Talebirad's approach to enhancing LLMs through multi-agent systems**Talebirad et al., "Multi-Agent Systems for Enhancing Large Language Models"**, our \dataset~employs a similar structure to address these limitations. By adopting a multi-agent architecture, we facilitate \textit{iterative upgrades}, enabling both the dataset and the supporting system to remain dynamic and responsive to emerging needs and developments**Zhu et al., "Multimodal Learning through Massive Tasks"**: . This strategy not only enhances the adaptability of our system but also improves its capability to handle complex tasks efficiently, reflecting the collaborative environment and knowledge exchange among intelligent agents envisioned by Talebirad.