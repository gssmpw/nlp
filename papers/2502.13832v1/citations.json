[
  {
    "index": 0,
    "papers": [
      {
        "key": "huang2024survey",
        "author": "Huang, Jiaxing and Zhang, Jingyi",
        "title": "A Survey on Evaluation of Multimodal Large Language Models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "chen2024gmai",
        "author": "Chen, Pengcheng and Ye, Jin and Wang, Guoan and Li, Yanjun and Deng, Zhongying and Li, Wei and Li, Tianbin and Duan, Haodong and Huang, Ziyan and Su, Yanzhou and others",
        "title": "GMAI-MMBench: A Comprehensive Multimodal Evaluation Benchmark Towards General Medical AI"
      },
      {
        "key": "yu2023mm",
        "author": "Yu, Weihao and Yang, Zhengyuan and Li, Linjie and Wang, Jianfeng and Lin, Kevin and Liu, Zicheng and Wang, Xinchao and Wang, Lijuan",
        "title": "Mm-vet: Evaluating large multimodal models for integrated capabilities"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "liu2023mmbench",
        "author": "Liu, Yuan and Duan, Haodong and Zhang, Yuanhan and Li, Bo and Zhang, Songyang and Zhao, Wangbo and Yuan, Yike and Wang, Jiaqi and He, Conghui and Liu, Ziwei and others",
        "title": "MMBench: Is Your Multi-modal Model an All-around Player?"
      },
      {
        "key": "bai2023qwen",
        "author": "Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren",
        "title": "Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "chen2024plug",
        "author": "Chen, Jiaxing and Liu, Yuxuan and Li, Dehu and An, Xiang and Feng, Ziyong and Zhao, Yongle and Xie, Yin",
        "title": "Plug-and-play grounding of reasoning in multimodal large language models"
      },
      {
        "key": "zhao2023vlchecklist",
        "author": "Tiancheng Zhao and Tianqi Zhang and Mingwei Zhu and Haozhan Shen and Kyusong Lee and Xiaopeng Lu and Jianwei Yin",
        "title": "VL-CheckList: Evaluating Pre-trained Vision-Language Models with Objects, Attributes and Relations"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "liu2023mmbench",
        "author": "Liu, Yuan and Duan, Haodong and Zhang, Yuanhan and Li, Bo and Zhang, Songyang and Zhao, Wangbo and Yuan, Yike and Wang, Jiaqi and He, Conghui and Liu, Ziwei and others",
        "title": "MMBench: Is Your Multi-modal Model an All-around Player?"
      },
      {
        "key": "li2023seed",
        "author": "Bohao Li and Yuying Ge and Yixiao Ge and Guangzhi Wang and Rui Wang and Ruimao Zhang and Ying Shan",
        "title": "SEED-Bench: Benchmarking Multimodal Large Language Models"
      },
      {
        "key": "yu2023mm",
        "author": "Yu, Weihao and Yang, Zhengyuan and Li, Linjie and Wang, Jianfeng and Lin, Kevin and Liu, Zicheng and Wang, Xinchao and Wang, Lijuan",
        "title": "Mm-vet: Evaluating large multimodal models for integrated capabilities"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "liu2023mmbench",
        "author": "Liu, Yuan and Duan, Haodong and Zhang, Yuanhan and Li, Bo and Zhang, Songyang and Zhao, Wangbo and Yuan, Yike and Wang, Jiaqi and He, Conghui and Liu, Ziwei and others",
        "title": "MMBench: Is Your Multi-modal Model an All-around Player?"
      },
      {
        "key": "awadalla2023openflamingo",
        "author": "Anas Awadalla and Irena Gao and Josh Gardner and Jack Hessel and Yusuf Hanafy and Wanrong Zhu and Kalyani Marathe and Yonatan Bitton and Samir Gadre and Shiori Sagawa and Jenia Jitsev and Simon Kornblith and Pang Wei Koh and Gabriel Ilharco and Mitchell Wortsman and Ludwig Schmidt",
        "title": "OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "liu2023mmbench",
        "author": "Liu, Yuan and Duan, Haodong and Zhang, Yuanhan and Li, Bo and Zhang, Songyang and Zhao, Wangbo and Yuan, Yike and Wang, Jiaqi and He, Conghui and Liu, Ziwei and others",
        "title": "MMBench: Is Your Multi-modal Model an All-around Player?"
      },
      {
        "key": "dai2023instructblip",
        "author": "Wenliang Dai and Junnan Li and Dongxu Li and Anthony Meng Huat Tiong and Junqi Zhao and Weisheng Wang and Boyang Li and Pascale Fung and Steven Hoi",
        "title": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "liu2023mmbench",
        "author": "Liu, Yuan and Duan, Haodong and Zhang, Yuanhan and Li, Bo and Zhang, Songyang and Zhao, Wangbo and Yuan, Yike and Wang, Jiaqi and He, Conghui and Liu, Ziwei and others",
        "title": "MMBench: Is Your Multi-modal Model an All-around Player?"
      },
      {
        "key": "achiam2023gpt",
        "author": "Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others",
        "title": "Gpt-4 technical report"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "huang2024survey",
        "author": "Huang, Jiaxing and Zhang, Jingyi",
        "title": "A Survey on Evaluation of Multimodal Large Language Models"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "lin2024draw",
        "author": "Lin, Weifeng and Wei, Xinyu and An, Ruichuan and Gao, Peng and Zou, Bocheng and Luo, Yulin and Huang, Siyuan and Zhang, Shanghang and Li, Hongsheng",
        "title": "Draw-and-Understand: Leveraging Visual Prompts to Enable MLLMs to Comprehend What You Want"
      },
      {
        "key": "yuan2024ospreypixelunderstandingvisual",
        "author": "Yuqian Yuan and Wentong Li and Jian Liu and Dongqi Tang and Xinjie Luo and Chi Qin and Lei Zhang and Jianke Zhu",
        "title": "Osprey: Pixel Understanding with Visual Instruction Tuning"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "liu2024ii",
        "author": "Liu, Ziqiang and Fang, Feiteng and Feng, Xi and Du, Xinrun and Zhang, Chenhao and Wang, Zekun and Bai, Yuelin and Zhao, Qixuan and Fan, Liyang and Gan, Chengguang and others",
        "title": "II-Bench: An Image Implication Understanding Benchmark for Multimodal Large Language Models"
      },
      {
        "key": "reid2024gemini",
        "author": "Reid, Machel and Savinov, Nikolay and Teplyashin, Denis and Lepikhin, Dmitry and Lillicrap, Timothy and Alayrac, Jean-baptiste and Soricut, Radu and Lazaridou, Angeliki and Firat, Orhan and Schrittwieser, Julian and others",
        "title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "liu2023mmbench",
        "author": "Liu, Yuan and Duan, Haodong and Zhang, Yuanhan and Li, Bo and Zhang, Songyang and Zhao, Wangbo and Yuan, Yike and Wang, Jiaqi and He, Conghui and Liu, Ziwei and others",
        "title": "MMBench: Is Your Multi-modal Model an All-around Player?"
      },
      {
        "key": "bai2023touchstone",
        "author": "Bai, Shuai and Yang, Shusheng and Bai, Jinze and Wang, Peng and Zhang, Xingxuan and Lin, Junyang and Wang, Xinggang and Zhou, Chang and Zhou, Jingren",
        "title": "Touchstone: Evaluating vision-language models by language models"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "huang2024survey",
        "author": "Huang, Jiaxing and Zhang, Jingyi",
        "title": "A Survey on Evaluation of Multimodal Large Language Models"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "liu2023llava",
        "author": "Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae",
        "title": "Visual instruction tuning"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "liu2023llava",
        "author": "Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae",
        "title": "Visual instruction tuning"
      },
      {
        "key": "achiam2023gpt",
        "author": "Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others",
        "title": "Gpt-4 technical report"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "li2023seed",
        "author": "Bohao Li and Yuying Ge and Yixiao Ge and Guangzhi Wang and Rui Wang and Ruimao Zhang and Ying Shan",
        "title": "SEED-Bench: Benchmarking Multimodal Large Language Models"
      },
      {
        "key": "li2023llava",
        "author": "Li, Chunyuan and Wong, Cliff and Zhang, Sheng and Usuyama, Naoto and Liu, Haotian and Yang, Jianwei and Naumann, Tristan and Poon, Hoifung and Gao, Jianfeng",
        "title": "Llava-med: Training a large language-and-vision assistant for biomedicine in one day"
      },
      {
        "key": "lau2018dataset",
        "author": "Lau, Jason J and Gayen, Soumya and Ben Abacha, Asma and Demner-Fushman, Dina",
        "title": "A dataset of clinically generated visual questions and answers about radiology images"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "dai2017scannet",
        "author": "Dai, Angela and Chang, Angel X and Savva, Manolis and Halber, Maciej and Funkhouser, Thomas and Nie{\\ss}ner, Matthias",
        "title": "Scannet: Richly-annotated 3d reconstructions of indoor scenes"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "kim2019audiocaps",
        "author": "Kim, Chris Dongjoo and Kim, Byeongchang and Lee, Hyunmin and Kim, Gunhee",
        "title": "Audiocaps: Generating captions for audios in the wild"
      },
      {
        "key": "chen2015microsoft",
        "author": "Chen, Xinlei and Fang, Hao and Lin, Tsung-Yi and Vedantam, Ramakrishna and Gupta, Saurabh and Doll{\\'a}r, Piotr and Zitnick, C Lawrence",
        "title": "Microsoft coco captions: Data collection and evaluation server"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "lee2022coauthor",
        "author": "Lee, Mina and Liang, Percy and Yang, Qian",
        "title": "Coauthor: Designing a human-ai collaborative writing dataset for exploring language model capabilities"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "vahdat2015learning",
        "author": "Vahdat, Mehrnoosh and Oneto, Luca and Anguita, Davide and Funk, Mathias and Rauterberg, Matthias",
        "title": "A learning analytics approach to correlate the academic achievements of students with interaction data from an educational simulator"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "pechenizkiy2009process",
        "author": "Pechenizkiy, Mykola and Trcka, Nikola and Vasilyeva, Ekaterina and van der Aalst, Wil MP and De Bra, PME",
        "title": "Process mining online assessment data"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "trcka2009local",
        "author": "Trcka, Nikola and Pechenizkiy, Mykola",
        "title": "From local patterns to global models: Towards domain driven educational process mining"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "vahdat2015learning",
        "author": "Vahdat, Mehrnoosh and Oneto, Luca and Anguita, Davide and Funk, Mathias and Rauterberg, Matthias",
        "title": "A learning analytics approach to correlate the academic achievements of students with interaction data from an educational simulator"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "cropley2022automated",
        "author": "Cropley, David H and Marrone, Rebecca L",
        "title": "Automated scoring of figural creativity using a convolutional neural network."
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "patterson2024audra",
        "author": "Patterson, John D and Barbot, Baptiste and Lloyd-Cox, James and Beaty, Roger E",
        "title": "AuDrA: An automated drawing assessment platform for evaluating creativity"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "lee2022coauthor",
        "author": "Lee, Mina and Liang, Percy and Yang, Qian",
        "title": "Coauthor: Designing a human-ai collaborative writing dataset for exploring language model capabilities"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "lee2024design",
        "author": "Lee, Mina and Gero, Katy Ilonka and Chung, John Joon Young and Shum, Simon Buckingham and Raheja, Vipul and Shen, Hua and Venugopalan, Subhashini and Wambsganss, Thiemo and Zhou, David and Alghamdi, Emad A and others",
        "title": "A Design Space for Intelligent and Interactive Writing Assistants"
      }
    ]
  },
  {
    "index": 27,
    "papers": [
      {
        "key": "talebirad2023multi",
        "author": "Talebirad, Yashar and Nadiri, Amirhossein",
        "title": "Multi-agent collaboration: Harnessing the power of intelligent llm agents"
      }
    ]
  }
]