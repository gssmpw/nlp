@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{awadalla2023openflamingo,
  title={OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models},
  author={Anas Awadalla and Irena Gao and Josh Gardner and Jack Hessel and Yusuf Hanafy and Wanrong Zhu and Kalyani Marathe and Yonatan Bitton and Samir Gadre and Shiori Sagawa and Jenia Jitsev and Simon Kornblith and Pang Wei Koh and Gabriel Ilharco and Mitchell Wortsman and Ludwig Schmidt},
  journal={arXiv preprint arXiv:2308.01390},
  year={2023}
}

@article{bai2023qwen,
  title={Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond},
  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2308.12966},
  volume={3},
  number={1},
  year={2023}
}

@article{bai2023touchstone,
  title={Touchstone: Evaluating vision-language models by language models},
  author={Bai, Shuai and Yang, Shusheng and Bai, Jinze and Wang, Peng and Zhang, Xingxuan and Lin, Junyang and Wang, Xinggang and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2308.16890},
  year={2023}
}

@article{chen2015microsoft,
 title={Microsoft coco captions: Data collection and evaluation server},
 author={Chen, Xinlei and Fang, Hao and Lin, Tsung-Yi and Vedantam, Ramakrishna and Gupta, Saurabh and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
 journal={arXiv preprint arXiv:1504.00325},
 year={2015}
}

@article{chen2024gmai,
  title={GMAI-MMBench: A Comprehensive Multimodal Evaluation Benchmark Towards General Medical AI},
  author={Chen, Pengcheng and Ye, Jin and Wang, Guoan and Li, Yanjun and Deng, Zhongying and Li, Wei and Li, Tianbin and Duan, Haodong and Huang, Ziyan and Su, Yanzhou and others},
  journal={arXiv preprint arXiv:2408.03361},
  year={2024}
}

@article{chen2024plug,
  title={Plug-and-play grounding of reasoning in multimodal large language models},
  author={Chen, Jiaxing and Liu, Yuxuan and Li, Dehu and An, Xiang and Feng, Ziyong and Zhao, Yongle and Xie, Yin},
  journal={arXiv preprint arXiv:2403.19322},
  year={2024}
}

@article{cropley2022automated,
  title={Automated scoring of figural creativity using a convolutional neural network.},
  author={Cropley, David H and Marrone, Rebecca L},
  journal={Psychology of Aesthetics, Creativity, and the Arts},
  year={2022},
  publisher={Educational Publishing Foundation}
}

@inproceedings{dai2017scannet,
 title={Scannet: Richly-annotated 3d reconstructions of indoor scenes},
 author={Dai, Angela and Chang, Angel X and Savva, Manolis and Halber, Maciej and Funkhouser, Thomas and Nie{\ss}ner, Matthias},
 booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
 pages={5828--5839},
 year={2017}
}

@misc{dai2023instructblip,
      title={InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning}, 
      author={Wenliang Dai and Junnan Li and Dongxu Li and Anthony Meng Huat Tiong and Junqi Zhao and Weisheng Wang and Boyang Li and Pascale Fung and Steven Hoi},
      year={2023},
      eprint={2305.06500},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{huang2024survey,
  title={A Survey on Evaluation of Multimodal Large Language Models},
  author={Huang, Jiaxing and Zhang, Jingyi},
  journal={arXiv preprint arXiv:2408.15769},
  year={2024}
}

@inproceedings{kim2019audiocaps,
 title={Audiocaps: Generating captions for audios in the wild},
 author={Kim, Chris Dongjoo and Kim, Byeongchang and Lee, Hyunmin and Kim, Gunhee},
 booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
 pages={119--132},
 year={2019}
}

@article{lau2018dataset,
 title={A dataset of clinically generated visual questions and answers about radiology images},
 author={Lau, Jason J and Gayen, Soumya and Ben Abacha, Asma and Demner-Fushman, Dina},
 journal={Scientific data},
 volume={5},
 number={1},
 pages={1--10},
 year={2018},
 publisher={Nature Publishing Group}
}

@inproceedings{lee2022coauthor,
  title={Coauthor: Designing a human-ai collaborative writing dataset for exploring language model capabilities},
  author={Lee, Mina and Liang, Percy and Yang, Qian},
  booktitle={Proceedings of the 2022 CHI conference on human factors in computing systems},
  pages={1--19},
  year={2022}
}

@inproceedings{lee2024design,
  title={A Design Space for Intelligent and Interactive Writing Assistants},
  author={Lee, Mina and Gero, Katy Ilonka and Chung, John Joon Young and Shum, Simon Buckingham and Raheja, Vipul and Shen, Hua and Venugopalan, Subhashini and Wambsganss, Thiemo and Zhou, David and Alghamdi, Emad A and others},
  booktitle={Proceedings of the CHI Conference on Human Factors in Computing Systems},
  pages={1--35},
  year={2024}
}

@article{li2023llava,
 title={Llava-med: Training a large language-and-vision assistant for biomedicine in one day},
 author={Li, Chunyuan and Wong, Cliff and Zhang, Sheng and Usuyama, Naoto and Liu, Haotian and Yang, Jianwei and Naumann, Tristan and Poon, Hoifung and Gao, Jianfeng},
 journal={arXiv preprint arXiv:2306.00890},
 year={2023}
}

@article{li2023seed,
      title={SEED-Bench: Benchmarking Multimodal Large Language Models}, 
      author={Bohao Li and Yuying Ge and Yixiao Ge and Guangzhi Wang and Rui Wang and Ruimao Zhang and Ying Shan},
      year={2023},
      eprint={2307.16125},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{lin2024draw,
  title={Draw-and-Understand: Leveraging Visual Prompts to Enable MLLMs to Comprehend What You Want},
  author={Lin, Weifeng and Wei, Xinyu and An, Ruichuan and Gao, Peng and Zou, Bocheng and Luo, Yulin and Huang, Siyuan and Zhang, Shanghang and Li, Hongsheng},
  journal={arXiv preprint arXiv:2403.20271},
  year={2024}
}

@article{liu2023llava,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={arXiv preprint arXiv:2304.08485},
  year={2023}
}

@article{liu2023mmbench,
  title={MMBench: Is Your Multi-modal Model an All-around Player?},
  author={Liu, Yuan and Duan, Haodong and Zhang, Yuanhan and Li, Bo and Zhang, Songyang and Zhao, Wangbo and Yuan, Yike and Wang, Jiaqi and He, Conghui and Liu, Ziwei and others},
  journal={arXiv preprint arXiv:2307.06281},
  year={2023}
}

@article{liu2024ii,
  title={II-Bench: An Image Implication Understanding Benchmark for Multimodal Large Language Models},
  author={Liu, Ziqiang and Fang, Feiteng and Feng, Xi and Du, Xinrun and Zhang, Chenhao and Wang, Zekun and Bai, Yuelin and Zhao, Qixuan and Fan, Liyang and Gan, Chengguang and others},
  journal={arXiv preprint arXiv:2406.05862},
  year={2024}
}

@article{patterson2024audra,
  title={AuDrA: An automated drawing assessment platform for evaluating creativity},
  author={Patterson, John D and Barbot, Baptiste and Lloyd-Cox, James and Beaty, Roger E},
  journal={Behavior Research Methods},
  volume={56},
  number={4},
  pages={3619--3636},
  year={2024},
  publisher={Springer}
}

@inproceedings{pechenizkiy2009process,
  title={Process mining online assessment data},
  author={Pechenizkiy, Mykola and Trcka, Nikola and Vasilyeva, Ekaterina and van der Aalst, Wil MP and De Bra, PME},
  booktitle={Educational Data Mining 2009: 2nd International Conference on Educational Data Mining: proceedings [EDM'09], Cordoba, Spain. July 1-3, 2009},
  pages={279--288},
  year={2009},
  organization={International Working Group on Educational Data Mining}
}

@article{reid2024gemini,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={Reid, Machel and Savinov, Nikolay and Teplyashin, Denis and Lepikhin, Dmitry and Lillicrap, Timothy and Alayrac, Jean-baptiste and Soricut, Radu and Lazaridou, Angeliki and Firat, Orhan and Schrittwieser, Julian and others},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024}
}

@article{talebirad2023multi,
  title={Multi-agent collaboration: Harnessing the power of intelligent llm agents},
  author={Talebirad, Yashar and Nadiri, Amirhossein},
  journal={arXiv preprint arXiv:2306.03314},
  year={2023}
}

@inproceedings{trcka2009local,
  title={From local patterns to global models: Towards domain driven educational process mining},
  author={Trcka, Nikola and Pechenizkiy, Mykola},
  booktitle={2009 Ninth international conference on intelligent systems design and applications},
  pages={1114--1119},
  year={2009},
  organization={IEEE}
}

@inproceedings{vahdat2015learning,
  title={A learning analytics approach to correlate the academic achievements of students with interaction data from an educational simulator},
  author={Vahdat, Mehrnoosh and Oneto, Luca and Anguita, Davide and Funk, Mathias and Rauterberg, Matthias},
  booktitle={Design for Teaching and Learning in a Networked World: 10th European Conference on Technology Enhanced Learning, EC-TEL 2015, Toledo, Spain, September 15-18, 2015, Proceedings 10},
  pages={352--366},
  year={2015},
  organization={Springer}
}

@article{yu2023mm,
  title={Mm-vet: Evaluating large multimodal models for integrated capabilities},
  author={Yu, Weihao and Yang, Zhengyuan and Li, Linjie and Wang, Jianfeng and Lin, Kevin and Liu, Zicheng and Wang, Xinchao and Wang, Lijuan},
  journal={arXiv preprint arXiv:2308.02490},
  year={2023}
}

@misc{yuan2024ospreypixelunderstandingvisual,
      title={Osprey: Pixel Understanding with Visual Instruction Tuning}, 
      author={Yuqian Yuan and Wentong Li and Jian Liu and Dongqi Tang and Xinjie Luo and Chi Qin and Lei Zhang and Jianke Zhu},
      year={2024},
      eprint={2312.10032},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2312.10032}, 
}

@misc{zhao2023vlchecklist,
      title={VL-CheckList: Evaluating Pre-trained Vision-Language Models with Objects, Attributes and Relations}, 
      author={Tiancheng Zhao and Tianqi Zhang and Mingwei Zhu and Haozhan Shen and Kyusong Lee and Xiaopeng Lu and Jianwei Yin},
      year={2023},
      eprint={2207.00221},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2207.00221}, 
}

