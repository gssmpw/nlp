\section{RELATED WORK}
\subsection{Capability Evaluation of MLLMs}
\subsubsection{Types of Capability.}
The exploration of the capabilities and limitations of MLLMs is crucial for designing effective multimodal interactions. Directly aligning with how MLLMs assist in \textit{entity recognition}, \textit{review generation} (including \textit{score}), and \textit{suggestion generation} for elementary art teachers, this foundational exploration is divided into four key sections~\cite{huang2024survey}:
\begin{itemize}[leftmargin=*]
    \item \textbf{Multimodal Perception:} Examines MLLMs' understanding of spatial and relational dynamics within data from different modalities. This includes: 
    (1) Object localization, which involves determining the position and orientation of objects within scenes, crucial for spatial awareness~\cite{chen2024gmai,yu2023mm}; 
    (2) Object relation, identifying spatial and contextual relationships between objects~\cite{liu2023mmbench,bai2023qwen}; 
    (3) Object interaction, recognizing interactions that involve actions, movements, or functional relationships within a visual context~\cite{chen2024plug,zhao2023vlchecklist}. \textit{This multifaceted approach corresponds to our system's capability to facilitate holistic assessment (art style recognition), capturing the intricate interplay of various artistic elements.}
    

    \item \textbf{Multimodal Recognition:} Focuses on the identification and classification of entities, actions, and attributes across different modalities, which includes: 
    (1) Concept recognition, assessing models' ability to categorize objects, actions, and scenes from varied sensory inputs~\cite{liu2023mmbench,li2023seed,yu2023mm}; 
    (2) Attribute recognition, evaluating the detection of styles, emotions, and quantities across different modalities~\cite{liu2023mmbench,awadalla2023openflamingo}; 
    (3) Action recognition, interpreting actions within various contexts~\cite{liu2023mmbench,dai2023instructblip}; 
    (4) Text recognition, determining the ability to transcribe text from images, vital for processes like automated documentation~\cite{liu2023mmbench,achiam2023gpt}. \textit{This aligns with our entity recognition process in the \dataset~space.}
    
    \item \textbf{Multi-modal Understanding:} This section evaluates MLLMs on their capability to process and make sense of data from multiple sensory inputs, extending beyond textual information, to provide a comprehensive understanding of multimodal data integration~\cite{huang2024survey}. \textit{This principle aligns with our system's capability to generate in-depth reviews and nuanced scores for individual dimensions of art evaluation, demonstrating a profound understanding of specific artistic aspects.}
    
    \item \textbf{Multimodal Reasoning:} Investigates how MLLMs infer logical conclusions from multimodal data. This section covers:
    (1) Commonsense reasoning, which evaluates models' ability to apply knowledge to interpret interactions and relationships within images~\cite{lin2024draw,yuan2024ospreypixelunderstandingvisual};
    (2) Relation reasoning, testing understanding of social, physical, or natural relations among various elements~\cite{liu2024ii,reid2024gemini};
    (3) Logic reasoning, assessing the application of logical principles in analyzing and interpreting multimodal information~\cite{liu2023mmbench,bai2023touchstone}. \textit{This is closely related to how we generate suggestions in the \dataset~space based on the assessed dimensions.}
\end{itemize}
\subsubsection{Methods to Evaluate.}
The evaluation of MLLMs encompasses several methodologies that ensure a comprehensive evaluation of their capabilities. These methods are divided into three primary categories~\cite{huang2024survey}:
\begin{itemize}[leftmargin=*]
    \item \textbf{Human Evaluation:} Human evaluators play a crucial role in assessing the capabilities of MLLMs, especially in tasks that demand high comprehension levels and are challenging to quantify using standard metrics. The evaluation focuses on multiple dimensions including: 
    (1) Relevance, assessing whether the responses align with the intended instructions~\cite{liu2023llava}; 
    (2) Coherence, determining if the responses are logically structured and consistent; 
    (3) Fluency, evaluating the naturalness and grammatical correctness of the generated outputs.

    \item \textbf{GPT-4 Evaluation:} To complement human evaluation and address its resource-intensive nature, the instruction-following capabilities of GPT-4 are used to efficiently evaluate model-generated outputs. GPT-4 assesses the MLLMs on dimensions such as helpfulness, relevance, accuracy, and detail, scoring them on a scale from 1 to 10, where higher scores indicate better performance. This approach not only provides scores but also detailed explanations for the evaluations, offering insights into the model's strengths and areas for improvement~\cite{liu2023llava, achiam2023gpt}.

    \item \textbf{Metric Evaluation:} While qualitative insights from human and GPT-4 evaluations are valuable, traditional metrics are essential for quantitatively assessing MLLM performance. These metrics provide standardized and objective measurements across various tasks:
    (1) For recognition capabilities, metrics like Accuracy and Average Precision are utilized~\cite{li2023seed, li2023llava, lau2018dataset}; 
    (2) For perception capabilities, measures such as mIoU, mAP, and Dice are adopted~\cite{dai2017scannet}; 
    (3) For evaluating text or image generation capabilities, metrics such as BLEU, ROUGE, and METEOR are widely employed~\cite{kim2019audiocaps, chen2015microsoft}, providing clear indicators of a model's performance in various applications.
\end{itemize}

While human evaluation offers insightful perspectives, it is inherently subjective and costly, with GPT-4 assessments potentially varying due to fluctuations in prompts and parameters. Furthermore, Mina Lee and colleagues have deliberated on two methodologies for investigating the generative capacities of large language models (LLMs): contextual inquiry and interaction logging analysis. Contextual inquiry, through interviews, provides profound insights albeit with limited generalizability \cite{lee2022coauthor}; interaction logging analysis, though broad in scope, lacks depth. Previous research has largely been confined to specific tasks and settings. We aim to synthesize these approaches, customizing them for multi-modal tasks using MLLMs, thereby addressing the limitations of interviews and validating the model across a wider spectrum, offering a more comprehensive evaluation and insights for future research. Consequently, \textit{we integrate traditional machine learning metrics with our innovative natural language processing techniques to deliver a nuanced, robust, and reliable assessment of MLLMs.}
\subsection{Process-oriented HCI Datasets in Education}
\subsubsection{The Growing Focus on Educational Process Mining in HCI} In the educational domain, the process of learning is often considered more critical and analytically valuable than the final outcomes \cite{vahdat2015learning}. Capturing this process, however, poses significant challenges due to the complexity of documenting and analyzing process-oriented data, which tends to be sparse and less frequently analyzed. Common applications of process mining techniques have been demonstrated in online assessment data to analyze the processes involved in answering questions or requesting student feedback \cite{pechenizkiy2009process}. Additionally, frameworks integrating educational process data mining have been introduced to facilitate the handling of interactive process data and assist educators in analyzing educational processes based on formal modeling \cite{trcka2009local}. Despite these advancements, the collection and analysis of such data remain labor-intensive and inherently complex. The complexity of process-oriented data in educational settings correlates strongly with concepts such as simplicity, ease of use, uncertainty, and the context of application, making it a focal point for HCI designers \cite{vahdat2015learning}. These challenges underscore the need for innovative approaches in HCI to enhance the usability and effectiveness of process mining tools in educational environments. \textit{Consequently, the significance and ongoing challenges of Educational Process Mining in HCI have garnered increasing attention, highlighting the urgency for developing more efficient and accessible tools.}
\subsubsection{Emerging Trends in Process-Oriented Artwork Evaluation}
In the field of artwork evaluation, process-oriented approaches are beginning to take shape. Currently, there are very few widely validated methods for automated process-oriented visual arts assessment. In fact, result-oriented methods are also scarce; one of the few examples is the Torrance Tests of Creative Thinking—Drawing Task, which scores creativity using artificial neural networks \cite{cropley2022automated}. One emerging process-oriented method involves providing an initial artwork that allows students to further develop the piece by adding patterns. The evaluation is then conducted based on both the initial and final artworks \cite{patterson2024audra}. This method attempts to document the creative process of students' artwork creation as much as possible, but recording the entire creative process remains a significant challenge. This raises an important question: \textit{Can the process of artwork evaluation under AI assistance be effectively documented?} Through the design of \dataset, we aim to facilitate interaction between educators and the system, deepening their understanding of students' artworks, thereby advancing the development of HCI in the domain of art education.

\subsection{Spaces for Iterative Multi-Agent Upgrades}
\subsubsection{Fractionalization and Dominance.}
The \textit{CoAuthor} study has significantly contributed to the HCI community by highlighting the generative capabilities of large language models  (LLMs) in creative and argumentative writing contexts \cite{lee2022coauthor}. While \textit{CoAuthor} effectively advocates for the curation and analysis of large interaction datasets to make these capabilities more transparent and accessible, it does not segment the creative process into distinct phases that could provide deeper, context-specific insights. Building on these viewpoints, we propose that applying a similar phased approach to the art evaluation process—identifying distinct stages like conception, development, and presentation—could refine our analyses even further. This approach, known as \textit{fractionalization}, involves dividing a complex process into manageable segments, with each segment handled by a dedicated agent. \textit{Dominance} in this context refers to the strategic control and optimization of each segment by its respective agent, ensuring that the overall system maintains coherence and maximizes efficiency. By implementing a system based on these principles, not only is the granularity of the obtained insights improved, but the overall effectiveness and adaptability of the system within dynamic HCI environments are also enhanced. Similarly, when applied to the \textit{Evaluation of Artworks}, this structured approach allows specialized agents to precisely manage different stages of artistic creation and interpretation, thereby enhancing the precision and depth of art evaluations. Consequently, our \dataset~is structured around multi-agent concepts based on fractionalization and dominance principles.
\subsubsection{Living Artifact and Iterative Upgrades.}
Following the \textit{CoAuthor} research, Lee proposed a dynamic and adaptive framework aimed at continuously enhancing technologies for writing assistance \cite{lee2024design}. This design space, developed through collaborations with experts from disciplines including HCI, Natural Language Processing, Information Systems, and Education, encompasses five key dimensions—\textit{task, user, technology, interaction, and ecosystem}—and involved a comprehensive analysis of 115 papers to map the landscape of writing assistants. The framework is designed as a \textit{living artifact}, intended to evolve through community contributions of new research, annotations, and discussions, keeping pace with advancements in the field. However, while this framework is insightful, it still relies on a traditional data collection system. Inspired by Talebirad's approach to enhancing LLMs through multi-agent systems \cite{talebirad2023multi}, our \dataset~employs a similar structure to address these limitations. By adopting a multi-agent architecture, we facilitate \textit{iterative upgrades}, enabling both the dataset and the supporting system to remain dynamic and responsive to emerging needs and developments. This strategy not only enhances the adaptability of our system but also improves its capability to handle complex tasks efficiently, reflecting the collaborative environment and knowledge exchange among intelligent agents envisioned by Talebirad.