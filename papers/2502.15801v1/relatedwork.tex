\section{Related Work}
\label{appendix0}




\paragraph{Transformer circuit interpretation.}
Mechanistic interpretability of transformers began with analysis of simplified models, identifying attention heads as modular components that implement specific functions. In their seminal work, \citet{Elhage2021-vq} and \citet{Olsson2022-hk} introduced "induction heads" as critical components for in-context learning in small attention-only models. These heads perform pattern completion by attending to prior token sequences, forming the basis for later work on compositional generalization. Case studies have dissected transformer circuits for specific functions, such as the 'greater than' circuit \citep{Hanna2023-zy}, the 'docstring' circuit \citep{Heimersheim2023-ka}, the 'indirect object' circuit \citep{Wang2024-vp}, and the 'max of list' circuit \citep{Hofstatter2023-rv}. These case studies successfully reverse-engineered the transformer into the minimal-algorithm responsible for the target behavior.

To facilitate identification of relevant circuits, researchers have proposed circuit discovery methods such as logit lens \citep{nostalgebraist2020-du}, path patching \citep{Goldowsky-Dill2023-zr}, causal scrubbing \cite{LawrenceC2022-zy}.   For large-scale transformers, automated circuit discovery methods are also proposed \citep{Conmy2023-ej, Hsu2024-xo, Bhaskar2024-mn}. So far, transformer interpretability work still requires extensive human efforts in the loop for hypothesis generation and testing. We point to a review paper for a more comprehensive review \citep{Rai2024-yg}.

\paragraph{Compositional generalization in transformers.} In their study, \citet{Hupkes2019-qi} evaluated compositional generalization ability on different families of models, and found that transformer outperformed RNN and ConvNet in systematic generalization, i.e., recombination of known elements, but still uncomparable to human performance.  \citet{Zhang2024-yl} pointed out that transformers struggle with composing recursive structures. Recently, \citet{Lake2023-cp} showed that after being pre-trained with data generated by a 'meta-grammar', small transformers (less than 1 million parameters) can exhibit human-like compositional ability in novel in-context learning cases. This is in line with the success of commercial large language models (LLM) in solving complex out-of-distribution reasoning tasks \citep{Bubeck2023-ho,DeepSeek-AI2024-bs}, where compositional genralization is necessary. 

Several studies highlighted factors that facilitate transformer's compositional ability. \citet{Wang2024-mi} identified initialization scales as a critical factor in determining whether models rely on memorization or rule-based reasoning for compositional tasks. \citet{Zhang2025-id} revealed that low-complexity circuits enable out-of-distribution generalization by condensing primitive-level rules. \citep{Sanford2024-ye} identified logarithmic depth as a key constraint for transformers to emulate computations within a sequence. Here, we offer a complementary mechanistic understanding of how trasnformers perform compositional computations.