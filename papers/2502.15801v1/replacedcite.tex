\section{Related Work}
\label{appendix0}




\paragraph{Transformer circuit interpretation.}
Mechanistic interpretability of transformers began with analysis of simplified models, identifying attention heads as modular components that implement specific functions. In their seminal work, ____ and ____ introduced "induction heads" as critical components for in-context learning in small attention-only models. These heads perform pattern completion by attending to prior token sequences, forming the basis for later work on compositional generalization. Case studies have dissected transformer circuits for specific functions, such as the 'greater than' circuit ____, the 'docstring' circuit ____, the 'indirect object' circuit ____, and the 'max of list' circuit ____. These case studies successfully reverse-engineered the transformer into the minimal-algorithm responsible for the target behavior.

To facilitate identification of relevant circuits, researchers have proposed circuit discovery methods such as logit lens ____, path patching ____, causal scrubbing ____.   For large-scale transformers, automated circuit discovery methods are also proposed ____. So far, transformer interpretability work still requires extensive human efforts in the loop for hypothesis generation and testing. We point to a review paper for a more comprehensive review ____.

\paragraph{Compositional generalization in transformers.} In their study, ____ evaluated compositional generalization ability on different families of models, and found that transformer outperformed RNN and ConvNet in systematic generalization, i.e., recombination of known elements, but still uncomparable to human performance.  ____ pointed out that transformers struggle with composing recursive structures. Recently, ____ showed that after being pre-trained with data generated by a 'meta-grammar', small transformers (less than 1 million parameters) can exhibit human-like compositional ability in novel in-context learning cases. This is in line with the success of commercial large language models (LLM) in solving complex out-of-distribution reasoning tasks ____, where compositional genralization is necessary. 

Several studies highlighted factors that facilitate transformer's compositional ability. ____ identified initialization scales as a critical factor in determining whether models rely on memorization or rule-based reasoning for compositional tasks. ____ revealed that low-complexity circuits enable out-of-distribution generalization by condensing primitive-level rules. ____ identified logarithmic depth as a key constraint for transformers to emulate computations within a sequence. Here, we offer a complementary mechanistic understanding of how trasnformers perform compositional computations.