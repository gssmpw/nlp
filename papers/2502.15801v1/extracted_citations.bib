@INPROCEEDINGS{Bhaskar2024-mn,
  title     = "Finding Transformer Circuits With Edge Pruning",
  author    = "Bhaskar, Adithya and Wettig, Alexander and Friedman, Dan and
               Chen, Danqi",
  booktitle = "The Thirty-eighth Annual Conference on Neural Information
               Processing Systems",
  abstract  = "The path to interpreting a language model often proceeds via
               analysis of circuits---sparse computational subgraphs of the
               model that capture specific aspects of its behavior. Recent work
               has automated the task of discovering circuits. Yet, these
               methods have practical limitations, as they either rely on
               inefficient search algorithms or inaccurate approximations. In
               this paper, we frame circuit discovery as an optimization problem
               and propose \_Edge Pruning\_ as an effective and scalable
               solution. Edge Pruning leverages gradient-based pruning
               techniques, but instead of removing neurons or components, prunes
               the \_edges\_ between components. Our method finds circuits in
               GPT-2 that use less than half the number of edges than circuits
               found by previous methods while being equally faithful to the
               full model predictions on standard circuit-finding tasks. Edge
               Pruning is efficient on tasks involving up to 100,000 examples,
               outperforming previous methods in speed and producing
               substantially better circuits. It also perfectly recovers the
               ground-truth circuits in two models compiled with Tracr. Thanks
               to its efficiency, we scale Edge Pruning to CodeLlama-13B, a
               model over 100x the size of GPT-2. We use this setting for a case
               study, where we compare the mechanisms behind instruction
               prompting and in-context learning. We find two circuits with more
               than 99.96\% sparsity that match the performance of the full
               model. Further analysis reveals that the mechanisms in the two
               settings overlap substantially. This shows that Edge Pruning is a
               practical and scalable tool for interpretability, which can shed
               light on behaviors that only emerge in large models.",
  month     =  nov,
  year      =  2024
}

@ARTICLE{Bubeck2023-ho,
  title         = "Sparks of Artificial General Intelligence: Early experiments
                   with {GPT}-4",
  author        = "Bubeck, Sébastien and Chandrasekaran, Varun and Eldan, Ronen
                   and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and
                   Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg,
                   Scott and Nori, Harsha and Palangi, Hamid and Ribeiro, Marco
                   Tulio and Zhang, Yi",
  journal       = "arXiv [cs.CL]",
  abstract      = "Artificial intelligence (AI) researchers have been developing
                   and refining large language models (LLMs) that exhibit
                   remarkable capabilities across a variety of domains and
                   tasks, challenging our understanding of learning and
                   cognition. The latest model developed by OpenAI, GPT-4, was
                   trained using an unprecedented scale of compute and data. In
                   this paper, we report on our investigation of an early
                   version of GPT-4, when it was still in active development by
                   OpenAI. We contend that (this early version of) GPT-4 is part
                   of a new cohort of LLMs (along with ChatGPT and Google's PaLM
                   for example) that exhibit more general intelligence than
                   previous AI models. We discuss the rising capabilities and
                   implications of these models. We demonstrate that, beyond its
                   mastery of language, GPT-4 can solve novel and difficult
                   tasks that span mathematics, coding, vision, medicine, law,
                   psychology and more, without needing any special prompting.
                   Moreover, in all of these tasks, GPT-4's performance is
                   strikingly close to human-level performance, and often vastly
                   surpasses prior models such as ChatGPT. Given the breadth and
                   depth of GPT-4's capabilities, we believe that it could
                   reasonably be viewed as an early (yet still incomplete)
                   version of an artificial general intelligence (AGI) system.
                   In our exploration of GPT-4, we put special emphasis on
                   discovering its limitations, and we discuss the challenges
                   ahead for advancing towards deeper and more comprehensive
                   versions of AGI, including the possible need for pursuing a
                   new paradigm that moves beyond next-word prediction. We
                   conclude with reflections on societal influences of the
                   recent technological leap and future research directions.",
  month         =  mar,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Conmy2023-ej,
  title         = "Towards automated circuit Discovery for mechanistic
                   interpretability",
  author        = "Conmy, Arthur and Mavor-Parker, Augustine N and Lynch, Aengus
                   and Heimersheim, Stefan and Garriga-Alonso, Adrià",
  journal       = "arXiv [cs.LG]",
  abstract      = "Through considerable effort and intuition, several recent
                   works have reverse-engineered nontrivial behaviors of
                   transformer models. This paper systematizes the mechanistic
                   interpretability process they followed. First, researchers
                   choose a metric and dataset that elicit the desired model
                   behavior. Then, they apply activation patching to find which
                   abstract neural network units are involved in the behavior.
                   By varying the dataset, metric, and units under
                   investigation, researchers can understand the functionality
                   of each component. We automate one of the process' steps: to
                   identify the circuit that implements the specified behavior
                   in the model's computational graph. We propose several
                   algorithms and reproduce previous interpretability results to
                   validate them. For example, the ACDC algorithm rediscovered
                   5/5 of the component types in a circuit in GPT-2 Small that
                   computes the Greater-Than operation. ACDC selected 68 of the
                   32,000 edges in GPT-2 Small, all of which were manually found
                   by previous work. Our code is available at
                   https://github.com/ArthurConmy/Automatic-Circuit-Discovery.",
  month         =  apr,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{DeepSeek-AI2024-bs,
  title         = "{DeepSeek}-{V3} Technical Report",
  author        = "{DeepSeek-AI} and Liu, Aixin and Feng, Bei and Xue, Bing and
                   Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao,
                   Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong
                   and Dai, Damai and Guo, Daya and Yang, Dejian and Chen, Deli
                   and Ji, Dongjie and Li, Erhang and Lin, Fangyun and Dai,
                   Fucong and Luo, Fuli and Hao, Guangbo and Chen, Guanting and
                   Li, Guowei and Zhang, H and Bao, Han and Xu, Hanwei and Wang,
                   Haocheng and Zhang, Haowei and Ding, Honghui and Xin, Huajian
                   and Gao, Huazuo and Li, Hui and Qu, Hui and Cai, J L and
                   Liang, Jian and Guo, Jianzhong and Ni, Jiaqi and Li, Jiashi
                   and Wang, Jiawei and Chen, Jin and Chen, Jingchang and Yuan,
                   Jingyang and Qiu, Junjie and Li, Junlong and Song, Junxiao
                   and Dong, Kai and Hu, Kai and Gao, Kaige and Guan, Kang and
                   Huang, Kexin and Yu, Kuai and Wang, Lean and Zhang, Lecong
                   and Xu, Lei and Xia, Leyi and Zhao, Liang and Wang, Litong
                   and Zhang, Liyue and Li, Meng and Wang, Miaojun and Zhang,
                   Mingchuan and Zhang, Minghua and Tang, Minghui and Li,
                   Mingming and Tian, Ning and Huang, Panpan and Wang, Peiyi and
                   Zhang, Peng and Wang, Qiancheng and Zhu, Qihao and Chen,
                   Qinyu and Du, Qiushi and Chen, R J and Jin, R L and Ge, Ruiqi
                   and Zhang, Ruisong and Pan, Ruizhe and Wang, Runji and Xu,
                   Runxin and Zhang, Ruoyu and Chen, Ruyi and Li, S S and Lu,
                   Shanghao and Zhou, Shangyan and Chen, Shanhuang and Wu,
                   Shaoqing and Ye, Shengfeng and Ye, Shengfeng and Ma, Shirong
                   and Wang, Shiyu and Zhou, Shuang and Yu, Shuiping and Zhou,
                   Shunfeng and Pan, Shuting and Wang, T and Yun, Tao and Pei,
                   Tian and Sun, Tianyu and Xiao, W L and Zeng, Wangding and
                   Zhao, Wanjia and An, Wei and Liu, Wen and Liang, Wenfeng and
                   Gao, Wenjun and Yu, Wenqin and Zhang, Wentao and Li, X Q and
                   Jin, Xiangyue and Wang, Xianzu and Bi, Xiao and Liu, Xiaodong
                   and Wang, Xiaohan and Shen, Xiaojin and Chen, Xiaokang and
                   Zhang, Xiaokang and Chen, Xiaosha and Nie, Xiaotao and Sun,
                   Xiaowen and Wang, Xiaoxiang and Cheng, Xin and Liu, Xin and
                   Xie, Xin and Liu, Xingchao and Yu, Xingkai and Song, Xinnan
                   and Shan, Xinxia and Zhou, Xinyi and Yang, Xinyu and Li,
                   Xinyuan and Su, Xuecheng and Lin, Xuheng and Li, Y K and
                   Wang, Y Q and Wei, Y X and Zhu, Y X and Zhang, Yang and Xu,
                   Yanhong and Xu, Yanhong and Huang, Yanping and Li, Yao and
                   Zhao, Yao and Sun, Yaofeng and Li, Yaohui and Wang, Yaohui
                   and Yu, Yi and Zheng, Yi and Zhang, Yichao and Shi, Yifan and
                   Xiong, Yiliang and He, Ying and Tang, Ying and Piao, Yishi
                   and Wang, Yisong and Tan, Yixuan and Ma, Yiyang and Liu,
                   Yiyuan and Guo, Yongqiang and Wu, Yu and Ou, Yuan and Zhu,
                   Yuchen and Wang, Yuduan and Gong, Yue and Zou, Yuheng and He,
                   Yujia and Zha, Yukun and Xiong, Yunfan and Ma, Yunxian and
                   Yan, Yuting and Luo, Yuxiang and You, Yuxiang and Liu, Yuxuan
                   and Zhou, Yuyang and Wu, Z F and Ren, Z Z and Ren, Zehui and
                   Sha, Zhangli and Fu, Zhe and Xu, Zhean and Huang, Zhen and
                   Zhang, Zhen and Xie, Zhenda and Zhang, Zhengyan and Hao,
                   Zhewen and Gou, Zhibin and Ma, Zhicheng and Yan, Zhigang and
                   Shao, Zhihong and Xu, Zhipeng and Wu, Zhiyu and Zhang,
                   Zhongyu and Li, Zhuoshu and Gu, Zihui and Zhu, Zijia and Liu,
                   Zijun and Li, Zilin and Xie, Ziwei and Song, Ziyang and Gao,
                   Ziyi and Pan, Zizheng",
  journal       = "arXiv [cs.CL]",
  abstract      = "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE)
                   language model with 671B total parameters with 37B activated
                   for each token. To achieve efficient inference and
                   cost-effective training, DeepSeek-V3 adopts Multi-head Latent
                   Attention (MLA) and DeepSeekMoE architectures, which were
                   thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3
                   pioneers an auxiliary-loss-free strategy for load balancing
                   and sets a multi-token prediction training objective for
                   stronger performance. We pre-train DeepSeek-V3 on 14.8
                   trillion diverse and high-quality tokens, followed by
                   Supervised Fine-Tuning and Reinforcement Learning stages to
                   fully harness its capabilities. Comprehensive evaluations
                   reveal that DeepSeek-V3 outperforms other open-source models
                   and achieves performance comparable to leading closed-source
                   models. Despite its excellent performance, DeepSeek-V3
                   requires only 2.788M H800 GPU hours for its full training. In
                   addition, its training process is remarkably stable.
                   Throughout the entire training process, we did not experience
                   any irrecoverable loss spikes or perform any rollbacks. The
                   model checkpoints are available at
                   https://github.com/deepseek-ai/DeepSeek-V3.",
  month         =  dec,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@MISC{Elhage2021-vq,
  title        = "A Mathematical Framework for Transformer Circuits",
  author       = "Elhage, Nelson and Nanda, Neel and Olsson, Catherine and
                  Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell,
                  Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and
                  DasSarma, Nova and Drain, Dawn and Ganguli, Deep and
                  Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and
                  Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and
                  Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared
                  and McCandlish, Sam and Olah, Chris",
  year         =  2021,
  howpublished = "\url{https://transformer-circuits.pub/2021/framework/index.html}",
  note         = "Accessed: 2025-2-4",
  language     = "en"
}

@ARTICLE{Goldowsky-Dill2023-zr,
  title         = "Localizing model behavior with path patching",
  author        = "Goldowsky-Dill, Nicholas and MacLeod, Chris and Sato, Lucas
                   and Arora, Aryaman",
  journal       = "arXiv [cs.LG]",
  abstract      = "Localizing behaviors of neural networks to a subset of the
                   network's components or a subset of interactions between
                   components is a natural first step towards analyzing network
                   mechanisms and possible failure modes. Existing work is often
                   qualitative and ad-hoc, and there is no consensus on the
                   appropriate way to evaluate localization claims. We introduce
                   path patching, a technique for expressing and quantitatively
                   testing a natural class of hypotheses expressing that
                   behaviors are localized to a set of paths. We refine an
                   explanation of induction heads, characterize a behavior of
                   GPT-2, and open source a framework for efficiently running
                   similar experiments.",
  month         =  apr,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@INPROCEEDINGS{Hanna2023-zy,
  title     = "How does {GPT}-2 compute greater-than?: Interpreting mathematical
               abilities in a pre-trained language model",
  author    = "Hanna, Michael and Liu, Ollie and Variengien, Alexandre",
  booktitle = "Thirty-seventh Conference on Neural Information Processing
               Systems",
  abstract  = "Pre-trained language models can be surprisingly adept at tasks
               they were not explicitly trained on, but how they implement these
               capabilities is poorly understood. In this paper, we investigate
               the basic mathematical abilities often acquired by pre-trained
               language models. Concretely, we use mechanistic interpretability
               techniques to explain the (limited) mathematical abilities of
               GPT-2 small. As a case study, we examine its ability to take in
               sentences such as ``The war lasted from the year 1732 to the year
               17'', and predict valid two-digit end years (years > 32). We
               first identify a circuit, a small subset of GPT-2 small's
               computational graph that computes this task's output. Then, we
               explain the role of each circuit component, showing that GPT-2
               small's final multi-layer perceptrons boost the probability of
               end years greater than the start year. Finally, we find related
               tasks that activate our circuit. Our results suggest that GPT-2
               small computes greater-than using a complex but general mechanism
               that activates across diverse contexts.",
  month     =  nov,
  year      =  2023
}

@ARTICLE{Heimersheim2023-ka,
  title    = "A circuit for Python docstrings in a 4-layer attention-only
              transformer",
  author   = "Heimersheim, Stefan and Janiak, Jett",
  abstract = "Produced as part of the SERI ML Alignment Theory Scholars Program
              under the supervision of Neel Nanda - Winter 2022 Cohort. …",
  year     =  2023
}

@ARTICLE{Hofstatter2023-rv,
  title    = "Explaining the Transformer Circuits Framework by Example",
  author   = "Hofstätter, Felix",
  abstract = "Acknowledgement: I want to thank Joseph Bloom, Michael Ripa,
              Tilman Räuker and Alexander Spies for their feedback and comments
              on the draft of this p…",
  year     =  2023
}

@ARTICLE{Hsu2024-xo,
  title         = "Efficient automated circuit discovery in transformers using
                   contextual decomposition",
  author        = "Hsu, Aliyah R and Zhou, Georgia and Cherapanamjeri, Yeshwanth
                   and Huang, Yaxuan and Odisho, Anobel Y and Carroll, Peter R
                   and Yu, Bin",
  journal       = "arXiv [cs.AI]",
  abstract      = "Automated mechanistic interpretation research has attracted
                   great interest due to its potential to scale explanations of
                   neural network internals to large models. Existing automated
                   circuit discovery work relies on activation patching or its
                   approximations to identify subgraphs in models for specific
                   tasks (circuits). They often suffer from slow runtime,
                   approximation errors, and specific requirements of metrics,
                   such as non-zero gradients. In this work, we introduce
                   contextual decomposition for transformers (CD-T) to build
                   interpretable circuits in large language models. CD-T can
                   produce circuits of arbitrary level of abstraction, and is
                   the first able to produce circuits as fine-grained as
                   attention heads at specific sequence positions efficiently.
                   CD-T consists of a set of mathematical equations to isolate
                   contribution of model features. Through recursively computing
                   contribution of all nodes in a computational graph of a model
                   using CD-T followed by pruning, we are able to reduce circuit
                   discovery runtime from hours to seconds compared to
                   state-of-the-art baselines. On three standard circuit
                   evaluation datasets (indirect object identification,
                   greater-than comparisons, and docstring completion), we
                   demonstrate that CD-T outperforms ACDC and EAP by better
                   recovering the manual circuits with an average of 97\% ROC
                   AUC under low runtimes. In addition, we provide evidence that
                   faithfulness of CD-T circuits is not due to random chance by
                   showing our circuits are 80\% more faithful than random
                   circuits of up to 60\% of the original model size. Finally,
                   we show CD-T circuits are able to perfectly replicate
                   original models' behavior (faithfulness $ = 1$) using fewer
                   nodes than the baselines for all tasks. Our results
                   underscore the great promise of CD-T for efficient automated
                   mechanistic interpretability, paving the way for new insights
                   into the workings of large language models.",
  month         =  jun,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI"
}

@ARTICLE{Hupkes2019-qi,
  title         = "Compositionality decomposed: how do neural networks
                   generalise?",
  author        = "Hupkes, Dieuwke and Dankers, Verna and Mul, Mathijs and
                   Bruni, Elia",
  journal       = "arXiv [cs.CL]",
  abstract      = "Despite a multitude of empirical studies, little consensus
                   exists on whether neural networks are able to generalise
                   compositionally, a controversy that, in part, stems from a
                   lack of agreement about what it means for a neural model to
                   be compositional. As a response to this controversy, we
                   present a set of tests that provide a bridge between, on the
                   one hand, the vast amount of linguistic and philosophical
                   theory about compositionality of language and, on the other,
                   the successful neural models of language. We collect
                   different interpretations of compositionality and translate
                   them into five theoretically grounded tests for models that
                   are formulated on a task-independent level. In particular, we
                   provide tests to investigate (i) if models systematically
                   recombine known parts and rules (ii) if models can extend
                   their predictions beyond the length they have seen in the
                   training data (iii) if models' composition operations are
                   local or global (iv) if models' predictions are robust to
                   synonym substitutions and (v) if models favour rules or
                   exceptions during training. To demonstrate the usefulness of
                   this evaluation paradigm, we instantiate these five tests on
                   a highly compositional data set which we dub PCFG SET and
                   apply the resulting tests to three popular
                   sequence-to-sequence models: a recurrent, a convolution-based
                   and a transformer model. We provide an in-depth analysis of
                   the results, which uncover the strengths and weaknesses of
                   these three architectures and point to potential areas of
                   improvement.",
  month         =  aug,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Lake2023-cp,
  title    = "Human-like systematic generalization through a meta-learning
              neural network",
  author   = "Lake, Brenden M and Baroni, Marco",
  journal  = "Nature",
  abstract = "The power of human language and thought arises from systematic
              compositionality-the algebraic ability to understand and produce
              novel combinations from known components. Fodor and Pylyshyn1
              famously argued that artificial neural networks lack this capacity
              and are therefore not viable models of the mind. Neural networks
              have advanced considerably in the years since, yet the
              systematicity challenge persists. Here we successfully address
              Fodor and Pylyshyn's challenge by providing evidence that neural
              networks can achieve human-like systematicity when optimized for
              their compositional skills. To do so, we introduce the
              meta-learning for compositionality (MLC) approach for guiding
              training through a dynamic stream of compositional tasks. To
              compare humans and machines, we conducted human behavioural
              experiments using an instruction learning paradigm. After
              considering seven different models, we found that, in contrast to
              perfectly systematic but rigid probabilistic symbolic models, and
              perfectly flexible but unsystematic neural networks, only MLC
              achieves both the systematicity and flexibility needed for
              human-like generalization. MLC also advances the compositional
              skills of machine learning systems in several systematic
              generalization benchmarks. Our results show how a standard neural
              network architecture, optimized for its compositional skills, can
              mimic human systematic generalization in a head-to-head
              comparison.",
  month    =  oct,
  year     =  2023,
  language = "en"
}

@MISC{LawrenceC2022-zy,
  title        = "Causal Scrubbing: a method for rigorously testing
                  interpretability hypotheses [Redwood Research]",
  author       = "{LawrenceC} and Garriga-alonso, Adrià and Goldowsky-Dill,
                  Nicholas and {ryan\_greenblatt} and Radhakrishnan, Ansh and
                  {Buck} and Thomas, Nate",
  abstract     = "Causal scrubbing is a new tool for evaluating mechanistic
                  interpretability hypotheses. The algorithm tries to replace
                  all model activations that shou…",
  year         =  2022,
  howpublished = "\url{https://www.lesswrong.com/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing}",
  note         = "Accessed: 2025-2-5"
}

@ARTICLE{Olsson2022-hk,
  title         = "In-context learning and induction heads",
  author        = "Olsson, Catherine and Elhage, Nelson and Nanda, Neel and
                   Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and
                   Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna
                   and Conerly, Tom and Drain, Dawn and Ganguli, Deep and
                   Hatfield-Dodds, Zac and Hernandez, Danny and Johnston, Scott
                   and Jones, Andy and Kernion, Jackson and Lovitt, Liane and
                   Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark,
                   Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris",
  journal       = "arXiv [cs.LG]",
  abstract      = "``Induction heads'' are attention heads that implement a
                   simple algorithm to complete token sequences like [A][B] ...
                   [A] -> [B]. In this work, we present preliminary and indirect
                   evidence for a hypothesis that induction heads might
                   constitute the mechanism for the majority of all ``in-context
                   learning'' in large transformer models (i.e. decreasing loss
                   at increasing token indices). We find that induction heads
                   develop at precisely the same point as a sudden sharp
                   increase in in-context learning ability, visible as a bump in
                   the training loss. We present six complementary lines of
                   evidence, arguing that induction heads may be the mechanistic
                   source of general in-context learning in transformer models
                   of any size. For small attention-only models, we present
                   strong, causal evidence; for larger models with MLPs, we
                   present correlational evidence.",
  month         =  sep,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{Rai2024-yg,
  title         = "A practical review of mechanistic interpretability for
                   transformer-based language models",
  author        = "Rai, Daking and Zhou, Yilun and Feng, Shi and Saparov,
                   Abulhair and Yao, Ziyu",
  journal       = "arXiv [cs.AI]",
  abstract      = "Mechanistic interpretability (MI) is an emerging sub-field of
                   interpretability that seeks to understand a neural network
                   model by reverse-engineering its internal computations.
                   Recently, MI has garnered significant attention for
                   interpreting transformer-based language models (LMs),
                   resulting in many novel insights yet introducing new
                   challenges. However, there has not been work that
                   comprehensively reviews these insights and challenges,
                   particularly as a guide for newcomers to this field. To fill
                   this gap, we present a comprehensive survey outlining
                   fundamental objects of study in MI, techniques that have been
                   used for its investigation, approaches for evaluating MI
                   results, and significant findings and applications stemming
                   from the use of MI to understand LMs. In particular, we
                   present a roadmap for beginners to navigate the field and
                   leverage MI for their benefit. Finally, we also identify
                   current gaps in the field and discuss potential future
                   directions.",
  month         =  jul,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI"
}

@ARTICLE{Sanford2024-ye,
  title         = "Transformers, parallel computation, and logarithmic depth",
  author        = "Sanford, Clayton and Hsu, Daniel and Telgarsky, Matus",
  journal       = "arXiv [cs.LG]",
  abstract      = "We show that a constant number of self-attention layers can
                   efficiently simulate, and be simulated by, a constant number
                   of communication rounds of Massively Parallel Computation. As
                   a consequence, we show that logarithmic depth is sufficient
                   for transformers to solve basic computational tasks that
                   cannot be efficiently solved by several other neural sequence
                   models and sub-quadratic transformer approximations. We thus
                   establish parallelism as a key distinguishing property of
                   transformers.",
  month         =  feb,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{Wang2024-mi,
  title         = "Understanding the expressive power and mechanisms of
                   Transformer for sequence modeling",
  author        = "Wang, Mingze and E, Weinan",
  journal       = "arXiv [cs.LG]",
  abstract      = "We conduct a systematic study of the approximation properties
                   of Transformer for sequence modeling with long, sparse and
                   complicated memory. We investigate the mechanisms through
                   which different components of Transformer, such as the
                   dot-product self-attention, positional encoding and
                   feed-forward layer, affect its expressive power, and we study
                   their combined effects through establishing explicit
                   approximation rates. Our study reveals the roles of critical
                   parameters in the Transformer, such as the number of layers
                   and the number of attention heads. These theoretical insights
                   are validated experimentally and offer natural suggestions
                   for alternative architectures.",
  month         =  feb,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{Wang2024-vp,
  title         = "How transformers get rich: Approximation and dynamics
                   analysis",
  author        = "Wang, Mingze and Yu, Ruoxi and E, Weinan and Wu, Lei",
  journal       = "arXiv [cs.LG]",
  abstract      = "Transformers have demonstrated exceptional in-context
                   learning capabilities, yet the theoretical understanding of
                   the underlying mechanisms remains limited. A recent work
                   (Elhage et al., 2021) identified a ``rich'' in-context
                   mechanism known as induction head, contrasting with ``lazy''
                   $n$-gram models that overlook long-range dependencies. In
                   this work, we provide both approximation and dynamics
                   analyses of how transformers implement induction heads. In
                   the {\em approximation} analysis, we formalize both standard
                   and generalized induction head mechanisms, and examine how
                   transformers can efficiently implement them, with an emphasis
                   on the distinct role of each transformer submodule. For the
                   {\em dynamics} analysis, we study the training dynamics on a
                   synthetic mixed target, composed of a 4-gram and an
                   in-context 2-gram component. This controlled setting allows
                   us to precisely characterize the entire training process and
                   uncover an {\em abrupt transition} from lazy (4-gram) to rich
                   (induction head) mechanisms as training progresses.",
  month         =  oct,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{Zhang2024-yl,
  title    = "Transformer-based models are not yet perfect at learning to
              emulate structural recursion",
  author   = "Zhang, Dylan and Tigges, Curt and Zhang, Zory and Biderman, Stella
              and Raginsky, Maxim and Ringer, Talia",
  journal  = "Trans. Mach. Learn. Res.",
  volume   =  2024,
  abstract = "This paper investigates the ability of transformer-based models to
              learn structural recursion from examples. Recursion is a universal
              concept in both natural and formal languages. Structural recursion
              is central to the programming language and formal mathematics
              tasks where symbolic tools currently excel beyond neural models,
              such as inferring semantic relations between datatypes and
              emulating program behavior. We introduce a general framework that
              nicely connects the abstract concepts of structural recursion in
              the programming language domain to concrete sequence modeling
              problems and learned models' behavior. The framework includes a
              representation that captures the general \textit{syntax} of
              structural recursion, coupled with two different frameworks for
              understanding their \textit{semantics} -- one that is more natural
              from a programming languages perspective and one that helps bridge
              that perspective with a mechanistic understanding of the
              underlying transformer architecture. With our framework as a
              powerful conceptual tool, we identify different issues under
              various set-ups. The models trained to emulate recursive
              computations cannot fully capture the recursion yet instead fit
              short-cut algorithms and thus cannot solve certain edge cases that
              are under-represented in the training distribution. In addition,
              it is difficult for state-of-the-art large language models (LLMs)
              to mine recursive rules from in-context demonstrations. Meanwhile,
              these LLMs fail in interesting ways when emulating reduction
              (step-wise computation) of the recursive function.",
  month    =  jan,
  year     =  2024
}

@ARTICLE{Zhang2025-id,
  title         = "Complexity control facilitates reasoning-based compositional
                   generalization in Transformers",
  author        = "Zhang, Zhongwang and Lin, Pengxiao and Wang, Zhiwei and
                   Zhang, Yaoyu and Xu, Zhi-Qin John",
  journal       = "arXiv [cs.CL]",
  abstract      = "Transformers have demonstrated impressive capabilities across
                   various tasks, yet their performance on compositional
                   problems remains a subject of debate. In this study, we
                   investigate the internal mechanisms underlying Transformers'
                   behavior in compositional tasks. We find that complexity
                   control strategies significantly influence whether the model
                   learns primitive-level rules that generalize
                   out-of-distribution (reasoning-based solutions) or relies
                   solely on memorized mappings (memory-based solutions). By
                   applying masking strategies to the model's information
                   circuits and employing multiple complexity metrics, we reveal
                   distinct internal working mechanisms associated with
                   different solution types. Further analysis reveals that
                   reasoning-based solutions exhibit a lower complexity bias,
                   which aligns with the well-studied neuron condensation
                   phenomenon. This lower complexity bias is hypothesized to be
                   the key factor enabling these solutions to learn reasoning
                   rules. We validate these conclusions across multiple
                   real-world datasets, including image generation and natural
                   language processing tasks, confirming the broad applicability
                   of our findings.",
  month         =  jan,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@MISC{nostalgebraist2020-du,
  title        = "interpreting {GPT}: the logit lens",
  author       = "{nostalgebraist}",
  booktitle    = "AI Alignment Forum",
  abstract     = "This post relates an observation I've made in my work with
                  GPT-2, which I have not seen made elsewhere. …",
  year         =  2020,
  howpublished = "\url{https://www.alignmentforum.org/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens}",
  note         = "Accessed: 2025-2-5"
}

