@ARTICLE{DeepSeek-AI2024-bs,
  title         = "{DeepSeek}-{V3} Technical Report",
  author        = "{DeepSeek-AI} and Liu, Aixin and Feng, Bei and Xue, Bing and
                   Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao,
                   Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong
                   and Dai, Damai and Guo, Daya and Yang, Dejian and Chen, Deli
                   and Ji, Dongjie and Li, Erhang and Lin, Fangyun and Dai,
                   Fucong and Luo, Fuli and Hao, Guangbo and Chen, Guanting and
                   Li, Guowei and Zhang, H and Bao, Han and Xu, Hanwei and Wang,
                   Haocheng and Zhang, Haowei and Ding, Honghui and Xin, Huajian
                   and Gao, Huazuo and Li, Hui and Qu, Hui and Cai, J L and
                   Liang, Jian and Guo, Jianzhong and Ni, Jiaqi and Li, Jiashi
                   and Wang, Jiawei and Chen, Jin and Chen, Jingchang and Yuan,
                   Jingyang and Qiu, Junjie and Li, Junlong and Song, Junxiao
                   and Dong, Kai and Hu, Kai and Gao, Kaige and Guan, Kang and
                   Huang, Kexin and Yu, Kuai and Wang, Lean and Zhang, Lecong
                   and Xu, Lei and Xia, Leyi and Zhao, Liang and Wang, Litong
                   and Zhang, Liyue and Li, Meng and Wang, Miaojun and Zhang,
                   Mingchuan and Zhang, Minghua and Tang, Minghui and Li,
                   Mingming and Tian, Ning and Huang, Panpan and Wang, Peiyi and
                   Zhang, Peng and Wang, Qiancheng and Zhu, Qihao and Chen,
                   Qinyu and Du, Qiushi and Chen, R J and Jin, R L and Ge, Ruiqi
                   and Zhang, Ruisong and Pan, Ruizhe and Wang, Runji and Xu,
                   Runxin and Zhang, Ruoyu and Chen, Ruyi and Li, S S and Lu,
                   Shanghao and Zhou, Shangyan and Chen, Shanhuang and Wu,
                   Shaoqing and Ye, Shengfeng and Ye, Shengfeng and Ma, Shirong
                   and Wang, Shiyu and Zhou, Shuang and Yu, Shuiping and Zhou,
                   Shunfeng and Pan, Shuting and Wang, T and Yun, Tao and Pei,
                   Tian and Sun, Tianyu and Xiao, W L and Zeng, Wangding and
                   Zhao, Wanjia and An, Wei and Liu, Wen and Liang, Wenfeng and
                   Gao, Wenjun and Yu, Wenqin and Zhang, Wentao and Li, X Q and
                   Jin, Xiangyue and Wang, Xianzu and Bi, Xiao and Liu, Xiaodong
                   and Wang, Xiaohan and Shen, Xiaojin and Chen, Xiaokang and
                   Zhang, Xiaokang and Chen, Xiaosha and Nie, Xiaotao and Sun,
                   Xiaowen and Wang, Xiaoxiang and Cheng, Xin and Liu, Xin and
                   Xie, Xin and Liu, Xingchao and Yu, Xingkai and Song, Xinnan
                   and Shan, Xinxia and Zhou, Xinyi and Yang, Xinyu and Li,
                   Xinyuan and Su, Xuecheng and Lin, Xuheng and Li, Y K and
                   Wang, Y Q and Wei, Y X and Zhu, Y X and Zhang, Yang and Xu,
                   Yanhong and Xu, Yanhong and Huang, Yanping and Li, Yao and
                   Zhao, Yao and Sun, Yaofeng and Li, Yaohui and Wang, Yaohui
                   and Yu, Yi and Zheng, Yi and Zhang, Yichao and Shi, Yifan and
                   Xiong, Yiliang and He, Ying and Tang, Ying and Piao, Yishi
                   and Wang, Yisong and Tan, Yixuan and Ma, Yiyang and Liu,
                   Yiyuan and Guo, Yongqiang and Wu, Yu and Ou, Yuan and Zhu,
                   Yuchen and Wang, Yuduan and Gong, Yue and Zou, Yuheng and He,
                   Yujia and Zha, Yukun and Xiong, Yunfan and Ma, Yunxian and
                   Yan, Yuting and Luo, Yuxiang and You, Yuxiang and Liu, Yuxuan
                   and Zhou, Yuyang and Wu, Z F and Ren, Z Z and Ren, Zehui and
                   Sha, Zhangli and Fu, Zhe and Xu, Zhean and Huang, Zhen and
                   Zhang, Zhen and Xie, Zhenda and Zhang, Zhengyan and Hao,
                   Zhewen and Gou, Zhibin and Ma, Zhicheng and Yan, Zhigang and
                   Shao, Zhihong and Xu, Zhipeng and Wu, Zhiyu and Zhang,
                   Zhongyu and Li, Zhuoshu and Gu, Zihui and Zhu, Zijia and Liu,
                   Zijun and Li, Zilin and Xie, Ziwei and Song, Ziyang and Gao,
                   Ziyi and Pan, Zizheng",
  journal       = "arXiv [cs.CL]",
  abstract      = "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE)
                   language model with 671B total parameters with 37B activated
                   for each token. To achieve efficient inference and
                   cost-effective training, DeepSeek-V3 adopts Multi-head Latent
                   Attention (MLA) and DeepSeekMoE architectures, which were
                   thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3
                   pioneers an auxiliary-loss-free strategy for load balancing
                   and sets a multi-token prediction training objective for
                   stronger performance. We pre-train DeepSeek-V3 on 14.8
                   trillion diverse and high-quality tokens, followed by
                   Supervised Fine-Tuning and Reinforcement Learning stages to
                   fully harness its capabilities. Comprehensive evaluations
                   reveal that DeepSeek-V3 outperforms other open-source models
                   and achieves performance comparable to leading closed-source
                   models. Despite its excellent performance, DeepSeek-V3
                   requires only 2.788M H800 GPU hours for its full training. In
                   addition, its training process is remarkably stable.
                   Throughout the entire training process, we did not experience
                   any irrecoverable loss spikes or perform any rollbacks. The
                   model checkpoints are available at
                   https://github.com/deepseek-ai/DeepSeek-V3.",
  month         =  dec,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Rai2024-yg,
  title         = "A practical review of mechanistic interpretability for
                   transformer-based language models",
  author        = "Rai, Daking and Zhou, Yilun and Feng, Shi and Saparov,
                   Abulhair and Yao, Ziyu",
  journal       = "arXiv [cs.AI]",
  abstract      = "Mechanistic interpretability (MI) is an emerging sub-field of
                   interpretability that seeks to understand a neural network
                   model by reverse-engineering its internal computations.
                   Recently, MI has garnered significant attention for
                   interpreting transformer-based language models (LMs),
                   resulting in many novel insights yet introducing new
                   challenges. However, there has not been work that
                   comprehensively reviews these insights and challenges,
                   particularly as a guide for newcomers to this field. To fill
                   this gap, we present a comprehensive survey outlining
                   fundamental objects of study in MI, techniques that have been
                   used for its investigation, approaches for evaluating MI
                   results, and significant findings and applications stemming
                   from the use of MI to understand LMs. In particular, we
                   present a roadmap for beginners to navigate the field and
                   leverage MI for their benefit. Finally, we also identify
                   current gaps in the field and discuss potential future
                   directions.",
  month         =  jul,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI"
}

@ARTICLE{Lake2023-cp,
  title    = "Human-like systematic generalization through a meta-learning
              neural network",
  author   = "Lake, Brenden M and Baroni, Marco",
  journal  = "Nature",
  abstract = "The power of human language and thought arises from systematic
              compositionality-the algebraic ability to understand and produce
              novel combinations from known components. Fodor and Pylyshyn1
              famously argued that artificial neural networks lack this capacity
              and are therefore not viable models of the mind. Neural networks
              have advanced considerably in the years since, yet the
              systematicity challenge persists. Here we successfully address
              Fodor and Pylyshyn's challenge by providing evidence that neural
              networks can achieve human-like systematicity when optimized for
              their compositional skills. To do so, we introduce the
              meta-learning for compositionality (MLC) approach for guiding
              training through a dynamic stream of compositional tasks. To
              compare humans and machines, we conducted human behavioural
              experiments using an instruction learning paradigm. After
              considering seven different models, we found that, in contrast to
              perfectly systematic but rigid probabilistic symbolic models, and
              perfectly flexible but unsystematic neural networks, only MLC
              achieves both the systematicity and flexibility needed for
              human-like generalization. MLC also advances the compositional
              skills of machine learning systems in several systematic
              generalization benchmarks. Our results show how a standard neural
              network architecture, optimized for its compositional skills, can
              mimic human systematic generalization in a head-to-head
              comparison.",
  month    =  oct,
  year     =  2023,
  language = "en"
}

@ARTICLE{Zhang2025-id,
  title         = "Complexity control facilitates reasoning-based compositional
                   generalization in Transformers",
  author        = "Zhang, Zhongwang and Lin, Pengxiao and Wang, Zhiwei and
                   Zhang, Yaoyu and Xu, Zhi-Qin John",
  journal       = "arXiv [cs.CL]",
  abstract      = "Transformers have demonstrated impressive capabilities across
                   various tasks, yet their performance on compositional
                   problems remains a subject of debate. In this study, we
                   investigate the internal mechanisms underlying Transformers'
                   behavior in compositional tasks. We find that complexity
                   control strategies significantly influence whether the model
                   learns primitive-level rules that generalize
                   out-of-distribution (reasoning-based solutions) or relies
                   solely on memorized mappings (memory-based solutions). By
                   applying masking strategies to the model's information
                   circuits and employing multiple complexity metrics, we reveal
                   distinct internal working mechanisms associated with
                   different solution types. Further analysis reveals that
                   reasoning-based solutions exhibit a lower complexity bias,
                   which aligns with the well-studied neuron condensation
                   phenomenon. This lower complexity bias is hypothesized to be
                   the key factor enabling these solutions to learn reasoning
                   rules. We validate these conclusions across multiple
                   real-world datasets, including image generation and natural
                   language processing tasks, confirming the broad applicability
                   of our findings.",
  month         =  jan,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Wang2024-mi,
  title         = "Understanding the expressive power and mechanisms of
                   Transformer for sequence modeling",
  author        = "Wang, Mingze and E, Weinan",
  journal       = "arXiv [cs.LG]",
  abstract      = "We conduct a systematic study of the approximation properties
                   of Transformer for sequence modeling with long, sparse and
                   complicated memory. We investigate the mechanisms through
                   which different components of Transformer, such as the
                   dot-product self-attention, positional encoding and
                   feed-forward layer, affect its expressive power, and we study
                   their combined effects through establishing explicit
                   approximation rates. Our study reveals the roles of critical
                   parameters in the Transformer, such as the number of layers
                   and the number of attention heads. These theoretical insights
                   are validated experimentally and offer natural suggestions
                   for alternative architectures.",
  month         =  feb,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@INPROCEEDINGS{Nanda2022-ds,
  title     = "Progress measures for grokking via mechanistic interpretability",
  author    = "Nanda, Neel and Chan, Lawrence and Lieberum, Tom and Smith, Jess
               and Steinhardt, Jacob",
  booktitle = "The Eleventh International Conference on Learning Representations",
  abstract  = "Neural networks often exhibit emergent behavior in which
               qualitatively new capabilities that arise from scaling up the
               number of parameters, training data, or even the number of steps.
               One approach to understanding emergence is to find the continuous
               \textit{progress measures} that underlie the seemingly
               discontinuous qualitative changes. In this work, we argue that
               progress measures can be found via mechanistic
               interpretability---that is, by reverse engineering learned models
               into components and measuring the progress of each component over
               the course of training. As a case study, we study small
               transformers trained on a modular arithmetic tasks with emergent
               grokking behavior. We fully reverse engineer the algorithm
               learned by these networks, which uses discrete fourier transforms
               and trigonometric identities to convert addition to rotation
               about a circle. After confirming the algorithm via ablation, we
               then use our understanding of the algorithm to define progress
               measures that precede the grokking phase transition on this task.
               We see our result as demonstrating both that it is possible to
               fully reverse engineer trained networks, and that doing so can be
               invaluable to understanding their training dynamics.",
  month     =  sep,
  year      =  2022
}

@INPROCEEDINGS{He2024-ws,
  title     = "Learning to grok: Emergence of in-context learning and skill
               composition in modular arithmetic tasks",
  author    = "He, Tianyu and Doshi, Darshil and Das, Aritra and Gromov, Andrey",
  booktitle = "The Thirty-eighth Annual Conference on Neural Information
               Processing Systems",
  abstract  = "Large language models can solve tasks that were not present in
               the training set. This capability is believed to be due to
               in-context learning and skill composition. In this work, we study
               the emergence of in-context learning and skill composition in a
               collection of modular arithmetic tasks. Specifically, we consider
               a finite collection of linear modular functions $z = a x + b y
               \text{ mod } p$ labeled by the vector $(a, b) \in
               \mathbb{Z}_p^2$. We use some of these tasks for pre-training and
               the rest for out-of-distribution testing. We empirically show
               that a GPT-style transformer exhibits a transition from
               in-distribution to out-of-distribution generalization as the
               number of pre-training tasks increases. We find that the smallest
               model capable of out-of-distribution generalization requires two
               transformer blocks, while for deeper models, the
               out-of-distribution generalization phase is *transient*,
               necessitating early stopping. Finally, we perform an
               interpretability study of the pre-trained models, revealing
               highly structured representations in both attention heads and
               MLPs; and discuss the learned algorithms. Notably, we find an
               algorithmic shift in deeper models, as we go from few to many
               in-context examples.",
  month     =  nov,
  year      =  2024
}

@ARTICLE{Hsu2024-xo,
  title         = "Efficient automated circuit discovery in transformers using
                   contextual decomposition",
  author        = "Hsu, Aliyah R and Zhou, Georgia and Cherapanamjeri, Yeshwanth
                   and Huang, Yaxuan and Odisho, Anobel Y and Carroll, Peter R
                   and Yu, Bin",
  journal       = "arXiv [cs.AI]",
  abstract      = "Automated mechanistic interpretation research has attracted
                   great interest due to its potential to scale explanations of
                   neural network internals to large models. Existing automated
                   circuit discovery work relies on activation patching or its
                   approximations to identify subgraphs in models for specific
                   tasks (circuits). They often suffer from slow runtime,
                   approximation errors, and specific requirements of metrics,
                   such as non-zero gradients. In this work, we introduce
                   contextual decomposition for transformers (CD-T) to build
                   interpretable circuits in large language models. CD-T can
                   produce circuits of arbitrary level of abstraction, and is
                   the first able to produce circuits as fine-grained as
                   attention heads at specific sequence positions efficiently.
                   CD-T consists of a set of mathematical equations to isolate
                   contribution of model features. Through recursively computing
                   contribution of all nodes in a computational graph of a model
                   using CD-T followed by pruning, we are able to reduce circuit
                   discovery runtime from hours to seconds compared to
                   state-of-the-art baselines. On three standard circuit
                   evaluation datasets (indirect object identification,
                   greater-than comparisons, and docstring completion), we
                   demonstrate that CD-T outperforms ACDC and EAP by better
                   recovering the manual circuits with an average of 97\% ROC
                   AUC under low runtimes. In addition, we provide evidence that
                   faithfulness of CD-T circuits is not due to random chance by
                   showing our circuits are 80\% more faithful than random
                   circuits of up to 60\% of the original model size. Finally,
                   we show CD-T circuits are able to perfectly replicate
                   original models' behavior (faithfulness $ = 1$) using fewer
                   nodes than the baselines for all tasks. Our results
                   underscore the great promise of CD-T for efficient automated
                   mechanistic interpretability, paving the way for new insights
                   into the workings of large language models.",
  month         =  jun,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI"
}

@INPROCEEDINGS{Bhaskar2024-mn,
  title     = "Finding Transformer Circuits With Edge Pruning",
  author    = "Bhaskar, Adithya and Wettig, Alexander and Friedman, Dan and
               Chen, Danqi",
  booktitle = "The Thirty-eighth Annual Conference on Neural Information
               Processing Systems",
  abstract  = "The path to interpreting a language model often proceeds via
               analysis of circuits---sparse computational subgraphs of the
               model that capture specific aspects of its behavior. Recent work
               has automated the task of discovering circuits. Yet, these
               methods have practical limitations, as they either rely on
               inefficient search algorithms or inaccurate approximations. In
               this paper, we frame circuit discovery as an optimization problem
               and propose \_Edge Pruning\_ as an effective and scalable
               solution. Edge Pruning leverages gradient-based pruning
               techniques, but instead of removing neurons or components, prunes
               the \_edges\_ between components. Our method finds circuits in
               GPT-2 that use less than half the number of edges than circuits
               found by previous methods while being equally faithful to the
               full model predictions on standard circuit-finding tasks. Edge
               Pruning is efficient on tasks involving up to 100,000 examples,
               outperforming previous methods in speed and producing
               substantially better circuits. It also perfectly recovers the
               ground-truth circuits in two models compiled with Tracr. Thanks
               to its efficiency, we scale Edge Pruning to CodeLlama-13B, a
               model over 100x the size of GPT-2. We use this setting for a case
               study, where we compare the mechanisms behind instruction
               prompting and in-context learning. We find two circuits with more
               than 99.96\% sparsity that match the performance of the full
               model. Further analysis reveals that the mechanisms in the two
               settings overlap substantially. This shows that Edge Pruning is a
               practical and scalable tool for interpretability, which can shed
               light on behaviors that only emerge in large models.",
  month     =  nov,
  year      =  2024
}

@ARTICLE{Chen2021-ys,
  title         = "Evaluating large language models trained on code",
  author        = "Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming
                   and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and
                   Edwards, Harri and Burda, Yuri and Joseph, Nicholas and
                   Brockman, Greg and Ray, Alex and Puri, Raul and Krueger,
                   Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry,
                   Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott
                   and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and
                   Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and
                   Tillet, Philippe and Such, Felipe Petroski and Cummings, Dave
                   and Plappert, Matthias and Chantzis, Fotios and Barnes,
                   Elizabeth and Herbert-Voss, Ariel and Guss, William Hebgen
                   and Nichol, Alex and Paino, Alex and Tezak, Nikolas and Tang,
                   Jie and Babuschkin, Igor and Balaji, Suchir and Jain,
                   Shantanu and Saunders, William and Hesse, Christopher and
                   Carr, Andrew N and Leike, Jan and Achiam, Josh and Misra,
                   Vedant and Morikawa, Evan and Radford, Alec and Knight,
                   Matthew and Brundage, Miles and Murati, Mira and Mayer, Katie
                   and Welinder, Peter and McGrew, Bob and Amodei, Dario and
                   McCandlish, Sam and Sutskever, Ilya and Zaremba, Wojciech",
  journal       = "arXiv [cs.LG]",
  abstract      = "We introduce Codex, a GPT language model fine-tuned on
                   publicly available code from GitHub, and study its Python
                   code-writing capabilities. A distinct production version of
                   Codex powers GitHub Copilot. On HumanEval, a new evaluation
                   set we release to measure functional correctness for
                   synthesizing programs from docstrings, our model solves
                   28.8\% of the problems, while GPT-3 solves 0\% and GPT-J
                   solves 11.4\%. Furthermore, we find that repeated sampling
                   from the model is a surprisingly effective strategy for
                   producing working solutions to difficult prompts. Using this
                   method, we solve 70.2\% of our problems with 100 samples per
                   problem. Careful investigation of our model reveals its
                   limitations, including difficulty with docstrings describing
                   long chains of operations and with binding operations to
                   variables. Finally, we discuss the potential broader impacts
                   of deploying powerful code generation technologies, covering
                   safety, security, and economics.",
  month         =  jul,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{Brown2020-ut,
  title         = "Language Models are Few-Shot Learners",
  author        = "Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah,
                   Melanie and Kaplan, Jared and Dhariwal, Prafulla and
                   Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and
                   Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel
                   and Krueger, Gretchen and Henighan, Tom and Child, Rewon and
                   Ramesh, Aditya and Ziegler, Daniel M and Wu, Jeffrey and
                   Winter, Clemens and Hesse, Christopher and Chen, Mark and
                   Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess,
                   Benjamin and Clark, Jack and Berner, Christopher and
                   McCandlish, Sam and Radford, Alec and Sutskever, Ilya and
                   Amodei, Dario",
  journal       = "arXiv [cs.CL]",
  abstract      = "Recent work has demonstrated substantial gains on many NLP
                   tasks and benchmarks by pre-training on a large corpus of
                   text followed by fine-tuning on a specific task. While
                   typically task-agnostic in architecture, this method still
                   requires task-specific fine-tuning datasets of thousands or
                   tens of thousands of examples. By contrast, humans can
                   generally perform a new language task from only a few
                   examples or from simple instructions - something which
                   current NLP systems still largely struggle to do. Here we
                   show that scaling up language models greatly improves
                   task-agnostic, few-shot performance, sometimes even reaching
                   competitiveness with prior state-of-the-art fine-tuning
                   approaches. Specifically, we train GPT-3, an autoregressive
                   language model with 175 billion parameters, 10x more than any
                   previous non-sparse language model, and test its performance
                   in the few-shot setting. For all tasks, GPT-3 is applied
                   without any gradient updates or fine-tuning, with tasks and
                   few-shot demonstrations specified purely via text interaction
                   with the model. GPT-3 achieves strong performance on many NLP
                   datasets, including translation, question-answering, and
                   cloze tasks, as well as several tasks that require on-the-fly
                   reasoning or domain adaptation, such as unscrambling words,
                   using a novel word in a sentence, or performing 3-digit
                   arithmetic. At the same time, we also identify some datasets
                   where GPT-3's few-shot learning still struggles, as well as
                   some datasets where GPT-3 faces methodological issues related
                   to training on large web corpora. Finally, we find that GPT-3
                   can generate samples of news articles which human evaluators
                   have difficulty distinguishing from articles written by
                   humans. We discuss broader societal impacts of this finding
                   and of GPT-3 in general.",
  month         =  may,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Vaswani2017-cg,
  title     = "Attention is All you Need",
  author    = "Vaswani, Ashish and Shazeer, Noam M and Parmar, Niki and
               Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser,
               Lukasz and Polosukhin, Illia",
  editor    = "Guyon, I and Luxburg, U Von and Bengio, S and Wallach, H and
               Fergus, R and Vishwanathan, S and Garnett, R",
  journal   = "Neural Inf Process Syst",
  publisher = "Curran Associates, Inc.",
  volume    =  30,
  pages     = "5998--6008",
  abstract  = "The dominant sequence transduction models are based on complex
               recurrent or convolutional neural networks in an encoder-decoder
               configuration. The best performing models also connect the
               encoder and decoder through an attention mechanism. We propose a
               new simple network architecture, the Transformer, based solely on
               attention mechanisms, dispensing with recurrence and convolutions
               entirely. Experiments on two machine translation tasks show these
               models to be superior in quality while being more parallelizable
               and requiring significantly less time to train. Our model
               achieves 28.4 BLEU on the WMT 2014 English-to-German translation
               task, improving over the existing best results, including
               ensembles by over 2 BLEU. On the WMT 2014 English-to-French
               translation task, our model establishes a new single-model
               state-of-the-art BLEU score of 41.8 after training for 3.5 days
               on eight GPUs, a small fraction of the training costs of the best
               models from the literature. We show that the Transformer
               generalizes well to other tasks by applying it successfully to
               English constituency parsing both with large and limited training
               data.",
  month     =  jun,
  year      =  2017
}

@ARTICLE{Hendrycks2020-ca,
  title         = "Measuring massive multitask language understanding",
  author        = "Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou,
                   Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob",
  journal       = "arXiv [cs.CY]",
  abstract      = "We propose a new test to measure a text model's multitask
                   accuracy. The test covers 57 tasks including elementary
                   mathematics, US history, computer science, law, and more. To
                   attain high accuracy on this test, models must possess
                   extensive world knowledge and problem solving ability. We
                   find that while most recent models have near random-chance
                   accuracy, the very largest GPT-3 model improves over random
                   chance by almost 20 percentage points on average. However, on
                   every one of the 57 tasks, the best models still need
                   substantial improvements before they can reach expert-level
                   accuracy. Models also have lopsided performance and
                   frequently do not know when they are wrong. Worse, they still
                   have near-random accuracy on some socially important subjects
                   such as morality and law. By comprehensively evaluating the
                   breadth and depth of a model's academic and professional
                   understanding, our test can be used to analyze models across
                   many tasks and to identify important shortcomings.",
  month         =  sep,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CY"
}

@ARTICLE{Wang2024-vp,
  title         = "How transformers get rich: Approximation and dynamics
                   analysis",
  author        = "Wang, Mingze and Yu, Ruoxi and E, Weinan and Wu, Lei",
  journal       = "arXiv [cs.LG]",
  abstract      = "Transformers have demonstrated exceptional in-context
                   learning capabilities, yet the theoretical understanding of
                   the underlying mechanisms remains limited. A recent work
                   (Elhage et al., 2021) identified a ``rich'' in-context
                   mechanism known as induction head, contrasting with ``lazy''
                   $n$-gram models that overlook long-range dependencies. In
                   this work, we provide both approximation and dynamics
                   analyses of how transformers implement induction heads. In
                   the {\em approximation} analysis, we formalize both standard
                   and generalized induction head mechanisms, and examine how
                   transformers can efficiently implement them, with an emphasis
                   on the distinct role of each transformer submodule. For the
                   {\em dynamics} analysis, we study the training dynamics on a
                   synthetic mixed target, composed of a 4-gram and an
                   in-context 2-gram component. This controlled setting allows
                   us to precisely characterize the entire training process and
                   uncover an {\em abrupt transition} from lazy (4-gram) to rich
                   (induction head) mechanisms as training progresses.",
  month         =  oct,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{Olsson2022-hk,
  title         = "In-context learning and induction heads",
  author        = "Olsson, Catherine and Elhage, Nelson and Nanda, Neel and
                   Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and
                   Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna
                   and Conerly, Tom and Drain, Dawn and Ganguli, Deep and
                   Hatfield-Dodds, Zac and Hernandez, Danny and Johnston, Scott
                   and Jones, Andy and Kernion, Jackson and Lovitt, Liane and
                   Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark,
                   Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris",
  journal       = "arXiv [cs.LG]",
  abstract      = "``Induction heads'' are attention heads that implement a
                   simple algorithm to complete token sequences like [A][B] ...
                   [A] -> [B]. In this work, we present preliminary and indirect
                   evidence for a hypothesis that induction heads might
                   constitute the mechanism for the majority of all ``in-context
                   learning'' in large transformer models (i.e. decreasing loss
                   at increasing token indices). We find that induction heads
                   develop at precisely the same point as a sudden sharp
                   increase in in-context learning ability, visible as a bump in
                   the training loss. We present six complementary lines of
                   evidence, arguing that induction heads may be the mechanistic
                   source of general in-context learning in transformer models
                   of any size. For small attention-only models, we present
                   strong, causal evidence; for larger models with MLPs, we
                   present correlational evidence.",
  month         =  sep,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{Song2024-ud,
  title         = "Out-of-distribution generalization via composition: a lens
                   through induction heads in Transformers",
  author        = "Song, Jiajun and Xu, Zhuoyan and Zhong, Yiqiao",
  journal       = "arXiv [cs.CL]",
  abstract      = "Large language models (LLMs) such as GPT-4 sometimes appear
                   to be creative, solving novel tasks often with a few
                   demonstrations in the prompt. These tasks require the models
                   to generalize on distributions different from those from
                   training data -- which is known as out-of-distribution (OOD)
                   generalization. Despite the tremendous success of LLMs, how
                   they approach OOD generalization remains an open and
                   underexplored question. We examine OOD generalization in
                   settings where instances are generated according to hidden
                   rules, including in-context learning with symbolic reasoning.
                   Models are required to infer the hidden rules behind input
                   prompts without any fine-tuning. We empirically examined the
                   training dynamics of Transformers on a synthetic example and
                   conducted extensive experiments on a variety of pretrained
                   LLMs, focusing on a type of components known as induction
                   heads. We found that OOD generalization and composition are
                   tied together -- models can learn rules by composing two
                   self-attention layers, thereby achieving OOD generalization.
                   Furthermore, a shared latent subspace in the embedding (or
                   feature) space acts as a bridge for composition by aligning
                   early layers and later layers, which we refer to as the
                   common bridge representation hypothesis.",
  month         =  aug,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Crosbie2024-em,
  title         = "Induction heads as an essential mechanism for pattern
                   matching in in-context learning",
  author        = "Crosbie, Joy and Shutova, Ekaterina",
  journal       = "arXiv [cs.CL]",
  abstract      = "Large language models (LLMs) have shown a remarkable ability
                   to learn and perform complex tasks through in-context
                   learning (ICL). However, a comprehensive understanding of its
                   internal mechanisms is still lacking. This paper explores the
                   role of induction heads in a few-shot ICL setting. We analyse
                   two state-of-the-art models, Llama-3-8B and InternLM2-20B on
                   abstract pattern recognition and NLP tasks. Our results show
                   that even a minimal ablation of induction heads leads to ICL
                   performance decreases of up to ~32\% for abstract pattern
                   recognition tasks, bringing the performance close to random.
                   For NLP tasks, this ablation substantially decreases the
                   model's ability to benefit from examples, bringing few-shot
                   ICL performance close to that of zero-shot prompts. We
                   further use attention knockout to disable specific induction
                   patterns, and present fine-grained evidence for the role that
                   the induction mechanism plays in ICL.",
  month         =  jul,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@MISC{Elhage2021-vq,
  title        = "A Mathematical Framework for Transformer Circuits",
  author       = "Elhage, Nelson and Nanda, Neel and Olsson, Catherine and
                  Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell,
                  Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and
                  DasSarma, Nova and Drain, Dawn and Ganguli, Deep and
                  Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and
                  Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and
                  Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared
                  and McCandlish, Sam and Olah, Chris",
  year         =  2021,
  howpublished = "\url{https://transformer-circuits.pub/2021/framework/index.html}",
  note         = "Accessed: 2025-2-4",
  language     = "en"
}

@ARTICLE{Heimersheim2023-ka,
  title    = "A circuit for Python docstrings in a 4-layer attention-only
              transformer",
  author   = "Heimersheim, Stefan and Janiak, Jett",
  abstract = "Produced as part of the SERI ML Alignment Theory Scholars Program
              under the supervision of Neel Nanda - Winter 2022 Cohort. …",
  year     =  2023
}

@ARTICLE{Hofstatter2023-rv,
  title    = "Explaining the Transformer Circuits Framework by Example",
  author   = "Hofstätter, Felix",
  abstract = "Acknowledgement: I want to thank Joseph Bloom, Michael Ripa,
              Tilman Räuker and Alexander Spies for their feedback and comments
              on the draft of this p…",
  year     =  2023
}

@ARTICLE{Lake2016-kh,
  title         = "Building Machines That Learn and Think Like People",
  author        = "Lake, Brenden M and Ullman, Tomer D and Tenenbaum, Joshua B
                   and Gershman, Samuel J",
  journal       = "arXiv [cs.AI]",
  abstract      = "Recent progress in artificial intelligence (AI) has renewed
                   interest in building systems that learn and think like
                   people. Many advances have come from using deep neural
                   networks trained end-to-end in tasks such as object
                   recognition, video games, and board games, achieving
                   performance that equals or even beats humans in some
                   respects. Despite their biological inspiration and
                   performance achievements, these systems differ from human
                   intelligence in crucial ways. We review progress in cognitive
                   science suggesting that truly human-like learning and
                   thinking machines will have to reach beyond current
                   engineering trends in both what they learn, and how they
                   learn it. Specifically, we argue that these machines should
                   (a) build causal models of the world that support explanation
                   and understanding, rather than merely solving pattern
                   recognition problems; (b) ground learning in intuitive
                   theories of physics and psychology, to support and enrich the
                   knowledge that is learned; and (c) harness compositionality
                   and learning-to-learn to rapidly acquire and generalize
                   knowledge to new tasks and situations. We suggest concrete
                   challenges and promising routes towards these goals that can
                   combine the strengths of recent neural network advances with
                   more structured cognitive models.",
  month         =  apr,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI"
}

@ARTICLE{Hupkes2019-qi,
  title         = "Compositionality decomposed: how do neural networks
                   generalise?",
  author        = "Hupkes, Dieuwke and Dankers, Verna and Mul, Mathijs and
                   Bruni, Elia",
  journal       = "arXiv [cs.CL]",
  abstract      = "Despite a multitude of empirical studies, little consensus
                   exists on whether neural networks are able to generalise
                   compositionally, a controversy that, in part, stems from a
                   lack of agreement about what it means for a neural model to
                   be compositional. As a response to this controversy, we
                   present a set of tests that provide a bridge between, on the
                   one hand, the vast amount of linguistic and philosophical
                   theory about compositionality of language and, on the other,
                   the successful neural models of language. We collect
                   different interpretations of compositionality and translate
                   them into five theoretically grounded tests for models that
                   are formulated on a task-independent level. In particular, we
                   provide tests to investigate (i) if models systematically
                   recombine known parts and rules (ii) if models can extend
                   their predictions beyond the length they have seen in the
                   training data (iii) if models' composition operations are
                   local or global (iv) if models' predictions are robust to
                   synonym substitutions and (v) if models favour rules or
                   exceptions during training. To demonstrate the usefulness of
                   this evaluation paradigm, we instantiate these five tests on
                   a highly compositional data set which we dub PCFG SET and
                   apply the resulting tests to three popular
                   sequence-to-sequence models: a recurrent, a convolution-based
                   and a transformer model. We provide an in-depth analysis of
                   the results, which uncover the strengths and weaknesses of
                   these three architectures and point to potential areas of
                   improvement.",
  month         =  aug,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Bubeck2023-ho,
  title         = "Sparks of Artificial General Intelligence: Early experiments
                   with {GPT}-4",
  author        = "Bubeck, Sébastien and Chandrasekaran, Varun and Eldan, Ronen
                   and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and
                   Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg,
                   Scott and Nori, Harsha and Palangi, Hamid and Ribeiro, Marco
                   Tulio and Zhang, Yi",
  journal       = "arXiv [cs.CL]",
  abstract      = "Artificial intelligence (AI) researchers have been developing
                   and refining large language models (LLMs) that exhibit
                   remarkable capabilities across a variety of domains and
                   tasks, challenging our understanding of learning and
                   cognition. The latest model developed by OpenAI, GPT-4, was
                   trained using an unprecedented scale of compute and data. In
                   this paper, we report on our investigation of an early
                   version of GPT-4, when it was still in active development by
                   OpenAI. We contend that (this early version of) GPT-4 is part
                   of a new cohort of LLMs (along with ChatGPT and Google's PaLM
                   for example) that exhibit more general intelligence than
                   previous AI models. We discuss the rising capabilities and
                   implications of these models. We demonstrate that, beyond its
                   mastery of language, GPT-4 can solve novel and difficult
                   tasks that span mathematics, coding, vision, medicine, law,
                   psychology and more, without needing any special prompting.
                   Moreover, in all of these tasks, GPT-4's performance is
                   strikingly close to human-level performance, and often vastly
                   surpasses prior models such as ChatGPT. Given the breadth and
                   depth of GPT-4's capabilities, we believe that it could
                   reasonably be viewed as an early (yet still incomplete)
                   version of an artificial general intelligence (AGI) system.
                   In our exploration of GPT-4, we put special emphasis on
                   discovering its limitations, and we discuss the challenges
                   ahead for advancing towards deeper and more comprehensive
                   versions of AGI, including the possible need for pursuing a
                   new paradigm that moves beyond next-word prediction. We
                   conclude with reflections on societal influences of the
                   recent technological leap and future research directions.",
  month         =  mar,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Zhang2024-yl,
  title    = "Transformer-based models are not yet perfect at learning to
              emulate structural recursion",
  author   = "Zhang, Dylan and Tigges, Curt and Zhang, Zory and Biderman, Stella
              and Raginsky, Maxim and Ringer, Talia",
  journal  = "Trans. Mach. Learn. Res.",
  volume   =  2024,
  abstract = "This paper investigates the ability of transformer-based models to
              learn structural recursion from examples. Recursion is a universal
              concept in both natural and formal languages. Structural recursion
              is central to the programming language and formal mathematics
              tasks where symbolic tools currently excel beyond neural models,
              such as inferring semantic relations between datatypes and
              emulating program behavior. We introduce a general framework that
              nicely connects the abstract concepts of structural recursion in
              the programming language domain to concrete sequence modeling
              problems and learned models' behavior. The framework includes a
              representation that captures the general \textit{syntax} of
              structural recursion, coupled with two different frameworks for
              understanding their \textit{semantics} -- one that is more natural
              from a programming languages perspective and one that helps bridge
              that perspective with a mechanistic understanding of the
              underlying transformer architecture. With our framework as a
              powerful conceptual tool, we identify different issues under
              various set-ups. The models trained to emulate recursive
              computations cannot fully capture the recursion yet instead fit
              short-cut algorithms and thus cannot solve certain edge cases that
              are under-represented in the training distribution. In addition,
              it is difficult for state-of-the-art large language models (LLMs)
              to mine recursive rules from in-context demonstrations. Meanwhile,
              these LLMs fail in interesting ways when emulating reduction
              (step-wise computation) of the recursive function.",
  month    =  jan,
  year     =  2024
}

@ARTICLE{Conmy2023-ej,
  title         = "Towards automated circuit Discovery for mechanistic
                   interpretability",
  author        = "Conmy, Arthur and Mavor-Parker, Augustine N and Lynch, Aengus
                   and Heimersheim, Stefan and Garriga-Alonso, Adrià",
  journal       = "arXiv [cs.LG]",
  abstract      = "Through considerable effort and intuition, several recent
                   works have reverse-engineered nontrivial behaviors of
                   transformer models. This paper systematizes the mechanistic
                   interpretability process they followed. First, researchers
                   choose a metric and dataset that elicit the desired model
                   behavior. Then, they apply activation patching to find which
                   abstract neural network units are involved in the behavior.
                   By varying the dataset, metric, and units under
                   investigation, researchers can understand the functionality
                   of each component. We automate one of the process' steps: to
                   identify the circuit that implements the specified behavior
                   in the model's computational graph. We propose several
                   algorithms and reproduce previous interpretability results to
                   validate them. For example, the ACDC algorithm rediscovered
                   5/5 of the component types in a circuit in GPT-2 Small that
                   computes the Greater-Than operation. ACDC selected 68 of the
                   32,000 edges in GPT-2 Small, all of which were manually found
                   by previous work. Our code is available at
                   https://github.com/ArthurConmy/Automatic-Circuit-Discovery.",
  month         =  apr,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@INPROCEEDINGS{Wang2022-ha,
  title     = "Interpretability in the Wild: a Circuit for Indirect Object
               Identification in {GPT}-2 Small",
  author    = "Wang, Kevin Ro and Variengien, Alexandre and Conmy, Arthur and
               Shlegeris, Buck and Steinhardt, Jacob",
  booktitle = "The Eleventh International Conference on Learning Representations",
  abstract  = "Research in mechanistic interpretability seeks to explain
               behaviors of ML models in terms of their internal components.
               However, most previous work either focuses on simple behaviors in
               small models, or describes complicated behaviors in larger models
               with broad strokes. In this work, we bridge this gap by
               presenting an explanation for how GPT-2 small performs a natural
               language task that requires logical reasoning: indirect object
               identification (IOI). Our explanation encompasses 28 attention
               heads grouped into 7 main classes, which we discovered using a
               combination of interpretability approaches including causal
               interventions and projections. To our knowledge, this
               investigation is the largest end-to-end attempt at
               reverse-engineering a natural behavior ``in the wild'' in a
               language model. We evaluate the reliability of our explanation
               using three quantitative criteria - faithfulness, completeness
               and minimality. Though these criteria support our explanation,
               they also point to remaining gaps in our understanding. Our work
               provides evidence that a mechanistic understanding of large ML
               models is feasible, opening opportunities to scale our
               understanding to both larger models and more complex tasks.",
  month     =  sep,
  year      =  2022
}

@INPROCEEDINGS{Hanna2023-zy,
  title     = "How does {GPT}-2 compute greater-than?: Interpreting mathematical
               abilities in a pre-trained language model",
  author    = "Hanna, Michael and Liu, Ollie and Variengien, Alexandre",
  booktitle = "Thirty-seventh Conference on Neural Information Processing
               Systems",
  abstract  = "Pre-trained language models can be surprisingly adept at tasks
               they were not explicitly trained on, but how they implement these
               capabilities is poorly understood. In this paper, we investigate
               the basic mathematical abilities often acquired by pre-trained
               language models. Concretely, we use mechanistic interpretability
               techniques to explain the (limited) mathematical abilities of
               GPT-2 small. As a case study, we examine its ability to take in
               sentences such as ``The war lasted from the year 1732 to the year
               17'', and predict valid two-digit end years (years > 32). We
               first identify a circuit, a small subset of GPT-2 small's
               computational graph that computes this task's output. Then, we
               explain the role of each circuit component, showing that GPT-2
               small's final multi-layer perceptrons boost the probability of
               end years greater than the start year. Finally, we find related
               tasks that activate our circuit. Our results suggest that GPT-2
               small computes greater-than using a complex but general mechanism
               that activates across diverse contexts.",
  month     =  nov,
  year      =  2023
}

@BOOK{Fodor1979-eg,
  title     = "The language of thought",
  author    = "Fodor, Jerry A",
  publisher = "Harvard University Press",
  address   = "London, England",
  series    = "The Language and Thought Series",
  month     =  jul,
  year      =  1979
}

@ARTICLE{Goldowsky-Dill2023-zr,
  title         = "Localizing model behavior with path patching",
  author        = "Goldowsky-Dill, Nicholas and MacLeod, Chris and Sato, Lucas
                   and Arora, Aryaman",
  journal       = "arXiv [cs.LG]",
  abstract      = "Localizing behaviors of neural networks to a subset of the
                   network's components or a subset of interactions between
                   components is a natural first step towards analyzing network
                   mechanisms and possible failure modes. Existing work is often
                   qualitative and ad-hoc, and there is no consensus on the
                   appropriate way to evaluate localization claims. We introduce
                   path patching, a technique for expressing and quantitatively
                   testing a natural class of hypotheses expressing that
                   behaviors are localized to a set of paths. We refine an
                   explanation of induction heads, characterize a behavior of
                   GPT-2, and open source a framework for efficiently running
                   similar experiments.",
  month         =  apr,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@MISC{nostalgebraist2020-du,
  title        = "interpreting {GPT}: the logit lens",
  author       = "{nostalgebraist}",
  booktitle    = "AI Alignment Forum",
  abstract     = "This post relates an observation I've made in my work with
                  GPT-2, which I have not seen made elsewhere. …",
  year         =  2020,
  howpublished = "\url{https://www.alignmentforum.org/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens}",
  note         = "Accessed: 2025-2-5"
}

@MISC{LawrenceC2022-zy,
  title        = "Causal Scrubbing: a method for rigorously testing
                  interpretability hypotheses [Redwood Research]",
  author       = "{LawrenceC} and Garriga-alonso, Adrià and Goldowsky-Dill,
                  Nicholas and {ryan\_greenblatt} and Radhakrishnan, Ansh and
                  {Buck} and Thomas, Nate",
  abstract     = "Causal scrubbing is a new tool for evaluating mechanistic
                  interpretability hypotheses. The algorithm tries to replace
                  all model activations that shou…",
  year         =  2022,
  howpublished = "\url{https://www.lesswrong.com/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing}",
  note         = "Accessed: 2025-2-5"
}

@ARTICLE{Sanford2024-ye,
  title         = "Transformers, parallel computation, and logarithmic depth",
  author        = "Sanford, Clayton and Hsu, Daniel and Telgarsky, Matus",
  journal       = "arXiv [cs.LG]",
  abstract      = "We show that a constant number of self-attention layers can
                   efficiently simulate, and be simulated by, a constant number
                   of communication rounds of Massively Parallel Computation. As
                   a consequence, we show that logarithmic depth is sufficient
                   for transformers to solve basic computational tasks that
                   cannot be efficiently solved by several other neural sequence
                   models and sub-quadratic transformer approximations. We thus
                   establish parallelism as a key distinguishing property of
                   transformers.",
  month         =  feb,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}
