\vspace{-0.1in}
\section{Experimental analysis}
\label{sec:experiment analysis}
\subsection{Model Size}

Observing the performance across different model sizes in \autoref{fig:model size}, we notice that as the model size increases, there is a general improvement in performance across all scenarios. For the Internvl2 models, we consider four different sizes: 1B, 2B, 8B, and 26B parameters. In the \textbf{Add Image} scenario, the exact match scores increase from 79.70 for the 1B model to 94.45 for the 8B model, with a slight decrease to 93.40 at the 26B model. In the \textbf{Insert Image} scenario, performance improves steadily from 83.47 (1B) to 95.14 (26B). For the \textbf{Add Hints} scenario, scores rise from 80.55 (1B) to 93.60 (8B), then slightly decrease to 92.80 (26B). In the \textbf{Insert Hint} scenario, scores increase from 82.85 (1B) to 96.55 (26B). These results indicate that increasing the model size generally enhances performance, particularly up to the 8B parameter. The slight decrease or plateau in performance at the 26B size for some scenarios suggests that beyond a certain point, increasing model size yields diminishing performance or requires more sophisticated training techniques to leverage the additional parameters effectively.
\label{subsec:model size}
\begin{figure}[ht]
  \centering
  \begin{subfigure}{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figure/internvl2_performance_line.pdf}
    % \caption{Performance Metrics for Internvl2 Models}
    \label{fig:internvl2_performance_line}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figure/llava_performance_line.pdf}
    % \caption{Performance Metrics for Llava Models}
    \label{fig:llava_performance_line}
  \end{subfigure}
  \vspace{-0.15in}
  \caption{Comparison of Exact Match Score for Internvl2(top) and Llava Models(bottom).}
  \label{fig:model size}
\vspace{-0.15in}
\end{figure}


Similarly, for the Llava models with sizes 7B, 13B, and 34B, we observe performance trends in the distraction scenarios that reflect improvement with increased model size. In the \textbf{Add Image} scenario, scores increase from 68.05 (7B) to 87.50 (34B). In the \textbf{Insert Image} scenario, performance improves from 66.36 (7B) to 79.51 (34B). For the \textbf{Add Hint} scenario, scores rise from 63.80 (7B) to 82.65 (34B). In the \textbf{Insert Hint} scenario, scores increase from 69.30 (7B) to 83.00 (34B). The Llava models also show a clear trend of performance improvement with increased model size across all scenarios. The performance gains are more pronounced between the 7B and 34B models, suggesting that larger models can better handle distractions and integrate additional information effectively.

Comparing both models, the Internvl2 models generally outperform the Llava models at similar parameter sizes, especially in higher model sizes. For instance, the Internvl2 (8B) model achieves higher distraction scores than the Llava (13B) model across all scenarios, indicating that the Internvl2 architecture or training data may be more efficient in leveraging parameters for scenario performance. These observations underscore the significance of model scaling in enhancing performance, but they also highlight that architecture design and training data are crucial in maximizing the benefits of increased model size. 

\subsection{Analysis on training dataset 
 and model architecture}
\label{subsec:training data and model architecture}
 
\input{figure/dataset_model_encoder}

% \subsection{Error Analysis}
% % sample from eye shut figure 3
% We pick some samples and use visualisation method from promptbench.

% \input{figure/attention_table}

\subsection{Defending against distractions}
 \label{subsec:denfending}
 
\begin{table*}[ht]
  \centering
  \small \resizebox{\textwidth}{!}{
  \begin{tabular}{l|cc|cc|cc|cc}
    \toprule
    \textbf{Model} & \multicolumn{2}{c|}{\textbf{Add Image (\%)}} & \multicolumn{2}{c|}{\textbf{Insert Image (\%)}} & \multicolumn{2}{c|}{\textbf{Add Hints (\%)}} & \multicolumn{2}{c}{\textbf{Insert Hint (\%)}} \\
    \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
    & No Defense & Defense & No Defense & Defense & No Defense & Defense & No Defense & Defense \\
    \midrule
    Qwen2-VL-Instruct (2B) & 63.30 & 73.80 & 63.26 & 65.60 & 54.45 & 62.60 & 64.20 & 70.35 \\
    Qwen2-VL-Instruct (7B) & 83.10 & 81.35 & 68.08 & 68.60 & 68.00 & 68.05 & 74.10 & 74.90 \\
    CogVLM2 (19B)           & 71.70 & 70.15 & 87.47 & 85.18 & 70.50 & 70.20 & 80.85 & 79.10 \\
    \bottomrule
  \end{tabular}
  }
\caption{Exact match scores achieved by the models using a naive prompt compared to a prompt with instructions to ignore distractions.}
\label{tab:prompt defense}
\end{table*}

The findings in \autoref{tab:prompt defense} demonstrate that although prompt engineering techniques—such as adding instructions to guide the model's focus toward the question and away from distractions—can partially mitigate the effects of distractions, models still struggle to ignore them. For instance, in the \textbf{Add Image} scenario, the performance of Qwen2-VL-Instruct (2B) improves from 63.30 to 73.80 when defense mechanisms are applied, indicating that appropriate prompts can enhance the model's focus on relevant information. Similarly, in the \textbf{Insert Hint} scenario, the same model's performance increases from 64.20 to 70.35 with defense strategies.

However, the improvements are not uniform across all models and tasks. The Qwen2-VL-Instruct (7B) model shows a slight decrease in performance in the \textbf{Add Image} scenario when defenses are applied, dropping from 83.10 to 81.35. This suggests that the effectiveness of defense mechanisms may vary depending on the model's architecture and size. Moreover, the CogVLM2 (19B) model exhibits a minor reduction in performance across most tasks with defense prompts, indicating that larger models are not necessarily better at ignoring distractions when prompted to do so. For example, in the \textbf{Insert Image} scenario, its performance decreases from 87.47 to 85.18 even with defense.

\begin{table*}[ht]
  \centering
  \small
  \caption{Exact Math Scores achieved by LLava Models with different vision encoders.}
\label{tab:robust vision encoder}
  \resizebox{\textwidth}{!}{
  \begin{tabular}{l|cc|cc|cc|cc}
    \toprule
    \textbf{Model (Vision Encoder)} & \multicolumn{2}{c|}{\textbf{Add Image (\%)}} & \multicolumn{2}{c|}{\textbf{Insert Image (\%)}} & \multicolumn{2}{c|}{\textbf{Add Hints (\%)}} & \multicolumn{2}{c}{\textbf{Insert Hint (\%)}} \\
    \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
    & Original & Distraction & Original & Distraction & Original & Distraction & Original & Distraction \\
    \midrule
    LLava-7B (Robust-clip) & N/A & 70.40 & 67.30 & 63.55 & 71.78 & 67.48 & 64.31 & 63.25 \\
    LLava-7B (Clip)        & N/A & 69.55 & 68.95 & 64.32 & 76.22 & 72.49 & 64.39 & 63.25 \\
    \bottomrule
  \end{tabular}
  }
\end{table*}

Table~\ref{tab:robust vision encoder} summarizes the performance of LLava-7B models with two different vision encoders: \textit{robust-clip}~\citep{schlarmann2024robustclip} and \textit{clip}~\citep{ilharco_gabriel_2021_5143773}. The models are evaluated across original and distraction scenarios, focusing on the effects of adding or inserting images and hints. Since the LLava-7B model with \textit{robust-clip} can only process inputs that include both text and visual content, samples without images were excluded from this evaluation. The \textit{robust-clip} encoder only outperforms the \textit{clip} encoder slightly in the \textbf{Add Image} scenario by 0.85. In other scenario, the performance of the \textit{robust-clip} encoder is lower than that of the \textit{clip} encoder. These findings suggest that \textit{robust-clip} shows very limited efficacy in defending against visual distractions.

These results suggest potential areas for future improvements in model training and design. Developing more effective prompting techniques and enhancing model architectures could help models better filter out irrelevant information. Additionally, incorporating training data that specifically addresses the handling of distractions may improve models' robustness in real-world applications where irrelevant data is commonplace.




\subsection{Bi-Modal Distraction}
\label{subsec:bi modal distraction}


\begin{table}[ht]
  \centering
  \small
  \caption{Exact Match Score under bi-modal distractions.}
  \label{tab:bi_modal_injection}
  \begin{tabular}{lcccc}
    \toprule
    \multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{c}{\textbf{Add Hint}} & \multicolumn{2}{c}{\textbf{Insert Hint}} \\
    \cmidrule(lr){2-3} \cmidrule(lr){4-5}
    & No Img & With Img & No Img & With Img \\
    \midrule
    Qwen2-VL-2B & 57.68 & 57.68 & 72.86 & 72.86 \\
    Qwen2-VL-7B & 71.43 & 71.43 & 88.57 & 88.57 \\
    CogVLM2-19B & 57.45 & 56.91 & 78.31 & 79.35 \\
    \bottomrule
  \end{tabular}
\end{table}
The results in~\autoref{tab:bi_modal_injection} examine the models' performances under conditions where textual distractions are present, with and without the simultaneous presence of visual distractions. Specifically, the ``No Image'' columns represent scenarios with only textual distractions, while the ``With Image'' columns include both textual and visual distractions. 

Analyzing the data, we observe that the performance of Qwen2-VL-Instruct (2B) and Qwen2-VL-Instruct (7B) remains unchanged between the ``No Image'' and ``With Image'' conditions across both ``Add Hint'' and ``Insert Hint'' scenarios. This suggests that the addition of visual distractions does not significantly impact these models when textual distractions are already present. In contrast, the CogVLM2 (19B) model shows a slight decrease in performance from 57.45\% to 56.91\% in the ``Add Hint'' scenario when an image is added, indicating a minor negative effect of visual distractions in conjunction with textual ones. Interestingly, in the ``Insert Hint'' scenario, its performance slightly improves from 78.31\% to 79.35\% with the addition of an image, suggesting that under certain conditions, visual distraction might compete with textual distraction.

Overall, these findings imply that the models' abilities to handle bi-modal distractions are nuanced. While some models maintain consistent performance regardless of the presence of additional visual information, others exhibit minor fluctuations. This highlights the importance of designing models that can effectively integrate and prioritize multi-modal inputs, ensuring robustness in environments where distractions are prevalent across different modalities.

