\section{Introduction}
\label{sec:introduction}

Despite the impressive capabilities of vision-language models (VLMs) in understanding images and generating human-like text~\citep{liu2023llava, dai2023instructblip, viscpm}, their susceptibility to irrelevant information remains a critical challenge. In real-world, it is common that visual and text inputs could have noisy distractions. Such distractions can lead to performance degradation, potentially resulting in incorrect interpretations or responses with hallucination from VLMs \citep{zhou2024analyzing, chen2024halc}.


\begin{figure}[ht]
    \centering
    \includegraphics 
    [width=1\linewidth, height=0.6\textheight, keepaspectratio]
    {figure/insert_distraction_dimensions.pdf}
    \vspace{-0.15in}
    \caption{Diagram illustrating various scenario of distraction we apply to the samples in \emph{Science-QA} dataset.}
    \label{fig:build dataset}
\vspace{-0.25in}
\end{figure}

Existing benchmarks for VLMs are typically designed under the assumption that inputs in both visual and textual domains are carefully curated without distractions. This assumption, however, fails to reflect real-world scenarios. Previous research~\citep{shi2023large} has demonstrated that large language models (LLMs) are vulnerable to textual distractions. With the rapid development of VLMs, it is crucial to understand how these models handle distractions not only in the textual domain but also in the visual domain. Compared to LLMs, VLMs face the additional challenge of potential distractions from \textit{bi-modal} inputs, making the situation more complex. 


Moreover, current evaluation benchmarks~\citep{lu2022scienceqa,singh2019towards,lu2024mathvistaevaluatingmathematicalreasoning} do not adequately account for the presence of distractions within the input. They often emphasize clean and well-structured datasets, which do not mirror the complexities and noise inherent in real-world data. This oversight limits the ability to assess the true robustness and reliability of VLMs when deployed in practical settings where distractions are inevitable. Consequently, there is a pressing need for benchmarks that systematically introduce and evaluate various types of distractions to better understand and improve VLM performance under realistic conditions.

To address this gap, we present I-ScienceQA, a comprehensive benchmark designed to investigate the robustness of VLMs towards distractions. Our benchmark, built upon the ScienceQA dataset~\citep{lu2022scienceqa}, incorporates various types of distractions to simulate more realistic scenarios. Specifically, we aim to answer the following questions:

\begin{itemize}[leftmargin=2em]
\setlength\itemsep{0em}
     \item How vulnerable are VLMs towards distractions across different modalities? 
     \item Which modality, visual or textual, causes more degradation in model performance when distracted? 
     \item What techniques can mitigate the impact of distractions and improve the robustness of VLMs? 
\end{itemize}

To build \emph{I-ScienceQA}, we leveraged different generative models, including GPT-3.5-turbo~\citep{openai2024gpt35turbo} and Stable diffusion models~\citep{Rombach2021HighResolutionIS}. Our benchmark comprises 8,100 samples with four scenarios of distractions in both visual and textual domains. Specifically, we utilized stable diffusion models to generate visual distractions, such as neutral backgrounds, generic landscapes, abstract art, and everyday objects. For textual distractions, we employed GPT-3.5-turbo to produce textual distractions such as contradictory information, irrelevant details. This approach allowed us to simulate a wide range of real-world scenarios where VLMs might encounter noisy or irrelevant information. More information about the definition of distractions can be found in Appendix.

Through extensive evaluation of the various state-of-the-art VLMs, our key findings include:

\begin{itemize}[leftmargin=2em]
\setlength\itemsep{0em}
    \item VLMs exhibit varying degrees of vulnerability to distractions, with performance degradation observed across different models and scenarios (see Section~\ref{sec:experiment results}).
    \item Textual distractions tend to have a more significant impact on VLMs compared to visual distractions, particularly in the ``Add Hints'' scenario (see Section~\ref{sec:experiment results}).
    \item Larger models generally demonstrate better robustness against distractions, with some models like Internvl2 (8B) showing minimal performance drops in certain scenarios (see Section~\ref{subsec:model size}).
    \item Prompt engineering techniques or robust encoders offer limited enhancement to VLM performance against distractions, with their effectiveness varying across different models and tasks (see Section~\ref{subsec:denfending}). 
    \item The impact of bi-modal distractions (both visual and textual) on VLMs is nuanced, with some models showing consistent performance while others exhibiting minor fluctuations (see Section~\ref{subsec:bi modal distraction}).
\end{itemize}

Our research not only provides valuable insights into the current limitations of VLMs but also highlights potential areas for improvement in model design and training methodologies. By addressing these challenges, we can develop more robust and reliable VLMs for real-world applications.