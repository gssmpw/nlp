\section{Related Work}
\label{sec:related_work}

\textbf{Model Evaluations} VLMs have traditionally been evaluated using standard Visual Question Answering (VQA) tasks such as TextVQA~\citep{singh2019towards}, VQAv2~\citep{vqav2}, and GQA~\citep{hudson2019gqa}. Mecently, studies like MM-Vet~\citep{yu2023mm}, POPE~\citep{li2023pope}, and MM-Bench~\citep{liu2023mmbench} have emerged to  evaluate VLMs, in key challenges such as hallucination, reasoning. These efforts have demonstrated that multimodal LLMs encounter significant issues, such as hallucination~\citep{guan2023hallusionbench} and insufficient robustness~\citep{fu2023mme}. In this papr, we introduce the I-ScienceQA benchmark, which highlights that even advanced VLMs, such as GPT-4o~\citep{gpt4v}, struggle with basic visual questions when irrelevant distractions are present in the input. 

% \noindent\textbf{Benchmarks with Input Perturbations} 
% The use of input perturbations has been a common strategy in natural language tasks, with approaches ranging from model-agnostic input transformations~\citep{liang2022holistic,ravichander2022condaqa} to adversarial example generation targeting specific models~\citep{jia2017adversarial,shi2018learning,morris2020textattack,wang2021adversarial}. Notably, prior research has constructed arithmetic reasoning benchmarks by paraphrasing or rewriting sentences from clean datasets~\citep{patel2021nlp,kumar2021adversarial, shi2023large}. 

\noindent\textbf{Robustness of VLM} To test the robustness of model reasoning, researchers have constructed arithmetic reasoning benchmarks by paraphrasing or rewriting sentences from clean datasets~\citep{patel2021nlp,kumar2021adversarial, shi2023large}.
Recent studies have increasingly concentrated on the adversarial robustness of VLMs~\cite{qi2024visual, carlini2024alignedneuralnetworksadversarially, schlarmann2023adversarial, zhao2023evaluating, dong2023robust}. \citet{schlarmann2023adversarial} demonstrate that imperceptible perturbations in input images can enable attackers to manipulate LVLMs into generating specific outputs. Visual adversarial attacks designed to jailbreak LVLMs are introduced in works such as~\citet{carlini2024alignedneuralnetworksadversarially} and~\citet{qi2024visual}. Recently, studies have focused on training adversary-robust vision encoders ~\cite{schlarmann2024robustclip, mao2023understanding}.