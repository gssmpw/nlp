\section{Benchmark}


\subsection{Overview of \emph{I-ScienceQA}}


% In order to create a comprehensive benchmark for assessing the robustness of VLMs, it is essential to introduce minor distractions while ensuring that the hints for solving the questions remain accessible in either the textual or visual context. In this paper, we present the \emph{I-ScienceQA} benchmark, a collection of 8,100 samples. This dataset encompasses 4 scenarios of distractions and integrates data from various sources.\wjdd{Is creating this benchmark challenging, or just some common practice? We need to write more on the challenges in doing this, showing our contribtions in the dataset level.}

In order to create a comprehensive benchmark for assessing the robustness of VLMs, it is essential to introduce minor distractions while ensuring that the hints for solving the questions remain accessible in either the textual or visual context. Developing \emph{I-ScienceQA} presented several challenges. Firstly, ensuring the diversity and relevance of distractions across both visual and textual modalities required meticulous selection and generation strategies. Additionally, maintaining the semantic integrity of the original questions while injecting distractions demanded advanced techniques in data augmentation and validation. To overcome these challenges, we leveraged state-of-the-art generative models, such as GPT-3.5-turbo~\citep{openai2024gpt35turbo} for textual distractions and Stable diffusion models~\citep{Rombach2021HighResolutionIS} for visual distractions, ensuring that the introduced noise was both diverse and contextually appropriate. These efforts resulted in a robust and versatile benchmark that not only fills the gaps left by existing datasets but also provides a nuanced framework for evaluating and enhancing the resilience of VLMs in practical applications. In this paper, we introduce the I-ScienceQA benchmark, consisting of 8,100 samples distributed across four distraction scenarios.


\textbf{Data Collection}  \autoref{fig:build dataset} illustrates the models we utilized to construct the dataset. In our study, we employed  LLMs to introduce textual distractions and stable diffusion models to generate visual distractions. As depicted in \autoref{fig:build dataset}, we took use of GPT-3.5-turbo to generate short textual contexts or insert brief distractions into existing text. For the visual domain, we employed stable diffusion models to create various image distractions. We also applied masks to the main objects in existing images and added distractions to other areas to ensure that the models could still extract useful information to answer the questions. \autoref{fig:build dataset} shows the detailed process for data generation.

\textbf{Dataset Statistics} Built upon the ScienceQA dataset, our dataset is crafted as a comprehensive and diversified benchmark for evaluating the robustness of VLMs against distractions. In Appendix, we present samples from the dataset for some of the distraction types. There is the dataset statistics in Appendix. Specifically, the \emph{I-ScienceQA} dataset contains 8,100 samples, which include 4,000 text-based distractions and 4,100 image-based distractions. This dataset encompasses 4 scenarios of distractions. The data are collected from four types of sources including stable diffusion\citep{Rombach2021HighResolutionIS}, GPT-3.5, Unsplash API\citep{unsplashapi}, and PromeAI\citep{promeai2024}. It offers a broad spectrum of distractions. We believe that \emph{I-ScienceQA} can serve as a comprehensive benchmark for evaluating the robustness of VLMs. In the following sections, we will describe how we established the \emph{I-ScienceQA} benchmark.



\subsection{Data Collection and Augmentation Strategies}

\textbf{Scenario I: Add Image} After randomly selecting 2,000 samples from the test partition of examples in \emph{ScienceQA}~\citep{lu2022scienceqa} that originally do not include images, we added images to these samples to introduce visual contexts that test the model's ability to integrate and prioritize textual information when paired with unrelated visual content. We employed stable diffusion models to create these images. The types of images added are shown in Appendix and their definition can be found in Appendix. We generated a variety of images, ranging from neutral backgrounds to emotional contexts. In \autoref{fig:build dataset}, we present an example where original sample lacks image context, and it is then augmented with image generated from stable diffusion model. 

The selection of 2,000 samples was strategically chosen to facilitate an even distribution across eight subtypes of visual distractions under scenario of \textbf{Add Image}, allocating the same number of samples to each subtype(see Appendix). This approach ensures that each subtype of distraction is adequately represented, providing a balanced and comprehensive evaluation. Additionally, limiting the number of samples to 2,000 makes the dataset manageable in size, allowing for efficient processing and analysis. Random selection was employed to minimize selection bias and ensure that the distractions are uniformly distributed, enhancing the benchmark's reliability and validity. Similarly, for remaining scenarios, we adopted the same sample selection scheme. Each of those scenarios involved randomly selecting 2,000 samples from \emph{ScienceQA} and evenly distributing them across their respective distraction subtypes. 

\textbf{Scenario II: Insert Image}
After randomly selecting another 2,000 samples from the test partition of examples in \emph{ScienceQA}~\citep{lu2022scienceqa} that already include images, we inserted visual distractions to them to test the VLMs' robustness against visual noise and their ability to maintain focus on relevant elements. We mainly collected visual distraction images from the Unsplash API \citep{unsplashapi} and then combined them with the original images side by side. The types of images we collected are the same as in the previous section, as shown in Appendix. Additionally, we randomly selected 100 samples with large blank areas in the images from these 2,000 samples and employed diffusion model-based methods~\citep{promeai2024} to in-paint distractions into these blank areas. For this small subset, we considered inserting distractions such as flying objects or sitting pets. More details of this diffusion inpainting can be found in Appendix. In \autoref{fig:build dataset}, we show an example where there is existing visual context in the original sample, and an object is inserted by inpainting.

\textbf{Scenario III: Add Hint}
We also explored the integration of textual distractions. Inspired by the findings that large language models can be significantly distracted by irrelevant context~\citep{shi2023large}, we designed textual distractions using the GPT-3.5-turbo  to challenge the VLMs' ability to focus on relevant content. We first randomly selected 2,000 samples from the test partition of examples in \emph{ScienceQA}~\citep{lu2022scienceqa} that have the textual hint as ``N/A'' and then replace it with GPT-3.5-turbo generated content. In \autoref{fig:build dataset}, we present an example where there is no textual context as hints in the original sample, and it is augmented with textual hints generated from GPT-3.5-turbo. More details of this scenario of textual distraction can be found in Appendix.

\textbf{Scenario IV: Insert Hint}
We randomly selected 2,000 samples from the test partition of examples in \emph{ScienceQA}~\citep{lu2022scienceqa} where explicit textual hint is provided. Inserting distractions requires careful integration to challenge the models' capacity to maintain focus on the relevant information. These distractions are designed to test the model's resilience against misleading cues without completely diverging from the context. We employed the GPT-3.5-turbo to insert textual distractions. Unlike the previous section, we fed the existing textual hint from each sample to better leverage the LLMs' ability to create distractions based on the existing hint. In \autoref{fig:build dataset}, we present an example where there is existing textual hints in the original sample, and it is inserted with textual distractions generated from GPT-3.5-turbo. Types of distributions are elaborated in Appendix.

Each of these scenarios introduces a layer of complexity into the interaction between text and image, leveraging detailed contexts to test the model's ability to navigate and prioritize information effectively. Additionally, we ensured that all generated images and texts adhere to strict ethical guidelines, avoiding the inclusion of harmful, biased, or inappropriate content. By implementing rigorous filtering  and manual reviews, we maintain the integrity and responsibility of our benchmark, thereby preventing the introduction of unethical concerns.

% \subsection{Design Principle}

% The design of our multimodal benchmark for distractions is grounded in the principle of creating realistic and challenging scenarios that accurately reflect the complexities of real-world environments where VLMs are deployed. The goal is to assess the robustness and adaptability of these models by introducing a variety of distractions they might encounter in practical applications. Here are the core principles guiding the benchmark's design:

% \begin{itemize}[leftmargin=2em]
% \setlength\itemsep{0em}
%   \item \textbf{Realism and Relevance:} Every element of the benchmark—from the selection of images and texts to the types of introduced distractions—is designed to closely mimic real-life conditions. 
%   \item \textbf{Comprehensive Challenge:} The benchmark is desinged to challenge the models across multiple dimensions. This includes their ability to process and interpret visual and textual information, filter out irrelevant data, and maintain focus on the task at hand. Distractions are varied to comprehensively test the models' capabilities.

%   \item \textbf{Generative Model-Based Generation:} Generative models have demonstrated their ability to generate enriched samples in both the textual and visual domains. Inspired by recent study~\citep{shu2023exploitabilityinstructiontuning}, we propose to leverage generative models for multimodal data collection, based on the existing image-question pairs from \emph{ScienceQA}.

% \end{itemize}

 