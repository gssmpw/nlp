\begin{figure*}[ht]
\centering
\begin{tikzpicture}[
    scale=0.65, % Reduced from 0.75 to 0.65
    node distance=0.3cm and 1.2cm, % Reduced vertical and horizontal node distances
    dataset/.style={
        rectangle,
        draw,
        rounded corners,
        align=center,
        minimum height=0.4cm, % Reduced from 0.5cm
        minimum width=3cm,
        font=\scriptsize % Reduced font size
    },
    pretrain/.style={
        dataset,
        fill=green!20
    },
    model/.style={
        rectangle,
        draw,
        rounded corners,
        align=center,
        minimum height=0.6cm, % Reduced from 0.7cm
        minimum width=2.5cm,
        fill=blue!20,
        font=\bfseries\scriptsize % Reduced font size
    },
    vision/.style={
        rectangle,
        draw,
        rounded corners,
        align=center,
        minimum height=0.4cm, % Reduced from 0.5cm
        minimum width=2cm,
        fill=orange!20,
        font=\scriptsize % Reduced font size
    },
    language/.style={
        rectangle,
        draw,
        rounded corners,
        align=center,
        minimum height=0.4cm, % Reduced from 0.5cm
        minimum width=2cm,
        fill=yellow!20,
        font=\scriptsize % Reduced font size
    },
    arrow/.style={
        ->,
        very thin,
        >=stealth,
        draw=gray
    },
    arrow_light/.style={
        ->,
        very thin,
        >=stealth,
        draw=gray!50
    },
    highlight/.style={
        ->,
        black,
        thick,
        >=stealth
    },
    title/.style={
        font=\bfseries\small
    },
    legend/.style={
        rectangle,
        draw,
        align=left,
        fill=white,
        font=\scriptsize % Reduced font size
    }
]

% Models (centered)
\node[model] (LLaVA) at (0,3) {LLaVA (7B, 13B)};
\node[model, below=1cm of LLaVA] (CogVLM2) {CogVLM2 (19B)};
\node[model, below=1cm of CogVLM2] (InstructBLIP) {InstructBLIP (7B, 13B)};
\node[model, below=1cm of InstructBLIP] (InternVL2) {InternVL2 (2B, 8B)};

% Training Datasets (left side)
\node[pretrain] (coco) at (-6,4) {COCO};
\node[pretrain, below=0.2cm of coco] (scienceqa) {ScienceQA};
\node[pretrain, below=0.2cm of scienceqa] (vqav2) {VQAv2};
\node[pretrain, below=0.2cm of vqav2] (okvqa) {OKVQA};
\node[pretrain, below=0.2cm of okvqa] (textvqa) {TextVQA};
\node[pretrain, below=0.2cm of textvqa] (textcaps) {TextCaps};
\node[pretrain, below=0.2cm of textcaps] (ocrvqa) {OCR-VQA};
\node[pretrain, below=0.2cm of ocrvqa] (chartqa) {ChartQA};
\node[pretrain, below=0.2cm of chartqa] (visualdialog) {Visual Dialog};
\node[pretrain, below=0.2cm of visualdialog] (docvqa) {DocVQA};

% Vision Encoders and Language Models (right side)
\node[vision] (LLaVA_vision) at (6,3) {CLIP ViT-L/14};
\node[language, right=0.2cm of LLaVA_vision] (LLaVA_lang) {Vicuna (7B, 13B)};

\node[vision, below=1.2cm of LLaVA_vision] (CogVLM2_vision) {EVA-CLIP-E};
\node[language, right=0.2cm of CogVLM2_vision] (CogVLM2_lang) {LLaMA3 (8B)};

\node[vision, below=1.2cm of CogVLM2_vision] (InstructBLIP_vision) {CLIP ViT-G/14};
\node[language, right=0.2cm of InstructBLIP_vision] (InstructBLIP_lang) {Vicuna (7B, 13B)};

\node[vision, below=1.2cm of InstructBLIP_vision] (InternVL2_vision) {InternViT};
\node[language, right=0.2cm of InternVL2_vision] (InternVL2_lang) {InternLM2};

% Group Datasets with Boxes
\node[draw, dashed, rounded corners, inner sep=0.2cm, fit=(coco) (docvqa), label=above:{\textbf{\scriptsize Common Datasets}}] (dataset_group) {};

% Group Vision Encoders and Language Models with an increased width
\node[
    draw,
    dashed,
    rounded corners,
    inner sep=0.2cm,
    minimum width=5cm, % Reduced from 6cm
    fit=(LLaVA_vision) (InternVL2_lang),
    label=above:{\textbf{\scriptsize Vision Encoders and Language Models}}
] (vision_lang_group) {};

% Connections from Datasets to Models

% LLaVA
\draw[arrow_light] (coco) -- (LLaVA);
\draw[highlight] (scienceqa) -- (LLaVA);

% InstructBLIP
\draw[arrow_light] (coco) -- (InstructBLIP);
\draw[arrow_light] (textcaps) -- (InstructBLIP);
\draw[arrow] (vqav2) -- (InstructBLIP);
\draw[arrow] (okvqa) -- (InstructBLIP);
\draw[arrow] (ocrvqa) -- (InstructBLIP);
\draw[arrow] (visualdialog) -- (InstructBLIP);

% CogVLM2
\draw[arrow_light] (coco) -- (CogVLM2);
\draw[arrow] (okvqa) -- (CogVLM2);
\draw[arrow] (vqav2) -- (CogVLM2);
\draw[arrow] (docvqa) -- (CogVLM2);
\draw[arrow] (ocrvqa) -- (CogVLM2);
\draw[arrow] (textvqa) -- (CogVLM2);
\draw[highlight] (scienceqa) -- (CogVLM2);
\draw[arrow] (chartqa) -- (CogVLM2);

% InternVL2
\draw[arrow_light] (coco) -- (InternVL2);
\draw[arrow_light] (textcaps) -- (InternVL2);
\draw[arrow] (vqav2) -- (InternVL2);
\draw[arrow] (okvqa) -- (InternVL2);
\draw[arrow] (visualdialog) -- (InternVL2);
\draw[highlight] (scienceqa) -- (InternVL2);
\draw[arrow] (docvqa) -- (InternVL2);
\draw[arrow] (ocrvqa) -- (InternVL2);
\draw[arrow] (textvqa) -- (InternVL2);
\draw[arrow] (chartqa) -- (InternVL2);

% Connections from Models to Vision Encoders and Language Models (parallel lines)
\draw[arrow] (LLaVA) -- ++(2,0) |- (LLaVA_vision);
% \draw[arrow] (LLaVA) -- ++(2,0) |- (LLaVA_lang);

\draw[arrow] (CogVLM2) -- ++(2,0) |- (CogVLM2_vision);
% \draw[arrow] (CogVLM2) -- ++(2,0) |- (CogVLM2_lang);

\draw[arrow] (InstructBLIP) -- ++(2,0) |- (InstructBLIP_vision);
% \draw[arrow] (InstructBLIP) -- ++(2,0) |- (InstructBLIP_lang);

\draw[arrow] (InternVL2) -- ++(2,0) |- (InternVL2_vision);
% \draw[arrow] (InternVL2) -- ++(2,0) |- (InternVL2_lang);

% Legend (Compact Layout)
\node[legend, above=1.2cm of LLaVA, anchor=north, xshift=2.4cm] (legend) {
\begin{tikzpicture}[inner sep=0, node distance=0.3cm and 0.8cm]
    % Training Dataset
    \node[pretrain, minimum width=0.4cm, minimum height=0.3cm] (p) {};
    \node[right=0.1cm of p, font=\scriptsize] {Training Dataset};
    
    % Separator
    \node[draw=none, right=0.3cm of p] (sep1) {};
    
    % Model
    \node[model, right=1.5cm of sep1, minimum width=0.4cm, minimum height=0.3cm] (m) {};
    \node[right=0.2cm of m, font=\scriptsize] {Model};
    
    % Separator
    \node[draw=none, right=0.3cm of m] (sep2) {};
    
    % Vision Encoder
    \node[vision, right=0.8cm of sep2, minimum width=0.4cm, minimum height=0.3cm] (v) {};
    \node[right=0.1cm of v, font=\scriptsize] {Vision Encoder};
    
    % Separator
    \node[draw=none, right=0.3cm of v] (sep3) {};
    
    % Language Model
    \node[language, right=1.5cm of sep3, minimum width=0.4cm, minimum height=0.3cm] (l) {};
    \node[right=0.1cm of l, font=\scriptsize] {Language Model};
    
\end{tikzpicture}
};

\end{tikzpicture}
\caption{Training datasets, vision encoders, and language models for LLaVA, CogVLM2, InstructBLIP, and InternVL2. Non-QA datasets are connected with lighter lines. InternVL2 employs the most diverse QA datasets, enhancing its robustness. Connections to the \emph{ScienceQA} dataset are highlighted. See Appendix for details.}
\label{fig:model_comparison}
\vspace{-0.1in}
\end{figure*}



The performance of the VLMs is influenced by their training datasets and architectural designs. Figure~\ref{fig:model_comparison} summarizes the models' training datasets, vision encoders, and language models. Notably, some models, such as \emph{InternVL2}, are trained on the ScienceQA dataset, raising concerns about potential data contamination. Since the evaluation tasks may overlap with their training data, their performance metrics might be artificially inflated.

The \emph{InternVL2} models combine the InternViT vision encoder with the InternLM2 language model and are trained on a diverse set of datasets, including COCO, VQAv2, OKVQA, Visual Dialog, and ScienceQA. Similarly, \emph{LLaVA} models utilize the CLIP ViT-L/14 vision encoder and \emph{Vicuna} language models, trained on COCO and ScienceQA. In contrast, models like \emph{InstructBLIP} do not include ScienceQA in their training data. They use datasets such as COCO, VQAv2, OKVQA, and Visual Dialog, leveraging the CLIP ViT-G/14 vision encoder and \emph{Vicuna} language models. Their performance is less likely to be influenced by data contamination, providing a more accurate reflection of their capabilities on unseen data.

Overall, while diverse training data and sophisticated architectures contribute to model performance, the inclusion of evaluation datasets in training can artificially inflate results. It is crucial to consider potential data contamination when interpreting performance metrics to ensure fair and accurate assessments of model capabilities.