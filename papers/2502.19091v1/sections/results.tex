In this section, we present the experimental evaluation of the proposed Nexus framework. Our objective is to assess both the performance and robustness of the tool across a diverse set of benchmarks and use cases, thereby providing a comprehensive and unbiased analysis of its capabilities. The discussion is organized into three main case studies, each addressing a distinct use case.

\subsection{Methodology}

In all experiments, except those reported in Section~\ref{sec:ppa}, performance was evaluated using the {\em pass rate} (denoted by $\mathcal{A}$), which is defined as the ratio between the number of samples that pass all checks and the total number of samples in the benchmark suite. In some cases, such as with coding-related tasks, we distinguish between $\mathcal{A}_{s}$, i.e., the success rate of solutions passing all syntax checks, and $\mathcal{A}_{f}$, which reflects the success rate of designs that are not only syntactically correct but also functionally accurate. Notably, $\mathcal{A}_{f}$ was determined by executing the tests provided in the benchmark suite, ensuring a comprehensive validation of the proposed approach. For the agents, we employed Claude 3.5 Sonnet v1 or Claude 3.5 Sonnet v2~\cite{claude3.5}. Both models\footnote{Models accessed through AWS Bedrock, with the following identifiers: anthropic.claude-3-5-sonnet-20240620-v1:0 and anthropic.claude-3-5-sonnet-20241022-v2:0.} were configured with a {\em temperature} of 0.7 and a {\em top\_p} of 1.

\subsection{Case Study I: Coding Tasks}

In this section, we assess the effectiveness of the Nexus framework in addressing programming-related tasks. Our evaluation encompasses two benchmark families: HumanEval~\cite{chen2021evaluating}, a suite of 164 problems focused on Python code generation, and VerilogEval-Human~\cite{liu2023verilogeval}, which comprises 156 challenges involving Verilog code generation and verification. Notably, our approach leverages a single, unified Nexus architecture that is consistently applied across both sets of coding challenges.

\begin{figure}[h]
    \centering
    \includegraphics[width=.8\linewidth]{images/coding-tasks}
    \caption{Unified Nexus-based MAS architecture for solving code-related tasks.}
    \label{fig:coding_arch}
\end{figure}

As depicted in Figure~\ref{fig:coding_arch}, the proposed MAS architecture comprises the following core agents: ($i$) a {\em Coder} agent, responsible for generating code solutions (in either Python or Verilog) along with corresponding unit tests or testbenches; ($ii$) a {\em Reviewer} agent, tasked with reviewing and refining code to identify and correct syntax or compilation issues; and ($iii$) a {\em Verification} agent, which executes tests or simulations to assess functionality. As mentioned earlier, the overall structure remains consistent for both benchmark suites, with the specific tools employed (i.e., {\tt pytest} or {\tt iverilog}) chosen to suit the target domain. The workflow proceeds as follows:
\begin{enumerate}
    \item \textbf{Planning \& Task Delegation:} The {\em Supervisor} receives the user's prompt, decomposes the problem into multiple tasks, and assigns the {\em Coder} agent to initiate the first iteration.
    \item \textbf{Code Generation:} The {\em Coder} agent produces the Python (or Verilog) solution along with unit tests (or testbench), storing the output via the \texttt{save\_code} tool, a utility function defined as part of the {\em File System I/O} interface.
    \item \textbf{Syntax Adjustments:} The {\em Reviewer} analyzes the proposed implementation by leveraging the \texttt{get\_code} tool and attempts to execute or compile it using the Python interpreter (or {\tt iverilog}). If syntax errors are detected, corrective prompts are issued and sent back to the {\em Coder}, which iterates on the previous version to generate a revised solution.
    \item \textbf{Functional Adjustments:} Once the loop between the {\em Coder} and {\em Reviewer} concludes and the candidate solution is syntactically correct, the {\em Supervisor} delegates the {\em Verification} agent to run the unit tests using \texttt{pytest} (or again {\tt iverilog}) to assess functionality and correctness. In this stage, any errors are analyzed and communicated back to the {\em Coder} agent for further refinement.
    \item \textbf{Wrap-up:} The {\em Supervisor} collects the verified output and returns the final solution to the user.
\end{enumerate}

It is important to note that this workflow is considered {\em self-verifying}, meaning that the overall system autonomously devises tests without any external input.

\subsubsection{Ablation Study}
Table~\ref{tab:coding_results} summarizes the effectiveness of the proposed self-verifying workflow. For both benchmark suites, we report the Claude 3.5 version used in each experiment (column {\em LLM Version}), along with the syntax pass rate (columns $\mathcal{A}_{s}$) and functional pass rate ($\mathcal{A}_{f}$) achieved by both the baseline Claude 3.5 and by Nexus employing the same underlying LLM.

\begin{table}[!h]
    \centering
    \resizebox{.85\textwidth}{!}{
        \normalsize
        \addtolength\tabcolsep{8pt}
        \begin{tabular}{lrrrrrrr}\toprule
            \multirow{2}{*}{\textbf{Benchmark Suite}} &\multirow{2}{*}{\textbf{LLM Version}} &\multicolumn{2}{c}{\textbf{Baseline}} &\multicolumn{3}{c}{\textbf{Nexus (self-verifying)}} \\\cmidrule(lr{8pt}){3-4} \cmidrule(lr{8pt}){5-7}
            & &\textbf{\textbf{$\mathcal{A}_{s}$}} &\textbf{$\mathcal{A}_{f}$} &\textbf{$\mathcal{A}_{s}$} &\textbf{$\mathcal{A}_{f}$} &\textbf{$\Delta\mathcal{A}_{f}$} \\\midrule
            \textbf{HumanEval} &Claude 3.5 v1 &99.39 &87.80 &100 &96.95 &\textbf{\textcolor{blue}{$\uparrow$ 10.42\%}} \\
            \textbf{HumanEval} &Claude 3.5 v2 &98.78 &92.07 &100 &98.78 &\textbf{\textcolor{blue}{$\uparrow$ 7.28\%}} \\
            \textbf{VerilogEval-Human} &Claude 3.5 v2 &90.38 &67.30 &100 &85.90 &\textbf{\textcolor{blue}{$\uparrow$ 27.63\%}} \\
            \bottomrule
        \end{tabular}
        
    }
    \vspace{8pt}
    \caption{Ablation study results on pass rate, with column {\bf $\Delta\mathcal{A}_{f}$} reporting the percentage improvement of the proposed workflow over the corresponding baseline model in terms of functional pass rate. All values are expressed as percentages.}
    \label{tab:coding_results}
\end{table}

As can be observed, the proposed self-verifying workflow maximizes $\mathcal{A}_{s}$ across all scenarios. Moreover, it substantially enhances functional accuracy in every experiment, achieving a remarkable 27.63\% improvement on the VerilogEval-Human benchmark.

\subsubsection{Comparison with State-of-the-Art Approaches}
Applying LLMs to solve coding challenges is an emerging topic that has already captured significant attention, as these models are rapidly transforming how developers approach programming tasks on a daily basis~\cite{etsenake2024understanding}. Table~\ref{tab:coding_comp} compares the performance of the proposed workflow with the most recent and relevant solutions in this fast-evolving application field.

\begin{table}[!h]
    \centering
    \resizebox{.8\textwidth}{!}{
        \normalsize
        \addtolength\tabcolsep{8pt}
        \begin{tabular}{lrrrrr}\toprule
            \textbf{Benchmark Suite} &\textbf{Technology} &\textbf{Self Verifying} &\textbf{Model} &\textbf{$\mathcal{A}_{f}$} \\\midrule
            \multirow{8}{*}{\textbf{HumanEval}} &L2MAC~\cite{holt2023l2mac} &Yes &GPT-4 &90.2 \\
            &MapCoder~\cite{islam2024mapcoder} &Yes &GPT-4 &93.9 \\
            &AgentCoder~\cite{huang2023agentcoder} &Yes &GPT-4 &96.3 \\
            &LLMDebugger~\cite{zhong2024ldb} &Yes &GPT-4o &98.2 \\
            &LPW~\cite{lei2024planning} &Yes &GPT-4o &98.2 \\
            &QualityFlow~\cite{hu2025qualityflow} &Yes &Claude 3.5 &98.8 \\
            &\cellcolor{blue!25}\textbf{Nexus (this work)} &\cellcolor{blue!25}\textbf{Yes} &\cellcolor{blue!25}\textbf{Claude 3.5} &\cellcolor{blue!25}\textbf{98.8} \\
            &LLMDebugger~\cite{zhong2024ldb} &Yes &o1 &99.4 \\ \midrule\midrule
            \multirow{9}{*}{\textbf{VerilogEval-Human}} &RTLFixer~\cite{tsai2024rtlfixer} &Yes &GPT-3.5 &36.8 \\
            &VeriAssist$^{*}$~\cite{huang2024towards} &Yes &GPT-4 &48.3 \\
            &AIvril~\cite{sami2024aivril} &Yes &Claude 3.5 &67.3 \\
            &AIvril2~\cite{sami2024eda} &Yes &Claude 3.5 &77 \\
            &\cellcolor{blue!25}\textbf{Nexus (this work)} &\cellcolor{blue!25}\textbf{Yes} &\cellcolor{blue!25}\textbf{Claude 3.5} &\cellcolor{blue!25}\textbf{85.9} \\\cmidrule(lr){2-5}
            &VeriAssist$^{*}$~\cite{huang2024towards} &No &GPT-4 &50.5 \\
            &VerilogCoder~\cite{ho2024verilogcoder} &No &GPT-4 &94.2 \\
            &MAGE~\cite{zhao2024mage} &No &Claude 3.5 &94.8 \\
            &\cellcolor{blue!25}\textbf{Nexus (this work)} &\cellcolor{blue!25}\textbf{No} &\cellcolor{blue!25}\textbf{Claude 3.5} &\cellcolor{blue!25}\textbf{100} \\
            \bottomrule
            \end{tabular}
    }
    \vspace{8pt}
    \caption{Comparison of the proposed {\em self-verifying} and {\em non-self-verifying} workflows based on Nexus versus relevant existing solutions. HumanEval numbers have been gathered from the Papers With Code leaderboard~\cite{humaneval_leaderboard}. ($^{*}$) This work employs a dual-mode verification mechanism.}
    \label{tab:coding_comp}
\end{table}

For a fair comparison, we divide the results into three main categories: ($i$) self-verifying MASs designed to solve software programming tasks, as measured by the HumanEval benchmark suite; ($ii$) self-verifying MASs intended to produce and autonomously verify Verilog solutions to help hardware engineers meet their stringent domain requirements; and ($iii$) non-self-verifying MASs that, in addition to the user prompt, incorporate guidance on how to verify the RTL produced by the LLM. For the latter, we introduce a slightly revised workflow compared to the one presented earlier in this section. Instead of generating the testbench autonomously, this approach uses the testbench provided in the benchmark suite as a blueprint to create its own version, following a principle similar to that adopted in previous works~\cite{huang2024towards,ho2024verilogcoder,zhao2024mage}. As the numbers indicate, on HumanEval the proposed workflow ranks {\em ex aequo} in second place behind LLMDebugger~\cite{zhong2024ldb}. This result is particularly noteworthy given that the same workflow, when applied to a completely different programming language, and thus operating sub-optimally relative to its intended domain, remains effective despite being orthogonal to the programming language stack. As a result, when considering the VerilogEval-Human benchmark suite, both the {\em self-verifying} and {\em non-self-verifying} workflows outperform existing solutions. Notably, the {\em non-self-verifying} version achieves a remarkable 100\% accuracy\textemdash a feat that, to the best of the authors' knowledge, has never been achieved before.

\begin{figure}[h]
    \centering
    \includegraphics[width=.5\linewidth]{images/reasoning_comp}
    \caption{Comparison of reasoning models and the Nexus self-verifying workflow for code-related tasks.}
    \label{fig:reasoning_comp}
\end{figure}

Figure~\ref{fig:reasoning_comp} presents another comparison, this time against emerging {\em reasoning} models\footnote{We originally planned to include DeepSeek-R1~\cite{guo2025deepseek} in our evaluations; however, due to access limitations with DeepSeek's official API, we were only able to obtain results for the VerilogEval-Human dataset, where DeepSeek-R1 achieved a pass rate of 65.38\%.}, i.e., complex systems that leverage self-reflective CoT mechanisms to autonomously decompose tasks into iterative intermediate steps, thereby yielding enhanced accuracy and cost efficiency in solving multi-step problems. In particular, we analyze the recently released o3-mini~\cite{o3mini} from OpenAI\footnote{At the time of writing, o3-mini is the most advanced reasoning model available through the OpenAI API platform.} on both HumanEval and VerilogEval-Human. We consider three levels of reasoning effort\textemdash low, medium, and high\textemdash and compare the resulting pass rates against our proposed self-verifying workflow.

As can be noted, our approach is on par with o3-mini on HumanEval, where all solutions achieve a pass rate exceeding 97\%. In contrast, on VerilogEval-Human, our workflow substantially outperforms all three versions of o3-mini, with the largest margin observed against o3-mini (low) with a 33\% higher pass rate. In conclusion, Nexus enables users to craft workflows that not only match or exceed the performance of state-of-the-art models but also require significantly less effort to create and deploy.

\subsection{Case Study II: Math \& Reasoning Tasks}

To demonstrate the effectiveness of Nexus in solving complex mathematical problems, we conducted a case study using the MATH dataset~\cite{hendrycks2021measuring}. This dataset poses significant challenges for LLMs, particularly due to their limited ability to perform precise calculations without the assistance of external tools.

We devised the Nexus workflow depicted in Figure~\ref{fig:math_arch}, which comprises a {\em Supervisor}, a {\em Mathematician} agent, and a {\em Reviewer} agent, all powered by Claude 3.5 v2. The {\em Supervisor} orchestrates the overall problem-solving process, the {\em Mathematician} generates solutions using the {\tt SymPy} Python package~\cite{meurer2017sympy} for symbolic mathematics, and the {\em Reviewer} evaluates the solutions.

\begin{figure}[h]
    \centering
    \includegraphics[width=.5\linewidth]{images/math-tasks}
    \caption{Proposed Nexus-based MAS architecture for solving problems from the MATH dataset.}
    \label{fig:math_arch}
\end{figure}

The overall workflow can be detailed as follows:

\begin{enumerate}
\item \textbf{Problem Intake \& Task Assignment:} The {\em Supervisor} receives the math problem from the user, unravels its execution, and assigns tasks to the {\em Mathematician} agent.
\item \textbf{Solution Generation:} The {\em Mathematician} employs {\tt SymPy} to derive a solution.
\item \textbf{Solution Evaluation:} After generating the solution, the {\em Supervisor} forwards both the original problem and the proposed solution to the {\em Reviewer}.
\item \textbf{Feedback and Correction:} The {\em Reviewer} assesses the solution and provides detailed feedback. The {\em Supervisor} then uses this feedback to request corrections if necessary, thereby iterating the process until the solution meets the desired accuracy.
\end{enumerate}

\begin{table}[!h]
    \centering
    \resizebox{.9\textwidth}{!}{
        \normalsize
        \addtolength\tabcolsep{8pt}
        \begin{tabular}{lrrr}\toprule
            \textbf{Problem \#ID} & \textbf{Baseline} & \textbf{Nexus} & \textbf{Observations} \\\midrule
            \textbf{Number Theory \#227} & Failed~\testfailed & Passed~\testpassed & {\tt SymPy} tool crucial \\
            \textbf{Algebra \#2} & Failed~\testfailed & Passed~\testpassed & Baseline far from accurate \\
            \textbf{Geometry \#1140} & Passed~\testpassed & Passed~\testpassed & - \\
            \textbf{Intermediate Algebra \#24256} & Passed~\testpassed & Passed~\testpassed & - \\
            \textbf{Counting and Probability \#25780} & Failed~\testfailed & Passed~\testpassed & Reviewer identified logical flaws \\
            \bottomrule
            \end{tabular}
    }
    \vspace{8pt}
    \caption{Ablation study on the MATH dataset. Columns {\em Baseline} and {\em Nexus} report the assessment of the final answer, comparing it to the ground truth response for both Claude 3.5 v2 and our proposed architecture, respectively. Column {\em Observations} highlights the most significant aspect that emerged from each experiment.}
    \label{tab:math_results}
\end{table}

As summarized in Table~\ref{tab:math_results}, we evaluated this workflow on five randomly selected level-5 problems (i.e., the most challenging problems) drawn from various categories of the MATH dataset. The annotated results indicate whether each system's final output passed the validity check and was deemed correct. The results demonstrate that the proposed Nexus-based workflow successfully solved all five problems, whereas the baseline LLM correctly solved only two.

To better illustrate the effectiveness of the employed workflow in greater detail, let us focus on problem \#25780 from the Counting and Probability category. This problem presents a complex scenario involving probabilistic elevator movements in a 15-story hotel, and its complete definition is provided below:

\begingroup
\renewcommand{\theproblem}{\#25780}
\begin{problem}
    While staying in a 15-story hotel, Polya plays the following game. She enters an elevator on the 6th floor. She flips a fair coin five times to determine her next five stops. Each time she flips heads, she goes up one floor. Each time she flips tails, she goes down one floor. What is the probability that each of her next five stops is on the 7th floor or higher? Express your answer as a common fraction.
\end{problem}
\endgroup

This problem challenges the solver to account for several key aspects: the starting position (6th floor), the probabilistic nature of movements determined by coin flips, the constraint of remaining on the 7th floor or higher for all five stops, and the need to compute the probability of all valid sequences. In essence, the complexity arises from the interplay between the random walks induced by coin flips and the geometric constraints imposed by the floor numbers, thus resulting in a particularly challenging task for typical LLMs.

In this scenario, the baseline LLM erroneously concluded that all five moves must be upward, resulting in a probability of $(\frac{1}{2})^5 = \frac{1}{32}$. The model's reasoning is summarized as follows:

\begin{thinking}[CoT Excerpt\textemdash Baseline (Claude 3.5 v2).]
Assuming each heads (H) moves up one floor and each tails (T) moves down one floor, to ensure we're always at floor 7 or higher:
\begin{itemize}
    \item First flip must be H
    \item Second flip must be H
    \item Third flip must be H
    \item Fourth flip must be H
    \item Fifth flip must be H
\end{itemize}
We need all heads for this to work. Any tails at any point would drop us below floor 7 at some step.
\end{thinking}

This demonstrates the limitations of LLMs in complex probability calculations without external aids. The baseline model failed to consider valid scenarios where some downward movements are possible while still maintaining the condition of staying on the 7th floor or higher. In contrast, the proposed Nexus-based workflow, while initially struggling, eventually produced the correct answer. What follows is the last review phase submitted by the {\em Reviewer} agent to the {\em Supervisor}:

\begin{thinking}[CoT Excerpt\textemdash Nexus (Reviewer)]
After carefully reviewing this final solution, here is my detailed analysis:

\begin{enumerate}[label=\arabic*.]
    \item \textbf{Systematic Sequence Analysis:}
    \begin{itemize}[label={[\texttt{OK}]}]
        \item Correctly starts with H (required to reach 7th floor)
        \item Systematically examines all possible sequences starting with H
        \item Properly tracks floor numbers at each stop
        \item Good organization showing each sequence's path
    \end{itemize}
    
    \item \textbf{Validity Check of Each Sequence:}
    \begin{itemize}[label={[\texttt{OK}]}]
        \item HHHHH: 7,8,9,10,11 (all stops $\geq 7$)
        \item HHHHT: 7,8,9,10,9 (all stops $\geq 7$)
        \item HHHTH: 7,8,9,8,9 (all stops $\geq 7$)
        \item HHHTT: 7,8,9,8,7 (all stops $\geq 7$)
        \item HHTHH: 7,8,7,8,9 (all stops $\geq 7$)
        \item HHTHT: 7,8,7,8,7 (all stops $\geq 7$)
        \item Correctly identifies invalid sequences (HHTTH, HHTTT, HTHXX, HTTXX)
    \end{itemize}
    
    \item \textbf{Completeness Check:}
    \begin{itemize}[label={[\texttt{OK}]}]
        \item All possible sequences starting with H are accounted for
        \item Correctly eliminates sequences going below 7th floor
        \item Properly branches through all possibilities
        \item No valid sequences appear to be missed
    \end{itemize}
    
    \item \textbf{Verification Process:}
    \begin{itemize}[label={[\texttt{OK}]}]
        \item Shows floor numbers at each stop
        \item Clear marking of valid (\texttt{[OK]}) and invalid (\texttt{[X]}) sequences
        \item Explains why invalid sequences are rejected
        \item Demonstrates that all 6 valid sequences stay $\geq 7$th floor
    \end{itemize}
    
    \item \textbf{Probability Calculation:}
    \begin{itemize}[label={[\texttt{OK}]}]
        \item Correct denominator: $2^5 = 32$ (all possible 5-move sequences)
        \item Correct numerator: 6 (verified valid sequences)
        \item Correct reduction: $\frac{6}{32} = \frac{3}{16}$
    \end{itemize}
\end{enumerate}

\bigskip

\textbf{Overall Assessment:}\\[0.5em]
This solution is mathematically correct and complete. It:
\begin{itemize}
    \item Uses a systematic approach
    \item Correctly identifies all valid sequences
    \item Properly verifies the $\geq 7$th floor requirement
    \item Accurately calculates the final probability
    \item Provides clear reasoning and documentation
\end{itemize}

The final answer of $\frac{3}{16}$ is correct and properly justified.
\end{thinking}

As can be noted, the {\em Reviewer} verified that each valid sequence maintains the required height condition and confirmed the soundness of the reasoning and methodology. As a result, the {\em Mathematician}, aided by {\tt SymPy}, correctly determined the probability as 6 (valid sequences) out of 32 (total possible sequences), resulting in $\frac{6}{32} = \frac{3}{16}$.

The complete problem-solving process, including intermediate steps and agent interactions, is presented in Appendix~\ref{appendix:math-problem}, while the complete architecture definition is reported in Appendix~\ref{appendix:yaml-math}. The results demonstrate that Nexus can significantly outperform standalone Claude 3.5 Sonnet in solving complex mathematical problems, showcasing its potential for enhancing AI-driven problem-solving in various domains. By leveraging the strengths of the underlying LLM within a structured multi-agent framework and incorporating external tools, Nexus achieves a level of mathematical problem-solving capability that surpasses the baseline model's performance.

\subsection{Case Study III: Automated Timing Closure in EDA Applications}\label{sec:ppa}
Achieving timing closure and optimizing resource utilization are fundamental challenges in hardware design. In practice, synthesis, placement, and routing strategies are routinely employed to balance stringent timing constraints with efficient hardware resource usage, a balance that is critical for the successful deployment of complex applications on modern computer architectures. To assess the efficacy of our Nexus framework, we conducted extensive experiments using benchmark designs from the well-established VTR benchmark suite~\cite{murray2020vtr}. These benchmarks span a diverse range of application domains, including computer vision ({\tt stereovision0}, {\tt stereovision1}), signal processing ({\tt diffeq1}, {\tt diffeq2}), cryptography ({\tt sha}), and various encoding-decoding applications. In our experimental setup, we leveraged the Xilinx Vivado 2020 Design Suite (hereafter, Vivado) for synthesis, placement, and routing, with all implementations targeting the Xilinx Alveo U200 card.

\begin{figure}[h]
    \centering
    \includegraphics[width=.5\linewidth]{images/synth-tasks}
    \caption{Proposed Nexus architecture for achieving timing closure and design optimization in EDA.}
    \label{fig:synth-tasks}
\end{figure}

For the baseline, the designs were synthesized and mapped by specifying only its target timing constraints, without employing additional optimization techniques. On the other hand, Figure~\ref{fig:synth-tasks} illustrates the adopted Nexus-based architecture, in which all agents are powered by Claude 3.5 v2. The workflow is summarized as follows:
\begin{itemize}
\item \textbf{User Input:} The process begins with the user providing the RTL code along with a prompt that details the timing constraints and optimization goals.
\item \textbf{Analysis:} The \emph{Supervisor} processes these inputs to generate design constraints and commands specifically tailored to meet the timing requirements using Vivado.
\item \textbf{Execution:} The generated constraints and commands are forwarded to the \emph{EDA} agent, which writes them to tool-specific files (e.g., {\tt .xcd} for constraints and {\tt .tcl} for commands) and interfaces directly with the Vivado \emph{Command-Line Interface} (CLI). The \emph{EDA} agent then issues these commands and retrieves reports that provide detailed information on resource utilization, power consumption, timing metrics, and critical paths.
\item \textbf{Feedback:} The \emph{Supervisor} reviews the reports and iteratively refines the optimization strategy until timing closure is achieved or no further improvements are possible.
\end{itemize}

Table~\ref{tab:synth_results} reports the figures of merit for each benchmark. In particular, the column {\em Frequency} reports the target frequency for each benchmark, columns {\em LUTs} and {\em FFs} report resource utilization, while column {\em WSN} reports the worst negative slack and the last column reports the total power. As can be noted, across all benchmarks, the proposed Nexus-based architecture not only achieved timing closure at the target frequencies, but it also delivered significant improvements in key metrics, with an average LUT reduction of 26.64\% and a power reduction of nearly 30\%.

\begin{table}[!h]
    \centering
    \resizebox{\textwidth}{!}{
        \huge
        \addtolength\tabcolsep{8pt}
        \begin{tabular}{lrrrrrrrrrr}\toprule
            \multirow{2}{*}{\textbf{Design}} &\multirow{2}{*}{\textbf{Frequency (MHz)}} &\multicolumn{2}{c}{\textbf{LUTs}} &\multicolumn{2}{c}{\textbf{FFs}} &\multicolumn{2}{c}{\textbf{WNS (ns)}} &\multicolumn{2}{c}{\textbf{Power (W)}} \\\cmidrule(lr{8pt}){3-4} \cmidrule(lr{8pt}){5-6} \cmidrule(lr{8pt}){7-8} \cmidrule(lr{8pt}){9-10}
            & &\textbf{Baseline} &\textbf{Nexus} &\textbf{Baseline} &\textbf{Nexus} &\textbf{Baseline} &\textbf{Nexus} &\textbf{Baseline} &\textbf{Nexus} \\\midrule
            \textbf{diffeq1} &150 &357 &345 &209 &209 &-0.187~\testfailed &0.022~\testpassed &2.634 &2.6 \\
            \textbf{blob\_merge} &200 &5400 &5227 &575 &575 &0.402~\testpassed &0.0384~\testpassed &2.512 &2.51 \\
            \textbf{stereovision0} &333 &3959 &3176 &10290 &7595 &0.5~\testpassed &0.313~\testpassed &2.995 &3 \\
            \textbf{stereovision1} &200 &13321 &1281 &11843 &6186 &1.269~\testpassed &1.42~\testpassed &2.963 &2.9 \\
            \textbf{diffeq2} &167 &229 &232 &111 &111 &0.011~\testpassed &0.032~\testpassed &2.618 &2.621 \\
            \textbf{sha} &300 &1031 &998 &895 &895 &0.3~\testpassed &0.298~\testpassed &2.577 &2.551 \\
            \textbf{stereovision2} &154 &9862 &8062 &13589 &17619 &0.403~\testpassed &0.8~\testpassed &3.268 &3.14 \\
            \textbf{stereovision3} &500 &57 &78 &99 &144 &0.9~\testpassed &0.806~\testpassed &2.493 &0.61 \\
            \textbf{mkPktMerge} &500 &12 &16 &16 &16 &0.389~\testpassed &0.174~\testpassed &3.904 &2.01 \\
            \textbf{mkSMAdapter4B} &200 &910 &885 &859 &865 &1.363~\testpassed &0.92~\testpassed &2.612 &2.62 \\
            \textbf{LU8PEEng} &65 &14581 &13740 &5703 &3569 &-11.409~\testfailed &0.24~\testpassed &2.905 &0.98 \\
            \textbf{bgm} &200 &10801 &10292 &5063 &6150 &0.257~\testpassed &0.133~\testpassed &2.947 &0.98 \\
            \textbf{boundtop} &770 &221 &218 &205 &444 &0.459~\testpassed &0.279~\testpassed &2.5 &0.685 \\
            \textbf{ch\_intrinsics} &1250 &25 &25 &90 &122 &-0.029~\testfailed &0.018~\testpassed &2.68 &0.768 \\\midrule
            \textcolor{blue}{\textbf{$\Delta$Avg.}} & & &\textcolor{blue}{\textbf{-26.64\%}} & &\textcolor{blue}{\textbf{-10.19\%}} & & & &\textcolor{blue}{\textbf{-29.37\%}} \\
            \bottomrule
            \end{tabular}
    }
    \vspace{8pt}
    \caption{Results for timing closure tasks. Symbols in column {\tt WSN} indicate whether timing constraints were met (\testpassed) or not (\testfailed).}
    \label{tab:synth_results}
\end{table}

Among the benchmarks, the {\tt LU8PEEng} case study stands out as a compelling demonstration of our proposed architecture's optimization capabilities. This design posed significant challenges for the baseline strategy, with a WNS of -11.409ns observed at 65MHz. In contrast, our architecture achieved a positive slack of 0.24ns while maintaining the target frequency. Additionally, the framework improved resource utilization: LUT usage decreased by 5.77\% (from 14,581 to 13,740) and flip-flop utilization dropped by 37.42 (from 5,703 to 3,569). This dual optimization of timing and resources highlights the framework's ability to effectively navigate complex design trade-offs. Furthermore, the architecture strategically reallocated block RAM resources, increasing BRAM utilization from 42 to 71 units while maintaining DSP usage at 16 units, to achieve optimal implementation. Notably, these improvements were accompanied by a substantial 3$\times$ reduction in power consumption, lowering it from 2.9W to 0.98W. This significant power optimization, alongside enhanced timing and maintained functionality, underscores the architecture's ability to efficiently exploit advanced features in professional-grade EDA tools, thereby enabling the simultaneous achievement of multiple competing optimization objectives without human intervention. Detailed step-by-step interactions observed in this experiment are provided in Appendix~\ref{appendix:ppa-chat}.