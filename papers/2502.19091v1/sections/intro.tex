Since their inception in the 1980s, {\em Multi-Agent Systems} (MASs) have become foundational in {\em Distributed Artificial Intelligence}, enabling the decomposition of complex tasks into smaller, more manageable components executed by autonomous agents~\cite{vlassis2022concise,dorri2018multi}. These agents draw on historical knowledge, interactions with other agents, and environmental cues to make decisions and act autonomously. This built-in autonomy and flexibility distinguish MASs from traditional distributed problem-solving systems, enhancing their ability to operate effectively in dynamic and uncertain environments. Consequently, MASs have been widely applied in fields as diverse as robotic control~\cite{czimmermann2021autonomous,cena2013cooperative,verma2021multi}, traffic management~\cite{balaji2007multi,hamidi2018approach}, smart grids~\cite{shobole2021multiagent,merabet2014applications}, network security~\cite{gorodetski2003multi,herrero2009multiagent,gorodetski2002multi}, and many others~\cite{dorri2018multi}.

Despite their versatility, conventional MAS architectures have historically relied on predefined rules and heuristic-driven approaches for coordination and decision-making. However, coordination among agents, secure task allocation, and scalability in large systems still remain critical challenges. To address these issues, researchers have explored several methods such as leader-follower hierarchies, wherein leader agents define global objectives and delegate subtasks, and middle-agent frameworks that streamline service discovery and coordination among agents~\cite{dorri2018multi}. Generally speaking, all these contributions fall under the umbrella of traditional MAS techniques. Indeed, recent progress in {\em Large Language Models} (LLMs) is now rapidly reshaping the MAS landscape, equipping systems with {\em near-human} reasoning capabilities. When integrated into MASs, LLMs can serve as central reasoning agents, substantially enhancing adaptability, collaboration, and decision-making in dynamic environments. Consequently, such advancements have recently propelled MAS applications into areas such as multimodal reasoning, autonomous GUI navigation, and complex mathematical problem-solvingâ€”tasks that were once beyond the scope of traditional MAS approaches~\cite{wang2024survey, zhuge2023mindstorms}.

To capitalize on these breakthroughs, LLM-based MASs typically rely on two core principles: ($i$) a robust, task-specific architecture that maximizes the effectiveness of LLMs, and ($ii$) custom methodologies to embed domain-specific knowledge and adaptive strategies within and among the agents and their surrounding environment. However, prescribing architectural designs {\em a priori} can constrain scalability and limit adaptability across different domains. Moreover, integrating LLMs with external knowledge and tools adds another layer of non-negligible complexity. Lastly, developing an LLM-based MAS from scratch typically presents a steep learning curve, posing significant development and usability challenges, especially for non-experts.

In this paper, we introduce Nexus, a novel open-source Python framework that allows users to easily design MAS architectures in a low-code fashion. In addition, Nexus is lightweight, scalable, and orthogonal to both LLMs and application domains, thereby enabling intelligent automation across a wide variety of tasks. The contributions of our work can be summarized as follows:

\setlist{nolistsep}
\begin{enumerate}[noitemsep]
\item We introduce a flexible multi-supervisor hierarchy for efficient and scalable task delegation among agents. This hierarchy consists of a single root {\em Supervisor} agent that acts as a global orchestrator, along with dedicated {\em Task Supervisors} distributed throughout the MAS structure. This design readily supports a divide-and-conquer approach to solving complex problems while minimizing design effort.
\item We enable support for architecture design and workflow definition using simple, plain-text YAML files, streamlining system design and eliminating the need for extensive programming expertise.
\item We evaluate Nexus-designed architectures on three different domains of application: ($i$) {\bf coding tasks}, where we assess the ability of our MASs to produce correct code in both Python (using the HumanEval dataset~\cite{chen2021evaluating}) and Register-Transfer Level (RTL) design (using the VerilogEval-Human dataset~\cite{liu2023verilogeval}); ($ii$) complex {\bf math problem solving}, by randomly selecting five difficult-level problems from the MATH dataset~\cite{hendrycks2021measuring} to evaluate reasoning and problem-solving capabilities; and ($iii$) {\bf optimization in {\em Electronic Design Automation} (EDA)}, by challenging our framework with a compact architecture capable of autonomously performing timing closure on a set of industry-standard designs from the VTR benchmark suite~\cite{murray2020vtr}.
\item We release the source code of this project on GitHub at \url{https://github.com/PrimisAI/nexus} under a permissive open-source license.
\end{enumerate}

Experimental results demonstrate that Nexus empowers MAS architectures to achieve state-of-the-art performance across multiple domains. In coding tasks, the system attains a 99\% pass rate on HumanEval and 100\% on VerilogEval-Human, thus outperforming recent reasoning language models like o3-mini~\cite{o3mini} and DeepSeek-R1~\cite{guo2025deepseek}, while, at the same time, also excelling in complex reasoning and math problem solving. Moreover, Nexus-based architectures effectively tackle challenging timing closure tasks in EDA, achieving multi-objective optimizations that yield an average power saving of nearly 30\%.

The remainder of the paper is organized as follows: Section~\ref{sec:background} provides a brief introduction to MASs and their evolution from heuristics-based agents to LLM-based agents. Section~\ref{sec:methods} details the internal mechanisms of the proposed Nexus framework, highlighting key differences with respect to existing solutions. Section~\ref{sec:results} illustrates the experimental results, while Section~\ref{sec:conclusions} concludes the paper with a summary of the contributions and findings.