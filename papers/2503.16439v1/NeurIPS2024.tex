\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024


% ready for submission
%\usepackage{neurips_2024}
\usepackage{graphicx}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
\usepackage[final]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2024}
%usepackage{natbib}

\bibliographystyle{abbrvnat}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{amsmath}
\usepackage{paralist}
\usepackage{comment}
\usepackage[affil-it]{authblk}
%\title{Affective Dream Reliving with 3D Generative AI and LLM}
\title{DreamLLM-3D: Affective Dream Reliving using Large Language Model and 3D Generative AI}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author[1]{Pinyao Liu}
\author[2]{Keon Ju Lee}
\author[1]{Alexander Steinmaurer}
\author[3]{Claudia Picard-Deland}
\author[3]{Michelle Carr}
\author[2]{Alexandra Kitson}

\affil[1]{Interdisciplinary Transformation University Austria}
\affil[2]{Simon Fraser University}
\affil[3]{Université de Montréal}


\begin{document}


\maketitle

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{Teaser_wide.JPG}
\caption{During the dream-reliving experience, the immersant whispers their dream into the system. The spoken dream characters and objects are then represented as 3D point clouds in the shared immersive space.}
\label{fig:teaser}
\end{figure}
\begin{abstract}
%Dreaming is a fundamental component of the human experience and creativity. Modern-day psychologists and neuroscientists use “dreamwork” to describe a variety of strategies that deepen and engage with dreams for personal insights. Re-experiencing the dream as if reliving the memory, feelings, and bodily sensations from the dream is a key element shared by many dreamwork practices.
We present DreamLLM-3D, a composite multimodal AI system behind an immersive art installation for dream re-experiencing. It enables automated dream content analysis for immersive dream-reliving, by integrating a Large Language Model (LLM) with text-to-3D Generative AI. The LLM processes voiced dream reports to identify key dream entities (characters and objects), social interaction, and dream sentiment. The extracted entities are visualized as dynamic 3D point clouds, with emotional data influencing the color and soundscapes of the virtual dream environment. Additionally, we propose an experiential AI-Dreamworker Hybrid paradigm. Our system and paradigm could potentially facilitate a more emotionally engaging dream-reliving experience, enhancing personal insights and creativity. 
\end{abstract}


\section{Introduction}
% introduction to dreamwork
%\subsection{Experiential Dreamwork and Analytical Dream Analysis}

Dreams have a long history of inspiring creativity. Salvador Dalí's iconic melting clocks were inspired by a dream that embodies the fluidity of time and reality, while Niels Bohr discovered the structure of the atom and credited a dream where he saw the electrons revolving around the nucleus like the solar system.  Attaining personal insights from interpreting dreams has been the subject of extensive interest for centuries. Psychologists and neuroscientists have put forth several "dreamwork" strategies that support people in deeply engaging with their dreams \citep{hill_use_2010}. One of the more salient strategies includes re-experiencing the dream as if reliving the memory, feelings, and bodily sensations from the dream \citep{ellis_common_2019}.

In addition to the experiential approach of dreamwork, a more quantitative approach has been used for dream content analysis. One of the most widely used dream content analysis systems is the Hall–Van de Castle (HVDC) scheme \citep{hall_content_1966}. It consists of ten categories of elements appearing in dreams (see Section 2), together with detailed rules to recognize and measure those elements from written reports. 

%\subsection{AI-assisted Dream Research}

In dream content analysis, manually scoring dreams according to coding rules can be very time-consuming \citep{fogli_our_2020}. AI, therefore, could be helpful to automate the process. More and more dream researchers have been deploying automation and AI for dream analysis -- such as automatically scoring dream reports by operationalizing the HVDC coding rules \citep{fogli_our_2020}, mapping dream contents for semantic structures of characters, activities, and setting using a transformer based Natural Language Processing architecture \citep{gutman_music_mapping_2022}, and automatic scoring of dream reports with LLMs \citep{bertolini_automatic_2023,picard-deland_c_vestibular_2023}. These AI-assisted dream content analyses have mainly been used by researchers to discover themes and patterns among a vast database of dream reports but are rarely used in the dreamwork context to attain personal insights.

Relevant to experiential dreamwork, researchers are exploring AI for dream imagery visualization and re-experiencing. Some directly look at how AI can help decode fMRI signals and reconstruct visual images \citep{miyawaki_visual_2008,kay_identifying_2008,takagi_high-resolution_2023}, and further decode visual imagery during sleep \citep{horikawa_neural_2013} and dreamed objects \citep{horikawa_hierarchical_2017}. Other researchers have started to explore Generative AI for dream image visualization, mostly using text-to-2D AI models to visualize dreamers' dream record \citep{canet_sola_dream_2022,fuse_onirica_nodate,liu_falling_2024}. More recently, \citet{liu_virtual_2024} explored text-to-3D AI for re-experiencing dreams through transforming whispered dream speech into 3D dream objects in Virtual Reality. However, this dream-reliving system does not account for the rich emotional and social information from the dream narration. Emotions are suggested an important component of most dream experiences and may enhance dream recall \citep{blagrove_relationship_2004}. Moreover, Social Simulation Theory (SST) postulates that dreams virtually simulate socially significant interactions, bonds, and networks during our waking lives \citep{revonsuo_avatars_2015}. By incorporating emotional and social information into the immersive re-experiencing of dream imagery, we could potentially allow for a more affective dream-reliving experience for deepened personal insights. 



In summary, we see the potential of AI-assisted dream content analysis to enhance experiential dreamwork. We propose to bridge the divide between the experiential and the analytical with a multimodal AI system that combines dream content analysis and dream imagery re-experiencing, using text-to-3D generative diffusion model and LLM. We further propose an experiential AI-Dreamworker Hybrid paradigm and discuss its ethical implications. Our system and paradigm could potentially facilitate a more emotionally engaging dream-reliving experience, supporting deeper personal insights, while provoking meaningful discussions within dream and AI research communities. 

\section{AI-assisted Affective Dream Reliving Overview}
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{DreamGPT_Framework_New4.png}
    \caption{A composite multimodal AI system performing real-time dream content analysis for immersive dream-reliving, using LLM and 3D generative AI. }
    \label{fig:mechanism}
\end{figure}
We developed an LLM-assisted dream content analysis system behind an immersive art installation \textbf{for} an affective dream-reliving experience inspired by the HVDC scheme. The HVDC scheme consists of ten categories: 
\begin{inparaenum}
  \item Characters
  \item Social Interactions
  \item Emotions
  \item Activities (walking, talking, thinking, etc.)
  \item Success and Failure
  \item Misfortune and Good Fortune
  \item Settings and Objects
  \item Descriptive Elements (modifiers, time, negatives)
  \item Food and Eating
  \item Elements from the Past
\end{inparaenum} 
\citep{hall_content_1966}. \citet{domhoff_scientific_2003} suggests that the three categories of characters, social interactions and emotions are the most valuable and informative, which we thus chose to output as structured information from each dream.

During the dream-reliving experience, the immersant whispers their dream into the system through a microphone (see Figure \ref{fig:teaser}). In each whispering session, the system activates speech recognition when detecting voice input and stops when sensing a pause or stop in the vocal narration. Each whispering input usually contains one or two sentences of a dream report. An LLM pipeline analyzes and categorizes key dream entities, sentiments, and social interaction in real-time. The key dream entity information is output as automatized prompts for the text-to-3D diffusion model. The social interaction information is mapped into the soundscape, while the sentiment information is mapped into both the visual (color and motion of 3D generative visualization) and the soundscape. The immersant then re-experiences one's dream memories through AI-generated 3D dream objects and characters, with affective color mapping and context-aware soundscape.

Below we detail the three LLM dream analysis modules, and their integration with the text-to-3D model. We further detail the affective color mapping, and the context-aware soundscape system. 
\subsection{LLM System}

\textbf{Technical Setup}
We used a local LLM Mistral 7B \citep{jiang_mistral_2023} with temperature $T = 0$. Additionally, we used Nomic-Embed-Text \citep{nussbaum_nomic_2024} as our embedding model. %over traditional Sentence Transformers, since it supports longer contexts and flexible embedding dimensions through Matryoshka Representation Learning for considerably faster processing. We set the dimensionality parameter set to 768 when initializing the embeddings for high performance.
We then used Chroma as the search library, which natively supports embedding normalization and cosine similarity search, since we are interested in the association of terms with social interactions (the direction of vectors).%, not the absolute values of vectors. 

We used a generative diffusion text-to-3D model \textit{Point-E} \citep{nichol_point-e_2022} for dream imagery visualization, which produces a 3D point cloud representation of dream entities (characters and objects) in about 17 seconds on an A100 GPU. These 3D point clouds are then parsed in the Unity3D engine for real-time rendering.  

%The sentiment and social interaction modules further analyze the dream content and influence the audio-visual experience in real-time. 

\textbf{Dream Entity Extraction Module}
The HVDC scheme only classifies living entities as characters, and views non-living dream objects as tangible 'props' - part of the physical environment that these living entities interact with \citep{hall_content_1966}. However, Hall also pointed out that "A dream symbol is an image, usually a visual image, of an object, activity, or scene; the referent for the symbol is a conception" \citep{hall_cognitive_1953}. This suggests the potential significant symbolic meaning attached to a non-living entity in dreams. Therefore, simplifying dream objects as physical props might lead to the loss of significant symbolic meanings in an experiential dreamwork approach. Thus, we placed the significance of non-living objects as equal to that of living humans/animals/fictional figures and used the term \textit{"dream entity"} to encompass both living dream characters and non-living dream objects. %The first component of the LLM performs segmentation and extracts each dream entity with prefix as an automatically generated prompt for the text-to-3D generative model. 

One challenge with the \textit{point-E} model is that, in the pure text-conditional generation, it grapples with prompts that combine multiple entities and works better with simple prompts that describe a single entity. Thus, we designed the dream entity extraction module to perform segmentation for each dream whispering interaction. It breaks down a complex dream scene description into automized prompts of single dream entities, tailored to serve the text-to-3D generative diffusion model (see Figure \ref{fig:mechanism} A). 
%%example
%For instance, by inputting a dream record "" through speech recognition, the LLM extracts the living entity and non-living entity as automized prompts "" and "". These prompts are then sent to the generative diffusion text-to-3D model \textit{Point-E} for 3D point cloud generation.  

%In this way, the LLM module not only serves the purpose of dream analysis, but becomes a prompt extraction and automization tool for another text-to-3D AI model through natural language interaction, within a larger multimodal interaction framework.

\textbf{Social Interaction Classification Module}
The second component of the LLM analyzes the social interactions among the dream entities. According to the HVDC scheme, social interaction in dreams was coded into three classes: aggressive, friendly, and sexual interactions. We used the definitions of each class and its subclass as embeddings, which categorize Aggressions into 8 levels from A1 (Covert hostility or anger) to A8 (Death by aggression), Friendliness into 7 levels from F1 (Felt but unexpressed friendliness) to F7 (Long-term relationship), and Sexual Interactions into 5 levels from S1 (Sexual thoughts or fantasies) to S5 (Sexual intercourse or attempt). %Every subclass was accompanied by at least one example. See an overview of the embedding in the appendix. 
The LLM module outputs one dominant social interaction subclass for each whispering interaction (see Figure \ref{fig:mechanism} B).

 %The detected social interactions influence the soundscape within the immersive experience in real-time. The music composer composed each soundscape layer for each category of social interaction unique to its characteristic, in addition to a foundational neutral soundscape layer. Throughout the experience, the neutral layer is in the background. When the system detects a category of social interaction through a whispered input, it overlays the corresponding soundscape on the neutral layer to craft an affective auditory environment. (see Figure \ref{fig:graphical-note})

%This soundscape layer is adaptive and responsive, capable of reflecting various degrees of social interaction labels, thereby enriching the immersive experience through nuanced auditory modulation.



%Integrating social interaction layers within soundscapes involves using pre-composed musical pieces crafted to evoke specific social interactions. Each layer in the soundscape features music deliberately composed to represent a particular type of social interaction. 


%Within the framework of social interaction classification and dream sentiment analysis (as detailed in the subsequent section), 



\textbf{Dream Sentiment Analysis Module}
The third component of the LLM classifies representative dream emotions. According to the HVDC scheme, dream emotions were coded into five classes: Anger (AN), Apprehension (AP), Sadness (SD), Confusion (CO), and Happiness (HA). The LLM module analyzes the dream speech and outputs one dominant emotion class for each whispering interaction (see Figure \ref{fig:mechanism} C).





\subsection{Affective Color Mapping}
The dream sentiment output further influences the color mapping of the AI-generated 3D point clouds during real-time rendering. We refer to affective color psychology and map each dream emotion category to a single representative color, based on cross-cultural studies on color and emotion \citep{madden_managing_2000,bartram_affective_2017}. We also situate the emotions within Russell's Circumplex model of affect \citep{russell_circumplex_1980} and map the dream emotions into single representative colors and their relative positions on the Valence and Arousal coordinate.
%(see Figure \ref{fig:emotion}). 
We created subclasses for Happiness to map peacefulness (high Valence, low Arousal) into light blue, and excitement (High Valence, high Arousal) into gold. We mapped Anger (low Valence, high Arousal) into dark red, and Apprehension (low Valence, neutral Arousal) into purple, Sadness (low Valence, low Arousal) into dark blue. Not seen as a typical emotion, confusion (neutral Valence, neutral to high Arousal) was less mentioned in affective color theory. The HVDC scheme refers to confusion as a fleeting state of "cognitive ambiguity" when confronting the unexpected or uncertain \citep{hall_content_1966}. We chose to map confusion into white glow in the artwork using the metaphor of fog.

%For future discussion: confusion and transcendence
%Representative words include "surprised, astonished, amazed, awestruck, mystified, puzzled, perplexed, strange, bewildered, doubtful, conflicted, undecided, and uncertain." (Domhoff 2003) This "cognitive ambiguity" might also connect to the "cognitive dissonance" in transformative experience, when the original mental framework could not accommodate the bizarre, and leads to a new perspective. This is in parallel with transcendent dreams or big dreams, where "white light experience" emerge as a common theme

\begin{comment}
\begin{figure}
  \centering
  \includegraphics[width=0.9\textwidth]{Emotion_Visualization-draft2.png}
  \caption{Affective color mapping for each emotion class, with Valence on the X-axis and Arousal on the Y-axis.  }
  \label{fig:emotion}
\end{figure}
\end{comment}

%\begin{align*} 
%Valence = - 0.169 - (0.061 \times Loudness\_Mean) + (0.588 \times Spectral\_Flatness\_1\_Mean) \\ + (0.302 \times MFCC\_1\_STD) + (0.361 \times MFCC\_5\_STD) \\ - (0.229 \times Perceptual\_Spectral\_decrease\_STD) 
%\end{align*}


%\begin{align*} 
%Arousal = - 1.551 + (0.060 \times Loudness\_Mean) + (0.087 \times Loudness\_STD) + (1.905 \times \\ Perceptual\_Tristimulus\_2\_STD) + (0.698 \times  Perceptual\_Tristimulus\_3\_Mean) \\ + (0.560 \times MFCC\_3\_STD) - (0.421 \times MFCC\_5\_STD) + (1.164 \times MFCC\_11\_STD)  
%\end{align*}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{Graphical_Note3.png}
  \caption{Example of a graphic score for soundscape composition illustrating the integration of sound layers for the emotion classes and social interaction classes (X-axis = time, and Y-axis = sound layers).}
  \label{fig:graphical-note}
\end{figure}
\subsection{Context-Aware Soundscape System}
We designed a context-aware soundscape system, interacting with both social interaction and dream sentiment output, to enhance the affective experience. First, the music composer composed a neutral sound layer as the base layer for a composite soundscape. Then, he composed a single sound layer for each specific emotional class and social interaction class. During the experience, the system detected the emotion class and social interaction class in real-time and adjusted the composite sound layers accordingly. For instance, as depicted in Figure \ref{fig:graphical-note} with a graphic score, when the emotion class \textit{happiness}  is detected, the happiness sound layer is integrated with the neutral layer. And then, when the emotion class \textit{sadness} is detected, the sadness sound layer substitutes the happiness sound layer. Additionally, if a social interaction class \textit{aggression} is identified, an aggression sound layer blends on top of the emotion sound layer. 


%Analogous to the social interaction soundscape, the detected emotion class will adjust the soundscape by integrating a pre-composed layer corresponding to a specific emotional class. These pre-composed musical pieces are designed by a music composer through the computation of valence and arousal using equations derived from prior research on soundscape emotion recognition [citation]. Valence measures whether an emotion is positive or negative, while arousal indicates the level of physiological activation or intensity. Together, they provide a way to quantify and analyze emotional states. 


%The equations used to calculate valence and arousal are employed to quantify and identify emotions within soundscapes, aiming to enhance the immersive auditory experience. This process involves evaluating various combinations of low-level audio features extracted from multiple soundscape segments of the original soundscape to fine-tune the desired emotional effect. The mean of these calculated features is computed, with the results illustrated in Figure \ref{fig:emotion}. In this figure, the valence and arousal values for each soundscape layer are mapped within the valence-arousal plane and categorized by emotion classes and social interactions, represented by distinct symbols. When a single user input includes both social interaction and emotional classes, the resultant layers are integrated to create a composite soundscape. This composite soundscape is designed to represent and animate a multifaceted dream environment through music and sound. Currently, the predominant emotion and social interaction class are selected by the dream sentiment analysis and social interaction classification framework.  


\section{Discussion and Conclusion}
Our contributions are twofold: first, we implemented a composite AI System that extracts affective dream information for immersive dream-reliving experiences, using the Large Language Model and text-to-3D Generative AI model; second, we propose an experiential AI-dreamworker hybrid paradigm as a future interface for dreamwork.

\subsection{A Composite Multimodal AI System for Affective Dream Re-experiencing.} 
%First, we break down complex dream scenes involving combined dream entities in one dream whispering, and extract the dream entities as separate single entities. We envisioned this workflow as a larger framework for other applications such as text-to-image and video

%Dictionary and lexical database-based approach and distributional semantics approach of AI analysis

%Within Dictionary and lexical database-based approach, \citet{fogli_our_2020}, characters, social interaction, and emotions. these approaches are based on the analysis of single words and thus have little-to-no access to the full content and context of each dream report, and may thus fail to correctly capture emotions or social interactions described in dream reports.

%Another approach is distributional semantics approach based on word embedding. They may perform better than dictionary-based methods at capturing  the semantic content of a report as they include  partial information about the ‘context’. 




Through our immersive art installation, we implemented an automated dream sentiment and social interaction analysis enabled by LLM, with the output data influencing a multisensory immersive dream-reliving experience. Within AI-assisted dream content analysis, \citet{bertolini_automatic_2023} are the first to use language models to predict the absence or presence of emotions, while \citet{cortal_sequence--sequence_2024} use a sequence-to-sequence language model to automate the annotation of both characters and their emotions. These approaches both follow the HVDC scheme, but focus only on pattern finding within a large database rather than individual dreamwork. While in the experiential dreamwork approach, previous work focuses on text-to-image generation based on the dream report, without considering the rich affective information from dream narration (dream sentiment and social interaction) \citep{fuse_onirica_nodate,canet_sola_dream_2022,liu_falling_2024}.

%the only LLM-assisted dream interpretation we are aware of is using LLM for Freudian or Jungian interpretations of a dream \citep{kelly_bulkeley_freud_2024}, which fall under association-based interpretation rather than integrating emotional and social information from content analysis. 
To the best of our knowledge, our work represents the first attempt to integrate on-the-fly content analysis with experiential dreamwork, bridging the long-existing divide between the experiential \citep{hill_use_2010,ellis_diving_2023} and the analytical (e.g. HVDC). The continuity hypothesis posits that most dreams are a continuation of what is happening in everyday life and that patterns of dream content reflect the dreamer’s most important waking life concerns, interests, and activities \citep{hall_content_1966,domhoff_scientific_2003}. Thus, by integrating AI-assisted dream content extraction into experiential dreamwork, we could potentially provide the dreamer with more personal insights, and dream researchers with a new direction in integrating the analytical data into the nuanced phenomenological experience of the dreamer.
%In addition, most previous AI assistants have only one modality (text-to-text) \citep{bertolini_automatic_2023,cortal_sequence--sequence_2024, picard-deland_c_vestibular_2023}, while artwork on dream interpretation mainly explores text-to-image. Our architecture combines a language model and a text-to-3D model into a \textbf{multimodal} AI system. By visualizing dream imagery in 3D and sonifying dreams based on emotional content, we propose a new way of communicating and \textbf{experiencing dream data} beyond textual and 2D graphics formats. %which again prompting the experiential might benefit both dreamwork practitioners and the dream content analysis community.
In the future, we plan to adapt the experience into a longitudinal re-experiencing and analysis tool and further evaluate its potential in facilitating affective dream re-experiencing and insight gain.

%1. All these approaches are based on HVDC coding system. However, we are the first to integrate social interaction classification using an embedding in a language-model based approach, which could provide an early attempt example for dream research community who are interested in automated dream analysis using language-model based approach. Our proposed architecture can also be adapted to be used with an LLMs pre-trained on different languages, or classifying other categories under HVDC.

%Previous work exploring re-experiencing dream in VR did not account for important emotional and social information from dream narration \citep{liu_virtual_2024}. 

%within the experiential dreamwork framework, immersion is important according to the the immersive spatio-temporal hallucination model (windt 2010). We now implemented a more context aware dream reliving system where on-the-fly analysis feed into the virtual environment.


% experiencing the data

%through a multimodal channel (visual and audio). By visualizing dreams in 3D, or sonifying dreams based on emotional content, we propose a new way of representing and communicating dream data beyond text and 2D graphics, which might benefit both dreamwork and the content analysis community.

%Moreover, different from the dream research approach where researchers conduct analysis on a vast database of dream reports, we provide an \textbf{interactive} approach where the dreamer whispers their dream on the fly while experiencing the dream data 



%\citet{gutman_music_mapping_2022} use a pre-trained Sentence-BERT model to identify prototypical dreams of flight or attack through embedding and clustering narratives in a vector space.



 

\subsection{Towards an Experiential AI-Dreamworker Hybrid Paradigm} 
%In traditional dreamwork, a human dreamworker guides the dreamer to re-enter dreams and derive insights \citep{hill_use_2010}.  
The role of AI in the field of dream research has evolved through distinct phases, each contributing unique capabilities to the analysis and interpretation of dreams:

\textbf{AI as a Textual Dream Interpreter:} These systems analyze dream reports through either automating content analysis based on specific coding rules \citep{gutman_music_mapping_2022,cortal_sequence--sequence_2024,bertolini_automatic_2023},  thematic analyses \citep{picard-deland_c_vestibular_2023}, or association-based interpretation \citep{kelly_bulkeley_freud_2024}. While these approaches effectively identify key themes, entities, and narratives, their primary focus has been on static analysis rather than engaging with the experiential aspects of dreaming or dreamwork.
%They remained detached from the user's subjective experience, functioning more as tools for static analysis rather than dynamic interaction.

\textbf{AI as a Visuospatial Dream Interpreter:}
Most previous artworks and studies focus on text-to-image for dream visualization \citep{canet_sola_dream_2022,fuse_onirica_nodate,liu_falling_2024}. The recent incorporation of 3D generative models has transformed AI into a visuospatial interpreter, allowing users to explore dream imagery spatially for deepened experiential engagement \citep{liu_virtual_2024}. %While these approaches enriched visual and spatial experiences, there remains potential for a system that perceive or respond to the dreamer's emotional and interactive engagement with the dream content, making the dream re-experiencing more responsive and personalized.

\textbf{Experiential AI-Dreamworker Hybrid:}
We propose the concept of an AI-dreamworker hybrid, which will represent a significant advancement in this trajectory. The AI dream worker will not only extract and analyze affective information from dreams—such as emotions and social interactions as per our current system—but will also take this a step further by \textbf{observing} the dreamer’s behavior during the dream reliving process, such as in a Virtual Reality environment with a Head Mounted Display. The soundscape and dream imagery could be transformed based on the interaction between the dreamer and the dream entities and environment (e.g. proximity and hand interaction). Moreover, the hybrid AI dreamworker system can adaptively \textbf{guide} the dreamer's attention by highlighting a specific dream entity or even \textbf{suggesting} ways to interact with the dream objects with a voice, leveraging established experiential methods (e.g. embodying a dream element, dreaming the dream onward, prompting dialogue with a dream character, or focusing on specific emotions or bodily sensations \citep{ellis_common_2019,ellis_diving_2023}). By integrating affective knowledge and real-time observation, the AI will become a more interactive and responsive participant in the dreamwork process, potentially aiding in deeper self-reflection and insight. Meanwhile, we envision that the human dreamworker, equipped with intuition and a deeper understanding of the subjective nuances of dreaming, will collaborate with AI to guide and enrich the interpretive process. The AI's guidance and extraction of affective information serve as a foundation, while the human dreamworker can offer personalized facilitation and interpretation, creating a holistic approach to dreamwork that balances computational precision with human empathy and creativity.


%We speculate that an AI dreamworker could potentially spur insights and offer interpretations that were once exclusive to human analysts.  


%This concept parallels Bulkeley's work on AI interpretation, which suggests that AI can assist in uncovering hidden meanings in dreams following, for example, a Freudian or Jungian interpretation of a dream \citep{kelly_bulkeley_freud_2024}. %However, the future of dream interpretation with human-AI collaboration envisioned by Bulkeley still relies heavily on textual communication and less engaged with the multisensory experience.

%In the future, we envision that the AI-dreamworker hybrid could enable interaction between the dreamer and the immersive experience. In this case, the dreamers' behavior during the reliving process is sensed by the AI system, and the soundscape and dream imagery are transformed based on the interaction between the dreamer and the dream entities and environment.

%Moreover, with privacy protected, the dreamer's background details (cultural background, personal beliefs, life situation) could be input as embedding for the LLM to suggest possible meanings and how to explore/interact with the dream entities in positive ways.



%Thousands of years ago, in ancient Egypt and Greece, individuals would gather in sleep temples, where they believed their personal problems could be healed from a dream brought by a god. These temples further developed into a public healthcare network for the 'helpless'.
%Our artwork serve as a starting point for discussion and reimagining a public mental healthcare system in modern society, not centered but distributed, where AI makes the dreamwork and entry-level dream analysis more accessible and affordable.



\subsection{Ethical implication:} 
%overreliance and giving out agency 
%One guideline in dreamwork is that the dreamer should always be in charge of the process and always hold the final responsibility for deciding what the dream means. 
While AI makes the dreamwork and entry-level dream analysis more accessible and affordable, it also introduces ethical challenges that require careful consideration. One risk is that the overuse of AI dreamwork and analysis systems might lead to \textbf{isolation and disconnection} to human communities of dream-sharing, which is crucial to meaning-making and community-finding for the dreamer. Therefore, it's essential to integrate AI in ways that preserve and enhance human-to-human interactions in dream-sharing. In addition, AI systems are susceptible to \textbf{biases} related to gender, race, and cultural stereotypes, which can lead to misleading interpretations or reinforce harmful narratives. These biases stem from the large datasets on which both language models and text-to-3D generative models are trained. Addressing these risks requires ongoing efforts in curating datasets, refining models, and critically evaluating AI outputs. Finally, the dreamer or dreamworker may become overly reliant on AI-generated guidance or interpretation, and \textbf{lose one's agency} in determining a dream's meaning. It is crucial to ensure that AI remains a tool to support, rather than dictate, the interpretive process, keeping the dreamer and dreamworker as active participants.


In \textit{The Apology of Socrates} \citep{plato_apology_1901}, Socrates describes his \textit{"Daimonion"} as an inner voice or divine sign:
\textit{"This sign, which is a kind of voice, first began to come to me when I was a child; it often holds me back (from doing something wrong or harmful to himself) but never commands me to do anything."} In essence, we envision a modern equivalent of Socrates' \textit{"Daimonion"} in dreamwork through AI — a voice that gently guides dream exploration, supporting the dreamer's journey without dictating their path, while fostering human insights and creativity flourishment. 



\bibliography{NeurIPS2024Bib}

%\appendix

%\section{Appendix / supplemental material}
%Embedding Details



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





\end{document}