\section{RELATED WORK}
\subsection{Object Detection }
%From Frame-level to Video-level 
Object detection has been a fundamental and challenging
problem in computer vision. Faster RCNN \cite{ren2015faster} is the cornerstone detector in the deep learning era that generates and refines region proposals in a unified learning framework. To overcome the computation redundancy of two-stage detectors \cite{cai2018cascade,ren2015faster}, one-stage detectors \cite{redmon2018yolov3,dai2016r,lin2017focal} aim to strike a balance between accuracy and speed. Anchor-free detectors are proposed to avoid manual tuning of anchor configurations and can be classified into two types: anchor-point detectors \cite{tian2019fcos,zhang2020bridging} and key-point detectors \cite{zhou2019objects,dong2020centripetalnet}. Recently, DETR-based detectors \cite{carion2020end, zhu2020deformable, zhang2022dino} have emerged as a new paradigm,  eliminating the need for many hand-engineered components through the transformer's self-attention mechanism. To effectively leverage the temporal information, we survey two relevant areas: video object detection and spatio-temporal action detection.

Video Object Detection. Feature degradation, such as motion blur, occlusion, and defocus, presents the primary challenge of video object detection. Early box-level video object detection methods tackle this problem in a post-processing way by linking bounding boxes predicted by still frames \cite{kang2017t, han2016seq, feichtenhofer2017detect, chen2018optimizing}. Feature-level video object detection methods aggregate temporal contexts to improve the feature representation. The feature can either be improved at the image level to boost the single-frame detector \cite{bertasius2018object, xiao2018video, zhu2017flow, zhu2018towards,zhu2017deep} or at the object level through exploration of semantic and spatio-temporal correspondence among the region proposals \cite{wu2019sequence, deng2019relation, shvets2019leveraging, yao2020video, han2020mining, deng2019object, gong2021temporal,wang2018fully}.  To avoid the redundant computational cost of applying object detectors to every frame, previous video object detection methods have focused on propagating useful information from key frame features to non-key frame features. 
%Recently, TransVOD \cite{zhou2022transvod} introduces the transformer architecture to aggregate both the spatial object queries and the feature memories.


%which pre-localizes actors using Faster RCNN, and surrounding image patches are then cropped and resized into I3D \cite{carreira2017quo} for further classification. 
Spatio-temporal Action Detection. The action detection task aims to identify and localize human actions in videos. Two-stage action detectors \cite{tang2020asynchronous,wu2020context,pan2021actor} first predict the bounding boxes of actors and then perform actor-centric action recognition, such as Context-Aware RCNN \cite{wu2020context}.
The end-to-end action detectors \cite{sun2018actor,girdhar2019video,li2020actions,zhao2022tuber,chen2021watch} simultaneously train the actor proposal network and the action classification network. For example, MOC \cite{li2020actions} jointly optimizes localization and classification losses based on the video feature maps which concatenates frame features along the temporal axis. STMixer \cite{wu2023stmixer} dynamically integrates video features across both spatial and temporal dimensions, with its cross-attention decoder independently processing spatial and temporal queries. Action detection methods utilize 2D-CNN or classic 3D-CNN architectures, such as I3D \cite{carreira2017quo} and SlowFast \cite{feichtenhofer2019slowfast}, as the feature extraction backbone, and focus on temporal interaction based on the extracted features in a post-process manner.

In conclusion, previous video-level detectors primarily rely on established frameworks for proposing region proposals. They pay more attention to the temporal relationship modeling after localization to achieve a deeper understanding of video content. However, this paradigm might be unsatisfactory for the GOD task, which necessitates considering the unique gaseous characteristics and requires more emphasis on collaborative representation of spatio-temporal features in the region proposal stage. 
%in video object detection and action detection tasks

\subsection{Spatio-temporal Feature Extraction}

Various techniques for action recognition \cite{chen2021deep}, such as two-stream networks, 3D-CNNs, and compute-effective 2D-CNNs have been explored for the extraction of spatio-temporal information. The classic paradigm for two-stream networks incorporates extra modalities including optical flow, which acts as a second input pathway to capture temporal motion details \cite{cheron2015p,wang2016temporal}. 3D-CNNs facilitate the direct extraction of spatio-temporal information from unprocessed input streams, explicitly representing spatio-temporal features\cite{tran2015learning,carreira2017quo,hara2018can}. Nevertheless, the high computational demands of 3D convolution kernels have led researchers to explore 3D factorization techniques to reduce complexity \cite{qiu2017learning, tran2018closer}. Alternatively, frame-level features can be extracted using 2D-CNNs, followed by modeling of temporal correlations. Compute-effective 2D-CNNs employ operations such as temporal shift \cite{fan2019more,shao2020temporal,lin2019tsm}, low-cost difference operations \cite{wang2021tdn} and establishing correspondences across adjacent frames \cite{kwon2020motionsqueeze}. While these methods are primarily tailored for video classification tasks, there remains a lack of exploration of spatio-temporal architecture suitable for video-level detection tasks.

\subsection{Gas Leak Detection}
%Rapidly identifying the source of gas leaks during industrial production, storage, and transportation is crucial to averting major safety hazards.   as an emerging cutting-edge technology,
Traditional gas leak detection primarily employs point sensors \cite{murvay2012survey}, similar to electronic noses, which require the gas to diffuse into the sensor for identification and have a limited range ($\leq$ 10 m). In contrast,  gas imaging cameras \cite{hagen2020survey,gaalfalk2016making} serve as ``intelligent eyes", which offer distinct advantages such as extensive monitoring coverage, rapid response speeds, and the unique capability of gas dispersion visualization. The potential of gas imaging technology can be substantially enhanced  with advanced artificial intelligence visual analysis. Early attempts for visual-based gas detection involve background modeling \cite{zeng2018gas,wang2018infrared,hong2019vocs} and optical flow methods \cite{hagen2020survey}. Background modeling  captures the dynamic changes of gases across a sequence of frames, but it suffers from the interference of moving objects and requires that the cameras remain stationary. Optical flow estimates how each parcel of gas moves within the image based on the ``brightness constancy" assumption \cite{fleet2006optical}, which necessitates the gas to have a relatively high concentration. Traditional hand-crafted descriptors struggle to handle the complex and variable characteristics of gaseous objects.

A plausible solution involves implicitly extracting gaseous features using deep-learning based methods. VideoGasNet \cite{wang2022videogasnet} considers it a video classification problem and classifies the videos by methane leak volume. TBLD \cite{bin2021tensor} first takes advantage of the tensor decomposition based background subtraction algorithm to identify the foreground area, and then investigates different classifiers in the leakage classification stage. The above works focus on utilizing deep learning techniques for video gas classification tasks.  Additionally, some works \cite{bin2022multimodal,bin2022foreground,shi2020real} adopt the classic object detector, such as Faster RCNN \cite{ren2015faster}, for localizing gaseous objects. However, the frame-level detector cannot leverage the temporal correlation for the dynamic diffusion of gases, underscoring the need for the spatio-temporal feature extraction specifically designed for gaseous object detection. In conclusion,  research in this area is still in its nascent stages and remains relatively scarce.
%video-level detectors 


\begin{figure}[t]
	%\vspace{-0.5em}
	\begin{center}
		%\fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
		\includegraphics[width=1.0\linewidth]{principle.png}
	\end{center}
	\vspace{-1.0em}
	\caption{The spectral transmittance curves of representative gases in the mid-infrared band. Despite differences in gas types and spectral ranges, based on the Lambert-Beer's law, they exhibit similar visual characteristics in gas imaging cameras.  }
	%\label{fig:short}
	\label{fig:principle}
		\vspace{-0.0em}
\end{figure}