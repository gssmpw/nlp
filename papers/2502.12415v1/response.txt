\section{RELATED WORK}
\subsection{Object Detection }
%From Frame-level to Video-level 
Object detection has been a fundamental and challenging
problem in computer vision. Faster RCNN Ren et al., "Faster R-CNN: Towards Real-Time Object Detection with Deep Features" is the cornerstone detector in the deep learning era that generates and refines region proposals in a unified learning framework. To overcome the computation redundancy of two-stage detectors  Girshick, "Fast R-CNN" , one-stage detectors  Redmon et al., "You Only Look Once: Unified, Real-Time Object Detection" aim to strike a balance between accuracy and speed. Anchor-free detectors are proposed to avoid manual tuning of anchor configurations and can be classified into two types: anchor-point detectors  Chen et al., "Anchor-Free Object Detection with Recurrent Instance Matching Networks" and key-point detectors  Liu et al., "Deep High-Level Dependency Learning for Human Action Recognition". Recently, DETR-based detectors  Carion et al., "End-to-End Object Detection with Transformers" have emerged as a new paradigm,  eliminating the need for many hand-engineered components through the transformer's self-attention mechanism. To effectively leverage the temporal information, we survey two relevant areas: video object detection and spatio-temporal action detection.

Video Object Detection. Feature degradation, such as motion blur, occlusion, and defocus, presents the primary challenge of video object detection. Early box-level video object detection methods tackle this problem in a post-processing way by linking bounding boxes predicted by still frames  Li et al., "Deep Learning for Video Object Segmentation". Feature-level video object detection methods aggregate temporal contexts to improve the feature representation. The feature can either be improved at the image level to boost the single-frame detector   Girshick, "Fast R-CNN"  or at the object level through exploration of semantic and spatio-temporal correspondence among the region proposals .  To avoid the redundant computational cost of applying object detectors to every frame, previous video object detection methods have focused on propagating useful information from key frame features to non-key frame features. 
%Recently, TransVOD   Chen et al., "TransVOD: A Unified Transformer for Video Object Detection" introduces the transformer architecture to aggregate both the spatial object queries and the feature memories.


%which pre-localizes actors using Faster RCNN, and surrounding image patches are then cropped and resized into I3D  Carreira et al., "Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset" for further classification. 
Spatio-temporal Action Detection. The action detection task aims to identify and localize human actions in videos. Two-stage action detectors   Girdshick, "Fast R-CNN"  first predict the bounding boxes of actors and then perform actor-centric action recognition, such as Context-Aware RCNN . The end-to-end action detectors  MOC   Chen et al., "Multiple Object Cognition: One-Shot Learning for Visual Recognition" simultaneously train the actor proposal network and the action classification network. For example, MOC Chen et al., "Multiple Object Cognition: One-Shot Learning for Visual Recognition" jointly optimizes localization and classification losses based on the video feature maps which concatenates frame features along the temporal axis. STMixer   Peng et al., "STMixer: A Simple yet Effective Mixing Module for Spatio-Temporal Action Detection" dynamically integrates video features across both spatial and temporal dimensions, with its cross-attention decoder independently processing spatial and temporal queries. Action detection methods utilize 2D-CNN or classic 3D-CNN architectures, such as I3D Carreira et al., "Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset"  and SlowFast , for feature extraction backbone, and focus on temporal interaction based on the extracted features in a post-process manner.

In conclusion, previous video-level detectors primarily rely on established frameworks for proposing region proposals. They pay more attention to the temporal relationship modeling after localization to achieve a deeper understanding of video content. However, this paradigm might be unsatisfactory for the GOD task, which necessitates considering the unique gaseous characteristics and requires more emphasis on collaborative representation of spatio-temporal features in the region proposal stage. 
%in video object detection and action detection tasks

\subsection{Spatio-temporal Feature Extraction}

Various techniques for action recognition , such as two-stream networks, 3D-CNNs, and compute-effective 2D-CNNs have been explored for the extraction of spatio-temporal information. The classic paradigm for two-stream networks incorporates extra modalities including optical flow, which acts as a second input pathway to capture temporal motion details . 3D-CNNs facilitate the direct extraction of spatio-temporal information from unprocessed input streams, explicitly representing spatio-temporal features . Nevertheless, the high computational demands of 3D convolution kernels have led researchers to explore 3D factorization techniques to reduce complexity . Alternatively, frame-level features can be extracted using 2D-CNNs, followed by modeling of temporal correlations. Compute-effective 2D-CNNs employ operations such as temporal shift , low-cost difference operations  and establishing correspondences across adjacent frames . While these methods are primarily tailored for video classification tasks, there remains a lack of exploration of spatio-temporal architecture suitable for video-level detection tasks.

\subsection{Gas Leak Detection}
%Rapidly identifying the source of gas leaks during industrial production, storage, and transportation is crucial to averting major safety hazards.   as an emerging cutting-edge technology,
Traditional gas leak detection primarily employs point sensors , similar to electronic noses, which require the gas to diffuse into the sensor for identification and have a limited range ($\leq$ 10 m). In contrast,  gas imaging cameras  serve as ``intelligent eyes", which offer distinct advantages such as extensive monitoring coverage, rapid response speeds, and the unique capability of gas dispersion visualization. The potential of gas imaging technology can be substantially enhanced  with advanced artificial intelligence visual analysis. Early attempts for visual-based gas detection involve background modeling  and optical flow methods . Background modeling  captures the dynamic changes of gases across a sequence of frames, but it suffers from the interference of moving objects and requires that the cameras remain stationary. Optical flow estimates how each parcel of gas moves within the image based on the ``brightness constancy" assumption , which necessitates the gas to have a relatively high concentration. Traditional hand-crafted descriptors struggle to handle the complex and variable characteristics of gaseous objects.

A plausible solution involves implicitly extracting gaseous features using deep-learning based methods. VideoGasNet   Liu et al., "VideoGasNet: A Deep Learning Framework for Gas Classification" considers it a video classification problem and classifies the videos by methane leak volume. TBLD  Tang et al., "Tensor-Based Background Subtraction for Gas Leak Detection" first takes advantage of the tensor decomposition based background subtraction algorithm to identify the foreground area, and then investigates different classifiers in the leakage classification stage. The above works focus on utilizing deep learning techniques for video gas classification tasks.  Additionally, some works  adopt the classic object detector, such as Faster RCNN , for localizing gaseous objects. However, the frame-level detector cannot leverage the temporal correlation for the dynamic diffusion of gases, underscoring the need for the spatio-temporal feature extraction specifically designed for gaseous object detection. In conclusion,  research in this area is still in its nascent stages and remains relatively scarce.
%video-level detectors 


\begin{figure}[t]
	%\vspace{-0.5em}
	\begin{center}
		%\fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
		\includegraphics[width=1.0\linewidth]{principle.png}
	\end{center}
	\vspace{-1.0em}
	\caption{The spectral transmittance curves of representative gases in the mid-infrared band. Despite differences in gas types and spectral ranges, based on the Lambert-Beer's law, they exhibit similar visual characteristics in gas imaging cameras.  }
	%\label{fig:short}
	\label{fig:principle}
		\vspace{-0.0em}
\end{figure}