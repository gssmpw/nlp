\section{Related work}
\label{sec:related work}


\noindent \textbf{Vision Mamba.} VMamba~\cite{vmamba} introduced visual state space blocks that combine Mamba with 2D convolution layers and a hierarchical design similar to the Swin transformer~\cite{liu2021swin}, employing tricks like reducing the SSM states and expansion ratio to optimize throughput. EfficientVMamba~\cite{efficientvmamba} enhances VMamba by using an atrous-based selective scanning strategy for efficient global feature extraction, integrating SSMs with convolution branches. GroupMamba~\cite{groupmamba} addresses scalability and stability with a Modulated Group Mamba layer featuring multi-directional scanning and enhanced cross-channel communication. MambaVision~\cite{mambavision} reconfigures Mamba by incorporating convolutional layers and interleaved Mamba-Transformer blocks, achieving a new state-of-the-art in accuracy and throughput. Our average pooling in FastVim can be readily applied to these advancements.


\noindent \textbf{Sparse contextualization methods.} Inspired by efforts to enhance efficiency in ViTs, numerous studies~\cite{ryoo2021tokenlearner, hou2022token, bolya2022token, renggli2022learning, liang2022not, rao2021dynamicvit} have examined the reduction of tokens across layers through merging or pruning. Similarly, Famba-V~\cite{famba} utilizes a token fusion technique to consolidate similar tokens in Vim, thereby reducing training and inference time. Vim-prune~\cite{zhan2024exploring, zhan2024rethinking} addresses the challenges of naive pruning in Mamba due to its sequential nature by introducing pruning-aware hidden state alignment to stabilize neighborhoods. Our FastVim method aligns with this sparse contextualization approach, offering simplicity and maintaining performance beyond the tiny model size, in contrast to Famba and Vim-prune. Additionally, our proposed extension, FastMaskVim, can be seamlessly integrated with Vim-prune or Famba for further speedup, albeit with potential performance trade-offs. 
