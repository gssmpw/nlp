\clearpage
% \setcounter{page}{1}
\maketitlesupplementary


\definecolor{lightblue}{RGB}{173, 216, 255}
\begin{table}[ht]
    \caption{Classification benchmarks on \textbf{ImageNet-1k}~\cite{imagenet} dataset. All models are trained from scratch on Image size of $224\times224$. $\dag$ denotes we extend the training of Vim to base-size model.} 
    \begin{center}
    \resizebox{0.85\columnwidth}{!}{
    \begin{tabular}{lccc}
        \toprule
        Model & \#Params &  FLOPs & Top-1 \\

         & (M) & (G) & (\%) \\
        
        \midrule
        \multicolumn{4}{c}{Conv-Based} \\
        \midrule

        % ResNet-50 & 25M & - & 76.2 \\
        % ResNet-152 & 60M & - & 78.3 \\
        ConvNeXt-T~\cite{liu2022convnet} & 29M & 4.5G & 82.1 \\
        ConvNeXt-S~\cite{liu2022convnet} & 50M & 8.7G & 83.1 \\
        ConvNeXt-B~\cite{liu2022convnet} & 89M & 15.4G & 83.8 \\
        \midrule
        \multicolumn{4}{c}{Transformer-Based} \\
        \midrule
        DeiT-T~\cite{touvron2021training} & 6M & 1.3G & 72.2 \\
        DeiT-S~\cite{touvron2021training} & 22M & 4.6G & 79.8 \\
        DeiT-B~\cite{touvron2021training} & 86M & 17.5G & 81.8 \\
        % DeiT-B & $384$ & 86M & - & 83.1 \\

        Swin-T~\cite{liu2021swin} & 28M & 4.5G & 81.3 \\
        Swin-S~\cite{liu2021swin} & 50M & 8.7G & 83.2 \\
        Swin-B~\cite{liu2021swin} & 88M & 15.4G & 83.5 \\

        \midrule
        \multicolumn{4}{c}{Hybrid (Mamba + \{2D convolution, Attention module\}) } \\
        \midrule

        VMamba-T~\cite{vmamba} & 31M & 4.9G & 82.5 \\
        VMamba-S~\cite{vmamba} & 50M & 8.7G & 83.6 \\
        VMamba-B~\cite{vmamba} & 89M & 15.4G & 83.9 \\


        Eff.VMamba-T~\cite{efficientvmamba} & 6M & 0.8G & 76.5 \\
        Eff.VMamba-S~\cite{efficientvmamba} & 11M & 1.3G & 78.7 \\
        Eff.VMamba-B~\cite{efficientvmamba} & 33M & 4.0G & 81.8 \\

        MambaVision-T~\cite{mambavision} & 32M & 4.4G & 82.3 \\
        % MambaVision-T2 & 35M & 5.1G & 82.7 \\
        MambaVision-S~\cite{mambavision} & 50M & 7.5G & 83.3 \\
        MambaVision-B~\cite{mambavision} & 98M & 15.0G & 84.2 \\
        % MambaVision-L & 228M & 34.9G & 85.0 \\
        % MambaVision-L2 & 242M & 37.5G & 85.3 \\

        \midrule
        \multicolumn{4}{c}{Pure Mamba architecture} \\
        \midrule

        Vim-T~\cite{vim} & 7M & 1.8G & 76.1 \\
        Vim-S~\cite{vim} & 26M & 5.9G & 80.5 \\
        
        % Vim-B & 98M & - & 81.9 \\

        Vim-B~\cite{vim}$\dag$ & 98M & 20.9G & 80.7 \\
        Vim-B w/ LN$\dag$ & 98M & 21.0G & 82.6 \\

        PlainMamba-L1~\cite{plainmamba} & 7M & 3.0G & 77.9 \\
        PlainMamba-L2~\cite{plainmamba} & 25M & 8.1G & 81.6 \\
        PlainMamba-L3~\cite{plainmamba} & 50M & 14.4G & 82.3 \\
        
        Mamba\textsuperscript{\textregistered}-T~\cite{wang2024mamba} & 9M & 1.9G & 77.4 \\
        Mamba\textsuperscript{\textregistered}-S~\cite{wang2024mamba} & 28M & 6.3G & 81.1 \\
        Mamba\textsuperscript{\textregistered}-B~\cite{wang2024mamba} & 99M & 22.1G & 82.9 \\

        \rowcolor[HTML]{E4E8FF}  
        FastVim-T & 7M & 1.17 & 75.4 \\
        
        \rowcolor[HTML]{E4E8FF}  
        FastVim-S & 26M & 4.43 & 81.1 \\
        
        \rowcolor[HTML]{E4E8FF}  
        FastVim-B & 98M & 17.23 & 82.6 \\
        

        \bottomrule
    \end{tabular}
}
\end{center}
    \label{tab:additional_main_results}
\end{table}



In this supplementary material,  details are provided on the following: 
\begin{itemize}

    \item Self-Supervised Learning: MAE (additional) (\ref{additional_mae_section})

    \item Additional ablations 
    (\ref{additional_ablations})

    \item JUMP-CP (additional)  
    (\ref{additional_jumpcp_implementation_section})


    \item Additional Throughput analysis
    (\ref{additional_throughput})


    \item Semantic Segmentation implementation details  
    (\ref{additional_semantic_implementation_section})

    \item Object Detection and Instance Segmentation implementation details  
    (\ref{additional_objectdet_implementation_section})


    \item Kernel details
    (\ref{kernel_details})

    \item Model configurations
    (\ref{model_sizes})


\end{itemize}


\section{Self-Supervised Learning: MAE (additional)}
\label{additional_mae_section}

\noindent \textbf{Implementation Details}. We closely followed the pre-training (Table~\ref{tab:mae_pretraining}), fine-tuning (Table~\ref{tab:mae_finetuning}), and linear-probing (Table~\ref{tab:mae_linearprobe}) settings from the Masked Autoencoders~\cite{mae} codebase. All MAE pretraining is done for 1600 epochs in this study. A few key changes, particularly for fine-tuning and linear probing, are discussed below.

\begin{table}[h]
    \caption{MAE: Pre-training setting.} 
    \begin{center}
    \resizebox{0.8\columnwidth}{!}{
    \begin{tabular}{l|l}
        \toprule
        config & value \\
        \midrule
optimizer                     & AdamW~\cite{adamw}    \\ 
base learning rate            & 1.5e-4  \\ 
weight decay                  & 0.05    \\ 
optimizer momentum            & $\beta_1, \beta_2=0.9, 0.95$    \\ 
batch size                    & 4096    \\ 
learning rate schedule        & cosine decay~\cite{loshchilov2016sgdr}  \\ 
training epochs    & 1600    \\ 
warmup epochs    & 40    \\ 
augmentation                  & RandomResizedCrop        \\      
        \bottomrule
    \end{tabular}
}
\end{center}
    \label{tab:mae_pretraining}
\end{table}



\begin{table}[h]
    \caption{MAE: End-to-end fine-tuning setting. Note that layer-wise lr decay is applied after every two blocks instead of one.} 
    \begin{center}
    \resizebox{0.8\columnwidth}{!}{
    \begin{tabular}{l|l}
        \toprule
        config & value \\
        \midrule
optimizer                & AdamW                                             \\ 
base learning rate       & 5e-4 (B), 1e-3 (L/H)                                             \\ 
weight decay             & 0.05                                              \\ 
optimizer momentum       & $\beta_1$, $\beta_2$=0.9, 0.999                   \\ 
layer-wise lr decay~\cite{bao2021beit} & 0.65 (B), 0.75  (L/H)                     \\ 
batch size               & 1024                                              \\ 
learning rate schedule   & cosine decay                                      \\ 
warmup epochs            & 5                                                 \\ 
training epochs          & 100 (B), 50 (L/H)                                 \\ 
augmentation             & RandAug (9, 0.5)~\cite{cubuk2020randaugment}                    \\ 
label smoothing & 0.1                                         \\ 
mixup~\cite{zhang2017mixup}       & 0.8                                               \\ 
cutmix~\cite{yun2019cutmix}      & 1.0                                               \\ 
drop path~\cite{huang2016deep}   & \textbf{0.3}                                 \\

        \bottomrule
    \end{tabular}
}
\end{center}
    \label{tab:mae_finetuning}
\end{table}





\begin{table}[h]
    \caption{MAE: Linear probing setting.} 
    \begin{center}
    \resizebox{0.8\columnwidth}{!}{
    \begin{tabular}{l|l}
        \toprule
        config & value \\
        \midrule
optimizer                & SGD                                             \\ 
base learning rate       & 0.1                                             \\ 
weight decay             & 0                                             \\ 
optimizer momentum       & 0.9                    \\ 
batch size               & 4096                                              \\ 
learning rate schedule   & cosine decay                                      \\ 
warmup epochs            & 10                                                 \\ 
training epochs          & 90                                 \\ 
augmentation             & RandomResizedCrop                    \\ 

        \bottomrule
    \end{tabular}
}
\end{center}
    \label{tab:mae_linearprobe}
\end{table}



\noindent \textbf{Key Recipe Details.} 

1) Since Vim contains twice the number of layers compared to ViTs, we decreased the layer-wise learning rate decay every two blocks, instead of every block as in the MAE codebase for ViT fine-tuning, to ensure adequate fine-tuning of the initial layers. 2) We applied a scaling factor of \(1 - \text{mask ratio}\) (75\% masking by default) during fine-tuning and linear probing when pooling tokens before the SSM block. During pretraining, each row averaged 25\% of the tokens. In FastMaskVim, we sum the unmasked tokens and then divide by the number of columns for pooling instead of using mean pooling. To align this in fine-tuning and linear probing tasks, a scaling factor of 0.25 was necessary to achieve better performance.
\\

\noindent \textbf{Ablations}.


\begin{enumerate}
    

\item \textbf{Divide by number of columns vs. mean pool in FastMaskVim.} In Table~\ref{tab:ablation_fastmae_constantdivide}, we compare the performance of pre-training FastMaskVim using the default setting, where the sum of tokens in a row is divided by the number of columns, against mean pooling, where each row's sum is divided by the number of unmasked tokens present in the row. We observe in MAE pre-training that mean pooling performs slightly worse compared to the constant divide technique in corresponding downstream fine-tuning. However, exploring the comparison between mean pooling and constant divide in the context of supervised training is left for future research.

\begin{table}[!h]
    \caption{Comparison of constant divide vs. mean pool in pre-training FastMaskVim} 
    \begin{center}
    \resizebox{1\columnwidth}{!}{
    \begin{tabular}{lcc}
        \toprule
        FastMaskVim-B & Constant divide (default) & Mean Pool \\
        \midrule
        Top-1 ($\%$)  & 83.0 &  82.8 \\ 
        \bottomrule
    \end{tabular}
}
\end{center}
    \label{tab:ablation_fastmae_constantdivide}
\end{table}

\item \textbf{Finetuning with alternate layer lr decay.} In Table~\ref{tab:ablation_fastmae_lrdecay}, we compare the performance of fine-tuning pre-trained FastMaskVim using alternate layer learning rate decay instead of per-layer decay as in the MAE codebase. We observe that, since Vim typically contains twice the number of layers compared to ViTs with a similar parameter count, adjusting the decay logic to apply the learning rate decay every 2 blocks was necessary to ensure adequate fine-tuning of the early layers. With this simple adjustment, we were able to improve performance by a significant margin of 1\%. This analysis motivates us to believe that with further recipe improvements, FastVim can match the performance of ViTs with MAE pre-training, where we currently observe a lag of 0.6-1\% across Base to Huge model sizes (see Table~\ref{tab:mae_results}).

\begin{table}[!h]
    \caption{Comparison of alternate layer lr decay vs. per layer lr decay in finetuning} 
    \begin{center}
    \resizebox{1\columnwidth}{!}{
    \begin{tabular}{lcc}
        \toprule
        FastMaskVim-L & Alternate lr decay (default) & All layer lr decay \\
        \midrule
        Top-1 ($\%$)  & 84.9 &  83.9 \\ 
        \bottomrule
    \end{tabular}
}
\end{center}
    \label{tab:ablation_fastmae_lrdecay}
\end{table}


\item \textbf{Finetuning with scaling factor.} In Table~\ref{tab:ablation_fastmae_finetune}, we demonstrate the effect of using a scaling factor (0.25) in the fine-tune transfer of pre-trained FastMaskVim. Applying the scaling factor results in an improvement of 0.3\% compared to the default mean pooling in fine-tuning without multiplying by the scaling factor. As shown in Fig.~\ref{fig:finetune_scaling}, when scaling is not used, the initial performance is much lower, although it catches up closely by the end of the training schedule.


\begin{table}[!h]
    \caption{Effect of scaling factor in finetuning} 
    \begin{center}
    \resizebox{0.8\columnwidth}{!}{
    \begin{tabular}{lcc}
        \toprule
        FastMaskVim-L & w/ scaling (default) & w/o scaling \\
        \midrule
        Top-1 ($\%$)  & 84.9 &  84.6 \\ 
        \bottomrule
    \end{tabular}
}
\end{center}
    \label{tab:ablation_fastmae_finetune}
\end{table}



\begin{figure}[h]
\centering
    \includegraphics[width=0.8\linewidth]{figures/finetune_scaling.png}
    \caption{Effect of scaling factor in finetuning performance from MAE pretraining FastMaskVim-L on ImageNet-1k.
    }
    \label{fig:finetune_scaling}
\end{figure}

\item \textbf{Linear probing with scaling factor.} In Table~\ref{tab:ablation_fastmae_linearprobe}, we compare the linear probing performance of FastMaskVim with and without the scaling factor (0.25). We observe a drastic difference in performance and note that without the scaling factor, the model was unable to train due to the significant difference between the pre-training and linear probing distributions of number of unmasked tokens. During pre-training, on average, each row had 25\% of the number of columns (or number of rows when transposed in alternate layers) as unmasked tokens. Since we divided by the number of columns following the sum operation, the signal magnitude was in a lower range. In contrast, during linear probing, because all tokens are unmasked, we add the number of column tokens and divide by the number of columns, resulting in a very different signal range. We further compared the performance with pre-trained Vim's performance in linear probing and found that it performs considerably worse than FastVim. In MAE pre-training, random masking disrupts the sequential token positions, as demonstrated by Vim-prune~\cite{zhan2024exploring}. In contrast, during linear probing, this issue does not exist, causing a shift in the neighborhood distribution and resulting in low linear probe performance. This issue does not occur in our FastVim since, in both pre-training and linear probing, the number of rows/columns remains the same, ensuring that the neighborhood remains consistent. 

\begin{table}[!h]
    \caption{Effect of scaling factor in linear probing} 
    \begin{center}
    \resizebox{1\columnwidth}{!}{
    \begin{tabular}{lccc}
        \toprule
        Method & Vim-L & FastMaskVim-L w/ scaling & w/o scaling \\
        \midrule
        Top-1 ($\%$) & 45.6 & 60.2 &  0.02 \\ 
        \bottomrule
    \end{tabular}
}
\end{center}
    \label{tab:ablation_fastmae_linearprobe}
\end{table}

\end{enumerate}




\section{Additional ablations}
\label{additional_ablations}


\begin{enumerate}

\item \textbf{Effect of using class token in FastVim.} In Table~\ref{tab:ablation_classtoken}, we compare the performance of FastVim-S with a class token versus without a class token (default). We observe that having a class token improves performance but leads to slower reshape-transpose, pooling, and repeat operations to handle the middle class token. Since the goal of this study is to improve throughput while maintaining performance relative to the Vim baseline, we proceeded with all experiments without a class token. However, it is worth noting that even with a class token, our method is faster than Vim, according to our preliminary analysis. One future direction could be to mean pool only middle rows/columns for image-level representation instead of current mean pooling of all tokens for image representation. 
\\

\begin{table}[!h]
    \caption{Effect of using class token in FastVim on ImageNet-1K.} 
    \begin{center}
    \resizebox{0.8\columnwidth}{!}{
    \begin{tabular}{lcc}
        \toprule
        Model & FastVim-S & w/ Class token \\
        \midrule
        Top-1 ($\%$)  & 81.1 &  81.3 \\ 
        \bottomrule
    \end{tabular}
}
\end{center}
    \label{tab:ablation_classtoken}
\end{table}


\item  \textbf{The performance impact of different input norm and post-ssm norm combinations.} In Table~\ref{tab:ablation_norm_combinations}, we empirically demonstrate the performance of FastVim trained with different combinations of input normalization and post-SSM normalization. We found that using RMS normalization as the input norm and LayerNorm as the post-SSM norm yields the best performance.
\\

\begin{table}[!h]
    \caption{Effect of using different normalization combination in FastVim-S on ImageNet-1K.} 
    \begin{center}
    \resizebox{0.8\columnwidth}{!}{
    \begin{tabular}{lccc}
        \toprule
        Model & RMS-LN  & RMS-RMS & LN-LN \\
        \midrule
        Top-1 ($\%$)  & 81.1 & 80.7 & 80.9  \\ 
        \bottomrule
    \end{tabular}
}
\end{center}
    \label{tab:ablation_norm_combinations}
\end{table}




\item  \textbf{Effect of decompression after the skip connection on models' performance.} In Table~\ref{tab:ablation_decompress}, we explore whether in Fig.~\ref{fig:model}, we can move the skip connection \(\mathbf{D} x_t\) before repeating/decompressing the output to achieve even more speedup. However, we empirically found that it leads to a significant decrease in performance.
\\

\begin{table}[!h]
    \caption{Effect of decompression after the skip connection in FastVim-S on ImageNet-1K.} 
    \begin{center}
    \resizebox{0.8\columnwidth}{!}{
    \begin{tabular}{lcc}
        \toprule
        Model & Before $\mathbf{D}$ (default)  & After $\mathbf{D}$ \\
        \midrule
        Top-1 ($\%$)  & 81.1 & 78.7  \\ 
        \bottomrule
    \end{tabular}
}
\end{center}
    \label{tab:ablation_decompress}
\end{table}

\end{enumerate}


\section{JUMP-CP (additional)}
\label{additional_jumpcp_implementation_section}

\noindent \textbf{Implementation details}.

We followed the implementation details primarily from ChannelViT~\cite{channelvit}. Specifically, we used a learning rate of \(1 \times 10^{-3}\), a batch size of 256, and trained the model for 100 epochs, including 10 warmup epochs. We set the drop path rate to 0.05 and did not use EMA. All details and configuration files will be made available in the code.
\\

\noindent \textbf{Ablations}. 

\begin{enumerate}


\item  \textbf{ChannelVim-S: Effect of Spatial-First vs. Channel-First with and without sorted HCS.} In Table~\ref{tab:jumpcp_results_channelspatial}, we demonstrate the key configurations required to extend ChannelViT~\cite{channelvit} to the Mamba-based encoder, termed ChannelVim. As explained in detail in Sec.~\ref{subsec:channelmodeling}, due to the sequential processing in Mamba, the order of tokens matters. We found that the Channel-First method performs significantly better than Spatial-First. Whereas, the effect of sorting the output of hierarchical channel sampling (HCS) is opposite: it might be acting as an augmentation in the Spatial-First approach due to the order covering channel-by-channel, while it might be causing disruption in the neighborhood in the Channel-First approach since every next token in the sequence is another channel. Randomly shuffling the channel order (no sort) makes it difficult for learning.
\\

\begin{table}[!h]
    \caption{ChannelVim-S: Effect of Spatial-First vs. Channel-First with and without sorted HCS on 160-way perturbed gene prediction on JUMP-CP dataset. All methods use hierarchical channel sampling~\cite{channelvit} for training, and testing is done using all 8 channels. Each cell image is of resolution $224 \times 224 \times 8$.}
    \begin{center}
    \resizebox{0.7\columnwidth}{!}{
    \begin{tabular}{lcc}
        \toprule
        Method & HCS & Top-1 \\
        \midrule 
         Channel-First & Sorted  & 73.5 \\
          & Unsorted & 69.4 \\

        \midrule
         Spatial-First & Sorted & 65.9 \\
          & Unsorted & 67.9 \\
         
            \bottomrule    
    \end{tabular}
}
\end{center}
    \label{tab:jumpcp_results_channelspatial}
\end{table}


\item  \textbf{FastChannelVim-S: Effect of different pooling methods (mean, max, and attention pooling):} In this study, we use average pooling of tokens to compress the tokens before the SSM scan. We then explore the effect of different pooling methods, such as max pooling~\cite{ranasinghe2023perceptual} and attention pooling~\cite{abmil}, as detailed in Table~\ref{tab:jumpcp_results_meanmaxatt} on the JUMP-CP dataset. For attention pooling, we added a simple linear layer before each pooling layer to project each token to a size of one. This is followed by a SoftMax operation across tokens in the row, which is multiplied by a learned attention value and then summed across the row. We found that at a patch size of 16, all methods perform comparably. However, at a patch size of 8, max pooling and attention pooling methods start to perform better, likely due to the increased number of tokens in a row, allowing them to capture the most discriminative signals more effectively than mean pooling. Based on the accuracy-throughput trade-off, max pooling emerges as the best choice on the JUMP-CP dataset, as it is as fast as mean pooling while performing very close to attention pooling. Exploring effect of these pooling operation in natural imaging is left for future studies. 
\\

\begin{table}[!h]
    \caption{FastChannelVim-S: Effect of different pooling methods (mean, max, and attention pooling) on 160-way perturbed gene prediction on JUMP-CP dataset. All methods use hierarchical channel sampling~\cite{channelvit} for training, and testing is done using all 8 channels. Each cell image is of resolution $224 \times 224 \times 8$.}
    \begin{center}
    \resizebox{0.7\columnwidth}{!}{
    \begin{tabular}{lcc}
        \toprule
        Pooling & patch-size & Top-1 \\
        \midrule 
         Mean & 16  & 73.6 \\
          Max &  16 &  72.9 \\
          Att &  16 &  73.1 \\

        \midrule
        Mean & 8  & 83.1 \\
          Max &  8 &  85.0 \\
          Att &  8 &  85.8 \\

            \bottomrule    
    \end{tabular}
}
\end{center}
    \label{tab:jumpcp_results_meanmaxatt}
\end{table}


\item  \textbf{FastChannelVim-S: Effect of Pooling across 2 dimensions:} So far, we have explored pooling along only one spatial dimension, either across rows or columns. Now, we preliminarily explore pooling along two dimensions, which is particularly applicable in 3-dimensional datasets. When performing channel-wise tokenization, we obtain a 3D token grid. We experiment with the following pooling combinations in sequence every three blocks: column-channel pooling - row-channel pooling - row-column pooling - repeat. This approach provides much stronger compression, reducing the 3D token grid to a 1D token grid for the SSM scan.

\begin{table}[!h]
    \caption{FastChannelVim-S: Effect of Pooling across 2 dimensions on 160-way perturbed gene prediction on JUMP-CP dataset. All methods use hierarchical channel sampling~\cite{channelvit} for training, and testing is done using all 8 channels. Each cell image is of resolution $224 \times 224 \times 8$.}
    \begin{center}
    \resizebox{0.7\columnwidth}{!}{
    \begin{tabular}{lcc}
        \toprule
       Pooling & patch-size & Top-1 \\
        \midrule 
         Mean - 1D & 16  & 73.6 \\
          Max - 1D &  16 &  72.9 \\
         Mean - 2D & 16  & 74.3 \\
          Max - 2D &  16 &  73.5 \\
        \midrule
        Mean - 1D & 8  & 83.1 \\
          Max - 1D &  8 &  85.0 \\
        Mean - 2D & 8  & 78.4 \\
          Max - 2D &  8 &  84.0 \\
            \bottomrule    
    \end{tabular}
}
\end{center}
    \label{tab:jumpcp_results_2dpooling}
\end{table}


In Table~\ref{tab:jumpcp_results_2dpooling}, we demonstrate that at a patch size of 16 (token grid 14x14x8), both mean pooling and max pooling with 2D pooling work well and are on par with 1D pooling. In contrast, at a patch size of 8 (token grid 28x28x8), given the significantly larger number of tokens to pool (28x28 in row-column, 28x8 in row-channel, 28x8 in column-channel pooling blocks), mean pooling does not perform well. However, when we use max pooling, it performs much better, achieving results on par with ChannelVim with a patch size of 8 (see Table~\ref{tab:jumpcp_results}). Thus, even after pooling a much larger number of tokens, our method, FastChannelVim, still performs well with max pooling. This has implications in making the video models even faster~\cite{kahatapitiya2025object}.



\end{enumerate}





\section{Additional Throughput analysis}
\label{additional_throughput}

All throughput analysis is done on the H100 and involves inference throughput unless otherwise specified. 
\\

\begin{enumerate}
    \item \textbf{Effect of Autocast.} In Fig.~\ref{fig:throughput_autocast_on}, we compare the throughput of ViT-T, Vim-T, and our FastVim-T across different resolutions, both with and without the autocast functionality for Vim and FastVim, since a few parameters need to be in floating point (fp) 32 in Mamba. In contrast, for ViT, we used fp16 directly. As illustrated, the Mamba-based methods showed little improvement in throughput with autocast enabled. However, ViT's throughput increased by approximately seven times with fp16 because FlashAttention-2~\cite{dao2023flashattention} is enabled at fp16, unlike in fp32. We would like to note that the goal of this study is to improve throughput in the Vision Mamba domain, which we have been able to achieve drastically. Further hardware-aware optimizations for our redundant repeat operation (see Sec.~\ref{kernel_details}) could provide additional improvements and potentially allow competition with ViTs in throughput speed at higher resolution, even with FlashAttention-2 enabled.
\\

    \item \textbf{Throughput on A100.} In Fig.~\ref{fig:throughput_a100}, we compare the throughput of Vim and FastVim on both A100 and H100 GPUs. As shown, at a resolution of 1536, FastVim provides almost a 100\% improvement on the A100 compared to a 70\% speedup on the H100 over Vim. The likely reason for this discrepancy is that our repeat operation, illustrated in Fig.~\ref{fig:model}, is computationally expensive and does not benefit significantly from the transition from A100 to H100. In contrast, other matrix operations become faster, resulting in a more pronounced improvement on the A100 GPU. 
\\

    \item \textbf{Effect of LayerNorm post-SSM.} In Fig.~\ref{fig:throughput_nolayernorm}, we illustrate the effect of using LayerNorm post-SSM on throughput for both Vim and FastVim. It is evident that adding LayerNorm results in slower throughput but is essential for maintaining stability, as shown in Fig.~\ref{fig:stability}. Unlike BatchNorm, LayerNorm requires computation even during inference, leading to a decrease in speed. However, previous works such as \textit{High-Performance Large-Scale Image Recognition Without Normalization}~\cite{brock2021high} and \textit{Vision Transformers Inference Acceleration Based on Adaptive Layer Normalization}~\cite{keddous2024vision} can be integrated to enhance FastVim's inference speed with the default setting of added LayerNorm post-SSM, without compromising stability.
\\

    \item \textbf{Throughput across model sizes.} In Fig.~\ref{fig:throughput_tiny_small_base}, we display the throughput of Vim and FastVim across Tiny, Small, and Base-sized models with a batch size of 16. Across all model sizes, our method consistently provides a speedup in throughput compared to the Vim baseline.
\\

    \item \textbf{Throughput on per-channel modeling tasks.} In Table~\ref{tab:throughput_channelvim}, we demonstrate the throughput improvement in FastChannelVim compared to ChannelVim. With a longer token sequence (patch size 8), FastChannelVim delivers a speedup of $62.3\%$ over ChannelVim without any drop in accuracy (see Table~\ref{tab:jumpcp_results}).
\\

\begin{table}[!h]
    \caption{Comparison of inference throughput analysis between ChannelVim and FastChannelVim across patch sizes 16 and 8. Autocast is set to false, and LayerNorm is added post-SSM. Each cell image has a resolution of 224 $\times$ 224 $\times$ 8, and the batch size is set to 8.}
    \begin{center}
    \resizebox{1\columnwidth}{!}{
    \begin{tabular}{lcc}
        \toprule
        Method & Token-grid & Throughput (it/s) \\
        \midrule 
         ChannelVim-S/16 & $14^2 \times$ 8  & 234 \\
         FastChannelVim-S/16 & $14^2 \times$ 8 & 318 \\

        \midrule
         ChannelVim-S/8 & $28^2 \times$ 8  & 61 \\
         FastChannelVim-S/8 & $28^2 \times$ 8 & 99 \\

            \bottomrule    
    \end{tabular}
}
\end{center}
    \label{tab:throughput_channelvim}
\end{table}


\begin{table*}[!h]
    \caption{Dissecting SSM processing time (in milliseconds) at inference for only the Forward and Backward SSM layer in one block of Vim-T versus FastVim-T. For Vim, SSM includes parameter projection + SSM scan (with skip connection in CUDA kernel); for FastVim, pool + parameter projection + SSM scan + repeat + skip connection. Note that since skip connection is added in CUDA kernel for SSM scan for Vim, we don't report the time for skip-conn. separately for Vim.}
    \begin{center}
    \resizebox{2\columnwidth}{!}{
    \begin{tabular}{c|cc|cc|cc|cc}
        \toprule
        Operations & Vim (224) & FastVim (224) & Vim (512) & FastVim (512) & Vim (1024) & FastVim (1024) & Vim (2048) & FastVim (2048) \\
    \midrule
        SSM scan  & 0.79 &  0.44 & 3.20 & 0.41   & 14.52 & 0.42   & 58.20 & 0.30 \\
         Parameter proj.  & 0.17 & 0.07 & 0.44 & 0.08 & 1.80 & 0.11 & 6.90 & 0.20 \\
         Pool  & - & 0.10 & - & 0.80 & - & 1.70 & - & 3.46 \\
         Repeat  & - & 0.06 & - & 0.26 & - & 1.00 & - & 3.90 \\
         Skip conn.  & - & 0.17  & - & 0.78  & - & 3.10  & - & 12.2 \\
\midrule
Total & 0.96 & 0.84 & 3.64 & 2.33 & 16.32 & 6.33 & 65.10 & 20.06  \\
            \bottomrule    
    \end{tabular}
}
\end{center}
    \label{tab:dissecting_SSM_time}
\end{table*}

\item \textbf{Dissecting SSM processing time.} Here, we calculate the processing time for Forward SSM + Backward SSM in only one block (see Fig.~\ref{fig:model}) for Vim-T versus FastVim-T. The SSM time include the parameter projection (\(\mathbf{B}, \mathbf{C}, \mathbf{\Delta}\)) for selective scan, the SSM \textbf{scan} time, and the skip connection (\(\mathbf{D} x_t\)). Note that since the Mamba implementation enables the skip connection inside the CUDA kernel for faster processing, for Vim, we put the skip connection inside the kernel. However, for FastVim, we can't input the skip connection matrix (\(\mathbf{D}\)) to the kernel since we need to first perform the repeat operation and then add it with the skip connection, which takes place outside the CUDA kernel in FastVim. This results in significant overhead for FastVim, but since our SSM scan and parameter projection is lot more computationally cheaper due to compressed input after pool, FastVim still gives significant speedup over Vim for the Forward SSM + Backward SSM in a block. 

\begin{figure}[!h]
\centering
    \includegraphics[width=1\linewidth]{figures/fastvim_vs_vim_ssm_plot_withpool_DinkernelforVim.png} 
    \caption{Comparison of inference processing time for only the Forward and Backward SSM layer in one block of Vim-T versus FastVim-T. For FastVim, we calculate Pool + Repeat + SSM time. The annotations indicate SSM time for Vim; for FastVim, the upper value indicates the time for Pool, the middle value indicates the time for Repeat, and the lower value indicates the time for SSM.}
\label{fig:fastvim_vs_vim_ssm_plot}
\end{figure}

\end{enumerate}

In Fig.~\ref{fig:fastvim_vs_vim_ssm_plot}, for Vim-T, we calculate the time for SSM, whereas for FastVim-T, since we perform pooling and repeat operations, we measure the time for pooling and SSM and repeat. We can observe that the time taken in Vim scales quadratically with increasing resolution (approximately more than 4$\times$ increase in time for a 2$\times$ resolution increase), whereas ours scales sub-quadratically (approximately less than 3$\times$ increase in time for a 2$\times$ resolution increase). Thus at higher resolution (2048), FastVim (Pool + SSM + Repeat) is 3.24$\times$ faster than Vim (SSM). It can also be observed that as resolution increases, the repeat operation becomes increasingly expensive, taking almost 25\% of the time for FastVim (Pool + SSM + Repeat) compared to only about 8\% at resolution 224. Preliminary optimizations for this \textbf{redundant} repeat operation are discussed in Sec.~\ref{kernel_details}. Note that the time is recorded in milliseconds, and is for 1 batch with a batch size of 128. We use enough warmup runs to make sure the reported times are correct in practice. Residual (not the skip connection one) is omitted in these calculations for Vim and FastVim, and transpose operation is omitted as well in FastVim.
\\



In Table~\ref{tab:dissecting_SSM_time}, we mention the time taken by each component in more details for Forward SSM + Backward SSM for Vim and FastVim. For Vim, we report the parameter projection time and the SSM scan time. Note that since the Mamba kernel enables the skip connection in its CUDA kernel, for Vim, we do not separately report the skip connection time as it already becomes negligible in the Mamba kernel implementation. However, we observe that when the skip connection is not included inside the kernel, it takes significantly more time, similar to the skip connection time values mentioned for FastVim (in Table~\ref{tab:dissecting_SSM_time}). For FastVim, we measure the time for pooling, parameter projection, SSM scan, repeat operation, and skip connection, since it can't be added in the CUDA kernel due to the required repeat operation beforehand. It can be observed that even with a large image size of \(2048 \times 2048\), FastVim's SSM scan time and parameter projection time are lower than Vim's SSM scan time and parameter projection time at a much smaller 224 resolution. This is because, following tokenization and pooling, we have just \textbf{128} tokens for a 2048 resolution image, whereas Vim has 196 tokens for a 224 resolution image during the SSM scan and parameter projection. We would like to note that at higher resolutions, for FastVim, the pooling, repeat, and skip-connection operations take the majority of the time, whereas the SSM scan and parameter projection take significantly less time. These operations can be fused in CUDA kernel in future studies to achieve even more speedup. 




\section{Semantic Segmentation implementation details}
\label{additional_semantic_implementation_section}

In line with Vim~\cite{vim} and LocalVim~\cite{huang2024localmamba}, we used a batch size of 16 and an input size of 512x512. We employed the AdamW optimizer with a weight decay of 0.01. A Poly learning rate schedule was used, decaying over 160K iterations, with an initial learning rate of \(6 \times 10^{-5}\). For Tiny and Small models, we used drop path rate of 0.05, and for Base, we used 0.4. For evaluation, we used sliding window prediction with crop size of 512 and stride of 341. We utilized the code provided by LocalVim~\cite{huang2024localmamba}, which is based on MMSegmentation~\cite{contributors2020openmmlab}. 


\section{Object Detection and Instance Segmentation implementation details}
\label{additional_objectdet_implementation_section}

Following the code from LocalVim~\cite{huang2024localmamba}, we utilize the neck architecture from ViTDet and train Cascade Mask R-CNN as the detector. We employed the AdamW optimizer with a weight decay of 0.05, with a total batch size of 64. Initial learning rate is set to \(1 \times 10^{-4}\) and incorporates linear decay in the learning rate. We used drop path rate of 0.1 for Tiny and Small sized models, and 0.4 for Base sized model.


\section{Kernel details}
\label{kernel_details}

In FastVim (refer to Fig.~\ref{fig:model}), we apply mean pooling to the tokens before performing the SSM scan. Consequently, this operation must be repeated before integrating with the skip connection (\(\mathbf{D}\) in Eq.~\ref{eq:dtm}). When implementing this in PyTorch, we utilize the \texttt{repeat\_interleave} function to duplicate the output of the SSM scan prior to adding it with \(\mathbf{D} x_t\). However this operation becomes computationally expensive and redundant as demonstrated in Table~\ref{tab:dissecting_SSM_time}. Instead, we preliminarily explored modifying this repetition and moving the skip connection in the new CUDA kernel. 


\begin{table}[!h]
    \caption{Comparison of inference throughput analysis with our kernel versus default Mamba kernel on a H100 gpu. Autocast is set to False, and LayerNorm is added post-SSM, image resolution is 224, and batch size is set to 128.}
    \begin{center}
    \resizebox{0.8\columnwidth}{!}{
    \begin{tabular}{lc}
        \toprule
        FastVim-T & Throughput (it/s) \\
        \midrule 
         with Mamba kernel  & 3680 \\
         with our kernel & 4009 \\

            \bottomrule    
    \end{tabular}
}
\end{center}
    \label{tab:throughput_with_kernel}
\end{table}

Specifically, given a input flattened token sequence ($x_t$) with a length $L = h \times w$, the compressed output (pool across column) will have a length of $h$. Our objective is to have the $i^{\text{th}}$ element of this compressed output directly added to the token sequence spanning \(i \cdot w\) to \((i+1) \cdot w\) within the skip connection \(\mathbf{D} x_t\). This technique can reduce the time spent on redundant repetition in a naive PyTorch implementation, translating to practical speedup. In Table~\ref{tab:throughput_with_kernel}, we demonstrate the increased throughput for FastVim at a resolution of 224 with our kernel compared to default Mamba kernel that we used in this study. With the current optimizations, we observe an improvement in speed for 224-sized images; however, there is a decrease in speed at higher resolutions. This indicates the need for further refinement to optimize our kernel. Therefore, we also plan to release our kernel implementation to the open-source community so that others can build upon it.









\begin{figure*}[!h]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/speed_comparison_autocastfalse_h100.png}
        \caption{Autocast as False}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/speed_comparison_h100_withautocast_fp16vit.png}
        \caption{Autocast as True}
    \end{subfigure}
    \caption{Comparison of Inference Throughput (it/s) for FastVim, Vim, and ViT across different resolutions. Tested on H100 GPU with batch size of 128, and with LayerNorm post-SSM in Vim and FastVim.}
    \label{fig:throughput_autocast_on}
\end{figure*}



\begin{figure*}[!h]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/speed_comparison_autocastfalse_h100_novit_1536.png}
        \caption{On H100 GPU}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/speed_comparison_autocastfalse_a100_novit.png}
        \caption{On A100 GPU}
    \end{subfigure}
    \caption{Comparison of Inference Throughput (it/s) for FastVim, Vim, and ViT across different resolutions. Tested with batch size of 128, with autocast as False, and with LayerNorm post-SSM in Vim and FastVim.}
    \label{fig:throughput_a100}
\end{figure*}







\begin{figure*}[!h]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/speed_comparison_autocastfalse_h100_novit_2048.png}
        \caption{with LayerNorm post-SSM (default)}
        \label{fig:throughput_tiny}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/speed_comparison_h100_nolayernorm_autocastfalse.png}
        \caption{without LayerNorm post-SSM}
        \label{fig:throughput_base}
    \end{subfigure}
    \caption{Comparison of Inference Throughput (it/s) for FastVim, Vim, and ViT across different resolutions. Tested on H100 GPU with batch size of 128, and with autocast as False.}
    \label{fig:throughput_nolayernorm}
\end{figure*}


\begin{figure*}[!h]
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/speed_comparison_tiny_batch16_autocastfalse_h100.png}
        \caption{Tiny}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/speed_comparison_small_batch16_autocastfalse_h100.png}
        \caption{Small}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/speed_comparison_base_batch16_autocastfalse_h100.png}
        \caption{Base}
    \end{subfigure}
    \caption{Comparison of Inference Throughput (it/s) for FastVim, Vim, and ViT across different resolutions. Tested on H100 GPU with \textbf{batch size of 16}, autocast as False, and LayerNorm post-SSM in Vim and FastVim.}
    \label{fig:throughput_tiny_small_base}
\end{figure*}



\section{Model configurations}
\label{model_sizes}

\begin{table}[!h]
    \caption{Model configurations for FastVim}
    \begin{center}
    \resizebox{0.6\columnwidth}{!}{
    \begin{tabular}{c|c|c}
        \toprule
        Model & Layers & Embedding dim. \\
    \midrule
         Tiny  & 24 & 192 \\
         Small  & 24 & 384 \\
         Base  & 24 & 768 \\
         Large  & 48 & 1024 \\
         Huge  & 64 & 1280 \\

            \bottomrule    
    \end{tabular}
}
\end{center}
    \label{tab:model_sizes}
\end{table}

\newpage

\begin{figure}[!h]
\centering
    \includegraphics[width=0.7\linewidth]{figures/fastmaskvim_teaser.pdf} 
    \caption{Illustration of pooling and repeat operations in FastMaskVim. Instead of naive mean pooling of tokens in a row, we add the tokens and then divide it by number of columns in the token grid. Similarly when alternatively pooling tokens in a column.}
\label{fig:fastmaskvim_teaser}
\end{figure}


\begin{figure}[!h]
\centering
    \includegraphics[width=0.7\linewidth]{figures/fastchannelvim_teaser.pdf} 
    \caption{Illustration of pooling and repeat operations in FastChannelVim. These two operations are performed independently for each channel in per-channel tokenization paradigm.}
\label{fig:fastchannelvim_teaser}
\end{figure}




\begin{figure}[!h]
\centering
    \includegraphics[width=1\linewidth]{figures/channelvim_scanpath.pdf} 
    \caption{Illustration of flattened scanpath options available following per-channel tokenization in ChannelVim due to sequential nature of Mamba.}
\label{fig:channelvim_scanpath}
\end{figure}


\begin{figure}[!h]
\centering
    \includegraphics[width=1\linewidth]{figures/fastchannelvim_scanpath.pdf} 
    \caption{Illustration of flattened scanpath options available following per-channel tokenization in FastChannelVim due to sequential nature of Mamba.}
\label{fig:fastchannelvim_scanpath}
\end{figure}




