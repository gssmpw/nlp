\section{Experiments and Results}
\label{sec:experiments}

\subsection{Image Classification}

\noindent \textbf{Settings.}
We conduct training on the ImageNet-1k dataset~\cite{imagenet} consisting of 1.28M training images, and utilize the 50K ImageNet-1k validation images for evaluation. We follow exact training settings from~\cite{vim}, i.e. we train our models for 300 epochs using a batch size of 1,024, the AdamW optimizer, and EMA. A cosine annealing learning rate schedule with an initial value of $1\times10^{-3}$, a 5-epoch warmup period, and a weight decay of 0.05 is used. For data augmentation, we apply standard techniques such as random cropping, horizontal flipping, label-smoothing regularization, mixup, and random erasing. For FastVim-B we use a higher drop path rate of 0.4 instead of default 0.05 to avoid over-fitting. 


\definecolor{lightblue}{RGB}{173, 216, 255}
\begin{table}[ht]
    \caption{Classification benchmarks on \textbf{ImageNet-1k}~\cite{imagenet} dataset. All models are trained from scratch on Image size of $224\times224$. $\dag$ denotes we extend the training of Vim to base-size model. T refers to Tiny, S to Small, and B to Base size models.} 
\vspace{-13pt}
    \begin{center}
    \resizebox{0.80\columnwidth}{!}{
    \begin{tabular}{lccc}
        \toprule
        Model & \#Params &  FLOPs & Top-1 \\

         & (M) & (G) & (\%) \\
        
        \midrule

        Vim-T~\cite{vim} & 7M & 1.8G & 76.1 \\
        Vim-S~\cite{vim} & 26M & 5.9G & 80.5 \\
        

        Vim-B~\cite{vim}$\dag$ & 98M & 20.9G & 80.7 \\
        Vim-B w/ LN$\dag$ & 98M & 21.0G & 82.6 \\


\midrule
        \rowcolor[HTML]{E4E8FF}  
        FastVim-T & 7M & 1.17 & 75.4 \\
        
        \rowcolor[HTML]{E4E8FF}  
        FastVim-S & 26M & 4.43 & 81.1 \\
        
        \rowcolor[HTML]{E4E8FF}  
        FastVim-B & 98M & 17.23 & 82.6 \\
        

        \bottomrule
    \end{tabular}
}
\end{center}
\vspace{-8pt}
    \label{tab:imagenet_results}
\end{table}



\noindent \textbf{Results.} As seen in Table~\ref{tab:imagenet_results}, our proposed FastVim models perform on-par to the baseline Vim models across all model sizes. Since our approach is parameter-free, achieving comparable performance demonstrates that the proposed token pooling of FastVim still maintains sufficient token interaction across SSM blocks in Vim. Our empirical findings indicate that Vim can be effectively trained using a much sparser interaction of token. We report performance comparison with ViTs and other Mamba baselines in Supplement Table.~\ref{tab:additional_main_results}. It's important to mention that our research complements recent developments in the Vision Mamba field, such as VMamba~\cite{vmamba}, MambaVision~\cite{mambavision}, and GroupMamba~\cite{groupmamba}. Our pooling-based token reduction technique can be easily integrated within these methods to further enhance the speed of those high-performing architectures.


\noindent \textbf{Stability Issue in Vim.} As shown in Fig.~\ref{fig:stability}, we trained Vim-B from scratch on ImageNet-1k using default settings and a drop path rate of 0.4. During training, we encountered the issue of loss spikes, which caused instability. At the convergence, the model reached a peak accuracy of 81.2\%, consistent with recent work~\cite{ren2024autoregressive}. Inspired by the findings in~\cite{vmamba, jamba}, we experimented with adding LayerNorm post SSM scan. This modification resulted in much more stable training, overcoming the instability and ultimately improving the performance of Vim-B to 82.6\%. Our FastVim-B, which already incorporates LayerNorm post SSM operation (see fig.~\ref{fig:model}), achieved the same performance of 82.6\%. This illustrates the necessity of including the extra norm in the Vim and FastVim modules, aligning with the presence of two normalization layers in each transformer block, as well as in VMamba~\cite{vmamba}. In our experience, this spikes starts emerging from the small size Vim.


\begin{figure}[t]
\centering
    \includegraphics[width=0.8\linewidth]{figures/stability_plot.png}
    \caption{Stability Issue in Vim-B on ImageNet-1k.
    }
     \vspace{-10pt}
    \label{fig:stability}
\end{figure}


\subsection{Efficiency Analysis}
\label{subexp:throughput}

Here, we demonstrate the reduction in FLOPs and the increase in throughput achieved by our FastVim compared to Vim. In Fig.~\ref{fig:flops}, we compare the FLOPs requirements of FastVim, Vim, and ViT. At a lower resolution of 224, Vim demands the most operations, whereas ViT and FastVim have similar computational needs. As the resolution increases, ViT's computational requirements grow quadratically, while both Vim and FastVim scale linearly, with FastVim using up to 38\% fewer FLOPs. Notably, within a Mamba block, all components scale linearly in terms of FLOPs with the number of tokens, leading to a quadratic increase with respect to resolution for vision tasks. FastVim optimizes computations exclusively in the SSM, reducing its scaling to linear with respect to resolution. As a result, the other layers remain unchanged and maintain the same quadratic scaling as in Vim. Consequently, the overall FLOPs reduction in FastVim compared to Vim does not widen significantly with increasing resolution. The computational savings become more apparent at the SSM level, but this widening effect is muted at the block level, with FastVim-T using 35\% fewer FLOPs at 224 resolution and 38.5\% fewer FLOPs at 2048 resolution compared to Vim-T.


\begin{figure}[!h]
\centering
    \includegraphics[width=0.9\linewidth]{figures/flops_comparison.png} 
    \vspace{-8pt}
    \caption{
    Comparison of FLOPs (G) for FastVim, Vim, and ViT across different resolutions. 
    }
    \vspace{-4pt}
    \label{fig:flops}
\end{figure}


However, as observed in the case of throughput (Fig.~\ref{fig:throughput}), the gap between Vim and FastVim widens with increasing resolution, as FastVim's throughput relative to Vim's continually improves. Based on our observations (detailed in Supplement Table~\ref{tab:dissecting_SSM_time}), we found that the SSM scan time remains nearly constant for FastVim across resolutions from 224 to 2048, whereas it increases by up to 74$\times$ with an 8$\times$ increase in resolution (64$\times$ increase in tokens) in Vim. The reasoning behind this observation is that even at an image size of 2048, after tokenization and pooling, FastVim's SSM scan processes only 128 tokens, compared to 196 tokens in Vim's SSM scan at a much lower resolution of 224. Unlike no contribution of pooling and repeating operations to FLOPs, these operations incur overhead processing time in FastVim. However, with increasing resolution, the rapid decrease in SSM scan processing time in FastVim overpowers the increasing overhead (see Supplement Table~\ref{tab:dissecting_SSM_time}). Thus, at a resolution of 2048, the time taken by the Forward and Backward SSM layer in a block shows a 324\% speedup in FastVim compared to Vim, translating to nearly a 72.5\% speedup in the overall model, as the other MLP and gating layers remain unchanged in both Vim and FastVim. We also observe that, at a resolution of 1024 and beyond, our method outperforms ViT in terms of speed, while requiring less than four times the FLOPs at 1024. This translates to a significantly lower memory requirement, with the computation gap widening rapidly at higher resolutions (see Fig.~\ref{fig:flops}). Thus, at high resolution, our solution is not only faster than both Vim and ViT, but it also consumes substantially less memory than ViT. Note that we used LayerNorm post-SSM for both Vim and FastVim (see Fig.~\ref{fig:stability}), and comparisons without the added LayerNorm can be found in Supplement.~\ref{additional_throughput}. By default, we set autocast to false and evaluated FastVim, Vim, and ViT (without FlashAttention~\cite{dao2023flashattention, dao2022flashattention}) using float32 precision in line with VMamba~\cite{vmamba}. Additional analysis on larger-sized models and the impact of enabling or disabling autocast is available in Supplement.~\ref{additional_throughput}.




\begin{figure}[!h]
\centering
    \includegraphics[width=1\linewidth]{figures/speed_comparison_autocastfalse_h100.png} 
    \vspace{-12pt}
    \caption{
    Comparison of Inference Throughput (it/s) for FastVim, Vim, and ViT across different resolutions. Tested on H100 GPUs with batch size of 128.
    }
    \vspace{-6pt}
    \label{fig:throughput}
\end{figure}


\subsection{Self-Supervised Learning: MAE}
\label{subexp:mae}

\noindent \textbf{Setting.} We extend the training paradigm of our FastVim models using self-supervision without labels. Specifically, we explore the Mask Autoencoder~\cite{mae} (MAE) approach, commonly used for self-supervising vision transformers, in the context of our proposed FastMaskVim method. We adhered to the same pre-training and similar fine-tuning settings as MAE's GitHub repository, with further details in the Supplement.~\ref{additional_mae_section}. For fine-tuning, we applied a drop path rate of 0.3 across all models, and found gradient clipping at 3 necessary for images sized \(448\). In MAE training, FastMaskVim serves as the encoder, while a lightweight decoder with Vim at a default depth of 2 and dimension of 512 was used. We also pre-trained Vim (base and large, both with layer norm for stability, see fig.~\ref{fig:stability}) with MAE to establish baselines. All models were trained for 1600 epochs with a masking ratio of 0.75. 


\begin{table}[ht]
    \caption{Comparison of FastMaskVim with Vim and ViT, btoh pretrained with MAE, and other pretrained Vim based baselines ARM~\cite{arm} and HybridMH~\cite{hybridmh}. All models pre-trained with 224 image size on ImageNet-1k, and then end-to-end fine-tuned on 224 size image unless otherwise specified. B refers to Base, L to Large, H to Huge size models.}
    \vspace{-8pt}
    \begin{center}
    \resizebox{0.8\columnwidth}{!}{
    \begin{tabular}{lcccc}
        \toprule
        Model & B & L & H & H$_{448}$ \\
        \midrule
        ViT~\cite{mae} & 83.6 & 85.9 & 86.9 & 87.8 \\
        \midrule
        Vim & 83.3 & 85.1 & -- & -- \\
        % \hdashline
        ARM~\cite{arm} & 83.2 & 84.5 & 85.0 & -- \\
        % \hdashline
        HybridMH~\cite{hybridmh} & 84.9 & 85.0 & -- & -- \\
        \midrule
        \rowcolor[HTML]{E4E8FF} 
        FastMaskVim & 83.0 & 84.9 & 86.1 & 86.7 \\
        \bottomrule    
    \end{tabular}
}
\end{center}
\vspace{-4pt}
    \label{tab:mae_results}
\end{table}


\noindent \textbf{Results.} As illustrated in Table~\ref{tab:mae_results}, our proposed FastMaskVim, pretrained with MAE, performs on par with the Vim baseline with a minimal drop in performance of 0.3$\%$ and 0.2$\%$ across base and large model sizes respectively while being faster in all settings: pre-training, fine-tuning, and inference. Recently, the pure Mamba-based model ARM~\cite{arm} and the hybrid Mamba-based model HybridMH~\cite{hybridmh} have demonstrated state-of-the-art performance with autoregressive pretraining and masked autoregressive pretraining on ImageNet-1k, respectively. Here, we show that with minimal adjustments in the finetuning setup (further detailed in the Supplement~\ref{additional_mae_section}) and the application of post-SSM LayerNorm, MAE pretrained Vim and our FastVim can achieve comparable performance. Lastly, we demonstrate the scalability of FastMaskVim with images sized at 448, establishing a new state-of-the-art performance for Mamba-based methods in vision. We acknowledge that ViT excels in pretraining with MAE when compared with Vim, unlike in supervised training (Table~\ref{tab:additional_main_results}), highlighting the need for more exploration of MAE pre-training/fine-tuning recipe for the Vision Mamba. 

\subsection{Cell imaging: JUMP-CP}
\label{subexp:cell}

\noindent \textbf{Settings.} The JUMP-CP benchmark~\cite{chandrasekaran2023jump} serves as a microscopy imaging standard. The dataset includes a 160 perturbation classification task. We concentrated on the BR00116991 plate, containing 127k training, 45k validation, and 45k testing images. Each image has 8 channels: 5 for fluorescence and 3 for brightfield data. By default we keep channel-first scanning path (refer to Sec.~\ref{subsec:channelmodeling}) along with sorted HCS for both ChannelVim and FastChannelVim. Further ablations and implementation details can be found in Supplement.~\ref{additional_jumpcp_implementation_section}. 


\begin{table}[ht]
    \caption{Benchmarks of 160-way perturbed gene prediction on JUMP-CP dataset. All methods use hierarchical channel sampling~\cite{channelvit} for training, and testing is done using all 8 channels. Each cell image is of resolution $224 \times 224 \times 8$}
    \vspace{-6pt}
    \begin{center}
    \resizebox{0.7\columnwidth}{!}{
    \begin{tabular}{lcc}
        \toprule

        Method & Token Grid & Top-1 \\
        &  &  (\%) \\

        \specialrule{1pt}{1pt}{1pt}

        
         ViT-S/16 & $14^2$  & 58.9 \\
         Vim-S/16 & $14^2$  & 61.0 \\
        \midrule
         ChannelViT-S/16 & $14^2\times8$  & 68.6 \\
         ChannelVim-S/16 & $14^2\times8$  & 73.5 \\
         \rowcolor[HTML]{E4E8FF} 
         FastCha.Vim-S/16 & $14^2\times8$  & 73.6 \\
        
        \specialrule{1pt}{1pt}{1pt}


         ViT-S/8 & $28^2$  & 67.6 \\
         Vim-S/8 & $28^2$  & 66.4 \\

        \midrule
         ChannelViT-S/8 & $28^2\times8$& 74.8 \\
         ChannelVim-S/8 & $28^2\times8$ & 83.0 \\
         \rowcolor[HTML]{E4E8FF} 
         FastCha.Vim-S/8 & $28^2\times8$  & 83.1 \\
            \bottomrule   

    \end{tabular}
}
\end{center}
\vspace{-6pt}
    \label{tab:jumpcp_results}
\end{table}

\noindent \textbf{Results.} In Table~\ref{tab:jumpcp_results}, we present our findings on the 160-way classification task. 
Due to the highly complementary nature of channel information, motivated by ChannelViT~\cite{channelvit}, we focus here on our proposed methods: ChannelVim and FastChannelVim. We use patch sizes of $16 \times 16$ and a high-resolution model with a patch size of $8 \times 8$. 

We observe that without per-channel tokenization, the Vim method performs on par with the ViT model for patch sizes 16 and 8. However, when using large token sequences with per-channel tokenization ($\times 8$ tokens), ChannelVim significantly surpasses ChannelViT by approximately 5\% at a patch size of 16, and this advantage grows to 8\% with even longer token sequences at a patch size of 8. 
These phenomenal improvements over the current standard transformer architectures underscore the necessity of replacing the Transformer backbone with the Mamba backbone for visual encoders in per-channel tokenization paradigms, such as microscopy cell imaging which has implications in the field of drug discovery~\cite{kenyon2024vitally, pham2024enhancing}. Additionally, it is evident that FastChannelVim-S maintains similar performance to the full-contextualization method ChannelVim-S, while offering significant speedup. From this observation, we believe patch size can be decreased further to get even more performance boost insipred by per-pixel tokenization~\cite{nguyen2024image}, which is extremely efficient with FastChannelVim and its extensions when compared to ChannelVim and ChannelViT. 


\subsection{Semantic Segmentation}
\label{subexp:segmentation}

\noindent \textbf{Settings.} Here we conduct experiments on the ADE20K~\cite{ade20k} dataset using UperNet~\cite{upernet} as a segmentation framework for all backbones. The dataset contains 150 fine-grained semantic categories, with 20K, 2K, and 3K images for the train, validation, and test splits, respectively. Further settings can be found in Supplement.~\ref{additional_semantic_implementation_section}.

\begin{table}[ht]
    \caption{Semantic segmentation benchmarks on \textbf{ADE20K}~\cite{ade20k} dataset. UperNet~\cite{upernet} framework is used for all comparison backbones, with a crop size of 512 $\times$ 512.}
   \vspace{-12pt}
    \begin{center}
    \resizebox{0.4\columnwidth}{!}{
    \begin{tabular}{lcc}
        \toprule
        Backbone  &  mIoU \\

        \midrule
        DeiT-T  & 39.2 \\
         DeiT-S + MLN  & 43.8 \\
         DeiT-B + MLN  & 45.5 \\
        \midrule
        Vim-T  & 41.0 \\
         Vim-S  & 44.9 \\
         \midrule

        \rowcolor[HTML]{E4E8FF} 
        FastVim-T & 41.8 \\
        \rowcolor[HTML]{E4E8FF} 
        FastVim-S & 44.9 \\
        \rowcolor[HTML]{E4E8FF} 
        FastVim-B & 47.8 \\
        \bottomrule
        
    \end{tabular}
}
\end{center}
\vspace{-12pt}
    \label{tab:segmentation_results}
\end{table}


\noindent \textbf{Results.} As shown in Table~\ref{tab:segmentation_results}, FastVim consistently outperforms DeiT while achieving performance on par with Vim. The main aim of this study is to accelerate visual processing for larger images while ensuring competitive results. 
Previously, Vim~\cite{vim} demonstrated its significant advantages over DeiT in terms of GPU memory efficiency and speed as resolution increases. Our development further extends these benefits by creating an even faster yet capable Vision Mamba encoder: FastVim. 




\subsection{Object Detection and Instance Segmentation}
\label{subexp:detection}

\noindent \textbf{Settings.} Here we conduct experiments on the MSCOCO 2017 dataset~\cite{coco} using Cascade Mask R-CNN with ViTDet~\cite{vitdet} for all backbones in line with Vim~\cite{vim}. The dataset contains 118K, 5K, and 20K images for training, validation, and testing, respectively. Further settings can be found in Supplement.~\ref{additional_objectdet_implementation_section}.


\begin{table}[ht]
\caption{Object detection and instance segmentation benchmarks on COCO dataset using Cascaded Mask R-CNN~\cite{mask_rcnn} framework. $^*$detection transfer conducted using provided Vim-S (GitHub).}
\vspace{-6pt}
    \begin{center}
    \resizebox{\columnwidth}{!}{
\begin{tabular}{l|ccc|ccc}
\toprule
Backbone & AP\textsuperscript{box} & AP\textsuperscript{box}$_{50}$ & AP\textsuperscript{box}$_{75}$ & AP\textsuperscript{mask} & AP\textsuperscript{mask}$_{50}$ & AP\textsuperscript{mask}$_{75}$ \\
\midrule
DeiT-T & 44.4 & 63.0 & 47.8 & 38.1 & 59.9 & 40.5  \\

\midrule
Vim-T & 45.7 & 63.9 & 49.6 & 39.2 & 60.9 & 41.7 \\
Vim-S$^*$ & 47.1 & 65.8 & 50.7 & 40.6 & 62.9 & 43.5 \\
\midrule
\rowcolor[HTML]{E4E8FF} 
FastVim-T & 45.1 & 63.7 & 48.5 & 39.0 & 60.8 & 41.6 \\
\rowcolor[HTML]{E4E8FF} 
FastVim-S & 48.4 & 67.2 & 52.2 & 41.8 & 64.3 & 44.7 \\
\rowcolor[HTML]{E4E8FF} 
FastVim-B & 50.0 & 68.7 & 54.2 & 43.2 & 66.0 & 46.6 \\

\bottomrule
\end{tabular}
}
\end{center}
\vspace{-6pt}
    \label{tab:detection_results}

\end{table}

\noindent \textbf{Results.} In Table~\ref{tab:detection_results}, we see our FastVim-T performs comparably to Vim-T while surpassing Vim-S by 1.3 $\text{AP}^\text{box}$/1.2 $\text{AP}^\text{mask}$. These results highlight the effectiveness of our FastVim, even when handling larger 64×64 token grids in 1024×1024 MSCOCO images. Our method remains competitively performant despite pooling 64 tokens at once—significantly more than the 14 or 32 tokens pooled in ImageNet-1k and ADE20K—illustrating the scalability of our approach to higher resolutions.

\subsection{Ablation Study}
\label{subexp:ablation}

\textbf{Effect of Alternating Dimension Pooling on FastVim.} Here we investigate the importance of alternating spatial dimensions for pooling the token grid after each block in FastVim. As demonstrated in Table~\ref{tab:ablation_rotate}, FastVim with alternating pooling dimensions outperforms configurations that consistently pool tokens either across columns (Pool$_{col}$) or rows (Pool$_{row}$) across all blocks. This suggests that alternating pooling dimensions facilitates more effective sparse communication between tokens. 


\begin{table}[!h]
    \caption{Effect of alternating dimension pooling on ImageNet-1k.} 
    \vspace{-15pt}
    \begin{center}
    \resizebox{0.7\columnwidth}{!}{
    \begin{tabular}{lccc}
        \toprule
        Model & FastVim-S & Pool$_{col}$ & Pool$_{row}$ \\
        \midrule
        Top-1 ($\%$)  & 81.1 & 80.0 & 79.6 \\ 
        \bottomrule
    \end{tabular}
}
\end{center}
\vspace{-14pt}
    \label{tab:ablation_rotate}
\end{table}


\noindent \textbf{Exploring pooling in ViT.} We now examine how our pooling method performs with the contextualization module in Transformers, namely Self-Attention. As seen in Table~\ref{tab:ablation_vit}, our method, which applies alternating Pool$_{col}$ and Pool$_{row}$ pooling across blocks, performs significantly worse compared to the baseline ViT-S, which was trained using the default settings from DeiT\cite{deit}. It can be argued that Mamba has a 1D convolution (conv1d) layer which can do contextualization of tokens to an extent. To address that we trained 2 variations, FastVim without conv1d layers and ViT-S with Pool and conv1d for fair comparison. We can observe that conv1d is particularly helpful only in the Mamba case and ViT can't benefit from our proposed pooling method. This outcome highlights that our proposed approach is particularly well-suited to the emerging Mamba architecture. We note that while the failure of pooling approach in retaining performance in ViT as compared to Vim is interesting, it has opportunities for further exploration. 


\begin{table}[ht]
    \caption{Effect of pooling in ViT on ImageNet-1k.} 
    \vspace{-15pt}
    \begin{center}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{lccccc}
        \toprule
        Model & ViT-S & w/ Pool & w/ Pool w/ conv1d & FastVim-S &  w/o conv1d \\
        \midrule
        Top-1 (\%)  & 80.1 & 73.9 & 74.0 & 81.1 & 78.4 \\ 
        \bottomrule
    \end{tabular}
}
\end{center}
\vspace{-14pt}
    \label{tab:ablation_vit}
\end{table}


\noindent \textbf{Additional Ablations:} In Supplement.~\ref{additional_ablations} and~\ref{additional_jumpcp_implementation_section}, we additionally explore 1) the effect of using a class token in FastViM, 2) the performance impact of different input norm and post-ssm norm combinations such as RMS-LN (default in FastViM), RMS-RMS, and LN-LN, 3) Effect of decompression after the skip connection ($\mathbf{D}$ in fig.~\ref{fig:model}), and 4) comparisons between mean, max, and attention pooling in FastVim.


