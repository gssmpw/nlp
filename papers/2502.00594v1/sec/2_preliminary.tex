\section{Preliminaries}
\label{sec:preliminaries}


\begin{figure*}[t]
\centering
    \includegraphics[width=1\linewidth]{figures/FastVIM_model.pdf} 
    \caption{\textbf{Overview of FastVim}: Input image tokens are fed to norm and expansion layers, then output $x$ is transposed ($T$) every block for alternate pooling of rows and columns. Tokens are pooled post-\texttt{Conv1D}, processed by SSM, and decompressed before skip-connection ($\mathbf{D}$ in eq.~\ref{eq:dtm}). Note that the flattened tokens are reshaped into a 2D grid prior to the transpose and pooling layers, and are flattened again after these operations. In c) we illustrate the comparison of Forward SSM + Backward SSM inference time in one layer of Vision Mamba with Forward SSM + Backward SSM + pooling + repeat inference time in one layer of Fast Vision Mamba. We observe that with increase in resolution, FastVim needs significantly less time than Vim for contextualization module (further detailed in Supplement~\ref{additional_throughput}).}
    \vspace{- 4pt}
    \label{fig:model}
\end{figure*}


\textbf{State Space Models (SSMs)} are mathematical frameworks that model continuous-time sequences by transforming an input sequence $x(t) \in \mathbb{R}$ into an output sequence $y(t) \in \mathbb{R}$ via a hidden state $h(t) \in \mathbb{R}^N$, as governed by:

\begin{equation}
\begin{aligned}
h^{'}(t) &= \mathbf{A} h(t) + \mathbf{B} x(t), \\
y(t) &= \mathbf{C} h(t) + \mathbf{D} x_t,
\end{aligned}
\label{eq:ctm}
\end{equation}

where $\mathbf{A} \in \mathbb{R}^{N \times N}$ describes how the current state evolves, $\mathbf{B} \in \mathbb{R}^{N \times 1}$ describes how the input influences the state, $\mathbf{C} \in \mathbb{R}^{1 \times N}$ describes how the current state translates to the output, $\mathbf{D} \in \mathbb{R}$ describes how the input directly influences the output, acting as a skip connection, and $N$ being number of states.

To be applied on discrete sequence datasets, SSMs are discretized using zero-order hold over a sampling interval $\Delta$, resulting in discrete parameters $\overline{\mathbf{A}}$ and $\overline{\mathbf{B}}$:

\begin{equation}
\begin{aligned}
\overline{\mathbf{A}} &= e^{\Delta \mathbf{A}}, \quad 
\overline{\mathbf{B}} &= (\Delta \mathbf{A})^{-1} \left( e^{\Delta \mathbf{A}} - \mathbf{I} \right) \Delta \mathbf{B}
\end{aligned}
\label{eq:discrete}
\end{equation}

The discrete-time SSM equations are then modified as:

\begin{equation}
\begin{aligned}
h_t &= \overline{\mathbf{A}} h_{t-1} + \overline{\mathbf{B}} x_t, \quad y_t &= \mathbf{C} h_t + \mathbf{D} x_t
\end{aligned}
\label{eq:dtm}
\end{equation}


The above equation can be computed like a recurrent neural network, viewing $h_t$ as a hidden state with transition matrix $\overline{\mathbf{A}}$. However, since it is not practical for training on modern hardware due to sequential processing, the well-known connections between linear time-invariant (LTI) SSMs (eq.~\ref{eq:dtm}) and continuous convolutions can be used. Specifically, the eq.~\ref{eq:dtm} can be computed as $y = \overline{\mathbf{K}} * x$, where $\overline{\mathbf{K}}$ is the SSM convolution kernel. However, computing $\overline{\mathbf{K}}$ is computationally prohibitive due to repeated matrix multiplication of $\overline{\mathbf{A}}$ (for more depth, please see Linear State Space Layer (LSSL)~\cite{gu2021combining}). To address this, the Structured State Space sequence model (S4)~\cite{gu2021efficiently} proposed a novel parameterization of $\overline{\mathbf{A}}$, making it significantly faster and less memory consuming, while exceeding the LSSLâ€™s performance empirically.


\noindent \textbf{Selective State Space Models} termed as Mamba~\cite{mamba} enhance S4 further by allowing input-dependent parameters, enabling the model to capture richer contextual information. Mamba modifies the parameters $\mathbf{B}$, $\mathbf{C}$, and $\Delta$ as functions of the token sequence $x \in \mathbb{R}^{B \times L \times D}$, where $B$ is the batch size, $L$ is the sequence length, and $D$ is the token embedding dimension, as follows:



\begin{equation}
\begin{aligned}
\mathbf{B} &= \text{s}_{B}(x), \quad \mathbf{C} = \text{s}_{C}(x), \\
\mathbf{\Delta} &= \tau_\Delta(\text{Parameter} + \text{s}_{\Delta}(x)),
\end{aligned}
\end{equation}


where $\text{s}_{B}(x) = \text{Linear}_{N}(x)$, $\text{s}_{C}(x) = \text{Linear}_{N}(x)$, $\text{s}_{\Delta}(x) = \text{Broadcast}_{D}(\text{Linear}_1(x))$, $\tau_\Delta = \text{softplus}$, and $N$ being number of states, while $\text{Linear}_{d}$ is a parameterized projection to dimension $d$. This allows the model to adaptively modulate state transitions since the parameters are now token-dependent, thus improving its ability to model complex sequences. 

However, this comes at the cost of not being able to use the convolution operation like in S4 since the parameters are no longer linear time-invariant. This results in two challenges: the sequential nature of recurrence and the large memory usage. To address the latter, Mamba proposed a hardware-aware implementation that leads to significant speedup. For sequential recurrence, Mamba accelerated it by employing parallel scanning~\cite{smith2022simplified}, thus reducing the steps from $L$ sequential steps to a logarithmic scale, $\log(L)$ parallel steps.

\textbf{Vim} extends Mamba, which was originally designed for 1-D sequences, to handle visual data by transforming 2-D imaging datasets in line with adaptation similar to transformers in Vision Transformers. An input image of size $(H \times W \times C)$ is divided into flattened patches of size $(P \times P \times C)$, where $H$ is the height, $W$ is the width, $C$ is the number of channels, and $P$ is the patch size. This results in $L$ patches, with $L = \frac{H \times W}{P^2}$. These patches are projected into tokens of dimension $D$ using a linear layer, forming a token sequence $x \in \mathbb{R}^{L \times D}$.
Vim further proposed using two SSM modules in each layer, namely Forward SSM and Backward SSM, along with a 1-D causal convolution before both SSMs to allow for bi-directional contextualization needed for understanding non-causal imaging data. For more details about the overall architecture we refer the readers to~\cite{vim} and Fig.~\ref{fig:model} ignoring our modifications of transpose $T$, pool, repeat, and post-SSM norm operations.
