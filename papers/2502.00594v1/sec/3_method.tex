\section{Method}
\label{sec:method}



In this section, we present the details of our Fast Vision Mamba (FastVim), providing an overview in Fig.~\ref{fig:model}. A detailed description of our proposed pooling method, designed to accelerate contextualization in Vision Mamba, is provided in Sec.~\ref{subsec:pooling}. In Sec.~\ref{subsec:masking}, we explore extensions to masking paradigms, whereas in Sec.~\ref{subsec:channelmodeling} we illustrate the extension to the domain of per-channel tokenization modeling~\cite{channelvit}. 



\subsection{Spatial Pooling for faster contextualization}
\label{subsec:pooling}

In this paper, we propose a novel method to reduce the number of recurrent steps in Vim through spatial pooling (FastVim). Specifically, as detailed in Algorithm 1, we propose mean pooling across one spatial dimension of a 2-D image's token grid ($x$). Suppose we have a square grid where $h=w$ and $L=h \times w=h^2$ (where $h=H/P$ and $w=W/P$ are spatial dimensions of token sequence $x$ before flattening), this pooling reduces the sequence length to $h$ from $h^2$, resulting in a $1\times$ parallel steps (when using parallel scan) in FastVim ($\log(h)$) compared to $2\times$ parallel steps in Vim ($\log(h^2)$). Note that we use a square grid for a simpler example, but FastVim is generalizable to any image dimensions. This approach fits within the sparse contextualization paradigm because, instead of all tokens interacting, only pooled tokens interact with each other across one spatial dimension. Following the scan operation, the output is repeated to get back the sequence of size $h^2$. 
Average pooling is used as a default; variants with max and attention pooling are in the Supplement.

\begin{algorithm}
\caption{SSM + Selection + Spatial Pooling}
\textbf{Input:} $x : (B,L,D)$,  where $B = \text{batch size}$, $L = h \times w$, and $D = \text{embedding dimension}$ \\
\textbf{Output:} $y : (B,L,D)$
\begin{algorithmic}[1]
    \State $\textbf{A} : (D,N) \gets$ Parameter
    \Statex \Comment{Represents structured $N \times N$ matrix, where $N = \text{number of states}$}
    \State $x : (B,h,w,D) \gets \text{reshape}(x)$
    \State $x_{\text{pooled}} : (B,h,1,D) \gets  \text{pool}(\textbf{$x$}[:, :, :w, :])$ 
    \Statex \Comment{Pool spatial dimension}

    \State $\boldsymbol{B} : (B,h,1,N) \gets s_B(x_{\text{pooled}})$
    \State $\boldsymbol{C} : (B,h,1,N) \gets s_C(x_{\text{pooled}})$
    \State $\boldsymbol{\Delta} : (B,h,1,D) \gets \tau_\Delta(\text{Parameter} + s_\Delta(x_{\text{pooled}}))$
    
    
    \State $\bar{\textbf{A}}, \bar{\textbf{B}} : (B,h,1,D,N) \gets$ discretize($\Delta, A, B$)
    
    \State $y_{\text{pooled}} : (B,h,1,D) \gets \text{SSM}(\bar{\textbf{A}}, \bar{\textbf{B}}, \textbf{C})(x_{\text{pooled}})$
    \Statex \Comment{Time-varying: recurrence (scan) only}

    \State $y : (B,h,w,D) \gets \text{repeat}(y_{\text{pooled}}, \text{along } w)$

    \State $y : (B,L,D) \gets \text{reshape}(y)$
    
    \State \textbf{return} $y$
\end{algorithmic}
\end{algorithm}


Intuitively, pooling might lead to inadequate contextualization of tokens in a row when pooling tokens across columns (Pool$_{col}$), and similarly for tokens in a column when pooling tokens across rows (Pool$_{row}$). We address this issue by alternating the pooling operation across rows and columns across layers in FastVim. This enhances effective interactions among pooled tokens in different rows \textit{(row-wise interaction)} in Pool$_{col}$ and pooled tokens in different columns \textit{(col-wise interaction)} in Pool$_{row}$ (as shown in Fig.~\ref{fig:teaser}). We empirically demonstrate that this adjustment is crucial for achieving performance comparable to the baseline Vim. As shown in Fig.~\ref{fig:model}, this is carried out using a transpose of the token grid at every block, as we want to apply a 1D-conv in same direction as SSM scan. In practice as we use transpose, we always pool tokens across columns. 




\subsection{FastMaskVim: Incorporating Masking}
\label{subsec:masking}

So far, we have described FastVim in the context of a regular token grid of size $h \times w$. However, this approach cannot be directly utilized when faced with an irregular grid, a situation often encountered in scenarios involving masked tokens such as in Masked Autoencoders~\cite{mae, zhouhypermae} (MAE) and DINOv2~\cite{dinov2}, or in multiple instance learning~\cite{abmil, simil} (MIL) in pathology, where tissue samples can contain gaps. To enable FastVim to function effectively in such domains, we need to modify the pooling and transpose operations.

Specifically, instead of using a simple transpose operation on the token grid, we employ advanced indexing techniques to transpose a sparse token grid of shape $h \times w$, but only including the unmasked tokens. For pooling, we sum the tokens in each row and then divide by the number of columns, i.e., $w$, instead of naively performing mean pooling (see Fig.~\ref{fig:fastmaskvim_teaser}), as mean pooling could result in the loss of information regarding the number of tokens present in the row. These simple modifications have proven effective, as is  demonstrated by the MAE-pretrained FastMaskVim in Sec.~\ref{subexp:mae}.



\subsection{FastChannelVim: Per-Channel tokenization}
\label{subsec:channelmodeling}

In 2-D imaging datasets, a region of size $P \times P \times C$ is typically projected into a single token of dimension $D$, where $P$ is the patch size and $C$ is the number of channels, thus forming a token sequence $x \in \mathbb{R}^{L \times D}$ for $L$ tokens. However, this tokenization approach has been shown to be inadequate for certain imaging modalities where per-channel information is highly complementary, such as in microscopy cell imaging and satellite imaging, unlike the RGB channels in natural images. As established by ChannelViT~\cite{channelvit}, per-channel tokenization can address this limitation, though at the cost of increasing the number of tokens by a factor of $C$, thus forming a token sequence $x \in \mathbb{R}^{(L.C) \times D}$. In this paradigm, channel embedding is added along with position embedding to preserve order information.


Building on the benefits (performance and efficiency) of Mamba over Transformers in long sequence settings, we introduce an extension of Vim with per-channel tokenization, which we term ChannelVim. To implement this extension, we must address two key considerations due to the sequential nature of SSM scan in Mamba, in contrast to the set-like, permutation-invariant nature of self-attention in transformers. First, for the scan path, as illustrated in Fig.~\ref{fig:channelvim_scanpath}, we have two options: we can either traverse across all spatial tokens within a channel and then proceed to the next channel (spatial-first approach), or we can traverse across all channels at a given spatial position and then move to the next spatially adjacent position and repeat (channel-first approach). Second, it has been shown that hierarchical channel sampling (HCS), where some channels are randomly dropped during training, improves performance~\cite{channelvit}. We incorporate such HCS in ChannelVim. However unlike the original implementation, the output of the HCS module needs to be sorted, as order of channels matters in sequential modeling. We provide thorough evaluation of the effect of both above mentioned considerations in the Supplement.~\ref{additional_jumpcp_implementation_section}.

Finally, we adapt our FastVim to this domain, which we term FastChannelVim. In the main paper, we explore compressing tokens only across the spatial dimensions  (see Fig.~\ref{fig:fastchannelvim_teaser}). Thus, for each scan operation, we input either $h \times C$ (see Fig.~\ref{fig:fastchannelvim_scanpath}) or $w \times C$ tokens, instead of the entire $h \times w \times C$ tokens. In Supplement~\ref{additional_jumpcp_implementation_section}, we also explore compressing across the channel dimension.




