\section{Introduction}
\label{sec:introduction}

Recent developments in neural network architectures for computer vision tasks have used State Space Models~\cite{gu2021efficiently} (SSM) with selective scan (Mamba~\cite{mamba}) to enhance computational efficiency by replacing the quadratic complexity of self-attention in transformers~\cite{vaswani2017attention} with Mamba's linear complexity while retaining context dependence unlike recent SSMs~\cite{gu2021efficiently}. Models like Vision Mamba~\cite{vim} (Vim) and VMamba~\cite{vmamba} have shown that they can outperform their transformer-based counterparts, like Vision Transformer~\cite{vit} (ViT) and Swin~\cite{liu2021swin}, in vision tasks, while being particularly advantageous for tasks involving high-resolution images due to its efficient scaling. While Mamba supports content-based reasoning through selective scan in State Space Models (SSM), it cannot utilize efficient convolutions, necessitating a sequential recurrent approach that limits parallel processing. To address this, Mamba incorporates a parallel scan algorithm~\cite{blelloch1990prefix}, reducing sequential steps to a \textit{lower bound} of logarithmic scale~\cite{smith2022simplified} with respect to the number of tokens. While this approach significantly reduces sequential steps, in the vision domain, the number of tokens scale \textit{quadratically with increasing resolution}. Consequently, this results in a quadratic increase in the number of sequential recurrent steps translating to 2× increase in the number of parallel steps when using parallel scan, which challenges throughput in high-resolution imaging. 


\begin{figure}[!t]
\centering
    \includegraphics[width=1\linewidth]{figures/fastvim_teaser_horizontal.pdf} 
    \caption{
    FastVim accelerates Vim by mean pooling tokens across columns or rows, transforming token scaling from quadratic to linear with resolution. FastVim requires $log(h)$ parallel steps, compared to Vim's $log(h^2)$ parallel steps in Mamba's contextualization module SSM where $h$ is the number of tokens along height or width of the image with $L = h^2$ token inputs to the model.
    }
     \vspace{-6pt}
    \label{fig:teaser}
\end{figure}


In this work, we explore the possibility of scaling the number of recurrent computations in Vision Mamba linearly with image resolution, as opposed to scaling quadratically. We do this by applying average pooling across one dimension of the 2D token grid before the recurrent SSM block. This raises the question: \textit{Can we reduce the number of recurrent computations in Vision Mamba without compromising model performance?}

The answer is yes. In this paper, we utilize a simple, parameter-free technique of average pooling to reduce the number of recurrent steps in Vision Mamba while maintaining strong predictive power. 
An important consideration is alternating the pooling dimensions (as shown in Fig.~\ref{fig:teaser}) across stacked Mamba blocks. Pooling tokens across column (Pool$_{col}$) prevents interaction of tokens in a row and similarly across row pooling (Pool$_{row}$) prevents interaction of tokens in a column; thus, alternation ensures all tokens interact implicitly across multiple blocks. We empirically demonstrate that this alternation is a \textit{necessity} for achieving high performance in visual encoding, not just a desirable feature.

Our method, FastVim, is a purely Mamba-based neural network architecture (built on Vim) that uses pooling to accelerate contextualization in SSM scans. As shown in Fig.~\ref{fig:model}, in each forward and backward scan branch, mean pooling is applied after a 1D convolution layer to compress tokens across rows or columns, resulting in a one-dimensional token grid. These compressed tokens are projected to input-dependent parameters via a linear layer for selective scan, followed by interaction in the SSM module. The output is then repeated to restore the original token grid before the skip connection, followed by norm layer. Thus, across blocks, the number of tokens remains unchanged: they are compressed with pooling before the SSM scan and decompressed with repetition afterward, as shown in Fig~\ref{fig:model}. To alternate pooling dimensions and align 1D convolution direction with the SSM scanpath direction, we transpose the token grids every block. We further investigated whether our pooling approach is effective in ViT, and experimental results demonstrate that while it works in Vim, it fails in ViT, highlighting the need to further study Mamba's contextualizing capabilities versus Transformers. 

To extend our approach across other domains, we propose the following two adaptations of FastVim: FastMaskVim - incorporating masking in FastVim for applications like Mask Autoencoders~\cite{mae} (MAE), DINOv2~\cite{dinov2}, and pathology datasets~\cite{chen2024towards} having non-regular grids, and FastChannelVim - utilizing per-channel tokenization as introduced by ChannelViT~\cite{channelvit}, beneficial for datasets like microscopy cell~\cite{chandrasekaran2023jump} and satellite imaging. Remarkably, we trained the most effective pure Mamba-based monolithic visual encoder using our pooling method with MAE on ImageNet-1K~\cite{imagenet} to date, achieving state-of-the-art performance (SOTA). In per-channel tokenization task, FastChannelVim showed phenomenal gains in accuracy over ChannelViT baselines on microscopy image classification task, demonstrating the benefits of our method for long token sequences in vision. In summary, our main contributions are:


\begin{itemize}
    \item FastVim, a Vim-based architecture that utilizes average pooling, achieving 1× parallel steps in logarithmic scale from 2×, translating to $72.5\%$ speedup in overall framework.

    \item FastVim is adapted into FastMaskVim and FastChannelVim, extending its utility to applications with irregular grids and multi-channel imaging, respectively.

    \item Our methods set a new SOTA of \textit{86.7}\% on ImageNet-1k~\cite{imagenet} (with MAE pretraining) with a Mamba-based encoder and show substantial improvements over transformer baselines in long token sequence modeling in per-channel tokenization on microscopy imaging by $8.3\%$.
    
\end{itemize}
