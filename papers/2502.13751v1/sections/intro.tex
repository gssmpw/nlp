
With the increasing use of Machine Learning (ML) models to aid decision-making in high-stakes fields such as healthcare~\cite{shaheen2021applications} and finance~\cite{cao2022ai}, there is a growing need for better explainability of these models. Counterfactual Explanations (CEs)~\cite{guidotti2024counterfactual} are often leveraged in Explainable AI (XAI) to this end due to their intelligibility and alignment with human reasoning~\cite{Miller_19,Byrne19}. In particular, CEs can offer insights into the predictions produced by an ML model by showing how small changes in its input may lead to different (often more desirable) outcomes. To see what benefits CEs can bring, consider an illustration of a loan application with features \emph{27} years of age, \emph{low} credit rating, and \emph{15K} loan amount. Assume a bank's ML model classifies the application as not creditworthy. A CE for this outcome could be an altered input where a \emph{medium} credit rating (with the other features unchanged) would result in the application being classified as creditworthy, thus giving the applicant an idea of what is required to have their loan approved.

\begin{figure}[t!]
    \begin{subfigure}[b]{0.23\textwidth} % Adjust width as needed
        \centering
        \fbox{\includegraphics[width=\textwidth, clip, trim={5cm 3.5cm 5cm 4.5cm}]{images/CE_before_retrain.pdf}}
        \caption{}
        \label{fig:sub1} % Label for referencing
    \end{subfigure}
    % \hfill % Adds horizontal space between subfigures
    \hspace{0.1cm}
    \begin{subfigure}[b]{0.23\textwidth} % Adjust width as needed
        \centering
        \fbox{\includegraphics[width=\textwidth, clip, trim={5cm 3.5cm 5cm 4.5cm}]{images/CE_after_retrain.pdf}}
        \caption{}
        \label{fig:sub2} % Label for referencing
    \end{subfigure}
    \caption{A lack of robustness may invalidate CEs, here demonstrated on a neural network classifier trained to solve a binary classification task. An input (yellow circle) receives an initial classification, and two counterfactuals (red and green crosses) are generated for it (Figure~\ref{fig:sub1}). After a fine-tuning step occurs (Figure~\ref{fig:sub2}), the decision boundary slightly changes (from dashed to full black line), and previously generated CEs may be invalidated (red cross).}
    \label{fig:main} % Label for referencing the entire figure
\end{figure}

Despite their potential, current approaches to generating CEs often fall short in generating \emph{robust explanations}. Consequently, these methods may produce explanations whose validity is compromised by slight changes in the scenario being explained. For instance, recent work~\cite{UpadhyayJL21,JiangL0T23,DBLP:conf/icml/HammanNMMD23} has highlighted that even small alterations in the parameters of an ML model, e.g. following fine-tuning, may invalidate previously generated CEs. An example of this scenario is captured in Figure~\ref{fig:main}, where a lack of robustness is demonstrated on a model trained for binary classification tasks. In Figure~\ref{fig:sub1}, an input (yellow circle) receives an initial classification (blue class), and two counterfactuals are generated for it: one (red cross) laying exactly on the decision boundary (full black line) and one deeper inside the counterfactual class (green cross). In Figure~\ref{fig:sub2}, we observe that the decision boundary of the model undergoes slight changes, induced by fine-tuning on a slightly shifted input distribution. As a result, previously generated CEs may cease to be valid if no precautions are taken to ensure robustness. For instance, we observe that the CE corresponding to the red cross is now classified as belonging to the blue class and is thus invalid. Now consider the consequences of these behaviours in our loan example: after fine-tuning, the applicant changing their credit rating to medium no longer ensures the success of the loan application, as the CE was not robust. When this happens, the CE previously generated by the bank is invalidated, and the bank may be liable for inconsistent statements made to customers regarding loan terms.

This and many other forms of robustness of CEs have been the subject of intense research efforts recently, and numerous algorithms to evaluate the robustness of CEs have been proposed (a recent survey identified about 40 methods~\cite{ijcai2024survey}). However, the current state of robust CE research is fragmented, with various methods developed independently and implemented in different, often incompatible, ways. This lack of standardisation has resulted in challenges for the broader research community, as comparing the effectiveness of robust CE generation methods is impractical. 
 
We fill this gap in this paper and introduce \name, an open-source Python library to standardise and streamline the generation, evaluation, and benchmarking of robust CEs. \name{} provides flexible, extensible, and customisable tools to implement custom CE methods. Differently from existing frameworks, e.g.~\cite{PawelczykBHRK21,AgarwalKSPJPZL22}, our library focuses on providing a consistent framework for testing robustness and systematically comparing various methods, ensuring fair and reliable evaluations. \name{} addresses key limitations on library tools in the current landscape (\cite{DBLP:conf/ijcai/KeaneKDS21,ijcai2024survey}) by offering a standardised approach to robust CE development while also promoting extensibility, allowing users to integrate new datasets and explanation algorithms as needed. The library, including documentation and tutorials, is publicly available at the following link: 
\begin{center}
    \textit{\url{https://github.com/RobustCounterfactualX/RobustX}}
\end{center}

The reminder of this paper is organised as follows. Section~\ref{sec:overview} presents the main components of the library, providing details about their functionalities. Section~\ref{sec:comparison} demonstrates how easy it is to use \name{} to benchmark existing CE generation algorithms and compare them using different metrics. Finally, Section~\ref{sec:conclusion} offers some concluding remarks and pointers for future work.

 
