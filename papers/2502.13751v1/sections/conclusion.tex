We presented \name{}, a Python framework to generate, evaluate and compare robust CE for ML models. Our library fills a major gap in the existing literature on robust CEs, providing an easy-to-use and extensible platform to benchmark existing algorithms for robust CEs. Building upon extensive research in the area, \name{} provides a unified platform to run and compare existing approaches, as well as implementing new ones, reducing the need to re-implement software from scratch. Work is underway to further expand the list of available generation algorithms, evaluation methods and additional software facilities for testing and validation to ensure the correctness of the implementations. We believe \name{} will streamline research efforts in robust CEs, accelerating the development of innovative solutions and fostering collaboration within this rapidly growing field.