In this section we provide an example on how to use \name{} in practice; additional examples are available online. Figure~\ref{fig:example} shows how to run and compare six methods supported by \name{}. In this example we focus on robustness against model changes~\cite{UpadhyayJL21} and perform a comparison between four robust methods and two non-robust baselines. To this end, we first import the (pre-loaded) \texttt{ionosphere} dataset for binary classification (line~6) and apply standard pre-processing. We then create and train a simple three-layer neural network model (line~8).
% , i.e. a feed-forward neural network with input dimension $34$, one hidden layer of $10$ ReLU units and one output node with sigmoid activation. 
A task object is then created from the dataset and model. Then, we specify the CE generation and evaluation methods of interest (line~16 and 17) and run the benchmarking procedure (line~19). The default benchmark function in this example runs each method with its default hyperparameters, although customised hyperparameters can be configured. It then generates CEs for all instances in the dataset which are predicted with an undesirable class (here 102 points with neg\_value=0), and runs the specified evaluation methods. In this example, we evaluate CEs along three metrics: validity, proximity~\cite{Wachter17} and $\Delta$-robustness~\cite{JiangL0T23}. The results along the selected evaluation metrics are then printed in a structured table, so that 
% We use the \texttt{TrexNN} method from~\cite{DBLP:conf/icml/HammanNMMD23} to generate CEs for all inputs with label $0$ from the imported dataset. These instances can be obtained automatically by calling the \texttt{generate\_for\_all} (line~16). The resulting CEs are then evaluated using one of the evaluators available in \name{}. Here we evaluate the robustness of CEs using the \texttt{RobustnessProportionEvaluator} (lines~17-18) which finds the proportion of CEs that are \(\Delta\)-robust~\cite{JiangL0T23}. 
we can easily compare how each method performs. 
% For instance, Table~\ref{tab:results} reports results obtained using both non-robust (BLS, MCE, and Wachter) and robust (MCER, RNCE) methods to generate CEs on $125$ inputs from Ionosphere dataset~\cite{ionosphere_52}. For each method, 

Table \ref{tab:results} shows the results obtained, and reports the computation time, the percentage of valid CEs, average proximity (L2 distance to the input), and the percentage of CEs that are $\Delta$-robust. Observing this table, users of \name{} will be able to identify that robustness performance improvements from the non-robust baselines (NNCE, MCE) to the robust methods (MCER, RNCE, STCE, PROPLACE) are notable, although MCER fails to achieve 100\% robustness. Based on these results, users might conclude that \texttt{RNCE} is the optimal CE generator for this task, balancing between computation time, proximity, and robustness. 

\input{table}