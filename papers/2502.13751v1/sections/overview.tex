\begin{figure}[ht!]
    \centering
    \includegraphics[width=\linewidth, clip, trim={25.5cm 9.5cm 14.5cm 3.5cm}]{images/diagram_new.pdf}
    \caption{Internal work-flow of \name{}. Users can choose whether to generate robust CEs using our library or evaluate the robustness of externally generated CEs in our \name{}'s evaluation facilities. }
    \label{fig:robustx}
\end{figure}

RobustX implements a complete pipeline for robust CE generation and evaluation (Figure \ref{fig:robustx}) with three major components: \textbf{Task}, \textbf{CE generator}, and \textbf{CE evaluator}. Each has an abstract class template for easy customisation. Users start by creating a task which defines the model and inputs to be explained. Then, the user can choose whether to use \name{} to generate CEs, or directly evaluate the robustness of previously (externally) generated CEs.
% This allows for extensibility, as new methods can be seamlessly integrated with existing components. RoCELib has 5 major components: Models, Datasets, Tasks, CE Generation methods and CE Evaluators, described below.

\textbf{Task} objects, providing functionality for interactions between models and datasets, are the basic class passed into the CE generation and evaluation pipelines. Our current implementation assumes a \texttt{ClassificationTask} by default, as this is the most commonly considered use case in the literature; however, users can also implement customised \texttt{Task} objects for learning problems other than classification. \name{} natively supports models trained using sklearn\;\cite{pedregosa2011scikit}, Keras\;\cite{chollet2015keras} and PyTorch\;\cite{pytorch}. Models trained using other frameworks can also be used by instantiating a \texttt{BaseModel} wrapper class. As far as datasets are concerned, \name{} offers a selection of pre-loaded example datasets that can be readily loaded using the \texttt{DatasetLoader} class. Additionally, this class also allows the uploading of custom datasets if needed via \textit{.csv} files. 
% RobustX allows creating custom datasets with .csv files and Pandas DataFrame objects with \texttt{CsvDatasetLoader} and \texttt{DatasetLoader}, 
% while example datasets available. 
% It is possible to create a custom \texttt{DatasetLoader}, this requires assigning the \texttt{data} attribute within the class and specifying how to get \texttt{X} and \texttt{y}. \texttt{X} and \texttt{data} must be DataFrames, while \texttt{y} must be a Series, to integrate with existing components.
% \paragraph{Tasks} 
% RoCELib has \texttt{ClassificationTask} which represents the combination of a \texttt{BaseModel} and \texttt{DatasetLoader}. There is a \texttt{Task} abstract class if users wish to define their own version of a Task, but these must implement some basic functionality.

\begin{figure*}[t!]
    \centering
    \begin{lstlisting}[language=Python]
# first prepare a task
from robustx.datasets.ExampleDatasets import get_example_dataset
from robustx.lib.models.pytorch_models.SimpleNNModel import SimpleNNModel
from robustx.lib.tasks.ClassificationTask import ClassificationTask

data = get_example_dataset("ionosphere")
data.default_preprocess()
model = SimpleNNModel(34, [8], 1)
model.train(data.X, data.y)
task = ClassificationTask(model, data)

# specify the names of the methods and evaluations we want to use, run benchmarking
# This will find CEs for all instances predicted with the undesirable class (0) and compare
from robustx.lib.DefaultBenchmark import default_benchmark

methods = ["KDTreeNNCE", "MCE", "MCER", "RNCE", "STCE", "PROPLACE"]
evaluations = ["Validity", "Distance", "Delta-robustness"]

default_benchmark(task, methods, evaluations, neg_value=0, column_name="target", delta=0.005)
\end{lstlisting}
    \caption{Code snippet exemplifying how \name{} can be used to easily benchmark CE methods. Table \ref{tab:results} lists the benchmarking results.
    % The default benchmark function runs each method with its default hyperparameters, generates CEs for all instances in the dataset (in this example 102 points) which are predicted with an undesirable class (neg\_value=0), and runs the specified evaluation metrics. }
    }
    \label{fig:example}
\end{figure*}

\name{} currently implements nine \textbf{robust CE generation methods} across different robustness use cases reported in the yellow box in Figure\;\ref{fig:robustx}: \texttt{AP$\Delta$S} \cite{DBLP:conf/ecai/MarzariLCF24}, \texttt{ArgEnsembling} \cite{DBLP:conf/aamas/JiangL0T24}, \texttt{DiverseRobustCE} \cite{LeofanteP24}, \texttt{MCER} \cite{JiangL0T23}, \texttt{ModelMultiplicityMILP} \cite{DBLP:conf/kr/LeofanteBR23}, \texttt{PROPLACE} \cite{DBLP:conf/acml/JiangLL0T23}, \texttt{RNCE} \cite{JiangLRT24}, \texttt{ROAR} \cite{UpadhyayJL21}, \texttt{STCE} \cite{DBLP:conf/icml/DuttaLMTM22,DBLP:conf/icml/HammanNMMD23}. It also provides four popular non-robust methods that can be used as baselines to crease new generation methods: \texttt{BLS} \cite{LeofanteP24}, \texttt{MCE} \cite{MohammadiKBV21}, \texttt{KDTreeNNCE} \cite{DBLP:journals/datamine/BrughmansLM24}, and the seminal work by \cite{Wachter17}. All methods inherit from the abstract class \texttt{CEGenerator} and implement the  \texttt{\_generation\_method()} function, providing an easy interface to other components in the pipeline. 

\textbf{CE evaluation methods} can take in CEs, either generated within or outside \name{}, and benchmark their robustness along with other common properties identified in the literature. Currently, \name{} provides a \texttt{CEEvaluator} class to evaluate the validity and proximity of CEs~\cite{Wachter17}, as well as five classes specifically focusing on robustness evaluation metrics:
\texttt{VaRRobustnessEvaluator} to assess the validity of CEs after retraining~\cite{DBLP:conf/icml/DuttaLMTM22}, \texttt{DeltaRobustnessEvaluator}~\cite{JiangL0T23} to assess the robustness of CEs under plausible model changes, \texttt{ApproximateDeltaRobustnessEvaluator}~\cite{DBLP:conf/ecai/MarzariLCF24} assessing probabilistic robustness to plausible model changes, \texttt{SetDistanceRobustnessEvaluator} \cite{LeofanteP24} for assessing stability of CEs when the input is perturbed, and \texttt{MultiplicityValidityRobustnessEvaluator}~\cite{DBLP:conf/kr/LeofanteBR23} for checking CE robustness under model multiplicity. When needed, additional robustness evaluators can be easily added through the extensible interface provided by \name{}.
 
% Similar to generation methods, the \texttt{RecourseEvaluator} base class takes in a Task. It must implement the \texttt{evaluate()} method which takes in recourses and generates some evaluation score. RoCELib comes with default evaluation metrics, such as \texttt{DistanceEvaluator}, \texttt{ValidityEvaluator} and \texttt{RobustnessProportionEvaluator}. To assess the robustness of individual instances, RoCELib offers the abstract classes which allow for the implementation of custom robustness evaluation methods. For instance, the library currently provides a \texttt{ModelChangesRobustnessEvaluator} to evaluate robustness to model changes~\cite{UpadhyayJL21} and also provides a pre-implemented version of 
% \(\Delta\)-robustness~\cite{JiangL0T23} through the \texttt{DeltaRobustnessEvaluator}. 

