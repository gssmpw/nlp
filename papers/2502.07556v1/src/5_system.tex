\subsection{System Interface}\label{ssec:system}
The interface of \tool is built with a back-end based on Python and a front-end based on Vue.js with additional Fabric.js-supported canvas interactions.  
As shown in Figure~\ref{fig:interface}, the \tool system consists of four main views: Canvas view, Prompt Recommend view, Sketch Refine view, and Result view.

\textbf{Canvas view}.
The Canvas view (Figure~\ref{fig:interface} (a)) is the main playground of our system where users interactively specify initial spatial control.
Its main body is a canvas on which users can freely draw rough sketches of different colors corresponding to different types of objects. 
On the right side is a series of control buttons that allow users to perform basic brush control such as changing stroke width, switching to eraser and undoing previous stroke.

\textbf{Prompt Recommend view}. 
As shown in Figure~\ref{fig:interface} (b), in this view, users can input sophisticated prompt based on our semantic space to control the sketch of each individual object or background.
They can choose from manual input or click the \emph{Inference} button upon finishing the sketch on Canvas view to leverage our sketch-aware prompt recommendation to automatically suggest the appropriate prompt for each dimension.

\textbf{Sketch Refine view}.
This view enables users to utilize our spatial-conditioned sketch refinement method to refine their initial sketch by selecting a more realistic and precise recommended sketch for each object.
The refined sketch is displayed on a canvas on the right, where users can further adjust the position and size.
For example, as shown in Figure~\ref{fig:interface} (c), the users refine the sketches for both \emph{girl} and \emph{boy}.

\textbf{Result view}.
Finally, the Result view (Figure~\ref{fig:interface} (d)) displays the generated images. 
Users can choose the number of generated samples to show.
They can also save their favorite results.

% On the right is semantic input panel, where users can provide fine-grained multi-faceted semantic control on both global and individual mask level based on our design space.
% Particularly, users can choose to either manually enter the semantics on different aspects or leverage our retrieval augmented spatial-aware prompt inference to automatically suggest appropriate words to fill in each entry.

% \textbf{Image Gallery}.
% The image gallery is designed to display pre-generated single object images as well as final result image.
% For the single-object images, we display four recommended samples on the top, where users can select appropriate mask for single object.
% When a particular image is selected, the single-object mask extracted from this image will be applied to the original user-created spatial sketch, replacing the previous mask of the corresponding object.
% The adjusted spatial masks will be displayed below.
% Users can directly drag the masks to perform further modification including scaling and moving to create a more satisfactory layout.
% At the bottom of the Image Gallery, we display the final generated result.
% Users can choose the number of generated samples to show.








