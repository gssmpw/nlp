\section{Evaluation}\label{sec:evaluation}
To evaluate the effectiveness of \tool, we conducted a user study comparing \tool with two baseline systems: a text-to-image (T2I) and a region-to-image (R2I) generation system. 
We focus on how well our system supports novice users in creating images with minimal input while offering greater control and higher-quality outcomes compared to existing methods. 
Specifically, we assess: 1) quantitative comparison of user performance between baseline systems and \tool,  2) user satisfaction with the generated images compared to baseline systems, 3) the effectiveness of the system’s features, and 4) the overall usefulness of the system.

\subsection{Experiment Setup}

\subsubsection{Participants}
We recruited 12 participants (7 males, 5 females), aged from 18 to 30, most of whom were postgraduate students from a research university with diverse backgrounds in art, design, and computer science. 
In terms of experience in GenAI, 9 participants were novice users with less than one year or no experience, 2 participants had more than one year of experience, and 1 participant had over a year of experience. 
Regarding painting skills, 5 participants had no formal training, 5 had basic painting skills, and 2 were proficient. All participants were invited to take part in the user study in person.

\subsubsection{Baseline Models}
Since our focus is on novice users, we choose baseline image generative models that are easy to use and widely adopted by beginners. 
We select two methods: T2I generation and R2I generation, both of which allow hands-on exploration without requiring advanced expertise:
\begin{itemize}
    \item \textbf{T2I Generation} serves as a foundational approach, where users generate images solely based on text input. It is considered a baseline due to its simplicity and widespread use among novices. 
    \item \textbf{R2I Generation} allows users to specify prompts for particular areas within the canvas, providing more control over the final image. 
    For this purpose, we employed a state-of-the-art region-based generative model, with a Dense Diffusion method~\cite{kim2023dense,omost}, which offers advanced control while remaining accessible to novice users.
\end{itemize}

\subsubsection{System Implementation}
To ensure a fair comparison, all image generative models, including those in our system and the baseline systems, were build on the same backbone: the $ColorfulXL-Lightning$ model~\cite{colorful}. 
This model is a fine-tuned version of the SDXL model, optimized for generating images with high aesthetic scores, ensuring consistency in prompt interpretation, image quality, and style across all comparisons.
% \new{All three models were implemented as local web applications, enabling users to interact directly with them on a browser.}
For T2I generation, users were asked to use the Stable Diffusion WebUI~\cite{sd}, which is the most widely used local web platform.
For both the R2I generation system and \tool, they are implemented within the same web application, as illustrated in Figure~\ref{fig:interface}. 
Both the R2I system and \tool allow users to sketch in the same way; however, in the R2I generation condition, the advanced features of \tool are disabled. 
Specifically, in the R2I condition, users can only access a sketch board and a single text box to specify prompts for the selected region, 
without the sketch-aware prompt recommendation and spatial-condition sketch refinement.
% rather than using separate semantic spaces (e.g., type, attribute, etc.), which disabled text boxes are made invisible on the front end.


% To further assist users, they were allowed to use ChatGPT, Omost, and Promptist to generate prompts when working with the baseline models. 

\subsubsection{Procedure}
Each participant followed the procedure as outlined below. Users were allowed to take a break after each task.

\begin{itemize}
    \item \textbf{Introduction}.
    We began by collecting demographic information from the participants and obtaining their consent to participate in the study and allow for the collection and analysis of the result data. 
    Following this, we introduced both our system and the baseline systems, explaining their functionality. 
    Participants were then given five minutes to freely explore and familiarize themselves with each system.

    
    \item \textbf{Close-Ended Task}.
    We carefully designed two image generation tasks. 
    The first task involved generating a simple image featuring two main objects, while the second, more complex task required creating an image with four main objects, with more intricate relationships between them. 
    For each task, we provided a reference image and participants were asked to use both the baseline systems and our system to generate images as similar as possible to the given reference image.
    To accommodate different creative preferences, we offered three style options: realistic, animated, and painting, allowing participants to freely choose their preferred style for each task. 
    % The style-related prompts were pre-written, enabling participants to focus solely on the content of the image.
    Task assignment order was randomized to mitigate learning effects between tasks.
    The tool assignment order was fixed to T2I, R2I, and \tool. 
    % This order ensures that there is no explicit prior knowledge leakage between tools. 
    This order ensures that there is no explicit prior knowledge leakage from advanced tool to baselines. 
    For instance, using R2I after \tool could result in participants already knowing the recommended prompt from \tool when they use the R2I system, which should not be available to users in the baselines.


\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{src/img/outcome.pdf}
    \vspace{-2mm}
    \caption{Outcome satisfaction survey indicates that \tool significantly outperforms the baseline text-to-image model and region-to-image model in image quality, cohesiveness and intention alignment.}
    \Description{Outcome satisfaction survey indicates that \tool significantly outperforms the baseline text-to-image model and region-to-image model in image quality, cohesiveness and intention alignment.}
    \label{fig:outcome}
    \vspace{-3mm}
\end{figure*}
    
    \item \textbf{Open-Ended Task}.
    The open-ended task allowed users to freely create their own work using our system without any constraints. 
    They were encouraged to explore and experiment, using their own ideas throughout the process.
    To ensure the tasks accurately reflected real-world scenarios and iterative exploration, images in both the close-ended and open-ended tasks were generated using random seeds. 
    The random seeds enable users to iteratively explore different results, simulating real-world conditions of free generation.
    % where users typically do not have a fixed 'seed' for the images they aim to generate.   
    
    \item \textbf{Survey and Interview}.
    After completing the tasks, participants were asked to fill out a 7-point Likert scale questionnaire to assess the system's outcome satisfaction, feature usability, and overall performance. 
    For system features, we focused on prompt recommendation, single object decomposition, and single object adjustment. 
    In terms of overall performance, participants rated the system based on its usefulness, flexibility, controllability, and engagement. 
    Following the questionnaire, we conducted a semi-structured interview with each participant to gather detailed feedback on their experience using our system. 
    For brevity, we use parentheses (e.g., 5/12) to indicate the number of users who agreed or disagreed during the interview.

\end{itemize}



\subsection{Quantitative Evaluation of User Performance}



We evaluated user performance on close-ended tasks using Intersection over Union (IoU), which measures the area alignment between objects in reference images and user-generated results. 
To segment object masks (e.g., girl, cat in Task 1 and girl, boy, train in Task 2), we employed Segment Anything (SAM)~\cite{kirillov2023segment} for open-set segmentation and Dino~\cite{liu2023grounding} for semantic guidance. 
A higher IoU score indicates better alignment and superior performance.


\begin{table}[ht] \small
    \centering
    \caption{User performance in close-ended task measured by IoU.}
    \vspace{-2mm}
    \begin{tabular}{lcccc|cc}
    \hline 
    \textbf{Methods} & \multicolumn{2}{c}{\textbf{Task 1}} & \multicolumn{2}{c}{\textbf{Task 2}} & \multicolumn{2}{c}{\textbf{Overall}} \\
    & \textbf{mean} & \textbf{SD} & \textbf{mean} & \textbf{SD} & \textbf{mean} & \textbf{SD} \\
    \hline 
    Text-to-Image (T2I) & 0.312 & 0.115 & 0.316 & 0.083 & 0.314 & 0.100 \\
    Region-to-Image (R2I) & 0.440 & 0.105 & 0.396 & 0.099 & 0.418 & 0.102 \\
    SketchFlex  & \textbf{0.613} & 0.073 & \textbf{0.456} & 0.107 & \textbf{0.535} & 0.092 \\
    \hline
    \end{tabular}
    % \vspace{-3mm}
    \label{tab:IoU}
\end{table}


The results are shown in Table~\ref{tab:IoU}.
Overall, \tool achieved an IoU score of 0.535, outperforming the T2I method (0.314) and the R2I method (0.418). 
We first performed Kruskal-Wallis tests to evaluate the significant difference among the three methods and Welch’s t-tests to measure the significant difference between each two methods.
Tasks 1 and 2 produced Kruskal-Wallis test results with $p<0.001$ and $p<0.05$.
In task 1, \tool achieved a score of 0.613, significantly outperforming T2I (0.312, $p<0.001$) and R2I (0.440, $p<0.001$). 
However, in Task 2, the performance gap narrows, with \tool scoring 0.456 compared to T2I (0.316, $p<0.01$) and R2I (0.396, $p = 0.19$).

Task complexity appears to play a key role in these trends. 
Task 1 involves fewer objects (two) with larger areas, making it easier for users to adjust object shapes and sizes. 
In contrast, task 2 features three smaller objects, requiring more precise arrangement to match the reference image, which increases difficulty. 
This is further reflected in the standard deviation (SD) values. 
In task 1, \tool has a lower SD (0.073), indicating more consistent performance, while in Task 2, its SD increases to 0.107—slightly higher than T2I (0.083) and R2I (0.099). 
By analyzing the result, we find that firstly the flexibility of \tool allows users to make more subjective decisions about how to align masks with the reference image, and such subjective gap is enlarged in more complex task. 
For instance, some users focus on the size of objects while overlooking their exact positions, or vice versa, resulting in inconsistent outcomes across different users.
Also, while skilled users of painting can leverage this flexibility to achieve highly accurate results, less experienced users may struggle with detailed manipulations, leading to greater variability in performance. 
We further discuss the different performance for users with different skill levels in Section~\ref{sec:nVe}.
Finally, while the R2I method (0.418) performs better than T2I (0.314), its improvement is more modest compared to \tool, as loosely defined regions lack consistent size, shape, and position, limiting its precision and overall effectiveness.


\subsection{Outcome Satisfaction}\label{ssec:outcome}

\subsubsection{Quantitative Comparison}
% \subsubsection{Outcome Satisfaction}

    We compared the results generated from our system with those produced by the two baselines in close-ended tasks. 
    Outcome satisfaction was evaluated across three key dimensions: \textbf{Intention Alignment}, which measures how well the generated result aligns with the user's intended outcome; \textbf{Cohesiveness}, which assesses whether the generated image includes all objects specified in the prompt and whether they are depicted in a natural state with coherent relationships; and \textbf{Image Quality}, which evaluates the overall quality of the image, including resolution, color vibrancy, visual appeal, and absence of distortion.

    To determine significant differences between approaches, we first performed Kruskal-Wallis tests, followed by pairwise Wilcoxon signed-rank tests with Bonferroni correction for p-values ($\alpha = 0.05$). Significant values are reported at the levels of $p<0.05 (*)$, $p<0.01 (**)$, and $p<0.001 (***)$. 
    All three evaluated dimensions yielded Kruskal-Wallis test results with $p<0.001$.
    Figure~\ref{fig:outcome} presents user ratings in terms of outcome satisfaction.


\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\textwidth]{src/img/outcome_gallery_zoom.pdf}
    \vspace{-2mm}
    \caption{Outcome examples of Task 1 and Task2 show that while region-based generation offers better spatial control than Text-to-Image, it can not perform precise control, \tool can help users more precisely replicate the spatial composition of the given reference than other two baselines.}
    \Description{Outcome examples of Task 1 and Task2 show that while region-based generation offers better spatial control than Text-to-Image, it can not perform precise control, \tool can help users more precisely replicate the spatial composition of the given reference than other two baselines.}
    \label{fig:outcome_task1}
    \vspace{-1mm}
\end{figure*}


\begin{itemize}
    \item \textbf{Image Quality}.
    \tool’s outcomes (mean = 6.17, SD = 0.55) significantly outperformed both the text-to-image method (mean = 3.75, SD = 1.78, $p$ < 0.01) and the region-based image generation method (mean = 4.33, SD = 1.31, $p$ < 0.01) in terms of image quality. 
    While all participants agreed that the image quality produced by \tool was superior, most (8/12) felt that the gap in image quality between the methods was not substantial. 
    This can be attributed to the backbone model, which is fine-tuned to generate images with high aesthetic scores, demonstrating strong generative capabilities across different conditions.
    Additionally, half of the participants (7/12) reported that region-based generation with initial prompts sometimes resulted in lower image quality compared to text-to-image generation under similar prompt settings. 
    For instance, P6 mentioned, "\emph{I wrote a very simple prompt for the colored regions and didn’t write a prompt for the background, and the generated result ended up being just some decorative patterns without any meaningful content}."
    This occurs because prompts are more critical in region-based models, and when certain areas are left without specific prompts, it can result in noticeable defects in image quality. 
    This observation partially explains why there was no significant gap between the text-to-image and region-based image generation methods ($p$ = 0.159).

    \item \textbf{Cohesiveness}.
    The results show that \tool generates more cohesive images (mean = 5.92, SD = 0.64) compared to both text-to-image (mean = 3.50, SD = 1.19, $p<0.01$) and region-based image generation (mean = 4.25, SD = 1.42, $p<0.01$).
    Half of the participants (7/12) reported that the low cohesiveness in images generated by the text-to-image method was due to misunderstandings about the number of objects or the mixing of attributes between different objects,
    while region-based generation improved this issue through region control($p<0.05$).
    A few participants (3/12) reported that the improvement of outcome in cohesiveness by \tool compared to region-based generation was largely due to the prompt recommendation system, which assigns coherent prompts to specific regions.
    As P3 noted, "\emph{When I only used simple prompts for the region-based generation, it often missed some objects. But once the system filled in these prompts, the entire image looked much better, with all the objects appearing in the right place}" (P3, Figure~\ref{fig:outcome_task1}).

    \item \textbf{Intention Alignment}.
    Participants gave significantly higher ratings for \tool in generating intention-aligned images (mean = 6.16, SD = 0.89) compared to the text-to-image method (mean = 3.16, SD = 1.34, $p<0.01$) and the region-based image generation method (mean = 4.08, SD = 1.55, $p<0.01$). 
    All participants noted that the precise shape fixing and spatial adjustments made it much easier to create the desired image.
    P9 remarked, "\emph{I can easily adjust the location and relationships between objects, and it closely matches the image in my mind after I make adjustments}." P7 also noted, "\emph{The system allows me to first fix the content of the image in my mind... The interpretation of text varies, but this visually maps what I am thinking}."
    Additionally, two participants mentioned that the improved intention alignment was partly due to the prompt recommendation feature, which provided suitable prompts corresponding to their sketches, particularly for state-related prompts they may not have considered on their own.
\end{itemize}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\textwidth]{src/img/outcome_create.pdf}
    \vspace{-3mm}
    \caption{Outcome examples of open-ended task show that users can freely create sophisticated images with varying degree of complexity.}
    \Description{Outcome examples of open-ended task show that users can freely create sophisticated images with varying degree of complexity.}
    \label{fig:outcome_task2}
    % \vspace{-3mm}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{src/img/feature.pdf}
    \vspace{-2mm}
    \caption{User ratings of main features of \tool, including Prompt Recommendation, Single Object Decomposition and Single Object Adjustment.}
    \Description{User ratings of main features of \tool, including Prompt Recommendation, Single Object Decomposition and Single Object Adjustment.}
    \label{fig:feature}
    % \vspace{-3mm}
\end{figure*}

\subsubsection{Qualitative Examples and feedback}
Figure ~\ref{fig:outcome_task1} shows examples of user-generated outcomes in Task 1 and Task 2, showcasing both realistic and anime styles.
For task 1, \tool generates a result that closely resembles the reference image, with the cat positioned in the left half and the girl's kneeling posture and hand occupying the right half. In contrast, the other results exhibit issues such as incorrect size or position.
For task 2, \tool accurately depicts two people holding hands while slightly facing each other. In comparison, the other results either fail to show them holding hands or display incorrect orientation or position.
Participants (5/12) reported that while using text to generate images can achieve similar postures, attributes, and actions, it often struggles to position objects correctly. 
Region-based generation offers better control over object location and size, but participants found it challenging to control the precise shape and state of objects with rough region sketches (9/12).
P3, P6, P7, and P10 mentioned the difficulty in positioning the train horizontally across the couple in the scene, but they found it much easier with our system.
P7 and P10 commented that the system greatly simplified this task.


With \tool, participants were able to generate images with objects in the exact shape, location, and relationships they intended, making the outcomes much more aligned with the reference images. 
P1 remarked, "\emph{I can choose the desired object shape and fix it in the generated result, like the girl and cat. The exact shape, posture, and direction are the most significant improvements compared to other methods. The system combines both text and region controls to provide shape fixing and adjustment, which is incredibly useful}" (P1, Figure~\ref{fig:outcome_task1}).
Figure ~\ref{fig:outcome_task2} illustrates examples of creative drawings produced by users with our system. Participants were able to create high-quality images that aligned with their rough region sketches. P2 commented, "The system can convert my simple input into an image, a highly complete one. It makes me feel like I can create much more freely."
% Some participants also noted that the system offered pleasant surprises and helped them explore more possibilities. P8 observed, "\emph{I initially drew a simple sketch and just thought the object should be roughly in the right position, but not exactly what it should look like. The system gave me options, and I realized that what it generated was actually what I wanted, even though I hadn’t expected it}" (P8, Figure~\ref{fig:outcome_task2}).

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{src/img/single_feature_ablation.pdf}
    \vspace{-2mm}
    \caption{Examples showing single feature influence on the results, including Prompt Recommendation, Single Object Decomposition, and Single Object Adjustment.}
    \Description{Examples showing single feature influence on the results, including Prompt Recommendation, Single Object Decomposition, and Single Object Adjustment.}
    \label{fig:feature_ablation}
    \vspace{-1mm}
\end{figure*}

\subsection{System Evaluation}
\subsubsection{System Feature Rating}\label{ssec:featurerating}
We report users' ratings of our system's different features: \textit{Prompt Recommendation}, \textit{Single Object Decomposition}, and \textit{Single Object Adjustment}.
The Prompt Recommendation component includes semantic space guidance for user input as well as automatic prompt recommendation. The Single Object Decomposition module handles the decomposition and refinement of individual objects, while the Single Object Adjustment enables users to adjust the location and size of each object's shape.
Overall, all features were well-received by participants. 
The results are shown in Figure~\ref{fig:feature}.

\textbf{Prompt Recommendation}. Most participants (9/12) reported that the system significantly reduced the time spent crafting detailed prompts for each region without compromising image quality (Q1, mean: 6.08, SD: 0.75). 
P11 commented, "\emph{It quickly helps me fill in the prompts. When there are many regions, I just don’t know what the appropriate words are}."
In addition, all participants agreed that the semantic space made it easier to adjust the prompts (Q2, mean: 6.16, SD: 0.55). 
P7 explained, "\emph{If the generated image has something I don't want, I know where to find the prompt and quickly change it. If there were only a text box, it would be time-consuming to find the right one to modify}."
However, a few participants mentioned that the prompt recommendation can sometimes cause frustration when recommended prompts didn’t match their expectations. 
P2 stated, "\emph{It recommends many prompts at once, and if there are a lot I want to change, I have to rewrite them one by one}."

\textbf{Single Object Decomposition}. 
The single object decomposition feature was the most appreciated aspect of \tool. 
Participants found it highly beneficial in refining rough sketches into their desired fine-grained shapes, helping to reduce cognitive load while aligning the generated result with their intentions (Q3 \& Q4, mean: 6.5, SD: 0.64 \& mean: 5.5, SD: 0.64). 
P11 noted, "\emph{The system can quickly generate options for your rough sketch. If you don’t like it, you can keep iterating until you find one that suits you}."

\textbf{Single Object Adjustment}. 
All participants agreed that the single object adjustment was intuitive and efficiently guided image generation. 
Some participants (4/12) particularly praised its ease of use and simplicity (Q5, mean: 6.1, SD: 0.79). 
Regarding the controllability of the shapes users chose and adjusted, participants reported that it largely generated shapes as intended (Q6, mean: 6.3, SD: 0.62).
Additionally, a few participants applauded the flexibility of the system, noting that the shapes weren’t fully fixed, leaving room for subtle variations in the generation. 
P4 shared, "\emph{I positioned the boy and girl closer together and wrote a prompt for them to hold hands. Initially, they were just standing, but when the image was generated, they were actually holding hands. It was amazing}."




\subsubsection{System Feature Influence on Results}
This section reports how individual feature of \tool affects generated results in the user study, with examples illustrated in Figure~\ref{fig:feature_ablation}.

\textbf{Prompt Recommendation}. Prompt recommendation plays a crucial role in enhancing the overall cohesiveness of generated images. 
As illustrated in the second column of Figure~\ref{fig:feature_ablation}, when participants create images using their sketches and self-written prompts, several issues arise. 
These include \textit{missing objects} in [column 2 - rows 3, 4], \textit{misaligned sizes} in [column 2 - rows 1, 2, 4], and \textit{lack of natural interaction} in [column 2 - rows 1, 2].
By using the recommended prompts, as shown in column 3, these issues are significantly mitigated. 
A majority of participants (8/12) acknowledged encountering issues such as missing objects or unnatural object interaction/perception, and noted that the recommended prompts alleviated these problems. 
As P1 reported, "\emph{[with prompt recommendation], the generated image is definitely more aligned with my sketch and looks more harmonious.}"


\textbf{Single Object Decomposition.} Our decomposed strategy for single object generation significantly enhances the alignment between the generated object shapes and user preferences by providing fine-grained shape candidates and enabling iterative adjustments, as shown in column 4.
For example, in row 1, the user generated and selected a little girl with her hands slightly lifted to touch the cat, instead of the original depiction of a mature girl with her hand resting on her leg. 
This action of touching the cat more closely aligns with reference image 1 in Figure~\ref{fig:outcome_task1}. 
In row 3, the user generated and selected a girl in the green area of the sketch with a jumping posture that is more expressive and aligns better with their preferences. 
These chosen object shapes are then fixed in subsequent generations to preserve the desired object features while allowing other elements of the image to change, as demonstrated in columns 5 and 6.
Additionally, some participants mentioned that single object generation helps them explore potential shapes when they have no clear idea in mind. 
For instance, P6 remarked, "\emph{The black cat surprisingly fit the scenario that I want. It is not exactly what I'm looking for at first, but when it was generated, I knew it was the right one.}"

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.82\textwidth]{src/img/overall.pdf}
    \vspace{-2mm}
    \caption{Overall usability ratings of system compared with two baselines.}
    \Description{Overall usability ratings of system compared with two baselines.}
    \label{fig:overall}
    \vspace{-3mm}
\end{figure*}

\textbf{Single Object Adjustment.} Single Object Adjustment during refinement provides flexible, fine-grained control, enabling iterative adjustments to better align the overall composition with user expectations, as shown in column 5. 
For example, in the first row, while maintaining the cat's fixed shape, its size was enlarged to better align with reference image 1 in Figure~\ref{fig:outcome_task1}.
In the second row, the train's size and location were slightly adjusted to more closely match reference image 2.
In the third row, the left person was moved downward slightly to increase the height gap between the two individuals, creating the impression that the right girl is jumping to defend the boy. 
We found that when the generated object did not perfectly match user expectations, all participants preferred using single object adjustment over adjusting their sketch. 
It indicates that single object adjustment was deemed a more flexible and stable way to achieve fine-grained refinements.

In most cases, users do not employ single object decomposition and adjustment as separate features but instead use them as complementary tools in an iterative generation process, as seen in column 6. 
For instance, in row 2, the user generated a new train and adjusted its position to stretch across the image. 
In row 3, the right girl was regenerated into a new shape and moved slightly to the right of the original mask position. 
Users alternated between modifying object shapes and adjusting their location or size until they were satisfied with the final result.

\subsubsection{System Overall Rating}\label{ssec:overallrating}

Figure~\ref{fig:overall} illustrates the user rating of the overall system comparison.

\textbf{Usefulness}. Most participants agreed that our system (mean = 6.16, SD = 0.55) was significantly more useful than both text-to-image (mean = 3.91, SD = 1.25) and region-based image generation methods (mean = 4.83, SD = 0.79).

\textbf{Flexibility}. In terms of flexibility, participants found that our system (mean = 6.41, SD = 1.60) offered far more input flexibility compared to text-to-image (mean = 3.50, SD = 1.60) and region-to-image (mean = 4.66, SD = 1.78). This flexibility stems from the ability to adjust not only text and sketches, but also individual objects, providing significantly more control over the image generation process (10/12 participants). As P5 noted, "\emph{With both text-to-image and region-based methods, there's always a part of the image that looks right and another part that doesn’t. But when I change the prompt or region, the whole image changes, making it difficult to get everything just right. This system solves that problem}."

\textbf{Controllability}. All participants agreed that \tool (mean = 6.25, SD = 0.43) provided better control than text-to-image (mean = 3.41, SD = 1.25) and region-based generation (mean = 4.50, SD = 1.26). 
\tool allows control over text and regions and to choose and fix specific shapes, enabling more precise adjustments.

\textbf{Engaging}. Participants also found our system to be more engaging (mean = 6.41, SD = 0.64) compared to text-to-image (mean = 3.41, SD = 1.25) and region-based methods (mean = 5.41, SD = 0.86). P6 remarked, "\emph{The various interaction methods made the process more engaging, helping me stay focused while iterating and refining the artwork to better match the imagery in my mind.}."

\textbf{Future use}. Lastly, participants saw greater future potential in our system (mean = 6.41, SD = 0.64) compared to text-to-image (mean = 4.50, SD = 1.61) and region-based generation (mean = 5.33, SD = 1.03), particularly for non-experts who could quickly create complete images with desired compositions (5/12 participants). However, a few participants (2/12) felt that the methods could have equally significant future potential.
% there was no significant difference in future potential among the methods. 
As P3 observed, "\emph{I think all the methods have future potential, but for different scenarios. Some areas require specific control, while others benefit from abstract control as inspiration. Overall, this system provides more options, especially in terms of controllability and ease of use}."













