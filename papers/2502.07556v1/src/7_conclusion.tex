\section{Discussion}\label{sec:discussion}



\subsection{From Interactive Prompting to Interactive Multi-modal Prompting}
The rapid advancements of large pre-trained generative models including large language models and text-to-image generation models, have inspired many HCI researchers to develop interactive tools to support users in crafting appropriate prompts.
% Studies on this topic in last two years' HCI conferences are predominantly focused on helping users refine single-modality textual prompts.
Many previous studies are focused on helping users refine single-modality textual prompts.
However, for many real-world applications concerning data beyond text modality, such as multi-modal AI and embodied intelligence, information from other modalities is essential in constructing sophisticated multi-modal prompts that fully convey users' instruction.
This demand inspires some researchers to develop multimodal prompting interactions to facilitate generation tasks ranging from visual modality image generation~\cite{wang2024promptcharm, promptpaint} to textual modality story generation~\cite{chung2022tale}.
% Some previous studies contributed relevant findings on this topic. 
Specifically, for the image generation task, recent studies have contributed some relevant findings on multi-modal prompting.
For example, PromptCharm~\cite{wang2024promptcharm} discovers the importance of multimodal feedback in refining initial text-based prompting in diffusion models.
However, the multi-modal interactions in PromptCharm are mainly focused on the feedback empowered the inpainting function, instead of supporting initial multimodal sketch-prompt control. 

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{src/img/novice_expert.pdf}
    \vspace{-2mm}
    \caption{The comparison between novice and expert participants in painting reveals that experts produce more accurate and fine-grained sketches, resulting in closer alignment with reference images in close-ended tasks. Conversely, in open-ended tasks, expert fine-grained strokes fail to generate precise results due to \tool's lack of control at the thin stroke level.}
    \Description{The comparison between novice and expert participants in painting reveals that experts produce more accurate and fine-grained sketches, resulting in closer alignment with reference images in close-ended tasks. Novice users create rougher sketches with less accuracy in shape. Conversely, in open-ended tasks, expert fine-grained strokes fail to generate precise results due to \tool's lack of control at the thin stroke level, while novice users' broader strokes yield results more aligned with their sketches.}
    \label{fig:novice_expert}
    % \vspace{-3mm}
\end{figure*}


% In particular, in the initial control input, users are unable to explicitly specify multi-modal generation intents.
In another example, PromptPaint~\cite{promptpaint} stresses the importance of paint-medium-like interactions and introduces Prompt stencil functions that allow users to perform fine-grained controls with localized image generation. 
However, insufficient spatial control (\eg, PromptPaint only allows for single-object prompt stencil at a time) and unstable models can still leave some users feeling the uncertainty of AI and a varying degree of ownership of the generated artwork~\cite{promptpaint}.
% As a result, the gap between intuitive multi-modal or paint-medium-like control and the current prompting interface still exists, which requires further research on multi-modal prompting interactions.
From this perspective, our work seeks to further enhance multi-object spatial-semantic prompting control by users' natural sketching.
However, there are still some challenges to be resolved, such as consistent multi-object generation in multiple rounds to increase stability and improved understanding of user sketches.   


% \new{
% From this perspective, our work is a step forward in this direction by allowing multi-object spatial-semantic prompting control by users' natural sketching, which considers the interplay between multiple sketch regions.
% % To further advance the multi-modal prompting experience, there are some aspects we identify to be important.
% % One of the important aspects is enhancing the consistency and stability of multiple rounds of generation to reduce the uncertainty and loss of control on users' part.
% % For this purpose, we need to develop techniques to incorporate consistent generation~\cite{tewel2024training} into multi-modal prompting framework.}
% % Another important aspect is improving generative models' understanding of the implicit user intents \new{implied by the paint-medium-like or sketch-based input (\eg, sketch of two people with their hands slightly overlapping indicates holding hand without needing explicit prompt).
% % This can facilitate more natural control and alleviate users' effort in tuning the textual prompt.
% % In addition, it can increase users' sense of ownership as the generated results can be more aligned with their sketching intents.
% }
% For example, when users draw sketches of two people with their hands slightly overlapping, current region-based models cannot automatically infer users' implicit intention that the two people are holding hands.
% Instead, they still require users to explicitly specify in the prompt such relationship.
% \tool addresses this through sketch-aware prompt recommendation to fill in the necessary semantic information, alleviating users' workload.
% However, some users want the generative AI in the future to be able to directly infer this natural implicit intentions from the sketches without additional prompting since prompt recommendation can still be unstable sometimes.


% \new{
% Besides visual generation, 
% }
% For example, one of the important aspect is referring~\cite{he2024multi}, linking specific text semantics with specific spatial object, which is partly what we do in our sketch-aware prompt recommendation.
% Analogously, in natural communication between humans, text or audio alone often cannot suffice in expressing the speakers' intentions, and speakers often need to refer to an existing spatial object or draw out an illustration of her ideas for better explanation.
% Philosophically, we HCI researchers are mostly concerned about the human-end experience in human-AI communications.
% However, studies on prompting is unique in that we should not just care about the human-end interaction, but also make sure that AI can really get what the human means and produce intention-aligned output.
% Such consideration can drastically impact the design of prompting interactions in human-AI collaboration applications.
% On this note, although studies on multi-modal interactions is a well-established topic in HCI community, it remains a challenging problem what kind of multi-modal information is really effective in helping humans convey their ideas to current and next generation large AI models.




\subsection{Novice Performance vs. Expert Performance}\label{sec:nVe}
In this section we discuss the performance difference between novice and expert regarding experience in painting and prompting.
First, regarding painting skills, some participants with experience (4/12) preferred to draw accurate and fine-grained shapes at the beginning. 
All novice users (5/12) draw rough and less accurate shapes, while some participants with basic painting skills (3/12) also favored sketching rough areas of objects, as exemplified in Figure~\ref{fig:novice_expert}.
The experienced participants using fine-grained strokes (4/12, none of whom were experienced in prompting) achieved higher IoU scores (0.557) in the close-ended task (0.535) when using \tool. 
This is because their sketches were closer in shape and location to the reference, making the single object decomposition result more accurate.
Also, experienced participants are better at arranging spatial location and size of objects than novice participants.
However, some experienced participants (3/12) have mentioned that the fine-grained stroke sometimes makes them frustrated.
As P1's comment for his result in open-ended task: "\emph{It seems it cannot understand thin strokes; even if the shape is accurate, it can only generate content roughly around the area, especially when there is overlapping.}" 
This suggests that while \tool\ provides rough control to produce reasonably fine results from less accurate sketches for novice users, it may disappoint experienced users seeking more precise control through finer strokes. 
As shown in the last column in Figure~\ref{fig:novice_expert}, the dragon hovering in the sky was wrongly turned into a standing large dragon by \tool.

Second, regarding prompting skills, 3 out of 12 participants had one or more years of experience in T2I prompting. These participants used more modifiers than others during both T2I and R2I tasks.
Their performance in the T2I (0.335) and R2I (0.469) tasks showed higher scores than the average T2I (0.314) and R2I (0.418), but there was no performance improvement with \tool\ between their results (0.508) and the overall average score (0.528). 
This indicates that \tool\ can assist novice users in prompting, enabling them to produce satisfactory images similar to those created by users with prompting expertise.



\subsection{Applicability of \tool}
The feedback from user study highlighted several potential applications for our system. 
Three participants (P2, P6, P8) mentioned its possible use in commercial advertising design, emphasizing the importance of controllability for such work. 
They noted that the system's flexibility allows designers to quickly experiment with different settings.
Some participants (N = 3) also mentioned its potential for digital asset creation, particularly for game asset design. 
P7, a game mod developer, found the system highly useful for mod development. 
He explained: "\emph{Mods often require a series of images with a consistent theme and specific spatial requirements. 
For example, in a sacrifice scene, how the objects are arranged is closely tied to the mod's background. It would be difficult for a developer without professional skills, but with this system, it is possible to quickly construct such images}."
A few participants expressed similar thoughts regarding its use in scene construction, such as in film production. 
An interesting suggestion came from participant P4, who proposed its application in crime scene description. 
She pointed out that witnesses are often not skilled artists, and typically describe crime scenes verbally while someone else illustrates their account. 
With this system, witnesses could more easily express what they saw themselves, potentially producing depictions closer to the real events. "\emph{Details like object locations and distances from buildings can be easily conveyed using the system}," she added.

% \subsection{Model Understanding of Users' Implicit Intents}
% In region-sketch-based control of generative models, a significant gap between interaction design and actual implementation is the model's failure in understanding users' naturally expressed intentions.
% For example, when users draw sketches of two people with their hands slightly overlapping, current region-based models cannot automatically infer users' implicit intention that the two people are holding hands.
% Instead, they still require users to explicitly specify in the prompt such relationship.
% \tool addresses this through sketch-aware prompt recommendation to fill in the necessary semantic information, alleviating users' workload.
% However, some users want the generative AI in the future to be able to directly infer this natural implicit intentions from the sketches without additional prompting since prompt recommendation can still be unstable sometimes.
% This problem reflects a more general dilemma, which ubiquitously exists in all forms of conditioned control for generative models such as canny or scribble control.
% This is because all the control models are trained on pairs of explicit control signal and target image, which is lacking further interpretation or customization of the user intentions behind the seemingly straightforward input.
% For another example, the generative models cannot understand what abstraction level the user has in mind for her personal scribbles.
% Such problems leave more challenges to be addressed by future human-AI co-creation research.
% One possible direction is fine-tuning the conditioned models on individual user's conditioned control data to provide more customized interpretation. 

% \subsection{Balance between recommendation and autonomy}
% AIGC tools are a typical example of 
\subsection{Progressive Sketching}
Currently \tool is mainly aimed at novice users who are only capable of creating very rough sketches by themselves.
However, more accomplished painters or even professional artists typically have a coarse-to-fine creative process. 
Such a process is most evident in painting styles like traditional oil painting or digital impasto painting, where artists first quickly lay down large color patches to outline the most primitive proportion and structure of visual elements.
After that, the artists will progressively add layers of finer color strokes to the canvas to gradually refine the painting to an exquisite piece of artwork.
One participant in our user study (P1) , as a professional painter, has mentioned a similar point "\emph{
I think it is useful for laying out the big picture, give some inspirations for the initial drawing stage}."
Therefore, rough sketch also plays a part in the professional artists' creation process, yet it is more challenging to integrate AI into this more complex coarse-to-fine procedure.
Particularly, artists would like to preserve some of their finer strokes in later progression, not just the shape of the initial sketch.
In addition, instead of requiring the tool to generate a finished piece of artwork, some artists may prefer a model that can generate another more accurate sketch based on the initial one, and leave the final coloring and refining to the artists themselves.
To accommodate these diverse progressive sketching requirements, a more advanced sketch-based AI-assisted creation tool should be developed that can seamlessly enable artist intervention at any stage of the sketch and maximally preserve their creative intents to the finest level. 

\subsection{Ethical Issues}
Intellectual property and unethical misuse are two potential ethical concerns of AI-assisted creative tools, particularly those targeting novice users.
In terms of intellectual property, \tool hands over to novice users more control, giving them a higher sense of ownership of the creation.
However, the question still remains: how much contribution from the user's part constitutes full authorship of the artwork?
As \tool still relies on backbone generative models which may be trained on uncopyrighted data largely responsible for turning the sketch into finished artwork, we should design some mechanisms to circumvent this risk.
For example, we can allow artists to upload backbone models trained on their own artworks to integrate with our sketch control.
Regarding unethical misuse, \tool makes fine-grained spatial control more accessible to novice users, who may maliciously generate inappropriate content such as more realistic deepfake with specific postures they want or other explicit content.
To address this issue, we plan to incorporate a more sophisticated filtering mechanism that can detect and screen unethical content with more complex spatial-semantic conditions. 
% In the future, we plan to enable artists to upload their own style model

% \subsection{From interactive prompting to interactive spatial prompting}


\subsection{Limitations and Future work}

    \textbf{User Study Design}. Our open-ended task assesses the usability of \tool's system features in general use cases. To further examine aspects such as creativity and controllability across different methods, the open-ended task could be improved by incorporating baselines to provide more insightful comparative analysis. 
    Besides, in close-ended tasks, while the fixing order of tool usage prevents prior knowledge leakage, it might introduce learning effects. In our study, we include practice sessions for the three systems before the formal task to mitigate these effects. In the future, utilizing parallel tests (\textit{e.g.} different content with the same difficulty) or adding a control group could further reduce the learning effects.

    \textbf{Failure Cases}. There are certain failure cases with \tool that can limit its usability. 
    Firstly, when there are three or more objects with similar semantics, objects may still be missing despite prompt recommendations. 
    Secondly, if an object's stroke is thin, \tool may incorrectly interpret it as a full area, as demonstrated in the expert results of the open-ended task in Figure~\ref{fig:novice_expert}. 
    Finally, sometimes inclusion relationships (\textit{e.g.} inside) between objects cannot be generated correctly, partially due to biases in the base model that lack training samples with such relationship. 

    \textbf{More support for single object adjustment}.
    Participants (N=4) suggested that additional control features should be introduced, beyond just adjusting size and location. They noted that when objects overlap, they cannot freely control which object appears on top or which should be covered, and overlapping areas are currently not allowed.
    They proposed adding features such as layer control and depth control within the single-object mask manipulation. Currently, the system assigns layers based on color order, but future versions should allow users to adjust the layer of each object freely, while considering weighted prompts for overlapping areas.

    \textbf{More customized generation ability}.
    Our current system is built around a single model $ColorfulXL-Lightning$, which limits its ability to fully support the diverse creative needs of users. Feedback from participants has indicated a strong desire for more flexibility in style and personalization, such as integrating fine-tuned models that cater to specific artistic styles or individual preferences. 
    This limitation restricts the ability to adapt to varied creative intents across different users and contexts.
    In future iterations, we plan to address this by embedding a model selection feature, allowing users to choose from a variety of pre-trained or custom fine-tuned models that better align with their stylistic preferences. 
    
    \textbf{Integrate other model functions}.
    Our current system is compatible with many existing tools, such as Promptist~\cite{hao2024optimizing} and Magic Prompt, allowing users to iteratively generate prompts for single objects. However, the integration of these functions is somewhat limited in scope, and users may benefit from a broader range of interactive options, especially for more complex generation tasks. Additionally, for multimodal large models, users can currently explore using affordable or open-source models like Qwen2-VL~\cite{qwen} and InternVL2-Llama3~\cite{llama}, which have demonstrated solid inference performance in our tests. While GPT-4o remains a leading choice, alternative models also offer competitive results.
    Moving forward, we aim to integrate more multimodal large models into the system, giving users the flexibility to choose the models that best fit their needs. 
    


\section{Conclusion}\label{sec:conclusion}
In this paper, we present \tool, an interactive system designed to help novice users create high-quality, fine-grained images that align with their intentions based on rough sketches. 
The system first refines the user's initial prompt into a complete and coherent one that matches the rough sketch, ensuring the generated results are both stable, coherent and high quality.
To further support users in achieving fine-grained alignment between the generated image and their creative intent without requiring professional skills, we introduce a decompose-and-recompose strategy. 
This allows users to select desired, refined object shapes for individual decomposed objects and then recombine them, providing flexible mask manipulation for precise spatial control.
The framework operates through a coarse-to-fine process, enabling iterative and fine-grained control that is not possible with traditional end-to-end generation methods. 
Our user study demonstrates that \tool offers novice users enhanced flexibility in control and fine-grained alignment between their intentions and the generated images.
