\section{\tool System}\label{sec:system_description}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.85\textwidth]{src/img/inference.pdf}
    \vspace{-2mm}
    \caption{Our sketch-aware prompt recommendation first builds a semantic space through data-driven analysis of key semantic elements covering single object and cross object properties. Then the semantic space is integrated with retrieval of attributes and relationships reference from semantic dataset. Finally, these semantic guidance is combined with users' initial sketch to form a sketch-aware multi-modal prompt to the MLLM to support spatial-aware inference.}
    \Description{Our sketch-aware prompt recommendation first builds a semantic space through data-driven analysis of key semantic elements covering single object and cross-region properties. Then the semantic space is integrated with retrieval of attributes and relationships reference from semantic dataset. Finally, these semantic guidance is combined with users' initial sketch to form a sketch-aware multi-modal prompt to the MLLM to support spatial-aware inference.}
    \label{fig:inference}
    % \vspace{-3mm}
\end{figure*}

Based on the design goals, we design \tool, an interactive system that allows users to generate images controlled by inputs of rough sketches and simple prompts, while generating semantically cohesive and region-controlled images.
The overall framework of \tool is shown in Figure~\ref{fig:workflow}. 
The framework consists of three stages: 1) the user can draw a sketch, assign a corresponding regional prompt, and use automatic prompt recommendation to refine their initial prompt \textbf{(G1, G2)}; 2) the sketch with object decomposition and single-object generation \textbf{(G3)}; and 3) spatial adjustment and anchoring of object shapes   \textbf{(G1, G3)}.


\subsection{Sketch-Aware Prompt Recommendation}
\label{ssec:prompt_rec}
Crafting effective prompts for rough sketch-based image generation is a challenging task, as users must not only create prompts for each individual region but also ensure coherence across the entire image. 
We introduce a prompt recommendation method that automatically enhances the user’s initial input, to produce a spatially cohesive prompt that aligns with the overall composition.
Figure~\ref{fig:inference} shows the overall workflow of this process.


\subsubsection{Semantic Space Reasoning}
\begin{table}[thb] \small
    \centering
    \caption{Common examples in the semantic space across various T2I description datasets.}
    \vspace{-2mm}
    \begin{tabular}{|l|l|c|}
    \hline 
    \textbf{Space Item} & \textbf{Property} & \textbf{Instance} \\
    \hline 
    \multirow{3}{*}{\textbf{Single Object}} 
    & Type &   Man, Car, Dog, Tree, Window \\
    && Table, Ocean, Park, Wall\\ \cline{2-3}
    & Attribute & Wooden, Tall, Red, Large\\
    &&Fluffy, Round, Slim, Silver   \\ \cline{2-3}
    & State &   Standing, Moving, Swaying, \\
    &&Broken, Sleeping, Lying\\\hline
    \multirow{2}{*}{\textbf{Cross Object}} 
    &  Direction  &  Facing to, Aligned in, \\
    &&Diagonally placed\\\cline{2-3}
    & Relationship & Next to, Under, Parked on, \\
    &&Sitting by, Supporting\\ \hline
    \multirow{3}{*}{\textbf{Overall}}  
    & Lightning  &   Natrual daylight, indoor lighting, \\
    && Soft light, Moon light\\\cline{2-3}
    & Camera  &   Close-up shot, Wide-angle shot, \\
    && Overhead shot, Extreme long shot \\\cline{2-3}
    & Style  &   Realistic, Minimalist, Cinematic, \\
    && Abstractm, Anime, Oil painting\\
    \hline
    \end{tabular}
    \vspace{-1mm}
    \label{tab:example_space}
\end{table}



Previous prompt-tuning methods have primarily focused on text-to-image models, which offer limited support for refining individual region prompts and often struggle to ensure cohesiveness across the entire image.
To address this, the first step is to identify the types of prompts needed to generate a coherent image under rough sketch-based control.
Specifically, we define an "\emph{semantic space}" that provides intuitive guidance that helps users to easily input and adjust their prompts in the appropriate regions.
The process of constructing this semantic space involves analyzing online sources~\cite{sd,wang2022diffusiondb,civitai,xie2023prompt} and prompt-guideline literature~\cite{oppenlaender2023prompting,oppenlaender2023taxonomy,liu2022design} to identify critical elements that contribute to high-quality image generation. Additionally, examining image description datasets in computer vision—including traditional task datasets~\cite{caesar2018coco,pham2021learning,krishna2017visual} and recent datasets tailored for image generation~\cite{onoe2024docci}—helps uncover relevant dimensions for describing images.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.995\textwidth]{src/img/refinement.pdf}
    \vspace{-2mm}
    \caption{Spatial-condition sketch refinement can help novice users refine their sketch by generating more realistic and accurate sketch for each object through single object decomposition and generation, and subsequently allowing users to interactively refine the sketch by object selection and spatial adjustment.}
    \Description{Spatial-condition sketch refinement can help novice users refine their sketch by generating more realistic and accurate sketch for each object through single object decomposition and generation, and subsequently allowing users to interactively refine the sketch by object selection and spatial adjustment.}
    \label{fig:decompose}
    \vspace{-3mm}
\end{figure*}

\begin{table*}[thb] 
    \centering
    \caption{Example Statistics of objects, attributes and relationships in Visual Genome~\cite{krishna2017visual} and VAW~\cite{pham2021learning}.}
    \vspace{-2mm}
    \begin{tabular}{lccccc}
    \hline 
    \textbf{Space Item} & \textbf{1st} & \textbf{2nd} & \textbf{3rd} & \textbf{20th} & \textbf{50th}  \\
    \hline 
    Object & window (52k)  & man (52k) & shirt (39k) & trees (17k) & sidewalk (8k)  \\
    Attribute & white (311k) & black (195k) &  blue (118k) & clear (15k) & colorful (5.8k)   \\
    Relationship  & on (645k) & has (245k) &  in (219k) & sitting on (13k) & laying on (3.5k)    \\
    \hline
    \end{tabular}
    \vspace{-1mm}
    \label{tab:statsitic}
\end{table*}

We summarize these prompt aspects and conduct experiments to identify the key components essential for high-quality output. 
% Finally, we propose a semantic space that integrates these necessary elements to ensure coherence when generating images from rough sketches.
Table~\ref{tab:example_space} presents the common dimensions and example instances of the identified semantic space within the T2I datasets.
The specific elements within the space are listed below.
\begin{itemize}
    \item \textbf{Single Object Prompt} contains local prompts for each single object. 
    It contains \textit{type} of the object; \textit{attribute} of object including main attributes such as color, texture, shape; and \textit{state} indicates how the object acts, including still, standing, running, etc. The background is a special object that only has type and attribute.
    \item \textbf{Cross Object Prompt} explicitly specifies how objects in different regions interact, which is crucial for the coherence of the generated image.
    It includes \textit{direction} of objects and \textit{relationship} that multiple objects interact with each other.
    \item \textbf{Overall Prompt} does not directly affect multi-object cohesiveness but allows users to optionally specify the overall visual effect, including \textit{lighting}, \textit{style} and \textit{camera}.
    % \item \textbf{Overall Prompt} is not indispensable for a semantically cohesive image generation, but has significant impact in visual effect, include \textit{lighting}, \textit{style} and \textit{camera}. 
    % Previous works consider quality modifiers (e.g., best quality, high resolution). In our experiment, the state-of-the-art model can already generate high quality images without these modifiers.
\end{itemize}




Figure~\ref{fig:inference} (right) illustrates a completed semantic space for a sketch featuring a girl, a cat, and a background (\textit{i.e.}, areas without objects).
Once the individual prompt components are defined, the separate prompts within a single region are concatenated into one unified prompt using commas (\textit{e.g.}, "type: girl, attribute: long hair" becomes "girl, long hair"). 
% To ensure that the concatenated prompt remains within the 77-token limit imposed by CLIP, we employ the bag-of-conditions approach~\cite{omost}.}



\subsubsection{Sketch-Augmented Prompting}
While the semantic space simplifies the prompt input and adjustment process, manually entering all prompts and identifying the appropriate ones can still be labor intensive and cognitively demanding, particularly when dealing with many objects. 
To alleviate this burden, we utilize a MLLM GPT-4o, to automatically complete the semantic space based on the user's sketch and initial prompt.

Specifically, the user’s initial prompt and rough sketch are input into the MLLM, with the semantic space acting as a contextual guide within the prompt template. The model generates prompts to populate the semantic space, incorporating both the initial input prompt and the sketch. It utilizes spatial reasoning, guided by the Chain-of-Thought~\cite{wei2022chain} strategy, to account for the \textit{shape}, \textit{location}, and \textit{interaction} of the objects within the user's sketch.
However, relying solely on the MLLM to fill the semantic space can be risky, as it may overfit to certain content or produce results with reduced coherence~\cite{cao2023beautifulprompt}. 
To address this, we further enhance the process by retrieving reference attributes and relationships between objects from crowd-sourced text-image datasets based on real-world images~\cite{pham2021learning,krishna2017visual}. 
The datasets are organized into a dictionary, where object names serve as keys and their corresponding attributes or relationships are stored as values. 
During retrieval, \tool randomly samples $k=10$ examples from the values based on the given object names as keys, providing the MLLM with reference data. 
The raw datasets are sourced and publicly available from ~\cite{pham2021learning,krishna2017visual}.
Example statistics are shown in Table~\ref{tab:statsitic}. 
The rich semantics in these datasets enhance both diversity and coherence in the completed semantic space.

% Table~\ref{tab} presents example statistics of objects, attributes, and relationships within these datasets. 
The overall prompt generation process, as shown in Figure~\ref{fig:inference}, incorporates user input, semantic space, and retrieved attributes and relationships to guide generation. 
We employ few-shot learning~\cite{wang2023large} to enhance the quality and robustness of the generated prompts.


\begin{figure*}[t]
    \centering
    \includegraphics[width=0.995\textwidth]{src/img/ablation_zoom.pdf}
    \vspace{-2mm}
    \caption{Ablation study shows that prompt recommendation avoids common issues like missing objects and unrealistic relationships while sketch refinement further enhances fine-grained control.}
    \Description{Ablation study shows that prompt recommendation avoids common issues like missing objects and unrealistic relationships while sketch refinement further enhances fine-grained control.}
    \label{fig:ablation}
    \vspace{-4mm}
\end{figure*}

\subsection{Spatial-Condition Sketch Refinement}
\label{ssec:sketch_refine}
To help users refine their rough sketches into fine-grained shapes that align with their intentions and allow for iterative refinement, we propose a decompose-and-recompose approach. 
As Figure~\ref{fig:decompose} shows, the sketch is first decomposed into individual objects.
The users can then generate, select, and adjust the desired single-object images. 
Finally, these selected objects, along with their spatial conditions, are combined to generate the final result.

\subsubsection{Single Object Decomposition}
Instead of directly using a single object sketch for generation, we first classify objects into two categories: \textbf{thing} as foreground object with specific shapes, such as humans, animals, or chairs, and \textbf{stuff} as background object without a defined shape, like oceans, grass, or sky, based on ~\cite{caesar2018coco}. 
% \new{"Things" are objects with specific shapes, such as humans, animals, or chairs, while "stuff" refers to objects without a defined shape, like oceans, grass, or the sky.}
To classify an object, we compute the word embedding of its type and find the nearest match in a category list containing "things" and "stuff" in the object list in the COCO-Stuff dataset~\cite{caesar2018coco}.
During single object decomposition stage, each thing object is extracted using FAST SAM (Segment Anything)~\cite{zhao2023fast}, to make sure the single object generation maximally preserves consistency to the original sketch.
Once the single object sketch is decomposed, the generation of each individual object is carried out. 
Since the goal is to generate fine-grained object shapes but not the final image, the process can be accelerated by using low-step inference (6 steps) with the Lightning Diffusion model~\cite{luo2023lcm} and a lower resolution (512x512). 
In our experiment, generating 12 images took approximately 4 seconds on a GTX 4090.


To ensure that the generated result aligns more closely with the user's sketch, we filter the generated images based on the Intersection over Union (IoU) and CLIP score~\cite{hessel2021clipscore}, which measure spatial correspondence and semantic alignment, respectively, between the user sketch and the generated single object. 
We compute a weighted sum of the IoU and CLIP scores, then sort the images and select the top four for the user to choose from.
Once the user selects an image containing the desired object shape, the target object is automatically extracted using FAST SAM~\cite{zhao2023fast}.
Each object with refined shape will automatically replace the original rough sketch.
If users have no desired image in one generation, they can perform multi-round generation until finding the desired one.
Figure~\ref{fig:decompose} shows an example in which the rough sketch of a girl and a dog is refined to specific shapes.


\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\textwidth]{src/img/interface.pdf}
    \vspace{-3mm}
    \caption{\tool interface consists of (a) Canvas view, (b) Prompt Recommend view, (c) Sketch Refine view and (d) Result view.}
    \Description{SketchFlex interface consists of (a) Canvas view, (b) Prompt Recommend view, (c) Sketch Refine view and (d) Result view.}
    \label{fig:interface}
    \vspace{-3mm}
\end{figure*}

\subsubsection{Single Object Adjustment}
Our system provides flexible control not only over the shapes of objects but also over their size and position. As shown in Figure~\ref{fig:decompose}
, users can easily adjust the size and spatial placement of each object. 
Once adjustments are made, the corresponding single object shape mask is moved accordingly. 
All individual object shapes are then combined into an "anchor" image.
% The term "anchor" represents the idea that the generated result, like a boat floating within the rough sketch regions, may vary with each generation, just as a boat drifts on water. 
% However, with the anchor in place, the boat remains fixed to a specific location—similar to how the selected object shapes remain fixed in the generated result. 
Shape anchoring refers to the process of fixing the object shapes in the generated result.
To apply this shape anchoring, we extract the edges of the selected object using Canny edge detection and feed them into ControlNet to ensure that the final output adheres to the user's shape preferences. 
Once users have made their desired adjustments, they can generate an image with the exact fine-grained shapes they prefer.


A related issue with the original rough sketch-based generation is that a single prompt is applied to each region separately. 
To generate images that capture relationships between objects, we create a joint mask, \textit{i.e.}, the union of object masks, for two related objects, allowing the relationship prompt to influence the interaction between them. 
Equation 1 illustrates the creation of a joint mask for the relationship between region $i$ and region $j$, 
where $\oplus$ denotes the concatenation operation.

\begin{equation}
M_{ij} = [M_i \oplus M_j].
\end{equation}

However, using a joint mask alone can sometimes result in multiple objects being generated within a single object area. 
To address this, we apply negative prompts to exclude objects outside the intended region, preventing unwanted elements from appearing in the wrong areas, as shown in Equations 2-3.
In Equation 2, the cross-attention map that correlates text and image patches is updated such that the text embedding $C_i$ is amplified by a scalar $\lambda_{m_i}$ within the masked region $M_i$.
In Equation 3, the relationship embedding condition is reinforced within $M_{ij}$, while the influence of $C_i / C_j$ is reduced in the complementary areas.

\begin{equation}
A_i \leftarrow \lambda_{m_i} \cdot C_i \odot M_i,
\end{equation}

\begin{equation}
A_{(i,j)} \leftarrow \lambda_{m_{ij}} \cdot C_{ij} \odot M_{ij} - \lambda_{m_{ij}} \cdot C_i \odot (M_{ij}-M_i) - \lambda_{m_{ij}} \cdot C_j \odot (M_{ij} - M_j).
\end{equation}



Figure~\ref{fig:ablation} shows the ablation results of our system’s functions.
Without prompt recommendation and sketch refinement, the rough sketch-based generation often produces undesirable outcomes, such as missing objects, unrealistic relationships, and incorrect perspectives.
By incorporating our prompt recommendation, which refines prompts to be spatially aligned with the sketch, the results become more stable and coherent, with the generated objects closely matching the original sketch.
Finally, with prompt recommendation and sketch refinement, users can achieve fine-grained control, enabling them to generate detailed results that align with their sketches and creative intentions. For example, in the first column, the posture of the man lying on the chair with both arms spread out is more accurately captured in our result compared to the other two. 
In the third column, the girl's head is positioned slightly to the right and below the car within the mask, which aligns with our result. In contrast, in the other two results, the girl's head overlaps with the car, resulting in an incorrect spatial relationship.









