\section{Related work}\label{sec:related_wotk}

\subsection{Generative Model Assisted Creative Painting}
Recent years have witnessed the rapid development of text-to-image (T2I) generative models, exemplified by Midjourney~\cite{MID}, Stable Diffusion~\cite{rombach2022high}, DALL-E~\cite{ramesh2022hierarchical} and Imagen~\cite{saharia2022photorealistic}.
These models have revolutionized creative painting, offering rich inspiration and automation~\cite{vimpari2023adapt,ko2023large,chiou2023designing}, and enabling the public to create their own visual art without needing professional painting skills~\cite{davis2015enactive, hutson2023generative, shi2023understanding}.
The impressive capabilities of T2I models have raised concerns within the art and design communities.
% ~\cite{huang2024plantography,lawton2023tool,jiang2023ai,li2024realtimegen}.
A key concern is that the complete control exerted by generative models may diminish human contribution~\cite{jiang2023ai,boucher2024resistance}.
Existing T2I models are predominantly designed for end-to-end generation, bypassing the iterative "creation process" that enables creators to actively explore and refine their work~\cite{jiang2023ai,lawton2023tool}.
In contrast, traditional visual art creation with tactile and spatial control allows users to intuitively shape and manipulate creative projects~\cite{ko2023large,aharoni2017pigment, huang2024plantography, li2024realtimegen}.
To address these, researchers have been working towards human-AI co-creative painting, where AI acts not just as an autonomous creator but as a tool that refines or enhances the user-initiated content.
Sketch-to-image~\cite{zhang2023adding,mou2024t2i,koley2023picture} and image-to-image~\cite{rombach2022high, kodaira2023streamdiffusion} models can take user input and leverage latent consistency models~\cite{luo2023lcm,luo2023latent,lee2024streammultidiffusion} to refine and accelerate image generation with image conditioning.
In particular, some of these AI-assisted painting techniques have been integrated into industry standard tools such as Adobe Firefly~\cite{adobe} and Krita~\cite{krita}.

% lead to the key point that different type of user have different input level. rough input should be supported for novice/average user while also for expert who only have vague idea or time.
However, these condition control models require high-quality fine-grained user inputs, such as precise line drawing~\cite{zhang2023adding} or color blocking~\cite{rombach2022high}, to achieve the desired output.
When provided with rough input, the results can be unpredictable and unsatisfactory without accommodating a coarse-to-fine refinement process.
% Studies indicate significant differences in tool usage scenarios among different user groups~\cite{shi2023understanding}. 
% This is particularly relevant in Human-AI co-creative painting, where the user's skill level greatly impacts the quality of AI-generated images.
Experts who sketch well can fully leverage these models, whilst those with less painting ability often struggle to produce sketches that lead to the desired results.
% However, most previous works have focused on interaction through precise line drawing~\cite{zhang2023adding} or color blocking~\cite{rombach2022high}, making these models less user-friendly for individuals without professional training.
Our work targets novice users who may lack advanced drawing skills, but still wish to use AI-assisted tools to create their desired images.
These novice users prefer to use rough sketches to express their ideas, which differs significantly from the approach taken by experts~\cite{shi2023understanding}.
% Building on previous interaction processes, our system combines rough region sketch input with precise, fine-grained control over the output, offering an course-to-fine process. 
% The rough sketch is processed and enhanced through prompt tuning tailored to spatial conditions, while the unstable generation results from rough sketches are further refined through the system's canny-based fine-grained control.
To meet these requirements, we propose sketch-aware prompt recommendation, alongside a decompose-and-recompose strategy to refine rough sketches into precise controls.
Our approach provides an iterative coarse-to-fine process, mimicking the natural creative workflow.


\subsection{Prompt Tuning in Text-to-Image Generation}
% opening
The input text description, often termed a "prompt," plays a pivotal role in shaping the quality of images produced by T2I models. 
Therefore, designing and refining prompts is crucial for steering the model towards generating the desired images, referred to as "prompt tuning".
%prompt guidelines from existing research and community
Researchers have proposed guidelines for prompt design based on experimental studies and insights from online communities~\cite{wang2022diffusiondb}. 
These guidelines highlight key elements that contribute to generating high-quality images, including various types of prompt modifiers (\eg, subject, style, quality boosters)~\cite{oppenlaender2023prompting,oppenlaender2023taxonomy,liu2022design}. 
Auto-prompting techniques have also been proposed, using gradient descent to optimize or modify generated images by identifying enhanced or negative prompts~\cite{wang2024discrete, pryzant2023automatic, guo2023connecting}.
These methods are often paired with prompt datasets for better performance and guidance~\cite{cao2023beautifulprompt,hao2024optimizing}.
% interactive prompt refinement: prompt charm, promptmagician, propmt this,Ranni
In addition, interactive systems have been developed, allowing users to interactively align the prompts with their intentions~\cite{guo2024prompthis,brade2023promptify,wang2024promptcharm,feng2023promptmagician,promptpaint,zeng_2024_intent}. 
Some systems typically allow users to input an initial prompt, which is then refined through recommendations based on prompt datasets~\cite{wang2022diffusiondb}, followed by visualizations that enable users to perceive and adjust the prompt.
% For instance, 
% PromptCharm~\cite{wang2024promptcharm} refines the user’s prompt using promptist~\cite{hao2024optimizing} and allows further adjustments based on attention visualizations of tokens (i.e., input words). 
% PromptMagician~\cite{feng2023promptmagician} retrieves images and recommends prompts from a prompt database based on users’ input, with visualizations to evaluate these retrieved keywords. 
% Additionally, PrompTHis~\cite{guo2024prompthis} uses an innovative graph structure to visualize prompt branches, with nodes representing images and edges indicating differences in prompts.

% However, these interactive prompting systems focus solely on refining text, which is not aligned with users' input in the form of rough sketches.
Most of these interactive prompting systems primarily focus on refining text prompts. 
Recently, some efforts have been made to support direct image manipulation, such as inpainting~\cite{wang2024promptcharm,promptpaint}.
However, these works primarily optimize the visual appearance of specific regions, often neglecting the coherence across multiple regions.
% As a result, these methods are ineffective when the intended images contain multiple objects, as they fail to infer the relationships between the objects.
While LLMs have been employed to refine prompts with specific requirements such as format, style, and spatial arrangement~\cite{omost,yang2024mastering}, our experiments reveal that these approaches lack the ability to infer users' spatial control intents accurately.
To address this challenge, we integrate crowd-sourced data with fine-grained object attributes and relationships, to enhance the reasoning process, ensuring that the results are coherent and reflective of real-world scenarios.
Our system allows for direct sketch modifications, and the prompt refinement is updated automatically, enhancing the overall user experience.

% and does not allow users to interactively specify multi-modal prompts for spatial-semantic control.

% Others have developed datasets containing simple raw prompts paired with detailed, high-quality prompts, which are then used to train models that can convert initial prompts into more refined ones through fine-tuning and reinforcement learning~\cite{cao2023beautifulprompt,hao2024optimizing}.
% Leveraging the capabilities of large language models (LLMs) or Multi-modal large language models (MLLMs), some researchers prompt these language models to generate or refine prompts with specific requirements such as format,style and spatial arrangement~\cite{omost,yang2024mastering}.
% \yl{However, most automatic methods focus on text modality and do not consider users' spatial control intents.}



% Our work builds on the strengths of previous research \yl{while extending to the scenario of multi-modal prompting}. 
% We introduce a prompt template to guide prompt tuning, specifically tailored for spatially conditioned image generation.
% While we leverage MLLMs for automatic prompting, we go beyond text by incorporating user spatial input and spatial reasoning. 
% We also integrate crowd-sourced data to enhance the reasoning process, ensuring that results are coherent and reflective of real-world scenarios. 
% Unlike prior approaches that limit adjustments to prompt modifications, our system allows direct sketch modifications to influence prompt refinement, providing users with a more intuitive way to enhance their results.


\subsection{Spatial Control in Text-to-Image Generation}
%opening
% Though T2I models can generate visually appealing images from text descriptions, 
Relying solely on text-based conditioning for T2I generation models often falls short of meeting the diverse and complex needs of real-world applications, particularly when it comes to expressing precise spatial information~\cite{zhang2023adding}.
Recent reseach on spatial control of pre-trained T2I models introduces novel spatial conditional controls using canny~\cite{canny1986computational}, depth maps~\cite{lasinger2019towards}, and segmentation maps~\cite{zhou2017scene}.
% These methods involve training lightweight adapters to augment the original models. 
% While these fine-grained spatial conditions allow for precise control over the generated images, they also 
These methods, however, present challenges for users who may not have expertise in computer science or art.
For instance, preparing a canny-like sketch requires either preprocessing a reference image or possessing painting skills, 
% Creating a depth map also involves preprocessing or crafting a 3D model. 
% These tasks demand time for users to acquire the necessary skills.
which are time-consuming and require steep learning curve for novice users.
% region-based like
Alternatively, region-based spatial control allows users to specify prompts in different regions to guide the generation, which demands less in terms of spatial input from the user.
Current region-based methods can be categorized into two groups: training-based and training-free. Training-based methods~\cite{wang2024instancediffusion,li2023gligen,zheng2023layoutdiffusion} offer stronger control compared to training-free methods, but require retraining for new models that often cause style shifts due to updates in the model weights~\cite{omost}.
Training-free methods~\cite{endo2023masked,kim2023dense,bar2023multidiffusion,chen2024training,xie2023boxdiff} bypass the need for model retraining by utilizing T2I models internal mechanisms to constrain prompts to specific areas of the image. 
% These methods can be directly applied to most Stable Diffusion models with region control. 

Despite its advantages, region-based generation struggles to produce precise and consistent content within specified regions and is highly dependent on the quality of the prompt.
We leverage sketch-aware prompt recommendation with crowd-sourced datasets to generate high-quality prompts that are consistent with real-world scenarios.
In addition, we propose a decompose-and-recompose strategy that combines the advantages of high-quality spatial control and the flexibility of region-based generation.
The strategy entails decomposing a rough region sketch with multiple objects into individual objects, refining detailed spatial conditions for each object, and all spatial conditions to produce the final output.
This process not only ensures that the final output aligns with the user's intent but also maintains ease of use.




% ours
% In summary, while fine-grained control methods like ControlNet~\cite{zhang2023adding} and T2I-Adapter~\cite{mou2024t2i} offer precise spatial constraints for image generation, they pose challenges for non-expert users. 
% Conversely, region-based controls are more user-friendly but can produce unstable outputs due to variations in prompt quality and user input.
% Spatial control is crucial for accurately representing detailed scenarios and specific layouts in generated images, which is often required in real-world applications. 
% However, for novice users, mastering the necessary skills for precise spatial input can be daunting and time-consuming.
% Our system bridges these approaches by leveraging region-based inputs while stabilizes rough region using ControlNet's canny conditioning, ensuring the final output aligns with user intent while maintaining ease of use.
