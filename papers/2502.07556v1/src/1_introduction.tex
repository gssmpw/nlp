\section{Introduction}\label{sec:introduction}
% text2image shows success in different areas
The rapid development of text-to-image (T2I) generative models, such as Midjourney~\cite{MID}, Stable Diffusion~\cite{rombach2022high}, and DALL-E~\cite{ramesh2022hierarchical}, has demonstrated impressive capabilities in generating high-quality, visually appealing images from text input.
These T2I models have been successfully utilized in various fields, including industrial design~\cite{liu20233dall}, visualization design~\cite{xiao2023let}, and visual art creation~\cite{ko2023large}.

% challenge for region control
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.995\textwidth]{src/img/failure.pdf}
    \vspace{-1mm}
    \caption{Failure cases of existing methods for rough sketch based image generation: a) missing object for the green sketch, b) wrong perspective of the man sitting on the bench, c) unnatural relationship as the man is not holding the umbrella, and d) unrealistic scenario for water in the carriage. (a)-(c) are generated by Dense Diffusion~\cite{kim2023dense}, (d) is generated by MultiDiffusion~\cite{bar2023multidiffusion}.}
    \Description{Failure cases of existing methods for rough sketch based image generation: a) missing object for the green sketch, b) wrong perspective of the man sitting on the bench, c) unnatural relationship as the man is not holding the umbrella, and d) unrealistic scenario for water in the carriage. (a)-(c) are generated by Dense Diffusion~\cite{kim2023dense}, while (d) is generated by MultiDiffusion~\cite{bar2023multidiffusion}.}
    \label{fig:problem_identify}
    \vspace{0mm}
\end{figure*}

To better control the outcomes of the T2I models, prompt-tuning methods have been proposed to help craft more effective prompts to generate the desired images~\cite{guo2024prompthis,wang2024promptcharm,feng2023promptmagician}. 
However, these models still confine users to abstract and sometimes ambiguous text modality interaction, failing to provide detailed spatial control of image composition~\cite{zhang2023adding}.
In response, researchers have developed various spatial conditioning techniques to allow for more refined control beyond prompt-based methods, including line sketches and color blocks, as well as depth maps, segmentation maps, and pose skeletons commonly used in computer vision~\cite{zhang2023adding,mou2024t2i,koley2023picture, kodaira2023streamdiffusion,lee2024streammultidiffusion}. 
These fine-grained spatial controls enable users to more easily translate their creative intent into images, contributing to the widespread adoption of these tools within the generative AI (GenAI) community.


% fine-grained control benefits, but not friendly to novice
There are differences in the way various user groups use AI tools~\cite{shi2023understanding}.
Although experienced users find the existing fine-grained spatial control models highly beneficial, these models present challenges for novice users.
In particular, novice users often encounter difficulties in attempting to create fine-grained spatial controls such as line sketches, depth maps, and semantic segmentation maps.
This challenge hinders novice users from effectively utilizing advanced control techniques to express their creative intentions.
In contrast, the rough sketch control~\cite{kim2023dense,bar2023multidiffusion,wang2024instancediffusion} offers a more accessible way for novice users to interact with. 
These methods allows users to roughly sketch regions within an image and assign specific prompts to those regions, ensuring the generated image aligns with the user's sketch. 
Existing works, such as StreamMultiDiffusion~\cite{lee2024streammultidiffusion}, incorporate rough sketch control with latent consistency models~\cite{luo2023lcm} to enable real-time painting effects, making it easier for novice users to express their creative intent through spatial control.


However, while rough sketch control offers convenience, it also introduces several challenges.
First, rough sketch control still requires the appropriate use of text prompts to guide generative models in producing desired images.
Constructing prompts that closely align with the rough sketch presents a significant challenge (\textbf{C1}).
In particular, novice users often struggle to articulate the relationships between objects.
Second, rough sketches created by novice users are typically of low quality.
Relying solely on these rough sketches can result in semantically incoherent images, particularly for situations where multiple objects are involved~\cite{bar2023multidiffusion}.
Potential issues include missing objects, wrong perspective, unnatural object relationships, and unrealistic scenarios, as shown in Figure~\ref{fig:problem_identify}.
The process of refining rough sketches into high-quality spatial conditions is nontrivial (\textbf{C2}).
Moreover, current models operate primarily in an end-to-end generation approach, which can be challenging to generate fully satisfactory results in a single iteration and does not align with the iterative design process~\cite{li2024realtimegen,huang2024plantography}. 
It is not uncommon for one aspect of the image to meet the user's expectations while another falls short, making iterative refinement highly desirable.
However, the lack of an appropriately designed interactive process can hinder iterative refinement (\textbf{C3}).


% facing these chanllenge, for novice user to create their work with flexible control condition, we propose...
To address these challenges, we develop \tool, an interactive system designed to empower novice users to flexibly create semantically cohesive images from rough sketches and prompts, with key modules as follows: 
\begin{enumerate}
\item \emph{Sketch-aware prompt recommendation} (Sect.~\ref{ssec:prompt_rec}) for refining prompts aligned with the rough sketch (\textbf{C1}).
Specifically, the system employs a multimodal large language model (MLLM) to interpret user prompts and sketches, generating content within a semantic space designed for multi-region spatial conditioning to ensure completeness across all regions. The prompt generation is grounded in semantic knowledge drawn from fine-grained visual attributes, states, and relationships of common objects found in large-scale real-world datasets, such as Visual Genome~\cite{krishna2017visual} and VAW~\cite{pham2021learning}, ensuring the diversity and coherence of the refined prompt.

\item \emph{Spatial-condition sketch refinement} (Sect.~\ref{ssec:sketch_refine}) for refining rough sketches into high-quality and intention-aligned spatial conditions (\textbf{C2}).
Sketches created by novice users often exhibit unrealistic elements, such as uncoordinated shapes and inappropriate sizes.
This contrasts with the realistic masks typically used to guide T2I generative models, resulting in dissatisfactory outcomes, particularly for images with multiple objects.
To refine the sketch, we propose a sketch decompose-and-recompose approach:
First, the decompose stage iteratively focuses on each foreground object to generate reference images containing the object with a more realistic and accurate shape.
Second, in the recompose stage, we leverage Segment Anything Model~\cite{kirillov2023segment} to extract the refined mask for each object and recompose these masks together into one coherent mask.
The mask is further coupled with Canny edge~\cite{canny1986computational} conditioned ControlNet to stably generate images precisely matching user-selected shapes.

    
\end{enumerate}

We further build a prototype interface (Sect.~\ref{ssec:system}) that allows users to easily perform sketch control and prompt editing for interactive refinement (\textbf{C3}).
For sketch control, users can freely draw the initial sketch and adjust the recommended sketch produced by our decompose-and-recompose method and mask manipulation for each object.
For prompt editing, users can modify the detailed prompt for each semantic attribute.
We conduct a user study with 12 participants to compare our system with existing baselines (Sect.~\ref{sec:evaluation}) in terms of outcome satisfaction (Sect.~\ref{ssec:outcome}) and system evaluation, including system feature rating (Sect.~\ref{ssec:featurerating}), and system overall rating (Sect.~\ref{ssec:overallrating}). 
The results and user feedback reveal that \tool provides users with greater flexibility in expressing their creative intent and produces more precise results that closely align with their intentions. 
Finally, we discuss the applicability, model interpretability, and ethical considerations of \tool and similar approaches (Sect.~\ref{sec:discussion}).
The dataset and code are available at \url{https://github.com/SellLin/SketchFlex}.

Our contributions can be summarized as follows:
\begin{itemize}
    \item We introduce a novel framework that integrates sketch-aware prompt recommendation and spatial-condition sketch refinement for sketch-based image generation, to improve generation quality and ensure the alignment with user intentions, especially for novice users.
    
    \item We design a comprehensive semantic space that incorporates single-object visual attributes and multi-object relationships for generating images guided by sketches.
    The space facilitates prompt recommendation and sketch refinement, with the goal of improving spatial-semantic coherence in the generated images, especially when multiple objects are present.
    
    \item We develop a user interface that allows users to interactively refine text prompts and sketches. A user study has been conducted to evaluate the usability and effectiveness of the interactive system.
\end{itemize}

