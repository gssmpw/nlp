\section{PRELIMINARY STUDY}\label{sec:formative study}
To understand the practice of T2I generative models, we conducted in-depth interviews with eight users about their experience of using GenAI. 
We aim to identify the advantages and limitations of existing T2I generative models and identify design goals for potential improvement beyond existing methods. 

\subsection{Study Design}
\textbf{Participants}.
We recruited eight users of GenAI aged between 25 and 33 (4 females and 4 males), including four PhD students studying Digital Media or Computer Science (P1, P2, P7, P8), one UI/UX designer (P3) and three game concept designers (P4-P6).
All users have experience using popular generative models and tools like Stable Diffusion and Midjourney, with varying degree of expertise in GenAI, including four novice users with less than 6 months' usage (P1, P2, P4, P7), and four expert users with at least one-year experience and familiar with different models (P3, P5, P6, P8). 
In terms of painting expertise, three novices (P1, P7, P8) have no experience in painting, one beginner (P2) has limited knowledge about painting, and four experts (P3-P6) are proficient in painting.

\noindent
\textbf{Procedure}.
Each interview lasted 60–90 minutes. 
Initially, participants were asked to provide a self-introduction, focusing on their experience in GenAI usage, with questions like "\emph{have you used any text-to-image models like Midjourney or Stable Diffusion?}". 
Next, we explored various scenarios in which participants used or would like to use generative models, using both live demonstrations with Stable Diffusion or Midjourney 
% and explained prompt-based and spatial. 
together with explanations of prompt-based and spatial control.
Finally, we engaged in an open discussion about the advantages and limitations of the use cases presented, as well as the participants' expectations for future developments.

\subsection{Findings}

Two authors independently used open coding to analyze and code audio-recorded user feedback, generating initial codes to capture key insights from participants' experiences and expectations. 
The first author then worked with another co-author to refine and validate the themes through iterative discussions.
Based on this analysis, we distilled three primary user requirements, which are summarized as follows:

\subsubsection{Rough sketch based spatial control is preferable by novice users.}
Users with varying levels of experience exhibit different preferences in using spatial conditioning models. 
Experienced users in both GenAI and painting (P3, P6) have integrated these models into their workflow, significantly increasing their productivity. 
P6 shared his experience: "\emph{we create a line draft or use 3D modeling to generate a DepthMap, then use ControlNet for drawing}."
% He also mentioned that such workflow is widely used not only in his department, but also other companies.
Conversely, novice users in either GenAI or painting, despite being aware of spatial control features, often find these models difficult to use or insufficiently flexible to meet their expectations.
Some novice users in GenAI (e.g., P1, P4, P7) explained that these methods involve a steep learning curve, with program settings being overly complex. 
For instance, P7 noted, "\emph{I have tried using Canny, for example, but it doesn't understand my sketch very well. You need to adjust some settings, such as the guidance coefficient.}" 
Similarly, novice users in painting described a reliance on pre-existing spatial conditions found online, which limited their ability to fully realize their creative intentions. 
"\emph{I usually search images on the internet that are similar to what I want, like a person with a specific posture. Then I use the searched image to apply ControlNet. 
But in most cases, the searched image does not fully meet my expectations. 
Sometimes I can't even find an existing image that is close to my idea}," explained P8.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.99\textwidth]{src/img/overall_framework.pdf}
    \vspace{-2mm}
    \caption{\tool mainly consists of three components: (1) sketch-aware prompt recommendation that support users in crafting effective prompts for the rough sketch; (2) object shape refinement through single object decomposition and generation; and (3) spatial adjustment and anchoring of object shapes.}
    \Description{\tool mainly consists of three components: (1) sketch-aware prompt recommendation that support users in crafting effective prompts for the rough sketch; (2) object shape refinement through single object decomposition and generation; and (3) spatial adjustment and anchoring of object shapes.}
    \label{fig:workflow}
    \vspace{-4mm}
\end{figure*}

Novice painters expressed strong interest in spatial control through rough sketches.
Participants with a professional painting background preferred to use line sketches and color blocks, as these techniques closely aligned with their established drawing workflows and design practices.
In contrast, novice painters prefer scribbles or regions, as these methods offer greater flexibility and require less precision when representing image elements.
P7 envisioned an interaction approach suited to their needs: "\emph{If I just draw a stick man, it knows it is a person. Then if I draw a few lines, it knows that it is an ocean. And a simple circle would represent the sun}."

\subsubsection{Combining prompt tuning and spatial control is challenging.}
Some participants familiar with spatial conditioning models reported that prompt tuning becomes more difficult given extra spatial conditions. 
This challenge arises from a potential misalignment between spatial conditions and prompts.
Such misalignment mostly occur when the intended image contains multiple objects. 
P5 found that the prompt must match the spatial condition between objects, and had to curate proper words for such prompt each time. 
P6 shared a similar point, "\emph{The count is important—if there are three people in your conditioned image but your prompt does not specify three people, the model might generate the wrong number. 
Relationships are also crucial; If you don’t specify connections, the generated result can seem disjointed, which is especially prominent in multi-diffusion}." 
The MultiDiffusion model~\cite{bar2023multidiffusion} is one of control methods that P6 frequently uses for region control.
Regarding the prompting difficulty, both P5 and P6 expressed a desire for a tool that could recommend prompts based on spatial conditions to reduce their time and cognitive load.
Despite these challenges were primarily raised by experienced participants, as novice users rarely engage with these techniques, it is clear that prompt tuning would be even more difficult for novice users.


\subsubsection{Iterative generation and refinement is difficult in end-to-end generation.}
All participants mentioned that refining end-to-end generated results is challenging. 
Specifically, they often want to change a part of the generated image while keeping other elements unchanged, as it is hard to generate image that all parts fulfill their requirement. 
However, even minor adjustments to the prompt or spatial condition can lead to significant changes in the entire generated image. 
P3 noted, "\emph{In a project, I used a reference image and made a slight change to the prompt, like adding `wearing glasses' to the description. The result looked similar, but it was just...different}."
P8 also shared, "\emph{I use AI-generated images to illustrate the scene layout for my project, including environment settings, object positioning, and content alignment. In most cases, it’s sufficient if the generated image fulfills two of these three requirements. However, in my experience, achieving all three requirements without iterating on specific areas is impossible}."
Some experienced GenAI users employ techniques like in-painting or use Photoshop for manual regional editing, but such methods require additional efforts. 
P3 explained, "\emph{I use in-painting and Photoshop to edit the parts I want to change. First, I break down the elements in the image using Photoshop, then in-paint or manually re-draw some of them. It’s time-consuming, and the in-painting doesn’t have the same coherent effect as generating the image from scratch}."
These feedback suggest that users require an iterative generation process that can progressively adjust specific parts to reach a satisfactory outcome.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Design Goals}
The formative study illustrates the diverse methods available for spatially controlling T2I models and the needs for users in using these methods. 
However, the study also highlights that novice users often find it challenging to fine-tune prompts and prepare spatial conditions to match their intended outcomes with AI-generated images.
Our goal is to empower users with limited expertise in art and computer science to more freely create with GenAI. %using AI's generative and control capabilities. 
To achieve this, the design of a new interactive tool supporting this application should incorporate user-friendly interactions for more flexible, less demanding input and iteration while delivering high-quality results aligned with user intentions.

Based on our findings, we propose the following design goals for \tool to enhance flexible spatial control in image generation, specifically tailored for novice users.
\begin{itemize}
    \item \textbf{G1: Providing Flexible Spatial Control}.
    \tool aims to empower novice users to easily and intuitively achieve spatial control without requiring expertise in computer science or painting. 
    This control should allow users to input rough spatial conditions and provide tools for refining and enhancing the final generated result. Based on the study, we will implement region control as the rough sketch input preferred by novice users.
    \item \textbf{G2: Automating Prompt Tuning for Spatial Conditioned T2I Generation}.
    \tool should include an automated prompt tuning feature that considers both spatial conditions and the initial text prompt. 
     This can reduce the cognitive load of users while ensuring the generation of high-quality, cohesive images that accurately reflect the user's intent.
    \item \textbf{G3: Enhancing Flexibility in Image Adjustment by Decomposed Generation}.
    \tool should implement a decomposed generation approach, allowing users to make adjustments at the level of individual objects or elements within the image to facilitate iterative generation. 
    This will help avoid unintended changes to the overall image when making local adjustments in different generation rounds, thus iteratively improving user control and satisfaction.
\end{itemize}

