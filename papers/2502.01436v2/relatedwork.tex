\section{Related Work}
\label{Related Work}
%-------------------------------------------------------------------------------

The versatility of tasks and responses enabled by LLMs comes with inherent challenges in controlling their outputs and ensuring alignment with predefined guidelines. Prior research has highlighted several risks stemming from these limitations, including the generation of malware and phishing messages~\cite{kucharavy2024llms}, the dissemination of misinformation and fake news~\cite{Weidinger2022, Zellers2019}, and the creation of malicious bots~\cite{kucharavy2024llms, Derner2024}. Additionally, LLMs face technical shortcomings such as hallucinations—wherein they produce incorrect or nonsensical information~\cite{gao2024}—and code injection vulnerabilities, which adversaries can exploit to manipulate LLMs, leading to unintended and potentially harmful outputs~\cite{gao2024}.

These challenges are rooted in the inherent complexity of LLMs. Their black-box nature and intricate internal mechanisms make it difficult to predict and control their behavior under diverse user inputs, exacerbating the challenges of safeguarding them~\cite{sarker2024llm, ullah2024challenges}. Moreover, the widespread adoption of LLMs among non-specialized users amplifies the need for strong safety considerations. Striking a balance between training LLMs to be helpful and ensuring their safety remains an ongoing challenge. Excessive focus on helpfulness can lead to harmful content generation, whereas overly restrictive safety tuning risks rejecting legitimate prompts and negatively affecting utility~\cite{bianchi2024safety, chehbouni2024}. OpenAI has invested significant resources into addressing these challenges~\cite{openai_safety}, employing security policies, conducting both manual and automated evaluations, and implementing dedicated red-teaming efforts to mitigate risks~\cite{openai2023redteaming}.

To further advance safety evaluations, open datasets have been developed to systematically assess LLMs, as documented in a recent systematic review~\cite{rottger2025}. This review underscores gaps in the ecosystem, including the predominance of English-centric datasets and the limitations of existing benchmarks in comprehensively assessing safety dimensions. Building on this, researchers have proposed various frameworks and benchmarks for safety evaluations. For instance, Xie et al.~\cite{xie2024onlinesafety} developed a public dataset for evaluating LLMs using black-box, white-box, and gray-box approaches. Another benchmark expands the scope of safety evaluation to 45 distinct categories, analyzing 40 models and identifying Claude-2 and Gemini-1.5 as the most robust with respect to safety~\cite{xie2024sorrybench}. Additional benchmarks offer larger prompt sets for evaluation~\cite{zhang2024safetybench}, as well as support for multilingual assessments in English and Chinese~\cite{yuan2024, sun2023safety}.

The LLM-as-a-judge technique has also emerged as a widely adopted method for evaluating LLM outputs~\cite{huang2024empirical, li2025generation, gu2025surveyllmasajudge}. This approach, which involves using LLMs to assess the quality or performance of other systems, has demonstrated high agreement rates with human evaluations, further validating its efficacy~\cite{Zheng2023}.

The customization of LLMs introduces another dimension of complexity to their safety assessment. Hung et al.~\cite{huang2024lisa} proposed a method demonstrating that fine-tuning can enhance safety by reducing harmful outputs while maintaining task accuracy. Conversely, other studies have shown that fine-tuning can inadvertently degrade safety alignment, even when benign datasets are used, rendering models more vulnerable to harmful or adversarial instructions~\cite{qi2023finetuning}. Additionally, fine-tuning has been linked to increased susceptibility to jailbreaking attacks~\cite{kumar2024finetuning}, and when applied to develop agent applications, it can lead to unintended safety lapses if failed interaction data is not properly utilized~\cite{wang2024learningfailure}. Achieving a balance between safety and utility is essential, as an excessive emphasis on safety during fine-tuning may cause models to overly reject valid prompts, negatively impacting their usability~\cite{hsu2025safe}.

OpenAI’s Custom GPTs represent an advancement in enabling end-users to personalize LLMs. Prior studies have noted that some features of these systems may introduce significant security risks~\cite{antebi2024gptsheeps}, including security and privacy concerns within the GPT store~\cite{tao2023openingpandoras}. Zhang et al.~\cite{zhang2024lookgptappslandscape} conducted the first longitudinal study of the platform, analyzing metadata from 10,000 Custom GPTs and identifying a growing interest in these systems. Similarly, Su et al.~\cite{su2024gptstoremininganalysis} examined the GPT store, analyzing user preferences, algorithmic influences, and market dynamics. Their study identified Custom GPTs that contradicted OpenAI’s usage policies, raising concerns about the effectiveness of the platform’s review mechanisms.

The work by Yu et al.~\cite{yu2024assessingpromptinjection} is most closely related to our study. In their research, the authors evaluated over 200 Custom GPTs using adversarial prompts, demonstrating the susceptibility of these systems to prompt injection attacks. Their methodology included generating dynamic red-teaming prompts tailored to the characteristics of the targeted Custom GPTs. Similar approaches, focusing on general LLM evaluation, were proposed by Liu et al.~\cite{Liu2024} and Shen et al.~\cite{Shen2024DoAnythingNow}, who explored the types of adversarial prompts most effective in bypassing safety measures. In line with this, Yu et al.~\cite{Yu2024DontListen} conducted a systematic study on jailbreak prompts, revealing the ease with which users can craft prompts that circumvent LLM safeguards. Yu et al.~\cite{yu2024gptfuzzerredteaminglarge} also introduced GPTFuzzer, a black-box fuzzing framework based on automated generation of jailbreak prompts, achieving over 90\% attack success rates against foundational models like ChatGPT and LLaMa-2.

In contrast, our study focuses on evaluating the alignment of Custom GPTs with OpenAI’s usage policies, leveraging an automated framework that spans the entire evaluation pipeline—from identifying Custom GPTs to producing compliance assessments. To the best of our knowledge, this work represents the first attempt to automate large-scale policy evaluations of Custom GPTs on the GPT store.

%-------------------------------------------------------------------------------