[
  {
    "index": 0,
    "papers": [
      {
        "key": "kucharavy2024llms",
        "author": "Andrei Kucharavy and Octave Plancherel and Valentin Mulder and Alain Mermoud and Vincent Lenders",
        "title": "Large Language Models in Cybersecurity: Threats, Exposure and Mitigation"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "Weidinger2022",
        "author": "Weidinger, Laura and Uesato, Jonathan and Rauh, Maribeth and Griffin, Conor and Huang, Po-Sen and Mellor, John and Glaese, Amelia and Cheng, Myra and Balle, Borja and Kasirzadeh, Atoosa and Biles, Courtney and Brown, Sasha and Kenton, Zac and Hawkins, Will and Stepleton, Tom and Birhane, Abeba and Hendricks, Lisa Anne and Rimell, Laura and Isaac, William and Haas, Julia and Legassick, Sean and Irving, Geoffrey and Gabriel, Iason",
        "title": "Taxonomy of Risks posed by Language Models"
      },
      {
        "key": "Zellers2019",
        "author": "Rowan Zellers and Ari Holtzman and Hannah Rashkin and Yonatan Bisk and Ali Farhadi and Franziska Roesner and Yejin Choi",
        "title": "Defending Against Neural Fake News"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "kucharavy2024llms",
        "author": "Andrei Kucharavy and Octave Plancherel and Valentin Mulder and Alain Mermoud and Vincent Lenders",
        "title": "Large Language Models in Cybersecurity: Threats, Exposure and Mitigation"
      },
      {
        "key": "Derner2024",
        "author": "Derner, Erik and Batisti\u010d, Kristina and Zah\u00e1lka, Jan and Babu\u0161ka, Robert",
        "title": "A Security Risk Taxonomy for Prompt-Based Interaction With Large Language Models"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "gao2024",
        "author": "Gao, Zhengjie and Liu, Xuanzi and Lan, Yuanshuai and Yang, Zheng",
        "title": "A brief survey on safety of large language models"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "gao2024",
        "author": "Gao, Zhengjie and Liu, Xuanzi and Lan, Yuanshuai and Yang, Zheng",
        "title": "A brief survey on safety of large language models"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "sarker2024llm",
        "author": "Sarker, Iqbal H.",
        "title": "LLM Potentiality and Awareness: A Position Paper from the Perspective of Trustworthy and Responsible AI Modeling"
      },
      {
        "key": "ullah2024challenges",
        "author": "Ullah, Ehsan and Parwani, Anil and Baig, Mirza Mansoor and Singh, Rajendra",
        "title": "Challenges and Barriers of Using Large Language Models (LLM) Such as ChatGPT for Diagnostic Medicine with a Focus on Digital Pathology \u2013 A Recent Scoping Review"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "bianchi2024safety",
        "author": "Federico Bianchi and Mirac Suzgun and Giuseppe Attanasio and Paul R\u00f6ttger and Dan Jurafsky and Tatsunori Hashimoto and James Zou",
        "title": "Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions"
      },
      {
        "key": "chehbouni2024",
        "author": "Khaoula Chehbouni and Megha Roshan and Emmanuel Ma and Futian Andrew Wei and Afaf Taik and Jackie CK Cheung and Golnoosh Farnadi",
        "title": "From Representational Harms to Quality-of-Service Harms: A Case Study on Llama 2 Safety Safeguards"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "openai_safety",
        "author": "OpenAI",
        "title": "OpenAI Safety"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "openai2023redteaming",
        "author": "OpenAI",
        "title": "OpenAI Red Teaming Network"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "rottger2025",
        "author": "Paul R\u00f6ttger and Fabio Pernisi and Bertie Vidgen and Dirk Hovy",
        "title": "SafetyPrompts: a Systematic Review of Open Datasets for Evaluating and Improving Large Language Model Safety"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "xie2024onlinesafety",
        "author": "Xuan Xie and Jiayang Song and Zhehua Zhou and Yuheng Huang and Da Song and Lei Ma",
        "title": "Online Safety Analysis for LLMs: a Benchmark, an Assessment, and a Path Forward"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "xie2024sorrybench",
        "author": "Tinghao Xie and Xiangyu Qi and Yi Zeng and Yangsibo Huang and Udari Madhushani Sehwag and Kaixuan Huang and Luxi He and Boyi Wei and Dacheng Li and Ying Sheng and Ruoxi Jia and Bo Li and Kai Li and Danqi Chen and Peter Henderson and Prateek Mittal",
        "title": "SORRY-Bench: Systematically Evaluating Large Language Model Safety Refusal Behaviors"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "zhang2024safetybench",
        "author": "Zhexin Zhang and Leqi Lei and Lindong Wu and Rui Sun and Yongkang Huang and Chong Long and Xiao Liu and Xuanyu Lei and Jie Tang and Minlie Huang",
        "title": "SafetyBench: Evaluating the Safety of Large Language Models"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "yuan2024",
        "author": "Xiaohan Yuan and Jinfeng Li and Dongxia Wang and Yuefeng Chen and Xiaofeng Mao and Longtao Huang and Hui Xue and Wenhai Wang and Kui Ren and Jingyi Wang",
        "title": "S-Eval: Automatic and Adaptive Test Generation for Benchmarking Safety Evaluation of Large Language Models"
      },
      {
        "key": "sun2023safety",
        "author": "Hao Sun and Zhexin Zhang and Jiawen Deng and Jiale Cheng and Minlie Huang",
        "title": "Safety Assessment of Chinese Large Language Models"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "huang2024empirical",
        "author": "Hui Huang and Yingqi Qu and Xingyuan Bu and Hongli Zhou and Jing Liu and Muyun Yang and Bing Xu and Tiejun Zhao",
        "title": "An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Model is not a General Substitute for GPT-4"
      },
      {
        "key": "li2025generation",
        "author": "Dawei Li and Bohan Jiang and Liangjie Huang and Alimohammad Beigi and Chengshuai Zhao and Zhen Tan and Amrita Bhattacharjee and Yuxuan Jiang and Canyu Chen and Tianhao Wu and Kai Shu and Lu Cheng and Huan Liu",
        "title": "From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge"
      },
      {
        "key": "gu2025surveyllmasajudge",
        "author": "Jiawei Gu and Xuhui Jiang and Zhichao Shi and Hexiang Tan and Xuehao Zhai and Chengjin Xu and Wei Li and Yinghan Shen and Shengjie Ma and Honghao Liu and Yuanzhuo Wang and Jian Guo",
        "title": "A Survey on LLM-as-a-Judge"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "Zheng2023",
        "author": "Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric Xing and Hao Zhang and Joseph E. Gonzalez and Ion Stoica",
        "title": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "huang2024lisa",
        "author": "Tiansheng Huang and Sihao Hu and Fatih Ilhan and Selim Furkan Tekin and Ling Liu",
        "title": "Lisa: Lazy Safety Alignment for Large Language Models against Harmful Fine-tuning Attack"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "qi2023finetuning",
        "author": "Xiangyu Qi and Yi Zeng and Tinghao Xie and Pin-Yu Chen and Ruoxi Jia and Prateek Mittal and Peter Henderson",
        "title": "Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "kumar2024finetuning",
        "author": "Divyanshu Kumar and Anurakt Kumar and Sahil Agarwal and Prashanth Harshangi",
        "title": "Fine-Tuning, Quantization, and LLMs: Navigating Unintended Outcomes"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "wang2024learningfailure",
        "author": "Renxi Wang and Haonan Li and Xudong Han and Yixuan Zhang and Timothy Baldwin",
        "title": "Learning From Failure: Integrating Negative Examples when Fine-tuning Large Language Models as Agents"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "hsu2025safe",
        "author": "Chia-Yi Hsu and Yu-Lin Tsai and Chih-Hsun Lin and Pin-Yu Chen and Chia-Mu Yu and Chun-Ying Huang",
        "title": "Safe LoRA: the Silver Lining of Reducing Safety Risks when Fine-tuning Large Language Models"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "antebi2024gptsheeps",
        "author": "Sagiv Antebi and Noam Azulay and Edan Habler and Ben Ganon and Asaf Shabtai and Yuval Elovici",
        "title": "GPT in Sheep's Clothing: The Risk of Customized GPTs"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "tao2023openingpandoras",
        "author": "Guanhong Tao and Siyuan Cheng and Zhuo Zhang and Junmin Zhu and Guangyu Shen and Xiangyu Zhang",
        "title": "Opening A Pandora's Box: Things You Should Know in the Era of Custom GPTs"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "zhang2024lookgptappslandscape",
        "author": "Zejun Zhang and Li Zhang and Xin Yuan and Anlan Zhang and Mengwei Xu and Feng Qian",
        "title": "A First Look at GPT Apps: Landscape and Vulnerability"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "su2024gptstoremininganalysis",
        "author": "Dongxun Su and Yanjie Zhao and Xinyi Hou and Shenao Wang and Haoyu Wang",
        "title": "GPT Store Mining and Analysis"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "yu2024assessingpromptinjection",
        "author": "Jiahao Yu and Yuhang Wu and Dong Shu and Mingyu Jin and Sabrina Yang and Xinyu Xing",
        "title": "Assessing Prompt Injection Risks in 200+ Custom GPTs"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "Liu2024",
        "author": "Liu, Yi and Deng, Gelei and Xu, Zhengzi and Li, Yuekang and Zheng, Yaowen and Zhang, Ying and Zhao, Lida and Zhang, Tianwei and Wang, Kailong",
        "title": "A Hitchhiker\u2019s Guide to Jailbreaking ChatGPT via Prompt Engineering"
      }
    ]
  },
  {
    "index": 27,
    "papers": [
      {
        "key": "Shen2024DoAnythingNow",
        "author": "Shen, Xinyue and Chen, Zeyuan and Backes, Michael and Shen, Yun and Zhang, Yang",
        "title": "\"Do Anything Now\": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models"
      }
    ]
  },
  {
    "index": 28,
    "papers": [
      {
        "key": "Yu2024DontListen",
        "author": "Zhiyuan Yu and Xiaogeng Liu and Shunning Liang and Zach Cameron and Chaowei Xiao and Ning Zhang",
        "title": "Don{\\textquoteright}t Listen To Me: Understanding and Exploring Jailbreak Prompts of Large Language Models"
      }
    ]
  },
  {
    "index": 29,
    "papers": [
      {
        "key": "yu2024gptfuzzerredteaminglarge",
        "author": "Jiahao Yu and Xingwei Lin and Zheng Yu and Xinyu Xing",
        "title": "GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts"
      }
    ]
  }
]