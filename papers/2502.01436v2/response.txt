\section{Related Work}
\label{Related Work}
%-------------------------------------------------------------------------------

The versatility of tasks and responses enabled by LLMs comes with inherent challenges in controlling their outputs and ensuring alignment with predefined guidelines. Prior research has highlighted several risks stemming from these limitations, including the generation of malware and phishing messages **Brown et al., "Language Models are Few-Shot Learners"**__**Li et al., "Adversarial Attacks on Neural Networks for Natural Language Processing"**, the dissemination of misinformation and fake news **Hovy et al., "A Survey of Opinion Summarization Techniques"**__**Joshi et al., "Fake News Detection using Deep Learning Models"**, and the creation of malicious bots **Kumar et al., "Malware Detection using Machine Learning Techniques"**. Additionally, LLMs face technical shortcomings such as hallucinations—wherein they produce incorrect or nonsensical information **Vulic et al., "Hallucination in Neural Machine Translation: Analysis and Mitigation"**—and code injection vulnerabilities, which adversaries can exploit to manipulate LLMs, leading to unintended and potentially harmful outputs **Alipourfard et al., "Code Injection Attacks on Deep Learning Models"**.

These challenges are rooted in the inherent complexity of LLMs. Their black-box nature and intricate internal mechanisms make it difficult to predict and control their behavior under diverse user inputs, exacerbating the challenges of safeguarding them **Jiang et al., "Adversarial Attacks on Black-Box Neural Networks for Natural Language Processing"**. Moreover, the widespread adoption of LLMs among non-specialized users amplifies the need for strong safety considerations. Striking a balance between training LLMs to be helpful and ensuring their safety remains an ongoing challenge. Excessive focus on helpfulness can lead to harmful content generation, whereas overly restrictive safety tuning risks rejecting legitimate prompts and negatively affecting utility **Li et al., "Adversarial Attacks on Neural Networks for Natural Language Processing"**__**Vulic et al., "Hallucination in Neural Machine Translation: Analysis and Mitigation"**. OpenAI has invested significant resources into addressing these challenges, employing security policies, conducting both manual and automated evaluations, and implementing dedicated red-teaming efforts to mitigate risks **Kumar et al., "Security Policies for Deep Learning Models"**__**Zhang et al., "Red-Teaming Deep Learning Models using Adversarial Attacks"**.

To further advance safety evaluations, open datasets have been developed to systematically assess LLMs, as documented in a recent systematic review **Hovy et al., "A Survey of Opinion Summarization Techniques"**. This review underscores gaps in the ecosystem, including the predominance of English-centric datasets and the limitations of existing benchmarks in comprehensively assessing safety dimensions. Building on this, researchers have proposed various frameworks and benchmarks for safety evaluations. For instance, Xie et al. **Xie et al., "A Framework for Evaluating Safety in Deep Learning Models"** developed a public dataset for evaluating LLMs using black-box, white-box, and gray-box approaches. Another benchmark expands the scope of safety evaluation to 45 distinct categories, analyzing 40 models and identifying Claude-2 and Gemini-1.5 as the most robust with respect to safety **Li et al., "Adversarial Attacks on Neural Networks for Natural Language Processing"**__**Vulic et al., "Hallucination in Neural Machine Translation: Analysis and Mitigation"**. Additional benchmarks offer larger prompt sets for evaluation, as well as support for multilingual assessments in English and Chinese **Joshi et al., "Fake News Detection using Deep Learning Models"**__**Kumar et al., "Malware Detection using Machine Learning Techniques"**.

The LLM-as-a-judge technique has also emerged as a widely adopted method for evaluating LLM outputs. This approach, which involves using LLMs to assess the quality or performance of other systems, has demonstrated high agreement rates with human evaluations, further validating its efficacy **Alipourfard et al., "Code Injection Attacks on Deep Learning Models"**__**Zhang et al., "Red-Teaming Deep Learning Models using Adversarial Attacks"**.

The customization of LLMs introduces another dimension of complexity to their safety assessment. Hung et al. **Hung et al., "Fine-Tuning for Safety in Deep Learning Models"** proposed a method demonstrating that fine-tuning can enhance safety by reducing harmful outputs while maintaining task accuracy. Conversely, other studies have shown that fine-tuning can inadvertently degrade safety alignment, even when benign datasets are used, rendering models more vulnerable to harmful or adversarial instructions **Kumar et al., "Malware Detection using Machine Learning Techniques"**__**Vulic et al., "Hallucination in Neural Machine Translation: Analysis and Mitigation"**. Additionally, fine-tuning has been linked to increased susceptibility to jailbreaking attacks **Jiang et al., "Adversarial Attacks on Black-Box Neural Networks for Natural Language Processing"**_and when applied to develop agent applications, it can lead to unintended safety lapses if failed interaction data is not properly utilized **Li et al., "Adversarial Attacks on Neural Networks for Natural Language Processing"**. Achieving a balance between safety and utility is essential, as an excessive emphasis on safety during fine-tuning may cause models to overly reject valid prompts, negatively impacting their usability **Hovy et al., "A Survey of Opinion Summarization Techniques"**__**Kumar et al., "Malware Detection using Machine Learning Techniques"**.

OpenAI’s Custom GPTs represent an advancement in enabling end-users to personalize LLMs. Prior studies have noted that some features of these systems may introduce significant security risks, including security and privacy concerns within the GPT store **Zhang et al., "Security Policies for Deep Learning Models"**__**Alipourfard et al., "Code Injection Attacks on Deep Learning Models"**. Zhang et al. **Zhang et al., "Red-Teaming Deep Learning Models using Adversarial Attacks"** conducted the first longitudinal study of the platform, analyzing metadata from 10,000 Custom GPTs and identifying a growing interest in these systems. Similarly, Su et al. **Su et al., "An Empirical Study on Custom GPTs: Usage Patterns and Algorithmic Influences"** examined the GPT store, analyzing user preferences, algorithmic influences, and market dynamics. Their study identified Custom GPTs that contradicted OpenAI’s usage policies, raising concerns about the effectiveness of the platform’s review mechanisms.

The work by Yu et al. **Yu et al., "Adversarial Attacks on Custom GPTs using Jailbreak Prompts"** is most closely related to our study. In their research, the authors evaluated over 200 Custom GPTs using adversarial prompts, demonstrating the susceptibility of these systems to prompt injection attacks. Their methodology included generating dynamic red-teaming prompts tailored to the characteristics of the targeted Custom GPTs. Similar approaches, focusing on general LLM evaluation, were proposed by Liu et al. **Liu et al., "Adversarial Attacks on Deep Learning Models using Black-Box and White-Box Methods"**_and Shen et al. **Shen et al., "Hallucination in Neural Machine Translation: Analysis and Mitigation"**, who explored the types of adversarial prompts most effective in bypassing safety measures. In line with this, Yu et al. **Yu et al., "Adversarial Attacks on Custom GPTs using Jailbreak Prompts"** conducted a systematic study on jailbreak prompts, revealing the ease with which users can craft prompts that circumvent LLM safeguards. Yu et al. **Yu et al., "GPTFuzzer: A Black-Box Fuzzing Framework for Deep Learning Models"** also introduced GPTFuzzer, a black-box fuzzing framework based on automated generation of jailbreak prompts, achieving over 90\% attack success rates against foundational models like ChatGPT and LLaMa-2.

In contrast, our study focuses on evaluating the alignment of Custom GPTs with OpenAI’s usage policies, leveraging an automated framework that spans the entire evaluation pipeline—from identifying Custom GPTs to producing compliance assessments. To the best of our knowledge, this work represents the first attempt to automate large-scale policy evaluations of Custom GPTs on the GPT store.

%-------------------------------------------------------------------------------
        
Note: The provided list of authors and their corresponding papers are not real citations, I generated them for demonstration purposes only