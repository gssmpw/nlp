\begin{table*}[h]
\begin{center}
    \resizebox{1.0\textwidth}{!}{
\begin{tabular}{|p{3.6em}|p{3em}|p{6em}|p{6em}|p{7em}|p{4em}|p{7.5em}|p{3em}|}
\hline
 & Full GP & Subset of Data (Sec.~\ref{subsec:subsetdatasingle}) & Expert-based methods (Sec.~\ref{sec:multiple_models}) & FITC \cite{snelson_sparse_2005}/VFE \cite{titsias_variational_2009} (Sec.~\ref{sec:methodsinducing})  & SSGP (Sec.~\ref{sec:finitedim}) & SKI \cite{wilson_kernel_2015} & SVGP \cite{hensman_gaussian_2013}\\
 \hline
 Training &$\mathcal{O}(N^3)$ &$\mathcal{O}(M^3)$ &$\mathcal{O}(NM_e^2)$ &$\mathcal{O}(NM^2)$ &$\mathcal{O}(Np^2)$ &$\mathcal{O}(N+ M\log M)$ &$\mathcal{O}(M^3)$ \\
 \hline
 Inference &$\mathcal{O}(N^2)$ &$\mathcal{O}(M^2)$ &$\mathcal{O}(M_e^2)$ &$\mathcal{O}(M^2)$ &$\mathcal{O}(p^2)$ &$\mathcal{O}(M\log M)$ &$\mathcal{O}(M^2)$\\
 \hline
\end{tabular}}
\vspace{1em}
\caption{Summary of computational complexities of scalable Gaussian process regression methods reviewed in Section \ref{sec:scalableGPs}.  \textbf{Acronyms:} FITC=Fully Independent Training Conditional; VFE=Variational Free Energy; SSGP=Sparse Spectrum Gaussian Processes; SKI=Structured Kernel Interpolation; SVGP=stochastic variational Gaussian Processes.
\textbf{Notation:} $N$ is the number of training data points; $M$ denotes the subset of data or inducing points; $p$ is the number of features in the finite-dimensional representations of the kernel operator -- we present it here for sparse-spectrum Gaussian processes, but it holds also for the covariance series expansion; $M_e$ is the number of data points allocated to each expert.  As regards the latter method, we report only the most favorable computational complexity.}
    \label{tab:scalableGPs}
\end{center}
\end{table*}
