\section{Introduction and Related Work}
Medical systematic reviews (SRs) synthesise evidence from the literature, requiring high recall to avoid missing relevant studies. The screening process is critical to ensure high recall and is a two-stage process: Firstly, the title and abstract of studies are assessed by medical researchers or librarians for relevance, followed by the full text. The former title and abstract (T\&A) screening generally involves tens of thousands of studies~\cite{borah2017analysis}, leading to a high workload and cost. Several tools and products have been developed to reduce this workload, including
ASReview~\cite{van2021open},%
\footnote{\url{https://asreview.nl/}} 
Covidence,%
\footnote{\url{https://www.covidence.org/}}
DistillerSR,%
\footnote{\url{https://www.distillersr.com/}}
and RobotAnalyst~\cite{przybyla2018prioritising}.%
\footnote{\url{https://nactem.ac.uk/robotanalyst/}}
These tools classify studies using classical machine learning. Each study is suggested for inclusion or exclusion or labelled with a confidence score by the model. 

Prior work has proposed to use active learning (AL)~\cite{settles2009active} to strategically select studies for manual judgement in order to iteratively train models to more effectively prioritise relevant studies. The use of AL in systematic review automation tools is so far limited~\cite{van2021open}. Furthermore, recent studies~\cite{yang2022goldilocks,mao2024reproducibility} showed that neural models such as BERT have the potential to prioritise studies much more effectively than previous approaches using AL, especially when pre-trained on domain-specific data (e.g., bio-medicine)~\cite{mao2024reproducibility}. However, one downside to these highly effective models, and in fact all AL methods, is that they still require bootstrapping in the form of pre-selected relevant studies. Their computational cost also makes them considerably slower than traditional classification methods. 

In this paper, we demonstrate DenseReviewer, a screening tool leveraging dense retrievers and tailored queries (i.e., PICO: patient/population, intervention, comparison, and outcome~\cite{scells2017integrating}) for T\&A screening. Key features of DenseReviewer are summarised in table~\ref{tab:tool-compare}, and compared with popular SR tools mentioned above.
DenseReviewer iteratively updates a PICO query efficiently via the Rocchio's algorithm for dense retrieval~\cite{li2023pseudo} based on the screener's feedback (i.e., the judgement of each screened study). 
Our previous work~\cite{mao2024dense} showed that this dense retrieval-based approach is more effective for screening than logistic regression-based and BERT-based active learning workflows, while maintaining efficiency comparable to traditional machine learning-based methods. 

\begin{table}[t!]
	\centering
	\caption{Comparison of key features between DenseReviewer and popular SR tools. `Full Screen' shows the title and abstract of one study at a time, while `Ranking List' presents studies with their titles and abstracts for screening in an order, typically by relevance.}
	\begin{tabular}{p{9em}p{13em}p{8em}ll@{}}
		\toprule
		\textbf{Screening Tool} & \textbf{Core Technology} & \textbf{Interface} & \textbf{Code} \\ \midrule
		ASReview       & \multirow{4}{*}{\shortstack[l]{Machine Learning \\ and Active Learning \\ (model training)}}         & Full Screen          & Open Source   \\
		Covidence     &         & Ranking List        & Proprietary   \\
		DistillerSR      &       & Ranking List        & Proprietary   \\
		RobotAnalyst  &     & Ranking List         & Proprietary   \\ \midrule
		\raisebox{+0.5\totalheight}{DenseReviewer}   & {\shortstack[l]{Dense Retrieval \\ and Relevance Feedback \\ (query vector updating)}}         & {\shortstack[l]{Ranking List \\ and Full Screen}}     & \raisebox{+0.5\totalheight}{Open Source}   \\ \bottomrule
	\end{tabular}
	\label{tab:tool-compare}
\end{table}

