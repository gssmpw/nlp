\section{Architecture and Library}
DenseReviewer comprises six Docker containers that can all be deployed on a single cloud instance:\enlargethispage{2\baselineskip} (i) a web-based front end, (ii) a REST API back end, (iii) a database for storing information such as user activity, uploaded corpora, and ranking studies, (iv) a message broker for managing asynchronous task queues, (v) a service dedicated to tasks such as parsing, encoding, indexing, and initial ranking when new datasets are uploaded to review, and (vi) a service responsible for handling re-ranking. We deployed DenseReviewer on an AWS EC2 accelerated computing instance (g4dn.xlarge: 1 T4 Tensor GPU, 16 GB GPU memory, 4 vCPUs, and 16 GB instance memory). The architecture can scale to more powerful instances, such as the G5 or P4 series with NVIDIA A10G and A100 GPUs, without requiring major modifications. 

\begin{figure}[t!]
\lstset{basicstyle=\ttfamily, columns=fullflexible, keepspaces=true,frame=tb}
\scriptsize
\newsavebox{\training}
\begin{lrbox}{\training}
\begin{minipage}{.48\textwidth}
\begin{lstlisting}
python tevatron_pipe.py \
	--collection_split clef19_dta_train \
	--model_path biolinkbert \
	--q_max_len 128 \
	--p_max_len 256 \
	--train_n 11 \
	--train_epoch 60
	
	
\end{lstlisting}
\end{minipage}
\end{lrbox}
\newsavebox{\screening}
\begin{lrbox}{\screening}
\begin{minipage}{.48\textwidth}
\begin{lstlisting}
python dense_query_tar.py \
	--collection_split clef19_dta_test \
	--model biolinkbert_128_256_11 \
	--n_iteration 20 \
	--top_k 20 \
	--output_path ourput_dir \
	--alpha 1.0 \
	--beta 0.8 \
	--gamma 0.2
\end{lstlisting}
\end{minipage}
\end{lrbox}
\subcaptionbox{Training\label{fig:python.training}}[0.48\textwidth]{\usebox{\training}}
\subcaptionbox{Screening\label{fig:python.screening}}[0.48\textwidth]{\usebox{\screening}}
\setcounter{figure}{2}    
\captionof{figure}{Command line usage of DenseReviewer for training and screening.}
\label{fig:python}
\end{figure}

Figure~\ref{fig:python} shows the command line usage for (i) training dense retrievers based on \texttt{Tevatron} (Figure~\ref{fig:python.training}) and (ii)  running experiments with the trained dense retrievers and feedback methods (Figure~\ref{fig:python.screening}).
