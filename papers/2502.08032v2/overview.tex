\section{Overview of techniques}
\label{sec:overview}

\danupon{To do: Define things not defined in the intro. If there are too many things, consider moving this after the prelim.}

\subsection{Lower bound: Hardness of \texorpdfstring{$(n^\eps, n^\eps)$}{(n,n)}-approximation}

Here we give a brief overview of our approach and techniques to prove \Cref{thm:intro:lowerbound}. For convenience, we refer to the problem of approximating shortcut of a given directed graph as \os{}. Indeed, our goal is to prove that \os{} is $(n^{\epsilon}, n^{\epsilon})$-hard to approximate even when assuming that $s=\Omega(m)$ and diameter $d=n^{\epsilon}$ for some small constant $\epsilon$. As mentioned in the introduction, the hardness result for this parameter range would imply the same hardness for TC-spanners as well (see, e.g.,~\cref{lem:redTCtoSh} for a formal statement).

 

 
\paragraph{Label Cover.} Our hardness result is based on a reduction from the well-known \labcov{} problem  to \os{}. We now briefly state what \labcov{} is: Given an input instance denoted by $\I = (G,\L, \pi)$, where $G=(A \cup B,E)$ is a bipartite undirected graph, $\cL$ is an alphabet, and $\pi = \{\pi_e \subseteq  \L \times \L\}_{e \in E}$ is a family of relations defined for each edge $e \in E$, we say, for any assignment (labeling)  $\psi: A \cup B \rightarrow \L$, that $e=(u,v)$ is covered if $(\psi(u), \psi(v)) \in \pi_e$.

The objective of this problem is to compute a labeling $\psi$ that covers as many edges as possible. We sometimes allow the labeling function to assign more than one label to vertices, i.e., a multi-labeling $\psi: A \cup B \rightarrow 2^{\L}$ covers edge $(u,v)$ if $\psi(u) \times \psi(v)$ contains at least one pair of labels in $\pi_{(u,v)}$. The cost of such multi-labeling is the total number of labels assigned, that is, $\sum_{u \in A \cup B}|\psi(u)|$. 
Assuming the projection game conjecture (PGC)~\cite{Moshkovitz15}, it is hard to distinguish between the following two cases: In the completeness case, there exists an assignment $\psi$ that covers every edge,  while in the soundness case, we cannot cover more than $|\I|^{-\epsilon}$  fraction of edges even when using a multi-labeling $\psi:A \cup B \rightarrow 2^{\L}$ of cost $|\I|^{\epsilon}(|A|+|B|)$. 





\paragraph{Initial attempt (\cref{subsec:warmup}).} We start from a simple (base) reduction that takes a label cover instance $\I$ and a target diameter $\rho$  as input and produces a directed graph $H_{\I, \rho}$, or $H$ is short,  with $|V(H)| = \poly(\I)$, such that the following happens (see~\cref{fig:minrepgraph}):
\begin{enumerate}
    \item[-] \textbf{Bijection:} There is a special set of pairs of vertices $C_H \subseteq V(H) \times V(H)$, denoted as the \textit{canonical set}, such that any subset of $C_H$ corresponds to a multi-labeling in $\I$ and vice versa.

    \item[-] \textbf{Completeness:} A multi-labeling in $\I$ would precisely correspond to a shortcut solution that chooses only the pairs in $C_H$; moreover, if the multi-labeling covers all edges, then the corresponding shortcut will reduce the diameter of $H$ to be at most $d=\rho+1$\yonggang{Is it a good idea to define $d=\rho+1$? We also use $d_H$ to refer to the density later.}\yonggang{It would be good to just say $\rho$, but in the construction of $H_{I,\rho}$, the truth is $d=\rho+1$. Now I realize that I should really define $H_{I,\rho}=H_{I,\rho-1}$}. This means any optimal perfectly covering labeling $\psi$ for $\I$ corresponds to feasible shortcuts $C_{\psi} \subseteq C_H$ of exactly the same cost. 

    \item[-] \textbf{Soundness:} Conversely, we prove that any shortcut solution $C'$ that reduces the diameter of $H$ to $d$ corresponds to a multi-labeling of roughly the same size (thereby, corresponds to a \textit{different} shortcut within $C_H$ of roughly the same size), while covering at least $|\I|^{-\epsilon}$ fraction of edges in $\I$.
\end{enumerate}




To summarize, this reduction establishes that the optimal shortcut value in $H$ is roughly the same as the optimal cost of multi-labeling in $\I$. In the completeness case, we therefore are guaranteed to have a shortcut set of cost $|A|+|B|$, while in the soundness case, there is no feasible shortcut set of cost less than $|\I|^{\epsilon}(|A|+|B|)$. This would imply a factor of $|\I|^{\epsilon} \approx |V(H)|^{\Omega(\epsilon)}$ hardness.

The construction is similar to the previous work for proving the hardness of approximating directed spanner~\cite{ElkinP07}. 
However, it has two major deficiencies. \begin{enumerate}[(i)]
    \item \textbf{Small shortcut set:} Recall from the statement of \Cref{thm:intro:lowerbound} that we need $s = \omega(|E(H)|)$. This is crucial to transfer our shortcut lower bound to TC-spanner lower bound (see \Cref{lem:redShtoTC}). However, by construction, the canonical set $C_H$, and thereby the shortcut set, is much smaller than $|E(H)|$, i.e., $s \leq |C_H| = o(|E(H)|)$. Phrased somewhat differently, by construction, the density of the canonical set, $d_H  =|C_H|/|E(H)| = o(1)$ is vanishing which we cannot tolerate.
    
    \item \textbf{Small diameter approximation:} The reduction only rules out $(O(1), n^{\epsilon})$-approximation algorithms under PGC (i.e. $\alpha_D$ is very small). That is unavoidable if we use  constructions similar to $H$ (see~\cref{fig:minrepgraph}), because $H$ itself has diameter $O(d)$ without adding any shortcut. To get a hardness result for much larger $\apxD$, we need a construction where the shortcuts can reduce the diameter significantly. 
\end{enumerate} 
Overcoming these two deficiencies is the main technical contribution of this paper.

\paragraph{Boosting canonical set density (\cref{subsec:largebicriteria}).} In order to overcome the first drawback, we want, as a necessary condition, a construction that boosts the density $d_H$ of the canonical solution. One natural idea to this end (which is also implicitly used by~\cite{BhattacharyyaGJRW12}) is to ``compose'' the base construction $H$ with some ``combinatorial object'' $O$ that has desirable properties (for instance, one such property is that the density of canonical pairs inside $O$ should be high, that is, $\omega(1)$.).
In particular, the reduction takes \labcov{} instance $\I$ and outputs $H^* = H \odot O$ (where $\odot$ represents some kind of composition between $H$ and $O$ that would not be made explicit here). 
The object $O$ would be chosen so that the density $d_{H^*} = |C_{H^*}|/|E(H^*)|$ can be made $\omega(1)$. In this way, we have $|C_{H^*}|=\omega(|E(H^*)|)$ which means $s$ can be $\omega(m)$.\footnote{In fact, the value of $s$ can be much less than the size of the canonical set. For convenience of discussion, this overview focuses on boosting the size of the canonical set (which is, strictly speaking, still not sufficient to prove our hardness result.)} %




Let us denote by $\gamma_O = d_{H\odot O}/d_H$ the (effective) \textbf{boosting factor} of the object $O$; this parameter reflects how much we can increase the contribution of the canonical set after composing the base construction with $O$. If we can make $\gamma_O$ to be sufficiently high, this would allow us to obtain the desired result for large shortcut set.  %




In the case of~\cite{BhattacharyyaGJRW12}, the properties of the combinatorial object they need are provided by the butterfly graph (denoted by $O_B$), whose effective boosting factor decreases exponentially in the target diameter $d$, that is $\gamma_{O_B} = O(n^{1/d})$ (or equivalently, the butterflies grow exponentially, i.e., $|O_B|=\gamma_{O_B}^{\Omega(d)}$), and therefore, when $d = \Omega(\log n)$, such a construction faces its theoretical limit (the boosting factor is a constant), obtaining only an NP-hardness result. We circumvent this barrier via two new ideas: 

\begin{itemize}
    
    \item \textbf{Intermediate problem(see~\cref{def:gadget})}: We introduce a ``Steiner'' variant of the \os{} problem as an intermediate problem (that we call the \textit{minimum steiner shortcut problem} ({\sf MinStShC})). In this variant, we are additionally given, as input, two disjoint sets $L, R \subseteq V(H): L \cap R = \emptyset$, and a set of pairs $P\subseteq L\times R$. Our goal is to distinguish whether (i) adding a size $s$ shortcut reduces the distance between any pairs $(u,v) \in L \times R$ to constant, or (ii) adding a size $sn^{\epsilon}$ shortcut cannot reduce the distance of even $o(1)$ fraction of pairs in $P$ to $d/3$. We shortly explain why we consider this variant.



    \item \textbf{More efficient combinatorial object}: Our goal now turns into canonical boosting for {\sf MinStShC} (instead of the \os{} problem) by composition with a combinatorial object having the desired properties. Our combinatorial object is inspired by the techniques of Hesse~\cite{Hesse03}, Huang and Pettie~\cite{HuangP21}\footnote{We note here that even though \cite{BhattacharyyaGJRW12} realised the upshot of using the gadget from \cite{Hesse03} regarding canonical boosting, they need a gadget with more structure in order for their technique to work, which is the butterfly graph. Because of our use of the intermediate problem, we do not face this bottleneck.}. It ensures weaker combinatorial properties than the butterflies but would still be sufficient for our purpose (crucially because we work with the intermediate problem). We denote our object by $O^*$. The main advantage of $O^*$ is its relatively compact size, i.e., to obtain a boosting factor of $\gamma_{O^*}$, the size of $O^*$ required is only $|O^*| \leq {\rm poly}(\gamma_{O^*})$. %

\end{itemize}

We briefly explain our rationale why we work with the intermediate problem {\sf MinStShC} now. Our use of {\sf MinStShC} allows a \textit{simpler and cleaner canonical boosting process}. For example, \cite{BhattacharyyaGJRW12} needs to modify the \labcov{} instance to be ``noise-resilient''  (and therefore their reduction adds an extra pre-processing step that turns any \labcov{} instance $\I$ into a noise-resilient instance $\I'$ which is suitable for performing canonical boosting). In contrast, our reduction works with  any given \labcov{} instance $\I$ in a blackbox fashion. \yonggang{I dont't understand this paragraph up to here}%
We remark that, unlike \cite{BhattacharyyaGJRW12}, our canonical boosting process would not work with the original \os{} problem, so the use of the intermediate problem is really crucial for us. 

As our ultimate goal is to obtain hardness for \os{}, we somehow need to convert the hardness of {\sf MinStShC} to \os (We explain more on this in the next paragraph.). To accomplish this, we need \textit{the gap version} of {\sf MinStShC} hardness (for technical reasons). This is not a significant concern in the reduction from $\labcov{}$ to {\sf MinStShC}, because the hardness for \labcov{} is itself a gap version.






\paragraph{From {\sf MinStShC} to $(n^{\epsilon}, n^{\epsilon})$ hardness for \os{} (\cref{subsec:bicriterialower}).}
It turns out that our object $O^*$ can be used in two crucial ways: (i) to perform canonical boosting and (ii) to serve as a gadget that turns the hardness of the steiner variant into the hardness of \os{} in a way that preserves all important parameters (the hardness factor and the relative size of the canonical set) while obtaining the $(n^{\epsilon}, n^{\epsilon})$-bicriteria hardness (boosting the value of $\alpha_D$ from $O(1)$ to $n^{\epsilon}$ as a by-product of this composition). 
In particular, the hard instance $J$ of {\sf MinStShC} can be composed with $O^*$ to obtain the final instance $J \oplus O^*$ for \os{}. 
This  step bears a certain similarity with some known constructions in the literature of hardness of approximation, e.g.,~\cite{guruswami1999near,chuzhoy2009polynomial} for directed disjoint path problems and the use of graph products~\cite{chalermsook2014pre,chalermsook2013graph}. However, the combinatorial objects necessary (for the composition step) are often problem-specific, and therefore these works are all technically very different  from this paper. 


To summarize, our reduction takes \labcov{} instance $\I$ and produces a base instance $H$ of the steiner shortcut problem. After that, we perform the canonical boosting by composing $H$ with $O^*$, obtaining the hard instance $H \odot O^*$ of the {\sf MinStShC} with our desired density parameter. Finally, we perform another composition to turn the instance $H \odot O^*$ into the instance $(H\odot O^*) \otimes O^*$ of \os{} (again $\otimes$ denotes a certain composition between instances that would not be made explicit in this section).   
 See Figure~\ref{fig:reduction} for illustration. 


\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/reduction.png}
    \caption{A high-level structure of our reduction. The boosting step  handles the instances of the intermediate problem. }
    \label{fig:reduction}
\end{figure}




 












\subsection{Upper Bound: An \texorpdfstring{$(\apxD, \apxS)$}{apxDapxS}-approximation algorithm}
In this section, we give an overview of an \textit{approximately instance optimal} shortcut (and TC-spanner) algorithm as mentioned in \Cref{thm:intro:upperbound}. Our goal is to find an \ssss{\apxS s}{\apxD d} in a given graph $G$ and integers $d$ and $s$ such that $G$ admits an \ssss{s}{d}. Our algorithm incorporates many ideas from the previous algorithm of \cite{BermanBMRY13}. Below, we present an overview and discuss how our techniques differ from theirs. 


Denote by $\pset \subseteq V(G) \times V(G)$ the (ordered) set of reachable pairs in $G$. 
The shortcut edges are chosen from $E^T \setminus E$ where $E^T$ are the edges in the transitive closure of $G$. We say that $F \subseteq (E^T \setminus E)$ $d'$-settles the pair $(u,v)$ if the distance between them in $E \cup F$ is at most $d'$.  
Therefore, to get $(\alpha_D, \alpha_S)$-approximation, it suffices to compute a subset $F \subseteq E^T \setminus E$ so that every pair in $\pset$ is $(\alpha_D \cdot d)$-settled by $F$.  
We will handle two types of pairs in $\pset$ separately. 
We say that  $(u,v) \in \pset$ is \textit{thick} if the total number of vertices $w$ that are reachable from $u$ and can reach $v$ is at least $\beta$; otherwise, the pair $(u,v)$ is \textit{thin}. Intuitively, a thick pair is ``very well connected''. 
Divide $\pset$ into $\pset = \pset_{thick} \cup \pset_{thin}$.

The high-level ideas in dealing with these cases roughly follow~\cite{BermanBMRY13}. At a high-level, a generic approach to turn a (single-criteria) approximation algorithm into a bicriteria one is to prove a certain ``scaling advantage'' result, e.g., proving that a $(1,\alpha_S)$-approximation implies $(\alpha_D, O(\alpha_S/\alpha_D))$-approximation (so when $\alpha_D=1$, we achieve roughly the same result). 


 


\paragraph{Settling thick pairs.} Sample $V' \subseteq V(G): |V'| =  \tilde{\Theta}(n/\beta)$ uniformly. For each sampled vertex $v \in V'$, add edges from $v$ to all $v'$ such that $v'$ is reachable from $v$ or can reach $v$. 
Denote by $F_1$ the set of edges that are added by this process, so $|F_1| = \tilde{\Theta}(n^2/\beta)$. It is easy to see (via a hitting set argument) that, with constant probability, this set of shortcut edges reduces the diameter to two and the total number of shortcut edges is at most $\tilde O(n^2/\beta)$.


This simple sampling strategy has already been explored by \cite{BermanBMRY13} in the context of spanner construction.\footnote{In the context of spanners, instead of connecting a sampled $v$ with in- and out-edges with vertices in $V$, they consider in- and out-arborescence rooted at $v$. In our context (of TC-spanners and shortcuts), these arborescences are exactly the edges we described above.} 
To incorporate the scaling advantage, we adapt the technique used in a recent paper demonstrating how to construct an \ssss{n}{\tOh(n^{1/3})} for any graph~\cite{KoganP22}. This adaptation reduces the size of the shortcut set to $\tilde{O}\left(n^2/\beta(\apxD d)^2\right)$ while still ensuring that the endpoints of all thick edges   have a distance of at most $\apxD d$. 









\paragraph{Settling the thin pairs.} Thin pairs are handled via LP-rounding techniques. 
We say that a set $A\subseteq E^T \backslash E$ is $d'$-\textit{critical} for thin pair $(u,v) \in \pset_{thin}$ if in $E^T \setminus A$, the distance from $u$ to $v$ is larger than $d$; in other words, not taking any edge from $A$ would make the solution infeasible. 
Denote by $\aset_{d'}$ the set of all minimal and $d'$-critical edges.  
Our definition of critical set is analogous to the notion of antispanners in~\cite{BermanBMRY13}. The following claim is intuitive: It asserts an alternative characterization of shortcuts as a set of edges that hit every critical set. 


\begin{claim}[Adapted from \cite{BermanBMRY13}] \label{clm:shortcut-hittingset}
    An edge set $E'$ is a $d'$-shortcut set for all thin pairs if and only if $E' \cap A \neq \emptyset$  for all $A\in\mathcal{A}_{d'}$. 
\end{claim}





The above claim allows us to write the following LP constraints for finding a $(d,s)$-shortcut. 


\begin{align*}
	\text{(LP)}: \qquad\sum_{e\in E^T\backslash E}x_e &\le s \\
	\sum_{e\in A}x_e &\ge 1\qquad \forall A\in \mathcal{A}_d\\
	x_e &\geq 0\qquad \forall e\in E^T \setminus E
\end{align*}

Each variable $x_e$ indicate whether edge $e \in E^T \setminus E$ is included into the shortcut solution. Each constraint $\sum_{e \in A} x_e \geq 1$ asserts that critical edge set $A$ must be ``hit'' by the solution. There can be exponentially many constraints, but an efficient separation oracle exists, as we sketch below. 



The high-level idea of~\cite{BermanBMRY13} (when translated into our setting) is a ``round-or-cut'' procedure that, from a feasible solution $\textbf{x} \in [0,1]^{E^T \setminus E}$, randomly computes $F_2 \subseteq E^T \setminus E$ of  size $O(\beta s)$ that either (i) successfully $d$-settles all thin pairs in $\pset_{thin}$ or (ii) can be used to find a critical set $A \in \aset_{d}$ such that $\sum_{e \in A} x_e < 1$. 
If Case (i) happens, we have successfully computed the solution $F_2$. Otherwise, when Case (ii) happens, we have a separation oracle. 
In order to incorporate the scaling advantage into this algorithm, we prove a decomposition lemma (\cref{lem:findantispanner}) which asserts that each $(\alpha_D \cdot d)$-critical set $A \in\mathcal{A}_{\apxD d}$ can be partitioned into $\apxD$  sets in $\mathcal{A}_{d}$, and such decomposition can be computed efficiently.
This decomposition lemma allows us to generalize~\cite{BermanBMRY13} to the bicriteria setting, leading to the set of size $O(\beta s/\alpha_D)$ that $(\alpha_D d)$-settles thin pairs. 



















