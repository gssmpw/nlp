\section{Related Work}
Benchmarking is a critical step in advancing large language models, serving as a cornerstone to guide their application and development. However, as expectations for LLMs continue to rise, general language understanding benchmarks no longer suffice for specialized professional domains.
In the medical domain, numerous datasets and benchmarks~\cite{pal2022medmcqa,zhang2021cblue,suthar2023artificial,singhal2023large,zhu2023promptcblue,goodman2023accuracy,rao2023assessing,lim2023benchmarking,li2023chatgpt,wang2023cmb,liu2024benchmarking,hager2024evaluation,longwell2024performance,DBLP:conf/aaai/0001WWMZWH24} have emerged to evaluate the medical capabilities of LLMs. 
These studies aim to enhance clinical utility and alleviate the workload of healthcare professionals. 
For example, Pal et al.~\cite{pal2022medmcqa} established their benchmark with $194$k high-quality AIIMS \& NEET PG entrance exam multiple-choice questions, and Wang et al.~\cite{wang2023cmb} introduced the CMB, a localized medical benchmark designed entirely within the native Chinese linguistic and cultural framework.

Among these studies~\cite{pal2022medmcqa,zhang2021cblue,suthar2023artificial,singhal2023large,zhu2023promptcblue,goodman2023accuracy,rao2023assessing,lim2023benchmarking,li2023chatgpt,wang2023cmb,liu2024benchmarking,hager2024evaluation,longwell2024performance,DBLP:conf/aaai/0001WWMZWH24}, multiple-choice questions are typically preferred because they simplify the evaluation process and offer an objective assessment.
In contrast, open-ended questions require subjective input from medical experts, which can be time-consuming and labor-intensive, yet they provide greater flexibility and deeper insights into an LLM's capabilities.
With the development of ``LLM-as-a-judge'' methods~\cite{son2024llm,cao2024compassjudger}, the dependence on human evaluation has been reduced, allowing assessments to be conducted automatically by LLMs.
Therefore, combining multiple-choice and open-ended questions within a single benchmark is preferable to provide a more comprehensive assessment of an LLM's capabilities~\cite{li2024multiple}. 

Despite the above progress, evaluating LLMs in the professional medical field remains challenging due to variations in specialized knowledge across different departments or diseases, as well as regional differences in medical systems and language use.
To address these complexities, benchmarks have become increasingly specialized, focusing on particular departments~\cite{zhang2024pediabench,DBLP:journals/corr/abs-2406-01126}, regions~\cite{DBLP:conf/aaai/0001WWMZWH24,jiang2024jmedbench}, or diseases~\cite{DBLP:conf/kdd/ChenMGWZC24}.
With regard to departmental disparities, Zhang et al.~\cite{zhang2024pediabench} introduced a Chinese pediatric dataset to compensate for the limited evaluations of LLMs in pediatrics.
Focusing on disease types, Chen et al.~\cite{DBLP:conf/kdd/ChenMGWZC24} examined rare diseases to investigate the capabilities and limitations of LLMs.
Meanwhile, concerning region-specific contexts,  Cai et al.~\cite{DBLP:conf/aaai/0001WWMZWH24} proposed MedBench to accommodate China's linguistic, and clinical standards and procedures. 

Although several studies have investigated LLMs' practical utility in ophthalmology~\cite{lim2023benchmarking,antaki2023capabilities,bernstein2023comparison,huang2024assessment}, they often lack comprehensiveness and a standardized evaluation framework and alignment with the ophthalmic clinical workflow.
There is an urgent need for an objective, systematic, and standardized benchmark tailored to the unique characteristics of Chinese ophthalmology to accurately assess LLM performance and its clinical applicability. 
To address this challenge, we propose OphthBench, a specialized benchmark incorporating diverse question types, designed to comprehensively assess LLM capabilities in this context, as summarized in Table~\ref{tab:benchmarks}.

\begin{table}[tb!]
    \centering
    \caption{OphthBench comprehensively incorporates the core clinical workflow and various question types.}
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Benchmark} & \textbf{SCQ}  & \textbf{MCQ} & \textbf{QA} & \textbf{Clinical Workflow} \\
        \midrule
        CMExam~\cite{liu2024benchmarking} & \textcolor{green}{\ding{51}}& \textcolor{red}{\ding{55}}& \textcolor{green}{\ding{51}} & \textcolor{red}{\ding{55}}\\
        CMB~\cite{wang2023cmb} & \textcolor{green}{\ding{51}}& \textcolor{red}{\ding{55}}& \textcolor{green}{\ding{51}} & \textcolor{red}{\ding{55}}\\
        MedMCQA~\cite{pal2022medmcqa} & \textcolor{green}{\ding{51}}& \textcolor{red}{\ding{55}}& \textcolor{green}{\ding{51}} & \textcolor{red}{\ding{55}}\\
        MedBench (paper)~\cite{DBLP:conf/aaai/0001WWMZWH24} & \textcolor{green}{\ding{51}}& \textcolor{green}{\ding{51}}& \textcolor{red}{\ding{55}} & \textcolor{red}{\ding{55}}\\
        MedBench (website) & \textcolor{green}{\ding{51}}& \textcolor{green}{\ding{51}}& \textcolor{green}{\ding{51}} & \textcolor{red}{\ding{55}}\\
        MultiMedQA~\cite{singhal2023large} & \textcolor{green}{\ding{51}}& \textcolor{red}{\ding{55}}& \textcolor{green}{\ding{51}}& \textcolor{red}{\ding{55}}\\
        PromptCBLUE~\cite{zhu2023promptcblue} & \textcolor{green}{\ding{51}}& \textcolor{green}{\ding{51}}& \textcolor{green}{\ding{51}} & \textcolor{red}{\ding{55}}\\
        OphthBench (Ours) & \textcolor{green}{\ding{51}} & \textcolor{green}{\ding{51}}& \textcolor{green}{\ding{51}} & \textcolor{green}{\ding{51}}\\
        \bottomrule
    \end{tabular}
    \label{tab:benchmarks}
\end{table}