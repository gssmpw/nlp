[
  {
    "index": 0,
    "papers": [
      {
        "key": "pal2022medmcqa",
        "author": "Pal, Ankit and Umapathi, Logesh Kumar and Sankarasubbu, Malaikannan",
        "title": "Medmcqa: A large-scale multi-subject multi-choice dataset for medical domain question answering"
      },
      {
        "key": "zhang2021cblue",
        "author": "Zhang, Ningyu and Chen, Mosha and Bi, Zhen and Liang, Xiaozhuan and Li, Lei and Shang, Xin and Yin, Kangping and Tan, Chuanqi and Xu, Jian and Huang, Fei and others",
        "title": "Cblue: A Chinese biomedical language understanding evaluation benchmark"
      },
      {
        "key": "suthar2023artificial",
        "author": "Suthar, Pokhraj P and Kounsal, Avin and Chhetri, Lavanya and Saini, Divya and Dua, Sumeet G",
        "title": "Artificial intelligence (AI) in radiology: a deep dive into ChatGPT 4.0's accuracy with the American Journal of Neuroradiology's (AJNR)\" Case of the Month\""
      },
      {
        "key": "singhal2023large",
        "author": "Singhal, Karan and Azizi, Shekoofeh and Tu, Tao and Mahdavi, S Sara and Wei, Jason and Chung, Hyung Won and others",
        "title": "Large language models encode clinical knowledge"
      },
      {
        "key": "zhu2023promptcblue",
        "author": "Zhu, Wei and Wang, Xiaoling and Zheng, Huanran",
        "title": "PROMPTCBLUE: A Chinese Prompt Tuning Benchmark for the Medical Domain"
      },
      {
        "key": "goodman2023accuracy",
        "author": "Goodman, Rachel S and Patrinely, J Randall and Stone, Cosby A and Zimmerman, Eli and Donald, Rebecca R and Chang, Sam S and others",
        "title": "Accuracy and reliability of chatbot responses to physician questions"
      },
      {
        "key": "rao2023assessing",
        "author": "Rao, Arya and Pang, Michael and Kim, John and Kamineni, Meghana and Lie, Winston and Prasad, Anoop K and others",
        "title": "Assessing the utility of chatgpt throughout the entire clinical workflow: development and usability study"
      },
      {
        "key": "lim2023benchmarking",
        "author": "Lim, Zhi Wei and Pushpanathan, Krithi and Yew, Samantha Min Er and Lai, Yien and Sun, Chen-Hsin and Lam, Janice Sing Harn and others",
        "title": "Benchmarking large language models\u2019 performances for myopia care: a comparative analysis of chatgpt-3.5, chatgpt-4.0, and google bard"
      },
      {
        "key": "li2023chatgpt",
        "author": "Li, Sarah W and Kemp, Matthew W and Logan, Susan JS and Dimri, Pooja Sharma and Singh, Navkaran and Mattar, Citra NZ and Dashraath, Pradip and Ramlal, Harshaana and Mahyuddin, Aniza P and Kanayan, Suren and others",
        "title": "ChatGPT outscored human candidates in a virtual objective structured clinical examination in obstetrics and gynecology"
      },
      {
        "key": "wang2023cmb",
        "author": "Wang, Xidong and Chen, Guiming Hardy and Song, Dingjie and Zhang, Zhiyi and Chen, Zhihong and Xiao, Qingying and Jiang, Feng and Li, Jianquan and Wan, Xiang and Wang, Benyou and others",
        "title": "Cmb: A comprehensive medical benchmark in chinese"
      },
      {
        "key": "liu2024benchmarking",
        "author": "Liu, Junling and Zhou, Peilin and Hua, Yining and Chong, Dading and Tian, Zhongyu and Liu, Andrew and Wang, Helin and You, Chenyu and Guo, Zhenhua and Zhu, Lei and others",
        "title": "Benchmarking large language models on cmexam-a comprehensive Chinese medical exam dataset"
      },
      {
        "key": "hager2024evaluation",
        "author": "Hager, Paul and Jungmann, Friederike and Holland, Robbie and Bhagat, Kunal and Hubrecht, Inga and Knauer, Manuel and others",
        "title": "Evaluation and mitigation of the limitations of large language models in clinical decision-making"
      },
      {
        "key": "longwell2024performance",
        "author": "Longwell, Jack B and Hirsch, Ian and Binder, Fernando and Conchas, Galileo Arturo Gonzalez and Mau, Daniel and Jang, Raymond and others",
        "title": "Performance of large language models on medical oncology examination questions"
      },
      {
        "key": "DBLP:conf/aaai/0001WWMZWH24",
        "author": "Yan Cai and\nLinlin Wang and\nYe Wang and\nGerard de Melo and\nYa Zhang and\nYanfeng Wang and\nLiang He",
        "title": "MedBench: {A} large-scale Chinese benchmark for evaluating medical\nLarge Language Models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "pal2022medmcqa",
        "author": "Pal, Ankit and Umapathi, Logesh Kumar and Sankarasubbu, Malaikannan",
        "title": "Medmcqa: A large-scale multi-subject multi-choice dataset for medical domain question answering"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "wang2023cmb",
        "author": "Wang, Xidong and Chen, Guiming Hardy and Song, Dingjie and Zhang, Zhiyi and Chen, Zhihong and Xiao, Qingying and Jiang, Feng and Li, Jianquan and Wan, Xiang and Wang, Benyou and others",
        "title": "Cmb: A comprehensive medical benchmark in chinese"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "pal2022medmcqa",
        "author": "Pal, Ankit and Umapathi, Logesh Kumar and Sankarasubbu, Malaikannan",
        "title": "Medmcqa: A large-scale multi-subject multi-choice dataset for medical domain question answering"
      },
      {
        "key": "zhang2021cblue",
        "author": "Zhang, Ningyu and Chen, Mosha and Bi, Zhen and Liang, Xiaozhuan and Li, Lei and Shang, Xin and Yin, Kangping and Tan, Chuanqi and Xu, Jian and Huang, Fei and others",
        "title": "Cblue: A Chinese biomedical language understanding evaluation benchmark"
      },
      {
        "key": "suthar2023artificial",
        "author": "Suthar, Pokhraj P and Kounsal, Avin and Chhetri, Lavanya and Saini, Divya and Dua, Sumeet G",
        "title": "Artificial intelligence (AI) in radiology: a deep dive into ChatGPT 4.0's accuracy with the American Journal of Neuroradiology's (AJNR)\" Case of the Month\""
      },
      {
        "key": "singhal2023large",
        "author": "Singhal, Karan and Azizi, Shekoofeh and Tu, Tao and Mahdavi, S Sara and Wei, Jason and Chung, Hyung Won and others",
        "title": "Large language models encode clinical knowledge"
      },
      {
        "key": "zhu2023promptcblue",
        "author": "Zhu, Wei and Wang, Xiaoling and Zheng, Huanran",
        "title": "PROMPTCBLUE: A Chinese Prompt Tuning Benchmark for the Medical Domain"
      },
      {
        "key": "goodman2023accuracy",
        "author": "Goodman, Rachel S and Patrinely, J Randall and Stone, Cosby A and Zimmerman, Eli and Donald, Rebecca R and Chang, Sam S and others",
        "title": "Accuracy and reliability of chatbot responses to physician questions"
      },
      {
        "key": "rao2023assessing",
        "author": "Rao, Arya and Pang, Michael and Kim, John and Kamineni, Meghana and Lie, Winston and Prasad, Anoop K and others",
        "title": "Assessing the utility of chatgpt throughout the entire clinical workflow: development and usability study"
      },
      {
        "key": "lim2023benchmarking",
        "author": "Lim, Zhi Wei and Pushpanathan, Krithi and Yew, Samantha Min Er and Lai, Yien and Sun, Chen-Hsin and Lam, Janice Sing Harn and others",
        "title": "Benchmarking large language models\u2019 performances for myopia care: a comparative analysis of chatgpt-3.5, chatgpt-4.0, and google bard"
      },
      {
        "key": "li2023chatgpt",
        "author": "Li, Sarah W and Kemp, Matthew W and Logan, Susan JS and Dimri, Pooja Sharma and Singh, Navkaran and Mattar, Citra NZ and Dashraath, Pradip and Ramlal, Harshaana and Mahyuddin, Aniza P and Kanayan, Suren and others",
        "title": "ChatGPT outscored human candidates in a virtual objective structured clinical examination in obstetrics and gynecology"
      },
      {
        "key": "wang2023cmb",
        "author": "Wang, Xidong and Chen, Guiming Hardy and Song, Dingjie and Zhang, Zhiyi and Chen, Zhihong and Xiao, Qingying and Jiang, Feng and Li, Jianquan and Wan, Xiang and Wang, Benyou and others",
        "title": "Cmb: A comprehensive medical benchmark in chinese"
      },
      {
        "key": "liu2024benchmarking",
        "author": "Liu, Junling and Zhou, Peilin and Hua, Yining and Chong, Dading and Tian, Zhongyu and Liu, Andrew and Wang, Helin and You, Chenyu and Guo, Zhenhua and Zhu, Lei and others",
        "title": "Benchmarking large language models on cmexam-a comprehensive Chinese medical exam dataset"
      },
      {
        "key": "hager2024evaluation",
        "author": "Hager, Paul and Jungmann, Friederike and Holland, Robbie and Bhagat, Kunal and Hubrecht, Inga and Knauer, Manuel and others",
        "title": "Evaluation and mitigation of the limitations of large language models in clinical decision-making"
      },
      {
        "key": "longwell2024performance",
        "author": "Longwell, Jack B and Hirsch, Ian and Binder, Fernando and Conchas, Galileo Arturo Gonzalez and Mau, Daniel and Jang, Raymond and others",
        "title": "Performance of large language models on medical oncology examination questions"
      },
      {
        "key": "DBLP:conf/aaai/0001WWMZWH24",
        "author": "Yan Cai and\nLinlin Wang and\nYe Wang and\nGerard de Melo and\nYa Zhang and\nYanfeng Wang and\nLiang He",
        "title": "MedBench: {A} large-scale Chinese benchmark for evaluating medical\nLarge Language Models"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "son2024llm",
        "author": "Son, Guijin and Ko, Hyunwoo and Lee, Hoyoung and Kim, Yewon and Hong, Seunghyeok",
        "title": "LLM-as-a-Judge \\& Reward Model: What They Can and Cannot Do"
      },
      {
        "key": "cao2024compassjudger",
        "author": "Cao, Maosong and Lam, Alexander and Duan, Haodong and Liu, Hongwei and Zhang, Songyang and Chen, Kai",
        "title": "CompassJudger-1: All-in-one Judge Model Helps Model Evaluation and Evolution"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "li2024multiple",
        "author": "Li, Wangyue and Li, Liangzhi and Xiang, Tong and Liu, Xiao and Deng, Wei and Garcia, Noa",
        "title": "Can Multiple-choice Questions Really Be Useful in Detecting the Abilities of LLMs?"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "zhang2024pediabench",
        "author": "Zhang, Qian and Chen, Panfeng and Li, Jiali and Feng, Linkun and Liu, Shuyu and Chen, Mei and Li, Hui and Wang, Yanhao",
        "title": "PediaBench: A Comprehensive Chinese Pediatric Dataset for Benchmarking Large Language Models"
      },
      {
        "key": "DBLP:journals/corr/abs-2406-01126",
        "author": "Wenjing Yue and\nXiaoling Wang and\nWei Zhu and\nMing Guan and\nHuanran Zheng and\nPengfei Wang and\nChangzhi Sun and\nXin Ma",
        "title": "TCMBench: {A} Comprehensive Benchmark for Evaluating Large Language\nModels in Traditional Chinese Medicine"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "DBLP:conf/aaai/0001WWMZWH24",
        "author": "Yan Cai and\nLinlin Wang and\nYe Wang and\nGerard de Melo and\nYa Zhang and\nYanfeng Wang and\nLiang He",
        "title": "MedBench: {A} large-scale Chinese benchmark for evaluating medical\nLarge Language Models"
      },
      {
        "key": "jiang2024jmedbench",
        "author": "Jiang, Junfeng and Huang, Jiahao and Aizawa, Akiko",
        "title": "JMedBench: A Benchmark for Evaluating Japanese Biomedical Large Language Models"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "DBLP:conf/kdd/ChenMGWZC24",
        "author": "Xuanzhong Chen and\nXiaohao Mao and\nQihan Guo and\nLun Wang and\nShuyang Zhang and\nTing Chen",
        "title": "RareBench: Can LLMs Serve as Rare Diseases Specialists?"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "zhang2024pediabench",
        "author": "Zhang, Qian and Chen, Panfeng and Li, Jiali and Feng, Linkun and Liu, Shuyu and Chen, Mei and Li, Hui and Wang, Yanhao",
        "title": "PediaBench: A Comprehensive Chinese Pediatric Dataset for Benchmarking Large Language Models"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "DBLP:conf/kdd/ChenMGWZC24",
        "author": "Xuanzhong Chen and\nXiaohao Mao and\nQihan Guo and\nLun Wang and\nShuyang Zhang and\nTing Chen",
        "title": "RareBench: Can LLMs Serve as Rare Diseases Specialists?"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "DBLP:conf/aaai/0001WWMZWH24",
        "author": "Yan Cai and\nLinlin Wang and\nYe Wang and\nGerard de Melo and\nYa Zhang and\nYanfeng Wang and\nLiang He",
        "title": "MedBench: {A} large-scale Chinese benchmark for evaluating medical\nLarge Language Models"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "lim2023benchmarking",
        "author": "Lim, Zhi Wei and Pushpanathan, Krithi and Yew, Samantha Min Er and Lai, Yien and Sun, Chen-Hsin and Lam, Janice Sing Harn and others",
        "title": "Benchmarking large language models\u2019 performances for myopia care: a comparative analysis of chatgpt-3.5, chatgpt-4.0, and google bard"
      },
      {
        "key": "antaki2023capabilities",
        "author": "Antaki, Fares and Milad, Daniel and Chia, Mark A and Gigu{\\`e}re, Charles-{\\'E}douard and Touma, Samir and El-Khoury, Jonathan and Keane, Pearse A and Duval, Renaud",
        "title": "Capabilities of GPT-4 in ophthalmology: an analysis of model entropy and progress towards human-level medical question answering"
      },
      {
        "key": "bernstein2023comparison",
        "author": "Bernstein, Isaac A and Zhang, Youchen and Govil, Devendra and Majid, Iyad and Chang, Robert T and Sun, Yang and Shue, Ann and Chou, Jonathan C and Schehlein, Emily and Christopher, Karen L and others",
        "title": "Comparison of Ophthalmologist and Large Language Model Chatbot Responses to Online Patient Eye Care Questions"
      },
      {
        "key": "huang2024assessment",
        "author": "Huang, Andy S and Hirabayashi, Kyle and Barna, Laura and Parikh, Deep and Pasquale, Louis R",
        "title": "Assessment of a large language model\u2019s responses to questions and cases about glaucoma and retina management"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "liu2024benchmarking",
        "author": "Liu, Junling and Zhou, Peilin and Hua, Yining and Chong, Dading and Tian, Zhongyu and Liu, Andrew and Wang, Helin and You, Chenyu and Guo, Zhenhua and Zhu, Lei and others",
        "title": "Benchmarking large language models on cmexam-a comprehensive Chinese medical exam dataset"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "wang2023cmb",
        "author": "Wang, Xidong and Chen, Guiming Hardy and Song, Dingjie and Zhang, Zhiyi and Chen, Zhihong and Xiao, Qingying and Jiang, Feng and Li, Jianquan and Wan, Xiang and Wang, Benyou and others",
        "title": "Cmb: A comprehensive medical benchmark in chinese"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "pal2022medmcqa",
        "author": "Pal, Ankit and Umapathi, Logesh Kumar and Sankarasubbu, Malaikannan",
        "title": "Medmcqa: A large-scale multi-subject multi-choice dataset for medical domain question answering"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "DBLP:conf/aaai/0001WWMZWH24",
        "author": "Yan Cai and\nLinlin Wang and\nYe Wang and\nGerard de Melo and\nYa Zhang and\nYanfeng Wang and\nLiang He",
        "title": "MedBench: {A} large-scale Chinese benchmark for evaluating medical\nLarge Language Models"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "singhal2023large",
        "author": "Singhal, Karan and Azizi, Shekoofeh and Tu, Tao and Mahdavi, S Sara and Wei, Jason and Chung, Hyung Won and others",
        "title": "Large language models encode clinical knowledge"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "zhu2023promptcblue",
        "author": "Zhu, Wei and Wang, Xiaoling and Zheng, Huanran",
        "title": "PROMPTCBLUE: A Chinese Prompt Tuning Benchmark for the Medical Domain"
      }
    ]
  }
]