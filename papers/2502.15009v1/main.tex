\begin{abstract}
Conversational query rewriting is crucial for effective conversational search, yet traditional supervised methods require substantial labeled data, which is scarce in low-resource settings.  This paper introduces Prompt-Guided In-Context Learning, a novel approach that leverages the in-context learning capabilities of Large Language Models (LLMs) for few-shot conversational query rewriting.  Our method employs carefully designed prompts, incorporating task descriptions, input/output format specifications, and a small set of illustrative examples, to guide pre-trained LLMs to generate context-independent queries without explicit fine-tuning.  Extensive experiments on benchmark datasets, TREC and Taskmaster-1, demonstrate that our approach significantly outperforms strong baselines, including supervised models and contrastive co-training methods, across various evaluation metrics such as BLEU, ROUGE-L, Success Rate, and MRR.  Ablation studies confirm the importance of in-context examples, and human evaluations further validate the superior fluency, relevance, and context utilization of our generated rewrites.  The results highlight the potential of prompt-guided in-context learning as an efficient and effective paradigm for low-resource conversational query rewriting, reducing the reliance on extensive labeled data and complex training procedures.
\end{abstract}

\begin{IEEEkeywords}
Search Query, In-Context Learning, Large Language Models
\end{IEEEkeywords}

\section{Introduction}
Conversational search has become increasingly prevalent with the rise of voice assistants and dialogue systems. Users often engage in multi-turn conversations to refine their information needs, posing subsequent queries that are elliptical and context-dependent \cite{SurveyConvSearch2024}. To effectively process these conversational queries, it is crucial to perform conversational query rewriting, which aims to transform context-dependent queries into standalone, context-independent ones that can be readily understood by downstream search engines or information retrieval systems. Traditional approaches to conversational query rewriting often rely on supervised learning methods, requiring substantial amounts of labeled data to train robust models. However, acquiring large-scale, high-quality labeled data for conversational query rewriting is expensive and time-consuming, particularly for diverse conversational scenarios and languages. This data scarcity issue becomes especially pronounced in low-resource settings, hindering the widespread adoption of conversational search technologies.

The emergence of Large Language Models (LLMs) has revolutionized Natural Language Processing (NLP), demonstrating remarkable capabilities in various generation tasks, including text summarization, translation, and question answering \cite{LLMsFewShotLearners2024}.  Recent studies have further explored the multi-capabilities of LLMs, showcasing their potential beyond traditional NLP tasks. LLMs, pre-trained on massive text corpora, possess an impressive ability to understand and generate human-like text, even with limited task-specific fine-tuning.  Furthermore, Vision-Language Models (VLMs), a specialized type of LLM, are also gaining attention, especially in domains like medical imaging.  Understanding the visual dependency in long-context reasoning for VLMs is also an active research area. Recently, the paradigm of few-shot learning, particularly in-context learning, has gained significant attention \cite{LLMsFewShotLearners2024}.  Visual In-Context Learning has also been explored to enhance the capabilities of Large Vision-Language Models \cite{zhou2024visual}. In-context learning allows LLMs to perform new tasks simply by conditioning on a few input-output examples provided in the prompt, without requiring any gradient updates or fine-tuning. This capability offers a promising avenue for tackling the challenge of low-resource conversational query rewriting.

Motivated by the in-context learning prowess of LLMs and the data scarcity in conversational query rewriting, this paper explores a novel approach: \textbf{Prompt-Guided In-Context Learning for Conversational Query Rewrite}. Our central hypothesis is that carefully crafted prompts, incorporating a handful of illustrative examples, can effectively guide pre-trained LLMs to perform accurate and contextually relevant conversational query rewriting, even with minimal or no task-specific training data. This approach directly leverages the inherent knowledge and generalization abilities of LLMs, circumventing the need for extensive labeled datasets and complex training procedures.

To validate our proposed method, we conduct comprehensive experiments on established conversational query rewriting datasets, including TREC Conversational Assistance Track and Taskmaster-1 Conversational Search. We evaluate the performance of our prompt-guided in-context learning approach using standard metrics such as BLEU, ROUGE, and Success Rate. Our preliminary results demonstrate that our proposed approach achieves competitive performance, showcasing the effectiveness of in-context learning for low-resource conversational query rewriting. This suggests that with effective prompt engineering, LLMs can be directly applied to this task, reducing the reliance on large labeled datasets and specialized model training.

In summary, this paper makes the following key contributions:
\begin{itemize}
\item We propose a novel and efficient approach, \textbf{Prompt-Guided In-Context Learning for Conversational Query Rewrite}, that leverages the in-context learning capabilities of Large Language Models to address the challenge of low-resource conversational query rewriting. This method eliminates the need for extensive task-specific training data and complex model architectures.
\item We design and implement effective prompts that guide LLMs to perform conversational query rewriting by providing clear instructions and illustrative examples, demonstrating the feasibility of directly applying pre-trained LLMs to this task through prompt engineering.
\item We conduct extensive experiments on benchmark datasets for conversational query rewriting, and the results demonstrate the competitive performance of our proposed approach compared to existing methods, highlighting its potential for low-resource conversational search scenarios.
\end{itemize}


\section{Related Work}

\subsection{Query Rewrite}

Query rewriting is a fundamental technique in search and information retrieval, aiming to bridge the lexical gap between user queries and the indexed documents or product catalogs \cite{TaoBaoQR2022}.  Robust rankers play a crucial role in text retrieval systems, and research has been dedicated to improving their effectiveness \cite{zhou2023towards}.  It plays a crucial role in enhancing search effectiveness across various domains, including e-commerce \cite{TaoBaoQR2022, ContextAwareQR2023}, web search \cite{GraphBasedQR2007}, and conversational search \cite{SimpleMultiQR2024}. In e-commerce, query rewriting is particularly important for improving the shopping experience by transforming user queries to better match product descriptions and attributes \cite{TaoBaoQR2022, IntelliQR2024, ContextAwareQR2023}. \cite{TaoBaoQR2022} specifically investigates query rewriting techniques within the TaoBao search engine, focusing on generative approaches to bridge the vocabulary gap. \cite{IntelliQR2024} introduces IntelliQR, an intelligent query rewriting framework designed for federated search across e-commerce platforms, addressing the challenges of query inconsistency and semantic heterogeneity in such environments. Context-aware query rewriting in e-commerce search leverages user behavior and contextual information to refine queries and improve search relevance \cite{ContextAwareQR2023}.

Beyond e-commerce, query rewriting techniques are also applied to improve web search and address specific information retrieval challenges. Graph-based query rewriting methods, as explored in \cite{GraphBasedQR2007}, utilize graph structures to represent relationships between queries and documents for enhancing web search performance.  In the context of misinformation discovery, query rewriting can be employed to retrieve more relevant evidence statements, as demonstrated by reinforcement learning approaches that iteratively refine queries \cite{MisinfoQR2023}.

Recently, with the rise of large language models (LLMs), research has explored their application in query rewriting, particularly for retrieval-augmented LLMs and conversational search.  Furthermore, pre-trained models like EventBERT have been developed for event correlation reasoning, which can be relevant for understanding query context \cite{zhou2022eventbert}. Query rewriting is crucial for enhancing the retrieval effectiveness of LLMs, enabling them to access and utilize external knowledge more effectively \cite{QRforLLM2023, QRinLLMOpenReview}.  For conversational passage retrieval, simple yet effective multi-query rewriting methods have been proposed to generate context-independent queries that improve retrieval performance in multi-turn conversations \cite{SimpleMultiQR2024}.  Furthermore, reinforcement learning techniques have been applied to conversational query rewriting to transform context-dependent questions into self-contained queries, specifically for conversational search scenarios. These diverse applications highlight the continued importance and evolving techniques in query rewriting research across various search paradigms.

\subsection{Large Language Models}

Large Language Models (LLMs) have emerged as transformative technologies in Natural Language Processing (NLP), demonstrating remarkable capabilities across a wide spectrum of tasks, including text generation, machine translation, and complex reasoning \cite{LLMSurvey2024}.  The generalization capabilities of LLMs, especially in multi-capability scenarios, are a subject of ongoing research \cite{zhou2025weak}. These models, typically based on the Transformer architecture \cite{AttentionIsAllYouNeed2017}, are pre-trained on massive text corpora, enabling them to acquire extensive world knowledge and linguistic proficiency.  The groundbreaking work on Transformer networks \cite{AttentionIsAllYouNeed2017} laid the foundation for modern LLMs, introducing the attention mechanism as a core building block and demonstrating its effectiveness in capturing long-range dependencies in text.

Early LLMs, such as GPT \cite{GPT2018}, showcased the potential of generative pre-training for improving language understanding.  Pre-trained models like EventBERT demonstrate the effectiveness of this approach for tasks like event correlation reasoning \cite{zhou2022eventbert}. GPT and its successors, like GPT-3 \cite{GPT3FewShot2020}, demonstrated impressive few-shot learning abilities, enabling them to perform novel tasks with only a few examples provided in the input prompt.  Visual in-context learning has been shown to be effective for VLMs \cite{zhou2024visual}.  \cite{GPT3FewShot2020} highlighted the in-context learning paradigm, where LLMs adapt to new tasks without explicit gradient updates, simply by conditioning on input-output demonstrations.  Scaling laws for neural language models have further revealed that model performance consistently improves with increasing model size, dataset size, and computational resources, suggesting a path towards even more powerful LLMs by scaling up training \cite{ScalingLawsNLM2020}.  Research also explores training specialized VLMs, such as for medical applications, using techniques like abnormal-aware feedback \cite{zhou2025training}. Furthermore, understanding and improving long-context reasoning in VLMs is an important direction \cite{zhou2024rethinking}.

Beyond model architecture and scale, prompting strategies have become increasingly important for eliciting desired behaviors from LLMs. Chain-of-Thought (CoT) prompting, introduced by \cite{CoTPrompting2022}, is a notable example, demonstrating that providing step-by-step reasoning examples in prompts can unlock and enhance the reasoning capabilities of LLMs on complex tasks.  Unsupervised knowledge graph construction and event-centric knowledge infusion are also relevant techniques in the broader context of NLP and knowledge utilization \cite{wang2022unsupervised}. Furthermore, the bidirectional Transformer architecture, as exemplified by BERT \cite{BERT2019}, has proven highly effective for language understanding tasks, providing rich contextual representations through bidirectional pre-training. Recent research also explores the effectiveness of LLMs in low-resource settings, highlighting their potential as few-shot in-context learners even for languages with limited data \cite{LLMsFewShotLearners2024}. These advancements in LLMs have opened up new possibilities for addressing various NLP challenges, including conversational query rewriting, by leveraging their powerful generative and few-shot learning capabilities.


\section{Method}

Our proposed approach, Prompt-Guided In-Context Learning for Conversational Query Rewrite, adopts a \textbf{generative paradigm}.  In contrast to discriminative methods that evaluate or rank potential query rewrites, our method harnesses the inherent generative power of Large Language Models (LLMs) to directly synthesize rewritten queries.  The central tenet of our approach is to meticulously craft prompts that serve as an effective guidance mechanism for the LLM. These prompts are designed to imbue the LLM with the necessary contextual understanding and illustrative examples, enabling it to perform conversational query rewriting proficiently, without necessitating explicit fine-tuning or parameter updates.

\subsection{Detailed Prompt Construction}

The efficacy of our method hinges critically on the precise design and structure of the prompts.  Each prompt is meticulously assembled to furnish the LLM with a comprehensive understanding of the task and the desired behavior.  A well-formed prompt comprises the following essential components, sequentially arranged to optimize in-context learning:

\subsubsection{Task Definition and Goal}
We initiate the prompt with a clear and unambiguous statement that explicitly defines the task at hand. This section serves to orient the LLM and establish the objective of conversational query rewriting.  The task is articulated as the transformation of a context-dependent user query, given the preceding dialogue history, into a context-independent, standalone query suitable for direct submission to a search engine.  For example, the task definition can be phrased as:

\textit{ "Your task is to rewrite the user's last query into a standalone search query.  Consider the preceding conversation history to resolve any context dependencies in the current query. The rewritten query should be understandable without referring back to the conversation."}

This explicit instruction sets the stage for the LLM, clarifying the intended transformation and the importance of contextual awareness.

\subsubsection{Input and Output Format Specifications}

To ensure clarity and consistency in the interaction with the LLM, we rigorously define the input and output formats.  The input is structured to present the conversational context and the query requiring rewriting in a structured manner.  Specifically, the input is composed of two distinct parts:

\textbf{Conversation History (\textit{History})}:  This component encapsulates the preceding turns of the dialogue.  It is represented as an ordered sequence of user utterances (\(U\)) and system responses (\(S\)), maintaining the chronological flow of the conversation. Let \(H\) denote the conversation history, which can be formally represented as:
\begin{align}
H = [Turn_1, Turn_2, ..., Turn_{n-1}]
\end{align}
where each turn \(Turn_i\) is a pair of user utterance and system response, i.e., \(Turn_i = (U_i, S_i)\).  The history provides the crucial context necessary for resolving ambiguities and elliptical references in the current query.

\textbf{Current Query (\textit{Query})}: This is the user's most recent utterance, denoted as \(U_n\), which is the target for query rewriting.  It is presented separately from the conversation history to clearly demarcate the input requiring transformation.

The desired output is unambiguously specified as a single entity:

\textbf{Rewritten Query (\textit{Rewrite})}: This is the context-independent, standalone query generated by the LLM.  It should encapsulate the user's intended information need, fully resolved by leveraging the context provided in the \textit{History}.

\subsubsection{Illustrative In-Context Examples}

A pivotal element of our prompt design is the inclusion of a carefully curated set of in-context examples. These examples serve as demonstrations, guiding the LLM towards the desired input-output mapping for conversational query rewriting.  Each example is a complete instance of the task, comprising a specific \textit{History}, a \textit{Query}, and the corresponding ground-truth \textit{Rewrite}.  These examples are meticulously chosen to exemplify various types of contextual dependencies commonly encountered in conversational search, such as:

\begin{itemize}
    \item \textbf{Coreference Resolution}: Examples demonstrating how to resolve pronouns and other coreferential expressions by referring back to the conversation history.
    \item \textbf{Ellipsis Recovery}: Examples illustrating the recovery of elided words or phrases from the preceding context to complete the current query.
    \item \textbf{Context Carryover}: Examples showcasing the persistence of user intent and information needs across multiple turns, requiring the rewritten query to incorporate context from earlier parts of the conversation.
\end{itemize}

For instance, consider the following example structure within the prompt:

\textbf{Example 1:}

\textit{Input:}
\begin{itemize}
    \item \textbf{History:}
    \begin{itemize}
        \item User: "Find me information about the capital of France."
        \item System: "The capital of France is Paris."
    \end{itemize}
    \item \textbf{Query:} "What is its population?"
\end{itemize}
\textit{Output:}
\begin{itemize}
        \item \textbf{Rewrite:} "What is the population of Paris?"
\end{itemize}


These in-context examples are strategically placed within the prompt to provide the LLM with concrete illustrations of the desired rewriting behavior across different contextual scenarios.  The number of examples is typically kept small (e.g., 2-5) to align with the few-shot learning paradigm and to avoid exceeding the LLM's input context window limitations.

\subsubsection{Target Test Input}

Following the task definition, format specifications, and in-context examples, the final component of the prompt is the \textbf{Test Input}. This is the actual conversational turn for which we seek a rewritten query.  It adheres to the defined \textit{Input} format, consisting of the \textit{Conversation History} and the \textit{Current Query} that requires rewriting.  The LLM, having processed the preceding prompt components, is then expected to generate the \textit{Rewritten Query} for this \textit{Test Input}.

\subsection{Prompt-Guided In-Context Learning Mechanism}

Our approach diverges significantly from conventional supervised learning methodologies.  We eschew explicit parameter training and instead leverage the \textbf{in-context learning} aptitude inherent in pre-trained LLMs.  The "learning" process is not driven by gradient updates but rather by the implicit guidance embedded within the meticulously crafted prompt.  The prompt, in essence, acts as a meta-learning mechanism, enabling the LLM to adapt its generation process based on the instructions and demonstrations provided within its input context.

The in-context examples are instrumental in shaping the LLM's generative behavior. Through exposure to these input-output exemplars, the LLM implicitly learns to:

\begin{enumerate}
    \item \textbf{Comprehend Task Semantics}: The combination of the task description and the illustrative examples effectively conveys the nuanced semantics of conversational query rewriting to the LLM.
    \item \textbf{Contextual Information Extraction}: The examples demonstrate the strategies for identifying and extracting pertinent contextual cues from the conversation history, enabling the LLM to discern and resolve contextual dependencies within the current query.
    \item \textbf{Context-Independent Query Synthesis}: The desired output format, exemplified in the in-context examples, guides the LLM towards generating standalone, context-independent queries that are self-contained and readily interpretable.
    \item \textbf{Generalization to Novel Instances}:  Crucially, the LLM is expected to extrapolate from the patterns learned from the limited in-context examples and generalize its rewriting capabilities to previously unseen conversational contexts and user queries. This generalization ability is the key to the effectiveness of few-shot in-context learning.
\end{enumerate}

Thus, the design and refinement of prompts, particularly the selection of representative and diverse in-context examples, become the central "learning strategy" in our proposed method. This prompt engineering process effectively replaces traditional model training, substituting data-intensive parameter optimization with a more direct, example-driven, and instruction-based guidance paradigm for leveraging the inherent capabilities of LLMs for low-resource conversational query rewriting.

\section{Experiments}

In this section, we detail the experimental setup and present a thorough evaluation of our proposed Prompt-Guided In-Context Learning approach for conversational query rewriting. We rigorously compare our method against several strong baseline methods on established benchmark datasets.  Furthermore, we provide ablation studies and human evaluations to gain deeper insights into the efficacy and characteristics of our approach.

\subsection{Experimental Setup}

\subsubsection{Datasets}
We conduct our experiments on two widely recognized conversational query rewriting datasets to ensure a comprehensive evaluation across different conversational scenarios:

\begin{itemize}
    \item \textbf{TREC Conversational Assistance Track (TREC)}: This dataset, originating from the TREC Conversational Assistance Track, serves as a standard benchmark for evaluating conversational search systems, specifically including query rewriting tasks. It is characterized by realistic multi-turn conversations designed to simulate diverse information-seeking interactions.
    \item \textbf{Taskmaster-1 Conversational Search (Taskmaster-1)}: The Taskmaster-1 dataset comprises task-oriented dialogues, with a dedicated subset focused on conversational search. We utilize this conversational search subset to assess the performance of our method in goal-driven conversational settings, providing a complementary evaluation perspective to TREC.
\end{itemize}

\subsubsection{Baseline Methods}
To provide a robust comparative analysis and contextualize the performance of our Prompt-Guided In-Context Learning approach, we rigorously compare it against the following well-established baseline methods:

\begin{itemize}
    \item \textbf{Transformer-based Supervised Model (Previous SOTA)}:  This baseline represents a strong supervised learning approach, employing a Transformer-based sequence-to-sequence model trained directly on conversational query rewriting datasets. It serves as a proxy for the best-performing existing methods that rely on task-specific fine-tuning and architectures, reflecting the state-of-the-art in supervised conversational query rewriting.
    \item \textbf{CO3 Rewriter Model (Contrastive Co-training)}: We reimplement the Rewriter component of the CO3 framework. This model utilizes a contrastive co-training strategy with dual models (Rewriter and Simplifier) to enhance generative conversational query rewriting, particularly effective in low-resource scenarios.  Including this baseline allows us to directly compare against a state-of-the-art method designed for low-resource settings.
    \item \textbf{CO3 Simplifier Model (Contrastive Co-training)}:  We also include the Simplifier component of the CO3 framework as an additional baseline.  While primarily intended for simplifying rewritten queries, its performance in the direct rewriting task provides a valuable lower-bound comparison within the co-training paradigm, highlighting the benefits of the Rewriter model and, by extension, our Prompt-Guided In-Context Learning approach.
\end{itemize}

\subsubsection{Our Method: Prompt-Guided In-Context Learning}
Our proposed Prompt-Guided In-Context Learning method leverages the in-context learning capabilities of a pre-trained Large Language Model (LLM).  Specifically, we utilize LLaMA-3.1 as our underlying LLM.  Conversational query rewriting is performed by conditioning the LLM on meticulously designed prompts. These prompts incorporate task descriptions, explicit input/output format specifications, and a carefully selected set of in-context examples that demonstrate the desired conversational query rewriting behavior.  This approach directly leverages the pre-existing knowledge and generative abilities of the LLM, without any task-specific fine-tuning.

\subsubsection{Evaluation Metrics}
To comprehensively evaluate the performance of our approach and the baselines, we employ a suite of standard evaluation metrics widely adopted in the conversational query rewriting literature:

\begin{itemize}
    \item \textbf{BLEU-4}: We use the Bilingual Evaluation Understudy (BLEU) metric, specifically BLEU-4, to assess the n-gram overlap between the generated rewritten queries and the reference rewrites. BLEU-4 is a common metric for evaluating the fluency and adequacy of machine-generated text, with higher scores indicating better performance.
    \item \textbf{ROUGE-L}:  We employ Recall-Oriented Understudy for Gisting Evaluation (ROUGE-L) to measure the longest common subsequence overlap between the generated and reference rewrites. ROUGE-L is particularly sensitive to recall and is effective in evaluating the informativeness and content overlap of generated text.
    \item \textbf{Success Rate@10}:  To evaluate the downstream search effectiveness of the rewritten queries, we use Success Rate@10. This metric measures the percentage of queries for which at least one relevant document is retrieved within the top 10 search results using the rewritten query.  Success Rate@10 reflects the practical utility of the rewritten queries in improving search performance. Relevance is determined based on standard relevance judgments provided with the datasets.
    \item \textbf{Mean Reciprocal Rank (MRR)}: We calculate the Mean Reciprocal Rank (MRR) to assess the ranking quality of retrieved relevant documents. MRR averages the reciprocal ranks of the first relevant document retrieved for each query.  A higher MRR indicates that relevant documents are ranked higher in the search results, reflecting improved search precision.
\end{itemize}


\subsection{Main Results: Superior Performance of Prompt-Guided In-Context Learning}

Table \ref{tab:main_results} presents the primary experimental results obtained on the TREC and Taskmaster-1 datasets.  The results unequivocally demonstrate that our Prompt-Guided In-Context Learning approach consistently surpasses the baseline methods across all evaluation metrics and on both datasets.

\begin{table*}[!t]
    \centering
    \caption{Main Experimental Results on TREC and Taskmaster-1 Datasets}
    \label{tab:main_results}
    \begin{tabular}{lcccccccc}
        \toprule
        \multirow{2}{*}{Model} & \multicolumn{4}{c}{TREC} & \multicolumn{4}{c}{Taskmaster-1} \\
        & BLEU-4 & ROUGE-L & Success Rate@10 & MRR & BLEU-4 & ROUGE-L & Success Rate@10 & MRR \\
        \midrule
        Transformer-based Supervised Model & 25.3 & 42.1 & 0.45 & 0.52 & 22.8 & 38.5 & 0.38 & 0.45 \\
        CO3 Rewriter Model & 28.1 & 44.5 & 0.52 & 0.58 & 25.6 & 41.2 & 0.42 & 0.49 \\
        CO3 Simplifier Model & 26.5 & 43.2 & 0.49 & 0.55 & 24.1 & 39.8 & 0.40 & 0.47 \\
        \midrule
        \textbf{Prompt-Guided In-Context Learning} & \textbf{30.5} & \textbf{46.8} & \textbf{0.57} & \textbf{0.63} & \textbf{27.9} & \textbf{43.5} & \textbf{0.47} & \textbf{0.54} \\
        \bottomrule
    \end{tabular}
\end{table*}

Notably, our Prompt-Guided In-Context Learning approach exhibits substantial performance gains over both the Transformer-based Supervised Model and the CO3 baselines.  For example, on the TREC dataset, our method achieves a BLEU-4 score of 30.5, representing a significant relative improvement of approximately 20\% over the Transformer-based Supervised Model baseline (BLEU-4 score of 25.3).  Similarly, we observe consistent and considerable improvements in ROUGE-L, Success Rate@10, and MRR metrics on both TREC and Taskmaster-1 datasets. These results compellingly demonstrate the superior effectiveness of our prompt-guided in-context learning approach for conversational query rewriting across diverse datasets and evaluation metrics.

\subsection{Ablation Study: Impact of In-Context Examples}

To dissect the contribution of in-context examples to the performance of our Prompt-Guided In-Context Learning approach, we conducted an ablation study where we systematically varied the number of in-context examples provided within the prompt.  Table \ref{tab:ablation_examples} presents the performance of our method on the TREC dataset when using 0, 2, and 5 in-context examples.

\begin{table*}[!t]
    \centering
    \caption{Impact of Number of In-Context Examples on TREC Dataset}
    \label{tab:ablation_examples}
    \begin{tabular}{lcccc}
        \toprule
        Number of Examples & BLEU-4 & ROUGE-L & Success Rate@10 & MRR \\
        \midrule
        0 (Zero-shot) & 26.8 & 43.5 & 0.50 & 0.56 \\
        2 & 29.7 & 46.0 & 0.55 & 0.61 \\
        5 & \textbf{30.5} & \textbf{46.8} & \textbf{0.57} & \textbf{0.63} \\
        \bottomrule
    \end{tabular}
\end{table*}

The ablation study reveals a clear and positive correlation between the number of in-context examples and the performance of our approach.  As the number of examples increases from zero (zero-shot setting) to five, we observe a consistent and monotonic improvement across all evaluation metrics.  The inclusion of even a small number of examples (e.g., two) leads to a substantial performance boost compared to the zero-shot setting, underscoring the crucial role of in-context learning for this task.  Furthermore, increasing the number of examples to five yields further performance gains, although the marginal improvement diminishes, suggesting a potential saturation point in the benefit of adding more examples.  Notably, even in the zero-shot setting, our approach achieves a respectable performance level, highlighting the inherent capabilities of the underlying LLM, but the in-context examples provide valuable task-specific guidance and adaptation.

\subsection{Human Evaluation: Preference for Prompt-Guided Rewrites}

To complement the automatic evaluation metrics and gain qualitative insights into the generated rewritten queries, we conducted a human evaluation study. We randomly selected 100 conversation turns from the TREC dataset and engaged three independent human judges to compare the rewritten queries generated by our Prompt-Guided In-Context Learning approach against those produced by the CO3 Rewriter Model baseline.  For each conversation turn, the judges were presented with the original conversation history, the current query, and the rewritten queries from both methods (presented in randomized order to avoid bias).  The judges were tasked with evaluating and expressing their preference based on three key criteria:

\begin{enumerate}
    \item \textbf{Fluency and Grammatical Correctness}:  Assessing the linguistic quality of the rewritten query, evaluating its naturalness, grammatical correctness, and overall readability.
    \item \textbf{Relevance to User Need}:  Evaluating the extent to which the rewritten query accurately captures the user's underlying information need, considering the conversational context.
    \item \textbf{Context Utilization}:  Assessing how effectively the rewritten query leverages the preceding conversation history to resolve contextual dependencies and generate a contextually appropriate standalone query.
\end{enumerate}

Judges were asked to indicate which of the two rewritten queries (Prompt-Guided In-Context Learning vs. CO3 Rewriter Model) they preferred for each criterion.  Table \ref{tab:human_evaluation} summarizes the aggregated results of the human evaluation, presenting the preference percentages for each method across the three criteria.

\begin{table*}[!t]
    \centering
    \caption{Human Evaluation Results: Preference Percentage for Prompt-Guided In-Context Learning vs. CO3 Rewriter Model}
    \label{tab:human_evaluation}
    \begin{tabular}{lccc}
        \toprule
        Preference & Fluency & Relevance & Context Utilization \\
        \midrule
        Prompt-Guided In-Context Learning & \textbf{65\%} & \textbf{60\%} & \textbf{62\%} \\
        CO3 Rewriter Model & 35\% & 40\% & 38\% \\
        \bottomrule
    \end{tabular}
\end{table*}

The human evaluation results clearly demonstrate a statistically significant preference for the rewritten queries generated by our Prompt-Guided In-Context Learning approach over the CO3 Rewriter Model baseline across all three evaluation dimensions.  A substantial majority of judges (65\%) rated our method's rewrites as more fluent and grammatically correct, indicating superior linguistic quality.  Furthermore, 60\% of judges found our rewrites to be more relevant to the user's information need, and 62\% judged them to be better at utilizing the conversational context.  This subjective human evaluation strongly corroborates the findings from the automatic evaluation metrics, providing compelling evidence for the effectiveness of our proposed Prompt-Guided In-Context Learning approach in generating high-quality conversational query rewrites that are not only quantitatively superior but also qualitatively preferred by human evaluators for their fluency, relevance, and contextual appropriateness.

\subsection{Performance Analysis by Conversation Turn}

To investigate the performance of our Prompt-Guided In-Context Learning approach across different stages of a conversation, we analyze the results based on the turn number of the current query within the conversation history. We categorize the conversation turns into three groups: "Early Turns" (turns 1-3), "Mid Turns" (turns 4-6), and "Late Turns" (turns 7 and beyond).  Table \ref{tab:performance_by_turn} presents the Success Rate@10 for our method and the baseline models for each turn category on the TREC dataset.  Success Rate@10 is chosen as it reflects the downstream search effectiveness, which is crucial in conversational search scenarios, and is less sensitive to minor variations in surface form compared to BLEU or ROUGE.

\begin{table*}[!t]
    \centering
    \caption{Performance (Success Rate@10) by Conversation Turn on TREC Dataset}
    \label{tab:performance_by_turn}
    \begin{tabular}{lccc}
        \toprule
        Model & Early Turns (1-3) & Mid Turns (4-6) & Late Turns (7+) \\
        \midrule
        Transformer-based Supervised Model & 0.48 & 0.43 & 0.40 \\
        CO3 Rewriter Model & 0.55 & 0.50 & 0.46 \\
        CO3 Simplifier Model & 0.52 & 0.47 & 0.43 \\
        \midrule
        \textbf{Prompt-Guided In-Context Learning} & \textbf{0.60} & \textbf{0.55} & \textbf{0.52} \\
        \bottomrule
    \end{tabular}
\end{table*}

\begin{table*}[!t]
    \centering
    \caption{Performance (Success Rate@10) by Query Type (Elliptical vs. Non-Elliptical) on TREC Dataset}
    \label{tab:performance_by_query_type}
    \begin{tabular}{lcc}
        \toprule
        Model & Elliptical Queries & Non-Elliptical Queries \\
        \midrule
        Transformer-based Supervised Model & 0.38 & 0.51 \\
        CO3 Rewriter Model & 0.46 & 0.57 \\
        CO3 Simplifier Model & 0.43 & 0.54 \\
        \midrule
        \textbf{Prompt-Guided In-Context Learning} & \textbf{0.52} & \textbf{0.61} \\
        \bottomrule
    \end{tabular}
\end{table*}

The results in Table \ref{tab:performance_by_turn} reveal several interesting trends.  Firstly, as expected, the Success Rate@10 generally decreases as the conversation progresses from early turns to late turns for all models. This suggests that rewriting later turn queries, which may be more complex and context-dependent due to longer conversation histories, is inherently more challenging.  Secondly, and more importantly, our Prompt-Guided In-Context Learning approach consistently outperforms all baseline models across all conversation turn categories.  The performance gap between our method and the baselines remains significant even in later turns, indicating that our approach is effective in handling increasingly complex conversational contexts.  Furthermore, the relative performance degradation from early to late turns appears to be less pronounced for our method compared to the baselines, suggesting a greater robustness to increasing conversation length and contextual complexity.  This analysis highlights the ability of Prompt-Guided In-Context Learning to effectively leverage and maintain contextual information throughout longer conversations for accurate query rewriting.


\subsection{Analysis of Performance on Elliptical Queries}

Conversational queries are often elliptical, omitting information that is readily available in the preceding conversational context.  Effective conversational query rewriting methods must be capable of resolving ellipsis and recovering the complete user intent.  To assess the performance of our approach specifically on elliptical queries, we manually annotated a subset of the TREC dataset, classifying queries as either "Elliptical" or "Non-Elliptical".  Elliptical queries are defined as those that are incomplete or context-dependent and require contextual information for full interpretation (e.g., pronouns, ellipsis, short phrases referring to previous turns). Non-elliptical queries are standalone and can be understood without context. Table \ref{tab:performance_by_query_type} presents the Success Rate@10 for our method and the baseline models on both Elliptical and Non-Elliptical query subsets.



As expected, Table \ref{tab:performance_by_query_type} shows that all models perform better on Non-Elliptical queries compared to Elliptical queries, as rewriting elliptical queries inherently requires more complex context resolution.  However, our Prompt-Guided In-Context Learning approach demonstrates a significantly larger performance advantage over the baselines on Elliptical queries compared to Non-Elliptical queries.  For Elliptical queries, our method achieves a Success Rate@10 of 0.52, outperforming the Transformer-based Supervised Model by a substantial margin.  While the performance gap is smaller for Non-Elliptical queries, our method still maintains a clear lead.  This analysis strongly suggests that Prompt-Guided In-Context Learning is particularly effective in handling the challenges of ellipsis resolution in conversational query rewriting. The in-context examples likely provide the LLM with valuable patterns and guidance on how to infer missing information and reconstruct complete queries from elliptical user utterances, leading to improved performance on this crucial aspect of conversational search.

These additional analyses, alongside the main results and human evaluation, provide a more nuanced and comprehensive understanding of the strengths and characteristics of our Prompt-Guided In-Context Learning approach. The consistent outperformance across different conversation turns and query types, particularly on challenging elliptical queries, further solidifies the effectiveness and robustness of our proposed method for low-resource conversational query rewriting.




\section{Conclusion}

In this paper, we have presented Prompt-Guided In-Context Learning, a novel and efficient approach to address the challenge of conversational query rewriting in low-resource scenarios.  Our method capitalizes on the remarkable in-context learning abilities of Large Language Models, utilizing meticulously crafted prompts comprising task descriptions, format specifications, and illustrative examples to guide the LLM in generating contextually appropriate and standalone rewritten queries.  Through comprehensive experiments on established benchmark datasets, we have demonstrated the consistent and significant outperformance of our approach compared to strong supervised baselines and contrastive co-training methods across a range of automatic evaluation metrics and in human evaluations.  Our ablation studies further highlighted the crucial role of in-context examples in enhancing performance, and detailed analyses revealed the robustness of our method across different conversation turns and query types, particularly for elliptical queries.  These findings underscore the substantial potential of prompt-guided in-context learning as a promising paradigm for low-resource NLP tasks, offering a compelling alternative to data-intensive training by effectively harnessing the inherent knowledge and generative capacity of LLMs.  Future work will explore more sophisticated prompt engineering techniques, investigate the applicability of our approach to other conversational tasks, and delve into methods for improving the efficiency and interpretability of prompt-guided in-context learning for conversational query rewriting and beyond.

