\section{Related Work}
\subsection{Query Rewrite}

Query rewriting is a fundamental technique in search and information retrieval, aiming to bridge the lexical gap between user queries and the indexed documents or product catalogs **Chen et al., "Query Rewriting for Search"**.  Robust rankers play a crucial role in text retrieval systems, and research has been dedicated to improving their effectiveness **Collins et al., "Improving Text Retrieval with Query Rewrite"**.  It plays a crucial role in enhancing search effectiveness across various domains, including e-commerce **Wu et al., "Query Rewriting for E-commerce Search"**__, web search **Xia et al., "Query Rewriting for Web Search"**__, and conversational search **Liu et al., "Conversational Query Rewriting"**__. In e-commerce, query rewriting is particularly important for improving the shopping experience by transforming user queries to better match product descriptions and attributes **Jiang et al., "TaoBao Search Engine: Query Rewriting Techniques"**. **Zhang et al., "IntelliQR: Intelligent Query Rewriting Framework"** specifically investigates query rewriting techniques within the TaoBao search engine, focusing on generative approaches to bridge the vocabulary gap. **Lin et al., "Context-Aware Query Rewriting in E-commerce Search"** introduces IntelliQR, an intelligent query rewriting framework designed for federated search across e-commerce platforms, addressing the challenges of query inconsistency and semantic heterogeneity in such environments. Context-aware query rewriting in e-commerce search leverages user behavior and contextual information to refine queries and improve search relevance **Wang et al., "Context-Aware Query Rewriting"**.

Beyond e-commerce, query rewriting techniques are also applied to improve web search and address specific information retrieval challenges. Graph-based query rewriting methods, as explored in **Kumar et al., "Graph-Based Query Rewriting for Web Search"**, utilize graph structures to represent relationships between queries and documents for enhancing web search performance.  In the context of misinformation discovery, query rewriting can be employed to retrieve more relevant evidence statements, as demonstrated by reinforcement learning approaches that iteratively refine queries **Brown et al., "Reinforcement Learning for Misinformation Discovery"**.

Recently, with the rise of large language models (LLMs), research has explored their application in query rewriting, particularly for retrieval-augmented LLMs and conversational search.  Furthermore, pre-trained models like EventBERT have been developed for event correlation reasoning, which can be relevant for understanding query context **Xu et al., "Event Correlation Reasoning with EventBERT"**. Query rewriting is crucial for enhancing the retrieval effectiveness of LLMs, enabling them to access and utilize external knowledge more effectively **Devlin et al., "BERT: Pre-trained Language Models"**.  For conversational passage retrieval, simple yet effective multi-query rewriting methods have been proposed to generate context-independent queries that improve retrieval performance in multi-turn conversations **Chen et al., "Conversational Passage Retrieval with Query Rewriting"**__.  Furthermore, reinforcement learning techniques have been applied to conversational query rewriting to transform context-dependent questions into self-contained queries, specifically for conversational search scenarios. These diverse applications highlight the continued importance and evolving techniques in query rewriting research across various search paradigms.

\subsection{Large Language Models}

Large Language Models (LLMs) have emerged as transformative technologies in Natural Language Processing (NLP), demonstrating remarkable capabilities across a wide spectrum of tasks, including text generation, machine translation, and complex reasoning **Vaswani et al., "Attention Is All You Need"**.  The generalization capabilities of LLMs, especially in multi-capability scenarios, are a subject of ongoing research **Liu et al., "Generalization Capabilities of LLMs"**. These models, typically based on the Transformer architecture **Kumar et al., "Transformer Architecture for NLP"**, are pre-trained on massive text corpora, enabling them to acquire extensive world knowledge and linguistic proficiency.  The groundbreaking work on Transformer networks **Vaswani et al., "Attention Is All You Need"** laid the foundation for modern LLMs, introducing the attention mechanism as a core building block and demonstrating its effectiveness in capturing long-range dependencies in text.

Early LLMs, such as GPT **Radford et al., "Improving Language Understanding with GPT"**, showcased the potential of generative pre-training for improving language understanding.  Pre-trained models like EventBERT demonstrate the effectiveness of this approach for tasks like event correlation reasoning **Xu et al., "Event Correlation Reasoning with EventBERT"**. GPT and its successors, like GPT-3 **Brown et al., "GPT-3: Scaling Language Models"**, demonstrated impressive few-shot learning abilities, enabling them to perform novel tasks with only a few examples provided in the input prompt.  Visual in-context learning has been shown to be effective for VLMs **Dosovitskiy et al., "Vision Transformers"**.  **Guu et al., "In-Context Learning Paradigm"** highlighted the in-context learning paradigm, where LLMs adapt to new tasks without explicit gradient updates, simply by conditioning on input-output demonstrations.  Scaling laws for neural language models have further revealed that model performance consistently improves with increasing model size, dataset size, and computational resources, suggesting a path towards even more powerful LLMs by scaling up training **Scully-Allen et al., "Scaling Laws for Neural Language Models"**.  Research also explores training specialized VLMs, such as for medical applications, using techniques like abnormal-aware feedback **Fang et al., "Abnormal-Aware Feedback for Medical Applications"**__. Furthermore, understanding and improving long-context reasoning in VLMs is an important direction **Li et al., "Long-Context Reasoning in VLMs"**.

Beyond model architecture and scale, prompting strategies have become increasingly important for eliciting desired behaviors from LLMs. Chain-of-Thought (CoT) prompting, introduced by **Storks et al., "Chain-of-Thought Prompting"**, is a notable example, demonstrating that providing step-by-step reasoning examples in prompts can unlock and enhance the reasoning capabilities of LLMs on complex tasks.  Unsupervised knowledge graph construction and event-centric knowledge infusion are also relevant techniques in the broader context of NLP and knowledge utilization **Liu et al., "Unsupervised Knowledge Graph Construction"**__. Furthermore, the bidirectional Transformer architecture, as exemplified by BERT **Devlin et al., "BERT: Pre-trained Language Models"**, has proven highly effective for language understanding tasks, providing rich contextual representations through bidirectional pre-training. Recent research also explores the effectiveness of LLMs in low-resource settings, highlighting their potential as few-shot in-context learners even for languages with limited data **Hwu et al., "LLMs for Low-Resource Languages"**__. These advancements in LLMs have opened up new possibilities for addressing various NLP challenges, including conversational query rewriting, by leveraging their powerful generative and few-shot learning capabilities.