\section{Related Work}
\subsection{Query Rewrite}

Query rewriting is a fundamental technique in search and information retrieval, aiming to bridge the lexical gap between user queries and the indexed documents or product catalogs ____.  Robust rankers play a crucial role in text retrieval systems, and research has been dedicated to improving their effectiveness ____.  It plays a crucial role in enhancing search effectiveness across various domains, including e-commerce ____, web search ____, and conversational search ____. In e-commerce, query rewriting is particularly important for improving the shopping experience by transforming user queries to better match product descriptions and attributes ____. ____ specifically investigates query rewriting techniques within the TaoBao search engine, focusing on generative approaches to bridge the vocabulary gap. ____ introduces IntelliQR, an intelligent query rewriting framework designed for federated search across e-commerce platforms, addressing the challenges of query inconsistency and semantic heterogeneity in such environments. Context-aware query rewriting in e-commerce search leverages user behavior and contextual information to refine queries and improve search relevance ____.

Beyond e-commerce, query rewriting techniques are also applied to improve web search and address specific information retrieval challenges. Graph-based query rewriting methods, as explored in ____, utilize graph structures to represent relationships between queries and documents for enhancing web search performance.  In the context of misinformation discovery, query rewriting can be employed to retrieve more relevant evidence statements, as demonstrated by reinforcement learning approaches that iteratively refine queries ____.

Recently, with the rise of large language models (LLMs), research has explored their application in query rewriting, particularly for retrieval-augmented LLMs and conversational search.  Furthermore, pre-trained models like EventBERT have been developed for event correlation reasoning, which can be relevant for understanding query context ____. Query rewriting is crucial for enhancing the retrieval effectiveness of LLMs, enabling them to access and utilize external knowledge more effectively ____.  For conversational passage retrieval, simple yet effective multi-query rewriting methods have been proposed to generate context-independent queries that improve retrieval performance in multi-turn conversations ____.  Furthermore, reinforcement learning techniques have been applied to conversational query rewriting to transform context-dependent questions into self-contained queries, specifically for conversational search scenarios. These diverse applications highlight the continued importance and evolving techniques in query rewriting research across various search paradigms.

\subsection{Large Language Models}

Large Language Models (LLMs) have emerged as transformative technologies in Natural Language Processing (NLP), demonstrating remarkable capabilities across a wide spectrum of tasks, including text generation, machine translation, and complex reasoning ____.  The generalization capabilities of LLMs, especially in multi-capability scenarios, are a subject of ongoing research ____. These models, typically based on the Transformer architecture ____, are pre-trained on massive text corpora, enabling them to acquire extensive world knowledge and linguistic proficiency.  The groundbreaking work on Transformer networks ____ laid the foundation for modern LLMs, introducing the attention mechanism as a core building block and demonstrating its effectiveness in capturing long-range dependencies in text.

Early LLMs, such as GPT ____, showcased the potential of generative pre-training for improving language understanding.  Pre-trained models like EventBERT demonstrate the effectiveness of this approach for tasks like event correlation reasoning ____. GPT and its successors, like GPT-3 ____, demonstrated impressive few-shot learning abilities, enabling them to perform novel tasks with only a few examples provided in the input prompt.  Visual in-context learning has been shown to be effective for VLMs ____.  ____ highlighted the in-context learning paradigm, where LLMs adapt to new tasks without explicit gradient updates, simply by conditioning on input-output demonstrations.  Scaling laws for neural language models have further revealed that model performance consistently improves with increasing model size, dataset size, and computational resources, suggesting a path towards even more powerful LLMs by scaling up training ____.  Research also explores training specialized VLMs, such as for medical applications, using techniques like abnormal-aware feedback ____. Furthermore, understanding and improving long-context reasoning in VLMs is an important direction ____.

Beyond model architecture and scale, prompting strategies have become increasingly important for eliciting desired behaviors from LLMs. Chain-of-Thought (CoT) prompting, introduced by ____, is a notable example, demonstrating that providing step-by-step reasoning examples in prompts can unlock and enhance the reasoning capabilities of LLMs on complex tasks.  Unsupervised knowledge graph construction and event-centric knowledge infusion are also relevant techniques in the broader context of NLP and knowledge utilization ____. Furthermore, the bidirectional Transformer architecture, as exemplified by BERT ____, has proven highly effective for language understanding tasks, providing rich contextual representations through bidirectional pre-training. Recent research also explores the effectiveness of LLMs in low-resource settings, highlighting their potential as few-shot in-context learners even for languages with limited data ____. These advancements in LLMs have opened up new possibilities for addressing various NLP challenges, including conversational query rewriting, by leveraging their powerful generative and few-shot learning capabilities.