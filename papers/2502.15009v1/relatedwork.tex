\section{Related Work}
\subsection{Query Rewrite}

Query rewriting is a fundamental technique in search and information retrieval, aiming to bridge the lexical gap between user queries and the indexed documents or product catalogs \cite{TaoBaoQR2022}.  Robust rankers play a crucial role in text retrieval systems, and research has been dedicated to improving their effectiveness \cite{zhou2023towards}.  It plays a crucial role in enhancing search effectiveness across various domains, including e-commerce \cite{TaoBaoQR2022, ContextAwareQR2023}, web search \cite{GraphBasedQR2007}, and conversational search \cite{SimpleMultiQR2024}. In e-commerce, query rewriting is particularly important for improving the shopping experience by transforming user queries to better match product descriptions and attributes \cite{TaoBaoQR2022, IntelliQR2024, ContextAwareQR2023}. \cite{TaoBaoQR2022} specifically investigates query rewriting techniques within the TaoBao search engine, focusing on generative approaches to bridge the vocabulary gap. \cite{IntelliQR2024} introduces IntelliQR, an intelligent query rewriting framework designed for federated search across e-commerce platforms, addressing the challenges of query inconsistency and semantic heterogeneity in such environments. Context-aware query rewriting in e-commerce search leverages user behavior and contextual information to refine queries and improve search relevance \cite{ContextAwareQR2023}.

Beyond e-commerce, query rewriting techniques are also applied to improve web search and address specific information retrieval challenges. Graph-based query rewriting methods, as explored in \cite{GraphBasedQR2007}, utilize graph structures to represent relationships between queries and documents for enhancing web search performance.  In the context of misinformation discovery, query rewriting can be employed to retrieve more relevant evidence statements, as demonstrated by reinforcement learning approaches that iteratively refine queries \cite{MisinfoQR2023}.

Recently, with the rise of large language models (LLMs), research has explored their application in query rewriting, particularly for retrieval-augmented LLMs and conversational search.  Furthermore, pre-trained models like EventBERT have been developed for event correlation reasoning, which can be relevant for understanding query context \cite{zhou2022eventbert}. Query rewriting is crucial for enhancing the retrieval effectiveness of LLMs, enabling them to access and utilize external knowledge more effectively \cite{QRforLLM2023, QRinLLMOpenReview}.  For conversational passage retrieval, simple yet effective multi-query rewriting methods have been proposed to generate context-independent queries that improve retrieval performance in multi-turn conversations \cite{SimpleMultiQR2024}.  Furthermore, reinforcement learning techniques have been applied to conversational query rewriting to transform context-dependent questions into self-contained queries, specifically for conversational search scenarios. These diverse applications highlight the continued importance and evolving techniques in query rewriting research across various search paradigms.

\subsection{Large Language Models}

Large Language Models (LLMs) have emerged as transformative technologies in Natural Language Processing (NLP), demonstrating remarkable capabilities across a wide spectrum of tasks, including text generation, machine translation, and complex reasoning \cite{LLMSurvey2024}.  The generalization capabilities of LLMs, especially in multi-capability scenarios, are a subject of ongoing research \cite{zhou2025weak}. These models, typically based on the Transformer architecture \cite{AttentionIsAllYouNeed2017}, are pre-trained on massive text corpora, enabling them to acquire extensive world knowledge and linguistic proficiency.  The groundbreaking work on Transformer networks \cite{AttentionIsAllYouNeed2017} laid the foundation for modern LLMs, introducing the attention mechanism as a core building block and demonstrating its effectiveness in capturing long-range dependencies in text.

Early LLMs, such as GPT \cite{GPT2018}, showcased the potential of generative pre-training for improving language understanding.  Pre-trained models like EventBERT demonstrate the effectiveness of this approach for tasks like event correlation reasoning \cite{zhou2022eventbert}. GPT and its successors, like GPT-3 \cite{GPT3FewShot2020}, demonstrated impressive few-shot learning abilities, enabling them to perform novel tasks with only a few examples provided in the input prompt.  Visual in-context learning has been shown to be effective for VLMs \cite{zhou2024visual}.  \cite{GPT3FewShot2020} highlighted the in-context learning paradigm, where LLMs adapt to new tasks without explicit gradient updates, simply by conditioning on input-output demonstrations.  Scaling laws for neural language models have further revealed that model performance consistently improves with increasing model size, dataset size, and computational resources, suggesting a path towards even more powerful LLMs by scaling up training \cite{ScalingLawsNLM2020}.  Research also explores training specialized VLMs, such as for medical applications, using techniques like abnormal-aware feedback \cite{zhou2025training}. Furthermore, understanding and improving long-context reasoning in VLMs is an important direction \cite{zhou2024rethinking}.

Beyond model architecture and scale, prompting strategies have become increasingly important for eliciting desired behaviors from LLMs. Chain-of-Thought (CoT) prompting, introduced by \cite{CoTPrompting2022}, is a notable example, demonstrating that providing step-by-step reasoning examples in prompts can unlock and enhance the reasoning capabilities of LLMs on complex tasks.  Unsupervised knowledge graph construction and event-centric knowledge infusion are also relevant techniques in the broader context of NLP and knowledge utilization \cite{wang2022unsupervised}. Furthermore, the bidirectional Transformer architecture, as exemplified by BERT \cite{BERT2019}, has proven highly effective for language understanding tasks, providing rich contextual representations through bidirectional pre-training. Recent research also explores the effectiveness of LLMs in low-resource settings, highlighting their potential as few-shot in-context learners even for languages with limited data \cite{LLMsFewShotLearners2024}. These advancements in LLMs have opened up new possibilities for addressing various NLP challenges, including conversational query rewriting, by leveraging their powerful generative and few-shot learning capabilities.