\section{Proofs}\label{sec:appendix:proof}

\subsection{Proof of \cref{thm:completeness}}
\begin{lemma}\label{lemma:appendix:mapping}
Let $\textsc{Partition}(e, G)$ be the index partition formed by $e$ in G. If for each pair of edges $e_1, e_2$ in a tree tensor network $G$, $\textsc{Partition}(e_1,G) \not= \textsc{Partition}(e_2, G)$, then there exists a program $P = \osplit(\ind_1, r_1); \ldots; \osplit(\ind_n, r_n)$ such that $\sem{P}(G_0, \error) = (G, \error)$ where $G_0 = (\{\net{G}\}, \emptyset)$.
\end{lemma}
\begin{proof}
For each edge $e \in \edges$ of $G = (\nodes, \edges)$ where $e = (v_1, v_2, r_e)$ and $e$ partitions the free indices into $\ind_{e}$ and the remaining ones, we create a split operation $\osplit(\ind_{e}, r_e)$. By this construction, we next show that
if $P = \osplit(\ind_{e_1}, r_{e_1}); \osplit(\ind_{e_2}, r_{e_2}); \ldots \osplit(\ind_{e_{\size{\edges}}}, r_{e_{\size{\edges}}})$, then
$\sem{P}(G_0, \error) = (G, \error)$ where $G_0 = (\{\net{G}\}, \emptyset)$.

Each edge in $G$ corresponds to a contraction operation, and the data tensor $\net{G}$ can be achieved by contracting all edges in $G$.
%
Reversing the order all contractions and inverting each contraction into a split is equivalent to execution of the program $P$, which brings us back from $\net{G}$ to $G$ without any information loss.
%
Therefore, the constructed $P$ satisfies $\sem{P}(G_0, \error) = (G, \error)$ where $G_0 = (\{\net{G}\}, \emptyset)$.
\end{proof}

\begin{theorem}[Completeness of Output-Directed Splits]
If $G$ is the optimal tree tensor network for a tensor $\ten{T}$ and an error bound $\error$,
then there exists a program with output-directed splits $P$ that $\sem{P}(G_0, \error) = G$ where $G_0 = (\{\ten{T}\},\emptyset)$.
\end{theorem}
\begin{proof}
By \cref{lemma:appendix:mapping}, we only need to show that each pair of edges $e_1, e_2$ in an optimal tree tensor network satisfies $\textsc{Partition}(e_1,G) \not= \textsc{Partition}(e_2, G)$, and we prove this by contradiction.

Suppose there exist two edges $e_1$ and $e_2$ in an optimal tensor network $G$ where $e_1$ and $e_2$ correspond to the free index partition $\ind_s$, and other edges form unique partitions different from $\ind_s$.

First, let us prove that $e_1$ and $e_2$ share at least one vertex.
%
If $e_1 = (u_1, v_1, r_1)$ and $e_2 = (u_2, v_2, r_2)$ do not share any vertex, then by the connectivity property of trees, we have that at least one of the pairs in $\{u_1, v_1\} \times \{u_2, v_2\}$ are connected by a path $\pi$.
%
There is no free index attached to any nodes on $\pi$, otherwise $e_1$ and $e_2$ form different index partition.
%
Also, any edge on $\pi$ forms the same index partition as $e_1$ and $e_2$, which contradicts with the setting that only $e_1$ and $e_2$ correspond to the same free index partition.

Next, let us show that the network containing $e_1$ and $e_2$ are not optimal.
%
Without loss of generality, let us assume that $e_1 = (u, v, r_1)$, $e_2 = (v, w, r_2)$, and $r_1 < r_2$.
%
In this case, merging $v$ and $w$ and removing $e_2$ gives us a tensor network with a smaller size.
%
Therefore, $G$ is not an optimal tensor network, \ie optimal tensor networks may never have two edges form the same free index partition.
\end{proof}

\subsection{Proof of \cref{thm:rank-bound} and \cref{thm:cost-bound}}
\begin{definition}[Subtensor]
Suppose $\ten{T} \in \real^{n_1 \times n_2 \times \ldots n_d}$ is a $d$-dimensional tensor. Its subtensor $\ten{S}  \in \real^{m_1 \times m_2 \times \ldots m_d}$ (written as $\ten{S} \sqsubseteq \ten{T}$) is also a $d$-dimensional tensor obtained by restricting the index sets corresponding to dimension $\mu$ to $m_\mu$ elements where for all $\mu \in \{1,2,\ldots, d\}$, $m_\mu \leq n_\mu$. If $\ten{S}$ is a subtensor of $\ten{T}$, there exists a set of binary matrices with orthonormal rows $\pi_1, \dots\pi_d$ such that $\ten{S} = (\pi_1 \otimes \pi_2 \otimes \cdots \otimes \pi_d) \ten{T}$.
\end{definition}

\begin{lemma}[Subtensors Preservation on Permutation]\label{lemma:appendix:subtensor-permute}
Let $\ten{T} \in \real^{n_1 \times n_2 \times \ldots \times n_d}$ be a $d$-dimensional tensor, $\ten{S}  \in \real^{m_1 \times m_2 \times \cdots \times m_d}$, and $\ten{S} \sqsubseteq \ten{T}$.
%
If $\Pi \in \{1,\ldots, d\} \rightarrow \{1,\ldots, d\}$ is a permutation of the dimensions,
then $\mathtt{permute}(\ten{S}, \Pi(1), \Pi(2), \ldots, \Pi(d)) \sqsubseteq \mathtt{permute}(\ten{T}, \Pi(1), \Pi(2), \ldots, \Pi(d))$.
\end{lemma}
\begin{proof}
As $\ten{S} \sqsubseteq \ten{T}$, there exists a list of binary matrices with orthonormal rows $[\pi_\mu]_{1\leq \mu \leq d}$ such that $\ten{S} = (\pi_1 \otimes \pi_2 \otimes \cdots \otimes \pi_d) \ten{T}$.
%
Then, we can see that
\begin{align*}
    \mathtt{permute}\left(\ten{S}, \Pi(1), \Pi(2), \ldots, \Pi(d)\right)= \mathtt{permute}\left((\pi_{1} \otimes \pi_{2} \otimes \cdots \otimes \pi_{d})\ten{T}, \Pi(1), \Pi(2), \ldots, \Pi(d))\right)
\end{align*}
%
Since each $\pi_\mu$ only modifies values along the corresponding mode $\mu$, and permutation only moves dimensions without altering values, we can move the projection outside the $\mathtt{permute}$ by reordering the projections according to the dimension permutation, and get 
\begin{align*}
&\mathtt{permute}\left((\pi_{1} \otimes \pi_{2} \otimes \cdots \otimes \pi_{d})\ten{T}, \Pi(1), \Pi(2), \ldots, \Pi(d))\right) \\
&= (\pi_{\Pi(1)} \otimes \pi_{\Pi(2)} \otimes \cdots \otimes \pi_{\Pi(d)})\mathtt{permute}\left(\ten{T}, \Pi(1), \Pi(2), \ldots, \Pi(d)\right)
\end{align*}
%
Therefore, $\mathtt{permute}(\ten{S}, \Pi(1), \Pi(2), \ldots, \Pi(d)) \sqsubseteq \mathtt{permute}(\ten{T}, \Pi(1), \Pi(2), \ldots, \Pi(d))$ holds.
\end{proof}

\begin{lemma}[Subtensors Preservation on Reshape]\label{lemma:appendix:subtensor-reshape}
Let $\ten{T} \in \real^{n_1 \times n_2 \times \cdots n_d}$ be a $d$-dimensional tensor with indices $I_1, I_2, \ldots, I_d$, and $\ten{S} \in \real^{m_1 \times m_2 \times \ldots m_d}$ be a $d$-dimensional tensor with indices $J_1, J_2, \ldots, J_d$.
%
If $\ten{S} \sqsubseteq \ten{T}$, then for any $s \subset \{1,2,\ldots, d\}$, we have $\matric{T}{\ind_s} \sqsubseteq \matric{S}{\mathcal{J}_s}$, and vice versa.
\end{lemma}
\begin{proof}
Since $\ten{S} \sqsubseteq \ten{T}$, there exists a list of binary matrices with orthonormal rows $[\pi_\mu]_{\mu \in \{1,2,\ldots, d\}}$ such that 
$$\ten{S} = \left(\pi_{1} \otimes \pi_{2} \otimes \cdots \otimes \pi_{d}\right)\ten{T}$$
%
For a set of indices $\ind_s$, by \cref{lemma:appendix:subtensor-permute} and the associativity of tensor product, we get that
$$
\matric{S}{\mathcal{J}_s} = \left(\bigotimes_{\mu \in s} \pi_\mu \otimes \bigotimes_{\mu \not\in s}\pi_\mu\right)\matric{T}{\ind_s}
$$
%
Therefore, we have that $\matric{S}{\mathcal{J}_s} \sqsubseteq \matric{T}{\mathcal{I}_s}$.

Similarly, the property can be proved for the reverse direction by applying the above two steps in the reverse order.
\end{proof}

\begin{lemma}[Singular Value Upper Bound in Subtensors]\label{lemma:appendix:subtensor}
Let $\ten{T} \in \real^{n_1 \times n_2 \times \cdots \times n_d}$ be a $d$-dimensional tensor with indices $I_1, I_2, \ldots, I_d$, $\ten{S}  \in \real^{m_1 \times m_2 \times \ldots \times m_d}$ be a $d$-dimensional tensor with indices $J_1, J_2, \ldots, J_d$, and $\ten{S} \sqsubseteq \ten{T}$. Define $\sigma_i(A)$ to be the $i$th smallest singular value of a matrix $A$. Then, for all $s \subset \{1, 2, \ldots, d\}$, if $\ind_s = \{I_i\}_{i \in s}$ and $\mathcal{J}_s = \{J_i\}_{i \in s}$, we have $\sigma_i(\matric{S}{\mathcal{J}_s}) \leq \sigma_i(\matric{T}{\ind_s})$.
\end{lemma}
\begin{proof}
From $\ten{S} \sqsubseteq \ten{T}$ and \cref{lemma:appendix:subtensor-permute}, we know that $\matric{S}{\mathcal{J}_s}) \sqsubseteq \matric{T}{\ind_s}$. Then, the result can be obtained by applying the Poincar\'e separation theorem to $\matric{T}{\ind_s}\matric{T}{\ind_s}^{*}$.
\end{proof}

\begin{lemma}[Singular Value Upper Bound in Truncations]\label{lemma:appendix:trunc}
Let $\ten{T} \in \real^{n_1 \times n_2 \times \cdots \times n_d}$ be a $d$-dimensional tensor with indices $I_1, I_2, \ldots, I_d$.
%
Let $\ind_s \subset \{I_1, I_2, \ldots, I_d\}$ be a set of indices.
Suppose $\textsc{SVD}(\matric{T}{\ind_s}) = U \Sigma V$. After truncation to some rank $r$, we get $\widetilde{\ten{T}}_{\langle \ind_s \rangle} = \widetilde{U}\widetilde{\Sigma}\widetilde{V}$ where $\widetilde{U} = U[:,:r]$, $\widetilde{\Sigma} = \Sigma[:r, :r]$, and $\widetilde{V} = V[:r]$.
%
Then we have that, for all $\ind_t \subset \{I_1, I_2, \ldots, I_d\}$, if $\ind_t \subseteq \ind_s, \ind_s \subseteq \ind_t$, or $\ind_s \cap \ind_t = \emptyset$, then  $\sigma_i(\widetilde{\ten{T}}_{\langle \ind_t \rangle}) \leq \sigma_i(\matric{T}{\ind_t})$.
\end{lemma}

\begin{proof}
%
In this proof, matrices $U$ and $V$ can be treated as $(n_s + 1)$ and $(d - n_s + 1)$ dimensional tensors where $n_s = |\ind_s|$. The same considerations are made for $\widetilde{U}$ and $\widetilde{V}$. Due to the tree structure, we consider the following three cases of relations between $\ind_t$ and $\ind_s$.

\begin{itemize}
    \item Case I: $\ind_t = \ind_s$. The singular values are discarded without other modification, so $\sigma_i(\widetilde{T}_{\langle\ind_t\rangle}) = \sigma_i(\matric{T}{\ind_t})$.
    
    \item Case II: $\ind_t \subset \ind_s$. 
    %
    By the definition of matricization and SVD, we know that $\ind_s \subset \textsc{Indices}(\widetilde{U})$. Hence, $\ind_t \subset \textsc{Indices}(\widetilde{U})$.
    By \cref{lemma:appendix:subtensor-reshape}, \cref{lemma:appendix:subtensor}, and $\widetilde{U}\widetilde{\Sigma} \sqsubseteq U \Sigma$, we know that $\sigma_i(\widetilde{U}\widetilde{\Sigma}_{\langle \ind_t \rangle}) \leq \sigma_i(U \Sigma_{\langle \ind_t \rangle})$.
    %
    Therefore,
    $\sigma_i(\widetilde{\ten{T}}_{\langle \ind_t \rangle}) = \sigma_i((\widetilde{U}\widetilde{\Sigma})_{\langle \ind_t \rangle}) \leq \sigma_i((U\Sigma)_{\langle \ind_s \rangle}) = \sigma_i(\ten{T}_{\langle \ind_s \rangle})$.
    
    \item Case III: $\ind_s \subset \ind_t$ or $\ind_t \cap \ind_s = \emptyset$. By the definition of matricization and SVD, we know that $\ind_s \subset \textsc{Indices}(\widetilde{U})$. Hence, $\ind_t \subset \textsc{Indices}(\widetilde{\ten{T}}) - \textsc{Indices}(\widetilde{U}) \subset \textsc{Indices}(\widetilde{V})$. By \cref{lemma:appendix:subtensor-reshape}, \cref{lemma:appendix:subtensor}, and $\widetilde{\Sigma}\widetilde{V} \sqsubseteq \Sigma V$, we know that $\sigma_i((\widetilde{\Sigma}\widetilde{V})_{\langle \ind_t \rangle}) \leq \sigma_i((\Sigma V)_{\langle \ind_t \rangle})$.
    %
    Therefore,
    $\sigma_i(\widetilde{\ten{T}}_{\langle \ind_s \rangle}) = \sigma_i((\widetilde{\Sigma}\widetilde{V})_{\langle \ind_s \rangle}) \leq \sigma_i((\Sigma V)_{\langle \ind_s \rangle}) = \sigma_i(\ten{T}_{\langle \ind_s \rangle})$.
\end{itemize}

To summarize, we have $\sigma_i(\widetilde{\ten{T}}_{\langle \ind_t \rangle}) \leq \sigma_i(\matric{T}{\ind_t})$ for all possible choices of $\ind_t$.
\end{proof}

\begin{theorem}[Singular Value Upper Bound]\label{thm:appendix:sv-bound}
Let $\ten{T} \in \real^{n_1 \times \cdots \times n_d}$ be a $d$-dimensional tensor with free indices $I_1, I_2, \ldots, I_d$, and $G=(\{\ten{T}\},\emptyset)$ be the graph with a single tensor.
%
If a complete program $P = \osplit(\ind_1, r_1); \ldots; \osplit(\ind_n, r_n)$ such that $\sem{P}(G, \error) = (G_n, \error_n)$,
then for every $1 \leq i, s \leq n$, we have $\sigma_j(\net{G_{i-1}}_{\langle\ind_s\rangle}) \leq \sigma_j(\matric{T}{\ind_s})$ where $\sigma_j(A)$ is the $j$th smallest singular value of the matrix $A$.
\end{theorem}
\begin{proof}
First of all, since $\sem{P}(G, \error) = (G_n, \error_n)$ where the execution succeeds, it means that $P$ is a valid program.
%
Therefore, for each pair of $1 \leq i < j \leq n$, there could only be three relations between $\ind_i$ and $\ind_j$: $\ind_i \subset \ind_j$, $\ind_j \subset \ind_i$, or $\ind_i \cap \ind_j = \emptyset$.
%
Now, let us prove the theorem by induction.

%New proof
Suppose the network obtained after the $k^{th}$ split is denoted as $G_k$. The network obtained after performing an output-directed split on $G_k$ through an index set $\ind_k$ is $G_{k+1}$. Specifically, $\sem{P}(G_k, \error_k) = (G_{k+1}, \error_{k+1})$ where $P = \osplit(\ind_k, r_k)$.

Performing the split defined above is equivalent to performing a truncated SVD on $\net{G_k}_{\langle 
\ind_k \rangle}$. Formally, we can say that if $\net{G_k}_{\langle 
\ind_k \rangle} = U \Sigma V$, then $\net{G_{k+1}}_{\langle 
\ind_k \rangle} = \widetilde{U} \widetilde{\Sigma} \widetilde{V}$. Consequently, we can use lemma \ref{lemma:appendix:trunc} to say that, for $\ind_t \subset \{I_1, I_2, \ldots, I_d\}$, such that $\ind_t \subseteq \ind_s, \ind_s \subseteq \ind_t$, or $\ind_s \cap \ind_t = \emptyset$; $\sigma_i(\net{G_{k+1}}) \leq \sigma_i(\net{G_k})$ for all possible values of $i$ and $k$.

From the above result, we can conclude that $\sigma_i(\net{G_{k}}_{\langle 
\ind_t \rangle}) \leq \sigma_i(\net{G_0}_{\langle 
\ind_t \rangle}) = \sigma_i(\ten{T}_{\langle 
\ind_t \rangle})$ for all valid choices of $\ind_t$ and all possible values of $i$ and $k$.

\end{proof}

\begin{theorem}[Upper Bound of Costs]\label{thm:appendix:cost-bound}
For a data tensor $\ten{T}$ and a sketch program $\osplit(\indices_1, \hole_1); \ldots; \osplit(\indices_n, \hole_n)$ and an error bound $\error$, if $r_1, \ldots, r_n$ is a solution to \cref{eqn:ip} with singular values $\sigma_i(\matric{T}{\ind_s})$ for all $\ind_s \in \textsc{Indices}(\ten{T})$, then $r_1, \ldots, r_n$ is also a solution to \cref{eqn:ip} with singular values $\varsigma_i \leq \sigma_i(\matric{T}{\ind_s})$.
\end{theorem}
\begin{proof}
It is easy to see that
$$
\sum_{i=1}^{s}\sum_{j=1}^{r_i} \varsigma_{ij}^{2} \leq \sum_{i=1}^{s}\sum_{j=1}^{r_i} \sigma_{ij}^2 \leq \left(\error\norm{\ten{T}}\right)^2
$$
which also satisfies the linear programming constraints.
\end{proof}

\section{Additional Algorithms}\label{sec:appendix:alg}
\begin{algorithm}[h]
\small
\caption{Execution of an output-directed split.}\label{alg:out-split}
\begin{algorithmic}[1]
    \Require{A tensor network $G$, the error bound $\error$, the target free index partition $\ind$, and the target split rank $r$.}
    \Ensure{A tensor network $G'$ with the desired index partition and the split rank, and the updated error bound, or $\bot$ if the execution fails.}
    \Function{ExecOSplit}{$G, \error, \ind, r$}
        \State $(\nodes, \edges) \gets G$
        \For{$\ten{T} \in \nodes$}
            \State $b \gets 1$
            \State $\ind_s \gets \emptyset$
            \For{$I_\mu \in$ \Call{Indices}{$\ten{T}$}}
                \State $\ind_\mu \gets$ \Call{FreeIndices}{$G, \ten{T}, I_\mu$}
                \If{$\ind_\mu = \ind$}
                    \State \Return $(G, \error)$
                \ElsIf{$\ind_\mu \subset \ind$}
                    \State $\ind_s \gets \ind_s \cup \{I_\mu\}$
                \ElsIf{$\ind_\mu \cap \ind \not= \emptyset$}
                    \State $b \gets 0$
                    \State break
                \EndIf
            \EndFor
            \If{$b = 1$}
                \State \Return \Call{ExecISplit}{$G, \error, \ten{T}, \ind_s, r$}
            \EndIf
        \EndFor
        \State \Return $\bot$
    \EndFunction
\end{algorithmic}    
\end{algorithm}

\cref{alg:out-split} presents the execution of output-directed splits. The algorithm iterates over all nodes in the network and checks which node satisfies the required constraints.
%
Specifically, for a given node $\ten{T}$, the algorithm examines all child subtrees of $\ten{T}$ and computes the free indices $\ind_\mu$ for each subtree $\mu$.
%
If $\ind_\mu$ is a proper subset of $\ind$, indicating that this subtree should be separated out through a split operation, the corresponding index $I_\mu$ is added to the $\isplit$ index set $\ind_s$ (Line 8-9).
%
If the free indices $\ind_\mu$ partially overlap with the requested index partition block $\ind$, splitting this node cannot produce the desired partition, so the node is skipped (Line 10-12).
%
In a special case where $\ind_\mu$ is identical to $\ind$, the algorithm detects that an edge already exists in the network realizing the requested index partition.
%
In this case, the node is also skipped, and performing this operation would not make any change to the structure.
%
Once a suitable node for splitting is identified, the procedure \textsc{ExecISplit} is called to execute the input-directed node split.
%
It is also possible that no suitable node is found when the combination of output-directed splits is invalid.
%
For instance, $\osplit(\{I_1, I_2\}, r)$ followed by $\osplit(\{I_1, I_3\}, r)$ is invalid as we cannot put the index $I_1$ in two partition blocks if one is not a subset of the other.
%
Line 21 of the algorithm handles this case and returns failure.
%
During program synthesis, the function \validexpr\ in~\cref{alg:synth} takes charge of filtering out invalid combinations to avoid failures.
%
Execution of an input-directed split is built on truncated Singular Value Decomposition (SVD) with minor modifications.

\begin{algorithm}[h]
\small
\caption{Execution of an input-directed split.}\label{alg:appendix:in-split}
\begin{algorithmic}[1]
    \Require{A tensor network $G$, the error bound $\error$, the operand node $\ten{T}$, the index partition $\ind$, and the target rank $r$.}
    \Ensure{A tensor network $G'$ with the desired index partition and the split rank, and the updated error bound, or $\bot$ if the execution fails.}
    \Function{ExecISplit}{$G, \error, \ten{T}, \ind, r$}
        \State $G \gets$\Call{Orthonormalize}{$G, \ten{T}$}
        \State $\begin{bmatrix}U & U_\error\end{bmatrix},  \begin{bmatrix}\Sigma & \\ & \Sigma_\error\end{bmatrix}, \begin{bmatrix}V \\ V_\error\end{bmatrix} \gets$ \Call{Svd}{$\matric{T}{\ind}, \error$}
        \If{len($S$) $> r$}
            \State \Return $\bot$
        \EndIf
        \State $\error' \gets$ \Call{Truncate}{$\Sigma_\error, r$}
        \State $\ten{T}_1 \gets$ \textsc{RestoreShape}$\left(\begin{bmatrix}U & U_{\error'}\end{bmatrix}\right)$
        \State $\ten{T}_2 \gets$ \textsc{RestoreShape}$\left(\begin{bmatrix}\Sigma & \\ & \Sigma_{\error'}\end{bmatrix} \begin{bmatrix}V \\ V_{\error'}\end{bmatrix}\right)$
        \State $(\nodes, \edges) \gets G$
        \State $\nodes' \gets \nodes \cup \{\ten{T}_1, \ten{T}_2\} \backslash \{\ten{T}\}$
        \State $\edges' \gets \edges \cup \{(\ten{T}_1, \ten{T}_2, I_{fresh})\}$
        \For{$e \in \edges$}
            \If{$e = (\ten{T}, \ten{S}, I_\mu) \lor e = (\ten{S}, \ten{T}, I_\mu)$}
                \If{$I_\mu \in \ind$}
                    \State $\edges' \gets \edges \cup \{(\ten{T}_1, \ten{S}, I_\mu)\} \backslash \{e\}$
                \Else
                    \State $\edges' \gets \edges \cup \{(\ten{T}_2, \ten{S}, I_\mu)\} \backslash \{e\}$
                \EndIf
            \EndIf
        \EndFor
        \State \Return $(\nodes', \edges'), \error'$
    \EndFunction
\end{algorithmic}
\end{algorithm}

\cref{alg:appendix:in-split} presents the algorithm of our implemented truncated SVD.
%
It takes the current tensor network $G$, the error bound $\error$, the splitting node $\ten{T}$, the indices $\ind$ of $\ten{T}$ and the target rank size $r$ as inputs.
%
As outputs, it returns the transformed graph $G'$ and the remaining error bound $\error'$ after the transformation.
%
The core part of truncated SVD between line 2-8 is standard.
%
One difference is on line 3-5 where if the specified rank cannot be realized, the program returns $\bot$, marking the execution fail.
%
Another difference is on line 6 where we calculate the remaining error bound $\error'$ after truncating the target edge to rank $r$.
%
Some extra work is done on line 9-20, which modifies the node connections by adding new nodes and edges to the input graph.

\section{Encoding of Rank Search}\label{sec:appendix:encoding}
%
Given a data tensor $\ten{T}$, an error bound $\error$, and a sketch $\sketch = \osplit(\ind_1, \hole_1); \cdots; \osplit(\ind_S, \hole_S);$, the whole encoding goes through the following few steps.

\paragraph{Precomputation of Singular Values}
%
Let $\textsc{Indices}(\ten{T}) = \{I_1, I_2, \ldots, I_d\}$.
%
For an index subset $\ind_s \subset \textsc{Indices}(\ten{T})$,
we perform the SVD on $\matric{T}{\ind_s} = U \Sigma V$, and get the list of singular values $\Sigma$.
%
Then, we compute the prefix sum for squares of singular values for all possible truncation options.
%
Each element in the set of prefix sums $\Lambda$ is computed as $\lambda_{i} = \sum_{j=1}^{i} \sigma_{j}^{2}$.
%
In the encoding, we introduce a binary integer variable $x_{s,i}$ for each prefix sum value satisfying $\lambda_i \leq \left(\error \norm{\ten{T}}\right)^2$. This variable indicates that if the hole $\hole_s$ is filled with the value $i$, then the corresponding error incurred in this step is $\lambda_i$.
%
However, in practice, we encounter the challenge of too many rank candidates, which leads to slow constraint solving performance.
%
In extreme cases, solving constraints with complete rank candidates can take more than a day.
%
To mitigate this issue, we sample from the rank candidates.
%
Intuitively, when two errors $\lambda_i$ and $\lambda_j$ are close to each other, it is less likely for the result network to have a drastically change in its cost.
%
Therefore, we group prefix sums into a small number of bins, written as $\lambda_{b^s_i}$ where $b^s_i \in \{1,2,\ldots, \len(\Lambda)\}$, $\len(\many{b^s_i}) \leq \len(\Lambda)$, and $\lambda_{b^s_i} \leq \left(\error \norm{\ten{T}}\right)^2$.
%
Each bin satisfies that $\lambda_{b^s_i} - \lambda_{b^s_{i-1}} = c (\error \norm{\ten{T}})^2$, where $c$ is a hyperparameter. We choose $c=0.05$ in the SVDinsTN experiments and $c=0.1$ in all others..
%
Finally, we store the mapping $\ind_s \mapsto \many{\lambda_{b^s_i}}$ for all choices of $b^s_i$, and create binary integer variables $r^s_{b^s_i}$ for each $\lambda_{b^s_i}$.

\paragraph{Encoding into 0-1 ILP}
%
Now, let us introduce how constraints and the objective function are encoded.
%
First of all, we need to ensure that each hole has only one rank assignment, which is
\begin{equation}
    \forall 1 \leq s \leq S. \sum_{i=1}^{\len(\many{b^s_i})} r^s_{b^s_i} = 1
\end{equation}
The second constraint is that the total truncation error should not go beyond the prescribed error bound:
\begin{equation}
    \sum_{s=1}^{S} \sum_{i=1}^{\len(\many{b^s_i})} \lambda_{b^s_i}r^s_{b^s_i} \leq \left(\error \norm{\ten{T}}\right)^2
\end{equation}
%
In this equation, the innermost multiplication says that, for the step $s$ where the split produces the the index partition $\ind_s$, if the rank is chosen to be $\len(\Sigma_s) - b^s_i$, then the truncation error is $\lambda_{b^s_i}$.
%
Going one level up, all choices of $b_i^s$ are added together but only one of them would take effect because the previous constraint guarantees that.
%
The outermost summation accumulates the truncation errors from all steps in a program and this total truncation error is required to be no more than the specified relative error budget.

Regarding the objective function $\size{G_S}$ where $\sem{P}(G_0, \error) = (G_S, \error_S)$,
the size term is expanded into $\size{G_S} = \sum_{\ten{T} \in \nodes}\size{\ten{T}} = \sum_{\ten{T} \in \nodes}\prod_{\mu=1}^{d} I_\mu$.
%
The value of $I_\mu$ is dependent on the rank assignment value $x^s_{b^s_i}$ too.
%
Suppose a node $\ten{T}$ is associated with several index partition blocks $s_1, s_2, \ldots, s_n$, then its size is $b^{s_1}_{i_1} b^{s_2}_{i_2} \cdots b^{s_n}_{i_n}$.
%
Unfortunately, this is a multiplication of integer variables but not a linear expression, which complicates the constraint solving process.
%
To overcome this problem, whenever there is such a multiplication term, we introduce a new variable $y$ to replace $b^{s_1}_{i_1} b^{s_2}_{i_2} \cdots b^{s_n}_{i_n}$ and add constraints
\begin{equation}
    \begin{aligned}
        0 \leq b^{s_k}_{i_k} &\leq 1 \\
        y & \leq b^{s_k}_{i_k} \\
        y & \geq \sum_{k=1}^{n} b^{s_k}_{i_k} - 1 \\
    \end{aligned}
\end{equation}

\paragraph{Optimization in Encoding}
%
As we are picking the sketch with the top-$k$ upper bound during synthesis, the encoding can progressively give the objective function a tighter upper boundary to accelerate the constraint solving.
%
In detail, if the current top-$k$ upper bound is $c_{\mathtt{topk}}$, we add an additional constraint that requires the objective value to be better than the current top-$k$ values, which is
\begin{equation}
\size{\sem{\sketch[\many{\subst{\hole_s}{(\Sigma_s - r_s)}}]}(G_0, \error)} \leq c_{\mathtt{topk}}
\end{equation}

\paragraph{Other Hyperparameters}
%
There are two hyperparameters in 
Through experiments, we observe that the sketch with the lowest upper bound usually result in good enough compression ratio, and the further exploration of the search space brings marginal improvement.
%
Therefore, in the evaluation section, we report the results of programs that is originated from the sketch with the lowest upper bound cost.

\section{More Evaluation Results}
%
\subsection{Introduction to BigEarthNet}
%
BigEarthNet dataset includes multi-spectral images collected by Sentinel-2 sensor.
%
It contains $590,326$ image patches from 125 Sentinel-1 and Sentinel-2 tiles.
%
In the experiment, we consider only Sentinel-2 bands, leaving us with $269695$ data points.
%
In Sentinel-2 bands, each tile has $12$ spectral bands with size of $120 \times 120$ pixels.
%
From these data, we randomly sample $30$ and $3000$ to construct small and large data tensors respectively.
%
For large tensors, we reshape them into $5 \times 20 \times 30 \times 12 \times 120 \times 120$ to for higher dimensions.

\subsection{Introduction to PDEBench}
%
PDEBench data includes sampling data of $10$ different PDEs for scientific machine learning.
%
In the evaluation, we take data for the 3D compressible Navier-Stokes problem, of which the spatial dimension is $3$, the spatial resolution is $64 \times 64 \times 64$, the temporal resolution is $21$, and the total sample size is $100$.
%
These equations describe a fluid flow,
\begin{align*}
\partial_t \rho + \nabla \cdot (\rho \mathbf{v}) &= 0, \\
\rho (\partial_t \mathbf{v} + \mathbf{v} \cdot \nabla \mathbf{v}) &= -\nabla p + \eta \Delta \mathbf{v} + (\zeta + \eta / 3) \nabla (\nabla \cdot \mathbf{v}), \\
\partial_t \left[ \epsilon + \frac{\rho v^2}{2} \right] + \nabla \cdot \left[ \left( \epsilon + p + \frac{\rho v^2}{2} \right) \mathbf{v} - \mathbf{v} \cdot \sigma' \right] &= 0,
\end{align*}
where $\rho$ is the mass density, $\mathbf{v}$ is the velocity, $p$ is the gas pressure, $\epsilon = \frac{p}{\Gamma - 1}$ is the internal energy, $\Gamma = \frac{5}{3}$, $\sigma'$ is the viscous stress tensor, $\eta$ is the shear viscosity, and $\zeta$ is the bulk viscosity.

\subsection{Example Structures}
In this section, we present several structures that discovered by our tool and features the necessity of TN-SS.
\begin{figure}[h]
    \centering
    \begin{minipage}{0.33\linewidth}
        \includegraphics[width=0.95\linewidth]{figs/example_1.png}
    \end{minipage}
    %
    \begin{minipage}{0.33\linewidth}
        \includegraphics[width=0.95\linewidth]{figs/example_4.png}
    \end{minipage}
    %
    \begin{minipage}{0.33\linewidth}
        \includegraphics[width=0.95\linewidth]{figs/example_5.1.png}
    \end{minipage}
    \caption{These structures showcase that our tool can discover non-standard structures other than TT, Binary-tree HT, etc. One single internal node suffices to provide good compression ratio in these cases.}
    \label{fig:appendix:non-standard}
\end{figure}

\begin{figure}[h]
    \centering
    \begin{minipage}{0.33\linewidth}
        \includegraphics[width=0.95\linewidth]{figs/example_2.png}
    \end{minipage}
    %
    \begin{minipage}{0.33\linewidth}
        \includegraphics[width=0.95\linewidth]{figs/example_3.png}
    \end{minipage}
    \caption{These two structures are similar to tensor trains but they have clustered and reordered indices, which allow them to have better compression ratios than traditional tensor trains.}
    \label{fig:appendix:reorder}
\end{figure}