\section{Related Work}
\label{sec:related}
\paragraph{Tensor Network Structure Search}
%
TN-SS has been explored in prior work from different aspects.
%
One branch of TN-SS focuses on optimizing rank assignments~\cite{pmlr-v32-rai14,mickelin2020algorithms,Sedighin2021Adaptive,Yin_Phan_Zang_Liao_Yuan_2022,pmlr-v202-ghadiri23a}.
%
These studies target specific tensor network topologies, proposing efficient algorithms to sample rank assignments and verifying whether the sampled ranks meets the error constraints.
%
Another branch concentrates on topology search while keeping the internal rank assignments fixed~\cite{hashemizadeh2020adaptive,Li_Sun_2020,PhysRevResearch.5.013031,Haberstich23}.
%
More recent work address the complete TN-SS problem~\cite{Li_Zeng_Tao_Zhao_2022,Li_Zeng_Li_Caiafa_Zhao_2023,zheng2024svdinstn,zengtngps}, incorporating both topology and rank search.
%
Most of these methods adopt a sampling-based design, where they sample both the topology and rank assignments, and validate each candidate structure to check if it satisfies the desired error bound.
%
An alternative approach is introduced by \citet{zheng2024svdinstn} which avoids sampling.
%
Their method encodes both topology and rank search into a unified optimization problem, and employs the alternating direction method of multipliers (ADMM) to find a solution.
%
Our idea can be viewed as an variant of sampling-based methods.
%
However, our dedicated domain-specific language (DSL) for tensor network transformation, combined with the constraint-based sketch completion, effectively reduces the number of required samples and accelerates the search.
%
\paragraph{Low-Rank Tensor Decomposition}
%
Low-rank tensor decomposition has been studied for many different structures.
%
Tucker decomposition~\cite{Tucker_1966,hosvd} factorizes a high-order tensor into a core tensor with several low-rank tensors, one for each mode.
%
Tensor train decomposition~\cite{Oseledets_2011,tt-ice} expresses a high-order tensor as a linear multiplication of 3-order tensors.
%
Hierarchical Tucker decomposition~\cite{hackbusch2009new,ht,falco2021tree} generalizes tensor trains and tuckers to arbitrary trees, offering greater flexibility and compression potential.
%
Beyond tree-based structures, several prior work~\cite{zhao2016tensor,handschuh2015numerical,espig2012note,mickelin2020algorithms,yang2017loop} explores tensor decomposition for structures with cycles, such as tensor rings or tensor chains.
%
In contrast, our work focuses on searching for the optimal tree structure representation to compress the input data tensor.
%
Cyclic structures are currently outside our search space, and we plan to consider them in future work.
%
\begin{figure}[t]
    \centering
    \begin{minipage}{.4\linewidth}
    \centering
    \small
    \begin{align*}
        &\osplit(\{I_1\}, r_1);\\
        &\osplit(\{I_1, I_2\}, r_2);\\
        &\osplit(\{I_2\}, r_3);
    \end{align*}
    \end{minipage}
    %
    \hfill
    %
    \begin{minipage}{.55\linewidth}
    \centering
    \resizebox{.85\linewidth}{!}{
        \input{figs/tn_example}
    }
    \end{minipage}    
    \caption{An example transformation program (left) and the result tensor network (right).}
    \label{fig:example-program}
\end{figure}
\paragraph{Sketch-Based Program Synthesis}
%
Sketches have been widely used in the program synthesis community.
%
The idea of sketch generation and completion was first introduced by~\citet{sketch05armando} for bit-manipulation programs.
%
Since then, sketches have demonstrated their effectiveness in various domains such as stencil computations~\cite{solar2007sketching}, database programs~\cite{sqlizer}, Java programming~\cite{sypet},  and regular expression synthesis~\cite{regexsyn}.
%
These methods share a common structure: a sketch generation phase followed by sketch completion, and the sketch completion phase uses constraints to encode syntactic and semantic requirements.
%
We adapt this framework to the TN-SS problem by encoding the error bound constraints as an integer programming problem and optimizing for network costs.
%
Additionally, we show that a smart prioritization strategy at the sketch level significantly reduces the number of required samples and enhances efficiency.