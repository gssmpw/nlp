\begin{figure*}[t]
    \centering
    \includegraphics[width=.95\linewidth]{figs/workflow.png}
    \caption{The overall workflow of our TN-SS algorithm.}
    \label{fig:workflow}
\end{figure*}
%
\section{Introduction}\label{sec:intro}
Tensor networks have found widespread applications in machine learning~\cite{lebedev2014speeding,novikov2015tensorizing,phan2020stable,memmel2022position}, scientific computing~\cite{pmlr-v139-richter21a,PhysRevB.95.045117,PhysRevX.14.011009,ma2024approximate}, quantum computing~\cite{verstraete2008matrix,banuls2023tensor,Montangero_2018}, and many other areas because they allow effective low-rank approximations of high-dimensional data.
%
Over the past decade, various tensor network structures, such as tensor train~\cite{Oseledets_2011}, Tucker~\cite{Tucker_1966}, and hierarchical Tuckers~\cite{ht}, have been deployed. Each such structure offers distinct advantages for specific scenarios, with no single optimal representation.
%
This observation brings up an important question: given a data tensor and an optimization objective, how could one efficiently determine the most suitable tensor network structure to achieve the desired goal?
%
This question have evolved into a research topic known as tensor network structure search (TN-SS).

TN-SS has two highly inter-related parts: (1) a graph structure where each node corresponds to a tensor and each edge represents a shared dimension between connecting tensors; and (2) an assessment of the compression and approximation quality of each graph.
%
More specifically, the task of finding an optimal tensor network structure involves two subtasks:
%
\begin{enumerate*}[label=(\arabic*)]
    \item topology search: identity the optimal connections between nodes;
    \item rank search: find the optimal dimension sizes (also called ranks) for edges.
\end{enumerate*}
%
While this division provides a clear framework to address the TN-SS problem, existing approaches face significant challenges in effectively solving these subproblems.

\paragraph{Open Problems}
%
First, a key challenge in topology search is the introduction of internal nodes, which is crucial for achieving high compression performance.
%
Prior work~\cite{Li_Sun_2020,hashemizadeh2020adaptive} either assumes a fixed number of nodes, or uses greedy strategies to introduce internal nodes.
%
These na\"ive approaches not only limit the search performance but also fail to scale to large tensors.
%
Second, despite substantial progress in rank search~\cite{pmlr-v32-rai14,mickelin2020algorithms,Sedighin2021Adaptive,Yin_Phan_Zang_Liao_Yuan_2022,pmlr-v202-ghadiri23a}, current methods rely on computationally expensive techniques like alternating least squares~\cite{als} or gradient descent~\cite{kolda2020stochastic}.
%
The development of a more efficient rank search algorithm is necessary.
%
Third, as we see in \cref{sec:related}, prior work often addresses each subproblem independently, leading to suboptimal results.
%
Achieving the good coordination between topology and rank search remains an underexplored area.
%
Some prior work~\cite{hashemizadeh2020adaptive,Li_Zeng_Li_Caiafa_Zhao_2023,Li_Zeng_Tao_Zhao_2022,zengtngps} considers both aspects simultaneously, but they rely on sampling and validating a large number of candidate structures.
%
Unfortunately, this framework repeatedly evaluates the optimization objective, which is also computationally expensive and hence drastically slows down the entire search process.
%
A more recent approach, SVDinsTN~\cite{zheng2024svdinstn}, abandons the sample-evaluation strategy and combines topology search and rank search into one single optimization problem to improve search time.
%
However, it requires considerable human effort for hyperparameter tuning to get a desired error bound and fails to scale to large tensors, which we demonstrate empirically in \cref{sec:eval}.
%
\paragraph{TN-SS as Program Synthesis}
%
In this work, we reduce TN-SS into a program synthesis problem, through which we hope to apply pruning techniques from program synthesis to the search of tensor network structures.
%
The core drawback of prior work is that they define the search space as different graphs, which is too specific to effectively implement pruning and prioritization strategies.
%
Therefore, we design a higher-level search space that facilitates the integration of domain-specific knowledge to accelerate the search.
%
Inspired by tensor decomposition methods that generate tensor networks by a series of split operations from the original data tensor~\cite{handschuh2015numerical}, we notice that there exists a mapping between split operations and tensor network structures.
%
If we design a specialized language for tensor network transformations, executing programs written in this language over a data tensor can produce different tensor network structures.
%
Consequently, the TN-SS problem can be framed as the task of synthesizing such transformation programs and selecting the optimal one.
%
\paragraph{Overall Workflow}
%
The overall workflow of our algorithm is illustrated in~\cref{fig:workflow}.
%
Given a data tensor and an error bound, the first step of the algorithm is to generate \emph{sketch programs}.
%
A sketch program is a program with holes in it, where each hole corresponds to different rank assignments for an edge, and hence each sketch corresponds to a set of tensor networks with the same topology but potentially different ranks.
%
Particularly, our sketch programs are composed of \emph{output-directed splits} rather than the traditional node splits as used by~\citet{handschuh2015numerical}.
%
This design helps in pruning away suboptimal structures and reducing the search space as we show in \cref{sec:algo:split}.
%
After sketch generation, the algorithm completes the sketches by determining what ranks to provide for each hole.
%
To achieve this goal, singular values are computed for different unfoldings of the data tensor.
%
They are then used to generate constraints that encode structural information and error consumption requirements for the synthesized tensor network.
%
A constraint solver is employed to find rank assignments that satisfy these constraints, and the rank assignments are used to calculate the approximate costs for sketches.
%
Lastly, the algorithm selects top-$k$ sketches, performs actual node splits, applies rounding, and returns the optimal one.

\paragraph{Contributions}
%
In summary, we make the following contributions in this paper:
%
\begin{itemize}[parsep=-.5ex]
    \item We design a domain specific language for tensor network transformations and reduce the TN-SS problem into a program synthesis problem (\cref{sec:algo:program}).
    \item We propose to replace traditional split operations with \emph{output-directed splits} (\cref{sec:algo:split}) in the language for the effective search of tensor network structures.
    \item We enumerate programs based on sketches and complete sketches by encoding requirements into integer programming constraints (\cref{sec:algo:fillhole}).
    \item We demonstrate the effectiveness of the proposed ideas through empirical evaluations and show that our approach runs significantly faster, achieves better compression ratios in most cases, and scales to larger tensors that baselines cannot handle. (\cref{sec:eval}).
\end{itemize}