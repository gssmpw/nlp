
\section{Structure Search with Program Synthesis}\label{sec:algo}
%
\subsection{Preliminaries}
\begin{definition}[Tensor, Tensor Size]
Let $d \in \nat$ and $n_1, n_2, \ldots, n_d \in \nat$.
%
A tensor $\ten{T} \in \real^{n_1 \times n_2 \times \cdots \times n_d}$ is a $d$-dimensional array.
%
Each dimension $\mu \in \{1, 2, \ldots, d\}$ has a name $I_\mu$ and $\size{I_\mu} = n_\mu$.
%
We use $\ten{T}_{i_1, i_2, \ldots, i_d}$ to denote an element in the tensor where $i_\mu \in \{1, \ldots, n_\mu\}$.
%
The size of the tensor $\ten{T}$ is defined as $\size{\ten{T}} = \prod_{\mu=1}^{d} n_\mu$.
\end{definition}

\begin{definition}[Matricization]
For a $d$-dimensional tensor $\ten{T} \in \real^{n_1 \times n_2 \times \cdots \times n_d}$ and a set of dimensions $\ind_s \subseteq \{I_1,\ldots, I_d\}$, let $\ind_t = \{I_1, \ldots, I_d\}\ \backslash\ \ind_s$ be the complementary mode set, $\ten{T}' = \mathtt{permute}\left(\ten{T}, [\mu]_{I_\mu \in \ind_s} + [\nu]_{I_\nu \in \ind_t}\right)$ be the permuted tensor with the modes $s$ at the front, then the corresponding \emph{$\ind_s$-matricization} $\matric{T}{\ind_s}$ is defined as $
\matric{T}{\ind_s} = \mathtt{reshape}\left(\ten{T}', \prod_{I_\mu \in \ind_s} n_{\mu}, \prod_{I_\nu \in \ind_t} n_{\nu}\right)$.
\end{definition}
\begin{definition}[Tensor Network]
A tensor network is an undirected graph $G=(\nodes,\edges)$ where each vertex in $\nodes$ is a tensor and each edge in $\edges$ is a tuple of three elements: two node names and their shared index name. Particularly, tensor networks without cycles are called \emph{tree tensor networks}. The tensor represented by a graph $G$ is the contraction of all tensors over shared modes, denoted by $\net{G}$. The size of a tensor network is $\size{G} = \sum_{\ten{T} \in \nodes} \size{\ten{T}}$.
\end{definition}
%
Given a tensor network $G$, we call edges with a dangling end \emph{free edges}, and those without dangling ends \emph{contracted edges}.
%
For example, the network in \autoref{fig:example-program} has four free edges $I_1, I_2, I_3, I_4$ and three contracted edges $r_1, r_2, r_3$.
%
The tensor represented by this network is computed as
$
\net{G}_{ijkl} = \sum_{a=1}^{r_1}\sum_{b=1}^{r_2}\sum_{c=1}^{r_3} \ten{A}_{ia}\ten{B}_{jb}\ten{C}_{abc}\ten{D}_{c k l}
$

\begin{definition}[Tensor Network Structure Search]
The tensor network structure search (TN-SS) problem finds the optimal tensor network that reduces the storage space while maintaining accuracy. Specifically, given a TN-SS problem $(\ten{T}, \error)$ where $\ten{T}$ is the data tensor and $\error$ is a prescribed error bound, the goal of the TN-SS algorithm is to solve the following optimization problem
$$
\arg\min_{G} \quad \size{G} \quad
\textrm{s.t.} \quad \norm{\net{G} - \ten{T}}\leq \error\norm{\ten{T}}
$$
\end{definition}
In other words, the TN-SS problem aims at finding the most compressed tensor network within a given error bound.
%
In this work, we target arbitrary tree structures and do not consider structures with cycles.

\begin{algorithm}[t]
\small
\captionsetup{font=small} % set size of caption font
\caption{Tensor network structure search algorithm}\label{alg:high-level}
\begin{algorithmic}[1]
    \Require The data tensor $\ten{T}$, the error bound $\error$, the param $k$
    \Ensure The result tensor network $G$ such that $\size{G} \leq \size{\ten{T}}$, and $\norm{\net{G} - \ten{T}} \leq \error \norm{\ten{T}}$
    \Function{SearchStructure}{$\ten{T}, \error, k$}
        \State $G_0 \gets (\{\ten{T}\}, \emptyset)$
        \State $G_{min} \gets G_0$
        \For{$(G, \error') \in \textsc{Synth}(\ten{T}, \error, k)$}
            \State $G \gets$ \Call{Round}{$G, \error'$}
            \If{$\size{G} < \size{G_{min}}$}
                \State $G_{min} \gets G$
            \EndIf
        \EndFor
        \State \Return $G_{min}$
    \EndFunction
\end{algorithmic}
\end{algorithm}

\subsection{From TN-SS To Program Synthesis}\label{sec:algo:program}
%
To solve a TN-SS problem $(\ten{T},\error)$, we propose to find a near-optimal solution by generating a transformation program that incrementally splits nodes to produce a tensor network with reduced size.
%
The algorithm that integrates program synthesis with TN-SS is presented in~\cref{alg:high-level}.
%
The process begins by constructing an initial graph $G_0$ containing a single node $\ten{T}$.
%
Then, the algorithm enters a loop, where it repeatedly calls the function \textsc{Synth} (Line 4).
%
This function uses a program synthesizer to enumerate and execute transformation programs, generating various tensor networks $G$.
%
At the end of each iteration, the algorithm calls the procedure \textsc{Round} for further compression, and updates the current minimum structure $G_{min}$ if the newly generated structure has a lower cost.
%
Finally, the algorithm returns the structure with the minimum cost.
%
In the remaining section, we introduce the design of the transformation language and the formal reduction of TN-SS to program synthesis.

\begin{figure}[t]
    \centering
    \small
    \vspace{-1em}
    \begin{alignat*}{2}
        \text{Programs or sketches}\quad & P,\sketch && := \emptyprog \mid \expr \mid \seq{P}{\expr} \\
        \text{Expressions}\quad & \expr &&:= \osplit(\ind, r) \\
        \text{Index sets}\quad & \ind  &&:= \{I_\mu\} \mid \ind \cup \{I_\mu\} \\
        \text{Ranks or holes}\quad & r     &&:=\ \hole \mid 1 \mid 2 \mid \cdots \\
        \text{Dimensions}\quad & \mu   &&:= 1, 2, 3, \ldots
    \end{alignat*}
    \vspace{-2em}
    \begin{align*}
    \sem{E}(\bot) = \bot \quad&\quad \sem{\emptyprog}(G, \error) = (G, \error) \\
    \sem{\osplit(\indices, r)}(G, \error) &= \textsc{ExecOSplit}(G, \error, \ind, r)\\
    \sem{\seq{P}{\expr}}(G,\error) &= \sem{\expr}(\sem{P}(G,\error))
    \end{align*}
    \vspace{-2em}
    \caption{Syntax and semantics for the DSL.}\label{fig:dsl}
    % \vspace{-1em}
\end{figure}

\paragraph{Syntax and Semantics of the Language}
%
As illustrated in~\cref{fig:dsl}, a program $P$ can either be empty, denoted by $\emptyprog$, or a sequence of expressions $\seq{P}{E}$.
%
Each expression represents a split operation that takes a set of indices $\ind$ and a rank $r$ as inputs.
%
The index set consists of indices from different modes $I_\mu$.
%
The rank $r$ can be either an integer or a hole $\hole$, which can be filled later.
%
If a program consists of ranks as holes, it is referred to as a \emph{program sketch}.
%
A sketch $\sketch$ can be completed with a rank assignment $\many{\hole_s \mapsto r_s}$ (we use $\many{X}$ to denote a set of similar elements), which is a mapping from holes to integers.
%
A rank assignment can \emph{complete} a sketch by filling holes with corresponding integers, resulting in a complete program, represented as $\sketch[\many{\subst{\hole_s}{r_s}}]$.
%
The execution of a complete program $\sem{P}$ takes a tensor network $G$ and an error bound $\error$ as inputs, and outputs a new tensor network $G'$ and the remaining error bound $\error' \leq \error$.
%
If an execution fails, it skips the remaining expressions and return $\bot$.
%
Multiple expressions in a program are executed sequentially.
%
For each split operation, it is executed using the method \textsc{ExecOSplit}, which produces an updated tensor network while ensuring the result stays within the specified error bound.
%
For example, \cref{fig:example-program} (left) shows a program of three splits.
%
Its execution produces the network structure depicted on the right.
%
The steps of how this result is reached is detailed in \cref{fig:output-directed-example}, and further explained in \cref{sec:algo:split}.

\paragraph{Reduction to Program Synthesis}
%
Using the language defined above, we reduce the TN-SS problem $(\ten{T}, \error)$ to an optimal program synthesis problem:
\begin{align*}
\arg\min_{P}~\size{G} \quad
\textrm{s.t.} \quad \sem{P}(G_0, \error) = (G, \error^{*}) \land \error \leq \error^{*}
\end{align*}
where $G_0 = (\{\ten{T}\}, \emptyset)$ is the initial tensor network containing the data tensor and $\error^{*}$ is the remaining error.
%
Its solution is a program, execution of which produces a tensor network $G$ of the smallest size within the error bound $\error$.

\cref{alg:synth} outlines our high-level program synthesis algorithm.
%
The core idea is to enumerate and rank sketches, and then complete sketches by filling in their holes.
%
The algorithm starts from an empty sketch, and incrementally appends new expressions to construct all possible sketches (Line 3-7).
%
The details of sketch construction and the implementation of the function \validexpr\ are described in \cref{sec:algo:split}.
%
Then, the algorithm fills holes in each sketch and creates complete programs that generate various network structures.
%
As the last step, the algorithm extracts the top-$k$ network structures according to their approximated costs (Line 12).
%
Both the sketch completion algorithm and the cost computation are elaborated in \cref{sec:algo:fillhole}.

\begin{algorithm}[t]
\small
\captionsetup{font=small} % set size of caption font
\caption{The high-level program synthesis algorithm}\label{alg:synth}
\begin{algorithmic}[1]
    % \Require The data tensor $\ten{T}$, the error bound $\error$, the param $k$
    % \Ensure Networks $Gs$ such that $\norm{\net{G}-\ten{T}} \leq \error \norm{\ten{T}}$
    \Function{Synth}{$\ten{T}, \error, k$}
        \State $\sketch s, Gs \gets \{\emptyprog\}, \emptyset$
        \For{$\sketch \in Ss$} %\Comment{Compute all sketch structures}
            \For{$\expr \in \validexpr(\ten{T}, \sketch)$}
                \For{$P \in \fillholes(\seq{\sketch}{\expr}, \error)$}
                    \State $(G, \error') \gets \sem{P}(G_0, \error)$
                    \State $Gs \gets Gs \cup \{(G, \error')\}$
                \EndFor
                \State $Ss \gets Ss \cup \{\seq{\sketch}{\expr}\}$
            \EndFor
        \EndFor
        \State \Return{\Call{TopK}{$Gs, k$}}
    \EndFunction
\end{algorithmic}
\end{algorithm}

\subsection{Programs with Output-Directed Splits}\label{sec:algo:split}
%
\paragraph{Output-Directed Splits}
%
An output-directed split expression $\osplit(\ind, r)$ takes two arguments: a partition block $\ind$ of free indices from the data tensor and a target rank $r$.
%
The semantic of this expression is as follows: given a tensor network $G$ and an error bound $\error$, the expression transforms $G$ by splitting a node into two.
%
The resulting edge connecting the two new nodes forms a free index partition that includes the block $\ind$.
%
\cref{fig:output-directed-example} provides a step-by-step walk-through of the execution of output-directed splits from the program shown in \cref{fig:example-program}.
%
In step \textcircled{\scriptsize 1}, the split expression aims to create a partition where one block is $\{I_1\}$.
%
Since the current network consists of a single node $\ten{T}$, it is picked and split into $\ten{T}_1$ and $\ten{T}_2$.
%
A new edge labelled $r_1$ is created between them, dividing the free indices into two disjoint sets $\{I_1\}$ and $\{I_2, I_3, I_4\}$ and thereby satisfying the split requirement.
%
In step \textcircled{\scriptsize 2}, the goal is to form a new partition block $\{I_1, I_2\}$.
%
The execution procedure discovers that splitting $\ten{T}_2$ and isolating $I_2$ and $r_1$ fulfills the requirement.
%
Finally, step \textcircled{\scriptsize 3} expects to further split a previously created partition block.
%
Splitting the node $\ten{T}_3$ at the index $I_2$ meets the expectation.

The key idea of executing output-directed splits is to convert $\osplit(\ind, r)$ into equivalent, natural node splits $\isplit(\ten{T}, \ind_s, r)$, which takes the node name as arguments and we call them \emph{input-directed splits}.
%
Execution of an input-directed split is built on truncated singular value decomposition (SVD) with minor modifications.
%
Due to the space limitation, we leave the details of output- and input-directed splits to \cref{sec:appendix:alg}.
%
Note that the execution of output-directed splits may fail if the combination  is invalid.
%
For instance, $\osplit(\{I_1, I_2\}, r)$ followed by $\osplit(\{I_1, I_3\}, r)$ is invalid as we cannot put the index $I_1$ in two partition blocks if one is not a subset of the other.
%
%
During program synthesis, the function \validexpr\ in~\cref{alg:synth} takes charge of filtering out invalid combinations to avoid failures.
%

\begin{figure}[t]
    \centering
    \resizebox{0.85\linewidth}{!}{
    \input{figs/split_example_step_1}
    }
    \resizebox{0.95\linewidth}{!}{
    \input{figs/split_example_step_2}
    }
    \resizebox{\linewidth}{!}{
    \input{figs/split_example_step_3}
    }
    % \input{figs/split_example_step_4}
    \caption{Execution of output-directed splits from \cref{fig:example-program}. Nodes involved in split operations are highlighted in purple.}
    \label{fig:output-directed-example}
\end{figure}

\begin{figure}[t]
    \centering
    \resizebox{\linewidth}{!}{
    \input{figs/suboptimal_step}
    }
    \caption{The problem of suboptimality in input-directed splits: the resulting structure of this operation is suboptimal.}
    \label{fig:suboptimal}
\end{figure}

\begin{figure*}[t]
    \centering
    \begin{minipage}{.45\linewidth}
    \centering
    \resizebox{.9\linewidth}{!}{
    \input{figs/input_split_step11}
    }
    \resizebox{\linewidth}{!}{
    \input{figs/input_split_step12}
    }
    \end{minipage}
    \hfill
    \vline
    \hfill
    \begin{minipage}{.45\linewidth}
    \centering
    \resizebox{.9\linewidth}{!}{
    \input{figs/input_split_step21}
    }
    \resizebox{\linewidth}{!}{
    \input{figs/input_split_step22}
    }
    \end{minipage}
    \caption{The problem of redundancy in input-directed splits: two different sequences of input-directed splits result in the same network structure, and reasoning of their equivalence is complicated.}
    \label{fig:input-splits}
\end{figure*}


\paragraph{Output- versus Input-Directed Splits}
%
Output-directed splits have advantages over input-directed splits in several aspects.
%
First, output-directed splits create a more succinct search space while maintaining the same level of expressiveness.
%
They exclude many suboptimal structures produced by input-directed splits, as shown in~\cref{fig:suboptimal}.
%
This structure consists of two edges $r_1$ and $r_2$ corresponding to the same partition of free indices, which makes the structure unlikely to be an optimal one because the middle node $\ten{T}_4$ is redundant and can be merged with $\ten{T}_3$ or $\ten{T}_2$ to get a more compact structure.
%
%
Second, input-directed splits result in an infinite search space because one can keep splitting the new node introduced by a previous split operation and get a fresh topology.
%
This is problematic and a boundary limit is needed to make the search algorithm tractable.
%
Third, different orders of input-directed splits may produce the same structure, and it is challenging to reason about their equivalence without execution.
%
For example, \cref{fig:input-splits} (left) and (right) are two programs of input-directed splits that lead to the same structure.
%
It is difficult to tell these two programs are equivalent without sophisticated analysis.
%
However, the equivalence of programs with output-directed splits is simple because programs of the same set of splits are equivalent.

\begin{theorem}[Completeness of Output-Directed Splits]\label{thm:completeness}
If $G$ is the optimal tree tensor network for a tensor $\ten{T}$ and an error bound $\error$,
then there exists a program with output-directed splits $P$ that $\sem{P}(G_0, \error) = G$ where $G_0 = (\{\ten{T}\},\emptyset)$.
\end{theorem}

\subsection{Synthesis of Transformation Programs}\label{sec:algo:fillhole}
%
As mentioned in~\cref{sec:algo:program}, our synthesis algorithm consists of two components: sketch generation and sketch completion.
%
In the phase of sketch generation, the algorithm exhaustively enumerates all semantically correct sketches.
%
The second component, sketch completion, is particularly interesting in the context of tensor networks.
%
Given a sketch $\sketch$, an initial tensor network $G_0$, and an error bound $\error$, the goal of sketch completion is to find a rank assignment $\many{\hole_i \mapsto r_i}$  such that the program execution succeeds after the completion, \ie $\sem{\sketch[\many{\hole_i \mapsto r_i}]}(G_0, \error) \not= \bot$.
%
Through sketch completion, we hope to discover which sketches are more promising to produce tensor network structures with high compression ratio.
%
Achieving this requires calculating the optimal rank assignment for each sketch, but trying all possible rank assignments is computationally prohibitive.
%
An alternative approach is to compress each sketch by distributing errors equally between splits, as done in TT or HT.
%
This approach sacrifices the compression quality for faster speed, but it also suffers from performance issues when dealing with large tensors or small error bounds as we show in \cref{sec:eval:ablation}.
%
To address this challenge, we propose to encode the sketch completion task as integer programming constraints.
%
By leveraging the efficiency of constraint solvers, we can quickly estimate the optimal rank assignment without the need for costly program executions.

\paragraph{Truncation Algorithm}
%
Before detailing the encoding for sketch completion, we illustrate how to find valid rank assignments for a hole.
%
Given a data tensor $\ten{T}$ and an error bound $\error$, completions of the hole in $\osplit(\ind_s, \hole)$ are computed by truncated SVD.
%
Specifically, after converting $\osplit(\ind_s, \hole)$ into an input-directed split, we apply SVD to compute singular values $\Sigma$ for the splitting node, where $\Sigma = \many{\sigma_i}$ and $\sigma_1 \leq \sigma_2 \leq \cdots \leq \sigma_{\size{\Sigma}}$.
%
A valid assignment $\hole \mapsto (\size{\Sigma} - r)$ is constructed if $\sum_{i=1}^{r} \sigma_{i}^2 \leq (\error \norm{\ten{T}})^2$.
%
Truncated SVD achieves a low-rank approximation of the original tensor by discarding the $r$ smallest singular values.
%
This connection between discarded singular values and the error bound forms the foundation of our sketch completion method.

\paragraph{Sketch Completion Using Integer Programming}
%
Given a data tensor $\ten{T}$, an error bound $\error$, and a sketch $\sketch = \many{\osplit(\ind_s, \hole_s)}$, we formulate the problem of finding the optimal rank assignment within the error bound as an integer programming problem.
%
Each hole in the sketch is assigned an integer variable $r_s$.
%
The network cost and the error bound requirements are encoded as follows:
\begin{equation}
\begin{aligned}
    \min_{r_1, r_2, \ldots, r_s} &\size{\sem{\sketch[\many{\subst{\hole_s}{(\size{\Sigma_s} - r_s)}}]}(G_0, \error)} \\
    \text{s.t.} & \quad \sum_{i=1}^{s}\sum_{j=1}^{r_i} \sigma_{ij}^2 \leq \left(\error\norm{\ten{T}}\right)^2
\end{aligned}
\label{eqn:ip}
\end{equation}
Here, $\sigma_{ij}$ is the $j$th smallest singular values at the $i$th split, $G_0$ is the initial tensor network containing the input data tensor.
%
The constraint accumulates the truncation errors, \ie the sum of truncated singular values, from all split operations, guaranteeing that the total error remains within the specified error bound.
%
The objective minimizes the symbolic representation of the network cost and asks the solver to find an optimal solution.
%
However, this encoding is not directly feasible because the singular values for a split operation cannot be determined until all preceding ones are completed.
%
Earlier truncation affect the singular values of subsequent split operations.
%
To overcome this challenge, we utilize the following property:
\begin{theorem}[Upper Bound of Singular Values]\label{thm:rank-bound}
Let $\ten{T} \in \real^{n_1 \times \cdots \times n_d}$ be a $d$-dimensional tensor with free indices $I_1, I_2, \ldots, I_d$, and $G=(\{\ten{T}\},\emptyset)$ be the graph with a single tensor.
%
If a complete program $P = \osplit(\ind_1, r_1); \ldots; \osplit(\ind_n, r_n)$ such that $\sem{P}(G, \error) = (G_n, \error_n)$,
then for every $1 \leq i, s \leq n$, we have $\sigma_j(\net{G_{i-1}}_{\langle\ind_s\rangle}) \leq \sigma_j(\matric{T}{\ind_s})$ where $\sigma_j(A)$ is the $j$th smallest singular value of the matrix $A$.
\end{theorem}

In other words, for each index partition $\ind_s$ of the data tensor, the singular values at this partition in the original tensor provide an upper bound for the singular values of any truncated tensor at the same partition.
%
This theorem allows us to approximate singular values for all splits, which enables us to calculate an upper-bound cost through \cref{eqn:ip}.

\begin{theorem}[Upper Bound of Costs]\label{thm:cost-bound}
Given a data tensor $\ten{T}$ and an error bound $\error$, if \cref{eqn:ip} with upper bounded singular values yields $r_1, \ldots, r_n$ for a sketch program $\osplit(\indices_1, \hole_1); \ldots; \osplit(\indices_n, \hole_n)$, then $\many{\hole_i \mapsto (\Sigma_i - r_i)}$ is a valid completion.
\end{theorem}

This rank assignment is not only used to select the promising sketches, but also serves as the initial completion of the holes, which is followed by a rounding operation that further compresses the network structure~\cite{ht}.
