[
  {
    "index": 0,
    "papers": [
      {
        "key": "antol2015vqa",
        "author": "Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Zitnick, C Lawrence and Parikh, Devi",
        "title": "Vqa: Visual question answering"
      },
      {
        "key": "zhao2018sound",
        "author": "Zhao, Hang and Gan, Chuang and Rouditchenko, Andrew and Vondrick, Carl and McDermott, Josh and Torralba, Antonio",
        "title": "The sound of pixels"
      },
      {
        "key": "zhao2019sound",
        "author": "Zhao, Hang and Gan, Chuang and Ma, Wei-Chiu and Torralba, Antonio",
        "title": "The sound of motions"
      },
      {
        "key": "tian2020unified",
        "author": "Tian, Yapeng and Li, Dingzeyu and Xu, Chenliang",
        "title": "Unified multisensory perception: Weakly-supervised audio-visual video parsing"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "yun2021pano",
        "author": "Yun, Heeseung and Yu, Youngjae and Yang, Wonsuk and Lee, Kangil and Kim, Gunhee",
        "title": "Pano-avqa: Grounded audio-visual question answering on 360deg videos"
      },
      {
        "key": "yang2022avqa",
        "author": "Yang, Pinci and Wang, Xin and Duan, Xuguang and Chen, Hong and Hou, Runze and Jin, Cong and Zhu, Wenwu",
        "title": "Avqa: A dataset for audio-visual question answering on videos"
      },
      {
        "key": "diao2023av",
        "author": "Diao, Xingjian and Cheng, Ming and Cheng, Shitong",
        "title": "Av-maskenhancer: Enhancing video representations through audio-visual masked autoencoder"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "li2022learning",
        "author": "Li, Guangyao and Wei, Yake and Tian, Yapeng and Xu, Chenliang and Wen, Ji-Rong and Hu, Di",
        "title": "Learning to answer questions in dynamic audio-visual scenarios"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "lin2023vision",
        "author": "Lin, Yan-Bo and Sung, Yi-Lin and Lei, Jie and Bansal, Mohit and Bertasius, Gedas",
        "title": "Vision transformers are parameter-efficient audio-visual learners"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "dosovitskiy2020image",
        "author": "Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others",
        "title": "An image is worth 16x16 words: Transformers for image recognition at scale"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "duan2024cross",
        "author": "Duan, Haoyi and Xia, Yan and Mingze, Zhou and Tang, Li and Zhu, Jieming and Zhao, Zhou",
        "title": "Cross-modal Prompts: Adapting Large Pre-trained Models for Audio-Visual Downstream Tasks"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "antol2015vqa",
        "author": "Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Zitnick, C Lawrence and Parikh, Devi",
        "title": "Vqa: Visual question answering"
      },
      {
        "key": "lei2018tvqa",
        "author": "Lei, Jie and Yu, Licheng and Bansal, Mohit and Berg, Tamara L",
        "title": "Tvqa: Localized, compositional video question answering"
      },
      {
        "key": "yu2019activitynet",
        "author": "Yu, Zhou and Xu, Dejing and Yu, Jun and Yu, Ting and Zhao, Zhou and Zhuang, Yueting and Tao, Dacheng",
        "title": "Activitynet-qa: A dataset for understanding complex web videos via question answering"
      },
      {
        "key": "garcia2020knowit",
        "author": "Garcia, Noa and Otani, Mayu and Chu, Chenhui and Nakashima, Yuta",
        "title": "KnowIT VQA: Answering knowledge-based questions about videos"
      },
      {
        "key": "ravi2023vlc",
        "author": "Ravi, Sahithya and Chinchure, Aditya and Sigal, Leonid and Liao, Renjie and Shwartz, Vered",
        "title": "VLC-BERT: visual question answering with contextualized commonsense knowledge"
      },
      {
        "key": "yu2024self",
        "author": "Yu, Shoubin and Cho, Jaemin and Yadav, Prateek and Bansal, Mohit",
        "title": "Self-chained image-language model for video localization and question answering"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "fayek2020temporal",
        "author": "Fayek, Haytham M. and Johnson, Justin",
        "title": "Temporal reasoning via audio question answering"
      },
      {
        "key": "lipping2022clotho",
        "author": "Lipping, Samuel and Sudarsanam, Parthasaarathy and Drossos, Konstantinos and Virtanen, Tuomas",
        "title": "Clotho-aqa: A crowdsourced dataset for audio question answering"
      },
      {
        "key": "sudarsanam2023attention",
        "author": "Sudarsanam, Parthasaarathy and Virtanen, Tuomas",
        "title": "Attention-Based Methods For Audio Question Answering"
      },
      {
        "key": "li2023multi",
        "author": "Li, Guangyao and Xu, Yixin and Hu, Di",
        "title": "Multi-Scale Attention for Audio Question Answering"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "yun2021pano",
        "author": "Yun, Heeseung and Yu, Youngjae and Yang, Wonsuk and Lee, Kangil and Kim, Gunhee",
        "title": "Pano-avqa: Grounded audio-visual question answering on 360deg videos"
      },
      {
        "key": "li2022learning",
        "author": "Li, Guangyao and Wei, Yake and Tian, Yapeng and Xu, Chenliang and Wen, Ji-Rong and Hu, Di",
        "title": "Learning to answer questions in dynamic audio-visual scenarios"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "yun2021pano",
        "author": "Yun, Heeseung and Yu, Youngjae and Yang, Wonsuk and Lee, Kangil and Kim, Gunhee",
        "title": "Pano-avqa: Grounded audio-visual question answering on 360deg videos"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "li2022learning",
        "author": "Li, Guangyao and Wei, Yake and Tian, Yapeng and Xu, Chenliang and Wen, Ji-Rong and Hu, Di",
        "title": "Learning to answer questions in dynamic audio-visual scenarios"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "lin2023vision",
        "author": "Lin, Yan-Bo and Sung, Yi-Lin and Lei, Jie and Bansal, Mohit and Bertasius, Gedas",
        "title": "Vision transformers are parameter-efficient audio-visual learners"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "duan2024cross",
        "author": "Duan, Haoyi and Xia, Yan and Mingze, Zhou and Tang, Li and Zhu, Jieming and Zhao, Zhou",
        "title": "Cross-modal Prompts: Adapting Large Pre-trained Models for Audio-Visual Downstream Tasks"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "liu2024tackling",
        "author": "Liu, Xiulong and Dong, Zhikang and Zhang, Peng",
        "title": "Tackling Data Bias in MUSIC-AVQA: Crafting a Balanced Dataset for Unbiased Question-Answering"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "zhuang2023toolqa",
        "author": "Zhuang, Yuchen and Yu, Yue and Wang, Kuan and Sun, Haotian and Zhang, Chao",
        "title": "Toolqa: A dataset for llm question answering with external tools"
      },
      {
        "key": "wang2024healthq",
        "author": "Ziyu Wang and Hao Li and Di Huang and Amir M. Rahmani",
        "title": "HealthQ: Unveiling Questioning Capabilities of LLM Chains in Healthcare Conversations"
      },
      {
        "key": "saito2024unsupervised",
        "author": "Saito, Kuniaki and Sohn, Kihyuk and Lee, Chen-Yu and Ushiku, Yoshitaka",
        "title": "Unsupervised llm adaptation for question answering"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "huang2024audiogpt",
        "author": "Huang, Rongjie and Li, Mingze and Yang, Dongchao and Shi, Jiatong and Chang, Xuankai and Ye, Zhenhui and Wu, Yuning and Hong, Zhiqing and Huang, Jiawei and Liu, Jinglin and others",
        "title": "Audiogpt: Understanding and generating speech, music, sound, and talking head"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "shen2024hugginggpt",
        "author": "Shen, Yongliang and Song, Kaitao and Tan, Xu and Li, Dongsheng and Lu, Weiming and Zhuang, Yueting",
        "title": "Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "copet2024simple",
        "author": "Copet, Jade and Kreuk, Felix and Gat, Itai and Remez, Tal and Kant, David and Synnaeve, Gabriel and Adi, Yossi and D{\\'e}fossez, Alexandre",
        "title": "Simple and controllable music generation"
      },
      {
        "key": "dai2022missing",
        "author": "Dai, Shuqi and Yu, Huiran and Dannenberg, Roger B",
        "title": "What is missing in deep music generation? a study of repetition and structure in popular music"
      },
      {
        "key": "agostinelli2023musiclm",
        "author": "Agostinelli, Andrea and Denk, Timo I and Borsos, Zal{\\'a}n and Engel, Jesse and Verzetti, Mauro and Caillon, Antoine and Huang, Qingqing and Jansen, Aren and Roberts, Adam and Tagliasacchi, Marco and others",
        "title": "Musiclm: Generating music from text"
      },
      {
        "key": "lu2023musecoco",
        "author": "Lu, Peiling and Xu, Xin and Kang, Chenfei and Yu, Botao and Xing, Chengyi and Tan, Xu and Bian, Jiang",
        "title": "Musecoco: Generating symbolic music from text"
      }
    ]
  }
]