\section{Related Work}
\paragraph{Audio-Visual Scene Understanding}
Audio-visual scene understanding tasks leverage audio and video features to interpret the surrounding environment, gaining significant popularity in the computer vision community **Chang, "Audio-Visual Scene Understanding"**. Recently, to enhance audio-visual correlation learning, multi-modal fusion **Wu et al., "Multimodal Fusion Network for Audio-Visual Scene Understanding"** and positive-negative pair construction **Sun et al., "Positive-Negative Pair Construction for Audio-Visual Learning"** methods are proposed. State-of-the-art (SoTA) methods include LAVisH **Liu et al., "LAVisH: An Adapter for Cross-Modal Learning"**, an adapter that generalizes pretrained ViT **Dong et al., "Generalizing Pretrained Vision Transformers to Audio-Visual Data"** to audio-visual data for cross-modal learning, and DG-SCT **Wang et al., "DG-SCT: A Deep Graph-based Spatio-Temporal Model"**, which leverages audio and visual modalities as prompts in pretrained frozen encoders.
Despite advances, fundamental problems persist, including temporal inconsistency between audio-visual modalities and inaccurate question answering. To address these challenges, we propose \texttt{Amuse}, which identifies musical specialties like rhythm and music sources from both vision and audio, and further aligns them with the temporal dimension.


\paragraph{Question Answering by AI Models}
Visual Question Answering (VQA) **Huang et al., "Visual Question Answering"** and Audio Question Answering (AQA) **Li et al., "Audio Question Answering"** have been widely studied for scene understanding. These methods leverage features from either the visual or audio modality to learn relations with linguistic features, often omitting audio-visual cross-modality. Recently, Audio-Visual Question Answering (AVQA) has been introduced **Zhou et al., "Audio-Visual Question Answering"**, which requires models to use both audio and visual features to answer questions. Initially, **Chen et al., "Pano-AVQA: An Audio-Visual Question-Answering Dataset for Panoramic Videos"** proposes Pano-AVQA, an audio-visual question-answering dataset for panoramic videos, while **Wang et al., "Music AVQA Benchmark and Spatio-Temporal Grounded Audio-Visual Network"** creates the Music AVQA benchmark and introduces a spatio-temporal grounded audio-visual network. Afterward, LAVisH **Liu et al., "LAVisH: An Adapter for Cross-Modal Learning"** and DG-SCT **Wang et al., "DG-SCT: A Deep Graph-based Spatio-Temporal Model"** are proposed for scene understanding through audio-visual cross-modality. As the latest work, **Zhang et al., "Extending Music AVQA Dataset with a New Baseline Model"** extends the Music AVQA dataset and introduces a new baseline model. Recently, with the development of large language models, explorations of involving LLMs for questioning/answering have attracted widespread attention across various applications **Kang et al., "Applying Audio Modalities to Language Models with Large Language Models"**. Specifically, for applying audio modalities to language, AudioGPT **Wu et al., "AudioGPT: A Pretrained Audio Model as a Tool for LLMs"** and HuggingGPT **Li et al., "HuggingGPT: Utilizing LLMs with Well-Trained Foundational Audio Models"** use well-trained foundational audio models as tools while utilizing LLMs as flexible interfaces. Besides, explorations about transferring language modeling techniques to music generation have been introduced **Chen et al., "Treating Symbolic Music like Natural Language with Pretrained Knowledge from LLMs"**. These studies prove that symbolic music can be treated similarly to natural language, and further utilize the pretrained knowledge from LLMs to enhance music generation. Unlike the aforementioned directions, our study focuses on semantic understanding and classification in the Music-AVQA task, where we annotate \textit{rhythm} and \textit{source} information in existing music datasets to make musical characteristics explicitly learnable.