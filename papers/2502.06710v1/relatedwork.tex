\section{Related Work}
\paragraph{Audio-Visual Scene Understanding}
Audio-visual scene understanding tasks leverage audio and video features to interpret the surrounding environment, gaining significant popularity in the computer vision community \cite{antol2015vqa, zhao2018sound, zhao2019sound, tian2020unified}. Recently, to enhance audio-visual correlation learning, multi-modal fusion \cite{yun2021pano, yang2022avqa, diao2023av} and positive-negative pair construction \cite{li2022learning} methods are proposed. State-of-the-art (SoTA) methods include LAVisH \cite{lin2023vision}, an adapter that generalizes pretrained ViT \cite{dosovitskiy2020image} to audio-visual data for cross-modal learning, and DG-SCT \cite{duan2024cross}, which leverages audio and visual modalities as prompts in pretrained frozen encoders.
Despite advances, fundamental problems persist, including temporal inconsistency between audio-visual modalities and inaccurate question answering. To address these challenges, we propose \texttt{Amuse}, which identifies musical specialties like rhythm and music sources from both vision and audio, and further aligns them with the temporal dimension.


\paragraph{Question Answering by AI Models}
Visual Question Answering (VQA) \cite{antol2015vqa, lei2018tvqa, yu2019activitynet, garcia2020knowit, ravi2023vlc, yu2024self} and Audio Question Answering (AQA) \cite{fayek2020temporal, lipping2022clotho, sudarsanam2023attention, li2023multi} have been widely studied for scene understanding. These methods leverage features from either the visual or audio modality to learn relations with linguistic features, often omitting audio-visual cross-modality. Recently, Audio-Visual Question Answering (AVQA) has been introduced \cite{yun2021pano, li2022learning}, which requires models to use both audio and visual features to answer questions. Initially, \citet{yun2021pano} proposes Pano-AVQA, an audio-visual question-answering dataset for panoramic videos, while \citet{li2022learning} creates the Music AVQA benchmark and introduces a spatio-temporal grounded audio-visual network. Afterward, LAVisH \cite{lin2023vision} and DG-SCT \cite{duan2024cross} are proposed for scene understanding through audio-visual cross-modality. As the latest work, \citet{liu2024tackling} extends the Music AVQA dataset and introduces a new baseline model. Recently, with the development of large language models, explorations of involving LLMs for questioning/answering have attracted widespread attention across various applications \cite{zhuang2023toolqa, wang2024healthq, saito2024unsupervised}. Specifically, for applying audio modalities to language, AudioGPT \cite{huang2024audiogpt} and HuggingGPT \cite{shen2024hugginggpt} use well-trained foundational audio models as tools while utilizing LLMs as flexible interfaces. Besides, explorations about transferring language modeling techniques to music generation have been introduced \cite{copet2024simple, dai2022missing, agostinelli2023musiclm, lu2023musecoco}. These studies prove that symbolic music can be treated similarly to natural language, and further utilize the pretrained knowledge from LLMs to enhance music generation. Unlike the aforementioned directions, our study focuses on semantic understanding and classification in the Music-AVQA task, where we annotate \textit{rhythm} and \textit{source} information in existing music datasets to make musical characteristics explicitly learnable.