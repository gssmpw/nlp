\section{Related Work}
\paragraph{Audio-Visual Scene Understanding}
Audio-visual scene understanding tasks leverage audio and video features to interpret the surrounding environment, gaining significant popularity in the computer vision community ____. Recently, to enhance audio-visual correlation learning, multi-modal fusion ____ and positive-negative pair construction ____ methods are proposed. State-of-the-art (SoTA) methods include LAVisH ____, an adapter that generalizes pretrained ViT ____ to audio-visual data for cross-modal learning, and DG-SCT ____, which leverages audio and visual modalities as prompts in pretrained frozen encoders.
Despite advances, fundamental problems persist, including temporal inconsistency between audio-visual modalities and inaccurate question answering. To address these challenges, we propose \texttt{Amuse}, which identifies musical specialties like rhythm and music sources from both vision and audio, and further aligns them with the temporal dimension.


\paragraph{Question Answering by AI Models}
Visual Question Answering (VQA) ____ and Audio Question Answering (AQA) ____ have been widely studied for scene understanding. These methods leverage features from either the visual or audio modality to learn relations with linguistic features, often omitting audio-visual cross-modality. Recently, Audio-Visual Question Answering (AVQA) has been introduced ____, which requires models to use both audio and visual features to answer questions. Initially, ____ proposes Pano-AVQA, an audio-visual question-answering dataset for panoramic videos, while ____ creates the Music AVQA benchmark and introduces a spatio-temporal grounded audio-visual network. Afterward, LAVisH ____ and DG-SCT ____ are proposed for scene understanding through audio-visual cross-modality. As the latest work, ____ extends the Music AVQA dataset and introduces a new baseline model. Recently, with the development of large language models, explorations of involving LLMs for questioning/answering have attracted widespread attention across various applications ____. Specifically, for applying audio modalities to language, AudioGPT ____ and HuggingGPT ____ use well-trained foundational audio models as tools while utilizing LLMs as flexible interfaces. Besides, explorations about transferring language modeling techniques to music generation have been introduced ____. These studies prove that symbolic music can be treated similarly to natural language, and further utilize the pretrained knowledge from LLMs to enhance music generation. Unlike the aforementioned directions, our study focuses on semantic understanding and classification in the Music-AVQA task, where we annotate \textit{rhythm} and \textit{source} information in existing music datasets to make musical characteristics explicitly learnable.