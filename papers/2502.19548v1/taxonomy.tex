
\tikzstyle{my-box}=[
    rectangle,
    draw=hidden-draw,
    rounded corners,
    text opacity=1,
    minimum height=1.5em,
    minimum width=5em,
    inner sep=2pt,
    align=center,
    fill opacity=.5,
    line width=0.8pt,
]

\tikzstyle{leaf}=[my-box, minimum height=1.5em,
    fill=bblue!40, text=black, align=left,font=\normalsize,
    inner xsep=2pt,
    inner ysep=4pt,
    line width=0.8pt,
    draw=black,
]

\begin{figure*}[t!]
    \centering
    \resizebox{1.0\textwidth}{!}{
        \begin{forest}
            forked edges,
            for tree={
                grow'=0,
                draw,
                reversed=true,
                anchor=base west,
                parent anchor=east,
                child anchor=west,
                base=left,
                font=\large,
                rectangle,
                rounded corners,
                align=left,
                minimum width=4em,
                edge+={darkgray, line width=1pt},
                s sep=3pt,
                inner xsep=2pt,
                inner ysep=3pt,
                line width=0.8pt,
                ver/.style={rotate=90, child anchor=north, parent anchor=south, anchor=center},
            },
            where level=1{text width=11.5em,font=\normalsize,}{},
            where level=2{text width=18em,font=\normalsize,}{},
            where level=3{text width=8em,font=\normalsize,}{},
            where level=4{text width=5em,font=\normalsize,}{},
			[
			Speech-LLM Integration, ver
			    [
			      Text-based \textcolor{blueref}{(Sec.~\ref{sec:text-based-integration})}
			        [
			        Cascaded Integration \textcolor{blueref}{(Sec.~\ref{sec:cascaded-integration})}
    			        [
                             AudioGPT \cite{huang-2024-audio-gpt}{, } HuggingGPT \cite{shen-2023-hugginggpt}{, }\citet{dighe-2024-uncertainty}
                            , leaf, text width=46em
    			        ]
			        ]
			        [
		            LLM Rescoring \textcolor{blueref}{(Sec.~\ref{sec:llm-rescoring})}
			            [  
                            \citet{chen-2023-longform}{, }\citet{udagawa-2022-rescoring}{, }\citet{li-2023-asr-domain}{, }\citet{hu-2023-shallow-fusion}{, }\citet{huang-2024-fusion-comprehensive}
                            , leaf, text width=46em
			            ]
			        ]
                    [
                    LLM GER \textcolor{blueref}{(Sec.~\ref{sec:llm-ger})}
			            [  
                            HyPoradise \cite{chen-2023-hyporadise}{, }\citet{yang-2023-tap}{, }GenTranslate \cite{hu-etal-2024-gentranslate}{, }\citet{ma-2023-error-correction}{, }\\\citet{hu-2024-noise-robust}{, }Whispering Llama \cite{radhakrishnan2023whispering}{, }MMGER \cite{mu2024mmger}{, }\\\citet{yang2024large}{, }\citet{ko2024benchmarking}{, }LipGER~\cite{ghosh2024lipger}{, }\citet{li2024investigating}{, }\citet{tang2024contextualized} {, }\\ \citet{chen2024s} {, }\citet{dighe2024leveraging}
                            , leaf, text width=46em
			            ]
                    ]
			    ]
                [
                Latent-representation-based\\ \textcolor{blueref}{(Sec.~\ref{sec:latent-representation-based-integration})}
                    [
                    Convolutional Downsampling \textcolor{blueref}{(Sec.~\ref{sec:downsampling})}
                        [
                        \citet{hono-2023-integration}{, }\citet{yu-2024-connecting}{, }\citet{fathullah-2024-prompting}{, }LTU \cite{gong-2024-ltu}{, }\citet{pham-2024-comprehensive}{, }\\BLSP \cite{wang2023blsp}{, }SALM \cite{chen-2024-salm}{, }WavLLM \cite{hu-2024-wavllm}{, }DeSTA \cite{lu-2024-desta}{, }\\Llama-Omni \cite{fang-etal-2024-llamaomni}{, }Seed-ASR \cite{bai2024seed}{, }SpeechVerse \cite{das2024speechverse}{, }\\SLAM-ASR \cite{ma2024embarrassingly}{, }\citet{denisov2024teaching} {, }MaLa-ASR~\cite{yang2024mala}{, }\\WHISMA~\cite{li2024whisma}{, }\citet{lakomkin2024end}{, }\citet{noroozi2024instruction}{, }\citet{kang2024prompting}{, }\\ \citet{li2024using} {, }\citet{chen2024s}
                        , leaf, text width=46em
                        ]
                    ]
                    [
                    CTC Compression \textcolor{blueref}{(Sec.~\ref{sec:ctc-compression})}
                        [
                        \citet{hono-2023-integration}{, }Speech-Llama \cite{wu-2023-decoder-only}{, }\citet{ling-2024-fully-formatted}{, } \citet{fathullah2023towards}
                        , leaf, text width=46em
                        ]
                    ]
                    [
                    Q-Former \textcolor{blueref}{(Sec.~\ref{sec:qformer})}
                        [
                        \citet{yu-2024-connecting}{, }SALMONN \cite{tang-etal-2024-salmonn}{, }COSMIC \cite{pan-2023-cosmic}{, }XLLM \cite{chen-2023-xllm}{, }\\DeSTA \cite{lu-2024-desta}{, }Secap \cite{xu2024secap}{, }BLSP-KD \cite{wang2024blsp}{, } CoT-ST~\cite{du2024cot} 
                        , leaf, text width=46em
                        ]
                    ]
                    [
                    Other Adaptation Methods
                        [
                        SLM \cite{wang-2023-slm}{, } Qwen-Audio \cite{chu-2023-qwen-audio}{, } Qwen2-Audio \cite{chu-2024-qwen2-audio}{, } \\LauraGPT \cite{du-etal-2024-lauragpt}{, } LLaST \cite{chen2024llast}{, } BESTOW \cite{chen2024bestow}{, }\\ SALSA \cite{mittal2024salsa}{, } Ideal-LLM~\cite{xue2024ideal} {, }\citet{xu2024comparing}{, }\citet{chen2024s}
                        , leaf, text width=46em
                        ]
                    ]
                ]
			    [
		          Audio-token-based \textcolor{blueref}{(Sec.~\ref{sec:audio-token-based-integration})}
			        [
			        Semantic Tokens \textcolor{blueref}{(Sec.~\ref{sec:semantic-token})}
			            [
                            VoxtLM \cite{maiti-etal-2024-voxtlm}{, }SpeechGPT \cite{zhang-etal-2023-speechgpt}{, }TWIST \cite{hassid-etal-2023-twist}{, }\\Spirit-LM \cite{nguyen-etal-2025-spiritlm}{, } GPT-Talker \cite{liu-2024-gpt-talker}{, } BASE TTS \cite{lajszczak2024base}{, }\\Llama-Omni \cite{fang-etal-2024-llamaomni}{, }SpeechGPT-Gen \cite{zhang-etal-2024-speechgptgen}{, }  DiscreteSLU \cite{shon2024discreteslu}{, }\\ PSLM~\cite{mitsui2024pslm}{, }\citet{cheng2024task}{, }VSP-LLM~\cite{yeo2024visual}{, }\citet{xu2024comparing}
			            , leaf, text width=46em
			            ]
			        ]
			        [
                    Acoustic Tokens \textcolor{blueref}{(Sec.~\ref{sec:acoustic-token})}
			            [
                            \citet{hao-2024-boosting}{, } LauraGPT \cite{du-etal-2024-lauragpt}{, }\citet{neekhara2024improving}{, }Uniaudio~\cite{yang2023uniaudio}
			              , leaf, text width=46em
			            ]
			        ]
			        [
                    Semantic and Acoustic Tokens \textcolor{blueref}{(Sec.~\ref{sec:semantic-and-acoustic})}
			            [
                            AudioPaLM \cite{rubenstein-etal-2023-audiopalm}{, }Moshi \cite{defossez-etal-2024-moshi}{, }Hibiki~\cite{labiausse-etal-2025-hibiki}{, }\\ Emova~\cite{chen2024emova}{, }\citet{shen2024get}
			              , leaf, text width=46em
			            ]
			        ]
			    ]
			]
        \end{forest}
    }
    \caption{Taxonomy for speech-LLM integration. ``Latent-representation-based'' integration is categorized by different modality adaptation strategies. ``Other Adaptation Methods'' also includes studies that do not explicitly mention their modality adaptation methods. }
    \label{fig:taxonomy}
\end{figure*}
