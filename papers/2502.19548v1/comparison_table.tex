\begin{table*}[t]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{cccccc}
    \toprule
    \textbf{Task} & \makecell[c]{\textbf{Dataset}} & \textbf{Metrics} & \textbf{Model}  & \textbf{Integration Method} & \textbf{Performance} \\
    \midrule
    \multirow{22}{*}{ASR} & \multirow{12}{*}{\makecell[c]{\textbf{Librispeech}\\ \small \textit{dev-clean} | \textit{dev-other} | \\ \small \textit{test-clean} | \textit{test-other}}} & \multirow{12}{*}{WER $\downarrow$} &  Whisper-large-v2 \cite{radford-etal-2023-whisper} & Non-LLM  & --- | --- | 2.7 | 5.2 \\
    &&& HyPoradise \cite{chen-2023-hyporadise} & Text-based $\rightarrow$ LLM GER & --- | --- | 1.8 | 3.7 \\
    &&&  BLSP \cite{wang2023blsp} & Latent-representation-based $\rightarrow$ Convolutional Downsampling  & --- | --- | 10.4 | --- \\
    &&&  SpeechVerse \cite{das2024speechverse} & Latent-representation-based $\rightarrow$ Convolutional Downsampling  & --- | --- | 2.1 | 4.4 \\
    &&&  Seed-ASR \cite{bai2024seed} & Latent-representation-based $\rightarrow$ Convolutional Downsampling  & --- | --- | \textbf{1.5} | \textbf{2.8} \\
    &&&  SALMONN \cite{tang-etal-2024-salmonn} & Latent-representation-based $\rightarrow$ Q-Former  & --- | --- | 2.1 | 4.9\\
    &&&  SLM \cite{wang-2023-slm} & Latent-representation-based $\rightarrow$ Other Adaptation Methods  &   --- | --- | 2.6 | 5.0  \\
    &&& Qwen-Audio \cite{chu-2023-qwen-audio} & Latent-representation-based $\rightarrow$ Other Adaptation Methods   & 1.8 | 4.0 | 2.0 | 4.2 \\
    &&&  Qwen2-Audio \cite{chu-2024-qwen2-audio} & Latent-representation-based $\rightarrow$ Other Adaptation Methods  & \textbf{1.3} | \textbf{3.4} | 1.6 | 3.6\\
    &&& LauraGPT \cite{du-etal-2024-lauragpt} & Latent-representation-based $\rightarrow$ Other Adaptation Methods & --- | --- | 4.4 | 7.7 \\
    &&&  SpeechGPT-Gen \cite{zhang-etal-2024-speechgptgen} & Audio-token-based $\rightarrow$ Semantic Token & --- | --- | 2.4 | --- \\
    &&& SLAM-ASR \cite{ma2024embarrassingly} & Latent-representation-based $\rightarrow$ Convolutional Downsampling   & --- | --- | 1.9 | 3.8 \\
    &&& BESTOW \cite{chen2024bestow} & Latent-representation-based $\rightarrow$ Other Adaptation Methods   & --- | --- | --- | 3.2 \\
    &&& \citet{hao-2024-boosting} & Audio-token-based $\rightarrow$ Acoustic Tokens & 3.7 | 6.6 | 3.4 | 7.1 \\
    \cline{2-6}
    & \multirow{4}{*}{\makecell[c]{\textbf{Fleurs}\\ \small \textit{zh} | \textit{en}}} & \multirow{4}{*}{WER $\downarrow$} &  Whisper-large-v2 \cite{radford-etal-2023-whisper} & Non-LLM & 4.2 | 14.7 \\
    &&& \citet{huang-2024-fusion-comprehensive} w/ PaLM2 & Text-based $\rightarrow$ LLM Rescoring &  13.1 | --- \\
    &&&  Seed-ASR \cite{bai2024seed} & Latent-representation-based $\rightarrow$ Convolutional Downsampling &  \textbf{3.43} | ---  \\
    &&&  Qwen2-Audio \cite{chu-2024-qwen2-audio} & Latent-representation-based $\rightarrow$ Other Adaptation Methods  &   --- | \textbf{7.5}  \\
    &&&  DiscreteSLU \cite{shon2024discreteslu} & Audio-token-based $\rightarrow$ Semantic Token  &   12.6 | ---  \\
    \cline{2-6}
    & \multirow{4}{*}{\makecell[c]{\textbf{AISHELL-2} \\ \small \textit{Mic | iOS | Android}}} & \multirow{4}{*}{WER $\downarrow$} &  Seed-ASR \cite{bai2024seed} & Latent-representation-based $\rightarrow$ Convolutional Downsampling & \textbf{2.2} | \textbf{2.2} | \textbf{2.2} \\ 
    &&& Qwen-Audio \cite{chu-2023-qwen-audio} & Latent-representation-based $\rightarrow$ Other Adaptation Methods & 3.3 | 3.1 | 3.3 \\
    &&&  Qwen2-Audio \cite{chu-2024-qwen2-audio} & Latent-representation-based $\rightarrow$ Other Adaptation Methods & 3.0 | 3.0 | 2.9 \\
    &&& LauraGPT \cite{du-etal-2024-lauragpt} & Latent-representation-based $\rightarrow$ Other Adaptation Methods & --- | 3.2 | --- \\
    \cline{2-6}
    & \multirow{2}{*}{\makecell[c]{\textbf{VoxPopoli}\\ \small \textit{All}}} & \multirow{2}{*}{WER $\downarrow$} &  SpeechVerse \cite{das2024speechverse} & Latent-representation-based $\rightarrow$ Convolutional Downsampling  & \textbf{6.5} \\ 
    &&& AudioPaLM \cite{rubenstein-etal-2023-audiopalm} & Audio-token-based $\rightarrow$ Semantic and Acoustic Tokens & 9.8 \\
    \hline
    
    \multirow{11}{*}{S2TT} & \multirow{11}{*}{\makecell[c]{\textbf{CoVoST2}\\ \small \textit{en-de} | \textit{de-en} |\\ \small \textit{en-zh} | \textit{zh-en} |\\ \small \textit{es-en} | \textit{fr-en} |\\ \small \textit{it-en} | \textit{ja-en}}} & \multirow{11}{*}{BLEU $\uparrow$}
    &  Whisper-large-v2 \cite{radford-etal-2023-whisper} & Non-LLM & --- | 36.3 | --- | 18.0 | 40.1 | 36.4 | 30.9 | 26.1 \\
    &&&  GenTranslate \cite{hu-etal-2024-gentranslate} & Text-based $\rightarrow$ LLM GER  & --- | 39.2 | --- | 21.6 | 42.0 | 41.7 | --- | 22.9 \\
    &&&  GenTranslate-V2 \cite{hu-etal-2024-gentranslate} & Text-based $\rightarrow$ LLM GER & --- | 40.6 | --- | 23.3 | 43.6 | 42.7 | --- | 25.4 \\
    &&& BLSP \cite{wang2023blsp} & Latent-representation-based $\rightarrow$ Convolutional Downsampling & 24.4 | --- | 41.3 | --- | --- | --- | --- | --- \\
    &&&  Speech-Llama \cite{wu-2023-decoder-only} & Latent-representation-based $\rightarrow$ CTC Compression & --- | 27.1 | --- | 12.3 | 27.9 | 25.2 | 25.9 | 19.9 \\
    &&& SALMONN \cite{tang-etal-2024-salmonn} & Latent-representation-based $\rightarrow$ Q-Former & 18.6 | --- | 33.1 | --- | --- | --- | --- | --- \\
    &&&  Qwen-Audio \cite{chu-2023-qwen-audio} & Latent-representation-based $\rightarrow$ Other Adaptation Methods & 25.1 | 33.9 | 41.5 | 15.7 | 39.7 | 38.5 | 36.0 | --- \\
    &&& Qwen2-Audio \cite{chu-2024-qwen2-audio} & Latent-representation-based $\rightarrow$ Other Adaptation Methods & \textbf{29.9} | 35.2 | \textbf{45.2} | 24.4 | 40.0 | 38.5 | 36.3 | --- \\
    &&&  LauraGPT \cite{du-etal-2024-lauragpt} & Latent-representation-based $\rightarrow$ Other Adaptation Methods & --- | --- | 38.5 | --- | --- | --- | --- | --- \\
    &&& LLaST \cite{chen2024llast} & Latent-representation-based $\rightarrow$ Other Adaptation Methods & --- | 41.2 | --- | 24.8 | \textbf{46.1} | \textbf{45.1} | \textbf{43.0} | \textbf{28.8} \\
    &&&  AudioPaLM \cite{rubenstein-etal-2023-audiopalm} & Audio-token-based $\rightarrow$ Semantic and Acoustic Tokens & --- | \textbf{43.4} | --- | \textbf{25.5} | 44.2 | 44.8 | --- | 25.9 \\
    &&& Ideal-LLM \cite{xue2024ideal} & Latent-representation-based $\rightarrow$ Other Adaptation Methods & 25.9 | 38.5 | --- | --- | 41.5 | 40.0 | 38.0 | --- \\
    \hline
    
    \multirow{4}{*}{S2ST} & \multirow{4}{*}{\makecell[c]{\textbf{CVSS S2ST}\\ \small \textit{de-en} | \textit{zh-en} |\\ \small \textit{es-en} | \textit{fr-en} |\\ \small \textit{it-en} | \textit{ja-en}}} & \multirow{4}{*}{ASR-BLEU $\uparrow$} &  Translatotron 2 + pretraining &  &  \\
    &&&   + TTS aug \cite{jia-etal-2022-s2st} & \multirow{-2}{*}{Non-LLM} &  \multirow{-2}{*}{33.6 | 13.1 | 38.5 | 36.5 | 35.7 | 8.5} \\
    &&& & & \\
    &&& \multirow{-2}{*}{AudioPaLM \cite{rubenstein-etal-2023-audiopalm}} & \multirow{-2}{*}{Audio-token-based $\rightarrow$ Semantic and Acoustic Tokens} & \multirow{-2}{*}{\textbf{37.2} | \textbf{20.0} | \textbf{40.4} | \textbf{38.3} | \textbf{39.4} | \textbf{20.9}} \\
    \hline

    \multirow{18}{*}{TTS} & \multirow{3}{*}{\makecell[c]{\textbf{AISHELL-1}}} & \multirow{3}{*}{\makecell[c]{CER $\downarrow$ | SECS $\uparrow$ |\\ MOSNet $\uparrow$}} &  VALL-E Phone \cite{wang-etal-2023-valle} & Non-LLM & \textbf{4.75} | \textbf{0.91} | \textbf{3.22} \\
    &&& VALL-E Token \cite{wang-etal-2023-valle} & Non-LLM & 6.52 | \textbf{0.91} | 3.19 \\
    &&&  LauraGPT \cite{du-etal-2024-lauragpt} & Audio-token-based $\rightarrow$ Acoustic Tokens &  6.91 | 0.90 | 3.14 \\
    \cline{2-6}
    & \multirow{5}{*}{\makecell[c]{\textbf{LibriTTS}}} & \multirow{5}{*}{\makecell[c]{WER $\downarrow$ | SECS $\uparrow$ |\\ MOSNet $\uparrow$}} & VALL-E Phone \cite{wang-etal-2023-valle} & Non-LLM & \textbf{4.30} | 0.92 | 3.28 \\
    &&&  VALL-E Token \cite{wang-etal-2023-valle} & Non-LLM & 6.57 | \textbf{0.93} | 3.28 \\
    &&& SpeechGPT-Gen (zero-shot) \cite{zhang-etal-2024-speechgptgen} & Audio-token-based $\rightarrow$ Semantic Token & 3.10 | 0.63 | 3.63 \\
    &&&  VoxtLM \cite{maiti-etal-2024-voxtlm} & Audio-token-based $\rightarrow$ Semantic Token & --- | --- | \textbf{4.36} \\
    &&& LauraGPT \cite{du-etal-2024-lauragpt} & Audio-token-based $\rightarrow$ Acoustic Tokens &   8.62 | 0.91 | 3.26 \\
    \cline{2-6}
    & \multirow{5}{*}{\makecell[c]{\textbf{Topic-StoryCloze}}} & \multirow{5}{*}{\makecell[c]{TSC $\uparrow$}} &  Spirit-LM \cite{nguyen-etal-2025-spiritlm} & Audio-token-based $\rightarrow$ Semantic Token  & 82.9 \\
    &&& TWIST-1.3B \cite{hassid-etal-2023-twist} & Audio-token-based $\rightarrow$ Semantic Token & 70.6 \\
    &&&  TWIST-7B \cite{hassid-etal-2023-twist} & Audio-token-based $\rightarrow$ Semantic Token & 74.1 \\
    &&& TWIST-13B \cite{hassid-etal-2023-twist} & Audio-token-based $\rightarrow$ Semantic Token & 76.4 \\
    &&&  Moshi \cite{defossez-etal-2024-moshi} & Audio-token-based $\rightarrow$ Semantic and Acoustic Tokens & \textbf{83.6} \\
    \cline{2-6}
    & \multirow{5}{*}{\makecell[c]{\textbf{StoryCloze}}} & \multirow{5}{*}{\makecell[c]{SSC $\uparrow$}} & Spirit-LM \cite{nguyen-etal-2025-spiritlm} & Audio-token-based $\rightarrow$ Semantic Token & 61.0 \\
    &&&  TWIST-1.3B \cite{hassid-etal-2023-twist} & Audio-token-based $\rightarrow$ Semantic Token & 52.4 \\
    &&& TWIST-7B \cite{hassid-etal-2023-twist} & Audio-token-based $\rightarrow$ Semantic Token & 55.3 \\
    &&&  TWIST-13B \cite{hassid-etal-2023-twist} & Audio-token-based $\rightarrow$ Semantic Token & 55.4 \\
    &&& Moshi \cite{defossez-etal-2024-moshi} & Audio-token-based $\rightarrow$ Semantic and Acoustic Tokens & \textbf{62.7} \\
    
    \bottomrule
\end{tabular}
}
\caption{Quantitative comparison based on applications. The \textbf{Bold} denotes the best result.}
\label{tab:benchmarks}
\end{table*}
