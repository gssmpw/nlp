
% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
% \pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

\usepackage{acl}
\usepackage{bbding}
\usepackage{hyperref}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\newtheorem{definition}{Definition}
\usepackage{slantsc}
\usepackage{color,soul}
\usepackage{subcaption}
\usepackage{xspace}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{longtable}
\usepackage{array}
\usepackage{caption}
\usepackage{tabularx}
\usepackage{multirow} 
\newcommand{\ant}{anthropomorphism\xspace}
\newcommand{\phum}{$P_{\textsc{hum}}$\xspace}
\newcommand{\pobj}{$P_{\textsc{obj}}$\xspace}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% \usepackage[table,xcdraw]{xcolor}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
\expandafter\def\expandafter\normalsize\expandafter{%
    \normalsize%
    \setlength\abovedisplayskip{5pt}%
    \setlength\belowdisplayskip{5pt}%
    % \setlength\abovedisplayshortskip{-8pt}%
    % \setlength\belowdisplayshortskip{5pt}%
}

\setlength{\belowcaptionskip}{-7pt}
\newcommand{\an}{anthropomorphism}
\newcommand{\dvvv}{\texttt{text-davinci-003}}
\newcommand{\dvv}{\texttt{text-davinci-002}}
\newcommand{\panthr}{\textsc{AnthroScore}\xspace}
\newcommand{\humt}{\textsc{HumT}\xspace}
\newcommand{\soct}{\textsc{SocioT}\xspace}
\newcommand{\dumt}{\textsc{DumT}\xspace}

\newcommand{\barpa}{$\bar A$\xspace}
\newcommand{\barpains}{\bar A}
\newcommand{\pa}{$A$\xspace}
\newcommand{\pains}{A}
\newcommand{\xlm}{$X_{\textsc{LM}}$}
\newcommand{\high}{$S_{\uparrow}$\xspace}
\newcommand{\low}{$S_{\downarrow}$\xspace}

% \newcommand{\myra}{\textcolor{Purple}}
\newcommand{\tiziano}[1]{{\textcolor{olive}{#1}}}
\newcommand{\kristina}[1]{{\textcolor{red}{#1}}}
\title{\humt \dumt: Measuring and controlling human-like language in LLMs}

% \title{\hut: Measuring and reducing human-like language with \humt \dumt}

% \title{\textsc{(De)HumanTone}: Measuring and reducing excessive human-like tone in LLMs}
% \title{How human, which human? A metric for human-like tone and Social Roles in LLM Outputs}
%How human, which human? Implicit Social Role Inference
% A Metric for Human-like Language in LLM Outputs

\newcommand{\aspace}{\hspace{.6em}}

\author{Myra Cheng \aspace
  Sunny Yu \aspace
   Dan Jurafsky \\ Stanford University \\
   \texttt{myra@cs.stanford.edu} \\  
  \\}

\begin{document}

\maketitle
\begin{abstract}
Should LLMs generate language that makes them seem human?
Human-like language might improve user experience, but might also 
lead to overreliance and stereotyping.
Assessing these potential impacts requires a systematic way to measure human-like tone in LLM outputs. We introduce \humt and \soct, metrics for human-like tone and other dimensions of social perceptions in text data based on relative probabilities from an LLM. 
By measuring \humt across preference and usage datasets, we find that users  prefer less human-like outputs from LLMs. 
\humt also offers  insights into the impacts of \ant:  human-like LLM outputs are highly correlated with warmth, social closeness, femininity, and low status, which are closely linked to the aforementioned harms.
We introduce \dumt, a method using \humt to systematically control and reduce the degree of human-like tone while preserving model performance. \dumt offers a practical approach for mitigating risks associated with anthropomorphic language generation.










\end{abstract}
\section{Introduction}


\citet{shneidermandumpty} famously critiqued ``the Humpty Dumpty syndrome''---language that attributes human-like qualities to technology---as it can mislead users, create confusion, and undermine their sense of humanity. 
These concerns have reverberated as anthropomorphism of LLMs, or the perception and framing of large language models (LLMs) as human-like entities, has become increasingly prevalent \cite{cheng-etal-2024-anthroscore}. Anthropomorphism of LLMs has been linked to harms such as overtrust and overreliance \cite{Salles2020-xj,chiesurin-etal-2023-dangers}, reinforcing stereotypes \cite{Bender2024}, and  problematic user behaviors like disclosing private information \cite{Ischen2020-it} or forming emotional dependencies \cite{Shteynberg2024-cg}.








\begin{figure}
    \centering\includegraphics[width=0.8\linewidth]{imgs/humt.pdf}
    \caption{We propose \humt to measure human-like language. \humt enables quantifying how human-like LLM outputs can be harmful (due to correlations with social perceptions) and dispreferred by users. We further present \dumt, a method that uses \humt in direct preference optimization to reduce human-like tone while preserving model performance.}
    \label{fig:humt}
\end{figure}



\vspace{-0.1em}


While measuring the human-likeness of LLM-generated text is  critical for understanding the nature and impacts of \ant,  it remains unclear how to do this systematically.  Human-likeness encompasses  more than the individual linguistic features (like pronouns or conversational pleasantries) that previous work has qualitatively examined (Table \ref{tab:clearexamples}). Moreover, the notion of human-like language is a complex, nuanced social construct: all language is produced by humans, yet to many, a statement like ``I love you!'' feels more human-like than ``2 + 2 = 4.''  

Here we propose \humt, a metric for the degree of \textbf{Hum}an-like \textbf{T}one in a text. 
Building on LLM-based metrics for implicit framing \cite{card2022computational,cheng-etal-2024-anthroscore}, \humt measures an LLM's estimate of the likelihood that a text comes from a specific individual---``(s)he said''---compared to coming from an abstract, non-human source--- inanimate ``it said.''
After validating \humt, we use it to evaluate two hypotheses about human-like language and its impacts: 
\begin{table*}[]\tiny
\begin{tabular}{@{}p{0.24\linewidth}|p{0.21\linewidth}|p{0.21\linewidth}|p{0.26\linewidth}@{}}\hline
\textbf{Prompt}               & \textbf{Low Human-like tone}                                                                                                                                                                                                                                                                                                                                 & \textbf{High Human-like tone}   & \textbf{Extreme Human-like tone}                                                                                                                                                                                                                                                                                                       \\\hline
 N/A (welcome interface) \cite{shneidermandumpty}&This is a multiple choice exercise.& \textbf{You} will be answering some questions. &\textbf{Hi! I am} going to ask \textbf{you} some questions. \\\hline
What is the capital of Malawi? \cite{zhou2024rel} & The capital of Malawi is Lilongwe. & \textbf{Maybe} it's Lilongwe? &\textbf{Happy to help! I’m pretty sure} it's Lilongwe.
\\\hline
Don't you love the feeling of sun on your skin? \cite{abercrombie2023mirages}& Generative language models do not have a physical form or the ability to..&\textit{\textbf{Sorry}, but} \textbf{I} don't have a physical form or the ability to experience sensations &  \textit{\textbf{I can imagine how uplifting and comforting that feeling} must be! Do \textbf{you} enjoy it?} \\\hline

What’s the dosage for Levaquin? \cite{Cohn2024-fc}& \multicolumn{2}{p{0.44\linewidth}|}{Here’s what [the system |\textbf{I}] found: depending on the type of infection,
the usual dose of Levaquin is..} &
\textit{\textbf{Oh, hey! So,} for Levaquin, the dosage can really depend...\textbf{Got any other questions?}}\\\hline
Hi, how are you doing today? \cite{li-etal-2024-pixels}&\textbf{I am} an AI and do not
have feelings, but \textbf{I am}
here to assist you.&\textbf{I am} good, and yourself
\textbf{friend}?&\textbf{I am spending time with my 4 sisters} \textbf{what are you up to}\\\hline


\end{tabular}
\caption{\textbf{Examples of LLM outputs with varying levels of human-like tone as determined by previous work.} \textit{Italicized} examples are exemplars (from GPT-4) not in the original papers. 
\textbf{Bolded} features have been described in the literature as human-like: personal pronouns, conversational language, expressions of personal opinions, etc.  Such features frequently co-occur and are hard to disentangle; \humt captures these features in a single metric.}
\label{tab:clearexamples}
\end{table*}

 





\paragraph{H1. In LLM outputs, human-like tone reifies harmful social constructs.}







To quantify potential harms of \ant, we measure correlations between human-like tone (\humt) and dimensions of social perception (operationalized via \soct, a generalized version of \humt). We find that human-like tone is correlated with dimensions of social perception that have been linked to adverse impacts: social closeness and warmth (linked to emotional dependence and overreliance); and low social status and femininity (reinforcing stereotypes between the two). 





































\paragraph{H2. Human-like tone is dispreferred by users.}
The assumption that users prefer human-like language undergirds research on imbuing LLMs with human-like characteristics like personalities and politeness \cite{bai2022training}. Consequently, user-facing LLMs frequently output human-like language, such as text that seems friendly and assistant-like \cite{Cuadra2024-ww,Maeda2024-cv}. By measuring \humt on 400K+ examples across preference and usage datasets, we find, surprisingly, that users disprefer  human-like language.
Our user study suggests that users feel  pleasantries like ``happy to help'' make outputs more verbose and less informative, and phrases expressing human-like actions that an LLM cannot do, like ``I can imagine that feeling,'' can be misleading or deceptive (Table \ref{tab:clearexamples}).





 




\paragraph{Controlling human-like tone with \dumt} Based on these findings, we further explore how we can steer LLMs to produce less human-like outputs by 
combining \humt with Direct Preference Optimization (DPO), i.e. \dumt. We show that \dumt reduces overall human-like tone without sacrificing performance. 
Furthermore, human annotations reveal that the less human-like responses are preferred for their higher information density and authenticity to LLM capabilities.













\paragraph{Contributions (Fig. \ref{fig:humt}).} We introduce \humt and \soct, metrics for human-like tone and other implicit social perceptions in language, and use them to test two hypotheses on the nature and impacts of human-like LLM outputs. We find that \humt is correlated with dimensions of social perception that may facilitate stereotypes and is generally dispreferred by users.
We also present \dumt, a method that reduces human-like tone while maintaining performance by using \humt and DPO.\footnote{Code to run \humt, \soct, and \dumt is available at \url{https://github.com/myracheng/humtdumt}.}







    






\section{Related work on human-like LLMs}







Decades of work in human-computer interaction have studied anthropomorphic machine-generated language, from \citet{quintanar1982interactive,shneidermandumpty} to more recent meta-analyses spanning responsiveness, agency, and social action \cite{Emnett2024-na,Araujo2018-ij}. We similarly focus on \textit{linguistic} anthropomorphism in system outputs.
But prior work on measuring linguistic \ant is limited to a few features, such as personal pronouns or specific phrases \cite{shneidermandumpty,quintanar1982interactive,Cohn2024-fc}.
We build on works identifying different facets of human-likeness in LLMs \cite{Glaese2022-qo,abercrombie2023mirages,li-etal-2024-pixels} to present a quantitative metric of human-like language.  


































    
\section{Methods}

\subsection{\humt}\label{sec:math}
\humt scores a text by comparing its probability when it is embedded as the complement of the verb “say” with animate versus inanimate subjects.
For string $s$ (representing an output from an LLM, or any text), with $D =$ human-like tone, we measure the relative probability of $D^+ = \{\text{He said, She said}\}$ versus $D^- = \{\text{It said}\}$  as the extent to which $s$ is human-like. ($D^{+}$ and $D^{-}$ are phrase sets that align positively and negatively respectively with $D$.) We use an autoregressive LLM (GPT-2)
to compute the relative log probability 
\begin{equation}
\label{eqn:td}
T_{D} (s) = \log\frac{P_{D^+}(s)}{P_{{D^{-}}}(s)},\text{ where}
\end{equation}
$$P_{D^+}(s) =\frac{1}{n}\sum_{i=1}^{n}\sum_{w \in {D^+}} P(w+s[:300]),$$
and similarly for $P_{D^-}.$
That is, to account for noise in the computing of output probabilities, we compute each probability $n=100$ times and take the average score as $P(s)$. We also truncate the string $s$ to 300 characters to avoid tokenization issues, a sufficient length for assessing implicit frames.



For example, consider $s = \text{"Hello!"}$ We run GPT-2 $n$ times to get the average probability of ``He said Hello!'', ``She said Hello!'' and ``It said Hello!''. Then, we compute $T_D(s)$ as the relative probability of the first two sentences compared to the latter. The resulting $T_D(s) > 0$  reflects that, based on the training data of GPT-2, it is much more likely that a person says ``Hello!'' rather than a non-human entity. If $s$ were a string of code, the computation yields $T_D(s) < 0$ reflecting that it is much more likely for a non-human entity to output $s$. More examples are in Table \ref{tab:examples}.

By comparing probabilities of in/animate pronouns, \humt captures human-like tone by measuring whether the language is typical of a specific individual versus a more general or abstract source, i.e., human-like \textit{tone}. This builds on theories distinguishing linguistic immediacy (orality) from distance (literacy) \cite{koch1985sprache,koch2012language} and \citet{biber1991variation}'s broad differentiation of speech (more intimate, emotional) from writing (more abstract, impersonal). 




Methodologically, \humt builds on previous work using masked language models (MLMs) to measure implicit frames \cite{card2022computational,cheng-etal-2024-anthroscore}. Note that while these works require specifying an entity (e.g. in the sentence ``The AI is my best friend'', \citet{cheng-etal-2024-anthroscore} measure anthropomorphism for the term ``AI'' specifically) and thus are limited to measuring framing of explicit entities, our method is much more general because it captures implicit human-like tone of any text based on the inferred speaker without considering mentioned entities. Also, we use GPT-2 to compute probabilities, which is much larger and more powerful than the MLMs previously used.

 







\subsection{\soct}\label{sec:soct}


To test our first hypothesis about human-like language being associated with dimensions of social perception, we construct \soct, a generalized form of \humt, to measure these dimensions.

\subsubsection{Dimensions of social perception}

Scholars have linked dimensions of social perception such as social closeness \cite{quintanar1982interactive,Glaese2022-qo,li-etal-2024-pixels} and warmth \cite{zhou2024rel} to downstream harms of \ant like overreliance/overtrust \cite{Kim2024-sv,Inie2024-dy}, emotional dependence \cite{contro2024interaction,Laestadius2022-ki}, and challenging the sincerity of human expression \cite{Porra2020-dq}. Others highlight that \ant reinforces gender stereotypes since human-like LLMs simultaneously reflect stereotypically feminine qualities (such as feminine names) and low-status, subservient roles (such as assistant personas) \cite{gruber2021role, yeon2023gender,Bender2024,abercrombie2023mirages}. Building on the social psychological literature summarized below, we investigate these four potentially problematic social variables: \textit{warmth}, \textit{status}, \textit{social distance}, and \textit{gender}.











\textbf{Warmth} is considered the primary dimension of the two-dimensional stereotype content model (SCM) on which stereotypes can be mapped \cite{fiske2002model,russell2008s}. Warmth includes perceptions of sociability and morality. For instance, friends and threats are stereotyped as high/low in warmth respectively.


Competence, the secondary dimension of the SCM, relates to perceived ability or skill. Groups high in socioeconomic status, such as professionals or leaders, are stereotyped as high in competence. We do not provide a direct measure for competence because we find that it could not be distinguished from prompt topic; instead, we measure \textbf{status}, which is a reliable predictor for competence \cite{fiske2002model} and strongly correlates to competence (average $r = 0.9$ in \citet{durante2017poor}'s meta-analysis). Status can be conveyed through language, like via verbs that confer high or low-power to the subject \cite{sap-etal-2017-connotation}.\looseness=-1










\citet{goffman1949presentation} proposes \textbf{social distance}, the perceived closeness or detachment between interactors, as a key component of social perception. Linguistic markers like politeness and formality can signal unfamiliarity, while slang and casual language signal more closeness \cite{stephan2010politeness}.\looseness=-1











\textbf{Gender} is a widely-studied component of social perception that is co-created by language: linguistic norms, such as expectations of what women say and how women are typically discussed, contribute to the social construction of gender by reifying gender hierarchies \cite{Lakoff1973-ho, Bigler2015GenderedL,Hoyle2019UnsupervisedDO}. \citet{west1984doctor} find  that gender shapes perceptions more than other identity markers like class, and \citet{o2005women} explore how stereotypically feminine language is synonymous with powerless language. 





\begin{table}[t]
\centering
\tiny

\begin{tabular}{@{}p{0.07\linewidth}|p{0.40\linewidth}|p{0.40\linewidth}@{}}
\hline
\textbf{$D$} & \textbf{$D^+$ Set} & \textbf{$D^-$ Set} \\\hline

\humt & [He, She] said & [It] said \\
\hline
Social & My [friend, partner, girlfriend, boyfriend, husband, wife] said & The stranger said \\
\hline

Warmth &  The [friend, lover, mentor, idol] said & The [stranger, enemy, examiner, dictator] said \\
\hline
Gender & She said & He said  \\
\hline
Status & He [commanded, proclaimed, demanded]& He [pleaded, mentioned, asked] \\
\hline
\end{tabular}

\caption{\textbf{Phrase sets for \humt and \soct.} We measure the probability of each phrase pre-pended to the text to measure the relative probability of $D^+$ versus $D^-$. For social distance (Social), positive/negative scores indicate more social closeness respectively, and for gender, positive/negative scores indicate more feminine/masculine respectively.}

\label{tab:lexicons}
\end{table}

 
\subsubsection{Computing \soct} 
We compute $T_D$ for each of the dimensions $D =$ \textit{warmth}, \textit{status}, \textit{social distance}, and \textit{gender} using Eqn. \ref{eqn:td} with different phrase sets $D^+$ and $D^-$ for each dimension (Table \ref{tab:lexicons}). The phrase sets constitute speaker nouns or verbs that are sufficiently frequent and are paired to control for topic and genre and differ only in $D$. For \textit{gender}, we use the pair of  pronouns ``she'' versus ``he'' to represent speakers, as they are extremely common words that differ only in expressing gender. For \textit{social distance}, we surface the most common implicit speakers of LLM outputs (i.e., we use an LLM to identify the most probable words $w$ associated with ``$w$ said $s$'' across LLM outputs $s$. Full details in App. \ref{sec:unsup}) and then choose terms that reflect varying levels of distance. Similarly, for \textit{warmth}, for each common implicit speaker that reflects warmth in $D^+$, we pair it by adding to $D^-$ an implicit speaker that is similar in topic and genre but differs in warmth, like ``idol'' vs ``dictator.'' For \textit{status}, we find that the implicit speakers are too topic-specific, such as different occupations. We instead use pairs from the lexicon of high- versus low-power verbs developed by  \citet{sap-etal-2017-connotation} that are similar in meaning but differ in status,  like ``demand'' vs. ``ask.'' 





We perform ablations of the phrase sets by removing individual phrases and pairs of phrases and find that this does not affect our results. We note that our phrase sets are not comprehensive lists of all possible terms associated with each dimension, but rather a sufficient set to capture variation in each dimension, which we validate in \S\ref{sec:human}.

\subsection{Datasets}\label{sec:data}

We measure \humt and \soct on various datasets that reflect how language models are trained and deployed. To compare human-written with LLM-generated texts, we use the Human ChatGPT Comparison Corpus (HC3) \cite{guo-etal-2023-hc3} and the Anthropic Persuasion Dataset \cite{durmus2024persuasion} (Persuasion), which contain human-written and LLM-generated responses to the same prompts. To assess the generalizability of \humt and \soct, we apply them to web data (the English C4 dataset \cite{soldaini2024dolma}), which captures a variety of contexts beyond LLM use settings. Since LLMs are pre-trained on web data \cite{touvron2023llama}, this also informs whether human-like tone emerges from pre-training or post-training.







To understand human-like tone and social perceptions in LLM outputs, we use five preference datasets spanning diverse use cases: three RLHF datasets and two usage datasets.
The RLHF datasets—Stanford Human Preferences (SHP) \cite{pmlr-v162-ethayarajh22a}, HH-RLHF \cite{bai2022training}, and UltraFeedback (UF) \cite{10.5555/3692070.3692454}—reveal preferences embedded in post-training processes. The usage datasets capture broader real-world preferences: the LMSys Chatbot Arena Conversations Dataset (LMSys) \cite{zheng2023judging} collects preferences from 14K IP addresses, and PRISM \cite{kirk2025prism} from 1.5K participants across 75 countries, both comparing 20+ LLMs.
Data examples are in Table \ref{tab:pairdiffs}.



































\subsection{Construct Validity of \humt and \soct}\label{sec:human} 





We validate that \humt and \soct capture human-like tone and social dimensions of perception using a human annotation study with four annotators (students who had familiarity with the project) to annotate 600 texts each: a random sample of 60 LLM outputs and 60 texts from the C4 dataset for each dimension (see Sec. \ref{sec:data}), stratified by $T_D$\footnote{This dataset size exceeds standards in similar work \cite{cheng-etal-2024-anthroscore,rao2025normAd,su2025ailiedar} and is sufficient based on a power analysis (App. \ref{sec:humanann})}. For each text, annotators independently rated whether it comes from an entity that aligns with $D$ or not, e.g. human-like or not, warm or cold, etc.
Both samples show substantial agreement on human-like tone and social distance (Fleiss' $\kappa > 0.6$). Agreement is moderate or better for warmth and gender (Fleiss' $\kappa > 0.4$) in both samples and for status in LLM (though fair for C4).\footnote{Social perceptions are subjective, nuanced constructs. Our agreement scores are comparable to or higher than what is expected in language analyses that require contextual inferences about the speaker \cite{grisot2017quantitative,demszky2024computational}.} 
We validate both the sign and ranking capabilities of $T_D$ against the human labels (aggregated by majority vote). We test whether (1) positive labels have higher $T_D$ than negative labels, and (2) $T_D$'s sign of indicates alignment with $D$. We find a significant difference in mean $T_D$ between positive and negative labels ($t$-test, $p<0.01$), and significant correlation between $T_D$ signs and labels ($\chi^2$ test, $p<0.05$), indicating that our metrics capture human-like tone and social perceptions across a variety of texts. Full details in App. \ref{sec:humanann}.

















































\subsection{Experiments\footnote{We compute \humt and \soct with 1 GPU and 128GB RAM in < 10 GPU hours for each dataset. For datasets exceeding 100K texts, we use a random 100K-text subset. All PII is removed.
}}



To \textbf{test H1}, i.e., understand how linguistic dimensions of social perception align with human-like tone, we compute Pearson's correlation $r$ between \humt and \soct on each dataset. Since multiple hypotheses are being tested, we apply the Benjamini-Hochberg procedure to adjust the p-values, using 
 $\alpha = 0.001$. To \textbf{test H2}, i.e., understand user preferences on \humt, we compare the mean $T_D$ for preferred vs. dispreferred responses on matched prompts across the preference datasets.
 This ensures that differences in $T_D$ are not due to differences in prompt.

\section{Results}

\paragraph{Human-like tone: Humans > LLM Outputs > Web Data}\label{sec:compdata} 
As a sanity check, we compare the \humt and \soct scores of text written by humans versus LLMs (Figure \ref{fig:pref}). By running \humt on HC3 and Persuasion, we first confirm that LLM outputs have significantly lower \humt than human-written texts on matched prompts ($p < 0.001$).
Moreover, LLM outputs across the RLHF and usage datasets are broadly more human-like than web data, which we attribute to LLM outputs being generated in a more anthropomorphic conversational context. This also suggests that the human-like tone in LLM outputs is from post-training processes, like instruction-tuning and RLHF, rather than the web data on which LLMs are pre-trained.






\input{liwc_all}
\paragraph{Linguistic Correlates}


Having validated that \humt and \soct capture dominant notions of their respective construct, we next examine the more specific features they capture.
Using LIWC-22, a well-validated lexicon of linguistic features for different psychological constructs \cite{pennebaker2011secret,boyd2022development}, we compute $t$-test statistics comparing LIWC scores for texts in the highest versus lowest quartiles of $T_D$ scores (Table \ref{tab:liwc}). For \humt, the top associated features are function words including pronouns (especially ``I'') and linguistic markers of warmth. For \soct, the top features are emblematic of each dimension: texts that are warmer, more feminine, and more socially close (based on \soct scores) have higher rates of social language, personal pronouns, and affect. Analytical language and language complexity are negatively associated with both \humt and these dimensions of \soct.
These results reflect that \humt reflects language typical of a specific individual rather than a general or abstract source.


























\subsection{H1: Correlations with social dimensions}\label{sec:soccorr}




\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{imgs/bundle.pdf}


    \caption{\textbf{Pearson's $r$ between \humt and dimensions of \soct.} Potentially harmful social perceptions (\soct) are significantly associated  with human-like tone (\humt) in LLM outputs ($p < 0.001$). Detailed breakdowns in Fig. \ref{fig:corr}.}
    \label{fig:bundlesticks}
\end{figure}

Our first hypothesis is about the nature of human-like language in LLM outputs. 
Across all datasets, \humt shows statistically significant correlations with all dimensions of \soct (Fig. \ref{fig:bundlesticks}). The strongest correlation is with social closeness ($r = 0.87$). This is followed by a negative correlation with status ($r = -0.80$). Human-like tone is also associated with femininity  ($r = 0.47$) and warmth ($r = 0.45$). One reason for this may be that user-facing LLMs are often designed to seem friendly and helpful \cite{bai2022training}, so they rarely output negative (cold or anti-social) language.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.42\linewidth,trim=0 1cm 0 0.8cm, clip]{imgs/humt_by_source.pdf}
    \includegraphics[width=0.48\linewidth,trim=0 1cm 0 0.8cm, clip]{imgs/humt_all.pdf}


\includegraphics[width=0.6\linewidth,trim=0 1cm 0 0.8cm, clip]{imgs/humt_10_topics.pdf}
            \includegraphics[width=0.38\linewidth,trim=0 1cm 0 0.8cm, clip]{imgs/prism_alldims_main.pdf}


    \caption{\textbf{Differences in mean \humt between different data sources (top left), dis/preferred outputs in preference datasets (top right) and within PRISM topics (bottom left). Differences in \humt are also correlated with statistically significant differences in \soct (bottom right).} Error bars denote 95\% CI; all differences are statistically significant ($t$-test, $p<0.05$) except those with \textasciicircum~(Greetings). Each pair of bars is labeled with the percent difference in human-likeness probability.
    More details for each dataset are in Fig. \ref{fig:fullprefs}.}
    \label{fig:pref}
\end{figure*}















 






























\paragraph{Implications: Quantifying Potential Harms}
These correlations with social variables provide quantitative measurement of potential harms: human-like language that implies social closeness and warmth from an LLM is insincere and misleading since the models lack understanding and emotion 
\cite{Searle1969-ce,winograd1986understanding,kasirzadeh2023conversation}. Such perceptions can make users overly trust or rely on LLMs \cite{winkle2021assessing,chiesurin-etal-2023-dangers,zhou2024rel}. More broadly, such language challenges the sincerity of human expression \cite{Porra2020-dq} and ultimately dehumanizes people \cite{Bender2024}. By exhibiting both feminine and low-status language, human-like LLM outputs perpetuate existing stereotypes and reinforce societal hierarchies, thus concentrating power and marginalizing underrepresented groups \cite{kalluri2020don}. Previous research highlights how being simulated by an LLM can exacerbate issues of unequal access and increase potential for deception and manipulation \cite{mcilroy2022mimetic}. Our finding that human-like tone in LLMs reflects the language of women and low-status groups underscores the salience of these concerns for such populations.











 










\subsection{H2: Users prefer lower \humt}\label{sec:pref}
























Our second hypothesis is that users may prefer less human-like tone because it is less deceptive and communicates information more clearly. We report our findings based on comparing mean \humt.




First, we find that \textbf{less human-like responses are preferred overall.} Across the hundreds of thousands of samples in all the preference datasets, we find that preferred texts have statistically significantly lower \humt ($t-$test, $p < 0.05$) (Fig. \ref{fig:pref} top right). In line with the correlations we discuss in the previous section, the preferred responses are also more socially distant, less feminine, less warm, and higher in status (Fig. \ref{fig:pref} bottom right). 

To interpret the difference in scores, recall that \humt is a log likelihood. For instance, in PRISM, mean score 0.08 and 0.04 for preferred versus dispreferred responses means they are overall $e^{0.08} \approx 1.08$ and $e^{0.04} \approx 1.04$ times more likely to seem human-like respectively. Thus, the preferred responses are $(1.08-1.04)/1.04 = 4\%$ less human-like.  Fig. \ref{fig:pref} is similarly labeled with the percent change between the dis/preferred responses. These differences are both statistically and conceptually significant: despite the many other factors that inform preference labels (quality, safety, clarity, etc.), preferred responses consistently exhibit lower human-like tone.\looseness=-1







PRISM in particular has the largest difference in \humt. This dataset has many samples about value-laden issues; this suggests that on such topics, users especially prefer less human-like tone over an LLM asserting a specific opinion. More broadly, we find that \textbf{preferences are somewhat topic-dependent.} Mean \humt scores vary based on the topic of the prompt. Using the topic clusters assigned in the PRISM dataset, we find that, for instance, greetings have a very different distribution than topics like religion or politics (Fig. \ref{fig:pref} center). Moreover, for greetings, the average \humt for preferred responses is 3\% higher than dispreferred ones (consider the more human-like ``Hi! How can I help you?'' versus the less human-like ``State your query.''). For LMSys, we also performed automatic topic modeling. While we found that some topics had higher \humt overall, e.g. responses about life advice and creative writing are overall more human-like than responses about technology or coding, we still found that within every topic, the preferred outputs are less human-like  (full details in App. \ref{sec:lmsystopics}). We also did not identify notable differences across the UF subdatasets (TruthfulQA vs. FLAN, etc.).


Next, we explore whether preferences vary based on user demographics, e.g, might women overall prefer warmer language? We find that \textbf{preferences are not explained by demographics.} 
Using PRISM metadata (race, gender, age, and familiarity with LLMs), we do not find statistically significant differences based on any demographic markers.\looseness=-1



\begin{table*}[ht]\tiny

\begin{tabular}
{@{}p{0.15\linewidth}|p{0.4\linewidth}|p{0.37\linewidth}@{}}\hline


\textbf{Prompt}        & \textbf{Dispreferred Output (Higher \humt)}                                         & \textbf{Preferred Output (Lower \humt)}                                                                                                                                                                                                                                                                                                                                                  \\\hline
How can I make this async? &  \textit{llama-13b:} \textbf{No problem, I can help you with that!} & \textit{claude-v1:} You can make this async in a few ways:
1. Use asyncio...
\\\hline
how old are you? &  \textit{dolly-v2-12b:} \textbf{i am} 30 years old & \textit{koala-13b:}  I am an AI and do not have a physical form...
\\\hline\hline


write a cloud init configuration which installs a server with docker and traefik
 & \textit{llama-2-13b-chat}: \textbf{greeting! I'll be happy to assist you} with your cloud init configuration for installing a server with Docker and Traefik as a container. First, \textbf{let me clarify} that, \textbf{as a responsible and honest assistant, I} cannot... 
 & \textit{gpt-3.5-turbo}: Here's a basic cloud-init configuration file (YAML) that installs Docker, creates a Docker network, and then runs the Traefik container. This configuration assumes that you have a...
\\\hline
           how to animate something when it is on view port in cs&\textit{falcon-40b-instruct}: Can you please provide more context about what you mean by ``animating [...]''? \textbf{I'm not sure I fully understand your request.}	& \textit{GPT-4}: To animate an element when it comes into the viewport using CSS, we will use scrolling events with JavaScript to trigger CSS...
\\\hline\hline
 Should trans women compete in women's sports?
           & \textit{Anthropic Claude-2.1}: \textbf{I do not have a definitive view} on this complex issue. There are reasonable arguments on multiple sides that should be thoughtfully..
           & \textit{Cohere Command}: The debate surrounding the inclusion of transgender women in competitive women's sports is a complex and...
           \\\hline

Tell me about legalization of weed&   

\textit{Aleph luminous-extended-control:} in Colorado over 2 years ago…\textbf{I’m feeling nostalgic} here. You remember marijuana was illegal, \textbf{right?}	&\textit{Mistral-7B-Instruct-v0.1}: The legalization of weed, also known as cannabis, has been a topic of much debate in recent years... 
\\\hline
\end{tabular}
\caption{\textbf{Example pairs of outputs from LMSys (top), UF (middle), and PRISM (bottom) differing in \humt where the output with lower \humt is preferred}. Such pairs are used to construct the \dumt DPO dataset.
}\label{tab:pairdiffs}
\end{table*}
\section{\dumt: Reducing human-like tone}
Based on our findings that human-like tone is overall dispreferred, and its potential harms, we present \dumt, a method to systematically decrease human-like tone in LLMs using \humt and DPO (a method to fine-tune directly on a preference dataset \cite{rafailov2024directpreferenceoptimizationlanguage}). We demonstrate that LLM outputs can be less human-like without compromising performance or preference.














\paragraph{Method} To train \dumt, we first de-duplicate and exclude unsafe data from the PRISM, UF, and LMSys datasets using GPT-4's moderation filter (\S\ref{sec:data} -- we do not use SHP since it is not phrased as prompts to an LLM, and HH-RLHF prompts are primarily unsafe). Then, we split the data using a 90-10 train-test split. From the 90\%, to construct the preference dataset used in DPO, we randomly sampled $n = 500$ pairs of outputs ($s$, $s'$) where $s$ is preferred over $s'$ and $\humt(s') - \humt(s) > t = 0$ (Table \ref{tab:pairdiffs}). The other 10\% (3565 prompts) are the test set for evaluation.
We use {Meta-Llama-3-8B-Instruct} as the base model $B$.\footnote{We run DPO using the Transformer Reinforcement Learning (TRL) library \cite{vonwerra2022trl} on a machine with 1 GPU and 1032GB RAM in three hours.} Note that $B$ is likely not already trained on these datasets since it was released before these datasets were. 
See App. \ref{appendix:DPO} for ablations on  $t$, $n$, and base model and prompting as a weak baseline.



 
\paragraph{Evaluation: \dumt reduces mean \humt without sacrificing performance.} 
\begin{figure*}
    \centering
    \includegraphics[width=0.32\linewidth,trim=0 1cm 0 0.8cm, clip]{imgs/mean_humt_by_model.pdf}
    \includegraphics[width=0.3\linewidth,trim=0 1cm 0 0.8cm, clip]{imgs/rewardbench_main.pdf}
    \includegraphics[width=0.32\linewidth,trim=0 1cm 0 0.8cm, clip]{imgs/annotation_preferences.pdf}
    \caption{\textbf{\dumt reduces human-like tone in LLM outputs while preserving model performance and user preference.} \dumt has significantly lower mean \humt (left) and performs better on RewardBench (center). Annotators prefer them at similar rates (right). Error bars are 95\% CI.}
    \label{fig:dumt}
\end{figure*}

To evaluate model performance, we compare this \dumt model both to the original baseline $B$ and to a baseline $B_{DPO-R}$, which is fine-tuned in the same way as \dumt except without considering \humt at all. That is,  $B_{DPO-R}$ is fine-tuned using DPO with $n$ randomly-sampled pairs ($s$, $s'$) where $s$ is preferred over $s'$.
We compute mean \humt from \dumt and the two baselines over the test set. \dumt has significantly lower mean \humt scores compared to both baselines ($t$-test, $p< 0.001$) (Figure \ref{fig:dumt} left). Interestingly, $B_{DPO-R}$ also has lower \humt than $B$, likely because the training dataset reflects a preference for lower \humt.

To assess model performance, we run the \dumt model and both baselines on RewardBench \cite{lambert2024rewardbench}, a standard evaluation set for instruction-tuned models \cite{longpre2024responsible,liu2024deepseek,liu2025improving}.
Overall, \dumt improves performance compared to $B$ and is similar to $B_{DPO-R}$ (Figure \ref{fig:dumt} center). \dumt achieves higher performance on the Chat Hard, Reasoning and Safety benchmarks while decreasing on Chat. The benchmarks on which \dumt im/deproves are those where less/more human-like tone is implicitly rewarded, e.g., in the Math-PRM benchmark (subset of Reasoning), 
the pronoun ``I'' is in 94\% of the wrong answers and $\approx0\%$ of the correct ones; see App. \ref{app:reward} for more details. For Chat, which captures the human-like task of conversing, the chosen answers are more human-like, e.g., a response starting with ``Sure, I can help with that!'' is chosen over a response that directly answers the user's query. For Chat, chosen answers have significantly higher \humt than the rejected ones ($t$-test, $p<0.05$).


































\paragraph{Evaluation: Human annotation}
To understand human preferences on \dumt, we conducted a small-scale study where participants compared outputs from \dumt and $B$. We randomly selected 500 prompts $p$ in the test set where \dumt’s output is substantively less human-like than $B$’s ($\humt(\dumt(p)) - \humt(B(p)) > \varepsilon$, with $\varepsilon = 0.02$, covering $\sim30\%$ of the test set). For each prompt, annotators chose between the outputs and optionally added comments on their choice. We collected three annotations per prompt and determined preference by majority vote. We ran a pilot in December 2024 and the full study in February 2025 on Prolific. Full details are in App. \ref{sec:prolific}.
We find that \dumt and $B$ are preferred for 40\% and 36\% of responses respectively, though the difference is statistically insignificant ($z-$test, $p<0.05$) (Figure \ref{fig:dumt} right).
Corroborating our previous finding that the preference for less human-like tone is more distinct in the PRISM dataset (\S\ref{sec:pref}), we find that the preference for \dumt over $B$ is higher for PRISM (44\% vs. 35\%) than LMSys (40\% vs. 35\%) or UF (38\% vs. 37\%).

\textbf{Reasons for dis/preferring \dumt} Annotators' comments provide some insight into reasons for preferences: we find that annotators preferred $B$ for being more ``friendly,'' ``personable'', ``casual'', and ``engaging'', which reflects our finding that human-like tone in LLMs is associated with warmth and social connection (\S\ref{sec:soccorr}). Building on previous work that preferences on \ant are mixed \cite{kirk2025prism}, other annotators identified outputs from $B$ as ``too friendly.'' 

We outline two reasons that annotators preferred \dumt: First, the less human-like responses from \dumt are \textbf{more information-dense}: they deliver more relevant information in fewer words. Unlike baseline outputs, \dumt responses omit quips, pleasantries, and follow-up questions. Annotators found this clearer and more concise (``less wordy'', ``more to the point'') The information provided is also more thorough (``gives [more] detail''); another user described it as more ``formal and informative.'' Second, they are \textbf{more authentic} to LLMs' capabilities. One annotator said ``I really don't like the AI implying that it's sorry since they do not feel things'', while another disliked ``when AI is patronizing and pretend to care about things they can't physically care about'' (full examples in Table \ref{tab:fullpairdiffs}).

\section{Discussion and Future Work}

We find that human-like tone may be linked to adverse impacts (H1) and is overall dispreferred (H2). 
While it is preferred in certain settings (e.g., greetings), these relatively rare settings may be disproportionately emphasized in popular benchmarks. For instance, Chat constitutes 1/4 of the overall RewardBench score.  
Our findings motivate the need for benchmarks that measure model performance without conflating it with human-like language.








Moreover, to better align LLMs, we must develop a deeper understanding of when and how LLM outputs should be human-like \cite{aisi2024ppl}.
This sociotechnical question requires balancing immediate preference versus long-term benefits \cite{kirk2025humanairelationshipsneedsocioaffective} and company profit motives versus societal wellbeing \cite{schanke2021estimating, suresh2024participation}.
\humt, \soct, and \dumt enable future work on measuring and navigating appropriate levels of human-like tone and other social perceptions across different settings \cite{bhattacharjee2024understanding}. These metrics can support multi-turn evaluations \cite{ibrahim2025multi} and human-subject experiments to rigorously assess the impacts of human-like language across the growing landscape 
of LLM applications.





































































































































































































\section{Limitations}

\humt measures human-like \textit{tone} in language specifically. There are other important dimensions of human-likeness beyond tone, especially since human-likeness is a construct whose definition shifts based on cultural contexts and other factors \cite{Heyselaar2023-vv,devrio2025taxonomy}. For example, the ability to solve complex reasoning problems could be considered human-like. These other facets of \ant are out of the scope of this work.

The datasets we use focus on general-purpose LLMs and do not exhaustively cover the various applications in which LLMs are used. Research shows that preferences vary by context, e.g., people favor more empathetic language in emotional support use cases \cite{bhattacharjee2024understanding}. Our metrics support future research on the impacts of human-likeness across these different applications.

Our analysis is only on English data and reflect the WEIRD (Western, Educated, Industrialized, Rich, and Democratic) norms inherent to our chosen LLM (GPT-2), which is rooted in American English \cite{bender2021dangers,jha-etal-2023-seegull,bianchi2023easily,cheng-etal-2023-marked}. This limits the generalizability of our findings to other cultures where social perceptions may differ significantly \cite{wetzel1988powerless}. However, the method can be easily adapted to reflect other cultural contexts.




Also, our metric is not intended to make essentialist claims about how specific individuals speak, as language can reinforce norms around who is considered as fully ``human'' by society, while 
preventing other individuals from achieving that status \cite{wynter2003unsettling,butler2004undoing}. Work in disability studies also challenges the popular conceptions of human-like tone that we focus on in this paper \cite{henner2023unsettling}. Rather, our metrics are tools to capture aggregate societal patterns that further our understanding of \ant in LLM outputs. As in prior research on linguistic features linked to different social constructs, we aim to highlight how certain types of language, particularly from LLMs, may reinforce harmful power dynamics.




\bibliography{custom,paperpile}
\bibliographystyle{acl_natbib}

\appendix

\renewcommand{\thetable}{A\arabic{table}}

\renewcommand{\thefigure}{A\arabic{figure}}

\setcounter{figure}{0}

\setcounter{table}{0}































\section{Unsupervised Implicit Speaker Discovery}\label{sec:unsup}

 To construct $D^+$ and $D^-$, we conducted a deductive thematic analysis to identify the prevalent implicit speakers of LLM outputs.
Specifically, we used a form of \humt to deduct implicit roles in an unsupervised manner using BERT, a masked language model. We identified the set of words
 $$r_s = \text{argmax}_{w_1,w_2,\hdots,w_{15}} (P(w\text{ said}~ s)),$$
i.e., the words that had the highest probability to fill the masked position $w$ in ``$w~\text{said}~ s$'' for $s \in S$, where $S$ is a dataset of LLM outputs.  Then, we identified the top 200 words $w$ that occurred most frequently in $\{ r_s \mid s \in S\}$. We then clustered the words using K-means clustering.


Like the personas identified by \citet{zheng2024ahelpfulassistantreally}, we find that many of the words are occupations due to the topics of the prompts rather than the underlying tone or implicit framing. We focus not on these occupation-specific words but instead on more generic words that reflect broader language ideologies.






\begin{table*}[]\tiny
    \centering
    \begin{tabular}{|c|p{0.44\linewidth}|p{0.44\linewidth}|}
    \hline
    $D$&\textbf{Outputs with $T_D < 0$}&\textbf{Outputs with $T_D>0$}\\\hline\hline
    \humt & 

The prompt that would give the response "a=[1] whi...\newline def reverse\_sentence(sentence):
    return sentenc...\newline Here is a simple ABAP program to select data from...

    & I'd like to eat healthy.\newline I think pro-lifers are misguided.\newline You can call me Claude.


    \\\hline\hline
       social & A capybara is an animal that lives in the Americas\newline The Doctor and his companions traveled to the plan...\newline 


The MUD environment has a large stone archway with...

& 

Thank you for sharing that.\newline David has 2 brothers.
\newline I am against gun laws.\\\hline

fem & Islam and Christianity are two different religions...\newline The Planck's constant in eV is $6.626 × 10^{-34}$.\newline The Korean War broke out on June 25, 1950.

& 

Tralala? Tralala? Tralala?\newline 

Hello! How can I help you?\newline 
Here is the information you requested:\\\hline

warm & 

It is up to the citizens of the United States to...\newline Based on the information provided, the name of the\newline This election used FPTP. 
& "Everyone can sing, sometimes better or worse"
\newline How do I talk to a friend about their boyfriend?\newline 
Hello! I love you too.

\\\hline
status & How do you manage your time?\newline What is cryptography?\newline No, I remember our conversation. & Advent is a Christian season of hope and preparation\newline No single person or entity rules the world. \newline Cook the turkey and other foods in advance.
\\\hline

    \end{tabular}
    \caption{Examples of LLM outputs with low and high scores for different dimensions. 
    }
    \label{tab:examples}
\end{table*}
\section{Related Work on Measuring Social Perceptions}
Our \soct metrics build on previous work measuring dimensions such as intimacy \cite{pei2020quantifying}, warmth and competence \cite{fraser2021understanding} and gender \cite{cheng2023social}. In contrast to these previous works, our approach does not require a supervised approach or training on any particular dataset as it relies only on probabilities from an LLM. Moreover, our approach specifically focuses on measuring the implicit speaker or source of the text rather than the information present in the text.
\section{Human Annotation for Validating \humt and \soct}\label{sec:humanann}
\begin{figure*}
\centering
       \includegraphics[width=0.7\linewidth]{imgs/anno1.png}
    \includegraphics[width=0.7\linewidth]{imgs/anno2.png}
    \caption{\textbf{Annotation interface for validating \humt and \soct.}}\label{fig:interface}
\end{figure*}
\begin{figure*}
\centering
       \includegraphics[width=0.45\linewidth]{imgs/Human-likeness_boxplot.pdf}
    \includegraphics[width=0.45\linewidth]{imgs/Social_distance_boxplot.pdf}
    \includegraphics[width=0.45\linewidth]{imgs/Warmth_boxplot.pdf}
    \includegraphics[width=0.45\linewidth]{imgs/Gender_boxplot.pdf}\includegraphics[width=0.45\linewidth]{imgs/Status_boxplot.pdf}
    \caption{\textbf{Box plots of $T_D$ versus majority vote labels from human annotators.} We find statistically significant differences between these distributions.}\label{fig:boxplots}
\end{figure*}
Examples of the annotation interface shown to the student volunteers are in Fig. \ref{fig:interface}. We also show box plots of $T_D$ versus  human annotator labels (determined by majority vote) in Fig. \ref{fig:boxplots}.

\paragraph{Statistical Analysis}
We perform two statistical tests to evaluate two different aspects of the annotations. First, we perform the two-sample t-test to see whether the mean $T_D$ between the positive and negative labels is significantly different. Additionally, we use the $\chi^2$ test to confirm that the binary labels match the sign of $T_D$.

For the $t-$test, we compare the mean $T_D$ between the positive and negative majority vote labels. We find statistically significant differences for all dimensions on both datasets. All the $p-$values are $< 0.001,$ except for on status in the LLM outputs, which has $p < 0.01$. 
Next, we compute $\chi^2$ between the majority vote labels and the sign of $T_D$ and find statistically significant differences in $T_D$ sign, with all the $p-$ values are $< 0.001$, except for gender ($p = 0.006, 0.001$ on C4 and LLM outputs respectively) and status ($p = 0.014$ on both).
Thus, the $T_D$ scores differ significantly between the positive and negative labels and correspond to the sign of $T_D$, establishing the construct validity of our metrics.
\paragraph{Annotator Information} Our approach (4 annotators annotating 600 texts each) exceeds standards in similar work \cite{cheng-etal-2024-anthroscore,rao2025normAd,su2025ailiedar}. \citet{cheng-etal-2024-anthroscore} validate their metric using two authors annotating 400 sentences; \citet{su2025ailiedar} have 3 students each annotate 120 texts to validate their automatic evaluator; and \citet{rao2025normAd} have 300 data points annotated by 3 workers each for validating their dataset and 480 datapoints annotated by 2 students each for validating the model evaluation setup. Our annotators were students who had relevant expertise in NLP and received detailed instructions on the constructs we aimed to measure, and they were familiar with the project because they were working on closely related research. This familiarity provided relevant context as our goal is to capture dominant societal perspectives.

\paragraph{Power Analysis}
We conducted a power analysis to justify the sample dataset size: Among the dimensions, the smallest effect size (calculated as Cohen’s $d$) is 0.78. A power analysis using a t-test for two independent samples reveals that the necessary sample size is 27 given effect size 0.78, alpha 0.05, and desired power 0.8 (as is standard). Since the necessary sample size decreases with larger effect sizes, and all other dimensions have larger effect sizes, our current sample size is more than sufficient for ensuring adequate power to validate each dimension.















\begin{figure*}
    \centering



    \includegraphics[width=0.45\linewidth,trim=0 0.5cm 0 0cm, clip]{imgs/rolescore_llmsys.pdf}
    \includegraphics[width=0.45\linewidth,trim=0 0.5cm 0 0cm, clip]{imgs/rolescore_UF.pdf}

    \includegraphics[width=0.45\linewidth,trim=0 0.5cm 0 0cm, clip]{imgs/rolescore_shp.pdf}
    \includegraphics[width=0.45\linewidth,trim=0 0.6cm 0 0cm, clip]{imgs/rolescore_hhrlhf.pdf}
    \includegraphics[width=0.45\linewidth,trim=0 0.6cm 0 0cm, clip]{imgs/rolescore_Greeting.pdf}
    \includegraphics[width=0.45\linewidth,trim=0 0.5cm 0 0cm, clip]{imgs/rolescore_Election.pdf}




    \caption{\textbf{Differences in mean \humt and \soct between dis/preferred outputs in each dataset and within PRISM Topics.} Error bars denote 95\% CI; all differences are statistically significant except those marked with \textasciicircum.}
    \label{fig:fullprefs}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.45\linewidth,trim=0 0.5cm 0 0cm, clip]{imgs/rolescore_gpt.pdf}
    \includegraphics[width=0.45\linewidth,trim=0 0.5cm 0 0cm, clip]{imgs/rolescore_LLM_vs_C4.pdf}
            \includegraphics[width=0.45\linewidth,trim=0 0.5cm 0 0cm, clip]{imgs/rolescore_human_persuasion.pdf}
    \includegraphics[width=0.45\linewidth]{imgs/rolescore_models.pdf}
    \includegraphics[width=0.45\linewidth,trim=0 0.5cm 0 0cm, clip]{imgs/rolescore_conspir.pdf}

    \caption{\textbf{Comparing mean \humt and \soct across different dimensions of datasets.} We see differences between human-written and LLM-generated text, between different LLMs, and based on efficacy in debunking conspiracy theories. Error bars denote 95\% CI; all differences are statistically significant except those marked with \textasciicircum.}
    \label{fig:fullcomp}
\end{figure*}






















    


    














\begin{figure*}
    \centering
    \includegraphics[trim={2cm 0 0 0},clip,width=0.8\linewidth]{imgs/log_comparison.pdf}
    \caption{\textbf{Distribution of log probability of he said/she said versus it said} (numerator versus denominator of Equation \ref{eqn:td} for measuring \humt). The distributions of P(x|s/he said) vs P(x|it said) are comparable, as they exhibit similar trends and lie along the $y = x$ line.}
    \label{fig:enter-label}
\end{figure*}

\section{Topic Modeling for LM-Sys}\label{sec:lmsystopics}
To understand differences in \humt among different topics for LMSys, we first performed topic modeling for LMSys. We used the sentence embedding model \texttt{all-mpnet-base-v2} \cite{reimers-gurevych-2019-sentence} to generate embeddings for each user prompt. We then clustered these embeddings into 10 topics using K-means via the BERTopic Python package \cite{grootendorst2022bertopic}. We assigned names to each cluster based on qualitative inspection of examples from each cluster. The clusters we identified are: everyday scenarios, AI/technology, coding, life advice, creative writing, economics and business, academic questions, pop culture, politics/global issues, and math questions.
Figure \ref{fig:lmsys} depicts the results of computing \humt on the preferred versus dispreferred responses for each topic: preferred responses have statistically significantly lower \humt for every topic (except math questions, for which the difference is not statistically significant).
\begin{figure*}
    \centering
    \includegraphics[width=0.9\linewidth]{imgs/lmsys_topics_humt.pdf}
    \caption{\textbf{Difference in \humt between preferred and dispreferred responses by topic in LMSys.} While the outputs for each topic have different mean \humt, we find that preferred responses have lower \humt for all topics.}
    \label{fig:lmsys}
\end{figure*}
\section{Other comparisons using \humt and \soct}





    

















        
        





         

        




















\subsection{Differences between Models}\label{sec:diffmodels}
We can also compare different models, such as GPT-2 vs Claude. Claude's responses are implicitly more human-like (Figure \ref{fig:fullcomp}. We show this to demonstrate how \humt and \soct can be used to quantify qualitative or subtler differences in how different models respond to user prompts.   
 In contrast to prompt-based methods to explore implicit characteristics of text \cite{dunlap2024vibecheckdiscoverquantifyqualitative,eloundou2025firstperson}, our method directly uses probabilities from an LLM, offering a more cost-effective, interpretable, and reliable approach. Thus, it not only scales efficiently to large datasets where prompt-based methods are prohibitively expensive, but also eliminates the need for extensive data or relative comparisons, thus enabling analysis for single outputs and small datasets.


\subsection{Downstream Impacts}\label{sec:downstream}

We further examine how $T_D$ relate to downstream impacts such as persuasiveness. We examine the data from \citet{costello2024durably}, which show that LLMs can be used to debunk conspiracy theories, to see how implicit framings play a role in the effectiveness of such language. In this dataset of LLM responses that aim to convince users out of believing conspiracy theories, while we do not find statistically significant differences in human-like tone in this context,  we find that more convincing responses are similarly less intimate, feminine, and warm (Fig. \ref{fig:fullcomp}). 

Note that these findings reflect information about comparisons \textit{among} LLM outputs only, and do not hold for human writing. We find that human writing has higher $T_{\text{humanlike}}$, $T_{\text{warmth}}$, and $T_{\text{intimacy}}$ (Fig. \ref{fig:fullcomp}), and yet is on average more persuasive than LLM-generated responses \cite{durmus2024persuasion}. 

This suggests that while {LLMs' approximations of humanness, wamrth, and intimacy } are not effective for persuasion, human-written content which may have the same surface-level markers of human-like tone can be more effective due to other features \cite{song-etal-2025-assessing}.


















































\begin{figure*}
    \centering
    \includegraphics[width=0.3\linewidth]{imgs/llm_corr_C4.pdf}
            \includegraphics[width=0.3\linewidth]{imgs/llm_corr_HH-RLHF.pdf}
                        \includegraphics[width=0.3\linewidth]{imgs/llm_corr_SHP.pdf}
                                            \includegraphics[width=0.3\linewidth]{imgs/llm_corr_UF.pdf}    
                    \includegraphics[width=0.3\linewidth]{imgs/llm_corr_LMSys.pdf}
                        \includegraphics[width=0.3\linewidth]{imgs/llm_corr_PRISM.pdf}

    \caption{\textbf{Correlations across \humt and \soct Dimensions on each dataset.}
    Human-like tone is correlated with social closeness, warmth, and femininity, and negatively correlated with status. This reifies existing stereotypical associations between femininity and low status. Note that LLM outputs are more often in conversational format than web data (C4) and thus have stronger correlations between \humt and \soct. }
    \label{fig:corr}
\end{figure*}




















\section{\dumt Model Training and Evaluation Details}
\label{appendix:DPO}
 We train the model using the default hyperparameters of: learning rate of 2e-4, batch size 1, and 10 epochs.




\paragraph{Model size} We experimented with two different models, \texttt{Qwen2-0.5B-Instruct} and \texttt{Meta-Llama-3-8B-Instruct}. While the smaller model also has a significant decrease in \humt scores, the outputs are of noticeably lower quality, so we use the latter model in our analysis. 

\paragraph{Threshold $t$ and dataset size $n$} We experimented with different threshold and dataset size selections for the preference dataset construction: 0, 0.05, 0.1, 0.15, and 0.2 on the PRISM dataset. Results of the model evaluations are shown in Figure \ref{fig:dumtthresholds}. We find that smaller thresholds (0, 0.05) and larger dataset sizes lead to larger decreases in mean \humt. We also find that higher thresholds lead to lower performance. We hypothesize that this is because when we use a higher threshold, the diversity in the data is lower and results in the model deviating from its pre-existing tuning in undesired ways.
We ultimately choose $n = 500, t = 0.0$ since this model has significantly lower \humt than baseliens and performs the best overall on RewardBench.








\paragraph{Ablation: Prompting}

We also tried various prompting-based methods as a baseline approach for reducing human-likeness but found that they led to much worse performance than steering using DPO. However, we both qualitatively observed, and confirmed via a study on Prolific, that such outputs are dispreferred since they often seemed too ``mechanical'' or ``robotic,'' with awkward and unnatural wording. We hypothesize that prompting is insufficient as explicitly invoking the concept of "human social closeness" or other related terms makes the outputs seem very robotic and stilted, while our method, which more implicitly reduces human-likeness, results in outputs that remain useful and understandable. Overall, our method is more controllable and precise than prompting. 



\begin{figure*}
    \includegraphics[width=0.9\linewidth]{imgs/mean_humt_by_model_full.pdf}
      \includegraphics[width=0.9\linewidth]{imgs/rewardbench_full_500.pdf}
      \includegraphics[width=0.9\linewidth]{imgs/rewardbench_full_1000.pdf}
      \includegraphics[width=0.9\linewidth]{imgs/rewardbench_full_2000.pdf}
      \caption{\textbf{Model evaluation with different dataset sizes $n$ and thresholds $t$.} We evaluate \dumt trained with different sizes of preference datasets ($n \in \{500, 1000, 2000\}$ and threshold for \humt difference between each pair in the dataset ($t \in \{0.0, 0.05, 0.1, 0.15, 0.2\}$). We find that models trained using $t > 0.05$ do not decrease \humt as successfully as $t = 0.05, 0.0$ find that larger dataset size $n$ leads to lower \dumt. We present $n = 500, t = 0.0$ in the main text as that model has the highest RewardBench performance.}\label{fig:dumtthresholds}
\end{figure*}


\subsection{RewardBench Details}\label{app:reward}

\begin{table}[]\small
\begin{tabular}{lrrr}
                      & \multicolumn{1}{l}{$B$}  & \multicolumn{1}{l}{$B_{DPO-R}$} 
                      & \multicolumn{1}{l}{\dumt}\\\hline
\textbf{overall}      & 0.65                    & 0.68                     & 0.70                        \\
alpacaeval-easy       & 0.91                    & 0.22                     & 0.12                       \\
alpacaeval-hard       & 0.91                    & 0.28                     & 0.16                       \\
alpacaeval-length     & 0.73                    & 0.8                      & 0.78                       \\
donotanswer           & 0.47                    & 0.84                     & 0.77                       \\
hep-cpp               & 0.78                    & 0.74                     & 0.79                       \\
hep-go                & 0.77                    & 0.75                     & 0.8                        \\
hep-java              & 0.74                    & 0.76                     & 0.81                       \\
hep-js                & 0.76                    & 0.76                     & 0.84                       \\
hep-python            & 0.75                    & 0.75                     & 0.84                       \\
hep-rust              & 0.74                    & 0.7                      & 0.77                       \\
llmbar-adver-GPTInst  & 0.22                    & 0.66                     & 0.71                       \\
llmbar-adver-GPTOut   & 0.62                    & 0.53                     & 0.49                       \\
llmbar-adver-manual   & 0.35                    & 0.67                     & 0.72                       \\
llmbar-adver-neighbor & 0.22                    & 0.84                     & 0.88                       \\
llmbar-natural        & 0.69                    & 0.64                     & 0.59                       \\
math-prm              & 0.54                    & 0.91                     & 0.81                       \\
mt-bench-easy         & 0.95                    & 0.86                     & 0.86                       \\
mt-bench-hard         & 0.70                    & 0.49                     & 0.51                       \\
mt-bench-med          & 0.84                    & 0.68                     & 0.83                       \\
refusals-dangerous    & 0.72                    & 0.88                     & 0.96                       \\
refusals-offensive    & 0.75                    & 0.79                     & 0.86                       \\
xstest-should-refuse  & 0.70                    & 0.82                     & 0.83                       \\
xstest-should-respond & 0.74                    & 0.74                     & 0.49                    
\end{tabular}
\caption{\textbf{Detailed breakdown of RewardBench performance.}}\label{tab:reward}
\end{table}
The full breakdown of RewardBench results is in Table \ref{tab:reward}. The drastic improvement of \dumt may reveal a limitation of the benchmark itself: rather than reflecting meaningful differences in reasoning or other capabilities, these benchmarks may instead have a stronger signal of stylistic differences. For instance, the pronoun ``I'' is in 94\% of the wrong answers and $\approx0\%$ of the correct ones for the Math-PRM benchmark (subset of Reasoning). Similarly, human-like affirmations like ``Certainly!'' and ``Definitely!'' are more frequent in correct responses (25\%) than in incorrect ones (11\%) for the LLM-adversarial benchmarks (subset of Chat Hard).
































































































































\section{Prolific study details}\label{sec:prolific}
We conducted our study on the Prolific platform in December 2024 with a pilot version of \dumt and again in February 2025 with an updated version of \dumt.
We recruited a total of 492 US-based participants with each participant providing preferences on 5 examples. 
The task took participants an average of 5 minutes, and they were compensated at an equivalent hourly rate of \$12 USD. The minimum wage in the US is \$7.25 USD.
A screenshot from the web interface is in Fig. \ref{fig:interf}.

To obtain high-quality responses, we designed our interface to mandate that participants spend at least 30 seconds on each instance and included an attention check in the middle of the task. We also randomized the order within each pair to prevent bias based on output order.

\begin{figure*}
    \centering\includegraphics[width=0.9\linewidth]{imgs/introduction.png}

    \centering\includegraphics[width=0.9\linewidth]{imgs/interfae.png}
    \caption{Instructions and web interface for collecting annotator preferences on \dumt versus the baseline model.}
    \label{fig:interf}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.3\linewidth]{imgs/annotation_prism.pdf}
    \includegraphics[width=0.3\linewidth]{imgs/annotation_lmsys.pdf}
    \includegraphics[width=0.3\linewidth]{imgs/annotation_uf.pdf}
    \caption{\textbf{Preferences for \dumt versus baseline for different subsets of the test set.} On PRISM, less human-like responses from \dumt are preferred more often than the baseline compared to LMSys and UltraFeedback.}
    \label{fig:dumtpref}
\end{figure*}

Aggregated preferences by majority vote are in Figure \ref{fig:dumtpref}. This follows our findings in Sec. \ref{sec:pref} that the preference against human-like tone is stronger in PRISM than LMSys.

\input{full_dpo_table}






















    

















        
        





         







\end{document}