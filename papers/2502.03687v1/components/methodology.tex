In this work, we present diffusion classifiers for medical imaging classification tasks. We first present an overview of diffusion models in Section \ref{diffusion}. Next, in Section \ref{diffusionclassifiers}, we define conditional diffusion models and demonstrate how they can perform classification. Section \ref{extensions} introduces all extensions to the diffusion classifier, including: our novel algorithm for improving classification performance through majority voting, as well as the ability to perform counterfactual explainability and uncertainty quantification without any modifications.%, in Sections \ref{explainability} and \ref{uncertainty}, respectively.


\subsection{Diffusion Models}\label{diffusion}

Diffusion models (DM) are likelihood-based models that learn to approximate a data distribution through a process of iterative noising and denoising involving two key phases: a fixed forward process and a learned backward process. In the forward process, Gaussian noise $\bm{\epsilon} \sim \mathcal{N}(0, \bm{\text{I}})$ is gradually added to data in a controlled manner, destroying its structure until it is pure Gaussian noise. 
%A diffusion model generates data through the reversal of a destruction process. Most conveniently, this destruction process is the gradual addition of Gaussian noise, $\bm{\epsilon} \sim \mathcal{N}(0, \bm{\text{I}})$, to data. 
This process, which is done on a sample over time, can be expressed by its marginal for all $t$ on a continuous interval, $[0,1]$:
\begin{equation}
    q(\bm{z}_{t}|\bm{x}) = \alpha_\lambda \bm{x} + \sigma_\lambda \bm{\epsilon} \text{ where } \bm{\epsilon} \sim \mathcal{N}(0, \bm{\text{I}}).\label{reparameterized_marginal}
\end{equation}
The forward process %(or destruction of $\bm{x}$) 
is defined to be variance-preserving, imposing the constraint $\alpha_\lambda^2 = \text{sigmoid}(\lambda)$, $\sigma_\lambda^2=\text{sigmoid}(-\lambda)$, where $\lambda$ is the log-SNR given by $\lambda = \log \alpha_\lambda^2/\sigma_\lambda^2$. The noise schedule is a monotonically decreasing and invertible function, $f_\lambda(t)$, that connects the time variable, $t$, with the log-SNR, $\lambda$: $\lambda= f_\lambda(t)$. During training, $t$ is sampled from $\mathcal{U}(0,1)$ which is then used to compute $\lambda$. The resulting distribution over noise levels can be defined as $p(\lambda)=-1/f'_\lambda(t)$~\cite{kingma2023understandingdiffusionobjectiveselbo}.%  Noise distributions were chosen to be derived from the shifted cosine schedule ~\cite{Hoogeboom:arXiv:2023:simpleDiffusion}. % are commonly derived from cosine schedules~\cite{nichol2021improveddenoisingdiffusionprobabilistic}, though alternative options are routinely explored in literature~\cite{Hoogeboom:arXiv:2023:simpleDiffusion, karras2022elucidatingdesignspacediffusionbased, hang2024efficientdiffusiontrainingminsnr}.

In the backward process, a neural network attempts to learn how to remove the added noise and recover an approximate sample from the original data distribution. Kingma et al. show that the variational lower bound objective (VLB) function for training diffusion models can be derived in continuous time with respect to its log-SNR, $\lambda$, noise sampling distribution, $p(\lambda)$ and weighting function, $w(\lambda)$~\cite{kingma2023variationaldiffusionmodels}. This VLB is:
\begin{equation}
    \log p(x) = \mathcal{L}_x + \mathcal{L}_T - \mathbb{E}_{\epsilon \sim \mathcal{N}(0,\text{I}), \lambda \sim p(\lambda)} \left[ \frac{w(\lambda)}{p(\lambda)} ||\bm{x} - \hat{\bm{x}}_\theta(\bm{z}_\lambda; \lambda)||_2^2 \right]. \label{eq:ELBO}
\end{equation}
Where $\mathcal{L}_x=-\log p(\bm{x}|\bm{z}_0) \approx 0$ for discrete $\bm{x}$ and $\mathcal{L}_T=D_{KL}(q(\bm{z}_T|x)||p(\bm{z}_T)) \approx 0$ for a well-defined forward process. We use a min-SNR weighing function~\cite{hang2024efficientdiffusiontrainingminsnr}, a shifted-cosine noise schedule~\cite{Hoogeboom:arXiv:2023:simpleDiffusion}, and v-prediction parameterization for greater stability during training and sampling~\cite{salimans2022progressivedistillationfastsampling}.

% Conventionally, an un-weighted loss is used, where $w(\lambda)=1/p(\lambda)$ \cite{ho2020denoisingdiffusionprobabilisticmodels}.  and parameterize the marginal in %Additionally, the parameterization of the marginal inEquation~\ref{reparameterized_marginal} %leaves the door open for such that $\hat{\bm{x}}_\theta$ can be obtained via  v-prediction ($\hat{\bm{v}}_\theta$) for improved stability~\cite{salimans2022progressivedistillationfastsampling}. %alternative predictions that have been found to be more stable, including epsilon- ($\hat{\bm{\epsilon}}_\theta$) and v-prediction ($\hat{\bm{v}}_\theta$)~\cite{salimans2022progressivedistillationfastsampling}.

\begin{comment}
Commonly, a large, pre-trained variational autoencoder (VAE) is used to reduce the dimensionality of images~\cite{rombach2022highresolutionimagesynthesislatent}. However, outside of natural images, the scale of data required to train a VAE for a specific use case is not always available.
\end{comment}

\begin{comment}
Various approaches can be made to reduce the computational burden of high resolution feature maps while training diffusion models. Discrete wavelet transforms (DWTs)~\cite{grgic1999imagecompression} offer a lossless alternative to VAEs for image compression by decomposing an image into its frequency responses, which naturally have dimensions 1/2 the size and a channel count 4x the size, per level of decomposition. Modern accelerators offset the cost of scaling the number of channels, while the dimension reduction provides ample relief in memory burden.
\end{comment}

%Conditioning diffusion models on text or categorical inputs can be addressed via architectural design, such that the prediction becomes $\hat{\bm{x}}_\theta(\bm{z}_\lambda, c)$ where $c$ is a conditioning embedding. Rombach et al. \cite{rombach2022highresolutionimagesynthesislatent}, proposed cross-attention mechanisms at lower resolution levels of a UNet~\cite{Ronneberger:arXiv:2015:unet} to incorporate conditioning information. Peebles et al. proposed the diffusion transformer (DiT) architecture \cite{peebles2023scalablediffusionmodelstransformers}, where images are patchified and projected as a sequence of tokens, and adaptive layer normalization is used to embed conditioning information. We implement both conditional UNet and DiT architectures for performance comparison.

\begin{comment}
\subsection{Conditional Diffusion Models}

Diffusion models can be made conditional with architectural adaptations, with guided sampling strategies, or with both.

\subsubsection{Architecture-Based Conditioning}

Conditioning diffusion models on text or categorical input can be addressed by architectural design, such that the prediction becomes $\hat{\bm{x}}_\theta(\bm{z}_\lambda, \bm{c})$ where $\bm{c}$ is a conditioning embedding. Rombach et al.~\cite{rombach2022highresolutionimagesynthesislatent}, proposed the latent diffusion model (LDM) architecture, which operates in the latent space of a variational autoencoder and uses cross-attention mechanisms at lower resolution levels of a UNet to incorporate conditioning information. Peebles et al. proposed the diffusion transformer (DiT) architecture~\cite{peebles2023scalablediffusionmodelstransformers}, where pixel- or latent-space images are patchified and projected as a sequence of tokens, and adaptive layer normalization is used to embed conditioning information. 
\end{comment}

\subsection{Conditional Diffusion Models as Classifiers} \label{diffusionclassifiers}

Conditional diffusion models incorporate text or categorical inputs, such that the prediction becomes $\hat{\bm{x}}_\theta(\bm{z}_\lambda, c)$ where $c$ is a conditioning embedding. In this paper, we implement conditioning through cross-attention in a UNet-based diffusion model ~\cite{rombach2022highresolutionimagesynthesislatent}, and adaptive layer normalization in DiTs~\cite{peebles2023scalablediffusionmodelstransformers}. 
%Rombach et al.~\cite{rombach2022highresolutionimagesynthesislatent}, proposed the latent diffusion model (LDM) architecture, which operates in the latent space of a variational autoencoder and uses cross-attention mechanisms at lower resolution levels of a UNet to incorporate conditioning information. Peebles et al. proposed the diffusion transformer (DiT) architecture~\cite{peebles2023scalablediffusionmodelstransformers}, where pixel- or latent-space images are patchified and projected as a sequence of tokens, and adaptive layer normalization is used to embed conditioning information.   

Recent works~\cite{Li:arXiv:2023:diffusionClassifier, clark2023texttoimagediffusionmodelszeroshot, Krojer:arXiv:2023:DiffusionReasoners, chen2024robustclassificationsinglediffusion} have explored using conditional diffusion models as discriminative classifiers. As shown in Figure~\ref{fig:architecture}, classification is performed by comparing reconstruction errors across images generated for each possible conditioning input. Specifically, using the labels, $\bm{C}=\{c_i\}$, and Bayes' theorem on model predictions, $p(\bm{x}|c_i)$, we can derive $p(c_i|\bm{x})$:
\begin{equation}
    p(c_i|\bm{x}) \approx \frac{\text{exp} \{\mathbb{E}_{\epsilon \sim \mathcal{N}(0,\text{I}), \lambda \sim p(\lambda)} \left[ ||\bm{x} - \hat{\bm{x}}_\theta(\bm{z}_\lambda, c_i)||_2^2 \right]\}}{\exp \{\sum_j\mathbb{E}_{\epsilon \sim \mathcal{N}(0,\text{I}), \lambda \sim p(\lambda)} \left[ ||\bm{x} - \hat{\bm{x}}_\theta(\bm{z}_\lambda, c_j)||_2^2 \right]\}}. \label{eq:softmaxDiffClass}
\end{equation}
A Monte Carlo estimation of the expectation for an arbitrary class, $c_j$, can be computed by sampling $N$ noise level pairs, $(\bm{\epsilon}, \lambda)$ and averaging the reconstruction error:
\begin{equation}
    \frac{1}{N}\sum^N_{k=1} \left[ ||\bm{x} - \hat{\bm{x}}_\theta(\alpha_{\lambda_k} \bm{x} + \sigma_{\lambda_k} \bm{\epsilon}_k, c_j)||_2^2 \right].\label{eq:diffClass}
\end{equation}
Equation~\ref{eq:diffClass} shows that classifying one sample requires $N$ many steps per condition, where the Monte Carlo estimate becomes more accurate as the number of steps increases. To reduce the variance of prediction for a given image, $\bm{x}$, an identical set of $(\bm{\epsilon}_k, \lambda_k) \in S\{(\bm{\epsilon}_k, \lambda_k)\}_{k=1}^N$ is used for every condition, which increases the accuracy of the prediction $p(\bm{C}|\bm{x})$. In practice, Eq. \ref{eq:softmaxDiffClass} is equivalent to choosing the class with the minimum average reconstruction error. %A high-level overview of this process is outlined in Figure~\ref{fig:architecture}.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{images/architecture.pdf}
    \caption{\textbf{Diffusion classifiers} are conditional diffusion models repurposed as classifiers. First, a sample, $\bm{x}$, is noised at a randomly chosen noise level, $(\bm{\epsilon}_k, \lambda_k)$. The noised sample is then denoised by the diffusion network with each possible conditioning input, $c_j$. The conditioning variable, $c_j$, that results in the denoised output, $\hat{\bm{x}}_\theta (\bm{z}_\lambda, c_j)$, with the smallest reconstruction error is selected as the class. This process is repeated for a set of $N$ noise levels $(\bm{\epsilon}, \lambda)$ with the reconstruction errors aggregated (e.g., average/majority voting) for a more accurate prediction.}
    \label{fig:architecture}
\end{figure}

\subsection{Extensions on Diffusion Classifiers} 
\label{extensions}

\paragraph{Majority Voting:} \label{majority-voting} In this work, we introduce a novel  majority-vote-based algorithm for determining the predicted class. Here, we tally votes across all $(\bm{\epsilon}, \lambda)$ pairs by identifying the class with the lowest error as the prediction for that pairing, and take the final predicted class as the one with the majority of individual votes (see Appendix~\ref{ref: majority-vote}). We posit that averaging reconstruction error over all noise levels inherently weights higher values of $\lambda$ more, which is not always beneficial (i.e., reconstructions at higher values of $\lambda$ are naturally much harder and thus have greater error, introducing more noise into the average reconstruction error). %Empirically, we find this rings true and that a simple majority-vote results in improved performance over an averaged estimate.
%Intuition

\paragraph{Intrinsic Explainability:} \label{explainability}
Diffusion classifiers use Classifier-Free Guidance (CFG)~\cite{ho2022classifierfreediffusionguidance} to understand which features are most influential in generating certain classes. CFG is a common approach in which a conditional diffusion model is simultaneously trained for an unconditional task by randomly dropping out $c$ ($\sim$10\% of the time). In doing so, sampling can be guided towards an intended class with a guidance-scale, $w$:
\begin{equation}
    \tilde{\bm{x}}_\theta(\bm{z}_\lambda, c) = (1+w)\hat{\bm{x}}_\theta(\bm{z}_\lambda, c) - w\hat{\bm{x}}_\theta(\bm{z}_\lambda, \varnothing).
\end{equation}
At inference, the model permits explainability for free, 
%Given their current success \cite{}, explainability is provided through the the difference maps provided 
through the conditional generation of the factual and counterfactual images of the input image. First, noise is added to obscure the input images, while preserving enough image structure to make reconstruction possible. Then, by varying the condition at inference, the model can shift its generation process to any possible conditional class. These generated images represent the reconstruction of the image provided by the true class, and any counterfactual image(s), where difference maps can be created to highlight class-specific regions modified by the network. %counterfactual explanations of the classifier decision for free.
%To generate counterfactual explanations, an image is input to the forward process of the diffusion classifier at a single noise level. The noised image is then conditioned (separately) on all potential classes, and is subsequently denoised to produce the output images corresponding to the possible classes. These images represent the reconstruction of the true class, and counterfactual image(s) corresponding to non-factual classes. Difference maps are created to highlight the regions modified by the network.

\paragraph{Uncertainty Quantification:} \label{uncertainty}
Diffusion classifiers are also able to produce uncertainty estimates without any additional modifications to the model. The set of $N$ $(\bm{\epsilon}, \lambda)$ pairs required for accurate classification results (Eq.~\ref{eq:diffClass}) results in numerous predictions generated for each sample and thus inherently resembles the  uncertainty estimation strategy via MC dropout or ensemble methods. As explained in Section \ref{diffusionclassifiers}, to achieve accurate classification, a single sample requires $N$ steps (repeated per condition) where reconstruction errors for that sample are calculated at different $(\bm{\epsilon}, \lambda)$ noise levels  (Eq.~\ref{eq:diffClass}). To quantify the uncertainty of the overall predicted class,  %(determined through majority voting from the $N$ noise level predictions), 
%given the majority voting used to select the predicted class, 
%we first calculate the proportion of correctly classified predictions, $y$, from the $N$ predictions. This proportion represents the probability of predicting the correct class, %using the proportion as probability measure, 
%and therefore we can use entropy as an uncertainty measure.
we construct a Bernoulli distribution from each of the $N$ predictions. This creates a probability density from which uncertainty can be computed. % via entropy.
%, calculated as $H(Y)=-\sum_{i}^{\textbf{C}}P(y=c_i)\ln P(y=c_i)$.  %Note that if the mean across all $(\epsilon_i, \lambda_i)$ pairs was used for class selection, standard uncertainty methods such as variance could be used on the predicted reconstructed error.