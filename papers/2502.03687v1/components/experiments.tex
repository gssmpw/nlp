We first evaluate the performance of the average reconstruction error objective (Section \ref{diffusionclassifiers}) against a majority voting alternative, demonstrating that the latter yields superior results in our tasks. We then compare the classification performance of diffusion classifiers with state-of-the-art discriminative baselines, and furthermore, show that conditional diffusion models are interpretable out-of-the-box, and capable of producing uncertainty quantifications.

\subsection{Datasets}

\paragraph{ISIC Melanoma:} A publicly available dataset~\cite{Rotemberg2020APD} containing over 35,000 images of skin lesions and corresponding labels for the presence of melanoma. We balance the dataset for our experiments, resulting in 10,212 total images. The data are randomly split into an 80/10/10 train/validation/test set.

\paragraph{CheXpert:} A publicly available dataset~\cite{irvin2019chexpert} containing over 200,000 chest X-ray images with binary labels for 14 diseases and the presence of support devices. For our experiments, we use ``Pleural Effusion'' and ``No Findings'' as mutually exclusive labels and filter for frontal views of the chest, resulting in a balanced dataset of 20,404 samples. The data are randomly split into an 80/10/10 train/validation/test set.

\begin{comment}
    \textbf{Failure Sets}: To investigate how different classifier architectures complement one another, we construct a ``failure set'' for each modelâ€”defined as the subset of the CheXpert validation data where the model achieves zero accuracy. By evaluating the performance of models on various failure sets, we gain insights into whether different architectures, methods, and perspectives of classification can compensate for limitations of their counterparts.
\end{comment}

\subsection{Baseline Models}

To establish a comparative baseline, we evaluate the performance of both convolutional and transformer-based architectures. We use \verb|torchvision| implementations of ResNet-18 and ResNet-50~\cite{he2015deep}, and \verb|timm| implementations of ViT-S/16 and ViT-B/16~\cite{dosovitskiy2020image}, EfficientNet-B0 and EfficientNet-B4~\cite{tan2019efficientnet}, and Swin-B Transformer~\cite{liu2021swintransformer}. More details can be found in Appendix \ref{ref: experimental-details}.

\subsection{Conditional Diffusion Models}

We implement a UNet backbone based on the ADM architecture~\cite{dhariwal2021diffusionmodelsbeatgans} at $256^2$ resolution, incorporating improvements from simple diffusion~\cite{Hoogeboom:arXiv:2023:simpleDiffusion}, such as scaling the number of ResBlocks at lower resolutions to save memory at higher resolutions. For transformer-based diffusion models, we include the DiT-B/4 variant from~\cite{peebles2023scalablediffusionmodelstransformers}. Unless otherwise noted, all images are compressed with a single-stage discrete wavelet transform (DWT) using a Haar wavelet. More details can be found in Appendix \ref{ref: experimental-details}.

\subsection{Foundation Models}

Ideally, foundation models like Stable Diffusion can be repurposed as zero-shot classifiers. However, we find that such models are not trained on enough medical data to perform adequately by default. Thus, to ensure a fair comparison, we fine-tune Stable Diffusion v2-base~\cite{rombach2022highresolutionimagesynthesislatent} on an amalgamation of our CheXpert and ISIC Melanoma training splits. Given that the model is designed for text-to-image generation, we replace labels in the datasets with text prompts, e.g., ``a benign skin lesion'', or, ``a frontal chest xray of a sick patient with pleural effusion''. More details can be found in Appendix \ref{ref: sd-fine-tuning}.
