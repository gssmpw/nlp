Deep learning applications in medicine have received significant attention in recent years due to their potential to revolutionize healthcare outcomes. For instance, the ability to accurately classify disease pathology from medical images using discriminative classifiers (e.g., ResNet~\cite{he2015deep}, ViT~\cite{dosovitskiy2020image}) is central to advancing early diagnosis, personalized treatment, and overall patient care. %Discriminative architectures like the ResNet~\cite{he2015deep} and the ViT~\cite{dosovitskiy2020image} have become a cornerstone in image classification, achieving state-of-the-art performance in both natural and medical imaging benchmarks like ImageNet~\cite{russakovsky2015imagenet} and CheXpert~\cite{irvin2019chexpert}, respectively. 
In the ideal scenario, discriminative classifiers are robust and generalizable; however, state-of-the-art performance often relies heavily on data augmentation and hyperparameter tuning, which can be time- and computation-expensive, and may still be prone to overfitting and/or learning shortcuts~\cite{Geirhos:2020:ShortcutLearning}.
%highlighting the need for accurate explainability and uncertainty measures.
%where unseen variations of data features, for example, in clinical settings or imaging modalities, can significantly degrade performance~\cite{Sun:arXiv:2023:RightWrongReason}.
Even with strong classification performance, models must be explainable and provide uncertainty estimates to ensure reliable and trustworthy predictions for safe clinical deployment.
Current explainability and uncertainty methods depend largely on post-hoc analysis or model modifications. For example, explainability often relies on gradient-based analysis after training~\cite{gradcam} or counterfactual generation with a separate model~\cite{sun2023inherentlyinterpretablemultilabelclassification},
%, which highlight general, non-specific areas of images and may be unreliable~\cite{adebayo2018sanitycheckssaliencymaps,arun2021trustworthinesssaliencymaps}. 
whereas uncertainty methods range from simple model modifications, like Monte Carlo (MC) dropout, to expensive ensembling methods. Thus, there remains limitations to the safe use of discriminative classifiers in medical imaging, particularly due to the lack of built-in explainability and uncertainty analysis.

\begin{comment}
Furthermore, the global and ever-growing adoption of deep learning in our societies has raised concerns about the increasing environmental burden of training large models~\cite{Patterson:arXiv:2021:carbonEmissionsLargeNeural}. Amidst the backdrop of remarkable advancements looms the substantial computational resources and energy required for training deep learning models. Given the escalating scale of modern architectures, research into more sustainable development practices~\cite{Sangarya:arXiv:2024:estimatingenvironmentalcostmodels}, including the repurposing of pre-trained models, is gaining traction. This approach enables researchers to build on existing computational investments rather than training new models from scratch, thereby reducing the environmental impact.
\end{comment}

Diffusion models~\cite{ho2020denoisingdiffusionprobabilisticmodels} make up one class of generative models that has shown remarkable flexibility and robustness across various deep learning tasks, achieving state-of-the-art performance in image~\cite{dhariwal2021diffusionmodelsbeatgans}, video~\cite{ho2022videodiffusionmodels}, and audio~\cite{kong2020diffwave} generation tasks. Recently, generative models have been used directly for image classification~\cite{Li:arXiv:2023:diffusionClassifier, clark2023texttoimagediffusionmodelszeroshot, Krojer:arXiv:2023:DiffusionReasoners, chen2024robustclassificationsinglediffusion} in natural imaging, showing that large pre-trained models like Stable Diffusion~\cite{rombach2022highresolutionimagesynthesislatent} can be used as classifiers that are competitive with state-of-the-art supervised discriminative classifiers~\cite{he2015deep, dosovitskiy2020image}. Diffusion models are increasingly being used in the medical domain for data augmentation~\cite{guo2024maisimedicalaisynthetic}, segmentation~\cite{wu2023medsegdiffmedicalimagesegmentation, wu2023medsegdiffv2diffusionbasedmedical}, anomaly detection~\cite{wolleb2022diffusionmodelsmedicalanomaly}, and probabilistic classification~\cite{shen2024improvingrobustnessreliabilitymedical}. However, despite many conditional diffusion models developed for medical image analysis, they have yet to be explored as classifiers that can provide explainability and uncertainty-estimation for free.

In this work, we present a comprehensive evaluation of how conditional diffusion models can be re-purposed and leveraged for image classification, explainability, and uncertainty estimation in the medical domain. First, we propose a novel majority voting-based method that improves the performance of diffusion classifiers in medical imaging. We then demonstrate that classifiers derived from foundation and trained-from-scratch diffusion models perform competitively with state-of-the-art medical image discriminative classifiers through extensive experiments on the publicly available CheXpert~\cite{irvin2019chexpert} and ISIC Melanoma skin cancer~\cite{Rotemberg2020APD} datasets, despite not being trained for classification. Next, we show that diffusion classifiers offer explainability (via counterfactual generation) and uncertainty quantification (via entropy) out-of-the-box. We validate the uncertainty by showing that when the model is confident, it is correct, and vice versa. This is shown as model accuracy drastically improves as its uncertainty threshold increases. For example, Stable Diffusion reaches classification accuracies of 100\% and 95\% on ISIC and CheXpert, respectively, with only 45\% of its most uncertain samples filtered out. %These qualities make these models particularly well-suited for applications in the medical domain where trustworthiness is paramount.

\begin{comment}
    In doing so, we seek to demonstrate that diffusion models, can offer competitive performance in discriminative tasks, providing a new avenue for a more environmentally sustainable approach to deploying advanced deep learning solutions in medical imaging.
\end{comment}