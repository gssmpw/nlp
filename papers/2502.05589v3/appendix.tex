\section{Appendix}

\subsection{Details of Conversation Segmentation Model}
\label{sec: segmentation_details}

We use GPT-4-0125 as the backbone LLM for segmentation. The zero-shot segmentation prompt is provided in Figure~\ref{fig: prompt4seg-zero-shot}. It instructs the segmentation model to generate all segmentation indices at once, avoiding the iterative segmentation process used in LumberChunker~\citep{duarte2024lumberchunker}, which can lead to unacceptable latency. We specify that the output should be in \textbf{JSONL} format to facilitate subsequent processing.
To generate segmentation guidance, we select the top 100 poorly segmented samples with the largest Window Diff metric from the training set. The segmentation guidance consists of two parts: (1) \textit{\textbf{Segmentation Rubric}}: Criteria items on how to make better segmentation. (2) \textit{\textbf{Representative Examples}}: The most representative examples that include the ground-truth segmentation, the model's prediction, and the reflection on the model's errors.
The number of rubric items is set to 10. To meet this requirement, we divide the top 100 poorly segmented samples into 10 mini-batches and prompt the LLM-based segmentation model to reflect on each batch individually. The segmentation model is also asked to select the most representative example in each batch, which is done concurrently with rubric generation. Figure~\ref{fig: prompt4rubric} presents the prompt used to generate rubric. The generated rubric is shown at Fig.~\ref{fig: segmentation_rubric_tiage} and Fig.~\ref{fig: segmentation_rubric_superseg} on \textit{TIAGE} and \textit{SuperDialSeg}, respectively. After the segmentation guidance is learned, we utilize the prompt shown in Figure~\ref{fig: prompt4seg} as a few-shot segmentation prompt. For simplicity and fair comparison, we do not use any rubric for conversation segmentation in \textit{LOCOMO} and \textit{Long-MT-Bench+}.

\input{figures/prompt4seg-zero-shot}
\input{figures/prompt4rubric}
\input{figures/prompt4seg}
\input{figures/segmentation_rubric}
\input{figures/poorly_segmented_example}

\subsection{Additional Cost Analysis}
\label{sec: cost}

\input{tables/cost_analysis}
Table~\ref{tab: cost_analysis} compares the overall costs involved in memory construction, memory retrieval, and response generation across different methods. The results demonstrate that our method significantly enhances performance compared to the baseline while only slightly increasing computational overhead, and it outperforms the MemoChat method in both efficiency and effectiveness.


\subsection{The Analogy between the Reflection Augmentation and Prefix-Tuning}
When a small amount of conversation data with segment annotations is available, we explore how to leverage this data to transfer segmentation knowledge and better align the LLM-based segmentation model with human preferences. Inspired by the prefix-tuning technique~\citep{li2021prefix} and reflection mechanism~\citep{shinn2023reflexion,renze2024self}, we treat the segmentation prompt as the ``prefix'' and iteratively optimize it through LLM self-reflection, ultimately obtaining a segmentation guidance $\bm{G}$.

Prefix-tuning seeks to learn a prefix matrix $\bm{P}$ to boost the performance of the language model $\operatorname{LM}_{\phi}$ without fine-tuning its parameter $\phi$. The prefix matrix $\bm{P}$ is prepended to the activation $h$ of the Transformer layer: 
\begin{equation}
h_i= \begin{cases}\bm{P}[i,:], & \text { if } i \in \mathcal{P}_{idx} \\ \operatorname{LM}_\phi\left(z_i, h_{<i}\right), & \text { otherwise }\end{cases}
\end{equation}
where $\mathcal{P}_{idx}$ is the prefix indices.

In the context of our segmentation scenario, our goal is to ``learn'' a textual guidance $\boldsymbol{G}$ that directs the segmentation model toward improved segmentation outcomes. The process of updating the segmentation guidance $\boldsymbol{G}$ parallels the optimization of the prefix parameter $\bm{P}$ in prefix-tuning. Initially, the segmentation guidance $\boldsymbol{G}_{0}$ is set to empty, analogous to the initial prefix parameter $\bm{P}_{0}$. During each iteration of guidance updating, we first apply our conversation segmentation model in a zero-shot manner to a batch of conversation data. Building upon the insights that LLMs possess the ability for self-reflection and improvement~\citep{shinn2023reflexion, renze2024self}, we then instruct the segmentation model to reflect on its mistakes given the ground-truth segmentation and update the segmentation guidance $\boldsymbol{G}$. This process mirrors Stochastic Gradient Descent (SGD) optimization:
\begin{equation}
    \boldsymbol{G}_{m+1}=\boldsymbol{G}_m-\eta \nabla \mathcal{L}\left(\boldsymbol{G}_m\right), 
\end{equation}
where $\nabla \mathcal{L}\left(\boldsymbol{G}_m\right)$ denotes the gradient of segmentation loss, which we assume is estimated implicitly by the LLM itself and used to adjust the next segmentation guidance $\boldsymbol{G}_{m+1}$.


\subsection{Prompt for GPT-4 Evaluation}
\label{sec: prompt4eval}

We use the same evaluation prompts as  MemoChat~\citep{lu2023memochat}. The LLM-powered evaluation consists of single-sample scoring (GPT4Score) and pair-wise comparison. The evaluation prompts are displayed in Figure~\ref{fig: prompt4eval}. For pair-wise comparison, we alternate the order of the responses and conduct a second comparison for each pair to minimize position bias. 

\input{figures/prompt4eval}

\subsection{Evaluation Results on the Official QA Pairs of LOCOMO}
\label{sec: main_locomo2}
As \textit{LOCOMO}~\citep{maharana2024evaluating} released a subset containing QA pairs recently. To ensure reproducibility, we evaluate our method on these official QA pairs. Table~\ref{tab: main_locomo2} presents the evaluation results. The superiority of our \sysname\ is also evident on these QA pairs, demonstrating its superior effectiveness and robustness.

\input{tables/main_locomo2}

\subsection{Case Study}
\label{sec: case_study}

To further demonstrate the advantages of our method, we conduct a qualitative evaluation. Figure~\ref{fig: case_study_segment_vs_turn} presents a specific case comparing the segment-level memory with the turn-level memory. It demonstrates that using turn-level memory units fails to address the user's request. We attribute this to the fragmentation of user-agent turns, and the critical turns may not explicitly contain or relate to the keywords in the user's request.

Similarly, using session-level memory units is also sub-optimal, as illustrated in Figure~\ref{fig: case_study_segment_vs_session}. This issue arises because a session often includes multiple topics, introducing a significant amount of irrelevant information that hampers effective retrieval. The irrelevant information also distracts the LLM, as noted in previous studies~\citep{shi2023large, liu2024lost}. 

We also conduct a case study to compare our method with two recent, powerful memory management techniques: \textit{RecurSum}~\citep{wang2023recursively} and \textit{ConditionMem}~\citep{yuan2023evolving}, as shown in Figure~\ref{fig: case_study_segment_vs_rsum} and Figure~\ref{fig: case_study_segment_vs_condmem}. The results indicate that the summarization process in these methods often omits detailed information that is essential for accurately answering the user's request.

\input{figures/case_study}

\subsection{Details of Dataset Construction}
\label{sec: dataset_details}

(i) \textit{LOCOMO}~\citep{maharana2024evaluating}: this dataset contains the longest conversations to date, with an average of  more than 9K tokens per sample. Since \textit{LOCOMO} does not release the corresponding question-answer pairs when we conduct our experiment, we prompt GPT-4 to generate QA pairs for each session as in \citet{alonso2024toward}. We also conduct evaluation on the recently released official QA pairs in Appendix~\ref{sec: main_locomo2}.

(ii) \textit{Long-MT-Bench+}: \textit{Long-MT-Bench+} is reconstructed from the 
\textit{MT-Bench+}~\citep{lu2023memochat} dataset. In \textit{MT-Bench+}, human experts are invited to expand the original questions and create long-range questions as test samples. However, there are two drawbacks when using this dataset to evaluate the memory mechanism of conversational agents: (1) the number of QA pairs is relatively small, with only 54 human-written long-range questions; and (2) the conversation length is not sufficiently long, with each conversation containing an average of 13.3 dialogue turns and a maximum of 16 turns. In contrast, the conversation in \textit{LOCOMO} has an average of 300 turns and 9K tokens. To address (1), we use these human-written questions as few-shot examples and ask GPT-4 to generate a long-range test question for each dialogue topic. For (2), following~\citep{yuan2023evolving}, we merge five consecutive sessions into one, forming longer dialogues that are more suitable for evaluating memory in long-term conversation. We refer to the reconstructed dataset as \textit{Long-MT-Bench+} and present its statistics in Table~\ref{tab: datasets_statistics}.

\input{tables/dataset_stats}

\subsection{Details of Retrieval Performance Measurement}
\label{sec: retrieval_measurement}

We measure the retrieval performance in terms of the discounted cumulative gain (DCG) metric~\citep{jarvelin2002cumulated}: 
\begin{equation} 
\textit{DCG}=\sum_{i=1}^{p}\frac{rel_{i}}{\log_{2}(i+1)}, 
\label{eq: dcg} 
\end{equation} 
where $rel_{i}$ denotes the relevance score of the retrieved user-agent turn ranked at position $i$, and $p$ represents the total number of retrieved turns. Note that in the \textit{Long-MT-Bench+} dataset, answering a single question often requires referring to several consecutive turns. Therefore, we distribute the relevance score evenly across these relevant turns and set the relevance score of irrelevant turns to zero. For instance, assume that the ground truth reference turn set for question $q$ is $\mathcal{R}(q) = \{r_{k+j}\}_{j=1}^{N}$, which is provided by the dataset. In this case, the relevance score for each turn is set as follows:
$$
\textit{rel}_{i} =
\begin{cases}
0 & i < k+1 \\ 
\frac{1}{N} & k+1 \leq i \leq k+N \\ 
0 & i > k+N
\end{cases}.
$$
This approach allows us to evaluate retrieval performance at different granularity.

\subsection{Additional Experiments on CoQA and Persona-Chat}

To further validate SeCom's robustness and versatility across a broader range of dialogue types, we conduct additional experiments on other benchmarks, \textbf{Persona-Chat}~\citep{zhang-etal-2018-personalizing} and \textbf{CoQA}~\citep{reddy2019coqa}.

Given the relatively short context length of individual samples in these datasets, we adopt an approach similar to Long-MT-Bench+ by aggregating multiple adjacent samples into a single instance. For CoQA, each sample is supplemented with the text passages of its 10 surrounding samples. Since CoQA answers are derived from text passages rather than dialogue turns, we replace the turn-level baseline with a sentence-level baseline.
For Persona-Chat, we utilize the expanded version provided by~\citet{jandaghi2023faithful}. Conversations are aggregated by combining each sample with its 5 surrounding samples. Following the next utterance prediction protocol, we include the personas of both conversational roles in the prompt. Due to the large scale of these datasets, we select subsets for experimentation. From CoQA, we randomly sample 50 instances from an initial pool of 500, resulting in a subset containing over 700 QA pairs. Similarly, for Persona-Chat, we randomly select 100 instances, encompassing over 1,000 utterances in total. 

As shown in Table~\ref{tab: results_on_coqa} and Table~\ref{tab: results_on_spc}, \sysname\ consistently outperforms baseline methods across these datasets, highlighting its effectiveness in handling diverse dialogue scenarios, including open-ended and multi-turn interactions.

\input{tables/results_on_coqa}
\input{tables/results_on_spc}

\subsection{Human Evaluation Results}
\label{sec: human_evaluation}

To ensure a holistic assessment, we conduct human evaluation to gauge the quality of the LLM's response in conversation. We adopt the human evaluation scheme of COMEDY~\citep{chen2024compress}, which encompasses five perspectives: Coherence, Consistency, Engagingness, Humanness and Memorability. Ten human annotators are asked to score the responses following a detailed rubric for each perspective. Results in Table~\ref{tab: human_evaluation} show that the rank of different methods from human evaluation is generally consistent with those obtained from automated metrics, confirming the practical effectiveness of our proposed approach. 

\input{tables/human_evaluation}

\subsection{Performance Using Smaller Segmentation Model}

To make our method applicable in resource-constrained environments, we conduct additional experiments by replacing the \texttt{GPT-4-Turbo} used for the segmentation model with the \texttt{Mistral-7B-Instruct-v0.3} and a \texttt{RoBERTa} based model fine-tuned on SuperDialseg~\citep{jiang2023superdialseg}. Table~\ref{tab: main_results_slm_seg} shows that \sysname\ maintains the advantage over baseline methods when switching from GPT-4 to Mistral-7B. Notably, even with a RoBERTa based segmentation model, \sysname\ retains a substantial performance gap over other granularity-based baselines.

\input{tables/main_results_SLM_seg}

% \input{tables/unsupervised_segment}