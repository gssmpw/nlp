\documentclass{article}
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{xurl}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage[most]{tcolorbox}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{soul}
\usepackage{amsmath}
\usepackage{makecell}
\usepackage{bm}
\usepackage{pifont}
\usepackage[export]{adjustbox}
\usepackage{booktabs}
\usepackage{ulem}

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
\usepackage{algorithm2e}
\usepackage{booktabs}
% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\usepackage{color}
\usepackage{colortbl}
\usepackage{xcolor}
% \definecolor{red_dark}{rgb}{1,0,0}
% \definecolor{red_medium}{rgb}{1,0,0,0.5}
% \definecolor{red_light}{rgb}{1,0,0,0.25}
% \definecolor{}{rgb}{1,0,0,0.5} % 50% alpha 红色
\usepackage{wrapfig}
\usepackage{subfig}
% \usepackage{subfigure}
% \usepackage{subcaption}

\definecolor{light_gray}{RGB}{211,211,211}
\definecolor{plum}{RGB}{160,43,147}
\definecolor{brown}{RGB}{192,79,21}
\definecolor{light_green}{RGB}{217,242,208}
\definecolor{dark_green}{RGB}{59,125,35}
\definecolor{light_blue}{RGB}{220,234,247}
\definecolor{dark_blue}{RGB}{33,95,154}

\newcommand{\sysname}{{\scshape SeCom}}
\newcommand{\ie}{\textit{i.e.}}
\newcommand{\eg}{\textit{e.g.}}
\newcommand{\wrt}{\textit{w.r.t}}
\newcommand{\etc}{\textit{etc}}
\newcommand{\citehere}{\textcolor{red}{[citation]}}
\newcommand{\red}[1]{\textcolor{red}{#1}}

\definecolor{darkred}{rgb}{0.5, 0.0, 0.0}
\definecolor{darkblue}{rgb}{0.0, 0.0, 0.5}
\definecolor{darkgreen}{rgb}{0.0, 0.5, 0.0}
\definecolor{darkcyan}{rgb}{0.0, 0.5, 0.5}
\definecolor{darkmagenta}{rgb}{0.5, 0.0, 0.5}
\definecolor{darkgray}{rgb}{0.3, 0.3, 0.3}
\definecolor{navy}{rgb}{0.0, 0.0, 0.5}
\definecolor{olive}{rgb}{0.5, 0.5, 0.0}
\definecolor{brown}{rgb}{0.5, 0.25, 0.0}

\title{On Memory Construction and Retrieval for Personalized Conversational Agents}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Zhuoshi Pan\textsuperscript{1}\footnotemark[2], Qianhui Wu\textsuperscript{2}\footnotemark[3], Huiqiang Jiang\textsuperscript{2}, Xufang Luo\textsuperscript{2}, Hao Cheng\textsuperscript{2}, \\ \textbf{Dongsheng Li\textsuperscript{2}, Yuqing Yang\textsuperscript{2}, Chin-Yew Lin\textsuperscript{2}, H. Vicky Zhao\textsuperscript{1}\footnotemark[3], Lili Qiu\textsuperscript{2}, Jianfeng Gao\textsuperscript{2}} \\
\\
\texttt{\textsuperscript{1} Tsinghua University, \textsuperscript{2} Microsoft Corporation} \\
% \texttt{\{qianhuiwu, hjiang, xufang.luo\}@microsoft.com}
}

\renewcommand{\thefootnote}{\fnsymbol{footnote}}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}

\maketitle
\footnotetext[2]{Work during internship at Microsoft.}
\footnotetext[3]{Corresponding author.}
\footnotetext[4]{Project page:~\href{https://aka.ms/secom}{https://aka.ms/secom}}

% \vspace{-9mm}
% \hspace{0.5em} {Project page:~\href{https://aka.ms/secom}{https://aka.ms/secom}}
% \vspace{3mm}

% \begin{center}
%  \vspace{-10mm}
%   % \fontsize{9pt}{\baselineskip}\selectfont
%   {Project page:}~\href{https://www.microsoft.com/en-us/research/project/secom/}{\textbf{https://www.microsoft.com/en-us/research/project/secom}}
%  \vspace{3mm}
% \end{center}

\begin{abstract}
To deliver coherent and personalized experiences in long-term conversations, existing approaches typically perform retrieval augmented response generation by constructing memory banks from conversation history at either the turn-level, session-level, or through summarization techniques.
In this paper, we present two key findings: (1) The granularity of memory unit matters: Turn-level, session-level, and summarization-based methods each exhibit limitations in both memory retrieval accuracy and the semantic quality of the retrieved content. (2) Prompt compression methods, such as \textit{LLMLingua-2}, can effectively serve as a denoising mechanism, enhancing memory retrieval accuracy across different granularities.

Building on these insights, we propose \textbf{\sysname}, a method that constructs the memory bank at segment level by introducing a conversation \textbf{{\scshape Se}}gmentation model that partitions long-term conversations into topically coherent segments, while applying \textbf{{\scshape Com}}pression based denoising on memory units to enhance memory retrieval.
Experimental results show that {\sysname} exhibits a significant performance advantage over baselines on long-term conversation benchmarks \textit{LOCOMO} and \textit{Long-MT-Bench+}. Additionally, the proposed conversation segmentation method demonstrates superior performance on dialogue segmentation datasets such as \textit{DialSeg711}, \textit{TIAGE}, and \textit{SuperDialSeg}.

\end{abstract}

\section{Introduction}
\label{sec:intro}
Large language models (LLMs) have developed rapidly in recent years and have been widely used in conversational agents.
In contrast to traditional dialogue systems, which typically focus on short conversations within specific domains \citep{dinanwizard}, 
LLM-powered conversational agents engage in significantly more interaction turns across a broader range of topics in open-domain conversations~\citep{kim2023aligning, zhou-etal-2023-facilitating}.
Such long-term, open-domain conversations over multiple sessions present significant challenges, as they require the system to retain past events and user preferences to deliver coherent and personalized responses~\citep{chen2024compress}. 

Some methods maintain context by concatenating all historical utterances or their summarized versions~\citep{langchain2023buffer, wang2023recursively}.
However, these strategies can result in excessively long contexts that include irrelevant information, which may not be relevant to the user's current request.
As noted by~\citet{maharana2024evaluating}, LLMs struggle with understanding lengthy conversations and grasping long-range temporal and causal dynamics, particularly when the dialogues contain irrelevant information~\citep{jiang2023longllmlingua}.
Some other works focus on 
retrieving query-related conversation history to 
enhance response generation~\citep{yuan2023evolving, alonso2024toward, kim2024theanine, maharana2024evaluating}.
These approaches typically construct a memory bank from the conversation history at either the \textit{turn-level}~\citep{yuan2023evolving} or \textit{session-level}~\citep{wang2023recursively}. 
% \citet{xu2022beyond}, 
\citet{chen2024compress}, \citet{li2024hello} and \citet{zhong2024memorybank} further leverage \textit{summarization} techniques to build memory units, which are then retrieved as context for response generation.

Building on these works, a key question arises: Which level of memory granularity—turn-level, session-level, or their summarized forms—yields the highest effectiveness? Moreover, is there a novel memory structure that could outperform these three formats?

In this paper, we first systematically investigate the impact of different memory granularities on conversational agents within the paradigm of retrieval augmented response generation~\citep{lewis2020retrieval,ye2024boosting}.
Our findings indicate that turn-level, session-level, and summarization-based methods all exhibit limitations in terms of the accuracy of the retrieval module as well as the semantics of the retrieved content, which ultimately lead to sub-optimal responses, as depicted in Figure~\ref{fig: intro_example}, Figure~\ref{fig: intro_statistics}, and Table~\ref{tab: main_results}.

\input{figures/intro_case_study}
\input{figures/motivation_granularity}

Specifically, users often interact with agents over multiple turns to achieve their goals, causing relevant information to be dispersed across multiple interactions. This dispersion can pose a great challenge to the retrieval of turn-level memory units as some of the history conversation turns may not explicitly contain or relate to keywords mentioned in the current request (\eg, Turn-5 in Figure~\ref{fig: intro_example}). As a result, the retrieved contexts (\eg, Turn-3 and Turn-6 in Figure~\ref{fig: intro_example}) can be fragmentary and fail to encompass the complete request-related information flow, leading to responses that may lack coherence or omit essential information.
On the other hand, a single conversation session may cover multiple topics, especially when users do not initiate a new chat session upon switching topics. Therefore, constructing memory units at the session level risks including irrelevant content (\eg, definition of the prosecutor's fallacy and reasons of World War II in Figure~\ref{fig: intro_example}). Such extraneous content in the session-level memory unit may not only distract the retrieval module but also disrupt the language model's comprehension of the context, causing the agent to produce responses that are off-topic or include unnecessary details.

Long conversations are naturally composed of coherent discourse units. To capture this structure, we introduce a conversation segmentation model that partitions long-term conversations into topically coherent segments, constructing the memory bank at the segment level. During response generation, we directly concatenate the retrieved segment-level memory units as the context as in \citet{yuan2023evolving, kim2024theanine},
bypassing summarization to avoid the information loss that often occurs when converting dialogues into summaries~\citep{maharana2024evaluating}.

Furthermore, inspired by the notion that natural language tends to be inherently redundant~\citep{shannon1951prediction, jiang2023llmlingua, pan2024llmlingua}, we hypothesize that such redundancy can act as noise for retrieval systems, complicating the extraction of key information~\citep{grangier2003information, ma2021simple}.
Therefore, we propose removing such redundancy from memory units prior to retrieval by leveraging prompt compression methods such as LLMLingua-2~\citep{pan2024llmlingua}.
Figure~\ref{fig: similarity_mpnet} shows the results obtained with a BM25 based retriever and an MPNet based retriever~\citep{song2020mpnet} on \textit{Long-MT-Bench+}.
As demonstrated in Figure~\ref{fig: recall_wrt_comp_rate_bm25} and Figure~\ref{fig: recall_wrt_comp_rate_mpnet}, LLMLingua-2 consistently improves retrieval recall given different retrieval budgets $K$ (\ie, the number of retrieved segments) when the compression rate exceeds 50\%.
Figure~\ref{fig: similarity_change} further illustrates that, after denoising, similarity between the query and relevant segments increases, while the similarity with irrelevant segments decreases.

\input{figures/motivation_denoising}

Our contributions can be summarized as follows:
\begin{itemize}
\item We systematically investigate the effects of memory granularity on retrieval augmented response generation in conversational agents. Our findings reveal that turn-level, session-level, and summarization-based approaches each face challenges in ensuring precise retrieval and providing a complete, relevant, and coherent context for generating accurate responses.

\item We contend that the inherent redundancy in natural language can act as noise for retrieval systems. We demonstrate that prompt compression technique, LLMLingua-2, can serve as an effective denoising method to enhance memory retrieval performance.

\item We present \sysname, a system that constructs memory bank at segment level by introducing a conversation {\scshape Se}gmentation model, while applying {\scshape Com}pression based denoising on memory units to enhance memory retrieval. The experimental results show that \sysname\ outperforms baselines on two long-term conversation benchmark LOCOMO and Long-MT-Bench+. Further analysis and ablation studies confirm the contributions of the segment-level memory units and the compression-based denoising technique within our framework. 

\end{itemize}



\section{SeCom}
\subsection{Preliminary}
Let $\mathcal{H} = \{\bm{c}_i\}_{i=1}^C$ represent the available conversation history between a user and an agent, which consists of $C$ sessions.
$\bm{c}_i = \{\bm{t}_j\}_{j=1}^{T_{i}}$ denotes the $i$-th session that is composed of $T_{i}$ sequential user-agent interaction turns, with each turn $\bm{t}_j=(u_j,r_j)$ consisting of a user request $u_j$ and the corresponding response from the agent $r_j$.
Denote the base retrieval system as $f_R$ and the response generation model as $f_{\text{LLM}}$.
The research framework here can be defined as:
(1) \textit{Memory construction}: construct a memory bank $\mathcal{M}$ using conversation history $\mathcal{H}$; For a turn-level memory bank, each memory unit $\bm{m}\in\mathcal{M}$ corresponds to an interaction turn $\bm{t}$, with $|\mathcal{M}|=\sum_{i=1}^C T_i$. For a session-level memory bank, each memory unit $\bm{m}$ corresponds to a session $\bm{c}$, with $|\mathcal{M}|=C$.
(2) \textit{Memory retrieval}: given a target user request $u^*$ and context budget $N$, retrieve $N$ memory units $\{\bm{m}_n\in\mathcal{M}\}_{n=1}^N \leftarrow f_R(u^*, \mathcal{M}, N)$ that are relevant to user request $u^*$;
(3) \textit{Response generation}: take the retrieved $N$ memory units in time order as the context and query the response generation model for response $r^*=f_{\text{LLM}}(u^*, \{\bm{m}_n\}_{n=1}^N)$.

In the remainder of this section, we first elaborate on the proposed conversation segmentation model that splits each session $\bm{c}_i$ into $K_i$ topical segments $\{\bm{s}_{k}\}_{k=1}^{K_i}$ in Section~\ref{sec: method_segment}, with which we construct a segment-level memory bank with each memory unit $\bm{m}$ corresponding to a segment $\bm{s}$ and $|\mathcal{M}|=\sum_{i=1}^{C}K_i$.
In Section~\ref{sec: method_retrieval}, we describe how to denoise memory units to enhance the accuracy of memory retrieval.

\subsection{Conversation Segmentation}
\label{sec: method_segment}

\paragraph{Zero-shot Segmentation} Given a conversation session $\bm{c}$, the conversation segmentation model $f_{\mathcal{I}}$ aims to identify \textit{a set of segment indices} $\mathcal{I}=\{(p_{k}, q_{k})\}_{k=1}^{K}$, where $K$ denotes the total number of segments within the session $\bm{c}$, $p_{k}$ and $q_{k}$ represent the indexes of the first and last interaction turns for the $k$-th segment $\bm{s}_{k}$, with $p_{k} \leq q_{k}$, $p_{k+1} = q_k + 1$.
This can be formulated as:
\begin{equation}
    f_{\mathcal{I}}(\bm{c}) = \{\bm{s}_k\}_{k=1}^K, \\
    \text{where}~\bm{s}_k =\{\bm{t}_{p_k}, \bm{t}_{p_k+1}, ..., \bm{t}_{q_k}\}
\end{equation}
However, building a segmentation model for open-domain conversation is challenging, primarily due to the difficulty of acquiring large amounts of annotated data.
As noted by \citet{jiang2023superdialseg}, the ambiguous nature of segmentation points complicates data collection, making the task difficult even for human annotators.
Consequently, we employ GPT-4 as the conversation segmentation model $f_{\mathcal{I}}$ to leverage its powerful text understanding ability across various domains. To provide clearer context and facilitate reasoning, we enhance session data $\bm{c}$ by adding turn indices and role identifiers to each interaction $\bm{t}_j$ as: ``\text{Turn $j$: \textbackslash n[user]: $u_j$\textbackslash n[agent]: $r_j$}''. We empirically demonstrate that segmentation can also be accomplished  with more lightweight models, such 
as \textit{Mistral-7B} and even \textit{RoBERTa} scale models, making our approach applicable in resource-constrained environments. Figure~\ref{fig: prompt4seg-zero-shot} in Appendix~\ref{sec: segmentation_details} presents the detailed instruction used for zero-shot conversation segmentation here. 

\paragraph{Segmentation with Reflection on Limited Annotated Data}
When a small amount of conversation data with segment annotations is available, we leverage this annotated data to inject segmentation knowledge into LLMs and better align the LLM-based segmentation model with human preferences. Inspired by the prefix-tuning technique~\citep{li2021prefix} and reflection mechanism~\citep{shinn2023reflexion,renze2024self}, we treat the segmentation prompt as the ``prefix'' and iteratively optimize it through LLM self-reflection, ultimately obtaining a segmentation guidance $\bm{G}$.

Specifically, in each iteration, we first apply our segmentation model in a zero-shot manner to a batch of conversation data and select the ``hard examples'', \textit{i.e.,} the top $K$ sessions with the most significant segmentation errors based on the WindowDiff metric~\citep{pevzner2002critique}. The LLM-based segmentation model is then instructed to reflect on its mistakes given the ground-truth segmentation annotations and update the segmentation guidance $\bm{G}$. This process mirrors Stochastic Gradient Descent (SGD) optimization, \textit{i.e.,} $\boldsymbol{G}_{m+1}=\boldsymbol{G}_m-\eta \nabla \mathcal{L}\left(\boldsymbol{G}_m\right)$, where $\nabla \mathcal{L}\left(\boldsymbol{G}_m\right)$ denotes the gradient of segmentation loss, which we assume is estimated implicitly by the LLM itself and is used to adjust the next segmentation guidance $\boldsymbol{G}_{m+1}$.
Figure~\ref{fig: prompt4rubric} shows the self-reflection prompt and Figure~\ref{fig: prompt4seg} illustrates the final prompt with the learned rubric for segmentation.

\subsection{Compression based Memory Denoising}
\label{sec: method_retrieval}
Given a target user request $u^*$ and context budget $N$, the memory retrieval system $f_R$ retrieves $N$ memory units $\{\bm{m}_n\in\mathcal{M}\}_{n=1}^N$ from the memory bank $\mathcal{M}$ as the context in response to the user request $u^*$ .
With the consideration that the inherent redundancy in natural language can act as noise for the retrieval system~\citep{grangier2003information, ma2021simple}, we denoise memory units by removing such redundancy via a prompt compression model $f_{Comp}$ before retrieval:
\begin{equation}
\{\bm{m}_n\in\mathcal{M}\}_{n=1}^N \leftarrow f_R(u^*, f_{Comp}(\mathcal{M}), N).
\end{equation}
Specifically, we use LLMLingua-2~\citep{pan2024llmlingua} as the denoising function $f_{Comp}$ here. 

\section{Experiments}
\label{sec: experiments}

\paragraph{Implementation Details}
We use \texttt{GPT-35-Turbo} for response generation in our main experiment. We also adopt \texttt{Mistral-7B-Instruct-v0.3}\footnote{\url{https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3}}~\citep{jiang2023mistral7b} for robustness evaluation across different LLMs. 
We employ zero-shot segmentation for QA benchmarks and further incorporate the reflection mechanism for segmentation benchmarks to leverage the available annotated data. To make our method applicable in resource-constrained environments, we conduct additional experiments by using  \texttt{Mistral-7B-Instruct-v0.3} and a \texttt{RoBERTa} based model fine-tuned on SuperDialseg~\citep{jiang2023superdialseg}.
Details for the conversation segmentation such as the prompt and hyper-parameters are described in Appendix~\ref{sec: segmentation_details}.
We use \texttt{LLMLingua-2}~\citep{pan2024llmlingua} with a compression rate of 75\% and \texttt{xlm-roberta-large}~\citep{conneau2019unsupervised} as the base model to denoise memory units.
Following \citet{alonso2024toward}, we apply MPNet (\texttt{multi-qa-mpnet-base-dot-v1})~\citep{song2020mpnet} with FAISS~\citep{johnson2019billion} and BM25~\citep{Amati2009} for memory retrieval.

\paragraph{Datasets \& Evaluation Metrics}
We evaluate {\sysname} and other baseline methods for long-term conversations on the following benchmarks:

(i) \textit{LOCOMO}~\citep{maharana2024evaluating}, which is the longest conversation dataset to date, with an average of 300 turns with 9K tokens per sample. 
For the test set, we prompt GPT-4 to generate QA pairs for each session as in \citet{alonso2024toward}.
We also conduct evaluation on the recently released official QA pairs in Appendix~\ref{sec: main_locomo2}.

(ii) \textit{Long-MT-Bench+}, which is reconstructed from \textit{MT-Bench+}~\citep{lu2023memochat}, where human experts are invited to expand the original questions and create long-range questions as test user requests. Since each conversation only contains an average of 13.3 dialogue turns, following \citet{yuan2023evolving}, we merge five consecutive sessions into one long-term conversation. We also use these human-written questions as few-shot examples to prompt GPT-4 to generate a long-range test question for each dialogue topic as the test set. More details such as the statistics of the constructed \textit{Long-MT-Bench+} are listed in Appendix~\ref{sec: dataset_details}.

For evaluation metrics, we use the conventional \textit{BLEU}~\citep{papineni2002bleu}, \textit{ROUGE}~\citep{lin2004rouge}, and \textit{BERTScore}~\citep{zhangbertscore} for basic evaluation. Inspired by~\citep{pan2023rewards}, we employ \textit{GPT4Score} 
for more accurate evaluation, where \texttt{GPT-4-0125} is prompted to assign an integer rating from 0 (poor) to 100 (excellent).
We also perform \textit{pairwise comparisons} by instructing GPT-4 to determine the superior response. The evaluation prompts are detailed in Figure~\ref{fig: prompt4eval} of Appendix~\ref{sec: prompt4eval}. Human evaluation is also conducted, with results summarized in Table~\ref{tab: human_evaluation} in Appendix~\ref{sec: human_evaluation}.

\paragraph{Baselines} We evaluate our method against four intuitive approaches and four state-of-the-art models. As Figure~\ref{fig: similarity_mpnet} indicates, the compression-based memory denoising mechanism can benefit % also benefits the turn-level and session-level 
memory retrieval, in the main results, we directly compare our method to the denoising-enhanced turn-level and session-level baselines.
(1) \textit{Turn-Level}, which constructs the memory bank by treating each user-agent interaction as a distinct memory unit.
(2) \textit{Session-Level}, which uses each entire conversation session as a memory unit.
(3) \textit{Zero History}, which generates responses without incorporating any conversation history, operating in a zero-shot manner.
(4) \textit{Full History}, which concatenates all prior conversation history as the context for response generation.
(5) \textit{SumMem}~\citep{langchain2023summary}, which dynamically generates summaries of past dialogues relevant to the target user request, and uses these summaries as context for response generation. % No retriever here.
(6) \textit{RecurSum}~\citep{wang2023recursively}, which recursively updates summary using current session and previous summaries, and takes the updated summary of current session as the context. % No retriever here.
(7) \textit{ConditionMem}~\citep{yuan2023evolving}, which generates summaries and knowledge for each dialogue turn, then retrieves the most relevant summary, knowledge, and raw conversation turn as the context in response to a new user request. 
(8) \textit{MemoChat}~\citep{lu2023memochat}, which operates memories at segment level, but focuses on tuning LLMs for both memory construction and retrieval.

\input{tables/main_results}
\input{figures/gpt4compare}

\paragraph{Main Results}
As shown in Table~\ref{tab: main_results} and Figure~\ref{fig: main_compare}, \textit{{\sysname} outperforms all baseline approaches}, exhibiting a significant performance advantage, particularly on the long-conversation benchmark LOCOMO.
Interestingly, there is a significant performance disparity in Turn-Level and Session-Level methods when using different retrieval models. For instance, switching from the MPNet-based retriever to the BM25-based retriever results in performance improvements up to 11.98 and 7.89 points in terms of GPT4Score on LOCOMO and Long-MT-Bench+, respectively. 
In contrast, \textit{{\sysname} demonstrates greater robustness in terms of the deployed retrieval system}. We attribute this to the following reason:
As discussed in Section \ref{sec:intro}, turn-level memory units are often fragmented and may not explicitly include or relate to keywords mentioned in the target user request. On the other hand, session-level memory units contain a large amount of irrelevant information. Both of these scenarios
make the retrieval performance sensitive to the capability of the deployed retrieval system.
However, topical segments in {\sysname} can strike a balance between including more relevant, coherent information while excluding irrelevant content, thus leading to more robust and superior retrieval performance.
Table~\ref{tab: main_results} and Figure~\ref{fig: main_compare} also reveal that \textit{summary based methods, such as SumMem and RecurSum fall behind turn-level or session-level baselines}. Our case study, Figure~\ref{fig: case_study_segment_vs_rsum} and~\ref{fig: case_study_segment_vs_condmem} in Appendix~\ref{sec: case_study}, suggests that this is likely due to the loss of crucial details during the process of converting dialogues into summaries~\citep{maharana2024evaluating}, which are essential for accurate question answering. Furthermore, Table~\ref{tab: main_results} shows that \sysname\ maintains the advantage over baseline methods when switching the segmentation model from GPT-4 to Mistral-7B. Notably, even with a RoBERTa-based segmentation model, \sysname\ retains competitive performance compared to other granularity-based baselines. 
% We also conduct additional experiments on a subset of \textit{Persona-Chat} and \textit{CoQA}. The results in Table~\ref{tab: results_on_coqa} and~\ref{tab: results_on_spc} further validate SeCom's robustness in more conversation scenarios.

\paragraph{Ablation Study on Granularity of Memory Units}
Figure~\ref{fig: bm25_granularity}, Figure~\ref{fig: mpnet_granularity}, and Table~\ref{tab: main_mistral} have clearly demonstrated the superiority of segment-level memory over turn-level and session-level memory in terms of both retrieval accuracy and end-to-end QA performance.
Figure~\ref{fig: gpt4score_bm25} and Figure~\ref{fig: gpt4score_mpnet} further compare QA performance across different memory granularities under varying context budgets. Compression-based memory unit denoising was applied in all experiments here to isolate the end-to-end impact of memory granularity on performance. The results show that segment-level memory consistently outperforms both turn-level and session-level memory across a range of context budgets, reaffirming its superiority. Figures~\ref{fig: case_study_segment_vs_turn} and \ref{fig: case_study_segment_vs_session} in Appendix~\ref{sec: case_study} provide detailed case studies.

\input{figures/score_wrt_token}

\begin{table*}[!h]
\centering
\small
% \setlength{\tabcolsep}{1.5mm}
% \caption{Ablation study on compression based memory denoising. Compression rate: 75\%. Retriever: MPNet.}
\caption{Ablation study on compression-based memory denoising with a compression rate of 75\% using the MPNet based retriever.}
\label{tab: ablation_compression}
    % \newcommand{\redcross}{\textcolor{red}{$\times$}}
    % \begin{adjustbox}{width=\linewidth, height=6cm, keepaspectratio}
    \resizebox{1.\columnwidth}{!}{

    \begin{tabular}{l|cccc|cccc}
    \toprule
    \multirow{2}{*}{\textbf{Methods}} &  \multicolumn{4}{@{}c|}{{\bf LOCOMO}} &  \multicolumn{4}{@{}c}{{\bf Long-MT-Bench+}}\\
    \cmidrule (lr){2-5} \cmidrule (lr){6-9}
    & GPT4Score & BLEU & Rouge2 &  BERTScore & GPT4Score & BLEU & Rouge2 &  BERTScore\\
    \midrule
    {\sysname} & \textbf{69.33} & \textbf{7.19} & \textbf{13.74} & \textbf{88.60} & \textbf{88.81} & \textbf{13.80} & \textbf{19.21} & \textbf{87.72}\\
    {~~$-$ Denoise} & 59.87 & 6.49 & 12.11 & 88.16 & 87.51 & 12.94 & 18.73 & 87.44 \\ 
    \bottomrule
    \end{tabular}
    }
\end{table*}
\paragraph{Ablation Study on Compression based Memory Denoising}
As shown in Table~\ref{tab: ablation_compression}, removing the proposed compression based memory denoising mechanism will result in a performance drop up to 9.46 points of GPT4Score on LOCOMO, highlighting the critical role of this denoising mechanism: by effectively improving the retrieval system (Figure~\ref{fig: recall_wrt_comp_rate_mpnet}), it significantly enhances the overall effectiveness of the system.

\paragraph{Mistral-7B Powered Response Generation}
Table~\ref{tab: main_mistral} presents the results of {\sysname} and baselines using \texttt{Mistral-7B-Instruct-v0.3}\footnote{\url{https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3}}~\citep{jiang2023mistral7b} as the response generator.
Our method demonstrates a significant performance gain over other baselines, showcasing its good generalization ability across different LLM-powered conversation agents.
Interestingly, although the Mistral-7B here features a 32K context window capable of accommodating the entire conversation history, in other words, it is able to include and comprehend the entire conversation history without truncation, the performance of the ``Full History'' approach still falls short compared to \sysname. This highlights the effectiveness of our memory construction and retrieval mechanisms, which prioritize relevant context and reduce noise, leading to more accurate and contextually appropriate responses.

\input{tables/main_mtbp_mistral}

\paragraph{Evaluation of Conversation Segmentation Model}
To evaluate the conversation segmentation module described in Section \ref{sec: method_segment} independently, we use three widely used dialogue segmentation datasets: DialSeg711~\citep{xu2021topic}, TIAGE~\citep{xie2021tiage}, and SuperDialSeg~\citep{jiang2023superdialseg}. In addition to the unsupervised (zero-shot) setting, we also assess performance in a transfer learning setting, where baseline models are trained on the full training set of the source dataset, while our model learns the segmentation rubric through LLM reflection on the top 100 most challenging examples.
We evaluate transfer learning only using SuperDialSeg and TIAGE as the source datasets since DialSeg711 lacks a training set.
For evaluation metrics, following \citet{jiang2023superdialseg}, we use the F1 score, $P_k$~\citep{beeferman1999statistical}, Window Diff (WD)~\citep{pevzner2002critique} and the segment score\footnote{Recommended by ICASSP2023 General Meeting Understanding and Generation Challenge \url{https://2023.ieeeicassp.org/signal-processing-grand-challenges}.}:
\begin{equation}
\textit{Score}=\frac{2 * F 1+\left(1-\textit{P}_k\right)+(1-\textit{WD})}{4}.
\label{eq:segment_score}
\end{equation}
Table \ref{tab: segment_main} presents the results, showing that our segmentation model consistently outperforms baselines in the unsupervised setting.
In the transfer learning setting, despite the segmentation rubric being learned from LLM reflection on only 100 examples from the source dataset, it generalizes well to the target dataset, surpassing the baseline model trained on the full source training set and even outperforming some supervised baselines. 
% \textcolor{red}{We also experiment with more light-weight segmentation models, \textit{Mistral-7B-Instruct-v0.3} and a model fine-tuned from \textit{RoBERTa}. The experiment results in Table~\ref{tab: main_results_slm_seg} demonstrate that smaller segmentation models can perform segmentation well, making our approach applicable to low-resource environments.}

\input{tables/segment_all}


\section{Related Works}
\subsection{Memory Management in Conversation}
Long-term open-domain conversation~\citep{feng2020doc2dial, xu2022beyond, maharana2024evaluating} poses significant challenges for LLM-powered conversational agents. To address this, memory management~\citep{lu2023memochat, wang2023recursively, zhong2024memorybank, wu2024tokenselect, li2024hello, zhang2024survey} is widely adopted. The core of memory management involves leveraging dialogue history to provide background information, extract persona, understand the user's intent, and generate history-aware responses.
For instance, MPC~\citep{lee2023prompted}, MemoryBank~\citep{zhong2024memorybank} and COMEDY~\citep{chen2024compress} further summarize past events in the conversation history as memory records. Methods such as RecurSum~\citep{wang2023recursively} and ConditionMem~\citep{yuan2023evolving} consider the memory updating process through recursive summarization.

Inspired by the success of retrieval-augmented generation (RAG), many recent works introduce retrieval modules into memory management. For example, MSC~\citep{xu2022beyond} utilizes a pre-trained Dense Passage Retriever (DPR)~\citep{karpukhin2020dense} model to select the top \textit{N} relevant summaries. 
Instead of using a retrieval model, MemoChat~\citep{lu2023memochat} employs an LLM to retrieve relevant memory records.
Recently, \citet{maharana2024evaluating} release a dataset, \textit{LOCOMO}, which is specifically designed to assess long-term conversational memory, highlighting the effectiveness of RAG in maintaining long-term memory. Their experiment results indicate that long-context LLMs are prone to generating hallucinations, and summary-only memory  results in sub-optimal performance due to information loss.

\subsection{Chunking Granularity in RAG System}

Chunking granularity~\citep{duarte2024lumberchunker} (i.e., how the entire context is segmented into retrieval units) is a crucial aspect of RAG systems. Ineffective segmentation can result in incomplete or noisy retrieval units, which can impair the retrieval module~\citep{yu2023chain} and negatively impact the subsequent response generation~\citep{shi2023large}.

Semantic-based chunking strategies~\citep{anurag2023chunkingstrategies, antematter2024optimizing, kamradt2024semantic} use representation similarity to identify topic shifts and decide chunk boundaries. 
With the advancement of LLMs, some studies leverage their capabilities to segment context into retrieval units. For instance, 
LumberChunker~\citep{duarte2024lumberchunker} segments narrative documents into semantically coherent chunks using Gemini~\citep{team2023gemini}. However, existing research mainly focuses on document chunking, overlooking conversation chunking. Common chunking practices~\citep{langchain2023conversational, llamaindex2023buffer} in conversations directly rely on the natural structure (\textit{i.e.,} utterances or dialogue turns) of dialogue to divide conversation into retrieval units.

\subsection{Denoising in RAG system}
Recent studies have observed that noise in conversations can negatively impact the retrieval module in RAG systems. For example, COTED~\citep{mao2022curriculum} found that redundant noise in dialogue rounds significantly impairs conversational search. 
Earlier research~\citep{strzalkowski1998summarization, wasson2002using} investigates the use of summaries in retrieval systems. With the advent of LLM, recent approaches~\citep{ravfogel2023retrieving, lee2024effective} denoise raw dialogues by prompting LLMs to summarize. Subsequently, they fine-tune the retriever's embedding model to align vector representations of original text with those of generated summaries. However, these methods have several drawbacks: (1) summarization introduces latency and computational costs, whereas dialogue state methods require high-quality annotated data. (2) Fine-tuning the retriever's embedding model limits flexibility and scalability, restricting it from being used as a plug-and-play method. (3) Fine-tuning risks overfitting and catastrophic forgetting \citep{mccloskey1989catastrophic, lee2022sequential}, potentially impeding domain adaptation and generalization ability of pre-trained retrievers.

\section{Conclusion}

In this paper, we systematically investigate the impact of memory granularity on retrieval-augmented response generation for long-term conversational agents. Our findings reveal the limitations of turn-level and session-level memory granularities, as well as summarization-based methods. To overcome these challenges, we introduce \sysname, a novel memory management system that constructs a memory bank at the segment-level and employs compression-based denoising techniques to enhance retrieval performance. The experimental results underscore the effectiveness of \sysname\ in handling long-term conversations. Further analysis and ablation studies confirm the contributions of the segment-level memory units and the compression-based denoising technique within our framework. 

\input{iclr2025_conference.bbl}
% \bibliography{iclr2025_conference}
% \bibliographystyle{iclr2025_conference}

\appendix

\input{appendix}

\end{document}