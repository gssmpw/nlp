\begin{table*}[!t]
    \small
    \centering
    \setlength{\tabcolsep}{1mm}
    \caption{Performance comparison on the official question-answer pairs of \textit{LOCOMO} using MPNet retriever. All other settings remain the same as in Table~\ref{tab: main_results}. MemoChat~\citep{lu2023memochat} is not applicable in \textit{Mistral-7B-Instruct-v0.3} due to Mistral's inability to execute the ``Memo Writing'' step, as it often fails to generate a valid JSON response needed to construct the memory bank in \citet{lu2023memochat}.
    }
    \label{tab: main_locomo2}
    \resizebox{1\columnwidth}{!}{
    \begin{tabular}{l|cccccc|cc}
    \toprule
    
    \multirow{2}{*}{\textbf{Methods}} &  \multicolumn{6}{@{}c|}{{\bf QA Performance}} & \multicolumn{2}{@{}c}{{\ \bf Context Length}} \\
    \cmidrule (lr){2-7} \cmidrule (lr){8-9}
    & GPT4Score & BLEU & Rouge1 & Rouge2 & RougeL & BERTScore & \# Turns & \# Tokens \\
    \midrule
    \multicolumn{9}{@{}c}{ \textit{ GPT-35-Turbo } } \\
    \midrule
    Full History & 66.28 & 7.51 & 28.73 & 14.07 & 27.90 & 87.82 & 293 & 18,655 \\
    MemoChat & 75.77 & 11.28 & 32.91 & 18.82 & 29.78 & 87.98 & - & 1,159 \\
    \midrule
    Turn-Level & 81.52 & 11.91 & 36.00 & 19.59 & 34.99 & \textbf{88.64} & 55.00 & 3,026 \\
    Session-Level & 74.20 & 10.95 & 29.92 & 14.64 & 29.27 & 87.96 & 54.48 & 3,442 \\
    \midrule
    \sysname\ & \textbf{84.21} & \textbf{12.80} & \textbf{36.70} & \textbf{19.90} & \textbf{35.61} & 88.59 & 56.49 & 3,565 \\
    \midrule
    \multicolumn{9}{@{}c}{ \textit{ Mistral-7B-v0.3 } } \\
    \midrule
    Full History & 69.13 & 6.77 & 30.40 & 15.02 & 29.20& 87.29 & 293 & 18,655 \\
    \midrule
    Turn-Level & 78.82 & 10.09 & 32.75 & 16.25 & 31.75 & \textbf{87.97} & 55.00 & 3,026 \\
    Session-Level & 62.68 & 7.37 & 26.68 & 12.38 & 25.86 & 86.98 & 54.48 & 3,442 \\
    \midrule
    \sysname\ & \textbf{80.07} & \textbf{10.67} & \textbf{32.82} & \textbf{16.65} & \textbf{31.81} & 87.87 & 56.49 & 3,565 \\
    \bottomrule
    \end{tabular}
    }
\end{table*}