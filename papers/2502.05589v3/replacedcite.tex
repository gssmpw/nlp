\section{Related Works}
\subsection{Memory Management in Conversation}
Long-term open-domain conversation____ poses significant challenges for LLM-powered conversational agents. To address this, memory management____ is widely adopted. The core of memory management involves leveraging dialogue history to provide background information, extract persona, understand the user's intent, and generate history-aware responses.
For instance, MPC____, MemoryBank____ and COMEDY____ further summarize past events in the conversation history as memory records. Methods such as RecurSum____ and ConditionMem____ consider the memory updating process through recursive summarization.

Inspired by the success of retrieval-augmented generation (RAG), many recent works introduce retrieval modules into memory management. For example, MSC____ utilizes a pre-trained Dense Passage Retriever (DPR)____ model to select the top \textit{N} relevant summaries. 
Instead of using a retrieval model, MemoChat____ employs an LLM to retrieve relevant memory records.
Recently, ____ release a dataset, \textit{LOCOMO}, which is specifically designed to assess long-term conversational memory, highlighting the effectiveness of RAG in maintaining long-term memory. Their experiment results indicate that long-context LLMs are prone to generating hallucinations, and summary-only memory  results in sub-optimal performance due to information loss.

\subsection{Chunking Granularity in RAG System}

Chunking granularity____ (i.e., how the entire context is segmented into retrieval units) is a crucial aspect of RAG systems. Ineffective segmentation can result in incomplete or noisy retrieval units, which can impair the retrieval module____ and negatively impact the subsequent response generation____.

Semantic-based chunking strategies____ use representation similarity to identify topic shifts and decide chunk boundaries. 
With the advancement of LLMs, some studies leverage their capabilities to segment context into retrieval units. For instance, 
LumberChunker____ segments narrative documents into semantically coherent chunks using Gemini____. However, existing research mainly focuses on document chunking, overlooking conversation chunking. Common chunking practices____ in conversations directly rely on the natural structure (\textit{i.e.,} utterances or dialogue turns) of dialogue to divide conversation into retrieval units.

\subsection{Denoising in RAG system}
Recent studies have observed that noise in conversations can negatively impact the retrieval module in RAG systems. For example, COTED____ found that redundant noise in dialogue rounds significantly impairs conversational search. 
Earlier research____ investigates the use of summaries in retrieval systems. With the advent of LLM, recent approaches____ denoise raw dialogues by prompting LLMs to summarize. Subsequently, they fine-tune the retriever's embedding model to align vector representations of original text with those of generated summaries. However, these methods have several drawbacks: (1) summarization introduces latency and computational costs, whereas dialogue state methods require high-quality annotated data. (2) Fine-tuning the retriever's embedding model limits flexibility and scalability, restricting it from being used as a plug-and-play method. (3) Fine-tuning risks overfitting and catastrophic forgetting ____, potentially impeding domain adaptation and generalization ability of pre-trained retrievers.