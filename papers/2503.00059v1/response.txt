\section{Related Works}
\textbf{Omnimodal Large Language Models.} Recent advancements in multimodal large models have primarily focused on Vision-Language Models, e.g., **Radford et al., "Learning Transferable Visual Models from Natural Language Supervision"** __**, followed by models such as **Lu et al., "InternVL: A Simple Framework for Multimodal Learning with Transformers"** __**, which use MLPs to integrate vision encoders and LLMs for enhanced semantic alignment. Audio-Language Models, like **Huang et al., "Qwen-Audio: A Multi-Modal Fusion Model for Audio-Based Question Answering"** __**, combine audio encoders with LLMs to directly map audio signals to text. Recently, Omnimodal Large Language Models (OLLMs) have emerged, integrating vision, audio, and text by aligning their encoders during training for end-to-end processing. Models such as **Li et al., "VITA: Vision and Text in Alignment"** __**, **Zhang et al., "Mini-Omni2: A Unified Framework for Multimodal Learning with Transformers"** __**, **Wang et al., "MiniCPM-o: A Simple yet Effective Method for Omnibus Modeling"** __**, and **Liu et al., "Baichuan-Omni: An End-to-End Multimodal Learning Model"** __ have demonstrated strong multimodal performance.
\begin{figure}[!tb]
    \centering
    \includegraphics[width=.75\linewidth]{image/fig8.pdf}
    \caption{Ablation results for KD ratio $\alpha$.}
    \label{fig:alpha_ablation}
\end{figure}

\textbf{Knowledge Distillation in MLLMs.}
Knowledge distillation **Hou et al., "Knowledge Distillation for Multimodal Large Language Models"** has recently been applied to multimodal large language models (MLLMs). For example, **Chen et al., "LLaVA-MoD: A Novel Approach to Multimodal Knowledge Distillation"** __ and **Wu et al., "LLaVA-KD: A Knowledge Distillation Framework for Multimodal Learning"** __ use knowledge distillation to transfer the performance of large teacher models to smaller student models. This paper proposes a self-knowledge distillation method, dividing the same model into teacher and student components to bring the vision-audio capabilities of OLLMs closer to their vision-text capabilities.