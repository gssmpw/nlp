[
  {
    "index": 0,
    "papers": [
      {
        "key": "radford2021clip",
        "author": "Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others",
        "title": "Learning transferable visual models from natural language supervision"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "chen2024internvl25",
        "author": "Chen, Zhe and Wang, Weiyun and Cao, Yue and Liu, Yangzhou and Gao, Zhangwei and Cui, Erfei and Zhu, Jinguo and Ye, Shenglong and Tian, Hao and Liu, Zhaoyang and others",
        "title": "Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "chu2024qwen2audio",
        "author": "Chu, Yunfei and Xu, Jin and Yang, Qian and Wei, Haojie and Wei, Xipin and Guo, Zhifang and Leng, Yichong and Lv, Yuanjun and He, Jinzheng and Lin, Junyang and others",
        "title": "Qwen2-audio technical report"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "fu2024vita",
        "author": "Fu, Chaoyou and Lin, Haojia and Long, Zuwei and Shen, Yunhang and Zhao, Meng and Zhang, Yifan and Dong, Shaoqi and Wang, Xiong and Yin, Di and Ma, Long and others",
        "title": "Vita: Towards open-source interactive omni multimodal llm"
      },
      {
        "key": "fu2025vita1.5",
        "author": "Fu, Chaoyou and Lin, Haojia and Wang, Xiong and Zhang, Yi-Fan and Shen, Yunhang and Liu, Xiaoyu and Li, Yangze and Long, Zuwei and Gao, Heting and Li, Ke and others",
        "title": "VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "xie2024miniomni2",
        "author": "Xie, Zhifei and Wu, Changqiao",
        "title": "Mini-omni2: Towards open-source gpt-4o with vision, speech and duplex capabilities"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "MiniCPMo",
        "author": "MiniCPM-o Team, OpenBMB",
        "title": "MiniCPM-o 2.6: A GPT-4o Level MLLM for Vision, Speech, and Multimodal Live Streaming on Your Phone"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "li2024baichuanomni",
        "author": "Li, Yadong and Sun, Haoze and Lin, Mingan and Li, Tianpeng and Dong, Guosheng and Zhang, Tao and Ding, Bowen and Song, Wei and Cheng, Zhenglin and Huo, Yuqi and others",
        "title": "Baichuan-omni technical report"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "hinton2015distilling",
        "author": "Hinton, Geoffrey",
        "title": "Distilling the Knowledge in a Neural Network"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "2024arXiv240815881S",
        "author": "{Shu}, Fangxun and {Liao}, Yue and {Zhuo}, Le and {Xu}, Chenning and {Zhang}, Lei and {Zhang}, Guanghao and {Shi}, Haonan and {Chen}, Long and {Zhong}, Tao and {He}, Wanggui and {Fu}, Siming and {Li}, Haoyuan and {Li}, Bolin and {Yu}, Zhelun and {Liu}, Si and {Li}, Hongsheng and {Jiang}, Hao",
        "title": "{LLaVA-MoD: Making LLaVA Tiny via MoE Knowledge Distillation}"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "2024arXiv241016236C",
        "author": "{Cai}, Yuxuan and {Zhang}, Jiangning and {He}, Haoyang and {He}, Xinwei and {Tong}, Ao and {Gan}, Zhenye and {Wang}, Chengjie and {Bai}, Xiang",
        "title": "{LLaVA-KD: A Framework of Distilling Multimodal Large Language Models}"
      }
    ]
  }
]