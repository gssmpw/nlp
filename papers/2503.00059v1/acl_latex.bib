@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{yang2024qwen2,
  title={Qwen2. 5 Technical Report},
  author={Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and others},
  journal={arXiv preprint arXiv:2412.15115},
  year={2024}
}

@article{wang2024qwen2vl,
  title={Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution},
  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and others},
  journal={arXiv preprint arXiv:2409.12191},
  year={2024}
}

@article{chen2024internvl25,
  title={Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling},
  author={Chen, Zhe and Wang, Weiyun and Cao, Yue and Liu, Yangzhou and Gao, Zhangwei and Cui, Erfei and Zhu, Jinguo and Ye, Shenglong and Tian, Hao and Liu, Zhaoyang and others},
  journal={arXiv preprint arXiv:2412.05271},
  year={2024}
}

@inproceedings{liu2024llava,
  title={Improved baselines with visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={26296--26306},
  year={2024}
}

@article{fang2024llamaomni,
  title={Llama-omni: Seamless speech interaction with large language models},
  author={Fang, Qingkai and Guo, Shoutao and Zhou, Yan and Ma, Zhengrui and Zhang, Shaolei and Feng, Yang},
  journal={arXiv preprint arXiv:2409.06666},
  year={2024}
}

@article{chu2024qwen2audio,
  title={Qwen2-audio technical report},
  author={Chu, Yunfei and Xu, Jin and Yang, Qian and Wei, Haojie and Wei, Xipin and Guo, Zhifang and Leng, Yichong and Lv, Yuanjun and He, Jinzheng and Lin, Junyang and others},
  journal={arXiv preprint arXiv:2407.10759},
  year={2024}
}

@article{hurst2024gpt4o,
  title={Gpt-4o system card},
  author={OpenAI},
  journal={arXiv preprint arXiv:2410.21276},
  year={2024}
}


@article{fu2024vita,
  title={Vita: Towards open-source interactive omni multimodal llm},
  author={Fu, Chaoyou and Lin, Haojia and Long, Zuwei and Shen, Yunhang and Zhao, Meng and Zhang, Yifan and Dong, Shaoqi and Wang, Xiong and Yin, Di and Ma, Long and others},
  journal={arXiv preprint arXiv:2408.05211},
  year={2024}
}

@article{fu2025vita1.5,
  title={VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction},
  author={Fu, Chaoyou and Lin, Haojia and Wang, Xiong and Zhang, Yi-Fan and Shen, Yunhang and Liu, Xiaoyu and Li, Yangze and Long, Zuwei and Gao, Heting and Li, Ke and others},
  journal={arXiv preprint arXiv:2501.01957},
  year={2025}
}

@article{xie2024miniomni2,
  title={Mini-omni2: Towards open-source gpt-4o with vision, speech and duplex capabilities},
  author={Xie, Zhifei and Wu, Changqiao},
  journal={arXiv preprint arXiv:2410.11190},
  year={2024}
}

@article{li2024baichuanomni,
  title={Baichuan-omni technical report},
  author={Li, Yadong and Sun, Haoze and Lin, Mingan and Li, Tianpeng and Dong, Guosheng and Zhang, Tao and Ding, Bowen and Song, Wei and Cheng, Zhenglin and Huo, Yuqi and others},
  journal={arXiv preprint arXiv:2410.08565},
  volume={3},
  number={7},
  year={2024}
}
@misc{megrez,
  author = {InfinigenceAI},
  title = {Megrez-3B-Omni: The First Open-Source End-Side Full Modality Understanding Model},
  howpublished = {\url{https://huggingface.co/Infinigence/Megrez-3B-Omni}},
  year={2024}
}



# dataset
@article{mme,
  author       = {Chaoyou Fu and
                  Peixian Chen and
                  Yunhang Shen and
                  Yulei Qin and
                  Mengdan Zhang and
                  Xu Lin and
                  Zhenyu Qiu and
                  Wei Lin and
                  Jinrui Yang and
                  Xiawu Zheng and
                  Ke Li and
                  Xing Sun and
                  Rongrong Ji},
  title        = {{MME:} {A} Comprehensive Evaluation Benchmark for Multimodal Large
                  Language Models},
  journal      = {CoRR},
  volume       = {abs/2306.13394},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2306.13394},
  doi          = {10.48550/ARXIV.2306.13394},
  eprinttype    = {arXiv},
  eprint       = {2306.13394},
  timestamp    = {Mon, 18 Nov 2024 08:02:15 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2306-13394.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{guan2024hallusionbench,
  title={HallusionBench: an advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models},
  author={Guan, Tianrui and Liu, Fuxiao and Wu, Xiyang and Xian, Ruiqi and Li, Zongxia and Liu, Xiaoyu and Wang, Xijun and Chen, Lichang and Huang, Furong and Yacoob, Yaser and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14375--14385},
  year={2024}
}
@misc{realworldqa,
  author = {xai},
  title = {Grok-1.5 vision preview},
  howpublished = {\url{https://x.ai/blog/grok-1.5v}},
  year={2024}
}
@inproceedings{singh2019textvqa,
  title={Towards vqa models that can read},
  author={Singh, Amanpreet and Natarajan, Vivek and Shah, Meet and Jiang, Yu and Chen, Xinlei and Batra, Dhruv and Parikh, Devi and Rohrbach, Marcus},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={8317--8326},
  year={2019}
}
@article{masry2022chartqa,
  title={Chartqa: A benchmark for question answering about charts with visual and logical reasoning},
  author={Masry, Ahmed and Long, Do Xuan and Tan, Jia Qing and Joty, Shafiq and Hoque, Enamul},
  journal={arXiv preprint arXiv:2203.10244},
  year={2022}
}
@inproceedings{mathew2021docvqa,
  title={Docvqa: A dataset for vqa on document images},
  author={Mathew, Minesh and Karatzas, Dimosthenis and Jawahar, CV},
  booktitle={Proceedings of the IEEE/CVF winter conference on applications of computer vision},
  pages={2200--2209},
  year={2021}
}
@inproceedings{mathew2022infographicvqa,
  title={Infographicvqa},
  author={Mathew, Minesh and Bagal, Viraj and Tito, Rub{\`e}n and Karatzas, Dimosthenis and Valveny, Ernest and Jawahar, CV},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={1697--1706},
  year={2022}
}
@inproceedings{duan2024vlmevalkit,
  title={Vlmevalkit: An open-source toolkit for evaluating large multi-modality models},
  author={Duan, Haodong and Yang, Junming and Qiao, Yuxuan and Fang, Xinyu and Chen, Lin and Liu, Yuan and Dong, Xiaoyi and Zang, Yuhang and Zhang, Pan and Wang, Jiaqi and others},
  booktitle={Proceedings of the 32nd ACM international conference on multimedia},
  pages={11198--11201},
  year={2024}
}



# 分析
@article{bi2024unveiling,
  title={Unveiling Visual Perception in Language Models: An Attention Head Analysis Approach},
  author={Bi, Jing and Guo, Junjia and Tang, Yunlong and Wen, Lianggong Bruce and Liu, Zhang and Xu, Chenliang},
  journal={arXiv preprint arXiv:2412.18108},
  year={2024}
}
@article{zhang2025llava,
  title={LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token},
  author={Zhang, Shaolei and Fang, Qingkai and Yang, Zhe and Feng, Yang},
  journal={arXiv preprint arXiv:2501.03895},
  year={2025}
}

@article{Yuksekgonul0KJ023bow,
  author       = {Mert Y{\"{u}}ksekg{\"{o}}n{\"{u}}l and
                  Federico Bianchi and
                  Pratyusha Kalluri and
                  Dan Jurafsky and
                  James Zou},
  title        = {When and Why Vision-Language Models Behave like Bags-Of-Words, and
                  What to Do About It?},
  booktitle    = {The Eleventh International Conference on Learning Representations,
                  {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023},
  year         = {2023},
}

# 实验
@article{chen2024InternVL,
  title={Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling},
  author={Chen, Zhe and Wang, Weiyun and Cao, Yue and Liu, Yangzhou and Gao, Zhangwei and Cui, Erfei and Zhu, Jinguo and Ye, Shenglong and Tian, Hao and Liu, Zhaoyang and others},
  journal={arXiv preprint arXiv:2412.05271},
  year={2024}
}
@article{Qwen2-VL,
  title={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},
  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},
  journal={arXiv preprint arXiv:2409.12191},
  year={2024}
}
@inproceedings{radford2023whisper,
  title={Robust speech recognition via large-scale weak supervision},
  author={Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
  booktitle={International conference on machine learning},
  pages={28492--28518},
  year={2023},
  organization={PMLR}
}
@inproceedings{panayotov2015librispeech,
  title={Librispeech: an asr corpus based on public domain audio books},
  author={Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},
  booktitle={2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
  pages={5206--5210},
  year={2015},
  organization={IEEE}
}
@article{ardila2019commonvoice,
  title={Common voice: A massively-multilingual speech corpus},
  author={Ardila, Rosana and Branson, Megan and Davis, Kelly and Henretty, Michael and Kohler, Michael and Meyer, Josh and Morais, Reuben and Saunders, Lindsay and Tyers, Francis M and Weber, Gregor},
  journal={arXiv preprint arXiv:1912.06670},
  year={2019}
}
@article{chen2021gigaspeech,
  title={Gigaspeech: An evolving, multi-domain asr corpus with 10,000 hours of transcribed audio},
  author={Chen, Guoguo and Chai, Shuzhou and Wang, Guanbo and Du, Jiayu and Zhang, Wei-Qiang and Weng, Chao and Su, Dan and Povey, Daniel and Trmal, Jan and Zhang, Junbo and others},
  journal={arXiv preprint arXiv:2106.06909},
  year={2021}
}
@inproceedings{kang2024libriheavy,
  title={Libriheavy: a 50,000 hours asr corpus with punctuation casing and context},
  author={Kang, Wei and Yang, Xiaoyu and Yao, Zengwei and Kuang, Fangjun and Yang, Yifan and Guo, Liyong and Lin, Long and Povey, Daniel},
  booktitle={ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={10991--10995},
  year={2024},
  organization={IEEE}
}

# 相关工作
@article{hinton2015distilling,
  title={Distilling the Knowledge in a Neural Network},
  author={Hinton, Geoffrey},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}

@misc{MiniCPMo,
  author = {MiniCPM-o Team, OpenBMB},
  title = {MiniCPM-o 2.6: A GPT-4o Level MLLM for Vision, Speech, and Multimodal Live Streaming on Your Phone},
  howpublished = {\url{https://huggingface.co/openbmb/MiniCPM-o-2_6}},
  year={2025}
}

@inproceedings{radford2021clip,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@ARTICLE{2024arXiv240815881S,
       author = {{Shu}, Fangxun and {Liao}, Yue and {Zhuo}, Le and {Xu}, Chenning and {Zhang}, Lei and {Zhang}, Guanghao and {Shi}, Haonan and {Chen}, Long and {Zhong}, Tao and {He}, Wanggui and {Fu}, Siming and {Li}, Haoyuan and {Li}, Bolin and {Yu}, Zhelun and {Liu}, Si and {Li}, Hongsheng and {Jiang}, Hao},
        title = "{LLaVA-MoD: Making LLaVA Tiny via MoE Knowledge Distillation}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computer Vision and Pattern Recognition},
         year = 2024,
        month = aug,
          eid = {arXiv:2408.15881},
        pages = {arXiv:2408.15881},
          doi = {10.48550/arXiv.2408.15881},
archivePrefix = {arXiv},
       eprint = {2408.15881},
 primaryClass = {cs.CV},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2024arXiv240815881S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{Jacobs_Jordan_Nowlan_Hinton_1991,   title={Adaptive Mixtures of Local Experts},  url={http://dx.doi.org/10.1162/neco.1991.3.1.79},  DOI={10.1162/neco.1991.3.1.79},  journal={Neural Computation},  author={Jacobs, Robert A. and Jordan, Michael I. and Nowlan, Steven J. and Hinton, Geoffrey E.},  year={1991},  month={Feb},  pages={79–87},  language={en-US}  }

@ARTICLE{2024arXiv241016236C,
       author = {{Cai}, Yuxuan and {Zhang}, Jiangning and {He}, Haoyang and {He}, Xinwei and {Tong}, Ao and {Gan}, Zhenye and {Wang}, Chengjie and {Bai}, Xiang},
        title = "{LLaVA-KD: A Framework of Distilling Multimodal Large Language Models}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computer Vision and Pattern Recognition},
         year = 2024,
        month = oct,
          eid = {arXiv:2410.16236},
        pages = {arXiv:2410.16236},
          doi = {10.48550/arXiv.2410.16236},
archivePrefix = {arXiv},
       eprint = {2410.16236},
 primaryClass = {cs.CV},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2024arXiv241016236C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

