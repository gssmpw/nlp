\section{Related Works}
\textbf{Omnimodal Large Language Models.} Recent advancements in multimodal large models have primarily focused on Vision-Language Models, e.g., CLIP \cite{radford2021clip}, followed by models such as Intern-VL \cite{chen2024internvl25}, which use MLPs to integrate vision encoders and LLMs for enhanced semantic alignment. Audio-Language Models, like Qwen-Audio \cite{chu2024qwen2audio}, combine audio encoders with LLMs to directly map audio signals to text. Recently, Omnimodal Large Language Models (OLLMs) have emerged, integrating vision, audio, and text by aligning their encoders during training for end-to-end processing. Models such as VITA \cite{fu2024vita, fu2025vita1.5}, Mini-Omni2 \cite{xie2024miniomni2}, MiniCPM-o \cite{MiniCPMo}, and Baichuan-Omni \cite{li2024baichuanomni} have demonstrated strong multimodal performance.
\begin{figure}[!tb]
    \centering
    \includegraphics[width=.75\linewidth]{image/fig8.pdf}
    \caption{Ablation results for KD ratio $\alpha$.}
    \label{fig:alpha_ablation}
\end{figure}

\textbf{Knowledge Distillation in MLLMs.}
Knowledge distillation \cite{hinton2015distilling} has recently been applied to multimodal large language models (MLLMs). For example, LLaVA-MoD \cite{2024arXiv240815881S} and LLaVA-KD \cite{2024arXiv241016236C} use knowledge distillation to transfer the performance of large teacher models to smaller student models. This paper proposes a self-knowledge distillation method, dividing the same model into teacher and student components to bring the vision-audio capabilities of OLLMs closer to their vision-text capabilities.