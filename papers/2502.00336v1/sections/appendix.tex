%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Score matching}
\subsection{Proof that minimizers of $\cL_{\text{DSM}}$ and $\cL_{\text{SM}}$ are the same}\label{appndx:dsm_sm_equiv}
From the $L_2$ minimization property of conditional expectation, the minimizer of $\cL_{\text{DSM}}$ is given by $\hat{s}(t,x) = \expectCond{\nabla\log P_t(x_t|x_0)}{x_t=x}$. We have 
%\begin{align*}
%    &\expectCond{\nabla\log P_t(x_t|x_0)}{x_t=x} \\
%    &\qquad= \int \dd x_0 \frac{\nabla P_t(x_t=x|x_0)}{P_t(x_t=x|x_0)} P_t(x_0|x_t=x)\\
%    &= \int \dd x_0 \nabla P_t(x_t=x|x_0)\frac{P_0(x_0)}{P_t(x)}\\
%    &= \frac{1}{P_t(x)}\nabla\int \dd x_0 P_t(x_t=x|x_0)P_0(x_0)\\
%    &= \nabla \log P_t(x) \;.
%\end{align*}
\begin{align*}
    \expectCond{\nabla\log P_t(x_t|x_0)}{x_t=x} =& \int \dd x_0 \frac{\nabla P_t(x_t=x|x_0)}{P_t(x_t=x|x_0)} P_t(x_0|x_t=x)\\
    =& \int \dd x_0 \nabla P_t(x_t=x|x_0)\frac{P_0(x_0)}{P_t(x)}\\
    =& \frac{1}{P_t(x)}\nabla\int \dd x_0 P_t(x_t=x|x_0)P_0(x_0)\\
    =& \nabla \log P_t(x) \;.
\end{align*}
Hence, we have shown that the minimizer of $\cL_{\text{DSM}}$ is same as the minimizer of $\cL_{\text{SM}}$.

\iffalse
\subsection{Simplification of $\cL_{\text{DSM}}$}\label{appndx:dsm_simpl}
\begin{align*}
    \cL_{\text{DSM}}(s) &= \frac{1}{T}\int_0^T  \dd t \; \bE{}{\norm{s(t,x_t)-\nabla\log P_t(x_t|x_0)}^2},\\
    &= \frac{1}{T}\int_0^T \dd t \;  \bE{x_0\sim \pi}{\bE{x_t\sim P_t(x_t|x_0)}{\norm{s(t,x_t)-\nabla\log P_t(x_t|x_0)}^2}},\\
    &= \frac{1}{T}\int_0^T  \dd t \; \bE{x_0\sim \pi}{\bE{z\sim\cN{0,I_d}}{\norm{s(t,a_tx_0+\sqrt{h_t}z)+\frac{z}{\sqrt{h_t}}}^2}}.
\end{align*}

\subsection{Denoising score matching with Random Features model}\label{appndx:dsm_rfm_opt}
In this section, we assume $s_\theta(t,x)=\rat{A_t}{p}\varrho(\rat{W_t}{d}x)$, $W_t\in\R^{p\times d}$, $A_t\in\R^{d\times p}$, where the elements of $W_t$ are distributed as $\cN{0,1}$, and $x_i\sim\pi\equiv\cN{0,I_d}$, $z_{ij}\sim\cN{0,I_d}$.
\begin{align*}
    \hat{\cL}_t(A_t) &= \frac{1}{nm}\sum_{i=1}^{n}{\sum_{j=1}^{m}\norm{\rat{A_t}{p}\varrho(\rat{W_t}{d}(a_tx_i+\sqrt{h_t}z_{ij})+\frac{z_{ij}}{\sqrt{h_t}}}^2}+\frac{\lambda}{p}\norm{A_t}_F^2,\\
    &= \frac{1}{nm}\sum_{i=1}^{n}{\norm{\rat{A_t}{p}\varrho(\rat{W_t}{d}(a_tx_i\textbf{1}^T+\sqrt{h_t}Z_i)+\frac{Z_i}{\sqrt{h_t}}}_F^2}+\frac{\lambda}{p}\norm{A_t}_F^2,\\
    &=\textbf{tr}\{\rat{A_t^T}{p}\rat{A_t}{p} (U+\lambda I_p)\}+\frac{2}{\sqrt{h_t}}\textbf{tr}\{\rat{A_t}{p} V\}+\frac{d}{h_t},\\
\end{align*}
where $Z_i = [z_{i1},z_{i2},\cdots,z_{im}]$,
$$U = \frac{1}{nm}\sum_{i=1}^n{\varrho(\rat{W_t}{d}(a_tx_i\textbf{1}^T+\sqrt{h_t}Z_i))\varrho(\rat{W_t}{d}(a_tx_i\textbf{1}^T+\sqrt{h_t}Z_i))^T}$$ and 
$$V = \frac{1}{nm}\sum_{i=1}^n{\varrho(\rat{W_t}{d}(a_tx_i\textbf{1}^T+\sqrt{h_t}Z_i))Z_i^T}.$$
Thus we get the optimal $A_t$ as
\begin{equation}
    \rat{\hat{A}_t}{p} = -\frac{1}{\sqrt{h_t}}V^T (U+\lambda I_p)^{-1}.
\end{equation}
\fi

\subsection{Proof of Lemma~\ref{lem:bias_variance_minf}}\label{appndx:bias_var_proof}
Let $P^e_t$ denote the joint probability distribution of $(y_t,z)$, where $y_t= a_tx+\sqrt{h_t}z$, $x\sim\frac{1}{n}\sum_{i=1}^n\delta_{x_i}$ and $z\sim\cN{0,I_d}$. We have 
\begin{align*}
    \cE^\infty_{\text{train}}(\hat{A}_t) &= \frac{1}{dn}\sum_{i=1}^{n}{\shortexpect_z\norm{\sqrt{h_t}\rat{\hat{A}_t}{p}\act{\rat{W_t}{d}(a_tx_i+\sqrt{h_t}z)}+z}^2}\\
    &= \frac{1}{d}{\bE{P_t^e}{\norm{\sqrt{h_t}\rat{\hat{A}_t}{p}\act{\rat{W_t}{d}y_t}+z}^2}}\\
    &\stackrel{(a)}{=} \frac{1}{d}\bE{P_t^e}{\norm{\sqrt{h_t}\rat{\hat{A}_t}{p}\act{\rat{W_t}{d}y_t}-\sqrt{h_t}s^e(t,y_t)}^2+\norm{\sqrt{h_t}s^e(t,y_t)+z}^2}\\
    &= h_t\cM_t+\cV_t \;.
\end{align*}
The equality $(a)$ follows from the following:
\begin{align*}
    &\bE{P_t^e}{\left\langle \rat{\hat{A}_t}{p}\act{\rat{W_t}{d}y}-s^e(t,y),\sqrt{h_t}s^e(t,y)+z \right\rangle}\\ 
    &\qquad=\bE{P_t^e}{\expectCond{\left\langle \rat{\hat{A}_t}{p}\act{\rat{W_t}{d}y}-s^e(t,y),\sqrt{h_t}s^e(t,y)+z \right\rangle}{y}}\\
    &\qquad=\bE{P_t^e}{\left\langle \rat{\hat{A}_t}{p}\act{\rat{W_t}{d}y}-s^e(t,y),\sqrt{h_t}s^e(t,y)+\expectCond{z}{y} \right\rangle}\\
    &\qquad= \bE{P_t^e}{\left\langle \rat{\hat{A}_t}{p}\act{\rat{W_t}{d}y}-s^e(t,y),\sqrt{h_t}s^e(t,y)-\sqrt{h_t}\nabla\log P_t^e(y) \right\rangle}\\
    &\qquad= 0 \;,
\end{align*}
since we have $\expectCond{z}{y} = -\sqrt{h_t}\nabla\log P_t^e(y)$.

%\section{Linear Pencils}\label{appndx:linear_pencils}

\section{Proof of Theorem~\ref{thm:lc_minf}}\label{appndx:lc_minf_proof}
When $m=\infty$, the denoising score matching loss is given by 
\begin{align*}
    \cL^\infty_t(A_t) &=\frac{1}{dn}\sum_{i=1}^{n}{\bE{z}{\norm{\sqrt{h_t}\rat{A_t}{p}\act{\rat{W_t}{p}(a_tx_i+\sqrt{h_t}z)}+z}^2}}+\frac{h_t\lambda}{dp}\norm{A_t}_F^2\\
    &=\frac{h_t}{d}\tr{\rat{A_t}{p}^T\rat{A_t}{p} U}+\frac{2\sqrt{h_t}}{d}\tr{\rat{A_t}{p} V}+1+\frac{h_t\lambda}{d}\tr{\rat{A_t^T}{p}\rat{A_t}{p}}\;,\\
\end{align*}
where 
$$U = \frac{1}{n}\sum_{i=1}^n\bE{z}{\act{\rat{W_t}{d}(a_tx_i+\sqrt{h_t}z)}\act{\rat{W_t}{d}(a_tx_i+\sqrt{h_t}z)}^T}\;,$$
and 
$$V = \frac{1}{n}\sum_{i=1}^n\bE{z}{\act{\rat{W_t}{d}(a_tx_i+\sqrt{h_t}z)}z^T}\;.$$ Thus we get the optimal $A_t$ as
\begin{equation}
    \rat{\hat{A}_t}{p} = -\frac{1}{\sqrt{h_t}}V^T(U+\lambda I_p)^{-1}\;.
\end{equation}
Now, we compute test error (generalization error) and train error when $P_0\equiv\cN{0,I_d}$. We note that in this case, $P_t$ remains $\cN{0,1}$ for all $t$.\\\\
\textbf{Test Error:}
\begin{align*}
    \cE^\infty_{\text{test}}(\hat{A}_t) &= \frac{1}{d}\bE{x\sim P_t}{\norm{\rat{\hat{A}_t}{p}\act{\rat{W_t}{d}x}-\nabla\log P_t(x)}^2}\\
    &= \frac{1}{d}\bE{x\sim P_t}{\norm{\rat{\hat{A}_t}{p}\act{\rat{W_t}{d}x}+x}^2}\\
    &=1 - \frac{2}{d}\tr{\frac{1}{\sqrt{h_t}}V^T (U+\lambda I_p)^{-1}\underbrace{\bE{x}{\act{\rat{W_t}{d}x}x^T}}_{:=\Tilde{V}}}\\
    &\qquad+\frac{1}{d}\tr{\frac{1}{h_t}(U+\lambda I_p)^{-1}V V^T(U+\lambda I_p)^{-1}\underbrace{\bE{x}{\act{\rat{W_t}{d}x}\act{\rat{W_t}{d}x}^T}}_{:=\Tilde{U}}}\;.
\end{align*}
Since we focus on a single time instant, we drop the subscript $t$ in the above expressions. However, it is important to note that $a$ and $h$ depend on $t$, and we have the relation $a^2+h=1$.

We need to compute $V,U,\Tilde{V},\Tilde{U}$ in order to get an expression for $\cE^\infty_{\text{test}}$. We will first consider $\Tilde{V}$:
\begin{equation*}
    \Tilde{V} = \bE{x}{\act{\rat{W}{d}x}x^T}\;.
\end{equation*}
Let $P^\gamma$ denote the bivariate standard Gaussian distribution with correlation coefficient $\gamma$. Explicitly,
\begin{equation}\label{eqn:bivariate_gaussian_pdf}
    P^\gamma(x,y) = \frac{1}{2\pi\sqrt{1-\gamma^2}}e^{-\frac{x^2+y^2-2\gamma xy}{2(1-\gamma^2)}}\;.
\end{equation}
Let $w_i$ denote the $i^{\text{th}}$ row of $W$. For large $d$, $\frac{\norm{w_i}^2}{d}$ concentrates to $1$. Then:
\begin{align*}
    \Tilde{V}_{ij} &= \bE{x}{\act{\rat{w_i^Tx}{d}}x_j}\\
    &= \bE{(u,v)\sim P^{\rat{w_{ij}}{d}}}{\act{u}v}\\
    &\stackrel{(a)}{=} \sum_{k=0}^{\infty} \frac{(\rat{w_{ij}}{d})^k}{k!}\bE{u}{\act{u}\text{He}_k(u)}\bE{v}{v\text{He}_k(v)}\\
    &= \mu_1\rat{w_{ij}}{d} \;,
\end{align*}
where in $(a)$ we used the Mehler Kernel formula \cite{kibble_extension_1945}.
Hence, we have $\Tilde{V} = \mu_1\rat{W}{d}$. Now, we consider $\Tilde{U}$:
\begin{align*}
    \Tilde{U}_{ij} &= \bE{x}{\act{\rat{w_i^Tx}{d}}\act{\rat{w_j^Tx}{d}}}\\
    &= \bE{(u,v)\sim P^{\frac{w_i^Tw_j}{d}}}{\varrho(u)\varrho(v)}\\
    &= \sum_{k=0}^{\infty} \frac{(\frac{w_i^Tw_j}{d})^k}{k!}\bE{u}{\varrho(u)\text{He}_k(u)}\bE{u}{\varrho(v)\text{He}_k(v)}\\
    &= \sum_{k=0}^{\infty} \frac{(\frac{w_i^Tw_j}{d})^k}{k!}\bE{u}{\varrho(u)\text{He}_k(u)}^2 \;.
\end{align*}
Let $\mu_0 = \bE{g}{\varrho(g)},\; \mu_1 = \bE{g}{\varrho(g)g},\; \norm{\varrho}^2 = \bE{g}{\varrho(g)^2}$, where $g\sim\cN{0,1}$. Then, we have
\begin{equation*}
    \Tilde{U}_{ij} = \begin{cases}
        \mu_0^2+\mu_1^2 \frac{w_i^Tw_j}{d} + O(1/d)\quad &\text{if } i\neq j \;,\\
        \norm{\varrho}^2\quad &\text{if } i=j \;.
    \end{cases}
\end{equation*}
Let $v^2 = \norm{\rho}^2-\mu_0^2-\mu_1^2$. The $\bigO{1/d}$ term cannot give rise to a $\bigO{1}$ change in the asymptotic spectrum. Hence, we neglect it. We have
\begin{equation*}
    \Tilde{U} = \mu_0^2 \bone_p\bone_p^T + \mu_1^2 \rat{W}{d}\rat{W^T}{d} + v^2 I_p \;.
\end{equation*}
Now we will consider $V$. Let
\begin{equation*}
    V^l = \bE{z}{\act{\rat{W}{d}(ax_l+\sqrt{h}z)}z^T}\;.
\end{equation*}
We have 
\begin{align*}
    V^l_{ij} &= \bE{z}{\act{\rat{w_i^T(ax_l+\sqrt{h}z)}{d}}z_j}\\
    &= \bE{(u,v)\sim P^{\rat{w_{ij}}{d}}}{\act{\rat{a w_i^Tx_l}{d}+\sqrt{h}u}v}\\
    &= \sum_{k=0}^{\infty} \frac{(\rat{w_{ij}}{d})^k}{k!}\bE{u}{\act{\rat{a w_i^Tx_l}{d}+\sqrt{h}u}\text{He}_k(u)}\bE{v}{v\text{He}_k(v)}\\
    &= \rat{w_{ij}}{d}\bE{u}{\act{\rat{a w_i^Tx_l}{d}+\sqrt{h}u}u}\\
    &= \rat{w_{ij}}{d}\varrho_1\left(\rat{a w_i^Tx_l}{d}\right)\;,
\end{align*}
where $\varrho_1(y) = \bE{u}{\varrho(y+\sqrt{h}u)u}$. Summing over the $n$ data samples:
\begin{align*}
    V_{ij} &= \frac{1}{n}\sum_{l=1}^{n} V^l_{ij}\\
    &= \rat{w_{ij}}{d} \frac{1}{n}\sum_{l=1}^{n} \varrho_1\left(\rat{a w_i^Tx_l}{d}\right)\\
    &= \rat{w_{ij}}{d} \bE{x}{\varrho_1\left(\rat{a w_i^Tx}{d}\right)} + O(1/d)\\
    &= \rat{w_{ij}}{d} \bE{g}{\varrho_1(ag)} + O(1/d)\\
    &= \rat{w_{ij}}{d} \bE{g,u}{\varrho(ag+\sqrt{h}u)u} + O(1/d)\\
    &= \sqrt{h}\mu_1\rat{w_{ij}}{d} + O(1/d)\;.
\end{align*}
Neglecting $O(1/d)$ terms, we have $V = \sqrt{h}\mu_1\rat{W}{d}$. Now, let's consider $U$. Let
\begin{align*}
    U^l &= \bE{z}{\act{\rat{W}{d}(ax_l+\sqrt{h}z)}\act{\rat{W}{d}(ax_l+\sqrt{h}z)}^T}\;.
\end{align*}
For $i\neq j $ we have,
\begin{align*}
    U^l_{ij} &= \bE{z}{\act{\rat{w_i^T(ax_l+\sqrt{h}z)}{d}}\act{\rat{w_j^T(ax_l+\sqrt{h}z)}{d}}}\\
    &= \bE{(u,v)\sim P^{\frac{w_i^Tw_j}{d}}}{\act{a\rat{w_i^Tx_l}{d} + \sqrt{h}u}\act{a\rat{w_j^Tx_l}{d} + \sqrt{h}v}}\\
    &= \sum_{k=0}^{\infty} \frac{(\frac{w_i^Tw_j}{d})^k}{k!}\bE{u}{\act{a\rat{w_i^Tx_l}{d} + \sqrt{h}u}\text{He}_k(u)}\bE{v}{\act{a\rat{w_j^Tx_l}{d} + \sqrt{h}v}\text{He}_k(v)}\\
    &= \varrho_0\left(a\rat{w_i^Tx_l}{d}\right)\varrho_0\left(a\rat{w_j^Tx_l}{d}\right)+\frac{w_i^Tw_j}{d}\varrho_1\left(a\rat{w_i^Tx_l}{d}\right)\varrho_1\left(a\rat{w_j^Tx_l}{d}\right)+O(1/d)\;,
\end{align*}
%where $\varrho_0(y) = \bE{u}{\varrho(y+\sqrt{h}u)}$ and $\varrho_1(y) = \bE{u}{\varrho(y+\sqrt{h}u)u}$. Let $X=[x_1, x_2,\cdots,x_n]\in\R^{d\times n}$:
where $\varrho_0(y) = \bE{u}{\varrho(y+\sqrt{h}u)}$ and $\varrho_1(y) = \bE{u}{\varrho(y+\sqrt{h}u)u}$. Summing over the $n$ data samples:
\begin{align*}
    U_{ij} &= \frac{1}{n}\sum_{l=1}^{n} U^l_{ij}\\
    &= \frac{1}{n}\sum_{l=1}^{n}\varrho_0\left(a\rat{w_i^Tx_l}{d}\right)\varrho_0\left(a\rat{w_j^Tx_l}{d}\right)+\frac{w_i^Tw_j}{d}\frac{1}{n}\sum_{l=1}^{n}\varrho_1\left(a\rat{w_i^Tx_l}{d}\right)\varrho_1\left(a\rat{w_j^Tx_l}{d}\right) + O(1/d)\\
    &= \frac{1}{n}\sum_{l=1}^{n}\varrho_0\left(a\rat{w_i^Tx_l}{d}\right)\varrho_0\left(a\rat{w_j^Tx_l}{d}\right)+\frac{w_i^Tw_j}{d}\bE{x}{\varrho_1\left(a\rat{w_i^Tx}{d}\right)\varrho_1\left(a\rat{w_j^Tx}{d}\right)} + O(1/d)\\
    &= \frac{1}{n}\sum_{l=1}^{n}\varrho_0\left(a\rat{w_i^Tx_l}{d}\right)\varrho_0\left(a\rat{w_j^Tx_l}{d}\right)+\frac{w_i^Tw_j}{d}\bE{g}{\varrho_1(ag)}^2 + O(1/d)\\
    &= \frac{1}{n}\sum_{l=1}^{n}\varrho_0\left(a\rat{w_i^Tx_l}{d}\right)\varrho_0\left(a\rat{w_j^Tx_l}{d}\right)+h\mu_1^2\frac{w_i^Tw_j}{d} + O(1/d)\;.\\
\end{align*}
For $i=j$, we have:
\begin{align*}
    U^l_{ii} &= \bE{z}{\left(\act{\rat{w_i^T(ax_l+\sqrt{h}z)}{d}}\right)^2} \;,\\
\end{align*}
and
\begin{align*}
    U_{ii} &= \frac{1}{n}\sum_{l=1}^n\bE{z}{\left(\act{\rat{w_i^T(ax_l+\sqrt{h}z)}{d}}\right)^2}\\
    &= \bE{z,x}{\left(\act{\rat{w_i^T(ax_l+\sqrt{h}z)}{d}}\right)^2}+O(1/\sqrt{d})\\
    &= \norm{\varrho}^2+O(1/\sqrt{d}) \;.
\end{align*}
The $\bigO{1/\sqrt{d}}$ term in the above equation can be neglected, since there are only $\bigO{d}$ terms on the diagonal. Let $X=[x_1,x_2,\cdots,x_n]\in\R^{d\times n}$. We can write $U$ as:
\begin{equation*}
    U = \rat{\varrho_0\left(a\rat{W}{d}X\right)}{n}\rat{\varrho_0\left(a\rat{W}{d}X\right)^T}{n}+h\mu_1^2\rat{W}{d}\rat{W^T}{d}+s^2I_p \;,
\end{equation*}
where
\begin{align*}
s^2 &= \norm{\varrho}^2-\bE{g}{\varrho_0(ag)^2}-h\mu_1^2\\
&= \norm{\varrho}^2-\bE{g}{\bE{u}{\varrho(ag+\sqrt{h}u)}^2}-h\mu_1^2\\
&= \norm{\varrho}^2-c(a^2)-h\mu_1^2 \;,
\end{align*}
with $c(\gamma) = \bE{u,v\sim P^\gamma}{\varrho(u)\varrho(v)}$.

We can use Gaussian equivalence principle to replace the nonlinear term in $U$. A Gaussian equivalent for $\varrho_0(a\rat{W}{d}X)$ is given by
\begin{align*}
    G &= \bE{g}{\varrho_0(ag)}\bone_p\bone_n^T+\bE{g}{\varrho_0(ag)g}\rat{W}{d}X+\left(\bE{g}{\varrho_0(ag)^2}-\bE{g}{\varrho_0(ag)}^2-\bE{g}{\varrho_0(ag)g}^2\right)^{1/2}\Omega\\
    &= \mu_0\bone_p\bone_n^T+a\mu_1\rat{W}{d}X+\left(\underbrace{c(a^2)-\mu_0^2-a^2\mu_1^2}_{:=v_0^2}\right)^{1/2}\Omega\;,
\end{align*}
where $\Omega\in\R^{p\times n}$ is a random matrix with standard Gaussian entries.
Hence we have
\begin{equation*}
    U = \rat{G}{n}\rat{G^T}{n}+h\mu_1^2\rat{W}{d}\rat{W^T}{d}+s^2I_p \;,
\end{equation*}
with
\begin{equation*}
    G = \mu_0\bone_p\bone_n^T+a\mu_1\rat{W}{d}X+v_0\Omega\;.
\end{equation*}
We now have expressions for all terms in the generalization error.
\begin{align*}
    \cE^\infty_{\text{test}}(\hat{A}_t) &=1 - \frac{2}{d}\tr{\frac{1}{\sqrt{h}}V^T (U+\lambda I_p)^{-1}\Tilde{V}}+\frac{1}{d}\tr{\frac{1}{h}(U+\lambda I_p)^{-1}V V^T(U+\lambda I_p)^{-1}\Tilde{U}}\\
    &= 1-\frac{2\mu_1^2}{d}\tr{\rat{W^T}{d}(U+\lambda I_p)^{-1}\rat{W}{d}}\\&\qquad+\frac{\mu_1^2}{d}\tr{(U+\lambda I_p)^{-1}\rat{W}{d}\rat{W^T}{d}(U+\lambda I_p)^{-1}\left(\mu_0^2 \bone_p\bone_p^T + \mu_1^2 \rat{W}{d}\rat{W^T}{d} + v^2 I_p\right)}\;.
\end{align*}
For simplicity in presentation, assume that $\mu_0=0$ and write
\begin{align*}
    \cE^\infty_{\text{test}}(\hat{A}_t) &= 1-2\mu_1^2E_1+\mu_1^4E_2+\mu_1^2v^2E_3 \;,
\end{align*}
with
\begin{align*}
    E_1 &= \frac{1}{d}\tr{\rat{W^T}{d}(U+\lambda I_p)^{-1}\rat{W}{d}}\;,\\
    E_2 &= \frac{1}{d}\tr{\rat{W^T}{d}(U+\lambda I_p)^{-1}\rat{W}{d}\rat{W^T}{d}(U+\lambda I_p)^{-1}\rat{W}{d}}\;,\\
    E_3 &= \frac{1}{d}\tr{\rat{W^T}{d}(U+\lambda I_p)^{-2}\rat{W}{d}}\;.
\end{align*}
Now define the following matrix
\begin{equation*}
    U(q) :=  \rat{G}{n}\rat{G^T}{n}+(h\mu_1^2+q)\rat{W}{d}\rat{W^T}{d}+s^2I_p \;,
\end{equation*}
and the resolvent of $U(q)$ as
\begin{equation*}
    R(q,z) = (U(q)-zI_p)^{-1}\;.
\end{equation*}
Let 
\begin{equation*}
    K(q,z) = \frac{1}{d}\tr{\rat{W^T}{d}R(q,z)\rat{W}{d}}\;.
\end{equation*}
Using the identities $\frac{\partial R}{\partial q} = -R(q,z)\frac{\dd U}{\dd q}R(q,z)$ and $\frac{\partial R}{\partial z} = R(q,z)^2$, we observe that 
\begin{align*}
    E_1 &= K(0,-\lambda)\;,\\
    E_2 &= -\frac{\partial K}{\partial q}(0,-\lambda)\;,\\
    E_3 &= \frac{\partial K}{\partial z}(0,-\lambda)\;.
\end{align*}
Therefore, assuming we can take limit of the expectation inside the derivative, it suffices to have the function $\cK(q,z):=\lim_{d\to\infty}\bE{}{K(q,z)}$ to have an expression for $\lim_{d\to\infty}\bE{}{\cE^\infty_{\text{test}}(\hat{A}_t)}$. We have
\begin{align}\label{eqn:Etest_minf}
    \lim_{d\to\infty}\bE{}{\cE^\infty_{\text{test}}(\hat{A}_t)} &= 1-2\mu_1^2\lim_{d\to\infty}\bE{}{E_1}+\mu_1^4\lim_{d\to\infty}\bE{}{E_2}+\mu_1^2v^2\lim_{d\to\infty}\bE{}{E_3} \;,\nonumber\\
    &= 1-2\mu_1^2\cK(0,-\lambda)+\mu_1^4\frac{\partial \cK}{\partial q}(0,-\lambda)+\mu_1^2v^2\frac{\partial \cK}{\partial z}(0,-\lambda) \;,\nonumber\\
    &= 1-2\mu_1^2e_1+\mu_1^4e_2+\mu_1^2v^2e_3 \;,
\end{align}
where we define $e_1 = \cK(0,-\lambda),\; e_2 = -\frac{\partial \cK}{\partial q}(0,-\lambda),\; e_3 = \frac{\partial \cK}{\partial z}(0,-\lambda)$. The above computations can also be performed avoiding the change in order of derivatives and expectation, by first differentiating $K(q,z)$ (instead of $\mathcal{K}(q, z)$), and then constructing a linear pencil matrix that is almost twice the size of the current one. This is more tedious, and we opt for the smaller linear pencil matrix here.

We derive an expression for $\cK$ using the Linear Pencils method. We start by constructing the following $4\times 4$ block matrix:
\begin{equation*}
    L = \mleft[
        \begin{array}{c|ccc}
        (s^2-z)I_p & a\mu_1\rat{W}{d} & v_0\rat{\Omega}{n} & (h\mu_1^2+q)\rat{W}{d}\\
        \hline
        0 & I_d & -\rat{X}{n} & 0\\
        -v_0\rat{\Omega^T}{n} & 0 & I_n & -a\mu_1\rat{X^T}{n}\\
        -\rat{W^T}{d} & 0 & 0 & I_d
    \end{array}
    \mright] = \mleft[\begin{array}{cc}
        L_{11} & L_{12} \\
        L_{21} & L_{22}
    \end{array}\mright] \;.
\end{equation*}
First, we can invert $L$ and verify that $K$ is trace of one of the blocks in $L^{-1}$. Let $\bar{L}_{22} = L_{11}-L_{12}L_{22}^{-1}L_{21}$. Using the block matrix inversion formula, we have:
\begin{equation*}
    L^{-1} = \mleft[
            \begin{array}{cc}
              \bar{L}_{22}^{-1}   &  -\bar{L}_{22}^{-1}L_{12}L_{22}^{-1}\\
               -L_{22}^{-1}L_{21}\bar{L}_{22}^{-1}  &  L_{22}^{-1}+ L_{22}^{-1}L_{21}\bar{L}_{22}^{-1}L_{12}L_{22}^{-1}
            \end{array}
    \mright]\;,
\end{equation*}
\begin{equation*}
    L_{22}^{-1} = \mleft[\begin{array}{ccc}
       I_d  &  \rat{X}{n} & a\mu_1\rat{X}{n}\rat{X^T}{n}\\
       0  & I_n & a\mu_1\rat{X^T}{n}\\
       0  & 0 & I_d\\
    \end{array}\mright]\;,
\end{equation*}
\begin{align*}
    \bar{L}_{22} &= (s^2-z)I_p + v_0\rat{G}{n}\rat{\Omega^T}{n}+a\mu_1\rat{G}{n}\rat{X^T}{n}\rat{W^T}{d}+(h\mu_1^2+q)\rat{W}{d}\rat{W^T}{d}\\
    &= (s^2-z)I_p + \rat{G}{n}\rat{G^T}{n}+(h\mu_1^2+q)\rat{W}{d}\rat{W^T}{d}\\
    &= R^{-1}(q,z)\;.    
\end{align*}
Hence,
\begin{multline*}
    L^{-1} = \left[
    \begin{array}{ccc}
       R(q,z)  & -a\mu_1R(q,z)\rat{W}{d} & -R(q,z)\rat{G}{n} \\
        \rat{X}{n}\rat{G^T}{n}R(q,z) & I_d-a\mu_1\rat{X}{n}\rat{G^T}{n}R(q,z)\rat{W}{d} & \rat{X}{n}-\rat{X}{n}\rat{G^T}{n}R(q,z)\rat{G}{n}\\
        \rat{G^T}{n}R(q,z) & -a\mu_1\rat{G^T}{n}R(q,z)\rat{W}{d} & I_n-\rat{G^T}{n}R(q,z)\rat{G}{n}\\
        \rat{W^T}{d} & -a\mu_1\rat{W^T}{d}R(q,z)\rat{W}{d} & -\rat{W^T}{d}R(q,z)\rat{G}{n}
    \end{array}\right.\\
    \left.\begin{array}{c}
         -R(q,z)(a\mu_1\rat{G}{n}\rat{X^T}{n}+(h\mu_1^2+q)\rat{W}{d})  \\
          a\mu_1\rat{X}{n}\rat{X^T}{n}-\rat{X}{n}\rat{G^T}{n}R(q,z)(a\mu_1\rat{G}{n}\rat{X^T}{n}+(h\mu_1^2+q)\rat{W}{d})\\
          a\mu_1\rat{X^T}{n}-\rat{G^T}{n}R(q,z)(a\mu_1\rat{G}{n}\rat{X^T}{n}+(h\mu_1^2+q)\rat{W}{d})\\
          I_d-\rat{W^T}{d}R(q,z)(a\mu_1\rat{G}{n}\rat{X^T}{n}+(h\mu_1^2+q)\rat{W}{d})
    \end{array}
    \right] \;.
\end{multline*}

% \begin{equation*}
% \footnotesize
%     L^{-1} = \mleft[
%     \begin{array}{cccc}
%        R(q,z)  & -a\mu_1R(q,z)\rat{W}{d} & -R(q,z)\rat{G}{n} & -R(q,z)(a\mu_1\rat{G}{n}\rat{X^T}{n}+(h\mu_1^2+q)\rat{W}{d}) \\
%         \rat{X}{n}\rat{G^T}{n}R(q,z) & I_d-a\mu_1\rat{X}{n}\rat{G^T}{n}R(q,z)\rat{W}{d} & \rat{X}{n}-\rat{X}{n}\rat{G^T}{n}R(q,z)\rat{G}{n} & a\mu_1\rat{X}{n}\rat{X^T}{n}-\rat{X}{n}\rat{G^T}{n}R(q,z)(a\mu_1\rat{G}{n}\rat{X^T}{n}+(h\mu_1^2+q)\rat{W}{d})\\
%         \rat{G^T}{n}R(q,z) & -a\mu_1\rat{G^T}{n}R(q,z)\rat{W}{d} & I_n-\rat{G^T}{n}R(q,z)\rat{G}{n} & a\mu_1\rat{X^T}{n}-\rat{G^T}{n}R(q,z)(a\mu_1\rat{G}{n}\rat{X^T}{n}+(h\mu_1^2+q)\rat{W}{d})\\
%         \rat{W^T}{d} & -a\mu_1\rat{W^T}{d}R(q,z)\rat{W}{d} & -\rat{W^T}{d}R(q,z)\rat{G}{n} & I_d-\rat{W^T}{d}R(q,z)(a\mu_1\rat{G}{n}\rat{X^T}{n}+(h\mu_1^2+q)\rat{W}{d})
%     \end{array}
%     \mright]
% \end{equation*}
We see that $(L^{-1})^{4,2}$ gives the desired term. We use the linear pencil formalism to derive the traces of the square blocks in $L^{-1}$. Let $g$ be the matrix of traces of square blocks in $L^{-1}$ divided by the block size. For example, if $L^{i,j}$ is a square matrix of dimension $N$, then $g_{ij}=\frac{1}{N}\tr{(L^{-1})^{i,j}}$. We assign $g_{ij}=0$ if $L^{i,j}$ is not a square matrix. We have
\begin{equation*}
    g = \mleft[
    \begin{array}{cccc}
        g_{11} & 0 & 0 & 0  \\
        0 & g_{22} & 0 & g_{24}  \\
        0 & 0 & g_{33} & 0  \\
        0 & g_{42} & 0 & g_{44}  \\
    \end{array}
    \mright]\;.
\end{equation*}
Notice that the constant matrices in $L$ are all multiples of identity. Let $B$ be the matrix that contains the coefficients of these constant matrices, given by
\begin{equation*}
    B = \mleft[
    \begin{array}{cccc}
        s^2-z & 0 & 0 & 0  \\
        0 & 1 & 0 & 0  \\
        0 & 0 & 1 & 0  \\
        0 & 0 & 0 & 1  \\
    \end{array}
    \mright]\;.
\end{equation*}
If $L^{il}$ and $L^{jk}$ are square matrices, let $\sigma_{ij}^{kl}$ be the covariance between an element of $L^{ij}$ and $L^{kl}$ multiplied by the block size of $L^{jk}$. Let $L^{ij}$ be of dimension $N_i\times N_j$ and $M$ denote the non constant part of $L$. Then,
\begin{equation*}
    \sigma_{ij}^{kl} = N_j\bE{}{M^{ij}_{uv}M^{kl}_{vu}}\;.
\end{equation*}
Let $S = \{(i,j): N_i=N_j\}$ be the set of indices of square blocks in $L$. Then, a mapping $\eta_L$ is defined such that
\begin{equation*}
    \eta_L(G)_{il} = \sum_{(jk) \in S} \sigma_{ij}^{kl}g_{jk}\;,
\end{equation*}
for $(il)$ in $S$. We get
\begin{equation*}
    \eta_L(g) = \mleft[
    \begin{array}{cccc}
        \sigma_{12}^{41}g_{24}+\sigma_{13}^{31}g_{33}+\sigma_{14}^{41}g_{44} & 0 & 0 & 0  \\
        0 & 0 & 0 & \sigma_{23}^{34}g_{33}  \\
        0 & 0 & \sigma_{31}^{13}g_{11}+\sigma_{34}^{23}g_{42} & 0  \\
        0 & \sigma_{41}^{12}g_{11} & 0 & \sigma_{41}^{14}g_{11}  \\
    \end{array}
    \mright]\;.
\end{equation*}
We have $N_1=p, N_2=d, N_3=n, N_4=d$ :
%\begin{align}
%    & \sigma_{12}^{41} = -a\mu_1 \;,\qquad\qquad\;\;\sigma_{41}^{12}= -a\mu_1\psi_p \;,\\
%    & \sigma_{13}^{31} = -v_0^2 \;, \qquad\qquad\;\;\;\;\sigma_{31}^{13} = -v_0^2\psi_p/\psi_n\;,\\
%    & \sigma_{14}^{41} = -(h\mu_1^2+q) \;,\qquad \sigma_{41}^{14} = -(h\mu_1^2+q)\psi_p\;,\\
%    & \sigma_{23}^{34} = a\mu_1 \;, \qquad\qquad\;\;\;\;\;\sigma_{34}^{23}=a\mu_1/\psi_n \;.
%\end{align}
\begin{alignat}{2}
  &\sigma_{12}^{41} = -a\mu_1       \;,&&\quad\quad \sigma_{41}^{12}= -a\mu_1\psi_p          \;,\\
  &\sigma_{13}^{31} = -v_0^2        \;,&&\quad\quad \sigma_{31}^{13} = -v_0^2\psi_p/\psi_n   \;,\\
  &\sigma_{14}^{41} = -(h\mu_1^2+q) \;,&&\quad\quad \sigma_{41}^{14} = -(h\mu_1^2+q)\psi_p   \;,\\
  &\sigma_{23}^{34} = a\mu_1        \;,&& \quad\quad \sigma_{34}^{23}=a\mu_1/\psi_n          \;. 
\end{alignat}
We have that $g$ satisfies the following fixed point equation
\begin{equation*}
    (B-\eta_L(g))g = I \;.
\end{equation*}
Hence:
\begin{multline*}
    \mleft[
    \begin{array}{cccc}
        s^2-z+a\mu_1g_{24}+v_0^2g_{33}+(h\mu_1^2+q)g_{44} & 0 & 0 & 0  \\
        0 & 1 & 0 & -a\mu_1g_{33}  \\
        0 & 0 & 1+v_0^2\psi_p/\psi_ng_{11}-a\mu_1/\psi_ng_{42} & 0  \\
        0 & a\mu_1\psi_pg_{11} & 0 & 1+(h\mu_1^2+q)\psi_pg_{11}  \\
    \end{array}
    \mright]\\
    \times
    \mleft[
    \begin{array}{cccc}
        g_{11} & 0 & 0 & 0  \\
        0 & g_{22} & 0 & g_{24}  \\
        0 & 0 & g_{33} & 0  \\
        0 & g_{42} & 0 & g_{44}  \\
    \end{array}
    \mright] = I \;.
\end{multline*}
This gives the following set of equations:
\begin{align*}
    g_{11}(s^2-z+a\mu_1g_{24}+v_0^2g_{33}+(h\mu_1^2+q)g_{44}) &= 1 \;,\\
    g_{22} -a\mu_1g_{33}g_{42} &= 1 \;,\\
    g_{24} -a\mu_1g_{33}g_{44} &= 0 \;,\\
    g_{33}(1+v_0^2\psi_p/\psi_ng_{11}-a\mu_1/\psi_ng_{42}) &=1 \;,\\
    a\mu_1\psi_pg_{11}g_{22}+(1+(h\mu_1^2+q)\psi_pg_{11})g_{42} &= 0 \;,\\
    a\mu_1\psi_pg_{11}g_{24}+(1+(h\mu_1^2+q)\psi_pg_{11})g_{44} &= 1 \;,\\
\end{align*}
The above set of 6 equations can be reduced to the following set of 4 equations:
\begin{align*}
    \zeta_1(s^2-z+a_t^2\mu_1^2\zeta_2\zeta_4+v_0^2\zeta_2+(h_t\mu_1^2+q)\zeta_4) -1= 0 \;,\nonumber\\
    \zeta_2(\psi_n+v_0^2\psi_p\zeta_1-a_t\mu_1\zeta_3)-\psi_n =0 \;,\nonumber\\
    a_t\mu_1\psi_p\zeta_1(1+a_t\mu_1\zeta_2\zeta_3)+(1+(h_t\mu_1^2+q)\psi_p\zeta_1)\zeta_3 = 0 \;,\nonumber\\
    a_t^2\mu_1^2\psi_p\zeta_1\zeta_2\zeta_4+(1+(h_t\mu_1^2+q)\psi_p\zeta_1)\zeta_4 -1= 0 \;,\nonumber\\
    \end{align*}
where $\zeta_1=g_{11},\; \zeta_2=g_{33},\; \zeta_3=g_{42},\; \zeta_4=g_{44}$. We solve the above set of equations numerically to obtain $\zeta_1,\zeta_2,\zeta_3,$ and $\zeta_4$. We compute $\cK = \lim_{d\to\infty}\bE{}{K}$ by using the expression $\cK(q,z)=-\frac{\zeta_3(q,z)}{a\mu_1}$. Finally, we use \eqref{eqn:Etest_minf} to compute $\lim_{d\to\infty}\bE{}{\cE^\infty_{\text{test}}(\hat{A}_t)}$.




Next, we look at the train error.\\\\
\textbf{Train error:}
\begin{align*}
    \cE^\infty_{\text{train}}(\hat{A}_t) &= \cL(\hat{A})-\frac{h_t\lambda}{pd}\norm{\hat{A}}^2_F\\
    &=\frac{h}{d}\tr{\rat{\hat{A}_t}{p}^T\rat{\hat{A}_t}{p} (U+\lambda I_p)}+\frac{2\sqrt{h}}{d}\tr{\rat{\hat{A}_t}{p} V}+1-\frac{h\lambda}{d}\tr{\rat{\hat{A}_t}{p}^T\rat{\hat{A}_t}{p}}\\
    &=\frac{1}{d}\tr{(U+\lambda I_p)^{-1}VV^T}-\frac{2}{d}\tr{V^T(U+\lambda I_p)^{-1}V}+1-\frac{\lambda}{d}\tr{V^T(U+\lambda I_p)^{-2}V}\\
    &=-\frac{h\mu_1^2}{d}\tr{(U+\lambda I_p)^{-1}\rat{W}{d}\rat{W^T}{d}}+1-\frac{h\mu_1^2\lambda}{d}\tr{(U+\lambda I_p)^{-2}\rat{W}{d}\rat{W^T}{d}}\\
    &=-h\mu_1^2K(0,-\lambda)-h\lambda\mu_1^2\frac{\partial K}{\partial z}(0,-\lambda)+1 \;.\\
\end{align*}
Thus,
\begin{align}\label{eqn:Etrain_minf}
    \lim_{d\to\infty}\bE{}{\cE^\infty_{\text{train}}(\hat{A}_t)} &= -h\mu_1^2\cK(0,-\lambda)-\lambda h\mu_1^2\frac{\partial \cK}{\partial z}(0,-\lambda)+1 \;,\nonumber\\
    &= 1-h\mu_1^2e_1-h\lambda\mu_1^2e_3 \;,
\end{align}
where $e_1 = \cK(0,-\lambda),\; e_3 = \frac{\partial \cK}{\partial z}(0,-\lambda)$.
This concludes the proof of Theorem~\ref{thm:lc_minf}.
\section{Proof of Theorem~\ref{thm:lc_m1}}\label{appndx:lc_m1_proof}
When $m=1$, the loss function reduces to:
\begin{align}
    \cL^1_t(A_t) &=\frac{1}{dn}\sum_{i=1}^{n}{{\norm{\sqrt{h_t}\rat{A_t}{p}\act{\rat{W_t}{p}(a_tx_i+\sqrt{h_t}z_i)}+z_i}^2}}+\frac{h_t\lambda}{dp}\norm{A_t}_F^2 \;.
\end{align}

Let $X=[x_1,x_2,\cdots,x_n]$, $Z=[z_1,z_2,\cdots,z_n]$, and let $Y$ be a matrix with its $i^{th}$ column given as $y_i = a_tx_i+\sqrt{h_t}z_i$. Also let $F=\varrho(\rat{W}{d}Y)$. We then write:
\begin{align*}
    \cL^1_t(A_t) &=\frac{1}{dn}\sum_{i=1}^{n}{{\norm{\sqrt{h_t}\rat{A_t}{p}\act{\rat{W_t}{p}(a_tx_i+\sqrt{h_t}z_i)}+z_i}^2}}+\frac{h_t\lambda}{dp}\norm{A_t}_F^2 \\
    &=\frac{h_t}{d}\tr{\rat{A_t}{p}^T\rat{A_t}{p} U}+\frac{2\sqrt{h_t}}{d}\tr{\rat{A_t}{p} V}+1+\frac{h_t\lambda}{d}\tr{\rat{A_t^T}{p}\rat{A_t}{p}} \;,\\
\end{align*}
where 
$$U = \frac{1}{n}\act{\rat{W_t}{d}Y}\act{\rat{W_t}{d}Y}^T = \rat{F}{n}\rat{F^T}{n} \;,$$
and 
$$V = \frac{1}{n}{\act{\rat{W_t}{d}Y}Z^T} = \rat{F}{n}\rat{Z^T}{n} \;.$$ Thus we get the optimal $A_t$ as
\begin{equation}
    \rat{\hat{A}_t}{p} = -\frac{1}{\sqrt{h_t}}V^T(U+\lambda I_p)^{-1} = -\frac{1}{\sqrt{h_t}}\rat{Z}{n}\rat{F^T}{n}\left(\rat{F}{n}\rat{F^T}{n}+\lambda I_p\right)^{-1}.
\end{equation}
\textbf{Test error:}
%\begin{align*}
%    \cE^1_{\text{test}}(\hat{A}_t) &= \frac{1}{d}\bE{x\sim P_t}{\norm{\rat{\hat{A}_t}{p}\act{\rat{W_t}{d}x}-\nabla\log P_t(x)}^2}\\
%    &= \frac{1}{d}\bE{x\sim P_t}{\norm{\rat{\hat{A}_t}{p}\act{\rat{W_t}{d}x}+x}^2}\\
%    &=1 + \frac{2}{d}\tr{\rat{\hat{A}_t}{p}\underbrace{\bE{x}{\act{\rat{W_t}{d}x}x^T}}_{:=\Tilde{V}}}\\
%    &\qquad+\frac{1}{d}\tr{\rat{\hat{A}^T_t}{p}\rat{\hat{A}_t}{p}\underbrace{\bE{x}{\act{\rat{W_t}{d}x}\act{\rat{W_t}{d}x}^T}}_{:=\Tilde{U}}}\;.
%\end{align*}
\begin{align*}
    \cE^1_{\text{test}}(\hat{A}_t) &= \frac{1}{d}\bE{x\sim P_t}{\norm{\rat{\hat{A}_t}{p}\act{\rat{W_t}{d}x}-\nabla\log P_t(x)}^2}\\
    &= \frac{1}{d}\bE{x\sim P_t}{\norm{\rat{\hat{A}_t}{p}\act{\rat{W_t}{d}x}+x}^2}\\
    &=1 + \frac{2}{d}\tr{\rat{\hat{A}_t}{p}\underbrace{\bE{x}{\act{\rat{W_t}{d}x}x^T}}_{:=\Tilde{V}}} +\frac{1}{d}\tr{\rat{\hat{A}^T_t}{p}\rat{\hat{A}_t}{p}\underbrace{\bE{x}{\act{\rat{W_t}{d}x}\act{\rat{W_t}{d}x}^T}}_{:=\Tilde{U}}}\;.
\end{align*}

We have already derived expressions for $\Tilde{V}$ and $\Tilde{U}$ in Section~\ref{appndx:lc_minf_proof}. We have $\Tilde{V} = \mu_1\rat{W}{d}$,\; $\Tilde{U} = \mu_0^2\textbf{1}\textbf{1}^T+ \mu_1^2\rat{W}{d}\rat{W^T}{d}+v^2I_p$. For simplicity, we assume $\mu_0=0$. Thus:
\begin{align*}
    \cE^1_{\text{test}}(\hat{A}_t) &= \frac{1}{d}\bE{x\sim P_t}{\norm{\rat{\hat{A}_t}{p}\act{\rat{W_t}{d}x}+x}^2}\\
    &=1 - \frac{2\mu_1}{\sqrt{h_t}d}\tr{\rat{Z}{n}\rat{F^T}{n}\left(\rat{F}{n}\rat{F^T}{n}+\lambda I_p\right)^{-1}\rat{W}{d}}\\
    &\qquad+\frac{\mu_1^2}{h_td}\tr{\left(\rat{F}{n}\rat{F^T}{n}+\lambda I_p\right)^{-1}\rat{F}{n}\rat{Z^T}{n}\rat{Z}{n}\rat{F^T}{n}\left(\rat{F}{n}\rat{F^T}{n}+\lambda I_p\right)^{-1}\rat{W}{d}\rat{W^T}{d}}\\
    &\qquad+\frac{v^2}{h_td}\tr{\left(\rat{F}{n}\rat{F^T}{n}+\lambda I_p\right)^{-1}\rat{F}{n}\rat{Z^T}{n}\rat{Z}{n}\rat{F^T}{n}\left(\rat{F}{n}\rat{F^T}{n}+\lambda I_p\right)^{-1}} \;.
\end{align*}
Since we focus on a single time instant, we have dropped the subscript $t$ in the above expressions. However, it is important to keep the time-dependence of $a$ and $h$, and the relation $a^2+h=1$. Also, we use Gaussian equivalence principle to handle the non-linearity in $F$. In particular, we use the following expression for $F$:
\begin{equation*}
    F = \mu_1\rat{W}{d}Y+v\Omega\;.
\end{equation*}
Let 
\begin{equation}
    R(q,z) = \left(\rat{F}{n}\rat{F^T}{n}+q\rat{W}{d}\rat{W^T}{d}-zI_p\right)^{-1} \;,
\end{equation}
and
\begin{align*}
    E_1 &= \frac{1}{d}\tr{\rat{Z}{n}\rat{F^T}{n}R(0,-\lambda)\rat{W}{d}}\;,\\
    E_2 &= \frac{1}{d}\tr{R(0,-\lambda)\rat{F}{n}\rat{Z^T}{n}\rat{Z}{n}\rat{F^T}{n}R(0,-\lambda)\rat{W}{d}\rat{W^T}{d}}\;,\\
    E_3 &= \frac{1}{d}\tr{\rat{F}{n}\rat{Z^T}{n}\rat{Z}{n}\rat{F^T}{n}R(0,-\lambda)^2}\;.
\end{align*}
With above definitions, we can write $\cE^1_{\text{test}}$ as 
\begin{equation*}
   \cE^1_{\text{test}}(\hat{A}_t) = 1-\frac{2\mu_1}{\sqrt{h}}E_1+\frac{\mu_1^2}{h}E_2+\frac{v^2}{h}E_3 \;.
\end{equation*}
Letting
\begin{align*}
    K(q,z) = \frac{1}{d}\tr{R(q,z)\rat{F}{n}\rat{Z^T}{n}\rat{Z}{n}\rat{F^T}{n}} \;,
\end{align*}
we have
\begin{align*}
    E_2 &= -\frac{\dd K}{\dd q}(0,-\lambda) \;,\\
    E_3 &= \frac{\dd K}{\dd z}(0,-\lambda) \;.
\end{align*}

Therefore, assuming we can interchange limit, derivatives and expectations, it suffices to have $e_1 = \lim_{d\to\infty}\bE{}{E_1}$ and the function $\cK(q,z):=\lim_{d\to\infty}\bE{}{K(q,z)}$ to have an expression for $\lim_{d\to\infty}\bE{}{\cE^1_{\text{test}}(\hat{A}_t)}$. We have
\begin{align}\label{eqn:Etest_m1}
    \lim_{d\to\infty}\bE{}{\cE^1_{\text{test}}(\hat{A}_t)} &= 1-\frac{2\mu_1}{\sqrt{h}}\lim_{d\to\infty}\bE{}{E_1}+\frac{\mu_1^2}{h}\lim_{d\to\infty}\bE{}{E_2}+\frac{v^2}{h}\lim_{d\to\infty}\bE{}{E_3} \; ,\nonumber\\
    &= 1-\frac{2\mu_1}{\sqrt{h}}e_1+\frac{\mu_1^2}{h}e_2+\frac{v^2}{h}e_3 \; ,
\end{align}

where we define $e_2 = -\frac{\partial \cK}{\partial q}(0,-\lambda),\; e_3 = \frac{\partial \cK}{\partial z}(0,-\lambda)$. As in the previous theorem one could avoid the interchange of limit of expectation and derivatives by differentiating first $K(q,z)$ and using a larger linear pencil.

As in the proof of Theorem~\ref{thm:lc_minf} given in Appendix~\ref{appndx:lc_minf_proof}, we use linear pencils to obtain the desired terms. Explicitly, the following $6\times 6$ linear pencil matrix:
\begin{equation*}
    L = \mleft[
        \begin{array}{c|ccccc}
        -zI_p & q\rat{W}{d} & v_0\rat{\Omega}{n} & \mu_1\rat{W}{d} & 0 & 0\\
        \hline
        -\rat{W^T}{d} & I_d & 0 & 0 & 0 & 0\\
        -v\rat{\Omega^T}{n} & -\mu\rat{Y^T}{n} & I_n & 0 & 0 & 0\\
        0 & 0 & -\rat{Y}{n} & I_d & 0 & 0\\
        0 & 0 & -\rat{Z}{n} & 0 & I_d & 0\\
        0 & 0 & 0 & 0 & -\rat{Z^T}{n} & I_n\\
    \end{array}
    \mright] = \mleft[\begin{array}{cc}
        L_{11} & L_{12} \\
        L_{21} & L_{22}
    \end{array}\mright] \;.
\end{equation*}
By computing $L^{-1}$, we obtain:
\begin{align}
    (L^{-1})^{5,4}(q,z) &= -\mu_1\rat{Z}{n}\rat{F^T}{n}R(q,z)\rat{W}{d} \;,\\
    (L^{-1})^{6,3}(q,z) &= -\rat{Z^T}{n}\rat{Z}{n}\rat{F^T}{n}R(q,z)\rat{F}{n}+\rat{Z^T}{n}\rat{Z}{n} \;.
\end{align}
Hence,
\begin{align}
    E_1 &= \frac{1}{d}\tr{\rat{Z}{n}\rat{F^T}{n}R(0,-\lambda)\rat{W}{d}} = -\frac{1}{\mu_1 d}\tr{(L^{-1})^{5,4}(0,-\lambda)}\;,\\
    K(q,z) &= \frac{1}{d}\tr{R(q,z)\rat{F}{n}\rat{Z^T}{n}\rat{Z}{n}\rat{F^T}{n}} = 1-\frac{1}{d}\tr{(L^{-1})^{6,3}(q,z)}\;. 
\end{align}
We compute the traces of blocks in $L^{-1}$ as in the proof of Theorem~\ref{thm:lc_minf} given in Appendix~\ref{appndx:lc_minf_proof}. We have 
%\begin{equation*}
%    g = \mleft[
%    \begin{array}{cccccc}
%        g_{11} & 0 & 0 & 0 & 0 & 0 \\
%        0 & g_{22} & 0 & g_{24} & 0 & 0 \\
%        0 & 0 & g_{33} & 0 & 0 & 0 \\
%        0 & g_{42} & 0 & g_{44} & 0 & 0 \\
%        0 & g_{52} & 0 & g_{54} & 1 & 0 \\
%        0 & 0 & g_{63} & 0 & 0 & 1  \\
%    \end{array}
%    \mright]\;,
%\end{equation*}
%\begin{equation*}
%    B = \mleft[
%    \begin{array}{cccccc}
%        -z & 0 & 0 & 0 & 0 & 0 \\
%        0 & 1 & 0 & 0 & 0 & 0 \\
%        0 & 0 & 1 & 0 & 0 & 0 \\
%        0 & 0 & 0 & 1 & 0 & 0 \\
%        0 & 0 & 0 & 0 & 1 & 0 \\
%        0 & 0 & 0 & 0 & 0 & 1  \\
%    \end{array}
%    \mright] \;,
%\end{equation*}
\begin{alignat*}{2}
  g=&\mleft[
    \begin{array}{cccccc}
        g_{11} & 0 & 0 & 0 & 0 & 0 \\
        0 & g_{22} & 0 & g_{24} & 0 & 0 \\
        0 & 0 & g_{33} & 0 & 0 & 0 \\
        0 & g_{42} & 0 & g_{44} & 0 & 0 \\
        0 & g_{52} & 0 & g_{54} & 1 & 0 \\
        0 & 0 & g_{63} & 0 & 0 & 1  \\
    \end{array}
    \mright] \;,  \quad  B=&&  \mleft[
    \begin{array}{cccccc}
        -z & 0 & 0 & 0 & 0 & 0 \\
        0 & 1 & 0 & 0 & 0 & 0 \\
        0 & 0 & 1 & 0 & 0 & 0 \\
        0 & 0 & 0 & 1 & 0 & 0 \\
        0 & 0 & 0 & 0 & 1 & 0 \\
        0 & 0 & 0 & 0 & 0 & 1  \\
    \end{array}
    \mright] \;,
\end{alignat*}
\begin{equation*}
    \eta_L(g) = \mleft[
    \begin{array}{cccccc}
        \sigma_{12}^{21}g_{22}+\sigma_{13}^{31}g_{33}+\sigma_{14}^{21}g_{42} & 0 & 0 & 0 & 0 & 0 \\
        0 & \sigma_{21}^{12}g_{11} & 0 & \sigma_{21}^{14}g_{11} & 0 & 0 \\
        0 & 0 & \sigma_{31}^{13}g_{11}+\sigma_{32}^{43}g_{24} & 0 & 0 & 0 \\
        0 & \sigma_{43}^{32}g_{33} & 0 & 0 & 0 & 0 \\
        0 & \sigma_{53}^{32}g_{33} & 0 & 0 & 0 & 0 \\
        0 & 0 & \sigma_{65}^{43}g_{54}+\sigma_{65}^{53} & 0 & 0 & 0  \\
    \end{array}
    \mright] \;,
\end{equation*}
and
%\begin{align}
%    & \sigma_{12}^{21} = -q,\qquad \sigma_{21}^{12}= -q\psi_p,\qquad \sigma_{21}^{14}= -\mu_1\psi_p,\\
%    & \sigma_{13}^{31} = -v^2,\qquad \sigma_{31}^{13} = -v^2\psi_p/\psi_n,\qquad \sigma_{32}^{43} = \mu_1/\psi_n,\qquad \sigma_{43}^{32} = \mu_1,\\
%    & \sigma_{14}^{21} = -\mu_1,\qquad \sigma_{53}^{32} = \mu_1\sqrt{h},\\
%    & \sigma_{65}^{43} = \sqrt{h}/\psi_n,\qquad \sigma_{65}^{53}=1/\psi_n.\\
%\end{align}
\begin{alignat}{4}
  & \sigma_{12}^{21} = -q \;,&&\quad\quad \sigma_{21}^{12}= -q\psi_p \;,&&\quad\quad \sigma_{21}^{14}= -\mu_1\psi_p \;,&& \\
  &\sigma_{13}^{31} = -v^2 \;,&&\quad\quad \sigma_{31}^{13} = -v^2\psi_p/\psi_n \;,&&\quad\quad \sigma_{32}^{43} = \mu_1/\psi_n \;, &&\quad\quad  \sigma_{43}^{32} = \mu_1 \;, \\
  &\sigma_{14}^{21} = -\mu_1 \;,&&\quad\quad \sigma_{53}^{32} = \mu_1\sqrt{h} \;,&&  &&  \\
  &\sigma_{65}^{43} = \sqrt{h}/\psi_n \;,&&\quad\quad  \sigma_{65}^{53}=1/\psi_n \;.&& &&   
\end{alignat}
We have that $g$ satisfies the following fixed point equation
\begin{equation*}
    (B-\eta_L(g))g = I \;.
\end{equation*}
Then:
\begin{multline*}
    \mleft[
    \begin{array}{cccccc}
        -z+qg_{22}+v^2g_{33}+\mu_1g_{42} & 0 & 0 & 0 & 0 & 0  \\
        0 & 1+q\psi_p g_{11} & 0 & \mu_1\psi_p g_{11} & 0 & 0  \\
        0 & 0 & 1+v^2\psi_p/\psi_ng_{11}-\mu_1/\psi_ng_{24} & 0 & 0 & 0  \\
        0 & -\mu_1g_{33} & 0 & 1 & 0 & 0  \\
        0 & -\mu_1\sqrt{h}g_{33} & 0 & 0 & 1 & 0  \\
        0 & 0 & -\sqrt{h}/\psi_n g_{54}-1/\psi_n & 0 & 0 & 1  \\
    \end{array}
    \mright]\\
    \times
    \mleft[
    \begin{array}{cccccc}
        g_{11} & 0 & 0 & 0 & 0 & 0 \\
        0 & g_{22} & 0 & g_{24} & 0 & 0 \\
        0 & 0 & g_{33} & 0 & 0 & 0 \\
        0 & g_{42} & 0 & g_{44} & 0 & 0 \\
        0 & g_{52} & 0 & g_{54} & 1 & 0 \\
        0 & 0 & g_{63} & 0 & 0 & 1  \\
    \end{array}
    \mright] = I\;.
\end{multline*}
This gives the following set of equations:
\begin{align*}
    g_{11}(-z+qg_{22}+v^2g_{33}+\mu_1g_{42}) &= 1 \;,\\
    g_{22}(1+q\psi_p g_{11}) +\mu_1\psi_pg_{11}g_{42} &= 1 \;,\\
    g_{24}(1+q\psi_p g_{11}) +\mu_1\psi_pg_{11}g_{44} &= 0 \;,\\
    g_{33}(1+v^2\psi_p/\psi_ng_{11}-\mu_1/\psi_ng_{24}) &= 1 \;,\\
    -\mu_1g_{22}g_{33}+g_{42} &= 0 \;,\\
    -\mu_1g_{24}g_{33}+g_{44} &= 1 \;,\\
    -\mu_1\sqrt{h}g_{33}g_{22}+g_{52} &= 0 \;,\\
    -\mu_1\sqrt{h}g_{33}g_{24}+g_{54} &= 0 \;,\\
    g_{33}(-\sqrt{h}/\psi_n g_{54}-1/\psi_n)+g_{63} &= 0 \;.
\end{align*}
The above set of 9 equations can be reduced to 6 equations:
\begin{align*}
    \zeta_1(-z+q\zeta_2+v^2\zeta_4+\mu_1^2\zeta_2\zeta_4) -1&= 0 \;,\\
    \zeta_2(1+q\psi_p \zeta_1) +\mu_1^2\psi_p\zeta_1\zeta_2\zeta_4 -1&= 0 \;,\\
    \zeta_3(1+q\psi_p \zeta_1) +\mu_1\psi_p\zeta_1(1+\mu_1\zeta_3\zeta_4) &= 0 \;,\\
    \zeta_4(\psi_n+v^2\psi_p\zeta_1-\mu_1\zeta_3)-\psi_n &= 0 \;,\\
    \zeta_4(-h_t\mu_1\zeta_3\zeta_4-1)+\psi_n\zeta_5 &= 0 \;, 
\end{align*}
where $\zeta_1=g_{11},\; \zeta_2=g_{22},\; \zeta_3=g_{24},\; \zeta_4=g_{33},\; \zeta_5=g_{63}$.
As earlier, we solve the above set of equations numerically to obtain $\zeta_1,\zeta_2,\zeta_3,\zeta_4,$ and $\zeta_5$. We obtain $e_1, \cK$ by using the expressions $e_1 = -\sqrt{h_t}\zeta_3(0,-\lambda)\zeta_4(0,-\lambda)$, $\cK(q,z)=-\sqrt{h_t}\zeta_3(q,z)\zeta_4(q,z)$. Finally, we use \eqref{eqn:Etest_m1} to compute $\lim_{d\to\infty}\bE{}{\cE^1_{\text{test}}(\hat{A}_t)}$.\\\\
\textbf{Train error:}
To compute train error, we proceed as follows:
\begin{align*}
    \cE^1_{\text{train}}(\hat{A}_t)    &=\frac{h}{d}\tr{\rat{\hat{A}_t}{p}^T\rat{\hat{A}_t}{p} (U+\lambda I_p)}+\frac{2\sqrt{h}}{d}\tr{\rat{\hat{A}_t}{p} V}+1-\frac{h\lambda}{d}\tr{\rat{\hat{A}_t}{p}^T\rat{\hat{A}_t}{p}}\\
    &=\frac{1}{d}\tr{(U+\lambda I_p)^{-1}VV^T}-\frac{2}{d}\tr{V^T(U+\lambda I_p)^{-1}V}+1-\frac{\lambda}{d}\tr{(U+\lambda I_p)^{-2}VV^T}\\
    &=-\frac{1}{d}\tr{(U+\lambda I_p)^{-1}\rat{F}{n}\rat{Z^T}{n}\rat{Z}{n}\rat{F^T}{n}}+1-\frac{\lambda}{d}\tr{(U+\lambda I_p)^{-2}\rat{F}{n}\rat{Z^T}{n}\rat{Z}{n}\rat{F^T}{n}}\\
    &=-K(0,-\lambda)-\lambda\frac{\partial K}{\partial z}(0,-\lambda)+1 \;.
\end{align*}
Thus,
\begin{align}\label{eqn:Etrain_m1}
    \lim_{d\to\infty}\bE{}{\cE^1_{\text{train}}(\hat{A}_t)} &= 1-\cK(0,-\lambda)-\lambda\frac{\partial \cK}{\partial z}(0,-\lambda) \;,\nonumber\\
    &= 1-\cK(0,-\lambda)-\lambda e_3 \;,
\end{align}
where $e_3 = \frac{\partial \cK}{\partial z}(0,-\lambda)$.
This concludes the proof of Theorem~\ref{thm:lc_m1}.
\section{Illustrations of Analytically Computed Test and Train Errors}
In this section, we provide additional plots to further illustrate the analytical predictions of test and train errors derived from Theorems~\ref{thm:lc_minf} and \ref{thm:lc_m1}. First, we present the learning curves for the case of $m=1$, which were omitted from the main text due to space constraints. Next, we include a plot demonstrating the impact of the regularization strength $\lambda$ on the learning curves. Finally, we provide learning curves for cases where different activation functions are employed, highlighting their influence on the model's performance. 

\subsection{Test and train errors for $m=1$}\label{appndx:learning_curves_m1}
Fig.~\ref{fig:lc_m1} shows test and train errors for $m=1$ case. Fig.~\ref{fig:lc_m1_nfixed} presents the learning curves as a function of $t$ for different values of $\psi_p$ and while keeping $\psi_n$ fixed. In Fig.~\ref{fig:lc_m1_tfixed} we plot the learning curves as a function of $\psi_p$ for different values of $\psi_n$ and keeping $t$ fixed and small.

As already discussed in the main text, the learning curves reveal several trends. The train error decreases monotonically with increasing $\psi_p$ for all $t$, reflecting the model's capacity to interpolate the training data. However, the test error shows a non-monotonic behavior with $\psi_p$. The dependence of test error on $t$ is also evident. The test error increases as $t$ decreases, but for small $t$, it remains at least two orders of magnitude lower than in the $m=\infty$ case. Furthermore, the test error decreases  as $\psi_p$ increases beyond $\psi_n$. This suggests that the model does not display memorization behavior under these conditions. This is in contrast to the $m=\infty$ case, where memorization significantly impacts the test error. These findings indicate that larger values of $m$ increases the tendency of diffusion models to memorize the initial dataset.

Lastly, as observed in the earlier works such as \cite{mei_generalization_2022, bodin_model_2021, hu_asymptotics_2024}, we also observe the double descent phenomenon which is characterized by peak in test error at $\psi_p=\psi_n$. This is depicted in Fig.~\ref{fig:lc_m1_tfixed}.
\begin{figure}
     \centering
     \begin{subfigure}[b]{1\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/rfm_err_analytical_lam0p001_relu_nfixed_m1.pdf}
         \caption{Test and train error as a function of $t$ for different values of $\psi_p$ and a fixed $\psi_n=20.0$.}
    \label{fig:lc_m1_nfixed}
     \end{subfigure}
     \hfill
     
     \begin{subfigure}[b]{1\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/rfm_err_analytical_lam0p001_relu_tfixed_m1.pdf}
         \caption{Test and train error as a function of $\psi_p$ for different values of $\psi_n$ and a fixed $t = 0.01$. The dashed vertical lines indicate $\psi_p=\psi_n$.}
    \label{fig:lc_m1_tfixed}
     \end{subfigure}
     \hfill
        \caption{Learning curves for $m=1$.We used $\lambda = 0.001$ and the activation function used is ReLU.}
        \label{fig:lc_m1}
\end{figure}

\subsection{Plots for different values of $\lambda$}\label{appndx:lc_lambda}

\begin{figure}
     \centering
     \begin{subfigure}[b]{1\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/appndx_figs/rfm_err_analytical_lam_nfixed_minf.pdf}
         \caption{Test and train error as a function of $t$ for different values of $\lambda$ for a fixed $\psi_n=20.0$ and $\psi_p = 1024.0$.}
    \label{fig:lc_lambda_minf_nfixed}
     \end{subfigure}
     \hfill
     
     \begin{subfigure}[b]{1\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/appndx_figs/rfm_err_analytical_lam_tfixed_minf.pdf}
         \caption{Test and train error as a function of $\psi_p$ for different values of $\lambda$ for a fixed $\psi_n=64.0$ and $t = 0.01$. The dashed vertical line indicates $\psi_p=\psi_n$.}
    \label{fig:lc_lambda_minf_tfixed}
     \end{subfigure}
     \hfill
        \caption{Learning curves for different values of $\lambda$ for $m=\infty$. The activation function used is ReLU.}
        \label{fig:lc_lambda_minf}
\end{figure}

\begin{figure}
     \centering
     \begin{subfigure}[b]{1\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/appndx_figs/rfm_err_analytical_lam_nfixed_m1.pdf}
         \caption{Test and train error as a function of $t$ for different values of $\lambda$ for a fixed $\psi_n=20.0$ and $\psi_p = 1024.0$.}
    \label{fig:lc_lambda_m1_nfixed}
     \end{subfigure}
     \hfill
     
     \begin{subfigure}[b]{1\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/appndx_figs/rfm_err_analytical_lam_tfixed_m1.pdf}
         \caption{Test and train error as a function of $\psi_p$ for different values of $\lambda$ for a fixed $\psi_n=64.0$ and $t = 0.01$. The dashed vertical line indicates $\psi_p=\psi_n$.}
    \label{fig:lc_lambda_m1_tfixed}
     \end{subfigure}
     \hfill
        \caption{Learning curves for different values of $\lambda$ for $m=1$. The activation function used is ReLU.}
        \label{fig:lc_lambda_m1}
\end{figure}

In this section, we illustrate the behavior of the test and train errors for different values of $\lambda$. Fig.~\ref{fig:lc_lambda_minf} shows the learning errors for $m=\infty$. We see that, as $\lambda$ increases, the train error increases and the test error decreases for small $t$. This can be explained as follows: In this regime, we have $s^e(t,a_tx_i+\sqrt{h_t}z)\approx -\frac{z}{\sqrt{h_t}}$. However, as regularization increases, $\hat{A}_t$ cannot take very large values. So, $\cM_t$ scales as $\frac{1}{h_t}$. This makes $\cE^\infty_{\text{train}}(\hat{A}_t)\approx 1$ for very small values of $t$. This leads to lower memorization as well, as evidenced by the small test error.

Fig.~\ref{fig:lc_lambda_m1} shows the learning errors for $m=1$. In this case, larger values of $\lambda$ decreases the peak at $\psi_p=\psi_n$ due to double descent. This is expected since the regularization will help in reducing overfitting.

\subsection{Plots for different activation functions}\label{appndx:lc_activation}

%In this section, we illustrate the behavior of the learning curves for different activation functions. We compare three different activation functions: relu, tanh, and sigmoid. We introduces a shift and scaling to these functions inorder for them to have $\mu_0=0$ and have same $L_2$ norm. Concretely, these functions are given by:
In this section, we illustrate the behavior of the learning curves for different activation functions: ReLU, tanh, and sigmoid. To make them have the same $L_2$ norm and $\mu_0=0$, we introduce proper shiftings and rescalings. Concretely:
\begin{align*}
    \text{ReLU}(x) &= x\mathds{1}\{x\ge 0\}-\frac{1}{\sqrt{2\pi}} \;, \\
    \text{tanh}(x) &= 0.93\left(\frac{e^x-e^{-x}}{e^x+e^{-x}}\right) \;,\\
    \text{sigmoid}(x) &= \frac{2.8}{1+e^{-x}}-1.4  \;.
\end{align*}
Fig.~\ref{fig:lc_act_minf} displays the results for $m=\infty$. The activation function enters the analysis through the coefficients $\mu_1, v, v_0,$ and $s$. 

Fig.~\ref{fig:lc_act_m1} presents the case $m=1$.

\begin{figure}
     \centering
     \begin{subfigure}[b]{1\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/appndx_figs/rfm_err_analytical_nl_nfixed_minf.pdf}
         \caption{Test and train error as a function of $t$ for different activation functions for a fixed $\psi_n=20.0$ and $\psi_p = 1024.0$.}
    \label{fig:lc_act_minf_nfixed}
     \end{subfigure}
     \hfill
     
     \begin{subfigure}[b]{1\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/appndx_figs/rfm_err_analytical_nl_tfixed_minf.pdf}
         \caption{Test and train error as a function of $\psi_p$ for different activation functions for a fixed $\psi_n=64.0$ and $t = 0.01$. The dashed vertical line indicates $\psi_p=\psi_n$.}
    \label{fig:lc_act_minf_tfixed}
     \end{subfigure}
     \hfill
        \caption{Learning curves for different activation functions for $m=\infty$. We used $\lambda=0.001$.}
        \label{fig:lc_act_minf}
\end{figure}

\begin{figure}
     \centering
     \begin{subfigure}[b]{1\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/appndx_figs/rfm_err_analytical_nl_nfixed_m1.pdf}
         \caption{Test and train error as a function of $t$ for different activation functions for a fixed $\psi_n=20.0$ and $\psi_p = 1024.0$.}
    \label{fig:lc_act_m1_nfixed}
     \end{subfigure}
     \hfill
     
     \begin{subfigure}[b]{1\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/appndx_figs/rfm_err_analytical_nl_tfixed_m1.pdf}
         \caption{Test and train error as a function of $\psi_p$ for different activation functions for a fixed $\psi_n=64.0$ and $t = 0.01$. The dashed vertical line indicates $\psi_p=\psi_n$.}
    \label{fig:lc_act_m1_tfixed}
     \end{subfigure}
     \hfill
        \caption{Learning curves for different activation functions for $m=1$. We used $\lambda=0.001$.}
        \label{fig:lc_act_m1}
\end{figure}

\section{Comparison with Numerically Obtained Learning Curves}\label{appndx:lc_numerical_comparison}
Here, we show the learning curves obtained numerically for different values of $m$. The test error was illustrated in Fig.~\ref{fig:lc_sim}. In Fig.~\ref{fig:lc_sim_comp_appndx} we exhibit the corresponding train errors as well.
%\begin{figure*}[ht]
%    \centering
%    \includegraphics[width=1.0\linewidth]{figs/rfm_err_sim_lam0p001.pdf}
%    \caption{Learning curves obtained numerically for $\psi_p=20.0$. We used $d=100$ to get the numerical results. The upper row is for $\psi_n=2.0$ and the lower row is for $\psi_n=50.0$.}
%    \label{fig:lc_sim_comp_appndx}
%\end{figure*}
\begin{figure*}[ht]
    \centering
    \includegraphics[width=1.0\linewidth]{figs/rfm_err_sim_lam0p001.pdf}
    \caption{Simulation results ($d=100$) for different values of $m$ and fixed $\psi_p=20$; with $\psi_n=2$ (upper plots) and $\psi_n=50$ (lower plots). Theoretical results for $m=1$ and $m=\infty$ are depicted as continuous lines.}
    \label{fig:lc_sim_comp_appndx}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\iffalse
\newpage
\input{sections/denoiser}
\fi

%\begin{figure}[ht]
%    \centering
%    \includegraphics[width=1.0\linewidth]{figs/rfm_err_sim_lam0p001_test.pdf}
%    \caption{Simulation results ($d=100$) for different values of $m$ and fixed $\psi_p=20$; with $\psi_n=2$ (upper plot) and $\psi_n=50$ (lower plot). Theoretical results for $m=1$ and $m=\infty$ are depicted as continuous lines.}
%    \label{fig:lc_sim}
%\end{figure}