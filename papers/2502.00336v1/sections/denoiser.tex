\section{Test and train error when learning denoiser}

\subsection{Random Features model}
In this section, we assume $s_\theta(t,x)=\rat{A_t}{p}\varrho(\rat{W_t}{d}x)$, $W_t\in\R^{p\times d}$, $A_t\in\R^{d\times p}$, where the elements of $W_t$ are distributed as $\cN{0,1}$, and $x_i\sim\pi\equiv\cN{0,I_d}$, $z_{ij}\sim\cN{0,I_d}$.
\begin{align*}
    \hat{\cL}_t(A_t) &= \frac{1}{nm}\sum_{i=1}^{n}{\sum_{j=1}^{m}\norm{\rat{A_t}{p}\varrho(\rat{W_t}{d}(a_tx_i+\sqrt{h_t}z_{ij})-x_i}^2}+\frac{\lambda}{p}\norm{A_t}_F^2,\\
    &= \frac{1}{nm}\sum_{i=1}^{n}{\norm{\rat{A_t}{p}\varrho(\rat{W_t}{d}(a_tx_i\textbf{1}^T+\sqrt{h_t}Z_i)-x_i\textbf{1}^T}_F^2}+\frac{\lambda}{p}\norm{A_t}_F^2,\\
    &=\textbf{tr}\{\rat{A_t^T}{p}\rat{A_t}{p} (U+\lambda I_p)\}-2\textbf{tr}\{\rat{A_t}{p} V\}+d,\\
\end{align*}
where $Z_i = [z_{i1},z_{i2},\cdots,z_{im}]$,
$$U = \frac{1}{nm}\sum_{i=1}^n{\varrho(\rat{W_t}{d}(a_tx_i\textbf{1}^T+\sqrt{h_t}Z_i))\varrho(\rat{W_t}{d}(a_tx_i\textbf{1}^T+\sqrt{h_t}Z_i))^T}$$ and 
$$V = \frac{1}{nm}\sum_{i=1}^n{\varrho(\rat{W_t}{d}(a_tx_i\textbf{1}^T+\sqrt{h_t}Z_i))(x_i\textbf{1}^T)^T}.$$
Thus we get the optimal $A_t$ as
\begin{equation}
    \rat{\hat{A}_t}{p} = V^T (U+\lambda I_p)^{-1}.
\end{equation}
We now consider two extreme cases: when $m=1$ and $m=\infty$.\\
\subsubsection{Case: $m=\infty$}
When $m=\infty$, the loss function reduces as follows:
\begin{align}
    \cL_t(s) &= \frac{1}{n}\sum_{i=1}^{n}{\bE{z\sim\cN{0,I_d}}{\norm{s(t,a_tx_i+\sqrt{h_t}z)-x_i}^2}}.
\end{align}
We have $s_\theta(t,x)=\rat{A_t}{p}\varrho(\rat{W_t}{d}x)$, $W_t\in\R^{p\times d}$, $A_t\in\R^{d\times p}$.
\begin{align*}
    \cL_t(A_t) &=\frac{1}{n}\sum_{i=1}^{n}{\bE{z}{\norm{\rat{A_t}{p}\varrho(\rat{W_t}{p}(a_tx_i+\sqrt{h_t}z))-x_i}^2}}+\frac{\lambda}{p}\norm{A_t}_F^2,\\
    &=\tr{\rat{A_t}{p}^T\rat{A_t}{p} U}-2\tr{\rat{A_t}{p} V}+d+\lambda\tr{\rat{A_t^T}{p}\rat{A_t}{p}},\\
\end{align*}
where 
$$U = \frac{1}{n}\sum_{i=1}^n\bE{z}{\varrho(\rat{W_t}{d}(a_tx_i+\sqrt{h_t}z))\varrho(\rat{W_t}{d}(a_tx_i+\sqrt{h_t}z))^T}$$
and 
$$V = \frac{1}{n}\sum_{i=1}^n\bE{z}{\varrho(\rat{W_t}{d}(a_tx_i+\sqrt{h_t}z))x_i^T}.$$ Thus we get the optimal $A_t$ as
\begin{equation}
    \rat{\hat{A}_t}{p} = V^T(U+\lambda I_p)^{-1}.
\end{equation}
Now, we compute test error (generalization error) and train error when $\pi\equiv\cN{0,I_d}$. We note that $P_t\equiv\cN{0,1}$.\\
\textbf{Test Error:}
\begin{align*}
    \cE_{\text{test}} &= \frac{1}{d}\bE{x\sim P_t}{\norm{\rat{\hat{A}_t}{p}\varrho(\rat{W_t}{d}x)-ax}^2}\\
    &=a^2 - \frac{2a}{d}\tr{V^T (U+\lambda I_p)^{-1}\underbrace{\bE{x}{\varrho(\rat{W_t}{d}x)x^T}}_{:=\Tilde{V}}}\\
    &\qquad+\frac{1}{d}\tr{(U+\lambda I_p)^{-1}V V^T(U+\lambda I_p)^{-1}\underbrace{\bE{x}{\varrho(\rat{W_t}{d}x)\varrho(\rat{W_t}{d}x)^T}}_{:=\Tilde{U}}}
\end{align*}
Since we focus on a single time instant, we drop the subscript $t$ in the above expressions. However, we will keep in mind that $a$ and $h$ depend on $t$ and we have the relation $a^2+h=1$.

We need to compute $V,U,\Tilde{V},\Tilde{U}$ in order to get an expression for $\cE_{\text{test}}$. We will first consider $\Tilde{V}$.

\begin{equation*}
    \Tilde{V} = \bE{x}{\varrho(\rat{W}{d}x)x^T}
\end{equation*}
Let $w_i$ denote the $i^{\text{th}}$ row of $W$. Let $P_\gamma$ denote the bivariate standard Gaussian distribution with correlation coefficient $\gamma$. We have 
\begin{align*}
    \Tilde{V}_{ij} &= \bE{x}{\varrho(\rat{w_i^Tx}{d})x_j}\\
    &= \bE{(u,v)\sim P_{\rat{w_{ij}}{d}}}{\varrho(u)v}\\
    &\stackrel{(a)}{=} \sum_{k=0}^{\infty} \frac{(\rat{w_{ij}}{d})^k}{k!}\bE{u}{\varrho(u)\text{He}_k(u)}\bE{v}{v\text{He}_k(v)}\\
    &= \mu_1\rat{w_{ij}}{d},
\end{align*}
where in $(a)$ we used the Mehler Kernel formula.
Hence, we have $\Tilde{V} = \mu_1\rat{W}{d}$. Now, we consider $\Tilde{U}$.
\begin{align*}
    \Tilde{U}_{ij} &= \bE{x}{\varrho(\rat{w_i^Tx}{d})\varrho(\rat{w_j^Tx}{d})}\\
    &= \bE{(u,v)\sim P_{\frac{w_i^Tw_j}{d}}}{\varrho(u)\varrho(v)}\\
    &= \sum_{k=0}^{\infty} \frac{(\frac{w_i^Tw_j}{d})^k}{k!}\bE{u}{\varrho(u)\text{He}_k(u)}\bE{u}{\varrho(v)\text{He}_k(v)}\\
    &= \sum_{k=0}^{\infty} \frac{(\frac{w_i^Tw_j}{d})^k}{k!}\bE{u}{\varrho(u)\text{He}_k(u)}^2.
\end{align*}
Let $\mu_0 = \bE{g}{\varrho(g)}, \mu_1 = \bE{g}{\varrho(g)g}, \norm{\varrho}^2 = \bE{g}{\varrho(g)^2}$. Then, we have
\begin{equation*}
    \Tilde{U}_{ij} = \begin{cases}
        \mu_0^2+\mu_1^2 \frac{w_i^Tw_j}{d} + O(1/d)\quad &\text{if } i\neq j,\\
        \norm{\varrho}^2\quad &\text{if } i=j.
    \end{cases}
\end{equation*}
Let $v^2 = \norm{\rho}^2-\mu_0^2-\mu_1^2$. Neglecting $O(1/d)$ terms, we have
\begin{equation*}
    \Tilde{U} = \mu_0^2 \bone\bone^T + \mu_1^2 \rat{W}{d}\rat{W^T}{d} + v^2 I_p
\end{equation*}
Now we will consider $V$. Let
\begin{equation*}
    V^l = \bE{z}{\varrho(\rat{W}{d}(ax_l+\sqrt{h}z))x_l^T}
\end{equation*}
We have 
\begin{align*}
    V^l_{ij} &= \bE{z}{\varrho(\rat{w_i^T(ax_l+\sqrt{h}z)}{d})x_{lj}}\\
    &= \bE{u}{\varrho(\rat{a w_i^Tx_l}{d}+\sqrt{h}u)}x_{lj}\\
    &= \varrho_0(\rat{a w_i^Tx_l}{d})x_{lj}.
\end{align*}
where $\varrho_0(y) = \bE{u}{\varrho(y+\sqrt{h}u)}$.
\begin{align*}
    V_{ij} &= \frac{1}{n}\sum_{l=1}^{n} V^l_{ij},\\
    &= \frac{1}{n}\sum_{l=1}^{n} \varrho_0(\rat{a w_i^Tx_l}{d})x_{lj},\\
\end{align*}
We have $V = \rat{\varrho_0(a\rat{W}{d}X)}{n}\rat{X^T}{n}$. Now, let's consider $U$. Let
\begin{align*}
    U^l &= \bE{z}{\varrho(\rat{W}{d}(ax_l+\sqrt{h}z))\varrho(\rat{W}{d}(ax_l+\sqrt{h}z))^T}\\
\end{align*}
For $i\neq j $ we have,
\begin{align*}
    U^l_{ij} &= \bE{z}{\varrho(\rat{w_i^T(ax_l+\sqrt{h}z)}{d})\varrho(\rat{w_j^T(ax_l+\sqrt{h}z)}{d})}\\
    &= \bE{(u,v)\sim P_{\frac{w_i^Tw_j}{d}}}{\varrho(a\rat{w_i^Tx_l}{d} + \sqrt{h}u)\varrho(a\rat{w_j^Tx_l}{d} + \sqrt{h}v)}\\
    &= \sum_{k=0}^{\infty} \frac{(\frac{w_i^Tw_j}{d})^k}{k!}\bE{u}{\varrho(a\rat{w_i^Tx_l}{d} + \sqrt{h}u)\text{He}_k(u)}\bE{u}{\varrho(a\rat{w_j^Tx_l}{d} + \sqrt{h}v)\text{He}_k(v)}\\
    &= \varrho_0(a\rat{w_i^Tx_l}{d})\varrho_0(a\rat{w_j^Tx_l}{d})+\frac{w_i^Tw_j}{d}\varrho_1(a\rat{w_i^Tx_l}{d})\varrho_1(a\rat{w_j^Tx_l}{d})+O(1/d),
\end{align*}
where $\varrho_0(y) = \bE{u}{\varrho(y+\sqrt{h}u)}$ and $\varrho_1(y) = \bE{u}{\varrho(y+\sqrt{h}u)u}$. Let $X=[x_1, x_2,\cdots,x_n]\in\R^{d\times n}$. 
\begin{align*}
    U_{ij} &= \frac{1}{n}\sum_{l=1}^{n} U^l_{ij},\\
    &= \frac{1}{n}\sum_{l=1}^{n}\varrho_0(a\rat{w_i^Tx_l}{d})\varrho_0(a\rat{w_j^Tx_l}{d})+\frac{w_i^Tw_j}{d}\frac{1}{n}\sum_{l=1}^{n}\varrho_1(a\rat{w_i^Tx_l}{d})\varrho_1(a\rat{w_j^Tx_l}{d}) + O(1/d)\\
    &= \frac{1}{n}\sum_{l=1}^{n}\varrho_0(a\rat{w_i^Tx_l}{d})\varrho_0(a\rat{w_j^Tx_l}{d})+\frac{w_i^Tw_j}{d}\bE{x}{\varrho_1(a\rat{w_i^Tx}{d})\varrho_1(a\rat{w_j^Tx}{d})} + O(1/d)\\
    &= \frac{1}{n}\sum_{l=1}^{n}\varrho_0(a\rat{w_i^Tx_l}{d})\varrho_0(a\rat{w_j^Tx_l}{d})+\frac{w_i^Tw_j}{d}\bE{g}{\varrho_1(ag)}^2 + O(1/d)\\
    &= \frac{1}{n}\sum_{l=1}^{n}\varrho_0(a\rat{w_i^Tx_l}{d})\varrho_0(a\rat{w_j^Tx_l}{d})+h\mu_1^2\frac{w_i^Tw_j}{d} + O(1/d)\\
\end{align*}
For $i=j$,
\begin{align*}
    U^l_{ii} &= \bE{z}{\left(\varrho(\rat{w_i^T(ax_l+\sqrt{h}z)}{d})\right)^2}\\
\end{align*}
\begin{align*}
    U_{ii} &= \frac{1}{n}\sum_{l=1}^n\bE{z}{\left(\varrho(\rat{w_i^T(ax_l+\sqrt{h}z)}{d})\right)^2}\\
    &= \bE{z,x}{\left(\varrho(\rat{w_i^T(ax_l+\sqrt{h}z)}{d})\right)^2}+O(1/\sqrt{d})\\
    &= \norm{\varrho}^2+O(1/\sqrt{d}).
\end{align*}
Let $X=[x_1,x_2,\cdots,x_n]$. We can write $U$ as:
\begin{equation*}
    U = \rat{\varrho_0(a\rat{W}{d}X)}{n}\rat{\varrho_0(a\rat{W}{d}X)^T}{n}+h\mu_1^2\rat{W}{d}\rat{W^T}{d}+s^2I_p,
\end{equation*}
where
\begin{align*}
s^2 &= \norm{\varrho}^2-\bE{g}{\varrho_0(ag)^2}-h\mu_1^2\\
&= \norm{\varrho}^2-\bE{g}{\bE{u}{\varrho(ag+\sqrt{h}u)}^2}-h\mu_1^2\\
&= \norm{\varrho}^2-c(a^2)-h\mu_1^2,
\end{align*}
where $c(\gamma) = \bE{u,v\sim P_\gamma}{\varrho(u)\varrho(v)}$.

We can use Gaussian equivalence principle to replace the nonlinear term in $U$. A Gaussian equivalent for $\varrho_0(a\rat{W}{d}X)$ is given by
\begin{align*}
    G &= \bE{g}{\varrho_0(ag)}\bone\bone^T+\bE{g}{\varrho_0(ag)g}\rat{W}{d}X+\left(\bE{g}{\varrho_0(ag)^2}-\bE{g}{\varrho_0(ag)}^2-\bE{g}{\varrho_0(ag)g}^2\right)^{1/2}\Omega\\
    &= \mu_0\bone\bone^T+a\mu_1\rat{W}{d}X+\left(\underbrace{c(a^2)-\mu_0^2-a^2\mu_1^2}_{:=v_0^2}\right)^{1/2}\Omega,
\end{align*}
where, $\Omega\in\R^{p\times n}$ is a random matrix with standard Gaussian entries.
Hence we have
\begin{align*}
    U &= \rat{G}{n}\rat{G^T}{n}+h\mu_1^2\rat{W}{d}\rat{W^T}{d}+s^2I_p,\\
    V &= \rat{G}{n}\rat{X^T}{n}
\end{align*}
where,
\begin{equation*}
    G = \mu_0\bone\bone^T+a\mu_1\rat{W}{d}X+v_0\Omega.
\end{equation*}
We now have expressions for all terms in the generalization error.
\begin{align*}
    \cE_{\text{test}} &=a^2 - \frac{2a}{d}\tr{V^T (U+\lambda I_p)^{-1}\Tilde{V}}+\frac{1}{d}\tr{(U+\lambda I_p)^{-1}V V^T(U+\lambda I_p)^{-1}\Tilde{U}}\\
    &= a^2-\frac{2a\mu_1}{d}\tr{\rat{X}{n}\rat{G^T}{n}(U+\lambda I_p)^{-1}\rat{W}{d}}\\&\qquad+\frac{1}{d}\tr{(U+\lambda I_p)^{-1}\rat{G}{n}\rat{X^T}{n}\rat{X}{n}\rat{G^T}{n}(U+\lambda I_p)^{-1}\left(\mu_0^2 \bone\bone^T + \mu_1^2 \rat{W}{d}\rat{W^T}{d} + v^2 I_p\right)}
\end{align*}
For simplicity in presentation, assume that $\mu_0=0$.
\begin{align*}
    \cE_{\text{test}} &= a^2-2a\mu_1E_1+\mu_1^2E_2+v^2E_3.
\end{align*}
\begin{align*}
    E_1 &= \frac{1}{d}\tr{\rat{X}{n}\rat{G^T}{n}(U+\lambda I_p)^{-1}\rat{W}{d}}\\
    E_2 &= \frac{1}{d}\tr{\rat{W^T}{d}(U+\lambda I_p)^{-1}\rat{G}{n}\rat{X^T}{n}\rat{X}{n}\rat{G^T}{n}(U+\lambda I_p)^{-1}\rat{W}{d}}\\
    E_3 &= \frac{1}{d}\tr{(U+\lambda I_p)^{-2}\rat{G}{n}\rat{X^T}{n}\rat{X}{n}\rat{G^T}{n}}
\end{align*}
We define the following matrix
\begin{equation*}
    U(q) :=  \rat{G}{n}\rat{G^T}{n}+(h\mu_1^2+q)\rat{W}{d}\rat{W^T}{d}+s^2I_p,
\end{equation*}
and define the resolvent of $U(q)$ as
\begin{equation*}
    R(q,z) = (U(q)-zI_p)^{-1}.
\end{equation*}
Let 
\begin{equation*}
    K(q,z) = \frac{1}{d}\tr{R(q,z)\rat{G}{n}\rat{X^T}{n}\rat{X}{n}\rat{G^T}{n}}
\end{equation*}
Using the identities $\frac{\partial R}{\partial q} = -R(q,z)\frac{\dd U}{\dd q}R(q,z)$ and $\frac{\partial R}{\partial z} = R(q,z)^2$, we observe that 
\begin{align*}
    % E_1 &= K(0,-\lambda)\\
    E_2 &= -\frac{\partial K}{\partial q}(0,-\lambda)\\
    E_3 &= \frac{\partial K}{\partial z}(0,-\lambda)
\end{align*}
Therefore, it suffices to have the function $K$ and $E_1$ to have an expression for $\cE_{\text{test}}$.
We derive an expression for $K$ and $E_1$ using Linear pencils. We construct the following $4\times 4$ block matrix.
\begin{equation*}
    L = \mleft[
        \begin{array}{c|ccc}
        (s^2-z)I_p & a\mu_1\rat{W}{d} & v_0\rat{\Omega}{n} & (h\mu_1^2+q)\rat{W}{d}\\
        \hline
        0 & I_d & -\rat{X}{n} & 0\\
        -v_0\rat{\Omega^T}{n} & 0 & I_n & -a\mu_1\rat{X^T}{n}\\
        -\rat{W^T}{d} & 0 & 0 & I_d
    \end{array}
    \mright] = \mleft[\begin{array}{cc}
        L_{11} & L_{12} \\
        L_{21} & L_{22}
    \end{array}\mright].
\end{equation*}
First, we can invert $L$ and verify that $K$ is trace of one of the blocks in $L^{-1}$. Let $\bar{L}_{22} = L_{11}-L_{12}L_{22}^{-1}L_{21}$. Using Block matrix inversion formula, we have 
\begin{equation*}
    L^{-1} = \mleft[
            \begin{array}{cc}
              \bar{L}_{22}^{-1}   &  -\bar{L}_{22}^{-1}L_{12}L_{22}^{-1}\\
               -L_{22}^{-1}L_{21}\bar{L}_{22}^{-1}  &  L_{22}^{-1}+ L_{22}^{-1}L_{21}\bar{L}_{22}^{-1}L_{12}L_{22}^{-1}
            \end{array}
    \mright]
\end{equation*}.
\begin{equation*}
    L_{22}^{-1} = \mleft[\begin{array}{ccc}
       I_d  &  \rat{X}{n} & a\mu_1\rat{X}{n}\rat{X^T}{n}\\
       0  & I_n & a\mu_1\rat{X^T}{n}\\
       0  & 0 & I_d\\
    \end{array}\mright]
\end{equation*}
\begin{align*}
    \bar{L}_{22} &= (s^2-z)I_p + v_0\rat{G}{n}\rat{\Omega^T}{n}+a\mu_1\rat{G}{n}\rat{X^T}{n}\rat{W^T}{d}+(h\mu_1^2+q)\rat{W}{d}\rat{W^T}{d}\\
    &= (s^2-z)I_p + \rat{G}{n}\rat{G^T}{n}+(h\mu_1^2+q)\rat{W}{d}\rat{W^T}{d}\\
    &= R^{-1}(q,z)    
\end{align*}
Hence,
\begin{equation*}
\footnotesize
    L^{-1} = \mleft[
    \begin{array}{cccc}
       R(q,z)  & -a\mu_1R(q,z)\rat{W}{d} & -R(q,z)\rat{G}{n} & -R(q,z)(a\mu_1\rat{G}{n}\rat{X^T}{n}+(h\mu_1^2+q)\rat{W}{d}) \\
        \rat{X}{n}\rat{G^T}{n}R(q,z) & I_d-a\mu_1\rat{X}{n}\rat{G^T}{n}R(q,z)\rat{W}{d} & \rat{X}{n}-\rat{X}{n}\rat{G^T}{n}R(q,z)\rat{G}{n} & a\mu_1\rat{X}{n}\rat{X^T}{n}-\rat{X}{n}\rat{G^T}{n}R(q,z)(a\mu_1\rat{G}{n}\rat{X^T}{n}+(h\mu_1^2+q)\rat{W}{d})\\
        \rat{G^T}{n}R(q,z) & -a\mu_1\rat{G^T}{n}R(q,z)\rat{W}{d} & I_n-\rat{G^T}{n}R(q,z)\rat{G}{n} & a\mu_1\rat{X^T}{n}-\rat{G^T}{n}R(q,z)(a\mu_1\rat{G}{n}\rat{X^T}{n}+(h\mu_1^2+q)\rat{W}{d})\\
        \rat{W^T}{d} & -a\mu_1\rat{W^T}{d}R(q,z)\rat{W}{d} & -\rat{W^T}{d}R(q,z)\rat{G}{n} & I_d-\rat{W^T}{d}R(q,z)(a\mu_1\rat{G}{n}\rat{X^T}{n}+(h\mu_1^2+q)\rat{W}{d})
    \end{array}
    \mright]
\end{equation*}
We see that $(L^{-1})^{2,2}$ and $(L^{-1})^{2,4}$ give the desired terms. We have
\begin{align*}
    \frac{1}{d}\tr{(L^{-1})^{2,2}} &= 1-a\mu_1E_1\\ 
    \frac{1}{d}\tr{(L^{-1})^{2,4}} &= a\mu_1-a\mu_1K(q,z)-(h\mu_1^2+q)E_1\\ 
\end{align*}
We use the linear pencil formalism to derive the traces of of square blocks in $L^{-1}$. Let $g$ be the matrix of traces of square blocks in $L^{-1}$ divided by the block size. For example, if $L^{i,j}$ is a square matrix of dimension $N$, then $g_{ij}=\frac{1}{N}\tr{(L^{-1})^{i,j}}$. We assign $g_{ij}=0$ if $L^{i,j}$ is not a square matrix.
\begin{equation*}
    g = \mleft[
    \begin{array}{cccc}
        g_{11} & 0 & 0 & 0  \\
        0 & g_{22} & 0 & g_{24}  \\
        0 & 0 & g_{33} & 0  \\
        0 & g_{42} & 0 & g_{44}  \\
    \end{array}
    \mright]
\end{equation*}
Notice that the constant matrices in $L$ are all multiples of identity. Let $B$ be the matrix that contains the coefficients of these constant matrices.
\begin{equation*}
    B = \mleft[
    \begin{array}{cccc}
        s^2-z & 0 & 0 & 0  \\
        0 & 1 & 0 & 0  \\
        0 & 0 & 1 & 0  \\
        0 & 0 & 0 & 1  \\
    \end{array}
    \mright]
\end{equation*}
If $L^{il}$ and $L^{jk}$ are square matrices, let $\sigma_{ij}^{kl}$ be the covariance between an element of $L^{ij}$ and $L^{kl}$ multiplied by the block size of $L^{jk}$. Let $L^{ij}$ be of dimension $N_i\times N_j$ and $M$ denote the non constant part of $L$. Then,
\begin{equation*}
    \sigma_{ij}^{kl} = N_j\bE{}{M^{ij}_{uv}M^{kl}_{vu}}.
\end{equation*}
Let $S = \{(i,j): N_i=N_j\}$, the set of indices of square blocks in $L$. Then, a mapping $\eta_L$ is defined such that
\begin{equation*}
    \eta_L(G)_{il} = \sum_{(jk) \in S} \sigma_{ij}^{kl}g_{jk}
\end{equation*}
for $(il)$ in $S$. 
\begin{equation*}
    \eta_L(g) = \mleft[
    \begin{array}{cccc}
        \sigma_{12}^{41}g_{24}+\sigma_{13}^{31}g_{33}+\sigma_{14}^{41}g_{44} & 0 & 0 & 0  \\
        0 & 0 & 0 & \sigma_{23}^{34}g_{33}  \\
        0 & 0 & \sigma_{31}^{13}g_{11}+\sigma_{34}^{23}g_{42} & 0  \\
        0 & \sigma_{41}^{12}g_{11} & 0 & \sigma_{41}^{14}g_{11}  \\
    \end{array}
    \mright]
\end{equation*}
We have $N_1=p, N_2=d, N_3=n, N_4=d$.
\begin{align}
    & \sigma_{12}^{41} = -a\mu_1,\qquad \sigma_{41}^{12}= -a\mu_1\psi_p\\
    & \sigma_{13}^{31} = -v_0^2,\qquad \sigma_{31}^{13} = -v_0^2\psi_p/\psi_n\\
    & \sigma_{14}^{41} = -(h\mu_1^2+q),\qquad \sigma_{41}^{14} = -(h\mu_1^2+q)\psi_p\\
    & \sigma_{23}^{34} = a\mu_1,\qquad \sigma_{34}^{23}=a\mu_1/\psi_n\\
\end{align}
We have that $g$ satisfies the following fixed point equation
\begin{equation*}
    (B-\eta_L(g))g = I.
\end{equation*}
Hence we have,
\begin{multline*}
    \mleft[
    \begin{array}{cccc}
        s^2-z+a\mu_1g_{24}+v_0^2g_{33}+(h\mu_1^2+q)g_{44} & 0 & 0 & 0  \\
        0 & 1 & 0 & -a\mu_1g_{33}  \\
        0 & 0 & 1+v_0^2\psi_p/\psi_ng_{11}-a\mu_1/\psi_ng_{42} & 0  \\
        0 & a\mu_1\psi_pg_{11} & 0 & 1+(h\mu_1^2+q)\psi_pg_{11}  \\
    \end{array}
    \mright]\\
    \times
    \mleft[
    \begin{array}{cccc}
        g_{11} & 0 & 0 & 0  \\
        0 & g_{22} & 0 & g_{24}  \\
        0 & 0 & g_{33} & 0  \\
        0 & g_{42} & 0 & g_{44}  \\
    \end{array}
    \mright] = I
\end{multline*}
This gives the following set of equations:
\begin{align*}
    g_{11}(s^2-z+a\mu_1g_{24}+v_0^2g_{33}+(h\mu_1^2+q)g_{44}) &= 1\\
    g_{22} -a\mu_1g_{33}g_{42} &= 1\\
    g_{24} -a\mu_1g_{33}g_{44} &= 0\\
    g_{33}(1+v_0^2\psi_p/\psi_ng_{11}-a\mu_1/\psi_ng_{42}) &=1\\
    a\mu_1\psi_pg_{11}g_{22}+(1+(h\mu_1^2+q)\psi_pg_{11})g_{42} &= 0\\
    a\mu_1\psi_pg_{11}g_{24}+(1+(h\mu_1^2+q)\psi_pg_{11})g_{44} &= 1\\
\end{align*}
We solve the above set of equations numerically to obtain the elements of matrix $g$. We obtain $E_1, K$ using the expressions $E_1 = \frac{1-g_{22}}{a\mu_1}$, $K(q,z)=\frac{a\mu_1-g_{24}-(h\mu_1^2+q)E_1}{a\mu_1}$. Figs.~\ref{fig:rfm_minf_test_train_err} and \ref{fig:rfm_minf_test_train_err_time} shows the test error thus obtained. Next, we look at the train error:\\
\textbf{Train error:}
\begin{align*}
    \cE_{\text{train}} &= \frac{1}{d}\cL(\hat{A})\\
    &=\frac{1}{d}\tr{\rat{\hat{A}_t}{p}^T\rat{\hat{A}_t}{p} (U+\lambda I_p)}-\frac{2}{d}\tr{\rat{\hat{A}_t}{p} V}+1,\\
    &=\frac{1}{d}\tr{(U+\lambda I_p)^{-1}VV^T}-\frac{2}{d}\tr{V^T(U+\lambda I_p)^{-1}V}+1,\\
    &=-\frac{1}{d}\tr{(U+\lambda I_p)^{-1}\rat{G}{n}\rat{X^T}{n}\rat{X}{n}\rat{G^T}{n}}+1,\\
    &=-K(0,-\lambda)+1,\\
\end{align*}
Figs.~\ref{fig:rfm_minf_test_train_err} and \ref{fig:rfm_minf_test_train_err_time} shows the train error.
\begin{figure}
    \centering
\includegraphics[width=1\linewidth]{figures/lp_minf_err.png}
    \caption{Test error and train error for $m=\infty$ case.}
    \label{fig:rfm_minf_test_train_err}
\end{figure}

\begin{figure}
    \centering
\includegraphics[width=1\linewidth]{figures/lp_err_minf_time.png}
    \caption{Test error and train error for $m=\infty$ case.}
    \label{fig:rfm_minf_test_train_err_time}
\end{figure}
\subsubsection{Case: $m=1$}
When $m=1$, the loss function reduces to:
\begin{align}
    \cL_t(s) &= \frac{1}{n}\sum_{i=1}^{n}{\norm{s(t,a_tx_i+\sqrt{h_t}z_i)-x_i}^2}.
\end{align}

Let $s_\theta(t,x)=\rat{A_t}{p}\varrho(\rat{W_t}{d}x)$, $W_t\in\R^{p\times d}$, $A_t\in\R^{d\times p}$, $X=[x_1,x_2,\cdots,x_n]$, $Z=[z_1,z_2,\cdots,z_n]$, and let $Y$ be a matrix with its $i^{th}$ column given as $y_i = a_tx_i+\sqrt{h_t}z_i$. Let $F=\varrho(\rat{W}{d}Y)$.
\begin{align*}
    \cL_t(A_t) &=\frac{1}{n}\sum_{i=1}^{n}{{\norm{\rat{A_t}{p}\varrho(\rat{W_t}{p}(a_tx_i+\sqrt{h_t}z_i))-x_i}^2}}+\frac{\lambda}{p}\norm{A_t}_F^2,\\
    &=\tr{\rat{A_t}{p}^T\rat{A_t}{p} U}-2\tr{\rat{A_t}{p} V}+d+\lambda\tr{\rat{A_t^T}{p}\rat{A_t}{p}},\\
\end{align*}
where 
$$U = \frac{1}{n}\varrho(\rat{W_t}{d}Y)\varrho(\rat{W_t}{d}Y)^T = \rat{F}{n}\rat{F^T}{n}$$
and 
$$V = \frac{1}{n}{\varrho(\rat{W_t}{d}Y)Z^T} = \rat{F}{n}\rat{X^T}{n}.$$ Thus we get the optimal $A_t$ as
\begin{equation}
    \rat{\hat{A}_t}{p} = V^T(U+\lambda I_p)^{-1} = \rat{X}{n}\rat{F^T}{n}\left(\rat{F}{n}\rat{F^T}{n}+\lambda I_p\right)^{-1}.
\end{equation}
\textbf{Test error:}
\begin{align*}
    \cE_{\text{test}} &= \frac{1}{d}\bE{x\sim P_t}{\norm{\rat{\hat{A}_t}{p}\varrho(\rat{W_t}{d}x)-ax}^2}\\
    &=a^2 - \frac{2a}{d}\tr{\rat{\hat{A}_t}{p}\underbrace{\bE{x}{\varrho(\rat{W_t}{d}x)x^T}}_{:=\Tilde{V}}}\\
    &\qquad+\frac{1}{d}\tr{\rat{\hat{A}^T_t}{p}\rat{\hat{A}_t}{p}\underbrace{\bE{x}{\varrho(\rat{W_t}{d}x)\varrho(\rat{W_t}{d}x)^T}}_{:=\Tilde{U}}}
\end{align*}

We have already derived expressions for $\Tilde{V}$ and $\Tilde{U}$ in the previous section. We have $\Tilde{V} = \mu_1\rat{W}{d}$, $\Tilde{U} = \mu_0^2\textbf{1}\textbf{1}^T+ \mu_1^2\rat{W}{d}\rat{W^T}{d}+v^2I_p$. For simplicity, we will assume that $\mu_0=0$. Thus we have

\begin{align*}
    \cE_{\text{test}} &= \frac{1}{d}\bE{x\sim P_t}{\norm{\rat{\hat{A}_t}{p}\varrho(\rat{W_t}{d}x)-ax}^2}\\
    &=a^2 - \frac{2a\mu_1}{d}\tr{\rat{X}{n}\rat{F^T}{n}\left(\rat{F}{n}\rat{F^T}{n}+\lambda I_p\right)^{-1}\rat{W}{d}}\\
    &\qquad+\frac{\mu_1^2}{d}\tr{\left(\rat{F}{n}\rat{F^T}{n}+\lambda I_p\right)^{-1}\rat{F}{n}\rat{X^T}{n}\rat{X}{n}\rat{F^T}{n}\left(\rat{F}{n}\rat{F^T}{n}+\lambda I_p\right)^{-1}\rat{W}{d}\rat{W^T}{d}}\\
    &\qquad+\frac{v^2}{d}\tr{\left(\rat{F}{n}\rat{F^T}{n}+\lambda I_p\right)^{-1}\rat{F}{n}\rat{X^T}{n}\rat{X}{n}\rat{F^T}{n}\left(\rat{F}{n}\rat{F^T}{n}+\lambda I_p\right)^{-1}}
\end{align*}
Since we focus on a single time instant, we drop the subscript $t$ in the above expressions. However, we will keep in mind that $a$ and $h$ depend on $t$ and we have the relation $a^2+h=1$. Also, we use Gaussian equivalence principle to handle the non-linearity in $F$. In particular, we use the following expression for $F$:
\begin{equation*}
    F = \mu_1\rat{W}{d}Y+v\Omega.
\end{equation*}
Let 
\begin{equation}
    R(q,z) = \left(\rat{F}{n}\rat{F^T}{n}+q\rat{W}{d}\rat{W^T}{d}-zI_p\right)^{-1},
\end{equation}
and
\begin{align*}
    E_1 &= \frac{1}{d}\tr{\rat{X}{n}\rat{F^T}{n}R(0,-\lambda)\rat{W}{d}}\\
    E_2 &= \frac{1}{d}\tr{R(0,-\lambda)\rat{F}{n}\rat{X^T}{n}\rat{X}{n}\rat{F^T}{n}R(0,-\lambda)\rat{W}{d}\rat{W^T}{d}}\\
    E_3 &= \frac{1}{d}\tr{\rat{F}{n}\rat{X^T}{n}\rat{X}{n}\rat{F^T}{n}R(0,-\lambda)^2}
\end{align*}
With above definitions, we can write $\cE_{\text{test}}$ as 
\begin{equation*}
    \cE_{\text{test}} = a^2-2a\mu_1E_1+\mu_1^2E_2+v^2E_3.
\end{equation*}
Let 
\begin{align*}
    K(q,z) = \frac{1}{d}\tr{R(q,z)\rat{F}{n}\rat{X^T}{n}\rat{X}{n}\rat{F^T}{n}}.
\end{align*}
Then we have 
\begin{align*}
    E_2 &= -\frac{\dd K}{\dd q}(0,-\lambda)\\
    E_3 &= \frac{\dd K}{\dd z}(0,-\lambda)\\
\end{align*}
Hence, we need to find the terms $E_1$ and $K$ in order to derive an expression for the generalization error. As earlier, we use linear pencils to obtain the desired terms. We use the following $6\times 6$ linear pencil matrix for this:

\begin{equation*}
    L = \mleft[
        \begin{array}{c|ccccc}
        -zI_p & q\rat{W}{d} & v_0\rat{\Omega}{n} & \mu_1\rat{W}{d} & 0 & 0\\
        \hline
        -\rat{W^T}{d} & I_d & 0 & 0 & 0 & 0\\
        -v\rat{\Omega^T}{n} & -\mu\rat{Y^T}{n} & I_n & 0 & 0 & 0\\
        0 & 0 & -\rat{Y}{n} & I_d & 0 & 0\\
        0 & 0 & -\rat{X}{n} & 0 & I_d & 0\\
        0 & 0 & 0 & 0 & -\rat{X^T}{n} & I_n\\
    \end{array}
    \mright] = \mleft[\begin{array}{cc}
        L_{11} & L_{12} \\
        L_{21} & L_{22}
    \end{array}\mright].
\end{equation*}
By computing $L^{-1}$, we find that
\begin{align}
    (L^{-1})^{5,4}(q,z) &= -\mu_1\rat{X}{n}\rat{F^T}{n}R(q,z)\rat{W}{d}\\
    (L^{-1})^{6,3}(q,z) &= -\rat{X^T}{n}\rat{X}{n}\rat{F^T}{n}R(q,z)\rat{F}{n}+\rat{X^T}{n}\rat{X}{n}\\
\end{align}
Hence,
\begin{align}
    E_1 &= \frac{1}{d}\tr{\rat{X}{n}\rat{F^T}{n}R(0,-\lambda)\rat{W}{d}} = -\frac{1}{\mu_1 d}\tr{(L^{-1})^{5,4}(0,-\lambda)}\\
    K(q,z) &= \frac{1}{d}\tr{R(q,z)\rat{F}{n}\rat{X^T}{n}\rat{X}{n}\rat{F^T}{n}} = 1-\frac{1}{d}\tr{(L^{-1})^{6,3}(q,z)}. 
\end{align}
We can compute the traces of blocks in $L^{-1}$ as earlier. We have 
\begin{equation*}
    g = \mleft[
    \begin{array}{cccccc}
        g_{11} & 0 & 0 & 0 & 0 & 0 \\
        0 & g_{22} & 0 & g_{24} & 0 & 0 \\
        0 & 0 & g_{33} & 0 & 0 & 0 \\
        0 & g_{42} & 0 & g_{44} & 0 & 0 \\
        0 & g_{52} & 0 & g_{54} & 1 & 0 \\
        0 & 0 & g_{63} & 0 & 0 & 1  \\
    \end{array}
    \mright]
\end{equation*}

\begin{equation*}
    B = \mleft[
    \begin{array}{cccccc}
        -z & 0 & 0 & 0 & 0 & 0 \\
        0 & 1 & 0 & 0 & 0 & 0 \\
        0 & 0 & 1 & 0 & 0 & 0 \\
        0 & 0 & 0 & 1 & 0 & 0 \\
        0 & 0 & 0 & 0 & 1 & 0 \\
        0 & 0 & 0 & 0 & 0 & 1  \\
    \end{array}
    \mright]
\end{equation*}
\begin{equation*}
    \eta_L(g) = \mleft[
    \begin{array}{cccccc}
        \sigma_{12}^{21}g_{22}+\sigma_{13}^{31}g_{33}+\sigma_{14}^{21}g_{42} & 0 & 0 & 0 & 0 & 0 \\
        0 & \sigma_{21}^{12}g_{11} & 0 & \sigma_{21}^{14}g_{11} & 0 & 0 \\
        0 & 0 & \sigma_{31}^{13}g_{11}+\sigma_{32}^{43}g_{24} & 0 & 0 & 0 \\
        0 & \sigma_{43}^{32}g_{33} & 0 & 0 & 0 & 0 \\
        0 & \sigma_{53}^{32}g_{33} & 0 & 0 & 0 & 0 \\
        0 & 0 & \sigma_{65}^{43}g_{54}+\sigma_{65}^{53} & 0 & 0 & 0  \\
    \end{array}
    \mright]
\end{equation*}

\begin{align}
    & \sigma_{12}^{21} = -q,\qquad \sigma_{21}^{12}= -q\psi_p,\qquad \sigma_{21}^{14}= -\mu_1\psi_p\\
    & \sigma_{13}^{31} = -v^2,\qquad \sigma_{31}^{13} = -v^2\psi_p/\psi_n,\qquad \sigma_{32}^{43} = \mu_1/\psi_n,\qquad \sigma_{43}^{32} = \mu_1\\
    & \sigma_{14}^{21} = -\mu_1,\qquad \sigma_{53}^{32} = \mu_1a\\
    & \sigma_{65}^{43} = a/\psi_n,\qquad \sigma_{65}^{53}=1/\psi_n\\
\end{align}
We have that $g$ satisfies the following fixed point equation
\begin{equation*}
    (B-\eta_L(g))g = I.
\end{equation*}
Hence we have,
\begin{multline*}
    \mleft[
    \begin{array}{cccccc}
        -z+qg_{22}+v^2g_{33}+\mu_1g_{42} & 0 & 0 & 0 & 0 & 0  \\
        0 & 1+q\psi_p g_{11} & 0 & \mu_1\psi_p g_{11} & 0 & 0  \\
        0 & 0 & 1+v^2\psi_p/\psi_ng_{11}-\mu_1/\psi_ng_{24} & 0 & 0 & 0  \\
        0 & -\mu_1g_{33} & 0 & 1 & 0 & 0  \\
        0 & -\mu_1ag_{33} & 0 & 0 & 1 & 0  \\
        0 & 0 & -a/\psi_n g_{54}-1/\psi_n & 0 & 0 & 1  \\
    \end{array}
    \mright]\\
    \times
    \mleft[
    \begin{array}{cccccc}
        g_{11} & 0 & 0 & 0 & 0 & 0 \\
        0 & g_{22} & 0 & g_{24} & 0 & 0 \\
        0 & 0 & g_{33} & 0 & 0 & 0 \\
        0 & g_{42} & 0 & g_{44} & 0 & 0 \\
        0 & g_{52} & 0 & g_{54} & 1 & 0 \\
        0 & 0 & g_{63} & 0 & 0 & 1  \\
    \end{array}
    \mright] = I
\end{multline*}
This gives the following set of equations:
\begin{align*}
    g_{11}(-z+qg_{22}+v^2g_{33}+\mu_1g_{42}) &= 1\\
    g_{22}(1+q\psi_p g_{11}) +\mu_1\psi_pg_{11}g_{42} &= 1\\
    g_{24}(1+q\psi_p g_{11}) +\mu_1\psi_pg_{11}g_{44} &= 0\\
    g_{33}(1+v^2\psi_p/\psi_ng_{11}-\mu_1/\psi_ng_{24}) &= 1\\
    -\mu_1g_{22}g_{33}+g_{42} &= 0\\
    -\mu_1g_{24}g_{33}+g_{44} &= 1\\
    -\mu_1ag_{33}g_{22}+g_{52} &= 0\\
    -\mu_1ag_{33}g_{24}+g_{54} &= 0\\
    g_{33}(-a/\psi_n g_{54}-1/\psi_n)+g_{63} &= 0 
\end{align*}
As earlier, we solve the above set of equations to obtain the elements of matrix $g$. We obtain $E_1, K$ by using the expressions $E_1 = -g_{54}/\mu_1$, $K(q,z)=1-\psi_ng_{63}$. Fig.~\ref{fig:rfm_m1_test_train_err} and \ref{fig:rfm_m1_test_train_err_time} shows the test error thus obtained.\\
\textbf{Train error:}
\begin{align*}
    \cE_{\text{train}}    &=\frac{1}{d}\tr{\rat{\hat{A}_t}{p}^T\rat{\hat{A}_t}{p} (U+\lambda I_p)}+\frac{2}{\sqrt{h}d}\tr{\rat{\hat{A}_t}{p} V}+\frac{1}{h},\\
    &=\frac{1}{hd}\tr{(U+\lambda I_p)^{-1}VV^T}-\frac{2}{hd}\tr{V^T(U+\lambda I_p)^{-1}V}+\frac{1}{h},\\
    &=-\frac{1}{hd}\tr{(U+\lambda I_p)^{-1}\rat{F}{n}\rat{Z^T}{n}\rat{Z}{n}\rat{F^T}{n}}+\frac{1}{h},\\
    &=-\frac{K(0,-\lambda)}{h}+\frac{1}{h},\\
\end{align*}
Figs~\ref{fig:rfm_m1_test_train_err} and \ref{fig:rfm_m1_test_train_err_time} shows the train error.
\begin{figure}
    \centering
\includegraphics[width=1\linewidth]{figures/lp_m1_err.png}
    \caption{Test error and train error for $m=1$ case.}
    \label{fig:rfm_m1_test_train_err}
\end{figure}

\begin{figure}
    \centering
\includegraphics[width=1\linewidth]{figures/lp_err_m1_time.png}
    \caption{Test error and train error for $m=1$ case.}
    \label{fig:rfm_m1_test_train_err_time}
\end{figure}
