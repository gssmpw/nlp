\section{Introduction}
Generative models are at the heart of the on-going revolution in artificial intelligence. Formally, they aim to generate new samples from an unknown probability distribution, given
$n$ i.i.d. samples drawn from it.
Commercial generative models demonstrate remarkable capabilities across various modalities, including text, speech, images, and videos, with new advancements being reported regularly. 
The impressive capabilities of these generative models are mainly driven by two key architectures: \textit{transformers} and \textit{diffusion models}. While transformers excel primarily on text data, diffusion models show exceptional proficiency in generating natural-looking images from text prompts. Despite their success, the scientific community remains divided on whether these models are truly creative or merely imitate based on the examples that they have seen during training~\cite{ukpaka_creative_2024,kamb_analytic_2024}. An even more pressing concern is that of \textit{memorization}, where the model's response  partially or fully resembles training data. The memorization phenomenon raises serious implications, particularly regarding the privacy of data used to train these models \cite{carlini_extracting_2021, carlini_extracting_2023}. The limited theoretical understanding of these models prevents addressing such questions effectively. 

% In this study, we focus on diffusion models. Two important aspects of diffusion models that lacks a complete understanding are as follows: First, how are diffusion models able to generate natural-looking images when trained on a relatively small number of images ($\sim10^7$), despite the theoretical requirement of $\bigO{e^d}$ samples for learning an arbitrary probability distribution on $\R^d$? Second, when trained long enough using parameterized neural networks, diffusion models are shown to memorize training samples. What is the mechanism behind this memorization? 

% A high-level explanation to the first question is that the probability distributions of natural images have a certain low dimensional structure, and the inductive biases of diffusion models (diffusion framework along with neural network and optimization algorithm) are well aligned to be able to exploit this structure \cite{kadkhodaie_generalization_2024}. However, theoretical studies on memorization aspects of diffusion models has been done only with \textit{empirical optimal score} functions \cite{biroli_dynamical_2024, achilli_losing_2024}. On the other hand, empirical results have demonstrated the occurrence of memorization when neural networks are employed, as is common in practice \cite{yoon_diffusion_2023}. This discrepancy raises an important question: can the phenomenon of memorization be theoretically shown when using a parametric class of functions for the score function? This is precisely the context of our study. By analyzing the learning process of a specific instance of diffusion models, we provide theoretical insights into the mechanisms underlying memorization. 

% A well understood aspect of diffusion models is the errors introduced by its generative process \cite{chen_sampling_2022, benton_nearly_2023,chen_improved_2023, bortoli_convergence_2022}. These efforts have largely concentrated on providing bounds on the quality of samples they generate  assuming that the associated learning task can be performed with a certain level of accuracy. More recently, several studies have focused on analyzing the learning process as well \cite{chen_score_2023, oko_diffusion_2023, cui_analysis_2024, shah_learning_2023}.


%In this study, we focus on diffusion models and provide a detailed characterization of their learning task. The learning task in diffusion models involves accurately estimating the \textit{score} function associated with perturbed versions of the target distribution. 
In this study, we focus on diffusion models and provide a detailed characterization of their learning task. It involves accurately estimating the \textit{score} function associated with perturbed versions of the target distribution. 
%Several works have examined the theoretical aspects of diffusion models. These efforts have largely concentrated on providing bounds on the quality of samples they generate \cite{chen_sampling_2022, benton_nearly_2023,chen_improved_2023, bortoli_convergence_2022} assuming that the associated learning task can be performed with a certain level of accuracy. 
Several studies have focused on analyzing the learning process in diffusion models \cite{chen_score_2023, oko_diffusion_2023, cui_analysis_2023,cui_precise_2025, shah_learning_2023}. These efforts have largely concentrated on the generalization properties of the learned score. However, a theoretical study of memorization aspects of diffusion models has been done only with the \textit{empirical optimal score} function \cite{biroli_dynamical_2024, achilli_losing_2024} (see Section~\ref{sec:empirical_opt_score}). On the other hand, empirical results have demonstrated the occurrence of memorization when neural networks are employed, as is common in practice \cite{somepalli_diffusion_2023,somepalli_understanding_2024,carlini_extracting_2023,zhang_emergence_2024,yoon_diffusion_2023,gu_memorization_2023,ross_geometric_2024}. This gap in theory and practice raises an important question: can the phenomenon of memorization be theoretically shown when using a parametric class of functions for the score function? This is precisely the context of our study. By analyzing the learning process of a specific diffusion model instance, we provide insights into generalization and memorization in diffusion models. 

% We theoretically study an instance of diffusion model, giving rise to a precise asymptotic characterization its learning task. Our results provides insights into the emergence of memorization phenomenon.   

\subsection{Main Contributions}\label{sec:main_contributions}
In this section, we provide a brief overview of the key contributions and findings of our work. Our study focuses on the learning aspect of diffusion models, which is to learn the score function of perturbed versions of a target distribution $P_0$, given in (\ref{eqn:marginal_prob_forward}). The score function is obtained by minimizing a loss function called \textit{denoising score matching} objective (see \ref{eqn:dsm_mfinite}) over a parametric class of functions. The class of functions that we consider is the random features neural network, and the target distribution is the $d$-dimensional standard Gaussian distribution. Let $n$ denote the number of samples, $d$ the data dimension and $p$ the number of features of random feature neural network. We operate in a regime where $d,n,p\rightarrow\infty$, while the ratios $\psi_n = \frac{n}{d}$ and $\psi_p=\frac{p}{d}$ are kept fixed. The denoising score matching involves an additional parameter $m$, which is the number of noise samples per data sample used in forming the loss function, Eq.~\eqref{eqn:dsm_mfinite}. In this setting, we make the following contributions:  
% 1) We analytically compute the  test and train errors of the minimizer of denoising score matching loss.
% 2) Using the obtained test and train errors, we study the generalization and memorization behavior in diffusion models.
% 3) We show that a crossover transition between generalization and memorization behaviors occur when the number of features equals the number of samples. .
% 4) We demonstration that increasing the value of $m$ enhances generalization when $p<n$, while it intensifies memorization when $p>n$.
\begin{itemize}
    \item We analytically compute the  test and train errors of the minimizer of denoising score matching loss.
    \item Using the obtained test and train errors, we study the generalization and memorization behavior in diffusion models.
    \item We show that a crossover transition between generalization and memorization behaviors occurs when the number of features equals the number of samples. %This is in contrast to the $\bigO{e^d}$ samples required for generalization when the empirical optimal score function is used.
    \item We demonstrate that increasing the value of $m$ enhances generalization when $p<n$, while it intensifies memorization when $p>n$.
\end{itemize}

Our observations and findings can be schematically summarized in the phase diagram presented in Fig.~\ref{fig:phase_diagram}. 
\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.5]{figs/phase_diagram_mp.pdf}
    \caption{Phase diagram showing regimes of generalization and memorization. The gradient in color with $m$ indicates the change in strength of the phenomenon.}
    \label{fig:phase_diagram}
\end{figure}

%We consider the simple isotropic Gaussian as the target distribution for two primary reasons. First, when studying the memorization phenomenon, the exact form of the distribution is less critical, as the focus lies on the specific samples used during training. Second, choosing a Gaussian target distribution provides an analytically tractable setting, allowing for a more precise theoretical analysis.
We consider the simple isotropic Gaussian as the target distribution for two primary reasons. First, when studying the memorization phenomenon, the exact form of the distribution is less critical, as the focus lies on the specific samples used during training. Second, this choice provides an analytically tractable setting, allowing for a more precise theoretical analysis.
\subsection{Related Works}
\paragraph{Diffusion models} Diffusion models \cite{sohl-dickstein_deep_2015,song_generative_2019,ho_denoising_2020,song_score-based_2020} rely on the non-equilibrium dynamics of a diffusion process for generative modeling. Since their introduction, there were several improvements \cite{dhariwal_diffusion_2021,rombach_high-resolution_2022,ho_classifier-free_2021,nichol_glide_2022} that led them to become the state-of-the-art in generative modeling of images.  Further, the design aspects of diffusion models was studied in \cite{karras_elucidating_2022}.
\paragraph{Sampling accuracy, generalization, and memorization} Several works have investigated the theoretical aspects of diffusion models. Sampling accuracy of the generative process in terms of distance from the target distribution was derived in \cite{chen_sampling_2022,benton_nearly_2023,chen_improved_2023,bortoli_convergence_2022} for various settings. These works assume that the score function has been learned \emph{a priori} with a certain level of accuracy. The score learning process was studied in \cite{cui_analysis_2023,shah_learning_2023,han_neural_2023}. The works~\cite{kadkhodaie_generalization_2023,chen_score_2023,wang_evaluating_2024} performed an end-to-end study of diffusion models giving a better understanding of their generalization properties. Further, the critical nature of feature emergence in diffusion models was studied in \cite{li_critical_2024,sclocchi_phase_2025}.
More recently, statistical physics tools were employed to study the memorization phenomenon in the high-dimensional regime for models using the empirical optimal score function~\cite{biroli_dynamical_2024,raya_spontaneous_2023,ambrogioni_search_2024,achilli_losing_2024}. 
%\paragraph{Sampling accuracy, generalization, and memorization} Several works have investigated the theoretical aspects of diffusion models. Sampling accuracy of the generative process in diffusion models in terms of distance from the target distribution was derived in \cite{chen_sampling_2022,benton_nearly_2023,chen_improved_2023,bortoli_convergence_2022} for various settings. These works assume that the score function has been learned a \emph{priori} with a certain level of accuracy. The score learning process was studied in \cite{cui_analysis_2024,shah_learning_2023,han_neural_2023}. The works \cite{kadkhodaie_generalization_2024,chen_score_2023,wang_evaluating_2024} performed an end-to-end study of diffusion models giving a better understanding of their generalization properties.
%More recently, statistical physics tools were employed to study the memorization phenomenon in diffusion models that use empirical optimal score function, in the large data regime \cite{biroli_dynamical_2024,raya_spontaneous_2023,ambrogioni_search_2023,achilli_losing_2024}. 


\subsection{Our Techniques}
In the setting briefly described in Section~\ref{sec:main_contributions} and to be detailed in Section~\ref{sec:main_results}, we analytically compute the test and train errors of the minimizer of the denoising score matching loss. Our theoretical results are summarized in Theorem~\ref{thm:lc_minf} and Theorem~\ref{thm:lc_m1}. Two main ingredients allowing the computation of exact asymptotic learning curves are the \textit{Gaussian equivalence principle} \cite{gerace_generalisation_2020,goldt_gaussian_2022,hu_universality_2023} and the theory of \textit{linear pencils} \cite{far_spectra_2006,helton_applications_2018,adlam_neural_2020,bodin_random_2024}. We briefly describe them here.
\paragraph{Gaussian Equivalence Principle}
Suppose $W\in\R^{p\times d}$, and $X\in\R^{d\times n}$ are random matrices with i.i.d. Gaussian entries. Let $\varrho$ be a non-linear function and let $F=\varrho\left(\rat{W}{d}X\right)$, where $\varrho$ acts element-wise on a matrix. Then the Gaussian equivalence principle states that in the calculation of test and train errors, it is asymptotically equivalent to substitute $F$ with the matrix $\hat{F} = \mu_0\bone_p\bone_n^T+\mu_1\rat{W}{d}X+v\Omega$, where $\Omega\in\R^{p\times n}$ is a random matrix with i.i.d. Gaussian entries, independent of $W, X$. The coefficients are given by $\mu_0 = \bE{g}{\varrho(g)},\; \mu_1 = \bE{g}{\varrho(g)g},\; v^2 = \bE{g}{\varrho(g)^2}-\mu_0^2-\mu_1^2$, where $g\sim\cN{0,1}$ \;.
%\paragraph{Linear Pencils} The theory of linear pencils is a powerful technique that allows us  to compute traces of rational functions involving random matrices with Gaussian entries. It amounts to constructing an appropriate block matrix called \textit{linear pencil}, whose inverse gives the desired rational functions. For further details on the linear pencils method, we refer the reader to Chapter~3 of \cite{bodin_random_2024} .
\paragraph{Linear Pencils} The theory of linear pencils is a powerful technique allowing to compute traces of rational functions involving random matrices with Gaussian entries. It amounts to constructing an appropriate block matrix called \textit{linear pencil}, whose inverse gives the desired rational functions. For further details on the linear pencils method, we refer the reader to Chapter~3 of \cite{bodin_random_2024} .
%A short exposition of the linear pencils method can be found in Appendix~\ref{appndx:linear_pencils}.

 %In the present context, the test and train errors can be expressed as a sum of traces of rational functions of random matrices. However, some of these random matrices have non-Gaussian entries due to the presence of an activation function $\varrho$. We circumvent this using the Gaussian equivalence principle. Subsequently, we use the linear pencils method to compute the traces.

In our context, the test and train errors can be expressed as a sum of traces of rational functions of random matrices. Some of these matrices have non-Gaussian entries due to the presence of an activation function $\varrho$. We circumvent this by using the Gaussian equivalence principle and, subsequently, the linear pencils method to compute the traces.
 
 %albeit non-linear due to the presence of an activation function $\varrho$. We linearize the terms using Gaussian equivalence principle. Subsequently, we use the linear pencils method to compute the traces.
% \paragraph{Notations}
% We denote the $d$-dimensional Gaussian distribution with mean $\mu$ and covariance $\Sigma$ by $\cN{\mu,\Sigma}$. The $d$-dimensional identity matrix is denoted by $I_d$. $\norm{\cdot}$ denotes the $l_2$ norm of a vector, while $\norm{\cdot}_F$ denotes the Frobenius norm of a matrix. The operator $\nabla$ represents the gradient of a scalar function. 
\subsection{Notations}
We denote the $d$-dimensional Gaussian distribution with mean $\mu$ and covariance $\Sigma$ by $\cN{\mu,\Sigma}$. The $d$-dimensional identity matrix is denoted by $I_d$, and $\bone_d$ denotes the $d$-dimensional all ones vector. $\norm{\cdot}$ denotes the $l_2$ norm of a vector, while $\norm{\cdot}_F$ denotes the Frobenius norm of a matrix. The operator $\nabla$ represents the gradient of a scalar function. 
\section{Preliminaries}

\subsection{Diffusion Models}
Consider a set of $n$ i.i.d. samples $\cS=\{x_1,x_2,\cdots,x_n\}$ from an unknown distribution $P_0$ on $\R^d$. Generative modeling aims to leverage the information in $\cS$ to draw new samples from $P_0$. Diffusion models address the problem by time reversing a diffusion process that transports $P_0$ to a known distribution such as the isotropic Gaussian. 
In this work, we let the forward process to be an Ornstein-Uhlenbeck (OU) process~\cite{gardiner_stochastic_2009} governed by the following stochastic differential equation (SDE):
\begin{equation}\label{eqn:forward_ou}
    \dd X_t = - X_t \;  \dd t +\sqrt{2} \; \dd B_t\;,\quad X_0\sim P_0\;.
\end{equation}
Here, $B_t$ is a standard $d$-dimensional Brownian motion. The distribution of $X_t$ given $X_0$ can be computed in closed form and is given by $\cN{a_tX_0,h_tI_d}$, where $a_t=e^{-t}$, and $h_t=1-e^{-2t}$ (hence, $a_t^2+h_t=1$). As $t\to\infty$, the distribution of $X_t$ converges to the $d$-dimensional standard Gaussian, regardless of $X_0$, since $a_t\to 0$ and $h_t\to 1$. 
%Note that, as $t\rightarrow\infty$, we have $a_t\rightarrow 0, h_t\rightarrow 1$. So, irrespective of $X_0$, the distribution of $X_t$ tends to the standard $d-$dimensional Gaussian as $t\rightarrow \infty$. 

Let $P_t$ denote the probability distribution of $X_t$:
\begin{equation}\label{eqn:marginal_prob_forward}
    P_t(x) = {(2\pi h_t)^{-d/2}}\int_{\R^d}  \dd P_0(x_0) \; e^{-\frac{\norm{x-a_tx_0}^2}{2h_t}}\;.
\end{equation}
Then, for a fixed $T>0$ and $Y_T\sim P_T$, we define the \textit{time reversal} of the forward process (\ref{eqn:forward_ou}) as 
\begin{equation}\label{eqn:backward_sde}
    -\dd Y_t = \left(Y_t+2\nabla\log P_{t}(Y_t)\right) \;  \dd t+\sqrt{2} \; \dd \tilde{B}_t\quad \;,
\end{equation}
where the SDE runs backward in time starting from $Y_T$, and $\tilde{B}_t$ is a different instance of standard Brownian motion. 
The term \emph{time reversal} here means that the distributions of $Y_t$ and $X_t$ are identical for every $t$ \cite{anderson_reverse-time_1982, haussmann_time_1986}.
% When we say that $Y_t$ is a time reversal \cite{anderson_reverse-time_1982, haussmann_time_1986} of $X_t$, we mean that their distributions are identical for every $t$. 
%This can be verified by writing the Fokker-Planck equations satisfied by the time marginals of processes $X_t$ and $Y_t$. 
If we initiate the backward process with $Y_T\sim P_T$, the distribution of $Y_0$ will be $P_0$. However, since $P_T$ is unknown due to the lack of knowledge of $P_0$, we instead start the reverse process with $Y_T\sim\cN{0,I_d}$. This approximation introduces minimal error, owing to the exponentially fast convergence of the OU process to $\cN{0,I_d}$. 

The main ingredient required to implement the backward process in (\ref{eqn:backward_sde}) is $\nabla\log P_t$, known as the \textit{score} function of $P_t$. In diffusion models, the learning task amounts to estimating the function $\nabla\log P_t$ using the dataset $\cS$ of samples drawn from $P_0$. We consider minimizing the following score matching \cite{hyvarinen_estimation_2005} objective for this:
\begin{equation*}
    \cL_{\text{SM}}(s) = \int_0^T  \dd t \; \bE{x_t\sim P_t}{\norm{s(t,x_t)-\nabla\log P_t(x_t)}^2} .
\end{equation*}
The above loss function is not practical, as $\nabla\log P_t(x)$ is unknown. However, it is possible to construct an equivalent objective, the denoising score matching (DSM) loss \cite{vincent_connection_2011}:
\begin{align*}
    \cL_{\text{DSM}}(s) &= \int_0^T  \dd t \; w(t)\shortexpect\norm{s(t,x_t)-\nabla\log P_t(x_t|x_0)}^2 ,
\end{align*}
where $w$ is a weighting function and the expectation is with respect to $x_0$ and $x_t$. 
%Here, by an abuse of notation, $P_t$ also denotes the joint {\color{blue} conditional ?} probability distribution of the process $X_t$.
In Appendix~\ref{appndx:dsm_sm_equiv} we show that $\cL_{\text{DSM}}$ is equal to $\cL_{\text{SM}}$ up to a time dependent scaling factor and offset. Hence, the minimizer of $\cL_{\text{DSM}}$ is same as the minimizer of $\cL_{\text{SM}}$. Following \cite{song_score-based_2020}, we choose the weighting function to be $w(t) = (\shortexpect_{x_0,x_t}{\norm{\nabla\log P_t(x_t|x_0)}^2})^{-1}$.
For OU process, we can compute $\nabla\log P_t(x_t|x_0)$ in closed form. We can write $x_t\sim P_t$ as  $x_t = a_t x_0+\sqrt{h_t}z$, where $x_0\sim P_0,\; z\sim\cN{0,I_d}$ are independent rvs and $a_t=e^{-t}$,\; $h_t=1-e^{-2t}$. Consequently, $\nabla\log P_t(x_t|x_0) = -\frac{(x_t-a_tx_0)}{h_t}=-\frac{z}{\sqrt{h_t}}$. The weight function is given by $w(t)=\frac{h_t}{d}$. Substituting these, we can write the $\cL_{\text{DSM}}$ as
\begin{align*}
    \cL_{\text{DSM}}(s) &= \int_0^T  \dd t  \frac{1}{d}\shortexpect\norm{\sqrt{h_t}s(t,a_tx_0+\sqrt{h_t}z)+z}^2,
\end{align*}
where the expectation is with respect to $x_0$ and $z$. Since $P_0$ is unknown and only samples from it are available, we use an empirical estimate to approximate the expectation with respect to $x_0$. With $y_i^t(z)=a_tx_i+\sqrt{h_t}z$, we define
\begin{align}\label{eqn:dsm_minf}
    \cL_{\text{DSM}}^\infty(s) &= \int_0^T \dd t \; \frac{1}{dn}\sum_{i=1}^{n}{\shortexpect_{z}{\norm{\sqrt{h_t}s(t,y_i^t(z))+z}^2}} .
\end{align}
When $s$ is a complicated function such as a neural network, it is difficult to compute even the expectation with respect to $z$. If we use an empirical estimate for the expectation with respect to $z$ as well, we get the following loss function, 
\begin{align}\label{eqn:dsm_mfinite}
    \cL_{\text{DSM}}^m(s) &= \int_0^T \dd t \; \frac{1}{dnm}{\sum_{i,j=1}^{n,m}\norm{\sqrt{h_t}s(t,y^t_{ij})+z^t_{ij}}^2} 
\end{align}
where $y^t_{ij} = a_tx_i+\sqrt{h_t}z^t_{ij}$.
\subsection{Empirical Optimal Score and Memorization}\label{sec:empirical_opt_score}
%Consider the loss function given in (\ref{eqn:dsm_minf}). There is a unique minimizer for this loss function, and it is given by 
Consider the loss function given in (\ref{eqn:dsm_minf}). It has an unique minimizer: 
\begin{equation}\label{eqn:empirical_opt_score}
    s^e(t,x) = \nabla\log P^e_t(x) \;,
\end{equation}
where $P^e_t$ is given by
\begin{equation}\label{eqn:marginal_prob_empirical}
    P_t^e(x) = \frac{1}{n(2\pi h_t)^{d/2}}\sum_{i=1}^ne^{-\frac{\norm{x-a_tx_i}^2}{2h_t}} \;.
\end{equation}
The score $s^e$ is often referred to as the \textit{empirical optimal score}. A backward process using this score converges in distribution to the empirical distribution of the dataset $\cS$ as $t\rightarrow 0$. That is, the backward process collapses to one of the data samples as $t\rightarrow 0$. This is intimately connected to the memorization phenomenon in diffusion models as explored in \cite{biroli_dynamical_2024}, although their study focuses on the regime where $n=\bigO{e^d}$. 

%\textit{Inspired by this connection, we define memorization in diffusion models as occurring when the score function learned using denoising score matching closely approximates the empirical optimal score function instead of the exact score.} 
\textit{Inspired by this connection, we define memorization as occurring when the score function learned using denoising score matching closely approximates the empirical optimal score function instead of the exact score.} 


\subsection{Random Features Model}
In practice, the score function $s$ is typically chosen from a parametric class of functions, and the DSM objective (\ref{eqn:dsm_mfinite}) is minimized within this class, with appropriate regularization. In this work, we represent the score function using a \textit{random features} neural network (RFNN) \cite{rahimi_random_2007}. A RFNN is a two-layer neural network in which the first layer weights are randomly chosen and fixed, while the second layer weights are learned during training. It is a function from $\R^d$ to $\R^d$ of the form
$s_A(x|W) = \rat{A}{p}\act{\rat{W}{d}x},$
where $W\in\R^{p\times d}$ is a random matrix with its elements chosen i.i.d. from $\cN{0,1}$, $\varrho$ is an activation function acting element-wise and $A\in\R^{d\times p}$ are the second layer weights that need to be learned. The RFNN is a simple neural network amenable to theoretical analysis and is able to capture interesting characteristics observed in more complicated neural network models, such as the double descent curve related to overparametrized regimes~\cite{mei_generalization_2022,bodin_model_2021}.