\section{Main Results}\label{sec:main_results}
We study the denoising score matching loss $\cL_{\text{DSM}}^m$ given in (\ref{eqn:dsm_mfinite}) when the score function is modeled using a RFNN and $P_0\equiv\cN{0,I_d}$. % is a $d$-dimensional standard Gaussian distribution. 
Our results characterize the asymptotic learning curves (test and train errors) of the minimizer of $\cL_{\text{DSM}}^m$ (\ref{eqn:dsm_mfinite}) in this setting. We analytically compute the learning curves for two extreme values of $m$: $m=1$ and $m=\infty$. These correspond to the extremes of the number of noise samples per data sample used during score learning. Based on the derived learning curves, we discuss the generalization and memorization behaviors in diffusion models.%, a phenomenon observed in several previous works (see, e.g., \cite{yoon_diffusion_2023}).

We assume that at each time instant $t$, a different instance of RFNN is used to learn the score function specific to that time. Although this is a simplification compared to the methods employed in practice, it has been adopted in prior theoretical studies, e.g.,~\cite{cui_analysis_2023}. When using a different instance of RFNN at each $t$, note that minimizing the DSM loss (\ref{eqn:dsm_mfinite}) is equivalent to minimizing the integrand therein at each time instant. Henceforth, we focus on minimizing the DSM loss for a fixed time instant $t$. Introducing a regularization parameter $\lambda>0$, the loss function becomes:
\begin{align}\label{eqn:dsm_loss_rfm}
    \cL_t^m(A_t) &= \frac{1}{dnm}\sum_{i,j=1}^{n,m}{\norm{\sqrt{h_t}\rat{A_t}{p}\act{\rat{W_t}{d}y^t_{ij}}+z^t_{ij}}^2}\nonumber\\
    &\qquad+\frac{h_t\lambda}{dp}\norm{A_t}_F^2 \;,
\end{align}
where we recall that $y^t_{ij} = a_tx_i+\sqrt{h_t}z^t_{ij}$ with $z^t_{ij}\sim\cN{0,I_d}$. We emphasize that $W_t$ is a different and independent random matrix for each time $t$, and $A_t$ is learned separately at each $t$.
Denote $\hat{A}_t$ as the second layer weights learned by some learning algorithm minimizing (\ref{eqn:dsm_loss_rfm}). The respective performance is evaluated by the test and train errors defined as follows:
\begin{align*}
    \cE^m_{\text{test}}(\hat{A}_t) &= \frac{1}{d}\shortexpect_{x\sim P_t}{\norm{\rat{\hat{A}_t}{p}\act{\rat{W_t}{d}x}-\nabla\log P_t(x)}^2}\;,\\
    \cE^m_{\text{train}}(\hat{A}_t) &= \frac{1}{dnm}\sum_{i=1}^{n}{\sum_{j=1}^{m}\norm{\sqrt{h_t}\rat{\hat{A}_t}{p}\act{\rat{W_t}{d}y^t_{ij}}+z^t_{ij}}^2}.
\end{align*}
The quantities $\cE^m_{\text{test}}$ and $\cE^m_{\text{train}}$ are random, where the randomness comes from $W_t$, $\{x_i\}_{i=1}^n$, and $\{z^t_{ij}\}_{i,j=1}^{n,m}$. We expect them to concentrate on their expectations as $d\to\infty$.  

The rationale behind studying the test and training errors of DSM to assess the performance of diffusion models stems from their direct connection to the model's generative accuracy. Specifically, as described in \cite{song_maximum_2021}, the error in diffusion models can be upper bounded using the test error. Suppose $\hat{P}_t$ denotes the probability distribution of the backward process when we use the learned score function instead of the true score, and assume that $\hat{P}_T = P_T$. Then, the Kullback-Leibler (KL) divergence between $P_0$ and $\hat{P}_0$ can be upper bounded as
    $D_{KL}(P_0||\hat{P}_0)\le \frac{d}{2}\int_0^T\dd t\; \cE^m_{\text{test}}(\hat{A}_t).$

\subsection{Infinite Noise Samples per Data Sample: $m=\infty$}
For $m=\infty$, the average over $z^t_{ij}$ in the DSM loss becomes an expectation. Letting $y^t_i(z) = a_tx_i+\sqrt{h_t}z$, we write
\begin{align}\label{eqn:dsm_loss_rfm_minf}
    \cL_t^\infty(A_t) &= \frac{1}{dn}\sum_{i=1}^{n}{\shortexpect_{z}{\norm{\sqrt{h_t}\rat{A_t}{p}\act{\rat{W_t}{d}y^t_i(z)}+z}^2}}\nonumber\\
    &\qquad +\frac{h_t\lambda}{dp}\norm{A_t}_F^2 \;.
\end{align}
We characterize the minimizer of the above loss function in the asymptotic regime where $d,n,p\rightarrow\infty$, while keeping the ratios $\psi_n = \frac{n}{d}$ and $\psi_p = \frac{p}{d}$ fixed. We make the following assumption about the the activation function $\varrho$.

\begin{assumption}\label{assmptn:activation_fn}
   The activation function $\varrho$ has a Hermite polynomial expansion given by $\varrho(y) = \sum_{l=0}^\infty\mu_l \text{He}_l(y)$, where $\text{He}_l$ is the $l^{th}$ Hermite polynomial. For ease of presentation, assume that $\mu_0=0$. The $L_2$ norm of $\varrho$ with respect to standard Gaussian measure is denoted by $\norm{\varrho}$, with $P^\gamma$ denoting the bivariate standard Gaussian distribution with correlation coefficient $\gamma$ (see Eq.~\eqref{eqn:bivariate_gaussian_pdf} in the Appendix~\ref{appndx:lc_minf_proof}). The function $c$ is defined as $c(\gamma) = \bE{u,v\sim P^\gamma}{\varrho(u)\varrho(v)}$.  
\end{assumption}
 
With these notations and assumptions, the following theorem characterizes the test and train errors of the denoising score matching loss for $m=\infty$ case.
\begin{theorem}\label{thm:lc_minf}
    Suppose $P_0\equiv\cN{0,I_d}$ and $\varrho$ satisfies Assumption~\ref{assmptn:activation_fn}. Let $s^2 = \norm{\varrho}^2-c(a_t^2)-h_t\mu_1^2$,\; $v_0^2=c(a_t^2)-a_t^2\mu_1^2$, and $v^2 = \norm{\varrho}^2-\mu_1^2$. Let $\psi_n = \frac{n}{d}$, and $\psi_p = \frac{p}{d}$. Let $\zeta_1,\zeta_2,\zeta_3,\zeta_4$ be the solution of the following set of algebraic equations as a function of $q$ and $z$.
    \begin{align*}
    \zeta_1(s^2-z+a_t^2\mu_1^2\zeta_2\zeta_4+v_0^2\zeta_2+(h_t\mu_1^2+q)\zeta_4) -1= 0,\nonumber\\
    \zeta_2(\psi_n+v_0^2\psi_p\zeta_1-a_t\mu_1\zeta_3)-\psi_n =0,\nonumber\\
    a_t\mu_1\psi_p\zeta_1(1+a_t\mu_1\zeta_2\zeta_3)+(1+(h_t\mu_1^2+q)\psi_p\zeta_1)\zeta_3 = 0,\nonumber\\
    a_t^2\mu_1^2\psi_p\zeta_1\zeta_2\zeta_4+(1+(h_t\mu_1^2+q)\psi_p\zeta_1)\zeta_4 -1= 0.\nonumber\\
    \end{align*}
    Define the function $\cK(q,z) = -\frac{\zeta_3(q,z)}{a_t\mu_1}$. Let $e_1 = \cK(0,-\lambda),\; e_2 = -\frac{\partial \cK}{\partial q}(0,-\lambda),\; e_3 = \frac{\partial \cK}{\partial z}(0,-\lambda)$. Then, for the minimizer of (\ref{eqn:dsm_loss_rfm_minf}) $\hat{A}_t$, as $d,n,p\to\infty$: 
    \begin{align*}
        \lim_{d,n,p\to\infty}\bE{}{\cE^\infty_{\text{test}}(\hat{A}_t)} &= 1-2\mu_1^2e_1+\mu_1^4e_2+\mu_1^2v^2e_3 \;,\\
    \lim_{d,n,p\to\infty}\bE{}{\cE^\infty_{\text{train}}(\hat{A}_t)} &= 1-h_t\mu_1^2e_1-\lambda h_t\mu_1^2e_3 \;.
    \end{align*}
\end{theorem}
We defer the proof to Appendix~\ref{appndx:lc_minf_proof}. 
\begin{remark}
    We expect $\cE^\infty_{\text{test}}(\hat{A}_t)$ and $\cE^\infty_{\text{train}}(\hat{A}_t)$ to concentrate on their expectations as $d\to\infty$. However, a rigorous proof of this result is beyond the scope of the current work. Henceforth, we use the term test (train) error for both $\cE^\infty_{\text{test}}$ ($\cE^\infty_{\text{train}}$) and $\bE{}{\cE^\infty_{\text{test}}}$ ($\bE{}{\cE^\infty_{\text{train}}}$), interchangeably.
\end{remark}
Theorem~\ref{thm:lc_minf} can be used to compute the test and train errors for different values of $t,\psi_n,\psi_p$.
\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\linewidth]{figs/rfm_err_analytical_lam0p001_relu_nfixed_minf.pdf}
    \caption{Learning curves for $m=\infty$, with $\psi_n=20.0,\lambda=0.001$. The activation function is ReLU.}
    \label{fig:lc_minf_rp}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\linewidth]{figs/rfm_err_analytical_lam0p001_relu_tfixed_minf_single.pdf}
    \caption{Learning curves for $m=\infty$, with $t=0.01,\lambda=0.001$, $\varrho\equiv $ReLU. The solid lines are for test error and dashed lines are for train error. The dotted vertical lines are for $\psi_p=\psi_n$.}
    \label{fig:lc_minf_tfixed}
\end{figure}

Fig.~\ref{fig:lc_minf_rp} shows the learning curves as a function of $t$ for different values of $\psi_p$ and a fixed $\psi_n$. We can comprehend them by decomposing $\cE^\infty_{\text{train}}(\hat{A}_t)$ into bias and variance components through the following Lemma.
\begin{lemma}\label{lem:bias_variance_minf}
    Suppose $\cM_t$ and $\cV_t$ are given by
\begin{align*}
    \cM_t &= \frac{1}{dn}\sum_{i=1}^{n}{\shortexpect_{z}{\norm{\rat{\hat{A}_t}{p}\act{\rat{W_t}{d}y_i(t,z)}-s^e(t,y^t_i(z))}^2}} \;, \\
    \cV_t &= \frac{1}{dn}\sum_{i=1}^{n}{\shortexpect_{z}{\norm{\sqrt{h_t}s^e(t,y^t_i(z))+z}^2}}\;,
\end{align*}
where $s^e$ is the empirical optimal score function given in (\ref{eqn:empirical_opt_score}). Then, $\cE^\infty_{\text{train}}(\hat{A}_t) = \cV_t+h_t\cM_t.$
\end{lemma}
%A proof of Lemma~\ref{lem:bias_variance_minf} can be found in Appendix~\ref{appndx:bias_var_proof}.
Lemma~\ref{lem:bias_variance_minf} is proved in Appendix~\ref{appndx:bias_var_proof}.

 We discuss the behavior of $\cM_t$ and $\cV_t$ for different values of $t$, thereby explaining the learning curves in Fig.~\ref{fig:lc_minf_rp}. The parameter $\psi_p$ is used to control the approximation power of RFNNs. A general observation is that, since $\cV_t$ is independent of $\hat{A}_t$, any change in $\cE^\infty_{\text{train}}(\hat{A}_t)$ as $\psi_p$ varies must be attributed to the changes in $\cM_t$. A smaller value of $\cM_t$ indicates that the learned score function is closer to the empirical optimal score function, as its definition suggests. A score function close to the empirical optimal score function can negatively affect the generalization performance of diffusion models. This can be understood as follows: 
 %For large $t$, we have $a_t\approx 0,h_t\approx 1$, which gives that $s^e(t,x)\approx -x$.
 For small $t$, $h_t\approx 0$, and hence, in the neighborhood of $a_tx_i$, $P_t^e$ is dominated by the $i^{th}$ term in the summation (\ref{eqn:marginal_prob_empirical}). Therefore, in the vicinity of $a_tx_i$, we have $s^e(t,x)\approx -\frac{x-a_tx_i}{h_t}$. In the backward process, this translates to a component in the drift that points towards $a_tx_i$. Consequently, the trajectories will tend to move toward training samples, causing the output of the backward process to resemble one of the training samples. This behavior is referred to as \textit{memorization}, which occurs when the learned score function closely approximates the empirical optimal score function. With this understanding, we now qualitatively discuss the learning curves in Fig.~\ref{fig:lc_minf_rp} for different values of $t$.
\begin{itemize}
    \item \textbf{$t=\infty$}: At $t=\infty$, we have $a_t=0,h_t=1$ and $s^e(t,y)=-y$. Substituting these, we get
        $\cE^\infty_{\text{train}}(\hat{A}_t) = \cM_t = \shortexpect_z\norm{\rat{\hat{A}_t}{p}\act{\rat{W_t}{d}z}+z}^2.$
    Hence, the train and test errors are equal, and depends only on how well the RFNN can approximate a linear function. As $\psi_p$ increases, the approximation power of the RFNN increases, and thus the train and test errors decrease. 

    \item \textbf{$t\gg1$}: In this regime, we still have $s^e(t,y)\approx -y$, giving $\cV_t\approx a_t^2$. Hence, $\cE^\infty_{\text{train}}(\hat{A}_t)\approx h_t\cM_t+a_t^2$. When $\psi_p$ is large, $\cM_t$ is small, and the train error is dominated by the $a_t^2$ term. Therefore, we see that train error increases rapidly as $t$ decreases. However, the test error remains constant, since it is approximately equal to $\cM_t$.

    \item \textbf{$t\ll1$}: At these times, the empirical optimal score function satisfies $s^e(t,a_tx_i+\sqrt{h_t}z)\approx -\frac{z}{\sqrt{h_t}}$ for any data point $x_i$. Consequently, $\cV_t\approx 0$. This leads to the result $\cE^\infty_{\text{train}}(\hat{A}_t)\approx h_t\cM_t$. In this regime, the train error depends on how well RFNN can approximate the empirical optimal score function $s^e$. With an increase in $\psi_p$, the approximation power increases and the train error decreases. However, since the actual score function significantly deviates from the empirical score function in this regime, the test error increases rapidly with $\psi_p$. In the neighborhood of $a_tx_i$, learning the empirical optimal score function instead of the actual score makes the drift in the backward process to pull the trajectories towards $a_tx_i$. Thus, if the backward trajectory comes in the vicinity of $a_tx_i$ at time $t$, the diffusion model tend to recover the sample $x_i$. %This is referred to as memorization.

    % \item \textbf{$t\approx 0$}: In this regime we still have $s^e(t,a_tx_i+\sqrt{h_t}z)\approx -\frac{z}{\sqrt{h_t}}$. However, due to regularization, $\hat{A}_t$ cannot take very large values. So, $\cM_t$ scales as $\frac{1}{h_t}$. This makes $\cE^\infty_{\text{train}}(\hat{A}_t)\approx 1$ for very small values of $t$. When $\psi_p$ is large, the test error remains high due to the memorization phenomenon.
\end{itemize}
The analysis of Fig.~\ref{fig:lc_minf_rp} hints that the RFNN starts to show memorization behavior for $t<1$ and large values of $\psi_p$. To explore this further, we plot in Fig.~\ref{fig:lc_minf_tfixed} the learning curves as a function of $\psi_p$ for different values of $\psi_n$ and keeping $t$ fixed and small.
From the plot, we observe the following: 1) For $\psi_p\ll\psi_n$, the train error remains constant and test error is small, indicating generalization. 2) For $\psi_p\gg\psi_n$, the train error is small and test error is high indicating the presence of memorization. 3) In the region $\psi_p\approx\psi_n$, the test error rises rapidly, signaling the onset of memorization.

Thus, $\psi_p=\psi_n$ acts as a crossover point at which the behavior of the score transitions from generalization phase to memorization phase. This transition is depicted in Fig.~\ref{fig:phase_diagram} as a phase diagram. We note that this shift in behavior is not a sharp transition but rather a gradual change.   

More figures illustrating the effects of $\lambda$ and the activation function $\varrho$ on test and train errors can be found in Appendices~\ref{appndx:lc_lambda} and \ref{appndx:lc_activation}.
\subsection{Single Noise Sample per Data Sample: $m=1$}
%We now focus on the $m=1$ case. With $y^t_i = a_tx_i+\sqrt{h_t}z^t_i$, we can write the denoising score matching loss as
For $m=1$, the DSM loss~\eqref{eqn:dsm_loss_rfm} reduces to
\begin{align}\label{eqn:dsm_loss_rfm_m1}
    \cL^1(A_t) &= \frac{1}{dn}\sum_{i=1}^{n}{\norm{\sqrt{h_t}\rat{A_t}{p}\act{\rat{W_t}{d}y^t_i}+z^t_i}^2}\nonumber\\
    &\qquad +\frac{h_t\lambda}{dp}\norm{A_t}_F^2 \;.
\end{align}
The following theorem characterizes the test and train errors of minimizer of the loss given in (\ref{eqn:dsm_loss_rfm_m1}).

\begin{theorem}\label{thm:lc_m1}
    Let $P_0\equiv\cN{0,I_d}$, and $\varrho$ satisfies Assumption~\ref{assmptn:activation_fn}. Let $v^2 = \norm{\varrho}^2-\mu_1^2$,\; $\psi_n = \frac{n}{d}$, and $\psi_p = \frac{p}{d}$. Let $\zeta_1,\zeta_2,\zeta_3,\zeta_4,\zeta_5$ be the solution of the following set of algebraic equations as a function of $q$ and $z$.
    \begin{align*}
    \zeta_1(-z+q\zeta_2+v^2\zeta_4+\mu_1^2\zeta_2\zeta_4) -1&= 0\;,\\
    \zeta_2(1+q\psi_p \zeta_1) +\mu_1^2\psi_p\zeta_1\zeta_2\zeta_4 -1&= 0\;,\\
    \zeta_3(1+q\psi_p \zeta_1) +\mu_1\psi_p\zeta_1(1+\mu_1\zeta_3\zeta_4) &= 0\;,\\
    \zeta_4(\psi_n+v^2\psi_p\zeta_1-\mu_1\zeta_3)-\psi_n &= 0\;,\\
    \zeta_4(-h_t\mu_1\zeta_3\zeta_4-1)+\psi_n\zeta_5 &= 0\;. 
\end{align*}
    Define the function $\cK(q,z) = 1-\psi_n\zeta_5(q,z)$. Let $e_1 = -\sqrt{h_t}\zeta_3(0,-\lambda)\zeta_4(0,-\lambda),\; e_2 = -\frac{\partial \cK}{\partial q}(0,-\lambda)$, and $e_3 = \frac{\partial \cK}{\partial z}(0,-\lambda)$. Then, for
    the minimizer of (\ref{eqn:dsm_loss_rfm_m1}) $\hat{A}_t$, as $d,n,p\to\infty$:
    \begin{align*}
        \lim_{d,n,p\to\infty}\bE{}{\cE^1_{\text{test}}(\hat{A}_t)} &= 1-\frac{2\mu_1}{\sqrt{h_t}}e_1+\frac{\mu_1^2}{h_t}e_2+\frac{v^2}{h_t}e_3\;,\\
    \lim_{d,n,p\to\infty}\bE{}{\cE^1_{\text{train}}(\hat{A}_t)} &= 1-\cK(0,-\lambda)-\lambda e_3\;.
    \end{align*}
\end{theorem}
The proof is presented in Appendix~\ref{appndx:lc_m1_proof}. Using the Theorem~\ref{thm:lc_m1}, we calculate the test and train errors as a function of $t,\psi_n$ and $\psi_p$, which is illustrated in Fig.~\ref{fig:lc_m1} in Appendix~\ref{appndx:learning_curves_m1}.% shows the plots thus obtained.

Fig.~\ref{fig:lc_m1_nfixed} shows the learning curves as a function of $t$ for different values of $\psi_p$ while keeping $\psi_n$ fixed. The learning curves reveal several notable trends. The test error increases as $t$ decreases; however, it shows a non-monotonic behavior with $\psi_p$. The train error decreases monotonically with increasing $\psi_p$ for all $t$, indicating the model's progressive capacity to interpolate the training data. 

Note that for small $t$, the test error remains at least two orders of magnitude smaller than in the $m=\infty$ case. Further, the test error decreases  as $\psi_p$ increases beyond $\psi_n$. These observations suggest that the model does not display memorization behavior when $m$ equals $1$. This contrasts the $m=\infty$ case, where memorization significantly impacts the test error. These findings indicate that larger values of $m$ increase the propensity of diffusion models to memorize the initial dataset, justifying the illustration in Fig.~\ref{fig:phase_diagram}.

The DSM loss (\ref{eqn:dsm_loss_rfm_m1}) shares similarities with the loss function used for RFNN in regression settings. Several works such as \cite{mei_generalization_2022, bodin_model_2021, hu_asymptotics_2024}, have analyzed the learning curves of RFNN in regression contexts. Notably, they predict the presence of a double descent phenomenon, where the test error exhibits a peak at $\psi_p=\psi_n$, followed by a decrease for $\psi_p<\psi_n$ or $\psi_p>\psi_n$. The point $\psi_p=\psi_n$ is called interpolation threshold. In our study, we also observe the double descent phenomenon in the DSM setting with $m=1$. This is depicted in Fig.~\ref{fig:lc_m1_tfixed}. 

\section{Numerical Experiments}
In this section, we validate the analytical predictions made in the previous section through simulations. In addition, we simulate the learning curves for intermediate values of $m$. Lastly, we present numerical results indicating memorization when the learned score function is used in the backward diffusion.
\begin{figure}[ht]
    \centering
     \includegraphics[width=1.0\linewidth]{figs/rfm_err_sim_lam0p001_test.pdf}
    \caption{Simulation results ($d=100$) for different values of $m$ and fixed $\psi_p=20$; with $\psi_n=2$ (upper plot) and $\psi_n=50$ (lower plot). Theoretical results for $m=1,\infty$ are depicted as solid lines.}
    \label{fig:lc_sim}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figs/rfm_mem_sim_lam0p001.pdf}
    \caption{Results of experiment on memorization. We use $N=5000$ and $\delta=1/3$. The dotted vertical lines are for $\psi_n=\psi_p$.}
    \label{fig:mem_sim}
\end{figure}

Fig.~\ref{fig:lc_sim} presents the test errors obtained numerically for various values of $m$ (corresponding train errors can be found in Fig.~\ref{fig:lc_sim_comp_appndx} in Appendix~\ref{appndx:lc_numerical_comparison}). The upper plot displays the learning errors for the case $\psi_p>\psi_n$, while the lower plot corresponds to $\psi_p<\psi_n$. Based on the previous discussions, memorization is expected when $\psi_p>\psi_n$, and this behavior is evident in Fig.~\ref{fig:lc_sim} when $t$ is small. Additionally, the extent of memorization increases with $m$, indicating that large $m$ is detrimental to generalization (small test error) in this regime. This is in contrast to the behavior of test and train errors in the $\psi_p<\psi_n$ regime, where the generalization improves as $m$ increases, evidenced by a decrease in the test error.

The solid lines in Fig.~\ref{fig:lc_sim} plots the analytical predictions derived in the previous section for $m=1$ and $m=\infty$. These predictions align closely with the numerical results for $m=1$ and $m=100$ respectively, validating its consistency with the observed data. Minor mismatches between theoretical and numerical curves in the small $t$ and large $t$ regimes can be attributed to finite size effects.

Next, through experiments, we show the effect of memorization when the score function learned using RFNN is used in the backward diffusion. We demonstrate that, in the memorization regime, once the backward diffusion process enters the vicinity of a data sample, it exhibits a tendency to remain within that neighborhood. Specifically, we simulate $N$ instances of the backward diffusion
\begin{equation*}
    -\dd Y_t = \left(Y_t+2s_{\hat{A}_t}(Y_t|W_t)\right) \;  \dd t+\sqrt{2} \; \dd \tilde{B}_t \;,
\end{equation*}
where $s_{\hat{A}_t}(y|W_t)=\rat{\hat{A}_t}{p}\act{\rat{W_t}{d}y}$ with $\hat{A}_t$ being the minimizer of (\ref{eqn:dsm_loss_rfm}). We start the backward diffusion at time $t_1=0.1$ in the vicinity of one of the training samples $x_l$. More precisely, we start the backward diffusion at $Y_{t_1} = a_{t_1}x_l+\sqrt{h_{t_1}}z$, where $l$ is selected uniformly at random from the set $\{1,2,\cdots,n\}$, and $z\sim\cN{0,I_d}$. We stop the simulation at $t_0=10^{-5}$ and check whether $Y_{t_0}$ is closer to one of the training samples compared to others \cite{yoon_diffusion_2023}. In particular, let $\text{NN}_i(x)$ denote the $i^{th}$ nearest neighbor of $x$ in $\{x_1,x_2,\cdots,x_n\}$. We say that the backward diffusion retrieves a data sample if $\frac{\norm{Y_{t_0}-\text{NN}_1(Y_{t_0})}}{\norm{Y_{t_0}-\text{NN}_2(Y_{t_0})}}<\delta$, with $\delta >0$. We measure memorization as the fraction of $N$ backward diffusion instances that retrieves one of the data samples. Fig.~\ref{fig:mem_sim} shows the measure of memorization thus obtained as a function of $\psi_n$ for different values of $\psi_p$ and $m$. We observe that the result of this experiment is in line with the predictions made in the previous sections. In particular, we note the following: 1) For $m=50$ and $\psi_n<\psi_p$, the memorization is high. 2) The memorization decreases as $m$ decreases. 3) Memorization is zero when $\psi_n>\psi_p$.
\section{Discussion and Future Work}
In this work, we provided theoretical insights into the mechanisms underlying generalization and memorization in diffusion models.
%-- a phenomenon whereby the output of diffusion models resemble one of the training sample.
We studied generalization and memorization by analyzing the test and train errors of denoising score matching (DSM) with random features and unstructured data. To the best of our knowledge, this work is the first to provide a theoretical study of memorization when a parametric score is used. Memorization is typically observed in practice when the diffusion models are trained long enough using highly over parameterized neural networks. In line with these empirical observations, our findings indicate that the complexity of the model ($p$) and the number of noise samples per data sample ($m$) used during DSM play a significant role in memorization. In practical implementations, $m$ increases as the number of training epochs increases.

As the complexity of the model increases, it can approximate more complex functions, leading to a better approximation of the empirical optimal score. Furthermore, as $m$ increases, an optimizer of DSM loss (\ref{eqn:dsm_mfinite}) tends to be close to the empirical optimal score.  These effects that lead to memorization are captured in our results and are illustrated as a phase diagram in Fig.~\ref{fig:phase_diagram}. As expected, generalization occurs when the complexity of the model is limited, i.e., $p<n$.

 Our theoretical analysis was facilitated by several simplifications, which can also be viewed as limitations of this work. First, we employ RFNNs to represent the score function and study the minimizer of the corresponding DSM loss. This is a significant departure from practical implementations, where complex neural architectures are employed to parameterize the score function and gradient-based optimization methods are applied to find the minimizer. Furthermore, practical score parameterizations typically incorporate time $t$ as an explicit argument, whereas we assume a separate instance of RFNN for each $t$, optimized independently. Lastly, our analysis focuses on unstructured data, while real-world datasets often posses various forms of structure. 

Our work represents a preliminary step in understanding generalization and memorization in diffusion models. Future directions could focus on addressing some of the simplifications inherent in our analysis. Specifically, can we use a time-parameterized model for the score and derive its learning errors? Another direction is to analyze the entire backward diffusion using the learned score and demonstrate the generalization and memorization behaviors. These advancements could further improve our understanding on diffusion models.

