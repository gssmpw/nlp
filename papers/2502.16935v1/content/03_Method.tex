
\section{Problem Definition}
\label{sec:problem}


Our problem is a variant of the multivariate time series prediction that is commonly considered for traffic forecasting. 
We formulate a more general problem that can handle high sparsity and is not restricted by spatial structures such as roads. 
This flexible formulation enables the usage of unstructured observations that are moving and are inconsistent in time. 
Important for a proper reconstruction within sparse and unstructured observations is the detection of latent dependencies in a series of observations which can be applied across the sparse samples.
There is a high chance that those dependencies will only be partially part of a single sparse training sample which is in contrast to the common traffic prediction.

Let $\mathcal{S}$ be a spatial domain, which can be continuous or discrete, and $\mathcal{T} = \{t_0, \dots, t_{m-1}, t_{m}\}$ a temporal domain with $m+1$ discrete timesteps.
Then let $\mathbf{O}$ be a set of observations in $\mathcal{S}$ throughout the first $m$ timesteps $\mathcal{T}_{obs} = \mathcal{T} \setminus \{t_{m}\}$.
Each observation is a tuple of a time $t$, a spatial position $s$, and a vector of the observed values $\bm{y} \in \mathbb{R}^{d_f}$ with dimension $d_f \in \mathbb{N}$ :
\begin{equation}
    \mathbf{O} \subseteq \mathcal{T}_{obs} \times \mathcal{S} \times \mathbb{R}^{d_f}
\end{equation}
The high flexibility of the problem is expressed by the unstructured observations, a varying amount of observations for each timestep $t$, and observation positions which can change for each $t$.
High sparsity within the spatial and temporal domain can be modeled by this problem definition because it is possible to have few observations in a region that do not have to appear in any other past or future timestep. 

The goal is to predict for a subset of query locations $Q \subseteq \mathcal{S}$ the future values $\hat{\bm{y}} \in \mathbb{R}^{d_f}$ for the timestep $t_m \in \mathcal{T}$.
Therefore we learn a function $F: \mathcal{P}(\bm{O}) \times \mathcal{S} \to \mathbb{R}^{d_f}$ which predicts the future timestep from the observations $\mathbf{O}$ and the target position $s$. 
\begin{align}
    \hat{\bm{y}} = F(\bm{O}, s) ,& &  \forall \, s \in Q    
\end{align}
Note that $Q$ can be either a set of independent points of interest or a regular grid as frequently considered in previous works.
The problem challenges possible solutions to gather the information of the observations scattered across the timesteps $\mathcal{T}_{obs}$ and combine them to a reconstructed dense traffic state.
The combination is required because the observations can be arbitrarily scattered in space and in time.
Specifically, in the case of very sparse observations, a possible solution will have to learn spatio-temporal correlations, which are only partly represented in a single sample $o$ or are split between multiple samples and then fused by training over the complete data set.
For such a problem it is most important to fuse the data from previous sparse times steps for the prediction in $t_m$. 
While other problem definitions use nearly complete traffic states at each timestep as input, this problem definition forces the algorithm to exploit the temporal structures even more when the data is sparse.

Within common traffic prediction methods the road network and the information about the fixed sensor locations are often utilized, which we count as prior knowledge because the information is not dependent on the observations.
From a broader perspective such a usage of the road network to design or guide the graph graph construction as in \cite{Yu18, Zhou20} is an example of great informed machine learning algorithms \cite{vonrueden2023}.
Furthermore, our definition targets a more general case, where observations are not restricted to roads, but distributed in space without prior knowledge of the spatial structure. 
The road network is, at least in Static Graph solutions (see section \ref{sec:related}), explicitly incorporated, because it is freely available prior information.
Due to these requirements the solutions are strongly tied to the car traffic prediction task as other traffic predictions like ships or planes are not restricted in this way.
Considering traffic prediction in smaller cities with a lot fewer sensors on the road, we could handle moving sensors, e.g. sensors installed in cars.
This introduces a new complexity to the problem definition because hidden spatial structures have to be learned additionally.
% Without the power of the stationary sensors most recent works are not able to model all possible positions in a spatial domain as single graph nodes because there exist continuous spatial domains.
% We argue that such small changes require a more general problem definition as we introduced.



\section{Method}
\label{sec:method}


Our goal is to infer from sparse observations which are spatially distributed a complete traffic state also for spatial locations where no observations are available.
Therefore we need to learn and utilize spatio-temporal correlations that describe the state of unobserved regions from just a few present observations.
In section \ref{subsec:imagine} we describe how we achieve a latent dense spatial description for a traffic state represented by the input sample. 
The following section \ref{subsec:merge} describes how we create a modeled graph from these latent descriptions and use state-of-the-art methods to predict a future latent description. 
Finally, we query our latent description with query locations to receive values of interest, which is described in section \ref{subsec:query}.

\begin{figure*}
    \centering    \includegraphics[width=\textwidth]{figures/Architecture.pdf}
    \caption{Framework of SUSTeR with an observation encoding, a residual architecture for hidden traffic state reconstruction with a variable amount of observations and a decoding from the dense hidden traffic state into the original space.}
    \label{fig:architecture}
\end{figure*}



\subsection{Reconstruction of Traffic State}
\label{subsec:imagine}

We want SUSTeR to learn complex spatio-temporal correlations across multiple input samples which we aim to achieve with explicit spatio-temporal correlations.
This is important because due to the sparseness of the observations those correlations are not completely represented in a single input sample.
A graph is a good representation of locations connected by spatio-temporal correlations as it was done in many works before \cite{Zhou20, Yu18, Shao22}.
In early approaches each sensor location was modeled as a single graph node \cite{Li2018} while later work grouped similar locations to districts to learn more abstract correlations \cite{Li2021}. 
We extend this idea and use a fixed number of graph nodes set $V$. 
However, these do not correspond to a specific location or sensor and the information of the observations can be assigned freely to those nodes making the usage as flexible as possible.
Such a strategy gives the framework a higher degree of freedom to find similar regions with similar correlations and assign those to the same node.
From the point of our introduced problem in section \ref{sec:problem}, the observations can have variable locations and their positions can be unique throughout the entire data, which is why we need such a flexible solution.
Each node $v \in V$ is assigned to a row in $X_{i} \in \mathbb{R}^{|V| \times d_e}, \, t_i \in \mathcal{T}_{obs}$ which contains the embedding vector with $d_e \in \mathbb{N} $ dimensions and is used to encode latent information to all spatial points connected to this graph node.
The connectivity between the introduced graph nodes is described further in section \ref{subsec:merge}.

To tackle the problem of distributed correlations in the training data we introduce a context that can be used to remember similarities across training samples.
Context information in a spatio-temporal learning setup describes the overall environment.
For example, at Sunday 08:00 am the relevant context could be the weekend.
The temporal context models a broader influence on the traffic state which can differ greatly between weekends and weekdays.
We model the context as a function $C_{\theta}: \mathcal{T} \to \mathbb{R}^{\mid V \mid \times d_e}$ that maps the temporal context information of $t_0$, to our graph nodes as a bootstrap of the node embedding for estimating the initial traffic state $\widebar{X}$.
\begin{equation}
    \widebar{X} = C_{\theta}(t_0)
    \label{eq:mean_state}
\end{equation}
We mark all functions with learnable weight with $\theta$ representing the weights.
The context $\widebar{X}$ is consecutively assimilated to the following observations $\mathbf{O}$, as illustrated in figure \ref{fig:architecture}.
We use a residual structure to enrich $\widebar{X}$ in each timestep with the observations, adding information and reducing uncertainty.
With the function $enc_{\theta}: \mathbf{O} \times \mathbb{R}^{|V| \times d_e} \to \mathbb{R}^{|V| \times d_e}$ we compute the change $\Delta X$ that a given observation $o$ imparts on the previous traffic state $X$:
\begin{equation}
    \Delta X = enc_{\theta}(o, X) = sample_{\theta}(s) \cdot inf_{\theta}(o, X)^T,
\end{equation}
where $sample_{\theta}: \mathcal{S} \to \mathbb{R}^{|V|}$ creates a one-hot assignment to select a graph node, and $inf_{\theta}: \mathbf{O} \times \mathbb{R}^{|V| \times d_e} \to \mathbb{R}^{d_e}$ contains the residual information.
Please note the observations within a single timestep can not influence each other but can utilize the past encodings.
This allows for an architecture handling various amounts of observations throughout the time in a single sample.
% to make the assignment from an observation to the nodes interpretable.
% The multiplication between the output of $sample_{\theta}$ and $inf_{\theta}$ works as an assignment from the observation to a single graph node.
% In such way we create an assignment to a single graph node and the information propagation between those nodes is the task of an existing spatio-temporal correlation mining algorithm.

% This is similar to human intuition when driving through a city and an anomaly from the mean traffic state is observed, we already change our expectation in remote districts which could be influenced by our observation.
% With each new observation our uncertainty will decrease for certain areas which is determined by the location and context information of the observation.
% In contrast to an ego perspective, in this example our method can process multiple unrelated observations in the same timestep, which could result from traffic cameras or social media.
% From the design of our framework we do not connect observations in the same timestep because the framework should relay on past reconstructed traffic states and not have expectations on simultaneously occurring observations.
%\todo{Finden wir noch einen Platz?}

% One of our advantages is that this approach can handle varying sizes of $\mathbf{O}_i = \{o \mid t_i \in \mathcal{T}_{obs}\}$ which is the subset of observation at the same time $t_i$.
%\todo{Introdcution?}
%because we can not know if observations contain redundant information.
% A sum of redundant information can increase the confidence of the information expressed through the change $\Delta X$.
From the temporal node changes $\Delta X$ we create a sequence of graph node embeddings $(X_0, \dots, X_{m-1})$, which contains the residual information accumulated over all observations.
The state changes $\Delta X$ from all timesteps are aggregated as follows:
\begin{align}
X_i = 
\begin{cases}
    \widebar{X} + \,\, \mathlarger{\sum}\limits_{j=0}^{i-1} \, X_j + \mathlarger{\sum}\limits_{o \in O_i} enc_{\theta}(o, X_{i-1}) & i > 0\\[15pt]
    \widebar{X} + \mathlarger{\sum}\limits_{o \in O_i} enc_{\theta}(o, \widebar{X}) & i = 0
\end{cases}
\label{eq:Vi}
\end{align}
% Note that the function $enc_{\theta}$ has information available about the previous changes to the context information in $\widebar{X}$.



\subsection{Merging Graph Information}
\label{subsec:merge}

From the node embeddings $(X_{0}, \dots, X_{m-1})$ we define a sequence of graphs $\bm{G} = (G_{0}, \dots, G_{m-1})$, which have a common set of nodes $V$, and a common adjacency matrix $A$ representing weighted edges:
\begin{align}
    G_i = \left(V, A, X_i \right), & &  A \in \mathbb{R}^{|V| \times |V|}
\end{align}
In the following, we discuss how to learn $A$ from the node embeddings.
We use a self-adaptive adjacency matrix for SUSTeR, where $A$ is learned by the architecture itself because recent work showed that an adaptive adjacency matrix outperforms static road network adjacency \cite{Lan22, Wu2019, Bai20}.
Intuitively, two nodes in a spatio-temporal graph should have a strong edge when they are strongly correlated.
We adopt the idea from Bai et al. \cite{Bai20} and calculate the connectivity directly as the Laplacian from the similarity of the node embeddings.
To compute the Laplacian we use the last element of the node embedding sequence $X_{m-1}$, as it contains all the accumulated information by the nature of its construction (Eq. \ref{eq:Vi}). 
One could argue that for each graph $G_i$ the Laplacian from the node embedding $X_i$ could be used but we see the connectivity of the nodes and therefore the flow of information dependent on the overall situation which can only be represented by the finished accumulation of information.
The Laplacian $\mathcal{L}$ is computed as:
\begin{equation}
    \mathcal{L} = D^{-\frac{1}{2}} A D^{-\frac{1}{2}} = softmax\left(ReLU \left(X^{\,}_{m-1} \cdot X_{m-1}^T \right) \right)
\end{equation}

The resulting sequence of graphs $\bm{G}$ encodes all the information from the sparse observations including spatio-temporal correlations.
Note that the graphs in the final sequence only differ in their node embeddings $X_i$.

Because we transferred the sparse representation of our problem to a sequence of spatio-temporal graphs $\bm{G}$, we support any spatio-temporal graph neural network $\mathcal{X}_{\theta}$ for the aggregation of the correlation graphs, such as STGCN \cite{Yu18}, D2STGNN \cite{Shao22}, MegaCRNN \cite{jiang23}, Wavenet \cite{Wu2019}, etc.
In SUSTeR we use STGCN to provide the future graph $G_m$ at timestep $t_m$:
\begin{equation}
    G_m = \mathcal{X}_{\theta}(G_{0}, \dots, G_{m-1})
\end{equation}
The final graph $G_m = (V, A, X_m)$ encodes the reconstructed traffic state with fewer graph nodes than sensors.


\subsection{Querying of Locations}
\label{subsec:query}

A set of query locations $Q \subseteq \mathcal{S}$ can be chosen freely for traffic prediction.
The decoder function $dec_{\theta}: \mathbb{R}^{|V| \times d_e} \times \mathcal{S} \to \mathbb{R}^{d_f}$ predicts the next value $\hat{\bm{y}}$ for any location $s \in \mathcal{S}$.
\begin{align}
    \hat{\bm{y}} = dec_{\theta}(X_m, s),& &  \forall s \in Q
\end{align}
In the case of sparse traffic prediction, we choose the position of all traffic sensors from the original traffic datasets in order to evaluate the accuracy on ground truth.
