
\section{Evaluation}

In this section, we first introduce in section \ref{subsec:data} the dataset that we are using and describe how we modify it to fit our problem definition of Sparse Traffic Prediction. 
Section \ref{subsec:baselines} introduces the baselines and their modification for a performance comparison, which is done in section \ref{subsec:performance}.
The rest of the section is divided into a runtime comparison, a study about the necessary training set size, and in section \ref{subsec:ablation} we demonstrate the effect of important parameters.


\subsection{Data}
\label{subsec:data}

For comparison with other approaches in this field we choose the \textbf{Metr-LA} \cite{Li2018} and \textbf{PEMS-BAY} \cite{pems} dataset as a well-studied baseline for the problem of traffic prediction.
Both datasets were also used for small sparseness rates which we further elaborate on in section \ref{sec:related}.
The goal of these datasets is to predict the average speed in five-minute intervals located in two different cities Los Angeles and San Francisco respectively.
Metr-LA has data for 207 sensors from 34,272 timeslices and the PEMS-BAY 325 sensors with 52,116 samples.
As these datasets do not involve any sparseness, we need to introduce this artificially.
We select different dropout ratios and sample sensors from a uniform distribution and set their values to zero.

% ------------------------ First try ------------------------

The choice to take a uniform sampling of all sensors was taken in absence of a more realistic strategy.
We are aware that even mobile sensors would not sample the traffic spatially uniform.
A systematic review of different skewed distributions would go beyond the scope of this work but will be pat of further investigations.
First experiments have shown that the dependency between the spatial areas with higher sampling rates and a reduction of the reconstruction error is neither linear nor symmetric.

% -----------------------------------------------------------

Algorithm \ref{alg:sparsify_dataset} describes the dropout application to obtain a sparse dataset from a dense dataset with $n$ being the number of samples and $k$ the number of sensors.
% TODO: Add the thoughts to skewed distributions

\begin{algorithm}[!h]
\caption{Sparsifying Dense Traffic Dataset}
\label{alg:sparsify_dataset}
\Input{Dropout probability $P_{do} \in \left[ 0, 1 \right]$,\\dense dataset $D \in \mathbb{R}^{n \times k \times d_f}$}
    \Output{Sparse dataset $\overline{D} \in \mathbb{R}^{n \times k \times d_f}$}
    \nonl\hrulefill
    
    $m \gets \left[ 0 \right]_{n \times k \times d_f}$       \tcp*{initialize zero mask}
    \For{$i \in \lbrace 0, \dots, n-1 \rbrace$}{
        \For{$j \in \lbrace 0, \dots, s-1 \rbrace$}{
            $P_{keep} \sim U \left[0, 1\right]$ \;
            \If(\tcp*[f]{keep sensor}){$P_{keep} > P_{do}$}{
                $m\left[i, j, :\right] \gets 1$ \;
            }
        }
    }
    \Return $D \odot m$         \tcp*{return masked, sparse dataset}
\end{algorithm}

As input features we chose the original traffic volume, time of day, day of week, (cf. \cite{Li2021}) and the positional information of the sensors as longitude and latitude.
We add the positional feature because we don not use the prior knowledge of the road network which is in previous works the way of recognizing spatial distance.
This five-dimensional vector is used as input feature with a lead time of an hour (12 timesteps) and the target for the model is to predict the traffic volume for all sensors at the next timestep.

\newcommand{\res}[2]{\small{$#1 \pm #2$}}
\newcommand{\head}[1]{\multicolumn{1}{c}{#1}}

\begin{table*}[h!]
    \centering
    \begin{tabular}{cc|rrrr|rrr}
        &&\head{D2STGNN}&\head{STGCN}&\head{STGCN$_{adj}$}&\head{STGCN$^{perm}$}&\head{STGCN$^{perm}_{adj}$}&\head{D2STGNN$^{perm}_{adj}$}&\head{SUSTeR}\\
        \hline
        \multirow{3}{*}{\shortstack{10\%\\Dropout}} &MAE&\res{2.656}{0.011}&\res{2.758}{0.051}&\res{2.646}{0.016}&\res{5.195}{0.073}&\res{\textbf{4.835}}{0.071}&\res{5.136}{0.139}&\res{13.280}{0.024}\\
        &RMSE&\res{6.077}{0.056}&\res{6.192}{0.101}&\res{6.062}{0.066}&\res{11.907}{0.041}&\res{\textbf{11.429}}{0.114}&\res{11.712}{0.116}&\res{20.900}{0.068}\\
        &MAPE&\res{0.062}{0.000}&\res{0.067}{0.003}&\res{0.060}{0.001}&\res{0.117}{0.001}&\res{\textbf{0.101}}{0.002}&\res{0.110}{0.006}&\res{0.241}{0.001}\\
        \hline
        \multirow{3}{*}{\shortstack{80\%\\Dropout}} &MAE&\res{4.940}{0.124}&\res{4.214}{0.061}&\res{3.484}{0.035}&\res{5.173}{0.133}&\res{4.986}{0.046}&\res{4.993}{0.025}&\res{\textbf{3.969}}{1.985}\\
        &RMSE&\res{9.787}{0.075}&\res{9.599}{0.080}&\res{8.257}{0.038}&\res{11.910}{0.180}&\res{11.666}{0.055}&\res{11.731}{0.051}&\res{\textbf{9.287}}{4.644}\\
        &MAPE&\res{0.118}{0.002}&\res{0.118}{0.004}&\res{0.083}{0.002}&\res{0.116}{0.007}&\res{0.108}{0.002}&\res{0.109}{0.001}&\res{\textbf{0.089}}{0.044}\\
        \hline
        \multirow{3}{*}{\shortstack{90\%\\Dropout}} &MAE&\res{5.692}{0.091}&\res{4.975}{0.089}&\res{4.024}{0.022}&\res{5.288}{0.121}&\res{5.054}{0.037}&\res{5.066}{0.010}&\res{\textbf{4.963}}{0.023}\\
        &RMSE&\res{11.392}{0.152}&\res{11.084}{0.164}&\res{9.560}{0.027}&\res{12.115}{0.101}&\res{11.807}{0.032}&\res{11.827}{0.007}&\res{\textbf{11.615}}{0.041}\\
        &MAPE&\res{0.148}{0.004}&\res{0.135}{0.006}&\res{0.091}{0.001}&\res{0.122}{0.005}&\res{0.110}{0.002}&\res{0.110}{0.001}&\res{0.111}{0.001}\\
        \hline
        \multirow{3}{*}{\shortstack{99\%\\Dropout}} &MAE&\res{6.346}{1.089}&\res{6.182}{0.139}&\res{5.517}{0.085}&\res{7.379}{0.017}&\res{7.016}{3.139}&\res{5.494}{0.088}&\res{\textbf{5.274}}{0.059}\\
        &RMSE&\res{13.247}{1.042}&\res{13.386}{0.319}&\res{12.634}{0.168}&\res{14.608}{0.328}&\res{14.207}{3.343}&\res{12.508}{0.133}&\res{\textbf{12.236}}{0.107}\\
        &MAPE&\res{0.149}{0.033}&\res{0.142}{0.007}&\res{0.121}{0.003}&\res{0.189}{0.014}&\res{0.145}{0.048}&\res{0.124}{0.001}&\res{\textbf{0.120}}{0.001}\\
        \hline
        \multirow{3}{*}{\shortstack{99.9\%\\Dropout}} &MAE&\res{8.996}{0.545}&\res{11.007}{0.533}&\res{10.766}{1.451}
&\res{13.288}{0.008}&\res{13.304}{0.013} & \res{9.802}{0.227}& \res{\textbf{6.986}}{0.061}\\
        &RMSE&\res{17.397}{0.639}&\res{19.051}{0.419}&\res{18.822}{1.238}&\res{20.890}{0.026}&\res{20.943}{0.068} & \res{17.966}{0.173} & \res{\textbf{15.081}}{0.066}\\
        &MAPE&\res{0.191}{0.006}&\res{0.253}{0.004}&\res{0.239}{0.025}&\res{0.241}{0.000}&\res{0.242}{0.000} & \res{0.186}{0.004}& \res{\textbf{0.143}}{0.001}\\
        \hline
    \end{tabular}
    \caption{Performance of baselines and SUSTeR on the modified Metr-LA data set with various dropout rates. Reported are mean and standard deviation of the metrics of five runs for each cell. The two first columns show the original baselines with prior knowledge. The following columns present the independent influence of each modification to the STGCN baseline. The last three columns compare the performance of the modified baselines with SUSTeR.}
    \label{tab:baseline}
\end{table*}


\subsection{Implementation and Resources}
\label{subsec:impl}

We implemented our code in python using the PyTorch library \cite{paszke19} for the Neural Network.
For our baselines, we used the implementation in the library BasicTS at github \cite{liang23}.
Since SUSTeR leverages a spatio-temporal GNN, we modified their implementation of STGCN to use it as the inner spatio-temporal Neural Network $\mathcal{X}_{\theta}$ (cf. section \ref{subsec:merge}).
Specifically, we introduce a second input, the Laplacian matrix, besides the graph node features to replace the static matrix.

We choose the following hyperparameters which we found by a grid search: 
The Adam optimizer \cite{kingma2014adam} is executed with a learning rate of $5e^{-4}$ and a $L_2$-loss for the weights with $\beta = 1e^{-5}$.
A batch of the training data contains 32 samples and is shuffled throughout the epochs.
We divide our data into training (70\%), validation (10\%), and test (20\%).
Each experiment is trained on 50 epochs and we test the model from the epoch with the best validation metric.
Within the training and the validation, the loss and metric is the mean absolute error (MAE) and we report the performance additionally as root mean square error (RMSE) and mean absolute percentage error (MAPE).
All baselines were executed with the parameters proposed in their respective publications for the Metr-LA dataset.

All functions mentioned in section \ref{sec:method} with learnable parameters are neural networks mostly built of fully-connected layers and a ReLU \cite{nair2010rectified} activation function.
We denote the fully-connected layers with $FC^d$, with $d \in \mathbb{N}$ being the output dimension and use $\sigma$ as the ReLU activation function.
The individual functions are detailed below:
\begin{align*}
    C_{\theta} & : \,\, t_0 \to FC^{d_e} \to \sigma \to FC^{d_e \times \mid V \mid} \\
    inf_{\theta} & : \,\, concat(X,o) \to FC^{2d_e} \to \sigma \to FC^{2d_e}\to \sigma \to FC^{d_e}\\
    sample_{\theta} & : \,\, s\to FC^{\mid V \mid} \to \sigma \to FC^{\mid V \mid}\to softmax\\
    dec_{\theta} & : \,\, concat(X,s) \to FC^{256} \to \sigma \to FC^{128} \to FC^1 
\end{align*}
The graph node assignment $sample_{\theta}$ is obtained by feeding an observation position $s$ to a multi-layer perceptron (MLP).
The MLP's output is converted to a distribution with the softmax function, from which a single graph node index is sampled.
This index is one-hot encoded into a vector of length $|V|$.

% For more details on the inner workings of STGCN $\mathcal{X}$ we refer the reader to \cite{Yu18}, as we follow their design.
All experiments were executed on a HPC cluster node with 16 allocated cores of an Intel Xeon Gold 6226R, with 16GB of RAM, and an NVIDIA Tesla V100-GPU.
We used CUDA version 11.1.0 together with cuDNN in version 8.0.4.30, and PyTorch version 1.10.



\subsection{Baselines}
\label{subsec:baselines}

We use two baselines for the common traffic prediction problem and adapt them to our introduced version of the traffic prediction problem in section \ref{sec:problem}.
First, we select STGCN \cite{Yu18} as a competitor since SUSTeR uses STGCN in part.
Second, D2STGNN \cite{Shao22} is a recent state-of-the-art solution, which works with a diffusion approach that should be able to cover at least some missing values by design.
As both competitors are built for traffic prediction and use prior knowledge we modify both for a fair comparison which we describe in the following.

Both architectures originally use the static road network as prior information for their graph, which technically violates our problem definition.
To avoid this, we use a random adjacency matrix for both architectures, where the values are drawn at random from $\mathcal{N}(0, 1)$.
From the random matrix we compute the normalized Laplacian in the case of STGCN, and the bidirectional transition flow matrix for D2STGNN.
The random matrix is initialized once the training starts and is constant throughout the epochs.
If this modification is enabled we mark the results on the baselines as STGCN$_{adj}$.

The second assumption is the non-stationary observations. Our inputs are so sparse that the chance that in less than ten observations two are at the same location is minimal.
In the baselines, the same sensor is every time on the same input node which creates a prior knowledge that our problem definition prohibits because the input should have variable sizes with variability in the spatial domain.
Permuting the input sensors will break this static assignment which we do before processing a batch.
In exchange we provide the baselines with the location of the sensors as latitude and longitude.
We mark the permutation modification of a baseline as STGCN$^{perm}$.

These changes impair the performance of the baselines but are necessary to fit our problem definition for a fair comparison.
Table \ref{tab:baseline} shows the impact of both modifications on different dropout rates to get an impression of the complexity.
The left four columns show the D2STGNN without any modifications and the STGCN with all possible combinations.
% The single modifications are missing for the D2STGNN because the behavior is similar to the STGCN approach.
The results show that the permutation of the input sensors decreases the performance the most although the location is then part of the input.
With higher dropout rates the performances between no and both modifications moving closer together which depicts the increasing complexity of the task with high sparsity.
Surprising is the minimal decrease of performance between (STGCN$_{adj}$) with and (STGCN) without the random adjacency matrix which raises questions regarding the importance of the road network for the STGCN approach.
This is on par with the findings in previous works \cite{Lan22, Wu2019, Bai20} and further strengthens the decision in our problem definition to omit the prior knowledge of the road network.
\begin{table}[h]
    \centering
    \begin{tabular}{cc|c|c|c}
        & & STGCN & D2STGNN & SUSTeR \\
        \hline
        \multirow{3}{*}{\shortstack{10\%\\Dropout}} & MAE & \res{\textbf{2.293}}{0.011}& \res{2.359}{0.021} & \res{4.711}{0.002}\\
        & RMSE & \res{4.532}{0.019}&\res{\textbf{4.419}}{0.028} &\res{8.203}{0.004}\\
        & MAPE &\res{\textbf{0.052}}{0.000} & \res{0.055}{0.001}& \res{0.135}{0.000}\\
        \hline
        \multirow{3}{*}{\shortstack{80\%\\Dropout}} & MAE & \res{\textbf{2.340}}{0.013} & \res{2.353}{0.016}&\res{3.183}{1.013}\\
        & RMSE &\res{4.596}{0.026} & \res{\textbf{4.455}}{0.032}& \res{5.908}{1.625}\\
        & MAPE &\res{\textbf{0.054}}{0.000} & \res{0.056}{0.001}& \res{0.085}{0.036}\\
        \hline
        \multirow{3}{*}{\shortstack{90\%\\Dropout}} & MAE & \res{2.356}{0.015} & \res{\textbf{2.348}}{0.010}& \res{2.369}{0.011}\\
        & RMSE &\res{4.617}{0.015} &\res{\textbf{4.453}}{0.006} & \res{4.584}{0.013}\\
        & MAPE & \res{\textbf{0.054}}{0.000}& \res{0.055}{0.000} & \res{0.056}{0.000}\\
        \hline
        \multirow{3}{*}{\shortstack{99\%\\Dropout}} & MAE & \res{3.377}{1.091}
        & \res{2.491}{0.020}& \res{\textbf{2.457}}{0.017}\\
        & RMSE &\res{6.164}{1.652} &\res{\textbf{4.644}}{0.025} & \res{4.720}{0.011}\\
        & MAPE &\res{0.089}{0.038} & \res{0.058}{0.000}& \res{\textbf{0.057}}{0.000}\\
        \hline
        \multirow{3}{*}{\shortstack{99.9\%\\Dropout}} & MAE &\res{4.325}{0.776} & \res{3.814}{0.933}& \res{\textbf{2.572}}{0.013}\\
        & RMSE &\res{7.623}{1.155} & \res{6.460}{1.230}& \res{\textbf{4.915}}{0.012}\\
        & MAPE &\res{0.121}{0.027} &\res{0.102}{0.033} & \res{\textbf{0.060}}{0.000}\\
        \hline
    \end{tabular}
    \caption{Performance measured of SUSTeR and the competitors on the PEMS-BAY dataset with the mean-absolute-error, root-mean-squared-error, and mean-absolute-percentage-error.}
    \label{tab:pems_bay}
\end{table}
\subsection{Performance Results}
\label{subsec:performance}
The three right-most columns in table \ref{tab:baseline} show the performance of SUSTeR compared against both baselines with the necessary modifications (see section \ref{subsec:baselines}) for the METR-LA dataset.
The same experiment was done with the PEMS-BAY dataset and the results can be seen in table \ref{tab:pems_bay}.
We report the average metric on the test set together with the standard deviation over five runs.
While the baselines have an edge in settings with high densities, our framework clearly outperforms the baselines with very high sparsity rates of 99\% and more in all three metrics for the METR-LA dataset.
In the case of PEMS-BAY our algorithm is also superior in the very sparse regime of the experiment but stands behind for nearly complete data.
For an intuitive comparison, figure \ref{fig:performance_mae} and \ref{fig:performance_mae_bay} show the mean absolute error with additional additional dropout rates 95\% and 99.5\%.
% With those additional rates SUSTeR shows only a linear increment of the error while both other approaches stronger increase their mean absolute error.

% ------------------ New ------------------
As the main goal is to handle very sparse data we would like to refer the performance of nearly complete data to the aggregation function of $\Delta X$ and the small amount of proxies.
The length of the aggregated $\Delta X$ for a timeslice can highly vary due to the amount of observations that are present creating trouble for low dropout rates when many observations are present in the data. 
Also, in the real datasets each sensor is mapped to a unique graph node where our approach limits to only ten nodes, which is fine for high dropout rates to learn the essential spatiotemporal correlations but seems inappropriate for more information. 
This is reasonable as SUSTeR does not have the capacity to learn all correlations.
% -----------------------------------------

SUSTeR even has superior performance in sparse settings when the baselines are allowed access to additional knowledge in the form of the road network when comparing to single or no modified baselines.
Our framework clearly shows that for high dropouts there is more to a good prediction than the prior knowledge and that our designed architecture can handle such a high sparsity well.


\begin{figure}[ht]
    \centering
    \includegraphics[width = \linewidth]{figures/mae_pred.png}
    \caption{Mean absolute error for SUSTeR and the baselines D2STGNN and STGCN with both modifications as average over 5 runs. The dropout rates are plotted in logarithmic scaling for better visualization.}
    \label{fig:performance_mae}
\end{figure}



\begin{figure}[ht]
    \centering
    \includegraphics[width = \linewidth]{figures/pems-bay-mae.png}
    \caption{The mean absolute error (MAE) for the PEMS-BAY dataset evaluated on different dropout rates.}
    \label{fig:performance_mae_bay}
\end{figure}



\subsection{Runtime}
\label{subsec:runtime}
In this section, we compare the runtime of our approach with the chosen baselines for the METR-LA dataset.
The number of learnable parameters in all approaches is independent of the dropout rate on the input.
Therefore we picked the runtimes of the five executions for the baseline comparison in section \ref{subsec:baselines} for the 99\% dropout datasets.
Our approach uses on average 13:54 minutes ($\pm$4s) and clearly outperforms the D2STGNN architecture with 2:25 hours ($\pm$1min) for the fixed amount of 50 epochs.
Also STGCN was slower with 19:21 minutes ($\pm$6s) which is still reasonable although our framework wraps the STGCN architecture.
SUSTeR uses only half of the layer sizes of the original STGCN (see section \ref{subsec:ablation}) which reduces the number of parameters and we input only few graph nodes in comparison to all sensors in the original work.
Fewer nodes reduce the amount of convolutions that are needed within the Graph Convolution Layers (GCN) and therefore reduce the overall computation time.


\subsection{Training Data Size}
We evaluate the robustness of our approach with different sizes of training data to test also this dimension of sparseness along the training samples.
We keep the validation and test data fixed and take only the first 10\% to 70\% from the overall training data.
Figure \ref{fig:fraction_training_data} shows the mean absolute error of the test set with our framework and both modified baselines on the 99\% dropout dataset.
With less than 50\% of the training data we clearly outperform D2STGNN and are slightly better than the STGCN baseline.
It is also worth pointing out that at 50\% training data, SUSTeR remains competitive to the baseline that have been trained on 100\% of the training data.
Above the 50\% training data it is not possible for STGCN to draw an advantage out of more data while SUSTeR is still improving.
This shows that our framework can better learn the spatio-temporal correlations which are scattered across multiple training samples due to the high sparsity.

\begin{figure}[ht]
    \centering
    \includegraphics[width = \linewidth]{figures/train_fraction.png}
    \caption{Training only on a fraction of the training set while keeping validation and test set the same. Executed with a 99\% dropout rate.}
    \label{fig:fraction_training_data}
\end{figure}


\subsection{Ablation Study}
\label{subsec:ablation}

To study the impact of the subparts and parameters in our framework we conduct ablation experiments on various aspects of SUSTeR.

\paragraph{\textbf{Graph nodes and Embedding}} 
A crucial parameter of the framework is the number of graph nodes $|V|$ and the embedding dimension $d_e$ which are used for the hidden traffic state.
Both parameters have a direct influence on the number of parameters in the framework and should be carefully chosen to reduce training time while still keeping a good performance.
Table \ref{tab:proxyembed} shows the results of a comparison on the 99\% dropout dataset as the mean absolute error over three runs with standard deviation.
The best values are achieved with the combinations 10 graph nodes and 32 hidden embedding dimension or 25 graph nodes and half of the embedding dimension. 
We also note that a selection of 10 to 25 graph nodes is a good decision, even a very high number of graph nodes does not increase the performance 
%which can happen due to overfitting or the parameter space gets too large that the number of epochs is not enough.

\begin{table}[ht]
    \centering
    \begin{tabular}{c|cccc}
        \multicolumn{1}{c}{}& \multicolumn{4}{c}{$d_e$}\\
        $|V|$ & 8 & 16 & 32 & 64 \\
        \hline
         1  & \res{5.499}{0.029}& \res{5.400}{0.078}& \res{5.317}{0.028}& \res{5.373}{0.075}\\
         5  & \res{5.329}{0.051}& \res{5.283}{0.017}& \res{5.268}{0.065}& \res{5.361}{0.045}\\
         10 & \res{5.283}{0.055}& \res{5.310}{0.090}& \res{\textbf{5.255}}{0.030}& \res{5.305}{0.015}\\
         25 & \res{5.302}{0.060}& \res{\textbf{5.244}}{0.020}& \res{5.269}{0.032}& \res{5.298}{0.041}\\
         50 & \res{7.240}{2.527}& \res{5.312}{0.035}& \res{5.306}{0.018}& \res{5.368}{0.097}\\
    \end{tabular}
    \caption{Different number of graph nodes and embedding dimensions tested on the 99\% dropout data with the mean absolute error. Each configuration was executed 5 times.}
    \label{tab:proxyembed}
\end{table}

% \paragraph{\textbf{Static vs. Adaptive Context}}
% For the initialization of the framework we use context information $C_{\theta}(t_0)$, which is the base for the hidden spatial structure.
% We SUSTeR with context information to version without context information, i.e. a static state $\widebar{X}$ independent of the context.
% Across three executions the mean absolute error improves from $5.356$ with a static context to $5.293$ with our adaptive context information.

\paragraph{\textbf{STGCN Size}}
A central component in SUSTeR is the spatio-temporal correlation mining module $\mathcal{X}$.
%In our experiments we use STGCN, we investigate its impact in this ablation experiment.
The used module (STGCN) is designed for a spatio-temporal graph with the same node count as the number of sensors.
In our approach we use only a fraction of the possible sensors for our hidden traffic representation.
Therefore we evaluate multiple sizes of parameter sets of STGCN, specifically full-size, a half, a quarter, and the absence of STGCN by using fractions of the hidden dimensions.
When disabling the STGCN we take the average of the node embeddings $\{X_0, \dots X_{m-1}\}$ across the $m$ past timesteps to create the $X_m$ embedding directly.
Table \ref{tab:factor} shows that the half-size network is favorable, especially when considering that this shortens the training time compared to the full-size network.
The disabling of of the STGCN shows worse results, which clearly justify the usage of such an architecture in our framework. 
Barring that version, there is little spread to be observed, which indicates robust performance.

\begin{table}[ht]
    \centering
    \begin{tabular}{r|ccc}
        $factor$& MAE&RMSE&MAPE\\
        \hline
        1.00 & \res{5.287}{0.103} & \res{12.256}{0.189} & \res{3.444}{0.092} \\
        0.50 &\res{\textbf{5.257}}{0.043} & \res{\textbf{12.254}}{0.092} & \res{\textbf{3.409}}{0.059} \\
        0.25 & \res{5.320}{0.043} & \res{12.255}{0.044} & \res{3.421}{0.051} \\
        None & \res{6.212}{0.071} & \res{13.498}{0.151} & \res{4.090}{0.121}\\
    \end{tabular}
    \caption{Experiment on the 99\% dropout data with different layer sizes factors of the inner STGCN. None is the replacement of the STGCN with an average aggregation.}
    \label{tab:factor}
\end{table}

\paragraph{\textbf{STGCN vs. D2STGNN}}
In this experiment we want to show what happens if we exchange the inner correlation mining module $\mathcal{X}$ from the chosen STGCN to the also tested D2STGNN.
Beforehand note the architecture of D2STGNN is in contrast to STGCN not designed for all different kinds of the spatio-temporal problems as also the \textit{time of day} and \textit{day of week} are strongly interwined into the architecture.
We changed to original implementation by replacing the static graph weights with the adjacency matrix that is learned by SUSTeR and kept the hidden dimension $d_e$ as well as the amount of proxies $\mid V \mid$ the same as in previous experiments.
Interestingly, the changes for 10\% and 80\% dropout are large (see figure \ref{fig:improvement}). 
One time a strong improvement, making the results nearly comparable to baseline STGCN/D2STGNN results (see table \ref{tab:baseline}), and on the other hand encountering a strong decrease in performance for $80\%$.
For higher dropouts the results are changing less then one percent and therefore we argue that SUSTeR itself is responsible for the great performance in the very sparse regime.
While the error is nearly not changed the runtime for SUSTeR with the D2STGNN as core is 38:59 ($\pm$57s) minutes about three times longer than with STGCN (see section \ref{subsec:runtime}).
From the minimal changing error, the longer runtime and the less modifications to the algortihm we decided to create SUSTeR around the STGCN algorithm.



\begin{figure}[ht]
    \centering
    \includegraphics[width= \linewidth]{figures/improvement.png}
    \caption{Relative improvement of error metrics when using D2STGNN instead of STGCN as correlation mining module $\mathcal{X}$ within SUSTeR.}
    \label{fig:improvement}
\end{figure}



