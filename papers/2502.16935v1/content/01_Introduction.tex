
\section{Introduction}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/Introduction.pdf}
    \caption{Showing the novel problem statement applied to traffic prediction use case. Multiple unstructured observations from the past are used to reconstruct a hidden traffic state from which a full traffic state is forecast with a set of query locations. }
    \label{fig:intro}
\end{figure*}

% Was sagen denn die anderen warum Traffic Prediction gut ist? 
Forecasting the traffic in the near future is an important task for city management.
Data from the near past is used to predict future traffic states with spatio-temporal Graph Neural Networks \cite{bui22}.
Accurate prediction provides the opportunity to optimize traffic flow, reduce traffic jams and increase air quality \cite{Po19}.

% Wieso ist Sparsity in allen Dimensionen wichtig.
While traffic prediction relies on the availability of data from traffic sensors, there exists a plethora of reasons why sensors may stop working temporarily, such as simple errors, energy saving, or overloaded communication systems.
Considering small- or medium-sized cities, the coverage of sensors may be low because the sensors are too expensive or not available.
Also, the sensors are typically static and do not adapt to changes in the traffic flow (e.g. caused by a construction site), which motivates moving sensors that for example could be mounted on cars. 
However, both missing and moving sensors introduce sparsity, since measurements may not be available for all locations at all times.
This sparsity must be explicitly addressed in traffic prediction for a realistic application scenario, which is illustrated in figure \ref{fig:intro}.
From one hour of data on Sunday morning, only few observations of the traffic state are available at each timestep.
The number of observations may differ throughout the observed time and the observation itself can be distributed arbitrarily in the city. 
We assume a relatively low number of sensors to account for resource saving and sensor failure in our proposed framework SUSTeR.
The task is to predict the dense traffic state one timestep after the observations at all possible sensor locations.
We study this problem on the traffic dataset Metr-LA and PEMS-BAY to test our assumption that only a fraction of the sensor values would be enough for good predictions.
By modifying an existing traffic dataset, we are able to compare our results from very sparse observations to the bottom line with all information available.
A successful study will provide insights in how sensors in new cities can be reduced before installing them and further mobile sensors would save more resources and are able to adapt to new traffic situations.
We argue that in order to be adaptable to other cities and changes in traffic flows, prior information like the road network should be neglected and just the sparse observations considered.
This comes with the added benefit of making our solution applicable in regions where no openly available road network is maintained or pathways change frequently (e.g. flood areas, animal observations). 


The aforementioned problem is novel and more challenging than the commonly considered traffic prediction problem, since there exist very few observations in each input sample.
Current works for the traffic prediction problem do not consider any missing values. \cite{Li2021, Shao22}
A common method among state of the art approaches is the usage of Graph Neural Networks on graphs that model the sensor network \cite{bui22}.
The values of a sensor are applied to the same graph node for each timestep which prohibits any non-stationary sensors . 
With fixed sensor locations, the resulting sensor network is highly correlated with the road network.
Streets connecting two intersections with sensors should be also an interesting point for correlations in the sensor network.
However, variable observations and high temporal sparsity rates can not be modeled adequately in a static network.
We show in our experiments that the road network has only a small influence on the traffic predictions.

Besides the traffic prediction for future timesteps, some works explore the field of traffic speed imputation \cite{Cini22, Cuza22} where missing sensor values are predicted.
But the amount of missing values is assumed to be at most 80\%, which on average are still over 40 given sensors in each timestep in the Metr-LA dataset with a total of 207 sensors.
We consider up to 99.9\% missing values which are on average 2.4 observations in each timestep that are used as input.
Such high sparsity rates drastically decrease the chance that multiple values are present in one input sample from the same sensor location, which makes it challenging to recognize and learn temporal correlations for each location on its own.

High sparsity rates (>95\%) result in few sensor values, but if a reconstruction of the traffic state would be possible, we question if spatio-temporal graphs require nodes for each sensor.
In SUSTeR we utilize only a small amount of graph nodes for the encoding of information and do not relate such nodes to the sensor network.
We call this the hidden graph (see figure \ref{fig:intro}), which is still able to reconstruct the complete traffic state.
Due to the reduced number of nodes SUSTeR achieves faster runtimes, as shown in the experiments.
This hidden graph is not embedded directly in the spatial domain, which is why the assignment of observations, as well as the querying of the future traffic, is done with an encoder and a decoder, implemented as neural networks.
The decoding from the hidden graph to future values depends on a set of query locations.
Figure \ref{fig:intro} shows the query locations as given from outside and in combination with the reconstructed traffic state the future values are predicted.

To construct the hidden graph we encode observations from each timestep into from multiple graphs, one for each timestep. 
The graphs are created in a residual style and information is added to the node embeddings from the previous timesteps.
We choose this method to incorporate all timesteps equally into the hidden state because the redundant information along the past is non-existing for high sparsity rates.
From the sequence of graphs where our framework inserted the observations step by step we apply STGCN \cite{Yu18}, an algorithm for traffic prediction to find and learn the spatio-temporal correlations on our small number of graph nodes.
The first future timestep of the STGCN is our hidden graph in which the traffic state is reconstructed. 

% Recent work has an implicit embedding of the graph nodes into the spatial domain as the assignment from the sensor to graph node is fixed one by one.
% Because the graph has the same structure as the road network spatio-temporal correlations can be learned between those sensors.
% We reduce the number of nodes and use a non-linear assignment learned data-driven from the observations.

We find in the experiments that SUSTeR outperforms the plain STGCN and modern traffic prediction frameworks like D2STGNN for high sparsity rates $(\geq 99\%)$.
This is equivalent to only $0.2$ to $2.4$ observation for each timestep on average.
SUSTeR uses fewer parameters than the baselines and can train faster and with less training data.
Our main contributions can be summarized as follows:
\begin{itemize}
    \item We introduce a sparse and unstructured variant of the traffic prediction problem with sparsity in all dimensions. The sensors report only a fraction of their values and are arbitrarily distributed in the spatial domain.
    \item We propose SUSTeR, a framework around the STGCN architecture, which maps sparse observations onto a dense hidden graph to reconstruct the complete traffic state.
    Our code is available at github.\footnote{https://github.com/ywoelker/SUSTeR}
    \item We conducts experiments that show that SUSTeR outperforms the baselines in very sparse situations ($\geq 95\%$) and has a competitive performance in low sparsity rates.
    % \item SUSTeR trains a third faster than the next competitor.
\end{itemize}
