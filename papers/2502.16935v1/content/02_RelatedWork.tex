
\section{\textbf{Related Work}}
\label{sec:related}

The field of multivariate time series prediction is well-studied with the use-case of forecasting the traffic speed in cities given some observation history from street sensors \cite{Yu18, Li2021, Lan22}. 
State of the art works achieve satisfying results on the prediction of the next future timesteps from an equal amount of timesteps in the past.
In a timestep the values of all sensors are available and can be used to exploit spatio-temporal correlations for a prediction of all sensors in the future timesteps. 
This problem is continuously adapted to new problem variants, like Ramhormozi et al. introduced the forecasting of trucks dependent on weather conditions \cite{Ramhormozi22}.
To solve the problem of traffic prediction, spatio-temporal patterns between the traffic sensors play a crucial role.
Existing approaches can be divided into approaches that model the spatial dependencies in graphs, and in multi-dimensional tensors \cite{choy20194d}. 
Multi-dimensional with high sparsity rates tensors would be strongly zero-inflated, which results in a large computational overhead.
We present multivariate time series prediction methods and how they model the spatial correlations in graphs to provide a background for our design choices:

\paragraph*{\textbf{Static Graphs}}
For traffic prediction benchmark sets like Metr-LA the road network is available providing the connectivity of the sensors. 
The streets and intersections are assumed as prior knowledge and are incorporated as a static adjacency matrix into the correlation mining \cite{Yu18, Zhou20}.
Such knowledge remains unchanged over the entire prediction process and is used to build the adjacency matrix of the sensor graph.
Within the graph convolutions, the information of the sensors spreads out along the road network.
The motivation is that cars and traffic events (e.g. congestion) propagate along roads which is therefore the base of information flow. 
In addition to the road network, Shao et al. \cite{Shao22} have incorporated an attention mechanism that exploits the road network to strengthen connections that are highly correlated and weaken streets that are rarely used. 
We count such methods to static graphs because they are still relying on the known street network and assume that the graph does not change.

\paragraph*{\textbf{Dynamic graphs}}
\label{subsec:dynamic}
Street networks can not capture all spatio-temporal correlations because some correlations are not dependent solely on the streets (e.g. rush hours in office districts) \cite{Li2021, Shao22}.
Also long-range correlations between distant locations can not be exploited because the information between two sensors with many hops in between will be averaged out.
Different from the static graphs, a self-adaptive transition matrix is created completely data-driven. 
One of the earliest works in the traffic forecasting context with such a dynamic graph is Graph Wavenet \cite{Wu2019}, where the similarity of node embeddings is transformed into an adjacency matrix.
Lan et al. created an architecture, which infers the important (i.e., highly correlated) sensor connections only from the sensor time series \cite{Lan22}.
This motivates our usage of a similar approach to not rely on the road network for the connection of the nodes in our hidden graph. 
A static linking between the sensors and the graph nodes can not capture observation from moving sensors. 
Li et al. advocate for a dynamic graph arguing that distant districts of a city could still be correlated, despite not being directly connected in a road network \cite{Li2021}. 
For example, two office districts are likely to have similar rush hour behavior, even if they are located in different parts of the city.
Such latent connections are not explicitly available prior to training (even when considering the road network) and need to be learned from the observations in a data-driven approach.

In dynamic graphs the connection between similar districts can be strengthened by the learning process, but the graph nodes across different districts will behave similarly because the Graph Convolutions share the same weights for all nodes, although the districts can be different.
Bai et al. approach this by creating a shared weights dictionary, which is queried by the node embedding. \cite{Bai20}
The node embedding acts as clustering, connecting similar districts through the learned adjacency matrix and applying different weights to different districts. 

\paragraph*{\textbf{Sparse Time Series Forecasting}}
The previously discussed approaches assume that the available traffic data is complete.
However, missing data due to sensor failure or lack of sensors calls for the consideration of missing values in traffic prediction.
Recent imputation strategies on sparse traffic prediction handle up to 80\% missing values \cite{Cuza22}, which can be also achieved to a certain extent by traffic prediction methods which are not designed for missing values.
Considering the resource efficiency for traffic monitoring as few sensors as possible should be deployed which raises the question if less than 20\% of the values is also suffcient for a traffic prediction.
Cuza et al. \cite{Cuza22} proposed an imputation method that learns the traffic probability distribution in four velocity bins, which is then used to sample for the missing values.
They introduced a context-aware graph convolutional network (GCN), which is able to differentiate between observed and unobserved nodes in the graph, and only considers the observed nodes as context.
Over multiple iterations, the information is passed through the unobserved nodes, whose context is updated to remember which information was used as input.
Their approach assumes a previously known, static `edge graph`, which is extracted from the road network and has the same disadvantages as the static graphs approach.
Cui et al.'s work \cite{Cui20} has a lower imputation rate of up to 40\% and solves the problem with a spectral graph Markov network which uses a Markov process to model the temporal dependencies.
Another recent imputation work `Filling the G\_AP\_S` \cite{Cini22} proposed a recurrent graph neural network approach, which uses a spatio-temporal encoder to embed the nodes into a latent space.
Their imputation is primarily based on this latent space to impute temporal correlations at a single point in space, because they argue that sensor failures in such a network are rare and will involve only single sensors.

% In contrast to learning single time series profiles on pre-defined nodes we see our challenge in such high sparsity that the information of surrounding sensors are not available.
% Also, we see the future in moving sensors due to saving resources and therefore we rely on graphs that can handle sensors in changing locations and are not expecting the same sensor locations in each timestep as is the case for all three mentioned imputation approaches.
