\section{Related Work}
\label{related_work}
\vspace{-3pt}

The ESPnet-SpeechLM toolkit builds upon prior works in two main directions:

\noindent\textbf{Text LLM ecosystem:} 
Some popular development tools in text LLM ecosystems can be generalized to any large-scale sequential modeling task, which means they are also suitable for SpeechLM training. Examples of this include DeepSpeed ____ and FlashAttention ____.
To preserve text capability, it is common to initialize SpeechLMs from pre-trained text LLMs, which can rely on open-source platforms like HuggingFace Transformers\footnote{\url{https://github.com/huggingface/transformers}}. These tools are integrated into ESPnet-SpeechLM. 
We also noticed that current text LLM training frameworks ____ provide limited support for speech features. Our toolkit is presented as a supplement in this direction.

\noindent\textbf{Open-Sourced SpeechLMs and Speech Toolkits:} Current research on SpeechLMs and their transparency has been significantly advanced by prior open-source SpeechLM research works ____.
SpeechLM research also greatly benefits from general speech processing and sequence-to-sequence modeling toolkits ____, as they provide a wide range of components applicable to SpeechLM development. 
ESPnet-SpeechLM is presented as a combination of cutting-edge SpeechLM research and well-established speech processing techniques within the open-sourced community. 
More specifically, it is built upon the existing ESPnet ____ codebase to better exploit prior community efforts and compare with existing non-SpeechLM works.
We summarize ESPnet-SpeechLM and related codebases in Tab.\ref{tab:comparison}.

% Chat-GPT revision:
% The ESPnet-SpeechLM toolkit builds upon prior works in two primary directions:

% \noindent\textbf{Text LLM Ecosystem: }
% Popular tools in the text LLM ecosystem have proven effective for large-scale sequential modeling and are also applicable to SpeechLM training. Notable examples include DeepSpeed ____ and FlashAttention ____, which optimize training efficiency for models with substantial parameter counts. Additionally, initializing SpeechLMs from pre-trained text LLMs is a common practice to preserve textual capabilities, supported by open-source platforms like HuggingFace Transformers\footnote{\url{https://github.com/huggingface/transformers}}. ESPnet-SpeechLM seamlessly integrates these tools to facilitate streamlined development.

% However, existing text LLM training frameworks, such as Megatron ____ and LlamaFactory ____, provide limited functionality specific to speech processing tasks. ESPnet-SpeechLM addresses this gap, complementing these frameworks by incorporating tailored features for SpeechLM development.

% \noindent\textbf{Open-Source SpeechLMs and Speech Toolkits: }
% The transparency and accessibility of SpeechLM research have been significantly enhanced by open-source initiatives, including GLM-Voice ____, Mini-Omni ____, MOSHI ____, UniAudio ____, and VoxTLM ____. These works have advanced state-of-the-art SpeechLM capabilities, such as instruction following, multi-tasking, and real-time interaction.

% Further, SpeechLM development has benefited greatly from general-purpose speech processing and sequence-to-sequence modeling toolkits. Resources such as ESPnet ____, SpeechBrain ____, Amphion ____, S3PRL ____, NVIDIA NeMo ____, and Fairseq ____ have provided reusable components essential for tasks like data preprocessing, tokenization, and model architecture design.

% ESPnet-SpeechLM combines cutting-edge SpeechLM research with these established speech processing techniques. By building on the foundation of ESPnet ____, it leverages prior community contributions to enhance efficiency and ensure compatibility with non-SpeechLM frameworks. A comparative summary of ESPnet-SpeechLM and related codebases is presented in Tab.\ref{tab:comparison}.