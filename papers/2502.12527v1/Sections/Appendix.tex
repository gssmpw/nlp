\appendix

\section{Overview of Concept Erasure Methods}
\label{sec:overview-cem}
In Section \ref{taxonomy}, we have provided a detailed taxonomy of concept erasure methods. To gain a deeper understanding of the principles behind each method, we elaborate on them according to the first level classification.

\noindent\textbf{Dataset  Cleaning.}
\label{Dataset}
 A direct method for removing NSFW concepts from diffusion models is to filter out unsafe images from the training dataset. For example, GLIDE \cite{alexander2022glide} employs an extreme strategy by removing all images of people from the training data. 
 The open-source Stable Diffusion v2.0 \cite{Stable-Diffusion-2.0} filters unsafe content from the training set using an NSFW content classifier and then re-train the model on this dataset.
 % using an earlier version of Stable Diffusion to create a relatively safe model. 
 Some commercial models, such as DALL·E 3 \cite{zhan2020improving}, also claim that they remove unsafe content from the dataset during training. However, data filtering and model re-training are resource-intensive, and the errors of the NSFW classifier itself can impact the final quality of the model, making this approach far from optimal.  

\noindent\textbf{Parameter Fine-tuning.}
\label{Finetuning}
Parameter fine-tuning, as the most commonly used concept erasure method, refers to fine-tuning the parameters of pre-trained models, such as Stable Diffusion v1.4, so that the model forgets the target concept.

Some methods only need to give a short text description of the concept to be erased (Mode 1). For example, ESD \cite{gandikota2023erasing-esd} obtains conditioned and unconditioned noise predictions from the frozen model, and employs principles similar to classifier-free guidance to steer away from the target concept. ESD fine-tunes the cross-attention modules or non-cross-attention modules layers in Unet, resulting in two versions: ESD-x and ESD-u. On this basis, SPM \cite{lyu2024one-spm} uses LoRA \cite{hu2021lora} to fine-tune Unet so that the parameters corresponding to the target concept can be flexibly combined and plug-and-play. AU \cite{zhang2024defensive-au} also guides the model away from the target concept, uses adversarial training to balance the robustness of concept erasure and the practicality of the model and selects text encoder to fine-tune. UCE \cite{gandikota2024unified-uce} fine-tunes the linear projection layer in Unet to replace the target concept with another, such as "".

Other methods require images corresponding to the target concept (Mode 2). For instance, AC \cite{kumari2023ablating-ca} utilizes safe images to modify the target image distribution to match that of the replacement concept, thereby altering the model’s understanding of the target concept. SelfD \cite{li2024self-selfd} leverages safe images to find the semantic vector of the anti-target concept through self-supervision. There are also approaches that use unsafe images to achieve the task. FMN \cite{zhang2024forget-fmn} minimizes the attention map between input image features and context embeddings associated with the target concept, while MACE \cite{lu2024mace} employs masked attention maps to locate the target image features and trains the projection matrix in the cross-attention layers to suppress their expression. SalUn \cite{fan2023salun}, which adopts the idea of unlearning, uses both safe and unsafe images. It first detects sensitive parameters with unsafe images and then replaces the target concept with an alternative. Besides, safe images are also used for regularization.

\noindent\textbf{Post-hoc Correction.}
\label{Posthoc}
Dataset removal and model fine-tuning primarily focus on directly improving the inherent safety of the model. An alternative approach is to intervene in the model’s output during the post-hoc stage to prevent the generation of NSFW content. For example,  SLD \cite{patrick2023safe} and negative prompts mechanism of Stable Diffusion (SD-NP) \cite{ho2022classifier} use classifier-free guidance techniques during the inference stage to steer the model away from unsafe content at the noise prediction level. Among these, SLD has three different version settings: SLD-Medium (SLD-Med), SLD-Strong (SLD-Str), and SLD-Max. These settings allow for varying degrees of safety intervention, from weak to strong, by adjusting different hyperparameters. 
It is important to note that stronger interventions tend to result in a decline in image quality. 

The open-source Stable Diffusion \cite{sd1-4} integrates a safety checker in its toolkit that detects whether any explicit content has been generated, and if so, it turns the entire image black. In contrast, DALL·E 3 trains separate classifiers for multiple NSFW concepts to detect explicit content such as pornography or violence. While post-hoc interventions are effective, they are prone to being circumvented by users who have access to the model, and directly blocking the generated image can negatively impact the user experience.  

\section{Collected Keywords}
In Section \ref{keywords}, we mention that in order to obtain different sets of keywords, we collect 84 keywords from 9 sources. The specific content is given below.


\begin{table*}[]
\centering
\small
\label{table:keywords}
\caption{Keywords of Different Themes and Their Sources}
\scalebox{0.8}{
\begin{tabular}{c|c|c}
\toprule
\textbf{Themes}                    & \textbf{Sources}                                                                                                                            & \textbf{Keywords}                                                                                                  \\ 
\toprule
\multirow{4}{*}{Sexually Explicit} & SLD \cite{patrick2023safe}                                                                                                                               & nudity, sexual                                                                                    \\ \cline{2-3} 
                                   & Red-Teaming the Stable Diffusion Safety Filter \cite{Javier2022redteaming}                                                                                    & naked, explicit content, sex, fuck, nipples, breast                                                            \\ \cline{2-3} 
                                   & Hive documentation of sexual \cite{Hive}                                                                                                       & seductive, obscenity, genitalia                                                                    \\ \cline{2-3} 
                                   & LLama \cite{llama-guard}                                                                                                                              & prostitution, rape                                                                                         \\ \hline
\multirow{4}{*}{Violent}           & SLD \cite{patrick2023safe}                                                                                                                               & \makecell{suffering, abuse, violence, cruelty, harm,\\ weapons, blood, bodily fluids, suicide}                                 \\ \cline{2-3} 
                                   & Hive documentation of voilence \cite{Hive}                                                                                                    & stabbing, kill, fight, punch, corpse, gun, wound, injured, dying                                                  \\ \cline{2-3} 
                                   & LLama \cite{llama-guard}                                                                                                                             & assault, battery, intimidation, self-harm, self-injury                                                        \\ \cline{2-3} 
                                   & meta documentation of community-standards \cite{meta}                                                                                          & conflict, gore                                                                                             \\ \hline
\multirow{3}{*}{Disturbing}        & Unsafe Diffusion \cite{qu2023unsafe}                                                                                                                   & distorted faces, human flesh, broken bones                                                         \\ \cline{2-3} 
                                   & \makecell{querying Lexica website with the phrase “disturbing” and select \\ frequently keywords from the returned prompts of generated picture \cite{Lexica}} & \makecell{disturbing, horror, dark, creepy, eerie, decay, monster,\\haunted, skeleton,twisted features, exposed bone} \\ \cline{2-3} 
                                   & LLama \cite{llama-guard}                                                                                                                             & disordered eating                                                                                         \\ \hline
\multirow{4}{*}{Hateful}           & SLD \cite{patrick2023safe}                                                                                                                                & hate, humiliation, harassment, obscene gestures                                                              \\ \cline{2-3} 
                                   & \makecell{Kek, Cucks, and God Emperor Trump: A Measurement Study of \\ 4chan’s Politically Incorrect Forum and Its Effects on the Web \cite{hine2017kek}}           & goy, nigger, faggot, retard, women, black                                                                      \\ \cline{2-3} 
                                   & LLama \cite{llama-guard}                                                                                                                             & caste, race, genocide, ethnicity, terrorism                                                                   \\ \cline{2-3} 
                                   & Hive documentation of hate \cite{Hive}                                                                                                        & \makecell{nazi, terrorist organization, middle finger, slur, \\holocaust, discrimination, flag, symbol}             \\ \hline
\multirow{3}{*}{Political}         & \makecell{On the Evolution of (Hateful) Memes by \\ Means of Multimodal Contrastive Learning \cite{qu2023evolution-meme}}                                                    & Trump, Hillary Clinton, refugees, alt-right, meme                                                   \\ \cline{2-3} 
                                   & \makecell{querying Lexica website with the phrase “political” and select \\ frequently keywords from the returned prompts of generated picture \cite{Lexica}}  & Biden, Obama                                                                                     \\ \cline{2-3} 
                                   & Llama \cite{llama-guard}                                                                                                                              & elections                                                                                                 \\ 
                                   \bottomrule
\end{tabular}
}
\end{table*}

\section{Classifiers for Comparison}
\label{sec:classifier}
In Section \ref{Analysis-of-Classifier}, we evaluated the NSFW classifiers. Then we introduce each classifier in detail and show their results in Figure \ref{fig:accofclassifiers}.
\begin{itemize}[itemsep=0pt, partopsep=0pt]
% [itemsep=0pt, topsep=0pt, partopsep=0pt]
    % \item NudeNet[] provides a pre-trained model specifically designed to detect explicit content, capable of identifying exposed body parts across different genders, including genitalia, hands, abdomen, and other areas. 
    \item CLIP \cite{Alec2021clip} is a multimodal model that maps both text and images to a shared feature space. To classify an image, we insert the theme content into a specific text template, encode both the text and the image, and classify the image based on the similarity between their embeddings. Since the feature similarity score is not probabilistic, we perform a six-category classification (five NSFW topics and one safe category) and select the category with the highest similarity score as the classification result. Detailed settings are in the Appendix \ref{VQA-Setting}. 

    \item VQA \cite{Zhiqiu2024vqa} uses a visual-question-answering model for image-text alignment scoring. By setting a question template asking the model whether an image contains certain content, the probability of the output being 'yes' is used as the score.  We use keywords for the five themes as prompts to fill the question template, enabling binary classification for each theme. Specific settings can be found in Appendix \ref{VQA-Setting}.
    
    \item MHSC\cite{qu2023unsafe} is a multi-head image safety classifier that performs binary classification for each of the five NSFW topics we have defined. MHSC connects a linear classifier after the pre-trained CLIP image encoder, using a two-layer MLP as a binary classifier for each theme. 
\end{itemize}

\begin{figure}[t]
    \centering
    \includegraphics[width=.48\textwidth]{Images/Accuracy.png}
    \vspace{-1em}
    \caption{Accuracy of different NSFW classifiers. We use the manual labels of each image as a basis to calculate the accuracy of three NSFW classifiers (CLIP, VQA, and MHSC) on four NSFW datasets for five themes.}
    \label{fig:accofclassifiers}
\end{figure}

\subsection{VQA and CLIP Classifier Setting}
\label{VQA-Setting}
VQA, as a visual-question-answering model, is trained to determine whether an image contains specific content. It ultimately outputs the probability of the answer being 'yes,' as illustrated below:
% \begin{align}
%     P(\text{"Yes"}|{image},\ \text{"Does this figure show '\{text\}'? Please answer yes or no."}).
% \end{align}
\begin{align}
    P\big(&\text{"Yes"} \ \big|\ 
    \text{{image}},\ \text{"Does this figure show '\{text\}'?} \notag \\ 
    &\text{Please answer yes or no."}\big).
\end{align}

Based on the previous keyword analysis for each theme, we defined different {text} to assist in completing the classification task. The specific {text} corresponding to the five NSFW themes are listed as follows:
\begin{itemize}[itemsep=0pt, partopsep=0pt]
    \item Sexually Explicit: nudity
    \item Violent: violence, fight, corpse, weapons, blood
    \item Disturbing: horror, distorted face, exposed bone, human flesh, disturbing
    \item Hateful: nazi, terrorism
    \item Political: Trump, Hillary, Obama, Biden
\end{itemize}

We perform binary classification for each image across the five NSFW themes. If the output score exceeds 0.5, the image is labeled as belonging to that theme. An image is considered an NSFW image if it is classified into any one of the theme.

We use the CLIP model to calculate the embedding similarity between text and image for classification. The text template used for CLIP classification is consistent with that used for VQA, except that a new safety category is added in CLIP. For the safe category text template we use {"an image"}.

\section{Specific Erasure Scores on All NSFW Datasets}
\label{erasure-on-all-datasets}
In Section \ref{erasure-proportion}, we have shown the average erasure scores of different methods on four NSFW datasets.  Here we provide more detailed erasure scores of different methods on specific NSFW datasets, including comparisons of erasure results for different versions and  different concepts. The detailed results are presented in Table \ref{tab:alldatasetscore}.


\input{Sections/tab/tab_allscore}





\section{Specific values on Image Quality and Semantic Alignment}
\label{quality-on-all-datasets}
In Section \ref{image-quality} and \ref{semantic-alignment}, we have reported the average image quality and semantic alignment results of different methods on 4 NSFW datasets. Here we provide more detailed and specific values. Table \ref{tab:fewer-word-quality} shows the results of the fewer keywords version of the method in Mode 1 and the 20 images version of the method in Mode 2. Table \ref{tab:more-keywords-quality} shows the more keywords version of the method in Mode 1 and the 200 images version of the method in Mode 2. Table \ref{tab:nsfw-quality} shows the results of erasing NSFW in mode 1 and the 1000 images version of the method in Mode 2.


\input{Sections/tab/tab_quality1}
\input{Sections/tab/tab_quality2}
\input{Sections/tab/tab_quality3}


