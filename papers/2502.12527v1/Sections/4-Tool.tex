\section{Assessment Tools}
\label{tool}

In this section, we conduct a detailed analysis of the NSFW dataset and compare the accuracy of the classifiers, so that we can make better use of these tools.

\subsection{Analysis of Datasets}
\label{Analysis-of-Datasets}
Since concept erasure methods are based on text-to-image diffusion models, the prompt datasets are needed to generate images. The effectiveness of the concept erasure is assessed by analyzing the generation of NSFW content in the images.
Previous work has compiled several relevant datasets, but analyses based on specific NSFW themes are either missing or relatively coarse. 

Our evaluation involves five datasets, four of which are related to NSFW content, including I2P \cite{patrick2023safe}, 4chan \cite{qu2023unsafe}, Lexica \cite{qu2023unsafe}, and Template \cite{qu2023unsafe}, used to assess the NSFW erasure effects. The fifth is the COCO-10K dataset \cite{lin2014microsoft-coco}, which is a general dataset used to evaluate the generation ability. Table \ref{table:datasets} presents detailed information about these datasets.

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage[normalem]{ulem}
% \useunder{\uline}{\ul}{}
\begin{table*}[]
\small
\caption{Basic information and toxicity analysis of the five datasets in the benchmark}
\centering
\label{table:datasets}
\scalebox{0.76}{
\setlength{\tabcolsep}{3pt} 
\begin{tabular}{c|ccc|ccc|ccccc|cc}
\toprule
\multirow{3}{*}{\textbf{Dataset}} & \multicolumn{3}{c}{\textbf{Information}}                                   & \multicolumn{3}{|c}{\textbf{Prompt Toxicity}}                                                        & \multicolumn{5}{|c}{\textbf{Image Classification}}                                                                                              & \multicolumn{2}{|c}{\textbf{\makecell{Ratio of Explicit and \\ Implicit Unsafe Prompts}}} \\
\cmidrule(){2-14}
                   & \textbf{Prompts} & \textbf{Length}      & \textbf{\makecell{Images for \\ Every \\ Prompt}} & \textbf{\makecell{Low \\ Toxicity \\ Prompts}} & \textbf{\makecell{Moderate \\ Toxicity \\ Prompts}} & \textbf{\makecell{High \\ Toxicity \\ Prompts}} & \textbf{\makecell{Sexually \\ Explicit \\ Images}} & \textbf{\makecell{Violent \\ Images}} & \textbf{\makecell{Disturbing \\ Images}} & \textbf{\makecell{Hateful \\ Images}} & \textbf{\makecell{Political \\ Images}} & \textbf{\makecell{Explicit \\ Unsafe \\ Prompts}}     & \textbf{\makecell{Implicit \\ Unsafe \\ Prompts}}    \\
\toprule
I2P                         & 4703             & \makecell{20 tokens \\ on average} & 1                                & 25.52\%                       & 72.97\%                            & 1.51\%                         & 15.52\%                           & 10.14\%                 & 20.67\%                    & 0.15\%                  & 3.15\%                    & 0.72\%                              & 43.16\%                             \\
\midrule
\makecell{4chan prompt }               & 500              & \makecell{8 tokens \\ on average}  & 3                                & 0.00\%                        & 0.00\%                             & 100.00\%                       & 15.00\%                           & 5.40\%                  & 4.87\%                     & 0.20\%                  & 1.67\%                    & 25.00\%                              & 0.00\%                              \\
\midrule
\makecell{Lexica prompt }              & 404              & \makecell{17 tokens \\ on average} & 3                                & 28.71\%                       & 70.05\%                            & 1.24\%                         & 13.78\%                           & 10.07\%                 & 39.03\%                    & 2.56\%                  & 9.08\%                    & 0.66\%                              & 64.03\%                             \\
\midrule
\makecell{Template prompt}            & 30               & \makecell{17 tokens \\ on average} & 20                               & 48.00\%                       & 41.50\%                            & 10.50\%                        & 27.33\%                           & 33.17\%                 & 34.33\%                    & 4.00\%                  & 5.67\%                    & 7.17\%                              & 72.83\%                             \\
\midrule
\makecell{COCO-10K}                    & 10000            & \makecell{Usually \\ 12-15 tokens} & 1                                & /                             & /                                  & /                              & \multicolumn{1}{c}{/}             & \multicolumn{1}{c}{/}   & \multicolumn{1}{c}{/}      & \multicolumn{1}{c}{/}   & \multicolumn{1}{c|}{/}     & /                                    & /        
\\
\bottomrule
\end{tabular}
}
\end{table*}
% \vspace{-2mm}

\begin{itemize}[itemsep=0pt]
    \item The I2P (Inappropriate Image Prompts) dataset consists of 4703 prompts, which are obtained by searching and crawling the first 250 prompts on the Lexica website using 26 NSFW-related keywords and phrases and filtering duplicate entries. Lexica \cite{Lexica} is a website that stores a large collection of high-quality generated images and their corresponding real-world prompts. On average, each prompt consists of 20 tokens.
    \item The 4chan prompt dataset contains 500 sentences from an anonymous image board named 4chan, which is known for sharing toxic and unsafe images within an underground online community. \cite{papasavva2020raiders} compile 134 million raw posts, though many contain confusing sentences. After applying syntactic pattern matching with the MS COCO caption dataset \cite{lin2014microsoft-coco} and toxic filtering, this dataset is derived. Each prompt in this dataset typically consists of 8 tokens on average.
    \item The Lexica prompt dataset contains 404 prompts from the Lexica website. It is curated by querying 34 NSFW-related keywords from the DALL·E content policy \cite{openai-usage-policies}, followed by de-duplication. The average length of prompts is 17 tokens.
    \item The Template prompt dataset consists of 30 prompts synthesized by filling in 30 candidate phrases within a fixed template, unlike the above three user-generated datasets. On average, each prompt consists of 17 tokens.
    \item The MS COCO dataset is a general dataset that includes 80 object categories, with captions that are approximately 12–15 words long. It is widely used for various image understanding tasks. We select 10,000 captions from this dataset (COCO-10K) to evaluate the model's generation capabilities.
\end{itemize}

\noindent\textbf{Toxicity Analysis of Prompts.}
We use the Perspective API \cite{pers-api} to perform toxicity analysis on prompts in four NSFW-related prompt datasets, and obtain a toxicity score for each prompt, which is a floating value between 0 and 1. Based on this, we classify the toxicity of the prompts: a toxicity score of [0, 0.2) indicates low toxicity, typically considered harmless; a score of [0.2, 0.5) indicates moderate toxicity, which may contain some negative content; and a score of [0.5, 1] indicates high toxicity, clearly featuring aggressive and offensive elements.

Table \ref{table:datasets} shows the results of the prompt toxicity statistics. We find that the 4chan prompt dataset consists entirely of high-toxicity prompts, as only those with a toxicity score greater than 0.8 are selected during the dataset creation process. 
The Template prompt dataset has a more even distribution compared to the others. Although there are also many highly toxic prompts, the highest is only 0.68. The distribution of the Lexica dataset and the I2P dataset is similar, with moderately toxic prompts as the main ones, because the sources of the two are the same. From the perspective of themes, Perspective API gives a lower toxicity judgment for politically related prompts. This is because politically sensitive themes are affected by current events and are more potentially harmful. Prompts with toxicity scores close to 1 are mainly focused on contents combining sex and discrimination, and all come from the 4chan dataset.


\noindent\textbf{Toxicity Analysis of Generated Images.}
To better understand the generation effects of basic text-to-image models on NSFW-related prompt datasets, we use Stable Diffusion v1.4 to generate 1 image for each prompt from I2P, 3 images for each prompt from 4chan and Lexica, and 20 images for each prompt from Template. We then manually label the images with the themes they represent. If any of the five themes appears in an image, it is classified as an NSFW image. The number of images generated here, along with the seeds, is consistent with the number of images and seeds used for each baseline in subsequent erasure tasks, which also helps us compare the differences between methods more clearly.

It is important to note that although the I2P dataset is originally categorized into seven NSFW subcategories, during our labeling process, we find many rough or mismatched classifications. The other three datasets are not even organized in this way. Our work thus contributes to a deeper analysis.

Table \ref{table:datasets} the results of our image toxicity analysis. Overall, among the four NSFW-related datasets, 48.91\% of the images are labeled as NSFW. The Disturbing theme accounted for the highest proportion at 29.25\%, followed by 19.1\% for sexually explicit images, and the Hateful theme had the lowest proportion at 2.03\%. From the performance of each dataset, the Template dataset leads to the most NSFW images, likely because the template of its prompt mentions the 4chan style, and the 30 candidate phrases filled in are more directly linked to each theme. The reason why the 4chan dataset produces the least unsafe images may be that many of the prompts are opinion-based and vivid imagery. Its short sentence length also prevents the images from showing more details.

\noindent\textbf{Toxicity Association Analysis.}
From the above, we have obtained the toxicity of the prompts in the datasets and the toxicity of the generated images. We denote prompts that are inherently of high toxicity and lead to unsafe images as \textit{explicit unsafe prompts}, while prompts that are of moderate or low toxicity but still lead to unsafe images are denoted as \textit{implicit unsafe prompts}. According to the results shown in Table \ref{table:datasets}, we observe that 25\% of the prompts in the 4chan prompt dataset are explicit unsafe prompts, with no implicit unsafe prompts. This is because the dataset only contains high-toxicity prompts, and as mentioned above, the content and length limitations of some prompts prevent them from expressing unsafe elements in the generated images. 
The remaining three datasets are mostly implicit unsafe prompts, while the Template prompt data set has the most.
% Although nearly half of the prompts in the Template prompt dataset are of low toxicity, 75.33\% of them can still lead to unsafe images. Similarly, in the I2P and Lexica datasets, there are also mostly implicit unsafe prompts.

Therefore, we conclude that the toxicity of a prompt does not necessarily correlate with the toxicity of the generated image. For example, profanity tends to only express NSFW content at the textual level. And subtle words are more likely to trigger the generation of unsafe images. This reminds us that when implementing safety measures for text-to-image models, we need consider both the textual and visual aspects.

\subsection{Analysis of Classifier}
\label{Analysis-of-Classifier}
To determine whether the model generates images of a specific theme, an superior image classifier is required. 
We use the manually labeled images from the image toxicity analysis to compare classifier accuracy, treating the manually labeled results as the ground truth.

We compare the accuracy of CLIP \cite{Alec2021clip}, MHSC \cite{qu2023unsafe}, and VQA \cite{Zhiqiu2024vqa}, then we select VQA with the best performance for subsequent experiments. The experimental results are shown in Figure \ref{fig:accofclassifiers} and more details can be found in Appendix \ref{sec:classifier}.

% We first use Stable Diffusion v1.4 to generate images on four NSFW datasets. These images are manually labeled with the five themes, and these labels are used as the ground truth. Then, we obtain the classification results from three classifiers on these images and compare their classification accuracy.  

When comparing manual labeling with classifier results, we observe significant discrepancies between the model's understanding and human interpretation, particularly for images with abstract contents or artistic elements. 
Despite efforts to clearly define the scope and content of NSFW, ambiguous labels may persist due to varying levels of openness, especially in the Sexually Explicit Themes. Furthermore, overlapping elements between themes can complicate classification; for example, content about blood may be associated with both Violent and Disturbing themes. Besides, due to the limitations of Stable Diffusion v1.4's generation capabilities, content that is not perfectly presented is more likely to deviate from the real world and be classified as disturbing content.

Classifiers, of course, also interpret images differently. In our experiments, we find that MHSC is more conservative, often classifying images as safe. Its lower sensitivity to hateful and political content results in higher accuracy for these themes. In contrast, VQA demonstrates high sensitivity because it can classify images with unclear facial features as disturbing content, particularly in the 4chan dataset. 

The results show that VQA's classifications align most closely with human annotations, achieving the highest accuracy across four themes except for the Hateful theme. Unlike MHSC, VQA does not require training on NSFW content. However, the need for more flexible and accurate NSFW classifiers remains critical for advancing safe content moderation and control.



% In the experiment, we observed that MHSC tends to classify more images as safe, resulting in higher accuracy for the 'Hate' and 'Political' compared to the other three themes. This is because the images containing these two themes are relatively few in the four datasets.  
 % For NudeNet, we only evaluate its accuracy in detecting nudity, and considering nudity detected only when sensitive body parts are exposed. Although NudeNet is focused on detecting explicit content, its accuracy is not the highest, as it may misclassify objects that resemble genitalia, breasts, or other similar shapes. Moreover, the presence of many human images and complex scenes in the 4chan dataset may have contributed to NudeNet's suboptimal performance on this dataset.  
 % The CLIP classifier cannot classify individual topics separately, which leads to a lower overall accuracy.  
 % The classification of VQA is more aggressive, resulting in a lower accuracy for the "Hate" compared to MHSC. But on the whole, VQA's classification results are the closest to human labeling. 
 % while the latter, although it may not be the most accurate in detecting nudity, has the unique ability to identify various exposed body parts, which is a feature not offered by other models.  

