\section{Background and Related works}

\subsection{NSFW (Not Safe for Work) Content}

In this paper, we use the definition of NSFW in \cite{gebru2021datasheets}, "[data that] if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety". Texts and images containing NSFW information are referred to as "unsafe text" or "unsafe pictures", respectively.

With the development of the internet and social media, NSFW has become a prominent issue, prompting various countries and organizations to propose laws and restrictions to address this problem. 
For example, the European Union's Digital Services Act \cite{eu-cybercrime} holds platforms accountable for harmful online activities, as well as the dissemination of disinformation. The introduction of the UN Convention against cybercrime \cite{un-cybercrime} further encourages international cooperation to tackle issues related to cybercrime and the proliferation of illegal content. Leading social platforms and online communities, such as YouTube and Twitter, have established specific guidelines and review standards to restrict users from encountering sensitive content in public spaces. The rapid development of generative AI has brought this issue to the forefront.
% , prompting tech companies to implement safety measures. 
OpenAI, a pioneering research organization, has adopted both universal and service-specific policies \cite{openai-usage-policies} and integrated safety controls in testing, user prompts, and model alignment \cite{openai-safety-updates}.

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.95\textwidth]{Images/Define.pdf}
    \vspace{-1em}
    \caption{NSFW is divided into five themes. We provide descriptions for these five themes and include image examples for a more concrete illustration. Since the erasure methods use keyword sets as erasure targets, we also present the complete keyword set for the more keywords version and a more generalized less keywords version.}
    \label{fig:define}
\end{figure*}

\begin{table*}[t!]
\small
\centering
\caption{Comparison of ours and other benchmarks}
\label{tab:benchmarks}
\scalebox{0.78}{
\setlength{\tabcolsep}{2pt}
\begin{tabular}{c|ccc|cc|cccccc}
\toprule
\multirow{2}{*}{\textbf{\makecell{ \\ \\Benchmark}}} & \multirow{2}{*}{\textbf{\makecell{\\Taxonomy of \\ Concept \\ Erasure Methods}}} & \textbf{\multirow{2}{*}{\makecell{\\Toxicity \\ Analysis of \\ Datasets}}} & \multirow{2}{*}{\textbf{\makecell{\\Comparison \\ of \\ Classifiers}}} & \multicolumn{2}{c|}{\textbf{Assessment  Content}}                   & \multicolumn{6}{c}{\textbf{Assessment  Perspectives}}                                                                                                \\
\cline{5-12}

                           &                                                                  &                                                &                                             & \textbf{\makecell{Specific Theme \\ in \\ NSFW Content}} & \textbf{\makecell{ Impact of \\ Data \\ Required}} & \textbf{\makecell{Erasure \\ Proportion}} & \textbf{\makecell{Impact of Explicit \\ and Implicit \\ Unsafe Prompts}} & \textbf{\makecell{Excessive \\ Erasure}} & \textbf{\makecell{Image \\ Quality}} & \textbf{\makecell{Semantic \\ Alignment}} & \textbf{Robustness}  \\
\toprule
UnsafeD \cite{qu2023unsafe}                    & \textbf{\ding{55}}                                                               & \textbf{\ding{51}}                                            & \textbf{\ding{51}}                                         & \textbf{\ding{55}}                             & \textbf{\ding{55}}                      & \textbf{\ding{51}}                & \textbf{\ding{55}}                                             & \textbf{\ding{55}}                & \textbf{\ding{55}}            & \textbf{\ding{55}}                 & \textbf{\ding{55}}          \\
UCANVAS \cite{zhang2024unlearncanvas}                   & \textbf{\ding{55}}                                                               & \textbf{\ding{55}}                                             & \textbf{\ding{55}}                                          & \textbf{\ding{55}}                             & \textbf{\ding{55}}                      & \textbf{\ding{51}}                & \textbf{\ding{55}}                                             & \textbf{\ding{55}}                & \textbf{\ding{51}}           & \textbf{\ding{55}}                 & \textbf{\ding{55}}          \\
HUB \cite{moon2024holistic}                       & \textbf{\ding{55}}                                                               & \textbf{\ding{55}}                                             & \textbf{\ding{55}}                                          & \textbf{\ding{55}}                             & \textbf{\ding{55}}                      & \textbf{\ding{51}}                & \textbf{\ding{55}}                                             & \textbf{\ding{55}}                & \textbf{\ding{51}}           & \textbf{\ding{55}}                 & \textbf{\ding{55}}          \\
Ours                       & \textbf{\ding{51}}                                                              & \textbf{\ding{51}}                                            & \textbf{\ding{51}}                                         & \textbf{\ding{51}}                            & \textbf{\ding{51}}                     & \textbf{\ding{51}}                & \textbf{\ding{51}}                                            & \textbf{\ding{51}}               & \textbf{\ding{51}}           & \textbf{\ding{51}}                & \textbf{\ding{51}}        \\
\bottomrule
\end{tabular}
}
\end{table*}

However, the categorization of NSFW has not formed a consensus and may vary according to context, culture, and personal factors. Previous work \cite{patrick2023safe,llama-guard} has proposed various categorization frameworks. To facilitate the identification of unsafe images in our subsequent work, we exclude content such as illegal activities and choose the categorization given in \cite{qu2023unsafe}, which classifies NSFW content into five themes: "Sexually Explicit, Violent, Disturbing, Hateful, and Political."
The description of such categorization and corresponding image examples are illustrated in Figure \ref{fig:define}.


\subsection{Text-to-image Diffusion Models}

Diffusion models for image generation are predominantly based on DDPM\cite{ho2020denoising}, which formulates both the diffusion and denoising processes as markov processes. The forward process is a noise-adding process, where at each time step $t$, Gaussian noise with varying intensity is added to the clean image data $x_0$ to obtain $x_t = \sqrt{\alpha_{t}} x_{0}+\sqrt{1-\alpha_{t}} \epsilon$ where $\alpha$ is a time-dependent noise intensity hyperparameter and $\epsilon$ is Gaussian noise. After $T$ time steps of adding noise, the clean image will ultimately become pure Gaussian noise.
% \begin{equation}
%     x_{t}=\sqrt{\alpha_{t}} x_{0}+\sqrt{1-\alpha_{t}} \epsilon_{t}
% \end{equation}
During the reverse denoising process, the model is trained to predict the noise added during the forward process, enabling the recovery of the original data $x_0$ from the noisy data. This process can be expressed as:  
\begin{equation}
    p_{\theta}\left(x_{t-1} \mid x_{t}\right)=\mathcal{N}\left(x_{t-1} ; \mu_{\theta}\left(x_{t}, t\right), \Sigma_{\theta}\left(x_{t}, t\right)\right)
\end{equation}
where $\mu_{\theta}$ and $\Sigma_{\theta}$ is the mean and variance learned from model.

Latent Diffusion Models (LDM) \cite{rombach2022high} build upon DDPM by performing the diffusion and denoising processes in a low-dimensional latent space. Compared to operations in the pixel space, this significantly enhances the efficiency of diffusion models.   This spatial mapping is typically achieved using a pre-trained encoder $\mathcal{E}$ and a decoder $\mathcal{D}$. For an input image $x$, the encoder maps $x$ to a latent code $z = \mathcal{E}(x)$. The decoder $\mathcal{D}$ can then map a latent code back to the corresponding image, such as $\mathcal{D}(\mathcal{E}(x))\approx x$. 
The network currently used by LDM to predict noise is usually UNet, in which the text condition interacts with the image through the cross-attention layer. 
Given a conditional input $c$, the training objective function of LDM is as follows:  
\begin{equation}
    \mathcal{L}=\mathbb{E}_{z\sim\mathcal{E}(x),t,c,\epsilon\sim\mathcal{N}(0,1)}\left[\|\epsilon-\epsilon_\theta(z_t,c,t)\|_2^2\right]
\end{equation}


Classifier-free guidance technique \cite{ho2022classifier} allows the diffusion model to use an implicit classifier to guide the generation process. During training, the model learns both conditional and unconditional generation. By moving towards conditional scores while simultaneously moving away from unconditional scores, the model generates images that follow the given conditions during the generation phase. Given a guidance scale $\alpha$, the predicted noise at time step $t$ can be expressed as follows:  
\begin{equation}
\tilde{\epsilon}_\theta(z_t,c,t)=\epsilon_\theta(z_t,t)+\alpha(\epsilon_\theta(z_t,c,t)-\epsilon_\theta(z_t,t)).
\end{equation}



\subsection{Risk of Diffusion Model Misuse}

Due to their impressive generative capabilities and accessibility, diffusion models are gaining increasing popularity among users. However, this rise in adoption also exacerbates the risk of misuse. Some studies have highlighted that open-source diffusion models can be easily exploited to generate NSFW images, with users producing pornographic or violent content simply by providing specific prompts\cite{Javier2022redteaming,patrick2023safe}. 
% A community called Unstable Diffusion specializes in generating pornographic images using Stable Diffusion, which attracts a large number of users to create and spread unsafe content on it. 
% At the same time, some online creation platforms (e.g., Lexica, Artbreeder) have been found to allow users to bypass platform restrictions and generate explicit illegal images. Additionally, AI "de-clothing" applications have emerged in various parts of the world, using generative models to create nude images of specific individuals from photos. 
% In 2024, the "AI pimping" industry emerged, with virtual influencers using deepfake technology to promote adult content; the X platform updated its policy to allow adult content while restricting access for minors. In 2025, a scandal in Sydney involved a student using AI to create and distribute deepfake explicit images. These events all demonstrate that the misuse of diffusion models has already caused widespread negative impacts.
The "AI pimping" industry \cite{ai-pimping} replaces the faces of adult content creators in videos with AI-generated ones, creating a surge of virtual influencers who profit by linking to platforms. 
Deepfake \cite{deepfake} technology manipulates videos or images by swapping faces or altering appearances, often creating explicit content without consent, leading to reputational damage and legal issues. Collectively, these cases demonstrate the pressing need for robust safeguards and ethical considerations in the development and deployment of generative models. 



% \subsection{Red Teaming Methods}

% Recent studies suggest using red teaming tools to identify potential vulnerabilities within generative models. In text-to-image models, a typical red teaming method involves constructing adversarial prompts to detect whether the model generates harmful images. For example, CI \cite{minh2024circumventing} employs the textual inversion technology[ ] to train a pseudo-token, enabling the model to reproduce unsafe images. P4D \cite{zhiyi2024p4d} utilizes the original SD model to generate harmful images and then optimizes an adversarial prompt $P^*$ to enable concept erasure methods to produce images similar to the harmful ones. UnlearnDiffAtk \cite{yimeng2024unlearndiffatk} adopts a similar approach, but during training, noise is directly added to the harmful images. And RAB \cite{yu2024ring} does not require access to a specific model. Instead, it leverages a pre-trained text encoder and generates adversarial prompts through relative textual semantics combined with genetic algorithms. In this work, RAB was employed to construct adversarial prompts for various NSFW concepts, evaluating the robustness of concept erasure methods.  