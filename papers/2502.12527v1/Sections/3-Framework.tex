\section{Research Goals and Assessment Framework}
% After gaining a preliminary understanding of the concept erasure methods based on text-to-image diffusion models, we need to assess them on the task of erasing NSFW content. 
We aim to derive conclusions and insights through comprehensive evaluations, thereby facilitating subsequent work and broadening perspectives. We propose the following questions, which will be addressed in the subsequent sections:

\begin{enumerate}[label=\textbf{RQ\arabic*}, 
left=0pt, labelsep=10pt, itemindent=0pt, itemsep=0pt, topsep=0pt, partopsep=0pt, parsep=0pt
]
    % \item \sout{How effective are the erasure methods in removing NSFW content, and what are their respective strengths and weaknesses?}
    \item Field Progress: How effective are current erasure methods in removing NSFW content? What advancements have been made in this area, and what are the overarching trends observed?
    % \item \sout{As mentioned earlier, NSFW encompasses more specific themes. Will the effectiveness of the methods vary across different themes? What about their performance on overall NSFW content?}
    \item Method Performance by Theme: Do the effectiveness and performance of a single method vary across different NSFW themes? How do these variations reflect on the overall NSFW content?
    \item Strengths and Weaknesses of Methods:  What are the differences between methods that rely on different data types in two modes? How does the performance vary when the method is trained with different amounts of data, resulting in different versions?
    \item Insights and Conclusions: What insights can we draw from these experiments? What are our reflective thoughts on future direction about this field?
\end{enumerate}

To answer \textbf{RQ1}, we propose six assessment perspectives, namely erasure proportion, excessive erasure, impact of explicit and implicit unsafe prompts, image quality, semantic alignment and robustness, which are elaborated in Section \ref{character}, with the assessment tools analyzed in detail in Section \ref{tool}. 
To address \textbf{RQ2}, we experiment with five specific NSFW themes as erasure targets and check the overall effect on NSFW. 
For \textbf{RQ3}, we test and compare versions of the erasure methods within two modes, which are trained with varying amounts of data. The details of the assessment objects are introduced in Section \ref{keywords}.
Based on the results, we provide conclusions in Section \ref{effect} and insights in Section \ref{discuss} to address the questions raised in \textbf{RQ4}. As shown in Figure \ref{fig:framework}, we organize and present our work in the form of a framework from three parts: Assessment Tools, Assessment Objects, and Assessment Content.


\begin{table*}[t!]
\small
\caption{Taxonomy of Concept Erasure Methods}
\label{table:methods}
\centering
\scalebox{0.9}{
\setlength{\tabcolsep}{4pt}
\begin{tabular}{c|c|l|c|c|c} 
\toprule
\multicolumn{1}{c|}{\textbf{Stage}}   & \multicolumn{2}{c|}{\textbf{Required Data Types}}                                                                            & \textbf{Core Principles}                           & \multicolumn{1}{l|}{\textbf{Parameters Involved}} & \textbf{Reference}               \\ 
\toprule
\begin{tabular}[c]{@{}c@{}}Dataset\\~Cleaning\end{tabular}                       & \multicolumn{2}{c|}{Corresponding NSFW labels for images}                                                                               & \multicolumn{1}{c|}{Remove NSFW data}                   & Full                                        & Stable Diffusion v2.0 \cite{Stable-Diffusion-2.0}                 \\ 
\midrule
\multirow{6}{*}{\begin{tabular}[c]{@{}c@{}}Parameter\\~Fine-Tuning\end{tabular}} & \multicolumn{2}{c|}{\multirow{3}{*}{Only target text concepts (Mode 1)}}                                                    & \multirow{2}{*}{Away from target concept} & Unet                                     & ESD\cite{gandikota2023erasing-esd}, SPM\cite{lyu2024one-spm}                \\ 
\cline{5-6}
                                                                                 & \multicolumn{2}{c|}{}                                                                                               &                                           & Encoder                                  & AU\cite{zhang2024defensive-au}                      \\ 
\cline{4-6}
                                                                                 & \multicolumn{2}{c|}{}                                                                                               & Close replacement concept                 & Unet                                     & UCE\cite{gandikota2024unified-uce}                     \\ 
\cline{2-6}
                                                                                 & \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Images\\(Mode 2)\end{tabular}} & Safe images opposite to target concepts & Close replacement concept                 & Unet                                     & AC\cite{kumari2023ablating-ca}, SelfD\cite{li2024self-selfd}               \\ 
\cline{3-6}
                                                                                 &                                                                           & Unsafe images related to target concept & \multicolumn{1}{c|}{Other}                & Unet                                     & FMN\cite{zhang2024forget-fmn}, MACE\cite{lu2024mace}               \\ 
\cline{3-6}
                                                                                 &                                                                           & Both safe and unsafe images             & Close replacement concept                 & Unet                                     & SalUn\cite{fan2023salun}                   \\ 
\midrule
\begin{tabular}[c]{@{}c@{}}Post-hoc \\Correction\end{tabular}                    & \multicolumn{2}{c|}{Only target text concepts~}                                                                     & Away from target concept                  & /                                        & \makecell{SLD\cite{patrick2023safe}, SD-NP\cite{ho2022classifier}, \\ Safety Checker\cite{sd1-4}}  \\
\bottomrule
\end{tabular}
}
\end{table*}

\subsection{Taxonomy of Concept Erasure Methods}
\label{taxonomy}
In this section, we provide a taxonomy of the existing concept erasure methods and their properties as shown in Table \ref{table:methods} and categorize them from four levels.
1) stage: we categorize based on the stage of intervention in the model: dataset cleaning before training, fine-tuning on a pre-trained model, and output correction through inference or classifier filtering. 
2) Modes: According to the data required for erasing the target concept. We classify the methods into two modes: Mode 1, which requires only target text concepts as data, and Mode 2, which requires images for training. 
3) Core principles: the core idea of each method.
4) Trainable parameters. In the Appendix \ref{sec:overview-cem}, we provide a detailed explanation of concept erasure methods according to the first level classification.


% The first level is "stage", where The second level shows the data required for erasing the target concept. We classify the methods into two modes: 
% Mode 1, which requires only target text concepts as data, and Mode 2, which requires images for training. The third level outlines the core principles of each method, while the fourth level delves into the parameters involved in model fine-tuning. In the Appendix \ref{sec:overview-cem}, we provide a detailed explanation of concept erasure methods according to the first level classification.

It is important to note that the dataset cleaning requires a large-scale dataset for each concept, making it excessively costly and impractical for widespread use. Therefore, our assessment does not include such methods.



\subsection{Assessment Perspectives}
\label{character}
To comprehensively and fairly evaluate concept erasure methods and answer \textbf{RQ1}, we considered six perspectives: erasure proportion, excessive erasure, impact of explicit and implicit unsafe prompts, image quality, semantic alignment and robustness.  The following subsections will detail the motivations for them and the evaluation tools employed.  

 \noindent \textbf{\uline{Erasure Proportion.}} 
The goal of concept erasure is to prevent the generation of images corresponding to target concepts. Therefore, the primary criterion for evaluating an erasure method is its effectiveness in this regard. The fewer the generated images related to the target concept, the more effective the erasure. Based on the performance comparison of various classifiers, which is detailed in Section \ref{Analysis-of-Classifier}, we selected the VQA \cite{Zhiqiu2024vqa} with the highest average accuracy as our five NSFW themes classifier. In order to more intuitively express the effectiveness of the erasure method here, we propose the erasure score indicator, which has the following formula:
\begin{align}
    \text{Erasure Score}=(N_{SD} - N)/N_{SD},
\end{align}
where $N_{SD}$ represents the number of images generated using the original Stable Diffusion v1.4 \cite{sd1-4} that are classified as theme $c$, and $N$ represents the number of images generated using the erasure method, targeting the erasure of concept (theme) $c$, that are still classified as theme $c$.  
 
 \noindent \textbf{\uline{Excessive Erasure.}} When erasing the target concept, it is crucial to avoid affecting unrelated concepts. This is particularly evident when erasing nudity, where we should achieve a higher degree of erasure for genitalia body parts compared to other ordinary body parts. We refer to this situation as excessive erasure.
% Excessive erasure can lead to a decrease in model usability, such as impacting image diversity and causing misinterpretations of subtle semantics. 
 % Therefore, evaluating the trade-offs of erasure methods in this regard is important. 
To assess whether the erasure method causes excessive erasure for the Sexually Explicit theme, we use NudeNet \cite{bedapudinudenet} to detect the exposure of various body parts and then compare the changes in the ratio of genital body parts (e.g.Buttock, Breast, Genitalia) to the total number of body parts before and after erasure, which is referred to genital ratio difference.
The formula is as follows:
\begin{align}
   \text{Genital Ratio Difference} =(N_{SD}^{g}/N_{SD}^{all})-(N^{g}/N^{all}),
\end{align}
where $N_{SD}^{g}$ and $N_{SD}^{all}$ denote the counts of detected genital body parts and total body parts in images generated by Stable Diffusion v1.4, respectively. Similarly, $N^{g}$ and $N^{all}$ represent these counts after erasing sexually explicit content.

\noindent \textbf{\uline{Impact of Explicit and Implicit Unsafe Prompts.}}
In practical scenarios, user input prompts may not explicitly include terms related to the target concept; however, implicit unsafe prompts, including obscure or ambiguous terms, can still result in unsafe image generation. Building on the findings regarding explicit and implicit unsafe prompts in Section \ref{Analysis-of-Datasets}, we evaluate the erasure scores of different methods for both explicit and implicit unsafe prompts.

 \noindent \textbf{\uline{Image Quality.}} 
In addition to reducing the generation of images containing the erasure concept, an effective erasure method must also preserve image quality. If the method degrades the model’s general performance, resulting in blurry or distorted images, it would be counterproductive. Two commonly used metrics to evaluate image quality are FID \cite{Martin2017fid} and LPIPS \cite{Richard2018LPIPs}. FID measures the Fréchet distance between the distributions of generated and real data, with a lower value indicating better image quality. Similarly, LPIPS assesses the perceptual difference between images by extracting features via a pre-trained network, where a lower value reflects higher similarity between the images.

\noindent \textbf{\uline{Semantic Alignment.}} A key reason for the popularity of diffusion models is their ability to generate images that accurately reflect the text prompt. Therefore, an effective erasure method must ensure that removing a specific concept does not disrupt the alignment of unrelated concepts. Unlike image quality, which focuses on the visual appeal of the generated image, image-text alignment emphasizes the image's ability to faithfully represent the text prompt.
To assess semantic alignment, we use CLIPScore \cite{Alec2021clip} and ImageReward \cite{Jiazheng2023ImageReward}. CLIPScore measures the similarity between image and text embeddings, encoded using CLIP, to evaluate alignment. ImageReward, on the other hand, uses a reward model trained on a human-labeled image-text preference dataset to provide human preference scores.

\noindent \textbf{\uline{Robustness.}}  
% In real-world applications, some malicious attackers may construct toxic prompts to bypass the model's safety mechanisms and generate unsafe content. Recent studies \textcolor{cd}{more cite} suggest using red teaming tools to identify potential security vulnerabilities in models. In text-to-image models, common red teaming tools involve adversarial attack methods to create malicious prompts, testing whether the model generates harmful images. We use RAB\cite{yu2024ring} to construct toxic prompt sets to evaluate the robustness of the erasure methods, because RAB is a black-box attack method that does not involve a specific model. Furthermore, it utilizes a pre-trained text encoder to generate adversarial prompts using relative text semantics and genetic algorithms, ensuring fairness in the evaluation. Specifically, we use the adversarial prompt sets for the Sexually explicit and Violent themes provided by RAB, which contain 150 and 248 prompts, respectively.  
In real-world applications, malicious attackers may craft toxic prompts to circumvent safety mechanisms and generate unsafe content. Recent studies \cite{yimeng2024unlearndiffatk,zhiyi2024p4d} suggest using red teaming tools to uncover potential security vulnerabilities in models. In text-to-image models, common red teaming tools involve adversarial attacks to create harmful prompts and test whether the model generates unsafe images. To evaluate the robustness of erasure methods, we use RAB \cite{yu2024ring}, a black-box attack method that does not rely on a specific model. RAB generates adversarial prompts using relative text semantics and genetic algorithms, ensuring fairness in evaluation. We specifically use the adversarial prompt sets for the Sexually Explicit and Violent themes provided by RAB, containing 150 and 248 prompts, respectively.



 

\subsection{Assessment Concepts}
\label{keywords}
As outlined in Section \ref{taxonomy}, concept erasure methods can be broadly categorized into Mode 1, which only requires text data, and Mode 2, which necessitates image data. Therefore, we propose \textbf{RQ3} to compare the differences between these two modes and evaluate the effects of training with different amounts of data. For methods related to Mode 1, we divide them into two versions: the \textbf{more keywords} version, which provides a comprehensive set of keywords to fully cover the corresponding theme as the erasure target, and the \textbf{fewer keywords} version, which aims to achieve erasure performance with fewer keywords based on the generalization relationships between them. For methods related to Mode 2, we create three versions: one requiring 20 images for training, one with 200 images, and one with 1000 images. These data quantities are determined based on the amounts specified in the original works of the erasure methods.

To gather the text keywords required for Mode 1, we collect a total of 84 keywords for five unsafe themes from 9 sources, listed in Table \ref{table:keywords}. Specifically, we select 13 sexually explicit keywords from the word list in \cite{patrick2023safe,Javier2022redteaming,Hive}, 25 keywords reflecting violence from \cite{patrick2023safe,Hive,llama-guard,meta}, 15 disturbing-related keywords from \cite{qu2023unsafe,llama-guard,Lexica}, 23 keywords conveying hate-related content from \cite{patrick2023safe,hine2017kek,llama-guard,Hive}, and 8 political-related keywords from \cite{Lexica,qu2023evolution-meme,llama-guard}.


% The definition given in \cite{patrick2023safe} for erasing the complete NSFW concept consists of 20 words, namely \textit{hate, harassment, violence, suffering, humiliation, harm, suicide, sexual, nudity, bodily fluids, blood, obscene gestures, illegal activity, drug use, theft, vandalism, weapons, child abuse, brutality, and cruelty}. However, such a definition cannot meet the needs of erasing specific NSFW subtopics, so we compile each subtopic using a set of keywords that we collected ourselves.
% We collect a total of 84 keywords for five unsafe topics from 9 sources, listed in Table X in  Appendix. 

We select the appropriate keywords by letting GPT 4o write unsafe prompts with these keywords, and then feeding these prompts to Stable Diffusion v1.4 to generate images. After analyzing these images, we have the following findings. 
First, images generated under different themes may exhibit similar semantic content. For example, the keyword \textit{blood} belongs to the Violent theme, but when it appears in a horror scene outside of a fight, it is easily associated with the Disturbing theme. 
Second, the performance of text unsafe and image unsafe is asymmetric. Some keywords can only reflect unsafe content at the text level, but not at the image level. For example, a textual prompt referring to discrimination might result in an image that only shows the appearance of a person but does not convey a specific emotion or demonstrate discriminatory behavior. 
Third, the generalization and expression capabilities of keywords are different. Some images corresponding to one keyword contain many elements related to other keywords, while others present almost identical content. Additionally, some words may require combination with other words or contextual sentences to express unsafe meanings, such as \textit{dark} or \textit{suicide}. The image associated with \textit{dark} may not necessarily be scary, while images generated from just the word \textit{suicide} tend to convey sadness.

Based on the observations above, we believe it is necessary to select keywords from the collected set that are directly related to the theme, possess expressive capabilities for unsafe elements, and demonstrate a certain degree of generalization. Accordingly, we compile the more keywords version as the first version of Mode 1.

Furthermore, we attempt to identify the smallest set of keywords that could cover the corresponding theme. Using the initial erasure method SLD \cite{patrick2023safe}, we examine whether other keywords are reflected when one keyword is selected as the erasure target. We discover the following generalization relationships and show the keyword sets for two versions of each theme in Figure \ref{fig:define} (">" indicates inclusion relationship, "," indicates parallel relationship): 
\begin{itemize}
% [label=\textbf{RQ\arabic*}, 
[left=0pt, labelsep=10pt, itemindent=0pt, itemsep=0pt, topsep=0pt, partopsep=0pt, parsep=0pt
]
% [itemsep=0pt, partopsep=0pt]
    \item Sexually Explicit: \textit{nudity} > \textit{sex} > \textit{genitalia} > \textit{seductive};
    \item Violent: \textit{violence} > \textit{fight} >\textit{weapons}, \textit{blood} > \textit{corpse};
    \item Disturbing: \textit{horror} > \textit{disturbing} $\approx$ \textit{distorted face} $\approx$ \textit{exposed bone}, \textit{human flesh}; 
    \item Hateful: \textit{nazi}, \textit{terrorism};
    \item Political: \textit{Trump}, \textit{Hillary}, \textit{Obama}, \textit{Biden}.
\end{itemize}

As for Mode 2, we provide three versions of the related methods, which leverage 20, 200, and 1000 images for training, respectively. The training image set for different version consists of a uniform number of images corresponding to the unsafe prompts for each keyword.
