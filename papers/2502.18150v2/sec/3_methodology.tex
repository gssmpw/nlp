\section{Realistic 3D shapes of clothed humans and objects}
\label{sec:method}
\begin{figure*}
  \centering
\includegraphics[width=\linewidth]{images/arch_overview.pdf}
\vspace{-8mm}
\caption{\name overview: Given an input image of a human-object scene, we first use a generative model to inpaint occluded human body regions, guided by a mask of missing areas and the segmented input of the human. Next, the generated image, along with an estimated normal map, the input image, the segmented object image, and estimated pose parameters, are processed by an attention-based neural implicit model. This model jointly estimates the implicit representation of the human-object shape.}
\label{fig:overview}
\vspace{-5mm}
\end{figure*}
We introduce \name, a novel framework for Realistic Clothed Humans and Objects Reconstructions from a single RGB image depicting both the human and the object. In the proposed pipeline, shown in~\cref{fig:overview}, the image is first segmented to separate the human from the object, and pose parameters of the SMPL-H~\cite{smplh} model and object are estimated. Regions of the human body that  are occluded by the object in the input image are inpainted using the generative power of an image-conditioned diffusion model. Implicit representations of both the human $s_h$ and the object $s_o$ are then estimated using a novel attention-based neural implicit model that incorporates the input RGB image $I_f$, the generated full-body human image $I_h$, the object image $I_o$ and the estimated pose parameters. Compared to related works, our approach jointly reconstructs realistic clothed humans and objects while avoiding depth ambiguity between them.
%
\subsection{Inpainting of occluded human body regions}
\label{ssec:diff_model}
This work addresses cases in images where an object occludes the human. When such occlusion happens, regions of the human body are missing in the input image, leaving the neural implicit model without the information needed to estimate these regions. We propose to leverage the generative capability of diffusion models to inpaint the occluded body regions, using the partial human image $I_p$ along with a mask of the occluded regions $M_i$. Given the input image $I_f$, we first apply semantic segmentation to separate the human from the object, while estimating the SMPL-H~\cite{smplh} and object pose. Using the SMPL-H model, we then generate a mask of the human $M_s$, and we obtain the human-object intersection mask $M_i$ as $M_i=M_s-M_p$, where $M_p$ is the segmented partial human mask.
%
\\Inspired by SiTH~\cite{ho2024sith}, we leverage an image-condition latent diffusion model~\cite{rombach2022high} (LDM) to learn the conditional distribution of the missing human body regions. Rather than training the LDM from scratch, we adopt a fine-tuning strategy~\cite{kumari2023multi, zhang2023adding} that optimizes the cross-attention layers of a pretrained diffusion U-Net~\cite{rombach2022high}. The learning is conditioned using a ControlNet~\cite{zhang2023adding}, which processes $M_i$ to guide the generation. Additionally, we condition the U-Net with features from the partial human image $I_p$, extracted via a pre-trained CLIP~\cite{clip} image encoder and a VAE encoder $\varepsilon$,  to ensure that the generated image $I_h$ matches the appearance of $I_p$. These features, along with randomly sampled noise $\epsilon$, are input to the LDM model $\epsilon_{\theta}$, conditioned by the ControlNet, which generates a latent code $z$ within the VAE latent distribution $z = \varepsilon(I^{gt}_h)$, where $I^{gt}_h$ is the ground-truth full-body human image. The output full-body image $I_h$ is then generated by decoding with the VAE decoder  $\mathcal{D}$  a latent code $\tilde{z}$ derived through iterative denoising of Gaussian noise: $I_h = \mathcal{D}(\tilde{z})$.
\\We define the objective function for fine-tuning as:
\vspace{-0.2cm}
\begin{equation}
    \min_{\theta}\mathbb{E}_{z \sim \varepsilon(I^{gt}_h),t, \epsilon \sim \mathcal{N}(\textbf{0},\textbf{I}) }\left\lVert \epsilon - \epsilon_{\theta}(z_t, t, I_p, M_i) \right\rVert^2_2
\vspace{-0.2cm}
\end{equation}
The ground-truth latent code $z_0 = \varepsilon(I^{gt}_h)$ is diffused over 
$t$ time steps, resulting in the noisy latent $z_t$. The image-conditioned LDM model $\epsilon_{\theta}$ predicts the noise $\epsilon$ added to the noisy latent $z_t$, based on time step $t \sim [0, 1000]$ and conditional inputs $I_p$ and $M_i$.
\\To inpaint the occluded body regions during inference, we generate a latent $\tilde{z}_0$ by starting with Gaussian noise $z_T \sim \mathcal{N}(\textbf{0},\textbf{I})$ and using an iterative denoising process. The final full-body image $I_h$ is  obtained with the decoder $\mathcal{D}$:
\vspace{-0.2cm}
\begin{equation}
    I_h=\mathcal{D}(\tilde{z}_0)=\mathcal{D}(f_{\theta}(z_T,I_h,M_i))
\vspace{-0.2cm}
\end{equation}
where $f_{\theta}$ indicates the iterative denoising process of $\epsilon_{\theta}$.
Finally, a 2D normal map $S_N$ is estimated from $I_h$ and concatenated with it. For simplicity, we continue to refer to this concatenated result as $I_h$ throughout the paper.
%
\begin{figure}
  \centering
\includegraphics[width=\linewidth]{images/arch2.pdf}
\vspace{-8mm}
\caption{The attention-based neural implicit model first extracts pixel-aligned features from the input images to capture local details. It then uses a transformer encoder to merge these features, learning global and local contextual information about the scene. Finally, the model estimates the implicit representations for both humans and objects. Human-object pose priors provide 3D spatial information to address depth ambiguity.}
\label{fig:fig_module}
\vspace{-6mm}
\end{figure}
\subsection{Attention-based neural implicit model}
\label{ssec:neural}
To reconstruct the 3D shapes of both humans and objects, we introduce a novel attention-based neural implicit model that jointly estimates their implicit representations. This model aims to estimate an implicit representation that defines a surface as the level set of a function $f$, \ie $f(X) = 0$ where $X$ is a set of 3D points in $\mathbb{R}^3$.  The reconstructed surface is then defined as the zero level-set of $f$:
\vspace{-0.2cm}
\begin{equation}
    f' = \{ x:\; f(x) = 0,\; x\in\mathbb{R}^3\}
\vspace{-0.2cm}
\end{equation}
The proposed implicit model comprises  of multiple modules, as shown in 
\cref{fig:fig_module}.
%
\\\textbf{Pixel-aligned feature extractors}: Previous methods on 3D human reconstruction~\cite{pifu,pifuhd,surs} demonstrate that projecting a 3D point $x \in \mathbb{R}^3$ in the embedded image feature space $\phi(I)$ extracted with a convolutional stacked hourglass network significantly increases the quality of 3D human shapes. \name first extracts a feature embedding for each input image  $\phi_{\{h, o, f\}}(I_{\{h, o, f\}})$. A first set of points $X_h$ is then projected onto $I_f$ and $I_h$ via perspective projection $\pi$ and linked to the corresponding features $\phi_h$ and $\phi_f$ to obtain pixel-alignment with the human: $\Phi_{\{h,f\}}^h=\phi_{\{h,f\}}(\pi(X_h,I_{\{h,f\}}))$. Similarly, the pixel-aligned object features $\Phi_{\{o,f\}}^o=\phi_{\{o,f\}}(\pi(X_o,I_{\{o,f\}}))$ are obtained by projecting a different set of points $X_o$ on $I_o$ and $I_f$ and indexing them with the  corresponding features $\phi_o$ and $\phi_f$.
%
\\\textbf{Human-object pose prior features:} Multiple 3D spatial configurations of humans and objects can project to the same 2D image, leading to difficulties in estimating their position in 3D space and depth-scale ambiguity. Since parametric model-based human-object reconstruction methods optimize 3D location, depth, and scale using geometric and spatial constraints, we leverage them to address depth-scale ambiguity and anchor the 3D spatial location. We condition our model on semantic features of the SMPL-H and object template estimated by the parametric model-based method, and we compute the neural representations of both human and object relative to the predicted SMPL-H center.  The object position is defined relative to this center, to learn its 3D spatial relationship with the human. To define the human pose features, for a query point $x_h \in X_h$, we look for the closest point $x_h^*$ on SMPL-H, \ie $x_h^* = \arg \min_{x_h^p} || x_h - x_h^p||$ where $x_h^p$ are points on the SMPL-H mesh. Human pose prior features $\sigma_h(x_h) = [d_h, v_h, z_h]$ comprise three elements: a signed distance value $d_h$ between $x_h$ and $x_h^*$, a visibility label $v_h \in \{1, 0\}$ where $v_h$ indicates if $x_h^*$ is visible in the image when SMPL-H and the object mesh is projected together, and a relative depth-aware feature $z_h = (x_h^z - z_c)$ where $z_c$ is the depth of the SMPL-H center and $x_h^z$ is the depth of the query point $x_h$. Analogously, for a query point $x_o \in X_o$ and its closest point on the object mesh, $x_o^*$, the object pose prior features $\sigma_o(x_o) = [d_o, v_o, z_o]$ are three elements: a signed distance value $d_o$ between $x_o$ and $x_o^*$, a visibility label $v_o \in \{1, 0\}$ and a relative depth-aware feature $z_o = (x_o^z - z_c)$ where $x_o^z$ is the depth of the query 
point $x_o$. 
%
\\\textbf{Global-local context learning via transformer encoders}: As shown in~\cref{ssec:abl}, simply concatenating the features of the human $\Phi_h^h$ and object $\Phi_o^o$ with those of the input image $\Phi_f^{\{h,o\}}$ deteriorates network performance, making it less robust to noise of the input image. Additionally, the network's ability to integrate and reason about features across the entire image is limited by the receptive field of convolutional layers.  To better understand the relationship between the human and the object in the space, we propose using two attention-based encoders, $A_o$ and $A_h$ that merge the feature extracted by the feature extractor module. An attention score is computed for the two paired images by assessing the compatibility between a query and its corresponding key, producing two feature embeddings:  one for the human $\varphi_h=A_h(\Phi_h^h,\Phi_f^h,\sigma_h(x_h))$ and another for the object $\varphi_o=A_o(\Phi_o^o,\Phi_f^o,\sigma_o(x_o))$. The pose features $\sigma_{\{h,o\}}$ obtained in the previous module are also concatenated to integrate spatial information. Each feature $\varphi_{\{h,o\}}$ integrates the information from its corresponding local image $I_{\{h,o\}}$, combined with the global context of the input image $I_f$. A more comprehensive understanding of the scene is obtained by contextualizing the global and local information. As a result, the human feature $\varphi_{h}$ contains detailed information from the human image, as well as contextual information of the global scene from the input image. Similarly, the object feature $\varphi_{o}$ integrates scene-level information from $I_f$.
%
\\\textbf{Implicit function estimation}: The implicit functions of the human $f_h$ and object $f_o$ are modeled with two separate multi-layer perceptrons (MLPs), which jointly estimate the occupancy values of  $X_h$ and $X_o$:
\vspace{-0.2cm}
\begin{equation}
    \begin{aligned}
\hat{s} = \begin{cases} \hat{s}_h=f_h(\varphi_h, X_h), \quad \hat{s}_h \in \mathbb{R},  \\ \hat{s}_o=f_o(\varphi_o, X_o), \quad \hat{s}_o \in \mathbb{R} \end{cases}
\end{aligned}
\vspace{-0.2cm}
\end{equation}
where $\hat{s}$ is the implicit representation of the final human-object shape, with $\hat{s}_h$ representing the human component and $\hat{s}_o$ representing the object component.
\\The neural implicit model is trained end-to-end with the following objective function $\mathcal{L}=\mathcal{L}_{h}+\mathcal{L}_{o}$:
\vspace{-0.2cm}
\begin{equation}
    \mathcal{L}_{h}=\frac{1}{N_h}\sum_{j=1}^{N_h}\left|f_{h}(\varphi^j_h, X^j_h)-f_{h}^{gt}(\varphi^j_h X^j_h)\right|^2
\vspace{-0.2cm}
\end{equation}
where $N_h$ is the number of points $X_h$ and $f_{h}^{gt}$ is the ground-truth implicit function for the human shape, and 
\vspace{-0.2cm}
\begin{equation}
    \mathcal{L}_{o}=\frac{1}{N_o}\sum_{j=1}^{N_o}\left|f_{o}(\varphi^j_o, X^j_o)-f_{o}^{gt}(\varphi^j_o, X^j_o)\right|^2
\vspace{-0.2cm}
\end{equation}
where $N_o$ is the number of points $X_o$ and $f_{o}^{gt}$ is the ground-truth implicit function for the object shape. 
%
\\\textbf{Inference}: During inference, an input RGB image $I_f$, containing both a human and an object, is first processed with semantic segmentation to generate separate masks for human $M_p$ and object $M_o$. Human and object poses are also estimated from $I_f$. If the object partially occludes the human, the segmented human image $I_p$ and the intersection mask $M_i$, are processed by the diffusion module. This results in a full-body image $I_h$ from which the 2D normal map $S_N$ is estimated. These outputs, along with the input image $I_f$, the object image $I_o$ and the pose features $\sigma_{\{h,o\}}$ are then processed by the attention-based neural implicit model. This model estimates the occupancy $\hat{s}$ of a set of random 3D points $X$ of the 3D space. Finally, the 3D shape is obtained by extracting iso-surface $f = 0.5$ of the probability field $\hat{s}$ at threshold 0.5 with the Marching Cubes algorithm \cite{lorensen1987marching}.

