\clearpage
\setcounter{page}{1}
\maketitlesupplementary
\section{Overview}
In the following sections of the supplementary material we present:
\begin{itemize}
    \item Additional details about the implementation of \name (\cref{sec:impl});
    \item A table containing a list of symbol used in the main paper is presented in~\cref{sec:notations}
    \item Additional ablation studies to demonstrate the effectiveness of the proposed framework (\cref{sec:abl_suppl});
    \item Additional results of \name and related works on both \dataname and BEHAVE~\cite{bhatnagar2022behave} datasets (\cref{sec:comp_supp});
    \item A more detailed discussion about results, limitations and future works (\cref{sec:discussion});
\end{itemize}
\section{Implementation details}
\label{sec:impl}
\subsection{Diffusion model}
Inspired by SitH~\cite{ho2024sith}, our image-conditioned latent diffusion model~\cite{rombach2022high} used to generate missing body regions as described in \cref{ssec:diff_model}, is trained by fine-tuning the diffusion U-Net’s~\cite{rombach2022high} weights. These weights are initialized using the Zero-1-to-3~\cite{lugaresi2019mediapipe} model, combined with a trainable ControlNet~\cite{zhang2023adding} model, following the default network setups with input channel adjustment. The ControlNet input is a 1 channel mask and the diffusion U-Net input is a 3-channel RGB image of size 512 x 512. We train the models with a batch size of 6 images on a single A100 NVIDIA GPU, with a learning rate of $4\times10^-6$ and adopting a constant warmup scheduling. The ControlNet model’s conditioning scale is fixed at 1.0. We use classifier-free guidance in our training, involving a dropout rate of 0.05 for the image-conditioning. During inference, a classifier-free guidance scale of 2.5 is applied to generate the output images.
\begin{table}[t!]
\centering
\resizebox{\linewidth}{!}
{\input{tables/notation}}
\vspace{-3mm}
\caption{List of notations used in the main paper.}
\label{tab:notation}
\end{table}
\subsection{Attention-based neural implicit model}
We detail the implementation of our attention-based neural implicit model described in \cref{ssec:neural}. For each input image of size 2048x1536, we take a crop around the human-object bounding box center and resize it to 512px for the network. Following PIFu~\cite{pifu}, we use a four-stack Hourglass network that yields a 256-dimensional feature map for querying pixel-aligned features. A standard multi-head transformer-style architecture is applied for the two attention-based encoders  $A_h$ and $A_o$, as shown in~\cref{fig:fig_module}. Given three vectors query $Q=M_q\phi$, key $K=M_k\phi$ and value $V=M_v\phi$ as the embedding of the original feature $\Phi$ and parameterized by matrices $M_q$, $M_k$ and $M_v$, an attention score is computed for each input feature $\Phi^h_{\{h,f\}}$ for $A_h$ and $\Phi^o_{\{o,f\}}$ for $A_o$ based on the compatibility of a query with a corresponding key:
\begin{equation}
    Attention(Q,K,V)=softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{equation}
where $d_k$ is the common dimension of $K$, $Q$ and $V$. Multiple heads are used to compute features for the human and object:
%srry it was easier ahah. thanks :)
%this is the other, you can modify it eventually if needed
\begin{equation}
\begin{gathered}
    MultiHead(Q, K, V)=concat(H_1, ..., H_h)W^o
    \\
    H_i=Attention(QW^q_i, KW^k_i,V W^v_i)
\end{gathered}
\end{equation}
where $QW^q_i$, $KW^k_i$,$V W^v_i$  are the parameters of $Q$, $K$ and $V$, and $W^o$o the parameters of the final projection. The final feature is computed as the mean of the processed features with each merged feature $\varphi_{\{h,o\}}$ containing local information from the human or object image along with global scene information from the input image. Finally, to estimate the neural implicit representations, each decoder $f_h$, $f_o$ is an MLP with the number of neurons (259, 1024, 512, 256, 128, 1) and skip connections at 2nd, 3rd and 4th layers. We train the network end-to-end using Adam optimizer with a learning rate of $1e-4$ and batch size 4. 
\section{Notations}
\label{sec:notations}
To facilitate the understanding of the main paper,~\cref{tab:notation} presents a list of the notations used in the paper.
\section{Additional ablation studies}
\label{sec:abl_suppl}
\subsection{Effect of different inputs}
\cref{fig:abl2} of the main paper shows examples of human-object shapes reconstructed using different combinations of input. Additional results from the configurations analyzed in the second ablation study of~\cref{ssec:abl} and not included in~\cref{fig:abl2} are shown in~\cref{fig:abl2_supp}. 
%
The highest quality human-object shapes are obtained using \name, confirming what was proved in~\cref{ssec:abl}. Smoother shapes are reconstructed when normal maps are excluded (No $S_N$) while omitting pose features (No $\sigma_{{h,o}}$) introduces depth ambiguity. Excluding the full-body human input (No $I_h$) or the object input (No $I_o$) prevents the network from reconstructing the human or object, respectively.
%
\begin{figure*}[b]
  \centering
\includegraphics[width=\linewidth]{images/ablationn2_supp.pdf}
\vspace{-6mm}
\caption{Qualitative results showing the effect of using different configurations of the attention-based neural implicit model with methods that have not been shown in~\cref{fig:abl2} of the main paper. Front and side views are shown from \dataname in the top row, from BEHAVE~\cite{bhatnagar2022behave} in the bottom.}
\label{fig:abl2_supp}
\end{figure*}
\subsection{Training configurations}
\begin{table}[h]
\centering
\resizebox{\linewidth}{!}
{\input{tables/train_conf}}
\vspace{-3mm}
\caption{Quantitative results obtained with different training configurations of \name.}
\label{tab:train_config}
\end{table}
\begin{figure*}[t]
  \centering
\includegraphics[width=\linewidth]{images/training_config.pdf}
\caption{Examples of results obtained with different training configurations of \name. Front and side views are shown from \dataname in the top row, from BEHAVE~\cite{bhatnagar2022behave} in the bottom.}
\label{fig:train_config}
\end{figure*}
\noindent The proposed attention-based neural implicit model is trained end-to-end to enable better joint reasoning about the human-object scene. This section presents results obtained by modifying \name's training configuration:
\begin{itemize}
    \item $\mathbf{Sep_{no}}$: Two separate networks are trained. One reconstructs the human shape using the full-body human image generated by the inpainting diffusion model and the relative human pose feature. The other reconstructs the object shape using the object image and pose feature. No transformer encoder is used.
    \item $\mathbf{Sep}$: Similar to $\mathbf{Sep_{no}}$, two networks are trained separately to estimate the human and object implicit representations. However, in this configuration, the input RGB image is also considered. Each network extracts features from the input RGB image and merges them with either the full-body human or object features using transformer encoders.
\end{itemize}
As demonstrated by the quantitative evaluation in~\cref{tab:train_config} and the qualitative results (\cref{fig:train_config}), training the attention-based neural implicit model end-to-end significantly improves performance. Both $\mathbf{Sep}$ and $\mathbf{Sep_{no}}$ training configurations result in less accurate human-object reconstructions. Training separate networks for humans and objects leads to reconstruction errors in interaction regions. This demonstrates how the proposed design can embed contextual information about global and local scenes, learning spatial relationship between human and object.
\begin{figure*}[t]
  \centering
\includegraphics[width=0.85\linewidth]{images/no_inpainting.pdf}
\vspace{-4mm}
\caption{Effect of not applying the inpainting module of \name before estimating the implicit representation. The same examples used in the paper are illustrated with (w.) and without (w.o) the object.}
\label{fig:no_inp}
\end{figure*}
\subsection{Inpainting module}
\begin{table}[h]
\centering
\resizebox{\linewidth}{!}
{\input{tables/inp}}
\vspace{-3mm}
\caption{Quantitative evaluation of changing the diffusion module of \name. The human-object shapes reconstructed with the attention-based neural implicit module of \name are used for evaluation.}
\label{tab:inp}
\end{table}
\begin{figure*}[t]
  \centering
\includegraphics[width=\linewidth]{images/diff_img.pdf}
\vspace{-6mm}
\caption{Visual comparisons of inpainted images generated using different inpainting approaches.}
\label{fig:rgb_inp}
\end{figure*}
\noindent In scenes containing both humans and objects, the object often occlude parts of the human body. When only partial regions of the body are visible, the network fails to reconstruct the occluded regions. 
This limitation is shown in~\cref{fig:no_inp}, where \name is applied without the inpainting module to generate a full-body human image. Instead, the partial human image is directly input into the attention-based neural implicit model, which fails to reconstruct these occluded regions, producing holes in the human shapes. Inpainting the missing body regions in the input image is crucial to reconstruct realistic clothed human and object shapes.
\\\name addresses the challenge of human occlusion by leveraging the generative capability of diffusion models to inpaint the occluded body regions. A fine-tuning strategy is adopted to optimize the cross-attention layers of a pre-trained diffusion model, conditioning it on the mask of the body regions requiring inpainting. In this section, we explore different solutions for inpainting, including MAT~\cite{li2022mat}, a transformer-based model for large hole inpainting, conditioning the diffusion model with the SMPL~\cite{smplh} normal map (Normal), and conditioning with both the mask and the normal map (Both). 
The effect of using these approaches for the inpainting module of \name is quantitatively evaluated by estimating the implicit representation of human-object shapes using their generated full-body human image (\cref{tab:inp}). Examples of human-object reconstructions using these methods for inpainting are shown in~\cref{fig:mesh_inp} while~\cref{fig:rgb_inp} illustrates the input images shown in the paper and generated using the discussed inpainting methods.
\\As expected, the worst results occur when no inpainting is applied (No inpainting). MAT struggles to produce realistic body regions in the RGB images, propagating noise into the final reconstruction. Conditioning the diffusion model with both the SMPL normal map and the mask also degrades performance. The best results are achieved when either the SMPL normal or the mask of the occluded regions is processed using ControlNet. In particular, processing only the mask proves more robust, as seen in the top-left example of~\cref{fig:rgb_inp} and in the reconstructed human-object shape in the top row of~\cref{fig:mesh_inp}, where unnatural black regions in the full-body human image generated with Normal configuration caused gaps in the reconstructed shape.
\\Note that we do not quantitatively evaluate the generated images against ground truth, since the main goal of \name is 3D reconstruction rather than inpainting.
\begin{figure*}[h]
  \centering
\includegraphics[width=\linewidth]{images/inpaint_comp.pdf}
\vspace{-6mm}
\caption{Human-object shapes reconstructed by changing the diffusion module of \name. Front view with and without the object and side views without the object are shown from \dataname in the top row, from BEHAVE~\cite{bhatnagar2022behave} in the bottom.}
\label{fig:mesh_inp}
\end{figure*}
\section{Additional visual comparisons}
\label{sec:comp_supp}
In this section, we present additional visual results of human-object  shapes reconstructed using \name, as well as comparisons with related methods that aim to reconstruct high-quality 3D humans (PIFuHD~\cite{pifuhd}, ECON~\cite{econ}, SiTH~\cite{ho2024sith}) and the baseline models introduced in~\cref{ssec:comp} of the main paper. 
Specifically,~\cref{fig:comp1_supp} presents results of the methods not included in~\cref{fig:comp1} of the main paper on \dataname dataset, while~\cref{fig:comp2_supp} illustrates reconstructed human-object shapes on the BEHAVE~\cite{bhatnagar2022behave} dataset that were omitted from~\cref{fig:comp2}.~\cref{fig:thuman_supp} and~\cref{fig:behave_supp} show new examples of reconstructed shapes using the considered methods on \dataname and BEHAVE dataset, respectively. 
Across all the presented figures, our approach reconstructs the most realistic human-object shapes with the fewest artefacts compared to related works. Human-focused reconstruction approaches cannot reconstruct objects, failing to achieve our goal of joint human-object reconstruction.
Even retraining PIFuHD on \dataname ($\mathrm{PIFuHD}_{\mathrm{ho}}$), incorporating  pose features ($\sigma\mathrm{PIFuHD}_{\mathrm{ho}}$,) or using two MLPs ($2\mathrm{PIFuHD}_{\mathrm{ho}}$), does not sufficiently improve results, with severe depth ambiguities in the reconstructed objects. 
The proposed baselines that leverage pose features along with two MLPs ($2\sigma\mathrm{PIFuHD}_{\mathrm{ho}}$, $2\sigma\mathrm{PIFuHD}^{all}_{\mathrm{ho}}$, $2\sigma\mathrm{PIFuHD}^{sep}_{\mathrm{ho}}$) can reconstruct objects but are significantly more prone to noise and artifacts compared to \name. The attention-based neural implicit model introduced by \name ensures the joint reconstruction of realistic clothed human and object shapes from single images, outperforming all related approaches.
\begin{figure*}[b!]
  \centering
\includegraphics[width=0.99\linewidth]{images/comp1_supp.pdf}
\vspace{-4mm}
\caption{Visual comparisons from \dataname with related works not shown in~\cref{fig:comp1} of the main paper. Front and side views are shown.}
\label{fig:comp1_supp}
\end{figure*}
\section{Discussion}
\label{sec:discussion}
Existing works~\cite{econ, ho2024sith, pifuhd} for realistic human reconstruction are not designed to reconstruct objects. Consequently, when these methods are applied to real images containing both humans and objects, they behave differently in accordance with their network design, as illustrated in~\cref{fig:comp2_supp}. For instance, if these methods are conditioned on the SMPL model~\cite{ho2024sith,econ}, the object is not reconstructed unless it is very  close to the human. In such cases, these methods misinterpret the object as part of the clothing, merging it with the human mesh and resulting in a severe depth ambiguity. Similarly, methods that integrate normals~\cite{pifuhd} to reconstruct human shapes struggle to distinguish between clothing and objects, treating them as a single entity.
\\One potential solution is to crop the object out of the image and reconstruct only the human with the above methods. However, this requires reconstructing the object separately, without considering its spatial relationship with the human. In contrast, our approach is explicitly designed to jointly reason about humans and objects, distinguishing them as two distinct yet connected entities and reconstructing both in a shared 3D space.
\\We also highlight the novelty of our feature extraction and fusion strategy, where pixel-aligned features from each input are merged via transformer encoders. This design allows the model to jointly learn both global and local contextual information about the scene. Capturing global information improves the understanding of human-object spatial relationships, while local information  allows the reconstruction of realistic shapes.
\\Despite its strengths, our approach is prone to certain limitations.  First, the reconstruction quality of smaller body parts, such as fingers and hair, can be improved. This refinement will improve the modeling of human-object interactions and will be addressed in future work. In addition, increasing the quality of the ground-truth object shapes of our dataset will allow the network to learn finer details for the reconstructed object shapes, further increasing the realism of the reconstruction.
\\
Although \name can reconstruct arbitrary object shapes, it currently relies on object pose priors from a method~\cite{xie2022chore} that requires known object templates, limiting its generalization to objects without predefined templates. This limitation can be addressed in the future by integrating priors from recent template-free methods~\cite{xie2023template_free}. 
\\Our method relies on state-of-the-art works for human-object pose and normal map estimation. As a result, if these estimations are noisy, the noise propagates through the pipeline, causing artifacts in the reconstructed human shape.
%~\cref{fig:limitation} shows an example where misaligned SMPL-H estimation produces holes in the reconstruction, while proper alignment resolves the issue. 
Note that all the quantitative results on \dataname use ground-truth SMPL-H models and occlusion masks, while 2D normal maps are generated with pix2pixHD~\cite{wang2018high}  from the inpainted images.
\\Finally, a key direction for future work involves estimating the appearance of clothed human-object reconstructions to further enhance realism.
\begin{figure*}[t]
  \centering
\includegraphics[width=\linewidth]{images/comp2_suppv2.pdf}
\vspace{-6mm}
\caption{Visual comparisons from BEHAVE~\cite{bhatnagar2022behave} dataset with approaches that aim to reconstruct 3D humans as well as with baselines designed for fair comparisons. The same examples shown in~\cref{fig:comp2} are illustrated. Front and side views are shown.}
\label{fig:comp2_supp}
\end{figure*}
\begin{figure*}[t]
  \centering
\includegraphics[width=\linewidth]{images/thuman_supp.pdf}
\vspace{-6mm}
\caption{Additional visual results from \dataname. Examples obtained with methods that aim to reconstruct 3D humans as well as with the baselines considered in the main paper are shown. Front and side views are shown.}
\label{fig:thuman_supp}
\end{figure*}

\begin{figure*}[t]
  \centering
\includegraphics[width=\linewidth]{images/behave_supp.pdf}
\vspace{-6mm}
\caption{Additional visual results from BEHAVE~\cite{bhatnagar2022behave}. Examples obtained with methods that aim to reconstruct 3D humans as well as with the baselines considered in the main paper are shown. Front and side views are shown.}
\label{fig:behave_supp}
\end{figure*}
