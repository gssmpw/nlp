\begin{abstract}

% Recent works to jointly reconstruct 3D human and object from a single RGB image, are mostly model-based, that fail to capture the fine details of the clothed human body and object surface. In this paper, we introduce ReCHOR, a novel, model-free, first-method to produce realistic clothed human-object reconstructions from a monocular view. This is extremely challenging due to human-object occlusions, diverse interactions and depth ambiguity, as it needs to infer both 3D spatial awareness and high resolution details. Our core idea is based on estimating neural implicit representations for human and object respectively by an attention-based neural implicit model that attends to pixel-aligned features from both the global human-object image for spatial awareness and  the local separate view of human and object images for high quality details. Additionally, the network is conditioned on semantic features from an initial estimated human-object pose prior and a generative diffusion model that inpaints occluded regions, thus enabling the retrieval of details from them.
% We also propose a synthetic dataset with rendered scenes of diverse, inter-occluded 3D human and object scans, to train our network. We evaluate our method on the synthetic and real world BEHAVE dataset. Our experiments show that our method outperforms the SOTA in achieving realistic clothed human-object reconstructions.
Recent approaches to jointly reconstruct 3D humans and objects from a single RGB image represent 3D shapes with template-based or coarse models, which fail to capture details of loose clothing on human bodies. In this paper, we introduce a novel implicit approach for jointly reconstructing realistic 3D clothed humans and objects from a monocular view. For the first time, we model both the human and the object with an implicit representation, allowing to capture more realistic details such as clothing. This task is extremely challenging due to human-object occlusions and the lack of 3D information in 2D images, often leading to poor detail reconstruction and depth ambiguity. To address these problems, we propose a novel attention-based neural implicit model that leverages image pixel alignment from both the input human-object image for a global understanding of the human-object scene and from local separate views of the human and object images to improve realism with, for example, clothing details. Additionally, the network is conditioned on semantic features derived from an estimated human-object pose prior, which provides 3D spatial information about the shared space of humans and objects. To handle human occlusion caused by objects, we use a generative diffusion model that inpaints the occluded regions, recovering otherwise lost details. For training and evaluation, we introduce a synthetic dataset featuring rendered scenes of inter-occluded 3D human scans and diverse objects. Extensive evaluation on both synthetic and real-world datasets demonstrates the superior quality of the proposed human-object reconstructions over competitive methods.
\end{abstract}