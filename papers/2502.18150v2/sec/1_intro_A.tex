\vspace{-0.5cm}
\section{Introduction}
\label{sec:intro}
Realistic, personalized human avatars that seamlessly coexist with objects will shape the future of movies, games, telepresence, and the metaverse. The joint reconstruction of clothed humans and objects will be key to this vision. Emphasis will be on achieving realism in the reconstructed shapes to accurately reflect real-world characteristics. 
\begin{figure}[!t]
  \centering
\includegraphics[width=\linewidth]{images/intro.pdf}
\vspace{-7mm}
\caption{\name jointly reconstructs realistic clothed humans and objects from synthetic (a) and real (b) images by first handling human occlusion with a conditioned generative model followed by attention-based neural implicit model estimation.}
\label{fig:intro}
\vspace{-8mm}
\end{figure}
Motivated by this, we aim to jointly reconstruct realistic clothed humans and objects from a single-view human-object scene. This task presents significant challenges due to human-object occlusions and unknown camera parameters, which make it difficult to accurately infer the 3D spatial configuration (depth, scale, pose) as well as the realistic shape for reconstruction. Existing methods ~\cite{xie2022chore, zhang2020perceiving, nam2024contho, xie2023template_free, xie2023vistracker} that reconstruct 3D humans and objects from a single RGB image focus mainly on optimizing the 3D spatial configuration, failing to capture realistic details such as clothing, hairstyles, and the free-form geometry of human-object shapes. They represent 3D shapes using either parametric, template-based, or coarse models, which constrain the surface geometry, thereby limiting realism. We therefore explore implicit representations~\cite{mescheder2019occupancy, chen2019learning, park2019deepsdf}, which, unlike parametric models, enable realistic reconstruction without constraining the topology. However, while implicit representations can model realistic shapes and poses, they are prone to depth ambiguity if the only input is a 2D image without any explicit 3D depth information.
\\
To address these challenges, we propose \name, a novel framework for Realistic Clothed Human and Object joint Reconstruction. To obtain realistic surface details, \name incorporates a novel attention-based neural implicit model to estimate implicit representations of human-object shapes, assisted by a generative diffusion model that recovers details from regions of the human body occluded by the object. To resolve depth ambiguity, the neural implicit model is also conditioned on an estimated human-object pose prior, integrating 3D spatial information into the estimation process.
Specifically, we first segment the input human-object image to obtain separate human and object images. The generative diffusion model then inpaints the body regions occluded by the object in the human image, generating a full-body human image.  This image, along with the object image and additional inputs, form the 'local' context which embeds local details information. The input human-object image serves as the 'global' context, providing spatial cues between the human and the object. The neural implicit model uses pixel-aligned features from both image contexts, merging them through an attention-based architecture before estimating implicit representations, allowing the retrieval of realistic details while considering the contextual relationship between the human and the object.   
\\
Additionally, the neural implicit model is conditioned on semantic features derived from human-object pose priors, which are estimated using parametric human-object reconstruction methods~\cite{xie2022chore, zhang2020perceiving, nam2024contho, xie2023template_free}. These methods enforce geometric and spatial constraints to optimize for the 3D location, depth and scale of the human and object, providing \name with 3D spatial information essential to address the problem of depth ambiguity. The proposed model then computes the implicit representations in the reference frame of the predicted 3D location prior. 
By effectively decoupling depth ambiguity and detail surface retrieval, \name enables more accurate reconstruction of realistic clothed humans and objects, as shown in~\cref{fig:overview}.
\\
Due to the lack of datasets with high-quality 3D ground-truth human-object scenes, we create \dataname, a synthetic dataset for Human Object Reconstruction to train and evaluate \name. We 
generate several 3D spatial configurations of human-object scenes by randomly placing 3D human scans from THuman2.0~\cite{yu2021function4d} with selected object meshes from BEHAVE~\cite{bhatnagar2022behave} and HODome~\cite{zhang2023neuraldome}.  We also evaluate our model on the real-world BEHAVE~\cite{bhatnagar2022behave} dataset and demonstrate superior performance of reconstructions against state-of-the-art methods.
Our key contributions include: 
\begin{itemize}
  \item A novel framework to jointly reconstruct realistic clothed humans and objects from single images. This is the first work that represents realistic human details in the joint reconstruction of a non-parametric human-object shape.
  \item A novel attention-based neural implicit network to estimate the implicit representation of realistic clothed humans and objects. Pixel-aligned features are extracted from local and global views and then merged along with 3D spatial information via transformer encoders, capturing realistic details while learning contextual information across local and global scenes.
  \item We demonstrate superior reconstruction quality compared to the state-of-the-art methods, both quantitatively and qualitatively, on synthetic and real datasets. 
\end{itemize}