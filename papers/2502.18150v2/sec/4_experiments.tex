\section{Experiments}
\label{sec:experiments}
\textbf{Datasets:} Due to the lack of 3D human-object datasets with high-quality meshes that represent clothing details, we introduce the dataset \dataname to train our model. \dataname is a synthetic dataset created using $526$ 3D human scans from Thuman 2.0~\cite{yu2021function4d} and $27$ 3D object scans including all 20 objects from BEHAVE~\cite{bhatnagar2022behave} and selected 7 from HODome~\cite{zhang2023neuraldome}. We picked random pairs of human and object meshes, where the object was initialized with a random pose and optimized to be in contact with the human. We then simulated 6 random translations of the human-object pair within the FOV of a perspective camera placed at the origin, and rendered 180 views for each translation. Finally, we discard the images where the object is not in view.  We train \name with 500 subjects of \dataname. For quantitative evaluation, we use 99 images from \dataname. We test the generalization power of \name by inferring the human-object shapes from real images taken from BEHAVE~\cite{bhatnagar2022behave}. BEHAVE is a real-world dataset that captures interactions between 8 humans and 20 objects. Due to its real-world setting, high-quality ground truth meshes are unavailable; therefore, we perform only a qualitative evaluation.
\\\textbf{Implementation details.} We use SAM~\cite{kirillov2023segment} to segment the input image into object and human and create the masks $M_p$ and $M_i$. CHORE~\cite{xie2022chore} is applied for pose estimation of the human and object. Once the diffusion model generates the inpainted image, we segment the occluded body region with $M_i$ and merge it into the partial human image $I_p$ to obtain the full-body human image $I_h$. The 2D normal map is estimated from $I_h$  using pix2pixHD~\cite{wang2018high} as in PIFuHD~\cite{pifuhd}.
To train the attention-based neural implicit model, we sample two sets of $N=200000$ points around the ground-truth object and human surfaces, using a mix of uniform and importance sampling with variances $\sigma=0.06, 0.01, 0.035$. From these, we randomly select $N_h=N_o=20000$ points to create the $X_h$ and $X_o$ subsets. These points are projected into the input images following the Kinect camera model from the BEHAVE dataset~\cite{bhatnagar2022behave}. Both the diffusion and neural implicit models are trained on a single A100 GPU.
See supplementary for additional details about implementation.
\\\textbf{Metrics:} We follow the evaluation
in SiTH~\cite{ho2024sith} to compute 3D metrics point-to-surface distance (P2S),
Chamfer distance (CD), normal consistency (Normal), Intersection over Union (IoU) and fScore~\cite{tatarchenko2019single} on the generated meshes. The metrics are computed between the combined 3D human-object ground truth and the estimated reconstruction, with the estimated shape aligned to the ground truth by rescaling and translating it to match the human centroid.
\subsection{Comparisons}
\label{ssec:comp}
\vspace{-1mm}
Our goal is to demonstrate that \name can jointly reconstruct realistic clothed human and object shapes from single images. To the best of our knowledge, \name is the first framework that represents realistic human details in the joint reconstruction using an unconstrained topology for the human-object shape since related methods rely on template-based or coarse representations, reducing reconstruction realism. We also compare \name with approaches that focus solely on high-quality human reconstruction to highlight their inability to reconstruct objects.
\begin{figure*}[h]
  \centering
\includegraphics[width=\linewidth]{images/comp2.pdf}
\vspace{-7mm}
\caption{Qualitative evaluations against methods which aim to reconstruct human-object jointly with examples from BEHAVE dataset~\cite{bhatnagar2022behave}. Note that HDM generates point clouds rather than meshes. Front and side views are shown.}
\label{fig:comp2}
\vspace{-6mm}
\end{figure*}

\noindent{\textbf{Parametric human-object reconstruction methods}: We compare \name against state-of-the-art human-object reconstruction methods from a single image that use a template-based or coarse representation of the 3D shapes, namely CHORE~\cite{xie2022chore}, CONTHO~\cite{nam2024contho} and HDM~\cite{xie2023template_free}. As shown in ~\cref{fig:comp2}, \name is the only method that reconstructs realistic clothed human shapes along with the objects. Related works use a parametric-based representation of humans, which lacks detail and significantly reduces realism. These approaches are designed solely to predict the human-object spatial configuration. \name leverages them as priors and significantly improves the realism of the reconstruction while preserving the 3D spatial configuration. Note that we do not quantitatively evaluate parametric methods as the quality of their human shapes is much lower than non-parametric shapes.} 
\begin{table}[t]
\centering
\resizebox{\linewidth}{!}
{\input{tables/comp}}
\vspace{-3.5mm}
\caption{Quantitative comparisons on \dataname dataset between \name and related works that reconstruct high-quality 3D shape in the bottom part. Results from comparisons with baselines created from PIFuHD~\cite{pifuhd} are also presented.}
\vspace{-6mm}
\label{tab:comp}
\end{table}

\noindent{\textbf{Realistic human reconstruction methods}: We evaluate \name against methods designed for high-quality human shape reconstruction from single images using neural implicit models (PIFuHD~\cite{pifuhd}, ECON~\cite{econ}, SiTH~\cite{ho2024sith}). To demonstrate that these works cannot jointly reconstruct human and object shapes, we use the RGB image $I_f$ as input. We also retrain PIFuHD on synHOR ($\mathrm{PIFuHD}_{\mathrm{ho}}$) and introduce several baseline models based on the PIFuHD architecture to showcase \name's superiority in joint human-object reconstruction. We first use two MLPs instead of one to estimate an implicit representation of both human and object ($2\mathrm{PIFuHD}_{\mathrm{ho}}$). Since PIFuHD does not rely on 3D spatial information, we repeat the previous experiments by concatenating the human-object pose feature $\sigma$ to the extracted features (indicated with the prefix $\sigma$). Features are then extracted from only segmented human and object images ($2\sigma\mathrm{PIFuHD}^{sep}_{\mathrm{ho}}$) and finally, the images ($I_f, I_h, I_o$) are concatenated together as input before feature extraction ($2\sigma\mathrm{PIFuHD}^{all}_{\mathrm{ho}}$). We do not  repeat these experiments with ECON and SiTH because adapting their methods to our dataset would require substantial modifications. As shown in~\cref{tab:comp}, \name achieves the best quantitative results and, as seen in~\cref{fig:comp1}, is the only approach that can jointly reconstruct realistic clothed human and object shapes without noise. Human-focused approaches (ECON, SiTH, PIFuHD) struggle with object reconstruction, impacting human shape quality as well. Retraining PIFuHD with \dataname dataset ($\mathrm{PIFuHD}_{\mathrm{ho}}$) and even adding two MLPs and pose parameters ($2\sigma\mathrm{PIFuHD}_{\mathrm{ho}}$) is insufficient to achieve the realism of \name, which outperforms all methods significantly. See supplementary for more comparisons.}
\begin{figure*}[t]
  \centering
\includegraphics[width=\linewidth]{images/comp1.pdf}
\vspace{-7mm}
\caption{Visual comparisons from \dataname dataset with approaches that aim to reconstruct 3D humans as well as with baselines designed for fair comparisons. Front and side views are shown.}
\label{fig:comp1}
\vspace{-6.5mm}
\end{figure*}
\vspace{-1mm}
\subsection{Ablation Study}
\label{ssec:abl}
\vspace{-1mm}
\indent \textbf{Effectiveness of architecture configuration}: We demonstrate the significant improvement achieved by using the architecture configuration  explained in~\cref{ssec:neural}. 6 alternate configurations of the architecture of the  attention-based neural implicit model of \name are trained and tested:
\begin{itemize}[noitemsep]
\item\textbf{Single MLP}: To prove the effectiveness of estimating an implicit representation for each human and object, a single MLP is used on concatenated features $\varphi_{\{h,o\}}$ to estimate a single occupancy $\hat{s}$ for both human and object. 
\item\textbf{Single Trans}: Use of only a single transformer encoder $A$ to merge human, object and input image features. $\hat{s}_h$ and $\hat{s}_o$ are estimated from the merged feature using $f_h$ and $f_o$.
\item\textbf{Single All}: Combination of the previous two configurations with a single transformer encoder and a single MLP.
\item\textbf{No Trans}: The embeddings $\phi_{\{h,o,f\}}$ from the feature extractors are simply concatenated without applying the transformer encoders, highlighting the benefit of using self-attention. $\hat{s}_h$ and $\hat{s}_o$ are estimated.
\item\textbf{Concat Trans}: Instead of extracting features directly from $I_f$, this configuration concatenates  $I_f$ with both $I_h$ and $I_o$. $\phi_f$ is not extracted. $\phi_o$ and $\phi_h$ are merged with a transformer encoder before estimating $\hat{s}_h$ and $\hat{s}_o$.
\item\textbf{Concat No Trans}: Similar to \textbf{Concat Trans}, but  $\phi_o$ and $\phi_h$ are processed separately by  $f_h$ and $f_o$ respectively.
\end{itemize}
Both quantitative (\cref{tab:abl1}) and qualitative (\cref{fig:abl1}) results prove that the \name's architecture outperforms the other configurations. Specifically, replacing two MLPs with one (\textbf{Single MLP}) reduces reconstruction accuracy  and detail, as seen in the hand of the BEHAVE model. 
If two MLPs process the same feature obtained by merging all the features with a single transformer (\textbf{Single Trans}), human-specific information is lost, resulting in an incomplete human reconstruction.  Using both a single MLP and a single transformer degrades quality, producing smoother surfaces than \name. Omitting the transformer encoder altogether (\textbf{No Trans}) makes the network less robust and introduces noise in the reconstruction, showing the importance of global-local contextualization. Finally, concatenating the input image $I_f$ directly with $I_o$ and $I_h$, significantly reduces quality compared to \name, both when a transformer is used to merge human-object features (\textbf{Concat Trans}) and when it is not (\textbf{Concat No Trans}), proving that extracting features directly from $I_f$ improves performance. 
\begin{figure*}[t]
  \centering
\includegraphics[width=\linewidth]{images/abl1.pdf}
\vspace{-7mm}
\caption{Qualitative results showing the effect of using different configurations of the attention-based neural implicit model. Front and side views are shown from \dataname in the top row, from BEHAVE~\cite{bhatnagar2022behave} in the bottom.}
\label{fig:abl1}
\vspace{-4mm}
\end{figure*}
\begin{figure*}[h]
  \centering
\includegraphics[width=0.85\linewidth]{images/ablation2.pdf}
\vspace{-3mm}
\caption{Qualitative results showing the effect of using different combinations of input data. Front and side views are shown from \dataname in the top row, and on BEHAVE~\cite{bhatnagar2022behave} in the bottom row.}
\label{fig:abl2}
\vspace{-6mm}
\end{figure*}
\begin{table}[t]
\vspace{-3mm}
\centering
\resizebox{\linewidth}{!}
{\input{tables/abl1}}
\vspace{-3.5mm}
\caption{Quantitative results on \dataname dataset obtained by modifying the architecture of the network.}
\vspace{-6.5mm}
\label{tab:abl1}
\end{table}
\indent \textbf{Effect of different inputs:}
\name uses multiple inputs to reconstruct the 3D human-object, including the RGB image $I_f$, full-body human image $I_h$, 2D surface normal $S_N$, segmented object $I_o$ and human-object poses $\sigma_{\{h,o\}}$. In this ablation study, we show the advantages of combining all these inputs. We adapt \name architecture to process different combinations of input data. In each configuration, two MLPs are still used to estimate $\hat{s}_h$ and $\hat{s}_o$.
\begin{itemize}[topsep=0pt,partopsep=0pt,itemsep=0pt,parsep=0pt] 
    \item\textbf{No} $\mathbf{S_N}$: Same  as \name but without  2D normal map $S_N$ concatenated to $I_h$.    
    \item\textbf{No} $\boldsymbol{\sigma_{\{h,o\}}}$:  Same as \name but without  processing the human-object pose features $\sigma_{\{h,o\}}$.
    \item\textbf{Only RGB}: Same as \name without 2D normal map $S_N$ and human-object pose features $\sigma_{\{h,o\}}$.
    \item\textbf{No} $\mathbf{I_f}$: No input image $I_f$, so the image features $\Phi_f^{\{h,o\}}$  are not extracted. The human image feature $\Phi_h^{h}$ is merged with the object one $\Phi_o^{o}$ with a transformer encoder.
    \item\textbf{No} $\mathbf{I_o}$: No object image $I_o$. The human image feature $\Phi_h^{h}$ is merged with the input image feature $\Phi_f^{h}$ using $A_h$.
    \item\textbf{No} $\mathbf{I_h}$: No full-body human image $I_h$. The object feature $\Phi_o^{o}$ is merged with the input image feature $\Phi_f^{o}$ using $A_o$.
    \item\textbf{Only} $\mathbf{I_f}$: Only the input image $I_f$ is processed. No transformer is applied.
\end{itemize}
\begin{table}[t]
\centering
\resizebox{\linewidth}{!}
{\input{tables/abl2}}
\vspace{-3.5mm}
\caption{Quantitative results on \dataname dataset  obtained with different combinations of input data.}
\vspace{-6mm}
\label{tab:abl2}
\end{table}
The benefit of incorporating all input data types is demonstrated by the superior quantitative results in~\cref{tab:abl2}. This is further supported by the visual results in~\cref{fig:abl2}, where more realistic and accurate human-object shapes are obtained with \name. When the input RGB image is omitted (\textbf{No } $\mathbf{I_f}$), the model lacks global scene understanding, resulting in severe artifacts. Likewise, if human and object images are not used (\textbf{Only}  $\mathbf{I_f}$), the network lacks local information about them. \textbf{Only RGB} shows the importance of using both normal maps and pose features to reproduce realistic details and avoid depth ambiguity in the reconstruction. Additional ablations are in the supplementary. 
\\\textbf{Limitations and Future work:} Modeling realistic clothed human and object interactions is an extremely challenging problem. 
This work represents the first step toward achieving this goal. The accuracy of the interaction between humans and objects will be further improved in future works. We also aim to improve the quality of the object shape by introducing higher quality ground-truth object shapes in the dataset to learn high-frequency details for the objects as well.  While our approach specifically addresses cases where a human is occluded by an object, we will extend it to handle object occlusions as well. 
%Although \name can reconstruct arbitrary object shapes, it currently relies on object pose priors from a method~\cite{xie2022chore} that requires known object templates, limiting its generalization to unseen objects. This limitation can be addressed by integrating priors from recent template-free methods~\cite{xie2023template_free}. 
We also aim to retrieve textures for clothed human-object reconstructions. 