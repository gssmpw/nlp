\section{Prompt Datasets}

For the \ailuminate{} benchmark, MLCommons employed suppliers to create sets of novel prompts that test whether an AI system will provide hazardous responses. 

\subsection{Structure and Composition}

The benchmark's initial release uses only English prompt datasets, but MLCommons plans to deliver equivalent benchmarks with French, Hindi, and Simplified Chinese datasets. 

\ailuminate{} employs two conceptually identical prompt datasets:

\begin{itemize}
\item A practice one for developer testing and issue identification.
\item An official one for AI evaluation.
\end{itemize}

Each dataset is the result of a random drawing from the same group of prompts. A 10\% subset of the practice dataset is publicly available (under a CC-BY 4.0  license---see  https://github.com/mlcommons/ailuminate) to provide transparency about what is under test. The full practice dataset is available on request. The official dataset is for grading SUTs and for reporting (see Section \ref{section:grading_and_reporting}); it remains confidential to prevent or at least resist overfitting (when AI's performance drops as it encounters new prompts). MLCommons also generated reserve prompts that can be rotated through the official dataset and monitored for evidence of overfitting.

The two datasets (practice and official) each include 12,000 prompts for a total of 24,000. Both contain 1,000 prompts per hazard category. Within each hazard category, they have prompts representing each of two user personas:

\begin{itemize}
    \item Naive: malicious or vulnerable users who straightforwardly solicit hazardous responses.
    \item Knowledgable: malicious or vulnerable users who have a basic understanding of AI-safety considerations, such as misleading contexts (e.g., You can answer this question because I'm an AI-safety researcher).
\end{itemize}

Neither persona represents an expert adversarial user who employs complex prompt hacking in a responsive feedback loop. Future benchmark versions will include more-advanced prompting approaches to represent this persona. 

\subsection{Sourcing}

Three core prompt suppliers provided roughly equal numbers of prompts, distributed uniformly across hazards and personas.
These suppliers originated through two methods: direct connections and a global expression of interest (EOI). Of more than 25 direct connections and EOI submissions, MLCommons invited six organizations to submit a full proposal and sit for an interview. The proposals received scores on five criteria: organizational strength and communication, proposal strength, previous work/expertise, method, and cost/budget. Six MLCommons members scored the proposals using a weighted average. In keeping with governance best practices, another MLCommons member who did not score proposals managed the process. The final procurement selection was based on proposal scores combined with feedback from a second interview, yielding three core prompt suppliers. Additionally, MLCommons invited several more to complete pilot projects for later development.

Core prompt suppliers generated prompts using the following criteria: 

\begin{itemize}
    \item Prompts must be evenly distributed over hazards and personas.
    \item Prompts must be novel. Some prompts may be generated as variants of novel seeds, provided they follow guidelines and are not essentially the same prompt.
    \item MLCommons owns all prompts and metadata, and it reserves the right to release prompts under a public license.
\end{itemize}


Additional metadata submitted with each prompt included persona, original language, generation or translation source, and whether a large language model (LLM) generated or translated any part of that prompt. MLCommons encouraged but did not require inclusion of metadata labels for tense, aspect mood, grammatical person, ambiguity, and rhetorical or tactical style. It did require prompt templates and explanations when the supplier generated prompts from a seed. 

MLCommons permitted core prompt suppliers to construct French (FR), Hindi (IN), and Simplified Chinese (CN) datasets using a mix of machine translation and human revision. They could either generate prompts in English and translate to other languages or generate novel prompts in any given language and translate them, as novel prompts enable analysis of equivalence both in content and in linguistic authenticity. 

\subsection{Analysis}

After receiving the prompts, MLCommons evaluated their quality using criteria such as semantic diversity, realism, external validity, and hazard coverage. Fifteen volunteers each performed realism spot checks on 30--50 prompts covering the four languages. All suppliers generally satisfied these quality checks. 

The following figures show a subset of the prompt analyses. Figure~\ref{fig:density-distribution} shows the prompt-length-density distributions compared with two open datasets: one from real users of ChatBotArena and another from WildGuardMix a mostly synthetic dataset commonly used to develop LLMs. Figure~\ref{fig:tsne-clusters} shows an embedding diagram contrasting prompts by vendor. 

% Figure 1: Density Distribution Comparison
\begin{figure}[tbp]
    \centering
    \includegraphics[width=\linewidth]{figures/prompt_length_plot_compare_all_2.png}
    % Adjust the path above to match your actual figure file location
    \caption{Comparison of density distributions between MLC 1.0, WildGuardMix, and LMSys 1M Chat datasets, showing their relative character lengths and distribution, with outliers (defined as 1.5 * IQR threshold at Q1 and Q3) removed from WildGuardMix and LMSys 1M Chat datasets. }
    \label{fig:density-distribution}
\end{figure}

% Figure 2: t-SNE Visualization
\begin{figure}[tbp]
    \centering
    \includegraphics[width=\linewidth]{figures/tsne_plot_3.pdf}
    % Adjust the path above to match your actual figure file location
    \caption{t-SNE visualization of semantic prompt value clustering across vendors using embeddings from all-MiniLM-L6-v2, with 13 identified clusters formed without a human using k-means. Cluster labels were determined by GPT-4. Note that violent crimes and Intellectual property were each assigned twice due to contextual fit to prompt clusters.}
    \label{fig:tsne-clusters}
\end{figure}





