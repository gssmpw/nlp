\section{Initial Results}

Along with \ailuminate{}, MLCommons is releasing initial results for select systems-under-test (SUTs). The choice of these SUTs was on the following basis:
\begin{itemize}
    \item It included the four SUT vendors of greatest public interest: Anthropic, Google, Meta, and OpenAI.
    \item For each target language, one SUT vendor received priority as being of greatest public interest in the country with the most speakers of that language. For French, it was Mistral. (French was slated to appear in the initial release but was pushed back late in the process.)
    \item Companies that sponsored the effort had the option to be included. 
    \item MLCommons used OLMo v1.0 as a control because it comes from a high-trust nonprofit organization, provides open training data, and is older and less safety optimized. This model demonstrates the benchmark's difficulty.
\end{itemize}

All results are available at \url{https://mlcommons.org/ailuminate/}. Because they may receive live updates when issues arise, we excluded them from this report. 

\section{Testing Integrity}
Protecting the benchmark's methodological integrity is critical to producing results that people can rely on to make safety-informed decisions. Methodological integrity ensures the benchmark accurately measures and presents the qualities it purports to measure, but changing circumstances may threaten its longevity.

Central considerations for AI-safety methodological integrity include the following:
\begin{itemize}
    \item \textbf{Correctness:} Could the scores be biased in some unknown or undisclosed way?
    \item \textbf{Comprehensiveness:} Would a reasonable person relying on the benchmark believe it covers an application, context, or SUT characteristic that is not covered?
    \item \textbf{Consistency:} Does the score have unreasonably high variance for its intended purpose?
    \item \textbf{Intelligibility:} Will a reasonable person understand the SUT characteristics for applications and contexts that the benchmark identifies?
\end{itemize}

The ML Commons benchmark-safety program offers one or more answers to each of these questions. A registry of these responses appears in Appendix \ref{section:failure_mode}.

The response registry also contains a collection of practices for defending the longevity of the benchmark's integrity. These practices include periodically refreshing the benchmark as well as enforcing policy requirements on all SUT developers. The main policies for maintaining the benchmark program's longevity include the following: 


\begin{enumerate}
    \item Techniques that are likely to increase test performance without commensurately increasing the safety factor are discouraged and may result in exclusion from the benchmark. For example, publishers cannot analyze the topics in the hazard-taxonomy categories and tune the SUT to selectively refuse answering questions about them.
    \item Publishers must include the benchmark version number and prominently declare that results from deprecated versions are ``obsolete and should not be used for safety assessment or decision-making.'' New results from deprecated versions are only to be used for internal development and in scientific publications where the newest benchmark is also reported.
    \item The system prompts, weights, and safety features (including refusal mechanisms) of AI systems whose results are advertised cannot vary from those under test conditions. Untested systems (such as previously tested AI systems that add a new system prompt) must be clearly presented as untested.
\end{enumerate}


Adherence to these requirements is subject to confirmation through various means, including technical measures and periodic declarations from the SUT developers. Future benchmark versions may require disclosures consistent with shared industrial practices as developed in various settings, such as the NIST AI Risk Management Framework. Noncompliance may incur restricted access to benchmark trademarks as well as public statements correcting the record. Both accidental and intentional violations against these requirements can result in the SUT suffering a permanent ban from the benchmark.

\section*{\section{Acknowledgements}}
We thank everyone who gave feedback on the taxonomy, prompts and/or benchmark, contributed to our research and outreach process or gave feedback on our work. This includes everyone who has joined the AI Risk and Reliability Working Group, and the following individuals and organizations: Tarunima Prabhakar (Tattle Civic Tech),  Tajuddeen Gwadabe (Masakhane), Ziad Reslan (OpenAI), Brian Fuller (Meta), and Carlos Ignacio Gutierrez (Google). We also want to thank Tattle Civic Tech's for its contributions to the Exploratory Studies. Tattle Civic Tech's team included Aatman Vaidya, Denny George, Kaustubha Kalidindi, Maanas B, Mansi Gupta, Saumya Gupta, Srravya C, Tarunima Prabhakar, and Vamsi Krishna Pothuru, along with a large group of experts cited in its report to MLCommons. 
We particularly thank all of the team at MLCommons.
\newpage
