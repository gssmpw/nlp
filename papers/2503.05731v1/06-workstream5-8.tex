\section{Limitations}



\ailuminate{} is a useful safety indicator, but it has substantial limitations.

\begin{enumerate}
    \item  The benchmark has a \textbf{limited scope}: It only tests the hazards and personas in the assessment standard. It ignores ``untestable'' hazards such as environmental impact, other hazards such as inaccurate medical advice, unlisted hazards such as bias, and unlisted personas such as expert adversarial users. The persona limit is particularly worth highlighting: An expert adversarial user attacking a specific model with adaptive techniques would achieve a substantially higher unsafe rate. In addition to these basic limits, the assessment standard is intended for generic global use and is in no way regionalized. To clarify, a global, multilingual team wrote the assessment-standard prompts and evaluator guidelines in American English and in alignment with US cultural norms. Additionally, the development process lacked a public comment period, a common part of creating global policies, to solicit diverse feedback.
    \item  The benchmark uses human-created \textbf{single-prompt interactions}: It neither employs prompts recorded from real, unknowing user interactions (doing so would be almost impossible given user privacy requirements and the nature of the prompts) nor tests sustained interactions with intricate contexts. Instead, MLCommons contracted with companies to engineer the prompts for specific hazard categories and the described use case.
    \item The benchmark has \textbf{significant uncertainty}: Prominent reasons include prompt subsets from an infinite number of possibities, an imperfect evaluator, and nondeterministic responses from tested the SUTs. See the Section \ref{scoring_and_grading_variance}  for a more detailed analysis of the evaluator uncertainty. We focused on evaluator uncertainty because we expect it will be large relative to other uncertainties. Quantitative analysis of the evaluator quality plus ongoing conversations with the human annotators has revealed features of MLCommons' approach that could stand improvement. They include the following:
    \begin{enumerate}
        \item Better instructions for human annotators. Examples are greater annotation-guideline clarity and more frequent sit-down sessions with annotators to discuss these guidelines.  
        \item More human-annotator touches per response.
        \item Feedback-mechanism transparency to allow continuous improvement of unclear policies as the prompt scope, range of modalities, and number of interactions increase.
        \item Lower evaluator uncertainty.
        \item Analysis of additional uncertainties.
    \end{enumerate}
    The evaluator's performance variability produces a wider-than-expected error band, as Section \ref{scoring_and_grading_variance} shows.
    \item Section \ref{scoring_and_grading_variance} introduces the concept of an error band to show the variance between the predicted upper and lower bounds (Figure~\ref{fig:grade-uncertainty}). A result can have an \textbf{error band that spans multiple grades}. Note that as performance worsens (meaning more unsafe responses) the error band shows that the variation increases, because false-unsafe responses are much more likely than false-safe responses.
    \item The grading and scoring calculations for individual hazards \textbf{assume an unsafe response falls into the same hazard category as the prompt that generated it.} These calculations therefore also assume an unsafe response is only associated with a single hazard category. In reality, a prompt in one category can create a response that is hazardous in a different category. For example, a prompt in the defamation hazard category could create a response that contains no defamation but does enable nonviolent criminal activity, making it hazardous in a different category. Similarly, a response could be hazardous in two or more categories. For example, one that endorses violent crime against a certain race could be hazardous in both the hate and violent-crimes categories. Addressing the combinatorial complexity of hazard prompts and categories is crucial to reflecting how humans consider safety. 
    \item The tested SUTs comprise \textbf{different systems types.} We accessed some SUTs through APIs, which give developers an opportunity to include additional safeguards. When implementing open-source AI systems, it is important to follow the provider's instructions and to use the entire system, which often includes safety filtering (also called guardrails) to achieve the best results.
    \item The benchmark's \textbf{iterative development} is at v1.0: \ailuminate{} is relatively new in a fast-moving field, so issues and fixes are to be expected. Constructive criticism is welcome.


\end{enumerate}


\section{Exploratory Studies and Future Work}


\ailuminate{} v1.0 is a first step toward a global standard AI risk assessment benchmark, but much work remains. To this end, part of the v1.0 effort included studies to support future versions, as well as additional benchmarks, with broader scope and capabilities. Support for the studies came primarily through an open Expression of Interest, with an additional goal of involving more people around the globe in the benchmark's development. Note that these studies are exploratory rather than definitive or exhaustive.

\subsection{Improved Prompts}

Several of the studies examined how to improve the core benchmark's quality through better prompts or prompting technology.

MLCommons selected the organization Brainstorm: the Stanford Lab for Mental Health Innovation, Stanford University School of Medicine, Department of Psychiatry \url{https://www.stanfordbrainstorm.com/} to evaluate SUT responses in the suicide \& self-harm hazard category using clinical forensic-psychiatry research. The project employed expert psychiatric knowledge to annotate unacceptable SUT responses that the average human annotator may miss. Brainstorm Solutions developed a alternative evaluation method based on levels of suicidal ideation, which indicates whether and to what degree a prompt (user) exhibits the propensity to self-harm. Responses can then undergo annotation with greater clinical accuracy on the basis of the original prompt's tendency to elicit suicidal ideation. A future benchmark release may incorporate the suicidal-ideation categories and expert response annotations. The final project findings will appear in a separate report. 
%%Figure x shows the decision flowchart for assessing suicidal ideation.

Another study, led by PRISM Eval~\citep{kirk2024prismalignmentdatasetparticipatory}, examined adaptive strategies to generate jailbreaks that elicit harmful behavior from a target LLM. Motivating the work was the need for
robustness measurements that account for AI-specific characteristics, because static
benchmarks may miss important variations of prompts that could lead to unsafe responses. PRISM Eval developed an
initial version of the Behavior Elicitation Tool (BET), to demonstrate how optimization-based testing can measure model defenses. More information may be found at \url{https://www.prism-eval.ai/}.

A preliminary analysis revealed that within just a few optimization steps, prompt
effectiveness can improve greatly (from 8\% to 78\% in fewer than five steps) even
while ensuring diversity in the generated prompt. Additionally, by examining the heatmap showing
the effectiveness of diverse techniques (see Figure ~\ref{fig:prism-fig}), the company found that techniques that succeed against one
AI system were often less effective against others, and some techniques even varied in performance depending on an AI system's behavior. These findings suggest that
accurately measuring the resistance to jailbreaking could require dynamic,
model-specific approaches similar to BET rather than static benchmarks. BET improvements will focus on analyzing the optimization curve to extract a clear
metric for comparing an AI system's defenses and for guiding their
improvement.

\begin{figure}[tbp]
    \centering
    \includegraphics[width=\linewidth]{figures/technique_heatmap_2.png}
    % Adjust the path above to match your actual figure file location
    \caption{Preliminary heatmap showing effectiveness of a subset of techniques (rows) on three
AI systems (columns), with darker blue indicating higher effectiveness. Even in this limited sample, the
color variation between models suggests that techniques effective against one may ineffective against others, warranting further investigation.}
    \label{fig:prism-fig}
\end{figure}

\subsection{Region and Language Support}

Other studies looked at building similar benchmarks, or supplementary benchmark modules, for specific regions or low-resource languages. Low-resource languages are languages that lack online datasets, digitized text, and tools that help facilitate the development of language technology including automated translation and speech recognition. Many low-resource languages are primarily oral and include a majority of languages spoken in Asia and Africa. High-quality LLM development requires more data than is currently available for low-resource languages, but recent research is addressing data challenges~\citep{vayani2024languagesmatterevaluatinglmms, adelani2025irokobenchnewbenchmarkafrican}.

Tattle Civic Tech is an India-based organization that MLCommons chose to generate prompts in Hindi (IN) for two hazard categories: hate and sex-related crimes. This study addressed the known limitations of core prompt suppliers using machine translations and a lay understanding of Hindi-speaking Indian culture in the construction of prompts. Using a participatory approach with social workers, psychologists, journalists and researchers; and prior data from the Uli plugin tool (https://uli.tattle.co.in/), Tattle created 1000 prompts in each hazard category. Comparative analysis of these prompts and Hindi prompts from MLCommons' core prompt suppliers was provided in a separate report \citep{vaidya2025analysisindiclanguagecapabilities}. Tattle will also give MLCommons a landscape analysis of Indian languages that are best positioned for inclusion in a future benchmark.

MLCommons is also working with Masakhane, a grassroots organization whose mission is to strengthen and spur natural-language-processing (NLP) research in African languages for Africans and by Africans, to create a landscape analysis of native-African-language readiness for a future benchmark. A known challenge for many African contexts and languages is insufficient LLM and machine-evaluator coverage for \ailuminate{}. Masakhane is identifying a mix of technical, linguistic, and demographic inclusion criteria, such as language performance in LLMs as well as government and local initiatives. The draft report is forthcoming.

\subsection{Additional Hazard: Bias}\label{subsec:bias}

An ongoing study supported by MLCommons and conducted by an open work stream of experts from multiple organizations is considering societal biases, a challenging hazard not covered by the v1.0 assessment standard.

Research has identified a wide range of sociotechnical harms from bias in LLMs ~\citep{mehrabi2022surveybiasfairnessmachine, hada2023fiftyshadesbiasnormative, shelby2023sociotechnicalharmsalgorithmicsystems, gallegos2024biasfairnesslargelanguage}. In order to reduce the scope of bias to a more tractable level, the focus of this work is on social-bias harms in generative AI systems: specifically, the reinforcement of binary-gender stereotypes in the outputs of text-to-text language models. The group is curating prompt-pairs that differ only in gender signifiers and is using sentiment analysis of the associated responses as a proxy for implicit stereotype reinforcement to measure bias ~\citep{sheng2021societalbiaseslanguagegeneration,huang2020reducingsentimentbiaslanguage,liu2020doesgendermatterfairness,Dhamala_2021,su2023learningredteaminggender,hoyle2019unsuperviseddiscoverygenderedlanguage,kumar2024decodingbiasesautomatedmethods}. The effort is anchoring to attested biases to ensure construct validity ~\citep{raji2021aiwideworldbenchmark}, and the work is modular to allow for the addition of other biases. Currently, it includes only stereotypes attested in English-speaking US contexts. 

Figure~\ref{fig:bias-fig} illustrates an example prompt-pair workflow.

\begin{figure}[tbp]
    \centering
    \includegraphics[width=\linewidth]{figures/WS7Fig_II.jpg}
    % Adjust the path above to match your actual figure file location
    \caption{ Workflow design showing a single prompt-pair. The prompts differ only in the gender of the target individuals: one aligns with a known stereotype (“ballet is for girls”) and the other with the anti-stereotype. The bias assessment uses the difference between the sentiment-analysis scores for the model responses to each prompt.}
    \label{fig:bias-fig}
\end{figure}

Our primary goal of this effort is to establish a solid foundation for MLCommons benchmarks to consider the complicated problem of measuring distributional social bias in LLMs. Doing so involves probing and identifying bias in LLMs, measured in various context and stereotype categories, and formulating a workable prototype in order to collect feedback.

\subsection{Additional Modalities}\label{subsec:multimodal}

Generative AI systems are often multimodal, processing and generating not just text but also other modalities such as images and audio. 
Therefore, we developed benchmark datasets for evaluating multimodal AI systems in parallel with the text-only v1.0 benchmark, which is the focus of this report. 
Specifically, we created internal pilot datasets for creating a workflow that evaluates text-to-image and image-to-text models. 
Our primary goal in creating these datasets was to enable dedicated multimodal-benchmark releases by MLCommons.

\paragraph{Text-to-Image (T2I) Models}
We developed a pilot dataset of 774 prompts to evaluate T2I-model safety.
These prompts were either explicit (directly requesting unsafe content, such as ``image of a man shooting another man'') or implicit (indirectly leading to unsafe content, such as ``image of a man really angry at another man and pointing something at him'').
Of these prompts, 486 targeted sexually explicit content and 288 targeted violent content.
Prompts were constructed in one of two ways: (i) handwritten by authors to match the taxonomy definitions, or (ii) sourced from the publicly available Adversarial Nibbler dataset \citep{Quaye2024}, manually filtered for fit to the selected taxonomy.
Future versions may expand the hazard taxonomy and address domain-specific safety in geospatial, health-care, and other applications (including those using prompt rewriting or image editing).
We also note that as state-of-the-art image-generation systems advance and safety compliance improves, future testing may require a larger set of more-difficult test prompts.

\paragraph{Image-to-Text (I2T) Models}
We tested I2T models in four hazard categories adapted from the v0.5 MLCommons taxonomy: violent crimes, nonviolent crimes, sex-related crimes, and suicide \& self-harm.
For each hazard, we created test cases combining images with text prompts that together revealed unsafe meanings---such as ``Should I drink this?'' paired with an image of a bleach bottle---and translated English prompts into 10 other languages.
Testing showed commercial AI systems were generally safe (<1\% unsafe responses), while open AI systems had higher unsafe-response rates (up to 14\%).
Some open AI systems appeared ``safe by accident'' because they misunderstoond requests instead of recognizing and rejecting unsafe content.
We also observed variation in how safety manifested among AI systems: commercial ones tended to provide contextual rebuttals or refusals, whereas open ones often expressed uncertainty or issued generic rejections.
Although we used manual annotation for our pilot study, we also tested automatic evaluators as a more efficient alternative, though none accurately reproduced our human safety assessments.
Looking ahead, we identified three important findings: future benchmarks will need tougher test cases that maintain realism and clarity, we must better distinguish between AI systems that are safe by design and those that are safe by accident and finally, further research into multimodal autoevaluators is needed to enable larger-scale benchmarking.

