\section{Response Evaluator}
\label{section:response_evaluator}
The \ailuminate{} evaluator system categorizes SUT responses to prompts as either violating or nonviolating with respect to the assessment standard. Human evaluation of all model responses, which typically provides the highest-quality ground truth, is posssible, but it lacks scalability and makes obtaining repeatable judgments difficult. Instead, \ailuminate{} v1.0 uses an automatic evaluation mechanism; a small number of human ratings serve to assess and improve evaluator performance.

\subsection{Software Architecture}

The evaluator system is standalone software that manages the entire SUT evaluation life cycle. Called ModelBench (\url{https://github.com/mlcommons/modelbench}), it includes the following components:

\begin{enumerate}
    \item A \textbf{prompt database} containing prompts for benchmarks as well as associated metadata.
    \item A \textbf{benchmark runner} that performs five functions:
    \begin{enumerate}
        \item Send benchmark prompts to a SUT.
        \item Receive responses from the SUT.
        \item Send the prompt/response items to the reponse evaluator for annotation.
        \item Receive policy-violation annotations from the the respone evaluator.
        \item Send annotated items to a data store of benchmark runs.
    \end{enumerate}
    \item A \textbf{benchmark-run-journal data store}, which holds all outcome data and metadata from a benchmark run.
    \item A \textbf{report generator}, which uses the benchmark-run journal to generate a safety violation (i..e policy-violation) report.
    \item A set of functions that implement the \ailuminate{} \textbf{benchmark grading logic} (see Section \ref{section:grading_and_reporting}).
    \item A \textbf{benchmark-report data store}, which holds safety-violation reports for public viewing.
\end{enumerate}

Figure~\ref{fig:software_architecture} shows an overview of ModelBench's execution logic.

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/AILuminate_v1.0_architecture.drawio.png}
    \caption{ModelBench software architecture, which implements the entire \ailuminate{} benchmark}
    \label{fig:software_architecture}
\end{figure}

\subsection{Evaluator Architecture}
The \ailuminate{} automatic evaluation system uses an ensemble of LLMs that jointly assess the benchmark responses, avoiding reliance on a single "off-the-shelf" evaluator---which could introduce bias, especially if it favors systems from its developer. \ailuminate{} therefore uses multiple evaluators that function like a jury to ensure fairness. Some components are open evaluator models fine-tuned for the benchmark. Others are high-performing generic LLMs that are prompt-engineered to generate safety-violation judgements. 

Using AI to build an evaluator for responses from AI systems creates an intrinsic dilemma: the evaluator must be able to better distinguish between violating and nonviolating responses than the system under test (SUT). Fortunately, training an evaluator purely for a benchmark is easy because the task is precisely and narrowly defined: for instance, the full prompt space is bounded and known even though it makes the evaluator much less useful in a general setting as the SUT responses are unknown. 

Figure~\ref{fig:eval_pipeline} depicts the  development flow for selecting and fine-tuning the models in the ensemble. This section describes the method for constructing that ensemble but omits details such as the exact models and fine-tuning data. MLCommons keeps that information confidential to prevent the ensemble from being used as a guard model that enables a SUT to achieve a perfect score despite being less than perfectly safe. 

\subsection{Baseline Evaluators}

The ensemble's baseline evaluators come from state-of-the-art, high-performance safety-moderation models, also called guard models, and from general-purpose base and instruct LLMs. We considered safety guard models such as LlamaGuard~\citep{inan2023llama}, WildGuard~\citep{han2024wildguard}, AegisGuard~\citep{ghosh2024aegis}, and ShieldGemma~\citep{zeng2024shieldgemmagenerativeaicontent}. Additionally, we also considered general-purpose LLMs such as the family of Llama models~\citep{grattafiori2024llama3herdmodels} and Mistral models~\footnote{https://huggingface.co/mistralai/Mistral-7B-v0.3, https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3, https://huggingface.co/mistralai/Mistral-7B-v0.1, https://huggingface.co/mistralai/Mixtral-8x22B-v0.1, and https://huggingface.co/mistralai/Mistral-Large-Instruct-2407}. At different stages of the evaluator pipeline, we considered models such as Mistral Nemo~\footnote{https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407}, multilingual  models such as Aya~\citep{aya}, and others. A candidate evaluator acts as a classifier that categorizes a SUT responses as safe or unsafe (i.e. policy nonviolating or violating) the benchmark's policy and also identifies violation type with respect to a policy if unsafe.
\begin{figure*}
    \centering
    \includegraphics[width=0.8\textwidth, trim = 3cm 3cm 1cm 3cm]{figures/Evaluator/evaluator_pipeline.pdf}
    \caption{\ailuminate{} evaluator-ensemble-development pipeline.}
    \label{fig:eval_pipeline}
\end{figure*}
\subsection{Fine-Tuning Dataset}

MLCommons used a fine-tuning dataset to specialize the baseline evaluators for \ailuminate{}.
This dataset contained responses by several AI systems to prompts from the benchmark practice dataset. MLCommons contracted two companies to conduct human annotations of the prompts and System-Under-Test (SUT) generated responses on the basis of the assessment standard. Each sampled prompt-response pair received either a nonviolating or violating label from each of three raters. A simple majority vote determined the final result. 

\subsubsection{Adjustible Classification Thresholds Based on Entropy}
\label{entropy_eval}

Response classifications from each evaluator underwent adjustment using a tunable threshold based on entropy. Equation~\ref{eq:entropy} expresses the entropy of a model's output-classification probabilities:
\begin{equation}
\label{eq:entropy}
   H\left(\hat{y}\right) = - \sum_{c} p\left(\hat{y}_{c}\right) \log p\left(\hat{y}_{c}\right)
\end{equation} 
where $\hat{y}$ is the logits vector and ${p(\hat{y}_{c})}$ is the probability assigned to class $c$. For a evaluator, $c \in \{\texttt{safe}, \texttt{unsafe}\}$. From Equation~\ref{eq:entropy}, a safety classifier that has a higher likelihood for the most probable class has lower output entropy. ~\citep{wang2020tent} showed that samples with lower output entropy are more likely to receive a correct classification. Threshold tuning enabled adjustment of the false-positive-to-false-negative ratio, makes the evaluators more or less conservative in their safety assessments. 
Furthermore, specific finetuning strategies, data augmentation, and data sampling strategies also contributed towards a configurable threshold. 

\subsection{Ensemble Strategy}
In the next step a combiner based on an ensemble mixing logic combines the predictions from multiple fine-tuned, guard or prompt engineered evaluator models in the ensemble. MLCommons considered several strategies for computing the final label from the ensemble on the basis of individual model labels. Although no strategy is perfect, the final selection prioritizes the lowest rate for false non-violating evaluations and highest benchmark-run repeatability.

Figure~\ref{fig:eval_ensemble} shows an illustrative diagram of an ensemble of evaluators where the individual evaluators are either fine-tuned, prompt-engineered, or are standard guard models. Each evaluator provides a safety assessment to the ensemble. An ensemble logic combines individual assessments to generate a final assessment. The exact logic of the ensemble is not discussed here. MLCommons keeps this information confidential to prevent the ensemble logic from being used to obtain a perfect score. Humans in the loop can confirm the assessment on a subset of samples. 
\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/Evaluator/evaluator_ensemble.pdf}
    \caption{\ailuminate{} evaluator-ensemble-human-verification-concept-diagram.}
    \label{fig:eval_ensemble}
\end{figure*}


\subsection{Assessing Evaluation Quality}
The individual evaluators become members of the ensemble and are individually and jointly as part of the ensemble, designed to minimize the false-safe rate: how frequently the evaluator incorrectly classifies a violating response as nonviolating. Since most models tested by the benchmark have undergone safety-alignment training, they are more likely to generate safe responses. When these models do produce unsafe, policy-violating output, however, the evaluator must accurately identify and classify these rare instances. The individual models and the ensemble strategy will undergo improvement as further analysis of the benchmark runs becomes available. 
