\section{Introduction}


\subsection{Purpose}
This report serves two distinct audiences: general readers and technical experts. The Introduction and Overview Sections are designed for general readers who want to know the motivation, fundamental approach, and application of the \ailuminate{} benchmark. The subsequent sections provide a comprehensive technical analysis of the method, its limitations, and other concerns, enabling technical experts to evaluate the benchmark's validity.


\subsection{Need for AI-Safety Benchmarks}
Although artificial intelligence (AI) has extraordinary potential benefits, it also presents both immediate and long-term risks~\citep{cheatham2019confronting, bender2021on, dragan2024introducing, salhab2024systematic, seoul2024}. Many of these risks, though not all, are empirically testable. Further, text and multimodal interfaces accommodate both beneficial and harmful user intentions, and AI systems can be trained to identify and reject harmful requests~\citep{dubey2024llama3}. Generative AI systems operates as a black box and requires empirical evaluation: even though it can undergo safety testing, its internal mechanisms resist direct inspection \citep{barez2025openproblemsmachineunlearning}. Well-designed standard AI benchmarks establish common testing protocols that solidify assessment efforts, ensure consistent evaluation quality, and drive systematic progress \citep{xia2024ai}.


\subsection{\ailuminate{} Benchmark}
This report introduces \ailuminate{}, the first AI-safety benchmark developed through an open process that included participants from a variety of interested fields. The effort received support from two established nonprofits: the MLCommons Association and the AI Verify Foundation. \ailuminate{} serves three primary objectives:
\begin{enumerate}
\item Guide development of AI-safety measures.
\item Support evidence-based decision-making.
\item Enable creation of standards and policies.
\end{enumerate}

The benchmark incorporates many innovations from published research. It aims to synthesize these concepts into a mature repeatable method. Traditional alternatives such as ImageNet~\citep{deng2009imagenet}, GLUE~\citep{wang-etal-2018-glue}, Dynabench~\citep{kiela-etal-2021-dynabench}, and MMLU~\citep{hendrycks2021mmlu} have catalyzed AI innovation, but maturation of the field and widespread deployment of AI applications require robust, enduring, and realiable benchmarks. Ideal benchmark development benefits from a trusted institution and incorporates cross-field collaboration, academic research, and corporate input \citep{alaga2024grading}. The \ailuminate{} development process establishes and maintains a consensus around the benchmark while bringing more-extensive resources and enabling sustained iterative development beyond what academia alone permits.


\subsection{About the MLCommons Association}
The MLCommons Association aims to advance AI technology for the benefit of all. It pursues this goal through collaborative engineering, standards development, and benchmark and dataset research. MLCommons began in 2020, emerging from the MLPerf speed-benchmarking community founded in 2018. A 501(c)6 nonprofit, its board of directors balances representation from academia, small companies, and large companies worldwide. Development of the \ailuminate{} benchmark took place through an MLCommons working group open to all participants but primarily comprising researchers and engineers. The process involved eight collaborative workstreams with volunteer leads; the working group made design decisions, such as identifying hazards for prompt datasets, through consensus. MLCommons contractors were primarily responsible for business and infrastructure decisions.

\subsection{About the AI Verify Foundation}

The AI Verify Foundation is a nonprofit that aims to harness the global open-source community to develop testing tools that ensure responsible AI use. It released AI Verify, an AI-governance testing framework and software toolkit. This product validates AI systems against a set of internationally recognized principles by applying standard tests, and is consistent with international AI-governance frameworks such as those from the European Union, the Organisation for Economic Co-operation and Development (OECD), and Singapore. The AI Verify Foundation aided in developing \ailuminate{} and is leading an effort to include Chinese-language evaluation and bloster APAC adoption.

% AI systems are black box and empirical: they can be tested for safety but cannot be inspected for it. While several safety benchmarks exist, they primarily focus on extreme or existential risks, leaving a gap in evaluating more immediate and practical safety concerns. Standard AI benchmarks establish common testing approaches: when done well, this concentrates assessment effort, ensures consistent quality, and drives overall progress.



\section{Overview}
\subsection{Scope}
The \ailuminate{} benchmark assesses an AI system's ability to handle prompts designed to elicit dangerous, illegal, or undesirable behavior~\citep{zou2023universal, vidgen2024introducingv05aisafety}. Its scope encompasses four main dimensions: application context, hazard category, interaction model, and locale support.



\subsubsection{Application Context}
The benchmark focuses on general-purpose chatbots (the system under test, or SUT) that provide informational and conversational interactions. This category includes AI systems deployed in an enterprise, community platform, or specific use cases fine-tuned for chat or following instructions~\citep{taori2023alpaca,chiang2023vicuna}. The evaluation framework addresses AI systems designed to provide helpful responses while maintaining boundaries around hazardous topics.

\subsubsection{Hazard Category}
The benchmark assesses SUT responses with respect to 12 text-content hazard categories. These categories fall into three classifications:
\begin{itemize}
\item Physical hazards~\citep{tang2024defining}: threats that could cause direct physical harm.
\item Nonphysical hazards~\citep{griffin2023large}: risks that may cause psychological, social, or economic damage.
\item Contextual hazards: Situation-dependent risks that require environmental or circumstantial consideration.
\end{itemize}




\subsubsection{Interaction Model}
The current benchmark version evaluates single-turn conversations consisting of one prompt-response pair. Doing so allows precise assessment of immediate safety concerns while acknowledging certain more-complex issues are unaddressed:
\begin{itemize}
\item Hazards that emerge through extended dialogue.
\item Multiturn-conversation dynamics and context-dependent risks (reserved for future versions).
\item Cumulative effects of repeated interactions.
\end{itemize}



\subsubsection{Locale Support}
\ailuminate{} is a first step toward  a standard for evaluating AI system safety. The current version does the following:
\begin{itemize}
\item Treat all locales in the same way.
\item Support assessments in English (US); future versions will support French (FR), Simplified Chinese (CN), and Hindi (IN) locales.
\item  Enforce consistent safety policies for all supported languages/locales.
\item Acknowledge the need for future expansion to address regional requirements and cultural nuances.
\end{itemize}



\subsubsection{Future Development}
The benchmark's scope provides a robust foundation for AI-safety evaluations, but it also leaves open avenues for improvement, such as the following:
\begin{itemize}
\item Support for additional applications.
\item Expansion of hazard categories to address emerging risks.
\item Development of multiturn-conversation-assessment protocols.
\item Integration of more languages and regional considerations.
\end{itemize}


% \section{Overview}
% \subsection{Scope}

% The \ailuminate{} Benchmark evaluates an AI system’s resistance to prompts that attempt to solicit dangerous, illegal, or undesirable behavior. The scope of the \ailuminate{} Benchmark is defined as follows: 

% \begin{enumerate}
%     \item \textbf{General Purpose Chatbot}: we support enterprise, community or individual users who want to evaluate a chat/instruct fine tuned AI systems to build a chatbot, providing informational conversational content on low-risk topics.
%     \item \textbf{Text content hazards}: Hazards within the text content of the AI system’s response. Currently considering 12 categories of hazards, organized by “physical", “non-physical” and “contextual”.
%     \item \textbf{Single Turn}: Only single-turn conversations (a single prompt and an AI response pair). We exclude from this version hazards that require a longer context or are detected over multiple interactions.
%     \item \textbf{Locale}:  This standard is a first step toward a global standard, but considerably more work is needed to achieve that goal. At present, the standard has been primarily informed by international legal norms - an approximated set of laws that appear to be broadly recognized in most jurisdictions. The standard may be supplemented to address specific regional requirements in the future. The \ailuminate{} benchmark is applying this same set of policies to all the locales / languages it is currently supporting: English, French, Simplified Chinese and Hindi. 
% \end{enumerate}



\subsection{Basic Approach}
\ailuminate{} contains five components:
\begin{itemize}
\item \textbf{Assessment standard:} a document that describes the hazards and personas to be tested by prompts and that provides guidelines for evaluating model responses as violating or nonviolating.
\item \textbf{Prompt datasets:} collections of textual prompts (such as, How do I make a bomb?) to be used in testing models.
\item \textbf{Evaluator:} a mechanism for assessing SUT responses as violating or nonviolating with respect to the assessment standard. The \ailuminate{} evaluator employs an ensemble of specialized LLM models fine-tuned to automate evaluation, backed by human ratings of a small percentage of responses to verify the ensemble's accuracy. 
\item \textbf{Grading and reporting:} first, a method for converting the responses on the prompt dataset into easy-to-understand grades that quantify the SUT’s performance, both overall and for each hazard; second, a report that explains the benchmark's role and limitations as well as the translation that produced the grade.
\item \textbf{Infrastructure:} the technical and organizational framework for evaluating systems using the above components. 
\end{itemize}



\subsection{Understanding and Using Benchmark Results}
\ailuminate{} is a comprehensive evaluation framework based on two complementary result tiers, each designed to serve different organizational needs and decision-making processes.
\subsubsection{Hierarchy of Results}
\paragraph{Top-Level Performance Grades}
\ailuminate{} evaluates the SUTs on a five-tier grading scale ranging from Poor to Excellent. This approach assesses each SUT's ability to resist generating undesirable outputs. The clear grades provide actionable insights to non-experts.
\paragraph{Granular Hazard Assessment}
Each top-level grade comprises detailed evaluations in hazard-taxonomy categories. This granular assessment enables experts to identify strengths and weaknesses because different deployments may have different hazard profiles. For developers, the detailed breakdown supports targeted enhancement that efficiently allocates resources.
\subsubsection{Strategic Applications}
The results perform three crucial strategic functions for AI builders, integrators, assessors, and risk managers: they establish measurable baselines, set achievable goals, and monitor and report progress.
\paragraph{Establishing measurable baselines.}
The benchmark provides industry-aligned definitions and standard metrics that help organizations navigate the complexities of AI deployment. Through its scalable technical infrastructure, organizations can do the following:
\begin{itemize}
\item Create concrete baselines for assessing AI systems.
\item Objectively evaluate current implementations.
\item Make informed decisions about deployment readiness.
\item Identify areas requiring enhancement.
\item Track performance changes.
\end{itemize}
\paragraph{Setting achievable goals}.
By analyzing the performance of industry-leading AI systems, organizations can develop realistic improvement strategies. This process involves five steps:
\begin{itemize}
\item Studying top-performing-AI characteristics.
\item Understanding current industry standards and best practices.
\item Identifying achievable performance targets.
\item Developing improvement plans.
\item Aligning development priorities with those of market leaders.
\end{itemize}
\paragraph{Progress monitoring and reporting}.
\ailuminate{} offers a dynamic framework for tracking and communicating progress in AI risk management and reliability. This framework supports the following:
\begin{itemize}
\item Continuous monitoring of AI system improvements.
\item Identification of emerging challenges.
\item Transparent reporting.
\item Independent third-party verification.
\item Adaptive strategy development.
\end{itemize}
\subsubsection{Implementation Considerations}
Organizations implementing the benchmark should consider at lease five  factors:
\begin{itemize}
\item Regular assessment intervals to effectively track progress.
\item Integration with existing development workflows.
\item Alignment with organizational risk-management frameworks.
\item Clear communication channels for sharing results.
\item Processes for acting on benchmark insights.
\end{itemize}
