
\begin{figure*}[ht]
    \centering
    \vskip -0.12in
    \begin{minipage}[c]{0.69\linewidth}
        \centering
        \includegraphics[width=\linewidth]{body/figures/fix_weight_only_smoothed_0.9_1.png}
    \end{minipage}
    \hfill
    \begin{minipage}[c]{0.3\linewidth} % Adjust width as needed
        \centering
        \vskip 0.2in
        \begin{tiny}
            \begin{sc}
                \begin{tabular}{cc} % Use c|c for a vertical line
                    \toprule
                    Node ID & WCNTS Ratio \\
                    \midrule
                    1 & 0.037 \\
                    2 & 0.002 \\
                    3 & 1.34$e$-15 \\
                    4 & 0.019 \\
                    5 & 0.011 \\
                    6 & 0.004 \\
                    7 & 0.008 \\
                    8 & 0.006 \\
                    9 & 0.017 \\
                    10 & 0.010 \\
                    11 & 0.051 \\
                    12 & 0.002 \\
                    13 & 0.004 \\
                    14 & 0.037 \\
                    15 & 4.16$e$-4 \\
                    \bottomrule
                \end{tabular}
            \end{sc}
        \end{tiny}

    \end{minipage}
    \vskip -0.1in
    \caption{$L_2$-norm of the gradient difference and the ground-truth gradients over steps. The left table shows Worst Case Noise-to-Signal (WCNTS) ratios for unhealthy nodes.}
    \label{fig:fix_weight_only}
    \vskip -0.1in
\end{figure*}

\section{Experiments for RQ2} \label{sec:sdc_single_optimizer_step}
\underline{\textbf{RQ2}}: \emph{What is the impact of SDCs on the gradients of model weights at a single optimizer step?}

\subsection{Experiment Setups}
\label{methods:differing_gradients}
We train same models on each pair of unhealthy and healthy nodes simultaneously and use the \textbf{parameter synchronization} mechanism discussed in Section \ref{sec:research_questions}. 
After the forward and backward pass are finished  at step $j$, we compute the $L_2$ norm of elementwise difference between the gradients of model weights on the $i$-th unhealthy node $g'_{i,j}$ and the ground-truth gradients on the healthy node $g_{j}$. After taking an optimizer step, we use parameter synchronization to overwrite the model parameters on the unhealthy node. 
We also report the \emph{worst case noise-to-signal (WCNTS) ratio} to measure how significant SDC-induced noise to gradients is:
\begin{equation}
    \label{equation:worst_case_noise_to_signal}
    WCNTS_i = \max_{j}{\frac{\|g'_{i,j} - g_{j} \|_2}{\|g_{j}\|_2}}
\end{equation}
Using the same decoder block architecture as in Section \ref{sec:primitive}, we train a $32$-layer Transformer decoder with hidden state size as $H=4096$. More details can be found in Appendix \ref{appendix:model_training}.

\subsection{Results}
\label{results:model_gradient_norm_diff}

Figure \ref{fig:fix_weight_only} shows the $L_2$ norm of gradient difference and the WCNTS ratio for the gradients over optimizer steps across unhealthy nodes. We observe that gradients computed on unhealthy nodes deviate minimally from those computed on the healthy node. Although the absolute value of $L_2$ norm of the gradient difference is large before the $100$-th step, it is still relatively small compared to the $L_2$ norm of the ground-truth gradients and continues to decrease as the norm of the ground-truth gradients decreases.
In the worst case on Node 11, the $L_2$ norm of gradient difference is $5.1\%$ of that of the ground-truth gradients, showing that the SDC-induced noise in the gradients is relatively small relative to the ground-truth gradients. 

\section{Experiments for RQ3} \label{sec:multiple_training_steps}


\underline{\textbf{RQ3}}: \emph{What is the accumulated impact of SDCs on the model quality over multiple training steps?}

\subsection{Experiment Setups}
To provide a better understanding of how different the learned representations and model decision boundaries from unhealthy nodes would be, we design experiments for both model pre-training from scratch and fine-tuning of a pre-trained model.

\textbf{\myexperimentlabel{experiment:parameter_drift}}: \emph{How different is the learned model under accumulated SDC error from the ground truth during model pre-training?} 
We pretrain same models on each pair of unhealthy node and healthy node simultaneously. We follow the same experiment setting in Section \ref{methods:differing_gradients} except we \emph{do not use parameter synchronization mechanism}. To observe how SDCs impact the learned models during training, we report the training loss and the \emph{parameter difference} which is $L_2$ norm of the element-wise difference between model parameters on healthy and unhealthy node.

\textbf{\myexperimentlabel{experiment:finetuning}}: \emph{For downstream tasks, how would SDCs affect model finetuning?} 
We want to further understand how model quality is affected by SDCs when the model is fine-tuned on a downstream task. 
In this experiment, we fine-tune \verb|Mistral-7B-v0.3| \cite{jiang2023mistral7b} on six multiple-choice question answering tasks (CosmosQA \cite{cosmos-qa}; MathQA \cite{mathqadataset}; ScienceQA \cite{lu2022learn}; OpenbookQA \cite{OpenBookQA2018}; BoolQ \cite{clark2019boolqexploringsurprisingdifficulty}; and RACE \cite{raceLai2017large}) by instruction tuning \cite{weifinetuned}. 
For each task, we use a fixed random seed for shuffling the training dataset and evaluate the test accuracy (TA) of the models fine-tuned on the healthy node and on unhealthy nodes. Using the predictions of the model fine-tuned on the healthy node as the standard, we report the disagreement percentage (DP), which is defined as the percentage of the difference in predictions on the test set. To better understand the prediction difference, we fine-tune a model on healthy node with a different random seed as a baseline to contextualize the effect of SDC error relative to the effect of data ordering. More details can be found in Appendix \ref{appendix:finetuning}.

\subsection{Results}
\label{results:model_quality}


\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{body/figures/no_synchronization_2.png}
    \vskip -0.15in
    \caption{The curves for parameter difference, gradient norms and training loss on unhealthy nodes. Note that the loss curves on all unhealthy nodes are plotted but identical to that on the healthy node.}
    \label{fig:no_synchronization}
    \vskip -0.2in
\end{figure}

\textbf{Results for Experiment \ref{experiment:parameter_drift}.} 
Figure \ref{fig:no_synchronization} shows the parameter difference, gradient norm and training loss on unhealthy nodes during pre-training. Despite training loss on each unhealthy node nearly identical to the healthy node, model weights on unhealthy nodes incrementally drift away from those on the healthy node, suggesting that SDCs are pushing models towards different local minima. 

Note that Node 13 shows no parameter difference before step 450, which indicates that no SDC occurs during this period. It is aligned with the finding in Section \ref{sec:submodule_outputs_results} that SDCs do not occur uniformly during training. After step 450, the model on Node 13 begins to quickly drift away from the ground-truth weights. We further find that the rates at which the parameter differences increase are similar on most unhealthy nodes, although the unhealthy nodes produce SDCs with different degrees of frequency and severity as shown in Section \ref{sec:submodule_outputs_results} This suggests that the rate of parameter drift is more likely to be driven by the sharp loss surface than purely by SDCs. In other words, SDCs serve as a trigger to push the optimization trajectory onto a different path through this sharp loss landscape, leading to the divergence of the parameters. 

\begin{table}[t]

\begin{center}
\begin{tiny}
\begin{sc}
\setlength{\tabcolsep}{3pt}
\begin{tabular}{cccc}
\toprule
\multirow{2}{*}{Configuration} & CosmosQA & MATH & OpenbookQA \\
 & TA (DP) & TA (DP) & TA (DP) \\
\midrule
without fine-tuning & 56.33 (44.80) & 24.02 (82.35) & 74.70 (29.30) \\
\midrule
Healthy Node & 90.79 (-) & 37.22 (-) & 83.80 (-) \\
Healthy Node (Seed=$43$) & 89.50 (6.70) & 38.83 (56.75) & 86.30 (18.70) \\
\midrule
Unhealthy Node 1 & 90.53 (5.15) & 36.78 (42.24) & 85.00 (16.80) \\
Unhealthy Node 2 & 90.79 (0.00) & 37.22 (0.00) & 83.80 (0.00) \\
Unhealthy Node 3 & 90.77 (4.96) & 34.47 (41.84) & 83.40 (16.10) \\
Unhealthy Node 4 & 90.32 (5.59) & 36.42 (37.76) & 84.30 (16.60) \\
Unhealthy Node 5 & 90.79 (0.00) & 37.19 (34.91) & 85.10 (14.10) \\
Unhealthy Node 6 & 0.00 (100.00) & 36.92 (36.82) & 84.70 (16.30) \\
Unhealthy Node 7 & 90.32 (4.99) & 37.22 (0.00) & 83.80 (0.00) \\
Unhealthy Node 8 & 89.84 (6.23) & 38.22 (36.62) & 85.20 (15.50) \\
Unhealthy Node 9 & 89.97 (5.38) & 35.78 (37.49) & 85.00 (17.40) \\
Unhealthy Node 10 & 89.97 (4.93) & 37.05 (37.05) & 84.10 (17.20) \\
Unhealthy Node 11 & 90.61 (4.54) & 36.82 (38.53) & 87.10 (15.60) \\
Unhealthy Node 12 & 90.79 (0.00) & 37.22 (0.00) & 83.80 (0.00) \\
Unhealthy Node 13 & 90.79 (0.00) & 37.22 (0.00) & 83.80 (0.00) \\
Unhealthy Node 14 & 89.74 (6.12) & 38.63 (32.29) & 85.00 (15.90) \\
Unhealthy Node 15 & 90.77 (3.27) & 38.53 (40.87) & 83.80 (0.00) \\
\bottomrule
\end{tabular}


\end{sc}
\end{tiny}
\end{center}
\vskip -0.1in
\caption{Finetuning results for three question answering tasks on different nodes. For each task, we report the test accuracy (TA) and the disagreement percentage (DP).}
\label{tab:finetuning_results_short}

\vskip -0.2in
\end{table}

\textbf{Results for Experiment \ref{experiment:finetuning}.} 
Table \ref{tab:finetuning_results_short} shows the fine-tuning results for three of the question-answering tasks on different nodes. The full results can be found in Appendix \ref{appendix:full_finetuning_results}. We find that the models fine-tuned on most unhealthy nodes are significantly better than the base model without fine-tuning and also have similar performance compared to the models fine-tuned on the healthy node. The disagreement percentage caused by SDCs on unhealthy nodes is not larger than using a different random seed for data shuffling. Aligned with the findings in Experiment \ref{experiment:parameter_drift}, this again affirms that SDCs on unhealthy nodes push the models towards different local minima.

However, SDCs are not necessarily harmless to model fine-tuning. Figure \ref{fig:finetuning_loss_spikes} shows the training loss during fine-tuning on CosmosQA and we find that significant loss spikes can occur on some unhealthy nodes. On Node 4, Node 6 and Node 7, the loss spikes occur in the middle of fine-tuning while later the training is again stabilized, which makes the final models still have benign performance. However, on Node 6, the loss spike occurs near the end of fine-tuning, which leads to the resulting model having zero test accuracy on CosmosQA as shown in Table \ref{tab:finetuning_results_short}. It indicates that the loss spikes caused by SDCs pose a threat to the model quality. We also note that loss spikes do not occur in every fine-tuning task. For example, training loss curves for OpenbookQA on unhealthy nodes are all identical to that on the healthy node, similar to the situation in Experiment \ref{experiment:parameter_drift}. Therefore, we conclude that the impact of SDC on model training is closely related to the loss surface of the training task.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{body/figures/finetune_loss_cosmos_qa.png}
    \vskip -0.15in
    \caption{Training loss curves on different nodes during fine-tuning on CosmosQA dataset. The loss curves for other unhealthy nodes that are identical to the healthy node are not shown in this figure.}
    \label{fig:finetuning_loss_spikes}
    \vskip -0.2in
\end{figure}





