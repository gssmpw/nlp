\section{Discussion}

\subsection{Silent Nature of SDCs in LLM Training}

Our results show that SDCs can silently occur without any clear indication from training loss. For example, we find from Section \ref{sec:submodule_outputs_results} and Section \ref{results:model_gradient_norm_diff} that SDCs on Node 10 consistently perturb the submodule computation and the gradients, but the training loss on Node 10 is still identical to that the on healthy node without any loss spike during both pre-training and fine-tuning in Section \ref{results:model_quality}. It indicates that before a loss spike appears, SDCs may have already affected model training for an unknown period.


\subsection{Mitigating the Impact of SDCs}
Given its silent nature, it is important to mitigate the impact of SDC on LLM training. One direction is timely SDC detection. We examine if algorithm-based-fault tolerance (ABFT), specifically check-summed matrix multiplication \cite{utexas_abft}, could be used at the submodule level to detect SDCs, with the full experiment details in Appendix \ref{appendix:abft}.  We find that ABFT fails to reliably detect SDCs with high precision and recall: therefore, additional recomputation may be the best solution to reliably detect SDCs in practice. One possible approach is using an additional shadow data-parallel replica during training. At each training step, the shadow replica chooses a target data-parallel rank whose inputs are used for its computation. The gradients from the shadow data-parallel replica and the target data-parallel rank can be compared during gradient all-reduce over data-parallel ranks to identify if an SDC has occurred. This idea is aligned with the SDC scanners on hot standbys used in training Gemini \cite{geminiteam2024geminifamilyhighlycapable}. 

Another direction is to mitigate the impact of SDCs on the training trajectory and the model quality when SDCs are not detected timely. As discussed in Section \ref{results:model_quality}, the impact of SDCs on model quality is closely related to the sharpness of the loss surface. Therefore, one future direction is to conduct a deeper analysis of the connections between loss spikes and SDC. Future work could examine whether methods that reduce the sharpness of the loss surface or avoid optimization towards sharp regions \cite{lee2023explicitcurvatureregularizationdeep, bahri2022sharpnessawareminimizationimproveslanguage} can help reduce the divergence of model parameters and loss spikes caused by SDCs.




\section{Conclusion}
In this work, we propose a study setting that enables us to thoroughly investigate the effects of real-world SDC on LLM training. Pairing healthy and unhealthy nodes together using our synchronization mechanisms, we isolate and examine the impacts of SDCs at different levels. 
We show that although in most cases the perturbations from SDCs on submodule computation and gradients are relatively small, SDCs can make models converge to different optima with different weights. We further show that SDCs can be evasive if we only monitor the training loss while in the extreme case, they can cause training loss spikes and fully corrupt the model. Our study reveals that the impact of SDC varies on different unhealthy nodes and is closely related to the loss surface of the training task.
Our work further provides concrete insights for improving SDC detection and mitigating the impact of SDC in the future.