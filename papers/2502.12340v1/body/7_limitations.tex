\section{Limitations}
We note a few limitations in this work. First, the number of unhealthy nodes we could access for our experiments was restricted because unhealthy nodes are rare due to rigorous manufacturer testing and filtering of hardware components before reaching the data-center.
The unhealthy nodes retained after detection by the production fleet management flow and used in our study \emph{were only temporarily held for the purpose of our study}. They are subsequently being repaired and returned to the production pool.

Second, our study focuses on training with tensor parallelism only on a single node. Since each node exhibits different degrees of SDC, we could not run large-scale training with more complex parallelism strategies for every unhealthy node as we did in the tensor parallel setting due to resources and limited time. However, our experimental setting is still meaningful for understanding SDCs in LLM training. For large-scale LLM training, tensor parallelism is commonly adopted with the degree set to be the number of accelerators in a single node \cite{narayanan2021efficient}, which makes our setting highly relevant. Furthermore, in our work, each modelâ€™s training is fully computed on an unhealthy node, which makes the effects of SDCs more visible.

Third, our work shows a necessary trade-off between fine-grained analysis and SDC occurrence. Our synchronization mechanisms inevitably introduce additional overhead and reduce accelerator utilization, which can lead to a decrease in the frequency of SDCs. As shown in Figure \ref{fig:attention_forward_mismatch_frequency_over_time} where we use computation synchronization at each submodule computation, Node 14 exhibits SDCs very infrequently over 4000 optimizer steps. By contrast, in Figure \ref{fig:fix_weight_only} where we use parameter synchronization at each step, the same node shows mismatched gradient norms at every step. The difference in SDC occurrence on the same node suggests more synchronization will decrease the accelerator utilization and further reduce the frequency of SDC. 
However, this trade-off is necessary to analyze SDCs at a finer granularity apart from high-level metrics like loss curves or gradient norms. Our synchronization mechanism allows us to characterize the impact of SDC at different granularities because we need to prevent the SDC error from being propagated to the next measurement. We will study how to mitigate the impact of this trade-off and propose more effective methods to analyze SDCs in the future.

Finally, we only observe the loss spikes in some fine-tuning experiments but not in our pre-training experiments. However, loss spikes can occur in practice during pre-training  \cite{chowdhery2023palm}. This discrepancy might be because the number of optimizer steps is not large enough to enter into a region where SDCs cause loss spikes or because the size of our model is not large enough. We also note that it can be challenging to reproduce and investigate the loss spikes caused by SDCs due to the randomness of SDCs. As observed in our work, if we monitor intermediate computation tensors during training, it decreases accelerator utilization, which affects the frequency of SDC and potentially prevents the occurrence of loss spikes. Future work can continue pre-training for a longer period on unhealthy nodes, potentially in a multi-node setting, to characterize the relation of loss spikes or NaN issues with SDC.

\section*{Acknowledgments}
We thank Thomas Fussell, Catalin Gabriel Manciu, Alexander Zhipa, Tushar Sharma, Manish Reddy, Stan Ivashkevich, Mohammad El-Shabani, Dave Goodell and Ron Diamant for providing advice.
