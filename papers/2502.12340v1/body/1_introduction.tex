\section{Introduction}

Large language models (LLMs) have demonstrated remarkable advancements in different tasks. To further explore the potentials of LLMs, recent efforts have been put into scaling up the model size to hundred-billions or even trillions of parameters. As a result, training such large models requires extensive computational resources over a long training period. For example, Llama 3 405B was trained on 16K H100 GPUs \cite{dubey2024llama3herdmodels}.

However, as training scale increases, the likelihood of hardware failures also increases. Silent data corruption (SDC) is an emerging error that causes impacted hardware to inadvertently output wrong calculation results silently without any user indication \cite{dixit2021silentdatacorruptionsscale}. Meta reported 6 unplanned job interruptions were attributed to SDC during a 54-day pre-training snapshot \cite{dubey2024llama3herdmodels} and Google estimated an SDC event occurs every week or two during Gemini training \cite{geminiteam2024geminifamilyhighlycapable}. 
In practice, SDCs observed during large-scale training usually result from latent hardware defects that cause corruption only under certain conditions or after sufficient stress over the hardware's lifetime. Once a machine begins to be affected by SDCs, it pollutes training outputs and can impact the model optimization trajectory \cite{hepermanenthardwarefailures}.
Although many works studied the effect of SDCs in large-scale CPU systems \cite{dixit2021silentdatacorruptionsscale, wang2023understandingsdc}, autonomous systems \cite{wan2022analyzing, yushunsdcrobotics2024} and deep learning accelerators \cite{zhang2018analyzing, liunderstandingerrorpropagation, rech2022reliability}, no public work has characterized the impact of real-world SDCs on LLM training in detail. 

In this work, we are the first to investigate the impact of real-world SDCs on LLM training. 
We work with a cloud-computing platform to gain access to unhealthy nodes that failed production fleet management testing due to SDCs.
While unhealthy hardware is generally excluded from production workloads, latent defects and hardware failures can turn previously healthy hardware unhealthy, emphasizing the importance of our investigation.

By leveraging deterministic execution from the XLA compiler and adopting the same training setup, we can compare the results from unhealthy nodes and healthy nodes to characterize the impact of SDCs during training.
We break down our investigation into three levels: (1) the impact on submodule computation; (2) the impact on the gradients of model weights at a single optimizer step; and (3) the impact on the model quality over a training period. Since SDC error can accumulate, we isolate the impact of SDCs at different levels by overwriting the intermediate results computed on the unhealthy node with results from the healthy node. Specifically, we design a computation synchronization mechanism to ensure the input of every submodule is the same on healthy nodes and on unhealthy nodes for (1), and a parameter synchronization mechanism to ensure the model weights are the same before each optimizer step for (2).

We conduct quantitative comparisons with the computations on healthy node at different levels. To investigate the impact of SDCs on submodule computation, we check the forward and backward computation of the self-attention and feed-forward network (FFN) across unhealthy nodes. To investigate the impact at each optimizer step, we examine the difference in gradients. To investigate the accumulated impact on the model quality, we track the loss and parameter difference during pretraining and also examine the finetuning performance on unhealthy nodes. 

Our empirical results show that SDCs do not occur uniformly during training and exhibit different patterns on different unhealthy nodes. We find that SDCs can cause certain values in the submodule computation results to differ by large factors, while the average mismatch frequency is generally low. Furthermore, the noise to gradients caused by SDC error within an optimizer step is small relative to the true gradient norm. For the accumulated impact over training steps, although the pretraining loss remains similar, SDCs can cause model parameters to drift away from ground-truth weights, which indicates that models on different nodes converge to different optima. Meanwhile, although the models fine-tuned on most unhealthy nodes have similar performance compared to the models fine-tuned on the healthy node, loss spikes do occur during fine-tuning on some unhealthy nodes, which can fully corrupt the model weights in some cases.

In summary, our contributions are:
\begin{itemize}
    \item We are the first to investigate the impact of read-world SDCs on LLM training in detail by obtaining access to realistic unhealthy nodes flagged by the production fleet management.
    \item By pairing unhealthy nodes with healthy nodes and introducing synchronization mechanisms, we design experiments to precisely isolate the impact of SDCs at different levels.
    \item We reveal the characteristics of SDCs at various levels of model training empirically and further provide insightful analysis which sheds light on the future work on understanding and mitigating the impact of SDCs.
\end{itemize}

 

