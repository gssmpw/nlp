

\section{Additional Background on SDCs}
\label{appendix:sdc_related_work}

We further discuss background and prior work on detecting and correcting silent data corruption (SDC). Generally, SDCs cannot be guarded against effectively by common mechanisms like ECC memory \cite{assetintertechSilentData} as they arise during computation. Therefore, several prior works have investigated SDC detection and correction.

\textbf{Simulating SDC Faults.} 
Some prior works focus on understanding how simulated SDC-like faults propagate through training and inference in deep learning systems. For example, \citet{heunderstandingandmitigatinghardwarefailures2023} simulate hardware faults during training of several different deep learning architectures, characterize the error modes that result from simulated faults and error propagating through model training, and understand the conditions in which these faults must occur to destabilize training. However, relative to modern LLMs, the model sizes in their experiments are small and the training workloads are different compared with LLMs. Although the simulation is comprehensive, it is unknown how real-world SDC would affect LLM training in practice. In contrast, our work uses the real-world unhealthy hardware to train Transformer models with billions of parameters to understand how real-world SDCs behave in LLM training.

\textbf{Detection via Aggregate Statistics.} Some prior works focus on SDC detection primarily through only monitoring aggregate training statistics. For example, \citet{hepermanenthardwarefailures} examines monitoring Inf/NaN results and loss spikes during training to identify when SDCs occur. Likewise, \citet{madrdna2024} detect SDCs with high precision using error signatures derived from analyzing the distribution of model neuron activations.

\textbf{Soft Redundancy and Protecting Inference Computation.} However, since SDCs can also occur silently without impacting aggregate quantities like loss or gradient norm, other works add minor levels of redundancy to detect SDCs with higher precision. For example, algorithm based fault tolerance (ABFT) approaches compute low overhead checksum computations alongside the original operation to check against \cite{wupanruosdcresilientabftmatmul, zhaiftblas}.
Most prior work using ABFT examines using detection in safety-critical applications \cite{Kosaian_2021} and specifically to protect deep learning inference computations\cite{Zhao_2021}. More recent work examined using this to protect the inference of vision transformers \cite{xue2023approxabftapproximatealgorithmbasedfault}.


\textbf{Detection with Exact Recomputation.} Finally, some works fully recompute values to check for SDC occurrence during training. For example, Gemini uses additional hardware to scan for SDCs and isolates incorrect computations by deterministically replaying computations \cite {geminiteam2024geminifamilyhighlycapable}.

\section{Model Pretraining Details} \label{appendix:model_training}
\subsection{Dataset and Preprocessing}

% Sections 

In this section, we describe the dataset preprocessing done during the training experiments described in Sections \ref{sec:primitive}, \ref{sec:sdc_single_optimizer_step}, \ref{sec:multiple_training_steps} and Appendix Section \ref{appendix:abft}. We train on the \verb|20220301.en| split of the Wikipedia dataset \cite{wikidump} with a sequence length of $L=4096$ tokens. Using the full \verb|20220301.en| split, we first tokenize the dataset using the pre-trained Byte-Pair Encoding (BPE) Llama-3 tokenizer \cite{dubey2024llama3herdmodels} . We then concatenate the entire token sequence and chunk the dataset into chunks of sequence length 4096 to maximize context and accelerator utilization during training. The entire epoch of sequences is then shuffled with a fixed random seed by sequence then saved to disk. At training time, using \verb|torch.distributed|, each tensor parallel rank ingests the same subset of data at the same time using a distributed dataloader via PyTorch/XLA's parallel dataloader\footnote{\url{https://github.com/pytorch/xla/blob/master/torch_xla/distributed/parallel_loader.py}}, which provides buffering to hide CPU to device data load latency.

\subsection{Model Architecture}

We use a Llama3-8B style model architecture \cite{dubey2024llama3herdmodels} trained from Kaiming and Xavier uniform initialization on the weights and biases, where each decoder layer has 32 self-attention heads with group query attention (GQA) over 8 Key-Value heads and an feed-forward network using SwiGLU. All models are trained using tensor parallelism and ZeRO-1 with sequence parallelism \cite{rajbhandari2020zeromemoryoptimizationstraining, korthikanti2022reducingactivationrecomputationlarge} with the XLA backend corresponding to the respective healthy-unhealthy node pair.

In this section, we describe the model architecture trained in the experiments described in Sections \ref{sec:primitive}, \ref{sec:sdc_single_optimizer_step}, \ref{sec:multiple_training_steps} and Appendix Section \ref{appendix:abft}. In the experiments described in Sections \ref{sec:primitive} and Appendix Section \ref{appendix:abft}, the model trained contains only 16 decoder block layers (and half the number of parameters as the Llama3-8B configuration), while in Sections \ref{sec:sdc_single_optimizer_step} and \ref{sec:multiple_training_steps}, the model trained contains 32 decoder block layers and is equivalent to the Llama3-8B model configuration (with number of parameters). The details of each decoder block are given below, with sequence length $n=4096$ and token dimension $d=4096$:
\begin{enumerate}
    \item Given an input of token embeddings $X \in \mathbb{R}_{n \times d}$ sharded by sequence length (dimension $n$), we perform an all-gather such that each TP rank has the entire input $X$ and perform Group Query Attention (GQA)  \cite{ainslie2023gqatraininggeneralizedmultiquery} with 32 heads, head dimension of $d / 32 = 128$, and 8 key-value heads.
    We do not use FlashAttention \cite{dao2022flashattentionfastmemoryefficientexact} for our model architecture, instead using a standard GQA implementation.
    \item After the concatenation of head results and subsequent output row-parallel linear projection, the results are reduce-scattered to sequence parallelism, such that each TP rank has an equal split of tokens. We add a residual connection (add the original token embeddings) followed by a Layer Normalization.
    \item We then enter the FFN primitive by all-gathering, such that each TP rank has the entire new input. We perform an standard FFN with Swish-Gated Linear Unit (SwiGLU) activation \cite{shazeer2020gluvariantsimprovetransformer}, projecting to an intermediate dimension of 16384 (4x), performing SwiGLU, then projecting back down into $d=4096$. We perform the FFN with gradient checkpointing \cite{chen2016trainingdeepnetssublinear} on the intermediate dimension to save memory and avoid needing to persist forward matrices of size $4096 \times 16384$ to HBM for the backwards pass.
\end{enumerate}

\subsection{Model Hyperparameters}

Our model training hyperparameters are given below. For the primitive investigation in Section \ref{sec:primitive} and Appendix Section \ref{appendix:abft}, we train at a global batch size of 16, due to increased throughput from cross-node communication, while for the single and multiple optimizer step settings in Sections \ref{sec:sdc_single_optimizer_step} and \ref{sec:multiple_training_steps}, we train at a global batch size of 256.
\begin{itemize}
    \item Sequence length: 4096
    \item Embedding dimension: 4096
    \item Sharding strategy: ZeRO-1 with sequence parallelism
    \item Optimizer: Adam with $\beta_1 = 0.9$ and $\beta_2$ = 0.99
    \item Weight decay ($L_2$ regularization): 0.01
    \item Learning rate (LR) schedule: linear aramup, cosine annealing
    \item Total Steps (for LR schedule): 100,000
    \item Warmup Steps (for LR schedule): 2,000
    \item Training precision: \verb|bfloat16|
    \item Mixed precision: False
    \item Micro-batch size (for gradient accum.): 1
    \item Gradient norm clipping with max norm 1.0
    \item Rounding mode: Round to Nearest
\end{itemize}

Specifically for Sections \ref{sec:primitive} (submodule investigation), we train $M=4500$ optimizer steps using mixed-precision Adam \cite{micikevicius2018mixedprecisiontraining} and a global batch size of $B=16$. Note that we use a smaller batch size and number of decoder layers than those in later experiments because computation synchronization brings additional cross-node communication and comparison, which greatly decreases training throughput and increases required memory usage.

Specifically for Sections \ref{sec:sdc_single_optimizer_step}, \ref{sec:multiple_training_steps} (gradient and synchronization-free setting), we train $M=2500$ optimizer steps using mixed-precision Adam \cite{micikevicius2018mixedprecisiontraining}.

\section{Submodule Investigation Implementation Details} \label{appendix:primivite_investigation}

\subsection{Submodule Investigation Integration with TorchXLA and Autograd}

To integrate the lock-step communication mesh used in the primitive investigation in Section \ref{sec:primitive} into the forwards and backwards computations of a LLM training run, we developed a set of `torch.autograd.Function` implementations, which implement the communication mesh as forwards or backwards behavior. We can then insert and call these functions at the locations in which we'd like to investigate the transformer primitive outputs. When \verb|ComparisonForwardAutograd| is called on a tensor, it does no checks in the backwards pass but checks and outputs the computes SDC infrequency and severity for that tensor in the forward pass. When \verb|ComparisonBackwardAutograd| is called on a tensor, it does no checks in the forwards pass but checks and outputs the computed SDC infrequency and severity for the gradient of the activation corresponding to that tensor (i.e. the tensor propogated backwards at that location in the backwards pass). For implementations, see Figure \ref{fig:auto_grad_implementation} below.

As in Figure \ref{fig:primitive_investigation_appendix}, we insert \verb|ComparisonFwdAutograd| calls at any of the red arrow locations, as we want to compare the computed forward tensor values for each corresponding set of TP ranks between healthy and unhealthy hosts prior to a reduce-scatter. Likewise, to check the values prior to the reduce scatter after backwards primitives, we insert calls to \verb|ComparisonBwdAutograd| at the locations of the blue arrows, so that autograd will communicate and compare the backward pass input-gradients and return mismatch statistics.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{body/figures/primitive_investigation.png}
    \vskip -0.05in
    \caption{Illustration of a transformer decoder block under our ``lockstep parallelism" in the primitive investigation setting, where the arrows indicate the intermediate tensors from the unhealthy corrected by corresponding tensors from the healthy host (red in forwards pass, blue in backwards pass) and where the PyTorch autograd functions in Figure \ref{fig:auto_grad_implementation} are inserted in implementation. Note that in the forward pass $g$ is an all-gather and $\bar{g}$ is a reduce-scatter, while in the backwards pass $g$ is an reduce-scatter and $\bar{g}$ is an all-gather.}
    \vskip -0.15in
    \label{fig:primitive_investigation_appendix}
\end{figure}

This implementation allows the PyTorch autograd engine to automatically handle our primitive investigation communication mesh as it computes forwards and backwards passes for the model.

\begin{figure*}[htbp]
    \begin{longfbox}[title=Autograd Implementation]{}
    \begin{small}
    \begin{verbatim}
@torch.no_grad()
def check_across_data_parallel_ranks(
  tensor_to_check: torch.tensor,
  check_args: CheckConfig,
  layer_count: int,
  tag: Optional[str] = None,
) -> Tuple[torch.Tensor, Optional[Dict[str, Any]]]:
  # For a given tensor:
  # (1) gathers the copy of the tensor on both the healthy and unhealthy host.
  # (2) computes the difference between them and calculates statistics on 
  #     infrequency/severity of mismatching values.
  # (3) returns the correctly computed version of the tensor (from the healthy 
  #     host) and computed error stats.
  ...
  return new_tensor, error_statistics

class ComparisonFwdAutograd(torch.autograd.Function):
  @staticmethod
  def forward(
    ctx, input_tensor, forward_args: CheckConfig, layer_count: int, tag: str
  ):
    # Check mismatching values and return error statistics in fwd pass.
    input_tensor, forward_error_statistics = check_across_data_parallel_ranks(
        input_tensor, forward_args, layer_count + 1, tag=tag
    )
    return input_tensor, forward_error_statistics

  @staticmethod
  def backward(ctx, grad_tensor, _unused_error_dict):
    # Do nothing on the bwd pass.
    return grad_tensor, None, None, None

class ComparisonBwdAutograd(torch.autograd.Function):
  @staticmethod
  def forward(
    ctx,
    input_tensor,
    backward_args: CheckConfig,
    layer_count: int,
    backwards_error_statistics: ErrorDict,
    prefix: str,
  ):
    # Do nothing in the fwd pass.
    ...
    return input_tensor

  @staticmethod
  def backward(ctx, grad_tensor):
    # Check mismatching values and return error statistics in bwd pass.
    new_grad_tensor, backwards_error_statistics = check_across_data_parallel_ranks(
        grad_tensor, ctx.backward_args, ctx.layer_count, tag=ctx.prefix
   )
   backwards_error_statistics = backwards_error_statistics.add_prefix(ctx.prefix)
   ctx.backwards_error_statistics.add_inplace(backwards_error_statistics)
   return new_grad_tensor, None, None, None, None
\end{verbatim}
    \end{small}
    \end{longfbox}
    \caption{Abbreviated implementation of helper autograd functions, which are inserted into transformer primitive locations prior to the reduce-scatter to analyze tensors of interest. The autograd engine then handles the primitive investigation communication as we compute forward and backwards passes.}
    \label{fig:auto_grad_implementation}
\end{figure*}


\subsection{Detailed Primitive Investigation Results} \label{appendix:primitive_investigation_detailed_results}

We generally do not observe any per decoder layer trends in our results and provide detailed breakdowns of mismatch frequency and severity below.

\subsubsection{Frequency of Mismatching Elements}
Detailed per node and per decoder results on the frequency of mismatching tensor elements in the forward and backwards passes are shown in Tables \ref{fig:forward_primitive_frequency} and \ref{fig:backwards_primitive_frequency}, respectively.

\subsubsection{Severity of Mismatching Elements}
Detailed per node results on the average maximum severity of mismatching tensor elements in the forward and backwards passes are shown in Tables \ref{fig:forward_primitive_max_severity} and \ref{fig:backwards_primitive_max_severity}, respectively.

\section{Algorithm Based Fault Tolerance (ABFT) Discussion}  \label{appendix:abft}

\subsection{Experiment Details}

\textbf{Experiment.} \emph{Can algorithm-based fault tolerance (ABFT) detect the errors in Transformer submodule outputs?}
Algorithm-based fault tolerance (ABFT) is one common scheme to detect SDC errors during deep learning computations \cite{Zhao_2021, xue2023approxabftapproximatealgorithmbasedfault}. Specifically, \citet{utexas_abft} propose adding a checksum to matrix multiplication to flag if SDCs have occurred during computation, comparing the results of two data paths, the matrix multiplication result and the checksum element. For a floating-point matrices $A \in \mathbb{R}^{m \times k}, B \in \mathbb{R}^{k\times n}$, $C = AB$ and a column vector of ones $w$, ABFT reports an SDC if:
\begin{equation}
    \label{equation:abft_condition}
    \| C w - A( Bw) \|_\infty > \tau  \| A \|_\infty \| B \|_\infty
\end{equation}
where $\tau = ku$ and $u$ is the unit-roundoff determined by the FP precision used in the matrix multiplication. 

In this experiment, we added ABFT into the computation of all linear layers in the forward and backwards pass of model training. For each kind of matrix multiplication, we record the frequency of ABFT-flags across TP ranks and layers using the condition in \ref{equation:abft_condition}. We train the Transformer model with $D=8$ decoder layers under the precision of \verb|float32|, with rest of hyperparameters the same as detailed in Appendix \ref{appendix:model_training}. Note that underlying theoretical assumptions mean that this ABFT method cannot be applied in lower precision data types like \verb|bf16|. More discussion and details can be found in below in Appendix Section \ref{appendix:abft_bf16}. 

\subsection{Experiment Results}

We find ABFT fails to flag any error on most of the unhealthy nodes, except Node 14. In Figure \ref{fig:abft_example}, we observe ABFT only frequently flags SDCs on Node 14 while flagging little to no errors on other unhealthy nodes. We also note that the rates of SDCs flagged in this \verb|float32| ABFT setting do not necessarily correspond to submodule output mismatches at \verb|bfloat16| induced by SDCs in the Section \ref{sec:submodule_outputs_results} results. As from Figure \ref{fig:attention_forward_mismatch_frequency_over_time}, SDCs on Node 14 occur as a singular spike of mismatches at \verb|bfloat16| as observed in Section \ref{sec:submodule_outputs_results}, which conflicts with how often ABFT flags SDCs on the same node at \verb|float32|.

\begin{figure}[t]
    \vskip -0.1in
    \centering
    \includegraphics[width=0.85\linewidth]{body/figures/abft_example.png}
    \vskip -0.1in
    \caption{Frequency of ABFT flaging an SDC in the forward computation of the output projection in the attention module during training.}
    \label{fig:abft_example}
    \vskip -0.1in
\end{figure}

\subsection{Possible Explainations for ABFT Detection Failure} \label{appendix:submodule_vs_abft}

There are several possible reasons for why ABFT cannot reliably detect SDCs in our experiments. First, due to the low mismatch frequency, the overall impact on matrix norm could be smaller than floating point error bounds, which breaks the assumption of ABFT. Second, introducing ABFT changes the executed workload and decreases compute utilization in our implementation, which might also decrease the rate of SDC occurrence. Finally, SDCs may occur outside the ABFT-protected matrix multiplication. More detail on each is provided below:
\begin{itemize}
    \item \emph{Low frequency and severity of SDC-induced tensor mismatches:} As noted in the submodule results in Section \ref{sec:submodule_outputs_results}, the mismatch severity and frequency of silent data corruption errors results in low impact on matrix norms, which ABFT relies on to avoid flagging false positives. Assuming the SDC arises during matrix multiplication, the observed SDC occurrence is well within floating point error bounds and thus not flagged by ABFT methods. For example, for Node 10, we observe that 4.78$e$-3 of the forward attention output elements are perturbed by an worst-case factor of 1120 times. Using Eq. \ref{equation:abft_condition}, we see that, in the worst case, when all of these errors lie on a single row, one of the row-sum values in $Cw$ changes by an estimated factor of $4.78$e$-3 \times 1120 = 5.35$. Likewise, the LHS quantity $\|Cw - A(Bw)\|_\infty$ would be impacted by roughly the same estimated factor, which is less than an order of magnitude increase from the previous value and unlikely to cause a failure against the RHS threshold in Eq. \ref{equation:abft_condition}.
    \item \emph{ABFT overhead decreases accelerator utilization}: We observe that frequency of SDC occurrence is dependent on accelerator utilization adding ABFT overhead decreases utilization, which might decrease the rate of SDC occurrence. In Section \ref{sec:submodule_outputs_results}, our results on the frequency of SDC-induced mismatches show that SDC occurrence potentially is a function of system-level metrics like power draw and overall system-level utilization. ABFT decreases accelerator utilization and potentially changes this system-level profile, reducing SDC occurrence.
    \item \emph{SDCs occurring outside the matrix multiplication:} We hypothesized that checksummed matrix multiplication would be able to flag SDC errors due to matrix multiplications generally being the most compute-intensive and  highest utilization stage of a deep learning model. However, ABFT flagging no SDCs on known unhealthy hosts may suggest that, preconditioned on existing manufacturer testing and vetting, SDCs might more commonly arise outside of matrix multiplication.
\end{itemize}

\subsection{ABFT and Reduced Precision Datatypes} \label{appendix:abft_bf16}

We run our ABFT detection at \verb|float32| to respect the assumptions to derive the threshold limits for ABFT. Despite lower precision datatypes like \verb|bf16| and even \verb|fp8| are commonly used in LLM training \cite{lee2024fp8againquantifyingeffects}, the error bounds for ABFT are derived from IEEE-754 floating point (\verb|fp32| or higher precision) error bounds for matrix multiplication \cite{matrixcopmutations2013}. These bounds are derived under strict assumptions on the number of mantissa bits (and size of unit-roundoff), which do not hold for \verb|fp16| or \verb|bf16| datatypes. In the ABFT experiment, we ran our ABFT detection at \verb|float32|: when running the check as-is at \verb|bf16|, we observed several false positives on healthy nodes, confirming this conjecture. 

We elaborate on reasons why algorithm-based fault tolerance (ABFT), specifically checksummed matrix multiplication, required additional consideration to be deployed in real-world LLM training settings. Specifically, we noted that checksummed matrix multiplication of floating point matricies was based in bound on floating point error, which make several assumptions on the unit-roundoff or machine epsilon of computation data types.

Recall that \citet{panruowu_online_soft_error_correction} and \citet{utexas_abft} proposed adding a checksum row and column to two-sided matrix multiplication for online floating-point aware detection of silent data corruption errors in matrix multiplication. For some matrix product $C = A \times B$, column vector $w$, we say an checksum error (and SDC) has occurred if:
\begin{equation} \label{equation:abft_condition2}
    \| C w - A( Bw) \| > \tau  \| A \|_\infty \| B \|_\infty
\end{equation}
where $\tau = ku$, where $k$ is the contraction dimension of the matrix multiplication and $u$ is the unit-roundoff of the datatype precision used in the matrix multiplication. We can determine $u$ in PyTorch by using the function \verb|torch.finfo(<dtype>).eps|. The above threshold is derived under the assumptions of Equation 2.7.11 from \citet{matrixcopmutations2013}, a bound for round-off error in dot products, restated below for convenience.

\emph{Equation 2.7.11}: If $n\mathbf{u} \leq 0.01$ where $n$ is the size of the dot-product shared dimension, $\mathbf{u}$ is the unit round-off of the datatype used for computation, and $fl(x^T y)$ then
\begin{gather*}
    |fl(x^Ty) - x^t y| \leq 1.01 n \mathbf{u} |x|^T|y|
\end{gather*}
For types like \verb|float32| or \verb|float64| (and their corresponding \textbf{u} values) the assumption $n\mathbf{u} \leq 0.01$ is very reasonable. In our model configuration case, for a model with embedding dimension $4096$:
\begin{enumerate}
    \item \verb|float32|: $\mathbf{u}= 1.19215e-07, n\mathbf{u} \approx 0.0005 < 0.01$
    \item \verb|float16|: $\mathbf{u}= 0.0009765625, n\mathbf{u} \approx 4 \nless 0.01$
    \item \verb|bfloat16|: $\mathbf{u}= 0.0078125, n\mathbf{u} \approx 32 \nless 0.01$
\end{enumerate}
We see that for types less precise than \verb|float16|, ABFT error bound in Equation \ref{equation:abft_condition2} no longer always holds true and checksummed matrix multiplication methods would possibly raise false positives. We observed this empirically to be true, where the above threshold at \verb|bfloat16| raised false positive flags on healthy nodes that passed production margin tests, while raising no errors on the same node at \verb|float32|. Thus, more work in error analysis is needed to extend ABFT methods properly to low precision datatypes.

\section{Finetuning Experiment Details} \label{appendix:finetuning}

\subsection{Dataset Details}
Details and a brief description of each dataset are provided below:
\begin{enumerate}
    \item ScienceQA \cite{lu2022learn} is a multiple-choice question answering (QA) dataset of grade school science and humanities questions: note that we remove all questions with an image context.
    \item BoolQ \cite{clark2019boolqexploringsurprisingdifficulty} is a QA dataset for naturally occuring yes/no questions each with a page of contextual and relevant information.
    \item OpenbookQA \cite{OpenBookQA2018} is a dataset containing multiple-choice questions each with context modeled like an open-book exam, requiring multi-step reasoning, use of additional common and commonsense knowledge, and rich text comprehension. 
    \item MathQA \cite{mathqadataset} is a datset of English multiple-choice math word problems covering multiple math domain categories.
    \item RACE \cite{raceLai2017large} is a large-scale reading comprehension multiple-choice question answering dataset, collected from English examinations in China, which are designed for middle school and high school students. 
\end{enumerate}

\subsection{Finetuning Prompts}
In Figure \ref{fig:finetuning_prompt_structure}, we show our prompts for finetuning, which are structrued in the following general form. During training we include the full context including the correct answer, while during evaluation we remove everything after \verb|### CORRECT ANSWER| and ask the model to continue generating the answer.
\begin{figure}[htbp]
    \begin{longfbox}[title=Finetuning Prompt Structure]{}
    \begin{small}
    \begin{verbatim}
### QUESTION
{question}

### CONTEXT
{context}

### CHOICES
A: {choice1}
B: {choice2}
...

### CORRECT ANSWER
A: {choice 1}
\end{verbatim}
    \end{small}
    \end{longfbox}
    \caption{General finetuning prompt structure. }
    \label{fig:finetuning_prompt_structure}
\end{figure}


\subsection{Masking and Padding Details}

For finetuning, we right-pad sequences to a fixed sequence length of 2048 tokens (with the exception of 4096 for the RACE dataset) using the Mistral tokenizer \verb|EOS| token. Furthermore, we mask out padding tokens during training so that they do not contribute to loss and gradient computation by setting the \verb|labels| for padding token positions to \verb|-100|, so that they are ignored by PyTorch cross-entropy loss calculation \footnote{\url{https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html}}.
\subsection{Hyperparameters and Finetuning Configuration}

We use the HuggingFace \verb|optimum| package to finetune the \verb|mistralai/Mistral-7B-v0.3| model on our experiment nodes using tensor parallelism and ZeRO-1 optimizer with sequence parallelism. For finetuning, we use the following hyperparameters.

\begin{itemize}
    \item Sequence length: 4096 for RACE, 2048 otherwise
    \item Sharding strategy: ZeRO-1 with sequence parallelism
    \item Optimizer: Adam with $\beta_1 = 0.9$ and $\beta_2 = 0.999$
    \item Weight decay ($L_2$ regularization): 0.001
    \item Learning rate schedule: constant with linear warmup.
    \item Warmup Steps (for computing cosine LR scheduler): 5\% of total steps
    \item Training precision: \verb|bfloat16|
    \item Global batch size: 32
    \item Micro-batch size (for gradient accum.): 1
    \item Gradient norm clipping with max norm 0.3
    \item Rounding mode: Round to Nearest
\end{itemize}

For learning rates, we chose the following values, noted alongside their training dataset size. These were tuned such that the finetuning on the train split improves the model on evaluation set performance on a healthy, non-SDC producing node.
\begin{table}[h]

\caption{Finetuning learning rate (LR) and dataset size for each dataset used in Experiment \ref{experiment:finetuning}.}
\vskip -0.15in

\begin{center}
\begin{small}
\begin{sc}
\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{1.2} 
\begin{tabular}{ccc}
\hline
\toprule
Dataset & LR & Dataset Size (\# train seqs.) \\
\midrule
BoolQ & 5$e$-6 & 9,430 \\
CosmosQA & 5$e$-6 & 25,260 \\
MathQA & 1$e$-5 & 29,837 \\
OpenbookQA & 2$e$-6 & 11,920 \\
ScienceQA & 1$e$-6 & 12,700 \\
RACE & 1$e$-5 & 87,900 \\
\bottomrule
\end{tabular}

\end{sc}
\end{small}
\end{center}


\vskip -0.2in
\label{fig:finetuning_lr_data_size}

\end{table}

\subsection{Full Finetuning Test Accuracy and Disagreement Percentage Results} \label{appendix:full_finetuning_results}

The full table of finetuning results across all datasets is shown below in Table \ref{tab:finetuning_results}. We again note that across all datasets, finetuned models on unhealthy nodes achieve improved performance over the unfinetuned baseline and similar (though differing performance) compared to models deterministically finetuned on healthy nodes. Specficially, we observe, that this disagreement percentage (or delta from the healthy node finetuning) is comparable to finetuning under a differnt dataset shuffling seed.

\begin{table*}[t]

\begin{center}
\begin{tiny}
\begin{sc}

\begin{tabular}{ccccccccccccc}
\toprule
\multirow{2}{*}{Configuration} & \multicolumn{2}{c}{CosmosQA} & \multicolumn{2}{c}{MathQA} & \multicolumn{2}{c}{ScienceQA} & \multicolumn{2}{c}{OpenbookQA} & \multicolumn{2}{c}{BoolQ} & \multicolumn{2}{c}{RACE} \\
 & TA & DP & TA & DP & TA & DP & TA & DP & TA & DP & TA & DP\\
\midrule
without fine-tuning & 56.33 & 44.80 & 24.02 & 82.35 & 72.62 & 34.22 & 74.70 & 29.30 & 70.21 & 27.89 & 73.63 & 25.44 \\
\midrule
Healthy Node & 90.79 & - & 37.22 & - & 84.17 & - & 83.80 & - & 90.06 & - & 87.47 & - \\
Healthy Node (Seed=$43$) & 89.50 & 6.70 & 38.83 & 56.75 & 87.90 & 14.57 & 86.30 & 18.70 & 90.31 & 5.26 & 87.62 & 9.95 \\
\midrule
Unhealthy Node 1 & 90.53 & 5.15 & 36.78 & 42.24 & 86.74 & 13.26 & 85.00 & 16.80 & 89.94 & 3.49 & 87.82 & 6.61  \\
Unhealthy Node 2 & 90.79 & 0.00 & 37.22 & 0.00 & 84.17 & 0.00 & 83.80 & 0.00 & 90.06 & 0.00 & 87.47 & 0.00 \\
Unhealthy Node 3 & 90.77 & 4.96 & 34.47 & 41.84 & 85.79 & 14.61 & 83.40 & 16.10 & 90.21 & 3.64 & 87.47 & 0.00 \\
Unhealthy Node 4 & 90.32 & 5.59 & 36.42 & 37.76 & 85.48 & 15.38 & 84.30 & 16.60 & 90.61 & 4.10 & 88.12 & 6.75  \\
Unhealthy Node 5 & 90.79 & 0.00 & 37.19 & 34.91 & 84.17 & 0.00 & 85.10 & 14.10 & 90.06 & 0.00 & 87.47 & 6.45 \\
Unhealthy Node 6 & 0.00 & 100.00 & 36.92 & 36.82 & 84.26 & 2.52 & 84.70 & 16.30 & 90.03 & 2.66 & 0.00 & 100.00 \\
Unhealthy Node 7 & 90.32 & 4.99 & 37.22 & 0.00 & 85.21 & 17.81 & 83.80 & 0.00 & 90.80 & 3.36 & 87.41 & 6.36  \\
Unhealthy Node 8 & 89.84 & 6.23 & 38.22 & 36.62 & 85.66 & 12.14 & 85.20 & 15.50 & 90.49 & 3.00 & 87.11 & 6.61 \\
Unhealthy Node 9 & 89.97 & 5.38 & 35.78 & 37.49 & 85.30 & 21.49 & 85.00 & 17.40 & 90.12 & 3.43  & 87.41 & 6.57 \\
Unhealthy Node 10 & 89.97 & 4.93 & 37.05 & 37.05 & 88.31 & 15.15 & 84.10 & 17.20 & 90.64 & 3.09  & 87.94 & 6.59 \\
Unhealthy Node 11 & 90.61 & 4.54 & 36.82 & 38.53 & 85.57 & 16.41 & 87.10 & 15.60 & 90.31 & 3.73  & 87.15 & 6.95 \\
Unhealthy Node 12 & 90.79 & 0.00 & 37.22 & 0.00 & 84.17 & 0.00 & 83.80 & 0.00 & 90.06 & 0.00  & 87.47 & 0.00\\
Unhealthy Node 13 & 90.79 & 0.00 & 37.22 & 0.00 & 84.17 & 0.00 & 83.80 & 0.00 & 90.06 & 0.00  & 87.11 & 6.77 \\
Unhealthy Node 14 & 89.74 & 6.12 & 38.63 & 32.29 & 84.17 & 0.00 & 85.00 & 15.90 & 90.06 & 0.00  & 87.62 & 6.40 \\
Unhealthy Node 15 & 90.77 & 3.27 & 38.53 & 40.87 & 84.17 & 0.00 & 83.80 & 0.00 & 90.06 & 0.00  & 87.39 & 6.61 \\
\bottomrule
\end{tabular}

\end{sc}
\end{tiny}
\end{center}

\caption{Finetuning results for six question answering tasks on different nodes. For each task, we report the test accuracy (TA) and the disagreement percentage (DP).  By default, the random seed for data shuffling is set as $42$. }
\label{tab:finetuning_results}

\vskip -0.15in
\end{table*}



\input{body/tables/primitive_forward_frequency}
\input{body/tables/primitive_backward_frequency}

\input{body/tables/primitive_forward_max_severity}
\input{body/tables/primitive_backward_max_severity}
