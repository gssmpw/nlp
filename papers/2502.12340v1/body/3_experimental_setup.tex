
 
\section{Methodology} \label{sec:methodology}

In this section, we discuss our methodology for investigating the effect of real-world SDCs on LLM training. We first describe a high-level mechanism used to collect the unhealthy nodes for our experiments. Then, we break down the investigation of the SDC impact into three granularities and ask three key research questions (RQs). To answer these RQs, we further propose two synchronization mechanisms to isolate the impact of SDC.

\subsection{Hardware Collection in the Real World} 

To study the effect of real-world SDCs, we identify \emph{nodes that failed production margin stress tests and thus were not allowed into production availability}. Figure \ref{fig:fleet_management} shows a high-level production fleet management flow for identifying failing nodes. Before entering this flow, components such as machine learning accelerators go through stages of testing at the manufacturer and after system assembly.

\label{sec:sdc_fleet_management}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{body/figures/fleet_management.png}
    \vskip -0.05in
    \caption{Illustration of fleet management flow, where nodes are vetted through several rounds of testing at different granularities.}
    \vskip -0.2in
    \label{fig:fleet_management}
\end{figure}

Once a node is installed in a datacenter, additional stress tests are triggered to identify any compound hardware issue.
These tests include full-node communication collective stress tests, compute unit stress tests, and a small LLM training run where training outputs are compared with pre-computed golden truth values.
With deterministic workload execution, the difference from ground truth values or other non-determinism indicates SDC on the node.
To guard against hardware degradation over time, tests are also run when a node is reclaimed from customer usage either due to customer workloads ending without any indication of error or because the customer returned the hardware after receiving a signal indicating a hardware health issue from the cloud provider. 

Using this flow, we define two types of nodes:
\begin{itemize}
    \item \underline{\emph{Unhealthy nodes}} are nodes that fail the fleet management tests due to exhibiting SDC. 
    \item \underline{\emph{Healthy nodes}} are nodes from production that have passed the aforementioned tests. 
\end{itemize}
Each category contains fifteen (15) nodes. We have confirmed that \emph{all healthy nodes in our experiments will output the same result for the same computation} and we denote them as the healthy node for simplicity. However, unhealthy nodes can exhibit different symptoms for SDC and we assign each unhealthy node a unique identifier, namely Node 1 to Node 15. 

Unhealthy nodes flagged by fleet management are meaningful for studying real-world SDCs. Given that fleet management can only be run when the nodes are not used by customers, during multi-month periods of a large-scale LLM training run, healthy nodes that were originally healthy can degrade and produce SDCs affecting training before fleet management can isolate them.
We confirm that some of the unhealthy nodes in our experiments were initially healthy and began to fail the pre-checks after being used in training.

\subsection{Key Research Questions} \label{sec:research_questions}
To better understand the impact of SDCs, we break down our investigation into three levels and ask three main research questions (RQs):

\underline{\textbf{RQ1}}: \emph{What is the impact of SDCs on Transformer submodule computation results?}

\underline{\textbf{RQ2}}: \emph{What is the impact of SDCs on the gradients of model weights at a single optimizer step?} 

\underline{\textbf{RQ3}}: \emph{What is the accumulated impact of SDCs on the model quality over training steps?}

Investigating \textbf{RQ1} helps us understand the frequency and severity of SDCs, critical for designing a solution to detect SDC at the submodule level. Investigating \textbf{RQ2} and \textbf{RQ3} gives us more insight into understanding optimization dynamics when SDCs occur.
Most importantly, these research questions help prioritize future directions of detecting, mitigating, and recovering from SDC by providing concrete real-world SDC characteristics.

To compare SDC-induced incorrect computations with ground truth, we pair each unhealthy node with a healthy node and train identical models simultaneously on each node with exactly the same 
training setup. 
We employ the XLA compiler \cite{xlapaper} to ensure fully deterministic instruction ordering to isolate away non-SDC sources of non-determinism like floating point error. 
In other words, we confirm that \emph{the computation results are exactly the same on any two healthy nodes with the same compiler and the difference in computation results can be entirely attributed to SDC.}

Since SDC error can accumulate over computation, we need to correct the error on unhealthy nodes with the ground-truth results on the healthy node to isolate the impact of SDCs at different levels. Specifically, we design two synchronization mechanisms for \textbf{RQ1} and \textbf{RQ2}.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{body/figures/primitive_investigation.png}
    \vskip -0.05in
    \caption{Illustration of our lock-step parallelism works in a Transformer decoder layer. The arrows indicate intermediate tensors corrected by communicating values from the healthy to the unhealthy node (red in forwards pass, blue in backwards pass). In forward pass, $g$ is an all-gather and $\bar{g}$ is a reduce-scatter, while in the backwards pass $g$ is an reduce-scatter and $\bar{g}$ is an all-gather.}
    \vskip -0.2in
    \label{fig:primitive_investigation}
\end{figure}

\textbf{Computation Synchronization.}  To isolate the impact of SDCs at submodule level (\textbf{RQ1}), we set up a novel communication mesh called \textbf{lock-step parallelism} shown in Figure \ref{fig:primitive_investigation}. 
For each pair of same TP ranks on the unhealthy and healthy node, in the forward pass, we communicate and compare the output values after the forward computation of each submodule (self-attention or FFN) before the reduce-scatter of sequence parallelism, which avoids incorrect values on certain ranks spreading to tensor shards on other ranks after communication. Then, we overwrite the values on the unhealthy node with those from the healthy node (red arrows) to prevent SDC error from accumulating to the next submodule. Likewise, in the backward pass, we compare the input gradient after the backward computation of each submodule before the reduce-scatter (blue arrow) and overwrite the gradient values to prevent error from accumulating through backpropagation. More details can be found in Appendix \ref{appendix:primivite_investigation}.

\textbf{Parameter Synchronization.} To isolate the impact of SDCs at a single training step (\textbf{RQ2}), we do parameter synchronization at the end of each training step.
After taking an optimizer step, we broadcast the updated model parameters from the healthy node to overwrite the parameters on the unhealthy node. In this way, both nodes start from the same parameters for the next optimizer step.

For our synchronization mechanisms, we assume that \emph{SDCs do not occur when communicating tensors between the unhealthy and healthy node}. 
First, all nodes used in our experiments consistently passed stress tests for communication collectives. Second, the communication primitives used in our synchronization mechanism do not involve any compute unit. Finally, parity checks or error correcting codes are utilized for communication across the network. 
To avoid SDCs occurring during the arithmetic of tensor comparisons between healthy and unhealthy nodes, we always compute the comparisons on the healthy node.
