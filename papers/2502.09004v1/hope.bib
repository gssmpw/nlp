@article{fortuna,
author = {Fortuna, Paula and Nunes, S\'{e}rgio},
title = {A Survey on Automatic Detection of Hate Speech in Text},
year = {2018},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3232676},
doi = {10.1145/3232676},
abstract = {The scientific study of hate speech, from a computer science point of view, is recent. This survey organizes and describes the current state of the field, providing a structured overview of previous approaches, including core algorithms, methods, and main features used. This work also discusses the complexity of the concept of hate speech, defined in many platforms and contexts, and provides a unifying definition. This area has an unquestionable potential for societal impact, particularly in online communities and digital media platforms. The development and systematization of shared resources, such as guidelines, annotated datasets in multiple languages, and algorithms, is a crucial step in advancing the automatic detection of hate speech.},
journal = {ACM Comput. Surv.},
month = {jul},
articleno = {85},
numpages = {30},
keywords = {natural language processing, text mining, literature review, opinion mining, Hate speech}
}


@inproceedings{Capitol,
  author       = {Ashiqur R. KhudaBukhsh and
                  Rupak Sarkar and
                  Mark S. Kamlet and
                  Tom M. Mitchell},
  title        = {Fringe News Networks: Dynamics of {US} News Viewership following the
                  2020 Presidential Election},
  booktitle    = {WebSci '22: 14th {ACM} Web Science Conference 2022},
  pages        = {269--278},
  publisher    = {{ACM}},
  year         = {2022}
}

@inproceedings{KhudaBukhshPolarization,
  author       = {Ashiqur R. KhudaBukhsh and
                  Rupak Sarkar and
                  Mark S. Kamlet and
                  Tom M. Mitchell},
  title        = {We Don't Speak the Same Language: Interpreting Polarization through
                  Machine Translation},
  booktitle    = {Thirty-Fifth {AAAI} Conference on Artificial Intelligence},
  pages        = {14893--14901},
  publisher    = {{AAAI} Press},
  year         = {2021}
  }

@article{muller-karsten,
    author = {Müller, Karsten and Schwarz, Carlo},
    title = "{Fanning the Flames of Hate: Social Media and Hate Crime}",
    journal = {Journal of the European Economic Association},
    volume = {19},
    number = {4},
    pages = {2131-2167},
    year = {2020},
    month = {10},
    abstract = "{This paper investigates the link between social media and hate crime. We show that antirefugee sentiment on Facebook predicts crimes against refugees in otherwise similar municipalities with higher social media usage. To establish causality, we exploit exogenous variation in the timing of major Facebook and internet outages. Consistent with a role for “echo chambers,” we find that right-wing social media posts contain narrower and more loaded content than news reports. Our results suggest that social media can act as a propagation mechanism for violent crimes by enabling the spread of extreme viewpoints.}",
    issn = {1542-4766},
    doi = {10.1093/jeea/jvaa045},
    url = {https://doi.org/10.1093/jeea/jvaa045},
    eprint = {https://academic.oup.com/jeea/article-pdf/19/4/2131/39651047/jvaa045.pdf}
}
@article{agarwal2022deplatforming,
  title={Deplatforming and the control of misinformation: Evidence from parler},
  author={Agarwal, Saharsh and Ananthakrishnan, Uttara M and Tucker, Catherine E},
  journal={Available at SSRN},
  year={2022}
}
@article{benesch2014countering,
  title={Countering dangerous speech: New ideas for genocide prevention},
  author={Benesch, Susan},
  journal={Available at SSRN 3686876},
  year={2014}
}

@inproceedings{mathew2019thou,
  title={Thou shalt not hate: Countering online hate speech},
  author={Mathew, Binny and Saha, Punyajoy and Tharad, Hardik and Rajgaria, Subham and Singhania, Prajwal and Maity, Suman Kalyan and Goyal, Pawan and Mukherjee, Animesh},
  booktitle={Proceedings of the international AAAI conference on web and social media},
  volume={13},
  pages={369--380},
  year={2019}
}

@inproceedings{Ngo2023ZootopiAH,
  title={Zootopi at HOPE2023@IberLEF: Is Zero-Shot ChatGPT the Future of Hope Speech Detection?},
  author={Anh Ngo and Hanh Thi Hong Tran},
  booktitle={IberLEF@SEPLN},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:265309660}
}

@article{hangartner2021,
author = {Hangartner, Dominik and Gennaro, Gloria and Alasiri, Sary and Bahrich, Nicholas and Bornhoft, Alexandra and Boucher, Joseph and Demirci, Buket and Derksen, Laurenz and Hall, Aldo and Jochum, Matthias and Munoz, Maria and Richter, Marc and Vogel, Franziska and Wittwer, Salomé and Wüthrich, Felix and Gilardi, Fabrizio and Donnay, Karsten},
year = {2021},
month = {12},
pages = {e2116310118},
title = {Empathy-based counterspeech can reduce racist hate speech in a social media field experiment},
volume = {118},
journal = {Proceedings of the National Academy of Sciences},
doi = {10.1073/pnas.2116310118}
}

@article{palakodety2019hope,
  title={Hope speech detection: A computational analysis of the voice of peace},
  author={Palakodety, Shriphani and KhudaBukhsh, Ashiqur R and Carbonell, Jaime G},
  journal={arXiv preprint arXiv:1909.12940},
  year={2019}
}
@article{palakodety2020voice,
author = {Palakodety, Shriphani and Khudabukhsh, Ashiqur and Carbonell, Jaime},
year = {2020},
month = {04},
pages = {454-462},
title = {Voice for the Voiceless: Active Sampling to Detect Comments Supporting the Rohingyas},
volume = {34},
journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
doi = {10.1609/aaai.v34i01.5382}
}
@inproceedings{chakravarthi-etal-2022-overview-shared,
    title = "Overview of the Shared Task on Hope Speech Detection for Equality, Diversity, and Inclusion",
    author = "Chakravarthi, Bharathi Raja  and
      Muralidaran, Vigneshwaran  and
      Priyadharshini, Ruba  and
      Cn, Subalalitha  and
      McCrae, John  and
      Garc{\'\i}a, Miguel {\'A}ngel  and
      Jim{\'e}nez-Zafra, Salud Mar{\'\i}a  and
      Valencia-Garc{\'\i}a, Rafael  and
      Kumaresan, Prasanna  and
      Ponnusamy, Rahul  and
      Garc{\'\i}a-Baena, Daniel  and
      Garc{\'\i}a-D{\'\i}az, Jos{\'e}",
    editor = "Chakravarthi, Bharathi Raja  and
      Bharathi, B  and
      McCrae, John P  and
      Zarrouk, Manel  and
      Bali, Kalika  and
      Buitelaar, Paul",
    booktitle = "Proceedings of the Second Workshop on Language Technology for Equality, Diversity and Inclusion",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.ltedi-1.58",
    doi = "10.18653/v1/2022.ltedi-1.58",
    pages = "378--388",
    abstract = "Hope Speech detection is the task of classifying a sentence as hope speech or non-hope speech given a corpus of sentences. Hope speech is any message or content that is positive, encouraging, reassuring, inclusive and supportive that inspires and engenders optimism in the minds of people. In contrast to identifying and censoring negative speech patterns, hope speech detection is focussed on recognising and promoting positive speech patterns online. In this paper, we report an overview of the findings and results from the shared task on hope speech detection for Tamil, Malayalam, Kannada, English and Spanish languages conducted in the second workshop on Language Technology for Equality, Diversity and Inclusion (LT-EDI-2022) organised as a part of ACL 2022. The participants were provided with annotated training {\&} development datasets and unlabelled test datasets in all the five languages. The goal of the shared task is to classify the given sentences into one of the two hope speech classes. The performances of the systems submitted by the participants were evaluated in terms of micro-F1 score and weighted-F1 score. The datasets for this challenge are openly available"
}
@inproceedings{chakravarthi-muralidaran-2021-findings,
    title = "Findings of the Shared Task on Hope Speech Detection for Equality, Diversity, and Inclusion",
    author = "Chakravarthi, Bharathi Raja  and
      Muralidaran, Vigneshwaran",
    editor = "Chakravarthi, Bharathi Raja  and
      McCrae, John P.  and
      Zarrouk, Manel  and
      Bali, Kalika  and
      Buitelaar, Paul",
    booktitle = "Proceedings of the First Workshop on Language Technology for Equality, Diversity and Inclusion",
    month = apr,
    year = "2021",
    address = "Kyiv",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.ltedi-1.8",
    pages = "61--72",
    abstract = "Hope is considered significant for the well-being, recuperation and restoration of human life by health professionals. Hope speech reflects the belief that one can discover pathways to their desired objectives and become roused to utilise those pathways. To encourage research in natural language processing towards positive reinforcement approach, we created a hope speech detection dataset. This paper reports on the shared task of hope speech detection for Tamil, English, and Malayalam languages. The shared task was conducted as a part of the EACL 2021 workshop on Language Technology for Equality, Diversity, and Inclusion (LT-EDI-2021). We summarize here the datasets for this challenge which are openly available at \url{https://competitions.codalab.org/competitions/27653}, and present an overview of the methods and the results of the competing systems. To the best of our knowledge, this is the first shared task to conduct hope speech detection."
}

@inproceedings{prabhakaran2024grasp,
  title={GRASP: A Disagreement Analysis Framework to Assess Group Associations in Perspectives},
  author={Prabhakaran, Vinodkumar and Homan, Christopher and Aroyo, Lora and Davani, Aida Mostafazadeh and Parrish, Alicia and Taylor, Alex and D{\'\i}az, Mark and Wang, Ding and Serapio-Garc{\'\i}a, Gregory},
  booktitle={Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
  pages={3473--3492},
  year={2024}
}

@inproceedings{homan2024intersectionality,
  title={Intersectionality in AI Safety: Using Multilevel Models to Understand Diverse Perceptions of Safety in Conversational AI},
  author={Homan, Christopher and Serapio-Garcia, Gregory and Aroyo, Lora and D{\'\i}az, Mark and Parrish, Alicia and Prabhakaran, Vinodkumar and Taylor, Alex and Wang, Ding},
  booktitle={Proceedings of the 3rd Workshop on Perspectivist Approaches to NLP (NLPerspectives)@ LREC-COLING 2024},
  pages={131--141},
  year={2024}
}

@inproceedings{pei-jurgens-2023-annotator,
    title = "When Do Annotator Demographics Matter? Measuring the Influence of Annotator Demographics with the {POPQUORN} Dataset",
    author = "Pei, Jiaxin  and
      Jurgens, David",
    booktitle = "Proceedings of the 17th Linguistic Annotation Workshop (LAW-XVII)",
    month = jul,
    year = "2023",
    publisher = "Association for Computational Linguistics",
    pages = "252--265",
    abstract = "Annotators are not fungible. Their demographics, life experiences, and backgrounds all contribute to how they label data. However, NLP has only recently considered how annotator identity might influence their decisions. Here, we present POPQUORN (the Potato-Prolific dataset for Question-Answering, Offensiveness, text Rewriting and politeness rating with demographic Nuance). POPQUORN contains 45,000 annotations from 1,484 annotators, drawn from a representative sample regarding sex, age, and race as the US population. Through a series of analyses, we show that annotators{'} background plays a significant role in their judgments. Further, our work shows that backgrounds not previously considered in NLP (e.g., education), are meaningful and should be considered. Our study suggests that understanding the background of annotators and collecting labels from a demographically balanced pool of crowd workers is important to reduce the bias of datasets. The dataset, annotator background, and annotation interface are available at \url{https://github.com/Jiaxin-Pei/potato-prolific-dataset}."
}

@article{goyal2022your,
  title={Is your toxicity my toxicity? exploring the impact of rater identity on toxicity annotation},
  author={Goyal, Nitesh and Kivlichan, Ian D and Rosen, Rachel and Vasserman, Lucy},
  journal={Proceedings of the ACM on Human-Computer Interaction},
  volume={6},
  number={CSCW2},
  pages={1--28},
  year={2022},
  publisher={ACM New York, NY, USA}
}

@inproceedings{InfrastructureOmbudsman,
author = {Chowdhury, Md Towhidul Absar and Datta, Soumyajit and Sharma, Naveen and KhudaBukhsh, Ashiqur R.},
title = {Infrastructure Ombudsman: Mining Future Failure Concerns from Structural Disaster Response},
year = {2024},
isbn = {9798400701719},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3589334.3648153},
doi = {10.1145/3589334.3648153},
abstract = {Current research concentrates on studying discussions on social media related to structural failures to improve disaster response strategies. However, detecting social web posts discussing concerns about anticipatory failures is under-explored. If such concerns are channeled to the appropriate authorities, it can aid in the prevention and mitigation of potential infrastructural failures. In this paper, we develop an infrastructure ombudsman -- that automatically detects specific infrastructure concerns. Our work considers several recent structural failures in the US. We present a first-of-its-kind dataset of 2,662 social web instances for this novel task mined from Reddit and YouTube.},
booktitle = {Proceedings of the ACM on Web Conference 2024},
pages = {4664–4673},
numpages = {10},
series = {WWW '24}
}



@inproceedings{larimore2021reconsidering,
  title={Reconsidering annotator disagreement about racist language: Noise or signal?},
  author={Larimore, Savannah and Kennedy, Ian and Haskett, Breon and Arseniev-Koehler, Alina},
  booktitle={Proceedings of the Ninth International Workshop on Natural Language Processing for Social Media},
  pages={81--90},
  year={2021}
}

@inproceedings{al2020identifying,
  title={Identifying and measuring annotator bias based on annotators’ demographic characteristics},
  author={Al Kuwatly, Hala and Wich, Maximilian and Groh, Georg},
  booktitle={Proceedings of the fourth workshop on online abuse and harms},
  pages={184--190},
  year={2020}
}

@inproceedings{sap2019risk,
  title={The risk of racial bias in hate speech detection},
  author={Sap, Maarten and Card, Dallas and Gabriel, Saadia and Choi, Yejin and Smith, Noah A},
  booktitle={Proceedings of the 57th annual meeting of the association for computational linguistics},
  pages={1668--1678},
  year={2019}
}

@article{pavlick2019inherent,
  title={Inherent disagreements in human textual inferences},
  author={Pavlick, Ellie and Kwiatkowski, Tom},
  journal={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={677--694},
  year={2019},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{poole1984polarization,
  title={The polarization of American politics},
  author={Poole, Keith T and Rosenthal, Howard},
  journal={The journal of politics},
  volume={46},
  number={4},
  pages={1061--1079},
  year={1984},
  publisher={Southern Political Science Association}
}


@article{iyengar2015fear,
  title={Fear and loathing across party lines: New evidence on group polarization},
  author={Iyengar, Shanto and Westwood, Sean J},
  journal={American Journal of Political Science},
  volume={59},
  number={3},
  pages={690--707},
  year={2015},
  publisher={Wiley Online Library}
}


@article{huber2017political,
  title={Political homophily in social relationships: Evidence from online dating behavior},
  author={Huber, Gregory A and Malhotra, Neil},
  journal={The Journal of Politics},
  volume={79},
  number={1},
  pages={269--283},
  year={2017},
  publisher={University of Chicago Press Chicago, IL}
}



@inproceedings{DBLP:conf/acl/GuptaDGB0A23,
  author       = {Rishabh Gupta and
                  Shaily Desai and
                  Manvi Goel and
                  Anil Bandhakavi and
                  Tanmoy Chakraborty and
                  Md. Shad Akhtar},
  editor       = {Anna Rogers and
                  Jordan L. Boyd{-}Graber and
                  Naoaki Okazaki},
  title        = {Counterspeeches up my sleeve! Intent Distribution Learning and Persistent
                  Fusion for Intent-Conditioned Counterspeech Generation},
  booktitle    = {Proceedings of the 61st Annual Meeting of the Association for Computational
                  Linguistics (Volume 1: Long Papers), {ACL} 2023},
  pages        = {5792--5809},
  publisher    = {Association for Computational Linguistics},
  year         = {2023}
}

@inproceedings{DBLP:conf/ijcai/SahaSKM022,
  author       = {Punyajoy Saha and
                  Kanishk Singh and
                  Adarsh Kumar and
                  Binny Mathew and
                  Animesh Mukherjee},
  title        = {CounterGeDi: {A} Controllable Approach to Generate Polite, Detoxified
                  and Emotional Counterspeech},
  booktitle    = {Proceedings of the Thirty-First International Joint Conference on
                  Artificial Intelligence, {IJCAI} 2022, Vienna, Austria, 23-29 July
                  2022},
  pages        = {5157--5163},
  publisher    = {ijcai.org},
  year         = {2022}
}

@book{hartman2019war,
  title={A war for the soul of America: A history of the culture wars},
  author={Hartman, Andrew},
  year={2019},
  publisher={University of Chicago Press}
}


@inproceedings{DBLP:conf/naacl/HenglePSBA024,
  author       = {Amey Hengle and
                  Aswini Padhi and
                  Sahajpreet Singh and
                  Anil Bandhakavi and
                  Md. Shad Akhtar and
                  Tanmoy Chakraborty},
  title        = {Intent-conditioned and Non-toxic Counterspeech Generation using Multi-Task
                  Instruction Tuning with {RLAIF}},
  booktitle    = {Proceedings of the 2024 Conference of the North American Chapter of
                  the Association for Computational Linguistics: Human Language Technologies
                  (Volume 1: Long Papers), {NAACL} 2024},
  pages        = {6716--6733},
  publisher    = {Association for Computational Linguistics},
  year         = {2024}
}

@inproceedings{chakravarthi-2020-hopeedi,
    title = "{H}ope{EDI}: A Multilingual Hope Speech Detection Dataset for Equality, Diversity, and Inclusion",
    author = "Chakravarthi, Bharathi Raja",
    editor = "Nissim, Malvina  and
      Patti, Viviana  and
      Plank, Barbara  and
      Durmus, Esin",
    booktitle = "Proceedings of the Third Workshop on Computational Modeling of People's Opinions, Personality, and Emotion's in Social Media",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.peoples-1.5",
    pages = "41--53",
    abstract = "Over the past few years, systems have been developed to control online content and eliminate abusive, offensive or hate speech content. However, people in power sometimes misuse this form of censorship to obstruct the democratic right of freedom of speech. Therefore, it is imperative that research should take a positive reinforcement approach towards online content that is encouraging, positive and supportive contents. Until now, most studies have focused on solving this problem of negativity in the English language, though the problem is much more than just harmful content. Furthermore, it is multilingual as well. Thus, we have constructed a Hope Speech dataset for Equality, Diversity and Inclusion (HopeEDI) containing user-generated comments from the social media platform YouTube with 28,451, 20,198 and 10,705 comments in English, Tamil and Malayalam, respectively, manually labelled as containing hope speech or not. To our knowledge, this is the first research of its kind to annotate hope speech for equality, diversity and inclusion in a multilingual setting. We determined that the inter-annotator agreement of our dataset using Krippendorff{'}s alpha. Further, we created several baselines to benchmark the resulting dataset and the results have been expressed using precision, recall and F1-score. The dataset is publicly available for the research community. We hope that this resource will spur further research on encouraging inclusive and responsive speech that reinforces positiveness."
}
@article{garcia2023hope,
  title={Hope speech detection in Spanish: The LGBT case},
  author={Garc{\'\i}a-Baena, Daniel and Garc{\'\i}a-Cumbreras, Miguel {\'A}ngel and Jim{\'e}nez-Zafra, Salud Mar{\'\i}a and Garc{\'\i}a-D{\'\i}az, Jos{\'e} Antonio and Valencia-Garc{\'\i}a, Rafael},
  journal={Language Resources and Evaluation},
  pages={1--28},
  year={2023},
  publisher={Springer}
}
@inproceedings{chakravarthi-etal-2022-overview,
    title = "Overview of The Shared Task on Homophobia and Transphobia Detection in Social Media Comments",
    author = "Chakravarthi, Bharathi Raja  and
      Priyadharshini, Ruba  and
      Durairaj, Thenmozhi  and
      McCrae, John  and
      Buitelaar, Paul  and
      Kumaresan, Prasanna  and
      Ponnusamy, Rahul",
    editor = "Chakravarthi, Bharathi Raja  and
      Bharathi, B  and
      McCrae, John P  and
      Zarrouk, Manel  and
      Bali, Kalika  and
      Buitelaar, Paul",
    booktitle = "Proceedings of the Second Workshop on Language Technology for Equality, Diversity and Inclusion",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.ltedi-1.57",
    doi = "10.18653/v1/2022.ltedi-1.57",
    pages = "369--377",
    abstract = "Homophobia and Transphobia Detection is the task of identifying homophobia, transphobia, and non-anti-LGBT+ content from the given corpus. Homophobia and transphobia are both toxic languages directed at LGBTQ+ individuals that are described as hate speech. This paper summarizes our findings on the {``}Homophobia and Transphobia Detection in social media comments{''} shared task held at LT-EDI 2022 - ACL 2022 1. This shared taskfocused on three sub-tasks for Tamil, English, and Tamil-English (code-mixed) languages. It received 10 systems for Tamil, 13 systems for English, and 11 systems for Tamil-English. The best systems for Tamil, English, and Tamil-English scored 0.570, 0.870, and 0.610, respectively, on average macro F1-score."
}

@article{jimenez2023overview,
  title={Overview of HOPE at IberLEF 2023: Multilingual Hope Speech Detection},
  author={Jim{\'e}nez-Zafra, Salud Mar{\'\i}a and Garcia-Cumbreras, Miguel {\'A}ngel and Garc{\'\i}a-Baena, Daniel and Garcia-D{\'\i}az, Jos{\'e} Antonio and Chakravarthi, Bharathi Raja and Valencia-Garc{\'\i}a, Rafael and Ure{\~n}a-L{\'o}pez, Luis Alfonso},
  journal={Procesamiento del Lenguaje Natural},
  volume={71},
  pages={371--381},
  year={2023}
}

@inproceedings{snow-etal-2008-cheap,
    title = "Cheap and Fast {--} But is it Good? Evaluating Non-Expert Annotations for Natural Language Tasks",
    author = "Snow, Rion  and
      O{'}Connor, Brendan  and
      Jurafsky, Daniel  and
      Ng, Andrew",
    editor = "Lapata, Mirella  and
      Ng, Hwee Tou",
    booktitle = "Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2008",
    address = "Honolulu, Hawaii",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D08-1027",
    pages = "254--263"
}

@misc{qlora,
      title={QLoRA: Efficient Finetuning of Quantized LLMs}, 
      author={Tim Dettmers and Artidoro Pagnoni and Ari Holtzman and Luke Zettlemoyer},
      year={2023},
      eprint={2305.14314},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{Mendelsohn_2020,
   title={A Framework for the Computational Linguistic Analysis of Dehumanization},
   volume={3},
   ISSN={2624-8212},
   url={http://dx.doi.org/10.3389/frai.2020.00055},
   DOI={10.3389/frai.2020.00055},
   journal={Frontiers in Artificial Intelligence},
   publisher={Frontiers Media SA},
   author={Mendelsohn, Julia and Tsvetkov, Yulia and Jurafsky, Dan},
   year={2020},
   month=aug }
@misc{guo2024investigationlargelanguagemodels,
      title={An Investigation of Large Language Models for Real-World Hate Speech Detection}, 
      author={Keyan Guo and Alexander Hu and Jaden Mu and Ziheng Shi and Ziming Zhao and Nishant Vishwamitra and Hongxin Hu},
      year={2024},
      eprint={2401.03346},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      url={https://arxiv.org/abs/2401.03346}
}
@misc{dutta2024toxicityrabbitholenovel,
      title={Down the Toxicity Rabbit Hole: A Novel Framework to Bias Audit Large Language Models}, 
      author={Arka Dutta and Adel Khorramrouz and Sujan Dutta and Ashiqur R. KhudaBukhsh},
      year={2024},
      eprint={2309.06415},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.06415},
}

@inproceedings{davidson2017automated,
  title={Automated hate speech detection and the problem of offensive language},
  author={Davidson, Thomas and Warmsley, Dana and Macy, Michael and Weber, Ingmar},
  booktitle={Proceedings of the international AAAI conference on web and social media},
  volume={11},
  number={1},
  pages={512--515},
  year={2017}
}

@inproceedings{wiegand2019detection,
  title={Detection of abusive language: the problem of biased datasets},
  author={Wiegand, Michael and Ruppenhofer, Josef and Kleinbauer, Thomas},
  booktitle={Proceedings of the 2019 conference of the North American Chapter of the Association for Computational Linguistics: human language technologies, volume 1 (long and short papers)},
  pages={602--608},
  year={2019}
}

@inproceedings{scheffer2001active,
  title={Active hidden markov models for information extraction},
  author={Scheffer, Tobias and Decomain, Christian and Wrobel, Stefan},
  booktitle={International Symposium on Intelligent Data Analysis},
  pages={309--318},
  year={2001},
  organization={Springer}
}


@inproceedings{Vicarious,
  author       = {Tharindu Cyril Weerasooriya and
                  Sujan Dutta and
                  Tharindu Ranasinghe and
                  Marcos Zampieri and
                  Christopher Homan and
                  Ashiqur R. KhudaBukhsh},
  title        = {Vicarious Offense and Noise Audit of Offensive Speech Classifiers:
                  Unifying Human and Machine Disagreement on What is Offensive},
  booktitle    = {Proceedings of the 2023 Conference on Empirical Methods in Natural
                  Language Processing, {EMNLP} 2023},
  pages        = {11648--11668},
  publisher    = {Association for Computational Linguistics},
  year         = {2023}
}

@article{abreu2018cyberbullying,
  title={Cyberbullying and LGBTQ youth: A systematic literature review and recommendations for prevention and intervention},
  author={Abreu, Roberto L and Kenny, Maureen C},
  journal={Journal of Child \& Adolescent Trauma},
  volume={11},
  pages={81--97},
  year={2018},
  publisher={Springer}
}

@article{mcinroy2019lgbtq+,
  title={LGBTQ+ youths’ community engagement and resource seeking online versus offline},
  author={McInroy, Lauren B and McCloskey, Rebecca J and Craig, Shelley L and Eaton, Andrew D},
  journal={Journal of Technology in Human Services},
  volume={37},
  number={4},
  pages={315--333},
  year={2019},
  publisher={Taylor \& Francis}
}

@article{blackwell2015seeing,
  title={Seeing and being seen: Co-situation and impression formation using Grindr, a location-aware gay dating app},
  author={Blackwell, Courtney and Birnholtz, Jeremy and Abbott, Charles},
  journal={New media \& society},
  volume={17},
  number={7},
  pages={1117--1136},
  year={2015},
  publisher={Sage Publications Sage UK: London, England}
}

@article{kirby2021queering,
  title={Queering the Map: Stories of love, loss and (be) longing within a digital cartographic archive},
  author={Kirby, Emma and Watson, Ash and Churchill, Brendan and Robards, Brady and LaRochelle, Lucas},
  journal={Media, Culture \& Society},
  volume={43},
  number={6},
  pages={1043--1060},
  year={2021},
  publisher={SAGE Publications Sage UK: London, England}
}

@inproceedings{wang2024human,
  title={Human-LLM collaborative annotation through effective verification of LLM labels},
  author={Wang, Xinru and Kim, Hannah and Rahman, Sajjadur and Mitra, Kushan and Miao, Zhengjie},
  booktitle={Proceedings of the CHI Conference on Human Factors in Computing Systems},
  pages={1--21},
  year={2024}
}

@inproceedings{halterman-etal-2021-corpus,
    title = "Corpus-Level Evaluation for Event {QA}: The {I}ndia{P}olice{E}vents Corpus Covering the 2002 {G}ujarat Violence",
    author = "Halterman, Andrew  and
      Keith, Katherine  and
      Sarwar, Sheikh  and
      O{'}Connor, Brendan",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-acl.371",
    doi = "10.18653/v1/2021.findings-acl.371",
    pages = "4240--4253"
}


@inproceedings{demszky-etal-2019-analyzing,
    title = "Analyzing Polarization in Social Media: Method and Application to Tweets on 21 Mass Shootings",
    author = "Demszky, Dorottya  and
      Garg, Nikhil  and
      Voigt, Rob  and
      Zou, James  and
      Shapiro, Jesse  and
      Gentzkow, Matthew  and
      Jurafsky, Dan",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    pages = "2970--3005"
}

@inproceedings{o2010tweets,
  title={From tweets to polls: Linking text sentiment to public opinion time series},
  author={O'Connor, Brendan and Balasubramanyan, Ramnath and Routledge, Bryan and Smith, Noah},
  booktitle={Proceedings of the international AAAI conference on web and social media},
  volume={4},
  number={1},
  pages={122--129},
  year={2010}
}



@inproceedings{leonardelli-etal-2021-agreeing,
    title = "Agreeing to Disagree: Annotating Offensive Language Datasets with Annotators{'} Disagreement",
    author = "Leonardelli, Elisa  and
      Menini, Stefano  and
      Palmero Aprosio, Alessio  and
      Guerini, Marco  and
      Tonelli, Sara",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.822",
    doi = "10.18653/v1/2021.emnlp-main.822",
    pages = "10528--10539",
    abstract = "Since state-of-the-art approaches to offensive language detection rely on supervised learning, it is crucial to quickly adapt them to the continuously evolving scenario of social media. While several approaches have been proposed to tackle the problem from an algorithmic perspective, so to reduce the need for annotated data, less attention has been paid to the quality of these data. Following a trend that has emerged recently, we focus on the level of agreement among annotators while selecting data to create offensive language datasets, a task involving a high level of subjectivity. Our study comprises the creation of three novel datasets of English tweets covering different topics and having five crowd-sourced judgments each. We also present an extensive set of experiments showing that selecting training and test data according to different levels of annotators{'} agreement has a strong effect on classifiers performance and robustness. Our findings are further validated in cross-domain experiments and studied using a popular benchmark dataset. We show that such hard cases, where low agreement is present, are not necessarily due to poor-quality annotation and we advocate for a higher presence of ambiguous cases in future datasets, in order to train more robust systems and better account for the different points of view expressed online."
}
@inproceedings{bai-etal-2021-pre,
    title = "Pre-train or Annotate? Domain Adaptation with a Constrained Budget",
    author = "Bai, Fan  and
      Ritter, Alan  and
      Xu, Wei",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.409",
    doi = "10.18653/v1/2021.emnlp-main.409",
    pages = "5002--5015",
    abstract = "Recent work has demonstrated that pre-training in-domain language models can boost performance when adapting to a new domain. However, the costs associated with pre-training raise an important question: given a fixed budget, what steps should an NLP practitioner take to maximize performance? In this paper, we study domain adaptation under budget constraints, and approach it as a customer choice problem between data annotation and pre-training. Specifically, we measure the annotation cost of three procedural text datasets and the pre-training cost of three in-domain language models. Then we evaluate the utility of different combinations of pre-training and data annotation under varying budget constraints to assess which combination strategy works best. We find that, for small budgets, spending all funds on annotation leads to the best performance; once the budget becomes large enough, a combination of data annotation and in-domain pre-training works more optimally. We therefore suggest that task-specific data annotation should be part of an economical strategy when adapting an NLP model to a new domain."
}

@inproceedings{IJCAI2024RabbitHole,
  title     = {Down the Toxicity Rabbit Hole: A  Framework to Bias Audit Large Language Models with Key Emphasis on Racism, Antisemitism, and Misogyny},
  author    = {Dutta, Arka and Khorramrouz, Adel and Dutta, Sujan and KhudaBukhsh, Ashiqur R.},
  booktitle = {Proceedings of the Thirty-Third International Joint Conference on
               Artificial Intelligence, {IJCAI-24}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  pages     = {7242--7250},
  year      = {2024},
  month     = {8},
  note      = {AI for Good},
  doi       = {10.24963/ijcai.2024/801},
  url       = {https://doi.org/10.24963/ijcai.2024/801}
}


@inproceedings{sap-etal-2022-annotators,
    title = "Annotators with Attitudes: How Annotator Beliefs And Identities Bias Toxic Language Detection",
    author = "Sap, Maarten  and
      Swayamdipta, Swabha  and
      Vianna, Laura  and
      Zhou, Xuhui  and
      Choi, Yejin  and
      Smith, Noah A.",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.431",
    doi = "10.18653/v1/2022.naacl-main.431",
    pages = "5884--5906",
    abstract = "The perceived toxicity of language can vary based on someone{'}s identity and beliefs, but this variation is often ignored when collecting toxic language datasets, resulting in dataset and model biases. We seek to understand the *who*, *why*, and *what* behind biases in toxicity annotations. In two online studies with demographically and politically diverse participants, we investigate the effect of annotator identities (*who*) and beliefs (*why*), drawing from social psychology research about hate speech, free speech, racist beliefs, political leaning, and more. We disentangle *what* is annotated as toxic by considering posts with three characteristics: anti-Black language, African American English (AAE) dialect, and vulgarity. Our results show strong associations between annotator identity and beliefs and their ratings of toxicity. Notably, more conservative annotators and those who scored highly on our scale for racist beliefs were less likely to rate anti-Black language as toxic, but more likely to rate AAE as toxic. We additionally present a case study illustrating how a popular toxicity detection system{'}s ratings inherently reflect only specific beliefs and perspectives. Our findings call for contextualizing toxicity labels in social variables, which raises immense implications for toxic language annotation and detection."
}
@inproceedings{attenberg-dual-supervision,
author = {Attenberg, Josh and Melville, Prem and Provost, Foster},
title = {A unified approach to active dual supervision for labeling features and examples},
year = {2010},
isbn = {364215879X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-15880-3_9},
doi = {10.1007/978-3-642-15880-3_9},
abstract = {When faced with the task of building accurate classifiers, active learning is often a beneficial tool for minimizing the requisite costs of human annotation. Traditional active learning schemes query a human for labels on intelligently chosen examples. However, human effort can also be expended in collecting alternative forms of annotation. For example, one may attempt to learn a text classifier by labeling words associated with a class, instead of, or in addition to, documents. Learning from two different kinds of supervision adds a challenging dimension to the problem of active learning. In this paper, we present a unified approach to such active dual supervision: determining which feature or example a classifier is most likely to benefit from having labeled. Empirical results confirm that appropriately querying for both example and feature labels significantly reduces overall human effort—beyond what is possible through traditional one-dimensional active learning.},
booktitle = {Proceedings of the 2010th European Conference on Machine Learning and Knowledge Discovery in Databases - Volume Part I},
pages = {40–55},
numpages = {16},
location = {Barcelona, Spain},
series = {ECMLPKDD'10}
}
@article {Entube,
	Title = {EnTube: A Dataset for YouTube Video Engagement Analytics},
	Author = {Le, Truong and Nguyen-Thi, Minh-Vuong and Le, Huy and Vo, Quoc-Thang and Le, Tung and Nguyen, Huy Tien},
	DOI = {10.21203/rs.3.rs-2085784/v1},
	Abstract = {YouTube, one of the largest online video-sharing platforms today, has provided a place for content creators to share information and earn extra income. Anticipating whether a video will be engaged by viewers or not is an essential factor in helping video creators improve video content and quality before publishing. To facilitate this task, we build an annotated dataset of 23,738 videos collected from 72 YouTube channels in Vietnam that were in four categories (i.e., comedy, travel-and-events, education, science-and-technology) and published over 12 years. We evaluate a number of metrics for measuring video engagement to propose a novel measure which determines the engagement of a video via Q score. Using our proposed measure, we annotate videos with three levels of engagement including Engage, Neutral, and not Engage. From the supervised dataset, we constructed a multimodal model to infer the degree of engagement based on the content of a YouTube video such as title, audio, thumbnails, video, and tags. We believe our dataset and metric to be useful for engagement analytics as well as studies on social media content.},
	Journal = {Research Square},
	Year = {2022},
	URL = {https://doi.org/10.21203/rs.3.rs-2085784/v1}
}
@inproceedings{Lopez-youtubers,
  title={Formula for measuring the engagement of the viewer on YouTube : exploratory research on the main Spanish youtubers},
  author={Alberto-Jes{\'u}s L{\'o}pez-Navarrete and Marga Cabrera-M{\'e}ndez and Rebeca D{\'i}ez-Somavilla},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:232074567}
}
@inproceedings{inproceedings,
author = {Dutta, Sujan and Li, Beibei and Nagin, Daniel and Khudabukhsh, Ashiqur},
year = {2022},
month = {07},
pages = {5025-5031},
title = {A Murder and Protests, the Capitol Riot, and the Chauvin Trial: Estimating Disparate News Media Stance},
doi = {10.24963/ijcai.2022/698}
}
@misc{yoo2023auditingrobustifyingcovid19misinformation,
      title={Auditing and Robustifying COVID-19 Misinformation Datasets via Anticontent Sampling}, 
      author={Clay H. Yoo and Ashiqur R. KhudaBukhsh},
      year={2023},
      eprint={2310.07078},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.07078},
}


@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@misc{Guardian1,
      title={{
Facebook is hiring moderators. But is the job too gruesome to handle?}}, 
      author={Olivia Solon},
      year={2017},
      note = {The Guardian},
      url       = {https://www.theguardian.com/technology/2017/may/04/facebook-content-moderators-ptsd-psychological-dangers
}
}

@inproceedings{bang-etal-2024-measuring,
    title = "Measuring Political Bias in Large Language Models: What Is Said and How It Is Said",
    author = "Bang, Yejin  and
      Chen, Delong  and
      Lee, Nayeon  and
      Fung, Pascale",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.600",
    doi = "10.18653/v1/2024.acl-long.600",
    pages = "11142--11159",
    abstract = "We propose to measure political bias in LLMs by analyzing both the content and style of their generated content regarding political issues. Existing benchmarks and measures focus on gender and racial biases. However, political bias exists in LLMs and can lead to polarization and other harms in downstream applications. In order to provide transparency to users, we advocate that there should be fine-grained and explainable measures of political biases generated by LLMs. Our proposed measure looks at different political issues such as reproductive rights and climate change, at both the content (the substance of the generation) and the style (the lexical polarity) of such bias. We measured the political bias in eleven open-sourced LLMs and showed that our proposed framework is easily scalable to other topics and is explainable."
}


@inproceedings{feng-etal-2023-pretraining,
    title = "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair {NLP} Models",
    author = "Feng, Shangbin  and
      Park, Chan Young  and
      Liu, Yuhan  and
      Tsvetkov, Yulia",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.656",
    doi = "10.18653/v1/2023.acl-long.656",
    pages = "11737--11762",
    abstract = "Language models (LMs) are pretrained on diverse data sources{---}news, discussion forums, books, online encyclopedias. A significant portion of this data includes facts and opinions which, on one hand, celebrate democracy and diversity of ideas, and on the other hand are inherently socially biased. Our work develops new methods to (1) measure media biases in LMs trained on such corpora, along social and economic axes, and (2) measure the fairness of downstream NLP models trained on top of politically biased LMs. We focus on hate speech and misinformation detection, aiming to empirically quantify the effects of political (social, economic) biases in pretraining data on the fairness of high-stakes social-oriented tasks. Our findings reveal that pretrained LMs do have political leanings which reinforce the polarization present in pretraining corpora, propagating social biases into hate speech predictions and media biases into misinformation detectors. We discuss the implications of our findings for NLP research and propose future directions to mitigate unfairness."
}

@book{kahneman2021noise,
  title={Noise: A flaw in human judgment},
  author={Kahneman, Daniel and Sibony, Olivier and Sunstein, Cass R},
  year={2021},
  publisher={Little, Brown}
}


@inproceedings{bender2021dangers,
  title={{On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?}},
  author={Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  booktitle={ACM FaccT},
  pages={610--623},
  year={2021}
}


@inproceedings{sindhwani2009uncertainty,
  title={Uncertainty sampling and transductive experimental design for active dual supervision},
  author={Sindhwani, Vikas and Melville, Prem and Lawrence, Richard D},
  booktitle={ICML},
  pages={953--960},
  year={2009}
}


@article{braun2017enhanced,
  title={Enhanced prediction of user-preferred YouTube videos based on cleaned viewing pattern history},
  author={Braun, Peter and Cuzzocrea, Alfredo and Doan, Lam MV and Kim, Suyoung and Leung, Carson K and Matundan, Jose Francisco A and Singh, Rashpal Robby},
  journal={Procedia Computer Science},
  volume={112},
  pages={2230--2239},
  year={2017},
  publisher={Elsevier}
}

@article{harrington2019deconstructing,
  title={Deconstructing community-based collaborative design: Towards more equitable participatory design engagements},
  author={Harrington, Christina and Erete, Sheena and Piper, Anne Marie},
  journal={CSCW},
  volume={3},
  pages={1--25},
  year={2019}
}


@article{birhane2022power,
  title={{Power to the People? Opportunities and Challenges for Participatory AI}},
  author={Birhane, Abeba and Isaac, William and Prabhakaran, Vinodkumar and D{\'\i}az, Mark and Elish, Madeleine Clare and Gabriel, Iason and Mohamed, Shakir},
  journal={EAAMO 2022},
  pages={1--8},
  year={2022}
}



@inproceedings{lu-jurgens-2022-subtle,
    title = "The subtle language of exclusion: Identifying the Toxic Speech of Trans-exclusionary Radical Feminists",
    author = "Lu, Christina  and
      Jurgens, David",
    editor = "Narang, Kanika  and
      Mostafazadeh Davani, Aida  and
      Mathias, Lambert  and
      Vidgen, Bertie  and
      Talat, Zeerak",
    booktitle = "Proceedings of the Sixth Workshop on Online Abuse and Harms (WOAH)",
    month = jul,
    year = "2022",
    address = "Seattle, Washington (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.woah-1.8",
    doi = "10.18653/v1/2022.woah-1.8",
    pages = "79--91",
    abstract = "Toxic language can take many forms, from explicit hate speech to more subtle microaggressions. Within this space, models identifying transphobic language have largely focused on overt forms. However, a more pernicious and subtle source of transphobic comments comes in the form of statements made by Trans-exclusionary Radical Feminists (TERFs); these statements often appear seemingly-positive and promote women{'}s causes and issues, while simultaneously denying the inclusion of transgender women as women. Here, we introduce two models to mitigate this antisocial behavior. The first model identifies TERF users in social media, recognizing that these users are a main source of transphobic material that enters mainstream discussion and whom other users may not desire to engage with in good faith. The second model tackles the harder task of recognizing the masked rhetoric of TERF messages and introduces a new dataset to support this task. Finally, we discuss the ethics of deploying these models to mitigate the harm of this language, arguing for a balanced approach that allows for restorative interactions."
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}