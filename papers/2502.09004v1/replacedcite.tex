\section{Related Work}
Our work is closely related with the \textit{hope speech} literature____ and the broader literature of counter speech____. Our work contributes to this literature through our unique focus on LGBTQ+ discourse in mainstream US politics and our investigation on how rater subjectivity and political leanings. 

Political polarization in the US is widely studied across multiple disciplines in rich and diverse settings that include congressional voting patterns on policy issues____, mate selection____, allocation of scholarship funds____, and annotating offensive content____. While how political leanings might associate with the task of rating negative content has been studied both in the context of US politics____ and on hot button issues such as reproductive rights, gun control/rights____, and US infrastructure____, our work demonstrates that political identities of the raters may influence how they perceive positive content for the LGBTQ+ community. Our study thus contributes to the broader literature of annotator subjectivity____. 

Our annotation study design is grounded in prior literature____ and draws from____ in seeking diverse and balanced political perspectives including from the independents. Our work also touches upon political bias in LLMs as we observe that zero-shot classification of LGBTQ+ content of several models are more aligned with liberal raters____. To tackle class imbalance, we construct an ensemble active learning pipeline much akin to____ leveraging well-known active learning strategies (e.g., certainty sampling____ and margin sampling____).  

%Developing machine learning and NLP models for hope speech detection offers potential insights into online discourse patterns and the varied reception of supportive content across different internet demographics. Within this under-explored field, two areas stand out as particularly promising for further investigation: hope speech directed towards the LGBTQ+ community and the application of state-of-the-art Large Language Models (LLMs) for its detection and analysis.

%\par Our research offers three significant contributions to the emerging field of hope speech detection:

%\begin{enumerate}
%    \item We extend the concept of hope speech specifically for the LGBTQ+ community, constructing an annotated dataset of supportive comments, in the context of US politics.
    
%    \item We provide novel insights into audience engagement patterns and sentiment dynamics surrounding LGBTQ+ content, enhancing our understanding of digital discourse in these spaces.

    
%    \item We analyze the impact of political views on the creation and recognition of LGBTQ+ supportive content, providing insights into the intersection of politics and online discourse.
%\end{enumerate}

%\par This study represents the first application of large language models (LLMs) to examine LGBTQ+ hope speech at this scale and specificity. By creating and analyzing a novel dataset from comments on FOX, CNN, and MSNBC YouTube videos, we aim to advance hope speech detection techniques and explore the distribution of positive LGBTQ+ discourse among different audiences. The results of our model demonstrate the significant impact that both negative and positive comments can have in online spaces. As shown in Table \ref{tab:youtube-comments}, negative comments can often contain explicit threats, derogatory language, and harmful stereotypes that can create a hostile environment for LGBTQ+ individuals. In contrast, positive comments express support, empathy, and calls for respect, potentially creating a more welcoming atmosphere. For instance, one supportive comment emphasizes the desire for "basic human decency" and respect.  Our work not only seeks to improve technical models but also contributes to the broader goal of fostering a more inclusive digital landscape for marginalized communities.