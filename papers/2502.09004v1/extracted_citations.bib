@inproceedings{DBLP:conf/acl/GuptaDGB0A23,
  author       = {Rishabh Gupta and
                  Shaily Desai and
                  Manvi Goel and
                  Anil Bandhakavi and
                  Tanmoy Chakraborty and
                  Md. Shad Akhtar},
  editor       = {Anna Rogers and
                  Jordan L. Boyd{-}Graber and
                  Naoaki Okazaki},
  title        = {Counterspeeches up my sleeve! Intent Distribution Learning and Persistent
                  Fusion for Intent-Conditioned Counterspeech Generation},
  booktitle    = {Proceedings of the 61st Annual Meeting of the Association for Computational
                  Linguistics (Volume 1: Long Papers), {ACL} 2023},
  pages        = {5792--5809},
  publisher    = {Association for Computational Linguistics},
  year         = {2023}
}

@inproceedings{DBLP:conf/ijcai/SahaSKM022,
  author       = {Punyajoy Saha and
                  Kanishk Singh and
                  Adarsh Kumar and
                  Binny Mathew and
                  Animesh Mukherjee},
  title        = {CounterGeDi: {A} Controllable Approach to Generate Polite, Detoxified
                  and Emotional Counterspeech},
  booktitle    = {Proceedings of the Thirty-First International Joint Conference on
                  Artificial Intelligence, {IJCAI} 2022, Vienna, Austria, 23-29 July
                  2022},
  pages        = {5157--5163},
  publisher    = {ijcai.org},
  year         = {2022}
}

@inproceedings{DBLP:conf/naacl/HenglePSBA024,
  author       = {Amey Hengle and
                  Aswini Padhi and
                  Sahajpreet Singh and
                  Anil Bandhakavi and
                  Md. Shad Akhtar and
                  Tanmoy Chakraborty},
  title        = {Intent-conditioned and Non-toxic Counterspeech Generation using Multi-Task
                  Instruction Tuning with {RLAIF}},
  booktitle    = {Proceedings of the 2024 Conference of the North American Chapter of
                  the Association for Computational Linguistics: Human Language Technologies
                  (Volume 1: Long Papers), {NAACL} 2024},
  pages        = {6716--6733},
  publisher    = {Association for Computational Linguistics},
  year         = {2024}
}

@inproceedings{InfrastructureOmbudsman,
author = {Chowdhury, Md Towhidul Absar and Datta, Soumyajit and Sharma, Naveen and KhudaBukhsh, Ashiqur R.},
title = {Infrastructure Ombudsman: Mining Future Failure Concerns from Structural Disaster Response},
year = {2024},
isbn = {9798400701719},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3589334.3648153},
doi = {10.1145/3589334.3648153},
abstract = {Current research concentrates on studying discussions on social media related to structural failures to improve disaster response strategies. However, detecting social web posts discussing concerns about anticipatory failures is under-explored. If such concerns are channeled to the appropriate authorities, it can aid in the prevention and mitigation of potential infrastructural failures. In this paper, we develop an infrastructure ombudsman -- that automatically detects specific infrastructure concerns. Our work considers several recent structural failures in the US. We present a first-of-its-kind dataset of 2,662 social web instances for this novel task mined from Reddit and YouTube.},
booktitle = {Proceedings of the ACM on Web Conference 2024},
pages = {4664–4673},
numpages = {10},
series = {WWW '24}
}

@inproceedings{Vicarious,
  author       = {Tharindu Cyril Weerasooriya and
                  Sujan Dutta and
                  Tharindu Ranasinghe and
                  Marcos Zampieri and
                  Christopher Homan and
                  Ashiqur R. KhudaBukhsh},
  title        = {Vicarious Offense and Noise Audit of Offensive Speech Classifiers:
                  Unifying Human and Machine Disagreement on What is Offensive},
  booktitle    = {Proceedings of the 2023 Conference on Empirical Methods in Natural
                  Language Processing, {EMNLP} 2023},
  pages        = {11648--11668},
  publisher    = {Association for Computational Linguistics},
  year         = {2023}
}

@inproceedings{al2020identifying,
  title={Identifying and measuring annotator bias based on annotators’ demographic characteristics},
  author={Al Kuwatly, Hala and Wich, Maximilian and Groh, Georg},
  booktitle={Proceedings of the fourth workshop on online abuse and harms},
  pages={184--190},
  year={2020}
}

@inproceedings{bang-etal-2024-measuring,
    title = "Measuring Political Bias in Large Language Models: What Is Said and How It Is Said",
    author = "Bang, Yejin  and
      Chen, Delong  and
      Lee, Nayeon  and
      Fung, Pascale",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.600",
    doi = "10.18653/v1/2024.acl-long.600",
    pages = "11142--11159",
    abstract = "We propose to measure political bias in LLMs by analyzing both the content and style of their generated content regarding political issues. Existing benchmarks and measures focus on gender and racial biases. However, political bias exists in LLMs and can lead to polarization and other harms in downstream applications. In order to provide transparency to users, we advocate that there should be fine-grained and explainable measures of political biases generated by LLMs. Our proposed measure looks at different political issues such as reproductive rights and climate change, at both the content (the substance of the generation) and the style (the lexical polarity) of such bias. We measured the political bias in eleven open-sourced LLMs and showed that our proposed framework is easily scalable to other topics and is explainable."
}

@article{benesch2014countering,
  title={Countering dangerous speech: New ideas for genocide prevention},
  author={Benesch, Susan},
  journal={Available at SSRN 3686876},
  year={2014}
}

@inproceedings{chakravarthi-2020-hopeedi,
    title = "{H}ope{EDI}: A Multilingual Hope Speech Detection Dataset for Equality, Diversity, and Inclusion",
    author = "Chakravarthi, Bharathi Raja",
    editor = "Nissim, Malvina  and
      Patti, Viviana  and
      Plank, Barbara  and
      Durmus, Esin",
    booktitle = "Proceedings of the Third Workshop on Computational Modeling of People's Opinions, Personality, and Emotion's in Social Media",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.peoples-1.5",
    pages = "41--53",
    abstract = "Over the past few years, systems have been developed to control online content and eliminate abusive, offensive or hate speech content. However, people in power sometimes misuse this form of censorship to obstruct the democratic right of freedom of speech. Therefore, it is imperative that research should take a positive reinforcement approach towards online content that is encouraging, positive and supportive contents. Until now, most studies have focused on solving this problem of negativity in the English language, though the problem is much more than just harmful content. Furthermore, it is multilingual as well. Thus, we have constructed a Hope Speech dataset for Equality, Diversity and Inclusion (HopeEDI) containing user-generated comments from the social media platform YouTube with 28,451, 20,198 and 10,705 comments in English, Tamil and Malayalam, respectively, manually labelled as containing hope speech or not. To our knowledge, this is the first research of its kind to annotate hope speech for equality, diversity and inclusion in a multilingual setting. We determined that the inter-annotator agreement of our dataset using Krippendorff{'}s alpha. Further, we created several baselines to benchmark the resulting dataset and the results have been expressed using precision, recall and F1-score. The dataset is publicly available for the research community. We hope that this resource will spur further research on encouraging inclusive and responsive speech that reinforces positiveness."
}

@inproceedings{feng-etal-2023-pretraining,
    title = "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair {NLP} Models",
    author = "Feng, Shangbin  and
      Park, Chan Young  and
      Liu, Yuhan  and
      Tsvetkov, Yulia",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.656",
    doi = "10.18653/v1/2023.acl-long.656",
    pages = "11737--11762",
    abstract = "Language models (LMs) are pretrained on diverse data sources{---}news, discussion forums, books, online encyclopedias. A significant portion of this data includes facts and opinions which, on one hand, celebrate democracy and diversity of ideas, and on the other hand are inherently socially biased. Our work develops new methods to (1) measure media biases in LMs trained on such corpora, along social and economic axes, and (2) measure the fairness of downstream NLP models trained on top of politically biased LMs. We focus on hate speech and misinformation detection, aiming to empirically quantify the effects of political (social, economic) biases in pretraining data on the fairness of high-stakes social-oriented tasks. Our findings reveal that pretrained LMs do have political leanings which reinforce the polarization present in pretraining corpora, propagating social biases into hate speech predictions and media biases into misinformation detectors. We discuss the implications of our findings for NLP research and propose future directions to mitigate unfairness."
}

@article{goyal2022your,
  title={Is your toxicity my toxicity? exploring the impact of rater identity on toxicity annotation},
  author={Goyal, Nitesh and Kivlichan, Ian D and Rosen, Rachel and Vasserman, Lucy},
  journal={Proceedings of the ACM on Human-Computer Interaction},
  volume={6},
  number={CSCW2},
  pages={1--28},
  year={2022},
  publisher={ACM New York, NY, USA}
}

@inproceedings{homan2024intersectionality,
  title={Intersectionality in AI Safety: Using Multilevel Models to Understand Diverse Perceptions of Safety in Conversational AI},
  author={Homan, Christopher and Serapio-Garcia, Gregory and Aroyo, Lora and D{\'\i}az, Mark and Parrish, Alicia and Prabhakaran, Vinodkumar and Taylor, Alex and Wang, Ding}

@article{huber2017political,
  title={Political homophily in social relationships: Evidence from online dating behavior},
  author={Huber, Gregory A and Malhotra, Neil},
  journal={The Journal of Politics},
  volume={79},
  number={1},
  pages={269--283},
  year={2017},
  publisher={University of Chicago Press Chicago, IL}
}

@article{iyengar2015fear,
  title={Fear and loathing across party lines: New evidence on group polarization},
  author={Iyengar, Shanto and Westwood, Sean J},
  journal={American Journal of Political Science},
  volume={59},
  number={3},
  pages={690--707},
  year={2015},
  publisher={Wiley Online Library}
}

@inproceedings{larimore2021reconsidering,
  title={Reconsidering annotator disagreement about racist language: Noise or signal?},
  author={Larimore, Savannah and Kennedy, Ian and Haskett, Breon and Arseniev-Koehler, Alina},
  booktitle={Proceedings of the Ninth International Workshop on Natural Language Processing for Social Media},
  pages={81--90},
  year={2021}
}

@inproceedings{mathew2019thou,
  title={Thou shalt not hate: Countering online hate speech},
  author={Mathew, Binny and Saha, Punyajoy and Tharad, Hardik and Rajgaria, Subham and Singhania, Prajwal and Maity, Suman Kalyan and Goyal, Pawan and Mukherjee, Animesh},
  booktitle={Proceedings of the international AAAI conference on web and social media},
  volume={13},
  pages={369--380},
  year={2019}
}

@article{palakodety2019hope,
  title={Hope speech detection: A computational analysis of the voice of peace},
  author={Palakodety, Shriphani and KhudaBukhsh, Ashiqur R and Carbonell, Jaime G},
  journal={arXiv preprint arXiv:1909.12940},
  year={2019}
}

@article{palakodety2020voice,
author = {Palakodety, Shriphani and Khudabukhsh, Ashiqur and Carbonell, Jaime},
year = {2020},
month = {04},
pages = {454-462},
title = {Voice for the Voiceless: Active Sampling to Detect Comments Supporting the Rohingyas},
volume = {34},
journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
doi = {10.1609/aaai.v34i01.5382}
}

@article{pavlick2019inherent,
  title={Inherent disagreements in human textual inferences},
  author={Pavlick, Ellie and Kwiatkowski, Tom},
  journal={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={677--694},
  year={2019},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@inproceedings{pei-jurgens-2023-annotator,
    title = "When Do Annotator Demographics Matter? Measuring the Influence of Annotator Demographics with the {POPQUORN} Dataset",
    author = "Pei, Jiaxin  and
      Jurgens, David",
    booktitle = "Proceedings of the 17th Linguistic Annotation Workshop (LAW-XVII)",
    month = jul,
    year = "2023",
    publisher = "Association for Computational Linguistics",
    pages = "252--265",
    abstract = "Annotators are not fungible. Their demographics, life experiences, and backgrounds all contribute to how they label data. However, NLP has only recently considered how annotator identity might influence their decisions. Here, we present POPQUORN (the Potato-Prolific dataset for Question-Answering, Offensiveness, text Rewriting and politeness rating with demographic Nuance). POPQUORN contains 45,000 annotations from 1,484 annotators, drawn from a representative sample regarding sex, age, and race as the US population. Through a series of analyses, we show that annotators{'} background plays a significant role in their judgments. Further, our work shows that backgrounds not previously considered in NLP (e.g., education), are meaningful and should be considered. Our study suggests that understanding the background of annotators and collecting labels from a demographically balanced pool of crowd workers is important to reduce the bias of datasets. The dataset, annotator background, and annotation interface are available at \url{https://github.com/Jiaxin-Pei/potato-prolific-dataset}."
}

@article{poole1984polarization,
  title={The polarization of American politics},
  author={Poole, Keith T and Rosenthal, Howard},
  journal={The journal of politics},
  volume={46},
  number={4},
  pages={1061--1079},
  year={1984},
  publisher={Southern Political Science Association}
}

@inproceedings{prabhakaran2024grasp,
  title={GRASP: A Disagreement Analysis Framework to Assess Group Associations in Perspectives},
  author={Prabhakaran, Vinodkumar and Homan, Christopher and Aroyo, Lora and Davani, Aida Mostafazadeh and Parrish, Alicia and Taylor, Alex and D{\'\i}az, Mark and Wang, Ding and Serapio-Garc{\'\i}a, Gregory},
  booktitle={Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
  pages={3473--3492},
  year={2024}
}

@inproceedings{sap-etal-2022-annotators,
    title = "Annotators with Attitudes: How Annotator Beliefs And Identities Bias Toxic Language Detection",
    author = "Sap, Maarten  and
      Swayamdipta, Swabha  and
      Vianna, Laura  and
      Zhou, Xuhui  and
      Choi, Yejin  and
      Smith, Noah A.",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.431",
    doi = "10.18653/v1/2022.naacl-main.431",
    pages = "5884--5906",
    abstract = "The perceived toxicity of language can vary based on someone{'}s identity and beliefs, but this variation is often ignored when collecting toxic language datasets, resulting in dataset and model biases. We seek to understand the *who*, *why*, and *what* behind biases in toxicity annotations. In two online studies with demographically and politically diverse participants, we investigate the effect of annotator identities (*who*) and beliefs (*why*), drawing from social psychology research about hate speech, free speech, racist beliefs, political leaning, and more. We disentangle *what* is annotated as toxic by considering posts with three characteristics: anti-Black language, African American English (AAE) dialect, and vulgarity. Our results show strong associations between annotator identity and beliefs and their ratings of toxicity. Notably, more conservative annotators and those who scored highly on our scale for racist beliefs were less likely to rate anti-Black language as toxic, but more likely to rate AAE as toxic. We additionally present a case study illustrating how a popular toxicity detection system{'}s ratings inherently reflect only specific beliefs and perspectives. Our findings call for contextualizing toxicity labels in social variables, which raises immense implications for toxic language annotation and detection."
}

@inproceedings{sap2019risk,
  title={The risk of racial bias in hate speech detection},
  author={Sap, Maarten and Card, Dallas and Gabriel, Saadia and Choi, Yejin and Smith, Noah A},
  booktitle={Proceedings of the 57th annual meeting of the association for computational linguistics},
  pages={1668--1678},
  year={2019}
}

@inproceedings{scheffer2001active,
  title={Active hidden markov models for information extraction},
  author={Scheffer, Tobias and Decomain, Christian and Wrobel, Stefan},
  booktitle={International Symposium on Intelligent Data Analysis},
  pages={309--318},
  year={2001},
  organization={Springer}
}

@inproceedings{sindhwani2009uncertainty,
  title={Uncertainty sampling and transductive experimental design for active dual supervision},
  author={Sindhwani, Vikas and Melville, Prem and Lawrence, Richard D},
  booktitle={ICML},
  pages={953--960},
  year={2009}
}

