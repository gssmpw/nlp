\section{Related work}
\subsection{Benchmarks}
Before the advent of the MTEB____, the evaluation of text embeddings was fragmented across various task-specific and domain-specific benchmarks. Early efforts like the semantic textual similarity benchmarks, including STS-Benchmark____ and SICK____ datasets, primarily assessed embeddings for their ability to capture semantic relationships between text pairs. While effective in measuring specific aspects, % like semantics, 
these benchmarks were narrow in scope, focusing on small datasets and failing to represent real-world diversity.

The GLUE____ Benchmark broadened the evaluation landscape by incorporating tasks such as natural language inference, sentiment analysis, and sentence similarity. However, the benchmark is dedicated exclusively to the English language, limiting its applicability to multilingual NLU research and offering limited insight into the generalization capabilities of raw embeddings. Similarly, information retrieval benchmarks like MS MARCO____ evaluate embeddings for domain-specific applications, such as search engine optimization. While these benchmarks are invaluable for retrieval tasks, they do not generalize %lack the ability to generalize 
effectively across a wide range of embedding evaluation use-cases.


The fragmented nature of these benchmarks underscore several limitations. First, task diversity is insufficient; most benchmarks aim at specific applications such as similarity, classification, or retrieval without covering clustering or zero-shot classification. 
% Second, there was limited focus on multilingual evaluation, despite the increasing demand for embeddings that perform well across languages.
Second, inconsistencies in evaluation protocols and metrics make it challenging to compare results across models. Finally, existing benchmarks are not designed for scalability or extensibility; this complicates incorporating %making it difficult to incorporate 
new datasets or tasks as embedding techniques evolve.

These limitations led to the creation of MTEB, a unified and comprehensive benchmark that addresses these gaps. MTEB evaluates text embeddings in a wide range of tasks, including semantic similarity, clustering, classification, retrieval, bitext mining, pair classification, and reranking.

Despite the advancements brought by MTEB, its main focus remains on the English language, creating a need for benchmarks tailored to other languages. Although MTEB supports multilingual evaluation, the representation of certain languages and specific linguistic nuances can be limited. To address this shortcoming, new benchmarks inspired by MTEB have been introduced for languages such as Chinese____, Polish____, and French____. These language-specific benchmarks aim to evaluate the quality of text embeddings in tasks and datasets that reflect the unique linguistic and cultural characteristics of these languages, ensuring broader applicability of text embedding models across the global linguistic landscape.

For the Persian language, besides a limited number of evaluation datasets for isolated tasks, no comprehensive evaluation dataset for text embedding models has been introduced yet. %Only a limited number of evaluations have been published for isolated tasks. 
In the STS task, the Farsick____ dataset is available, which is a machine-translated version of the SICK dataset. %, has been provided. 
For the IR task, the multilingual MIRACL____ dataset  supports the Persian language. %However, despite these datasets, no comprehensive dataset capable of evaluating the overall capabilities of text embedding models has yet been introduced.

\subsection{Embedding models}
The evolution of text embedding models could be seen as a significant shift from traditional methods like GloVe____ to more advanced context-sensitive models. GloVe and Word2Vec____ set the foundation for dense word embeddings by representing words as fixed vectors based on co-occurrence statistics. Although these models worked well to capture word-level semantics, they were limited by their static nature, which did not account for polysemy or contextual meaning.

This limitation was addressed with the introduction of ELMo____ and later BERT____, which offered contextual embeddings by considering the surrounding text. BERT, leveraging the transformer architecture, became a breakthrough by generating deep bidirectional embeddings, capturing richer contextual information. However, BERT's focus on token-level embeddings necessitated fine-tuning for specific tasks, which could be computationally expensive.

To solve this, sentence transformers, like SBERT____ were developed to provide high-quality, task-specific sentence embeddings. These models used the transformer architecture with a Siamese network structure to generate embeddings suitable for tasks such as semantic similarity and information retrieval, making them efficient and versatile for sentence-level understanding.

In recent years, open-source models such as BGE____, E5____, and GTE____ have been introduced for text embedding, demonstrating strong performance in tasks like STS, retrieval, and clustering in English. In the Persian language, various foundational models such as ParsBERT____, FaBERT____, and TookaBERT____ have been released, which are capable of understanding textual information well; however, they cannot be directly applied to a wide range of tasks. To date, models that perform well in Persian text embedding tasks mainly consist of multilingual networks such as BGE-m3____, mE5____, and OpenAI's text embedding models text-embedding-3 and text-embedding-4.