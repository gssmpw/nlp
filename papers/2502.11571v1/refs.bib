@article{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}

@article{karpukhin2020dense,
  title={Dense passage retrieval for open-domain question answering},
  author={Karpukhin, Vladimir and O{\u{g}}uz, Barlas and Min, Sewon and Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau},
  journal={arXiv preprint arXiv:2004.04906},
  year={2020}
}

@inproceedings{muennighoff-etal-2023-mteb,
    title = "{MTEB}: Massive Text Embedding Benchmark",
    author = "Muennighoff, Niklas  and
      Tazi, Nouamane  and
      Magne, Loic  and
      Reimers, Nils",
    editor = "Vlachos, Andreas  and
      Augenstein, Isabelle",
    booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.eacl-main.148",
    doi = "10.18653/v1/2023.eacl-main.148",
    pages = "2014--2037",
    abstract = "Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings todate. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-theart results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at \url{https://github.com/embeddings-benchmark/mteb}.",
}

@inproceedings{cer-etal-2017-semeval,
    title = "{S}em{E}val-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation",
    author = "Cer, Daniel  and
      Diab, Mona  and
      Agirre, Eneko  and
      Lopez-Gazpio, I{\~n}igo  and
      Specia, Lucia",
    editor = "Bethard, Steven  and
      Carpuat, Marine  and
      Apidianaki, Marianna  and
      Mohammad, Saif M.  and
      Cer, Daniel  and
      Jurgens, David",
    booktitle = "Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017)",
    month = aug,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/S17-2001",
    doi = "10.18653/v1/S17-2001",
    pages = "1--14",
    abstract = "Semantic Textual Similarity (STS) measures the meaning similarity of sentences. Applications include machine translation (MT), summarization, generation, question answering (QA), short answer grading, semantic search, dialog and conversational systems. The STS shared task is a venue for assessing the current state-of-the-art. The 2017 task focuses on multilingual and cross-lingual pairs with one sub-track exploring MT quality estimation (MTQE) data. The task obtained strong participation from 31 teams, with 17 participating in \textit{all language tracks}. We summarize performance and review a selection of well performing methods. Analysis highlights common errors, providing insight into the limitations of existing models. To support ongoing work on semantic representations, the \textit{STS Benchmark} is introduced as a new shared training and evaluation set carefully selected from the corpus of English STS shared task data (2012-2017).",
}

@inproceedings{marelli-etal-2014-sick,
    title = "A {SICK} cure for the evaluation of compositional distributional semantic models",
    author = "Marelli, Marco  and
      Menini, Stefano  and
      Baroni, Marco  and
      Bentivogli, Luisa  and
      Bernardi, Raffaella  and
      Zamparelli, Roberto",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Loftsson, Hrafn  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)",
    month = may,
    year = "2014",
    address = "Reykjavik, Iceland",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2014/pdf/363_Paper.pdf",
    pages = "216--223",
    abstract = "Shared and internationally recognized benchmarks are fundamental for the development of any computational system. We aim to help the research community working on compositional distributional semantic models (CDSMs) by providing SICK (Sentences Involving Compositional Knowldedge), a large size English benchmark tailored for them. SICK consists of about 10,000 English sentence pairs that include many examples of the lexical, syntactic and semantic phenomena that CDSMs are expected to account for, but do not require dealing with other aspects of existing sentential data sets (idiomatic multiword expressions, named entities, telegraphic language) that are not within the scope of CDSMs. By means of crowdsourcing techniques, each pair was annotated for two crucial semantic tasks: relatedness in meaning (with a 5-point rating scale as gold score) and entailment relation between the two elements (with three possible gold labels: entailment, contradiction, and neutral). The SICK data set was used in SemEval-2014 Task 1, and it freely available for research purposes.",
}

@inproceedings{wang-etal-2018-glue,
    title = "{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
    author = "Wang, Alex  and
      Singh, Amanpreet  and
      Michael, Julian  and
      Hill, Felix  and
      Levy, Omer  and
      Bowman, Samuel",
    editor = "Linzen, Tal  and
      Chrupa{\l}a, Grzegorz  and
      Alishahi, Afra",
    booktitle = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-5446",
    doi = "10.18653/v1/W18-5446",
    pages = "353--355",
    abstract = "Human ability to understand language is \textit{general, flexible, and robust}. In contrast, most NLU models above the word level are designed for a specific task and struggle with out-of-domain data. If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a unified model that can execute a range of linguistic tasks across different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE, gluebenchmark.com): a benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo (Peters et al., 2018), a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested, with some exceptions.",
}

@article{DBLP:journals/corr/NguyenRSGTMD16,
  author       = {Tri Nguyen and
                  Mir Rosenberg and
                  Xia Song and
                  Jianfeng Gao and
                  Saurabh Tiwary and
                  Rangan Majumder and
                  Li Deng},
  title        = {{MS} {MARCO:} {A} Human Generated MAchine Reading COmprehension Dataset},
  journal      = {CoRR},
  volume       = {abs/1611.09268},
  year         = {2016},
  url          = {http://arxiv.org/abs/1611.09268},
  eprinttype    = {arXiv},
  eprint       = {1611.09268},
  timestamp    = {Thu, 11 Apr 2024 13:33:57 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/NguyenRSGTMD16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{10.1145/3626772.3657878,
author = {Xiao, Shitao and Liu, Zheng and Zhang, Peitian and Muennighoff, Niklas and Lian, Defu and Nie, Jian-Yun},
title = {C-Pack: Packed Resources For General Chinese Embeddings},
year = {2024},
isbn = {9798400704314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626772.3657878},
doi = {10.1145/3626772.3657878},
abstract = {We introduce C-Pack, a package of resources that significantly advances the field of general text embeddings for Chinese. C-Pack includes three critical resources. 1) C-MTP is a massive training dataset for text embedding, which is based on the curation of vast unlabeled corpora and the integration of high-quality labeled corpora. 2) C-MTEB is a comprehensive benchmark for Chinese text embeddings covering 6 tasks and 35 datasets. 3) BGE is a family of embedding models covering multiple sizes. Our models outperform all prior Chinese text embeddings on C-MTEB by more than +10\% upon the time of the release. We also integrate and optimize the entire suite of training methods for BGE. Along with our resources on general Chinese embedding, we release our data and models for English text embeddings. The English models also achieve state-of-the-art performance on the MTEB benchmark; meanwhile, our released English data is 2 times larger than the Chinese data. Both Chinese and English datasets are the largest public release of training data for text embeddings. All these resources are made publicly available at https://github.com/FlagOpen/FlagEmbedding.},
booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {641â€“649},
numpages = {9},
keywords = {benchmark, pre-trained models, text embeddings, training data},
location = {Washington DC, USA},
series = {SIGIR '24}
}

@article{poswiata2024pl,
  title={PL-MTEB: Polish Massive Text Embedding Benchmark},
  author={Po{\'s}wiata, Rafa{\l} and Dadas, S{\l}awomir and Pere{\l}kiewicz, Micha{\l}},
  journal={arXiv preprint arXiv:2405.10138},
  year={2024}
}

@article{ciancone2024mteb,
  title={MTEB-French: Resources for french sentence embedding evaluation and analysis},
  author={Ciancone, Mathieu and Kerboua, Imene and Schaeffer, Marion and Siblini, Wissam},
  journal={arXiv preprint arXiv:2405.20468},
  year={2024}
}

@INPROCEEDINGS{9721521,
  author={Ghasemi, Zahra and Keyvanrad, Mohammad Ali},
  booktitle={2021 11th International Conference on Computer Engineering and Knowledge (ICCKE)}, 
  title={FarSick: A Persian Semantic Textual Similarity And Natural Language Inference Dataset}, 
  year={2021},
  volume={},
  number={},
  pages={194-199},
  keywords={Measurement;Weight measurement;Plagiarism;Natural languages;Semantics;Text categorization;Neural networks;Persian dataset;Semantic Textual Similarity;Natural Language Inference;paraphrase expressions;plagiarism detection;deep learning;Natural Language Processing},
  doi={10.1109/ICCKE54056.2021.9721521}}

@article{zhang-etal-2023-miracl,
    title = "{MIRACL}: A Multilingual Retrieval Dataset Covering 18 Diverse Languages",
    author = "Zhang, Xinyu  and
      Thakur, Nandan  and
      Ogundepo, Odunayo  and
      Kamalloo, Ehsan  and
      Alfonso-Hermelo, David  and
      Li, Xiaoguang  and
      Liu, Qun  and
      Rezagholizadeh, Mehdi  and
      Lin, Jimmy",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "11",
    year = "2023",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2023.tacl-1.63",
    doi = "10.1162/tacl_a_00595",
    pages = "1114--1131",
    abstract = "MIRACL is a multilingual dataset for ad hoc retrieval across 18 languages that collectively encompass over three billion native speakers around the world. This resource is designed to support monolingual retrieval tasks, where the queries and the corpora are in the same language. In total, we have gathered over 726k high-quality relevance judgments for 78k queries over Wikipedia in these languages, where all annotations have been performed by native speakers hired by our team. MIRACL covers languages that are both typologically close as well as distant from 10 language families and 13 sub-families, associated with varying amounts of publicly available resources. Extensive automatic heuristic verification and manual assessments were performed during the annotation process to control data quality. In total, MIRACL represents an investment of around five person-years of human annotator effort. Our goal is to spur research on improving retrieval across a continuum of languages, thus enhancing information access capabilities for diverse populations around the world, particularly those that have traditionally been underserved. MIRACL is available at http://miracl.ai/.",
}

@inproceedings{pennington-etal-2014-glove,
    title = "{G}lo{V}e: Global Vectors for Word Representation",
    author = "Pennington, Jeffrey  and
      Socher, Richard  and
      Manning, Christopher",
    editor = "Moschitti, Alessandro  and
      Pang, Bo  and
      Daelemans, Walter",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1162",
    doi = "10.3115/v1/D14-1162",
    pages = "1532--1543",
}

@inproceedings{10.5555/2999792.2999959,
author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
title = {Distributed representations of words and phrases and their compositionality},
year = {2013},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling.An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
booktitle = {Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3111â€“3119},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'13}
}

@inproceedings{peters-etal-2018-deep,
    title = "Deep Contextualized Word Representations",
    author = "Peters, Matthew E.  and
      Neumann, Mark  and
      Iyyer, Mohit  and
      Gardner, Matt  and
      Clark, Christopher  and
      Lee, Kenton  and
      Zettlemoyer, Luke",
    editor = "Walker, Marilyn  and
      Ji, Heng  and
      Stent, Amanda",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1202",
    doi = "10.18653/v1/N18-1202",
    pages = "2227--2237",
    abstract = "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
}

@inproceedings{Devlin2019BERTPO,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:52967399}
}

@inproceedings{Reimers2019SentenceBERTSE,
  title={Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks},
  author={Nils Reimers and Iryna Gurevych},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:201646309}
}

@article{Wang2022TextEB,
  title={Text Embeddings by Weakly-Supervised Contrastive Pre-training},
  author={Liang Wang and Nan Yang and Xiaolong Huang and Binxing Jiao and Linjun Yang and Daxin Jiang and Rangan Majumder and Furu Wei},
  journal={ArXiv},
  year={2022},
  volume={abs/2212.03533},
  url={https://api.semanticscholar.org/CorpusID:254366618}
}

@misc{li2023generaltextembeddingsmultistage,
      title={Towards General Text Embeddings with Multi-stage Contrastive Learning}, 
      author={Zehan Li and Xin Zhang and Yanzhao Zhang and Dingkun Long and Pengjun Xie and Meishan Zhang},
      year={2023},
      eprint={2308.03281},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2308.03281}, 
}

@article{Farahani2020ParsBERTTM,
  title={ParsBERT: Transformer-based Model for Persian Language Understanding},
  author={Mehrdad Farahani and Mohammad Gharachorloo and Marzieh Farahani and Mohammad Manthouri},
  journal={Neural Processing Letters},
  year={2020},
  volume={53},
  pages={3831 - 3847},
  url={https://api.semanticscholar.org/CorpusID:218889376}
}

@article{Masumi2024FaBERTPB,
  title={FaBERT: Pre-training BERT on Persian Blogs},
  author={Mostafa Masumi and Seyed Soroush Majd and Mehrnoush Shamsfard and Hamid Beigy},
  journal={ArXiv},
  year={2024},
  volume={abs/2402.06617},
  url={https://api.semanticscholar.org/CorpusID:267617229}
}

@misc{sadraeijavaheri2024tookabertstepforwardpersian,
      title={TookaBERT: A Step Forward for Persian NLU}, 
      author={MohammadAli SadraeiJavaheri and Ali Moghaddaszadeh and Milad Molazadeh and Fariba Naeiji and Farnaz Aghababaloo and Hamideh Rafiee and Zahra Amirmahani and Tohid Abedini and Fatemeh Zahra Sheikhi and Amirmohammad Salehoof},
      year={2024},
      eprint={2407.16382},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.16382}, 
}

@inproceedings{chen-etal-2024-m3,
    title = "{M}3-Embedding: Multi-Linguality, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation",
    author = "Chen, Jianlyu  and
      Xiao, Shitao  and
      Zhang, Peitian  and
      Luo, Kun  and
      Lian, Defu  and
      Liu, Zheng",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.137",
    doi = "10.18653/v1/2024.findings-acl.137",
    pages = "2318--2335",
    abstract = "In this paper, we introduce a new embedding model called M3-Embedding, which is distinguished for its versatility in Multi-Linguality, Multi-Functionality, and Multi-Granularity. It provides a uniform support for the semantic retrieval of more than 100 working languages. It can simultaneously accomplish the three common retrieval functionalities: dense retrieval, multi-vector retrieval, and sparse retrieval. Besides, it is also capable of processing inputs of different granularities, spanning from short sentences to long documents of up to 8,192 tokens. The effective training of M3-Embedding presents a series of technical contributions. Notably, we propose a novel self-knowledge distillation approach, where the relevance scores from different retrieval functionalities can be integrated as the teacher signal to enhance the training quality. We also optimize the batching strategy, which enables a large batch size and high training throughput to improve the discriminativeness of embeddings. M3-Embedding exhibits a superior performance in our experiment, leading to new state-of-the-art results on multilingual, cross-lingual, and long-document retrieval benchmarks.",
}

@misc{wang2024multilinguale5textembeddings,
      title={Multilingual E5 Text Embeddings: A Technical Report}, 
      author={Liang Wang and Nan Yang and Xiaolong Huang and Linjun Yang and Rangan Majumder and Furu Wei},
      year={2024},
      eprint={2402.05672},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.05672}, 
}

@inproceedings{rosenberg-hirschberg-2007-v,
    title = "{V}-Measure: A Conditional Entropy-Based External Cluster Evaluation Measure",
    author = "Rosenberg, Andrew  and
      Hirschberg, Julia",
    editor = "Eisner, Jason",
    booktitle = "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL})",
    month = jun,
    year = "2007",
    address = "Prague, Czech Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D07-1043",
    pages = "410--420",
}

@article{Thakur2021BEIRAH,
  title={BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models},
  author={Nandan Thakur and Nils Reimers and Andreas Ruckl'e and Abhishek Srivastava and Iryna Gurevych},
  journal={ArXiv},
  year={2021},
  volume={abs/2104.08663},
  url={https://api.semanticscholar.org/CorpusID:233296016}
}

@article{DBLP:journals/corr/abs-2108-13897,
  author       = {Luiz Henrique Bonifacio and
                  Israel Campiotti and
                  Roberto A. Lotufo and
                  Rodrigo Frassetto Nogueira},
  title        = {mMARCO: {A} Multilingual Version of {MS} {MARCO} Passage Ranking Dataset},
  journal      = {CoRR},
  volume       = {abs/2108.13897},
  year         = {2021},
  url          = {https://arxiv.org/abs/2108.13897},
  eprinttype    = {arXiv},
  eprint       = {2108.13897},
  timestamp    = {Tue, 26 Nov 2024 13:45:00 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2108-13897.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{10553090,
  author={Zinvandi, Erfan and Alikhani, Morteza and Pourbahman, Zahra and Kazemi, Reza and Amini, Arash},
  booktitle={2024 12th Iran Workshop on Communication and Information Theory (IWCIT)}, 
  title={Persian Web Document Retrieval Corpus}, 
  year={2024},
  volume={},
  number={},
  pages={1-3},
  keywords={Conferences;Search engines;Internet;Task analysis;Information theory;Ad hoc information retrieval;passage reranking;Persian text collection},
  doi={10.1109/IWCIT62550.2024.10553090}}

@misc{nlp_twitter_analysis,
  author       = {Hamed Feizabadi},
  title        = {hamedhf/nlp\_twitter\_analysis},
  year         = {2023},
  url          = {https://huggingface.co/datasets/hamedhf/nlp_twitter_analysis},
}

@misc{hezar2023,
  title =        {Hezar: The all-in-one AI library for Persian},
  author =       {Aryan Shekarlaban and Pooya Mohammadi Kazaj},
  publisher =    {GitHub},
  howpublished = {\url{https://github.com/hezarai/hezar}},
  year =         {2023}
}

@article{PourmostafaRoshanSharami2020DeepSentiPersND,
  title={DeepSentiPers: Novel Deep Learning Models Trained Over Proposed Augmented Persian Sentiment Corpus},
  author={Javad Pourmostafa Roshan Sharami and Parsa Abbasi Sarabestani and Seyed Abolghasem Mirroshandel},
  journal={ArXiv},
  year={2020},
  volume={abs/2004.05328},
  url={https://api.semanticscholar.org/CorpusID:215745014}
}

@article{Hosseini2018SentiPersAS,
  title={SentiPers: A Sentiment Analysis Corpus for Persian},
  author={Pedram Hosseini and Ali Ahmadian Ramaki and Hassan Maleki and Mansoureh Anvari and Seyed Abolghasem Mirroshandel},
  journal={ArXiv},
  year={2018},
  volume={abs/1801.07737},
  url={https://api.semanticscholar.org/CorpusID:20559797}
}

@misc{farsi_news,
  author       = {Mehdi Allahyar},
  title        = {community-datasets/farsi\_news},
  year         = {2020},
  url          = {https://huggingface.co/datasets/community-datasets/farsi_news},
}

@article{Lawrie2023OverviewOT,
  title={Overview of the TREC 2022 NeuCLIR Track},
  author={Dawn J Lawrie and Sean MacAvaney and James Mayfield and Paul McNamee and Douglas W. Oard and Luca Soldaini and Eugene Yang},
  journal={ArXiv},
  year={2023},
  volume={abs/2304.12367},
  url={https://api.semanticscholar.org/CorpusID:258309196}
}

@misc{lawrie2024overviewtrec2023neuclir,
      title={Overview of the TREC 2023 NeuCLIR Track}, 
      author={Dawn Lawrie and Sean MacAvaney and James Mayfield and Paul McNamee and Douglas W. Oard and Luca Soldaini and Eugene Yang},
      year={2024},
      eprint={2404.08071},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2404.08071}, 
}

@ONLINE{wikidump,
    author = "Wikimedia Foundation",
    title  = "Wikimedia Downloads",
    year   =  {2023},
    url    = "https://dumps.wikimedia.org"
}

@INPROCEEDINGS{9786243,
  author={Sadeghi, Reyhaneh and Karbasi, Hamed and Akbari, Ahmad},
  booktitle={2022 8th International Conference on Web Research (ICWR)}, 
  title={ExaPPC: a Large-Scale Persian Paraphrase Detection Corpus}, 
  year={2022},
  volume={},
  number={},
  pages={168-175},
  doi={10.1109/ICWR54782.2022.9786243}}

@article{DBLP:journals/corr/abs-2009-08820,
  author       = {Hossein Amirkhani and
                  Mohammad AzariJafari and
                  Zohreh Pourjafari and
                  Soroush Faridan{-}Jahromi and
                  Zeinab Kouhkan and
                  Azadeh Amirak},
  title        = {FarsTail: {A} Persian Natural Language Inference Dataset},
  journal      = {CoRR},
  volume       = {abs/2009.08820},
  year         = {2020},
  url          = {https://arxiv.org/abs/2009.08820},
  eprinttype    = {arXiv},
  eprint       = {2009.08820},
  timestamp    = {Thu, 23 Sep 2021 11:46:10 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2009-08820.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{khashabi-etal-2021-parsinlu,
    title = "{P}arsi{NLU}: A Suite of Language Understanding Challenges for {P}ersian",
    author = "Khashabi, Daniel  and
      Cohan, Arman  and
      Shakeri, Siamak  and
      Hosseini, Pedram  and
      Pezeshkpour, Pouya  and
      Alikhani, Malihe  and
      Aminnaseri, Moin  and
      Bitaab, Marzieh  and
      Brahman, Faeze  and
      Ghazarian, Sarik  and
      Gheini, Mozhdeh  and
      Kabiri, Arman  and
      Mahabagdi, Rabeeh Karimi  and
      Memarrast, Omid  and
      Mosallanezhad, Ahmadreza  and
      Noury, Erfan  and
      Raji, Shahab  and
      Rasooli, Mohammad Sadegh  and
      Sadeghi, Sepideh  and
      Azer, Erfan Sadeqi  and
      Samghabadi, Niloofar Safi  and
      Shafaei, Mahsa  and
      Sheybani, Saber  and
      Tazarv, Ali  and
      Yaghoobzadeh, Yadollah",
    editor = "Roark, Brian  and
      Nenkova, Ani",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "9",
    year = "2021",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2021.tacl-1.68/",
    doi = "10.1162/tacl_a_00419",
    pages = "1147--1162",
    abstract = "Despite the progress made in recent years in addressing natural language understanding (NLU) challenges, the majority of this progress remains to be concentrated on resource-rich languages like English. This work focuses on Persian language, one of the widely spoken languages in the world, and yet there are few NLU datasets available for this language. The availability of high-quality evaluation datasets is a necessity for reliable assessment of the progress on different NLU tasks and domains. We introduce ParsiNLU, the first benchmark in Persian language that includes a range of language understanding tasks{---}reading comprehension, textual entailment, and so on. These datasets are collected in a multitude of ways, often involving manual annotations by native speakers. This results in over 14.5k new instances across 6 distinct NLU tasks. Additionally, we present the first results on state-of-the-art monolingual and multilingual pre-trained language models on this benchmark and compare them with human performance, which provides valuable insights into our ability to tackle natural language understanding challenges in Persian. We hope ParsiNLU fosters further research and advances in Persian language understanding.1"
}

@inproceedings{fitzgerald-etal-2023-massive,
    title = "{MASSIVE}: A 1{M}-Example Multilingual Natural Language Understanding Dataset with 51 Typologically-Diverse Languages",
    author = "FitzGerald, Jack  and
      Hench, Christopher  and
      Peris, Charith  and
      Mackie, Scott  and
      Rottmann, Kay  and
      Sanchez, Ana  and
      Nash, Aaron  and
      Urbach, Liam  and
      Kakarala, Vishesh  and
      Singh, Richa  and
      Ranganath, Swetha  and
      Crist, Laurie  and
      Britan, Misha  and
      Leeuwis, Wouter  and
      Tur, Gokhan  and
      Natarajan, Prem",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.235/",
    doi = "10.18653/v1/2023.acl-long.235",
    pages = "4277--4302",
    abstract = "We present the MASSIVE dataset{--}Multilingual Amazon Slu resource package (SLURP) for Slot-filling, Intent classification, and Virtual assistant Evaluation. MASSIVE contains 1M realistic, parallel, labeled virtual assistant utterances spanning 51 languages, 18 domains, 60 intents, and 55 slots. MASSIVE was created by tasking professional translators to localize the English-only SLURP dataset into 50 typologically diverse languages from 29 genera. We also present modeling results on XLM-R and mT5, including exact match accuracy, intent classification accuracy, and slot-filling F1 score. We have released our dataset, modeling code, and models publicly."
}

@misc{farsi_paraphrase_detection,
  author       = {Ali Ghasemi},
  title        = {alighasemi/farsi\_paraphrase\_detection},
  year         = {2022},
  url          = {https://huggingface.co/datasets/alighasemi/farsi_paraphrase_detection},
}

@misc{persian_text_emotion,
  author       = {Seyed Ali Mir Mohammad Hosseini},
  title        = {SeyedAli/Persian-Text-Emotion},
  year         = {2022},
  url          = {https://huggingface.co/datasets/SeyedAli/Persian-Text-Emotion},
}



@inproceedings{zhang-etal-2024-mgte,
    title = "{mGTE}: Generalized Long-Context Text Representation and Reranking Models for Multilingual Text Retrieval",
    author = "Zhang, Xin  and
      Zhang, Yanzhao  and
      Long, Dingkun  and
      Xie, Wen  and
      Dai, Ziqi  and
      Tang, Jialong  and
      Lin, Huan  and
      Yang, Baosong  and
      Xie, Pengjun  and
      Huang, Fei  and
      Zhang, Meishan  and
      Li, Wenjie  and
      Zhang, Min",
    editor = "Dernoncourt, Franck  and
      Preo{\c{t}}iuc-Pietro, Daniel  and
      Shimorina, Anastasia",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track",
    month = nov,
    year = "2024",
    address = "Miami, Florida, US",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-industry.103/",
    doi = "10.18653/v1/2024.emnlp-industry.103",
    pages = "1393--1412",
    abstract = "We present systematic efforts in building long-context multilingual text representation model (TRM) and reranker from scratch for text retrieval. We first introduce a text encoder (base size) enhanced with RoPE and unpadding, pre-trained in a native 8192-token context (longer than 512 of previous multilingual encoders). Then we construct a hybrid TRM and a cross-encoder reranker by contrastive learning. Evaluations show that our text encoder outperforms the same-sized previous state-of-the-art XLM-R. Meanwhile, our TRM and reranker match the performance of large-sized state-of-the-art BGE-M3 models and achieve better results on long-context retrieval benchmarks. Further analysis demonstrate that our proposed models exhibit higher efficiency during both training and inference. We believe their efficiency and effectiveness could benefit various researches and industrial applications."
}

@misc{sturua2024jinaembeddingsv3multilingualembeddingstask,
      title={jina-embeddings-v3: Multilingual Embeddings With Task LoRA}, 
      author={Saba Sturua and Isabelle Mohr and Mohammad Kalim Akram and Michael GÃ¼nther and Bo Wang and Markus Krimmel and Feng Wang and Georgios Mastrapas and Andreas Koukounas and Andreas Koukounas and Nan Wang and Han Xiao},
      year={2024},
      eprint={2409.10173},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.10173}, 
}

@inproceedings{feng-etal-2022-language,
    title = "Language-agnostic {BERT} Sentence Embedding",
    author = "Feng, Fangxiaoyu  and
      Yang, Yinfei  and
      Cer, Daniel  and
      Arivazhagan, Naveen  and
      Wang, Wei",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.62/",
    doi = "10.18653/v1/2022.acl-long.62",
    pages = "878--891",
    abstract = "While BERT is an effective method for learning monolingual sentence embeddings for semantic similarity and embedding based transfer learning BERT based cross-lingual sentence embeddings have yet to be explored. We systematically investigate methods for learning multilingual sentence embeddings by combining the best methods for learning monolingual and cross-lingual representations including: masked language modeling (MLM), translation language modeling (TLM), dual encoder translation ranking, and additive margin softmax. We show that introducing a pre-trained multilingual language model dramatically reduces the amount of parallel training data required to achieve good performance by 80{\%}. Composing the best of these methods produces a model that achieves 83.7{\%} bi-text retrieval accuracy over 112 languages on Tatoeba, well above the 65.5{\%} achieved by LASER, while still performing competitively on monolingual transfer learning benchmarks. Parallel data mined from CommonCrawl using our best model is shown to train competitive NMT models for en-zh and en-de. We publicly release our best multilingual sentence embedding model for 109+ languages at \url{https://tfhub.dev/google/LaBSE}."
}

@inproceedings{reimers-2019-sentence-bert,
    title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
    author = "Reimers, Nils and Gurevych, Iryna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
    month = "11",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    url = "http://arxiv.org/abs/1908.10084",
}