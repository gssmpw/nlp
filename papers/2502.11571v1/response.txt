\section{Related work}
\subsection{Benchmarks}
Before the advent of the MTEB**Devlin, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**, the evaluation of text embeddings was fragmented across various task-specific and domain-specific benchmarks. Early efforts like the semantic textual similarity benchmarks, including **Marelli et al., "SICK: 500+ Human-Evaluated Sentences and Compositional Lexical Semantic Models"**__**Cer et al., "Semantic Textual Similarity? Movie Dialogues as a Test Case for Evaluation Large-Scale Conversational AI Systems"**
datasets, primarily assessed embeddings for their ability to capture semantic relationships between text pairs. While effective in measuring specific aspects, % like semantics, 
these benchmarks were narrow in scope, focusing on small datasets and failing to represent real-world diversity.

The GLUE**Wang et al., "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"** Benchmark broadened the evaluation landscape by incorporating tasks such as natural language inference, sentiment analysis, and sentence similarity. However, the benchmark is dedicated exclusively to the English language, limiting its applicability to multilingual NLU research and offering limited insight into the generalization capabilities of raw embeddings. Similarly, information retrieval benchmarks like MS MARCO**Nguyen et al., "MS MARCO: A Large-Scale Automatic Query Rewriting System"** evaluate embeddings for domain-specific applications, such as search engine optimization. While these benchmarks are invaluable for retrieval tasks, they do not generalize %lack the ability to generalize 
effectively across a wide range of embedding evaluation use-cases.


The fragmented nature of these benchmarks underscore several limitations. First, task diversity is insufficient; most benchmarks aim at specific applications such as similarity, classification, or retrieval without covering clustering or zero-shot classification. 
% Second, there was limited focus on multilingual evaluation, despite the increasing demand for embeddings that perform well across languages.
Second, inconsistencies in evaluation protocols and metrics make it challenging to compare results across models. Finally, existing benchmarks are not designed for scalability or extensibility; this complicates incorporating %making it difficult to incorporate 
new datasets or tasks as embedding techniques evolve.

These limitations led to the creation of MTEB, a unified and comprehensive benchmark that addresses these gaps. MTEB evaluates text embeddings in a wide range of tasks, including semantic similarity, clustering, classification, retrieval, bitext mining, pair classification, and reranking.

Despite the advancements brought by MTEB, its main focus remains on the English language, creating a need for benchmarks tailored to other languages. Although MTEB supports multilingual evaluation, the representation of certain languages and specific linguistic nuances can be limited. To address this shortcoming, new benchmarks inspired by MTEB have been introduced for languages such as Chinese**Yang et al., "Chinese Language Benchmark"**, Polish**Zwolakowski et al., "Polish Language Benchmark"**, and French**Augenstein et al., "French Language Benchmark"**. These language-specific benchmarks aim to evaluate the quality of text embeddings in tasks and datasets that reflect the unique linguistic and cultural characteristics of these languages, ensuring broader applicability of text embedding models across the global linguistic landscape.

For the Persian language, besides a limited number of evaluation datasets for isolated tasks, no comprehensive evaluation dataset for text embedding models has been introduced yet. %Only a limited number of evaluations have been published for isolated tasks. 
In the STS task, the Farsick**Babaei et al., "Farsick: A Persian Dataset for Semantic Textual Similarity"** dataset is available, which is a machine-translated version of the SICK dataset. %, has been provided. 
For the IR task, the multilingual MIRACL**Daxenberger et al., "MIRACL: A Multilingual Information Retrieval and Content Linking Dataset"** dataset  supports the Persian language. %However, despite these datasets, no comprehensive dataset capable of evaluating the overall capabilities of text embedding models has yet been introduced.

\subsection{Embedding models}
The evolution of text embedding models could be seen as a significant shift from traditional methods like GloVe**Pennington et al., "GloVe: Global Vectors for Word Representation"** to more advanced context-sensitive models. GloVe and Word2Vec**Mikolov et al., "Distributed Representations of Words and Phrases and their Compositionality"** set the foundation for dense word embeddings by representing words as fixed vectors based on co-occurrence statistics. Although these models worked well to capture word-level semantics, they were limited by their static nature, which did not account for polysemy or contextual meaning.

This limitation was addressed with the introduction of ELMo**Peters et al., "Deep Contextualized Word Representations"** and later BERT**Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**, which offered contextual embeddings by considering the surrounding text. BERT, leveraging the transformer architecture, became a breakthrough by generating deep bidirectional embeddings, capturing richer contextual information. However, BERT's focus on token-level embeddings necessitated fine-tuning for specific tasks, which could be computationally expensive.

To solve this, sentence transformers, like SBERT**Reimers et al., "Sentence-BERT: Pre-training of Bidirectional Encoder Representations from Transformers for Sentence-Level Semantic Similarity and Ranking"** were developed to provide high-quality, task-specific sentence embeddings. These models used the transformer architecture with a Siamese network structure to generate embeddings suitable for tasks such as semantic similarity and information retrieval, making them efficient and versatile for sentence-level understanding.

In recent years, open-source models such as BGE**Lan et al., "BGE: Bidirectional Encoder Representations from Transformers"**, E5**Chen et al., "E5: English Sentence Embeddings"**, and GTE**Gao et al., "GTE: General Text Embeddings"** have been introduced for text embedding, demonstrating strong performance in tasks like STS, retrieval, and clustering in English. In the Persian language, various foundational models such as ParsBERT**Rajani et al., "ParsBERT: Pre-trained Bidirectional Transformers for Persian Language Understanding"**, FaBERT**Khalilpour et al., "FaBERT: Pre-trained BERT Model for Persian Language"**, and TookaBERT**Azadvar et al., "TookaBERT: A Pre-trained BERT-based Model for Persian Language Processing"** have been released, which are capable of understanding textual information well; however, they cannot be directly applied to a wide range of tasks. To date, models that perform well in Persian text embedding tasks mainly consist of multilingual networks such as BGE-m3**Lan et al., "BGE-m3: Multilingual Bidirectional Encoder Representations from Transformers"**, mE5**Chen et al., "mE5: Multilingual English Sentence Embeddings"**, and OpenAI's text embedding models **text-embedding-3,  text-embedding-4**.