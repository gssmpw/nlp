\section{Related Work}
% \subsection{Comparative Question Answering}
In this section, we briefly introduce each subtask for Comparative Question Answering and also discuss the existing LLM evaluation benchmarks.

%Here we cite \cite{wang-etal-2022-squality} bla bla bla

\subsection{Comparative Question Answering}

Here, we introduce each subtask and list several papers that addressed the topic.
% The framework we propose here is based on key checks that are supposed to mark a summary on various grounds.
% Rouge evaluation?

\textbf{Comparative Question Identification} aims at classifying questions into two types: comparative and non-comparative. This classification task is solved with both Encoder and Decoder Transformer models \cite{bondarenko2020comparative,DBLP:conf/wsdm/BondarenkoADHBH22,shallouf-etal-2024-cam}.
\textbf{Object and Aspect Identification} is a sequence labelling task, aims at finding objects and aspect of comparison in the question. There exist varios datasets and approaches to solve the task, mostly, with Trasnformer models \cite{chekalina-etal-2021-better,beloucif-etal-2022-elvis,DBLP:conf/wsdm/BondarenkoADHBH22,shallouf-etal-2024-cam}.
\textbf{Stance Classification} is another classification task, that identifies the stance of comparative sentences.  \citet{panchenko-etal-2019-categorizing}, \citet{DBLP:conf/wsdm/BondarenkoADHBH22}, and \citet{DBLP:journals/corr/abs-2310-08523} solve the task using standard ML classifier, Encoder-based Transformer, and GPT-4 respectively.
\textbf{Summary Generation} is only partially tackled by \citet{chekalina-etal-2021-better} and \citet{shallouf-etal-2024-cam}. The closest work on multi-document summarization of differing opinions is by \citet{iso-etal-2022-comparative}, which focuses on aggregating diverse opinions and synthesizing them into a coherent summary.

\subsection{LLM Evaluation Benchmarks}
 
 %It is important to specify that the question asks the model to provide an answer on a scale of 1 to 5. 

% Another approach to replicating human evaluation with the use of an LLM is presented by \cite{liu-etal-2023-g} and is called G-Eval. G-Eval is an experiment that uses chain-of-thoughts and a form-filling paradigm together to augment the quality of LLM-authored evaluation. The authors experimented with both text summarization and dialogue generation, showing that their approach achieved a Spearman correlation of 0.514 with human evaluation on the summarization task, which was an unprecedented success. 

Apart from the well-known benchmarks like SuperGLUE \cite{sarlin2020supergluelearningfeaturematching}, MTEB \cite{muennighoff-etal-2023-mteb}, and SQuAD \cite{rajpurkar2016squad100000questionsmachine}, several more challenging benchmarks have gained prominence: MMLU (Massive Multitask Language Understanding) \cite{DBLP:conf/iclr/HendrycksBBZMSS21},  TruthfulQA \cite{lin-etal-2022-truthfulqa}, BIG-Bench Hard \cite{suzgun-etal-2023-challenging}. The LLM evaluation framework proposed by \cite{chiang-lee-2023-large} involves presenting a Large Language Model with task instructions, a sample to be evaluated and a question. The researchers use LLM evaluation to score parts of both generated and human-written stories. %Under these conditions, LLMs proved to be remarkably successful at performing evaluations, regardless of specific task instructions' formatting. 
To facilitate comprehensive evaluations, several initiatives aggregate multiple benchmarks:
Hugging Face's Big Benchmarks Collection (a centralized platform for various leaderboards), and 
LMSys Chatbot Arena \cite{chiang2024chatbot,zheng2023judgingllmasajudgemtbenchchatbot}: it also utilizes user ratings and GPT-4 evaluations to assess chatbot performance.