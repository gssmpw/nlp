% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").


@inproceedings{lin-2004-rouge,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W04-1013/",
    pages = "74--81"
}

@inproceedings{reimers-gurevych-2019-sentence,
    title = "Sentence-{BERT}: Sentence Embeddings using {S}iamese {BERT}-Networks",
    author = "Reimers, Nils  and
      Gurevych, Iryna",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1410/",
    doi = "10.18653/v1/D19-1410",
    pages = "3982--3992",
    abstract = "BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ({\textasciitilde}65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods."
}

@article{gudibande2023false,
  title={The false promise of imitating proprietary llms},
  author={Gudibande, Arnav and Wallace, Eric and Snell, Charlie and Geng, Xinyang and Liu, Hao and Abbeel, Pieter and Levine, Sergey and Song, Dawn},
  journal={arXiv preprint arXiv:2305.15717},
  year={2023}
}

@misc{zhelnin2024giftswgaussiannoiseinjected,
      title={GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs}, 
      author={Maxim Zhelnin and Viktor Moskvoretskii and Egor Shvetsov and Egor Venediktov and Mariya Krylova and Aleksandr Zuev and Evgeny Burnaev},
      year={2024},
      eprint={2408.15300},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2408.15300}, 
}

@misc{hu2021loralowrankadaptationlarge,
      title={LoRA: Low-Rank Adaptation of Large Language Models}, 
      author={Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
      year={2021},
      eprint={2106.09685},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2106.09685}, 
}

@misc{rajbhandari2020zeromemoryoptimizationstraining,
      title={ZeRO: Memory Optimizations Toward Training Trillion Parameter Models}, 
      author={Samyam Rajbhandari and Jeff Rasley and Olatunji Ruwase and Yuxiong He},
      year={2020},
      eprint={1910.02054},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1910.02054}, 
}

@misc{vonwerra2022trl,
  author = {Leandro von Werra and Younes Belkada and Lewis Tunstall and Edward Beeching and Tristan Thrush and Nathan Lambert and Shengyi Huang},
  title = {TRL: Transformer Reinforcement Learning},
  year = {2020},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/huggingface/trl}}
}

@misc{schulman2017proximalpolicyoptimizationalgorithms,
      title={Proximal Policy Optimization Algorithms}, 
      author={John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
      year={2017},
      eprint={1707.06347},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1707.06347}, 
}

@article{lin2024flame,
  title={Flame: Factuality-aware alignment for large language models},
  author={Lin, Sheng-Chieh and Gao, Luyu and Oguz, Barlas and Xiong, Wenhan and Lin, Jimmy and Yih, Wen-tau and Chen, Xilun},
  journal={arXiv preprint arXiv:2405.01525},
  year={2024}
}

@inproceedings{DBLP:conf/emnlp/WangPCPB22,
  author       = {Alex Wang and
                  Richard Yuanzhe Pang and
                  Angelica Chen and
                  Jason Phang and
                  Samuel R. Bowman},
  editor       = {Yoav Goldberg and
                  Zornitsa Kozareva and
                  Yue Zhang},
  title        = {SQuALITY: Building a Long-Document Summarization Dataset the Hard
                  Way},
  booktitle    = {Proceedings of the 2022 Conference on Empirical Methods in Natural
                  Language Processing, {EMNLP} 2022, Abu Dhabi, United Arab Emirates,
                  December 7-11, 2022},
  pages        = {1139--1156},
  publisher    = {Association for Computational Linguistics},
  year         = {2022},
  url          = {https://doi.org/10.18653/v1/2022.emnlp-main.75},
  doi          = {10.18653/V1/2022.EMNLP-MAIN.75},
}

@misc{wang2023elementaware,
      title={Element-aware Summarization with Large Language Models: Expert-aligned Evaluation and Chain-of-Thought Method}, 
      author={Yiming Wang and Zhuosheng Zhang and Rui Wang},
      year={2023},
      eprint={2305.13412},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{maynez2020faithfulness,
      title={On Faithfulness and Factuality in Abstractive Summarization}, 
      author={Joshua Maynez and Shashi Narayan and Bernd Bohnet and Ryan McDonald},
      year={2020},
      eprint={2005.00661},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{gao2023retrieval,
  title={Retrieval-augmented generation for large language models: A survey},
  author={Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Haofen},
  journal={arXiv preprint arXiv:2312.10997},
  year={2023}
}

@inproceedings{schildwachter2019answering,
  title={Answering comparative questions: Better than ten-blue-links?},
  author={Schildw{\"a}chter, Matthias and Bondarenko, Alexander and Zenker, Julian and Hagen, Matthias and Biemann, Chris and Panchenko, Alexander},
  booktitle={Proceedings of the 2019 Conference on Human Information Interaction and Retrieval},
  pages={361--365},
  year={2019}
}

@article{haspelmath2017equative,
  title={Equative constructions in world-wide perspective},
  author={Haspelmath, Martin and others},
  journal={Similative and equative constructions: A cross-linguistic perspective},
  pages={9--32},
  year={2017},
  publisher={Benjamins Amsterdam}
}

@article{li2011comparable,
  title={Comparable entity mining from comparative questions},
  author={Li, Shasha and Lin, Chin-Yew and Song, Young-In and Li, Zhoujun},
  journal={IEEE transactions on knowledge and data engineering},
  volume={25},
  number={7},
  pages={1498--1509},
  year={2011},
  publisher={IEEE}
}

@inproceedings{bondarenko2020comparative,
  title={Comparative web search questions},
  author={Bondarenko, Alexander and Braslavski, Pavel and V{\"o}lske, Michael and Aly, Rami and Fr{\"o}be, Maik and Panchenko, Alexander and Biemann, Chris and Stein, Benno and Hagen, Matthias},
  booktitle={Proceedings of the 13th International Conference on Web Search and Data Mining},
  pages={52--60},
  year={2020}
}

@inproceedings{chiang-lee-2023-large,
    title = "Can Large Language Models Be an Alternative to Human Evaluations?",
    author = "Chiang, Cheng-Han  and
      Lee, Hung-yi",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.870",
    doi = "10.18653/v1/2023.acl-long.870",
    pages = "15607--15631",
}

@inproceedings{liu-etal-2023-g,
    title = "{G}-Eval: {NLG} Evaluation using Gpt-4 with Better Human Alignment",
    author = "Liu, Yang  and
      Iter, Dan  and
      Xu, Yichong  and
      Wang, Shuohang  and
      Xu, Ruochen  and
      Zhu, Chenguang",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.153",
    doi = "10.18653/v1/2023.emnlp-main.153",
    pages = "2511--2522",
}


@inproceedings{iso-etal-2022-comparative,
    title = "Comparative Opinion Summarization via Collaborative Decoding",
    author = "Iso, Hayate  and
      Wang, Xiaolan  and
      Angelidis, Stefanos  and
      Suhara, Yoshihiko",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.261",
    doi = "10.18653/v1/2022.findings-acl.261",
    pages = "3307--3324",
}


@inproceedings{liang-etal-2024-learning,
    title = "Learning to Trust Your Feelings: Leveraging Self-awareness in {LLM}s for Hallucination Mitigation",
    author = "Liang, Yuxin  and
      Song, Zhuoyang  and
      Wang, Hao  and
      Zhang, Jiaxing",
    editor = "Yu, Wenhao  and
      Shi, Weijia  and
      Yasunaga, Michihiro  and
      Jiang, Meng  and
      Zhu, Chenguang  and
      Hajishirzi, Hannaneh  and
      Zettlemoyer, Luke  and
      Zhang, Zhihan",
    booktitle = "Proceedings of the 3rd Workshop on Knowledge Augmented Methods for NLP",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.knowledgenlp-1.4",
    pages = "44--58", 
}

@article{GUPTA201949,
title = {Abstractive summarization: An overview of the state of the art},
journal = {Expert Systems with Applications},
volume = {121},
pages = {49-65},
year = {2019},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2018.12.011},
url = {https://www.sciencedirect.com/science/article/pii/S0957417418307735},
author = {Som Gupta and S. K Gupta},
}

@article{SHAKIL2024128255,
title = {Abstractive text summarization: State of the art, challenges, and improvements},
journal = {Neurocomputing},
volume = {603},
pages = {128255},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128255},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224010269},
author = {Hassan Shakil and Ahmad Farooq and Jugal Kalita},
keywords = {Automatic summarization, Abstractive summarization, Extractive summarization, Knowledge representation, Text generation}
}

@inproceedings{lin-bilmes-2011-class,
    title = "A Class of Submodular Functions for Document Summarization",
    author = "Lin, Hui  and
      Bilmes, Jeff",
    editor = "Lin, Dekang  and
      Matsumoto, Yuji  and
      Mihalcea, Rada",
    booktitle = "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2011",
    address = "Portland, Oregon, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P11-1052",
    pages = "510--520",
}

@misc{vicuna,
    title = {{V}icuna: An open-source chatbot impressing {GPT}-4 with 90%* {ChatGPT} quality},
    author = {Wei-Lin Chiang and Zhuohan Li and Zi Lin and Ying Sheng and Zhanghao Wu and Hao Zhang, Lianmin Zheng and Siyuan Zhuang and Yonghao Zhuang and Joseph E Gonzalez},
    howpublished={https://vicuna. lmsys.org},
     note={Accessed: 13 September 2024}, 
    year={2023}
}

@InProceedings{bondarenko:2022e,
  address =                  {Berlin Heidelberg New York},
  author =                   {Alexander Bondarenko and Maik Fr{\"o}be and Johannes Kiesel and Shahbaz Syed and Timon Gurcke and Meriem Beloucif and Alexander Panchenko and Chris Biemann and Benno Stein and Henning Wachsmuth and Martin Potthast and Matthias Hagen},
  booktitle =                {Experimental IR Meets Multilinguality, Multimodality, and Interaction. 13th International Conference of the CLEF Association (CLEF 2022)},
  doi =                      {10.1007/978-3-031-13643-6_21},
  editor =                   {Alberto Barr{\'o}n-Cede{\~n}o and Giovanni {Da San Martino} and Mirko Degli Esposti and Fabrizio Sebastiani and Craig Macdonald and Gabriella Pasi and Allan Hanbury and Martin Potthast and Guglielmo Faggioli and Nicola Ferro},
  ids =                      {potthast:2022o},
  month =                    sep,
  numpages =                 29,
  publisher =                {Springer},
  series =                   {Lecture Notes in Computer Science},
  site =                     {Bologna, Italy},
  title =                    {{Overview of Touch{\'e} 2022: Argument Retrieval}},
  volume =                   13390,
  year =                     2022
}


@article{DBLP:journals/corr/abs-1904-09675,
  author       = {Tianyi Zhang and
                  Varsha Kishore and
                  Felix Wu and
                  Kilian Q. Weinberger and
                  Yoav Artzi},
  title        = {BERTScore: Evaluating Text Generation with {BERT}},
  journal      = {CoRR},
  volume       = {abs/1904.09675},
  year         = {2019},
  url          = {http://arxiv.org/abs/1904.09675},
  eprinttype    = {arXiv},
  eprint       = {1904.09675},
  timestamp    = {Wed, 03 Jun 2020 10:08:39 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1904-09675.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{zhao2019moverscore,
  title = {MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance},
  month = {August},
  year = {2019},
  author = {Wei Zhao and Maxime Peyrard and Fei Liu and Yang Gao and Christian M. Meyer and Steffen Eger},
  address = {Hong Kong, China},
  publisher = {Association for Computational Linguistics},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing},
}

% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@inproceedings{nonkes-etal-2024-leveraging,
    title = "Leveraging Graph Structures to Detect Hallucinations in Large Language Models",
    author = "Nonkes, Noa  and
      Agaronian, Sergei  and
      Kanoulas, Evangelos  and
      Petcu, Roxana",
    editor = "Ustalov, Dmitry  and
      Gao, Yanjun  and
      Panchenko, Alexander  and
      Tutubalina, Elena  and
      Nikishina, Irina  and
      Ramesh, Arti  and
      Sakhovskiy, Andrey  and
      Usbeck, Ricardo  and
      Penn, Gerald  and
      Valentino, Marco",
    booktitle = "Proceedings of TextGraphs-17: Graph-based Methods for Natural Language Processing",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.textgraphs-1.7",
    pages = "93--104",
    abstract = "Large language models are extensively applied across a wide range of tasks, such as customer support, content creation, educational tutoring, and providing financial guidance. However, a well-known drawback is their predisposition to generate hallucinations. This damages the trustworthiness of the information these models provide, impacting decision-making and user confidence. We propose a method to detect hallucinations by looking at the structure of the latent space and finding associations within hallucinated and non-hallucinated generations. We create a graph structure that connects generations that lie closely in the embedding space. Moreover, we employ a Graph Attention Network which utilizes message passing to aggregate information from neighboring nodes and assigns varying degrees of importance to each neighbor based on their relevance. Our findings show that 1) there exists a structure in the latent space that differentiates between hallucinated and non-hallucinated generations, 2) Graph Attention Networks can learn this structure and generalize it to unseen generations, and 3) the robustness of our method is enhanced when incorporating contrastive learning. When evaluated against evidence-based benchmarks, our model performs similarly without access to search-based methods.",
}

@inproceedings{DBLP:conf/cikm/KimZ09,
  author       = {Hyun Duk Kim and
                  ChengXiang Zhai},
  title        = {Generating comparative summaries of contradictory opinions in text},
  booktitle    = {{CIKM}},
  pages        = {385--394},
  publisher    = {{ACM}},
  year         = {2009}
}

@inproceedings{DBLP:conf/wsdm/BondarenkoADHBH22,
  author       = {Alexander Bondarenko and
                  Yamen Ajjour and
                  Valentin Dittmar and
                  Niklas Homann and
                  Pavel Braslavski and
                  Matthias Hagen},
  title        = {Towards Understanding and Answering Comparative Questions},
  booktitle    = {{WSDM}},
  pages        = {66--74},
  publisher    = {{ACM}},
  year         = {2022}
}


@inproceedings{NEURIPSBENCHMARKS2021_65ded535,
 author = {Pavlichenko, Nikita and Stelmakh, Ivan and Ustalov, Dmitry},
 booktitle = {Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks},
 editor = {J. Vanschoren and S. Yeung},
 pages = {},
 title = {CrowdSpeech and Vox DIY: Benchmark Dataset for Crowdsourced Audio Transcription},
 url = {https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/2021/file/65ded5353c5ee48d0b7d48c591b8f430-Paper-round1.pdf},
 volume = {1},
 year = {2021}
}


@inproceedings{DBLP:conf/clef/BondarenkoFKSGB22,
  author       = {Alexander Bondarenko and
                  Maik Fr{\"{o}}be and
                  Johannes Kiesel and
                  Shahbaz Syed and
                  Timon Gurcke and
                  Meriem Beloucif and
                  Alexander Panchenko and
                  Chris Biemann and
                  Benno Stein and
                  Henning Wachsmuth and
                  Martin Potthast and
                  Matthias Hagen},
  title        = {Overview of Touch{\'{e}} 2022: Argument Retrieval},
  booktitle    = {{CLEF} (Working Notes)},
  series       = {{CEUR} Workshop Proceedings},
  volume       = {3180},
  pages        = {2867--2903},
  publisher    = {CEUR-WS.org},
  year         = {2022}
}

@inproceedings{DBLP:conf/comma/FegerSM20,
  author       = {Marc Feger and
                  Jan Steimann and
                  Christian Meter},
  title        = {Structure or Content? Towards Assessing Argument Relevance},
  booktitle    = {{COMMA}},
  series       = {Frontiers in Artificial Intelligence and Applications},
  volume       = {326},
  pages        = {203--214},
  publisher    = {{IOS} Press},
  year         = {2020}
}

@inproceedings{liang-etal-2024-learning,
    title = "Learning to Trust Your Feelings: Leveraging Self-awareness in {LLM}s for Hallucination Mitigation",
    author = "Liang, Yuxin  and
      Song, Zhuoyang  and
      Wang, Hao  and
      Zhang, Jiaxing",
    editor = "Yu, Wenhao  and
      Shi, Weijia  and
      Yasunaga, Michihiro  and
      Jiang, Meng  and
      Zhu, Chenguang  and
      Hajishirzi, Hannaneh  and
      Zettlemoyer, Luke  and
      Zhang, Zhihan",
    booktitle = "Proceedings of the 3rd Workshop on Knowledge Augmented Methods for NLP",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.knowledgenlp-1.4",
    pages = "44--58",
    abstract = "We evaluate the ability of Large Language Models (LLMs) to discern and express their internal knowledge state, a key factor in countering factual hallucination and ensuring reliable application of LLMs. We observe a robust self-awareness of internal knowledge state in LLMs, evidenced by over 85{\%} accuracy in knowledge state probing. However, LLMs often fail to faithfully express their internal knowledge during generation, leading to factual hallucinations. We develop an automated hallucination annotation tool, DreamCatcher, which merges knowledge probing and consistency checking methods to rank factual preference data. Using knowledge preference as reward, We propose a Reinforcement Learning from Knowledge Feedback (RLKF) training framework, leveraging reinforcement learning to enhance the factuality and honesty of LLMs. Our experiments across multiple models show that RLKF training effectively enhances the ability of models to utilize their internal knowledge state, boosting performance in a variety of knowledge-based and honesty-related tasks.",
}

@inproceedings{schimanski-etal-2024-towards,
    title = "Towards Faithful and Robust {LLM} Specialists for Evidence-Based Question-Answering",
    author = "Schimanski, Tobias  and
      Ni, Jingwei  and
      Kraus, Mathias  and
      Ash, Elliott  and
      Leippold, Markus",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.105",
    pages = "1913--1931",
    abstract = "Advances towards more faithful and traceable answers of Large Language Models (LLMs) are crucial for various research and practical endeavors. One avenue in reaching this goal is basing the answers on reliable sources. However, this Evidence-Based QA has proven to work insufficiently with LLMs in terms of citing the correct sources (source quality) and truthfully representing the information within sources (answer attributability). In this work, we systematically investigate how to robustly fine-tune LLMs for better source quality and answer attributability. Specifically, we introduce a data generation pipeline with automated data quality filters, which can synthesize diversified high-quality training and testing data at scale. We further introduce four test sets to benchmark the robustness of fine-tuned specialist models. Extensive evaluation shows that fine-tuning on synthetic data improves performance on both in- and out-of-distribution. Furthermore, we show that data quality, which can be drastically improved by proposed quality filters, matters more than quantity in improving Evidence-Based QA.",
}

@inproceedings{ravaut-etal-2024-context,
    title = "On Context Utilization in Summarization with Large Language Models",
    author = "Ravaut, Mathieu  and
      Sun, Aixin  and
      Chen, Nancy  and
      Joty, Shafiq",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.153",
    pages = "2764--2781",
    abstract = "Large language models (LLMs) excel in abstractive summarization tasks, delivering fluent and pertinent summaries. Recent advancements have extended their capabilities to handle long-input contexts, exceeding 100k tokens. However, in question answering, language models exhibit uneven utilization of their input context. They tend to favor the initial and final segments, resulting in a U-shaped performance pattern concerning where the answer is located within the input. This bias raises concerns, particularly in summarization where crucial content may be dispersed throughout the source document(s). Besides, in summarization, mapping facts from the source to the summary is not trivial as salient content is usually re-phrased. In this paper, we conduct the first comprehensive study on context utilization and position bias in summarization. Our analysis encompasses 6 LLMs, 10 datasets, and 5 evaluation metrics. We introduce a new evaluation benchmark called MiddleSum on the which we benchmark two alternative inference methods to alleviate position bias: hierarchical summarization and incremental summarization. Our code and data can be found here: https://github.com/ntunlp/MiddleSum.",
}


@inproceedings{shi-etal-2024-generate,
    title = "Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering",
    author = "Shi, Zhengliang  and
      Zhang, Shuo  and
      Sun, Weiwei  and
      Gao, Shen  and
      Ren, Pengjie  and
      Chen, Zhumin  and
      Ren, Zhaochun",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.397",
    pages = "7339--7353",
    abstract = "Multi-Hop Question Answering (MHQA) task presents a significant challenge for large language models (LLMs) due to the intensive knowledge required. Current solutions, like Retrieval-Augmented Generation, typically retrieve potential documents from an external corpus to read an answer. However, the performance of this retrieve-then-read paradigm is constrained by the retriever and the inevitable noise in the retrieved documents. To mitigate these challenges, we introduce a novel generate-then-ground (GenGround) framework, synergizing the parametric knowledge of LLMs and external documents to solve a multi-hop question. GenGround empowers LLMs to alternate two phases until the final answer is derived: (1) formulate a simpler, single-hop question and directly generate the answer; (2) ground the question-answer pair into retrieved documents, amending any wrong predictions in the answer. We also propose an instructional grounding distillation method to generalize our method into smaller models. Extensive experiments conducted on four datasets illustrate the superiority of our method. To further facilitate future research, we have collected a dataset that traces the reasoning process.",
}


@inproceedings{beloucif-etal-2022-elvis,
    title = "Elvis vs. {M}. {J}ackson: Who has More Albums? Classification and Identification of Elements in Comparative Questions",
    author = "Beloucif, Meriem  and
      Yimam, Seid Muhie  and
      Stahlhacke, Steffen  and
      Biemann, Chris",
    editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Thirteenth Language Resources and Evaluation Conference",
    month = jun,
    year = "2022",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2022.lrec-1.402",
    pages = "3771--3779",
    abstract = "Comparative Question Answering (cQA) is the task of providing concrete and accurate responses to queries such as: {``}Is Lyft cheaper than a regular taxi?{''} or {``}What makes a mortgage different from a regular loan?{''}. In this paper, we propose two new open-domain real-world datasets for identifying and labeling comparative questions. While the first dataset contains instances of English questions labeled as comparative vs. non-comparative, the second dataset provides additional labels including the objects and the aspects of comparison. We conduct several experiments that evaluate the soundness of our datasets. The evaluation of our datasets using various classifiers show promising results that reach close-to-human results on a binary classification task with a neural model using ALBERT embeddings. When approaching the unsupervised sequence labeling task, some headroom remains.",
}


@inproceedings{panchenko-etal-2019-categorizing,
    title = "Categorizing Comparative Sentences",
    author = "Panchenko, Alexander  and
      Bondarenko, Alexander  and
      Franzek, Mirco  and
      Hagen, Matthias  and
      Biemann, Chris",
    editor = "Stein, Benno  and
      Wachsmuth, Henning",
    booktitle = "Proceedings of the 6th Workshop on Argument Mining",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-4516",
    doi = "10.18653/v1/W19-4516",
    pages = "136--145",
    abstract = "We tackle the tasks of automatically identifying comparative sentences and categorizing the intended preference (e.g., {``}Python has better NLP libraries than MATLAB{''} → Python, better, MATLAB). To this end, we manually annotate 7,199 sentences for 217 distinct target item pairs from several domains (27{\%} of the sentences contain an oriented comparison in the sense of {``}better{''} or {``}worse{''}). A gradient boosting model based on pre-trained sentence embeddings reaches an F1 score of 85{\%} in our experimental evaluation. The model can be used to extract comparative sentences for pro/con argumentation in comparative / argument search engines or debating technologies.",
}

@inproceedings{shallouf-etal-2024-cam,
    title = "{CAM} 2.0: End-to-End Open Domain Comparative Question Answering System",
    author = "Shallouf, Ahmad  and
      Herasimchyk, Hanna  and
      Salnikov, Mikhail  and
      Garrido Veliz, Rudy Alexandro  and
      Mestvirishvili, Natia  and
      Panchenko, Alexander  and
      Biemann, Chris  and
      Nikishina, Irina",
    editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
    booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.lrec-main.238",
    pages = "2657--2672",
    abstract = "Comparative Question Answering (CompQA) is a Natural Language Processing task that combines Question Answering and Argument Mining approaches to answer subjective comparative questions in an efficient argumentative manner. In this paper, we present an end-to-end (full pipeline) system for answering comparative questions called CAM 2.0 as well as a public leaderboard called CompUGE that unifies the existing datasets under a single easy-to-use evaluation suite. As compared to previous web-form-based CompQA systems, it features question identification, object and aspect labeling, stance classification, and summarization using up-to-date models. We also select the most time- and memory-effective pipeline by comparing separately fine-tuned Transformer Encoder models which show state-of-the-art performance on the subtasks with Generative LLMs in few-shot and LoRA setups. We also conduct a user study for a whole-system evaluation.",
}

@inproceedings{chekalina-etal-2021-better,
    title = "Which is Better for Deep Learning: Python or {MATLAB}? Answering Comparative Questions in Natural Language",
    author = "Chekalina, Viktoriia  and
      Bondarenko, Alexander  and
      Biemann, Chris  and
      Beloucif, Meriem  and
      Logacheva, Varvara  and
      Panchenko, Alexander",
    editor = "Gkatzia, Dimitra  and
      Seddah, Djam{\'e}",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-demos.36",
    doi = "10.18653/v1/2021.eacl-demos.36",
    pages = "302--311",
    abstract = "We present a system for answering comparative questions (Is X better than Y with respect to Z?) in natural language. Answering such questions is important for assisting humans in making informed decisions. The key component of our system is a natural language interface for comparative QA that can be used in personal assistants, chatbots, and similar NLP devices. Comparative QA is a challenging NLP task, since it requires collecting support evidence from many different sources, and direct comparisons of rare objects may be not available even on the entire Web. We take the first step towards a solution for such a task offering a testbed for comparative QA in natural language by probing several methods, making the three best ones available as an online demo.",
}


@inproceedings{DBLP:conf/emnlp/WangPCPB22,
  author       = {Alex Wang and
                  Richard Yuanzhe Pang and
                  Angelica Chen and
                  Jason Phang and
                  Samuel R. Bowman},
  editor       = {Yoav Goldberg and
                  Zornitsa Kozareva and
                  Yue Zhang},
  title        = {SQuALITY: Building a Long-Document Summarization Dataset the Hard
                  Way},
  booktitle    = {Proceedings of the 2022 Conference on Empirical Methods in Natural
                  Language Processing, {EMNLP} 2022, Abu Dhabi, United Arab Emirates,
                  December 7-11, 2022},
  pages        = {1139--1156},
  publisher    = {Association for Computational Linguistics},
  year         = {2022},
  url          = {https://doi.org/10.18653/v1/2022.emnlp-main.75},
  doi          = {10.18653/V1/2022.EMNLP-MAIN.75},
  timestamp    = {Thu, 10 Aug 2023 12:35:22 +0200},
  biburl       = {https://dblp.org/rec/conf/emnlp/WangPCPB22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}

@misc{wang2023elementaware,
      title={Element-aware Summarization with Large Language Models: Expert-aligned Evaluation and Chain-of-Thought Method}, 
      author={Yiming Wang and Zhuosheng Zhang and Rui Wang},
      year={2023},
      eprint={2305.13412},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{maynez2020faithfulness,
      title={On Faithfulness and Factuality in Abstractive Summarization}, 
      author={Joshua Maynez and Shashi Narayan and Bernd Bohnet and Ryan McDonald},
      year={2020},
      eprint={2005.00661},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{gao2023retrieval,
  title={Retrieval-augmented generation for large language models: A survey},
  author={Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Haofen},
  journal={arXiv preprint arXiv:2312.10997},
  year={2023}
}

@inproceedings{schildwachter2019answering,
  title={Answering comparative questions: Better than ten-blue-links?},
  author={Schildw{\"a}chter, Matthias and Bondarenko, Alexander and Zenker, Julian and Hagen, Matthias and Biemann, Chris and Panchenko, Alexander},
  booktitle={Proceedings of the 2019 Conference on Human Information Interaction and Retrieval},
  pages={361--365},
  year={2019}
}

@article{haspelmath2017equative,
  title={Equative constructions in world-wide perspective},
  author={Haspelmath, Martin and others},
  journal={Similative and equative constructions: A cross-linguistic perspective},
  pages={9--32},
  year={2017},
  publisher={Benjamins Amsterdam}
}

@article{li2011comparable,
  title={Comparable entity mining from comparative questions},
  author={Li, Shasha and Lin, Chin-Yew and Song, Young-In and Li, Zhoujun},
  journal={IEEE transactions on knowledge and data engineering},
  volume={25},
  number={7},
  pages={1498--1509},
  year={2011},
  publisher={IEEE}
}

@article{DBLP:journals/corr/abs-2406-18403,
  author       = {Anna Bavaresco and
                  Raffaella Bernardi and
                  Leonardo Bertolazzi and
                  Desmond Elliott and
                  Raquel Fern{\'{a}}ndez and
                  Albert Gatt and
                  Esam Ghaleb and
                  Mario Giulianelli and
                  Michael Hanna and
                  Alexander Koller and
                  Andr{\'{e}} F. T. Martins and
                  Philipp Mondorf and
                  Vera Neplenbroek and
                  Sandro Pezzelle and
                  Barbara Plank and
                  David Schlangen and
                  Alessandro Suglia and
                  Aditya K. Surikuchi and
                  Ece Takmaz and
                  Alberto Testoni},
  title        = {LLMs instead of Human Judges? {A} Large Scale Empirical Study across
                  20 {NLP} Evaluation Tasks},
  journal      = {CoRR},
  volume       = {abs/2406.18403},
  year         = {2024}
}


@article{lawrence-reed-2019-argument,
    title = "Argument Mining: A Survey",
    author = "Lawrence, John  and
      Reed, Chris",
    journal = "Computational Linguistics",
    volume = "45",
    number = "4",
    month = dec,
    year = "2019",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/J19-4006",
    doi = "10.1162/coli_a_00364",
    pages = "765--818",
    abstract = "Argument mining is the automatic identification and extraction of the structure of inference and reasoning expressed as arguments presented in natural language. Understanding argumentative structure makes it possible to determine not only what positions people are adopting, but also why they hold the opinions they do, providing valuable insights in domains as diverse as financial market prediction and public relations. This survey explores the techniques that establish the foundations for argument mining, provides a review of recent advances in argument mining techniques, and discusses the challenges faced in automatically extracting a deeper understanding of reasoning expressed in language in general.",
}

@inproceedings{DBLP:conf/acl/VecchiFJL20,
  author       = {Eva Maria Vecchi and
                  Neele Falk and
                  Iman Jundi and
                  Gabriella Lapesa},
  title        = {Towards Argument Mining for Social Good: {A} Survey},
  booktitle    = {{ACL/IJCNLP} {(1)}},
  pages        = {1338--1352},
  publisher    = {Association for Computational Linguistics},
  year         = {2021}
}

@article{DBLP:journals/nlpj/ShaikTDXLG23,
  author       = {Thanveer Shaik and
                  Xiaohui Tao and
                  Christopher Dann and
                  Haoran Xie and
                  Yan Li and
                  Linda Galligan},
  title        = {Sentiment analysis and opinion mining on educational data: {A} survey},
  journal      = {Nat. Lang. Process. J.},
  volume       = {2},
  pages        = {100003},
  year         = {2023}
}

@inproceedings{hardalov-etal-2022-survey,
    title = "A Survey on Stance Detection for Mis- and Disinformation Identification",
    author = "Hardalov, Momchil  and
      Arora, Arnav  and
      Nakov, Preslav  and
      Augenstein, Isabelle",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2022",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-naacl.94",
    doi = "10.18653/v1/2022.findings-naacl.94",
    pages = "1259--1277",
    abstract = "Understanding attitudes expressed in texts, also known as stance detection, plays an important role in systems for detecting false information online, be it misinformation (unintentionally false) or disinformation (intentionally false information). Stance detection has been framed in different ways, including (a) as a component of fact-checking, rumour detection, and detecting previously fact-checked claims, or (b) as a task in its own right. While there have been prior efforts to contrast stance detection with other related tasks such as argumentation mining and sentiment analysis, there is no existing survey on examining the relationship between stance detection and mis- and disinformation detection. Here, we aim to bridge this gap by reviewing and analysing existing work in this area, with mis- and disinformation in focus, and discussing lessons learnt and future challenges.",
}

@inproceedings{DBLP:conf/clef/BondarenkoFKSBRHLRSPH23,
  author       = {Alexander Bondarenko and
                  Maik Fr{\"{o}}be and
                  Johannes Kiesel and
                  Ferdinand Schlatt and
                  Valentin Barri{\`{e}}re and
                  Brian Ravenet and
                  L{\'{e}}o Hemamou and
                  Simon Luck and
                  Jan Heinrich Reimer and
                  Benno Stein and
                  Martin Potthast and
                  Matthias Hagen},
  title        = {Overview of Touch{\'{e}} 2023: Argument and Causal Retrieval},
  booktitle    = {{CLEF}},
  series       = {Lecture Notes in Computer Science},
  volume       = {14163},
  pages        = {507--530},
  publisher    = {Springer},
  year         = {2023}
}


@inproceedings{DBLP:conf/clef/BondarenkoGFBAP21a,
  author       = {Alexander Bondarenko and
                  Lukas Gienapp and
                  Maik Fr{\"{o}}be and
                  Meriem Beloucif and
                  Yamen Ajjour and
                  Alexander Panchenko and
                  Chris Biemann and
                  Benno Stein and
                  Henning Wachsmuth and
                  Martin Potthast and
                  Matthias Hagen},
  title        = {Overview of Touch{\'{e}} 2021: Argument Retrieval},
  booktitle    = {{CLEF}},
  series       = {Lecture Notes in Computer Science},
  volume       = {12880},
  pages        = {450--467},
  publisher    = {Springer},
  year         = {2021}
}

@inproceedings{heinisch-etal-2022-overview,
    title = "Overview of the 2022 Validity and Novelty Prediction Shared Task",
    author = "Heinisch, Philipp  and
      Frank, Anette  and
      Opitz, Juri  and
      Plenz, Moritz  and
      Cimiano, Philipp",
    editor = "Lapesa, Gabriella  and
      Schneider, Jodi  and
      Jo, Yohan  and
      Saha, Sougata",
    booktitle = "Proceedings of the 9th Workshop on Argument Mining",
    month = oct,
    year = "2022",
    address = "Online and in Gyeongju, Republic of Korea",
    publisher = "International Conference on Computational Linguistics",
    url = "https://aclanthology.org/2022.argmining-1.7",
    pages = "84--94",
    abstract = "This paper provides an overview of the Argument Validity and Novelty Prediction Shared Task that was organized as part of the 9th Workshop on Argument Mining (ArgMining 2022). The task focused on the prediction of the validity and novelty of a conclusion given a textual premise. Validity is defined as the degree to which the conclusion is justified with respect to the given premise. Novelty defines the degree to which the conclusion contains content that is new in relation to the premise. Six groups participated in the task, submitting overall 13 system runs for the subtask of binary classification and 2 system runs for the subtask of relative classification. The results reveal that the task is challenging, with best results obtained for Validity prediction in the range of 75{\%} F1 score, for Novelty prediction of 70{\%} F1 score and for correctly predicting both Validity and Novelty of 45{\%} F1 score. In this paper we summarize the task definition and dataset. We give an overview of the results obtained by the participating systems, as well as insights to be gained from the diverse contributions.",
}

@inproceedings{DBLP:conf/bigdataconf/AlamboLMPFBTR20,
  author       = {Amanuel Alambo and
                  Cori Lohstroh and
                  Erik Madaus and
                  Swati Padhee and
                  Brandy Foster and
                  Tanvi Banerjee and
                  Krishnaprasad Thirunarayan and
                  Michael L. Raymer},
  title        = {Topic-Centric Unsupervised Multi-Document Summarization of Scientific
                  and News Articles},
  booktitle    = {{IEEE} BigData},
  pages        = {591--596},
  publisher    = {{IEEE}},
  year         = {2020}
}

@inproceedings{zhu-etal-2024-fanoutqa,
    title = "{F}an{O}ut{QA}: A Multi-Hop, Multi-Document Question Answering Benchmark for Large Language Models",
    author = "Zhu, Andrew  and
      Hwang, Alyssa  and
      Dugan, Liam  and
      Callison-Burch, Chris",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-short.2",
    pages = "18--37",
    abstract = "One type of question that is commonly found in day-to-day scenarios is {``}fan-out{''} questions, complex multi-hop, multi-document reasoning questions that require finding information about a large number of entities. However, there exist few resources to evaluate this type of question-answering capability among large language models. To evaluate complex reasoning in LLMs more fully, we present FanOutQA, a high-quality dataset of fan-out question-answer pairs and human-annotated decompositions with English Wikipedia as the knowledge base. We formulate three benchmark settings across our dataset and benchmark 7 LLMs, including GPT-4, LLaMA 2, Claude-2.1, and Mixtral-8x7B, finding that contemporary models still have room to improve reasoning over inter-document dependencies in a long context. We provide our dataset, along with open-source tools to run models to encourage evaluation.",
}


@inproceedings{obonyo-etal-2022-exploring,
    title = "Exploring the limits of a base {BART} for multi-document summarization in the medical domain",
    author = "Obonyo, Ishmael  and
      Casola, Silvia  and
      Saggion, Horacio",
    editor = "Cohan, Arman  and
      Feigenblat, Guy  and
      Freitag, Dayne  and
      Ghosal, Tirthankar  and
      Herrmannova, Drahomira  and
      Knoth, Petr  and
      Lo, Kyle  and
      Mayr, Philipp  and
      Shmueli-Scheuer, Michal  and
      de Waard, Anita  and
      Wang, Lucy Lu",
    booktitle = "Proceedings of the Third Workshop on Scholarly Document Processing",
    month = oct,
    year = "2022",
    address = "Gyeongju, Republic of Korea",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.sdp-1.23",
    pages = "193--198",
    abstract = "This paper is a description of our participation in the Multi-document Summarization for Literature Review (MSLR) Shared Task, in which we explore summarization models to create an automatic review of scientific results. Rather than maximizing the metrics using expensive computational models, we placed ourselves in a situation of scarce computational resources and explore the limits of a base sequence to sequence models (thus with a limited input length) to the task. Although we explore methods to feed the abstractive model with salient sentences only (using a first extractive step), we find the results still need some improvements.",
}

@misc{rtf,
  title = {{The Ultimate ChatGPT Prompting Guide:} Role > Task > Format},
  howpublished = {\url{https://www.lewis-lin.com/blog/the-ultimate-\\chatgpt-prompting-guide-role-gt-task-gt\\-format}},
  author = {Lewis Lin},
  year = {2023},
  note = {Accessed: 2024-09-10}
}


@inproceedings{loya-etal-2023-exploring,
    title = "Exploring the Sensitivity of {LLM}s{'} Decision-Making Capabilities: Insights from Prompt Variations and Hyperparameters",
    author = "Loya, Manikanta  and
      Sinha, Divya  and
      Futrell, Richard",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.241",
    doi = "10.18653/v1/2023.findings-emnlp.241",
    pages = "3711--3716",
}

@article{ELKASSAS2021113679,
title = {Automatic text summarization: A comprehensive survey},
journal = {Expert Systems with Applications},
volume = {165},
pages = {113679},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.113679},
url = {https://www.sciencedirect.com/science/article/pii/S0957417420305030},
author = {Wafaa S. El-Kassas and Cherif R. Salama and Ahmed A. Rafea and Hoda K. Mohamed},
keywords = {Automatic text summarization, Text summarization approaches, Text summarization techniques, Text summarization evaluation},
}

@article{DBLP:journals/corr/abs-2407-21783,
  author       = {Abhimanyu Dubey and
                  Abhinav Jauhri and
                  Abhinav Pandey and
                  Abhishek Kadian and
                  Ahmad Al{-}Dahle and
                  Aiesha Letman and
                  Akhil Mathur and
                  Alan Schelten and
                  Amy Yang and
                  Angela Fan and
                  Anirudh Goyal and
                  Anthony Hartshorn and
                  Aobo Yang and
                  Archi Mitra and
                  Archie Sravankumar and
                  Artem Korenev and
                  Arthur Hinsvark and
                  Arun Rao and
                  Aston Zhang and
                  Aur{\'{e}}lien Rodriguez and
                  Austen Gregerson and
                  Ava Spataru and
                  Baptiste Rozi{\`{e}}re and
                  Bethany Biron and
                  Binh Tang and
                  Bobbie Chern and
                  Charlotte Caucheteux and
                  Chaya Nayak and
                  Chloe Bi and
                  Chris Marra and
                  Chris McConnell and
                  Christian Keller and
                  Christophe Touret and
                  Chunyang Wu and
                  Corinne Wong and
                  Cristian Canton Ferrer and
                  Cyrus Nikolaidis and
                  Damien Allonsius and
                  Daniel Song and
                  Danielle Pintz and
                  Danny Livshits and
                  David Esiobu and
                  Dhruv Choudhary and
                  Dhruv Mahajan and
                  Diego Garcia{-}Olano and
                  Diego Perino and
                  Dieuwke Hupkes and
                  Egor Lakomkin and
                  Ehab AlBadawy and
                  Elina Lobanova and
                  Emily Dinan and
                  Eric Michael Smith and
                  Filip Radenovic and
                  Frank Zhang and
                  Gabriel Synnaeve and
                  Gabrielle Lee and
                  Georgia Lewis Anderson and
                  Graeme Nail and
                  Gr{\'{e}}goire Mialon and
                  Guan Pang and
                  Guillem Cucurell and
                  Hailey Nguyen and
                  Hannah Korevaar and
                  Hu Xu and
                  Hugo Touvron and
                  Iliyan Zarov and
                  Imanol Arrieta Ibarra and
                  Isabel M. Kloumann and
                  Ishan Misra and
                  Ivan Evtimov and
                  Jade Copet and
                  Jaewon Lee and
                  Jan Geffert and
                  Jana Vranes and
                  Jason Park and
                  Jay Mahadeokar and
                  Jeet Shah and
                  Jelmer van der Linde and
                  Jennifer Billock and
                  Jenny Hong and
                  Jenya Lee and
                  Jeremy Fu and
                  Jianfeng Chi and
                  Jianyu Huang and
                  Jiawen Liu and
                  Jie Wang and
                  Jiecao Yu and
                  Joanna Bitton and
                  Joe Spisak and
                  Jongsoo Park and
                  Joseph Rocca and
                  Joshua Johnstun and
                  Joshua Saxe and
                  Junteng Jia and
                  Kalyan Vasuden Alwala and
                  Kartikeya Upasani and
                  Kate Plawiak and
                  Ke Li and
                  Kenneth Heafield and
                  Kevin Stone and
                  et al.},
  title        = {The Llama 3 Herd of Models},
  journal      = {CoRR},
  volume       = {abs/2407.21783},
  year         = {2024}
}

@article{DBLP:journals/corr/abs-2303-08774,
  author       = {OpenAI},
  title        = {{GPT-4} Technical Report},
  journal      = {CoRR},
  volume       = {abs/2303.08774},
  year         = {2023}
}

@article{DBLP:journals/corr/abs-2401-04088,
  author       = {Albert Q. Jiang and
                  Alexandre Sablayrolles and
                  Antoine Roux and
                  Arthur Mensch and
                  Blanche Savary and
                  Chris Bamford and
                  Devendra Singh Chaplot and
                  Diego de Las Casas and
                  Emma Bou Hanna and
                  Florian Bressand and
                  Gianna Lengyel and
                  Guillaume Bour and
                  Guillaume Lample and
                  L{\'{e}}lio Renard Lavaud and
                  Lucile Saulnier and
                  Marie{-}Anne Lachaux and
                  Pierre Stock and
                  Sandeep Subramanian and
                  Sophia Yang and
                  Szymon Antoniak and
                  Teven Le Scao and
                  Th{\'{e}}ophile Gervet and
                  Thibaut Lavril and
                  Thomas Wang and
                  Timoth{\'{e}}e Lacroix and
                  William El Sayed},
  title        = {Mixtral of Experts},
  journal      = {CoRR},
  volume       = {abs/2401.04088},
  year         = {2024}
}

@inproceedings{DBLP:conf/nips/BrownMRSKDNSSAA20,
  author       = {Tom B. Brown and
                  Benjamin Mann and
                  Nick Ryder and
                  Melanie Subbiah and
                  Jared Kaplan and
                  Prafulla Dhariwal and
                  Arvind Neelakantan and
                  Pranav Shyam and
                  Girish Sastry and
                  Amanda Askell and
                  Sandhini Agarwal and
                  Ariel Herbert{-}Voss and
                  Gretchen Krueger and
                  Tom Henighan and
                  Rewon Child and
                  Aditya Ramesh and
                  Daniel M. Ziegler and
                  Jeffrey Wu and
                  Clemens Winter and
                  Christopher Hesse and
                  Mark Chen and
                  Eric Sigler and
                  Mateusz Litwin and
                  Scott Gray and
                  Benjamin Chess and
                  Jack Clark and
                  Christopher Berner and
                  Sam McCandlish and
                  Alec Radford and
                  Ilya Sutskever and
                  Dario Amodei},
  title        = {Language Models are Few-Shot Learners},
  booktitle    = {NeurIPS},
  year         = {2020}
}

@misc{sarlin2020supergluelearningfeaturematching,
      title={SuperGlue: Learning Feature Matching with Graph Neural Networks}, 
      author={Paul-Edouard Sarlin and Daniel DeTone and Tomasz Malisiewicz and Andrew Rabinovich},
      year={2020},
      eprint={1911.11763},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1911.11763}, 
}

@inproceedings{muennighoff-etal-2023-mteb,
    title = "{MTEB}: Massive Text Embedding Benchmark",
    author = "Muennighoff, Niklas  and
      Tazi, Nouamane  and
      Magne, Loic  and
      Reimers, Nils",
    editor = "Vlachos, Andreas  and
      Augenstein, Isabelle",
    booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.eacl-main.148",
    doi = "10.18653/v1/2023.eacl-main.148",
    pages = "2014--2037"}

@misc{chiang2024chatbot,
    title={Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference},
    author={Wei-Lin Chiang and Lianmin Zheng and Ying Sheng and Anastasios Nikolas Angelopoulos and Tianle Li and Dacheng Li and Hao Zhang and Banghua Zhu and Michael Jordan and Joseph E. Gonzalez and Ion Stoica},
    year={2024},
    eprint={2403.04132},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}

@misc{rajpurkar2016squad100000questionsmachine,
      title={SQuAD: 100,000+ Questions for Machine Comprehension of Text}, 
      author={Pranav Rajpurkar and Jian Zhang and Konstantin Lopyrev and Percy Liang},
      year={2016},
      eprint={1606.05250},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1606.05250}, 
}

@inproceedings{suzgun-etal-2023-challenging,
    title = "Challenging {BIG}-Bench Tasks and Whether Chain-of-Thought Can Solve Them",
    author = {Suzgun, Mirac  and
      Scales, Nathan  and
      Sch{\"a}rli, Nathanael  and
      Gehrmann, Sebastian  and
      Tay, Yi  and
      Chung, Hyung Won  and
      Chowdhery, Aakanksha  and
      Le, Quoc  and
      Chi, Ed  and
      Zhou, Denny  and
      Wei, Jason},
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.824",
    doi = "10.18653/v1/2023.findings-acl.824",
    pages = "13003--13051",
    abstract = "BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that focuses on tasks believed to be beyond the capabilities of current language models. Language models have already made good progress on this benchmark, with the best model in the BIG-Bench paper outperforming average reported human-rater results on 65{\%} of the BIG-Bench tasks via few-shot prompting. But on what tasks do language models fall short of average human-rater performance, and are those tasks actually unsolvable by current language models? In this work, we focus on a suite of 23 challenging BIG-Bench tasks which we call BIG-Bench Hard (BBH). These are the tasks for which prior language model evaluations did not outperform the average human-rater. We find that applying chain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the average human-rater performance on 10 of the 23 tasks, and Codex (code-davinci-002) to surpass the average human-rater performance on 17 of the 23 tasks. Since many tasks in BBH require multi-step reasoning, few-shot prompting without CoT, as done in the BIG-Bench evaluations (Srivastava et al., 2022), substantially underestimates the best performance and capabilities of language models, which is better captured via CoT prompting. As further analysis, we explore the interaction between CoT and model scale on BBH, finding that CoT enables emergent task performance on several BBH tasks with otherwise flat scaling curves.",
}

@inproceedings{lin-etal-2022-truthfulqa,
    title = "{T}ruthful{QA}: Measuring How Models Mimic Human Falsehoods",
    author = "Lin, Stephanie  and
      Hilton, Jacob  and
      Evans, Owain",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.229",
    doi = "10.18653/v1/2022.acl-long.229",
    pages = "3214--3252",
    abstract = "We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58{\%} of questions, while human performance was 94{\%}. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.",
}

@inproceedings{DBLP:conf/iclr/HendrycksBBZMSS21,
  author       = {Dan Hendrycks and
                  Collin Burns and
                  Steven Basart and
                  Andy Zou and
                  Mantas Mazeika and
                  Dawn Song and
                  Jacob Steinhardt},
  title        = {Measuring Massive Multitask Language Understanding},
  booktitle    = {{ICLR}},
  publisher    = {OpenReview.net},
  year         = {2021}
}

@misc{zheng2023judgingllmasajudgemtbenchchatbot,
      title={Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena}, 
      author={Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric P. Xing and Hao Zhang and Joseph E. Gonzalez and Ion Stoica},
      year={2023},
      eprint={2306.05685},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.05685}, 
}

@article{DBLP:journals/corr/abs-2310-08523,
  author       = {Inwon Kang and
                  Sikai Ruan and
                  Tyler Ho and
                  Jui{-}Chien Lin and
                  Farhad Mohsin and
                  Oshani Seneviratne and
                  Lirong Xia},
  title        = {LLM-augmented Preference Learning from Natural Language},
  journal      = {CoRR},
  volume       = {abs/2310.08523},
  year         = {2023}
}