\section{Related Work}
% \subsection{Comparative Question Answering}
In this section, we briefly introduce each subtask for Comparative Question Answering and also discuss the existing LLM evaluation benchmarks.

%Here we cite ____ bla bla bla

\subsection{Comparative Question Answering}

Here, we introduce each subtask and list several papers that addressed the topic.
% The framework we propose here is based on key checks that are supposed to mark a summary on various grounds.
% Rouge evaluation?

\textbf{Comparative Question Identification} aims at classifying questions into two types: comparative and non-comparative. This classification task is solved with both Encoder and Decoder Transformer models ____.
\textbf{Object and Aspect Identification} is a sequence labelling task, aims at finding objects and aspect of comparison in the question. There exist varios datasets and approaches to solve the task, mostly, with Trasnformer models ____.
\textbf{Stance Classification} is another classification task, that identifies the stance of comparative sentences.  ____, ____, and ____ solve the task using standard ML classifier, Encoder-based Transformer, and GPT-4 respectively.
\textbf{Summary Generation} is only partially tackled by ____ and ____. The closest work on multi-document summarization of differing opinions is by ____, which focuses on aggregating diverse opinions and synthesizing them into a coherent summary.

\subsection{LLM Evaluation Benchmarks}
 
 %It is important to specify that the question asks the model to provide an answer on a scale of 1 to 5. 

% Another approach to replicating human evaluation with the use of an LLM is presented by ____ and is called G-Eval. G-Eval is an experiment that uses chain-of-thoughts and a form-filling paradigm together to augment the quality of LLM-authored evaluation. The authors experimented with both text summarization and dialogue generation, showing that their approach achieved a Spearman correlation of 0.514 with human evaluation on the summarization task, which was an unprecedented success. 

Apart from the well-known benchmarks like SuperGLUE ____, MTEB ____, and SQuAD ____, several more challenging benchmarks have gained prominence: MMLU (Massive Multitask Language Understanding) ____,  TruthfulQA ____, BIG-Bench Hard ____. The LLM evaluation framework proposed by ____ involves presenting a Large Language Model with task instructions, a sample to be evaluated and a question. The researchers use LLM evaluation to score parts of both generated and human-written stories. %Under these conditions, LLMs proved to be remarkably successful at performing evaluations, regardless of specific task instructions' formatting. 
To facilitate comprehensive evaluations, several initiatives aggregate multiple benchmarks:
Hugging Face's Big Benchmarks Collection (a centralized platform for various leaderboards), and 
LMSys Chatbot Arena ____: it also utilizes user ratings and GPT-4 evaluations to assess chatbot performance.