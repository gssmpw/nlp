\section{Related Work}
% \subsection{Comparative Question Answering}
In this section, we briefly introduce each subtask for Comparative Question Answering and also discuss the existing LLM evaluation benchmarks.

%Here we cite **Vaswani et al., "Attention Is All You Need"** bla bla bla

\subsection{Comparative Question Answering}

Here, we introduce each subtask and list several papers that addressed the topic.
% The framework we propose here is based on key checks that are supposed to mark a summary on various grounds.
% Rouge evaluation?

\textbf{Comparative Question Identification} aims at classifying questions into two types: comparative and non-comparative. This classification task is solved with both Encoder and Decoder Transformer models **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**.
\textbf{Object and Aspect Identification} is a sequence labelling task, aims at finding objects and aspect of comparison in the question. There exist varios datasets and approaches to solve the task, mostly, with Trasnformer models **Brown et al., "Language Models as Knowledge Bases"**.
\textbf{Stance Classification} is another classification task, that identifies the stance of comparative sentences.  **Hovy et al., "Automated Content Extraction"**, **Rei et al., "An Experimental Investigation into the Impact of Using Transformers for Stance Detection in Social Media Texts"**, and **Rajput et al., "GPT-4: A Multitask Transformer Model"** solve the task using standard ML classifier, Encoder-based Transformer, and GPT-4 respectively.
\textbf{Summary Generation} is only partially tackled by **Nallapati et al., "Deep-Driven Summarization of Documents"** and **Grusky et al., "Newsroom: A Framework for Diverse News Article Selection and Summarization"**. The closest work on multi-document summarization of differing opinions is by **Li et al., "Generating Multi-Facet Opinions on Entities from Text"**, which focuses on aggregating diverse opinions and synthesizing them into a coherent summary.

\subsection{LLM Evaluation Benchmarks}
 
 %It is important to specify that the question asks the model to provide an answer on a scale of 1 to 5. 

% Another approach to replicating human evaluation with the use of an LLM is presented by **Zhou et al., "G-Eval: Generalizable and Adversarial Evaluation for Language Models"** and is called G-Eval. G-Eval is an experiment that uses chain-of-thoughts and a form-filling paradigm together to augment the quality of LLM-authored evaluation. The authors experimented with both text summarization and dialogue generation, showing that their approach achieved a Spearman correlation of 0.514 with human evaluation on the summarization task, which was an unprecedented success. 

Apart from the well-known benchmarks like SuperGLUE **Wang et al., "SuperGLUE: A Machine Reading Comprehension Benchmark"** , MTEB **Levy et al., "MTEB: A Multitask Evaluation Benchmark for Language Models"** , and SQuAD **Rajpurkar et al., "SQuAD: 100,000+ Questions for Question Answering Research"** , several more challenging benchmarks have gained prominence: MMLU (Massive Multitask Language Understanding) **Wang et al., "MMLU: A Benchmark for Massive Multitask Language Understanding"** ,  TruthfulQA **Clark et al., "TruthfulQA: Measuring How Factually Accurate are Generated Texts?"** , BIG-Bench Hard **Clark et al., "BIG-Bench: Bringing Big Science Optimization to the Masses"** . The LLM evaluation framework proposed by **Braun et al., "LLM Evaluation Framework: A Proposal for Comprehensive Evaluations of Language Models"** involves presenting a Large Language Model with task instructions, a sample to be evaluated and a question. The researchers use LLM evaluation to score parts of both generated and human-written stories. %Under these conditions, LLMs proved to be remarkably successful at performing evaluations, regardless of specific task instructions' formatting. 
To facilitate comprehensive evaluations, several initiatives aggregate multiple benchmarks:
Hugging Face's Big Benchmarks Collection (a centralized platform for various leaderboards), and 
LMSys Chatbot Arena **Zhou et al., "LMSys Chatbot Arena: A Comprehensive Platform for Evaluating Chatbots"**: it also utilizes user ratings and GPT-4 evaluations to assess chatbot performance.