\section{Introduction}
In recent years, Transformers have gained significant attention and demonstrated remarkable achievements in computer vision. A notable development in this field was the introduction of the Vision Transformer (ViT) \cite{dosovitskiy2020image}, which utilizes pure Transformers for image classification tasks. Following ViT, several models have been proposed to improve performance, achieving promising results in a range of vision tasks, including image classification, object detection, and segmentation \cite{setyawan2024multi, liang2024swin, gao2023metformer, yu2024spikingvit, hsu2024inpainting}.

Despite the strong performance of the vanilla Vision Transformer (ViT) \cite{dosovitskiy2020image}, it requires between 85 million and 632 million parameters to handle ImageNet classification tasks. Its forward propagation demands substantial computational resources, resulting in slow inference speeds and making it unsuitable for many specific applications. Applying Transformer models in low-cost settings, such as mobile and edge devices, is particularly challenging due to constraints on memory, processing power, and battery life \cite{setyawan2024fpga}. Therefore, our work focuses on constructing a lightweight and efficient deep learning model that reduces computational requirements and power consumption, while ensuring fast inference and high performance on edge devices.

Several studies have attempted to lower the computational complexity of Vision Transformers by integrating them with Convolutional Neural Networks (CNNs) \cite{mehta2021mobilevit, wu2021cvt}. For example, MobileViT uses an inverted bottleneck convolution block from MobileNetV2 \cite{sandler2018mobilenetv2} in the early stages to reduce the computational complexity of ViT. Numerous studies \cite{wang2021pyramid, mehta2022separable, maaz2022edgenext} have identified that the self-attention (SA) mechanism, particularly in the spatial feature mixer of Transformers, is the most computationally demanding component. Pyramid Vision Transformer (PVT) reduces the quadratic complexity ($\mathcal{O}(n^2)$) of SA by applying Spatial Reduction Attention (SRA) to shorten the token length. MobileViTv2 proposes Separable Linear Attention \cite{mehta2022separable} to alleviate the burden of SA, while EdgeNeXt \cite{maaz2022edgenext} employs transposed SA to tackle the complexity challenges for edge device implementation. Another approach, SHViT, suggests that Multi-Head Self-Attention (MHSA) can suffer from feature redundancy across heads, and introduces single-head attention that processes only a quarter of the image tokens or features. However, most of these methods are developed separately, without considering power consumption in limited-resource environments like edge computing devices. 

This paper introduces MicroViT, a novel Vision Transformer model optimized for edge device deployment. The proposed architecture significantly reduces computational complexity and power consumption by utilizing Efficient Single Head Attention (ESHA), which minimizes feature redundancy through group convolutions and single-head SA, processing only a quarter of the overall channels. Built upon the MetaFormer architecture, MicroViT ensures fast inference and low power usage while maintaining high accuracy while evaluated on edge devices, making it ideal for energy-constrained environments.