\section{Result}
For the evaluation of MicroViT, the ImageNet-1K dataset \cite{russakovsky2015imagenet} comprising 1.28 million training images and 50,000 validation images over 1,000 categories was employed. Following the DeiT training method \cite{touvron2021training}, models were trained for 300 epochs at a 224Ã—224 resolution with an initial learning rate of 0.004, utilizing various data augmentations. The AdamW optimizer \cite{loshchilov2017decoupled} was used with a batch size of 512 across three A6000 GPUs.

We assessed model throughput in various computation environments, including a GPU (RTX-3090), a CPU (Intel i5-13500), and specifically the Jetson Orin Nano edge device. For throughput, the GPU and CPU had a batch size of 256, whereas the edge device used a batch size of 64 with ONNX Runtime. To enhance performance during inference, BN layers were fused with adjacent layers when possible. On the Jetson Orin Nano, we also examined power and energy usage during latency tests with 1000 images at a consistent resolution.

We further evaluate MicroViT on object detection on the COCO dataset \cite{lin2014microsoft} utilizing RetinaNet \cite{ross2017focal} and conduct training for 12 epochs (1$\times$ schedule), adhering to the configuration used by \cite{liu2023efficientvit} in mmdetection \cite{mmdetection}. In the object detection experiments, we employ AdamW \cite{loshchilov2017decoupled} with a batch size of 16, a learning rate of $1\times10^{-3}$, and a weight decay rate of 0.025. 


\begin{table}[!ht]
\centering
\caption{Comparison of All MicroViT Variant with SOTA on ImageNet-1K Dataset. Res, Par and FLPs denotes as input resolution, parameters and Floating Operation. GPU and CPU denotes a inference throughput (img/s) in device respectively.}
\begin{tabular}{ m{2.6cm}|c|c|c|c|>{\centering}m{0.5cm}|c }
\hline
Model   & Res  & Par & FLPs &  GPU  & CPU &   Top-1      \\ \hline
% Fasternet-T1\cite{liu2022convnet}              & 224 & 7.6  & 0.85 & \textbf{7037}  & 149.3  &   & 76.2      \\
MobileNetV2-1.0\cite{sandler2018mobilenetv2}& 224 & 3.5 & 0.314 & 4527 & 82 & 72.0   \\
MobileViT-XXS\cite{mehta2021mobilevit}  & 256 & 1.3 & 0.261 & 3218 & 99  & 69.0   \\
MobileViTV2-0.5\cite{mehta2022separable}  & 256 & 1.4 & 0.480 & 3885 & 68  & 70.2   \\
EdgeNeXt-XXS\cite{maaz2022edgenext}  & 256 & 1.3 & 0.261 & 3975 & 245  & 71.2   \\
Fasternet-T0\cite{chen2023run}  & 224 & 3.9 & 0.340 & 11775 & 311 & 71.9   \\
SHViT-S1\cite{yun2024shvit} & 224  & 6.3 & 0.241 & 15280 & 475 & 72.8 \\
\rowcolor{gray!30}
\textbf{MicroViT-S1}                    & 224  & 6.4 & 0.231 & 17466 & 552 & 72.6 \\ \hline 
EFormerV2-S0\cite{li2023rethinking}& 224  & 3.6 & 0.407 & 1191 & 91 & 73.7 \\ 
EdgeNeXt-XS\cite{maaz2022edgenext}   & 256 & 2.3 & 0.536 & 2935 & 139  & 75.0   \\
EfficientViT-M4\cite{liu2023efficientvit} & 224 & 8.8 & 0.303 & 10093 & 379 & 74.3   \\
MobileViT-XS\cite{mehta2021mobilevit}   & 256 & 2.3 & 0.935 & 1740 & 43 & 74.8   \\
MobileNetV3-L\cite{howard2019searching}& 224 & 3.5 & 0.314 & 4527 & 82 & 75.2  \\
SHViT-S2\cite{yun2024shvit}            & 224  & 11.5 & 0.366 & 12007 & 367 & 75.2 \\
\rowcolor{gray!30}
\textbf{MicroViT-S2}                    & 224 & 10.0 & 0.345 & 14154 & 435 & 74.6 \\ \hline
FastViT-T8\cite{vasu2023fastvit}   & 256 & 4.0 & 0.687 & 3719 & 83 & 76.2   \\
Fasternet-T1\cite{chen2023run}  & 224 & 7.6 & 0.851 & 7151 & 130 & 76.2   \\
EfficientViT-M5\cite{liu2023efficientvit}& 224 & 12.5 & 0.526 & 6807 & 233 & 77.1   \\
SHViT-S3\cite{yun2024shvit}            & 224  & 14.1 & 0.601 & 8180 & 224 & 77.8 \\
% \hline 
\rowcolor{gray!30}
\textbf{MicroViT-S3}                   & 224 & 16.7 & 0.580 & 9288 & 232 & 77.1 \\ \hline 
    \end{tabular}
    \label{tab:imgnet-result}
\end{table}
\subsection{ImageNet-1K Classification Result}
Table \ref{tab:imgnet-result} presents a comparison of various MicroViT variants with state-of-the-art (SOTA) models on the ImageNet-1K dataset. The evaluation focuses on models' computational efficiency and accuracy, highlighting the trade-offs between resource consumption and performance. 

MicroViT-S1 demonstrated superior performance compared to traditional CNN models, surpassing MobileNetV2-1.0\cite{sandler2018mobilenetv2} and Fasternet-T0\cite{mehta2022separable}, with a $3.6 \times$ faster in GPU and $6.7 \times$ in CPU throughput, while maintaining an accuracy advantage of 0.8 over MobileNetV2-1.0. Additionally, MicroViT-S2 outperformed mobile transformers like EfficientFormerV2-S0\cite{li2023rethinking} and EfficientViT-M4\cite{liu2023efficientvit}, achieving $0.3\%$ better accuracy with similar efficiency metrics. Across the MicroViT models, CPU throughput is robust, notably with MicroViT-S1 achieving 552 img/s, which is $8 \times$ faster than several EfficientViT variants, illustrating MicroViT's adaptability to both high-end GPU and CPU settings.


Table \ref{tab:edge-result} presents the performance of MicroViT variants against various SOTA models on the Edge device using ONNX. MicroViT-S1's throughput reaches 773 img/s, efficiently managing large image volumes on the Jetson Orin Nano. This surpasses several SOTA models like MobileViT-XS\cite{mehta2021mobilevit} and EfficientFormer-V2-S0\cite{li2023rethinking}, making MicroViT-S1 optimal for rapid image processing applications. Furthermore, it has a 9.1 ms latency, outperforming MobileNetV2-1.0\cite{sandler2018mobilenetv2} and EdgeNeXt-XS\cite{maaz2022edgenext}, supporting real-time use. It consumes 2147 Joules, achieving high energy efficiency with $\eta=3.7$. Likewise, MicroViT-S2 and MicroViT-S3 balance throughput and energy use, maintaining accuracy, thus ideal for resource-constrained edge devices with superior power efficiency over other lightweight vision transformers.
\begin{table}[!ht]
\centering
\caption{Comparison of All MicroViT Variant and SOTA on ImageNet-1K Dataset with NVIDIA Jetson Orin Nano Edge Device using ONNX format.}
\begin{tabular}{ m{2.6cm}|>{\centering}m{0.5cm}|>{\centering}m{0.6cm}|>{\centering}m{0.9cm}|>{\centering}m{0.8cm}|c }
\hline
\multirow{2}{*}{Model} & Thg & Lat & Avg Pow & Energy  & $\eta$ \\ 
         & img/s & (ms) & (W) & (Joule) & \% / J \\ \hline
MobileNetV2-1.0\cite{sandler2018mobilenetv2} & 234 & 6.7 & 3549 & 23.9 & 3.01 \\
MobileViT-XXS\cite{mehta2021mobilevit}  & 184 & 9.6 & 3428 & 32.4 & 2.13   \\
MobileViTV2-0.5\cite{mehta2022separable}  & 208 & 10.9 & 2887 & 31.6 & 2.22 \\
EdgeNeXt-XXS\cite{maaz2022edgenext}     & 257 & 8.1 & 2805 & 22.6 &  3.15  \\
Fasternet-T0\cite{chen2023run}   & 675 & 8.4 & 2419 & 20.4 &  3.52 \\
SHViT-S1 \cite{yun2024shvit}       & 813  & 12.6 & 2069 & 26.0 & 2.80  \\
\rowcolor{gray!30}
\textbf{MicroViT-S1}                & 773  & 9.1 & 2147 & 19.6 & 3.7 \\ \hline 
EFormerV2-S0\cite{li2023rethinking}  & 257  & 10.9 & 2847 & 31.1 & 2.37   \\ 
EdgeNeXt-XS\cite{maaz2022edgenext}      & 168 & 11.1 & 3031 & 33.6 &  2.32   \\
EfficientViT-M4\cite{liu2023efficientvit} & 587 & 18.0 & 2019 & 36.3 &  2.05   \\
MobileViT-XS\cite{mehta2021mobilevit}   & 96.6 & 14.3 & 3730 & 53.3 & 1.40    \\
MobileNetV3-L\cite{howard2019searching} & 310 & 8.3 & 2743 & 22.6 &  3.33 \\
SHViT-S2 \cite{yun2024shvit}            & 598  & 12.7 & 2306 & 29.4 & 2.56  \\
\rowcolor{gray!30}
\textbf{MicroViT-S2}                      & 567 & 9.9 & 2481 & 24.3 & 3.07 \\ \hline
FastViT-T8\cite{vasu2023fastvit}          & 176 & 8.6 & 3622 & 30.8 &  2.47  \\
Fasternet-T1\cite{chen2023run}     & 421 & 8.8 & 2860 & 25.3 &  3.01  \\
EfficientViT-M5\cite{liu2023efficientvit} & 409 & 21.0 & 2180 & 45.9 & 1.68    \\
SHViT-S3\cite{yun2024shvit}               & 425  & 14.6 & 2500 & 36.6 & 2.13  \\
% \hline 
\rowcolor{gray!30}
\textbf{MicroViT-S3}                    & 398 & 10.9 & 2609 & 28.6 & 2.69  \\ \hline 
    \end{tabular}
    \label{tab:edge-result}
\end{table}

\subsection{Object Detection Result}
We compare MicroViT-3 with efficient models \cite{sandler2018mobilenetv2, howard2019searching, liu2023efficientvit} on the COCO \cite{lin2014microsoft} object detection task, and present the results in Table \ref{tab:obj} Specifically, MicroViT-3 surpasses MobileNetV2 \cite{sandler2018mobilenetv2} by
7.7\% AP with comparable Flops. Compared to the EfficientViT-M4, our MicroViT-3 uses 46.8\% fewer Flops while achieving 3.3\% higher AP, demonstrating its capacity and generalization ability in different vision tasks.

\begin{table}[!ht]
\centering
\caption{Object detection on COCO val2017 with RetinaNet. $AP^b$ denote bounding box average precision. The FLOPs (G) are measured at resolution 1280 $\times$ 800.
}
\begin{tabular}{ m{2.4cm}|>{\centering}m{0.8cm}|>{\centering}m{0.8cm}|>{\centering}m{0.8cm}|>{\centering}m{0.8cm}|c}
\hline
Backbone & $AP^{b}$  & $AP^{b}_{50}$ & $AP^{b}_{75}$ & Par & FLPs  \\ \hline
MobileNetV2\cite{sandler2018mobilenetv2} & 28.3& 46.7& 29.3& 3.4& 300 \\
MobileNetV3\cite{howard2019searching} & 29.9& 49.3& 30.8& 5.4 & 217   \\ 
EfficientViT-M4\cite{liu2023efficientvit}&32.7 &52.2 &34.1 & 8.8 & 299  \\
\rowcolor{gray!30}
\textbf{MicroViT-S3} & 36.0 & 56.6 & 38.2 & 26.7 & 159    \\ \hline

\end{tabular}
\label{tab:obj}
\end{table}

\subsection{Ablation Study}

Table \ref{tab:abl-result} assesses the effects of three ablations on the MicroViT-S2 model as the baseline. The first ablation, low resolution SA with SR=2, slightly increases the parameter count from 11.0M to 11.1M, with GPU throughput remaining nearly unchanged from 12009 to 12029 images per second. However, it increases latency from 9.9 ms to 10.7 ms and decreases Top-1 accuracy to 72.5\%, resulting in a minor drop in efficiency ($\eta$) from 3.0 to 2.8. This indicates a small trade-off between accuracy and computational cost due to the architectural modification.

The Next ablation, without group convolutions, increases the model's complexity significantly to 20.5M parameters, with higher GFLOPs. This results in lower throughput and efficiency ($\eta=1.8$) but achieves the highest accuracy. However, this variant consumes more energy, making it ideal for use cases where accuracy is prioritized over efficiency.

The ablation study shows that even though spatial reduction can increase throughput, the latency is increased, resulting in an efficiency drop. Group convolution successfully increases the efficiency in ESHA compared to other attention models, such as vanilla attention in MobileViT, maintaining lower complexity and energy usage.

\begin{table}[!ht]
\centering
\caption{Ablation study of MicroViT  on ImageNet-1K Dataset. The Baseline model is MicroViT-S2.}
\begin{tabular}{ m{1.6cm}|c|c|c }
\hline
\textbf{Ablation} & Baseline & Low Res Attn & W/O Group  \\ \hline
\textbf{Param}      & 11.0  & 11.1  & 20.5   \\  
\textbf{GFLOPs}     & 0.343 & 0.344 & 0.4977 \\ 
\textbf{GPU}        & 14154 & 14179 & 13511  \\
\textbf{Throughput} & 567   & 570   & 503    \\
\textbf{Latency}    & 9.9   & 10.7  & 12.5   \\
\textbf{Avg Pow}    & 2481  & 2512  & 3288   \\
\textbf{Energy}     & 24.29  & 25.8  & 41.2   \\
\textbf{Top-1}      & 72.7  & 72.5  & 73.2   \\
\textbf{Efficiency ($\eta$)}& 3.07  & 2.8 & 1.8 \\
\hline


    \end{tabular}
    \label{tab:abl-result}
\end{table}

