\pdfoutput=1
\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\makeatletter
\newcommand*\titleheader[1]{\gdef\@titleheader{#1}}
\AtBeginDocument{%                                                               
  \let\st@red@title\@title
  \def\@title{%                                                                 
    \bgroup\normalfont\normalsize\centering\@titleheader\par\egroup
    \vskip1ex\st@red@title}
}
\makeatother

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{array}
\usepackage{graphicx}
\usepackage{textcomp}
% \usepackage{xcolor}
\usepackage[table]{xcolor}
\usepackage{multirow}
\usepackage{subcaption}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{listings}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    % keywordstyle=\color{magenta},
    keywordstyle = {\color{magenta}},
    keywordstyle = [2]{\color{lime}},
    keywordstyle = [3]{\color{yellow}},
    keywordstyle = [4]{\color{teal}},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

\title{MicroViT: A Vision Transformer with Low Complexity Self Attention for Edge Device \\
%\thanks{Identify applicable funding agency here. If none, delete this.}
}
\author{\IEEEauthorblockN{Novendra Setyawan\textsuperscript{1,3}, Chi-Chia Sun\textsuperscript{*,2}, Mao-Hsiu Hsu\textsuperscript{1}, Wen-Kai Kuo\textsuperscript{1}, Jun-Wei Hsieh\textsuperscript{4}}
\IEEEauthorblockA{\textit{\textsuperscript{1}Department of Electro-Optics Engineering, National Formosa University, Taiwan}\\
\textit{\textsuperscript{2}Department of Electrical Engineering, National Taipei University, Taiwan} \\
\textit{\textsuperscript{3}Department of Electrical Engineering, University of Muhammadiyah Malang, Indonesia} \\
\textit{\textsuperscript{4}College of Artificial Intelligence and Green Energy, National Yang Ming Chiao Tung University, Taiwan} \\
\textit{chichiasun@gm.ntpu.edu.tw\textsuperscript{*}}
} 
}

\titleheader{\vspace{-20pt}To appear at the 2025 IEEE International Symposium on Circuits and Systems (ISCAS), May 25-28 2025, London, UK.}

\begin{document}

\maketitle

\begin{abstract}

The Vision Transformer (ViT) has demonstrated state-of-the-art performance in various computer vision tasks, but its high computational demands make it impractical for edge devices with limited resources. This paper presents MicroViT, a lightweight Vision Transformer architecture optimized for edge devices by significantly reducing computational complexity while maintaining high accuracy. The core of MicroViT is the Efficient Single Head Attention (ESHA) mechanism, which utilizes group convolution to reduce feature redundancy and processes only a fraction of the channels, thus lowering the burden of the self-attention mechanism. MicroViT is designed using a multi-stage MetaFormer architecture, stacking multiple MicroViT encoders to enhance efficiency and performance. Comprehensive experiments on the ImageNet-1K and COCO datasets demonstrate that MicroViT achieves competitive accuracy while significantly improving  $3.6 \times$ faster inference speed and reducing energy consumption with 40\% higher efficiency than the MobileViT series, making it suitable for deployment in resource-constrained environments such as mobile and edge devices.

\end{abstract}

\begin{IEEEkeywords}
Classification, Self Attention, Vision Transformer, Edge Device.
\end{IEEEkeywords}

\input{section/1_introduction}
\input{section/2_related_works}
\input{section/3_method}
\input{section/4_result}
\input{section/5_conclusion}

\bibliography{biblio}
\bibliographystyle{ieeetr}
\end{document}
