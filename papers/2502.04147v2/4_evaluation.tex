% !TeX root = 0_main


\section{\toolname's Evaluation}
We conducted a preliminary evaluation to measure \toolname's models' predictive performance as well as \toolname's usability and usefulness. \toolname's GitHub repository provides details of the evaluation methodology, results, and necessary data to replicate and verify the evaluation~\cite{repl_pack}. 

\subsection{Model Evaluation}
% \os{we need to explain why we chose those models: didn't we do a literature review and selected the best by tracking which models were evaluated and outperformed others? we need to provide details along those lines}
To select appropriate models for our features, we conducted an extensive literature review, experimented with various models, and chose those that displayed the best predictive performance. Our approach was to replicate the original evaluation of the models by following the methodologies and test datasets provided in their respective papers. For the similar issue detection feature, we chose the RTA model~\cite{fang2023representthemall} fine-tuned in RTA's duplicate issues training dataset~\cite{representThemAllDataset}. Their test dataset~\cite{representThemAllDataset} has 15,288 issue pairs (approximately 60\% of the pairs were duplicates, and the remaining were non-duplicates) from 6 large open-source projects. Overall, we obtained 97.3\% accuracy, 97.5\% precision, and 98.8\% recall. For issue severity prediction, we selected the RTA model~\cite{fang2023representthemall} fine-tuned in RTA's issue severity training dataset~\cite{representThemAllDataset}. For this task, the test dataset has 15,510 issues of 6 projects with 5 severity classes. The distribution of the severity classes was between 15\% and 24\%.  Overall, we obtained 65.6\% accuracy, 67.3\% precision, and 64.6\% recall. For the bug localization feature, we fine-tuned the Llama-2-7b-chat model on the dataset provided by Bogomolov \etal~\cite{longCodeArena}, and we evaluated the modelâ€™s prediction capability on their 150 test issues~\cite{longCodeBLDataset}. We achieved 34\% accuracy, 20\% Recall@2 (R@2), 31\% Precision@2 (P@2), and a MAP of 29\%. 
\looseness=-1
% \os{briefly describe the dataset: size, systems they come from, domains of the systems, data splits, etc., same for the other models/features -- update: I see some info is provided at the end of this sentence, but still more details might be needed}
% \os{were the results of the original paper replicated?}.
% -> yes
% \os{this sounds like a low performance. Do other models perform like this? why did you choose this model?}
% -> this model had the best accuracy among the free open-source LLMs

\subsection{User Study}
We conducted a user study involving five professional developers with three to eight years of issue management experience who work on different projects at Samsung Research Bangladesh. 
%We selected the participants from our professional network at Samsung R\&D Institute - Bangladesh.
%, but we ensured they adhered to a formal agreement to remain unbiased. 
The goal of the study was to evaluate (i) \toolname's usefulness/usability {(RQ1/RQ2)} and (ii) the predictive accuracy of \toolname's suggestions {(RQ3)}.
% \os{we need to describe how we recruited the participants, and how we ensured the gave unbiased answers (since all if not most of them know Ahmed)}
% \os{since we will need space, this paragraph can be summarized drastically, without losing information too much}

\subsubsection{Methodology}
For the similar issue detection feature, we first selected 2 issues as queries and their respective duplicates and 8 non-duplicate issues from the RTA~\cite{fang2023representthemall} model's duplicate bug report test dataset~\cite{representThemAllDataset}. These issues come from the OpenOffice project~\cite{openoffice}. To evaluate the severity prediction feature, focusing on the same project, we first categorized the RTA~\cite{fang2023representthemall} model's issue severity test dataset~\cite{representThemAllDataset} into 2 groups: (i) issues for which the model predicted the severity correctly and (ii) issues where the model mispredicted the severity. Then, we chose one issue from each group randomly. For the bug localization feature, we chose one of the projects from Bogomolov \etal's dataset~\cite{longCodeBLDataset}: wso2/testgrid~\cite{wso2}. Each issue in this project had two buggy code files on average. Then, we ran the bug localization model on each issue three times since LLM outputs can vary slightly each time and selected the common suggestions that occurred in all three runs. After that, we divided these issues into two groups: (i) issues for which the model predicted at least one correct buggy code file (\ie present in the ground truth) in its top-5 suggestions and (ii) issues where the model failed to predict any buggy code files in the top-5 suggestions. We selected one issue from each group as queries. Our goal in selecting both successful and unsuccessful cases across various features was to ensure a fair evaluation, allowing participants to experience these scenarios and better assess the tool's usefulness and usability.

% \os{I think we need to explain briefly what the users had to do in the study, including how they got familiar with the tool, what kind of questions were asked (Likert-scale + open-ended questions, and that we gave them the ground truth to guide them on how to judge the tool suggestions}
% \os{remove this sentence, instead, as my last comment says: briefly describe the methodology}

In the study, the participants first were introduced to the tool with detailed guidelines. Then, they engaged in answering a questionnaire that assessed the accuracy, usefulness, and usability of the tool's suggestions. The study survey questions were a combination of Likert-scale and open-ended questions. We provided participants with ground truth data for reference.

\subsubsection{RQ1/RQ2 Results}
The participants (\aka users) evaluated how easy-to-use and practical \toolname is.

% \os{not sure we need to report \%, consider dropping them if we need space, i.e., to save lines})
\underline{Readability of \toolname's Suggestions:} Of five users, three rated \toolname's suggestions for the three features as ``very easy" to understand, while two rated them as ``moderately easy" (on a 5-point Likert scale, these options are the most positive).
\looseness=-1

%The other available options were ``neutral," ``moderately hard," and ``very hard."\newline
% \os{this is a likert-scale question, are these options the extreme ones in the scale? what were the other options? clarify}
\underline{{\toolname Overall Usability:}} Four users found \toolname easy to use, while one user was unsure. All the users agreed that \toolname's feedback comments were easy to understand and displayed useful information.

\underline{{\toolname Responsiveness:}} \toolname's average response time for showing results for all three features was approximately 90 seconds (for a repository with $\approx$20 issues and $\approx$50 code files). Four users found \toolname ``very responsive'' and one user found it ``moderately responsive'' (on a 5-point Likert scale, these are the most positive options).

\underline{{\toolname Usefulness for Issue Management:}} Four users ``completely agreed," while one user ``somewhat agreed" with the statement: ``The combination of \toolname's three features is helpful for issue management".

\underline{New feature suggestions}: The participants suggested additional features for \toolname, such as automated issue content identification (\eg identifying the reproduction steps), quality assessment, automated program repair, and developer recommendations for issue resolution.
%\os{I dropped this, since it doesn't add much value and we need space}\textbf{RQ1/RQ2 Summary:} Overall, the developers found SPRINT's features, suggestions, and responsiveness very useful and practical in issue management. Moreover, they suggested potential improvements, such as including more issue information as summaries in the feedback comments. \newline

\subsubsection{RQ3 Results} 
The participants evaluated the accuracy of the suggestions made by the \toolname's three main features.

\underline{{Similar Issue Identification:}} All the users agreed that \toolname correctly suggests all the similar issues for the two queries. However, the reporters found that \toolname suggests one or two extra non-duplicates for the first query---though, it perfectly predicts the two expected similar issues for the second query.

\underline{{Severity Prediction:}} Three of five users agreed that \toolname correctly predicted the severity of the issues, whereas two users were unsure about the predictions. They stated that \toolname correctly identified the severity class for one query and mispredicted the other, but the misprediction was close to the actual severity class (\toolname predicted `Trivial' instead of `Minor').
\looseness=-1

% \os{add here the mispredicted and close classes}.
\underline{{Bug Localization:}} 
\rev{All five users identified correct and incorrect predictions, aided by the ground truth references provided in the study survey. Two users identified one correct prediction in the top-5 suggestions sufficient, while three others suggested improving \toolname's accuracy. Overall, all participants valued the feature's responsiveness and potential usefulness.}
%\rev{All five users successfully recognized both correct and incorrect predictions, aided by the detailed guidelines provided in the user study questionnaire. Among them, two users considered at least one correct prediction in the top 5 suggestions as good predictive performance, while the remaining three suggested that \toolname the predictive accuracy should be better. Overall, participants appreciated the feature's responsiveness and saw its potential value, provided its accuracy is further improved.}
%\rev{All five users recognized the correct predictions and also identified all the incorrect predictions since we provided a detailed guideline about the buggy code files in the user study questionnaire. Among them, 2 users agreed that 1 correct prediction in the top-5 suggestions indicated good predictive performance while 3 users stated that \toolname should suggest more buggy files in its top-5 suggestions. Overall, the participants praised the responsiveness of the feature and thought it might be useful if its accuracy is improved.}
%\newline
%\os{due to space reasons, I dropped this, but incorporated some of it above -- feel free to improve it}\textbf{RQ3 Summary:} Overall, the developers found SPRINT's features quite accurate in terms of predictive performance. Though they were somewhat unsure about the accuracy of the bug localization feature, they also stated that a responsive run-time feature like this, even with low accuracy is very promising.\newline