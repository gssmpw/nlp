\section{Related Work}
\label{related-works}
\textbf{Hallucination mitigation: } Recent efforts aim to enhance LLMs' truthfulness during inference through contrastive decoding or representation editing. Representation editing involves identifying a truthful direction using human-labeled data and adjusting model activations to align with truthfulness, as seen in methods like ITI **Zellers**, "Do I Need to Understand the Question?"** __**Peng**, "Few-Shot Adversarial Training for Task-Agnostic Transfer"__. Contrastive decoding methods modify output probabilities by comparing probability distributions between different models or layers of a single model, such as SH2 **Liu**, "Improving Multi-Step Reasoning via Effective Utilization of External Knowledge"** __**Shin**, "Learning to Generate Synonyms and Relations for Natural Language Inference"__, to improve truthfulness. However, these methods often suffer from high computational costs and reliance on labeled data, making them less practical for resource-constrained applications.
\\
\textbf{LLM Ensemble: } We can broadly categorize this field into supervised and unsupervised learning. In unsupervised methods, answer consistency, as exemplified by **Lewis**, "Pre-training versus fine-tuning: An empirical study on neural conversation models"__, generates multiple solutions from different LLMs and uses majority voting to produce the final output. **Steedman**, "The Structure of Reference"__ selects the solution with the highest similarity to the input as the ensemble output. Among supervised ensemble methods, LLM-Blender **Hendrycks**, "Pre-training in Natural Language Processing: A Survey"__ is the most popular, requiring pairwise comparisons of models in the pool and a costly training procedure.
\end{comment}

\begin{figure}
\centering
\includegraphics[width=.5\textwidth]{images/fuser.png}
\caption{Overview of our UAF architecture}
\label{fig:fuser}
\end{figure}

\vspace{-1pt}