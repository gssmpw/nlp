\section{Experiments}
\label{sec:exper}
We investigate the following questions:
\begin{itemize}[leftmargin=*]
    \item \textbf{RQ1:} How effective are uncertainty methods in detecting hallucinations?
    \item \textbf{RQ2:} To what extent do LLMs' relative accuracy and hallucination detection abilities vary across examples?
    \item \textbf{RQ3:} How does UAF perform against SOTA baselines?
    \item \textbf{RQ4:} How does UAF performance vary with ensemble size $K$?
\end{itemize}
\vspace{-4pt}
\subsection{Experimental Setup}
\textbf{Algorithms: }We implement UAF using three uncertainty measures: Perplexity\cite{selgen}, Semantic Entropy\cite{semantic_unc}, and Haloscope\cite{haloscope}, each chosen to represent  distinct category of approaches as mentioned in Section \ref{sec:intro}. We exclude prompting-based methods due to LLMs' tendency to be \textit{overconfident} when verbalizing their confidence \cite{can_llm_unc}. We sample $10\%$ of the data as a validation set for our SELECTOR module and tune the hyperparameter $K$ on this set.
We compare UAF against representation editing methods: ITI \cite{iti} and TruthX \cite{truthx}, as well as contrastive decoding-based methods: DoLa \cite{dola} and SH2 \cite{sh2}. We also evaluate UAF against three prominent ensemble-based methods: Consistent \cite{wang2022self}, MLM \cite{mlm}, and LLM-Blender \cite{llmblender}.We adapt LLM-Blender for zero-shot evaluation by prompting Llama-13B to generate the final answer from the ensemble's candidate responses, instead of training the fusion model.
%\textbf{Uncertainty Algorithms:} We experiment with one method from each category: Perplexity, Semantic Entropy, and Haloscope. We exclude prompting-based methods due to LLMs' tendency to be \textit{overconfident} when verbalizing their confidence \cite{can_llm_unc}.
%\\
%\textbf{Baseline Hallucination mitigation algorithms: }We compare UAF against representation editing methods (ITI \cite{iti}, TruthX \cite{truthx}) and contrastive decoding-based methods (DoLa \cite{dola} and SH2 \cite{sh2}).
%\\
%\textbf{Baseline LLM-ensemble based algorithms:} We evaluate UAF against three prominent zero-shot methods for combining responses from multiple LLMs to address hallucinations: (1) Majority voting to select the most consistent answer (Consistent) \cite{wang2022self}, (2) Semantic similarity to choose the answer most aligned with the question (MLM) \cite{mlm}, and (3) LLM-Blender\cite{llmblender}, which learns to generate the final answer by fusing candidate responses. For zero-shot evaluation, we adapt LLM-Blender by prompting another LLM (Llama-13B) to generate the final answer from all candidate responses.
\\
\textbf{Datasets:} For our evaluation,  we consider two open-book QA datasets,  TruthfulQA\cite{truthfulqa} and FACTOR-news\cite{factor}, which contain 817 and 1,036 multiple-choice QA pairs, respectively, following prior works \cite{iti,dola,truthx} as well as a generative QA dataset TriviaQA \cite{triviaqa} (\textit{rc.nocontext subset}) with $9960$ QA pairs.
%Following \cite{iti,dola,truthx}, to measure truthfulness, we consider two open-book QA datasets for evaluation - TruthfulQA\cite{truthfulqa} and FACTOR-news\cite{factor}. Both of these are multiple choice QA task having $817$ and $1036$ QA pairs respectively. We also evaluate on generative QA dataset TriviaQA\cite{triviaqa} (\textit{rc.nocontext subset}) having $9960$ QA pairs.
\\
\textbf{Metrics:} For multiple-choice tasks (TruthfulQA, FACTOR), correctness is determined based on selection of the gold answer while in case of Trivia-QA, correctness is based on exact match of the ground truth. We report two metrics: \texttt{accuracy}, i.e., the fraction of correctly answered questions, and  \texttt{self assessment of hallucination}, i.e., the area under ROC curve (AUROC) for detecting hallucinations using uncertainty score. 
\\
\textbf{LLM-Pool:} We use open-source LLMs like Llama2-13B, llama 3.2-3B, Alpaca, Vicuna, Mistralv3-7B and Opt-13B. Details of these models can be found in \cite{llm}.

\subsection{RQ1: Effectiveness of uncertainty methods in hallucination detection}
\begin{table}[t]
\centering
\caption{AUROC of uncertainty methods in TruthfulQA. Higher values above $0.5$ is preferred.}
\begin{adjustbox}{width=\columnwidth,center}
\def\arraystretch{1.2}%
\begin{tabular}{l|ccc}
\hline
LLMs         & Perplexity\cite{selgen} & Haloscope\cite{haloscope} & Semantic Entropy\cite{semantic_unc}  \\
\hline
Alpaca-7B    & 0.71      & 0.72        & 0.69                           \\
Vicuna-7B    & 0.8       & 0.87        & 0.82                            \\
Llama2-13B   & 0.68      & 0.71        & 0.72                           \\
Llama3.2-3B  & 0.60    & 0.69         &  0.66                           \\
Mistralv3-7B & 0.51      & 0.78        & 0.63                           \\
Opt-13B      & 0.71      & 0.73        & 0.7                          \\
\hline
\end{tabular}
\end{adjustbox}
\label{table:unc_metrics}
\end{table}

We compare the performance of three uncertainty methods—Perplexity, Semantic Entropy, and Haloscope—across various LLMs in Table \ref{table:unc_metrics}. For each example in TruthfulQA, we prompt the LLMs and mark their responses as either truthful or hallucinatory based on the ground-truth answer. We also generate a corresponding normalized uncertainty score, $\in (0,1)$, for each response using one of the above methods.
We report the area under the receiver operating characteristic curve (AUROC) in Table \ref{table:unc_metrics}, which measures the performance of binary classification of truthful vs. hallucinatory responses by varying the thresholds of uncertainty scores. As shown in Table \ref{table:unc_metrics}, most LLMs achieve high AUROC scores (above the random chance threshold of $0.5$), indicating that uncertainty methods are effective at detecting the models' hallucinatory responses

%\textbf{Different LLMs may be uncertain about different data points.} Columns $>$Vic($\%$) and $>$Mist($\%$) show the percentage of examples where each LLM correctly detects hallucinations missed by Vicuna and Mistralv3-7B, respectively, using the Haloscope uncertainty measure. While Vicuna and Mistralv3 perform well, other LLMs still outperform them in some cases. This highlights that LLMs may not share uncertainty on the same data points, emphasizing the value of pooling uncertainty from multiple models in an ensemble. 



\subsection{RQ2: Variation in accuracy and hallucination detection across LLMs}
We prompt five individual LLMs from our LLM pool on TruthfulQA and compute uncertainty scores using the Haloscope method for each generated response. An incorrect response is considered correctly detected as a hallucination if its uncertainty score is greater than 0.5. In Table \ref{table:variation}, each cell in the $i^{th}$ row and $j^{th}$ column shows a tuple: (1) the percentage of examples where the $j^{th}$ LLM generates the correct response missed by the $i^{th}$ LLM, and (2) the percentage where the $j^{th}$ LLM detects hallucinations missed by the $i^{th}$ LLM. From this, we conclude that each LLM outperforms others in both accuracy and hallucination detection for a substantial proportion of examples. This highlights that the optimal model can vary significantly across data points, emphasizing the value of pooling strengths from multiple models in an ensemble.

\begin{table}[h!]
\centering
\caption{Variation in accuracy and hallucination detection capability across each LLMs in TruthfulQA. }
\begin{adjustbox}{width=\columnwidth,center}
\def\arraystretch{1.2}%
\begin{tabular}{l|ccccc}
\hline
             & Alpaca-7B  & Vicuna-7B  & Llama2-13B  & Llama3.2-3B & Mistralv3-7B \\
             \toprule
Alpaca-7B    & NA         & 6.8 , 17   & 12.3 , 28.2 & 19 , 8.6    & 23.6 , 32.1  \\
Vicuna-7B    & 5.6 , 21.3 & NA         & 14.9 , 33.2 & 17.5 , 19.5 & 21.1 , 16.4  \\
Llama2-13B   & 9.5 , 21   & 8.2 , 19.1 & NA          & 16.6 , 10   & 18 , 25.4    \\
Llama3.2-3B  & 8.6 , 16.6 & 10.2 , 24  & 17 , 28.7   & NA          & 11.5 , 30    \\
Mistralv3-7B & 11 , 19.6  & 6.6 , 20.8 & 15.8 , 25.2 & 8.6 , 14.8  & NA          \\
\bottomrule
\end{tabular}
\end{adjustbox}
\label{table:variation}
\end{table}

\begin{table*}[t!]
\centering
\caption{Results on TruthfulQA, TriviaQA and FACTOR-news datasets. We report accuracy (\%). Best results are bolded.}
\begin{adjustbox}{width=\textwidth,center}

\begin{tabular}{l|cc|cc|ccc|ccc|c}
\hlineB{3}
\multicolumn{1}{c|}{} & \multicolumn{2}{c|}{Representation Editing} & \multicolumn{2}{c|}{Contrastive Decoding} & \multicolumn{3}{c|}{Ensemble} & \multicolumn{3}{c|}{UAF}    & \multicolumn{1}{c}{GPT-4}      \\[4pt]
                     & ITI                & TruthX                & DoLa                & SH2                & Consistent  & MLM  & LLM-Blender & Perplexity & Haloscope & Semantic-Entropy & (as reported \cite{gpt4})\\[3pt]
                    \toprule
TruthfulQA           &    $38.9$                &         $54.2$              &           $33.2$          &          $34.2$          &      $41.7$        &   $29.0$   &   $48.4$     &     $56.5$       &    $\mathbf{60.8}$       &     $51.3$   & $59$ \\[2pt]
TriviaQA             &      $65.9$              &        $69.8$               &         $66.1$            &      $70.2$              &      $65.6$        &   $57.6$   &   $59$     &       $\mathbf{76}$     &     $71.5$      &   $66.4$  & $\mathbf{87}$    \\[2pt]
FACTOR-news               &        $53.3$            &          $65.8$             &        $66.2$             &       $77$             &      $61.4$        &   $50.8$   &    $64.7$    &     $72.5$       &     $\mathbf{81.4}$      &  $69.2$ & -
\\
\hlineB{3}
\end{tabular}
\end{adjustbox}
\label{table:uaf_perf}
\end{table*}

\vspace{-2mm}

\subsection{RQ3: UAF results on benchmark datasets}
Table \ref{table:uaf_perf} shows UAF results across all datasets. Representation editing and contrastive decoding methods are applied to each LLM in our LLM-Pool, reporting the best result, while ensembling uses all LLMs in the pool. We implemented UAF using three different uncertainty measures: Perplexity, Haloscope, and Semantic-Entropy. UAF (with Haloscope) outperforms GPT-4 by $3\%$ on TruthfulQA and surpasses the best baseline by $12\%$. In TriviaQA and Factor-news, UAF outperforms the best baseline by at least $8\%$ and $6\%$, respectively, and narrows the gap with GPT-4. UAF with Semantic-Entropy performs relatively worse, likely due to the method's limited effectiveness in detecting hallucinations. This highlights that weak uncertainty modeling can hurt overall ensemble performance. Interestingly, ensembling strategies like Consistent and LLM-Blender match or outperform some complex hallucination mitigation methods, emphasizing the benefits of ensembling framework.
%\textbf{LLMs have diverse strengths and weakness: } In Table \ref{table:indi}, we report the zero-shot accuracy of each individual LLM on TruthfulQA. Mistralv3-7B and Llama3.2-3B are the top performers. The columns $>Mistralv3(\%)$ and $>Llama3.2(\%)$ show the percentage of examples where other LLMs outperform Mistralv3-7B and Llama3.2-3B, respectively. Despite their strong performance, both still have a substantial number of cases where other LLMs surpass them. This further highlights the benefits of our ensembling framework.
\begin{comment}
    

\begin{table}[h!]
\centering
\caption{Performance of individual LLMs in TruthfulQA. }
\begin{adjustbox}{width=\columnwidth}

\begin{tabular}{l|cccccc}
\hlineB{3}
            & Alpaca-7B & Vicuna-7B & Llama2-13B & Llama3.2-3B & Mistralv3-7B & Opt-13B \\[4pt]
\toprule
Accuracy $\uparrow$ &   $27.8$    &     $33.4$    &    $34.6$       &   $41.0$    &   $48.7$   &      $35.1$  \\[2pt]
$>Mistralv3(\%)$ $\uparrow$ & $11$     & $16.6$        & $15.8$          & $8.6$         &   NA    & $12.8$  \\[2pt]
$>Llama3.2(\%)$ $\uparrow$ & $8.6$     & $10.2$        & $17.0$          & NA        &   $11.5$    & $12.2$  \\[2pt]
\bottomrule
\end{tabular}
\end{adjustbox}
\label{table:indi}
\end{table}

\vspace{-2mm}
\end{comment}

\subsection{RQ4: Ablation study with hyperparameter $K$}
Figure \ref{fig:hypk} shows the impact of varying $K$, the number of LLMs chosen by SELECTOR, on UAF accuracy across  datasets. For TruthfulQA, the highest accuracy ($60.8\%$) is achieved with top 5 LLMs (Mistralv3-7B, Vicuna, Llama3.2-3B, Opt-13B, and Llama2-13B). 
For TriviaQA and FACTOR, the top 4 LLMs (Llama2-13B, Mistralv3-7B, Alpaca, Llama3.2-3B) and top 3 LLMs (Mistralv3-7B, Llama3.2-3B, Llama2-13B) yield optimal results, with performance sharply declining for larget $K$. These results highlight the importance of the SELECTOR module in the UAF method, demonstrating that carefully choosing a small subset of high-performing LLMs enhances ensemble performance more effectively than using the entire LLM pool.


\begin{figure}[h!]
\centering
\includegraphics[width=.3\textwidth]{images/hypk.png}
\caption{Impact of $K$ on UAF performance.}
\label{fig:hypk}
\end{figure}

\vspace{-2mm}

\section{Conclusion}
In this work, we introduced an ensemble framework UAF that reduces hallucinations in factoid question answering (QA) by leveraging both the accuracy and self-assessment capabilities of multiple LLMs. By incorporating uncertainty estimation, UAF strategically combines model responses, improving factual accuracy on multiple benchmark datasets. Since UAF combines the responses of multiple LLMs, the inference time scales linearly with the number of models in the ensemble, requiring multiple forward passes through each model to generate responses and uncertainty estimates. Additionally, the uncertainty estimation itself may introduce some overhead, depending on the specific method used to compute it. Despite the added computational complexity of ensemble models, the significant improvement in factual accuracy justifies the approach. Future work could explore dynamic LLM selection based on each data point and integrate reinforcement learning for better adaptation to diverse data, enhancing both accuracy and efficiency.

%In this work, the computational complexity primarily arises from the ensemble-based nature of (UAF) framework. Since UAF combines the responses of multiple LLMs, the inference time scales linearly with the number of models in the ensemble, requiring multiple forward passes through each model to generate responses and uncertainty estimates. Additionally, the uncertainty estimation itself may introduce some overhead, depending on the specific method used to compute it. Despite these increased computational demands, UAF's design strives to balance efficiency with performance gains, showing significant improvements in factual accuracy without overwhelming the system's resources. However, in practical deployments, especially with larger ensembles, it is important to account for the trade-off between the additional computational cost and the accuracy gains achieved through more reliable and moderated predictions.
%Future work could explore dynamic LLM selection based on each data point and integrate reinforcement learning for better adaptation to diverse data, enhancing both accuracy and efficiency.


