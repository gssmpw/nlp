\section{Proposed Method}
\textbf{Problem Statement: } Let $\mathcal{X}$ and $\mathcal{Y}$ denote the input and output spaces, respectively, and $D = \{(x_i, y_i)\}_{i=1}^n$ the dataset, where $x_i \in \mathcal{X}$ and $y_i \in \mathcal{Y}$ are the $i^{th}$ question-answer pair. For each $x_i$, the goal is to generate a response $\hat{y}_i$ that maximizes the overall accuracy. The goal is to achieve this using a pool  of $N$ pretrained foundational LLMs without additional training or fine-tuning.


\begin{comment}
    

Let $\mathcal{X}$ and $\mathcal{Y}$ denote input and output spaces respectively and  $\mathcal{D}= \{(x_i,y_i)\}_{i=1}^n$ denote Dataset, where $x_i \in \mathcal{X}$ and $y_i \in \mathcal{Y}$ is the $i^{th}$ question-answer(QA) pair. For each $x_i$ we want to generate a response $\hat{y}_i$  such that overall accuracy denoted by $\frac{1}{n}\sum_{i=1}^n\mathbf{I}(y_i=\hat{y}_i)$ is maximized.  We want to maximize this using ensemble of $N$ LLMs without any additional training or fine-tuning.
\end{comment}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\begin{algorithm}[t]
    \caption{Components of UAF - SELECTOR and FUSER}
    \label{alg:selector}
    
    \begin{algorithmic}
    \REQUIRE $D_{val}$, Pool of LLMs $\mathcal{M}= \{M^j\}_{j=1}^{N}$, Uncertainty function $U_f(.,.,.)$, Ensemble size $K$, Test data point $x_{test}$
    \ENSURE Test data response $\hat{y}_{test}$
    \newline

    \STATE \textbf{procedure }SELECTOR ($D_{val}, \mathcal{M}, U_f, K$)
    \FOR{$M^j \in \mathcal{M}$}
    %\STATE $L_j, U_j = \emptyset,\emptyset$
    \FOR{$(x_i, y_i) \in D_{val}$}
    \STATE $\hat{y}_i^j = M^j(x_i)$   \quad;\quad  $s_i^j = \mathbf{1}(\hat{y}_i^j == y_i)$
    \STATE $u_i^j = U_f(M^j, x_i, \hat{y}_i^j)$
    %\STATE $L_j \leftarrow L_j \cup IsCorrect_i^j$   \quad;\quad    $U_j \leftarrow U_j \cup u_{val_i}^j$
    \ENDFOR

    \STATE $Acc_j = \frac{1}{|D_{val}|}\sum_i s_i^j$  ;\quad $SAH_j = ROC\_AUC\_score(\{s_i^j,u_i^j\}_i)$
    \STATE $Cscore_j = Acc_j \times SAH_j$
    \ENDFOR
    \STATE \textbf{return:} $\mathcal{M}^{sel} =  TopK (\{Cscore_j\}_{j=1}^N)$,  \hfill\COMMENT{//Top $K$ LLMs}
    \STATE \quad  \quad      $Acc^{sel} = \{Acc_j| j \in \mathcal{M}^{sel}\}$ \hfill\COMMENT{//Accuracy of selected $K$ LLMs}
    %\STATE $j_1, j_2, \dots, j_K = \arg\max_{j \in \{1, 2, \dots, N\}} Cscore_j$ 
    %\ENSURE $j_1, j_2, \dots, j_K$    \hfill\COMMENT{//Indices of Top K llms}
    \newline
\STATE \textbf{procedure } FUSER ($x_{test}, \mathcal{M}^{sel}, Acc^{sel}, U_f, K$)
\FOR{$M^k \in \mathcal{M}^{sel}$}
\STATE $\hat{y}_{test}^k = M^k(x_{test})$
\STATE $u_{test}^k = U_f(M^k, x_{test}, \hat{y}_{test}^k)$
\ENDFOR
\STATE $\hat{y}_{test} = \hat{y}_{test}^{k^*} \text{, where } k^* = argmax_{k \in \{1,\dots K\}} Acc_k \times (1-u_{test}^k)$
\STATE \textbf{return:} $\hat{y}_{test}$

\end{algorithmic}
\end{algorithm}

\subsection{Uncertainty Aware Fusion (UAF)}\label{sec:uaf}
Figure \ref{fig:fuser} provides an overview of our UAF framework. At a high level, UAF consists of two modules: SELECTOR and FUSER. Given a specific task, the SELECTOR selects the top $K$ LLMs from a pool of $N$ LLMs based on performance metric. FUSER then combines the outputs of these $K$ LLMs to produce the final response.

\subsubsection{SELECTOR}
Given a pool of $N$ LLMs denoted by $\mathcal{M}$, SELECTOR selects $K$ LLMs (where $K<N$) to optimize computational efficiency and enhance overall factual accuracy by pruning underperforming LLMs. Selection is based on two criteria: (1) task-specific accuracy and (2) self-assessment of hallucinations based on an given uncertainty measure. Given a specified uncertainty measure $U_f(\cdot)$, and  a validation set $D_{val}$, we prompt each LLM with input $x_i$,   obtaining response  $\hat{y}_i^j$  and corresponding uncertainty score $u_i^j$ from the $j^{th}$ LLM $M^j$.  We compute the accuracy $Acc_j$ of $M^j$ as the fraction of correct responses. We also measure the LLMs ability for  self-assessment of hallucinations $SAH_j$ as the area under the ROC curve for the binary classification of truthful vs. hallucinatory responses using uncertainty scores. We then compute a combined score $Cscore_j = Acc_j \times SAH_j$ for each LLM. The top K models with the highest combined scores are selected greedily, where K is a   hyperparameter tuned for specific tasks.

\begin{comment}
To achieve this we sample $10\%$ of examples from $D$ and denote it as  $D_{val}$. We use the rest denoted by $D_{test}$ for evaluation. For each $(xval_i,yval_i)\in D_{val}$ we prompt each of the $N$ LLMs with $xval_i$. Let $\hat{yval}_i^j$ denote the response and $u_i^j$ its corresponding uncertainty score computed with a particular uncertainty method for $j^{th}$ LLM. Accuracy of $j^{th}$ LLM, denoted by $Acc_j$ is defined as percentage of correct responses. We also measure area under the receiver operator characteristic curve  $Unc\_auroc_j$ of $j^{th}$ LLM, by measuring the performance of classifying its own correct(truthful) from incorrect(hallucinatory) responses by varying the thresholds on the set of uncertainty scores $\{u_i^j\}_{i=1}^{nval}$. For each $j^{th}$ LLM we compute a combined score $Cscore_j = Acc_j*Unc\_auroc_j$. We use greedy method to select $K$ LLMs based on top $K$ highest $Cscore$. Here $K$ is the hyperparameter which we tune using $D_{val}$. Algorithm \ref{alg:selector} presents the pseudo-code for this method.
\end{comment}

\subsubsection{FUSER}
Given the selected ensemble of $K$ models $\mathcal{M}^{sel}$, 
%with respective accuracies $\{Acc_1, \dots, Acc_K \}$,
for each unseen example $x_{test}$, we generate outputs from the $K$ LLMs denoted by $\{\hat{y}_{test}^1,\dots,\hat{y}_{test}^K\}$ along with the  corresponding instance-specific uncertainty scores.  denoted by $\{u_{test}^1,\dots,u_{test}^K\}$. 
While there can be several fusion strategies, since we are dealing with natural language responses, the simplest one is to example-specific selection from the candidate outputs, i.e., 
\[
\hat{y}_{test} = \hat{y}_{test}^{k^*}, \quad \text{where} \quad {k^*} = \arg\max_{k \in \{1, \dots, K\}} f^k.
\]
Selection criterion $f^k$ could be based on validation set accuracy alone, inverse uncertainty or some combination of both such as $\text{Acc}_k \cdot (1 - u_{test}^k)$  or $\frac{\text{Acc}_k}{u_{test}^k}$.
The first strategy essentially reduces the ensemble to a single most accurate model, while the second one elevates the most confident one. However, both of these approaches are sub-optimal compared to combined criteria, specifically 
\[
f^k = \text{Acc}_k \cdot (1 - u_{test}^k),
\]
which yields the best performance. Experiments with  other combined selection criteria shows similar behavior to the aforementioned one and hence,  we omit the results for brevity.


%There are multiple ways to choose the final answer from these candidate responses - $\hat{y}_{test} = \hat{y}_{test}^j where j = argmax_{k \in \{1,\dots K\}}f^k$. We can define $f^k$ as $Acc_k$ , $1-u_{test}^k$ ,$Acc_k*(1-u_{test}^k)$ or $Acc_k/u_{test}^k$. First strategy essentially reduces the ensemble to a single model having highest accuracy, while second one picks the final answer as the one with the least uncertainty. Both of these are suboptimal compared to  $f_k = Acc_k*(1-u_{test}^k)$ with which we experiment here in this work. Alternative strategy $f_k = Acc_k/u_{test}^k$ shows similar behaviour as above and we omit it's analysis for brevity.

%Although there are multiple ways to aggregate these candidate responses we propose a simple aggregation technique where we choose the final answer as the one with the least uncertainty ie. $\hat{y}_{test} = \hat{y}_{test}^j where j = argmin_{k \in \{1,\dots K\}}u_{test}^k$. Algorithm \ref{alg:selector} presents the pseudo-code of UAF components.

