\begin{figure}
\centering
\includegraphics[width=.3\textwidth]{images/pie.png}
\caption{Fraction of examples where each LLM generates the most confident correct answer. Optimal LLM ( high accuracy and confidence) varies across examples.}
\label{fig:pie}
\end{figure}


\section{Introduction}
\label{sec:intro}
Large language models (LLMs) have yielded remarkable performance boosts across diverse natural language processing (NLP) tasks \cite{gpt4}. However,  their tendency to 'hallucinate', i.e., generating outputs not grounded in factual training data\cite{hal_survey}, remains a critical limitation, 
impeding deployment, particularly in high-stakes applications where factual accuracy is paramount.\\
\textbf{Related work:} 
Existing work on hallucination mitigation can be broadly categorized into three groups.
The first group of techniques tries to enhance truthfulness at inference time using contrastive decoding or representation editing. Representation editing methods like ITI \cite{iti} and TruthX \cite{truthx} identify a truthful direction using human-labeled data and adjust model activations accordingly. Contrastive decoding approaches such as SH2 \cite{sh2} and DoLa \cite{dola} modify output probabilities by comparing distributions between different models or layers. However, these methods often face challenges with implementation complexity, reliance on extensive labeled data, and limited generalization across domains. \\
The second group employs ensemble learning techniques to improve model performance. Methods such as LLM-Blender \cite{llmblender} learn to combine diverse candidate responses from multiple LLMs, while MLM \cite{mlm} selects the response most similar to the input. Another approach exemplified by Consistent \cite{wang2022self}, generates multiple candidate responses and uses majority voting for the final output. However, these approaches often overlook the inherent uncertainties in candidate responses, potentially limiting their effectiveness.\\
The third group of methods is based on the observation that LLMs often have an internal sense of truthfulness, even when producing false statements  \cite{kadavath2022language,eigen}. This insight is exploited in various approaches such as logit probability-based methods \cite{selgen,gales_unc}, consistency-based techniques \cite{semantic_unc,selfckgpt}, probing internal representations \cite{eigen,haloscope}, and prompting-based strategies \cite{kadavath2022language} to obtain uncertainty scores without additional training or model editing, offering promising avenues for hallucination detection and mitigation.\\
\textbf{Contributions: } In this work, we focus on mitigating LLM hallucinations for factoid question answering (QA). First, we observe that the proliferation of open-source LLMs, each with distinct strengths and weaknesses, precludes any single model from consistently outperforming others in factual accuracy and self-assessment. Figure \ref{fig:pie} illustrates this phenomenon for the TruthfulQA \cite{truthfulqa} dataset, showing the distribution of examples where each LLM produces the most confident correct response. Despite Mistralv3-7B achieving the highest overall accuracy, it leads in confidence for only 18\% of examples, indicating that the rest are better handled by other models. This diversity in model performance across examples presents an opportunity to leverage the complementary strengths of multiple LLMs to reduce hallucinations and enhance overall accuracy. Our key contributions are summarized below:\\
%\begin{enumerate}
\noindent 1. We demonstrate, through extensive experiments with six LLMs across multiple benchmarks, that the relative factual accuracy and self-assessment capabilities of LLMs  vary significantly across examples, with no single dominant model. 

\noindent 2. We propose Uncertainty-Aware Fusion (UAF), an ensemble framework that strategically combines multiple LLMs to achieve superior performance. UAF consists of two key modules: (i) SELECTOR, which chooses the top $K$ LLMs from the entire pool based on accuracy and hallucination detection abilities, and (ii) FUSER, which merges the  outputs of these $K$ LLMs to generate the final result.

\noindent 3. Experiments on benchmarks such as  TruthfulQA \cite{truthfulqa}, TriviaQA \cite{triviaqa}, and FACTOR \cite{factor} datasets reveal that our ensembling technique significantly enhances factual accuracy with  UAF outperforming SOTA baselines by 8\% in accuracy, while narrowing the performance gap with, or even surpassing, GPT-4 \cite{gpt4}. We also present ablation studies examining the impact of uncertainty measure, model selection criteria, and ensemble size on overall performance.

%\end{enumerate}

\begin{comment}


\section{Introduction}
\label{sec:intro}
Large language models (LLMs) have demonstrated remarkable capabilities across diverse natural language processing (NLP) tasks \cite{gpt4}. However, despite the continued increase in performance, their tendency to 'hallucinate'—i.e., generate content that deviates from real-world facts observed during pretraining \cite{hal_survey}—remains a persistent challenge. This represents a major bottleneck in their deployment, especially for high-stakes applications where the reliable generation of truthful text is crucial.
\\
\textbf{Related work:} Recent state-of-the-arts (SOTA) efforts aim to enhance LLMs' truthfulness during inference through contrastive decoding or representation editing. Representation editing involves identifying a truthful direction using human-labeled data and adjusting model activations to align with truthfulness, as seen in methods like ITI \cite{iti} and TruthX \cite{truthx}. Contrastive decoding methods modify output probabilities by comparing probability distributions between different models or layers of a single model, such as SH2 \cite{sh2} and DoLa \cite{dola}, to improve truthfulness. However, these methods often suffer from high implementation complexity, dependency on extensive labeled data, and may not generalize well across diverse domains.
%Traditional state-of-the-art (SOTA) hallucination mitigation methods, such as ITI \cite{iti}, TruthX \cite{truthx}, DoLa \cite{dola}, and SH2 \cite{sh2}, involve complex model editing and supervised training procedures, which limit their utility as they often suffer from high computational costs, dependence on task-specific data, and challenges in generalization and implementation complexity.
\\
Ensemble learning is another common technique to enhance model performance by combining multiple weaker models \cite{aniol2019ensemble}. In generative QA tasks with LLMs, approaches like LLM-Blender \cite{llmblender} learns a model to combine diverse candidate responses from multiple LLMs to generate the final output, while MLM \cite{mlm} selects the response most similar to the input as the final solution. Another approach, exemplified by Consistent \cite{wang2022self}, generates multiple candidate responses and uses majority voting for the final output. However, these methods overlook the inherent uncertainties in candidate responses, making them suboptimal.
\\
Recent literature such as \cite{kadavath2022language,eigen} observed that LLMs usually have an internal representation of the likelihood of something being true, even as they produce falsehoods on the surface. 
Works like \cite{haloscope,selgen,semantic_unc,gales_unc} leverage this to obtain uncertainty scores without additional training or model editing, helping detect hallucinations in LLM outputs. Uncertainty-based methods for detecting hallucinations in LLM responses can be broadly categorized as: (1) Logit probability-based methods, such as \cite{selgen} and \cite{gales_unc}; (2) Consistency-based methods, like \cite{semantic_unc} and  \cite{selfckgpt}; (3) Probing internal representations, such as \cite{eigen} and \cite{haloscope}; and (4) Prompting-based strategies \cite{kadavath2022language}.
\\
\\
\textbf{Contribution: } In this work, we focus on factoid question answering (QA). We observe that, with the growing number of open-source LLMs, each with its own strengths and weaknesses, no single model consistently outperforms others in terms of factual accuracy or hallucination detection. Figure \ref{fig:pie} shows the distribution of examples in which each LLM generates the correct response with the least uncertainty (or maximum confidence) on the TruthfulQA \cite{truthfulqa} dataset. While Mistralv3-7B achieves the highest overall percentage, it ranks first in confidence for only $18\%$ of the examples, highlighting that a significant number of cases are better handled by other models, which are more confident and accurate. This also underscores that the optimal LLM for each example can vary significantly, with models complementing each other by accurately predicting where others are uncertain. This creates an opportunity to leverage the diverse strengths of multiple LLMs to reduce hallucinations and improve overall accuracy. Below we summarize our key contributions:\\
\begin{enumerate}
\item We show that LLMs exhibit diverse hallucination detection and factual response generation abilities across different examples, with no single dominant model.

\item We introduce Uncertainty-Aware Fusion (UAF), an ensembling framework designed to achieve consistently superior performance by combining the outputs of multiple LLMs. UAF consists of two modules: SELECTOR and UNC-FUSER. Given a particular task, SELECTOR selects the top $K$ LLMs from a pool of $N$ LLMs based on their accuracy and hallucination detection abilities. UNC-FUSER then fuses the outputs of these $K$ LLMs to generate the final output.

\item  Our ensembling technique enhances factual accuracy on TruthfulQA \cite{truthfulqa}, TriviaQA \cite{triviaqa}, and FACTOR \cite{factor} datasets. UAF outperforms SOTA baselines by $8\%$ in accuracy and narrows the performance gap with, or exceeds, GPT-4 \cite{gpt4}.
    
\end{enumerate}
\end{comment}

%\cite{selgen2,selgen} use these methods to decide when to abstain or selectively generate responses to improve accuracy. However, with the growing number of open-source LLMs with varied strengths and weaknesses, no single model consistently outperforms others in this task.
%Figure \ref{fig:pie} shows the distribution of examples where each LLM generates the correct response with the least uncertainty (or maximum confidence) on the TruthfulQA dataset. While Mistralv3-7B achieves the highest percentage, it ranks first in confidence for only $18\%$ of the examples highlighti. The chart also highlights that the optimal LLMs for different examples can vary significantly, with models complementing each other by accurately predicting where others are uncertain. This underscores the importance of dynamically ensembling models to leverage their complementary strengths, improving truthfulness, generalization, and accuracy.

%In this work, we introduce Uncertainty-Aware Fusing (UAF), an ensembling framework designed to achieve consistently superior performance by combining the outputs of multiple LLMs. UAF consists of two modules: SELECTOR and UNC-FUSER. Given a particular task, SELECTOR selects the top $K$ LLMs from a pool of $N$ LLMs based on the strength of each LLM. UNC-FUSER then fuses the outputs of these $K$ LLMs to generate the final output.


\begin{comment}
    

\section{Related Work} \label{related-works}
\textbf{Hallucination mitigation: } Recent efforts aim to enhance LLMs' truthfulness during inference through contrastive decoding or representation editing. Representation editing involves identifying a truthful direction using human-labeled data and adjusting model activations to align with truthfulness, as seen in methods like ITI \cite{iti} and TruthX \cite{truthx}. Contrastive decoding methods modify output probabilities by comparing probability distributions between different models or layers of a single model, such as SH2 \cite{sh2} and DoLa \cite{dola}, to improve truthfulness. However, these methods often suffer from high computational costs and reliance on labeled data, making them less practical for resource-constrained applications.
\\
\textbf{LLM Ensemble: } We can broadly categorize this field into supervised and unsupervised learning. In unsupervised methods, answer consistency, as exemplified by \cite{wang2022self}, generates multiple solutions from different LLMs and uses majority voting to produce the final output. \cite{mlm} selects the solution with the highest similarity to the input as the ensemble output. Among supervised ensemble methods, LLM-Blender \cite{llmblender} is the most popular, requiring pairwise comparisons of models in the pool and a costly training procedure.
\end{comment}

\begin{figure}
\centering
\includegraphics[width=.5\textwidth]{images/fuser.png}
\caption{Overview of our UAF architecture}
\label{fig:fuser}
\end{figure}

\vspace{-1pt}