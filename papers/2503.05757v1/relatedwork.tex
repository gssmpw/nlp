\section{Related Work}
\label{related-works}
\textbf{Hallucination mitigation: } Recent efforts aim to enhance LLMs' truthfulness during inference through contrastive decoding or representation editing. Representation editing involves identifying a truthful direction using human-labeled data and adjusting model activations to align with truthfulness, as seen in methods like ITI \cite{iti} and TruthX \cite{truthx}. Contrastive decoding methods modify output probabilities by comparing probability distributions between different models or layers of a single model, such as SH2 \cite{sh2} and DoLa \cite{dola}, to improve truthfulness. However, these methods often suffer from high computational costs and reliance on labeled data, making them less practical for resource-constrained applications.
\\
\textbf{LLM Ensemble: } We can broadly categorize this field into supervised and unsupervised learning. In unsupervised methods, answer consistency, as exemplified by \cite{wang2022self}, generates multiple solutions from different LLMs and uses majority voting to produce the final output. \cite{mlm} selects the solution with the highest similarity to the input as the ensemble output. Among supervised ensemble methods, LLM-Blender \cite{llmblender} is the most popular, requiring pairwise comparisons of models in the pool and a costly training procedure.
\end{comment}

\begin{figure}
\centering
\includegraphics[width=.5\textwidth]{images/fuser.png}
\caption{Overview of our UAF architecture}
\label{fig:fuser}
\end{figure}

\vspace{-1pt}