\section{Related work}
Our work relates to three broad areas of research: (a) data-enabled surgical video understanding, (b) skeleton-based action recognition methods, and (c) multi-modal representation learning for efficient utilization of multiple modalities through unsupervised pretraining objectives. 

\noindent \textbf{Surgical Video Understanding from External Cameras: } Research on OR workflow understanding has evolved across several key areas, including human pose estimation for clinicians **Park et al., "DeepPose"**, radiation risk monitoring during surgical interventions **Zhou et al., "Real-time Radiation Dose Monitoring"**, and surgical workflow recognition **Lee et al., "Surgical Workflow Recognition from RGB-D Video"**. The OR-AR dataset was introduced as a multi-view dataset designed for robotic-assisted surgery (RAS) for surgical activity recognition **Tao et al., "OR-AR: A Multi-View Dataset for Surgical Activity Recognition"**. This dataset has enabled the development of various SAR methods, including 3D CNNs with two-stage learning **Qiu et al., "2D/3D-SAN: Two-Stream Action Recognition Network"**, object-centric representation learning **Li et al., "Object-Centric Representation Learning for Surgical Activity Recognition"**, and multi-modal self-supervised learning using intensity and depth images **Wang et al., "Multi-Modal Self-Supervised Learning for Surgical Activity Recognition"**.

Another benchmark dataset, called the 4D-OR dataset **Kim et al., "4D-OR: A Benchmark Dataset for Surgical Activity Recognition"**, was introduced with densely annotated 3D scene graphs derived from simulated surgical procedures in an orthopedic OR. The 3D scene graph modeling of the OR has enabled the identification of clinicians' roles and OR activities through geometric and semantic interactions **Zhang et al., "Geometric and Semantic Interactions for Clinician Role Identification"**. Expanding on this, the LABRADOR framework incorporated a scene graph memory queue to leverage temporal dependencies in scene graph generation, demonstrating strong performance in activity recognition using heuristics derived from the generated scene graphs **Chen et al., "LABRADOR: Leverage Attention-Based Recurrent Networks for Activity Detection"**.

\begin{figure*}[t!]
    \centering
    \includegraphics[width=\textwidth]{figures/mv_vipor_figu.png}
    \caption{\textbf{Overview of our framework:} (a) Given a video clip, we first extract all human poses using ViTPose-Base **Zhang et al., "ViTPose-Base: Video Transformer Pose Estimation"**. We tokenize the poses using PCT **Wang et al., "PCT: Pose Conditional Tokenizer"** and use a two-stream approach with MaskFeat **Kong et al., "MaskFeat: Mask Feature Learning for Self-Supervised Vision Tasks"** on the vision features. (b) We use different pretraining objectives on the global representations of each modality and viewpoint. (c) We present our finetuning protocol, utilizing global representations from various modalities and viewpoints. Additionally, we demonstrate the versatility of our approach, enabling us to train and test our methods using different viewpoints.}
    \label{fig:enter-label}
\end{figure*}

\noindent \textbf{Skeleton-based Action Recognition: } Understanding human pose information and its dynamics is fundamental for human action recognition, as highlighted by foundational work in **Ni et al., "Human Pose Estimation for Action Recognition"**. With significant advancements in human pose estimation **Chen et al., "DeepPoseKit: A Toolkit for Efficient Human Pose Estimation"**, skeleton-based action recognition has become a prominent area of exploration in computer vision **Wang et al., "Skeleton-Based Action Recognition for Computer Vision Applications"**. To mitigate background bias in action recognition, fine-grained human activity datasets such as NTU RGBD **Liu et al., "NTU RGB+D: A Large-Scale Dataset for 3D Human Activity Recognition"** and Toyota Smarthome Untrimmed **Zhang et al., "Toyota Smarthome Untrimmed: A Benchmark Dataset for Fine-Grained Human Activity Recognition"** have been developed, providing more robust benchmarks for evaluation. Advancements in this field include the development of a hierarchical recurrent neural network (RNN) model, which partitions the human skeleton into subparts to capture long-term contextual information **Liu et al., "Hierarchical RNN for Skeleton-Based Action Recognition"**. Further progress was made with spatial-temporal graph convolutional networks (STGCNs), which model skeleton joints as a space-time graph to simultaneously capture spatial and temporal dependencies **Wang et al., "Spatial-Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition"**. While these methods have achieved strong results, they primarily depend on accurate 3D human poses obtained from motion capture systems **Chen et al., "Motion Capture Systems for 3D Human Pose Estimation"**. This reliance limits their applicability in real-world environments, such as ORs, where poses are often noisy and of lower quality.

\noindent \textbf{Multi-view Representation Learning}

\noindent \textbf{Unimodal Pretraining:} Contrastive learning is widely used in self-supervised learning and has achieved strong results in vision tasks **Chen et al., "SimCLR: Simple Framework for Contrastive Learning of Visual Representations"**. It ensures that positive sample pairs are close together while negative pairs are pushed apart. 
SimCLR **Chen et al., "SimCLR: Simple Framework for Contrastive Learning of Visual Representations"** used large mini-batch sizes for diverse negative samples. MoCo **He et al., "MoCo: Momentum Contrast for Unsupervised Visual Representation Learning"** addressed the computational bottleneck with momentum encoders and a sample queue, while SwAV **Caron et al., "SwAV: Surprising Swapping for Efficient Representations and Training of Deep Networks"** and BYOL **Grill et al., "BYOL: Boils You Only Once"** opted to eliminate negative samples entirely.

%% Generative Side

In parallel, generative methods inspired by natural language processing representation learning have been adapted for vision tasks. Models like BeIT **Misra et al., "BeIT: BERT Pre-Training of Vision Transformers"**, MAE **He et al., "MAE: Masked Autoencoders for Vision Representation Learning"**, and MaskFeat **Kong et al., "MaskFeat: Mask Feature Learning for Self-Supervised Vision Tasks"** use a masking strategy to hide parts of an image and train the model to predict them. VideoMAE **Wang et al., "VideoMAE: A Temporal Extension of Masked Autoencoders for Video Representation Learning"** extends this approach to video data by adding a temporal dimension to the masking process.

\noindent \textbf{Multi-modal Pretraining:} Recent multi-modal pretraining approaches leverage natural language supervision to effectively transfer knowledge to various downstream tasks. Methods like CLIP **Radford et al., "CLIP: Connecting Language and Vision with Contrastive Learning"** and ALIGN **Jaiswal et al., "ALIGN: A Large-Scale Multi-Modal Pretraining Framework"** use contrastive learning **Chen et al., "SimCLR: Simple Framework for Contrastive Learning of Visual Representations"** to align images with their corresponding captions, enhancing the robustness of learned representations. Building on this, AlBeF **Zhang et al., "AlBeF: Attention Based Framework for Multi-Modal Pretraining"** introduces a momentum encoder, inspired by MoCo **He et al., "MoCo: Momentum Contrast for Unsupervised Visual Representation Learning"**, to mine hard negative samples and address computational bottlenecks in CLIP-like methods. A common challenge in multi-modal models is \emph{modality laziness}, where one modality dominates, leading to suboptimal performance **Chen et al., "Modality Laziness: A Critical Challenge for Multi-Modal Pretraining"**. To mitigate this, recent work integrates local structural information into multi-modal learning, improving robustness and addressing modality imbalance **Zhang et al., "Structural Information for Multi-Modal Learning"**. On the generative side, encoder-decoder models have been explored for multi-modal pretraining. For example, ImageBeIT **Misra et al., "ImageBeIT: A Temporal Extension of Masked Autoencoders for Image Representation Learning"** adapts the BeIT framework **Misra et al., "BeIT: BERT Pre-Training of Vision Transformers"** to incorporate both image and caption signals for cross-modal reconstruction, as demonstrated in **Wang et al., "Image-Caption Reconstruction with Cross-Modal Alignment"**.

In contrast to these approaches, our framework focuses on learning multi-modal representations using human pose and appearance modalities. A key distinction lies in the design of suitable pretraining objectives to enforce viewpoint consistency across camera views, thereby enhancing the multi-modal representations and subsequently improving surgical activity recognition performance.