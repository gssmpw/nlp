\section{Related work}
Our work relates to three broad areas of research: (a) data-enabled surgical video understanding, (b) skeleton-based action recognition methods, and (c) multi-modal representation learning for efficient utilization of multiple modalities through unsupervised pretraining objectives. 

\noindent \textbf{Surgical Video Understanding from External Cameras: } Research on OR workflow understanding has evolved across several key areas, including human pose estimation for clinicians~\citep{MVORVinkle,srivastav2020self}, radiation risk monitoring during surgical interventions~\citep{rodas2016see,Ladikos2010EstimatingRE}, and surgical workflow recognition~\citep{zhang2021real,Twinanda2017MultiStreamDA,MoreThan}. The OR-AR dataset was introduced as a multi-view dataset designed for robotic-assisted surgery (RAS) for surgical activity recognition~\citep{Sharghi2020}. This dataset has enabled the development of various SAR methods, including 3D CNNs with two-stage learning~\citep{Sharghi2020}, object-centric representation learning~\citep{hamoud24a}, and multi-modal self-supervised learning using intensity and depth images~\citep{Jamal-ISI}.

Another benchmark dataset, called the 4D-OR dataset~\citep{4DOR}, was introduced with densely annotated 3D scene graphs derived from simulated surgical procedures in an orthopedic OR. The 3D scene graph modeling of the OR has enabled the identification of clinicians' roles and OR activities through geometric and semantic interactions~\citep{4DOR}. Expanding on this, the LABRADOR framework incorporated a scene graph memory queue to leverage temporal dependencies in scene graph generation, demonstrating strong performance in activity recognition using heuristics derived from the generated scene graphs~\citep{Labrador,Ege_IJCARS}.

\begin{figure*}[t!]
    \centering
    \includegraphics[width=\textwidth]{figures/mv_vipor_figu.png}
    \caption{\textbf{Overview of our framework:} (a) Given a video clip, we first extract all human poses using ViTPose-Base~\citep{vitpose}. We tokenize the poses using PCT~\citep{PCT} and use a two-stream approach with MaskFeat~\citep{MaskFeat} on the vision features. (b) We use different pretraining objectives on the global representations of each modality and viewpoint. (c) We present our finetuning protocol, utilizing global representations from various modalities and viewpoints. Additionally, we demonstrate the versatility of our approach, enabling us to train and test our methods using different viewpoints.}
    \label{fig:enter-label}
\end{figure*}

\noindent \textbf{Skeleton-based Action Recognition: } Understanding human pose information and its dynamics is fundamental for human action recognition, as highlighted by foundational work in~\citep{Johansson1973VisualPO}. With significant advancements in human pose estimation~\citep{cao2017realtime,PCT}, skeleton-based action recognition has become a prominent area of exploration in computer vision~\citep{duan2022revisiting,zhou2023learning}. To mitigate background bias in action recognition, fine-grained human activity datasets such as NTU RGBD~\citep{shahroudy2016ntu} and Toyota Smarthome Untrimmed~\citep{Dai2020ToyotaSU} have been developed, providing more robust benchmarks for evaluation. Advancements in this field include the development of a hierarchical recurrent neural network (RNN) model, which partitions the human skeleton into subparts to capture long-term contextual information~\citep{Du_RNN}. Further progress was made with spatial-temporal graph convolutional networks (STGCNs), which model skeleton joints as a space-time graph to simultaneously capture spatial and temporal dependencies~\citep{kipf2016semi,STGCN}. While these methods have achieved strong results, they primarily depend on accurate 3D human poses obtained from motion capture systems~\citep{MoCAP}. This reliance limits their applicability in real-world environments, such as ORs, where poses are often noisy and of lower quality.

\noindent \textbf{Multi-view Representation Learning}

\noindent \textbf{Unimodal Pretraining:} Contrastive learning is widely used in self-supervised learning and has achieved strong results in vision tasks~\citep{SimCLR,MoCo,Swav,BYOL}. It ensures that positive sample pairs are close together while negative pairs are pushed apart. 
SimCLR~\citep{SimCLR} used large mini-batch sizes for diverse negative samples. MoCo~\citep{MoCo} addressed the computational bottleneck with momentum encoders and a sample queue, while SwAV~\citep{Swav} and BYOL~\citep{BYOL} opted to eliminate negative samples entirely.

%% Generative Side

In parallel, generative methods inspired by natural language processing representation learning have been adapted for vision tasks. Models like BeIT~\citep{BEiT}, MAE~\citep{MAE}, and MaskFeat~\citep{MaskFeat} use a masking strategy to hide parts of an image and train the model to predict them. VideoMAE~\citep{VMAE} extends this approach to video data by adding a temporal dimension to the masking process. %MaskFeat~\citep{MaskFeat} is a self-supervised learning method that utilizes masked feature prediction, leveraging the rich semantic information in the features.
%Recent advancements have introduced skeleton-based masking methods~\citep{SkeletonMAE,MAMP}. In SkeletonMAE~\citep{SkeletonMAE}, the authors present a graph-based encoder-decoder model that reconstructs joint and edge information. In contrast, MAMP~\citep{MAMP} focuses on reconstructing the temporal motion of masked human joints, providing richer semantic insights.

\noindent \textbf{Multi-modal Pretraining:} Recent multi-modal pretraining approaches leverage natural language supervision to effectively transfer knowledge to various downstream tasks. Methods like CLIP~\citep{CLIP} and ALIGN~\citep{jia2021scaling} use contrastive learning~\citep{oord2018representation} to align images with their corresponding captions, enhancing the robustness of learned representations. Building on this, AlBeF~\citep{AlignBF} introduces a momentum encoder, inspired by MoCo~\citep{MoCo}, to mine hard negative samples and address computational bottlenecks in CLIP-like methods. A common challenge in multi-modal models is \emph{modality laziness}, where one modality dominates, leading to suboptimal performance~\citep{modLaz}. To mitigate this, recent work integrates local structural information into multi-modal learning, improving robustness and addressing modality imbalance~\citep{Yang2022VisionLanguagePW}. On the generative side, encoder-decoder models have been explored for multi-modal pretraining. For example, ImageBeIT~\citep{ImageBeIT} adapts the BeIT framework~\citep{BEiT} to incorporate both image and caption signals for cross-modal reconstruction, as demonstrated in~\citep{ImageBeIT,PeVLPV}.

In contrast to these approaches, our framework focuses on learning multi-modal representations using human pose and appearance modalities. A key distinction lies in the design of suitable pretraining objectives to enforce viewpoint consistency across camera views, thereby enhancing the multi-modal representations and subsequently improving surgical activity recognition performance.