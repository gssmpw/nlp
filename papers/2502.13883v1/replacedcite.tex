\section{Related work}
Our work relates to three broad areas of research: (a) data-enabled surgical video understanding, (b) skeleton-based action recognition methods, and (c) multi-modal representation learning for efficient utilization of multiple modalities through unsupervised pretraining objectives. 

\noindent \textbf{Surgical Video Understanding from External Cameras: } Research on OR workflow understanding has evolved across several key areas, including human pose estimation for clinicians____, radiation risk monitoring during surgical interventions____, and surgical workflow recognition____. The OR-AR dataset was introduced as a multi-view dataset designed for robotic-assisted surgery (RAS) for surgical activity recognition____. This dataset has enabled the development of various SAR methods, including 3D CNNs with two-stage learning____, object-centric representation learning____, and multi-modal self-supervised learning using intensity and depth images____.

Another benchmark dataset, called the 4D-OR dataset____, was introduced with densely annotated 3D scene graphs derived from simulated surgical procedures in an orthopedic OR. The 3D scene graph modeling of the OR has enabled the identification of clinicians' roles and OR activities through geometric and semantic interactions____. Expanding on this, the LABRADOR framework incorporated a scene graph memory queue to leverage temporal dependencies in scene graph generation, demonstrating strong performance in activity recognition using heuristics derived from the generated scene graphs____.

\begin{figure*}[t!]
    \centering
    \includegraphics[width=\textwidth]{figures/mv_vipor_figu.png}
    \caption{\textbf{Overview of our framework:} (a) Given a video clip, we first extract all human poses using ViTPose-Base____. We tokenize the poses using PCT____ and use a two-stream approach with MaskFeat____ on the vision features. (b) We use different pretraining objectives on the global representations of each modality and viewpoint. (c) We present our finetuning protocol, utilizing global representations from various modalities and viewpoints. Additionally, we demonstrate the versatility of our approach, enabling us to train and test our methods using different viewpoints.}
    \label{fig:enter-label}
\end{figure*}

\noindent \textbf{Skeleton-based Action Recognition: } Understanding human pose information and its dynamics is fundamental for human action recognition, as highlighted by foundational work in____. With significant advancements in human pose estimation____, skeleton-based action recognition has become a prominent area of exploration in computer vision____. To mitigate background bias in action recognition, fine-grained human activity datasets such as NTU RGBD____ and Toyota Smarthome Untrimmed____ have been developed, providing more robust benchmarks for evaluation. Advancements in this field include the development of a hierarchical recurrent neural network (RNN) model, which partitions the human skeleton into subparts to capture long-term contextual information____. Further progress was made with spatial-temporal graph convolutional networks (STGCNs), which model skeleton joints as a space-time graph to simultaneously capture spatial and temporal dependencies____. While these methods have achieved strong results, they primarily depend on accurate 3D human poses obtained from motion capture systems____. This reliance limits their applicability in real-world environments, such as ORs, where poses are often noisy and of lower quality.

\noindent \textbf{Multi-view Representation Learning}

\noindent \textbf{Unimodal Pretraining:} Contrastive learning is widely used in self-supervised learning and has achieved strong results in vision tasks____. It ensures that positive sample pairs are close together while negative pairs are pushed apart. 
SimCLR____ used large mini-batch sizes for diverse negative samples. MoCo____ addressed the computational bottleneck with momentum encoders and a sample queue, while SwAV____ and BYOL____ opted to eliminate negative samples entirely.

%% Generative Side

In parallel, generative methods inspired by natural language processing representation learning have been adapted for vision tasks. Models like BeIT____, MAE____, and MaskFeat____ use a masking strategy to hide parts of an image and train the model to predict them. VideoMAE____ extends this approach to video data by adding a temporal dimension to the masking process. %MaskFeat____ is a self-supervised learning method that utilizes masked feature prediction, leveraging the rich semantic information in the features.
%Recent advancements have introduced skeleton-based masking methods____. In SkeletonMAE____, the authors present a graph-based encoder-decoder model that reconstructs joint and edge information. In contrast, MAMP____ focuses on reconstructing the temporal motion of masked human joints, providing richer semantic insights.

\noindent \textbf{Multi-modal Pretraining:} Recent multi-modal pretraining approaches leverage natural language supervision to effectively transfer knowledge to various downstream tasks. Methods like CLIP____ and ALIGN____ use contrastive learning____ to align images with their corresponding captions, enhancing the robustness of learned representations. Building on this, AlBeF____ introduces a momentum encoder, inspired by MoCo____, to mine hard negative samples and address computational bottlenecks in CLIP-like methods. A common challenge in multi-modal models is \emph{modality laziness}, where one modality dominates, leading to suboptimal performance____. To mitigate this, recent work integrates local structural information into multi-modal learning, improving robustness and addressing modality imbalance____. On the generative side, encoder-decoder models have been explored for multi-modal pretraining. For example, ImageBeIT____ adapts the BeIT framework____ to incorporate both image and caption signals for cross-modal reconstruction, as demonstrated in____.

In contrast to these approaches, our framework focuses on learning multi-modal representations using human pose and appearance modalities. A key distinction lies in the design of suitable pretraining objectives to enforce viewpoint consistency across camera views, thereby enhancing the multi-modal representations and subsequently improving surgical activity recognition performance.