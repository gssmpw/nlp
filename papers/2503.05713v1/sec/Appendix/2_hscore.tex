\section{Metric Details}
\label{appendixmetric}
\noindent \textbf{Longest Common Substring}
The Longest Common Substring (LCS) metric measures the extent of verbatim reproduction in generated lyrics. This metric is particularly useful in legal contexts, as copyright law often considers a threshold of verbatim copying when determining infringement. To improve consistency throughout languages, we calculate LCS at the token level.

\noindent \textbf{ROUGE-L}
Since LCS may not fully capture shorter copyrighted materials such as lyrics \cite{liu-etal-2024-shield}, we also employ the ROUGE-L score \cite{lin2004rouge} to assess token-level similarity. Specifically, we use the F1 score of ROUGE-L to quantify the volume of copyrighted content present in the model’s output.

\noindent \textbf{Refusal Rate} 
The refusal rate measures how often a large language model declines to provide a response (e.g., “I’m sorry, but…”). Following previous work \cite{xu2024llms}, which found that LLM judgments align with human annotations in 98\% of cases, we use GPT-4o to evaluate model responses. Each response is assigned a score of 1 if the model refuses to generate lyrics and 0 otherwise.


\noindent \textbf{Hallucination Rate}
A language model may exhibit a low refusal rate while generating entirely fabricated lyrics, which pose no threat to copyright holders. Therefore, it is essential to account for hallucination in the evaluation. In this context, hallucination refers to “non-factual” content \cite{mishra2024finegrained, li-etal-2024-dawn}.
While the combination of refusal rate and ROUGE-L score may offer some insights into hallucination, it does not reveal hallucination bias across languages. Also, previous study has found that ROUGE metric is less effective in evaluating hallucination \cite{kang2024comparing}. A more direct approach is to quantify it using a dedicated metric: the percentage of generated lyrics that do not match the original lyrics. We measure this at the sentence level by leveraging GPT-4o. 






\section{Hallucination Evaluation Details}
\label{appendixh}
We leverage the inference capabilities of large language models, GPT-4o in our case, to evaluate the degree of hallucination in the generated lyrics of language models. GPT-4o is instructed to first identify whether the model actually generates lyrics regardless of true or false. If the model does generated lyrics, GPT-4o will count the total number of sentences in the generated lyrics and then determine the number of sentences that do not belong to the original song lyrics (which are also provided in the prompt), ultimately computing the ratio. In this way, refusal rate could also be calculated. To ensure the reliability of this evaluation approach, we sample 100 cases for human annotation and calculate the Pearson correlation coefficient, obtaining a score of 0.85, indicating strong alignment between GPT-4o’s evaluation and human judgments. Additionally, we manually go through the evaluation results to double-check for better accuracy. Examples of GPT-4o's analysis can be found in Tables \ref{tab:appendix-prompts1}, \ref{tab:appendix-prompts2}, \ref{tab:appendix-prompts3}.
