\section{Conclusion}
In this paper, we study the multilingual performance of large language models in protecting copyrighted content. Through extensive experiments with seven popular models on our curated dataset consisting of copyrighted lyrics in four languages, 
we find that LLMs exhibit notable multilingual biases in copyright protection, both in terms of the language of the copyrighted content and the language of the prompt. 
Our research critically underscores the further need for more robust, language-agnostic copyright protection mechanisms in LLMs to ensure fair and consistent enforcement across languages, ultimately promoting more equitable and legally compliant AI systems.