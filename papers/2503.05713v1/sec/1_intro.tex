\section{Introduction}
% background and previous work
Large Language Models (LLMs) have profoundly transformed society since the introduction of ChatGPT \cite{openai2022chatgpt} in 2022. However, as LLMs continue to evolve, they have also raised regulatory concerns \cite{bommasani2021opportunities}. The fair use of copyright-protected content has emerged as a key issue \cite{chang2023speak, shidetecting}. LLMs are often trained on vast amounts of original content, allowing them to learn intricate details such as plot structures and narrative styles. However, research has shown that these models may, at times, reproduce copyrighted material verbatim during inference, potentially violating copyright laws \cite{karamolegkou2023copyright, chang2023speak, liu-etal-2024-shield, Mueller_Görge_Bernzen_Pirk_Poretschkin_2024}. This issue has led to a growing number of legal disputes, most notably The New York Times’ lawsuit against OpenAI and Microsoft over alleged copyright infringement. As a result, assessing the extent to which LLMs replicate copyright-protected content has become a critical concern for both the AI and legal communities.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{sec/fig/pic1.pdf} 
\caption{LLM copyright compliance is affected by both the language of the copyrighted content and the language of prompt. \textcolor{red}{Red} indicates copyrighted content, while \textcolor{violet}{purple} represents hallucinated output.}
\label{fig1}
\end{figure}

Various evaluation approaches have been proposed to assess copyright compliance in large language models. \citet{karamolegkou2023copyright} introduces direct probing and prefix probing to elicit copyrighted content, while \citet{chang2023speak} employs cloze probing to evaluate a language model’s memorization of copyrighted material. \citet{liu-etal-2024-shield} applies jailbreaking techniques to extract such content. Additionally, previous studies \cite{DSouza2023TheCA, liu-etal-2024-shield, karamolegkou2023copyright, Mueller_Görge_Bernzen_Pirk_Poretschkin_2024, wei2024evaluating} have constructed test datasets using copyrighted works from various domains, including poetry, song lyrics, books, movie scripts, news articles, and LeetCode problems.
However, these studies predominantly focus on the English language, overlooking the \textbf{multilingual} dimensions of copyright protection (example shown in Fig \ref{fig1}). Since copyrighted works exist in multiple languages, created by individuals from diverse linguistic and cultural backgrounds, it is crucial to extend evaluations beyond English to ensure a fair and comprehensive assessment. Furthermore, existing approaches to eliciting copyrighted content from LLMs via prompting \cite{karamolegkou2023copyright, liu-etal-2024-shield} have exclusively relied on English prompts, neglecting the potential impact of multilingual prompts.
Moreover, while LLMs may generate responses that appear to contain copyrighted material, they are also prone to hallucination—producing fabricated content that does not correspond to any real copyrighted work. Therefore, it is essential to quantify the degree of hallucinations when evaluating copyright compliance in LLMs.

In this work, we investigate the multilingual challenges of copyright protection in large language models. Specifically, we explore the following two research questions: 1. Do LLMs exhibit bias in protecting copyrighted works across different languages? 2. Is it easier to elicit copyrighted content using prompts in specific languages?
To address these questions, we manually curate a dataset of popular song lyrics in four languages—English, French, Chinese, and Korean—and probe for multilingual copyrighted materials using prompts in these languages. We evaluate four API-based models and three open-source models. During evaluation, we leverage the assessment capabilities of LLM to detect hallucinations and employ four metrics in total, which collectively measure the volume of copyrighted material generated (LCS and ROUGE-L), the model’s tendency to decline requests for copyrighted content (Refusal Rate), and the degree of hallucination (Hallucination Rate).
Our main contributions are as follows:
\begin{itemize}
    \item To the best of our knowledge, this is the first study to examine multilingual bias in copyright protection within large language models.
    \item We construct a carefully curated test dataset consisting of popular lyrics in four languages: English, Chinese, French, and Korean. Besides previous metrics, we utilize GPT-4o to assess hallucination rates, ensuring a more comprehensive evaluation.
    \item Our experiments demonstrate that many popular LLMs exhibit a language bias in copyright protection. We also provide analysis on this imbalance.
\end{itemize}

% 实验结果概述