%File: paper.tex
%release 2025.0
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai25}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{bm}
\usepackage{tabularray}
%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2025.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.
\setlength{\tabcolsep}{3pt}

% The file aaai25.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{FactorGCL: A Hypergraph-Based Factor Model with Temporal Residual Contrastive Learning for Stock Returns Prediction}

\author{
    Yitong Duan,
    Weiran Wang,
    Jian Li
}
\affiliations{
    Tsinghua University, Beijing, China\\
    \{dyt19, wang-wr21\}@mails.tsinghua.edu.cn, lijian83@mail.tsinghua.edu.cn
}


\begin{document}

\maketitle

\begin{abstract}
    As a fundamental method in economics and finance, the factor model has been extensively utilized in quantitative investment. In recent years, there has been a paradigm shift from traditional linear models with expert-designed factors to more flexible nonlinear machine learning-based models with data-driven factors, aiming to enhance the effectiveness of these factor models. However, due to the low signal-to-noise ratio in market data, mining effective factors in data-driven models remains challenging. In this work, we propose a hypergraph-based factor model with temporal residual contrastive learning (FactorGCL) that employs a hypergraph structure to better capture high-order nonlinear relationships among stock returns and factors. To mine hidden factors that supplement human-designed prior factors for predicting stock returns, we design a cascading residual hypergraph architecture, in which the hidden factors are extracted from the residual information after removing the influence of prior factors. Additionally, we propose a temporal residual contrastive learning method to guide the extraction of effective and comprehensive hidden factors by contrasting stock-specific residual information over different time periods. Our extensive experiments on real stock market data demonstrate that FactorGCL not only outperforms existing state-of-the-art methods but also mines effective hidden factors for predicting stock returns.
\end{abstract}


\section{Introduction}
In the domain of stock investment, the factor model has long been a cornerstone for explaining and predicting stock returns, which employs specific variables, known as factors, to elucidate fluctuations in stock prices \cite{daniel2020short, fama2021multifactor}. This method, prevalent in both academia and industry, has demonstrated a strong ability to explain and predict stock returns. Consequently, establishing an effective factor model is of paramount importance in stock investment.


The factor model explains stock returns by utilizing various factors, including fundamental, technical, and macroeconomic indicators. Specifically, in a factor model, stocks are described by factors and their corresponding factor exposures, which represent the impact of factors on stocks. In traditional factor models, these factors are designed based on expert practical experience. For instance, the well-known Fama-French model \cite{eugene1992cross} employs three manually designed factors: \textit{market}, \textit{size}, and \textit{value}. However, these human-designed factors, while effective, are limited in number and may not sufficiently explain stock returns. For example, as illustrated in Figure~\ref{fig:stock_trend}, stock price trends across different industries exhibited high correlations that could not be adequately explained by industry-specific factors based on prior human experience. Moreover, most existing factor models are linear, explaining stock returns through a linear combination of factors weighted by factor exposures. However, recent studies \cite{levin1995stock, almeida2023nonlinear, bansal2004risks, he2013intermediary} have identified complex nonlinear relationships between factors and stock returns in real markets. This discrepancy highlights the limitations of linear factor models in capturing actual market behavior. Consequently, a core issue in current factor model research is how to mine more effective factors that are applicable to real market behavior.

\begin{figure}[t]
    \begin{centering}
        \includegraphics[width=0.95\columnwidth]{fig/stock_trend2.pdf}
        \caption{Stock price trends vary across different sectors and industries. During the COVID-19 pandemic, the stock price trends of the electronic consumer and medical industries exhibited high correlations, which are insufficiently explained using human-designed industry factors.}
        \label{fig:stock_trend}
    \end{centering}
\end{figure}


Recent advancements in machine learning (ML) have introduced a new perspective for factor model research \cite{kelly2019characteristics, uddin2020latent, gu2021autoencoder}. ML-based approaches can learn complex and nonlinear market patterns in a data-driven manner and construct more market-adaptive models \cite{li2024alphafin, zou2022astock, cui2023temporal}. However, the low signal-to-noise ratio in stock market data may complicate the learning process for ML-based factor models. Most current ML-based methods extract factors from market data without effectively leveraging prior human experience, which may lead to overfitting of the extracted factors to market noise rather than capturing effective patterns. This limitation presents a significant obstacle to the application of ML in factor models.

To address this obstacle, we propose a hypergraph-based factor model with temporal residual contrastive learning (FactorGCL) that supplements human-designed prior factors with data-driven hidden factors, thereby enhancing the effectiveness of factor models in predicting stock returns. Our model utilizes a hypergraph, a generalized graph structure, to better capture high-order relationships among stock returns and factors. Specifically, stocks are treated as nodes in the hypergraph, factors are represented as hyperedges, and the mining of hidden factors is framed as a hyperedge generation task. As shown in Figure~\ref{fig:overview}, FactorGCL employs a cascading residual hypergraph architecture where stock returns are decomposed into three components: prior beta, hidden beta, and individual alpha. Each component is extracted from the residuals after removing the influence of the previous component. Additionally, we propose a temporal residual contrastive learning method to guide the model's learning process by contrasting individual stock residuals across different time periods.


\begin{figure}[t]
    \begin{centering}
        \includegraphics[width=0.75\columnwidth]{fig/overview.pdf}
        \caption{Brief illustration of FactorGCL.}
        \label{fig:overview}
    \end{centering}
\end{figure}


In summary, the contributions of our work are as follows:
\begin{itemize}

    \item We propose FactorGCL, a novel factor model that utilizes a hypergraph structure to capture high-order nonlinear relationships among stock returns and factors. It employs a cascading residual hypergraph architecture to mine hidden factors, supplementing human-designed prior factors, thereby enhancing the prediction of stock returns.

    \item We design a self-supervised learning method called temporal residual contrastive learning. This method enhances the model's ability to extract effective and comprehensive hidden factors by contrasting stock-specific residuals across different time periods to better guide the mining of hidden factors.

    \item We conduct extensive experiments on real stock market data. The results demonstrate that our method not only surpasses existing state-of-the-art baselines in stock trend prediction but also mines effective hidden factors for predicting stock returns.

\end{itemize}


\section{Related Work}

\subsection{Factor Model}
Factor models are widely utilized in stock investments. The original factor model, the capital asset pricing model (CAPM) \cite{treynor1961toward, sharpe1964capital, lintner1975valuation}, attributes differences in stock returns to varying exposures to a single market factor. Later, in a seminal work \cite{eugene1992cross}, it was observed that firm value and size also contribute to explaining expected stock returns, and proposed the Fama-French three-factor model.
With the advancement of machine learning, some machine learning-based factor models have emerged. \cite{levin1995stock} proposed a nonlinear factor model based on neural networks to model possible interactions between different factors. \cite{gu2021autoencoder} proposed a latent dynamic factor model using a conditional autoencoder network to capture non-linearity in return dynamics, demonstrating that the nonlinear factor model outperforms other leading linear methods. Furthermore, \cite{duan2022factorvae} introduced a probabilistic factor model based on variational autoencoders to better extract effective factors from the market data with high noise levels.

\subsection{Hypergraph Neural Network}
Hypergraphs have proven to be an efficient approach for modeling high-order correlations among data. \cite{zhou2006learning} first introduced hypergraph learning as a propagation process on hypergraph structures. \cite{feng2019hypergraph} further advanced this concept by developing the hypergraph convolutional neural network using deep learning methods for data representation learning. Hypergraphs have also been widely applied in the field of stock return prediction\cite{li2022hypergraph, han2023stock, su2024attention}, \cite{sawhney2020spatiotemporal} initially applied the hypergraph neural network to learn stock price evolution based on stock relationships. Subsequently, \cite{sawhney2021stock} improved hypergraph neural network for stock trend prediction by incorporating ranking loss. \cite{xu2021hist} designed a concept-oriented graph framework to mine hidden concepts for stock trend forecasting. Additionally, \cite{xia2024ci} developed a dynamic hypergraph for stock selection problem using a transformer-based pretraining mechanism.

\subsection{Contrastive Learning}
This work is also related to contrastive learning , a promising class of self-supervised methods that leverage the semantic dependencies of sample pairs to capture the essence of data \cite{chen2021exploring, lin2022prototypical, li2020prototypical, caron2020unsupervised}. Contrastive learning has been widely used in various applications. For instance, \cite{oord2018representation} introduced Contrastive Predictive Coding to learn useful representations for predicting future data. \cite{liu2022towards} proposed a unsupervised deep graph structure learning method based on contrastive learning. In the financial domain, \cite{hou2021stock} introduced a contrastive learning method for multi-granularity stock data and used it as a regularization term to improve stock trend prediction tasks.


\section{Preliminaries}
% In this section, we first introduce the basic concepts of hypergraphs and hypergraph convolutional neural networks. We then outline the fundamental principles of contrastive learning.

In this section, we first introduce the basic concepts of hypergraph convolutional neural networks, and then formally describe the research problem.


\subsection{Hypergraph Convolutional Neural Network}

A hypergraph generalizes a graph by allowing an edge, termed a hyperedge, to connect two or more nodes. This structure enables the hypergraph to capture the group-wise correlations beyond pair-wise connections. Formally, a hypergraph is defined as \( \mathcal{G} = (\mathcal{V}, \mathcal{E}, W) \), where \( V \) and \( E \) denote the sets of vertices and hyperedges, respectively, and \( W \) is a diagonal matrix assigning weights to the hyperedges. The pair \((\mathcal{V}, \mathcal{E})\) in a hypergraph can be represented by an incidence matrix \( H \in \mathbb{R}^{|\mathcal{V}| \times |\mathcal{E}|} \), where \( H^{(i, j)} \) indicates the connection between the \(i\)-th vertex \( \mathcal{V}^{(i)} \) and the \(j\)-th hyperedge \( \mathcal{E}^{(j)} \), defined as:
\(
H^{(i, j)} =
\begin{cases}
    1, & \text{if } \mathcal{V}^{(i)} \in \mathcal{E}^{(j)}    \\
    0, & \text{if } \mathcal{V}^{(i)} \notin \mathcal{E}^{(j)}
\end{cases}
\)

The hypergraph convolutional neural network (HyperGCN) \cite{feng2019hypergraph} extends graph convolutional networks to hypergraphs, enabling the capture of high-order relationships inherent in hypergraph structures. The core of HyperGCN involves a message propagation rule that updates node features by aggregating information from their connected hyperedges, which are influenced by all nodes connected by those hyperedges. Formally, the node features at the \((l+1)\)-th layer of HyperGCN, \(e^{(l+1)} \in \mathbb{R}^{|\mathcal{V}| \times d_{l+1}}\), are computed using the formula:
\begin{equation}
    e^{(l+1)} = \sigma(D_{n}^{-1/2} H W D_{e}^{-1} H^T D_{n}^{-1/2} e^{(l)} w)
\end{equation}
where \( \sigma \) is a non-linear activation function, \(D_{n} \in \mathbb{R}^{|\mathcal{V}| \times |\mathcal{V}|}\) and \(D_{e} \in \mathbb{R}^{|\mathcal{E}| \times |\mathcal{E}|}\) are diagonal matrices representing node degrees and hyperedge degrees, respectively. \(W \in \mathbb{R}^{|\mathcal{E}| \times |\mathcal{E}|}\) is a diagonal matrix representing hyperedge weights, \( w \in \mathbb{R}^{d_l \times d_{l+1}}\) is a learnable weight matrix, with \(d_l\) representing the dimensions of node features at the \(l\)-th layer.


% \subsection{Contrastive Learning}

% Contrastive learning is a self-supervised learning method that differentiates between samples to create a representation space where similar samples are close together, and dissimilar samples are far apart. The underlying motivation for contrastive learning is the InfoMax principle, which aims to maximize the mutual information (MI)  between similar sample pairs. Formally, let \( f: \mathbb{X} \rightarrow \mathbb{R}^{d} \) be an encoder that transforms a sample \( x \in \mathbb{X} \) into an embedding vector \( e \in \mathbb{R}^{d} \), i.e., \( e = f(x) \). For a pair of semantically dependent samples \((x, x')\), the objective of contrastive learning is to maximize

% \begin{equation}
%     \scalebox{0.8}{$
%             \begin{aligned}
%                 MI\left(x^{\prime}, e\right) =\mathbb{E}_{(x, e)}\left[\log \frac{p\left(x^{\prime} \mid e\right)}{p\left(x^{\prime}\right)}\right] =\mathbb{E}_{\left(x, x^{\prime}\right)}\left[\log \frac{p\left(x^{\prime} \mid f(x)\right)}{p\left(x^{\prime}\right)}\right]
%             \end{aligned}
%         $}
% \end{equation}


% Since mutual information is challenging to calculate directly, the Contrastive Predictive Coding (CPC) method proposes maximizing a lower bound of MI: \( \operatorname{MI}(x', e) \geq \log (N) - \mathcal{L}_{N} \), where \( \mathcal{L}_{N} \) is the InfoNCE loss. This can be formulated as

% \begin{equation}
%     \mathcal{L}_{N}=-\mathbb{E}_{X}\left[\log \frac{D\left(x^{\prime}, f(x)\right)}{\sum_{x_{j} \in X} D\left(x_{j}, f(x)\right)}\right],
% \end{equation}
% where \( X = \{ x', x_{1}, \ldots, x_{N-1} \} \) is a set of \( N \) samples, including one positive sample \( x' \) that has semantic dependency with \( x \), and \( N-1 \) negative samples randomly selected from the dataset. The discriminator \( D: \mathbb{X} \times \mathbb{R}^{d} \rightarrow \mathbb{R} \) characterizes the similarity between sample pairs. Optimizing the InfoNCE loss encourages the encoder \( f \) to generate similar representations for semantically related pairs and dissimilar representations for unrelated pairs. This leads the encoder to effectively capture and represent the underlying structure of the data.


\subsection{Problem Formulation}
% \(S = \{s_1, s_2, \ldots, s_N\}\) \(F = \{f_1, f_2, \ldots, f_K\}\)
Given $N$ stocks in cross-section of the stock market, and a set of $K$ factors, the traditional linear factor model calculates expected stock returns as a linear combination of factors weighted by factor exposures:
\begin{equation}
    \begin{aligned}
        y_{t} = \sum_{k=1}^{K}\beta_{t}^{(k)} z_{t}^{(k)} + \alpha_{t}
    \end{aligned}
\end{equation}
where \(y_{t} = \frac{\text{price}_{t+\Delta t} - \text{price}_{t}}{\text{price}_{t}} \in \mathbb{R}^{N}\) denotes the future returns of \(N\) stocks at trading day \(t\), \(\beta_{t} \in \mathbb{R}^{N \times K}\) is the factor exposure matrix of stocks at trading day \(t\), \(\beta_{t}^{(k)} \in \mathbb{R}^{N}\) represents the \(k\)-th factor exposure of stocks at trading day \(t\), \(z_{t} \in \mathbb{R}^{K}\) is the vector of \(K\) factor returns, and \(\alpha_{t} \in \mathbb{R}^{N}\) denotes the idiosyncratic returns of the stocks.

\cite{levin1995stock} extends the linear factor model to a nonlinear version, where the stock returns are calculated using a nonlinear function $h$ of factor returns and factor exposures:
\begin{equation}
    \begin{aligned}
        {y_{t}} = h(\beta_{t}, z_{t}) + \alpha_{t}
    \end{aligned}
\end{equation}


Typically, factor returns and stock individual returns are estimated using factor exposures and historical stock returns. For example, in a linear factor model, factor returns \( z \) are estimated using the slopes from the linear regression on historical data, while individual returns \( \alpha \) are estimated using the residuals. Therefore, we set $z_t = f_{z}(\beta_t, x_t)$ and $\alpha_t = f_{\alpha}(\beta_t, x_t)$, where $x_t \in \mathbb{R}^{N\times T \times D}$ represents the historical data of stocks at trading day $t$, with \( T \) being the length of historical data and \( D \) being the feature dimension of each stock's data.

Finally, the task in this work is to learn a nonlinear factor model based on given factor exposures $\beta_{t}$ and historical stock market data $x_{t}$, for predicting future stock returns.

\begin{equation}
    \begin{aligned}
        {\hat{y}_{t}}=h(\beta_{t}, z_t) + \alpha_t =f_{\beta}(\beta_{t}, x_t) + f_{\alpha}(\beta_{t}, x_t)
    \end{aligned}
\end{equation}

In the following sections, we simplify our notation by omitting the time subscript \(t\). Unless otherwise specified, all references to \(w\) and \(b\) pertain to the weights and biases of linear layers, respectively, and will not be further elaborated.



\section{Methodology}

% This section details the design of FactorGCL. We first formally describe the research problem, and then introduce the cascading residual hypergraph architecture of our model, which extracts prior beta, hidden beta, and individual alpha components to predict stock returns. Next, we present a temporal residual contrastive learning method to guide the model in extracting effective and comprehensive hidden factors.

This section introduces the design of FactorGCL. We first design a cascading residual hypergraph architecture for our model, which extracts prior beta, hidden beta, and individual alpha components to predict stock returns. Next, we propose a temporal residual contrastive learning method to guide the model in extracting effective and comprehensive hidden factors.


\begin{figure}[t]
    \begin{centering}
        \includegraphics[width=0.99\columnwidth]{fig/residual.pdf}
        \caption{Overview of the cascading residual hypergraph architecture in FactorGCL. Stock returns are decomposed into prior beta, hidden beta, and individual alpha components. Each component is extracted from the residuals after removing the influence of the preceding component.}
        \label{fig:residual}
    \end{centering}
\end{figure}


\subsection{Cascading Residual Hypergraph Architecture}


As previously mentioned, we utilize hypergraphs to construct a nonlinear factor model. Inspired by \cite{xu2021hist}, we design a cascading residual hypergraph architecture to better extract hidden factors. As illustrated in Figure~\ref{fig:residual}, we decompose the predicted stock returns into three components: prior beta, hidden beta, and individual alpha. Specifically, we first extract stock features from the raw data and then use the prior beta module to extract the representations of prior factors. Next, we mine hidden factors from the residuals after removing the prior beta information using the hidden beta module. Finally, we extract individual alpha from the residuals after removing both prior and hidden beta information. The final prediction of our model is obtained by summing these three components. The detailed design of our architecture is as follows.


\subsubsection{Feature Extractor}

Given the raw sequential market data \(x \in \mathbb{R}^{N \times T \times D}\), the feature extractor \(\phi_{\text{feat}}\) encodes the stock temporal feature \(e_s \in \mathbb{R}^{N \times H}\) to capture rich temporal information. This process is defined as \(e_s = \phi_{\text{feat}}(x)\), where \(H\) represents the dimension of the feature embeddings. To capture long-term dependencies in sequential data, we utilize a gated recurrent unit with a batch normalization as the feature extractor, using the hidden state at the last time step as the stock feature embeddings.


\subsubsection{Prior Beta Module}
To leverage expert knowledge from given $K$ prior factors \(\beta \in \mathbb{R}^{N \times K}\), we employ a hypergraph convolutional neural network (HyperGCN) to model the nonlinear relationships among stock returns and these factors. Specifically, we represent stocks as nodes in the hypergraph and factors as hyperedges. Stocks exposed to the same factor are connected by the same hyperedge, with the incidence matrix representing factor exposures. We posit that the information propagation mechanism in the HyperGCN effectively captures the nonlinear influence of factors on stocks. This process, illustrated in Figure~\ref{fig:prior}, comprises the following steps:

\begin{itemize}
    \item \textbf{Message extraction}: Applies a transformation matrix to the each node features to extract expressive information.
    \item \textbf{Message aggregation}: Aggregates the information from stock nodes connected by the same hyperedge, representing the shared information of the corresponding factor.
    \item \textbf{Message sharing}: Integrates the node embeddings with the shared factor information as the influence of factors on stocks.
\end{itemize}

\begin{figure}[t]
    \begin{centering}
        \includegraphics[width=0.99\columnwidth]{fig/prior.pdf}
        \caption{Illustration of the information propagation process in the HyperGCN.
            The HyperGCN can model the nonlinear influence of factors on stocks by aggregating information from stock nodes connected by the same hyperedge.}
        \label{fig:prior}
    \end{centering}
\end{figure}


Formally, given the stock feature embeddings \( e_s \) output by the feature extractor, we build a hypergraph \(\mathcal{G}_{p}\) with node features \(e_s \) and incidence matrix \(\beta \). We then calculate the prior beta embeddings by applying the HyperGCN to the hypergraph \(\mathcal{G}_{p}\), expressed as:

\begin{equation}
    \begin{aligned}
        e_{p} & = \phi_{\text{prior}}(e_s, \beta)                                        \\
              & = \sigma(D_{n}^{-1/2} \beta W D_{e}^{-1} \beta^T D_{n}^{-1/2} e_s w_{p})
    \end{aligned}
\end{equation}
where \( \sigma \) is the LeakyReLU activation function, \( D_{n} \) and \( D_{e} \) are the degree matrices of the nodes and hyperedges, respectively, and \( W = I \) is the identity matrix. The resulting prior beta embeddings \( e_{p} \in \mathbb{R}^{N \times H} \) represent the influence of prior factors on stocks.


\subsubsection{Hidden Beta Module}

As previously mentioned, factors based on human prior knowledge may not adequately capture stock returns. To address this, we designed a hidden beta module to extract hidden factors that supplement these prior factors. Specifically, we regard the extraction of hidden factors as a hyperedge generation task: after removing the prior factor information, the hidden beta module generates new hyperedges from the residual embeddings and constructs a new hypergraph to model the nonlinear influence of these hidden factors on stocks.


Formally, we first calculate the residual embeddings by subtracting the prior factor embeddings from the stock feature embeddings, i.e., \( e_r = e_s - e_p \). Next, we construct \( M \) learnable vectors \( \{c^{(i)}\}_{i=1}^{M} \), where \( c^{(i)} \in \mathbb{R}^{H} \), referred to as hidden factor prototypes. Hidden factors are mined by calculating the similarity between the residual embeddings and the hidden factor prototypes:$
    \beta_{h}^{(i, j)} = \text{Sigmoid}(e_r^{(i)} \cdot {c^{(j)}}^{T})
$
where \( \beta_{h} \in \mathbb{R}^{N \times M} \) is the hidden factor exposure matrix.

Similar to the prior beta module, we construct a hypergraph \(\mathcal{G}_{h}\) with node features \(e_r\) and incidence matrix \(\beta_{h}\), and then extract the influence of hidden factors on stocks by applying the HyperGCN to the hypergraph \(\mathcal{G}_{h}\):

\begin{equation}
    \begin{aligned}
        e_{h}  = \phi_{\text{hidden}}(e_r, \beta_{h})  = \text{HyperGCN}(e_r, \beta_{h})
    \end{aligned}
\end{equation}
where \( e_{h} \in \mathbb{R}^{N \times H} \) is the hidden beta embeddings. Note that we generate "soft" hyperedges $\beta_{h}$ in the hidden beta module, with values ranging between [0, 1], enhancing the flexibility of the hidden factors.


\subsubsection{Individual Alpha Module}

In addition to the influence of prior and hidden factors, the idiosyncratic information of the stock itself, or alpha, also significantly impacts stock returns. The individual alpha module handles the residual embeddings after removing the prior and hidden factor embeddings to capture the stock-specific information. We calculate the individual alpha embeddings \( e_{\alpha} \in \mathbb{R}^{N \times H} \) by applying a linear layer with a LeakyReLU activation function:
\begin{equation}
    \begin{aligned}
        e_{\alpha} = \text{LeakyReLU}(w_{\alpha}(e_{s} - e_{p} - e_{h}) + b_{\alpha})
    \end{aligned}
\end{equation}


\subsubsection{Prediction}
We obtain the model's prediction by performing a linear mapping on the embeddings output by the prior beta module, hidden beta module, and individual alpha module, and then summing them up. Additionally, we design a multi-label prediction that requires the model to predict stock returns over multiple forward periods. This approach aims to enhance the robustness and reliability of our model by ensuring its predictive power extends across different future time frames.

\begin{equation}
    \begin{aligned}
        \hat{y}^{(l)} = w_{o1}^{(l)} e_{p} + w_{o2}^{(l)} e_{h} + w_{o3}^{(l)} e_{\alpha} + b_{\text{o}}^{(l)}
    \end{aligned}
\end{equation}
where \( \hat{y}^{(l)} \in \mathbb{R}^{N} \) represents the predicted stock returns of the \( l \)-th forward prediction period.


\subsection{Temporal Residual Contrastive Learning}

As previously mentioned, data-driven factor models face the challenge of a low signal-to-noise ratio in market data. Specifically, the hidden factors mined through such models encounter two main issues:

\begin{itemize}
    \item \textbf{Effectiveness}: The hidden factors extracted from historical data should remain consistently effective in the future. However, factors extracted by data-driven approaches are prone to overfitting market noise, thus lacking effectiveness in predicting future stock returns.
    \item \textbf{Comprehensiveness}: The hidden factors should supplement prior factors to provide a comprehensive description of stock returns. Nonetheless, market noise complicates factor mining, making the model more likely to extract simplistic factors while neglecting others, thereby failing to adequately represent stock returns.

\end{itemize}

To address these issues, we have developed a self-supervised contrastive learning method for FactorGCL, termed temporal residual contrastive learning. The motivation behind this method is as follows: for a factor model with effective and comprehensive factors, after removing all factor information, the residual, represented by the alpha embeddings \(e_{\alpha}\), should contain only idiosyncratic information unique to each stock, independent of other stocks. Based on this intuition, we draw inspiration from \cite{oord2018representation} and design a cross-temporal contrastive learning approach at the stock node level. As illustrated in the Figure~\ref{fig:contrastive_learning}, given the same prior and hidden factors, we use our model to calculate alpha embeddings based on both past and future market data. We then treat the past and future alpha embeddings of the same stock as positive pairs, and the embeddings of different stocks as negative pairs. By training the model with a cascading residual hypergraph architecture to extract temporally consistent alpha embeddings through a contrastive learning objective function, our model can be guided to mine hidden factors that are both effective and comprehensive.


\begin{figure}[t]
    \begin{centering}
        \includegraphics[width=0.9\columnwidth]{fig/contrastive_learning.pdf}
        \caption{Illustration of the temporal residual contrastive learning method. The model contrasts the past and future alpha embeddings of the same stock as positive pairs, and the embeddings of different stocks as negative pairs.}
        \label{fig:contrastive_learning}
    \end{centering}
\end{figure}


Formally, given future data \( x' \in \mathbb{R}^{N \times T' \times D} \), with \( T' \) being the length of future data,
we use the prior factor exposure \( \beta \) and hidden factor exposure \( \beta_{h} \) extracted from historical data to calculate the future alpha embedding \(e_{\alpha}'\):

\begin{gather}
    e'_{s}      = \phi'_{\text{feat}}(x') \\
    e_{\alpha}' = e'_{s} - \phi'_{\text{prior}}(e'_{s}, \beta) - \phi'_{\text{hidden}}(e'_{r}, \beta_{h})
\end{gather}
where \( \phi'_{\text{feat}} \), \( \phi'_{\text{prior}} \), and \( \phi'_{\text{hidden}} \) represent the feature extractor, prior beta module, and hidden beta module for the future data, respectively. Note that \( \phi'_{\text{prior}} \) and \( \phi'_{\text{hidden}} \)  share parameters with \( \phi_{\text{prior}} \) and \( \phi_{\text{hidden}} \), respectively.


In this context, we employ the InfoNCE loss function from \cite{oord2018representation} as the contrastive learning loss function, formulated as follows:
\begin{equation}\scriptsize
    \mathcal{L}_{\text{CL}}=-\frac{1}{N} \sum_{i=1}^{N} \log \frac{\exp\left(\text{sim}(p(e_{\alpha}^{(i)}), p({e'}_{\alpha}^{(i)})) / \tau\right)}{\sum_{j=1}^{N} \exp\left(\text{sim}(p(e_{\alpha}^{(i)}), p({e'}_{\alpha}^{(j)})) / \tau\right)}
\end{equation}
where \( p(x) = \) is a 2-layer MLP with LeakyReLU activation functions, and \( \text{sim}(x, y) \) represents the cosine similarity function, \( \tau \) is the temperature parameter.



\subsubsection{Objective Function}

Our objective function consists of two parts. The first part is the mean squared error (MSE) over multiple forward periods, which aims to minimize the prediction error. The second part is the contrastive learning loss, which guides the model to mine hidden factors that are both effective and comprehensive. The overall objective function is:
\begin{gather}
    \mathcal{L}_{\text{mse}} = \frac{1}{N \cdot L} \sum_{l=1}^{L} \sum_{i=1}^{N} (\hat{y}^{(i,l)} - y^{(i,l)})^{2} \\
    \mathcal{L} = \mathcal{L}_{\text{mse}} + \gamma  \mathcal{L}_{\text{CL}}
\end{gather}
where \( L \) is the number of forward prediction periods, \( y^{(i,l)} \) is the true return of the \( i \)-th stock at the \( l \)-th forward period, and \( \gamma \) is a hyperparameter that balances the contributions of the mean squared error loss and the contrastive learning loss.



\begin{table*}[ht]\fontsize{8pt}{10.5pt}\selectfont
    \centering
    \begin{tabular}{ll|cc|cc|cc|cc}
        \hline
        \multirow{2}{*}{Methods}                         & \multirow{2}{*}{Description}     & \multicolumn{2}{c|}{$\Delta t=1$} & \multicolumn{2}{c|}{$\Delta t=5$} & \multicolumn{2}{c|}{$\Delta t=10$} & \multicolumn{2}{c}{$\Delta t=20$}                                                                         \\
                                                         &                                  & IC                                & ICIR                              & IC                                 & ICIR                              & IC              & ICIR            & IC              & ICIR            \\ \hline
        \textbf{MLP}                                     & Multi-layer   perceptron model   & 0.0579                            & 0.6032                            & 0.0650                             & 0.7043                            & 0.0674          & 0.7155          & 0.0716          & 0.8029          \\
        \textbf{GRU} \cite{cho2014learning}              & RNN model based on   GRU         & 0.0637                            & 0.7045                            & 0.0813                             & 0.9277                            & 0.0829          & 0.9378          & 0.0861          & 1.0083          \\
        \textbf{TCN} \cite{bai2018empirical}             & Temporal   convolutional network & 0.0596                            & 0.6271                            & 0.0719                             & 0.8018                            & 0.0704          & 0.7880          & 0.0688          & 0.7986          \\
        \textbf{Transformer} \cite{vaswani2017attention} & Time-series   Transformer        & 0.0617                            & 0.6764                            & 0.0748                             & 0.8292                            & 0.0739          & 0.8523          & 0.0723          & 0.9102          \\
        \textbf{ALSTM} \cite{qin2017dual}                & Attention-based LSTM             & 0.0646                            & 0.7320                            & 0.0813                             & 0.9597                            & 0.0818          & 0.9687          & 0.0815          & 1.0091          \\
        \textbf{SFM}  \cite{zhang2017stock}              & Discrete Fourier transform       & 0.0621                            & 0.6758                            & 0.0789                             & 0.8425                            & 0.0821          & 0.8624          & 0.0853          & 0.9354          \\
        \textbf{GAT}  \cite{velivckovic2017graph}        & Graph attention   network        & 0.0538                            & 0.5197                            & 0.0667                             & 0.6743                            & 0.0678          & 0.7093          & 0.0669          & 0.7650          \\
        \textbf{HIST}  \cite{xu2021hist}                 & Mining hidden concepts           & 0.0571                            & 0.6059                            & 0.0705                             & 0.7954                            & 0.0703          & 0.8036          & 0.0622          & 0.7656          \\
        \textbf{HyperGCN} \cite{bai2021hypergraph}       & Hypergraph convolutional         & 0.0554                            & 0.5899                            & 0.0665                             & 0.7359                            & 0.0688          & 0.7502          & 0.0708          & 0.7552          \\
        \textbf{STHAN-SR}  \cite{sawhney2021stock}       & Spatio-temporal  hypergraph      & 0.0597                            & 0.6500                            & 0.0765                             & 0.9162                            & 0.0791          & 0.9724          & 0.0812          & 1.0201          \\
        \textbf{FactorVAE}  \cite{duan2022factorvae}     & A factor model based   on VAE    & 0.0646                            & 0.6699                            & 0.0807                             & 0.8680                            & 0.0756          & 0.7496          & 0.0722          & 0.6526          \\
        \textbf{CI-STHPAN}  \cite{xia2024ci}             & Spatial-temporal   pre-training  & 0.0554                            & 0.5460                            & 0.0707                             & 0.7570                            & 0.0727          & 0.7703          & 0.0711          & 0.7123          \\ \hline
        \textbf{FactorGCL}                               & Our proposed model               & \textbf{0.0684}                   & \textbf{0.7487}                   & \textbf{0.0885}                    & \textbf{0.9787}                   & \textbf{0.0915} & \textbf{1.0327} & \textbf{0.0929} & \textbf{1.0350} \\ \hline
    \end{tabular}
    \caption{The stock returns prediction performance of all compared methods on the test dataset; the higher, the better.}
    \label{tab:metric}
\end{table*}


\section{Experiments}

In this section, we present a series of experiments to demonstrate the effectiveness of our proposed method in real-world stock markets. Our discussion is structured around the following key research questions:

\begin{itemize}
    \item \textbf{RQ1:} How does our method compare to existing stock trend prediction methods in terms of performance?
    \item \textbf{RQ2:} What impact does each module in our model have on its overall performance?

    \item \textbf{RQ3:} How does varying the number of hidden factors affect the model's performance?
          % \item \textbf{RQ3:} What market characteristics are reflected by the hidden factors identified by our model?

    \item \textbf{RQ4:} Can our method achieve higher investment profits in simulated investment scenarios?

\end{itemize}



\subsection{Experiment Settings}



% \subsubsection{Dataset}

We conduct our experiments on the China A-shares market, utilizing a dataset spanning from 01/01/2014 to 06/30/2023. This dataset includes 5028 stocks, excluding suspended or otherwise abnormal stocks, and is constructed from a sequence of market data comprising day-level price-volume data (\textit{high}, \textit{open}, \textit{low}, \textit{close}, \textit{volume-weighted average price}, and \textit{trading volume}). In detail, the cross-sectional standardizated future stock returns with multiple periods ($\Delta t=1, 5, 10, 20$) are used as labels, and the future return is calculated by the formula $y_{t} = \frac{\text{price}_{t+\Delta t+ 1} - \text{price}_{t+1}}{\text{price}_{t+1}}$, where $\text{price}_{t}$ is the volume-weighted average price at trading day $t$. The length of historical squence data $x$ is $T=60$, and the length of future data $x'$ is $T'=20$. We adopt secondary industry factors as the prior factors (83 industries, if a stock belongs to the industry, the value of corresponding factor exposure is 1, otherwise 0).

We follow the temporal order to split the dataset into training set, validation set and test set, where the time length is \textit{5~years} : \textit{1~year} : \textit{2~years}, and adopt a rolling method for training and testing. The overall test period is from 01/01/2020 to 06/30/2023. The other details about the experiment are provided in the supplementary material\footnote{\url{https://tinyurl.com/bdhffkch}}.
% https://drive.google.com/file/d/1dObXIQMwg3-caQcQlg-gW6v_m1S8BOiH/view

\subsection{Main Results}
In the experiment, we compare our proposed model with competitive baselines on the stock trend prediction task. In order to evaluate the performance of the compared methods, we adopt the information coefficient (IC) as metric, which are the widely-used evaluation metrics in finance. Besides, we also report the information ratio of information coefficient (ICIR) to evaluate the stability of prediction.

Table~\ref{tab:metric} summarizes the performances of all compared methods on the test dataset. Our method achieves the highest IC and ICIR among all the compared methods. From the experimental results, we have the following observations:

\begin{itemize}
    \item FactorGCL can achieve better results than existing stock trend prediction methods like ALSTM, SFM and STHAN-SR, which illustrates the effectiveness of the proposed method for stock trend prediction.
    \item Moreover, compared with some baselines which can also extract the hidden relations from market data, like HIST, FactorVAE and CI-STHPAN, our model can mine the hidden factors more effectively and comprehensively, to achieve higher prediction performance.
\end{itemize}



\subsection{Ablation Study}


To verify the effectiveness of different modules in our framework, we build four variants of proposed model by removing the prior beta module, hidden beta module, individual alpha module, and contrastive learning loss, respectively. Table~\ref{tab:ablation} lists the IC of these variants on the test dataset. The results show that each module in our model contributes to the overall performance of our model.

\begin{table}[h]\fontsize{9pt}{12pt}\selectfont
    \centering
    \begin{tabular}{c|cccc}
        \hline
                               & $IC_{\Delta t=1}$ & $IC_{\Delta t=5}$ & $IC_{\Delta t=10}$ & $IC_{\Delta t=20}$ \\ \hline
        -\textit{wo Prior}     & 0.0654            & 0.0842            & 0.0904             & 0.0906             \\
        -\textit{wo Hidden}    & 0.0649            & 0.0819            & 0.0828             & 0.0851             \\
        -\textit{wo Alpha\&CL} & 0.0618            & 0.0808            & 0.0857             & 0.0856             \\
        -\textit{wo CL}        & 0.0661            & 0.0856            & 0.0899             & 0.0912             \\
        \hline
        FactorGCL              & \textbf{0.0684}   & \textbf{0.0885}   & \textbf{0.0915}    & \textbf{0.0929}    \\ \hline
    \end{tabular}
    \caption{The ablation study results on the test dataset}
    \label{tab:ablation}
\end{table}


We also conducted an experiment to assess the impact of varying the number of hidden factors \( M \) on the model's performance. In this experiment, we adjusted the number of hidden factors produced by the hidden beta module while keeping all other components of the model constant. The results, shown in Figure~\ref{fig:hidden}, demonstrate an improvement in model performance as the number of hidden factors increases, indicating that hidden factors play a crucial role in enhancing the model's predictive power.
However, beyond a certain point, further increases in hidden factors lead to a decline in performance, suggesting that an excessive number of hidden factors can cause overfitting and degrade the model's efficacy, which is consistent with our intuition.


\begin{figure}[t]
    \begin{centering}
        \includegraphics[width=0.90\columnwidth]{fig/hidden.pdf}
        \caption{The performances of FactorGCL with different numbers of hidden factors.}
        \label{fig:hidden}
    \end{centering}
\end{figure}


\begin{table}[b]\fontsize{9pt}{10pt}\selectfont
    \centering
    \begin{tabular}{c|ccc|ccc}
        \hline
        \multirow{2}{*}{Methods} & \multicolumn{3}{c|}{CSI300} & \multicolumn{3}{c}{CSI500}                                                                     \\
                                 & AR                          & IR                         & RoMaD          & AR             & IR             & RoMaD          \\ \hline
        MLP                      & 0.034                       & 0.957                      & 0.180          & -0.001         & -0.015         & -0.003         \\
        GRU                      & 0.083                       & 2.281                      & 1.071          & 0.043          & 1.137          & 0.401          \\
        TCN                      & 0.042                       & 1.121                      & 0.418          & 0.010          & 0.261          & 0.067          \\
        Transformer              & 0.058                       & 1.675                      & 0.742          & 0.040          & 1.082          & 0.396          \\
        ALSTM                    & 0.078                       & 2.248                      & 0.891          & 0.046          & 1.323          & 0.413          \\
        SFM                      & 0.095                       & 2.913                      & 1.122          & 0.025          & 0.743          & 0.185          \\
        GAT                      & 0.004                       & 0.100                      & 0.037          & -0.020         & -0.515         & -0.117         \\
        HIST                     & 0.048                       & 1.369                      & 0.592          & -0.022         & -0.620         & -0.172         \\
        HyperGCN                 & 0.015                       & 0.454                      & 0.107          & 0.007          & 0.200          & 0.069          \\
        STHAN-SR                 & 0.047                       & 1.376                      & 0.544          & 0.033          & 0.906          & 0.165          \\

        FactorVAE                & 0.092                       & 2.446                      & 0.740          & 0.050          & 1.271          & 0.411          \\

        CI-STHPAN                & 0.044                       & 1.271                      & 0.251          & 0.028          & 0.797          & 0.252          \\
        \hline
        FactorGCL                & \textbf{0.169}              & \textbf{4.208}             & \textbf{2.978} & \textbf{0.087} & \textbf{2.092} & \textbf{0.737} \\ \hline
    \end{tabular}
    \caption{The investment simulation results on CSI300 and CSI500 stocks; the higher, the better.}
    \label{tab:backtest}
\end{table}



\subsection{Investment Simulation}


To further evaluate the profitability of our method in the real stock market, we conduct an investment simulation. Specifically, we adopt a simple stock selection strategy, referred to as the \textit{TopK} strategy. This strategy involves investing in the \textit{TopK} stocks with the highest predicted scores each trading day and selling them after holding for \(\Delta t\) days, where \(\Delta t\) represents the model's prediction period. In the simulation backtest, we select stocks from the CSI 300 and CSI 500 indexes, which are representative of large-cap and mid to small-cap stocks, respectively, providing a balanced and comprehensive evaluation of our investment strategy across different market segments. We use the equally weighted CSI300 and CSI500 portfolio as the benchmark, and set $\textit{TopK}=30$, $\Delta t=10$, and the transaction cost to 0.3\%.


We present the cumulative return (CR) curves of all compared methods in Figure~\ref{fig:backtest} and report the annualized return (AR), information ratio (IR), and return over maximum drawdown (RoMaD) of cumulative excess return (CER) relative to the benchmark in Table~\ref{tab:backtest}, and the meaning of these metrics can be found in the supplementary material. The investment simulation results show that our method achieves the best performance across all metrics, indicating that our model can achieve profitable investments in the real market.


\begin{figure}[t]
    \begin{centering}
        \includegraphics[width=1.0\columnwidth]{fig/backtest3.pdf}
        \caption{The results of investment simulation.}
        \label{fig:backtest}
    \end{centering}
\end{figure}




\section{Conclusion}


In this paper, we propose a novel hypergraph-based factor model with temporal residual contrastive learning (FactorGCL), which leverages both human-designed prior factors and data-driven hidden factors to predict stock returns. Specifically, our model follows a cascading residual hypergraph architecture, in which the hidden factors are extracted from the residual information after removing the prior factor information. To enhance the effectiveness and comprehensiveness of the hidden factors, we design a temporal residual contrastive learning method that contrasts stock-specific residuals embeddings over different time periods. Our extensive experiments demonstrate that our proposed FactorGCL outperforms existing state-of-the-art baselines in terms of predictive accuracy and investment profitability, and the ablation study further verifies our method can effectively extract hidden factors to improve the model's performance. In the future, we plan to apply the hypergraph-based factor model to risk factor mining and portfolio optimization.


\clearpage

\bibliographystyle{plain}
\input{paper.bbl}

\end{document}
