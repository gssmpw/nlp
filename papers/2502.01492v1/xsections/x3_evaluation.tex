\section{Designing Evaluations for Systems Engineering}
We highlight core trade-offs associated with building systems, namely efficiency, scalability and adaptability. We argue that systems engineering training and evaluation environments must be \textit{dynamic} and \textit{open-ended} to adequately assess the dynamic equilibrium of these characteristics.

\subsection{Real-World Intuition}
In the design phase of a system engineering project, the focus is on delivering a proposal that meets various requirements and user preferences for features and costs. This requires deep domain expertise since many valid proposals can exist, yet have varying profiles in terms of up-front costs, maintenance costs, implementation time, complexity, regulatory compliance, scalability, and so on. Design capability is readily tested in the software industry with system design interviews that pose questions such as “How would you design a real-time collaborative word processing application like Google Docs?” or, more bluntly, “Design Google Docs.”, “Design Uber.”, “Design Twitter.”, etc. These questions are not meant to be answered in a single pass, but rather serve as a starting point for iteratively gathering requirements and proposing increasingly detailed solutions.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\columnwidth]{zimages/_systems/requisite_variety3.pdf}
    \caption{\textit{The Law of Requisite Variety.} $T_1: A \rightarrow E$ and $T_2: A \rightarrow D\prime$ are trajectories of viable and unviable systems respectively over time. A system is viabile within the total state space $S$ when the variety of the environment at that time $V_E$ remains a subset of variety the system can handle $V_R$. Systems must adapt proactively ($A \rightarrow B$) to ensure this condition is met, but then also reduce variety to improve efficiency and maintainability ($D\rightarrow E$).}
    \label{fig:requisite_variety}
\end{figure}

After a real system is designed, the implementation phase begins and often never truly ends. Successful systems typically continue to expand in scope because increased outputs fuel greater demand. This pattern is evident in large software services, energy networks, and public transportation systems. Even if overall scale plateaus, there is an ongoing need for repair and maintenance—particularly in physical systems but also in software, which must periodically upgrade dependencies and refactor for performance.  Consequently, the longevity and effectiveness of a system fundamentally depend on its capacity to assimilate feedback and adapt to inevitable changes.

Because adaptability can be expensive to build in, it becomes one of the pivotal trade-offs when designing and implementing systems. Ackoff’s insights into “messy,” holistic problems \cite{ackoff1974} suggest that investing in adaptability generally leads to long-term gains, well beyond the initial concerns of scaling and maintenance. The practical implementation of adaptability relies on having feedback mechanisms. This is done through automated means like logging in software or more manually such as accepting verbal customer feedback. Using the feedback to make changes to the system is thus core to ensuring it meets expectations through key performance indicators. 

Some future scenarios are more serious and difficult to fully predict. Recent examples such as the COVID-19 pandemic required large-scale adaptations not seen since World War II, and the volatility of geopolitics—as highlighted by the conflict in Ukraine—continues to demand swift adjustments in global systems. Natural disasters like hurricanes and wildfires, technological breakthroughs such as the generative AI boom, major cybersecurity incidents, and new discoveries of key commodities further underscore the need for flexible system design.

\subsection{Supporting Theory}
Fortunately, the study of systems has long acknowledged the value of adaptability, leading to foundational frameworks that inform real-world solutions. One such lineage is \emph{cybernetics} \cite{wiener1948}, which reveals how continuous feedback loops and robust communication channels allow systems to counter external disturbances. Building on this, the “viable system model” \cite{beer1959, beer1972} emphasizes hierarchical structures that encourage \textit{proactive} responses, enabling organizations and infrastructures to pivot swiftly under changing requirements. Likewise, Ashby’s \emph{law of requisite variety} \cite{ashby1956} stresses that systems must possess enough internal complexity to handle the range of potential external disruptions. 

These same principles of adaptability can sometimes clash with the pursuit of automation, which aims to reduce complexity and boost efficiency. For instance, a mechanized assembly line can mass-produce a single product more rapidly than a human worker, yet the latter may be more versatile in producing a variety of items. In software, production-level code is often streamlined through rigid abstractions, whereas one-off scripts are less optimized but highly flexible. Even in the study of LLMs, the choice between prompt-engineering and finetuning weights reflect this same trade-off. System engineers thus face a persistent challenge of preserving \emph{dynamic equilibrium}, in which the benefits of automation and scale do not compromise a system’s capacity to adapt \cite{forrester1961industrial, holling1973resilience, sterman2000business}.

From a machine learning perspective, adaptability has been explored under many paradigms, including domain adaptation \cite{redko2022domainadaptationtheory}, meta-learning for agents \cite{beck2024metareinforcementlearning}, continual learning \cite{wang2024continuallearning}, in-context learning \cite{dong2024incontextlearning}, and out-of-distribution generalization \cite{liu2023outofdistribution}. Central themes across these fields involve developing robust representations, ensuring sample-efficient training, and promoting safe exploration. By weaving AI-driven automation into systems, we now have the opportunity to significantly enhance both efficiency and adaptability—two objectives that have traditionally been at odds.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\columnwidth]{zimages/_systems/viable_system2.pdf}
    \caption{\textit{The Viable System Model.} Systems are organized into five levels which are concisely: 1. operational units, 2. coordination, 3. control, 4. future planning, and 5. identity. Longer descriptions areg
    These levels are only responsible for the \textit{variety} associated with that level and can escalate or delegate as needed (Left). A key aspect is how Level 5 effectively balances out the tension between Levels 3 and 4 which are more present- and future-focused respectively (Right).}
    \label{fig:requisite_variety}
\end{figure}

\subsection{Evaluations for AI Agents}
Recent advances in AI research have fueled efforts to build virtual agents capable of increasingly complex interactions with real-world interfaces. These developments can be seen in emerging chatbot features such as code execution, web navigation, and project artifact management. As these cognitive capabilities continue to mature, they provide a foundation for agents to meaningfully contribute to systems engineering projects. Realizing this vision, however, requires a fundamental rethinking of how we both train and evaluate virtual AI agents.

Evaluation methods for LLM-derived agents naturally began with classic NLP benchmarks, such as question answering in MMLU \cite{hendrycks2021mmlu}. They have since evolved to encompass multi-turn interaction \cite{zheng2023mtbench}, multimodality \cite{yue2024mmmu}, and external tool use \cite{zhou2023agentbench, he2024webvoyager}—capabilities expected of advanced AI agents. SWE-bench \cite{jimenez2024swebench} (and its multimodal extension \cite{yang2024swebenchmultimodalaisystems}) is likely the most challenging agent benchmark in use today. It requires agents to resolve issues in codebases by modifying multiple files and subsequently passing unit tests. Though it involves reasoning and multi-step planning, it remains a static evaluation that does not measure adaptability and the capacity to maintain dynamic equilibrium. This would hold true even for an extension of the benchmark in which agents designed a system like Google Docs and implemented it, yet never had to respond to changing requirements or circumstances.

By contrast, non-LLM-based agents have often been evaluated in \emph{dynamic} environments. This is the case for agents achieving superhuman performance in competitive games such as Go \cite{silver2017alphazero} and StarCraft II \cite{vinyals2019alphastar}, where the presence of an opponent forces rapid adaptations to both the agent’s own actions and those of adversaries. For a time, increasingly complex games appeared to be a promising route to building general intelligence, culminating in work on \textit{Minecraft} via Voyager \cite{wang2023voyager} and MineDojo \cite{fan2022minedojo}. These agents achieved goals in a \emph{dynamic}, \emph{open-ended} environment, with effectively unconstrained objectives demanding resource gathering, multi-step planning, and adaptability to emergent challenges. 


\subsection{The Ideal Evaluation Environment for Systems Engineering}
Interest in dynamic, open-ended environments waned somewhat after the advent of LLM-based generalist models. However, the rapid evolution of ChatGPT and its successors—featuring multimodality, tool-use capabilities, and ample test-time compute—opens new possibilities for resurrecting this research agenda in a more advanced form.

Based on the discussion thus far, \emph{sandbox games} appear to be the ideal setting for evaluating system engineering. They let researchers specify high-level objectives and observe an agent’s ability to break down tasks, weigh trade-offs, and implement solutions. Over time, the researcher can change these objectives or introduce disruptions, testing the agent’s capacity to maintain a healthy dynamic equilibrium through iterative adaptation. The more open-ended the sandbox, the wider the variety of conditions the agent must handle. Simulated environments additionally have the benefits of being fundamentally safer than real-world testing and can manage the trade-off between world physics complexity and scalability.

Drawing on \textit{Minecraft} as inspiration, one can envision an “ideal” environment that focuses on abstractions relevant to system engineering while omitting excessively detailed physics. Full 3D simulations can be computationally expensive and often distract from the higher-level reasoning crucial for scaling and process orchestration. Accordingly, a game environment centered on resource flows, balancing trade-offs, and long-horizon planning is preferable. Core properties of such an environment include:
\begin{itemize}
    \item \textbf{Automation.} The agent’s action space should permit automating processes and managing the associated trade-offs between efficiency and adaptability.
    \item \textbf{Complex Evaluation Metrics.} Long-horizon performance, resource usage, and resilience under partial failures become measurable, enabling richer assessments than single-turn tests.
    \item \textbf{Multi-Agent Support.} Collaboration with peers, hierarchical coordination, and competition with adversaries significantly increase complexity, further testing an agent’s capacity to adapt.
    \item \textbf{Modding Support.} Allowing users and artificial agents to create modifications or extensions fosters adaptation to out-of-distribution scenarios.
    \item \textbf{Scalability.} The environment mechanics should be at the right level of abstraction to facilitate systems reasoning, planning, and implementation without requiring excessive computational resources.
\end{itemize}

There are many candidate sandbox games—\textit{Cities: Skylines}, \textit{The Sims}, \textit{Stardew Valley}, \textit{Kerbal Space Program}, \textit{No Man's Sky}, \textit{Satisfactory}, among others—that support a form of systems engineering. Yet they each have limitations with respect to one or more of the above criteria. As the next section will show, \emph{Factorio} stands out for providing an ideal testbed for AI system engineering: its mechanics inherently encourage large-scale “megabase” building, resource management, automation, and iterative adaptation.
