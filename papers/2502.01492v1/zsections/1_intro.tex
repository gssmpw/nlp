
\section{Introduction}
Since the release of ChatGPT in November 2022, the field of generative AI has experienced an explosive surge in both attention and investment. Early successes in large language models (LLMs) have demonstrated that tuning foundation models to follow instructions and optimize for human preferences can yield AI systems capable of a wide range of tasks—often approaching or matching human-level proficiency in specific domains. This has led researchers and industry experts alike to speculate that we now possess the fundamental building blocks for artificial general intelligence (AGI).

A logical progression beyond chatbots and prompt-based LLMs is the development of \textbf{AI agents}. Unlike conventional models that simply output text in response to queries, agentic AI systems combine language comprehension with memory, tools, and other interfaces, allowing them to interact with environments in near-human ways. This paradigm shift has ignited excitement about the possibility of autonomous, always-on AI “workers” that can undertake many tasks currently performed by humans—ranging from data analysis to coding, from supply-chain management to design optimization.

Yet, for all the excitement around AI agents, today’s systems often remain limited in scope and capability. Part of this is due to a lack of robustness and a need for continued integration with real-world interfaces, but it is also due to limitations of static development environments for agents. We argue that these constraints can be overcome by explicitly training and evaluating agents in system engineering tasks, where scalability, adaptability, and long-term strategic thinking become paramount. By building, optimizing, and maintaining complex, real-world systems—or close simulations thereof—agents can push well beyond static benchmarks toward generalized \textbf{superhuman problem-solving}.

This paper makes three main points:

\begin{enumerate}
    \item \textbf{System engineering} is a uniquely high-leverage capability. Societies worldwide face challenges that demand new levels of coordination and innovation in designing and managing complex infrastructures and processes.

    \item \textbf{Sandbox-style simulation platforms} are essential for training and evaluating AI agents on their capacity to handle real-world complexities. Such platforms can capture the interplay between \emph{adaptability}, \emph{automation}, and other \emph{dynamic trade-offs} that static benchmarks fail to represent, thereby enabling more realistic and robust testing.

    \item \textbf{\textit{Factorio}} stands out as the ideal sandbox game for this purpose as its entire nature centers on designing and automating complex systems along with key technical advantages like robust support for modifying and augmenting game mechanics.
\end{enumerate}

We explore each of these points in detail in subsequent sections individually, and provide an Appendix which visually illustrates \textit{Factorio} for newcomers to gain intuition about its gameplay.