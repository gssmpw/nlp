\section{Related works}
\label{sec-related}

Processing data streams comes with inevitable challenges related to~the~volume of~the~data and~its \textit{temporal} nature**Kolde, "Stream Mining Real-Time Internet Data"**. One of~the~most frequently addressed difficulties of~this data type is~the~data nonstationarity, resulting from \textit{concept drifts}**Zliobaite, "How and to What Extent Should Concept Drift be Handled in the Framework of Incremental Classifier Learning?**. The significance of~recognizing concept changes stems from the~fact that they usually harm the~recognition quality of~methods since the~knowledge generalized in~machine learning models becomes outdated.

The~primary axis of~the~concept drift taxonomy describes its impact on~the recognition model or, alternatively, the~data distribution shift in~relation to~the~decision boundary**Gama et al., "Learning with Landmarks: A Framework and Algorithms for Concept Drift Detection"**. Changes that do not affect the~recognition quality -- and~therefore cannot be recognized when monitoring the~quality of~the~model -- are~referred to~as \textit{virtual}. Meanwhile, those that affect the~decision boundary are~referred to~as \textit{real}**Kifer et al., "Detecting Change in Streams"**. It is~worth keeping in~mind that the~potentially insignificant \textit{virtual} changes can be visible in~the~initial stage of~non-sudden \textit{real} concept changes**Gama et al., "Learning with Landmarks: A Framework and Algorithms for Concept Drift Detection"**. The~other axes of~the~concept drift taxonomy consider the~drift dynamics and~its recurrence. The~transition between the~consecutive concepts can be \textit{sudden} -- where one can see a~single time instant after which the~samples come from the~new concept. The~other categories describe slower-paced changes in~the~form of~\textit{gradual} or~\textit{incremental} drifts, in~which one can observe a~period of~concept transition. In~the~\textit{gradual} changes, the~samples in~the~transition period are~sampled from both the~previous and~the~emerging concepts, while in~the~\textit{incremental} changes, they form a~temporary superposition of~the~two transitioning concepts. Finally, regardless of~the~dynamics of~drift, the~concepts that appeared in~the~past may reoccur, which is~typical of~the~problems describing the~phenomena of~cyclic nature**Liu et al., "Detecting Concept Drift with Multi-granularity Features"**.

\paragraph{Concept drift detection}

Since concept changes may have a~real effect on~recognition quality, it has become a~standard procedure to~monitor the~state of~a~system in~search for a~concept drift**Kifer et al., "Detecting Change in Streams"**. For this purpose, many solutions have been proposed. The~initial drift detection methods exploited the~fact that concept drift affects the~classification quality. Those methods include the~\textit{Adaptive Windowing}**Bifet et al., "Learning from Time-Changing Data with Adaptive Windowing"**, which uses varying-width windows to~compare the~frequency of~errors. Methods that monitor the~quality of~a~classification model are~described as \textit{explicit}**Kifer et al., "Detecting Change in Streams"**. The~primary benefit of~such a~drift detection approach is~the~possibility to~directly act upon a~change by adapting the~classification model to~the~current data. The~use of~\textit{explicit} methods also has some drawbacks. Since their operation is~based on~recognition quality, the~method will not be able to~detect the~\textit{virtual} drifts or~the~initial phases of~real ones that do not yet impact recognition quality. Another disadvantage is~the~reliance on~the~availability of~labels, which, in~the~data streams with high velocity, are~often delayed or~not available entirely**Zliobaite et al., "Handling Drifts and Concept Shifts in Stream Classification"**.

Another category of~methods monitor characteristics of~data stream processing other than those related to~the~quality of~the~model -- the~\textit{implicit} drift detection methods. Those include both supervised and~unsupervised approaches. The~supervised drift detectors can use labels to~monitor the~quality of~the~data distribution that are~not related to~the~errors made by the~classifier. The~\textit{Centroid Distance Drift Detector}**Zliobaite et al., "Handling Drifts and Concept Shifts in Stream Classification"** is~a~simple yet effective approach that monitors the~class centroids to~detect concept changes. Labels are~also used in~the~\textit{Complexity-based Drift Detector}**Gama et al., "Learning with Landmarks: A Framework and Algorithms for Concept Drift Detection"**, which relies on~the~monitoring of~complexity measures to~express the~difficulty of~the~classification task. Although the~supervised \textit{implicit} methods offer some independence from the~base classifier, they still rely on~access to~labels.

In the family of \textit{implicit} drift detectors, most of the methods are~\textit{unsupervised}. Those are~especially valuable in~the~context of~the~velocity of~the~data stream -- where the~time of~providing the~labels affects the~moment of~drift detection**Bifet et al., "Learning from Time-Changing Data with Adaptive Windowing"**. Unsupervised methods can monitor quite a~wide range of~data characteristics, including the~data distribution analysis with hypothesis testing**Gama et al., "Learning with Landmarks: A Framework and Algorithms for Concept Drift Detection"**, or~the~percentage of~outliers measured with the~one-class classifier**Zliobaite et al., "Handling Drifts and Concept Shifts in Stream Classification"**. Some interesting unsupervised methods utilize the~classification model but only to~measure label-independent characteristics of~the~underlying classification model. One of~the~most interesting ones of~this type is~the~\textit{Margin Density Drift Detector}**Liu et al., "Detecting Concept Drift with Multi-granularity Features"**, which examines the~distribution of~samples near the~decision boundary.

All those characteristics considered in~drift detection -- from the~model quality and~its confidence to~the~temporal complexity of~the~classification task -- can be described as metafeatures of~the~data**Gama et al., "Learning with Landmarks: A Framework and Algorithms for Concept Drift Detection"**. This was directly addressed in~the~\textit{Meta-Feature-based Concept Evolution Detection} framework**Liu et al., "Detecting Concept Drift with Multi-granularity Features"**, where selected data distribution metrics captured the~statistical metafeatures of~the~data. Metafeatures were also used to~identify the~concept in~the~\textit{Fingerprinting with Combined Supervised and~Unsupervised Meta-Information} (\textsc{ficsum})**Zliobaite et al., "Handling Drifts and Concept Shifts in Stream Classification"**, where various metafeatures were used not only to~detect a~concept change but also to~re-identify it in~the~case of~recurrence. Some of~the~metafeatures used in~\textsc{ficsum} were previously used in~the~\textit{Feature Extraction for Explicit Concept Drift Detection}**Gama et al., "Learning with Landmarks: A Framework and Algorithms for Concept Drift Detection"**, which selects the~best model for current data distribution. A~similar strategy was used in~the~already mentioned \textsc{ficsum}**Zliobaite et al., "Handling Drifts and Concept Shifts in Stream Classification"**, which selects the~classifier dedicated to~the~currently solved task based on~the~gathered meta-information. Such a~selection is~especially valuable when processing the~data streams with recurring concepts, as it offers an opportunity to~use the~previous knowledge instead of~the~incremental adaptation from the~ground up**Bifet et al., "Learning from Time-Changing Data with Adaptive Windowing"**.