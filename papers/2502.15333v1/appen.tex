% !TEX root = paper.tex
\section{Appendix}

%\begin{fact}\label{fact:norms} For any vector $x\in \C^n$, and for any $0<r<p$, we have $||x||_p\leq ||x||_r\leq n^{(1/r-1/p)} ||x||_p$. \end{fact}

%\begin{fact}\label{ineq} Let $\frac{a_1}{b_1},\frac{a_2}{b_2},\ldots,\frac{a_n}{b_n}$ be any $n$ fractions. Let $\lambda_1,\lambda_2,\ldots,\lambda_n\geq 0$ be such that $\sum_{i=1}^n \lambda_i=1$. Then, $\frac{\sum_{i=1}^n \lambda_i a_i}{\sum_{i=1}^n \lambda_i b_i}\leq \max\{\frac{a_1}{b_1},\frac{a_2}{b_2},\ldots,\frac{a_n}{b_n}\}$. \end{fact}

%\begin{lemma}(Chernoff bound)\label{lem:chernoff} Let $Z_1,Z_2,\ldots,Z_v$ be independent and identically distributed Bernoulli random variables such that $\Pr[Z_i=1]=2/3$ for all $i$. Let $Z=\sum_{i=1}^v Z_i$. Then, $\Pr[Z\leq(1-\gamma) \EE[Z]]\leq e^{-1/2 \cdot \gamma^2 \cdot \EE[Z]}$ . \end{lemma}

%\subsection{More precise analysis of sample complexity} \label{sec:correction}

%In the analysis of Algorithm \ref{alg:estmoments}, we assumed $\wt{W}=W$, and we had $\ti{p}_j=p_j$ for all $j$. However, since we are estimating $\wt{W}$, conditioned on $\E$, we only have $\wt{W}\in (1-\eps_1,1+\eps_1)W$. In this section we calculate the sample complexity of an $(\eps,\delta)$-estimate of $S_t$ given an $(\eps_1,\delta/2)$-estimate $\wt{W}$ of $W$ for non-zero $\eps_1$. Using Claim \ref{claim:prob}, we get that conditioned on $\E$, for any $j$, $\frac{p_j}{1+\eps_1}\leq \ti{p}_j\leq \frac{p_j}{1-\eps_1}$. The random variables $X_j=\frac{w(A[j])^t}{\ti{p}_j}$ will no longer be an unbiased estimator of $S_t$, rather we have $(1-\eps_1)S_t\leq \EE[X_j]\leq (1+\eps_1)S_t$. Let $X=\sum_{j=1}^l X_j/l$. Then, using linearity of expectation, we have $(1-\eps_1)S_t\leq \EE[X]\leq (1+\eps_1)S_t$. Using Chebyshev's inequality, we can obtain the tail bounds as $\Pr[|X-S_t|>\eps S_t]\leq \Pr[|X-\EE[X]|>(\eps-\eps_1)S_t]\leq \frac{\var[X]}{(\eps-\eps_1)^2 S_t^2}$. Hence, the sample complexity for an $(\eps,\delta/2)$-estimate would be at most $O(\frac{\var[X]\log 2/\delta}{(\eps-\eps_1)^2 S_t^2})\leq O(\frac{(1+\eps_1)^2}{(\eps-\eps_1)^2} n^{1-1/t} \log 1/\delta)$. 

%Given parameters $\eps,\delta$, if we choose $\eps_1=\eps/2$, then sample complexity is $O(\frac{\sqrt{n} \log 1/\delta}{\eps} + \frac{(1+\eps)^2}{\eps^2} n^{1-1/t} \log 1/\delta)$.

%\section{Estimation of Moments using Hybrid Sampling}\label{sec:lower-hybrid}

%In this section we prove a lower bound showing that for the moment estimation problem, the hybrid sampling framework does not provide any significant advantage over access to just the proportional sampling oracle. In contrast, note that for the sum estimation problem, hybrid sampling-based algorithms in fact give much better sample complexity bounds over proportional sampling \cite{MPX2007,BT2022}. We prove the following result.

%\begin{thm} For any $\eps,\delta>0$ and $t>1$, any algorithm having access to a hybrid sampling oracle requires to make at least $\Omega(\frac{n^{1-1/t}\ln 1/\delta}{\eps^2})$ queries to compute an $(\eps,\delta)$-estimate for $S_t$. \end{thm}

%We show that the lower bound instance described in Section \ref{sec:lower-proportional} yields a lower bound for the hybrid sampling as well. In order to distinguish these instances, one is required to sample an element of weight $n$. We have seen that using just proportional sampling $\Omega(\frac{n^{1-1/t} \log 1/\delta}{\eps^2})$ samples are required. The probability of sampling an element of weight $d_2$ using uniform sampling is given as $\frac{n_2}{n_1+n_2}$. This probability using the values of the parameters from Section \ref{sec:lower-proportional} equals $\frac{n_2}{n_1+n_2} = \frac{1}{1+\frac{n}{\eps^{\frac{2t-1}{t-1}}}}$. The instances are distinguished if an element of weight $n$ is sampled using either proportional sampling or uniform sampling. These two probabilities are given as $\frac{1}{1+\frac{n^{1-1/t}}{\eps^2}}$ and $\frac{1}{1+\frac{n}{\eps^{\frac{2t-1}{t-1}}}}$, respectively. Overall, we get a lower bound of $\Omega(\min\{\frac{n^{1-1/t}}{\eps^2}, \frac{n}{\eps^{\frac{2t-1}{t-1}}}\}\ln 1/\delta)=\Omega(\frac{n^{1-1/t}\ln 1/\delta}{\eps^2})$ for the $(\eps,\delta)$ moment estimation problem using hybrid sampling.

%\newpage
%\section{Omitted proofs} \label{sec:omit}
%\begin{claim} $\frac{\var[X]}{(\eps-\eps_1)^2 S_t^2}\leq \frac{(1+\eps_1)^2}{l(\eps-\eps_1)^2} \cdot n^{1-1/t}$. \end{claim}

%\begin{proof}
%\begin{align*}
%\frac{\var[X]}{(\eps-\eps_1)^2 S_t^2}
%& \leq \frac{\EE[X_j^2]}{l(\eps-\eps_1)^2 \cdot S_t^2} \\
%& \leq \frac{(1+\eps_1)^2}{l(\eps-\eps_1)^2} \cdot \frac{W \cdot \sum_{a_j\in A} w(a_j)^{2t-1}}{S_t^2} && \text{(Using Equation (\ref{eqn:upper-sample}))}\\
%& = \frac{(1+\eps_1)^2}{l(\eps-\eps_1)^2} \cdot \frac{W \cdot ||w(A)||_{2t-1}^{2t-1}}{S_t^2} && \text{($w(A)$ is a vector of length $n$)} \\
%& \leq \frac{(1+\eps_1)^2}{l(\eps-\eps_1)^2} \cdot \frac{W \cdot {(||w(A)||_t^t)}^{2-1/t}}{S_t^2} && \text{(Using Fact~\ref{fact:norms}, $||w(A)||_{2t-1}\leq ||w(A)||_t$)} \\
%& = \frac{(1+\eps_1)^2}{l(\eps-\eps_1)^2} \cdot \frac{||w(A)||_1 \cdot (||w(A)||_t^t)^{2-1/t}}{(||w(A)||_t^t)^2}\\
%& = \frac{(1+\eps_1)^2}{l(\eps-\eps_1)^2} \cdot \frac{||w(A)||_1}{||w(A)||_t}\\
%& \leq \frac{(1+\eps_1)^2}{l(\eps-\eps_1)^2} \cdot n^{1-1/t} && \text{(Using Fact~\ref{fact:norms}, $||w(A)||_1\leq n^{1-1/t} ||w(A)||_t$)}
%\end{align*}
%\end{proof}

%\section{Discussion} We next discuss about moment estimation for $t<1$. We saw that for $1/2<t<1$, we have an algorithm for estimating $S_t$ using $n^{\frac{1}{t}-1}$ proportional samples and for $t\leq 1/2$, no sublinear algorithm exists for this problem that uses only proportional sampling. We explore whether it is possible to do better using hybrid sampling.

%\subsection{Hybrid sampling for $t>1/2$} We adapt the hybrid sampling based algorithm of Motwani~\etal~\cite{MPX2007} for moment estimation for $1/2<t<1$. Motwani~\etal~partition the elements into two sets Large and Small using a threshold $z$. The threshold $z$ is chosen such that there are exactly $n^{\alpha}$ elements in the set Large. How this is done is sketched in Motwani~\etal. Now, we use the proportional sampling based algorithm for moment estimation to estimate moment of elements in Large. For elements in Small, we first use bucketing to group these elements into buckets of geometrically large sizes. Let us assume that we use $n^{\beta}$ uniform samples. This way, for buckets in which there are less than $n^{1-\beta}$ elements, these buckets would be poorly estimated. We show that the error due to elements in these buckets is bounded.


%\subsection{Lower bound using hybrid sampling for $t\leq 1/2$} We create two families of instances of $n$ elements each. In both there exists an element of weight $n^{3/2}$, $n-\sqrt{n}-1$ elements of weight $\frac{\eps}{n-\sqrt{n}-1}$. In one instance we have $\sqrt{n}$ elements of weight $\sqrt{n}$ and in the other $\sqrt{n}$ elements of weight $0$.

%Probability of sampling any of the $\sqrt{n}$ elements of weight $\sqrt{n}$ using uniform sampling is at most $\frac{\sqrt{n}}{n}$. Hence, one requires $\Omega(\sqrt{n})$ samples to distinguish.

%Probability of sampling any of the $\sqrt{n}$ elements of weight $\sqrt{n}$ using proportional sampling is at most $\frac{n}{n+n^{3/2}}$. Hence, one requires $\Omega(\sqrt{n})$ samples to distinguish.

%For $t=1/2$, the moment values of the two instances are given as: $n^{3/4}+(n-\sqrt{n}-1) \frac{\eps^{1/2}}{(n-\sqrt{n}-1)^{1/2}} + \sqrt{n} (\sqrt{n})^{1/2}$ and $n^{3/4}+(n-\sqrt{n}-1) \frac{\eps^{1/2}}{(n-\sqrt{n}-1)^{1/2}}$.


%\subsection{Upper bound for $t\leq 1/2$} The problem with the approach of dividing the elements into Large and Small parts, and estimating the moment of each is that for $t\leq 1/2$, proportional sampling based approaches require $\Omega(n)$ samples. How do we fix that?
