% !TEX root = paper.tex
\section{Estimation of Moments using Proportional Sampling}\label{sec:moments}

We describe our algorithm for the moment estimation problem using proportional sampling. We mentioned earlier that for $t>1$, our upper bounds match those of Aliakbarpour~\etal~\cite{ABGPRY2018}. However, since our algorithm works in strictly more general settings and the algorithm for $1/2< t<1$ uses the same ideas, we describe it in detail. Let $A$ be a set of $n$ weighted elements. We assume access to a proportional sampling oracle on the weights of the elements in $A$. For a proportional sample, the oracle returns an element $a_j\in A$ with probability $w(a_j)/W$, where $W=\sum_{a_j\in A} w(a_j)$. Given parameters $t>1,\eps,\delta\in (0,1)$, we design an $(\eps,\delta)$-estimate of $S_t=\sum_{a_j\in A} w(a_j)^t$.

\begin{thm} There exists an algorithm $ALG$ that given proportional sampling access to the weights of the elements in a set $A$ and parameters $t>1,\eps,\delta\in (0,1)$, provides an $(\eps,\delta)$-estimate of $S_t$ using $O(\frac{\sqrt{n}\log 1/\delta}{\eps} + \frac{n^{1-1/t} \log 1/\delta}{\eps^2})$ samples. \end{thm}

\begin{algorithm}[H]
    \caption{Moment Estimation using Proportional Sampling}
    \label{alg:estmoments}
    \begin{algorithmic}[1] % The number tells where the line numbering should start
        \Procedure{MomentEstimator}{$A,t,\eps,\delta$} %\Comment{The g.c.d. of a and b}
                \State Let $\wt{W}$ denote an $(\eps_1=\eps/2,\delta/2)$-estimate of $W$ using the sum estimation algorithm of \cite{BT2022}. This step requires $480\cdot \frac{\sqrt{n}\log (2/\delta)}{\eps}$ proportional samples.
                \For $~r=1$ to $v = 48 \cdot \log 2/\delta$  %median of means improvement
                        \For $~j=1$ to $l=48 \cdot n^{1-1/t}/\eps^2$
                                \State Let $a_j$ denote a proportional sample of weight $w(a_j)$. 
				\State Compute $\ti{p}_j=\frac{w(a_j)}{\wt{W}}$.
                                \State Set $X_j=\frac{w(a_j)^t}{\ti{p}_j}$
                        \EndFor
                        \State $Y_r = \frac{\sum_{j=1}^l X_j}{l}$
                \EndFor %median of means improvement
                \State \textbf{return} median$(Y_1,\ldots,Y_v)$
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

Algorithm \ref{alg:estmoments} first computes an $(\eps_1,\delta/2)$-estimate $\wt{W}$ of the sum $W$ of the weights of elements in $A$ using the sum estimation algorithm in \cite{BT2022}\footnote{\cite{BT2022} states the sample complexity for probability of success at least $2/3$. Here, we are stating the bounds for an $(\eps_1,\delta/2)$-estimate. This is obtained using an application of the medians of means technique.}. Let the probability of sampling element $a_j$ using proportional sampling be given as $p_j=\frac{w(a_j)}{W}$. Note that we don't know $p_j$, however we can obtain a good approximation of it as follows. For a proportional sample $a_j$, we know its weight $w(a_j)$, and $\wt{W}$ gives us an approximation of $W$. Using this we get a approximation for $p_j$ as $\ti{p}_j=\frac{w(a_j)}{\wt{W}}$. Let $\E$ denote the event that $\wt{W}\in [(1-\eps_1)W,(1+\eps_1)W]$. In what follows we condition on event $\E$.

\begin{claim}\label{claim:prob} Conditioned on $\E$, for any $j\in [n]$, we have $\frac{p_j}{1+\eps_1}\leq \ti{p}_j\leq \frac{p_j}{1-\eps_1}$. \end{claim}
\begin{proof} Conditioned on $\E$, $(1-\eps_1)W\leq \wt{W}\leq (1+\eps_1)W$. The above inequality follows. \end{proof}

Conditioned on $\E$, for any $j$, we have $\frac{p_j}{1+\eps_1}\leq \ti{p}_j\leq \frac{p_j}{1-\eps_1}$. Given a proportional sample $a_j$, we define a random variable $X_j$ with value $\frac{w(a_j)^t}{\ti{p}_j}$. Then, we have $(1-\eps_1)S_t\leq \EE[X_j]\leq (1+\eps_1)S_t$. Here, $X_j$ is not an unbiased estimator of $S_t$. Next, we bound the variance of this estimator given as $\var[X_j]=\EE[X_j^2]-\EE^2[X_j]\leq \EE[X_j^2]$. 

\begin{align}
\EE[X_j^2]
& = \sum_{a_j\in A} \frac{w(a_j)^{2t}}{\ti{p}_j^2} p_j \nonumber\\
& \leq (1+\eps_1)^2 \sum_{a_j\in A} \frac{w(a_j)^{2t}}{p_j} \nonumber\\
& = (1+\eps_1)^2 \cdot W \cdot \sum_{a_j\in A} \frac{w(a_j)^{2t}}{w(a_j)} && \text{(Substituting $p_j$ with $\frac{w(a_j)}{W}$)} \nonumber\\
& = (1+\eps_1)^2 \cdot W \cdot \sum_{a_j\in A} w(a_j)^{2t-1} \label{eqn:upper-sample}
\end{align}

Let us obtain $l$ independent samples using proportional sampling and let these random variables be $X_1,\ldots,X_l$. Let $X=\frac{1}{l} \sum_{j=1}^l X_j$. We have $(1-\eps_1)S_t\leq \EE[X]=\EE[X_j]\leq (1+\eps_1)S_t$, and $\var[X]=\frac{\var[X_j]}{l}$. Using Chebyshev's inequality, we have $\Pr[|X-S_t|>\eps S_t]\leq \Pr[|X-\EE[X]|>(\eps-\eps_1) S_t]]\leq \frac{\var[X]}{(\eps-\eps_1)^2 S_t^2}$. For appropriate choice of parameter $l$, we show this probability to be at most a small constant using the following claim. 
%bound this probability to be at most $\frac{(1+\eps_1)^2}{l(\eps-\eps_1)^2} \cdot n^{1-1/t}$ in the following claim whose proof is given in Appendix \ref{sec:omit}.

%\begin{claim} $\frac{\var[X]}{(\eps-\eps_1)^2 S_t^2}\leq \frac{(1+\eps_1)^2}{l(\eps-\eps_1)^2} \cdot n^{1-1/t}$. \end{claim}

\begin{claim} $\frac{\var[X]}{(\eps-\eps_1)^2 S_t^2}\leq \frac{(1+\eps_1)^2}{l(\eps-\eps_1)^2} \cdot n^{1-1/t}$. \end{claim}

\begin{proof}
\begin{align*}
& \frac{\var[X]}{(\eps-\eps_1)^2 S_t^2} \\
\leq &\frac{\EE[X_j^2]}{l(\eps-\eps_1)^2 \cdot S_t^2} \\
 \leq &\frac{(1+\eps_1)^2}{l(\eps-\eps_1)^2} \cdot \frac{W \cdot \sum_{a_j\in A} w(a_j)^{2t-1}}{S_t^2} && \text{(Using Equation (\ref{eqn:upper-sample}))}\\
 = & \frac{(1+\eps_1)^2}{l(\eps-\eps_1)^2} \cdot \frac{W \cdot ||w(A)||_{2t-1}^{2t-1}}{S_t^2} && \text{(vector $w(A)$ has length $n$)} \\
 \leq &\frac{(1+\eps_1)^2}{l(\eps-\eps_1)^2} \cdot \frac{W \cdot {(||w(A)||_t^t)}^{2-1/t}}{S_t^2} && \text{(Using Fact~\ref{fact:norms}, $||w(A)||_{2t-1}\leq ||w(A)||_t$)} \\
 = &\frac{(1+\eps_1)^2}{l(\eps-\eps_1)^2} \cdot \frac{||w(A)||_1 \cdot (||w(A)||_t^t)^{2-1/t}}{(||w(A)||_t^t)^2}\\
 = &\frac{(1+\eps_1)^2}{l(\eps-\eps_1)^2} \cdot \frac{||w(A)||_1}{||w(A)||_t}\\
 \leq &\frac{(1+\eps_1)^2}{l(\eps-\eps_1)^2} \cdot n^{1-1/t} && \text{(Using Fact~\ref{fact:norms}, $||w(A)||_1\leq n^{1-1/t} ||w(A)||_t$)}
\end{align*}
\end{proof}


Let $\eps_1=\eps/2$. For $l=48n^{1-1/t}/\eps^2$, this failure probability is at most $1/3$. Using the standard median trick, we show that for $v=48\log 2/\delta$, this failure probability can be reduced to be at most $\delta/2$. Let us define independent Bernoulli random variables $Z_1,\ldots,Z_v$ such that $\Pr[Z_i=1]=2/3$ for all $i$. Let $Z=\sum_{r=1}^v Z_r$. Now, conditioned on $\E$, the probability that the output of Algorithm \ref{alg:estmoments} does not lie in the interval $[(1-\eps)S_t,(1+\eps)S_t]$ is the same as the probability that the median of $Y_1,\ldots,Y_v$ lies outside the interval $[(1-\eps)S_t,(1+\eps)S_t]$. This probability is at most $\Pr[Z<v/2]$. Using a standard application of a Chernoff bound, given in Lemma \ref{lem:chernoff}, we have $\Pr[Z<v/2]\leq \delta/2$.

\paragraph{Correctness and Sample complexity bounds}

We need to show that Algorithm \ref{alg:estmoments} returns an estimate $ALG(A,t,\eps,\delta)$ for which with probability at least $1-\delta$, we have $(1-\eps)S_t\leq ALG(A,t,\eps,\delta)\leq (1+\eps)S_t$. Step $2$ of Algorithm \ref{alg:estmoments} uses $\Theta{(\frac{\sqrt{n}\log (2/\delta)}{\eps_1})}$ proportional samples to obtain an $(\eps_1,\delta/2)$ estimate $\wt{W}$ of $W$. Algorithm \ref{alg:estmoments} fails if either $\E$ does not hold or the estimate in Step $10$ is incorrect. Since both of these failure probabilities are at most $\delta/2$, the algorithm succeeds with probability at least $1-\delta$. 

Algorithm \ref{alg:estmoments} uses $\Theta{(\frac{\sqrt{n}\log (2/\delta)}{\eps_1})}$ proportional samples in Step 2 and uses $O(\frac{n^{1-1/t}\log 1/\delta}{\eps^2})$ proportional samples in Steps 3 and 4. Therefore, the required number of proportional samples is $O(\frac{\sqrt{n}\log 1/\delta}{\eps_1} + \frac{n^{1-1/t}\log 1/\delta}{\eps^2})$. For $\eps_1=\eps/2$, this gives $O(\frac{\sqrt{n}\log 1/\delta}{\eps} + \frac{n^{1-1/t}\log 1/\delta}{\eps^2})$.


\section{Lower bound for Moment Estimation using Proportional Sampling}\label{sec:lower-proportional}

We use Yao's minimax lemma to prove the sample complexity lower bound for obtaining an $(\eps,\delta)$ estimate of $S_t$ using any randomized algorithm. We construct two families of instances on which $S_t$ differs by at least a $(1\pm \eps)$-factor and show that it is hard to distinguish these two instances using a small number of proportional samples.

Our lower bound constructions are as follows. There are $n_1$ elements of weight $d_1$ and $n_2$ elements of weight $d_2$ in both the instances, where the values of the parameters $n_1, n_2$ and $d_1$ are the same in both instances, and the instances differ in the value of parameter $d_2$. The exact values of these parameters will be set below. In one instance we set $d_2=n$, where as for the other instance $d_2=0$. These choices for parameter values creates a gap of a multiplicative $(1\pm \eps)$ factor between the $S_t$ values of the two instances. One can differentiate these two instances only when an element of weight $d_2=n$ is sampled using proportional sampling, and we show that this requires a lot of samples. 

\begin{thm} For any $\eps,\delta \in (0,1)$ and $t>1$, any randomized algorithm that computes an $(\eps,\delta)$-estimate of $S_t$ requires $\Omega(\frac{n^{1-1/t}\ln 1/\delta }{\eps^2})$ proportional samples. \end{thm}

\begin{proof} We construct two families of instances which we show are hard to be distinguished using a few proportional samples by Yao's lemma. In the first instance we have $n_1$ elements with weight $d_1$ and $n_2$ elements of weight $0$, and in the second instance, there are $n_1$ elements of weight $d_1$ and $n_2$ elements of weight $d_2$, where the following values are used. The parameter values used in the lower bound constructions of the two instances are given as follows. 

%\begin{tabular}{p{8cm}|p{8cm}}
%Instance $1$ & Instance $2$\\ 
%$n_1  =  \frac{n^2}{n + \eps^{\frac{2t-1}{t-1}}}$ & $n_1  =  \frac{n^2}{n+ \eps^{\frac{2t-1}{t-1}}}$\\
%$d_1  =  n^{1-1/t} \eps^{1/(t-1)}$ & $d_1 =  n^{1-1/t} \eps^{1/(t-1)}$\\
%$n_2  =  \frac{n \eps^{\frac{2t-1}{t-1}}}{n + \eps^{\frac{2t-1}{t-1}}}$ & $n_2  =  \frac{n \eps^{\frac{2t-1}{t-1}}}{n + \eps^{\frac{2t-1}{t-1}}}$\\
%$d_2  =  0$ & $d_2 = n$
%\end{tabular}

\begin{gather}
\begin{align*}
\qquad \qquad \quad n_1 & =  \frac{n^2}{n + \eps^{\frac{2t-1}{t-1}}} & n_1 & =  \frac{n^2}{n+ \eps^{\frac{2t-1}{t-1}}}\\
\qquad \qquad \quad d_1 & =  n^{1-1/t} \eps^{1/(t-1)} & d_1 & =  n^{1-1/t} \eps^{1/(t-1)}\\
\qquad \qquad \quad n_2 & =  \frac{n \eps^{\frac{2t-1}{t-1}}}{n + \eps^{\frac{2t-1}{t-1}}} & n_2 & =  \frac{n \eps^{\frac{2t-1}{t-1}}}{n + \eps^{\frac{2t-1}{t-1}}}\\
\qquad \qquad \quad d_2 & =  0 & d_2 & = n
\end{align*}
\end{gather}


\noindent The $S_t$ value for the first instance is given as follows.
\begin{align*}
n_1 \cdot d_1^t + n_2 \cdot 0
& = \frac{n^2}{n + \eps^{\frac{2t-1}{t-1}}} \cdot n^{t-1} \eps^{t/(t-1)}\\
& = \frac{n^{t+1} \eps^{t/(t-1)}}{n + \eps^{\frac{2t-1}{t-1}}} 
\end{align*}


\noindent The $S_t$ value for the second instance is 
\begin{align*}
n_1\cdot d_1^t + n_2 \cdot d_2^t 
& = \frac{n^{t+1} \eps^{t/(t-1)}}{n + \eps^{\frac{2t-1}{t-1}}} +  \frac{n \eps^{\frac{2t-1}{t-1}}}{n + \eps^{\frac{2t-1}{t-1}}} \cdot n^t\\
& = \frac{n^{t+1} \eps^{t/(t-1)}}{n + \eps^{\frac{2t-1}{t-1}}} + \eps \frac{n^{t+1} \eps^{t/(t-1)}}{n + \eps^{\frac{2t-1}{t-1}}}\\
& = (1+\eps) \frac{n^{t+1} \eps^{t/(t-1)}}{n +\eps^{\frac{2t-1}{t-1}}}
\end{align*}

The above two instances differ in their $S_t$ values by a multiplicative factor of $(1+\eps)$. In order to distinguish these two instances, one is required to sample an element of weight $d_2=n$. Using proportional sampling, the probability of sampling an element of weight $n$ in the above instance is given to be at least


\begin{align*}
\frac{n_2 d_2}{n_2 d_2 + n_1 d_1}
& = \frac{\frac{n \eps^{\frac{2t-1}{t-1}}}{n + \eps^{\frac{2t-1}{t-1}}} \cdot n} {\frac{n \eps^{\frac{2t-1}{t-1}}}{n + \eps^{\frac{2t-1}{t-1}}} \cdot n + \frac{n^2 }{n + \eps^{\frac{2t-1}{t-1}}} \cdot n^{1-1/t} \eps^{1/(t-1)}}\\
& =  \frac{n^2 \eps^{\frac{2t-1}{t-1}}}{n^2 \eps^{\frac{2t-1}{t-1}} + n^2 \cdot n^{1-1/t} \cdot \eps^{\frac{1}{t-1}}}\\
& =  \frac{1}{1+\frac{n^{1-1/t}}{\eps^2}}
\end{align*}

Let $p=\frac{1}{1+\frac{n^{1-1/t}}{\eps^2}}$. The lower bound on the sample complexity for this instance distinguishing problem is given as the number of samples required to observe a \textit{success} with probability at least $(1-\delta)$ while drawing independent samples from $Geom(p)$. Here, $Geom(p)$ denotes a geometric distribution with success probability $p$. The number of samples required to observe one \textit{success} from $Geom(p)$ with probability at least $(1-\delta)$ is at least $\Omega(\frac{\ln 1/\delta}{p})$. Therefore, $\Omega(\frac{n^{1-1/t}\ln1/\delta}{\eps^2})$ samples are required to distinguish these two instances with probability at least $1-\delta$.

\end{proof}

\section{Estimation of Moments using Hybrid Sampling}\label{sec:lower-hybrid}

In this section we prove a lower bound showing that for the moment estimation problem, the hybrid sampling framework does not provide any significant advantage over access to just the proportional sampling oracle. In contrast, note that for the sum estimation problem, hybrid sampling-based algorithms in fact give much better sample complexity bounds over proportional sampling \cite{MPX2007,BT2022}. We prove the following result.

\begin{thm} For any $\eps,\delta>0$ and $t>1$, any algorithm having access to a hybrid sampling oracle requires to make at least $\Omega(\frac{n^{1-1/t}\ln 1/\delta}{\eps^2})$ queries to compute an $(\eps,\delta)$-estimate for $S_t$. \end{thm}

We show that the lower bound instance described in Section \ref{sec:lower-proportional} yields a lower bound for the hybrid sampling as well. In order to distinguish these instances, one is required to sample an element of weight $n$. We have seen that using just proportional sampling $\Omega(\frac{n^{1-1/t} \log 1/\delta}{\eps^2})$ samples are required. The probability of sampling an element of weight $d_2$ using uniform sampling is given as $\frac{n_2}{n_1+n_2}$. This probability using the values of the parameters from Section \ref{sec:lower-proportional} equals $\frac{n_2}{n_1+n_2} = \frac{1}{1+\frac{n}{\eps^{\frac{2t-1}{t-1}}}}$. The instances are distinguished if an element of weight $n$ is sampled using either proportional sampling or uniform sampling. These two probabilities are given as $\frac{1}{1+\frac{n^{1-1/t}}{\eps^2}}$ and $\frac{1}{1+\frac{n}{\eps^{\frac{2t-1}{t-1}}}}$, respectively. Overall, we get a lower bound of $\Omega(\min\{\frac{n^{1-1/t}}{\eps^2}, \frac{n}{\eps^{\frac{2t-1}{t-1}}}\}\ln 1/\delta)=\Omega(\frac{n^{1-1/t}\ln 1/\delta}{\eps^2})$ for the $(\eps,\delta)$ moment estimation problem using hybrid sampling.
