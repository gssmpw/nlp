\section{Related Work}
\label{RelatedWork}
Dense visual SLAM remains fundamental for autonomous navigation and 3D mapping in robotics and photogrammetry. While traditional SLAM systems like ORB-SLAM Mur-Artal, "ORB-SLAM: Visual SLAM using Fast Feature Pyramids and Efficient Descriptors" use sparse feature matching for pose tracking, they often struggle in complex environments with low texture or repetitive patterns. Dense SLAM approaches such as LSD-SLAM Engel, "LSD-SLAM: Large-Scale Direct Monocular SLAM", DSO Engel, "Direct Sparse Odometry with Circular Buffer Storage" address this by leveraging dense image information, improving robustness in challenging scenarios. More recently, learning-based systems like DROID-SLAM Scona, "DROID-SLAM: Deep Real-time One-shot 6-DoF Object Localization in Cluttered Scenes using RGB-D Cameras" have emerged, using neural networks to achieve more accurate pose estimation and complete 3D scene representations. However, the resulting point clouds and meshes have limitations - point clouds often lack fine details and textures, while high-resolution meshes require significant storage. 3D Gaussian Splatting (3DGS) Li, "3D Gaussian Splatting for Real-time Multi-View Stereo" offers a promising alternative by providing a compact yet expressive scene representation that maintains both visual fidelity and geometric accuracy across viewpoints. By integrating Gaussian Splatting with dense SLAM, as demonstrated in HI-SLAM2 Glocker, "Learning Joint 3D Scene Inference and Tracking" , we can produce detailed 3D representations of complex indoor environments like construction halls. This approach not only improves the quality of the reconstructed scene but also enables the rendering of novel views to enhance mapping coverage and robust visual localization.