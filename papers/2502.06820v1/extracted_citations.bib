@article{chen2023parameter,
  title={Parameter-efficient fine-tuning design spaces},
  author={Chen, Jiaao and Zhang, Aston and Shi, Xingjian and Li, Mu and Smola, Alex and Yang, Diyi},
  journal={arXiv preprint arXiv:2301.01821},
  year={2023}
}

@article{gao2024parameter,
  title={Parameter-Efficient Fine-Tuning with Discrete Fourier Transform},
  author={Gao, Ziqi and Wang, Qichao and Chen, Aochuan and Liu, Zijing and Wu, Bingzhe and Chen, Liang and Li, Jia},
  journal={arXiv preprint arXiv:2405.03003},
  year={2024}
}

@article{hao2024flora,
  title={Flora: Low-Rank Adapters Are Secretly Gradient Compressors},
  author={Hao, Yongchang and Cao, Yanshuai and Mou, Lili},
  journal={arXiv preprint arXiv:2402.03293},
  year={2024}
}

@inproceedings{houlsby2019parameter,
  title={Parameter-efficient transfer learning for NLP},
  author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle={International conference on machine learning},
  pages={2790--2799},
  year={2019},
  organization={PMLR}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@article{lawton2023neural,
  title={Neural architecture search for parameter-efficient fine-tuning of large pre-trained language models},
  author={Lawton, Neal and Kumar, Anoop and Thattai, Govind and Galstyan, Aram and Steeg, Greg Ver},
  journal={arXiv preprint arXiv:2305.16597},
  year={2023}
}

@article{liu2024dora,
  title={Dora: Weight-decomposed low-rank adaptation},
  author={Liu, Shih-Yang and Wang, Chien-Yi and Yin, Hongxu and Molchanov, Pavlo and Wang, Yu-Chiang Frank and Cheng, Kwang-Ting and Chen, Min-Hung},
  journal={arXiv preprint arXiv:2402.09353},
  year={2024}
}

@article{ruckle2020adapterdrop,
  title={Adapterdrop: On the efficiency of adapters in transformers},
  author={R{\"u}ckl{\'e}, Andreas and Geigle, Gregor and Glockner, Max and Beck, Tilman and Pfeiffer, Jonas and Reimers, Nils and Gurevych, Iryna},
  journal={arXiv preprint arXiv:2010.11918},
  year={2020}
}

@article{wallace1991jpeg,
  title={The JPEG still picture compression standard},
  author={Wallace, Gregory K},
  journal={Communications of the ACM},
  volume={34},
  number={4},
  pages={30--44},
  year={1991},
  publisher={AcM New York, NY, USA}
}

@article{zaken2021bitfit,
  title={Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models},
  author={Zaken, Elad Ben and Ravfogel, Shauli and Goldberg, Yoav},
  journal={arXiv preprint arXiv:2106.10199},
  year={2021}
}

@article{zhang2023adalora,
  title={AdaLoRA: Adaptive budget allocation for parameter-efficient fine-tuning},
  author={Zhang, Qingru and Chen, Minshuo and Bukharin, Alexander and Karampatziakis, Nikos and He, Pengcheng and Cheng, Yu and Chen, Weizhu and Zhao, Tuo},
  journal={arXiv preprint arXiv:2303.10512},
  year={2023}
}

