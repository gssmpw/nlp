
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{amsmath, amsthm, amssymb, appendix, bm, graphicx, hyperref, mathrsfs, makecell}
\usepackage{bbm, stfloats, subfig, pythonhighlight, CJK, algorithm, algorithmicx, algpseudocode,booktabs,graphicx,multirow,wrapfig,lipsum}
\usepackage{url}
\usepackage{marvosym}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}


\title{LoCA: Location-Aware Cosine Adaptation for Parameter-Efficient Fine-Tuning}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Zhekai Du$^{\dagger,\ddagger}$\thanks{This work was done when Zhekai Du was a visiting student at The University of Melbourne.}, \ Yinjie Min$^\diamond$, \ Jingjing Li$^\dagger$\textsuperscript{\Letter}, \ Ke Lu$^\dagger$, Changliang Zou$^\diamond$, Liuhua Peng$^\ddagger$\\
\textbf{Tingjin Chu$^\ddagger$}, \ \textbf{Mingming Gong$^{\ddagger, \star}$}\\
$^\dagger$ University of Electronic Science and Technology of China \ \ 
$^\ddagger$ The University of Melbourne \\
$^\diamond$ Nankai University \ \
$^\star$ Mohamed bin Zayed University of Artificial Intelligence \\
\texttt{\{zhekaid, jjl, kel\}@uestc.edu.cn, \{nk.yjmin, nk.chlzou\}@gmail.com} \\
\texttt{\{liuhua.peng, tingjin.chu, mingming.gong\}@unimelb.edu.au}
% \texttt{\{qingru.zhang,abukharin3,tourzhao\}@gatech.edu} \\
% \texttt{mc0750@princeton.edu} \\
% \texttt{\{nikosk,penhe,yu.cheng,wzchen\}@microsoft.com}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}

\maketitle

\begin{abstract}
   % Low-rank adaptation (LoRA) has become a prevalent method for adapting pre-trained large language models to downstream tasks. However, the low-rank decomposition form may limit optimization flexibility. To address this, a recent frequency-domain technique has demonstrated empirical success by parameterizing weight updates using inverse Discrete Fourier Transform (iDFT). Nevertheless, it remains unclear whether frequency-domain decomposition is superior to low-rank decomposition for parameter-efficient fine-tuning (PEFT).
   % In this paper, we answer this question through a comprehensive theoretical analysis of frequency-domain PEFT. Our findings reveal that iDFT-based methods with randomly selected components are less expressive than LoRA. However, we show that variants with carefully selected frequency components can surpass LoRA. Furthermore, we prove that the optimal iDFT-based variant can be equivalently implemented using an inverse Discrete Cosine Transform (iDCT) approach, which allows for more efficient selection and tuning of frequency components.
   % Drawing on these insights, we introduce Location-aware Cosine Adaptation (LoCA), a novel iDCT-based PEFT method that optimizes both the coefficients and locations of frequency components. We employ finite-difference approximation to estimate gradients for discrete location variables, enabling dynamic selection of the most informative frequency components.
   % Extensive experiments across various natural language processing and computer vision tasks demonstrate that LoCA outperforms existing PEFT methods, offering enhanced parameter efficiency while maintaining computational feasibility comparable to low-rank methods.

   Low-rank adaptation (LoRA) has become a prevalent method for adapting pre-trained large language models to downstream tasks. However, the simple low-rank decomposition form may constrain the hypothesis space. To address this limitation, we introduce Location-aware Cosine Adaptation (LoCA), a novel frequency-domain parameter-efficient fine-tuning method based on inverse Discrete Cosine Transform (iDCT) with selective locations of learnable components. We begin with a comprehensive theoretical comparison between frequency-domain and low-rank decompositions for fine-tuning pre-trained large models. Our analysis reveals that frequency-domain approximation with carefully selected frequency components can surpass the expressivity of traditional low-rank-based methods. Furthermore, we demonstrate that iDCT offers a more efficient implementation compared to inverse Discrete Fourier Transform (iDFT), allowing for better selection and tuning of frequency components while maintaining equivalent expressivity to the optimal iDFT-based adaptation. By employing finite-difference approximation to estimate gradients for discrete locations of learnable coefficients on the DCT spectrum, LoCA dynamically selects the most informative frequency components during training. Experiments on diverse language and vision fine-tuning tasks demonstrate that LoCA offers enhanced parameter efficiency while maintains computational feasibility comparable to low-rank-based methods.
   
   
   
   \end{abstract}
   
   \section{Introduction}
   Pre-trained large language models (LLMs) \citep{radford2019language,liu2019roberta,brown2020language} have shown strong capabilities in learning language knowledge and adapting to various natural language processing (NLP) tasks through fine-tuning (FT). This FT paradigm has extended to vision \citep{dosovitskiy2020image,liu2021swin} and multi-modal domains \citep{radford2021learning,li2022blip}, leveraging the Transformer architecture \citep{vaswani2017attention}. However, as models grow larger, fine-tuning the entire model becomes too costly for practical use.
   
   % To address this challenge, various Parameter-Efficient Fine-Tuning (PEFT) methods \citep{houlsby2019parameter} have been developed to reduce trainable parameters while maintaining performance comparable to full fine-tuning. Among them, {\it adapter-based} methods \citep{hu2023llm,he2021towards} insert small trainable modules into transformer layers. {\it Prompt-based} approaches \citep{lester2021power,wang2023non} prepend learnable vectors to input or hidden states. However, these methods often introduce non-negligible inference overhead due to the additional modules or prompt vectors. {\it Partial fine-tuning} \citep{zaken2021bitfit,xu2021raise} selectively updates a subset of existing model parameters, but they still suffer from suboptimal performance compared to full FT. To address these limitations,
   % Low-Rank Adaptation (LoRA) \citep{hu2021lora} offers an alternative by reparameterizing incremental updates of pre-trained weights using low-rank decomposition. For a pre-trained weight matrix $W_0 \in \mathbb{R}^{p \times q}$ in an attention layer or a feed-forward layer, LoRA approximates fine-tuned weights as $W^{\prime} = W_0 + \Delta W = W_0 + BA$, where $B \in \mathbb{R}^{p \times r}$, $A \in \mathbb{R}^{r \times q}$, and $r \ll \min(p, q)$. During FT, only $A$ and $B$ are updated. This allows LoRA to significantly reduce the number of trainable parameters while still achieving impressive performance.

    To address this challenge, various Parameter-Efficient Fine-Tuning (PEFT) methods \citep{houlsby2019parameter} have been developed. {\it Adapter-based} methods \citep{hu2023llm,he2021towards} insert small trainable modules into Transformer layers. {\it Prompt-based} approaches \citep{lester2021power,wang2023non} prepend learnable vectors to input or hidden states. However, these methods often introduce non-negligible inference overhead. {\it Partial FT} \citep{zaken2021bitfit,xu2021raise} selectively updates a subset of existing model parameters, but they still suffer from suboptimal performance compared to full FT. To address these limitations,
    Low-Rank Adaptation (LoRA) \citep{hu2021lora} offers an alternative by reparameterizing incremental updates of pre-trained weights using low-rank decomposition. For a pre-trained weight matrix $W_0 \in \mathbb{R}^{p \times q}$ in an attention layer or a feed-forward layer, LoRA reparameterizes fine-tuned weights as $W^{\prime} = W_0 + \Delta W = W_0 + BA$, where $B \in \mathbb{R}^{p \times r}$, $A \in \mathbb{R}^{r \times q}$, and $r \ll \min(p, q)$. During FT, only $A$ and $B$ are updated. This allows LoRA to significantly reduce the number of trainable parameters while still achieving impressive performance.
   
   The success of LoRA has inspired a series of subsequent work. These LoRA variants typically aim to better utilize the parameter budget \citep{zhang2023adalora,valipour2022dylora,kopiczko2023vera}, improve computational efficiency \citep{dettmers2024qlora,zhang2023memory,hedegaard2024structured}, enable diverse learning
   patterns \citep{liu2024dora}, or achieve a higher rank \citep{hyeon2021fedpara,edalati2022krona,hao2024flora}. However, they still reparameterize weight update with the low-rank decomposition form, which may limit the \textcolor{black}{hypothesis space} and prevent further parameter reduction. To address this issue, FourierFT \citep{gao2024parameter} proposes to reparameterize $\Delta W$ with a randomly selected set of frequency-domain components by inverse Discrete Fourier Transform (iDFT). This implicitly allows for \textcolor{black}{enhanced expressivity} and flexible parameter budget.
   
   While FourierFT has shown empirical success, its advantages over low-rank methods have not been theoretical analyzed. To fill this gap, we aim to provide a comprehensive understanding of frequency-domain PEFT. We begin with a systematic analysis of weight updates during FT, and identify the asymptotic normality of weight incremental matrices through both empirical observations and theoretical justification. This foundation enables a rigorous mathematical comparison of the expressivity between frequency-domain and low-rank methods. Interestingly, our analysis reveals that iDFT-based methods with randomly selected locations of learnable frequency components exhibit lower expressivity than low-rank methods. In response, we design iDFT-based variants with carefully selected components, which consequently surpass the expressivity of low-rank-based methods. We further demonstrate that the best choice of iDFT-based variants can be equivalently and more efficiently implemented using inverse Discrete Cosine Transform (iDCT).
   
   Building on these insights, we introduce Location-aware Cosine Adaptation (LoCA), an iDCT-based PEFT method that optimizes both the coefficients and locations of frequency components. By employing finite-difference approximation to estimate gradients for discrete location variables, LoCA dynamically selects the most informative frequency components for each weight update matrix. We demonstrate that LoCA offers enhanced parameter efficiency while maintaining computational feasibility comparable to low-rank methods.
   Experiments across various language and vision tasks show that LoCA matches state-of-the-art PEFT performance using significantly fewer parameters.
   
   \section{Preliminary Analysis of Fine-Tuning Modern LLMs} \label{sec:Preliminary_Analysis}
   % Modern LLMs are predominantly built upon the Transformer architecture \citep{vaswani2017attention}, where each Transformer block consists of a multi-head self-attention (MHSA) module and a feed-forward network (FFN). Given an input sequence $x \in \mathbb{R}^{n \times d}$, the MHSA projects $x$ into query, key, and value matrices for each head $h$ using weight matrices $W_q^h, W_k^h, W_v^h \in \mathbb{R}^{d \times d/H}$, respectively, where $H$ is the number of heads. It then computes the attention scores to aggregate the value vectors. The aggregated output is then processed by the FFN, which consists of two fully connected layers parameterized by $W_{f1} \in \mathbb{R}^{d \times d_m}$ and $W_{f2} \in \mathbb{R}^{d_m \times d}$, where $d_m$ is the hidden dimension of the FFN.

    Modern LLMs are predominantly built upon the Transformer architecture  \citep{vaswani2017attention}, where each Transformer block has a multi-head self-attention (MHSA) and a feed-forward network (FFN). For input $x \in \mathbb{R}^{n \times d}$, MHSA projects $x$ into query, key, and value matrices per head $h$ using $W_q^h, W_k^h, W_v^h \in \mathbb{R}^{d \times d/H}$, where $H$ is the number of heads. The FFN then processes the attention output using $W_{f1} \in \mathbb{R}^{d \times d_m}$ and $W_{f2} \in \mathbb{R}^{d_m \times d}$, where $d_m$ is the hidden dimension.
   
   To systematically analyze the behavior of fine-tuning LLMs, we fine-tune a pretrained LLaMA-7b model \citep{touvron2023llama} on the Alpaca-52K dataset \citep{taori2023stanford}. For each fine-tuned weight matrix $W^{'} \in \mathbb{R}^{p \times q}$ $(p \geq q)$, we get the incremental matrix $\Delta W=W^{'}-W_{0}$ and examine its properties from various perspectives. Our empirical observations reveal that the weights in each $\Delta W$ closely approximate a Gaussian distribution (Fig. \ref{fig:analysis_1}). We claim that this normality can be theoretically justified.
   Consider a pre-trained model $f$ with a pre-trained weight matrix $W_0$. 
   Assume the fine-tuning dataset is sampled from $P(X, Y;\overline{W})$, where $\overline{W}$ can be considered as the distribution parameter as well as the oracle solution of fine-tuning, $X$ and $Y$ denote the input data and corresponding labels, respectively.
   During the FT process, we obtain the parameter $W^{\prime}$ by minimizing the empirical loss. Consequently, $W^{\prime}$ can be regarded as an M-estimator of $\overline{W}$, which satisfies $\mathbb{P}_n\psi(W^{\prime})\overset{def.}{=}\mathbb{P}_n\nabla \ell \left[ Y-f(X;W^{\prime}) \right]^2=0$,
   where $\mathbb{P}_n$ is the empirical average over $n$ samples drawn from $P(X, Y;\overline{W})$, $\psi$ is the score function, and $\ell$ is an objective function. Under fairly general conditions, $W^{\prime}-\overline{W}$ is known to be asymptotically normal \citep{yohai1979asymptotic}: $\sqrt{n}\left( W^{\prime}-\overline{W} \right)^V\overset{d.}{\rightarrow}\mathcal{N}_{pq}\left( 0, \Sigma_{\overline{W}} \right)$, where $\cdot^V$ denotes vectorization. We further assert that, under some mild assumptions, the incremental matrix $\Delta W$ also exhibits asymptotic normality.
   % \begin{enumerate}
   % \item[(A1)] For real-world datasets, $\overline{W}^V$ has a prior distribution  $\mathcal{N}_{MN}\left( \overline{W}_0^V,\overline{\sigma}^2I_{MN} \right)$, where $\cdot^V$ denotes vectorization. The pre-training dataset follows the distribution $P(\overline{W}_0)$.
   % \item[(A2)] For any given $\overline{W}$, the M-estimator $\hat{W}$ obtained using $n$ training samples satisfies $\sqrt{n}\left( \hat{W}-\overline{W} \right)^V\overset{d.}{\rightarrow}\mathcal{N}_{MN}\left( 0,\sigma_0^2I_{MN} \right)$. Moreover, $\hat{W}-\overline{W}$ is independent of $\overline{W}$.
   % \end{enumerate}
   
   % To further verify this property, we analyzed the empirical spectral density (ESD) of each $\Delta W$, which is defined as the probability density of the eigenvalues $\{\lambda_i\}_{i=1}^{N}$ of the correlation matrix $\Delta C = \frac{1}{M}\Delta W^T \Delta W \in \mathbb{R}^{N \times N}$. 
   
   
   % According to the Marchenko-Pastur (MP) law \citep{}, as $N, M \rightarrow \infty$ with a fixed aspect ratio $Q = M/N$, the ESD of $\Delta C$ converges almost surely to the MP distribution determined by the element-wise variance $\sigma_{mp}^2$. Therefore, the agreement between the ESD and the MP distribution provides evidence that the matrix exhibits .
   
   
   % Fix any $\overline{W}$, independently sample $n$ samples from $P(\overline{W})$, and obtain estimated parameter $\hat{W}$ by minimizing the empirical loss. $\hat{W}$ satisfies
   % \begin{equation*}
   %     \mathbb{P}_n\psi(\hat{W})\overset{def.}{=}\mathbb{P}_n\nabla\left[ Y-f(X;\hat{W}) \right]^2=0,
   % \end{equation*}
   % where $\mathbb{P}_n$ is the empirical summation over $n$ samples. Then $\hat{W}$ is a M-estimator of $\overline{M}$ and under some conditions and is asymptotically normal. 
   
   % We claim that under some conditions the incremental matrix $\Delta W$ is asymptotically normal. We give two assumptions here, which we'll further justify in appendix.
   % \begin{enumerate}
   %     \item[(A1)] The prior distribution of $\overline{W}^V$ follows $\mathcal{N}_{MN}\left( \overline{W}_0^V,\overline{\sigma}^2I_{MN} \right)$, where $\cdot^V$ means vectorization. The pre-training dataset follows distribution $P(\overline{W}_0)$.
   %     \item[(A2)] The M-estimator $\hat{W}$ of any given $\overline{W}$ with $n$ samples satisfies $\sqrt{n}\left( \hat{W}-\overline{W} \right)^V\overset{d.}{\rightarrow}\mathcal{N}_{MN}\left( 0,\sigma_0^2I_{MN} \right)$, and $\hat{W}-\overline{W}$ is independent of $\overline{W}$.
   % \end{enumerate}
   
   \begin{proposition}\label{normalmatrixprop}
   Let $W_0 \in \mathbb{R}^{K\times K}$ and $W^{\prime} \in \mathbb{R}^{K \times K}$ be the pre-trained weight matrix and fine-tuned weight trained on datasets with $N$ and $n^{\prime}$ data samples, respectively. Assume that
   (A1) The pre-training dataset follows $P(X, Y;\overline{W}_0)$. For real-world fine-tuning datasets, the vectorized $\overline{W}^V$ follows a prior distribution $\mathcal{N}_{K^2}(\overline{W}_0^V, \overline{\sigma}^2 I_{K^2})$, where $\overline{\sigma}$ is a constant.
   (A2) For any given $\overline{W}$, let $W^{\prime}$ be an M-estimator that satisfies asymptotic normality. The elements on $W^{\prime}-\overline{W}$ are asymptotically independent and identically distributed, and the estimation error $W^{\prime}-\overline{W}$ is independent of $\overline{W}$.
   Under these assumptions, there exists $\sigma_0>0$, the weight update matrix $\Delta W = W^{\prime} - W_0$ satisfies:
   \begin{equation*}
   \Delta W^V \sim \mathcal{N}_{K^2}\left(0, \left(\frac{\sigma_0^2}{n^{\prime}} + \overline{\sigma}^2\right)I_{K^2}\right) + o_P\left(\frac{1}{\sqrt{n^{\prime}}}\right) + O_P\left(\frac{1}{\sqrt{N}}\right).
   \end{equation*}
   \end{proposition}
   
   \begin{figure*}[t] 
     \centering 
     \subfloat[Empirical Distribution of $\Delta W$]{
     \includegraphics[width=0.32\linewidth] {Figures//visualization_distribution.pdf} 
     \label{fig:analysis_1}
     }
     \subfloat[Hypothesis Testing]{
     \includegraphics[width=0.33\linewidth] {Figures//pvalue.pdf}
     \label{fig:analysis_2}
     }
     \subfloat[Empirical Spectral Density]{
     \includegraphics[width=0.31\linewidth] {Figures//ESD_layer_4.pdf}
     \label{fig:analysis_3}
     }
     \vspace{-5pt}
     \caption{Analysis of the weight incremental matrices. (a) Empirical distribution of the incremental query ($\Delta W_q$) and value ($\Delta W_v$) projection matrices for a representative middle layer. (b) p-values of the hypothesis test for $\Delta W_q$ and $\Delta W_v$ across different layers. (c) Empirical spectral density (ESD) of $\Delta W_q$ and $\Delta W_v$ for layer 4. Same phenomena are observed in other weight matrices.} 
     \label{fig:spectral_analysis} 
     \vspace{-14pt} 
   \end{figure*}
   
   We justify the reasonability of these assumptions in Appendix \ref{sec:justification}. For ease of representation, we use square matrices for theoretical analysis without loss of generality. Proposition \ref{normalmatrixprop} shows that during FT, the weight update follows an isotropic Gaussian, plus two error terms. In practice, the second term can be assumed to be zero due to the vast amount of pre-training data. However, the last term, which is related to the size of the FT dataset, causes the final distribution to deviate slightly from a Gaussian distribution. To examine the impact of this error term, we design a hypothesis test, where the null hypothesis posits that the total variation (TV) between the distribution of parameters $w \in \Delta W$ and the normal distribution is less than a constant $\varepsilon$, i.e., $H_0: d_{TV}(P(w), \mathcal{N}(w;\hat{\mu}, \hat{\sigma}^2)) \leq \epsilon$, where $d_{TV}(\cdot, \cdot)$ denotes the total variation, $P(w)$ is the true distribution of $w$, $\hat{\mu}$ and $\hat{\sigma}$ are the empirical mean and standard deviation of $w$ respectively. We use the TV between the the empirical distribution of $w$ and $\mathcal{N}(w;\hat{\mu}, \hat{\sigma}^2)$ as the test statistic and employ a bootstrap-like method to estimate its distribution (the details are described in Appendix \ref{sec:hypothesis_test}). Fig. \ref{fig:analysis_2} illustrates the results for $\Delta W_q$ and $\Delta W_v$ across different layers. We choose $\epsilon=0.001$ and significance level 0.05 for this test. The large p-values across all tests in Fig. \ref{fig:analysis_2} mean that the null hypothesis $H_0$ cannot be rejected, i.e., the parameter updates indeed asymptotically follow a Gaussian distribution.
   
   Another observation from Proposition \ref{normalmatrixprop} is that the parameters in $\Delta W$ are asymptotically i.i.d. To examine this, we analyze the empirical spectral density (ESD) of each $\Delta W$, which is defined as the probability density of the eigenvalues $\{\lambda_i\}_{i=1}^{q}$ of the correlation matrix $\Delta C = \frac{1}{p}\Delta W^T \Delta W \in \mathbb{R}^{q \times q}$. ESD is extensively studied in random matrix theory and helps understand the asymptotic behavior of the eigenvalues of large random matrices with i.i.d. elements. According to the Marchenko-Pastur (MP) law \citep{yang2012testing}, as $p, q \rightarrow \infty$ with a fixed aspect ratio $Q = p/q$, the ESD for a random matrix converges to the MP distribution determined by the element-wise variance $\sigma_{mp}^2$. The agreement between the ESD and the MP distribution in Fig. \ref{fig:analysis_3} suggests that $\Delta W$ behaves like an i.i.d. random matrix. This property will help us to better analyze various PEFT methods.
   
   \section{Comparison between frequency-space and low-rank Adaptation}
   Given the asymptotic Gaussian nature of $\Delta W$, we can now analytically compare the expressivities of low-rank-based and frequency-space-based adaptation methods. We regard {\it expressivity} as the ability to approximate a fully fine-tuned weight incremental matrix using the same parameter budget.
   
   Given any $\Delta W \in \mathbb{R}^{p \times q}$ obtained through full fine-tuning, low-rank-based methods approximate it as $\hat{W}_R = BA$ with $N_0 = (p+q)r$ parameters, where $r$ is the chosen rank. In contrast, FourierFT \citep{gao2024parameter} adopts a frequency-domain approach by randomly selecting $N_1$ components on the Fourier spectrum $F=\mathcal{F}(\Delta W)$ to learn, setting others to zero, and approximates $\Delta W$ as $\hat{W}_F^{(1)} = \mathcal{F}^{-1}(\hat{F}^{(1)})$, where $\mathcal{F},\mathcal{F}^{-1}$ denote the FFT and inverse FFT respectively, and $\hat{F}^{(1)} \in \mathbb{C}^{p \times q}$ is the learned spectrum, which has non-zero values at randomly selected locations $Id^{(1)}=\{ id^{(1)}_i=(x^{(1)}_i,y^{(1)}_i)\}_{i=1}^ {N_1}$.
   However, FourierFT only considers learning the real part on $\hat{F}^{(1)}$, and simply discards the imaginary part after the inverse FFT. Besides, it fails to exploit the conjugate symmetry property inherent in the Fourier spectra for real-valued matrices. We argue that this could lead to information loss and inefficient utilization of the parameter budget.
   To address these concerns, we consider a more comprehensive approach that leverages both the real and imaginary parts of the Fourier spectrum while exploiting the conjugate symmetry property. Specifically, we select learnable locations only on the non-redundant half (i.e., the left half) of $F$, and learn both real and imaginary coefficients at these locations. We still denote the result of the improved version as $\hat{W}_F^{(1)}$.
   
   Intuitively, when approximating a matrix through low-rank decomposition, the learned low-rank matrices are effectively the left and right singular matrices corresponding to the largest $r$ singular values of $\Delta W$. However, for frequency-domain methods, this order statistic is not inherently involved. To incorporate this information, we consider an oracle variant that selects $N_2$ locations in the non-redundant half of $\mathcal{F}(\Delta W)$ with the largest amplitude values (the search space is $\Omega_1 = [p] \times [q/2]$), and sets other locations to 0. We denote the resulting sparse Fourier spectrum with optimal locations as $\hat{F}^{(2)}$, yielding $\hat{W}_F^{(2)} = \mathcal{F}^{-1}(\hat{F}^{(2)})$.
   Furthermore, we explore an additional variant leveraging the fact that each location in the Fourier spectrum has a real and an imaginary coefficient, which need not be bound together for selection. We propose selecting $N_3$ learnable coefficients individually with a search space $\Omega_2 = [p] \times [q/2] \times [2]$. In this case, the optimal strategy is to choose the top $N_3$ coefficients with the largest absolute values in the non-redundant half of $\mathcal{F}(\Delta W)$ for learning. Denoting the spectrum with these optimal coefficients as $\hat{F}^{(3)}$, we obtain $\hat{W}_F^{(3)} = \mathcal{F}^{-1}(\hat{F}^{(3)})$.
   We show that, given the asymptotic Gaussian nature of $\Delta W$, we can mathematically compare these PEFT methods. In our theoretical analysis, we account for location indexing within the parameter budget. For a fair comparison with rank $r$ decomposition, we set $N_1 = N_3 = 1/2 N_0$ and $N_2 = 2/3 N_0$ \footnote{A 2D location can be represented by a 1D index given the matrix height $p$ and width $q$.}.
   
   \begin{theorem} \label{theorem_1}
   Let $W \in \mathbb{R}^{K \times K} \sim G$ be a weight matrix where each element independently follows a standard normal distribution $\mathcal{N}(0, 1)$. Define the reconstruction error $L(W, \hat{\mathcal{W}}) =||W - \hat{\mathcal{W}}||_F^2$, where $\hat{\mathcal{W}}$ can be $\hat{W}_R$, $\hat{W}_F^{(1)}$, $\hat{W}_F^{(2)}$, or $\hat{W}_F^{(3)}$ stated above. Then, for $r < K/3$, we have
   \begin{equation*}
       \mathbb{E}_{Id^{(1)}}\mathbb{E}_{W \sim G}[L(W, \hat{W}_F^{(1)})] > \mathbb{E}_{W \sim G}[L(W, \hat{W}_R)] > \mathbb{E}_{W \sim G}[L(W, \hat{W}_F^{(2)})] > \mathbb{E}_{W \sim G}[L(W, \hat{W}_F^{(3)})].
   \end{equation*}
   \end{theorem}
   
   Note that we use $\mathcal{N}(0,1)$ in Theorem \ref{theorem_1} without loss of generality, as any matrix can be rescaled to have zero mean and unit variance. Importantly, Theorem \ref{theorem_1} shows that randomly selecting learnable coefficients in the frequency domain, i.e., $\hat{W}_F^{(1)}$, has worse expressivity than all other method, highlighting the importance of strategic selection of frequency components. On the other hand, the superior performance of $\hat{W}_F^{(3)}$, which allows for individual selection of (real or imaginary) coefficients, indicates that this increased flexibility in frequency component selection can lead to better approximations. These findings have significant implications for the design of PEFT methods.
   
   % that approximates it by inverse Fast Fourier Transform (iFFT) with $N_1$ randomly located non-zero elements on the Fourier matrix $F_L \in \mathbb{R}^{p \times q \times 2}$, i.e., $\Delta \hat{W} = \mathcal{F}^{-1}(F_L)$, where $\mathcal{F}^{-1}$ is the inverse discrete Fourier operator. 
   
   \section{Location-aware Cosine Adaptation}
   \subsection{Problem Formulation}
   In this work, we regard the goal of PEFT as effectively reparameterizing a weight incremental matrix. Building on our previous analysis, we aim to propose a frequency-domain PEFT method that considers both the coefficients and locations of frequency components. Formally, given a pre-trained weight matrix $W_0 \in \mathbb{R}^{p \times q}$, our objective is to fine-tune it on a specific dataset to obtain the fine-tuned weight matrix $W^{\prime} = W_0 + \Delta W = W_0 + \alpha \mathcal{F}^{-1}(\mathcal{S}(\bm{a}, \bm{l}, \bm{k}))$, where $\alpha$ is a scaling coefficient, $\bm{a}=\{a_i\}_{i=1}^{\mathcal{B}}$ represents the learnable coefficients, $\bm{l}=\{(l_i^1,l_i^2)\}_{i=1}^\mathcal{B}$ stores the component locations, $\bm{k}=\{0,1\}^{\mathcal{B}}$ indicates real (1) or imaginary (0) coefficients, $\mathcal{B}$ is the component budget, and $\mathcal{S}(\cdot)$ is an operator that scatters $\bm{a}$ onto a zero matrix according to $\bm{l}$ and $\bm{k}$.
   
   However, its practical implementation presents significant challenges, primarily due to the requirement for extensive discrete optimization of $\bm{l}$ and $\bm{k}$. This motivates our exploration of alternative formulations that balance the benefits of frequency-space adaptation with computational feasibility.
   
   \subsection{Inverse Discrete Cosine Transform-based Reparameterization} \label{sec:idct_para}
   Individually selecting learnable coefficients requires deciding whether to learn the real or imaginary part on each location in $\bm{l}$, which involves extensive discrete optimization of $\bm{k}$ in practical implementation. To address this issue, we introduce the discrete cosine transform (DCT). We prove that in this problem, individually selecting learnable coefficients on the Fourier spectrum is equivalent to selecting locations on the DCT spectrum, which involves only real-valued coefficients.
   
   \begin{theorem} \label{theorem2}
   Let $W \in \mathbb{R}^{K \times K} \sim G$ be a weight matrix where each element independently follows a standard normal distribution $\mathcal{N}(0, 1)$. Let $\mathcal{D}(\cdot)$ and $\mathcal{D}^{-1}(\cdot)$ denote the discrete cosine transform (DCT) and inverse DCT, respectively, and $\mathcal{F}(\cdot)$ denote the discrete Fourier transform. Define $F_D$ as the sparse matrix that preserves the $N_D$ coefficients with the largest absolute values on $\mathcal{D}(W)$ and sets others to 0. With $\hat{W}_D = \mathcal{D}^{-1}(F_D)$, and $L(\cdot, \cdot)$, $N_3$, $\hat{W}_{F}^{(3)}$ stated above, if $N_D = N_3$, then:
   $$E_{W \sim G} [L(W, \hat{W}_F^{(3)})] = E_{W \sim G} [L(W, \hat{W}_D)].$$
   \end{theorem}
   Theorem \ref{theorem2} guides us towards a more efficient alternative by utilizing the iDCT instead of the iDFT. By reparameterizing $\Delta W$ using iDCT, We can maintain the equivalent expressivity while avoiding the optimization of $\bm{k}$. This is because DCT operates in the real domain, which simplifies computations and reduces the complexity of parameter selection. It is known that iDCT is essentially a linear transformation \citep{ahmed1974discrete}. We can express the reparameterization based on 2D iDCT by
   \begin{equation} \label{eq:original_dct}
   W^{\prime} = W_0 + \Delta W = W_0 + \alpha [C^{T} \mathcal{S}(\bm{a}, \bm{l}, \bm{1}) D],
   \end{equation}
   where $C \in \mathbb{R}^{p \times p}$, $D \in \mathbb{R}^{q \times q}$ are the DCT matrices. The elements of $C$ are defined as:
   \begin{equation}
   C_{ij} = \sqrt{\frac{2}{p}} \cdot k_i \cdot \cos\left(\frac{\pi(2j+1)i}{2p}\right),
   \text{where}~
   k_i = \begin{cases}
   \frac{1}{\sqrt{2}}, & \text{if } i = 0 \\
   1, & \text{if } i > 0.
   \end{cases}
   \end{equation}
   The formulation is similar for $D$. In practice, when $\mathcal{S}(\bm{a}, \bm{l}, \bm{1})$ is highly sparse, we can further simplify the computation by
   % \begin{equation}
   % $
   % \Delta W = \alpha[C^T \mathcal{S}(\bm{a},\bm{l},\bm{1}) D] = \alpha \sum_{i=1}^\mathcal{B} a_i \mathbf{c}{l^1_i} \mathbf{d}{l^2_i}^T
   % $
   % \end{equation}
   % where $\mathbf{c}{l^1_i}$ is the $l^1_i$-th column of $C^T$, and $\mathbf{d}{l^2_i}$ is the $l^2_i$-th row of $D$. This simplification reduces the computation complexity of iDCT from $O(p^2q^2)$ to $O(\mathcal{B}pq)$. In contrast, when a large number of frequency components are needed, it is recommended to use the fast DCT algorithm with an asymptotic complexity of $O(log(pq)pq)$. A
   % detailed discussion of computation complexity can be found in Appendix \ref{sec:computation}.
   $
   \Delta W = \alpha[C^T \mathcal{S}(\bm{a},\bm{l},\bm{1}) D] = \alpha \sum_{i=1}^\mathcal{B} a_i C_{l^1_i\cdot}^T D_{l^2_i\cdot}
   $,
   where $C_{l^1_i\cdot}$ is the $l^1_i$-th row of $C$, and $D_{l^2_i\cdot}$ is the $l^2_i$-th row of $D$. This simplification reduces the computation complexity of iDCT from $O(p^2q^2)$ to $O(\mathcal{B}pq)$. In contrast, when more frequency components are needed, it is recommended to use the fast DCT algorithm with an asymptotic complexity of $O(log(pq)pq)$. A
   detailed discussion of computation complexity can be found in Appendix \ref{sec:computation}. Noting that we can pre-generate $C$ and $D$ with only one global copy, which does not consume additional memory usage.
   
   \subsection{Estimating location gradient using finite-difference approximation} \label{sec:gradient_estimate}
   While the coefficients $\bm{a}$ can be directly optimized through backpropagation, the operation $\mathcal{S}(\cdot)$ does not produce gradients with respect to the locations $\bm{l}$. Furthermore, $\bm{l}$ needs to be treated as a discrete variable, which prevents us from directly learning the locations through backpropagation.
   
   To address this issue, we draw inspiration from the straight-through estimator (STE) \citep{bengio2013estimating}, a technique that allows gradient-based optimization of neural networks with discrete variables by using a surrogate gradient. However, unlike traditional STE that simply bypasses the gradient computation for discrete variables, e.g., the STE used in VQ-VAE \citep{van2017neural}, we estimate their gradients using the central difference approximation, as we elaborate below.
   
   \textbf{Forward Pass.}
   To enable gradient-based learning of location variables, we first redefine the locations $\bm{l}$ as continuous variables. During the forward pass, we discretize $\bm{l}$ by $\hat{\bm{l}} = round(\bm{l}) = \{(\hat{l}_i^1,\hat{l}_i^2)\}_{i=1}^\mathcal{B}$, where $round(\cdot)$ maps each element of $\bm{l}$ to its nearest integer.
   
   \textbf{Backward Pass.}
   During the backward propagation, we estimate the gradient of the loss function $\mathcal{L}$ to each element in $\bm{l}$. For clarity, we take $l_n^1$ and $a_n$ as an example. 
   The location gradient is
   \begin{equation} \label{eq:position_gradient}
   \frac{\partial \mathcal{L}}{\partial l_n^1} = \sum_{i=1}^{p} \sum_{j=1}^{q} \frac{\partial \mathcal{L}}{\partial \Delta W_{ij}} \frac{\partial \Delta W_{ij}}{\partial l_n^1} = tr[(\frac{\partial \mathcal{L}}{\partial \Delta W})^T (\frac{\partial \Delta W}{\partial l_n^1})].
   \end{equation}
   Here, ${\partial \mathcal{L}} /{\partial \Delta W}$ can be obtained directly through backpropagation. The tricky part is how to estimate $\partial \Delta W / \partial l_n^1$. In this work, we choose to use central difference approximation, i.e.,
   \begin{equation}
   \frac{\partial \Delta W}{\partial l_n^1} = \frac{\alpha C^T [\mathcal{S}(a_n, (\hat{l_n^1} + 1, \hat{l_n^2}), 1)-\mathcal{S}(a_n, (\hat{l_n^1} - 1, \hat{l_n^2}), 1)] D}{2}.
   \end{equation}
   For simplicity, we denote $\mathcal{S}(a_n, (\hat{l_n^1} + 1, \hat{l_n^2}), 1)-\mathcal{S}(a_n, (\hat{l_n^1} - 1, \hat{l_n^2}), 1)$ as $\Delta \mathcal{S}$, then Eq. (\ref{eq:position_gradient}) becomes
   \begin{equation}\label{eq:position_gradient2}
   \frac{\partial \mathcal{L}}{\partial l_n^1} = \frac{\alpha}{2} tr[(\frac{\partial \mathcal{L}}{\partial \Delta W})^T  C^T \Delta \mathcal{S} D] = \frac{\alpha}{2} tr[\underbrace{D (\frac{\partial \mathcal{L}}{\partial \Delta W})^T  C^T}_{DCT} \Delta \mathcal{S}].
   \end{equation}
   Eq. (\ref{eq:position_gradient2}) demonstrates that the gradient estimate for $l_n^1$ can be obtained by first applying a DCT to $(\partial \mathcal{L}/ \partial \Delta W)^T$ (we denote the resulting matrix as $Z$), and then multiplying it with $\Delta S$. Note that $\Delta S$ is a matrix with non-zero elements only at locations $(\hat{l_n^1} - 1, \hat{l_n^2})$ and $(\hat{l_n^1} + 1, \hat{l_n^2})$. Therefore, the result of Eq. (\ref{eq:position_gradient2}) can be simplified as $\alpha a_n(Z_{\hat{l_n^2}, \hat{l_n^1} + 1} - Z_{\hat{l_n^2}, \hat{l_n^1} - 1})/2$. Since $Z$ can be reused for computing gradients for all locations $\bm{l}$ and coefficients $\bm{a}$ (the gradient to $\bm{a}$ can also be obtained from $Z$), Eq. (\ref{eq:position_gradient2}) introduces almost no additional computational burden (see Appendix \ref{sec:computation_grad}). 

% \begin{algorithm}[t]
% \caption{SSD Fine-tuning}
% \begin{algorithmic}[1]
% \Require Pre-trained weight $W_0$, dataset $\mathcal{D}$, hyperparameters $\mathcal{B}_s, \mathcal{B}_a, \mathcal{B}_l$, total training iterations $T$
% \Ensure Fine-tuned weight $W'$
% \State Initialize $\bm{a} \leftarrow 0$ and initialize $\bm{l}$ randomly
% \For{$t = 1$ to $T$}
%     \State Sample a mini-batch from $\mathcal{D}$ and compute the loss $\mathcal{L}$
%     \If{$t \leq B_s$}
%         \If{$t \bmod (B_a + B_l) < B_a$}
%             \State Update $a$ by $a \leftarrow a - \eta_a \nabla_a L$
%         \Else
%             \State Update $l$: $l \leftarrow l - \eta_l \frac{\partial L}{\partial l}$ (using central difference)
%         \EndIf
%     \Else
%         \State Update $a$: $a \leftarrow a - \eta_a \nabla_a L$
%     \EndIf
% \EndFor
% \State \Return $W' = W_0 + \alpha[C^T S(a, l, 1)D]$
% \end{algorithmic}
% \end{algorithm}


   \subsection{Alternating Optimization Strategy}
   To effectively optimize both the coefficients $\bm{a}$ and locations $\bm{l}$, we implement an alternating optimization scheme inspired by coordinate ascent methods \citep{wright2015coordinate}, which have shown remarkable efficacy in tackling multi-variable optimization problems. Specifically, we initially train the coefficients $\bm{a}$ for $B_a$ steps while maintaining fixed locations $\bm{l}$. Subsequently, we fix $\bm{a}$ and optimize the locations $\bm{l}$ for $B_l$ steps. This alternating process continues for totally $\mathcal{B}_s$ iterations. After that, we only optimize the coefficients $\bm{a}$ until convergence. This strategy facilitates an efficient exploration of the frequency domain while progressively refining the selected components in the early training state, while focusing on the coefficients of the identified important frequency components in the remaining stage. A detailed training procedure can be found in Appendix \ref{sec:algorithm}.
   
   
   % \subsection{Training Procedure}
   % To balance the learning of both position and coefficient variables while ensuring training stability, we adopt a two-phase training procedure. During the first $J$\% of the total FT iterations, we jointly update both the position variables $\bm{l}$ and the coefficient variables $\bm{a}$. This allows the model to explore the frequency space and find optimal component locations.
   % However, continuously updating locations throughout the entire training process can lead to instability, particularly in later stages when the model should be refining its predictions rather than drastically changing its structure. Therefore, for the remaining iterations, we fix the position variables $\bm{l}$ and only update the coefficients $\bm{a}$.
   
   % \subsection{Computational Complexity}
   % Based on above formulation, it is obvious that our method has a cmplexity of $O(\mathcal{B}pq)$. In contrast, the computational complexity of LoRA is $O(rpq)$. From an asymptotic analysis perspective, the complexities of SSD and LoRA are comparable. However, it is important to note that $\mathcal{B}$ is typically much larger than $r$, which suggests that SSD may has a higher computational cost. In our experimental comparisons, we observed that our method is approximately 10\% slower than LoRA. This difference is less significant than the asymptotic analysis might suggest, likely due to implementation details and hardware-specific optimizations. It is worth noting that our current implementation is based on a naive iDCT. We believe there is potential for further optimization by leveraging fast DCT algorithms, which we leave as future work.
   
   \section{Experiments}
   We mainly evaluate LoCA across four domains: natural language understanding (NLU), \textcolor{black}{natural language generation (NLG)}, instruction tuning, and computer vision. For NLU tasks, we fine-tune RoBERTa models on the GLUE benchmark \citep{wang2018glue}. For NLG, we fine-tune GPT-2 (medium/large) on  E2E NLG Challenge. For instruction tuning, we fine-tune LLaMA-family models on the Alpaca-52K dataset \citep{taori2023stanford} and evaluate them on the MT-Bench \citep{zheng2024judging} and Vicuna \citep{chiang2023vicuna} datasets. 
   %We also compare the instruction tuning performance on the MathInstruct Benchmark \cite{yue2023mammoth}. 
   For vision tasks, we fine-tune Vision Transformer (ViT) models on 8 classification datasets. More experiments can be found in Appendix.
   
   
   
   % $$
   % \frac{\partial \mathcal{L}}{\partial l_i^1} = \frac{\partial \mathcal{L}}{\partial \Delta W} \cdot \frac{\partial \Delta W}{\partial \bm{l}} = \frac{\partial \mathcal{L}}{\partial \Delta W} \cdot \frac{\alpha C^T [\mathcal{S}(a_i, l_i^1 + 1, l_i^2)-\mathcal{S}(a_i, l_i^1 - 1, l_i^2)] D}{2}
   % $$  
   {\bf Implementation Details.} We implement our method using the PyTorch framework. Our code is built on the PEFT library \citep{peft} from Huggingface, and all pre-trained models are sourced from Huggingface's Transformers library \citep{transformers}. For the alternating optimization, we used $\mathcal{B}_a$ = 10 and $\mathcal{B}_l$ = 20. The coefficients $\bm{a}$ are initialized to be zeros and the locations $\bm{l}$ are randomly initialized with a uniform distribution. We scale $\bm{l}$ to the range [0, 1] for optimization. All PEFT experiments are conducted on a single NVIDIA Tesla H100 GPU.
   Noting that while LoCA initially optimizes both $\bm{a}$ and $\bm{l}$, the locations are fixed after $\mathcal{B}_s$ iterations. Therefore, the reported number of trainable parameters only includes the final coefficient parameters.
   
   \textcolor{black}{{\bf Baseline Methods.} We compare our LoCA with \textit{Full fine-tuning} (FF), \textit{BitFit} \citep{zaken2021bitfit}, \textit{Adapter-based methods} \citep{houlsby2019parameter}, \textit{LoRA} \citep{hu2021lora}, \textit{AdaLoRA} \citep{zhang2023adalora}, \textit{VeRA} \citep{kopiczko2023vera} , \textit{DoRA} \citep{liu2024dora} and \textit{FourierFT} \citep{gao2024parameter}}.
    
   %  $\bullet$
   %  \textit{Full fine-tuning} (FF) updates all parameters of the pre-trained model during the fine-tuning process, allowing for comprehensive adaptation at the cost of significant computational resources.
    
   %  $\bullet$
   %  \textit{BitFit} \citep{zaken2021bitfit} solely fine-tunes the bias weights while keeping other parameters frozen.

   %  $\bullet$
   %  \textit{Adapter-based methods} inject extra trainable modules into pre-trained models and keep the original model parameters frozen. In our comparison, we primarily focused on two types of Adapters: Adapter\textsuperscript{H} \citep{houlsby2019parameter}, which inserts a two-layer adapter between the self-attention module (or the FFN module) and the subsequent residual connections, and Adapter\textsuperscript{D} \citep{ruckle2020adapterdrop} that further enhances efficiency by strategically dropping inactive adapter layers.

   %  $\bullet$
   %  \textit{LoRA} \citep{hu2021lora} reparameterizes $\Delta W$ using two trainable low-rank matrices. Therefore, the number of trainable parameters is controlled by the chosen rank and the shape of weight matrixs.

   %  $\bullet$
   %  \textit{AdaLoRA} \citep{zhang2023adalora} extends LoRA by introducing an adaptive mechanism to dynamically allocate the rank budget across different parameter matrices.
    
   % $\bullet$
   % \textit{DoRA} \citep{liu2024dora} is a LoRA variant that decomposes pre-trained weights into magnitude and direction components for fine-tuning. It demonstrates learning patterns closer to full fine-tuning.

   % $\bullet$
   % \textit{FourierFT} \citep{gao2024parameter} treats weight changes as spatial-domain matrices and reparameterizes them with a set of learnable frequency components. The number of trainable parameters is controlled by the number of frequency components, allowing for more flexible scaling of parameter budgets.
   
   \begin{table}[t]
   \centering
   \caption{Fine-tuning results with RoBERTa-base/large on the GLUE benchmark. We report the overall accuracy (matched and mismatched) for MNLI, Matthew's correlation coefficient (MCC) for CoLA and use the Pearson correlation coefficient (PCC) for STS-B. Accuracy (Acc.) is reported for all other tasks. $\dagger, \ddagger, *$ denote values from prior works. 
   %Values in parentheses indicate performance when the parameter budget matches LoRA. 
   Best results are shown in \textbf{bold}.}
   \vspace{-5pt}
   \label{tab:glue_results}
   \resizebox{0.9\linewidth}{!}{
   \begin{tabular}{c|l|r|ccccccccc}
   \toprule
   {\bf Model} & {\bf FT Method}  &  {\bf Param.}   & {\begin{tabular}[c]{@{}c@{}} {\bf CoLA} \\ MCC\end{tabular}} & {\begin{tabular}[c]{@{}c@{}} {\bf MNLI} \\ Acc\end{tabular}} & {\begin{tabular}[c]{@{}c@{}} {\bf MRPC} \\ Acc\end{tabular}} & {\begin{tabular}[c]{@{}c@{}} {\bf QNLI} \\ Acc\end{tabular}} & {\begin{tabular}[c]{@{}c@{}} {\bf QQP} \\ Acc\end{tabular}}  & {\begin{tabular}[c]{@{}c@{}} {\bf RTE} \\ Acc\end{tabular}}  & {\begin{tabular}[c]{@{}c@{}} {\bf SST-2} \\ Acc\end{tabular}} & {\begin{tabular}[c]{@{}c@{}} {\bf STS-B} \\ PCC\end{tabular}} & {\begin{tabular}[c]{@{}c@{}} {\bf All} \\ Avg.\end{tabular}}    \\ \midrule \midrule
         & FT $\ddagger$  & 125M        & 63.6 & {\bf 87.6} & 90.2 & 92.8 & {\bf 91.9} & 78.7 & 94.8 & 91.2  & {\bf 86.4}    \\
   \multirow{6}{*}{\rotatebox[origin=c]{90}{RoBERTa-base}}  & BitFit $\ddagger$ & 0.1M       & 62.0   & 84.7 & {\bf 92.7} & 91.8 & 84.0 & {\bf 81.5} & 93.7 & 90.8  & 85.2  \\
         & Adapter\textsuperscript{D} $\ddagger$  & 0.9M   & 62.6 & 87.3 & 88.4 & 93.0   & 90.6 & 75.9 & 94.7 & 90.3  & 85.4   \\
         & LoRA   & 0.3M  & 62.8 & 86.6 & 89.7 & 93.3 & 90.8 & 79.3 & 94.9 & 91.4  & 86.1  \\
         & AdaLoRA  & 0.3M & 63.0   & 86.8  & 90.2 & {\bf 93.4} & 90.9 & 80.4   & 94.6 & 90.9  & 86.3 \\
         & DoRA &  0.31M   & 63.5 & 87.0   & 90.2 & 93.1 & 91.4 & 78.6 & {\bf 95.2} & {\bf 91.5}  & 86.3  \\
         & \textcolor{black}{VeRA} $\dagger$ & \textcolor{black}{0.043M}   & \textcolor{black}{{\bf 65.6}} & \textcolor{black}{85.1}   & \textcolor{black}{89.5} & \textcolor{black}{91.8} & \textcolor{black}{89.6} & \textcolor{black}{78.7} & \textcolor{black}{94.6} & \textcolor{black}{90.7}  & \textcolor{black}{85.7}  \\
         & FourierFT $^*$ & 0.024M  & 63.8 & 84.9 & 90.0   & 92.2 & 88.2 & 79.1 & 94.2 & 90.8  & 85.4  \\
         & {\bf LoCA}    &  0.024M      & 64.5 & 85.2 & 90.5 & 92.0 & 88.7 & {\bf 81.5} & 94.6 & 90.9  & 86.0 \\ \midrule
   \multirow{7}{*}{\rotatebox[origin=c]{90}{RoBERTa-large}} & FT $\ddagger$  & 355M          & 68.0   & 90.2 & 90.9 & 94.7 & {\bf 92.2}   & 86.6 & {\bf 96.4} & 92.4  & {\bf 88.9}    \\
         & Adapter\textsuperscript{H} $\ddagger$  & 6M  & 66.5 & 89.9 & 88.7 & 94.7 & 92.1   & 83.4 & 96.2 & 91.0    & 87.8  \\
         & LoRA   &   0.8M    & 68.4 & 90.5 & 90.2 & 94.4 & 91.6 & 85.7 & 96.2 & 92.4  & 88.7   \\
         & AdaLoRA  & 0.8M    & 67.9 & {\bf 90.6} & 90.6 & 94.2 & 91.6 & 86.4 & 95.9 & {\bf 92.7}  & 88.7   \\
         & DoRA   & 0.83M  & 68.3 & 90.5 & 90.7 & {\bf 94.8} & 91.8 & 85.4 & 96.3 & 92.4  & 88.8 \\
         & \textcolor{black}{VeRA} $\dagger$  & \textcolor{black}{0.061M}  & \textcolor{black}{68.0} & \textcolor{black}{90.2} & \textcolor{black}{90.9} & \textcolor{black}{94.4} & \textcolor{black}{90.3} & \textcolor{black}{85.9} & \textcolor{black}{96.1} & \textcolor{black}{91.7}  & \textcolor{black}{88.4} \\
         & FourierFT $^*$  & 0.048M & 67.1 & 88.9 & 90.9 & 94.4 & 89.2 & 87.4 & 96.0   & 91.9  & 88.2   \\
         & {\bf LoCA}    & 0.048M       & {\bf 68.8} & 89.4 & {\bf 91.0} & 94.4 & 90.0 & {\bf 87.9} & {\bf 96.4} & 92.0  & 88.7    \\ \bottomrule
   \end{tabular}}
   \vspace{-15pt}
   \end{table}

    \subsection{Natural Language Understanding} \label{sec:NLU}
    We evaluate our method on NLU tasks using the GLUE benchmark \citep{wang2018glue}, which consists of diverse tasks that cover various aspects of language understanding, including single-sentence classification, similarity and paraphrase, and inference task. For our experiments, we fine-tune RoBERTa-base and RoBERTa-large models \citep{liu2019roberta} on 8 GLUE tasks using different adaptation methods. Following \citet{zhang2023adalora, gao2024parameter}, we report the best results on the validation set for each task. Mean results are reported after 3 runs with different random seeds.

    \textbf{Implementation Details.} For LoRA and its variants, we use a rank $r=8$ and a scaling value $\alpha=8$. To maintain consistency with FourierFT, we set the number of frequency components $\mathcal{B}$ to 1000 for both frequency-domain methods, resulting in significantly less parameters compared to low-rank decomposition methods. Since FourierFT does not report results for the MNLI and QQP tasks, we obtained these results by our own runs with tuned hyperparameters. Following the settings in \citet{hu2021lora,gao2024parameter}, all low-rank decomposition methods and frequency-domain decomposition methods are applied only to the {\it query} and {\it value} matrices, and the best performance on the validation set for each run is recorded. Detailed hyperparameters can be found in Table \ref{tab:glue_hyper}.

    \textbf{Experimental Results.}
    Table \ref{tab:glue_results} presents the results for RoBERTa-base and RoBERTa-large models. Our LoCA achieves competitive average scores of 86.0 and 88.7 respectively, approaching cutting-edge performance while using significantly fewer parameters. LoCA consistently outperforms FourierFT across most tasks despite the same parameter budget, and shows comparable or superior results to LoRA-family methods on several tasks. Notably, LoCA achieves the highest scores on CoLA for both model sizes, surpassing even FF. For challenging tasks (e.g., QQP), we will show in Section \ref{sec:analytical} that if we appropriately increase the parameter budget, the performance of LoCA will improve significantly, which eventually surpasses LoRA with the same parameter budget.

    \subsection{Natural Language Generation} \label{sec:nlg}
    \textcolor{black}{We evaluate LoCA on the E2E NLG Challenge dataset \citep{novikova2017e2e}, a widely-used benchmark for data-to-text generation. The dataset consists of over 50K samples in the restaurant domain, with each input being a set of slot-value pairs and the corresponding output being a natural language description. We conduct experiments on both GPT-2 medium and GPT-2 large.}

    \textcolor{black}{\textbf{Implementation Details.} Following \citet{hu2021lora}, we train our models using AdamW optimizer with a linear learning rate decay schedule for 5 epochs. We set the batch size to 32 and use a label} 
    \begin{wraptable}{r}{0.61\textwidth}
    \centering
    \small
    \vspace{-2mm}
    \caption {\textcolor{black}{Results of tuning GPT-2 Medium/Large models on the E2E benchmark. Higher values indicate better performance for all metrics. $\dagger, \ddagger, *$ denote values from prior works.}}
    \vspace{-5pt}
    \label{tab:NLG}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{@{}c|l|r|ccccc@{}}
    \toprule
    {\bf Model} & {\bf FT Method} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}{\bf Param.}\end{tabular}} & {\bf BLEU} & {\bf NIST} & {\bf METEOR} & {\bf ROUGE-L} & {\bf CIDEr} \\ \midrule
    \multirow{6}{*}{\rotatebox{90}{\begin{tabular}[c]{@{}c@{}} GPT-2\\ Medium\end{tabular}}} & FF*        & 354.92M & 68.2 & 8.62 & 46.2 & 71.0 & 2.47 \\
    % & \multicolumn{1}{l|}{$\text{Adpt}^{\text{L}}$*}         & \multicolumn{1}{r|}{0.37M}   & 66.3 & 8.41 & 45.0 & 69.8 & 2.40 \\
    & \multicolumn{1}{l|}{$\text{Adpt}^{\text{L}}$*}         & \multicolumn{1}{r|}{11.09M}  & 68.9 & 8.71 & 46.1 & 71.3 & 2.47 \\
    & \multicolumn{1}{l|}{$\text{Adpt}^{\text{H}}$*}         & \multicolumn{1}{r|}{11.09M}  & 67.3\textsubscript{$\pm$.6} & 8.5\textsubscript{$\pm$.07} & 46.0\textsubscript{$\pm$.2} & 70.7\textsubscript{$\pm$.2} & 2.44\textsubscript{$\pm$.01} \\
    & \multicolumn{1}{l|}{LoRA $\ddagger$}     & \multicolumn{1}{r|}{0.35M}   & 68.9\textsubscript{$\pm$.3} & 8.76\textsubscript{$\pm$.06} & 46.6\textsubscript{$\pm$.1} & 71.5\textsubscript{$\pm$.1} & \textbf{2.53}\textsubscript{$\pm$.03} \\
    & \multicolumn{1}{l|}{VeRA $\dagger$}     & \multicolumn{1}{r|}{0.098M}   & {\bf 70.1} & 8.81 & 46.6 & 71.5 & 2.50 \\
    & \multicolumn{1}{l|}{FourierFT $\ddagger$} & \multicolumn{1}{r|}{0.048M}   & 69.1\textsubscript{$\pm$.1}  & 8.82 \textsubscript{$\pm$.05} & \textbf{47.0} \textsubscript{$\pm$.3} & 71.8 \textsubscript{$\pm$.1} & 2.51\textsubscript{$\pm$.02} \\
    & \multicolumn{1}{l|}{\textbf{LoCA}} & \multicolumn{1}{r|}{0.048M} & 69.7 \textsubscript{$\pm$.2} & \textbf{8.85} \textsubscript{$\pm$.04} & 46.6 \textsubscript{$\pm$.2} & \textbf{72.1} \textsubscript{$\pm$.3}  & 2.52 \textsubscript{$\pm$.06} \\
    \midrule
    \multirow{5}{*}{\rotatebox{90}{\begin{tabular}[c]{@{}c@{}} GPT-2\\ Large\end{tabular}}} & 
    \multicolumn{1}{l|}{FF*}        & \multicolumn{1}{r|}{774.03M} & 68.5 & 8.78 & 46.0 & 69.9 & 2.45 \\
    % & \multicolumn{1}{l|}{$\text{Adpt}^{\text{L}}$*}         & \multicolumn{1}{r|}{0.88M}   & 69.1\textsubscript{$\pm$.1} & 8.68\textsubscript{$\pm$.03} & 46.3\textsubscript{$\pm$.0} & 71.4\textsubscript{$\pm$.2} & 2.49\textsubscript{$\pm$.0} \\
    & \multicolumn{1}{l|}{$\text{Adpt}^{\text{L}}$*}         & \multicolumn{1}{r|}{23.00M}  & 68.9\textsubscript{$\pm$.3} & 8.70\textsubscript{$\pm$.04} & 46.1\textsubscript{$\pm$.1} & 71.3\textsubscript{$\pm$.2} & 2.45\textsubscript{$\pm$.02} \\
    & \multicolumn{1}{l|}{LoRA $\ddagger$}     & \multicolumn{1}{r|}{0.77M}   & 70.1\textsubscript{$\pm$.3} & 8.83\textsubscript{$\pm$.02} & 46.8\textsubscript{$\pm$.2} & 72.0\textsubscript{$\pm$.3} & 2.47\textsubscript{$\pm$.02} \\
    & \multicolumn{1}{l|}{VeRA $\dagger$}     & \multicolumn{1}{r|}{0.17M}   & 70.3 & 8.85 & 46.9 & 71.6 & {\bf 2.54} \\
    & \multicolumn{1}{l|}{FourierFT $\ddagger$} & \multicolumn{1}{r|}{0.072M} & 70.2\textsubscript{$\pm$.2} & \textbf{8.90}\textsubscript{$\pm$.02} & 47.0\textsubscript{$\pm$.2} & 71.8\textsubscript{$\pm$.1} &  2.50 \textsubscript{$\pm$.02} \\ 
    & \multicolumn{1}{l|}{\textbf{LoCA}} & \multicolumn{1}{r|}{0.072M} & \textbf{70.4} \textsubscript{$\pm$.2}  & 8.88 \textsubscript{$\pm$.05} & \textbf{47.2} \textsubscript{$\pm$.02} & {\bf 72.1} \textsubscript{$\pm$.2} & \textbf{2.54} \textsubscript{$\pm$.02} \\
    \bottomrule
    \end{tabular}}
    \vspace{-5pt}
    \end{wraptable}    
    \textcolor{black}{smoothing factor of 0.1. We only adapt the {\it query} and {\it value} matrices, with 1000 frequency components for both LoCA and FourierFT. See Table \ref{tab:e2e_hyper} for more details.}

    \textcolor{black}{\textbf{Experimental Results.} Table \ref{tab:NLG} shows that LoCA achieves superior performance compared to previous PEFT methods including FourierFT and LoRA across multiple metrics. Specifically, when using GPT-2 large as the base model, LoCA outperforms others on BLEU, METEOR and ROUGE-L scores.}
    
    \subsection{Instruction Tuning}
    %Due to limited space, we only show the FT results of various LLaMA-family models \citep{touvron2023llama,touvron2023llama2} using the Alpaca-52K dataset \citep{taori2023stanford} in this section. 
    We fine-tune various LLaMA-family models \citep{touvron2023llama,touvron2023llama2} using the Alpaca-52K dataset \citep{taori2023stanford}.
    The Alpaca-52K dataset, derived from the self-instruct technique, provides a diverse set of instruction-following examples. In this experiment, we mainly compare our method with FF, LoRA and FourierFT. 
    After fine-tuning, we evaluate the model on the MT-Bench \citep{zheng2024judging} and Vicuna \citep{chiang2023vicuna} datasets, which offer challenging multi-turn and open-ended scenarios for LLM evaluation. We employed GPT-4 to assign scores on a scale of 1-10 based on the quality, relevance, and coherence of the responses. 
    % Experiments on MathInstruct Benchmark can be found in Appendix \ref{}.
    
    {\bf Implementation Details.} We apply all PEFT methods to the {\it query} and {\it value} matrices. For 
    \begin{wraptable}{r}{0.5\textwidth}
    \centering
    \small
    \vspace{-2mm}
    \caption {Evaluation results for fine-tuned LLaMA-family models on MT-Bench and Vicuna datasets, using GPT-4 as the judge with a 1-10 scoring scale. Bold and underlined values indicate the best and second best results, respectively.}
    \label{tab:Instruction}
    \resizebox{0.95\linewidth}{!}{
    \begin{tabular}{l|l|l|cc}
    \toprule
    {\bf Model}                & {\bf FT Method} & {\bf Param.} & {\bf MT-Bench} & {\bf Vicuna} \\ \midrule \midrule
    \multirow{4}{*}{LLaMA1-7b}                  & FF      &    6.8B    & 4.46     &   \underline{7.24}     \\
                               & LoRA      &  33.5M    &    {\bf 4.52}      &  {\bf 7.52}      \\
                               & FourierFT &   9.6M    &   4.33   &   6.97    \\
                               & {\bf LoCA}       &   9.6M     &  \underline{4.47}    &  7.18 \\ \midrule
    \multirow{4}{*}{LLaMA1-13b} & FF        &  13B  &  4.78  &    7.68    \\
                               & LoRA      &   52.4M     &   {\bf 4.87}       &   \underline{7.82}     \\
                               & FourierFT &    12M    &    4.70      &   7.61    \\ 
                               & {\bf LoCA}       &   12M    &   \underline{4.83}    &   {\bf 7.85}    \\ \midrule
    \multirow{4}{*}{LLaMA2-7b} & FF        &    6.8B    &    {\bf 4.94}      & {\bf 7.81}       \\
                               & LoRA      &   33.5M   &   4.67       &    7.68    \\
                               & FourierFT &    9.6M    &   4.65     &   7.62    \\
                               & {\bf LoCA}       &    9.6M    &    \underline{4.82}      &   \underline{7.78}      \\
                               \midrule
    \multirow{4}{*}{LLaMA2-13b} & FF     &   13B     &    {\bf 5.55}   &   {\bf 8.13}     \\
                               & LoRA      &   52.4M     &     5.48     & 8.03        \\
                               & FourierFT &   12M     &    5.37      &   7.95    \\ 
                               & {\bf LoCA}       &   12M     &   \underline{5.52}    &   \underline{8.11}     \\
                               \bottomrule
    \end{tabular}
    } 
    \end{wraptable}
    LoRA, we set the rank $r$ to 64 and the scaling value $\alpha$ to 16. For FourierFT, we use 150K frequency components and tune other hyperparameters to ensure the optimal performance, since we cannot reproduce the results in \citet{gao2024parameter}.
    For LoCA, we also use 150K frequency components, and set the scaling value $\alpha$ to 1. We utilize the {\it LLM-as-a-Judge} repository \citep{zheng2024judging} for fair evaluation. We train LLaMA-1-7b/LLaMA-2-7b for 3 epochs and LLaMA-1-13b/LLaMA-2-13b for 1 epoch. Quantization \citep{dettmers2024qlora} is used for LLaMA-1-13b/LLaMA-2-13b to ensure feasible FT on a single GPU. Detailed hyperparameters can be found in Table \ref{tab:It_hyper}. \\[7pt]
    {\bf Experimental Results.} The results in Table \ref{tab:Instruction} demonstrate the competitive performance of our method across various LLaMA model sizes and architectures. Notably, LoCA consistently outperforms FourierFT and, in many scenarios, either approaches or surpasses the performance of LoRA, despite the latter utilizing a larger parameter budget. This underscores the superior efficiency of LoCA in parameter utilization and its effectiveness in acquiring task-specific knowledge.
    
    \subsection{Image Classification} \label{sec:Vision}
    We evaluate our method on computer vision tasks by conducting experiments on 8 image classification datasets, including OxfordPets \citep{parkhi2012cats}, StanfordCars \citep{krause20133d}, CIFAR10 \citep{krizhevsky2009learning}, DTD \citep{cimpoi2014describing}, EuroSAT \citep{helber2019eurosat}, FGVC \citep{maji2013fine}, RESISC45 \citep{cheng2017remote} and CIFAR100 \citep{krizhevsky2009learning}. We fine-tune ViT/16-base and ViT/16-large models \citep{dosovitskiy2020image}, both pre-trained on ImageNet-21k \citep{ridnik2021imagenet}. In this experiment, we compares LoCA against several baselines: Linear Probing (LP), FF, LoRA, and FourierFT. Noting that we encountered significant discrepancies when attempting to reproduce the results reported in \citet{gao2024parameter}, possibly due to the lack of detailed hyperparameter setup. To ensure a fair comparison, we re-run all methods using our own hyperparameter settings. All results are obtained after 5 random trials.

    {\bf Implementation Details.}  
    To ensure a fair comparison across all methods, the classification head is configured identically for all approaches. For LoRA, we  a rank of 16 and a scaling factor $\alpha$ of 16. \textcolor{black}{Following \citet{gao2024parameter}, FourierFT is implemented with 3000 and 10,000 frequency components and a scaling factor of 300. For our LoCA, we also evaluate 3000 and 10,000 frequency components for both base and large models.} The learning rates for all methods are carefully tuned to ensure good performance across different tasks and model sizes. We report the number of trainable parameters excluding the classification head to provide a clear comparison of parameter efficiency. Detailed hyperparameter configurations for all methods can be found in Table \ref{tab:vit_hyper}.

   {\bf Experimental Results.} The results are presented in Table \ref{tab:vit_results}. 
   Notably, LoCA achieves superior performance compared to FourierFT while using the same number of parameters. For instance, with ViT-Base, LoCA using 72K parameters outperforms FourierFT on most datasets, with obvious improvements on StanfordCars and FGVC. Furthermore, when increasing the parameter budget to 10,000 for LoCA, we observe performance comparable to LoRA across most tasks. These results demonstrate that LoCA achieves a favorable balance between parameter efficiency and performance.

\begin{table}[t]
   \centering
   \caption{Fine-tuning results on 8 image classification datasets with ViT-base and ViT-large models. For fair comparison, we report the accuracy (\%) and standard deviation after 10 epochs of training for all methods. Best results are shown in \textbf{bold}.}
   \label{tab:vit_results}
   \resizebox{\linewidth}{!}{
   \begin{tabular}{c|l|r|ccccccccc}
   \toprule
   {\bf Model} & {\bf FT Method}   & {\bf Param.} & {\bf OxfordPets}  & {\bf StanfordCars} & {\bf CIFAR10}     & {\bf DTD}   & {\bf EuroSAT}     & {\bf FGVC}        & {\bf RESISC45}    & {\bf CIFAR100}    & {\bf Avg.}     \\ \midrule \midrule
   \multirow{6}{*}{\rotatebox[origin=c]{90}{ViT-base}}
              & LP            & -                                        & 92.94\textsuperscript{\scriptsize{$\pm$0.12}} & 47.02\textsuperscript{\scriptsize{$\pm$0.23}}  & 96.82\textsuperscript{\scriptsize{$\pm$0.01}} & 76.47\textsuperscript{\scriptsize{$\pm$0.22}} & 94.78\textsuperscript{\scriptsize{$\pm$0.02}} & 29.21\textsuperscript{\scriptsize{$\pm$1.33}} & 86.13\textsuperscript{\scriptsize{$\pm$0.10}} & 86.05\textsuperscript{\scriptsize{$\pm$0.08}} & 76.18  \\
              & FF            & 85.8M                                    & 93.09\textsuperscript{\scriptsize{$\pm$0.11}} & \textbf{84.71}\textsuperscript{\scriptsize{$\pm$0.03}}  & \textbf{98.89}\textsuperscript{\scriptsize{$\pm$0.00}} & 77.37\textsuperscript{\scriptsize{$\pm$0.30}} & 98.91\textsuperscript{\scriptsize{$\pm$0.09}} & \textbf{63.83}\textsuperscript{\scriptsize{$\pm$1.13}} & \textbf{95.72}\textsuperscript{\scriptsize{$\pm$0.21}} & 90.72\textsuperscript{\scriptsize{$\pm$0.23}} & \textbf{87.91}   \\
              & LoRA          & 581K                                     & 93.26\textsuperscript{\scriptsize{$\pm$0.28}} & 82.12\textsuperscript{\scriptsize{$\pm$0.22}}  & 98.51\textsuperscript{\scriptsize{$\pm$0.07}} & 79.54\textsuperscript{\scriptsize{$\pm$0.72}} & 98.65\textsuperscript{\scriptsize{$\pm$0.06}} & 55.67\textsuperscript{\scriptsize{$\pm$1.24}} & 94.82\textsuperscript{\scriptsize{$\pm$0.45}} & 91.51\textsuperscript{\scriptsize{$\pm$0.12}} & 86.76    \\
              & FourierFT  & 72K                                      & 93.07\textsuperscript{\scriptsize{$\pm$0.34}} & 73.74\textsuperscript{\scriptsize{$\pm$0.13}}  & 98.64\textsuperscript{\scriptsize{$\pm$0.02}} & 77.72\textsuperscript{\scriptsize{$\pm$0.74}} & 98.32\textsuperscript{\scriptsize{$\pm$0.05}} & 48.24\textsuperscript{\scriptsize{$\pm$1.09}} & 92.89\textsuperscript{\scriptsize{$\pm$0.07}} & 91.23\textsuperscript{\scriptsize{$\pm$0.04}} & 84.23 \\
              & \textcolor{black}{{\bf LoCA }}     & \textcolor{black}{72K}                           & \textcolor{black}{93.36\textsuperscript{\scriptsize{$\pm$0.03}}} &   \textcolor{black}{77.78\textsuperscript{\scriptsize{$\pm$0.14}}} & 
              \textcolor{black}{98.66\textsuperscript{\scriptsize{$\pm$0.21}}} &
              \textcolor{black}{78.44\textsuperscript{\scriptsize{$\pm$0.31}}} &  
              \textcolor{black}{98.94\textsuperscript{\scriptsize{$\pm$0.06}}} &
              \textcolor{black}{53.23\textsuperscript{\scriptsize{$\pm$0.96}}} &
              \textcolor{black}{93.88\textsuperscript{\scriptsize{$\pm$0.20}}} &
              \textcolor{black}{91.40\textsuperscript{\scriptsize{$\pm$0.11}}} & \textcolor{black}{85.71} \\
              & \textcolor{black}{FourierFT}  & \textcolor{black}{239K}                                      & \textcolor{black}{93.44\textsuperscript{\scriptsize{$\pm$0.31}}} & \textcolor{black}{79.34\textsuperscript{\scriptsize{$\pm$0.14}}}  & \textcolor{black}{98.70\textsuperscript{\scriptsize{$\pm$0.08}}} & \textcolor{black}{79.43\textsuperscript{\scriptsize{$\pm$1.15}}} & \textcolor{black}{98.81\textsuperscript{\scriptsize{$\pm$0.05}}} & \textcolor{black}{52.26\textsuperscript{\scriptsize{$\pm$1.50}}} & \textcolor{black}{94.19\textsuperscript{\scriptsize{$\pm$0.06}}} & \textcolor{black}{91.60\textsuperscript{\scriptsize{$\pm$0.15}}} & 86.02 \\
              & \textcolor{black}{{\bf LoCA }}     & \textcolor{black}{239K}                                     & \textcolor{black}{\textbf{94.10}\textsuperscript{\scriptsize{$\pm$0.21}}} & \textcolor{black}{80.11\textsuperscript{\scriptsize{$\pm$0.58}}}  & \textcolor{black}{98.62\textsuperscript{\scriptsize{$\pm$0.21}}} & \textcolor{black}{\textbf{80.15}\textsuperscript{\scriptsize{$\pm$0.61}}} & \textcolor{black}{\textbf{99.04}\textsuperscript{\scriptsize{$\pm$0.08}}} & \textcolor{black}{54.86\textsuperscript{\scriptsize{$\pm$0.65}}} & \textcolor{black}{94.73\textsuperscript{\scriptsize{$\pm$0.18}}} & \textcolor{black}{\textbf{91.68\textsuperscript{\scriptsize{$\pm$0.43}}}} & \textcolor{black}{86.66} \\ \midrule
   \multirow{6}{*}{\rotatebox[origin=c]{90}{ViT-large}}
              & LP            & -                                        & 91.93\textsuperscript{\scriptsize{$\pm$0.21}} & 43.24\textsuperscript{\scriptsize{$\pm$0.30}}  & 97.78\textsuperscript{\scriptsize{$\pm$0.23}} & 72.52\textsuperscript{\scriptsize{$\pm$0.35}} & 93.76\textsuperscript{\scriptsize{$\pm$0.18}} & 26.55\textsuperscript{\scriptsize{$\pm$0.86}} & 83.52\textsuperscript{\scriptsize{$\pm$0.38}} & 88.73\textsuperscript{\scriptsize{$\pm$0.34}} & 74.75 \\
              & FF            & 303.3M                                    & 94.13\textsuperscript{\scriptsize{$\pm$0.12}} & 85.84\textsuperscript{\scriptsize{$\pm$0.17}}  & 99.22\textsuperscript{\scriptsize{$\pm$0.15}} & \textbf{81.64}\textsuperscript{\scriptsize{$\pm$0.29}} & 99.13\textsuperscript{\scriptsize{$\pm$0.07}} & 63.33\textsuperscript{\scriptsize{$\pm$0.37}} & {\bf 96.21}\textsuperscript{\scriptsize{$\pm$0.11}} & \textbf{94.67}\textsuperscript{\scriptsize{$\pm$0.09}} & \textbf{89.27} \\
              & LoRA          & 1.57M                                     & 94.34\textsuperscript{\scriptsize{$\pm$0.36}} & \textbf{85.92\textsuperscript{\scriptsize{$\pm$0.24}}}  & 98.93\textsuperscript{\scriptsize{$\pm$0.02}} & 79.90\textsuperscript{\scriptsize{$\pm$0.88}}  & 98.91\textsuperscript{\scriptsize{$\pm$0.07}} & \textbf{64.47}\textsuperscript{\scriptsize{$\pm$0.63}} & 95.63\textsuperscript{\scriptsize{$\pm$0.13}} & 92.37\textsuperscript{\scriptsize{$\pm$0.02}} & 88.81 \\
              & FourierFT  & 144K                                      & 94.52\textsuperscript{\scriptsize{$\pm$0.53}} & 75.35\textsuperscript{\scriptsize{$\pm$0.32}}  & {\bf 99.12\textsuperscript{\scriptsize{$\pm$0.42}}} & 79.78\textsuperscript{\scriptsize{$\pm$0.76}} & 98.79\textsuperscript{\scriptsize{$\pm$0.35}} & 48.32\textsuperscript{\scriptsize{$\pm$0.89}} & 94.18\textsuperscript{\scriptsize{$\pm$0.41}} & 93.01\textsuperscript{\scriptsize{$\pm$0.14}} & 85.38 \\
              & \textcolor{black}{{\bf LoCA }}     & \textcolor{black}{144K}                                 & \textcolor{black}{94.60\textsuperscript{\scriptsize{$\pm$0.03}}} & \textcolor{black}{82.04\textsuperscript{\scriptsize{$\pm$0.25}}}  & \textcolor{black}{98.92\textsuperscript{\scriptsize{$\pm$0.03}}} & \textcolor{black}{79.02\textsuperscript{\scriptsize{$\pm$0.18}}} & \textcolor{black}{98.97\textsuperscript{\scriptsize{$\pm$0.05}}} & \textcolor{black}{57.62\textsuperscript{\scriptsize{$\pm$0.02}}} & \textcolor{black}{94.41\textsuperscript{\scriptsize{$\pm$91.76}}} & \textcolor{black}{91.76\textsuperscript{\scriptsize{$\pm$0.09}}} &  \textcolor{black}{87.17} \\
              & \textcolor{black}{FourierFT}  & \textcolor{black}{480K}                                      & \textcolor{black}{{\bf 94.78}\textsuperscript{\scriptsize{$\pm$0.09}}} & \textcolor{black}{82.27\textsuperscript{\scriptsize{$\pm$0.30}}}  & \textcolor{black}{99.00\textsuperscript{\scriptsize{$\pm$0.08}}} & \textcolor{black}{79.03\textsuperscript{\scriptsize{$\pm$0.04}}} & \textcolor{black}{98.95\textsuperscript{\scriptsize{$\pm$0.10}}} & \textcolor{black}{56.96\textsuperscript{\scriptsize{$\pm$1.09}}} & \textcolor{black}{95.53\textsuperscript{\scriptsize{$\pm$0.03}}} & \textcolor{black}{92.56\textsuperscript{\scriptsize{$\pm$0.04}}} & \textcolor{black}{87.39} \\
              & \textcolor{black}{{\bf LoCA }}     & \textcolor{black}{480K}                                & \textcolor{black}{94.47\textsuperscript{\scriptsize{$\pm$0.82}}} & \textcolor{black}{83.47\textsuperscript{\scriptsize{$\pm$0.32}}}  & \textcolor{black}{99.02\textsuperscript{\scriptsize{$\pm$0.03}}} & \textcolor{black}{80.21\textsuperscript{\scriptsize{$\pm$0.66}}} & \textcolor{black}{\textbf{99.03}\textsuperscript{\scriptsize{$\pm$0.18}}} & \textcolor{black}{63.02\textsuperscript{\scriptsize{$\pm$0.61}}} & \textcolor{black}{95.49\textsuperscript{\scriptsize{$\pm$0.15}}} & \textcolor{black}{92.65\textsuperscript{\scriptsize{$\pm$0.22}}} & \textcolor{black}{88.42} \\ \bottomrule
   \end{tabular}}
   \vspace{-15pt}
   \end{table}

    \subsection{Analytical Experiments} \label{sec:analytical}
    {\bf Effectiveness of Gradient Estimation.} To validate the reliability of our estimated location gradients, we present the training process on 4 selected datasets in Fig. \ref{fig:training_process}. The left figure shows
    \begin{wrapfigure}{r}{0.5\linewidth}
    \vspace{-3pt}
    \centering
    \begin{minipage}[t]{0.485\linewidth}
    \centering
    \includegraphics[width=\linewidth]{Figures/loss_curves.pdf}
    \end{minipage}%
    \begin{minipage}[t]{0.495\linewidth}
    \centering
    \includegraphics[width=\linewidth]{Figures/acc_curves.pdf}
    \end{minipage}%
    \centering
    \caption{Evaluation loss (left) and performance (right) of our method with RoBERTa-base and ViT-base models. We record every 10 steps. The solid lines represent alternating optimization of coefficients and locations, while the dashed lines represent optimizing coefficients only.}
    \label{fig:training_process}
    \vspace{-6pt}
    \end{wrapfigure}
 that during the alternating optimization phase, the validation loss generally decreases in most steps, particularly for StanfordCars and CIFAR10. The right figure demonstrates corresponding improvements in validation accuracy (or Pearson correlation). These trends indicate that our central difference approximation method effectively guides the optimization process, enabling successful updates to frequency component locations. We also conduct a toy experiment to show the convergence of the alternating optimization strategy in Appendix \ref{sec:toy_experiment}. \\[7pt]
    {\bf Performance under Different Parameter Budgets.} Fig. \ref{fig:budget} compares various methods under same parameter budgets. Here we focus on QQP and FHVC, which present significant challenges for LoRA. The parameter budget is standardized using LoRA's rank $r$ as the base unit. Our results reveal that FourierFT often underperforms LoRA when using fewer parameters. This observation aligns with expectations, as the locations of frequency components becomes increasingly critical under constrained parameter budgets. Notably, LoCA consistently outperforms LoRA and FourierFT across the tested scenarios.
    It is worth noting that our theoretical analysis centers on expected performance. While specific task structures may allow FourierFT to surpass LoRA in certain instances, these exceptions do not undermine our overall conclusions and analytical framework.

    {\bf Choice of Scaling value $\alpha$ and Alternating Optimization Steps $\mathcal{B}_s$.} Fig. \ref{fig:parameter_sen} demonstrates the impact of different choices of $\alpha$ and $\mathcal{B}_s$ on the MRPC task. We empirically find that a scaling value between 1-2 can achieve better results. Additionally, setting $\mathcal{B}_s$ to between 10\%-20\% of the total training steps is more appropriate (with a total of 5750 steps for the MRPC task).

{\bf Ablation Study of the Alternating Optimization Strategy.} 
    Table \ref{tab:ablation} compares several variants of our method:
{\bf V1} only optimizes coefficients with randomly initialized locations.
{\bf V2} alternately optimizes coefficients and locations throughout the training.
{\bf V3} jointly optimizes locations and coefficients in each step for $B_s$ steps.
{\bf V4} and {\bf V5} use forward and backward difference approximation for gradient estimation, respectively. Hyperparameters are identical 
\begin{wraptable}{r}{0.5\textwidth}
\centering
\small
% \vspace{-6pt}
\caption {Comparison between different optimization strategies on 4 datasets. We use RoBERTa-base and ViT-base models for this experiment. Best results are shown in {\bf bold}.}
\label{tab:ablation}
\vspace{-5pt}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|c|c|c|c}
\toprule
\multirow{2}{*}{{\bf Variants}} & \multicolumn{2}{c}{{\bf Vision Tasks ($\mathcal{B}$ =5000)}} & \multicolumn{2}{c}{{\bf Language Tasks ($\mathcal{B}$ =1000)}} \\ \cmidrule{2-5} 
                         & OxfordPets         & DTD         & ~~~~~QQP~~~~~             & CoLA            \\ \midrule \midrule
V1                       & 92.8               & 76.8        & 87.7             & 63.2            \\
V2                       & 91.9               & 76.3        & 86.5             & 61.6            \\
V3                       & 93.4               & 79.1        & 88.0             & 64.1            \\
V4                       & {\bf 93.8}               & 79.5        & 88.6             & 64.3            \\
V5                       & {\bf 93.8}               & {\bf 79.7}        & 88.4             & 64.4            \\
\midrule
\textcolor{black}{LoCA} &  \textcolor{black}{{\bf 93.8}} & \textcolor{black}{{\bf 79.7}} & \textcolor{black}{{\bf 88.7}} & \textcolor{black}{{\bf 64.5}} \\
\bottomrule
\end{tabular}
}
\vspace{-10pt}
\end{wraptable} 
to the ones in Section \ref{sec:NLU} and \ref{sec:Vision}.
It can be observed that alternating optimization throughout the entire process leads to instability, resulting in a suboptimal performance. 
Simultaneously optimizing coefficients makes convergence not guaranteed, thus being less effective than alternating optimization. Both one-side (forward and backward) difference approximations show effectiveness, but it is challenging to theoretically analyze which is superior. Therefore, we choose using the central difference approximation as the default implementation.

    \section{Related Work}
    The recent surge in LLM research has reignited interest in PEFT research. To pursue favorable task performance while using only a small number of trainable parameters, current PEFT methods primarily lie in four categories: adding extra trainable modules \citep{houlsby2019parameter,ruckle2020adapterdrop}, selectively training a small subset of key parameters \citep{zaken2021bitfit,lawton2023neural}, employing reparameterization techniques like low-rank decomposition to the incremental matrices \citep{hu2021lora,zhang2023adalora,liu2024dora,hao2024flora}, or combining multiple strategies \citep{chen2023parameter}. Among them, low-rank methods have garnered significant attention due to their mergable nature and parameter efficiency. These low-rank methods, which aim to approximate large weight matrices using a few principal components, is highly analogous to techniques employed in data compression. In fact, low-rank decomposition (or singular value decomposition) and frequency-domain decomposition (e.g., JPEG compression) represents two fundamental tools in image compression and signal processing.
    
    For image compression, frequency-domain reconstruction (e.g., DCT) are preferred due to the inherent smoothness prior of image data \citep{wallace1991jpeg}. However, when dealing with the complex data structures of neural network parameter matrices, the relative efficacy of these approaches remains unexplored. To the best of our knowledge, although FourierFT \citep{gao2024parameter} has made an empirical study of frequency-domain PEFT by employing Fourier Transform, no prior work has conducted a rigorous comparison between low-rank and frequency-domain decomposition methods in the context of PEFT. Our work aims to bridge this gap by providing a comprehensive theoretical analysis and designing a more efficient frequency-domain PEFT method. 

\begin{figure}[t]
    \centering
    \begin{minipage}{0.63\textwidth}
        \centering
    \begin{minipage}[t]{0.43\linewidth}
    \centering
    \includegraphics[width=\linewidth]{Figures/qqp.pdf}
    \end{minipage}%
    \begin{minipage}[t]{0.43\linewidth}
    \centering
    \includegraphics[width=\linewidth]{Figures/fgvc.pdf}
    \end{minipage}
    \centering
    \caption{Performance comparison under different parameter budgets on QQP (RoBERTa-base) and FGVC (ViT-base).}
    \label{fig:budget}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{Figures/img_para.pdf}
        \caption{Influence of $\alpha$ and $\mathcal{B}_s$ on MRPC (RoBERTa-base).}
        \label{fig:parameter_sen}
    \end{minipage}
    \vspace{-13pt}
\end{figure}


   \section{Conclusion}
   This paper provides a theoretical foundation for frequency-domain PEFT methods. We prove that carefully selected frequency components can outperform low-rank approaches, leading to the development of location-aware frequency-domain PEFT method. Our method optimizes both coefficients and locations of frequency components using iDCT and difference approximation. We show that our method enhances expressiveness while maintaining computational efficiency. Extensive experiments across NLP and computer vision tasks demonstrate the superior performance and parameter efficiency compared to existing PEFT methods. 
   
   %     Given any matrix $W\in\mathbb{R}^{K\times K}$, previous work choose $N_1$ elements of $\mathcal{F}(W)$ randomly to reconstruct $W$, where $\mathcal{F}$ is the discrete Fourier operator. Low rank methods reconstruct $W$ by finding $k$ most important rows and columns pattern. Intuitively, previous work has lower flexibility and thus the reconstruction square loss relative to $||W||_2^2$ should be higher. However, we reconstruct $W$ by first finding $N_2$ largest elements of $\mathcal{F}(W)$, which gives higher flexibility and should performs better than low rank method.
       
   
   %     We expect this reconstruction enhancement happens on a dominating subset of $W$'s support. Empirically speaking, $W$ as weight update matrix in deep network usually follows random normal distribution. To be specific $W\sim N_{n,n}(\mu\mathbbm{1}_n\mathbbm{1}_n^T,\sigma_2I_n,I_n), W^V\sim N_{n^2}(\mu\mathbbm{1}_{n^2},\sigma^2I_{n^2})$, where $\mathbbm{1}_n=(1,\ldots,1)^T\in\mathbb{R}^n$. As we can always rescale $W$ to zero expectation and 1 variance, we take $\mu=0$ and $\sigma=1$ for simplicity. We expect the reconstruction enhancement happens at least under this ideal circumstance. But first we show that previous work underachieve compared with low rank method. Given $W\sim N_{n,n}(0,I_n,I_n)$, and $W^V\sim N_{n^2}(0,I_{n^2})$, define the $k$-th low rank approximation is $\hat{W}_R=\hat{U}\hat{V}^T$, where $\hat{U}\in\mathbb{R}^{n\times k},\hat{V}\in\mathbb{R}^{n\times k}$.
       
       
   %     The previous work randomly takes $2N_1$ numbers of $\mathcal{F}(W)$, and does inverse discrete Fourier transform to get an approximated matrix of $W$. However, $\mathcal{F}(W)$ retains conjugate symmetric propertry, that is
   %     \begin{gather*}
   %         \mathcal{F}(W)(u,v)=\mathcal{F}(W)^*(n-u,n-v),1\leq u,v\leq n-1,\\
   %         \mathcal{F}(W)(0,v)=\mathcal{F}(W)^*(0,n-v),1\leq v\leq n-1,\\
   %         \mathcal{F}(W)(u,0)=\mathcal{F}(W)^*(n-u,0),1\leq u\leq n-1,
   %     \end{gather*}
   %     where $\cdot^*$ means conjugation. At most four locations do not have its counterpart. Thus randomly picking $N_1^\prime$ items leads to a complex approximation, whose imaginary part is useless and causes information loss. 
       
       
   %     To avoid information loss, we first give a slightly better version. Assume $N_1$ different locations $Id^{(1)}=\{id_1^{(1)}=(x_1^{(1)},y_1^{(1)}),\ldots,id_{N_1}^{(1)}=(x_{N_1}^{(1)},y_{N_1}^{(1)})\}$ which are randomly selected from all $n^2$ locations. To reconstruct original $W$, write $\hat{F}^{(1)}$ as the matrix retains the elements at $Id^{(1)}$ location of $\mathcal{F}(W)$. The reverse Fourier approximation is defined as $\hat{W}_F^{(1)}=\mathcal{F}^{-1}(\hat{F}^{(1)})$.
   
   
   %     Define the reconstruction loss as
   %     \begin{gather*}
   %         L_R=\dfrac{||W-\hat{W}_R||_2^2}{||W||_2^2},
   %         L_F^{(1)}=\dfrac{||W-\hat{W}_F^{(1)}||_2^2}{||W||_2^2},
   %     \end{gather*}
   
   
   %     \begin{theorem}\label{underachieve}
   %         With $W,\hat{W}_R,\hat{W}_F^{(1)}$ stated above, and assure $\hat{W}_R,\hat{W}_F^{(1)}$ use same amount of information, for given $k$ rank $\hat{W}_R$, $2nk$ numbers are required in total. As the location can be chosen beforehead and we require two numbers for each $\mathcal{F}(W)(id)$, we should take $N_1=nk$. Thus we have
   %         \begin{equation*}
   %             \mathbb{E}_{Id^{(1)}}\mathbb{E}_{W^V\sim N_{n^2}(0,I_{n^2})}L_F^{(1)}>\mathbb{E}_{W^V\sim N_{n^2}(0,I_{n^2})}L_R.
   %         \end{equation*}
   %     \end{theorem}
   %     \textbf{Remark.} This theorem shows that choosing location randomly to keep parameters will enlarge the reconstruction loss. Thus a natural idea is to choose the location carefully.
   
   
   %     When reconstructing a matrix through low-rank decomposition, the reconstruction loss is essentially the squared norm of the smallest K-r singular values of the matrix. This automatically selects the top k largest singular values for the reconstruction, and the average squared sum of these selected singular values is greater than the average squared sum of the entire set, thereby capturing the primary structure of the matrix. In the Fourier transform method of reconstruction, according to Parseval's theorem, the reconstructed part is the sum of the squared elements of the estimated Fourier transform matrix. If the original matrix is independently and identically distributed, the Fourier-transformed matrix will also be approximately identically distributed. Thus, the average of the squared sums at any selected locations will be roughly equal to the overall average.
   
   
   %     Thereby we aim to incorporate the concept of order into the process of reconstructing a matrix using the Fourier transform method. First, a natural idea is to consider the non-symmetric half of the matrix elements after the Fourier transform. From these, select the top $N_2$ locations with the largest magnitudes, denoted as $Id^{(2)}$. Retain the elements at these locations in $\mathcal{F}(W)$, set all others to zero to get $\hat{F}^{(2)}$, and denote the reconstructed matrix as $\hat{W}_F^{(2)}=\mathcal{F}^{-1}(\hat{F}^{(2)})$. Since the locations of the largest elements in $W$ vary for each different $W$, each position requires a number to record it. Additionally, both the real and imaginary parts of each position need a separate number. To align with the rank-$k$ low-rank decomposition method, we take $N_2=2nk/3$.
   
   
   %     Another equally natural idea is that since each position in $\mathcal{F}(W)$ has both a real part and an imaginary part, they do not necessarily need to be bound together. Therefore, we can directly select the elements with distinct real and imaginary parts, choose the top $N_3$ locations with the largest absolute values, and their corresponding real or imaginary parts, denoted as $Id^{(3)}=\{id_1^{(3)}=(x_1^{(3)},y_1^{(3)},r_1^{(3)}),\ldots,id_{N_3}^{(3)}=(x_{N_3}^{(3)},y_{N_3}^{(3)},r_{N_3}^{(3)})\}$. We retain the elements of $\mathcal{F}(W)$ at these locations, set others to zero to obtain $\hat{F}^{(3)}$, and denote the reconstructed matrix as $\hat{W}_F^{(3)}=\mathcal{F}^{-1}(\hat{F}^{(3)})$. As each $id_i^{(3)}$ requires a number to record and the value needs one. To align with the rank-$k$ low-rank decomposition method, we take $N_3=nk$. Similarly define the reconstruction loss as $L_F^{(j)}=\dfrac{||W-\hat{W}_F^{(j)}||_2^2}{||W||_2^2},j=2,3$.
   
   
   %     \begin{theorem}\label{mainth1}
   %         With $W,\hat{W}_R,\hat{W}_F^{(2)},\hat{W}_F^{(3)}$ stated above, if $k<n/3$, we have
   %         \begin{equation*}
   %             \mathbb{E}_{W^V\sim N_{n^2}(0,I_{n^2})}L_R>\mathbb{E}_{W^V\sim N_{n^2}(0,I_{n^2})}L_F^{(2)}>\mathbb{E}_{W^V\sim N_{n^2}(0,I_{n^2})}L_F^{(3)}.
   %         \end{equation*}
   %     \end{theorem}
   
   
   %     Theoretical analysis suggests that estimating $\hat{W}_F^{(3)}$ is optimal, but this is practically difficult to implement. Learning locations could be achieved through approximating the gradient of the position using differences. However, deciding whether to learn on the real part or the imaginary part involves extensive discrete optimization, which is impractical for training large models. Therefore, optimization is necessary. 
       
       
   %     The discrete cosine transform only contains real number operation. We propose a simple analogue to $\hat{W}_F^{(3)}$. Perform a discrete cosine transform on $W$, denote the operator by $\mathcal{D}$, resulting in the transformed matrix $\mathcal{D}(W)$. Identify the $N_D$ locations with the largest absolute values in $\mathcal{D}(W)$, denoted as $Id_D$, and retain the elements at these locations while setting all others to zero to get $\hat{D}$, yielding the reconstructed matrix $\hat{W}_D=\mathcal{D}^{-1}(\hat{D})$. define the reconstruction loss as $L_D=\dfrac{||W-\hat{W}_D||_2^2}{||W||_2^2}$.
   
   
   %     \begin{theorem}\label{mainth2}
   %         With $W,\hat{W}_F^{(3)},\hat{W}_D$ stated above, we have
   %         \begin{equation*}
   %             \mathbb{E}_{W^V\sim N_{n^2}(0,I_{n^2})}L_D=\mathbb{E}_{W^V\sim N_{n^2}(0,I_{n^2})}L_F^{(3)}.
   %         \end{equation*}
   %     \end{theorem}
   
   
   %     \textbf{Remark.} The most vital assumption here is $W^V\sim N_{n^2}(0,I_{n^2})$. Actually this assumption equals to $W_{ij}\sim N(0,1)$. However, we test in a wide range of weight update matrix and find slight distribution shift between $W$ and random normal matrix exists. In other words for some univariate distribution $G(\cdot)$, and $W_{ij}\sim G$ Denote the total variation operator as $d_{TV}(\cdot,\cdot)$ we verify later that for $\epsilon=0.002$, $d_{TV}(G,N(0,1))\leq\epsilon$.
   
   
   %     \begin{theorem}\label{mainth3}
   %         Assume $W\in\mathbb{R}^{K\times K}$ and $W_{ij}\overset{i.i.d.}{\sim} G$, where $d_{TV}(G,N(0,1))\leq\epsilon=0.002$. We have
   %         \begin{equation*}
   %             \mathbb{E}_{W_{ij}\overset{i.i.d.}{\sim} G}L_R>\mathbb{E}_{W_{ij}\overset{i.i.d.}{\sim} G}L_D.
   %         \end{equation*}
   %     \end{theorem}
   %     \textbf{Remark.} Both Theorem \ref{mainth2} and \ref{mainth3} gives that inverse cosine approximation $\hat{W}_D$ is better than low rank approximation.
   \WFclear

   \bibliography{iclr2025_conference}  
   \bibliographystyle{iclr2025_conference}
   
   \newpage
   \appendix
   \section{Justification of Assumptions} \label{sec:justification}
   In the pre-training and fine-tuning paradigm, deep neural networks are initially trained on a large dataset with distribution $P(X, Y; \overline{W}_0)$ and subsequently fine-tuned on a specific down-stream dataset with distribution $P(X, Y; \overline{W})$. In this context, $\overline{W}$ becomes a random variable associated with a specific data distribution.
   
   
   First for assumption (A1), the large dataset used for pre-training represents an aggregation of numerous sub-datasets. Each sub-dataset contributes to the overall distribution $P(X, Y;\overline{W}_0)$. The parameter $\overline{W}_0$ can be seen as the central tendency (mean) of the parameters for all sub-datasets. This aggregation naturally leads to a central limit theorem effect, where the mixture of multiple sub-datasets can be approximated by a normal distribution around $\overline{W}_0$, which also reflects the idea of symmetry in the distribution of sub-datasets. In the absence of strong directional biases, it is reasonable to consider that the parameters for different sub-datasets are symmetrically distributed.
   Note that our proposition is based on all sub-datasets, which also follows the philosophy of the No Free Lunch (NFL) theorem in machine learning. By modeling $\overline{W}$ as a distribution centered on $\overline{W}_0$, we account for the variability across different sub-datasets.
   
   
   % On the other hand, for assumption (A2), a general model for neural network is $y=f(x;W)+\varepsilon$, where $W$ represents the true parameters and $\varepsilon$ is the error term. If the input distribution $x$ is exchangeable, meaning that its components can be permuted without affecting the distribution, it is reasonable to assume that the parameters of the neural network also exhibit some form of symmetry. Therefore, the covariance structure of the parameters should reflect this symmetry, leading to a diagonal covariance matrix with equal variances.
   
   \begin{figure*}[h] 
     \centering 
     \subfloat[Layer 10]{
     \includegraphics[width=0.32\linewidth] {Figures//ESD_layer_10_W.pdf} 
     \label{fig:appendix_esd1}
     }
     \subfloat[Layer 20]{
     \includegraphics[width=0.32\linewidth] {Figures//ESD_layer_20_W.pdf}
     \label{fig:appendix_esd2}
     }
     \subfloat[Layer 30]{
     \includegraphics[width=0.32\linewidth] {Figures//ESD_layer_30_W.pdf}
     \label{fig:appendix_esd3}
     }
     % \vspace{-5pt}
     \caption{Empirical spectral density of the fine-tuned $W^{\prime}$ across multiple layers. The experimental settings are the same as those in Section \ref{sec:Preliminary_Analysis}.} 
     \label{fig:spectral_analysis_appendix} 
     % \vspace{-14pt} 
   \end{figure*}
   
   Regarding assumption (A2), the asymptotic normality of M-estimators is a commonly used assumption in statistics and machine learning. The strongest assumption here should be that the elements of $W^{\prime}-\overline{W}$ are asymptotically independent and identically distributed given $\overline{W}$. To demonstrate the reasonability of this assumption. We first consider the asymptotically i.i.d. property of $W^{\prime}$. While the strict i.i.d. property of parameters in trained neural networks remains a subject of ongoing research, several studies have shwon that certain statistical properties of these parameters resemble those of random i.i.d. matrices \citep{thamm2022random,martin2021implicit,lee2017deep}. Our work extends this line by examining the spectral properties of the trained weight during LLM fine-tuning. Specifically, we use the Marchenko-Pastur (MP) law to test the fit between the empirical spectral densities of $W^{\prime}$ and that of random matrices.
   The MP law is a fundamental result in random matrix theory. It describes the asymptotic behavior of the eigenvalue distribution of large random matrices. The law can be formally stated as follows:
   Consider a $p \times q$ random matrix $W$, where each element is an independent and identically distributed random variable with mean $0$ and variance $\sigma^2$. Let $C = (1/p)W^{\prime T}W^{\prime}$ be the covariance matrix.
   As $p, q \rightarrow \infty$ with a fixed aspect ratio, the empirical spectral distribution of the eigenvalues of $C$ converges almost surely to a deterministic probability distribution known as the Marchenko-Pastur distribution. Here we are dealing with large Transformer weight matrices. If they are asymptotically i.i.d. matrixes, the ESD of them should closely approximate the MP distribution corresponding to their current aspect ratios. We visualize the ESD of the fine-tuned $W^{\prime}$ across multiple layers, as shown in Fig. \ref{fig:spectral_analysis_appendix}. And the results show that $W^{\prime}$ behaves like an i.i.d random matrix. As each element on $\overline{W}$ is permutable due to the equal role of different positions, we can summarize that $\overline{W}$ has a zero-expectation influence on $W^\prime-\overline{W}$. Therefore, the asymptotically i.i.d property of $W^\prime-\overline{W}$ does not violate our observations.
   % As their works verified asymptotic property of $W^\prime$, and the $\overline{W}_0^V$ should be to some extent symmetric due to the equal role of parameters, the $W^\prime$ can exhibit independent property. 
   The assumption that $W^\prime-\overline{W}$ and $\overline{W}$ are independent is analogous to treating $W^\prime-\overline{W}$ as noise, while $\overline{W}$ is the true signal. This is a common assumption in the context of asymptotic analysis, where the estimation error (or noise) is considered to be independent of the true parameter.
   
   \section{Details of the Hypothesis Testing} \label{sec:hypothesis_test}
   We now describe the detailed procedure of the hypothesis testing adopted in Section \ref{sec:Preliminary_Analysis}. Recall that our goal is to test whether the elements $w$ from the weight incremental matrix $\Delta W$ follows a distribution that is close to a Gaussian. Formally, we have the following hypothesis setup and test statistic.
   
   {\bf Hypothesis Setup:}
   $$
   H_0: d_{TV}(P(w), \mathcal{N}(w; \hat{\mu}, \hat{\sigma}^2)) \leq \epsilon,~~~H_1: d_{TV}(P(w), \mathcal{N}(w; \hat{\mu}, \hat{\sigma}^2)) > \epsilon
   $$
   Where $d_{TV}(\cdot, \cdot)$ denotes the total variation distance, $P(w)$ is the true distribution of elements in $\Delta W$, and $\mathcal{N}(\hat{\mu}, \hat{\sigma}^2)$ is the normal distribution with sample mean and variance as parameters.
   
   {\bf Test Statistic:}
   $$
   T = d_{TV}(\hat{P}_n(w), \mathcal{N}(w; \hat{\mu}, \hat{\sigma}^2))
   $$
   Where $\hat{P}_n(w)$ is the empirical distribution of $w$.
   
   {\bf Testing Procedure:}
   
   Given a $\Delta W \in \mathbb{R}^{p \times q}$ yielded by full fine-tuning, our test procedure consists of the following steps.
   
   
   \begin{enumerate}
       \item From the observed $\Delta W$, compute the empirical mean $\hat{\mu}$ and variance $\hat{\sigma}^2$.
       \item Generate $1e^5$ samples from $\mathcal{N}(w; \hat{\mu}, \hat{\sigma}^2)$, denoted this set of samples by $\mathcal{G}$.
       \item Generate $B$ perturbed distributions:
       \begin{itemize}
           \item Add small random perturbations $e \sim \mathcal{N}(e; 0, {\sigma_e}^2)$ to the $M$ samples, where ${\sigma_e} = 1e^{-5}$.
           \item Calculate the empirical distribution of the perturbed samples.
           \item Compute the total variation distance between the obtained empirical distribution and $\mathcal{G}$.
           \item If the total variation distance is less than $\epsilon$, keep this distribution.
           \item Repeat until $100$ valid perturbed distributions are obtained.
       \end{itemize}
       \item For each of the $100$ perturbed distributions:
       \begin{itemize}
           \item Sample 10 sets of $p \times q$ points.
           \item  For each set, calculate the total variation distance between the empirical distribution of this set and $\mathcal{G}$. This results in $M \times P$ total variation distances, forming the distribution of the test statistic under $H_0$.
       \end{itemize}
       \item Calculate the total variation distance between the empirical distribution of $\Delta W$ and $\mathcal{G}$, denoted by $T$.
       \item The p-value is the percentile of $T$ in the $M \times P$ total variation distances.
       \item Reject $H_0$ if the p-value is less than the chosen significance level (e.g., 0.05). Otherwise, accept $H_0$.
   \end{enumerate}
   
   Note that although this process is not strictly a bootstrap (as it does not directly resample from the original data), it does use the idea of repeated sampling to generate the distribution of the test statistic. Traditional bootstrap typically resamples with replacement directly from the original data, whereas our method first generates a series of perturbed distributions and then samples from these distributions. The advantage of this approach is that it allows us to explore the behavior of distributions that are close to a Gaussian distribution, while allowing for small variations. This method is more akin to a Monte Carlo simulation, used to estimate the distribution of total variation under the null hypothesis.

   \section{Details about Baseline Methods}
    $\bullet$
    \textit{Full fine-tuning} (FF) updates all parameters of the pre-trained model during the fine-tuning process, allowing for comprehensive adaptation at the cost of significant computational resources.
    
    $\bullet$
    \textit{BitFit} \citep{zaken2021bitfit} solely fine-tunes the bias weights while keeping other parameters frozen.

    $\bullet$
    \textit{Adapter-based methods} inject extra trainable modules into pre-trained models and keep the original model parameters frozen. In our comparison, we primarily focused on three types of Adapters: Adapter\textsuperscript{H} \citep{houlsby2019parameter}, which inserts a two-layer adapter between the self-attention module (or the FFN module) and the subsequent residual connections, Adapter\textsuperscript{L} \citep{lin2020exploring} that inserts a lightweight adapter layer with a bottleneck architecture after the MLP module and a LayerNorm layer in each Transformer block, and Adapter\textsuperscript{D} \citep{ruckle2020adapterdrop} that further enhances efficiency by strategically dropping inactive adapter layers.

    $\bullet$
    \textit{LoRA} \citep{hu2021lora} reparameterizes $\Delta W$ using two trainable low-rank matrices. Therefore, the number of trainable parameters is controlled by the chosen rank and the shape of weight matrixs.

    $\bullet$
    \textit{AdaLoRA} \citep{zhang2023adalora} extends LoRA by introducing an adaptive mechanism to dynamically allocate the rank budget across different parameter matrices.

   $\bullet$
   \textcolor{black}{\textit{VeRA} \citep{kopiczko2023vera} extends LoRA by introducing trainable scaling vectors ($d$ and $b$) to adaptively adjust the contribution of each dimension in the low-rank matrices, achieving comparable performance with significantly fewer parameters.}

   $\bullet$
   \textit{DoRA} \citep{liu2024dora} is a LoRA variant that decomposes pre-trained weights into magnitude and direction components for fine-tuning. It demonstrates learning patterns closer to full fine-tuning.

   $\bullet$
   \textit{FourierFT} \citep{gao2024parameter} treats weight changes as spatial-domain matrices and reparameterizes them with a set of learnable frequency components. The number of trainable parameters is controlled by the number of frequency components, allowing for more flexible scaling of parameter budgets.   
   
   
   \section{Hyperparameters} 
   Table \ref{tab:glue_hyper}, \ref{tab:It_hyper}, \ref{tab:e2e_hyper} and \ref{tab:vit_hyper} summarize the hyperparameters we used in each experiment. It is worth noting that for LoCA, the weight decay is not applied to the optimization of the location variables. Regarding the total number of alternating learning steps $\mathcal{B}_s$, we set it to approximately 10\% of the total training steps, based on the size of different datasets.

   It is worth noting that our method has very stable hyperparameters (especially the scaling value) across different tasks on GLUE, while FourierFT requires extensive parameter tuning to achieve satisfactory results, as can be seen from \citet{gao2024parameter}. 
   %This aligns with our theoretical analysis: the expected expressivity of FouriorFT is weaker than LoRA. Therefore, to obtain better results, it requires very careful adjustment of hyperparameters.
   
\begin{table}[H]
   \centering
   \caption{Hyperparameters for our method on the GLUE benchmark.}
   \label{tab:glue_hyper}
   \resizebox{0.95\linewidth}{!}{
\begin{tabular}{llcccccccc}
\toprule
Model                          & Datasets                                                                    & CoLA & MNLI & MRPC & QNLI & QQP  & RTE  & SST2 & STS-B \\ \midrule \midrule
\multirow{8}{*}{Common}        & Optimizer                                                                   & \multicolumn{8}{c}{AdamW}                              \\
                               & LR Schedule                                                                 & \multicolumn{8}{c}{Linear}                             \\
                               & Batch Size                                                                  & \multicolumn{8}{c}{32}                                 \\
                               & Where                                                                & \multicolumn{8}{c}{Query, Value}                               \\
                               & Warmup Ratio                                                                & \multicolumn{8}{c}{0.06}                               \\
                               & $\mathcal{B}$ & \multicolumn{8}{c}{1000}                               \\
                               & Learning Rate (Postions)                                                    & \multicolumn{8}{c}{1e-4}                               \\
                               & Scaling Value $\alpha$                                                              & \multicolumn{8}{c}{1}                                  \\
                               & Random Seeds                                                                & \multicolumn{8}{c}{\{6,66,666\}}                       \\ \midrule
\multirow{6}{*}{RoBERTa-base}  & Learning Rate (Head)                                                        & 5e-3 & 5e-4 & 6e-3 & 1e-3 & 5e-4 & 6e-3 & 1e-3 & 1e-3  \\
                               & Learning Rate (Coefficients)                                                & 5e-3 & 5e-4 & 1e-2 & 5e-3 & 5e-4 & 5e-3 & 5e-3 & 5e-3  \\
                               & Max Seq. Len                                                                & \multicolumn{8}{c}{512}                                \\
                               & Weight Decay                                                                & 1e-4 & 1e-4    & 1e-4 & 5e-4 & 1e-4 & 0    & 5e-4 & 5e-4  \\
                               & Epochs                                                                      & 80   & 30   & 50   & 40   & 35   & 80  & 30   & 50    \\
                               & $\mathcal{B}_s$                                                                        & 2100 & 3000 & 600  & 3000 & 3000 & 600  & 3000 & 600   \\ \midrule
\multirow{6}{*}{RoBERTa-large} & Learning Rate (Head)                                                        & 5e-3 & 5e-4 & 5e-3 & 1e-3 & 5e-4 & 5e-3 & 1e-3 & 1e-3  \\
                               & Learning Rate (Coefficients)                                                & 5e-3 & 5e-4 & 1e-2 & 5e-3 & 5e-4 & 5e-3 & 5e-3 & 5e-3  \\
                               & Max Seq. Len                                                                & \multicolumn{8}{c}{512}                                \\
                               & Weight Decay                                                                & 1e-4 & 1e-4 & 1e-4 & 5e-4 & 1e-4 & 0 & 5e-4 & 5e-4  \\
                               & Epochs                                                                      & 40   & 15   & 30   & 25   & 20   & 50   & 20   & 50    \\
                               & $\mathcal{B}_s$                                                                        & 1000 & 3000 & 400  & 3000 & 3000 & 300  & 3000 & 600   \\ \bottomrule
\end{tabular}}
   \end{table}

\begin{table}[H]
\centering
\caption{\textcolor{black}{Hyperparameter configuration of LoCA on the E2E benchmark.}}
\label{tab:e2e_hyper}
% \resizebox{0.4\textwidth}{!}{%
\begin{tabular}{@{}l|c@{}}
\toprule
Hyperparameter & GPT-2 Medium/Large \\ \midrule
Optimizer & AdamW \\
Dropout & 0 \\
Warmup Steps & 100 \\
Epochs & 5 \\
Where & Query, Value \\
Label Smooth &  0.1  \\
LR Schedule & Linear  \\
Learning Rate (Coefficients) & 5e-3 \\
Learning Rate (Positions) & 1e-4 \\
Learning Rate (Head) & 2e-4 \\
Batch Size & 32 \\
Weight Decay & 0.01 \\
$\mathcal{B}$ & 1000 \\
Learning iterations $\mathcal{B}_s$ & 1200 \\
Scaling Value $\alpha$ & 1 \\ \bottomrule
\end{tabular}%
% }
\end{table}


\begin{table}[H]
   \centering
   \caption{Hyperparameter configuration for all methods on the instruction tuning task.}
   % \resizebox{0.65\linewidth}{!}{
   \label{tab:It_hyper}
   \begin{tabular}{lccc}
   \toprule
   Method                   & Hyperparameter             & LLaMA-7B       & LLaMA-13B    \\ \midrule \midrule
   \multirow{5}{*}{Common}    & Optimizer                  & \multicolumn{2}{c}{AdamW}     \\
                              & LR schedule                & \multicolumn{2}{c}{Linear}    \\
                              & Batch Size                 & \multicolumn{2}{c}{16}       \\
                              & Where                 & \multicolumn{2}{c}{Query, Value}       \\
                              & Weight Decay               & \multicolumn{2}{c}{0}      \\ 
                              & Epochs               & 3 & 1  \\
                              & Accumulation Steps   & 
                              \multicolumn{2}{c}{4}  \\
                              
                              \midrule
   \multirow{3}{*}{LoRA}      & Rank                       & \multicolumn{2}{c}{64}        \\
                              & Scaling Value                & \multicolumn{2}{c}{16}        \\
                              & Learning Rate        & \multicolumn{2}{c}{3e-4}      \\ \midrule
    FF    & Learning Rate         & 2e-5 & 1e-5        \\ \midrule
    
   \multirow{3}{*}{FourierFT} & Frequency Components       & \multicolumn{2}{c}{150000}      \\
                              & Scaling Value                      & \multicolumn{2}{c}{64}       \\
                              & Learning Rate        & \multicolumn{2}{c}{1e-3}      \\ \midrule
   \multirow{5}{*}{LoCA}                        & Frequency Components       & \multicolumn{2}{c}{150000} \\
                              & Learning Rate (coefficient)        & \multicolumn{2}{c}{5e-4}      \\
                              & Scaling Value                      & \multicolumn{2}{c}{1}         \\
                              & Learning iterations ($\mathcal{B}_s$) & 600 & 300   \\
                              & Learning Rate (locations)  & \multicolumn{2}{c}{1e-4}      \\ \bottomrule
   \end{tabular}
   \end{table}


\begin{table}[H]
   \centering
   \caption{Hyperparameter configuration for all methods on eight image classification datasets.}
   % \resizebox{0.65\linewidth}{!}{
   \label{tab:vit_hyper}
   \begin{tabular}{lccc}
   \toprule
   Method                   & Hyperparameter             & ViT-Base       & ViT-Large    \\ \midrule \midrule
   \multirow{5}{*}{Common}    & Optimizer                  & \multicolumn{2}{c}{AdamW}     \\
                              & LR schedule                & \multicolumn{2}{c}{Linear}    \\
                              & Batch Size                 & \multicolumn{2}{c}{128}       \\
                              & Where                 & \multicolumn{2}{c}{Query, Value}       \\
                              & Learning Rate (Head)       & 1e-2           & 1e-3         \\
                              & Weight Decay               & \multicolumn{2}{c}{5e-5}      \\ 
                              & Random Seeds                      & \multicolumn{2}{c}{\{2020, 2021, 2022, 2023, 2024\}} \\
                              \midrule
   \multirow{3}{*}{LoRA}      & Rank                       & \multicolumn{2}{c}{16}        \\
                              & Scaling Value                & \multicolumn{2}{c}{16}        \\
                              & Learning Rate (ViT)        & \multicolumn{2}{c}{5e-3}      \\ \midrule
   \multirow{3}{*}{FourierFT} & Frequency Components       & \multicolumn{2}{c}{3000 and 10,000}      \\
                              & Scaling Value                      & \multicolumn{2}{c}{300}       \\
                              & Learning Rate (ViT)        & \multicolumn{2}{c}{5e-2}      \\ \midrule
   \multirow{5}{*}{LoCA}                        & Frequency Components       & \multicolumn{2}{c}{3000 and 10,000} \\
                              & Learning Rate (ViT)        & \multicolumn{2}{c}{5e-2}      \\
                              & Scaling Value                      & \multicolumn{2}{c}{1 and 0.5}         \\
                              & Learning iterations ($\mathcal{B}_s$) & \multicolumn{2}{c}{120}       \\
                              & Learning Rate (locations)  & \multicolumn{2}{c}{1e-4}      \\ \bottomrule
   \end{tabular}
   \end{table}

   
   \section{Training Procedure} \label{sec:algorithm}
   We provide a pseudo code of our LoCA fine-tuning method in Algorithm \ref{loca_algorithm}.
   \begin{algorithm}
\caption{LoCA Fine-tuning} \label{loca_algorithm}
\begin{algorithmic}[1]
\Require Pre-trained weight $W_0$, dataset $\mathcal{D}$, learning rates $\eta_a$, $\eta_l$, number of alternating iterations $\mathcal{B}_s$, number of coefficient update steps $\mathcal{B}_a$, number of location update steps $\mathcal{B}_l$, total iterations $T$, scaling factor $\alpha$
\Ensure Fine-tuned weight $W'$
\State Initialize $\bm{a} \leftarrow 0$, $\bm{l}$ randomly
\For{$t = 1$ to $T$}
    \State Sample a mini-batch $\mathcal{D}$ and compute the training loss $\mathcal{L}$ 
    \If{$t \leq \bm{B}_s$}
        \If{$t \bmod (\bm{B}_a + \bm{B}_l) < \bm{B}_a$}
            \State Update $\bm{a}$ by $\bm{a} \leftarrow \bm{a} - \eta_a \nabla_a \mathcal{L}$
        \Else
            \State Update $\bm{l}$ by $\bm{l} \leftarrow \bm{l} - \eta_l \frac{\partial \mathcal{L}}{\partial \bm{l}}$ using Eq. (\ref{eq:position_gradient2})
        \EndIf
    \Else
        \State Update $\bm{a}$ by $\bm{a} \leftarrow \bm{a} - \eta_a \nabla_a \mathcal{L}$
    \EndIf
\EndFor
\State \Return $W' = W_0 + \alpha[C^T \mathcal{S}(\bm{a}, \bm{l}, 1)D]$
\end{algorithmic}
\end{algorithm}
   
   \section{Derivation of Proposition \ref{normalmatrixprop}}
       Given any parameter $\overline{W}$ for a down-stream dataset, we assume that the M-estimator $W^\prime$ has asymptotic normality, the estimation error $W^{\prime}-\overline{W}$ is independent of $\overline{W}$ and are asymptotically independent and identically distributed, which can be specified as
       \begin{equation}
           \sqrt{n^\prime}\left(W^\prime-\overline{W} \right)^V\mid\overline{W}\overset{d.}{\rightarrow}\mathcal{N}_{K^2}\left( 0,\sigma_0^2I_{K^2} \right),\label{asymptoticnormalofWprime}
       \end{equation}
   where $n^{\prime}$ is the number of samples in the dataset, $K$ is the width (length) of the weight matrix and $\sigma_0>0$ is a constant independent of $\overline{W}$.
   
       % \begin{lemma}
       %     Assume for random variable sequence $X_1,X_2,\ldots$ and parameterized function $g(X_n,s)$ satisfies $\sqrt{n}g(X_n,s)\overset{d.}{\rightarrow}\mathcal{N}_k(0,I_k),\forall s\in\mathcal{S}$, then for any given random variable $S$ taking values in $\mathcal{S}$ and independent of $X_n$ we can generalize that $\sqrt{n}g(X_n,S)\overset{d.}{\rightarrow}\mathcal{N}_k(0,I_k)$.
       % \end{lemma}
        \begin{lemma}
        % Let $X_1,X_2,\ldots$ be a sequence of $k$-dimensional random variables and $g(X_n,s)$ be a parameterized function with $\mathcal{S}$ as its parameter space, such that for all $s \in \mathcal{S}$,
        % $\sqrt{n}g(X_n,s)\overset{d.}{\rightarrow}\mathcal{N}_k(0,I_k)$. Then, for any random variable $S$ taking values in $\mathcal{S}$ and independent of ${X_n}$, we have
        % $\sqrt{n}g(X_n,S)\overset{d.}{\rightarrow}\mathcal{N}_k(0,I_k)$
        Let $X_1, X_2, \ldots$ be a sequence of $k$-dimensional random variables, and let $g(X_n, s)$ be a parameterized function with parameter space $\mathcal{S}$, such that for all $s \in \mathcal{S}$,
$\sqrt{n}g(X_n, s) \overset{d}{\rightarrow} \mathcal{N}_k(0, I_k)$.
Then, for any random variable $S$ taking values in $\mathcal{S}$ and independent of ${X_n}$, we have
$\sqrt{n}g(X_n, S) \overset{d}{\rightarrow} \mathcal{N}_k(0, I_k)$.
        \end{lemma}
       \begin{proof}
           Fix any point $t\in\mathbb{R}^k$, denote all coordinates of $X_n$ not larger than $t$ by $X_n\leq t$. Assume the distribution of $S$ and $X_n$ are $P_S,P_n$ respectively. Thus
           \begin{align*}
               \mathbb{P}\left( \sqrt{n}g(X_n,S)\leq t \right)=&\int_{\sqrt{n}g(x,s)\leq t}dP_S(s)dP_n(x)\\
               =&\int_{s\in\mathcal{S}}\mathbb{P}\left( \sqrt{n}g(X_n,s)\leq t \right)dP_{S}(s).
           \end{align*}
           As $\sqrt{n}g(X_n,s)\overset{d.}{\rightarrow}\mathcal{N}_k(0,I_k),\forall s\in\mathcal{S}$ implies $\mathbb{P}\left( \sqrt{n}g(X_n,s)\leq t \right)\rightarrow\Phi_k(t),\forall s\in\mathcal{S}$, where $\Phi_k(\cdot)$ is the C.D.F of standard multivariate normal distribution. Based on dominate convergence theorem and $\mathbb{P}\left( \sqrt{n}g(X_n,s)\leq t \right)\leq 1$, we have
           \begin{equation*}
               \mathbb{P}\left( \sqrt{n}g(X_n,S)\leq t \right)\rightarrow\Phi_k(t),
           \end{equation*}
           which is $\sqrt{n}g(X_n,S)\overset{d.}{\rightarrow}\mathcal{N}_k(0,I_k)$.
       \end{proof}
       Note that we can replace $\mathcal{N}_k(0,I_k)$ with any continuous distribution in $\mathbb{R}^k$ and the result still holds. 
       Based on our assumption and Eq. (\ref{asymptoticnormalofWprime}), we consider $\sqrt{n^\prime}\left(W^\prime-\overline{W} \right)^V\mid\overline{W}$ as a random variable parameterized by $\overline{W}$. Therefore, there exists a constant $\sigma_0$ such that we have:
       \begin{equation*}
           \sqrt{n^\prime}\left( W^\prime-\overline{W} \right)^V\overset{d.}{\rightarrow}\mathcal{N}_{K^2}\left( 0,\sigma_0^2I_{K^2} \right),
       \end{equation*}
       in other words,
       \begin{equation}
           \left( W^\prime-\overline{W} \right)^V=\mathcal{N}_{K^2}\left( 0,\dfrac{\sigma_0^2}{n^\prime}I_{K^2} \right)+o_P\left( \dfrac{1}{\sqrt{n^\prime}} \right).\label{asymptoticnormalofWprime1}
       \end{equation}
   
   
       Besides, the assumption gives
       \begin{equation*}
           \left( \overline{W}-\overline{W}_0 \right)^V=\mathcal{N}_{K^2}\left( 0,\overline{\sigma}^2I_{K^2} \right).
       \end{equation*}
       Adding it to Eq. (\ref{asymptoticnormalofWprime1}), we have
       \begin{equation}
           \left( W^\prime-\overline{W}_0 \right)^V=\mathcal{N}_{K^2}\left( 0,\left( \dfrac{\sigma_0^2}{n^\prime}+\overline{\sigma}^2 \right)I_{K^2} \right)+o_P\left( \dfrac{1}{\sqrt{n^\prime}} \right).\label{asymptoticnormalofWprime2}
       \end{equation}
   
   
       On the other hand, $W_0$ is the M-estimator of $\overline{W}_0$ using $N$ samples, we have
       \begin{equation*}
           W_0-\overline{W}_0=O_P\left( \dfrac{1}{\sqrt{N}} \right).
       \end{equation*}
       Combining it with Eq. (\ref{asymptoticnormalofWprime2}) we have
       \begin{equation*}
           \Delta W^V=\left( W^\prime-W_0 \right)^V=\mathcal{N}_{K^2}\left( 0,\left( \dfrac{\sigma_0^2}{n^\prime}+\overline{\sigma}^2 \right)I_{K^2} \right)+o_P\left( \dfrac{1}{\sqrt{n^\prime}} \right)+O_P\left( \dfrac{1}{\sqrt{N}} \right).
       \end{equation*}
   
   
   \section{Proof of Theorem \ref{theorem_1}}
       Before proving the proposed theorem, we first give a proposition.
   
   
       For any matrix $W \in \mathbb{R}^{K \times K}$, let its singular values be $|\lambda_1| \geq \ldots \geq |\lambda_K|$.
Define the discrete Fourier transform of $W$ as $\mathcal{F}(W) = HWH$, where $H \in \mathbb{C}^{K \times K}$ is the DFT matrix. More specifically, we can express $H$ as $H = Re(H) + i Im(H)$, where $i$ is the imaginary unit, and $Re(H), Im(H) \in \mathbb{R}^{K \times K}$ are the real and imaginary coefficients, respectively.
Let $F = (F_{ij})_{1 \leq i,j \leq K} = \mathcal{F}(W)$. 
% we sort the elements of $F$ by magnitude in descending order and denote them as $|F_{(1)}| \geq \ldots \geq |F_{(K^2)}|$.
For each location $(i,j)$, we define a reference matrix $R = (R_{ij})_{1 \leq i,j \leq K}$ as follows:
\begin{equation*}    
R_{ij} = \begin{cases}
-1, & \text{if $F_{ij}$ has a symmetric counterpart and $(i,j)$ satisfies condition $U$} \\
1, & \text{if $F_{ij}$ has a symmetric counterpart but $(i,j)$ does not satisfy condition $U$} \\
0, & \text{otherwise}
\end{cases}
\end{equation*}
Here the condition $U$ is a set of conjugate that
       \begin{gather*}
           \left[ (i=0)\land(j>K-j) \right]\lor\left[ (j=0)\land(i>K-i) \right]\lor~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\left[ (j>0)\land(j>K-j) \right]\lor\left[ (j=n-j)\land(i>K-i) .\right]
       \end{gather*}
       We then define the half matrix of $F$ by $F^H=(F^H_{ij})_{1\leq i,j\leq K}$, where
       \begin{equation*}
           F^H_{ij}=\left\{ 2\mathbbm{1}(R_{ij}=1)+\mathbbm{1}(R_{ij}=0) \right\}|F_{ij}|^2.
       \end{equation*}
       Similarly, we define the real and imaginary part half matrix of $F$ by $F^R$ and $F^I$, where
       \begin{gather*}
           F^R_{ij}=\left\{ 2\mathbbm{1}(R_{ij}=1)+\mathbbm{1}(R_{ij}=0) \right\}Re(F_{ij})^2,\\
           F^I_{ij}=\left\{ 2\mathbbm{1}(R_{ij}=1)+\mathbbm{1}(R_{ij}=0) \right\}Im(F_{ij})^2.
       \end{gather*}
       Based on the definition, we have $F^H=F^R+F^I$. We then sort $F^H$ in descending order, denoting it as $F^H_{(1)}\geq\ldots\geq F^H_{(K^2)}=0$. It can be inferred that approximately half of these elements are equal to 0. Consider the separate matrix $F^S=(F^R,F^I)\in\mathbb{R}^{K\times 2K}$, and also sort it in descending order, denoted as $F^S_{(1)}\geq\ldots\geq F^S_{(2K^2)}=0$. There are also about half of these elements equal to 0. For the simplicity of notations, we define $L_R=\mathbb{E}_{W\sim G}L(W,\hat{W}_R),L_F^{(i)}=\mathbb{E}_{W\sim G}L(W,\hat{W}_F^{(i)})$ for $i=1,2,3$. Denote $\widetilde{Id}^{(1)}$ be the set of locations that are symmetric counterparts of $Id^{(1)}$.
       \begin{proposition}
           With the notations defined above, for $r<K$, we have
           \begin{gather*}
               L_R=\overset{K}{\underset{i=K-r+1}\sum}|\lambda_i|^2,\\
               L_F^{(1)}=\overset{}{\underset{(i,j)\notin Id^{(1)}\cup \widetilde{Id}^{(1)}}\sum}|F_{ij}|^2,\ L_F^{(2)}=\overset{K^2}{\underset{i=N_2+1}\sum}F^H_{(i)},\ L_F^{(3)}=\overset{2K^2}{\underset{i=N_3+1}\sum}F^S_{(i)},\\
               s.t.\ \ \ ||W||_2^2=||F||_2^2=\overset{K}{\underset{i=1}\sum}|\lambda_i|^2=\overset{K}{\underset{i=1}\sum}\overset{K}{\underset{j=1}\sum}|F_{ij}|^2=\overset{K^2}{\underset{i=1}\sum}F_{(i)}^H=\overset{2K^2}{\underset{i=1}\sum}F^S_{(i)},
           \end{gather*}
           % where for $id=(i,j)$, $F_{id}$ means $F_{ij}$.
       \end{proposition}
       \begin{proof}
           First let us explore the reconstruction loss of low rank approximation. For any $W\in\mathbb{R}^{K\times K}$, its SVD decomposition is given by
       \begin{gather*}
           W=U\Lambda V^T,\ \Lambda=diag(\lambda_1,\ldots,\lambda_K),\\
           U^TU=V^TV=I_K,\ |\lambda_1|\geq\ldots\geq|\lambda_K|.
       \end{gather*}
       The best $\hat{W}_R$ that minimize the reconstruction loss in terms of Frobenius norm is
       \begin{gather*}
           \hat{W}_R=\hat{U}\hat{V}^T,\ \hat{U}=U\Lambda_r^{1/2},\ \hat{V}=V\Lambda_r^{1/2},\\
           \Lambda_r=\left( diag(\lambda_1,\ldots,\lambda_r),0_{r\times(K-r)} \right)^T.
       \end{gather*}
       Thus we can easily calculate the reconstruction loss
       \begin{align*}
           L_R = ||W-\hat{W}_R||_2^2&=||U(\Lambda-\Lambda_r)V^T||_2^2\\
           &=tr\left( \left\{U(\Lambda-\Lambda_r)V^T\right\}^T\left\{U(\Lambda-\Lambda_r)V^T\right\} \right)\\
           &=tr\left( (\Lambda-\Lambda_r)^T(\Lambda-\Lambda_r) \right)\\
           &=\overset{K}{\underset{i=K-r+1}\sum}|\lambda_i|^2.
       \end{align*}
       Before moving on to $L_F^{(i)},i=1,2,3$, we introduce discrete Parseval theorem first.
   
   
       \begin{lemma}[Discrete Parseval Theorem]
           For a matrix $X$ of size $K\times K$, with its Discrete Fourier Transform (DFT) denoted by $F$, the sum of the squares of the elements in the original matrix is equal to the sum of the squares of the elements in the DFT matrix, scaled by $1/K$. Formally, if $X$ is the original matrix and $F$ is its DFT, then:
   
           \begin{equation*}
               ||X||_2^2=\sum_{i=0}^{K-1} \sum_{j=0}^{K-1} |X_{ij}|^2 = \frac{1}{K} \sum_{i=0}^{K-1} \sum_{j=0}^{K-1} |F_{ij}|^2=\dfrac{1}{K}||F||_2^2.
           \end{equation*}
       \end{lemma}
   
   
       Since $F=\mathcal{F}(W),W=\mathcal{F}^{-1}(F)$, and Fourier transform is linear transform, we have
       \begin{align*}
           L_F^{(i)}&=||W-\hat{W}_F^{(i)}||_2^2=||W-\mathcal{F}^{-1}(\hat{F}^{(i)})||_2^2\\
           &=||\mathcal{F}^{-1}(F)-\mathcal{F}^{-1}(\hat{F}^{(i)})||_2^2\\
           \text{linearity of Fourier Transformation }&=||\mathcal{F}^{-1}(F-\hat{F}^{(i)})||_2^2\\
           \text{Parseval Theorem }&=||F-\hat{F}^{(i)}||_2^2.
       \end{align*}
       Check $i=1,2,3$ separately and we have
       \begin{equation*}
           L_F^{(1)}=\overset{}{\underset{(i,j)\notin Id^{(1)}\cup\widetilde{Id}^{(1)}}\sum}|F_{ij}|^2,\ L_F^{(2)}=\overset{K^2}{\underset{i=N_2+1}\sum}F_{(i)}^H,\ L_F^{(3)}=\overset{2K^2}{\underset{i=N_3+1}\sum}F^S_{(i)}.
       \end{equation*}
       \end{proof}
   
   
       As we assume $W\sim \mathcal{N}_{K,K}(0,I_K,I_K)$, we then define $A=W^TW\sim W_K(K,I_K,0)$, which follows a central Wishart distribution. Recall the SVD of $W$, i.e., $W=U\Lambda V^T$, and
       \begin{equation*}
           A=W^TW=V\Lambda^2V^T,\ \Lambda^2=diag(\lambda_1^2,\ldots,\lambda_K^2),
       \end{equation*}
       we can conclude that $\lambda_i^\prime=\lambda_i^2$ is the eigenvalue of the matrix that follows $W_K(K,I_K,0)$ distribution.
   
       Next we present a commonly used result about the Wishart distribution in random matrix theory.
       \begin{lemma}\label{WishartEigenvalueDensity}
           The joint density of $\Lambda^2=diag(\lambda_1^\prime,\ldots,\lambda_K^\prime)=diag(\lambda_1^2,\ldots,\lambda_K^2)$ is
           \begin{equation*}
               g_L(\Lambda^2)=C\left[ \overset{K}{\underset{i=1}{\prod}}\lambda_i^{\prime-1/2}e^{-\lambda_i^\prime/2} \right]\left[ \overset{}{\underset{i<j}{\prod}}|\lambda_i^\prime-\lambda_j^\prime| \right].
           \end{equation*}
       \end{lemma}
   
   
       Noting that Lemma \ref{WishartEigenvalueDensity} is a direct corollary of Weyl's Integration Formula in Lemma \ref{Weyl's Integration Formula} and \ref{exchangeableOrderStatistics}.
       \begin{lemma}\label{Weyl's Integration Formula}\citep{brocker2013representations}.
           If $X\in\mathbb{R}^{K\times K}$ is a real symmetric random matrix with density $g(\lambda_1^\prime,\ldots,\lambda_K^\prime)$, where $g$ is exchangeable, and $\lambda_1^\prime,\ldots,\lambda_K^\prime$ are eigenvalues. Thus the joint density of $(\lambda_1^\prime,\ldots,\lambda_K^\prime)$ is
           \begin{equation*}
               f^\prime(\lambda_1^\prime,\ldots,\lambda_K^\prime)=Cg(\lambda_1^\prime,\ldots,\lambda_K^\prime)\overset{}{\underset{i<j}{\prod}}|\lambda_i^\prime-\lambda_j^\prime|,
           \end{equation*}
           where $C$ is some constant such that
           \begin{equation*}
               \int Cg(\lambda_1^\prime,\ldots,\lambda_K^\prime)\overset{}{\underset{i<j}{\prod}}|\lambda_i^\prime-\lambda_j^\prime|d\lambda_1^\prime\ldots d\lambda_K^\prime=1.
           \end{equation*}
       \end{lemma}
       \textbf{Remark.} Exchangeable function $g$ means for any permutation $\pi:[K]\rightarrow [K]$ and $\lambda_1^\prime,\ldots,\lambda_K^\prime$,
       \begin{equation*}
           g(\lambda_1^\prime,\ldots,\lambda_K^\prime)=g(\lambda_{\pi(1)}^\prime,\ldots,\lambda_{\pi(K)}^\prime).
       \end{equation*}
   
   
       Wishart distribution $W_K(K,I_K,0)$ has density
       \begin{equation*}
           g(A)=\dfrac{|A|^{-1/2}\exp\left\{ -tr(A)/2 \right\}}{2^{K^2/2}\pi^{K(K-1)/4}\overset{K}{\underset{i=1}{\prod}}\Gamma((K-i+1)/2)},
       \end{equation*}
       where 
       \begin{gather*}
           |A|^{-1/2}=\left( \overset{K}{\underset{i=1}{\prod}}\lambda_i^\prime \right)^{-1/2}=\overset{K}{\underset{i=1}{\prod}}\lambda_i^{-1},\\
           tr(A)=\overset{K}{\underset{i=1}\sum}\lambda_i^\prime=\overset{K}{\underset{i=1}\sum}\lambda_i^2.
       \end{gather*}
       This directly yields an unordered version of the result in Lemma \ref{WishartEigenvalueDensity}. Specifically, let $\lambda_1^\prime, \ldots, \lambda_K^\prime$ be the unordered eigenvalues of $A$. To avoid confusion, we denote these unordered eigenvalues as $\tilde{\Lambda}^2 = (\tilde{\lambda}_1^\prime, \ldots, \tilde{\lambda}_K^\prime)$. Their joint density function is given by:
       \begin{equation}
           \tilde{g}_L(\tilde{\Lambda}^2)=\tilde{C}\left[ \overset{K}{\underset{i=1}{\prod}}\tilde{\lambda}_i^{\prime-1/2}e^{-\tilde{\lambda}_i^\prime/2} \right]\left[ \overset{}{\underset{i<j}{\prod}}|\tilde{\lambda}_i^\prime-\tilde{\lambda}_j^\prime| \right].\label{unorderedEigenvalue}
       \end{equation}
   
   
       Note that in the density function of $\Lambda^2$, all $\lambda_1^\prime,\ldots,\lambda_K^\prime$ are exchangeable, and for exchangeable random variables we have Lemma \ref{exchangeableOrderStatistics}.
       \begin{lemma}\label{exchangeableOrderStatistics}
           For any $K$ exchangeable variables $X_1,\ldots,X_K$, which means for any permutation $\pi:[K]\rightarrow [K]$, the following equation holds,
           \begin{equation*}
               (X_1,\ldots,X_K)\overset{d.}{=}(X_{\pi(1)},\ldots,X_{\pi(K)}).
           \end{equation*}
            Let $g$ be the density function of $X_1,\ldots,X_K$. Denote their order statistics as $X_{(1)} \geq \ldots \geq X_{(K)}$. If we use $\overline{g}$ to represent the joint distribution of these order statistics, then we have:
            \begin{equation*}
               \overline{g}(x_{(1)},\ldots,x_{(K)})=K!g(x_1,\ldots,x_K).
           \end{equation*}
       \end{lemma}
   
   
       Based on Lemma \ref{exchangeableOrderStatistics} and Eq. (\ref{unorderedEigenvalue}), let $g_L$ denote the density function of the random variables with joint density $\tilde{g}_L$, and we finally have
       \begin{equation*}
           g_L(\Lambda^2)=C\left[ \overset{K}{\underset{i=1}{\prod}}\lambda_i^{\prime-1/2}e^{-\lambda_i^\prime/2} \right]\left[ \overset{}{\underset{i<j}{\prod}}|\lambda_i^\prime-\lambda_j^\prime| \right],
       \end{equation*}
       where the constant $C$ has following representation \citep{muirhead2009aspects}:
       \begin{equation*}
           C=\left( \dfrac{\pi}{2} \right)^{K^2/2}\dfrac{1}{\Gamma_K^2(K/2)},
       \end{equation*}
       here $\Gamma_p(a)$ is the multivariate gamma function. To summarize, we can calculate $L_R$ by taking expectation over distribution $g_L$,
       \begin{equation*}
           L_R=\int\overset{K}{\underset{i=K-r+1}\sum}\lambda_i^\prime g_L(\Lambda^2)d\lambda_1^\prime\ldots d\lambda_K^\prime.
       \end{equation*}
   
   
       Note that if $K/2\in\mathbb{N}$, there are in total $C_{K^2/2+2}^{Kr}$ possible choice of $Id^{(1)}$ with equal probability.
       \begin{align*}
           &\mathbb{E}_{Id^{(1)}}\left[ \mathbb{E}_{W\sim G}\left( K^2-L_F^{(1)} \right) \right]\\
           =&\dfrac{1}{C_{K^2/2+2}^{Kr}}\overset{}{\underset{Id^{(1)}}\sum}\mathbb{E}_{W\sim G}\left( K^2-L_F^{(1)} \right)\\
           =&\dfrac{1}{C_{K^2/2+2}^{Kr}}\overset{}{\underset{Id^{(1)}}\sum}\mathbb{E}_{W\sim G}\left\{ \overset{}{\underset{id\in Id^{(1)}\cup\widetilde{Id}^{(1)}}\sum}|F_{id}|^2 \right\}\\
           =&\dfrac{1}{C_{K^2/2+2}^{Kr}}\overset{}{\underset{Id^{(1)}}\sum}\mathbb{E}_{W\sim G}\left[ \overset{K}{\underset{i=1}\sum}\overset{K}{\underset{j=1}\sum}|F_{ij}|^2\mathbbm{1}\left\{ (i,j)\in Id^{(1)}\cup\widetilde{Id}^{(1)} \right\} \right]\\
           =&\dfrac{1}{C_{K^2/2+2}^{Kr}}\mathbb{E}_{W\sim G}\left[ \overset{K}{\underset{i=1}\sum}\overset{K}{\underset{j=1}\sum}|F_{ij}|^2\overset{}{\underset{Id^{(1)}}\sum}\mathbbm{1}\left\{ (i,j)\in Id^{(1)}\cup\widetilde{Id}^{(1)} \right\} \right]\\
           =&\dfrac{C_{K^2/2+1}^{Kr-1}}{C_{K^2/2+2}^{Kr}}\mathbb{E}_{W\sim G}\left( \overset{K}{\underset{i=1}\sum}\overset{K}{\underset{j=1}\sum}|F_{ij}|^2 \right)\\
           =&\dfrac{K^3r}{K^2/2+2}<2Kr,
       \end{align*}
       which aligns with intuition that random choice gives average performance. Similarly, if $(K+1)/2\in\mathbb{N}$, there are in total $C_{(K^2+1)/2}^{Kr}$ possible choices of $Id^{(1)}$ with equal probability. And 
       \begin{align*}
           \mathbb{E}_{Id^{(1)}}\mathbb{E}_{W\sim G}\left( K^2-L_F^{(1)} \right)=\dfrac{K^3r}{(K^2+1)/2}<2Kr.
       \end{align*}
   
       On the other hand,
       \begin{align*}
           \mathbb{E}_{W\sim G}\left( K^2-L_R \right)&=\mathbb{E}_{W\sim G}\left( \overset{r}{\underset{i=1}\sum}|\lambda_i|^2 \right)\\
           &=\int g_L(\Lambda^2)\overset{r}{\underset{i=1}\sum}\lambda_i^\prime d\lambda_K^\prime\ldots d\lambda_1^\prime.
       \end{align*}
       % The inequality comes from $|\lambda_1|\geq\ldots\geq|\lambda_K|$, and only takes equality when all $|\lambda_i|$ are exactly the same, which is of probability less than 1. Thus we get Theorem \ref{underachieve},
       % \begin{equation*}
       %     \mathbb{E}_{Id^{(1)}}\mathbb{E}_{W^V\sim N_{n^2}(0,I_{n^2})}L_F^{(1)}>1-\dfrac{k}{n}>\mathbb{E}_{W^V\sim N_{n^2}(0,I_{n^2})}L_R.
       % \end{equation*}
       This calculation is complicated and does not have a closed-form expression. Next, we demonstrate
       \begin{equation*}
           \mathbb{E}_{W\sim G}\left(K^2-L_R\right)>\mathbb{E}_{Id^{(1)}}\mathbb{E}_{W\sim G}\left(K^2-L_F^{(1)}\right).
       \end{equation*}
       We begin by proving that this inequality holds for the case where $r=1$ and $K$ is sufficiently large. Following this, we extend our analysis by numerically approximating the exact values of the integrals for various combinations of $r$ and $K$.       
       We first prove that for $r=1$ and sufficiently large $K$, the inequality $\mathbb{E}_{W\sim G}|\lambda_1|^2=\mathbb{E}_{W\sim G}\lambda_1^\prime>2Kr$ holds. $\lambda_1^\prime,\ldots,\lambda_K^\prime$ has density
       \begin{equation}
           g_L(\Lambda^2)=\left( \dfrac{\pi}{2} \right)^{K^2/2}\dfrac{1}{\Gamma_K^2(K/2)}\left[ \overset{K}{\underset{i=1}{\prod}}\lambda_i^{\prime-1/2}e^{-\lambda_i^\prime/2} \right]\left[ \overset{}{\underset{i<j}{\prod}}|\lambda_i^\prime-\lambda_j^\prime| \right],\label{glgamma}
       \end{equation}
       and $\lambda_1^\prime$ is the largest eigenvalue of a standard Wishart ensemble. We refer to the large deviation result under this circumstance that for large $K$ there exists $c\leq 1$ and
       \begin{equation}
           \lambda_1^\prime=\left( \dfrac{1}{\sqrt{c}}+1 \right)^2K+c^{1/6}\left( \dfrac{1}{\sqrt{c}}+1 \right)^{4/3}K^{1/3}\chi,\label{maxeigenvaluecalcu}
       \end{equation}
       where the random variable $\chi$ has an $K$-independent limiting distribution, which is Tracy-Widom distribution \citep{vivo2007large,johnstone2001distribution,johansson2000shape}. Take expectation on both sides of Eq. (\ref{maxeigenvaluecalcu}) and
       \begin{equation*}
           \mathbb{E}_{W\sim G}\left( K^2-L_R \right)=\mathbb{E}\lambda_1^\prime=\left( \dfrac{1}{\sqrt{c}}+1 \right)^2K+O(K^{1/3}).
       \end{equation*}
       Thus $\dfrac{\mathbb{E}_{W\sim G}\left( 1-L_R \right)}{K}\rightarrow\left( \dfrac{1}{\sqrt{c}}+1 \right)^2\geq 4>2$, which concludes the first inequality in Theorem \ref{theorem_1}. For $r=1$ but not sufficiently large $K$, we directly calculate the $\mathbb{E}\lambda_1^\prime$ and compare it with $2K$. For $r>1$ we can apply similar analysis but that will be much more complex. We demonstrate the result in later numerical approximation (Fig. \ref{fig:sim_1} and \ref{fig:sim_2}).
   
   
       Now we turn to $L_F^{(i)},i=1,2,3$. Remember we have
       \begin{align*}
           \mathcal{F}(W)&=\left\{ Re(H)+\bm{i}Im(H) \right\}W\left\{ Re(H)+\bm{i}Im(H) \right\}\\
           &=\left\{ Re(H)WRe(H)-Im(H)WIm(H) \right\}+\bm{i}\left\{ Im(H)WRe(H)+Re(H)WIm(H) \right\}\\
           &=Re(\mathcal{F}(W))+\bm{i}Im(\mathcal{F}(W)).
       \end{align*}
   
   
       After vectorization,
       \begin{gather*}
           Re(\mathcal{F}(W))^V=\left\{ Re(H)\otimes Re(H)-Im(H)\otimes Im(H) \right\}W^V,\\
           Im(\mathcal{F}(W))^V=\left\{ Re(H)\otimes Im(H)+Im(H)\otimes Re(H) \right\}W^V.
       \end{gather*}
       As $W^V\sim N_{K^2}\left( 0,I_{K^2} \right)$, and the linear transform of multivariate normal is still normal, we have
       \begin{gather*}
           Re(\mathcal{F}(W))^V\sim N_{n^2}(0,\Sigma_R),Im(\mathcal{F}(W))^V\sim N_{n^2}(0,\Sigma_I), \text{where}\\
           \Sigma_R=\left\{ Re(H)\otimes Re(H)-Im(H)\otimes Im(H) \right\}\left\{ Re(H)\otimes Re(H)-Im(H)\otimes Im(H) \right\}^T,\\
           \Sigma_I=\left\{ Re(H)\otimes Im(H)+Im(H)\otimes Re(H) \right\}\left\{ Re(H)\otimes Im(H)+Im(H)\otimes Re(H) \right\}^T.
       \end{gather*}
       Next we propose that $Re(H)Im(H)=0$.
   
   
       \begin{lemma}
           For any $K$, $H$ is the 2d DFT $K\times K$ matrix defined by
           \begin{equation*}
               H_{u,v}=\dfrac{1}{\sqrt{K}}\left\{ \cos(2\pi uv/K)-\bm{i}\sin(2\pi uv/K) \right\}\,,
           \end{equation*}
           we have $Re(H)Im(H)=0$.
       \end{lemma}
       \begin{proof}
           First, let us denote the real part $R$ and the imaginary part $I$ of $H$ as follows:
           \begin{equation*}
               R_{u,v} = \dfrac{1}{\sqrt{K}} \cos\left(\dfrac{2\pi uv}{K}\right),I_{u,v} = -\dfrac{1}{\sqrt{K}} \sin\left(\dfrac{2\pi uv}{K}\right)
           \end{equation*}
   
           We calculate the matrix product $R \cdot I$, where $R$ and $I$ are $K \times K$ matrices. The element of the resulting matrix at location $(u, w)$ is given by:
           \begin{equation*}
               (RI)_{u,w} = \overset{K-1}{\underset{v=0}\sum} R_{u,v} I_{v,w}.
           \end{equation*}
           Substituting the expressions for $R$ and $I$:
           \begin{align*}
               (RI)_{u,w} &= \overset{K-1}{\underset{v=0}\sum} \left\{ \dfrac{1}{\sqrt{K}} \cos\left(\dfrac{2\pi uv}{K}\right) \right\} \left\{ -\dfrac{1}{\sqrt{K}} \sin\left(\dfrac{2\pi vw}{K}\right) \right\}\\
               &=-\dfrac{1}{K} \overset{K-1}{\underset{v=0}\sum} \cos\left(\dfrac{2\pi uv}{K}\right) \sin\left(\dfrac{2\pi vw}{K}\right).
           \end{align*}
           Next, we use the trigonometric identity that $\cos(x) \sin(y) = \left[ \sin(x + y) - \sin(x - y) \right]/2$. Applying this identity, we have
           \begin{equation*}
               \cos\left(\dfrac{2\pi uv}{K}\right) \sin\left(\dfrac{2\pi vw}{K}\right) = \frac{1}{2} \left\{ \sin\left(\dfrac{2\pi uv}{K} + \dfrac{2\pi vw}{K}\right) - \sin\left(\dfrac{2\pi uv}{K} - \dfrac{2\pi vw}{K}\right) \right\}\,.
           \end{equation*}
           Substituting back, we get
           \begin{equation*}
               (RI)_{u,w} = -\dfrac{1}{2K} \overset{K-1}{\underset{v=0}\sum}\left\{ \sin\left(\dfrac{2\pi (u+ w)v}{K}\right) - \sin\left(\dfrac{2\pi (u - w)v}{K}\right) \right\}=0\,.
           \end{equation*}
       \end{proof}
   
   
       This lemma gives $Re(H)Im(H)=Im(H)Re(H)=0$. Therefore
       \begin{align*}
           &\left\{ Re(H)\otimes Re(H)-Im(H)\otimes Im(H) \right\}\left\{ Re(H)\otimes Im(H)+Im(H)\otimes Re(H) \right\}\\
           =&\left\{ Re(H) \right\}^2\otimes Re(H)Im(H)+Re(H)Im(H)\otimes \left\{ Re(H) \right\}^2-\\
           &Im(H)Re(H)\otimes \left\{ Im(H) \right\}^2-\left\{ Im(H) \right\}^2\otimes Im(H)Re(H)\\
           =&0,
       \end{align*}
       which indicates $\Sigma_R\Sigma_I=0$, due to the normality, $Re(\mathcal{F}(W))$ is independent of $Im(\mathcal{F}(W))$. $H$ has slightly different property when $K$ is an odd or even number. For the simplicity of proof, we assume $K/2\in\mathbb{N}$, the odd case can be proved similarly.
   
   
       \begin{lemma}\label{reimform}
           When $K/2\in\mathbb{N}$, $Re(H)Re(H)^T$ and $Im(H)Im(H)^T$ have the following property:
           \begin{gather*}
               \left\{ Re(H)Re(H)^T \right\}_{u,w}=\left\{
                   \begin{aligned}
                   &1,\ u=w=0,K/2,\\
                   &\dfrac{1}{2},\ u=w\neq 0,K/2,\\
                   &\dfrac{1}{2},\ u\neq w,\ u+w=K,\\
                   &0,\ otherwise.
                   \end{aligned}
               \right.\\
               \left\{ Im(H)Im(H)^T \right\}_{u,w}=\left\{
                   \begin{aligned}
                   &0,\ u=w=0,K/2,\\
                   &\dfrac{1}{2},\ u=w\neq 0,K/2,\\
                   &-\dfrac{1}{2},\ u\neq w,\ u+w=K,\\
                   &0,\ otherwise.
                   \end{aligned}
               \right.
           \end{gather*}
       \end{lemma}
       \begin{proof}
           Follow previous notations,
           \begin{equation*}
               \left( RR^T \right)_{u,w}=\dfrac{1}{2K}\overset{K-1}{\underset{v=0}\sum}\left\{ \cos\left( \dfrac{2\pi (u+w)v}{K} \right)+\cos\left( \dfrac{2\pi (u-w)v}{K} \right) \right\}\,.
           \end{equation*}
           First we get $\left( RR^T \right)_{0,0}=\left( RR^T \right)_{K/2,K/2}=1$. When $u=w\neq 0,K/2$,
           \begin{equation*}
               \left( RR^T \right)_{u,w}=\dfrac{1}{2K}\overset{K-1}{\underset{v=0}\sum}\cos\left( \dfrac{2\pi (u+w)v}{K} \right)+\dfrac{1}{2}=\dfrac{1}{2},
           \end{equation*}
           since $K\nmid (u+w)$. When $u\neq w$ but $u+w=K$,
           \begin{equation*}
               \left( RR^T \right)_{u,w}=\dfrac{1}{2K}\overset{K-1}{\underset{v=0}\sum}\cos\left( \dfrac{2\pi (u-w)v}{K} \right)+\dfrac{1}{2}=\dfrac{1}{2},
           \end{equation*}
           since $K\nmid (u-w)$. For other $u,w$, it is easy to derive $\left( RR^T \right)_{u,w}=0$. 
           
           
           Moreover, $H\overline{H}^T=I_K$, where $\overline{\cdot}$ means conjugation, indicating that $RR^T+II^T=I_K$, and we get the result for $II^T$.
       \end{proof}
   
   
       As $Re(H)Im(H)=Im(H)Re(H)=0$, we can calculate
       \begin{gather*}
           \Sigma_R=\left\{ Re(H)Re(H)^T \right\}\otimes\left\{ Re(H)Re(H)^T \right\}+\left\{ Im(H)Im(H)^T \right\}\otimes\left\{ Im(H)Im(H)^T \right\},\\
           \Sigma_I=\left\{ Re(H)Re(H)^T \right\}\otimes\left\{ Im(H)Im(H)^T \right\}+\left\{ Im(H)Im(H)^T \right\}\otimes\left\{ Re(H)Re(H)^T \right\}.
       \end{gather*}
       Based on Lemma (\ref{reimform}), we can assert that there are $4$ locations in $\Sigma_R$ containing the element $1$. These locations are $(0,0)$, $(K/2,K/2)$, $(K^2/2,K^2/2)$, and $((K^2+K)/2,(K^2+K)/2)$. Excluding rows and columns $0$, $K/2$, $K^2/2$, and $(K^2+K)/2$, each of the remaining rows and columns contains $2$ locations with the value $0.5$. There exists a row permutation matrix $U\in\mathbb{R}^{K\times K}$, such that
       \begin{gather}
           U\Sigma_RU^T=\left(
                   \begin{array}{ccccc}
                       I_4 & & & &  \\
                        & \Delta_2 & & &  \\
                        & & \Delta_2 & & \\
                        & & & \ddots & \\
                        & & & & \Delta_2
                   \end{array}
               \right),\ \ 
           \Delta_2=\left(\begin{array}{cc}
               0.5 & 0.5 \\
               0.5 & 0.5
           \end{array}\right).\label{sigmar}
       \end{gather}
       Since
       \begin{align*}
           \Sigma_R+\Sigma_I&=\left\{ Re(H)Re(H)^T+Im(H)Im(H)^T \right\}\otimes\left\{ Re(H)Re(H)^T+Im(H)Im(H)^T \right\}\\
           &=\left( H\overline{H}^T \right)\otimes \left( H\overline{H}^T \right)=I_K\otimes I_K=I_{K^2},
       \end{align*}
       we have similar results on $\Sigma_I$ that 
       \begin{gather}
           U\Sigma_IU^T=\left(
                   \begin{array}{ccccc}
                       {\bf{0}}_4 & & & &  \\
                        & \Delta_2^- & & &  \\
                        & & \Delta_2^- & & \\
                        & & & \ddots & \\
                        & & & & \Delta_2^-
                   \end{array}
               \right),\ \ 
           \Delta_2^-=\left(\begin{array}{cc}
               0.5 & -0.5 \\
               -0.5 & 0.5
           \end{array}\right).\label{sigmai}
       \end{gather}
       This analysis aligns with the definitions of $F^R$ and $F^I$. Given that $W^V$ follows a standard normal distribution and $\Sigma_R\Sigma_I = 0$, we can represent $\Sigma_R$ and $\Sigma_I$ as shown in Eq. (\ref{sigmar}) and Eq. (\ref{sigmai}), respectively. Let $R$ be the reference matrix, for $i,j$ with $R_{ij}=0$, the $i,j$-th element corresponds to the element with variance 1, and $F_{ij}^R\sim \chi_1^2,F_{ij}^I=0$; for $i,j$ with $R_{ij}=1$, $F_{ij}^R,F_{ij}^I\sim\chi_1^2$; for $i,j$ such that $R_{ij}=-1$, $F_{ij}^R=F_{ij}^I=0$. And for all $i,j$ with $R_{ij}\neq -1$, $F_{ij}^R$ and $F_{ij}^I$ are independent.
       
       When $R_{ij}=0$, $\left|F_{ij}^H\right|^2=Re(F_{ij})^2\sim\chi_1^2$; when $R_{ij}=1$, $\left|F_{ij}^H\right|^2=2Re(F_{ij})^2+2Im(F_{ij})^2\sim\chi_2^2$. Thus we can reformulate $L_F^{(2)}$ and $L_F^{(3)}$ in a more clear way. Define $\psi_1,\ldots,\psi_{K^2}\overset{i.i.d.}{\sim}\chi_1^2$, $\phi_1,\ldots,\phi_{(K^2-4)/2}\overset{i.i.d.}{\sim}\chi_2^2,\phi_{(K^2-2)/2},\ldots,\phi_{(K^2+4)/2}\overset{i.i.d.}{\sim}\chi_1^2$. Denote the order statistics of $\psi_i,\phi_i$ as $\psi_{(1)}\geq\ldots\geq\psi_{(K^2)}$ and $\phi_{(1)}\geq\ldots\geq\phi_{((K^2+4)/2)}$, we then have
       \begin{equation}
           L_F^{(2)}\overset{d.}{=}\overset{(K^2+4)/2}{\underset{i=N_2+1}\sum}\phi_{(i)},\ L_F^{(3)}\overset{d.}{=}\overset{K^2}{\underset{i=N_3+1}\sum}\psi_{(i)}\,,\label{reformulate}
       \end{equation}
       where $\overset{d.}{=}$ means equality in distribution.      
       In other words,
       \begin{equation*}
           \mathbb{E}_{W\sim G}\left( K^2-L_F^{(2)} \right)=\overset{N_2}{\underset{i=1}\sum}\mathbb{E}\phi_{(i)},\ \mathbb{E}_{W\sim G}\left( K^2-L_F^{(3)} \right)=\overset{N_3}{\underset{i=1}\sum}\mathbb{E}\psi_{(i)}\,,
       \end{equation*}
       i.e., $\mathbb{E}_{W\sim G}\left( K^2-L_F^{(3)} \right)$ is the summation of i.i.d. chi square order statistics' expectation. Similarly, we can bound $\mathbb{E}_{W\sim G}\left( K^2-L_F^{(2)} \right)$, by defining
       \begin{equation*}
           \xi_1^{(1)},\ldots,\xi_{(K^2-4)/2}^{(1)},\xi_1^{(2)},\ldots,\xi_{(K^2+4)/2}\sim\chi_2^2,
       \end{equation*}
       and corresponding order statistics
       \begin{equation*}
           \xi_{(1)}^{(1)}\geq\ldots\geq\xi_{((K^2-4)/2)}^{(1)},\ \ \xi_{(1)}^{(2)}\geq\ldots\geq\xi_{((K^2+4)/2)}^{(2)}.
       \end{equation*}
       Define $M_1=\overset{N_2}{\underset{i=1}\sum}\mathbb{E}\xi_{(i)}^{(1)}$ and $M_2=\overset{N_2}{\underset{i=1}\sum}\mathbb{E}\xi_{(i)}^{(2)}$, we have $M_1\leq \mathbb{E}_{W\sim G}\left( K^2-L_F^{(2)} \right)\leq M_2$.
       \begin{lemma}
           For any $n$ i.i.d. random variables with pdf $f(x)$ and cdf $H(x)$, the $l$-th largest order statistic has density $h_l(x)=nC_{n-1}^{l-1}h(x)H(x)^{l-1}\left\{ 1-H(x) \right\}^{n-l}$.
       \end{lemma}
   
   
       We claim that for given $r<K/3$,
       \begin{align}
           \overset{N_3}{\underset{i=1}\sum}\mathbb{E}\psi_{(i)}\geq M_2\geq M_1\geq \int g_L(\Lambda^2)\overset{r}{\underset{i=1}\sum}\lambda_i^\prime d\lambda_K^\prime\ldots d\lambda_1^\prime,\label{finaltarget}
       \end{align}
       where $g_L(\Lambda^2)$ is given in Eq. (\ref{glgamma}). We verify this inequality by numerical calculation, since each item in Eq. (\ref{finaltarget}) is already a closed form integration. Specifically, we compare the ratios $\frac{L}{K^2}$ for various combinations of $K$ and $r$, where $L$ represents $L_R$, $L_F^{(1)}$, $L_F^{(2)}$, and $L_F^{(3)}$. For commonly used $r$ values, we examined $K$ from 100 to 300, while for larger matrices with $K = 768$ and $K = 4096$, we tested $r$ values from 8 to 32. Throughout these tests, we employ specific definitions for the different $L$ values: $L_F^{(1)} = 2Kr$, $K^2 - M_2 \leq L_F^{(2)} \leq K^2 - M_1$, and $L_F^{(3)} = L_D$, with the last definition verified by Theorem \ref{theorem2}. 
   
   
       \textbf{Remark.} Given that the four integrals in Eq. (\ref{finaltarget}) are not easily expressed in a straightforward manner, directly proving the inequality is impractical. Beyond numberical approximation for commonly used $r$ and $K$ in Fig. \ref{fig:sim_1} and \ref{fig:sim_2}, we offer an intuitive illustration to show why the inequality holds.
   
   
       A tight bound on order statistics is given by \citet{arnold1979bounds,bertsimas2006tight}: for $X_1,\cdots,X_n$ i.i.d. with expectation $\mu$ and variance $\sigma^2$, the expectation of $l$-th order statistic is bounded by $\mu+\sigma\sqrt{\dfrac{n-l}{l}}$. Consider using this bound to approximate $\overset{N_3}{\underset{i=1}\sum}\mathbb{E}\psi_{(i)}$ and $M_1,M_2$:
       \begin{gather*}
           n_1=K^2,\mu_1=\mathbb{E}\psi_i=1,\sigma_1=\sqrt{Var(\psi_i)}=\sqrt{2},\\
           n_2=K^2/2+2,\mu_2=\mathbb{E}\xi_i^{(2)}=2,\sigma_2=\sqrt{Var(\xi_i^{(2)})}=2.
       \end{gather*}
       Thus
       \begin{align*}
           2 \overset{2Kr/3}{\underset{i=1}\sum}\sqrt{\dfrac{K^2/2+2-i}{i}}&=\sqrt{2}\overset{2Kr/3}{\underset{i=1}\sum}\sqrt{\dfrac{K^2}{i}+\dfrac{4}{i}-2}\\
           &\approx\sqrt{2}\overset{2Kr/3}{\underset{i=1}\sum}\sqrt{\dfrac{K^2}{i}-1}\\
           &<\sqrt{2}\overset{Kr}{\underset{i=1}\sum}\sqrt{\dfrac{K^2}{i}-1},
       \end{align*}
       which gives the upper bound of $M_2$ is smaller than that of $\overset{N_3}{\underset{i=1}\sum}\mathbb{E}\psi_{(i)}$.
   
   
       \begin{figure}[H]
           \centering
           \includegraphics[width=0.9\linewidth]{Figures/resultr.png}
           \caption{Reconstruction errors of different $r$, $K$ and methods. Specify $r=8,16,24,32$ and $K\in[100,300]$. R denotes the low rank method, the curve is $L_R/K^2$; M1 and M2 denotes $1-M_1/K^2,1-M_2/K^2$ respectively; D denotes $L_F^{(3)}/K^2$ or $L_D/K^2$; U denotes $1-2r/K$.}
           \label{fig:sim_1}
       \end{figure}
       \begin{figure}[H]
           \centering
           \includegraphics[width=0.9\linewidth]{Figures/resultK.png}
           \caption{Reconstruction errors of different $r$, $K$ and methods. Specify $K=768,4096$ and $r\in[8,32]$. R denotes the low rank method, the curve is $L_R/K^2$; M1 and M2 denotes $1-M_1/K^2,1-M_2/K^2$ respectively; D denotes $L_F^{(3)}/K^2$ or $L_D/K^2$; U denotes $1-2r/K$.}
           \label{fig:sim_2}
       \end{figure}
   
   
   \section{Proof of Theorem \ref{theorem2}}
   \begin{proof}
       Theorem \ref{theorem2} is a corollary of Eq. (\ref{reformulate}). For notation simplicity, denote the expectation of reconstruction loss of DCT method as $L_D=\mathbb{E}_{W\sim G}\left\{ L(W,\hat{W}_D) \right\}$. 
   
   
       Denote discrete cosine transform as $D=\mathcal{D}(W)=QWQ^T$, where $Q\in\mathbb{R}^{K\times K}$ is the DCT matrix satisfies $QQ^T=I_K$. Vectorize $D$ we get
       \begin{align*}
           D^V=(Q\otimes Q)W^V\sim\mathcal{N}_{K^2}(0,\Sigma_D),
       \end{align*}
       where $\Sigma_D=(Q\otimes Q)(Q\otimes Q)^T=(QQ^T)\otimes(QQ^T)=I_{K^2}$.
   
   
       Denote the order statistics of $D$'s elements as $D_{(1)}\geq\ldots\geq D_{(K^2)}$. The Parseval theorem also holds for DCT, thus
       \begin{equation*}
           L_D=\mathbb{E}_{W\sim G}\left\{ \overset{K^2}{\underset{i=N_D+1}\sum}|D_{(i)}|^2 \right\}=K^2-\mathbb{E}_{W\sim G}\left\{ \overset{N_D}{\underset{i=1}\sum}|D_{(i)}|^2 \right\}.
       \end{equation*}
       Under the situation of $W\sim G$, $|D_{ij}|^2\sim\chi_1^2$ and $K^2-L_D$ is the expectation of the largest $N_D$ out of $K^2$ random $\chi_1^2$ variables, which exactly equals to the $K^2-L_F^{(3)}$ in Eq. (\ref{reformulate}) when $N_D=N_3$.
   \end{proof}

   \section{Computational Efficiency of Gradient Estimation}
   \label{sec:computation_grad}
   Recall that the 2D iDCT can be represented as $\Delta W = \alpha[C^T \mathcal{S}(\bm{a},\bm{l},\bm{1}) D]$, here $C^T$ and $D$ are iDCT transformation matrices for rows and columns respectively. We show that the gradient of location $\bm{l}$ is computed alongside with the gradient of $\bm{a}$, introducing only negligible additional computations. 

    For ease of representation, we denote the sparse matrix $\mathcal{S}(\bm{a},\bm{l},\bm{1})$ as $W_s$. We first show how a change at location $(i,j)$ in $W_s$ affects location $(m,n)$ in $\Delta W$ \footnote{Here we use $[\cdot,\cdot]$ to denote the index operation on a matrix.}:
    \begin{equation}
    \frac{\partial \Delta W[m, n]}{\partial W_s[i,j]} = \alpha C^T[m,i] D[j,n].
    \end{equation}
    Now, consider $\partial \mathcal{L}/\partial \Delta W$ that we get during backpropagation, if we want to get the gradient of an element in $\bm{a}$ (indexed by $i, j$), we need to compute:
    \begin{equation} \label{eq:appendix_gradient}
    \frac{\partial \mathcal{L}}{\partial W_s[i,j]} = \sum_{m,n} (\frac{\partial \mathcal{L}}{\partial \Delta W[m,n]} \frac{\partial \Delta W[m,n]}{\partial W_s[i,j]}).
    \end{equation}
Expanding Eq. (\ref{eq:appendix_gradient}), we have
\begin{equation}
\frac{\partial \mathcal{L}}{\partial W_s[i,j]} =  \alpha \sum_{m,n} (\frac{\partial \mathcal{L}}{\partial \Delta W[m,n]} C^T[m,i] D[j,n])
= \alpha \underbrace{(D (\frac{\partial \mathcal{L}}{\partial \Delta W})^T C^T)^T}_{DCT, \text{matrix} Z}[i,j].
\end{equation}

Therefore, to get the gradient of $\bm{a}$, we also need to compute the matrix $Z$ in Eq. (\ref{eq:position_gradient2}). The gradient of each element in $\bm{a}$ can be directly indexed by locations, while the gradient of each element in $\bm{l}$ can be estimated according to Section \ref{sec:gradient_estimate}, which will introduce only negligible additional computation. 

   
   \section{Computational Complexity and Memory Cost Comparison} \label{sec:computation}
    As discussed in Section \ref{sec:idct_para}, the original implementation of DCT, i.e., Eq. (\ref{eq:original_dct}) can take two enhanced forms depending on the sparsity of the DCT spectrum: a sparse matrix-based implementation and a fast algorithm-based implementation. The computational complexity of using the sparse matrix implementation is $O(\mathcal{B}pq)$, where $\mathcal{B}$ is the number of frequency components, and $p$ and $q$ are the dimensions of the weight matrix. The fast algorithm-based implementation has a complexity of $O(pq \log(pq))$. It is worth noting that PyTorch currently lacks a specialized fast algorithm for DCT. To address this, we implemented a fast DCT based on FFT. In comparison, LoRA has a complexity of $O(rpq)$, where $r$ is the rank. FourierFT, which utilizes iFFT algorithm, has an asymptotic complexity of $O(pq \log(pq))$.

    From an asymptotic analysis perspective, the fast implementations of LoCA and FourierFT have the same complexity, while the complexity of LoRA is lower when $r < \log(pq)$. However, noting that the practical performance can differ significantly from theoretical asymptotic analysis due to various factors such as implementation details, hardware-specific optimizations, the constant coefficient in computation complexity and the actual values of $\mathcal{B}$, $r$, and $pq$. In our experimental comparisons, we observed that the actual running times of these methods are much closer than the asymptotic analysis might suggest. Table \ref{tab:complexity} presents a comparison of the three methods.

\begin{table}[H]
\centering
\caption{\textcolor{black}{Comparison of actual training speed and memory costs on a single Tesla H100 GPU. LoCA$^1$ represents the sparse matrix-based iDCT implementation, and LoCA$^2$ refers to the fast iDCT implementation based on iFFT. LoCA $^3$ is the DCT implementation in the original matrix multiplication form (default implementation). All experimental configurations are consistent with the ones in main experiments.}}
\label{tab:complexity}
\resizebox{\linewidth}{!}{
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{tabular}{lccccccc}
\toprule
\multirow{2}{*}{Method} & \multirow{2}{*}{Asymptotic Complexity} & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}} MRPC \\ RoBERTa-base\end{tabular}}                                                    & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}Alpaca-52K\\ LLaMA-1-7b\end{tabular}}                                                  & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}StanfordCars\\ ViT-base\end{tabular}}                                                  \\ \cmidrule{3-8} 
                        &                                        & \begin{tabular}[c]{@{}c@{}}Training Speed\\ (iterations/s)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Memory Cost\\ (MB)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Training Speed\\ (iterations/s)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Memory Cost\\ (MB)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Training Speed\\ (iterations/s)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Memory Cost\\ (MB)\end{tabular} \\ \midrule
LoCA$^1$                  &      $O(\mathcal{B} \log(pq))$         & 28.56                                                                   & 3,828                                                      & -                                                                       & -                                                          & 2.28                                                                    & 4,214                                                      \\
LoCA$^2$                  &     $O(pq \log(pq))$                   & 25.12                                                                   & 3,834                                                      & 0.63                                                                    & 57,152                                                     & 1.01                                                                    & 3,782                                                      \\
LoCA$^3$                  &        $O(p^2q^2)$                     & 27.77                                                                   & 3,793                                                      & 0.87                                                                    & 57,888                                                     & 2.33                                                                    & 3,754                                                      \\
FourierFT               &         $O(pq \log(pq))$               & 28.82                                                                   & 4,050                                                      & 0.89                                                                    & 58,868                                                     & 2.35                                                                    & 3,760                                                      \\
LoRA                    &          $O(rpq)$                      & 31.14                                                                   & 3,758                                                      & 1.18                                                                    & 53,154                                                     & 2.78                                                                    & 3,708                                                      \\ \bottomrule
\end{tabular}}
\end{table}

As shown in Table \ref{tab:complexity}, despite the differences in asymptotic complexities, the actual running speeds of LoCA and FourierFT are very close, with LoRA being slightly faster (since the matrix multiplication operation is highly optimized on the GPU). This suggests that the implementation efficiency and hardware utilization play significant roles in practical performance. \textcolor{black}{For the memory consumption, both LoCA and FourierFT exhibit marginally higher memory usage compared to LoRA. However, LoCA consistently maintains a lower memory footprint than FourierFT across all test scenarios.}

Notably, there is still potential for further optimization for our method. Since the current fast DCT implementation is based on FFT, a lot of redundant computation is introduced to construct a DCT into the form of a DFT. A specialized fast DCT algorithm could potentially improve the speed of LoCA even further, potentially bringing its speed closer to or even surpassing that of LoRA while maintaining its superior adaptation capability. For FourierFT, while FFT can handle complex data, it introduces computational redundancies when processing real-valued data, making DCT theoretically superior in terms of both computational speed and memory usage for real-number operations. We leave the development and implementation of such optimizations as future work.
   
\section{Examples of Instruction Tuning Experiments}
Fig. \ref{fig:instruction_example_1} and Fig. \ref{fig:instruction_example_2} show multi-turn conversations on two MT-Bench questions of a fine-tuned LLaMA model using LoCA and a pre-trained LLaMA model. From the dialogue, we can see that the instruction-following ability of the pre-trained LLaMA is quite limited. The first-turn answer does not perfectly address the given question, and in the second round, it barely responds according to the user request at all. However, after LoCA fine-tuning, there is a noticeable improvement in the quality of the answers, which can be seen from the evaluation of GPT-4.

       \begin{figure}[H]
           \centering
           \includegraphics[width=1.0\linewidth]{Figures/Chat.pdf}
           \caption{Question 149 from MT-Bench with the multi-turn interactions of pre-trained LLaMA-7b (left) and fine-tuned LLaMA-7b using LoCA (right). We also attach the single-answer grading GPT-4 judgment for reference.}
           \label{fig:instruction_example_1}
       \end{figure}

       \begin{figure}[H]
           \centering
           \includegraphics[width=1.0\linewidth]{Figures/Chat2.pdf}
           \caption{Question 82 from MT-Bench with the multi-turn interactions of pre-trained LLaMA2-13b (left) and fine-tuned LLaMA2-13b using LoCA (right). We also attach the single-answer grading GPT-4 judgment for reference.}
           \label{fig:instruction_example_2}
       \end{figure}
Fig. \ref{fig:instruction_example_3} shows the responses of LLaMA-2-7b models fine-tuned with LoCA and FourierFT on two open-ended questions from the Vicuna dataset. It can be observed that while both fine-tuned models answered the questions well, the LoCA fine-tuned model provided more detailed and insightful responses, as evidenced by GPT-4's evaluation.

       \begin{figure}[H]
           \centering
           \includegraphics[width=1.0\linewidth]{Figures/Chat3.pdf}
           \caption{Question 6 and 7 from Vicuna-Bench. We present the pairwise-version judgment by GPT-4 to differentiate the performance of our fine-tuned LLaMA2-7b (left) and FourierFT fine-tuned LLaMA2-7b (right).}
           \label{fig:instruction_example_3}
       \end{figure}


\section{Instruction Tuning Stable Diffusion}
We show how our PEFT method can be used to fine-tune Stable Diffusion \citep{rombach2022high} so that it can perform specific image editing tasks according to instructions. Our experiment is based on InstructPix2Pix \citep{brooks2023instructpix2pix}, which performs instruction fine-tuning on numerous generated image pairs and prompts using pretrained Stable Diffusion checkpoints. The public InstructPix2Pix model is good at executing general instructions, but may not be skilled at specific instructions. 

Following \citet{Paul2023instruction-tuning-sd}, we choose {\it cartoonlization} as the target task for fine-tuning. The fine-tuning dataset includes 5000 paired image-cartoon images as well as the corresponding prompting texts. The original images are randomly sampled from the training set of ImageNette \citep{howard2020fastai}, and the corresponding edited images are obtained with the Whitebox Cartoonizer model \citep{wang2020learning}. The prompts are generated using ChatGPT \footnote{https://chatgpt.com/}. All pretrained models are from the Huggingface Diffusers 
\footnote{https://huggingface.co/docs/diffusers/index} library. We apply PEFT methods to the {\it Key, Query, Value} and {\it Out} matrixs in the Unet of Stable Diffusion for fine-tuning. After fine-tuning, we randomly choose some images from the {\it photo} domain of the PACS dataset \citep{li2017deeper} for evaluation, using the prompt \texttt{Change the natural image to a cartoon-style image}. We provide the hyperparameters for our LoCA and FourierFT in Table \ref{tab:hyper_diffusion}. 

\begin{figure}[H]
           \centering
           \includegraphics[width=1.0\linewidth]{Figures/Diffusion.pdf}
           \caption{Comparison of the instruction-following abilities of InstructPix2Pix, FourierFT and Our LoCA on the cartoonlization task.}
           \label{fig:diffusion_example}
       \end{figure}

From Fig. \ref{fig:diffusion_example}, we can see that the pre-trained InstructPix2Pix model does not perform perfectly on this specific cartoonization task, especially in terms of preserving the original content. After fine-tuning, there is a noticeable improvement in the quality of the edits. However, the images produced by our fine-tuning method show better detail preservation compared to those generated by FourierFT.

\begin{table}[H]
\centering
\caption{Hyperparameters of FourierFT and LoCA for the Stable Diffusion fine-tuning experiment.}
\label{tab:hyper_diffusion}
\begin{tabular}{lcc}
\toprule
Hyperparameter           & FourierFT               & LoCA              \\ \midrule \midrule
Optimizer                & \multicolumn{2}{c}{AdamW}                  \\
Weight Decay             & \multicolumn{2}{c}{1e-2}                   \\
Learning Rate            & 1e-3                    & 1e-4             \\
Scaling Value            & 64                      & 1                \\
Where                    & \multicolumn{2}{c}{Key, Query, Value, Out} \\
Accumulation Steps       & \multicolumn{2}{c}{4}                      \\
Batch Size               & \multicolumn{2}{c}{2}                      \\
Training Steps           & \multicolumn{2}{c}{10000}                  \\
Learning iterations ($\mathcal{B}_s$) & -                       & 1200             \\ \bottomrule
\end{tabular}
\end{table}

\section{Toy Experiment of the Convergence}
\label{sec:toy_experiment}
To visually demonstrate the convergence process of our method, we designed a toy experiment based on a regression task. 

{\bf Data Generation.} We generated 5000 6-dimensional samples $X \in \mathbb{R}^{5000 \times 6}$, where each dimension of each sample was independently sampled from a Gaussian distribution $\mathcal{N}(0, 20)$. 

{\bf Network and Ground-truth Labels Preparation.}
We design a simple three-layer neural network with parameter matrices $W_1, W_2$, and $W_3$, each with a shape of $6 \times 6$. We reparameterized $W_2$ as $W_2 = \texttt{iDCT}(F_2)$, where $F_2$ is a sparse frequency domain matrix with only 3 non-zero coefficients. Then, we randomly initialize $W_1$, the coefficients of $F_2$, and $W_3$ using $\mathcal{N}(0, 0.2)$, and initialize the locations of $F_2$'s non-zero coefficients using a uniform distribution. We denote these initialized network weights as the ground-truth weights $W_1^*$, $F_2^*$, $W_3^*$, and use them to generate ground-truth labels, i.e., $Y = W_3^* \texttt{iDCT}(F_2^*) W_1^* X$. 

{\bf Optimization Details.} We now fix $W1^*$ and $W3^*$, and re-initialize the coefficient locations of $F_2$, and set its coefficients to zero (the same as that in our method design). We aim to explore whether, through our alternating optimization method, the zero matrix $F_2$ could converge to $F_2^*$ \footnote{We ensure the uniqueness of the solution through a 6x6 full-rank matrix.}. The entire optimization process uses an SGD optimizer and mean squeue error loss function. We set the learning rate of coefficients and locations to 0.02 and 0.05, respectively, and alternately optimize the coefficients and locations of $F_2$ in a period of 10 steps.

{\bf Experimental Results.} From Fig. \ref{fig:toy_experiment}, we can see that after re-initialization, the locations of the learnable coefficients in $F_2$ have changed. If we only learn the coefficients without changing their locations, it would be impossible to converge to $F_2^*$. Through our alternating optimization strategy, the locations of the learnable coefficients begin to change gradually after 200 steps and eventually converge successfully to the ground-truth locations. At that, if we fix the locations and only learn the coefficients, we can perfectly converge to $F_2^*$, which can be observed in Fig. \ref{fig:toy_experiment_loss}. This is the rationale behind the design of our optimization method.

{\bf Remark.} It is worth noting that our location gradient is estimated through difference approximation and is not entirely precise. The most accurate calculation method would be to compute the one-sided gradients in 8 directions separately and then choose the direction with the largest gradient for movement. However, this approach would introduce implementation difficulties and additional computational costs. In our experiments, we find that the difference approximation generally works well. Given the large scale of the weight matrix in Transformer, using an approximate method is a more reasonable approach. Although in practical applications, it may be too demanding to require every coefficient to converge to its optimal locations, we show that even if one parameter moves in a better direction, it will improve the training effect. This can be observed from the loss descent curve in Fig. \ref{fig:toy_experiment_loss}.

\begin{figure}[H]
           \centering
           \includegraphics[width=0.8\linewidth]{Figures/toy_ex.png}
           \caption{Optimization process of $F_2$ for the toy experiment.}
           \label{fig:toy_experiment}
       \end{figure}

\begin{figure}[H]
           \centering
           \includegraphics[width=0.55\linewidth]{Figures/toy_loss_curves.pdf}
           \caption{Comparison of the training loss of our method with and without alternating optimization strategy on the toy experiment.}
           \label{fig:toy_experiment_loss}
       \end{figure}


\section{Comparison of Learning Patterns in Different Fine-tuning Methods}
\begin{figure}[H]
           \centering
           \includegraphics[width=\linewidth]{Figures/visualization_W.pdf}
           \caption{Visualization of learned $\Delta W_q$ and $\Delta W_v$ in different fine-tuning methods with RoBERTa-base. We choose layer 6 and layer 8 tuned on MNLI task as an example. For a clearer presentation, we use average pooling to downsample to 1/8 of the original size.}
           \label{fig:visualization_W}
       \end{figure}

To visually compare the differences in learning patterns between frequency domain methods and low-rank decomposition methods, we present in Fig. \ref{fig:visualization_W} the incremental matrices learned through FF, LoRA, and our LoCA. The hyperparameter settings for the experiment are the same as in Section \ref{sec:NLU}. It can be observed that the $\Delta W$ obtained from full fine-tuning shows more degrees of freedom across the entire matrix, exhibiting a Gaussian-like distribution. This aligns with the asymptotic normality we proposed in Proposition \ref{normalmatrixprop}. In contrast, the incremental weights learned by LoRA display a structured absence of many elements on the matrix, likely due to its low-rank approximation. This suggests that the optimization of LoRA may be constrained and it may not effectively capture the information present in the weight updates. LoCA circumvents the constraints of low-rank decomposition through frequency domain decomposition. As can be seen from Fig. \ref{fig:visualization_W}, the pattern of LoCA is more diverse compared to LoRA, thus enabling it to better capture the learning pattern of full fine-tuning.

\section{Extended Analysis on other LoRA Variants}
\textcolor{black}{Our theoretical analysis in Theorem \ref{theorem_1} focuses specifically on the classical low-rank reconstruction method LoRA \citep{hu2021lora}, which potentially constrains our comparative analysis with various LoRA variants. While it may not be feasible to encompass all low-rank methods within a single theorem, as some methods like VeRA \citep{kopiczko2023vera} are not explicitly designed for reconstruction, we can conduct case-by-case analyses since all low-rank-based methods are inherently bounded in their reconstruction capabilities.}

\textcolor{black}{For a given $\Delta W\in\mathbb{R}^{n\times n}$, VeRA decomposes it to $\Lambda_bB\Lambda_dA$ where $B,A$ are draw i.i.d. from a certain distribution and frozen and shared over all training steps and layers, $\Lambda_b,\Lambda_d$ are learnable diagonal matrix. From a reconstruction perspective, the $i$-th element of $\Lambda_b$ is the ordinary least squares (OLS) coefficient while setting the response as $i$-th row of $\Delta W$ and covariate as $i$-th row of $B\Lambda_dA$. This idea enables us to find $\Lambda_d$ that maximize the correlation between $i$-th row of $\Delta W$ and $i$-th row of $B\Lambda_dA$. However $A$ and $B$ are chosen randomly independent of $\Delta W$, the reconstruction error is approximately the error we learn from white noise.}

\textcolor{black}{We can conduct a detailed theoretical analysis of DoRA \citep{liu2024dora}, here we only give the outline. For a given $\Delta W$, DoRA first decomposes it as $\Delta W=A\Lambda$ where $\Lambda$ is diagonal and each column of $A$ has magnitude $1$. The $r$-rank approximation is $A_r\Lambda$, where $A_r=U_r\Lambda_rV_r^T$, and $U_r,V_r\in\mathbb{R}^{n\times r}$ and $\Lambda_r$ contains $r$ largest eigenvalues of $A$. If each element in $\Delta W$ follows i.i.d. standard normal, we can derive the independency of $A$ and $\Lambda$. Using total expectation, we have the following reconstruction loss
$$
\mathbb{E}(\|A\Lambda-A_r\Lambda\|^2)=\mathbb{E}\{\mathbb{E}(\|A\Lambda-A_r\Lambda\|^2|A)\}=\sqrt{2}\dfrac{\Gamma((n+1)/2)}{\Gamma(n/2)}\mathbb{E}(\|A-A_r\|^2)
$$
As each non-zero element in $\Lambda$ follows i.i.d. $\chi(n)$ distribution. Subsequent calculations only require computing the reconstruction loss based on the distribution of $A$. At this point, the reconstruction loss is consistent with the LoRA method, except that the distributions are different. This requires complex calculations, but since each column of $A$ is the direction of a random normal vector, the difference should not be significant. The loss corresponding to DoRA should therefore be approximately the same as that of LoRA.}

\section{Analysis of Non-i.i.d. Effects}
\label{appendix:non_iid}
\textcolor{black}{While our main theoretical analysis assumes independence of weight updates for analytical tractability, practical neural network training through gradient-based optimization introduces dependencies between parameters. In this section, we provide a detailed analysis of how deviations from the i.i.d. assumption affect our theoretical results.}

\textcolor{black}{
\textbf{Correlation Structure.}
To systematically study the impact of parameter dependencies, we consider a controlled correlation setting where the vectorized weight updates follow a multivariate normal distribution:}
\textcolor{black}{
\begin{equation}
    W^T \sim N_{K^2}(0,\Sigma)
\end{equation}
}
\textcolor{black}{where the covariance matrix $\Sigma$ takes the form:}
\textcolor{black}{
\begin{equation}
    \Sigma = \rho\mathbbm{1}\mathbbm{1}^T + I_{K^2}
\end{equation}}

\textcolor{black}{Here, $\mathbbm{1}=(1,\ldots,1)^T\in\mathbb{R}^{K^2}$ is the all-ones vector, $I_{K^2}$ is the identity matrix, and $\rho$ controls the strength of uniform correlation between all pairs of parameters. This structure allows us to precisely control the degree of dependency while maintaining the marginal distributions of individual parameters.}

\textcolor{black}{
\textbf{Critical Correlation Analysis.}
We conduct extensive numerical experiments to identify the critical correlation levels where the relative performance of different adaptation methods changes significantly. For these experiments, we fix the matrix size to $300 \times 300$ and vary the rank $r$ used in low-rank approximations.}
\textcolor{black}{
For each rank setting, we identified the critical correlation value $\rho_c$ where LoRA's reconstruction ability begins to outperform LoCA. The results are shown in Fig. \ref{fig:sim_3}.}


   \begin{figure}[H]
       \centering
       \includegraphics[width=0.9\linewidth]{Figures/CorrelationValue.pdf}
       \caption{\textcolor{black}{Reconstruction errors of different $r$ under different correlation values $\rho$. R, M1, M2, D, U denote the same meaning in Fig. \ref{fig:sim_1}.}}
       \label{fig:sim_3}
   \end{figure}
   
\textcolor{black}{
The results show that when $r = 8, 16, 24$, and $32$, the critical values $\rho_c$ are $0.09, 0.14, 0.17,$ and $0.19$, respectively, which are quite high and indicate our method remains effective under substantial dependencies.}


\textcolor{black}{
\textbf{Statistical Detection of Correlation.}
To validate that these critical correlation levels represent statistically significant departures from independence, we developed a test based on the Marchenko-Pastur (MP) law. According to the MP law, under independence, the eigenvalues of the sample correlation matrix should fall within a specific interval $[\lambda_-,\lambda_+]$. We define a test statistic:}
\textcolor{black}{
\begin{equation}
    T = \frac{\sum_{\lambda\notin[\lambda_-,\lambda_+]}\lambda}{\sum\lambda}.
\end{equation}
}
\textcolor{black}{
This statistic measures the proportion of eigenvalue mass that falls outside the MP bounds. Through Monte Carlo simulation, we determined that the critical value at the 0.95 significance level is 0.005. For our identified critical correlation values $\rho_c = 0.09, 0.14, 0.17, 0.19$, the corresponding test statistics are:}
\textcolor{black}{
\begin{itemize}
    \item $\rho_c = 0.09$: $T = 0.086$
    \item $\rho_c = 0.14$: $T = 0.134$
    \item $\rho_c = 0.17$: $T = 0.143$
    \item $\rho_c = 0.19$: $T = 0.146$
\end{itemize}}
\textcolor{black}{
All these test statistics substantially exceed the critical value, confirming that these levels of correlation are readily detectable and represent significant departures from independence.}

\textcolor{black}{
\textbf{Implications for Theory.}
These findings have several important implications:}

\begin{enumerate}
    \item \textcolor{black}{The critical correlation values where method performance characteristics change are statistically significant and detectable using standard random matrix theory diagnostics.}
    
    \item \textcolor{black}{The monotonic increase in critical correlation with rank suggests that higher-dimensional representations are more robust to dependencies.}
    
    \item \textcolor{black}{Even under substantial and detectable correlations, the performance advantages of frequency-domain methods persist, supporting the practical validity of our theoretical framework.}
\end{enumerate}

\textcolor{black}{
These results demonstrate that while strict independence is violated in practice, our theoretical insights remain valid under realistic levels of parameter dependency. The robustness of our results to substantial correlations, as quantified by both performance analysis and statistical tests, supports the practical applicability of frequency-domain adaptation methods.}



\end{document}
