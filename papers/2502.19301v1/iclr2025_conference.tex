\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}
\iclrfinalcopy
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{wrapfig}
\usepackage{subfigure,lipsum}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage[]{mdframed}
\usepackage{multirow} 
\usepackage{booktabs} 
\usepackage{makecell}

\usepackage{xcolor,colortbl}
\definecolor{Gray}{gray}{0.85}
\definecolor{LightCyan}{rgb}{0.88,1,1}
\newcolumntype{a}{>{\columncolor{Gray}}c}

\definecolor{Gray0}{gray}{0.95}
\definecolor{Gray1}{gray}{0.9}
\definecolor{Gray2}{gray}{0.85}
\definecolor{Gray3}{gray}{0.80}
\definecolor{Gray4}{gray}{0.75}
\definecolor{Gray5}{gray}{0.7}
\definecolor{Gray6}{gray}{0.65}
\definecolor{Gray7}{gray}{0.6}
\definecolor{Gray8}{gray}{0.55}
\definecolor{Gray9}{gray}{0.5}
\definecolor{Gray10}{gray}{0.45}
\definecolor{Gray11}{gray}{0.4}
\definecolor{Gray12}{gray}{0.35}
\definecolor{Gray13}{gray}{0.3}
\definecolor{Gray14}{gray}{0.25}
\definecolor{Gray15}{gray}{0.2}


\definecolor{citeColor}{RGB}{0,20,115}
\hypersetup{colorlinks,linkcolor={black},citecolor={citeColor},urlcolor={citeColor}}

\title{Rethinking LLM Unlearning Objectives: \\ A Gradient Perspective and Go Beyond}

% \makeatletter\def\@fnsymbol#1{\ensuremath{\ifcase#1\or \dagger\or \ddagger\or\mathsection\or \mathparagraph\or \|\or **\or \dagger\dagger\or \ddagger\ddagger \else\@ctrerr\fi}}\makeatother
    
% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Qizhou Wang$^{1}$\thanks{Work done during a remote visiting at Cornell University.}\quad Jin Peng Zhou$^{2}$\quad Zhanke Zhou$^{1}$\quad Saebyeol Shin$^{2}$\\\textbf{Bo Han}$^{1}$\thanks{Correspondence to Bo Han (bhanml@comp.hkbu.edu.hk).}\quad \textbf{Kilian Q. Weinberger}$^{2}$ \\ \\
$^1$TMLR Group, Department of Computer Science, Hong Kong Baptist University \\
$^2$Department of Computer Science, Cornell University\\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Large language models (LLMs) should undergo rigorous audits to identify potential risks, such as copyright and privacy infringements. Once these risks emerge, timely updates are crucial to remove undesirable responses, ensuring legal and safe model usage. It has spurred recent research into LLM unlearning, focusing on erasing targeted undesirable knowledge without compromising the integrity of other, non-targeted responses. Existing studies have introduced various unlearning objectives to pursue LLM unlearning without necessitating complete retraining. However, each of these objectives has unique properties, and no unified framework is currently available to comprehend them thoroughly. To fill the gap, we propose a toolkit of the gradient effect (G-effect), quantifying the impacts of unlearning objectives on model performance from a gradient perspective. A notable advantage is its broad ability to detail the unlearning impacts from various aspects across instances, updating steps, and LLM layers. Accordingly, the G-effect offers new insights into identifying drawbacks of existing unlearning objectives, further motivating us to explore a series of new solutions for their mitigation and improvements. Finally, we outline promising directions that merit further studies, aiming at contributing to the community to advance this important field. The code is publicly available at: \url{https://github.com/tmlr-group/G-effect}.

\end{abstract}


\section{Introduction}

Large language models (LLMs)~\citep{brown2020language,touvron2023llama2,achiam2023gpt} represent the cutting edge of machine learning for the field of language understanding. These models typically leverage multi-head attention decoder-based architectures~\citep{vaswani2017attention} with billions of learnable parameters and are autoregressively trained~\citep{zhao2023survey} over web-sourced datasets encompassing trillions of tokens. Such extensive scaling enables LLMs to handle a broad spectrum of complex linguistic tasks, demonstrating remarkable proficiency in understanding and generating languages across a board range of practical applications.

The scaling of LLMs, on the other side, also brings notable drawbacks alongside its benefits. A primary concern is their high tendency to memorize data, which can reproduce sensitive information once encountered during web-sourced training, such as copyright and privacy-related content~\citep{yao2023survey}. These issues are particularly concerning due to the potential misuse of LLMs for illegal activities~\citep{li2024wmdp}, also posing challenges to protect individual rights to be forgotten~\citep{zhang2023right}. Mitigating these undesirable behaviors in LLMs is non-trivial, involving regularly auditing LLMs to recognize sensitive content and subsequently adjusting the associated, parameterized knowledge. In previous works, supervised fine-tuning~\citep{de2021editing} and alignment methods~\citep{ouyang2022training} have been explored to overwrite LLMs against such undesirable model behaviors. However, these well explored directions face practical deficiencies---they can be costly~\citep{yao2023large}, require high-quality crafted preference datasets~\citep{chowdhury2024provably}, and exhibit concerns regarding robustness~\citep{patil2023can,qi2023fine,wang2024comprehensive}.


\textbf{LLM unlearning}~\citep{yao2023large} has emerged as a promising alternative, with a direct goal of removing parameterized knowledge targeted to be unlearned, meanwhile preserving the model integrity for all other, non-targeted data~\citep{wang2024unlearning}. Highlighted by~\citet{yao2023large}, LLM unlearning is cost-effective over aforementioned more demanding methods, thus attracting emerging research attention these days~\citep{liu2024rethinking}. A representative baseline of LLM unlearning is gradient ascent (GA)~\citep{maini2024tofu}, adjusting LLMs to increase the prediction losses for targeted data---thereby removing parameterized knowledge. GA offers a potentially viable path to implement LLM unlearning; however, it is severely susceptible to excessive unlearning~\citep{zhang2024negative}, where the effectiveness in removing undesirable data comes at the high cost of compromising the overall model integrity. It motivates a series of subsequent works that improve upon GA, such as negative preference optimization (NPO)~\citep{zhang2024negative}, preference optimization (PO)~\citep{maini2024tofu}, and representation misdirection for unlearning (RMU)~\citep{li2024wmdp}.


Given the increasing number of unlearning objectives, we need to discern good objectives from those less promising ones. 
A step further, it is also interesting to pinpoint beneficial components within existing methods, isolating those that are useless or potentially harmful. Sadly, to our knowledge, a general toolkit for in-depth analysis of various unlearning methods is still lacking. To bridge this gap, we propose the concept of the gradient effect (G-effect), which approximates the performance change associated with particular unlearning objectives via the dot product of their gradients, cf., Definition~\ref{def: gradient effect}.  The G-effect provides more than mere performance evaluations---it enables detailed examinations of various unlearning methods for their impacts with respect to data points, updating steps, and layers, cf., Section~\ref{sec: case studies}. We outline below for some of the general observations we achieved.

\begin{itemize}[leftmargin=15pt]
    \item \textbf{Unlearning affects shallow layers more.} Shallow layers are more affected than deeper layers, showing that general knowledge encoded in shallow layers undergoes substantial alterations. 

    \item \textbf{Unlearning compromises retention.} Although conceptually existing (cf., Section~\ref{sec: g-effect}), current unlearning objectives all fail to retain the overall performance when conducting unlearning.

    \item \textbf{Excessive unlearning is harmful.} An excessive extent of unlearning has severe impacts such that the deterioration in common model responses can outweigh improvements in unlearning.

    \item \textbf{Risk weighting is powerful.} Prioritizing certain points is justified to be effective for unlearning. However, there still exists a large space to further refine risk weighting mechanisms. 

    \item \textbf{Regularization is important.} Regularization terms continue to play a crucial role in maintaining overall model integrity, with the KL~\citep{maini2024tofu} emerging as a promising choice.

\end{itemize}

We benchmark both existing and new methods explored throughout our analysis on the well-established TOFU fictitious unlearning datasets~\citep{maini2024tofu}. Our experiments identify several new state-of-the-arts that merit further attention. 
Additionally, based on our analysis, we highlight promising research directions that warrant exploration to further advance the field.

\section{LLM Unlearning}\label{sec: preliminary}
We focus on auto-regressive LLMs parameterized by $\boldsymbol{\theta}$, which recursively estimate the probability distributions over the next tokens, denoted as $p(\cdot|s;\boldsymbol{\theta})$. LLMs are, in general, trained on large-scale, web-sourced corpora following the distribution $\mathcal{D}_{\mathrm{t}}$ with the negative log-likelihood (NLL) loss function of $-\log p(s;\boldsymbol{\theta})$, where $p(s;\boldsymbol{\theta})=\prod_{i=2}^{\vert s\vert}p(s^i|s^{<i};\boldsymbol{\theta})$ with $s^i$ the $i$-th token and $s^{<i}$ the prefix up to $s^i$. While LLMs are capable of handling a broad spectrum of language generation tasks, the use of training corpora sourced from the open world raises the risk that our LLMs will learn from sensitive data, precipitating a series of legal and ethical concerns~\citep{liu2023trustworthy}.


\textbf{LLM Unlearning.} These issues necessitate the need for a post-training mechanism that enables LLMs to eradicate any parameterized knowledge that is undesirable. This requirement motivates the recent research on LLM unlearning~\citep{yao2023large,maini2024tofu}, of which the main goals are in two folds---\textbf{(a)} ensuring the removal of data / knowledge targeted to be unlearned and \textbf{(b)} retaining the integrity of model responses for non-targeted data. Formally, we consider the data distribution $\mathcal{D}_{\mathrm{u}}$ that should be unlearned and define the risk metric $\mathcal{R}$ to assess model performance. 
Then, our goal
is to adjust the original LLM parameters $\boldsymbol{\theta}_{\mathrm{o}}$ to get the unlearned ones $\boldsymbol{\theta}_{\mathrm{u}}$, such that:
\begin{itemize}[leftmargin=15pt]
    \item \textbf{Removal.} The performance on the unlearning dataset \(\mathcal{D}_{\mathrm{u}}\) should significantly deteriorate, i.e., $\mathcal{R}(\mathcal{D}_{\mathrm{u}};\boldsymbol{\theta}_{\mathrm{u}})\gg\mathcal{R}(\mathcal{D}_{\mathrm{u}};\boldsymbol{\theta}_{\mathrm{o}})$, revealing effective unlearning on data targeted to be erased.
    \item \textbf{Retention.}  The performance on other data, i.e., \(\mathcal{D}_{\mathrm{t}} \backslash \mathcal{D}_{\mathrm{u}}\), should be maintained or enhanced, i.e., $\mathcal{R}(\mathcal{D}_{\mathrm{t}}\backslash\mathcal{D}_{\mathrm{u}};\boldsymbol{\theta}_{\mathrm{u}})\le\mathcal{R}(\mathcal{D}_{\mathrm{t}}\backslash\mathcal{D}_{\mathrm{u}};\boldsymbol{\theta}_{\mathrm{o}})$, 
    ensuring model responses on common data are not damaged.
\end{itemize}
Here, we consider a practical objective of erasing targeted knowledge as much as possible~\citep{liu2024rethinking}, i.e., full removal, slightly diverging from the classical definition of machine unlearning~\citep{bourtoule2021machine} that seeks to make models behave as if they were trained without the targeted data, i.e., influence removal. 
The pursuit of full removal simplifies our subsequent discussions, and as we will see later in Section~\ref{sec: experiment}, the outcomes of these two goals are typically aligned.

% Our goal is more suitable for LLM unlearning, driven by the need to eliminate content that poses privacy and copyright concerns, with the understanding that more thorough elimination leads to more favorable behaviors.

This paper delves into exploring various objective functions that implement LLM unlearning, a topic that requires our fundamental interest. 
As an example, GA~\citep{yao2023large} directly increases the NLL loss for targeted data, of which the objective is articulated as $\min_\theta\mathbb{E}_{s_{\mathrm{u}}\sim\mathcal{D}_{\mathrm{u}}}\log p(s_{\mathrm{u}};\boldsymbol{\theta})$. GA represents one of the pioneering methods for LLM unlearning, paving a feasible road to implement unlearning in practice. However, it often exhibits the propensity to excessive unlearning~\citep{zhang2024negative,wang2024unlearning}---the efficacy in eliminating undesirable knowledge comes at a high cost to compromise the {model integrity}. It motivates a series of subsequent works~\citep{zhang2024negative,maini2024tofu,li2024wmdp}, which will be discussed  later in Section~\ref{sec: case studies}.  

\section{G-effect}\label{sec: g-effect}
Before delving into specific methods, we need proper criteria for assessing whether an objective is suitable for unlearning or not. 
Recalling our earlier discussion on the main goals of unlearning, we can quantify the performance change before and after unlearning to evaluate their effects, i.e., 
$\mathcal{R}(\mathcal{D}_{\mathrm{u}};\boldsymbol{\theta}_{\mathrm{u}})-\mathcal{R}(\mathcal{D}_{\mathrm{u}};\boldsymbol{\theta}_{\mathrm{o}})$ for removal and $\mathcal{R}(\mathcal{D}_{\mathrm{t}}\backslash\mathcal{D}_{\mathrm{u}};\boldsymbol{\theta}_{\mathrm{u}}) - \mathcal{R}(\mathcal{D}_{\mathrm{t}}\backslash\mathcal{D}_{\mathrm{u}};\boldsymbol{\theta}_{\mathrm{o}})$ for retention. Sadly, merely comparing performance provides limited insights into understanding the underlying mechanisms (cf., Section~\ref{sec: case studies}). Therefore, we suggest a more insightful scheme that can facilitate the analysis of various unlearning methods from a gradient perspective, named the gradient effect (G-effect).

Generally speaking, the G-effect compares the gradients of the unlearning objective $\mathcal{L}_{\mathrm{u}}$ and the risk metric $\mathcal{R}$. If the gradients of $\mathcal{L}_{\mathrm{u}}$ align in similar directions to $\mathcal{R}$, model updating based on $\mathcal{L}_{\mathrm{u}}$ is capable to enhance model performance measured by $\mathcal{R}$, an obvious alternative of $\mathcal{R}(\mathcal{D};\boldsymbol{\theta}_{\mathrm{u}})-\mathcal{R}(\mathcal{D};\boldsymbol{\theta}_{\mathrm{o}})$ to measure the performance change.
The degree of such similarity between gradients can be quantified using their dot products~\citep{lopez2017gradient}: A positive dot product indicates that  $\mathcal{L}_{\mathrm{u}}$ is capable to improve $\mathcal{R}$, whereas a negative dot product suggests potential harm to $\mathcal{R}$. Please refer to Appendix~\ref{app: g-effect} for a formal derivation. It motivates the G-effect as follows. 


\begin{definition}[\textbf{G-effect}]\label{def: gradient effect}
    The G-effect $e^{(t)}$ for an unlearning objective $\mathcal{L}_{\mathrm{u}}$ at the $t$-th step of model updating is given by $\nabla_{\boldsymbol{\theta}}\mathcal{R}(\mathcal{D};\boldsymbol{\theta}^{(t)})^\top\nabla_{\boldsymbol{\theta}}\mathcal{L}_{\mathrm{u}}(\mathcal{D}_{\mathrm{u}};\boldsymbol{\theta}^{(t)})$. We further define the \textbf{unlearning G-effect}  $e_{\mathrm{u}}^{(t)} \leftarrow \nabla_{\boldsymbol{\theta}}\mathcal{R}(\mathcal{D}_{\mathrm{u}};\boldsymbol{\theta}^{(t)})^\top\nabla_{\boldsymbol{\theta}}\mathcal{L}_{\mathrm{u}}(\mathcal{D}_{\mathrm{u}};\boldsymbol{\theta}^{(t)})$ and the \textbf{retaining G-effect} $e_{\mathrm{r}}^{(t)} \leftarrow \nabla_{\boldsymbol{\theta}}\mathcal{R}(\mathcal{D}_{\mathrm{t}}\backslash\mathcal{D}_{\mathrm{u}};\boldsymbol{\theta}^{(t)})^\top\nabla_{\boldsymbol{\theta}}\mathcal{L}_{\mathrm{u}}(\mathcal{D}_{\mathrm{u}};\boldsymbol{\theta}^{(t)})$ to reflect the respective goals of removal and retention.
\end{definition}


\setlength{\intextsep}{-25pt}
\begin{wrapfigure}{R}{0.43\linewidth}
    \centering  
		\includegraphics[width=.75\linewidth]{motivation_gradientsV3.png}
	    \centering  
	\caption{\textbf{Gradient Directions and Unlearning Behaviors}. We show directions for $\nabla_{\boldsymbol{\theta}}\mathcal{R}(\mathcal{D}_{\mathrm{u}};\boldsymbol{\theta}_{\mathrm{o}})$ and $\nabla_{\boldsymbol{\theta}}\mathcal{R}(\mathcal{D}\backslash\mathcal{D}_{\mathrm{u}};\boldsymbol{\theta}_{\mathrm{o}})$ and regions ensuring $e^{(t)}_{\mathrm{u}}<0$ (red) and $e^{(t)}_{\mathrm{r}} \geq 0$ (blue). Their intersection (black dashed) fulfills the unlearning goals.} \label{fig: motivation}
\end{wrapfigure}

The G-effect measures the impacts of unlearning objectives on either targeted or non-targeted data when implementing gradient updates. Overall, to fulfill the unlearning goals outlined in Section~\ref{sec: preliminary}, we aim for notably negative values of $e^{(t)}_{\mathrm{u}}$ to pursue a sufficient removal of targeted knowledge and non-negative values of $e^{(t)}_{\mathrm{r}}$ to maintain the model integrity for non-targeted data. Figure~\ref{fig: motivation} further depicts these two essential gradient conditions to ensure effective unlearning:
\begin{itemize}[leftmargin=15pt]
    \item \textbf{Removal.} The red region indicates $e^{(t)}_{\mathrm{u}}<0$, ensuring  $\mathcal{L}_{\mathrm{u}}$ to eliminate targeted knowledge.
    \item \textbf{Retention.} The blue region represents $e^{(t)}_{\mathrm{r}} \geq 0$, ensuring  $\mathcal{L}_{\mathrm{u}}$ to retain the overall model integrity.
\end{itemize}

\textbf{What Can We Learn from the G-effect?} 
In Figure~\ref{fig: motivation}, the intersection, delineated by black dashed lines, is the region that meets the primary goals of unlearning---effective removal of targeted knowledge while retaining the integrity of  non-targeted data. This area highlights the conceptual possibilities of achieving a perfect unlearning objective, under the mild conjecture that $\nabla_{\boldsymbol{\theta}}\mathcal{R}(\mathcal{D}_{\mathrm{u}};\boldsymbol{\theta}_{\mathrm{o}})$ will differ from $\nabla_{\boldsymbol{\theta}}\mathcal{R}(\mathcal{D}\backslash\mathcal{D}_{\mathrm{u}};\boldsymbol{\theta}_{\mathrm{o}})$ more or less.  
Also, the dependency on $t$ enables us to examine the dynamics of unlearning, and the gradients helps us to explore the impacts of particular layers or data points involved during unlearning. It will facilitate our understanding of existing unlearning mechanisms, which will be detailed below. Please refer to Appendix~\ref{app: g-effect} for more discussions.


\section{Analysis for Unlearning Objectives}\label{sec: case studies}


\setlength{\intextsep}{5pt}
\begin{wrapfigure}{R}{0.5\linewidth}
    \centering  
		\includegraphics[width=.9\linewidth]{legends.png}
	    \centering  
	\caption{\textbf{Figure Legends}. We present the unlearning (unlearn) and the retaining (retain) G-effect, and also their values for specific layers, including input embedding layer (embed), layers 1-11 (shallow), layers 12-22 (middle), layers 23-33 (deep), and output unembedding layer (lm).}
    \label{fig: legend}
\end{wrapfigure}

In this section, we employ the G-effect to assess a range of unlearning objectives that are well-recognized, aiming to understand their mechanisms as well as identify their advantages and deficiencies. Due to the high costs in fully computing the G-effect, we focus on experiments based on 5\% TOFU fictitious unlearning~\citep{maini2024tofu} with Llama-2-7B~\citep{touvron2023llama} (cf. Appendix~\ref{app: experiment setups}). Later in Section~\ref{sec: experiment}, we will demonstrate that the conclusions can be generalized to  many other unlearning scenarios. All the methods will run for 5 epochs, totaling about 60 steps. Moreover, as indicated in Figure~\ref{fig: legend}, we will report the unlearning (red) and retaining (blue) G-effect, as well as their detailed values for particular layers within Llama-2-7B (dashed lines for the stacks of layers and dash-dotted lines for input/output layers). 
We default to consider the NLL for the risk metric $\mathcal{R}$.


\subsection{Gradient Ascent (GA)}
\label{sec: ga}

As discussed in Section~\ref{sec: preliminary}, GA represents one of the most basic unlearning objectives~\citep{yao2023editing}, which simply decreases the log-likelihood $\log p(s_{\mathrm{u}};\boldsymbol{\theta})$ for the unlearning data. 
% \footnote{+zk+ You might consider summarizing some \textit{observations} as a quick take-home message to help understand this section, which is quite long (around 5 pages).}

\textbf{The G-effect across Unlearning Steps.} We illustrate the G-effect of GA in Figure~\ref{fig: ga_and_wga_effect}(a). As we can see, the unlearning G-effect reflects the high capability of GA in erasing parameterized knowledge for targeted data, with its values rapidly declining from about $0$ to $-3.5\times10^{5}$. However, this excessive extent of unlearning incurs a large cost to the integrity for non-targeted data, evidenced by the trajectory of negative values in the retaining G-effect that mirror the scales and trends of the unlearning G-effect. Overall, such a scenario suggests that the improvements in unlearning are accompanied by similar, or even greater, deterioration on non-targeted data. 

Note that relatively near-zero values of the G-effect in the later updating stages do not imply that the model can relearn the knowledge. In general, the G-effect exhibits cumulative behaviors, where the presence of extremely negative G-effect, particularly between steps 20 to 40,  has already indicated a large deterioration on model performance. Smaller values of the G-effect in the later stages only suggest that the subsequent damage to model integrity is less severe, mainly due to the GA objective reaching its empirical convergence stage. Refer to {Figure~\ref{fig: ga_effect_appendix}(b) in Appendix~\ref{app: ga}} for more analysis.

\textbf{The G-effect across Layers.} We also observe that the G-effect values are notably greater in the shallow layers than those in the middle and deep layers, which can be more clearly shown in Figure~\ref{fig: ga_and_wga_effect}(b). It indicates that the general knowledge, which is parameterized within shallow layers, is notably distorted, while such side impacts are less severe for middle and deep layers with context-specific knowledge~\citep{geva2020transformer,belrose2023eliciting}. 
It is also worth noting that we isolate the input embedding layer (embed) from other shallow layers (shallow), where we observe that the input embedding layer has relatively negligible impacts on both removal and retention, highlighting the distinct influences of the GA unlearning procedure on the input embedding layer and other shallow layers. 
Furthermore, the middle and top layers exhibit much smaller G-effect values than that for shallow layers. However, the G-effect for the last layer, i.e., the output unembedding layer (LM), is notably large and do not converge to near zero. This behavior suggests that such a linear model performs some scaling operations to further reduce the GA objective that is lower unbounded.

\textbf{Unlearning Mechanisms.} We hope to further explore the unlearning behaviors behind GA, particularly focusing on its wrong tendency towards exhibit extremely negative values of the retaining G-effect. Specifically, the gradients of $\mathcal{L}_{\text{GA}}(\mathcal{D}_{\mathrm{u}};\boldsymbol{\theta})$ with respect to  $\boldsymbol{\theta}$, i.e., $\nabla_{\boldsymbol\theta}\mathcal{L}_{\text{GA}}(\mathcal{D}_{\mathrm{u}};\boldsymbol{\theta})$, are 
\begin{equation}
    \mathbb{E}_{s_{\mathrm{u}}\sim\mathcal{D}_{\mathrm{u}}}\sum_{i=2}^{\vert s\vert}\underbrace{\frac{1}{p(s_{\mathrm{u}}^i|s_{\mathrm{u}}^{<i};\boldsymbol{\theta})}}_{\text{inverse confidence}}\nabla_{\boldsymbol{\theta}} p(s_{\mathrm{u}}^i|s_{\mathrm{u}}^{<i};\boldsymbol{\theta}),\label{eq: ga graient}
\end{equation}
where the inverse confidence term tends to allocate more attention to those tokens that have been notably unlearned, along with the decrease of the likelihood $p(s_{\mathrm{u}}^i|s_{\mathrm{u}}^{<i};\boldsymbol{\theta})$ throughout GA. 

In this case, even minor negative values of each $\nabla_{\boldsymbol{\theta}}\mathcal{R}(\mathcal{D};\boldsymbol{\theta})^\top\nabla_{\boldsymbol{\theta}} p(s_{\mathrm{u}}^i|s_{\mathrm{u}}^{<i};\boldsymbol{\theta})$ can result in the corresponding $\nabla_{\boldsymbol{\theta}}\mathcal{R}(\mathcal{D};\boldsymbol{\theta})^\top{p(s_{\mathrm{u}}^i|s_{\mathrm{u}}^{<i};\boldsymbol{\theta})^{-1}}\nabla_{\boldsymbol{\theta}} p(s_{\mathrm{u}}^i|s_{\mathrm{u}}^{<i};\boldsymbol{\theta})$ to be extreme. This trend will lead to the extreme negative values of the unlearning G-effect, consistent with prior findings for the excessive unlearning of GA~\citep{wang2024unlearning}. Therefore, this inverse confidence mechanism is predominantly responsible for excessive unlearning. 
One can counteract the impacts of the inverse confidence by weighting the log-likelihood for each token via its own confidence, i.e.,
\begin{equation}
    \mathbb{E}_{s_{\mathrm{u}}\sim\mathcal{D}_{\mathrm{u}}}\sum_{i=2}^{\vert s\vert}w^{\mathrm{wga}}_{s_{\mathrm{u}},i}\log p(s_{\mathrm{u}}^i|s_{\mathrm{u}}^{<i};\boldsymbol{\theta})\label{eq: wga}
\end{equation}
with $w^{\mathrm{wga}}_{s_{\mathrm{u}},i}={p(s_{\mathrm{u}}^i|s_{\mathrm{u}}^{<i};\boldsymbol{\theta})^\alpha}$ the confidence weights for the $i$-th token and $\alpha$ the hyper-parameter of inverse temperature. 
We refer to this approach as the weighted GA (WGA). 
% \footnote{+zk+ This is a new objective. Putting the two objectives into a new section may be better. In this case, the new objectives in Sec. 5 (which can merge the evaluation section as well) are designed based on the understanding in Sec. 4.}
An example of its G-effect in mitigating excessive unlearning is illustrated in Figure~\ref{fig: ga_and_wga_effect}(c). Remarkably, we observe that the negative impact on common data is considerably less severe compared to the improvements observed on targeted data. 
Its underlying mechanism is not mystic, functioning as early stopping to curb the unlearning extent. Particularly, when the unlearning extent is well-controlled, even the original GA can outweigh the improvements of unlearning over the deterioration on integrity, a less obvious scenario that is further elaborated in Figure~\ref{fig: ga_effect_different_scale_appendix} of Appendix~\ref{app: ga}. Overall, the findings emphasize that excessive unlearning profoundly compromises the overall model integrity, necessitating careful controls. For more detailed discussions about WGA, please refer to Appendix~\ref{app: wga}.


\begin{figure}
    \centering
    \subfigure[GA]{\includegraphics[width=0.32\textwidth]{ga_effect_full_v2.pdf}} ~
    \subfigure[GA (detailed)]{\includegraphics[width=0.32\textwidth]{ga_effect_v2.pdf}} ~
    \subfigure[{WGA ($\alpha=0.5$)}]{\includegraphics[width=0.32\textwidth]{wga_p5_effect_v3.pdf}}
    \caption{\textbf{The G-effect for GA and WGA.} We depict the G-effect for GA in (a) and its values in the range between about $-3.5\times10^4$ and 0 in (b). We further depict the G-effect for WGA, which improves upon GA following \eqref{eq: wga}, in (c). 
    The \textbf{legends} are summarized in Figure~\ref{fig: legend}. The \textbf{horizontal axis} denotes the unlearning step and the \textbf{vertical axis} denotes the values of the G-effect. 
    % \textcolor{blue}{+zk+ The legends might be too far away.}
    }
    \label{fig: ga_and_wga_effect}
\end{figure}



\subsection{Negative Preference Optimization (NPO)}
\label{sec: npo}
NPO is motivated by direct preference optimization (DPO), an  alignment method~\citep{rafailov2024direct}, which originally utilizes paired corpora comprising preferred versus dis-preferred data. NPO segregates the dis-preferred part from DPO, heuristically employing it as the unlearning objective, of which the formulation can be written as 
\begin{equation}
    \frac{2}{\beta}\mathbb{E}_{s_{\mathrm{u}}\sim\mathcal{D}_{\mathrm{u}}}\log \big(1 +(\frac{p(s_{\mathrm{u}};\boldsymbol{\theta})}{p(s_{\mathrm{u}};\boldsymbol{\theta}_{\textrm{o}})})^\beta\big),
    \label{eq: npo}
\end{equation}
where $\beta$ is the inverse temperature. NPO has shown notable enhancements over GA in preserving the model integrity, which is recognized as the current state-of-the-art within the community.

\textbf{The G-effect across Unlearning Steps.} We show the G-effect of NPO in Figure~\ref{fig: npo_ge}. We observe that its values converge much faster than GA, aligning with previous observations~\citep{zhang2024negative}. 
Moreover, the magnitudes of G-effect for NPO are notably smaller than those observed with GA. In terms of the unlearning G-effect, it indicates that the unlearning strength of NPO is weaker; however, for the retaining G-effect, it suggests that NPO better preserves the model integrity. More importantly, magnitudes of retaining G-effect outweigh those of unlearning when $\beta=1$ or $2$, signifying that the negative impacts on model integrity are less pronounced than the beneficial effects of unlearning, rendering NPO a promising method to mitigate excessive unlearning. 

\textbf{The G-effect across Layers and $\beta$.} Similar to GA, deeper layers exhibit weaker G-effect. However, both the input embedding and output linear layers display negligible values, which are different from the behaviors seen with GA. For both middle and deep layers, their retaining G-effect is relatively small. Furthermore, across different values of the inverse temperature, we observe that larger $\beta$ makes the G-effect converge faster and their magnitudes become smaller. This phenomenon generally arises because smaller $\beta$ causes the NPO formulation more closely resemble to that of GA~\citep{zhang2024negative}, of which the power in controlling the extent of unlearning is weaken. The relationship between GA and NPO is further elucidated below in \eqref{eq: npo gradient}.



\begin{figure}
    \centering
    \subfigure[{$\beta=0.1$}]{\includegraphics[width=0.32\textwidth]{npo_effect_5_p1_v3.png}} ~
    \subfigure[$\beta=1$]{\includegraphics[width=0.32\textwidth]{npo_effect_5_1_v2.pdf}} ~
    \subfigure[$\beta=2$]{\includegraphics[width=0.32\textwidth]{npo_effect_5_2_v2.pdf}}
    \caption{\textbf{The G-effect for NPO.} The \textbf{legends} are summarized in Figure~\ref{fig: legend}. The \textbf{horizontal axis} represents the unlearning step and the \textbf{vertical axis} indicates the values of the G-effect.  }
    \label{fig: npo_ge}
\end{figure}

\begin{figure}
    \centering
    \subfigure[NPO Weights ($\beta=1$)]{\includegraphics[width=0.32\textwidth]{npo_weight_5_1_v2.pdf}}  ~
    \subfigure[PG-effect ($\beta=1$)]{\includegraphics[width=0.32\textwidth]{npo_inst_1_ckpt10_short.pdf}} ~
    \subfigure[{TNPO ($\beta=1$)}]{\includegraphics[width=0.32\textwidth]{ins_npo_1_effect_v3.pdf}}
    \caption{\textbf{The NPO Weighting Mechanisms.} We depict the curves of average NPO weights in (a) and the relationship of NPO weights with PG-effect in (b). Distributions of PG-effect for different value ranges of $w_{s_{\mathrm{u}}}^{\mathrm{npo}}$ are depicted, considering the checkpoints at $5$, $10$, and $15$-th steps jointly. Moreover, darker shades within distribution contours signify the groups of $w_{s_{\mathrm{u}}}^{\mathrm{npo}}$  with overall larger weights. We further depict the G-effect for an improved version of NPO, named TNPO, in (c). The \textbf{horizontal axes} denote the unlearning step for (a) and (c), and the unlearning G-effect for (b). The \textbf{vertical axes} denote the NPO weights for (a), the retaining G-effect for (b), and the G-effect for (c). }
    \label{fig: understand_npo}
\end{figure}

\textbf{Unlearning Mechanisms.} We now aim to understand the factors that contribute to the efficacy of NPO. To begin with, we write the gradients of NPO with respect to $\boldsymbol{\theta}$ in the following
\begin{equation}
\mathbb{E}_{s_{\mathrm{u}}\sim\mathcal{D}_{\mathrm{u}}} w_{s_{\mathrm{u}}}^{\mathrm{npo}} \nabla_{\boldsymbol{\theta}}~\log p(s_{\mathrm{u}};\boldsymbol{\theta}), 
\label{eq: npo gradient}
\end{equation}
with $w_{s_{\mathrm{u}}}^{\mathrm{npo}}=\frac{2p(s_{\mathrm{u}};\boldsymbol{\theta})^\beta}{p(s_{\mathrm{u}};\boldsymbol{\theta})^\beta+p(s_{\mathrm{u}};\boldsymbol{\theta}_{\textrm{o}})^\beta}$. Notably, compared with GA in \eqref{eq: ga graient}, we find that NPO exhibits similar gradient formulation, albeit with a reweighting scheme $w_{s_{\mathrm{u}}}^{\mathrm{npo}}$. Therefore, $w_{s_{\mathrm{u}}}^{\mathrm{npo}}$ primarily contributes to the advantages of NPO over GA, thus requiring our in-depth focus. 


We illustrate the curves of $w^{\mathrm{npo}}_{s_{\mathrm{u}}}$ in Figure~\ref{fig: understand_npo}(a), observing a rapid decrease of $w^{\mathrm{npo}}_{s_{\mathrm{u}}}$ from 1 to 0. The formulation of $w_{s}^{\mathrm{npo}}$ reveals that, as the NPO risk decreases---indicative of the drop in the confidence $p(s_{\mathrm{u}};\boldsymbol{\theta})$---the weight $w_{s_{\mathrm{u}}}^{\mathrm{npo}}$ reduces consequently. This weighting behavior seems quite resemble to WGA. Then, the question arises whether $w_{s_{\mathrm{u}}}^{\mathrm{npo}}$ encompasses some intriguing mechanisms beyond early stopping as in WGA. To further elucidate the G-effect of NPO, we expand it as follows:
\begin{equation}
    \mathbb{E}_{s_{\mathrm{u}}\sim\mathcal{D}_{\mathrm{u}}} w_{s_{\mathrm{u}}}^{\mathrm{npo}} \underbrace{\nabla_{\boldsymbol{\theta}}\mathcal{R}(\mathcal{D};\boldsymbol{\theta}^{(t)}) \nabla_{\boldsymbol{\theta}}\log p(s_{\mathrm{u}};\boldsymbol{\theta}^{(t)})}_{\text{PG-effect of GA}},\label{eq: w vs gae}
\end{equation}
which details the G-effect on individual data points, represented as the product of the NPO weighting term $w_{s_{\mathrm{u}}}^{\mathrm{npo}}$ and the point-wise G-effect (refer to as PG-effect) of GA. 
Accordingly, we plot the joint distributions for the PG-effect of GA with respect to unlearning (i.e., $\mathcal{D}=\mathcal{D}_{\mathrm{u}}$) and retention (i.e., $\mathcal{D}=\mathcal{D}_{\mathrm{t}}\backslash\mathcal{D}_{\mathrm{u}}$) in Figure~\ref{fig: understand_npo}(b). These distributions are categorized into five groups based on the associated different value ranges of $w_{s_{\mathrm{u}}}^{\mathrm{npo}}$. As we can see, the distributions of PG-effects vary notably across different ranges of $w_{s_{\mathrm{u}}}^{\mathrm{npo}}$. Generally speaking, $w_{s_{\mathrm{u}}}^{\mathrm{npo}}$ tends to allocate larger weights to points where the retaining PG-effect is near-zero. it is a preferred scenario as $w_{s_{\mathrm{u}}}^{\mathrm{npo}}$ can prioritize data points that have small negative impacts on model integrity. However, the side effect is to emphasize those data points with less contributions to unlearning, thus compromising the unlearning strengths. We conclude that NPO weighting can prioritize certain points that have small negative impacts on non-targeted data, thereby enhancing the overall model integrity after unlearning. 



\textbf{One Step Further.} We also notice some shortcomings for the NPO weighting mechanism. First, there are many failure cases where some data points with near-zero retaining PG-effect while large unlearning PG-effect is inappropriately assigned with small weights. Also, the distribution of PG-effect with $w_{s_{\mathrm{u}}}^{\mathrm{npo}}$ in the range between 0.4 to 0.6 demonstrates a wrong trend in assigning large weights to those data points that have large negative impacts on model integrity, i.e., notably negative retaining PG-effect. Ideally, we hope the weighting mechanism can prioritize points that not only have near-zero retaining G-effect and also exhibit large negative unlearning G-effect, a capability that the current NPO weighting does not possess.


It is worth noting that our above analysis does not disqualify $w_{s_{\mathrm{u}}}^{\mathrm{npo}}$ as a meaningful mechanism. Indeed, when  $w_{s_{\mathrm{u}}}^{\mathrm{npo}}$ is applied token-wise, which allows for more granular control over the unlearning process, the unlearning procedures are notably more effective. Formally, we consider the objective
\begin{equation}
    \mathbb{E}_{s_{\mathrm{u}}\sim\mathcal{D}_{\mathrm{u}}}  \sum_{i=2}^{\vert s_{\mathrm{u}}\vert}w^{\mathrm{tnpo}}_{s_{\mathrm{u}}, i}\log p(s_{\mathrm{u}}^i|s_{\mathrm{u}}^{<i};\boldsymbol{\theta}),\label{eq: tnpo}
\end{equation}
where $w^{\mathrm{tnpo}}_{s_{\mathrm{u}}, i}= \frac{2p(s_{\mathrm{u}}^i|s_{\mathrm{u}}^{<i};\boldsymbol{\theta})^{\beta}}{p(s_{\mathrm{u}}^i|s_{\mathrm{u}}^{<i};\boldsymbol{\theta})^\beta+p(s_{\mathrm{u}}^i|s_{\mathrm{u}}^{<i};\boldsymbol{\theta}_{\textrm{o}})^\beta}$ generalizes the weighting mechanism of NPO for tokens. We refer to \eqref{eq: tnpo} as token-wise NPO (TNPO) and show its G-effect in Figure~\ref{fig: understand_npo}(c). Therein, we observe that the unlearning G-effect exhibits sufficiently large negative values while the retaining G-effect is overall close-to-zero. It underscores the efficacy of $w^{\mathrm{tnpo}}_{s_{\mathrm{u}}, i}$ in properly prioritizing certain tokens during unlearning, thus achieving unlearning efficacy. Please refer to Appendix~\ref{app: tnpo} for more discussions about TNPO, as well as its further improved version named WTNPO. 



\subsection{More  Objectives}

We also examine two other unlearning objectives that do not fall into the variants of GA.

\setlength{\intextsep}{10pt}
\begin{wrapfigure}{r}{0.45\textwidth}
  \begin{center}
    \includegraphics[width=0.35\textwidth]{idk_effect_part_v2.pdf}
  \end{center}
  \caption{\textbf{The G-effect for PO.} The \textbf{legends} for the G-effect are in Figure~\ref{fig: legend}. The \textbf{horizontal axis} denotes the unlearning step and the \textbf{vertical axis} is the values of the G-effect.}\label{fig: ge_idk}
\end{wrapfigure}

\textbf{Preference Optimization (PO)}~\citep{maini2024tofu} overwrites LLMs with new outcomes instead of erasing old ones. Given some prefix $s^{<i}$ and the new suffix $s_{\text{po}}$, the PO unlearning objective is given by
\begin{equation}
\mathbb{E}_{s_{\mathrm{u}}\sim\mathcal{D}_{\mathrm{u}}}-\log p(s_{\text{po}}|s^{<i};\boldsymbol{\theta}).
    \label{eq: po}
\end{equation}
It is particular suitable for LLMs fine-tuned for question answering, where $s^{<i}$ is the original question and $s_{\text{po}}$ is the new answer. We show its G-effect in Figure~\ref{fig: ge_idk}. Unfortunately, we note that the PO may not be suitable for LLM unlearning: Its validity in erasing targeted knowledge is limited to the early phases of model updating. Then, PO may even inadvertently facilitate the knowledge relearning. 

\textbf{Representation Misdirection for Unlearning (RMU)}~\citep{li2024wmdp} implements unlearning by perturbing model representation. Denote the embedding features by $\boldsymbol\phi(s;\boldsymbol{\theta})$, RMU is articulated as
\begin{equation}
    \mathbb{E}_{s_{\mathrm{u}}\sim\mathcal{D}_{\mathrm{u}}} \frac{1}{\vert s\vert-1}\sum_{i=1}^{\vert s\vert-1}\vert\vert\boldsymbol{\phi}(s^{<i};\boldsymbol{\theta})-c\cdot\boldsymbol{u}\vert\vert_2^2,
    \label{eq: rmu}
\end{equation}
where $\boldsymbol{u}$ is a random vector with elements sampled uniformly from $[0, 1)$ and $c$ is a scaling hyper-parameter. We adopt outputs for 11-th, 22-th, and 33-th (before unembedding) layers as $\boldsymbol\phi(s;\boldsymbol{\theta})$, and their G-effect is summarized in Figure~\ref{fig: rmu_effect}. We notice that its performance is very sensitive to different choices of $\boldsymbol\phi(s;\boldsymbol{\theta})$, where middle (22-th) layers seem to be a better choice than shallow (11-th) and deep (33-th) layers. In Appendix~\ref{app: rmu}, we further show that RMU is also sensitive to varying $c$, where a wrong configuration may be even completely contrary to the goal of unlearning. 

Moreover, we observe that the improvements on unlearning come at similar costs in terms of impairing the general utility, a phenomenon reminiscent of the challenges faced with the vanilla GA. 
It can also be considered as a scenario of excessive unlearning, where the magnitudes of parameter updates are too large, thus failing to preserve essential knowledge for common data. Given its current limitations, more explorations are needed to advance unlearning through embedding perturbation.





\begin{figure}
    \centering
    \subfigure[11-th layer ($c=5$)]{\includegraphics[width=0.32\textwidth]{rmu10_5_effect_v2.pdf}} ~
    \subfigure[22-th layer ($c=5$)]{\includegraphics[width=0.32\textwidth]{rmu21_5_effect_v3.pdf}} ~
    \subfigure[33-th layer ($c=5$)]{\includegraphics[width=0.32\textwidth]{rmu31_5_effect_v2.pdf}}
    \caption{\textbf{The G-effect for RMU}. The \textbf{legends} for the G-effect are summarized in Figure~\ref{fig: legend}. The \textbf{horizontal axis} denotes the unlearning step and the \textbf{vertical axis} indicates the values of the G-effect.}
    \label{fig: rmu_effect}
\end{figure}

\subsection{Regularization}
\label{sec: reg}
\begin{figure}
    \centering
    \subfigure[{GD}]{\includegraphics[width=0.32\textwidth]{reg_gd_effect_v3.png}} ~
    \subfigure[KL]{\includegraphics[width=0.32\textwidth]{reg_kl_effect_v2.pdf}} ~
    \subfigure[RR]{\includegraphics[width=0.32\textwidth]{reg_rr_effect_v2.pdf}}
    \caption{\textbf{The G-effect for Regularization}. The \textbf{legends} for G-effect are in Figure~\ref{fig: legend}. The \textbf{horizontal axis} denotes the unlearning step and the \textbf{vertical axis} indicates the values of the G-effect.}
    \label{fig: reg_effect}
\end{figure}

Although we have identified several promising objectives, the retaining G-effect overall remains negative. It indicates that there are still adverse effects on the common model integrity. We also want to note that, while some of the magnitudes are steadily small, e.g., for the retaining G-effect of TNPO in Figure~\ref{fig: understand_npo}(c), their accumulation 
across steps will still have a notable impact.  
A wide-accepted strategy to improve retention is by regularization, involving a set of additional common data to maintain the original model responses. In this section, we explore 3 representative regularization terms, named gradient difference (GD)~\citep{yao2023large}, KL divergence (KL)~\citep{maini2024tofu}, and representation retention (RR)~\citep{li2024wmdp} (cf., Appendix~\ref{app: reg}). We choose NPO as the unlearning objective, computing the G-effect for various regularization terms. The results are summarized in Figure~\ref{fig: reg_effect}. 
Overall, our observations indicate that RR does not serve for effective regularization due to its unstable G-effect behaviors. In contrast, both GD and KL effectively facilitate knowledge retention. However, the strength of the G-effect associated with KL surpasses that of GD, leading us to suggest KL as a default choice of regularization for retention.


\section{Evaluations}
\label{sec: experiment}

In this section, we further benchmark aforementioned unlearning objectives on the TOFU unlearning datasets~\citep{maini2024tofu}, focusing on the removal of fictitious author profiles from LLMs fine-tuned on them. Comprising a series of question-answer pairs about author profiles, the TOFU dataset is further separated into targeted and non-targeted parts, thereby providing an intuitive platform to effectively evaluate the impact of various unlearning methods. 


We test two popular LLMs: Phi-1.5~\citep{li2023textbooks} and Llama-2-7B~\citep{touvron2023llama2}, under three ratios---1\%, 5\%, and 10\%---of targeted data. 
For hyper-parameter tuning, we follow the unlearning with control (UWC) framework~\citep{wang2024unlearning}, which surpasses the challenges of trade-offs between removal and retention.
Please refer to Appendix~\ref{app: experiment setups} for additional details on the experimental setups and Appendix~\ref{app: tuning} for hyper-parameter configurations. 


\textbf{Configurations.}
We default to apply the following settings: the AdamW optimizer~\citep{loshchilov2017decoupled}, a batch size of $16$, a maximal gradient norm of $1$, and the (un)learning rate of $2e^{-5}$ for Phi-1.5 and $1e^{-5}$ for Llama-2-7b with linear warm-up for the first epoch. Each method is executed over a total of 5 epochs.
Moreover, the model-specific hyper-parameters after fine-tuning are as follows:
{For the 1\% and 5\% setups}, we set $\alpha=5$ for WGA; $\beta=0.5$ for NPO; $\beta=4$ for TNPO; $\alpha=1.5$ and $\beta=5$ for WTNPO. {For the 10\% setup}, we set $\alpha=7$ for WGA; $\beta=0.5$ for NPO; $\beta=5$ for TNPO; $\alpha=1.5$ and $\beta=7$ for WTNPO. 
For RMU, we set the 9-th layer with $c=4$ for Phi-1.5 and the 21-th layer with $c=2$ for Llama-2-7B.  
Moreover, our experiments are conducted on computation nodes equipped with NVIDIA-A100-80GB GPUs and Intel(R) Xeon(R) Gold 6248R CPUs. The systems utilize Transformers version 4.42.4 and CUDA version 12.1.





% For evaluation metrics, we report the PS scores suggested by~\citep{wang2024unlearning}, which quantify the extent to which knowledge parameterized within unlearned models. We also report FQ and MU as in~\citep{maini2024tofu}. 

\textbf{Evaluation Metrics.} We adopt the suggest evaluation metrics from~\citep{maini2024tofu}, specifically forget quality (FQ) for unlearning and  model utility (MU) for retention. FQ evaluates model performance by jointly examining output quality, confidence, and truth ratio, fully reflecting the common model integrity. MU produces $p$-values to assess the change of model outputs between the gold standard model, which is trained from scratch without targeted data, and the unlearned model. We utilize the log-scale for these $p$-value to make the results more readable. We aim for high values in both FQ and MU.
% Nevertheless, FQ is not ideally suited for the LLM goals of unlearning, as discussed in Section~\ref{sec: preliminary}---it can occur that, despite extensive removal of targeted data, FQ remains small as model behaviors of unlearning are much stronger than those of the gold standard models.
We further report the ES scores~\citep{wang2024unlearning}, which more directly quantify the extent of knowledge parameterized within models, potentially making them more effective metrics than FQ and MU.
The ES scores can be calculated for either targeted data or non-targeted data, thereby reflecting model performance of removal and retention, respectively. 
Notably, the ES scores are available in two variants: ES-exact, which is used for original data to reflect direct parametrization, and ES-perturb, which applies to their rephrasing to reflect generalization. Overall, the ES scores should be high for retention and low for removal. 


\begin{table}[t]
\centering
\caption{\textbf{Comparison between Unlearning Objectives} on TOFU with \textbf{KL regularization} to stabilize unlearning.  $\downarrow$ / $\uparrow$ indicate smaller / larger values are preferable. The log scale is used for FQ to improve readability. The top two results are in bold font for each unlearning setup. 
% The results with FQ are not highlighted, as it is a less meaningful metric that deviates the unlearning goalsâ€º of LLMs.
}\label{tab: ps main}\vspace{5pt}
\resizebox{0.99\textwidth}{!}{
\begin{tabular}{ccccccccccccccc}
\toprule[1.5pt]
\multicolumn{2}{c}{LLM} & \multicolumn{6}{c}{Phi-1.5} && \multicolumn{6}{c}{Llama-2-7B} \\ \cline{3-8}\cline{10-15}
\multirow{2}{*}{setup} & \multirow{2}{*}{method} &  \multicolumn{2}{c}{ES-exact} & \multicolumn{2}{c}{ES-perturb} & \multirow{2}{*}{MU $\uparrow$ } & \multirow{2}{*}{FQ $\uparrow$ } && \multicolumn{2}{c}{ES-exact} & \multicolumn{2}{c}{ES-perturb} & \multirow{2}{*}{MU $\uparrow$ } & \multirow{2}{*}{FQ $\uparrow$ }\\
& & retain $\uparrow$ & unlearn $\downarrow$ & retain $\uparrow$ & unlearn $\downarrow$ & & && retain $\uparrow$ & unlearn $\downarrow$ & retain $\uparrow$ & unlearn $\downarrow$ \\ 
\midrule[1.2pt]
\multicolumn{2}{c}{before unlearning} & 0.4433 & 0.5969 & 0.2115 & 0.1605 & 0.5232 & -5.8031 && 0.8277 & 0.8039 & 0.5302 & 0.4001  &0.6345 & -7.5930\\
\cline{3-8}\cline{10-15}
\multirow{7}{*}{1\%} 
& GA    & 0.1103 & 0.0530 & 0.0850 & 0.0828 & 0.3799 & -0.5471 && 0.4298 & \textbf{0.0570} & 0.2692 & \textbf{0.0422} & 0.5378 & -0.5471 \\
& PO    & \textbf{0.3667} & 0.8472 & 0.1622 & 0.3658 & 0.5112 & -4.2474 && \textbf{0.7508} & 0.8359 & \textbf{0.4724} & 0.5259 & 0.6246 & -5.8031 \\
& WGA   & 0.3629 & \textbf{0.0344} & \textbf{0.1857} & \textbf{0.0282} & \textbf{0.5191} & -0.5471 && {0.6701} & 0.0818 & 0.3814 & 0.0601 & \textbf{0.6541} & \textbf{-0.0847} \\
& NPO   & 0.2727 & 0.0916 & 0.1125 & 0.0733 & 0.4845 & -2.9162 && 0.4757 & 0.1216 & 0.3890 & 0.0905 & 0.6243 & -1.3254 \\
& TNPO  & 0.3351 & 0.0365 & 0.1239 & 0.0412 & 0.4991 & \textbf{-0.0847} && 0.5168 & \textbf{0.0337} & \textbf{0.4304} & \textbf{0.0337} & \textbf{0.6495} & \textbf{-0.0847} \\
& WTNPO & \textbf{0.4117} & \textbf{0.0285} & \textbf{0.1969} & \textbf{0.0255} & \textbf{0.5126} & \textbf{-0.2667} && \textbf{0.6701} & 0.0807 & 0.3734 & 0.0601 & 0.6453 & {-0.0847} \\
& RMU   & 0.2397 & 0.0850 & 0.1539 & 0.0567 & 0.4349 & -0.5471 && 0.2397 & 0.0850 & 0.1539 & 0.0567 & 0.5298 & -1.3254 \\
\midrule[0.8pt]
\multicolumn{2}{c}{before unlearning} & 0.4433 & 0.5619 & 0.2115 & 0.2374  &0.5232 & -29.6514&& 0.8277 & 0.7735 & 0.5302 & 0.4126  &0.6345 & -32.1330\\
\cline{3-8}\cline{10-15}
\multirow{7}{*}{5\%} 
& GA    & 0.0000 & \textbf{0.0000} & 0.0000 & \textbf{0.0000} & 0.0000 & -11.4040 && 0.0300 & \textbf{0.0000} & 0.0206 & \textbf{0.0000} & 0.0000 & \textbf{-12.4230} \\
& PO    & \textbf{0.2646} & 0.7986 & \textbf{0.1639} & 0.4925 & \textbf{0.5118} & -26.5061 && \textbf{0.5572} & 0.8437 & 0.3652 & 0.4933 & \textbf{0.6466} & -28.8476 \\
& WGA   & \textbf{0.2980} & 0.0179 & \textbf{0.1645} & 0.0199 & \textbf{0.5108} & \textbf{-1.3076} && 0.4709 & 0.0053 & \textbf{0.3982} & 0.0050 & \textbf{0.6438} & -16.3271 \\
& NPO   & 0.0876 & 0.1267 & 0.0876 & 0.0609 & 0.3841 & -7.7503 && 0.1747 & 0.0764 & 0.1273 & 0.0802 & 0.5285 & \textbf{-9.9550} \\
& TNPO  & 0.1695 & 0.0126 & 0.0803 & 0.0038 & 0.4673 & -2.1867 && \textbf{0.5017} & 0.0160 & 0.3495 & 0.0099 & 0.6348 & -32.1330 \\
& WTNPO & 0.2185 & 0.0179 & 0.1281 & 0.0188 & 0.5084 & {-3.2299} && 0.4595 & 0.0061 & \textbf{0.3989} & 0.0040 & 0.6410 & -21.4429 \\
& RMU   & 0.2162 & \textbf{0.0000} & 0.1299 & \textbf{0.0000} & 0.2744 & \textbf{-1.9514} && 0.1262 & \textbf{0.0000} & 0.1299 & \textbf{0.0000} & 0.5801 & -21.4429 \\
\midrule[0.8pt] 
\multicolumn{2}{c}{before unlearning} & 0.4433 & 0.4799 & 0.2115 & 0.1843  &0.5232 & -39.0042 && 0.8277 & 0.8307 & 0.5302 & 0.3099  &0.6345 & -44.4594\\
\cline{3-8}\cline{10-15}
\multirow{7}{*}{10\%} 
& GA    & 0.0000 & \textbf{0.0000} & 0.0000 & \textbf{0.0000} & 0.0000 & -45.2697 && 0.0000 & \textbf{0.0000} & 0.0000 & \textbf{0.0000} & 0.0000 & -20.8637 \\
& PO    & \textbf{0.3222} & 0.7321 & 0.1406 & 0.2667 & 0.5078 & -38.2556 && \textbf{0.5572} & 0.8437 & \textbf{0.3777} & 0.4305 & \textbf{0.6240} & -39.7604 \\
& WGA   & \textbf{0.3466} & \textbf{0.0000} & \textbf{0.1651} &\textbf{0.0000} & \textbf{0.5183} & -9.0636 && \textbf{0.6642} & 0.0287 & \textbf{0.4289} & \textbf{0.0123} & 0.6235 & {-24.8591} \\
& NPO   & 0.0859 & 0.0955 & 0.0716 & 0.0710 & 0.3878 & -10.5721 && 0.1296 & 0.1388 & 0.1085 & 0.1440 & 0.5055 & \textbf{-12.1912} \\
& TNPO  & 0.2085 & 0.0163 & 0.0991 & 0.0134 & 0.5009 & {-7.6651} && 0.4531 & \textbf{0.0192} & 0.2690 & 0.0165 & \textbf{0.6381} & \textbf{-13.4785} \\
& WTNPO & 0.2969 & 0.0048 & \textbf{0.1862} & 0.0105 & \textbf{0.5123} & \textbf{-7.0070} && 0.4997 & 0.0278 & 0.3246 & 0.0174 & {0.6236} & -26.6801 \\
& RMU   & 0.0317 & 0.0541 & 0.0357 & 0.0632 & 0.3163 & \textbf{-7.0070} && 0.2580 & 0.0194 & 0.2017 & 0.0174 & 0.5930 & -16.7271 \\
\bottomrule[1.5pt]
\end{tabular}}
\end{table}


\textbf{Analysis.} The results are summarized in Table~\ref{tab: ps main}, where we use KL regularization to stabilize the unlearning procedures. Among previous methods, PO is identified as the least attractive choice, which may even inadvertently maintain data that ought to be unlearned, corroborating our observations from the G-effect analysis. Conversely, GA is most effective in removing targeted data but at the expense of compromising model integrity. 
Both NPO and RMU offer a better balance between data removal and retention, with NPO overall outperforming RMU (except for 10\% unlearning with Llama-2-7B). This can be attributed to the more stable G-effect of NPO over that of RMU.

For our newly explored methods, WGA remarkably overcomes the drawbacks of GA, particularly in mitigating excessive unlearning, meanwhile maintaining its strong capability for the removal of targeted data. Additionally, both TNPO and WTNPO improve upon NPO by not only enhancing unlearning performance but also excelling in retaining common performance. WTNPO typically outperforms TNPO as it further mitigates the potential issues of excessive unlearning observed in TNPO. Overall, when comparing methods across different unlearning setups and models, WGA and WTNPO stand out as the most effective, underscoring the crucial role of loss weighting in the unlearning process for LLMs. However, we recommend the default use of WGA, as it requires tuning only one hyper-parameter and generally perform well, recognized as effective for LLM unlearning. 

It is worth noting that most of the above analyses are based on our results measured by ES. When it comes to MU and FQ, our refined methods can still outperform it previous counterparts in most cases, suggesting that the goals of full removal and influence removal are generally aligned. 
However, there is also an abnormal case of 5\% unlearning with Llama-2-7B, where the MU scores of our proposed methods remain high, while the FQ for NPO notably surpasses our methods. Given that our methods better preserve retention, we believe they offer more practical applicability than NPO. 
Nonetheless, the disagreement between ES and FQ metrics still deserves our further attention, necessitating deeper explorations into the reliability of these metrics and determining which goals, either full removal or influence removal, are better suitable for LLM unlearning. 

% When analyzing FQ, we find that our previous conclusions typically hold, except for the Llama-2-7B model under 5\% and 10\% unlearning setups, where the FQ values for NPO are better than those for WGA and TNPO. However, FQ measures the difference in model behaviors over the gold standard, which is trained without targeted data. In scenarios where the extent of removal after unlearning is much higher than that of the gold standard, this can also result in extremely low values of FQ. Jointly considering the results of ES, we find that the unlearned Llama-2-7B models have largely removed the targeted knowledge for WGA and TNPO, indicating that the above scenario in exceeding the gold standard occurs. Such results may further reflect that larger models have a greater capability to effectively unlearn targeted data.

\section{Conclusions}
LLM unlearning aims to eliminate unwanted knowledge while preserving the overall model integrity. This paper particularly focuses on understanding the mechanisms behind various unlearning objectives, based on our proposed evaluation tool named the G-effect. Our findings suggest that GA-based unlearning objectives remain to be promising, but we need to mitigate the risk of excessive unlearning and the potential harm on model integrity. We further introduce advanced unlearning objectives, such as WGA and TNPO, that set as new state-of-the-arts within the community. 

\textbf{Drawbacks of the G-effect.} As shown in Appendix~\ref{app: g-effect}, to motivate the G-effect, we assume that singular values of the matrix $A$ have low variance. However, it may neglect important model properties with unlearning smoothness. Refining the G-effect to better incorporate $A$ could make the evaluation scheme more accurate and insightful. However, its computation requires estimating the Hessian matrix, a tedious process that needs approximation~\citep{singh2020woodfisher}. Also, using NLL as the risk metric to define $\mathcal{R}$ may not be the optimal choice, given that model likelihood can be misleading to characterize the knowledge parametrization~\citep{duan2024membership}. 

\textbf{Promising Directions.} Although we achieve several powerful unlearning objectives, their practical implementations still require regularization for retention; otherwise, the common model integrity will be compromised. Thus, further enhancements in unlearning objectives are anticipated, such as devising improved weighting mechanisms~\citep{ren2018learning} and exploring robust representation methods. Beyond refining unlearning objectives, the investigation of advanced optimization approaches is also crucial, including sub-model updating~\citep{yao2024knowledge} and layer-adapted updating~\citep{schaul2013no}. On the data-oriented side, unlearning methods that incorporate filtering or prompting to foster improved G-effect  behaviors also be intriguing, while currently are not covered.


\section*{Ethic Statement and Reproducibility}

Unlearning mechanisms are crucial for LLMs, as they facilitate the removal of sensitive data that may lead to copyright and privacy violations, significantly boosting the overall confidentiality of models. By identifying and eradicating privacy risks, we fulfill the ethical obligation to respect individual privacy. Adapting LLMs to prevent the replication of sensitive information further aligns with the principles of responsible data use. In essence, the process of unlearning in LLMs enhances societal well-being by improving both the safety and legal compliance of these technologies.
Additionally, we benefit the research community by introducing a new analytical tool, the G-effect, designed to measures the comprehensive impacts of unlearning objectives on LLMs. This tool facilitates a detailed analysis of existing unlearning objectives and offers the potential to evaluate the efficacy of a broad range of new methods. The deployment of such a toolkit contributes to open inquiry and could encourage collaboration and further studies in this pivotal area.
For reproducibility, we have detailed the configurations, hyper-parameter setups, and hardware specifications. The code is publicly available at: \url{https://github.com/tmlr-group/G-effect}.



\section*{Acknowledgments}
JPZ is supported by grant from the Natural Sciences and Engineering Research Council of Canada (NSERC) (567916).
The authors also would like to express their sincere gratitude to the anonymous reviewers and the area chairs for their thorough review and constructive feedback. Their insightful comments and valuable suggestions have significantly enhanced the quality and clarity of this manuscript. We deeply appreciate their time and effort in helping us improve our work.


\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\clearpage
\appendix


\section{A Formal Motivation for the G-effect}
\label{app: g-effect}

\textbf{Overview.} To formalize our key concept of the G-effect, we begin by examining the impacts of an unlearning objective $\mathcal{L}_{\mathrm{u}}$ on model parameters $\boldsymbol{\theta}$ with mini-batch gradient updates. We simplify the expression for the unlearned parameters $\boldsymbol{\theta}_{\mathrm{u}}$ such that it is independent of the intermediate parameter stages, cf. \eqref{eq: sgd_accu_unlearning}. Then, substituting the approximation of $\boldsymbol{\theta}_{\mathrm{u}}$ into $\mathcal{R}(\mathcal{D};\boldsymbol{\theta}_{\mathrm{u}})$, we observe that the change in model performance  can be primarily characterized by the dot product of gradients between the risk metric $\mathcal{R}$ and the unlearning objective $\mathcal{L}_{\mathrm{u}}$, cf. \eqref{eq: estimated perf}. Its generalized version leads to our G-effect in Definition~\ref{def: gradient effect}. Please see below for a formal description.

Without loss of generality, we consider an objective $\mathcal{L}_{\mathrm{u}}$ and a sequence of mini-batches $\{S_{\mathrm{u}}^{(t)}\}_T$ that are randomly drawn from $\mathcal{D}_{\mathrm{u}}$. These batches are sequentially fed in LLMs to minimize $\mathcal{L}_{\mathrm{u}}$. Specifically, for the $t$-th iteration, the model parameters are updated from $\boldsymbol{\theta}^{(t-1)}$ to $\boldsymbol{\theta}^{(t)}$ following
\begin{equation}
    \boldsymbol{\theta}^{(t)}\leftarrow\boldsymbol{\theta}^{(t-1)}-\texttt{lr}\nabla_{\boldsymbol{\theta}}\mathcal{L}_{\mathrm{u}}(S_{\mathrm{u}}^{(t-1)};\boldsymbol{\theta}^{(t-1)}),\label{eq: sgd_unlearning}
\end{equation}
with $\texttt{lr}$ the (un)learning rate. To understand the impacts of \eqref{eq: sgd_unlearning} on model parameters and subsequent effects on model performance, we further simplify the accumulative effects of gradient updates: When assuming $\texttt{lr}$ is small and each point in $\mathcal{D}_{\mathrm{u}}$ occurs $k$ times within $\{S_{\mathrm{u}}^{(t)}\}_T$, we can approximate the final parameters after unlearning as
\begin{equation}
\boldsymbol{\theta}^{(T)}\approx\boldsymbol{\theta}^{(0)}-\texttt{lr} k A \nabla_{\boldsymbol{\theta}}\mathcal{L}_{\mathrm{u}}(\mathcal{D}_{\mathrm{u}};\boldsymbol{\theta}^{(0)}). \label{eq: sgd_accu_unlearning}
\end{equation}
$A$ is a symmetric matrix associated with model smoothness and orders of mini-batches. Also, $A$ will converge to the identity matrix when $\alpha$ approaches 0. Please see below for the detailed derivations.


\begin{proposition} \label{theorem1}
    Given the original parameters $\boldsymbol{\theta}^{(0)}$ and the objective $\mathcal{L}$. During the stochastic gradient updates, the model will receive a sequence of $T$ random mini-batches of samples $\{S^{(t)}\}_T$, which will be fed into the model orderly via $\boldsymbol{\theta}^{(t)}\leftarrow\boldsymbol{\theta}^{(t-1)}-\texttt{lr}\nabla_{\boldsymbol{\theta}}\mathcal{L}(S^{(t-1)};\boldsymbol{\theta}^{(t-1)})$. With a small  $\texttt{lr}$, we can approximate the final parameters $\boldsymbol{\theta}^{(T)}$ after stochastic gradient updates as
    \begin{equation}
        \boldsymbol{\theta}^{(T)}\approx
        \boldsymbol{\theta}^{(0)}-\texttt{lr} A\sum_{t=0}^{T-1}\nabla_{\boldsymbol{\theta}}\mathcal{L}(S^{(t)};\boldsymbol{\theta}^{(0)}),
    \end{equation}
    where $A=I-\texttt{lr}\sum_{t=1}^{T-1}\nabla^2_{\boldsymbol{\theta}}\mathcal{L}(S^{(t)};\boldsymbol{\theta}^{(0)})$ and $I$ is the identify matrix. The matrix $A$ characterizes the smoothness with respect to $\mathcal{L}$, the impacts of $\texttt{lr}$, and the influence of ordering within $\{S^{(t)}\}_T$.
\end{proposition}

\begin{proof}
We begin by showing parameter changes after two consecutive steps, i.e., from the $t$-th to the $t+2$-th step. Substituting $\boldsymbol{\theta}^{(t+1)}\leftarrow\boldsymbol{\theta}^{(t)}-\texttt{lr}\nabla_{\boldsymbol{\theta}}\mathcal{L}(S^{(t)};\boldsymbol{\theta}^{(t)})$ into $\boldsymbol{\theta}^{(t+2)}\leftarrow\boldsymbol{\theta}^{(t+1)}-\texttt{lr}\nabla_{\boldsymbol{\theta}}\mathcal{L}(S^{(t+1)};\boldsymbol{\theta}^{(t+1)})$, we can express the parameter update at $t+2$-th step in terms of $\boldsymbol{\theta}^{(t)}$ as
\begin{equation*}
    \boldsymbol{\theta}^{(t+2)}\leftarrow\boldsymbol{\theta}^{(t)}-\texttt{lr}\nabla_{\boldsymbol{\theta}}\mathcal{L}(S^{(t)};\boldsymbol{\theta}^{(t)})-\texttt{lr}\nabla_{\boldsymbol{\theta}}\mathcal{L}(S^{(t+1)};\boldsymbol{\theta}^{(t)}-\texttt{lr}\nabla_{\boldsymbol{\theta}}\mathcal{L}(S^{(t)};\boldsymbol{\theta}^{(t)})).
\end{equation*}
When further applying the first-order Taylor approximation around $\boldsymbol{\theta}^{(t)}$, we have
\begin{align*}
    \boldsymbol{\theta}^{(t+2)}\approx\boldsymbol{\theta}^{(t)}-\texttt{lr}\big[\nabla_{\boldsymbol{\theta}}\mathcal{L}(S^{(t)};\boldsymbol{\theta}^{(t)})+&\nabla_{\boldsymbol{\theta}}\mathcal{L}(S^{(t+1)};\boldsymbol{\theta}^{(t)})\\+&\nabla^2_{\boldsymbol{\theta}}\mathcal{L}(S^{(t+1)};\boldsymbol{\theta}^{(t)})(-\texttt{lr}\nabla_{\boldsymbol{\theta}}\mathcal{L}(S^{(t+1)};\boldsymbol{\theta}^{(t)}) )\big].
\end{align*}
The above formulation can be expanded to incorporating more updating steps: Considering the accumulations of gradient updating from the $0$-th to $T$-th steps, we have 
\begin{equation*}
    \boldsymbol{\theta}^{(T)}\approx\boldsymbol{\theta}^{(0)}-\texttt{lr}\sum_{t=0}^{T-1}\nabla_{\boldsymbol{\theta}}\mathcal{L}(S^{(t)};\boldsymbol{\theta}^{(0)})+\sum_{t=1}^{T-1}\psi^{(t)} \label{eq:theorem1_1}
\end{equation*}
where $\psi^{(t)}=-\texttt{lr}\nabla^2_{\boldsymbol{\theta}}\mathcal{L}(S^{(t)};\boldsymbol{\theta}^{(0)})\big(-\texttt{lr}\sum_{t'=0}^{T-1}\nabla_{\boldsymbol{\theta}}\mathcal{L}(S^{(t')};\boldsymbol{\theta}^{(0)})+\sum_{t'=0}^{T-1}\psi^{(t')}\big)$ and $\psi^{(0)}=0$. When the learning rate $\texttt{lr}$ is small (e.g., notably less than 1), the influence of higher-order terms with respect to $\texttt{lr}$ diminishes. Therefore, we can further simplify the formulation of $\psi^{(t)}$ as $\psi^{(t)}\approx\texttt{lr}^2\nabla^2_{\boldsymbol{\theta}}\mathcal{L}(S^{(t)};\boldsymbol{\theta}^{(0)})\sum_{t'=0}^{T-1}\nabla_{\boldsymbol{\theta}}\mathcal{L}(S^{(t')};\boldsymbol{\theta}^{(0)})$.
Substituting the approximation of $\psi^{(t)}$ back into the formulation of $\boldsymbol{\theta}^{(T)}$, we complete the proof. The
analysis is motivated by~\citep{thudi2022unrolling}.
\end{proof}


\textbf{What Ensures a Good Unlearning Objective?} We go beyond \eqref{eq: sgd_accu_unlearning} and substitute it into $\mathcal{R}(\mathcal{D};\boldsymbol{\theta}_{\mathrm{u}})$. When the difference between the unlearned model $\boldsymbol{\theta}_{\mathrm{u}}$ and the original model $\boldsymbol{\theta}_{\mathrm{o}}$ is acceptably small, we can apply the first-order Taylor expansion upon $\mathcal{R}(\mathcal{D};\boldsymbol{\theta}_{\mathrm{u}})$, which can help us to simplify the formulation of the performance change by
\begin{equation}
    \mathcal{R}(\mathcal{D};\boldsymbol{\theta}_{\mathrm{u}})-\mathcal{R}(\mathcal{D};\boldsymbol{\theta}_{\mathrm{o}})\approx-\texttt{lr} k \nabla_{\boldsymbol{\theta}}\mathcal{R}(\mathcal{D};\boldsymbol{\theta}_{\mathrm{o}})^\top A \nabla_{\boldsymbol{\theta}}\mathcal{L}_{\mathrm{u}}(\mathcal{D}_{\mathrm{u}};\boldsymbol{\theta}_{\mathrm{o}}).
\end{equation}
One step further, by eigenvalue decomposition, $\nabla_{\boldsymbol{\theta}}\mathcal{R}(\mathcal{D};\boldsymbol{\theta}_{\mathrm{o}})^\top A \nabla_{\boldsymbol{\theta}}\mathcal{L}_{\mathrm{u}}(\mathcal{D}_{\mathrm{u}};\boldsymbol{\theta}_{\mathrm{o}})$ is lower and upper bounded by $\lambda_{\text{min}}\vert\vert\nabla_{\boldsymbol{\theta}}\mathcal{R}(\mathcal{D};\boldsymbol{\theta}_{\mathrm{o}})\vert\vert~\vert\vert\nabla_{\boldsymbol{\theta}}\mathcal{L}_{\mathrm{u}}(\mathcal{D}_{\mathrm{u}};\boldsymbol{\theta}_{\mathrm{o}})\vert\vert$ and $\lambda_{\text{max}}\vert\vert\nabla_{\boldsymbol{\theta}}\mathcal{R}(\mathcal{D};\boldsymbol{\theta}_{\mathrm{o}})\vert\vert~\vert\vert\nabla_{\boldsymbol{\theta}}\mathcal{L}_{\mathrm{u}}(\mathcal{D}_{\mathrm{u}};\boldsymbol{\theta}_{\mathrm{o}})\vert\vert$. $\lambda_{\text{min}}$ and $\lambda_{\text{max}}$ are the minimal and the maximal eigenvalues of $A$.
Furthermore, when $\alpha$ is small, the difference between $\lambda_{\text{min}}$ and $\lambda_{\text{max}}$ is negligible, thus existing $\lambda\in[\lambda_{\text{min}},\lambda_{\text{max}}]$ such that $\lambda \nabla_{\boldsymbol{\theta}}\mathcal{R}(\mathcal{D};\boldsymbol{\theta}_{\mathrm{o}})^\top\nabla_{\boldsymbol{\theta}}\mathcal{L}_{\mathrm{u}}(\mathcal{D}_{\mathrm{u}};\boldsymbol{\theta}_{\mathrm{o}})$ is a good approximation of $\nabla_{\boldsymbol{\theta}}\mathcal{R}(\mathcal{D};\boldsymbol{\theta}_{\mathrm{o}})^\top A \nabla_{\boldsymbol{\theta}}\mathcal{L}_{\mathrm{u}}(\mathcal{D}_{\mathrm{u}};\boldsymbol{\theta}_{\mathrm{o}})$. Thus, 
\begin{equation}
    \mathcal{R}(\mathcal{D};\boldsymbol{\theta}_{\mathrm{u}})-\mathcal{R}(\mathcal{D};\boldsymbol{\theta}_{\mathrm{o}})\approx-\texttt{lr} k \lambda \nabla_{\boldsymbol{\theta}}\mathcal{R}(\mathcal{D};\boldsymbol{\theta}_{\mathrm{o}})^\top \nabla_{\boldsymbol{\theta}}\mathcal{L}_{\mathrm{u}}(\mathcal{D}_{\mathrm{u}};\boldsymbol{\theta}_{\mathrm{o}}).\label{eq: estimated perf}
\end{equation}
Moreover, when taking $\texttt{lr} k \lambda$ as a constant, we conclude that the dot product between $\nabla_{\boldsymbol{\theta}}\mathcal{R}(\mathcal{D};\boldsymbol{\theta}_{\mathrm{o}})$ and $\nabla_{\boldsymbol{\theta}}\mathcal{L}_{\mathrm{u}}(\mathcal{D}_{\mathrm{u}};\boldsymbol{\theta}_{\mathrm{o}})$ quantifies the impacts of $\mathcal{L}_{\mathrm{u}}$ on model performance measured by $\mathcal{R}(\mathcal{D};\boldsymbol{\theta}_{\mathrm{u}})$. 
Specifically, echoing the general goal of LLM unlearning in Section~\ref{sec: preliminary}, we can claim that a good unlearning objective should meet the following two conditions jointly:
\begin{itemize}[leftmargin=15pt]
    \item\textbf{Removal.} We define $e_{\mathrm{u}}=\nabla_{\boldsymbol{\theta}}\mathcal{R}(\mathcal{D}_{\mathrm{u}};\boldsymbol{\theta}_{\mathrm{o}})^\top\nabla_{\boldsymbol{\theta}}\mathcal{L}_{\mathrm{u}}(\mathcal{D}_{\mathrm{u}};\boldsymbol{\theta}_{\mathrm{o}})$, which should be much smaller than 0. It ensures the sufficient removal for targeted data, i.e., $\mathcal{R}(\mathcal{D}_{\mathrm{u}};\boldsymbol{\theta}_{\mathrm{u}})\gg\mathcal{R}(\mathcal{D}_{\mathrm{u}};\boldsymbol{\theta}_{\mathrm{o}})$.
    \item\textbf{Retention.} We define $e_{\mathrm{r}}=\nabla_{\boldsymbol{\theta}}\mathcal{R}(\mathcal{D}\backslash\mathcal{D}_{\mathrm{u}};\boldsymbol{\theta}_{\mathrm{o}})^\top\nabla_{\boldsymbol{\theta}}\mathcal{L}_{\mathrm{u}}(\mathcal{D}_{\mathrm{u}};\boldsymbol{\theta}_{\mathrm{o}})$, which should be greater than or equal to 0, ensuring performance on common data will not reduce, i.e., $\mathcal{R}(\mathcal{D}_{\mathrm{t}}\backslash\mathcal{D}_{\mathrm{u}};\boldsymbol{\theta}_{\mathrm{u}})\le\mathcal{R}(\mathcal{D}_{\mathrm{t}}\backslash\mathcal{D}_{\mathrm{u}};\boldsymbol{\theta}_{\mathrm{o}})$.
\end{itemize}

Although $e_{\mathrm{u}}$ and $e_{\mathrm{r}}$ can anticipate performance changes following a sequence of gradient updates based on $\mathcal{L}_{\mathrm{u}}$, their validity  relies  on our assumption that the difference between $\boldsymbol{\theta}_{\mathrm{o}}$ and $\boldsymbol{\theta}_{\mathrm{u}}$ remains small. Otherwise, the first-order Taylor approximation may introduce large errors. Therefore, we generalize $e_{\mathrm{u}}$ and $e_{\mathrm{r}}$ to make its expression depend on particular updating steps, thereby leading to our definition of the G-effect in Section~\ref{sec: g-effect}.

\textbf{Connection with Previous Works.}
The G-effect resembles the formulation of influence functions~\citep{koh2017understanding} when assessing the inherent influence of training data on test data via their respective gradients. However, there are also substantial differences between them. First, the G-effect primarily explores the roles of objectives in shaping performance, whereas influence functions focus on the impact of individual data points or features on performance.
Moreover, the G-effect is derived from the first-order approximation of the SGD dynamics, while influence functions are computed by the linearization of optimal solutions and are based on the average marginal contributions, thus serving completely different purposes. 

\textbf{Further Discussions.}  In Figure~\ref{fig: motivation}, the gradient behaviors are divided into four distinct regions:
\begin{itemize}
    \item 
    \textbf{Region 1} (Blue Region not intersecting with Red): Objectives with gradients in this region excel at retention but are not effective at unlearning.
    \item \textbf{Region 2} (Intersection of Red and Blue Regions): Objectives here are effective at unlearning but struggle with retention.
    \item \textbf{Region 3} (Red Region not intersecting with Blue): Objectives in this region demonstrate proficiency in both unlearning and retention.
    \item \textbf{Region 4} (White Region): Objectives here are ineffective at both unlearning and retention.
\end{itemize}
Overall, Regions 1 and 3 exhibit a trade-off between unlearning and retention. Region 2 contains ideal objectives for unlearning, whereas Region 4 is unsuitable for unlearning objectives. 

Our experiments further substantiate our claims to be correct. For example, when comparing the unlearning G-effects of NPO and GA in Figures~\ref{fig: ga_and_wga_effect}-\ref{fig: npo_ge}, GA exhibits a greater magnitude compared to NPO, indicating a stronger capability of GA in removing targeted data, further evidenced by the results in Table~\ref{tab: ps main}. Similarly, when comparing the retaining G-effects between GA and WGA in Figure~\ref{fig: ga_and_wga_effect}, the effect magnitude for WGA is notably smaller than that of GA, demonstrating the superior ability of NPO to maintain original performance, also detailed in Table~\ref{tab: ps main}. 


\clearpage


\section{Experimental Setups}
\label{app: experiment setups}
We provide more detailed information about our experimental setups.





\subsection{TOFU Benchmarks}
\label{app: tofu benchmarks}
Our evaluations are based on TOFU fictitious unlearning~\citep{maini2024tofu}, focusing on LLMs fine-tuned with a series of fictitious authors profiles. These profiles were created by prompting GPT-4~\citep{achiam2023gpt}, which has been filtered to avoid the occurrence of any real author profile, thus mitigating the inadvertent impacts of other unrelated variates. For each fictitious profile, TOFU crafted 20 question-answer pairs that can be used for fine-tuning, along with their paraphrased versions for evaluations. 

The pre-trained LLMs are further fine-tuned on such question-answer pairs, where we consider two popular LLMs, i.e., Phi-1.5~\citep{li2023textbooks} and Llama-2-7B~\citep{touvron2023llama} of their question-answering versions. For the unlearning setups, the original TOFU data are separated into targeted and non-targeted parts, of which the adopted proportions are 1:99 (1\% unlearning), 5:95 (5\% unlearning), and 10:90 (10\% unlearning). Moreover,  we separate 400 non-targeted data that are not involved during the unlearning procedure for evaluations, reflecting real-world situations where it is not feasible to go through all non-targeted data during the unlearning process.

\subsection{UWC Hyper-parameter Tuning}

We need to ensure common model integrity when conducting unlearning, but these two goals are often conflicting, failing to align with their Pareto frontiers~\citep{maini2024tofu}. It leads to the dilemma when comparing across unlearned models: Some models may excel at unlearning while others better maintain the overall integrity, making it hard to judge which one is overall better. 

The unlearning with control (UWC)~\citep{wang2024unlearning} framework offers an interesting solution. It allows for the adjustment of model parameters post-unlearning by mixing them with that before unlearning. By proper control of this mixture, different unlearned models can achieve comparable levels of common performance with minimal compromise on their extent of unlearning. Thereafter, we can compare between models by focusing on assessing their unlearning performance. During hyper-parameter tuning, we adopt the KL regularization to stabilize the unlearning procedure, ensuring the results to be general. In UWC, we permit a maximum performance reduction of 10\% for Phi-1.5 and 5\% for Llama-2-7B, following the default configuration adopted in~\citep{wang2024unlearning}. 

\subsection{Evaluation Metrics}


We consider the extraction strength (ES) as suggested by~\citep{wang2024unlearning}, which quantifies the amount of additional information required to fully restore the original outputs after unlearning. ES is calculated differently depending on data types, for either the original data (ES-exact) or their rephrased version (ES-perturb). For the purpose of removal, ES should be evaluated for data targeted to be unlearned, where lower values signify a stronger unlearning capability. Conversely, for the goal of retention, ES should be assessed for other common data, wherein higher values indicate the model integrity is more preserved. 
We further report on the evaluation metrics proposed by~\citep{maini2024tofu}, specifically MU and FQ. The MU metric is a composite measure designed to assess model integrity, encapsulating confidence in generating authentic outputs, the similarity between original and current outputs, and the probability ratio between correct and incorrect outputs. Generally, a higher MU is preferable. Moreover, FQ quantifies the effectiveness of unlearning by conducting a statistical test to compare the distribution of model outputs before and after unlearning, where, typically, a larger FQ value signifies more effective unlearning. Note that the log scale is used for FQ to make the results more readable. 

\clearpage

\section{More Discussions for Existing Unlearning Objectives} \label{app: more for existing objectives}
We present more results for the G-effect of GA, NPO, and RMU. 
\subsection{GA}
\label{app: ga}

\begin{figure}
    \centering
    \subfigure[G-effect]{\includegraphics[width=0.32\textwidth]{ga_effect_full_v2.pdf}} 
    \subfigure[Risk]{\includegraphics[width=0.32\textwidth]{ga_risk_v2.pdf}}
    \subfigure[Inverse Confidence]{\includegraphics[width=0.32\textwidth]{ga_weight_v2.pdf}}
    \caption{\textbf{The Unlearning Dynamics for GA.} We illustrate the G-effect throughout the GA procedure in (a), the unlearning risk in (b), and the inverse confidence (inv. conf.) in (c).  
    }
    \label{fig: ga_effect_appendix}
\end{figure}

\begin{figure}
    \centering
    \subfigure[range $-3.5\times10^5$ to 0]{\includegraphics[width=0.32\textwidth]{ga_effect_full_v2.pdf}} 
    \subfigure[range $-3\times10^4$ to 0]{\includegraphics[width=0.32\textwidth]{ga_effect_v2.pdf}}
    \subfigure[range -600 to 0]{\includegraphics[width=0.32\textwidth]{ga_effect_part_v2.pdf}}
    \caption{\textbf{The G-effect for GA.} Different ranges are considered for varying levels of clarity. }
    \label{fig: ga_effect_different_scale_appendix}
\end{figure}


\begin{figure}[t]
    \centering
    \subfigure[{G-effect ($\beta=0.1$)}]{\includegraphics[width=0.32\textwidth]{npo_effect_5_p1_v3.png}} 
    \subfigure[{Risk ($\beta=0.1$)}]{\includegraphics[width=0.32\textwidth]{npo_risk_5_p1_v3.pdf}}
    \subfigure[Weights ($\beta=0.1$)]{\includegraphics[width=0.32\textwidth]{npo_weight_5_p1_v2.pdf}}
    \subfigure[G-effect ($\beta=1$)]{\includegraphics[width=0.32\textwidth]{npo_effect_5_1_v2.pdf}} 
    \subfigure[Risk ($\beta=1$)]{\includegraphics[width=0.32\textwidth]{npo_risk_5_1_v2.pdf}}
    \subfigure[Weights ($\beta=1$)]{\includegraphics[width=0.32\textwidth]{npo_weight_5_1_v2.pdf}}
    \subfigure[G-effect  ($\beta=2$)]{\includegraphics[width=0.32\textwidth]{npo_effect_5_2_v2.pdf}} 
    \subfigure[{Risk ($\beta=2$)}]{\includegraphics[width=0.32\textwidth]{npo_risk_5_2_v3.pdf}}
    \subfigure[Weights ($\beta=2$)]{\includegraphics[width=0.32\textwidth]{npo_weight_5_2_v2.pdf}}
    \caption{\textbf{The Unlearning Dynamics for NPO}. We illustrate the G-effect, the unlearning risk, and the NPO weighting mechanism following~\eqref{eq: npo gradient}. The legends for the G-effect are summarized in Figure~\ref{fig: legend}.  }
    \label{fig: npo_effect_appendix}
\end{figure}



We report the G-effect in Figure~\ref{fig: ga_effect_appendix}(a) along with the curves of the unlearning risk in Figure~\ref{fig: ga_effect_appendix}(b) and the inverse confidence in Figure~\ref{fig: ga_effect_appendix}(c). First, we observe that the dynamics of the G-effect align precisely with those of the risk. Specifically, the sudden decrease in the G-effect from about the 20-th to 40-th steps mirrors the drop in the risk values. Moreover, there is a rapid increase in the inverse confidence, which exceeds more than $10^{17}$ around the 30-th steps, primarily contributing to excessive unlearning as discussed in Section~\ref{sec: ga}.

This steep rise in inverse confidence can be easily interpret: As the GA unlearning risk decreases, the values of $p(s_{\mathrm{u}};\boldsymbol\theta)$ decrease accordingly, further leading to the increase of its inverse, i.e., the inverse confidence $p(s_{\mathrm{u}};\boldsymbol\theta)^{-1}$. From a point-wise weighting perspective, the behaviors of the inverse confidence is problematic, suggesting that the unlearning dynamics wrongly focus on points that have already been largely unlearned. Obviously, it will lead to extreme over-fitting and catastrophic forgetting, as the associated gradient updates will completely overwhelm the parameters. 


We further provide the G-effect throughout GA at 3 different zoom levels for more detailed observations. In Figure~\ref{fig: ga_effect_different_scale_appendix}(a), we demonstrate that the deterioration to model integrity will outweigh the improvement in unlearning. In Figure~\ref{fig: ga_effect_different_scale_appendix}(b), we highlight that the G-effect for shallow layers is notably larger than those in middle and deep layers. Moreover, in Figure~\ref{fig: ga_effect_different_scale_appendix}(c), we reveal that in the early unlearning phases, e.g., before the 20-th step, the improvements on unlearning can be greater than the damages in retaining model performance. 

\subsection{NPO}
\label{app: npo}

We detail the G-effect along with the risk values and the weighting mechanisms throughout NPO in Figure~\ref{fig: npo_effect_appendix}, across different setups of $\beta$. As observed, the magnitudes of G-effect overall increase as the values of $\beta$ decrease. Simultaneously, the difference between retaining and unlearning G-effect also decreases, signifying a potential trade-off between removal and retention. In general, NPO can moderate the extent of unlearning and make the differences between unlearning and retention G-effect more distinct. Such an observation is particularly pronounced when $\beta$ is set relatively large.  Conversely, when $\beta$ is small, NPO gradually degenerates to the formulation of GA, as illustrated by \eqref{eq: npo gradient} with $\beta=0$. Thus, its behaviors increasingly resemble those of GA as $\beta$ decreases, cf., Figure~\ref{fig: ga_effect_appendix}. A close relationship between the risk values and the weighting mechanism is also noted, which may further signify that the inherent weighting mechanism $w_{s_{\mathrm{u}}}^{\mathrm{npo}}$ primarily contributes to the faster convergence rate of NPO compared to GA.



\subsection{RMU}
\label{app: rmu}
We present the G-effect for RMU across different embedding layers (11-th, 22-th, and 33-th layers) and the scaling hyper-parameter ($c=0$, $1$, and $5$). The results of G-effect are summarized in Figure~\ref{fig: rmu_effect_32l}. We observe that perturbing either middle (22-th) or shallow (11-th) layers is much preferred than that for deep (33-th) layers, where the perturbation of deep layers makes the overall unlearning procedure notably unstable. Additionally, the G-effect demonstrates instability across various scaling parameters, especially for shallow and deep layers. Therefore, we suggest defaulting to perturb the middle-layer representations when using RMU. 
However, we also note that the dynamics and values of the unlearning and retaining G-effect are quite similar during RMU, mirroring the scenarios observed with the original GA. This scenario can also be viewed as the consequences of excessive unlearning, probably stemming from the mapping of original features to completely noise. Such a formulation of perturbations can lead to prohibitively large updates of parameters, especially when the differences between the original and perturbed features are notably large. 


\begin{figure}[t]
    \centering
    \subfigure[ 33-th layer ($c=0$)]{\includegraphics[width=0.32\textwidth]{rmu31_0_effect_v2.pdf}} 
    \subfigure[ 33-th layer ($c=1$)]{\includegraphics[width=0.32\textwidth]{rmu31_1_effect_v2.pdf}}
    \subfigure[ 33-th layer ($c=5$)]{\includegraphics[width=0.32\textwidth]{rmu31_5_effect_v2.pdf}}
    \subfigure[{22-th layer ($c=0$)}]{\includegraphics[width=0.32\textwidth]{rmu21_0_effect_v3.png}} 
    \subfigure[ 22-th layer ($c=1$)]{\includegraphics[width=0.32\textwidth]{rmu21_1_effect_v3.pdf}}
    \subfigure[ 22-th layer ($c=5$)]{\includegraphics[width=0.32\textwidth]{rmu21_5_effect_v3.pdf}}
    \subfigure[ 11-th layer ($c=0$)]{\includegraphics[width=0.32\textwidth]{rmu10_0_effect_v2.pdf}} 
    \subfigure[ 11-th layer ($c=1$)]{\includegraphics[width=0.32\textwidth]{rmu10_1_effect_v2.pdf}}
    \subfigure[ 11-th layer ($c=5$)]{\includegraphics[width=0.32\textwidth]{rmu10_5_effect_v2.pdf}}
    \caption{\textbf{The G-effect for RMU.} The embedding features for various layers, including 33-th, 22-th, and 11-th layers, are considered. The legends for the G-effect are summarized in Figure~\ref{fig: legend}. }
    \label{fig: rmu_effect_32l}
\end{figure}



\clearpage
\section{More Discussions for New Unlearning Objectives}\label{app:ff}
In this section, we delve deeper into our newly proposed unlearning objectives, achieved during our analysis of existing literature. Specifically, inspired by the GA, we introduce weighted GA (WGA) to alleviate its excessive unlearning issues. Building on NPO, we propose token-wise NPO (TNPO) and its further refined version, named weighted TNPO (WTNPO), which better can take advantages of the weighting mechanisms derived from NPO. 

\subsection{WGA}
\label{app: wga}
WGA improves upon GA to mitigate its excessive unlearning issue, controlling the extent of the inverse confidence term during unlearning. Specifically, the formulation for the WGA objective is
\begin{equation}
    \mathbb{E}_{s_{\mathrm{u}}\sim\mathcal{D}_{\mathrm{u}}}\sum_{i=2}^{\vert s\vert}w^{\mathrm{wga}}_{s_{\mathrm{u}},i}\log p(s_{\mathrm{u}}^i|s_{\mathrm{u}}^{<i};\boldsymbol{\theta})\label{eq: wga_app}
\end{equation}
with $w^{\mathrm{wga}}_{s_{\mathrm{u}},i}={p(s_{\mathrm{u}}^i|s_{\mathrm{u}}^{<i};\boldsymbol{\theta})^\alpha}$ the confidence weighting for the $i$-th token and $\alpha$ the hyper-parameter. 
When $\alpha=0$, WGA degenerates to the original GA. Increasing $\alpha$ helps mitigate the drawbacks associated with inverse confidence, while its excessively large values may cause the unlearning procedure to converge too early. Therefore, carefully selecting $\alpha$ allows for a trade-off between excessive unlearning and potential under-fitting. We present the G-effect across different values of $\alpha$ in Figure~\ref{fig: wga_effect_full}. As we can see, counteracting the impacts of the inverse confidence term can notably improve the efficacy of unlearning, where the improvement of unlearning will outweigh the deterioration on integrity, even with only a small strength of the confidence weighting (i.e., $\alpha=0.1$). We also prefer relatively smaller values of $\alpha$, as its power of unlearning remains stronger, signifying by its large negative values of the unlearning G-effect. 


\begin{figure}[t]
    \centering
    \subfigure[{$\alpha=0.1$}]{\includegraphics[width=0.32\textwidth]{wga_p1_effect_v3.png}} 
    \subfigure[$\alpha=0.5$]{\includegraphics[width=0.32\textwidth]{wga_p5_effect_v3.pdf}} 
    \subfigure[{$\alpha=1.5$}]{\includegraphics[width=0.32\textwidth]{wga_1p5_effect_v3.png}}
    \caption{\textbf{The G-effect for WGA.} The legends for the G-effect are summarized in Figure~\ref{fig: legend}.}
    \label{fig: wga_effect_full}
\end{figure}


\subsection{TNPO and WTNPO}
\label{app: tnpo}

TNPO represents a modest modification over the original NPO, which is originally employed to explore the true efficacy of the NPO weighting mechanism. Recalling that, in Section~\ref{sec: npo}, we outline the inherent weighting mechanism of NPO, which possesses some capability to distinguish beneficial data points from potentially harmful ones. Despite these advantages, we also find failures of this weighting mechanism, cf.,  Section~\ref{sec: npo} and Appendix~\ref{app: weighting}. 

\begin{figure}[t]
    \centering
    \subfigure[$\beta=0.1$]{\includegraphics[width=0.32\textwidth]{ins_npo_p1_effect_v3.pdf}} 
    \subfigure[$\beta=1$]{\includegraphics[width=0.32\textwidth]{ins_npo_1_effect_v3.pdf}}
    \subfigure[{$\beta=2$}]{\includegraphics[width=0.32\textwidth]{ins_npo_2_effect_v3.pdf}}
    \caption{\textbf{The G-effect for TNPO.} The legends for the G-effect are summarized in Figure~\ref{fig: legend}.}
    \label{fig: tnpo_effect_all}
\end{figure}


However, we hypothesize that these shortcomings do not necessarily stem from its inherent deficiencies, but rather from its limited flexibility in controlling the unlearning procedure.
A direct approach to enhance the flexibility of the weighting mechanism is to apply it on a token-wise basis. This modification involves prioritizing certain tokens over entire data points, which is the primary distinction from the original NPO. To further clarify our discussion, we use the explicit form of the weighting mechanism, leading to the formulation of TNPO as follows:
\begin{equation}
    \mathbb{E}_{s_{\mathrm{u}}\sim\mathcal{D}_{\mathrm{u}}}  \sum_{i=2}^{\vert s_{\mathrm{u}}\vert}w^{\mathrm{tnpo}}_{s_{\mathrm{u}}, i}\log p(s_{\mathrm{u}}^i|s_{\mathrm{u}}^{<i};\boldsymbol{\theta}),\label{eq: tnpo_app}
\end{equation}
with $w^{\mathrm{tnpo}}_{s_{\mathrm{u}}, i}= \frac{2p(s_{\mathrm{u}}^i|s_{\mathrm{u}}^{<i};\boldsymbol{\theta})^{\beta}}{p(s_{\mathrm{u}}^i|s_{\mathrm{u}}^{<i};\boldsymbol{\theta})^\beta+p(s_{\mathrm{u}}^i|s_{\mathrm{u}}^{<i};\boldsymbol{\theta}_{\textrm{o}})^\beta}$. The G-effect values across several candidate values of $\beta$ are summarized in Figure~\ref{fig: tnpo_effect_all}. When the inverse temperature is relatively small, e.g., $\beta=1$, the improvement upon unlearning causes negligible deterioration on model integrity, making TNPO a very preferred unlearning objective for LLM unlearning.

\begin{wrapfigure}{r}{0.45\textwidth}
  \begin{center}
    \includegraphics[width=0.35\textwidth]{w_ins_npo_1_effect_v3.png}
  \end{center}
  \caption{{\textbf{The G-effect for WTNPO.}} The legends for the G-effect are in Figure~\ref{fig: legend}. }\label{fig: ge_wtnpo}
\end{wrapfigure}


For the case where \(\beta=0.1\), we observe that between the 30-th and 40-th steps, TNPO achieves better unlearning improvements compared to when \(\beta=1\). However, from about the 55-th to 60-th steps, TNPO further reduces the unlearning G-effect, but this comes with the downside that the retaining G-effect is also notably dropped. To address this issue, we recall that $w^{\mathrm{tnpo}}_{s_{\mathrm{u}}, i}$ will approach 1 when decreasing $\beta$ to 0, indicating that the excessive unlearning may still occur. To this end, we can further employ the weighting mechanism used by WGA, leading to the unlearning objective of weighted TNPO (WTNPO) in the following formulation:
\begin{equation}
    \mathbb{E}_{s_{\mathrm{u}}\sim\mathcal{D}_{\mathrm{u}}}  \sum_{i=2}^{\vert s_{\mathrm{u}}\vert}w^{\mathrm{wtnpo}}_{s_{\mathrm{u}}, i}\log p(s_{\mathrm{u}}^i|s_{\mathrm{u}}^{<i};\boldsymbol{\theta}),\label{eq: wtnpo}
\end{equation}
with $w^{\mathrm{wtnpo}}_{s_{\mathrm{u}}, i}= \frac{2p(s_{\mathrm{u}}^i|s_{\mathrm{u}}^{<i};\boldsymbol{\theta})^{\beta+\alpha}}{p(s_{\mathrm{u}}^i|s_{\mathrm{u}}^{<i};\boldsymbol{\theta})^\beta+p(s_{\mathrm{u}}^i|s_{\mathrm{u}}^{<i};\boldsymbol{\theta}_{\textrm{o}})^\beta}$. We present an example for the G-effect of WTNPO in Figure~\ref{fig: ge_wtnpo}, where we fix $\beta=0.1$ and consider $\alpha=0.5$. Employing the confidence weighting can further stabilize the unlearning procedure of TNPO, yet has the costs that the strength of unlearning is weaken. Therefore, there should be trade-off across different values of $\alpha$ when using WTNPO.



\section{Regularization}
\label{app: reg}
In this section, we provide an overview of the regularization terms discussed in Section~\ref{sec: reg}, including GD, KL, and RR. Both GD and KL originate from initial studies of GA to enhance the stability of their unlearning processes, and have since been further investigated in subsequent studies such as NPO.
Specifically, GD improves upon GA by decreasing the negative log-likelihood for non-targeted data, as expressed by the equation of 
\begin{equation}
\mathbb{E}_{(x,y)\sim\mathcal{D}_{\mathrm{t}}\backslash\mathcal{D}_{\mathrm{u}}} \ell\big(y|x;\boldsymbol{\theta}\big).
\end{equation}
KL aims to maintain the model responses for non-targeted data to that before unlearning. It is achieved by the token-wise KL divergence, as shown below:
\begin{equation}
    \mathbb{E}_{(x,y) \sim \mathcal{D}_{\mathrm{t}} \backslash \mathcal{D}_{\mathrm{u}}}  \sum_{k} \texttt{KL}\big(p(y^{<k} \mid x; \boldsymbol{\theta}) \Vert p(y^{<k} \mid x; \boldsymbol{\theta}_{\textrm{o}})\big),
\end{equation}
where $\texttt{KL}$ denotes the operator of the KL divergence. Moreover, RR, which originates from the studies of RMU, is designed to maintain the embedding features during unlearning. The formulation for RR is provided in the following equation:
\begin{equation}
    \mathbb{E}_{(x,y)\sim\mathcal{D}_{\mathrm{t}}\backslash\mathcal{D}_{\mathrm{u}}} \frac{1}{\vert y\vert}\sum_{i=1}^{\vert y\vert}\vert\vert\boldsymbol{\phi}([x,y^{<i}];\boldsymbol{\theta})-\boldsymbol{\phi}([x,y^{<i}];\boldsymbol{\theta}_{\textrm{o}})\vert\vert_2^2,
\end{equation}
To make our experiments easier, we assume that these regularization terms will be integrated directly into the unlearning objectives, without introducing additional trade-off hyper-parameters. 


\clearpage


\section{More Discussions for Weighting Mechanisms}

\label{app: weighting}


In our main discussion, we highlight the crucial role of loss weighting to enhance unlearning meanwhile preserving integrity, pointing out a promising direction that warrants in-depth studies. Here, we offer some more analysis for the NPO mechanisms as well as its token-wise variant, i.e., TNPO, with the aim of motivating future studies in this field.

\subsection{NPO Weighting Mechanisms}
In Section~\ref{sec: npo}, we discuss how the inherent weighting mechanism of NPO extends beyond merely early stopping, highlighting its capability to prioritize certain points with small retaining G-effect. Here, we present further results exploring the relationships between \( w_{s_{\mathrm{u}}}^{\mathrm{npo}} \) and the PG-effect with respect to GA, following \eqref{eq: w vs gae}. These results are analyzed across various inverse temperature settings in Figure~\ref{fig: npo_inst_ge} and NPO unlearning checkpoints in Figure~\ref{fig: inst_npo_effect_full}. 


For the distributions of PG-effect across varying $\beta$ in Figure~\ref{fig: npo_inst_ge}, we observe that larger $\beta$ enhance the distinction between distributions.  It can also be attributed to the behavior of $w_{s_{\mathrm{u}}}^{\mathrm{npo}}$ as $\beta$ approaches 0, where it converges to 1, causing the NPO to resemble the conventional GA. Moreover, the NPO weighting mechanisms for each setup are prone to make some mistakes. For example, at $\beta=1$, $w_{s_{\mathrm{u}}}^{\mathrm{npo}}$ tends to assign values in the range of 0.4 to 0.6 to data points exhibiting large negative retaining G-effect. Similarly, at $\beta=2$, $w_{s_{\mathrm{u}}}^{\mathrm{npo}}$ is likely to assign values in the range of 0.6 to 0.8 for such data points. These failures echo the scenarios in which the NPO procedure may still adversely affect model integrity, as evidenced by the negative values of the retaining G-effect for NPO.

We further report the distributions of PG-effect across different unlearning steps in Figure~\ref{fig: inst_npo_effect_full}. We do not report results before unlearning because $w_{s_{\mathrm{u}}}^{\mathrm{npo}}$ keeps constant at 1. Also, we do not present results beyond the 15-th step, as the NPO generally approaches to converge by that point, especially for $\beta=1$ or $2$. Across the unlearning steps, we observe that  $w_{s_{\mathrm{u}}}^{\mathrm{npo}}$  tends to make more errors initially than in later stages, with notable changes in the distribution layouts across steps, which is unstable.  It suggests the potential for further improvement of NPO through loss weighting. 

\begin{figure}[t]
    \centering
    \subfigure[$\beta=0.1$]{\includegraphics[width=0.32\textwidth]{npo_inst_p1_full.pdf}} 
    \subfigure[$\beta=1$]{\includegraphics[width=0.32\textwidth]{npo_inst_1_full.pdf}}
    \subfigure[$\beta=2$]{\includegraphics[width=0.32\textwidth]{npo_inst_2_full.pdf}}
    \caption{\textbf{Relationships between $w_{s_{\mathrm{u}}}^{\mathrm{npo}}$ and the PG-effect.} Distributions of PG-effect for different value ranges of $w_{s_{\mathrm{u}}}^{\mathrm{npo}}$ are depicted, jointly considering NPO unlearning checkpoints at $5$, $10$, and $15$-th checkpoints. 
    The  values of the PG-effect are categorized into five groups, based on the associated values of $w_{s_{\mathrm{u}}}^{\mathrm{npo}}$  within the ranges of (0.0, 0.2), (0.2, 0.4), (0.4, 0.6), (0.6, 0.8), and (0.8, 1.0). 
    The distributions of the G-effect for each weight group are depicted, using gradually darker shades of color for the distribution contour corresponding to groups with overall higher weight values. }
    \label{fig: npo_inst_ge}
\end{figure}



\begin{figure}[t]
    \centering
    \subfigure[5-th Checkpoint ($\beta=0.1$)]{\includegraphics[width=0.32\textwidth]{npo_inst_p1_ckpt5.pdf}} 
    \subfigure[10-th Checkpoint ($\beta=0.1$)]{\includegraphics[width=0.32\textwidth]{npo_inst_p1_ckpt10.pdf}}
    \subfigure[15-th Checkpoint ($\beta=0.1$)]{\includegraphics[width=0.32\textwidth]{npo_inst_p1_ckpt15.pdf}}

    \subfigure[5-th Checkpoint ($\beta=1$)]{\includegraphics[width=0.32\textwidth]{npo_inst_1_ckpt5.pdf}} 
    \subfigure[10-th Checkpoint ($\beta=1$)]{\includegraphics[width=0.32\textwidth]{npo_inst_1_ckpt10.pdf}}
    \subfigure[15-th Checkpoint ($\beta=1$)]{\includegraphics[width=0.32\textwidth]{npo_inst_1_ckpt15.pdf}}

    \subfigure[5-th Checkpoint ($\beta=2$)]{\includegraphics[width=0.32\textwidth]{npo_inst_2_ckpt5.pdf}} 
    \subfigure[10-th Checkpoint ($\beta=2$)]{\includegraphics[width=0.32\textwidth]{npo_inst_2_ckpt10.pdf}}
    \subfigure[15-th Checkpoint ($\beta=2$)]{\includegraphics[width=0.32\textwidth]{npo_inst_2_ckpt15.pdf}}
    
    \caption{\textbf{Relationships between $w_{s_{\mathrm{u}}}^{\mathrm{npo}}$ and the PG-effect.} We depict the distributions of PG-effect for the checkpoints of 5-th, 10-th, and 15-th steps separately.}
    \label{fig: inst_npo_effect_full}
\end{figure}


\subsection{TNPO and WTNPO Weighting Mechanisms}

Our above analysis have suggested that the NPO weighting mechanism can effectively prioritize certain tokens to benefit unlearning. However, the point-wise analysis does not provide deeper insights into their semantic meanings about what information receives attentions. Hence, we turn our focus to its token-wise variants, i.e., TNPO and WTNPO discussed in Appendix~\ref{app: tnpo}. 
We use color depth to denote the weight of each token, with darker shades indicating higher values for either $w_{s_{\mathrm{u}},i}^{\mathrm{tnpo}}$ or $w_{s_{\mathrm{u}},i}^{\mathrm{wtnpo}}$. We present the results across different unlearning epochs for a random selection of data involved in the unlearning process, which are demonstrated in the following. 



Unfortunately, the results might be difficult to interpret, where $w_{s_{\mathrm{u}},i}^{\mathrm{tnpo}}$ and $w_{s_{\mathrm{u}},i}^{\mathrm{wtnpo}}$ do not always tend to assign higher weights to those tokens that contain informative knowledge. For example,  for the first question, the string of "the illustrious Irwin Literary Prize" contains the key message, while some of the related tokens, such as "ill" and "Ir," are assigned with small weights by TNPO. Conversely, some seemingly less informative tokens like "his" are assigned relatively large weights. This counter-intuitive pattern is more obvious for WTNPO and is general across different examples. It remains unclear whether this issue represents an inherent flaw in the current NPO-based weighting mechanism or if it simply reflects the differences between models and human thinking.

\clearpage
\vspace{20pt}
\input{llama_examples/example1} \vspace{20pt}
\input{llama_examples/example2} \vspace{20pt}
\input{llama_examples/example3} \vspace{20pt}
\input{llama_examples/example4} \vspace{20pt}
\input{llama_examples/example5} 

\clearpage



\section{More Results}
\label{app: tuning}
\label{app: more results}

We benchmark the aforementioned works using existing evaluation metrics, further justifying our explorations and conclusions. Specifically, we employ the UWC evaluation framework and ES metrics as suggested by~\citep{wang2024unlearning}. This framework can quantify the extent of knowledge parametrization and ease the challenges associated with hyper-parameter, which often arise from the trade-off between unlearning and retention. % After hyper-parameter tuning, we further report on FQ and MU, aligning with the evaluation metrics suggested originally by~\citep{maini2024tofu}. 
All our experiments are conducted on TOFU fictitious unlearning datasets, please refer to Appendix~\ref{app: experiment setups} for more descriptions about the dataset details and experimental setups.  


\iffalse
\textbf{Results without Regularization.} We present the results incorporating KL regularization in the main body of this paper, further illustrated below in Table~\ref{tab: full w/ kl}. We also discuss the necessity of regularization and our choice of KL in Section~\ref{sec: reg}.
To provide a more comprehensive analysis, we also present results from experiments conducted without regularization in Table~\ref{tab: full w/o kl}. Despite implementing various weighting schemes, methods such as WGA, TNPO, and WTNPO encounter challenges with the numerical stability of gradient computations of logarithms near zero, thus leading to excessive unlearning.
Nonetheless, when evaluating the extent of unlearning, whether through ES on targeted data or FQ, these methods demonstrate significantly better outcomes compared to NPO. This observation suggests that NPO may be prone to insufficient unlearning.

\textbf{Other Choices over Regularization.} In addition to regularization, we further attempt to explore alternative schemes for retention. We are mainly driven by two key observations regarding the G-effect: \textbf{a)} The unlearning G-effect is close to 0 after approximately 2 epochs; \textbf{b)} the shallow layers exert a large impacts on the overall performance. These insights have guided our following experiments, which are summarized in Table~\ref{tab: early stopping}  where early stopping is implemented after 2 epochs of unlearning, and in Table~\ref{tab: shallow detach} where the shallow layers in models are fixed (the first 9 layers for Phi-1.5 and the first 11 layers for Llama-2-7B), both conducted without KL regularization.
Although these methods yield some improvementsâ€”particularly for forget01 and forget05 settingsâ€”compared to the original results without KL regularization, their overall effectiveness does not match that achieved with KL regularization. Consequently, we conclude that regularization remains the most powerful strategy for practical LLM unlearning. 



\clearpage




\begin{table}[t]
\centering
\caption{\textbf{Comparison between Unlearning Objectives} on TOFU fictitious unlearning, \textbf{KL regularization} is used to stabilize the results.  $\downarrow$ / $\uparrow$ indicate smaller / larger values are preferable. }
\label{tab: full w/ kl}
\resizebox{0.93\textwidth}{!}{
\begin{tabular}{ccccccccccccccc}
\toprule[1.5pt]
\multicolumn{2}{c}{LLM} & \multicolumn{6}{c}{Phi-1.5} && \multicolumn{6}{c}{Llama-2-7B} \\ \cline{3-8}\cline{10-15}
\multirow{2}{*}{setup} & \multirow{2}{*}{method} &  \multicolumn{2}{c}{ES-exact} & \multicolumn{2}{c}{ES-perturb} & \multirow{2}{*}{MU $\uparrow$ } & \multirow{2}{*}{FQ $\uparrow$ } && \multicolumn{2}{c}{ES-exact} & \multicolumn{2}{c}{ES-perturb} & \multirow{2}{*}{MU $\uparrow$ } & \multirow{2}{*}{FQ $\uparrow$ }\\
& & retain $\uparrow$ & unlearn $\downarrow$ & retain $\uparrow$ & unlearn $\downarrow$ & & && retain $\uparrow$ & unlearn $\downarrow$ & retain $\uparrow$ & unlearn $\downarrow$ \\ 
\midrule[1.2pt]
\multicolumn{2}{c}{before unlearning} & 0.4433 & 0.5969 & 0.2115 & 0.1605 & 0.5232 & -5.8031 && 0.8277 & 0.8039 & 0.5302 & 0.4001  &0.6345 & -7.5930\\
\cline{3-8}\cline{10-15}
\multirow{7}{*}{1\%} 
& GA    & 0.1103 & 0.0530 & 0.0850 & 0.0828  & 0.3799 & -0.5471 && 0.3134 & 0.1663 & 0.2101 & 0.0498  & 0.5127 & \textbf{-0.0847}\\
& PO    & 0.4029 & 0.8472 &  \textbf{0.2097} & 0.2698  & 0.5132 & -4.2474 &&  \textbf{0.8058} & 0.8451 &  \textbf{0.4732} & 0.4896  & 0.6164 & -5.8031\\
& WGA   & 0.3629 & 0.0344 & 0.1857 & 0.0282  & \textbf{0.5191} & -0.5471 && 0.5918 & 0.0650 & 0.2791 & 0.0488  & 0.6266 & -0.9049  \\
& NPO   & 0.2727 & 0.0916 & 0.1125 & 0.0733  & 0.4845 & -2.9162 && 0.5334 & 0.1046 & 0.3682 & 0.0986  & 0.6230 &  -1.3254 \\
& TNPO  & 0.3567 & 0.0699 & 0.1030 & 0.0468  & 0.5024 & -0.5471 && 0.4581 & \textbf{0.0503} & 0.3059 & \textbf{0.0472}   & \textbf{0.6368} &  -0.9049  \\
& WTNPO &  \textbf{0.4216} & \textbf{0.0285} & 0.1557  & \textbf{0.0274}  & \textbf{0.5142} & \textbf{-0.2667} && 0.5227 & 0.0600 & 0.2875 & 0.0488    & 0.6233 & -0.9049 \\
& RMU   & 0.1654 & 0.2151 & 0.0653 & 0.1504  & 0.4823 & -3.5542 && 0.2496 & 0.0634 & 0.1417 & 0.0629  & 0.5315 & -1.3254\\
\midrule[0.8pt]
\multicolumn{2}{c}{before unlearning} & 0.4433 & 0.5619 & 0.2115 & 0.2374  &0.5232 & -29.6514&& 0.8277 & 0.7735 & 0.5302 & 0.4126  &0.6345 & -32.1330\\
\cline{3-8}\cline{10-15}
\multirow{7}{*}{5\%} 
& GA    & 0.0000 & 0.0000 & 0.0000 & 0.0000  & 0.0000 & -11.4040 && 0.5901 & 0.5239 & 0.4170 & 0.2885  & 0.0000 & -13.4847\\
& PO    &  0.2646 & 0.7986 & 0.1639 & 0.4925  & \textbf{0.5118} & -26.5061 &&  \textbf{0.6918} & 0.8474 &  \textbf{0.4233} & 0.5348  & 0.6340 & -20.7647\\
& WGA   & \textbf{0.2980} & 0.0179 &  \textbf{0.1645} & 0.0199  & 0.5108 & \textbf{-1.3076} && 0.6228 & \textbf{0.0000} & 0.4024 & \textbf{0.0000}    &\textbf{0.6569} & -53.7054  \\
& NPO   & 0.0876 & 0.1267 & 0.0876 & 0.0609  & 0.3841 & -7.7503 && 0.1763 & 0.0764 & 0.1273 & 0.0802   & 0.4275 &  -15.8757  \\
& TNPO  & 0.1695 & \textbf{0.0126} & 0.0890 & \textbf{0.0038}  & 0.4756 & -1.9514 && 0.4611 & 0.0080 & 0.3944 & 0.0053    & 0.6332 &  -39.2770 \\
& WTNPO & 0.2000 & 0.0179 & 0.1514 & 0.0188  & 0.4990 & -2.1867 && 0.5624 & \textbf{0.0000} & 0.3844 & \textbf{0.0000}   &0.6553 & -46.1759   \\
& RMU   & 0.0344 & 0.1089 & 0.0400 & 0.0798  & 0.1603 & -2.4323 && 0.1590 & 0.0000 & 0.1291 & 0.0105  & 0.5941 & -18.7967\\
\midrule[0.8pt] 
\multicolumn{2}{c}{before unlearning} & 0.4433 & 0.4799 & 0.2115 & 0.1843  &0.5232 & -39.0042&& 0.8277 & 0.8307 & 0.5302 & 0.3099  &0.6345 & -44.4594\\
\cline{3-8}\cline{10-15}
\multirow{7}{*}{10\%} 
& GA    & 0.0000 & 0.0000 & 0.0000 & 0.0000  & 0.0000 & -45.2697 && 0.2196 & 0.3075 & 0.1724 & 0.3854  & 0.0000 & -26.6801\\
& PO    & 0.3222 & 0.7321 & 0.1406 & 0.2667  & 0.5078 & -38.2556 &&  \textbf{0.6749} & 0.8386 &  \textbf{0.4129} & 0.3985  & 0.6360 & -34.6266\\
& WGA   &  \textbf{0.3466} & \textbf{0.0000} &  \textbf{0.1651} & \textbf{0.0000}  & \textbf{0.5132} & -7.0070 && 0.6147 & 0.0258 & 0.3901 & 0.0277   &0.6623 & -30.5199 \\
& NPO   & 0.0859 & 0.0955 & 0.0716 & 0.0710  & 0.3878 & -10.5721 && 0.1109 & 0.1388 & 0.1085 & 0.1440   & 0.5059 &  -11.3678  \\
& TNPO  & 0.2127 & 0.0163 & 0.0949 & 0.0115  & 0.5004 & -7.3327 && 0.5385 & 0.0165 & 0.3865 & 0.0096    & 0.6690 &  -42.8621 \\
& WTNPO & 0.6045 & 0.0069 & 0.3993 & 0.0000  & 0.5089 & \textbf{-6.6882} && 0.6045 & \textbf{0.0069} & 0.3993 & \textbf{0.0000}    & 0.6621 & -63.1529  \\
& RMU   & 0.0352 & 0.0270 & 0.0359 & 0.0157  & 0.1584 & -15.4672 && 0.3176 & 0.0038 & 0.2555 & 0.0000  &0.6132 & -39.0042\\
\bottomrule[1.5pt]
\end{tabular}}
\end{table}





\begin{table}[t]
\centering
\caption{\textbf{Comparison between Unlearning Objectives} on TOFU fictitious unlearning, \textbf{no regularization} is used to stabilize the results.  $\downarrow$ / $\uparrow$ indicate smaller / larger values are preferable. }\label{tab: full w/o kl}
\resizebox{0.93\textwidth}{!}{
\begin{tabular}{ccccccccccccccc}
\toprule[1.5pt]
\multicolumn{2}{c}{LLM} & \multicolumn{6}{c}{Phi-1.5} && \multicolumn{6}{c}{Llama-2-7B} \\ \cline{3-8}\cline{10-15}
\multirow{2}{*}{setup} & \multirow{2}{*}{method} &  \multicolumn{2}{c}{ES-exact} & \multicolumn{2}{c}{ES-perturb} & \multirow{2}{*}{MU $\uparrow$ } & \multirow{2}{*}{FQ $\uparrow$ } && \multicolumn{2}{c}{ES-exact} & \multicolumn{2}{c}{ES-perturb} & \multirow{2}{*}{MU $\uparrow$ } & \multirow{2}{*}{FQ $\uparrow$ }\\
& & retain $\uparrow$ & unlearn $\downarrow$ & retain $\uparrow$ & unlearn $\downarrow$ & & && retain $\uparrow$ & unlearn $\downarrow$ & retain $\uparrow$ & unlearn $\downarrow$ \\ 
\midrule[1.2pt]
\multicolumn{2}{c}{before unlearning} & 0.4433 & 0.5969 & 0.2115 & 0.1605 &0.5232 & -5.8031&& 0.8277 & 0.8039 & 0.5302 & 0.4001  & 0.6345 & -7.5930\\
\cline{3-8}\cline{10-15}
\multirow{7}{*}{1\%} 
& GA    & 0.0664 & 0.0530 & 0.0891 & 0.1068 & 0.3792  & -0.5471  && 0.4052 & 0.0570 & 0.3082 & 0.0422 & 0.5253  & -0.5471 \\ % 0.4052 forget 0.0570
& PO    & 0.2510 & 0.2756 & 0.1348 & 0.2000 & 0.2915  & -0.5471  && 0.5315 & 0.2624 & 0.4581 & 0.2440 & 0.1743 & -1.3254 \\
& WGA   & 0.0605 & 0.0069 & 0.0485 & 0.0069 & 0.3893 & -3.5542 && 0.4350 & 0.0807 & 0.2482 & 0.0544  & 0.6095 & -0.9049 \\
& NPO   & 0.2727 & 0.0893 & 0.1125 & 0.0733 & 0.4769  & -2.9162 && 0.5334 & 0.1046 & 0.3871 & 0.1020 & 0.6226 & -1.3254 \\
& TNPO  & 0.1180 & 0.0387 & 0.0831 & 0.0224 & 0.4562 & -0.0100 && 0.5243 & 0.0308 & 0.3601 & 0.0308  & 0.6368 & -0.0847 \\
& WTNPO & 0.0711 & 0.0069 & 0.0591 & 0.0179 & 0.4268 & -2.3323 && 0.4725 & 0.0807 & 0.3286 & 0.0544 & 0.6097& -0.2667  \\
& RMU   & 0.1126 & 0.1623 & 0.0374 & 0.1226 & 0.4561 & -2.3323 && 0.0099 & 0.0000 & 0.0117 & 0.0019 & 0.0138 & -7.5930 \\
\midrule[0.8pt]
\multicolumn{2}{c}{before unlearning} & 0.4433 & 0.5619 & 0.2115 & 0.2374  & 0.5232 & -29.6514 && 0.8277 & 0.7735 & 0.5302 & 0.4126  & 0.6345 & -32.1330\\
\cline{3-8}\cline{10-15}
\multirow{7}{*}{5\%} 
& GA    & 0.0000 & 0.0000 & 0.0000 & 0.0000 & 0.0000  & -10.9105  && 0.0000 & 0.0000 & 0.0000 & 0.0000 & 0.0000  & -9.0417 \\
& PO    & 0.0539 & 0.0548 & 0.0521 & 0.0702 & 0.0487  & -1.9514  && 0.3506 & 0.1937 & 0.2562 & 0.1654 & 0.0000 & -9.4931 \\
& WGA   & 0.0000 & 0.0000 & 0.0000 & 0.0000 & 0.0000 & -44.1416 && 0.0000 & 0.0000 & 0.0000 & 0.0000 & 0.0030 & -33.8466\\
& NPO   & 0.0854 & 0.0387 & 0.0824 & 0.0387 & 0.3545 & -7.7503 && 0.1109 & 0.0764 & 0.1085 & 0.0802 & 0.5134 & -9.4931 \\
& TNPO  & 0.0000 & 0.0000 & 0.0000 & 0.0000 & 0.0066 & -32.9838 && 0.0034 & 0.0048 & 0.0016 & 0.0050  & 0.0127 & -42.1581 \\
& WTNPO & 0.0000 & 0.0000 & 0.0000 & 0.0000 & 0.0039 & -42.1581 && 0.0000 & 0.0000 & 0.0000 & 0.0000 & 0.0037 & -35.6082  \\
& RMU   & 0.0019 & 0.0000 & 0.0000 & 0.0000 & 0.0000 & -22.8330 && 0.0000 & 0.0000 & 0.0000 & 0.0000 & 0.0000 & -37.4182 \\
\midrule[0.8pt] 
\multicolumn{2}{c}{before unlearning} & 0.4433 & 0.4799 & 0.2115 & 0.1843  &0.5232 & -39.0042 && 0.8277 & 0.8307 & 0.5302 & 0.3099  & 0.6345& -44.4594\\
\cline{3-8}\cline{10-15}
\multirow{7}{*}{10\%} 
& GA    & 0.0000 & 0.0000 & 0.0000 & 0.0000 & 0.0000  & -43.6569  && 0.0000 & 0.0000 & 0.0000 & 0.0000 & 0.0000 & -25.4588 \\
& PO    & 0.0577 & 0.0660 & 0.0540 & 0.0690 & 0.0483 & -3.8742 && 0.2916 & 0.2386 & 0.2049 & 0.1908 & 0.0000 & -15.2927 \\
& WGA   & 0.0000 & 0.0000 & 0.0000 & 0.0000 & 0.0000 & -63.1526 && 0.0057 & 0.0000 & 0.0057 & 0.0000 & 0.0000 & -4.9183\\
& NPO   & 0.0550 & 0.0700 & 0.0489 & 0.0289 & 0.3048  & -8.3506 && 0.0800 & 0.1388 & 0.0772 & 0.1388 & 0.4931 & -10.5721 \\
& TNPO  & 0.0000 & 0.0000 & 0.0000 & 0.0000 & 0.0023 & -62.1809 && 0.0000 & 0.0000 & 0.0000 & 0.0000 & 0.0000 & -2.5106 \\
& WTNPO & 0.0000 & 0.0000 & 0.0000 & 0.0000 & 0.0015 & -66.1180 && 0.0000 & 0.0000 & 0.0000 & 0.0000 & 0.0000 & -45.2697 \\
& RMU   & 0.0000 & 0.0000 & 0.0000 & 0.0000 & 0.0000 & -35.3373 && 0.0000 & 0.0000 & 0.0000 & 0.0000 & 0.0000 & -48.5895 \\
\bottomrule[1.5pt]
\end{tabular}}
\end{table}




\begin{table}[t]
\centering
\caption{\textbf{Comparison between Unlearning Objectives} on TOFU  unlearning, \textbf{no regularization} is used and all methods are run for \textbf{2 epochs}.  $\downarrow$ / $\uparrow$ indicate smaller / larger values are preferable. }\label{tab: early stopping}
\resizebox{0.93\textwidth}{!}{
\begin{tabular}{ccccccccccccccc}
\toprule[1.5pt]
\multicolumn{2}{c}{LLM} & \multicolumn{6}{c}{Phi-1.5} && \multicolumn{6}{c}{Llama-2-7B} \\ \cline{3-8}\cline{10-15}
\multirow{2}{*}{setup} & \multirow{2}{*}{method} &  \multicolumn{2}{c}{ES-exact} & \multicolumn{2}{c}{ES-perturb} & \multirow{2}{*}{MU $\uparrow$ } & \multirow{2}{*}{FQ $\uparrow$ } && \multicolumn{2}{c}{ES-exact} & \multicolumn{2}{c}{ES-perturb} & \multirow{2}{*}{MU $\uparrow$ } & \multirow{2}{*}{FQ $\uparrow$ }\\
& & retain $\uparrow$ & unlearn $\downarrow$ & retain $\uparrow$ & unlearn $\downarrow$ & & && retain $\uparrow$ & unlearn $\downarrow$ & retain $\uparrow$ & unlearn $\downarrow$ \\ 
\midrule[1.2pt]
\multicolumn{2}{c}{before unlearning} & 0.4433 & 0.5969 & 0.2115 & 0.1605 &0.5232 & -5.8031&& 0.8277 & 0.8039 & 0.5302 & 0.4001  & 0.6345 & -7.5930\\
\cline{3-8}\cline{10-15}
\multirow{7}{*}{1\%} 
& GA    & 0.4572 & 0.1806 & 0.1711 & 0.1301 & 0.5088 & -6.6681 && 0.7874 & 0.4351 & 0.4777 & 0.3452 & 0.6396 & -7.5930 \\ 
& PO    & 0.3354 & 0.5500 & 0.1963 & 0.2055 & 0.5163 & -3.5542 && 0.7868 & 0.7587 & 0.5160 & 0.4228 &0.6371  & -5.8031 \\ 
& WGA   & 0.4567 & 0.1713 & 0.1992 & 0.0813 & 0.5125 & -3.5542 && 0.7874 & 0.3158 & 0.4465 & 0.2400 & 0.6351 & -6.6681 \\ 
& NPO   & 0.4499 & 0.2282 & 0.1792 & 0.1517 & 0.5110 & -5.8031 && 0.7874 & 0.3111 & 0.4777 & 0.3570 & 0.6400 & -5.8031 \\ 
& TNPO  & 0.4476 & 0.2282 & 0.1737 & 0.1301 & 0.5138 & -5.8031 && 0.7874 & 0.3077 & 0.4777 & 0.2746 & 0.6412 & -7.5930 \\ 
& WTNPO & 0.4733 & 0.1556 & 0.2074 & 0.0988 & 0.5160 & -4.9966 && 0.7874 & 0.3427 & 0.4434 & 0.2626 & 0.6326 & -7.5930 \\ 
& RMU   & 0.4439 & 0.5016 & 0.1901 & 0.1855 & 0.5192 & -5.8031 && 0.3987 & 0.2983 & 0.2035 & 0.2554 & 0.5882 & -4.9966 \\  
\midrule[0.8pt]
\multicolumn{2}{c}{before unlearning} & 0.4433 & 0.5619 & 0.2115 & 0.2374  & 0.5232 & -29.6514 && 0.8277 & 0.7735 & 0.5302 & 0.4126  & 0.6345 & -32.1330\\
\cline{3-8}\cline{10-15}
\multirow{7}{*}{5\%} 
& GA    & 0.0040 & 0.0000 & 0.0057 & 0.0000 & 0.0008 & -6.5526 && 0.1381 & 0.0488 & 0.1097 & 0.0715 & 0.4873 & -2.4323 \\ 
& PO    & 0.0658 & 0.1152 & 0.0685 & 0.1345 & 0.0622 & -5.1004 && 0.3927 & 0.2977 & 0.2916 & 0.2527 & 0.0000 & -13.4847 \\ 
& WGA   & 0.0000 & 0.0000 & 0.0000 & 0.0000 & 0.0409 & -14.5893 && 0.0625 & 0.0473 & 0.0606 & 0.0420 & 0.3859 & -7.7503 \\ 
& NPO   & 0.1148 & 0.1630 & 0.0950 & 0.0829 & 0.4089 & -9.9550 && 0.2993 & 0.2455 & 0.2443 & 0.1915 & 0.5673 & -15.1577 \\ 
& TNPO  & 0.0658 & 0.0472 & 0.0594 & 0.0335 & 0.3628 & -8.6008 && 0.1769 & 0.1116 & 0.1030 & 0.1753 & 0.5162 & -7.3407 \\ 
& WTNPO & 0.0058 & 0.0050 & 0.0079 & 0.0000 & 0.1235 & -4.4360 && 0.0843 & 0.0520 & 0.0843 & 0.0420 & 0.4266 &-6.1741  \\ 
& RMU   & 0.0000 & 0.0111 & 0.0000 & 0.0014 & 0.0082 & -6.9414 && 0.0000 & 0.0000 & 0.0000 & 0.0000 & 0.0003  & -25.0027 \\ 
\midrule[0.8pt] 
\multicolumn{2}{c}{before unlearning} & 0.4433 & 0.4799 & 0.2115 & 0.1843  &0.5232 & -39.0042 && 0.8277 & 0.8307 & 0.5302 & 0.3099  & 0.6345& -44.4594\\
\cline{3-8}\cline{10-15}
\multirow{7}{*}{10\%} 
& GA    & 0.0000 & 0.0000 & 0.0000 & 0.0000 & 0.0000 & -30.5199 && 0.0000 & 0.0000 & 0.0000 & 0.0000 & 0.0000 & -6.6882 \\ 
& PO    & 0.0559 & 0.1142 & 0.0595 & 0.1060 & 0.0490 & -4.3827 && 0.3568 & 0.2548 & 0.2557 & 0.2133 & 0.0260 & -15.7638 \\ 
& WGA   & 0.0000 & 0.0000 & 0.0000 & 0.0000 & 0.0013 & -56.5248 && 0.0000 & 0.0000 & 0.0000 & 0.0000 & 0.0006 & -31.1857 \\ 
& NPO   & 0.0622 & 0.0759 & 0.0509 & 0.0604 & 0.3303 & -9.4304 && 0.1109 & 0.1388 & 0.1085 & 0.1460 & 0.5074 & -12.6133 \\ 
& TNPO  & 0.0000 & 0.0000 & 0.0000 & 0.0000 & 0.0155 & -30.5199 && 0.0172 & 0.0283 & 0.0210 & 0.0201 & 0.0179 & -58.3772 \\ 
& WTNPO & 0.0000 & 0.0000 & 0.0000 & 0.0000 & 0.0035 & -52.9176 && 0.0000 & 0.0000 & 0.0000 & 0.0000 & 0.0027 & -56.5248 \\ 
& RMU   & 0.0000 & 0.0000 & 0.0000 & 0.0000 & 0.0002 & -23.6814 && 0.0000 & 0.0000 & 0.0000 & 0.0000 & 0.0000 & -40.5242 \\ 
\bottomrule[1.5pt]
\end{tabular}}
\end{table}


\begin{table}[t]
\centering
\caption{\textbf{Comparison between Unlearning Objectives} on TOFU  unlearning, \textbf{no regularization} is used and \textbf{shallow layers are not updated}. $\downarrow$ / $\uparrow$ indicate smaller / larger values are preferable. }\label{tab: shallow detach}
\resizebox{0.93\textwidth}{!}{
\begin{tabular}{ccccccccccccccc}
\toprule[1.5pt]
\multicolumn{2}{c}{LLM} & \multicolumn{6}{c}{Phi-1.5} && \multicolumn{6}{c}{Llama-2-7B} \\ \cline{3-8}\cline{10-15}
\multirow{2}{*}{setup} & \multirow{2}{*}{method} &  \multicolumn{2}{c}{ES-exact} & \multicolumn{2}{c}{ES-perturb} & \multirow{2}{*}{MU $\uparrow$ } & \multirow{2}{*}{FQ $\uparrow$ } && \multicolumn{2}{c}{ES-exact} & \multicolumn{2}{c}{ES-perturb} & \multirow{2}{*}{MU $\uparrow$ } & \multirow{2}{*}{FQ $\uparrow$ }\\
& & retain $\uparrow$ & unlearn $\downarrow$ & retain $\uparrow$ & unlearn $\downarrow$ & & && retain $\uparrow$ & unlearn $\downarrow$ & retain $\uparrow$ & unlearn $\downarrow$ \\ 
\midrule[1.2pt]
\multicolumn{2}{c}{before unlearning} & 0.4433 & 0.5969 & 0.2115 & 0.1605 &0.5232 & -5.8031&& 0.8277 & 0.8039 & 0.5302 & 0.4001  & 0.6345 & -7.5930\\
\cline{3-8}\cline{10-15}
\multirow{7}{*}{1\%} 
& GA    & 0.4572 & 0.1806 & 0.1711 & 0.1301 &  &  && 0.7874 & 0.4351 & 0.4777 & 0.3452 &  &  \\ 
& PO    & 0.3354 & 0.5500 & 0.1963 & 0.2055 &  &  && 0.7868 & 0.7587 & 0.5160 & 0.4228 &  &  \\ 
& WGA   & 0.4567 & 0.1713 & 0.1992 & 0.0813 &  &  && 0.7874 & 0.3158 & 0.4465 & 0.2400 &  &  \\ 
& NPO   & 0.4499 & 0.2282 & 0.1792 & 0.1517 &  &  && 0.7874 & 0.3111 & 0.4777 & 0.3570 &  &  \\ 
& TNPO  & 0.4476 & 0.2282 & 0.1737 & 0.1301 &  &  && 0.7874 & 0.3077 & 0.4777 & 0.2746 &  &  \\ 
& WTNPO & 0.4733 & 0.1556 & 0.2074 & 0.0988 &  &  && 0.7874 & 0.3427 & 0.4434 & 0.2626 &  &  \\ 
& RMU   & 0.4439 & 0.5016 & 0.1901 & 0.1855 &  &  && 0.3987 & 0.2983 & 0.2035 & 0.2554 &  &  \\  
\midrule[0.8pt]
\multicolumn{2}{c}{before unlearning} & 0.4433 & 0.5619 & 0.2115 & 0.2374  & 0.5232 & -29.6514 && 0.8277 & 0.7735 & 0.5302 & 0.4126  & 0.6345 & -32.1330\\
\cline{3-8}\cline{10-15}
\multirow{7}{*}{5\%} 
& GA    & 0.0040 & 0.0000 & 0.0057 & 0.0000 &  &  && 0.1381 & 0.0488 & 0.1097 & 0.0715 &  &  \\ 
& PO    & 0.0658 & 0.1152 & 0.0685 & 0.1345 &  &  && 0.3927 & 0.2977 & 0.2916 & 0.2527 &  &  \\ 
& WGA   & 0.0000 & 0.0000 & 0.0000 & 0.0000 &  &  && 0.0625 & 0.0473 & 0.0606 & 0.0420 &  &  \\ 
& NPO   & 0.1148 & 0.1630 & 0.0950 & 0.0829 &  &  && 0.2993 & 0.2455 & 0.2443 & 0.1915 &  &  \\ 
& TNPO  & 0.0658 & 0.0472 & 0.0594 & 0.0335 &  &  && 0.1769 & 0.1116 & 0.1030 & 0.1753 &  &  \\ 
& WTNPO & 0.0058 & 0.0050 & 0.0079 & 0.0000 &  &  && 0.0843 & 0.0520 & 0.0843 & 0.0420 &  &  \\ 
& RMU   & 0.0000 & 0.0111 & 0.0000 & 0.0014 &  &  && 0.0000 & 0.0000 & 0.0000 & 0.0000 &  &  \\ 
\midrule[0.8pt] 
\multicolumn{2}{c}{before unlearning} & 0.4433 & 0.4799 & 0.2115 & 0.1843  &0.5232 & -39.0042 && 0.8277 & 0.8307 & 0.5302 & 0.3099  & 0.6345& -44.4594\\
\cline{3-8}\cline{10-15}
\multirow{7}{*}{10\%} 
& GA    & 0.0000 & 0.0000 & 0.0000 & 0.0000 &  &  && 0.0000 & 0.0000 & 0.0000 & 0.0000 &  &  \\ 
& PO    & 0.0559 & 0.1142 & 0.0595 & 0.1060 &  &  && 0.3568 & 0.2548 & 0.2557 & 0.2133 &  &  \\ 
& WGA   & 0.0000 & 0.0000 & 0.0000 & 0.0000 &  &  && 0.0000 & 0.0000 & 0.0000 & 0.0000 &  &  \\ 
& NPO   & 0.0622 & 0.0759 & 0.0509 & 0.0604 &  &  && 0.1109 & 0.1388 & 0.1085 & 0.1460 &  &  \\ 
& TNPO  & 0.0000 & 0.0000 & 0.0000 & 0.0000 &  &  && 0.0172 & 0.0283 & 0.0210 & 0.0201 &  &  \\ 
& WTNPO & 0.0000 & 0.0000 & 0.0000 & 0.0000 &  &  && 0.0000 & 0.0000 & 0.0000 & 0.0000 &  &  \\ 
& RMU   & 0.0000 & 0.0000 & 0.0000 & 0.0000 &  &  && 0.0000 & 0.0000 & 0.0000 & 0.0000 &  &  \\ 
\bottomrule[1.5pt]
\end{tabular}}
\end{table}
\fi




\begin{table}[ht]
\centering
\caption{\textbf{UWC Tuning for WGA.}  $\downarrow$ / $\uparrow$ indicate smaller / larger values are preferable.  }\label{tab: wga}
\resizebox{0.99\textwidth}{!}{
\begin{tabular}{ccccccccccc}
\toprule[1.5pt]
\multicolumn{2}{c}{WGA} & \multicolumn{4}{c}{Phi-1.5} && \multicolumn{4}{c}{Llama-2-7B} \\ \cline{3-6}\cline{8-11}
\multirow{2}{*}{setup} & \multirow{2}{*}{$\alpha$} &  \multicolumn{2}{c}{ES-exact} & \multicolumn{2}{c}{ES-perturb}  && \multicolumn{2}{c}{ES-exact} & \multicolumn{2}{c}{ES-perturb} \\
& & retain $\uparrow$ & unlearn $\downarrow$ & retain $\uparrow$ & unlearn $\downarrow$ && retain $\uparrow$ & unlearn $\downarrow$ & retain $\uparrow$ & unlearn $\downarrow$ \\ 
\midrule[1.2pt]
\multicolumn{2}{c}{before unlearning} & 0.4433 & 0.5969 & 0.2115 & 0.1605 && 0.8277 & 0.8039 & 0.5302 & 0.4001 \\
\cline{3-6}\cline{8-11}
\multirow{9}{*}{{1\%}}
& 0.05 
& 0.4205 & 0.2587 & 0.1927 & 0.1274 && 0.7549 & 0.2021 & 0.4493 & 0.1250 \\
& 0.10 
& 0.3804 & 0.1899 & 0.2136 & 0.1274 && 0.7317 & 0.2666 & 0.4428 & 0.3139 \\
& 0.50 
& 0.4267 & 0.1524 & 0.2108 & 0.0652 && 0.7593 & 0.0897 & 0.4900 & 0.0767 \\
& 0.70 
& 0.4412 & 0.1695 & 0.2052 & 0.0890 && 0.7251 & 0.1680 & 0.4863 & 0.0767 \\
& 1.00 
& 0.4369 & 0.1712 & 0.2052 & 0.0527 && 0.7392 & 0.1376 & 0.4863 & 0.0767 \\
& 2.00 
& 0.4369 & 0.0877 & 0.2052 & 0.0764 && 0.7637 & 0.0736 & 0.4701 & 0.0767 \\
& 4.00 
& 0.4055 & 0.0765 & 0.1857 & 0.0220 && 0.7021 & 0.0736 & 0.4881 & 0.0844 \\
& 5.00 
& 0.4045 & 0.0805 & 0.2201 & 0.0425 && 0.7040 & 0.0736 & 0.4708 & 0.0793 \\
& 7.00 
& 0.4356 & 0.1685 & 0.2145 & 0.0397 && 0.7040 & 0.0999 & 0.4504 & 0.0969 \\
& 10.00 
& 0.4058 & 0.1264 & 0.2085 & 0.0512 && 0.7040 & 0.1334 & 0.4751 & 0.1293 \\
\midrule[0.8pt]
\multicolumn{2}{c}{before unlearning} & 0.4433 & 0.5619 & 0.2115 & 0.2374 && 0.8277 & 0.7735 & 0.5302 & 0.4126 \\
\cline{3-6}\cline{8-11}
\multirow{9}{*}{5\%}
& 0.05 
& 0.4557 & 0.3555 & 0.1986 & 0.2349 && 0.7749 & 0.5709 & 0.4970 & 0.3596 \\
& 0.10 
& 0.4695 & 0.3618 & 0.1792 & 0.2349 && 0.7555 & 0.5681 & 0.4910 & 0.4371 \\
& 0.50 
& 0.4186 & 0.3538 & 0.1985 & 0.2514 && 0.7534 & 0.4310 & 0.4778 & 0.4013 \\
& 0.70 
& 0.4021 & 0.3592 & 0.2356 & 0.1607 && 0.7534 & 0.4328 & 0.4872 & 0.4013 \\
& 1.00 
& 0.4520 & 0.4142 & 0.2551 & 0.1967 && 0.7463 & 0.3790 & 0.4853 & 0.3295 \\
& 2.00 
& 0.4000 & 0.2345 & 0.1791 & 0.0792 && 0.7534 & 0.3826 & 0.4807 & 0.3489 \\
& 4.00 
& 0.4454 & 0.3659 & 0.1665 & 0.0927 && 0.7496 & 0.1478 & 0.5200 & 0.3516 \\
& 5.00 
& 0.3913 & 0.2798 & 0.2197 & 0.0823 && 0.7533 & 0.0103 & 0.5302 & 0.3516 \\
& 7.00 
& 0.4433 & 0.3663 & 0.1731 & 0.0559 && 0.7524 & 0.0000 & 0.4825 & 0.1430 \\
& 10.00 
& 0.4415 & 0.4021 & 0.2225 & 0.0274 && 0.7880 & 0.0638 & 0.4887 & 0.1602 \\
\midrule[0.8pt] 
\multicolumn{2}{c}{before unlearning} & 0.4433 & 0.4799 & 0.2115 & 0.1843 && 0.8277 & 0.8307 & 0.5302 & 0.3099 \\
\cline{3-6}\cline{8-11}
\multirow{9}{*}{10\%} 
& 0.05 
& 0.4733 & 0.3563 & 0.1841 & 0.1445 && 0.7641 & 0.5997 & 0.4805 & 0.2947 \\
& 0.10 
& 0.4094 & 0.2927 & 0.2032 & 0.1560 && 0.7463 & 0.5997 & 0.4727 & 0.2947 \\
& 0.50 
& 0.4310 & 0.4711 & 0.1665 & 0.1425 && 0.7494 & 0.5230 & 0.4809 & 0.2959 \\
& 0.70 
& 0.3911 & 0.4711 & 0.1993 & 0.0840 && 0.7534 & 0.5363 & 0.4825 & 0.2884 \\
& 1.00 
& 0.4477 & 0.4272 & 0.2345 & 0.0616 && 0.7534 & 0.5363 & 0.4779 & 0.2677 \\
& 2.00 
& 0.4269 & 0.1369 & 0.1794 & 0.0379 && 0.7571 & 0.1646 & 0.5184 & 0.2896 \\
& 4.00 
& 0.4370 & 0.1177 & 0.2161 & 0.0193 && 0.7646 & 0.0160 & 0.5038 & 0.2989 \\
& 5.00 
& 0.4218 & 0.0935 & 0.1881 & 0.0105 && 0.7836 & 0.1289 & 0.4777 & 0.1289 \\
& 7.00 
& 0.4042 & 0.0908 & 0.1727 & 0.0472 && 0.7241 & 0.0331 & 0.4563 & 0.3183 \\
& 10.00 
& 0.3982 & 0.1287 & 0.2020 & 0.0670 && 0.7146 & 0.0321 & 0.4877 & 0.3258 \\
\bottomrule[1.5pt]
\end{tabular}}
\end{table}

\begin{table}[t]
\centering
\caption{\textbf{UWC Tuning for NPO.}  $\downarrow$ / $\uparrow$ indicate smaller / larger values are preferable.  }\label{tab: npo}
\resizebox{0.99\textwidth}{!}{
\begin{tabular}{ccccccccccc}
\toprule[1.5pt]
\multicolumn{2}{c}{NPO} & \multicolumn{4}{c}{Phi-1.5} && \multicolumn{4}{c}{Llama-2-7B} \\ \cline{3-6}\cline{8-11}
\multirow{2}{*}{setup} & \multirow{2}{*}{$\beta$} &  \multicolumn{2}{c}{ES-exact} & \multicolumn{2}{c}{ES-perturb}  && \multicolumn{2}{c}{ES-exact} & \multicolumn{2}{c}{ES-perturb} \\
& & retain $\uparrow$ & unlearn $\downarrow$ & retain $\uparrow$ & unlearn $\downarrow$ && retain $\uparrow$ & unlearn $\downarrow$ & retain $\uparrow$ & unlearn $\downarrow$ \\ 
\midrule[1.2pt]
\multicolumn{2}{c}{before unlearning} & 0.4433 & 0.5969 & 0.2115 & 0.1605 && 0.8277 & 0.8039 & 0.5302 & 0.4001 \\
\cline{3-6}\cline{8-11}
\multirow{9}{*}{{1\%}}
& 0.05 
& 0.4283 & 0.1587 & 0.2136 & 0.0702 && 0.7655 & 0.1262 & 0.5084 & 0.2545 \\
& 0.10 
& 0.4553 & 0.1587 & 0.2121 & 0.0945 && 0.7547 & 0.1857 & 0.4995 & 0.2113 \\
& 0.50 
& 0.4030 & 0.0947 & 0.2136 & 0.1083 && 0.6967 & 0.2513 & 0.4777 & 0.1898 \\
& 0.70 
& 0.3909 & 0.1072 & 0.2136 & 0.1083 && 0.7517 & 0.2607 & 0.4733 & 0.1863 \\
& 1.00 
& 0.4261 & 0.1806 & 0.2136 & 0.1083 && 0.7517 & 0.2607 & 0.4777 & 0.1863 \\
& 2.00 
& 0.3954 & 0.1166 & 0.2136 & 0.1655 && 0.7234 & 0.2876 & 0.4588 & 0.2025 \\
& 4.00 
& 0.4223 & 0.1166 & 0.2136 & 0.1551 && 0.7565 & 0.2941 & 0.4777 & 0.2089 \\
& 5.00 
& 0.4218 & 0.1806 & 0.2136 & 0.1551 && 0.7874 & 0.2941 & 0.4777 & 0.2089 \\
& 7.00 
& 0.4218 & 0.1806 & 0.2001 & 0.1551 && 0.7874 & 0.2941 & 0.4588 & 0.2197 \\
& 10.00 
& 0.4218 & 0.1806 & 0.2136 & 0.1551 && 0.7457 & 0.2893 & 0.4777 & 0.2197 \\
\midrule[0.8pt]
\multicolumn{2}{c}{before unlearning} & 0.4433 & 0.5619 & 0.2115 & 0.2374 && 0.8277 & 0.7735 & 0.5302 & 0.4126 \\
\cline{3-6}\cline{8-11}
\multirow{9}{*}{5\%}
& 0.05 
& 0.4265 & 0.3671 & 0.2052 & 0.2349 && 0.7523 & 0.5005 & 0.4957 & 0.3697 \\
& 0.10 
& 0.4161 & 0.3709 & 0.1942 & 0.2228 && 0.7652 & 0.5473 & 0.4976 & 0.4066 \\
& 0.50 
& 0.4433 & 0.4539 & 0.2098 & 0.2228 && 0.7780 & 0.4966 & 0.4773 & 0.4009 \\
& 0.70 
& 0.3970 & 0.3452 & 0.2058 & 0.2314 && 0.7459 & 0.5005 & 0.4903 & 0.4013 \\
& 1.00 
& 0.4086 & 0.4177 & 0.1982 & 0.2228 && 0.7836 & 0.5195 & 0.4918 & 0.3785 \\
& 2.00 
& 0.4086 & 0.3863 & 0.2043 & 0.2203 && 0.7572 & 0.5809 & 0.4976 & 0.3884 \\
& 4.00 
& 0.4433 & 0.4188 & 0.2043 & 0.2147 && 0.7836 & 0.5809 & 0.4781 & 0.3884 \\
& 5.00 
& 0.4433 & 0.4188 & 0.2150 & 0.2147 && 0.7836 & 0.5946 & 0.5175 & 0.3726 \\
& 7.00 
& 0.4127 & 0.4034 & 0.2109 & 0.1805 && 0.7836 & 0.5303 & 0.4887 & 0.3674 \\
& 10.00 
& 0.4433 & 0.4034 & 0.1848 & 0.2000 && 0.7836 & 0.5703 & 0.5012 & 0.3674 \\
\midrule[0.8pt] 
\multicolumn{2}{c}{before unlearning} & 0.4433 & 0.4799 & 0.2115 & 0.1843 && 0.8277 & 0.8307 & 0.5302 & 0.3099 \\
\cline{3-6}\cline{8-11}
\multirow{9}{*}{10\%} 
& 0.05 
& 0.4370 & 0.4360 & 0.2231 & 0.1526 && 0.7765 & 0.6204 & 0.4825 & 0.3137 \\
& 0.10 
& 0.4222 & 0.4290 & 0.2048 & 0.1383 && 0.7765 & 0.5818 & 0.4809 & 0.3137 \\
& 0.50 
& 0.4270 & 0.4708 & 0.2088 & 0.1645 && 0.7836 & 0.6310 & 0.4825 & 0.3271 \\
& 0.70 
& 0.4413 & 0.4781 & 0.2088 & 0.1645 && 0.7836 & 0.6545 & 0.4825 & 0.3271 \\
& 1.00 
& 0.4073 & 0.4689 & 0.2074 & 0.1588 && 0.7836 & 0.6291 & 0.4825 & 0.3271 \\
& 2.00 
& 0.4433 & 0.4712 & 0.2362 & 0.2224 && 0.7836 & 0.6375 & 0.4874 & 0.3244 \\
& 4.00 
& 0.4433 & 0.4771 & 0.2225 & 0.1996 && 0.7836 & 0.6018 & 0.4795 & 0.3030 \\
& 5.00 
& 0.4433 & 0.4771 & 0.2260 & 0.2105 && 0.7836 & 0.5387 & 0.5101 & 0.2989 \\
& 7.00 
& 0.4433 & 0.4954 & 0.2260 & 0.1967 && 0.7479 & 0.5387 & 0.4809 & 0.2672 \\
& 10.00 
& 0.4404 & 0.5465 & 0.1905 & 0.1990 && 0.7479 & 0.5387 & 0.4838 & 0.2774 \\
\bottomrule[1.5pt]
\end{tabular}}
\end{table}



\begin{table}[t]
\centering
\caption{\textbf{UWC Tuning for TNPO.}  $\downarrow$ / $\uparrow$ indicate smaller / larger values are preferable.  }\label{tab: tnpo}
\resizebox{0.99\textwidth}{!}{
\begin{tabular}{ccccccccccc}
\toprule[1.5pt]
\multicolumn{2}{c}{TNPO} & \multicolumn{4}{c}{Phi-1.5} && \multicolumn{4}{c}{Llama-2-7B} \\ \cline{3-6}\cline{8-11}
\multirow{2}{*}{setup} & \multirow{2}{*}{$\beta$} &  \multicolumn{2}{c}{ES-exact} & \multicolumn{2}{c}{ES-perturb}  && \multicolumn{2}{c}{ES-exact} & \multicolumn{2}{c}{ES-perturb} \\
& & retain $\uparrow$ & unlearn $\downarrow$ & retain $\uparrow$ & unlearn $\downarrow$ && retain $\uparrow$ & unlearn $\downarrow$ & retain $\uparrow$ & unlearn $\downarrow$ \\ 
\midrule[1.2pt]
\multicolumn{2}{c}{before unlearning} & 0.4433 & 0.5969 & 0.2115 & 0.1605 && 0.8277 & 0.8039 & 0.5302 & 0.4001 \\
\cline{3-6}\cline{8-11}
\multirow{9}{*}{{1\%}}
& 0.05 
& 0.4218 & 0.2626 & 0.2099 & 0.1274 && 0.7641 & 0.2021 & 0.4428 & 0.2897 \\
& 0.10 
& 0.4245 & 0.2613 & 0.2136 & 0.1274 && 0.7655 & 0.2720 & 0.4976 & 0.2720 \\
& 0.50 
& 0.3670 & 0.1899 & 0.2136 & 0.1274 && 0.7393 & 0.1354 & 0.4782 & 0.0669 \\
& 0.70 
& 0.3927 & 0.1524 & 0.2136 & 0.1274 && 0.7321 & 0.1150 & 0.4782 & 0.0479 \\
& 1.00 
& 0.4154 & 0.1524 & 0.2121 & 0.0702 && 0.7491 & 0.1507 & 0.4764 & 0.0768 \\
& 2.00 
& 0.4367 & 0.1524 & 0.2136 & 0.1369 && 0.7038 & 0.1281 & 0.4990 & 0.3538 \\
& 4.00 
& 0.4504 & 0.1092 & 0.1709 & 0.0652 && 0.7324 & 0.1507 & 0.5103 & 0.3148 \\
& 5.00 
& 0.4321 & 0.0967 & 0.1709 & 0.0702 && 0.7657 & 0.1507 & 0.4603 & 0.3025 \\
& 7.00 
& 0.4143 & 0.0740 & 0.2052 & 0.1126 && 0.7001 & 0.1628 & 0.4447 & 0.3242 \\
& 10.00 
& 0.4388 & 0.0967 & 0.2136 & 0.1655 && 0.7518 & 0.1771 & 0.4603 & 0.3679 \\
\midrule[0.8pt]
\multicolumn{2}{c}{before unlearning} & 0.4433 & 0.5619 & 0.2115 & 0.2374 && 0.8277 & 0.7735 & 0.5302 & 0.4126 \\
\cline{3-6}\cline{8-11}
\multirow{9}{*}{5\%}
& 0.05 
& 0.4072 & 0.3340 & 0.2136 & 0.2349 && 0.7558 & 0.5709 & 0.4857 & 0.3136 \\
& 0.10 
& 0.4522 & 0.3618 & 0.2121 & 0.2349 && 0.7678 & 0.5659 & 0.4910 & 0.3869 \\
& 0.50 
& 0.4172 & 0.4095 & 0.2002 & 0.2314 && 0.7836 & 0.5693 & 0.4891 & 0.4066 \\
& 0.70 
& 0.4193 & 0.3709 & 0.2068 & 0.2151 && 0.7514 & 0.4728 & 0.4807 & 0.3681 \\
& 1.00 
& 0.3673 & 0.3832 & 0.1903 & 0.2651 && 0.7494 & 0.4300 & 0.4856 & 0.3975 \\
& 2.00 
& 0.4315 & 0.3542 & 0.2503 & 0.2423 && 0.7534 & 0.3985 & 0.4888 & 0.2750 \\
& 4.00 
& 0.3993 & 0.3729 & 0.2075 & 0.1895 && 0.7490 & 0.2432 & 0.4828 & 0.2098 \\
& 5.00 
& 0.4214 & 0.4023 & 0.1557 & 0.1869 && 0.7450 & 0.1869 & 0.4868 & 0.2252 \\
& 7.00 
& 0.3974 & 0.4062 & 0.2256 & 0.1855 && 0.7662 & 0.0843 & 0.4788 & 0.2225 \\
& 10.00 
& 0.4433 & 0.4287 & 0.1852 & 0.1735 && 0.7501 & 0.0514 & 0.4788 & 0.0777 \\
\midrule[0.8pt] 
\multicolumn{2}{c}{before unlearning} & 0.4433 & 0.4799 & 0.2115 & 0.1843 && 0.8277 & 0.8307 & 0.5302 & 0.3099 \\
\cline{3-6}\cline{8-11}
\multirow{9}{*}{10\%} 
& 0.05 
& 0.4205 & 0.2633 & 0.1772 & 0.1445 && 0.7641 & 0.5864 & 0.4805 & 0.3049 \\
& 0.10 
& 0.4074 & 0.2927 & 0.1748 & 0.1445 && 0.7566 & 0.5997 & 0.4805 & 0.2947 \\
& 0.50 
& 0.4397 & 0.5129 & 0.1829 & 0.1253 && 0.7534 & 0.5164 & 0.4825 & 0.3240 \\
& 0.70 
& 0.3893 & 0.5129 & 0.2414 & 0.1225 && 0.7534 & 0.5164 & 0.4778 & 0.3214 \\
& 1.00 
& 0.4020 & 0.4975 & 0.2020 & 0.1310 && 0.7534 & 0.5164 & 0.4872 & 0.2947 \\
& 2.00 
& 0.3980 & 0.4838 & 0.1888 & 0.0921 && 0.7660 & 0.4395 & 0.5184 & 0.3373 \\
& 4.00 
& 0.3959 & 0.2943 & 0.2157 & 0.0562 && 0.7500 & 0.3028 & 0.4809 & 0.3014 \\
& 5.00 
& 0.4380 & 0.2840 & 0.2050 & 0.0562 && 0.7720 & 0.1481 & 0.4809 & 0.3040 \\
& 7.00 
& 0.4242 & 0.3317 & 0.2286 & 0.0562 && 0.7244 & 0.1530 & 0.4798 & 0.2393 \\
& 10.00 
& 0.4242 & 0.2145 & 0.1541 & 0.0888 && 0.7453 & 0.1781 & 0.5003 & 0.2880 \\
\bottomrule[1.5pt]
\end{tabular}}
\end{table}



\begin{table}[t]
\centering
\caption{\textbf{UWC Tuning for WTNPO ($\alpha=0.5$).} $\downarrow$ / $\uparrow$ indicate smaller / larger values are preferable. }\label{tab: wtnpo_0.5}
\resizebox{0.99\textwidth}{!}{
\begin{tabular}{ccccccccccc}
\toprule[1.5pt]
\multicolumn{2}{c}{WTNPO} & \multicolumn{4}{c}{Phi-1.5} && \multicolumn{4}{c}{Llama-2-7B} \\ \cline{3-6}\cline{8-11}
\multirow{2}{*}{setup} & \multirow{2}{*}{$\beta$} &  \multicolumn{2}{c}{ES-exact} & \multicolumn{2}{c}{ES-perturb}  && \multicolumn{2}{c}{ES-exact} & \multicolumn{2}{c}{ES-perturb} \\
& & retain $\uparrow$ & unlearn $\downarrow$ & retain $\uparrow$ & unlearn $\downarrow$ && retain $\uparrow$ & unlearn $\downarrow$ & retain $\uparrow$ & unlearn $\downarrow$ \\ 
\midrule[1.2pt]
\multicolumn{2}{c}{before unlearning} & 0.4433 & 0.5969 & 0.2115 & 0.1605 && 0.8277 & 0.8039 & 0.5302 & 0.4001 \\
\cline{3-6}\cline{8-11}
\multirow{9}{*}{{1\%}}
& 0.05 
& 0.4412 & 0.1538 & 0.2080 & 0.0700 && 0.7343 & 0.0833 & 0.4863 & 0.0767 \\
& 0.10 
& 0.4394 & 0.1801 & 0.2052 & 0.0652 && 0.7606 & 0.0679 & 0.4957 & 0.0929 \\
& 0.50 
& 0.4142 & 0.1524 & 0.2136 & 0.0677 && 0.7251 & 0.1629 & 0.4976 & 0.0929 \\
& 0.70 
& 0.4325 & 0.1524 & 0.1882 & 0.0527 && 0.7874 & 0.1629 & 0.4863 & 0.0865 \\
& 1.00 
& 0.4412 & 0.1524 & 0.1948 & 0.0527 && 0.7289 & 0.1121 & 0.4976 & 0.1064 \\
& 2.00 
& 0.3944 & 0.1412 & 0.1709 & 0.0527 && 0.6673 & 0.0904 & 0.5152 & 0.3242 \\
& 4.00 
& 0.3713 & 0.0620 & 0.2052 & 0.0527 && 0.7040 & 0.0979 & 0.4358 & 0.1252 \\
& 5.00 
& 0.4213 & 0.0620 & 0.1799 & 0.0527 && 0.7040 & 0.0979 & 0.5152 & 0.3690 \\
& 7.00 
& 0.4315 & 0.0620 & 0.2052 & 0.0813 && 0.7040 & 0.1153 & 0.4974 & 0.1951 \\
& 10.00 
& 0.4523 & 0.0647 & 0.2052 & 0.0813 && 0.7040 & 0.1509 & 0.4603 & 0.2975 \\
\midrule[0.8pt]
\multicolumn{2}{c}{before unlearning} & 0.4433 & 0.5619 & 0.2115 & 0.2374 && 0.8277 & 0.7735 & 0.5302 & 0.4126 \\
\cline{3-6}\cline{8-11}
\multirow{9}{*}{5\%}
& 0.05 
& 0.4374 & 0.3243 & 0.1849 & 0.2479 && 0.7520 & 0.4073 & 0.5122 & 0.4013 \\
& 0.10 
& 0.3745 & 0.3848 & 0.2222 & 0.2479 && 0.7494 & 0.4776 & 0.5122 & 0.4013 \\
& 0.50 
& 0.4041 & 0.3562 & 0.2414 & 0.1587 && 0.7534 & 0.4044 & 0.5109 & 0.3975 \\
& 0.70 
& 0.4080 & 0.4222 & 0.2478 & 0.1867 && 0.7534 & 0.4337 & 0.4809 & 0.3803 \\
& 1.00 
& 0.4560 & 0.4222 & 0.2523 & 0.1967 && 0.7476 & 0.4233 & 0.4809 & 0.3645 \\
& 2.00 
& 0.4402 & 0.3209 & 0.1841 & 0.1850 && 0.7534 & 0.4085 & 0.4888 & 0.2940 \\
& 4.00 
& 0.4433 & 0.3903 & 0.1921 & 0.1619 && 0.7533 & 0.0764 & 0.4872 & 0.1426 \\
& 5.00 
& 0.4454 & 0.3792 & 0.2515 & 0.1719 && 0.7691 & 0.1178 & 0.4950 & 0.1690 \\
& 7.00 
& 0.4454 & 0.3357 & 0.2133 & 0.1669 && 0.7451 & 0.0777 & 0.5022 & 0.1690 \\
& 10.00 
& 0.4454 & 0.3814 & 0.1807 & 0.1694 && 0.7725 & 0.0242 & 0.5319 & 0.2442 \\
\midrule[0.8pt] 
\multicolumn{2}{c}{before unlearning} & 0.4433 & 0.4799 & 0.2115 & 0.1843 && 0.8277 & 0.8307 & 0.5302 & 0.3099 \\
\cline{3-6}\cline{8-11}
\multirow{9}{*}{10\%} 
& 0.05 
& 0.4210 & 0.4711 & 0.1829 & 0.1339 && 0.7534 & 0.5363 & 0.4825 & 0.2884 \\
& 0.10 
& 0.4601 & 0.4711 & 0.1963 & 0.1425 && 0.7534 & 0.5363 & 0.4809 & 0.2757 \\
& 0.50 
& 0.3865 & 0.3518 & 0.2189 & 0.1321 && 0.7534 & 0.5363 & 0.4825 & 0.2677 \\
& 0.70 
& 0.4200 & 0.3753 & 0.1676 & 0.0788 && 0.7534 & 0.5363 & 0.5063 & 0.2872 \\
& 1.00 
& 0.4322 & 0.3432 & 0.1615 & 0.0538 && 0.7520 & 0.4619 & 0.4842 & 0.2769 \\
& 2.00 
& 0.4519 & 0.4117 & 0.2014 & 0.0583 && 0.7720 & 0.3741 & 0.5049 & 0.3335 \\
& 4.00 
& 0.3994 & 0.2390 & 0.1854 & 0.0453 && 0.7720 & 0.0446 & 0.5216 & 0.2989 \\
& 5.00 
& 0.4223 & 0.1658 & 0.2102 & 0.0974 && 0.7691 & 0.0283 & 0.4809 & 0.2898 \\
& 7.00 
& 0.4242 & 0.2035 & 0.1774 & 0.0888 && 0.7484 & 0.0355 & 0.4911 & 0.2118 \\
& 10.00 
& 0.4212 & 0.2742 & 0.1633 & 0.0517 && 0.7717 & 0.0355 & 0.4960 & 0.2537 \\
\bottomrule[1.5pt]
\end{tabular}}
\end{table}


\begin{table}[t]
\centering
\caption{\textbf{UWC Tuning for WTNPO ($\alpha=1$).}   $\downarrow$ / $\uparrow$ indicate smaller / larger values are preferable.  }\label{tab: wtnpo_1}
\resizebox{0.99\textwidth}{!}{
\begin{tabular}{ccccccccccc}
\toprule[1.5pt]
\multicolumn{2}{c}{WTNPO} & \multicolumn{4}{c}{Phi-1.5} && \multicolumn{4}{c}{Llama-2-7B} \\ \cline{3-6}\cline{8-11}
\multirow{2}{*}{setup} & \multirow{2}{*}{$\beta$} &  \multicolumn{2}{c}{ES-exact} & \multicolumn{2}{c}{ES-perturb}  && \multicolumn{2}{c}{ES-exact} & \multicolumn{2}{c}{ES-perturb} \\
& & retain $\uparrow$ & unlearn $\downarrow$ & retain $\uparrow$ & unlearn $\downarrow$ && retain $\uparrow$ & unlearn $\downarrow$ & retain $\uparrow$ & unlearn $\downarrow$ \\ 
\midrule[1.2pt]
\multicolumn{2}{c}{before unlearning} & 0.4433 & 0.5969 & 0.2115 & 0.1605 && 0.8277 & 0.8039 & 0.5302 & 0.4001 \\
\cline{3-6}\cline{8-11}
\multirow{9}{*}{{1\%}}
& 0.05 
& 0.4412 & 0.1738 & 0.2052 & 0.0659 && 0.7090 & 0.1376 & 0.4863 & 0.0767 \\
& 0.10 
& 0.4412 & 0.1738 & 0.1989 & 0.0659 && 0.7166 & 0.1376 & 0.4879 & 0.0767 \\
& 0.50 
& 0.4412 & 0.1738 & 0.1925 & 0.0527 && 0.7713 & 0.1319 & 0.4968 & 0.0767 \\
& 0.70 
& 0.4412 & 0.1738 & 0.1861 & 0.0567 && 0.7118 & 0.0840 & 0.4896 & 0.0767 \\
& 1.00 
& 0.4412 & 0.1738 & 0.2052 & 0.0619 && 0.7522 & 0.0897 & 0.4896 & 0.0767 \\
& 2.00 
& 0.4412 & 0.0647 & 0.1978 & 0.0465 && 0.6497 & 0.0648 & 0.4777 & 0.0793 \\
& 4.00 
& 0.4199 & 0.0647 & 0.1969 & 0.0452 && 0.7040 & 0.0736 & 0.4960 & 0.0844 \\
& 5.00 
& 0.3790 & 0.0385 & 0.2074 & 0.0527 && 0.7040 & 0.0736 & 0.4955 & 0.1140 \\
& 7.00 
& 0.4258 & 0.0425 & 0.1865 & 0.0527 && 0.7040 & 0.0999 & 0.4505 & 0.1505 \\
& 10.00 
& 0.4319 & 0.0620 & 0.2070 & 0.0813 && 0.7214 & 0.1359 & 0.5200 & 0.2588 \\
\midrule[0.8pt]
\multicolumn{2}{c}{before unlearning} & 0.4433 & 0.5619 & 0.2115 & 0.2374 && 0.8277 & 0.7735 & 0.5302 & 0.4126 \\
\cline{3-6}\cline{8-11}
\multirow{9}{*}{5\%}
& 0.05 
& 0.4560 & 0.4082 & 0.2259 & 0.1967 && 0.7534 & 0.3855 & 0.4841 & 0.3697 \\
& 0.10 
& 0.4000 & 0.4238 & 0.2242 & 0.1967 && 0.7491 & 0.3754 & 0.4780 & 0.3645 \\
& 0.50 
& 0.4320 & 0.4062 & 0.1990 & 0.1063 && 0.7534 & 0.3754 & 0.4888 & 0.2914 \\
& 0.70 
& 0.4200 & 0.4062 & 0.1992 & 0.0823 && 0.7463 & 0.4174 & 0.4869 & 0.2837 \\
& 1.00 
& 0.4278 & 0.3698 & 0.2557 & 0.1097 && 0.7317 & 0.4240 & 0.4812 & 0.2837 \\
& 2.00 
& 0.4029 & 0.2473 & 0.2134 & 0.1203 && 0.7534 & 0.3786 & 0.4848 & 0.2642 \\
& 4.00 
& 0.4454 & 0.3853 & 0.2077 & 0.1105 && 0.7658 & 0.0781 & 0.4807 & 0.1971 \\
& 5.00 
& 0.4454 & 0.2985 & 0.2227 & 0.1754 && 0.7625 & 0.0681 & 0.4772 & 0.1820 \\
& 7.00 
& 0.4254 & 0.2913 & 0.1644 & 0.1679 && 0.7594 & 0.0448 & 0.4795 & 0.1356 \\
& 10.00 
& 0.3894 & 0.2826 & 0.1639 & 0.1477 && 0.7887 & 0.0304 & 0.4873 & 0.1871 \\
\midrule[0.8pt] 
\multicolumn{2}{c}{before unlearning} & 0.4433 & 0.4799 & 0.2115 & 0.1843 && 0.8277 & 0.8307 & 0.5302 & 0.3099 \\
\cline{3-6}\cline{8-11}
\multirow{9}{*}{10\%} 
& 0.05 
& 0.4810 & 0.2738 & 0.2188 & 0.0595 && 0.7534 & 0.5363 & 0.4779 & 0.2677 \\
& 0.10 
& 0.4246 & 0.2024 & 0.2036 & 0.0637 && 0.7534 & 0.4953 & 0.4809 & 0.2884 \\
& 0.50 
& 0.4180 & 0.3978 & 0.1639 & 0.0434 && 0.7491 & 0.5030 & 0.5073 & 0.2947 \\
& 0.70 
& 0.4540 & 0.3663 & 0.2202 & 0.0417 && 0.7534 & 0.5030 & 0.4989 & 0.2675 \\
& 1.00 
& 0.4502 & 0.2201 & 0.1992 & 0.0494 && 0.7513 & 0.3768 & 0.4893 & 0.2989 \\
& 2.00 
& 0.4234 & 0.1453 & 0.2065 & 0.0107 && 0.7551 & 0.2972 & 0.5185 & 0.2575 \\
& 4.00 
& 0.4205 & 0.1344 & 0.1958 & 0.0193 && 0.7675 & 0.0402 & 0.4792 & 0.2553 \\
& 5.00 
& 0.4208 & 0.1260 & 0.1926 & 0.0239 && 0.7691 & 0.0378 & 0.4960 & 0.2255 \\
& 7.00 
& 0.3934 & 0.1464 & 0.1557 & 0.1002 && 0.7001 & 0.0335 & 0.4742 & 0.2090 \\
& 10.00 
& 0.3860 & 0.1123 & 0.1652 & 0.1132 && 0.7693 & 0.0525 & 0.4943 & 0.2459 \\
\bottomrule[1.5pt]
\end{tabular}}
\end{table}



\begin{table}[t]
\centering
\caption{\textbf{UWC Tuning for WTNPO ($\alpha=1.5$).}   $\downarrow$ / $\uparrow$ indicate smaller / larger values are preferable. }\label{tab: wtnpo_1.5}
\resizebox{0.99\textwidth}{!}{
\begin{tabular}{ccccccccccc}
\toprule[1.5pt]
\multicolumn{2}{c}{WTNPO} & \multicolumn{4}{c}{Phi-1.5} && \multicolumn{4}{c}{Llama-2-7B} \\ \cline{3-6}\cline{8-11}
\multirow{2}{*}{setup} & \multirow{2}{*}{$\beta$} &  \multicolumn{2}{c}{ES-exact} & \multicolumn{2}{c}{ES-perturb}  && \multicolumn{2}{c}{ES-exact} & \multicolumn{2}{c}{ES-perturb} \\
& & retain $\uparrow$ & unlearn $\downarrow$ & retain $\uparrow$ & unlearn $\downarrow$ && retain $\uparrow$ & unlearn $\downarrow$ & retain $\uparrow$ & unlearn $\downarrow$ \\ 
\midrule[1.2pt]
\multicolumn{2}{c}{before unlearning} & 0.4433 & 0.5969 & 0.2115 & 0.1605 && 0.8277 & 0.8039 & 0.5302 & 0.4001 \\
\cline{3-6}\cline{8-11}
\multirow{9}{*}{{1\%}}
& 0.05 
& 0.4412 & 0.1688 & 0.1925 & 0.0619 && 0.7118 & 0.1319 & 0.4685 & 0.0767 \\
& 0.10 
& 0.4412 & 0.1688 & 0.2052 & 0.0619 && 0.7094 & 0.1319 & 0.4911 & 0.0767 \\
& 0.50 
& 0.4412 & 0.1412 & 0.2010 & 0.0619 && 0.7141 & 0.0472 & 0.4895 & 0.0398 \\
& 0.70 
& 0.4135 & 0.0647 & 0.2052 & 0.0557 && 0.7189 & 0.0679 & 0.4740 & 0.0793 \\
& 1.00 
& 0.4327 & 0.0647 & 0.1818 & 0.0619 && 0.6186 & 0.0824 & 0.4798 & 0.0767 \\
& 2.00 
& 0.4391 & 0.0647 & 0.1693 & 0.0274 && 0.7021 & 0.0736 & 0.4704 & 0.0844 \\
& 4.00 
& 0.4183 & 0.0647 & 0.1963 & 0.0336 && 0.7021 & 0.0736 & 0.4974 & 0.0844 \\
& 5.00 
& 0.4173 & 0.0647 & 0.1911 & 0.0425 && 0.7040 & 0.0912 & 0.5022 & 0.1505 \\
& 7.00 
& 0.4258 & 0.0500 & 0.2033 & 0.0425 && 0.7040 & 0.1404 & 0.4583 & 0.1428 \\
& 10.00 
& 0.4243 & 0.0620 & 0.2053 & 0.0527 && 0.7040 & 0.1521 & 0.4589 & 0.1667 \\
\midrule[0.8pt]
\multicolumn{2}{c}{before unlearning} & 0.4433 & 0.5619 & 0.2115 & 0.2374 && 0.8277 & 0.7735 & 0.5302 & 0.4126 \\
\cline{3-6}\cline{8-11}
\multirow{9}{*}{5\%}
& 0.05 
& 0.4539 & 0.4062 & 0.2574 & 0.0926 && 0.7505 & 0.3786 & 0.5122 & 0.3783 \\
& 0.10 
& 0.4560 & 0.4062 & 0.2374 & 0.0646 && 0.7534 & 0.3911 & 0.4908 & 0.3295 \\
& 0.50 
& 0.3934 & 0.2448 & 0.1984 & 0.0672 && 0.7534 & 0.3911 & 0.4888 & 0.3628 \\
& 0.70 
& 0.4469 & 0.2448 & 0.1934 & 0.1012 && 0.7505 & 0.3786 & 0.4888 & 0.3295 \\
& 1.00 
& 0.4510 & 0.2448 & 0.1791 & 0.1203 && 0.7534 & 0.3786 & 0.4888 & 0.3052 \\
& 2.00 
& 0.3915 & 0.3621 & 0.2047 & 0.1067 && 0.7534 & 0.3354 & 0.4828 & 0.2456 \\
& 4.00 
& 0.4214 & 0.3393 & 0.2172 & 0.1217 && 0.7533 & 0.0427 & 0.4805 & 0.1257 \\
& 5.00 
& 0.4334 & 0.2879 & 0.2247 & 0.1320 && 0.7480 & 0.0753 & 0.4950 & 0.1916 \\
& 7.00 
& 0.4454 & 0.2879 & 0.2177 & 0.1154 && 0.7497 & 0.0100 & 0.4796 & 0.1895 \\
& 10.00 
& 0.3894 & 0.2071 & 0.2177 & 0.1154 && 0.7570 & 0.0198 & 0.4920 & 0.1342 \\
\midrule[0.8pt] 
\multicolumn{2}{c}{before unlearning} & 0.4433 & 0.4799 & 0.2115 & 0.1843 && 0.8277 & 0.8307 & 0.5302 & 0.3099 \\
\cline{3-6}\cline{8-11}
\multirow{9}{*}{10\%} 
& 0.05 
& 0.4262 & 0.1453 & 0.1816 & 0.0091 && 0.7534 & 0.4925 & 0.4852 & 0.2677 \\
& 0.10 
& 0.4704 & 0.1625 & 0.1926 & 0.0173 && 0.7534 & 0.4437 & 0.4896 & 0.2677 \\
& 0.50 
& 0.4519 & 0.2246 & 0.2185 & 0.0280 && 0.7720 & 0.3792 & 0.4977 & 0.2677 \\
& 0.70 
& 0.4145 & 0.1369 & 0.2167 & 0.0453 && 0.7683 & 0.2972 & 0.5154 & 0.2677 \\
& 1.00 
& 0.4254 & 0.1253 & 0.2110 & 0.0336 && 0.7720 & 0.0355 & 0.5202 & 0.2842 \\
& 2.00 
& 0.4345 & 0.1135 & 0.2090 & 0.0109 && 0.7625 & 0.0149 & 0.4825 & 0.2989 \\
& 4.00 
& 0.4234 & 0.1357 & 0.2190 & 0.0120 && 0.7549 & 0.0451 & 0.5133 & 0.2677 \\
& 5.00 
& 0.4306 & 0.1347 & 0.1998 & 0.0239 && 0.7807 & 0.0111 & 0.5061 & 0.2952 \\
& 7.00 
& 0.3934 & 0.1161 & 0.1660 & 0.1002 && 0.7735 & 0.0043 & 0.4976 & 0.2302 \\
& 10.00 
& 0.4149 & 0.1380 & 0.1591 & 0.1002 && 0.7691 & 0.1148 & 0.4911 & 0.2921 \\
\bottomrule[1.5pt]
\end{tabular}}
\end{table}



\begin{table}[t]
\centering
\caption{\textbf{UWC Tuning for RMU (shallow).}   $\downarrow$ / $\uparrow$ indicate smaller / larger values are preferable. }\label{tab: rmu_s}
\resizebox{0.99\textwidth}{!}{
\begin{tabular}{ccccccccccc}
\toprule[1.5pt]
\multicolumn{2}{c}{RMU} & \multicolumn{4}{c}{Phi-1.5} && \multicolumn{4}{c}{Llama-2-7B} \\ \cline{3-6}\cline{8-11}
\multirow{2}{*}{setup} & \multirow{2}{*}{$c$} &  \multicolumn{2}{c}{ES-exact} & \multicolumn{2}{c}{ES-perturb}  && \multicolumn{2}{c}{ES-exact} & \multicolumn{2}{c}{ES-perturb} \\
& & retain $\uparrow$ & unlearn $\downarrow$ & retain $\uparrow$ & unlearn $\downarrow$ && retain $\uparrow$ & unlearn $\downarrow$ & retain $\uparrow$ & unlearn $\downarrow$ \\ 
\midrule[1.2pt]
\multicolumn{2}{c}{before unlearning} & 0.4433 & 0.5969 & 0.2115 & 0.1605 && 0.8277 & 0.8039 & 0.5302 & 0.4001 \\
\cline{3-6}\cline{8-11}
\multirow{7}{*}{{1\%}}
& 0.00 
& 0.4530 & 0.5969 & 0.2007 & 0.1855 &&  0.7604 & 0.5993 & 0.4888 & 0.3816 \\
& 1.00 
& 0.4122 & 0.4356 & 0.2115 & 0.1855 &&  0.7502 & 0.6278 & 0.4890 & 0.4253 \\
& 2.00 
& 0.4312 & 0.4080 & 0.2072 & 0.1855 && 0.7653 & 0.6714 & 0.4531 & 0.4002 \\
& 4.00 
& 0.4245 & 0.4682 & 0.2115 & 0.1855 && 0.7356 & 0.7223  & 0.4758 & 0.4008 \\
& 5.00 
& 0.4398 & 0.5149 & 0.1981 & 0.1855 && 0.7163 & 0.6287 & 0.4871 & 0.4008 \\
& 7.00 
& 0.4460 & 0.5096 & 0.2201 & 0.1855 &&  0.7292 & 0.7128 & 0.4516 & 0.4104 \\
& 10.00 
& 0.4215 & 0.4816 & 0.2018 & 0.1855 && 0.7292 & 0.6195 & 0.4453 & 0.4104 \\
\midrule[0.8pt]
\multicolumn{2}{c}{before unlearning} & 0.4433 & 0.5619 & 0.2115 & 0.2374 && 0.8277 & 0.7735 & 0.5302 & 0.4126 \\
\cline{3-6}\cline{8-11}
\multirow{7}{*}{5\%}
& 0.00 
& 0.4164 & 0.4924 & 0.1918 & 0.2172 && 0.7516 & 0.7292 & 0.4676 & 0.3616 \\
& 1.00 
& 0.4284 & 0.5124 & 0.2194 & 0.2172 &&  0.7762 & 0.7357 & 0.4677 & 0.4504 \\
& 2.00 
& 0.4044 & 0.4774 & 0.1939 & 0.2172 && 0.7146 & 0.6370 & 0.4453 & 0.4126 \\
& 4.00 
& 0.4404 & 0.4252 & 0.2047 & 0.2147 && 0.7619 & 0.6758 & 0.4812 & 0.4126 \\
& 5.00 
& 0.4404 & 0.4838 & 0.2181 & 0.2207 && 0.7139 & 0.6758 & 0.4812 & 0.4164 \\
& 7.00 
& 0.4204 & 0.3772 & 0.2073 & 0.2339 && 0.7604 & 0.6758 & 0.4793 & 0.4126 \\
& 10.00 
& 0.4194 & 0.4114 & 0.1903 & 0.2339 &&  0.7146 & 0.6370 & 0.4453 & 0.4126 \\
\midrule[0.8pt] 
\multicolumn{2}{c}{before unlearning} & 0.4433 & 0.4799 & 0.2115 & 0.1843 && 0.8277 & 0.8307 & 0.5302 & 0.3099 \\
\cline{3-6}\cline{8-11}
\multirow{7}{*}{10\%} 
& 0.00 
& 0.4425 & 0.5761 & 0.2055 & 0.1424 && 0.7887 & 0.8165 & 0.4246 & 0.2662 \\
& 1.00 
& 0.4424 & 0.5968 & 0.2133 & 0.1567 && 0.7568 & 0.6869 & 0.4771 & 0.2989 \\
& 2.00 
& 0.4304 & 0.5961 & 0.2028 & 0.1360 && 0.7628 & 0.6755 & 0.4690 & 0.2989 \\
& 4.00 
& 0.4364 & 0.5208 & 0.1944 & 0.1547 && 0.7229 & 0.5784 & 0.4812 & 0.2766 \\
& 5.00 
& 0.4284 & 0.5184 & 0.2007 & 0.1547 && 0.7262 & 0.6268 & 0.4797 & 0.2944 \\
& 7.00 
& 0.4404 & 0.5184 & 0.2007 & 0.1754 && 0.7271 & 0.5778 & 0.4232 & 0.3033 \\
& 10.00 
& 0.4404 & 0.4693 & 0.2136 & 0.1675 && 0.7032 & 0.5455 & 0.4849 & 0.3033 \\
\bottomrule[1.5pt]
\end{tabular}}
\end{table}

\begin{table}[t]
\centering
\caption{\textbf{UWC Tuning for RMU (middle).}   $\downarrow$ / $\uparrow$ indicate smaller / larger values are preferable. }\label{tab: rmu_m}
\resizebox{0.99\textwidth}{!}{
\begin{tabular}{ccccccccccc}
\toprule[1.5pt]
\multicolumn{2}{c}{RMU} & \multicolumn{4}{c}{Phi-1.5} && \multicolumn{4}{c}{Llama-2-7B} \\ \cline{3-6}\cline{8-11}
\multirow{2}{*}{setup} & \multirow{2}{*}{$c$} &  \multicolumn{2}{c}{ES-exact} & \multicolumn{2}{c}{ES-perturb}  && \multicolumn{2}{c}{ES-exact} & \multicolumn{2}{c}{ES-perturb} \\
& & retain $\uparrow$ & unlearn $\downarrow$ & retain $\uparrow$ & unlearn $\downarrow$ && retain $\uparrow$ & unlearn $\downarrow$ & retain $\uparrow$ & unlearn $\downarrow$ \\ 
\midrule[1.2pt]
\multicolumn{2}{c}{before unlearning} & 0.4433 & 0.5969 & 0.2115 & 0.1605 && 0.8277 & 0.8039 & 0.5302 & 0.4001 \\
\cline{3-6}\cline{8-11}
\multirow{7}{*}{{1\%}}
& 0.00 
& 0.4203 & 0.5969 & 0.2153 & 0.2069 && 0.7606 & 0.5127 & 0.5115 & 0.4001 \\
& 1.00 
& 0.4203 & 0.5969 & 0.2180 & 0.1409 && 0.7416 & 0.5093 & 0.4878 & 0.4001 \\
& 2.00 
& 0.4203 & 0.5969 & 0.1831 & 0.1261 && 0.7512 & 0.4263 & 0.4644 & 0.3794 \\
& 4.00 
& 0.4203 & 0.5969 & 0.1831 & 0.1261 && 0.7559 & 0.5093 & 0.4096 & 0.3538 \\
& 5.00 
& 0.4203 & 0.5969 & 0.2073 & 0.1328 && 0.7413 & 0.4810 & 0.4927 & 0.4001 \\
& 7.00 
& 0.4218 & 0.5969 & 0.2119 & 0.1261 && 0.7413 & 0.4810 & 0.4927 & 0.4001 \\
& 10.00 
& 0.4203 & 0.5969 & 0.2119 & 0.1350 && 0.7655 & 0.4137 & 0.4927 & 0.3624 \\
\midrule[0.8pt]
\multicolumn{2}{c}{before unlearning} & 0.4433 & 0.5619 & 0.2115 & 0.2374 && 0.8277 & 0.7735 & 0.5302 & 0.4126 \\
\cline{3-6}\cline{8-11}
\multirow{7}{*}{5\%}
& 0.00 
& 0.4262 & 0.5723 & 0.1952 & 0.2207 && 0.8017 & 0.6376 & 0.4754 & 0.3884 \\
& 1.00 
& 0.4232 & 0.4999 & 0.2032 & 0.2207 &&  0.7381 & 0.4284 & 0.4798 & 0.3884 \\
& 2.00 
& 0.4232 & 0.5013 & 0.2229 & 0.2207 &&  0.7179 & 0.5146 & 0.4379 & 0.3884 \\
& 4.00 
& 0.4218 & 0.5309 & 0.1887 & 0.2030 &&  0.7112 & 0.4034 & 0.4927 & 0.3884 \\
& 5.00 
& 0.3578 & 0.3762 & 0.2119 & 0.2030 && 0.7438 & 0.6323 & 0.4927 & 0.3884 \\
& 7.00 
& 0.4218 & 0.5946 & 0.1990 & 0.1971 && 0.7438 & 0.6684 & 0.4927 & 0.4126 \\
& 10.00 
& 0.4262 & 0.4000 & 0.1968 & 0.2005 &&  0.7552 & 0.6615 & 0.4644 & 0.4126 \\
\midrule[0.8pt] 
\multicolumn{2}{c}{before unlearning} & 0.4433 & 0.4799 & 0.2115 & 0.1843 && 0.8277 & 0.8307 & 0.5302 & 0.3099 \\
\cline{3-6}\cline{8-11}
\multirow{7}{*}{10\%} 
& 0.00 
& 0.4262 & 0.4584 & 0.1952 & 0.1786 && 0.7463 & 0.6152 & 0.4754 & 0.3884 \\
& 1.00 
& 0.4203 & 0.4909 & 0.2108 & 0.1816 && 0.7493 & 0.7636 & 0.4379 & 0.3139 \\
& 2.00 
& 0.4232 & 0.5025 & 0.2212 & 0.1786 && 0.7374 & 0.7275 & 0.4831 & 0.3158 \\
& 4.00 
& 0.4394 & 0.5025 & 0.2117 & 0.1901 &&  0.7874 & 0.7526 & 0.4871 & 0.3196 \\
& 5.00 
& 0.4224 & 0.4511 & 0.2117 & 0.1799 &&  0.7874 & 0.6907 & 0.4653 & 0.3220 \\
& 7.00 
& 0.4005 & 0.4568 & 0.1496 & 0.1741 && 0.7434 & 0.5821 & 0.4776 & 0.2908 \\
& 10.00 
& 0.4522 & 0.4938 & 0.1542 & 0.2000 && 0.7534 & 0.6495 & 0.4927 & 0.3316 \\
\bottomrule[1.5pt]
\end{tabular}}
\end{table}



\begin{table}[t]
\centering
\caption{\textbf{UWC Tuning for RMU (deep).}   $\downarrow$ / $\uparrow$ indicate smaller / larger values are preferable. }\label{tab: rmu_d}
\resizebox{0.99\textwidth}{!}{
\begin{tabular}{ccccccccccc}
\toprule[1.5pt]
\multicolumn{2}{c}{UWC} & \multicolumn{4}{c}{Phi-1.5} && \multicolumn{4}{c}{Llama-2-7B} \\ \cline{3-6}\cline{8-11}
\multirow{2}{*}{setup} & \multirow{2}{*}{$c$} &  \multicolumn{2}{c}{ES-exact} & \multicolumn{2}{c}{ES-perturb}  && \multicolumn{2}{c}{ES-exact} & \multicolumn{2}{c}{ES-perturb} \\
& & retain $\uparrow$ & unlearn $\downarrow$ & retain $\uparrow$ & unlearn $\downarrow$ && retain $\uparrow$ & unlearn $\downarrow$ & retain $\uparrow$ & unlearn $\downarrow$ \\ 
\midrule[1.2pt]
\multicolumn{2}{c}{before unlearning} & 0.4433 & 0.5969 & 0.2115 & 0.1605 && 0.8277 & 0.8039 & 0.5302 & 0.4001 \\
\cline{3-6}\cline{8-11}
\multirow{7}{*}{{1\%}}
& 0.00 
& 0.3936 & 0.5219 & 0.2136 & 0.1574 &&  0.7836 & 0.6364 & 0.4927 & 0.4089 \\
& 1.00 
& 0.4156 & 0.5219 & 0.2117 & 0.1574 && 0.7461 & 0.4564 & 0.4442 & 0.3402 \\
& 2.00 
& 0.4212 & 0.5219 & 0.2080 & 0.1655 && 0.6977 & 0.2814 & 0.4847 & 0.2790 \\
& 4.00 
& 0.4212 & 0.5153 & 0.1951 & 0.1655 && 0.6913 & 0.2992 & 0.4428 & 0.2748 \\
& 5.00 
& 0.4212 & 0.5121 & 0.2062 & 0.1655 && 0.7122 & 0.3974 & 0.4976 & 0.1982 \\
& 7.00 
& 0.4212 & 0.5108 & 0.1885 & 0.1686 && 0.7509 & 0.3271 & 0.4428 & 0.2305 \\
& 10.00 
& 0.4184 & 0.4963 & 0.2136 & 0.1717 && 0.7106 & 0.3815 & 0.4428 & 0.2062 \\
\midrule[0.8pt]
\multicolumn{2}{c}{before unlearning} & 0.4433 & 0.5619 & 0.2115 & 0.2374 && 0.8277 & 0.7735 & 0.5302 & 0.4126 \\
\cline{3-6}\cline{8-11}
\multirow{7}{*}{5\%}
& 0.00 
& 0.4212 & 0.4953 & 0.2007 & 0.2182 && 0.7731 & 0.7074 & 0.4675 & 0.3953 \\
& 1.00 
& 0.4049 & 0.5144 & 0.2115 & 0.2182 && 0.7731 & 0.6488 & 0.4801 & 0.3850 \\
& 2.00 
& 0.4110 & 0.5602 & 0.1967 & 0.2227 && 0.7410 & 0.6683 & 0.4801 & 0.3714 \\
& 4.00 
& 0.4151 & 0.5621 & 0.1930 & 0.2227 && 0.7731 & 0.6031 & 0.4598 & 0.3869 \\
& 5.00 
& 0.4212 & 0.5271 & 0.2099 & 0.2394 &&  0.7464 & 0.7001 & 0.4613 & 0.3958 \\
& 7.00 
& 0.4212 & 0.5285 & 0.1951 & 0.2394 && 0.8113 & 0.6983 & 0.5015 & 0.4464 \\
& 10.00 
& 0.4064 & 0.4816 & 0.2025 & 0.2349 && 0.7319 & 0.7763 & 0.4600 & 0.4393 \\
\midrule[0.8pt] 
\multicolumn{2}{c}{before unlearning} & 0.4433 & 0.4799 & 0.2115 & 0.1843 && 0.8277 & 0.8307 & 0.5302 & 0.3099 \\
\cline{3-6}\cline{8-11}
\multirow{7}{*}{10\%} 
& 0.00 
& 0.4212 & 0.4935 & 0.2095 & 0.1933 && 0.7577 & 0.6868 & 0.4410 & 0.2884 \\
& 1.00 
& 0.4049 & 0.4935 & 0.2039 & 0.1963 && 0.7673 & 0.7560 & 0.4571 & 0.2906 \\
& 2.00 
& 0.4212 & 0.4935 & 0.1969 & 0.1933 &&  0.7731 & 0.7402 & 0.4865 & 0.3239 \\
& 4.00 
& 0.4212 & 0.4935 & 0.2115 & 0.1933 &&0.7731 & 0.7414 & 0.4426 & 0.2674 \\
& 5.00 
& 0.4212 & 0.4959 & 0.1967 & 0.1933 &&0.7486 & 0.7688 & 0.4738 & 0.2192 \\
& 7.00 
& 0.4212 & 0.4799 & 0.2097 & 0.1933 && 0.7620 & 0.7402 & 0.4784 & 0.2547 \\
& 10.00 
& 0.3934 & 0.4799 & 0.1951 & 0.1786 &&0.7394 & 0.7402 & 0.4890 & 0.2547 \\
\bottomrule[1.5pt]
\end{tabular}}
\end{table}





\end{document}
