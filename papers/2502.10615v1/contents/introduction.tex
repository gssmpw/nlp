\vspace{-1em}
\section{Introduction}
Many real-world applications, such as e-commerce dynamic search advertising~\citep{prabhu2018parabel} and semantic matching in product search~\citep{chang2021extreme}, can be formulated as eXtreme Multi-label Classification (\XMC) problems.
These tasks involve retrieving relevant labels from a label collection of extremely large size $L$, typically under a tight latency budget.
In such applications, $L$ can be millions or more, and the label space is often long-tailed, posing a significant challenge in designing \XMC methods that are good at labels with varying frequencies~\citep{dahiya2023ngame,gupta2024dual}.
For head labels, it necessitates methods capable of \textit{memorization}, which involves capturing all the detailed knowledge needed for accurate prediction.
For tail labels, it requires \textit{generalization}, as the model needs to capture general label representations that can be used to predict labels with few training instances.
%\peter{Tag for discussion}
%\jj{<= label ``features'' would be a bit confusing, maybe properpties or charateristics?} 


Traditional one-versus-all (OVA) methods prioritize memorization via trainable label classifiers, yet struggle to generalize effectively across tail labels~\citep{gupta2024dual}.
Recently, the emergence of pre-trained language models (LMs) has demonstrated strong generalization power, prompting the adoption of dual-encoder (\DE) architectures to effectively leverage label text for zero-shot \XMC tasks~\citep{gupta2021generalized,xiong2021extreme,aggarwal2023semsup}.
%With label text features, a tail label can be predicted if its description matches the input.
However, without extensive training, these models may not excel at memorization, suggesting a pure \DE model is not sufficient to achieve good performance at head labels of \XMC tasks.

To achieve both memorization and generalization power, competitive \XMC approaches, such as \NGAMEova~\citep{dahiya2023ngame} \DEXAova~\citep{dahiya2023deep}, and \OAK~\citep{mohan2024oak} usually follow a complex multi-stage solution.
They first train a \DE model that learns semantic embeddings between input and label text, which can generalize to unseen input queries and tail labels with text features.
Then the \DE model is further augmented by trainable one-versus-all (OVA) label classifiers, which enhance the memorization capability of head labels with diverse intent of queries.
Recently, the pioneering work, \DEXML~\citep{gupta2024dual}, achieved impressive results on several \XMC benchmark datasets via a pure \DE model, which significantly reduces the amount of trainable model parameters.
While being a strong predictive model, \DEXML requires extreme long training time to memorize the training signals of head labels, making it challenging to be applied to industrial applications.


% \begin{wrapfigure}{r}{0.44\textwidth}
%     \vspace{-1.2em}
%     \begin{center}
%         \includegraphics[width=0.44\textwidth]{./figures/LfWikiSmall-perf-vs-runtime}
%     \end{center}
%     \vspace{-1.0em}
%     \caption{Training time versus performance.}
%   %\vspace{-1em}
% \label{fig:intro}
% \end{wrapfigure}
In this paper, we introduce Retrieval-augmented Encoders for \XMC (\RAEXMC), a novel framework that equips a dual-encoder (\DE) model with the \textit{retrieval-augmented} capability.
%and addresses the inherent memorization challenges of \DE models for \XMC problems.
%By integrating the $k$-nearest neighbors algorithm,
\RAEXMC compensates for the poor memorization capabilities of \DE and eliminates the need for precise matching between input instance and label embedding spaces, thereby significantly reducing the training difficulty of \XMC.
%Through the retrieval of relevant instances from the training corpus during prediction, \RAEXMC eliminates the need for precise matching between input instance and label embedding spaces, thereby significantly reducing the training difficulty of \XMC.
Specifically, \RAEXMC trains the \DE model to contrast over a joint knowledge memory space that consists of label descriptions and input instances.
During the inference, given a test instance, \RAEXMC first retrieves the top $b$ keys from the knowledge memory.
\RAEXMC then generates predictions by aggregating the values (i.e., labels) of these keys based on their scores.
The contributions of this paper are threefold as follows:
\vspace{-.25em}
\begin{itemize}
    \item We introduce \RAEXMC, a novel retrieval-augmented framework for \XMC problems,
    which enhances the underlying \DE model with better memorization capability.
    Specifically, \RAEXMC offers controllable trade-off between memorization and generalization, namely the performance of head and tail label segments, respectively (cf., Section~\ref{sec:exp-head-tail}).
    \item \RAEXMC significantly reduces the training difficulty for \XMC problems in two ways. Firstly, it eliminates the need for training OVA label classifiers to memorize intricate complex patterns in head labels. Secondly, it alleviates meticulously training of the \DE model to match label and input instance spaces (cf., Section~\ref{sec:exp-other-enc}).
    \item We conducted extensive experiments to demonstrate the effectiveness and efficiency of \RAEXMC. Our approach not only advances the results of the current SOTA DE method, \DEXML~\citep{gupta2024dual}, but also achieves performance on par with the SOTA XMC method, OAK~\citep{mohan2024oak}, without relying on external knowledge. Additionally, it offers significantly lower training time, achieving more than a 10x speedup on the largest \LfAmznLarge dataset (cf. Section~\ref{sec:exp-perf-vs-time}).
\end{itemize}
