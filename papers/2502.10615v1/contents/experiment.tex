%\newpage
\section{Experiments}
\label{sec:exp}

\vspace{-.5em}
\paragraph{Datasets and Evaluation.}
We conduct experiments on four LF-\XMC datasets, including \LfAmznSmall, \LfWikiSmall, \LfWikiLarge, and \LfAmznLarge, where the prefix `LF' denotes the datasets contain label descriptions. 
Details of these datasets and its statistics are presented in Table~\ref{tab:datasets} of Appendix~\ref{sup:exp-setup}.
We use the same raw text input and training/test data splits as in ~\citep{gupta2024dual} to have a fair and reproducible comparison.
Following the evaluation setup from \XMC Repository~\citep{bhatia2016xmc} and \XMC literature~\citep{yu2022pecos,gupta2024dual,schultheis2024generalized},
we consider Precision (P@1 and P@5), Recall (R@100), and Macro average F1 to measure the standard \XMC and retrieval metrics.

\paragraph{Baselines.}
We compare \RAEXMC to competitive \XMC baselines, which fall into two categories.
The first category is dual-encoder (DE) models without learning OVA label classifiers, including \DPR~\citep{karpukhin2020dense}, \ANCE~\citep{xiong2021approximate}, \NGAME~\citep{dahiya2023ngame}, \DEXA~\citep{dahiya2023deep}, and \DEXML~\citep{gupta2024dual};
The second category is deep learning models with learning OVA label classifiers, such as \XRT~\citep{zhang2021fast} \NGAMEova~\citep{dahiya2023ngame}, \DEXAova~\citep{dahiya2023deep},  \OVAKNN~\citep{wang2022contrastive}, and OAK~\citep{mohan2024oak}.
For \OVAC, we apply One-versus-All Binary Cross-Entropy (OvA-BCE) as the training loss, and \OVAKNN~\cite{wang2022contrastive} applies kNN prediction on top of the OvA Classifier model, as described in Equation~\ref{eq:prediction_ablation}.
OAK is a recent retrieval-augmented method for XMC that incorporates external data, such as Wikipedia tags and ChatGPT-4-generated queries, as additional knowledge sources~\citep{mohan2024oak}. In contrast, \RAEXMC constructs its knowledge memory solely from the training split of XMC benchmark datasets, making the experimental setups different.

\RAEXMC falls into the first category, which has much smaller model parameters to learn.
Thus, we consider the state-of-the-art (SOTA) DE method, \DEXML as our main baseline. 
To have a fair comparison, we use the same 66M parameter distilbert-base transformer encoder~\citep{sanh2020distilbert} for \RAEXMC, as used in \NGAME, \DEXA, \DEXML, \OVAC and \OVAKNN~\citep{wang2022contrastive}.
%Note that the performance can be furthered improved with an advanced sentence encoder~\footnote{We conducted experiments on some sentence encoders in \url{https://huggingface.co/spaces/mteb/leaderboard}, and found that well-trained sentence encoder performs better.}.
%We fix the number of retrieved key as $b=200$, and Softmax temperature as $\tau=0.04$, unless otherwise specify.
All the experiments are conducted on an AWS p4d.24xlarge instance, with 8 Nvidia A100 GPUs and 96 CPUs.
See Appendix~\ref{sup:exp-setup} for more details on the experiment setup and hyper-parameters of \RAEXMC.



\begin{table*}[!ht]
    \centering
    \caption{
        Comparing \RAEXMC with recent \XMC methods on four public LF-\XMC datasets.
        Superscripts $^\dagger$, $^\star$, and $\blacklozenge$ indicate results excerpted from \XMC Repository~\citep{bhatia2016xmc}, \DEXA~\citep{dahiya2023deep}, and OAK~\citep{mohan2024oak} respectively.
        Superscripts $\ddagger$ denotes that we reproduce \DEXML results by evaluating their official released model checkpoints.
        TT denotes training time in hours. For \DEXML and \RAEXMC, TT is measured on 8 A100 GPUs.
        Blank entries indicate source does not have those numbers.
        We conduct significant test between \RAEXMC and \DEXML on three metrics (P@1, P@5, R@100) across all datasets.
        All results are significant (p-values smaller than $10^{-10}$) except for P@1 on \LfAmznLarge. See Appendix~\ref{sup:sigtest} for more details.
        %\peter{TODO: DEXML numbers and sig-test.} \yaushian{DEXML numbers have been updated}
        %All p-values are extremely small ($<10^{-10}$), except P@1 for \LfAmznLarge. See Appendix~\ref{sup:sigtest}.
    }
    \vspace{-.5em}
    \resizebox{0.9\textwidth}{!}{
    \begin{tabular}{l|c|cccr|cccr}
        \toprule
            Methods & \begin{tabular}[c]{@{}l@{}}OvA Label\\ Classifier\end{tabular} & P@1 & P@5 & R@100 & TT & P@1 & P@5 & R@100 & TT \\
        \midrule
            & & \multicolumn{4}{c}{\LfAmznSmall} & \multicolumn{4}{c}{\LfWikiSmall} \\   
        \midrule
        \midrule
            \OVAC          & \cmark & 37.13 & 18.32 &     64.93 &   1.2  & 41.10  & 20.43  & 72.81 & 2.2 \\
            \OVAKNN             & \cmark & 38.87 & 19.19 &  65.96 &  1.2 & 43.30 & 21.91 & 73.39 & 2.2 \\
            \XRTDagg            & \cmark & 38.10 & 18.32 &   -    & 35.4 & 42.57 & 21.30 &   -   & 119.5 \\
            \NGAMEovaDagg       & \cmark & \underline{46.01} & 21.47 &   -    & 12.6 & \underline{47.65}  & \underline{23.68} &   -  &  75.4 \\
            \DEXAovaDagg        & \cmark & \bf{46.42} & \underline{21.59} &  -    & 13.0 & 47.11 & 22.71 &   - &  78.6 \\
            \OAKStar          & \cmark & - & - & - &  - & \bf{48.57} & 23.28 & - &  - \\
        \midrule
            \DPRStar            & \xmark & 41.85 & 20.88 &   -   &   -  & 41.66 & 20.66 &   -   &     - \\
            \ANCEStar           & \xmark & 42.67 & 20.98 &   -   &   -  & 44.35 & 21.99 &   -   &     - \\
            \NGAMEStar          & \xmark & 42.61 & 20.69 &   -   &   -  & 43.58 & 20.86 &   -   &     - \\
            \DEXAStar           & \xmark & 44.76 & 21.18 &   -   &   -  & 46.57 & 22.26 &   -   &     - \\
            \DEXMLDagg          & \xmark & 42.24 & 20.47 & \underline{68.81} &  \underline{2.1} & 45.76 & 21.75 & \underline{72.87} &  \underline{12.8} \\
            \RAEXMC (ours)      & \xmark & 45.10 & \bf{21.95} & \bf{71.94} & \bf{0.6} & 48.04 & \bf{23.68} & \bf{79.92} & \bf{0.6} \\
        \midrule
            & & \multicolumn{4}{c}{\LfWikiLarge}& \multicolumn{4}{c}{\LfAmznLarge} \\   
        \midrule
        \midrule
            \OVAStar  & \cmark & 82.00 & 48.54 & - &     - & 48.72 & 39.09 & - & - \\
            \XRTDagg            & \cmark & 81.62 & 47.85 & - & 318.9 & 50.98 & 40.05 & - & 132.0 \\
            \NGAMEovaDagg       & \cmark & 84.01 & 49.97 & - &  54.9 & 56.75 & 44.09 & - &  97.8 \\
            \DEXAovaDagg        & \cmark & 84.92 & 50.51 & - &  57.5 & 56.63 & 43.90 & - & 103.1 \\
            \OAKStar          & \cmark & 85.23 & \bf{50.79}	 & - &  - & - & - & - &  - \\
        \midrule
            \DPRStar            & \xmark & 65.23 & 35.23 & - &  54.7 & 44.64 & 34.83 & - &  96.8 \\
            \ANCEStar           & \xmark & 63.33 & 33.12 & - &  75.1 & 46.44 & 37.59 & - & 447.3 \\
            \NGAMEStar          & \xmark & 77.92 & 40.95 & - &  41.9 & 45.82 & 35.48 & - &  \underline{75.5} \\
            \DEXAStar           & \xmark & 79.99 & 42.52 & - &  42.8 & 51.92 & 38.86 & - &  76.6 \\
            \DEXMLDagg          & \xmark & \underline{85.59} & \underline{50.39} & \bf{90.52} &  \underline{37.0} &  \underline{58.43} & \underline{45.48} & \underline{64.26} & 132.0 \\
            \RAEXMC (ours)      & \xmark & \bf{86.49} & 50.67 & \underline{90.24} & \bf{6.4} & \bf{58.48} & \bf{47.00} & \bf{66.88} & \bf{7.4} \\
        \bottomrule
    \end{tabular}
    \vspace{-.5em}
    }%
    \label{tab:main_results}
\end{table*}

\subsection{Main Results}
\label{sec:exp-main}

Table\ref{tab:main_results} presents the main results and the training time (in hours).
For large-scale datasets, such as \LfWikiLarge and \LfAmznLarge,
\RAEXMC not only advances the current SOTA \DE method, \DEXML~\citep{gupta2024dual}, but also achieves significant speedup in training time.
Specifically, \RAEXMC has a speedup of 6x and 18x over \DEXML, for \LfWikiLarge and \LfAmznLarge, respectively.

Compared with \OVAKNN~\citep{wang2022contrastive} which also leverages retrieval-augmentation using label information, \RAEXMC achieves significantly better performance, attributed to a more effective training loss and an improved prediction function.
More recently, OAK~\citep{mohan2024oak} incorporates external knowledge, such as Wikipedia and ChatGPT-4-generated queries, to construct augmented document representations using a three-stage training pipeline. While \RAEXMC achieves performance on par with OAK, our method does so without relying on external data and with significantly fewer trainable parameters. Additionally, OAK aggregates multiple retrieved auxiliary data points, regardless of their labels, whereas \RAEXMC explicitly leverages the label information of retrieved memories to improve predictions.

Compared with the complex two-stage \XMC methods that learn both a \DE model and trainable OVA label classifiers,
\RAEXMC outperforms \NGAMEova and \DEXAova across all four datasets, except P@1 on the smallest \LfAmznSmall. 
Note that the number of trainable parameters of \RAEXMC is significantly smaller than \OVAKNN, \NGAMEova, \DEXAova, and \OAK which scales linearly to the size of label space.
As the label space increases, the memory and training difficulty for OVA classifiers increase accordingly, which makes it challenging to apply to large-scale datasets.





%Another highlight of \RAEXMC is its exceptional performance on R@100.  Unlike conventional \DE or OVA methods, which may struggle to retrieve all relevant labels due to either poor memorization or poor generalization, our approach can effectively retrieve all kinds of labels that demand different capabilities. \yaushian{Do we still want to highlight this considering the R@100 of Wiki-500k?}
%Compared to conventional \XMC models with label OVA classifiers (e.g., \NGAMEova and \DEXAova), \RAEXMC has on-par or better performance, but with significantly less training time and much smaller number of trainable parameters. The number of trainable parameters of \NGAMEova and \DEXAova scales linearly to the size of label space. As the label space increases, the memory and training difficulty for OVA increase accordingly, which makes it challenging to apply to large-scale datasets.
%\RAEXMC achieves on par performance with state-of-the art \DE method, namely \DEXML~\citep{gupta2024dual}, but with only $1/3 \sim 1/20$ of total training time that includes the time of hard negative mining. Although \DE methods do not train any label classifier, there is a performance gap between \DE methods and OVA methods. To overcome this, \DEXML applies gradient cache approach, to have a precise estimation of model predicted label distribution, which significantly increases the training time. \RAEXMC doesn't require intensive training to match the instance space and label space, thus significantly reduces the training difficulty and training time.
%\yaushian{Can we claim we perform well on R@100 because we can match more tail labels?}


\vspace{-1.2em}
\subsection{Performance versus Training Efficiency}
\label{sec:exp-perf-vs-time}
In Figure~\ref{fig:perf-vs-runtime}, we study the trade-off between model predictive power (i.e., Precision@1) versus the corresponding model training time.
%For additional results on other metrics, see Appendix (\peter{TODO}).
To match \DEXML performance~\citep{gupta2024dual}, we see that \RAEXMC is extremely efficient to train.
\RAEXMC achieves 154x, 20x and 18x speedup to achieve on par performance of \DEXML, for \LfWikiSmall, \LfWikiLarge, and \LfAmznLarge , respectively.
As discussed in Section~\ref{sec:rae-inference}, the inference of \RAEXMC is equivalent to the inference of \DE models when $\lambda=0.0$, which is the green curve in Figure~\ref{fig:perf-vs-runtime}.
Eventually, we expect \RAEXMC with $\lambda=0.0$ progresses toward the performance of \DEXML (the magenta line), while it may take significantly longer time because of the large-scale label space.
On the other hand, \RAEXMC with $\lambda=1.0$, the blue curve in Figure~\ref{fig:perf-vs-runtime}, reduces to the vanilla $k$NN classifier, which have strong predictive power at the early stage of training while saturating at the later stage.
Finally, the proposed \RAEXMC method (w/ $\lambda=0.5$) effectively combines the best of both world between the vanilla kNN classifier and the DE model, as shown in the red curve in Figure~\ref{fig:perf-vs-runtime}.

\begin{figure}[!ht]
    \centering
    % \begin{subfigure}[b]{0.45\textwidth}
    %      \centering
    %      \includegraphics[width=\textwidth]{figures/LfAmznSmall-perf-vs-runtime.png}
    %      \caption{\LfAmznSmall}
    %      \label{fig:LfAmznSmall-perf-vs-runtime}
    % \end{subfigure}
    %\hfill
    \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/LfWikiSmall-perf-vs-runtime.png}
         \caption{\LfWikiSmall}
         \label{fig:LfAmznSmall-perf-vs-runtime}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/LfWikiLarge-perf-vs-runtime.png}
        \caption{\LfWikiLarge}
        \label{fig:LfWikiLarge-perf-vs-runtime}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/LfAmznLarge-perf-vs-runtime.png}
        \caption{\LfAmznLarge}
        \label{fig:LfAmznLarge-perf-vs-runtime}
    \end{subfigure}
    \caption{
        Model performance versus model training time on two large-scale LF-\XMC datasets.
        Y-axis and X-axis are Precision@1 metric and training time in hours measured on 8 Nvidia A100 GPUs.
    }
    \label{fig:perf-vs-runtime}
\end{figure}


\subsection{Memorization and Generalization Capability}
\label{sec:exp-head-tail}

\begin{wraptable}{r}{.52\textwidth}
    \centering
    \vspace{1.5em}
    \caption{
        Macro-average F1@5 metrics under four label segments: Head, Torso, Tail, and Extreme Tail (xTail), which is based on label frequency of $\{ (1000, \text{N}_{\text{trn}}), (100, 1000], (10, 100], (0, 10] \}$.
    }
    %\vspace{-.5em}
    \resizebox{0.51\textwidth}{!}{
    \begin{tabular}{l|rrrr}
        \toprule
             Methods & Head & Torso & Tail & xTail \\ 
        \midrule
            & \multicolumn{4}{c}{\LfWikiLarge} \\
        \midrule
        \DEXML~\citep{gupta2024dual}             & 72.1 & 43.6 & 40.1 & 41.1  \\
        $\text{RAE-XMC}$                         & 73.8 & 43.0 & 39.5 & 39.1  \\
        $\text{RAE-XMC}_{\lambda\rightarrow0.0}$ & 60.8 & 49.4 & 44.7 & 43.1  \\
        $\text{RAE-XMC}_{\lambda\rightarrow1.0}$ & 71.1 & 36.7 & 29.6 & 21.6  \\
        \midrule
            & \multicolumn{4}{c}{\LfAmznLarge} \\
        \midrule
        \DEXML~\citep{gupta2024dual}             & 28.7 & 15.2 & 13.3 & 20.6 \\
        $\text{RAE-XMC}$                         & 33.8 & 15.8 & 12.0 & 17.4 \\
        $\text{RAE-XMC}_{\lambda\rightarrow0.0}$ & 15.9 & 13.4 & 14.2 & 22.0 \\
        $\text{RAE-XMC}_{\lambda\rightarrow1.0}$ & 34.5 & 14.8 &  9.9 & 12.0 \\
        \bottomrule
    \end{tabular}
    }%
    \label{tab:head_vs_tail}
\end{wraptable}

Similar to NGAME~\citep{dahiya2023ngame} and \DEXML~\citep{gupta2024dual},
we dissect the label space into four segments: head, torso, tail, and extreme tail (xTail), based on the label frequency.
This study investigates the model's capability on memorization and generalization, where the model should excel at the head and xTail segments, respectively.
In Table~\ref{tab:head_vs_tail}, we report Macro-average F1@5 metric on two largest LF-\XMC datasets, which better measures the tail label performance~\citep{zhang2023long,schultheis2024generalized,schultheis2024consistent}.



By varying $\lambda$ at inference stage, \RAEXMC demonstrates controllable trade-off between memorization and generalization, namely the performance on head and tail label segments, respectively.
Take \LfAmznLarge as an example.
Comparing $\text{RAE-XMC}_{\lambda\rightarrow 1}$ and $\text{RAE-XMC}_{\lambda\rightarrow 0}$, the former solely relies on the memorization while the latter solely relies on the generalization component, respectively.
Thus, the former ($\lambda=1$) has a higher F1@5 for the head label segment (i.e., 34.5 vs 15.9), while the latter ($\lambda=0$) has a higher F1@5 for the xTail label segment (i.e., 12.0 vs. 22.0).
With the selected $\lambda$, \RAEXMC equips the underling \DE model with better memorization capability, which advances the current SOTA result of \DEXML~\citep{gupta2024dual} on the head label segment (i.e., 33.8 vs. 28.7).


% \begin{table*}[!ht]
%     \centering
%     \caption{
%         Macro-average F1@5 metrics under four label segments: Head, Torso, Tail, and Extreme Tail (xTail), which is based on label frequency of $\{ (1000, \text{N}_{\text{trn}}), (100, 1000], (10, 100], (0, 10] \}$.
%     }
%     %\vspace{-.5em}
%     \resizebox{0.8\textwidth}{!}{
%     \begin{tabular}{l|rrrr|rrrr}
%         \toprule
%             & \multicolumn{4}{c}{\LfWikiLarge} & \multicolumn{4}{c}{\LfAmznLarge} \\ 
%         \midrule
%             Methods & Head & Torso & Tail & xTail & Head & Torso & Tail & xTail \\ 
%         \midrule
%         \midrule
%         \DEXML~\citep{gupta2024dual}             & 72.1 & 43.6 & 40.1 & 41.1 & 28.7 & 15.2 & 13.3 & 20.6 \\
%         $\text{RAE-XMC}$                         & 73.8 & 43.0 & 39.5 & 39.1 & 33.8 & 15.8 & 12.0 & 17.4 \\
%         $\text{RAE-XMC}_{\lambda\rightarrow0.0}$ & 60.8 & 49.4 & 44.7 & 43.1 & 15.9 & 13.4 & 14.2 & 22.0 \\
%         $\text{RAE-XMC}_{\lambda\rightarrow1.0}$ & 71.1 & 36.7 & 29.6 & 21.6 & 34.5 & 14.8 &  9.9 & 12.0 \\
%         \bottomrule
%     \end{tabular}
%     }%
%     \label{tab:head_vs_tail}
% \end{table*}



\subsection{Ablation on Training Loss Functions}
As shown in Table~\ref{tab:train_loss}, using in-batch input instances as negatives in Equation~\ref{eq:in-batch} consistently improves performance, except for P@1 on \LfWikiLarge.
This demonstrates the effectiveness of training the model to contrast over a joint label and instance embedding space.
We found that hard-negative mining has a varied impact on different metrics.
It consistently improves P@1, but sometimes negatively affects R@100.
This is because hard-negative mining makes the model more selective, strictly removing labels that may have less relevance.




\begin{table*}[!ht]
    \centering
    \caption{
        Ablation studies of the training objective of \RAEXMC in Equation~\ref{eq:in-batch}.
        "In-batch neg X" denotes whether we include negative inputs within the batch in the denominator of Equation~~\ref{eq:in-batch}.
        "Hard-Neg" denotes whether we conduct hard negative mining from the label space.
    }

    \resizebox{0.975\textwidth}{!}{
    \begin{tabular}{cc|rrr|rrr|rrr}
        \toprule
            \multicolumn{2}{c}{Variants of \RAEXMC} & \multicolumn{3}{c}{\LfWikiSmall} & \multicolumn{3}{c}{\LfWikiLarge} & \multicolumn{3}{c}{\LfAmznLarge} \\ 
        \midrule
            \begin{tabular}[c]{@{}l@{}}In-batch neg X\end{tabular} & Hard-Neg & P@1 & P@5 & R@100 & P@1 & P@5 & R@100 & P@1 & P@5 & R@100  \\ 
        \midrule
        \midrule
        %\xmark & \xmark & 46.36 & 23.08 & 79.82 & 84.19 & 48.91 & 90.49 & 58.09 & 46.94 & 67.60 \\
        \xmark & \cmark & 46.51 & 22.97 & 78.79 & 86.71 & 49.75 & 89.18 & 58.12 & 46.75 & 66.46 \\
        \cmark & \xmark & 47.84 & 23.72 & 80.50 & 84.25 & 48.55 & 91.43 & 57.94 & 46.82 & 67.22 \\
        \cmark & \cmark & 48.04 & 23.68 & 79.92 & 86.49 & 50.67 & 90.24 & 58.43 & 47.00 & 66.88 \\
        \bottomrule
    \end{tabular}
    }%
    \label{tab:train_loss}
\end{table*}


\subsection{Ablation on Prediction Functions}

\begin{table*}[!ht]
    \centering
    \vspace{-1.2em}
    \caption{
        Ablation study of the prediction function of \RAEXMC in Eq~\ref{eq:rae-inference-exact} on \LfAmznLarge.
    }

    \resizebox{0.5\textwidth}{!}{
    \begin{tabular}{c|rrr|rrr}
        \toprule
            $\lambda$ & \multicolumn{3}{c}{\OVAKNN prediction} & \multicolumn{3}{c}{\RAEXMC prediction} \\ 
        \midrule
            & P@1 & P@5 & R@100 & P@1 & P@5 & R@100  \\
        \midrule
        0 & 49.72 & 39.45 & 61.18 & 49.72 & 39.45 & 61.18 \\
        0.30&53.59&44.56&64.14&57.37&47.03&67.16 \\
        0.50&\textbf{53.95}&\textbf{44.10}&\textbf{63.52}&\textbf{58.48}&\textbf{47.00}&\textbf{66.88} \\
        0.70& 55.37&43.96&62.95&58.60&46.81&66.65 \\
        1.00&53.98&43.25&61.67&53.98&43.25&61.67 \\
        \bottomrule
    \end{tabular}
    }%
    \label{tab:prediction_ablation}
\end{table*}


In Equation~\ref{eq:rae-inference-exact}, we retrieve keys from the unified instance and label memory $\rmK$ as part of our prediction function. 
Using the same encoder trained by \RAEXMC framework, we compare our inference method against the prediction function of \OVAKNN~\citep{wang2022contrastive}, which separately retrieves keys from input and label spaces, and performs the convex combination as final prediction scores:
\begin{equation}
    \hat{\rvp} = \lambda * \text{Softmax}(\rvq^\top \rmX^\top / \tau)\rmY + (1-\lambda) * \text{Softmax}(\rvq^\top \rmZ^\top / \tau).
    \label{eq:prediction_ablation}
\end{equation}
As shown in Table~\ref{tab:prediction_ablation}, although both methods show improvement over the baselines (i.e., $\lambda=0$ or $\lambda=1$), the improvement of \RAEXMC is much greater. 
This shows the effectiveness of ranking keys in a unified memory, leading to better calibration when combining two different spaces.



\subsection{RAE inference on Various Encoders}
\label{sec:exp-other-enc}

\begin{table*}[!ht]
    \centering
    \vspace{-1.2em}
    \caption{RAE inference results on various Encoders. SFT denotes "supervised fine-tuning".}
    %\vspace{-0.5em}
    \resizebox{0.48\textwidth}{!}{
    \begin{tabular}{l|c|rrr}
        \toprule
             Encoders & SFT & P@1 & P@5 & R@100 \\ 
        \midrule
            & \multicolumn{4}{c}{\LfWikiSmall} \\
        \midrule
        \DEXML~\citep{gupta2024dual}            & \cmark & 45.77 & 21.75 & 72.87 \\
        \quad +RAE (ours)                       & \cmark & 46.23 & 22.06 & 72.23 \\
        DistilBERT~\citep{sanh2020distilbert}   & \xmark &  2.47 &  1.36 & 14.19 \\
        \quad +RAE (ours)                       & \xmark & 16.82 &  9.38 & 56.99 \\
        \GTEbase~\citep{li2023towards}          & \xmark & 22.53 & 11.57 & 54.83 \\
        \quad +RAE (ours)                       & \xmark & 36.52 & 19.32 & 72.84 \\
        \midrule
            & \multicolumn{4}{c}{\LfAmznLarge} \\
        \midrule
        \DEXML~\citep{gupta2024dual}            & \cmark & 58.40 & 45.46 & 64.25 \\
        \quad +RAE (ours)                       & \cmark & 58.83 & 45.76 & 65.55 \\
        DistilBERT~\citep{sanh2020distilbert}   & \xmark & 21.40 & 11.86 & 17.16 \\
        \quad +RAE (ours)                       & \xmark & 35.59 & 28.65 & 46.44 \\
        \GTEbase~\citep{li2023towards}          & \xmark & 27.16 & 16.56 & 27.50 \\
        \quad +RAE (ours)                       & \xmark & 47.03 & 37.73 & 56.01 \\
        \bottomrule
    \end{tabular}
    }%
    \label{tab:encoder}
\end{table*}

In Table~\ref{tab:encoder}, we evaluate RAE inference on various encoders, including those with and without supervised fine-tuning (SFT).
For the methods without RAE inference, we employ standard dual-encoder inference on these encoders. 
Comparing \DEXML with \DEXML+RAE, we consistently observe performance enhancements with our proposed inference method. However, the improvement is relatively modest, mainly because \DEXML has already been well-trained and possesses strong memorization capability.

For the encoders without SFT, we observe significant performance gains with our proposed RAE inference.
Without SFT, pre-trained encoders may struggle to align input instances and label descriptions in embedding space.
Nonetheless, they can still retrieve relevant input instances to enhance the prediction.
Surprisingly, on \LfAmznLarge, even without any fine-tuning, the performance of \GTEbase+RAE can achieve results on par with previous supervised \DE methods, such as \ANCE~\citep{xiong2021approximate} and \NGAME~\citep{dahiya2023ngame}~\footnote{We use gte-base-en-v1.5 in \url{https://huggingface.co/spaces/mteb/leaderboard} as our \GTEbase model. Note that the performance may be further improved with more advanced encoders.}.
This further underscores the effectiveness of our approach in significantly reducing the need for intensive training of the \DE model.


% \begin{table*}[!ht]
%     \centering
%     \caption{RAE inference results on Other Encoders.}

%     \resizebox{0.97\textwidth}{!}{
%     \begin{tabular}{c|c|rrr|rrr|rrr}
%         \toprule
%           \multirow{2}{*}{Encoder} & \multirow{2}{*}{Training} & \multicolumn{3}{c}{\LfWikiSmall} & \multicolumn{3}{c}{\LfWikiLarge} & \multicolumn{3}{c}{\LfAmznLarge} \\  \cmidrule{3-11}
%             & & P@1 & P@5 & R@100 & P@1 & P@5 & R@100 & P@1 & P@5 & R@100  \\ 
%         \midrule
%         \midrule
%         $\text{DEXML}$ & \cmark & 45.76 & 21.74 & 72.91 & 84.77 & 48.91 & 88.30 & 58.43 & 45.48 & 64.26 \\
%         \text{DEXML}+RAE & \cmark & 46.23  & 22.06 & 72.23  & - & - & - & 58.83 & 45.76  & 65.55 \\
%         \midrule
%         DistilBERT~\citep{sanh2020distilbert} & \xmark &  16.82 & 9.38 & 56.99 & 42.18 & 21.48 & 65.57 & 35.59 & 28.65  &46.44 \\
%         gte-base~\citep{li2023towards} & \xmark & &  &  & - & - & - &  &  &  \\
%         gte-large~\citep{li2023towards} & \xmark & &  &  & - & - & - &  &  &  \\
%         \bottomrule
%     \end{tabular}
%     }%
%     \label{tab:encoder}
% \end{table*}



% \subsection{Performance on Tail Labels} \label{Sec:tail}
% Plot performance on different label segments



