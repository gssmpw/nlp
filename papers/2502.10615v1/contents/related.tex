\section{Related Work}

\vspace{-0.2em}
\paragraph{Extreme Multi-label Classification.}
Conventional \XMC methods finetune Transformer encoders and learn one-versus-all (OVA) label classifiers for the \XMC tasks~\citep{chang2020xmctransformer,zhang2021fast,kharbanda2022cascadexml}.
To tackle the large output space challenge, various label space partitioning techniques or surrogate loss functions have been extensively studied~\citep{prabhu2018parabel,jain2019slice,yu2022pecos}.
However, these approaches treat labels as featureless identifiers and learn classification vector for each label, which may not generalize to unseen labels.
On the other hand, there are increasing number of work leverage the label text feature by employing Dual-Encoder (\DE) models that learn embeddings from both input and label text~\citep{saini2021galaxc,mittal2021eclare,dahiya2021siamesexml,dahiya2023ngame,dahiya2023deep}.
However, the two representative methods in this line of work, namely NGAME~\citep{dahiya2023ngame} and DEXA~\citep{dahiya2023deep}, do not solely rely on a DE model.
Specifically, NGAME is a two-stage approach that involves first training input and label embeddings via a \DE model and then utilizing a classification network in the second stage. DEXA builds on NGAME to improve the encoder embeddings by augmenting them with auxiliary parameters.
\DEXML~\citep{gupta2024dual} is a pure \DE model that achieves performance on par with SOTA methods by leveraging a decoupled InfoNCE loss function, without the need for a separate label classifier.
Nevertheless, \DEXML suffers from extremely long training time issue as it relies on gradient cache~\citep{gao2021scaling} to have a precise estimation of the label Softmax distribution that costs $\gO(L)$ per training step.
More recently, OAK~\cite{mohan2024oak} has enhanced extreme multi-label classification by incorporating external auxiliary knowledge, such as Wikipedia and related queries generated by ChatGPT-4. It constructs augmented document representations by aggregating multiple retrieved knowledge sources within a three-stage training framework.

\vspace{-0.2em}
\paragraph{Retrieval Augmented Language Models.}
Using external knowledge to improve deep neural networks has been widely explored in the context of retrieval-augmented language models~\citep{khandelwal2020generalization, guu2020retrieval,lewis2020retrieval,izacard2023atlas,borgeaud2022improving,shi2023replug}.
For example, kNN-LM~\citep{khandelwal2020generalization} interpolates the next-token probability by the neural language model and the kNN model at inference stage. REALM~\citep{guu2020retrieval} and many of its follow-up work~\citep{izacard2023atlas,borgeaud2022improving} consider learning the knowledge retriever jointly with the underlying language models. 
While sharing similar intuition to the proposed \RAEXMC framework, both kNN-LM and REALM are designed for language modeling tasks, which did not consider a multi-label formulation of the knowledge source.

\vspace{-0.2em}
\paragraph{Retrieval Augmented Multi-label Text Classification.}
Retrieval-augment techniques have also been applied to \textit{small-scale} multi-label text classifications~\citep{chalkidis2023retrieval,wang2022contrastive},
where the number of labels is only a few hundreds.
Specifically, ~\citet{chalkidis2023retrieval} considers a multi-stage ranking setup: the first stage learns a text encoder and OVA label classifiers to build the knowledge memory; the second stage trains an expensive cross-attention encoder to produce scores given the test input and the retrieved top-K labels from the first stage.
~\citet{wang2022contrastive} considers training a text encoder and OVA label classifiers with the contrastive loss.
At inference time, they interpolates the OVA label classifier scores with the  top-K instances' label one-hot vector retrieved from the instance space.

We discussed some major difference between ~\citet{wang2022contrastive} and our \RAEXMC framework:
Regarding problem setups, they focus on small-scale multi-label problems with hundreds of labels and treat labels as featureless IDs.
In contrast, \RAEXMC tackles extreme-scale multi-label (XMC) problems with millions of labels and leverage the label text information. 
Speaking of model architectures, they consider OVA label classifiers where the number of trainable parameters scales linearly to $L$, which is not scalable for \XMC tasks.
On the other hand, \RAEXMC is parametrized by Dual-Encoders where the number of trainable parameters does not scale with the number of labels $L$.
Finally, for prediction functions, they conduct two ANN searches: one from the kNN classifier on the input space and the other from the label OvA classifier on the label space, to form final prediction scores.
On the contrary, \RAEXMC conducts a single ANN search on the union of input and label space.
See empirical comparison in Table~\ref{tab:main_results} and Table~\ref{tab:prediction_ablation} for more details.
%For detailed discussion, see Appendix~\ref{sup:related}.