\vspace{-.9em}
\section{Background Material}
\label{sec:background}

\vspace{-.4em}
We study the eXtreme Multi-label Text Classification (\XMC) problem with label text.
Consider a training set of $N$ examples $\gD=\{ (x_i, \rvy_i) \}_{i=1}^N$,
where $x_i \in \gX$ is the $i$-th input instance and $\rvy_i \in \{0, 1\}^L$ is the one-hot label vector with $y_{i,\ell}=1$ indicating that the label $\ell$ is relevant to the input instance $x_i$.
Depending on the availability of label text, $\ell$ denotes either a label index or label text, respectively.
%in scenarios without and with label text, respectively.
The label space $\gL$ consists of $|\gL|=L$ labels.
The goal of \XMC is to learn a scoring function $s_{\theta}: \gX \times \gL \rightarrow \sR$, parameterized by model parameters $\theta$, such that $s_{\theta}(x, \ell)$ indicates the relevance between input instance $x$ and label $\ell$.

\vspace{-.5em}
\paragraph{Model Parametrization.}
Based on the availability of label text features, there are two model families for the \XMC task: the One-versus-all (\OVA) classifier and the Dual-Encoder (\DE) model.
The \OVA classifier, which ignores the label text features, consists of an input text encoder $f_{\phi}: \gX \rightarrow \sR^d$ and a classification matrix $\rmW \in \sR^{L \times d}$. 
The scoring function of the \OVA classifier then becomes $s_{\theta}(x,\ell)=\langle \rvx , \rvw_\ell \rangle$ where $\rvx = f_{\phi}(x)$ is the input query embedding and $\rvw_\ell$ (the $\ell$th row of $\rmW$) is the classification vector for label $\ell$.
For the \OVA classifier, the trainable parameter $\theta=[\phi; \rmW]$ scales linearly to the size of label space $L$, which can be large and difficult to train.

In contrast, our paper studies the \DE model, where the scoring function is defined as $s_{\theta}(x, \ell) = \langle \rvx, \rvz_\ell \rangle = \langle f_\phi(x), h_\psi(\ell) \rangle$, with $f_\phi: \gX \rightarrow \sR^d$ as the input text encoder and $h_\psi: \gL \rightarrow \sR^d$ as the label text encoder. 
Compared to the \OVA classifier, the trainable parameters of the \DE model, $\theta=[\phi; \psi]$, are often much smaller since they do not scale with the size of the label space.


\vspace{-.5em}
\paragraph{Learning.}
%Given a training set $\gD$, the scorer~$s_\theta$ can be learned by minimizing the following objective:
% \begin{equation*}
%     \gJ(s_\theta; \gD) = \frac{1}{N} \sum_{i=1}^N J(x_i, \rvy_i; s_\theta)
% \end{equation*}
Given a training set $\gD$, the scorer~$s_\theta$ can be learned by minimizing some  surrogate loss functions for some metrics of interest (e.g., Precision@k and Recall@k).
One popular choice is the one-versus-all (OVA) reduction ~\citep{dembczynski2010bayes} to $L$ binary classification tasks and employs binary cross-entropy (BCE) loss function.
On the other hand, one can employ a multi-label to multi-class reduction~\citep{menon2019multilabel} and invoke the Softmax cross-entropy loss:
\begin{equation} 
    J(x, \rvy; s_\theta)
    = -\sum_{\ell \in [L]} y_\ell \cdot \log
    \biggl( \frac{ \exp\big(s_{\theta}(x,\ell)/\tau\big) }{ \sum_{\ell' \in [L]} \exp\big(s_{\theta}(x,\ell')/\tau\big) } \Biggr).
\label{eq:learning}
\end{equation}
In practice, various negative sampling techniques are used to approximate the partition function for the Softmax cross-entropy loss, such as within batch negatives~\citep{chang2020pretraining,karpukhin2020dense} and hard negative mining~\citep{xiong2021approximate,dahiya2023ngame}. 

\paragraph{Inference.}
Given a test query embedding $\rvq = f_\phi(q) \in \sR^d$ and the offline pre-computed label embedding matrix $\rmZ \in \sR^{L \times d}$,
we aim to retrieve the $k$ most relevant labels from $\rmZ$ in real-time (low inference latency), which is also known as the Maximum Inner Product Search (\MIPS) problem.
Exact inference of \MIPS requires $\gO(L)$ time complexity, which is prohibited for \XMC tasks where $L$ can be millions or more.  
Thus, practitioners leverage Approximate Nearest Neighbor Search (\ANN) methods to approximately solve it in time sub-linear to the size of the label space $L$.

Conventional \XMC methods with \OVA classifiers learn a tree-based~\citep{prabhu2018parabel,zhang2021fast,yu2022pecos} \ANN index or graph-based \ANN index~\citep{liu2021label,gupta2022elias} at the training stage, and deploy it for fast retrieving top $k$ labels in $\gO(\log L)$.
In contrast, \DE models~\citep{dahiya2023deep,gupta2024dual} encode the label embedding matrix $\rmZ$, and apply existing \ANN solvers (e.g., \FAISS~\citep{johnson2019billion}, \SCANN~\citep{guo2020accelerating}, \HNSWLIB~\citep{malkov2018efficient}) to build the ANN index and achieve fast retrieval in $\gO(\log L)$ time. 
