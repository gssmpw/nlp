
\section{Conclusion and Limitations}
\label{sec:conclusion}
We introduce the novel \RAEXMC framework for \XMC, which enhances the \DE model with retrieval-augmented capabilities and achieves performance on par with SOTA methods on large-scale LF-\XMC datasets.
While being effective and highly efficient to train, \RAEXMC also comes with two limitations.
First, the knowledge memory will bring additional inference overhead, both in space and time complexity.
How to select representative key-value pairs to reduce the size of knowledge memory is an interesting and challenging avenue for future work.
In addition, the performance of \RAEXMC may be hindered if there is huge discrepancy between the knowledge memory (training corpus and label space) versus the test instances.
How to mitigate this issue by learning data-driven $\lambda$ in the value matrix is another interesting direction, which we save for future work.
%We investigated a well known \XMC problems and studied it in the abstract form, so we donâ€™t envision significant additional negative implications for society.

%Specifically, \RAEXMC first retrieves keys from the knowledge memory of both input and label space, and aggregates the corresponding values as the prediction for \XMC tasks.
%\RAEXMC offers controllable trade-off between memorization and generalization, namely the performance of head and tail label segments, respectively.
%Through this framework, we effectively mitigate the training complexity of \DE for \XMC tasks and achieve new SOTA results on large-scale LF-\XMC datasets with significantly shorter training times.
     
%In conclusion, this paper introduces \RAEXMC, a novel dual-encoder framework tailored for extreme multi-label classification. At the training stage, \RAEXMC is optimized using contrastive loss over a joint input instance and label description space. Similarly, during inference, \RAEXMC retrieves the top-K similar keys over a joint input instance and label knowledge memory, aggregating the corresponding values to generate prediction scores. Through this framework, we effectively mitigate the training complexity associated with XMC and achieve SOTA results on various datasets  with significantly shorter training times.