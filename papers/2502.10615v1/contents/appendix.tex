\newpage
\appendix

% \section{Comparison to ~\citet{wang2022contrastive}}
% \label{sup:related}
% We highlight four major difference between the proposed \RAEXMC framework and the most related work~\citep{wang2022contrastive}:
% \begin{itemize}
%     \item \textbf{Problem Setup.} ~\citet{wang2022contrastive} concerns small multi-label problems without label text, where \#labels is only hundreds and the label distribution is often uniform.
%     Instead, RAE-XMC tackles extreme multi-label problems with label text features, where \#labels is millions. This problem is also referred to many-shot retrieval problem~\citep{gupta2024dual}. 
%     Multi-label classifiers often perform sub-optimal in XMC tasks for several reasons: (1) long-tail label distribution, with less than 10 training data for most labels (as high as 90\% of labels) (2) with millions of labels and limited training instance for each label, optimizing the label classifiers is challenging.
%     \item \textbf{Model architecture.} ~\citet{wang2022contrastive} considers a text encoder $f_\theta$ and one-vs-all (OVA) label classifiers, with space complexity $\gO(|\theta| + Ld)$.
%     In contrast, \RAEXMC is only parametrized by the dual encoder, with space complexity $\gO(|\theta|)$, which do not scale with the number of labels $L$.
%     Using DistilBERT on LF-AmazonTitles-1.3M as an example. The number of trainable parameters of \RAEXMC is \textit{16x smaller} than ~\citet{wang2022contrastive} approach (66M vs. 1068M), suggesting \RAEXMC is much scalable to \XMC tasks.
%     \item \textbf{Prediction Function.} ~\citet{wang2022contrastive} considers a convex combination of two predictors, one from OVA label classifiers and the other from kNN retrieved instances' label one-hot vectors. This requires two ANN searches, one from the input space and the other from the label space, resulting in ANN time complexity $\gO(log(NL))$.
%     On the contrary, \RAEXMC only requires one ANN search on the union of the input/label space, resulting in the ANN time complexity $\gO(log(N+L))$.
%     \item \textbf{Training Loss}. ~\citet{wang2022contrastive} is trained with two loss functions: Binary Cross Entropy (BCE) and Contrastive loss. On the other hand, \RAEXMC is trained by the Decoupled Softmax loss~\citep{gupta2024dual}, which is a variant of Contrastive loss tailored for the XMC problems. 
% \end{itemize}

% \section{Limitations and Future Work}
% \label{sup:limitation}
% The proposed \RAEXMC framework enables a \DE model with retrieval-augmented capability which has stronger memorization power and achieve new SOTA results for large-scale LF-\XMC datasets.
% Nonetheless, there are still challenges that need to be addressed. We highlight some interesting directions as future work:
% \begin{itemize}
%     \item \textbf{Inference Overhead.} Our \RAEXMC framework employs the \ANN searcher to retrieve top-$b$ keys from the knowledge memory (i.e., $\gO(\log(N + L))$) and aggregates the corresponding values via a sparse Matrix vector multiplication (i.e., $\gO(b\log L)$). These two operations may lead to additional inference overhead, in both time and space complexity.
%     How to select representative key-value pairs to reduce the size of knowledge memory while not degrading the predictive power is an interesting and challenging avenue for future work.

%     \item \textbf{Discrepancy between the Knowledge Memory and test instances.} Another limitation is the inherent dependency of retrieval-augmented methods on the alignment between the testing and training corpora. If there is a significant mismatch in instance space or label distributions, the performance of retrieving relevant input instances can suffer. To mitigate this issue, careful tuning of the inference hyper-parameter $\lambda$ becomes necessary. We discuss this in Appendix~\ref{sec:exp-lambda}.
% \end{itemize}

%The main limitation of our proposed method is the increased inference latency, where we discuss this in Appendix~\ref{sup:latency}. While conventional DE methods only search over the label space, our method searches over both the training instances and the label space, leading to a longer inference time. 

% \yaushian{TODO: more limitations? and future work.}

% \begin{itemize}
%     \item Inference Overhead: index size and latency
%     \item Training/Inference consistency: the loss function. {I prefer not to explicitly say this XD. It's hard to explain why optimizing Q2Q loss doesn't work.}
%     \item Others ?
% \end{itemize}

% \section{Broader Impacts} \label{sup:broader}
% %Our proposed method can significantly reduce the training time of XMC systems, thereby reducing GPU usage. XMC is a crucial research problem with a wide range of applications, including product search, recommendation systems, and advertisement. By making XMC models more efficient, our approach contributes to more sustainable machine learning practices and broader accessibility of advanced XMC technologies in various domains.

% This paper focuses on solving the foundational research challenge in XMC, so we do not see any direct path to negative societal impacts. Moreover, as XMC is a crucial research problem, our proposed method could lead to potential positive societal impacts by providing more efficient and affordable machine learning solutions to communities with limited computational resources.


\section{Implementation Details}
\label{sup:exp-setup}

\subsection{Datasets}
Following standard XMC literature, we downloaded the datasets from a public XMC repository\footnote{\url{http://manikvarma.org/downloads/XC/XMLRepository.html}}, which already splits the data into training and testing sets.
\begin{table*}[!ht]
    \centering
    \caption{Data Statistics. \Ntrn and \Ntst refer to the number of instances in the training and test set, respectively. $L$: the number of labels. $\bar{L}$: the average number of positive labels per instance. $\bar{N}$: the average number of instances per label.}
    \begin{tabular}{l|r|r|r|r|r}
        \toprule
            Dataset      & \Ntrn     & \Ntst & $L$ & $\bar{L}$ & $\bar{N}$ \\
        \midrule
            \LfAmznSmall &   294,805 & 134,835 &   131,073 &  2.29 &  5.15 \\
            \LfWikiSmall &   693,082 & 177,515 &   312,330 &  2.11 &  4.67 \\
            \LfWikiLarge & 1,813,391 & 783,743 &   501,070 &  4.74 & 17.15 \\
            \LfAmznLarge & 2,248,619 & 970,237 & 1,305,265 & 22.20 & 38.24 \\
        \bottomrule
    \end{tabular}
    \label{tab:datasets}
\end{table*}

\subsection{Evaluation Metrics}
For P@k (precision at k), we report $\frac{\text{TP@k}}{\text{k}}$, where TP@k denotes the number of true positives in the top k ranking list. For R@k, we report $\frac{\text{TP@k}}{\text{TP@k+FN@k}}$, where FN@k denotes false negatives in the top k ranking list.

To obtain macro-average F1@k, we average the each label F1 score over a label segment. 
For each label $l$, we compute $\text{F1}_l$@k as $2 \cdot \frac{\text{P}_{l}\text{@k} \cdot \text{R}_l\text{@k}}{\text{P}_{l}\text{@k} + \text{R}_l\text{@k}}$. 
The $P_{\text{l}}\text{@k}=\frac{\text{TP}_l\text{@k}}{\text{TP}_l\text{@k} + \text{FP}_l\text{@k}}$, and the $R_{\text{l}}\text{@k}=\frac{\text{TP}_l\text{@k}}{\text{TP}_l\text{@k}+\text{FN}_l\text{@k}}$.
The $\text{TP}_l\text{@k}$, $\text{FP}_l\text{@k}$, and $\text{FN}_l\text{@k}$ denote true positives, false positives, and false negatives respectively for each label $l$ in the top k ranking list.


\subsection{Hyper-parameters of \RAEXMC}
\label{sup:impl}

All experiments take place on an AWS p4d.24xlarge instance, which has 8 Nvidia A100 GPUs (40 GB memory per GPU) and 96 Intel CPUs. The CPU memory is 1.10TB in total.

\paragraph{Training Hyper-parameters.}
For fair comparison, we use the same 66M parameter distilbert-base transformer encoder~\citep{sanh2020distilbert} as used in \NGAME~\citep{dahiya2023ngame}, \DEXA~\citep{dahiya2023deep}, and \DEXML~\citep{gupta2024dual}, unless otherwise specify. 
We use average pooling of the last hidden states of the Transformer encoder as the final input/label embeddings, followed by l2-normalization to project embeddings onto the unit-sphere.
As the input/label embeddings are l2-normalized, the similarity scores are bounded between $[-1, 1]$, thus we fix the Softmax temperature $\tau$ to be $0.04$, which approximately amounts to $1/\sqrt{d}$ where $d$ is the embedding dimension.
We train \RAEXMC with the AdamW optimizer and use linear decay with the warm-up learning rate schedule.

Rest of the training hyper-parameters considered in our experiments are described below
\begin{itemize}
    \item max\_len: maximum length of the input text to the transformer encoder. For input instance text length, we use 128 for \LfWikiSmall and 192 for \LfWikiLarge. For short-text datasets (\LfAmznSmall and \LfAmznLarge), we set it to 32. For the label text, we always use a max length of 32.
    \item bsz/GPU: the batch size per GPU for training. The global batch size is equal to $8 \times \text{bsz/GPU}$.
    \item LR: the learning rate of the optimizer.
    \item max\_steps: the maximum number of training steps in optimization.
    \item hnm\_steps: the frequency to perform hard negative mining (HNM) during training.
    \item hnm\_topk:  the number of top-$k$ predicted labels used as the source of hard negatives.
    \item $m$: the number of hard negative labels sampled for each input $x$.
\end{itemize}

\begin{table*}[!ht]
    \centering
    \caption{Training Hyper-parameters of \RAEXMC}
    \begin{tabular}{l|rrrrrrrr}
        \toprule
        Dataset & max\_len & bsz/GPU & LR & max\_steps & hnm\_steps & hnm\_topk & $m$ \\
        \midrule
            \LfAmznSmall &  32 & 896 & 3e-4 &  3K & 1K & 50 & 2\\
            \LfWikiSmall & 128 & 576 & 2e-4 &  3K & 1K & 50 & 2\\
            \LfWikiLarge & 192 & 400 & 2e-4 & 28K & 7K & 25 & 2\\
            \LfAmznLarge &  32 & 896 & 3e-4 & 40K & 8K & 50 & 2\\
        \bottomrule
    \end{tabular}
    \label{tab:trn-hyperparam}
\end{table*}

\paragraph{Inference Hyper-parameters.}
We use HNSW algorithm~\citep{malkov2018efficient} to perform approximate nearest neighbor (\ANN) search at the inference stage.
The \ANN index building procedure is described in Algorithm~\ref{alg:rae-indexing}.
To build the HNSW index, we set the maximum edge per node $M=64$ and the queue size as $efC=500$.
The inference procedure of \RAEXMC is described in Algorithm~\ref{alg:rae-inference}.
For training/inference consistency, we keep the temperature $\tau=0.04$.
We set the queue size $efS=300$ for the HNSW searcher and select top $b=200$ keys from the knowledge memory to approximate the Softmax distribution of the knowledge retriever. 
We set $\lambda=0.5$ for all datasets except for \LfWikiLarge, which has $\lambda=0.01$.
The effect of using different $\lambda$ is discussed in Appendix~\ref{sec:exp-lambda}.

\begin{algorithm}[H]
    \caption{Indexing of \RAEXMC}
    \label{alg:rae-indexing}
    \begin{minted}{python}
def IndexingRAE(f_enc, X_txt, L_txt, Y_trn, lamb=0.5):
    X_emb = f_enc(X_txt) # [N, d]
    Z_emb = f_enc(L_txt) # [L, d]
    K_emb = vstack([X_emb, Z_emv]) # [(N+L), d], the Key matrix in Eq(3)
    ann_index = ANN.train(K_emb, ...) # Build ANN index on [N+L] space
    V_mat = vstack([lamb*Y_trn, (1-lamb)*I_L] # [(N+L), L], the Value matrix
    return (indexer, V_mat)
    \end{minted}
\end{algorithm}   



\section{Additional Experiment Results}

\subsection{Analysis of Sampled Key Number} \label{sec:key_num}
In Section~\ref{sec:rae-inference}, we retrieve top $b$ keys to approximate the softmax distribution.
With greater $b$, the performance increases accordingly, but with higher inference latency as a tradeoff.
In Figure~\ref{fig:perf-vs-b}, we examine the impact of  different $b$ regarding the ratio of retrieved query number / retrieved label number and the performance.


Regarding the ratio of retrieved query number to retrieved label number, distinct trends emerge between \LfAmznLarge and \LfWikiLarge datasets. 
In \LfAmznLarge, the ratio is lower, suggesting a higher reliance on label retrieval for predictions. Moreover, in \LfAmznLarge, the ratio tends to increase with larger values of $b$, whereas in \LfWikiLarge, it initially rises and then declines.
As $b$ increases, performance across all metrics consistently improves, but with varying impacts on individual metrics.
Achieving optimal performance in @k metrics require higher values of $b$ as $k$ increases.


\begin{figure}[!ht]
    \centering
     \begin{subfigure}[b]{0.24\textwidth}
          \centering
          \includegraphics[width=\textwidth]{figures/RAE-method.003.pdf}
          \caption{LF-Amazon-1.3M}
          \label{fig:LfAmznLarge-key-ratio}
     \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/RAE-method.004.pdf}
        \caption{\LfWikiLarge}
        \label{fig:LfWikiLarge-key-ratio}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/RAE-method.005.pdf}
        \caption{LF-Amazon-1.3M}
        \label{fig:LfAmznLarge-perf-vs-topk}
    \end{subfigure}
    \hfill
     \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/RAE-method.006.pdf}
        \caption{\LfWikiLarge}
        \label{fig:LfWikiLarge-perf-vs-topk}
    \end{subfigure}
    \caption{
        Analysis of different sources of top-$b$ keys retrieved from the knowledge retriever as well as the relation to the model performance.
    }
    \label{fig:perf-vs-b}
\end{figure}




\subsection{Comparison of Different Knowledge Source for Prediction}
\label{sec:exp-lambda}

\begin{table*}[!ht]
    \centering
    \caption{Comparison of different $\lambda$.}
    \resizebox{1.0\textwidth}{!}{
    \begin{tabular}{l|rr|rr|rr|rr|rr|rr|rr|rr}
        \toprule
        Dataset & \multicolumn{2}{c}{$\lambda=0$} & \multicolumn{2}{c}{$\lambda=0.01$} & \multicolumn{2}{c}{$\lambda=0.1$} & \multicolumn{2}{c}{$\lambda=0.3$} & \multicolumn{2}{c}{$\lambda=0.5$} & \multicolumn{2}{c}{$\lambda=0.7$} & \multicolumn{2}{c}{$\lambda=0.9$} & \multicolumn{2}{c}{$\lambda=1.0$}\\
        \midrule
          & P@1 & P@5 & P@1 & P@5 & P@1 & P@5 & P@1 & P@5 & P@1 & P@5 &  P@1 & P@5 &  P@1 & P@5 &  P@1 & P@5 \\
        \midrule
         \LfAmznSmall & 42.58 & 20.57 & 42.86 & 21.35 & 44.17 & 21.86 & 44.72 & 21.98 & 45.16 & 21.95 & 45.13 & 21.90 & 44.92 & 21.82 & 44.4 & 21.68 \\
         \LfWikiSmall & 43.11 & 20.29 & 44.95 & 21.89 & 47.25 & 23.27 & 48.11 & 23.67 & 48.02 & 23.59 & 47.56 & 23.32 & 46.52 & 22.83 &44.64 & 22.11\\
         \LfWikiLarge & 84.30 & 50.39 & 86.49 & 50.67 & 86.71 & 48.15 & 86.11 & 46.19 & 85.37 & 45.12 & 84.49 & 44.29 & 83.25  & 43.50 & 81.79 & 42.90 \\
         \LfAmznLarge & 49.41 & 38.71 &  49.61 & 41.90 & 54.36 & 45.87 & 57.24 & 46.55 & 57.98 & 46.39 & 57.89 & 46.15 & 57.43 & 45.83 & 51.93 & 41.61\\
        \bottomrule
    \end{tabular}
    }
    \label{tab:lambda}
\end{table*}

In Table~\ref{tab:lambda}, we examine the performance variation of using different value of $\lambda$ to compare the impact of using various knowledge source for inference.
By gradually increasing the value of $\lambda$, the weight of $k$NN classifier increases accordingly.
Comparing $\lambda=0.0$ and $\lambda=0.01/0.1$, we can find that even though the prediction weight of the kNN classifier is very small, it can still complement the dual-encoder prediction very well, thus improving the performance.

Across all datasets excluding \LfWikiLarge, \RAEXMC exhibits insensitivity to $\lambda$ variations.
Notably, setting $\lambda$ to $0.5$ suffices to attain optimal performance across most datasets, suggesting an equitable importance of both knowledge sources in prediction.
However, when there is a distinct disparity between training and testing corpora, meticulously tuning $\lambda$ may be necessary~\citep{NIPS2017_f44ee263,khandelwal2020generalization,chang2024pefa}.


\subsection{ Significant Test}
\label{sup:sigtest}
We conduct significant tests to verify the effectiveness of \RAEXMC in Table~\ref{tab:main_results}.
In particular, we first compute the instance-wise metrics (i.e., P@1, P@5 and R@100) for the 1st and 2nd place method in Table~\ref{tab:main_results}, namely \RAEXMC and \DEXML, respectively.
Then we perform the paired t-test between \RAEXMC and \DEXML for each of these metrics.
As shown in Table~\ref{tab:pvalue}, the results are significant on all the metrics, except P@1 of \LfAmznLarge.
%\peter{Why pvalue of R@100 on \LfWikiLarge is so small, given the numbers in Table~\ref{tab:main_results} are actually very close?}
\begin{table*}[!ht]
    \centering
    \caption{P-values of significant test between \RAEXMC and \DEXML on instance-wise metrics.}
    \begin{tabular}{l|ccc}
        \toprule
        Dataset & P@1 & P@5 & R@100 \\
        \midrule
            \LfAmznSmall & $9.5 \times 10^{-51}$ & $1.7 \times 10^{-67}$ & $4.9 \times 10^{-80}$\\
            \LfWikiSmall & $2.5 \times 10^{-42}$ & $2.3 \times 10^{-152}$ & $\approx 0.0$ \\
            \LfWikiLarge & $5.7 \times 10^{-59}$& $1.1 \times 10^{-10}$ & $4.4 \times 10^{-23}$\\
            \LfAmznLarge & 0.51 & $2.24 \times 10^{-170}$ &  $\approx0.0$ \\
        \bottomrule
    \end{tabular}
    \label{tab:pvalue}
\end{table*}

% \subsection{Inference latency} \label{sup:latency}
% In Table~\ref{tab:latency}, we study the inference latency of \RAEXMC under two inference setups, the real-time mode ($\text{bsz/GPU}=1$) and the batch prediction mode ($\text{bsz/GPU}=256$).
% As discussed in Section~\ref{sec:rae-inference}, the time complexity of \RAEXMC with $\lambda \in (0, 1)$ involves an extra sparse matrix vector multiplication (i.e., $\gO(b \times \bar{L})$) compared to the inference of \DE models (\RAEXMC with $\lambda=1.0$).
% For the real-time mode ($\text{bsz/GPU}=1$), the inference of $\lambda \in (0, 1)$ has a higher overhead compared to $\lambda=1$, ranging from 1.5x for \LfWikiSmall and 6.4x for \LfAmznLarge.
% This is due to the average number of labels per instance $\bar{L}$ is smaller for \LfWikiSmall ($\bar{L}=2.1$) and significantly larger for \LfAmznLarge ($\bar{L}=22.2$).
% For the batch prediction mode, the inference overhead become less severe, as we can leverage multiple CPUs for parallel compute the Sparse matrix vector multiplications.
% In summary, the real-time inference latency of \RAEXMC with $\lambda \in (0,1)$ is generally smaller than $100$ ms/q, which is still applicable to real-time \XMC applications.

% % Please add the following required packages to your document preamble:
% % \usepackage{multirow}
% \begin{table}[!h]
%     \centering
%     \caption{
%         Inference Latency of our \RAEXMC method on three LF-\XMC datasets,
%         which includes both the query encoding and the \ANN Search process.
%         Inference latency is measured by millisecond/query (ms/q) using a single A100 GPU
%         on the AWS p4d.24xlarge instance (w/ up to 96 CPUs).
%     }
%     \resizebox{0.8\textwidth}{!}{%
%     \begin{tabular}{l|rr|rrr}
%         \toprule
%                 & \multicolumn{5}{c}{Inference Latency (ms/q)} \\
%         Dataset & \multicolumn{1}{c}{bsz/gpu} & \multicolumn{1}{c}{\#CPU} & \multicolumn{1}{c}{$\lambda \in (0,1)$} & \multicolumn{1}{c}{$\lambda=1$} & Overhead \\
%         \midrule
%         \multirow{3}{*}{\LfWikiSmall}   &   1 &  1 & 24.30 & 16.51 & 1.5 \\
%                                         & 256 & 96 &  0.67 &  0.56 & 1.2 \\
%         \midrule
%         \multirow{3}{*}{\LfWikiLarge}   &   1 &  1 & 30.23 & 14.40 & 2.1 \\
%                                         & 256 & 96 &  0.76 &  0.52 & 1.5 \\
%         \midrule
%         \multirow{3}{*}{\LfAmznLarge}   &   1 &  1 & 82.13 & 12.82 & 6.4 \\
%                                         & 256 & 96 &  0.72 &  0.24 & 3.0 \\
%         \bottomrule
%     \end{tabular}
%     \label{tab:latency}
%     }%
% \end{table}