\section{Experiments}


\subsection{Datasets and Settings}
We employ seven commonly-used LLIE benchmark datasets for evaluation, including LOLv1 \cite{RetinexNet}, LOLv2 \cite{LOLv2}, DICM \cite{DICM}, LIME \cite{LIME}, MEF \cite{MEF}, NPE \cite{NPE}, and VV \cite{VV}. 
We also conduct further experiments on two extreme datasets, SICE \cite{SICE} (containing mix and grad test sets \cite{SICE-Mix}) and SID (Sony-Total-Dark) \cite{SID}. 

\textbf{LOL.} The LOL dataset has v1 \cite{RetinexNet} and v2 \cite{LOLv2} versions. Compared to LOL-v1 that contains both real and synthetic data, LOL-v2 is divided into real and synthetic subsets. 
For LOLv1 and LOLv2-Real, we crop the training images into $400\times400$ patches and train CIDNet for 1,500 epochs with a batch size of 8. For LOLv2-Synthetic, we set the batch size to 1 and train CIDNet for 500 epochs without cropping.

\textbf{SICE.} The original SICE dataset \cite{SICE} contains 589 low-light and overexposed images, with the training, validation, and test sets divided into three groups according to 7:1:2. CIDNet is optimized using $160\times160$ cropped images from the training set for 1,000 epochs with a batch size of 10 and tested on the datasets SICE-Mix and SICE-Grad \cite{SICE-Mix}. 
% We crop the original SICE image by $160\times160$ and train CIDNet.

\textbf{Sony-Total-Dark.} This dataset is a customized version of a SID subset \cite{SID}.
% , which is adopted for evaluation. 
To make this dataset more challenging, we convert the raw format images to sRGB images with \textit{no gamma correction}, resulting in images with extreme darkness. We crop the training images into $256\times256$ patches and train CIDNet for 1,000 epochs with a batch size of 4.

\textbf{Experiment Settings.} We implement our CIDNet by PyTorch. The model is trained with the Adam \cite{Adam} optimizer (\textit{$\beta_{1}$} = 0.9 and \textit{$\beta_{2}$} = 0.999) by using a single NIVIDA 2080Ti or 3090 GPU. The learning rate is initially set to $1 \times 10^{-4}$ and then steadily decreased to $1 \times 10^{-7}$ by the cosine annealing scheme \cite{sgdr} during the training process. 
% During inference, we pad the input images to be a multiplier of $8\times8$ using reflect padding on both sides. After that, we crop the padded image back to its original size. 
% Since there may exist outliers in the output of CIDNet, we apply a simple clip operation to the HVI space, \ie, $\mathbb{D}=\{p=(h,v,i)|~h^2 + v^2 \le \sin^\frac{2}{k}(\frac{\pi i}{2}),~0 \le i \le 1\}$, where $p=(h,v,i)$ is the three-dimensional coordinates in the HVI color space, and $k$ denotes the density-$k$ in Eq. \ref{eq:2}.

\textbf{Evaluation Metrics. }For the \textit{paired} datasets, we adopt the Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity (SSIM) \cite{SSIM} as the distortion metrics. 
To comprehensively evaluate the perceptual quality of the restored images, we report Learned Perceptual Image Patch Similarity (LPIPS) \cite{LPIPS} with AlexNet \cite{Alex} as the reference.
% as a perceptual metric. 
For the \textit{unpaired} datasets, we evaluate single recovered images using BRISQUE \cite{BRISQUE} and NIQE \cite{NIQE} perceptually.


\begin{table*}
    \centering
    \renewcommand{\arraystretch}{1.}
    \caption{Quantitative results of PSNR/SSIM$\uparrow$ and LPIPS$\downarrow$ on the LOL (v1 and v2) datasets. Due to the limited number of test set in LOLv1, we use GT mean method during testing to minimize errors. This approach will be explained in supplementary. The FLOPs is tested on a single $256\times256$ image. The best performance is in \textcolor{red}{red} color and the second best is in \textcolor{cyan}{cyan} color.}
\vspace{-0.2cm}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{c|c|cc|ccc|ccc|ccc}
        \Xhline{1.5pt}
        \multirow{2}{*}{\textbf{Methods}}&\multirow{2}{*}{\textbf{Color Model}}&\multicolumn{2}{c|}{\textbf{Complexity}}&\multicolumn{3}{c|}{\textbf{LOLv1}} & \multicolumn{3}{c|}{\textbf{LOLv2-Real}} & \multicolumn{3}{c}{\textbf{LOLv2-Synthetic}}\\
        
        ~&~&Params/M&FLOPs/G&PSNR$\uparrow$&SSIM$\uparrow$&LPIPS$\downarrow$&PSNR$\uparrow$&SSIM$\uparrow$&LPIPS$\downarrow$&PSNR$\uparrow$&SSIM$\uparrow$&LPIPS$\downarrow$\\
        \hline
        RetinexNet \cite{RetinexNet}& Retinex& 0.84& 584.47 &	18.915 &	0.427 & 0.470&	16.097 &	0.401 	&0.543 &	17.137 &	0.762 &	0.255
\\
        KinD \cite{KinD}& Retinex & 8.02& 34.99 &23.018& 	0.843& 0.156&	17.544& 	0.669 &	0.375	&	18.320 	&0.796 &0.252
\\
         ZeroDCE \cite{Zero-DCE}& RGB&0.075&4.83& 	21.880 &	0.640&0.335& 16.059 &	0.580 & 0.313&	17.712& 	0.815 &	0.169
\\

%         DRBN \cite{DRBN}& RGB& 5.47 & 48.61 & 19.550 & 0.746 & 0.155&20.290 & 0.831 & 0.147& 23.220 &0.825 & 0.174
         
% \\

         RUAS \cite{RUAS}& Retinex & 0.003& 0.83 &	18.654& 	0.518 &	0.270&15.326 &	0.488 &	0.310&13.765 	&0.638 	&0.305
\\
        LLFlow \cite{LLFlow}& RGB&17.42 & 358.4 &24.998 &	0.871&	0.117&17.433 &	0.831 & 0.176&	24.807 &	0.919 &	0.067
\\
         EnlightenGAN \cite{EnGAN}& RGB& 114.35 & 61.01 &  20.003 & 0.691&0.317 &18.230 & 0.617 &0.309 & 16.570& 0.734&0.220
\\

         SNR-Aware \cite{SNR-Aware}& SNR+RGB& 4.01 & 26.35 &26.716 	&0.851 	&0.152 &21.480 &	\textcolor{cyan}{0.849} &0.163	&	24.140 &	0.928 & 0.056
\\
        Bread \cite{Bread}& YCbCr&2.02 & 19.85 &25.299 	&0.847 	&0.155 &20.830 &	0.847 &0.174	&	17.630 &	0.919 & 0.091
\\
        PairLIE \cite{PairLIE}& Retinex& 0.33 & 20.81 &	23.526 &	0.755& 0.248&	19.885 &	0.778 & 0.317&	19.074	&0.794&	0.230
\\
        LLFormer \cite{LLFormer}& RGB& 24.55&22.52&25.758&0.823&0.167&20.056&0.792& 0.211&24.038&0.909&0.066
\\
         RetinexFormer \cite{RetinexFormer}& Retinex& 1.53 & 15.85 & 	27.140 & 	0.850  & 0.129&	\textcolor{cyan}{22.794}  &	0.840  & 0.171&	\color{cyan}{25.670}  &	\textcolor{cyan}{0.930}  &	0.059
\\
        GSAD \cite{hou2024global}& RGB& 17.36 & 442.02 & 	\color{cyan}{27.605}& 	\color{cyan}{0.876}  & \color{cyan}{0.092}&	20.153  &	0.846  & \color{cyan}{0.113}&	24.472 &	0.929  &	\color{cyan}{0.051}
\\
       QuadPrior \cite{wang2024zero}& Kubelka-Munk& 1252.75 & 1103.20 & 	22.849& 	0.800  & 0.201&	20.592  &	0.811  & 0.202&	16.108 &	0.758 &	0.114
\\
         % \Xhline{1.5pt}
        \textbf{CIDNet(Ours)}& HVI& 1.88 & 7.57 & 	\color{red}{28.201} &	\color{red}{0.889} &\color{red}{0.079}	&\color{red}{24.111}& 	\color{red}{0.871} &\color{red}{0.108}  &	\textcolor{red}{25.705} &	\color{red}{0.942} & \color{red}{0.045}
\\
         \Xhline{1.5pt}
    \end{tabular}
    }
    \label{tab:table-LOL}
\end{table*}


\begin{figure*}[!t]
\centering
\begin{minipage}[t]{0.12\linewidth}
    \centering
    \vspace{3pt}
    \centerline{\includegraphics[width=\textwidth]{img/v1/c-input.pdf}}
    \centerline{\includegraphics[width=\textwidth]{img/v2/c-input.pdf}}
    \centerline{\small Input}
\end{minipage}
\begin{minipage}[t]{0.12\linewidth}
    \centering
    \vspace{3pt}
    \centerline{\includegraphics[width=\textwidth]{img/v1/c-RUAS.pdf}}
    \centerline{\includegraphics[width=\textwidth]{img/v2/c-RUAS.pdf}}
    \centerline{\small RUAS \cite{RUAS}}
\end{minipage}
\begin{minipage}[t]{0.12\linewidth}
    \centering
    \vspace{3pt}
    \centerline{\includegraphics[width=\textwidth]{img/v1/c-LLflow.pdf}}
    \centerline{\includegraphics[width=\textwidth]{img/v2/c-LLflow.pdf}}
    \centerline{\small LLFlow \cite{LLFlow}}
\end{minipage}
\begin{minipage}[t]{0.12\linewidth}
    \centering
    \vspace{3pt}
    \centerline{\includegraphics[width=\textwidth]{img/v1/c-SNR.pdf}}
    \centerline{\includegraphics[width=\textwidth]{img/v2/c-SNR.pdf}}
    \centerline{\small SNRNet \cite{SNR-Aware}}
\end{minipage}
\begin{minipage}[t]{0.12\linewidth}
    \centering
    \vspace{3pt}
    \centerline{\includegraphics[width=\textwidth]{img/v1/c-RetinexFormer.pdf}}
    \centerline{\includegraphics[width=\textwidth]{img/v2/c-RetinexFormer.pdf}}
    \centerline{\small RetFormer \cite{RetinexFormer}}
\end{minipage}
\begin{minipage}[t]{0.12\linewidth}
    \centering
    \vspace{3pt}
    \centerline{\includegraphics[width=\textwidth]{img/v1/c-GSAD.pdf}}
    \centerline{\includegraphics[width=\textwidth]{img/v2/c-GSAD.pdf}}
    \centerline{\small GSAD \cite{GSAD}}
\end{minipage}
\begin{minipage}[t]{0.12\linewidth}
    \centering
    \vspace{3pt}
    \centerline{\includegraphics[width=\textwidth]{img/v1/c-Ours.pdf}}
    \centerline{\includegraphics[width=\textwidth]{img/v2/c-Ours.pdf}}
    \centerline{\small CIDNet(Ours)}
\end{minipage}
\begin{minipage}[t]{0.12\linewidth}
    \centering
    \vspace{3pt}
    \centerline{\includegraphics[width=\textwidth]{img/v1/c-gt.pdf}}
    \centerline{\includegraphics[width=\textwidth]{img/v2/c-gt.pdf}}
    \centerline{\small GroundTruth}
\end{minipage}
\vspace{-0.2cm}
 \caption{Visual comparison of the enhanced images yielded by different SOTA methods on LOLv1 (top row) and LOLv2 (bottom row).}
 \label{fig:LOL}
\end{figure*}


 \begin{table}
      \centering
        \renewcommand{\arraystretch}{1.}
        \caption{Quantitative result on SICE, Sont-Total-Dark, and the five unpaired datasets (DICM \cite{DICM}, LIME \cite{LIME}, MEF \cite{MEF}, NPE \cite{NPE}, and VV \cite{VV}). The top-ranking score is in \textcolor{red}{red}.}
\vspace{-1.8mm}
        \resizebox{\linewidth}{!}{
        \begin{tabular}{c|cc|cc|cc}
        \Xhline{1.5pt}
        \multirow{2}{*}{\textbf{Methods}}&
         \multicolumn{2}{c|}{\textbf{SICE}}& 
         \multicolumn{2}{c|}{\textbf{Sony-Total-Dark}} & 
         \multicolumn{2}{c}{\textbf{Unpaired}}\\

        ~&
            PSNR$\uparrow$&	SSIM$\uparrow$& 
            PSNR$\uparrow$&	SSIM$\uparrow$&
            BRIS$\downarrow$& NIQE$\downarrow$\\
            
            \hline

            RetinexNet \cite{RetinexNet}&
            12.424& 0.613&
            15.695& 0.395& 
            \color{red}{23.286}& 4.558
\\
            ZeroDCE \cite{Zero-DCE}&
            12.452& 0.639& 
            14.087&	0.090& 
            26.343&	4.763
\\
            URetinexNet \cite{URetinexNet}&
            10.899& 0.605& 
            15.519& 0.323&
            26.359& 3.829
\\
            RUAS \cite{RUAS}&
            8.656& 	0.494& 
            12.622& 0.081&  
            36.372& 4.800 
\\
            LLFlow \cite{LLFlow}&
            12.737&	0.617& 
            16.226&	0.367& 
            28.087&	4.221 
\\
%             PairLIE&
%             12.668&	0.579&
%             22.375& 0.654&
%             29.84& 0.648& 	0.471 
% \\

             % \Xhline{1.5pt}
             \textbf{CIDNet (Ours)}&
            \color{red}{13.435}&	\color{red}{0.642}&
            \color{red}{22.904}&	\color{red}{0.676}&
            23.521&	\color{red}{3.523}\\
            \Xhline{1.5pt}
        \end{tabular}
        }
        \label{tab:SID}
\end{table}


\subsection{Main Results}
\textbf{Results on LOL Datasets.} 
In Tab. \ref{tab:table-LOL}, it can be found that our method is optimal on all metrics for both LOLv1 and LOLv2 datasets with 1.88M parameters and 7.57 GFLOPs. 
We outperform the best RGB-based method GSAD (diffusion)
% , our method achieves results closer to the GroundTruth 
in terms of all PSNR, SSIM, and LPIPS metrics, while utilizing only 10.8\% parameters of GSAD.
Compared to RetinexFormer, a SOTA method based on Retinex theory, CIDNet delivers higher image quality while reducing computational cost by 8.28 GFLOPs.
Subjective results are illustrated in Fig. \ref{fig:LOL}, which demonstrates that our method not only more accurately recovers multi-color regions compared to GroundTruth but also achieves stable brightness enhancement, thanks to the HVI color space. 
More visualization comparison can be found in supplementary materials.

\textbf{Results on SICE and Sony-total-Dark.} 
To validate the performance on large-scale datasets, we evaluate CIDNet on SICE (including Mix and Grad) and SID-Total-Dark. 
% Given the difficulty of assessing the enhancement based on visual perception in these two datasets, we opted to rely solely on quantitative evaluation.
The results are presented in Table \ref{tab:SID}, where it is clear that CIDNet is the best performer in both PSNR and SSIM metrics on the two datasets. 
Notably, on Sony-Total-Dark, our model surpasses the second-best method by 6.678 dB in PSNR. 
This improvement is due to the extreme darkness of the dataset images, which substantially increases the difficulty of distinguishing details from noise. 
However, CIDNet leverages the intensity collapse function $\mathbf{C}_k$ to effectively maintain an optimal signal-to-noise ratio during training, enabling better detail recovery.



\textbf{Results on Unpaired Datasets.} 
We evaluate the effectiveness of models trained on LOLv1 or LOLv2-Syn using various methods, and report their performance using BRISQUE and NIQE metrics in Tab. \ref{tab:SID}. Our method exhibits a substantial improvement in the NIQE metric compared to other approaches.
% Comparing Tab. \ref{tab:SID} with 
As shown in Fig. \ref{fig:unpaired}, while CIDNet does not outperform RetinexNet in the BRISQUE metric in Tab. \ref{tab:SID}, its recovered perceptual results are closer to realistic appearances than RetinexNet. 
This may be attributed to the fact that building upon the HSV space, the HVI color space is derived from real-world perceptual models \cite{gevers2012color}.


 \begin{figure*}[!t]
    \centering
    \includegraphics[width=1\linewidth]{img/comp-patch3.pdf}
    \vspace{-0.55cm}
    \caption{Visual comparison on the five unpaired datasets. Follow RetinexFormer \cite{RetinexFormer}, we select one image in each dataset to compare our method with the other methods. More visual comparison can be found in the supplementary materials.}
    \label{fig:unpaired}
\end{figure*}



\begin{table*}[!t]
    \centering
    \renewcommand{\arraystretch}{1.}
    \caption{Results of applying HVI transformation as a plug-in to various LLIE methods
    % . Embedding HVI transform into other methods and training/testing 
    on LOLv2-Real. 
    % Since the frequency stage in the FourLLIE \cite{wang2023fourllie} method utilizes characteristics specific to the sRGB color space, we applied the HVI transformation only in the Spatial Stage. 
    \textcolor{red}{Values} in brackets represent the absolute improved performance gain. The best PSNR/SSIM$\uparrow$, LPIPS$\downarrow$, and inference time$\downarrow$ are in \textbf{bolded}.} 
\vspace{-1.8mm}
    % The lowest inference time on a RTX3090 GPU with $256\times256$ image is also in \textbf{bolded}.}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{cccccccc}
    \Xhline{1.5pt}
         \cellcolor{gray!10}Methods& 
         \cellcolor{gray!10}FourLLIE \cite{wang2023fourllie}&
         \cellcolor{gray!10}LEDNet \cite{LEDNet}&
         \cellcolor{gray!10}SNR-Aware \cite{SNR-Aware}& 
         \cellcolor{gray!10}LLFormer \cite{LLFormer}& 
         \cellcolor{gray!10}GSAD \cite{GSAD}& 
         \cellcolor{gray!10}DiffLight \cite{feng2024difflight}&
         \cellcolor{gray!10}\textbf{CIDNet}\\
    \Xhline{1.5pt}
         PSNR$\uparrow$&
         22.730(\textcolor{red}{+0.381})&
         23.394(\textcolor{red}{+3.456})&
         22.251(\textcolor{red}{+0.771})&
         22.671(\textcolor{red}{+2.615})&		
         23.715(\textcolor{red}{+3.562})&	
         23.969(\textcolor{red}{+1.364})&
         \textbf{24.111}
\\
         SSIM$\uparrow$&
         0.856(\textcolor{red}{+0.009})&
         0.837(\textcolor{red}{+0.010})&
         0.840(-0.009)&	
         0.852(\textcolor{red}{+0.060})&		
         \textbf{0.876}(\textcolor{red}{+0.030})&	
         0.859(\textcolor{red}{+0.003})&
         0.871

\\
        LPIPS$\downarrow$&
        0.125(+0.011)&
        0.115(\textcolor{red}{-0.005})&	
        0.117(\textcolor{red}{-0.054})&	
        0.117(\textcolor{red}{-0.094})&	
        \textbf{0.103}(\textcolor{red}{-0.010})&	
        0.109(\textcolor{red}{-0.012})&
        0.108
\\
        GPU Time/s$\downarrow$&
        0.075&
        0.054& 	
        0.070& 
        0.139& 	
        0.315& 
        0.578&
        \textbf{0.053}
\\
%          CPU Time/s$\downarrow$&
%          &
%          &
%          0.846& 	
%          1.751&
%          & 
%          & 	 
%         \textbf{0.416}
% \\
        Model Type&
        CNN&
        CNN&
        Transformer&
        Transformer&		
        Diffusion&
        CNN+Diffusion&
        Transformer
\\
    \Xhline{1.5pt}
    \end{tabular}
    }
    \label{tab:HVI}
\end{table*}



\begin{table}[!t]
\centering
\captionof{table}{Model ablation. Dual denotes the dual-branch network. }
% The final version consists of dual-branch, cross attention, and both sRGB and HVI losses.}
\label{tab:ablation}
\vspace{-1.8mm}
\renewcommand{\arraystretch}{1.}
\resizebox{\linewidth}{!}{
    \begin{tabular}{ll|ccc}
    \Xhline{1.5pt}
         \multicolumn{2}{l|}{\cellcolor{gray!10}\textbf{Metrics}}&	\cellcolor{gray!10}PSNR$\uparrow$&	\cellcolor{gray!10}SSIM$\uparrow$&\cellcolor{gray!10}LPIPS$\downarrow$
\\
    \Xhline{1.5pt}
         \multirow{4}{*}{\textbf{Color Space}}&	sRGB	&20.062& 	0.825& 0.137
\\
         ~&HSV		&21.349 & 0.801	& 0.167
\\
         ~&HVI (w/ Polarization Only)& 21.558 &0.821 & 0.149
\\
         ~&HVI (w/ $\mathbf{C}_k$ Only) &  21.536  & 0.825 & 0.179
\\
\midrule
         \multirow{3}{*}{\textbf{Structure}}&UNet Baseline \cite{petit2021unet}&  19.306  &  0.778 & 	0.222
\\
         ~&SelfAttn \cite{Restormer}  &22.313&	0.835& 0.126
\\
         ~&Dual+SelfAttn \cite{Restormer}&23.159& 	0.856&   0.116
\\
\midrule
        \multirow{2}{*}{\textbf{Loss}}&HVI Only&23.221& 0.854&0.132
\\
        ~&sRGB Only&23.319& 0.857& 0.123
\\
\midrule
        \multicolumn{2}{l|}{\textbf{Full Model (HVI-CIDNet)}}&24.111& 0.871&0.108
\\

    \Xhline{1.5pt}
    \end{tabular}
    }
\end{table}



\textbf{Generalizing HVI to Other LLIE Models.}
To verify the effectiveness of the HVI color space, we further evaluate its performance when it is used with different LLIE models. In particular, HVIT, together with its inverse mapping PHVIT, is used as a plug-and-play module into six SOTA methods that use sRGB images as input and are independent of specific color space characteristics. 
The results are reported in Tab. \ref{tab:HVI}. It is clear that transforming to the HVI color space improves PSNR, SSIM, and LPIPS metrics across various methods compared to the results in the sRGB color space. 
Notably, the GSAD method demonstrates the most significant improvement, with a PSNR increase of 3.562 dB.
This demonstrates not only the generalizability of HVI to various sRGB-based methods but also its general effectiveness as a color space for the LLIE task. 
% More visual comparison can be found in supplementary.

For the inference time results in Tab. \ref{tab:HVI}, it is evident that the diffusion-based methods require longer GPU time but achieve better enhancement results. 
In contrast, CIDNet shows the most efficient inference while achieving the highest PSNR and the second-best SSIM and LPIPS scores. 
This highlights the strong ability of CIDNet in balancing the efficiency and effectiveness within the HVI color space.
    
\subsection{Ablation Study}
We validate our HVI color space and the key modules in CIDNet using both quantitative (Tab. \ref{tab:ablation}) and qualitative results (Figs. \ref{fig:hvi2} and \ref{fig:struct}). The experiments are all performed on LOLv2-Real for fast convergence and stable performance.

% 需要视觉对比图
\textbf{HVI Color Space.} 
It can be seen in Tab. \ref{tab:ablation} that the enhancement in the sRGB color space leads to chromatic aberration and luminance bias, as demonstrated by the difference between Figs. \ref{fig:hvi2}(b) and (g). 
Compared to sRGB, using HSV yields images aligned more closely with the GroundTruth in both luminance and color due to its effectiveness on decoupling brightness from color, which can be observed the enhancement from Fig. \ref{fig:hvi2}(b) to Fig. \ref{fig:hvi2}(c).  
This results in improved PSNR and LPIPS in Tab. \ref{tab:ablation}. However, it significantly introduces more noise due to the red discontinuity in HSV, leading to the noisy black spots in the red regions in Fig. \ref{fig:hvi2}(c) and the degraded SSIM in Tab. \ref{tab:ablation}. 
Using the polarization or the $\mathbf{C}_k$ solely in the HVI space can lead to similar image quality in PSNR and SSIM compared to those in the HSV color space. Qualitatively,
as shown in Fig. \ref{fig:hvi2}(d), using polarization only helps cluster similar red tones, avoiding the red discontinuity.
% thereby achieving higher visual quality in the output.
% However, in Tab. \ref{tab:ablation}, the LPIPS score after removing the polarization is worse than that of both HSV and the version without $\mathbf{C}_k$.
% This outcome is due to the increased Euclidean distances between similar red tones after removing the polar coordinate transformation, which causes dot-like artifacts in red regions of the output image. 
% Without the $\mathbf{C}_k$ formula, the abnormal expansion of similar red areas prevents optimal compression of low-light regions in the HVI space, resulting in color shifts, as Fig. \ref{fig:hvi2}(d).
% The purpose of polarization is to cluster similar red tones together. 
% Without this operation, 
Relying solely on the intensity function $\mathbf{C}_k$ helps adjust the brightness, but leads to confusion between red and other colors.
% , making it difficult for CIDNet to accurately distinguish color information. 
Consequently, dot-like artifacts appear not only in the red regions but also color shifts in other areas, as shown in Fig. \ref{fig:hvi2}(e).
% As shown in Fig. \ref{fig:hvi2} (c). 
% After applying polarization, this issue is visibly mitigated, resulting in an increase of 0.2 in SSIM.
% As Fig. \ref{fig:hvi2} (d) and Tab. \ref{tab:ablation}, in images trained without polarization, PSNR and SSIM scores are higher than HSV, yet the visual quality deteriorates (\ie, low LPIPS), leading to amplified noise and more artifacts and color shifts. 
% This problem arises because the $\mathbf{C}_k$ function compresses Euclidean distances between disparate colors, hindering the network’s ability to differentiate noise from the image details. 
These issues are all effectively mitigated when the polarization and the $\mathbf{C}_k$ function are applied together, as shown by consistent improvement of the Full Model in all three metrics in Tab. \ref{tab:ablation} and the better image enhancement in Fig. \ref{fig:hvi2}(f).
% to achieve better LLIE results.





\begin{figure*}[htp]
\centering
\begin{minipage}[t]{0.136\linewidth}
    \centering
        \vspace{1pt}
        \centerline{\includegraphics[width=\textwidth]{img/ab-color/00700_cut/1.png}}
        \centerline{\includegraphics[width=\textwidth]{img/ab-color/00758_cut/1.png}}
        \centerline{\includegraphics[width=\textwidth]{img/ab-color/space_cut/1.png}}
    \centerline{\small (a) Input}
\end{minipage}
\hfill
\begin{minipage}[t]{0.136\linewidth}
    \centering
        \vspace{1pt}
        \centerline{\includegraphics[width=\textwidth]{img/ab-color/00700_cut/2.png}}
        \centerline{\includegraphics[width=\textwidth]{img/ab-color/00758_cut/2.png}}
        \centerline{\includegraphics[width=\textwidth]{img/ab-color/space_cut/2.png}}
    \centerline{\small(b) sRGB}
\end{minipage}
\hfill
\begin{minipage}[t]{0.136\linewidth}
    \centering
        \vspace{1pt}
        \centerline{\includegraphics[width=\textwidth]{img/ab-color/00700_cut/3.png}}
        \centerline{\includegraphics[width=\textwidth]{img/ab-color/00758_cut/3.png}}
        \centerline{\includegraphics[width=\textwidth]{img/ab-color/space_cut/3.png}}
    \centerline{\small(c) HSV}
\end{minipage}
\hfill
\begin{minipage}[t]{0.136\linewidth}
    \centering
        \vspace{1pt}
        \centerline{\includegraphics[width=\textwidth]{img/ab-color/00700_cut/5.png}}
        \centerline{\includegraphics[width=\textwidth]{img/ab-color/00758_cut/5.png}}
        \centerline{\includegraphics[width=\textwidth]{img/ab-color/space_cut/5.png}}
    \centerline{\small(d) w/ Polarization}
\end{minipage}
\hfill
\begin{minipage}[t]{0.136\linewidth}
    \centering
        \vspace{1pt}
        \centerline{\includegraphics[width=\textwidth]{img/ab-color/00700_cut/4.png}}
        \centerline{\includegraphics[width=\textwidth]{img/ab-color/00758_cut/4.png}}
        \centerline{\includegraphics[width=\textwidth]{img/ab-color/space_cut/4.png}}
    \centerline{\small(e) w/ $\mathbf{C}_k$}
\end{minipage}
\hfill
\begin{minipage}[t]{0.136\linewidth}
    \centering
        \vspace{1pt}
        \centerline{\includegraphics[width=\textwidth]{img/ab-color/00700_cut/6.png}}
        \centerline{\includegraphics[width=\textwidth]{img/ab-color/00758_cut/6.png}}
        \centerline{\includegraphics[width=\textwidth]{img/ab-color/space_cut/6.png}}
    \centerline{\small(f) HVI}
\end{minipage}
\hfill
\begin{minipage}[t]{0.136\linewidth}
    \centering
        \vspace{1pt}
        \centerline{\includegraphics[width=\textwidth]{img/ab-color/00700_cut/7.png}}
        \centerline{\includegraphics[width=\textwidth]{img/ab-color/00758_cut/7.png}}
        \centerline{\includegraphics[width=\textwidth]{img/ab-color/space_cut/7.png}}
    \centerline{\small(g) GroundTruth}
\end{minipage}
\vspace{-0.18cm}
\caption{Top and middle rows are ablation results on LOLv2-Real for five different color spaces used by CIDNet. The bottom row provides a visual comparison by mapping the pixel values of the results to sRGB.
Note that due to the dual-branch and the cross-attention mechanism are specifically designed for HVI, we only use UNet \cite{petit2021unet} with self attentions \cite{Restormer} for a fair comparison. }
% Please see appendix for more comparative results.}
\label{fig:hvi2}
\end{figure*}
% 需要视觉对比图
\textbf{Dual-branch Network Structure.} 
In Tab. \ref{tab:ablation}, adding self-attention to the baseline noticeably improves all three metrics, indicating that transformer-based models hold potential for application in the HVI color space. 
We then modified the architecture from a single-branch to a dual-branch structure without cross-attention, resulting in a PSNR increase of 0.846 dB, while SSIM and LPIPS showed minimal change. 
Further incorporating the cross-attention into the I-branch and HV-branch (Full Model) obtains the best color restoration, light enhancement, and optimal metric performance, as shown in Tab. \ref{tab:ablation}. This can also be observed in Fig. \ref{fig:struct}.

\textbf{Loss Function.} 
% This experiment was designed to evaluate the role and effectiveness of the loss function in the HVI color space. 
As shown in Tab. \ref{tab:ablation}, 
% using only the HVI loss results in lower performance across all three metrics compared to using sRGB loss alone. 
compared to using both HVI and sRGB losses, relying solely on the HVI loss lacks pixel-level spatial consistency constraints, leading to a loss of structural detail in the image and thus lower performance across the three metrics, especially in the LPIPS metric. 
On the other hand, using only sRGB loss is focused on pixel-space enhancement, neglecting the low-light probability distribution in the HVI color space, resulting in undesired color imbalance. 
% Thus, utilizing both losses achieves better outcomes. 
% Additional visual comparison results are provided in the appendix.
% The declined performance is fundamentally related to the incomplete convergence of $k$. Effective convergence of $k$ parameter requires dual constraints: the HVI loss constrains the color distribution within the HVI space, aligning it more closely with the probabilistic distribution of sRGB, while the sRGB loss enforces pixel-level texture distribution in image space. 
% Therefore, using both losses together achieves best performance.
% Relying solely on sRGB loss does not effectively supervise the parameter $k$ in the HVI color space, resulting in suboptimal compression by the $\mathbf{C}_k$ formula and causing some detail distortion in the output image.
% We obtain consistently the best performance when both losses are used. 

% The discrepancy between the individual loss and the combined one arises because HVI loss is computed prior to sRGB loss, preventing the $k$ parameter from converging to an optimal value. 
% As digital images are predominantly stored in the sRGB color space, parameter $k$ is required to convert HVI to sRGB \cite{castleman1996digital}. 
% However, if storing images directly in the HVI color space becomes feasible in the future, this conversion step could be bypassed, thereby eliminating the need for sRGB loss.

% \begin{table}[t]
%     \centering
%     \renewcommand{\arraystretch}{1.2}
%     \caption{Ablation of three different types of inputs in Enhancement Network. Each distinct convolution layers will extract and generate corresponding intensity features (I-Feature) and HV-features from different input maps. The type of input is indicated in the columns I-Feature and HV-Feature.}
%     \resizebox{\linewidth}{!}{
%     \begin{tabular}{l|cc|ccc}
%     \Xhline{1.5pt}
%          \cellcolor{gray!10}Types&\cellcolor{gray!10}I-Features&\cellcolor{gray!10}HV-Features&\cellcolor{gray!10}PSNR$\uparrow$&	\cellcolor{gray!10}SSIM$\uparrow$&	\cellcolor{gray!10}LPIPS$\downarrow$
% \\
% \Xhline{1.5pt}
%          (1) Half-HVIT& intensity & HVI-Map&24.111&0.871& 	0.108 
% \\
%          (2) Separate-HVIT& intensity & HV-Map&23.734& 	0.857& 	0.121
% \\
%          (3) Full-HVIT& HVI-Map & HVI-Map&23.814& 	0.859& 	0.117 
% \\

% \Xhline{1.5pt}
%     \end{tabular}
%     }
%     \label{tab:ablation2}
% \end{table}

\begin{figure}
\centering
\begin{minipage}[t]{0.237\linewidth}
    \centering
        \vspace{1pt}
        \centerline{\includegraphics[width=\textwidth]{img/ab-structure/00690_cut/1.png}}
        \centerline{\includegraphics[width=\textwidth]{img/ab-structure/00722_cut/1.png}}
        \centerline{\includegraphics[width=\textwidth]{img/ab-structure/00787_cut/1.png}}
    \centerline{\small SelfAttn}
\end{minipage}
\hfill
\begin{minipage}[t]{0.237\linewidth}
    \centering
        \vspace{1pt}
        \centerline{\includegraphics[width=\textwidth]{img/ab-structure/00690_cut/2.png}}
        \centerline{\includegraphics[width=\textwidth]{img/ab-structure/00722_cut/2.png}}
        \centerline{\includegraphics[width=\textwidth]{img/ab-structure/00787_cut/2.png}}
    \centerline{\small Dual + Self}
\end{minipage}
\hfill
\begin{minipage}[t]{0.237\linewidth}
    \centering
        \vspace{1pt}
        \centerline{\includegraphics[width=\textwidth]{img/ab-structure/00690_cut/3.png}}
        \centerline{\includegraphics[width=\textwidth]{img/ab-structure/00722_cut/3.png}}
        \centerline{\includegraphics[width=\textwidth]{img/ab-structure/00787_cut/3.png}}
    \centerline{\small Dual + Cross}
\end{minipage}
\hfill
\begin{minipage}[t]{0.237\linewidth}
    \centering
        \vspace{1pt}
        \centerline{\includegraphics[width=\textwidth]{img/ab-structure/00690_cut/4.png}}
        \centerline{\includegraphics[width=\textwidth]{img/ab-structure/00722_cut/4.png}}
        \centerline{\includegraphics[width=\textwidth]{img/ab-structure/00787_cut/4.png}}
    \centerline{\small GroundTruth}
\end{minipage} 
\vspace{-0.18cm}
\caption{Results of using different structures on LOLv2-Real.}
% All training and testing for this experiment are performed in the HVI color space.}
\vspace{-0.15cm}
\label{fig:struct}
\end{figure}


% \textbf{Variant HVIT.} To investigate how the first two $3\times3$ convolution layers (in Enhancement Network) learned to generate I-feature and HV-feature, we further develop three different HVIT by changing the inputs. 
% As shown in Table \ref{tab:ablation2} row (1), the Half-HVIT utilized in our pipeline generates intensity features (hereinafter abbreviated as I-Features) from the intensity Map and HV-Features from the HVI-Map. 
% We have respectively modified the input of I-Features and HV-Features, leading to the creation of two new HVIT models in Table \ref{tab:ablation2} row (2) and row (3). The Separate-HVIT replaces the input of HV-Feature with HV-Map, while the Full-HVIT substitutes the input of I-Feature with HVI-Map.
% As a result, the Half-HVIT achieves the best restoration results among the three HVIT models. 
% The performance drop of the Separate-HVIT is more pronounced. 
% This can be attributed to the lower information content in the HV features compared to the HVI-Map, which lacks guidance from the intensity part. On the other hand, the performance decline of the Full-HVIT was due to the interference noise information in the HVI-Map for extracting I-Features, leading to convolutional layers failing to accurately extract key features.





