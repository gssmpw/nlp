

\vspace{-0.45cm}
\section{Introduction}
\label{sec:intro}


Under low-light conditions, imaging sensors often capture weak light signals with severe noise, resulting in poor visual quality for low-light images.
Obtaining high-quality images from such degraded images necessitates Low-Light Image Enhancement (LLIE) that aims at improving the image brightness while simultaneously reducing the impact of noise and color bias \cite{2022LLE}.

The majority of existing LLIE approaches \cite{KinD, EnGAN,Zero-DCE,Han_ECCV24_GLARE,10543170,GSAD} focus on finding an appropriate image brightness, which is typically done by employing deep neural networks to learn a mapping relationship between low-light images and normal-light images within the standard RGB (sRGB) space.
However, the image brightness exhibits a strong coupling with the color from the three sRGB channels, \textit{a.k.a.} high color sensitivity in \cite{gevers2012color,lee2024rethinking}, causing an obvious color distortion of the restored image in these LLIE methods \cite{RetinexFormer,EnGAN}, as shown in Fig. \ref{fig:1} (a).

Inspired by Kubelka-Munk theory \cite{gevers2012color}, recent methods \cite{li2020low,zhou2023low,zhang2021better} have sought to transform images from the sRGB color space to the Hue, Saturation and Value (HSV) color space. These methods help achieve the brightness enhancement more accurately, but they amplify local \textbf{color space noise} \cite{gevers2012color}, introducing severe artifacts in the results. 
As illustrated in Fig. \ref{fig:1} (b), the transformation from sRGB to the HSV disrupts the continuity of the red (\textcolor{red}{\ding{172} Red Discontinuity Noise}) and black (\textcolor{blue}{\ding{173} Black Plane Noise}) color, resulting in increased Euclidean distances for similar color and the introduction of artifacts in the final images (see the zoomed in images of \ding{172} and \ding{173}). 
% Similarly, the black region expands across the entire low-Value plane, intensifying dark noise. 
These two types of noise can cause severe artifacts in the enhancement of red-dominated or extremely dark images.

To address these issues,
% aforementioned issue between the low-light image enhancement task and existing color spaces, 
we introduce a new color space named \textit{\textbf{H}orizontal/\textbf{V}ertical-\textbf{I}ntensity (\textbf{HVI})}, designed specifically for the LLIE task.
% cater to the needs of low-light enhancement tasks. 
% As shown in Fig. \ref{fig:1}, 
% The goal of the 
% HVI 
% color space is to ensure 
% is based on t
The key intuition 
% which aims to achieve
is that minimizing color space noise, by reducing Euclidean distances in similar colors.
% similar colors have smaller Euclidean distances.
% For red discontinuity issue,
To this end, we polarize in the Hue and Saturation (HS) plane to enforce smaller distances for similar red point coordinates, which eliminates the red discontinuity noise in the primary HSV space (see Fig. \ref{fig:1} (c)). 
For the black plane noise issue, we introduce a trainable darkness density parameter $k$ and its corresponding adaptive intensity collapse function $C_k$, which compresses the radius of low-light regions to zero, with the flexibility to gradually expand to the value of one as the intensity increases, as illustrated in Fig. \ref{fig:1} (d). This helps remove the black noise artifacts while maintaining the primary image appearance.
% , allowing it to achieve the best result.

% Building upon the HVI color space, to fully leverage the chromatic and intensity information,
We further propose an LLIE network, named \textit{\textbf{C}olor and \textbf{I}ntensity \textbf{D}ecoupling Network (\textbf{CID}Net)}, to leverage the chromatic and intensity information for optimizing the color and intensity in the HVI space. After transforming the image into the HVI space, CIDNet leverages two network branches, namely HV-branch and intensity-branch, to respectively model decoupled color and brightness information for
% which makes full use of light intensity and chromatic decoupled information to generate high-quality results. 
% After applying the HVI transformation to the image, color gamut information based on intensity supervision and brightness information is separately fed into the HV-branch to extract color information, and the intensity-branch to 
learning accurate photometric mapping under different lighting conditions and restoring more natural colors. 
% This yields an optimized noise-free HVI color space that largely enhances the brightness of low-light images while preserving their natural colors.
% This yields a HVI color space with optimal image enhancement, effectively preserving the color and brightness from HSV while at the same time removing the red and black artifacts.

% Note that the proposed method exhibits relatively small parameters (1.88M) and computational loads (7.57G), achieving a good balance between effectiveness and efficiency on edge devices.

% Furthermore, we conduct experiments and ablation studies on multiple datasets to validate our approach. The experimental results demonstrate that CIDNet effectively enhances the brightness of low-light images while preserving their natural colors, which validates the compatibility of the proposed HVI color space with low-light image enhancement tasks. Note that the proposed method exhibits relatively small parameters (1.88M) and computational loads (7.57G), achieving a good balance between effectiveness and efficiency on edge devices.

Our contributions can be summarized as follows:
\begin{itemize}
    \item We introduce a new HVI color space for the LLIE task, which is uniquely defined by polarized HS and trainable intensity. This offers an effective tool that eliminates the color space noise arising from the HSV space, largely enhancing the brightness of the low-light images.
    % while preserving their natural colors.
    % while preserving its color and brightness enhancement.
    \item We further propose a novel LLIE network, CIDNet, to concurrently model the intensity and chromatic of low-light images in the HVI space. Despite being lightweight and computationally-efficient with relatively small parameters (1.88M) and computational loads (7.57GFLOPs), it enables the learning of accurate photometric mapping under different lighting conditions.
    % in the HVI space.
    % , while being relatively small parameters (1.88M) and computational loads (7.57G)
    \item Comprehensive results from quantitative and qualitative experiments show that our approach outperforms various types of state-of-the-art (SOTA) methods on different metrics across 10 datasets. 
\end{itemize}
