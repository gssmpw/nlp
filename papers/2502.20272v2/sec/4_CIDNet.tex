\section{Color and Intensity Decoupling Network}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=1\linewidth]{img/DVCNet-pipeline.pdf}
    \caption{The overview of the proposed CIDNet. \textbf{(a)} HVI Color Transformation (HVIT) takes an sRGB image as input and generates HV color map and intensity map as outputs. \textbf{(b)} Enhancement Network performs the main processing, utilizing a dual-branch UNet architecture with six Lighten Cross-Attention (\textcolor[RGB]{255,95,95}{LCA}) blocks. Lastly, we apply Perceptual-inverse HVI Transform (PHVIT) to take a light-up HVI map as input and transform it into an sRGB-enhanced image.}
    \label{fig:CIDNet-pipeline}
\end{figure*}


% \subsection{Overview of CIDNet}
To more effectively utilize chromatic and brightness information in the HVI space, we introduce a novel dual-branch LLIE network, named Color and Intensity Decoupling Network (CIDNet), to separately model the HV-plane and I-axis information in the HVI space, as shown in Fig. \ref{fig:CIDNet-pipeline}. CIDNet employs HV-branch to suppress the noise and chromaticity in the dark regions and utilizes I-branch to estimate the illuminance of the whole images.

The overall framework of CIDNet can be divided into three consecutive main steps. There is an HVI transformation applied before the dual-branch enhancement network. After the enhancement, CIDNet performs perceptual-inverse HVI transformation to map the image to the sRGB space. Below we introduce each step in detail.

\subsection{HVI Transformation}
As described in Sec. \ref{sec:HVI}, the HVI transformation decomposes the sRGB image into two components: an intensity map containing scene illuminance information and an HV color map containing scene color and structure information. 
Specifically, we first calculate the intensity map using Eq. \ref{eq:1}, which is $\mathbf{I_{I}}=\mathbf{I}_{max}$. Subsequently, we utilize the intensity map and the original image to generate HV color map using Eq. \ref{eq:7}.
Furthermore, a trainable density-$k$ is employed to adjust the color point density of the low-intensity color plane, as shown in Fig. \ref{fig:CIDNet-pipeline}(a).

% Based on the UNet architecture, which has an encoder, decoder, and multiple skip connections, the enhancement network takes an intensity map and HV color map as input. 
% As depicted in Fig. \ref{fig:CIDNet-pipeline}(b), the encoder includes three Lighten Cross-Attention (LCA) blocks and downsample and $3\times3$ kernel convolutional layers to obtain the features with same dimension in each branch.
% Similarly, the decoder consists of three LCA modules, and upsampling layers. 
% The final outputs of UNet are the refined intensity and HV maps.
% To decrease the difficulty of the learning process, we employ a residual mechanism to add the original HVI map.
% Finally, we perform a Perceptual-invert HVI Transformation (PHVIT) with the trainable parameter density-$k$ to map the image to the sRGB space. More LCA and PHVIT architecture details can be found in the supplementary.

\subsection{Dual-branch Enhancement Network}
\label{sec:dual}
As illustrated in Fig. \ref{fig:CIDNet-pipeline} (b), the dual-branch network is built upon the UNet architecture, involving an encoder and an decoder with respective three Lighten Cross-Attention (LCA) modules, and multiple skip connections. Two key designs in the network are the dual-branch structure and the LCA module.
% , which applies cross-attention between the two branches. 
We explain the key intuition behind these designs as follows.

The LLIE task can be decomposed into two sub-tasks: noise removal in low-light regions and brightness enhancement. 
By converting the image to the HVI color space, where luminance and color are decoupled, we can apply brightness mapping to the Intensity map and denoising to the HV color map. 
Since these two sub-tasks follow distinct statistical patterns \cite{2022LLE}, inspired by Retinex-based methods \cite{RetinexNet,URetinexNet,PairLIE}, we use separate branches, the I-branch and HV-branch, to address each sub-task individually.
Additionally, the input to the HV branch is formed by concatenating the Intensity map with the HV color map, as we observed that severely low-light images contain a small amount of noise in the luminance component as well.

The cross-attention between the two branches is used, rather than using self-attention individually to each branch. One main reason is that the illumination intensity is inversely proportional to image noise intensity. 
A low-light image may also contain high-illumination regions requiring only minimal denoising and enhancement. 
Therefore, using the intensity features to guide the HV-branch in denoising can reduce global color shifts and achieve more effective noise suppression.
For another reason, the noisy intensity information, after being denoised in the HV-branch, is transferred to the I-branch through cross-attention, resulting in smoother enhancement outcomes.



\subsection{Perceptual-inverse HVI Transformation}
To convert HVI back to the HSV color space, we perform a Perceptual-inverse HVI Transformation (PHVIT), which is a surjective mapping while allowing for the independent adjustment of the imageâ€™s saturation and brightness.

% To transform injectively, t
The PHVIT sets $\hat{h}$ and $\hat{v}$ as an intermediate variable as $\hat{h}=\frac{ \mathbf{\hat{H}}}{\mathbf{C}_k+\mathcal{\varepsilon}}$,$\hat{v}=\frac{ \mathbf{\hat{V}}}{\mathbf{C}_k+\mathcal{\varepsilon}}$, where $\mathcal{\varepsilon}=1\times10^{-8}$ is used to avoid gradient explosion. 
Then, we convert $\hat{h}$ and $\hat{v}$ to HSV color space.
The Hue ($\mathbf{H}$), Saturation ($\mathbf{S}$) and Value ($\mathbf{V}$) map can be estimated as
\begin{equation}
\begin{split}
    \mathbf{H} &= \arctan(\frac{\hat{v}}{\hat{h}})\mod 1,\\
    \mathbf{S} &= \alpha_{S}\sqrt{\hat{h}^{2}+\hat{v}^{2}},\\
    \mathbf{V} &= \alpha_{I}\hat{\mathbf{I}}_{\mathbf{I}},
\end{split}
\end{equation}
where $\alpha_{S},\alpha_{I}$ are the customizing linear parameters to change the image color saturation and brightness. Finally, we will obtain the sRGB image with HSV image \cite{Foley1982FundamentalsOI}.

\subsection{Loss Function}
% To integrate the advantages of HVI space and the sRGB space, the loss function consists of both color spaces. 
To provide comprehensive supervision for training CIDNet, we guide the enhancement from two key perspectives, including the GroundTruth in the sRGB space and the HVI map in the HVI space. Specifically, given the enhanced HVI map $\mathbf{\hat{I}_{HVI}}$ and the restored sRGB image $\mathbf{\hat{I}}$ output from CIDNet, we aim to minimize their difference to the primary sRGB GroundTruth $\mathbf{I}$ and its corresponding HVI map $\mathbf{I_{HVI}}$:
 % . Let , our overall loss $L$ can then be defined as: 
\begin{equation}
L= \lambda\cdot l(\mathbf{\hat{I}_{HVI}},\mathbf{I_{HVI}}) + l(\mathbf{\hat{I}},\mathbf{I}),
\end{equation}
where $\lambda$ is a weighting hyperparameter to balance the losses in the two different color spaces. This helps achieve not only more closely with the probabilistic distribution of sRGB in the HVI space, especially the red and black ones, due to the optimized $k$ and $\mathbf{C}_k$, but also inheritance of pixel-level structure detail in the sRGB space. 