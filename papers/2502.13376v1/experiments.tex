We evaluate our proposed approach, which we refer to as \textbf{LOTaD} (\textbf{L}earning \textbf{O}ptimal \textbf{Ta}sk \textbf{D}ecompositions), in a variety of MARL settings with varying task complexity. Our code is available at \href{https://github.com/thomasychen/LOTaD}{https://github.com/thomasychen/LOTaD}.

In our experiments, we seek to answer the following research questions: \textbf{(RQ1)} Does LOTaD outperform existing approaches to RM-guided MARL that are not learning-informed, including approaches that do not perform task decomposition? \textbf{(RQ2)} Can LOTaD learn to avoid pitfalls in learning that may occur due to dependent dynamics across agents? \textbf{(RQ3)} How does varying the number of candidate decompositions $k = |\decompositions|$ affect the learning performance of LOTaD?

\subsection{Environments and tasks}

Our environments include the \textbf{Repairs} environment and RM task presented in Figure~\ref{fig:running_example}. The environment is instantiated as a 7x12 grid in which agents can move in any of the cardinal directions, or take a no-op action where no movement occurs. We make the Repairs environment stochastic by giving a small chance of an agent ``slipping'' and taking a random action rather than the action selected by their policy. Each agent observes only their position in the world at each timestep.

In addition to the Repairs environment, we also include two ``Buttons'' environments that require a team of agents to press a series of buttons in a particular order. We instantiate two environment-task pairs: First, we use \textbf{Cooperative Buttons}, a task from~\cite{neary2020reward} where any agent must reach a specified goal location, but in order to do so, must traverse regions that can only be crossed once a corresponding button has been pressed. Second, we use \textbf{Four-Buttons}, a task where two agents must press four buttons (yellow, green, blue, and red) in an environment, with an ordering constraint that the yellow button must be pressed before the red button. Both of these environments are represented as 10x10 grids with the same observation space, action space, and ``slipping'' dynamics as the Repair environment. Visualizations of these environments are presented in our Appendix.

Lastly, we evaluate LOTaD on two environments from the popular multi-agent benchmark \textit{Overcooked}~\cite{carroll2019overcooked}. In both environments, we provide the same simple task of delivering a soup to the delivery station, which requires putting three onions in a pot, plating, and delivering the soup. Agents must coordinate depending on the dynamics of their environment to efficiently cook and deliver the soup. We use a \textbf{Cramped-Corridor} environment, where two agents must navigate around one another to reach the pot at the end of a small corridor in a cramped room, and an \textbf{Asymmetric-Advantages} environment, where two agents are in separate rooms, but have access to an asymmetric set of resources in their respective rooms. We visualize both Overcooked environments in our Appendix. The observation space and action space for these environments are the same as those provided in the Overcooked implementation from~\cite{flair2023jaxmarl} with the adjustment that we do not expose the number of onions in the pot or whether the soup has finished cooking in order to ensure that no information provided by the labeling function is redundant in the agents' observations.


 In all environments, we apply a discount factor $\gamma < 1.0$ to the reward offered by our RM to incentivize our agents to accomplish the task as efficiently as possible. We generate $k=10$ decomposition candidates in all experiments using the generation method described in Section~\ref{subsec:generating_decompositions} for LOTaD to search amongst. We provide additional information regarding each environment and task in our Appendix. 

\subsection{Baselines}

To evaluate LOTaD, we compare against a baseline that selects a decomposition using the \textbf{ATAD} method~\cite{smith2023automatic}. This approach selects a set decomposition prior to learning based on the scoring method described in Section~\ref{subsec:generating_decompositions}. We break ties between top scoring decompositions arbitrarily. In addition to this baseline, we also compare against a baseline approach that assigns each agent the overall task. We call this baseline \textbf{Monolithic} and use it to evaluate how MARL would perform if no decomposition of the RM was used. %Lastly, we compare against a \textbf{Naive} selection approach, that considers the same set of possible decompositions $\decompositions$ as LOTaD, but randomly selects a decomposition to assign to our agents at each new episode of training. 
We use PPO~\cite{schulman2017ppo} with a Gaussian policy over the action space for each agent as our RL algorithm in every environment. 

\subsection{Results}

\begin{figure*}[t]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/reapirs_task_LOTaD_mono_ablate.png}
        % \caption{The Four Buttons environment with an example trajectory.}
        % \label{fig:4_buttons_env}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/cramped-corridor_LOTaD_mono_ablate.png}
        % \caption{The coordination task from~\cite{neary2020reward} with an example trajectory.}
        % \label{fig:cyrus_env}
    \end{minipage}
    \caption{Training curves for LOTaD in the Repairs Task and Cramped-Corridor environments demonstrating the effect of conditioning on the overall task state along with individual sub-task states for each agent.}
    \label{fig:no_mono_ablation_curves}
\Description{Training curves for our overall encoding ablation study.}
\end{figure*}

We plot training curves for all experimental domains in Figure~\ref{fig:combined_results}. In these curves, we report the current best discounted reward achieved by LOTaD amongst all decomposition candidates under consideration. We find that LOTaD outperforms both the monolithic and ATAD baselines across all environments, answering \textbf{(RQ1)} in the affirmative. In many of our environments, learning decompositions with LOTaD allows for agents to explicitly parallelize their contributions towards achieving the task. For example, in the Four-Buttons environment, a candidate decomposition allows for one agent to visit the yellow and green buttons while the other agent visits the blue button. 

Interestingly, LOTaD-learned decompositions are useful even when explicit parallelization is not possible, such as the Overcooked RM task. Decompositions of this task can still facilitate multi-agent learning: for example, one agent may be tasked with putting all three onions in the pot, while the other agent must wait for the soup to be cooked before plating and delivering the soup. In this decomposition, the agent tasked with plating and delivering the soup may fetch a plate and stand near the pot so that the soup can be quickly delivered upon completion of cooking.

In comparison to LOTaD, the ATAD baseline is unable to consistently find performant task decompositions. Although ATAD selects a decomposition based on the same scoring method by which we select $\decompositions$, there exist many possible decompositions tied for the highest achievable score in a given task. As a result, ATAD may select the optimal decomposition when the optimal decomposition is also the highest scoring, but performs suboptimally when this is not the case. In addition to lower reward, this leads to a higher variance in performance by ATAD, as evidenced by Figure~\ref{fig:combined_results}. The Monolithic baseline consistently achieves the lowest reward due to the sparsity of the overall task reward. Similar to ATAD, we notice that policy learning with the Monolithic baseline is unreliable and tends to converge more slowly than LOTaD.


We find that LOTaD is able to successfully accomplish the task in the Repairs and Overcooked Cramped-Corridor environments, which both involve dependent dynamics amongst agents. LOTaD is indeed able to avoid issues when training agent teams in environments with dependent dynamics, affirming \textbf{(RQ2)}. To further investigate our hypothesis of whether a global task view alleviates these issues, we ran LOTaD \textit{without} an encoding of the overall task in the Repairs and Cramped-Corridor environment. We present the training curves from this ablation study in Figure~\ref{fig:no_mono_ablation_curves}. We find that without the overall task encoding, LOTaD is largely unable to accomplish the Repairs task, and is slower and less stable in accomplishing the Cramped-Corridor task. This suggests that the overall task encoding enables our agents to learn policies that progress towards completion of the overall task even when an agent's individual sub-task is already achieved. 
%Without the overall task encoding, agents that have completed their respective sub-tasks may prevent other agents from completing their sub-tasks because they no longer have any reward incentive. 
In our other environment settings, where codependent agent dynamics do not exist, removing the overall task encoding does not significantly affect training.

\begin{figure}
\centering
\includegraphics[width=0.9\columnwidth]{figures/4-buttons_mono_off_cand_ablations.png}
\caption{Training curves for the UCB selection strategy on increasingly sized $\decompositions$ for the Four-Buttons task. Results are averaged over 10 random seeds.}
\Description{Ablation study varying LOTaD's performance on varying decomposition set sizes.}
\label{fig:scaling_results}
\end{figure} 


To answer \textbf{(RQ3)}, we perform ablation studies where we vary the size of $\decompositions$. These training curves are visualized in Figures~\ref{fig:scaling_results}. We see that values of $k$ that are either very small or very large make the learning problem more challenging for LOTaD. We reason that if $k$ is too large, more exploration is required, and LOTaD may struggle to find the optimal decomposition amongst many candidates. If $k$ is too small, there is a lower chance of ``good'' decompositions appearing in the set $\decompositions$, which inherently limits the performance potential of LOTaD. We note that the inherent randomness of LOTaD, as well as the choice of environment, task, and the amount of exploration prioritized by our UCB hyperparameter $\beta$, may confound results.
