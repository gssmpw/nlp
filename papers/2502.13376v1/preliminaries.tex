% \subsection{Preliminaries}
\subsection{MDPs and Labeled Markov Games}
A Markov Decision Process (MDP) $\mdp = (S, A, T^\mdp, d_0, \gamma)$ is a tuple that consists of a state space $S$, an action space $A$, a transition function $T^\mdp: S \times A \rightarrow S$, an initial state distribution $d_0 \in \Delta(S)$, and a discount factor $0 < \gamma < 1$. A (stationary) policy $\pi: S \rightarrow \Delta(A)$ in $\mdp$ produces a distribution over actions given a state. Following standard RL notation, we define $\pi(a_t | s_t)$ as the probability of taking action $a_t$ in state $s_t$ at timestep $t$. A policy takes an action in a given state in $\mdp$, transitions to a new state via $T^\mdp$, and repeats, generating a \textbf{trajectory} of length $\tau$ as $(s_0, a_0, \dots s_\tau, a_\tau)$. 

To generalize to the MARL setting, we extend $\mdp$ to a cooperative Markov game with homogeneous action spaces. We define a cooperative Markov game with $n$ agents as $\markovgame = (\states, \actions, T^\markovgame, d_{0^1} \dots d_{0^n}, \gamma, L^\markovgame$), which corresponds to the joint set of states $\states = S_1 \times \dots \times S_n$, the joint set of actions $\actions = A_1 \times \dots \times A_n$, a joint transition operator $T^\markovgame: \states \times \actions \rightarrow \states$, a set of independent initial distributions for each agent $d_{0^1} \times \dots \times d_{0^n}$, and
a discount factor $\gamma$ that remains unchanged. We consider the \textit{centralized training, decentralized execution} \cite{bernstein2002complexity} setting of MARL, where each agent receives independent observations and deploys their individual policies $\pi_i: S_i \rightarrow \Delta(A_i)$ at execution time, but are jointly trained. The joint policy $\boldsymbol{\pi}: \states \rightarrow \Delta(\actions)$ executes all individual policies simultaneously at each timestep, and successive states are dictated by $T^\markovgame$, generating joint trajectories $((s_{0^0}, a_{0^0},\dots s_{0^n}, a_{0^n}), \dots (s_{\tau^0}, a_{\tau^0},\dots s_{\tau^n}, a_{\tau^n}))$. 

In addition to the standard components of our Markov Game, we assume access to a known \textit{labeling function} $L^\markovgame: \states \rightarrow 2^\Sigma$ that maps states in $S$ to a set of \textit{environment events}, denoted by $\Sigma$. Each environment event $e \in \Sigma$ is represented by a variable that takes on a Boolean truth value (True or False). For example, in Figure~\ref{fig:running_example}, if Agent 1 is in the yellow station, Agent 2 is in the red station, and Agent 3 has not moved from its initial position, the labeling function for this joint state would return $\{Y_S, R_S\}$. 


\subsection{Task completion Reward Machines} 
In this work, we consider task completion reward machines (RMs)~\cite{xu2020joint, neary2020reward, smith2023automatic} as our team objective. A task completion RM is specified by a tuple $\rewmach = (\rmstates, u_{-1}, \Sigma, \delta, \rmstates^*)$ consisting of a set of states $\rmstates$, an alphabet of events $\Sigma$ that trigger transitions in $\rewmach$ via the transition function $\delta: \rmstates \times \Sigma \rightarrow \rmstates$, an initial state $u_{-1}$, and a set of goal states $\rmstates^*$. There are no outgoing transitions from any goal state $u^* \in \rmstates^*$. Task completion RMs additionally define an output scoring function $\sigma: \rmstates \times \rmstates \rightarrow \mathbb{R}$, where $\sigma(u, u') = 1$ whenever $u \notin \rmstates^*$ and $u' \in \rmstates^*$, and $0$ otherwise. Task completion RMs are Mealy machines \cite{mealy1955method} where reaching the goal state represents a valid completion of the task. We remark that $\delta$ and $\sigma$ are partial functions defined on subsets of $\rmstates \times \Sigma$ and $\rmstates \times \rmstates$.
%In task completion RMs, goal states are defined as any state $u^* \in \rmstates$ where all incoming transitions from non-goal states have a score of 1; that is, $\sigma(u, u^*) = 1$
The transition operator for a task completion RM takes in single events $e$ in as input; when multiple events occur simultaneously, the events are passed in sequence to the RM in arbitrary order. %For example, in Figure~\ref{fig:running_example}, if the yellow and red buttons are pressed simultaneously, this is deemed unsatisfactory with respect to the task, and $r$ will be seen by the RM before $y$, causing the task to fail.

A task completion RM is connected to a Markov game by the labeling function $L^\markovgame$. We can project a trajectory in $\markovgame$ to a trajectory in $\rewmach$ by applying $L^\markovgame$ to each state, creating a sequence of event sets $(\{e_i \dots e_k\}_0, \dots \{e_i \dots e_k\}_\tau)$. This sequence of event sets transitions $\rewmach$ to create a trajectory $(u_0, \dots u_{\tau})$, where $u_0$ is the state resulting from $\delta(u_{-1}, \{e_i \dots e_k\}_0)$, and so forth. We say $\rewmach$ \textit{accepts} a trajectory if  $u_{\tau} \in \rmstates^*$. By definition, the cumulative score of $\sigma$ will be 1 for accepting trajectories, and 0 for all non-accepting trajectories.


\subsection{Using task completion RMs in MARL}

Recall our problem setting of \textit{centralized training, decentralized execution}: each agent will receive independent observations during execution. In other words, each agent will view the cooperative Markov game as an MDP, where the presence and actions of other agents are captured by the dynamics of their respective environments. 
To train our agent team, we can use the acceptance condition of a task completion RM as a reward function for MARL. This objective can be naturally compiled down to a reward function expressed over a \textbf{product MDP} for individual agent policies $\pi_1 \dots \pi_n$. Concretely, a product MDP synchronizes $\mdp$ and $\rewmach$ so that an agent may learn a policy over the joint space by coupling the reward machine state $u_t$ during a trajectory with the MDP's state $s_t$, producing an action conditioned on both: $\pi(a_t | s_t, u_t)$. If the agents transition to a goal state in $\rewmach$, they will receive a reward of one; all other transitions will receive a reward of zero. Existing deep RL algorithms, such as PPO~\cite{schulman2017ppo}, have shown success in learning performant policies for RMs in a variety of single-agent settings by learning policies over the product MDP~\cite{Li2024noisyuncertain, voloshin2023eventual}.  


However, in a MARL setting, providing the full task completion RM for each agent creates difficulties in learning. If all agents see the same states and transitions in $\rewmach$, then all agents will receive credit when the task is completed, even if one or more agents did not contribute to completion of the task. Recall our running example in Figure~\ref{fig:running_example}. Suppose Agent 1 and Agent 2 visit HQ at the same time, and then Agent 2 visits the yellow and red stations, while Agent 3 remains stationary. Because the task is completed, all agents will receive equal reward for that episode. As a result, Agent 3 will think that its stationary behavior is desirable and be encouraged to act similarly in future episodes. 

Existing work addresses this shortcoming by introducing the notion of \textit{task decomposition} for a given task completion RM~\cite{neary2020reward}. A decomposition of a task completion RM creates sub-tasks in the form of $n$ smaller RMs $\rewmach_1 \dots \rewmach_n$, derived from the original, that can then be assigned to each agent. Each agent is only concerned with accomplishing their own sub-task encoded by the RM assigned to them. In order to compute a decomposition, a practitioner provides \textit{Local Event Sets} (LES) for each agent. An LES is a subset of events $\Sigma_i \in \Sigma$ that is deemed relevant to agent $i$'s individual sub-task and restricts agent $i$ to only observing events in $\Sigma^i$. A task completion RM is then \textit{projected} onto these local event sets to create an individual's sub-task reward machine $\rewmach_i = (\rmstates_i, u_{-1^i}, \Sigma_i, \delta_i, \rmstates^*_i)$. The states, initial state, and goal states of $\rewmach_i$ are sets of equivalence classes over states in $\rewmach$ based on an equivalence relation that subsumes states whose transitions do not contain any event in $\Sigma_i$. The transition function is a projection of the original $\delta$ where a transition between two states $u_j$ and $u_k$ in $\rewmach_i$ exists only if an event in $\Sigma_i$ triggered a transition between two distinct states in $\rewmach$ that were subsumed by $u_j$ and $u_k$ respectively. For the exact procedure of this projection, see~\cite{neary2020reward}. 

For each agent's sub-task RM $\rewmach_i$, we define an individual labeling function $L_i$ that projects the set of environment events returned by $L$ to the events belonging to an agent's local event set $\Sigma_i$. We say an event $e$ is a \textit{shared event} if it belongs to more than one local event set. For example, the event \textit{"Signal"} in Figure~\ref{fig:running_example} is a shared event. In the case of shared events, agents' sub-task RMs must \textit{synchronize} on this event. This means that a synchronized event must trigger a transition for all agents' RMs that share the event (i.e. all sub-task RMs must be in the appropriate state for the synchronized event to cause a transition), or no transition is taken from encountering that event for any agent's RM. 

%The decomposition strategy proposed in~\cite{neary2020reward} relies on constructing ``Local Event Sets'' (LES) for each agent, where each agent is assigned a subset of elements in $\Sigma$ that are deemed relevant to that agent's individual sub-task. The approach guarantees that completion of the decomposed sub-tasks will ensure completion of the original task. Further details on this decomposition algorithm can be found in~\cite{neary2020reward}. 
The decomposition approach outlined in~\cite{neary2020reward} relies on a practitioner manually designing the local event sets to assign to each agent.  
In order to help guide the search for an appropriate assignment of local event sets,~\cite{neary2020reward} provides a notion of \textit{validity} for a given decomposition: a decomposition is valid if and only if the parallel composition of all reward machines $\rewmach_1 \dots \rewmach_n$ is bisimilar to the original $\rewmach$. If a decomposition is valid, then for any trajectory of events $\xi$, $\rewmach$ accepts $\xi$ if and only if all sub-task RMs $\rewmach_1 \dots \rewmach_n$ accept $\xi$.
% accepts $\xi$ in a concurrent execution of all sub-task RMs. A concurrent execution of each sub-task RM steps through each RM using events in $\xi$, but if a synchronized event (an event shared across local event sets) is encountered, it must trigger a transition for all sub-task RMs that share the event (i.e. all sub-task RMs must be in the appropriate state for the sychronized event to cause a transition) or no transition is taken from encountering that event.
In other words, a trajectory of events that accomplishes all decomposed sub-tasks encoded by $\rewmach_1, \dots, \rewmach_n$ is guaranteed to solve the overall task. 
