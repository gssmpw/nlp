We introduce a novel approach for learning the optimal decomposition of a task completion RM for MARL through model free interactions with the environment. Our framework 
%leverages task-conditioned policies and value estimates of possible decompositions to 
simultaneously learns the optimal decomposition and the policies that solve that decomposition amongst a set of candidates. Our experiments show that we improve the sample efficiency of multi-agent learning in model-free deep RL settings even when the optimal decomposition is not known a priori. 
Through our ablations, we have shown that both including the encoding of the overall task and intelligent decomposition selection is critical for sample efficient learning of reward machines in multiagent settings. 

We see a number of compelling directions for future work. For example, we are interested in exploring how different representations of an RM may enable greater sample efficiency by exploiting semantic similarity amongst overlapping sub-tasks. 
%We are also interested in exploiting counterfactual experience replay for reward machines~\cite{Icarte2020RewardMachine} to more efficiently leverage data collected during training.
Lastly, we are interested in exploiting the inherent curriculum present in decomposition exploration by generalizing our multi-agent setting to solving multiple tasks, building on prior work in goal-conditioned RL on automata-based tasks~\cite{vaezipoor2021ltl2action, qiu2023gcrlltl}.