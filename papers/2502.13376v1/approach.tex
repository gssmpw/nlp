In practice, multiple valid decompositions often exist for a given RM. However, a valid decomposition may not be \textit{feasible} under the dynamics of a given MDP; that is, the resulting learned policies may not be able to achieve the individual tasks prescribed by a decomposition. Moreover, when multiple feasible decompositions do exist, we do not know which decomposition most efficiently achieves the task or provides the best credit assignment for learning. Recall the Repairs environment and task in Figure~\ref{fig:running_example}. A feasible decomposition of the task would be for Agent 1 and Agent 2 to meet at HQ, then for Agent 2 to visit the yellow, then red stations in that order. However, this decomposition is less efficient than a decomposition where Agents 1 and 3 visit HQ, then Agent 2 visits the yellow station and leaves the hazardous region while Agent 3 visits the red station (assuming Agents complete their sub-tasks with optimal efficiency).

Recent work attempts to automate the search for RM decompositions by leveraging additional information provided by a practitioner~\cite{smith2023automatic}. In this work, subsets of events in $\Sigma$ can be provided that either require or forbid an event to belong to a specific agent's local event set $\Sigma_i$, along with a utility function that quantifies how valuable an event would be to a specific agent. This information is then used to find a subset of events in $\Sigma$ that still leads to a valid decomposition of $\rewmach$, if one exists. However, the aforementioned approach does not leverage knowledge gained about the MDP during training and therefore cannot ensure that the decomposition generated from their method is feasible or optimally efficient. In what follows, we will introduce our approach, which aims to find the optimal decomposition by learning a policy on-the-fly for many possible sub-tasks during training.


Our approach can be broken down into three primary components: \textbf{(1)} automatically generating a set of possible (candidate) decompositions for our task, \textbf{(2)} using a task-conditioned policy to generalize learning across multiple decompositions, and \textbf{(3)} employing the Upper Confidence Bounds (UCB) strategy~\cite{auer02ucb} to balance exploration and exploitation of candidates throughout training. %These components allow us to learn the optimal decomposition and policy for that decomposition simultaneously during training. %We emphasize that our approach is \textit{agnostic} to the symbolic concept class used to encode the task. 


\subsection{Generating candidate decompositions}
\label{subsec:generating_decompositions}
Approaches to procedurally generate decompositions of reward machines and similar automaton-based task specifications have been explored in the literature~\cite{smith2023automatic, lauffer2022dfadecomp}. Such methods can be used to generate a finite set of candidate decompositions $\decompositions = \{(\rewmach_{1}^1 \dots \rewmach_{n}^1), \dots \\(\rewmach_{1}^{|\decompositions|} \dots \rewmach_{n}^{|\decompositions|})\}$ from which the optimal decomposition can be found. We will denote an individual decomposition in our set as $d \in \decompositions$. In our work, we will use the decomposition approach for task completion RMs introduced in~\cite{smith2023automatic}. In this approach, all possible \textit{valid} decompositions are generated given $\Sigma$, and each one is assigned a score based on three factors: (1) minimizing the average number of events assigned to each agent's local event set, (2) maximizing the similarity amongst the sizes of all local event sets, and (3) maximizing the average `utility' of each agent's local event set, based on a practitioner-provided utility function that maps the assignment of an event to an agent to a scalar value. In our work, we assume that no utility function has been provided. We will therefore enumerate the `top-$k$' valid candidate decompositions based on a weighted sum of scores pertaining to factors (1) and (2), where $k$ is a hyperparameter that sets the number of decompositions we will consider, i.e. $k = |\decompositions|$. Our objective is to find the decomposition $d^* \in \decompositions$ that leads to a learned policy which will most efficiently achieve the original task $\rewmach$.



\begin{figure*}[t]
\centering
\includegraphics[width=0.9\linewidth]{figures/observation_viz.png}
\caption{A visualization of the information each agent receives using the policy architecture described in section~\ref{subsec:task_conditioned_setup} for the Repairs task from Figure~\ref{fig:running_example}. In addition to the observation gathered from the MDP, each agent's policy is conditioned on (1) the current state of the original RM task, (2) which decomposition is currently selected, and (3) the current state of their assigned sub-task RM within the selected decomposition.
%would view their own reward machine if training on a sub-task vs. the whole task. In this case they are on a step where they have hit yellow and blue. Goal states are encoded with concentric black circles.
}
\label{fig:observation_viz}
\Description{Visualization of the information provided to agent policies.}
\end{figure*} 


\subsection{Task-conditioned policies}
\label{subsec:task_conditioned_setup}
We consider each agent policy in our team as sharing a \textit{task-conditioned} policy. Instead of learning separate policies for individual sub-tasks, we learn a single policy that outputs actions conditioned on a specific sub-task RM belonging to a decomposition generated in section~\ref{subsec:generating_decompositions}. We can then deploy separate copies of our policy on each agent during execution time.
\paragraph{Policy Architecture} Our policy is represented by a feedforward neural network that is shared and learned across all agents. The neural policy receives four inputs, depending on the individual agent:
\begin{itemize}
    \item an observation from the underlying MDP (different for each agent);
    \item an encoding of the selected decomposition $d_j \in \decompositions$ for the current execution episode (same for each agent); 
    \item the current state in their sub-task $\rewmach_{i}^{j} \in d_j$ (different for each agent);
    \item an encoding of the overall task and the current state in the overall task (same for each agent).
\end{itemize}
% and outputs a distribution over actions. 

We visualize these inputs in Figure~\ref{fig:observation_viz} for an example decomposition of our Repairs task.
Learning a task-conditioned policy allows us to distinguish between different sub-tasks within different decompositions while exploiting the generalization capabilities of multi-task learning~\cite{qiu2023gcrlltl, vaezipoor2021ltl2action} as well as the natural curriculum learning inherent in decomposition exploration. Let us return to the running example from Figure \ref{fig:running_example}. Imagine that as part of a decomposition selected during training, agent $A_1$ is only assigned the task of meeting at HQ and nothing else. Even if this decomposition is not optimal, succeeding in this sub-task teaches $A_1$ the valuable skill of how to reach HQ. This experience will be useful in the future when $A_1$ is tasked with more complicated sub-tasks, such as \textit{``first go to HQ and then the red station"}.


Conditioning on the overall task allows us to relax the assumption of independent dynamics as we will describe in Section \ref{sec:indep_dyn}. In our experiments, we use one-hot encodings of the selected sub-task and the current position in the sub-task during the rollout. However, the embeddings could in principle be generated in any way, such as learned embeddings from a graph neural network that encode the structure of an RM~\cite{yalcinkaya2023automata}.


\subsection{Selecting decompositions during training}
\label{subsec:decomp_strategies}


A naive approach to our objective of finding an optimal ordered decomposition would be to learn a task-conditioned policy for each $d \in \decompositions$ independently, and then choose the decomposition that performs the best after a certain amount of training. As the number of candidate decomposition increases, this approach quickly becomes intractable. We seek to improve the efficiency of this process by simultaneously learning both the most efficient decomposition and the corresponding policies that achieve this decomposition. 

A key insight of our work is that we can use rewards from previous executions of a sub-task $\rewmach_{i}^{j}$ to estimate the satisfaction likelihood of that sub-task. These estimates, which we call \textit{value estimates}, are used as a heuristic in decomposition selection to assess how well a policy is performing on a specific sub-task. Our approach computes value estimates as an \textit{exponential weighted moving sum} of previous rewards, as more recent rewards are typically a more accurate reflection of the performance of the current policy.

We will denote the value estimates for a sub-task $\rewmach_{i}^{j}$ as $V_{\rewmach_{i}^{j}}$. On episode $H$ of training on decomposition $j$, we can compute a sub-task's value estimate as $V_{\rewmach_{i}^{j}} = \sum_{h=0}^H \alpha^{H-h} r_h$. Here, $ r_h$ represents the reward achieved by agent $i$ under sub-task $\rewmach_i$ from the $h$-th execution and $\alpha$ is a hyperparameter defining the decay rate.

With value estimates in hand for each agent policy in our team, on each sub-task, we can consider a number of heuristic approaches to utilize them. In this work, we use the Upper Confidence Bound (UCB) algorithm~\cite{auer02ucb}, which balances exploring different decompositions with exploiting higher scoring decomposition via a hyperparameter $\beta$. The score assigned to each decomposition $d_j$ is an average of the decomposition's current value estimates for each sub-task $\{V_{\rewmach_{1}^{j}}, \dots, V_{\rewmach_{n}^{j}} \}$.


We note that the value estimates early in training may be arbitrarily inaccurate due to the lack of progress made in learning the policy for different sub-tasks by each agent. When inaccuracy is high early in training, exploration is critical so that agents can sufficiently optimize towards achieving each candidate sub-task before the value estimates are exploited to converge on the optimal decomposition. Once the optimal decomposition is converged upon, additional training will further optimize the policy conditioned on its corresponding sub-tasks. 




\subsection{Dealing with dependent dynamics} \label{sec:indep_dyn}
Prior work in RM-guided MARL \cite{neary2020reward, smith2023automatic} assumes that the underlying dynamics of the MDP are independent between agents. This prevents the use of an MDP with dynamics that model collisions or interactions between agents unless the interaction is explicitly modeled by the task's reward machine, which requires knowing about the interaction a priori. 

Consider the motivating example visualized in Figure~\ref{fig:running_example}. The example includes dependent dynamics between the agents in the form of the hazardous region. Only one agent can occupy the region at a time, meaning that an agent's ability to enter the region is dependent on the other agents' positions and actions.



Prior works \cite{neary2020reward, smith2023automatic} require the assumption of independent dynamics so that the completion of an individual agent's reward machine is independent of the behavior of other agents. With dependent dynamics, this is rarely the case. For example, imagine that Agent 3 is assigned to reach the yellow target and Agent 2 is assigned to reach the red target. If Agent 3 enters the hazardous region and reaches the yellow target, it accomplishes its sub-task. Now, completion of the overall task only requires Agent 2 to reach the red target. Since the red target is located in the hazardous region, this requires Agent 3 to exit the hazardous region. However, Agent 3 has already accomplished its sub-task (and has no knowledge of Agent 2's task), so it has no incentive to leave the hazardous region.

We are able to relax the assumption of independent dynamics made in previous work \cite{neary2020reward, smith2023automatic} and handle environments such as the one in Figure~\ref{fig:running_example} by allowing agents to condition not only on the status of their sub-task, but on the status of the overall task as well. In addition, agents are given a small reward for the completion of the overall task during training along with their primary reward for completing their assigned sub-task. This reward structure incentivizes agents to condition on the status of other agent's tasks (e.g., that Agent 2 needs to go to red) so that the overall task can be completed (e.g., Agent 3 is incentivized to leave the hazardous region). 


\begin{figure*}[t]
    \centering
    % Top row of three images
    {
        \includegraphics[width=0.32\textwidth]{figures/cooperative_buttons_main_results.png}
        \label{fig:cyrus_results}
    }
    {
        \includegraphics[width=0.32\textwidth]{figures/4-buttons_main_results.png}
        \label{fig:fig:challenge_results}
    }
    {
        \includegraphics[width=0.32\textwidth]{figures/repairs_task_main_results.png}
        \label{fig:motivating_results}
    }
    {
        \includegraphics[width=0.32\textwidth]{figures/asymmetric-advantages_main_results.png}
        \label{fig:asymm_results}
    }
    {
        \includegraphics[width=0.32\textwidth]{figures/cramped-corridor_main_results.png}
        \label{fig:corridor_results}
    }
    \caption{Training curves for LOTaD and baseline methods in our experimental domains. Results are averaged over 5 random seeds.}
    \label{fig:combined_results}
\Description{Training curves for LOTaD and comparative baselines in our experimental domains.}
\end{figure*}