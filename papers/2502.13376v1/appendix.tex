\section{Limitations}\label{appendix:limitations}
Our proposed approach, LOTaD, searches through a pre-defined set of decompositions $\decompositions$ to efficiently find a decomposition that optimally solves the task. However, the quality of our found decomposition will only be as good as the quality of $\decompositions$. If all candidates in $\decompositions$ are highly suboptimal, our approach will be limited in its efficacy. To avoid this, we use on a decomposition generation technique that relies on heuristics to create balanced decompositions amongst agents. However, there is no way to ensure a priori that these decompositions will be successful in the context of the unknown MDP dynamics. Studying how the generation of $\decompositions$ affects the learning outcome of LOTaD is left as a compelling direction for future work.

One of LOTaD's key features is that it provides a \textit{global} view of the overall task to each agent, so that agents can coordinate within the MDP to solve the overall task, even after having solved their respective sub-tasks. Providing this global view (in the form of the overall task RM) requires agents to receive the same information from a labeling function when environment events occur. This level of information sharing may be impractical in some cases. However, we reiterate that this does not mean that control is shared across agents. 
%We expect that the labeling function can be implemented as a lightweight communication protocol 

% limitations/future work:
% * including monolithic rm encoding requires synchronizing on all tasks which can be limiting
% * our best decomp will only be as good as the best of the top k we generate
% * generating a 'diverse' set of candidates is interesting future work
% * also want to improve embeddings

% \section{Reward Machines}\label{appendix:reward_machines}
% Table \ref{tab:reward_machines} shows the reward machines for each task in our environments.
% \begin{table*}[h]
%     \centering
%     \begin{tabular}{|c|c|}
%         \hline
%         \textbf{Environment} & \textbf{Reward Machine Task}\\ 
%         \hline
%         Four-Buttons & \makecell{\centering \includegraphics[width=0.3\textwidth]{figures/2a_4b_rm.png}} \\
%         Flexible Teamwork & \makecell{\centering \includegraphics[width=0.3\textwidth]{figures/rm_slides_3_mono.jpeg}}  \\
%         Coordination & \makecell{\centering \includegraphics[width=0.3\textwidth]{figures/cyrus_rm_002.jpeg}}  \\
%         \hline
%     \end{tabular}
%     \caption{Reward machines for various environments.}
%     \label{tab:reward_machines}
% \end{table*}

\section{Additional experimental details}
\label{appendix:experiment_details}


\begin{figure*}[t]
    \centering
    \begin{minipage}{0.245\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/2agents_4buttons.png}
        % \caption{The Four Buttons environment with an example trajectory.}
        % \label{fig:4_buttons_env}
    \end{minipage}
    \hfill
    \begin{minipage}{0.245\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/coordinationtask.png}
        % \caption{The coordination task from~\cite{neary2020reward} with an example trajectory.}
        % \label{fig:cyrus_env}
    \end{minipage}
    \hfill
    \begin{minipage}{0.298\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/new_env_viz/custom_island.png}
        % \caption{ The Asymmetric-Advantages Overcooked Environment.}
        % \label{fig:overcooked_env_viz}
    \end{minipage}
    \hfill
    \begin{minipage}{0.19\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/new_env_viz/corridor.png}
        % \caption{The Cramped-Corridor Overcooked Environment.}
        % \label{fig:cyrus_env2}
    \end{minipage}
    \caption{From left to right: The Four-Buttons environment, The Cooperative Buttons environment, the Asymmetric-Advantages Overcooked Environment, and the Cramped-Corridor Overcooked Environment.}
    \label{fig:all_other_envs}
\Description{Environment visualizations.}
\end{figure*}

\subsection{Reward Machines}
In Table~\ref{tab:reward_machines}, we provide the task completion RMs used in each experimental domain. For the Overcooked domains (Cramped-Corridor and Asymmetric-Advantages), the same RM task is used. We omit sink state transitions from the Four-Buttons RM for the sake of visual clarity. These transitions would be taken if the event $R_B$ is triggered at states $\{u_{-1}, u_1, u_2, u_3, u_5, u_6, u_7, u_{10}\}$ and would lead to a sink (rejecting) state with no outgoing transitions.

As discussed in section~\ref{subsec:generating_decompositions}, we use the ATAD method from~\cite{smith2023automatic} to generate candidate decompositions. The ATAD method uses weight hyperparameters to balance the importance of condition (1) (decomposition size) and (2) (decomposition balance); we use weights 2 and 0.5 respectively in all experiments for both LOTaD and the ATAD baseline.
The sub-task RMs generated by this approach are called \textit{accident avoidance} RMs, and transition sub-task RMs to sink states if any event is experienced that does not belong to any local event set. We use these accident avoidance RMs as our sub-tasks, with the modification that and transitions to sink states receive a reward of $0$ rather than a reward of $-1$.

 ATAD also allows for the provision of forbidden and required event sets, which specify certain environment events that either cannot or must appear in a specific agent's local event set, respectively. The forbidden event sets for the Repairs environment were $A_1 = \{A_2\text{HQ}, \neg A_2\text{HQ},A_3\text{HQ}, \neg A_3\text{HQ}\}$, $A_2 = \{A_1\text{HQ}, \neg A_1\text{HQ},A_3\text{HQ}, \neg A_3\text{HQ}\}$, and $A_3 = \{A_2\text{HQ}, \neg A_2\text{HQ}, A_1\text{HQ}, \neg A_1\text{HQ}\}$. These forbidden event sets prohibit events that are specific to other agents from appearing in an agent's local event set. The required event sets for the Repairs environment are $\{ \text{Signal}\}$ for all agents. For the Cooperative Buttons task, the forbidden event sets are $A_1 = \{A_{2}^{R_B}, A_{2}^{\neg R_B}, A_{3}^{R_B}, A_{3}^{\neg R_B}\}$, $A_2 = \{A_{3}^{R_B}, A_{3}^{\neg R_B}\}$, and $A_3 = \{A_{2}^{R_B}, A_{2}^{\neg R_B}\}$, following a similar pattern to the Repairs environment. For the Four-Buttons task and the Overcooked task, both the forbidden and required event sets for all agents are empty. All decompositions generated by ATAD are required to comply with these forbidden and required event sets. We note that the provided forbidden and required event sets do not exploit any information about the environment and are independent of the MDP's dynamics.

 \begin{table*}[ht]
    \centering
    \begin{tabular}{lccccc}
        \hline
        \textbf{Hyperparameter} & \textbf{Four-Buttons} &  \textbf{Cooperative Buttons} & \textbf{Repairs Task} & \textbf{Cramped-Corridor} & \textbf{Asymmetric-Advantages} \\
        \hline
        Learning rate          & 5e-4   &  5e-4     & 5e-4   &2.5e-4  &2.5e-4\\
        Batch size             & 256      &  256        & 256    &256  & 256\\
        Number of epochs       & 5        &  5          & 5  & 5 & 5\\
        Discount               & 0.97      &  0.95       & 0.95  & 0.99 & 0.99\\
        Entropy Coefficient    & 0.1     &  0.1       & 0.1  & 0.01 & 0.01\\
        % GAE($\lambda$)         & 0.95      &  0.95       & 0.95   \\
        % Clipping $\epsilon$    & 0.2      &  0.2        & 0.2  \\
        Max. grad. norm.       & 0.5      & 0.5         & 10  & 0.5 & 0.5\\
        Value loss coef.       & 0.5      & 0.5         & 10  & 0.5 & 0.5\\
        \hline
    \end{tabular}
    \caption{PPO training hyperparameters.}
    \label{tab:ppo_hyperparameters}
\end{table*}

\begin{table*}[ht]
    \centering
    \begin{tabular}{lccccc}
        \hline
        \textbf{Hyperparameter} & \textbf{Four-Buttons} &  \textbf{Cooperative Buttons} & \textbf{Repairs Task} & \textbf{Cramped-Corridor} & \textbf{Asymmetric-Advantages} \\
        \hline
        Exponential Decay $\alpha$        & 1   &  1     & 1   & 1  & 1\\
        UCB Exploration $\beta$      & 0.5   &  0.5   & 0.5  & 0.5  & 0.5\\
        Num. Candidates $k$         & 10   &  10  & 10 & 10  & 10 \\
        \hline
        Maximum Episode Length      & 100   &  100  & 400 & 400  & 200 \\
        \hline
    \end{tabular}
    \caption{Additional experiment hyperparameters.}
    \label{tab:hyperparameters}
\end{table*}

\label{appendix:reward_machines}
\begin{table*}[ht]
    \centering
    \begin{tabular}{|c|c|}
        \hline
        \textbf{Environment} & \textbf{Reward Machine Task}\\ 
        \hline
        Four-Buttons & \makecell{\centering \includegraphics[width=0.5\textwidth]{figures/4buttons_rm.png}} \\
        Cooperative Buttons & \makecell{\centering \includegraphics[width=0.5\textwidth]{figures/cooper_buttons_rm.png}}  \\
        Overcooked & \makecell{\centering \includegraphics[width=0.5\textwidth]{figures/overcooked_rm_1.png}}  \\
        \hline
    \end{tabular}
    \caption{Reward machines for various environments.}
    \label{tab:reward_machines}
\end{table*}



\subsection{Hyperparameters and Environment Details}
\label{appendix:hyperparameters_env_details}
Table \ref{tab:hyperparameters} shows the PPO hyperparameters used for each training run in our experiments.

% \begin{figure}[t]
% \centering
% \includegraphics[width=0.6\columnwidth]{figures/2agents_4buttons.png}
% \caption{The Four Buttons environment with an example trajectory.}
% \label{fig:4_buttons_env}
% \end{figure} 


% \begin{figure}[t]
% \centering
% \includegraphics[width=0.6\columnwidth]{figures/Cyrus_recreate.png}
% \caption{The coordination task from~\cite{neary2020reward} with an example trajectory.}
% \label{fig:cyrus_env}
% \end{figure} 

% \begin{figure}
% \centering
%     \begin{minipage}[b]{0.49\textwidth}
%         \centering
%         \includegraphics[width=0.49\linewidth]{figures/new_env_viz/custom_island.png}
%     % \end{subfigure}%\hfill
%     \end{minipage}
%     % \hspace{-1.0em}
%     % \begin{subfigure}{0.31\linewidth}
%     \begin{minipage}[b]{0.49\textwidth}
%             \centering
%     \includegraphics[width=0.39\linewidth]{figures/new_env_viz/corridor.png}
%         % \includesvg[inkscapelatex=false,width=0.80\linewidth]{figures/ring_buchi.svg}
%     % \end{subfigure}
%     \end{minipage}
% \caption{(Top) The Asymmetric-Advantages Overcooked Environment. (Bottom) The Cramped-Corridor Overcooked Environment. Agents are represented by the red and blue triangles, onions are represented by the yellow circles, plates are represented by white circles, and the delivery stations are encoded by the green squares.}
% \label{fig:overcooked_env_viz}
% \end{figure} 


\paragraph{Cooperative Buttons} In the Cooperative Buttons task from~\cite{neary2020reward} used in our experiments, a goal region (denoted as `Goal') must be reached by any of the three agents. There are additionally three colored regions (red, yellow, and green), and three buttons of the same color present. In order for any agent to enter a colored region, the corresponding button must be pressed by at least one agent, otherwise, the attempt to enter that region results in a no-op. In the case of the red region, two agents must press the red button in order to enable an agent to traverse the red region. Walls (in black) separate Agent 1 from Agents 2 and 3. To reach the goal location, Agent 1 must first press the yellow button, which allows Agent 2 to traverse the yellow region to press the green button. Then, Agent 3 can traverse the green region, and both Agents 2 and 3 can press the red button, allowing Agent 1 to traverse the red region and reach the goal.

\paragraph{Four Buttons} In the Four-Buttons environment, a team of two agents must press the four buttons placed in the environment with an ordering constraint that the yellow button must be pressed prior to the red button.


\paragraph{Overcooked Environments} In the two Overcooked environments used in our experiments, the teams share the same task: three onions must be placed in the pot in order for the soup to start cooking. After a small number of timesteps have passed, the soup is considered cooked, and an agent must then pick up a plate and bring it to the pot in order to plate it. An agent then must take the plated soup and bring it to a delivery station. In the Asymmetric-Advantages environment, agents are in separate rooms, and both agents have shared access to a pot. Each agent has a distinct advantage (for example, the red agent can more easily place onions in the pot, whereas the blue agent can more quickly plate and deliver a cooked soup). In the Cramped-Corridor environment, only one agent may interact with the pot at a time, and the pot is placed in a small corridor. This can cause issues due to dependent agent dynamics: for example, if one agent finishes putting all of the onions in the pot, it must get out of the corridor either so the other agent can plate and deliver the soup.



% things to include:
% * experimental details
%     * difference between ours and sophias rms (no negative rewards)
%     * weights used for ATAD decomposition
%     * other hyperparameters: num candidates, geometric discounting gamma sum, discount factor
%     * what the forbidden and required sets are for decomposition generation


% overcooked:
% * we find that even in a simple reward machine, that does not allow any explicit parallelization of task completion encoded by the RM, decomposition is still useful. i.e. one does plate and deliver and can go fetch the plate while the other is doing onions. in another case they both see onions and can both contribute to onions, but only one is doing plate and deliver, so the other can get out of the way after completing its task. also provides more reward than the sparse overcook setting, and our inclusion of mono embedding does a good job.


