

Using a single reward signal for a team of agents in multi-agent reinforcement learning (MARL) can make it challenging for agents to understand how their individual behavior impacts the overall reward. This challenge in MARL is known as the \textit{credit assignment problem} \cite{agogino2004creditassignment} and can severely limit the effectiveness of naive reinforcement learning approaches in the multi-agent setting \cite{Oroojlooy2023cmarlsurvey, sunehag2017value}. 


One method for addressing the credit assignment problem is to formulate the task as a \textit{symbolic concept} that can be precisely decomposed into sub-tasks for assignment to individual agents~\cite{neary2020reward, smith2023automatic}. By using the reward signal from each sub-task, agents are credited for completing the specific sub-task they are assigned, even if the overall task is not achieved. %In this way, decomposition serves as a form of \textit{reward shaping} for partial completion of the task.

Previous literature has established sufficient conditions for ``valid" decompositions of multi-agent \textit{reward machines}~\cite{Icarte2020RewardMachine}, an automaton-based symbolic task structure, where satisfaction of the sub-tasks provably satisfies the overall task \cite{neary2020reward}. Human-designed decompositions that satisfy these validity conditions help the agents learn to accomplish the task more quickly in tabular settings. Further work demonstrates that valid decompositions can be automatically generated based on human-designed heuristics \cite{smith2023automatic}.


\begin{figure}[t]
\centering
\includegraphics[width=1\columnwidth]{figures/manager_viz_overcooked.pdf}
\caption{Visualization of our learning framework. At each new episode of training, a selection method chooses a possible symbolic decomposition of the task and assigns sub-tasks to each agent in the team. As each agent learns the viability of different sub-tasks, our selection method simultaneously finds the optimal task decomposition.}
\Description{Teaser image.}
\label{fig:overview}
\end{figure} 


These prior approaches to reward machine decomposition are limited when many possible decompositions exist. In these cases, the optimal decomposition of a task must be carefully designed and selected by a human. Selecting a meaningful task decomposition amongst many possible options often requires prior knowledge of the environment that is not assumed in standard RL, making the process tedious at best and impossible at worst. If a task decomposition is selected arbitrarily, the decomposition may not be compatible with the specifics of the environment to effectively break down the task. 
%In addition to this issue, prior approaches also make an assumption that the dynamics of each agent are independent of other agents, so that each agent can be trained individually in an environment absent of other agents. This assumption is often impractical: it is inefficient to train individual agents independently, and in many cases, the dynamics of each agent depend on others. For example, an agent that has completed their sub-task may obstruct another agent from the completion of their own sub-task.

\textbf{Motivating example.} Consider the ``Repairs'' environment in Figure~\ref{fig:running_example}, where a team of three robot agents must visit a number of communication stations in their environment to make repairs. First, any two of the agents must meet at headquarters (HQ, denoted by the control tower symbol), at which point a ready signal is sent out to both stations (red and yellow) to inform the stations that agents will be visiting each station to perform repairs. The agents are then tasked to visit these stations in any order. The agents can independently move around in grid in any of the four cardinal directions, or remain still.  We can formulate this task in the form of a \textit{Reward Machine} (RM), an automaton that encodes the objective over high level `events' which occur in specific states of the environment (visualized in Figure~\ref{fig:running_example}). RMs offer a precise notion of task completion and can easily capture temporal dependencies required in objectives (i.e., visit the stations \textit{after} HQ).

In the Repairs environment, there exists a hazardous region, encoded in orange, that prevents more than one agent from entering the region at a time. % and creates a dependency amongst the dynamics of each agent. 
The location and existence of this hazardous region is \textit{not} known to the agents a priori, which prevents this constraint from being encoded in the reward machine as part of the task description.


\begin{figure}[t]
\centering
    \begin{minipage}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=0.85\linewidth]{figures/motivating_example_final_1.png}
    % \end{subfigure}%\hfill
    \end{minipage}
    % \hspace{-1.0em}
    % \begin{subfigure}{0.31\linewidth}
    \begin{minipage}[b]{0.49\textwidth}
    \includegraphics[width=0.99\linewidth]{figures/motivating_ex_rm_new_1.png}
        % \includesvg[inkscapelatex=false,width=0.80\linewidth]{figures/ring_buchi.svg}
    % \end{subfigure}
    \end{minipage}
\caption{(Top) The ``Repairs'' MDP with a team of 3 agents. (Bottom) A task completion reward machine (RM) encoding the task: agents must navigate the environment to visit the HQ control tower, and then visit a set of communication stations. The goal state of the RM is denoted by concentric circles.}
\label{fig:running_example}
\Description{Visualization of our running example.}
\end{figure} 

This task, even with no prior knowledge of the environment, can naturally be decomposed in a number of ways. For example, Agent 1 and Agent 2 can be tasked with meeting at HQ, and then Agent 2 can visit the yellow station and leave the hazardous region while Agent 3 visits the red station. Alternatively, Agents 2 and 3 can meet at HQ, and then Agent 1 can visit both the red and yellow stations. Although many plausible decompositions exist, knowing which decomposition of the overall task leads to the most efficient completion of the task (i.e., is \textit{optimal}) largely depends on the dynamics of the underlying environment. In our case, Agent 3 happens to be closest to the red station, which might mean that assigning that station to them would be optimal. However, without knowledge of the layout of the environment, as is standard in a model-free setting, this would be impossible to know ahead of time.

%Without manual intervention and prior knowledge, we do not know how to decompose our objective into assigned sub-tasks for each agent to independently accomplish.

\textbf{Our Contributions.} In this work, we address the aforementioned limitations by introducing {\em an approach to automatically find optimal decompositions of an overall task into sub-tasks} for a team of agents. Our method is a lightweight extension of standard MARL and is applicable to model-free settings with no prior information about the environment or individual capabilities of the agents. 

We summarize our approach briefly as follows. At the beginning of training, we generate possible \textit{candidate} decompositions of our overall task as assignments of sub-tasks for our agents to accomplish. Then, during training, we explore selecting different decompositions for our team of agents and observe their performance as they attempt to learn a goal-conditioned policy that can achieve the variety of possible sub-tasks captured by our candidates. We record the value of agents' performances for different sub-tasks, and use this information to intelligently select subsequent decompositions for our agent team. This allows us to simultaneously learn (1) the optimal decomposition for our task and (2) policies for each agent that optimize said decomposition. We leverage selection strategies popularized in the multi-armed bandit algorithm literature \cite{katehakis1987multi, auer2002finite, audibert2009exploration} to balance exploring different candidate decompositions with exploiting the value our agents achieve as they learn sub-tasks during training. We visualize our approach in Figure~\ref{fig:overview}.

In addition to our decomposition selection strategy, we introduce a novel training setup for RMâ€“driven MARL that rectifies issues caused by dependent agent dynamics. Prior approaches \cite{neary2020reward, smith2023automatic} to RM-driven MARL make the assumption that the dynamics of each agent are independent of other agents, so that each agent can be trained individually in an environment absent from other agents. This assumption is often impractical: it is inefficient to train individual agents independently, and in many cases, multi-agent dynamics are codependent. For example, an agent that has completed their sub-task may obstruct another agent from the completion of their own sub-task. Our setup gives each agent in the team a \textit{global} view of the overall task along with their sub-task, enabling agents to learn policies that accomplish their individual sub-tasks and help facilitate the completion of other agent's sub-tasks as well. 

We summarize our contributions as follows: \textbf{(1)} We introduce a method for learning optimal decompositions from model-free interactions with the environment. \textbf{(2)} Within our method, we provide a novel training architecture that allows multiple agents to train simultaneously within an environment and avoids conflicts that arise due to dependent dynamics across agents.
\textbf{(3)} We demonstrate our approach's improvement in learning multi-agent policies against baseline approaches in several cooperative environments. 
Our results show that MARL benefits substantially from the improved credit assignment of task decompositions even when the optimal decomposition is not known \textit{a priori}.% and itself needs to be learned. 

