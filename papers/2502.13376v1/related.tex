\paragraph{Automaton-based task specifications in RL} 
An extensive body of work has explored automaton-based task specification for RL agents. Previous efforts have proposed RL approaches to policy learning for reward machines~\cite{icarte2022reward, Icarte2020RewardMachine, Camacho2019LTLAndBeyond} or automata-based representations of temporal logic~\cite{VoloshinLCP2022, voloshin2023eventual, hasanbeig2020deep, alur22ltlframework, sadigh2014learning, FuLTLPAC, jothimurugan2019composable, shah2024ltldeeprl} by augmenting the state space of the MDP. These efforts focus primarily on single-agent settings and are not designed to handle the MARL case with shared objectives or formally decompose the automaton.

\paragraph{(Symbolic) task decomposition for multi-agent teams}
Symbolic structures have been leveraged to facilitate efficient multi-agent learning in a variety of settings. A number of these approaches rely on a known dynamics model~\cite{karimadini2011cooperative, schillinger2016fltldecomposiiton, schillinger2018simultaneous} for planning-based approaches. In contrast, we assume no prior knowledge of the environment dynamics as is standard in reinforcement learning.

Closely related to our contributions, previous works explored learning decompositions of tasks for teams of agents. Some previous works explored decomposing symbolic tasks by hand designing decompositions or using task- and environment-agnostic heuristics \cite{neary2020reward, smith2023automatic}. However, these works either require extensive human involvement in determining the optimal decomposition or do not offer a way to choose a decomposition based on MDP dynamics. Moreover, these approaches are limited to MDPs in which the dynamics of agents are independent. Other works learn role assignments for traditional reward functions (i.e., non-symbolic) with multi-agent teams \cite{wang2020rode} or decompose traditional (non-Markovian) reward functions \cite{sun2020reinforcement} for teams of agents but these methods do not easily extend to non-Markovian rewards which we consider.

\paragraph{Credit assignment in multi-agent RL}
Credit assignment, originally explored in the single-agent setting \cite{sutton1984temporal, riedmiller2018learning, ferret2019self, van2021expected}, is a problem that has been extensively explored in multi-agent learning literature and can be divided between \textit{explicit} and \textit{implicit} solutions. Explicit credit assignment most closely resembles our work, typically assuming that value functions are given in a specific form that allows certain types of decompositions, such as additive decompositions \cite{nguyen2017policy}, or 
value factorization \cite{wang2021towards, sunehag2017value}. Other explicit methods are based on assuming a hierarchical execution structure \cite{agogino2004unifying, feng2022multi, rashid2020monotonic}.
Implicit credit assignment instead attempts to perform credit assignment without an explicit structure, for example, by deriving individual policy gradients for each agent derived from a centralized critic \cite{zhou2020learning}. %Our method differs from these works by exploring symbolic decompositions that provably satisfy the overall task.