\section*{Limitations}

\paragraph{Prompt Design and Evaluation Settings}
This study evaluates the performance of LLMs on CTM using various prompts, including the most common settings of direct prompting and chain-of-thought (CoT).
However, it is acknowledged that the effectiveness of these prompts may vary across different tasks and models.
Future work could explore the possibility of dynamically adapting prompt designs to better suit specific temporal reasoning tasks, as well as expanding to more diverse few-shot and zero-shot settings.
As LLMs continue to evolve, it will be crucial to periodically update prompt strategies to ensure a robust and comprehensive evaluation.

\paragraph{Dataset Scale and Coverage}
While CTM currently includes a diverse range of Chinese temporal reasoning tasks, there is significant potential for expanding both its size and coverage.
With 8,750 examples already developed, the dataset can be further enriched with larger and more complex temporal scenarios, as well as longer historical events and a broader range of question types.
Additionally, the timeline Ito game data could be expanded to incorporate more intricate details and interesting themes, providing greater challenges for models and revealing their strengths and limitations.
