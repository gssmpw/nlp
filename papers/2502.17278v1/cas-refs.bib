@ARTICLE{Fortunato2010,
  author  = {Fortunato, S.},
  title   = {Community detection in graphs},
  journal = {Phys. Rep.-Rev. Sec. Phys. Lett.}, 
  volume  = {486},
  year    = {2010},
  pages   = {75-174}
}

@ARTICLE{NewmanGirvan2004,
  author  = {Newman, M. E. J. and Girvan, M.},
  title   = {Finding and evaluating community structure in networks},
  journal = {Phys. Rev. E.}, 
  volume  = {69},
  year    = {2004},
  pages   = {026113}
}

@ARTICLE{Vehlowetal2013,
  author  = {Vehlow, C. and Reinhardt, T. and Weiskopf, D.},
  title   = {Visualizing Fuzzy Overlapping Communities in Networks},
  journal = {IEEE Trans. Vis. Comput. Graph.}, 
  volume  = {19},
  year    = {2013},
  pages   = {2486-2495}
}

@ARTICLE{Raghavanetal2007,
  author  = {Raghavan, U. and Albert, R. and Kumara, S.},
  title   = {Near linear time algorithm to detect community structures in large-scale networks},
  journal = {Phys. Rev E.}, 
  volume  = {76},
  year    = {2007},
  pages   = {036106}
}

@ARTICLE{SubeljBajec2011a,
  author  = {\v{S}ubelj, L. and Bajec, M.},
  title   = {Robust network community detection using balanced propagation},
  journal = {Eur. Phys. J. B.}, 
  volume  = {81},
  year    = {2011},
  pages   = {353-362}
}

@ARTICLE{Louetal2013,
  author  = {Lou, H. and Li, S. and Zhao, Y.},
  title   = {Detecting community structure using label propagation with weighted coherent neighborhood propinquity},
  journal = {Physica A.}, 
  volume  = {392},
  year    = {2013},
  pages   = {3095-3105}
}

@ARTICLE{Clausetetal2004,
  author  = {Clauset, A. and Newman, M. E. J. and Moore, C.},
  title   = {Finding community structure in very large networks},
  journal = {Phys. Rev. E.}, 
  volume  = {70},
  year    = {2004},
  pages   = {066111}
}

@ARTICLE{Blondeletal2008,
  author  = {Blondel, V. D. and Guillaume, J. L. and Lambiotte, R. and Lefebvre, E.},
  title   = {Fast unfolding of communities in large networks},
  journal = {J. Stat. Mech.-Theory Exp.}, 
  volume  = {2008},
  year    = {2008},
  pages   = {P10008}
}

@ARTICLE{SobolevskyCampari2014,
  author  = {Sobolevsky, S. and Campari, R.},
  title   = {General optimization technique for high-quality community detection in complex networks},
  journal = {Phys. Rev. E.}, 
  volume  = {90},
  year    = {2014},
  pages   = {012811}
}

@ARTICLE{FortunatoBarthelemy2007,
  author  = {Fortunato, S. and Barthelemy, M.},
  title   = {Resolution limit in community detection},
  journal = {Proc. Natl. Acad. Sci. U. S. A.}, 
  volume  = {104},
  year    = {2007},
  pages   = {36-41}
}

@ARTICLE{SubeljBajec2011b,
  author  = {\v{S}ubelj, L. and Bajec, M.},
  title   = {Unfolding communities in large complex networks: Combining defensive and offensive label propagation for core extraction},
  journal = {Phys. Rev. E.}, 
  volume  = {83},
  year    = {2011},
  pages   = {036103}
}

@ARTICLE{WangLi2013,
  author  = {Wang, X. and Li, J.},
  title   = {Detecting communities by the core-vertex and intimate degree in complex networks},
  journal = {Physica A.}, 
  volume  = {392},
  year    = {2013},
  pages   = {2555-2563}
}

@ARTICLE{Lietal2013,
  author  = {Li, J. and Wang, X. and Eustace, J.},
  title   = {Detecting overlapping communities by seed community in weighted complex networks},
  journal = {Physica A.}, 
  volume  = {392},
  year    = {2013},
  pages   = {6125-6134}
}

@ARTICLE{Fabioetal2013,
  author  = {Fabio, D. R. and Fabio, D. and Carlo, P.},
  title   = {Profiling core-periphery network structure by random walkers},
  journal = {Sci. Rep.}, 
  volume  = {3},
  year    = {2013},
  pages   = {1467}
}

@ARTICLE{Chenetal2013,
  author  = {Chen, Q. and Wu, T. T. and Fang, M.},
  title   = {Detecting local community structure in complex networks based on local degree central nodes},
  journal = {Physica A.}, 
  volume  = {392},
  year    = {2013},
  pages   = {529-537}
}

@ARTICLE{Zhangetal2007,
  author  = {Zhang, S. and Wang, R. and Zhang, X.},
  title   = {Identification of overlapping community structure in complex networks using fuzzy c-means clustering},
  journal = {Physica A.}, 
  volume  = {374},
  year    = {2007},
  pages   = {483-490}
}

@ARTICLE{Nepuszetal2008,
  author  = {Nepusz, T. and Petr\'oczi, A. and N\'egyessy, L. and Bazs\'o, F.},
  title   = {Fuzzy communities and the concept of bridgeness in complex networks},
  journal = {Phys. Rev. E.}, 
  volume  = {77},
  year    = {2008},
  pages   = {016107}
}

@ARTICLE{FabricioLiang2013,
  author  = {Fabricio, B. and Liang, Z.},
  title   = {Fuzzy community structure detection by particle competition and cooperation},
  journal = {Soft Comput.}, 
  volume  = {17},
  year    = {2013},
  pages   = {659-673}
}

@ARTICLE{Sunetal2011,
  author  = {Sun, P. and Gao, L. and Han, S.},
  title   = {Identification of overlapping and non-overlapping community structure by fuzzy clustering in complex networks},
  journal = {Inf. Sci.}, 
  volume  = {181},
  year    = {2011},
  pages   = {1060-1071}
}

@ARTICLE{Wangetal2013,
  author  = {Wang, W. and Liu, D. and Liu, X. and Pan, L.},
  title   = {Fuzzy overlapping community detection based on local random walk and multidimensional scaling},
  journal = {Physica A.}, 
  volume  = {392},
  year    = {2013},
  pages   = {6578-6586}
}

@ARTICLE{Psorakisetal2011,
  author  = {Psorakis, I. and Roberts, S. and Ebden, M. and Sheldon, B.},
  title   = {Overlapping community detection using Bayesian non-negative matrix factorization},
  journal = {Phys. Rev. E.}, 
  volume  = {83},
  year    = {2011},
  pages   = {066114}
}

@CONFERENCE{ZhangYeung2012,
  author  = {Zhang, Y. and Yeung, D.},
  title   = {Overlapping Community Detection via Bounded Nonnegative Matrix Tri-Factorization},
  booktitle = {In Proc. ACM SIGKDD Conf.}, 
  year    = {2012},
  pages   = {606-614}
}

@ARTICLE{Liu2010,
  author  = {Liu, J.},
  title   = {Fuzzy modularity and fuzzy community structure in networks},
  journal = {Eur. Phys. J. B.}, 
  volume  = {77},
  year    = {2010},
  pages   = {547-557}
}

@ARTICLE{Havensetal2013,
  author  = {Havens, T. C. and Bezdek, J. C. and Leckie, C., Ramamohanarao, K. and Palaniswami, M.},
  title   = {A Soft Modularity Function For Detecting Fuzzy Communities in Social Networks},
  journal = {IEEE Trans. Fuzzy Syst.}, 
  volume  = {21},
  year    = {2013},
  pages   = {1170-1175}
}

@misc{Newman2013,
  author = {Newman, M. E. J.},
  title  = {Network data},
  howpublished = "\url{http://www-personal.umich.edu/~mejn/netdata/}",
  year = {2013}
}

@ARTICLE{SubeljBajec2012,
  author  = {\v{S}ubelj, L. and Bajec, M.},
  title   = {Ubiquitousness of link-density and link-pattern communities in real-world networks},
  journal = {Eur. Phys. J. B.}, 
  volume  = {85},
  year    = {2012},
  pages   = {1-11}
}

@ARTICLE{Lancichinettietal2008,
  author  = {Lancichinetti, A. and Fortunato, S. and Radicchi, F.},
  title   = {Benchmark graphs for testing community detection algorithms},
  journal = {Phys. Rev. E.}, 
  volume  = {78},
  year    = {2008},
  pages   = {046110}
}

@ARTICLE{Liuetal2014,
  author  = {Liu, W. and Pellegrini, M. and Wang, X.},
  title   = {Detecting Communities Based on Network Topology},
  journal = {Sci. Rep.}, 
  volume  = {4},
  year    = {2014},
  pages   = {5739}
}

@ARTICLE{Danonetal2005,
  author  = {Danon, L. and Diaz-Guilera, A. and Duch, J. and Arenas, A.},
  title   = {Comparing community structure identification},
  journal = {J. Stat. Mech.-Theory Exp.}, 
  volume  = {},
  year    = {2005},
  pages   = {P09008}
}

@ARTICLE{Gregory2011,
  author  = {Gregory, S.},
  title   = {Fuzzy overlapping communities in networks},
  journal = {J. Stat. Mech.-Theory Exp.}, 
  volume  = {},
  year    = {2011},
  pages   = {P02017}
}

@ARTICLE{LancichinettiFortunato2009,
  author  = {Lancichinetti, A. and Fortunato, S.},
  title   = {Benchmarks for testing community detection algorithms on directed and weighted graphs with overlapping communities},
  journal = {Phys. Rev. E.}, 
  volume  = {80},
  year    = {2009},
  pages   = {016118}
}

@CONFERENCE{HullermeierRifqi2009,
  author  = {Hullermeier, E. and Rifqi, M.},
  title   = {A Fuzzy Variant of the Rand Index for Comparing Clustering Structures},
  booktitle = {in Proc. IFSA/EUSFLAT Conf.}, 
  year    = {2009},
  pages   = {1294-1298}
}

@article{vaswani2017attention,
  title={{Attention Is All You Need}},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
  journal={arXiv preprint arXiv:1706.03762},
  year={2017}
}

@article{yang2019xlnet,
  title={{XLNet: Generalized Autoregressive Pretraining for Language Understanding}},
  author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V},
  journal={arXiv preprint arXiv:1906.08237},
  year={2019}
}

@article{justeson1995technical,
  title={Technical Terminology: Some Linguistic Properties and an Algorithm for Identification in Text},
  author={Justeson, John S and Katz, Slava M},
  journal={Natural Language Engineering},
  volume={1},
  number={1},
  pages={9--27},
  year={1995},
  publisher={Cambridge University Press}
}

@article{vintar2010bilingual,
  title={Bilingual Term Recognition Revisited: The Bag-of-equivalents Term Alignment Approach and its Evaluation},
  author={Vintar, Spela},
  journal={Terminology. International Journal of Theoretical and Applied Issues in Specialized Communication},
  volume={16},
  number={2},
  pages={141--158},
  year={2010},
  publisher={John Benjamins}
}

@article{kageura1996methods,
  title={Methods of Automatic Term Recognition. A Review},
  author={Kageura, Kyo and Umino, Bin},
  journal={Terminology. International Journal of Theoretical and Applied Issues in Specialized Communication},
  volume={3},
  number={2},
  pages={259--289},
  year={1996},
  publisher={John Benjamins}
}


@inproceedings{daille1994towards,
  title={Towards Automatic Extraction of Monolingual and Bilingual Terminology},
  author={Daille, B{\'e}atrice and Gaussier, {\'E}ric and Lang{\'e}, Jean-Marc},
  booktitle={COLING 1994 Volume 1: The 15th International Conference on Computational Linguistics},
  year={1994}
}

@article{repar2019termensembler,
  title={Term{E}nsembler: An Ensemble Learning Approach to Bilingual Term Extraction and Alignment},
  author={Repar, Andra{\v{z}} and Podpe{\v{c}}an, Vid and Vavpeti{\v{c}}, An{\v{z}}e and Lavra{\v{c}}, Nada and Pollak, Senja},
  journal={Terminology. International Journal of Theoretical and Applied Issues in Specialized Communication},
  volume={25},
  number={1},
  pages={93--120},
  year={2019},
  publisher={John Benjamins}
}

@article{meyers2018termolator,
  title={The {T}ermolator: Terminology Recognition Based on Chunking, Statistical and Search-Based Scores},
  author={Meyers, Adam L and He, Yifan and Glass, Zachary and Ortega, John and Liao, Shasha and Grieve-Smith, Angus and Grishman, Ralph and Babko-Malaya, Olga},
  journal={Frontiers in Research Metrics and Analytics},
  volume={3},
  pages={19},
  year={2018},
  publisher={Frontiers}
}

@inproceedings{rigouts2020termeval,
  title={Term{E}val 2020: Shared Task on Automatic Term Extraction Using the Annotated Corpora for Term Extraction Research ({ACTER}) Dataset},
  author={Rigouts Terryn, Ayla and Hoste, Veronique and Drouin, Patrick and Lefever, Els},
  booktitle={Proceedings of the 6th International Workshop on Computational Terminology},
  pages={85--94},
  year={2020}
}

@article{martin2019camembert,
  title={{CamemBERT: a Tasty French Language Model}},
  author={Martin, Louis and Muller, Benjamin and Su{\'a}rez, Pedro Javier Ortiz and Dupont, Yoann and Romary, Laurent and de la Clergerie, {\'E}ric Villemonte and Seddah, Djam{\'e} and Sagot, Beno{\^\i}t},
  journal={arXiv preprint arXiv:1911.03894},
  year={2019}
}


@inproceedings{devlin2018bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@article{sanh2019distilbert,
  title={{DistilBERT, a Distilled Version of BERT: Smaller, Faster, Cheaper and Lighter}},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}

@article{liu2019roberta,
  title={{RoBERTa: A Robustly Optimized BERT Pretraining Approach}},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}


@article{terryn2020no,
  title={In No Uncertain Terms: A Dataset for Monolingual and Multilingual Automatic Term Extraction from Comparable Corpora},
  author={Terryn, Ayla Rigouts and Hoste, V{\'e}ronique and Lefever, Els},
  journal={Language Resources and Evaluation},
  volume={54},
  number={2},
  pages={385--418},
  year={2020},
  publisher={Springer}
}

@inproceedings{amjadian2016local,
  title={Local-Global Vectors to Improve Unigram Terminology Extraction},
  author={Amjadian, Ehsan and Inkpen, Diana and Paribakht, Tahereh and Faez, Farahnaz},
  booktitle={Proceedings of the 5th International Workshop on Computational Terminology},
  pages={2--11},
  year={2016}
}

@inproceedings{wang2016featureless,
  title={Featureless Domain-Specific Term Extraction with Minimal Labelled Data},
  author={Wang, Rui and Liu, Wei and McDonald, Chris},
  booktitle={Proceedings of the Australasian Language Technology Association Workshop},
  pages={103--112},
  year={2016}
}

@inproceedings{khan2016term,
  title={{T}erm {R}anker: A Graph-Based Re-Ranking Approach},
  author={Khan, Muhammad Tahir and Ma, Yukun and Kim, Jung-jae},
  booktitle={Proceedings of the Twenty-Ninth International Florida Artificial Intelligence Research Society Conference},
  pages={310--315},
  year={2016}
}

@article{zhang2017semre,
author = {Zhang, Ziqi and Gao, Jie and Ciravegna, Fabio},
year = {2018},
month = {03},
pages = {1--41},
title = {Sem{R}e-{R}ank: Improving Automatic Term Extraction By Incorporating Semantic Relatedness With Personalised PageRank},
volume = {12},
journal = {ACM Transactions on Knowledge Discovery from Data}
}

@inproceedings{kucza2018term,
  title={Term Extraction via Neural Sequence Labeling a Comparative Evaluation of Strategies Using Recurrent Neural Networks},
  author={Kucza, Maren and Niehues, Jan and Zenkel, Thomas and Waibel, Alex and St{\"u}ker, Sebastian},
  booktitle={Proceedings of INTERSPEECH},
  pages={2072--2076},
  year={2018}
}

@inproceedings{gao2019feature,
  title={Feature-less End-to-end Nested Term extraction},
  author={Gao, Yuze and Yuan, Yu},
  booktitle={Proceedings of the CCF International Conference on Natural Language Processing and Chinese Computing},
  pages={607--616},
  year={2019}
}

@inproceedings{puais2020termeval,
  title={{TermEval} 2020: {RACAI}’s Automatic Term Extraction System},
  author={P{\u{a}}iș, Vasile and Ion, Radu},
  booktitle={Proceedings of the 6th International Workshop on Computational Terminology},
  pages={101--105},
  year={2020}
}

@inproceedings{hazem2020termeval,
  title={{TermEval} 2020: {TALN-LS2N} System for Automatic Term Extraction},
  author={Hazem, Amir and Bouhandi, M{\'e}rieme and Boudin, Florian and Daille, B{\'e}atrice},
  booktitle={Proceedings of the 6th International Workshop on Computational Terminology},
  pages={95--100},
  year={2020}
}

@inproceedings{johnson2000iate,
  title={{IATE-Inter-Agency Terminology Exchange: Development of a Single Central Terminology Database for the Institutions and Agencies of the European Union}},
  author={Johnson, Ian and Macphail, Alastair},
  booktitle={Workshop on Terminology resources and computation},
  year={2000}
}

@article{chiu2016named,
  title={{Named Entity Recognition with Bidirectional LSTM-CNNs}},
  author={Chiu, Jason PC and Nichols, Eric},
  journal={Transactions of the Association for Computational Linguistics},
  volume={4},
  pages={357--370},
  year={2016},
  publisher={MIT Press}
}

@inproceedings{lample2016neural,
  title={{Neural Architectures for Named Entity Recognition}},
  author={Lample, Guillaume and Ballesteros, Miguel and Subramanian, Sandeep and Kawakami, Kazuya and Dyer, Chris},
  booktitle={Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={260--270},
  year={2016}
}

@inproceedings{ma2016end,
  title={{End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF}},
  author={Ma, Xuezhe and Hovy, Eduard H},
  booktitle={ACL (1)},
  year={2016}
}

@inproceedings{liu2018empower,
  title={{Empower Sequence Labeling with Task-aware Neural Language Model}},
  author={Liu, Liyuan and Shang, Jingbo and Ren, Xiang and Xu, Frank F and Gui, Huan and Peng, Jian and Han, Jiawei},
  booktitle={32nd AAAI Conference on Artificial Intelligence, AAAI 2018},
  pages={5253--5260},
  year={2018},
  organization={AAAI Press}
}

@article{leng2016deep,
  title={{A Deep Learning Approach for Relationship Extraction from Interaction Context in Social Manufacturing Paradigm}},
  author={Leng, Jiewu and Jiang, Pingyu},
  journal={Knowledge-Based Systems},
  volume={100},
  pages={188--199},
  year={2016},
  publisher={Elsevier}
}

@inproceedings{aquino2014keyword,
  title={{Keyword Extracting using Auto-associative Neural Networks}},
  author={Aquino, Germ{\'a}n Osvaldo and Hasperu{\'e}, Waldo and Lanzarini, Laura Cristina},
  booktitle={XX Congreso Argentino de Ciencias de la Computaci{\'o}n (Buenos Aires, 2014)},
  year={2014}
}

@article{huang2015bidirectional,
  title={{Bidirectional LSTM-CRF Models for Sequence Tagging}},
  author={Huang, Zhiheng and Xu, Wei and Yu, Kai},
  journal={arXiv preprint arXiv:1508.01991},
  year={2015}
}
@inproceedings{sundermeyer2012lstm,
  title={{LSTM Neural Networks for Language Modeling}},
  author={Sundermeyer, Martin and Schl{\"u}ter, Ralf and Ney, Hermann},
  booktitle={Thirteenth annual conference of the international speech communication association},
  year={2012}
}

@inproceedings{alzaidy2019bi,
  title={{Bi-LSTM-CRF Sequence Labeling for Keyphrase Extraction from Scholarly Documents}},
  author={Alzaidy, Rabah and Caragea, Cornelia and Giles, C Lee},
  booktitle={The world wide web conference},
  pages={2551--2557},
  year={2019}
}

@inproceedings{ljubevsic2019kas,
  title={Kas-term: Extracting {S}lovene Terms from Doctoral Theses via Supervised Machine Learning},
  author={Ljube{\v{s}}i{\'c}, Nikola and Fi{\v{s}}er, Darja and Erjavec, Toma{\v{z}}},
  booktitle={International Conference on Text, Speech, and Dialogue},
  pages={115--126},
  year={2019},
  organization={Springer}
}

@article{rigouts2021hamlet,
  title={{HAMLET}: Hybrid Adaptable Machine Learning approach to Extract Terminology},
  author={Rigouts Terryn, Ayla and Hoste, V{\'e}ronique and Lefever, Els},
  journal={Terminology. International Journal of Theoretical and Applied Issues in Specialized Communication},
  year={2021},
  volume={27},
  number={2},
  pages={254--293},
  publisher={John Benjamins}
}


@article{10.1162/coli_a_00402,
    author = {de Marneffe, Marie-Catherine and Manning, Christopher D. and Nivre, Joakim and Zeman, Daniel},
    title = "{Universal Dependencies}",
    journal = {Computational Linguistics},
    volume = {47},
    number = {2},
    pages = {255-308},
    year = {2021},
    month = {07},
    abstract = "{Universal dependencies (UD) is a framework for morphosyntactic annotation of human language, which to date has been used to create treebanks for more than 100 languages. In this article, we outline the linguistic theory of the UD framework, which draws on a long tradition of typologically oriented grammatical theories. Grammatical relations between words are centrally used to explain how predicate–argument structures are encoded morphosyntactically in different languages while morphological features and part-of-speech classes give the properties of words. We argue that this theory is a good basis for crosslinguistically consistent annotation of typologically diverse languages in a way that supports computational natural language understanding as well as broader linguistic studies.}",
    issn = {0891-2017},
    
    
}



@inproceedings{krek2020gigafida,
  title={Gigafida 2.0: The Reference Corpus of Written Standard {S}lovene},
  author={Krek, Simon and Holdt, {\v{S}}pela Arhar and Erjavec, Toma{\v{z}} and {\v{C}}ibej, Jaka and Repar, Andra{\v{z}} and Gantar, Polona and Ljube{\v{s}}i{\'c}, Nikola and Kosem, Iztok and Dobrovoljc, Kaja},
  booktitle={Proceedings of the 12th Language Resources and Evaluation Conference},
  pages={3340--3345},
  year={2020}
}

 @misc{11356/1277,
 title = {{ELMo} embeddings models for seven languages},
 author = {Ul{\v c}ar, Matej},
 url = {http://hdl.handle.net/11356/1277},
 note = {{S}lovenian language resource repository {CLARIN}.{SI}},
 copyright = {Apache License 2.0},
 year = {2019} }
 
  @misc{11356/1035,
 title = {Written corpus {ccGigafida} 1.0},
 author = {Logar, Nata{\v s}a and Erjavec, Toma{\v z} and Krek, Simon and Gr{\v c}ar, Miha and Holozan, Peter},
 url = {http://hdl.handle.net/11356/1035},
 note = {{S}lovenian language resource repository {CLARIN}.{SI}},
 copyright = {Creative Commons - Attribution-{NonCommercial}-{ShareAlike} 4.0 International ({CC} {BY}-{NC}-{SA} 4.0)},
 year = {2013} }
 
 @inproceedings{lang2021transforming,
  title={Transforming Term Extraction: Transformer-Based Approaches to Multilingual Term Extraction Across Domains},
  author={Lang, Christian and Wachowiak, Lennart and Heinisch, Barbara and Gromann, Dagmar},
  booktitle={Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021},
  pages={3607--3620},
  year={2021}
}

@article{meyes2019ablation,
  title={Ablation studies in artificial neural networks},
  author={Meyes, Richard and Lu, Melanie and de Puiseau, Constantin Waubert and Meisen, Tobias},
  journal={arXiv preprint arXiv:1901.08644},
  year={2019}
}

@inproceedings{peters-etal-2018-deep,
    title = "Deep Contextualized Word Representations",
    author = "Peters, Matthew E.  and
      Neumann, Mark  and
      Iyyer, Mohit  and
      Gardner, Matt  and
      Clark, Christopher  and
      Lee, Kenton  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    pages = "2227--2237",
    abstract = "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
}

@inproceedings{karan2012evaluation,
  title={Evaluation of Classification Algorithms and Features for Collocation Extraction in {C}roatian},
  author={Karan, Mladen and {\v{S}}najder, Jan and Ba{\v{s}}ic, Bojana Dalbelo},
  booktitle={Proceedings of the 12th Language Resources and Evaluation Conference},
  pages={657--662},
  year={2012}
}

@inproceedings{foo2010using,
  title={Using machine learning to perform automatic term recognition},
  author={Foo, Jody and Merkel, Magnus},
  booktitle={LREC 2010 Workshop on Methods for automatic acquisition of Language Resources and their evaluation methods},
  pages={49--54},
  year={2010},
  organization={European Language Resources Association}
}


@inproceedings{qasemizadeh2014evaluation,
    title = "Evaluation of Technology Term Recognition with Random Indexing",
    author = "Zadeh, Behrang  and
      Handschuh, Siegfried",
    booktitle = "Proceedings of the Ninth International Conference on Language Resources and Evaluation",
    month = may,
    year = "2014",
    abstract = "In this paper, we propose a method that combines the principles of automatic term recognition and the distributional hypothesis to identify technology terms from a corpus of scientific publications. We employ the random indexing technique to model terms{'} surrounding words, which we call the context window, in a vector space at reduced dimension. The constructed vector space and a set of reference vectors, which represents manually annotated technology terms, in a k-nearest-neighbour voting classification scheme are used for term classification. In this paper, we examine a number of parameters that influence the obtained results. First, we inspect several context configurations, i.e. the effect of the context window size, the direction in which co-occurrence counts are collected, and information about the order of words within the context windows. Second, in the k-nearest-neighbour voting scheme, we study the role that neighbourhood size selection plays, i.e. the value of k. The obtained results are similar to word space models. The performed experiments suggest the best performing context are small (i.e. not wider than 3 words), are extended in both directions and encode the word order information. Moreover, the accomplished experiments suggest that the obtained results, to a great extent, are independent of the value of k.",
}

@inproceedings{pinnis2012term,
  title={Term extraction, tagging, and mapping tools for under-resourced languages},
  author={Pinnis, Marcis and Ljube{\v{s}}ic, Nikola and Stefanescu, Dan and Skadina, Inguna and Tadic, Marko and Gornostay, Tatiana},
  booktitle={Proceedings of the 10th Conference on Terminology and Knowledge Engineering},
  pages={20--21},
  year={2012}
}

@inproceedings{fivser2016terminology,
  title={Terminology extraction for academic {S}lovene using {S}ketch {E}ngine},
  author={Fi{\v{s}}er, Darja and Suchomel, V{\'\i}t and Jakub{\'\i}cek, Milo{\v{s}}},
  booktitle={Tenth Workshop on Recent Advances in Slavonic Natural Language Processing, RASLAN 2016},
  pages={135--141},
  year={2016}
}

@article{frantzi2000automatic,
  title={Automatic Recognition of Multi-word Terms: The {C}-value/ {NC}-value Method},
  author={Frantzi, Katerina and Ananiadou, Sophia and Mima, Hideki},
  journal={International Journal on Digital Libraries},
  volume={3},
  number={2},
  pages={115--130},
  year={2000},
  publisher={Springer}
}

@inproceedings{inproceedings,
author = {Frantzi, Katerina and Ananiadou, Sophia and Mima, Hideki},
year = {2000},
month = {08},
pages = {115-130},
title = {Automatic Recognition of Multi-word Terms: The C-value/ NC-value Method},
volume = {3},
isbn = {978-3-540-65101-7},
journal = {Int. J. on Digital Libraries}
}

@misc{11356/1400,
 title = {Corpus of term-annotated texts {RSDO5} 1.0},
 author = {Jemec Toma{\v z}in, Mateja and Trojar, Mitja and {\v Z}agar, Mojca and Atel{\v s}ek, Simon and Fajfar, Tatjana and Erjavec, Toma{\v z}},
 url = {http://hdl.handle.net/11356/1400},
 note = {{S}lovenian language resource repository {CLARIN}.{SI}},
 copyright = {Creative Commons - Attribution-{ShareAlike} 4.0 International ({CC} {BY}-{SA} 4.0)},
 year = {2021} }
 
@inproceedings{ljubesic-dobrovoljc-2019-neural,
    title = "What does Neural Bring? {A}nalysing Improvements in Morphosyntactic Annotation and Lemmatisation of {S}lovenian, {C}roatian and {S}erbian",
    author = "Ljube{\v{s}}i{\'c}, Nikola  and
      Dobrovoljc, Kaja",
    booktitle = "Proceedings of the 7th Workshop on Balto-Slavic Natural Language Processing",
    month = aug,
    year = "2019",
    pages = "29--34"
    }

@inproceedings{Gardner2017AllenNLP,
    title = "{A}llen{NLP}: A Deep Semantic Natural Language Processing Platform",
    author = "Gardner, Matt  and
      Grus, Joel  and
      Neumann, Mark  and
      Tafjord, Oyvind  and
      Dasigi, Pradeep  and
      Liu, Nelson F.  and
      Peters, Matthew  and
      Schmitz, Michael  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of Workshop for {NLP} Open Source Software ({NLP}-{OSS})",
    month = jul,
    year = "2018",
    pages = "1--6",
    abstract = "Modern natural language processing (NLP) research requires writing code. Ideally this code would provide a precise definition of the approach, easy repeatability of results, and a basis for extending the research. However, many research codebases bury high-level parameters under implementation details, are challenging to run and debug, and are difficult enough to extend that they are more likely to be rewritten. This paper describes AllenNLP, a library for applying deep learning methods to NLP research that addresses these issues with easy-to-use command-line tools, declarative configuration-driven experiments, and modular NLP abstractions. AllenNLP has already increased the rate of research experimentation and the sharing of NLP components at the Allen Institute for Artificial Intelligence, and we are working to have the same impact across the field.",
}

 @misc{11356/1198,
 title = {Terminology identification dataset {KAS}-term 1.0},
 author = {Erjavec, Toma{\v z} and Fi{\v s}er, Darja and Ljube{\v s}i{\'c}, Nikola and Arhar Holdt, {\v S}pela and Bren, Urban and Robnik-{\v S}ikonja, Marko and Udovi{\v c}, Bo{\v s}tjan},
 url = {http://hdl.handle.net/11356/1198},
 note = {{S}lovenian language resource repository {CLARIN}.{SI}},
 copyright = {Creative Commons - Attribution-{ShareAlike} 4.0 International ({CC} {BY}-{SA} 4.0)},
 year = {2018} }
 
 @article{reimers2019alternative,
  title={Alternative weighting schemes for {ELMo} embeddings},
  author={Reimers, Nils and Gurevych, Iryna},
  journal={arXiv preprint arXiv:1904.02954},
  year={2019}
}