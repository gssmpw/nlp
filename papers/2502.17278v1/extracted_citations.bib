@misc{11356/1198,
 title = {Terminology identification dataset {KAS}-term 1.0},
 author = {Erjavec, Toma{\v z} and Fi{\v s}er, Darja and Ljube{\v s}i{\'c}, Nikola and Arhar Holdt, {\v S}pela and Bren, Urban and Robnik-{\v S}ikonja, Marko and Udovi{\v c}, Bo{\v s}tjan},
 url = {http://hdl.handle.net/11356/1198},
 note = {{S}lovenian language resource repository {CLARIN}.{SI}},
 copyright = {Creative Commons - Attribution-{ShareAlike} 4.0 International ({CC} {BY}-{SA} 4.0)},
 year = {2018} }

@inproceedings{amjadian2016local,
  title={Local-Global Vectors to Improve Unigram Terminology Extraction},
  author={Amjadian, Ehsan and Inkpen, Diana and Paribakht, Tahereh and Faez, Farahnaz},
  booktitle={Proceedings of the 5th International Workshop on Computational Terminology},
  pages={2--11},
  year={2016}
}

@inproceedings{daille1994towards,
  title={Towards Automatic Extraction of Monolingual and Bilingual Terminology},
  author={Daille, B{\'e}atrice and Gaussier, {\'E}ric and Lang{\'e}, Jean-Marc},
  booktitle={COLING 1994 Volume 1: The 15th International Conference on Computational Linguistics},
  year={1994}
}

@inproceedings{devlin2018bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@inproceedings{fivser2016terminology,
  title={Terminology extraction for academic {S}lovene using {S}ketch {E}ngine},
  author={Fi{\v{s}}er, Darja and Suchomel, V{\'\i}t and Jakub{\'\i}cek, Milo{\v{s}}},
  booktitle={Tenth Workshop on Recent Advances in Slavonic Natural Language Processing, RASLAN 2016},
  pages={135--141},
  year={2016}
}

@inproceedings{foo2010using,
  title={Using machine learning to perform automatic term recognition},
  author={Foo, Jody and Merkel, Magnus},
  booktitle={LREC 2010 Workshop on Methods for automatic acquisition of Language Resources and their evaluation methods},
  pages={49--54},
  year={2010},
  organization={European Language Resources Association}
}

@article{frantzi2000automatic,
  title={Automatic Recognition of Multi-word Terms: The {C}-value/ {NC}-value Method},
  author={Frantzi, Katerina and Ananiadou, Sophia and Mima, Hideki},
  journal={International Journal on Digital Libraries},
  volume={3},
  number={2},
  pages={115--130},
  year={2000},
  publisher={Springer}
}

@inproceedings{gao2019feature,
  title={Feature-less End-to-end Nested Term extraction},
  author={Gao, Yuze and Yuan, Yu},
  booktitle={Proceedings of the CCF International Conference on Natural Language Processing and Chinese Computing},
  pages={607--616},
  year={2019}
}

@inproceedings{hazem2020termeval,
  title={{TermEval} 2020: {TALN-LS2N} System for Automatic Term Extraction},
  author={Hazem, Amir and Bouhandi, M{\'e}rieme and Boudin, Florian and Daille, B{\'e}atrice},
  booktitle={Proceedings of the 6th International Workshop on Computational Terminology},
  pages={95--100},
  year={2020}
}

@article{justeson1995technical,
  title={Technical Terminology: Some Linguistic Properties and an Algorithm for Identification in Text},
  author={Justeson, John S and Katz, Slava M},
  journal={Natural Language Engineering},
  volume={1},
  number={1},
  pages={9--27},
  year={1995},
  publisher={Cambridge University Press}
}

@article{kageura1996methods,
  title={Methods of Automatic Term Recognition. A Review},
  author={Kageura, Kyo and Umino, Bin},
  journal={Terminology. International Journal of Theoretical and Applied Issues in Specialized Communication},
  volume={3},
  number={2},
  pages={259--289},
  year={1996},
  publisher={John Benjamins}
}

@inproceedings{karan2012evaluation,
  title={Evaluation of Classification Algorithms and Features for Collocation Extraction in {C}roatian},
  author={Karan, Mladen and {\v{S}}najder, Jan and Ba{\v{s}}ic, Bojana Dalbelo},
  booktitle={Proceedings of the 12th Language Resources and Evaluation Conference},
  pages={657--662},
  year={2012}
}

@inproceedings{khan2016term,
  title={{T}erm {R}anker: A Graph-Based Re-Ranking Approach},
  author={Khan, Muhammad Tahir and Ma, Yukun and Kim, Jung-jae},
  booktitle={Proceedings of the Twenty-Ninth International Florida Artificial Intelligence Research Society Conference},
  pages={310--315},
  year={2016}
}

@inproceedings{kucza2018term,
  title={Term Extraction via Neural Sequence Labeling a Comparative Evaluation of Strategies Using Recurrent Neural Networks},
  author={Kucza, Maren and Niehues, Jan and Zenkel, Thomas and Waibel, Alex and St{\"u}ker, Sebastian},
  booktitle={Proceedings of INTERSPEECH},
  pages={2072--2076},
  year={2018}
}

@inproceedings{lang2021transforming,
  title={Transforming Term Extraction: Transformer-Based Approaches to Multilingual Term Extraction Across Domains},
  author={Lang, Christian and Wachowiak, Lennart and Heinisch, Barbara and Gromann, Dagmar},
  booktitle={Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021},
  pages={3607--3620},
  year={2021}
}

@inproceedings{ljubevsic2019kas,
  title={Kas-term: Extracting {S}lovene Terms from Doctoral Theses via Supervised Machine Learning},
  author={Ljube{\v{s}}i{\'c}, Nikola and Fi{\v{s}}er, Darja and Erjavec, Toma{\v{z}}},
  booktitle={International Conference on Text, Speech, and Dialogue},
  pages={115--126},
  year={2019},
  organization={Springer}
}

@article{meyers2018termolator,
  title={The {T}ermolator: Terminology Recognition Based on Chunking, Statistical and Search-Based Scores},
  author={Meyers, Adam L and He, Yifan and Glass, Zachary and Ortega, John and Liao, Shasha and Grieve-Smith, Angus and Grishman, Ralph and Babko-Malaya, Olga},
  journal={Frontiers in Research Metrics and Analytics},
  volume={3},
  pages={19},
  year={2018},
  publisher={Frontiers}
}

@inproceedings{peters-etal-2018-deep,
    title = "Deep Contextualized Word Representations",
    author = "Peters, Matthew E.  and
      Neumann, Mark  and
      Iyyer, Mohit  and
      Gardner, Matt  and
      Clark, Christopher  and
      Lee, Kenton  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    pages = "2227--2237",
    abstract = "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
}

@inproceedings{pinnis2012term,
  title={Term extraction, tagging, and mapping tools for under-resourced languages},
  author={Pinnis, Marcis and Ljube{\v{s}}ic, Nikola and Stefanescu, Dan and Skadina, Inguna and Tadic, Marko and Gornostay, Tatiana},
  booktitle={Proceedings of the 10th Conference on Terminology and Knowledge Engineering},
  pages={20--21},
  year={2012}
}

@inproceedings{puais2020termeval,
  title={{TermEval} 2020: {RACAI}’s Automatic Term Extraction System},
  author={P{\u{a}}iș, Vasile and Ion, Radu},
  booktitle={Proceedings of the 6th International Workshop on Computational Terminology},
  pages={101--105},
  year={2020}
}

@inproceedings{qasemizadeh2014evaluation,
    title = "Evaluation of Technology Term Recognition with Random Indexing",
    author = "Zadeh, Behrang  and
      Handschuh, Siegfried",
    booktitle = "Proceedings of the Ninth International Conference on Language Resources and Evaluation",
    month = may,
    year = "2014",
    abstract = "In this paper, we propose a method that combines the principles of automatic term recognition and the distributional hypothesis to identify technology terms from a corpus of scientific publications. We employ the random indexing technique to model terms{'} surrounding words, which we call the context window, in a vector space at reduced dimension. The constructed vector space and a set of reference vectors, which represents manually annotated technology terms, in a k-nearest-neighbour voting classification scheme are used for term classification. In this paper, we examine a number of parameters that influence the obtained results. First, we inspect several context configurations, i.e. the effect of the context window size, the direction in which co-occurrence counts are collected, and information about the order of words within the context windows. Second, in the k-nearest-neighbour voting scheme, we study the role that neighbourhood size selection plays, i.e. the value of k. The obtained results are similar to word space models. The performed experiments suggest the best performing context are small (i.e. not wider than 3 words), are extended in both directions and encode the word order information. Moreover, the accomplished experiments suggest that the obtained results, to a great extent, are independent of the value of k.",
}

@article{repar2019termensembler,
  title={Term{E}nsembler: An Ensemble Learning Approach to Bilingual Term Extraction and Alignment},
  author={Repar, Andra{\v{z}} and Podpe{\v{c}}an, Vid and Vavpeti{\v{c}}, An{\v{z}}e and Lavra{\v{c}}, Nada and Pollak, Senja},
  journal={Terminology. International Journal of Theoretical and Applied Issues in Specialized Communication},
  volume={25},
  number={1},
  pages={93--120},
  year={2019},
  publisher={John Benjamins}
}

@inproceedings{rigouts2020termeval,
  title={Term{E}val 2020: Shared Task on Automatic Term Extraction Using the Annotated Corpora for Term Extraction Research ({ACTER}) Dataset},
  author={Rigouts Terryn, Ayla and Hoste, Veronique and Drouin, Patrick and Lefever, Els},
  booktitle={Proceedings of the 6th International Workshop on Computational Terminology},
  pages={85--94},
  year={2020}
}

@article{rigouts2021hamlet,
  title={{HAMLET}: Hybrid Adaptable Machine Learning approach to Extract Terminology},
  author={Rigouts Terryn, Ayla and Hoste, V{\'e}ronique and Lefever, Els},
  journal={Terminology. International Journal of Theoretical and Applied Issues in Specialized Communication},
  year={2021},
  volume={27},
  number={2},
  pages={254--293},
  publisher={John Benjamins}
}

@article{vintar2010bilingual,
  title={Bilingual Term Recognition Revisited: The Bag-of-equivalents Term Alignment Approach and its Evaluation},
  author={Vintar, Spela},
  journal={Terminology. International Journal of Theoretical and Applied Issues in Specialized Communication},
  volume={16},
  number={2},
  pages={141--158},
  year={2010},
  publisher={John Benjamins}
}

@inproceedings{wang2016featureless,
  title={Featureless Domain-Specific Term Extraction with Minimal Labelled Data},
  author={Wang, Rui and Liu, Wei and McDonald, Chris},
  booktitle={Proceedings of the Australasian Language Technology Association Workshop},
  pages={103--112},
  year={2016}
}

@article{zhang2017semre,
author = {Zhang, Ziqi and Gao, Jie and Ciravegna, Fabio},
year = {2018},
month = {03},
pages = {1--41},
title = {Sem{R}e-{R}ank: Improving Automatic Term Extraction By Incorporating Semantic Relatedness With Personalised PageRank},
volume = {12},
journal = {ACM Transactions on Knowledge Discovery from Data}
}

