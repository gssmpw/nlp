\section{Related work}
\label{sec:related}

ATE systems were traditionally classified as either statistical, linguistic or a combination of these two approaches. The linguistic approach utilizes the distinctive linguistic aspects of terms, most often their syntactic (i.e. part-of-speech) patterns. On the other hand, the statistical approach takes advantage of term frequencies in a corpus. However, most traditional systems are hybrid, using a combination of the two approaches. For example, \cite{justeson1995technical} first define part-of-speech (POS) patterns of terms and then use simple frequencies to filter the term candidates.  

Many terminology extraction algorithms are based on the concepts of term\-hood and unit\-hood defined by \cite{kageura1996methods}: termhood is “the degree to which a stable lexical unit is related to some domain-specific concepts” and unithood is “the degree of strength or stability of syntagmatic combinations and collocations”. Termhood-based statistical measures \citep{vintar2010bilingual} function on a presumption that a term’s relative frequency will be higher in domain-specific corpora than in the general language, while common statistical measures, such as mutual information \citep{daille1994towards}, are used to measure unithood. These two approaches have been used as a basis of several hybrid systems, such as Termolator \citep{meyers2018termolator} and TermEnsembler \citep{repar2019termensembler}.

However, in the last decade, this division has become too simplistic due to the emergence of new machine learning and deep learning approaches that could not be classified as either linguistic, statistical or hybrid in the traditional sense. The advances in embeddings and deep neural networks have also influenced the terminology extraction field. \cite{amjadian2016local} were one of the first to leverage embeddings for terminology extraction by trying to represent unigram terms as a combination of local and global vectors. Other works involving non-contextual word embeddings include the approaches that either devise a co-training system involving two neural networks to determine whether a term is domain relevant or not \citep{wang2016featureless}, use word embeddings to estimate term similarity in a graph-based ranking system \citep{khan2016term},  employ word embeddings to measure semantic relatedness of term candidates in order to re-rank term candidates generated with traditional term extraction methods \citep{zhang2017semre},  identify term candidates using sequence labeling and word-level and character-level embeddings \citep{kucza2018term}, or devise a nested term extraction classifier with features from various (non-contextual and contextual) word embedding models \citep{gao2019feature}. Contextual word representations, such as eLMO \citep{peters-etal-2018-deep} and BERT \citep{devlin2018bert}, can encode additional information about terms as illustrated by the fact that the winning approach in the TermEval2020 competition \citep{rigouts2020termeval} uses the BERT model. TALN-LS2N \citep{hazem2020termeval}, the winning approach for English and French, uses BERT in a binary classification setting, where a combination of n-grams and a sentence are used as an instance and the classifier needs to determine for each n-gram inside the sentence whether it is a term or not. On the other hand, the winning approach for Dutch \citep{rigouts2020termeval} uses pretrained GloVe word embeddings that are fed into a bi-directional LSTM-based neural architecture. Another well-performing system used in this competition that combines word embeddings with statistical approaches compares the performance of terminology extraction built on an improved TextRank, TFIDF, clustering, and termhood features \citep{puais2020termeval}. Recently, state-of-the-art results for English, French and Dutch have been reported by \cite{lang2021transforming} using a sequence labeling approach.

Another distinct approach is to utilize machine learning with feature engineering. It involves first extracting a set of term candidates, followed by the calculation of various features and training of a machine learning model, where term extraction is treated as a bilingual classification task. Various types of machine learning algorithms have been used, such as decision trees \citep{karan2012evaluation}, rule induction \citep{foo2010using}, k-nearest neighbours \citep{qasemizadeh2014evaluation}, support vector machines \citep{ljubevsic2019kas} and random forest \citep{rigouts2021hamlet}. The last two approaches are particularly relevant for our work. 

The first approach, developed by \cite{ljubevsic2019kas} is the state-of-the-art system for Slovene. It first extracts CTs with the CollTerm tool \citep{pinnis2012term}, which uses a complex language-specific set of term patterns originally developed for the SketchEngine terminology extraction module \citep{fivser2016terminology}. A total of 31 patterns were defined from unigrams up to four-grams and CTs (i.e. lemmatized versions of terms) with a frequency of 3 or more were considered. The resulting term lists were annotated by four annotators as either in-domain, out-of-domain, academic or irrelevant terms (which is a similar setup as used in the ACTER and RSDO datasets). The annotations were then used as training data for a machine learning approach with the following features: term frequency, 5 statistical measures from the SketchEngine terminology module (chi-square, dice, pointwise mutual information, t-score, tf-idf), C-value \citep{frantzi2000automatic}, oversampling (based on instances where the annotators were in agreement), candidate length, average token length, term pattern and context\footnote{Calculated by using a context-based SVM classifier with a linear kernel with features of the classifier being frequencies of tokens occurring in a 3-token window around all the occurrences of a term candidate in the respective document.}. Since some of the statistical measures can be calculated only for multi-word units, they trained separate classifiers for single-word (SWU) and multi-word (MWU) units. They evaluated the models in a cross-validation setting on the Slovene KAS dataset \citep{11356/1198}, obtaining F1 scores of around 0.5 (i.e. when only the \textit{irrelevant} annotation is considered negative and the remaining annotations are treated as valid terms, which is similar to the setting used in our experiments). 

The second approach, which also uses the machine learning paradigm with feature engineering (as we do in our work) was developed by \cite{rigouts2021hamlet}. It first identifies the linguistic patterns of the annotated terms in the ACTER corpus and then uses these patterns to identify term candidates. They generate 177 features in 6 subgroups: shape (e.g., number of tokens in a CT), linguistic (e.g., POS tag of the first token of the CT), frequency (e.g., relative frequency of the CT in a specialized corpus), statistical (various termhood/unithood measures), contextual (e.g., whether the CT occurs between parentheses or right before/after parentheses), variational (number of different variations of the CT). They experimented with several different classification algorithms in the \textit{sklearn} Python library and obtained the best F1 scores with the random forest classifier. In the setup with a held-out test set (3 domains are used for training, one for testing and the experiments are run 4 times, each time with a different test domain) they achieved F1 scores between 0.338 and 0.436 for English, between 0.288 and 0.520 for French and between 0.361 and 0.616 for Dutch.