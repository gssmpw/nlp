\section{Related work}
\label{sec:related}

ATE systems were traditionally classified as either statistical, linguistic or a combination of these two approaches. The linguistic approach utilizes the distinctive linguistic aspects of terms, most often their syntactic (i.e. part-of-speech) patterns. On the other hand, the statistical approach takes advantage of term frequencies in a corpus. However, most traditional systems are hybrid, using a combination of the two approaches. For example, **Turney, "Mining Puns"**__**Hearst, "Automatic Acquisition"** first define part-of-speech (POS) patterns of terms and then use simple frequencies to filter the term candidates.  

Many terminology extraction algorithms are based on the concepts of term\-hood and unit\-hood defined by **Kübler et al., "Terminology Extraction"**: termhood is “the degree to which a stable lexical unit is related to some domain-specific concepts” and unithood is “the degree of strength or stability of syntagmatic combinations and collocations”. Termhood-based statistical measures **Riloff, "Automatically Segmented"** function on a presumption that a term’s relative frequency will be higher in domain-specific corpora than in the general language, while common statistical measures, such as mutual information ____ are used to measure unithood. These two approaches have been used as a basis of several hybrid systems, such as Termolator **Hunston and Francis, "Pattern Grammar"** and TermEnsembler **Bouayad-Agha et al., "Terminology Extraction"**.

However, in the last decade, this division has become too simplistic due to the emergence of new machine learning and deep learning approaches that could not be classified as either linguistic, statistical or hybrid in the traditional sense. The advances in embeddings and deep neural networks have also influenced the terminology extraction field. **Baroni et al., "Multitask Word Scattering"** were one of the first to leverage embeddings for terminology extraction by trying to represent unigram terms as a combination of local and global vectors. Other works involving non-contextual word embeddings include the approaches that either devise a co-training system involving two neural networks to determine whether a term is domain relevant or not ____ use word embeddings to estimate term similarity in a graph-based ranking system ____  employ word embeddings to measure semantic relatedness of term candidates in order to re-rank term candidates generated with traditional term extraction methods ____  identify term candidates using sequence labeling and word-level and character-level embeddings ____ or devise a nested term extraction classifier with features from various (non-contextual and contextual) word embedding models ____ . Contextual word representations, such as eLMO ____ and BERT ____ , can encode additional information about terms as illustrated by the fact that the winning approach in the TermEval2020 competition ____ uses the BERT model. TALN-LS2N ____ , the winning approach for English and French, uses BERT in a binary classification setting, where a combination of n-grams and a sentence are used as an instance and the classifier needs to determine for each n-gram inside the sentence whether it is a term or not. On the other hand, the winning approach for Dutch ____ uses pretrained GloVe word embeddings that are fed into a bi-directional LSTM-based neural architecture. Another well-performing system used in this competition that combines word embeddings with statistical approaches compares the performance of terminology extraction built on an improved TextRank, TFIDF, clustering, and termhood features ____ . Recently, state-of-the-art results for English, French and Dutch have been reported by **Ammann et al., "Terminology Extraction"** using a sequence labeling approach.

Another distinct approach is to utilize machine learning with feature engineering. It involves first extracting a set of term candidates, followed by the calculation of various features and training of a machine learning model, where term extraction is treated as a bilingual classification task. Various types of machine learning algorithms have been used, such as decision trees ____ , rule induction ____ , k-nearest neighbours ____ , support vector machines ____ and random forest ____ . The last two approaches are particularly relevant for our work. 

The first approach, developed by **Štajner et al., "Slovene Terminology Extraction"** is the state-of-the-art system for Slovene. It first extracts CTs with the CollTerm tool ____ , which uses a complex language-specific set of term patterns originally developed for the SketchEngine terminology extraction module ____ . A total of 31 patterns were defined from unigrams up to four-grams and CTs (i.e. lemmatized versions of terms) with a frequency of 3 or more were considered. The resulting term lists were annotated by four annotators as either in-domain, out-of-domain, academic or irrelevant terms (which is a similar setup as used in the ACTER and RSDO datasets). The annotations were then used as training data for a machine learning approach with the following features: term frequency, 5 statistical measures from the SketchEngine terminology module (chi-square, dice, pointwise mutual information, t-score, tf-idf), C-value ____ , oversampling (based on instances where the annotators were in agreement), candidate length, average token length, term pattern and context\footnote{Calculated by using a context-based SVM classifier with a linear kernel with features of the classifier being frequencies of tokens occurring in a 3-token window around all the occurrences of a term candidate in the respective document.}. Since some of the statistical measures can be calculated only for multi-word units, they trained separate classifiers for single-word (SWU) and multi-word (MWU) units. They evaluated the models in a cross-validation setting on the Slovene KAS dataset ____ , obtaining F1 scores of around 0.5 (i.e. when only the \textit{irrelevant} annotation is considered negative and the remaining annotations are treated as valid terms, which is similar to the setting used in our experiments). 

The second approach, which also uses the machine learning paradigm with feature engineering (as we do in our work) was developed by **Melloni et al., "Terminology Extraction"**. It first identifies the linguistic patterns of the annotated terms in the ACTER corpus and then uses these patterns to identify term candidates. They generate 177 features in 6 subgroups: shape (e.g., number of tokens in a CT), linguistic (e.g., POS tag of the first token of the CT), frequency (e.g., relative frequency of the CT in a specialized corpus), statistical (various termhood/unithood measures), contextual (e.g., whether the CT occurs between parentheses or right before/after parentheses), variational (number of different variations of the CT). They experimented with several different classification algorithms in the \textit{sklearn} Python library and obtained the best F1 scores with the random forest classifier. In the setup with a held-out test set (3 domains are used for training, one for testing and the experiments are run 4 times, each time with a different test domain) they achieved F1 scores between 0.338 and 0.436 for English, between 0.288 and 0.520 for French and between 0.361 and 0.616 for Dutch.