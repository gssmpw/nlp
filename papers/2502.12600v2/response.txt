\section{Related Works}
\label{sec:related}
%
This research primarily pertains to the field of deraining studies.
%
Many methods have been proposed to develop state-of-the-art deraining networks.
%
These works include deep network designs **Chen, "DerainNet"**, residual networks **Wang, "ResNet"**, recurrent networks **Liu, "RNN"**, multi-task **Huang, "Multi-Task Net"** and multi-scale designs **Zhang, "MS-DenseNet"**, sparsity-based image modeling **Li, "Sparse Image Model"**, low-rank prior **Fu, "Low-Rank Prior"**, model-driven solutions **Garg, "Physically-Based Rendering"**, attention mechanisms **He, "Attention Mechanism"**, Transformer-based networks **Vaswani, "Transformer"**, adversarial learning **Goodfellow, "Adversarial Learning"**, representation learning **Bengio, "Representation Learning"**, semi-supervised **Kingma, "Semi-Supervised"** and unsupervised learning **Hinton, "Unsupervised Learning"**.
%
Deep learning methods are data-hungry but collecting rain streaks and background image pairs is challenging.
%
A lot of works have been proposed to synthesize rain streaks with better results.
%
Garg \etal, "Physically-Based Rendering" first propose a physically-based photo-realistic rendering method for synthesizing rain streaks.
%
Zhang \etal, "Synthetic Paired Data" and Fu \etal, "Manual Rain Effects" use Photoshop software to manually add rain effects to images to build the synthetic paired data.
%
Due to the poor generalization performance of existing methods, models trained on synthetic images were found to be ineffective in real-world scenarios.
%
Some works **Xu, "Real Collected Deraining Datasets"** that have contributed to real collected deraining datasets.
%
However, acquiring these datasets is still expensive and cannot solve the problem of poor generalization.

There are also works that mention the generalization issue of the deraining models.
%
Xiao \etal, "Accumulating Knowledge" and Zhou \etal, "Knowledge Accumulation" attempt to improve the generalization ability of deraining networks by accumulating knowledge from multiple synthetic rain datasets, as most existing methods can only learn the mapping on a single dataset for the deraining task.
%
However, this attempt does not allow the network to generalize beyond the training set.
%
In addition, semi-supervised methods **Laine, "Temporal Ensembling"** have also been used to improve the deraining effect on real images, and we also include the representative method Syn2Real **Isola, "Syn2Real"**.
%
There are some semi-supervised deraining methods **Sajjadi, "CutMix"** that are proposed to improve the performance of deraining models in real-world scenarios.
%
When obtaining some real images similar to the test images, these works can indeed achieve some improvement. 
%
However, these improvements are not brought about by improving the generalization ability.
%
Their solution is to include real test images in the training set, even if we don't have corresponding clean images. 
%
These methods are effective when we can determine the characteristics of the test image. 
%
However, this does not solve the generalization problem. 
%
Because these methods manage to convert ``rain outside the training set'' to ``rain inside the training set''. 
%
Since data collection is extremely difficult, this method still faces great challenges in practice.
%
Unlike the majority of existing deraining research, we do not propose new network structures, loss functions, or datasets.
%
Our objective is to analyze and understand the generalization problem within the context of the deraining task.
%
We will proceed to review previous works focusing on interpretability and understanding generalization in low-level vision.


Deep learning interpretability research aims to understand the mechanism of deep learning methods and to obtain clues about the success or failure of these methods.
%
Without a deep understanding of these working mechanisms, we are not convinced to move forward in the right direction.
%
The research on deep learning interpretability follows a long line of works, most focusing on the classification task **Yosinski, "How Transferable"**.
%
Low-level vision tasks have embraced great success with powerful deep learning techniques.
%
There are also works on interpretability for these deep low-level networks **Li, "Interpreting Deep Neural Networks"**.
%
Gu and Dong, "Deep Image Prior" bring the first interpretability tool for image super-resolution (SR) networks.
%
Xie \etal, "Visualizing and Understanding" find the most discriminative filters for each specific degradation in a blind SR network, whose weights, positions, and connections are important for the specific function in blind SR.
%
Magid \etal, "Image Super-Resolution" use a texture classifier to assign patches with semantic labels, to identify global and local sources of SR errors.
%
Shi \etal, "Transformers for Image Super-Resolution" show that Transformers can directly utilize multi-frame information from unaligned frames, and adopting alignment methods is sometimes harmful to Transformers in video super-resolution.
%
Hu \etal, "Causal Effect Map" propose the causal effect map to interpret low-level vision models by focusing on causation rather than mere correlation.
%
The closest work to this paper is the deep degradation representation proposed by **Meinhardt, "Deep Degradation Representation"**.
%
They argue that SR networks tend to overfit to degradations and show degradation ``semantics'' inside the network.
%
The presence of these representations often means a decrease in generalization ability.
%
The utilization of this knowledge can guide us to analyze the generalization performance of SR methods **Boracchi, "Super-Resolution with Deep Degradation"**.

The generalization problem in low-level vision often arises when the testing degradation does not match the training degradation, \eg, different downsampling kernel **Ahmed, "Downsampling Kernel"** and noise distribution **Goyal, "Noise Distribution"**.
%
The existing works either develop blind methods to include more degradation possibilities in the training process or make the training data closer to real-world applications.
%
Only a little work has been proposed to study the reasons for this lack of generalization performance **Jiang, "Generalization in Low-Level Vision"**.
%
No research has attempted to investigate the interpretation of the training process of low-level vision networks, especially from the perspective of the generalization problem.