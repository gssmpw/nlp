\documentclass[lettersize,journal]{IEEEtran}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{cite}
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{colortbl}



% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\newcommand{\ie}{\emph{i.e.}}    % notation of `i.e.`
\newcommand{\eg}{\emph{e.g.}}    % notation of `e.g.`
\newcommand{\vs}{\emph{v.s. }}    % notation of `v.s.`
\newcommand{\etc}{\emph{etc}}     % notation of `etc.`
\newcommand{\wo}{\emph{w/o }}     % notation of `without`
\newcommand{\wi}{\emph{w/ }}      % notation of `with`
\newcommand{\wrt}{\emph{w.r.t. }} % notation of `w.r.t.`
\newcommand{\etal}{\emph{et al. }} % notation of `w.r.t.`

\newcommand{\expand}[1]{{\color{red} #1}}
\newcommand{\mod}[1]{{\color{blue} #1}}
\begin{document}

\title{Revisiting the Generalization Problem of Low-level Vision Models Through the Lens of Image Deraining}

\author{Jinfan Hu*, Zhiyuan You*, Jinjin Gu, Kaiwen Zhu, Tianfan Xue, Chao Dong

\thanks{This work was supported in part by the National Natural Science Foundation of China (Grant No. 62276251), the Joint Lab of CAS-HK and RGC Early Career Scheme (ECS) No. 24209224. (* Equal contributions, listed in alphabetical order. Corresponding author: Chao Dong).}

\thanks{Jinfan Hu is with the Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen 518055, China, and also with the University of Chinese Academy of Sciences, Beijing 100049, China.  (e-mail: jf.hu1@siat.ac.cn)}

\thanks{Zhiyuan You is with The Chinese University of Hong Kong, Hong Kong 999077, China, and also with the Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen 518055, China. (e-mail: zhiyuanyou@foxmail.com)}

\thanks{Jinjin Gu is with The University of Sydney, NSW 2006, Australia. (e-mail: hellojasongt@gmail.com)}

\thanks{Kaiwen Zhu is with the Shanghai Jiao Tong University, Shanghai 200240, China, and also with the Shanghai Artificial Intelligence Laboratory, Shanghai 200232, China. (e-mail: sqzhukaiwen@sjtu.edu.cn)}

\thanks{Tianfan Xue is with The Chinese University of Hong Kong, Hong Kong 999077, China. (e-mail: tfxue@ie.cuhk.edu.hk)}

\thanks{Chao Dong is with the Shenzhen Key Lab of Computer Vision and Pattern Recognition, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, and also with Shenzhen University of Advanced Technology, Shenzhen 518055, China. (e-mail: chao.dong@siat.ac.cn)}

}
% \author{IEEE Publication Technology,~\IEEEmembership{Staff,~IEEE,}
%         % <-this % stops a space
% \thanks{This paper was produced by the IEEE Publication Technology Group. They are in Piscataway, NJ.}% <-this % stops a space
% \thanks{Manuscript received April 19, 2021; revised August 16, 2021.}}

% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}%
{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

% \IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle

\begin{abstract}

Generalization remains a significant challenge for low-level vision models, which often struggle with unseen degradations in real-world scenarios despite their success in controlled benchmarks. 
%
In this paper, we revisit the generalization problem in low-level vision models.
%
Image deraining is selected as a case study due to its well-defined and easily decoupled structure, allowing for more effective observation and analysis.
%
Through comprehensive experiments, we reveal that the generalization issue is not primarily due to limited network capacity but rather the failure of existing training strategies, which lead networks to overfit specific degradation patterns.
%
Our findings show that guiding networks to focus on learning the underlying image content, rather than the degradation patterns, is key to improving generalization.
%
We demonstrate that balancing the complexity of background images and degradations in the training data helps networks better fit the image distribution.
%
Furthermore, incorporating content priors from pre-trained generative models significantly enhances generalization.
%
Experiments on both image deraining and image denoising validate the proposed strategies.
%
We believe the insights and solutions will inspire further research and improve the generalization of low-level vision models.

\end{abstract}

\begin{IEEEkeywords}
Low-level Vision, Image Deraining, Generalization, Interpretability.
\end{IEEEkeywords}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/teaser.pdf}

    \caption{The existing deraining models suffer from severe generalization problems. After training with synthetic rainy images, when feeding \textbf{(a)} an image with different rain streaks, its output \textbf{(b)} shows a limited deraining effect. Two intuitive ways to improve generalization performance -- \textbf{(c)} adding background images, and \textbf{(d)} adding rain patterns, cannot effectively relieve the generalization issue. In this paper, we provide a new counter-intuitive insight -- \textbf{(e)} we improve the generalization ability of the deraining networks by selecting \emph{much less} training background images for training.}
    \label{fig:teaser}
\end{figure*}

\section{Introduction}

\IEEEPARstart{G}{eneralization} challenges persist as fundamental constraints in deep learning systems, where mismatches between synthetic training data and real-world testing conditions often induce performance deterioration.
%
This issue is also pronounced in low-level vision (LV) tasks, where models are typically trained on synthetic data that fails to capture the full complexity of natural images.
%
It further widens the gap between simulation and reality.
%
Overcoming this challenge is essential for the reliable deployment of LV models in unconstrained environments.
%
However, comprehending generalization in LV tasks is far from straightforward; it is not a simplistic extension of generalization research conducted within high-level vision tasks.
%
LV tasks commonly aim to process images affected by various degradation patterns, such as blur, noise, or structured artifacts like rain streaks.
%
Current models demonstrate proficiency in removing seen degradation patterns during training to reconstruct image content, they often fail to properly handle or completely ignore unseen degradation.
%
The response to different degradation patterns serve as a critical indicator of model generalization capability.
%
However, when degradations are intertwined with image content, accurately assessing degradation removal becomes challenging.
%
This entanglement also makes it difficult to evaluate the fidelity of image reconstruction, complicating both qualitative and quantitative assessments of the modelâ€™s generalization ability.
%
In this paper, we focus on additive degradations, which are easier to decouple from image content, making them an ideal starting point for analyzing the generalization performance of low-level vision models.

Image deraining, as an important and representative LV task, aims to remove additive rain streaks from images.
%
It can be formulated as a decomposition problem with a relatively simple linear superimposition degradation model.
%
When a network fails to generalize, the rain streaks typically persist, while the background image remains largely unaffected.
%
Notably, existing models often fail to handle rain streaks that fall outside their training distribution, as illustrated in \figurename~\ref{fig:teaser} (b).
%
A key advantage of image deraining is that rain streaks are spatially separable from the image content.
%
It allows us to mask the degraded regions and perform quantitative evaluations specifically on the degradation.
%
In contrast, the noise in image denoising tasks is often distributed across the entire image, making it difficult to isolate and analyze the degradation and background separately.
%
This clear separation of degradation and content makes image deraining an intuitive and quantifiable case study for analyzing the generalization performance of LV models.
%
Contrary to previous works that solely rely on overall image quality metrics, we propose to decouple the deraining task into two distinct components: rain removal and background reconstruction.
%
A key motivation behind this approach is the recognition that overall quality deterioration may result from either unsuccessful rain removal or poor background reconstruction. 
%
Given that the generalization problem in the task primarily pertains to the removal of rain streaks, our research methodology enables us to isolate and minimize the influence of extraneous factors.


In this paper, we argue that the generalization problem arises when the network overfits the degradation in the training set.
%
A significant factor contributing to this outcome is the inappropriate training objective.
%
We commence our analysis with the most fundamental element involved in formulating the training objective, the training data.
%
Numerous studies have attempted to improve real-world performance by increasing the complexity of training data.
%
This approach originates from a natural but unproven ``acknowledegment'' in low-level vision.
%
It suggests that increasing the number of training data and collecting images with rich and clear details help the network learn a higher-quality image space.
%
As a result, better generalization is obtained.
%
This ``acknowledgement'' has also permeated the deraining community, suggesting that a network exposed to a more diverse training set (both in terms of background images and rain streaks) will be better equipped to generalize to unseen scenarios.
%
Nonetheless, this approach does not address the anticipated generalization issue effectively.
%
We contend that the model's poor generalization performance arises precisely from the excess complex background information provided during the training phase.
%
Consequently, the model fails to reconstruct the image content and instead overfits to the degradation pattern.
%
By employing our analysis method, which separately measures background reconstruction and rain removal, we arrive at some counter-intuitive conclusions.

\textbf{Our key findings:}\quad
%
We find that deep networks are slacking off during training, aiming to achieve the training objective in the simplest way.
%
This approach unfortunately leads to subpar generalization performance.
%
An improperly designed training objective is a key factor contributing to this problem.
%
It can be summarized as:

\begin{quote}
    \emph{In the task of separating image content from additive degradation, deep networks display a tendency to learn the less complex element.}
\end{quote}

Specifically, under common training settings where background complexity is high and rain complexity is low, the network naturally learns to identify and separate rain streaks, given that they are less complex and thus easier to learn.
%
However, when real-world scenarios deviate from the trained pattern of rain, the network tends to disregard them, resulting in poor generalization performance.
%
Conversely, when we train the model using a less complex background image set, it demonstrates superior generalization ability, as illustrated in \figurename~\ref{fig:teaser} (e).
%
This may be attributed to the fact that when the complexity of the training image background is less than that of the rain patterns, the network will again take a shortcut to reduce the loss -- in this case, by learning to reconstruct the background rather than overfitting to the rain streaks.
%
These counter-intuitive phenomena have not been previously studied or valued in the literature.




\textbf{Implication:}\quad
%
Our results offer intriguing and insightful contributions to the existing body of literature.
%
They underscore the critical role of the training objective in determining a model's generalization capabilities.
%
An inappropriate and incomplete training objective creates a loophole for deep networks to ``slack off''.
%
While we anticipate that the low-level vision network will learn the rich semantics inherent in natural images, it is often overlooked that the network can also achieve learning objectives through shortcuts, resulting in subpar generalization performance.
%
Our findings also highlight that a model with robust generalization capability should learn the distribution of image content rather than the degradation pattern.
%
Building on our insights, even the simplest networks can exhibit strong generalization capabilities, indicating the significant potential for practical application of our findings.

This work builds on our previous paper presented at NeurIPS 2023 \cite{gu2023networks}.
%
We further refine the writing logic and optimize the structure in this paper. 
%
Using the image deraining task as a focal point, we conduct a more comprehensive discussion on the generalization of low-level vision models, supported by a broader range of experiments and analyses.
%
This paper builds upon that foundation with several significant extensions.
%
First, we expand our discussion in the context of rain removal analysis to highlight the importance of background image sharpness in the training set.
%
We found that the network tends to learn the content of low-sharpness images more readily than it does the rain streaks, which enhances its generalization capabilities.
%
Additionally, we introduce a vision language model \cite{you2023depicting,you2024descriptive,zhu2024intelligent} to evaluate the deraining effects, moving beyond traditional quantitative metrics for a more holistic assessment.
%
Second, we design a series of analogical experiments that analogize image deraining to a simplified toy task. 
%
This toy task allows us to visualize the network's performance more intuitively, offering an additional perspective to validate our findings regarding the network's generalization performance.
%
Specifically, we are dealing with the task of removing additive Gaussian noise from univariate cosine functions.
%
Due to the simplicity of the function curve and noise pattern, it offers an analytically straightforward framework for exploring network behavior.
%
By analyzing the results from the task, we uncover valuable insights that can be applied to better understand neural networks.
%
Finally, based on our analysis, we propose a novel strategy to boost the model's generalization ability.
%
By leveraging the strong content priors from generative models, the networks can gain a better fitting of image content.
%
Experiments on image deraining and denoising tasks show significant generalization improvements compared to traditional methods.



The rest of this article is organized as follows.
% 
Section \ref{sec:related} reviews related works.
% 
Section \ref{sec:methodology} outlines a methodology for analyzing network generalization capabilities.
% 
Section \ref{sec:understanding} presents key insights derived from experimental observations.
% 
Section \ref{sec:analogous} further presents an analogous experiment that offers a more intuitive validation of our findings on the neural network's generalization problem.
% 
Section \ref{sec:implict} proposes two practical solutions to enhance the network's generalization ability.
% 
Finally, Section \ref{sec:conclusion} concludes the paper.


\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figs/output_analysis.pdf}

    \caption{\textbf{(Left)} The illustration of the rainy image synthesis. \textbf{(Right)} Our fine-grained analysis of the deraining results.}
    \label{fig:output}

\end{figure*}



\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/learning_objective.pdf}

    \caption{\textbf{(a)} Background images from different image datasets. It can be seen that the structure of the face image (CelebA) is relatively complex. Natural image patches (DIV2K) contain natural textures and patterns. The patterns in Manga109 and Urban100 are artificially created -- Manga images have sharp edges, while Urban images contain a lot of repeating patterns and self-similarities. \textbf{(b)} Rain streaks used in our experiments.}
    \label{fig:data}

\end{figure*}


\section{Related Works}\label{sec:related}
%
This research primarily pertains to the field of deraining studies.
%
Many methods have been proposed to develop state-of-the-art deraining networks.
%
These works include deep network designs \cite{fu2017clearing,wang2019erl}, residual networks \cite{fu2017removing,liu2019dual}, recurrent networks \cite{ren2019progressive,yang2019single,yang2019scale}, multi-task \cite{wang2019dtdn,du2020conditional} and multi-scale designs \cite{jiang2020multi,fu2019lightweight,yasarla2019uncertainty,yu2019gradual,wei2019coarse,wang2020dcsfn,zamir2021multi}, sparsity-based image modeling \cite{gu2017joint,zhu2017joint}, low-rank prior \cite{chang2017transformed}, model-driven solutions \cite{wang2020model,wang2020rethinking}, attention mechanisms \cite{wang2020joint,chen2021pre,fu2021rain}, Transformer-based networks \cite{chen2023learning,chen2022cross}, adversarial learning \cite{li2019heavy}, representation learning \cite{chen2021robust}, semi-supervised \cite{yasarla2020syn2real} and unsupervised learning \cite{chen2022unpaired}.
%
Deep learning methods are data-hungry but collecting rain streaks and background image pairs is challenging.
%
A lot of works have been proposed to synthesize rain streaks with better results.
%
Garg \etal \cite{garg2006photorealistic} first propose a physically-based photo-realistic rendering method for synthesizing rain streaks.
%
Zhang \etal \cite{zhang2018density} and Fu \etal \cite{fu2017clearing} use Photoshop software to manually add rain effects to images to build the synthetic paired data.
%
Due to the poor generalization performance of existing methods, models trained on synthetic images were found to be ineffective in real-world scenarios.
%
Some works \cite{yang2017deep,zhang2019image,wang2019spatial} that have contributed to real collected deraining datasets.
%
However, acquiring these datasets is still expensive and cannot solve the problem of poor generalization.

There are also works that mention the generalization issue of the deraining models.
%
Xiao \etal \cite{xiao2021improving} and Zhou \etal \cite{zhou2021image} attempt to improve the generalization ability of deraining networks by accumulating knowledge from multiple synthetic rain datasets, as most existing methods can only learn the mapping on a single dataset for the deraining task.
%
However, this attempt does not allow the network to generalize beyond the training set.
%
In addition, semi-supervised methods \cite{wei2019semi,huang2021memory} have also been used to improve the deraining effect on real images, and we also include the representative method Syn2Real \cite{yasarla2020syn2real,yasarla2021semi}.
%
There are some semi-supervised deraining methods \cite{wei2019semi,huang2021memory,yasarla2020syn2real,yasarla2021semi} that are proposed to improve the performance of deraining models in real-world scenarios.
%
When obtaining some real images similar to the test images, these works can indeed achieve some improvement. 
%
However, these improvements are not brought about by improving the generalization ability.
%
Their solution is to include real test images in the training set, even if we don't have corresponding clean images. 
%
These methods are effective when we can determine the characteristics of the test image. 
%
However, this does not solve the generalization problem. 
%
Because these methods manage to convert ``rain outside the training set'' to ``rain inside the training set''. 
%
Since data collection is extremely difficult, this method still faces great challenges in practice.
%
Unlike the majority of existing deraining research, we do not propose new network structures, loss functions, or datasets.
%
Our objective is to analyze and understand the generalization problem within the context of the deraining task.
%
We will proceed to review previous works focusing on interpretability and understanding generalization in low-level vision.


Deep learning interpretability research aims to understand the mechanism of deep learning methods and to obtain clues about the success or failure of these methods.
%
Without a deep understanding of these working mechanisms, we are not convinced to move forward in the right direction.
%
The research on deep learning interpretability follows a long line of works, most focusing on the classification task \cite{simonyan2013deep,springenberg2014striving,shrikumar2017learning,sundararajan2017axiomatic,zhou2018interpreting,lundberg2017unified}.
%
Low-level vision tasks have embraced great success with powerful deep learning techniques.
%
There are also works on interpretability for these deep low-level networks \cite{gu2021interpreting,xie2021finding,magid2022texture,shirethinking,hu2024interpreting}.
%
Gu and Dong \cite{gu2021interpreting} bring the first interpretability tool for image super-resolution (SR) networks.
%
Xie \etal \cite{xie2021finding} find the most discriminative filters for each specific degradation in a blind SR network, whose weights, positions, and connections are important for the specific function in blind SR.
%
Magid \etal \cite{magid2022texture} use a texture classifier to assign patches with semantic labels, to identify global and local sources of SR errors.
%
Shi \etal \cite{shi2022rethinking} show that Transformers can directly utilize multi-frame information from unaligned frames, and adopting alignment methods is sometimes harmful to Transformers in video super-resolution.
%
Hu \etal \cite{hu2024interpreting} propose the causal effect map to interpret low-level vision models by focusing on causation rather than mere correlation.
%
The closest work to this paper is the deep degradation representation proposed by \cite{liu2021discovering}.
%
They argue that SR networks tend to overfit to degradations and show degradation ``semantics'' inside the network.
%
The presence of these representations often means a decrease in generalization ability.
%
The utilization of this knowledge can guide us to analyze the generalization performance of SR methods \cite{liu2022evaluating}.

The generalization problem in low-level vision often arises when the testing degradation does not match the training degradation, \eg, different downsampling kernel \cite{gu2019blind,liu2022blind,kong2022reflash,chen2024low,zhang2023crafting} and noise distribution \cite{guo2019toward,chen2023masked}.
%
The existing works either develop blind methods to include more degradation possibilities in the training process or make the training data closer to real-world applications.
%
Only a little work has been proposed to study the reasons for this lack of generalization performance \cite{liu2021discovering,liu2022evaluating}.
%
No research has attempted to investigate the interpretation of the training process of low-level vision networks, especially from the perspective of the generalization problem.



\section{Analysis of Image Deraining}\label{sec:methodology}
\subsection{Construction of Training Objective}
In this subsection, we analyze the generalization performance of different deraining models by setting a variety of training objectives in order to observe their effects.
%
The training data and the loss function jointly determine the training objective of a deep network.
%
As shown in \figurename~\ref{fig:output} (left), a rainy image $I$ can be modelled using a linear model $I=B+R$, where $B$ is the image background, and $R$ is the additive rain streaks.
%
We change the training objectives with different background images and rain streaks.




\subsubsection{Background Images.}
%
Typically, image backgrounds are sampled from street view images \cite{geiger2012we} or natural image datasets \cite{schaefer2003ucid,arbelaez2010contour}, as these images are close to the application scenarios of deraining.
%
In literature, previous works \cite{fu2017removing,zhang2018density} claim that the model can learn the prior knowledge of reconstructing these scenes by training on a large number of such background images.
%
We break this common sense by constructing different training background image sets from the following two aspects.


First, we change the number of training background images.
%
The amount of training data partially determines the complexity of network learning.
%
We argue that as too much background data are provided for training, the model cannot faithfully learn to reconstruct the image content but turns to overfit the degradation patterns.
%
We reduce the complexity of the background images to see how the network behavior changes in extreme scenarios.
%
In our experiments, we use 8, 16, 32, 64, 128, 256, 512, and 1024 background image patches of size $128\times128$ to build the training datasets, respectively.
%
We also use a large number of patches (up to 30,000) to simulate the situation when the image background is sufficiently sampled.


In addition to the data scale, the image content will also affect network learning.
%
For images with many self-similar or regular patterns, it is easier for the network to fit the distribution.
%
While a face image that contains both short- and long-term dependent structures is apparently more complex than a skyscraper that consists of only repeated lines and grids \cite{bagrov2020multiscale}.
%
We select image distribution as the second aspect of our dataset construction.
%
We sample from four image datasets that are distinct from each other: 
%
CelebA (face images) \cite{liu2015deep}, DIV2K (natural image patches) \cite{timofte2017ntire}, Manga109 (comic image patches) \cite{matsui2017sketch}, and Urban100 (building image patches) \cite{huang2015single}.
%
Some examples of these images are shown in \figurename~\ref{fig:data} (a).





\begin{table}[t]

\caption{Different rain streaks settings.}\label{tab:rain}

\resizebox{\linewidth}{!}{
\begin{tabular}{c|cccc}
\hline
    Range & Quantity & Width & Length & Direction \\
    \midrule
    Small & $[200, 300]$ & \{5\} & $[30, 31]$ & $[-5^{\circ},5^{\circ}]$ \\
    Medium & $[200, 300]$ & \{5,7,9\} & $[20, 40]$ & $[-30^{\circ}, 30^{\circ}]$ \\
    Large & $[200, 300]$ & \{1,3,5,7,9\} & $[5, 60]$ & $[-70^{\circ}, 70^{\circ}]$ \\
    \bottomrule
\end{tabular}}
\end{table}



\subsubsection{Rain streaks synthesis.}
\label{sec:method:1:rain}
%
Since it is hard to collect a large number of real-world rainy/clean image pairs, we follow the previous deraining works \cite{garg2006photorealistic,fu2017removing} to synthesize rainy images for research.
%
We use two kinds of rain streaks for training and testing, separately.
%
For training, we use the computational model \footnote{The re-implementation of the PhotoShop rain streaks synthesis method. Please refer to \href{https://www.photoshopessentials.com/photo-effects/photoshop-weather-effects-rain/}{this link}.} to render the streaks left on the image by raindrops of varying sizes, densities, falling speeds, and directions.
%
This model allows us to sample rain streaks from different distributions.
%
We adopt three rain image ranges for training, where different ranges may lead to different generalization effects, see \figurename~\ref{fig:data} (b) for a convenient visualization and Tab. \ref{tab:rain} for the detailed settings.
%
For testing, we use the synthetic rain patterns in \cite{yang2017deep}.
%
Although the rain streaks are visually similar to humans, they still pose a huge generalization challenge to existing models.



\subsubsection{Loss Function.}
%
In low-level vision, the loss function is usually defined by the difference between the output image and the ground truth.
%
In our study, we use the $\ell_1$-norm loss, as it is the most commonly used and simplest loss function.


\subsection{Decoupling Analysis of Rain Removal Results}
\label{sec:method:2:fine}

%
Generally, the evaluation of a deraining model is to compute similarity metrics between the output and ground truth images \cite{gu2020pipal}.
%
However, such an evaluation of the whole image may lead to unfair comparison.
%
For example, an image with perfect background reconstruction but inferior rain removal may have a higher PSNR value than that with perfect rain removal but a slightly flawed background reconstruction (\eg, color shift).
%
Such quantitative results would introduce systematic errors.


In this work, we discuss the rain removal effect separately from the reconstruction of the background regions.
%
The generalization performance of a deraining model is mainly shown in the form of removing unseen rain.
%
The reconstruction of the background may affect the visual effect but is irrelevant to the removal of rain streaks.
%
It can be seen that the pixels in $R$ without rain streaks should be black, while rain streaks will appear brighter, as shown in \figurename~\ref{fig:data} (b).
%
After synthesis, these black areas reflect the background area, while the brighter areas indicate the rainy regions.
%
A perfect rain removal effect should do minimal damage to the background area and remove the additive signal from the rain streaks area.
%
By processing the $R$ to a binary mask $M$ using a threshold $t$, where $M_{[i,j]}=0$ if $R_{[i,j]}\leq t$ and $M_{[i,j]}=1$ if $R_{[i,j]}>t$, the output image $\tilde{I}$ is segmented into the rain streaks part $\tilde{I}\odot M$ and the background part $\tilde{I}\odot(1-M)$.
%
We then have two performance metrics as shown in Fig. \ref{fig:output} (b):
%

\begin{itemize}
\setlength{\itemsep}{2pt}
\setlength{\parsep}{0pt}
\setlength{\parskip}{0pt}
    \item $E_R=\sqrt{\mathbb{E}[(\tilde{I}\odot M-I\odot M)^2]}$ gives the \emph{Rain Removal Performance}. A network with poor generalization will not remove rain streaks. This term measures the changes made by the network in the rainy regions. A higher value reflects better rain removal performance.
    \item $E_B=\sqrt{\mathbb{E}[(\tilde{I}\odot (1-M)-B\odot (1-M))^2]}$ gives the effect of \emph{Background Reconstruction} by comparing the background regions with the ground truth. A large error in this term means poor reconstruction quality.

\end{itemize}




\subsection{Deep Models}

%
We summarize existing networks into three main categories.
%
The first category is a network composed of convolutional layers and deep residual connections, and we use the ResNet \cite{ledig2017photo} as a representative.
%
The second category is the network with an encoder-decoder design, and we use UNet \cite{ronneberger2015u} as a representative.
%
UNet introduces down-sampling and up-sampling layers to extract global and multi-scale features, which have been proven successful in many deraining networks.
%
The last category is the image processing Transformer.
%
Transformer \cite{shi2022rethinking,zhang2022accurate,zheng2022cross} is a new network structure characterized by self-attention operations.
%
We include SwinIR \cite{liang2021swinir} as a representative Transformer in our study.


\section{Understanding Generalization}\label{sec:understanding}

%
We next conduct experiments based on the above analysis methods.
%
Our analysis consists of two aspects -- the rain removal effect and the background reconstruction effect.



\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figs/generalization_div_geo.pdf}

    \caption{The relationship between the number of training patches and their rain removal performance. The $x$-axis represents the patch number, and the $y$-axis represents the rain removal effect $E_R$. Higher values on the $y$-axis mean better rain removal. The test rain patterns are not in the training set. The effect of rain removal at this time reflects the generalization performance. The qualitative results are obtained using ResNet.}
    \label{fig:rain-removal}

\end{figure*}

\subsection{Generalization on Rain Removal}

%
We conduct an analysis of the rain removal effect on unseen rain streaks.
%
Importantly, since we use different types of rain streaks for training and testing, the results presented in this section all reflect generalization performance.
%
After extensive experimentation, we arrive at the following observations.




\subsubsection{Training with fewer background images leads to a better deraining effect.}
%
Firstly, we maintain the range of rain streaks at the medium level, replacing the background images to construct different training objectives.
%
Experiments are conducted across all four categories of images.
%
For each category, we use a training set comprising different quantities of image patches.
%
We then test the rain removal efficacy of these models.
%
The test images utilize rain streaks proposed in \cite{yang2017deep}.
%
The testing background images are sampled from the corresponding categories and are distinct from those in the training set.
%
The experimental results are presented in \figurename~\ref{fig:rain-removal}.
%
Despite variations in background images and networks, these experimental results all speak to the same trend.
%
Remarkably, the deraining models trained on merely eight image patches can effectively handle unseen rain streaks.
%
Conversely, models trained with a large number of background patches fail to remove these rain streaks.
%
\emph{This observation deviates from conventional wisdom.}
%
In between these two extreme states, the rain removal effect deteriorates with an increase in the number of training images.
%
By the time the patch number reaches 256, the networks have already lost most of their rain-removal ability.
%
As the number of patches increases from 1024 to 30,000, the rain removal effect does not change significantly -- they all fail to remove unseen rain.
%
This trend is also reflected in the qualitative results.


Here, we attempt to elucidate this intriguing phenomenon.
%
Although we articulate the training objective as the removal of rain streaks from images (through the loss function), the network has two alternative strategies to minimize the training loss.
%
The first strategy involves recognizing and removing rain streaks, while the second involves recognizing and reconstructing the image background.
%
If the learning strategy is not specified, the network will ``opt'' for the simpler of these two strategies.
%
When a large number of background patches are utilized in training, learning to reconstruct backgrounds becomes significantly more complex than learning to remove rain.
%
Consequently, the network chooses to remove the rain.
%
This decision, however, can lead to an overfitting problem: when new rain streaks differ from those used in training, the network fails to recognize and remove them.
%
Conversely, when the background image comprises only a few image patches, learning the background becomes easier than learning rain streaks.
%
In this scenario, the network recognizes image components in the background without overfitting to the features of rain streaks.
%
As a result, the model demonstrates superior rain removal capabilities in images with unseen rain streaks.


\begin{figure}[t]
    \centering
    \hspace{-8pt}\includegraphics[width=0.98\linewidth]{figs/sharpness.pdf}

    \caption{Examples from DIV2K classified as low, medium, and high sharpness.}
    \label{fig:sharpness}

\end{figure}


\subsubsection{Training with less sharp background patches improves the deraining generalization.} \label{sec:sharpness}

\begin{figure}[t]
    \centering
    \hspace{-8pt}\includegraphics[width=0.98\linewidth]{figs/iter-rain-back.pdf}

    \caption{\textbf{(a)} The test performance of deraining generalization $E_R$ and background reconstruction $E_B$ of SwinIR, trained with training data of different sharpness levels. \textbf{(b)} Visual test results of models at different iterations. Zoom in for better comparison.}
    \label{fig:iter}

\end{figure}



\begin{figure}[t]
    \centering
    \hspace{-8pt}\includegraphics[width=0.9\linewidth]{figs/DepictQA_show.pdf}

    \caption{\textbf{(a)} Framework for evaluating rain removal performance using DepictQA. 
    \textbf{(b)} Comparative results of different SwinIR models evaluated with DepictQA.}
    \label{fig:depictQA_sharpness}

\end{figure}

In addition to the quantity of training data, we suggest that the sharpness of image patches during training also influences the network's generalization ability.
%
Sharper background images are more challenging for the network to learn, leading the network to ``shortcut'' by focusing on the relatively simpler rain streaks. 

Here, we calculate the sharpness value of an image by applying the Laplacian operator to its grayscale version. 
%
This operator, widely used in image processing, detects rapid changes in pixel intensity. 
%
It effectively highlights regions with significant transitions, such as edges, fine textures, and intricate details.
%
To quantify sharpness, we calculate the variance of the Laplacian output, which provides a numerical value: $ S = Var(Laplacian(I_{gray})) $.
%
Here, \( I_{gray} \) represents the grayscale image, and \(Var(\cdot)\) refers to the variance.
%
In this case, the variance is applied to the Laplacian of the grayscale image to assess its sharpness.



A higher sharpness value indicates an image with rich texture and details, while a lower value suggests a blurrier or less distinct image.
%
DIV2K is fixed as the training dataset.
%
We categorized the images into three levels based on their sharpness values: low sharpness (< 50), medium sharpness (500 to 1000), and high sharpness (> 5000). Fig. \ref{fig:sharpness} illustrates several examples from DIV2K.
%
Then, we selected 10,000 background images from the dataset for each sharpness category--low, medium, and high sharpness--to create three corresponding training sets.
%
The training rain range is set to be the medium range.
%
The test set consists of 100 images with Rain100L rain pattern as shown in Fig. \ref{fig:data} (b).
%
SwinIR is chosen as the representative model, and the total training iterations are 100,000. 
%
The average performance curves for rain removal and background reconstruction on the test set for three SwinIR models are shown in Fig. \ref{fig:iter} (a).


First, as illustrated in Fig. \ref{fig:iter} (a), SwinIR trained with low-sharpness images outperforms models trained with medium- and high-sharpness images in terms of unknown rain removal across all iterations.
%
This suggests that training with low-sharpness data helps enhance the network's generalization ability. 
%
However, due to the lack of sharpness in the training patches, the network requires more iterations to effectively reconstruct sharp backgrounds.


\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/rain_range.pdf}

    \caption{When trained with different rain ranges, the model exhibits different rain removal effects. The $y$-axis represents the quantitative rain removal effect. When the rain removal performance is lowered to the blue dashed line, the qualitative effect of removing rain starts to decrease significantly. We use ResNet in this experiment.}
    \label{fig:rain-range}

\end{figure*}



Second, significant differences between the three models are observed even at 5k iterations. 
%
SwinIR-medium and SwinIR-high obtain sharper background reconstruction than SwinIR-low as shown in Fig. \ref{fig:iter} (b), though at the cost of reduced rain removal generalization.
%
It indicates that the networks fit the training rain streaks early on.
%
Additionally, their rain removal values \( E_R \) remain almost unchanged throughout the training process, as illustrated by the blue and yellow lines in Fig. \ref{fig:iter} (a).
%
In contrast, SwinIR-low focuses more on fitting the low-sharpness background content.
%
This results in less sharp background reconstructions but better rain removal on the test set, as seen in the first row of Fig. \ref{fig:iter} (b).

Third, the performance curve of SwinIR-low in Fig. \ref{fig:iter} (a) reveals a tradeoff between the network's ability to generalize rain removal and its background reconstruction performance. 
%
As training progresses, the network improves its background reconstruction, as indicated by the increasingly clear background in the first row of Fig. \ref{fig:iter} (b). 
%
However, this also causes the network to fit the training rain streaks, thereby weakening its generalization ability, as the testing rain streaks become more noticeable.
%
Concluding from Fig. \ref{fig:iter}, we can rank the preferences of network fitting as follows: low-sharpness content > training rain patterns > medium- and high-sharpness content.




In addition to quantitative metrics like $E_R$, we also select the vision language model, DepictQA \cite{you2023depicting,you2024descriptive,zhu2024intelligent}, to depict image quality beyond scores, addressing the limitations of numerical-only evaluations.
%
DepictQA effectively describes image quality linguistically, aligning with human perception.
%
It supports scenarios such as assessment and comparison, as well as both full-reference and no-reference evaluations.
%
To this end, we fine-tune DepictQA to focus solely on the deraining effect, allowing it to evaluate the generalization capability of different networks when handling unseen rain patterns.
%
Fig. \ref{fig:depictQA_sharpness} (a) presents the image assessment paradigm.
%
The user inputs two images with a prompt asking DepictQA which image contains fewer rain artifacts.
%
DepictQA then provides an evaluative response.
%
Fig. \ref{fig:depictQA_sharpness} (b) presents rain removal evaluation results across 100 test images, comparing SwinIR-Low, SwinIR-Medium, and SwinIR-High. 
%
The results support the conclusion that using data with low sharpness leads to improved generalization.




\subsubsection{The relative complexity between the background and rain determines the network behavior.}
%
To corroborate the aforementioned conjecture, we modify the range of the rain streaks used in training, as outlined in Section \ref{sec:method:1:rain}.
%
When employing a medium rain range, the rain removal effect diminishes when training with 64 background patches.
%
According to our explanation, a larger range of rain streaks complicates the network's task of learning the rain pattern.
%
As a result, the rain removal effect does not deteriorate until a larger number of background patches are used for training.

The experimental results are depicted in \figurename~\ref{fig:rain-range}.
%
It can be observed that, across all three training rain ranges, the rain removal effect decreases as the number of background patches increases.
%
When a sufficient number of background images are used for training (30,000 patches), even a large training rain range fails to produce a model capable of achieving adequate rain removal performance.
%
This suggests that the large rain range does not encompass our testing scenarios.
%
When training with a large rain range, the network displays a significant drop in rain removal performance only when more than 512 background patches are used for training.
%
Conversely, a model trained on a small rain range cannot exhibit satisfactory rain removal even with only 16 background training image patches.
%
These results indicate that network behaviors are influenced by the relative relationship between the background image and rain streaks.
%
The complexity or learning difficulty of the medium range rain is approximately less than that of 64 training background patches, while the complexity of the large range rain is approximately less than that of 512 training background patches.
%
The network tends to take shortcuts, or ``select'' the easier learning pathway, depending on the situation.


\subsubsection{A more complex background set makes it harder for the network to learn.}
%
We next modify the category of the background images used for training and monitor the resulting behaviors of the models.
%
To facilitate comparison across different image categories, we normalize the deraining effect to a range between 0 and 1.
%
The results are depicted in \figurename~\ref{fig:complexity} (a).
%
The most intuitive conclusion is that, even when the number of training patches remains consistent, different image categories can lead to varying degrees of rain removal effectiveness.
%
For instance, in the case of CelebA images, a jump from 8 to 16 patches results in a sharp decline in deraining performance.
%
This contrast is evident when compared to natural image patches, where an increase to 16 patches does not result in a significant drop in rain removal effect.
%
Moreover, for image patches sourced from Manga109 and Urban100, the rain removal effect does not exhibit a significant drop until the patch count exceeds 32.
%
According to our interpretation, as the number of training patches increases, the more complex image categories will prompt the models to experience an earlier performance decline.
%
Our results suggest that the complexity of these four image categories can be ranked, in ascending order, as CelebA, DIV2K, Manga109, and Urban100.


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/complexity.pdf} 

    \caption{\textbf{(a)} The relationship between training patches amount and their normalized rain removal performance. When the value is lowered to the dashed line, the qualitative effect of removing rain starts to decrease significantly. \textbf{(b)} The averaged complexity of different image categories by \cite{bagrov2020multiscale}.}
    \label{fig:complexity}

\end{figure}



\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/background_curve.pdf}
    \vspace{-15pt}
    \caption{The relationship between the number of training patches and their background reconstruction effect. For each plot, the $x$-axis represents the patch number, and the $y$-axis represents the reconstruction error of the background.}
    \label{fig:background_curve}

\end{figure*}


This ordering roughly aligns with our human perception.
%
Face images, such as those in the CelebA set, exhibit strong global and local structures.
%
DIV2K images, while rich in texture, possess a comparatively simple global structure.
%
Manga images, on the other hand, lack complex textures but often contain text elements and intricate edges.
%
Lastly, Urban images predominantly comprise repetitive patterns, such as stripes and grids.
%
To corroborate our conclusion, we validate it using a complexity system derived from a mathematical model.
%
Bagrov \etal \cite{bagrov2020multiscale} proposed a computational method for estimating the structural complexity of natural patterns, which includes natural images.
%
We compute the multi-scale structural complexity for these four image categories, and the results corroborate our proposed ordering, as depicted in \figurename~\ref{fig:complexity} (b).
%
This provides mathematical evidence to support our claim.



\subsection{Reconstruction on Background}
\label{sec:back_recon}

%
The aforementioned results indicate that the deraining capability can be enhanced by limiting the complexity of the background images used for training.
%
However, utilizing only a restricted set of background images is not without its drawbacks.
%
While it prevents the network from overfitting to rain patterns, it may conversely prompt the network to overfit to the limited selection of background images.
%
We also conduct investigations to address this particular concern.


Using the decoupled evaluation metric $E_B$ outlined in Section~\ref{sec:method:2:fine}, we are able to independently assess the reconstruction of the background.
%
The results, as depicted in \figurename~\ref{fig:background_curve}, show that as the number of training images increases, the quality of background reconstruction also improves.
%
Remarkably, training with just 256 background patches can already yield a satisfactory background reconstruction effect.
%
Adding more training images beyond this point does not lead to further performance improvements.
%
These findings are surprising and counter-intuitive.
%
We typically assume that training low-level vision models requires a large number of images.
%
However, our research suggests that training with an excessive number of background images does not necessarily enhance the reconstruction performance, but rather exacerbates the model's tendency to overfit to rain streaks.
%
Another unexpected result is that a model trained with merely 256 images can already handle most image components.
%
This might imply that the complexity of image components and features for a low-level vision network might not be as high as traditionally believed.


\begin{table}[t]
\caption{Function denoising vs. image restoration: $(x, y + n)$ represents the noisy points, where $(x, y)$ denotes the true data, $n$ is the noise. Similarly, $B + R$ represents a low-quality image, with $B$ as the background and $R$ as the additive degradation.
}\label{tab:toy_example}

\resizebox{\linewidth}{!}{
\begin{tabular}{c|cc}
\hline
    Tasks & Function Denoising & Image Restoration \\
    \midrule
    Background Content & Cosine Functions & Natural Images \\
    Degradation Type & Gaussian  Noise $ n \sim \mathcal{N} (\mu, \sigma)$ & Additive Degradation $R$ \\
    Training Sample & Function Segments & Image Patches  \\
    Inference Input & Noisy Data $(x,y+n)$ & Low-Quality Image $B+R$\\
    Inference Output & Denoised Data $(x,y)$ & Restored Image $B$ \\
    \bottomrule
\end{tabular}}
\end{table}

\section{Further Validation with Analogous Task}\label{sec:analogous}

In this section, we further validate the "slacking off" behavior observed in neural networks through a more intuitive approach.
%
For this purpose, we designed an analogous task, providing a more fundamental and controllable scenario, thus enabling a clearer observation of network behavior.



\subsection{Construction of Function Denoising}

Image deraining can be viewed as recovering the ground truth (GT) signal from degraded observations with additive noise.
%
Inspired by this, we construct an analogous task following the same problem formulation. 
%
Specifically, we select univariate cosine functions as the GT signal and add Gaussian noise as the degradation.
%
The cosine function, with its distinct features, facilitates easy analysis of the fidelity in the networkâ€™s reconstruction results.
%
Meanwhile, the simplicity of Gaussian noise makes the degradation pattern easier to observe and analyze.
%
Specifically, the GT function is defined as follows:
%
\begin{equation}
y = f(x) = 10\cos\left(\frac{O\pi}{10}x\right), \quad x \in [-10, 10],
\end{equation}
%
where the order \( O \) controls the oscillation frequency of the function.
%
Moreover, varying \( O \) allows for directly adjusting the function's complexity, facilitating diverse experimental configurations.
%
The data is subsequently affected by Gaussian noise \( n \) drawn from the distribution \( \mathcal{N}(\mu, \sigma) \).
%
Overall, the goal is to train a neural network that maps the noisy data $(x, y + n)$ to the clean output $(x, y)$.


The corresponding relationships between the function denoising and image restoration are illustrated in Table \ref{tab:toy_example}. 
%
Different function orders introduce varying levels of complexity to the background function, corresponding to the differing complexities of content in background images.
%
While the degradation patterns can be adjusted by the hyperparameters $\mu$ and $ \sigma$ corresponding to the different degradation in the image restoration task.
%
During the training stage, acquiring training data is analogous to collecting small cropped patches from full images.
%
The training data consists of 10,000 function segments. 
%
Each segment is generated by first sorting all function points in ascending order along the x-axis.  
%
Next, a small range of x-values is randomly selected for each segment, and the corresponding points within this range form the segment.  
%
For example, a training function segment can be represented as $\{(x, y+n) \mid y = f(x), \ x \in (a, b), n \sim \mathcal{N}(\mu, \sigma)\}.$
%
Each segment contains 128 sample points, analogous to each image patch with a fixed pixel number. 
%
During the test stage, all noisy points are fed into the trained network \( D \) to produce the denoised output, analogous to processing a complete low-quality image.  
%
The network is small-scale, given the simplicity of the task.  
%
It consists of five convolutional layers with approximately 400k parameters, using Leaky ReLU as the activation function.  
%
The training loss function is defined as $L = \| (x, y) - D(x, y+n) \|$. 



\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/demoO1.pdf}

    \caption{Four test results of the network trained with the setting $O=1, \mu=0, \sigma=1$. Red titles indicate that the testing and training settings are the same.}
    \label{fig:demoO1}

\end{figure}
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/demoO8.pdf}

    \caption{Four test results of the network trained with the setting $O=8, \mu=0, \sigma=1$. Red titles indicate that the testing and training settings are the same.}
    \label{fig:demoO8}

\end{figure}


\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/showall_order2.pdf}

    \caption{Each row, from top to bottom, shows denoising networks trained with cosine function order $O = 1, 4, 8$, where Gaussian noise has $\mu = 0, \sigma = 1$. Each subplot title represents the order used during testing, with Gaussian noise remaining the same, $\mu = 0, \sigma = 1$. The testing Mean Square Error (MSE) is also reported in the title. Red titles indicate that the testing and training settings are the same.}
    \label{fig:all_order}
\end{figure*}
\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/showall_noise2.pdf}

    \caption{Each row from top to bottom shows denoising networks trained with cosine function order $O = 1, 4, 8$, where Gaussian noise has $\mu = 0, \sigma = 1$. Each subplot title represents the Gaussian noise used during testing, with the cosine function order remaining the same as the training one. The testing Mean Square Error (MSE) is also reported in the title. Red titles indicate that the testing and training settings are the same.}
    \label{fig:all_noise}

\end{figure*}



\subsection{Fitting the Background Function}

We train the network using a background function with \(O = 1\) and Gaussian noise characterized by \(\mu = 0\) and \(\sigma = 1\).
%
The GT function becomes $ y = 10\cos(\frac{\pi}{10}x)$ and the noise is sampled from the distribution $\mathcal{N}(0, 1)$.
%
Fig. \ref{fig:demoO1} shows four test results with different settings. By comparing these results, we can draw the following observations:
\begin{itemize}
    \item By examining the situation where \textit{the function type remains unchanged while the noise varies} (Fig. \ref{fig:demoO1} (a) and (b)), we observe that the predicted results closely match the ground truth (GT) function. This indicates that the network demonstrates strong generalization ability and effectively removes unknown noise.
    \item When comparing the case where \textit{the function type changes while the noise remains constant} (Fig. \ref{fig:demoO1} (a) and (c)), the network fails to recover the GT cosine function with \(O=8\). Instead, it generates a result similar to the training cosine function with \(O=1\), suggesting an overfit to the training background function.
    \item From the scenario where \textit{both the function type and noise change} (Fig. \ref{fig:demoO1} (a) and (d)), we observe that the network's predictions still resemble the cosine function with \(O=1\). It shows no resemblance to the pattern of the input data points. This further suggests that the training cosine function with \(O=1\) has a significant impact on the network's behavior.
\end{itemize}
%
From the above observations, it can be observed that the network has overfitted to the training background function with \(O=1\), while the noise pattern has little influence on the network's predictions.


We also train different networks with cosine functions for \(O = 1, 4, 8\), where fixed Gaussian noise has \(\mu = 0\) and \(\sigma = 1\), and then test them under various test settings.
%
The test results are shown in Fig.~\ref{fig:all_order}.
%
By comparing Fig.~\ref{fig:all_order} (a), (e), and (i), we observe that the MSE achieved by the network increases from 0.0009 to 0.0472. %
%
This suggests that higher-order background functions introduce greater challenges to the learning process.
%
The experimental results in Fig.~\ref{fig:all_order} reveal distinct behavioral patterns across networks trained with different function orders.
%
For \(O=1\) (Figs.~\ref{fig:all_order} (a)-(c)), the network predictions demonstrate strong fidelity to the training function morphology, maintaining consistent alignment even when evaluated under diverse test configurations.
%
Networks trained with $O=8$ (Figs.~\ref{fig:all_order} (g)-(i)) exhibit contrasting behavior, showing preferential alignment with the test background functions in test environments rather than retaining characteristics of the training functions.
%
The $O=4$ configuration (Figs.~\ref{fig:all_order} (d)-(f)) displays intermediate characteristics. 
%
While achieving general proximity to test functions, the predictions exhibit noticeable fluctuations - particularly evident in Fig.~\ref{fig:all_order} (d), where the trajectory reverts toward the training function pattern.
%
From the first to the last row in Fig.~\ref{fig:all_order}, we observe a transition in network behavior.
%
 \textit{As the order \(O\) of the training background function increases, the network shifts from overfitting to not fitting the training function during inference.}





\subsection{Fitting the Additive Noise}
Here, we consider the network training settings with a function order of \(O = 8\) and Gaussian noise characterized by \(\mu = 0\) and \(\sigma = 1\). 
%
Four test results are illustrated in Fig.~\ref{fig:demoO8}. 
%
We can draw the following observations:
\begin{itemize}
    \item By comparing the situation where \textit{the function type remains unchanged while the noise varies} (Fig. \ref{fig:demoO8} (c) and (d)), we can observe that the network's prediction closely follows the input noisy data. It does not exhibit the same behavior as the network trained with \(O=1\).
    \item When comparing the case where \textit{the function type changes while the noise remains the same} (Fig. \ref{fig:demoO8} (c) and (a)), we can observe that the network can reconstruct the corresponding GT function. This demonstrates the network's ability to remove Gaussian noise with \(\mu = 0\) and \(\sigma = 1\) across different background functions.
    \item From the scenario where \textit{both the function type and noise change} (Fig. \ref{fig:demoO8} (c) and (b)), the predicted result shows no resemblance to the training function's shape and falls within the noisy data. They deviate from the GT values by approximately 10, which corresponds to the mean of the noise. This indicates that the network behavior is more influenced by the noise.
    %
    
\end{itemize}
%
From the above observations, we can conclude that the network trained with \(O=8\) has overfitted to the training noise.
%
Its prediction falls within the noise distribution, treating all noise as if it were sampled from $\mathcal{N}(0, 1)$. 
%
Based on the experiments shown in Fig. \ref{fig:demoO1} and \ref{fig:demoO8}, we observe the following ranking of the network's tendency to overfit during training: first-order cosine function ($O=1$) > Gaussian noise $\mathcal{N}(0, 1)$ > higher-order cosine function ($O=8$).


The test results of networks trained with function orders \(O=1, 4, 8\) under different Gaussian noise types are reported in Fig.~\ref{fig:all_noise}.
%
As shown in Fig.~\ref{fig:all_noise} (a)â€“(c), the network trained with \( O = 1 \) produces predictions that closely follow the GT background function curves, even in the presence of different complex noises. 
%
In contrast, as illustrated in Fig.~\ref{fig:all_noise} (g)â€“(i), the network trained with \( O = 8 \) increasingly deviates from the GT curves as the noise level rises, aligning more with the noisy input. 
%
The performance of the network trained with \( O = 4 \) (see Fig.~\ref{fig:all_order} (d)â€“(f)) falls between these two cases.
%
For example, when comparing the results in Fig.~\ref{fig:all_noise} (c), (f) and (i), where the test noise is sampled from \( \mathcal{N}(10, 5) \), it becomes evident that increasing the training function order causes the networkâ€™s predictions to drift further from the GT curves and align more closely with the noisy data distribution. 
%
Specifically, the network trained with \( O = 1 \) demonstrates strong generalization to unseen noise, achieving a MSE of only 0.3544, while the network trained with \( O = 8 \) results in a significantly higher MSE of 49.7739, indicating reduced generalization.
%
In summary, \textit{as the order \( O \) of the training background function increases, the network progressively fits the noise pattern instead of the GT background function.}


\subsection{Conclusion of the Analogous Experiment}
In this section, we design a more intuitive experimental setting to facilitate easier observation and further analyze the generalization behavior of low-level vision networks.
%
We find that the relative complexity of the background function and the noise level both influence the network's learning.
%
Using a training function order of \(O=1\), with Gaussian noise \(\mu=0\) and \(\sigma=1\) as the baseline, the network overfits the background function.
%
The test results closely resemble the training function when faced with different noisy data. 
%
As the noise level remains constant and the training function becomes increasingly complex, the network gradually shifts from overfitting the training background function to the training noise pattern. 
%
At this point, the network can only handle noise encountered during the training phase.

Based on the observations above, \textbf{we believe that a network fitting to the background content instead of degradation exhibits robust generalization ability}.
% 
This allows the network to effectively handle unseen degradation.


\section{Implication}\label{sec:implict}
This paper concludes that the key to improving the generalization ability of low-level vision networks is to guide the network to learn the desired image content.
%
In this section, we propose some feasible solutions based on this insight.
% %
% First, by balancing the learning difficulty of the image content and rain streaks, we can prevent the network from overfitting to degradations. 
% %
% Second, we leverage the content priors embedded in pre-trained networks, enabling the network to fit the image content.

\subsection{Adjusting Training Set to Avoid Degradation Overfitting} \label{implication1}
Our previous experiments have yielded three significant practical findings:
%
(1) By limiting the number of background images used in training, the network can focus more on learning the image content instead of overfitting to the rain streaks;
%
(2) Enlarging the range of rain streaks in the training set can allow for the use of more background images in training;
%
(3) Surprisingly, a small number of background images can already yield satisfactory reconstruction performance.
%
These findings can be directly applied to enhance the generalization capabilities of existing models with minimal modifications.
%
Our strategy is straightforward: find a balance between the quantity of background images and the range of rain streaks in the training set to avoid overfitting to rain streaks.



\begin{table}[t]
 \caption{Quantitative comparisons between different models. $\uparrow$ means the higher the better while $\downarrow$ means the lower the better.}\label{tab:app}
    \centering
    \huge
    \resizebox{0.48\textwidth}{!}{
        \begin{tabular}{cc|ccc|ccc|ccc}
        \toprule
        \rowcolor[gray]{.9} \multicolumn{2}{c|}{Training Objective} & \multicolumn{3}{c|}{ResNet} & \multicolumn{3}{c|}{SPDNet~\cite{yi2021structure}} & \multicolumn{3}{c}{RCDNet~\cite{wang2020model}} \\
        \rowcolor[gray]{.9} Back. & Range & $E_R$ $\uparrow$ & $E_B$ $\downarrow$ & PSNR $\uparrow$ & $E_R$ $\uparrow$ & $E_B$ $\downarrow$ & PSNR $\uparrow$ & $E_R$ $\uparrow$ & $E_B$ $\downarrow$ & PSNR $\uparrow$ \\
        \midrule
        30k & Medium & 31.24 & 10.79 & 25.15 & 33.63 & 5.49 & 30.51 & 26.55 & 5.41 & 28.54 \\
        \midrule
        64 & Medium & 53.33 & 25.02 & 20.87 & -- & -- & -- & 45.47 & 14.78 & 25.32 \\
        512 & Large & -- & -- & -- & 39.88 & 8.91 & 28.57 & 37.53 & 7.16 & 29.60\\
        256 & Large & 45.64 & 16.51 & 24.30 & 38.87 & 8.03 & 29.40 & 40.40 & 8.52 & 29.08 \\
        128 & Large & 51.75 & 23.53 & 21.45 & 43.20 & 14.59 & 25.67 & 44.67 & 13.72 & 26.09\\
        \bottomrule
        \end{tabular}
        }
        \hfill
\end{table}

Some quantitative results are presented in Tab. \ref{tab:app}.
%
We use three deraining models as baselines (ResNet, SPDNet \cite{yi2021structure}, RCDNet \cite{wang2020model}) and demonstrate the power of the proposed simple strategy.
%
We use 30,000 background images and medium-range rain to train our baseline models.
%
The test set is the R100 dataset \cite{yang2017deep}.
%
We quantify the deraining effect and the background reconstruction effect according to the decouple evaluation metrics $E_R$ and $E_B$.
%
We also test PSNR as a reference.
%
It can be seen that using the existing training methods cannot generalize well to the unseen rain of R100, which is shown by the poor deraining performance in Tab. \ref{tab:app}.
%
However, due to the learning on a large number of images, the reconstruction errors of the baseline models are generally lower.
%
Thus the PSNR values cannot objectively reflect the rain removal effect.
%
We reduce the training background images to 64, which is the upper limit of the image number that can make the model generalize under medium range rain.
%
At this time, the rain removal performance has greatly improved, but at the cost of background reconstruction performance.
%
By enlarging the rain range and training with more background images, we are able to achieve a trade-off between rain removal performance and background reconstruction.


\begin{figure*}[ht]
    \centering
    % \vspace{-5pt}
    \includegraphics[width=\linewidth]{figs/compare.pdf}
    % \vspace{-15pt}
    \caption{Visualization of the deraining results on a synthetic image. Zoom in for better comparison.}
    \label{fig:compare}
% \vspace{-5pt}
\end{figure*}

\begin{figure*}[ht]
    \centering
    % \vspace{-5pt}
    \includegraphics[width=\linewidth]{figs/compare-real.pdf}
    % \vspace{-15pt}
    \caption{Qualitative results on real-world test images. Zoom in for better comparison.}
    \label{fig:real}
    % \vspace{-5pt}
\end{figure*}

\figurename~\ref{fig:compare} shows a qualitative comparison of these models under different training objectives.
%
It can be seen that even with the advanced network structure design, the rain removal effects of the baseline models of SPDNet and RCDNet are not satisfactory.
%
Using a larger range of rain can bring limited improvements.
%
In the case of medium range rain, reducing the background image to 64 significantly improves the rain removal effect and results in unstable image reconstruction.
%
When the rain range is enlarged, and the training background is set to 128 patches, the model can show excellent performance in rain removal and background reconstruction.
%
Note that we do not use additional data or improve the network structure throughout the process.
%
We only adjust the training data.

We also present the comparison on real images in \figurename~\ref{fig:real}.
%
In addition, semi-supervised methods \cite{wei2019semi,huang2021memory} have also been used to improve the deraining effect on real images, and we also include the representative method Syn2Real \cite{yasarla2020syn2real,yasarla2021semi}.
%
Syn2Real-Syn is trained on synthetic data, and Syn2Real-Real is trained on synthetic labeled data and real unlabeled data.
%
Due to the difference in the distribution of rain streaks, the models trained using synthetic data can not generate satisfactory rain removal effects.
%
When obtaining some real images, Syn2Real-Real can indeed achieve some improvement. However, these improvements are not brought by improving the generalization ability.
%
Because these methods manage to convert ``rain outside the training set'' to ``rain inside the training set''. Since data collection is extremely difficult, this method still faces great challenges in practice. Our method improves generalization performance and achieves better results on test images.


\subsection{Empowering Networks to Leverage Image Content Priors}
The key insight we previously discussed is that the network achieves better generalization when it fits the actual image content rather than the degradation information.
%
In Sec. \ref{implication1}, the strategy to improve generalization involved balancing the learning difficulty between the background and the rain streaks, preventing the network from overfitting to the degradation pattern.
%
On the other hand, we can also allow the network to directly leverage prior knowledge of the image content, eliminating the need for careful adjustment of its fitting direction during training from scratch.


In recent years, image restoration techniques based on generative models have been emerging rapidly \cite{pasd,stablesr,diffbir,supir}. 
%
Compared to traditional methods, these approaches not only generate more realistic image details but also exhibit significantly better generalization. 
%
However, the reason behind this improved generalization has not yet been thoroughly discussed in existing work.
%
Based on the findings of this paper, we suggest that the superior generalization of recent generative model-based image restoration methods can be attributed to their strong image content priors. 
%
Therefore, to enhance the network's generalization ability, leveraging content priors from pre-trained models is a promising idea. 
%
We conduct extensive experiments to validate the effectiveness of this strategy.


\begin{figure}[t]
\vspace{-10pt}
    \centering
    \includegraphics[width=\linewidth]{figs/vqgan.pdf}
    % \vspace{-10pt}
    \caption{Fine-tuning strategy of image draining with pre-trained VQGAN. }
    \label{fig:finetune}
    % \vspace{-10pt}
\end{figure}

\begin{figure}[pht]
% \vspace{-45pt}
    \centering
    \includegraphics[width=\linewidth]{figs/VQ_Ablation.pdf}
    \caption{\textbf{(a)} Ablation study of training from scratch without vector quantization. \textbf{(b)} The training performance on image deraining for the ablation VQGAN architecture without codebook learning. \textbf{(c)} The generalization performance of the ablation network. \textbf{(d)} The generalization performance of the fine-tuned VQGAN, as depicted in Fig.~\ref{fig:finetune}.}
    \label{fig:Ablation_generalization}
    % \vspace{-20pt}
\end{figure}


\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/VQ_compare.pdf}
    
    \caption{Visual comparisons across ResNet, SwinIR, UNet, and the fine-tuned VQGAN with content prior for image deraining and denoising tasks. }
    \label{fig:VQ_compare}
    
\end{figure*}



\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/Real_compare.pdf}
    
    \caption{Visual comparisons across ResNet, SwinIR, UNet, and the fine-tuned VQGAN with content prior in real-world image draining.}
    \label{fig:Real_VQ}
    
\end{figure*}



\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/DepictQA_compare_CNN.pdf}
    
    \caption{The preference results from DepictQA indicate that the content prior method outperforms other models in both deraining and denoising.}
    \label{fig:DepictQA}
    
\end{figure}

VQGAN \cite{esser2021taming} is a representative and fundamental generative model. 
%
It features an encoder-decoder architecture where the encoder compresses the input image into a lower-dimensional latent space, which is then quantized using vector quantization.
% 
The codebook in VQGAN, a finite set of learned vectors, plays a crucial role in the vector quantization process. 
% 
It enables the model to convert continuous latent features into discrete representations by mapping these features to the closest vectors in the codebook. 
% 
Each vector corresponds to a distinct feature or characteristic derived from the image dataset. 
% 
After quantization, the image is represented as a series of discrete codebook vectors that compress and simplify the original image content.
%
In this sense, the codebook can be regarded as an abstract content prior, capturing key features of the image space.
%
Therefore, we propose to fine-tune a pre-trained VQGAN, allowing the image deraining network to start with a strong prior of clear natural image content. 

Specifically, we utilize a VQGAN pre-trained on ImageNet, which has learned the content distribution of images from the dataset. 
%
We choose to fine-tune the encoder of this VQGAN while keeping the other modules frozen.
%
This allows the encoder to project input rainy images into the learned latent space, thus leveraging the pre-trained quantization and decoder modules.
%
The overall framework is illustrated in \figurename~\ref{fig:finetune}. 
%
The fine-tuning dataset includes 10,000 pairs of medium-range rainy images and corresponding ground truth images from ImageNet, trained over 50 epochs. 
% 
Meanwhile, the baseline traditional networks such as ResNet, SwinIR, and UNet are trained from scratch on 30,000 images from ImageNet for 100,000 iterations.
%
The test dataset comprises 300 images from ImageNet, with rain patterns derived from the Rain100L dataset.
%
As shown in \figurename~\ref{fig:VQ_compare} (a), the generalization comparison aligns closely with our expectations.
%
ResNet, SwinIR, and UNet struggle to effectively remove unfamiliar rain streaks, leaving noticeable streaks visible in the results.
%
In contrast, methods leveraging content priors demonstrate a significant advantage in generalization capability.
%
Even though the input rain streaks are not covered in the training stage, the method focuses on image content, allowing different rain patterns to be removed.


Furthermore, we extend this strategy to the image denoising task for further validation.
%
Similarly, we compare the ResNet, SwinIR, and UNet architectures.
%
ResNet, SwinIR, and UNet are trained for 100,000 iterations using 30,000 images with Gaussian noise $\mathcal{N}(0, 30)$.
%
While VQGAN is fine-tuned using 10,000 Gaussian noise images, also with $\mathcal{N}(0, 30)$, for 50 epochs.
%
The same 300 test images contain salt-and-pepper noise independently applied to the RGB channels, with salt and pepper each comprising 2\%.
%
As shown in \figurename~\ref{fig:VQ_compare} (b), the behavior of these networks is similar to theirs in the image deraining task.
%
Traditional networks trained from scratch still struggle to completely remove the noise, leaving noticeable artifacts in the output.
%
In contrast, methods incorporating content priors can effectively eliminate this previously unseen salt-and-pepper noise.
%
Additionally, similar to the assessment in Sec. \ref{sec:sharpness}, we utilize the DepictQA to qualitatively evaluate the rain removal generalization performance of the content prior method in comparison with other approaches.
%
As shown in Fig. \ref{fig:DepictQA}, whether for deraining or denoising tasks, the content prior method significantly outperforms others, achieving preference rates of over 90\%. 
%
From both qualitative and quantitative perspectives, we can conclude that the content prior approach does greatly enhance generalization.

The content-prior approach also proves effective in real-world image deraining tasks.
%
Fig. \ref{fig:Real_VQ} illustrates the comparative results of those models on real-world rainy images.
%
Traditional networks trained on synthetic rainy images (e.g., ResNet, SwinIR, and UNet) demonstrate limited deraining effectiveness, often producing outputs nearly indistinguishable from the input rainy images. 
%
In contrast, the content-prior method is able to remove entirely new, real rain patterns effectively.



During the fine-tuning stage, we trained only the encoder while keeping other modules fixed to develop a VQGAN-based image restoration approach, as illustrated in \figurename~\ref{fig:finetune}.  
%
This strategy compels the network to rely on the pre-trained codebook, which encodes features of the natural image space.  
%
We argue that this approach allows the network to better utilize image content features, leading to enhanced generalization performance.
%
To further verify this, we design a simple ablation network to observe the impact of incorporating content priors on the network's generalization performance.
%
Specifically, the general VQGAN network structure is adopted, but the vector quantization strategy is omitted. 
%
Thus, the ablation network is trained from scratch as a comparative experiment, as illustrated in \figurename~\ref{fig:Ablation_generalization} (a).
%
The training and testing image deraining results are shown in \figurename~\ref{fig:Ablation_generalization} (b) and (c). 
%
It can be seen that the ablation network performs very well on the training dataset, effectively removing all rain streaks.
%
However, when faced with unseen test rain streaks, the VQGAN architecture exhibits behavior similar to traditional networks, failing to remove unfamiliar rain streaks. 
%
The input rain streaks no longer retain their original shape but become part of the image content.
%
In the contrast, fine-tuned VQGAN can still handle the unseen testing rain patterns as shown in Fig.\ref{fig:Ablation_generalization} (d).
%
This ablation study further demonstrates that the generalization improvement indeed stems from the latent image content and features within the VQGAN's codebook.
%
The credit does not belong to the generative adversarial network training pipeline or the stacking of various modules within the architecture.


\section{Conclusion and Insights}\label{sec:conclusion}

In this paper, we investigate the generalization problem in LV models.
%
For more effective observation and analysis, we select the image deraining task as a case study due to its easily decoupled characteristics.
%
We argue that the generalization problem in LV is not simply due to insufficient network capacity.
%
Instead, our findings reveal that previous training strategies or consensus fail to promote generalization.
%
A promising solution is to guide networks to learn the underlying image content rather than degradation patterns.
%
We demonstrate that balancing the complexity of background images and degradations in the training set improves image distribution fitting.
%
Leveraging content priors from pre-trained generative models also enhances generalization.
%
Experiments on image deraining and denoising validate this strategy.
%
The lack of interpretability tools for LV models remains a challenge.
%
Our findings provide valuable insights into the generalization of LV models and aim to inspire future research in this field.


\bibliography{ref.bib}
\bibliographystyle{IEEEtran}




\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{figs/sources/Hu.jpg}}]{Jinfan Hu}
is currently a Ph.D. student at the Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, under the supervision of Prof. Chao Dong. He obtained his B.S. and M.S. degrees from the University of Electronic Science and Technology of China, Chengdu, where he was supervised by Ting-Zhu Huang and Lian-Jian Deng. His research interests focus on low-level computer vision and the interpretability of deep learning.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{figs/sources/zhiyuanyou1.jpg}}]{Zhiyuan You} is a Ph.D. student from Multimedia Laboratory at The Chinese University of Hong Kong, under the supervision of Prof. Chao Dong and Prof. Tianfan Xue. He obtained both his Bachelorâ€™s and Masterâ€™s degrees from Shanghai Jiao Tong University, where he was supervised by Prof. Xinyi Le and Prof. Yu Zheng. His research interests focus on low-level vision based on foundation models.
\end{IEEEbiography}
% % \vspace{11pt}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{figs/sources/jinjin.jpg}}]{Jinjin Gu} received his Ph.D. degree from the School of Electrical and Computer Engineering at the University of Sydney in 2024, under the supervision of Prof. Wanli Ouyang and Prof. Luping Zhou. Prior to that, he earned a Bachelor's degree in Computer Science and Engineering from The Chinese University of Hong Kong, Shenzhen, in 2020. He collaborates closely with Prof. Chao Dong and Prof. Junhua Zhao. His research focuses on computer vision and image processing, with additional interests in the interpretability of deep learning algorithms and the industrial applications of machine learning.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{figs/sources/kwzhu.jpg}}]{Kaiwen Zhu} is currently a Ph.D. student in Shanghai Jiao Tong University supervised by Prof. Chao Dong. He obtained his B.Eng. degree from Shanghai Jiao Tong University in 2024. He has been working as an intern researcher in Shanghai Artificial Intelligence Laboratory since 2023. His research focuses on low-level vision and intelligent agents.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{figs/sources/tfxue.jpg}}]{Tianfan Xue} is a Vice-Chancellor Assistant Professor at the Multimedia Lab in the Department of Information Engineering at the Chinese University of Hong Kong (CUHK). Prior to this, he worked in the Computational Photography Team at Google Research for over five years. He received his Ph.D. degree from the Computer Science and Artificial Intelligence Laboratory (CSAIL) at MIT in 2017. He also holds an M.Phil. degree from CUHK, obtained in 2011, and a Bachelorâ€™s degree from Tsinghua University. His research focuses on computational photography, 3D reconstruction, and generation. His work on bilateral based 3D reconstruction has won SIGGRAPH Honorable mention 2024. He also served as an area chair for WACV, CVPR, NeurIPS, and ACM MM.

\end{IEEEbiography}


\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{figs/sources/dongchao1.png}}]{Chao Dong} is a professor at Shenzhen Institutes of Advanced Technology, Chinese Academy of Science (SIAT), and Shanghai Artificial Intelligence Laboratory. In 2014, he first introduced deep learning method â€“ SRCNN into the super-resolution field. This seminal work was chosen as one of the top ten ``Most Popular Articles" of TPAMI in 2016. His team has won several championships in international challenges â€“NTIRE2018, PIRM2018, NTIRE2019, NTIRE2020 AIM2020 and NTIRE2022. He worked in SenseTime from 2016 to 2018, as the team leader of Super-Resolution Group.  In 202l, he was chosen as one of the World's Top 2\% Scientists. In 2022, he was recognized as the Al 2000 Most Influential Scholar Honorable Mention in computer vision. His current research interest focuses on low-level vision problems, such as image/video super-resolution, denoising and enhancement.

\end{IEEEbiography}




\end{document}


