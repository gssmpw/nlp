\section{Background and Related Works}
\label{sec:2}
% \subsection{LLM Inference and Serving}

\begin{figure}[!t] 
  \centering
  \includegraphics[width=0.8\linewidth]{img/pdcharater.pdf} % Adjust the width as needed
  \vspace{-1em}
  \caption{\small{Effects of batching on different phases (LLaMA-7B with each input having a sequence length of 1024).}}
  \label{fig:prefillvsdecode}
  \vspace{-1em}
\end{figure}

\noindent \textbf{Phases of LLM inference.}
Given an input prompt, the inference process of LLMs typically consists of two phases: \textit{the prefill phase} processes the prompt to compute the key-value (KV) cache and generates the first token in a single step, and \textit{the decode phase} takes the last generated token and KV cache as inputs to generate subsequent tokens. Different from the prefill phase, the decode phase is executed for several steps, with each step generating only one token, which makes the decode phase more memory bandwidth bounded than the computationally intensive prefill phase. 
% \jyh{There are three key metrics to evaluate LLM inference: \textit{time to first token (TTFT)}, \textit{time per output token (TPOT)}, and \textit{end-to-end (E2E) latency}, detailed explianation are shown in \autoref{appendix:pm}.}



\noindent \textbf{Performance metrics.}
% \subsection{Performance Metrics}
There are three key metrics to evaluate LLM inference: \textit{time to first token (TTFT)}, which measures the time of generating the first token; \textit{time per output token (TPOT)}, which quantifies the average time of generating each token in the decode phase; \textit{end-to-end (E2E) latency}, which assesses the overall processing time of a request (includes queuing, prefill, and decode costs).
For LLM serving systems, there are also two key metrics: \textit{Service Level Objective (SLO) attainment}, which represents the percentage (e.g., 99\%) of requests that can be served within a predefined time frame set by the SLO, and the SLO is often scaled to different multiples of single device execution latency (denoted as SLO scale) to evaluate system performance under different levels of SLO stringency; \textit{throughput}, which measures the number of requests a system can handle within a specified time period. Efficient LLM serving systems should optimize either of the metrics, and meanwhile meet the performance requirements of specific applications if necessary.








% There are four key metrics to evaluate LLM inference: 
% \mytextcircled{1} \textit{time to first token (TTFT)}, which measures the time cost of generating the first token; 
% \mytextcircled{2} \textit{time per output token (TPOT)}, which quantifies the average time cost of generating each token in the decode phase; 
% \mytextcircled{3} \textit{end-to-end (E2E) latency}, which assesses the overall time cost of a request (includes queuing, prefill, and decode costs); 
% \mytextcircled{4} \textit{throughput}, which represents the number of requests processed per second. 
% % An efficient LLM serving system should balance and optimize these metrics, and meanwhile satisfy the performance requirements of specific applications if necessary. 
% An LLM serving system should balance TTFT and TPOT for better user satisfaction, and optimize the E2E Service Level Objective (SLO) attainment or throughput of the system to be cost-effective. \jyh{Specifically, 
% % we test the system performance based on E2E SLO attainment and throughput. 
% SLO attainment represents the percentage of requests that can be served within a predefined time frame set by the SLO. And we scale the SLO to various multiples of the single request execution latency to evaluate system performance under varying degrees of SLO stringency.}


\noindent \textbf{Batching.}
% \subsection{Batching}
\label{sec:batching}
Due to the divergent workloads of the two phases, integrating batching strategies results in performance variations. 
% 
% \noindent \textbf{Batching in prefill.} 
In the prefill phase, a small batch size quickly saturates the GPU, yielding marginal benefits from further batching. Besides, execution latency increases linearly with batch size, rendering batching impractical when the TTFT constraints are strict. 
As shown in \autoref{fig:prefillvsdecode}, we conduct a small testbed with LLaMA-7B as an example, which reveals that when the total number of tokens in a batch exceeds 1024, GPU efficiency reaches a plateau rather than being further enhanced, showcasing the limited effect of batching on system performance during the prefill phase. 
% 
% \noindent \textbf{Batching in decode.} 
Due to the token-by-token processing nature of the decode phase, batching is essential for preventing low GPU utilization and enhancing efficiency. As shown in \autoref{fig:prefillvsdecode}, increasing the batch size enhances GPU efficiency\xzyao{, particularly in the decode phase}. In short, batching in the decode phase improves performance, even under stringent TPOT constraints.

% \subsection{Distributed LLM Deployment}
\label{subsec:distributedllmdeployment}
% Given the large scale of Large Language Models (LLMs), it is necessary to deploy it in a distributed manner. 


\noindent \textbf{Parallelism strategies.}
To parallelize the model over multiple GPUs, there are two prevalent forms of model parallelism, which are tensor model parallelism and pipeline model parallelism.
% , as depicted in \autoref{fig:parallel}.
\textit{Tensor model parallelism (TP)} \cite{shoeybi2019megatron,nagrecha2021model} divides model weights and computationally intensive operations such as matrix multiplication across various GPUs, thereby splitting data scanning and computation to minimize LLM inference latency, particularly the TTFT in the prefill phase. 
% This approach necessitates two all-reduce communication operations in one Transformer block to synchronize intermediate activations, demanding sufficiently high communication bandwidth among GPUs for a better performance. Thus, tensor model parallelism is typically constrained to avoid inter-node communication to prevent performance degradation, considering that intra-node communication is usually with higher bandwidth.
\textit{Pipeline model parallelism (PP)} \cite{huang2019gpipe,narayanan2019pipedream} divides the layers of a model into multiple stages. These stages are assigned to distinct GPUs for execution and they establish a pipeline. Only inter-layer activations are needed to be communicated between stages. 
% Unlike tensor model parallelism, pipeline model parallelism cannot reduce the inference latency, but it is capable of accommodating a broader range of network bandwidths (e.g., inter-node communication bandwidth) due to its much lower communication volume.
% \textit{Hybrid model parallelism} \cite{li2023alpaserve,zhong2024distserve,miao2023spotserve,jiang2024hexgen} is essential for deploying LLMs due to the significant impact of heterogeneous bandwidth on the effectiveness of parallelism strategies. For instance, in a scenario with two nodes (machines) and each equipped with several GPUs, the internal bandwidth within each node far exceeds the inter-node bandwidth. Using tensor model parallelism alone leads to substantial communication overhead due to limited inter-node bandwidth, while pipeline model parallelism incurs longer inference time since only one stage can be active for computation. Hybrid model parallelism overcomes these issues by employing tensor model parallelism within nodes to reduce latency and using pipeline model parallelism between nodes to lower communication overhead. This offers considerable performance improvements over using single parallelism strategies.


% 
% \noindent \textbf{Model replication.} Given a set of GPUs, in addition to model parallelism, we can choose to replicate a model into multiple model replicas. These replicas facilitate the concurrent processing of incoming requests, which linearly enhances the system’s throughput capacity. 


% \begin{figure}[!t] 
%   \centering
%   \includegraphics[width=0.8\linewidth]{img/combine2.pdf} % Adjust the width as needed
%   \caption{Illustration of different parallelism methods (top row) and phase splitting deployment (bottom row).}
%   \label{fig:parallel}
%   % \vspace{-1em}
% \end{figure}



\noindent \textbf{Phase splitting deployment.}
As the prefill and decode phases differ in \xzyao{workload characteristics (i.e., compute/memory-bound) significantly}, recent efforts propose to utilize different hardware resources for the two phases in order to avoid performance interference \cite{patel2023splitwise,zhong2024distserve,hu2024inference,jin2024p,qin2024mooncake}. In other words, there are two kinds of model replicas, one for prefill and the other for decode, respectively, and it is necessary to transmit the KV cache from the prefill replicas to the decode ones. 
% The illustration of this process is demonstrated in \autoref{fig:parallel}.
Due to the substantial size of KV cache, existing efforts essentially require high communication bandwidth for the KV cache transfer ---
\citet{zhong2024distserve} proposed to colocate prefill and decode replicas on GPUs within the same node, facilitating fast KV cache transfer with NVLink, while \citet{patel2023splitwise} and \citet{hu2024inference} utilized high-speed Infini-Band connections for inter-node communication. 
However, in cloud environments, GPUs are connected via limited bandwidth (typically, PCIe for intra-node connections and Ethernet for inter-node connections) rather than high-speed connections (typically, NVLink and Infini-Band). Therefore, KV cache transfer would lead to a huge cost and it necessitates enhancement.


\textbf{Heterogeneous GPU Computing.} Recent research has investigated diverse approaches to deploy large models on heterogeneous GPU clusters. 
% Metis \cite{um2024metis} develops a new search algorithm that automatically finds efficient parallelism plans for distributed training on heterogeneous GPUs. 
HexGen \cite{jiang2024hexgen} proposes asymmetric partitioning and advanced scheduling to deploy generative inference in a decentralized and heterogeneous setting. 
Helix \cite{mei2024helix} formulates the heterogeneous GPUs and network connections as a maxflow problem and adopts a mixed integer linear programming algorithm to discover highly optimized strategies to serve LLMs. 
Our work has a similar objective, but is the first effort that integrates phase splitting with the heterogeneous GPUs to provide high-performance cloud serving for LLMs.



% \subsection{Parallelism Strategies}
% Given the large scale of Large Language Models (LLMs), parallelizing the model over multiple GPUs becomes indispensable. The two most prevalent forms of model parallelism are tensor model parallelism and pipeline model parallelism.

% 
% \noindent \textbf{Tensor model parallelism.} Tensor model parallelism \cite{shoeybi2019megatron} divides model weights and computationally intensive operations such as matrix multiplication across various GPUs, thereby splitting data scanning and computation to minimize LLM inference latency, particularly the TTFT in the prefill phase. This approach necessitates two all-reduce communication operations in one Transformer block to synchronize intermediate activations, demanding sufficiently high communication bandwidth among GPUs for a better performance. Thus, tensor model parallelism is typically constrained to avoid inter-node communication to prevent performance degradation, considering that intra-node communication is usually with higher bandwidth.

% 
% \noindent \textbf{Pipeline model parallelism.} Pipeline model parallelism \cite{huang2019gpipe,narayanan2019pipedream} divides the layers of a model into multiple stages. These stages are assigned to distinct GPUs for execution and they establish a pipeline. Only inter-layer activations are needed to be communicated between stages. Unlike tensor model parallelism, pipeline model parallelism cannot reduce the inference latency, but it is capable of accommodating a broader range of network bandwidths (e.g., inter-node communication bandwidth) due to its much lower communication volume.

% 
% \noindent \textbf{Hybrid model parallelism.} Hybrid model parallelism \cite{li2023alpaserve,zhong2024distserve,miao2023spotserve,jiang2024hexgen} is essential for deploying LLMs due to the significant impact of heterogeneous bandwidth on the effectiveness of parallelism strategies. For instance, in a scenario with two nodes (machines) and each equipped with several GPUs, the internal bandwidth within each node far exceeds the inter-node bandwidth. Using tensor model parallelism alone leads to substantial communication overhead due to limited inter-node bandwidth, while pipeline model parallelism incurs longer inference time since only one stage can be active for computation. Hybrid model parallelism overcomes these issues by employing tensor model parallelism within nodes to reduce latency and using pipeline model parallelism between nodes to lower communication overhead. This offers considerable performance improvements over using single parallelism strategies.

% 
% \noindent \textbf{Model replication.} Given a set of GPUs, in addition to model parallelism, we can choose to replicate a model into multiple model replicas. These replicas facilitate the concurrent processing of incoming requests, which linearly enhances the system’s throughput capacity. The three parallelism methods are demonstrated in \autoref{fig:parallel}.

% \begin{figure}[!t] 
%   \centering
%   \includegraphics[width=0.9\linewidth]{img/combine.pdf} % Adjust the width as needed
%   \caption{Illustration of different parallelism methods (top row) and phase splitting deployment (bottom row).}
%   \label{fig:parallel}
% \end{figure}

% % \begin{figure}[!t] 
% %   \centering
% %   \includegraphics[width=0.9\linewidth]{img/kv cache communication.pdf} % Adjust the width as needed
% %   \caption{Illustration of phase splitting deployment.}
% %   \label{fig:kvtrans}
% % \end{figure}

% \subsection{Phase Splitting Deployment}
% \label{sec:pre_phase_splitting}
% As the prefill and decode phases differ in workloads significantly, recent efforts propose to utilize different hardware resources for the two phases in order to avoid performance interference \cite{patel2023splitwise,zhong2024distserve,hu2024inference}. In other words, there are two kinds of inference replicas, one for prefill and the other for decode, respectively, and it is necessary to transmit the KV cache from the prefill replicas to the decode ones. The illustration of this process is demonstrated in \autoref{fig:parallel}.
% Due to the substantial size of KV cache, existing efforts essentially require high communication bandwidth for the KV cache transfer ---
% \citet{zhong2024distserve} proposed to colocate prefill and decode replicas on GPUs within the same node, facilitating fast KV cache transfer with NVLink, while \citet{patel2023splitwise} and \citet{hu2024inference} utilized high-speed Infini-Band connections for inter-node communication. 
% However, in cloud environments, GPUs are connected via limited bandwidth (typically, PCIe for intra-node connections and Ethernet for inter-node connections) rather than high-speed connections (typically, NVLink and Infini-Band). Therefore, KV cache transfer would lead to a huge cost and it necessitates enhancement.



% \subsection{\jyhh{Challenges}}

% Due to the GPU shortage and cost-effectiveness issues, LLM serving on clouds is urgent and of great value. However, there are three key challenges as introduced in \S\ref{sec:1_intro}, which are \mytextcircled{1} heterogeneous and limited resource pool, \mytextcircled{2} heterogeneous and low network bandwidth, and \mytextcircled{3} \jyhh{workload variability}. We find that existing approaches fall short in addressing these challenges.

% Approaches for homogeneous environments, such as AlpaServe \cite{li2023alpaserve}, DistServe \cite{zhong2024distserve}, and Splitwise \cite{patel2023splitwise}, consider the deployment from view of a single model replica (i.e., how to deploy a model replica over multiple GPUs) and scale out by adding replicas with more homogeneous GPUs. 
% However, due to the heterogeneous and limited resource pool, we must consider the deployment from a global view on clouds --- given the available resources of diverse types, we should jointly consider which GPUs should be grouped together to serve one model replica, and whether this replica should serve as the prefill or decode phase.
% Besides, the heterogeneous and low network bandwidth also raises a hurdle for these approaches. 

% There have been several approaches developed for heterogeneous environments \cite{jiang2024hexgen,mei2024helix}. However, although these approaches take the heterogeneity in hardware capability (e.g., computing ability and network bandwidth) into account, they lack considerations for the heterogeneity in workloads between the prefill and decode phases. 
% Specifically, they only support colocating the prefill and decode phases on the same device, which leads to considerable interference and thereby compromises the optimal performance of model serving, as pointed out by many existing works \cite{patel2023splitwise,zhong2024distserve}.
% Our testbed in \autoref{fig:a40} indicates that devices within the cluster have different capacities for handling different inference phases, calling for distinct designs in resource selection.
% Consequently, as we will show in \S\ref{sec:eva}, disaggregating the two phases onto different devices improve the overall system efficiency.

% \jyhh{
% However, implementing phase splitting in cloud environments faces specific challenges related to KV cache state transitions --- the heterogeneous nature of network conditions would exacerbate the communication overhead associated with KV caches, thereby diminishing the performance improvements afforded by phase splitting. How to reduce the communication volume of KV cache communication is important for LLM serving on clouds.
% }

% Last but not least, available resources and serving workloads change over time in online services, and these instabilities can be augmented in cloud environments. To address resource instability and variability in LLM serving workloads, prior works \cite{jiang2024hexgen,zhong2024distserve} propose online re-planning to adjust the model deployment, which involves two steps: re-running the search algorithm and re-loading the LLM parameters. However, these steps are time-consuming as discussed in \S\ref{sec:1_intro}, leading to significant performance degradation in online services.

% \ffc{
% To tackle these challenges, this work proposes to integrate phase splitting deployment with LLM serving on clouds. In particular, we develop a novel scheduling algorithm to deduce the optimal model deployment by taking the heterogeneity in both hardware capabilities and inference workloads into account. In addition, we devise a lightweight re-scheduling approach to support fast and robust adjustment, without halting the online services.
% }

