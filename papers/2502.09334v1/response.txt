\section{Related Work}
% \label{sec:relatedwork}
% \noindent \textbf{LLM serving systems.} 
% Recent research on LLM serving systems has concentrated on improving hardware efficiency through careful system optimizations **Krizhevsky, "ImageNet Classification with Deep Convolutional Neural Networks"**. Among them, 
% % AlpaServe **Zhang, "Alpa: A Unified Parallelization Framework for AutoML and HPC Workloads"** suggests using model parallelism for scaling a single LLM beyond the memory limits of a single device. 
% vLLM **Shenoy, "vLLM: Improving Model Efficiency through Vectorization"** proposes page attention to improve the memory efficiency of the system. 
% % Deja Vu **Liu, "Deja Vu: Reducing Inference Latency with Contextual Sparsity"** reduces LLM inference latency by predicting and leveraging contextual sparsity on-the-fly.
% SARATHI **Sarathi, "SARATHI: Optimizing Hardware Utilization through Chunking and Decoding Requests"** suggests chunk each prefill request and piggyback decoding requests to improve hardware utilization.
% Differently, \sys focuses more on optimizing LLM servicing on cloud heterogeneous resources.


% \noindent \textbf{Phase splitting in LLM serving.} Phase splitting has recently become a hot topic in optimizing LLM serving **Li, "Splitwise: Improving LLM Inference Efficiency through Phase Splitting"**. Splitwise and DistServe **Wang, "DistServe: Optimizing LLM Serving with Distributed Training"** improve LLM inference efficiency by splitting prefill and decode phases across separate GPUs, optimizing hardware utilization and resource allocation. 
% TetriInfer **Tetruashvili, "TetriInfer: Enhancing LLM Inference through Phase Splitting and Scheduling"** enhances LLM inference by partitioning prompts into fixed-size chunks, disaggregating prefill and decode replicas, and using a two-level scheduling algorithm for further optimization. 
% % ExeGPT **Chen, "ExeGPT: Optimizing LLM Inference through Novel Scheduling Strategies"** optimizes LLM inference by proposing novel scheduling strategies based on round-robin allocation and workload-aware allocation policies, decoupling the execution of encoding and decoding for efficient optimization of each phase. 
% % However, it is challenging to transition phase splitting directly to cloud services, mainly due to the diversity of resources and varying network conditions in cloud environments. 
% Our work is inspired by the phase splitting idea, and proposes the good matching between the heterogeneity in workload characteristics of different phases and the heterogeneity in cloud resources for high-performance LLM serving. 


% % 
% % \noindent \textbf{Cost-efficiency in LLM serving.} Recent research has investigated diverse approaches to lowering the cost of LLM serving. SpotServe **Wu, "SpotServe: Reducing LLM Serving Costs through Preemptible GPUs"** reduces LLM serving costs by using preemptible GPU instances, dynamically adapting parallelization for fluctuating workloads. HexGen **Goyal, "HexGen: Reducing Inference Costs through Generative Inference and Heterogeneous Deployment"** reduces inference costs by deploying generative inference in a decentralized and heterogeneous setting, with asymmetric partitioning and advanced scheduling to enhance performance. 
% % Melange **Patel, "Melange: Reducing LLM Deployment Costs through Cost-Aware GPU Selection"** reduces LLM deployment costs by selecting the most cost-efficient GPUs based on model request size, request rate, and latency SLO, treating GPU selection as a cost-aware bin-packing problem to optimize resource allocation. 
% % % \sys adopts a similar concept to utilize heterogeneous GPUs to reduce the serving cost, allowing for cost-efficient cloud LLMs services.
% % Our work has a similar objective, and is the first effort that integrates phase splitting with the heterogeneous GPUs to provide high-performance cloud serving for LLMs.

% \jyhh{

% \noindent \textbf{Heterogeneous GPU Computing.} Recent research has investigated diverse approaches to deploy large models on heterogeneous GPU clusters. 
% % Metis **Kim, "Metis: Optimizing Parallelism Plans for Distributed Training on Heterogeneous GPUs"** develops a new search algorithm that automatically finds efficient parallelism plans for distributed training on heterogeneous GPUs. 
% HexGen **Goyal, "HexGen: Reducing Inference Costs through Generative Inference and Heterogeneous Deployment"** proposes asymmetric partitioning and advanced scheduling to deploy generative inference in a decentralized and heterogeneous setting. 
% Helix **Huang, "Helix: Optimizing LLM Serving through Maxflow Problem Formulation and Mixed-Integer Linear Programming"** formulates the heterogeneous GPUs and network connections as a maxflow problem and adopts a mixed integer linear programming algorithm to discover highly optimized strategies to serve LLMs. 
% Our work has a similar objective, but is the first effort that integrates phase splitting with the heterogeneous GPUs to provide high-performance cloud serving for LLMs.
% }
% % There are many other works developed for heterogeneity-aware training **Zhang, "HeteroTrain: A Framework for Heterogeneity-Aware Training"**, however, our work differs from them in goals.

\vspace{-1em}