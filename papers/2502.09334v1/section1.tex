\section{Introduction}
\label{sec:1_intro}

Large Language Models (LLMs) such as GPT \cite{achiam2023gpt}, LLaMA \cite{touvron2023llama}, OPT \cite{zhang2022opt} and Falcon \cite{falcon180b} have demonstrated strong performance across a wide range of advanced applications. However, serving LLMs is cost-demanding, requiring a large amount of hardware accelerators like GPUs to satisfy efficiency requirements such as latency and throughput.
% to ensure low-latency inference outcomes. 


% Current state-of-the-art efforts \cite{kwon2023efficient,tensorrt_llm}
Mainstream LLM serving systems primarily focus on high-performance GPUs like NVIDIA A100 and H100 in homogeneous GPU clusters. 
% , which often results in substantial fees. 
However, it is difficult for many LLM service providers to get access to sufficient high-performance GPUs, either due to the well known GPU shortage problem \cite{skypilot,euromlsysworkshop} or the substantial fees. 
Meanwhile, with more and more advanced GPU architectures announced in the past few years, there are many less-performant GPUs in former generations remaining under-utilized.
% And Nvidia typically releases new GPU generations every 24 months, e.g., Turing in 2018~\cite{Nvida_turing}, Ampere~\cite{Nvida_ampere} in 2020, Hopper~\cite{Nvida_hopper} in 2022, and Blackwell~\cite{Nvida_blackwell} scheduled for Q4 2024; but one particular version of GPU general remains in use for a much longer period. For example, Tesla K80 GPUs~\cite{Nvidia_tesla}, released in 2006, are still available on AWS as p2 instances~\cite{Amazon}.
Thus, real-world cloud environments usually consist of heterogeneous GPUs and diverse prices. As shown in Table~\ref{tab:gpu}, cloud environments offer a wide range of hardware specifications and rental prices, providing users with diverse options to reduce the costs associated with LLM deployment and serving. Recent efforts \cite{jiang2024hexgen,mei2024helix,griggs2024melange,miao2023spotserve} have demonstrated that serving LLM with heterogeneous GPUs presents opportunities in reducing the serving cost.
% 
% Given diverse types of GPUs, these efforts generally prefer to construct serving pipelines with the same type of GPUs, and these pipelines do not interact with each other. At first glance, by such means we can get rid of the interference among different types of GPUs. 
% Given diverse types of GPUs, these efforts generally construct multiple serving pipelines, and these pipelines do not interact with each other. 
However, we find that these heterogeneous serving systems mainly address the heterogeneity in \textit{hardwares} but fail to take account of the heterogeneity in \textit{the computation and memory-access workloads} of different inference phases\xzyao{, which hinders the utilization of GPU resources}. 
Such heterogeneity mainly comes from the distinct characteristics of LLM inference, and has raised a surge of research interests.
% it misses the chance to integrate the heterogeneous capabilities of GPUs with the heterogeneous workloads in LLM serving.
% To accommodate the heterogeneous capabilities among GPUs in cloud environments
% try to construct serving pipelines with diverse types of GPUs 
% deduce the optimal parallel configuration for each type of GPUs to construct several serving pipelines, and these pipelines do not interact with each other.
% Typically, given an input query (a.k.a. prompt), there are two phases to generate the response, which are the prefill and decoding phases. In the prefill phase, the LLM processes the prompt and generates the first token of the responses in a single step, which is usually computationally intensive. While in the decoding phase, the LLM takes multiple steps to generate subsequent tokens in an auto-regressive (i.e., token-by-token) manner, which is usually bounded by the memory bandwidth. Considering the distinct workload characteristics of the two phases,

\begin{table}[!t]
\centering
\caption{\small{GPU specifications and pricing}}
\small
\resizebox{\linewidth}{!}{
\begin{tabular}{l|c|c|c|c}
\hline
\specialcell{\textbf{GPU}\\\textbf{Type}} & \specialcell{\textbf{Memory Access}\\\textbf{Bandwidth}} & \specialcell{\textbf{Peak}\\\textbf{FP16 FLOPS}} & \specialcell{\textbf{Memory}\\\textbf{Limite}} & \specialcell{\textbf{Price}\\\textbf{(per GPU)}} \\
\hline
A100 & 2 TB/s & 312 TFLOPS & 80 GB & \$1.753/hr \\
A6000 & 768 GB/s & 38.7 TFLOPS & 48 GB & \$0.483/hr \\
A5000 & 626.8 GB/s & 27.8 TFLOPS & 24 GB & \$0.223/hr \\
A40 & 696 GB/s & 149.7 TFLOPS & 48 GB & \$0.403/hr \\
3090Ti & 1008 GB/s & 40 TFLOPS & 24 GB & \$0.307/hr \\
\hline
\end{tabular}
}
\label{tab:gpu}
\vspace{-1em}
\end{table}


Recent works~\cite{patel2023splitwise,hu2024inference,qin2024mooncake,jin2024p} have designed phase splitting approaches to utilize different amount of computational resources for prefill and decoding phase in LLM inference, which involves partitioning the two phases onto separate devices and transmitting the intermediate results (primarily KV caches) between them.
% , as depicted in \autoref{fig:kvcachetransfer}. 
Many empirical evidences have shown that such phase splitting approaches increase overall hardware utilization and system efficiency compared with the phase co-locating counterparts.

% \begin{figure}[!t]
%   \centering
%   \includegraphics[width=\linewidth]{img/fig_1.pdf} % Adjust the width as needed
%   \caption{Phase splitting process.}
%   \label{fig:kvcachetransfer}
% \end{figure}

% \begin{table}[h]
% \centering
% \begin{tabular}{c c c }
% \hline
% \textbf{GPU Type} & \textbf{Prefill Time} & \textbf{Decode Time} \\ \hline
% 3090Ti & 535 ms & 563 ms \\ \hline
% A40 & 349 ms & 1126 ms \\ \hline
% \end{tabular}
% \caption{Comparison of Prefill and Decode Times per request with input and output lengths of 512 and 16.}
% \label{tab:time_comparison}
% \end{table}


% \begin{figure*}[!t]
%     \centering
%     \begin{minipage}{0.6\linewidth}
%         \includegraphics[width=\linewidth]{img/transferpic.pdf}
%         \caption{Phase splitting process.}
%         \label{fig:kvcachetransfer}
%     \end{minipage}\hfill
%     \begin{minipage}{0.4\linewidth}
%         \includegraphics[width=\linewidth]{img/1ktokenprice.pdf}
%         \caption{Prefill and decode prices for a single request with input and output lengths of 512 and 16 on 3090Ti and A40.}
%         \label{fig:a40}
%     \end{minipage}
% \end{figure*}


% Moreover, we find that \textit{the heterogeneous capabilities among GPUs in cloud environments suit the phase splitting approaches well}. 
As the heterogeneity exists in both hardwares and \xzyao{workload characteristics (i.e., compute/memory-bound)}, we suggest that \textit{the phase splitting approach fits the heterogeneous capabilities among GPUs in cloud environments well}. 
In particular, since the two phases differ in the workload characteristics, it is an intuitive idea to leverage different types of GPUs for the two phases. For instance, as illustrated in \autoref{fig:a40}, the A40 GPU with 149.7 TFLOPS is more cost-effective for the compute-intensive prefill phase, whereas the 3090Ti with 1008 GB/s memory bandwidth is better suited for the memory-bounded decode phase.
Inspired by this, this work presents the first effort to integrate the phase splitting idea with the heterogeneity among GPUs, aiming to achieve high-performance and cost-effective LLM serving in cloud environments.
Nevertheless, the unique attributes of cloud environments pose three key challenges: 


% \ffc{To implement phase splitting LLM serving on clouds, a na\"ive solution is to determine which type of GPUs in the most cost-effective for each phase, and rent a sufficient amount of resources accordingly. Nevertheless, such a na\"ive solution is feasible due to the unique attributes of cloud environments. In particular, we summarize three key challenges as follows.}


% Inspired by this, this paper introduces \sys, an efficient LLM serving system for clouds. \sys is the first system that integrates the phase splitting idea with the heterogeneity among GPUs in cloud environments. 
% % To achieve high-performance and robust cloud services, it poses a series of challenges, and \sys manages to address them effectively. 
% Nevertheless, the unique attributes of cloud environments pose challenges to achieving high-performance and robust cloud services, as discussed below. 
% % \sys manages to address them effectively through a series of innovative designs. 
% % Below, we introduce each challenge and the key idea of \sys to solve them.

 
\noindent \textbf{{Challenge 1: heterogeneous and limited resource pool.}}
% To implement phase splitting LLM serving on clouds, a na\"ive solution is to determine which type of GPUs is the most cost-effective for each phase, and rent a sufficient amount of resources accordingly. Nevertheless, such a na\"ive solution assumes the amount of rentable resources for each type of GPUs is unlimited, which is impractical for clouds due to the GPU shortage problem \cite{skypilot,euromlsysworkshop}. In contrast, 
The available GPUs in cloud environments are usually in heterogeneous types, each with distinct specification (e.g., peak FLOPS, device memory bandwidth, and device memory limit), and the amount of each type is also restricted~\cite{skypilot,euromlsysworkshop}.
As a result, to deploy multiple copies (a.k.a. model replicas) of the same LLM, we must consider how to organize the available GPUs from a global view --- given the available resources of diverse types, we need to jointly consider which GPUs should be grouped together to serve one model replica, and whether this replica should serve as the prefill or decoding phase. To our knowledge, this is an unexplored problem so far.

% 1ktokenprice.pdf
\begin{figure}[!t]
  \centering
  \includegraphics[width=0.7\linewidth]{img/GPU_price.pdf} % Adjust the width as needed
  \vspace{-1em}
  \caption{\small{Prefill and decode prices for a single request with input and output lengths of 512 and 16 on 3090Ti and A40.}}
  \label{fig:a40}
  \vspace{-1em}
\end{figure}

% In essence, given the efficiency requirements (e.g., latency, throughput), prior works try to find out the most suitable way of deployment from the view of a single model replica. That is to say, determine how to deploy a model replica over multiple GPUs, and scale out by adding replicas with more homogeneous GPUs.
% % the best parallel configuration over a resource unit (e.g., an 8-GPU node in DistServe~\cite{zhong2024distserve}), and scale out by adding more replicas. 
% With the specifications and prices of diverse GPUs, an ideal solution is to determine which type of GPUs is the most cost-effective for each phase and rent a sufficient amount of resources accordingly.
% However, in cloud environments, the amount of rentable resources for each type of GPUs is restricted, making the na\"ive scaling infeasible.
% As a result, given the available resources of diverse types, it is necessary to deduce how to deploy the model replicas from a global view, along with whether each replica should serve as the prefill or decode phase, which has been under-explored so far. 
% \jyh{We name a set of GPUs with a model parallel configuration that responsible for prefill and decode phases as prefill and decode replicas.}
% As a result, given the available resources of diverse types, it is necessary to deduce the suitable GPUs and model placement for each phase, as well as how the \jyh{prefill and decode replicas} are orchestrated, which is an under-explored question so far.
% As a result, it necessitates solving a joint optimization problem of resource allocation and parallel configuration over diverse types of GPUs, as well as how the prefill and decoding instances are orchestrated, which is under-explored so far. 
% To be specific, we must deduce the suitable GPUs and model placement for each phase, along with the routing of requests among the prefill and decoding instances, which is a NP-hard problem (detailed in \S\ref{sec:scheduling_np_hard}).
% \red{(TODO: define prefill and decoding instances)}

% To address this challenge, we formulate this issue as a Job Shop Scheduling Problem (JSSP) \red{[X,X]}, which aims to partition the GPUs of diverse types into model serving groups, with each group holding a model replica, and designate the phase to each group. Subsequently, we propose to solve the problem via a brand new algorithm based on tabu search \red{[X,X]}. This effectively facilitates finding the most cost-efficient combination of model serving group construction and phase designation.

% To address this challenge, \sys aims to solve a joint optimization problem of resource allocation and parallel configuration over diverse types of GPUs, along with the routing of requests among the \jyh{prefill and decode replicas}. 
% We first prove that this problem is NP-hard, and devise a brand new scheduling algorithm based on Tabu search, which gradually \jyh{optimizes the system performance with respect to certain objective given the amount of rentable resources.}
% \red{XXX}.
% given the available resources, we deduce the suitable GPUs and model placement for each phase, along with the routing of requests among the prefill and decoding instances

 
\noindent \textbf{{Challenge 2: heterogeneous and low network bandwidth.}}
The second essential characteristic of cloud environments is that GPUs are usually connected through low network bandwidth, typically, PCIe for intra-node and ethernet for inter-node communication. And the network bandwidth also exhibits a high level of heterogeneity across different pairs of GPUs due to the discrepancy in connectivity (different PCIe versions, node locality, etc.). 
Such a network condition raises a hurdle for efficient LLM serving.
On the one hand, transmitting KV caches from prefill to decode replicas inevitably incurs significant communication volume. 
While prior works~\cite{patel2023splitwise,zhong2024distserve} simply assume high-speed network connections (e.g., NVLink and Infini-Band) are available and overlook the communication overhead of KV caches (detailed in \S\ref{subsec:distributedllmdeployment}), which is impractical for clouds.
On the other hand, due to the astonishing size of LLMs, model parallelism has been a cornerstone for LLM deployment. Thus, there expresses a need for designing heterogeneity-aware parallelization to facilitate phase splitting LLM serving on clouds. 
% \red{(seems weak)}

% Transmitting KV caches from prefill to decoding instances inevitably incurs significant communication volume. 
% Take LLaMA-30B as an example. Around 1GB of communication is required for a single 1024-token request, and this grows proportionally w.r.t. the input length and arrival rate. 
% While prior works simply assume high-speed network connections (e.g., NVLink and Infini-Band) are available and overlook the communication overhead of KV caches (detailed in \S\ref{sec:pre_phase_splitting}).
% % Consequently, high-speed network connections such as NVLink and Infini-Band are indispensable for prior works \red{(detailed in \S\ref{sec:pre_phase_splitting})}. 
% Nevertheless, in cloud environments, devices are usually connected through limited bandwidth (e.g., PCIe). 
% Consequently, it is vital to take such communication overhead into account when solving the aforementioned problem, even complicating the issue.

% % \sys makes two major efforts in response to the challenge. 
% Given an arbitrary combination of group construction and phase designation, \sys determines the most cost-efficient deployment plan through two innovations.  
% % we determine the most cost-efficient deployment plan by deducing the optimal parallel configuration for each group and the orchestration of prefill and decode replicas. 
% First, \red{XXX}. 
% Second, we analyze and model the communication cost of KV cache transmission under diverse connections (i.e., between arbitrary pair of GPUs), and embed such cost into a well-formulated two-stage transportation problem (TSTP) \red{[X]}. By solving the problem, we achieve the optimal orchestration of prefill and decode replicas, which guides how the requests should be dispatched among the two phases to maximize performance.

% \sys makes two efforts in response to this challenge. 
% First, we analyze and model the communication cost under diverse connections (i.e., between arbitrary pair of GPUs), and embed such cost into our scheduling algorithm.
% ensuring that 
% \red{XXX}.
% Second, we incorporate KV cache quantization that shrinks each element from 16-bit to 2-bit, reducing the communication volume by a large extent. To preserve the quality of generated contents, we dequantize the KV cache back to 16-bit. 
% By doing so, \sys facilitates the phase splitting LLM serving technique across cloud environments with diverse network bandwidth.

 
\noindent \textbf{{Challenge 3: workload variability.}}
Compared to in-house clusters, resources on clouds are more unstable \cite{miao2023spotserve,duan2024parcae,yousif2018cloud,erben2024can}, and the distribution of requests (e.g., average arrival rate, input and output length) may change over time in online services in practice \cite{wang2024burstgpt}.
These factors exacerbate the variability of serving workloads in cloud environments.
In order to adapt to such workload variations, prior works~\cite{zhong2024distserve} necessitate two steps: re-generating the deployment plan from scratch and re-loading the LLM parameters to adjust the model deployment.
% Although this seems feasible in in-house clusters, re-starts are very costly in cloud environments. 
However, both steps are costly.
Re-generating the deployment plan could take minutes to complete due to the complex hardware environments on cloud, and re-loading the LLM with a huge amount of parameters could be time-consuming. For instance, loading a 175B model with a disk bandwidth of 1.2 GBps takes over five minutes.
% The reason is that cloud environments usually have comparatively low disk bandwidth , making parameter re-loading extremely time-consuming. 
Such expensive steps would lead to severe interruption to the online services.

% To minimize the adaptation cost to dynamic workloads, we propose a simple-yet-effective lightweight re-scheduling process. The key idea is that prefill and decoding instances are interchangeable, and our empirical findings suggest that different workload patterns expect diverse prefill-to-decode ratios (i.e., the number of prefill instances divided by that of decoding instances). Therefore, our re-scheduling process only involves adjusting the prefill-to-decode ratio on-the-fly to adapt to workload variations, without re-starting the services nor reloading the LLM parameters, making it practical for clouds.

% \subsection{Our Solutions}

%  
% \noindent \textbf{{Our solutions.}}
 
% \noindent \textbf{(\textit{{Our Solutions})}}
To address these challenges, we develop \sys, an efficient and robust LLM serving system on clouds. 
% The core of \sys is a novel scheduling algorithm that generates the most efficient deployment plan for LLM serving in cloud environments. 
% Specifically, given the cloud GPUs of diverse types and the serving task, we formulate a two-level hierarchical optimization problem for the deduction of a deployment plan that maximizes the overall system performance.
\ffc{Our contributions are summarized as follows:}
% \vspace{-1em}

\textbf{Contribution 1:} We formulate the scheduling problem of LLM deployment and serving on cloud as a two-level hierarchical optimization problem, and develop a novel scheduling algorithm to optimize the deployment plan. 
In the upper-level, we develop a tabu search algorithm to partition the available GPUs of diverse types into model serving groups (with each group responsible for one model replica). In the lower-level, we determine the optimal parallel configuration for each group as well as the orchestration of prefill and decode replicas to optimize GPU and network usage.
% Concretely, in the upper-level, the tabu search algorithm partitions the available GPUs of diverse types into model serving groups (with each group holding a model replica); the lower-level determines the optimal parallel configuration for each group, and the orchestration of prefill and decode replicas.

% \item \jyhh{We integrate a KV cache compression technique into our system design to enable efficient KV cache communication in cloud environments.}

\textbf{Contribution 2:} We design a lightweight re-scheduling mechanism, which only involves adjusting the phase designation and orchestration in real-time, accelerates the re-generation of deployment plan by a large extent, and does not need to re-load the LLM parameters. It enables our system to adapt to workload shifts at minimal cost, thereby enhancing the robustness of LLM serving on cloud.


\textbf{Contribution 3:} Based on these techniques, we implement \sys, an efficient LLM serving system for clouds featuring phase splitting. \sys allows the two phases of LLM inference to be split onto separate GPUs with different resource allocations and parallel strategies. We further integrate a KV cache compression technique into our system, which performs a one-shot compression on the KV cache for efficient inter-phase communication on clouds while maintaining the model quality.

\textbf{Contribution 4:} \xzyao{The performance of \sys in the cloud environment is evaluated through comprehensive experiments. We compare its system and economic efficiency with state-of-the-art LLM serving systems, including HexGen in the same heterogeneous cloud environment, as well as DistServe and vLLM in a homogeneous in-house setting given the \textit{same} budget in terms of cloud service fees}. The empirical results demonstrate that \sys achieves \jyhh{up to 2.1$\times$ and on average 1.7$\times$} increase in throughput and \jyhh{up to 2.5$\times$ and on average 1.5$\times$} reduction in latency compared with existing systems, showcasing the potential of cost-effective LLM serving over clouds.

