\section{Evaluation}
\label{sec:eva}
% In this section, we evaluate \sys under different cluster settings and various workloads including coding and conversation workloads with different average request arrival rates on both system throughput and SLO attainment. The evaluation demonstrates that \sys consistently outperforms state-of-the-art systems across all settings. We also verify the effectiveness of our scheduling algorithm in finding the optimal deployment plan.

\subsection{Experimental Setup}
\label{sec:eva_setup}
\noindent \textbf{Hardware environments.}
We consider two types of hardware environments with almost the same price budgets.
% \jyhh{The homogeneous setting is established for baseline systems (vLLM, DistServe) that do not support heterogeneous GPUs.}
\begin{itemize}[topsep=0pt, leftmargin=*]
\item \textit{Heterogeneous GPUs on the cloud.} 
We rent GPUs from Vast.ai, a GPU cloud service provider. We rent four types of instances with 32 GPUs in total: two 4$\times$A6000 instances, \jyh{two 4$\times$A5000 instances}, one 8$\times$A40 instance and two 4$\times$3090Ti instances, with a total price of \$13.542/hour to represent the heterogeneous case.
% 
\item \textit{Homogeneous GPUs in a in-house server.} 
For baseline systems that do not support heterogeneous GPUs, we use one in-house server equipped with 8$\times$A100-80GB GPUs. 
According to \autoref{tab:gpu}, renting the same GPUs costs \$14.024/hour, which is close to the aforementioned price budget on the cloud. 
% We rent one server from Vast.ai with 8$\times$A100-80GB GPUs to simulate the in-house scenario. 
% We use one in-house server equipped with 8$\times$A100-80GB GPUs.
% The total price is measured as renting the same GPUs in \autoref{tab:gpu}, which is \$14.024/hour.
% We rent one AWS on-demand p4d.24xlarge instance, equipped with 8$\times$A100 GPUs with a total budget of \$14.021/hour to represent the standard homogeneous case in a data center.
\end{itemize}
% 
The GPU specifications are provided in \autoref{tab:gpu}, while the network bandwidth can be found in \autoref{appendix:comm_matrix}.


\noindent \textbf{Model and workloads.} We deploy the popular open-source LLaMA-30B model across two real-world workloads, coding and conversation, from the Azure Conversation dataset~\cite{patel2023splitwise}.
And we follow prior works \cite{li2023alpaserve,jiang2024hexgen} to generate the inference workload using a Poisson process determined by the request rate, with consecutive requests (inter-arrival times) following an exponential distribution.


\noindent \textbf{Evaluation metrics.} Following prior works \cite{patel2023splitwise,zhong2024distserve}, we focus on overall system SLO attainment and throughput when evaluating the performance. System SLO attainment indicates the percentage of requests completed within a predefined latency deadline. There are three types of SLO: TTFT, TPOT, and E2E SLO. We specifically measure system SLO attainment by the percentage of requests that meet the time criteria established by each SLO type. We scale the SLO to various multiples of the execution latency of A100 GPUs (SLO Scale in \autoref{fig:e2e}), which allows us to evaluate system performance under different levels of operational stringency. For a target \jyhh{SLO attainment} goal (e.g., 90\% and 99\%), we focus on the minimum latency deadline required to achieve the desired attainment.

% 
% \jyh{\noindent \textbf{Parameter setup for tabu search.} We implemented the tabu search algorithm with specific parameters to efficiently navigate and optimize the solution space. The algorithm performs 100 iterations and manages a tabu list of size 5 to prevent revisiting solutions. At each iteration, 10 neighbors are generated per current solution. To accommodate increasing problem complexity or cluster size, we adjust the number of iterations and tabu list size adaptively, improving exploration and preventing premature convergence.}


\noindent \textbf{Baselines.} 
We consider state-of-the-art systems under both the cloud and in-house settings, respectively. Under the cloud setting, we consider HexGen \cite{jiang2024hexgen}, an LLM serving system for clouds featuring advanced scheduling and asymmetric parallelism. For the in-house scenario, we consider \ffc{vLLM \cite{kwon2023efficient}, a prestigious LLM serving system, as well as} DistServe \cite{zhong2024distserve}, an LLM serving system featuring phase splitting. 
% Since \ffc{vLLM and} DistServe are only designed for homogeneous GPUs, we run them in the in-house setting to assess the cost-efficiency of deploying LLM services on clouds.

% % We carefully select state-of-the-art baselines to assess \sys's performance improvements and trade-offs compared to existing methods. We compare \sys with HexGen, an LLM serving system featuring advanced scheduling and asymmetric parallelism \cite{jiang2024hexgen}, to evaluate end-to-end performance in a cloud setting. 
% We also compare \sys with DistServe \cite{zhong2024distserve}, a distributed LLM serving system featuring phase splitting and a placement algorithm, to evaluate cost efficiency in cloud deployment. As DistServe is currently closed-sourced, we integrated its placement algorithm and parallelism strategies into our system to test performance in a data center cluster setting.
% % To understand the performance of the proposed scheduling algorithm, we compare its convergence with an popular policy based on genetic algorithm \cite{mirjalili2019genetic}.

% \begin{figure}[!t] 
%   \centering
%   \includegraphics[width=\linewidth]{img/one_big_pic_.pdf} % Adjust the width as needed
%   \caption{TTFT, TPOT, and E2E SLO attainments on coding (top row) and conversation (bottom row) workloads. The horizontal dashed line indicates the target SLO goal of 90\%, and the vertical dashed lines indicate the minimum latency deadline required to achieved the SLO goal.}
%   \label{fig:e2e}
% \end{figure}

% \begin{figure}[!t]
%   \centering
%   \includegraphics[width=\linewidth]{img/cloudtoinhouse_1.pdf} % Adjust the width as needed
%   \caption{
%   % SLO attainment results to evaluate \sys on cloud compared with data center.
%   E2E SLO attainments on coding (top row) and conversation (bottom row) workloads. The horizontal dashed lines indicates the target SLO goals of 90\% and 99\%, and the vertical dashed lines indicate the minimum latency deadline required to achieved the SLO goal.}
%   \label{fig:cloudvsdatacenter}
% \end{figure}

% original one
% \begin{figure*}[!t]
%     \centering
%     \begin{minipage}{0.45\linewidth}
%         \includegraphics[width=\linewidth]{img/one_big_pic_.pdf}
%         \caption{SLO attainment results on coding (top row) and conversation (bottom row) workloads.}
%         \label{fig:e2e}
%     \end{minipage}\hspace{10pt}
%     \begin{minipage}{0.45\linewidth}
%         \includegraphics[width=\linewidth]{img/cloudtoinhouse_1.pdf}
%         \caption{SLO attainment results to on coding (top row) and conversation (bottom row) workloads.}
%         \label{fig:cloudvsdatacenter}
%     \end{minipage}
% \end{figure*}

\begin{figure*}[!t]
    \centering
    \begin{minipage}{0.48\linewidth}
        \includegraphics[width=\linewidth]{img/figure_ts_vs_hexgen.pdf}
          \vspace{-2em}
        \caption{\jyh{\small{SLO attainment results on coding (top row) and conversation (bottom row) workloads.}}}
        \label{fig:e2e}
          \vspace{-1em}
    \end{minipage}\hspace{10pt}
    \begin{minipage}{0.48\linewidth}
        \includegraphics[width=\linewidth]{img/with_vllm.pdf}
          \vspace{-2em}
        \caption{\jyh{\small{SLO attainment results on coding (top row) and conversation (bottom row) workloads.}}}
        \label{fig:cloudvsdatacenter}
          \vspace{-1em}
    \end{minipage}
\end{figure*}

% \begin{figure*}[!t]
%     \centering
%     \begin{minipage}{0.48\linewidth}
%         \includegraphics[width=\linewidth]{img/throughput_.pdf}
%         \caption{Throughput scaled by \sys.}
%         \label{fig:throughput}
%     \end{minipage}\hspace{40pt}
%     \begin{minipage}{0.4\linewidth}
%         \includegraphics[width=\linewidth]{{img/estimatedslo.pdf}}
%         \caption{Convergence curves of scheduling from scratch for different cluster sizes.}
%         \label{fig:alg_efficiency}
%     \end{minipage}
% \end{figure*}

\begin{figure}[!t]
  \centering
  \includegraphics[width=0.8\linewidth]{img/all_pic.pdf} % Adjust the width as needed
    \vspace{-1em}
  \caption{\small{Throughput scaled by \sys.}}
  \label{fig:throughput}
    \vspace{-1em}
\end{figure}

\subsection{End-to-end Evaluation}
In this section, we compare the end-to-end performance of \sys against baselines on various workloads.



\noindent \textbf{System SLO attainment comparisons on the cloud.} 
We first evaluate the performance of \sys on the cloud. As shown in \autoref{fig:e2e}, typically higher average request arrival rates require higher SLO scales (i.e., longer latency deadlines) to meet the \jyhh{SLO attainment goal}. \sys consistently outperforms HexGen in terms of all TTFT, TPOT, and E2E SLO attainments.
% lead to higher SLO scale to  the latency requirements. The vertical dashed lines represent more than 90\% requests meet the latency requirements. We compare all TTFT, TPOT and E2E SLO attainments of the system, respectively.

On the coding workload, \sys achieves up to 1.8$\times$ and on average 1.4$\times$ lower E2E latency deadlines compared with existing approaches. Specifically, \ffc{as we will elaborate in \S\ref{sec:casestudyofscheduling}}, the coding workload makes our scheduling algorithm designate more prefill replicas than decode replicas, since the bottleneck is on prefilling given the relatively long input prompts. And the prefill-to-decode ratio decreases with the surge of the average request arrival rate, which matches our previous discussion in \S\ref{sec:method_light_reschedule}. On the conversation workload, \sys achieves up to 1.4$\times$ and on average 1.3$\times$ lower E2E latency deadlines. The conversation workload makes our scheduling algorithm deploy more decode replicas than prefill replicas, since the bottleneck is on decoding given the relatively long output responses. The phase splitting technique significantly reduces prefill-decode interference during inference, leading to improved TTFT and TPOT SLO attainments in all cases.


\noindent \textbf{Cost-efficiency of deploying LLM services on the cloud.} To assess the cost-efficiency of deploying LLM services on the cloud, we compare \sys in the cloud setting with DistServe and \jyh{vLLM} in the in-house setting, given the same price budget. As shown in \autoref{fig:cloudvsdatacenter}, \sys significantly outperforms DistServe and \jyh{vLLM}, achieving up to 2.5$\times$ and on average 1.8$\times$ lower E2E latency deadlines. This advantage stems from \sys's ability to deploy 3$\times$ more model replicas on the cloud than DistServe on the in-house server within the same price budget, which provides \sys with superior parallel processing capability. And the scheduling algorithm of \sys takes full advantage of the heterogeneity of cloud GPUs and improves system performance by allocating appropriate GPUs for prefilling and decoding, respectively. Thus, \sys greatly improves the cost-efficiency of deploying LLM services on the cloud.

% \begin{figure}[!t]
%   \centering
%   \includegraphics[width=0.7\linewidth]{img/estimatedslo.pdf} % Adjust the width as needed
%   \caption{Convergence curves of scheduling from scratch for different cluster sizes.}
%   \label{fig:alg_efficiency}
% \end{figure}


\noindent \textbf{System throughput comparisons.} 
% Throughput is also a critical metric in LLM serving systems, which measures how many requests the system can handle within a specified period of time. 
We further compare the system throughput between \sys and the baselines. 
As demonstrated in \autoref{fig:throughput}, compared with HexGen, \sys achieves 1.5 and 1.3$\times$ higher throughputs in coding and conversation workloads respectively. And compared with DistServe in the in-house setting, 1.5 and 2.1$\times$ higher throughputs are realized. These results demonstrate \sys's ability to effectively manage larger loads.

% \subsection{Evaluation over Dynamic GPU Resources}
% To evaluate the effectiveness of our scheduling algorithm over dynamic GPU resources, we consider a scenario where 4 GPUs become unavailable. Specifically, we remove two decode replicas and let \sys re-schedule on the fly. As demonstrated in \autoref{fig:offline}, the performance gap is relatively small under such dynamics, validating the ability and effectiveness of \sys to handle the dynamicity in cloud environments.
% Furthermore, since tabu search is done locally and we only adjust the phase designation and orchestration to adapt to different cluster size, our lightweight re-schuedling process takes merely seconds to complete, without any overhead on parameter reloading. 

\begin{figure*}[!t]
    \centering
    \begin{minipage}{0.25\linewidth}
        \includegraphics[width=\linewidth]{img/algorithm_pic.pdf}
          \vspace{-2em}
        \caption{\small{Convergence curves of scheduling from scratch for different cluster sizes.}}
          \vspace{-1em}
        \label{fig:schedulingoverhead}
    \end{minipage} \hspace{2pt}
    \begin{minipage}{0.35\linewidth}
        \includegraphics[width=\linewidth]{img/rescheduling_pic.pdf}
          \vspace{-2em}
        \caption{\small{SLO attainments of before (indicated by the solid line) and after (indicated by the dotted lines) 4 out of 32 GPUs offline.}}
          \vspace{-1em}
        \label{fig:rescheduling}
    \end{minipage} \hspace{2pt}
    \begin{minipage}{0.35\linewidth}
        \includegraphics[width=\linewidth]{img/compress_pic.pdf}
          \vspace{-2em}
        \caption{\small{Impact of KV cache compression and prefill and decode orchestration on the SLO attainments.}}
          \vspace{-1em}
        \label{fig:compress}
    \end{minipage}
\end{figure*}



\begin{table}[!t]
\centering
\small
\caption{\small{Model deployment discovered by \sys.}}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|l|l|l}
\hline
\textbf{Workload} & \textbf{GPU Configuration} & \textbf{Strategy} & \textbf{Type of Replicas} \\
\hline
% Coding 
\multirow{7}*{\rotatebox[origin=c]{90}{{Coding}}}
 & 8$\times$A40 & TP=2, PP=1 & 4 Prefill Replicas \\
 & 4$\times$A5000 & TP=4, PP=1 & 1 Prefill Replica \\
 & 4$\times$A6000 & TP=2, PP=1 & 2 Prefill Replicas \\
 & 2$\times$A5000+2$\times$3090Ti & TP=2, PP=2 & 1 Prefill Replica \\
 & 4$\times$3090Ti & TP=2, PP=2 & 1 Decode Replica \\
 & 4$\times$A6000 & TP=1, PP=2 & 2 Decode Replicas \\
 & 2$\times$A5000+2$\times$3090Ti & TP=2, PP=2 & 1 Decode Replica \\
\hline
% Conversation 
\multirow{7}*{\rotatebox[origin=c]{90}{{Conversation}}}
 & 6$\times$A40 & TP=2, PP=1 & 3 Prefill Replicas \\
 & 2$\times$A5000+2$\times$3090Ti & TP=2, PP=2 & 1 Prefill Replica \\
 & 4$\times$3090Ti & TP=2, PP=2 & 1 Decode Replica \\
 & 2$\times$A40 & TP=1, PP=2 & 1 Decode Replica \\
 & 4$\times$A5000 & TP=2, PP=2 & 1 Decode Replica \\
 & 8$\times$A6000 & TP=1, PP=2 & 4 Decode Replicas \\
 & 2$\times$A5000+2$\times$3090Ti & TP=2, PP=2 & 1 Decode Replica \\
\hline
\end{tabular}
}
\label{table:gpu_config_strategy}
  \vspace{-2em}
\end{table}


\subsection{Case Study of Scheduling}
\label{sec:casestudyofscheduling}
% We list the deployment plan generated by \sys from coding workload to conversation workload in the heterogeneous setting. We use the following representation to describe the scheduled results. We use an array to specify one independent model replica, with two numbers representing the degrees of tensor model parallelism and pipeline model parallelism. For example, (2,2) indicates a model replica with tensor model parallel degree of 2 and pipeline model parallel degree of 2 (2 pipeline stages). The scheduling results are demonstrated in \autoref{table:gpu_config_strategy}.
\ffc{Table~\ref{table:gpu_config_strategy} presents the model deployment discovered by our scheduling algorithm. It can be seen that \sys prioritizes the GPUs with better computing ability for prefilling (e.g., A40) and those with higher memory access bandwidth GPUs for decoding (e.g., 3090Ti). 
Additionally, \sys automatically assigns more decode replicas to the conversation workload due to its longer output lengths, while more prefill replicas to the coding workload due to the longer prompt lengths. 
These results verify that \sys is able to take the heterogeneity in both hardwares and serving workloads into account, generating model deployment that maximizes the system performance.}

\ffc{Compared to the in-house setting with an 8$\times$A100 instance, where only 4 model replicas can be served, \sys serves a maximum of 12 model replicas in the cloud setting within the same price budget. Although individual inference processes in the cloud setting may experience increased latency due to the lower hardware performance (i.e., the cloud GPUs are less performant than A100), the overall performance is improved due to the higher number of model replicas. Thus, \sys conveys the ability of high-performance and cost-efficient LLM serving on clouds.}

% 
% \noindent \textbf{Insights.} 
% \jyh{In the in-house setting, the 8$\times$A100 instance can only serve 4 model replicas, while in the cloud setting, the 32 cloud GPUs with various types can serve a maximum of 12 model replicas with various parallel configuration within the same price budget. In this case, although individual inference tasks in the cloud setting may experience increased latency due to the lower hardware performance (e.g., GPU flops and bandwidth), the overall system performance is improved due to the higher number of model replicas. Additionally, our scheduling algorithm prioritizes GPUs with high peak fp16 flops for prefilling (e.g., A40) and high memory bandwidth GPUs for decoding (e.g., 3090Ti), and selects the most suitable model parallel configuration for each phase to optimize the overall system performance.}


\subsection{Effectiveness and Ablation Studies}
\label{sec:effectiveness_and_ablation}
\noindent \textbf{Time cost of Scheduling.}
We evaluate the running time of our scheduling algorithm from scratch with cluster sizes of 16, 24 and 32 GPUs.
% on a single AWS on-demand p4d.24xlarge instance, equipped with 64 cores. We benchmarked the scheduling performance across cluster sizes of 16, 32, and 64 for comparative analysis. 
Leveraging our effectively designed neighborhood construction method, the algorithm based on tabu search scales well with the number of GPUs, requiring approximately 21, 36 and 54 seconds to converge, as shown in \autoref{fig:schedulingoverhead}. This search process is executed once before the initial deployment of the system, rendering its time cost negligible given the hourly scale of online services. 
% Additionally, the memory structure of the tabu search helps prevent the algorithm from revisiting recently explored solutions during reruns, enabling these reruns to complete within seconds.

% \begin{figure}[!t]
%   \centering
%   \includegraphics[width=\linewidth]{img/offline.pdf} % Adjust the width as needed
%   \caption{E2E SLO attainments of \sys before and after 4 out of 32 GPUs offline.}
%   \label{fig:offline}
% \end{figure}

% \begin{figure}[!t]
%   \centering
%   \includegraphics[width=\linewidth]{img/rescheduling___.pdf} % Adjust the width as needed
%   \caption{E2E SLO attainments of different re-scheduling approaches after 4 out of 32 GPUs offline.}
%   \label{fig:rescheduling}
% \end{figure}

% \begin{figure}[!t]
%   \centering
%   \includegraphics[width=\linewidth]{img/compress.pdf} % Adjust the width as needed
%   \caption{Impact of KV cache compression and orchestration on the SLO attainments.}
%   \label{fig:compress}
% \end{figure}


\noindent \textbf{Lightweight re-scheduling.}
To evaluate the effectiveness of our lightweight re-scheduling, we consider a scenario where 4 out of 32 GPUs become unavailable. Specifically, we remove two decode replicas and let \sys re-schedule on the fly. 
We compare our lightweight re-scheduling with \mytextcircled{1} a full re-scheduling approach, which involves re-starting the services and reloading parameters, and \mytextcircled{2} a no re-scheduling approach, which does not make any changes to the deployment plan and keeps the services using the remaining GPUs. 
% 
As shown in \autoref{fig:rescheduling}, our lightweight re-scheduling achieves similar SLO attainment to the full re-scheduling approach and outperforms the no re-scheduling approach, showing the strengths of our lightweight re-scheduling. 
More importantly, our lightweight re-scheduling process finishes within seconds, without any overhead on parameter reloading, far exceeding the full re-scheduling approach. This is reasonable as tabu search is done locally and we only adjust the phase designation and orchestration to adapt to different cluster sizes. 
Therefore, \sys is able to handle the dynamicity in cloud environments well.


% As demonstrated in \autoref{fig:rescheduling}, the performance gap is relatively small under such dynamics, validating the ability and effectiveness of \sys to handle the dynamicity in cloud environments.
% Furthermore, since tabu search is done locally and we only adjust the phase designation and orchestration to adapt to different cluster sizes, our lightweight re-scheduling process finishes within seconds, without any overhead on parameter reloading. 

% We further compare our lightweight re-scheduling with \mytextcircled{1} a full re-scheduling approach, which involves re-starting the services and reloading parameters, and \mytextcircled{2} a no re-scheduling approach, which does not make any changes to the deployment plan and keeps the services using the remaining GPUs.
% \jyh{We benchmarked the overhead of Lightweight Rescheduling and Full Rescheduling in \autoref{tab:lr}.}
% As shown in \autoref{fig:rescheduling}, our lightweight re-scheduling achieves similar SLO attainment to the full re-scheduling approach and outperforms the no re-scheduling approach, showing the strengths of our lightweight re-scheduling again. 
% This also indicates that changing phase designation and orchestration without any additional changes to the model serving group construction or parallel configuration is already satisfactory for improving system performance.

% And we assess the effects of KV cache compression and \jyh{lightweight rescheduling} on system performance by selectively disabling these features.

\begin{table}[!t]
\centering
\small
\caption{\jyh{\small{Overhead of Full and Lightweight Rescheduling.}}}
\jyh{
\begin{tabular}{l c c c}
\hline
\textbf{} & \textbf{Rescheduling} & \textbf{Reloading} & \textbf{Overall}\\ \hline
Full & 54($\pm$5)s & 103($\pm$10)s & 157($\pm$13)s \\ \hline
Lightweight & 13($\pm$2)s & 0s & 13($\pm$2)s \\ \hline
\end{tabular}
}
\label{tab:lr}
  \vspace{-2em}
\end{table}


\noindent \textbf{KV cache compression and orchestration.} 
There are two major techniques in \sys to address the communication cost of KV cache transmission from prefill to decode replicas, which are the KV cache compression and orchestration method. We conduct experiments to assess their effects. 
% KV cache compression and orchestration significantly enhance overall system performance. 
As illustrated in \autoref{fig:compress}, \sys exhibits a degraded performance in both coding and conversation scenarios without KV cache compression, incurring approximately 1.3$\times$ the overhead per single request. This undermines the benefits of phase splitting. If we further disable the orchestration method described in \S\ref{sec:schedule_lower_level} and substitute it with a random dispatching, there is another 4$\times$ of performance degradation. 
% \jyh{We have also conducted more experiments to evaluate the heterogeneous network impact on the scheduling and orchestration results,
% % including network connections within and across data centers. 
% due to the space constraint, we defer the experimental results and analysis to \autoref{appendix:networkeffect}.} 
In summary, \sys chooses the parallel configurations and KV cache communication paths that optimize overall system performance given the high heterogeneity of communication bandwidth on the cloud.
% empirical results demonstrate the necessity of orchestrating pipeline-to-pipeline communications, given the variability of communication bandwidth in cloud environments.

% \subsection{Ablation Studies}
% \label{sec:ablation}
% We investigate three primary innovations in \sys: rescheduling, KV cache compression, and \jyh{lightweight rescheduling}. To evaluate the impact of our rescheduling method on system SLO attainment, we compared it with \mytextcircled{1} a full rescheduling approach, which involves changing model parallel configurations and reloading parameters, and \mytextcircled{2} a no rescheduling approach, which avoids rescheduling after changes in cluster size. And we assess the effects of KV cache compression and \jyh{lightweight rescheduling} on system performance by selectively disabling these features.

% 
% \noindent \textbf{Lightweight rescheduling.} We conducted the experiment using the same settings described in \autoref{sec:effectiveness_and_ablation}. As shown in \autoref{fig:rescheduling}, \sys achieves similar SLO attainment to the full rescheduling approach and surpasses the performance of the no rescheduling approach. This indicates that changing phase designation \jyh{and orchestration} without any additional changes to the model parallel configuration or reloading of model parameters is already satisfactory for improving system performance.
% % In coding service, from an initial setup of 8 prefill and 4 decode instances, performance significantly drops when 2 decode instances are offlined. \sys responds by converting one prefill instance to decode and rerouting the communication paths between prefill and decode instances. For the conversation workload, starting with 3 prefill and 9 decode instances, the performance decline is less severe when 2 decode instances are offlined. \sys responds by converting one prefill instance to decode to enhance system goodput. And reloading model parameters during rescheduling may not always be necessary. 

% 
% \noindent \textbf{KV cache compression and \jyh{orchestration}.} KV cache compression and \jyh{orchestration} significantly enhance overall system performance. As illustrated in \autoref{fig:compress}, \sys exhibits a degraded performance in both coding and conversation scenarios without KV cache compression, incurring approximately 1.3$\times$ the overhead per single request. This undermines the benefits of phase splitting. And we disabled the \jyh{orchestration method} described in \S\ref{sec:schedule_lower_level} and substituted it with a random arrange method. Experimental results demonstrate the necessity of orchestrating pipeline-to-pipeline communications, given the variability of communication bandwidth in cloud environments.



\noindent \textbf{More Experiment Results.} 
We have also conducted more experiments to demonstrate the strengths of \sys, including more case studies, the effect of KV cache compression on model quality, and the accuracy of our simulator. Due to the space constraint, we refer interested readers to our supplemental material for more details. 


