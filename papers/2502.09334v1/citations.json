[
  {
    "index": 0,
    "papers": [
      {
        "key": "zhao2024llmpq",
        "author": "Juntao Zhao and Borui Wan and Yanghua Peng and Haibin Lin and Chuan Wu",
        "title": "LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and Adaptive Quantization"
      },
      {
        "key": "li2023alpaserve",
        "author": "Zhuohan Li and Lianmin Zheng and Yinmin Zhong and Vincent Liu and Ying Sheng and Xin Jin and Yanping Huang and Zhifeng Chen and Hao Zhang and Joseph E. Gonzalez and Ion Stoica",
        "title": "AlpaServe: Statistical Multiplexing with Model Parallelism for Deep Learning Serving"
      },
      {
        "key": "wu2023fast",
        "author": "Bingyang Wu and Yinmin Zhong and Zili Zhang and Gang Huang and Xuanzhe Liu and Xin Jin",
        "title": "Fast Distributed Inference Serving for Large Language Models"
      },
      {
        "key": "kwon2023efficient",
        "author": "Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion",
        "title": "Efficient memory management for large language model serving with pagedattention"
      },
      {
        "key": "tensorrt_llm",
        "author": "Nvidia",
        "title": "TensorRT-LLM"
      },
      {
        "key": "liu2023deja",
        "author": "Zichang Liu and Jue Wang and Tri Dao and Tianyi Zhou and Binhang Yuan and Zhao Song and Anshumali Shrivastava and Ce Zhang and Yuandong Tian and Christopher Re and Beidi Chen",
        "title": "Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time"
      },
      {
        "key": "agrawal2023sarathi",
        "author": "Amey Agrawal and Ashish Panwar and Jayashree Mohan and Nipun Kwatra and Bhargav S. Gulavani and Ramachandran Ramjee",
        "title": "SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "li2023alpaserve",
        "author": "Zhuohan Li and Lianmin Zheng and Yinmin Zhong and Vincent Liu and Ying Sheng and Xin Jin and Yanping Huang and Zhifeng Chen and Hao Zhang and Joseph E. Gonzalez and Ion Stoica",
        "title": "AlpaServe: Statistical Multiplexing with Model Parallelism for Deep Learning Serving"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "kwon2023efficient",
        "author": "Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion",
        "title": "Efficient memory management for large language model serving with pagedattention"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "liu2023deja",
        "author": "Zichang Liu and Jue Wang and Tri Dao and Tianyi Zhou and Binhang Yuan and Zhao Song and Anshumali Shrivastava and Ce Zhang and Yuandong Tian and Christopher Re and Beidi Chen",
        "title": "Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "agrawal2023sarathi",
        "author": "Amey Agrawal and Ashish Panwar and Jayashree Mohan and Nipun Kwatra and Bhargav S. Gulavani and Ramachandran Ramjee",
        "title": "SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "qin2024mooncake",
        "author": "Qin, Ruoyu and Li, Zheming and He, Weiran and Zhang, Mingxing and Wu, Yongwei and Zheng, Weimin and Xu, Xinran",
        "title": "Mooncake: Kimi's KVCache-centric Architecture for LLM Serving"
      },
      {
        "key": "jin2024p",
        "author": "Jin, Yibo and Wang, Tao and Lin, Huimin and Song, Mingyang and Li, Peiyang and Ma, Yipeng and Shan, Yicheng and Yuan, Zhengfan and Li, Cailong and Sun, Yajing and others",
        "title": "P/D-Serve: Serving Disaggregated Large Language Model at Scale"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "patel2023splitwise",
        "author": "Pratyush Patel and Esha Choukse and Chaojie Zhang and \u00cd\u00f1igo Goiri and Aashaka Shah and Saeed Maleki and Ricardo Bianchini",
        "title": "Splitwise: Efficient generative LLM inference using phase splitting"
      },
      {
        "key": "zhong2024distserve",
        "author": "Yinmin Zhong and Shengyu Liu and Junda Chen and Jianbo Hu and Yibo Zhu and Xuanzhe Liu and Xin Jin and Hao Zhang",
        "title": "DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "hu2024inference",
        "author": "Cunchen Hu and Heyang Huang and Liangliang Xu and Xusheng Chen and Jiang Xu and Shuang Chen and Hao Feng and Chenxi Wang and Sa Wang and Yungang Bao and Ninghui Sun and Yizhou Shan",
        "title": "Inference without Interference: Disaggregate LLM Inference for Mixed Downstream Workloads"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "oh2024exegpt",
        "author": "Hyungjun Oh and Kihong Kim and Jaemin Kim and Sungkyun Kim and Junyeol Lee and Du-seong Chang and Jiwon Seo",
        "title": "ExeGPT: Constraint-Aware Resource Scheduling for LLM Inference"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "miao2023spotserve",
        "author": "Xupeng Miao and Chunan Shi and Jiangfei Duan and Xiaoli Xi and Dahua Lin and Bin Cui and Zhihao Jia",
        "title": "SpotServe: Serving Generative Large Language Models on Preemptible Instances"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "jiang2024hexgen",
        "author": "Youhe Jiang and Ran Yan and Xiaozhe Yao and Yang Zhou and Beidi Chen and Binhang Yuan",
        "title": "HexGen: Generative Inference of Large-Scale Foundation Model over Heterogeneous Decentralized Environment"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "griggs2024melange",
        "author": "Tyler Griggs and Xiaoxuan Liu and Jiaxiang Yu and Doyoung Kim and Wei-Lin Chiang and Alvin Cheung and Ion Stoica",
        "title": "M\\'elange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "um2024metis",
        "author": "Um, Taegeon and Oh, Byungsoo and Kang, Minyoung and Lee, Woo-Yeon and Kim, Goeun and Kim, Dongseob and Kim, Youngtaek and Muzzammil, Mohd and Jeon, Myeongjae",
        "title": "Metis: Fast Automatic Distributed Training on Heterogeneous $\\{$GPUs$\\}$"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "jiang2024hexgen",
        "author": "Youhe Jiang and Ran Yan and Xiaozhe Yao and Yang Zhou and Beidi Chen and Binhang Yuan",
        "title": "HexGen: Generative Inference of Large-Scale Foundation Model over Heterogeneous Decentralized Environment"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "mei2024helix",
        "author": "Mei, Yixuan and Zhuang, Yonghao and Miao, Xupeng and Yang, Juncheng and Jia, Zhihao and Vinayak, Rashmi",
        "title": "Helix: Distributed Serving of Large Language Models via Max-Flow on Heterogeneous GPUs"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "miao2023sdpipe",
        "author": "Miao, Xupeng and Shi, Yining and Yang, Zhi and Cui, Bin and Jia, Zhihao",
        "title": "Sdpipe: A semi-decentralized framework for heterogeneity-aware pipeline-parallel training"
      },
      {
        "key": "miao2021heterogeneity",
        "author": "Miao, Xupeng and Nie, Xiaonan and Shao, Yingxia and Yang, Zhi and Jiang, Jiawei and Ma, Lingxiao and Cui, Bin",
        "title": "Heterogeneity-aware distributed machine learning training via partial reduce"
      },
      {
        "key": "zhang2022mics",
        "author": "Zhang, Zhen and Zheng, Shuai and Wang, Yida and Chiu, Justin and Karypis, George and Chilimbi, Trishul and Li, Mu and Jin, Xin",
        "title": "MiCS: near-linear scaling for training gigantic model on public cloud"
      },
      {
        "key": "um2024metis",
        "author": "Um, Taegeon and Oh, Byungsoo and Kang, Minyoung and Lee, Woo-Yeon and Kim, Goeun and Kim, Dongseob and Kim, Youngtaek and Muzzammil, Mohd and Jeon, Myeongjae",
        "title": "Metis: Fast Automatic Distributed Training on Heterogeneous $\\{$GPUs$\\}$"
      },
      {
        "key": "acc_par",
        "author": "Linghao Song and\nFan Chen and\nYouwei Zhuo and\nXuehai Qian and\nHai Li and\nYiran Chen",
        "title": "AccPar: Tensor Partitioning for Heterogeneous Deep Learning Accelerators"
      },
      {
        "key": "whale",
        "author": "Xianyan Jia and\nLe Jiang and\nAng Wang and\nWencong Xiao and\nZiji Shi and\nJie Zhang and\nXinyuan Li and\nLangshi Chen and\nYong Li and\nZhen Zheng and\nXiaoyong Liu and\nWei Lin",
        "title": "Whale: Efficient Giant Model Training over Heterogeneous GPUs"
      },
      {
        "key": "hap",
        "author": "Shiwei Zhang and\nLansong Diao and\nChuan Wu and\nZongyan Cao and\nSiyu Wang and\nWei Lin",
        "title": "{HAP:} {SPMD} {DNN} Training on Heterogeneous {GPU} Clusters with\nAutomated Program Synthesis"
      },
      {
        "key": "amp_hetero_model_parallel",
        "author": "Dacheng Li and\nHongyi Wang and\nEric P. Xing and\nHao Zhang",
        "title": "{AMP:} Automatically Finding Model Parallel Strategies with Heterogeneity\nAwareness"
      }
    ]
  }
]