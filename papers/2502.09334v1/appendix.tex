\appendix
\clearpage

% \section{Tabu Search Algorithm}
% \label{appendix:tabu}

% We demonstrate the algorithm of tabu search in Algorithm~\autoref{alg:tabu}. The tabu search algorithm starts from an initial solution, and leverages an iterative neighborhood search process to improve the solution.

% \begin{algorithm}[tb]
% \caption{Routine of solving the upper-level problem based on tabu search, where $N_{\text{step}}$ denotes the number of search steps, $N_{\text{nghb}}$ denotes the number of neighbors to navigation in each step, $N_{\text{mem}}$ denotes the maximum number of memorized solutions, and $f(\cdot)$ denotes the performance of a solution evaluated by solving the lower-level problem.
% }
% \label{alg:tabu}
% \begin{algorithmic}

% \STATE \textbf{function} \textsc{TabuSearch}($N_{\text{step}}$=100, $N_{\text{nghb}}$=10, $N_{\text{mem}}$=5)
% \STATE Initialize the current solution $x$
% \STATE Initialize tabu list $T \gets []$, best solution $x_{best} \gets x$
% \STATE {\color{gray}{/* Iterative Neighborhood Search */}}
% % \While{termination conditions not met}
% \FOR{$N_{\text{step}}$ search steps}
%     \STATE Construct $N_{\text{nghb}}$ neighbors of $x$ and exclude those in $T$ to form the neighborhood set $\mathcal{N}$ for navigation
%     % \State{\textcolor{gray}{/* Neighborhood Evaluation */}}
%     \STATE $x' \gets \argmax_{x'' \in \mathcal{N}} f(x'')$
%     % \State{\textcolor{gray}{/* Update Best Solution */}}
%     \STATE \textbf{if} $f(x') > f(x_{best})$ \textbf{then} $x_{best} \gets x'$
%     % \State{\textcolor{gray}{/* Manage Tabu List */}}
%     \STATE $T.\operatorname{append}(x')$
%     %\textbf{if} $x' \not\in T$ \textbf{then} $T.\operatorname{append}(x')$
%     \STATE \textbf{if} $\operatorname{len}(T) > N_{\text{mem}}$ \textbf{then} $T \gets T[-N_{\text{mem}}:\;]$
%     \STATE $x \gets x'$
% \ENDFOR
% % \EndWhile
% \STATE \textbf{return} $x_{best}$
% \STATE \textbf{endfunction}
% % \EndFunction
% \end{algorithmic}
% \end{algorithm}

\section{NP-Hardness of Deployment Planning}
\label{appendix:a}
Finding the optimal deployment plan that maximizes the overall SLO for deploying multiple models on a heterogeneous GPU cluster with variable interconnect topology and computational capabilities is non-trivial. In particular, we show that this problem is NP-hard by transforming it into the well-known NP-hard Job Shop Scheduling Problem (JSSP) \cite{sotskov1995np,omar2006job}. 
% adapted to reflect the unique aspects of maximizing goodput.


\noindent \textbf{Transformation of the deployment planning problem to JSSP.} Each GPU in the heterogeneous cluster serves as a distinct machine in the JSSP. These GPUs exhibit differences in computation power, memory, and communication capabilities. Each model, or its components depending on the placement method, is considered a job within JSSP. The deployment of each model involves multiple tasks or operations, each corresponding to the deployment of a part of a model on one or more GPUs, accompanied by specific resource requirements and execution constraints. Sequential dependencies are evident in scenarios where the completion of one operation on a GPU is prerequisite for the initiation of the next on another GPU, characteristic of pipeline model parallelism. Concurrent dependencies arise when operations must occasionally synchronize across GPUs, reflecting interdependencies that require coordination akin to those in tensor model parallelism. In this context, maximizing SLO does not solely involve minimizing idle and wait times but also necessitates the optimization of the allocation and scheduling of operations to ensure continuous and efficient GPU utilization. Thus, this challenge can be viewed as a variant of JSSP where the objective shifts from minimizing makespan to maximizing SLO, analogous to maximizing the number of completed jobs or operations within certain latency deadlines. This requires managing both the sequence and concurrency of operations across heterogeneous resources and optimizing overall system efficiency to mitigate bottlenecks and reduce synchronization overheads.

Job shop scheduling is recognized as NP-hard due to the complexity inherent in managing dependencies and varying capabilities across machines. By formulating this problem as a variant of JSSP adapted for SLO, we establish that solving the model placement problem is at least as hard as solving the classic NP-hard JSSP, thus confirming the NP-hardness of the problem.

\input{alg_model_configuration_deduction}

\section{Deduction of Parallel Configuration}
\label{appendix:b}
Given the group formation and the designated phase, we need to deduce the optimal parallel configuration for each group. Algorithm \autoref{alg:genmodelparallel} outlines the process. \mytextcircled{1} We enumerate all possible TP and PP combinations on each given group formation. Note that our first heuristic is to limit tensor model parallelism within single-type GPUs, so the TP degree should be smaller or equal to the minimum number of single-type GPUs in the group, which largely minimizes the search space. \mytextcircled{2} Dynamic programming algorithm is utilized to route the pipeline communication path. It optimizes communication routing in a network by using a bitmask to represent all possible subsets of stages, initializes each stage with a zero bandwidth and builds paths by calculating the potential bandwidth for each link between stages, updates the optimal path recursively if a higher bandwidth stage is found, and determines the maximum bandwidth path available by examining the states for the subset that includes all stages, ensuring the most efficient data transfer across the network.  \mytextcircled{3} We adjust the pipeline layer partition with respect to the memory capacity and computing ability of different GPU types. Specifically, the pipeline partition is adjusted in proportion to the total memory and computing capacity of the GPU set currently servicing this stage, while ensuring that the memory limits of individual GPUs are not exceeded. This heuristic has proven effective in determining an optimal pipeline partition. \mytextcircled{4} For the compute-bound prefill replicas, we select the latency optimal plans, for the memory bandwidth-bound decode replicas, we select the throughput optimal plans. To estimate the latency and throughput of each plan, we employ the cost model proposed by HexGen \cite{jiang2024hexgen}, which directly provides us with the inference memory and latency costs for both prefill and decode phases, relative to different request batch sizes. We calculate the throughput by dividing the maximum total batched token size that the device group can handle by the decode latency. Note that the estimated latency information is also provided to our simulator for SLO estimation.

% \jyh{
% \section{Performance Metrics}
% \label{appendix:pm}
% % \subsection{Performance Metrics}
% There are three key metrics to evaluate LLM inference: \textit{time to first token (TTFT)}, which measures the time cost of generating the first token; \textit{time per output token (TPOT)}, which quantifies the average time cost of generating each token in the decode phase; \textit{end-to-end (E2E) latency}, which assesses the overall processing time cost of a request (includes queuing, prefill, and decode costs).

% For LLM serving systems, there are also two key metrics: \textit{Service Level Objective (SLO) attainment}, which represents the percentage of requests that can be served within a predefined time frame set by the SLO, and the SLO is often scaled to different multiples of single device execution latency (denoted as SLO scale) to evaluate system performance under different levels of SLO stringency; \textit{throughput}, which measures the number of requests a system can handle within a specified time period. Efficient LLM serving systems should optimize either of the metrics, and meanwhile meet the performance requirements of specific applications if necessary.
% }

% \section{Communication Matrix}
\section{Inter-connection Bandwidth Matrix}
\label{appendix:comm_matrix}
The bandwidth distributions exhibit significant variability in cloud and in-house environments. We measure the communication bandwidth between each pair of GPUs via NCCL for both environments described in \S\ref{sec:eva_setup}. As shown in the left heatmap of \autoref{fig:comm_matrix}, the cloud environment demonstrates notable bandwidth heterogeneity, influenced by a range of GPU types and network configurations. This variability results in non-uniform connectivity patterns across the network. Conversely, the right heatmap showcases the in-house environment, characterized by a uniform GPU-to-GPU communication bandwidth, evidenced by consistently high connectivity values. These visualizations emphasize the distinctions between cloud and in-house environments.

\begin{figure}[!t] 
  \centering
  \includegraphics[width=\linewidth]{img/communication_matrix.pdf} % Adjust the width as needed
  % \caption{\jyh{Heat map of communication matrix in the cloud (left) and in-house (right) settings.}}
  \caption{\jyh{Heat map of inter-connection bandwidth matrix in the cloud (left) and in-house (right) settings.}}
  \label{fig:comm_matrix}
\end{figure}

% \jyh{
% \section{Implementation Optimizations}
% \label{appendix:io}
% \noindent \textbf{Dynamic request batching.}
% In real-world applications, we often observe a significant skew in prefill-decode lengths, which can degrade the performance of serving systems. To address this issue, we have implemented a simple yet effective request batching strategy that accommodates the variability in prefill-decode lengths. On the prefill side, operations are compute-bound, necessitating a balanced approach to request batching. To optimize GPU utilization without unnecessarily prolonging TTFT for batched requests, we determine the maximum batched total token size for each GPU type through a one-time profiling process. Consequently, multiple small requests are batched together, while longer requests exceeding this limit are processed individually, accounting for the heterogeneous compute capabilities of different GPU types. On the decode side, operations are memory-bound, we strive to maximize GPU utilization by batching as many requests as possible, thereby optimizing computational efficiency and leveraging the full capacity of heterogeneous GPUs within their memory limits. This integrated approach ensures that both memory usage and batching efficiency are managed effectively to optimize overall system performance.

% 
% \noindent \textbf{Parallel communication groups.} All communication primitives in \sys are implemented using NVIDIA Collective Communication Library (NCCL). Given the system's support for complex hybrid model parallelism strategies, multiple communication groups may be constructed among GPUs according to the parallelization plan. To circumvent the substantial overhead associated with constructing NCCL groups, \sys preemptively establishes a global communication group pool containing all potentially required groups. For KV cache communication, we employ NCCL's asynchronous send and receive functions to prevent GPU blocking during transmission. KV cache queues are maintained on the prefill replicas, and upon completion of a decoding round, the decode replicas retrieve KV caches from these queues, utilizing the GPU memory of the prefill replicas as queuing buffers.

% 
% \noindent \textbf{Parallel configuration-aware KV cache communication.} We implement a parallel configuration-aware technique for KV cache communication. This ensures that slices of KV cache from a prefill replica can be directly transferred to the appropriate devices in the decode replica, even when the two replicas are configured with different forms of model parallelism. This approach avoids unnecessary gather and broadcast operations of KV cache slices and maximizes the utilization of GPU-to-GPU bandwidth. \sys automatically handles the slicing and routing to optimize communication efficiency.
% }

\section{Ratio Impact on System SLO Attainment}
\label{appendix:ratio}

We show the impact of phase designation and orchestration on overall system SLO attainment in~\autoref{fig:ratio_goodput}.
The coding workload, characterized by relatively longer input length and
shorter output length, exhibits enhanced performance with more prefill replicas and fewer decode replicas. A ratio of 5:3 yields the optimal results. Conversely, the conversation workload, typified by relatively shorter prompts and longer responses, necessitates more decode replicas and fewer prefill replicas to prioritize resources to the long-running decoding. Here, a ratio of 3:5 achieves the best performance.

\begin{figure}[!t] 
  \centering
  \includegraphics[width=\linewidth]{img/ratiogoodput.pdf} % Adjust the width as needed
  \caption{{Impact of phase designation and orchestration on overall system \jyhh{SLO attainment}. We experiment with LLaMA-13B on both coding and conversation workloads across 16 A5000 GPUs, with two GPUs serving one replica.}}
  \label{fig:ratio_goodput}
  % \vspace{-2em}
\end{figure}

\section{Implementation Details}
\label{appendix:components}

\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{img/systemoverview_revised.pdf} % Adjust the width as needed
  \vspace{-1em}
  \caption{System overview of \sys.}
  \label{fig:sysoverview}
  \vspace{-1em}
\end{figure}

\noindent \textbf{Overview of \sys.} 
The architecture overview of \sys is shown in \autoref{fig:sysoverview}. There are three major components, which are the scheduler, the workload profiler, and the task coordinator.

% \noindent \textbf{Scheduler.} 
% The scheduler is the core of \sys for high-performance LLM serving in cloud environments.  
% The scheduler takes as input the model configurations (e.g., hidden size and layer number), workload patterns obtained from the workload profiler, cluster information (e.g., available GPUs and their corresponding types), and communication bandwidth matrix among all GPUs. 
% Then, it performs a novel scheduling algorithm to provide four results. 
% \mytextcircled{1} The \ul{\textit{group construction}}, i.e., how to partition the GPUs into multiple model serving groups, where each group is responsible for a model replica. 
% \mytextcircled{2} The \ul{\textit{phase designation}} that indicates whether each group should serve as the prefill or decode phase. 
% % is termed a prefill- or decode-instance, this ratio is defined as the proportion of total prefill-instances to decode-instances within the serving cluster. 
% \mytextcircled{3} The \ul{\textit{parallel configuration}} for each model replica.
% \mytextcircled{4} The \ul{\textit{orchestration of prefill and decode replicas}} to guide how the requests should be routed. 
% We refer these results of the scheduling algorithm as a \ul{\textit{deployment plan}}.
The \textit{scheduler} is the core of \sys for high-performance LLM serving in cloud environments.  
The scheduler takes as input the model configurations (e.g., hidden size and layer number), workload patterns obtained from the workload profiler, cluster information (e.g., available GPUs and their corresponding types), and communication bandwidth matrix among all GPUs. Then, it performs the scheduling algorithm introduced in \S\ref{sec:scheduling} to provide the optimal deployment plan. 
Should there be a detected shift in workload, or a GPU heartbeat timeout that suggests a need for cluster size adjustment, the scheduler will perform the lightweight re-scheduling process and adjust the deployment plan to adapt to the new workload or cluster size. 
% The scheduling algorithm is discussed in \S\ref{sec:scheduling}.


% \noindent \textbf{Workload profiler.}
% The workload profiler is responsible for monitoring the real-time workload patterns. We mainly focus on three key factors: average prompt length of incoming requests, average output length of generated responses, and average request arrival rate. The first two factors can be used to analyze the prefill and decode cost for each single request, and the third factor reflects the burstness in workloads. 
% Once an obvious shift in these factors are detected, the workload profiler will notify the scheduler. 
The \textit{workload profiler} monitors the real-time workload patterns, including the average prompt length of incoming requests and average output length of generated responses. These patterns are utilized to analyze the prefill and decode cost for each single request. 
For instance,  in contemporary LLM services, common workload scenarios include coding and conversation \cite{patel2023splitwise}, where both typically have a median prompt length exceeding 1000 tokens. However, the coding service produces much fewer output tokens, with a median of 13, while the conversation service generates a larger number of output tokens, with a median of 129. Undoubtedly, the overall system workload varies when the proportions of incoming requests for various services change in real-time.
% When the online service workload changes from coding to conversation, the workload profiler will notify the scheduler.
Once an obvious workload shift is detected, the workload profiler will notify the scheduler. 


% \noindent \textbf{Task coordinator.} 
The \textit{task coordinator} is in charge of the request dispatching among the prefill and decode replicas. 
Upon receiving a request, the task coordinator assigns the appropriate prefill replica and decode replica, respectively. The assignment is guided by the deployment plan generated by the scheduler.
% When a request is received by the task coordinator, the request will be directed to an appropriate prefill replica, and the generated KV cache will subsequently be routed to the corresponding decode replica.
The task coordinator is mainly based on an open-source implementation of decentralized computation coordination \cite{yao2023open} that utilizes libP2P \cite{libp2p} to establish connections among the work groups in a peer-to-peer network.

Based on these components, the overall routine of \sys is as follows.
\mytextcircled{1} To launch a serving process, the scheduler generates the deployment plan, which is then utilized to instantiate the model replicas over the cloud GPU resources. 
\mytextcircled{2} During the serving process, the coordinator dispatches the incoming requests across the prefill and decode replicas, and gathers the generated responses. 
\mytextcircled{3} At the same time, the workload profiler consistently monitors the workload and reports to the scheduler. 
\mytextcircled{4} Once a workload shift is detected, the scheduler triggers a lightweight re-scheduling process to adjust the deployment plan for better adaptation to the new workload.

\textbf{Parallel communication groups.} All communication primitives in \sys are implemented using NVIDIA Collective Communication Library (NCCL). To circumvent the substantial overhead associated with constructing NCCL groups, \sys preemptively establishes a global communication group pool containing all potentially required groups. For KV cache communication, we employ NCCL's asynchronous \texttt{SendRecv}/\texttt{CudaMemcpy} functions for KV cache communication to prevent GPU blocking and enable computation and communication overlapping during transmission. KV cache queues are maintained on the prefill replicas, and upon completion of a decoding round, the decode replicas retrieve KV caches from these queues, utilizing the GPU memory of the prefill replicas as queuing buffers.

\section{Case Study of Scheduling}
\label{appendix:scheduling_results}
We list the deployment plan generated by \sys from coding workload to conversation workload in the heterogeneous setting. We use the following representation to describe the scheduled results. We use an array to specify one independent model replica, with two numbers representing the degrees of tensor model parallelism and pipeline model parallelism. For example, (2,2) indicates a model replica with tensor model parallel degree of 2 and pipeline model parallel degree of 2 (2 pipeline stages).

We also provide the instances we considered in \S\ref{sec:eva_setup} here for better readability: two 4$\times$A6000 instances, \jyh{two 4$\times$A5000 instances}, one 8$\times$A40 instance and two 4$\times$3090Ti instances, making up to be 32 GPUs in total.


\noindent \textbf{Parallel configuration breakdown.} In the coding workload, the 8$\times$A40 instance employs a parallel strategy (2,1) to support four prefill replicas. One 4$\times$A6000 instance uses a parallel strategy (2,1) to support two prefill replicas, while the other one 4$\times$A6000 instance uses a parallel strategy (1,2) for two decode replicas. 
% The 8$\times$A5000 instance utilizes a parallel strategy (4,1) to support two prefill replicas.
\jyh{One 2$\times$A5000 and one 2$\times$3090Ti instances utilize a parallel strategy (2,2) to support one prefill replica, and the other one 2$\times$A5000 and one 2$\times$3090Ti instances utilize a parallel strategy (2,2) to support one decode replica. One 4$\times$A5000 instance utilizes a parallel strategy (4,1) to support one prefill replica. One 4$\times$3090Ti instance implements a parallel strategy (2,2) to support one decode replica.}
% And the two 4$\times$3090Ti instances implement a parallel strategy (2,2) to support two decode replicas. 

In the conversation workload, the 8$\times$A40 instance employs parallel strategies (2,1) and (1,2) to support three prefill replicas and one decode replica, respectively. The two 4$\times$A6000 instances utilize a parallel strategy (1,2) to support four decode replicas. % The 8$\times$A5000 instance implements a parallel strategy (2,2) to support two decode replicas. And the two 4$\times$3090Ti instances deploy a parallel strategy (2,2) to support two decode replicas.
\jyh{One 2$\times$A5000 and one 2$\times$3090Ti instances utilize a parallel strategy (2,2) to support one prefill replica, and the other one 2$\times$A5000 and one 2$\times$3090Ti instances utilize a parallel strategy (2,2) to support one decode replica. One 4$\times$A5000 instance utilizes a parallel strategy (2,2) to support one decode replica. One 4$\times$3090Ti instance implements a parallel strategy (2,2) to support one decode replica.}


\noindent \textbf{Insights.} In the in-house setting, the 8$\times$A100 instance can only serve 4 model replicas, while in the cloud setting, the 32 cloud GPUs with various types can serve a maximum of 12 model replicas with various parallel configuration within the same price budget. In this case, although individual inference tasks in the cloud setting may experience increased latency due to the lower hardware performance (e.g., GPU flops and bandwidth), the overall system performance is improved due to the higher number of model replicas. Additionally, our scheduling algorithm prioritizes GPUs with high peak fp16 flops for prefilling (e.g., A40) and high memory bandwidth GPUs for decoding (e.g., 3090Ti), and selects the most suitable model parallel configuration for each phase to optimize the overall system performance. \jyh{And although KV cache compression can linearly mitigate communication overhead, significant disparities in bandwidth across different cloud environments render extremely low bandwidth scenarios—such as those experienced between data centers—unsuitable for effective KV cache communication. Thanks to our scheduling and orchestration algorithms, \sys automatically identifies KV cache transmission paths that maintain overall performance.}

\section{Case Study of Lightweight Rescheduling}
\label{appendix:lightweight_rescheduling}
We list the deployment plan change during lightweight rescheduling with 4 out of 32 GPUs (one 4$\times$A6000 instance that support two decode replicas) become unavailable.

The deployment plan for the coding workload, detailed in Appendix \ref{appendix:scheduling_results}, initially includes 8 prefill and 4 decode replicas. After the offline of 4 GPUs, there are 8 prefill and 2 decode replicas remaining. A subsequent lightweight rescheduling converts one prefill replica, which uses 4 A5000 GPUs with a (4,1) strategy, into a decode replica. The adjustment is reasonable as this group of GPUs exhibits the highest overall memory bandwidth among the prefill replicas. 
The deployment plan for the conversation workload initially includes 4 prefill and 8 decode replicas. After the offline of 4 GPUs, there are 4 prefill and 6 decode replicas remaining. A subsequent lightweight rescheduling converts one prefill replica, which uses 2 A40 GPUs with a (2,1) strategy, into a decode replica.

\jyh{
\section{Case Study of Network Effect on Phase Splitting}
\label{appendix:networkeffect}

\begin{figure}[!t] 
  \centering
  \includegraphics[width=\linewidth]{img/caseab.pdf} % Adjust the width as needed
  \caption{\jyh{Two exampled network conditions on cloud.}}
  \label{fig:caseab}
\end{figure}


\begin{figure}[!t] 
  \centering
  \includegraphics[width=\linewidth]{img/caseab2.pdf} % Adjust the width as needed
  \caption{\jyh{\sys deployment plans on different cases.}}
  \label{fig:caseab2}
\end{figure}

\begin{table}[h]
\centering
\jyh{
% \fontsize{7.5pt}{9pt}\selectfont
\small
\caption{\jyh{Benchmarks of non-disaggregation baseline vs. \sys under high inter-instance communication bandwidth vs. \sys under low inter-instance communication bandwidth.}}
\resizebox{\linewidth}{!}{
\begin{tabular}{ l c c c c }
\hline
\textbf{Configuration} & \textbf{Prefill} & \textbf{KV Comm} & \textbf{Decode} & \textbf{E2E Throughput} \\
\hline
Baseline & 884 ms & 0 ms & 1689 ms & 1610 tokens/s\\
\hline
\sys (High) & 698 ms & 133 ms & 1126 ms & 3292 tokens/s \\
\hline
\sys (Low) & 964 ms & 41 ms & 1846 ms & 2196 tokens/s \\
\hline
\end{tabular}
\label{tab:dandnod}
}
}
\end{table}

% \begin{table}[!t]
% \centering
% \caption{\jyh{LLaMA perplexity results (16-bit vs. 4-bit) on WikiText2, PTB and CBT datasets.}}
% \jyh{
% \begin{tabular}{c c c c}
% \hline
% \textbf{Dataset} &  & \textbf{LLaMA-7B} & \textbf{LLaMA-30B} \\ \hline
% \multirow{2}{*}{WikiText2} & 16-bit & 3.53 & 2.73 \\ 
% % \cline{2-4} 
%                                     & 4-bit  & 3.55 & 2.75 \\ \hline
% \multirow{2}{*}{PTB} & 16-bit & 7.46 & 6.49 \\ 
% % \cline{2-4} 
%                                        & 4-bit  & 7.42 & 6.55 \\ \hline
% \multirow{2}{*}{CBT} & 16-bit & 7.66 & 6.31 \\ 
% % \cline{2-4} 
%                                                & 4-bit  & 7.70  & 6.30  \\ \hline
% \end{tabular}
% }
% \label{tab:PPL}
% \end{table}

\begin{table}[!t]
\centering
\small
\caption{\jyh{Impact of KV cache communication compression on the perplexity results on WikiText2, PTB and CBT datasets.}}
\jyh{
\begin{tabular}{c c c c}
\hline
\textbf{Dataset} &  & \textbf{LLaMA-7B} & \textbf{LLaMA-30B} \\ \hline
\multirow{2}{*}{WikiText2} & 16-bit & 3.53 & 2.73 \\ 
% \cline{2-4} 
                                    & 4-bit  & 3.55 & 2.75 \\ \hline
\multirow{2}{*}{PTB} & 16-bit & 7.46 & 6.49 \\ 
% \cline{2-4} 
                                       & 4-bit  & 7.42 & 6.55 \\ \hline
\multirow{2}{*}{CBT} & 16-bit & 7.66 & 6.31 \\ 
% \cline{2-4} 
                                               & 4-bit  & 7.70  & 6.30  \\ \hline
\end{tabular}
}
\label{tab:PPL}
\end{table}

\begin{table}[!t]
\centering
\small
\caption{\jyh{LLaMA rouge results (using 16-bit outputs as the ground truth and the 4-bit outputs as the prediction) on WikiText2, PTB and CBT datasets.}}
\jyh{
\begin{tabular}{c c c c}
\hline
\textbf{Dataset} &  & \textbf{LLaMA-7B} & \textbf{LLaMA-30B} \\ \hline
\multirow{3}{*}{WikiText2} & ROUGE-1 & 0.962 & 0.942 \\ 
% \cline{2-4} 
                           & ROUGE-2 & 0.941 & 0.928 \\ 
                           
                           % \cline{2-4} 
                           & ROUGE-L & 0.955 & 0.941 \\ \hline
\multirow{3}{*}{PTB}       & ROUGE-1 & 0.975 & 0.928 \\ 
% \cline{2-4} 
                           & ROUGE-2 & 0.950  & 0.911 \\ 
                           % \cline{2-4} 
                           & ROUGE-L & 0.971 & 0.928 \\ \hline
\multirow{3}{*}{CBT}       & ROUGE-1 & 0.925 & 0.946 \\ 
% \cline{2-4} 
                           & ROUGE-2 & 0.912 & 0.931 \\ 
                           % \cline{2-4} 
                           & ROUGE-L & 0.925 & 0.937 \\ \hline
\end{tabular}
}
\label{tab:rouge_performance}
\end{table}

\begin{table}[h]
\centering
\jyh{
\small
\caption{\jyh{Benchmarks of \sys with 16-bit vs. 4-bit communications.}}
\resizebox{\linewidth}{!}{
\begin{tabular}{ l c c c c }
\hline
\textbf{Configuration} & \textbf{Prefill} & \textbf{KV Comm} & \textbf{Decode} & \textbf{E2E Throughput} \\
\hline
16-bit & 684 ms & 584 ms & 1108 ms & 2450 tokens/s \\
\hline
4-bit & 698 ms & 133 ms & 1126 ms & 3292 tokens/s \\
\hline
\end{tabular}
\label{tab:add}
}
}
\end{table}

% Consider the use of \sys to serve LLaMA-30B model in a heterogeneous environment featuring two GPU instances: the first instance equipped with 4$\times$A40-48G GPUs, and the second with 4$\times$3090Ti-24G GPUs. We conducted tests on the inference throughput of this setup by feeding it continuous input sequences of length 1024 under two different inter-instance communication bandwidths: 40 Gbps and 5 Gbps. We established a non-disaggregating baseline that utilizes 4$\times$A40-48G GPUs to support one model replica and 4$\times$3090Ti-24G GPUs to support another.

Consider use \sys to serve LLaMA-30B model in a heterogeneous environment featuring two GPU instances: the first instance equipped with 4$\times$A40 GPUs, and the second with 4$\times$3090Ti GPUs. We conducted tests on the inference throughput of this setup by feeding it continuous input sequences of length 1024 under two different inter-instance communication bandwidths: 40 Gbps and 5 Gbps, as demonstrated in \autoref{fig:caseab}.

We established a non-disaggregating baseline that utilizes 4$\times$A40 GPUs to support one model replica and 4$\times$3090Ti GPUs to support another. By comparing the baseline with \sys under different network conditions, we observed some interesting results: With a bandwidth of 40 Gbps, \sys leverages the 4$\times$A40 GPUs with higher peak flops to support one prefill replica, and the 4$\times$3090Ti GPUs with higher memory access bandwidth to support one decode replica. This configuration optimizes system performance, achieving a 2$\times$ performance gain over the non-disaggregating baseline. However, at a lower bandwidth of 5 Gbps, the inter-instance communication bandwidth is insufficient for efficient KV cache communication. Consequently, \sys allocates 2$\times$A40 GPUs and 2$\times$3090Ti GPUs to both prefill and decode replica, which utilizes intra-instance high network bandwidth for KV cache communication and inter-instance low network bandwidth for pipeline communication, resulting in a 1.4$\times$ improvement over the non-disaggregating baseline. The illustration of deployment plans are demonstrated in \autoref{fig:caseab2}, the single request prefill/decode/KV cache communication time and overall system throughputs are demonstrated in \autoref{tab:dandnod}.
}

\begin{figure}[!t]
  \centering
  % \includegraphics[width=\linewidth]{img/kv cache compression 24816.pdf} % Adjust the width as needed
  \includegraphics[width=\linewidth]{img/kvq.pdf}
    \vspace{-1em}
  \caption{\jyh{Impact of KV cache communication compression. (Non-transparent: time cost of KV cache communication. Transparent: end-to-end processing time.)}}
  \label{fig:kvcache}
    \vspace{-1em}
\end{figure}

\jyh{
\section{PPL and ROUGE results on KV cache compression}
\label{appendix:ppl}
We list the PPL and ROUGE results of LLaMA-7B and LLaMA-30B models on WikiText2, PTB and CBT datasets with both 16-bit and 4-bit KV cache precision levels, as shown in \autoref{tab:PPL} and \autoref{tab:rouge_performance}. Experimental results have demonstrated that the PPL between 16-bit precision and 4-bit precision is within 1\% across all experimental scenarios, and the ROUGE-1, ROUGE-2 and ROUGE-L scores are around 0.95 across all cases, which confirms the validity of our approach. We also demonstrate the the benchmarks of \sys with 16-bit vs. 4-bit communications in \autoref{tab:add} with the same experimental setups as mentioned in \autoref{appendix:networkeffect}, and benchmarks in \autoref{fig:kvcache}, with two A5000 GPUs serving a LLaMA-7B model.
}

\begin{figure}[!t] 
  \centering
  \includegraphics[width=\linewidth]{img/estimated_vs_real.pdf} % Adjust the width as needed
  \caption{\jyhh{Comparison of benchmarked and estimated performance metrics for simulator (left) and alpha-beta model (right).}}
  \label{fig:accuracy}
\end{figure}

\jyhh{
\section{Simulator and alpha-beta model accuracy}
\label{appendix:simu}
To assess the accuracy of the simulator and alpha-beta model for KV cache communication, we conducted a series of micro-benchmarks using the LLaMA-30B model. These benchmarks varied in SLO scales and batched token sizes to evaluate our estimation outputs against actual execution metrics, specifically SLO attainment and latency. The results, detailed in \autoref{fig:accuracy}, indicate that the simulator and alpha-beta model closely correspond with actual execution performance.
}