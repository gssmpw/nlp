\section{Implementation}
\label{sec:impl}

\sys is a distributed LLM serving system designed to optimize online services in cloud environments, which develops a novel scheduling algorithm to partition the given cloud GPU resources into model serving groups, designate which phase each group should serve as, deduce the optimal parallel configuration for each group, and determine how the requests should be routed among groups.
% which enables hybrid model parallelism and phase splitting for enhanced inference performance, and develops a heuristic algorithm based on tabu search to determine the GPU partitioning and phase designation on cloud clusters. 
It is implemented using 20K lines of Python and C++/CUDA code. \ffc{Besides, \sys incorporates FlashAttention \cite{dao2022flashattention} and PagedAttention \cite{kwon2023efficient} to accelerate LLM inference, and leverages the batching strategy proposed by \citet{zhong2024distserve} for LLM serving.}
% \sys supports widely-used open-source LLMs, including the GPT series \cite{floridi2020gpt} and the LLaMA series \cite{touvron2023llama}. 
% More implementation details of \sys are introduced in \autoref{appendix:components}.


% \begin{figure}[!t]
%   \centering
%   % \includegraphics[width=\linewidth]{img/kv cache compression 24816.pdf} % Adjust the width as needed
%   \includegraphics[width=0.8\linewidth]{img/kvq.pdf}
%     \vspace{-1em}
%   \caption{\jyh{Impact of KV cache communication compression. (Non-transparent: time cost of KV cache communication. Transparent: end-to-end processing time.)}}
%   \label{fig:kvcache}
%     \vspace{-1em}
% \end{figure}

\begin{table}[!t]
\centering
\small
\caption{\jyh{\small{Impact of KV cache communication compression on the model accuracy on CoQA, TruthfulQA and GSM8K tasks.}}}
\jyh{
% \resizebox{\linewidth}{!}{
\begin{tabular}{c c c c}
\hline
\textbf{Task} &  & \textbf{LLaMA-7B} & \textbf{LLaMA-13B} \\ \hline
\multirow{2}{*}{CoQA} & 16-bit & 63.95 & 66.35 \\ 
% \cline{2-4} 
                                    & 4-bit  & 64.58 & 66.54 \\ \hline
\multirow{2}{*}{TruthfulQA} & 16-bit & 30.64 & 29.68 \\ 
% \cline{2-4} 
                                       & 4-bit  & 30.13 & 29.34 \\ \hline
\multirow{2}{*}{GSM8K} & 16-bit & 13.23 & 22.34 \\ 
% \cline{2-4} 
                                               & 4-bit  & 12.54  & 21.29  \\ \hline
\end{tabular}
}
% }
\label{tab:acc}
  \vspace{-1em}
\end{table}


% \jyh{
% 
% \noindent \textbf{Dynamic request batching.}
% In real-world applications, we often observe a significant skew in prefill-decode lengths, which can degrade the performance of serving systems. To address this issue, we have implemented a simple yet effective request batching strategy that accommodates the variability in prefill-decode lengths. On the prefill side, operations are compute-bound, necessitating a balanced approach to request batching. To optimize GPU utilization without unnecessarily prolonging TTFT for batched requests, we determine the maximum batched total token size for each GPU type through a one-time profiling process. Consequently, multiple small requests are batched together, while longer requests exceeding this limit are processed individually, accounting for the heterogeneous compute capabilities of different GPU types. On the decode side, operations are memory-bound, we strive to maximize GPU utilization by batching as many requests as possible, thereby optimizing computational efficiency and leveraging the full capacity of heterogeneous GPUs within their memory limits. This integrated approach ensures that both memory usage and batching efficiency are managed effectively to optimize overall system performance.
% }


% \noindent \textbf{Parallel communication groups.} All communication primitives in \sys are implemented using NVIDIA Collective Communication Library (NCCL). Given the system's support for complex hybrid model parallelism strategies, multiple communication groups may be constructed among GPUs according to the parallelization plan. To circumvent the substantial overhead associated with constructing NCCL groups, \sys preemptively establishes a global communication group pool containing all potentially required groups. For KV cache communication, we employ NCCL's asynchronous send and receive functions to prevent GPU blocking during transmission. KV cache queues are maintained on the prefill replicas, and upon completion of a decoding round, the decode replicas retrieve KV caches from these queues, utilizing the GPU memory of the prefill replicas as queuing buffers.

% 
% \noindent \textbf{Parallel configuration-aware KV cache communication.} We implement a parallel configuration-aware technique for KV cache communication. This ensures that slices of KV cache from a prefill replica can be directly transferred to the appropriate devices in the decode replica, even when the two replicas are configured with different forms of model parallelism. This approach avoids unnecessary gather and broadcast operations of KV cache slices and maximizes the utilization of GPU-to-GPU bandwidth. \sys automatically handles the slicing and routing to optimize communication efficiency.


% \textbf{Parallel communication groups.} All communication primitives in \sys are implemented using NVIDIA Collective Communication Library (NCCL). To circumvent the substantial overhead associated with constructing NCCL groups, \sys preemptively establishes a global communication group pool containing all potentially required groups. For KV cache communication, we employ NCCL's asynchronous \texttt{SendRecv}/\texttt{CudaMemcpy} functions for KV cache communication to prevent GPU blocking and enable computation and communication overlapping during transmission. KV cache queues are maintained on the prefill replicas, and upon completion of a decoding round, the decode replicas retrieve KV caches from these queues, utilizing the GPU memory of the prefill replicas as queuing buffers.

\textbf{Overall routine.} The overall routine of \sys is as follows.
\mytextcircled{1} To launch a serving process, the scheduling algorithm (\S\ref{sec:schedule_upper_level} and \S\ref{sec:schedule_lower_level}) generates the deployment plan, which is then utilized to instantiate the model replicas over the cloud GPU resources. 
\mytextcircled{2} During the serving process, the incoming requests are dispatched across the prefill and decode replicas, and the generated responses are gathered.
\mytextcircled{3} At the same time, the inference workload is constantly monitored and reported to the scheduling algorithm.
\mytextcircled{4} Once a workload shift is detected, the scheduling algorithm triggers the lightweight re-scheduling process (\S\ref{sec:method_light_reschedule}) to adjust the deployment plan in response to the new workload.
Due to the space constraint, we refer interested readers to \autoref{appendix:components} for more implementation details of \sys.



% \textbf{KV cache communication.} 
% \ffc{
% \sys employs NVIDIA Collective Communication Library (NCCL)'s asynchronous \texttt{SendRecv}/\texttt{CudaMemcpy} functions for KV cache communication to prevent GPU blocking and enable computation and communication overlapping during transmission. The KV cache queues are maintained on the prefill replicas, and upon completion of a decoding round, the decode replicas retrieve KV caches from these queues, utilizing the GPU memory of the prefill replicas as queuing buffers. 

\textbf{KV cache compression technique.} As discussed in \S\ref{subsec:distributedllmdeployment}, prior works rely on high-bandwidth connections (i.e., NVLINK or InfiniBand) for transferring KV cache in phase splitting deployment, which is impractical in cloud service scenarios characterized by heterogeneous network conditions among GPUs. 
To reduce the KV cache communication cost, we borrow the idea of low-precision quantization from KIVI \cite{kivi2024} to quantize KV cache to fewer bits, so that the size of each element (i.e. \(\text{N}_{\text{bytes}} \) in \autoref{eq:kv_comm_cost}) is shrinked.
However, unlike existing works in the field of KV cache quantization \cite{kivi2024,kang2024gear}, 
\jyhh{our system does not retain low bitwidths when using the KV cache values for computation.}
Specifically, the KV cache values in the prefill replica are quantized and packed for communication, and then immediately unpacked and dequantized after they are received by the decode replica. \ffc{Thus, both the prefill and decode phases are conducted using the 16-bit KV cache values rather than the quantized ones.}
By this means, we can significantly reduce the KV cache communication volume, without harming the model quality.

To elaborate, we conduct a small testbed with LLaMA-7B over two A5000 GPUs, which featured an inter-communication bandwidth of \jyh{40 Gbps} --- significantly lower than that of InfiniBand and NVLink. 
%As shown in \autoref{fig:kvcache},
Quantizing the 16-bit elements to \jyh{4-bit} significantly reduces KV cache communication costs \jyh{from 16-30\% to 4-9\%} of the total end-to-end inference costs, drastically improving the performance of the system. 
\jyhh{Besides, we demonstrate the accuracy results of LLaMA-7B and LLaMA-13B models on CoQA, TruthfulQA and GSM8K tasks with both 16-bit and 4-bit KV cache precision levels. As we do not retain low bitwidths when using the KV cache values for computation, our experiments in \autoref{tab:acc} consistently show that the accuracy drop when using 4-bit precision compared to 16-bit precision remains below 2\% across all experimental scenarios, which confirms the validity of our approach. Due to the space constraint, we provide more evaluation results in \autoref{appendix:ppl}, including perplexity (PPL) and ROUGE-1/2/L on the WikiText2, PTB, and CBT datasets, and the end-to-end throughput comparisons between 16-bit and 4-bit precision.}


% \begin{table}[!t]
% \centering
% \caption{\jyh{LLaMA perplexity results on WikiText2, PTB and CBT datasets.}}
% \jyh{
% \begin{tabular}{c c c c}
% \hline
% \textbf{Dataset} &  & \textbf{LLaMA-7B} & \textbf{LLaMA-30B} \\ \hline
% \multirow{2}{*}{WikiText2} & 16-bit & 3.53 & 2.73 \\ 
% % \cline{2-4} 
%                                     & 4-bit  & 3.55 & 2.75 \\ \hline
% \multirow{2}{*}{PTB} & 16-bit & 7.46 & 6.49 \\ 
% % \cline{2-4} 
%                                        & 4-bit  & 7.42 & 6.55 \\ \hline
% \multirow{2}{*}{CBT} & 16-bit & 7.66 & 6.31 \\ 
% % \cline{2-4} 
%                                                & 4-bit  & 7.70  & 6.30  \\ \hline
% \end{tabular}
% }
% \label{tab:PPL}
% \end{table}

% \begin{table}[!t]
% \centering
% \caption{\jyh{LLaMA rouge results (16-bit versus 4-bit) on WikiText2, PTB and CBT datasets.}}
% \jyh{
% \begin{tabular}{c c c c}
% \hline
% \textbf{Dataset} &  & \textbf{LLaMA-7B} & \textbf{LLaMA-30B} \\ \hline
% \multirow{3}{*}{WikiText2} & ROUGE-1 & 0.962 & 0.942 \\ 
% % \cline{2-4} 
%                            & ROUGE-2 & 0.941 & 0.928 \\ 
                           
%                            % \cline{2-4} 
%                            & ROUGE-L & 0.955 & 0.941 \\ \hline
% \multirow{3}{*}{PTB}       & ROUGE-1 & 0.975 & 0.928 \\ 
% % \cline{2-4} 
%                            & ROUGE-2 & 0.950  & 0.911 \\ 
%                            % \cline{2-4} 
%                            & ROUGE-L & 0.971 & 0.928 \\ \hline
% \multirow{3}{*}{CBT}       & ROUGE-1 & 0.925 & 0.946 \\ 
% % \cline{2-4} 
%                            & ROUGE-2 & 0.912 & 0.931 \\ 
%                            % \cline{2-4} 
%                            & ROUGE-L & 0.925 & 0.937 \\ \hline
% \end{tabular}
% }
% \label{tab:rouge_performance}
% \end{table}

% We randomly generated 1000 input contexts to assess the accuracy of our system's inference process using compressed KV cache communication (2-bit) compared to baseline half-precision inference (16-bit). The experimental outcomes demonstrate that, for both the OPT and LLaMA models, the difference in perplexity (PPL) between these two precision levels is less than 5\%, which confirms the validity of our approach.
% And to evaluate the effectiveness of compressed communication, we conducted a LLaMA-7b KV cache communication test using two A5000 GPUs, which featured an inter-communication bandwidth of 3 GBps—significantly lower than that of InfiniBand and NVLink. As shown in \autoref{fig:kvcache}, compressing 16-bit to 2-bit communication significantly reduces KV cache communication costs from 9-30\% to 2-5\% of the total end-to-end inference costs, dramatically improved the performance of the system.