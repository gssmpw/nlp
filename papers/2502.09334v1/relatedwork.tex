\section{Related Work}
% \label{sec:relatedwork}
% \noindent \textbf{LLM serving systems.} 
% Recent research on LLM serving systems has concentrated on improving hardware efficiency through careful system optimizations \cite{zhao2024llmpq,li2023alpaserve,wu2023fast,kwon2023efficient,tensorrt_llm,liu2023deja,agrawal2023sarathi}. Among them, 
% % AlpaServe \cite{li2023alpaserve} suggests using model parallelism for scaling a single LLM beyond the memory limits of a single device. 
% vLLM \cite{kwon2023efficient} proposes page attention to improve the memory efficiency of the system. 
% % Deja Vu \cite{liu2023deja} reduces LLM inference latency by predicting and leveraging contextual sparsity on-the-fly.
% SARATHI \cite{agrawal2023sarathi} suggests chunk each prefill request and piggyback decoding requests to improve hardware utilization.
% Differently, \sys focuses more on optimizing LLM servicing on cloud heterogeneous resources.


% \noindent \textbf{Phase splitting in LLM serving.} Phase splitting has recently become a hot topic in optimizing LLM serving \cite{qin2024mooncake,jin2024p}. Splitwise and DistServe \cite{patel2023splitwise, zhong2024distserve} improve LLM inference efficiency by splitting prefill and decode phases across separate GPUs, optimizing hardware utilization and resource allocation. 
% TetriInfer \cite{hu2024inference} enhances LLM inference by partitioning prompts into fixed-size chunks, disaggregating prefill and decode replicas, and using a two-level scheduling algorithm for further optimization. 
% % ExeGPT \cite{oh2024exegpt} optimizes LLM inference by proposing novel scheduling strategies based on round-robin allocation and workload-aware allocation policies, decoupling the execution of encoding and decoding for efficient optimization of each phase. 
% % However, it is challenging to transition phase splitting directly to cloud services, mainly due to the diversity of resources and varying network conditions in cloud environments. 
% Our work is inspired by the phase splitting idea, and proposes the good matching between the heterogeneity in workload characteristics of different phases and the heterogeneity in cloud resources for high-performance LLM serving. 


% % 
% % \noindent \textbf{Cost-efficiency in LLM serving.} Recent research has investigated diverse approaches to lowering the cost of LLM serving. SpotServe \cite{miao2023spotserve} reduces LLM serving costs by using preemptible GPU instances, dynamically adapting parallelization for fluctuating workloads. HexGen \cite{jiang2024hexgen} reduces inference costs by deploying generative inference in a decentralized and heterogeneous setting, with asymmetric partitioning and advanced scheduling to enhance performance. 
% % Melange \cite{griggs2024melange} reduces LLM deployment costs by selecting the most cost-efficient GPUs based on model request size, request rate, and latency SLO, treating GPU selection as a cost-aware bin-packing problem to optimize resource allocation. 
% % % \sys adopts a similar concept to utilize heterogeneous GPUs to reduce the serving cost, allowing for cost-efficient cloud LLMs services.
% % Our work has a similar objective, and is the first effort that integrates phase splitting with the heterogeneous GPUs to provide high-performance cloud serving for LLMs.

% \jyhh{

% \noindent \textbf{Heterogeneous GPU Computing.} Recent research has investigated diverse approaches to deploy large models on heterogeneous GPU clusters. 
% % Metis \cite{um2024metis} develops a new search algorithm that automatically finds efficient parallelism plans for distributed training on heterogeneous GPUs. 
% HexGen \cite{jiang2024hexgen} proposes asymmetric partitioning and advanced scheduling to deploy generative inference in a decentralized and heterogeneous setting. 
% Helix \cite{mei2024helix} formulates the heterogeneous GPUs and network connections as a maxflow problem and adopts a mixed integer linear programming algorithm to discover highly optimized strategies to serve LLMs. 
% Our work has a similar objective, but is the first effort that integrates phase splitting with the heterogeneous GPUs to provide high-performance cloud serving for LLMs.
% }
% % There are many other works developed for heterogeneity-aware training \cite{miao2023sdpipe,miao2021heterogeneity,zhang2022mics,um2024metis,acc_par,whale,hap,amp_hetero_model_parallel}, however, our work differs from them in goals.

\vspace{-1em}