\section{Introduction}

\begin{figure}[!t]
    \includegraphics[width=0.47\textwidth]{asset/figure/fig_teaser.pdf}
    \vspace{-5pt}
    \caption{
        \label{fig:teaser}
        Illustration of the fact that perplexity does not serve as a reliable indicator of logical reasoning performance; and therefore, neither does probability.
        The distributions of the probabilities assigned to the ground-truth answer (i.e., soft accuracy; Y-axis) by Llama-3-70B are plotted against the perplexity of the corresponding example question (X-axis) and grouped by (a) modality, (b) argument forms, and (c) logic interpretation content.
        Each group consists of 20 randomly selected examples with other factors controlled.
    }
    \vspace{-5pt}
\end{figure}

Logical reasoning is a fundamental aspect of building AI systems for reliable decision-making \interalia{kautz-etal-1992-planning}---given a set of premises, an AI system should be able to deduce valid conclusions.
With the advent of large language models \citep[LLMs; ][\textit{inter alia}]{touvronLlamaOpenFoundation2023,jiangMistral7B2023a,metaLlamaHerdModels2024}, there has been a surge of interest in using these models to assist planning and decision-making \interalia{huang-etal-2022-language};
therefore, understanding the logical reasoning capabilities becomes crucial in understanding the reliability and potential of LLMs in planning.
While recent work has shown that LLMs exhibit decent performance on logical reasoning problems \interalia{liuLogiQAChallengeDataset2020a,ontanonLogicInferenceNewDatasaet2022,wanLogicAskerEvaluatingImproving2024},
there is still a lack of fine-grained understanding of the logical forms---among many argument forms presented in natural language \citep{shieber-1993-problem}, do LLMs perform equally well, or do they exhibit preferences for certain argument forms?
Do more complex components of logical forms, such as modalities, matter for LLM performance?

In this work, we investigate the logical reasoning capabilities of LLMs by assessing their performance on different logical forms.
We curate a dataset of natural language statements and questions based on several logical forms in both propositional and modal logic, which is designed to mirror reasoning in daily communication.
An example is shown in \cref{subsec:involved-logical-forms}.
We then conduct a series of controlled experiments to analyze the performance of a set of LLMs on the dataset.
Although our findings generally align with those by \citet{gonen-etal-2023-demystifying} and \citet{mccoyEmbersAutoregressionShow2024}, who suggest that LLMs excel on examples with high probability, our results indicate that logical form, including but not limited to modalities and argument forms, is a crucial complementary factor in predicting the performance of LLMs (\cref{fig:teaser}).
Additionally, with meaningful real-world interpretations, we find that:
\begin{enumerate}[topsep=2pt, itemsep=-3pt, parsep=0.5em, leftmargin=*]
    \item LLMs are still far from being perfect in atomic-level propositional and modal logic reasoning.
    \item LLMs prefer an affirmative answer under the modality of possibility, whereas they prefer a negative answer under the modality of necessity.
    \item In line with the recent results on categorical syllogisms \citep{eisape-etal-2024-systematic}, we verify on hypothetical and disjunctive syllogisms that LLMs achieve better performance on certain logical forms that humans perform well.
          However, some logical forms receive favor from LLMs, while the phenomena lack support from human intuition or human behavioral data.
\end{enumerate}
This paper is structured as follows.
After reviewing related work (\cref{sec:related}), we describe the dataset synthesis process (\cref{sec:dataset}).
We report the LLM reasoning results on our data (\cref{sec:experiment}) and compare them with human performance (\cref{sec:human}).
We conclude by discussing the implications of our results and the limitations(\cref{sec:discussion}).
