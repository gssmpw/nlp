\section{Conclusion and Discussion}
\label{sec:discussion}
We present an analysis of hypothetical and disjunctive syllogisms on propositional and modal logic and systematically analyze the LLM performance on the dataset.
Our analysis provides novel insights on explaining and predicting LLM performance: in addition to the perplexity or probability of the input text, the underlying logic forms play an important role in determining the performance of LLMs.
In addition, we compare the behaviors of LLMs and humans using the same data through human behavioral experiments.
We discuss the implications of our results as follows.

\vspace{2pt}
\noindent\textbf{Probability in language models.}
Probability and perplexity are often used as intrinsic evaluation metrics for language models.
While \citet{gonen-etal-2023-demystifying} and \citet{mccoyEmbersAutoregressionShow2024} show that probability and perplexity correlate well with LLM performance, literature in program synthesis with LLMs shows little correlation between probability and execution-based evaluation results \citep{li2022competition,shi-etal-2022-natural}.
This work does not necessarily contradict either line but rather provides complementary factors for analyzing LLM performance.

We argue that probability may have become an overloaded term in analyzing LLMs.
Low probability may be due to one or more of the following non-exhaustive reasons: (1) out-of-context content, (2) ungrammatical language, or (3) grammatical but semantically awkward content (cf. the mirror dataset in \cref{sec:perplexity}), (4) reasonable but rare content.
We hypothesize that the probability of language models may not be essentially able to capture all these nuanced differences, and call for encoding and decoding algorithms---such as \citet{meister-etal-2023-locally}---that can better decompose the probability into finer-grained and explainable components.

\vspace{2pt}
\noindent\textbf{Comparing humans and LLMs.}
What is our goal for building LLMs?
To achieve better performance on practical tasks or to build a more human-like model?
Our results, together with \citet{eisape-etal-2024-systematic}, suggest that these two goals may not be perfectly aligned by revealing a mixture of similarity and discrepancy between LLMs and humans---for example, while LLMs exhibit higher benchmark performance than humans on our dataset and show the same argument form preferences with humans (\cref{fig:emmeans-lm,fig:emmeans-human}), they also show systematic biases that we do not find significant in human reasoning (e.g., disfavoring the necessity modality, \cref{subsec:affirmation-bias}).
While there has been positive evidence of using LLMs as human models in psycholinguistic studies \interalia{misra-kim-2024-generating}, our results suggest executing such approaches cautiously.

\vspace{2pt}
\noindent\textbf{On the relation between modality and performance.}
Our results show that there is a significant difference in performance between necessity and possibility modalities, with the former much lower than the latter (\cref{tab:softacc-base}).
Part of the reason for this is that LLMs have a significant tendency to say ``No'' to the necessity modality (\cref{fig:affirmation-rejection}).

On the one hand, our results extend the conclusion of \citet{dentella-etal-2023-systematic} that LLMs generally respond positively---LLM behaviors may be significantly affected by finer-grained factors, including but not necessarily limited to the modality involved in the input.
On the other hand, while LLMs systematically tend to answer ``No'' to questions in necessity modality, we do not find related evidence in human experiments, which leads us to hypothesize that such rejection bias comes from either the model architecture or the training strategies, such as the reinforcement learning with human feedback \citep[RLHF;][]{ouyang-etal-2022-training} protocol.
We leave this as an open question for future research.

\vspace{2pt}
\noindent\textbf{Modal logic and theory of mind.}
Modality, in principle, encodes mental states and beliefs.
The reasoning of beliefs also resonates with the theory of mind \interalia{premackDoesChimpanzeeHave1978,baron-cohenDoesAutisticChild1985} and machine theory of mind \interalia{rabinowitzMachineTheoryMind2018, maHolisticLandscapeSituated2023}.
Following the effort by \citet{sileo-lernould-2023-mindgames} that uses epistemic modal logic to model the machine theory of mind, our work assesses the behaviors of LLMs on alethic modal logic, distantly revealing the future potential of LLMs in achieving the theory of mind.
