\section{Dataset}
\label{sec:dataset}
We curate a dataset of natural-language multi-choice questions to measure the logical inference performance of LLMs.
Starting from propositional and modal logical forms as templates (\cref{subsec:syn-logic}), we assign meanings (e.g., real-world interpretations) to each variable and translate templates into natural-language Yes/No questions (\cref{subsec:syn-natural-language}).
A subsidiary visualization of the process is shown in \cref{fig:syn-pipeline}.

\begin{figure*}[t]
  \input{asset/figure/data-pipeline.pdf_tex}
  \caption{
    The data synthesis pipeline: for each variable in logic forms (\cref{subsec:syn-logic}), we assign meanings to them to obtain the natural language question-answering pairs (\cref{subsec:syn-natural-language}).
  }
  \label{fig:syn-pipeline}
\end{figure*}


\subsection{Background: Propositional and Modal Logic}
\label{subsec:syn-logic}

Propositional logic studies the relation between propositions.
In this framework, each proposition is typically represented by a variable, and multiple propositions combine with logical connectives (e.g., $\lor$ and $\rightarrow$) to form compound propositions.

In propositional logic, a proposition can be evaluated as either true or false; however, this system can be overly simplistic when dealing with the complexity of real-world events.
Consider the statement \textit{Alice is not eating}, while it is true in a world where Alice is not eating, it may become false in a hypothetical \textit{possible world} where Alice is indeed eating.
This idea, known as possible world semantics \citep{kripkeCompletenessTheoremModal1959}, provides a framework for more nuanced statements about event possibilities, such as \textit{Alice may be eating} and \textit{Alice must be eating}.
The former statement can be understood as there \textit{exists} a possible world where Alice is eating, and the latter can be understood as in \textit{all} possible worlds, Alice is eating.\footnote{
    The possible world semantics, therefore, connects the notion of \textit{necessity} and \textit{possibility} to the universal and existential quantification ($\forall, \exists$) under first-order logic.
}
Normal modal logic \citep{kripkeSemanticalAnalysisModal1963} formalizes this idea and extends propositional logic to reason about event necessity and possibility.
In the Backus--Naur form, a normal modal logic system $\mathcal{L}$ can be written as
\begin{align}
    \mathcal{L} : \varphi
    \coloneqq\; &
    p \mid
    \lnot \varphi  \mid
    \Box \varphi  \mid
    \Diamond \varphi  \mid \nonumber          \\
                & \varphi  \lor \varphi  \mid
    \varphi \land \varphi  \mid
    \varphi \to \varphi,
    \label{eqn:modal-logic-system-definition}
\end{align}
where $p$ is a propositional variable that serves as an atom in $\mathcal{L}$, $\lnot$ is the negation operator, $\Box$ is the necessity operator (\textit{must}), $\Diamond$ is the possibility operator (\textit{may}), $\lor$ is logical disjunction (\textit{or}), $\land$ is logical conjunction (\textit{and}), and $\to$ is the logical implication operator (\textit{if...then}).
$\varphi$ denotes the syntactic category of a formula in $\mathcal{L}$.
The right-hand side of \cref{eqn:modal-logic-system-definition} describes all possible logical formulas under the system $\mathcal{L}$: for example, if $\varphi \in \mathcal{L}$, the rules imply that $\lnot \varphi \in \mathcal{L}$, $\Box \varphi \in \mathcal{L}$, and so on.
Following the convention in logic, the operator precedence is $\{\lnot, \Box, \Diamond\} \succ \{\lor, \land\} \succ \{\to\}$.

Indeed, the operators $\left(\lnot, \Box, \to\right)$ forms a functional complete set of operators under $\mathcal{L}$.
Suppose $\varphi$ and $\psi$ are variables that represent logical formulas.
The logical or ($\lor$) and logical and ($\land$) operators can be rewritten with logical not ($\lnot$) and logical implication ($\to$), as follows:
\noindent
\begin{align}
    \varphi \lor \psi  & \Leftrightarrow \lnot \varphi \to \psi, \label{eqn:def-lor}          \\
    \varphi \land \psi & \Leftrightarrow \lnot \left(\varphi \to \lnot \psi\right). \nonumber
\end{align}
%
Possibility operator $\Diamond$ can also be derived from the necessity operator. \begin{align}\Diamond \varphi &\Leftrightarrow \lnot \Box \lnot \varphi\end{align}

\vspace{3pt}
\noindent\textbf{Deduction and sequent.}
Given a formula set $\Gamma$ as premises, if a deduction to a conclusion $\varphi$ exists using axiom schemata and inference rules under the normal modal logic, we say the premises \textit{infer} the conclusion, and the deduction can be represented as a logic \textit{sequent} \(\Gamma \vdash \varphi\).
If a formula set $\Gamma$ do not infer the conclusion, we denote it as \(\Gamma \nvdash \varphi\) and call it a \textit{non-entailment}.


\subsection{Translating Logic to Natural Language}
\label{subsec:syn-natural-language}
An \textit{interpretation} maps propositional variables to concrete meanings.
For example, under the interpretation that $p$ is ``\textit{Jane is eating apples}'' and $q$ is ``\textit{John is eating oranges}'', the logical formula $p \lor q$ becomes ``\textit{Jane is eating apples or John is eating oranges}.''

Choices of interpretation, i.e., the concrete content of the sentence, should not affect the underlying logical reasoning process.
However, in natural-language utterances, reasoning can be influenced by various confounding factors.
Knowledge bias is a common pitfall.
For example, given the logical form \(\left\{\Box p \to \Box \lnot q, \Box p\right\} \vdash \Box \lnot q\), regardless of $p$'s interpretation, if we interpret $\lnot q\coloneqq\textit{``Cats are not animals''}$ then the conclusion will be ``\textit{It is certain that cats are not animals.}''
But common-sense knowledge suggests that ``\textit{it is certain that cats are animals}'' ($\Box q$), which logically contradicts the existing premise set.\footnote{
    This confounding factor affects the examples in Table 9 of \citet{hanFolioNaturalLanguage2024}.
}
Such bias will complicate logical reasoning \citep{lampinenLanguageModelsHumans2024} and should be avoided in data curation.
Besides, each variable should have independent interpretation, as detailed in \cref{subsec:logic-translate-strategy}.

After being assigned interpretations, each logical form is further articulated as a yes-no question on whether the conclusion can be inferred from the premises.
To mitigate the ambiguity in natural language, we design heuristic rules to translate logic forms into less ambiguous English, which are detailed in \cref{subsec:logic-translate-strategy}.
For the exact wordings we used, see \cref{tab:question-full} in Appendices.
If a valid deduction exists (\(\vdash\)) for the logical form,
the ground truth answer is \texttt{Yes}, otherwise \texttt{No}.
The answer is solely determined by the logical form and is independent of the interpretation.


\subsection{Involved Logical Forms}
\label{subsec:involved-logical-forms}

Translated logical forms can have varying degrees of naturalness.
For example, the \textit{necessitation rule} \(\left\{\varphi\right\} \vdash \Box \varphi \), which translates to ``\(\varphi\) is true; therefore, it is certain that \(\varphi\) is true,'' appears to be unnatural due to redundancy.
\footnote{Nevertheless, we report the experiment results on necessitation rule in \cref{subsec:extra-intro-modality}.}
Based on the relationship between $\lor$ and $\to$ in \cref{eqn:def-lor}, we use hypothetical and disjunctive syllogisms with four basic variants:
\begin{align}
    \left\{\varphi \lor \psi, \lnot \varphi\right\} & \vdash \psi, \tag{$\lor^\mathrm{L}$} \label{eqn:inf-rule-or-left}                      \\
    \{\lnot \varphi \to \psi, \lnot \varphi\}       & \vdash \psi, \tag{$\to^{\mathrm{L}}$; modus ponens}   \label{eqn:inf-rule-imply-left}  \\
    \{\varphi \lor \psi , \lnot \psi\}              & \vdash \varphi \tag{$\lor^\mathrm{R}$}, \label{eqn:inf-rule-or-right}                  \\
    \{\lnot \varphi \to \psi, \lnot \psi\}          & \vdash \varphi \tag{$\to^\mathrm{R}$; modus tollens}. \label{eqn:inf-rule-imply-right}
\end{align}
Despite the semantic similarity, these logical forms translate to different natural-language questions.
For example, taking the interpretations of $\varphi := \textit{Jane is watching a show}$ and $\psi := \textit{John is reading a book}$, $\lor^\mathrm{L}$ translates to
\begin{table}[H]
    \vspace{-5pt}
    \begin{tabular}{p{0.92\linewidth}}
        Consider the following statements:                          \\
        \textit{Jane is watching a show or John is reading a book.} \\
        \textit{Jane isn't watching a show.}                        \\
        Question: Based on these statements, can we infer that \textit{John is reading a book}?
    \end{tabular}
    \vspace{-10pt}
\end{table}
\noindent With the same interpretation, $\to^\mathrm{L}$'s translation of the first statement is \textit{If Jane isn't watching a show, then John is reading a book.}

According to the commutativity of disjunction operator, we group $\lor^\mathrm{L}$ and $\lor^\mathrm{R}$ together as \textit{disjunctive syllogism}, alongside two hypothetical syllogism groups, modus ponens ($\to^\mathrm{L}$) and modus tollens ($\to^\mathrm{R}$).
All the logical forms shown above are valid sequents with ground-truth answer \texttt{Yes}.
To balance the dataset, we introduce some logic fallacies that generate questions with ground-truth label \texttt{No}.
By flipping the second premises and the conclusions, we obtain the following fallacies:
\begin{align*}
    \left\{\varphi \lor \psi, \psi\right\} & \nvdash \lnot \varphi \tag{$\lor^\mathrm{L}_{\nvdash}$}, \label{eqn:fallacy-inf-rule-or-left}       \\
    \{\lnot \varphi \to \psi, \psi\}       & \nvdash \lnot \varphi, \tag{$\to^{\mathrm{L}}_{\nvdash}$}   \label{eqn:fallacy-inf-rule-imply-left} \\
    \{\varphi \lor \psi , \varphi\}        & \nvdash \lnot \psi \tag{$\lor^\mathrm{R}_{\nvdash}$}, \label{eqn:fallacy-inf-rule-or-right}         \\
    \{\lnot \varphi \to \psi, \varphi\}    & \nvdash \lnot \psi \tag{$\to^\mathrm{R}_{\nvdash}$}, \label{eqn:fallacy-inf-rule-imply-right}
\end{align*}
where $\lor^\mathrm{L}_{\nvdash}$ and $\lor^\mathrm{R}_{\nvdash}$ are grouped as \textit{affirming the disjunction}, $\to^{\mathrm{L}}_{\nvdash}$ and $\to^\mathrm{R}_{\nvdash}$ corresponds to \textit{affirming the consequent} and \textit{denying the antecedent}, respectively.
In our dataset, we require the formulas \(\varphi\) and \(\psi\) to the form of \(\modal p\) and \(\modal q\),  where \(p\) and \(q\) are propositional variables, each assigned with an interpretation.
Both variables are constrained under the same modality \(\modal\), which can be necessity ($\Box$), possibility ($\Diamond$) or no modality ($\varnothing$).
Pairing with four rules and theorem--fallacy variations, we have a total of $3\times 4\times 2=24$ forms.

\subsection{Involved Logic Interpretations}

For logic interpretations, we generate a set of verb phrases by prompting the CodeLlama 2 model \citep{roziereCodeLlamaOpen2024}, and select 204 of them manually.
and combine them with top-200 popular baby names in the US into subject-verb-object pairs,\footnote{\url{https://www.ssa.gov/oact/babynames/names.zip}} such as \(\left(\textit{Ray}, \textit{make}, \textit{a pizza}\right)\).
We randomly generate $1000$ interpretations with two pairs each.
The same set of interpretations is applied to variables $p, q$ in each logic sequent's natural langauge template.
In total, there are $24\times 1000 = 24000$ question, with samples shown in \cref{tab:question-full}.
