\section{Experiment}
\label{sec:experiment}

\subsection{Metrics and Investigated Models}
\label{sec:experiment-measure}

\begin{table*}[!t]
    \centering
    \include{asset/table/tab-softacc-base}
    \caption{
        Overall and break-down accuracies of different models, as well as their HuggingFace OpenLLM Leaderboard performance and relative ranking \citep{open-llm-leaderboard-v2}.
        Each argument form category denotes the union of the fine-grained categories specified in the superscripts and subscripts---for example, $\lor^{\mathrm{L}, \mathrm{R}}_{\vdash}$ denotes the entire disjunctive syllogism group.
        \textbf{Boldfaced} values indicate the row-wise maximum for each factor.
        Note that due to technical limitations of commercial LLMs, results from OpenAI-o1 \citep{openai-o1} and Gemini-1.5-pro \citep{team2024gemini} are greedy-decoding based evaluation on 2,000 random samples that serve as references, and are therefore not directly comparable to other probability-based evaluations.
        Human results are detailed in \cref{sec:human}.
    }
    \label{tab:softacc-base}
\end{table*}

\citet{huPromptingNotSubstitute2023} have suggested that the standard approach of greedily decoding yes-no strings \citep{dentella-etal-2023-systematic} may underestimate the competence of a language model; therefore, we adopt a probability-based metric to evaluate the model performance.
In our evaluation protocol, the predicted likelihood of the tokens \texttt{Yes} and \texttt{No}, conditioned on the prompt \(s\)---denoted as \(p(\texttt{Yes}\mid s)\) and \(p(\texttt{No}\mid s)\), respectively---serve as the soft labels for yes-no answers.
The soft accuracy $\hat p$ on the single example with ground-truth answer $y\in \{\texttt{Yes}, \texttt{No}\}$ is defined as the relative probability of $y$:
\noindent
$$
    \hat p = \frac{p(\texttt{No} \mid s)\mathbbm{1}[y = \texttt{No}] + p(\texttt{Yes} \mid s) \mathbbm{1}[y = \texttt{Yes}]}{p(\texttt{No} \mid s) + p(\texttt{Yes} \mid s)},
$$
\noindent
where $\mathbbm{1}[\cdot]$ is the indicator function that returns 1 if the condition is true and 0 otherwise.
This relative probability can also be viewed as the confidence score of the model on the ground-truth answer.
The soft accuracy \(\mathit{Acc}_\mathrm{soft}\) of a model on the entire dataset $\mathcal{D}$ is defined as the average soft accuracy over all examples,
\begin{align*}
    \mathit{Acc}_\textit{soft} = \frac{1}{\lvert\mathcal{D}\rvert} \sum_{i=1}^{\lvert \mathcal{D}\rvert} \hat p_i.
\end{align*}
\vspace{-10pt}

\noindent We use a zero-shot setting to investigate the general performance of the models' logical inference capabilities---while adding detailed instructions or few-shot demonstrations may increase the absolute performance, they are at the cost of introducing possibly undesired confounding factors or behaviors, such as simply copy-pasting the answers in the examples.

We evaluate on the following models with open-sourced weights: mistral-7b-v0.2 and -8x7b \citep{jiangMistral7B2023a, jiangMixtralExperts2024};
llama-2-7b, -13b and -70b \citep{touvronLlamaOpenFoundation2023};
3.1 version of llama-3-8b and -70b \citep{metaLlamaHerdModels2024};
yi-34b \citep{yiOpenFoundation2024};
phi-2 and phi-3-mini \citep{javaheripiPhi2023, abdinPhi3TechnicalReport2024}.\footnote{
    Our evaluation protocol technically requires the conditional probabilities of specified answers given a prompt, which are not supported by most commercial models; however, we report the greedy-decoding accuracy of these models on a sample subset for reference.
}

\subsection{Results: Performance w.r.t. Logical Forms}
We evaluate the aforementioned models with the probability-based protocol (\cref{tab:softacc-base}).
Generally, models that rank higher in the leaderboard also achieve higher soft accuracy on our dataset.
The break-down accuracies on modalities and argument forms reveal that:

\begin{enumerate}[leftmargin=*,itemsep=0pt,topsep=2pt]
    \item (Modality) All models consistently perform better on the possibility ($\Diamond$) than necessity ($\Box$) or plain propositional logic.
    \item (Argument Forms) The pattern is more diverse, yet most of the models struggle the most on modus tollens ($\to^\mathrm{R}_\vdash$) within logic sequents (i.e., questions with ground-truth answers \texttt{Yes}), and affirming the consequent ($\to^\mathrm{L}_\nvdash$) within fallacies.
\end{enumerate}


\subsubsection{Analysis on Logic Sequents}
To systematically analyze the effect on model performance of each factor of interest, as well as cross-validating the observations above, we fit a linear mixed-effects model \citep{raudenbush-2002-hierarchical} to the soft accuracy data on valid logic sequents (i.e. with ground truth of \texttt{Yes}) across different LLMs and logical forms,
\begin{align}
    \mathit{Acc}_\textit{soft} & \sim \textit{Modality} + \textit{ArgForm} + \textit{Perplexity} \nonumber \\
                               & + (1 + \textit{Perplexity} \mid \textit{LLM}),
    \label{eqn: mixed-effects}
\end{align}
with the linear fixed effects of (i.) modality, (ii.) argument form, and (iii.) input perplexity.
Individual probability, coupled with a constant term, is modeled as a random effect to account for potential model-specific biases.
Here, \textit{Perplexity} denotes the perplexity of the input text ($x_1x_2\dots x_N$), which is defined as the exponential of the token-wise average negative log-likelihood of the text given a specific language model:
\begin{align*}
    \textit{Perplexity} = \exp\left(-\frac{1}{N}\sum_{i=1}^{N}\log p(x_i \mid x_{<i})\right)
\end{align*}
The mixed-effects model yields a marginal $R^2$ of $0.342$ and a conditional $R^2$ of $0.543$, suggesting a reasonable predictive power.
The likelihood ratio test on the full regression model vs. the null regression model without each of the fixed effects yields a significant result ($p<0.001$), suggesting the importance of all these factors in determining the model performance.

\begin{figure}[t]
    \centering
    \vspace{-5pt}
    \includegraphics[
        width=0.98\columnwidth,
        keepaspectratio,
    ]{emmeans-lm.pdf}
    \vspace{-10pt}
    \caption{
        Estimated marginal means of logical form factors in the mixed-effects model of \cref{eqn: mixed-effects}, along with their 95\% confidence intervals.
    }
    \vspace{-5pt}
    \label{fig:emmeans-lm}
\end{figure}
\begin{table}[t]
    \centering \small
    \begin{tabular}{lc}
        \toprule
        \textbf{Hypothesis}                           & \textbf{$p$-value} \\
        \midrule
        $\text{propositional} < \text{may}$           & $<0.001$           \\
        $\text{must} < \text{propositional}$          & $<0.001$           \\
        $\text{must} < \text{may}$                    & $<0.001$           \\
        \midrule
        $\text{disjunctive} < \text{modus ponens}$    & $<0.001$           \\
        $\text{modus tollens} < \text{modus ponens}$  & $<0.001$           \\
        $\text{modus tollens} < \text{disjunctive}  $ & $<0.001$           \\
        \bottomrule
    \end{tabular}
    \caption{
        \label{tab:hypothesis-test}
        Hypothesis testing results on the effect of logical form factors on soft accuracy (\cref{fig:emmeans-lm}).
        \vspace{-5pt}
    }
\end{table}

\vspace{3pt}\noindent\textbf{Fixed effects.}
In line with \citet{gonen-etal-2023-demystifying} and \citet{mccoyEmbersAutoregressionShow2024}, we find a negative correlation between perplexity and soft accuracy ($p<0.001$); however, the correlation is weak ($\rho=-0.09$), which suggests the necessity of the complementary factors below in predicting LLM performance.

For different modalities and argument forms, we estimate their marginal means on soft accuracy (\cref{fig:emmeans-lm}) and perform pairwise hypothesis testing on the estimated coefficients (\cref{tab:hypothesis-test}).
The results generally align with the general observations on the full dataset.
The only exception is that modus ponens ($\to^\mathrm{L}$), instead of disjunctive syllogisms ($\lor$), appears to be the easiest argument form (i.e., the one with the highest soft accuracy) among all.

\begin{figure}[t]
    \centering
    \vspace{-10pt}
    \includegraphics[width=\linewidth]{asset/figure/ranef.pdf}
    \vspace{-15pt}
    \caption{
        \label{fig:random-effects}
        Illustration of per-model random effects on soft accuracy in the mixed-effects model of \cref{eqn: mixed-effects} with 99.9\% confidence intervals.
        (a) Mixed effects (i.e., the sum of fixed and random effects) of perplexity. (b) Intercept random effects (i.e., constant term per model on soft accuracy), with the model performance rank (\cref{tab:softacc-base}) annotated in parentheses.
    }
    \vspace{-10pt}
\end{figure}
\begin{figure*}[t]
    \centering
    \vspace{-5pt}
    \includegraphics[width=1\textwidth,keepaspectratio]{ppl-acc-formula-mean.pdf}
    \vspace{-15pt}
    \caption{
        \label{fig:corr}
        Correlation between mean perplexity and mean confidence score on each logic sequent.
        Each point represents an average over a group of 1000 prompts that share the same underlying logic sequent.
        Two connected dots share the same logic formula.
    }
    \vspace{-10pt}
\end{figure*}

\vspace{3pt}\noindent\textbf{Random effects.}
We analyze the per-LLM random effects on the soft accuracy (\cref{fig:random-effects}).
All the model-specific mixed effects of perplexity are negative, suggesting the negative correlation between perplexity and soft accuracy is consistent across models (\cref{fig:random-effects}a).
While the intercept random effects are not perfectly aligned with the model performance---since the perplexity random effects may introduce confounding factors---higher-ranked models generally tend to have higher intercept random effects (\cref{fig:random-effects}b), which cross-validates the general performance ranking.

\subsubsection{Extended Analysis on the Negative Perplexity--Performance Correlation}
\label{sec:perplexity}
We further investigate the negative correlation between perplexity and model performance through a controlled experiment: we create a mirror dataset of the same size, keeping all the logical formulas while interpreting them with nonsensical words.
For example, the formula $\Diamond(\varphi \lor \psi)$ may be interpreted as \textit{it's possible that Neva is \uline{balaring} a \uline{montery} or Lucille is \uline{sweeling prandates}}, where the underlined words and phrases are nonsensical.
Intuitively, the perplexity of the problems in this mirror dataset should be much higher than that of the primary dataset problems (\cref{sec:dataset}) under any reasonably trained language model.

We analyze the correlation between perplexity and model performance (\cref{fig:corr}).
As desired, the perplexity of problems with nonsensical words are indeed much higher than that of the primary dataset ($\approx 30$ vs. $\approx 10$).
The significant portion of horizontal and inclined lines in the figures again suggests that perplexity is not a reliable predictor of model performance.
Meanwhile, the overall parallelism of the lines echos our results that logical forms are important factors for such prediction.

\begin{figure}[t]
    \centering
    \vspace{-5pt}
    \includegraphics[width=0.95\columnwidth,keepaspectratio]{emmeans-lm-yes.pdf}
    \vspace{-15pt}
    \caption{
        \label{fig:affirmation-rejection}
        Estimated marginal means of the factors in the mixed-effects model of \cref{eqn: mixed-effects-yesno} with 95\% confidence intervals. Higher coefficients indicate a higher tendency to affirm the claim.
    }
    \vspace{-10pt}
\end{figure}
\subsubsection{The Affirmation Bias over Modalities}
\label{subsec:affirmation-bias}

One key argument of \citet{dentella-etal-2023-systematic} is that large language models exhibit a bias towards affirming the claim, i.e., answering \texttt{Yes} more frequently than \texttt{No}.
We investigate this phenomenon by fitting a mixed-effects model
\begin{align}
     & \frac{P(\texttt{Yes}\mid s)}{P(\texttt{Yes} \mid s) + P(\texttt{No} \mid s)} \sim  \textit{Modality} + \textit{ArgForm} \nonumber \\
     & + \textit{Perplexity} + (1 + \textit{Perplexity} \mid \textit{LLM}),
    \label{eqn: mixed-effects-yesno}
\end{align}
which has the same structure as \cref{eqn: mixed-effects}, except the dependent variable being the relative probability of answering \texttt{Yes} conditioned on input text $s$.

We present the estimated marginal means of the factors in the mixed-effects model (\cref{fig:affirmation-rejection}).
While our results confirm the affirmation bias on propositional logic, such bias is slightly less pronounced on the possibility modality ($\Diamond$, around 0.03), and the models even show a bias towards rejecting claims under the necessity modality ($\Box$).
