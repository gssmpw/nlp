\documentclass[conference,10pt,twoside,twocolumn]{IEEEtran}

\usepackage{cite}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{subfigure}
\usepackage{booktabs} 
\usepackage{multirow}
\usepackage{microtype}
\usepackage{balance}
\usepackage{xcolor}
\usepackage{cite}
\usepackage{microtype}


\input{macros/vmr-symbols-vecbold}
\input{macros/standard-macros}
\input{macros/defs}

\usepackage{framed}
\newtheorem{alg}{Algorithm}


\setlength{\textfloatsep}{8.5pt} 
\setlength{\floatsep}{8.5pt} 
\setlength{\dbltextfloatsep}{8.5pt} 


\IEEEoverridecommandlockouts
\allowdisplaybreaks 



\begin{document}

\title{Fixed-Throughput GRAND with FIFO Scheduling}

\author{\IEEEauthorblockN{Filippo Christen, Darja Nonaca, and Christoph Studer}\\[0.0cm]
\IEEEauthorblockA{\em Department of Information Technology and Electrical Engineering, ETH Zurich, Switzerland}
\textit{email: fchrist@student.ethz.ch, dnonaca@iis.ee.ethz.ch, and studer@ethz.ch}
\thanks{FC and DN contributed equally to this work. The work of DN and CS was supported in part by an ETH grant, and has received funding from the Swiss State Secretariat for Education, Research, and Innovation (SERI) under the SwissChips initiative. The authors thank J\'er\'emy Guichemerre and Chao Ji for valuable input and feedback.}
}

\maketitle

\begin{abstract}
Guessing random additive noise decoding (GRAND) is a code-agnostic decoding method that iteratively guesses the noise pattern affecting the received codeword. The number of noise sequences to test depends on the noise realization. Thus, GRAND exhibits random runtime which results in nondeterministic throughput. 
%
However, real-time systems must process the incoming data at a fixed rate, necessitating a fixed-throughput decoder in order to avoid losing data. 
%
We propose a first-in first-out (FIFO) scheduling architecture that enables a fixed throughput while improving the block error rate (BLER) compared to the common approach of imposing a maximum runtime constraint per received codeword. 
%
Moreover, we demonstrate that the \emph{average throughput} metric of GRAND-based hardware implementations typically provided in the literature can be misleading as one needs to operate at approximately one order of magnitude lower throughput to achieve the BLER of an unconstrained decoder.

\end{abstract}
%Guessing random additive noise decoding (GRAND) is a code-agnostic decoding method that iteratively guesses the noise pattern affecting the received codeword. The number of noise sequences to test depends on the noise realization. Thus, GRAND exhibits random runtime which results in nondeterministic throughput. However, real-time systems must process the incoming data at a fixed rate, necessitating a fixed-throughput decoder in order to avoid losing data. We propose a first-in first-out (FIFO) scheduling architecture that enables a fixed throughput while improving the block error rate (BLER) compared to the common approach of imposing a maximum runtime constraint per received codeword. Moreover, we demonstrate that the average throughput metric of GRAND-based hardware implementations typically provided in the literature can be misleading as one needs to operate at approximately one order of magnitude lower throughput to achieve the BLER of an unconstrained decoder.

\section{Introduction}

Guessing random additive noise decoding (GRAND)~\cite{KD19} is an emerging maximum-likelihood decoding method that iteratively guesses the corrupting noise sequences in descending order of their likelihood. 
%
The number of noise sequences to be tested depends on the instantaneous noise realization, which results in a nondeterministic (i.e., random) decoding throughput. However, hardware implementations for real-time applications must operate at a fixed throughput in order to sustain a fixed input data rate without losing data. 

The common approach to achieve constant throughput for GRAND-based hardware implementations is to impose a decoding time limit for each code block. This approach is called GRAND with abandonment (GRANDAB)~\cite{KD19}, which can be accomplished by imposing an upper limit on the number of noise sequences to be tested (assuming a constant number of tests is executed per time unit). The choice of this maximum decoding time limit results in a trade-off between achievable throughput and block error rate (BLER), as terminating GRAND early can result in additional decoding errors. 
%
To illustrate this trade-off, \fref{fig:intro_tp} shows the  \emph{average throughput} (black $\boldsymbol{\times}$) achieved at $1\%$ BLER by the hardware design D1 from~\cite{CJ23} of ordered reliability bits GRAND (ORBGRAND)~\cite{KD22}, assuming that no runtime constraints were imposed. 
%
By imposing a hard limit on the number of noise sequences to be tested with the same hardware design (red curve with $\blacktriangle$ markers) one can realize a tradeoff between \emph{constant throughput} and minimum $E_\text{b}/N_\text{0}$ (in dB) required to reach 1\% BLER. 
%
From \fref{fig:intro_tp}, we observe that a real-time system with constant throughput can achieve a 1\% BLER only at an $E_\text{b}/N_\text{0}$ of 0.57\,dB higher than the $E_\text{b}/N_\text{0}$ predicted by an unconstrained (i.e., without imposing any runtime limit) ORBGRAND implementation. Similarly, to achieve the same $E_\text{b}/N_\text{0}$ at 1\% BLER as that of unconstrained ORBGRAND, a real-time system would suffer a throughput loss of approximately $21\times$.






\subsection{Contributions}
In order to achieve constant throughput as required by real-time systems while improving the BLER, we propose a FIFO-scheduling architecture that consists of an input first-in first-out (FIFO) buffer, one decoder or an array of parallel decoders, and an output re-order buffer (ROB). The input FIFO buffer stores received codewords and passes those codewords to available decoders.
%
The decoders considered in this paper implement ORBGRAND~\cite{KD22}.
%
The output ROB enables the decoder to store the recently decoded codewords, ready to be fetched in the correct order by the subsequent hardware block. 
%
In contrast to na\"ive approaches that impose a maximum iteration limit per codeword, our FIFO-scheduling architecture achieves (i) constant throughput and (ii) improved BLER by leveraging GRAND's random runtime which allows for higher runtime limits per codeword (see the blue curve with \scalebox{1.2}{$\bullet$} markers in \fref{fig:intro_tp}) at the cost of increased decoding latency. 
%
In order to demonstrate the efficacy of the proposed FIFO-scheduling architecture, we provide BLER, throughput, dynamic power, and latency results based on simulations with a random linear code and the ORBGRAND hardware design D1 from~\cite{CJ23}.

\begin{figure}[tp]
  \centering
  \includegraphics[width=0.75\columnwidth]{./figures_tikz/pdf/intro_plot.pdf}
  \caption{\emph{Average} throughput of ORBGRAND to achieve $1\%$ BLER versus maximum \emph{constant} throughput achieved by hardware design D1 from~\cite{CJ23} without FIFO scheduling (in red) and with FIFO scheduling (in blue).
  }
\label{fig:intro_tp}
\end{figure}

\subsection{Relevant Prior Work}

Several hardware implementations of ORBGRAND have been proposed recently~\cite{SA22,CJ23,abbas23,riaz21,blanc24}. Besides reporting the average throughput, these works also report the worst-case throughput---the true bottleneck for real-time applications. The work in~\cite{condo22} emphasizes the importance of considering the worst-case throughput when evaluating a decoder's performance, by improving the order of generated noise patterns to reduce the worst-case number of noise queries. %
In contrast to such ORBGRAND implementations, we propose a FIFO-scheduling architecture that leverages the random runtime of GRAND to improve error-rate performance. Our architecture achieves constant throughput while improving the BLER (see~the blue line in \fref{fig:intro_tp}) compared na\"ive solutions with runtime limits per code block. 
%
The work in~\cite{li21} proposes a multicore low-density parity check (LDPC) decoder architecture to increase the decoding throughput. In contrast, our architecture combines FIFO-scheduling with multicore processing in order to deal with a constant arrival rate of codewords while delivering a constant throughput (at a cost of increased latency). Our architecture can be used in combination with virtually any iterative decoder implementation that exhibits random runtime, such as (ORB)GRAND-based decoders and LDPC decoders.

\subsection{Notation}

Boldface lowercase and uppercase letters represent column vectors and matrices, respectively. 
%
For a vector $\bmx$, the $i$th entry is denoted by $x_i = [\bmx]_i$.
%
We denote conditional probabilities by $\Prob(\cdot|\cdot)$. 
%
For a $(n,k)$ linear block code, $n$ is the code length and $k$ the number of information bits; the code rate is $\varrho=k/n$ and the codebook is $\mathcal{C}$. A field of sequences of length $n$ is denoted $\mathbb{F}^n$. Addition in Galois field $\mathbb{F}_{2}$ is denoted by $\oplus$. 


\section{Prerequisites}\label{sec:sysModel}

We consider forward error correction, where a block of $k$ information bits at the transmitter are mapped to an $n$-length codeword $\vecc \in \mathbb{F}_{2}^n$ selected from a codebook $\mathcal{C}$.
%
Each code bit~$c_i$, $i=1,\dots,n$, is transmitted over a memoryless additive white Gaussian noise (AWGN) channel with binary phase-shift keying (BPSK) symbols mapped according to $x_i=(-1)^{c_i}$.
%
The input-output relation of the AWGN channel is  $y_i = x_i + n_i$, where $y_i$ is the received signal and $n_i$ is real-valued zero-mean Gaussian noise with variance $\sigma^2$.
%
In what follows, we measure the signal-to-noise ratio in terms of $E_b/N_0=(\varrho\,\sigma^2)^{-1}$. 
%
We consider soft-input decoding, where the receiver first computes log-likelihood ratio (LLR) values for each coded bit as in~\cite{Cooper88}. 
%
Each block of LLR values $\lambda_i=\frac{2y_i}{\sigma^2}$, $i=1,\dots,n$, is then passed to, e.g., ORBGRAND~\cite{KD22} or our FIFO scheduling architecture, which generates estimates of the transmitted information bits. 


ORBGRAND uses blocks of LLR values to iteratively guess the noise sequence $\vecz$ affecting the hard-demodulated received sequence $\hat{\vecy} = \frac{1}{2}(1-\sign(\vecy))$. More specifically, one generates noise sequences $\vecz$ in descending order of their likelihood (this is where the LLR values are being used) and iteratively tests these noise sequences by computing $\hat{\vecc} = \hat{\vecy} \oplus \vecz$ until a valid codeword is found, i.e., until $\hat{\vecc}\in \mathcal{C}$.




\section{Fixed-Throughput FIFO Scheduling}



Inspired by the FIFO scheduling architecture put forward in~\cite{CS09} in the context of sphere decoding for multi-antenna wireless communication systems, we now propose our FIFO scheduling architecture for GRAND-based decoders.

\subsection{FIFO Scheduling Architecture Overview}
\label{subsec:structure}

\begin{figure}[tp]
  \centering
  \includegraphics[width=0.98\columnwidth]{./figures/Structure.pdf}
  \vspace{-0.2cm}
  \caption{FIFO scheduling architecture overview: a) data source, b) input buffer, c) distribution unit, d) array of decoders, e) collection unit, f) ROB, g) data sink, h) booking mechanism, i) early termination, and j) distribution control.}
  \label{fig:structure}
\end{figure}
%
\fref{fig:structure} illustrates the general structure of our FIFO scheduling architecture. The operating principle is as follows: %
%
a)--b) Blocks of LLR values associated with each codeword entering the system are inserted into the input FIFO buffer of size $F$ (i.e., LLR values for up to $F$ received codewords can be stored). 
%
c)--d) A distribution unit passes one block of LLR values to an available decoder (chosen among $D$ decoder instances). 
%
e)--f) Once a codeword has been decoded (or the decoder was terminated early), a collection unit passes the codeword to the output re-order buffer (ROB), which can store up to $R$ decoded codewords. 
%
g) The decoded data is released from the ROB when it is requested by the subsequent hardware block.

This FIFO scheduling architecture allows a new decoding process to start as soon as a decoder becomes available, thereby reducing the decoders' idle time compared to an inflexible architecture in which a decoder must wait a fixed number of clock cycles before starting to decode a new codeword or terminating the decoding process early as soon as a new codeblock arrives at its input. Thus, FIFO scheduling exploits the decoder's random runtime, enabling some codewords to be processed longer than the average runtime, which reduces the BLER while maintaining constant throughput. 


%
\subsection{ROB Booking Mechanism}\label{sec:booking}

Our FIFO scheduling architecture can be designed with an array of $D\geq 1$ decoders operating in parallel. Given the random decoding runtime of GRAND-based decoders, it is possible that the order of decoded codewords no longer corresponds to the same order as they arrived at the FIFO's input when multiple parallel decoders are employed.
%
In order to preserve the order of the received data, the data ordering within the input FIFO is mapped to the ROB through a process we call \textit{ROB booking mechanism}. Before leaving the input FIFO, each block of LLR values is assigned to the next available slot in the ROB in a sequential, queued manner. Each time a codeword is decoded, it is inserted into its reserved ROB slot, thereby retaining the original input data order at the output.

\begin{figure}[tp]
\centering
\subfigure[]{\includegraphics[scale =0.8]{./figures/ETconditions.pdf}}\\
\hspace{-0.2cm}
\subfigure[]{\includegraphics[scale =0.8]{./figures/ETconsequences.pdf}} 
\caption{(a) Conditions that trigger an early termination (E.~T.) and (b) control flow taking place when an early termination is triggered.}
\label{fig:ET}
\end{figure}


\subsection{Early Termination Mechanism}

In order to avoid data loss at the input, a free FIFO slot must be guaranteed whenever a new block of LLR values enters the input FIFO. Moreover, a processed codeword must be present within the ROB whenever it is expected at the output. We enforce both conditions by an \textit{early termination} mechanism, which terminates the longest-running decoding process early and forces the associated decoder to output its data. The conditions that lead to such an early termination and the control flow triggered by it are illustrated in \fref{fig:ET}. 




\section{Results}\label{sec:results}

\subsection{Simulation Setup}
\label{subsubsec:Simulation}


%
We simulate the performance of different configurations of our FIFO scheduling architecture by processing $10^5$ codewords encoded with a random (256,234) linear block code.
%
The simulated configurations vary in FIFO size $F$, ROB size $R$ with $F=R\in \{1, 2, 4\}$, and $D\in \{1,2\}$ parallel decoders. We assume the ORBGRAND decoder design D1 from \cite{CJ23}. %
%
We define two simulation parameters: (i) the \textit{arrival interval}~$I$, measured in clock cycles per codeword, indicating the number of clock cycles after which a new codeword arrives at the decoder's input, and (ii) the \textit{data parallelism}~$P$, indicating the number of data vectors processed concurrently by the FIFO scheduling system. 
%
In order to limit the parameter space, we recall from \fref{sec:booking} that the input FIFO releases data only when the ROB has available slots. Therefore, the maximum data parallelism is $P_{\text{max}} = F+R$. We also ensure $R \geq D$ to keep all of the decoder cores busy.
%

In our system, a codeword is expelled from the ROB in the same clock cycle in which a new codeword arrives in the FIFO, both occurring every $I$ clock cycles. The FIFO scheduling system needs to accumulate some codewords before starting to output them in order to be able to fetch new data from the input FIFO once decoding has finished. 
%
Thus, a fixed input-output latency is enforced on the system during the initial $P\cdot I$ clock cycles of operation. After this initialization period, codewords are expelled from the ROB every $I$ clock cycles, ensuring constant throughput at the input and output interfaces. 
%
As shown in \fref{fig:scheduling}, this strategy results in a constant input-to-output latency, while internally, the decoding times can vary to accommodate ORBGRAND's random decoding time. 
%

 \begin{figure}[tp]
  \centering
  \includegraphics[width=0.98\columnwidth]{./figures/scheduling.pdf}    
  \caption{FIFO scheduling with a single decoder, more than one FIFO/ROB slot ($F, R > 1$, $D=1$), $I=4$, and a constant input-output latency of $16$ clock cycles. Each row illustrates a hypothetical data flow through our architecture, showing a constant input-output latency for the $i$th and $(i+1)$th codeword~$\boldsymbol{\lambda}^\text{(i)}$ and $\boldsymbol{\lambda}^\text{(i+1)}$, respectively, despite the random decoding time.}
\label{fig:scheduling}
\end{figure}

\subsection{Performance Metrics}

We assess the efficacy of different architecture configurations by utilizing the algorithm and hardware specifications of the ORBGRAND decoder design D1 from~\cite{CJ23}. Concretely, we consider a clock frequency of $f=746$\,MHz, assume that the number of noise sequences tested by one decoder core per clock cycle is $\alpha=4$, and use a dynamic decoding power of $p_\text{dec} = 86.1\,\text{mW}$ per decoder. In our comparisons, we use the following performance metrics.



\begin{figure*}[tp]
\centering
\vspace{-0.25cm}
\hspace{-0.5cm}
\hfill
\subfigure[]{\includegraphics[width=0.32\linewidth]{./figures_tikz/pdf/PvsBLER_F4.pdf} \label{fig:bler_diff_P}}
\hfill
\hspace{0.15cm}
  \subfigure[]{\includegraphics[width=0.31\linewidth]{./figures_tikz/pdf/bler_F4_D1_R4.pdf} \label{fig:bler_diff_I}}
  \hfill
  \subfigure[]{\includegraphics[width=0.31\linewidth]{figures_tikz/pdf/plot_diff_configs.pdf}\label{fig:bler_diff_configs}}
\hfill
\vspace{-0.2cm}
\caption{Influence of data parallelism $P$ on BLER in a ($F=R=4$, $D=1$) FIFO scheduling architecture, $I = 10$ clock cycles (a). BLER at different arrival intervals $I$ in a (4,1) configuration and unconstrained (Unc.) ORBGRAND (b). BLER for different FIFO scheduling configurations ($F=R=P$, $D$) (c).}\label{fig:figs_bler}
\vspace{-0.5cm}
\end{figure*}

\begin{figure*}[tp]
\centering
	  \subfigure[]{\includegraphics[width=0.31\linewidth]{./figures_tikz/pdf/tp_plot.pdf} \label{fig:tp}}
\hfill
	\subfigure[]{\includegraphics[width=0.32\linewidth]{figures_tikz/pdf/plot_lat_vs_tp.pdf}\label{fig:latency}}
\hfill
	\subfigure[]{\includegraphics[width=0.3\textwidth]{figures_tikz/pdf/plot_power_vs_tp.pdf}\label{fig:power}}
\vspace{-0.2cm}  
\caption{Maximum constant throughput achieved by different FIFO scheduling configurations ($F=R=P$, $D$) versus $E_\text{b}/N_\text{0}$ [dB] loss from 1\%\,BLER operating point achieved by an unconstrained ORBGRAND decoder (a). Latency (b) and dynamic power (c) versus throughput achieved by different FIFO scheduling configurations for 0.1 dB and 0.05 dB loss from 1\%\,BLER operating point achieved by an unconstrained ORBGRAND decoder.}
\end{figure*}
%


\subsubsection{Error-Rate Performance}
%
The BLER achieved by each configuration of our FIFO scheduling architecture is computed via $10^5$ Monte--Carlo trials per $E_\text{b}/N_\text{0}$ value.
%
The 1\%\,BLER operating point
is defined as the minimum $E_\text{b}/N_\text{0}$ (in decibels) required to achieve $1\%$ BLER.\footnote{Note that  $1\%$ BLER is conservative considering that the 3GPP 5G NR standard deems $2\%$ BLER acceptable~\cite[Table 8.1.1-1]{3gpp38133}.}
%
The $E_\text{b}/N_\text{0}$ loss is defined as the difference (in decibels) between the 1\%\,BLER operating point achieved by ORBGRAND without imposing a maximum iteration limit and the 1\%\,BLER operating point achieved by our FIFO scheduling architecture with a constant throughput.

\subsubsection{Throughput}
%
The throughput $\theta$ of our FIFO scheduling architecture is computed as $\theta = {kf}/{I}$\,bit/s.
%
The average throughput $\theta_{\text{avg}}$ refers to the throughput of an ORBGRAND decoder without runtime constraints and is computed as $\theta_\text{avg} = {\alpha k f}/{\beta}$\,bit/s, where $\beta$ is the average number of required iterations to decode a data vector at a given $E_\text{b}/N_\text{0}$. 

\subsubsection{Latency}
The constant input-output latency $\ell$ is an integer multiple of the arrival interval $I$, given by $\ell=(PI)/f$ seconds. 

\subsubsection{Dynamic Power}
%
To estimate the decoding power consumption $p$, we count the number of clock cycles $\delta_\text{act}(\mathrm{D_i})$ during which each decoder $\mathrm{D_i}$, $i=1,\ldots,D$ is actively decoding. We then compute the total number of clock cycles $\delta_\text{act,tot}=\sum_{i=1}^{D} \delta_\text{act}(\mathrm{D_i})$ during which the decoders are busy. 
%
We define the decoder's activity factor as $\eta_\text{act} = \delta_\text{act,tot}/(P I + I (10^5 - 1))$,  where the denominator is the number of clock cycles necessary to process all of the inputs. 
%
Finally, we estimate the dynamic decoding power as $p_\text{dyn} = \eta_\text{act} p_\text{dec}$ by ignoring control and FIFO/ROB power consumption.
%
\subsection{Results and Discussion}


We now discuss the design trade-offs in terms of BLER, throughput, latency, and power of different configurations of FIFO scheduling architecture. 

%
\fref{fig:bler_diff_P} shows the BLER for different data parallelisms $P$ and suggests the existence of an optimal $P_\text{opt}$ that minimizes the BLER for a given configuration of the FIFO scheduling architecture. Low $P$ causes frequent early terminations due to the ROB lacking the required processed codewords when requested;  high $P$ leads to input FIFO overflows, which also triggers early terminations. Thus, $P_\text{opt}$ must be chosen carefully.

%
\fref{fig:bler_diff_I} shows that increasing the arrival interval $I$ improves the BLER as it allows more decoding time per codeword. However, the BLER improvements diminish with larger $I$, converging to the BLER achieved by unconstrained ORBGRAND  at approximately $I = 10^5$. Thus, to reach the same BLER achieved by an unconstrained ORBGRAND design, a real-time system using a D1 instance from~\cite{CJ23} would need to tolerate latencies on the order of microseconds, which is impractical. 


\fref{fig:bler_diff_configs} compares the BLER for different FIFO scheduling architecture configurations with fixed arrival intervals $I = 1$ and $I = 10$. 
%
For $I = 1$, adding a second decoder improves the BLER up to $0.3$\,dB as it lowers the occurrence of a full input FIFO. For $I = 10$, increasing buffer sizes yields up to $0.2$\,dB improvement, while a second decoder adds only minor gains as larger FIFO/ROB buffer sizes ensure sufficient processing time for a single decoder. 
%
Therefore, when the arrival interval is higher than the average number of iterations needed per codeword, it is more beneficial to increase the FIFO/ROB buffer sizes rather than adding decoder instances. 
%

\fref{fig:tp} shows a constant throughput achievable by different FIFO scheduling architectures ($F=R=P$, $D$) and compares them to the ORBGRAND's average throughput at the 1\% BLER operating point (black $\boldsymbol{\times}$). This comparison is in terms of the $E_\text{b}/N_\text{0}$ loss, which is the difference in decibels from the $\boldsymbol{\times}$ marker and the 1\% BLER operating point achievable by a given FIFO scheduling architecture. 
%
The simplest configuration (1,1), which is equivalent to the na\"ive maximum runtime limit per codeword, incurs a $0.6$\,dB loss; larger buffer sizes in the (4,1) configuration (reported also in \fref{fig:intro_tp}), reduce the loss to $0.35$\,dB. 
%
Moreover, to reach $0$\,dB loss with a (4,1) configuration, a real-time system would need to operate at a throughput that is $21\times$ lower than the average throughput of an unconstrained~decoder.

\fref{fig:latency} highlights the cost to pay in terms of input-output latency to achieve a constant target throughput with a small $E_\text{b}/N_\text{0}$ loss.
%
\fref{fig:power} shows the dynamic power of one or several decoder(s). We see that at higher throughputs, both the decoding latency and power consumption increase, as both quantities are proportional to the number of decoding iterations.

\section{Conclusions}

GRAND-based decoder implementations typically exhibit random runtime, but real-world communication systems must provide constant throughput and latency to avoid data loss.
%
In order to address this issue, we have proposed a novel FIFO scheduling architecture that guarantees constant throughput and latency, while outperforming a naïve approach that imposes a maximum decoding time per codeword. While our architecture increases latency and hardware overhead, our results reveal that increasing the input/output buffer sizes improves BLER performance for a wide throughput range, whereas increasing the number of parallel decoders instances only provides notable BLER gains at very high throughput. 
%
Furthermore, we have shown that achieving 1\% BLER under a constant-throughput requirement results in approximately one order of magnitude lower throughput compared to ORBGRAND’s average throughput, which is typically reported in the literature. 



\bibliographystyle{IEEEtran}
\bibliography{bib/VIPabbrv,bib/confs-jrnls,bib/publishers,bib/VIP_190331, bib/24iscas_bib}

\end{document}
