\section{Related Work}
Monocular depth estimation is pivotal in applications such as robotic navigation, autonomous driving, and virtual reality. Traditional methods based on geometric principles, like Scale-Invariant Feature Transform (SIFT) and Conditional Random Fields (CRFs), often struggle with feature matching and computational demands in complex scenes. The advent of deep learning has significantly enhanced accuracy and efficiency, establishing learning-based approaches as the mainstream in monocular depth estimation \cite{c2}. Among these, generative models have shown substantial potential in depth prediction tasks \cite{c5}\cite{c8}\cite{c11}\cite{c19}. Below we discuss related work that have made use of generative models in depth estimation. 

\subsection{Generative Adversarial Networks}
Generative Adversarial Networks (GANs) \cite{c20} have been effectively applied to monocular depth estimation, particularly in enhancing fine-grained details. Jung et al. \cite{c5} utilized GAN-based generators to produce more detailed and realistic depth maps, improving depth estimation quality. Building upon this, Godard et al. introduced the MonoDepth2 model \cite{c19}, which employs a self-supervised learning framework for depth estimation. Instead of using a generator-discriminator architecture, MonoDepth2 relies on photometric consistency between consecutive frames or stereo pairs, enabling the model to learn depth without ground truth labels. The model introduces improvements such as minimum reprojection loss to handle occlusions and auto-masking to ignore invalid pixels, significantly advancing self-supervised depth estimation performance on datasets like KITTI. However, GANs often encounter training difficulties, such as mode collapse and convergence issues \cite{c6, c21}, which can limit their practical applicability. Additionally, GANs face challenges in generating high-resolution images, often struggling to maintain image quality and detail as the resolution increases \cite{c22}\cite{c23}.

\subsection{Diffusion Models}
Diffusion models have emerged as a promising alternative to GANs, offering improved training stability by iteratively adding and removing Gaussian noise through a probabilistic process modeled as a Markov Chain \cite{c12}. This approach leverages the gradual introduction of noise during the forward process and its removal during the reverse process, allowing for more stable training compared to GANs \cite{c7}. While they circumvent common GAN issues, diffusion models face challenges with slow sampling times due to their iterative nature. Nevertheless, they have achieved state-of-the-art results in depth estimation when trained on large datasets like Waymo and KITTI, notably with DepthGen \cite{c8}. DepthGen incorporates self-supervised pretraining, where the model learns general image structures from tasks like colorization, and multi-task training to enhance depth estimation accuracy. This combination allows the model to generalize well across different datasets. To address computational demands, Rombach et al. proposed applying diffusion processes in a lower-dimensional latent space, significantly enhancing efficiency without compromising accuracy \cite{c13}. Building on this approach, Zhao et al. introduced the Visual Perception with Diffusion (VPD) framework \cite{c14}, extending Stable Diffusion to various vision tasks. VPD integrates a denoising UNet and a task-specific decoder, achieving state-of-the-art performance on the NYUv2 depth estimation dataset and the RefCOCO image segmentation dataset, demonstrating robust results across diverse tasks.


\subsection{Semantic Embeddings in Diffusion Models}
Semantic encoders play a important role in providing contextual information for depth estimation within diffusion models. The Contrastive Language-Image Pre-training (CLIP) model \cite{c15} has been leveraged to supply rich semantic guidance, as seen in models like VPD, which achieved leading results on the NYUv2 dataset. However, reliance on text prompts can be limiting in complex scenes where generating accurate textual descriptions is challenging. To overcome this limitation, researchers have explored alternatives. BLIP-2 \cite{c24} generates image captions to serve as scene descriptions, aiding semantic understanding. Some other studies treat semantic extraction as a classification task, utilizing pretrained image encoders like Vision Transformer (ViT) \cite{c25} alongside learnable embedding vectors. For more direct and flexible applications, models such as SeeCoder offer a promising solution by extracting semantic features directly from visual inputs without relying on text-image alignment or classification tasks. Similar to CLIP, SeeCoder is trained on large datasets like Laion2B-en \cite{c26} and COYO-700M \cite{c27}, and its Transformer-based architecture enables it to capture rich, multi-scale semantic features, making it highly effective for various visual understanding tasks.


\subsection{Spatial Enhancement Techniques}
Enhancing spatial structure understanding is critical in depth estimation, especially for outdoor scenes where depth maps are usually sparse. Techniques like the Convolutional Block Attention Module (CBAM) \cite{c28} and dilated convolutions \cite{c29} have been proposed to improve spatial awareness. CBAM enhances feature representations by sequentially applying channel and spatial attention mechanisms, allowing the network to focus on informative features and relevant spatial locations. Meanwhile, dilated convolutions expand the receptive field without increasing the number of parameters, enabling the capture of multi-scale contextual information crucial for depth estimation. Both methods augment the model's ability to capture spatial dependencies without significantly increasing computational complexity, making them effective plug-and-play solutions for enhancing depth estimation accuracy.