
\subsection{From Bayesian to Frequentist Inference}\label{sec:bayes}

A natural choice for the mixing distribution is the Bayesian posterior, which establishes a fundamental connection between frequentist confidence estimation and Bayesian inference. To explore this relationship, we first formally define the Bayesian inference model.
\begin{assumption}[Bayesian Inference]\label{a:bayes}
    In the Bayesian inference model, the learner defines a prior distribution $\mu_0 \in \sP(\Theta)$ over model parameters (independent of the data), and predicts using the posterior distribution $\mu_t(\theta) \propto \prod_{s=1}^{t-1} p_s(y_s|\theta) \mu_0(\theta)$.
\end{assumption}
The main result of this section establishes that if the mixing distributions are computed according to Bayes' rule, then prior likelihood mixing (\cref{result:prior_mixing}) and sequential likelihood mixing (\cref{result:posterior_mixing}) are equivalent. A further application of Bayes rule shows that any (realizable) Bayesian model can be turned into a $(1-\delta)$-confidence sequence by comparing the log posterior probability $\log \mu_t(\theta)$ to the log prior probability $\log \mu_0(\theta)$. This is known as \emph{prior-posterior ratio confidence set} \citep{waudby2020confidence}: 
\begin{align*}
    C_t =  \left\{ \theta \in \Theta: - \log \mu_t(\theta) \leq  \log \frac{1}{\delta} - \log \mu_0(\theta) \right\} \,.
\end{align*}
The equivalence result is foreshadowed in the works by \citet{waudby2020confidence} and \citet{neiswanger2021uncertainty}, who establish the posterior-ratio confidence set and the connection to the marginal likelihood. The explicit equivalence to the sequential mixing framework, however seems to be absent in prior works, and is formally given in \cref{result:mixing-equivalence} below. 
%As a consequence, the concentration bounds for sequential linear regression by \citet{neiswanger2021uncertainty,flynn2024improved,flynn2024tighter} and earlier work by \cite{abbasi2011improved} are essentially equivalent, as we illustrate below. \todoj{maybe move below Lemma 6, so that 'below' makes sense}

\begin{theorem}[Mixing Equivalence]\label{result:mixing-equivalence} If the mixing distributions are chosen according to Bayes' rule, prior likelihood mixing (\cref{result:prior_mixing}) and sequential mixing (\cref{result:posterior_mixing}) are equivalent.
\end{theorem}
\begin{proof}
   The result follows by applying Bayes' rule recursively to show the following equality, $\sum_{s=1}^t \log \int p_s(y_s|\nu) d\mu_{s-1}(\nu) = \log \int \prod_{s=1}^t p_s(y_s|\nu) d\mu_{0}(\nu)$.
    % \begin{align*}
    %     \sum_{s=1}^t \log \int p_s(y_s|\nu) d\mu_{s-1}(\nu) = \log \int \prod_{s=1}^t p_s(y_s|\nu) d\mu_{0}(\nu) \,. 
    % \end{align*}
\end{proof}

The surprising consequence is, that within the Bayesian inference model, sequential mixing provides no statistical advantage compared to averaging the likelihood over the prior. Less surprisingly though, Bayes' rule can be understood as an incremental update rule to compute the marginal likelihood. In this sense, the equivalence can be re-stated as recovering prior mixing (\cref{result:prior_mixing}) as a special case of sequential mixing (\cref{result:posterior_mixing}). However, note that for mixing distributions outside the Bayesian model, the equivalence does not hold in general, leaving the possibility to find non-Bayesian mixing distributions that achieve faster convergence. We come back to this idea in \cref{sec:oco}.

Next, we state a second implication of Bayes' rule, the prior-posterior ratio confidence set. 
\begin{lemma}[Prior-Posterior Ratio Confidence Set \citep{waudby2020confidence}] \label{lem:posterior_ratio_confidence_set}\\
    For any realizable Bayesian model, the following defines a $(1-\delta)$-confidence sequence:
    % The confidence sequence $C_t = \{\theta \in \Theta: L_t(\theta) \leq \log \frac{1}{\delta} - \log \int \prod_{s=1}^t p_s( y_s|\nu) d\mu_0(\nu)  \}$ can be equivalently written as follows:
\begin{align*}
    C_t &=  \left\{ \theta \in \Theta: - \log \mu_t(\theta) \leq  \log \frac{1}{\delta} - \log \mu_0(\theta) \right\} \,.
\end{align*}
Moreover, the confidence set is equivalent to the construction in \cref{result:prior_mixing,result:posterior_mixing}.
\end{lemma}
\begin{proof}
    Note that $\log \mu_t(\theta) = \log \mu_0(\theta)  + L_t(\theta) - \log \int \prod_{s=1}^t p_s(y_s|\nu) d\mu_{0}(\nu)$ holds for all $\theta \in \Theta$ by Bayes' rule. Substituting the equality into \cref{result:prior_mixing} gives the result.
\end{proof}
The remarkable conclusion is that any realizable Bayesian model can be turned into a frequentist confidence set by thresholding the log posterior probability relative to the log prior probability. As a caveat, it is tempting to think of $C_t$ as a Bayesian credible region, however, the posterior credible probability $\mu_{t-1}(C_t)$ is typically not $1-\delta$. Further, the confidence set, by construction, never rejects parameters in the null set of the prior distribution, unlike in classical Bayesian inference. In any case, a sensible choice is $\Theta = \supp(\mu_0)$, as long as the realizability condition (\cref{a:realizability}) is satisfied, that is, $\theta^* \in \Theta$ defines the true likelihood of the data. For an application of the prior-posterior confidence set to sequential sampling without replacement, we refer to \citet{waudby2020confidence}.

As a consequence of the prior-posterior ratio confidence set and the mixing equivalence, the confidence sets for sequential linear regression by \citet{neiswanger2021uncertainty,flynn2024improved,flynn2024tighter} and earlier work by \cite{abbasi2011improved} are essentially equivalent, as we demonstrate below. Moreover, a lower bound by \citet{lattimore2020bandit} shows that the construction is tight without further assumptions on the data generation distribution. 

\paragraph{Sequential Linear Regression} 
To illustrate the utility of the Bayesian perspective, we consider sequential linear regression with a Gaussian prior and likelihood. To preempt any concerns, we remark that the Gaussian assumption can be relaxed to sub-Gaussian distributions, as we explain in \cref{sec:subgaussian}. Formally, let $\theta^* \in \Theta = \bR^d$, with multivariate Gaussian prior $\cN(\theta_0, V_0^{-1})$ centered at $\theta_0 \in \bR^d$ and prior precision matrix $V_0 \in \bR^{d \times d}$, where commonly $V_0 = \lambda \eye_d \in \bR^{d\times d}$ for a regularizer $\lambda > 0$. The observation likelihood is Gaussian,  $y_t \sim \cN(x_t^\top\theta^*, \sigma^2)$ for a feature vector $x_t \in \bR^d$ and known observation variance $\sigma^2 > 0$. The Gaussian posterior is $\mu_t = \cN(\hat \theta_t^\RLS, V_t^{-1})$, where $\hat \theta_t^\RLS$ is the regularized least squares (RLS) estimate,
\begin{align*}
\hat \theta_t^\RLS = \argmin_{\theta \in \bR^d} \frac{1}{2 \sigma^2} \sum_{s=1}^t \big(\ip{x_s, \theta} - y_s\big)^2 + \frac{1}{2} \|\theta - \theta_0\|_{V_0}^2\,.
\end{align*}
Here, $V_t = \frac{1}{\sigma^2}\sum_{s=1}^t x_s x_s^\top + V_0$ is the posterior precision matrix, and we use the notation $\|\nu\|_A^2 = \nu^\top A \nu$ for $\nu \in \bR^d$ and $A \in \bR^{d\times d}$. The prior and posterior densities are explicitly given as follows:
\begin{align*}
    \mu_0(\theta) &= (2 \pi)^{-2/k} (\det V_0)^{1/2} \exp\big(- \tfrac{1}{2}\|\theta - \theta_0\|_{V_0}^2 \big) \\
    \mu_t(\theta) &= (2 \pi)^{-2/k} (\det V_t)^{1/2} \exp\big(- \tfrac{1}{2}\|\theta - \hat \theta_t^\RLS\|_{V_t}^2 \big)
\end{align*}
Applying \cref{lem:posterior_ratio_confidence_set} with the Gaussian posterior, we get the following $(1-\delta)$-confidence sequence:
\begin{align*}
    C_t^\RLS = \left\{ \theta \in \bR^d : \frac{1}{2}\|\theta - \hat \theta_t^\RLS\|_{V_t}^2 \leq \log \frac{1}{\delta} + \frac{1}{2}\log \det V_t - \frac{1}{2}\log \det V_0 + \frac{1}{2}\|\theta  - \theta_0\|_{V_0}^2 \right\}\,.
\end{align*}
An important feature of the bound is that it scales with the \emph{effective dimension} or \emph{total information gain} $\gamma_t = \frac{1}{2}\log \det V_t - \frac{1}{2}\log \det V_0$ of the data \citep[c.f.~][]{huang2021short}, which can be much smaller than the parameter dimension $d$ \citep{srinivas2009gaussian}. 
Note also that the confidence set does \emph{not} require a known bound on the norm $\|\theta^*\|_2 \leq S$, which is required in all prior work that we are aware of. If such a bound is available, a direct approach is to define the Gaussian prior and posterior directly over the restricted set $\cB_S = \{\theta \in \bR^d : \|\theta\|^2 \leq S\}$. However, in this case, the normalization constant is not easily computed in closed form. Instead, we intersect $C_t^\RLS$ with the norm ball $\cB_S$. Relaxing the confidence set further, and choosing $V_0 = \lambda \eye_d$ and $\theta_0 = 0$, we eventually arrive at
\begin{align*}
    % C_t &\subset \{ \theta \in \Theta : \frac{1}{2 \sigma^2} \|\theta - \hat \theta_t\|_{V_t}^2 \leq \log \frac{1}{\delta} + \log \det V_t - \log \det V_0 + S^2 \}\nonumber\\
    C_t^\RLS \cap \cB_S \subset \left\{ \theta \in \bR^d : \frac{1}{2} \|\theta - \hat \theta_t^\RLS\|_{V_t}^2 \leq \log \frac{1}{\delta} + \frac{1}{2}\log \det V_t - \frac{d}{2}\log \lambda+ \frac{\lambda}{2}S^2 \right\} \,.
\end{align*}
The last line essentially recovers the influential result by \citet{abbasi2011improved}, albeit avoiding a lower-order cross-term, improving the bound by up to a factor of two. 
The proof of \citet{abbasi2011improved} uses the method of mixtures, but mixing the noise martingale $S_t = \sum_{s=1}^t \epsilon_s x_t$ over a centered prior, instead of directly mixing the likelihood ratio. 
More recent work by \cite{flynn2024improved} achieves the tighter result using a similar sequential mixing idea, however, the likelihood framework and connection to Bayesian inference is not mentioned. A direct extension is to heteroscedastic noise, $y_t \sim \cN(x_t^\top\theta^*, \sigma_t^2)$ with known variance $\sigma_t^2$ \citep[c.f.,][]{kirschner2018information}. Another, more involved extension is to unknown mean and variance \cite[c.f.,][]{chowdhury2023bregman}. \looseness=-1

\paragraph{Gaussian Process Regression}
We remark that the confidence set for sequential linear regression can be equivalently stated for non-parametric Gaussian processes regression in infinite-dimensional kernel Hilbert spaces (RKHS) using the `kernel trick'. Our derivation improves (up to a factor of two) the results by \cite{abbasi2012thesis,chowdhury2017kernelized,whitehouse2023sublinear} and recovers more recent results by \cite{neiswanger2021uncertainty,flynn2024tighter}, who do not state the equivalence.

% In particular, we can restate the above confidence set $C_t$ on a separable RKHS space, and project the confidence set onto a specific evaluation $x$, via the reproducing kernel operation $f(x) = f^\top k(\cdot,x)$, to arrive at
% \[  C_t(x) = \{ f(x) | |f(x) - \hat{f}_t(x)| \leq  \} \]
% \todoj{make Gaussian processes explicit}


% Lastly, we remark that discussion extends to the more general class of sub-Gaussian likelihoods, which we discuss in \cref{sec:subgaussian}. 