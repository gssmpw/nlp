
\subsection{Variational Confidence Sets}\label{sec:elbo}
While the confidence set construction and its relation to Bayesian inference is universal and holds for any prior and family of likelihood functions, the price to pay is that computing the marginal likelihood, or equivalently, the posterior distribution, is intractable in general. Fortunately, approximate inference methods have been well studied in the field of Bayesian inference. 
Our starting point is a variational inequality for the marginal likelihood $\int \prod_{s=1}^t p_s(y_s|\theta)d\mu_0(\theta) = \int \exp(- L_t(\theta)) d\mu_0(\theta)$ and the Kullback-Leibner (KL) divergence, often attributed to \citet{donsker1983asymptotic}. 
\begin{lemma}[Variational Inequality]\label{lemma:variational-kl}
For any two distributions $\mu,\rho \in \sP(\Theta)$ and any measurable function $g : \Theta \rightarrow \bR$,
    \begin{align*}
    \log \int \exp(g(\theta)) d\mu \geq \int g(\theta) d\rho(\theta) - \KL(\rho\|\mu) \,.
    \end{align*}
    Moreover, the inequality becomes an equality for $d\rho(\theta) \propto \exp(g(\theta)) d\mu(\theta)$.
\end{lemma}
The inequality plays a central role in variational inference, and is typically stated as the \emph{evidence lower bound} (ELBO), by specializing \cref{lemma:variational-kl} using $g(\theta) = - L_t (\theta)$ and $\mu = \mu_0$,
\begin{align*}
 \log \int \exp(- L_t(\theta)) d\mu_0 \geq \ELBO_t(\rho) := -\int L_t(\theta) d\rho(\theta) - \KL(\rho\|\mu_0) \,.
\end{align*}
For the given choices, the inequality becomes tight when $\rho = \mu_t$ is the Bayesian posterior.
Variational inference aims at numerically maximizing the evidence lower bound over a parametric family of posterior distributions \citep{jordan1999introduction}, see also \citep{blei2017variational}. In the context of confidence estimation, the key insight is that the variational inequality allows to relax the marginal likelihood that defines the confidence sequence in \cref{result:prior_mixing}. This result has been recently stated by \citet{lee2024improved} in the specialized context of logistic and multinomial bandits, and similar bounds are well-known in the PAC-Bayes literature \citep[e.g.,][]{zhang2006varepsilon,chen2022unified,alquier2024user}. Here, we emphasize the connection to variational inference and the evidence lower bound as a way to define a confidence coefficient with valid anytime coverage.
\begin{theorem}[Evidence Lower Bound Confidence Set]\label{result:elbo_confidence_set}
    For any $\cF_t$-adapted sequence of distributions $\mu_t \in \sP(\Theta)$ and a data-independent prior $\mu_0 \in \sP(\Theta)$, define\looseness=-1
    % the $(1-\delta)$-confidence sequence $C_t = \left\{\theta \in \Theta : L_t(\theta) \leq  \log \frac{1}{\delta} - \log \int \prod_{s=1}^t p_s(y_s|\nu) d\mu_0(\nu) \right\}$, it holds that
    \begin{align*}
		 C_t = \left\{\theta \in \Theta : L_t(\theta) \leq  \log \frac{1}{\delta} - \ELBO_t(\mu_t)  \right\} \,.
	\end{align*}
    Then $C_t$ defines a $(1-\delta)$-confidence sequence. Moreover, if $\rho_t$ is chosen as the Bayesian posterior, the result is equivalent to the marginal likelihood confidence set in \cref{result:prior_mixing}.
    % Moreover, if $\rho$ is chosen as the Bayesian posterior, the inclusion becomes an equality.
\end{theorem}
The practical implication of this result is that it provides a tool to trade off computational tractability and statistical efficiency. In particular, standard variational inference methods can be converted into a confidence set with provable coverage, simply by thresholding the negative log-likelihood by the attained evidence lower bound. Another possibility is to make ad-hoc choices for the posterior to obtain closed-form expressions for confidence sets, e.g.~for logistic regression \citep{lee2024unified}. 
