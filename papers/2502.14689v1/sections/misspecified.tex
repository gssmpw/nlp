\section{Misspecified Model Classes}\label{sec:misspecified}

So far, we have assumed that the model class is realizable, that is, there exists a parameter $\theta^* \in \Theta$ such that $p_t(y|\theta^*) = \frac{d\bP_t}{d\xi}$ represents the density of the true data generating distribution. The realizability assumption is used to show that the sequential likelihood ratio is a (super)martingale. The can be significantly relaxed, and there are several ways in which we can construct supermartingales for misspecified model classes, as we elaborate now. 





% \subsubsection{Sub-Likelihood Robustness}
% In particular, we will assume that the following condition holds for the true data-generating process, we will refer to it as sub-likelihood condition. 

% \begin{definition}[Sub-likelihood]\label{def:2} Let $L$ be the log-likelihood of the true distribution of $\epsilon_t$ for all $t$, be $p$. We call $p$ to be sub-Likelihood $L$ if, 
%   \begin{equation}
%       \EE_{p}[e^{\nabla  L(\epsilon) \eta + B_{ L}(\epsilon-\eta,\epsilon)}] \leq 1,
%   \end{equation}  
%   where $B_{\log L}$ denotes Bregman divergence of $\log L$. 
% \end{definition}
% This sub-likelihood condition can be linked to a broader treatment of sub-processes by \cite{Howard2020}, where they examine a similar problem to define sub-families and construct confidence sets for these sub-families. Conceptually, our approaches are opposite. While they define a family of probability distributions that includes a given likelihood, we start with a fixed likelihood and identify robustness conditions for other distributions.

% In order to have a valid confidence interval with likelihood ratios, we need that 
% \begin{align}\label{eq:miss_lk}
%     E_t(\theta) = \prod_{s=1}^t \frac{\exp(L(f_{\hat \theta_{s-1}}(x_s) - y_s))}{\exp(L(f_\theta(x_s) - )y_s)}
% \end{align}
% is a super-martingale, as we show now. 
% \begin{theorem} Assume that the true data-generating satisfies Definition \ref{def:2}, then $E_t(\theta^*)$ in Eq. \eqref{eq:miss_lk} is a super-martingale adapted to the usual filtration $\cF_{t-1}$. 
% \end{theorem}
% \begin{proof}
% Let $\eta = f_{\hat \theta_t}(x_{t-1})$, and note that Bregman divergence, 
% $L(\epsilon-\eta) - L(\epsilon) = -\nabla L (\epsilon)^\top \eta - B_L(\epsilon-\eta,\epsilon)$. Then, 

% \begin{eqnarray}
% \EE_{\epsilon_t}[E_t(\theta^*)| \mathcal{F}_{t-1}]  &  = & E_{t-1}(\theta^*)\EE_{\epsilon}[\exp(L(\epsilon-\eta) - L(\epsilon))] \\ & = & E_{t-1}(\theta^*) \EE_{\epsilon}[\exp( \nabla L (\epsilon)^\top \eta - B_L(\epsilon-\eta,\epsilon))] \leq E_{t-1}(\theta^*)
% \end{eqnarray}

% \end{proof} 

% This result allows us to construct confidence intervals for $\theta^*$ that parametrizes the mean of the $y_t$ irrespective of the misspecification, albeit at loss of power since the $E_t(\theta^*)$ is only a super-martingale instead of a martingale as in the well-specified case. 

% \paragraph{Example 1: Sub-Gaussian Likelihoods}\label{sec:subgaussian}
% Assume that the true data generating distribution $p(y_t|x_t)$ is $\sigma$-sub-Gaussian, that is $\epsilon_t = y_t - \EE[y_t]$ satisfies
% \begin{align*}
%     \EE[\exp\left(\epsilon \eta - \frac{\sigma^2 \eta^2}{2}\right)] \leq 1 \quad \text{for all} \quad \eta \in \bR
% \end{align*}
% This assumption exactly coincides with Definition \ref{def:2} as $B_{L} = \frac{\eta^2}{2\sigma^2}$, and $\nabla L = \frac{\epsilon}{\sigma^2}$, which is equivalent to the above.

% \paragraph{Example 2: Sub-Poisson Likelihood}\label{sec:sub} Suppose that the mean is parametrized as $f_\theta(x) = \exp(\theta^\top x) $, and we are observing integer values as $y_t \in \{0,1,\dots \infty\}$. Now the true generating distribution $p_i$ for data point $y_i$ is sub-Poisson if, that is when using Poisson likelihood, we maintain coverage even if $p$ satisfied the following,
% \[ \EE [\exp\left( (\eta + \epsilon)\log (\lambda_i) + \log\left(\frac{(\epsilon + \eta)!}{\epsilon!}\right)\right) ] \leq 1 \quad \text{for all}  \quad \eta \in \bR. \]

\subsection{Sub-Gaussian Distributions}\label{sec:subgaussian}
In this section, we let $\cY = \bR$ and assume that the true data generating distribution $\bP_t$ is $\sigma$-sub-Gaussian for all $t \geq 1$, that is $\epsilon_t = y_t - \EE[y_t|\cF_{t}]$ satisfies,
\begin{align*}
\EE[e^{\epsilon_t \eta}|\cF_t] \leq e^{\frac{\sigma^2 \eta^2}{2}} \quad \text{for all} \quad \eta \in \bR \,.
\end{align*}
Further assume that we have a parametrized family of mean functions $f_\theta : \cX \rightarrow \bR$, and there exists a $\theta^* \in \Theta$ such that $\EE[y_t|\cF_t] = f_{\theta^*}(x_t)$.
For any $\cF$-adapted sequence of mixing distributions $\mu_0, \mu_1, \dots \in \sP(\Theta)$, define
    \begin{align*}
        E_t(\theta) = \prod_{s=1}^t \frac{ \int \exp(- \frac{1}{2 \sigma^2} (f_{\nu}(x_s) - y_s)^2) d\mu_{s-1}(\nu)}{\exp(- \frac{1}{2 \sigma^2} (f_\theta(x_s) - y_s)^2)} \,.
    \end{align*}
As we will see shortly, $E_t(\theta^*)$ is a $\cF_t$ adapted supermartingale, and $\EE[E_1(\theta^*)] \leq 1$.
Two remarks before we prove the claim. First, if the true data distribution is Gaussian with mean $f_\theta(x)$ and variance $\sigma^2$, then $E_t(\theta)$ is just the sequential marginal likelihood ratio, and the $\sigma$-sub-Gaussian condition holds with equality. Second, the result implies that we can proceed in constructing our confidence set as if the the Gaussian likelihood model was correct, and the coverage results remain true.\looseness=-1
\begin{theorem}
    For any $\cF$-adapted sequence of distributions $\mu_0, \mu_1, \mu_2, \dots$ in $\sP(\Theta)$, define
    \begin{align*}
        C_t = \left\{ \theta \in \Theta:   \sum_{s=1}^t \tfrac{1}{2 \sigma^2}(f_\theta(x_s) - y_s)^2 \leq \log \frac{1}{\delta} + \sum_{s=1}^t \log \int  \exp( -\tfrac{1}{2 \sigma^2} (f_{\nu}(x_s) - y_s)^2) d\mu_{s-1}(\nu)\right\}
    \end{align*}
    Then $C_t$ defines a $(1-\delta)$-confidence sequence.
\end{theorem}
\begin{proof}
We start by showing that $E_t(\theta^*)$ is a super-martingale. Fubini's theorem implies that
\begin{align*}
    \EE[E_t(\theta^*)|\cF_{t-1}] &= E_{t-1}(\theta^*) \int \EE[\exp\left( - \tfrac{1}{2\sigma^2} \big(f_\nu(x_t) - y_t\big)^2 + \tfrac{1}{2\sigma^2} \big(f_{\theta^*}(x_t) - y_t\big)^2 \right) |\cF_t ]  d \mu_{t-1}(\nu)%\\
    % &= E_{t-1}(\theta^*) \EE[\exp\left( - \tfrac{1}{2\sigma^2} f_{\hat \theta_t}(x_t)^2 - \tfrac{1}{\sigma^2} y_t f_{\hat \theta_t}(x_t) + \tfrac{1}{\sigma^2} y_t f_{\theta^*}(x_t) - \tfrac{1}{2\sigma^2} f_{\theta^*}(x_t)^2  \right) ]\\
    % &= E_{t-1}(\theta^*) \exp\left( - \tfrac{\sigma^2}{2} \big( f_{\hat \theta_s}(x_s)^2 - f_{\theta^*}(x_t)^2 \big) \right) \EE[\exp\left( - \tfrac{1}{2\sigma^2} \big( - 2 y_s f_{\hat \theta_s}(x_s) + 2 y_s f_\theta(x_s)  \big) \right) ]\\
\end{align*}
From here, we compute the conditional expectation inside the integral. We expand the squares, simplify and substitute $y_t = f_{\theta^*}(x_t) + \epsilon_t$. After a bit of work we arrive at 
\begin{align*}
    &\EE[\exp\left( - \tfrac{1}{2\sigma^2} \big(f_\nu(x_t) - y_t\big)^2 + \tfrac{1}{2\sigma^2} \big(f_{\theta^*}(x_t) - y_t\big)^2 \right) |\cF_t ] \\
    &= \exp\left(- \tfrac{1}{2\sigma^2} \big(f_{\hat \theta_{t-1}}(x_t) - f_{\theta^*}(x_t)\big)^2 \right) \EE[\exp\left( \epsilon_t \cdot \tfrac{1}{\sigma^2} \big(f_{\hat \theta_{t-1}}(x_t) - f_{\theta^*}(x_t)\big) \right) |\cF_t] \,.
\end{align*}
Next, we use that $\epsilon_t$ is $\sigma^2$-sub-Gaussian, which, by definition, implies that 
\begin{align*}
    \EE[\exp\left( \epsilon_t \cdot \tfrac{1}{\sigma^2} \big(f_{\hat \theta_{t-1}}(x_t) - f_{\theta^*}(x_t)\big) \right) ] \leq \exp\left(\tfrac{1}{2\sigma^2} \big(f_{\hat \theta_{t-1}}(x_t) - f_{\theta^*}(x_t)\big)^2 \right) \,.
\end{align*}
We conclude that $\EE[E_t(\theta^*)|\cF_{t-1}] \leq  E_{t-1}(\theta^*)$ and $\EE[E_1(\theta^*)] \leq 1$. The claim follows using Ville's inequality.
\end{proof}

% Lastly, we remark that one can reverse the setup, and start with a loss function $l_t : \Theta \times \cY \rightarrow \bR$. Let $\cP$ be the set of distributions for which the process $E_t(\theta) = \prod_{s=1}^t \exp(l_s(\theta, y_s) - l_s(\hat \theta_{s-1}, y_s) )$ is a supermartingale\todoj{how is $\theta^*$ and $\bP$ related?}. Then $\{ \theta \in \Theta : \log E_t(\theta) \leq \log \frac{1}{\delta} \}$ defines a confidence set, as long as $\bP \in \cP$. Setting  $l_t(\theta, y) = \frac{1}{2\sigma^2}(f_\theta(x_t) - y)^2$ recovers the sub-Gaussian case, and choosing the log-loss $l_t(\theta, y) = - \log p_t(y|\theta)$ recovers the standard likelihood ratio. 

% $$\theta^*_t = \argmin_{\theta \in \Theta} \sum_{s=1}^t \bE_s[l_s(\theta, y_s)]$$

% Define the \emph{generalized sequential likelihood ratio},
% $$E_t(\nu, \theta) = \prod_{s=1}^t \exp(l_s(\theta, y_s) - l_s(\nu, y_s) )$$
% Note that for the log loss, $E_t(\nu, \theta) = R_t(\nu, \theta)$ recovers the standard sequential likelihood ratio. Define
% % $$D_t^E(\nu \| \theta) = \sum_{s=1}^t \bE_s[\log E_s(\nu, \theta)]$$
% % and assume that $D_t^E$ is a divergence under the true distribution
% % Assume that there exists a par
% $$\theta^*_t = \argmin_{\theta \in \Theta} \sum_{s=1}^t \bE_s[\log E_s(\theta^*, \theta)]$$
% Note that the expectation recovers the KL (minimized by $\theta^*$) for the log loss, and the square loss over means for the sub-Gaussian case (offset by a variance term). 
% $$\theta^* = \argmin_{\theta \in \Theta} \sum_{s=1}^t \bE_s[\log E_s(\bP, \theta)]$$

\subsection{Convex Model Classes}\label{sec:convex}
We return to the original definition of the model class as a parameterized family of conditional densities $\cM = \{ p_\theta(y|x) : \theta \in \Theta\}$. Moreover, assume that there exists a conditional density $p^*(y|x)$ such that for all $t \geq 1$, $\frac{d\bP_t}{d\xi} = p^*(\cdot|x_t) d\xi$. Crucially, we do \emph{not} require that $p^* \in \cM$. 

Throughout this section, we make the assumption that $\cM$ is convex. Note that convexity is required in the space of distributions, i.e., all finite mixtures of densities in $\cM$ are contained in $\cM$. In general, convexity of $\Theta$ does not imply convexity of $\cM$. Nevertheless, there are many examples of convex model classes, including, for example, all finite mixtures of any family of distributions. 

Our main tool is the \emph{reverse information projection} theorem by \citet{li1999estimation}, see also \citet{lardy2024reverse}. Applied to our setup, the theorem states that for any sequence $q_n \in \cM$ such that $$\lim_{n \rightarrow \infty} \KL(p^*\|q_n) = \inf_{q \in \cM} \KL(p^*\|q) < \infty\,,$$ there exists a unique (sub-)probability measure $q^* d\xi$ such that $\KL(p^*\|q^*) = \inf_{q \in \cM} \KL(p^*\|q)$. Moreover, the reverse information projection theorem shows that for any $q_\theta \in \cM$,
\begin{align}
    \EE[\frac{p_\theta(y_t|x_t)}{q^*(y_t|x_t)}\big | \cF_t] \leq 1 \,. \label{eq:rips-e-value}
\end{align}
A technical condition is required to ensure that the limiting element $q^*$ is contained $\cM$. If we require that $\cY$ is a complete separable metric space, and $\cM$ is sequentially compact (with respect to the weak topology), then Prokhorov's theorem implies that $q^* \in \cM$. A similarly flavoured result (stated without mixing distributions) is by \citet[Proposition 7]{wasserman2020universal}

\begin{theorem}[Convex Model Classes] Assume that $\cM$ is convex and there exists $q^* \in \cM$ such that $\KL(p^*\|q^*) = \inf_{q \in \cM} \KL(p^*\|q)$. Then the sequential likelihood mixing confidence set, defined for any $\cF_t$-adapted sequence of distributions $\mu_0, \mu_1, \mu_2, \dots$ in $\sP(\Theta)$ (see \cref{result:posterior_mixing}), defines a $(1-\delta)$-confidence sequence for $q^* \in \cM$, i.e., $\bP[q^* \in C_t, \forall t \geq 1] \geq 1-\delta$.
\end{theorem}
\begin{proof}
Define the sequential marginal likelihood ratio w.r.t. to a conditional density $q(\cdot|x)$,
\begin{align*}
J_t(q) = \prod_{s=1}^t\frac{\int p_\nu(y_s|x_s) d\mu_{s-1}(\nu)}{q(y_s|x_s)}
\end{align*}
The reverse information projection theorem, specifically \cref{eq:rips-e-value}, implies that $J_t(q^*)$ is a non-negative supermartingale with $\EE[J_1] \leq 1$. The theorem follows using Ville's inequality.
\end{proof}
