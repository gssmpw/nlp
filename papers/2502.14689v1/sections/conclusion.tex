\section{Conclusion}

We presented a unifying framework for constructing confidence sequences using sequential likelihood mixing, with deep connections to Bayesian inference, maximum likelihood estimation and regret inequalities from online convex optimization. Many of the results presented here have appeared in some form in the literature before, although scattered and seemingly disconnected, or in specialized settings. An inevitably incomplete overview of related works is given in \cref{sec:literature}.\looseness=-1 %The sequential mixing framework provides a unifying perspective, allowing to simplify and tighten earlier results. %Given our setup of parametric estimation, we belief that the confidence bounds are most useful in settings where the (effective) parameter dimension is relatively small and well-specified.



%  Moreover, when applied to special cases, the resulting concentration inequalities match existing lower bounds and hence are non-improvable in key parameters without further assumptions on the data generating distribution. 

Of course, the story does not end here. Importantly, the realizability assumption can be relaxed, for example, to sub-Gaussian families (Appendix \ref{sec:subgaussian}) and convex model classes (Appendix \ref{sec:convex}). The sequential likelihood ratio is not the only martingale that can be turned into a confidence sequence. Another natural extension to \emph{tempered likelihood ratios} is discussed in \cref{sec:tempered}, which has been suggested as a way to make the Bayesian model update more robust \citep[e.g.,][]{grunwald2012safe}. The essentially equivalent setting of sequential testing, and the literature on e-values is another staring point for further investigation \citep{grunwald2020safe}.

One of the main objection against the Bayesian approach is often that it is generally subjective, as it depends on the choice of prior. As we demonstrate here, this does not prevent us from constructing frequentist confidence sets, as long as the prior is chosen independently of the data. Although the confidence sets depend explicitly on the Bayesian posterior, the random deviations are controlled in the frequentist sense, with respect to the true data distribution. Nevertheless, it is true that the size of the confidence set depends on how well the prior is concentrated on the true data distribution, but this is analogous to specifying structural model assumptions in a purely frequentist setup. \looseness=-1

In a similar spirit, \citet{wasserman2020universal} briefly mentions the prior likelihood mixing for constructing a confidence set, noting that it ``requires specifying a prior and doing an integral''. We certainly do not deny the challenges associated with computing the marginal likelihood, but would argue that the marginal likelihood appears as a natural quantity in the construction of confidence sequences, and establishes a fundamental connection to Bayesian inference. Approximate inference techniques (e.g., variational inference and sampling-based approximations) make this idea viable, maintaining provable coverage guarantees. Moreover, in settings where prior data is available, a promising direction is to learn structured priors from the data. 

