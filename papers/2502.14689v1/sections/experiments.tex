
\section{Bandits Optimization and the UCB algorithm}

Here we introduce briefly the stochastic bandits problem, see \citep{lattimore2020bandit} for an in-depth treatment of the topic. A bandits problem is a sequential game where a learner $L$ interacts with an environment $E$ at each time $t = 1, \ldots, H$, where $H$ is a finite horizon. At every timestep $t$, $L$ chooses an action $a_t$ from a set $\mathcal{A}_t$, called the \textit{arms} set; then $E$ yields a reward $r_t$. The goal of the problem is to maximize $\sum_{t = 1}^H r_t$. When $r_t$ is drawn from arm-dependent distributions $P_t(a_i)$ for $i \in [\| \mathcal{A}_t \| ]$ we  are dealing with the \textit{stochastic} bandits problem. 

In the following section, we will assume that $r_t$ is drawn from a Bernoulli distribution with mean $\sigma( \langle a_t, \theta^*  \rangle)$, where $\sigma(\cdot )$ is the sigmoid function, and $\theta^*$ is the true parameter. This is known as the \textit{logistic} bandits problem.

We work on the logistic bandits problem under the following assumptions, which are standard, see \citep{filippi2010parametric}:

\begin{assumption}{(Bandit parameter)}
We have $\theta \in B(0, S)$, where $B(0, S)$ is an $\mathbb{R}^d$ ball centered at the origin, and   $|| \theta^*||_2 \leq S$.
\end{assumption}

\begin{assumption}{(Arm set)}
We have $||a||_2 \leq 1$ for all $a \in \{ \mathcal{A}_t \}_{t = 1}^H$.
\end{assumption}

A prototypical bandit algorithm is the UCB algorithm for stochastic bandits, which at time $t$ chooses the arm maximizing the optimistic estimate of the reward for that round. For the logistic bandits problem, the UCB algorithm is as follows 

\begin{equation}
    a_t = \argmax_{a} \max_{\theta \in C_t} \sigma(\langle \theta, a \rangle) \,,
\end{equation}
where $\mathcal{C}_t$ is a confidence set on $\theta^*$. We wish for tight $C_t$ that contain $\theta^*$ with high probability. 

\section{Experiments}

\subsection{Logistic Bandits}

We showcase an application of the confidence sets discussed in the paper in the context of bandit optimization, where tight confidence sequences are used as an essential component of the UCB algorithm. In particular, we compare implementations of the Prior Likelihood Mixing confidence set (Theorem \ref{result:prior_mixing}) and an approximation of the confidence set obtained by sequential mixing (Lemma \ref{lem:posterior_ratio_confidence_set}), against confidence sequences from prior work in the context of logistic bandits.

Our code builds upon \citep{lee2024unified} and \citep{faury2022jointly}. A description of the implementation details will follow. 

\subsubsection{Baselines} 

We benchmark implementations of the confidence sequences discussed in this work against the ones introduced in \citep{Emm23} (\textcolor{blue}{EMK}) and Theorem 3.1 in \citep{lee2024unified} (\textcolor{red}{OFUGLB}), which are the best performers among the baselines considered in \citep{lee2024unified}. We refer the reader to this work also for the discussion of other baselines for logistic bandits.

The implementations of \textcolor{blue}{EMK} and \textcolor{red}{OFUGLB} are left unchanged relative to the codebase of \citep{lee2024unified}.
The algorithm \textcolor{green}{MQ} (Marginal Quadrature) computes close to machine precision the marginal likelihood appearing in Theorem \ref{result:prior_mixing}. Similarly to  \textcolor{red}{OFUGLB}, the computed confidence sets are level sets of the log-likelihood function. We use a uniform prior on $B(0, S)$ for the computations. 
The algorithm \textcolor{orange}{PL} (Posterior Laplace) computes a Laplace approximation of the posterior appearing on the left hand side in Lemma \ref{lem:posterior_ratio_confidence_set}. Thus we are effectively fitting a normal distribution to the posterior, and threshold its logarithm. Unlike \textcolor{green}{MQ}, it performs an approximation and not an exact computation, and thus it does not have provable coverage. \textcolor{orange}{PL} is advantageous when $\theta^*$ has a large number of dimensions, since it does not face the issue of high-dimensional integration like \textcolor{green}{MQ}. Moreover, the computation of an Hessian and the subsequent dependence on the square number of dimensions, could be avoided with further approximations \citep{antoran2022sampling}. Also for this algorithm we use a uniform uninformative prior.
We observe that both of our algorithms do not need any  tuning of hyperparameters.

\subsubsection{Setup} 

We share the same setting as in \citep{lee2024unified}.

We generate at each timestep $t = 1, \ldots, 1000$ a set of arms $\mathcal{A}_t$ with $| \mathcal{A} | = 10$ arms sampled uniformly on $B(0, S)$. We vary $S \in \{ 4, 6, 8, 10\}$ through the experiments.  The true parameter $\theta^*$ is set to be $(\frac{S-1}{\sqrt{d}}, \frac{S-1}{\sqrt{d}})$. The confidence bands for the regret are computed as the standard deviation over 5 runs.

\subsubsection{Results}
We can see the results in Figure \ref{fig:regret_and_confidence}. We observe that \textcolor{green}{MQ} has the best performance in terms of regret for $S = 6, 8, 10$, whereas \textcolor{blue}{EMK} outperforms the other baselines for $S = 4$. 
In terms of the sizes of the confidence sets, \textcolor{green}{MQ} and \textcolor{orange}{PL} outperform the other baselines, recalling that \textcolor{green}{MQ} also maintains provable coverage. 
\textcolor{red}{OFUGLB} seems in general to achieve tighter confidence sets than \textcolor{blue}{EMK}, despite having a higher regret for $S = 4, 6, 8$.
Seeing the good performance of \textcolor{green}{MQ} in terms of regret and tightness of the sets, confirm that marginal likelihood confidence sets are a good way to tackle the problem of confidence estimation, while other methods based on approximate inference remain competitive.

\begin{figure}[htp]
    \centering
    \includegraphics[width=5.5in]{total_h1001d2ttv_discrete_20250206_184004.pdf}
    \caption{Regret plots and confidence sets}
    \label{fig:regret_and_confidence}
\end{figure}



\subsection{Sparse Priors}
In this experiment we seek to demonstrate the flexibility of our framework and showcase the improvement in confidence sequences for a likelihood parameterized by a sparse parameter vector. To gain upon classical methods, we mix with a sparse prior, which is supported on the value true parameter, as well as having higher mass on the sparse parameters.

More concretely, we compare four different confidence sets on a set of parameters that describe a polynomial function. Assume a set of Chebyschev polynomials $\{\Phi_i(x)\}_{i=1}^d$, where $i$ indexes the order of the polynomial. The deterministic signal, i.e. a function, is parametrized as $f(x) = \sum_i \theta_i \Phi_i(x)$ such that $||\theta||_2^2 \leq 2$, and $||\theta||_0 \leq 2$. In other words, only two out of $d=20$ components are non-zero. We chose the test function such that $\theta_1 = \theta_2$ are equal to $1$, while the rest have to be zero. 

We assume that the ground-truth signal is corrupted by a Gaussian noise with a known variance, in other words,  $\epsilon\sim \mathcal{N}(0, \sigma^2)$, and $y_i = f(x_i) + \epsilon_i$. We construct four different confidence sets. The first two are based on the work of \cite{Emm23} abbreviated EMK. In one case we use a sparse prior $||\theta||_0 =2$, which leads to enumeration of solutions, and in the other we use only the prior $||\theta||^2_2 \leq 2$. We hope to see an improvement as with the proper prior the sequence of running MLE estimator can have a lower regret. 

The second group of methods implements the prior-posterior ratio confidence sets of this work. We use the abbreviation MQ, and also a variant that incorporates a sparse mixing prior $P(\theta) = \prod_{i=1}P(\theta_i)\bI_{||\theta||_0 \leq 2}$, incorporating indicator whether the set of parameters is 2-sparse. 

The four different confidence sets in terms of \eqref{eq:level-set} are presented bellow. We use the shorthand $L(D|\theta) = \sum_{i=1}^d L(x_i,y_i|\theta)$, and $L(D|\theta_i)$ which are both likelihoods where the second term is such that for all  $\theta_j$, s.t. $j\neq i$ is set to zero. 

\begin{description}
    \item[EMK]
     $ C = \left\{ \theta ~|~  ||\theta||_0 \leq 2 ~|~ L(D|\theta) \leq \log\left(\frac{1}{\delta}\right) + \sum_{i=1}^n L(y_i,x_i| \hat{\theta}_{i-1}) \right\} $
    \item[EMK with sparse prior]
    $ C = \left\{ \theta ~|~  ||\theta||_0 \leq 2 ~|~ L(D|\theta) \leq \log\left(\frac{1}{\delta}\right) + \sum_{i=1}^n L(y_i,x_i| \hat{\theta}_{i-1,0}) \right\} $,
    where $\hat{\theta}_{0,i}$ is $||\theta_{i,0}||_0=2$ sparse estimator using the data up to data point index $i$.
    \item[Posterior-Prior Ratio] $ C = \left\{ \theta ~|~  ||\theta||_0 \leq 2 ~|~ L(D|\theta) \leq \log\left(\frac{1}{\delta}\right) + \log\left( \int_{\theta \in \bR^d} \exp(-L(D|\theta)) p_0(\theta) d\theta\right) \right\} $
    \item[Posterior-Prior Ratio (sparse prior)] \hfill \\
    $ C = \left\{\theta ~|~ ||\theta||_0 \leq 2 ~|~ L(D|\theta) \leq \log\left(\frac{1}{\delta}\right) + \log\left(\frac{1}{d^2}\sum_{S \in \mathcal{S} } \int_{\theta_S \in \bR^2} \exp(-L(D|\theta_S)) p_0(\theta_S) d\theta_S\right) \right\} $
    $\mathcal{S}$ contains all possible variable pairs including the ones where only one variable is active. 
\end{description}

We report the results in Figure \ref{fig:sparse-models} an example of confidence bands for the individual parameters $\theta_i$ after $n=20$ data points have been observed. In Figure \ref{fig:sparse-models-avg}, we provide an average over confidence bands for ten reruns to average out possible stochastic effects that might give preference to one or another method.  Akin to our prior results, we see that confidence sets based on sparse estimator or sparse prior generally outperform the non-sparse ones, and the confidence sets based on the mixing distributions are better than point estimates. In this specific case, computational effort is mitigated by having a closed form of the marginal likelihood. However, we still had to execute $d^2$ integrals rendering higher orders of sparsity.


    \begin{figure}%[b]{0.45\textwidth}
    \centering
        \includegraphics[width=0.75\textwidth]{figure20.png}
            \caption{An example of confidence sets for each parameter $\theta_i$ for different methods using the same data stream. In this case we generated the data using i.i.d. sampling. Notice that the sparse prior prior-posterior ratio confidence sets are best performing in this example.}
        \label{fig:sparse-models}    
    \end{figure}
~
    \begin{figure}%[b]{0.45\textwidth}
        \centering
        \includegraphics[width=0.75\textwidth]{avg_bars.png}
        \caption{Confidence width for each parameter $\theta_i$ for different methods using the same data steam for 10 reruns to average random effect. One standard deviation errors bars are provided.}
        \label{fig:sparse-models-avg}    
\end{figure}
