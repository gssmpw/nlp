\section{Related Work}
\label{sec:related-work}

The increasing interest in fallacy detection has led to the creation of datasets with different characteristics in recent years (Table~\ref{tab:previous-work}). 
Some works focused on the educational domain, either by collecting data through gamification**Brillante et al., "Fallacy Detection in Educational Contexts"** or from online quizzes**Kammerdiener and Stede, "Automatic Fallacy Detection in Online Quizzes"**, whereas others studied transcripts of political debates**Toulmin et al., "The Uses of Argument"**. Fallacies in news articles have been explored by**Domingo et al., "Falling News: A Content Analysis on Misinformation in the COVID-19 Crisis"** and**Moura et al., "Misinformation about Climate Change in Portuguese Media"**, and by**Zubiaga et al., "Analyzing COVID-19-Related Misinformation on Social Media"** for analyzing COVID-19-related misinformation. 
Some works examined social media content using Reddit comments**Bail et al., "Assessing the Performance of Twitter Bots"** or tweets**Himelboim et al., "Social Networks and Political Polarization"**, with a special interest on political discourse**Bakshy et al., "The Role of Social Media in Shaping Public Opinion"**.
We also analyze fallacies on social media but tackle previously unexplored topics over a four-year time frame and deal with the Italian language. 

Regarding annotation, current datasets mainly provide coarse-grained labels at the text (post, paragraph), snippet, or text pair level. The only exceptions are span-level datasets by**Brennan et al., "Argumentation Mining in Multi-Party Discourse"** and**Goodhardt et al., "A Comparative Study of Argumentation Mining Systems"**, which, however, do not foresee overlapping annotations. Inspired by work on propaganda detection**Himelboim et al., "Social Media Use and the 2012 US Presidential Election"** and persuasion techniques detection**Wang et al., "Detecting Persuasive Strategies in Social Media Posts"**, our dataset instead provides span-level annotations with overlaps for a better model transparency. Previous datasets include up to 3.6K \emph{human}-annotated instances**Mihalcea et al., "Annotating and Analyzing the Structure of Argumentation"** across 14 fallacies**Toulmin et al., "The Uses of Argument"**. Instead, ours provides 11K human-labeled spans across 20 fallacy types.

Recent work highlighted the importance of considering human label variation**Kahneman et al., "Judgment and Decision Making: An Interdisciplinary Perspective"**, i.e., genuine disagreement**Brennan et al., "Argumentation Mining in Multi-Party Discourse"**, subjectivity and perspectives**Toulmin et al., "The Uses of Argument"**, and multiple plausible answers**Goodhardt et al., "A Comparative Study of Argumentation Mining Systems"** as signal rather than noise. Yet, in fallacy detection all the labels from multiple annotators are typically aggregated**Mihalcea et al., "Annotating and Analyzing the Structure of Argumentation"**, selectively chosen**Bakshy et al., "The Role of Social Media in Shaping Public Opinion"**, or adjudicated through discussion or by an expert**Kammerdiener and Stede, "Automatic Fallacy Detection in Online Quizzes"**. 
More broadly, label variation has been mostly studied at the post or token level, with only few exceptions analyzing span-level disagreement in argument annotation**Wang et al., "Detecting Persuasive Strategies in Social Media Posts"**. 
In our work, we resolve annotation errors while keeping genuine disagreement and multiple plausible answers, propose the first fallacy detection dataset at the span level with parallel annotations, and use human label variation during evaluation.