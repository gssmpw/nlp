\section{Related Work}
\label{sec:related-work}

The increasing interest in fallacy detection has led to the creation of datasets with different characteristics in recent years (Table~\ref{tab:previous-work}). 
Some works focused on the educational domain, either by collecting data through gamification~\citep{habernal-etal-2017-argotario,habernal-etal-2018-adapting} or from online quizzes~\citep{jin-etal-2022-logical}, whereas others studied transcripts of political debates~\citep{goffredo2022fallacious,goffredo-etal-2023-argument}. Fallacies in news articles have been explored by~\citet{jin-etal-2022-logical} and~\citet{alhindi-etal-2022-multitask} in climate change discourse, and by~\citet{musi2022developing} for analyzing COVID-19-related misinformation. 
Some works examined social media content using Reddit comments~\citep{habernal-etal-2018-name,sahai-etal-2021-breaking} or tweets~\citep{sheng-etal-2021-nice}, with a special interest on political discourse~\citep{macagno2022argumentation,shultz-2024-entity}.
We also analyze fallacies on social media but tackle previously unexplored topics over a four-year time frame and deal with the Italian language. 

Regarding annotation, current datasets mainly provide coarse-grained labels at the text (post, paragraph), snippet, or text pair level. The only exceptions are span-level datasets by~\citet{sahai-etal-2021-breaking} and~\citet{goffredo-etal-2023-argument} which, however, do not foresee overlapping annotations. Inspired by work on propaganda detection~\citep{da-san-martino-etal-2019-findings,da-san-martino-etal-2019-fine,da-san-martino-etal-2020-semeval} and persuasion techniques detection~\citep{piskorski-etal-2023-multilingual}, our dataset instead provides span-level annotations with overlaps for a better model transparency. Previous datasets include up to 3.6K \emph{human}-annotated instances~\citep{habernal-etal-2018-name} across 14 fallacies~\citep{goffredo2022fallacious}. Instead, ours provides 11K human-labeled spans across 20 fallacy types.

Recent work highlighted the importance of considering human label variation~\citep{plank-2022-problem}, i.e., genuine disagreement~\citep{poesio-artstein-2005-reliability}, subjectivity and perspectives~\citep{aroyo-and-welty-2015-truth,cabitza-etal-2023-toward}, and multiple plausible answers~\citep{nie-etal-2020-learn} as signal rather than noise. Yet, in fallacy detection all the labels from multiple annotators are typically aggregated~\citep{sahai-etal-2021-breaking}, selectively chosen~\citep{musi2022developing,habernal-etal-2017-argotario}, or adjudicated through discussion or by an expert~\citep[][\emph{inter alia}]{jin-etal-2022-logical,macagno2022argumentation,goffredo2022fallacious,goffredo-etal-2023-argument}. 
More broadly, label variation has been mostly studied at the post or token level, with only few exceptions analyzing span-level disagreement in argument annotation~\cite{lindahl-2024-disagreement,hautli-janisz-etal-2022-disagreement}. 
In our work, we resolve annotation errors while keeping genuine disagreement and multiple plausible answers, propose the first fallacy detection dataset at the span level with parallel annotations, and use human label variation during evaluation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%