\begin{figure*}[t]
    \centering
    % \vspace{-7mm}
    \includegraphics[width=0.9\linewidth]{figs/intro_img.pdf}
    \vspace{-3mm}
    \caption{
    Comparision between text, relative trajectory, and absolute trajectory based camera-controlled image-to-video generation methods on aspects of camera control precision and usability.
    % Comparison of existing attention mechanisms for tracking displaced noised features.} 
    % Temporal attention is limited to features at the same location of picture, rendering it ineffective for significant camera movements. In contrast, 3D full attention facilitates cross-frame tracking due to its broad receptive field.
    }
    \label{fig:intro}
    \vspace{-2mm}
\end{figure*}
\vspace{-2mm}
\section{Introduction}
\label{Introduction}
Recent advancements in image-to-video generation \cite{Guo2023, he2022latent, chen2023videocrafter1, chen2024videocrafter2, brooks2024video, yang2024cogvideox} have significantly improved controllability over synthesized videos.
However, challenges remain in achieving realistic, controllable camera movement within complex real-world scenes.
Text-based camera-control methods \cite{Guo2023, Blattmann2023, li2024image, hu2024motionmaster, jain2024peekaboo, wang2024videocomposer}, like traditional diffusion-based video generation, are intuitive and straightforward but lack precision in explicit control over camera parameters, such as angle, scale, and movement direction.
This limitation has spurred the development of camera-trajectory-guided approaches, which attempt to address these issues by offering finer control over camera movement.


% particularly in camera-trajectory-guided frameworks that offer greater precision than text-based methods.   

Current camera-trajectory-guided methods typically rely on relative camera trajectories, as seen in models like MotionCtrl  \cite{Wang2024Motionctrl}, CameraCtrl \cite{He2024Cameractrl}, CamCo \cite{xu2024camco}, and CamI2V \cite{zheng2024cami2v}. While these approaches provide more control than text-based models, they are fundamentally limited by their reliance on relative scale trajectories. Training on relative scales results in inconsistencies when applied to real-world scenes, where absolute scale is crucial for realistic depth perception. Additionally, without access to depth information, users find it challenging to draw precise trajectories, making these methods difficult to use effectively.

To overcome these limitations, we propose RealCam-I2V, a video generation framework that integrates monocular depth estimation as a preprocessing step to construct a robust, absolute-scale 3D scene. Our approach leverages the Depth Anything v2~\cite{depth_anything_v2} model to predict the metric depth of a user-provided reference image, reprojecting its pixels back into camera space to create a stable 3D representation. This 3D scene serves as the foundation for camera control, providing a consistent and absolute scale that is critical for real-world applications.

In the training stage, we align the reconstructed 3D scene of the reference image with the point cloud of each video sample, reconstructed using COLMAP \cite{Schonberger2016}, a structure-from-motion (SfM) method. This alignment allows us to rescale COLMAP-annotated camera parameters to the Depth Anything metric, providing an absolute, stable, and robust scale across training data. By aligning relative-scale camera parameters to absolute scales, we can condition the video generation model on accurately scaled camera trajectories, achieving greater control and scene consistency across diverse real-world images.

During inference, RealCam-I2V provides an interactive interface where users can intuitively design camera trajectories by drawing within the reconstructed 3D scene of the reference image. This interface renders preview videos of the trajectory in a static scene, offering users real-time feedback and greater control over camera movement. This interactive feature enhances usability, allowing precise trajectory control even for users without specialized knowledge of scene depth.
To further improve video quality and control precision, we introduce scene-constrained noise initialization as a mechanism to shape the generation process in its high-noise stages. By using the preview video of the static 3D scene, RealCam-I2V injects scene-visible regions with controlled noise, guiding the video diffusion model’s early generation stages. This high-noise feature constrains the initial layout and camera dynamics, providing a strong foundation for the remaining denoising stages. As denoising progresses, the condition-based approach, trained on absolute-scale camera trajectories, preserves global layout and completes the dynamic scene in previously unseen regions. This approach maintains the video diffusion model’s capacity for dynamic content generation while ensuring accurate, coherent camera control.

Our experimental results show that RealCam-I2V achieves significant performance gains in video quality and controllability. When relative scales are aligned to absolute scales, models such as MotionCtrl, CameraCtrl, and CamI2V see substantial improvements in video quality. Furthermore, with the introduction of scene-constrained noise initialization, RealCam-I2V surpasses state-of-the-art performance benchmarks, particularly on datasets like RealEstate10K \cite{zhou2018stereo} and out-of-domain images. These results demonstrate the effectiveness of our approach in both controlled and diverse real-world settings.
In summary, our contributions are as follows:
\begin{itemize}
    \item We identify scale inconsistencies and real-world usability challenges in existing trajectory-based methods and introduce a simple yet effective monocular 3D reconstruction into the preprocessing step of the generation pipeline, serving as a reliable intermediary reference for both training and inference.
    \item  With reconstructed 3D scene, we enable absolute-scale training and provide an interactive interface during inference to easily design camera trajectories with preview feedback, along with proposed scene-constrained noise shaping to significantly enhance scene consistency and camera controllability.
    \item Our method overcomes critical real-world application challenges and achieves substantial improvements on the RealEstate10K dataset, establishing a new sota benchmark both in video quality and control precision.
    % \item We introduce monocular depth estimation as a preprocessing step to construct a 3D scene for real-world camera trajectory alignment.
    % \item We develop a training approach that aligns camera parameters to an absolute scale, enhancing compatibility and usability across diverse scenes.
    % \item We implement an interactive interface for intuitive trajectory design, allowing users to control camera movements precisely.
    % \item Our scene-constrained noise initialization further improves video quality and control precision, enabling RealCam-I2V to surpass current state-of-the-art performance.
\end{itemize}
    
% Image-to-video (I2V) generation, which transforms static images into dynamic video sequences, has become increasingly relevant across a range of applications. In fields like augmented reality (AR) and virtual reality (VR), I2V generation enables immersive experiences by generating realistic transitions and movements within a virtual environment. In film production, it provides a tool for pre-visualization, allowing directors to explore scene compositions and camera motions. Additionally, in architecture and urban planning, I2V generation aids in visualizing designs through animated sequences, offering professionals an efficient way to simulate and assess spatial arrangements from limited visual data.

% Traditional I2V generation techniques have primarily relied on text-based controls, where users provide textual descriptions to guide camera paths and scene dynamics. Although useful in some contexts, these text-based approaches are limited in their ability to specify complex, detailed camera movements. Text inputs lack the precision needed for nuanced trajectories, making it challenging to produce realistic outputs for applications requiring exact camera motions, such as cinematic VR and architectural visualizations. Consequently, there has been a strong demand for more flexible control mechanisms in I2V generation that allow for intricate camera paths and smooth transitions.

% To address these limitations, recent research has introduced trajectory-based camera control in I2V generation. This approach enables users to specify precise camera paths, offering a greater level of control over video generation compared to text-based methods. Trajectory-based control allows users to create complex camera motions, including loops, rotations, and other detailed movements, that significantly enhance the realism and flexibility of the generated videos. However, while trajectory-based control provides a solution to the limitations of text-based methods, it also presents new usability challenges.

% One significant challenge is the difficulty users face in drawing precise camera trajectories, especially when working with arbitrary real-world images. In these cases, users are often unable to specify accurate extrinsic parameters, such as camera position, rotation, and scale, due to a lack of depth and scene scale information. Additionally, existing datasets used for I2V training, such as RealEstate10K, often employ trajectories defined in relative scale. When users apply custom trajectories to real-world images, these paths frequently misalign with the dataset’s scale, resulting in outputs with inconsistent or unrealistic scene proportions. Such limitations reduce the practical applicability of trajectory-based I2V generation for real-world tasks where scene consistency and scale accuracy are critical.

% Given these challenges, there is a clear need for an I2V framework that can effectively handle arbitrary real-world images by reconstructing a unified 3D scene with absolute scale. This would allow users to intuitively create and adjust camera trajectories that are consistent with real-world spatial constraints. In response, we present RealCam-I2V, a framework designed to bridge these gaps by constructing a stable 3D scene from a single user-provided reference image using metric depth estimation. This approach not only enables alignment of relative-scale camera parameters in training but also supports an interactive interface during inference, where users can easily create accurate camera trajectories by dragging the camera within the reconstructed 3D scene. To further enhance scene consistency and camera control precision, we introduce a scene-constrained noise initialization method, which generates a preview video along the specified trajectory to guide the high-noise phase of generation. This structured initialization improves both scene consistency and camera control accuracy, while the condition-guided framework in subsequent noise stages preserves the model’s ability to produce dynamic scenes with high fidelity.

% In summary, RealCam-I2V addresses the core challenges of real-world I2V applications by combining condition-guided training methods with a training-free, interactive 3D interface in inference. Our framework enables high-precision camera control and realistic scene scaling, making it a versatile tool for diverse applications. The key contributions of this work are as follows:

% 	1.	A robust 3D scene reconstruction pipeline using metric depth estimation from a single image, providing absolute scale and enhanced compatibility for real-world images.
% 	2.	An interactive interface that allows users to intuitively create camera trajectories by dragging within the 3D scene, significantly improving usability.
% 	3.	A scene-constrained noise initialization that generates a structured preview video for guiding the high-noise phase, resulting in improved scene consistency and camera control precision.
% 	4.	Extensive experiments demonstrating the effectiveness of RealCam-I2V in achieving improved controllability, scene coherence, and dynamic generation across various applications, including looping video generation and frame interpolation.

% To support further research, we will release training and evaluation code, along with RealCam-I2V models at resolutions of 256x256, 512x320, and 1024x576, based on DynamiCrafter. This release will include checkpoints for additional applications, such as camera-controlled looping video generation and generative frame interpolation. In future work, we plan to explore more advanced foundational models, such as CogVideoX and openSora.
% To summarize, our key contributions are as follows:
% \begin{itemize}
%     \item To address scale alignment and usability limitations in trajectory-based I2V models for real-world applications, we propose incorporating 3D scene reconstruction as an initial step in the traditional video generation framework, establishing a foundational reference for precise, scale-consistent video generation.
%     \item We introduce a method that leverages reconstructed 3D scenes to align traditional relative-scale training data to an absolute scale, enabling our model to generalize effectively to arbitrary real-world images.
%     \item We identify limitations in current trajectory-based I2V models, particularly in scale alignment and usability, and address these by introducing a 3D scene reconstruction framework for  . Using metric depth estimation from a single image, which serves as the foundation for aligning scene scale and maintaining real-world compatibility.
%     \item 
%     \item Enhanced Interactive Interface and Scene-Constrained Initialization: RealCam-I2V includes an interactive interface that allows users to intuitively create camera trajectories by dragging within the 3D scene. This is complemented by a scene-constrained noise initialization, which uses a structured preview video to guide the high-noise phase of generation, improving both scene consistency and camera control precision.
% 	3.	Comprehensive Experimental Validation: Extensive experiments validate RealCam-I2V’s effectiveness in enhancing controllability, scene coherence, and dynamic generation, demonstrating its versatility in applications such as looping video generation and frame interpolation
    
% \end{itemize}



% The remarkable 3D consistency demonstrated in videos generated by Sora~\citep{brooks2024video} has highlighted the powerful capabilities of diffusion models~\citep{ho2020denoising, rombach2022high}, showcasing their potential as a world simulator. 
% Many researchers have attempted to enable the model to understand real-world knowledge~\citep{chen2023videocrafter1, liu2023zero}. 

% Condition or guidance~\citep{ho2022classifier, dhariwal2021diffusion} is widely recognized as a crucial factor in enhancing generation quality.
% This is attributed to the fundamental principles that  diffusion models denoise along the gradient of the log probability density function (score function)~\citep{song2020score}, moving towards a high density region.
% However, this characteristic has varying effects at different noise levels~\citep{tang2023stable}. 
% As shown in Fig. \ref{fig:teaser}(a), the high density region under high noise level becomes the overlap of numerous noisy samples, resulting in visual blurriness.
% By providing the model with conditions such as \(c_\text{dog}\) and \(c_\text{cat}\), it can rapidly eliminate incorrect generations. 
% This illustrates that effective conditions can guide the model towards desired outcomes while reducing uncertainty. 

% Consequently, incorporating physics-related or more detailed conditions into the diffusion model is an effective way of improving its world understanding. 
% Considering that video generation requires providing condition for each frame, it is essential to identify a condition that is physics-related and also user-friendly.
% Recently, some camera-conditioned text-to-video diffusion models such as MotionCtrl~\citep{He2024Cameractrl} and CameraCtrl~\citep{Wang2024Motionctrl} have proposed using camera poses of each frame as a new type of condition. 
% However, these methods simply inject camera conditions through a side input (like T2I-Adapter~\citep{Mou2024}) and neglect the inherent physical knowledge of camera pose, resulting in imprecise camera control, inconsistencies, and also poor interpretability.
% This raises a critical question: \textit{How can we effectively inject camera poses into the diffusion model to enhance its ability to physical world knowledge?}

% In this paper, we integrate geometric priors into the model and propose a novel framework for injecting camera poses. 
% We first identify shortcomings in existing attention mechanisms regarding the ability of tracking noised features across frames. As illustrated in Fig. \ref{fig:atten_compare}, separated spatial and temporal attention serves as an indirect form of 3D attention. The cross-frame interaction in temporal attention is confined to features at the same location in the image, rendering it ineffective for tracking significant movements resulting from large camera shifts. 
% 3D full attention is widely applied in advanced video diffusion models such as OpenSora~\citep{opensora} and CogVideoX~\citep{yang2024cogvideox}, due to its extensive receptive field. 
% From the novel perspective of the noised conditions mentioned in Fig.~\ref{fig:teaser}, the broad receptive field of 3D full attention allows it to access more noised conditions. However, noised conditions act as a double-edged sword; at high noise levels, deterministic information is often obscured by random noise, resulting in minimal reduction of uncertainty despite increased access. 
% As previously highlighted in Fig.~\ref{fig:teaser}, the quality of a condition is determined solely by its ability to reduce the model's uncertainty, rather than by the number of conditions it can access.

% To address these issues, we propose to apply pl\"ucker coordinates~\citep{Plucker1828} as absolute 3d ray embedding for implicit learning of 3d space and propose a novel epipolar attention mechanism that introduces an explicit constraint. 
% This ensures that even when feature tracking is compromised under high noise conditions, only features along the epipolar line are aggregated. 
% By doing so, our approach minimizes the search space and reduces potential errors, ultimately enhancing 3D consistency across frames and improving overall controllability.
%  Our explicit epipolar attention can also be regarded as a form of clean condition, as it reliably filters out features that do not conform to 3D geometry across varying noise levels, significantly enhancing controllability and stability. 
%  Additionally, inspired by \citet{darcet2024vision}, we incorporate register tokens into epipolar attention to address scenarios where there are no intersections between frames, often caused by rapid camera movements, dynamic objects, or occlusions.


% % For inference,  we propose a multiple classifier-free guidance scale to control images, text, and camera respectively. If needed, several forward passes can be combined into a single pass by absorbing the scales of image, text, and camera into the model input, similar to timestep conditioning according to~\citep{meng2023distillation}. For evaluation, we identify inaccuracies and instability in the current measurements of camera controllability due to the intrinsic limitations of SfM-based methods such as COLMAP~\citep{Schonberger2016}, which rely on identifying keypoint pairs and is quite challenging on generated videos with low resolution, high frame stride, and 3D inconsistencies. Considering the importance of accurate evaluation in this field, we establish a more robust, precise, and reproducible evaluation pipeline by implementing several enhancements. More details are provided in Section~\ref{Experiments}.

% We conduct experiments on the RealEstate10k dataset and evaluate video generation quality using FVD~\citep{Unterthiner2018}, as well as camera controllability metrics including RotError, TranError~\citep{Wang2024Motionctrl}, and CamMC~\citep{He2024Cameractrl}. The results demonstrate that the proposed epipolar attention mechanism across all noised frames significantly enhances geometric consistency and improves camera controllability.
% To facilitate further research, we will release all models trained on open-source frameworks such as DynamiCrafter, along with high-resolution checkpoints and training/evaluation codes, as soon as possible. To summarize, our key contributions are as follows:
% \begin{itemize}
%     \item We expand the camera-conditioned text-to-video diffusion model to support reference image at random frame and propose multiple guidance scale to allow for precise control for text, image, and camera, respectively. 
%     \item From a novel perspective of noised condition, we propose epipolar attention added with register token as a explicit constraint for modeling all cross-frame relationships. This addresses the issue of noised features tracking due to the movement of camera, leading to precise camera control and better 3d consistency. It also reduces the controllability instability due to randomness introduced by noise during generation. 
%     \item We point out and analyze the reasons for inaccurate measurement of camera controllability caused by the inherent limitations of SfM evaluator and re-establish a more robust, accurate and reproducible evaluation pipeline. Furthermore, we achieve the sota performance on RealEstate10K dataset and out-of-domain dataset.
% \end{itemize}



