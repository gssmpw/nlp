\begin{abstract}
Recent advancements in camera-trajectory-guided image-to-video generation offer higher precision and better support for complex camera control compared to text-based approaches. 
However, they also introduce significant usability challenges, as users often struggle to provide precise camera parameters when working with arbitrary real-world images without knowledge of their depth nor scene scale.
To address these real-world application issues, we propose RealCam-I2V, a novel diffusion-based video generation framework that integrates monocular metric depth estimation to establish 3D scene reconstruction in a preprocessing step. 
During training, the reconstructed 3D scene enables scaling camera parameters from relative to absolute values, ensuring compatibility and scale consistency across diverse real-world images. 
In inference, RealCam-I2V offers an intuitive interface where users can precisely draw camera trajectories by dragging within the 3D scene.
To further enhance precise camera control and scene consistency, we propose scene-constrained noise shaping, which shapes high-level noise and also allows the framework to maintain dynamic, coherent video generation in lower noise stages.
RealCam-I2V achieves significant improvements in controllability and video quality on the RealEstate10K and out-of-domain images. We further enables applications like camera-controlled looping video generation and generative frame interpolation. 
We will release our absolute-scale annotation, codes, and all checkpoints. Please see dynamic results in \url{https://zgctroy.github.io/RealCam-I2V}.

% At inference, an interactive 3D interface allows users to design camera paths intuitively, with real-time previews that establish the layout, movement, and trajectory direction in the video’s initial stages (high-noise regions). This reference video then guides generation, while a condition-guided approach refines control, filling occlusions and enhancing dynamics in mid- to low-noise regions.

% To address these real-world application issues, we present RealCam-I2V, a framework centered on constructing a 3D scene from a single reference image using monocular metric depth prediction—a core distinction of our approach. This reconstructed 3D scene is crucial, as it is utilized in both training and inference, effectively integrating training-based condition-guided methods with training-free techniques during inference.
% Recently, camera pose, as a user-friendly and physics-related condition, has been introduced into text-to-video diffusion model for camera control.
% However, existing methods simply inject camera conditions through a side input. These approaches neglect the inherent physical knowledge of camera pose, resulting in imprecise camera control, inconsistencies, and also poor interpretability.
% In this paper, we emphasize the necessity of integrating explicit physical constraints into model design.
% Epipolar attention is proposed for modeling all cross-frame relationships from a novel perspective of noised condition.
% % This ensures that features are aggregated from corresponding epipolar lines in all noised frames, overcoming the limitations of current attention mechanisms in tracking displaced features across frames, especially when features move significantly with the camera and become obscured by noise.
% % Additionally, 
% we introduce register tokens to handle cases without intersections between frames, commonly caused by rapid camera movements, dynamic objects, or occlusions.
% We achieve a 25.5\% improvement in camera controllability on RealEstate10K while maintaining strong generalization to out-of-domain images. Only 24GB and 12GB are required for training and inference, respectively. We plan to release checkpoints, along with training and evaluation codes. 
% % Dynamic videos are viewed at  \url{https://zgctroy.github.io/CamI2V}.

% Abstract

% We present RealCam-I2V, a novel framework for image-to-video (I2V) generation with precise, interactive camera control in real-world scenes. Unlike previous models that rely on trajectory-based control but lack absolute scale, RealCam-I2V reconstructs a 3D scene from a single reference image using monocular depth prediction. This enables users to intuitively draw camera paths with real-world scaling, overcoming limitations of prior datasets like RealEstate10K, which only support relative scale and limited trajectory flexibility. Our method enhances usability and controllability, allowing for complex, real-scale camera movements in video generation tasks. Extensive experiments show RealCam-I2V significantly improves control accuracy, robustness, and video quality across multiple state-of-the-art models. To facilitate further research, we will release our absolute-scale dataset, code, and checkpoints.

% Recent advancements in trajectory-based camera-controlled image-to-video (I2V) generation have demonstrated significant success, offering greater precision and support for more complex camera motions compared to text-based methods. However, while trajectory-based control enhances controllability, it reduces usability by requiring users to draw precise camera paths—an often challenging task. This difficulty arises because the trajectory annotations in widely used datasets, such as RealEstate10K, are derived using SLAM + COLMAP techniques on real-world videos, which are unattainable for users who only have a reference image. Additionally, due to a lack of absolute scale in physical space, I2V models trained on RealEstate10K support only relative-scale trajectories, leading to scale inconsistency when generating videos from user-defined trajectories, thus hindering real-world applicability.
% In this paper, we address these issues to enable complex camera trajectory generation in real-world scenarios. Our approach comprises two primary innovations: (1) using monocular image-based metric depth prediction to reconstruct a 3D scene from a user-provided reference image, combined with an interactive interface where users can intuitively draw camera paths within this reconstructed 3D space; and (2) rescaling the relative-scale dataset to align with the absolute scale of the reconstructed 3D scene, allowing us to train an I2V model that supports camera control with real-world scale fidelity.
% As a plugin to state-of-the-art models, our method demonstrates that training with absolute-scale datasets not only enables applications from zero to real-world deployment—such as camera-controlled looping video generation and generative frame interpolation—but also significantly improves camera control accuracy, robustness, and video generation quality. Finally, to support future research, we will open-source our absolute-scale annotated dataset along with all training, testing, and checkpoint data.
\end{abstract}