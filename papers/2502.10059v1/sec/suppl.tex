\clearpage
\setcounter{page}{1}
\maketitlesupplementary
\appendix

\textit{We highly recommend that reviewers refer to 
\textbf{index.html} provided in our supplementary files.}

\section{Dataset}

The camera trajectory of each video clip from RealEstate10K\footnote{\url{https://google.github.io/realestate10k/}}~\cite{zhou2018stereo} is first derived by SLAM methods at lower resolution with the field of view fixed at \(90^\circ\).
The authors then refine each of camera sequence using a structure-from-motion (SfM) pipeline, performing feature extraction, feature matching and global bundle adjustment successively.
Given the unawareness of global scene scale, the resulted camera poses of RealEstate10K are up to an arbitrary scale per clip.
For each frame the authors compute the \(5\)-th percentile depth among all point depths from that frameâ€™s camera. 
Computing this depth across all cameras in a video clip gives a set of near plane depths and the whole scene is scaled so that the \(10\)-th percentile of this set of depths is \(1.25\)m. 

While using RealEstate10K's scenes and camera trajectories during inference avoids scale issues within the dataset, challenges arise in more general cases. Specifically, when pairing out-of-domain images with either in-domain or out-of-domain trajectories, the inconsistencies between training and inference scales become evident. \textit{These inconsistencies make it impossible to generate realistic and controllable videos.}

The solution lies in reconstructing an absolute-scale scene for any given image. By leveraging metric depth predictor, we can reconstruct the absolute-scale 3D scene for the reference image. This absolute-scale scene bridges the gap between training and inference, enabling robust generalization to real-world applications. With this alignment, the model becomes capable of handling diverse combinations of images and trajectories, ensuring consistent and reliable performance across various scenarios.

% Therefore, we need to align relative scene scales of the dataset to an absolute and uniform scene scale, 


% For metric depth alignment of absolute scene scale, we run COLMAP~\cite{Schonberger2016} point triangulator on each video clip with fixed camera intrinsics and extrinsics directly from RealEstate10K, obtaining the sparse point cloud of the reconstructed scene.
% % We then project the point clouds back to the image plane to obtain pixel coordinates \((ud, vd, d)\) and thus calculate depth scales against the metric depth prediction \(d'\) of the pixel \((u, v)\) from Depth Anything V2~\cite{yang2024depth}.
% We then calculate per-point depth scale against the metric depth from depth predictor.
% We term the median value of per-point depth scales in a frame as the frame-level depth scale. 
% To make stable the training, we discard outliers of video clip whose maximum frame-level depth scale of the whole scene is among the top \(2\%\) for too small values or the last \(2\%\) for too large values, assuming sorted in ascending order.
% The same quantile filtering strategy is also applied on the minimum frame-level depth scales of video clips.
% It remains \(\sim\)~\(5,800\) video clips for training and another \(\sim\)~\(6,000\) for test.
% During training, we follow DynamiCrafter to sample \(16\) frames from each single video clip while perform resizing, keeping the aspect ratio, and center cropping to fit in our training scheme.
% We train the model with a random frame stride ranging from \(1\) to \(10\) and take random condition frame as data augmentation.
% % The depth scale of the condition frame is selected as the scene-level depth scale, for the scene scale of RealEstate10K distribution to align with absolute depth priors from Depth Anything V2.
% We fix the frame stride to \(8\) and always use the first frame as the condition frame for inference.


\section{Depth Predictor}

We choose the metric depth version of Depth Anything V2\footnote{\url{https://github.com/DepthAnything/Depth-Anything-V2/tree/main/metric_depth}}~\cite{depth_anything_v2} as the metric depth predictor.
Compared to their basic versions, the authors fine-tune the pre-trained encoder on synthetic datasets for indoor and outdoor metric depth estimation.
The indoor model is capable of monocular metric depth estimation within a maximum depth of \(20\)m.
We choose Large as the model size, which has \(335.3\)M parameters, and the indoor version.
The scene scale of our model is aligned to the metric depth space of Depth Anything V2 Large Indoor, \ie absolute scene scale.

\section{Training}

We choose DynamiCrafter\footnote{\url{https://github.com/Doubiiu/DynamiCrafter}}~\cite{Xing2023} as our image-to-video (I2V) base model.
We trained proposed method on \(4\) publicly accessible variants of DynamiCrafter, namely \texttt{256}, \texttt{512}, \texttt{512\_interp} and \texttt{1024}.
We conduct ablation study on resolution \(256 \times 256\), due to the limitation of computing resource.
For resolution \(256 \times 256\), we train all models on \(\epsilon\)-prediction with effective batch size \(64\) on \(8\) NVIDIA H100 GPUs for \(50,000\) steps, taking about \(25\) hours.
For resolution \(512 \times 320\) and \(1024 \times 576\), we train RealCam-I2V on \(v\)-prediction while enable \texttt{perframe\_ae} and \texttt{gradient\_checkpoint} to reduce peak GPU memory consumption.
We apply the Adam optimizer with a constant learning rate of \(1 \times 10^{-4}\) with mixed-precision fp16 and DeepSpeed ZeRO-1.

For MotionCtrl~\cite{Wang2024Motionctrl} and CameraCtrl~\cite{He2024Cameractrl}, we reproduce all results on DynamiCrafter for fair comparison.
For CamI2V~\cite{zheng2024cami2v}, we implement hard mask epipolar attention and set \(2\) register tokens, aligned with the original paper. 
In quantitative comparison and ablation study, we set fixed text image CFG to \(7.5\) and camera CFG to \(1.0\).


\section{Camera Keyframe Interpolation}

\begin{figure}[!t]
    \centering
    % \vspace{-7mm}
    \includegraphics[width=0.8\linewidth]{figs/camera_interp.pdf}
    % \vspace{-3mm}
    \caption{Camera Trajectory Interpolation.
    }
    \label{fig:camera_interp}
    \vspace{-4mm}
\end{figure}

In real-world applications, user-provided camera trajectories often consist of a limited number of keyframes (\eg, \(4\) keyframes). To ensure smooth and continuous motion across the trajectory while adhering to the user's input, we perform linear interpolation in SE(3) space to expand the trajectory to a higher number of frames (\eg, 16 interpolated frames), as shown in~\cref{fig:camera_interp}. This step ensures that our model generates consistent and visually coherent videos without compromising the accuracy of user-defined camera movements.