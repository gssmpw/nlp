\section{Experiments}
\label{sec:experiments}



\subsection{Setup}



\noindent\textbf{Dataset.} We train our model on RealEstate10K~\cite{zhou2018stereo}, which contains \(\sim\)~\(70,000\) video clips with well-annotated camera poses. 
For metric depth alignment of absolute scene scale, we run COLMAP~\cite{Schonberger2016} point triangulator on each video clip with fixed camera intrinsics and extrinsics directly from RealEstate10K, obtaining the sparse point cloud of the reconstructed scene.
% We then project the point clouds back to the image plane to obtain pixel coordinates \((ud, vd, d)\) and thus calculate depth scales against the metric depth prediction \(d'\) of the pixel \((u, v)\) from Depth Anything V2~\cite{yang2024depth}.
We then calculate per-point depth scale against the metric depth from depth predictor.
We term the median value of per-point depth scales in a frame as the frame-level depth scale. 
To make stable the training, we discard outliers of video clip whose maximum frame-level depth scale of the whole scene is among the top \(2\%\) for too small values or the last \(2\%\) for too large values, assuming sorted in ascending order.
The same quantile filtering strategy is also applied on the minimum frame-level depth scales of video clips.
It remains \(58,000\) video clips for training and another \(6,000\) for test.
During training, we follow DynamiCrafter to sample \(16\) frames from each single video clip while perform resizing, keeping the aspect ratio, and center cropping to fit in our training scheme.
We train the model with a random frame stride ranging from \(1\) to \(10\) and take random condition frame as data augmentation.
% The depth scale of the condition frame is selected as the scene-level depth scale, for the scene scale of RealEstate10K distribution to align with absolute depth priors from Depth Anything V2.
We fix the frame stride to \(8\) and always use the first frame as the condition frame for inference.

\noindent\textbf{Implementation Details.} We choose DynamiCrafter~\cite{Xing2023} as our image-to-video (I2V) base model and seamlessly integrate proposed RealCam-I2V into it as a plugin. 
% For fair comparision, we also make reproduction work of MotionCtrl~\cite{Wang2024Motionctrl}, CameraCtrl~\cite{He2024Cameractrl} and CamI2V~\cite{zheng2024cami2v} on DynamiCrafter, since their public accessible version is not available or based on other base models.
For metric depth predictor, we choose Depth Anything V2 \cite{depth_anything_v2} Large Indoor, which is fine-tuned on metric depth estimation.
During depth-aligned training, we freeze all parameters of the base model and the depth predictor, while only parameters of proposed method are trainable. More details are listed in dataset section of Appendix.
% We project Pl\"ucker embedding along with depth map into the denoising U-Net by a pose encoder similar to the architecture in CameraCtrl.
% We set \(2\) extra register tokens for the epipolar module to attend when no relevant pixels are on the epipolar line. 
We supervise \(\epsilon\)-prediction on the model of \(256 \times 256\) resolution and \(v\)-prediction on the model of \(512 \times 320\) resolution respectively, following the pre-training scheme of DynamiCrafter.
We apply the Adam optimizer with a constant learning rate of \(1 \times 10^{-4}\) with mixed-precision fp16 and DeepSpeed ZeRO-1.
We train proposed method and variants on \(8\) NVIDIA H100 GPUs with an effective batch size of \(64\) for \(50,000\) steps. More details are listed in implementation section of Appendix.



\input{tables/quantative_comparision_with_sota.tex}

\begin{table*}[ht!]
  \centering
  % \vspace{-.1cm}
    \resizebox{1.0 \textwidth}{!}{
        % \Huge % 这里调整整个表格字体
        \renewcommand{\arraystretch}{1.0} % 调整行间距
        \begin{tabular}{|l|c|cccccc|ccc|}
            \toprule
            \multirow{4}{*}{Method} &  & \multicolumn{6}{c|}{Quality Score} & \multicolumn{3}{c|}{I2V Score} \\
              & Total  & subject & background & motion & dynamic & aesthetic  & imaging & i2v & i2v  & camera \\
              & Score   & consistency & consistency & smoothness & degree & quality  & quality &  subject  & background  & motion \\
              % &                                   & \uparrow & \uparrow & \uparrow & \uparrow  & \uparrow &  \uparrow  & \uparrow  & \uparrow & \uparrow \\
            % \Xhline{0.75pt}
            \midrule
            % \rowcolor{gray!15} \textbf{UNet-based} & & & & & & & & & & \\
            Dynamicrafter (official)              & 85.25 & 94.74 & 98.29 & 97.83 & 40.57 & 58.71 & 62.28 & 97.05 & 97.56 & 20.92 \\
            \midrule
            Dynamicrafter (reproduced)              & 84.22 & \textbf{95.44} & \textbf{98.54} & \textbf{98.08} & 34.15 & 58.98 & 62.35 & 95.93 & \textbf{95.43} & 22.67 \\
            $+$ CamI2V              & 83.77 & 91.24 & 96.06 & 97.35 & 36.99 & 58.32 & 63.03 & 94.71 & 92.65 & 91.48 \\
            % + + Absolute Scale (scale0.5)              & 84.33 & 90.99 & 96.23 & 97.36 & 46.75 & 58.37 & 62.91 & 94.73 & 93.44 & 85.85 \\
            % + + Absolute Scale (scale0.6)              & 84.36 & 90.99 & 96.23 & 97.36 & 46.75 & 58.37 & 62.91 & 94.73 & 93.44 & 87.16 \\
            $++$ Absolute Scale              & 84.37 & 90.99 & 96.23 & 97.36 & \textbf{46.75} & 58.37 & 62.91 & 94.73 & 93.44 & 87.42 \\
            % + + + Noise Shaping (ours,scale0.5)             & 85.63 & 94.86 & 97.89 & 97.81 & 25.20 & 60.05 & 63.04 & 96.49 & 95.59 & 93.32 \\
            % + + + Noise Shaping (ours,scale0.6)             & 85.73 & 94.48 & 97.69 & 97.73 & 31.30 & 60.02 & 63.07 & 96.26 & 95.49 & 93.32 \\
            $+++$ Noise Shaping (ours)             & \textbf{85.71} & 93.96 & 97.58 & 97.66 & 35.77 & \textbf{59.79} & \textbf{63.08} & \textbf{96.14} & 95.27 & \textbf{93.32} \\
            \bottomrule
      \end{tabular}
    }
  \vspace{-.1cm}
  \caption{Evaluation results on Vbench-I2V~\cite{huang2024vbench++}, a widely used benchmark suite with dynamic scenes and various types. Due to the efficient frozen parameters finetuning on Dynamicrafter, our method obtains the ability of camera control but decreases little in other metrics despite only training on static RealEstate10K.}
  \label{tab:rebuttal_Vbench-I2V}
  \vspace{-.1cm}
\end{table*}

\subsection{Metrics}

We follow previous works~\cite{Wang2024Motionctrl, He2024Cameractrl, xu2024camco, zheng2024cami2v} to evaluate camera-controllablity by RotErr, TransErr and CamMC on their estimated camera poses using structure-from-motion (SfM) methods, \eg COLMAP~\cite{Schonberger2016} and GLOMAP~\cite{pan2024glomap}.
We convert the camera pose of each frame in a video clip to be relative to the first frame as canonicalization.
We denote the \(i\)-th frame relative camera-to-world matrix of ground truth as \(\{R^{3\times3}_i,T^{3\times1}_i\}\), and that of generated video as \(\{\tilde{R}^{3\times3}_i,\tilde{T}^{3\times1}_i\}\).
We randomly select \(1,000\) samples from test set for evaluation.
% All metrics are measured under \(16\) frames at the resolution of \(256 \times 256\).
We sum up per-frame errors as the scene-level result for camera metrics.
Inspired by ~\citet{zheng2024cami2v}, we repetitively conduct \(5\) individual trials on each video clips for camera-control metrics to reduce the randomness introduced by SfM tools.
Metrics of one video clip are averaged on successful trials at first for later sample-wise average to get final results.
% It is due to the instability of keypoint matching and sparse reconstruction process caused by the inherent defects of generative videos, including low resolution, large motion dynamics, object distortion, 3D inconsistency, lighting flaws and \etc.

\noindent\textbf{RotError.} We calculate camera rotation errors by the relative angle between generated videos and ground truths in radians for rotation accuracy. 
\begin{equation}
    {\rm RotErr} = \sum_{i=1}^n \arccos{\frac{\mathop{\rm tr}(\tilde{R}_i R_i^{\rm T}) - 1}{2}}    
\end{equation}

\noindent\textbf{TransError.} For relative TransErr, we perform scene scale normalization on the camera positions of each video clip.
The scene scale of generated video \(\tilde{s}_i\) and grouth truth \(s_i\) are individually calculated as the \({\cal L}_2\) distance from the first camera to the farthest one for each video clip.
For absolute TransErr, we normalize both the video clip to the scene scale of ground truth video, \ie \(\tilde{s}_i=s_i\).
\begin{equation}
    {\rm TransErr} = \sum_{i=1}^n{\left\Vert \frac{\tilde{T}_{i}}{\tilde{s}_i} - \frac{T_{i}}{s_i} \right\Vert_2}
\end{equation}

\noindent\textbf{CamMC.} We perform the same scene scale normalization for relative metrics and absolute metrics as TransError, and evaluate the overall camera pose accuracy by directly calculating \({\cal L}_2\) similarity on camera-to-world matrices.
\begin{equation}
    {\rm CamMC} = \sum_{i=1}^n{\left\Vert \begin{bmatrix} \tilde{R}_i \,\Big|\, \dfrac{\tilde{T}_{i}}{\tilde{s}_i} \end{bmatrix}^{3\times4} - \begin{bmatrix} R_i \,\Big|\, \dfrac{T_{i}}{s_i} \end{bmatrix}^{3\times4} \right\Vert_2}
\end{equation}

% \noindent\textbf{Absolute Metrics.} Worth mentioning, above SfM methods are unaware of absolute scene scale of reconstruction, leading to only relative estimation of camera poses.
% To validate our design, we perform depth-alignment on generated videos and ground-truth videos respectively using the same protocol in Sec.~\ref{sec:depth_aligned_training}.
% Thus we can convert the relative camera estimation of COLMAP to the absolute scene scale by the metric depth prediction from Depth Anything V2.
% We rather canonicalize both the camera poses of generated videos and ground-truth videos to the scene scale of the latters.
% We term the metrics after depth alignment as Absolute TransErr and Absolute CamMC, while the original ones as Relative TransErr and Relative CamMC.

\noindent\textbf{FVD.} We also assess the visual quality of generative videos by the distribution distance FVD~\cite{Unterthiner2018} between generated videos and ground-truths.


\subsection{Comparison with SOTA Methods}

We compare our proposed method against models that either lack camera-condition training (DynamiCrafter~\cite{Xing2023}) or incorporate camera-condition training, namely DynamiCrafter+MotionCtrl~\cite{Wang2024Motionctrl} (\(3\times4\) camera extrinsics), DynamiCrafter+CameraCtrl~\cite{He2024Cameractrl} (Plücker embedding constructed from camera extrinsics and intrinsics as side input), and DynamiCrafter+CamI2V~\cite{zheng2024cami2v} (current SOTA using Plücker embedding and epipolar attention between all frames), as shown in~\cref{tab:comparison}.

Our method demonstrates significant improvements in visual quality (FVD) and camera control metrics (TransErr, RotErr, CamMC), particularly on absolute metrics. Specifically, absolute camera metrics improve by \(+27\%\), relative camera metrics by \(+14\%\), and FVD by \(+13\%\).
However, these improvements are not fully captured by the RealEstate10K dataset, which has limitations on movement speed and contains mostly static scenes.
It's strongly recommended to view the dynamic visualizations in the supplementary materials for a more comprehensive evaluation.


% Additionally, we achieve:
% \begin{itemize}
%     \item Real-world application breakthrough. We address challenges such as training-inference scale inconsistency and low usability, ensuring greater robustness and compatibility for real-world images.
%     \item Superior performance in complex camera motions. Our method performs better in large or rapid movements, rotations, and dynamics. 
% \end{itemize}

% Considering the difficulty of fully capturing the visual effects in the form of images within the paper.
% It's strongly recommended to view the dynamic visualizations in the supplementary materials for a more comprehensive evaluation.

% We compare with model without camera-condition training (DynamiCrafter), model with camera-condition training such as DynamiCrafter+MotionCtrl (3x4 RT or camera extrinsics), DynamiCrafter+CameraCtrl (plucker embedding constructed from camera extrinsics + intrinsics), DynamiCrafter+CamI2V (current sota using plucker embedding + epipolar attention between all frames).

% Significant improvement on video quality (FVD) and camera control (TransErr, RotErr, CamMC), especially on absolute metrics. Specifically, xxx \% improvement.

% In addition,

% 1. Breakthrough in real-world applications, solving the challenges such as training-inference scale inconsistency and low usability.

% 2.  Even better results in large/rapid movement, rotation, dynamics but not shown in RealEstate10K. Because RealEstate10K has a limit of movement speed and is mostly static. 

% Dynamic Visualizations comparision can be easily seen in videos provided in supplementary. It's strongly recommended to check the dynamics in the format of video.







\subsection{Ablation Study}
\input{tables/ablation_study}
\begin{figure}[!ht]
    \centering
    \vspace{-2mm}
    \includegraphics[width=\linewidth]{figs/ablation-a.png}
    % \vspace{-3mm}
    \caption{
    (a) \textbf{Without Scene-Constrained Noise Shaping.} (b) \textbf{With Scene-Constrained Noise Shaping.} The left shows failures in large movements without scene-constrained noise shaping, while the right illustrates a loss of dynamics when noise shaping is extended to lower noise stages.} 
    \label{fig:ablation_visualization}
    % \vspace{-4mm}
\end{figure}

\begin{figure}[!ht]
    \centering
    \vspace{-2mm}
    \includegraphics[width=\linewidth]{figs/rebuttal/rebuttal_black_points.png}
    % \vspace{-3mm}
    \caption{
    Without kernel size$\ge$3 in noise shaping, unvisible regions will be wrongly pasted to the generated video. } 
    \label{fig:rebuttal_kernel_size}
    % \vspace{-4mm}
\end{figure}


\noindent\textbf{Effect of absolute-scale training only.}
% 1. compared to relative-scale training, absolute-scale training improvement on metrics, especially the absolute metrics. 
% 2. absolute-scale tackle the real-world application issues by solving the training-inference scale inconsistency, allowing for interactive action in a unified scale and enhancing robustness, compatibability for real-world images
As shown in~\cref{tab:ablation}, compared to relative-scale training, absolute-scale training yields notable improvements, especially on absolute metrics. 
% For instance, CamI2V achieves around \(9\%\) gain on absolute TransErr and absolute CamMC. 
% These improvements highlight the effectiveness of training on an absolute scale.
It implies that models trained on absolute-scale data can more accurately capture true-to-scale translations and better understand camera rotations within a realistic spatial framework.
% Absolute-scale training directly tackles real-world application issues by resolving the training-inference scale inconsistency. 
The absolute scene scale enhances robustness and compatibility, ensuring that the framework adapts effectively to real-world images and applications.
This approach allows for interaction within a unified scale, enabling intuitive user control over camera actions. 

% for all baselines, we see a reduction in most of metrics with absolute-scale training compared to relative-scale training, especially the absolute metrics.
% This improvement indicates that absolute-scale training helps the model better understand camera rotation within a realistic spatial framework, which is essential for accurately simulating camera movements in 3D space.
% We see a large reduction in Absolute RotErr and CamMC, which implies that models trained on absolute-scale data can more accurately capture true-to-scale translations and better understand camera rotations within a realistic spatial framework, making the generated video trajectories more consistent with real-world scene scales. 
% This is especially valuable in applications requiring precise camera path control, such as augmented reality or visual effects.
% It also suggests that videos generated under absolute-scale training have a closer distribution to real-world video samples, meaning better visual quality and consistency with real-world footage.




\noindent\textbf{Effect of absolute-scale training + scene-constrained noise shaping.}
Adding scene-constrained noise shaping to a model trained with absolute-scale yields substantial gains in video quality and camera controllability.
% When scene-constrained noise shaping is applied to a model trained under absolute-scale conditions, it brings significant enhancements to both video generation quality and camera controllability. 
This improvement is evident across both camera metrics and FVD. %, \eg RotErr of CamI2V improves from 0.4596 to 0.4052. 
The synergy of absolute-scale training and scene-constrained noise shaping ensures robust and precise control in diverse scenarios.
As illustrated in~\cref{fig:ablation_visualization}, this combined approach delivers noticeably better dynamics compared to using scene-constrained noise shaping alone. 
Large camera movements, rotations, and rapid transitions, which previously struggled to maintain consistency and realism, now work seamlessly. 
This improvement underscores the strength of integrating absolute-scale training with noise shaping for complex motion scenarios.


% As shown in~\cref{fig:ablation_visualization}, 

% 1. Better Dynamics compared to scene-constrained noise shaping only

% 2. large movement, rotation, rapid movement, now works

% As shown in~\cref{tab:ablation}, scene-constrained noise shaping and be added onto model trained under absolute-scale setting. 

% 2. Significant improvement on video generation quality and camera controllability (both relative and absolute metrics)





\noindent\textbf{Effect of scene-constrained noise shaping only.}
As shown in~\cref{tab:ablation}, scene-constrained noise shaping can be used as the sole method for camera control when applied to a base model not trained with any camera conditions.
It provides notable improvements in metrics, exemplified by nearly \(50\%\) reduction on DynamiCrafter. 
However, this method underperforms compared to the combined method with absolute-scale training.
% Specifically, CamI2V achieves only about \(\sfrac14\) of DynamiCrafter's error, demonstrating the enhanced precision.
It also introduces challenges in parameter selection. 
Applying shaping only in the high-noise phase limits camera contrl in lower noise stages, while extending shaping to mid-noise phase can suppress dynamic elements, resulting in static video output.
% As illustrated in~\cref{fig:ablation_visualization}, using noise shaping alone results in less dynamic motion compared to the combined approach with absolute-scale training and noise shaping. 
This limitation affects the fluidity and responsiveness of generated camera movements, making the combination approach preferable for applications requiring natural dynamics.

% As shown in~\cref{tab:ablation}, scene-constrained noise shaping can be used as the only way for camera control when the model is base model and not trained with any camera condition like camera parameters. 

% 1. Significant improvement on video generation quality and camera controllability (both relative and absolute metrics) but worse than absolute-scale training + scene-constrained noise shaping (DynamiCrafter+scene-constrained noise shaping  vs. CamI2V+absoluta-scale training+scene-constrained noise shaping.

% 2. However, scene-constrained noise shaping only makes it hard for parameters choice. Shaping only in higher noise lacks camera control for mid and low noise. Shaping for longer steps (higher and mide) leads to elimination of dynamics and the video becomes static.



% As shown in~\cref{fig:ablation_visualization}, 

% 1. Worse Dynamics compared to absolute-scale + scene-constrained noise shaping




\subsection{Application}
\begin{figure}
% \begin{figure*}
% \vspace{-2mm}
  \centering
  % \begin{subfigure}{0.4975\linewidth}
    \includegraphics[width=1.0\linewidth]{figs/application.pdf}
    % \caption{\textbf{Visualization of applications.}}
    \label{fig:short-a}
  % \end{subfigure}
  % \hfill
  % \begin{subfigure}{0.4975\linewidth}
  %   \includegraphics[width=1.0\linewidth]{figs/application.pdf}
  %   \caption{examples of application.}
  %   \label{fig:short-b}
  % \end{subfigure}
  \caption{\textbf{Visualization of various applications}. Best viewed as dynamic videos in the supplementary materials.}
  \vspace{-4mm}
  \label{fig:application_visualization}
\end{figure}
% \end{figure*}


% We demonstrate the versatility of our method through visualization results across various applications. These include videos generated at resolutions of \(512 \times 320\) and \(1024 \times 576\) with camera control under complex scenarios, such as large movements and rotations. Additionally, we showcase results for camera-controlled loop video generation, generative frame interpolation, and smooth scene transitions. More extensive results are provided in the supplementary materials.

% Our approach achieves two significant breakthroughs:
% 1. **Real-World Application Breakthrough**: By addressing challenges such as training-inference scale inconsistency and low usability, our method ensures greater robustness and compatibility with real-world images.
% 2. **Superior Performance in Complex Camera Motions**: Our framework excels in handling large and rapid movements, rotations, and dynamic scene transitions, surpassing existing methods in these challenging scenarios.

% Considering the limitations of static images in conveying dynamic visual effects, we strongly encourage reviewers to refer to the videos in the supplementary materials for a comprehensive evaluation of our results.



As illustrated in~\cref{fig:application_visualization}, we demonstrate the versatility of our method through visualization results across various applications, including videos generated at resolutions of \(512 \times 320\) and \(1024 \times 576\) with camera control under complex scenarios, such as large movements or rotations. 
Additionally, our results include camera-controlled loop video generation, generative frame interpolation, and smooth scene transitions, highlighting the robustness of our approach. 
These visualizations showcase two major breakthroughs: first, our method achieves a real-world application breakthrough by addressing challenges like training-inference scale inconsistency and low usability, ensuring improved robustness and compatibility with real-world images. 
Second, our framework exhibits superior performance in complex camera motions, handling large and rapid movements, rotations, and dynamics more effectively than existing methods. 
More extensive results are provided in the supplementary materials.
% Since the dynamic visual effects are challenging to fully capture through static images, it is strongly recommended to view the videos provided in the supplementary materials for a more comprehensive evaluation.



% We show visualization results of different application including videos at resolution \(512\times320\) or \(1024\times576\) with camera control under complex setting, large movement setting, large rotation setting. We also show the results of camera-controlled loop video generation, generative frame interpolation, and transition.
% For more results are listed in supplementary materials.
% Additionally, we achieve:
% \begin{itemize}
%     \item Real-world application breakthrough. We address challenges such as training-inference scale inconsistency and low usability, ensuring greater robustness and compatibility for real-world images.
%     \item Superior performance in complex camera motions. Our method performs better in large or rapid movements, rotations, and dynamics. 
% \end{itemize}

% Considering the difficulty of fully capturing the visual effects in the form of images within the paper.
% It's strongly recommended to view the dynamic visualizations in the supplementary materials for a more comprehensive evaluation.

% In the application, there are several significant breakthrough:

% 1. real-world applications thanks to absolute-scale training

% 2. being able to generate large/rapid movement, rotation thaks to scene-constrained noise shaping based on absolute-scale trained model

% In \cref{fig:application_visualization}, 

% More results are listed in supplementary.



