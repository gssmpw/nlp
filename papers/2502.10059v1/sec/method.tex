
\begin{figure*}[!t]
    \centering
    \vspace{-2mm}
    \includegraphics[width=1.0\linewidth]{figs/pipeline.pdf}
    \vspace{-4mm}
    \caption{\textbf{Pipeline.} In training, we align  camera parameters in RealEstate10K from relative scale to absolute scale. In inference, we use metric depth estimation method to construct a 3d point cloud for users to interactively drawing camera traces.
    }
    \label{fig:pipeline}
    \vspace{-1mm}
\end{figure*}
\begin{figure*}[!t]
    \centering
    \vspace{-1mm}
    \includegraphics[width=1.0\linewidth]{figs/rebuttal/align.pdf}
    \vspace{-4mm}
    \caption{3D point cloud reconstructed from metric depth estimation (RGB) is robust and unified, whereas the SFM-based reconstruction by methods like COLMAP (Yellow) used in RealEstate10K annotations is in relative scale and may vary across images. Aligning these two 3D scenes enables the transformation from relative to absolute scale (real-world scale).
    }
    \label{fig:align}
    \vspace{-2mm}
\end{figure*}


\section{Method}
\label{sec:method}



% Our proposed RealCam-I2V consists of 3 main components to achieve camera-controllable image-to-video generation for real-world applications, as shown in Fig.~\ref{fig:pipeline}. 
% In Sec.~\ref{sec:3d_reconstruction}, we utilize off-the-shelf 3D reconstructors to obtain 3D point clouds, which bridge the gap between training and inference. 
% In Sec.~\ref{sec:depth_aligned_training}, we describe how to align the relative scene scale of RealEstate10K to the absolute scene scale of metric depth predictor for training. 
% In Sec.~\ref{sec:inference_with_ui}, we leverage monocular 3D reconstruction and provide an interactive UI for users to plot absolute camera trajectories in the metric depth space, thus leverage RealCam-I2V for camera-controllable video generation.




\subsection{Metric Depth Estimation for 3D Reconstrcution}
\label{sec:3d_reconstruction}
To obtain a depth map from a given input image, we use a metric depth predictor \( f_{\text{depth}} \), which takes the RGB image \( I \) as input and outputs the corresponding depth map \( D(u, v) \). 
The prediction process is formulated as:
\[
D(u, v) = f_{\text{depth}}(I),
\]
where $I$ is the input RGB image and $ D(u, v) $ is the predicted depth value for each pixel at coordinates $ (u, v) $. This predicted depth map \( D(u, v) \) serves as the foundation for projecting the image into 3D space, allowing us to construct a point cloud in the camera coordinate system.
The camera intrinsics matrix \( K \) is defined as:
\[
K = 
\begin{bmatrix}
f_x & 0 & c_x \\
0 & f_y & c_y \\
0 & 0 & 1
\end{bmatrix},
\]
where \( f_x \) and \( f_y \) are the focal lengths along the \( x \) and \( y \) axes,
\( (c_x, c_y) \) is the principal point of the camera.
Given a depth map \( D(u, v) \), 
the projected 3D coordinates in the camera coordinate system, \( \mathbf{p}_c = (x_c, y_c, z_c)^T \), are computed as:
\[
\mathbf{p}_c = D(u, v) \cdot K^{-1} \cdot 
\begin{bmatrix} 
u \\ 
v \\ 
1 
\end{bmatrix}.
\]
Here \( \begin{pmatrix} u, v, 1 \end{pmatrix} \) represents the homogeneous coordinates of the pixel,
  \( K^{-1} \) is the inverse of the intrinsic matrix, which maps pixel coordinates to normalized image coordinates.
By applying this transformation to all pixels in the depth map, we obtain a set of 3D points \( \{ \mathbf{p}_c \} \) in the camera coordinate system. 



\subsection{Absolute-Scale Training}
\label{sec:depth_aligned_training}
\noindent\textbf{Camera-controlled Image-to-Video Model.} Instead of directly modeling the video \( x \), the latent representation \( z = \mathcal{E}(x) \) is used for training. The diffusion model \( \epsilon_\theta \) learns to estimate the noise \( \epsilon \) added at each timestep \( t \), conditioned on both a text prompt \( c_\text{txt} \), a reference image \( c_\text{img} \), and camera condition \( c_\text{cam} \), with \( t \in \mathcal{U}(0, 1) \). The training objective simplifies to a reconstruction loss defined as:
\begin{equation}
    \mathcal{L} = \mathbb{E}_{z, c_\text{txt}, c_\text{img}, c_\text{cam}, \epsilon, t} 
    \left[ \left\| \epsilon - \epsilon_\theta\left(z_t, c_\text{txt}, c_\text{img}, c_\text{cam}, t\right) \right\|_2^2 \right],
\label{eq:diffusion_loss}
\end{equation}
where \( z \in \mathbb{R}^{F \times H \times W \times C} \) represents the latent code of a video, with \( F, H, W, C \) corresponding to frame count, height, width, and channel dimensions. The noise-corrupted latent code \( z_t \), derived from the ground-truth latent \( z_0 \), is expressed as:
\begin{equation}
    z_t = \alpha_t z_0 + \sigma_t \epsilon,
\end{equation}
where \( \sigma_t = \sqrt{1 - \alpha_t^2} \). Here, \( \alpha_t \) and \( \sigma_t \) are hyperparameters governing the diffusion process.

\noindent\textbf{Aligning from Relative Scale to Absolute Scale.}
To convert camera extrinsics from world-to-camera to an absolute-scale camera-to-world representation, we defines that the world-to-camera extrinsics matrix \( F_{\text{w2c}} \in \mathbb{R}^{4 \times 4} \) is inverted to obtain the corresponding camera-to-world matrix:
\[
F_{\text{c2w}} = F_{\text{w2c}}^{-1}.
\]
 
To express the transformations relative to the first frame, each \( F_{\text{c2w}} \) is left-multiplied by the camera-to-world matrix of the inverse of first frame \( F_{\text{c2w, 1}} \):
\[
c_{\text{cam}} = F_{\text{c2w,1}}^{-1} \cdot F_{\text{c2w}}.
\]
Here, \( c_{\text{cam}} \in \mathbb{R}^{F \times 4 \times 4} \) represents the camera-to-world transformations aligned relative to the first frame. However, the translation component of \( c_{\text{cam}} \) remains in a relative scene scale.
To convert the relative translation to an absolute scale, we align the metric 3D point cloud reconstructed by Depth Anything with the 3D point cloud reconstructed by COLMAP (Structure-from-Motion), as shown in~\cref{fig:align}. The alignment process yields a scale factor \( a \) and is applied to the translation component of \( c_{\text{cam}} \), resulting in an absolute-scale camera-to-world transformation:
\[
c_{\text{cam}}^{\text{abs}} = 
\begin{bmatrix}
R & a \cdot T \\
0 & 1
\end{bmatrix},
\]
where \( R \) is the rotation matrix, \( T \) is the relative translation vector.
The resulting \( c_{\text{cam}}^{\text{abs}} \in \mathbb{R}^{F \times 4 \times 4} \) represents the camera-to-world transformations with absolute scene scale, enabling robust and accurate real-world applications.

\subsection{Scene-Constrained Noise Shaping}
\label{sec:inference_with_ui}
Inspired by SDEdit~\cite{meng2021sdedit} and DDIM~\cite{song2020denoising} inversion, noised features $z_t$ can be used for shaping the  layout, camera control of the entire image, especially at timestep with high-level noise.
We propose \textit{scene-constrained noise shaping}, which utilizes preview videos generated along user-defined trajectories in the interactive 3D scene. Each frame of the preview video is treated as a reference frame and provided to the generation process during the high-noise stage. The reference frame's pixels are overlaid onto the model-predicted \( z_0 \) to achieve the shaping effect.

Next, we detail the process for selecting the pixels to be referenced. As illustrated in~\cref{fig:noise_shaping}, the primary criterion is that a pixel must be visible under the current camera viewpoint in the preview video. To mitigate issues such as holes caused by inaccurate depth predictions, we apply an additional filtering rule: if a visible pixel's \( k \times k \) neighborhood contains any invisible pixels, it is considered to lie on an object's edge and potentially affected by depth prediction errors. Such pixels are excluded from selection.
Finally, we define the noise shaping process with the following formula:
\[
z_\text{predict} = \text{mask} \cdot z_\text{preview} + (1 - \text{mask}) \cdot z_\text{predict},
\]
where the \textit{mask} identifies the selected reference pixels, \( z_\text{preview} \) represents the latent features from the preview video, and \( z_\text{predict} \) is the model-predicted latent representation.
\begin{figure}[!t]
    \centering
    % \vspace{-7mm}
    \includegraphics[width=0.9\linewidth]{figs/noise_shaping.pdf}
    % \vspace{-3mm}
    \caption{Pixels selected for scene-constrained noise shaping, where they are pasted onto the clean part (predicted $z_0$) of a noised latent $z_t$, typically at high noise levels $0.9<t<1.0$ is enough for camera control and maintain dynamics.
    }
    \label{fig:noise_shaping}
    \vspace{-4mm}
\end{figure}

% \subsection{Random Cond Frame Training}
% \lipsum[1]
\begin{figure}[!t]
    \centering
    % \vspace{-7mm}
    \includegraphics[width=0.9\linewidth]{figs/concatenation_mode.pdf}
    % \vspace{-3mm}
    \caption{Concatenation for different tasks, including basic mode, interpolation mode, and continuation mode.
    }
    \label{fig:concatenation_mode}
    \vspace{-3mm}
\end{figure}

\subsection{Interploatation, Loop and Continueation}
To support different tasks, including interpolation, looping, and continuation for long video generation, we train video diffusion model with different input concatenation mode, as shown in~\cref{fig:concatenation_mode}.
Given a video latents \( z \in \mathbb{R}^{F \times H \times W \times C} \), we define the noised latents of $f$-th frame at timestep $t$ as $z_{t}^{f}$. We then select $i$-th clean frame as the condition frame $z^{i}_0$. For interpolation mode, we define $z^{f-1}_0$ as the end condition frame. For continuation mode, we define all $1~i$-th as condition frame.


