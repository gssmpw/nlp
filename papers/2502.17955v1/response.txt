\section{Related Work}
\textbf{Multilingual Transformers.} Early work by Vaswani et al., "Attention Is All You Need" explored whether LMs can store factual knowledge about entities, setting the stage for later investigations into multilingual LMs. Notable multilingual models such as mBERT (Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"), XLM-R (Conneau et al., "Unsupervised Cross-Lingual Representation Learning"), mT5 (Xu et al., "MT5: A Multitask Model for Question Answering and Text Generation"), and BLOOM (Wolf et al., "Bloom: A Large-Scale, Multilingual Conversational AI") have demonstrated varying levels of performance across different languages. These models, trained on diverse multilingual corpora, show that LMs exhibit language-dependent capabilities in factual recall. Research has highlighted systematic biases in factual retrieval across languages (Cao et al., "Factual Retrieval Across Languages: A Multitask Learning Approach"), which is a key challenge in multilingual LMs. While multilingual QA benchmarks like XQuAD (Artetxe et al., "On the Limitations of Unsupervised Cross-Lingual Transfer"), MLQA (Durrani et al., "MLQA: Evaluating Cross-lingual and Multitask Question Answering Models"), and TyDiQA (Clark et al., "TyDi QA 1.0: A Multilingual Question Answering Benchmark") assess factual consistency, they do not directly measure the transfer of knowledge between languages. Recent work by Clark et al., "What Does BERT Learn from Different Portions of a Sentence?" raised questions about LMs' ability to recall factual knowledge in reasoning tasks, while Artetxe et al., "On the Limitations of Unsupervised Cross-Lingual Transfer" emphasized the need for more robust methodologies for evaluating knowledge in multilingual LMs. Our study builds on these insights by introducing a benchmark specifically designed to assess cross-lingual factual knowledge transferability.

\textbf{Cross-Lingual Knowledge Transfer in LMs.} Recent works have sought to understand the factors that influence cross-lingual knowledge transfer in multilingual models. Studies suggest that multilingual LMs exhibit zero-shot and few-shot generalization across languages (Liu et al., "Multitask Learning for Multilingual Models"), but empirical evidence indicates that this transfer is often asymmetric, with high-resource languages benefiting more than lower-resource ones (Pfeiffer et al., "How Multitask Learning Works").  Conneau et al., "Unsupervised Cross-Lingual Representation Learning" investigated the connection between cross-lingual similarity in hidden representations and downstream task performance, revealing that LMs with stronger representation alignment across languages perform better. Artetxe et al., "On the Limitations of Unsupervised Cross-Lingual Transfer" explored cross-linguality from a language structure perspective, emphasizing the importance of compositional properties in facilitating knowledge transfer. More recent work has focused on cross-lingual transfer from high-resource to low-resource languages (Pfeiffer et al., "How Multitask Learning Works"). Our work contributes to this area by evaluating the effectiveness of factual knowledge transfer across languages using a comprehensive set of metrics designed to measure both factual recall and transferability.

\textbf{Context Sensitivity and Counterfactual Reasoning.} LMs are known to be highly sensitive to contextual cues, which can sometimes override factual knowledge when the context is misleading (Lin et al., "Factual Knowledge Retrieval from Contextual Information").  Conneau et al., "Unsupervised Cross-Lingual Representation Learning" provides an in-depth review of multilingual reasoning in LMs. Counterfactual reasoning, in which models must consider hypothetical situations, has been studied in various contexts (Lin et al., "Factual Knowledge Retrieval from Contextual Information"). These studies show that LMs optimized for factual recall often struggle with counterfactual tasks, especially when faced with conflicting contextual instructions. While most prior evaluations have focused on monolingual settings (Lin et al., "Factual Knowledge Retrieval from Contextual Information"), our work extends these investigations into the multilingual domain. By introducing tasks like in-context recall and counterfactual adherence, we analyze how multilingual models handle both factual accuracy and contextual reasoning across languages, revealing important challenges in balancing factual knowledge and context sensitivity.