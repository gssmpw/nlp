\section{Related Work}
\textbf{Multilingual Transformers.} Early work by \cite{petroni-etal-2019-language} explored whether LMs can store factual knowledge about entities, setting the stage for later investigations into multilingual LMs. Notable multilingual models such as mBERT \cite{devlin-etal-2019-bert}, XLM-R \cite{conneau-etal-2020-unsupervised}, mT5 \cite{xue-etal-2021-mt5}, and BLOOM \cite{workshop2023bloom176bparameteropenaccessmultilingual} have demonstrated varying levels of performance across different languages. These models, trained on diverse multilingual corpora, show that LMs exhibit language-dependent capabilities in factual recall. Research has highlighted systematic biases in factual retrieval across languages \cite{artetxe-etal-2020-cross, liu-etal-2020-multilingual-denoising, kassner-etal-2021-multilingual}, which is a key challenge in multilingual LMs. While multilingual QA benchmarks like XQuAD \cite{artetxe-etal-2020-cross}, MLQA \cite{lewis-etal-2020-mlqa}, and TyDiQA \cite{clark-etal-2020-tydi} assess factual consistency, they do not directly measure the transfer of knowledge between languages. Recent work by \cite{wang-etal-2024-knowledge-mechanisms} raised questions about LMs' ability to recall factual knowledge in reasoning tasks, while \cite{fierro2025multilinguallanguagemodelsremember} emphasized the need for more robust methodologies for evaluating knowledge in multilingual LMs. Our study builds on these insights by introducing a benchmark specifically designed to assess cross-lingual factual knowledge transferability.

\textbf{Cross-Lingual Knowledge Transfer in LMs.} Recent works have sought to understand the factors that influence cross-lingual knowledge transfer in multilingual models. Studies suggest that multilingual LMs exhibit zero-shot and few-shot generalization across languages \cite{nooralahzadeh-etal-2020-zero, pfeiffer-etal-2020-mad}, but empirical evidence indicates that this transfer is often asymmetric, with high-resource languages benefiting more than lower-resource ones \cite{hu2020xtrememassivelymultilingualmultitask}. \cite{muller-etal-2021-first} investigated the connection between cross-lingual similarity in hidden representations and downstream task performance, revealing that LMs with stronger representation alignment across languages perform better. \cite{chai-etal-2022-cross} explored cross-linguality from a language structure perspective, emphasizing the importance of compositional properties in facilitating knowledge transfer. More recent work has focused on cross-lingual transfer from high-resource to low-resource languages \cite{zhao2024llama, zhao2024tracing}, further underscoring the asymmetries in cross-lingual knowledge integration. Our work contributes to this area by evaluating the effectiveness of factual knowledge transfer across languages using a comprehensive set of metrics designed to measure both factual recall and transferability.

\textbf{Context Sensitivity and Counterfactual Reasoning.} LMs are known to be highly sensitive to contextual cues, which can sometimes override factual knowledge when the context is misleading \cite{brown2020language, tirumala2022memorization, cotterellcontext}. \cite{ghosh2025multilingualmindsurvey} provides an in-depth review of multilingual reasoning in LMs. Counterfactual reasoning, in which models must consider hypothetical situations, has been studied in various contexts \cite{wu2023reasoning}. These studies show that LMs optimized for factual recall often struggle with counterfactual tasks, especially when faced with conflicting contextual instructions. While most prior evaluations have focused on monolingual settings \cite{shwartz-etal-2020-unsupervised, wang2020language}, our work extends these investigations into the multilingual domain. By introducing tasks like in-context recall and counterfactual adherence, we analyze how multilingual models handle both factual accuracy and contextual reasoning across languages, revealing important challenges in balancing factual knowledge and context sensitivity.

% \textbf{Multilingual Transformers.} \cite{petroni-etal-2019-language} first explored whether LMs have capacity of storing factual knowledge about entities. Prior work on multilingual models, such as mBERT \cite{devlin-etal-2019-bert}, XLM-R \cite{conneau-etal-2020-unsupervised}, mT5 \cite{xue-etal-2021-mt5} and BLOOM \cite{workshop2023bloom176bparameteropenaccessmultilingual}, has shown that LMs trained on multilingual corpora exhibit varying performance across languages. Studies have also highlighted systematic biases in factual retrieval across different languages \cite{artetxe-etal-2020-cross, liu-etal-2020-multilingual-denoising, kassner-etal-2021-multilingual, }. While multilingual QA benchmarks such as XQuAD \cite{artetxe-etal-2020-cross}, MLQA \cite{lewis-etal-2020-mlqa}, and TyDiQA \cite{clark-etal-2020-tydi} assess factual consistency, they do not explicitly measure knowledge transfer within LMs. \cite{wang-etal-2024-knowledge-mechanisms} instigated whether LMs are able to recall factual knowledge for reasoning tasks. \cite{fierro2025multilinguallanguagemodelsremember} highlights the need for new methodologies for knowledge evaluation in multilingual LMs.
% % To address this gap, we introduce a benchmark designed to evaluate cross-lingual factual knowledge transferability.


% \textbf{Cross-Lingual Knowledge Transfer in LMs.}  Several studies have delved into understanding the factors influencing the cross-lingual ability of multilingual models. While research suggests that multilingual LMs exhibit zero-shot and few-shot generalization across languages \cite{nooralahzadeh-etal-2020-zero, pfeiffer-etal-2020-mad}, empirical studies indicate that this transfer is often asymmetric, favoring high-resource languages \cite{hu2020xtrememassivelymultilingualmultitask}. \cite{muller-etal-2021-first} analyzed representation similarities and discovered a strong connection between hidden cross-lingual similarity and the
%  modelâ€™s performance on downstream tasks. \cite{chai-etal-2022-cross} examined cross-linguality from a language structure perspective, emphasizing the significance of the composition property in facilitating cross-lingual
%  transfer. Most recent work has focused on cross-lingual transfer from high-resource languages to lower-resource ones \cite{zhao2024llama, zhao2024tracing}. 

% \textbf{Context Sensitivity and Counterfactual Reasoning.}\cite{ghosh2025multilingualmindsurvey} provides the in-depth review of multilingual reasoning in LMs. LMs can be susceptible to contextual cues, often overriding stored knowledge when presented with misleading information \cite{brown2020language, tirumala2022memorization,cotterellcontext}. Counterfactual reasoning studies \cite{wu2023reasoning} show that models trained for high factual recall struggle with conflicting contextual instructions. While prior evaluations have been monolingual \cite{shwartz-etal-2020-unsupervised, wang2020language}, our study extends these investigations into the multilingual domain, introducing in-context recall and counterfactual adherence tasks to analyze cross-lingual reasoning.