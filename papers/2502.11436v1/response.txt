\section{Related Work}
\label{sec:related}
Numerous approaches have been proposed for modifying prompts to enhance LLM performance, such as In-Context Learning and Instruction Optimization. In-Context Learning concentrates on providing the LLM with additional in-prompt exemplars from the same task domain, typically in the form of input data paired with their corresponding labels or outputs **Brown et al., "In-Context Learning"**. This method capitalizes on the model's ability to generalize from in-prompt examples, enabling the LLM to better comprehend the expected output format and task-specific requirements based on the provided exemplars.

Instruction Optimization aims to modify the instruction part of the prompt to improve LLM performance. For example, **Huang et al., "Instruction Optimization"** points out that composing better instructions can greatly boost LLM's performance on task inferencing. **Bao et al., "CoT Reasoning"** proposes CoT reasoning, which introduces immediate reasoning steps into the output generation process. As demonstrated by **Khashabi et al., "Zero-Shot CoT"**, employing zero-shot CoT substantially improves LLM performance tasks including logical reasoning, fraud detection, among many others. Extending beyond manually crafted instructions, various studies have proposed automated methods to search for optimal instructions tailored to specific tasks **Tian et al., "APE"**. For instance, APE **Tian et al., "APE"** introduces an iterative Monte Carlo search to refine prompt instructions. It first uses an instruction-proposing LLM to generate a set of candidate instructions, then evaluates each on a validation set to select the best-performing candidates.

% This encourages the LLM to articulate its reasoning path, making the model's thought process more transparent and aligned with human expectations. 
Despite these advances, directly optimizing the presentation of input data has received little attention. In this work, we hypothesize that optimizing both the data content and format may yield performance improvement when employing LLM for task inferencing. Building on the principles of automatic prompt optimization, we propose a novel framework called Automatic Data Optimization (ADO). In ADO, an LLM, denoted as \text{LLM}$_{\mathcal{G}}$, iteratively proposes and searches data-optimization instructions aimed at maximizing LLM performance.


% as this allows the input to be more closely aligned with the pre-training distribution. 

% Building on the concepts of automatic prompt optimization, we propose a novel framework called Automatic Data Optimization (ADO). In ADO, an LLM, denoted as \text{LLM}$_{\mathcal{G}}$, iteratively proposes and searches data-optimization instructions aimed at maximizing LLM performance.

% This represents a new direction for enhancing LLM capabilities by focusing on the optimization of input data itself.

% Additionally, retrieving additional information from external knowledge bases [cite] has been explored to provide more context and mitigate hallucination. By augmenting LLMs with information retrieved from external databases, the responses can be grounded in more factual and relevant data. This additional context helps the LLM produce more accurate and contextually appropriate outputs, addressing one of the significant challenges in LLM communication.

% Additionally, we also employ a LLM for executing the instructions proposed by the LLM\text{\textsubscript{ins}} on the original input data, denoted as the LLM\text{\textsubscript{opt}}, where we submit to the LLM\text{\textsubscript{opt}} with the unprocessed input data (i.e., a health profile, a logical puzzle, etc.) as well as an instruction consisting of a set of steps on how to perform content engineering and then structural reformulation on the submitted data. The LLM\text{\textsubscript{opt}} then executes the instruction, outputting the input data in its optimized form. The optimized input data, along with the task-specific instruction, will then be submitted to the inference LLM.