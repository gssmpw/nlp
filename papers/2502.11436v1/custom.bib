@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{chen2023unleashing,
  title={Unleashing the potential of prompt engineering in Large Language Models: a comprehensive review},
  author={Chen, Banghao and Zhang, Zhaofeng and Langren{\'e}, Nicolas and Zhu, Shengxin},
  journal={arXiv preprint arXiv:2310.14735},
  year={2023}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{liu2022few,
  title={Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning},
  author={Liu, Haokun and Tam, Derek and Muqeeth, Mohammed and Mohta, Jay and Huang, Tenghao and Bansal, Mohit and Raffel, Colin A},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={1950--1965},
  year={2022}
}

@article{ye2023prompt,
  title={Prompt engineering a prompt engineer},
  author={Ye, Qinyuan and Axmed, Maxamed and Pryzant, Reid and Khani, Fereshte},
  journal={arXiv preprint arXiv:2311.05661},
  year={2023}
}

@article{chen2024prompt,
  title={Prompt optimization in multi-step tasks (promst): Integrating human feedback and preference alignment},
  author={Chen, Yongchao and Arkin, Jacob and Hao, Yilun and Zhang, Yang and Roy, Nicholas and Fan, Chuchu},
  journal={arXiv preprint arXiv:2402.08702},
  year={2024}
}

@inproceedings{turner2021bayesian,
  title={Bayesian optimization is superior to random search for machine learning hyperparameter tuning: Analysis of the black-box optimization challenge 2020},
  author={Turner, Ryan and Eriksson, David and McCourt, Michael and Kiili, Juha and Laaksonen, Eero and Xu, Zhen and Guyon, Isabelle},
  booktitle={NeurIPS 2020 Competition and Demonstration Track},
  pages={3--26},
  year={2021},
  organization={PMLR}
}

@article{tang2024unleashing,
  title={Unleashing the Potential of Large Language Models as Prompt Optimizers: An Analogical Analysis with Gradient-based Model Optimizers},
  author={Tang, Xinyu and Wang, Xiaolei and Zhao, Wayne Xin and Lu, Siyuan and Li, Yaliang and Wen, Ji-Rong},
  journal={arXiv preprint arXiv:2402.17564},
  year={2024}
}

@article{zhang2024revisiting,
  title={Revisiting OPRO: The Limitations of Small-Scale LLMs as Optimizers},
  author={Zhang, Tuo and Yuan, Jinyue and Avestimehr, Salman},
  journal={arXiv preprint arXiv:2405.10276},
  year={2024}
}

@article{mccoy2023embers,
  title={Embers of autoregression: Understanding large language models through the problem they are trained to solve},
  author={McCoy, R Thomas and Yao, Shunyu and Friedman, Dan and Hardy, Matthew and Griffiths, Thomas L},
  journal={arXiv preprint arXiv:2309.13638},
  year={2023}
}

@inproceedings{li2023prompt,
  title={Prompt distillation for efficient llm-based recommendation},
  author={Li, Lei and Zhang, Yongfeng and Chen, Li},
  booktitle={Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
  pages={1348--1357},
  year={2023}
}

@inproceedings{lin2024data,
  title={Data-efficient Fine-tuning for LLM-based Recommendation},
  author={Lin, Xinyu and Wang, Wenjie and Li, Yongqi and Yang, Shuo and Feng, Fuli and Wei, Yinwei and Chua, Tat-Seng},
  booktitle={Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={365--374},
  year={2024}
}

@article{si2022prompting,
  title={Prompting gpt-3 to be reliable},
  author={Si, Chenglei and Gan, Zhe and Yang, Zhengyuan and Wang, Shuohang and Wang, Jianfeng and Boyd-Graber, Jordan and Wang, Lijuan},
  journal={arXiv preprint arXiv:2210.09150},
  year={2022}
}

@article{rouzegar2024enhancing,
  title={Enhancing Text Classification through LLM-Driven Active Learning and Human Annotation},
  author={Rouzegar, Hamidreza and Makrehchi, Masoud},
  journal={arXiv preprint arXiv:2406.12114},
  year={2024}
}

@article{Fang2024,
 author = {Xi Fang and Weijie Xu and Fiona Anting Tan and Jiani Zhang and Ziqing Hu and Yanjun (Jane) Qi and Scott Nickleach and Diego Socolinsky and Srinivasan Sengamedu, "SHS" and Christos Faloutsos},
 title = {Large language models (LLMs) on tabular data: Prediction, generation, and understanding - a survey},
 year = {2024},
 url = {https://www.amazon.science/publications/large-language-models-llms-on-tabular-data-prediction-generation-and-understanding-a-survey},
 journal = {Transactions on Machine Learning Research},
}

@article{liang2024learning,
  title={Learning to trust your feelings: Leveraging self-awareness in llms for hallucination mitigation},
  author={Liang, Yuxin and Song, Zhuoyang and Wang, Hao and Zhang, Jiaxing},
  journal={arXiv preprint arXiv:2401.15449},
  year={2024}
}

@article{wu2024easily,
  title={How Easily do Irrelevant Inputs Skew the Responses of Large Language Models?},
  author={Wu, Siye and Xie, Jian and Chen, Jiangjie and Zhu, Tinghui and Zhang, Kai and Xiao, Yanghua},
  journal={arXiv preprint arXiv:2404.03302},
  year={2024}
}

@article{jiao2019survey,
  title={A survey on the new generation of deep learning in image processing},
  author={Jiao, Licheng and Zhao, Jin},
  journal={Ieee Access},
  volume={7},
  pages={172231--172263},
  year={2019},
  publisher={IEEE}
}

@article{yang2023large,
  title={Large language models as optimizers},
  author={Yang, Chengrun and Wang, Xuezhi and Lu, Yifeng and Liu, Hanxiao and Le, Quoc V and Zhou, Denny and Chen, Xinyun},
  journal={arXiv preprint arXiv:2309.03409},
  year={2023}
}

@article{wendler2024llamas,
  title={Do llamas work in english? on the latent language of multilingual transformers},
  author={Wendler, Chris and Veselovsky, Veniamin and Monea, Giovanni and West, Robert},
  journal={arXiv preprint arXiv:2402.10588},
  year={2024}
}

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@article{yu2024aipatient,
  title={AIPatient: Simulating Patients with EHRs and LLM Powered Agentic Workflow},
  author={Yu, Huizi and Zhou, Jiayan and Li, Lingyao and Chen, Shan and Gallifant, Jack and Shi, Anye and Li, Xiang and Hua, Wenyue and Jin, Mingyu and Chen, Guang and others},
  journal={arXiv preprint arXiv:2409.18924},
  year={2024}
}

@article{yu2024large,
  title={Large Language Models in Biomedical and Health Informatics: A Bibliometric Review},
  author={Yu, Huizi and Fan, Lizhou and Li, Lingyao and Zhou, Jiayan and Ma, Zihui and Xian, Lu and Hua, Wenyue and He, Sijia and Jin, Mingyu and Zhang, Yongfeng and others},
  journal={arXiv preprint arXiv:2403.16303},
  year={2024}
}

@article{li2024scoping,
  title={A scoping review of using Large Language Models (LLMs) to investigate Electronic Health Records (EHRs)},
  author={Li, Lingyao and Zhou, Jiayan and Gao, Zhenxiang and Hua, Wenyue and Fan, Lizhou and Yu, Huizi and Hagen, Loni and Zhang, Yonfeng and Assimes, Themistocles L and Hemphill, Libby and others},
  journal={arXiv preprint arXiv:2405.03066},
  year={2024}
}

@inproceedings{li2023large,
  title={Large language models in finance: A survey},
  author={Li, Yinheng and Wang, Shaofei and Ding, Han and Chen, Hang},
  booktitle={Proceedings of the fourth ACM international conference on AI in finance},
  pages={374--382},
  year={2023}
}

@article{wu2023bloomberggpt,
  title={Bloomberggpt: A large language model for finance},
  author={Wu, Shijie and Irsoy, Ozan and Lu, Steven and Dabravolski, Vadim and Dredze, Mark and Gehrmann, Sebastian and Kambadur, Prabhanjan and Rosenberg, David and Mann, Gideon},
  journal={arXiv preprint arXiv:2303.17564},
  year={2023}
}

@article{wu2024survey,
  title={A survey on large language models for recommendation},
  author={Wu, Likang and Zheng, Zhi and Qiu, Zhaopeng and Wang, Hao and Gu, Hongchao and Shen, Tingjia and Qin, Chuan and Zhu, Chen and Zhu, Hengshu and Liu, Qi and others},
  journal={World Wide Web},
  volume={27},
  number={5},
  pages={60},
  year={2024},
  publisher={Springer}
}

@inproceedings{hua2023tutorial,
  title={Tutorial on large language models for recommendation},
  author={Hua, Wenyue and Li, Lei and Xu, Shuyuan and Chen, Li and Zhang, Yongfeng},
  booktitle={Proceedings of the 17th ACM Conference on Recommender Systems},
  pages={1281--1283},
  year={2023}
}

@article{huang2022towards,
  title={Towards reasoning in large language models: A survey},
  author={Huang, Jie and Chang, Kevin Chen-Chuan},
  journal={arXiv preprint arXiv:2212.10403},
  year={2022}
}

@article{fan2023nphardeval,
  title={Nphardeval: Dynamic benchmark on reasoning ability of large language models via complexity classes},
  author={Fan, Lizhou and Hua, Wenyue and Li, Lingyao and Ling, Haoyang and Zhang, Yongfeng and Hemphill, Libby},
  journal={arXiv preprint arXiv:2312.14890},
  year={2023}
}

@article{baccouche2020ensemble,
  title={Ensemble deep learning models for heart disease classification: A case study from Mexico},
  author={Baccouche, Asma and Garcia-Zapirain, Begonya and Castillo Olea, Cristian and Elmaghraby, Adel},
  journal={Information},
  volume={11},
  number={4},
  pages={207},
  year={2020},
  publisher={MDPI}
}

@inproceedings{geng2022recommendation,
  title={Recommendation as language processing (rlp): A unified pretrain, personalized prompt \& predict paradigm (p5)},
  author={Geng, Shijie and Liu, Shuchang and Fu, Zuohui and Ge, Yingqiang and Zhang, Yongfeng},
  booktitle={Proceedings of the 16th ACM Conference on Recommender Systems},
  pages={299--315},
  year={2022}
}

@article{bsharat2023principled,
  title={Principled instructions are all you need for questioning llama-1/2, gpt-3.5/4},
  author={Bsharat, Sondos Mahmoud and Myrzakhan, Aidar and Shen, Zhiqiang},
  journal={arXiv preprint arXiv:2312.16171},
  year={2023}
}


@article{do2024automatic,
  title={Automatic Prompt Selection for Large Language Models},
  author={Do, Viet-Tung and Hoang, Van-Khanh and Nguyen, Duy-Hung and Sabahi, Shahab and Yang, Jeff and Hotta, Hajime and Nguyen, Minh-Tien and Le, Hung},
  journal={arXiv preprint arXiv:2404.02717},
  year={2024}
}

@article{li2024pap,
  title={PAP-REC: Personalized Automatic Prompt for Recommendation Language Model},
  author={Li, Zelong and Ji, Jianchao and Ge, Yingqiang and Hua, Wenyue and Zhang, Yongfeng},
  journal={arXiv preprint arXiv:2402.00284},
  year={2024}
}

@inproceedings{marvin2023prompt,
  title={Prompt engineering in large language models},
  author={Marvin, Ggaliwango and Hellen, Nakayiza and Jjingo, Daudi and Nakatumba-Nabende, Joyce},
  booktitle={International conference on data intelligence and cognitive informatics},
  pages={387--402},
  year={2023},
  organization={Springer}
}

@article{sahoo2024systematic,
  title={A systematic survey of prompt engineering in large language models: Techniques and applications},
  author={Sahoo, Pranab and Singh, Ayush Kumar and Saha, Sriparna and Jain, Vinija and Mondal, Samrat and Chadha, Aman},
  journal={arXiv preprint arXiv:2402.07927},
  year={2024}
}

@article{sclar2023quantifying,
  title={Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting},
  author={Sclar, Melanie and Choi, Yejin and Tsvetkov, Yulia and Suhr, Alane},
  journal={arXiv preprint arXiv:2310.11324},
  year={2023}
}

@article{saadany2021bleu,
  title={BLEU, METEOR, BERTScore: evaluation of metrics performance in assessing critical translation errors in sentiment-oriented text},
  author={Saadany, Hadeel and Orasan, Constantin},
  journal={arXiv preprint arXiv:2109.14250},
  year={2021}
}

@inproceedings{liu2024large,
  title={Large language models as evolutionary optimizers},
  author={Liu, Shengcai and Chen, Caishun and Qu, Xinghua and Tang, Ke and Ong, Yew-Soon},
  booktitle={2024 IEEE Congress on Evolutionary Computation (CEC)},
  pages={1--8},
  year={2024},
  organization={IEEE}
}

@article{yang2024large,
  title={Large Language Models as Optimizers},
  author={Yang, Chengrun and Wang, Xuezhi Wang and Lu, Yifeng Lu and Liu, Hanxiao and V. Le, Quoc and Zhou, Denny and Chen, Xinyun},
  journal={ICLR},
  year={2024}}

@article{zhou2023large,
  title={Large language models are human-level prompt engineers},
  author={Zhou, Yongchao and Muresanu, Andrei Ioan and Han, Ziwen and Paster, Keiran and Pitis, Silviu and Chan, Harris and Ba, Jimmy},
  journal={ICLR},
  year={2023}
}

@article{sordoni2024joint,
  title={Joint prompt optimization of stacked llms using variational inference},
  author={Sordoni, Alessandro and Yuan, Eric and C{\^o}t{\'e}, Marc-Alexandre and Pereira, Matheus and Trischler, Adam and Xiao, Ziang and Hosseini, Arian and Niedtner, Friederike and Le Roux, Nicolas},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{ahmad2018toward,
  title={Toward modeling and optimization of features selection in Big Data based social Internet of Things},
  author={Ahmad, Awais and Khan, Murad and Paul, Anand and Din, Sadia and Rathore, M Mazhar and Jeon, Gwanggil and Choi, Gyu Sang},
  journal={Future Generation Computer Systems},
  volume={82},
  pages={715--726},
  year={2018},
  publisher={Elsevier}
}

@inproceedings{zhou2004optimization,
  title={An optimization-based approach to dynamic data content selection in intelligent multimedia interfaces},
  author={Zhou, Michelle X and Aggarwal, Vikram},
  booktitle={Proceedings of the 17th annual ACM symposium on User interface software and technology},
  pages={227--236},
  year={2004}
}

@book{zheng2018feature,
  title={Feature engineering for machine learning: principles and techniques for data scientists},
  author={Zheng, Alice and Casari, Amanda},
  year={2018},
  publisher={" O'Reilly Media, Inc."}
}

@article{ling2021editgan,
  title={Editgan: High-precision semantic image editing},
  author={Ling, Huan and Kreis, Karsten and Li, Daiqing and Kim, Seung Wook and Torralba, Antonio and Fidler, Sanja},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={16331--16345},
  year={2021}
}

@inproceedings{barrett2002object,
  title={Object-based image editing},
  author={Barrett, William A and Cheney, Alan S},
  booktitle={Proceedings of the 29th annual conference on Computer graphics and interactive techniques},
  pages={777--784},
  year={2002}
}

@article{lin2024promptcrypt,
  title={Promptcrypt: Prompt encryption for secure communication with large language models},
  author={Lin, Guo and Hua, Wenyue and Zhang, Yongfeng},
  journal={arXiv preprint arXiv:2402.05868},
  year={2024}
}

@article{du2023improving,
  title={Improving factuality and reasoning in language models through multiagent debate},
  author={Du, Yilun and Li, Shuang and Torralba, Antonio and Tenenbaum, Joshua B and Mordatch, Igor},
  journal={arXiv preprint arXiv:2305.14325},
  year={2023}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020}
}

@article{wei2023larger,
  title={Larger language models do in-context learning differently},
  author={Wei, Jerry and Wei, Jason and Tay, Yi and Tran, Dustin and Webson, Albert and Lu, Yifeng and Chen, Xinyun and Liu, Hanxiao and Huang, Da and Zhou, Denny and others},
  journal={arXiv preprint arXiv:2303.03846},
  year={2023}
}

@article{dong2022survey,
  title={A survey on in-context learning},
  author={Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Wu, Zhiyong and Chang, Baobao and Sun, Xu and Xu, Jingjing and Sui, Zhifang},
  journal={arXiv preprint arXiv:2301.00234},
  year={2022}
}

@article{shin2022effect,
  title={On the effect of pretraining corpora on in-context learning by a large-scale language model},
  author={Shin, Seongjin and Lee, Sang-Woo and Ahn, Hwijeen and Kim, Sungdong and Kim, HyoungSeok and Kim, Boseop and Cho, Kyunghyun and Lee, Gichang and Park, Woomyoung and Ha, Jung-Woo and others},
  journal={arXiv preprint arXiv:2204.13509},
  year={2022}
}

@article{sharan2023llm,
  title={Llm-assist: Enhancing closed-loop planning with language-based reasoning},
  author={Sharan, SP and Pittaluga, Francesco and Chandraker, Manmohan and others},
  journal={arXiv preprint arXiv:2401.00125},
  year={2023}
}

@article{pryzant2023automatic,
  title={Automatic prompt optimization with" gradient descent" and beam search},
  author={Pryzant, Reid and Iter, Dan and Li, Jerry and Lee, Yin Tat and Zhu, Chenguang and Zeng, Michael},
  journal={arXiv preprint arXiv:2305.03495},
  year={2023}
}