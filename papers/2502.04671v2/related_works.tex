% \george{TODO for amitayush/george:  pypantograph needs to be cited (at least we need to know what differences there are)} 
Previous open-sourced tooling has been developed for interaction with formal proof assistants, but individually only using a single language. Oftentimes, this tooling also contains data extraction features, compiling proof datasets from popular formalization repositories such as Mathlib \citep{mathlib} for Lean, CompCert \citep{leroy2009formal} and Mathcomp \citep{githubGitHubMathcompmathcomp} for Coq. LeanDojo \citep{yang2023leandojo} provided open-source tooling for interaction with Lean 3 and extracted a proof step dataset from Mathlib.\footnote{LeanDojo now supports Lean 4, which is not backwards compatible to Lean 3.} NTP Toolkit \citep{ntptoolkit} supports extracting training data from arbitrary Lean repositories. CoqGym \citep{yang2019learning} is a framework for interaction and data collection with Coq up to versions 8.12.0 (because of dependency on SerAPI library\footnote{\href{https://github.com/rocq-archive/coq-serapi}{https://github.com/rocq-archive/coq-serapi}}). Proverbot \citep{sanchez2020generating} introduced Coq-Serapy, an interaction tool in Coq from which our Coq support is derived. CoqPyt \citep{CoqPyt} is a framework for interaction and data generation from Coq with emphasis on supporting LM-based methods. COPRA \citep{thakur2024incontext} introduces a framework for interaction with Lean 3 and Coq, but without tooling for data extraction or support for heavy parallelism during proof search. \citet{pantograph} introduced Pantograph, an interaction and data collection framework for Lean 4. We remark that one of our main contributions is an \emph{unified} framework for interacting with and collecting data from both Coq and Lean 4, with support for training and parallel search, hence affording automated theorem-proving researchers a common tool in the presence of multiple popular proof assistant languages.

A number of proof search methodologies have been proposed in the recent literature. GPT-$f$ \citep{polu2020generative} employed a best-first search approach with a trained transformer-based architecture for proof synthesis. LeanDojo \citep{yang2023leandojo} similarly employs a best-first search, though augments the neural prediction model with a retrieval model which predicts relevant premises. HyperTree Proof Search and ABEL \citep{lample2022hypertree, gloeckle2024abel} introduces an online variant of Monte Carlo Tree Search for the theorem-proving task. PACT \citep{han2021proof} introduces auxiliary training objectives derived from proof state data to learn a better prediction model for search. COPRA \citep{thakur2024incontext} uses large LMs as proof step prediction models, which can be conditioned on additional information such as retrieved lemmas, definitions, and execution information, for search. Graph2Tac \citep{blaauwbroek2024graph2taconlinerepresentationlearning} learns online hierarchical representations of definitions and theorems, and is used for proof search in Tactician \citep{Blaauwbroek_2020}. Several tools have been developed to help with live formalization efforts; these include LLMStep and LeanCopilot for Lean \citep{welleck2023llmstepllmproofstepsuggestions, song2024largelanguagemodelscopilots}, and CoqPilot for Coq \citep{coqpilot}.

Previous work has explored providing effective support for measuring models across various interactive theorem provers. \texttt{miniF2F} \citep{zheng2021minif2f} is a multi-language benchmark of high-school competition math problems formalized in Lean 3, HOL Light, Isabelle, and Metamath, though not in Coq. PutnamBench \citep{tsoukalas2024putnambenchevaluatingneuraltheoremprovers} is a collegiate-level benchmark for competition math in Lean 4, Coq, and Isabelle. We do not include evaluations on PutnamBench as our work is not targeted towards olympiad-style theorem-proving. MMA \citep{jiang2023multilingualmathematicalautoformalization} demonstrates that models trained on data from both languages yield downstream performance improvements for autoformalization in both languages, compared to models trained on just one language of data. In our experiments, we demonstrate that such transfer also occurs for neural models trained to perform proof step prediction.
