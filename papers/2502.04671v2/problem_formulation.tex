\newcommand{\init}{\mathit{in}}
\newcommand{\error}{\mathit{error}}
\newcommand{\leqhard}{\sqsubseteq}
\newcommand{\geqhard}{\sqsupseteq}
\newcommand{\QED}{\mathtt{QED}}
\newcommand{\Ob}{\mathcal{O}}
\newcommand{\Ib}{\mathcal{I}}
\newcommand{\glob}{\chi} 
\renewcommand{\error}{\mathit{Err}}

\newcommand{\edge}[1]{\stackrel{#1}{\longrightarrow}}

We view a theorem-prover as a system that systematically addresses a set of \emph{proof obligations} by applying a sequence of proof tactics.
Each obligation $o$ is a pair $(g, h)$, where $g$ is the goal to be proven and $h$ contains the hypotheses relevant to proving $g$. The system starts with an initial set of proof obligations; its ultimate goal is to reduce this set to an empty set. 
%Under this formulation, formal theorem-proving amounts to a discrete search problem through various states in a proof environment.
 
\Cref{fig:example} shows a formal proof of a theorem about block triangular matrices ---found using the \proofwala-\multi\; proof-step generation model---in the \lean\; 4 language \citep{de2015lean}.

As in \citet{thakur2024incontext}, we treat theorem-proving as a discrete search through the state space of an ITP. We abstractly model an ITP as a \emph{proof environment} consisting of:
\begin{figure}
%\vspace{-0.1in}
\begin{mdframed}[roundcorner=10pt]
\begin{minipage}{0.9\textwidth}
\begin{lstlisting}
theorem blockTriangular_stdBasisMatrix 
{i j : m} (hij : b i â‰¤ b j) (c : R) 
: BlockTriangular (stdBasisMatrix i j c) b  
:= by rintro i' j' hij'
  simp [stdBasisMatrix, hij, hij'.not_le]
  rintro rfl rfl
  exact (not_lt_of_le hij hij').elim
\end{lstlisting}
\end{minipage}
\end{mdframed}
%\reducevspacebetweenfigureandcaption
\vspace{-0.1in}
\caption{A \lean\; 4 theorem and a with a correct proof using \proofwala-\multi\; proof-step generation model. The theorem states that the standard basis matrix, where $c$ is placed in the $(i,j)th$ entry with zeroes elsewhere is block triangular. The first tactic \texttt{rintro i' j' hij'} unfolds the definition of \texttt{BlockTriangular} and adds the variables \texttt{i'}, \texttt{j'}, as well as the hypothesis \texttt{hij' : b j' < b i'} to the set of hypotheses. The proof proceeds by using established properties of the \texttt{stdBasisMatrix} and resolves by demonstrating an inconsistency with the hypothesis \texttt{hij : b i $\leq$ b j}.}
\label{fig:example}
\end{figure}

 
\begin{itemize}%[leftmargin=0.1in]

\item 
A set of \emph{states} $\Ob$, where each \emph{state} is a set $O = \{o_1,\dots, o_k\}$ of obligations $o_i$.

\item 
The \emph{initial state}, $\Ib$, consisting of a single obligation $(g_{\init}, h_{\init})$ extracted from a user-provided theorem.

\item A unique \emph{goal state} $\QED$ is the empty obligation set. 

\item A finite set of \emph{proof tactics}.

\item A transition function $T(O, a)$, which determines the result of applying a tactic $a$ to a state $O$. If $a$ can be successfully applied at state $O$, then $T(O, a)$ is the new set of obligations resulting from the application. If a tactic $a$ cannot be applied to the state $O$, then $T(O, a) = O$. 
\end{itemize}
 
We define the transition function $T_{seq}$ over a sequence of proof-steps (tactics), $\alpha = \langle a_1, a_2, \dots, a_n\rangle$, and proof state, $O \in \Ob$, as:  
$$
T_{seq}(O, \alpha) = 
 \left\{\begin{array}{l}
T(O, a_1) \qquad \textrm{if $n = 1$} \\
T(T_{seq}(O, \langle a_1,\dots, a_{n-1} \rangle), a_n) ~~~ \textrm{otherwise.}
\end{array}
\right. 
$$
The theorem-proving problem is now defined as follows: 
\begin{problem}[Theorem-proving]
Given an initial state $O_\init$ find a tactic sequence $\alpha$ (a \emph{proof}) satisfying
$T_{seq}(O_{\init}, \alpha) = \QED$. 
\end{problem}

\paragraph{Proving with LMs.} Our proof search approach reflects the strategy described in \citep{polu2020generative}, where a neural model predicts a tactic to apply given the current state, namely $p(a|O)$. Such a model can be implemented with a language model (LM) that generates a tokenized representation of a tactic $a$ in a token-by-token fashion: $p(\mathrm{tok}(a) | O)$. For simplicity of notation, we drop references to tokenization in the remainder of the paper, but notably, statements from different ITPs can be tokenized into a shared LM vocabulary. The learned model $p$ is the workhorse of our multilingual prover, which we will train using multilingual proof data.