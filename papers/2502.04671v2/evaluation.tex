\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\newcommand{\codetfive}[0]{\textsc{CodeT5}}
\newcommand{\greencheck}{\color{green}{\checkmark}}
\newcommand{\redcross}{\color{red}{\xmark}}
 
Using our trained \proofwala{} models, we investigate (i) the benefit of incorporating multilingual data into the training pipeline (ii) moreover, whether further fine-tuning multilingual models demonstrates superior adaptation to novel domains.

In particular, we use the \proofwala-\{\multi, \coq, \lean\} models inside the search module afforded by our framework (see \Cref{sec:searching-module}). Our experiments run proof search on the test split mentioned in \Cref{tab:data-mix-size} for the CompCert, MathComp, GeoCoq, CategoryTheory, and \lean data-mixes. This enables us to study the impact of transfer in the case of the \proofwala-\multi\; model for diverse ITPs and domains.

\subsection{Experiments}
\label{sec:experiments}

\paragraph{Setup.} 
All our experiments use the \proofwala\; proof step models for single-step prediction and then use \proofwala\; to conduct proof search. In our experiments we employ beam search as the search algorithm. We use the negative log-likelihood of the tokens generated by the \proofwala{} proof step prediction model to direct the search. \Cref{fig:proof-search-annotation} in \Cref{app:proof-trees} shows one such search result. Hyperparameters used in our search algorithm are listed in \Cref{tab:search-params} in \Cref{app:search-params}. We employ a timeout of \texttt{600} seconds for most of our experiments. However, for the GeoCoq data-mix, we set a higher timeout of \texttt{1200} seconds to accommodate the appreciably longer ground-truth proofs, which require more time to execute all generated proof steps.

We conduct ablations to study the impact of training \proofwala{} models on different data-mixes. We also run paired bootstrap hypothesis testing to better understand the significance of transfer happening between different data-mixes, and whether \proofwala-\multi{} has a significant edge over other monolingual models (\proofwala-\coq{} and \proofwala-\lean) while searching for proofs.

\paragraph{Aggregate Results.}
We run multiple experiments with different models as shown in \Cref{tab:all-experiments}. We compare the results for \emph{pass@k} \cite{chen2021evaluating} with $1 \leq k \leq 5$ for \lean, CompCert, MathComp, GeoCoq, and CategoryTheory data-mixes with various \proofwala$\;$models. 

From \Cref{tab:all-experiments}, we can see that the \proofwala-\multi\; model is a more effective prover compared to both monolingual models (\proofwala-\coq\; and \proofwala-\lean) trained only on proof-data corresponding to a single ITP. The \proofwala-\multi\;model outperforms the state-of-the-art, Proverbot \citep{sanchez2020generating}, on the CompCert dataset. 


We perform paired bootstrap significance tests on our experiment results for the different data-mixes for both \proofwala-\multi\, and monolingual (\proofwala-\coq\; and \lean) models. The results are summarized in \Cref{tab:all-experiments}.
% \gd{I removed the following sentence. fwiw I don't think that's the right null hypothesis. it should be that the two means are the same} 
%We tested the null hypothesis that the average number of proofs found using the \proofwala-\multi\; model is less than the average number of proofs found using the \proofwala-\lean/\coq\; model.
\proofwala-\multi{} significantly outperforms the \proofwala-\lean{} model on the \lean{} data-mix. For other data-mixes, the gap was small, or the data-mix size was small enough, for the significance test to be inconclusive. From \Cref{tab:all-experiments}, we can reasonably conclude that the \proofwala-\multi{} model generally outperforms the \proofwala-\coq{} and \proofwala-\lean{} models in proof search.

To test the generalization capabilities of our models, we finetune our trained  \proofwala-\multi{} and \proofwala-\coq{} models on the CategoryTheory data-mix. Remarkably, as can be seen from \Cref{tab:all-experiments}, we find that the multilingual model yields a statistically significant performance boost over the \coq{} model on the test set, by nearly $8\%$. This provides evidence that training on multilingual data enhances generalizability to unseen domains, especially when followed by further fine-tuning, compared to the monolingual baseline. This finding suggests that such multilingual models should be preferred as completion assistants for newly developing formal theorem-proving repositories.

\paragraph{Analysis of Specific Proofs.}\label{sec:quantitative-analysis} To better understand the effectiveness of multilingual training, we construct and analyze the search trees constructed in a subset of our problems. \Cref{fig:proof-search-annotation} (in \Cref{app:proof-trees}) shows a few such search trees, which we call \emph{proof trees}. To simplify our analysis, we only include those edges in the proof tree that are compilable for a given proof state, this ensures that each node in the proof tree corresponds to a valid proof state. 

%By analyzing these trees, we are able to provide insights as to why the multilingual model outperforms the monolingual models. 
Specifically, we compute the number of nodes and edges of the proof trees 
computed for the different data mixes (\Cref{tab:tree-stats}). 
We find that the proof trees for the multilingual models tend to 
%be \emph{larger} --- containing 
have more nodes and edges. This finding signifies that the search aided by these models can access more proof states and apply more tactics --- i.e., explore more. Such additional exploration 
generally increases the likelihood of finding correct proofs. 
%Thus, the proof search guided via the multilingual model contributes 
%which leads to additional exploration by increasing the number of applied tactics. 
\Cref{fig:proof-tree-stats-nodes} and \Cref{fig:proof-tree-stats-edges} (in \Cref{app:proof-tree-properties}) show the distribution of nodes and edges in the proof trees generated during the search. 


\begin{table}[ht]
    \centering
    \scalebox{0.65}{
    \begin{tabular}{lllllllll}
    \toprule
    \textbf{Data-Mix} &  
    \textbf{\name Model} &
    \multicolumn{3}{c}{\textbf{Avg. Proof Tree Stats}} &
    \\
    \cmidrule(lr){3-5}
     & 
     & 
    \textbf{\# Nodes} &
    \textbf{\# Edges} & 
    \textbf{\# Degree} \\
    \toprule
    \textbf{\lean} & 
    \lean & 
     {3.989} & 
     {3.536} & 
     {1.536}
    \\
     & 
     \multi & 
    4.729 & 
    4.689 & 
    1.983
    \\
    \hline
    \textbf{MathComp} & 
    \coq & 
    2.534 & 
    1.739 & 
    1.167
    \\
     &
     \multi & 
     2.576 & 
     1.822 & 
     1.207 
     \\
    \hline
    \textbf{GeoCoq} & 
    \coq & 
     15.358 & 
     14.457 & 
     1.180
    \\
    & 
    \multi & 
     17.144 & 
     15.75 & 
     1.353
    \\
    \hline
    \textbf{CompCert} &
    \coq &  
     8.048 & 
     7.480 & 
     1.404 
     \\
     &
     \multi & 
     8.318 & 
     8.200 & 
     1.584 
     \\
    \hline
    \textbf{CategoryTheory} & 
    \coq-\CatTheory & 
    5.674 & 
    5.804 & 
    2.301 
    \\
    & 
    \multi-\CatTheory & 
    7.056 & 
    7.130 & 
    2.193
    \\
    \bottomrule
    \end{tabular}}
    \caption{Comparison between the average number of nodes, edges, and degree of the proof trees generated on various \proofwala$\;$models over different data-mixes.}
    \label{tab:tree-stats}
\end{table}

Our belief that the multilingual approach can explore more is further bolstered by the fact that despite the same search timeout set for both approaches, the monolingual models often fail early (not finding any tactic that can expand the search tree), whereas the multilingual models explore more of the search space and found more proofs. \Cref{tab:app-proof-times-avg} (in \Cref{app:proof-tree-properties}) shows the average time taken to complete the proofs in various data mixes, and \Cref{fig:proof-tree-stats-time} (in \Cref{app:proof-tree-properties}), shows the distribution of the time taken to find the proof successfully.

\begin{figure*}[h]
\centering
\footnotesize
\includegraphics[scale=0.3]{img-degree-distribution.png}
\caption{Distribution of degree of nodes in the proof trees across various data mixes found by different \name\; models. Across all data mixes, \name-\multi\; models tend to have higher degrees per node. This indicates that \name-\multi\; often find more compilable tactics given a particular proof state, this can increase the chances of eventually finding a proof.}
\label{fig:high-degree}
\end{figure*}

We also observed that the average degree of nodes in the proof trees generated by the multilingual model was higher than that generated by the monolingual model (see \Cref{fig:high-degree}). Hence, the multilingual model found more correct and compilable proof steps for a given state than the monolingual models, thus contributing to further exploration of the search space. We can further corroborate this from the fact that the multilingual approach often found more than one proof for a given theorem. In \Cref{fig:proof-tree-stats-same-proof} (in \Cref{app:proof-tree-properties}), shows the distribution of the number of proofs found for a given theorem for both of the approaches. These experiments also demonstrate our framework's capabilities to analyze proof trees and easily identify these patterns.