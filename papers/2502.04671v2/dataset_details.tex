\newcommand{\CompCertUrl}[0]{\href{https://github.com/AbsInt/CompCert}{https://github.com/AbsInt/CompCert}}
\newcommand{\MathCompUrl}[0]{\href{https://github.com/math-comp/math-comp}{https://github.com/math-comp/math-comp}}
\newcommand{\GeoCoqUrl}[0]{\href{https://github.com/GeoCoq/GeoCoq}{https://github.com/GeoCoq/GeoCoq}}
\newcommand{\CategoryTheoryUrl}[0]{\href{https://github.com/jwiegley/category-theory}{https://github.com/jwiegley/category-theory}}
\newcommand{\LeanUrl}[0]{\href{https://github.com/leanprover-community/mathlib4}{https://github.com/leanprover-community/mathlib4}}
\newcommand{\CodeTFiveModelSize}[0]{\texttt{60 M}}
\newcommand{\CatTheory}[0]{\textsc{Cat-Theory}}

In this section, we explain our dataset construction and model training choices towards our demonstration of positive transfer from multilingual training as well as adaptability to new domains via further fine-tuning.
% We also investigate further fine-tuning them on held-out datasets to see if our models can adapt to on a different domains following the primary training. 

\subsection{Dataset Details}
We collect datasets across multiple languages and language versions of Coq and Lean 4, sourcing data from existing repositories. Our data collection approach involves collecting proof states from the ITP through tactic execution. We construct several data-mixes, of different subsets of the accumulated data, to train various monolingual and multilingual \proofwala{} models to perform proof step prediction. The training data is formatted into prompts as shown in \Cref{fig:prompt-format} (in \Cref{app:training-data}). We collect proof-step data for the various data mixtures as shown in \Cref{tab:data-mix}. 

\renewcommand\theadfont{}
\begin{table}[t!]
    \centering
    \scalebox{0.7}{
    \begin{tabular}{llll}
    \toprule
    \multicolumn{4}{c}{\thead{\textbf{Initial Fine-tuning}}}\\
    \hline
    \thead{\textbf{Data-mix}}  & \thead{\textbf{Data-mix Source}}  & \thead{\textbf{\name\;}\\\textbf{Models Trained}} & \thead{\textbf{Token Count}}\\
    \toprule
    1. CompCert\footref{fnote:CompCert-url} & CompCert Repo\footref{fnote:same-as-proverbot} & - & 61.6 M \\
    2. MathComp\footref{fnote:MathComp-url} & MathComp Repo & - & 18.2 M\\
    3. GeoCoq\footref{fnote:GeoCoq-url} & GeoCoq Repo & - & 91.2 M\\
    \renewcommand\theadfont{}
    4. \coq & {\textbf{Data-Mixes:} 1-3} & \thead[l]{\coq} & 171 M \\
    \renewcommand\theadfont{}
    5. \lean\footref{fnote:Lean-url} & Mathlib Repo\footref{fnote:same-as-reprover} & \thead[l]{\lean} & 99 M \\
    \renewcommand\theadfont{}
    6. \multi & {\textbf{Data-Mixes:} 4-5} & \thead[l]{\multi} & 270 M \\
    \hline
     \multicolumn{4}{c}{\thead{\textbf{Further Fine-tuning}}}\\
    \hline
    \renewcommand\theadfont{}
    7. CategoryTheory\footref{fnote:CategoryTheory-url} & CategoryTheory Repo & \thead[l]{\multi-\\\CatTheory\;\;\&\;\\\coq-\\\CatTheory} & 1.7 M\\
    \bottomrule
    \end{tabular}}
    \caption{Different data-mixes used to extract proof-step and proof state pair data. Various \proofwala\; models trained on these data mixes.}
    % \\\textsuperscript{*}\small{Same as Proverbot split \citep{sanchez2020generating}}
    % \\\textsuperscript{**}\small{Same as random split in ReProver \citep{yang2023leandojo}}}
    \label{tab:data-mix}
\end{table}
\footnotetext{\label{fnote:same-as-proverbot}Same as Proverbot split \citep{sanchez2020generating}}
\addtocounter{footnote}{+1}\footnotetext{\label{fnote:same-as-reprover}Same as random split in ReProver \citep{yang2023leandojo}}

We use different Coq and Lean repositories to generate this proof-step data. We use well-known repositories, namely CompCert,\footnote{\label{fnote:CompCert-url}\CompCertUrl} Mathlib,\footnote{\label{fnote:Lean-url}\LeanUrl} MathComp,\footnote{\label{fnote:MathComp-url}\MathCompUrl} GeoCoq,\footnote{\label{fnote:GeoCoq-url}\GeoCoqUrl} and CategoryTheory,\footnote{\label{fnote:CategoryTheory-url}\CategoryTheoryUrl} to generate the proof-step data. For CompCert we used the train-test split proposed by \citet{sanchez2020generating}, and for Mathlib we used the split proposed by \citet{yang2023leandojo}. Together we have \texttt{442607} proof-step pairs derived from over \texttt{76997} theorems across Lean and Coq (details of the split shown in \Cref{tab:data-mix-size}). We hold out the CategoryTheory dataset from initial training data-mixes for experimentation with further fine-tuning for our novel domain adaptation experiment.
% We also introduce some new 
% \george{I'm not sure what you mean by new, Coq-specific data collection have used these for sure. I would rephrase the beginning of this paragraph as (loosely) "We collect from Mathlib, Compcert, GeoCoq, CategoryTheory repositories which constritute some of the largest and most popular repositories in both ITPs"} proof-step data from MathComp\footnote{\label{fnote:MathComp-url}\MathCompUrl}, GeoCoq\footnote{\label{fnote:GeoCoq-url}\GeoCoqUrl}, and CategoryTheory\footnote{\label{fnote:CategoryTheory-url}\CategoryTheoryUrl} repositories in Coq.
% \gd{you need much more info here. how were these converted to Coq? what previous work does this build on? you need citations here and description inlined here, then you can remove them from the table, and ideally examples of the datasets in the appendix} 
 %We trained each of our models over \texttt{2.71 B} tokens. \amit{Change all the entries in \Cref{tab:data-mix} once the models are re-trained}

% \begin{table}[ht]
%     \centering
%     \footnotesize
%     \begin{tabular}{l|l|l|l|l|l|l}
%     \hline
%     \thead{\textbf{Data-mix}\\\textbf{Name}}  & \multicolumn{3}{c|}{\thead{\textbf{Proof-Step \&}\\\textbf{State Pair Count}}}  & \multicolumn{3}{c}{\thead{\textbf{Theorem}\\\textbf{Count}}}\\
%     \hline
%     & \textbf{Train} & \textbf{Test} & \textbf{Val} & \textbf{Train} & \textbf{Test} & \textbf{Val}\\
%     \hline
%     1. CompCert & 80288 & 6199 & - & 5440 & 501 & - \\
%     2. MathComp & 34543 & 711 & 720 & 11385 & 220 & 221 \\
%     3. GeoCoq & 104045 & 1866 & 2351 & 4537 & 89 & 88 \\
%     4. CategoryTheory & 4763 & 82 & 48 & 690 & 14 & 13 \\
%     5. \coq & 223639 & 8858 & 3119 & 22152 & 824 & 322 \\
%     6. \lean & 237003 & 4323 & 4220 & 56140 & 991 & 1035 \\
%     7. \multi & 460642 & 13181 & 7339 & 78292 & 1815 & 1357 \\
%     \end{tabular}
%     \caption{Size of different data-mixes. The \proofwala\;models were trained on the training split of \coq, \lean, and \multi\;data-mixes. After extracting proof-step and state pair data, the training, validation, and test split are randomly decided. For \lean\;and CompCert data-mix we used the same split as proposed by \citet{yang2023leandojo} and \citet{sanchez2020generating} respectively.}
%     \label{tab:data-mix-size}
% \end{table}

\begin{table}[ht]
    \centering
    % \footnotesize
    \scalebox{0.75}{
    \begin{tabular}{lrrrrrr}
    \hline
    & \multicolumn{3}{c}{\thead{\textbf{\# Proof-Step \& State Pairs}}}  & \multicolumn{3}{c}{\thead{\textbf{Theorem Count}}}\\
    \cmidrule(lr){2-4}\cmidrule(lr){5-7}
    \thead{\textbf{Data-mix}} & \textbf{Train} & \textbf{Test} & \textbf{Val} & \textbf{Train} & \textbf{Test} & \textbf{Val}\\
    \toprule
    1. CompCert & 80288 & 6199 & - & 5440 & 501 & - \\
    2. MathComp & 34196 & 1378 & 2285 & 11381 & 536 & 729 \\
    3. GeoCoq & 91120 & 12495 & 4928 & 4036 & 505 & 208 \\
    4. \coq & 205604 & 20072 & 7213 & 20857 & 1542 & 937 \\ %Update the sum
    5. \lean & 237003 & 4323 & 4220 & 56140\textsuperscript{\footref{fnote:lean-test-set-size}} & 991\textsuperscript{\footref{fnote:lean-test-set-size}} & 1035\textsuperscript{\footref{fnote:lean-test-set-size}} \\
    6. \multi & 442607 & 24395 & 11433 & 76997 & 2533 & 1972 \\
    7. CategoryTheory & 4114 & 610 & 208 & 573 & 101 & 43 \\
    \bottomrule
    \end{tabular}}
    \caption{Size of different data-mixes. The \proofwala\;models were trained on the training split of \coq, \lean, and \multi\;data-mixes. After extracting proof-step and state pair data, the training, validation, and test split are randomly decided so as to include at least 500 testing theorems, except in the case of CategoryTheory where the overall dataset is relatively small. For the \lean\;and CompCert data-mix we used the same split as proposed by \citet{yang2023leandojo}\footref{fnote:lean-test-set-size} and \citet{sanchez2020generating} respectively.
    }
    \label{tab:data-mix-size}
\end{table}
\addtocounter{footnote}{+1}
\footnotetext{\label{fnote:lean-test-set-size}While the LeanDojo dataset \citep{yang2023leandojo} officially has 2000 test theorems, only 991 of these are proved using tactics and have their tactics extracted in the dataset. Since our approach involves generating only tactic-based proofs, our Lean dataset is collected from those theorems with tactic-based proofs.}


\subsection{Model Details}
\label{sec:model-details}
We used the \codeTFive-\base\;\citep{wang2021codet5} pretrained model---which has \texttt{220} million parameters---to fine-tune models on the different data-mixes as described in \Cref{tab:data-mix}.
We trained three models \proofwala-\{\multi, \coq, \lean\} with the same step count and batch sizes for all settings. Training the models with the same number of steps aligns with recent work on training models for multilingual autoformalization \citep{jiang2023multilingualmathematicalautoformalization} which ensures that each model has the same number of gradient updates. Our models are initially trained on CompCert, Mathlib, MathComp, and GeoCoq. The hyperparameters used for training are described in \Cref{tab:hyperparams} in \Cref{app:hyperparams}. \Cref{app:hyperparams} also describes the amount of computing we used to train our models. 

To demonstrate the usefulness of our models on subsequent theorem-proving tasks, we perform further fine-tune of our \proofwala-\{\multi, \coq\} models on CategoryTheory\footref{fnote:CategoryTheory-url} theory data. We used the same hyperparameters as \Cref{tab:hyperparams} (in \Cref{app:hyperparams}) but we reduce the number of training steps to 1200 and batch size to 8.

\begin{table*}[ht]
    \centering
    \scalebox{0.8}{
    \begin{tabular}{lclllllll}
    \toprule
    \multicolumn{2}{c}{\textbf{Data-Mix}} &  
     & 
    \multicolumn{5}{c}{\textbf{Pass-at-$k$} \%} &
    \\
    \cmidrule(lr){1-2}\cmidrule(lr){4-8}
    \textbf{Name} & 
    \textbf{\# Theorems} & 
    \thead[c]{\textbf{Proof Step Model}} &
    \textbf{Pass@1} & 
    \textbf{Pass@2} & 
    \textbf{Pass@3} & 
    \textbf{Pass@4} & 
    \textbf{Pass@5} & 
    \thead[c]{\textbf{$p_{\mathrm{value}}$}\\(\textbf{$\alpha$}: 0.05)\footref{fnote:p-value}}\\
    \toprule
    \textbf{\lean} & 
    991 &
    \proofwala-\lean & 
    24.92 & 
    26.64 & 
    27.54 &
    28.05 &
    28.25 &
    \\
     & 
     & 
     \proofwala-\multi & 
     \textbf{26.84} & 
     \textbf{28.56} & 
     \textbf{29.67} &
     \textbf{29.97} &
     \textbf{30.58} &
     \textbf{0.018} \\
    \hline
    \textbf{MathComp} & 
    536 & 
    \proofwala-\coq & 
    \textbf{28.28} & 
    28.65 & 
    29.4 &
    29.59 &
    30.15 &
    \\
     &
     &
     \proofwala-\multi & 
     27.9 & 
     \textbf{29.21} & 
     \textbf{29.59} &
     \textbf{30.15} &
     \textbf{30.52} &
     0.355
     \\
    \hline
    \textbf{GeoCoq} & 
    505 & 
    \proofwala-\coq & 
    \textbf{32.87} & 
    \textbf{33.66} & 
    33.86 &
    34.06 &
    34.46 &
    \\
    & 
    & 
    \proofwala-\multi & 
    30.89 & 
    \textbf{33.66} & 
    \textbf{34.65} &
    \textbf{35.64} &
    \textbf{35.84} &
    0.135
    \\
    \hline
    \textbf{CompCert} &
    501 & 
    \proofwala-\coq &  
     17.56 & 
     18.76 & 
     19.16 &
     19.76 &
     20.76 &
     \\
     &
     & 
     \proofwala-\multi & 
     \textbf{17.96} & 
     \textbf{19.76} & 
     \textbf{20.56} &
     \textbf{21.16} &
     \textbf{21.96} &
     0.191 \\
    \hline
    \textbf{CategoryTheory} & 
    101 & 
    \proofwala-\coq-\CatTheory & 
    36.63 & 
    42.57 & 
    44.55 & 
    44.55 & 
    45.54 & \\
    & 
    & 
    \proofwala-\multi-\CatTheory & 
    \textbf{44.55} & 
    \textbf{51.49} & 
    \textbf{52.48} & 
    \textbf{53.47} & 
    \textbf{53.47} & 
    \textbf{0.008} \\
    \bottomrule
    \end{tabular}}
    \caption{Comparison between various \proofwala$\;$models and the \proofwala-\multi\; model on different data-mixes. We can see that transfer happening between Lean and Coq on all data-mixes from various domains in math and software verification. We observe that the \multi\; model outperforms the \lean\; and \coq\; models on all data mixes. The performance improvement is also statistically significant on the biggest data-mix \lean\; (Mathlib). We also observe that after further fine-tuning, the \multi\; model significantly outperforms the \coq\; model on the CategoryTheory dataset.
    }
    \label{tab:all-experiments}
\end{table*}
\addtocounter{footnote}{+1}
\footnotetext{\label{fnote:p-value}The results are statistically significant using a paired bootstrap test if $p_{\mathrm{value}} < 0.05$.}