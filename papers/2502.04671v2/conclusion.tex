 We introduced a unified framework for standardized data collection across ITPs like Lean and Coq, which supports proof completion by generating training data, training LMs for proof step prediction, and guiding search algorithms. Using this framework, we produced a multilingual proof step dataset and train the first multi-domain model across multiple ITPs, demonstrating improved transferability between Lean and Coq in mathematics and software verification. Beyond its technical contributions, the framework serves as a foundation for uniting and advancing theorem-proving research communities by providing a shared platform for experimentation and collaboration. In particular, by leveraging this framework, we established that multilingual training not only enables cross-language proof step completion but also outperforms monolingual models, underscoring the benefits of integrating data from diverse formal systems.
 
In future work, we propose exploring the integration of advanced search algorithms specifically tailored to our standardized framework. This could include developing adaptive search methods that dynamically adjust based on the complexity and characteristics of the theorem being proven. Additionally, further research could focus on optimizing the interaction between the LM and search algorithms to enhance proof efficiency and accuracy. Expanding the dataset to include more diverse ITPs and domains could also improve the model's generalizability and robustness. Finally, investigating the use of reinforcement learning to continuously improve the model based on feedback from successful and failed proof attempts could provide significant advancements in formal theorem proving.