\section{Related Work}
\label{related work}
Sequence recommendation is one of the crucial tasks in recommender systems, and its core goal is to accurately predict the possible future sequence of items of interest based on the user's historical behavior sequence. At the early stage of the research on sequence recommendation, scholars drew on methods such as Markov chains to deal with the recommendation problem of sequence information ____, ____. However, with the rapid development of deep neural networks, deep learning models such as Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN) have been introduced to sequence recommendation tasks. Although these traditional methods have achieved some success, they generally suffer from the long-tail effect, slow training speed, and limited ability to extract temporal features in long sequence recommendation scenarios.

In recent years, the attention mechanism, Graph Neural Network (GNN), and Contrastive Learning (CL) have been applied to sequence recommendation models due to their powerful comprehension capabilities, and have become mainstream research directions. In this section, we review the sequence recommendation models using these techniques.

\subsection{Attention Mechanism}
The attentional mechanism, an approach inspired by the human visual and cognitive systems, allows neural networks to selectively focus on important information within the input data during processing. This mechanism effectively improves the performance and generalization of the model. Zhou et al. ____ introduced the attention mechanism into the Gated Recurrent Unit (GRU) model and designed the AUGRU module to cope with the problem of user interest drift. This method enables the model to better adapt to changes in user interests, however, it significantly reduces the computational efficiency of the network due to the high complexity of the GRU module in the computation process. Kang et al. ____ proposed the SASRec recommendation model, an adaptation of the Transformer architecture ____. SASRec introduces trainable positional embeddings, enabling the model to differentiate items based on their positional context. Additionally, it employs a multi-head self-attention mechanism to capture intricate relationships within sequences, thereby enhancing the model's ability to represent inter-item dependencies. Shin et al. ____ proposed the BSARec model to balances the strengths of both approaches and mitigates the over-smoothing problem, which combines the Fourier transform and the self-attention mechanism. Liu et al. ____ proposed a novel linear attention mechanism in long sequence recommendation systems, named LinRec. LinRec reduces the complexity of the Transformer by changing the dot product order, $L_2$-normalization, and ELU activation, while maintaining the accuracy.
% Since the front-to-back order in a sequence is not necessarily strictly ordered, the left-to-right unidirectional modeling approach of RNN and SASRec can limit the ability of hidden representations in sequences of user behaviors. To address this problem, BERT4Rec ____ sequence recommendation model is proposed. BERT4Rec utilizes a bi-directional multi-head attention mechanism to capture sequence information and then learns this bi-directional representation model by predicting random masking terms in a sequence through joint conditioning on the left and right contexts of the random masking terms in the sequence. 

% In addition, some recommendation models combine the attention mechanism with Long Short-Term Memory (LSTM) to learn both long-term and short-term interest preferences in user sequence data ____. The model utilizes a multi-head self-attention mechanism to capture the multiple interest preferences present in a sequential session on the one hand and uses LSTM to learn long-term interest features on the other. Finally, the long and short-term user interests are fused by a simple gating network to synthesize the long-term and short-term interests of the users, thus improving the performance of the recommender system. Lin et al. ____ proposed an approach to fuse attention mechanisms with memory networks. This method uses a target item and a memory vector as a dual query to retrieve information in a long sequence, and then the retrieved sequence information is used to update the memory vector. Then, the updated memory vectors are used to query the sequence to model the long-term dependencies within the sequence, thus solving the difficulties faced in modeling very long sequences.

\subsection{Graph Neural Network}
GNN is a class of deep learning models specifically designed for processing graph-structured data, among which the Graph Convolutional Network (GCN) ____ is one of the most representative models. GCN extracts feature information by performing convolutional operations on graphs to achieve deep learning and analysis of graph data. In recent years, many researchers have applied graph neural networks to the recommendation domain, achieving significant experimental results. Wang et al. proposed ____ an innovative recommendation method, called NGCF, which considers both users and items as nodes in a graph structure and constructs an information network using user-item interaction records. By representing users and items as nodes and capturing their interactions, NGCF effectively extends user and item representations. LightGCN ____ simplifies the model structure by using a simple aggregation weighting method to enhance the efficiency of model training and implementation. Specifically, LightGCN utilizes the user-item interaction matrix to construct the graph structure and updates node features through weighted aggregation. Chang et al. ____ proposed a metric learning-based approach for building user interest graph structures. This method first constructs a dynamic graph by measuring node similarity as a key metric and connecting two nodes only when their similarity exceeds a predefined threshold. Subsequently, the importance of nodes surrounding the target node is modeled by computing attention scores between the target node and its neighbors, as well as the attention coefficients of neighboring nodes relative to the target node. TransGNN ____ combines the strengths of the Transformer and GNN to help the GNN expand its receptive field. TransGNN uses three types of positional encodings to capture graph structural information and then alternates between the Transformer and GNN layers to focus each node on the most relevant samples. 

% In the research of applying graph neural networks to sequence recommendation, researchers usually construct the historical user-item interaction sequences into a sequence graph structure and then use graph neural networks to aggregate the neighbor information, to achieve the updating of node features and representation optimization. SR-GNN ____ is a recommendation method based on graph neural networks, which is specifically used to deal with the information of user's session sequences. It first constructs the user behavior sequence as a directed graph structure and iteratively updates the feature information of each node using a gated graph neural network (GGNN). In this process, GGNN can effectively capture the transfer patterns and dependencies between nodes, thus reflecting the dynamics and complexity of user behavior. Finally, with the help of attention network, SR-GGNN weights and fuses the node features in the graph to obtain a representation of the whole session, which is used to predict the user's next behavior.  Ma et al. ____ proposed MA-GNN that integrates the long and short-term interests of users to efficiently model user sequences. For users' short-term interests, MA-GNN performs graph modeling of recent session sequences through a sliding window strategy. For the long-term interests of users, MA-GNN uses a memory network to assist in modeling. This memory network records the potential long-term interest characteristics of all users, providing the system with global long-term interest information. Finally, short-term interest, long-term interest, and other related features are fused through the gating network to achieve a comprehensive multi-dimensional consideration of user interest. This approach not only fully considers the user's long-term and short-term interests, but also realizes the effective fusion of multiple interest features through the design of the gating network, thus improving the accuracy and efficiency of interest modeling.

\subsection{Contrastive Learning}
The core goal of CL is to achieve effective data representation learning by minimizing the distance between positive samples and a given anchor while maximizing the distance between negative samples and the same anchor. In sequence recommendation, CL can address the challenges posed by sparse interaction data and enhance the learning of effective representations ____, ____. CL4SRec ____ is the first to introduce CL into the sequential recommendation domain. CL4SRec combines the traditional sequential prediction objective with a CL objective to improve the accuracy and versatility of recommender systems. Specifically, CL4SRec constructs user sequences from different perspectives and utilizes a contrastive loss function to learn more accurate user representations. DuoRec ____ improves the distribution of sequence representations and item embeddings by introducing CL regularization while utilizing both unsupervised and supervised contrastive samples. DCRec ____ improves the accuracy and diversity of recommender systems by unifying sequential pattern encoding with global synergistic relationship modeling through adaptive consistency-aware augmentation. In addition, DCRec utilizes CL for self-supervised signal extraction across different views, effectively capturing intra-sequence item transition patterns and inter-sequence user dependencies.