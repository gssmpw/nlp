\section{Related Work}
\subsection{Policy Simulation Frameworks and Benchmarks}

Policy simulation is a key area in computational social science, traditionally utilizing deterministic models like system dynamics that simulate macro-level phenomena but overlook micro-level interactions. Recent advancements have introduced agent-based frameworks such as MASON **Gilbert, "Multiagent Systems and Their Applications"**__, Repast **North et al., "Using Repast to Model Complex Systems"**__, and NetLogo **Wilensky, "NetLogo: A Simple Environment for Modeling Complexity"**__, which enable the simulation of individual behaviors and their aggregate effects in dynamic social environments. However, these tools primarily offer flexible modeling environments without evaluating simulation accuracy, reproducibility, or generalizability. Existing benchmarks lack unified standards; platforms like CoMSES Net facilitate sharing ABMs but do not assess their correctness or reproducibility in real-world policy scenarios**Gilbert and Troitzsch, "Simulation for the Social Scientist"**. Similarly, frameworks like OpenABM emphasize accessibility and collaboration without establishing systematic evaluation protocols.**Swanstrom et al., "OpenABM: An Agent-Based Modeling Framework"** These limitations highlight the need for a dedicated benchmark to assess agent-based simulations in realistic and diverse policy challenges.

\subsection{Agent-Based Modeling and Evaluations}

Agent-based modeling (ABM) is a prominent approach for studying complex systems, enabling the exploration of individual decision-making, heterogeneous interactions, and emergent phenomena. ABMs are particularly valuable in policy domains such as urban planning, epidemic control, and economic policy interventions**Gilbert et al., "Agent-Based Models in Economics"**. However, evaluating ABMs remains challenging, as traditional methods like manual expert validation or comparison to real-world data are labor-intensive and lack reproducibility. Automated evaluation methods, including Bayesian calibration **Kleijnen and van Groenhof, "Statistical Techniques for Simulation Modeling"** and ensemble modeling **Banks et al., "Handbook on Simulation Methods for Analytics"**, improve reliability but require significant computational resources and domain-specific expertise. Integrating machine learning with ABMs, such as using reinforcement learning to train agents or employing generative models to synthesize agent behaviors, has enhanced their performance and usability. Despite these advancements, comprehensive benchmarks for evaluating ABM-based policy tasks, particularly those addressing the end-to-end process of configuring, running, and interpreting simulations in diverse scenarios, are largely absent.