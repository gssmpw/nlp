\section{Related Work}
\subsection{Policy Simulation Frameworks and Benchmarks}

Policy simulation is a key area in computational social science, traditionally utilizing deterministic models like system dynamics that simulate macro-level phenomena but overlook micro-level interactions. Recent advancements have introduced agent-based frameworks such as MASON \cite{MASON}, Repast \cite{Collier2001}, and NetLogo \cite{tisue2004netlogo}, which enable the simulation of individual behaviors and their aggregate effects in dynamic social environments. However, these tools primarily offer flexible modeling environments without evaluating simulation accuracy, reproducibility, or generalizability. Existing benchmarks lack unified standards; platforms like CoMSES Net facilitate sharing ABMs but do not assess their correctness or reproducibility in real-world policy scenarios\cite{Windrum2007EmpiricalVO}. Similarly, frameworks like OpenABM emphasize accessibility and collaboration without establishing systematic evaluation protocols.\cite{Hinch2021} These limitations highlight the need for a dedicated benchmark to assess agent-based simulations in realistic and diverse policy challenges\cite{janssen2006empirically}.

\subsection{Agent-Based Modeling and Evaluations}

Agent-based modeling (ABM) is a prominent approach for studying complex systems, enabling the exploration of individual decision-making, heterogeneous interactions, and emergent phenomena. ABMs are particularly valuable in policy domains such as urban planning, epidemic control, and economic policy interventions\cite{GONZALEZMENDEZ2021105110,An2017,AN2021109685}. However, evaluating ABMs remains challenging, as traditional methods like manual expert validation or comparison to real-world data are labor-intensive and lack reproducibility. Automated evaluation methods, including Bayesian calibration\cite{thober2017, Lamperti2018} and ensemble modeling \cite{https://doi.org/10.1111/ajae.12174}, improve reliability but require significant computational resources and domain-specific expertise\cite{HUBER2018143}. Integrating machine learning with ABMs, such as using reinforcement learning to train agents or employing generative models to synthesize agent behaviors, has enhanced their performance and usability. Despite these advancements, comprehensive benchmarks for evaluating ABM-based policy tasks, particularly those addressing the end-to-end process of configuring, running, and interpreting simulations in diverse scenarios, are largely absent.