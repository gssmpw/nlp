\section{Introduction}

Fine-tuning pretrained Large Language Models (LLMs) has become a standard approach for downstream tasks. Traditional first-order (FO) optimizers like Adam~\cite{AdamKingmaB14}, commonly used for this process, rely on backpropagation. However, as highlighted in \citet{MalladiGNDL0A23Mezo}, computing gradients for LLMs can require up to 12 times the memory needed for inference. This scaling challenge becomes even more pronounced as models grow larger, imposing significant memory demands and complicating the fine-tuning process, especially in resource-constrained environments.

To address these computational challenges, Parameter-Efficient Fine-Tuning (PEFT) methods have been developed. These techniques freeze most of the model’s parameters and train only a small subset, significantly reducing both memory and computational overhead. Popular PEFT approaches include prompt tuning, LoRA fine-tuning, and prefix tuning. Prompt tuning~\cite{LesterAC21PromptTuning, QinE21PromptTuning,YuCL023} optimizes continuous prompt vectors that are concatenated with the input embeddings, while prefix tuning~\cite{LiL20PrefixTuning} introduces learnable prefix tokens that serve as conditioning variables at each transformer layer. LoRA (Low-Rank Adaptation)~\cite{HuSWALWWC22LORA, HoulsbyGJMLGAG19LORA} modifies the model's attention and feedforward layers by injecting low-rank trainable matrices, further reducing the resources required for fine-tuning.

While Parameter-Efficient Fine-Tuning (PEFT) methods reduce training costs and memory usage, they may not always achieve the same level of task-specific performance as full model fine-tuning. Research has shown that for tasks requiring high accuracy, complex adaptations, or domain-specific knowledge, full fine-tuning often outperforms PEFT approaches due to its ability to adjust all model parameters for better adaptation \cite{HuSWALWWC22LORA,LiL20PrefixTuning,ZakenGR22}. To make full model fine-tuning more computationally feasible, zeroth-order methods offer an alternative by reducing the high computational cost. Rather than computing gradients via backpropagation, zeroth-order methods estimate the gradient using only the forward pass. Initially explored in the 1990s \cite{Spall92, NesterovS17, GhadimiL13a, DuchiJWW15, LiuCKZHV20}, these methods have recently gained traction for fine-tuning LLMs \cite{MalladiGNDL0A23Mezo, DeLiMaSo23, LingCYL024} and have been shown to be able to outperform first-order PEFT methods given enough training time~\cite{ZhangLHLZZCLY0W24Zobench}.

However, zeroth-order methods often rely on simple, fixed prompts during fine-tuning. In tasks like sentiment analysis with the SST-2 dataset, templated prompts (e.g., \emph{``$<CLS>$ text data. It was [terrible $\mid$ great]. $<SEP>$"}) are crucial for success \cite{ZhangLHLZZCLY0W24Zobench}. These prompts effectively align the text data with task-specific objectives. Therefore, selecting such templates becomes a key hyperparameter, raising the question: Can we automatically discover effective prompts for zeroth-order fine-tuning through prompt tuning? More broadly, can PEFT methods complement zeroth-order fine-tuning for large models? In this work, we propose a new framework to answer this question. 

While our focus has thus far been on single-task fine-tuning, many scenarios necessitate multi-task fine-tuning. Multi-task learning (MTL) enables a model to handle multiple tasks simultaneously, fostering knowledge transfer between tasks and improving overall efficiency \cite{MinLZH22MetaICL, yang2024}. This approach is particularly valuable in low-resource settings, where collecting large labeled datasets can be costly, as is often the case with medical data. In such environments, few-shot learning—where a model is fine-tuned on a high-resource dataset to quickly adapt to new tasks with minimal data—becomes essential \cite{YeLR21CrossfitFewShot}.

To address the challenges of multi-task and few-shot learning in natural language processing, several meta fine-tuning methods have been proposed \cite{HuLLPDL23, ZhGWZYW24, YeLR21CrossfitFewShot, AsBKLZC24}. However, traditional meta fine-tuning approaches, such as MetaICL \cite{MinLZH22MetaICL}, still require full-model first-order gradient calculations, which become computationally expensive with large language models (LLMs) containing billions of parameters. Given the success of zeroth-order methods in fine-tuning LLMs for individual tasks, the potential for adapting their applicability to multi-task learning remains largely unexplored.

With the effectiveness of zeroth-order fine-tuning and the advantages of PEFT for single tasks, a natural question arises: Can we develop a new multi-task and few-shot learning methodology that significantly reduces computational costs while maintaining or even enhancing performance? Specifically, can we leverage the efficiency of zeroth-order fine-tuning alongside the adaptability of PEFT within multi-task and few-shot learning for large language models?


\subsection{Contributions}
\textbf{Bilevel Fine Tuning Framework:} In this work we introduce a bilevel framework to enhance zeroth-order fine-tuning for large pre-trained language models using Parameter-Efficient Fine-Tuning (PEFT). It involves two levels of optimization: one for fine-tuning the base model and another for selecting the best PEFT parameters. The two levels are nested such that each level is enhancing the performance of the other level.

\textbf{Efficient Training Method:} We propose the Bilevel Zeroth-Order-First-Order (Bilevel ZOFO) method to solve the bilevel optimization problem. It incorporates zeroth-order approximations into first-order bilevel methods, avoiding full model gradient computation and addressing the high resource demands of traditional methods, especially for large models. We provide the convergence guarantees of the  proposed method. Experiments demonstrate the superiority of Bilevel ZOFO, achieving better results than traditional PEFT and zeroth-order methods in single-task settings while maintaining similar memory efficiency.

\textbf{Mitigating the sensitivity of hard prompts in ZO fine tuning} The standard zeroth-order fine-tuning is very sensitive to the hard prompt used. Our method mitigates this sensitivity problem. In our experiments, the proposed method exhibits significantly lower sensitivity to the choice of hard prompts.


\textbf{Light-weight Method for Multi-Task Learning:} The framework is expanded to multi-task learning, where zeroth-order fine-tuning reduces computational costs, and PEFT models allow efficient task-specific fine-tuning. This results in a lightweight and adaptable meta-training process suitable for new tasks. In the experiments, our method outperforms existing approaches in multi-task learning scenarios.
