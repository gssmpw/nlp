\section{Related work}
\subsection{Zeroth order in fine tuning LLMs}
\cite{MalladiGNDL0A23} applies the zeroth-order method to fine-tune LLMs for downstream tasks. They demonstrate that their method is compatible with both full-parameter and parameter-efficient tuning techniques, such as LoRA and prefix tuning, while being much more computationally efficient. \cite{ZhLiHo24} provide a benchmark for zeroth-order optimization in the context of LLM fine-tuning, comparing different zeroth-order optimizers and applying the method to tune various models. \cite{GautamPZRH24} incorporate variance reduction techniques into zeroth-order methods for fine-tuning, enhancing stability and convergence. \cite{QinCQDLD24, LingCYL024} apply zeroth-order methods in federated fine-tuning. \cite{DengLMS24} implement zeroth-order optimization for the softmax unit in LLMs. \cite{GuoLZLY24, LiuZGCHY24} explore fine-tuning an extremely small subset of LLM parameters using zeroth-order methods by sparsifying the gradient approximation or the perturbations used in estimating the gradient. \cite{TaPNMM24} investigate the privatization of zeroth-order optimization methods.

\subsection{Fine-tuning LLMs for Multitask and Few-Shot Learning}
Typical meta-tuning employ first-order methods to train autoregressive LLMs on a collection of multitask data for different tasks, \cite{ZhongLZK21,MinLZH22,GuoXR24}.  
\cite{ZhongLZK21} apply meta-training to various tasks such as hate speech detection, question categorization, topic classification, and sentiment classification. \cite{GuoXR24} adopt the same method in \cite{MinLZH22} for generating stylish text.
\cite{MinLZH22} focus on enhance the in-context learning ability of the meta-trained model for multitask learning while \cite{ZhongLZK21} focus on the zeroth shot ability.
During training, \cite{MinLZH22} samples a task from a collection for each iteration to perform in-context learning. Compared to \cite{ZhongLZK21,MinLZH22}, we use a bilevel structure that  fine tune the full LLM at the upper level while tune PEFT models at the lower level.
During testing, we freeze the meta-tuned based model and only fine-tune with a few-shot example setup with the PEFT model, which is cheaper and more efficient.
Most importantly, we employ the zeroth-order method in  meta tuning the base model at the upper level. This allow us to avoid the need to compute back probagations of the meta-model, significantly reducing computational costs.
 As shown in our experiments, the bilevel structure that relate the meta-model with the best suitable PEFT mode our performs tuning the base model or the PEFT model sololy. 

\subsection{penalty methods for bilevel optimization}
Solving a bilevel optimization problem is challenging because the function value in the upper-level objective depends on the optimizer of the lower-level problem. This makes it difficult to calculate the gradient of the upper-level objective, also known as the hypergradient. Classical methods require computing the Hessian-vector multiplication to approximate the hypergradient \cite{FrDoFrPo17,FrFrSaGrPo18,FiAbLe17,LiGuHu21,RaFiKaLe19,GhWa18,ChKaZh22,LiXiFeZhYoPiUZ18}. However, when fine-tuning large language models, calculating the Hessian-vector multiplications through backpropagation becomes extremely expensive due to the computational and memory costs involved.

Recently, new frameworks for bilevel optimization methods have emerged \cite{LuMei24, ShenC23, LiuLYZZ24, KwonKWN23}. Instead of relying on hypergradients, which require second-order information, these methods treat the bilevel problem as a constrained optimization problem. The constraint is penalized, allowing the problem to be reformulated as a minimax problem that only requires first-order information. These new methods significantly reduce computational costs since they avoid the need to calculate Hessian-vector multiplications to obtain second-order information.

\cite{LiuYWSL22} propose a double loop method based on the dynamic barrier gradient descent method. They  analyzed the when the lower level is PL function.


 \cite{LuMei24} considers the two cases where the lower level problem is constrained and unconstrained when the lower level objective is nonsmooth convex.  They proposed a three loop method that solves an strongly convex strongly concave subproblem.


 \cite{KwonKWN23} considers the unconstrained lower level problem. They show that there is a bias between the hypergradients of the penalized objective function and the original one. Baes on this finding, they propose a double loop method that corrected this bias. They also propose a single loop method that accelerate the convergence with momentum. The convergence of the proposed methods in \cite{KwonKWN23}  are investigated when the lower level objective is strongly convex and twice continuously differentiable.

 \cite{ShenC23} considered the case where the lower level problem is constrained and unconstrained when the lower level objective is nonsmooth convex. Especially,  they investigate when the penalty reformulation recovers the solutions of the original bilevel problem under the assumption that the lower level objective function is a Polyak-Lojasiewicz (PL) function/the lower level problem satisfies the growth condition or error bound condition respectively for the unconstrained and constrained cases. They porposed a double loop method and analyzed its convergence rate. When is lower level problem is nonconvex, \cite{LiuLYZZ24} propose a single loop method that convexify the lower level problem with Moreau envelope.

  Note that only \cite{LiuLYZZ24} and \cite{LuMei24} investigate the convergence of the proposed method to the original problem while the others only shows the convergence to the penalized problem.  In this paper, we adapt the method in \cite{LuMei24} to approximate part of the upper level paramters with zeroth order approximation in order to address the huge number of training parameters in large language models. We provide convergence guarantee for this adapted Zoroth-order-first-order method.