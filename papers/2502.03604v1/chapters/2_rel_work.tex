\section{Related work}
\subsection{Zeroth order in fine tuning LLMs}
MeZO~\citep{MalladiGNDL0A23Mezo} is the first work to use Zeroth-Order~(ZO) methods to finetune LLMs for downstream tasks. They demonstrate that their method is compatible with both full-parameter tuning and parameter-efficient tuning techniques, such as LoRA and prefix tuning, while being significantly more computationally efficient. \citet{ZhangLHLZZCLY0W24Zobench} provide a benchmark for ZO optimization in the context of LLM fine-tuning, comparing different ZO optimizers and applying the method to various models. \citet{GautamPZRH24} introduce variance reduction techniques into ZO methods for fine-tuning, improving both stability and convergence. In addition, ZO methods are applied in federated fine-tuning by \citet{QinCQDLD24} and \citet{LingCYL024}. \citet{DengLMS24} implement ZO optimization for softmax units in LLMs. \citet{GuoLZLY24} and \citet{LiuZGCHY24} explore fine-tuning a minimal subset of LLM parameters using ZO methods by sparsifying gradient approximation or the perturbations used in gradient estimation. \citet{TaPNMM24} investigate the privacy of ZO optimization methods. {\color{blue}}

In contrast to previous approaches, we propose a bilevel training algorithm that effectively combines the strengths of both First-Order~(FO) Parameter-Efficient Fine-Tuning (PEFT) and ZO full-model fine-tuning. Our experiments demonstrate that the bilevel structure, when paired with the most suitable PEFT technique, outperforms both ZO full-model fine-tuning and FO PEFT methods individually.

\subsection{Fine-tuning LLMs for Multitask and Few-Shot Learning}
Typical meta-tuning approaches employ First-Order methods to train autoregressive LLMs on a multitask dataset for various tasks \citep{ZhongLZK21, MinLZH22MetaICL, GuoXR24}. \citet{ZhongLZK21} apply meta-training to tasks such as hate speech detection, question categorization, topic classification, and sentiment classification. \citet{GuoXR24} adopt the method from \citet{MinLZH22MetaICL} for generating stylistic text. While \citet{MinLZH22MetaICL} focus on enhancing the in-context learning ability of the meta-trained model for multitask learning, \citet{ZhongLZK21} focus on improving zero-shot performance.

During training, \citet{MinLZH22MetaICL} sample a task from the dataset for each iteration to perform in-context learning. In contrast to \citet{ZhongLZK21} and \citet{MinLZH22MetaICL}, our approach uses a bilevel structure: the full LLM is fine-tuned at the upper level, while parameter-efficient fine-tuning (PEFT) models are tuned at the lower level. At test time, we freeze the meta-tuned base model and fine-tune only the PEFT model using a few-shot setup, which is both more cost-effective and efficient. Crucially, \citet{MinLZH22MetaICL} fine tune the full model with first order methods, while we employ a ZO method in meta-tuning the base model at the upper level. Our approach allows us to bypass the need for backpropagation in the meta-model, significantly reducing computational costs.
 
\subsection{Penalty Methods for Bilevel Optimization}

Solving a bilevel optimization problem is challenging because the function value in the upper-level objective depends on the optimizer of the lower-level problem. This makes it difficult to compute the gradient of the upper-level objective, also known as the hypergradient. Classical methods require calculating Hessian-vector multiplications to approximate the hypergradient~\citep{FrDoFrPo17, FrFrSaGrPo18, FiAbLe17, LiGuHu21, RaFiKaLe19, GhWa18, ChKaZh22, LiXiFeZhYoPiUZ18}. However, when fine-tuning large language models, this process becomes extremely expensive due to the high computational and memory demands.

Recently, new frameworks for bilevel optimization have been introduced~\citep{LuMei24, ShenC23, LiuLYZZ24, KwonKWN23, LiuYWSL22}. These methods bypass the need for second-order information by reformulating the bilevel problem as a constrained optimization problem. The constraint is penalized, allowing the problem to be tackled as a minimax problem using only first-order information. These methods significantly reduce computational costs by eliminating the need for second-order information. Nevertheless, when fine tuning LLMs, back propagation for calculating the gradient of an LLM is still too expensive.

\citet{LiuLYZZ24} and \citet{LuMei24} explore the convergence of their proposed methods to the original bilevel problem, while other approaches only demonstrate convergence to the penalized problem. In this paper, we adapt the method from \citet{LuMei24} to approximate part of the upper-level parameters using a ZO approximation In order  to address the challenge posed by the large number of training parameters in large language models. We also provide convergence guarantees for this adapted zeroth-order-first-order method.

