\section{Experiments}

We conduct extensive experiments on various LLMs of different scales to demonstrate the effectiveness of bilevel-ZOFO in both single-task and multi-task meta-training settings. Our proposed structure is able to incorporate any variation of zeroth-order methods in the upper-level step and any PEFT method in the lower level. To maintain focus on testing the effectiveness of the proposed bilevel structure and its unique multitask learning capabilities, we used the classic MeZO ~\cite{MalladiGNDL0A23Mezo}. 

\subsection{Bilevel-ZOFO for Single-task Fine-tuning}
Following MeZO~\citep{MalladiGNDL0A23Mezo}, we evaluate our approach on a range of classification and multiple-choice tasks. In this setting, training and testing are conducted on the same task. We employ prompt-tuning~\citep{LesterAC21PromptTuning}, prefix-tuning~\citep{LiL20PrefixTuning}, and LoRA~\citep{HuSWALWWC22LORA}- well-known PEFT baselines-for lower-level training to validate bilevel-ZOFO under different conditions and resource constraints. During each lower-level update, we update only the PEFT parameters, and during the upper-level optimization step, we tune the full model using zeroth-order gradient approximation. We perform 10 lower-level updates between each pair of upper-level updates. For each task, we randomly sample 1000 examples for training, 500 examples for validation, and 1000 examples for testing. We use the Adam optimizer~\citep{AdamKingmaB14} and report test accuracy or F1-score.

We compare our method against several baselines, including MeZO for Full Model Fine-tuning, MeZO for  PEFT, and First-order PEFT. We fix the total memory budget of each step across bilevel-ZOFO and the baselines. We train zeroth-order methods for 10,000 steps, and first-order methods for 5000 steps. For all experimental details, refer to the Appendix~\ref{app:single-hyperparameters} and Appendix~\ref{app:sigle-task-training}. We also provide the training loss for the lower-level objective of the bilevel framework in Figure~\ref{fig:loss_curve} of Appendix~\ref{app:appendix-results} to show that consistent with the guarantees provided by our theoretical analysis in Section~\ref{sec:theory}, Bilevel-ZOFO converges. We compare the memory requirements of our method with the baselines in Figure~\ref{fig:memory-profiling}, and provide wall-clock analysis in Table~\ref{tab:wall_clock_time}. Figure~\ref{fig:memory-profiling} and Table~\ref{tab:wall_clock_time} show that bilevel-ZOFO does not increase the memory usage of PEFT and MeZO.


\textbf{Bileve-ZOFO mitigates sensitivity to hard prompts:} 
We begin our experiments by showing that Bilevel-ZOFO mitigates the sensitivity of MeZO to hard prompts. First, similar to Table 5 of the MeZO~\cite{MalladiGNDL0A23Mezo} paper, we experiment with different choices of hard prompts for both MeZO and our Bilevel-ZOFO. Table~\ref{tab:prompt_sensitivity_comparison} shows the results for tuning Opt 1.3b on SST2 and COPA. We can see that our method effectively mitigates the sensitivity of MeZO to hard prompts. The difference between the results with and without a simple hard prompt in our experiment is much less than MeZO's. See Appendix~\ref{sec:sensitivity-analysis} for a full discussion.


\begin{table}[!t]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{llccc}
\hline
\textbf{Method}         & \textbf{Task} & \textbf{w/ prompt (\%)} & \textbf{w/o prompt (\%)} & \textbf{Diff.} \\ \hline
\multirow{2}{*}{MeZO}   & SST-2               & 89.6                      & 51.9                        & -38.6               \\ 
                        & COPA                & 70.0                      & 54.8                        & -15.2               \\ \hline
\multirow{2}{*}{Bilevel-ZOFO} & SST-2         & 93.3                      & 92.9                        & \textbf{-0.4}                \\ 
                        & COPA                & 76.66                     & 73.6                        & \textbf{-3.06}               \\ \hline
\end{tabular}}
\caption{Prompt Sensitivity Comparison for MeZO and Bilevel-ZOFO. Bilevel-ZOFO effectively mitigates the sensitivity of MeZO to hard prompts.}
\label{tab:prompt_sensitivity_comparison}
\end{table}

\begin{table*}[!t]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{llcccccccccc} \toprule
Trainer & Mode & BoolQ & CB & Copa & ReCoRD & RTE & SST2 & WIC & WinoGrande & WSC & Average \\ \midrule 
\multirow{5}{*}{MeZO} & ft & 0.6927 & 0.7767 & 0.7000 & 0.6980 & 0.6587 & 0.8214 & 0.5543 & 0.5480 & 0.5054 & 0.6617 \\ 
& lora & 0.6860 & 0.7607 & 0.7200 & 0.7083 & 0.6755 & 0.8501 & 0.5549 & 0.5607 & 0.5570 & 0.6748 \\ 
& prefix & 0.6573 & 0.7945 & 0.7033 & 0.7047 & 0.6972 & 0.8218 & 0.5622 & 0.5370 & 0.5105 & 0.6654 \\ 
& prompt & 0.6260 & 0.5821 & 0.7067 & 0.7070 & 0.5415 & 0.7463 & 0.5574 & 0.5556 & 0.4654 & 0.6098 \\ \cmidrule{2-12} 
& average & 0.6655 & 0.7285 & 0.7075 & 0.7045 & 0.6432 & 0.8099 & 0.5572 & 0.5503 & 0.5096 & 0.6529 \\ \midrule 
\multirow{4}{*}{FO} & lora & 0.7456 & 0.8512 & 0.7500 & 0.7206 & 0.7292 & 0.9258 & 0.6463 & 0.5806 & 0.6474 & 0.7330 \\ 
& prefix & 0.7300 & 0.8571 & 0.7167 & 0.7093 & 0.7136 & 0.8133 & 0.5387 & 0.5787 & 0.5705 & 0.6920 \\ 
& prompt & 0.7150 & 0.7142 & 0.7466 & 0.7163 & 0.6936 & 0.8016 & 0.5386 & 0.5980 & 0.5062 & 0.6700 \\ \cmidrule{2-12} 
& average & 0.7302 & 0.8075 & 0.7378 & 0.7154 & 0.7121 & 0.8470 & 0.5745 & \textbf{0.5857} & 0.5747 & 0.6977 \\ \midrule 
\multirow{4}{*}{Ours} & lora & 0.7433 & 0.9167 & 0.7400 & 0.7183 & 0.7401 & 0.9331 & 0.6447 & 0.5903 & 0.6428 & \cellcolor[HTML]{C0C0C0}0.7410 \\ 
& prefix & 0.7340 & 0.8690 & 0.7267 & 0.7140 & 0.7304 & 0.8550 & 0.6317 & 0.5710 & 0.5810 & \cellcolor[HTML]{C0C0C0}0.7125 \\ 
& prompt & 0.7367 & 0.7679 & 0.7633 & 0.7257 & 0.6867 & 0.8335 & 0.6267 & 0.5900 & 0.5133 & \cellcolor[HTML]{C0C0C0}0.6938 \\ \cmidrule{2-12} 
& average & \textbf{0.7380} & \textbf{0.8512} & \textbf{0.7433} & \textbf{0.7193} & \textbf{0.7191} & \textbf{0.8739} & \textbf{0.6344} & 0.5838 & \textbf{0.5790} & \cellcolor[HTML]{C0C0C0}0.7158 \\ 
\bottomrule 
\end{tabular}
}
\caption{Single-Task Experiments on OPT-1.3B with 1000 samples. Values correspond to mean across three random seeds. FO: First-Order. FT: full-model fine-tuning. See Table~\ref{tab:single-task-opt} in the Appendix  for standard deviation values.}
\label{tab:single-task-opt-main}
\end{table*}



Table~\ref{tab:single-task-opt-main} presents the test metrics when applying bilevel-ZOFO and baselines to fine-tune OPT-1.3B~\citep{OPT} on a downstream task. Table~\ref{tab:single-task-llama2-7b-main} demonstrates the results for Llama2-7b~\citep{Llama2}. We can make the following observations:

\textbf{Bilevel-ZOFO outperforms MeZO on almost all tasks:} 
With the same memory allocation per training step, bilevel-ZOFO outperforms MeZO, even when trained for half the number of iterations across most tasks. 

\textbf{Bilevel-ZOFO outperforms FO PEFT on average:} 

Comparing each FO-PEFT setting with the corresponding bilevel-ZOFO setting, we see that bilevel-ZOFO outperforms the corresponding FO-PEFT methods across most instances and \textbf{on average}.


\textbf{Bilevel-ZOFO outperforms baselines more significantly in resource-constrained settings:} 
 The number of parameters tuned for prefix tuning and prompt tuning is lower than for LoRA (See Figure~\ref{fig:low_source_increases}). As shown in Table~\ref{tab:single-task-opt-main}, when fewer parameters are tuned, bilevel-ZOFO demonstrates a larger improvement over first-order methods in tuning PEFT models. Since memory usage and training steps remain the same, bilevel-ZOFO proves to be a more suitable option for fine-tuning LLMs in constrained environments compared to PEFT and MeZO.


\textbf{Bilevel-ZOFO scales effectively to larger LLMs:} Table~\ref{tab:single-task-llama2-7b-main} compares bilevel-ZOFO with the baselines when fine-tuning Llama2-7b~\citep{Llama2} on various classification and open-ended generation tasks. The results show that bilevel-ZOFO's advantages are not confined to smaller models like OPT-1.3b, but also extend to larger LLMs.


\subsection{Multi-Task Fine-Tuning Experiments}
Following \citet{MinLZH22MetaICL}, we evaluate the performance of bilevel-ZOFO as a fast and efficient meta-learning algorithm. We perform experiments using four of the distinct meta-learning settings: classification-to-classification, non-classification-to-classification, QA-to-QA, and non-QA-to-QA. For instance, in non-classification-to-classification setting, we train on a number of non-classification subtasks and test on a number of distinct classification subtasks. Each of these \emph{meta-learning tasks} includes a set of training sub-tasks and a different set of test sub-tasks. The sub-tasks are sourced from CROSSFIT~\citep{YeLR21CrossfitFewShot} and UNIFIEDQA~\citep{KhashabiMKSTCH20}, comprising a total of 142 unique sub-tasks. These sub-tasks cover a variety of problems, including text classification and question answering all in English. We use GPT2-Large~\cite{radford2019gpt2} as the base model for these experiments.

We compare our method against several baseline approaches:
\begin{itemize}
    \item \textbf{MetaICL}~\citep{MinLZH22MetaICL}: A method for meta-learning with in-context learning. MetaICL tunes all the parameters of the base model using the first-order method. In both training and testing, the model is given $k$ demonstration examples, ${(a_1,b_1), \dots, (a_k,b_k)}$, where $b_i$ represents either classification labels or possible answers in question-answering tasks, along with one test example $(a,b)$. The input is formed by concatenating the demonstration examples $a_1,b_1, \dots, a_k,b_k,a$. The model then computes the conditional probability of each label, and the label with the highest probability is selected as the prediction.
    \item \textbf{Zero-shot}: This method uses the pretrained language model (LM) without any tuning, performing zero-shot inference without any demonstration examples.
    \item \textbf{In-context Learning (ICL)}: This method uses the pretrained LM with in-context learning by conditioning on a concatenation of $k$ demonstration examples and 1 actual test sample similar to MetaICL.
\end{itemize}

We sample 768 examples from each training sub-task. We train MetaICL in their original setting for 30,000 steps. To train our method, we split the training dataset of each sub-task to two subsets, 256 samples as the development dataset for upper-level updates and 512 samples for lower-level training. For each outer iteration of our method, we randomly sample a subset of 5 training tasks. We perform 10 lower-level updates between each pair of upper-level updates. To keep bilevel-ZOFO as lightweight as possible, unlike MetaICL, we DO NOT include demonstration examples in the inputs. Since bilevel-ZOFO uses significantly less memory and has much faster updates compared to MetaICL, theoretically we are able to train it for many more iterations within the same total training duration as MetaICL. However, due to resource constraints, we only train bilevel-ZOFO for 50,000 iterations. Similar to ~\cite{MalladiGNDL0A23Mezo}, we did not observe a plateau in performance for bilevel-ZOFO, indicating that further training can yield additional improvements. 

For both ICL and MetaICL, during the testing phase the model is given $k=4$ demonstration examples for each test data point. We don't use demonstration examples in test samples for bilevel-ZOFO evaluation. We evaluate the zero-shot capabilities of our method as well as the performance of the final model LoRA-tuned for 10 additional iterations on 4 demonstration samples from each class of each test sub-task. Similar to \cite{MinLZH22MetaICL}, we report \textbf{Macro-averaged F1} as the evaluation metric. See Appendix \ref{app:multi-task-exp} for all training details.

Table \ref{tab:meta-learning} presents the meta-learning results. We observe that in zero-shot setting, bilevel-ZOFO~(ours(zeroshot)) outperforms zero-shot on all tasks.
Note that although ICL and  MetaICL  perform better than ours~(zero-shot) 1)MetaICL fine-tunes the entire base model using first-order methods, which incurs a significantly higher computational cost. 2)both ICL and MetaICL with $k=4$ demonstration examples take 4 times more time to do inference than our method with no demonstration examples. 
Nonetheless, after a lightweight 10-iteration LoRA fine-tuning phase, bilevel-ZOFO(ours(tuned)) surpasses ICL and MetaICL on nearly every hyper-task, highlighting its strong potential as a meta-learning algorithm. 

\begin{table}[!t]
\centering
\resizebox{0.95\linewidth}{!}{%
\begin{tabular}{llcccccc}
\toprule
Trainer & Mode & BoolQ & ReCoRD & SQuAD & SST2 & Average \\
\midrule
\multirow{4}{*}{MeZO} & ft & 0.7915 & 0.7890 & 0.7737 & 0.8646 & 0.8047 \\
& lora & 0.8020 & 0.7970 & 0.7412 & 0.8529 & 0.7983 \\
& prefix & 0.7830 & 0.7905 & 0.7093 & 0.8364 & 0.7798 \\
& prompt & 0.7787 & 0.7935 & 0.7014 & 0.8246 & 0.7746 \\ \midrule
& average & 0.7888 & 0.7925 & 0.7489 & 0.8397 & 0.7825 \\ \midrule
\multirow{3}{*}{FO} & lora & 0.8420 & 0.7920 & 0.8197 & 0.9557 & 0.8524 \\
& prefix & 0.7783 & 0.8013 & 0.7946 & 0.9243 & 0.8246 \\
& prompt & 0.8083 & 0.8023 & 0.7805 & 0.9284 & 0.8299 \\ \midrule
& average & \textbf{0.8095} & 0.7985 & 0.7983 & 0.9361 & 0.8356\\ \midrule
\multirow{3}{*}{Ours} & lora & 0.8473 & 0.8290 & 0.8160 & 0.9629 & \cellcolor[HTML]{C0C0C0} 0.8638 \\
& prefix & 0.8193 & 0.8067 & 0.8090 & 0.9382 & \cellcolor[HTML]{C0C0C0} 0.8433 \\
& prompt & 0.8145 & 0.8108 & 0.7960 & 0.9222 & \cellcolor[HTML]{C0C0C0} 0.8359 \\ 
\midrule
& average & 0.7937 & \textbf{0.8155} & \textbf{0.8070} & \textbf{0.9414} & \cellcolor[HTML]{C0C0C0} 0.8394 \\ 
\bottomrule
\end{tabular}
}
\caption{Single-Task Experiments on Llama2-7B with 1000 samples. Values correspond to mean across three random seeds. FO: First-Order. FT: full-model fine-tuning. See Table~\ref{tab:single-task-llama2-7b} for full details.}
\label{tab:single-task-llama2-7b-main}
\end{table}



\begin{table}[!t]
\centering
\resizebox{\linewidth}{!}{%
\begin{tabular}{lcccc}
\toprule
\multirow{2}{*}{Method} & class & non\_class & qa & non\_qa \\
 & $\rightarrow$ class & $\rightarrow$ class & $\rightarrow$ qa &  $\rightarrow$ qa \\
\midrule
Zero-shot & 34.2 & 34.2 & 40.2 & 40.2 \\
Few-shot & 34.9 (1.4) & 34.9 (1.4) & 40.5 (0.3) & 40.5 (0.4) \\
MetaICL & 46.4 (1.1) & 37.7 (1.7) & \textbf{45.5} (0.3) & 40.2 (0.6) \\ \midrule
Ours (Zero-shot) & 34.5 & 34.3 & 41.8 & 40.4 \\
Ours(Tuned) & \textbf{47.1} & \textbf{42.4} & 43.5 (1.3) & \textbf{41.9}  \\
\bottomrule
\end{tabular}
}
\caption{Multi-task Meta learning results using GPT2-Large as the base model. Values correspond to the mean and standard deviation over 5 test seeds which include different demonstration samples for each test task. class: Classification, qa: Question Answering}
\label{tab:meta-learning}
\end{table}


\subsection{Ablative Studies}
We perform an ablation study by varying the regularization parameter $\lambda$ (as defined in Equation \eqref{equation5}) and the number of lower-level training steps between each pair of upper-level updates. Figure~\ref{fig:ablation} shows the results. From Figure~\ref{fig:ablations-lambda}, the effect $\lambda$ appears to be non-linear, indicating the need to find an optimal balance. Nontheless, a moderate value like $10$ or $100$ seems to work reasonably well on all tasks.
As anticipated, Figure~\ref{fig:ablation-steps} demonstrates that performance generally degrades when the total number of upper-level updates is reduced, suggesting there is a trade-off between latency and performance. While more upper-level updates improve results, they also extend the overall training time.



\begin{figure}[!ht]
    \centering
    \begin{subfigure}[b]{0.8\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/lambda_vs_test_accuracy.png}
        \caption{$\lambda$}
        \label{fig:ablations-lambda}
    \end{subfigure}
    \begin{subfigure}[b]{0.8\linewidth} 
        \centering
        \includegraphics[width=\linewidth]{figures/lower_level_training_steps_vs_test_accuracy.png} % Path to your figure
        \caption{Number of lower-level training steps}
        \label{fig:ablation-steps}
    \end{subfigure}
    \caption{Ablation over $\lambda$ in \eqref{equation5} and the number of lower-level training steps before each upper-level update.}
    \label{fig:ablation}
\end{figure}

