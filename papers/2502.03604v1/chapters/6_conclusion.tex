\section{Conclusions}
In this work, we introduced a novel bilevel optimization framework designed to mitigate the downsides of PEFT and zeroth-order full model fine-tuning. We propose a new method that is more efficient than existing bilevel methods and thus more suitable for tuning full pre-trained large language models. Theoretically, we provide convergence guarantees for this new method. Empirically, we show that this method outperforms both zeroth-order and FO PEFT methods in single task settings. Additionally, we show this method is effective and efficient when adapted to do multi-task learning. With competitive and even better performance compared to existing meta-training methods, our method offers a significantly cheaper training process.