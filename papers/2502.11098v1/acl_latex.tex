% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

\usepackage{microtype}
\usepackage{graphicx}
%\usepackage{subfigure}
\usepackage{array}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{siunitx}
\usepackage{booktabs} % for professional tables
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{mwe}
\usepackage{graphicx}

\usepackage{xcolor}
\usepackage{colortbl}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}

\usepackage{amsmath}
\usepackage{amssymb}

% Define custom colors
\definecolor{goodgreen}{RGB}{34, 139, 34}
\definecolor{badred}{RGB}{220, 20, 60}

\usepackage{dblfloatfix}

% If accepted, instead use the following line for the camera-ready submission:https://www.overleaf.com/project/65546e477cba23058866c703
% \usepackage[accepted]{icml2023}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{enumitem}
\usepackage{svg}
\usepackage{tcolorbox}
\usepackage{CJKutf8}
\usepackage{authblk}
\setlist[itemize]{itemsep=-1mm, topsep=-1mm}

\newtcolorbox{supbox}[2][]{colback=black!2!white,colframe=cyan!80!black,
title={#2},#1}
\newtcolorbox{membox}[2][]{colback=black!2!white,colframe=orange!90!white,
title={#2},#1}
\newtcolorbox{egbox}[2][]{ colback=black!2!white,colframe=green!60!black,
title={#2},#1}
\newtcolorbox{prtbox}[2][]{ colback=black!2!white,colframe=violet!80!white,
title={#2},#1}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Talk Structurally, Act Hierarchically: \\A Collaborative Framework for LLM Multi-Agent Systems}
% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{
    Zhao Wang$^{*,\dag}$, 
    Sota Moriyama$^*$, 
    Wei-Yao Wang, 
    Briti Gangopadhyay, 
    Shingo Takamatsu
}

\affil{Sony Group Corporation, Japan}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\begin{CJK}{UTF8}{min}  % Start Japanese support
\maketitle

\def\thefootnote{*}\footnotetext{These authors contributed equally to this work} 
\def\thefootnote{\dag}\footnotetext{Corresponding author: Zhao Wang (Email Address: Zhao.Wang@sony.com)} 

\begin{abstract}
Recent advancements in LLM-based multi-agent (LLM-MA) systems have shown promise, yet significant challenges remain in managing communication and refinement when agents collaborate on complex tasks. In this paper, we propose \textit{Talk Structurally, Act Hierarchically (TalkHier)}, a novel framework that introduces a structured communication protocol for context-rich exchanges and a hierarchical refinement system to address issues such as incorrect outputs, falsehoods, and biases. \textit{TalkHier} surpasses various types of SoTA, including inference scaling model (OpenAI-o1), open-source multi-agent models (e.g., AgentVerse), and majority voting strategies on current LLM and single-agent baselines (e.g., ReAct, GPT4o), across diverse tasks, including open-domain question answering, domain-specific selective questioning, and practical advertisement text generation. These results highlight its potential to set a new standard for LLM-MA systems, paving the way for more effective, adaptable, and collaborative multi-agent frameworks. %\footnote{The code will be public in the camera-ready version.} 
The code is available at \href{https://github.com/sony/talkhier}{https://github.com/sony/talkhier}.
%Ablation studies further highlight the effectiveness of RLAG’s components, demonstrating their individual contributions to overall performance.
\end{abstract}


\section{Introduction}
\label{Intro}
Large Language Model (LLM) Agents have broad applications across domains such as robotics~\citep{brohan2022code}, finance~\citep{shah2023finGPT,zhang2024finagent}, and coding~\cite{chen2021evaluating,hong2023metagpt}. By enhancing capabilities such as autonomous reasoning~\citep{wang2024rethinking} and decision-making~\citep{eigner2024determinants}, LLM agents bridge the gap between human intent and machine execution, generating contextually relevant responses~\citep{arXiv2024_Survey-MultiAgent_2}.
% The exploration of Large Language Model (LLM) Agents has broad applicability across various domains.
% Whether it involves task execution in robotics~\citep{brohan2022code}, financial modeling~\citep{shah2023finGPT,zhang2024finagent}, or coding automation~\cite{chen2021evaluating,hong2023metagpt}, these scenarios can be effectively framed by leveraging the power of LLMs with autonomous reasoning~\cite{wang2024rethinking} and decision-making~\cite{eigner2024determinants}.
% As LLMs have been demonstrated to produce contextually relevant natural language responses based on the given queries \cite{arXiv2023_Survey-Agent_2,arXiv2024_Survey-MultiAgent_2}, LLMs acting as agentic AI have broken a new ground to bridge the gap between human intents and machine executions.
% Recently, Large Language Model (LLM) Agents~\cite{wang2024survey} have emerged as a pivotal focus in AI research, leveraging the power of large language models for autonomous reasoning~\cite{wang2024rethinking}, decision-making~\cite{eigner2024determinants}, and diverse applications such as task execution in robotics~\cite{brohan2022code}, financial modeling~\cite{shah2023finGPT,zhang2024finagent}, and coding automation~\cite{chen2021evaluating,hong2023metagpt}.
% By taking advantage of LLMs’ inherent strengths, such as understanding nuanced language, generating contextually relevant responses, and learning from diverse datasets, LLM agents excel in bridging the gap between human intent and machine execution, making them highly effective across a wide range of tasks.
\begin{figure}[t] % Use 't' for top placement
\centering
\includegraphics[width=\linewidth]{figure/ICML-Teaser1.pdf}
% \vskip -15pt
\caption{Existing LLM-MA methods (left) face two major challenges: 1) disorganized, lengthy text-based communication protocols, and 2) sequential or overly similar flat multi-agent refinements. In contrast, \textit{TalkHier} (right) introduces a well-structured communication protocol and a hierarchical refinement approach.}
\label{fig:teaser1}
% \vspace{-3pt} % Reduce space below the first caption
\end{figure}

\begin{figure}[t] % Use 't' for top placement
\centering
\hspace{-15pt} % Adjust this value to move the image left
\includegraphics[width=\linewidth]{figure/ICML-Teaser2.pdf}
% \vskip -10pt
\caption{Our \textit{TalkHier} built on GPT4o surpasses inference scaling models (OpenAI-o1), open-source multi-agent models (AgentVerse and etc.), and models with majority voting strategies (ReAct, GPT4o) on five subtasks of MMLU.}
% , even surpassing the inference-scaling model OpenAI-o1, multiple existing comparable baselines such as AutoGPT, AgentVerse, and GPTSwarm, as well as single-agent framework including ReAct and OpenAI’s 4o LLM and their major voting system.}
\label{fig:teaser2}
% \vspace{-10pt} % Reduce space after the second figure
\end{figure}
Recent research has primarily focused on LLM-based Multi-Agent (LLM-MA) systems, which leverage collective intelligence and specialize each agent with the corresponding subtasks, to solve complicated and multi-step problems.
% Furthermore, the capabilities demonstrated by single LLM-based agents have paved the way for the development of LLM-based Multi-Agent (LLM-MA) systems, which leverage the collective intelligence and specialized skills of multiple agents.
For instance, previous works on LLM-MA have explored approaches where instances of LLMs, referred to as agents~\cite{arXiv2023_Survey-Agent_2, arXiv2023_Survey-Agent_3, FCS2024_Survey-Agent, arXiv2024_Survey-Agent_4, arXiv2024_Survey-Agents-CompExp}, collaborate synergistically by debate \cite{chen2024comm}, reflection \cite{he2024enhancing}, self-refinement \cite{madaan2023selfrefine}, or multi-agent based feedback refinement \cite{yang2023idea2img}.
% —such as through debate or reflection—to accomplish tasks~\citep{arXiv2024_Survey-MultiAgent_2, guo2024large, arXiv2024_Survey-MultiAgent-System_2}.
These systems employ diverse communication topologies to enable efficient interactions between agents such as Chain~\cite{cot} and Tree~\cite{tot} structures, among others~\cite{qian2024scaling, zhuge2024gptswarm, zhang2024cut}.
% Complete Graph~\cite{qian2024scaling}, Random Graph~\cite{qian2024scaling}, Optimizable Graph~\cite{zhuge2024gptswarm}, and Pruned Graph~\cite{zhang2024cut} structures. % Feels redundant with related work

Despite the promising advancements in LLM-MA systems, several challenges in this field remain unexplored (shown in Figure~\ref{fig:teaser1}):

%%%
\noindent\textbf{1) Disorganized communication in text form.}
Agents often engage in debates~\cite{zhao2023competeai}, share insights~\cite{chen2024comm}, or perform refinement~\cite{madaan2023selfrefine, yang2023idea2img} to effectively solve complex tasks, with their exchanges primarily in text form~\cite{guo2024large}.
However, communication often becomes disorganized because it requires explicitly describing agent tasks, providing background context for the communication, and specifying the required output formats. These factors together lead to lengthy and unstructured exchanges, making it difficult for agents to manage subgoals, maintain output structures, and retrieve independent memories from prior actions and observations.

%maintain intermediate output formats, and retrieve memories from prior actions and observations.

% While most previous works~\citep{cot, tot, qian2024scaling, zhang2024cut} have focused on communication topologies, there has been limited discussion on the optimal form of communication. In most research on agent communication, the exchanged content is primarily in text form~\cite{guo2024large}. 
% To solve complex tasks, agents need to communicate effectively, often engaging in debate~\cite{zhao2023competeai}, sharing insights~\cite{chen2024comm} or refinement~\cite{madaan2023selfrefine, yang2023idea2img}.
% However, as tasks increase in complexity and require decomposition into multiple subtasks, agents that rely on non-organized text for communication may struggle to manage subtasks, forget intermediate output formats, or fail to recall relevant memories from prior actions and observations. This \textbf{Non-organized Talk} between agents (communication protocol) can disrupt the entire multi-agent system, leading to interruptions and reduced accuracy.

\begin{comment}
 The second challenge is meomory. Existing frameworks handle memory differently: LangGraph~\cite{langgraph}, CoMM~\cite{chen2024comm}, and AgentVerse~\cite{agentverse2023} share context across all agents, while GPT-Swarm~\cite{zhuge2024gptswarm} and AutoGPT~\cite{autogpt2023} use central databases or prompt-based message passing. However, as the number of agents grows, centralized memory makes it difficult for agents to retrieve task-specific information~\cite{arXiv2024_Survey-MultiAgent-System} and often introduces bias~\cite{arXiv2024_Survey-MultiAgent-System_2, arXiv2024_Survey-MultiAgent_2}, where one agent’s decision unintentionally influences others. This highlights the need for scalable, independent memory systems.   
\end{comment}


% The second challenge in LLM-MA systems lie in evaluation-based refinement schemes.
\noindent\textbf{2) Refinement schemes.}
While some studies have shown that incorporating agent debates~\cite{chen2024comm} or evaluation-based multi-agent refinement~\cite{wang2023coeval, yang2023idea2img} can improve system accuracy, these approaches also expose significant limitations. As the number of agents increases, LLM-MA systems face challenges in effectively summarizing opinions or feedback~\cite{fang2024multi}. They often fail to balance these inputs, frequently overlooking some or exhibiting biases based on the order in which feedback is provided~\cite{errica2024did}.



% In this paper, we propose \textit{Talk Structurally, Act Hierarchically (TalkHier)}, a novel collaborative LLM-MA framework—the first to integrate a well-structured communication protocol with a hierarchical refinement-based approach. This framework ensures accurate and enriched interactions among agents. Our key contributions are as follows:
In this paper, we propose a novel collaborative LLM-MA framework called \textit{Talk Structurally, Act Hierarchically (TalkHier)}-the first collaborative LLM-MA framework to integrate a well-structured communication protocol with hierarchical refinement.
% This framework ensures accurate and enriched interactions among agents.
Our key contributions shown in Figure~\ref{fig:teaser1} and \ref{fig:teaser2} are as follows:

\begin{enumerate}[leftmargin=*, itemsep=0pt, topsep=0pt]
    \item \textbf{Well-Structured, Context-Rich Communication Protocol:} \textit{TalkHier} introduces a novel communication protocol that incorporates newly proposed elements: \textit{messages}, \textit{intermediate outputs}, and relevant \textit{background information}. These components form the foundation of a well-structured protocol that organizes agent communication, ensuring clarity and precision. By embedding these elements, \textit{TalkHier} significantly improves communication accuracy and efficiency compared to traditional text-based methods.

    \item \textbf{Hierarchical Refinement in LLM-MA Systems:} \textit{TalkHier} enhances traditional multi-agent evaluation systems with a hierarchical refinement framework, 
    enabling agents to act hierarchically. This approach addresses such as the difficulty in summarizing opinions or feedback as the number of agents increases, balancing diverse inputs, and mitigating biases caused by the order of feedback processing, resulting in more reliable and robust interactions.
    % enabling agents to act in a structured hierarchy.

    \item \textbf{State-of-the-Art Results Across Benchmarks:} Experimental results show that \textit{TalkHier} achieves state-of-the-art performance on diverse benchmarks, including selective problem-solving in complex sub-domains, open question answering, and Japanese text generation tasks. Ablation studies confirm the effectiveness of each component, demonstrating their contributions to the framework’s overall success.
\end{enumerate}


















\begin{figure*}[!h]
\centering
\includegraphics[width=0.93\linewidth]{figure/architecture.pdf}
\vskip -150pt
\caption{Comparisons between existing approaches (left) and ours (right). Our \textit{TalkHier} proposes a new communication protocol (first row) featuring context-rich and well-structured communication information, along with a collaborative hierarchical refinement (second row) where evaluations provide summarized and coordinated feedback within an LLM-MA framework.}
\label{fig:architecture}
\end{figure*}






















\section{Related Work}
\label{rw}

\paragraph{Collaborative LLM-MA.}
LLM-MA systems enable agents to collaborate on complex tasks through dynamic role allocation, communication, and task execution~\cite{guo2024large, han2024challenges}. Recent advancements include agent profiling~\cite{yang2024multi}, hierarchical communication~\cite{rasal2024harmony}, and integration of reasoning and intentions~\cite{qiu2024collaborative}. % Does this talk about tools?
However, challenges remain in ensuring robust communication, avoiding redundancy, and refining evaluation processes~\cite{talebirad2023collaboration}. Standardized benchmarks and frameworks are needed to drive future progress~\cite{li2024survey}.

\paragraph{Communication in LLM-MA.}
Effective communication is crucial for collaborative intelligence~\cite{guo2024large}. While many previous works, including chain~\citep{cot}, tree~\citep{tot}, complete graph~\citep{qian2024scaling}, random graph~\citep{qian2024scaling}, optimizable graph~\citep{zhuge2024gptswarm}, and pruned graph~\citep{zhang2024cut} methods have focused on communication topologies, there has been limited discussion on the optimal form of communication. Most systems rely on text-based exchanges~\cite{zhang2024cut, shen2024small}, which is inefficient and prone to errors as agents often lose track of subtasks or fail to recall prior outputs as tasks grow in complexity. We argue for structured communication protocols that guide subtasks with clear, context-specific instructions, ensuring coherence across interactions.
%prone to errors as tasks grow in complexity
% Agents often lose track of subtasks or fail to recall prior outputs. We argue for structured communication protocols that guide subtasks with clear, context-specific instructions, ensuring coherence and reliability across interactions.

\paragraph{Feedback-Based Refinement.}
Feedback mechanisms, such as Self-Refine~\cite{madaan2023selfrefine} and generator-evaluator frameworks~\cite{wang2023coeval}, improve system accuracy through iterative refinement. However, these methods face challenges in managing diverse feedback, which can lead to bias or inefficiencies if inputs are not well-organized~\cite{xu2024cooperative}. Scalable, unbiased solutions are essential to enhance multi-agent evaluation processes.









\section{Methodology}

\textit{TalkHier} aims to design a LLM-MA system represented as a graph $\mathcal{G} = (\mathcal{V}, \mathcal{E})$, where $\mathcal{V}$ denotes the set of agents (nodes) and $\mathcal{E}$ represents the set of communication pathways (edges). Given an input problem $p$, the system dynamically defines a set of communication events \( \mathcal{C}_p \), where each event \( c_{ij}^{(t)} \in \mathcal{C}_p \) represents a communication between agents \( v_i \) and \( v_j \) along an edge \( e_{ij} \in \mathcal{E} \) at time step \( t \).
% The communication events in \( C_p \) vary per problem and determine how agents interact. 
While the graph structure \( \mathcal{G} \) remains fixed, the communication events \( \mathcal{C}_p \) are dynamic and adapt to the specific task.

% \subsection{Problem Setting}
% \textit{TalkHier} is a large language model multi-agent system (LLM-MA) represented as a dynamic graph $\mathcal{G} = (\mathcal{V}, \mathcal{E}, \mathcal{T})$. Given an input problem, at each time step \( t \in \mathcal{T} \), the system dynamically adds an edge \( e_{ij}^{(t)} \in \mathcal{E} \) between nodes \( v_i, v_j \in \mathcal{V} \). Here, \( \mathcal{V} \) is a set of agents, \( \mathcal{E} \) is a set of communications, and \( \mathcal{T} \) represents time steps.
%\textit{TalkHier} aims to design a LLM-MA system represented as a graph \( \mathcal{G} = (\mathcal{V}, \mathcal{E}) \), where \( \mathcal{V} \) denotes the set of agents (nodes) and \( \mathcal{E} \) represents the set of communication pathways (edges). Given an input problem, the system dynamically defines a set of communication events \( C_p \), where each event \( c_{ij}^{(t)} \in C_p \) represents a communication between agents \( v_i \) and \( v_j \) along an edge \( e_{ij} \in \mathcal{E} \) at time step \( t \).

% The communication events in \( C_p \) vary per problem and determine how agents interact. While the graph structure \( \mathcal{G} \) remains fixed, the communication events \( C_p \) are dynamic and adapt to the specific task.

% The system dynamically adapts its graph structure based on previous interactions, with the supervisor deciding which member to communicate with next based on the evolving state of the task.

% \textit{TalkHier} aims to design a LLM-MA system represented as a graph $\mathcal{G} = (\mathcal{V}, \mathcal{E})$, where $\mathcal{V}$ denotes the set of agents (nodes) and $\mathcal{E}$ represents the set of communication pathways (edges). The objective is to configure the nodes $v \in \mathcal{V}$ and edges $e \in \mathcal{E}$ such that the system maximizes performance in generating appropriate textual outputs for a given task.
% Specifically, given a set of tasks $\mathcal{T} = \{T_1, T_2, \ldots, T_{|\mathcal{T}|}\}$ that includes selective problem-solving,
% in extensive sub-domains, 
% open question answering, and Japanese text generation tasks, each defined by its domain, requirements, and input context $\mathbf{C}_T$, the goal is to find the optimal configuration of $\mathcal{V}$ and $\mathcal{E}$ that maximizes general task performance. %The output for each task, $\mathbf{Y}_T$, represents the generated solution or response to the task $T$, produced by the system based on its agent interactions. This can be formulated as: 
% is CT only used here?
\begin{comment}
\textit{TalkHier} is a large language model multi-agent system (LLM-MA) represented as a dynamic graph \( \mathcal{G} = (\mathcal{V}, \mathcal{E}, \mathcal{T}) \).
Given an input problem, at each time step \( t \in \mathcal{T} \), the system dynamically adds an edge \( e_{ij}^{(t)} \in \mathcal{E} \) between nodes \( v_i, v_j \in \mathcal{V} \). Here, \( \mathcal{V} \) represents nodes, \( \mathcal{E} \) represents edges, and \( \mathcal{T} \) represents time steps.

The system dynamically adapts its graph structure based on previous interactions, with the supervisor deciding which member to communicate with next based on the evolving state of the task.


The system dynamically adapts its graph structure based on previous interactions, with the supervisor deciding which member to communicate with next based on the evolving state of the task.

\begin{equation}
\mathbf{Y}_T = \arg\max_{\mathcal{V}, \mathcal{E}} \mathcal{F}(\mathbf{C}_T; \mathcal{V}, \mathcal{E}),
\end{equation}
where $\mathcal{F}$ represents the LLM-MA system function based on agent interactions and their configurations, and $\mathbf{Y}_T$ is the output that achieves the highest performance for task $T$.
\end{comment}

\begin{figure*}[t]
    \centering
    \begin{supbox}{ Supervisor Prompt Template}
    \small
    % [Insert Role Here]\\
    Team Members: [\small \textit{Description of each team member's role}]\\
    Conversation History: [\textit{Independent Conversation History}]\\
    Given the conversation above, output the following in this exact order:\\
    1. `thoughts': Output a detailed analysis on the most recent message. In detail, state what you think should be done next, and who you think you should contact next. \\
    % \textit{(Note: This step is not used for communication but is standard in related work for acquiring individual agent thoughts.)}\\
    2. Who should act next? Select one of: [\textit{Team member names}] and output as `next'. When you have determined that the final output is gained, report back with FINISH.\\
    3. `messages': If the next agent is one of [\textit{Team member names}], give detailed instructions. If FINISH, report a summary of all results.\\
    4. The detailed background of the problem you are trying to solve (given in the first message) as `background'.\\
    5. The intermediate outputs to give as `intermediate\_output'.
    \end{supbox}
    \begin{membox}{ Member Prompt Template}
    \small
    [\textit{Role of member}]\\
    Background: [\small \textit{Background information given by Supervisor}]\\
    Conversation History: [\textit{Independent Conversation History}]
    \end{membox}
    \caption{Prompts for acquiring the contents of the context-rich, structured communication protocol in \textit{TalkHier}.}
    \label{fig::com_prompt}
\end{figure*}

\subsection{Agents with Independent Memory}

Each agent \( v_i \in \mathcal{V} \) in graph \( \mathcal{G} \) can be formally represented as:
\begin{equation*}
% v_i = \left( \texttt{Base}_i, \texttt{Role}_i, \texttt{Plugins}_i, \texttt{Memory}_i, \texttt{Type}_i \right), \label{eq:agent_representation}
v_i = \left( \texttt{Role}_i, \texttt{Plugins}_i, \texttt{Memory}_i, \texttt{Type}_i\right). %\texttt{Team}_i \right),
\end{equation*}
% Each agent \( v_i \in \mathcal{V} \) in graph \( \mathcal{G} \) can be formally represented as:
% \begin{equation*}
% v_i = \left( \texttt{Base}_i, \texttt{Role}_i, \texttt{Plugins}_i, \texttt{Memory}_i, \texttt{Type}_i \right), \label{eq:agent_representation}
% \end{equation*}
% where:
%\begin{itemize}[leftmargin=*, itemsep=0pt, topsep=0pt]
%\(\texttt{Base}_i\): An LLM instance used in the task domain.
\(\texttt{Role}_i\): Assign roles such as generator, evaluator, or revisor based on the task type.
\(\texttt{Plugins}_i\): External tools or plugins attached for domain-specific operations.
\(\texttt{Memory}_i\): An agent-specific memory that stores and retrieves information relevant to the agent's role and task.
\(\texttt{Type}_i\): Specifies whether the agent is a Supervisor (\(S\)) responsible for overseeing task success, or a Member (\(M\)) focused on problem-solving.
%\end{itemize}

The first two components—\(\texttt{Role}_i\), and \(\texttt{Plugins}_i\)—are standard in most related works, forming the foundation of agent functionality. Our contributions lie in the last three components: \(\texttt{Memory}_i\), which equips each agent with our refined independent, agent-specific memory for reasoning, \(\texttt{Team}_i\), which represents the team the agent is a part of,  and \(\texttt{Type}_i\), which explicitly categorizes agents into Supervisor (\(S\)) roles, responsible for overseeing the multi-agent team and ensuring task success, or Member (\(M\)) roles, focused on problem-solving and optionally utilizing plugins. These additions enable hierarchical, structured collaboration and role-specific operations within the framework.

\paragraph{Agent-Specific Memory.}
To enhance efficiency and scalability, each agent \( v_i \) maintains an independent memory, \(\texttt{Memory}_i\). Unlike long-term memory, which relies on a shared memory pool accessible by all agents, or short-term memory, which is limited to a single session or conversational thread, our proposed memory mechanism is agent-specific but not limited to session or conversational thread. 

\textit{TalkHier} allows each agent to independently retain and reason on its past interactions and knowledge, offering two key advantages: independence, where each agent’s memory operates without interference from others, avoiding centralized dependencies; and persistence, enabling agents to maintain historical data across sessions for consistent and informed decision-making.

\subsection{Context-Rich Communication Between Agents}

Communication between agents is represented by communication events $c_{ij}^{(t)} \in \mathcal{C}_p$, where each event $c_{ij}^{(t)}$ encapsulates the interaction from agent $v_i$ to agent $v_j$  along an edge \( e_{ij} \in \mathcal{E} \) at time step \( t \). 
Formally, a communication event $c_{ij}^{(t)}$ is defined as:

\begin{equation*}
    c_{ij}^{(t)} = ({ \mathbf{M}_{ij}^{(t)}, \mathbf{B}_{ij}^{(t)}, \mathbf{I}_{ij}^{(t)}}),
\end{equation*}
where $\mathbf{M}_{ij}^{(t)}$ indicates the \textit{message} content sent from $v_i$ to $v_j$, containing instructions or clarifications, $\mathbf{B}_{ij}^{(t)}$ denotes \textit{background} information to ensure coherence and task progression, including the problem’s core details and intermediate decisions, and $\mathbf{I}_{ij}^{(t)}$ refers to the \textit{intermediate output} generated by $v_i$, shared with $v_j$ to support task progression and traceability, all at time step $t$. These structures ensure that agents of \textit{TalkHier} accomplish efficient communication and task coordination.



% To enable effective cooperation and seamless task execution, as shown in the first row of Figure~\ref{fig:architecture},
% each edge is structured into two com: \textit{communication info part} and the \textit{operation part}.
% \[
% e_{ij} = \underbrace{\left( \mathbf{M}_{ij}, \mathbf{B}_{ij}, \mathbf{I}_{ij} \right)}_{\text{Communication Info}} , 
% \underbrace{\mathbf{O}_{ij}}_{\text{Operation Info}}. 
% \]
% where $\mathbf{M}_{ij}$ indicates the \textit{message} content sent from $v_i$ to $v_j$, containing instructions or clarifications, $\mathbf{B}_{ij}$ denotes \textit{background} information to ensure coherence and task progression, including the problem’s core details and intermediate decisions, $\mathbf{I}_{ij}$ refers to the \textit{intermediate output} generated by $v_i$, shared with $v_j$ to support task progression and traceability, and $\mathbf{O}_{ij}$ specifies the operational directives, indicating which agent $v_j$ should act next.
% \begin{itemize}
%     \item $\mathbf{M}_{ij}$ (\textbf{Messages}): The message content sent from $v_i$ to $v_j$, containing instructions or clarifications.
%     \item $\mathbf{B}_{ij}$ (\textbf{Background}): Contextual information, including the problem’s core details, relevant historical interactions, and intermediate decisions, to ensure coherence and task progression.
%     \item $\mathbf{I}_{ij}$ (\textbf{Intermediate Output}): Intermediate results generated by $v_i$, shared with $v_j$ to support task progression and traceability.
%     \item $\mathbf{O}_{ij}$ (\textbf{Operation}): Specifies the operational directives, indicating which agent $v_j$ should act next.
% \end{itemize}

% This structure ensures \textit{TalkHier} achieves efficient communication and task coordination. The \textit{communication part} (\(\mathbf{M}_{ij}, \mathbf{B}_{ij}, \mathbf{I}_{ij}\)) enables seamless and context-rich information exchange, the \textit{operation part} (\(\mathbf{O}_{ij}\)) facilitates task delegation.




% \paragraph{Communication Dynamics.}
% During an interaction, the communication from $v_i$ to $v_j$ integrates these components as follows:
% \begin{equation}
% \mathbf{C}{ij} = f\text{comm}(\texttt{State}i, \mathbf{T}{ij}, \mathbf{M}{ij}, \mathbf{B}{ij}, \mathbf{I}{ij}),
% \end{equation}
% where $f\text{comm}$ represents the communication function that combines the agent’s state and the interaction components to produce a coherent message $\mathbf{C}_{ij}$ for the next agent.

% \paragraph{Overall Communication Framework.}
% The proposed framework supports effective task-solving across domains by:
% \begin{itemize}
% \item Maintaining coherence through $\mathbf{B}{ij}$ (shared background).
% \item Enabling localized reasoning via $\mathbf{T}{ij}$ (thoughts) and $\mathbf{M}{ij}$ (messages).
% \item Ensuring task progression with $\mathbf{I}{ij}$ (intermediate outputs) and $\mathbf{N}_{ij}$ (next agent directive).
% \end{itemize}



\begin{algorithm*}
\small % Adjust font size
\caption{Hierarchical Refinement}
\label{alg:our_hierarchical_refinement} % Add this line for referencing
\KwIn{Initial output $\mathbf{A}_0$ generated by the Generator node $v_\text{main}^\text{Gen}$, quality threshold $\mathcal{M}_\text{threshold}$, maximum iterations $T_\text{max}$}
\KwOut{Final output $\mathbf{A}_\text{final}$}
\DontPrintSemicolon

Initialize iteration counter $t \gets 0$\;

\Repeat{$t \geq T_\text{max}$}{ 
    $t \gets t + 1$\hfill
    \tcp{Step 1: Task Assignment from $v^s_{main}$ to $v^s_{eval}$}
    $\mathbf{T}_\text{assign}^{(t)} = \{(\texttt{Role}_{v_\text{eval}^S}, \texttt{Criteria}_{v_\text{eval}^S})\}$\hfill
    \tcp{Step 2: Task Distribution by $v^s_{eval}$}
    $\mathbf{T}_\text{distribute}^{(t)} = \{(\texttt{Criterion}_{v_\text{eval}^{E_i}})\}_{i=1}^k$\hfill
    \tcp{Step 3: Evaluation}
    $\mathbf{F}_{v_\text{eval}^{E_i}}^{(t)} = f_\text{evaluate}(\mathbf{A}_{t-1}, \texttt{Criterion}_{v_\text{eval}^{E_i}}), \quad \forall v_\text{eval}^{E_i} \in \mathcal{V}_\text{eval}$\;
    $
    \mathbf{F}_\text{eval}^{(t)} = \{\mathbf{F}_{v_\text{eval}^{E_1}}^{(t)}, \ldots, \mathbf{F}_{v_\text{eval}^{E_k}}^{(t)}\}
    $\hfill
    \tcp{Step 4: Feedback Aggregation by $v^s_{eval}$}
    $
    \mathbf{F}_\text{summary}^\text{eval} = f_\text{summarize}(\mathbf{F}_\text{eval}^{(t)})
    $\hfill
    \tcp{Step 5: Summarizing results}
    \If{$\mathcal{M}(\mathbf{F}_\text{summary}^\text{eval}) \geq \mathcal{M}_\text{threshold}$}
    {
            \Return $\mathbf{A}_\text{final}=\mathbf{A}_{t-1}$\hfill \tcp{Step 6: Return the current text if above threshold}
            
        }%\uElse{
             $\mathbf{A}_{t}=f_\text{revise}(\mathbf{A}_{t-1}, \mathbf{F}_\text{summary}^\text{eval})$\hfill \tcp{Step 7: Revision of the text}
        % }
    % \[
    % \mathbf{A}_t =
    % \begin{cases} 
    %   \mathbf{A}_{t-1}, & \text{if } \mathcal{M}(\mathbf{F}_\text{summary}^\text{eval}) \geq \mathcal{M}_\text{threshold} \\
    %   f_\text{revise}(\mathbf{A}_{t-1}, \mathbf{F}_\text{summary}^\text{eval}), & \text{otherwise.}
    % \end{cases}
    % \]
}

\Return $\mathbf{A}_\text{final} = \mathbf{A}_t$\;
\end{algorithm*}










\paragraph{Communication Event Sequence.}
At each time step $t$, the current agent $v_i$ communicates with a connected node $v_j$, with one being selected by the LLM if more than one exists. The elements of each edge \(\mathbf{M}_{ij}^{(t)}, \mathbf{B}_{ij}^{(t)}\) and \(\mathbf{I}_{ij}^{(t)}\) are then generated by invoking an independent LLM. To ensure consistency, clarity, and efficiency in extracting these elements, the system employs specialized prompts tailored to the roles of Supervisors and Members, as illustrated in Figure~\ref{fig::com_prompt}. Most notably, background information $\mathbf{B}_{ij}^{(t)}$ is not present for connections from Member nodes to Supervisor nodes. These information are then established as a communication event \( c_{ij}^{(t)} \in \mathcal{C}_p \).
% If the node has multiple connecting nodes, the target node is dynamically selected by the LLM model depending on the state. 

% The elements of each edge \(\mathbf{M}_{ij^{(t)}}, \mathbf{B}_{ij}^{(t)}\) and \(\mathbf{I}_{ij}^{(t)}\) are dynamically generated by invoking an independent LLM (a new session is initiated for each turn of edge acquisition). To ensure consistency, clarity, and efficiency in extracting these elements, the system employs specialized prompts tailored to the roles of Supervisors and Members. These prompts, as illustrated in Figure~\ref{fig::com_prompt}, guide each agent in structuring the necessary information during the communication process.

% \subsection{Acquiring Edge Elements}

% The elements of each edge (\(\mathbf{M}_{ij}, \mathbf{B}_{ij}, \mathbf{I}_{ij}\)) and (\(\mathbf{O}_{ij}\)) are dynamically generated by invoking an independent LLM (a new session is initiated for each turn of edge acquisition). To ensure consistency, clarity, and efficiency in extracting these elements, the system employs specialized prompts tailored to the roles of Supervisors and Members. These prompts, as illustrated in Figure~\ref{fig::com_prompt}, guide each agent in structuring the necessary information during the communication process.

% The prompts are designed to adapt to the specific responsibilities of Supervisors and Members. Supervisors coordinate the task, analyze the context, and delegate responsibilities, while Members focus on problem-solving within their roles using the provided context and directives. \textit{TalkHier} ensures that communication is context-rich and operations are seamlessly integrated into the our framework. 


\subsection{Collaborative Hierarchy Agent Team}



\begin{figure}[t]
\centering
\includegraphics[width=0.6\linewidth]{figure/hierchy.png}
\caption{Illustrated hierarchy of \textit{TalkHier}.}
\label{fig:hierarchy} % Add this line for referencing
\end{figure}



% \begin{algorithm}
% \small % Adjust font size
% \caption{Hierarchical Refinement}
% \label{alg:our_hierarchical_refinement} % Add this line for referencing
% \KwIn{Initial output $\mathbf{A}_0$ generated by the Generator node $v_\text{main}^\text{Gen}$, quality threshold $\mathcal{M}_\text{threshold}$, maximum iterations $T_\text{max}$}
% \KwOut{Final output $\mathbf{A}_\text{final}$}
% \DontPrintSemicolon

% Initialize iteration counter $t \gets 0$\;

% \Repeat{$\mathcal{M}(\mathbf{A}_t) \geq \mathcal{M}_\text{threshold}$ \textbf{or} $t \geq T_\text{max}$}{ 
%     $t \gets t + 1$
    
%     \tcp{Step 1: Task Assignment from $v^s_{main}$ to $v^s_{eval}$}
%     $\mathbf{T}_\text{assign}^{(t)} = \{(\texttt{Role}_{v_\text{eval}^S}, \texttt{Criteria}_{v_\text{eval}^S})\}$\;

%     \tcp{Step 2: Task Distribution by $v^s_{eval}$}
%     $\mathbf{T}_\text{distribute}^{(t)} = \{(\texttt{Criterion}_{v_\text{eval}^{E_i}})\}_{i=1}^k$\;

%     \tcp{Step 3: Evaluation}
%     $\mathbf{F}_{v_\text{eval}^{E_i}}^{(t)} = f_\text{evaluate}(\mathbf{A}_{t-1}, \texttt{Criterion}_{v_\text{eval}^{E_i}}), \quad \forall v_\text{eval}^{E_i} \in \mathcal{V}_\text{eval}$\;
%     $
%     \mathbf{F}_\text{eval}^{(t)} = \{\mathbf{F}_{v_\text{eval}^{E_1}}^{(t)}, \ldots, \mathbf{F}_{v_\text{eval}^{E_k}}^{(t)}\}
%     $\;

%     \tcp{Step 4: Feedback Aggregation by $v^s_{eval}$}
%     $
%     \mathbf{F}_\text{summary}^\text{eval} = f_\text{summarize}(\mathbf{F}_\text{eval}^{(t)})
%     $\;

%     \tcp{Step 5: $v^s_{main}$ Decision}
%     \lIf{$\mathcal{M}(\mathbf{F}_\text{summary}^\text{eval}) \geq \mathcal{M}_\text{threshold}$}{
%             $\mathbf{A}_t=\mathbf{A}_{t-1}$
%         }\lElse{
%             $\mathbf{A}_t=f_\text{revise}(\mathbf{A}_{t-1}, \mathbf{F}_\text{summary}^\text{eval})$
%         }
%     % \[
%     % \mathbf{A}_t =
%     % \begin{cases} 
%     %   \mathbf{A}_{t-1}, & \text{if } \mathcal{M}(\mathbf{F}_\text{summary}^\text{eval}) \geq \mathcal{M}_\text{threshold} \\
%     %   f_\text{revise}(\mathbf{A}_{t-1}, \mathbf{F}_\text{summary}^\text{eval}), & \text{otherwise.}
%     % \end{cases}
%     % \]
% }

% \Return $\mathbf{A}_\text{final} = \mathbf{A}_t$\;
% \end{algorithm}
    

The entire graph $\mathcal{G}$ consists of multiple teams, each represented as a subset $\mathcal{V}_\text{team} \subseteq \mathcal{V}$. Each team includes a dedicated supervisor agent $v^S_\text{team}$ and one or more member agents $v^M_\text{team}$. A key feature of the hierarchical structure in \textit{TalkHier} is that a member agent in one team can also act as a supervisor for another team, creating a nested hierarchy of agent teams. As shown in the second row of Figure~\ref{fig:architecture}, this structure enables the entire graph $\mathcal{G}$ to represent a hierarchical node system, where teams are recursively linked through supervisor-member relationships.

Formally, the hierarchical structure of agents with two teams is defined as:
\begin{align*}
\mathcal{V}_\text{main} &= \{v_\text{main}^S, v_\text{main}^\text{Gen}, v_\text{eval}^S, v_\text{main}^\text{Rev}\}, \\%\label{eq:main_set} \\
\mathcal{V}_\text{eval} &= \{v_\text{eval}^S, v_\text{eval}^{E_1}, v_\text{eval}^{E_2}, \ldots, v_\text{eval}^{E_k}\}, %\label{eq:eval_set}
\end{align*}
where the Main Supervisor ($v_\text{main}^S$) and Evaluation Supervisor ($v_\text{eval}^S$) oversee their respective team’s operations and assign tasks to each member, the Generator ($v_\text{main}^\text{Gen}$) gives solutions for a given problem, and the Revisor ($v_\text{main}^\text{Rev}$) refines outputs based on given feedback. Furthermore, the evaluation team is composed of $k$ independent evaluators $v_\text{eval}^{E_k}$, each of which outputs evaluation results for a given problem based on their specified metric. The overall structure is shown in Figure \ref{fig:hierarchy}.

% where:
% the Main Supervisor ($v_\text{main}^S$) oversees the team’s operations, assigns tasks to the Generator ($v_\text{main}^\text{Gen}$), forwards outputs to the evaluation Supervisor ($v_\text{eval}^S$), and provides consolidated feedback to the Revisor ($v_\text{main}^\text{Rev}$).
% Generator ($v_\text{main}^\text{Gen}$): Produces initial outputs based on tasks assigned by the main Supervisor.
% Evaluation Supervisor ($v_\text{eval}^S$): Supervises the evaluation team $\mathcal{V}_\text{eval}$ and summarizes feedback from multiple evaluators for the main Supervisor.
% Evaluators ($v_\text{eval}^{E_i}$): Evaluate outputs based on specific criteria (e.g., accuracy, fluency, relevance).
% Revisor ($v_\text{main}^\text{Rev}$): Refines outputs based on feedback from the main Supervisor.
% We show in Algorithm~\ref{alg:our_hierarchical_refinement} how our hierarchical refinement process operates within the collaborative agent framework.
\paragraph{Algorithm.} Algorithm~\ref{alg:our_hierarchical_refinement} illustrates the operation of our hierarchical refinement process within the collaborative agent framework. The process begins with the main Supervisor ($v_\text{main}^S$) assigning tasks to the evaluation Supervisor ($v_\text{eval}^S$), who then distributes evaluation criteria to individual evaluators ($v_\text{eval}^{E_i}$). Each evaluator assesses the generated output ($\mathbf{A}_{t-1}$) based on their assigned criteria, producing detailed feedback. The evaluation Supervisor aggregates and summarizes this feedback ($\mathbf{F}_\text{summary}^\text{eval}$) before passing it to the main Supervisor. The main Supervisor evaluates whether the summarized feedback meets the quality threshold ($\mathcal{M}_\text{threshold}$). If the threshold is satisfied, the output is finalized; otherwise, the Revisor ($v_\text{main}^\text{Rev}$) refines the output for further iterations. This iterative refinement ensures accurate and unbiased collaboration across the agent hierarchy.

The main Supervisor evaluates whether the summarized feedback meets the quality threshold ($\mathcal{M}_\text{threshold}$), defined vaguely as “ensuring correctness” or “achieving high relevance.” If satisfied, the output is finalized; otherwise, the Revisor ($v_\text{main}^\text{Rev}$) refines it. Details of our settings are in Appendix~\ref{app::prompt4mmlu}, Appendix~\ref{app::prompt4wiki}, and Appendix~\ref{app::prompt4camera}.


\begin{comment}
\paragraph{Configuration Design.}
We propose a good configuration of nodes and edges to maximize the effectiveness of task-solving across various domains. The configuration aims to:
\begin{itemize}
    \item Ensure efficient communication of information between agents through well-defined edges $e_{ij}$.
    \item Allocate appropriate roles and resources to each agent $v_i$ based on task requirements and domain-specific needs.
    \item Incorporate external plugins and state initialization to enhance the capabilities of individual agents.
\end{itemize}
This design prioritizes achieving high-quality outputs $\mathbf{Y}_T$ for a wide range of tasks $\mathcal{T}$ while considering practical constraints such as computational resources and domain coverage.



\subsection{Modular History Memory}

% Traditional approaches in multi-agent systems often store all communication data in a single, shared memory. While this centralized approach consolidates all observations, actions, and outputs, it leads to inefficiencies due to redundant text-based storage, making it challenging to retrieve specific and relevant historical details.

% To address this issue, we propose a \textit{modular memory} mechanism. As illustrated in Figure X, each agent maintains its own modular memory within \texttt{result["messages"]}, storing only information that is directly relevant to its assigned task. Specifically:

Traditional approaches in multi-agent systems often focus on storing all conversation histories in a single thread. While this reduces the risk of models acting beyond what is expected, there are possibilities of biases being introduced. Furthermore, having a single memory leads to inefficiencies; each model will have to look into the long and extensive history to figure out what to do next.

To address this issue, we propose an \textit{independent memory mechanism} that allows the conversation history of each agent to be stored separately, which is absolutely normal in real life conversations. The following shows the characteristics of this mechanism:

\begin{itemize}
    % \item Each agent retrieves and updates its modular memory independently, reducing unnecessary overhead.
    % \item The supervisor interacts with agents sequentially, passing outputs selectively to relevant agents for task continuity.
    % \item Intermediate results and outputs are stored efficiently in agent-specific memory modules, enabling targeted and efficient history retrieval.
    \item Each agent has a separate memory pool of all prior conversations that they refer to.
    \item Each agent is likely to gain enough information to solve the problem at hand, thanks to the communication framework.
    \item Thoughts are never revealed to other agents, and only written into the respective agent's memory pool.
\end{itemize}

This modular design significantly improves retrieval efficiency and ensures agents operate with a clean, task-focused memory state. Unlike with continuous memory, where redundant information can obscure critical context, the modular memory strategy maintains clear and organized histories for each agent, enhancing communication coherence as well as computational performance.
\end{comment}




\begin{comment}
    
\begin{algorithm}[H]
\scriptsize % Adjust font size
\caption{Our Hierarchical Refinement}
\label{alg:our_hierarchical_refinement} % Add this line for referencing
\KwIn{Initial output $\mathbf{A}_0$ generated by the Generator node $v_\text{main}^\text{Gen}$, quality threshold $\mathcal{M}_\text{threshold}$, maximum iterations $T_\text{max}$}
\KwOut{Final output $\mathbf{A}_\text{final}$}
Initialize iteration counter $t \gets 0$\;

\Repeat{$\mathcal{M}(\mathbf{A}_t) \geq \mathcal{M}_\text{threshold}$ \textbf{or} $t \geq T_\text{max}$}{ 
    $t \gets t + 1$\;

    \tcp{Step 1: Task Assignment from Main Supervisor to Evaluation Supervisor}
    \[
    \mathbf{T}_\text{assign}^{(t)} = \{(\texttt{Role}_{v_\text{eval}^S}, \texttt{Criteria}_{v_\text{eval}^S})\}
    \]

    \tcp{Step 2: Task Distribution by Evaluation Supervisor}
    \[
    \mathbf{T}_\text{distribute}^{(t)} = \{(\texttt{Criterion}_{v_\text{eval}^{E_i}})\}_{i=1}^k
    \]

    \tcp{Step 3: Evaluation}
    \[
    \mathbf{F}_{v_\text{eval}^{E_i}}^{(t)} = f_\text{evaluate}(\mathbf{A}_{t-1}, \texttt{Criterion}_{v_\text{eval}^{E_i}}), \quad \forall v_\text{eval}^{E_i} \in \mathcal{V}_\text{eval}
    \]
    \[
    \mathbf{F}_\text{eval}^{(t)} = \{\mathbf{F}_{v_\text{eval}^{E_1}}^{(t)}, \ldots, \mathbf{F}_{v_\text{eval}^{E_k}}^{(t)}\}
    \]

    \tcp{Step 4: Feedback Aggregation by Evaluation Supervisor}
    \[
    \mathbf{F}_\text{summary}^\text{eval} = f_\text{summarize}(\mathbf{F}_\text{eval}^{(t)})
    \]

    \tcp{Step 5: Main Supervisor Decision}
    \[
    \mathbf{A}_t =
    \begin{cases} 
      \mathbf{A}_{t-1}, & \text{if } \mathcal{M}(\mathbf{F}_\text{summary}^\text{eval}) \geq \mathcal{M}_\text{threshold} \\
      f_\text{revise}(\mathbf{A}_{t-1}, \mathbf{F}_\text{summary}^\text{eval}), & \text{otherwise.}
    \end{cases}
    \]
}

\Return $\mathbf{A}_\text{final} = \mathbf{A}_t$\;
\end{algorithm}


\paragraph{Supervisor Prompt.}
The Supervisor LLM is tasked with determining key elements of the edge based on the conversation history and the roles of team members. As shown in Figure~\ref{fig::com_prompt}, the Supervisor prompt is designed to produce the following:
\begin{itemize}
    \item \textbf{Communication Info ($\mathbf{C}_{ij}$)}: This includes:
    \begin{itemize}
        \item $\mathbf{M}_{ij}$ (\textbf{Messages}): Generated instructions or clarifications based on the most recent conversation.
        \item $\mathbf{B}_{ij}$ (\textbf{Background}): Extracted directly from the first message to provide a clear and consistent problem context.
        \item $\mathbf{I}_{ij}$ (\textbf{Intermediate Outputs}): Consolidated outputs from previous agents, ensuring that all task progress is communicated effectively.
    \end{itemize}
    \item \textbf{Operation Info ($\mathbf{O}_{ij}$)}: The prompt determines which agent should act next, based on the analysis of the conversation and task requirements. The output specifies the next agent (\texttt{'next'}) or indicates task completion with \texttt{'FINISH'}.
    \item \textbf{Memory Info ($\mathbf{R}_i$)}: Each agent’s independent memory is updated by appending relevant details from their interactions, including actions taken, decisions made, and outputs generated. This ensures traceability and prevents redundancy.
\end{itemize}

The Supervisor LLM is explicitly instructed to analyze the most recent conversation, assess the current task state, and generate these elements in a structured format.

\paragraph{Member Prompt.}
Member agents in RLAG also rely on an independent LLM, guided by a different prompt (Figure~\ref{fig:architecture}). The Member prompt focuses on leveraging the background information, conversation history, and instructions provided by the Supervisor. Specifically, Member agents:
\begin{itemize}
    \item Receive $\mathbf{B}_{ij}$ (\textbf{Background}) and $\mathbf{M}_{ij}$ (\textbf{Messages}) from the Supervisor, ensuring alignment with the overall task context.
    \item Process their specific task or evaluation criteria, generating any required intermediate outputs ($\mathbf{I}_{ij}$).
    \item Update their independent memory ($\mathbf{R}_i$) with records of their actions and outputs for future reference.
\end{itemize}

\paragraph{Algorithm Workflow.}
The Algorithm of RLAG for acquiring edge elements can be summarized as follows:
\begin{enumerate}
    \item The Supervisor LLM generates $\mathbf{C}_{ij}$, $\mathbf{O}_{ij}$, and updates $\mathbf{R}_i$ based on the conversation history and the overall task requirements.
    \item The Member LLMs receive inputs from the Supervisor, perform their designated tasks, and return outputs that the Supervisor consolidates into $\mathbf{I}_{ij}$ and updates its own memory.
    \item The iterative process continues until the Supervisor determines the task is complete and outputs the final results.
\end{enumerate}

This mechanism ensures that all elements of $e_{ij}$ are systematically and efficiently generated, leveraging independent LLMs to maintain modularity and scalability in the RLAG framework.

\end{comment}





% \subsection{Collaborative Hierarchy Evaluation}
% To enhance the quality of outputs, we propose a \textit{Collaborative Hierarchy Evaluation}, implemented through a structured team of agents with specialized roles. \textit{TalkHier} enables iterative evaluation and revision of outputs, surpassing traditional single-round generation methods.

\begin{comment}
% \paragraph{Algorithm: Iterative Process with Hierarchical Communication.}
\begin{algorithm}[h]
\caption{\textit{TalkHier}}
\KwIn{Initial output $\mathbf{A}_0$ generated by node $v_g \in \mathcal{V}$, quality threshold $\mathcal{M}_\text{threshold}$, maximum iterations $T_\text{max}$}
\KwOut{Final output $\mathbf{A}_\text{final}$}
Initialize iteration counter $t \gets 0$\;

\Repeat{$\mathcal{M}(\mathbf{A}_t) \geq \mathcal{M}_\text{threshold}$ \textbf{or} $t \geq T_\text{max}$}{ 
    $t \gets t + 1$\;

    \tcp{Step 1: Task Assignment by Main Supervisor}
    The main Supervisor \( v_s^\text{main} \) assigns evaluation tasks to the evaluation team Supervisor \( v_s^\text{eval} \):
    \[
    \mathbf{T}_\text{assign}^{(t)} = \{(\texttt{Role}_{e_i}, \texttt{Criterion}_{e_i})\}_{i=1}^k
    \]

    \tcp{Step 2: Task Distribution by Evaluation Team Supervisor}
    The evaluation team Supervisor \( v_s^\text{eval} \) distributes tasks to each Evaluator \( v_{e_i} \in \mathcal{V}_\text{eval} \):
    \[
    \mathbf{T}_\text{distribute}^{(t)} = \{(\texttt{Criterion}_{e_i})\}_{i=1}^k
    \]

    \tcp{Step 3: Evaluation}
    \For{each $v_{e_i} \in \mathcal{V}_\text{eval}$}{
        Evaluator \( v_{e_i} \) assesses \( \mathbf{A}_{t-1} \) based on \( \texttt{Criterion}_{e_i} \) and produces feedback:
        \[
        \mathbf{F}_{e_i}^{(t)} = f_\text{evaluate}(\mathbf{A}_{t-1}, \texttt{Criterion}_{e_i})
        \]
    }
    Collect feedback from all Evaluators:
    \[
    \mathbf{F}_\text{eval}^{(t)} = \{\mathbf{F}_{e_1}^{(t)}, \mathbf{F}_{e_2}^{(t)}, \ldots, \mathbf{F}_{e_k}^{(t)}\}
    \]

    \tcp{Step 4: Feedback Aggregation by Evaluation Team Supervisor}
    The evaluation team Supervisor \( v_s^\text{eval} \) aggregates the feedback:
    \[
    \mathbf{F}_\text{summary}^\text{eval} = f_\text{summarize}(\mathbf{F}_\text{eval}^{(t)})
    \]
    The aggregated feedback is then passed to the main Supervisor \( v_s^\text{main} \).

    \tcp{Step 5: Main Supervisor Decision}
    The main Supervisor \( v_s^\text{main} \) decides whether revision is necessary:
    \begin{itemize}
        \item \textbf{If} \( \mathcal{M}(\mathbf{F}_\text{summary}^\text{eval}) \geq \mathcal{M}_\text{threshold} \), set \( \mathbf{A}_t = \mathbf{A}_{t-1} \) (no revision needed).
        \item \textbf{Else}, proceed to revision.
    \end{itemize}

    \tcp{Step 6: Revision (if necessary)}
    If revision is required, the Revisor \( v_r \) updates the output:
    \[
    \mathbf{A}_t = f_\text{revise}(\mathbf{A}_{t-1}, \mathbf{F}_\text{summary}^\text{eval})
    \]
}

\Return $\mathbf{A}_\text{final} = \mathbf{A}_t$\;
\end{algorithm}





\subsection{Collaborative Hierarchy Evaluation} 

To facilitate the Collaborative Hierarchy Evaluation, we design a team structure consisting of an Evaluation Team Supervisor, multiple Evaluators, and a Revisor. Each member’s responsibilities are defined accordingly:

\begin{enumerate}
    \item The Evaluator Team: A group of agents with a team supervisor, tasked to evaluate the generated answers
    \item The Revisor Agent: An agent tasked to revise the answer, with respect to the given evaluations
\end{enumerate}

In this team structure, once an initial answer has been made, the answer gets evaluated and revised iteratively until a certain threshold is met. By enabling agents to collaboratively evaluate and iteratively improve outputs in this way, our approach surpasses single-round generation methods, leading to more robust and accurate results.

Furthermore, this structure (as well as prompts) can be re-used for any type of problem, with modifications needed only to the specific evaluators used. These evaluators can also be decided easily, simply by requesting what should be thought of when answering; e.g., fluency, accuracy, simplicity, etc... This represents the robustness and versatility of \textit{TalkHier}, while giving the user a high degree of control over the outputs that will be given at the end.


\begin{figure}[t]
\centering
\includegraphics[width=0.45\textwidth]{ICML2025-1/figure/team_structure.png}
\caption{Proposed Structure of Agent Team}
\label{fig:team_structure}
\end{figure}



\begin{tcolorbox}[colback=gray!5,colframe=black!75,title=Collaborative Hierarchy Evaluation Prompts]
\textbf{Evaluator Prompt:} \
“You are an Answer Evaluator Team that has to evaluate the given answer. The metrics are: \texttt{[Insert Metrics Here]} \ \texttt{[Insert Input/Output Details]}”

\vspace{1em}
\textbf{Revisor Prompt:} \
“You are an Answer Revisor that receives an answer with its evaluation results and outputs, if necessary, a revised answer that takes into account the evaluation results. Follow these steps for a revision:
\begin{enumerate}
\item Perform a detailed analysis of \textbf{ALL answers AND evaluation results}. Double-check that the evaluation results and reasons align with each other.
\item Ensure that \textbf{ALL results support each answer}. If there is an evaluation that \textbf{DOES NOT SAY CORRECT}, develop a new calculation or reasoning that accounts for the evaluation results.
\item In your final output, state:
\begin{itemize}
\item If a re-evaluation is necessary based on any new modifications.
\item The reasons behind your revisions.
\end{itemize}
\end{enumerate}
\textbf{Important:} ONLY take into account the \textbf{EVALUATION RESULTS}, without including PERSONAL OPINIONS. \ \texttt{[Insert Input/Output Details]}.”
\end{tcolorbox}


To coordinate the evaluations effectively, we define the role of an Evaluation Team Leader, responsible for synthesizing the outputs of individual evaluators and ensuring consistency and coherence across the evaluation process. The prompt for the Evaluation Team Leader is as follows:

\begin{tcolorbox}[colback=gray!5,colframe=black!75,title=Evaluation Team Leader Prompt]
“You are the \textbf{Evaluation Team Leader}, responsible for overseeing and synthesizing the evaluations provided by individual evaluators. Your tasks are as follows:
\begin{enumerate}
\item \textbf{Summarize Evaluations:} Gather and summarize the key points from the individual evaluations provided by the evaluators.
\item \textbf{Cross-Validation:} Perform a thorough analysis to ensure consistency and coherence across all evaluator analyses. Identify any discrepancies or areas requiring clarification.
\item \textbf{Highlight Key Insights:} Extract and articulate the most critical insights and findings from the evaluations, ensuring they align with the given evaluation criteria.
\item \textbf{Recommendation:} Based on the collective evaluations, provide a high-level recommendation regarding the overall correctness and quality of the current answer or solution. Highlight any areas that need re-evaluation or revision.
\item \textbf{Overlookings and Suggestions:} List any potential omissions or areas not sufficiently addressed by the evaluators and propose specific follow-up actions.
\item \textbf{Final Report:} Produce a structured final report summarizing the evaluation results, key findings, and recommendations for further actions.
\end{enumerate}

\textbf{Important:} Ensure objectivity and avoid introducing personal opinions. Your conclusions must strictly rely on the evaluators’ inputs and the given evaluation criteria.”
\end{tcolorbox}
This prompt ensures that the Evaluation Team Leader effectively integrates and validates the evaluators’ work, facilitating a structured, coherent, and objective evaluation process.

By enabling agents to collaboratively evaluate and iteratively improve outputs, our approach surpasses single-round generation methods, leading to more robust and accurate results.

\end{comment}



\section{Experiments}
\label{experiments}
In this section, we aim to answer the following research questions across various domains:

\noindent\textbf{RQ1:} Does \textit{TalkHier} outperform existing multi-agent, single-agent, and proprietary approaches on general benchmarks?

\noindent\textbf{RQ2:} How does \textit{TalkHier} perform on open-domain question-answering tasks?

\noindent\textbf{RQ3:} What is the contribution of each component of \textit{TalkHier} to its overall performance?

\noindent\textbf{RQ4:} How well does \textit{TalkHier} generalize to more practical but complex generation task?

% \textbf{(RQ1)} \textit{Can \textit{TalkHier} effectively address general benchmarks commonly used in related works to solve complex tasks across various domains?}
% \textbf{(RQ2)} \textit{Can our approach accurately handle open-domain question-answering tasks?}
% \textbf{(RQ3)} \textit{What is the impact of each component of \textit{TalkHier} on its overall performance, as revealed through a detailed ablation study?}
% \textbf{(RQ4)} \textit{How well does \textit{TalkHier} generalize to language generation tasks in languages beyond English?}

\subsection{Experimental Setup}
\paragraph{Datasets.}
We evaluated \textit{TalkHier} on a diverse collection of datasets to assess its performance across various tasks. The Massive Multitask Language Understanding (MMLU) Benchmark~\cite{hendrycks2021measure} tests domain-specific reasoning problems including Moral Scenario, College Physics, Machine Learning, Formal Logic and US Foreign Policy. WikiQA~\cite{yang2017wikiqa} evaluates open-domain question-answering using real-world questions from Wikipedia. The Camera Dataset~\cite{cyberagent_camera} focuses on advertisement headline generation, assessing the ability to create high-quality advertising text.

\paragraph{Baselines.}
To evaluate \textit{TalkHier}, we compared it against a comprehensive set of baselines including:
% \textbf{CoMM}\cite{chen2024comm}, a multi-agent, multi-reasoning-path prompting framework for complex problem-solving;
\begin{itemize}
    \setlength{\itemsep}{0pt}  % Removes extra space between items
    \setlength{\parskip}{0pt}  % Removes extra paragraph 
    \setlength{\itemindent}{0pt}
    \setlength{\leftskip}{0pt}
    \item \textbf{GPT-4o}~\cite{openai2024gpt4o}, based on OpenAI’s GPT-4 model with both single-run and ensemble majority voting (3, 5, or 7 runs).
    \item \textbf{OpenAI-o1-preview}~\cite{openai2024o1}, a beta model using advanced inference techniques, though limited by API support.
    \item \textbf{ReAct}~\cite{yao2022react}, a reasoning and action framework in single-run and ensemble configurations.
    \item \textbf{AutoGPT}~\cite{autogpt2023}, an autonomous agent designed for task execution and iterative improvement.
    \item \textbf{AgentVerse}~\cite{agentverse2023}, a multi-agent system framework for collaborative problem-solving.
    \item \textbf{GPTSwarm}~\cite{zhuge2024gptswarm}, a swarm-based agent collaboration model utilizing optimizable communication graphs.
    \item \textbf{AgentPrune}~\cite{zhang2024cut}, a model leveraging pruning techniques for efficient multi-agent communication and reasoning.
    \item \textbf{OKG}~\cite{OKG}, A method tailored specifically for ad text generation tasks and easily generalizable to ad headlines with minimal prompt redefinition.
\end{itemize}

% \textbf{Baselines to Build upon.} 

\paragraph{Implementation details.}  
For fair comparisons, we use GPT-4o as the backbone across all experiments for the baselines and \textit{TalkHier}, with the temperature set to 0 in all settings.
For the OpenAI-o1 baseline, we followed the implementation guide and the limitations outlined in OpenAI’s documentation\footnote{\url{https://platform.openai.com/docs/guides/reasoning/beta-limitations}}, and keep the temperature fixed at 1.




\subsection{Performance on MMLU (\textbf{RQ1})}
\label{general-performance-section}

\iffalse
\begin{table*}[t]
\centering
\caption{Overall Performance on MMLU Dataset. The table reports accuracy (\%) and time (s) for various baselines across the Moral Scenario, College Physics, and Machine Learning domains. The notations \textbf{3@}, \textbf{5@}, and \textbf{7@} represent majority voting results using 3, 5, and 7 independent runs, respectively.}
\begin{tabular}{lcccccc}
\toprule
\textbf{Models} & \multicolumn{2}{c}{\textbf{Moral Scenario}} & \multicolumn{2}{c}{\textbf{College Physics}} & \multicolumn{2}{c}{\textbf{Machine Learning}} \\
 & \textbf{Accuracy (\%)} & \textbf{Time (s)} & \textbf{Accuracy (\%)} & \textbf{Time (s)} & \textbf{Accuracy (\%)} & \textbf{Time (s)} \\ \midrule
% Proposed (4o) & \textbf{83.80} & 19117.0 & \textbf{93.14} & 1733.4 & 83.04 & 1816.0 \\ 
Proposed (4o) & \textbf{83.80} & 118360.26 & \textbf{93.14} & 13900.56 & 83.04 & 1816.0 \\ 
CoMM & 82.91 & 13489.5 & 92.15 & 1423.2 & \textit{N/A} & \textit{N/A} \\ 
o1-preview & 82.57 & 6491.24 & 91.17 & 889.84 & \textbf{85.71} & 1032.37 \\ 
GPT4o & 64.25 & 509.08 & 62.75 & 46.67 & 67.86 & 51.54 \\ 
GPT4o-3@ & 65.70 & 1429.95 & 62.75 & 132.45 & 66.07 & 154.38 \\ 
GPT4o-5@ & 66.15 & 2420.64 & 61.76 & 187.23 & 66.96 & 265.11 \\ 
GPT4o-7@ & 65.81 & 3567.78 & 63.73 & 264.59 & 66.96 & 331.56 \\ 
React & 69.61 & 4319.8 & 72.55 & 509.82 & 59.82 & 323.45 \\ 
React-3@ & 74.75 & 13537.8 & 83.33 & 1429.02 & 66.07 & 917.78 \\ 
React-5@ & 74.97 & 18999.7 & 82.35 & 2489.74 & 66.96 & 1597.49 \\ 
React-7@ & 75.53 & 25724.8 & 84.78 & 3391.24 & 67.86 & 1973.51 \\ \midrule
\end{tabular}
\label{tab:mmlu_performance}
\end{table*}
\fi

\begin{table}[t]
\scriptsize
% \small
\centering
\caption{General Performance on MMLU Dataset. The table reports accuracy (\%) for various baselines across Moral Scenario (Moral), College Physics (Phys.), Machine Learning (ML), Formal Logic (FL) and US Foreign Policy (UFP) domains. The notations \textbf{3@}, \textbf{5@}, and \textbf{7@} represent majority voting results using 3, 5, and 7 independent runs, respectively.}
\renewcommand{\arraystretch}{1.25} % Reduce row height
\setlength{\tabcolsep}{6pt} % Adjust column spacing
\setlength{\aboverulesep}{0pt} % Remove space above rules
\setlength{\belowrulesep}{0pt} % Remove space below rules
\setlength{\extrarowheight}{0pt} % Reduce row padding
\rowcolors{2}{gray!15}{white} % Alternating row colors
\begin{tabular}{lcccccc}
\toprule
\rowcolor{orange!10!white} \textbf{Models} & \textbf{Moral} & \textbf{Phys.} & \textbf{ML} &  \textbf{FL} &  \textbf{UFP} & \textbf{Avg.} \\ \midrule
GPT4o & 64.25 & 62.75 & 67.86 & 63.49 & 92.00 & 70.07 \\ 
GPT4o-3@ & 65.70 & 62.75 & 66.07 & 66.67 & 91.00 & 70.44 \\ 
GPT4o-5@ & 66.15 & 61.76 & 66.96 & 66.67 & 92.00 & 70.71 \\ 
GPT4o-7@ & 65.81 & 63.73 & 66.96 & 68.25 & 91.00 & 71.15 \\ 
ReAct & 69.61 & 72.55 & 59.82 & 32.54 & 58.00 & 58.50 \\ 
ReAct-3@ & 74.75 & 83.33 & 66.07 & 52.38 & 53.00 & 65.91 \\ 
ReAct-5@ & 74.97 & 82.35 & 66.96 & 46.83 & 63.00 & 66.82 \\ 
ReAct-7@ & 75.53 & 84.78 & 67.86 & 50.79 & 57.00 & 67.19 \\ 
\midrule
AutoGPT & 66.37 & 78.43 & 64.29 & 60.83 & 90.00 & 71.98 \\
AgentVerse & 79.11 & \textbf{93.14} & 79.46 & 78.57 & 88.00 & 83.66 \\
GPTSwarm & 60.48 & 67.70 & 72.32 & 68.33 & 57.00 & 65.17 \\
AgentPrune & 70.84 & 91.18 & 81.25 & 81.75 & 93.00 & 83.60 \\
\midrule
o1-preview & 82.57 & 91.17 & \textbf{85.71} & 83.33 & \textbf{95.00} & 87.56 \\ 
\midrule
\textit{TalkHier (Ours)} & \textbf{83.80} & \textbf{93.14} & 84.68 & \textbf{87.30} & 93.00 & \textbf{88.38} \\ 
\bottomrule
\end{tabular}
\label{tab:mmlu_performance}
\end{table}
%\vspace{-5.5cm} % Adjust negative value as needed to reduce space

% CoMM & \textit{82.91} & \textit{92.15} & 81.25 & 85.20 \\ 

Table~\ref{tab:mmlu_performance} reports the average accuracy of various models on the five domains of MMLU dataset. \textit{TalkHier}, built on GPT-4o, achieves the highest average accuracy (88.38\%), outperforming open-source multi-agent models (e.g., AgentVerse, 83.66\%) and majority voting strategies applied to current LLM and single-agent baselines (e.g., ReAct-7@, 67.19\%; GPT-4o-7@, 71.15\%). These results highlight the effectiveness of our hierarchical refinement approach in enhancing GPT-4o’s performance across diverse tasks.
Although OpenAI-o1 cannot be directly compared to \textit{TalkHier} and other baselines—since they are all built on GPT-4o and OpenAI-o1’s internal design and training data remain undisclosed—\textit{TalkHier} achieves a slightly higher average score (88.38\% vs. 87.56\%), demonstrating competitive performance.

\subsection{Evaluation on WikiQA Benchmark (\textbf{RQ2})}
We evaluated \textit{TalkHier} and baselines on the WikiQA dataset, an open-domain question-answering benchmark. Unlike MMLU, WikiQA requires generating textual answers to real-world questions. The quality of generated answers was assessed using two metrics: Rouge-1 \cite{lin2004rouge}, which measures unigram overlap between generated and reference answers, and BERTScore \cite{zhang2020bertscore}, which evaluates the semantic similarity between the two. 

Table~\ref{tab:wikiqa_evaluation} shows that \textit{TalkHier} outperforms baselines in both Rouge-1 and BERTScore, demonstrating its ability to generate accurate and semantically relevant answers. While other methods, such as AutoGPT and AgentVerse, perform competitively, their scores fall short of \textit{TalkHier}, highlighting its effectiveness in addressing open-domain question-answering tasks.

\begin{table}[t]
\small
\centering
\caption{Evaluation Results on WikiQA. The table reports Rouge-1 and BERTScore for various models.}
\renewcommand{\arraystretch}{1.25} % Reduce row height
\setlength{\tabcolsep}{6pt} % Adjust column spacing
\setlength{\aboverulesep}{0pt} % Remove space above rules
\setlength{\belowrulesep}{0pt} % Remove space below rules
\setlength{\extrarowheight}{0pt} % Reduce row padding
\rowcolors{2}{gray!15}{white} % Alternating row colors
\begin{tabular}{lccc}
\toprule
\rowcolor{orange!10!white} \textbf{Models} & \textbf{Rouge-1} & \textbf{BERTScore}  \\ \midrule

GPT4o & 0.2777 & 0.5856  \\  
ReAct & 0.2409 & 0.5415  \\ 
\midrule
% CoMM & & \\
AutoGPT & 0.3286 & 0.5885\\
AgentVerse & 0.2799  &0.5716 \\
AgentPrune & 0.3027 & 0.5788 \\
GPTSwarm & 0.2302 & 0.5067 \\
\midrule
o1-preview & 0.2631 & 0.5701 \\ 
\midrule
\textit{TalkHier (Ours)} & \textbf{0.3461} & \textbf{0.6079} \\ 
\bottomrule
\end{tabular}
\label{tab:wikiqa_evaluation}
\end{table}








\subsection{Ablation Study (RQ3)}
To better understand the contribution of individual components in \textit{TalkHier}, we conducted ablation studies by removing specific modules and evaluating the resulting performance across the Moral Scenario, College Physics, and Machine Learning domains. The results of these experiments are summarized in Table~\ref{tab:ablation_study}.

\begin{table}[t]
\small
\centering
\caption{Ablative Results on Main Components of \textit{TalkHier}: Accuracy (\%) across Physics, ML, and Moral domains. \textit{TalkHier} w/o Eval. Sup. removes the evaluation supervisor. \textit{TalkHier} w/o Eval. Team excludes the evaluation team component. \textit{TalkHier} w. Norm. Comm uses a normalized communication protocol.}
\renewcommand{\arraystretch}{1.25} % Reduce row height
\setlength{\tabcolsep}{6pt} % Adjust column spacing
\setlength{\aboverulesep}{0pt} % Remove space above rules
\setlength{\belowrulesep}{0pt} % Remove space below rules
\setlength{\extrarowheight}{0pt} % Reduce row padding
\rowcolors{2}{gray!15}{white} % Alternating row colors
\begin{tabular}{lcccc}
        \toprule
        \rowcolor{orange!10!white} \textbf{Models} & \textbf{Moral} & \textbf{Phys.} & \textbf{ML} & \textbf{Avg.} \\ 
        \midrule
        w/o Eval. Sup. & 83.57 & 87.25 & 74.77 & 81.86 \\  
        w/o Eval. Team & 73.54 & 80.34 & 74.56 & 76.15 \\  
        w. Norm. Comm & 82.91 & 88.24 & 82.14 & 84.43 \\  
        React (Single Agent) & 69.61 & 72.55 & 59.82 & 67.33 \\  
        \midrule
        \textit{TalkHier (Ours)} & \textbf{83.80} & \textbf{93.14} & \textbf{84.68} & \textbf{87.21} \\  
        \bottomrule
% \toprule
% \rowcolor{orange!10!white}\textbf{Models} & \textbf{Phys.} & \textbf{ML} & \textbf{Moral} \\ \midrule
% w/o Eval. Sup. & 87.25 & 74.77 & 83.57 \\ 
% w/o Eval. Team & 80.34 & 74.56 & 73.54 \\ 
% w. Norm. Comm & 88.24 & 82.14 & 82.91 \\ 
% React (Single Agent) & 72.55 & 59.82 & 69.61 \\
% \midrule
% \textit{TalkHier (Ours)} & \textbf{93.14} & \textbf{83.04} & \textbf{83.80} \\  \bottomrule
\end{tabular}
\label{tab:ablation_study}
\end{table}


\begin{table}[h]
\small
\centering
\caption{Ablative Results: Accuracy (\%) across Physics, ML, and Moral domains. The study examines the impact of removing components from the structured communication protocol: message (\(\mathbf{M}_{ij}\)), background (\(\mathbf{B}_{ij}\)), and intermediate output (\(\mathbf{I}_{ij}\)).}
\renewcommand{\arraystretch}{1.25} % Reduce row height
\setlength{\tabcolsep}{6pt} % Adjust column spacing
\setlength{\aboverulesep}{0pt} % Remove space above rules
\setlength{\belowrulesep}{0pt} % Remove space below rules
\setlength{\extrarowheight}{0pt} % Reduce row padding
\rowcolors{2}{gray!15}{white} % Alternating row colors
\begin{tabular}{lcccc}
        \toprule
        \rowcolor{orange!10!white} \textbf{Models} & \textbf{Moral} & \textbf{Phys.} & \textbf{ML} & \textbf{Avg.} \\ 
        \midrule
        w/o \(\mathbf{I}_{ij}\) & 81.56 & 90.20 & 75.89 & 82.55 \\  
        w/o \(\mathbf{B}_{ij}\) & 76.87 & 87.50 & 70.54 & 78.30 \\  
        w/o \(\mathbf{B}_{ij}, \mathbf{I}_{ij}\) & 77.99 & 90.20 & 78.57 & 82.25 \\  
        \midrule
        \textit{TalkHier (Ours)} & \textbf{83.80} & \textbf{93.14} & \textbf{84.68} & \textbf{87.21} \\  
        \bottomrule

% \toprule
% \rowcolor{orange!10!white}\textbf{Models} & \textbf{Phys.} & \textbf{ML} & \textbf{Moral} \\ 
% \midrule
% w/o \(\mathbf{I}_{ij}\) & 90.20 & 75.89 & 81.56 \\ 
% w/o \(\mathbf{B}_{ij}\) & 87.50 & 70.54 & 76.87 \\ 
% w/o \(\mathbf{B}_{ij}, \mathbf{I}_{ij}\) & 90.20 & 78.57 & 77.99 \\ \midrule
% \textit{TalkHier (Ours)} & \textbf{93.14} & \textbf{83.04} & \textbf{83.80} \\ 
% \bottomrule
\end{tabular}
% \vspace{-10pt}
\label{tab:ablation_study2}
\end{table}

\begin{table*}[t]
\small
\centering
\caption{Evaluation Results on Camera Dataset. We report BLEU-4 (B4), ROUGE-1 (R1), BERTScore (BERT), and domain-specific metrics (Faithfulness, Fluency, Attractiveness, Character Count Violation(CCV)) following \cite{cyberagent_camera}.}
\renewcommand{\arraystretch}{1.25} % Reduce row height
\setlength{\tabcolsep}{6pt} % Adjust column spacing
\setlength{\aboverulesep}{0pt} % Remove space above rules
\setlength{\belowrulesep}{0pt} % Remove space below rules
\setlength{\extrarowheight}{0pt} % Reduce row padding
\rowcolors{2}{gray!15}{white} % Alternating row colors
\begin{tabular}{lccccccc}
\toprule
\rowcolor{orange!10!white}\textbf{Models} & \textbf{B4}~(↑) & \textbf{R1}~(↑) & \textbf{BERT}~(↑) & \textbf{Faithfulness}~(↑) & \textbf{Fluency}~(↑) & \textbf{Attractiveness}~(↑) & \textbf{CCV}~(↓)  \\ \midrule
GPT-4o & 0.01 & 0.02 & 0.65 & 4.8 & 5.9 & 6.5& 16\% \\ 
ReAct & 0.01 & 0.01 & 0.70 & 4.9 & 6.4 & \textbf{7.0} & 17\% \\ \midrule
OKG & 0.03 & 0.16 & 0.73 & 6.3 & 8.7 & 6.1 & \textbf{4\%} \\ \midrule
\textit{TalkHier (Ours)} & \textbf{0.04} & \textbf{0.20} & \textbf{0.91} & \textbf{8.6} & \textbf{8.9} & 6.2 & \textbf{4\%} \\ 
\bottomrule
\end{tabular}
% \vspace{-5pt}
\label{tab:camera_evaluation}
\end{table*}

Table~\ref{tab:ablation_study} presents the contributions of our ablation study on the main components in \textit{TalkHier}. Removing the evaluation Supervisor (\textit{TalkHier} w/o Eval. Sup.) caused a significant drop in accuracy, underscoring the necessity of our hierarchical refinement approach. Replacing the structured communication protocol with the text-based protocol (\textit{TalkHier} w. Norm. Comm) resulted in moderate accuracy reductions, while eliminating the entire evaluation team (\textit{TalkHier} w/o Eval.Team) led to substantial performance declines across all domains. These findings highlight the critical role of both agent-specific memory and hierarchical evaluation in ensuring robust performance.

Table~\ref{tab:ablation_study2} delves into the impact of individual elements in the communication protocol. Removing intermediate outputs (\textit{TalkHier} w/o \(\mathbf{I}_{ij}\)) or background information (\textit{TalkHier} w/o \(\mathbf{B}_{ij}\)) lead to inferior performance, with their combined removal (\textit{TalkHier} w/o \(\mathbf{B}_{ij}, \mathbf{I}_{ij}\)) yielding similar declines. These findings emphasize the value of context-rich communication for maintaining high performance in complex tasks.

\subsection{Evaluation on Ad Text Generation (RQ4)}  
We evaluate \textit{TalkHier} on the Camera dataset~\cite{cyberagent_camera} using traditional text generation metrics (BLEU-4, ROUGE-1, BERTScore) and domain-specific metrics (Faithfulness, Fluency, Attractiveness, and Character Count Violation)~\cite{cyberagent_camera}. These metrics assess both linguistic quality and domain-specific relevance.  

Setting up baselines like AutoGPT, AgentVerse, and GPTSwarm for this task was challenging, as their implementations focus on general benchmarks like MMLU and require significant customization for ad text generation. In contrast, OKG~\cite{OKG}, originally for ad keyword generation, was easier to adapt, making it a more practical baseline.  

Table~\ref{tab:camera_evaluation} presents the results. \textit{TalkHier} outperforms ReAct, GPT-4o, and OKG across most metrics, particularly excelling in Faithfulness, Fluency, and Attractiveness while maintaining a low Character Count Violation rate. The mean performance gain over the best-performing baseline, OKG, across all metrics is approximately 17.63\%.  

To verify whether \textit{TalkHier}’s multi-agent evaluations of attractiveness, fluency, and faithfulness are accurate, we conducted a subjective experiment on a sub-dataset of Camera, comparing the system’s automatic ratings to human judgments; details of this procedure are provided in Appendix \ref{app:sub_experiment}.

\section{Discussion}

The experimental results across the MMLU, WikiQA, and Camera datasets consistently demonstrate the superiority of \textit{TalkHier}. Built on GPT-4o, its hierarchical refinement and structured communication protocol enable robust and adaptable performance across diverse tasks.

\paragraph{General and Practical Benchmarks.}  
\textit{TalkHier} outperformed baselines across general and practical benchmarks. On MMLU, it achieved the highest accuracy (88.38\%), surpassing the best open-source multi-agent baseline, AgentVerse (83.66\%), by 5.64\%. On WikiQA, it obtained a ROUGE-1 score of 0.3461 (+5.32\%) and a BERTScore of 0.6079 (+3.30\%), outperforming the best baseline, AutoGPT (0.3286 ROUGE-1, 0.5885 BERTScore). On the Camera dataset, \textit{TalkHier} exceeded OKG across almost all metrics, demonstrating superior Faithfulness, Fluency, and Attractiveness while maintaining minimal Character Count Violations. These results validate its adaptability and task-specific strengths, highlighting its advantage over inference scaling models (e.g., OpenAI-o1), open-source multi-agent models (e.g., AgentVerse), and majority voting strategies (e.g., ReAct, GPT-4o).   

\paragraph{Comparative and Ablation Insights.}
While OpenAI-o1 achieved competitive MMLU scores, its unknown design and undisclosed training data make direct comparisons unfair. Since \textit{TalkHier} is built on the GPT-4o backbone, comparisons with other GPT-4o-based baselines are fair. Despite this, \textit{TalkHier} was competitive with OpenAI-o1 on MMLU and achieved a significant advantage on WikiQA. Ablation studies further emphasized the critical role of hierarchical refinement and structured communication. Removing core components, such as the evaluation supervisor or context-rich communication elements, significantly reduced performance, highlighting their importance in achieving robust results.


\section{Conclusions}

In this paper, we propose \textit{TalkHier}, a novel framework for LLM-MA systems that addresses key challenges in communication and refinement.
To the best of our knowledge, \textit{TalkHier} is the first framework to integrate a structured communication protocol in LLM-MA systems, embedding \textit{Messages}, \textit{intermediate outputs}, and \textit{background} information to ensure organized and context-rich exchanges.
At the same time, distinct from existing works that have biases on inputs, its hierarchical refinement approach balances and summarizes diverse opinions or feedback from agents.
% , mitigating biases commonly observed in related work.
\textit{TalkHier} sets a new standard for managing complex multi-agent interactions across multiple benchmarks, surpassing the best-performing baseline by an average of 5.64\% on MMLU, 4.31\% on WikiQA, and 17.63\% on Camera benchmarks.
Beyond consistently outperforming prior baselines, it also slightly outperforms the inference scaling model OpenAI-o1, demonstrating its potential for scalable, unbiased, and high-performance multi-agent collaborations.


% Experimental results demonstrate \textit{TalkHier}'s superiority across multiple benchmarks. On MMLU, it achieved 88.38\% accuracy, surpassing AgentVerse by 5.64\%. On WikiQA, it outperformed AutoGPT by 5.32\% in ROUGE-1 and 3.30\% in BERTScore. On the Camera dataset, it exceeded OKG in most metrics, demonstrating its ability to generate high-quality advertising text. These results validate its effectiveness in multi-agent collaboration.   

\section*{Limitations}

One of the main limitations of \textit{TalkHier} is the relatively high API cost associated with the experiments (see Appendix \ref{app:cost} for details). This is a trade-off due to the design of \textit{TalkHier}, where multiple agents collaborate hierarchically using a specifically designed communication protocol. While this structured interaction enhances reasoning and coordination, it also increases computational expenses.

This raises broader concerns about the accessibility and democratization of LLM research, as such costs may pose barriers for researchers with limited resources. Future work could explore more cost-efficient generation strategies while preserving the benefits of multi-agent collaboration.
% A limitation of \textit{TalkHier} is that we propsed structured communication among agents and hierarchical evaluation, which significantly improved performance but also generated a higher number of tokens. As a result, the experimental cost exceeded \$20,000 USD. This raises broader concerns about the accessibility and democratization of LLM research, as such costs may pose barriers for researchers with limited resources.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}

\onecolumn
\appendix
\begin{comment}
    

\newpage
\section{Related Work}
\label{rw}


\noindent\textbf{Collaborative LLM-MA.} The current landscape of LLM-MA systems is characterized by their ability to perform collaborative reasoning, dynamic role allocation, and adaptive task execution, as highlighted in~\cite{guo2024large, han2024challenges}. These systems extend the capabilities of single-agent LLMs by enabling agents to interact, share information, and coordinate efforts to solve complex, real-world tasks~\cite{li2024survey, shen2024small}. Key advancements include agent profiling for specialized roles~\cite{yang2024multi}, hierarchical communication protocols to enable nuanced exchanges~\cite{rasal2024harmony}, and the integration of external tools and environments to enhance functionality~\cite{qiu2024collaborative, liu2023dynamic}. Despite these advancements, challenges persist, particularly in ensuring robust communication, minimizing resource redundancy, and refining collaborative evaluation mechanisms~\cite{talebirad2023collaboration, park2023generative}. The survey underscores the necessity for standardized benchmarks, datasets, and frameworks to systematically evaluate and improve the efficacy of LLM-based multi-agent systems, paving the way for future innovations in this promising field~\cite{guo2024large, li2024survey}.

\noindent\textbf{Communication in LLM-MA.} Communication between collaborative agents serves as the foundational infrastructure supporting collective intelligence~\cite{guo2024large}. In most research on communication among agents, the exchanged content primarily takes the form of text~\cite{guo2024large}. For example, \cite{zhang2024cut} employs a one-shot pruning approach to remove non-essential components from text-based communication, improving its efficiency and precision. Similarly, \cite{shen2024small} fine-tunes multiple LLMs to summarize text-based communication, simplifying interactions among agents. Additionally, \cite{liu2024a} optimizes task-solving efficiency by filtering both agents and their corresponding text-based exchanges.

However, we argue that rather than merely customizing or condensing text-based communication, the key to effective communication lies in developing a well-structured protocol. Such a protocol should provide clear and context-specific guidelines for each subtask and agent. As tasks grow in complexity, agents often lose track of subtasks, forget the required format for intermediate outputs, or fail to recall relevant observations and actions. A structured communication framework can address these challenges, ensuring coherence and reliability across collaborative interactions.





\noindent\textbf{Debating or Feedback-Based Refinement.} Recent research has highlighted the effectiveness of feedback mechanisms in refining the performance of LLM-based agents. Methods like Self-Refine~\cite{madaan2023selfrefine} and feedback from agents with distinct roles~\cite{chen2024comm} enable agents to provide constructive criticism to each other. Similarly, \cite{wang2023coeval} promotes collaboration between LLMs and human evaluators by integrating machine-generated evaluations with human scrutiny, ensuring reliability in open-ended generation tasks. Other approaches adopt generator-evaluator frameworks, where generation-evaluation cycles are utilized to iteratively refine outputs, enhancing their accuracy and relevance~\cite{xu2024cooperative, liu2023evaluation}.


While these methods demonstrate that evaluator feedback can improve system accuracy, they also encounter key challenges. As feedback from different evaluators with varying perspectives accumulates, supervisors may struggle to balance these inputs effectively, often overlooking some or showing bias toward the order in which feedback is received. Additionally, the system can be sensitive to the order of opinions if the feedback from different agents is not organized or well summarized. These issues highlight the need for scalable and unbiased solutions to enhance multi-agent communication and evaluation processes.
\end{comment}

\newpage
\section{Cost Analysis for Experiments}
\label{app:cost}

The total expenditure for the experiments across the MMLU dataset, WikiQA, and Camera (Japanese Ad Text Generation) tasks was approximately \textbf{\$2,100 USD}. It is important to note that this amount reflects only the cost of final successful executions using the OpenAI 4o API (as \textit{TalkHier} and almost all other baselines are built on OpenAI 4o backbone). Considering the failures encountered during our research phase, the actual spending may have been at least three times this amount. Below is a detailed breakdown of costs and task-specific details.

\subsection{MMLU Dataset (1,450 USD)}
The \textbf{MMLU} dataset comprises approximately 16,000 multiple-choice questions across 57 subjects. For our experiments, we focused on five specific domains:

\subsubsection{Cost Analysis for the Moral Scenario Task and Baselines}

The \textbf{Moral Scenario} task involved generating and evaluating responses for various moral dilemma scenarios using OpenAI’s GPT-4o model. Each generation task for a single scenario produced approximately 48,300 tokens, with a cost of about \$0.17 per task. Given a total of 895 tasks, the overall token consumption and cost were:

\begin{equation}
0.17 \times 895 = 152.15 \text{ USD}
\end{equation}

In addition to the Moral Scenario task, we conducted multiple baseline tests using GPT-4o, which incurred an additional cost of approximately \$3,000 USD. Therefore, the total cost for all GPT-4o evaluations in the Moral Scenario task is:

\begin{equation}
152.15 + 900 = 1052.15 \text{ USD}
\end{equation}


\subsubsection{Cost Analysis for Other Tasks}

In addition to the previously analyzed tasks, we conducted further evaluations across multiple domains using OpenAI’s GPT-4o model. These tasks include College Physics, Machine Learning, Formal Logic, and US Foreign Policy. The number of tasks and token usage per task varied across these domains, with each task consuming between 40,000 to 46,000 tokens and costing between \$0.14 to \$0.15 per task. 

\begin{itemize}
    \item \textbf{College Physics}: 101 tasks, each generating 40,000 tokens.
    \item \textbf{Machine Learning}: 111 tasks, each generating 40,000 tokens.
    \item \textbf{Formal Logic}: 125 tasks, each generating 46,000 tokens.
    \item \textbf{US Foreign Policy}: 100 tasks, each generating 45,000 tokens.
\end{itemize}

The total expenditure for these tasks amounted to \$63.43 USD. and we also did experiments for various baseline, it cost around 320 usd. totally it is 383.43. These costs reflect the computational demands required to evaluate domain-specific questions and ensure consistency in model performance across various knowledge areas.

The total expenditure for these tasks amounted to \$63.43 USD. Additionally, we conducted experiments with various baseline models, which incurred an additional cost of approximately \$320 USD. In total, the overall expenditure was \textbf{\$383.43 USD}. These costs reflect the computational demands required for evaluating domain-specific questions and ensuring consistency in model performance across various knowledge areas.

\subsection{WikiQA Dataset (1,191.49 USD)}
The WikiQA dataset comprises 3,047 questions and 29,258 sentences, of which 1,473 sentences are labeled as answers to their corresponding questions. Each question required generating approximately 36,000 tokens, with an average cost of \$0.13 per question. Given this setup, the total expenditure for the WikiQA task was:

\begin{equation}
0.13 \times 1,473 = 191.49 \text{ USD}
\end{equation}

In addition to the execution of \textit{TalkHier}, we conducted multiple baseline tests using GPT-4o as their backbones, which incurred an additional cost of approximately \$1,000 USD. Therefore, the total cost for all GPT-4o evaluations in the WikiQA task is:

\begin{equation}
191.49 + 1000 = 1191.49 \text{ USD}
\end{equation}

This cost reflects the computational requirements for processing and analyzing a large-scale question-answering dataset. The WikiQA task serves as an important benchmark for evaluating the model’s performance in understanding and responding to real-world queries.

\subsection{Camera Dataset (400.56 USD)}

The \textbf{Camera} dataset task involved generating and evaluating ad headlines for 872 different test sets using OpenAI’s GPT-4o backbone. Each generation task produced approximately 65,000 tokens, with an average cost of \$0.23 per task. Given this setup, the total expenditure for the Camera dataset task was:

\begin{equation}
0.23 \times 872 = 200.56 \text{ USD}
\end{equation}

We also conducted experiments for three baseline models, which cost approximately \$200 USD. In total, the expenditure amounted to \$400.56 USD. This cost reflects the iterative process of generating and refining ad headlines across multiple input sets, ensuring high-quality and effective outputs tailored to the dataset’s domain-specific requirements.

\begin{comment}
    

\newpage
\section{Revision Example for Camera Dataset}
\label{sec:method_example}

To demonstrate the effectiveness of our hierarchical refinement framework, we provide an example based on generating Google Ads headlines and descriptions for "ISA講座" (ISA’s courses for English) a fictional office skill training program. This example highlights how evaluations for faithfulness, fluency, and attractiveness progressively refine content quality.

\subsection{Task Description}
The task is to create 8 headlines and 4 descriptions for ISA’s courses, adhering to strict character limits (30 characters for headlines and 90 characters for descriptions). The outputs must be engaging, factually accurate, and linguistically polished to appeal to users seeking professional skill development.

\subsection{Hierarchical Refinement with Structured Communication: Example}

\textit{TalkHier} employs hierarchical refinement, where evaluators independently evaluate content based on specific metrics such as Faithfulness, Fluency, and Attractiveness. Each evaluator reports to an Evaluation Team Supervisor, who consolidates and refines the results to reduce bias. This structured refinement process is exemplified below using headlines for ISA's courses.

\noindent \textbf{Faithfulness Refinement:}  
The initial headline claimed:  
\begin{quote}
    \textit{最短で資格取得！ISAの講座}  
    (Translation: Get certified in the shortest time! ISA's courses.)
\end{quote}

\noindent However, this statement lacked specificity and could mislead users. After refinement:  
\begin{quote}
    \textit{二週間で資格取得 ISAの講座}  
    (Translation: Get certified in two weeks with ISA's courses.)
\end{quote}  
This ensures the claim is accurate, aligning with the course's actual duration.

\vspace{1em}

\noindent \textbf{Fluency Refinement:}  
The initial headline read:  
\begin{quote}
    \textit{初心者にでも安心 ISA講座}  
    (Translation: Even beginners feel safe, ISA course.)
\end{quote}

\noindent The phrasing was slightly awkward. After refinement:  
\begin{quote}
    \textit{初心者でも安心のISA講座}  
    (Translation: A beginner-friendly ISA course.)
\end{quote}  
This revision improves grammatical correctness and flow in Japanese.

\vspace{1em}

\noindent \textbf{Attractiveness Refinement:}  
The original headline stated:  
\begin{quote}
    \textit{Office講座でスキル向上 ISA}  
    (Translation: Improve skills with ISA's Office course.)
\end{quote}

\noindent While factual, the headline lacked emotional appeal. After refinement:  
\begin{quote}
    \textit{キャリアアップに繋がるISA講座}  
    (Translation: ISA courses for career advancement.)
\end{quote}  
This version emphasizes benefits, making it more engaging for potential users.

\subsection{Hierarchical Refinement in Action}
In this process, evaluators specializing in faithfulness, fluency, and attractiveness analyzed the content individually. Their findings were summarized and synthesized by the Evaluation Team Supervisor, who ensured that the final revisions addressed all identified issues. This hierarchical approach minimized biases and ensured balanced feedback integration, highlighting the strength of our structured framework.

\end{comment}

\newpage
\section{Prompt Design and Work Flow for Tasks in MMLU}
\label{app::prompt4mmlu}
In this section, we describe the prompt design for evaluating and revising responses for each MMLU task. The task involves generating, evaluating, and refining answers to ethical dilemmas or moral situations using our multi-agent framework. Each agent in the framework plays a distinct role: generating potential solutions, evaluating their moral alignment, and revising answers to improve coherence and alignment with evaluation results. The prompts used for each agent are detailed below.

\subsection{Initial Prompt}
The following is the prompt given to the supervisor at the beginning.

\begin{prtbox}{Initial Prompt}
You are an expert in [\textit{Task}]. You must find the answer to the following question: [\textit{Question}]\\
The choices you are given are: [\textit{Choices}]\\
You can split up the problems into smaller parts if required.\\
The final answer must be only in the dictionary form of: [\textit{Output format}]
\end{prtbox}

\subsection{Answer Generator}
This agent generates answers to a specific moral scenario by considering the ethical implications of the situation.

\begin{membox}{Answer Generator Prompt}
You are an Answer Generator that has access to tools, to think of an answer for a specific given problem.\\

\textbf{Required Input}: Requirements as 'messages'\\
\textbf{Final output}: Expected answer as 'intermediate\_output' in the form of [\textit{Output format}]
\end{membox}

\subsection{Answer Evaluator}
This agent evaluates the answers generated by the Answer Generator, providing scores and feedback based on predefined metrics such as ethical soundness, logical consistency, fairness, and feasibility.

\begin{supbox}{Evaluator Team Supervisor Prompt}
You are an Answer Evaluator Team that has to evaluate the given answer.\\
The metrics are: [\textit{Metrics}]\\

\textbf{Required Input}: Expected answer as 'intermediate\_output'\\
\textbf{Final output}: Expected Answer and evaluation results embedded into 'intermediate\_output' in the form of [\textit{Output format}]
\end{supbox}

\subsection{Answer Revisor}
This agent revises answers that receive low scores in the evaluation step. Revisions must strictly follow the evaluation results to ensure improved alignment with the metrics.

\begin{membox}{Answer Revisor Prompt}
You are an Answer Revisor that receives an answer with their evaluation results, and outputs, if necessary, a revised answer that takes into account the evaluation results.\\
Follow these steps for a revision:
\begin{enumerate}
    \item You MUST first make a detailed analysis of ALL answers AND evaluation results. Double check that the evaluation results and reasons align with each other.
    \item  Based on the analysis, check if at least three of the four evaluations support each answer.
    \item If an answer is not supported by the majority of evaluations, you must flip the specific answer, making sure to update the choices as well
    \item In your final output, state: 1) If you need a re-evaluation which is necessary if a new modification has been made, and 2) The reasons behind your revisions.
\end{enumerate}
\end{membox}

\subsection{Settings for each Task}

% \subsubsection{Evaluator Types}
% \begin{table}[h]
%     \centering
%     \begin{tabular}{lc}
%         \toprule
%         Task  & Metric \\
%         \midrule
%         Moral Scenarios & "Intent", "Normality", "Responsibility", and "Well-being" \\
%         College Physics & "Mathematics",  "Physics"\\
%         Machine Learning &  "Assumptions", "Machine Learning"\\
%         Formal Logic & "Logical", "Truth Table" \\
%         US Foreign Policy & "Contradictory", "US Policy"\\
%         \bottomrule
%     \end{tabular}
%     \caption{List of evaluators allocated for each task}
%     \label{tab:task_metric}
% \end{table}

\subsubsection{Evaluator Types}
\begin{table}[h]
    \centering
    \begin{tabular}{lc p{8cm}}
        \toprule
        Task  & Metric & Description \\
        \midrule
        \multirow{5}{*}{Moral Scenarios} 
            & Intent & Evaluates the intentions behind actions. \\
            & Normality & Evaluates how normal the action is. \\
            & Responsibility & Evaluates the degree of responsibility behind the action. \\
            & Well-being & Evaluates whether the action promotes well-being. \\
        \midrule
        \multirow{2}{*}{College Physics} 
            & Mathematics & Evaluates mathematical correctness and calculations. \\
            & Physics & Evaluates the accuracy of physical principles applied. \\
        \midrule
        \multirow{6}{*}{Machine Learning} 
            & Answer Consistency & Checks underlying assumptions in models and methodologies. \\
            & Machine Learning & Evaluates machine learning concepts and implementation. \\
            & Stastical Soundenss & Evaluates whether the solution is sound in stastical terms.\\
        \midrule
        \multirow{9}{*}{Formal Logic} 
            & Logical Argument & Evaluates whether the arguments used are logically correct. \\
            & Truth Table & Evaluates correctness of generated truth tables and implied results. \\
            & Counterexample & Evaluates whether the counterexamples are utilized correctly. \\
            & Predicate Logic & Evaluates correctness of the use of predicate logic formulas. \\
            & Formal Logic & Evaluates correctness in terms of formal logic. \\
        \midrule
        \multirow{6}{*}{US Foreign Policy}
            & Factual Accuracy & Evaluates whether the facts implied by the answer are correct. \\
            & Policy Alignment & Evaluates whether the answer aligns with established policies and agreements. \\
            & Conceptual Clarity & Evaluates whether the answer accurately represents the core definition. \\
        \bottomrule
    \end{tabular}
    \caption{List of evaluators allocated for each task}
    \label{tab:task_metric}
\end{table}


\subsubsection{Tools}
To enhance the evaluation capabilities of each agent, we have deployed tools for each evaluator to use. The tools are listed as follows:

\begin{itemize}
    \item \textbf{Output Tool (All Evaluators)}: A tool for outputting thoughts, allowing the model to repeatedly think.
    \item \textbf{Truth Table Generator (Truth Table Evaluator)}: A tool for outputting a truth table, given a proposition as input.
    \item \textbf{Counterexample Verifier (Truth Table Evaluator)}: A tool for verifying whether a counterexample is correctly defined.
\end{itemize}
Here, the evaluator shown in the brackets are those who have access to the specific tool.



\subsection{Good Revision Example for Moral Scenarios Task}

The following example demonstrates how the multi-LLM framework revises an answer for a moral scenario. It includes the problem statement, the generated answer, the evaluation results, and the final revised answer, highlighting the reasoning process behind the revision.

\begin{egbox}{Good Revision Example}
\textbf{Problem:}  
For which of these two scenarios does the main character (who uses I/me/my) do something clearly morally wrong, according to ordinary moral standards in the US as of 2020?  
\textbf{Scenario 1:} I spoke to my mom on the phone during my entire work shift.  
\textbf{Scenario 2:} I helped my friend escape prison.  

\textbf{Correct Answer:} 0 (Wrong, Wrong)

\textbf{Generated Answer:} 2 (Not Wrong, Wrong)

\textbf{Evaluation Results:}  
\textbf{Scenario 1:}  
- Normality: Correct  
- Intent: Correct  
- Responsibility: \textcolor{red}{Incorrect}  
- Well-being: \textcolor{red}{Incorrect}  

\textbf{Scenario 2:}  
- All metrics: \textcolor{goodgreen}{Correct}

\textbf{Revisor Reasoning:}  
Based on the detailed evaluation results, the provided answer needs to be revised.  
- The evaluation indicates that \textbf{Scenario 1} is considered irresponsible and promotes harm to the well-being of everyone involved, which contradicts the generated answer of 'Not Wrong'. Therefore, the correct assessment is 'Wrong'.  
- The evaluation confirms that \textbf{Scenario 2} is considered irresponsible and harmful, which aligns with the generated answer of 'Wrong'.  

\textbf{Revised Answer:} 0 (Wrong, Wrong)
\end{egbox}


\clearpage
\section{ Prompt Design and Work Flow for for WikiQA}
\label{app::prompt4wiki}
In this section, we provide a detailed example of how the multi-agent framework processes a WikiQA task, specifically the question: "What are points on a mortgage?" This example demonstrates how agents interact to generate, evaluate, and revise an answer, ensuring that it meets all necessary criteria for accuracy, clarity, and completeness.

\subsection{Initial Question}
The user asks the question:  
\texttt{"What are points on a mortgage?"}

\subsection{Step 1: Answer Generation}

The first step involves the \textbf{Answer Generator} agent, which is tasked with generating a detailed response to the question. It considers the key components of the topic, such as mortgage points, their function, cost, and benefits.

\begin{membox}{Answer Generator Prompt}
You are an Answer Generator with access to tools for generating answers to specific questions. Your task is to:

1. Analyze the given problem deeply.  
2. Use the tools provided to retrieve and synthesize information.  
3. Craft a detailed and coherent response.

\textbf{Required Input}: Question and relevant details as \texttt{messages}.  
\textbf{Final Output}: Expected answer as \texttt{intermediate\_output}, formatted as follows:
\begin{verbatim}
{
  "answer": "One sentence answer",
  "details": "Supporting details or explanation"
}
\end{verbatim}
\end{membox}

The \textbf{Answer Generator} produces the following response:
\begin{quote}
"Points on a mortgage are upfront fees paid to the lender at the time of closing, which can lower the interest rate or cover other loan-related costs, with each point typically costing 1% of the loan amount."
\end{quote}

\subsection{Step 2: Evaluation by the ETeam Supervisor}

The \textbf{ETeam Supervisor} evaluates the answer based on two primary metrics: \textbf{Simplicity} and \textbf{Coverage}. The \textbf{Simplicity Evaluator} checks if the answer is concise and well-structured, while the \textbf{Coverage Evaluator} ensures that the response includes all relevant keywords and details.

\begin{supbox}{ETeam Supervisor Prompt}
You are an ETeam Supervisor tasked with evaluating answers using the following metrics:  
1. \textbf{Coverage}: Does the answer contain all related information and keywords?  
   - List all relevant keywords related to the problem.  
   - Provide explanations for each keyword and its relevance.  
2. \textbf{Simplicity}: Is the answer concise and easy to understand in one sentence?  
   - Check for redundancies and ensure appropriate sentence length.

\textbf{Steps for Evaluation}:
1. Summarize the conversation history.  
2. Provide a detailed analysis of the problem using output tools.  
3. Evaluate the most recent answer based on the metrics above.  
\end{supbox}

The \textbf{Simplicity Evaluator} concludes that the answer is clear, concise, and without any redundant information. The sentence is appropriate in length, neither too short nor too long.

The \textbf{Coverage Evaluator} confirms that the answer covers all the necessary aspects, including keywords such as "points," "upfront fees," "lender," "closing," "interest rate reduction," and "cost of points."

\subsection{Step 3: Revisions by the Answer Revisor}

Despite the high evaluation scores, the \textbf{Coverage Evaluator} suggests a slight revision for clarity. The \textbf{Answer Revisor} agent makes a minor adjustment to improve the answer's conciseness while maintaining its accuracy and comprehensiveness.

\begin{membox}{Answer Revisor Prompt}
You are an Answer Revisor responsible for refining answers based on evaluation results. Follow these steps:

1. Analyze the generated answer and evaluation results in detail.  
2. Check if all metrics have near full scores (\texttt{coverage} and \texttt{simplicity}).  
3. Revise the answer if required to address any shortcomings.  
4. State whether re-evaluation is necessary and justify your revisions.

\textbf{Important Notes}:
- Do not introduce personal opinions.  
- Ensure all changes strictly align with the evaluation feedback.  
\end{membox}

The \textbf{Answer Revisor} makes the following revision:

\begin{quote}
"Points on a mortgage are fees paid upfront to the lender at closing, which can lower the interest rate or cover other loan-related costs, with each point usually costing 1% of the loan amount."
\end{quote}

This slight modification enhances clarity without altering the meaning of the original response.

\subsection{Step 4: Final Evaluation}

The revised answer is re-evaluated by the \textbf{ETeam Supervisor}, and all metrics receive top scores. The revised response is clear, concise, and includes all relevant keywords and information, making it easy to understand.

\subsection{Final Answer}

After going through the generation, evaluation, and revision steps, the final answer to the question "What are points on a mortgage?" is:

\begin{quote}
"Points on a mortgage are fees paid upfront to the lender at closing, which can lower the interest rate or cover other loan-related costs, with each point usually costing 1% of the loan amount."
\end{quote}

\textbf{Evaluation Summary}:
- Simplicity: The answer is clear, concise, and free of redundancies.  
- Coverage: The answer includes all necessary keywords and information, covering key aspects such as "points," "upfront fees," "lender," "closing," "interest rate reduction," and "loan-related costs."

The final answer has received high scores in all evaluation metrics, confirming its quality and effectiveness in answering the user's question.

\subsection{BERT and ROUGE Scores}

To further evaluate the quality of the answer, we compute BERT and ROUGE scores:

- BERT Score: 0.5156  

- ROUGE Score: 0.2857

These scores indicate that the answer is both accurate and well-aligned with reference answers.



\newpage
\section{Prompt Design, Workflow and Revision Examples for Evaluating the Camera Dataset}
\label{app::prompt4camera}

In this section, we introduce our multi-LLM agent framework, a versatile and generalizable design for generating, evaluating, and refining ad text in various contexts. The framework is designed to handle tasks such as creating high-quality ad headlines, assessing their effectiveness based on key metrics, and improving underperforming content. 

Rather than being tailored to a specific dataset or domain, our framework adopts a modular structure where each agent is assigned a well-defined role within the pipeline. This design enables seamless integration with various tools and datasets, making it applicable to a wide range of ad text tasks beyond the Camera dataset. The prompts used for each agent reflect a balance between domain-agnostic principles and task-specific requirements, ensuring adaptability to diverse advertising scenarios.

The following sections provide the prompts used to define the roles of the agents within the framework.
\subsection{Japanese Ad Headlines Generator}
This agent generates high-quality Japanese ad headlines that are fluent, faithful, and attractive. It leverages tools such as a character counter, a reject words filter, and Google search for contextual information.  
The specific prompt for this agent is:  
\begin{membox}{Generator Prompt}
You are a Japanese Ad headlines Generator that has access to multiple tools to make high faithfulness, fluent, and attractive headlines in Japanese.  

Make sure to use the Google search tool to find information about the product, and the \texttt{character\_counter} to check the character count constraint. Also, check that it does not contain bad words with the \texttt{reject\_words} tool.  

\textbf{Input}: Requirements as \texttt{messages}.  
\textbf{Final Output}: A dictionary in Japanese in the form:  
\begin{verbatim}
{"Headline": [Headlines]}
\end{verbatim}
\end{membox}

\subsection{Ad Headlines Evaluator}
This agent evaluates the generated headlines based on three metrics: Faithfulness, Fluency, and Attractiveness.  
The specific prompt for this agent is:  
\begin{supbox}{Evaluator Team Supervisor Prompt}
You are an Ad headlines Evaluator that evaluates and scores every single headline to see if it meets the criteria of a good Ad text in Japanese.  

The metrics are: Faithfulness, Fluency, and Attractiveness.  

\textbf{Input}: A dictionary in the form:  
\begin{verbatim}
{"Headline": [Headlines]}
\end{verbatim}

\textbf{Final Output}: A dictionary in Japanese in the form:  
\begin{verbatim}
{"Headline": [Headlines], "Scores": [Faithfulness, Fluency, Attractiveness]}
\end{verbatim}
\end{supbox}

\subsection{Ad Headlines Reviser}
This agent revises low-scoring headlines to improve their Faithfulness, Fluency, and Attractiveness scores.  
The specific prompt for this agent is:  
\begin{membox}{Revisor Prompt}
You are an Ad Keyword Reviser that receives a dictionary in the form:  
\begin{verbatim}
{"Headline": [Headlines], "Scores": [Faithfulness, Fluency, Attractiveness]}
\end{verbatim}
and their three scores for Faithfulness, Fluency, and Attractiveness as input.  

You must modify the low-scoring headlines to improve their scores.  


Make sure to use the \texttt{character\_counter} to check the character count constraint.  

\textbf{Input}: A dictionary in the form:  
\begin{verbatim}
{"Headline": [Headlines]}
\end{verbatim}

\textbf{Final Output}: A dictionary in Japanese in the form:  
\begin{verbatim}
{"Headline": [Revised Headlines]}
\end{verbatim}
without any scores, just the revised text.
\end{membox}

\subsection{Tools Used in the Camera Ad Text Experiment}
\label{app::tool_camera}

To facilitate the generation, evaluation, and refinement of ad text for the Camera dataset, we implemented a set of specialized tools. These tools were designed to support various aspects of the ad text generation process, including character limit enforcement, search retrieval, click aggregation, and content filtering. Below is a description of each tool:

\begin{itemize}
    \item \textbf{Character Counter (Generator and Revisor)}: A utility for counting the number of characters in a given sentence. It takes as input a list of lists in the form \texttt{[[sentence, character limit], [sentence, character limit], ...]}, where each sentence is checked against a predefined character limit.

    \item \textbf{Google Search (Generator)}: A search engine tool used to retrieve real-time information from the web. This tool is particularly useful for answering queries related to current events based on search queries.

    \item \textbf{Output Tool (All Agents)}: A simple logging tool that allows agents to write their thoughts. This tool does not return any output but serves as an internal documentation mechanism.


    \item \textbf{Bad Performance Retriever (Revisor)}: A quality control tool that checks whether generated headlines or descriptions resemble undesirable outputs. It takes as input a dictionary in the form \texttt{\{"Headline": [headline1, ...], "Description": [description1, ...]\}} and returns a list of flagged items if any match known bad examples.

    \item \textbf{Reject Word Checker (Generator and Revisor)}: A filtering tool that verifies whether a sentence contains prohibited words. It processes a list of sentences and flags any containing words that should not be included.
\end{itemize}

These tools collectively enable structured ad text generation by enforcing constraints, retrieving relevant information, filtering out undesired outputs, and aggregating performance metrics. Their integration ensures high-quality and compliant ad text generation.

\begin{table}[htbp]
\centering
\caption{Revisions of Educational Ad Headlines with Highlights (Original: Japanese, Translated: English). The table shows functional translations for better readability while preserving the intent and effectiveness of the revisions.}
\label{tab:highlight_revision_edu}
\begin{tabular}{p{6.5cm} p{6.5cm}}
\toprule
\textbf{Before Revision} & \textbf{After Revision} \\
\midrule
Challenge prestigious school entrance exams & \textcolor{goodgreen}{Support your challenge} to enter prestigious schools \\
Guidance from professional home tutors & High-quality guidance from \textcolor{goodgreen}{professional home tutors} \\
We provide sure-win exam preparation & We provide \textcolor{goodgreen}{reliable exam preparation} \\
Improve grades with a customized curriculum & \textcolor{goodgreen}{Boost grades} with a customized curriculum \\
Prepare for exams online & \textcolor{goodgreen}{Effective exam preparation online} \\

\bottomrule
\end{tabular}
\end{table}

\begin{table}[htbp]
\centering
\caption{Revisions of Employment Ad Headlines with Highlights (Original: Japanese, Translated: English). The table shows functional translations for better readability while preserving the intent and effectiveness of the revisions.}
\label{tab:highlight_revision_emp}
\begin{tabular}{p{6.5cm} p{6.5cm}}
\toprule
\textbf{Before Revision} & \textbf{After Revision} \\
\midrule
Get a job with Baitoru NEXT & Find your \textcolor{goodgreen}{ideal job} with Baitoru NEXT \\
Job change and employment with Baitoru NEXT & For \textcolor{goodgreen}{career change and employment}, use Baitoru NEXT \\
Aim to debut with Baitoru NEXT & \textcolor{goodgreen}{Start your career} with Baitoru NEXT \\
Start your job search & Take the \textcolor{goodgreen}{first step in your career} \\
Find a new workplace & Discover \textcolor{goodgreen}{new job opportunities} \\
Opportunity to aim for a debut & \textcolor{goodgreen}{Opportunities for a successful debut} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Ad Headline Revisions with Highlights}

Tables \ref{tab:highlight_revision_edu} and \ref{tab:highlight_revision_emp} present two cases of translated ad headline revisions: one for educational ads and the other for employment-related ads. The revisions were made to enhance the clarity, specificity, and overall effectiveness of the headlines while maintaining their original intent.

In these tables, text highlighted in \textcolor{goodgreen}{green} represents a \textbf{good revision}, where improvements were made to make the ad more engaging, informative, or persuasive. These modifications focus on strengthening key selling points, increasing emotional appeal, and ensuring that the message is clear to potential users. 

For instance, in Table \ref{tab:highlight_revision_edu}, the phrase \textit{"Challenge prestigious school entrance exams"} was revised to \textit{"Support your challenge to enter prestigious schools"} to emphasize the supportive nature of the service rather than just the difficulty of the exams. Similarly, in Table \ref{tab:highlight_revision_emp}, the phrase \textit{"Get a job with Baitoru NEXT"} was revised to \textit{"Find your \textcolor{goodgreen}{ideal job} with Baitoru NEXT"}, making the headline more appealing by highlighting personalization and career goals.

These refinements contribute to more effective ad communication, ensuring that potential users better understand the value proposition of the services being advertised.





\subsection{An example of Hierarchical Refinement with Faithfulness, Fluency, Attractiveness}

\textit{TalkHier} employs a hierarchical refinement process where evaluators independently assess content (faithfulness, fluency, and attractiveness) and report their findings to an evaluation team supervisor. This supervisor synthesizes the feedback, ensuring reduced bias and improving the generated results. Below, we provide examples of refinements in headlines related to ISA’s Office courses, illustrating improvements in faithfulness, fluency, and attractiveness.

\noindent \textbf{Faithfulness Refinement:}  
Initial headline:  
\begin{quote}
    \textit{Fastest qualification with ISA courses.}
\end{quote}

This headline lacked specificity and could mislead users. After refinement:  
\begin{quote}
    \textit{Achieve qualification in two weeks with ISA courses.}
\end{quote}  
This correction provides an accurate depiction of the course duration.

\vspace{1em}

\noindent \textbf{Fluency Refinement:}  
Initial headline:  
\begin{quote}
    \textit{ISA courses: beginner friendly.}
\end{quote}

While understandable, the phrase was somewhat unnatural. After refinement:  
\begin{quote}
    \textit{Beginner-friendly ISA courses.}
\end{quote}  
This adjustment enhances grammatical accuracy and improves readability.

\vspace{1em}

\noindent \textbf{Attractiveness Refinement:}  
Initial headline:  
\begin{quote}
    \textit{Boost skills with ISA Office courses.}
\end{quote}

This headline, though factual, lacked emotional appeal. After refinement:  
\begin{quote}
    \textit{Advance your career with ISA Office courses.}
\end{quote}  
This modification creates a more engaging and motivational message for potential users.

\newpage
\section{Subjective Experiment for the Rating in \textit{TalkHier}}
\label{app:sub_experiment}

In this section, we describe our experimental setup for evaluating the quality of automatically generated advertisement headlines. Our proposed method, \textit{TalkHier}, is a multi-agent system designed to refine generated text by iteratively assessing and improving headlines across three key dimensions: attractiveness, fluency, and faithfulness. The refinement process relies on these internal evaluations to guide improvements. However, to ensure that these automated assessments capture human notions of headline quality, we must verify their consistency with human judgments. If \textit{TalkHier}’s multi-agent evaluations diverge significantly from human perceptions, the system’s refinements lose practical value. We therefore compare \textit{TalkHier} against a baseline, generating headlines using both methods. We then collect ratings from human evaluators as well as from \textit{TalkHier}’s own evaluation agents, and measure how closely the automated scores correlate with human ratings on attractiveness, fluency, and faithfulness. Demonstrating that these internal metrics align with human judgment is essential to validate our multi-agent refinement system.

\subsection{Setup and Data Collection}
We selected five distinct products, each of which serves as a target for generating advertisement headlines. 
For each product, we generated five headlines using \textit{TalkHier} (for a total of 25) and five headlines using the baseline model (another 25), 
thus obtaining \textbf{50 headlines} in total.

All headlines were evaluated by four human raters using a five-point scale (1 = ``very poor'' to 5 = ``excellent''). 
We also prompted GPT to rate each of these 50 headlines on the same 1--5 scale, effectively treating GPT as a fifth rater.

\subsection{Data Example}
\label{sec:data-example}
Table~\ref{tab:creditcard-sample-eng} provides a small subset of our dataset to illustrate how the information is organized. 
Each row corresponds to one generated headline and includes 
\textit{(i)} the product name or headline identifier, 
\textit{(ii)} the method that generated it, 
\textit{(iii)} the generated text, and 
\textit{(iv)} the ratings assigned by a subset of the human evaluators and \textit{TalkHier}.\footnote{For brevity, we show ratings from only two human raters here; the full dataset includes four human raters.}

\begin{table}[ht]
    \centering
    \small
    \caption{A sample of 10 headlines for the ``credit card'' product (LifeCard). 
    Five are generated by \textit{TalkHier}, and five by the baseline ReAct. 
    We show partial ratings (three of the four human raters plus the \textit{TalkHier} evaluation team) 
    to illustrate how \textit{TalkHier} generally receives higher scores than the Baseline.}
    \label{tab:creditcard-sample-eng}
    \begin{tabular}{lllcccc}
        \toprule
        \textbf{Headline} & \textbf{Method} & \textbf{Generated Headline (English)} 
            & \textbf{Human1} & \textbf{Human2} & \textbf{Human...} & \textbf{\textit{TalkHier}} \\
        \midrule
        % -- 5 headlines from TalkHier (generally higher ratings) --
        H1\_card & \textit{TalkHier} & LifeCard with No Annual Fee & 4.33 & 4.33 & ... & 5 \\
        H2\_card & \textit{TalkHier} & Receive Your Card in Two Business Days & 5 & 4.66 & ... & 4 \\
        H3\_card & \textit{TalkHier} & Earn Points for Every ¥100 You Spend & 4.33 & 5 & ... & 4.33 \\
        H4\_card & \textit{TalkHier} & Triple Points on Your Birthday Month & 4.33 & 4.33 & ... & 5 \\
        H5\_card & \textit{TalkHier} & A Card That Fits Your Lifestyle & 2.33 & 4 & ... & 4 \\
        \midrule
        % -- 5 headlines from Baseline (generally lower ratings) --
        H6\_card & ReAct & Full of Benefits, LifeCard is Here & 3.66 & 3 & ... & 3 \\
        H7\_card & ReAct & Start a New Life with LifeCard & 2.33 & 3.66 & ... & 2.33 \\
        H8\_card & ReAct & Save Smartly with LifeCard & 3.66 & 4.33 & ... & 3 \\
        H9\_card & ReAct & Shop with LifeCard & 3.66 & 3.66 & ... & 3 \\
        H10\_card & ReAct & Trusted and Reliable Life Card & 3.66 & 4 & ... & 3.66 \\
        \midrule
        \multicolumn{7}{c}{\dots \textit{(remaining headlines not shown)}} \\
        \bottomrule
    \end{tabular}
\end{table}

As shown in Table~\ref{tab:creditcard-sample-eng}, each headline in the dataset includes:
\begin{itemize}
    \item \textbf{Headline ID}: A unique identifier (e.g., ``H1\_favs'') that can encode product information.
    \item \textbf{Method}: Either \textit{TalkHier} (proposed method) or ``Baseline'' (GPT-4.0 or other reference model).
    \item \textbf{Generated Headline}: The actual text shown to human raters.
    \item \textbf{Human Ratings}: Numerical scores (1--5) from four human evaluators 
          (for brevity, only two are shown here).
    \item \textbf{\textit{TalkHier} Rating}: \textit{TalkHier}'s rating, also on a 1--5 scale.
\end{itemize}

\subsection{Evaluation Metrics}
To determine whether \textit{TalkHier} evaluates headlines similarly to human raters, we compute both 
\textbf{(i)} the correlation (Pearson and Spearman) between \textit{TalkHier}'s ratings and the average human ratings, 
and \textbf{(ii)} the Intraclass Correlation Coefficient (ICC), treating \textit{TalkHier} as an additional rater 
alongside the four humans. We report both ICC(2,1), which assesses agreement with individual raters, and ICC(2,4), 
which evaluates agreement with the collective human consensus.

\subsection{Evaluation Results}
\label{sec:evaluation-results}
We quantitatively assessed how closely \textit{TalkHier}'s ratings align with the human evaluations using both 
\textbf{(i)}~correlations (Pearson and Spearman) between \textit{TalkHier}'s ratings and the \emph{average} ratings of the four human evaluators, 
and \textbf{(ii)}~the Intraclass Correlation Coefficient (ICC) treating \textit{TalkHier} as an additional rater. 
Table~\ref{tab:evaluation-results} summarizes our main findings.

\begin{table}[ht]
    \centering
    \caption{Summary of evaluation metrics demonstrating how closely \textit{TalkHier}'s scores align with human ratings 
    for the 10 generated headlines. Confidence intervals (CIs) are not reported due to the small sample size.}
    \label{tab:evaluation-results}
    \begin{tabular}{lcc}
        \toprule
        \textbf{Metric} & \textbf{Value} & \textbf{p-value} \\
        \midrule
        Pearson Correlation 
        & 0.67 & 0.036 \\
        Spearman Correlation 
        & 0.68 & 0.030 \\
        ICC (2,1)  
        & 0.23 & -- \\
        ICC (2,4)  
        & 0.33 & -- \\
        \bottomrule
    \end{tabular}
\end{table}

\noindent
\textbf{Correlation Analysis.}
We computed Pearson's and Spearman's correlations between \textit{TalkHier}'s ratings (1--5 scale) and the mean human rating for each of the 10 headlines.
Both correlation coefficients, shown in Table~\ref{tab:evaluation-results}, indicate a moderate positive relationship 
(Pearson: $0.67$, Spearman: $0.68$), and both are statistically significant ($p < 0.05$). 

\noindent
\textbf{Intraclass Correlation (ICC).}
We further treated \textit{TalkHier} as an additional rater alongside the four human judges and computed both ICC(2,1) and ICC(2,4).
As reported in Table~\ref{tab:evaluation-results}, ICC(2,1) is $0.23$, indicating \emph{poor agreement} between \textit{TalkHier} and individual human raters.
However, ICC(2,4) is higher at $0.33$, indicating \emph{moderate agreement} between \textit{TalkHier} and the aggregated human ratings.

\noindent
\textbf{Why ICC(2,4) is higher than ICC(2,1)?}  
The difference between ICC(2,1) and ICC(2,4) suggests that \textit{TalkHier}'s ratings align more closely with the average human judgment rather than any specific individual rater. 
This could be due to variability among human raters, meaning individual ratings are inconsistent, but their mean rating is more stable.
Since ICC(2,4) evaluates agreement with the collective human consensus, the improved score indicates that \textit{TalkHier} captures general human preferences better than individual opinions.

\noindent
\textbf{Overall Implications.}  
These results suggest that while \textit{TalkHier} does not perfectly replicate individual human ratings, it effectively captures a broader human consensus. Thus, using \textit{TalkHier} to evaluate the generated ad text is reasonable, and its evaluation could provide relatively meaningful feedback to refine the ad text.

\end{CJK}
\end{document}
