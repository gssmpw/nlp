@misc{cot,
   author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
   title = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
   pages = {arXiv:2201.11903},
   month = {January 01, 2022},
   abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
   keywords = {Computer Science - Computation and Language
Computer Science -
Artificial Intelligence},
   year = {2022},
   type = {Electronic Article}
}

@article{guo2024large,
  title={Large Language Model Based Multi-Agents: A Survey of Progress and Challenges},
  author={Guo, Taicheng and Chen, Xiuying and Wang, Yaqi and Chang, Ruidi and Pei, Shichao and Chawla, Nitesh V and Wiest, Olaf and Zhang, Xiangliang},
  journal={arXiv preprint arXiv:2402.01680},
  year={2024}
}

@article{han2024challenges,
  title={LLM Multi-Agent Systems: Challenges and Open Problems},
  author={Han, Shiyang and Zhang, Qian and Yao, Yue and Jin, Wenhao and Xu, Zhen and He, Cheng},
  journal={arXiv preprint arXiv:2402.03578},
  year={2024}
}

@article{li2024survey,
  title={A Survey on LLM-Based Multi-Agent Systems: Workflow, Infrastructure, and Challenges},
  author={Li, Xiaoyu and Wang, Shuang and Zeng, Shaohui and Wu, Yucheng and Yang, Yue},
  journal={Vicinagearth},
  volume={1},
  number={9},
  year={2024}
}

@article{madaan2023selfrefine,
  title={Self-Refine: Iteratively Improving Text via Self-Feedback},
  author={Madaan, Aman and Tandon, Niket and Downey, Doug and Han, Shrimai},
  journal={arXiv preprint arXiv:2303.17651},
  year={2023}
}

@article{qian2024scaling,
  title={Scaling Large-Language-Model-based Multi-Agent Collaboration},
  author={Qian, Chen and Xie, Zihao and Wang, Yifei and Liu, Wei and Dang, Yufan and Du, Zhuoyun and Chen, Weize and Yang, Cheng and Liu, Zhiyuan and Sun, Maosong},
  journal={arXiv preprint arXiv:2406.07155},
  year={2024}
}

@article{qiu2024collaborative,
  title={Towards Collaborative Intelligence: Propagating Intentions and Reasoning for Multi-Agent Coordination with Large Language Models},
  author={Qiu, Xue and Wang, Hongyu and Tan, Xiaoyun and Qu, Chengyi and Xiong, Yifan and Cheng, Yang and Xu, Yichao and Chu, Wei and Qi, Yiming},
  journal={arXiv preprint arXiv:2407.12532},
  year={2024}
}

@article{rasal2024harmony,
  title={LLM Harmony: Multi-Agent Communication for Problem Solving},
  author={Rasal, Sudhir},
  journal={arXiv preprint arXiv:2401.01312},
  year={2024}
}

@article{shen2024small,
  title={Small LLMs Are Weak Tool Learners: A Multi-LLM Agent},
  author={Shen, Wenjun and Li, Cheng and Chen, Hui and Yan, Meng and Quan, Xuesong and Chen, Hao and Zhang, Jian and Huang, Fangyu},
  journal={arXiv preprint arXiv:2401.07324},
  year={2024}
}

@article{talebirad2023collaboration,
  title={Multi-Agent Collaboration: Harnessing the Power of Intelligent LLM Agents},
  author={Talebirad, Yashar and Nadiri, Amir},
  journal={arXiv preprint arXiv:2306.03314},
  year={2023}
}

@misc{tot,
   author = {Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Thomas L. and Cao, Yuan and Narasimhan, Karthik},
   title = {Tree of Thoughts: Deliberate Problem Solving with Large Language Models},
   pages = {arXiv:2305.10601},
   month = {May 01, 2023},
   note = {Code repo with all prompts: https://github.com/ysymyth/tree-of-thought- llm},
   abstract = {Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/ysymyth/tree-of-thought-llm.},
   keywords = {Computer Science - Computation and Language
Computer Science -
Artificial Intelligence
Computer Science - Machine Learning},
   year = {2023},
   type = {Electronic Article}
}

@article{wang2023coeval,
  title={CoEval: A Framework for Collaborative Human and Machine Evaluation},
  author={Wang, Xiaoyu and Liu, Yuanhao and Zhang, Hao},
  journal={arXiv preprint arXiv:2310.19740},
  year={2023}
}

@article{xu2024cooperative,
  title={Cooperative Evaluation in Large Language Model Refinement},
  author={Xu, Li and Sun, Qiang and Zhao, Hui},
  journal={arXiv preprint arXiv:2401.10234},
  year={2024}
}

@article{yang2024multi,
  title={Multi-LLM-Agent Systems: Techniques and Business Perspectives},
  author={Yang, Yuanhao and Peng, Qingqing and Wang, Jian and Zhang, Wenbo},
  journal={arXiv preprint arXiv:2411.14033},
  year={2024}
}

@article{zhang2024cut,
  title={Cut the crap: An economical communication pipeline for llm-based multi-agent systems},
  author={Zhang, Guibin and Yue, Yanwei and Li, Zhixun and Yun, Sukwon and Wan, Guancheng and Wang, Kun and Cheng, Dawei and Yu, Jeffrey Xu and Chen, Tianlong},
  journal={arXiv preprint arXiv:2410.02506},
  year={2024}
}

@inproceedings{zhuge2024gptswarm,
  title={GPTSwarm: Language Agents as Optimizable Graphs},
  author={Zhuge, Mingchen and Wang, Wenyi and Kirsch, Louis and Faccio, Francesco and Khizbullin, Dmitrii and Schmidhuber, J{\"u}rgen},
  booktitle={Forty-first International Conference on Machine Learning},
year={2024}
}

