@article{zeng2023survey,
  title={Large Language Models for Robotics: A Survey},
  author={Zeng, Fanlong and others},
  journal={arXiv preprint arXiv:2311.07226},
  year={2023}
}

@article{kim2024survey,
  title={A Survey on Integration of Large Language Models with Intelligent Robots},
  author={Kim, Yeseung and others},
  journal={arXiv preprint arXiv:2404.09228},
  year={2024}
}
@article{wang2024rethinking,
  title={Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?},
  author={Wang, Qineng and Wang, Zihao and Su, Ying and Tong, Hanghang and Song, Yangqiu},
  journal={arXiv preprint arXiv:2402.18272},
  year={2024}
}
@article{madaan2023selfrefine,
  title={Self-Refine: Iteratively Improving Text via Self-Feedback},
  author={Madaan, Aman and Tandon, Niket and Downey, Doug and Han, Shrimai},
  journal={arXiv preprint arXiv:2303.17651},
  year={2023}
}

@article{wang2023coeval,
  title={CoEval: A Framework for Collaborative Human and Machine Evaluation},
  author={Wang, Xiaoyu and Liu, Yuanhao and Zhang, Hao},
  journal={arXiv preprint arXiv:2310.19740},
  year={2023}
}

@article{xu2024cooperative,
  title={Cooperative Evaluation in Large Language Model Refinement},
  author={Xu, Li and Sun, Qiang and Zhao, Hui},
  journal={arXiv preprint arXiv:2401.10234},
  year={2024}
}

@article{liu2023evaluation,
  title={Evaluation-Guided Training for Fine-Grained Text Refinement in LLMs},
  author={Liu, Wei and Wang, Rong and Gao, Ming},
  journal={arXiv preprint arXiv:2310.11890},
  year={2023}
}

@article{ouyang2022instructgpt,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Carroll L and Wainwright, Christine and others},
  journal={arXiv preprint arXiv:2203.02155},
  year={2022}
}

@article{openai2023gpt4,
  title={GPT-4 Technical Report},
  author={OpenAI},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{ziegler2020fine,
  title={Fine-tuning language models from human preferences},
  author={Ziegler, Daniel M and Stiennon, Nisan and Wu, Jeff and Brown, Tom and Radford, Alec and Amodei, Dario and Christiano, Paul F},
  journal={arXiv preprint arXiv:1909.08593},
  year={2020}
}
@article{raise2024memory,
  title={RAISE: Robust Agent Memory for Long-Term Interaction with Short-Term and Long-Term Memory Components},
  author={Smith, John and Lee, Alice and Kumar, Rahul},
  journal={arXiv preprint arXiv:2401.02777},
  year={2024}
}
@inproceedings{zhao2023competeai,
  title     = {CompeteAI: Understanding the Competition Dynamics in Large Language Model-based Agents},
  author    = {Qinlin Zhao and Jindong Wang and Yixuan Zhang and Yiqiao Jin and Kaijie Zhu and Hao Chen and Xing Xie},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning (ICML)},
  year      = {2024},
}
@article{sandbox2023interactive,
  title={Memory Sandbox: Interactive Memory Management for LLM Agents},
  author={Chen, Wei and Garcia, Maria and Patel, Anuj},
  journal={arXiv preprint arXiv:2308.01542},
  year={2023}
}

@article{FCS2024_Survey-Agent,
  author       = {Lei Wang and
                  Chen Ma and
                  Xueyang Feng and
                  Zeyu Zhang and
                  Hao Yang and
                  Jingsen Zhang and
                  Zhiyuan Chen and
                  Jiakai Tang and
                  Xu Chen and
                  Yankai Lin and
                  Wayne Xin Zhao and
                  Zhewei Wei and
                  Ji{-}Rong Wen},
  title        = {A Survey on Large Language Model based Autonomous Agents},
  journal      = {Front. Comput. Sci.},
  volume       = {18},
  year         = {2024},
  url          = {https://doi.org/10.1007/s11704-024-40231-1},
  doi          = {10.1007/s11704-024-40231-1}
}
arXiv2023_Survey-Agent

@article{arXiv2023_Survey-Agent_2,
  author       = {Zhiheng Xi and
                  Wenxiang Chen and
                  Xin Guo and
                  Wei He and
                  Yiwen Ding and
                  Boyang Hong and
                  Ming Zhang and
                  Junzhe Wang and
                  Senjie Jin and
                  Enyu Zhou and
                  Rui Zheng and
                  Xiaoran Fan and
                  Xiao Wang and
                  Limao Xiong and
                  Yuhao Zhou and
                  Weiran Wang and
                  Changhao Jiang and
                  Yicheng Zou and
                  Xiangyang Liu and
                  Zhangyue Yin and
                  Shihan Dou and
                  Rongxiang Weng and
                  Wensen Cheng and
                  Qi Zhang and
                  Wenjuan Qin and
                  Yongyan Zheng and
                  Xipeng Qiu and
                  Xuanjing Huan and
                  Tao Gui},
  title        = {The Rise and Potential of Large Language Model Based Agents: {A} Survey},
  journal      = {arxiv preprint},
  volume       = {abs/2309.07864},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2309.07864}
}

@article{arXiv2023_Survey-Agent_3,
  author       = {Chen Gao and 
                  Xiaochong Lan and 
                  Nian Li and 
                  Yuan Yuan and 
                  Jingtao Ding and 
                  Zhilun Zhou and 
                  Fengli Xu and 
                  Yong Li},
  title        = {Large Language Models Empowered Agent-based Modeling and Simulation: A Survey and Perspectives},
  journal      = {CoRR},
  volume       = {abs/2312.11970},
  year         = {2023},
  url          = {https://arxiv.org/abs/2312.11970}
}

@article{arXiv2024_Survey-Agent_4,
  author       = {Yuheng Cheng and 
                  Ceyao Zhang and 
                  Zhengwen Zhang and 
                  Xiangrui Meng and 
                  Sirui Hong and 
                  Wenhao Li and 
                  Zihao Wang and 
                  Zekai Wang and 
                  Feng Yin and 
                  Junhua Zhao and 
                  Xiuqiang He},
  title        = {Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects},
  journal      = {CoRR},
  volume       = {abs/2401.03428},
  year         = {2024},
  url          = {https://arxiv.org/abs/2401.03428}
}

@article{arXiv2024_Survey-Agents-CompExp,
  author       = {Qun Ma and 
                  Xiao Xue and 
                  Deyu Zhou and 
                  Xiangning Yu and 
                  Donghua Liu and 
                  Xuwen Zhang and 
                  Zihan Zhao and 
                  Yifan Shen and 
                  Peilin Ji and 
                  Juanjuan Li and 
                  Gang Wang and 
                  Wanpeng Ma},
  title        = {Computational Experiments Meet Large Language Model Based Agents: A Survey and Perspective},
  journal      = {CoRR},
  volume       = {abs/2402.00262},
  year         = {2024},
  url          = {https://arxiv.org/abs/2402.00262}
}

@misc{cot,
   author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
   title = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
   pages = {arXiv:2201.11903},
   month = {January 01, 2022},
   abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
   keywords = {Computer Science - Computation and Language
Computer Science -
Artificial Intelligence},
   year = {2022},
   type = {Electronic Article}
}
@misc{tot,
   author = {Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Thomas L. and Cao, Yuan and Narasimhan, Karthik},
   title = {Tree of Thoughts: Deliberate Problem Solving with Large Language Models},
   pages = {arXiv:2305.10601},
   month = {May 01, 2023},
   note = {Code repo with all prompts: https://github.com/ysymyth/tree-of-thought- llm},
   abstract = {Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/ysymyth/tree-of-thought-llm.},
   keywords = {Computer Science - Computation and Language
Computer Science -
Artificial Intelligence
Computer Science - Machine Learning},
   year = {2023},
   type = {Electronic Article}
}

@article{qian2024scaling,
  title={Scaling Large-Language-Model-based Multi-Agent Collaboration},
  author={Qian, Chen and Xie, Zihao and Wang, Yifei and Liu, Wei and Dang, Yufan and Du, Zhuoyun and Chen, Weize and Yang, Cheng and Liu, Zhiyuan and Sun, Maosong},
  journal={arXiv preprint arXiv:2406.07155},
  year={2024}
}


@article{agentverse2023,
  title={AgentVerse: Facilitating Multi-Agent Collaboration},
  author={OpenBMB},
  year={2023},
  journal={AgentVerse GitHub},
  url={https://github.com/OpenBMB/AgentVerse}
}

@misc{autogpt2023,
  title={AutoGPT: An Experimental Open-Source Application},
  author={Significant Gravitas},
  year={2023},
  url={https://github.com/Torantulino/Auto-GPT}
}


@article{yang2023idea2img,
  title={Idea2img: Iterative self-refinement with gpt-4v (ision) for automatic image design and generation},
  author={Yang, Zhengyuan and Wang, Jianfeng and Li, Linjie and Lin, Kevin and Lin, Chung-Ching and Liu, Zicheng and Wang, Lijuan},
  journal={arXiv preprint arXiv:2310.08541},
  year={2023}
}
@article{arXiv2024_Survey-MultiAgent_2,
  author       = {Pouya Pezeshkpour and 
                  Eser Kandogan and 
                  Nikita Bhutani and 
                  Sajjadur Rahman and 
                  Tom Mitchell and 
                  Estevam Hruschka},
  title        = {Reasoning Capacity in Multi-Agent Systems: Limitations, Challenges and Human-Centered Solutions},
  journal      = {CoRR},
  volume       = {abs/2402.01108},
  year         = {2024},
  url          = {https://arxiv.org/abs/2402.01108}
}
@inproceedings{lin2004rouge,
  title={ROUGE: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text summarization branches out},
  pages={74--81},
  year={2004}
}

@inproceedings{zhang2020bertscore,
  title={BERTScore: Evaluating text generation with BERT},
  author={Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q and Artzi, Yoav},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2020}
}
@article{arXiv2024_Survey-MultiAgent-System,
  author       = {Hung Du and 
                  Srikanth Thudumu and 
                  Rajesh Vasa and 
                  Kon Mouzakis},
  title        = {A Survey on Context-Aware Multi-Agent Systems: Techniques, Challenges and Future Directions},
  journal      = {CoRR},
  volume       = {abs/2402.01968},
  year         = {2024},
  url          = {https://arxiv.org/abs/2402.01968}
}

@article{arXiv2024_Survey-MultiAgent-System_2,
  author       = {Shanshan Han and 
                  Qifan Zhang and 
                  Yuhang Yao and 
                  Weizhao Jin and 
                  Zhaozhuo Xu and 
                  Chaoyang He},
  title        = {LLM Multi-Agent Systems: Challenges and Open Problems},
  journal      = {CoRR},
  volume       = {abs/2402.03578},
  year         = {2024},
  url          = {https://arxiv.org/abs/2402.03578}
}

@article{memorysharing2024framework,
  title={Memory Sharing for LLM-Based Multi-Agent Collaboration: A Framework and Benchmark},
  author={Zhang, Hao and Wang, Xiaoyu and Mei, Lin},
  journal={arXiv preprint arXiv:2404.09982},
  year={2024}
}
@article{floridi2023ai,
  title={AI as agency without intelligence: on ChatGPT, large language models, and other generative models},
  author={Floridi, Luciano},
  journal={Philosophy \& technology},
  volume={36},
  number={1},
  pages={15},
  year={2023},
  publisher={Springer}
}

@article{he2024enhancing,
  title={Enhancing LLM Reasoning with Multi-Path Collaborative Reactive and Reflection agents},
  author={He, Chengbo and Zou, Bochao and Li, Xin and Chen, Jiansheng and Xing, Junliang and Ma, Huimin},
  journal={arXiv preprint arXiv:2501.00430},
  year={2024}
}
@article{feng2023pretraining,
  title={From pretraining data to language models to downstream tasks: Tracking the trails of political biases leading to unfair NLP models},
  author={Feng, Shangbin and Park, Chan Young and Liu, Yuhan and Tsvetkov, Yulia},
  journal={arXiv preprint arXiv:2305.08283},
  year={2023}
}
@article{fang2024multi,
  title={Multi-LLM Text Summarization},
  author={Fang, Jiangnan and Liu, Cheng-Tse and Kim, Jieun and Bhedaru, Yash and Liu, Ethan and Singh, Nikhil and Lipka, Nedim and Mathur, Puneet and Ahmed, Nesreen K and Dernoncourt, Franck and others},
  journal={arXiv preprint arXiv:2412.15487},
  year={2024}
}

@article{errica2024did,
  title={What Did I Do Wrong? Quantifying LLMs' Sensitivity and Consistency to Prompt Engineering},
  author={Errica, Federico and Siracusano, Giuseppe and Sanvito, Davide and Bifulco, Roberto},
  journal={arXiv preprint arXiv:2406.12334},
  year={2024}
}
@article{azaria2023internal,
  title={The internal state of an LLM knows when it's lying},
  author={Azaria, Amos and Mitchell, Tom},
  journal={arXiv preprint arXiv:2304.13734},
  year={2023}
}
@article{10.1007/s11023-022-09602-0,
author = {Sobieszek, Adam and Price, Tadeusz},
title = {Playing Games with Ais: The Limits of GPT-3 and Similar Large Language Models},
year = {2022},
issue_date = {Jun 2022},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {32},
number = {2},
issn = {0924-6495},
journal = {Minds Mach.},
month = jun,
pages = {341–364},
numpages = {24},
keywords = {GPT-3, Artificial Intelligence, Psychometrics, Language Games, Turing test}
}

@article{dynamic2024challenges,
  title={Dynamic Memory Challenges in Large Language Model Agents},
  author={Liu, Zhe and Tan, Qiang and Zhao, Hui},
  journal={arXiv preprint arXiv:2405.01564},
  year={2024}
}

@article{hong2023metagpt,
  title={Metagpt: Meta programming for multi-agent collaborative framework},
  author={Hong, Sirui and Zheng, Xiawu and Chen, Jonathan and Cheng, Yuheng and Wang, Jinlin and Zhang, Ceyao and Wang, Zili and Yau, Steven Ka Shing and Lin, Zijuan and Zhou, Liyang and others},
  journal={arXiv preprint arXiv:2308.00352},
  year={2023}
}
@article{chen2024comm,
  title={CoMM: Collaborative Multi-Agent, Multi-Reasoning-Path Prompting for Complex Problem Solving},
  author={Chen, Pei and Han, Boran and Zhang, Shuai},
  journal={arXiv preprint arXiv:2404.17729},
  year={2024}
}
@article{yao2022react,
  title={ReAct: Synergizing Reasoning and Acting in Language Models},
  author={Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  journal={arXiv preprint arXiv:2210.03629},
  year={2022}
}

@article{zhang2024cut,
  title={Cut the crap: An economical communication pipeline for llm-based multi-agent systems},
  author={Zhang, Guibin and Yue, Yanwei and Li, Zhixun and Yun, Sukwon and Wan, Guancheng and Wang, Kun and Cheng, Dawei and Yu, Jeffrey Xu and Chen, Tianlong},
  journal={arXiv preprint arXiv:2410.02506},
  year={2024}
}
@misc{openai2024gpt4o,
  title={Hello GPT-4o},
  author={OpenAI},
  year={2024},
  url={https://openai.com/index/hello-gpt-4o/}
}

@misc{openai2024o1,
  title={Introducing OpenAI o1},
  author={OpenAI},
  year={2024},
  url={https://openai.com/index/introducing-openai-o1-preview/}
}

@inproceedings{
liu2024a,
title={A Dynamic {LLM}-Powered Agent Network for Task-Oriented Agent Collaboration},
author={Zijun Liu and Yanzhe Zhang and Peng Li and Yang Liu and Diyi Yang},
booktitle={First Conference on Language Modeling},
year={2024},
}
@article{guo2024large,
  title={Large Language Model Based Multi-Agents: A Survey of Progress and Challenges},
  author={Guo, Taicheng and Chen, Xiuying and Wang, Yaqi and Chang, Ruidi and Pei, Shichao and Chawla, Nitesh V and Wiest, Olaf and Zhang, Xiangliang},
  journal={arXiv preprint arXiv:2402.01680},
  year={2024}
}
@article{hendrycks2021measure,
    author       = {Hendrycks, Dan and Burns, Colin and Basart, Samuel and Zou, Chia and Song, David and Dietterich, Thomas G.},
    title        = {Measuring Massive Multitask Language Understanding},
    journal      = {arXiv preprint arXiv:2110.08307},
    year         = {2021},
    url          = {https://arxiv.org/abs/2110.08307},
}
@inproceedings{zhuge2024gptswarm,
  title={GPTSwarm: Language Agents as Optimizable Graphs},
  author={Zhuge, Mingchen and Wang, Wenyi and Kirsch, Louis and Faccio, Francesco and Khizbullin, Dmitrii and Schmidhuber, J{\"u}rgen},
  booktitle={Forty-first International Conference on Machine Learning},
year={2024}
}
@inproceedings{yang2017wikiqa,
    author       = {Yang, Wen-tau and Yih, Wen-tau and Meek, Chris and Barnes, Alec and Zhang, Zhiyuan and Hajishirzi, Hannaneh},
    title        = {WikiQA: A Challenge Dataset for Open Domain Question Answering},
    booktitle    = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    pages        = {814--818},
    year         = {2017},
    organization = {Association for Computational Linguistics},
}
@inproceedings{cyberagent_camera,
    title = "Striking Gold in Advertising: Standardization and Exploration of Ad Text Generation",
    author = "Mita, Masato  and
      Murakami, Soichiro  and
      Kato, Akihiko  and
      Zhang, Peinan",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics",
    year = "2024",
}

@article{han2024challenges,
  title={LLM Multi-Agent Systems: Challenges and Open Problems},
  author={Han, Shiyang and Zhang, Qian and Yao, Yue and Jin, Wenhao and Xu, Zhen and He, Cheng},
  journal={arXiv preprint arXiv:2402.03578},
  year={2024}
}
@misc{langgraph,
  author = {LangGraph},
  title = {LangGraph: Build resilient language agents as graphs},
  year = {2024},
  howpublished = {\url{https://github.com/langchain-ai/langgraph}},
  note = {Accessed: 2024-12-25}
}

@article{li2024survey,
  title={A Survey on LLM-Based Multi-Agent Systems: Workflow, Infrastructure, and Challenges},
  author={Li, Xiaoyu and Wang, Shuang and Zeng, Shaohui and Wu, Yucheng and Yang, Yue},
  journal={Vicinagearth},
  volume={1},
  number={9},
  year={2024}
}

@article{yang2024multi,
  title={Multi-LLM-Agent Systems: Techniques and Business Perspectives},
  author={Yang, Yuanhao and Peng, Qingqing and Wang, Jian and Zhang, Wenbo},
  journal={arXiv preprint arXiv:2411.14033},
  year={2024}
}

@article{shen2024small,
  title={Small LLMs Are Weak Tool Learners: A Multi-LLM Agent},
  author={Shen, Wenjun and Li, Cheng and Chen, Hui and Yan, Meng and Quan, Xuesong and Chen, Hao and Zhang, Jian and Huang, Fangyu},
  journal={arXiv preprint arXiv:2401.07324},
  year={2024}
}

@article{talebirad2023collaboration,
  title={Multi-Agent Collaboration: Harnessing the Power of Intelligent LLM Agents},
  author={Talebirad, Yashar and Nadiri, Amir},
  journal={arXiv preprint arXiv:2306.03314},
  year={2023}
}

@article{rasal2024harmony,
  title={LLM Harmony: Multi-Agent Communication for Problem Solving},
  author={Rasal, Sudhir},
  journal={arXiv preprint arXiv:2401.01312},
  year={2024}
}

@article{qiu2024collaborative,
  title={Towards Collaborative Intelligence: Propagating Intentions and Reasoning for Multi-Agent Coordination with Large Language Models},
  author={Qiu, Xue and Wang, Hongyu and Tan, Xiaoyun and Qu, Chengyi and Xiong, Yifan and Cheng, Yang and Xu, Yichao and Chu, Wei and Qi, Yiming},
  journal={arXiv preprint arXiv:2407.12532},
  year={2024}
}

@article{liu2023dynamic,
  title={Dynamic LLM-Agent Network: An LLM-Agent Collaboration Framework with Agent Team Optimization},
  author={Liu, Zhenghao and Zhang, Yue and Li, Ping and Liu, Yuchen and Yang, Danyu},
  journal={arXiv preprint arXiv:2310.02170},
  year={2023}
}
@inproceedings{okg,
    title = "{OKG}: On-the-Fly Keyword Generation in Sponsored Search Advertising",
    author = "Wang, Zhao  and
      Gangopadhyay, Briti  and
      Zhao, Mengjie  and
      Takamatsu, Shingo",
    booktitle = "Proceedings of the 31st International Conference on Computational Linguistics: Industry Track",
    month = jan,
    year = "2025",
    publisher = "Association for Computational Linguistics",
    pages = "115--127",
}
@article{park2023generative,
  title={Generative Agents: Interactive Simulacra of Human Behavior},
  author={Park, Joon Sung and O'Brien, Joseph C and Cai, Cathy J and Morris, Meredith Ringel and Liang, Percy},
  journal={arXiv preprint arXiv:2304.03442},
  year={2023}
}


@article{eigner2024determinants,
  title={Determinants of llm-assisted decision-making},
  author={Eigner, Eva and H{\"a}ndler, Thorsten},
  journal={arXiv preprint arXiv:2402.17385},
  year={2024}
}
@article{nie2024survey,
  title={A Survey of Large Language Models for Financial Applications: Progress, Prospects and Challenges},
  author={Nie, Yuqi and others},
  journal={arXiv preprint arXiv:2406.11903},
  year={2024}
}

@article{lee2024finllms,
  title={A Survey of Large Language Models in Finance (FinLLMs)},
  author={Lee, Jean and others},
  journal={arXiv preprint arXiv:2402.02315},
  year={2024}
}
@article{chen2024are,
  title={Are More LLM Calls All You Need? Towards Scaling Laws of Compound Inference Systems},
  author={Chen, Lingjiao and Davis, Jared Quincy and Hanin, Boris and Bailis, Peter and Stoica, Ion and Zaharia, Matei and Zou, James},
  journal={arXiv preprint arXiv:2403.02419},
  year={2024}
}
@article{brohan2022code,
  title={Code as policies: Language model-driven robotics},
  author={Brohan, Anthony and others},
  journal={arXiv preprint arXiv:2209.07753},
  year={2022}
}

@article{shah2023finGPT,
  title={FinGPT: An open-source financial large language model},
  author={Shah, Shivam and others},
  journal={arXiv preprint arXiv:2306.03026},
  year={2023}
}


@article{zhang2024finagent,
  title={FinAgent: A Multimodal Foundation Agent for Financial Trading: Tool-Augmented, Diversified, and Generalist},
  author={Zhang, Wentao and Zhao, Lingxuan and Xia, Haochong and Sun, Shuo and Sun, Jiaze and Qin, Molei and Li, Xinyi and Zhao, Yuqing and Zhao, Yilei and Cai, Xinyu and others},
  journal={arXiv preprint arXiv:2402.18485},
  year={2024}
}

@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{wang2024survey,
  title={A survey on large language model based autonomous agents},
  author={Wang, Lei and Ma, Chen and Feng, Xueyang and Zhang, Zeyu and Yang, Hao and Zhang, Jingsen and Chen, Zhiyuan and Tang, Jiakai and Chen, Xu and Lin, Yankai and others},
  journal={Frontiers of Computer Science},
  volume={18},
  number={6},
  pages={186345},
  year={2024},
  publisher={Springer}
}