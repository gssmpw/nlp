\documentclass{article}


\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{graphicx}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage[capitalize,noabbrev]{cleveref}
\graphicspath{ {./images/} }


\title{Self-Supervised Graph Contrastive Pretraining\\
for Device-level Integrated Circuits}


\author{
 Sungyoung Lee \\
  Department of Electrical and Computer Engineering\\
  The University of Texas at Austin\\
  \texttt{sylee@utexas.edu} \\
  \And
 Ziyi Wang \\
  Department of Computer Science and Engineering\\
  The Chinese University of Hong Kong\\
  \texttt{ziyiwang21@cse.cuhk.edu.hk} \\
  \And
 Seunggeun Kim \\
  Department of Electrical and Computer Engineering\\
  The University of Texas at Austin\\
  \texttt{sgkim@utexas.edu} \\
  \And
 Taekyun Lee \\
  Department of Electrical and Computer Engineering\\
  The University of Texas at Austin\\
  \texttt{taekyun@utexas.edu} \\  
  \And
 David Z. Pan \\
  Department of Electrical and Computer Engineering\\
  The University of Texas at Austin\\
  \texttt{dpan@ece.utexas.edu} \\
}

\begin{document}
\maketitle
\begin{abstract}
Self-supervised graph representation learning has driven significant advancements in domains such as social network analysis, molecular design, and electronics design automation (EDA). However, prior works in EDA have mainly focused on the representation of gate-level digital circuits, failing to capture analog and mixed-signal circuits. To address this gap, we introduce DICE: Device-level Integrated Circuits Encoder, the first self-supervised pretrained graph neural network (GNN) model for any circuit expressed at the device level. DICE is a message-passing neural network (MPNN) trained through graph contrastive learning, and its pretraining process is simulation-free, incorporating two novel data augmentation techniques. Experimental results demonstrate that DICE achieves substantial performance gains across three downstream tasks, underscoring its effectiveness for both analog and digital circuits.
\end{abstract}



\section{Introduction} \label{sec:intro}

Graph representation learning has emerged as a powerful tool for extracting meaningful patterns from graph data~\cite{hamilton2017representation, hamilton2020graph}.
In particular, self-supervised graph representation learning~\cite{jin2020self, wu2021self} has demonstrated great potential in fields where labeled data is scarce, such as healthcare~\cite{ghassemi2020review}, molecular chemistry~\cite{zang2023hierarchical}, and electronics design automation (EDA)~\cite{chen2024dawn}.
With abundant unlabeled data and domain knowledge, these methods significantly expand the dataset for effective training, and enables models to learn robust representations.


In the field of EDA, label scarcity is particularly pronounced.
This challenge arises from several factors, including the high costs of running the full hardware design flow, the limited availability of circuit designs for dataset formation, and the huge design space influenced by device parameters.
To address this issue, existing works have explored various self-supervised approaches that have contributed to advances in the design automation of gate-level digital circuits~\cite{iccad23-deepgate2, wang2022fgnn, wang2024fgnn2}.
\textbf{However, self-supervised graph representation learning for {device-level} analog and mixed-signal (AMS) circuits has not been thoroughly investigated.}
AMS circuits, which incorporate both analog and digital components, cannot be fully represented using only logic gates due to the presence of non-binary voltage outputs.
As a result, previous gate-level self-supervised learning methods cannot be directly applied to circuits defined at the device level.

Three key challenges hinder the application of self-supervised graph representation learning to device-level circuits.
\textbf{First, it is less straightforward to construct graphs for device-level analog circuits than for gate-level digital circuits.}
Unlike logic gates with fixed input and output directions, signal flow in transistor devices varies depending on voltage inputs, as shown in \Cref{fig:problem}(a).
This lack of clarity leads to inconsistent graph construction methods~\cite{ren2020paragraph, hakhamaneshi2022pretraining, li2023design, yamakaji2024circuit2graph, jiang2025boosting}.

\begin{figure}[t]
% \vskip 0.2in
    \centering
    \includegraphics[width=0.55\linewidth]{figs/problem.pdf}
    \caption{\textbf{Three challenges in applying self-supervised graph representation learning on device-level circuits.}}
    \label{fig:problem}
\vskip -0.2in
\end{figure}

\textbf{Second, traditional graph augmentation techniques, such as randomly adding nodes or edges, are inadequate for device-level circuits.}
As shown in \Cref{fig:problem}(b), even a small change like adding an edge to the ground can fix the output voltage at zero and disrupt the circuit’s functionality.
Logic synthesis tools~\cite{wolf2013yosys, brayton2010abc} can be useful in the data augmentation of gate-level digital circuits~\cite{li2022deepgate, wang2022fgnn}, but these are not available for device-level circuits.

\textbf{Third, there is a lack of benchmarks for device-level circuits that consider diverse circuit structures within each task.} 
As illustrated in~\Cref{fig:problem}(c), previous works typically focus on topology-specific learning that applies separate models for different topologies~\cite{settaluri2020autockt,budak2021dnn,poddar2024insight, wang2024learn}.
This highlights the need for a comprehensive cross-topology benchmark to better evaluate graph learning methods for device-level circuits.

\begin{figure*}[t]
% \vskip 0.2in
    \centering
    \includegraphics[width=0.96\linewidth]{figs/overview.pdf}
    \caption{\textbf{Overall framework.} First, a large pretraining dataset is created by augmenting the initial circuits. Positive augmentation produces circuits with similar high-level semantics to the initial ones, while negative augmentation modifies these semantics. DICE is then pretrained using graph contrastive learning, maximizing similarity between central and positive circuits while minimizing it for unequal circuits. After pretraining, DICE is integrated into new models to solve various downstream tasks.}
    \label{fig:overview}
\vskip -0.1in
\end{figure*}

Considering these challenges, we propose DICE: Device-level Integrated Circuits Encoder, a pretrained graph neural network (GNN) applicable to both analog and digital circuits.
As illustrated in~\Cref{fig:overview}, our method begins with a novel graph construction method that effectively distinguishes different net and device connections.
Following this, we propose two customized data augmentation techniques described in \Cref{fig:dataaugmentation}.
The augmentation either preserve or alter the high-level semantics of circuits, providing diversity to the pretraining dataset.
Then, we design a contrastive learning loss considering three different relationships between circuits: positive, negative, and unequal.
After pretraining, DICE is integrated to the full message passing neural network (MPNN) in \Cref{fig:fullmodelarch} and evaluated on our proposed benchmark. The benchmark consists of three downstream tasks, each with multiple circuit topologies.

The contributions of our work are as follows:
\begin{itemize}
    \item To the best of our knowledge, this is the first self-supervised graph representation learning method designed for device-level integrated circuits (IC).
    \item We propose a novel circuit-to-graph mapping that incorporates edge features to capture different interactions between voltage nets and transistors.
    \item We introduce two novel data augmentation techniques that do not use any circuit simulation or EDA tool.
    \item We present a benchmark comprising three graph learning tasks for device-level circuits, and all tasks are not restricted to a single circuit topology.
\end{itemize}




\section{Preliminaries} \label{sec:prelim}


\textbf{Graph Contrastive Learning} is one of the major self-supervised graph representation learning methods. 
These methods learn node, edge, or graph embeddings by maximizing similarities between positive (similar) samples and minimizing those between negative (dissimilar) samples.
To enable the learning of robust representations applicable to various tasks, the approach frequently uses data augmentation for self-supervision~\cite{zhao2021data}, such as adding nodes or perturbing edges within the graph.

One of the most widely used objectives for contrastive learning is the NT-Xent loss~\cite{chen2020simple}. Starting from $N$ number of data, each data is augmented twice to form $2N$ samples, forming $N$ positive pairs. For each sample $i\in\{1,2,\cdots,2N\}$, let $\mathbf{z}_i$ be its embedding and $\mathbf{z}_{j(i)}$ be the embedding of its positive counterpart. The NT-Xent loss is then given by:
\begin{equation}
\small
\label{eq:simclr}
\mathcal{L}_{\text{NT-Xent}}
=
\frac{1}{2N}
\sum_{i=1}^{2N}
-\log
\frac{\exp\bigl(\mathrm{sim}(\mathbf{z}_i, \mathbf{z}_{j(i)}) / \tau\bigr)}
{\sum_{k=1}^{2N} \mathbf{1}_{[k \neq i]}\exp\bigl(\mathrm{sim}(\mathbf{z}_i, \mathbf{z}_k) / \tau\bigr)}
\end{equation}
where $\mathrm{sim}$ is a similarity function (i.e. cosine similarity) and $\tau$ is a temperature parameter.
Minimizing $\mathcal{L}_{\text{NT-Xent}}$ clusters the features of positive pairs in the embedding space while pushing negative pairs apart, resulting in discriminative representations.
In graph settings, contrastive learning can be operated in three levels: node, edge, and graph level. Depending on the targeting task, the inputs $\mathbf{z}$ in Eq.~\eqref{eq:simclr} can be either node, edge, or graph embeddings.


\textbf{Graph Representation Learning on Digital Circuits.}
Logic gates, the basic components of digital circuits, can be constructed as directed acyclic graphs (DAGs)~\cite{mishchenko2006dag}. With these graphs, pioneering works have further pretrained models to reduce the excessive time and cost associated with large-scale digital hardware design. Both supervised~\cite{li2022deepgate, iccad23-deepgate2} and self-supervised~\cite{wang2022fgnn, wang2024fgnn2} pretraining methods exist for digital circuits, addressing tasks such as predicting functional similarities between circuit logics.
To address data scarcity, these works utilize logic synthesis tools such as Yosys~\cite{wolf2013yosys} and ABC~\cite{brayton2010abc}, which generate augmented circuit views preserving identical functionalities. Recently, large language models (LLMs) have also been explored as one approach for augmenting digital circuit data~\cite{chang2024data, liu2023verilogeval}, which are also used for multimodal circuit representation learning~\cite{anonymous2025circuitfusion}.
However, despite numerous studies on digital circuit representation learning, these approaches cannot be applied to AMS circuits as they can not be represented just with logic gates.

\textbf{Graph Representation Learning on AMS Circuits.} Due to ambiguous signal directions, various graph construction methods have been proposed for AMS circuits. CktGNN~\cite{dong2023cktgnn} form a DAG by setting voltage nets as nodes, and basic operational amplifier circuits and passive devices as edges.
Paragraph~\cite{ren2020paragraph} represents both devices and voltage nets as nodes, while also distinguishing different edge types to predict layout parasitics using the attention mechanism.
A supervised pretraining approach in~\cite{hakhamaneshi2022pretraining} sets every device pins as separate nodes and pretrains GNN to predict DC voltage outputs. The learned representations are then applied to predict simulation results on unseen circuits. Other methods include considering AC and DC current paths as different edges~\cite{li2023design}, setting only nets as nodes~\cite{jiang2025boosting}, or setting only devices as nodes~\cite{yamakaji2024circuit2graph}.
However, self-supervised pretraining methods remain underexplored, and one reason is the lack of a reliable data augmentation tool for AMS circuits.
While previous works on data augmentation for AMS circuits~\cite{deeb2023robust, deeb2024graph} exist, they are task-specific and primarily focus on analog circuits.
Large language models (LLMs) can be an alternative as they can also generate AMS circuits at the device level~\cite{lai2024analogcoder}, but their effectiveness as a data augmentation tool remains uncertain.






\section{Method} \label{sec:method}
Self-supervised pretraining for device-level circuits presents multiple challenges.
A significant requirement is the development of an appropriate graph construction method that accurately addresses the signal ambiguity of device-level circuits (\Cref{fig:problem}(a)).
Additionally, there is a necessity for a tailored graph augmentation technique that preserves the high-level semantics inherent in circuit designs.
To address these challenges, we propose \textbf{DICE},  
the first self-supervised pretrained model for device-level circuits.
In the following subsections, we will introduce a novel graph construction method for device-level circuits (\Cref{sec:constr}), a customized data augmentation method that leverages domain knowledge of devices in integrated circuits (\Cref{sec:aug}), and a contrastive learning framework based on our circuit augmentation scheme (\Cref{sec:loss}).
Moreover, we also state our method on how we incorporated the pretrained model to solve diverse downstream tasks (\Cref{sec:dwnstrm}).

\begin{figure*}[t]
    % \vskip 0.2in
    \begin{center}
    \centerline{\includegraphics[width=0.95\textwidth]{figs/DataAugmentation.pdf}}
    \caption{\textbf{Positive and Negative Data Augmentation.} Devices are modified while preserving all connections to other nodes. The theoretical intuition behind negative augmentation is detailed in Appendix~\ref{appendix:da}.}
    \label{fig:dataaugmentation}
    \end{center}
    \vskip -0.2in
\end{figure*}

\subsection{Graph Construction} \label{sec:constr}
The absence of a unified graph construction method for device-level circuits stems primarily from the complex interactions between transistor devices and the voltage nodes connected at each pin.
Recent studies~\cite{hakhamaneshi2022pretraining, anonymous2025analoggenie} have modeled each device pin as a distinct node, separate from its corresponding device node.
However, these methods utilize undirected edges, which fail to capture different types of connections between voltage nets and transistor devices.
For instance, the voltage signal from a transistor’s gate influences the current flow through the transistor, but the transistor itself yields significantly less influence on the signal at the gate node. Similarly, the interactions between drain and source nodes and the transistor differ from those with the gate node.
Previous approaches cannot distinguish these different connections.

To address this limitation, we introduce our new circuit-to-graph mapping designed specifically for device-level circuits.
The left part of \Cref{fig:overview} illustrates our proposed circuit-to-graph conversion.
To begin with, we utilize one-hot encoding to represent various device types and their connections, and form a homogeneous graph.
There are nine distinct node types, which include: ground voltage net (0), power voltage net (1), other voltage nets (2), current source (3), NMOS (4), PMOS (5), resistor (6), capacitor (7), and inductor (8). 
The relationships among these devices are categorized into five edge types: current flow path (0), connections from voltage nets to NMOS gates (1) or bulks (2), and connections from voltage nets to PMOS gates (3) or bulks (4).
Notably, the edges from gate and bulk voltage nets to MOSFETs are directed, as these nets exert significantly greater influence on the device than the device does on the nets.
With these settings, our graph construction method makes full use of edge features without separating device pin nodes from their respective devices.


\subsection{Positive and Negative Data Augmentation} \label{sec:aug}
Training a general-purpose model requires constructing a comprehensive dataset. A straightforward approach to creating a large-scale circuit dataset is to combine various device parameter configurations~\cite{mehradfar2024aicircuit, dong2023cktgnn}. However, this approach may not fully capture the breadth of circuit knowledge.
Consider a circuit with twenty devices, each having three possible parameter values. This setup results in $3^{20}$ possible combinations, generating an enormous dataset from a single circuit. Yet, limiting each device to only three parameter values is unlikely to encompass the full complexity of circuit behavior, as useful parameters vary significantly across tasks and technologies.
Furthermore, capturing structural information necessitates incorporating diverse circuit topologies, further expanding the dataset exponentially with inefficiency.

Therefore, instead of considering device parameters, we use data augmentation to generate a sufficiently large pretraining dataset that effectively captures structural differences between circuits.
We introduce two data augmentation methods: positive and negative augmentation.
Positive augmentation generates new circuit topologies while preserving the properties of the original circuit.
In each iteration, a random device is selected, and an identical device is added either in parallel or in series as in \Cref{fig:dataaugmentation}.
Although the device parameters are not considered, this modification is equivalent to changing the parameters of the selected device.
This implies that the augmented graphs retain properties closely resembling those of the original graph.
Samples generated with positive augmentation can be augmented again to generate new positive and negative samples.
By maximizing the similarities between these positive samples, DICE captures the underlying semantics shared across different designs.

On the other hand, previous studies have found that the effectiveness of contrastive learning is closely related to the quality of negative samples~\cite{cao2022exploring, zhou2025novel}.
Therefore, we design another negative data augmentation technique to increase the diversity of samples.
Specifically, this process involves randomly selecting a device and modifying it according to the rules in \Cref{fig:dataaugmentation}.
The goal is to introduce significant changes in frequency response or DC voltage operation, and this modification results in a new topology that has distinct graph-level properties.
Samples generated with negative augmentation cannot be augmented again, since the relationship between the initial circuit and the augmented view from that negative sample cannot be defined, meaning that automatic labeling with self-supervision is impossible for this case.



\subsection{Graph Contrastive Pretraining} \label{sec:loss}

Once the dataset is prepared using data augmentation, graph-level contrastive learning is performed as illustrated in the upper right part of~\Cref{fig:overview}.
Circuits that are either initial designs or their positively augmented variants are designated as central circuits, serving as the main targets of contrastive learning.
Each central circuit has three types of relationships with other circuits in the batch: positive, negative, and unequal. A positive relationship indicates that a circuit shares similar high-level semantics with the central circuit, meaning it originates from the same initial design. A negative relationship arises when a circuit is generated through negative augmentation of the central circuit. Unequal relationships include circuits with different initial designs and their corresponding positive and negative augmentations.

After processing the batch of circuits through a GNN, graph-level features are extracted.
Our contrastive learning framework then maximizes cosine similarities between central circuit features and their positive sample features while minimizing similarities with their unequal sample features.


\textbf{Pretraining Model (DICE) Architecture.}
For graph-level feature extraction, we pretrain a graph isomorphism network (GIN) with depth 2, including the edge feature updates.
\begin{figure}[h]
% \vskip 0.2in
  \small
  \begin{equation}
    \label{eq:gnn0}
    \begin{split}
    \mathbf{h}_v^{(1)} &= MLP_{\theta_0}(\mathbf{h}_v^{(0)}), \quad \mathbf{e}_{u\rightarrow v}^{(1)} = MLP_{\theta_1}(\mathbf{e}_{u\rightarrow v}^{(0)})
    \end{split}
  \end{equation}
  \begin{equation}
    \label{eq:gnn1}
    \begin{split}
    \mathbf{m}_v^{(k)} &= \sum_{u\in \mathcal{N}(v)}\mathbf{h}_v^{(k)}\cdot \mathbf{e}_{u\rightarrow v}^{(k)}, \\
    \mathbf{h}_v^{(k+1)} &= MLP_{\theta_{2}^{(k)}}((1+\phi_{h}^{(k)})\cdot \mathbf{h}_v^{(k)} + \mathbf{m}_v^{(k)}), \\
    \mathbf{e}_{u\rightarrow v}^{(k+1)} &= MLP_{\theta_{3}^{(k)}}((1+\phi_{e}^{(k)})\cdot \mathbf{e}_{u\rightarrow v}^{(k)} + \mathbf{m}_u^{(k)} - \mathbf{m}_v^{(k)}) \\
    \end{split}
  \end{equation}
  \begin{equation}
    \label{eq:gnn2}
    \begin{split}
    \mathbf{g}_{(V,E)} &= \sum_{v\in V}\mathbf{h}_v^{(l+1)}+\sum_{u\rightarrow v \in E}\mathbf{e}_{u\rightarrow v}^{(l+1)}
    \end{split}
  \end{equation}
\vskip -0.2in
\end{figure}
Eqs.~\eqref{eq:gnn0}–\eqref{eq:gnn2} illustrates the node and edge feature update rules of DICE, where $\theta_0, \theta_1, \theta_2^{(k)}, \theta_3^{(k)}, \phi_h^{(k)}, \phi_e^{(k)}$ are the training parameters. Batch normalization and dropout layers are included between linear layers. Eq.~\eqref{eq:gnn0} matches the initial dimensions of node and edge features (9 and 5) to the hidden dimension value, and $\mathbf{h}_u^{(0)}$, $\mathbf{e}_{v\rightarrow u}^{(0)}$ are the initial one-hot encoding of node and edge types. Eq.~\eqref{eq:gnn1} explains the message passing rule, where $k=1,...,l$ and $l(=2)$ is the depth of the GNN. The graph-level feature $\mathbf{g}_{(V,E)}$ is calculated in Eq.~\eqref{eq:gnn2}, by summing all the updated node and edge features within each graph $G=(V,E)$.


\textbf{Loss Function.}
In each batch, a joint distribution of $(x, x^+)$ exists, representing the distribution of two circuits with a positive relationship.
Given an arbitrary central circuit $x_i$ in this distribution, let $X_i^+$ and $X_i^-$ denote the sets of positive and negative augmented views derived from the same initial design as $x_i$, respectively.
Also, let $X_i^{\neq}$ be the set of circuits unequal to $x_i$.
The union $X_i^+\cup X_i^-\cup X_i^{\neq}$ forms the complete set of circuits in the batch, ensuring no overlap between the sets.
The objective is to maximize cosine similarities between $x_i$ and $x_i^+\in X_i^+$, while minimizing similarities between $x_i$ and $x_i^{\neq}\in X_i^{\neq}$.
Minimization between $x_i$ and $x_i^-$ is excluded since they share similar topological structures, originating from the same initial circuit.
However, their high-level semantics differ, so their similarities are also not maximized.
Although the similarities between $x_i$ and circuits in $X_i^-$ are neither maximized nor minimized, when considering other central circuit $x_{j\neq i}$ with different initial design, circuits in $X_i^-$ are included in $X_j^{\neq}$. As a result, the similarities between these circuits and $x_j$ are minimized.


\begin{equation}
\label{eq:contrastive_learning_loss}
    L_{\theta}=
    \mathbb{E}\bigg[{-\log \frac{e^{{f_{\theta}(x,x^+)}/{\tau}}}
    {\underset{z\in X^{+}}{\sum} e^{{f_{\theta}(x,z)}/{\tau_p}}
    +\underset{z\in X^{\neq}}{\sum} e^{{f_{\theta}(x,z)}/{\tau_n}}}
    \bigg]}
\end{equation}

Building on this, we introduce a loss variant of Eq.\eqref{eq:simclr}, formulated in Eq.\eqref{eq:contrastive_learning_loss}.
The GNN feature extractor $f_{\theta}$, parameterized by $\theta$, computes the cosine similarity between the graph features of two input graphs using temperature coefficients $\tau$, $\tau_p$ and $\tau_n$.
The expectation is computed over the joint probability distribution of $(x, x^+)$, and $X^+,X^{\neq}$ denote the sets of positive and unequal circuits, respectively, for an arbitrary circuit $x$ in the distribution. Details on the algorithm and loss calculation are provided in Appendix~\ref{appendix:cl}.



\subsection{Application to Downstream Tasks} \label{sec:dwnstrm}


\begin{figure}[h]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.7\columnwidth]{figs/modelarch.pdf}}
\caption{\textbf{Model Architecture for Downstream Tasks.}}
\label{fig:fullmodelarch}
\end{center}
\vskip -0.2in
\end{figure}

After pretraining, we integrate DICE into the full message passing neural network (MPNN) model shown in \Cref{fig:fullmodelarch} to address diverse downstream tasks.
The encoder consists of three GNNs: two parallel networks that process the same graph input, followed by a series-connected network. One of the parallel GNNs is either initialized and frozen with pretrained DICE parameters, while the other parallel GNN and the series-connected GNN remain fully trainable. The encoder remains fixed for each downstream task if all GNN depth values are specified.
We define $d_D,d_p,d_s$ as the depths of the GNNs in DICE, the parallel GNN other than DICE, and the series-connected GNN, respectively.

The decoder network takes the output graph from the encoder as input and incorporates device parameters if required by the downstream task. To encode device parameters, we represent each device type using a 9-dimensional one-hot vector, scaled by its respective parameter values. Since parameter values vary significantly in magnitude across devices, we apply a negative logarithmic transformation before scaling. These encoded parameters are then concatenated with the node features from the encoder output and passed through the subsequent layers of the decoder.







\section{Experiments} \label{sec:exp}
We first outline the experimental setup and present pretraining results.
Next, we evaluate the model on three downstream tasks, each involving graph-level prediction.
Hyperparameters for all training are detailed in Appendix~\ref{appendix:hyperparameters}.

\subsection{Pretraining}
\textbf{Settings.} The dataset comprises 55 initial circuit topologies, including both analog and digital designs. For pretraining, 40 topologies are each augmented $\times4000$ ($\times2000$ positive and $\times2000$ negative), yielding a training dataset of 160,000 graphs.
Similarly, all 55 topologies are augmented $\times500$ for both validation and test sets, resulting in 27,500 graphs each.
Data augmentation is performed independently for generating the training, validation, and test datasets.


\textbf{Results.}
\Cref{tsne} presents the t-SNE plots of the graph embedding vectors. Each of the 55 different colors represents a unique initial circuit, and its augmented views are marked the same color as the corresponding initial circuit. \Cref{tsne}(c) shows significant improvements in clustering compared to \Cref{tsne}(a) and (b), providing evidence of successful pretraining.
To further validate these results, we computed the mean and standard deviation of cosine similarities.
Table~\ref{tab:cosine_similarity} presents the cosine similarities between samples within $X_+$ (positive pairs), between $X_+$ and $X_{\neq}$ (unequal pairs), and between $X_+$ and $X_-$ (negative pairs). The results confirm that similarities are minimized between unequally related samples while remaining high between positively related pairs.
A key observation is that cosine similarity is also minimized between negatively related samples, even without explicit minimization in the loss function.

\begin{figure}[h]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.7\columnwidth]{figs/TSNE.pdf}}
\caption{\textbf{t-SNE visualization} of graph-level features for 55 circuit topologies in the test dataset. Each plot represents:
(a) initial embeddings,
(b) embeddings processed by an untrained GNN, and
(c) embeddings processed by DICE.}
\label{tsne}
\end{center}
\vskip -0.2in
\end{figure}


\begin{table}[h]
\small
\centering
\caption{\textbf{Cosine Similarities of Graphs in the Test Dataset.} Means/standard deviations are computed with five seeds.}
\label{tab:cosine_similarity}
\vskip 0.15in

\begin{tabular}{cccc}
\hline\hline
\begin{tabular}[c]{@{}c@{}}Graph Embeddings\end{tabular}\
& \begin{tabular}[c]{@{}c@{}}Positive Pairs\end{tabular}
& \begin{tabular}[c]{@{}c@{}}Unequal Pairs \end{tabular}
& \begin{tabular}[c]{@{}c@{}}Negative Pairs \end{tabular}\\ \hline\hline
Initial         & \begin{tabular}[c]{@{}c@{}}$0.9983\pm0.0026$\end{tabular}
                & \begin{tabular}[c]{@{}c@{}}$0.9825\pm0.0200$\end{tabular}
                & \begin{tabular}[c]{@{}c@{}}$0.9825\pm0.0200$\end{tabular}          \\ \hline
untrained GNN   & \begin{tabular}[c]{@{}c@{}}$0.9977\pm0.0037$\end{tabular}
                & \begin{tabular}[c]{@{}c@{}}$0.9804\pm0.0205$\end{tabular}
                & \begin{tabular}[c]{@{}c@{}}$0.9825\pm0.0200$\end{tabular}          \\ \hline
\begin{tabular}[c]{@{}c@{}}
DICE \end{tabular}
                & \begin{tabular}[c]{@{}c@{}}$0.9406\pm0.1412$\end{tabular}
                & \begin{tabular}[c]{@{}c@{}}$\textbf{0.2349}\pm0.4594$\end{tabular}
                & \begin{tabular}[c]{@{}c@{}}$\textbf{0.1393}\pm0.2864$\end{tabular}           \\
\hline\hline
\end{tabular}
\vskip -0.1in
\end{table}




\begin{figure*}[t]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\textwidth]{figs/baseline_plots.pdf}}
\caption{\textbf{Performance of Baselines during Downstream Task Training.} MPNN w DICE is our model and ParaGraph represents the GNN model used in \cite{ren2020paragraph}. DeepGen w/o DC and DeepGen w DC indicates the GNN backbone used in \cite{hakhamaneshi2022pretraining}. We did not pretrain DeepGen w/o DC, and pretrained DeepGen w DC to predict the node voltage values on 6 different circuit topologies. In each plot, $\mu\pm\sigma$ range is shown for all models trained with 3 different seeds.}
\label{fig:baseline_plots}
\end{center}
\vskip -0.3in
\end{figure*}

\begin{table*}[t]
\small
\centering
\caption{\textbf{Comparison with the baseline models.} We train each model three times using different seeds and report the average performance.}
\label{tab:experiment1_result}
\begin{tabular}{ccccccccc}
\hline\hline
\multicolumn{1}{c|}{\multirow{2}{*}{Baselines}}                                                                    & \multicolumn{1}{c|}{Task 1 (\%)} & \multicolumn{2}{c|}{Task 2 ($R^2$)}          & \multicolumn{5}{c}{Task 3 ($R^2$)}                                                                                                \\ \cline{2-9} 
\multicolumn{1}{c|}{}                                                                                              & \multicolumn{1}{c|}{Accuracy}     & Rise Delay & \multicolumn{1}{c|}{Fall Delay} & Power  & \multicolumn{1}{l}{V$_{\text{offset}}$} & \multicolumn{1}{l}{CMRR} & \multicolumn{1}{l}{Gain} & \multicolumn{1}{l}{PSRR} \\ \hline\hline
\multicolumn{1}{c|}{\cite{ren2020paragraph}}                                                                       & \multicolumn{1}{c|}{91.42}        & 0.9255     & \multicolumn{1}{c|}{0.6373}     & 0.9312 & 0.9585                                  & 0.7912                   & 0.8488                   & 0.6215                   \\
\multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}\cite{hakhamaneshi2022pretraining}$_\text{unpretrained}$\end{tabular}} & \multicolumn{1}{c|}{85.02}        & 0.8866     & \multicolumn{1}{c|}{0.6715}     & 0.8403 & 0.8478                                  & 0.5637                   & 0.6813                   & 0.3820                   \\ 
\multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}\cite{hakhamaneshi2022pretraining}$_\text{pretrained}$\end{tabular}}    & \multicolumn{1}{c|}{84.65}        & 0.9131     & \multicolumn{1}{c|}{0.6875}     & 0.4479 & 0.5190                                  & 0.3246                   & 0.3454                   & 0.0851                   \\ \hline\hline
\multicolumn{1}{c|}{DICE$_{(2,0,2)}$}                                                                              & \multicolumn{1}{c|}{\textbf{93.94}}        & \textbf{0.9468}     & \multicolumn{1}{c|}{\textbf{0.8085}}     & \textbf{0.9916} & \textbf{0.9950}                                  & \textbf{0.9374}                   & \textbf{0.9769}                   & \textbf{0.8809}                   \\ \hline\hline
\end{tabular}
\end{table*}




\subsection{Finetuning on Downstream Tasks}
We evaluate the generalizability of DICE across three graph-level prediction tasks.
Detailed settings for each task are provided in Appendix~\ref{appendix:hyperparameters}.

\textbf{Task 1: Circuit Similarity Prediction.}
This task involves predicting the relative similarities between circuits, including both analog and digital topologies.
The objective is to maximize prediction accuracy based on true similarity, which is determined by the number of shared labels: analog, digital, delay lines, amplifiers, logic gates, and oscillators.

\textbf{Task 2: Delay Prediction.}
In Task 2, the model is trained to predict simulation results for five digital delay line circuits.
Each simulation records two values: the time difference between the rising edges and the falling edges of the input and output clock signals.
The objective is to maximize $R^2$ across all simulations.

\textbf{Task 3: OPAMP Metric Prediction.}
This task focuses on predicting simulation results for five analog operational amplifier (OPAMP) circuits.
Each simulation records five values: power, DC voltage offset (V$_{\text{offset}}$), common mode rejection ratio (CMRR), gain, and power supply rejection ratio (PSRR).
The goal is to maximize $R^2$ across all simulations.




\textbf{Comparison with baselines.}
We included three baseline models to demonstrate the superiority of DICE in device-level circuit representation learning. For a fair comparison, we selected previous works that (1) explicitly define graph construction using transistors, (2) employ GNNs, and (3) predict circuit simulation results~\cite{ren2020paragraph,hakhamaneshi2022pretraining}.
Similar to our approach, \cite{ren2020paragraph} represents both devices and voltage nets as graph nodes while distinguishing different connections using multiple edge types. However, unlike our homogeneous graph construction, they utilize a heterogeneous GNN with an attention mechanism and do not include power and ground voltage nets as nodes. \cite{hakhamaneshi2022pretraining} pretrains the GNN proposed in \cite{li2020deepergcn} using a node-level DC voltage prediction task, which is a supervised method. Additionally, their graph construction does not distinguish between edge types and separates device pins into individual nodes apart from their corresponding devices. From this work, we selected both pretrained and unpretrained GNNs as baseline models.
The GNN models from the selected works serve as the encoder for solving the three downstream tasks in our benchmark, while the decoders remain identical across all models including our MPNN model.
We compare these baselines with our model using the architecture in \Cref{fig:fullmodelarch}, which has a GNN depth configuration of $(d_D,d_p,d_s)=(2,0,2)$.

\textbf{Ablation study.}
We also conducted an ablation study to further demonstrate the effectiveness of DICE.
We built and compared two different models: one incorporating, and the other not incorporating DICE.
These models are designed to have the same number of training parameters, following the architecture in \Cref{fig:fullmodelarch}.
We evaluated results for six possible combinations of GNN depths $(d_p, d_s)=(0,0), (0,1), (1,0), (1,1), (0,2), (2,0)$.
$d_D$ is set to 0 when the model excludes DICE and to 2 when the model includes DICE. While the encoder is fixed across all downstream tasks, the decoder varies but remains consistent across equivalent tasks.








\begin{table*}[t]
\centering
\small
\caption{\textbf{Comparison on models with and without DICE.} DICE leads to significant performance gain.}
\label{tab:experiment2_result}
\begin{tabular}{cc|c|cc|ccccc}
\hline\hline
\multicolumn{2}{r|}
{\multirow{2}{*}
{($d_D,d_p,d_s$)}}      & Task 1 (\%) & \multicolumn{2}{c|}{Task 2 ($R^2$)} & \multicolumn{5}{c}{Task 3 ($R^2$)}        \\ \cline{3-10} 
\multicolumn{2}{r|}{}   & Accuracy & Rise Delay & Fall Delay & Power & V$_{offset}$ & CMRR & Gain & PSRR      \\ \hline\hline
\multirow{6}{*}
{\begin{tabular}[c]
{@{}c@{}}
MPNN \\ without DICE
\end{tabular}}          & (0, 0, 0) & 76.91 & 0.7541 & 0.5456 & 0.9651 & 0.8261 & 0.4794 & 0.3754 & 0.2798 \\
                        & (0, 0, 1) & 74.84 & 0.7706 & 0.5486 & 0.9718 & 0.8976 & 0.6384 & 0.6441 & 0.5024 \\
                        & (0, 1, 0) & 77.90 & 0.7666 & 0.5405 & 0.9636 & 0.8825 & 0.6019 & 0.6013 & 0.4501 \\
                        & (0, 1, 1) & 84.68 & 0.9404 & \textbf{0.8124} & 0.9827 & \textbf{0.9876} & 0.9125 & \textbf{0.9633} & \textbf{0.8064} \\
                        & (0, 0, 2) & 88.57 & 0.9229 & 0.7790 & 0.9745 & 0.9863 & 0.9091 & 0.9605 & 0.7929 \\
                        & (0, 2, 0) & 86.28 & 0.9237 & 0.7618 & 0.9866 & 0.9867 & 0.9066 & 0.9610 & 0.7900 \\ \hline\hline
\multirow{6}{*}
{\begin{tabular}[c]
{@{}c@{}}
MPNN \\ with DICE
\end{tabular}}          & (2, 0, 0) & \textbf{92.13} & \textbf{0.9352} & \textbf{0.7885} & \textbf{0.9769} & \textbf{0.9820} & \textbf{0.8865} & \textbf{0.9402} & \textbf{0.7535} \\
                        & (2, 0, 1) & \textbf{93.41} & \textbf{0.9310} & \textbf{0.7805} & \textbf{0.9904} & \textbf{0.9876} & \textbf{0.9167} & \textbf{0.9659} & \textbf{0.8132} \\
                        & (2, 1, 0) & \textbf{93.43} & \textbf{0.9294} & \textbf{0.7820} & \textbf{0.9849} & \textbf{0.9868} & \textbf{0.9147} & \textbf{0.9634} & \textbf{0.8061} \\
                        & (2, 1, 1) & \textbf{\underline{94.54}} & \textbf{\underline{0.9417}} & 0.7873          & \textbf{0.9847} & 0.9872 & \textbf{0.9147} & 0.9627 & 0.8061 \\
                        & (2, 0, 2) & \textbf{94.01} & \textbf{0.9231} & \textbf{\underline{0.8134}} & \textbf{\underline{0.9918}} & \textbf{\underline{0.9950}} & \textbf{\underline{0.9371}} & \textbf{\underline{0.9769}} & \textbf{\underline{0.8776}} \\
                        & (2, 2, 0) & \textbf{93.98} & \textbf{0.9402} & \textbf{0.7851} & \textbf{0.9876} & \textbf{0.9874} & \textbf{0.9165} & \textbf{0.9641} & \textbf{0.8055} \\ 
\hline\hline
\end{tabular}
\end{table*}






\textbf{Results.}
We first show the comparison between the baseline models and DICE on all three downstream tasks.
In ~\Cref{fig:baseline_plots}, we show the performance curves of all models on the validation dataset during training.
Furthermore, the detailed final performance comparison on each task is listed in Table.~\ref{tab:experiment1_result}.
To eliminate uncertainty, we trained each model with three different seeds and report the average performance values from all three models.
Results show that our model outperforms baselines in all downstream tasks.

The results of the ablation study are presented in Table~\ref{tab:experiment2_result}, which compares the performance of two MPNN models: one with and the other without DICE.
The number of training parameters is kept equal when the depths of the parallel and series GNNs $(d_p, d_s)$ are the same, serving as comparable configurations. A total of 36 models are trained and values in Table~\ref{tab:experiment2_result} are highlighted if the final performance surpasses that of the corresponding configuration.
Except for some cases in $(d_p, d_s)=(1,1)$, all highlighted values correspond to the MPNN model with DICE.
This confirms the effectiveness of our pretraining method in capturing the underlying knowledge of device-level circuits.
Additionally, the best final performance values, marked with underlines, are all achieved by the MPNN model with DICE.


\textbf{Discussions.}
We make three key observations.
First, training models with DICE is highly stable.
Unlike other baseline models in \Cref{fig:baseline_plots}, performance plots remain consistent across different seeds, demonstrating robust training stability.
Second, in some cases, DICE outperforms GNNs trained from scratch.
For example, in Table.~\ref{tab:experiment2_result}, models with $(d_D, d_p, d_s)=(2,0,0),(0,2,0),(0,0,2)$ have the same total number of parameters. However, $(2,0,0)$ does not include additional trainable GNN layers.
Despite this, it achieves better results on Task 2, suggesting that DICE contributes significantly to model performance.
Third, even unpretrained GNN models outperform baselines.
For instance, models with $(d_D,d_p, d_s)=(0,1,1),(0,2,0),(0,0,2)$ in Table~\ref{tab:experiment2_result} generally achieve higher performance than the baselines in Table~\ref{tab:experiment1_result}. This provides evidence that our graph construction method is effective compared to previous approaches.





\section{Conclusion} \label{sec:conclu}

In this work, we propose DICE, a pretrained GNN model designed for general tasks in device-level integrated circuits. Our two novel data augmentation techniques mitigate the scarcity of circuit data, while graph-level contrastive learning enables task-agnostic pretraining.
The major contribution of this work is the introduction of the first self-supervised graph representation learning framework for device-level circuits. We argue that device-level interpretation provides a more general and flexible way to represent integrated circuits compared to logic gate-level representations, as it accommodates both analog and digital circuits.
Additional contributions include a novel circuit-to-graph mapping technique and the introduction of a device-level circuit benchmark, where all three tasks require handling multiple circuit topologies simultaneously. Experimental results demonstrate that applying DICE leads to significant improvements across all three downstream tasks. Furthermore, compared to previous graph learning methods for device-level circuits, our approach outperforms all prior models across all performance metrics on the proposed benchmark.
We envision this work as a stepping stone toward developing powerful foundational models for device-level circuits.




\section{Limitations and Future Work} \label{sec:future}

Graph representation learning for AMS circuits is still in its early stages. In this work, we focused on the main contributions while omitting detailed comparisons of minor aspects of our framework. Nevertheless, we believe our work opens several promising research directions.

First, more comprehensive comparisons are required for graph construction methods. While our method empirically outperforms others across three downstream tasks, more sophisticated evaluations—such as graph isomorphism tests—could provide deeper insights into the optimal graph.

Second, exploring alternative machine learning techniques could further enhance performance. For example, graph transformers may better leverage global structural information. Other graph representation learning methods, such as graph autoencoders and node/edge masking techniques, also hold potential for future exploration.

Third, solving more complex EDA tasks would provide stronger validation of the pretrained model's effectiveness.
Solving large-scale circuit timing prediction, congestion prediction, and sizing devices for optimal circuit performance represent a valuable avenue for further research.





\bibliographystyle{unsrt}  
\bibliography{references}


\newpage
\appendix
\onecolumn


\section{Details of Negative Data Augmentation}
\label{appendix:da}

\begin{figure}[h]
% \vskip 0.2in
    \begin{center}
    \centerline{\includegraphics[width=0.835\textwidth]{figs/da_detail.pdf}}
    \caption{\textbf{Conversion of characteristics through negative data augmentation.}}
    \label{dadetail}
    \end{center}
\vskip -0.2in
\end{figure}

In this section, we detail the augmentation of negative data corresponding to positive instances. For passive elements, we consider their impedance at DC (zero frequency) and in the high-frequency limit, modifying them to exhibit inverse impedance characteristics at both extremes. For current sources, which supply energy, we replace them with passive elements, as the latter primarily dissipate energy. Finally, for transistors, we account for switching behavior based on gate voltage and introduce counterpart transistors in both parallel and series configurations to invert this behavior, ensuring the corresponding negative design exhibits the opposite functionality.



\newpage
\section{Details of Contrastive Learning}
\label{appendix:cl}

\begin{figure*}[h]
% \vskip 0.2in
    \centering
    \includegraphics[width=0.8\linewidth]{figs/CLdetail.pdf}
    \caption{\textbf{Masking operation.} $\odot$ indicates the hadamard product (element-wise multiplication) between matrices.}
    \label{fig:cldetail}
% \vskip -0.2in
\end{figure*}

Masking is our key source of rapid pretraining, with the cost of GPU memory. Algorithm.~\ref{alg:contrastive_learning} provides the pseudocode and Fig.~\ref{fig:cldetail} visualize the details of the the proposed masking for contrastive learning. For each batch, we first matched the number of graphs for every circuit types. For example, consider the case when the number of graphs originated from the inverter circuit is 5. If this is the minimum number among all other circuit types in the batch, then 5 circuits are sampled for every circuit types and form a new batch. Then, the cosine similarity matrix is formed with graph-level features in the batch with masks indicating positive and non-equal. Element-wise products between the similarity matrix and the masks filter out the unnecessary parts in parallel. Multiple positive pairs are considered simultaneously with this operation, thereby significantly reducing the training speed with the cost of GPU memory.

\DeclareRobustCommand{\rchi}{{\mathpalette\irchi\relax}}
\newcommand{\irchi}[2]{\raisebox{\depth}{$#1\chi$}} % inner command, used by \rchi
\begin{algorithm}[h]
\caption{Graph-level Contrastive Learning}\label{alg:contrastive_learning}
\centering
\begin{minipage}{\columnwidth}
% \begin{tabularx}{\columnwidth}
\textbf{Input:} dataset with augmentation ($\rchi$), epoch ($T$), temperature coefficients ($\tau_p$, $\tau$, $\tau_n$), learning rate ($\eta$) \\
\textbf{Output:} pretrained model parameter ($\theta_T$) \\
\textbf{Initialize:} model parameter $\theta_0$ \\
1:\ \textbf{for} $t=0,1,2,...,T-1$ \textbf{do}\\
2:\quad \textbf{for} batch $B$ in $\rchi$ \textbf{do} \\
3:\quad\quad       $m$                    $\gets$ min number covering all circuit types in $B$\\
4:\quad\quad       $B_m$                  $\gets$ batch$\in \mathbb{R}^L$ having $m$ graphs for all circuit types (maintaining circuit diversity for every batch)\\
5:\quad\quad       $g$                    $\gets$ $GNN_{\theta_t}(B_m)$: graph-level features $\in \mathbb{R}^{L\times h}$ \\
6:\quad\quad       $S$                    $\gets$ $gg^T$: cosine similarity matrix $\in \mathbb{R}^{L\times L}$\\
7:\quad\quad       $M_+,M_{\neq}$         $\gets$ Mask matrices $\in \mathbb{R}^{L\times L}$ for positive and non-equal pairs\\
9:\quad\quad       $L_{\theta_t}$         $\gets \sum (M_+\ast\big[S/\tau - \log(\sum M_{+}\ast e^{S/\tau_p} + \sum M_{\neq}\ast e^{S/\tau_n})\big])/\sum M_+$\\
10:\quad\quad\!\!\!$\theta_{t+1}$         $\gets \theta_t-\eta\nabla_{\theta_t}L_{\theta_t}$\\
11:\quad \textbf{end for} \\
12:\ \textbf{end for}
% \end{tabularx}
\end{minipage}
\end{algorithm}


\newpage
\section{Details of the Experiments}
\label{appendix:exp}
\subsection{Hyperparameters}
\label{appendix:hyperparameters}

\begin{table}[h]
\centering
\small
\begin{tabular}{cccccc}
\hline\hline
\begin{tabular}[c]{@{}c@{}}Training\\ parameters\end{tabular} & \begin{tabular}[c]{@{}c@{}}DICE\\ Pretraining\end{tabular} & \begin{tabular}[c]{@{}c@{}}\cite{hakhamaneshi2022pretraining}\\ Pretraining\end{tabular} & \begin{tabular}[c]{@{}c@{}}Downstream\\ Task 1\end{tabular} & \begin{tabular}[c]{@{}c@{}}Downstream\\ Task 2\end{tabular} & \begin{tabular}[c]{@{}c@{}}Downstream\\ Task 3\end{tabular} \\ \hline\hline
learning rate                                                 & 0.0003                                                     & 0.000005                                                         & 0.00001                                                     & 0.0001                                                      & 0.0001                                                      \\
batch size                                                    & 1024                                                       & 1024                                                             & 50                                                          & 2048                                                        & 1024                                                        \\
epochs                                                        & 200                                                        & 200                                                              & 20000                                                       & 300                                                         & 300                                                         \\
$(\tau_p,\tau, \tau_n)$                                       & (0.2, 0.05, 0.05)                                          & --                                                               & --                                                          & --                                                          & --                                                          \\ \hline\hline
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\small
\begin{tabular}{ccccc}
\hline\hline
Hyperparameters     & DICE & Encoder & Decoder & DeepGEN in \cite{hakhamaneshi2022pretraining} \\ \hline\hline
hidden dimension    & 256  & 512     & 256     & 512                       \\ 
dropout probability & 0.2  & --      & 0.3     & --                        \\ 
activation          & GELU & GELU    & GELU    & GELU                      \\ 
GNN type            & GIN  & GIN     & --      & GAT                       \\ 
GNN depth           & 2    & depends & 0       & 3                         \\ \hline\hline
\end{tabular}
\end{table}

\subsection{Settings of Downstream Tasks}
\label{appendix:dwnstrm_settings}

\textbf{Task 1: Circuit Similarity Prediction (Analog + Digital).}
The training dataset consists of 50 circuits, including the 40 topologies from the pretraining dataset. The validation and test datasets each contain 5 additional circuits, resulting in a total of 55 circuits.
Given a target circuit and two comparison circuits, the model outputs a three-dimensional probability distribution: whether the first circuit is more similar, the second circuit is more similar, or the two circuits are equally similar to the target circuit.
In each training step, one target circuit from the training dataset is sampled, and all permuted pairs across the 50 circuits form a tensor of size $(50\times49=2950,\ \ 3\times graph\ feature\ dimension)$ for the training model.
Based on the number of shared labels, the logits indicating similarity comparisons are computed. Using these logits and the model’s output, the cross-entropy loss is calculated and minimized.

\textbf{Task 2: Predicting Rise and Fall Delays on Five Delay Line Circuits (Digital).}
For simulation, we used NGSPICE with BSIM4 45nm technology, which are the open-source circuit simulator and the technology file. The delay line circuits for task 2 are not included in the pretraining dataset, and the simulation result dataset includes 45,000 device parameter combinations for each topology. The dataset is split into 8:1:1 ratio for train, validation, and test dataset. The output dimension of the decoder model is 2.

\textbf{Task 3. Predicting Five different Metrics on Five Operational Amplifier Circuits (Analog).}
NGSPICE with BSIM4 45nm technology file is also used for task 3. The OPAMP circuits for task 3 are included in the pretraining dataset, and the simulation result dataset consists of 60,000 device parameter combinations for each topology.
Circuits and testbench files are sourced from the work in~\cite{li2024analoggym},
and the train, validation, and test datasets are split into 8:1:1 ratio.
The decoder network used in Task 3 is identical to that in Task 2, except that the output dimension is set to 5.

\end{document}
