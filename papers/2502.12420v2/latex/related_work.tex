\section{Related Work}
Modeling merging \citep{merge_survey, mergeKit}, as a complementary approach to training-based methods, has the capability to integrate multiple task-specialized models into a unified one \citep{model_soup, zipit, re-basin, dare}, to improve model performance on individual tasks by merging checkpoints without requiring additional training \citep{ties_merging, task_arithmetic}, and to alleviates the issue of catastrophic forgetting \citep{mitigating_cf}.
According to whether the based models are in same architecture, the model merging methods can be divided into heterogeneous model merging and homogeneous model merging.

\paragraph{Heterogeneous Model Merging.}
A brunch of work \citep{DBLP:conf/eccv/AvrahamiLF22,DBLP:conf/icassp/NguyenNNPBH23} attempts to perform architecture transformation before merging, aiming to transform multiple models with different architectures into a unified one. However, these approaches often rely on the learning process to align the models, which can potentially degrade their performance.
Recent research in this direction often builds upon the concept of mode connectivity \citep{DBLP:conf/iclr/FreemanB17, DBLP:conf/icml/FrankleD0C20, DBLP:conf/nips/TatroCDMSL20}, which suggests the existence of a connected path between multiple local minima of models, along which the loss remains nearly constant. Furthermore, \citet{DBLP:conf/iclr/EntezariSSN22} revealed that models permuted to the same loss basin can be merged by averaging their weights. Following these intuitions, more recent works \citep{re-basin,repair,zipit} focus on permutation strategies to achieve better heterogeneous model merging.

\paragraph{Homogeneous Model Merging.}
Task-specific models initialized from the same pre-trained model can often be merged without considering permutation symmetry \citep{model_soup, task_arithmetic}. 
% The merging of fine-tuned models has demonstrated several advantages, including improved performance on a single target task \citep{izmailov_2018_averaging, gupta_2020_stochastic, choshen_2022_fusing, model_soup}, enhanced out-of-domain generalization \citep{cha_2021_swad, rame_2022_diverse, rame_2023_model}, the creation of multi-task models from distinct tasks \citep{li_2022_branch, task_arithmetic, ties_merging}, and support for continual learning \citep{yadav_2022_exclusive, yadav_2023_exploring, aim}.
One of the most straightforward approaches to model merging is to directly weighted average the parameters of base models \citep{shoemake85, model_soup}. However, the performance of simple average merging is often suboptimal, as task-specific features are typically not uniformly distributed.
Task Arithmetic \citep{task_arithmetic} enhances the merging process by introducing task vectors, suggesting that simple arithmetic operations on these vectors can effectively edit models and produce a merged model. Building on the concept of task vectors, both DARE \citep{dare} and Ties \citep{ties_merging} employ pruning-then-scaling methods to merge task vectors, based on the assumption that not all parameters contribute equally to the final performance. This also aligns with our perspective. 
% While DARE is primarily designed for merging models with minor parameter changes, WIDEN \citep{widen} aims at merging models with significant parameter shifts by disentangling weight components.

Another line of research on model merging leverages information derived from the activations of training data. For example, \citet{fisher} suggested a probability-space approach, which uses the Fisher information matrix to identify the importance of model parameters and proposed Fisher Merging. \citet{reg_mean} introduced RegMean, a data-less merging method that merges models in parameter space by solving a linear system constructed from data and model parameters. 
% \cite{aim} integrates the information from the activation space of LLMs into the merging process to improve performance and robustness.
We posit that activations play a crucial role in identifying the key parameters within task vectors relevant to downstream tasks. To this end, we propose a novel sensitivity-guided activation method to facilitate more effective merging of key features.
