\section{Conclusion and Future Work}\label{sec:conclusion}


In this paper, we identify the memory pressure problem commonly seen in modern Deep Neural Network training among state-of-the-art platforms. Then we review the well-known insight proposed by Chen et al., i.e. {\em trading computation for memory}, and build a dynamic programming algorithm accordingly. From there, we refine the objective function for a more precise estimation of GPU memory cost to align our result with the prominent state-of-the-art deep-learning platform, PyTorch. With extensive experiments, we show that a more precise model specification can decrease peak GPU memory usage even more. While that means an increase in the input size, it is not a problem for our algorithm because it runs in linear time.

In the future, we would like to investigate the mechanism of GPU memory allocation in modern deep-learning platforms to reduce those {\em preserved memory} reported by Nvidia hardware. On the other hand, since we can divide an arbitrary computation graph into many linear sub-models, it will be interesting to generalize our algorithm for a general DNN model, including those with thousands of layers, which is apparently not analyzable by a naive $O(n^3)$ algorithm.
