\section{Introduction}\label{sec:introduction}

\subsection{The Memory Pressure Problem in Deep Learning}

% The trend of model size in Deep Learning.

In Deep Learning, it has been a trend to design larger Deep Neural Networks (DNNs) for the execution of more complex tasks and better accuracy in the past few years~\cite{He2015DeepRL, 10.5555/3298023.3298188, Wu2016GooglesNM, 2016arXiv160507146Z, Zoph2017LearningTA, Simonyan2014VeryDC}. For example, the idea of the residual block behind ResNets~\cite{He2015DeepRL} makes it possible to train networks with over one thousand layers.
Among all kinds of models, Convolutional Neural Networks (CNN) have become the standard method for object detection and most other computer vision applications.
However, the memory allocated for the intermediate data of these convolution layers and the growth of the number of model parameters has become a problem~\cite{Wu2016HighperformanceSS, 8237506, Yu2022CoCaCC, Tan2019EfficientNetRM, Hu2017SqueezeandExcitationN}.
On the other hand, some research shows that a sufficiently large batch size is required for good and fast convergence~\cite{Pudipeddi2020TrainingLN}.
What makes it worse, there is a piece of evidence that the number of parameters in the state-of-the-art neural network has doubled around every 2.4 years~\cite{Goodfellow-et-al-2016}.

% Existing solutions and their limitations.

Many solutions have been proposed to solve the memory pressure problem~\cite{Huang2020SwapAdvisorPD, Rhu2016vDNNVD, Le2018TFLMSLM, Han2015DeepCC, Han2015LearningBW, Gupta2015DeepLW, 10.1145/2925426.2926294}.
%% with hardware requirement.
Offloading moves the activations during the forward phase from the memory of GPUs to the CPU memory,  and then prefetches the data back at the appropriate stage during the backward phase~\cite{Rhu2016vDNNVD}.
While manufacturers have designed high-end hardware, e.g. NVLink~\cite{Le2018TFLMSLM}, to support efficient communication between GPU and CPU, this approach may be too expensive for some researchers since NVLink is expensive.

Besides hardware-dependent solutions, there are approaches to reducing GPU memory usage by pruning unimportant connections~\cite{Han2015DeepCC, Han2015LearningBW}. We can also reduce the size of data by reducing the precision of floating point numbers. Some research shows that we can train DNN models using 16-bit wide fixed-point number with little or no degradation in classification accuracy~\cite{Gupta2015DeepLW}. More importantly, it enables larger models to fit within the given memory budget~\cite{10.1145/2925426.2926294}.

%% related methods: Trading computation for memory

In addition to hardware-dependent solutions, we have to use some techniques to keep up with the pace of the growing memory requirement.
The general methodology {\em trading computation for memory}~\cite{chen2016training} or {\em rematerialization}~\cite{Gruslys2016MemoryEfficientBT, kirisame2021dynamic, herrmann2019optimal} aims to save GPU memory by not storing the activations of certain layers during the forward phase
%delays the computation of activations of a subset of layers during the forward phase to save GPU memory
and recomputing them in batch during the backward phase.
The method eliminates the memory and hardware requirement for training large-scale DNN models without loss of accuracy at the cost of a moderate increase in time on recomputing activation data.
Since the idea of rematerialization and offloading are independent, another approach is to combine the two to reduce memory usage~\cite{Beaumont2021EfficientCO}.
On the other hand, for image recognition tasks, an improvement of the ResNet~\cite{He2015DeepRL} model has been proposed to reduce memory consumption by making the activations of most layers reconstructible from the activation of the next layer. This eliminates the need to store intermediate data during the forward phase~\cite{Gomez2017TheRR}.

Our work will focus on efficiently finding the optimal checkpoint subset to achieve the least peak memory usage during the model training. To the best of our knowledge, most existing works are built upon the {\em sublinear memory cost} method proposed by Chen et al.~\cite{chen2016training} since it is effective and easy to implement.
%But with our extensive experiments, there is a huge difference in the peak memory usage with and without the optimal checkpoint subset.
But with our extensive experiments, there is a huge difference in the peak memory usage with the checkpoint subset found by their method and by our method.
On the other hand, the {\em arbitrary computation graph} solver proposed by Feng et al.~\cite{feng2021optimal}  is designed to solve for the optimal checkpoint subset using the same objective function as the sublinear memory cost method. We will show that the objective function is not precise in describing the behavior of the state-of-the-art deep learning platform PyTorch and thus the algorithm does not give the optimal checkpoint subset. We will demonstrate the difference in the experiment section.

In this paper, we first describe the theoretical background of training a neural network using mathematical equations. We use these equations to identify all essential data required during both the forward and backward phases to compute the gradient of weights of the model. We summarize our analysis with a piece of pseudocode to represent the implementation of existing deep learning platforms for ease of identifying the memory pressure problem and for ease of comparison with the algorithm we will propose in the later sections.
Instead of the greedy approach proposed by Chen et al., which uses some heuristic to find good results in seconds, we formulate our first optimization problem as {\em checkpoint selection problem} based on Algorithm 2 and propose an $O(n^3)$ algorithm for finding the optimal checkpoint subset using dynamic programming.
Even more, by tracing the memory usage reported from PyTorch and comparing it with our simulated results, we revise Algorithm 2 into Algorithm~\ref{alg:train-early-free} using the theoretical analysis of model training we have mentioned at the beginning and denote the new problem as {\em dynamic checkpoint selection}. We formulate an accurate objective function based on the tracing and propose an $O(n)$-time algorithm for finding the optimal checkpoint subset using dynamic programming.
%Through extensive experiments, we conclude that Algorithm~\ref{alg:train-early-free} can precisely describe the behavior of the state-of-the-art deep learning platform PyTorch.
Through extensive experiments, we conclude that our algorithm can precisely describe the behavior of the state-of-the-art deep learning platform PyTorch.


\subsection{Analysis of Backward Propagation}

We describe the computation of a neural network model abstractly.
The model has $n$ weights $w_1, \ldots, w_n$, an input $x$, and the ground truth $G$.
The training for input $x$ consists of a {\em forward phase} and a {\em backward phase}, which will update the weights.
During an epoch, the training uses a set of different inputs to train the weights.
The training repeats the epoch until the weights converge.

The forward phase computes data $d_i$ with data $d_{i-1}$ and weight $w_i$ with a function $f_i$, for $i$ from $1$ to $n$. 
Note that we define $d_0$ to be the input $x$ to ease the notation in Equation~\ref{eq:di}. 

\begin{equation}
d_i = f_i(d_{i-1}, w_i) \label{eq:di}
\end{equation}

Then we start the {\em backward phase} that updates weights according to a loss function.
We compare the final output from Equation~\ref{eq:di} ($d_n$) with the ground truth $G$ corresponding to input $x$, and compute a loss function $L(d_n, G)$.
Now we compute the gradient $g_i$ of $w_i$ so that we can update it to minimize the loss function.
Since the loss is a function of the ground truth $G$ and $d_n$, and $d_n$ is a function of $d_{n-1}$ and $w_n$, and so on, we can compute the gradient $g_i$ of $w_i$ as a product in Equation~\ref{eq:chain} by the chain rule.

\begin{equation}
g_i = \frac{\partial d_i}{\partial w_i} (\prod_{j=i+1,n} \frac{\partial d_j}{\partial d_{j-1}}) \frac{\partial L}{\partial d_{n}} \label{eq:chain}
\end{equation}

Note that in practice we do not need to compute $g_i$ as the product in Equation~\ref{eq:chain}.
Instead, let $r(i)$ be a part of the product in Equation~\ref{eq:chain} then we have the following recursion.

\begin{eqnarray}
r_n & = & \frac{\partial d_n}{\partial d_{n-1}} \frac{\partial L}{\partial d_{n}}  \label{eq:rn} \\
r_i & = & \frac{\partial d_i}{\partial d_{i-1}} r_{i+1}, 1 \leq i \leq n - 1 \label{eq:ri}
\end{eqnarray}

Now we can rewrite Equation~\ref{eq:chain} as Equation~\ref{eq:gi}.

\begin{equation}
g_i = \frac{\partial d_i}{\partial w_i} r_{i+1} \label{eq:gi}
\end{equation}

We now compute the gradients and update the weights {\em backward}.
We first compute $g_n$ from Equation~\ref{eq:rn} and \ref{eq:gi}, and update weight $w_n$.
We then compute the rest of the gradient $g_i$ ($1 \leq i \leq n - 1$) from Equation~\ref{eq:ri} and \ref{eq:gi}, then update weight $w_i$ with gradient $g_i$.  
This is called {\em backward propagation} in the literature.
Algorithm~\ref{alg:train} gives the pseudocode for the training.

\begin{algorithm}
\begin{algorithmic}
\caption{Neural Network Training}
\label{alg:train}
%\Comment{Forward phase}
\Require Weights $w_1, \ldots, w_n$, input $x$ ($d_0$), and the ground truth $G$.
\Ensure Updated $w_1, \ldots, w_n$.\\
        \State {Allocate memory for $d_i$, $1 \leq i \leq n$}
\For{$i \gets 1$ to $n$}        \Comment{Forward Phase}         
        \State {Compute $d_i$ with $d_{i-1}$ and $w_i$ (Equation~\ref{eq:di})}
\EndFor
\\ 
\For{$i \gets n$ to $1$}     \Comment{Backward Phase}
    \State {Compute $r_{i}$ (Equation~\ref{eq:rn} or \ref{eq:ri})}
    \State {Compute $g_i$ (Equation~\ref{eq:gi})}
    \State{Update $w_i$ with $g_i$} \Comment{Update the weight}
\EndFor 
\end{algorithmic}
\end{algorithm}

Most of the current deep-learning platforms allocate memory for all data before the computation starts to ensure memory is available during the entire training.
For example, Algorithm~\ref{alg:train} will allocate memory for all $d_i$, $1 \leq i \leq n$, so that the backward phase can proceed without allocating any more memory.
However, this allocation causes severe and unnecessary memory pressure, since the backward phase proceeds in layers and does not need all $d_i$ simultaneously.

Chen et al.~\cite{chen2016training} proposed a {\em checkpoint} technique that reduces memory pressure.
The checkpoint technique computes all the data $d_i$ during the forward phase just as in Algorithm~\ref{alg:train}, but only keeps a subset (called checkpoints) in memory. 
During the backward phase, we can {\em recompute} the data $d_i$ between two checkpoints so that the backward phase can proceed.
For ease of explanation, we will denote these non-checkpoints between two checkpoints as a {\em segment}.
It is easy to see that once we complete the backward phase on a segment, we can free it and allocate memory for the next segment.
As a result, the maximum memory usage is the sum of the checkpoints, since they are always in memory during the backward phase, and the maximum amount of memory of a segment since only one of them will appear in memory at any given time.
That is, two data $d_i$'s from different segments will not be in memory simultaneously, and we can reduce the peak memory requirement.
The pseudocode of the checkpoint algorithm is in Algorithm~\ref{alg:train-checkpoint}.

\begin{algorithm}[h!tb]
\begin{algorithmic}
\caption{Neural Network Training with Checkpoint}
\label{alg:train-checkpoint}
%\Comment{Forward phase}
\Require Weights $w_1, \ldots, w_n$, input $x$ ($d_0$), and the ground truth $G$.
\Ensure Updated $w_1, \ldots, w_n$.\\
\For{$i \gets 1$ to $n$}        \Comment{Forward Phase}         
        \State {Allocate memory for $d_i$}
        \State {Compute $d_i$ with $d_{i-1}$ and $w_i$ (Equation~\ref{eq:di})}
        \If {$d_{i-1}$ is not a checkpoint}
            \State {Free the memory of $d_{i-1}$} 
        \EndIf
\EndFor
\For{$i \gets n$ to $1$}     \Comment{Backward Phase}
    \If {$d_i$ is a checkpoint}
        \State {Free the memory of the segment after $d_i$}
        \State {Let $d_h$ be the previous checkpoint, and allocate $d_{h+1}, \ldots, d_{i-1}$}
        \State {Recompute $d_{h+1}, \ldots, d_{i-1}$ (Equation~\ref{eq:di})}
    \EndIf
    \State {Compute $r_i$ (Equation \ref{eq:ri})}
    \State {Compute $g_i$ (Equation~\ref{eq:gi})}
    \State{Update $w_i$ with $g_i$} \Comment{Update the weight}
\EndFor 
\end{algorithmic}
\end{algorithm}

One immediate question one needs to answer is which subset from the data set $\{d_1, \ldots, d_n\}$ should be checkpoints.
There is a trade-off between the amount of memory for checkpoints and the maximum-sized segment, i.e. the segment with the maximum sum of data memory.
We can choose more checkpoints to reduce the memory of the maximum-sized segment, but the memory for checkpoints will increase.
On the contrary, choosing fewer checkpoints will increase the memory of the maximum-sized segment.
Therefore, it is essential to choose the correct checkpoints to balance the memory of the maximum-sized segment and the checkpoints.
