\begin{figure*}[t]
	\includegraphics[width=\linewidth]{CPCL.png}
	\caption{Illustration of the proposed cross prompting confident learning (CPCL) strategy for developing promptable segmentation foundation model based on both high quality and low quality annotations.}
	\label{CPCL}
\end{figure*}


\section{Methodology}

In this paper, we aim to develop a modality-specific foundation model for universal segmentation from PET images. An overview of our work is shown in \cref{Overview}.
Building upon the proposed large-scale PETS-5k dataset, we design a 3D promptable segmentation framework and introduce a cross prompting confident learning strategy to facilitate model training with different annotation quality.
The details are elaborated in the following sections.




\begin{algorithm}[t]
	\caption{The training procedure of SegAnyPET.}
	\label{Algorithm}
    \renewcommand{\algorithmicrequire}{\textbf{Input:}}
    \renewcommand{\algorithmicensure}{\textbf{Output:}}
	\begin{algorithmic}[1]
 		\REQUIRE{Training data $\mathcal{D}_{H}=\{X_{(i)},Y_{H(i)} \}_{i=1}^{M}$ with high-quality annotations, and $\mathcal{D}_{L}=\{X_{(i)},Y_{L(i)} \}_{i=1}^{N}$ with low-quality annotations}
		\ENSURE{SegAnyPET model parameters: Image Encoder $\mathcal{F}_{IE}$, Prompt Encoder $\mathcal{F}_{PE}$, Mask Decoder $\mathcal{F}_{D}$}
		\STATE{Initialize training details}
        \FOR{$t \Leftarrow 1$ \textbf{to} $t_{max}$}
            \STATE{Extract image embeddings $f_{img} = \mathcal{F}_{IE}(X)$}
            \STATE{Initialize $\hat Y_{0}$ as all zero matrix $\mathbf{O}$.}
            \FOR{$i \Leftarrow 1$ \textbf{to} $n_{pt}$}
                \STATE{Input prompt $p_{i}$ $\Leftarrow$ PromptGenerate($Y$, $\hat Y_{i-1}$)}
                \STATE{Update mask $\hat{Y}_{i} = \mathcal{F}_{D}(f_{img}, \mathcal{F}_{PE}(p_{i},\hat Y_{i-1}))$}
            \ENDFOR
        % \STATE{\color{teal}{\textbf{\# Cross Prompting Confident Learning.}}}
        \STATE{Average prediction $\bar Y = \frac{1}{n_{pt}}\sum\limits_{i = 1}^{n_{pt}} \hat Y_{i} $ }
        \STATE{Estimate uncertainty $U = - \sum p(\bar Y|X) \log p(\bar Y|X) $ }
        \IF{X in $\mathcal{D}_{H}$}
            \STATE{Supervised loss $\mathcal{L}^{H}_{seg} \Leftarrow (\hat{Y}_{i},Y)$ }
        \ELSIF{X in $\mathcal{D}_{L}$}
            \STATE{Label rectification $\tilde Y \Leftarrow (Y,U)$ }
            \STATE{Supervised loss $\mathcal{L}^{L}_{seg} \Leftarrow (\hat{Y}_{i},Y,Y_{rec})$ }
            \STATE{Regularization loss $\mathcal{L}_{cps} \Leftarrow (\bar Y, \{ \hat Y_{1},...,\hat Y_{n_{pt}} \})$ }
        \ENDIF
        \STATE{Total loss $\mathcal{L} = \mathcal{L}^{H}_{seg} + \lambda (\mathcal{L}_{cps} + \beta \mathcal{L}^{L}_{seg})$ }
        \STATE{Update model parameters ($\mathcal{F}_{IE},\mathcal{F}_{PE},\mathcal{F}_{D}) \Leftarrow \mathcal{L}$}
        \ENDFOR
		\RETURN model
	\end{algorithmic}
\end{algorithm}




\subsection{Network Architecture}

Motivated by the recent advance of segmentation foundation models for natural images \cite{SAM}, we reformulate a holistic 3D structure in SegAnyPET to capture the spatial information directly from volumetric images for universal segmentation.
Specifically, SegAnyPET consists of three main components, including an image encoder $\mathcal{F}_{IE}$, a prompt encoder $\mathcal{F}_{PE}$, and a mask decoder $\mathcal{F}_{D}$.
The image encoder aims to transform input images into discrete embeddings, and the prompt encoder converts input prompts into compact embeddings by combining fixed positional encoding and adaptable prompt-specific embeddings. After that, the mask decoder receives the extracted information from both the image encoder and the prompt encoder and incorporates prompt self-attention and cross-attention in two directions for prompt-to-image and image-to-prompt attention to update the image embeddings and prompt embeddings. The processed feature map is then up-sampled and passes through a multi-layer perception to generate the output segmentation masks. 
Details of the network components are shown in the Appendix.
Given an input image $X$ and spatial prompt $p$, the output segmentation mask $\hat Y$ can be formally expressed as:

\begin{equation}
\hat Y_{i} = \left\{
\begin{array}{lllll}
\mathbf{O}_{H \times W \times D}, & i=0 \\ \\
\mathcal{F}_{D}(\mathcal{F}_{IE}(X), \mathcal{F}_{PE}(p_{i},\hat Y_{i-1})), & i \ge 1 \\
\end{array}\right.
\end{equation} 
where $\mathbf{O}_{H \times W \times D}$ is an all-zero matrix as the pre-defined initial segmentation mask. In the training loop, new point prompts are generated from error regions based on the previous segmentation mask to simulate the manual interactive segmentation workflow.


\begin{table*}[t]
	\centering
    \normalsize
    \setlength\tabcolsep{10pt}
	\renewcommand\arraystretch{1.15}
	\begin{tabular}{c|c|ccccc|c}
		\hline 	\hline
		\multirow{2}{*}{Method}  &  \multirow{2}{*}{Prompt} & \multicolumn{6}{c}{Organ Segmentation DSC performance [\%]}  \\
\cline{3-8}  &&  Liver & Kidney-L & Kidney-R & Heart & Spleen & Avg  \\ \hline
SAM  \cite{SAM} & N points & 26.55 & 9.38 & 9.10 & 14.44 & 6.30 & 13.15 \\
MedSAM  \cite{MedSAM} & N points & 0.25 & 0.19 & 1.32 & 0.27 & 0.27 & 0.46 \\
SAM-Med3D \cite{SAM-Med3D} & 1 point & 51.63 & 21.01 & 19.17 & 60.11 & 25.41 & 35.46 \\
SAM-Med3D-organ & 1 point & 80.25 & 44.70 & 35.76 & 74.00 & 69.23 & 60.79 \\ 
SAM-Med3D-turbo & 1 point & 79.46 & 66.95 & 72.81 & 73.03 & 68.19 & 72.09 \\ \hline
\rowcolor{gray!25} \textbf{SegAnyPET}  & 1 point & 93.06 & 89.84 & 90.61 & 88.29 & 90.67 & 90.49 \\ \hline
SAM  \cite{SAM} & 3N points & 43.85 & 23.21 & 22.16 & 29.09 & 11.83 & 26.03  \\
MedSAM  \cite{MedSAM} & 3N points & 26.59 & 28.86 & 28.98 & 18.82 & 32.96 & 27.24 \\
SAM-Med3D \cite{SAM-Med3D} & 3 points & 62.15 & 28.21 & 31.19 & 61.44 & 27.07 & 42.01 \\
SAM-Med3D-organ & 3 points & 84.82 & 47.33 & 48.57 & 75.85 & 74.60 & 66.23 \\
SAM-Med3D-turbo & 3 points & 84.11 & 74.05 & 76.17 & 75.24 & 73.34 & 76.58 \\ \hline
\rowcolor{gray!25} \textbf{SegAnyPET}  & 3 points & 93.36 & 90.25 & 90.95 & 88.86 & 91.10 & 90.90 \\ \hline
SAM  \cite{SAM} & 5N points & 54.49 & 47.16 & 37.42 & 42.19 & 18.79 & 40.01 \\
MedSAM  \cite{MedSAM} & 5N points & 36.53 & 37.53 & 39.22 & 24.71 & 41.30 & 35.86 \\
SAM-Med3D \cite{SAM-Med3D} & 5 points & 61.05 & 31.05 & 31.98 & 61.88 & 29.75 & 43.14 \\
SAM-Med3D-organ & 5 points & 85.52 & 49.56 & 54.40 & 76.30 & 75.13 & 68.18 \\
SAM-Med3D-turbo & 5 points & 85.56 & 76.74 & 78.08 & 76.16 & 75.20 & 78.35 \\ \hline
\rowcolor{gray!25} \textbf{SegAnyPET}  & 5 points & 93.42 & 90.39 & 91.24 & 88.95 & 91.22 & 91.05 \\ \hline \hline
SegResNet \cite{segresnet} & Auto & 92.28 & 85.56 & 82.94 & 88.21 & 86.55 & 87.11 \\
SwinUNETR \cite{hatamizadeh2021swin} & Auto & 93.03 & 89.23 & 83.62 & 89.03 & 87.93 & 88.57 \\
nnUNet \cite{isensee2020nnunet}  & Auto & 93.16 & 90.12 & 86.60 & 90.96 & 88.32 & 89.83 \\
STUNet \cite{huang2023stunet} & Auto & 93.15 & 90.11 & 85.60 & 90.38 & 88.83 & 89.61 \\
\hline  \hline
	\end{tabular}
 	\caption{Comparison of DSC performance [\%] with state-of-the-art segmentation foundation models for \textbf{\underline{zero-shot interactive}} segmentation  and task-specific models for \textbf{\underline{training-based automatic}} segmentation from PET images. N denotes the count of slices containing the target object (N ranges from 20 to 50 in our task).} \label{Table_Seen}
\end{table*}



\subsection{Cross Prompting Confident Learning}

Due to the dependency of domain knowledge for annotating medical images, acquiring large amounts of high-quality labeled data is infeasible.
A prevalent approach to mitigating this issue is collecting additional labeled data with varying annotation qualities like crowdsourcing from non-experts or using model-generated labels.
Given that some annotations may be fraught with noise or inaccuracies, which could impede the learning process, it is imperative to devise noise-robust learning strategies to efficiently learn from both expert-examined high-quality annotations (termed as HQ Set) and low-quality annotations with possible mistakes (termed as LQ Set).
To issue the challenge of discrepant annotation quality, we adopt a cross prompting confident learning (CPCL) strategy with a cross prompting consistency similarity regularization and an uncertainty-guided self-rectification process.
\Cref{CPCL} depicts the workflow of CPCL. The utilization of LQ set can be mainly divided into the following two parts.

\textbf{Exploit Knowledge Inside the Images.}
Given that LQ labels could be the encumbrance for model training, it is natural to adopt the semi-supervised learning setting that casts LQ labels away and exploits the knowledge inside the image data alone.
For promptable segmentation settings, the output heavily relies on the input prompts, and different prompts may cause variances in the segmentation results even when they refer to the same object given the same image. 
This phenomenon underscores the imperative to bolster the invariance of segmentation outputs across different prompts, which could be utilized as unsupervised regularization for training.
Based on this motivation, we harness the capabilities inherent to the prompting mechanism and develop a cross prompting consistency strategy to learn from unlabeled data.
Within each training loop, we estimate the unsupervised regularization loss as follows:

\begin{equation}
\bar Y = \frac{1}{n_{pt}}\sum\limits_{i = 1}^{n_{pt}} \hat Y_{i} 
\end{equation}
\begin{equation}
\mathcal{L}_{cps} = \sum_{i=1}^{n_{pt}} \| \bar Y - \hat Y_{i} \| ^{2}
\label{cps}
\end{equation}
where $n_{pt}$ represents the setting of prompt point number in each training loop and $\bar Y$ represents the averaged segmentation result in the loop.



\textbf{Self-Rectification Towards Effective Guidance.}
In addition to image-based information, effectively leveraging concomitant noisy labels inherent in LQ set is crucial to further improve the performance. To this end, we further propose an uncertainty-based self-rectification process to alleviate the potential misleading caused by label noises.
Uncertainty estimation refers to the process of predicting the uncertainty or confidence of model predictions by assigning a probability value that represents the self-confidence associated with the prediction, so as to quantify the reliability of the output and to identify when the network may not be performing well.
For image segmentation, the model assigns a probability value that represents the self-confidence associated with each pixel's prediction, and lower confidence (\textit{i.e.} higher uncertainty) can be a heuristic likelihood of being mislabeled pixels.

Building upon the foundation of promptable segmentation, instead of adding perturbations to input images or models, we leverage the advantage of the prompting mechanism and approximate the segmentation uncertainty with different input prompts in the training loop. Following \cite{zhang2023uncertainty}, we select the predictive entropy to approximate the uncertainty since it has a fixed range. The aleatoric uncertainty of a given image is formulated as follows:

\begin{equation}
U = - \sum p(\bar Y|X) \log p(\bar Y|X)
\label{unc}
\end{equation}

To exploit informative knowledge from LQ labels, we perform an uncertainty-based self-rectification process for noisy label refinement.
With the guidance of estimated uncertainty, we filter out relatively unreliable regions with higher uncertainty as potential error maps. Then the rectification operation is formulated as:
\begin{equation}
\tilde y_{i} = y_{i} + \mathbb I(u_{i}>H) \cdot (-1)^{y_{i}}
\label{rec}
\end{equation}
where $\mathbb I(.)$ is the indicator function, $y_{i}$ and $u_{i}$ are the initial annotation and estimated uncertainty at i-th voxel. Then the rectified label of the whole volume $\tilde Y$ is $\{\tilde y\} \in \{0,1\}_{H \times W \times D}$.
For LQ set, we utilize rectified label for supervised training instead of the original noisy label.
With the help of the label rectification process, the negative effects brought by label noises can be eliminated to some extent and provide more rewarding guidance.







\subsection{Overall Training Procedure}

\Cref{Algorithm} presents the overall training procedure of SegAnyPET. The total loss for model training is weighted combination of the supervised loss $\mathcal{L}^{H}_{seg}$ on HQ set, the unsupervised regularization loss $\mathcal{L}_{cps}$ on LQ set, and the rectified supervised loss $\mathcal{L}^{L}_{seg}$ on LQ set, calculated by:

\begin{equation}
\mathcal{L} = \mathcal{L}^{H}_{seg} + \lambda (\mathcal{L}_{cps} + \beta \mathcal{L}^{L}_{seg})
\end{equation}
where $\beta$ is a fixed weighting coefficient, and $\lambda$ is a ramp-up trade-off weighting coefficient to avoid domination by misleading targets at the early training stage.