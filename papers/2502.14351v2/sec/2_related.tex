\section{Related Work}
\label{sec:related}

\textbf{Vision Foundation Models.}
Foundation models represent a rapidly expanding field in artificial intelligence research, focusing on developing large-scale, general-purpose models with capabilities applicable across various domains and applications. Pioneering vision foundation models have been primarily based on pre-training methods such as CLIP \cite{CLIP} and ALIGN \cite{ALIGN} leverage contrastive learning techniques to train both text and image encoders. However, these models primarily excel in tasks that involve mapping images to text, such as classification.
For foundation model of image segmentation, the Segment Anything Model (SAM) \cite{SAM} represents a new paradigm of image segmentation with universal segmentation ability across different tasks and demonstrates strong potential in addressing a wide range of downstream tasks like medical image analysis \cite{zhang2024challenges,moor2023foundation}.

\textbf{Foundation Models for Medical Image Segmentation.}
As a crucial branch of image segmentation, recent studies have explored the application of SAM to the segmentation tasks from multi-modal multi-target biomedical images \cite{SAM-Empirical,SAM-SZU,ma2024segment}, while some research also focuses on fine-tuning SAM \cite{MedSAM} or adapting SAM-like foundation models \cite{SAM-Med3D,du2023segvol,wong2024scribbleprompt} on large-scale medical datasets. These studies indicate that SAM has limited generalization ability when directly applied to medical image segmentation due to the significant distribution gap, and fine-tuning SAM on medical datasets can improve the unsatisfactory performance \cite{SAM4MIS}.
In addition to the adaptation of model architectures, another line of research aims to extend the original promptable interactive segmentation method into end-to-end fully automatic segmentation by auto-prompting \cite{MedLSAM, UR-SAM} or synergy in training supervised segmentation models \cite{zhang2024semisam,li2023segment}.
However, due to the scarcity of public datasets, existing adaptions mostly focus on structural medical images like CT and MRI, ignoring molecular PET images. 


\textbf{Annotation-Efficient Learning.}
Despite the outstanding performance in many medical image segmentation tasks \cite{isensee2020nnunet,wasserthal2023totalsegmentator}, the success of existing deep neural networks heavily relies on the massive radiologist-examined labeled data.
Obtaining pixel-wise annotation is time-consuming and expertise-demanding, which motivates annotation-efficient learning strategies to train deep models with limited annotations \cite{cheplygina2019not,jiao2023learning,shi2024beyond}.
Despite recent efforts of large-scale medical datasets \cite{ye2023sa,bai2024m3d}, these datasets are the integration of existing public datasets with different annotation quality. While some labels may be noisy with mistakes and could potentially hinder the learning procedure, it is important to develop noise-robust learning strategies. 