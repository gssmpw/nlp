\section{Introduction}
\label{sec:intro}

\begin{figure}[t]
	\includegraphics[width=\linewidth]{Visualize.png}
	\caption{Visual comparison of an example case between molecular PET image and structural CT image.}
	\label{Visualize}
\end{figure}

%-------------------------------------------------------------------------
Medical imaging technology has become an indispensable tool in modern clinical diagnosis \cite{kasban2015comparative,Lynch2018NewMT}. The steady development of increasingly sophisticated imaging techniques has led to a dramatic increase in patient treatment outcomes, especially when diseases can only be observed internally within the patient's body \cite{MIA2017survey}. 
Segmentation of targets such as organs and tumors from medical images is one of the most representative and comprehensive research topics in both the computer vision and medical image analysis communities \cite{hesamian2019deep}. 
Accurate segmentation can provide reliable volumetric and shape information on target structures and assist in many clinical applications \cite{lalande2021deep,AbdomenCT-1K,zhang2024nasalseg}.

In addition to commonly used medical modalities such as computed tomography (CT) and magnetic resonance imaging (MRI), which can provide detailed physiological structural information, positron emission tomography (PET) has also been widely applied in medical examinations as an emerging molecular imaging modality \cite{schwenck2023advances}.
The purpose of PET imaging is to reveal the ongoing metabolic processes within the body of a patient. A radioactive tracer is injected into the patient, typically containing the radioactive isotope fluorine-18 synthesized fluorodeoxyglucose ($^{18}$F-FDG). 
The radioactive decay emits positrons, leading to gamma photons, which are produced by annihilation and captured by detectors surrounding the patient to create a 3D map of metabolic activity.
In FDG-PET examinations, the tracer is used to assess local glucose uptake and evaluate organ metabolism \cite{ren2019atlas,wang2025robust} and presence of tumor metastasis \cite{AutoPET}, so as to monitor progress during the treatment process.
However, due to the low resolution nature and the partial volume effects, PET images exhibit lower image quality and have less distinct boundaries compared to other structural images such as CT scans (see \cref{Visualize}), which presents a significant challenge for accurate segmentation.




\begin{figure*}[t]
	\includegraphics[width=\linewidth]{Overview.png}
	\caption{An overview of our work for PET segmentation foundation model. Firstly, we collect and construct PETS-5k, a large-scale PET segmentation dataset for developing the foundation model. Based on the principle of promptable segmentation, we develop SegAnyPET, a modality-specific 3D segmentation foundation model that can be efficiently adapted for universal segmentation of any target organs or lesions based on positional prompting from 3D PET images.}
	\label{Overview}
\end{figure*}



With the unprecedented developments in deep learning, deep neural networks have been widely applied and achieved great success in medical image segmentation \cite{ronneberger2015u,isensee2020nnunet}.
However, existing deep models are often tailored for specific tasks, which require task-specific training data for model development and lack the capacity for generalization across different tasks. 
The emergence of pre-trained foundation models has sparked a new era due to their remarkable generalization abilities across a wide range of downstream tasks \cite{moor2023foundation,willemink2022toward}.
For image segmentation, the introduction of the Segment Anything Model (SAM) \cite{SAM} has gained massive attention as a promptable foundation model capable of generating fine-grade segmentation masks to unseen targets using positional prompts like points or bounding boxes.
However, the original SAM is trained on natural images characterized by strong edge information, which differs significantly from medical images, especially PET images that exhibit low contrast and weak boundaries. When directly applying SAM without any adaptation, its performance significantly left behind task-specific segmentation models \cite{SAM4MIS,SAM-Empirical}.
Despite the attempts of fine-tuning and adapting SAM for medical images \cite{MedSAM,3DSAM-adapter}, these works mostly focus on CT and MRI images with detailed physiological structural information, ignoring the PET imaging. 
A significant obstacle lies in the scarcity of large-scale annotated PET datasets, which is attributable to the high costs associated with acquisition and annotation.
%Specifically, the cost of acquiring PET images is considerably higher than that of CT images.
Existing PET datasets \cite{AutoPET,hecktor} are relatively small and only focus on the segmentation of limited targets. 
Besides, the low image quality and the presence of partial volume effects complicate the annotation of segmentation targets from PET images, potentially leading to inconsistent annotation quality.



To overcome this challenge, we collect and construct a large-scale PET segmentation dataset named \textbf{PETS-5k}. The dataset consists of 5,731 three-dimensional whole-body PET images, encompassing over 1.3M 2D images, which is the largest PET dataset to date. Based on the empirical analysis, we found that existing foundation models show poor generalization performance on PET images.
To enable universal promptable segmentation from PET images, we propose \textbf{SegAnyPET}, a modality-specific foundation model that can be efficiently and robustly adapted to segment anything from PET images.
Other than the original 2D architecture of SAM, we reformulate a 3D architecture to fully utilize the inter-slice context information of 3D PET images.
To issue the challenge of discrepant annotation quality, we adopt a cross prompting confident learning (CPCL) strategy with an uncertainty-guided self-rectification process to efficiently learn from dataset with varying annotation quality for model training. The strategy does not require network modification and can be easily adapted to other promptable foundation models.
Extensive experiments demonstrate that SegAnyPET can correctly segment both seen and unseen targets using only one or few prompt points and outperform state-of-the-art foundation models and task-specific fully supervised models by a large margin, underpinning its general-purpose segmentation ability.
Our contribution can be summarized as follows:


\begin{itemize}
\item The up-to-date largest 3D PET segmentation dataset \textbf{PETS-5k}, with 5,731 three-dimensional PET images, encompassing over 1.3M 2D images, that are significantly larger than existing datasets.
\item A modality-specific foundational model \textbf{SegAnyPET} for universal promptable segmentation from PET images, demonstrating superior performance compared with state-of-the-art methods and strong generalization ability.
\item A cross prompting confident learning (CPCL) strategy for noisy-robust learning to efficiently learn from high-quality labeled data and low-quality noisy labeled data for model training.
\end{itemize}