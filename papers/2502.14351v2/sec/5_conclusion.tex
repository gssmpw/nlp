\section{Discussion and Conclusion}
In this work, we propose SegAnyPET, a modality-specific foundation model for universal promptable segmentation from 3D PET images.
To this end, we collect and construct a large-scale PET segmentation dataset, PETS-5k, which consists of 5,731 three-dimensional whole-body PET images, encompassing over 1.3M 2D images to train the foundation model.
To facilitate the model training with different annotation quality, we design a cross prompting confident learning strategy building on the foundation of promptable segmentation for noise-robust learning.
Through comprehensive evaluations on both internal and external datasets,
our model shows substantial capabilities in segmenting both seen and unseen targets using only one or few prompt points and robust generalization abilities to manage new data and tasks.
Its performance significantly exceeds both state-of-the-art segmentation foundation models and task-specific fully supervised models.

\begin{table}[h]
    \centering
    \normalsize
    \setlength\tabcolsep{9pt}
	\renewcommand\arraystretch{1.1}
	\begin{tabular}{c|c|c|c}
		\hline 	\hline
		Strategy  & Prompts & Seen & Unseen \\ \hline
\multirow{3}{*}{Fine-Tuning}  & 1 point & 87.61 & 62.74  \\
& 3 points & 88.02 & 63.65 \\
& 5 points & 88.13 & 63.94 \\ \hline 
\multirow{3}{*}{Consistency} & 1 point & 89.28 & 67.52 \\
& 3 points & 89.46 & 70.46 \\
& 5 points & 89.49 & 70.97 \\ \hline 
\multirow{3}{*}{CPCL} & 1 point & 90.49 & 73.96   \\ 
& 3 points & 90.90 & 77.09 \\
& 5 points & 91.05 & 77.87 \\  \hline \hline
	\end{tabular}
	\caption{Ablation analysis of different training strategies for zero-shot interactive segmentation from PET images. } \label{Table_Ablation}
\end{table}

While SegAnyPET boasts strong capabilities, there are still inherent limitations due to manual efforts for promptable segmentation, which could potentially reduce the usability of the model compared with fully automatic segmentation. 
We aim to incorporate semantic information together with positional prompts as additional support to enable efficient and precise auto-prompting for automatic segmentation in the future.
Besides, despite the differences between existing structural medical images, these large-scale medical datasets \cite{bai2024m3d} can still provide useful guidance for training in conjunction with PET images.
In conclusion, as the first foundation model for PET images, we believe that SegAnyPET will advance the developments of universal segmentation and serve as a powerful pre-trained model to facilitate the applications to various downstream tasks for molecular imaging.



