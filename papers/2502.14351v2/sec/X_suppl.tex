\clearpage
\setcounter{page}{1}
\maketitlesupplementary

\begin{table*}[t]
	\centering
    \normalsize
    \setlength\tabcolsep{7pt}
	\renewcommand\arraystretch{1.3}
	\begin{tabular}{c|ccc|cc}
		\hline
Dataset & Split   & Annotation Targets & Scans & New Data & New Label \\ \hline
 & Train LQ &  5 target organs & 5,631 & $\checkmark$ & $\checkmark$ \\ 
PETS-5k & Train HQ &  5 target organs & 40 & $\checkmark$ & $\checkmark$\\ 
 & Internal Test & all 12 organs & 60 & $\checkmark$ & $\checkmark$\\ \hline
AutoPET & External Test & tumor lesion & 1,014 \\ \hline
AutoPET-Organ & External Test & all 12 organs & 100 &&$\checkmark$ \\ \hline 
\end{tabular}
 \caption{Information summary of datasets involved in the construction and evaluation of SegAnyPET.} \label{Table_Dataset}
\end{table*}




\section*{A. Dataset Information}
\label{app_data}

In this work, our experiments are conducted on one private dataset (PETS-5k) and one public dataset (AutoPET) consisting of 3D whole-body $^{18}$F-fluorodeoxyglucose positron emission tomography ($^{18}$F-FDG-PET) images, which is the most widely used PET tracer in oncology.
As a non-specific tracer, $^{18}$F-FDG can be used for whole-body imaging to reflect tissue glucose metabolism, which makes the imaging useful in assessing the systemic distribution and metastasis of tumors.
Organ segmentation from $^{18}$F-FDG-PET images can be used to evaluate differences in the maximum standardized uptake values (SUVmax) of different organs, thereby assisting in the diagnosis of malignant tumors.





\subsection*{A.1. PETS-5k Dataset}

The proposed PETS-5k dataset consists of 5,731 three-dimensional whole-body $^{18}$F-FDG PET images collected from one local medical center. 
Patients were fasted for at least 6h and had a blood glucose level $<$ 200 mg/dL before the PET/CT examination. PET/CT imaging was performed at a median uptake time of 67 min (range from 53 to 81 min) after intra-venous injection of $^{18}$F-FDG (3.7 MBq/kg).
All data were acquired on PET/CT scanners (Siemens Biograph mCT) with 5 min per bed position. A low-dose CT scan (120 kVp; 40â€“100 mAs; 5 mm slice thickness) was performed from the upper thigh to the skull base, followed by a PET scan with 3D Flowmotion acquisition mode. PET images were reconstructed with 4.07 $\times$ 4.07 $\times$ 3 $mm^{3}$ voxels using CT-based attenuation correction by Siemens-specific TrueX algorithm.


\subsection*{A.2. AutoPET Dataset}

The public AutoPET dataset consists of 1,014 three-dimensional whole-body $^{18}$F-FDG PET images.
All data were acquired using cutting-edge PET/CT scanners, including the Siemens Biograph mCT, mCT Flow, and Biograph 64, as well as the GE Discovery 690. These scans were conducted following standardized protocols in alignment with international guidelines. The dataset encompasses whole-body examinations, typically ranging from the skull base to the mid-thigh level. More details can be found in the original paper \cite{AutoPET}.


\subsection*{A.3. Organ Selection and Annotation}

Due to the characteristics of molecular imaging, some target anatomical structures in segmentation tasks structural images like CT and MRI may not be apparent in PET images. As a result, we select out five most clinical-important target organs for training, including liver, left kidney, right kidney, heart, and spleen. 
To evaluate the model performance on training invisible organs, we further annotate seven organs in the internal test set including aorta, prostate, left lung lower lobe, right lung lower lobe, left lung upper lobe, right lung upper lobe, and right lung middle lobe.
These additional organs are not used for model training and only used as test set to evaluate of the generalization performance of SegAnyPET for universal segmentation of unseen targets.

For PETS-5k dataset, all the images are preliminary annotated by developed state-of-the-art segmentation model and one junior annotator using LIFEx v7.6.0 \cite{nioche2018lifex}. Among the dataset, 100 cases are then checked and refined by two senior experts, which serve as the HQ training set and test set, while the remaining cases serve as the LQ set in our task.
For AutoPET dataset, the original task is only focused on tumor lesion segmentation. In addition to the original tumor annotation, we select out and annotate a small subset of 100 cases to annotate all the 12 target organs, named AutoPET-Organ. The AutoPET-Organ is used as an external test set to evaluate the generalization performance of SegAnyPET on training invisible dataset.

\subsection*{A.4. Summary and Visualization}

An overview information summary and visualization of the datasets used in our work are shown in \cref{Table_Dataset} and \cref{data_vis}.


\begin{figure*}[t]
    \centering
	\includegraphics[width=0.98\linewidth]{Dataset.png}
	\caption{Visualization of PET images and corresponding organ annotations of PETS-5k dataset and AutoPET-Organ dataset.}
    \label{data_vis}
\end{figure*}





\begin{figure*}[t]
    \centering
	\includegraphics[width=\linewidth]{structure.png}
	\caption{The detailed architecture of the network components of SegAnyPET.}
    \label{structure}
\end{figure*}




\section*{B. Methodological Details}

\subsection*{B.1. SegAnyPET Architecture}
\label{app_architecture}

The detailed architecture of SegAnyPET is shown in \cref{structure}.
Following the design in \cite{SAM-Med3D}, for the image encoder, the input patch size is set to 16$\times$16$\times$16 with a patch embedding dimension of 768, paired with a learnable 3D absolute positional encoding.
Then the embeddings of patches are input to 3D self-attention blocks. The depth of self-attention blocks is set to 16.
Within the prompt encoder, sparse prompts are leverage by 3D position embedding to represent 3D spatial differences, while dense prompts are handled with 3D convolutions followed by layer normalization and GELU activation.
The mask decoder is integrated with 3D upscaling procedures, employing 3D transformer blocks and 3D transposed convolutions to get the final segmentation result.


\subsection*{B.2. Implementation Training Details}


Our method is implemented in Python with PyTorch and trained on 4 NVIDIA Tesla A100 GPUs, each with 80GB memory. 
We use the AdamW optimizer with an initial learning rate of 0.0008 and a weight decay factor of 0.1.
The training was performed for a total of 200 epochs on the constructed PETS-5k dataset.
The batch size is set to 12 with a volumetric input patch size of 128$\times$128$\times$128. 
To handle the learning rate schedule, we employed the MultiStepLR scheduler, which adjusts the learning rate in predefined steps with 120 and 180 epochs, with a gamma value of 0.1, indicating that the learning rate is reduced by 10\% of its original value at each step.
In distributed training scenarios, we utilized gradient accumulation with 20 steps to simulate larger effective batch sizes, which can improve model performance by providing a more accurate estimate of the gradient.
% \textbf{For manual prompting interactive segmentation task, the input prompts are simulated based on the ground truth.}
For total loss in the training loop, the ramp-up trade-off weighting coefficient $\lambda$ is scheduled by the time-dependent Gaussian function as $ \lambda=\omega_{max}*e^{-5(1-t/t_{max})}$, where $t_{max}$ is the maximum training iteration, $\omega_{max}$ is the maximum weight set as 0.1 and $\beta$ is set to 5.
The weighting coefficient can avoid the domination by misleading targets at the early training stage.

\subsection*{B.3. 2D/3D Prompt Generation Strategy}

As stated in the article, the input manual prompts are simulated based on the ground-truth mask for interactive segmentation.
Since the original SAM \cite{SAM} and MedSAM \cite{MedSAM} are designed for 2D segmentation tasks and cannot handle 3D inputs directly, a slice-by-slice procedure is conducted for the segmentation of the volume. The segmentation procedure of 2D foundation models necessitate input prompts for each 2D slice containing the target.
In contrast, SegAnyPET and other 3D SAM medical adaptions \cite{SAM-Med3D} can be directly utilized to segment the target organs from input volume with one or a few prompts. 
\Cref{prompting} (a) and (b) present the visualization of the segmentation workflow of 2D and 3D foundation models. Based on the comparison in \cref{prompting} (c), directly utilizing 3D foundation model for promptable segmentation can reduce the need of manual prompting with less inference time.


\begin{figure*}[t]
    \centering
	\includegraphics[width=\linewidth]{Prompting.png}
	\caption{Visualization of different prompting strategies. (a) The segmentation workflow of 2D foundation models. (b) The segmentation workflow of 3D foundation models. (c) Inference time comparison of different prompting strategies.}
    \label{prompting}
\end{figure*}



\subsection*{B.4. Evaluation Metric}

We use the Dice Similarity Coefficient (DSC) as the evaluation metric of the segmentation task, which is a widely used metric in the field of image segmentation to evaluate the similarity between two sets. The formula for DSC is given by:

\begin{equation*}
DSC(G, S) = \frac{2|G\cap S|}{|G| + |S|}
\label{DSC}
\end{equation*}
where $G$ represents the ground truth segmentation and $S$ represents the predicted segmentation. The DSC is calculated by taking twice the size of the intersection and dividing it by the sum of the sizes of the two sets. This normalization ensures that the coefficient ranges from 0 to 1, where 1 indicates perfect overlap between the ground truth and the predicted segmentation.


\section*{C. Additional Experiments and Discussion}

\subsection*{C.1. Generalization to Tumor Segmentation}


\begin{table}[h]
    \centering
    \normalsize
    \setlength\tabcolsep{9pt}
	\renewcommand\arraystretch{1.08}
	\begin{tabular}{c|c|c}
		\hline 	\hline
		Strategy  & Prompts & Tumor DSC \\ \hline
\multirow{3}{*}{SAM \cite{SAM}}  & N points & 19.03 \\
& 3N points & 27.89 \\
& 5N points & 40.91 \\ \hline 
\multirow{3}{*}{MedSAM \cite{MedSAM}} & N points & 1.77 \\
& 3N points & 26.13 \\
& 5N points & 32.14 \\ \hline 
\multirow{3}{*}{SAM-Med3D \cite{SAM-Med3D}} & 1 point & 11.45   \\ 
& 3 points & 14.93 \\
& 5 points & 16.32 \\  \hline 
\multirow{3}{*}{SegAnyPET} & 1 point & 19.38   \\ 
& 3 points & 24.57 \\
& 5 points & 24.93 \\  \hline \hline
	\end{tabular}
	\caption{Generalization performance to unseen out-of-distribution AutoPET dataset for zero-shot interactive tumor segmentation with comparison to state-of-the-art segmentation foundation models for zero-shot interactive segmentation from PET images.} \label{Table_Tumor}
\end{table}


In addition to the internal and external evaluation on organ segmentation, we also compare SegAnyPET with other state-of-the-art segmentation foundation models for zero-shot tumor segmentation on AutoPET dataset.
\Cref{Table_Tumor} presents the experimental results under different prompt settings. Contrary to the conclusions of organ segmentation, we observe that slice-by-slice segmentation of 2D foundation models outperforms 3D foundation models.
A significant difference is that the target organs in our task are all continuous entities, while the whole-body tumors in AutoPET dataset are scattered multiple small targets located in various places, as shown in \cref{tumor_vis}. Therefore, the 3D model cannot directly segment all these scattered tumors out using one or a few prompt points, which also indicates the limitations of our method for this task.
As an important application scenario, we aim to enlarge the dataset with instance-level tumor annotation for training and evaluation of tumor lesion segmentation in the future.

\begin{figure*}[t]
    \centering
	\includegraphics[width=\linewidth]{tumor.png}
	\caption{Visualization of an example case for whole-body tumor lesion segmentation of AutoPET dataset. The tumor regions are visualized in red.}
    \label{tumor_vis}
\end{figure*}



\begin{table*}[h]
\centering
\setlength\tabcolsep{4pt}
\renewcommand\arraystretch{1.4}
\begin{tabular}{c|c|ccc|ccc|ccc}
\hline
Model & TotalSegmentator \cite{wasserthal2023totalsegmentator} & \multicolumn{3}{c}{SAM-Med3D-turbo} & \multicolumn{3}{|c}{SAM-Med3D-turbo} & \multicolumn{3}{|c}{SegAnyPET} \\ \hline
Modality & Registrated CT & \multicolumn{3}{c|}{Registrated CT} & \multicolumn{3}{c|}{PET} & \multicolumn{3}{c}{PET} \\ \hline
Prompt & Auto & 1 point & 3 points & 5 points & 1 point & 3 points & 5 points & 1 point & 3 points & 5 points \\ \hline
Avg DSC & 88.71 & 66.59 & 73.77 & 76.01 & 72.09 & 76.58 & 78.35 & 90.49 & 90.90 & 91.05\\
\hline
\end{tabular}
	\caption{Quantitative comparison different automatic and promptablt segmentation models for organ segmentation from CT and PET images.} \label{Table_CT}
\end{table*}

% \begin{table}[t]
%     \centering
%     \normalsize
%     \setlength\tabcolsep{4pt}
% 	\renewcommand\arraystretch{1.2}
% 	\begin{tabular}{c|c|c|c}
% 		\hline 	\hline
% 		Model & Modality  & Prompts & Avg DSC \\ \hline
%         TotalSegmentator & Registrated CT & Auto & 88.71  \\ \hline
% \multirow{3}{*}{SAM-Med3D-turbo} & & 1 point & 66.59   \\ 
% & Registrated CT & 3 points & 73.77 \\
% && 5 points & 76.01 \\  \hline 
% \multirow{3}{*}{SAM-Med3D-turbo} & & 1 point & 72.09   \\ 
% & PET & 3 points & 76.58 \\
% && 5 points & 78.35 \\  \hline 
% \multirow{3}{*}{SegAnyPET} & & 1 point & 90.49   \\ 
% &PET& 3 points & 90.90 \\
% && 5 points & 91.05 \\  \hline \hline
% 	\end{tabular}
% 	\caption{Quantitative comparison of organ segmentation models from CT and PET images.} \label{Table_CT}
% \end{table}





\subsection*{C.2. Comparison with CT Segmentation}

Since the data used in this work are whole-body PET/CT images, we conduct additional evaluations with models for automatic and promptable organ segmentation from CT images.
Given that the segmentation is used to evaluate the organ metabolic intensity from PET, it is necessary to register the CT to the resolution of PET before segmentation. Through the experimental comparison between the registered CT and PET in \cref{Table_CT}, we observe the performance of segmentation from CT is inferior to that of direct segmentation from PET.
Furthermore, developing a PET segmentation model can be compatible to different scenarios such as PET/MRI or CT-free PET with self-attenuation correction.

