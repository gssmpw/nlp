
\begin{table*}[t]
	\centering
    \normalsize
    \setlength\tabcolsep{5pt}
	\renewcommand\arraystretch{1.15}
	\begin{tabular}{c|c|ccccccc|c}
		\hline 	\hline
		\multirow{2}{*}{Method}  &  \multirow{2}{*}{Prompt} & \multicolumn{8}{c}{Training Invisible Organ Segmentation DSC Performance [\%]}  \\
\cline{3-10}  && Aorta & Lung-LL & Lung-LR & Lung-UL & Lung-UR & Lung-MR & Prostate & Avg  \\ \hline
SAM  \cite{SAM} & N points & 2.55 & 9.60 & 12.24 & 11.17 & 15.23 & 9.75 & 8.01 & 9.79 \\
MedSAM  \cite{MedSAM} & N points & 0.10 & 0.89 & 0.70 & 0.97 & 1.70 & 2.57 & 0.51 & 1.06  \\
SAM-Med3D \cite{SAM-Med3D} & 1 point & 11.29 & 25.60 & 45.95 & 37.94 & 47.15 & 31.64 & 9.93 & 29.93 \\
SAM-Med3D-organ & 1 point & 24.12 & 45.71 & 42.16 & 56.32 & 61.71 & 37.85 & 32.25 & 42.87  \\
SAM-Med3D-turbo & 1 point & 21.28 & 31.55 & 37.82 & 44.36 & 53.56 & 39.34 & 25.46 & 36.20 \\ \hline
\rowcolor{gray!25} \textbf{SegAnyPET}  & 1 point & 82.99 & 88.32 & 89.76 & 89.47 & 91.91 & 89.17 & 91.67 & 89.04 \\ \hline
SAM  \cite{SAM} & 3N points & 6.45 & 13.91 & 16.88 & 16.40 & 22.04 & 15.45 & 19.30 & 15.78 \\
MedSAM  \cite{MedSAM} & 3N points & 17.95 & 24.85 & 20.18 & 18.29 & 21.23 & 23.86 & 25.82 & 21.74 \\
SAM-Med3D \cite{SAM-Med3D} & 3 points & 13.38 & 29.77 & 46.62 & 45.46 & 50.50 & 36.93 & 13.73 & 33.77 \\
SAM-Med3D-organ & 3 points & 28.76 & 48.77 & 50.06 & 64.17 & 69.10 & 51.05 & 33.84 & 49.39 \\
SAM-Med3D-turbo & 3 points & 27.37 & 36.55 & 47.00 & 57.91 & 66.17 & 57.66 & 31.21 & 46.27 \\ \hline
\rowcolor{gray!25} \textbf{SegAnyPET}  & 3 points & 84.51 & 89.09 & 90.54 & 90.26 & 92.52 & 89.72 & 92.19 & 89.83 \\ \hline
SAM  \cite{SAM} & 5N points & 14.58 & 19.37 & 23.37 & 22.97 & 31.62 & 24.88 & 41.03 & 25.40 \\
MedSAM  \cite{MedSAM} & 5N points & 21.73 & 27.45 & 25.55 & 23.05 & 27.91 & 28.24 & 37.69 & 27.37 \\
SAM-Med3D \cite{SAM-Med3D} & 5 points & 14.07 & 37.54 & 48.46 & 50.68 & 51.01 & 38.43 & 13.37 & 36.23 \\
SAM-Med3D-organ & 5 points & 29.95 & 53.12 & 54.02 & 65.87 & 71.35 & 52.55 & 35.25 & 51.73 \\
SAM-Med3D-turbo & 5 points & 31.92 & 42.05 & 48.36 & 64.34 & 69.65 & 61.15 & 32.77 & 50.03 \\ \hline
\rowcolor{gray!25} \textbf{SegAnyPET}  & 5 points & 84.63 & 89.21 & 90.69 & 90.34 & 92.63 & 89.80 & 92.57 & 89.98 \\ \hline \hline
	\end{tabular}
 	\caption{Generalization performance to \textbf{\underline{training invisible organs}} with comparison to state-of-the-art segmentation foundation models for zero-shot interactive segmentation from PET images.} \label{Table_Unseen}
\end{table*}



\section{Experiments}

\subsection{Datasets and Experimental Setup}

We conduct comprehensive experiments on two different PET datasets.
The first dataset is the proposed \textbf{PETS-5k dataset}, which consists of 5,731 three-dimensional whole-body PET images with the annotation of 5 target organs.
% including liver, left kidney, right kidney, heart, and spleen. 
Among the dataset, a subset of 100 images are with high-quality annotations verified by domain experts, which is split into 40 cases serving as HQ training set and 60 cases serving as internal test set.
The remaining 5,631 images are noisy-annotated with possible mislabeled or unlabeled pixels serving as the LQ training set.
To evaluate the generalization performance for universal segmentation, we annotate 7 additional training invisible organs from the internal test set.
% including aorta, prostate, left lung lower lobe, right lung lower lobe, left lung upper lobe, right lung upper lobe, and right lung middle lobe from the internal test set.
The second dataset is \textbf{AutoPET-Organ dataset} extended from the AutoPET dataset  \cite{AutoPET} consists of 1,014 three-dimensional whole-body PET images with corresponding annotation of tumor lesions. To evaluate the generalization ability, we further annotate a small subset with 100 cases with all 12 organs as an external test set.
Following the setting for promptable segmentation, the input prompts are simulated based on the ground truth mask with random perturbations. We compare proposed SegAnyPET with state-of-the-art segmentation foundation models \cite{SAM,MedSAM,SAM-Med3D} and training-based task-specific models \cite{segresnet,isensee2020nnunet,hatamizadeh2021swin,huang2023stunet}. 
More information of the datasets and implementation details are shown in the Appendix.



\subsection{Experiments}

\textbf{Comparison with Universal Foundation Models.}
The original SAM \cite{SAM} and MedSAM \cite{MedSAM} are designed for 2D segmentation tasks and cannot handle 3D inputs directly,
necessitating input prompts for each 2D slice containing the target to conduct slice-by-slice segmentation.
In contrast, SegAnyPET and other 3D SAM medical adaptions \cite{SAM-Med3D} can be directly utilized to segment the target organs from input volume with one or few prompts. 
\Cref{Table_Seen} presents the performance of SegAnyPET with state-of-the-art foundation models on our internal test set under different prompt settings.
We can observe that 2D models exhibit lower performance due to the lack of volumetric spatial information. In contrast, 3D models can achieve better performance with less manual prompting effort. Compared with state-of-the-art methods, SegAnyPET exhibits significantly better segmentation performance and outperforms the second-ranked method by a significant improvement up to 18.40\% on average DSC. 
We visualize the segmentation results of SegAnyPET and other segmentation foundation models for organ segmentation in \cref{organseg}, which intuitively show that SegAnyPET performs more precise segmentation with higher DSC.

\textbf{Comparison with Task-Specific Models.}
In addition to foundation models, we also conduct a comparison with state-of-the-art task-specific models in \cref{Table_Seen}. 
These task-specific models are trained on the HQ training set focusing on the segmentation task of target organs. When adapting to training invisible organs, additional annotation and model training are required for segmentation.
Despite the manual prompting efforts, we observe that SegAnyPET significantly outperforms task-specific models, while preserving the generalization ability for universal segmentation.


\begin{figure}
    \centering
	\includegraphics[width=\linewidth]{Segmentation.png}
	\caption{Visual comparison of segmentation results of different promptable segmentation foundation models for PET organ segmentation.}
    \label{organseg}
\end{figure}




\begin{table*}[h]
	\centering
    \small
    \setlength\tabcolsep{3pt}
	\renewcommand\arraystretch{1.3}
	\begin{tabular}{c|ccc|ccc|ccc|ccc}
		\hline 	\hline
Method & \multicolumn{3}{c}{SAM \cite{SAM}} & \multicolumn{3}{|c}{MedSAM \cite{MedSAM}} &  \multicolumn{3}{|c}{SAM-Med3D \cite{SAM-Med3D}}  &  \multicolumn{3}{|c}{\textbf{SegAnyPET}}   \\ \hline
Prompt & N points & 3N points & 5N points & N points & 3N points & 5N points &  1 point & 3 points & 5 points & 1 point & 3 points & 5 points \\
 \hline
Liver & 31.02 & 38.24 & 51.51 & 3.91 & 37.18 & 48.19 & 59.06 & 73.44 & 78.47 & 76.70  & 83.01 & 83.75\\ 
Kidney-L & 6.89 & 9.80 & 18.65 & 0.63 & 22.53 & 26.78 & 67.64 & 71.93 & 70.86 & 75.97  & 77.36 & 77.86 \\ 
Kidney-R & 7.08 & 9.97 & 17.69 & 1.08 & 25.82 & 33.37 & 54.82 & 62.57 & 63.71 & 71.56  & 73.95 & 75.25\\ 
Heart & 18.79 & 23.06 & 30.93 & 0.78 & 29.43 & 32.15 & 48.91 & 53.84 & 55.14 & 67.62 & 70.95 & 71.64 \\ 
Spleen & 11.05 & 15.14 & 23.94 & 0.74 & 30.52 & 32.53 & 37.58 & 43.59 & 49.69 & 77.97 & 80.16 & 80.84 \\ \hline
\rowcolor{gray!15} Aorta & 2.81 & 4.00 & 7.69 & 1.53 & 23.34 & 24.24 & 19.79 & 24.07 & 27.47 & 16.00 & 18.73 & 22.57 \\ 
\rowcolor{gray!15} Lung-LL & 13.16 & 15.49 & 21.93 & 2.84 & 21.81 & 22.19 & 32.27 & 38.05 & 41.77 & 13.32 & 24.09 & 26.73\\ 
\rowcolor{gray!15} Lung-LR & 16.49 & 19.45 & 26.11 & 1.65 & 26.48 & 28.52 & 45.18 & 47.99 & 49.08 & 26.67 & 37.87 & 41.35 \\ 
\rowcolor{gray!15} Lung-UL & 15.18 & 18.38 & 26.42 & 1.48 & 22.18 & 23.33 & 51.69 & 60.23 & 64.18 & 10.80 & 18.04 & 19.14 \\ 
\rowcolor{gray!15} Lung-UR & 18.36 & 21.65 & 29.13 & 1.70 & 29.10 & 33.74 & 41.31 & 48.86 & 49.92 & 19.08 & 39.95 & 43.34 \\ 
\rowcolor{gray!15} Lung-MR & 11.94 & 15.32 & 21.11 & 3.26 & 29.52 & 30.25 & 28.55 & 37.04 & 42.76 & 16.36 & 25.72 & 28.69 \\
\rowcolor{gray!15} Prostate & 3.96 & 6.60 & 17.51 & 0.96 & 23.80 & 29.71 & 31.52 & 43.11 & 43.48 & 35.93 & 38.47 & 39.87 \\ 
\hline \hline
	\end{tabular}
	\caption{Generalization performance to \textbf{\underline{unseen out-of-distribution}} AutoPET-Organ dataset with comparison to state-of-the-art segmentation foundation models for zero-shot interactive segmentation from PET images.} \label{Table_AutoPET}
\end{table*}




\textbf{Generalization to Unseen Organs and Dataset.}
To validate the generalization performance of SegAnyPET, we conduct additional experiments of foundation models for promptable segmentation of training invisible organs. As shown in \cref{Table_Unseen}, we observe SegAnyPET also exhibits strong generalization performance and outperforms state-of-the-art segmentation foundation models. The experimental results demonstrate that by efficiently learning representations from large-scale PET images, SegAnyPET exhibits comparable performance for training invisible organs. 
Furthermore, we evaluate the proposed method on training invisible AutoPET-Organ dataset with significant distribution gap compared with PETS-5k dataset. As shown in \cref{Table_AutoPET}, we observe that SegAnyPET can achieve satisfying generalization ability on unseen dataset.

\textbf{Ablation Analysis.}
In \cref{Table_Ablation}, we conduct ablation experiments to evaluate the effectiveness of proposed training strategy. 
Three different training strategies are conducted including fine-tuning with mixed HQ and LQ, training with additional unsupervised regularization for LQ, and the proposed cross prompting confident learning strategy. Experimental results show that the proposed strategy can achieve better performance on both seen and unseen targets.

