\documentclass[reqno,11pt]{amsart}
%\documentclass[11pt]{article}

%\usepackage[notref, notcite]{showkeys}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{color}
\usepackage{hyperref}
\usepackage[mathscr]{eucal}
\usepackage{enumerate}
\usepackage{times}
%\usepackage{authblk}
\usepackage[skip=10pt]{caption} % Adjust the skip value as needed


\numberwithin{equation}{section}
\allowdisplaybreaks[1]

\newtheorem{Def}{Definition}[section]
\newtheorem{Thm}[Def]{Theorem}
\newtheorem{Prop}[Def]{Proposition}
\newtheorem{Lemma}[Def]{Lemma}
\newtheorem{Remark}[Def]{Remark}
\newtheorem{Corollary}[Def]{Corollary}
\newtheorem{Example}[Def]{Example}
\newtheorem{Assumption}[Def]{Assumption}
\newcommand{\beq}{\begin{equation}}
\newcommand{\eeq}{\end{equation}}
\newcommand{\Proof}{\begin{proof}}
\newcommand{\QED}{\end{proof} \noindent}
\newcommand{\QEDrem}{\ \hfill $\Diamond$}
\newtheorem{proposition}[Def]{Proposition}

\newcommand{\spc}{\;\;\;\;\;\;\;\;\;\;}
\newcommand{\msp}{\hspace{-.1cm}}
\newcommand{\mm}{\hspace{-.08cm}\cdot \hspace{-.08cm}}

\newcommand{\M}{\mathcal{M}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\R}{\mathbb{R}}
%\newcommand{\1}{\mbox{\rm 1 \hspace{-1.05 em} 1}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\VM}{\mathcal{V}\hspace{-.05cm}\mathcal{M}}

\newcommand{\Gammati}{\tilde{\Gamma}}
\newcommand{\Riem}{{\rm Riem}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\Ati}{\tilde{\mathcal{A}}}
\newcommand{\Aop}{\mathcal{A}_\textbf{b}}
\newcommand{\X}{X}
\newcommand{\red}{\color{red}}
\newcommand{\cutoff}{\chi_{_\delta}}


\title[CayleyPy RL]{CayleyPy RL: Pathfinding and Reinforcement Learning on Cayley Graphs}

\author[A. Chervov]{A. Chervov}
\address{Institut Curie, Paris, France}
\email{al.chervov@gmail.com}

\author[A.Soibelman, ... ]{
A.Soibelman, S.Lytkin, I.Kiselev, S.Fironov, A.Lukyanenko, A.Dolgorukova, A.Ogurtsov,
F.Petrov, S.Krymskii, M.Evseev, L.Grunvald, D.Gorodkov, G.Antiufeev, G.Verbii, 
V.Zamkovoy, L.Cheldieva, I.Koltsov, A. Sychev, M.Obozov, 
A.Eliseev, S.Nikolenko, N.Narynbaev, R.Turtayev,
N. Rokotyan, S.Kovalev,
A.Rozanov, V.Nelin, S.Ermilov,
 L.Shishina, D.Mamayeva, A.Korolkova,
K.Khoruzhii, A.Romanov }

%\author[A.Chervov]{A.Chervov}
% \author[A.Soibelman]{A.Soibelman}
% \author[S.Lytkin]{S.Lytkin}
% \author[S.Fironov]{S.Fironov}
% \author[A.Lukyanenko]{A.Lukyanenko}
% \author[A.Dolgorukova]{A.Dolgorukova}
% \author[A.Ogurtsov]{A.Ogurtsov}
% \author[F.Petrov]{F.Petrov}
% \author[S.Krymskii]{S.Krymskii}
% \author[M.Evseev]{M.Evseev}
% \author[L.Grunvald]{L.Grunvald}
% \author[D.Gorodkov]{D.Gorodkov}
% \author[G.Antiufeev]{G.Antiufeev}
% \author[G.Verbii]{G.Verbii}
% \author[V.Zamkovoy]{V.Zamkovoy}
% \author[L.Cheldieva]{L.Cheldieva}
% \author[I.Koltsov]{I.Koltsov}
% \author[A.Sychev]{A.Sychev}
% \author[M.Obozov]{M.Obozov}
% \author[A.Eliseev]{A.Eliseev}
% \author[S.Nikolenko]{S.Nikolenko}
% \author[N.Narynbaev]{N.Narynbaev}
% \author[R.Turtayev]{R.Turtayev}
% \author[N.Rokotyan]{N.Rokotyan}
% \author[S.Kovalev]{S.Kovalev}
% \author[A.Rozanov]{A.Rozanov}
% \author[V.Nelin]{V.Nelin}
% \author[S.Ermilov]{S.Ermilov}
% \author[I.Kiselev]{I.Kiselev}
% \author[L.Shishina]{L.Shishina}
% \author[D.Mamayeva]{D.Mamayeva}
% \author[A.Korolkova]{A.Korolkova}
% \author[K.Khoruzhii]{K.Khoruzhii}
% \author[A.Romanov]{A.Romanov}


% \author[B.\ Temple]{Blake Temple \\ \\ January 28, 2024}
% \address{Department of Mathematics\\ University of California\\ Davis, CA 95616\\ USA}
% \email{temple@math.ucdavis.edu}





\includeonly{}

\begin{document} 

\begin{abstract}
This paper is the second in a series of studies on developing efficient artificial intelligence-based approaches to pathfinding on extremely large graphs (e.g. $10^{70}$ nodes) with a focus on Cayley graphs and mathematical applications. The open-source CayleyPy project is a central component of our research. The present paper proposes a novel combination of a reinforcement learning approach with a more direct diffusion distance approach from the first paper. Our analysis includes benchmarking various choices for the key building blocks of the approach:  architectures of the neural network, generators for the random walks and beam search pathfinding. We compared these methods against the classical computer algebra system GAP, demonstrating that they  "overcome the GAP" for the considered examples.
As a particular mathematical application we examine the Cayley graph of the symmetric group with cyclic shift and transposition  generators. We provide strong support for the OEIS-A186783 conjecture that the diameter is equal to n(n-1)/2 by machine learning and mathematical methods. We identify the conjectured longest element and generate its decomposition of the desired length. We prove a diameter lower bound of n(n-1)/2-n/2 and an upper bound of n(n-1)/2+ 3n by presenting the algorithm with given complexity. We also present several conjectures motivated by numerical experiments, including observations on the central limit phenomenon (with growth approximated by a Gumbel distribution), the uniform distribution for the spectrum of the graph, and a numerical study of sorting networks.
To stimulate crowdsourcing activity, we create challenges on the Kaggle platform and invite contributions to improve and benchmark approaches on Cayley graph pathfinding and other tasks.
\end{abstract}



\keywords{Machine learning, reinforcement learning, Cayley graphs}

\maketitle 

\setcounter{tocdepth}{1}
\small
\tableofcontents
\normalsize

\section{Introduction}

\subsection{Broader context}
Deep learning has revolutionized various fields of research and was recognized by several Nobel Prizes in 2024.  
In recent years, machine learning has been emerging as "a tool in theoretical science"~\cite{douglas2022machine}, leading to several noteworthy applications to mathematical problems:~\cite{lample2019deep,davies2021advancing, bao2021polytopes, 
romera2024mathematical,
coates2024machine,alfarano2024global, charton2024patternboost,shehper2024makes,swirszcz2025advancing}.

The paper presents advancements in applying artificial intelligence methods to the mathematical problem of Cayley graph pathfinding, which is equivalent to decomposing a group element into a product of generators. It is the second work in a series devoted to the CayleyPy project to develop efficient machine learning-based approaches and an open source library to deal with extra large graphs
(e.g. $10^{70}$) with a focus on finite Cayley graphs and mathematical applications.

From a broader perspective, pathfinding is a specific case of the planning problem, where one must determine a sequence of actions to transition between two given states. Similar challenges arise in robotics and games like chess or Go, where the objective is to plan moves that lead from the initial position to a winning position. Mathematically, such problems are modeled as pathfinding on graphs (state transition graphs), where nodes represent possible states, and edges correspond to state transitions based on actions or moves. The planning task then reduces to finding a path from a given initial node to one or more target nodes. The breakthrough works AlphaGo and AlphaZero~\cite{silver2016mastering}, 
\cite{silver2017mastering} by Google DeepMind have demonstrated that machine learning can significantly outperform previously known methods. They serve as both a prototype and an inspiration for many developments, including ours. In a nutshell, the method consists of two main components: a neural network trained to suggest moves from any given state, and a graph pathfinding algorithm that helps to find a path even when the neural network's predictions are not sufficiently precise. The core idea is similar across most approaches, but achieving optimal performance for each task requires analyzing and benchmarking various neural networks and graph pathfinding methods. One of the goals of this work is to present such an analysis for some particular Cayley graphs.


\subsection{Present paper: approach, benchmarks, mathematical theorems and conjectures}

\begin{figure}[h!]
   \centering
   \includegraphics[width=1.0\linewidth]{figs/LRX.jpg}
   \caption{LRX Cayley graph for $S_5$}\label{fig:LRX1}
\end{figure}

The paper presents both AI contributions and  mathematical results and conjectures. Here, we work with a particular Cayley graph defined by the L - left cyclic shift, R - right cyclic shift and X - eXchange of positions $0,1$, it will be denoted LRX following \href{https://oeis.org/A186783}{OEIS-A186783}. Its image for $S_5$ is shown on figure \ref{fig:LRX1}.

{\bf Developments of AI methodology.} We propose a modified deep Q-learning (DQN) method for the case of the graph pathfinding task which resolves  the so-called sparse reward problem of the standard DQN approach for graph pathfinding. The proposal combines DQN with the more simple diffusion distance approach from our previous paper ~\cite{chervov2025machinelearningapproachbeats}. An analysis and benchmarks of these approaches are presented, including tests of various neural network architectures such as transformers, convolutional networks, and perceptrons. It  is demonstrated that these methods allow finding paths for $S_n$ up to $n$ around 30 - for the particular class of generators (LRX) which are in the main focus of the present paper. We demonstrate that they "overcome the GAP" - outperform classical computer algebra system GAP on the same task which can work only up to $n$ around 20. 

{\bf Efficient pytorch implementation.} We pay significant attention to develop an original highly optimized code, the current version is specific to Cayley graphs of permutation groups (matrix groups will be supported later). We propose many technical solutions for fast parallel processing of large amounts of permutations, including product computations, efficient hashing, and extracting unique states. The code works on both CPU and GPU without modification.

{\bf One line of code  bringing  orders of magnitudes improvement.} We report a curious finding - a single-line modification in the code of our main graph pathfinding module (beam search), dramatically improves performance. This enhancement increases the feasible pathfinding size from symmetric groups $S_n$ with $n$ to $n$ around 100 (and apparently even further with more hardware). Unfortunately, this improvement is currently specific to the particular LRX Cayley graphs considered in this paper, but we hope it can be generalized. 

Mathematical contributions. % of the present paper are following are devoted to conjecture above and more general understanding of the properties of that Cayley graph and its relatives.


{\bf LRX Cayley graphs. Diameter conjecture \href{https://oeis.org/A186783}{OEIS-A186783}.} In the present paper, we focus on a specific family of Cayley graphs for the symmetric groups $S_n$ - those generated by the two cyclic shifts (L) - left, (R) - right and the transposition X=$(0,1)$ of the first two positions. Surprisingly, according to \href{https://oeis.org/A186783}{OEIS-A186783}, it is an open conjecture that the diameter of the Cayley graph is $n(n-1)/2$ for $S_n$. These generators and Cayley graphs will be denoted LRX as in OEIS.

{\bf The longest element.} 
By implementing an efficient brute-force breadth-first search algorithm, we traverse the full LRX graphs for $n\le 14$, confirming that the longest element has a length of $n(n-1)/2$, as predicted.  Moreover, we observe that this element is always unique and follows a clear pattern - it is a product of transpositions:  $(0,1)(n-1,2)(n-2,3)(n-3,4)...$,  so we expect this  for all $n$.  

{\bf AI and crowd-sourcing to support the conjecture.} For that particular element, we have launched various versions of our pipelines and have consistently found that the decomposition is never shorter than $n(n-1)/2$. We organized a \href{https://www.kaggle.com/competitions/lrx-oeis-a-186783-brainstorm-math-conjecture/overview}{challenge} on the Kaggle platform giving that particular element for $n=2...200$ and requesting decompositions of the shortest possible length. Once again, no participant found a decomposition shorter than $n(n-1)/2$, thereby confirming the expectation.

{\bf Rigorous lower and upper bounds.} We prove lower bound $n(n-1)/2 - n/2-1$  by a combinatorial argument (improving $n(n-1)/3$~\cite{babai1989small}); afterwards, we prove the diameter  upper bound   $n(n-1)/2 +3n$ by the algorithm  with such complexity (improving $3/2n^2$ ~\cite{kuppili2020upper}). %To the best of our knowledge these are the best estimates and algorithm. 
We also develop another algorithm which empirically shows better complexity:  $n(n-1)/2 + n/2$, but lacks the rigorous proof for such an estimate.  
Finally, we present explicit decomposition  for the conjecturally longest element of the desired length $n(n-1)/2$. 

{\bf Central limit phenomena for growth - Gumbel distribution.} 
Computations suggest the following conjecture: for large 
$n$, the growth function of LRX graphs follows an asymmetric Gumbel distribution. This is in the vein of field-shaping works by P. Diaconis et. al.~\cite{diaconis1977spearman, diaconis1988metrics,chatterjee2017central}, who demonstrated that the growth of the neighbor transposition graph and many related statistics asymptotically follow the Gaussian normal distribution.


We study numerically the spectrum of LRX graph and observe surprisingly uniform distribution of eigenvalues, we also analyze statistics of the paths from the longest element to the identity ("sorting networks") we observe numerically that pattern differs from the famous "sine curves" for Cayley graph of neighbor transpositions~\cite{angel2007random}; 
numerical analysis of random walks shows consistency with results of Diaconis~\cite{diaconis1993comparison} on mixing time for LRX Cayley graph. 

We also present similar analysis for the Schreier coset graph for the action of the LRX generators on binary strings consisting of $n/2$ zeros and ones (for even $n$). %(Might be thought as an analogue of Grassmanian $Gr(n,2n)$ over the field with one element.)

In summary, our analysis demonstrates that computational experiments, especially those involving AI, can be very helpful in advancing mathematical research on Cayley graphs. AI methods significantly outperform classical methods of computer algebra systems such as GAP, performing well for $S_n$ with $n$ around 30, and with a small addition of prior knowledge up to 90, while classical methods perform well only up to 20. 

\subsection{Finite Cayley graphs: pathfinding techniques, results and open problems}

Let us briefly summarize existing techniques for Cayley graphs pathfinding, and remind more general context on Cayley graphs research.

{\bf Optimal pathfinding is typically NP-hard.} Finding the shortest paths on generic finite Cayley graphs is an NP-hard problem~\cite{even1981minimum} (even P-space complete \cite{jerrum1985complexity}). As is the case for many specific group families, such as NxNxN Rubik's Cube groups~\cite{demaine2017solving} and others~\cite{bulteau2015pancake}.
For particular groups it is hard to develop practical optimal algorithm e.g. standard 3x3x3 Rubik's cube the first optimal algorithm has been proposed only 1997~\cite{korf1997finding}. It required 192 Gigabytes of precomputed data and 12-24 hours to solve single cube (methods were later improved~\footnote{M.Reid: https://www.cflmath.com/Rubik/optimal\_solver.html, H.Kociemba 2021: https://pypi.org/project/RubikOptimal/, Andrew Skalski:  https://github.com/Voltara/vcubem , \cite{Rokicki2014Diameter} }%\cite{Reid1997Optimal,HKociemba2021,Rokicki2014Diameter,Skalskivcube} 
to several cubes per second). To develop practical optimal solver for 4x4x4 Rubik's cube - is a challenge. 

{\bf Non-optimal pathfinding: Schreier-Sims algorithm, GAP implementation, non-effectivity for large groups.} 
The Schreier-Sims algorithm~\cite{sims1970computational},\cite{knuth1991efficient} (or its randomized versions) is the method typically used for decomposing group element into product of generators for permutation groups. It is implemented in GAP computer algebra system. However its outputs "are usually exponentially long"~\cite{fiat1989planning} and that forbids practical computations for large groups (typically the sizes beyond $10^{30}-10^{40}$ are out of reach - examples and analysis is given below). 


{\bf Non-optimal pathfinding: "small support" Kaggle Santa 2023 methods (generality issue).} The methods which can deal with extremely large groups (e.g. size $>10^{1000}$ like 33x33x33 Rubik's cube) were demonstrated by top participants of the recent challenge organized on the Kaggle platform - \href{https://www.kaggle.com/competitions/santa-2023}{Santa 2023}. However the generality of that approach is unclear.%What is not clear whether  such methods be applied only to some specific choices of generators or can be extended to any choices. 
That is contrast to AI methodology which is from the beginning is  general not even restricted to Cayley graphs. The idea of "Santa methods" is quite noteworthy:  one should first find "small support elements" achievable from original generators and take them as new generators, then one simply runs beam search with Hamming distance heuristics to find path. Method becomes effective since for small support generators Hamming distance and true graph distance are  quite related by obvious reason. "Small support" are those permutations which change only small number of positions. They are well-known for  Rubik's cube solvers - "commutator" or "Pif-Paf" moves~\cite{mulholland2016permutation}, \href{https://math.stackexchange.com/q/4962862/21498}{M.Brandenburg post}. In mathematics small support elements play an important role in deep works by Babai, Seress, Helfgott, et.al.~\cite{babai1988diameter, babai2004diameter, bamberg2014bounds, helfgott2014diameter, helfgott2019growth} (slides: \href{https://www.math.auckland.ac.nz/~conder/SODO-2012/Seress-SODO2012.pdf}{A.Seress},\href{https://simons.berkeley.edu/sites/default/files/docs/6206/symtalk.pdf}{H.Helfgott} ), however it is 
unclear whether one can effectively find small support elements from given generators, thus making unclear the generality of "small support methods".

{\bf Non-optimal pathfinding: algorithms for specific families of generators.} 
For some particular choices of generators there are algorithmic solvers which can effectively find non-optimal solution. It seems unlikely that one can produce similar algorithms for any generators. The prototypical example is bubble sort algorithm, which solves the problem for the neighbor transpositions generators of $S_n$. Algorithmic solvers exist for Rubik's cube of arbitrary sizes and for other puzzles related groups. The other families of generators  and algorithms are important in bioinformatics for estimation of the evolutionary distance. These generators are related to flips of subsequences - see e.g.~\cite{Pevzner1995human2mice, Pevzner1999cabbage2turnip, bulteau2019parameterized}. Related example is \href{https://en.wikipedia.org/wiki/Pancake_sorting}{pancake sorting} (or prefix sorting) problem with the first algorithm proposed by Bill Gates et.al. \cite{gates1979bounds}.

{\bf AI-methods for pathfinding.} 
To the best of our knowledge there is no systematic effort for Cayley graph pathfinding  with AI methods except our CayleyPy project. However for some particular cases for example Rubik's cube there exists a number of approaches, e.g. based on genetic algorithms:~\cite{swita2023solving}.%, which can be modified for the general case. 
The most notable are: DeepCube~\cite{mcaleer2019solving, agostinelli2019solving, khandelwal2024towards,agostinelli2024q} - the first AI based successful solver for 3x3x3 cube (\href{https://deepcube.igb.uci.edu/}{website}), the second one: EfficientCube~\cite{takano2023selfsupervision}, some other:~\cite{brunetto2017deep,johnson2021solving,amrutha2022deep,noever2022puzzle,chasmai2022cubetr,bedaywi2023solving,pan2021fourier} which not achieve complete solution. Noteworthy idea~\cite{pan2021fourier} is to combine neural networks with representations theory of the symmetric group ---  neural net  predicts the coefficients of the non-abelian Fourier transform for the distance function. The rationale is: observed sparsity (bandlimitedness) of Fourier transform of the common distance functions on $S_n$ ~\cite{swan2017harmonic}. The recent seminal achievement from S.Gukov's team: ~\cite{shehper2024makes} creates effective AI based pathfiding approach for infinite group of Andrews-Curtis moves and resolves the  Akbulut-Kirby conjecture remained opened for 39 years. AI-methods are also used for pathfinding in the context of planning movements in obstacle-rich environments and road networks it is related field, but with different focus \cite{pandy2022learning, kirilenko2023transpath} 

{\bf Cayley graphs: applications, diameters, random walks, open conjectures.}
Let us give a more general context on Cayley graph research.
Cayley graphs are fundamental in group theory~\cite{gromov1993geometric},\cite{tao2015expansion}, and have various applications: bioinformatics
~\cite{Pevzner1995human2mice, Pevzner1999cabbage2turnip,  bulteau2019parameterized}; processors interconnection networks~\cite{akers1989group, cooperman1991applications,heydemann1997cayley};in coding theory coding theory and cryptography~\cite{hoory2006expander,zemor1994hash,petit2013rubik}; in quantum computing~\cite{ruiz2024quantum,sarkar2024quantum,dinur2023good, acevedo2006exploring,gromada2022some}, etc.

Two focuses of pure mathematical research on finite Cayley graphs are related to diameters estimation and the behavior of random walks. In the case of the symmetric group $S_n$ there are two open conjectures which are easy to formulate, but somewhat representative for the field.  

{\bf Babai-like conjecture:} diameter of  $S_n$ is not more than $O(n^{2})$ - for any choices of generators (see e.g. H.Helfgott's surveys: ~\cite{helfgott2014diameter},\cite{helfgott2019growth},\cite{helfgott2015random} ; 

{\bf Diaconis ~\cite{diaconis2013some} conjecture:}  the mixing time for random walks is not more than $O(n^{3}log(n))$ (again for any choices of generators).

The first one can be thought as a particular case of the Babai conjecture~\cite{babai1992diameter}, \cite{tao2015expansion} which predicts that diameter of simple groups is not as  large as one may expect: $O(log^c|G|)$ for some absolute $c$ --- in contrast to abelian groups where it can be $O(|G|)$.  Thus conjecture represents an intriguing connection between algebraic properties of a group (which does not depend on the choice of generators) - being simple  and the  property of Cayley graphs - having short diameter (which is generator dependent property). 

Estimating diameters of Cayley graphs is a hard problem. One of its applications is latency estimation for processor interconnection problem~\cite{akers1989group},\cite{cooperman1991applications}, \cite{heydemann1997cayley} - since diameter corresponds to maximal latency. 
It is also  of interest for community of puzzles enthusiasts - like Rubik's cube. Diameter is also called a "God's number" - the number of moves the worst configuration can be solved by the best (optimal) algorithm. It is widely unknown for most of the puzzles, its determination for the Rubik's cube 3x3x3 for the standard choices of generators required significant efforts and computational resources~\cite{Rokicki2014Diameter}.  It is not known precisely for higher cubes, neither for 3x3x3 Rubik's with \href{https://www.speedsolving.com/wiki/index.php?title=Metric#STM}{STM, ATM} metrics.% cube with a little modified definitions of generators.  
A precise conjectures on 4x4x4 and 5x5x5 Rubik's cube diameters appeared recently~\cite{hirata2024probabilistic}, collections of conjectures and estimates for many puzzles can be found on \href{https://www.speedsolving.com/wiki/index.php?title=God%27s_Algorithm#Table_of_God.27s_Numbers}{speedsolving website}. 

As we will discuss below novel AI methods might provide insights via computational experiments to such kind of questions.

\section{Reinforcement learning (RL) and graphs pathfiding}
\subsection{Reminders: RL, sparse reward problem, Bellman equation for graph distances,  Q-algorithm, pathfinding}

{\bf RL in brief.}
Reinforcement learning is an important direction 
in AI that differs from supervised and unsupervised learning in the following ways: it focuses on optimizing {\bf cumulative rewards}, and it requires the agent to, in a sense, "{\bf create its own training set}". The agent must choose the best strategies to "explore the space" and select training examples on its own, in contrast to supervised learning, where the training set is provided in advance.


{\bf RL "equivalence" to graph pathfinding. }
The setup of reinforcement learning is very similar to the setup of graph pathfinding: states - are nodes of the graph, actions corresponds to edges/transitions to neighbor states by these edges, rewards corresponds to weights on the edges. The cumulative reward corresponds to  length of paths (with respect to edges weights). Typically in graph theory shortest paths are of interest - that means "reward" is minimized, not maximized as in RL, so it is better to use term not the "reward", but "penalty". 

{\bf Unweighted graphs and sparse reward problem.}
The sparse reward problem is a well-known challenge in reinforcement learning (RL), and it specifically arises in pathfinding on Cayley graphs and, more generally, on any unweighted graph, which makes standard RL approaches perform poorly.
The sparse reward problem occurs when rewards are given very rarely, making it difficult to generate data for training. This situation is particularly prevalent in graph pathfinding on large graphs, where the task is to find a path to a single node out of trillions of others. A reward is only given once the desired node is reached, but it is practically impossible to find it with a "cold start" through random exploration.
It's important to note that the condition of all weights being equal to one plays a significant role. If the weights were different, one could reasonably adopt a strategy of selecting the step with the best local reward. However, when all weights are identical, there is no local preference to guide the agent in making the right move.
There are several strategies to address this issue, and below, we propose a new approach.

{\bf Bellman value equation  is equation for distance functions.}
The value function (or "position estimation") in RL exactly corresponds to the distance functions on graphs. 
To make that analogy more precise: consider an unweighted  graph $G$ (all edges weights are equal to 1), choose some node $e$ (e.g. identity of a group in Cayley graph), consider a function $d(g)$ - the distance of the shortest path from $g$ to $e$ (it is "\href{https://en.wikipedia.org/wiki/Word_metric}{word metric}" in group theory). Then there is a simple relation:

\begin{align} 
%$$ 
\label{bel_eq}
d(g) & = 1 + \min_{n:~neigbours~of~g} d(n)  \text{~~~~~~~~~ Bellman equation}\\
%$$
d(e) & = 0 \text{~~~boundary condition}
\end{align} 


This relation is simple, but it is fundamental - it is a particular case of the Bellman's equation. The properties can be summarized in the following simple proposition, which is easy to see inductively considering neghbor nodes to $e$, then neighbors of neighbors and so on.


\begin{proposition}
    For any finite connected graph $G$ the only solution to the pair of equations above are given by the  distance function (length of the shortest path) to the node $e$. Moreover the only solutions to the Bellman's equation (without boundary condition) correspond to distance to some sets of nodes in $G$ (up to adding a constant). That set can be recovered as minimums of $d(g)$. 
\end{proposition}


{\bf Sense of Bellman equation - local condition for globally (cumulatively) defined object.}
The Bellman equation is equation for the length of the shortest path - that is globally/cumulatively defined object, but the condition itself is local - only at the neighbor of each point.  

{\bf Idea of Q-learning and deep Q-learning algorithms for value/distance function.}
These algorithms provide a way to compute solutions of the Bellman equation, which means to find the distance functions on graphs.  One starts with some  initialization for $d(g)$, then compute next iteration:\\ $d_k(g) = 1+\min_{n:~neigbours~of~g} d_{k-1}(n), d_{k}(e) =0$ by applying the right hand side of the  equations \ref{bel_eq}. There are theoretical guarantees that such a process will converge to the solution of the Bellman equation (and boundary condition). 

{\bf Idea of deep Q-learning algorithm.}
The deep Q-learning is used in the situations where the graph is large (can be of google size). 
One assumes that for each node of graph there is a feature vector attached. 
One considers a neural network which takes a feature vector as an input and should output the approximate solution for Bellman equations.
The training process is organized as follows.
Each epoch one selects some subset of nodes and from current neural network $d(g)$  computes the right hand side of the equations \ref{bel_eq}:
$t(g) = 1+\min_{n:~neigbours~of~g} d(n), t(e) =0$, denoting $t(g)$ as a result of that computation. 
Afterwards one runs gradient descent to minimize the error between $d(g)$ and $t(g)$. If the error would be zero that means that $d(g)$ solves the Bellman equations. And thus in graph theory setup it produces the distance function ("word metric" in the case of Cayley graphs).
Typically the choice of the training subset of nodes  is made with the help of walking over the graph combining both random steps and steps guided by the model. We will discuss that in details below.

{\bf Graph path finding.}
To find a path from a given node 
$g$ to the selected node 
$e$, one needs to run a greedy search algorithm. Starting from node  $g$, the algorithm inspects all its neighbors, evaluates the distance function for each of them, and then chooses the node with the minimum value of the distance function, moving to that node. The process is repeated until the destination node is found or the step limit is exceeded. If the distance function accurately reflects the true distance on the graph (i.e., the length of the shortest path), this algorithm will find the shortest path in an optimal way, performing the least number of operations. However, in the case of deep Q-learning, the predictions of the neural network are typically not very precise, and the greedy algorithm can get stuck in local minima.
To overcome this challenge, more advanced graph search algorithms, such as beam search or A-star, are used. This will be discussed further below.


\subsection{Diffusion distance pathfinding - CayleyPy-1}
Let us briefly remind the novel pathfinding approach proposed in our previous paper \cite{chervov2025machinelearningapproachbeats}. Its efficiency has been demonstrated on the Rubik's cube where it significantly outperforms all possible competitors providing shorter solutions than any existing approach for 3x3x3,4x4x4,5x5x5 cubes. It is also more computationally efficient and in some sense it is more simple. 

The idea of the approach is to work not with the true distance on the graph, but with the diffusion distance which might be thought as a length of the random path. The diffusion distance is easy to estimate by the number of steps of random walks. Generation of random walks is much more fast than computations of the neural networks predictions (the only way to generate target in Q-learning), thus generation of the training data is much more efficient than in Q-learning approach, though the training data might be of weaker quality. 

The assumption is that for each node of a graphs there is feature vector attached. For example for permutation groups it is a vector representing a permutation $(p_0,p_1,p_2,...p_{n-1}), p:i \to p_i $
We also assume that there is a selected node (e.g. identity of the group) where we need to find a path from any other node. 

Core of the approach consists of the three steps:

\begin{enumerate}
    \item {\bf Generation of training set by random walks.} 
    Generate $N$ random walks trajectories all starting from a selected node $e$ (identity of a group). Each random walk trajectory consists of up to $K_{\text{max}}$ steps, where $N$ and $K_{\text{max}}$ are integer parameters of the method. For some nodes encountered during the random walks, we store a set of pairs $(v, k)$, where $v$ represents the vector corresponding to the node and $k$ is the number of steps required to reach it via the random walk. 
    \item {\bf Generation of training set by random walks.}  The generated set of pairs $(v, k)$ serves as the training set for the neural network. Specifically, $v$ serves as the 'feature vector' (the input for the neural network), and $k$ represents the 'target' (the output that the network needs to predict). Thus, the neural network's predictions for a given node $v$ estimate the diffusion distance from $v$ to the selected destination node.

    \item {\bf Graph search guided by neural network. Beam search.} This step finds a path from a given node to the destination node. The neural network provides heuristics on where to make the next steps, while the graph pathfinding technique compensates for any possible incorrectness in the neural network predictions. The beam search pathfinding method is quite simple, but has proven to be the most effective for us and works as follows. Fix a positive integer $W$ - a parameter known as the "beam width" (or "beam size"). 
    Starting from a given node, we take all its neighboring nodes and compute the neural network predictions for all of them. We then select the $W$ nodes that are closest to the destination according to the neural network (i.e., the predictions have smaller values). For these selected $W$ nodes, we take their neighbors, drop duplicates, and again compute the neural network predictions, choosing the top $W$ nodes with the best (i.e., minimal) predictions. This process is repeated until the destination node is found (or limit of steps is exceeded).
  
\end{enumerate}

The last step is common for all methods - based on Q-learning or diffusion distance. One uses a neural network predictions as heuristic function and uses graph search algorithms to find a path. We are strongly in favor of beam search, while DeepCube team~\cite{mcaleer2019solving, agostinelli2019solving, khandelwal2024towards,agostinelli2024q} uses A-star algorithm and  develops its modifications.

\subsection{Beam search effectivity - toy model of research community }

To some extent, the success of the proposed scheme is due to beam search and our efficient implementation supporting large beams. We started with a modified greedy search, then moved to Metropolis and A-star, before finally finding relief in beam search. The larger the beam size, the more effective beam search becomes (probability to find path increases and paths become shorter). To maximize beam sizes, we developed an original and efficient PyTorch implementation capable of supporting beam sizes with millions of nodes and beyond (\href{https://www.kaggle.com/code/alexandervc/beamsearch-basic}{basic}, \href{https://github.com/iKolt/cayleypy/blob/main/cayleypy/cayley_graph.py#L447}{advanced} versions). On one hand, beam search is arguably one of the simplest methods. On the other hand, there are several key aspects worth highlighting.

The primary reason we need beam search is to avoid the problem of local minima. Greedy search can get stuck in them, whereas beam search can bypass them entirely if the beam size is larger than the depth of the local minima. It's similar to how an elephant wouldn't even notice small dips in the terrain, while for an ant, they could be insurmountable obstacles.

Another important aspect of beam search is that increasing the beam size not only linearly increases computation but also linearly increases memory consumption. On one hand, typically memory - not computational time - is the main bottleneck of the method. On the other hand, having the largest possible beam size is crucial for its performance.
Moreover, there is no simple way to reproduce the results of beam search with a beam size of $B+1$ using any algorithm that only consumes memory equivalent to beam size $B$.

To explain this point, we use an analogy with the work of a research community.

Imagine two researchers working together on a creative problem (a beam size of 2). The key factor is the exchange of ideas—if one researcher makes a breakthrough, they share it with the other, allowing both to continue their search from the breakthrough point. Without this exchange, the second researcher might get stuck in a dead end. This sharing of ideas helps avoid wasting time on unproductive directions.
The mechanism described above explains why increasing the beam width is so effective—it acts as an exchange of ideas, focusing on breakthroughs rather than following unproductive paths.
Essentially, this is what beam search does: it selects the top nodes across the entire beam neighborhood  rather than picking the best few for each individual node. This means that if the neighbors of one node are significantly closer to the destination (analogous to a breakthrough), those nodes are retained in the beam, while the neighbors of less successful nodes are discarded. In other words, the descendants of some nodes do not survive into the next generation. A kind of natural selection process.

From the above, it is clear that the size of a research community matters. Reducing the community size by half will not simply cut the research output in half — it will decrease it exponentially. That aspect is different from the more routine work - cutting it twice output will be cut twice. The same principle applies to beam search: the beam size is crucial, and there is no simple way to achieve the performance of a beam size 
$B$ using a smaller beam.

This logic also explains why evolutionary or nature-inspired global optimization methods can sometimes be successful. They rely on the same fundamental mechanism as beam search but offer better control over the diversity within the beam.
In some cases, it is important not to immediately jump to breakthroughs made by others but to continue exploring independent directions, as this might lead to even greater discoveries. For example, if medical doctors would stop doing their job to start follow breakthroughs in mathematics, it would lead to disastrous results — diversity is crucial in the long run.
The balance between "everyone follows the latest breakthrough" (as in beam search) and maintaining diversity is delicate. There is no universal solution, as different tasks require different trade-offs. Exploring various methods on Cayley graph pathfinding tasks is an interesting direction.

\subsection{Novel method -  combining diffusion distance with deep Q-learning}
~\\
{\bf Rationale. Why we need that?} The diffusion distance approach described above is quite simple and efficient however there is practical and theoretical disadvantage. The practical side consists of somewhat surprising phenomena that enlarging the training set (generating more random walks) does not always improve the quality of the model - such stagnation was  observed for Rubik's cube cases in our previous paper and it is even more prominent for LRX graphs. It is actually related to the quite clear theoretical fact: diffusion distance is not monotonically dependent on the true distance - one node can be further than another for the true distance, while oppositely ranged for the diffusion distance. (That problem is not only for random walks estimation, but for the diffusion distance itself (i.e. limit of infinite data - computed theoretically (\href{https://www.kaggle.com/code/fedmug/lrx-diffusion-distance-analysis}{analysis}) ). %Such phenomena will be discussed below in more details (see also our n with analysis). 
So diffusion distance is easy to estimate, but it is not precise. On the other hand DQN and Bellman equation allows to approximate the true distance, but for the price of more heavy computations. 
The idea of the proposal below is to combine less precise but light weight diffusion distance with more heavy and more precise Bellman equation approach. And to avoid the sparse reward problem.

Our proposal looks as follows.

{\bf Part 1. Warm-up  diffusion distance training. } Use the diffusion distance approach for preliminary training the neural network. I.e. apply the steps 1,2 above: generate random walks to create training data, and train neural network to predict number of steps of random work.

{\bf Part 2. Modified DQN training. }
The second phase will relies on the Bellman equation strategy with mild modifications, but on somewhat different space exploration strategy comparing to conventional DQN - borrowed from the diffusion distance approach:

Step 1. Generate $N$ non-backtracking random walks trajectories all starting from a selected node $e$ (identity of a group). Each random walk trajectory consists of up to $K_{\text{max}}$ steps, where $N$ and $K_{\text{max}}$ are integer parameters of the method. 

Step 2. For the obtained nodes compute the Bellman equation predictions
$t(g) = 1+\min_{n:~neigbours~of~g} d(n), t(e) =0$ in the standard way. 

Step 3. Clip the predictions by the number of steps of the random work since true distance is always smaller than number of steps. (Clip also predictions negative values). 

Step 4. Run gradient descent for the neural network to minimize the loss function between clipped $t(g)$ (target) and neural network predictions. 

{\bf Part 3. Graph search algorithm - beam search with neural network as an heuristics. } After the neural network is trained we utilize the same approach to find a path by beam search and neural network as in our previous paper.


The advantages of the proposed method are the following: there is no sparse reward problem - since we always start from the selected state, it always allows us to clip overestimated predictions, the warm-up phase allows to avoid long initialization process until when targets obtained by Bellman equation random since network starts from random weights - in our approach targets are always meaningful - first they are steps of random walks, at the second phase neural by itself produces meaningful targets since it was preliminary trained. We also suggest to use non backtracking random walks by rather clear reason - number of steps for non-backtracking is more similar to true distance. Conceptually that is related to the well-known phenomena - "Non-backtracking random walks mix faster"~\cite{alon2007non}.

{\bf Benchmarks and analysis.}
Below we present the analysis of that proposal. 
We observe  stable improvement of the method above comparing to diffusion distance approach, unfortunately the effect is not so big.  

The first experiment is concerned not large groups, where one can compute by brute force the true graph distance and compare it with the predictions of the neural network. We see that for small sizes of the group methods learn practically ideally the distance on a graph, however for large groups correlation becomes not so perfect. Additional training by DQN always improves the preliminary warm-up training by diffusion distance procedure. 

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        $n$  & epochs: 5+0  & epochs: 50+0   & epochs: 5+50   & epochs: 50+50  \\
        \hline
        6 &  0.955 & 0.99  & 0.99  & 0.99 \\
        8 &  0.954 & 0.969 & 0.98   & 0.982 \\
        10 & 0.883  & 0.923 & 0.939   &  0.938 \\
        \hline
    \end{tabular}
    \captionsetup{skip=10pt} 
    \caption{n - permutation length. Spearman correlation of the true distance (obtained by brute force on million states) vs predictions of the neural networks trained by different methods. Conclusion Additional training with DQN improves results. Epochs A+B denotes: A - training epochs in warm-up diffusion distance part, B - epochs in DQN part.  }
    \label{tab:example}
\end{table}

The second experiments concerns larger group $S_{28}$ with LRX generators. We train the model and search for path by the beam search. The length of the path is the measure of performance. Experiment confirms that additional training by DQN improves the results. The combinatation of the 30+200 epochs training with 30-warmup and   200 - second stage (DQN) - is one the best which we observed for that case. Average length is 402, best is 382 which is not far from the expected ideal length 378. (In all experiments we searched path from the same element - conjecturally the longest element for LRX generators). For each experiment models for retrained from scratch, thus due to random nature of the random walks models are significantly different. 
\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|}
        \hline
        Experiment  & epochs:  & epochs:   & epochs: & epochs:    \\
                    &  30+200  &  30+0   & 100+0 &  0+230   \\
        \hline
        1 &  490 & None  & 446  &  None \\% AC
        2 &  396 & 658   & 516  &  None \\% AC
        3 &  382 & None  & None  & None  \\% AC
        4 &  408 & 650   & 436  &  None \\ % AC
        5 &  428 & 706   & 1046  & None  \\ % Firnov
        6 &  396 & None  & None  & 508  \\ % Fironov
        7 &  426 & 890   & 614  &  None \\ % Fironov
        8 &  None& 598   & None &  None \\ %Rustem
        9 &  392 & 464   & 524  &  None  \\ % Shishina
        10 & None& 626   & 452  &  434 \\ %Liuda 
        Median & 402& 650   & 516 & 455.5  \\
        \hline
    \end{tabular}
    \captionsetup{skip=10pt} 
    \caption{Lengths of solutions obtained by different approaches. 
    $S_{28}$ LRX generators. 10 experiments to decompose the "longest" element with expected ideal length 378. Conclusion: additional training by DQN improves the results - both in length and probability to find a path. Pure DQN training is not performing well. }
    \label{tab:example}
\end{table}

The analysis can be found in \href{https://www.kaggle.com/code/alexandervc/lrx-cayleypy-rl-mdqn}{notebook}.  

\section{Benchmarks}

Goal - $S_{110}$. DeepMind's success with Go game with $10^{170}$ states - creates a challenge to achieve similar results in the other fields in particular for Cayley graphs, at the moment we are somewhat far from that, but there are plenty ideas not yet incorporated in our CayleyPy project and optimistically such a goal is achievable in near horizons. Moreover in the present study we achieve pathfinding for such scale groups, but unfortunately not by fully zero prior knowledge. Our zero prior knowledge approaches achieves pathfiding for  LRX generators of $S_{n}$ a bit above $30$, but at the moment  $40$ is a not achieved despite  bunch of neural networks we tried. What is surprising that almost all approaches we tried stuck slightly above $n=30$ - all in the same region, with small variations. It is significantly better than results of computer algebra system GAP which achieves only about $n=20$, which much more lengths and timings. 
%But somewhat far from the desirable goal

\subsection{One line of code to get orders of magnitude improvement from: $S_{30}$ to $S_{100}$. }

\texttt{if action == 'X' and current-state[0] < current-state[1]: continue }
The condition used in the beam search part. Its logic is rather clear - if the contents of positions 0 and 1 are already sorted - it is logical not to swap them i.e. not to apply $X$ generator. 

Surprisingly such a simple conditions is a game changer. Inserting it and using it with exactly the same pipelines which performed for around $n=30+$ we  are able to find paths for $n=100$ and (apparently more - just stopped by 16G RAM limit). So the size of graphs changes from $10^30+$ to $10^150+$.  It works the same for all models: perceptrons, gradient boostings, convolutional neural networks.  

The challenge is how to avoid such prior knowledge method and extend it to other generators.

It should also be noted that this condition prevents to find optimal paths. And for small sizes of $n \le 15 $ it typically leads to a bit longer paths, but for higher values of $n$ it is a relief.


Examples of lengths found by that method are presented
at table \ref{tab:lens_upto100} below:

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|}
        \hline
        $S_n$   & 100 & 90   & 80  & 70 & 60  \\
        \hline
        length        & 16632 & 33459  & 7510  & 8115 & 3650 \\
        \hline
        ideal length &  4950   & 4005  & 3160  & 2415 & 1770 \\
        \hline
        Time         &  8h 43m & 3h 57m   & 1h 5m  & 52m & 3h  \\
        \hline
        Nb version   &   255   &  242     & 241    & 233  & - \\
        \hline
    \end{tabular}
    \captionsetup{skip=10pt} 
    \caption{n - permutation length. Lengths of solutions for different $n$ obtained by ML model combined followed by beam search with "X-condition". (Not completely zero prior knowledge approach).    }
    \label{tab:lens_upto100}
\end{table}

Computation for $n=100$ took 8h 43m 53s GPU P100. Beam width $ {2}^{20}$, 30 epochs of training. ( \href{https://www.kaggle.com/code/alexandervc/lrx-cayleypy-rl-mdqn?scriptVersionId=223954525}{Notebook version 255}), for other cases we used beam size not more than $ {2}^{18}$, that is why for $n=90$ length is much bigger. The data for $n=60$ is obtained by the gradient boosting model, with slightly better lengths, but more timing, in the other cases single layer MLP model used. In all cases we decompose the conjecturally longest element of expected ideal length $n(n-1)/2$. 

\subsection{"Overcome the GAP" with zero prior knowledge AI. }

We use LRX Cayley graph of $S_n$ and its conjecturally longest element of expected length $n(n-1)/2$ as a benchmark. Below we demonstrate that AI methods significantly outperform classical methods of computer algebra system GAP. 

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|}
        \hline
        $n$  &  GAP    & Ideal            &   AI      \\
             &  Length & Length n(n-1)/2  & Length    \\
        \hline
        9    &  41     & 36               &  36   \\
        \hline
        10   &    51   & 45               &  45       \\
        \hline
        11   &     65      & 55             &  55     \\
        \hline
        12   &    78       &  66          & 66     \\
        \hline        
        13   &    99       &  78           &   78       \\
        \hline        
        14   &   111       &  91           &   91     \\
        \hline        
        15     &   268      &  105         &  105    \\
        \hline        
        16     &    2454    &  120         &  120     \\
        \hline        
        17   &     380     &  136         & 136    \\
        \hline        
        18   &     20441   &   153        & 153    \\
        \hline        
        19   &      3187      & 171        & 171    \\
        \hline      
        20   &     217944    & 190        & 190      \\
        \hline 
        21   &   -           & 210     & 210        \\
        \hline           
    \end{tabular}
    \captionsetup{skip=10pt} 
    \caption{n - permutation length. Comparison of GAP vs AI methods (zero prior knowledge). On the conjecturally longest element of LRX Cayley graph. (Its expected "ideal" length is $n(n-1)/2$.)  }
    \label{tab:example}
\end{table}
For $n=20$ GAP timing is 41min 18s,  while AI methods can find results much faster.
For example, simple neural network: \href{https://www.kaggle.com/code/alexandervc/lrx-cayleypy-rl-mdqn?scriptVersionId=224270083}{notebook version 423}: 3m 48s GPU P100. GAP benchmarks has been performed in the \href{https://www.kaggle.com/code/avm888/group-elements-decomposition-gap-limits}{notebook}. 

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|}
        \hline
        $n$  &   Ideal            &   AI      \\
             &   Length n(n-1)/2  & Length    \\
        \hline
        22    &       231           & 231   \\
        \hline
        23    &       253           & 253   \\
        \hline
        24    &      276            & 276   \\
        \hline
        25    &    300              & 300   \\
        \hline
        26    &     325             & 325   \\
        \hline
        27    &     351             & 351   \\
        \hline
        28    &     378             & 378   \\
        \hline
        29    &   406               & 408   \\
        \hline
        30    &     435             & 437   \\
        \hline
        31    &    465              & 478   \\
        \hline
        32    &    496              & 728   \\
        \hline
        33    &    528              & 642   \\
        \hline           
    \end{tabular}
    \captionsetup{skip=10pt} 
    \caption{n - permutation length. Comparison of GAP vs AI methods.   }
    \label{tab:example_nn}
\end{table}

The AI results above are aggregated results from experiment with different models and approaches. In particular based on gradient boostings and neural networks, all of them quite stably and perfectly solve cases with  $n\le 30$, but going beyond that  requires efforts for all our approaches.%, and in several steps in $n$ they loose ability to find a path. 
It is surprising that all types of models have the problem at the same range of $n$ and that length is not becoming gradually longer and longer, but  it  stays almost perfect, and then  quite sharply pipeline loses ability to find a path at all. For some cases  we use several reruns - if the path not found via single run, it sometimes be found by multiple relaunches of the pipeline. 
For example $n=32$ has been solved with our modified DQN strategy with 30 warmup epochs and 1000 DQN epochs (\href{https://www.kaggle.com/code/rustemturtayev/lrx-cayleypy-rl-mdqn-d44272?scriptVersionId=223961901}{notebook}), time: 2h 53m 53s GPU P100. 

\subsection{Architectures and parameter dependence. }
The tests of different architectures and parameters is discussed below.
The main conclusion that we observe that rather simple models like MLP and gradient boostings currently perform better, than more advanced: transformers, convolutional neural networks etc. One exception is one of our transformer models that can find paths up to $n=100$, however it is not zero prior knowledge - it uses features specific to LRX generators. Dependence on various parameters  was explored in our previous paper ~\cite{chervov2025machinelearningapproachbeats} for the case of the Rubik's cube group, the conclusions were that beam size is one the most important parameter the length of solution is almost linearly improving on logarithm of beam size, deeper neural networks are better than wider especially, but the effect of depth is better seen on small for higher cubes, we also observed stagnation of performance with increase of training set.  

For the LRX Cayley graph these phenomenons are less strongly expressed, or even negligible. 
Most experiments with deep neural networks have not lead to significant improvements over simple one layer perceptron with 128 neurons. Beam size effect is present, in but less expressed (see table below). 

{\bf Nonbacktracking random walks have quite strong effect.} In particular one can train models in just single epoch within a minute for e.g. $S_{20}$. We actually use improved version of the nonbacktracking based on the following ideas: 1) we forbid not only 1 step in history but typically 32 steps (it is a parameter) 2) we consider a bunch of trajectories and ban states for all of them at once 3) we ban not only visited states but their neighbors. The price for that is slowing down the generation, but that is not so important since gradient descent takes much longer time, and overall there is no significant slow down. 

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|}
        \hline
        Beam   &  Solution Length             &  Solution Length      \\
       Width   &  Nonbacktracking RW  & Simple RW    \\
        \hline
        2**10    &       None         & None   \\
        \hline
        2**12    &       252           & None   \\
        \hline      
        2**14    &       202           & None   \\
        \hline    
        2**16    &       194           & None   \\
        \hline 
        2**18    &       194           & None   \\
        \hline  
        2**20    &       190 (Ideal)   & None   \\
        \hline        
    \end{tabular}
    \captionsetup{skip=10pt} 
    \caption{Without non-backtracking system cannot perform well on single epoch training. Effect of beam size is clear, but not so strong.   }
    \label{tab:example_nn}
\end{table}


We generate 10000 trajectories at once each of the length 190, by the generalized nonbacktracking procedure above and train the neural network with batch size 1024 just for single epoch.
Entire time for each computation - both training and beam search are 1-5 minutes, experiments can be found in versions 427-430 of the \href{https://www.kaggle.com/code/alexandervc/lrx-cayleypy-rl-mdqn?scriptVersionId=224294767}{notebook}.

Theoretical reason why non-backtracking is helpful is clear. Imagine our graph is a tree, then even for non-backtracking  random walk number of steps coincide with the true distance (length of the shortest path). For complicated graphs it is not so, but still it more close to true distance and has less variability. I.e. the situation when the same node can be achieved in different number of steps - the fact which    equivalent to having a  noise in the training data and which makes models less precise. 

{\bf CNN.}
We also analyse convolutional neural networks - however they have  slower training process.  They show similar  results comparing to perceptrons
e.g. the length of the solution for $n=16$ is 122 (ideal is 120), for $n=20$ is 216 (ideal is 190). The basic version of the architecture uses two 1D CNN layers with Batch Normalization and Dropout to avoid overfitting. One of the successful variations is using temporal gated CNN. The first part of the architecture is the residual block - several convolutional blocks with gating based on sigmoid activation function. The second part of the architecture is stacking several layers of residual blocks with gradually increased dilation to increase the size of the receptive field.
Notebooks: \href{https://www.kaggle.com/code/artgor/encoding-as-permutation-matrix-1d-cnn}{1}, \href{https://www.kaggle.com/code/artgor/encoding-as-permutation-mat-simple-2d-cnn}{2}, \href{https://colab.research.google.com/drive/1oWuQg33bveHYqVET4IES_EWIH1nsa4yi}{3},\href{https://colab.research.google.com/drive/1oWuQg33bveHYqVET4IES_EWIH1nsa4yi}{4}.

{\bf Transformer.} We also peformed experiments with a transformer model, however it was able to solve only not so big groups like $n\le 15$. 
That model incorporated standard Transformer components, including multi-head self-attention, positional encoding, feed-forward layers, and dropout to mitigate overfitting. However, the model struggled to converge to a satisfactory solution, even for relatively short sequences. This issue became increasingly severe as the sequence length grew. For instance, with size = 9 (corresponding to path length = 107 and ideal = 36) or n-size = 13 (path length = 332, ideal = 78), increasing the projection dimension to 512 or 1024, adding more layers, or training for more epochs failed to address the underlying challenges. Notebooks: \href{https://www.kaggle.com/code/nursmen/lrx-transformer-training}{1}, \href{https://www.kaggle.com/code/nursmen/lrx-transformer-use}{2}.


{\bf Transformer based on LRX specific features.}
The features inspired by Hamming distance were used:  difference between an element and its position;  difference with the left neighbor;  difference with the right neighbor.  A standard multihead attention with one encoder layer and mlp was used to estimate the number of steps required to reach the ordered state. The transformer demonstrated the ability to handle sequences up to N=15 without using beam search. For larger N, beam search is employed, and the maximum N for which results were obtained was 100. The model and features only indirectly depend on N, therefore, when trained on micro batches constructed as random walks from an ordered permutation for N*(N-1)/2 moves, it can be used for different N values, both larger and smaller.  (\href{https://www.kaggle.com/code/sergeifironov/permutation-transformer-solver-100}{Notebook}). 

{\bf Gradient boostings.} We also applied CatBoost and XGboost in our diffusion distance pipeline, surprisingly they perform not worse than neural networks (it was not the cases for Rubik's cube - where boosting were not able to solve 4x4x4, 5x5x5 cubes), and even giving a bit shorter solutions. 
(\href{https://www.kaggle.com/code/antoninadolgorukova/lrx-gbm-powered-pathfinding}{Code}).

\subsection{Reinforcement learning on "small graphs"}
At the preliminary stage of the research we performed experiments with RL/dynamical programming methods for small graphs. In particular the question was how the initialization influence the performance. That is important to understand in view of our proposal to use initialization coming from the diffusion distance.
These experiments revealed interesting phenomenons presented at figure \ref{fig:pearson}. 
For some specific initialization the performance first degrades, and only after that achieves the convergence to the true distance which is guaranteed by the general theory. The number of steps for convergence is not more than diameter of the graph is initialization is positive, however can be larger in presence of negative values. Initialization coming from the machine learning model ("catboost") trained on diffusion distance (same as "warmup step" in our proposal) improves the convergence, though not in a radical manner. 

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\linewidth]{figs/pearson.png}
    \caption{Pearson correlation between $d_i(s)$ and $d(s)$ on each iteration of the DP algorithm, $\epsilon = 10^{-3}$}\label{fig:pearson}
\end{figure}

\section{Mathematical contributions}
In the section we prove lower and upper bounds for the diameter and for particular elements, describe decomposition of the conjecturally longest element of the desired length $n(n-1)/2$, describe algorithms to decompose elements in LRX generators and formulate several conjectures on the growth, random walks and spectrum of the LRX Cayley graph. 

\subsection{LRX Cayley graph}
LRX generators are:  L - left cyclic shift, R - right cyclic shift, X - eXchange (flip) of positions 0,1. It can be considered as a relative of the TopSpin and Hungarian Ring puzzles (figure \ref{fig:LRX_TopSpin_HR}). Despite its simplicicity its structure is not fully understood. In particular 
according to \href{https://oeis.org/A186783}{OEIS-A186783}  the diameter conjecture $n(n-1)/2$ remains open. 

Figure \ref{fig:LRX1} is a visualization of the LRX Cayley graph for $S_5$ which shows its complicated structure. More visualizations by the minimal distortion method  can be found at \href{https://www.kaggle.com/code/iggisv9t/drawing-cayley-graphs-with-mds}{notebook}, and by originally modified force directed layout \href{https://projects.interacta.io/graphs/IM_S_N_4.csv}{here}.

\begin{figure}[h!]
   \centering
   \includegraphics[width=1.0\linewidth]{figs/LRX_TopSpin_HR.jpg}
   \caption{LRX and related mechanical puzzles}\label{fig:LRX_TopSpin_HR}
\end{figure}


\subsection{Long and the longest element}

\begin{figure}[h!]
   \centering
   \includegraphics[width=1.0\linewidth]{figs/LongEl.jpg}
   \caption{Permutations arising from dihedral symmetries - are provebly long elements. The longest is conjecturally one of them. Surprisingly it is not the full inversion.}\label{fig:LongEl}
\end{figure}

We created efficient implementation of the brute force breadth first search algorithm \href{https://www.kaggle.com/code/ivankolt/lrx-4bit-uint64}{notebook} using bit operations over int64 and were able to traverse all the permutations for $n \le 14$, outperforming OEIS data by one step. From that computation we observed that the longest element is unique and have a specific form. It can be described as a symmetry of a regular polygon (figure \ref{fig:LongEl}), which is orthogonal to 0-1 edge. Or explicitly as product of transpositions $(0,1)(n-1,2)(n-2,3)(n-3,3)...$. We conjecture that this element is unique longest element and it has length $n(n-1)/2$. 
Surprisingly  not the full inversion ($i\to n-i-1$) is the longest. 

As we will prove below actually all the permutations which correspond to  symmetries of a regular polygon are also long elements.  


\begin{proposition}
    The element $(0,1)(n-1,2)(n-2,3)(n-3,3)...$ (conjecturally the longest)
    has the following explicit decomposition into product of LRX generators:
\begin{equation}
\label{eq:long}
\prod_{i=1}^{[ (n-1)/2]} ( X L^{(-1)^{(i-1)}} )^i 
\prod_{i=([ n/2]-1)}^{1} (L^{(-1)^{i+[(n-1)/2] - [n/2] }} X )^i R^{[n/2]}
\end{equation}    
\end{proposition}
The proof will be given elsewhere. The decompositions of that type were first found computationally: \href{https://www.kaggle.com/code/luxoove/lrx-optimal-algorithm-two-ways-bubble-sort}{notebook}.

%\subsection{The long elements,  and the formula for the longest one}


\subsection{Lower bound on the diameter}
Below, we provide a lower bound on the diameter using a combinatorial argument. The key idea is that cyclic shifts preserve the cyclic order of any given triple of numbers. Therefore, for each triple where the order is disrupted, t
$X$ is required  in the decomposition.   The symmetries of a regular polygon, as described above, correspond precisely to the permutations that maximize changes in cyclic order.   (The code to generate these long elements: \href{https://www.kaggle.com/code/alexandervc/lrx-long-elements-f-petrov-oeisa186752}{notebook}.) 

\begin{proposition}
    Diameter of LRX graph is larger or equal $n(n-1)/2-n/2-1$ 
\end{proposition}

At first, we reformulate the setup as follows.

The numbers $1,\ldots,n$  are written in the vertices of a regular $n$-gon $A_1A_2\ldots A_n$, each number being written once. At each move, one can either rotate the arrangement by $2\pi/n$  counterclockwise or clockwise, or switch the numbers at $A_1$ and $A_2$.

For two arrangements $\pi_1,\pi_2$ denote by $d(\pi_1,\pi_2)$ the minimal number of moves which suffice to get $\pi_2$ from $\pi_1$.

Claim. If $\pi_2$ is obtained from $\pi_1$ by an axial symmetry, then $d(\pi_1,\pi_2)\geqslant n^2/2-n-1$.

Proof. Assume that $\pi_2$ is obtained from $\pi_1$ by several moves. Call two numbers $a, b\in \{1,\ldots,n\}$ friends, if they were switched odd number of times. Since for every three numbers $a, b, c$ the orientation of a triangle formed by these numbers changed, there is odd number of pairs of friends between $a, b, c$. Let $A$ denote the set of all non-friends of element 1, $B$ the complement  of $A$, i. e. $B$ consists of 1 and its friends. Our condition yields that all elements of $A$ are mutual friends, and so are all elements of $B$, but there is no friendship between $A$ and $B$. Thus, the total number of pairs of friends is $n(n-1)/2-|A|\cdot |B|\geqslant n(n-1)/2-n^2/4$. Thus there were at least as many switches. Between every two switches there was a rotation (otherwise we do something useless). Totally, the number of operations is not less than $n(n-1)-n^2/2-1=n^2/2-n-1$.

\subsection{Upper bound on the diameter and the algorithms }
We develop two algorithms which can decompose any permutation in the product of LRX generators. For the first we prove  complexity bound $n(n-1)/2+3n$, thus bounding diameter from above.  The code and tests for the first algorithm can be found in \href{https://www.kaggle.com/code/mixnota/article-project}{notebook}.

Its brief description is the following:

The algorithm decomposes a permutation into the product of cycles. After that, it works with each cycle \( (a_1, a_2, \dots, a_n) \) in the following way:

1) We compute the permutation corresponding to the cycle.

2) We initialize the special variable \( x \).

3) The element \( a_1 \) is placed in the \( a_n \)-th position, and we perform a sequence of elementary transpositions to move \( a_1 \) from the \( a_n \)-th position to the \( a_1 \)-th position. During each elementary transposition, we either increase or decrease \( x \) by 1. The sign of the change depends on the direction of the elementary transposition. Essentially, \( x \) represents the current position of the transposition, but if our transposition is \( (1, N) \), the change in \( x \) is also \( \pm1 \), even though the position changes from 1 to \( N \) or from \( N \) to 1.

4) Next, we attempt to restore the position of \( a_2 \) and compute the sequence of elementary transpositions that move \( a_2 \) to the \( a_2 \)-th position. We apply the same operations to \( x \), and so on.

5) We repeat these actions for every element of the cycle.

6) In certain situations, the elementary transposition should be replaced by \( L \) or \( R \) (depending on the direction in which we are moving the current element). This replacement should occur if and only if the variable \( x \) satisfies the condition:

\[
x - c = \pm (N - 1)
\]


Complexity estimation.
First, when decomposing a cycle into transpositions, we do not shift anything by more than 1, except for elements that have already been affected by the cycle.
Second, when moving an element from position \( i \) (or \( i \pm 1 \)) to position \( \sigma(i) \), we perform a sequence of at most \( \text{dist}(i, \sigma(i)) + 1 \) transpositions.
Third, the transition from \( i \to \sigma(i) \) to \( \sigma(i) \to \sigma(\sigma(i)) \) takes at most one rotation.
Thus, we perform at most  
\[
\sum 2(\text{dist}(i, \sigma(i)) + 1) + (\text{number of elements in the cycle})
\]
operations. The transitions between cycles and returning back will take at most \( n \) rotations.
Therefore, the entire decomposition takes no more than  
\[
2n + \sum 2(\text{dist}(i, \sigma(i)))
\]
operations.

Finally, after minimization,
\[
\sum \text{dist}(i, \sigma(i)) \leq \frac{n^2}{4}
\]
which gives a total of  
\[
2n + \frac{n^2}{2} + \frac{n}{2}
\]
operations.



We also develop a second algorithm: \href{https://www.kaggle.com/code/luxoove/top1-lrx-inversions-based-algorithm}{notebook}. Currently it is our top performing algorithm from the practical tests, we expect its complexity  is bounded by $n(n-1)/2+n/2$, however it is not yet proved.

It might be expected that optimal algorithm with polynomial complexity can be developed, but it is not achieved yet. 

\subsection{Conjectures: Gumbel for growth, spectrum uniformity, random walks mixing, etc. }

Based on explicit computations for the growth for $n \le 14$ and its analysis (notebooks \href{https://www.kaggle.com/code/ogurtsov/gumbel}{1} \href{https://www.kaggle.com/code/ogurtsov/gumbel-for-binary-puzzle}{2} ) we come to the following conjecture, which can be thought as an analogue of the central limit theorems. It is in the vein of works by P.Diaconis et.al. ~\cite{diaconis1977spearman, diaconis1988metrics,chatterjee2017central} where for Coxeter generators growth and some other statistics were shown to follow Gaussian normal law in a limit. New point of our conjecture - appearance of asymmetric Gumbel distribution. Such asymmetry is quite natural - for the randomly chosen generators growth will be highly asymmetric with exponential growth and sharp abrupt (\href{https://mathoverflow.net/q/322877/10446}{Rubik's cube growth} is a prototypical example), while for Coxeter generators which are nearly commutative - growth is symmetric Gaussian close to commutative case. Slight asymmetry of growth for LRX generators indicates its intermediate nature between "exponential growth"/"commutative-like Gaussian growth" cases.  

{\bf The growth - Gumbel.} The growth distribution of the LRX graph follows the Gumbel distribution for large $n$.

\begin{figure}[h!]
   \centering
   \includegraphics[width=1.0\linewidth]{figs/LRX_spec.png}
   \caption{Spectrum distribution of the LRX graphs. }\label{fig:LRX-spec}
\end{figure}

The analysis of the spectrum of LRX graphs is performed in \href{https://www.kaggle.com/code/nikolenkosergei/spectrum-analysis}{notebook}. From the figure above it is natural to expect that spectrum tends to uniform distribution.

We have computed all possible shortest paths from the conjecturally longest element to the identity by methods of the dynamical programming (\href{https://www.kaggle.com/code/fedmug/lrx-longest-paths}{notebook} ). And after that analyzed averaged trajectories (\href{https://www.kaggle.com/code/antoninadolgorukova/lrx-sorting-networks}{notebook}) - which are analogs of random sorting networks~\cite{angel2007random}. The figure \ref{fig:LRX-sort} below represents the results. It seems the pattern is different from the "sine curves" discovered in~\cite{angel2007random}. 

\begin{figure}[h!]
   \centering
   \includegraphics[width=1.0\linewidth]{figs/LRX_sorting.jpg}
   \caption{Analogs of "random sorting networks" for LRX. Trajectories of individual elements under average of all possible shortest trajectories between longest node and identity. }\label{fig:LRX-sort}
\end{figure}

% We also numerically checked for even  $n \le 9$ (\href{https://www.kaggle.com/code/fedmug/lrx-shifts-starting-with-x}{notebook}) the following conjecture. It   would imply precise diameter lower bound $n(n-1)/2$. 

% {\bf Conjecture.} If the shortest  decomposition of an element starts from $X$, then its multiplications by $L^k$ $k<n/2$  have all different lengths.  

% Such a conjecture imply that half of the symmetries of the regular polygon have different lengths, but since their lengths greater than $n(n-1)/2-n/2$ one of the would have length $n(n-1)/2$ which is desired diameter. 

P.Diacons and L.Soloff-Coste studied random walks on LRX Cayley~\cite{diaconis1993comparison}. They estimated that mixing time is between
$n^2$ and $n^3log(n)$. We performed numerical experiments to make more precise conjecture (notebooks \href{https://www.kaggle.com/competitions/lrx-oeis-a-186783-brainstorm-math-conjecture}{1},\href{https://www.kaggle.com/code/bbbxttt/wip-symmetric-groups-with-lrx-random-walk?scriptVersionId=220660303}{2}) unfortunately  the achieved range of $n$ was not enough to draw conclusions, however it seems that mixing time is more close to $n^3log(n)$, moreover even for non-backtracking random walks, which is a bit surprising since they typically mix faster.

Finally we considered the Schreier coset graph for the action of $S_{2n}$ on binary strings with $n$ zeros and $n$ ones. (Analogue of $Gr(n,2n)$ over the "field with one element"). Efficient  \href{https://www.kaggle.com/code/ivankolt/lrx-4bit-uint64-coset}{code} which stores such strings in int64 allowed us to completely traverse the graph up to $n=42$ and compute its growth.
The analysis of the data presented in the 
\href{https://www.kaggle.com/code/ogurtsov/gumbel-for-binary-puzzle}{notebook}. 
Based on that it is tempting to conjecture that the diameters of such graphs are between $n^2/6$ and $n^2/5$, while growth again can be approximated by the Guimbel distribution.



\section{Code availability}

The code is available at the Kaggle platform, where it can be easily launched:
\href{https://www.kaggle.com/competitions/lrx-oeis-a-186783-brainstorm-math-conjecture/code}{LRX OEIS-A186783 brainstorm math conjecture}

The link above leads to one of  the three public Kaggle challenges which we created  to stimulate the research and interplay between artificial intelligence and mathematical communities, the other two:
\href{https://www.kaggle.com/competitions/lrx-binary-in-search-of-gods-number}{LRX discover math and God's algorithm}, \href{https://www.kaggle.com/competitions/lrx-discover-math-gods-algorithm}{LRX-binary: in search of God's number}. Kaggle infrastructure provides a convenient way to benchmark and compare different algorithms - just making the submissions to these challenges. The code, the data and discussions are stored at the same place. Code can be executed for free on Kaggle cloud servers. The first challenge asks to decompose  the conjecturally longest elements and to find their shortest decompositions - participants already achieved the length $n(n-1)/2$ an so if believe in the conjecture the optimum is found, if we do not believe - not. The second challenges asks to decompose random permutations and aims to find optimal algorithms (at the moment of writing optimum is not achieved), the third one asks the same but for the binary strings acted by LRX generators. 




% \section{Acknowledgments}
\section*{Acknowledgements}
A.C. is deeply grateful to M. Douglas for his interest in this work, engaging discussions, and invitation to present preliminary results at the Harvard CMSA program on "Mathematics and Machine Learning" in Fall 2024, to M. Gromov, S. Nechaev, and V. Rubtsov for their invitation to make a talk (\href{https://youtu.be/RkmBwlSyhfA?si=KgqQtRFaqx5ykd_s}{video}) at "Representations, Probability, and Beyond: A Journey into Anatoly Vershik World" at IHES, as well as for stimulating discussions. A.C. is grateful to J. Mitchel for involving  into the Kaggle Santa 2023 challenge, from which this project originated and to M.Kontsevich, Y.Soibelman,  S.Gukov, A. Hayat, T. Smirnova-Nagnibeda,  D.Osipov, V. Kleptsyn, G.Olshanskii, A.Ershler, J. Ellenberg, G. Williamson, A. Sutherland,  Y. Fregier, P.A. Melies, I. Vlassopoulos, F.Khafizov, A.Zinovyev,  H.Isambert for the discussions, interest and comments, to his wife A.Chervova and daugther K.Chervova for support, understanding and help with computational experiments.  

We are deeply grateful to the many colleagues who have contributed to the CayleyPy project at various stages of its development, including: N.Bukhal, J.Naghiev, A.Lenin, E.Uryvanov,  A. Abramov, M.Urakov, A.Kuchin,  B.Bulatov,  F.Faizullin, A.Aparnev, O.Nikitina, A.Titarenko, U.Kniaziuk, D.Naumov, A.Krasnyi, S.Botman, A.Kostin,
R.Vinogradov, I.Gaiur, 
% added
% D.Gorodkov, A.Rozanov,V.Nelin,  A. Ogurtsov, A. Trepetsky, A. Dolgorukova, S. Lytkin, S. Ermilov, L. Grunvald, A. Eliseev, G. Annikov, M. Evseev, F. Petrov, N. Narynbaev, S. Nikolenko, S. Krymskii, R. Turtayev, S. Kovalev, N. Rokotyan, G. Verbyi, L. Shishina, A.Korolkova, D.Mamaeva,  S.Fironov, A.Lukyanenko,
K.Yakovlev, V.Shitov, E.Durymanov, A.Kostin, R.Magdiev, M.Krinitskiy, P.Snopov. 







\begin{thebibliography}{10}

% \bibitem{ChoquetChristodoulou}   Y. Choquet-Bruhat and D. Christodoulou, ``Existence of global solutions of the Yang-Mills, Higgs and spinor field equations in $3+1$ dimensions'', Annales scientifiques de l'\'{E}.N.S, 4 s\'erie, (1981), 481-500.

\bibitem[Douglas22]{douglas2022machine}
Douglas, Michael R. "Machine learning as a tool in theoretical science." Nature Reviews Physics 4.3 (2022): 145-146.

\bibitem[Charton19]{lample2019deep}
Lample, Guillaume, and François Charton. "Deep learning for symbolic mathematics." arXiv preprint arXiv:1912.01412 (2019).

\bibitem[Davies21]{davies2021advancing}
Davies, A., Veli{\v{c}}kovi{\'c}, P., Buesing, L., Blackwell, S., Zheng, D., Tomašev, N., ...  Kohli, P. (2021). Advancing mathematics by guiding human intuition with AI. Nature, 600(7887), 70-74.

\bibitem[Bao23]{bao2021polytopes}
Bao, J., He, Y. H., Hirst, E., Hofscheier, J., Kasprzyk, A., Majumder, S. (2023). Polytopes and machine learning. International Journal of Data Science in the Mathematical Sciences, 1(02), 181-211.


\bibitem[Romera24]{romera2024mathematical}
Romera-Paredes, B., Barekatain, M., Novikov, A., Balog, M., Kumar, M. P., Dupont, E., ...  Fawzi, A. (2024). Mathematical discoveries from program search with large language models. Nature, 625(7995), 468-475.

\bibitem[Coates23]{coates2024machine}
Coates, T., Kasprzyk, A.,  Veneziale, S. (2023). Machine learning detects terminal singularities. Advances in Neural Information Processing Systems, 36, 67183-67194.

\bibitem[Hayat25]{alfarano2024global}
Alfarano, A., Charton, F.,  Hayat, A. (2025). Global Lyapunov functions: a long-standing open problem in mathematics, with symbolic transformers. Advances in Neural Information Processing Systems, 37, 93643-93670.

\bibitem[Williamson24]{charton2024patternboost}
Charton, F., Ellenberg, J. S., Wagner, A. Z.,  Williamson, G. (2024). PatternBoost: Constructions in mathematics with a little help from AI. arXiv preprint arXiv:2411.00566.

\bibitem[Gukov24]{shehper2024makes}
Shehper, A., Medina-Mardones, A. M., Lewandowski, B., Gruen, A., Kucharski, P.,  Gukov, S. (2024). What makes math problems hard for reinforcement learning: a case study. arXiv preprint arXiv:2408.15332.

\bibitem[Kohli25]{swirszcz2025advancing}
Swirszcz, G., Wagner, A. Z., Williamson, G., Blackwell, S., Georgiev, B., Davies, A., ... Kohli, P. (2025). Advancing Geometry with AI: Multi-agent Generation of Polytopes. arXiv preprint arXiv:2502.05199.

\bibitem[AlphaGo16]{silver2016mastering}
Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., ...  Hassabis, D. (2016). Mastering the game of Go with deep neural networks and tree search. nature, 529(7587), 484-489.

\bibitem[AlphaZero17]{silver2017mastering}
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., ... Hassabis, D. (2017). Mastering the game of go without human knowledge. nature, 550(7676), 354-359.

\bibitem[Chervov25]{chervov2025machinelearningapproachbeats}
Chervov, A., Khoruzhii, K., Bukhal, N., Naghiyev, J., Zamkovoy, V., Koltsov, I., ...  Romanov, A. (2025). A Machine Learning Approach That Beats Large Rubik's Cubes. arXiv preprint arXiv:2502.13266.

\bibitem[Babai89]{babai1989small}
Babai, L., Kantor, W. M.,  Lubotsky, A. (1989). Small-diameter Cayley graphs for finite simple groups. European Journal of Combinatorics, 10(6), 507-522.

\bibitem[Kuppili20]{kuppili2020upper}
Kuppili, S. S., Chitturi, B., Ravella, V. V.,  Datta, C. P. (2020, December). An Upper Bound for Sorting R n with LRE. In International Advanced Computing Conference (pp. 283-295). Singapore: Springer Singapore.

\bibitem[Diaconis77]{diaconis1977spearman}
Diaconis, P., Graham, R. L. (1977). Spearman's footrule as a measure of disarray. Journal of the Royal Statistical Society Series B: Statistical Methodology, 39(2), 262-268.

\bibitem[Diaconis88]{diaconis1988metrics}
Diaconis, P. (1988). Metrics on Groups, and Their Statistical Uses. In Group representations in probability and statistics (Vol. 11, pp. 102-131). Institute of Mathematical Statistics

\bibitem[Diaconis17]{chatterjee2017central}
Chatterjee, S.,  Diaconis, P. (2017). A central limit theorem for a new statistic on permutations. Indian Journal of Pure and Applied Mathematics, 48(4), 561-573.

\bibitem[Virág]{angel2007random}
Angel, O., Holroyd, A. E., Romik, D.,  Virág, B. (2007). Random sorting networks. Advances in Mathematics, 215(2), 839-868.

\bibitem[Diaconis93]{diaconis1993comparison}
Diaconis, P.,  Saloff-Coste, L. (1993). Comparison techniques for random walk on finite groups. The Annals of Probability, 2131-2156.

\bibitem[Goldreich81]{even1981minimum}
Even, S., Goldreich, O. (1981). The minimum-length generator sequence problem is NP-hard. Journal of Algorithms, 2(3), 311-313.

\bibitem[Jerrum85]{jerrum1985complexity}
Jerrum, M. R. (1985). The complexity of finding minimum-length generator sequences. Theoretical Computer Science, 36, 265-289.

\bibitem[Jerrum85]{jerrum1985complexity}
Jerrum, M. R. (1985). The complexity of finding minimum-length generator sequences. Theoretical Computer Science, 36, 265-289.

\bibitem[Demaine17]{demaine2017solving}
Demaine, E. D., Eisenstat, S.,  Rudoy, M. (2017). Solving the Rubik's Cube Optimally is NP-complete. arXiv preprint arXiv:1706.06708.

\bibitem[Bulteau15]{bulteau2015pancake}
Bulteau, L., Fertin, G.,  Rusu, I. (2015). Pancake flipping is hard. Journal of Computer and System Sciences, 81(8), 1556-1574.

\bibitem[Veličković24]{wilson2024cayley}
Wilson, J. J., Bechler-Speicher, M., Veličković, P. (2024). Cayley graph propagation. arXiv preprint arXiv:2410.03424.

\bibitem[Korf97]{korf1997finding}
Korf, R. E. (1997, July). Finding optimal solutions to Rubik's Cube using pattern databases. In AAAI/IAAI (pp. 700-705).

\bibitem[Rokicki14]{Rokicki2014Diameter}
Rokicki, T., Kociemba, H., Davidson, M.,  Dethridge, J. (2014). The diameter of the rubik's cube group is twenty. siam REVIEW, 56(4), 645-670.

\bibitem[Sims70]{sims1970computational}
Sims, C. C. (1970, January). Computational methods in the study of permutation groups. In Computational problems in abstract algebra (pp. 169-183). Pergamon.

\bibitem[Knuth91]{knuth1991efficient}
Knuth, D. E. (1991). Efficient representation of perm groups. Combinatorica, 11(1), 33-43.

\bibitem[Shamir89]{fiat1989planning}
Fiat, A., Moses, S., Shamir, A., Shimshoni, I.,  Tardos, G. (1989, October). Planning and learning in permutation groups. In 30th Annual Symposium on Foundations of Computer Science (pp. 274-279). IEEE Computer Society.

\bibitem[Mulholland16]{mulholland2016permutation}
Mulholland, J. (2016). Permutation puzzles: a mathematical perspective. Departement Of mathematics Simon fraser University.

\bibitem[Babai88]{babai1988diameter}
Babai, L.,  Seress, Á. (1988). On the diameter of Cayley graphs of the symmetric group. Journal of combinatorial theory, Series A, 49(1), 175-179

\bibitem[Babai04]{babai2004diameter}
Babai, L., Beals, R.,  Seress, Á. (2004, January). On the diameter of the symmetric group: polynomial bounds. In SODA (pp. 1108-1112).


\bibitem[Seress14]{bamberg2014bounds}
Bamberg, J., Gill, N., Hayes, T. P., Helfgott, H. A., Seress, Á.,  Spiga, P. (2014). Bounds on the diameter of Cayley graphs of the symmetric group. Journal of Algebraic Combinatorics, 40(1), 1-22.

\bibitem[Helfgott14]{helfgott2014diameter}
Helfgott, H. A., Seress, Á. (2014). On the diameter of permutation groups. Annals of mathematics, 611-658.

\bibitem[Helfgott19]{helfgott2019growth}
Helfgott, H. A. (2019). Growth in linear algebraic groups and permutation groups: towards a unified perspective. Groups St Andrews 2017 in Birmingham, 455, 300.


\bibitem[Gates79]{gates1979bounds}
Gates, W. H., Papadimitriou, C. H. (1979). Bounds for sorting by prefix reversal. Discrete mathematics, 27(1), 47-57.

\bibitem[Pevzner95]{Pevzner1995human2mice}
Hannenhalli, S., Pevzner, P. A. (1995, October). Transforming men into mice (polynomial algorithm for genomic distance problem). In Proceedings of IEEE 36th annual foundations of computer science (pp. 581-592). IEEE.

\bibitem[Pevzner99]{Pevzner1999cabbage2turnip}
Hannenhalli, S.,  Pevzner, P. A. (1999). Transforming cabbage into turnip: polynomial algorithm for sorting signed permutations by reversals. Journal of the ACM (JACM), 46(1), 1-27.


\bibitem[Bulteau19]{bulteau2019parameterized}
Bulteau, L.,  Weller, M. (2019). Parameterized algorithms in bioinformatics: an overview. Algorithms, 12(12), 256.

\bibitem[Świta23]{swita2023solving}
Świta, R.,  Suszyński, Z. (2023). Solving Full N× N× N Rubik’s Supercube Using Genetic Algorithm. International Journal of Computer Games Technology, 2023(1), 2445335.

\bibitem[McAleer19]{mcaleer2019solving}
McAleer, S., Agostinelli, F., Shmakov, A. K.,  Baldi, P. (2019, January). Solving the rubik's cube with approximate policy iteration. In International Conference on Learning Representations.

\bibitem[Agostinelli19]{agostinelli2019solving}
Agostinelli, F., McAleer, S., Shmakov, A.,  Baldi, P. (2019). Solving the Rubik’s cube with deep reinforcement learning and search. Nature Machine Intelligence, 1(8), 356-363.

\bibitem[Agostinelli24]{khandelwal2024towards}
Khandelwal, V., Sheth, A.,  Agostinelli, F. (2024). Towards Learning Foundation Models for Heuristic Functions to Solve Pathfinding Problems. arXiv preprint arXiv:2406.02598.

\bibitem[Agostinelli24q]{agostinelli2024q}
Agostinelli, F., Shperberg, S. S., Shmakov, A., McAleer, S., Fox, R., Baldi, P. (2024). Q* search: Heuristic search with deep q-networks. In ICAPS Workshop on Bridging the Gap between AI Planning and Reinforcement Learning.

\bibitem[Takano21]{takano2023selfsupervision}
Takano, K. (2021). Self-Supervision is All You Need for Solving Rubik's Cube. arXiv preprint arXiv:2106.03157.

\bibitem[TakanBrunetto17]{brunetto2017deep}
Brunetto, R.,  Trunda, O. (2017, September). Deep Heuristic-learning in the Rubik's Cube Domain: An Experimental Evaluation. In ITAT (pp. 57-64).


\bibitem[Johnson21]{johnson2021solving}
Johnson, C. G. (2021). Solving the Rubik's cube with stepwise deep learning. Expert Systems, 38(3), e12665.

\bibitem[Amrutha22]{amrutha2022deep}
Amrutha, B. V.,  Srinath, R. (2022). Deep Learning Models for Rubik’s Cube with Entropy Modelling. In ICDSMLA 2020: Proceedings of the 2nd International Conference on Data Science, Machine Learning and Applications (pp. 35-43). Springer Singapore.


\bibitem[Noever21]{noever2022puzzle}
Noever, D.,  Burdick, R. (2021). Puzzle solving without search or human knowledge: An unnatural language approach. arXiv preprint arXiv:2109.02797.


\bibitem[Chasmai21]{chasmai2022cubetr}
Chasmai, M. E. (2021). CubeTR: Learning to Solve the Rubik's Cube using Transformers.

\bibitem[Bedaywi23]{bedaywi2023solving}
Bedaywi, Mark and Longtai, (Ted) Deng and Shaul, Francus (2023). Solving the Rubik’s Cube via Sequence Modeling using
Transformer-based Models, https://github.com/tedtedtedtedtedted/Solve-Rubiks-Cube-Via-Transformer/blob/main/Report.pdf



\bibitem[Pan21]{pan2021fourier}
Pan, H.,  Kondor, R. (2021, March). Fourier Bases for Solving Permutation Puzzles. In International Conference on Artificial Intelligence and Statistics (pp. 172-180). PMLR.

\bibitem[Swan17]{swan2017harmonic}
Swan, J. (2017, June). Harmonic analysis and resynthesis of Sliding-Tile Puzzle heuristics. In 2017 IEEE Congress on Evolutionary Computation (CEC) (pp. 516-524). IEEE.


\bibitem[Leskovec22]{pandy2022learning}
Pándy, M., Qiu, W., Corso, G., Veličković, P., Ying, Z., Leskovec, J.,  Liò, P. (2022, December). Learning graph search heuristics. In Learning on Graphs Conference (pp. 10-1). PMLR.

\bibitem[Yakovlev23]{kirilenko2023transpath}
Kirilenko, D., Andreychuk, A., Panov, A.,  Yakovlev, K. (2023, June). Transpath: Learning heuristics for grid-based pathfinding via transformers. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 37, No. 10, pp. 12436-12443).

\bibitem[Gromov93]{gromov1993geometric}
Gromov, M. (1993). Geometric Group Theory: Asymptotic invariants of infinite groups (Vol. 2). Cambridge University Press.

\bibitem[Tao15]{tao2015expansion}
Tao, T. (2015). Expansion in finite simple groups of Lie type (Vol. 164). American Mathematical Soc..

\bibitem[Babai92]{babai1992diameter}
Babai, L.,  Seress, Á. (1992). On the diameter of permutation groups. European journal of combinatorics, 13(4), 231-243.

\bibitem[Akers89]{akers1989group}
Akers, S. B., Krishnamurthy, B. (1989). A group-theoretic model for symmetric interconnection networks. IEEE transactions on Computers, 38(4), 555-566.

\bibitem[Cooperman91]{cooperman1991applications}
Cooperman, G., Finkelstein, L.,  Sarawagi, N. (1991). Applications of cayley graphs. In Applied Algebra, Algebraic Algorithms and Error-Correcting Codes: 8th International Conference, AAECC-8 Tokyo, Japan, August 20–24, 1990 Proceedings 8 (pp. 367-378). Springer Berlin Heidelberg.

\bibitem[Heydemann97]{heydemann1997cayley}
Heydemann, M. C. (1997). Cayley graphs and interconnection networks. In Graph symmetry: algebraic methods and applications (pp. 167-224). Dordrecht: Springer Netherlands

\bibitem[Wigderson06]{hoory2006expander}
Hoory, S., Linial, N.,  Wigderson, A. (2006). Expander graphs and their applications. Bulletin of the American Mathematical Society, 43(4), 439-561.

\bibitem[Zémor94]{zemor1994hash}
Zémor, G. (1994). Hash functions and Cayley graphs. Designs, Codes and Cryptography, 4(3), 381-394.

\bibitem[Petit11]{petit2013rubik}
Petit, C., Quisquater, J. J. (2011). Rubik's for cryptographers. Cryptology ePrint Archive.

\bibitem[Kohli24]{ruiz2024quantum}
Ruiz, F. J., Laakkonen, T., Bausch, J., Balog, M., Barekatain, M., Heras, F. J., ...  Kohli, P. (2024). Quantum Circuit Optimization with AlphaTensor.(2024). URL https://arxiv. org/abs/2402.14396.

\bibitem[Sarkar24]{sarkar2024quantum}
Sarkar, R. S.,  Adhikari, B. (2024). Quantum circuit model for discrete-time three-state quantum walks on Cayley graphs. Physical Review A, 110(1), 012617.


\bibitem[Vidick23]{dinur2023good}
Dinur, I., Hsieh, M. H., Lin, T. C.,  Vidick, T. (2023, June). Good quantum LDPC codes with linear time decoders. In Proceedings of the 55th annual ACM symposium on theory of computing (pp. 905-918).

\bibitem[Acevedo06]{acevedo2006exploring}
Acevedo, O. L., Roland, J.,  Cerf, N. J. (2006). Exploring scalar quantum walks on Cayley graphs. arXiv preprint quant-ph/0609234.

\bibitem[Gromada22]{gromada2022some}
Gromada, D. (2022). Some examples of quantum graphs. Letters in Mathematical Physics, 112(6), 122.

\bibitem[Helfgott15]{helfgott2015random}
Helfgott, H. A., Seress, Á.,  Zuk, A. (2015). Random generators of the symmetric group: diameter, mixing time and spectral gap. Journal of Algebra, 421, 349-368.


\bibitem[Diaconis13]{diaconis2013some}
Diaconis, P. (2013). Some things we’ve learned (about Markov chain Monte Carlo).

\bibitem[Hirata24]{hirata2024probabilistic}
Hirata, S. (2024). Probabilistic estimates of the diameters of the Rubik's Cube groups. arXiv preprint arXiv:2404.07337.

\bibitem[Alon07]{alon2007non}
Alon, N., Benjamini, I., Lubetzky, E.,  Sodin, S. (2007). Non-backtracking random walks mix faster. Communications in Contemporary Mathematics, 9(04), 585-603.


\end{thebibliography}

\end{document} 