\section{Related Work}
\subsection{Generative Adversarial Networks}
Generative adversarial networks (GANs) \cite{Goodfellow2014} have been effectively applied to synthesize various forms of synthetic data \cite{Figueira2022, Yi2019}. In the context of tabular data, CTGAN is a GAN-based model specifically designed for tabular data generation \cite{Xu2019}. A considerable body of literature has demonstrated the fidelity and utility of data generated with CTGAN, across several applications and disciplines \cite{Xu2019, Figueira2022, Kang2023, Mendikowski2022}. However, like other GAN-based approaches, CTGAN requires access to RWD, which presents privacy concerns. Overfitting and memorization of training data increases susceptibility to privacy attacks and data leak \cite{Chen2020, Hayes2017}. This vulnerability is particularly of concern when GANs are trained on RWD with smaller sample sizes \cite{Chen2020}. GAN-based models also require some degree of technical expertise to generate high-quality synthetic data (e.g., GAN architecture, optimization, fine-tuning), which poses additional limitations to the accessibility of this approach. 
\subsection{Variational Autoencoders}
Variational autoencoders (VAEs) are another class of deep generative models used to synthesize synthetic data \cite{Kingma2013}. Several VAEs have demonstrated applicability toward tabular data generation, including TVAE \cite{Xu2019}, which was presented alongside CTGAN. Like GAN-based approaches, VAEs require access to RWD and technical expertise.
\subsection{Large Language Models}
LLMs are another form of generative artificial intelligence, which have been applied to synthetic data generation. This primarily includes text-based data, including synthetic interview transcripts \cite{Ham2023} and medical records \cite{CL2024}. Some approaches to LLM-based tabular data synthesis have also emerged, including GReaT \cite{Borisov2022} and TabuLa \cite{Zhao2023}. However, current LLM-based frameworks require access to RWD, pre-training, and/or fine-tuning. More recently, preliminary evidence has demonstrated the potential for zero-shot generation of synthetic tabular data with the LLM, GPT-4o \cite{BarrFAI2025, BarrarXiv2025}. Without access to RWD, GPT-4o was capable of generating tabular data with high fidelity to clinical RWD by using plain-language prompts which described the desired statistical properties. Notably, this initial evidence has demonstrated the preservation of relationships between parameters, synthesis of new and interrelated features, amplification of sample sizes, and the utility of LLM-generated data toward training ML models. However, it remains unclear whether observed fidelity would scale to further interactions between parameters and how performance would compare to GAN-based frameworks with further benchmarking metrics. The potential for tabular data generation without requiring technical expertise and access to RWD in a zero-shot setting warrants further investigation.