% To elaborate on the approaches we propose in our paper, we will start with agent setup and then proceed to detailed modules.  

\subsection{Search System Setup and Agent Workflow}
\label{sec:search}
Our search system is inspired by prior works such as ~\cite{ma2024understand, ouyang2024repograph}, which employ graph databases for indexing code repositories. Similarly, we construct a \textbf{CodeGraph}, a graph-based representation of the codebase $\mathcal{G}=\mathcal{(V,E)}$, to facilitate indexing and searching code entities. As illustrated in Figure~\ref{fig:overview}. (b), the CodeGraph $\mathcal{G}$ contains two primary edge types $e_1, e_2 \in \mathcal{E}$. $e_1$ is containment, which represents hierarchical relationships, such as methods within classes or classes within files. $e_2$ is the reference that represents relationships such as function calls between entities. The entities include functions, classes, methods, and files. Each code entity $v\in \mathcal{V}$ in the CodeGraph is assigned with a unique identifier (UID) using the format \texttt{file\_path(::cls)(::method)}. For example, in standalone functions, the UID is simply \texttt{file\_path::method}. These identifiers encode the containment hierarchy directly, with \texttt{::} representing the "containment" relationship. To enhance compatibility with the \textbf{CodeGraph}, we redeveloped the API from AutoCodeRover~\cite{autocoderover} to provide better support for CodeGraph-based searches (See Appendix~\ref{appendix:codegraph}).


Building upon the ideas of Chain of Thought (CoT)~\cite{wei2022chain} and ReACT~\cite{yao2022react}, \nickname follows a reason-and-act workflow with a constrained action space. We design a custom-designed LLM prompt, which will generate \textbf{Observation ($O$)}, \textbf{Potential Bug Locations ($PB$)}, and \textbf{Search Actions ($SA$)} in each step. Here, we formulate $PB$ as a set of entities $v_{PB}$: $PB=\{v_{PB}|v_{PB}\in \mathcal{V}\}$.
To better illustrate the agent workflow, we formulate it as a tuple $\mathcal{M}$, where $\mathcal{M}=(\mathcal{S,C,A,P}, p_0)$. Here, $\mathcal{S}$ means the state space, including previous observations, potential bug locations, and retrieved search results. $\mathcal{A}$ stands for action space, which is restricted by our search APIs. In $\mathcal{A}$, each action $a_k \in \mathcal{A}$ represents a query for retrieving relevant code snippets, generating a feedback as \textbf{Search Result ($SR$)}. $\forall SR \text{ with UID}, SR \equiv v_{SR} \in V.$ The context space $\mathcal{C}$ means for the environment, which contains the repository structure formulated by CodeGraph.

For the evolution of the agent state after action, we denote the transition function as $\mathcal{P}:\mathcal{S}\times \mathcal{A}\times \mathcal{C} \to \Delta (\mathcal{S})$. In our agent, LLM plays the key role of state transition, in which the next state $s_{t+1}$ is formed by adding new search results and refining potential bug locations. The agent follows policy $\pi : \mathcal{S} \times \mathcal{C} \to \Delta(\mathcal{A})$, which is co-managed by LLM and \textbf{Action Scheduler Queue} (ASQ). The policy determines the next action to execute based on priority, where we have a detailed description in Section~\ref{scheduler queue}. At step $t$, the action $a_t$ will also generated by the decomposition mechanism, which is described in Section~\ref{score ranking}.

The agent begins from the initial state $p_0$, which consists of the problem statement (See Figure~\ref{fig:overview}. (a)) and the reproducer information from the issue (See Appendix~\ref{appendix:tracer}), if available. Please note that these details are concatenated in our system prompt (See Appendix~\ref{appendix:prompt}) and will be provided to LLM at each subsequent step. 
During the exploration, LLM agent will generate $O_t$, $PB_t$, and $SA_t$ in every step $t$. In specific, the state transition would be
$O_{t+1}, PB_{t+1} \sim \mathcal{P}(O_{1\dots t}, SR^{CM}_{1\dots t})$, indicating the generated $O$ and $PB$ are dependent on all previous generated states. Here, $SR^{\text{CM}}_{1 \dots t}$ is the pruned set of search results managed by the \textbf{Context Manager (CM)}, see Section~\ref{context manager}. The process terminates when ASQ is empty or follows the convergence condition (See Appendix~\ref{appendix:convergence}). 
In the end, the conclusion step produces only the conclusion ($O_{\text{conclusion}}$) and the bug locations ($B$), summarizing the identified issues and their locations after all exploration steps are completed, see Figure~\ref{fig:overview}. (e). Here $B=\arg\max\limits_{PB}\mathcal{P}(PB|O_{\text{all}},{SR}_{\text{all}}) \subseteq \mathcal{V}$.

Unlike traditional reinforcement learning, where the goal is to maximize cumulative rewards, our agent is designed to converge to the correct bug location effectively. The evaluation target is elaborated in Section~\ref{exp:metrics}.


\subsection{Priority-Based Scheduling for LLM-Guided Actions} \label{scheduler queue}
To solve challenge 1) we discussed in Section~\ref{sec:intro}, \nickname provides a more robust framework, which leverages a priority queue to manage the LLM-generated actions, offering a more comprehensive and effective method for action planning.

To achieve a thorough reasoning COT, our agent limits each step to only processing one action. However, for $SA$ generated by LLM, it may have multiple action candidates based on the given context. To address this, we design a policy $\pi$ that uses a dynamic action scheduler queue (ASQ) on top of LLM-generated actions. The ASQ has priority management which is implemented on top of a heap data structure. 

In \nickname, action priorities are dynamically adaptable across different levels. The default priority for action $a_k \in SA$ is $1$. However, this priority can be elevated based on contextual relevance and strong relationships. For instance, in Figure~\ref{fig:overview}. (c), the step from \circlednumber{7} to \circlednumber{8} shows how the action involving the file \texttt{serializer.py} is assigned a higher priority due to its strong connection with \texttt{serializer\_factory}. 
The same principle is set for action decomposition, which is discussed in Section~\ref{score ranking}.

To account for urgency, we also keep a counter $C_{a_k}$ for each unique action $a_k$. When the LLM generates the same action repeatedly, the counter $C_{a_k}$ grows, indicating the LLM's focus on checking the content. The counter $C_{a_k}$ replaces the original priority value and adjusts the position of $a_k$' in the queue. This system ensures that the most important actions are carried out first. For example in Figure 1. (c), the step from \circlednumber{6} to \circlednumber{7} shows that \texttt{serializer\_factory} would come to the next step due to its counter has accumulated to 3, which even surpasses the file related action \texttt{models.py} corresponding to \texttt{CreateModel}.

Additionally, to address the unpredictability and hallucinations of LLMs, we set up a redundancy elimination mechanism to improve action scheduling. This mechanism ensures that redundant actions are avoided, enhancing efficiency and preventing unnecessary exploration. 

\begin{figure}[t]
    % \vspace{-5pt}
    % \vskip 0.2in
    \centering
    \centerline{\includegraphics[width=\columnwidth]{fig/disambiguation.pdf}}
    % \vspace{-10pt}
    \caption{Detailed examples for \nickname solving redundancy and disambiguation problem.}
    \label{fig:dis}
    \vspace{-15pt}
    % \vskip -0.2in
\end{figure}


Consider the previous agent API used by systems like~\cite{zhang2024autocoderover,ma2024understand}. When it comes to search class content, it has two different APIs \texttt{search\_class(cls)} and \texttt{search\_class\_in\_file(cls, f)} which will target at class searching. Initially, the LLM may lack precise information about the location of the target class, which leads to the use of the general method \texttt{search\_class(ModelChoiceField)}. However, after analyzing the returned content, the LLM will learn the file path and generate a subsequent, more specific action, such as \texttt{search\_class\_in\_file(ModelChoiceField, django/forms/models.py)}. Without careful handling of API ambiguities in scheduling, even a unique class like \texttt{ModelChoiceField} could result in duplicate actions and redundant content searches. 

To mitigate this, as illustrated in Figure~\ref{fig:dis} (a), we maintain an action search database. Before an action is passed to the agentâ€™s chain-of-thought (COT) reasoning, we prefetch its UID from CodeGraph and register its unique identifier (UID) in this database. This prefetching process ensures that each action is checked against previously executed actions, preventing duplicates and enabling more efficient scheduling. 




\subsection{Action Decomposition with Relevance Scoring}
\label{score ranking}
Achieving both conciseness and completeness simultaneously is challenging. Previous solutions~\cite{xia2024agentless, autocoderover} frequently employed skeletal techniques for huge classes or files, returning solely the class and methods signature. However, brutal traversal over all the methods could lead to noisy context and redundant actions. 
To overcome this challenge, we propose action decomposition with relevance scoring.  

Specifically, if the search result $SR$ of an action $a_k$ corresponds to a class $v_{SR}\in \mathcal{V}^{class}$, we employ a \textbf{score and rank sub-agent} to evaluate the relevance of each method in the class $\mathcal{N}_{v^{\text{class}}} = \{ v \mid v \to v^{\text{class}} \in e_1$ \} to the problem statement. The sub-agent (implemented by another LLM agent) will select the top-$k$ most relevant methods, which are recomposed as new search actions, denoted as $a_k^d$. These decomposed actions $a_k^d$ are assigned a higher priority (e.g. $2$), and pushed to the ASQ for execution. In this way, the main agent could work with the scoring sub-agent in a multi-agent workflow. Moreover, we extend this decomposition principle to handle large files. For a file that triggers skeleton mode, we collect code entities within the file, like functions and classes, and treat them as individual units for the sub-agent. We have shown the illustrated example in Figure~\ref{fig:overview}. (c). 

In addition to enhancing granularity, our method addresses ambiguities, which commonly appear in large software repositories such as function overrides, and inherited classes.  To resolve these issues, we implement a robust disambiguation mechanism within our decomposition strategy. We first constructed an inverted index that stores only the callable indices that exhibit ambiguities. The value of the index encloses the exact location, including the file, path, and relevant class, if applicable. 
As shown in Figure ~\ref{fig:dis}. (b), when our API finds a query with ambiguities, it will locate itself in the inverted index, enabling us to gather all the possible locations to form a disambiguation message for the LLM agent. Additionally, we will split the potential locations and fine-grainedly push back the related search actions in the action queue. 


\subsection{Distance-Aware Searched Context Pruning} \label{context manager}
To prune the irrelevant context and keep LLM focusing on useful information, we developed a distance-aware context pruning method, which we call as the \textbf{Context Manager (CM)}. The CM is designed to maintain a concise and relevant set of search results ($SR$) by evaluating their relationship to the potential bug locations ($PB$).

First of all, to enhance relevance, the CM retains only $SR$ entries linked to valid search query UIDs. Disambiguation messages (See Figure~\ref{fig:dis}. (b)) and skeleton UIDs, typically used for large files and classes, are explicitly excluded to prevent irrelevant data from polluting the context. 

The pruning process is guided by CodeGraph $\mathcal{G}$, where each search result $SR$ is mapped to a unique graph node \( v_{SR} \in V \). The CM evaluates each \( SR \) based on its distance to the potential bug locations \( PB \), which are also represented as nodes in the graph. Specifically, the CM computes the average shortest path distance between each node \( v_{SR} \) and the candidate nodes in \( PB \):
$ d(SR, PB) = \frac{1}{|PB|} \sum_{v \in PB} \min \left( d(v_{SR}, v), d(v, v_{SR}) \right)
$, where \( d(v_{SR}, v) \) represents the shortest path from \( v_{SR} \) to \( v \) in the directed CodeGraph, and \( d(v, v_{SR}) \) represents the reverse shortest path. The final distance metric for pruning is defined as the minimum of these two values.


Once distances are calculated, the CM prioritizes the most relevant results. It selects the top-$k$ candidates based on the calculated average distance, ensuring that LLM bypass those irrelevant code blocks. 
As shown in Figure~\ref{fig:overview}. (d), in the last step, the context will filter out the irrelevant info like \texttt{OperationWriter}, \texttt{CreateModel}, which will make the conclusion step have a stable and correct bug location output. 
Importantly, the CM is applied to every step during the exploration phase. 

By aligning $SR$ entries with the structural relationships within the CodeGraph, the CM helps the system focus on areas most likely to contain the bug. This approach not only streamlines the input context but also improves the accuracy and efficiency of the search process.



