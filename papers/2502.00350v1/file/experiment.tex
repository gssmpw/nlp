% We evaluated the effectiveness of \nickname through experiments on a widely recognized dataset, comprehensive metric calculations, and ablation studies examining the impact of various features.




\subsection{Setup}
\subsubsection{Datasets}

\textbf{SWE-bench} \cite{jimenez2023swe} is a widely used dataset for evaluating the ability of LLM systems to address real-world software engineering challenges. It comprises 2,294 task instances derived from 12 popular Python repositories, where each task requires a patch to resolve the issue described in its corresponding GitHub issue.

To reduce evaluation costs and complexity, the SWE-bench team introduced two refined subsets:
\squishlist
\item \textbf{SWE-bench Lite} contains 300 instances filtered using heuristics, such as removing tasks with images, external hyperlinks, or short descriptions. Each task includes functional tests to validate the correctness of submitted patches.
\item \textbf{SWE-bench Verified}, developed in collaboration with OpenAI, includes 500 instances manually validated by professional annotators, providing greater reliability.
\squishend

To further optimize costs for repeated experiments, we defined a smaller subset, \textbf{SWE-bench Common}, consisting of 93 instances that form the intersection of SWE-bench Lite and SWE-bench Verified. Its compact size and high reliability make it ideal for tasks such as ablation studies.

In our experiments, we evaluate the performance of \nickname using SWE-bench Lite and conduct ablation studies using SWE-bench Common.

\subsubsection{Baselines}

\input{file/bigtable}

We compare \nickname against 17 different approaches listed on the public leaderboard \cite{swebenchleaderboard} of SWE-bench Lite. These approaches are categorized into 2 groups: (1) closed-source solutions, such as Alibaba Lingma \cite{repounderstander}; (2) open-source solutions, including OpenHands \cite{wang2024openhands}, AutoCodeRover \cite{zhang2024autocoderover}, Agentless \cite{xia2024agentless}, RepoGraph \cite{ouyang2024repograph}, HyperAgent \cite{phan2024hyperagent}, and SWE-Agent \cite{yang2024swe}.
%Additionally, we included a baseline submission from the SWE-bench team employing retrieval-augmented generation (RAG) for comparison.

The SWE-bench Lite leaderboard mandates that each submission include the generated patches for addressing the given issues. This requirement enables the computation and comparison of a broader range of metrics beyond the resolved rate. In addition to analyzing the leaderboard data, we reproduced the Agentless-1.5 model for a direct comparison with \nickname, as its editor component is integrated into our system.

\subsubsection{Implementation} \label{eval_setup_implem}

\nickname is built on the LlamaIndex framework \cite{Liu_LlamaIndex_2022}, which supports various foundation models. For our experiments, we used Claude-3.5-Sonnet-20241022 \cite{claudethreefive} as the underlying model, with a sampling temperature set to 0.1 to prioritize deterministic results. 

%Given that many issues include a reproduction snippet in their problem statement, we developed a trace extractor based on VizTracer \cite{viztracer} to generate a high-quality starting point for the \nickname search agent. The extracted trace stack is processed using pruning and scoring techniques similar to those employed by the search agent, ensuring relevance and efficiency. For issues that lack a reproduction snippet, the search agent begins its process from an empty context.

For the top-$k$ values used in action decomposition (Section~\ref{score ranking}), we set $k = 3$ for class decomposition and $k = 2$ for file decomposition. In the context pruning (Section~\ref{context manager}), the context window size is configured to retain 12 entries (top-$k$). Our framework also supports a wide range of customizable configurations, enabling users to fine-tune their agent workflows. These settings include parameters such as class decomposition, file decomposition, disambiguation decomposition, priority adjustment, and the ability to enable or customize priority levels. This flexibility allows users to tailor their agent's behavior to specific use cases, enhancing both exploration and fine-tuning capabilities. The cost of searching is about \$0.87 per instance.

To evaluate the contribution of \nickname to the final Resolved Rate on SWE-bench Lite, we integrated the Repair, Patch Validation, and Patch Selection components of Agentless-1.5 \cite{xia2024agentless} by converting the output of \nickname into Agentless format. Inspired by Repograph \cite{ouyang2024repograph}, the dependencies of the output code are also added.
We largely adhered to the experimental setup outlined in the Agentless public repository, using the same LLM model, Claude-3.5-Sonnet-20241022. For the repair process, we generated 40 patches (1 at a temperature of 0 and the rest at 0.8) with the \texttt{str\char`_replace\char`_format} argument set. During patch validation, we employed both regression and reproduction tests. Regression tests were filtered with a temperature of 0, while reproduction tests were generated using 40 samples (1 at a temperature of 0 and the rest at 0.8). Finally, the results of selected regression and reproduction tests were used to identify the most effective patch among the 40 candidates.
The cost of editing is about \$0.90 per instance.


\subsubsection{Metrics}
\label{exp:metrics}

To evaluate the performance of \nickname, we utilized four metrics: Resolved Rate, Function Match Rate, File Match Rate, and Function Match Precision. Each metric is designed to provide unique insights into the effectiveness and quality of the agent. 
\squishlist
\item \textbf{Resolved Rate} is a metric originally proposed by the SWE-bench benchmark \cite{swebench}, which we adopted for our evaluation.
The benchmark assesses whether an issue is resolved by constructing a Docker container for each instance, applying the user-submitted patch, running regression tests within the container, and analyzing the test results.
% All necessary resources for the evaluation, including the container environment, regression tests, and the golden test/patch results, are provided by the benchmark.
The final metric is the percentage of the instances that are resolved.

\item \textbf{Function Match Rate} and \textbf{File Match Rate} assess the localization accuracy of \nickname by calculating the percentage of \textbf{Match} in instances. These metrics, inspired by prior works such as Agentless \cite{xia2024agentless} and Repograph \cite{ouyang2024repograph}, evaluate how well the agent’s outputs align with the golden patch. (To align with these works, we use the term function as a general term that includes functions and methods).

To determine \textbf{Function Match}, we define the golden and agent-generated localization function results for each instance $i$ as sets:
$B_{\text{i, golden}}^{\text{func}}, B_{\text{i, agent}}^{\text{func}} \subseteq \mathcal{V}$, following definitions in \cref{sec:search}.
A match is registered if the golden set is a subset of the agent's prediction:
$B_{\text{i, golden}}^{\text{func}} \subseteq B_{\text{i, agent}}^{\text{func}}$.
For \textbf{File Match}, we consider the subset of file nodes in the graph $\mathcal{G}$, denoted as:
$\mathcal{V}^{\text{file}}$.
According the definition of our graph, every node $v \in \mathcal{V}$ is either a file node or has an ancestor by containment edge that is a file node. Thus, we define a mapping function:
$\text{fileOf}: \mathcal{V} \to \mathcal{V}^{\text{file}}$,
which returns the file containing node $v$. The File Match is then determined as:
$B_{\text{i, golden}}^{\text{file}} \subseteq BF_{\text{i, agent}}^{\text{file}}$, where $B_{\text{i}}^{\text{file}} = \{ \text{fileOf}(v) \mid v \in B_i \}$.


\item \textbf{Function Match Precision} is a metric proposed by us to assess the quality of localization results. For instance, a localization output that includes every function in the repository would always ensure a function match but would be practically useless. To solve this problem, the Function Match Precision is computed for each instance as $\text{FMP}_i = |B_{\text{i, golden}}^{\text{func}} \cap B_{\text{i, agent}}^{\text{func}}| \,/\, |B_{\text{i, agent}}^{\text{func}}|$, and the final metric is the average of $\text{FMP}_i$ per instances.
\squishend


\subsection{Results}

\subsubsection{Performance on Leaderboard}

As shown in \cref{tab:bigtable}, our \nickname sets a new open-source State-Of-The-Art (SOTA) with a Function Match Rate of 65.33\% (196 out of 300) and a File Match Rate of 83.33\% (250 out of 300). These results demonstrate the effectiveness of our proposed localization methodology.

Moreover, \nickname demonstrates strong performance on the Resolved Rate metric, successfully resolving 41.00\% (123 out of 300) issues in the SWE-bench Lite dataset. By integrating the editing capabilities of Agentless-1.5, we achieved 6.67 percentage points improvement in function match rate and 6.33 percentage points increase in the final resolved rate over its performance. These results establish \nickname as a significant milestone in the research community's efforts toward developing more robust autonomous software engineering solutions.

\subsubsection{Impact of Localization on Resolved Rate}

To evaluate how \nickname’s improved localization enhances the final patch resolved rate, we fully reproduced Agentless-1.5 \cite{xia2024agentless} on SWE-bench Lite as a baseline. As shown in \cref{tab:loc}, \nickname outperforms Agentless-1.5 across all three key metrics: \textbf{Resolved Rate}, \textbf{Function Match Rate} and \textbf{Function Match Precision}.

Agentless-1.5 reports two sets of localization metrics due to its multi-sampling approach (four localization attempts per instance in the official reproduction). Patch generation then evenly distributes these samples, producing 10 patches per localization result (40 in total, as per \cref{eval_setup_implem}). To fairly evaluate localization performance under this setting, we compute metrics using two aggregation methods:

\squishlist

\item \textbf{Union of Locs}: Merges function sets from all localization attempts into a single aggregated union set per instance before computing metrics. This typically results in a higher Function Match Rate but a lower Function Match Precision, as more functions are included.

\item \textbf{Mean of Locs}: Computes metrics separately for each localization attempt and reports the average. This method generally yields a higher Function Match Precision but a lower Function Match Rate.

\squishend

As expected, the Union of Locs method captures more correct functions but also increases noise, whereas the Mean of Locs approach filters functions more precisely at the cost of match rate.

In both cases, \nickname achieves +6.67 percentage points improvement in Function Match Rate and a +4.62 percentage points increase in Function Match Precision compared to Agentless-1.5, demonstrating the effectiveness of our localization methodology. Crucially, the +6.33 percentage points gain in Resolved Rate confirms that our enhanced localization directly translates to better patch resolution.

\subsubsection{Unique Localizations and Solutions}

We analyze the unique issues localized and resolved by \nickname compared to other open-source agents including Agentless \cite{xia2024agentless}, AutoCodeRover \cite{zhang2024autocoderover} and OpenHands \cite{wang2024openhands}. As shown in \cref{fig:venn}, \nickname uniquely localized 6 issues, demonstrating the effectiveness of our approach. Additionally, it resolved 8 unique issues, emphasizing the impact of accurate localization in ASE. These results highlight \nickname's capability as a strong complement to other systems, even if they are developed with significantly larger resources (like OpenHands).

\begin{figure}[t]
    % \vspace{-10pt}
    \centering
    \centerline{\includegraphics[width=0.95\columnwidth]{fig/Venn1.pdf}}
    % \vspace{-5pt}
    \caption{Unique localizations and solutions of open source agents.}
    \label{fig:venn}
    % \vspace{-20pt}
\end{figure}

\subsubsection{Ablation Studies}

We conducted our ablation study on SWE-bench Common, a smaller subset of SWE-bench Lite, to evaluate the contributions of each proposed method. As shown in \cref{tab:ablation}, removing any of these methods caused a noticeable performance drop of approximately 3–5 percentage points. Specifically:

\squishlist

\item \textbf{Priority Scheduling (\cref{scheduler queue})}: Eliminating scheduler priority weakened \nickname’s heuristic planning ability, making it more susceptible to distractions from less important content.

\item \textbf{File \& Class / Disambiguation Decomposition (Section \ref{score ranking})}: Removing the decomposition approach restricted \nickname’s ability to explore a broader search space, thereby reducing overall performance. Notice here through the experiment we prove the LLM is hard to locate with correct info by only getting the disambiguation info (See Figure~\ref{fig:dis}. (b)). 

\item \textbf{Distance-Aware Context Pruning (\cref{context manager})}: Without distance-aware context pruning, \nickname was forced to handle a larger and noisier context, making it significantly more difficult to focus on the most relevant code snippet. Thus the noise will degrade the final bug localization accuracy. 

\squishend

\begin{table}[t]
\vspace{-5pt}
\centering
\caption{Impact of localization on resolved rate. UL stands for Union of Locations; ML stands for Mean of Locations.}
\label{tab:loc}
\vspace{5pt}
\begin{tabular}{lccc}
\toprule
\multirow{2}*{Agent} & \multirow{2}*{\% Resolved} & \multicolumn{2}{c}{Function Match} \\
 &  & Rate & Precision \\ 
\midrule
\textbf{OrcaLoca} & \textbf{41.00\%} & \textbf{65.33\%} & \textbf{38.34\%} \\
Agentless (UL) & \multirow{2}*{34.67\%} & 58.67\% & 29.01\%\\
Agentless (ML) &  & 47.33\% & 33.72\%\\
\bottomrule
\end{tabular}
\vspace{-15pt}
\end{table}

\begin{table}[t]
\centering
\caption{Ablation study results. Experiment completed on SWE-bench Common dataset.}
\label{tab:ablation}
\vspace{5pt}
\begin{tabular}{lc}
\toprule
Methods & Func. Match Rate \\
\midrule
\nickname & \textbf{76.34\%} (71) \\
 - w/o. priority scheduling & 73.12\% (68) \\
 - w/o. file \& class decom. & 72.04\% (67) \\
 - w/o. disambiguation decom. & 70.97\% (66) \\
 - w/o. context pruning & 72.04\% (67) \\
\bottomrule
\end{tabular}
\vspace{-15pt}
\end{table}