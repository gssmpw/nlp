\section{Related Work}
\subsection{Appropriate Reliance on AI}

Despite the rapid progress of technology, AI systems still frequently and unexpectedly fail. Without knowing when and how much to rely on a system, a user may experience low-quality interactions or even safety risks in high-stakes settings. 
Prior work has investigated how providing information about an AI system's accuracy \cite{yin2019understanding,He2023Accuracy,Yu2019IUI} and (un)certainty \cite{zhang2020effect,Bansal2021CHI,Green2019,Bucinca2021CSCW,Bussone2015}, explanations of outputs \cite{zhang2020effect,Gonzalez2021DoEH,Bansal2021CHI,Lai2019FAccT,Green2019,Bucinca2021CSCW,Bussone2015}, and onboarding materials \cite{Cai2021,Lai2020Tutorial} impact user reliance, as well as the roles played by human intuition \cite{chen2023understanding}, task complexity \cite{Salimzadeh2023UMAP,Salimzadeh2024CHI}, and other human, AI, and context-related factors \cite{Kim2023Trust}.
However, fostering appropriate reliance on AI remains difficult.
Findings on the effectiveness of proposed methods are mixed, and more research is needed on how reliance is shaped in real-world settings.

While most prior work on AI reliance has been in the context of classical AI models (e.g., specialized classification models), there is a growing body of work looking at reliance on systems based on LLMs or other modern generative AI models \cite{vasconcelos2023generation,spatharioti2023comparing,zhou2024relying,Kim2024FAccT,si2024fact,Lee2024FAccT}. 
For example, several recent studies explored the effect of communicating (un)certainty in LLMs by highlighting uncertain parts of LLM responses \cite{vasconcelos2023generation,spatharioti2023comparing} or inserting natural language expressions of uncertainty \cite{zhou2024relying,Kim2024FAccT}, finding that some but not all types of (un)certainty information help foster appropriate reliance.

Contributing to this line of work, we first take a bottom-up approach to identify the features of LLM responses that impact user reliance in the context of answering objective questions with the assistance of a popular LLM-infused application ChatGPT (\cref{sec:study1}).
In line with findings from prior work~\cite{si2024fact}, we see that reliance is shaped by the content of \textit{explanations} provided by the system, particularly whether or not these explanations contain \textit{inconsistencies}. We also observe that participants seek out \textit{sources} to verify the information provided in responses. We then design a large-scale, pre-registered, controlled experiment to isolate and study the effects of these features (\cref{sec:study2}). We discuss the relevant literature on these features and their impact on AI reliance next.


\subsection{Explanations and Inconsistencies}
\label{sec:llmresponses}


The impact of \emph{explanations} on human understanding and trust of AI systems has been studied extensively within the machine learning and human-computer interaction communities, often under the names explainable AI or interpretable machine learning~\cite{liao2021human,vaughan2021humancentered,arrieta2019explainable,RudinEtAlSurvey2022,Kim2023CHI}. Explanations are often motivated as a way to foster appropriate reliance and trust in AI systems, since in principle they provide clues about whether a system's outputs are reliable. However, empirical studies have shown mixed results, with a large body of work suggesting that providing explanations increases people's tendency to rely on an AI system even when it is incorrect~\cite{zhang2020effect,Bansal2021CHI,Poursabzi-Sangdeh-CHI2021,wang2021explanations}. One potential reason for this is that study participants do not make the effort to deeply engage with the explanations~\cite{kaur2020CHI,buccinca2020proxy,gajos2022people,liao2022designing,Vasconcelos2023CSCW}. That is, instead of encouraging deep, analytical reasoning (System 2 thinking~\cite{Kahneman2003,kahneman2011thinking}), study participants may resort to heuristics, such as the explanation's fluency or superficial cues to expertise~\cite{trout2008}, and defer to the system's response on this basis. People may also be more likely to assume an AI system is trustworthy simply because it provides explanations~\cite{ehsan2021explainable}. Further, some clues of unreliability may be difficult to pick up on without existing domain knowledge~\cite{chen2023understanding}.


Adopting the broad definition of an explanation as an answer to a why question \cite{Lombrozo2012,Wellman2011,bromberger1966why,Fraassen1980}, LLMs often provide explanations by default; when asked a question, LLMs rarely provide the answer alone. For factual questions, they provide details supporting the answer \cite{Lee2024FAccT,lfqa23}, and for math questions, they provide detailed steps to derive the answer \cite{Collins2024PNAS,hendrycks2021measuring}. 
This default behavior is likely due to human preference for verbose responses \cite{chiang2024overreasoning,saito2023verbosity,Zheng2020Verbose}.
Research in psychology has shown that explanations are often sought spontaneously \cite{malle1997behaviors,frazier2009preschoolers}, favored when they are longer, more detailed, or perceived to be more informative \cite{weisberg2015deconstructing,Zemla2017Everyday,bechlivanidis2017concreteness,Aronowitz2020TCS,Liquin2022Satisfaction}, and used to guide subsequent judgments and behaviors \cite{Lombrozo2023Selective,Lombrozo2016}. 
Since LLMs are often fine-tuned on human preference data via approaches such as Reinforcement Learning from Human Feedback (RLHF) \cite{ziegler2019finetuning,Christiano2017RLHF,Ouyang2024RLHF}, such preferences would shape the form of their outputs. We note that the default explanations that LLMs present typically provide evidence to support their answers, but do not necessarily reflect the internal processes by which the LLM arrived at the answer. This distinguishes these explanations from those traditionally studied in the explainable AI literature.


Explanations generated by LLMs are widely known to contain inaccurate information and other flaws \cite{Ji2023Hallucination,shuster2021eval,santhanam2022rome,Dahl2024Law}. We direct readers to recent surveys for comprehensive overviews \cite{huang2023hallucination,wang2023factuality}. In our studies, we found \textit{inconsistencies} in explanations to be an important unreliability cue that shapes participants' reliance. As documented in prior work, inconsistencies can occur within a response; they are sometimes referred to as logical fallacies or self-inconsistency in the NLP community \cite{huang2023reasoning,wang2023selfconsistency}. Inconsistencies can also occur between responses; many studies have demonstrated that LLMs often change their answer to a question when challenged, asked the question in a slightly different way, or re-asked the exact same question \cite{Lee2024FAccT,Elazar2021MeasuringAI,laban2024flipflop}. Such inconsistencies, when noticed, may impact people's evaluation of explanations and reliance on LLMs.



We contribute to this line of work in several ways. We first conduct a qualitative, think-aloud study to understand what features of LLM responses shape people's reliance, and find that reliance is shaped by explanations, inconsistencies in explanations, and sources. We then conduct a larger-scale, pre-registered, controlled experiment to quantitatively examine the effects of these features.
While a previous work by \citet{si2024fact} has studied the effects of LLM-generated explanations and inconsistencies on people's fact-checking performance through a small-scale study (16 participants per condition), our work provides a more holistic picture by studying what (else) might contribute to reliance and how the identified features affect a wider range of variables including people's evaluation of the LLM response's justification quality and actionability and likelihood of asking follow-up questions. 
As for the findings, first, consistent with \citet{si2024fact}, we find that explanations increase people's reliance, including overreliance on incorrect answers, and that inconsistencies in explanations can reduce overreliance. 
Additionally, we find that clickable sources --- which were not studied by \citet{si2024fact} --- increase appropriate reliance on correct answers, while reducing overreliance on incorrect answers, adding empirical knowledge on user reliance on LLMs. 
Lastly, our work also contributes nuanced insights on people's interpretation of LLMs' explanations, source clicking behavior, and interaction effects between explanations and sources.



\subsection{Sources}

The final feature of LLM responses that we study is the presence of \emph{sources}, i.e., clickable links to external material.\footnote{One might consider sources to be a component of an explanation. To simplify the exposition of our results, we treat them as a distinct component of LLM responses throughout this paper.} Sources are increasingly provided by LLM-infused applications, including general-purpose chatbots (e.g., ChatGPT, Gemini) and search engines (e.g., Perplexity AI, Copilot in Bing, SearchGPT). Sources are commonly sought by users, as found in prior work \cite{Kim2024ChatGPT} and supported in our studies. Similar to explanations, however, sources in LLM responses can be flawed in various ways \cite{liu2023evaluating,Alkaissi2023}. For instance, \citet{liu2023evaluating} conducted a human evaluation of popular LLM-infused search engines and found that their responses frequently contain inaccurate sources and unsupported statements. \citet{Alkaissi2023} conducted a case study of ChatGPT in the medical domain and found that it generates fake sources. These issues were observed in our studies as well. Currently there is active research on techniques such as Retrieval Augmented Generation (RAG) \cite{Lewis2020RAG,gao2024rag} to help LLMs provide more accurate information and sources.


It is well known that the presence and quality of sources impact how credible people find given content in other settings \cite{Rieh2007Credibility,Wathen2002Credibility}. However, there has been little work studying how people make use of and rely on sources in the context of LLM-infused applications. On the one hand, the presence of sources might reduce overreliance if people click on the provided links to verify the accuracy of the LLM's response. On the other hand, the presence of sources might increase reliance if people interpret them as signs of credibility and defer to the system without verifying the answers themselves. Indeed, in one study of uncertainty communication in LLM-infused search, participants were found to rarely click on source links~\cite{Kim2024FAccT}. Through a large-scale, pre-registered, controlled experiment (\cref{sec:study2}), we study how the presence of clickable sources impacts people's reliance, task accuracy, and other measures, and how this interacts with the presence of explanations and inconsistencies. In our studies, we use realistic explanations and sources, generated by state-of-the-art LLM-infused applications ChatGPT and Perplexity AI, and provide insights for fostering appropriate reliance on LLMs. \looseness=-1



\begin{figure*}[t!]
\centering
\includegraphics[width=\textwidth]{figures/schematic.png}
\caption{\textbf{Overview of our studies.} In Study 1, participants engaged in multi-turn interactions with ChatGPT to arrive at correct answers to objective questions. Based on a thematic analysis of think-aloud and behavioral data, we identified \textit{explanations}, \textit{inconsistencies}, and \textit{sources} as three features of LLM responses likely to influence user reliance. These three features were then investigated in a controlled experiment (Study 2), with features operationalized as indicated in the schematic illustration. Similar to Study 1, participants solved question-answering tasks. However, this time, they had access to one LLM response whose features we experimentally manipulated.}
\label{fig:schematic}
\end{figure*}