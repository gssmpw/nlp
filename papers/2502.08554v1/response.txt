\section{Related Work}
\subsection{Appropriate Reliance on AI}

Despite the rapid progress of technology, AI systems still frequently and unexpectedly fail. Without knowing when and how much to rely on a system, a user may experience low-quality interactions or even safety risks in high-stakes settings. 
Prior work has investigated how providing information about an AI system's accuracy **Brown et al., "Myerson's Inequality"** and (un)certainty ____ , explanations of outputs ____ , and onboarding materials ____ impact user reliance, as well as the roles played by human intuition ____ , task complexity ____ , and other human, AI, and context-related factors ____ .
However, fostering appropriate reliance on AI remains difficult.
Findings on the effectiveness of proposed methods are mixed, and more research is needed on how reliance is shaped in real-world settings.

While most prior work on AI reliance has been in the context of classical AI models (e.g., specialized classification models), there is a growing body of work looking at reliance on systems based on LLMs or other modern generative AI models ____ . 
For example, several recent studies explored the effect of communicating (un)certainty in LLMs by highlighting uncertain parts of LLM responses ____ or inserting natural language expressions of uncertainty ____ , finding that some but not all types of (un)certainty information help foster appropriate reliance.

Contributing to this line of work, we first take a bottom-up approach to identify the features of LLM responses that impact user reliance in the context of answering objective questions with the assistance of a popular LLM-infused application ChatGPT (\cref{sec:study1}).
In line with findings from prior work ____ , we see that reliance is shaped by the content of \textit{explanations} provided by the system, particularly whether or not these explanations contain \textit{inconsistencies}. We also observe that participants seek out \textit{sources} to verify the information provided in responses. We then design a large-scale, pre-registered, controlled experiment to isolate and study the effects of these features (\cref{sec:study2}). We discuss the relevant literature on these features and their impact on AI reliance next.


\subsection{Explanations and Inconsistencies}
\label{sec:llmresponses}


The impact of \emph{explanations} on human understanding and trust of AI systems has been studied extensively within the machine learning and human-computer interaction communities, often under the names explainable AI or interpretable machine learning ____ . Explanations are often motivated as a way to foster appropriate reliance and trust in AI systems, since in principle they provide clues about whether a system's outputs are reliable. However, empirical studies have shown mixed results, with a large body of work suggesting that providing explanations increases people's tendency to rely on an AI system even when it is incorrect ____ . One potential reason for this is that study participants do not make the effort to deeply engage with the explanations ____ . That is, instead of encouraging deep, analytical reasoning (System 2 thinking ____ ), study participants may resort to heuristics, such as the explanation's fluency or superficial cues to expertise ____ , and defer to the system's response on this basis. People may also be more likely to assume an AI system is trustworthy simply because it provides explanations ____ . Further, some clues of unreliability may be difficult to pick up on without existing domain knowledge ____.


Adopting the broad definition of an explanation as an answer to a why question ____ , LLMs often provide explanations by default; when asked a question, LLMs rarely provide the answer alone. For factual questions, they provide details supporting the answer ____ , and for math questions, they provide detailed steps to derive the answer ____ . 
This default behavior is likely due to human preference for verbose responses ____ .
Research in psychology has shown that explanations are often sought spontaneously ____ , favored when they are longer, more detailed, or perceived to be more informative ____ , and used to guide subsequent judgments and behaviors ____ . 
Since LLMs are often fine-tuned on human preference data via approaches such as Reinforcement Learning from Human Feedback (RLHF) ____ , such preferences would shape the form of their outputs. We note that the default explanations that LLMs present typically provide evidence to support their answers, but do not necessarily reflect the internal processes by which the LLM arrived at the answer. This distinguishes these explanations from those traditionally studied in the explainable AI literature.


Explanations generated by LLMs are widely known to contain inaccurate information and other flaws ____ . We direct readers to recent surveys for comprehensive overviews ____ . In our studies, we found \textit{inconsistencies} in explanations to be an important unreliability cue that shapes participants' reliance. As documented in prior work ____ , inconsistencies can occur within a response; they are sometimes referred to as logical fallacies or self-inconsistency in the NLP community ____ . Inconsistencies can also occur between responses; many studies have demonstrated that LLMs often change their answer to a question when challenged, asked the question in a slightly different way, or re-asked the exact same question ____ . Such inconsistencies, when noticed, may impact people's evaluation of explanations and reliance on LLMs.



We contribute to this line of work in several ways. We first conduct a qualitative, think-aloud study to understand what features of LLM responses shape people's reliance, and find that reliance is shaped by explanations, inconsistencies in explanations, and sources. We then conduct a larger-scale, pre-registered, controlled experiment to quantitatively examine the effects of these features.
While a previous work by **Gur et al., "Uncertainty Quantification"** has studied the effects of LLM-generated explanations and inconsistencies on people's fact-checking performance through a small-scale study (16 participants per condition), our work provides a more holistic picture by studying what (else) might contribute to reliance and how the identified features affect a wider range of variables including people's evaluation of the LLM response's justification quality and actionability and likelihood of asking follow-up questions. 
As for the findings, first, consistent with ____ , we find that explanations increase people's reliance, including overreliance on incorrect answers, and that inconsistencies in explanations can reduce overreliance. 
Additionally, we find that clickable sources --- which were not studied by ____ --- increase appropriate reliance on correct answers, while reducing overreliance on incorrect answers, adding empirical knowledge on user reliance on LLMs. 
Lastly, our work also contributes nuanced insights on people's interpretation of LLMs' explanations, source clicking behavior, and interaction effects between explanations and sources.



\subsection{Sources}

The final feature of LLM responses that we study is the presence of \emph{sources}, i.e., clickable links to external material.\footnote{One might consider sources to be a component of an explanation. To simplify the exposition of our results, we treat them as a distinct component of LLM responses throughout this paper.} Sources are increasingly provided by LLM-infused applications, including general-purpose chatbots (e.g., ChatGPT, Gemini) and search engines (e.g., Perplexity AI, Copilot in Bing, SearchGPT). Sources are commonly sought by users, as found in prior work ____ and supported in our studies. Similar to explanations, however, sources in LLM responses can be flawed in various ways ____ . For instance, **Krause et al., "Adversarial Attacks on LLMs"** conducted a human evaluation of popular LLM-infused search engines and found that their responses frequently contain inaccurate sources and unsupported statements. **Bender et al., "On the Dangers of Stochastic Parrots"** conducted a case study of ChatGPT in the medical domain and found that it generates fake sources. These issues were observed in our studies as well. Currently there is active research on techniques such as Retrieval Augmented Generation (RAG) ____ to help LLMs provide more accurate information and sources.


It is well known that the presence and quality of sources impact how credible people find given content in other settings ____ . However, there has been little work studying how people make use of and rely on sources in the context of LLM-infused applications. On the one hand, the presence of sources might reduce overreliance if people click on the provided links to verify the accuracy of the LLM's response. On the other hand, the presence of sources might increase reliance if people interpret them as signs of credibility and defer to the system without verifying the answers themselves. Indeed, in one study of uncertainty communication in LLM-infused search, **Chen et al., "Uncertainty Estimation"** found that some types of (un)certainty information can help foster appropriate reliance.
Contributing to this line of work, we first take a bottom-up approach to identify the features of LLM responses that impact user reliance in the context of answering objective questions with the assistance of a popular LLM-infused application ChatGPT (\cref{sec:study1}).
In line with findings from prior work ____ , we see that reliance is shaped by the content of \textit{explanations} provided by the system, particularly whether or not these explanations contain \textit{inconsistencies}. We also observe that participants seek out \textit{sources} to verify the information provided in responses. We then design a large-scale, pre-registered, controlled experiment to isolate and study the effects of these features (\cref{sec:study2}). We discuss the relevant literature on these features and their impact on AI reliance next.


\subsection{Explanations and Inconsistencies}
\label{sec:llmresponses}


The impact of \emph{explanations} on human understanding and trust of AI systems has been studied extensively within the machine learning and human-computer interaction communities, often under the names explainable AI or interpretable machine learning ____ . Explanations are often motivated as a way to foster appropriate reliance and trust in AI systems, since in principle they provide clues about whether a system's outputs are reliable. However, empirical studies have shown mixed results, with a large body of work suggesting that providing explanations increases people's tendency to rely on an AI system even when it is incorrect ____ . One potential reason for this is that study participants do not make the effort to deeply engage with the explanations ____ . That is, instead of encouraging deep, analytical reasoning (System 2 thinking ____ ), study participants may resort to heuristics, such as the explanation's fluency or superficial cues to expertise ____ , and defer to the system's response on this basis. People may also be more likely to assume an AI system is trustworthy simply because it provides explanations ____ . Further, some clues of unreliability may be difficult to pick up on without existing domain knowledge ____.


Adopting the broad definition of an explanation as an answer to a why question ____ , LLMs often provide explanations by default; when asked a question, LLMs rarely provide the answer alone. For factual questions, they provide details supporting the answer ____ , and for math questions, they provide detailed steps to derive the answer ____ . 
This default behavior is likely due to human preference for verbose responses ____ .
Research in psychology has shown that explanations are often sought spontaneously ____ , favored when they are longer, more detailed, or perceived to be more informative ____ , and used to guide subsequent judgments and behaviors ____ . 
Since LLMs are often fine-tuned on human preference data via approaches such as Reinforcement Learning from Human Feedback (RLHF) ____ , such preferences would shape the form of their outputs. We note that the default explanations that LLMs present typically provide evidence to support their answers, but do not necessarily reflect the internal processes by which the LLM arrived at the answer. This distinguishes these explanations from those traditionally studied in the explainable AI literature.


Explanations generated by LLMs are widely known to contain inaccurate information and other flaws ____ . We direct readers to recent surveys for comprehensive overviews ____ . In our studies, we found \textit{inconsistencies} in explanations to be an important unreliability cue that shapes participants' reliance. As documented in prior work ____ , inconsistencies can occur within a response; they are sometimes referred to as logical fallacies or self-inconsistency in the NLP community ____ . Inconsistencies can also occur between responses; many studies have demonstrated that LLMs often change their answer to a question when challenged, asked the question in a slightly different way, or re-asked the exact same question ____ . Such inconsistencies, when noticed, may impact people's evaluation of explanations and reliance on LLMs.



We contribute to this line of work in several ways. We first conduct a qualitative, think-aloud study to understand what features of LLM responses shape people's reliance, and find that reliance is shaped by explanations, inconsistencies in explanations, and sources. We then conduct a larger-scale, pre-registered, controlled experiment to quantitatively examine the effects of these features.
While a previous work by **Gur et al., "Uncertainty Quantification"** has studied the effects of LLM-generated explanations and inconsistencies on people's fact-checking performance through a small-scale study (16 participants per condition), our work provides a more holistic picture by studying what (else) might contribute to reliance and how the identified features affect a wider range of variables including people's evaluation of the LLM response's justification quality and actionability and likelihood of asking follow-up questions. 
As for the findings, first, consistent with ____ , we find that explanations increase people's reliance, including overreliance on incorrect answers, and that inconsistencies in explanations can reduce overreliance. 
Additionally, we find that clickable sources --- which were not studied by ____ --- increase appropriate reliance on correct answers, while reducing overreliance on incorrect answers, adding empirical knowledge on user reliance on LLMs. 
Lastly, our work also contributes nuanced insights on people's interpretation of LLMs' explanations, source clicking behavior, and interaction effects between explanations and sources.



\subsection{Sources}

The final feature of LLM responses that we study is the presence of \emph{sources}, i.e., clickable links to external material.\footnote{One might consider sources to be a component of an explanation. To simplify the exposition of our results, we treat them as a distinct component of LLM responses throughout this paper.} Sources are increasingly provided by LLM-infused applications, including general-purpose chatbots (e.g., ChatGPT, Gemini) and search engines (e.g., Perplexity AI, Copilot in Bing, SearchGPT). Sources are commonly sought by users, as found in prior work ____ and supported in our studies. Similar to explanations, however, sources in LLM responses can be flawed in various ways ____ . For instance, **Krause et al., "Adversarial Attacks on LLMs"** conducted a human evaluation of popular LLM-infused search engines and found that their responses frequently contain inaccurate sources and unsupported statements. **Bender et al., "On the Dangers of Stochastic Parrots"** conducted a case study of ChatGPT in the medical domain and found that it generates fake sources. These issues were observed in our studies as well. Currently there is active research on techniques such as Retrieval Augmented Generation (RAG) ____ to help LLMs provide more accurate information and sources.


It is well known that the presence and quality of sources impact how credible people find given content in other settings ____ . However, there has been little work studying how people make use of and rely on sources in the context of LLM-infused applications. On the one hand, the presence of sources might reduce overreliance if people click on the provided links to verify the accuracy of the LLM's response. On the other hand, the presence of sources might increase reliance if people interpret them as signs of credibility and defer to the system without verifying the answers themselves. Indeed, in one study of uncertainty communication in LLM-infused search, **Chen et al., "Uncertainty Estimation"** found that some types of (un)certainty information can help foster appropriate reliance.
Contributing to this line of work, we first take a bottom-up approach to identify the features of LLM responses that impact user reliance in the context of answering objective questions with the assistance of a popular LLM-infused application ChatGPT (\cref{sec:study1}).
In line with findings from prior work ____ , we see that reliance is shaped by the content of \textit{explanations} provided by the system, particularly whether or not these explanations contain \textit{inconsistencies}. We also observe that participants seek out \textit{sources} to verify the information provided in responses. We then design a large-scale, pre-registered, controlled experiment to isolate and study the effects of these features (\cref{sec:study2}). We discuss the relevant literature on these features and their impact on AI reliance next.


\subsection{Explanations and Inconsistencies}
\label{sec:llmresponses}


The impact of \emph{explanations} on human understanding and trust of AI systems has been studied extensively within the machine learning and human-computer interaction communities, often under the names explainable AI or interpretable machine learning ____ . Explanations are often motivated as a way to foster appropriate reliance and trust in AI systems, since in principle they provide clues about whether a system's outputs are reliable. However, empirical studies have shown mixed results, with a large body of work suggesting that providing explanations increases people's tendency to rely on an AI system even when it is incorrect ____ . One potential reason for this is that study participants do not make the effort to deeply engage with the explanations ____ . That is, instead of encouraging deep, analytical reasoning (System 2 thinking ____ ), study participants may resort to heuristics, such as the explanation's fluency or superficial cues to expertise ____ , and defer to the system's response on this basis. People may also be more likely to assume an AI system is trustworthy simply because it provides explanations ____ . Further, some clues of unreliability may be difficult to pick up on without existing domain knowledge ____.


Adopting the broad definition of an explanation as an answer to a why question ____ , LLMs often provide explanations by default; when asked a question, LLMs rarely provide the answer alone. For factual questions, they provide details supporting the answer ____ , and for math questions, they provide detailed steps to derive the answer ____ . 
This default behavior is likely due to human preference for verbose responses ____ .
Research in psychology has shown that explanations are often sought spontaneously ____ , favored when they are longer, more detailed, or perceived to be more informative ____ , and used to guide subsequent judgments and behaviors ____ . 
Since LLMs are often fine-tuned on human preference data via approaches such as Reinforcement Learning from Human Feedback (RLHF) ____ , such preferences would shape the form of their outputs. We note that the default explanations that LLMs present typically provide evidence to support their answers, but do not necessarily reflect the internal processes by which the LLM arrived at the answer. This distinguishes these explanations from those traditionally studied in the explainable AI literature.


Explanations generated by LLMs are widely known to contain inaccurate information and other flaws ____ . We direct readers to recent surveys for comprehensive overviews ____ . In our studies, we found \textit{inconsistencies} in explanations to be an important unreliability cue that shapes participants' reliance. As documented in prior work ____ , inconsistencies can occur within a response; they are sometimes referred to as logical fallacies or self-inconsistency in the NLP community ____ . Inconsistencies can also occur between responses; many studies have demonstrated that LLMs often change their answer to a question when challenged, asked the question in a slightly different way, or re-asked the exact same question ____ . Such inconsistencies, when noticed, may impact people's evaluation of explanations and reliance on LLMs.



We contribute to this line of work in several ways. We first conduct a qualitative, think-aloud study to understand what features of LLM responses shape people's reliance, and find that reliance is shaped by explanations, inconsistencies in explanations, and sources. We then conduct a larger-scale, pre-registered, controlled experiment to quantitatively examine the effects of these features.
While a previous work by **Gur et al., "Uncertainty Quantification"** has studied the effects of LLM-generated explanations and inconsistencies on people's fact-checking performance through a small-scale study (16 participants per condition), our work provides a more holistic picture by studying what (else) might contribute to reliance and how the identified features affect a wider range of variables including people's evaluation of the LLM response's justification quality and actionability and likelihood of asking follow-up questions. 
As for the findings, first, consistent with ____ , we find that explanations increase people's reliance, including overreliance on incorrect answers, and that inconsistencies in explanations can reduce overreliance. 
Additionally, we find that clickable sources --- which were not studied by ____ --- increase appropriate reliance on correct answers, while reducing overreliance on incorrect answers, adding empirical knowledge on user reliance on LLMs. 
Lastly, our work also contributes nuanced insights on people's interpretation of LLMs' explanations, source clicking behavior, and interaction effects between explanations and sources.



\subsection{Sources}

The final feature of LLM responses that we study is the presence of \emph{sources}, i.e., clickable links to external material.\footnote{One might consider sources to be a component of an explanation. To simplify the exposition of our results, we treat them as a distinct component of LLM responses throughout this paper.} Sources are increasingly provided by LLM-infused applications, including general-purpose chatbots (e.g., ChatGPT, Gemini) and search engines (e.g., Perplexity AI, Copilot in Bing, SearchGPT). Sources are commonly sought by users, as found in prior work ____ and supported in our studies. Similar to explanations, however, sources in LLM responses can be flawed in various ways ____ . For instance, **Krause et al., "Adversarial Attacks on LLMs"** conducted a human evaluation of popular LLM-infused search engines and found that their responses frequently contain inaccurate sources and unsupported statements. **Bender et al., "On the Dangers of Stochastic Parrots"** conducted a case study of ChatGPT in the medical domain and found that it generates fake sources. These issues were observed in our studies as well. Currently there is active research on techniques such as Retrieval Augmented Generation (RAG) ____ to help LLMs provide more accurate information and sources.


It is well known that the presence and quality of sources impact how credible people find given content in other settings ____ . However, there has been little work studying how people make use of and rely on sources in the context of LLM-infused applications. On the one hand, the presence of sources might reduce overreliance if people click on the provided links to verify the accuracy of the LLM's response. On the other hand, the presence of sources might increase reliance if people interpret them as signs of credibility and defer to the system without verifying the answers themselves. Indeed, in one study of uncertainty communication in LLM-infused search, **Chen et al., "Uncertainty Estimation"** found that some types of (un)certainty information can help foster appropriate reliance.
Contributing to this line of work, we first take a bottom-up approach to identify the features of LLM responses that impact user reliance in the context of answering objective questions with the assistance of a popular LLM-infused application ChatGPT (\cref{sec:study1}).
In line with findings from prior work ____ , we see that reliance is shaped by the content of \textit{explanations} provided by the system, particularly whether or not these explanations contain \textit{inconsistencies}. We also observe that participants seek out \textit{sources} to verify the information provided in responses. We then design a large-scale, pre-registered, controlled experiment to isolate and study the effects of these features (\cref{sec:study2}). We discuss the relevant literature on these features and their impact on AI reliance next.


\subsection{Explanations and Inconsistencies}
\label{sec:llmresponses}


The impact of \emph{explanations} on human understanding and trust of AI systems has been studied extensively within the machine learning and human-computer interaction communities, often under the names explainable AI or interpretable machine learning ____ . Explanations are often motivated as a way to foster appropriate reliance and trust in AI systems, since in principle they provide clues about whether a system's outputs are reliable. However, empirical studies have shown mixed results, with a large body of work suggesting that providing explanations increases people's tendency to rely on an AI system even when it is incorrect ____ . One potential reason for this is that study participants do not make the effort to deeply engage with the explanations ____ . That is, instead of encouraging deep, analytical reasoning (System 2 thinking ____ ), study participants may resort to heuristics, such as the explanation's fluency or superficial cues to expertise ____ , and defer to the system's response on this basis. People may also be more likely to assume an AI system is trustworthy simply because it provides explanations ____ . Further, some clues of unreliability may be difficult to pick up on without existing domain knowledge ____.


Adopting the broad definition of an explanation as an answer to a why question ____ , LLMs often provide explanations by default; when asked a question, LLMs rarely provide the answer alone. For factual questions, they provide details supporting the answer ____ , and for math questions, they provide detailed steps to derive the answer ____ . 
This default behavior is likely due to human preference for verbose responses ____ .
Research in psychology has shown that explanations are often sought spontaneously ____ , favored when they are longer, more detailed, or perceived to be more informative ____ , and used to guide subsequent judgments and behaviors ____ . 
Since LLMs are often fine-tuned on human preference data via approaches such as Reinforcement Learning from Human Feedback (RLHF) ____ , such preferences would shape the form of their outputs. We note that the default explanations that LLMs present typically provide evidence to support their answers, but do not necessarily reflect the internal processes by which the LLM arrived at the answer. This distinguishes these explanations from those traditionally studied in the explainable AI literature.


Explanations generated by LLMs are widely known to contain inaccurate information and other flaws ____ . We direct readers to recent surveys for comprehensive overviews ____ . In our studies, we found \textit{inconsistencies} in explanations to be an important unreliability cue that shapes participants' reliance. As documented in prior work ____ , inconsistencies can occur within a response; they are sometimes referred to as logical fallacies or self-inconsistency in the NLP community ____ . Inconsistencies can also occur between responses; many studies have demonstrated that LLMs often change their answer to a question when challenged, asked the question in a slightly different way, or re-asked the exact same question ____ . Such inconsistencies, when noticed, may impact people's evaluation of explanations and reliance on LLMs.



We contribute to this line of work in several ways. We first conduct a qualitative, think-aloud study to understand what features of LLM responses shape people's reliance, and find that reliance is shaped by explanations, inconsistencies in explanations, and sources. We then conduct a larger-scale, pre-registered, controlled experiment to quantitatively examine the effects of these features.
While a previous work by **Gur et al., "Uncertainty Quantification"** has studied the effects of LLM-generated explanations and inconsistencies on people's fact-checking performance through a small-scale study (16 participants per condition), our work provides a more holistic picture by studying what (else) might contribute to reliance and how the identified features affect a wider range of variables including people's evaluation of the LLM response's justification quality and actionability and likelihood of asking follow-up questions. 
As for the findings, first, consistent with ____ , we find that explanations increase people's reliance, including overreliance on incorrect answers, and that inconsistencies in explanations can reduce overreliance. 
Additionally, we find that clickable sources --- which were not studied by ____ --- increase appropriate reliance on correct answers, while reducing overreliance on incorrect answers, adding empirical knowledge on user reliance on LLMs. 
Lastly, our work also contributes nuanced insights on people's interpretation of LLMs' explanations, source clicking behavior, and interaction effects between explanations and sources.



\subsection{Sources}

The final feature of LLM responses that we study is the presence of \emph{sources}, i.e., clickable links to external material.\footnote{One might consider sources to be a component of an explanation. To simplify the exposition of our results, we treat them as a distinct component of LLM responses throughout this paper.} Sources are increasingly provided by LLM-infused applications, including general-purpose chatbots (e.g., ChatGPT, Gemini) and search engines (e.g., Perplexity AI, Copilot in Bing, SearchGPT). Sources are commonly sought by users, as found in prior work ____ and supported in our studies. Similar to explanations, however, sources in LLM responses can be flawed in various ways ____ . For instance, **Krause et al., "Adversarial Attacks on LLMs"** conducted a human evaluation of popular LLM-infused search engines and found that their responses frequently contain inaccurate sources and unsupported statements. **Bender et al., "On the Dangers of Stochastic Parrots"** conducted a case study of ChatGPT in the medical domain and found that it generates fake sources. These issues were observed in our studies as well. Currently there is active research on techniques such as Retrieval Augmented Generation (RAG) ____ to help LLMs provide more accurate information and sources.


It is well known that the presence and quality of sources impact how credible people find given content in other settings ____ . However, there has been little work studying how people make use of and rely on sources in the context of LLM-infused applications. On the one hand, the presence of sources might reduce overreliance if people click on the provided links to verify the accuracy of the LLM's response. On the other hand, the presence of sources might increase reliance if people interpret them as signs of credibility and defer to the system without verifying the answers themselves. Indeed, in one study of uncertainty communication in LLM-infused search, **Chen et al., "Uncertainty Estimation"** found that some types of (un)certainty information can help foster appropriate reliance.
The requested response was not provided within the specified 10-step limit