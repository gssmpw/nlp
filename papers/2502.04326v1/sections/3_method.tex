
\section{WorldSense}
In this section, we aim to detail the construction of WorldSense, 
including data collection procedure, annotation pipeline, and statistics.
Unlike existing benchmarks that assess modalities in isolation, WorldSense evaluates the MLLMs’ ability to perceive, understand, and reason about real-world scenarios through the integration of omni-modal information. As shown in Figure~\ref{fig:example}, all the multiple-choice questions are carefully crafted, to make sure the questions can only be answered through the comprehensive analysis of text, vision, and audio.




\subsection{Design Principle} 
\label{sec:design}

As for multi-modal evaluation, we base on the audio-visual synchronized videos, 
which capture temporal events, motion patterns, and audio-visual correlations. 
To curate the benchmark, we adhere to the following three principles, to ensure a rigorous and comprehensive evaluation.

\begin{table*}[htbp]
  \caption{\textbf{Static comparison.} A, V, I for modality represent audio, video, and image. \textbf{Len.} refers to the mean video duration in seconds. A and M for \textbf{Anno.} indicate automatic and manual annotation generation. \textbf{QA Tokens} represents the average token count in QA pairs, while \textbf{Sub. Tokens} denotes the mean number of subtitle tokens. \textbf{Multi-task} represents whether the dataset encompasses more than two question categories. \textbf{Open-domain} signifies whether the video content spans diverse domains. \textbf{Sub./Aud.} ispecifies the availability of audio signals or subtitle transcriptions. \textbf{A-V Correlations} indicates whether answering questions requires integration of omnimodal information.}
  \label{tab:static}
  \centering
  \begin{adjustbox}{width=\textwidth}
    \input{tables/static}
  \end{adjustbox}
  \vspace{-4mm}
\end{table*}

\textbf{Comprehensive Domain.} 
To ensure a thorough evaluation of MLLMs’ real-world understanding, we developed a systematic taxonomy that covers diverse domains and scenarios. 
This process started with primary categories reflecting core aspects of human experience, which were further subdivided into 67 subcategories to capture specific contexts. 
This hierarchical structure ensures that our video collection spans a broad range of real-world experiences, providing an ecologically valid basis for assessing multimodal understanding.

\textbf{Diverse Acoustic Signals.} 
In real-world scenarios, audio signals can be principally categorized into three fundamental types: speech, event and music. Our benchmark incorporates all three to ensure comprehensive coverage, enabling MLLMs to process and understand a wide spectrum of acoustic information—from semantic speech to abstract music and environmental sounds.



\textbf{Multilevel Assessment.} 
To evaluate the MLLMs’ perceptual and cognitive capabilities, we designed a multi-scale assessment at three levels: recognition~(basic audiovisual element detection), understanding~(comprehension of multimodal relationships), and reasoning~(advanced cognitive tasks like causal inference and abstract thinking). We developed 26 tasks across to assess different aspects of multimodal comprehension, with a focus on integrating audio-visual information at each level. 




\subsection{Data Collection \& Curation }

We primarily source our video content from FineVideo~\cite{Farr2024FineVideo}, a large-scale dataset of high-quality YouTube videos with strong audio-visual correlations across diverse real-world scenarios. To enhance the coverage of musical content, we additionally incorporate videos from MusicAVQA~\cite{li2022learning}.


Our data collection process involves a systematic filtering pipeline to ensure high-quality videos with rich visual-audio semantics and temporal dynamics, as illustrated in Figure~\ref{fig:data_anno}(a). The process follows three key steps: (i) filtering videos based on predefined taxonomic categories described in Section~\ref{sec:design} for comprehensive coverage; (ii) utilizing pre-computed metrics, including audio-visual correlation and dynamic content scores, to identify significant clips from an initial pool of approximately 8,000 videos; and (iii) conducting human expert review to assess video quality and real-world relevance. This rigorous process ultimately yields 1,662 high-quality video segments featuring strong audio-visual correlations across diverse real-world contexts.






\subsection{Annotation Protocol} 
\textbf{QA Annotation.} 
A team of 80 professional annotators is engaged in creating high-quality multiple-choice QA pairs. For each video clip, annotators conduct thorough reviews of both visual and auditory content to ensure comprehensive understanding. They then generate questions and corresponding answer options that specifically require the integration of both visual and audio information for correct responses, thereby enabling effective evaluation of MLLMs' multimodal understanding capabilities.


\textbf{Quality Control.} 
To ensure the quality of question-answer pairs, we implement a rigorous quality control process that combines human expertise with automated verification, as illustrated in Figure~\ref{fig:data_anno}(b). Professional quality control experts evaluate each QA pair based on three essential criteria: (i) linguistic clarity and coherence, (ii) necessity of both visual and audio information for correct answers, and (iii) appropriateness of the question's difficulty. Questions that fail to meet these standards are returned for revision.

We additionally employ MLLMs for automated verification. Vision-language models like Qwen2-VL\cite{wang2024qwen2} verify that questions require multiple modalities for correct answers. Furthermore, multimodal MLLMs capable of processing video, audio, and text, such as Video-LLaMA2\cite{cheng2024videollama} and OneLLM~\cite{han2024onellm} are used to assess question difficulty, with questions answered correctly by all models being flagged for manual revision as too simple.

This dual-verification system, combining expert review and automated testing, ensures that all questions in our benchmark are of high-quality and well-formulated, that requires multi-modal comprehension, and present significant challenges for the models. 

\begin{table*}[htbp]
  \caption{\textbf{Overall performance on WorldSense.} 
  We evaluate three types of MLLMs on WorldSense, 
  showing the significant limitations of existing MLLMs on real-world multi-modal understanding.}
  \label{tab:main_performance}
  \centering
  \begin{adjustbox}{width=\textwidth}
    \input{tables/main}
  \end{adjustbox}
  \vspace{-6mm}
\end{table*}




\subsection{Dataset Statistics}

As summarized in Table~\ref{tab:static}, the WorldSense dataset contains 1,662 video clips with synchronized audio, distributed across 8 primary categories and 67 subcategories. 
The average duration is 141.1 seconds, with lengths ranging from 30 seconds to over 10 minutes, capturing a wide variety of events and activities. In total, WorldSense includes 3,173 multiple-choice questions, covering three levels.

WorldSense encompasses diverse audio components including speech, environmental sounds, and music. Unlike existing benchmarks that use static images~({\em e.g.}, AV-Odyssey Bench~\cite{gong2024av}, OmniBench~\cite{li2024omnibench}) or feature weak audio-visual correlations~({\em e.g.}, Video-MME~\cite{fu2024video}), WorldSense is the first benchmark designed to evaluate MLLMs' real-world multimodal understanding. It distinguishes itself through: (i) open-domain videos with multi-task evaluation, (ii) original audio-visual content with complete transcriptions, and (iii) carefully crafted questions requiring true audio-visual integration, establishing a more comprehensive benchmark for real-world multimodal understanding assessment.

\subsection{Evaluation Paradigm}
In our evaluation framework, each test instance consists of a video clip with synchronized audio and a multiple-choice question. Models must process these multi-modal inputs and select the correct answer from several options. Performance is measured by accuracy, comparing the model’s selection to the ground-truth answers. 
A model’s success is determined by its ability to accurately align with the correct answer.

To rigorously assess the necessity of multimodal integration in real-world understanding, we conduct ablation studies across various modality configurations. This approach not only evaluates overall model performance but also quantifies the models’ reliance on individual modalities, highlighting the critical role of multimodal collaboration in real-world comprehension tasks.











