
\begin{figure}[htbp]
  \vspace{-1mm}
  \centering
  \includegraphics[width=0.97\linewidth]{pictures/data.pdf}
  \caption{\textbf{Data collection and QA annotation pipelines.} 
  (a) Data collection and curation process. (b) QA annotation and quality control pipeline.
  }
  \label{fig:data_anno}
  \vspace{-6mm}
\end{figure}

\section{Related Work}
\textbf{Multimodal Large Language Models.} 
Current Large Language Models (LLMs) are capable of processing multimodal information, including visual, text, and audio. Early works, such as~\cite{zhang2023llama,liu2024visual,zhu2023minigpt,driess2023palm,wang2024visionllm,pi2023detgpt}, successfully combine vision and text modalities. Subsequent research extends to temporal understanding~\cite{wang2024emu3,hurst2024gpt,team2024gemini,liu2024nvila,wang2024qwen2,wang2024longllava,li2024llava,fang2024vila,xu2024slowfast,zhang2024internlm,tong2024cambrian,chen2024far,lu2024deepseek,liu2024llava}, while parallel efforts~\cite{tang2023salmonn,chu2023qwen,chu2024qwen2} focus on audio processing. Recently, researchers shift attention to models~\cite{cheng2024videollama,sun2024video,team2024gemini,lu2024unified,team2024reka} capable of simultaneously processing text, vision, and audio inputs. Despite the growing interest in the models which can perform the omnimodality understanding, the absence of a comprehensive evaluation benchmark restricts the development. To address this limitation, we introduce our \textbf{WorldSense} to evaluate models' capabilities in perceiving and understanding real world omnimodal scenarios.




\textbf{Multimodal Benchmarks.} 
The development of MLLMs has been driven by benchmarks, evolving from static image understanding~\cite{zhang2024mme,liu2025mmbench,li2023seed,li2024seed,fu2024mmecomprehensiveevaluationbenchmark,yue2024mmmu} to temporal comprehension~\cite{li2024mvbench,liu2024tempcompass,song2024moviechat,zhou2024mlvu,fang2024mmbench,fu2024video,he2024mmworld,wang2024lvbench,xu2017video,yu2019activitynet,lin2024streamingbench,chandrasegaran2024hourvideo}. However, these benchmarks largely overlook the crucial role of audio in real-world perception. While several audio-visual benchmarks have been proposed, they face significant limitations. AV-Odyssey Bench~\cite{gong2024av} and OmniBench~\cite{li2024omnibench} focus on static images, Music-AVQA~\cite{li2022learning} and AVQA~\cite{yang2022avqa} are domain-specific with monotonous questions, and LongVALE~\cite{geng2024longvale} limits its assessment to captioning capabilities alone. Given that existing benchmarks fail to provide a comprehensive evaluation of MLLMs' real-world understanding capabilities, we introduce \textbf{WorldSense} to address this critical gap in the field.


