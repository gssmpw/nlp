\section{Experiments and Findings}
This section presents a comprehensive evaluation of both open-source and proprietary MLLMs on the \textbf{WorldSense} benchmark. We first delineate our experimental methodology and evaluation protocols, followed by a comprehensive analysis of quantitative results. Furthermore, we conduct detailed investigations into the important factors that affect performance, providing insights that illuminate potential directions for multi-modal understanding.

\begin{figure}
  \centering
  \includegraphics[width=0.9\linewidth]{pictures/fine_result.pdf}
  \caption{\textbf{Breakdown results.} (a) Fine-grained results on task category. (b) Fine-grained results on audio signals.}
  \label{fig:breakdown_result}
  \vspace{-6mm}
\end{figure}



\subsection{Settings}
To comprehensively assess the multi-modal understanding ability, 
we evaluate three types of MLLMs: (1) open-source audio-visual models, 
such as Unified-IO-2~\cite{lu2024unified}, OneLLM~\cite{han2024onellm}, and VideoLLaMA2~\cite{cheng2024videollama}; 
(ii) open-source MLLMs, such as Qwen2-VL~\cite{Qwen2VL}, LLaVA-OneVision~\cite{li2024llava}, InternVL2.5~\cite{chen2024expanding}, LLaVA-Video~\cite{zhang2024video}, and so on; 
(iii) proprietary MLLMs, such as Claude 3.5 Sonnet~\cite{claude}, GPT 4o~\cite{hurst2024gpt}, and Gemini 1.5 Pro~\cite{team2024gemini}. 
For all evaluations, we strictly adhere to each model's official implementation guidelines and the recommended pre-processing procedures. 
Video frame extraction follows the official configurations specified by corresponding MLLMs, while proprietary models are evaluated according to their API specifications and recommended input formats. Model performance is assessed through direct comparison between model outputs and ground-truth answers.





\subsection{Results on WorldSense}

\textbf{Main Results.} 
We present the comprehensive evaluation results on WorldSense in Table~\ref{tab:main_performance}. Our findings reveal several significant insights regarding the current state of multi-modal models in real-world understanding.



First, current open-source video models are limited in their performance as they process only visual information. This restriction highlights a significant gap in their ability to perform complex, multi-modal understanding tasks, as evidenced by their maximum performance score of only $40.2\%$. The results underscore the inadequacies of relying solely on visual processing, emphasizing the need to integrate audio inputs for a more comprehensive and accurate understanding in practical applications. 

\begin{table*}[htbp]
  \caption{\textbf{Impact of vision information.} We evaluate MLLMsâ€™ performance under different input configurations: audio-only input, audio combined with either video captions or video frames. }
  \label{tab:ablation_vision}
  \centering
  \begin{adjustbox}{width=\textwidth}
    \input{tables/ablation_vision}
  \end{adjustbox}
  \vspace{-6mm}
\end{table*}

Second and surprisingly, existing open-source audio-visual MLLMs perform even worse, achieving accuracy rates comparable to random guessing and notably below video-only MLLMs. This counter-intuitive finding reveals that despite having access to both modalities, these models struggle with effective audio-visual integration, suggesting that multimodal processing capability alone does not guarantee better performance without sophisticated integration mechanisms.


Third, among proprietary MLLMs, vision-only models GPT-4o and Claude 3.5 Sonnet perform similarly to the best open-source video MLLMs. Gemini 1.5 Pro, capable of processing both audio and visual information, achieves the highest accuracy of $48.0\%$. However, this performance still falls considerably short of requirements for reliable real-world applications, indicating substantial room for improvement.


These comprehensive results illuminate several critical insights: 
(i) the fundamental importance of audio-visual collaborative understanding in real-world scenarios; (ii) the current significant gap in models' capabilities for effective multimodal integration, and (iii) the need for more sophisticated approaches to combining and reasoning about multiple modalities. These findings point to crucial directions for future research and development in MLLMs.



\textbf{Breakdown Results.} We conduct a fine-grained analysis of model performance across different audio types and task categories, as shown in Figure~\ref{fig:breakdown_result}, revealing important insights into the limitations of existing multimodal models.

First, models consistently underperform on audio-related tasks ({\em e.g.}, audio recognition, audio counting) compared to other task types, demonstrating significant challenges in audio understanding. Second, spatial reasoning and counting tasks present notable difficulties for current models, a pattern consistently observed across multiple benchmarks. Third, emotion-related tasks prove particularly challenging, likely due to their requirement for integrating subtle and complex multimodal cues, including facial expressions, vocal tones, and contextual speech content. This underperformance in emotional understanding suggests a significant gap in current MLLMs' training data and capabilities, highlighting an important area for future development.

Additionally, performance varies across audio types. While Gemini 1.5 Pro performs best overall, it shows notably lower accuracy on event-related questions compared to speech or music tasks, possibly due to the complex nature of environmental sounds. Other models also exhibit inconsistent performance across audio types, underscoring a general limitation for existing models for audio understanding.


\subsection{Roadmap Towards Real-world Understanding}
Given the substantial performance gap revealed in above evaluation, 
we conduct an in-depth investigation into potential approaches to enhance the MLLMs' performance.

\textbf{Vision Information.} 
We investigate the impact of visual information through different input configurations: audio-only, audio with video captions, and audio with video frames. As shown in Table~\ref{tab:ablation_vision}, visual information generally improves performance, with Gemini 1.5 Pro's accuracy increasing from $34.6\%$ (audio-only) to $48.0\%$ (+video). However, impact varies across models, with UnifiedIO2 showing inconsistent gains and even degradation with captions. 

These findings suggest two important insights: (1) visual information is crucial for enhancing multi-modal understanding when properly integrated, and (2) current models' ability to effectively utilize visual information remains limited. 

\textbf{Audio Information.}
We examine the impact of audio information through three configurations: video-only, video with subtitles, and video with original audio. 

The results in Table~\ref{tab:ablation_audio} reveal intriguing patterns in how different forms of audio information influence model performance. For Gemini 1.5 Pro, accuracy increases from $34.4\%$ (video-only) to $39.3\%$ with subtitles, and further to $48.0\%$ with original audio. OneLLM shows similar improvements. These results demonstrate that both subtitles and acoustic features (including tone, emotion, and environmental sounds) contribute valuable information for multimodal understanding, beyond what subtitles alone can capture, highlighting the importance of complete acoustic cues for multimodal understanding.

Interestingly, UnifiedIO2 demonstrates performance degradation when integrating either subtitles or audio, with subtitles causing a notable accuracy decline, suggesting difficulties in multimodal processing. Conversely, Video-LLaMA2 improves with both modalities but performs better with subtitles than original audio, indicating a stronger reliance on textual rather than complex acoustic information.
We further evaluate video-only MLLMs by providing transcribed subtitles, as shown in Table~\ref{tab:ablation_audio_v}. Nearly all models show significant improvements with subtitle integration, reinforcing the importance of audio information. However, the performance gain is less pronounced in music-related questions, as subtitles cannot effectively capture inherent acoustic features such as melody, rhythm, and harmony.

These evaluations highlight several critical findings: (i) original audio contains rich information beyond what subtitles can capture, particularly for music; (ii) current models show significant limitations in multimodal processing. 
These insights suggest important directions for improving MLLMs' ability to integrate acoustic and textual information for comprehensive scene understanding.



\begin{table*}[t]
  \label{tab:ablations}
  \centering
  
  \begin{minipage}{\textwidth}
    \centering
    \caption{\textbf{Impact of audio information for Video-Audio MLLMs.} We conduct experiments across three input configurations: video-only, video with subtitles, and video with original audio.}
    \label{tab:ablation_audio}
    \begin{adjustbox}{width=\textwidth}
      \input{tables/ablation_audio}
    \end{adjustbox}
  \end{minipage}  
  \hfill
  \vspace{6mm}
  \begin{minipage}{\textwidth}
    \centering
    \caption{\textbf{Impact of audio information for Video MLLMs.} We provide video-only MLLMs with the subtitles and compare the performance with models with only video input.}
    \label{tab:ablation_audio_v}
    \tiny
    \renewcommand{\arraystretch}{0.2}
    \begin{adjustbox}{width=\textwidth}
      \input{tables/ablation_audio_v}
    \end{adjustbox}
  \end{minipage}
\end{table*}

\begin{figure}
  \centering
  \includegraphics[width=0.9\linewidth]{pictures/video_frame_curve.png}
  \caption{\textbf{Impact of video frames.} We compare performance of several Video MLLMs with different frames input.}
  \label{fig:ablation_video_frame}
  \vspace{-6mm}
\end{figure}

\textbf{Video Frames.} 
We investigate the effect of temporal sampling density by varying input frame numbers for video-only MLLMs. As shown in Figure~\ref{fig:ablation_video_frame}, most models demonstrate significant performance improvements with increased frame density, with LLaMA-3.2 being a notable exception. These gains likely result from better capture of fine-grained temporal dynamics and subtle visual changes, emphasizing the importance of dense temporal sampling.




