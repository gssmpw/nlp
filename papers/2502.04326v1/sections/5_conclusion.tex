\section{Conclusion}
In this paper, we introduce WorldSense, the \textbf{\textit{first}} benchmark designed to evaluate MLLMs' omnimodal understanding in real-world scenarios. Distinguished by its emphasis on joint omnimodal comprehension across diverse real-world contexts, WorldSense encompasses rich video categories and carefully curated question-answer pairs that necessitate the integration of visual and acoustic information. Through extensive experiments, we expose significant limitations in current MLLMs' ability to process and coherently integrate omnimodal information. Our analysis demonstrates the importance of omnimodal collaboration in real-world understanding. We hope that WorldSense can serve as a foundational benchmark for advancing human-like omnimodal understanding capabilities.

