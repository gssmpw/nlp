\begin{figure*}[!htb]
  \centering
  \includegraphics[width=0.95\linewidth]{pictures/sample.pdf}
  \caption{\textbf{Examples in WorldSense.} 
  Unlike existing benchmarks, WorldSense emphasizes the tight coupling of audio-visual perception in real-world scenarios, where accurate comprehension relies on integrating both modalities, as neither alone provides sufficient context for correct answers. In the \textbf{first} example, the video shows a man holding a fruit, but visual information alone is insufficient to determine his specific action, while audio alone cannot identify the fruit type. In the \textbf{second} example, both visual cues and audio are necessary to identify the country elements and determine which segment of music is “lively and energetic.” Through this design, WorldSense aims to serve as a platform for evaluating MLLMs’ real-world perception and understanding capabilities using omni-modal information.}
  \label{fig:example}
  \vspace{-6mm}
\end{figure*}

\section{Introduction}


The ability to comprehend and reason about multimodal inputs—ranging from visual and textual to auditory, tactile, and beyond—is fundamental for both human and artificial agents to navigate and interpret the world. For example, when driving a car, a human driver integrates visual information ({\em e.g.}, recognizing road signs, traffic lights, and obstacles), auditory cues ({\em e.g.}, hearing the honking of another car or a siren approaching from behind), and tactile feedback ({\em e.g.}, the feel of the steering wheel, the vibrations of the road, or the responsiveness of the brakes) to make real-time decisions and ensure safe navigation. This seamless multimodal integration enables intelligent agents to process complex, dynamic environments and respond to subtle cues—an ability that is essential for both human perception and the development of embodied agents designed to interact naturally in the world.

In the recent literature, the development of Multi-modal Large Language Models (MLLMs)~\cite{openai2023gpt,hurst2024gpt,openai2024gptv,team2023gemini,team2024reka,zhang2023llama,ma2024visual,fang2023instructseq} have led to remarkable progress on a series of tasks, for example, classification~\cite{liu2024revisiting}, captioning~\cite{alayrac2022flamingo,dai2023instructblip,liu2024visual}, question-answering~\cite{tang2024mtvqa,panagopoulou2023x,liu2024mmdu}, OCR~\cite{mathew2021docvqa,zhang2024vcr}, segmentation~\cite{lai2024lisa,xia2024gsva,he2024multi}, autonomous driving~\cite{nie2025reason2drive,sima2025drivelm,chen2024driving} and more. However, multi-modal analysis  primarily focuses on visual-language information, leaving out crucial modalities like audio, which results in an incomplete evaluation of their multimodal capabilities. While some benchmarks have begun incorporating both visual and audio modalities, they still exhibit several limitations. For example, OmniBench~\cite{li2024omnibench} and AV-Odyssey Bench~\cite{gong2024av} mainly emphasize image evaluation, whereas other benchmarks~\cite{geng2024longvale,li2022learning,yang2022avqa} either restrict to captioning tasks or are limited to simple scenarios, 
or suffer from low-quality, monotonous questioning patterns. 


In this paper, we introduce \textbf{WorldSense}, the first benchmark designed to comprehensively evaluate MLLMs’ ability to perceive, understand, and reason through the integration of omni-modal information in real-world contexts. WorldSense is defined by three key features: 
\textbf{(i) Collaboration of Omni-Modality:} At the core of WorldSense is the emphasis on the interplay between audio and visual information, as illustrated in Figure~\ref{fig:example}. Each question is designed to require both modalities for a correct answer; the absence of either modality leads to an incorrect response. Such design rigorously tests a model’s ability to simultaneously process multiple sensory inputs for accurate understanding; 
\textbf{(ii) Diversity of Videos and Tasks:} WorldSense encompasses 1,662 audio-visual synchronized videos across 8 domains and 67 fine-grained subcategories. It includes 3,172 multiple-choice questions spanning 26 distinct cognitive tasks, from basic perception to complex reasoning. This diversity enables a systematic and comprehensive evaluation of MLLMs’ multimodal understanding capabilities;
\textbf{(iii) High-Quality Annotations:} To ensure reliability, all QA pairs in WorldSense are manually labeled by 80 expert annotators and undergo multiple rounds of validation through both human review and automatic MLLM verification. This rigorous process ensures the benchmark’s accuracy and reliability. By integrating these features, WorldSense sets a new standard for evaluating MLLMs’ ability to handle real-world complexity, bridging the gap between artificial intelligence and human-like multimodal understanding.

\begin{figure*}[htp]
  \centering
  \includegraphics[width=0.95\linewidth]{pictures/distribution.pdf}
  \caption{\textbf{Distribution of WorldSense.} (a) Videos in WorldSense spans 8 primary categories with 67 fine-grained subcategories. (b) QA pairs are structured across 26 tasks. (c) Acoustic signals distribution. Individual videos may contain multiple audio categories, leading to overlapping counts in statistical analysis. Consequently, the cumulative sum of audio instances exceeds the total video count. (d) Video duration distribution. The average duration of videos is 141.1 seconds. }
  \vspace{-6mm}
  \label{fig:data_dist}
\end{figure*}

We conduct comprehensive experiments on WorldSense to evaluate a range of existing MLLMs, including open-source video models, video-audio models, and proprietary models. Our results highlight significant limitations in the current models’ ability to understand and reason through omni-modal information in real-world contexts. First, while open-source video-audio models can process both video and audio inputs, they achieve only around 25\% accuracy on our benchmark, comparable to random guessing. In contrast, proprietary models, such as Gemini 1.5 Pro, which processes both video and audio, achieve the highest accuracy at 48\%. However, when provided with only a single modality (either video or audio), Gemini 1.5 Pro’s performance drops significantly by approximately 15\%, underscoring the crucial role of omni-modal collaboration in real-world tasks. These findings emphasize the strong coupling of modalities in WorldSense and reveal a substantial gap in current MLLMs’ real-world understanding capabilities.

Additionally, we further conduct detailed ablation studies on how different modalities impact model performance. Our analysis shows that visual information plays significant for understanding, while adding audio—whether as raw signals or transcribed subtitles—also improves results. However, raw audio inputs yield superior gains, emphasizing the importance of paralinguistic features such as prosody, intonation, and acoustic context, which are often lost in text transcription. Additionally, increasing the density of video frame sampling, which provides richer temporal information, consistently improves performance. These findings underscore the complementary nature of visual and auditory modalities and highlight the necessity of integrating both for robust real-world understanding.



To summarise, in this paper, we have made three contributions:
(i) we introduce, \textbf{WorldSense}, the \textbf{\textit{first}} benchmark specifically designed to evaluate MLLMs' omni-modal understanding capabilities in real-world scenarios. WorldSense is characterized by the integration of omni-modality, diverse video categories, and high-quality QA pairs;
(ii) We have conducted extensive experiments to evaluate current MLLMs' capabilities in real-world omni-modal understanding. Our results demonstrate that open-source video-audio MLLMs perform only marginally better than random guessing, while even the best-performing proprietary models achieve only $48\%$ accuracy, revealing significant room for accurate omni-modal understanding; 
(iii) Through detailed analysis, we investigate the crucial factors affecting real-world omni-modal understanding. Our findings demonstrate that acoustic information, visual cues, and dense temporal sampling all substantially impact model performance. These finds provide valuable guidance for the future development of real-world omni-modal understanding.

