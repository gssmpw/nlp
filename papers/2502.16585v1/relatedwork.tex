\section{Related Work}
\subsection{General-domain Phrase Grounding}
Vision-language models pre-trained on large-scale image-text datasets, such as CLIP, have shown strong zero-shot learning and few-shot learning capabilities on global image understanding tasks \cite{pmlr-v139-radford21a}. GLIP extends this by pre-training on large-scale phrase grounding data \cite{9879567}. The learned representations demonstrate strong transferability to various local-level recognition tasks. Current pre-trained general-domain phrase grounding models are typically applied to two primary tasks: phrase localisation and referring expression comprehension. Phrase localisation focuses on identifying and locating multiple objects mentioned in a sentence. MDETR is a phrase localisation model, associating sub-phrases within a sentence with multiple object queries \cite{9710994}. In contrast, TransVG is a referring expression comprehension model---it detects a single object or region in an image for a whole sentence \cite{9710016}.

\subsection{Medical Phrase Grounding}
Due to the scarcity of annotated data, MPG has received limited attention in the literature. Boecking \textit{et al.} introduced MS-CXR, a phrase grounding chest X-ray benchmark dataset \cite{10.1007/978-3-031-20059-5_1}. Their objective with the dataset was to evaluate the grounding performance of their self-supervised biomedical vision-language model (BioViL). BioViL demonstrates strong zero-shot learning capabilities, given that it is not trained for MPG. Recently, Chen \textit{et al.} directly fine-tuned TransVG on a split of MS-CXR in order to directly learn MPG, forming MedRPG \cite{10.1007/978-3-031-43990-2_35}. Here, a bounding box supervised loss and a specific contrastive loss were leveraged. Unlike these models, we pre-train on large-scale anatomical grounding data using Chest ImaGenome, in order to provide in-domain pre-training.

\subsection{Anatomical Information in Medical Imaging}
Anatomical information has been effectively used in tasks like pathology detection and classification to improve accuracy and localisation. For example, the Anatomy-Driven Pathology Detection (ADPD) model \cite{muller_anatomy-driven_2023} used easy-to-annotate anatomical regions as proxies for pathologies, helping to locate disease locations without detailed pathology-specific bounding boxes. AnaXNet \cite{agu_anaxnet_2021} used anatomical relationships to improve classification by identifying the exact regions where findings occur. Despite these successes, no work has applied anatomical information to medical phrase grounding.