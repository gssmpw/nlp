@InProceedings{10.1007/978-3-031-43990-2_35,
author="Chen, Zhihao
and Zhou, Yang
and Tran, Anh
and Zhao, Junting
and Wan, Liang
and Ooi, Gideon Su Kai
and Cheng, Lionel Tim-Ee
and Thng, Choon Hua
and Xu, Xinxing
and Liu, Yong
and Fu, Huazhu",
editor="Greenspan, Hayit
and Madabhushi, Anant
and Mousavi, Parvin
and Salcudean, Septimiu
and Duncan, James
and Syeda-Mahmood, Tanveer
and Taylor, Russell",
title="Medical Phrase Grounding with Region-Phrase Context Contrastive Alignment",
booktitle="MICCAI",
year="2023",
publisher="Springer Nature Switzerland",
address="Cham",
pages="371--381",
abstract="Medical phrase grounding (MPG) aims to locate the most relevant region in a medical image, given a phrase query describing certain medical findings, which is an important task for medical image analysis and radiological diagnosis. However, existing visual grounding methods rely on general visual features for identifying objects in natural images and are not capable of capturing the subtle and specialized features of medical findings, leading to a sub-optimal performance in MPG. In this paper, we propose MedRPG, an end-to-end approach for MPG. MedRPG is built on a lightweight vision-language transformer encoder and directly predicts the box coordinates of mentioned medical findings, which can be trained with limited medical data, making it a valuable tool in medical image analysis. To enable MedRPG to locate nuanced medical findings with better region-phrase correspondences, we further propose Tri-attention Context contrastive alignment (TaCo). TaCo seeks context alignment to pull both the features and attention outputs of relevant region-phrase pairs close together while pushing those of irrelevant regions far away. This ensures that the final box prediction depends more on its finding-specific regions and phrases. Experimental results on three MPG datasets demonstrate that our MedRPG outperforms state-of-the-art visual grounding approaches by a large margin. Additionally, the proposed TaCo strategy is effective in enhancing finding localization ability and reducing spurious region-phrase correlations.",
isbn="978-3-031-43990-2"
}

@article{bernstein_can_2023,
	title = {Can incorrect artificial intelligence ({AI}) results impact radiologists, and if so, what can we do about it? {A} multi-reader pilot study of lung cancer detection with chest radiography},
	volume = {33},
	issn = {1432-1084},
	doi = {10.1007/s00330-023-09747-1},
	abstract = {To examine whether incorrect AI results impact radiologist performance, and if so, whether human factors can be optimized to reduce error.},
	number = {11},
	journal = {European Radiology},
	author = {Bernstein, Michael H. and Atalay, Michael K. and Dibble, Elizabeth H. and Maxwell, Aaron W. P. and Karam, Adib R. and Agarwal, Saurabh and Ward, Robert C. and Healey, Terrance T. and Baird, Grayson L.},
	month = nov,
	year = {2023},
	pages = {8263--8269},
}

@INPROCEEDINGS{10204026,
  author={Tanida, Tim and Müller, Philip and Kaissis, Georgios and Rueckert, Daniel},
  booktitle={CVPR}, 
  title={Interactive and Explainable Region-guided Radiology Report Generation}, 
  year={2023},
  volume={},
  number={},
  pages={7433-7442},
  keywords={Computer vision;Codes;Grounding;Decision making;Focusing;Object detection;Radiology;Medical and biological vision;cell microscopy},
  doi={10.1109/CVPR52729.2023.00718}}

@article{doi:10.1148/ryai.2020190043,
author = {Reyes, Mauricio and Meier, Raphael and Pereira, S\'{e}rgio and Silva, Carlos                             A. and Dahlweid, Fried-Michael and Tengg-Kobligk, Hendrik                             von and Summers, Ronald                             M. and Wiest, Roland},
title = {On the Interpretability of Artificial Intelligence in Radiology:                     Challenges and Opportunities},
journal = {Radiology: Artificial Intelligence},
volume = {2},
number = {3},
pages = {e190043},
year = {2020},
doi = {10.1148/ryai.2020190043},
    note ={PMID: 32510054},
eprint = {  
        https://doi.org/10.1148/ryai.2020190043
}
,
    abstract = { As artificial intelligence (AI) systems begin to make their way into clinical radiology practice, it is crucial to assure that they function correctly and that they gain the trust of experts. Toward this goal, approaches to make AI “interpretable” have gained attention to enhance the understanding of a machine learning algorithm, despite its complexity. This article aims to provide insights into the current state of the art of interpretability methods for radiology AI. This review discusses radiologists’ opinions on the topic and suggests trends and challenges that need to be addressed to effectively streamline interpretability methods in clinical practice. Supplemental material is available for this article. Keywords: Convolutional Neural Network (CNN), Informatics, Radiomics, Supervised learning, Technology Assessment © RSNA, 2020 See also the commentary by Gastounioti and Kontos in this issue. }
}

@INPROCEEDINGS{9879567,
  author={Li, Liunian Harold and Zhang, Pengchuan and Zhang, Haotian and Yang, Jianwei and Li, Chunyuan and Zhong, Yiwu and Wang, Lijuan and Yuan, Lu and Zhang, Lei and Hwang, Jenq-Neng and Chang, Kai-Wei and Gao, Jianfeng},
  booktitle={CVPR}, 
  title={Grounded Language-Image Pre-training}, 
  year={2022},
  volume={},
  number={},
  pages={10955-10965},
  keywords={Visualization;Computer vision;Image recognition;Head;Grounding;Object detection;Data models;Deep learning architectures and techniques; Recognition: detection;categorization;retrieval; Representation learning; Transfer/low-shot/long-tail learning; Vision + language},
  doi={10.1109/CVPR52688.2022.01069}}

@INPROCEEDINGS{9710994,
  author={Kamath, Aishwarya and Singh, Mannat and LeCun, Yann and Synnaeve, Gabriel and Misra, Ishan and Carion, Nicolas},
  booktitle={ICCV}, 
  title={MDETR - Modulated Detection for End-to-End Multi-Modal Understanding}, 
  year={2021},
  volume={},
  number={},
  pages={1760-1770},
  keywords={Visualization;Vocabulary;Image segmentation;Computer vision;Grounding;Detectors;Computer architecture;Vision + language;Detection and localization in 2D and 3D;Machine learning architectures and formulations;Visual reasoning and logical representation},
  doi={10.1109/ICCV48922.2021.00180}}

@InProceedings{10.1007/978-3-031-72970-6_3,
author="Liu, Shilong
and Zeng, Zhaoyang
and Ren, Tianhe
and Li, Feng
and Zhang, Hao
and Yang, Jie
and Jiang, Qing
and Li, Chunyuan
and Yang, Jianwei
and Su, Hang
and Zhu, Jun
and Zhang, Lei",
editor="Leonardis, Ale{\v{s}}
and Ricci, Elisa
and Roth, Stefan
and Russakovsky, Olga
and Sattler, Torsten
and Varol, G{\"u}l",
title="Grounding DINO: Marrying DINO with Grounded Pre-training for Open-Set Object Detection",
booktitle="ECCV",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="38--55",
abstract="In this paper, we develop an open-set object detector, called Grounding DINO, by marrying Transformer-based detector DINO with grounded pre-training, which can detect arbitrary objects with human inputs such as category names or referring expressions. The key solution of open-set object detection is introducing language to a closed-set detector for open-set concept generalization. To effectively fuse language and vision modalities, we conceptually divide a closed-set detector into three phases and propose a tight fusion solution, which includes a feature enhancer, a language-guided query selection, and a cross-modality decoder for modalities fusion. We first pre-train Grounding DINO on large-scale datasets, including object detection data, grounding data, and caption data, and evaluate the model on both open-set object detection and referring object detection benchmarks. Grounding DINO performs remarkably well on all three settings, including benchmarks on COCO, LVIS, ODinW, and RefCOCO/+/g. Grounding DINO achieves a 52.5 AP on the COCO zero-shot (In this paper, `zero-shot' refers to scenarios where the training split of the test dataset is not utilized in the training process) detection benchmark. It sets a new record on the ODinW zero-shot benchmark with a mean 26.1 AP. We release some checkpoints and inference codes at https://github.com/IDEA-Research/GroundingDINO.",
isbn="978-3-031-72970-6"
}

@inproceedings{zhao-titov-2023-transferability,
    title = "On the Transferability of Visually Grounded {PCFG}s",
    author = "Zhao, Yanpeng  and
      Titov, Ivan",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "EMNLP",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/2023.findings-emnlp.530",
    pages = "7895--7910",
    abstract = "There has been a significant surge of interest in visually grounded grammar induction in recent times. While a variety of models have been developed for the task and have demonstrated impressive performance, they have not been evaluated on text domains that are different from the training domain, so it is unclear if the improvements brought by visual groundings are transferable. Our study aims to fill this gap and assess the degree of transferability. We start by extending VC-PCFG (short for Visually-grounded Compound PCFG [[Zhao and Titov, 2020](https://aclanthology.org/2020.emnlp-main.354/)]) in such a way that it can transfer across text domains. We consider a zero-shot transfer learning setting where a model is trained on the source domain and is directly applied to target domains, without any further training. Our experimental results suggest that: the benefits from using visual groundings transfer to text in a domain similar to the training domain but fail to transfer to remote domains. Further, we conduct data and result analysis; we find that the lexicon overlap between the source domain and the target domain is the most important factor in the transferability of VC-PCFG."
}

@InProceedings{10.1007/978-3-031-20059-5_1,
author="Boecking, Benedikt
and Usuyama, Naoto
and Bannur, Shruthi
and Castro, Daniel C.
and Schwaighofer, Anton
and Hyland, Stephanie
and Wetscherek, Maria
and Naumann, Tristan
and Nori, Aditya
and Alvarez-Valle, Javier
and Poon, Hoifung
and Oktay, Ozan",
editor="Avidan, Shai
and Brostow, Gabriel
and Ciss{\'e}, Moustapha
and Farinella, Giovanni Maria
and Hassner, Tal",
title="Making the Most of Text Semantics to Improve Biomedical Vision--Language Processing",
booktitle="ECCV",
year="2022",
publisher="Springer Nature Switzerland",
address="Cham",
pages="1--21",
abstract="Multi-modal data abounds in biomedicine, such as radiology images and reports. Interpreting this data at scale is essential for improving clinical care and accelerating clinical research. Biomedical text with its complex semantics poses additional challenges in vision--language modelling compared to the general domain, and previous work has used insufficiently adapted models that lack domain-specific language understanding. In this paper, we show that principled textual semantic modelling can substantially improve contrastive learning in self-supervised vision--language processing. We release a language model that achieves state-of-the-art results in radiology natural language inference through its improved vocabulary and novel language pretraining objective leveraging semantics and discourse characteristics in radiology reports. Further, we propose a self-supervised joint vision--language approach with a focus on better text modelling. It establishes new state of the art results on a wide range of publicly available benchmarks, in part by leveraging our new domain-specific language model. We release a new dataset with locally-aligned phrase grounding annotations by radiologists to facilitate the study of complex semantic modelling in biomedical vision--language processing. A broad evaluation, including on this new dataset, shows that our contrastive learning approach, aided by textual-semantic modelling, outperforms prior methods in segmentation tasks, despite only using a global-alignment objective.",
isbn="978-3-031-20059-5"
}

@misc{wu2021chestimagenomedatasetclinical,
      title={Chest ImaGenome Dataset for Clinical Reasoning}, 
      author={Joy T. Wu and Nkechinyere N. Agu and Ismini Lourentzou and Arjun Sharma and Joseph A. Paguio and Jasper S. Yao and Edward C. Dee and William Mitchell and Satyananda Kashyap and Andrea Giovannini and Leo A. Celi and Mehdi Moradi},
      year={2021},
      eprint={2108.00316},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
}

@INPROCEEDINGS{9710016,
  author={Deng, Jiajun and Yang, Zhengyuan and Chen, Tianlang and Zhou, Wengang and Li, Houqiang},
  booktitle={ICCV}, 
  title={TransVG: End-to-End Visual Grounding with Transformers}, 
  year={2021},
  volume={},
  number={},
  pages={1749-1759},
  keywords={Visualization;Computer vision;Codes;Grounding;Manuals;Transformers;Cognition;Vision + language;Vision + other modalities},
  doi={10.1109/ICCV48922.2021.00179}}


@InProceedings{pmlr-v139-radford21a,
  title = 	 {Learning Transferable Visual Models From Natural Language Supervision},
  author =       {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  booktitle = 	 {ICML},
  pages = 	 {8748--8763},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/radford21a/radford21a.pdf},

  abstract = 	 {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.}
}


@inproceedings{muller_anatomy-driven_2023,
	address = {Cham},
	title = {Anatomy-{Driven} {Pathology} {Detection} on {Chest} {X}-rays},
	isbn = {978-3-031-43907-0},
	abstract = {Pathology detection and delineation enables the automatic interpretation of medical scans such as chest X-rays while providing a high level of explainability to support radiologists in making informed decisions. However, annotating pathology bounding boxes is a time-consuming task such that large public datasets for this purpose are scarce. Current approaches thus use weakly supervised object detection to learn the (rough) localization of pathologies from image-level annotations, which is however limited in performance due to the lack of bounding box supervision. We therefore propose anatomy-driven pathology detection (ADPD), which uses easy-to-annotate bounding boxes of anatomical regions as proxies for pathologies. We study two training approaches: supervised training using anatomy-level pathology labels and multiple instance learning (MIL) with image-level pathology labels. Our results show that our anatomy-level training approach outperforms weakly supervised methods and fully supervised detection with limited training samples, and our MIL approach is competitive with both baseline approaches, therefore demonstrating the potential of our approach.},
	booktitle = {MICCAI},
	publisher = {Springer Nature Switzerland},
	author = {Müller, Philip and Meissen, Felix and Brandt, Johannes and Kaissis, Georgios and Rueckert, Daniel},
	editor = {Greenspan, Hayit and Madabhushi, Anant and Mousavi, Parvin and Salcudean, Septimiu and Duncan, James and Syeda-Mahmood, Tanveer and Taylor, Russell},
	year = {2023},
	pages = {57--66},
}


@inproceedings{agu_anaxnet_2021,
	address = {Cham},
	title = {{AnaXNet}: {Anatomy} {Aware} {Multi}-label {Finding} {Classification} in {Chest} {X}-{Ray}},
	isbn = {978-3-030-87240-3},
	abstract = {Radiologists usually observe anatomical regions of chest X-ray images as well as the overall image before making a decision. However, most existing deep learning models only look at the entire X-ray image for classification, failing to utilize important anatomical information. In this paper, we propose a novel multi-label chest X-ray classification model that accurately classifies the image finding and also localizes the findings to their correct anatomical regions. Specifically, our model consists of two modules, the detection module and the anatomical dependency module. The latter utilizes graph convolutional networks, which enable our model to learn not only the label dependency but also the relationship between the anatomical regions in the chest X-ray. We further utilize a method to efficiently create an adjacency matrix for the anatomical regions using the correlation of the label across the different regions. Detailed experiments and analysis of our results show the effectiveness of our method when compared to the current state-of-the-art multi-label chest X-ray image classification methods while also providing accurate location information.},
	booktitle = {MICCAI},
	publisher = {Springer International Publishing},
	author = {Agu, Nkechinyere N. and Wu, Joy T. and Chao, Hanqing and Lourentzou, Ismini and Sharma, Arjun and Moradi, Mehdi and Yan, Pingkun and Hendler, James},
	editor = {de Bruijne, Marleen and Cattin, Philippe C. and Cotin, Stéphane and Padoy, Nicolas and Speidel, Stefanie and Zheng, Yefeng and Essert, Caroline},
	year = {2021},
	pages = {804--813},
}

@inproceedings{DBLP:conf/iclr/LoshchilovH19,
  author       = {Ilya Loshchilov and
                  Frank Hutter},
  title        = {Decoupled Weight Decay Regularization},
  booktitle    = {ICLR},
  year         = {2017},
  timestamp    = {Thu, 25 Jul 2019 14:26:04 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/LoshchilovH19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@INPROCEEDINGS{9710099,
  author={Huang, Shih-Cheng and Shen, Liyue and Lungren, Matthew P. and Yeung, Serena},
  booktitle={ICCV}, 
  title={GLoRIA: A Multimodal Global-Local Representation Learning Framework for Label-efficient Medical Image Recognition}, 
  year={2021},
  volume={},
  number={},
  pages={3922-3931},
  keywords={Representation learning;Deep learning;Training;Image segmentation;Image recognition;Image analysis;Neural networks;Medical;biological;and cell microscopy;Representation learning;Transfer/Low-shot/Semi/Unsupervised Learning;Vision + language},
  doi={10.1109/ICCV48922.2021.00391}}

@article{Schilham2006ADatabase,
    title = {{A computer-aided diagnosis system for detection of lung nodules in chest radiographs with an evaluation on a public database}},
    year = {2006},
    journal = {Medical Image Analysis},
    author = {Schilham, Arnold M. and van Ginneken, Bram and Loog, Marco},
    number = {2},
    month = {4},
    pages = {247--258},
    volume = {10},
    publisher = {Elsevier},
    doi = {10.1016/J.MEDIA.2005.09.003},
    issn = {1361-8415},
    pmid = {16293441},
    keywords = {Chest radiography, Computer-aided diagnosis, Lung cancer, Multi-scale techniques, Pulmonary nodules}
}

@article{Lee2018ADetection,
    title = {{A Deep-Learning System for Fully-Automated Peripherally Inserted Central Catheter (PICC) Tip Detection}},
    year = {2018},
    journal = {Journal of Digital Imaging},
    author = {Lee, Hyunkwang and Mansouri, Mohammad and Tajmir, Shahein and Lev, Michael H. and Do, Synho},
    number = {4},
    month = {8},
    pages = {393--402},
    volume = {31},
    publisher = {Springer New York LLC},
    url = {https://link.springer.com/article/10.1007/s10278-017-0025-z},
    doi = {10.1007/S10278-017-0025-Z/FIGURES/7},
    issn = {1618727X},
    pmid = {28983851},
    keywords = {Chest radiograph, Computer-aided detection, Deep learning, Machine learning, PICC, Radiology workflow}
}

@article{ZhouAPromises,
    title = {{A review of deep learning in medical imaging: Imaging traits, technology trends, case studies with progress highlights, and future promises}},
    author = {Zhou, S Kevin and Greenspan, Hayit and Davatzikos, Christos and Duncan, James S and Van Ginneken, Bram and Madabhushi, Anant and Prince, Jerry L and Rueckert, Daniel and Summers, Ronald M},
    arxivId = {2008.09104v2},
    keywords = {Index Terms-Medical imaging, deep learning, survey}
}

@article{Candemir2019AX-rays,
    title = {{A review on lung boundary detection in chest X-rays}},
    year = {2019},
    journal = {International Journal of Computer Assisted Radiology and Surgery},
    author = {Candemir, Sema and Antani, Sameer},
    number = {4},
    month = {4},
    pages = {563--576},
    volume = {14},
    publisher = {Springer Verlag},
    url = {https://link.springer.com/article/10.1007/s11548-019-01917-1},
    doi = {10.1007/S11548-019-01917-1/FIGURES/6},
    issn = {18616429},
    pmid = {30730032},
    keywords = {Chest X-ray, Lung region detection, Region of interest detection}
}

@article{Chen2020ARepresentations,
    title = {{A Simple Framework for Contrastive Learning of Visual Representations}},
    year = {2020},
    journal = {37th International Conference on Machine Learning, ICML 2020},
    author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
    month = {2},
    pages = {1575--1585},
    volume = {PartF168147-3},
    publisher = {International Machine Learning Society (IMLS)},
    url = {https://arxiv.org/abs/2002.05709v3},
    isbn = {9781713821120},
    arxivId = {2002.05709}
}

@article{Seibold2023AccuratePseudo-Labeling,
    title = {{Accurate Fine-Grained Segmentation of Human Anatomy in Radiographs via Volumetric Pseudo-Labeling}},
    year = {2023},
    author = {Seibold, Constantin and Jaus, Alexander and Fink, Matthias A and Kim, Moon and Rei{\ss}, Simon and Herrmann, Ken and Kleesiek, Jens and Stiefelhagen, Rainer},
    arxivId = {2306.03934v1}
}

@article{Seibold2023AccuratePseudo-Labelingb,
    title = {{Accurate Fine-Grained Segmentation of Human Anatomy in Radiographs via Volumetric Pseudo-Labeling}},
    year = {2023},
    author = {Seibold, Constantin and Jaus, Alexander and Fink, Matthias A and Kim, Moon and Rei{\ss}, Simon and Herrmann, Ken and Kleesiek, Jens and Stiefelhagen, Rainer},
    arxivId = {2306.03934v1}
}

@article{Moutakanni2024AdvancingLearning,
    title = {{Advancing human-centric AI for robust X-ray analysis through holistic self-supervised learning}},
    year = {2024},
    author = {Moutakanni, Théo and Bojanowski, Piotr and Chassagnon, Guillaume and Hudelot, Céline and Joulin, Armand and LeCun, Yann and Muckley, Matthew and Oquab, Maxime and Revel, Marie-Pierre and Vakalopoulou, Maria},
    month = {5},
    url = {https://arxiv.org/abs/2405.01469v1},
    arxivId = {2405.01469}
}

@article{Dou2021AnTransformers,
    title = {{An Empirical Study of Training End-to-End Vision-and-Language Transformers}},
    year = {2021},
    journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
    author = {Dou, Zi Yi and Xu, Yichong and Gan, Zhe and Wang, Jianfeng and Wang, Shuohang and Wang, Lijuan and Zhu, Chenguang and Zhang, Pengchuan and Yuan, Lu and Peng, Nanyun and Liu, Zicheng and Zeng, Michael},
    month = {11},
    pages = {18145--18155},
    volume = {2022-June},
    publisher = {IEEE Computer Society},
    url = {https://arxiv.org/abs/2111.02387v3},
    isbn = {9781665469463},
    doi = {10.1109/CVPR52688.2022.01763},
    issn = {10636919},
    arxivId = {2111.02387},
    keywords = {Deep learning architectures and techniques, Machine learning, Vision + language}
}

@article{Muller2023Anatomy-DrivenX-rays,
    title = {{Anatomy-Driven Pathology Detection on Chest X-rays}},
    year = {2023},
    author = {M{\"{u}}ller, Philip and Meissen, Felix and Brandt, Johannes and Kaissis, Georgios and Rueckert, Daniel},
    month = {9},
    url = {https://arxiv.org/abs/2309.02578v1},
    arxivId = {2309.02578},
    keywords = {Anatomical regions {\textperiodcentered}, Chest X-rays, Pathology detection {\textperiodcentered}}
}

@article{AguAnaXNet:X-ray,
    title = {{AnaXNet: Anatomy Aware Multi-label Finding Classification in Chest X-ray}},
    author = {Agu, Nkechinyere N and Wu, Joy T and Chao, Hanqing and Lourentzou, Ismini and Sharma, Arjun and Moradi, Mehdi and Yan, Pingkun and Hendler, James},
    journal = {2105.09937v1},
    keywords = {Chest X-ray image classification {\textperiodcentered}, Convolutional, Graph, Label, Multi-, Networks {\textperiodcentered}, Representation}
}

@article{Tang2018Attention-GuidedRadiographs,
    title = {{Attention-Guided Curriculum Learning for Weakly Supervised Classification and Localization of Thoracic Diseases on Chest Radiographs}},
    year = {2018},
    journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
    author = {Tang, Yuxing and Wang, Xiaosong and Harrison, Adam P. and Lu, Le and Xiao, Jing and Summers, Ronald M.},
    month = {7},
    pages = {249--258},
    volume = {11046 LNCS},
    publisher = {Springer Verlag},
    url = {https://arxiv.org/abs/1807.07532v1},
    isbn = {9783030009182},
    doi = {10.1007/978-3-030-00919-9{\_}29},
    issn = {16113349},
    arxivId = {1807.07532}
}

@article{Zhong2022AttractingESR,
    title = {{Attracting the next generation of radiologists: a statement by the European Society of Radiology (ESR)}},
    year = {2022},
    journal = {Insights into Imaging},
    author = {Zhong, Jim and Ho, Rosemary and Gourtsoyianni, Sofia and Oleaga, Laura and Catalano, Carlo and Becker, Minerva and Goh, Vicky},
    number = {1},
    month = {12},
    volume = {13},
    publisher = {Springer Science and Business Media Deutschland GmbH},
    url = {https://www.myesr.org/education/training-curricula/},
    doi = {10.1186/S13244-022-01221-8},
    issn = {18694101},
    keywords = {Diagnostic imaging, Education (Medical, Undergraduate), Preceptorship, Radiology (Interventional)}
}

@article{Hurt2020AugmentingMaps,
    title = {{Augmenting Interpretation of Chest Radiographs with Deep Learning Probability Maps}},
    year = {2020},
    journal = {Journal of Thoracic Imaging},
    author = {Hurt, Brian and Yen, Andrew and Kligerman, Seth and Hsiao, Albert},
    number = {5},
    month = {9},
    pages = {285--293},
    volume = {35},
    publisher = {Lippincott Williams and Wilkins},
    url = {https://journals.lww.com/thoracicimaging/fulltext/2020/09000/augmenting_interpretation_of_chest_radiographs.2.aspx},
    doi = {10.1097/RTI.0000000000000505},
    issn = {15360237},
    pmid = {32205817},
    keywords = {artificial intelligence, computer automated diagnoses, deep learning, pneumonia, radiography}
}

@article{Bao2021BEiT:Transformers,
    title = {{BEiT: BERT Pre-Training of Image Transformers}},
    year = {2021},
    journal = {ICLR 2022 - 10th International Conference on Learning Representations},
    author = {Bao, Hangbo and Dong, Li and Piao, Songhao and Wei, Furu},
    month = {6},
    publisher = {International Conference on Learning Representations, ICLR},
    url = {https://arxiv.org/abs/2106.08254v2},
    arxivId = {2106.08254}
}

@article{Devlin2018BERT:Understanding,
    title = {{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}},
    year = {2018},
    journal = {NAACL HLT 2019 - 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies - Proceedings of the Conference},
    author = {Devlin, Jacob and Chang, Ming Wei and Lee, Kenton and Toutanova, Kristina},
    month = {10},
    pages = {4171--4186},
    volume = {1},
    publisher = {Association for Computational Linguistics (ACL)},
    url = {https://arxiv.org/abs/1810.04805v2},
    isbn = {9781950737130},
    arxivId = {1810.04805}
}

@article{Zhang2023BiomedCLIP:Pairs,
    title = {{BiomedCLIP: a multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs}},
    year = {2023},
    author = {Zhang, Sheng and Xu, Yanbo and Usuyama, Naoto and Xu, Hanwen and Bagga, Jaspreet and Tinn, Robert and Preston, Sam and Rao, Rajesh and Wei, Mu and Valluri, Naveen and Wong, Cliff and Tupini, Andrea and Wang, Yu and Mazzola, Matt and Shukla, Swadheen and Liden, Lars and Gao, Jianfeng and Lungren, Matthew P. and Naumann, Tristan and Wang, Sheng and Poon, Hoifung},
    month = {3},
    url = {https://arxiv.org/abs/2303.00915v2},
    arxivId = {2303.00915}
}

@article{Li2023BLIP-2:Models,
    title = {{BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models}},
    year = {2023},
    journal = {Proceedings of Machine Learning Research},
    author = {Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
    month = {1},
    pages = {20351--20383},
    volume = {202},
    publisher = {ML Research Press},
    url = {https://arxiv.org/abs/2301.12597v3},
    issn = {26403498},
    arxivId = {2301.12597}
}

@article{Grill2020BootstrapLearning,
    title = {{Bootstrap your own latent: A new approach to self-supervised Learning}},
    year = {2020},
    journal = {Advances in Neural Information Processing Systems},
    author = {Grill, Jean Bastien and Strub, Florian and Altch{\'{e}}, Florent and Tallec, Corentin and Richemond, Pierre H. and Buchatskaya, Elena and Doersch, Carl and Pires, Bernardo Avila and Guo, Zhaohan Daniel and Azar, Mohammad Gheshlaghi and Piot, Bilal and Kavukcuoglu, Koray and Munos, Rémi and Valko, Michal},
    month = {6},
    volume = {2020-December},
    publisher = {Neural information processing systems foundation},
    url = {https://arxiv.org/abs/2006.07733v3},
    isbn = {2006.07733v3},
    issn = {10495258},
    arxivId = {2006.07733}
}

@article{Bernstein2023CanRadiography,
    title = {{Can incorrect artificial intelligence (AI) results impact radiologists, and if so, what can we do about it? A multi-reader pilot study of lung cancer detection with chest radiography}},
    year = {2023},
    journal = {European radiology},
    author = {Bernstein, Michael H. and Atalay, Michael K. and Dibble, Elizabeth H. and Maxwell, Aaron W.P. and Karam, Adib R. and Agarwal, Saurabh and Ward, Robert C. and Healey, Terrance T. and Baird, Grayson L.},
    number = {11},
    month = {11},
    pages = {8263--8269},
    volume = {33},
    publisher = {Eur Radiol},
    url = {https://pubmed.ncbi.nlm.nih.gov/37266657/},
    doi = {10.1007/S00330-023-09747-1},
    issn = {1432-1084},
    pmid = {37266657},
    keywords = {Artificial Intelligence*, Grayson L Baird, Humans, Lung Neoplasms* / diagnostic imaging, MEDLINE, Michael H Bernstein, Michael K Atalay, NCBI, NIH, NLM, National Center for Biotechnology Information, National Institutes of Health, National Library of Medicine, PMC10235827, Pilot Projects, PubMed Abstract, Radiography, Radiologists, Retrospective Studies, doi:10.1007/s00330-023-09747-1, pmid:37266657}
}

@article{Saab2024CapabilitiesMedicine,
    title = {{Capabilities of Gemini Models in Medicine}},
    year = {2024},
    journal = {‡ Technical Lead, † Senior Lead},
    author = {Saab, Khaled and Tu, Tao and Weng, Wei-Hung and Tanno, Ryutaro and Stutz, David and Wulczyn, Ellery and Zhang, Fan and Strother, Tim and Park, Chunjong and Vedadi, Elahe and Chaves, Juanma Zambrano and Hu, Szu-Yeu and Schaekermann, Mike and Kamath, Aishwarya and Cheng, Yong and Barrett, David G. T. and Cheung, Cathy and Mustafa, Basil and Palepu, Anil and McDuff, Daniel and Hou, Le and Golany, Tomer and Liu, Luyang and Alayrac, Jean-baptiste and Houlsby, Neil and Tomasev, Nenad and Freyberg, Jan and Lau, Charles and Kemp, Jonas and Lai, Jeremy and Azizi, Shekoofeh and Kanada, Kimberly and Man, SiWai and Kulkarni, Kavita and Sun, Ruoxi and Shakeri, Siamak and He, Luheng and Caine, Ben and Webson, Albert and Latysheva, Natasha and Johnson, Melvin and Mansfield, Philip and Lu, Jian and Rivlin, Ehud and Anderson, Jesper and Green, Bradley and Wong, Renee and Krause, Jonathan and Shlens, Jonathon and Dominowska, Ewa and Eslami, S. M. Ali and Chou, Katherine and Cui, Claire and Vinyals, Oriol and Kavukcuoglu, Koray and Manyika, James and Dean, Jeff and Hassabis, Demis and Matias, Yossi and Webster, Dale and Barral, Joelle and Corrado, Greg and Semturs, Christopher and Mahdavi, S. Sara and Gottweis, Juraj and Karthikesalingam, Alan and Natarajan, Vivek},
    month = {4},
    url = {https://arxiv.org/abs/2404.18416v2},
    arxivId = {2404.18416}
}

@article{Tay2021ChallengesCOVID-19,
    title = {{Challenges and optimization strategies in medical imaging service delivery during COVID-19}},
    year = {2021},
    journal = {World Journal of Radiology},
    author = {Tay, Yi Xiang and Kothan, Suchart and Kada, Sundaran and Cai, Sihui and Lai, Christopher Wai Keung},
    number = {5},
    month = {5},
    pages = {102},
    volume = {13},
    publisher = {Baishideng Publishing Group Inc},
    url = {/pmc/articles/PMC8188837/ /pmc/articles/PMC8188837/?report=abstract https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8188837/},
    doi = {10.4329/WJR.V13.I5.102},
    issn = {1949-8470},
    pmid = {34141091},
    keywords = {COVID-19, Medical imaging service, Optimization strategies, Pandemic, Radiography, Radiology department}
}

@article{WuChestReasoning,
    title = {{Chest ImaGenome Dataset for Clinical Reasoning}},
    author = {Wu, Joy T and Agu, Nkechinyere N and Lourentzou, Ismini and Sharma, Arjun and Paguio, Joseph A and Yao, Jasper S and Dee, Edward C and Mitchell, William and Kashyap, Satyananda and Giovannini, Andrea and Celi, Leo A and Moradi, Mehdi},
    journal = {2108.00316v1}
}

@article{Wu2021ChestReasoning,
    title = {{Chest ImaGenome Dataset for Clinical Reasoning}},
    year = {2021},
    author = {Wu, Joy T. and Agu, Nkechinyere N. and Lourentzou, Ismini and Sharma, Arjun and Paguio, Joseph A. and Yao, Jasper S. and Dee, Edward C. and Mitchell, William and Kashyap, Satyananda and Giovannini, Andrea and Celi, Leo A. and Moradi, Mehdi},
    month = {7},
    url = {https://arxiv.org/abs/2108.00316v1},
    journal = {2108.00316}
}

@article{Wu2021ChestReasoningb,
    title = {{Chest ImaGenome Dataset for Clinical Reasoning}},
    year = {2021},
    author = {Wu, Joy T. and Agu, Nkechinyere N. and Lourentzou, Ismini and Sharma, Arjun and Paguio, Joseph A. and Yao, Jasper S. and Dee, Edward C. and Mitchell, William and Kashyap, Satyananda and Giovannini, Andrea and Celi, Leo A. and Moradi, Mehdi},
    month = {7},
    url = {https://arxiv.org/abs/2108.00316v1},
    journal = {2108.00316}
}

@article{Kufel2023ChestIntelligence,
    title = {{Chest X-ray Foreign Objects Detection Using Artificial Intelligence}},
    year = {2023},
    journal = {Journal of Clinical Medicine},
    author = {Kufel, Jakub and Bargie{\l}-{\L}{\c{a}}czek, Katarzyna and Ko{\'{z}}lik, Maciej and Czogalik, Łukasz and Dudek, Piotr and Magiera, Mikołaj and Bartnikowska, Wiktoria and Lis, Anna and Paszkiewicz, Iga and Kocot, Szymon and Cebula, Maciej and Gruszczy{\'{n}}ska, Katarzyna and Nawrat, Zbigniew},
    number = {18},
    month = {9},
    pages = {12},
    volume = {12},
    publisher = {Multidisciplinary Digital Publishing Institute  (MDPI)},
    url = {/pmc/articles/PMC10531506/ /pmc/articles/PMC10531506/?report=abstract https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10531506/},
    doi = {10.3390/JCM12185841},
    issn = {20770383},
    pmid = {37762783},
    keywords = {artifacts, artificial intelligence, chest X-ray, convolutional neural network, foreign body}
}

@article{Kufel2023ChestIntelligenceb,
    title = {{Chest X-ray Foreign Objects Detection Using Artificial Intelligence}},
    year = {2023},
    journal = {Journal of Clinical Medicine},
    author = {Kufel, Jakub and Bargie{\l}-{\L}{\c{a}}czek, Katarzyna and Ko{\'{z}}lik, Maciej and Czogalik, Łukasz and Dudek, Piotr and Magiera, Mikołaj and Bartnikowska, Wiktoria and Lis, Anna and Paszkiewicz, Iga and Kocot, Szymon and Cebula, Maciej and Gruszczy{\'{n}}ska, Katarzyna and Nawrat, Zbigniew},
    number = {18},
    month = {9},
    pages = {12},
    volume = {12},
    publisher = {Multidisciplinary Digital Publishing Institute  (MDPI)},
    url = {/pmc/articles/PMC10531506/ /pmc/articles/PMC10531506/?report=abstract https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10531506/},
    doi = {10.3390/JCM12185841},
    issn = {20770383},
    pmid = {37762783},
    keywords = {artifacts, artificial intelligence, chest X-ray, convolutional neural network, foreign body}
}

@article{Wang2017ChestX-ray8:Diseases,
    title = {{ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases}},
    year = {2017},
    journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
    author = {Wang, Xiaosong and Peng, Yifan and Lu, Le and Lu, Zhiyong and Bagheri, Mohammadhadi and Summers, Ronald M.},
    month = {5},
    pages = {3462--3471},
    volume = {2017-January},
    publisher = {Institute of Electrical and Electronics Engineers Inc.},
    url = {http://arxiv.org/abs/1705.02315 http://dx.doi.org/10.1109/CVPR.2017.369},
    doi = {10.1109/CVPR.2017.369},
    arxivId = {1705.02315v5}
}

@article{MullerChEX:X-rays,
    title = {{ChEX: Interactive Localization and Region Description in Chest X-rays}},
    author = {M{\"{u}}ller, Philip and Kaissis, Georgios and Rueckert, Daniel},
    url = {https://github.com/philip-mueller/chex.},
    arxivId = {2404.15770v2},
    keywords = {Generation {\textperiodcentered}, Language, Modeling, Radiology, Report, Vision-}
}

@article{Chen2024CheXagent:Interpretation,
    title = {{CheXagent: Towards a Foundation Model for Chest X-Ray Interpretation}},
    year = {2024},
    author = {Chen, Zhihong and Varma, Maya and Delbrouck, Jean-Benoit and Paschali, Magdalini and Blankemeier, Louis and Van Veen, Dave and Valanarasu, Jeya Maria Jose and Youssef, Alaa and Cohen, Joseph Paul and Reis, Eduardo Pontes and Tsai, Emily B. and Johnston, Andrew and Olsen, Cameron and Abraham, Tanishq Mathew and Gatidis, Sergios and Chaudhari, Akshay S. and Langlotz, Curtis},
    month = {1},
    url = {https://arxiv.org/abs/2401.12208v1},
    arxivId = {2401.12208}
}

@article{Gaggion2023CheXmask:Images,
    title = {{CheXmask: a large-scale dataset of anatomical segmentation masks for multi-center chest x-ray images}},
    year = {2023},
    journal = {Scientific Data},
    author = {Gaggion, Nicolás and Mosquera, Candelaria and Mansilla, Lucas and Saidman, Julia Mariel and Aineseder, Martina and Milone, Diego H. and Ferrante, Enzo},
    number = {1},
    month = {7},
    volume = {11},
    publisher = {Springer Science and Business Media LLC},
    url = {http://arxiv.org/abs/2307.03293 http://dx.doi.org/10.1038/s41597-024-03358-1},
    doi = {10.1038/s41597-024-03358-1},
    arxivId = {2307.03293v4}
}

@article{Rajpurkar2017CheXNet:Learning,
    title = {{CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning}},
    year = {2017},
    author = {Rajpurkar, Pranav and Irvin, Jeremy and Zhu, Kaylie and Yang, Brandon and Mehta, Hershel and Duan, Tony and Ding, Daisy and Bagul, Aarti and Ball, Robyn L and Langlotz, Curtis and Shpanskaya, Katie and Lungren, Matthew P and Ng, Andrew Y},
    month = {11},
    url = {https://arxiv.org/abs/1711.05225v3},
    arxivId = {1711.05225}
}

@article{ChambonCheXpertFormats,
    title = {{CheXpert Plus: Augmenting a Large Chest X-ray Dataset with Text Radiology Reports, Patient Demographics and Additional Image Formats}},
    author = {Chambon, Pierre and Delbrouck, Jean-Benoit and Sounack, Thomas and Huang, Shih-Cheng and Chen, Zhihong and Varma, Maya and Steven, ♠ and Truong, Q H and Chu, ♡ and Chuong, The and Curtis, ♡ and Langlotz, P and Stanford, ♠ ♠ and Vinbrain, Aimi ♡},
    url = {https://github.com/Stanford-AIMI/},
    arxivId = {2405.19538v2}
}

@article{Chambon2024CheXpertFormats,
    title = {{CheXpert Plus: Augmenting a Large Chest X-ray Dataset with Text Radiology Reports, Patient Demographics and Additional Image Formats}},
    year = {2024},
    author = {Chambon, Pierre and Delbrouck, Jean-Benoit and Sounack, Thomas and Huang, Shih-Cheng and Chen, Zhihong and Varma, Maya and Steven, ♠ and Truong, Q H and Chu, ♡ and Chuong, The and Curtis, ♡ and Langlotz, P and Stanford, ♠ ♠ and Vinbrain, Aimi ♡},
    month = {5},
    url = {https://arxiv.org/abs/2405.19538v2},
    arxivId = {2405.19538}
}

@misc{CheXpert:Comparison,
    title = {{CheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison}},
    url = {https://stanfordmlgroup.github.io/competitions/chexpert/}
}

@article{Irvin2019CheXpert:Comparison,
    title = {{CheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison}},
    year = {2019},
    journal = {33rd AAAI Conference on Artificial Intelligence, AAAI 2019, 31st Innovative Applications of Artificial Intelligence Conference, IAAI 2019 and the 9th AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019},
    author = {Irvin, Jeremy and Rajpurkar, Pranav and Ko, Michael and Yu, Yifan and Ciurea-Ilcus, Silviana and Chute, Chris and Marklund, Henrik and Haghgoo, Behzad and Ball, Robyn and Shpanskaya, Katie and Seekins, Jayne and Mong, David A. and Halabi, Safwan S. and Sandberg, Jesse K. and Jones, Ricky and Larson, David B. and Langlotz, Curtis P. and Patel, Bhavik N. and Lungren, Matthew P. and Ng, Andrew Y.},
    month = {1},
    pages = {590--597},
    publisher = {AAAI Press},
    url = {https://arxiv.org/abs/1901.07031v1},
    isbn = {9781577358091},
    doi = {10.1609/aaai.v33i01.3301590},
    issn = {2159-5399},
    arxivId = {1901.07031}
}

@misc{ClinicalRANZCR,
    title = {{Clinical Radiology Training Program Handbook | RANZCR}},
    url = {https://www.ranzcr.com/college/document-library/clinical-radiology-training-program-handbook}
}

@article{Huang2020ClinicalBERT:Readmission,
    title = {{ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission}},
    year = {2020},
    author = {Huang, Kexin and Altosaar, Jaan and Ranganath, Rajesh},
    url = {https://github.com/kexinhuang12345/clinicalBERT},
    arxivId = {1904.05342v3}
}

@article{Huang2019ClinicalBERT:Readmission,
    title = {{ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission}},
    year = {2019},
    author = {Huang, Kexin and Altosaar, Jaan and Ranganath, Rajesh},
    month = {4},
    url = {https://arxiv.org/abs/1904.05342v3},
    arxivId = {1904.05342}
}

@article{Yu2022CoCa:Models,
    title = {{CoCa: Contrastive Captioners are Image-Text Foundation Models}},
    year = {2022},
    author = {Yu, Jiahui and Wang, Zirui and Vasudevan, Vijay and Yeung, Legg and Seyedhosseini, Mojtaba and Wu, Yonghui and Research, Google},
    month = {5},
    url = {https://arxiv.org/abs/2205.01917v2},
    arxivId = {2205.01917}
}

@article{Zhang2020ContrastiveText,
    title = {{Contrastive Learning of Medical Visual Representations from Paired Images and Text}},
    year = {2020},
    journal = {Proceedings of Machine Learning Research},
    author = {Zhang, Yuhao and Jiang, Hang and Miura, Yasuhide and Manning, Christopher D and Langlotz, Curtis P and Zhang, Y and Jiang, H and Miura, Y and Manning, C D and Langlotz, C P},
    month = {10},
    pages = {2--25},
    volume = {182},
    publisher = {ML Research Press},
    url = {https://arxiv.org/abs/2010.00747v2},
    issn = {26403498},
    arxivId = {2010.00747}
}

@article{Srinivasan2022CurriculumAlignment,
    title = {{Curriculum Learning for Data-Efficient Vision-Language Alignment}},
    year = {2022},
    journal = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
    author = {Srinivasan, Tejas and Ren, Xiang and Thomason, Jesse},
    month = {7},
    pages = {5619--5624},
    volume = {2023-June},
    publisher = {IEEE Computer Society},
    url = {https://arxiv.org/abs/2207.14525v1},
    isbn = {9798350302493},
    doi = {10.1109/CVPRW59228.2023.00595},
    issn = {21607516},
    arxivId = {2207.14525}
}

@article{LeeDataMining,
    title = {{Data and text mining BioBERT: a pre-trained biomedical language representation model for biomedical text mining}},
    author = {Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo},
    url = {https://academic.oup.com/bioinformatics/advance-article-abstract/doi/10.1093/bioinformatics/btz682/5566506},
    doi = {10.1093/bioinformatics/btz682}
}

@article{Loshchilov2017DecoupledRegularization,
    title = {{Decoupled Weight Decay Regularization}},
    year = {2017},
    journal = {7th International Conference on Learning Representations, ICLR 2019},
    author = {Loshchilov, Ilya and Hutter, Frank},
    month = {11},
    publisher = {International Conference on Learning Representations, ICLR},
    url = {https://arxiv.org/abs/1711.05101v3},
    arxivId = {1711.05101}
}


@article{Lakhani2017DeepNetworks,
    title = {{Deep Learning at Chest Radiography: Automated Classification of Pulmonary Tuberculosis by Using Convolutional Neural Networks}},
    year = {2017},
    journal = {Radiology},
    author = {Lakhani, Paras and Sundaram, Baskaran},
    number = {2},
    month = {8},
    pages = {574--582},
    volume = {284},
    publisher = {Radiology},
    url = {https://pubmed.ncbi.nlm.nih.gov/28436741/},
    doi = {10.1148/RADIOL.2017162326},
    issn = {1527-1315},
    pmid = {28436741},
    keywords = {Algorithms, Baskaran Sundaram, Computer*, Humans, MEDLINE, NCBI, NIH, NLM, National Center for Biotechnology Information, National Institutes of Health, National Library of Medicine, Neural Networks, Paras Lakhani, PubMed Abstract, Pulmonary / classification*, Pulmonary / diagnostic imaging*, ROC Curve, Radiography, Retrospective Studies, Sensitivity and Specificity, Thoracic / methods*, Tuberculosis, doi:10.1148/radiol.2017162326, pmid:28436741}
}

@article{Tolkachev2021DeepRadiologists,
    title = {{Deep Learning for Diagnosis and Segmentation of Pneumothorax: The Results on the Kaggle Competition and Validation against Radiologists}},
    year = {2021},
    journal = {IEEE Journal of Biomedical and Health Informatics},
    author = {Tolkachev, Alexey and Sirazitdinov, Ilyas and Kholiavchenko, Maksym and Mustafaev, Tamerlan and Ibragimov, Bulat},
    number = {5},
    month = {5},
    pages = {1660--1672},
    volume = {25},
    publisher = {Institute of Electrical and Electronics Engineers Inc.},
    doi = {10.1109/JBHI.2020.3023476},
    issn = {21682208},
    pmid = {32956067},
    keywords = {Kaggle competition, Pneumothorax, chest X-rays, convolutional neural network, deep learning, radiologist validation}
}

@article{Tolkachev2021DeepRadiologistsb,
    title = {{Deep Learning for Diagnosis and Segmentation of Pneumothorax: The Results on the Kaggle Competition and Validation Against Radiologists}},
    year = {2021},
    journal = {IEEE journal of biomedical and health informatics},
    author = {Tolkachev, Alexey and Sirazitdinov, Ilyas and Kholiavchenko, Maksym and Mustafaev, Tamerlan and Ibragimov, Bulat},
    number = {5},
    month = {5},
    pages = {1660--1672},
    volume = {25},
    publisher = {IEEE J Biomed Health Inform},
    url = {https://pubmed.ncbi.nlm.nih.gov/32956067/},
    doi = {10.1109/JBHI.2020.3023476},
    issn = {2168-2208},
    pmid = {32956067},
    keywords = {Alexey Tolkachev, Bulat Ibragimov, Computer, Computer-Assisted, Deep Learning*, Diagnosis, Humans, Ilyas Sirazitdinov, Image Processing, MEDLINE, NCBI, NIH, NLM, National Center for Biotechnology Information, National Institutes of Health, National Library of Medicine, Neural Networks, Non-U.S. Gov't, Pneumothorax* / diagnostic imaging, PubMed Abstract, Radiologists, Research Support, doi:10.1109/JBHI.2020.3023476, pmid:32956067}
}

@article{Sullivan2020DeepRadiographs,
    title = {{Deep learning methods for segmentation of lines in pediatric chest radiographs}},
    year = {2020},
    journal = {https://doi.org/10.1117/12.2550686},
    author = {Sullivan, Ryan P. and Holste, Greg and Burkow, Jonathan and Alessio, Adam},
    month = {3},
    pages = {577--583},
    volume = {11314},
    publisher = {SPIE},
    url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11314/113142I/Deep-learning-methods-for-segmentation-of-lines-in-pediatric-chest/10.1117/12.2550686.full https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11314/113142I/Deep-learning-methods-for-segmentation-of-lines-in-pediatric-chest/10.1117/12.2550686.short},
    isbn = {9781510633957},
    doi = {10.1117/12.2550686},
    issn = {16057422},
    keywords = {Chest imaging, Computer programming, Data modeling, Image classification, Image segmentation, Performance modeling, Statistical modeling}
}

@article{He2015DeepRecognition,
    title = {{Deep Residual Learning for Image Recognition}},
    year = {2015},
    journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
    author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
    month = {12},
    pages = {770--778},
    volume = {2016-December},
    publisher = {IEEE Computer Society},
    url = {https://arxiv.org/abs/1512.03385v1},
    isbn = {9781467388504},
    doi = {10.1109/CVPR.2016.90},
    issn = {10636919},
    arxivId = {1512.03385}
}

@article{Huang2016DenselyNetworks,
    title = {{Densely Connected Convolutional Networks}},
    year = {2016},
    journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
    author = {Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q.},
    month = {8},
    pages = {2261--2269},
    volume = {2017-January},
    publisher = {Institute of Electrical and Electronics Engineers Inc.},
    url = {https://arxiv.org/abs/1608.06993v5},
    isbn = {9781538604571},
    doi = {10.1109/CVPR.2017.243},
    arxivId = {1608.06993}
}

@article{XuELIXR:Encoders,
    title = {{ELIXR: Towards a general purpose X-ray articial intelligence system through alignment of large language models and radiology vision encoders}},
    author = {Xu, Shawn and Yang, Lin and Kelly, Christopher and Sieniek, Marcin and Kohlberger, Timo and Ma, Main and Weng, Wei-Hung and Kiraly, Atilla P and Kazemzadeh, Sahar and Melamed, Zakkai and Park, Jungyeon and Strachan, Patricia and Liu, Yun and Lau, Chuck and Singh, Preeti and Chen, Christina and Etemadi, Mozziyar and Kalidindi, Sreenivasa Raju and Matias, Yossi and Chou, Katherine and Corrado, Greg S and Shey, Shravya and Tse, Daniel and Prabhakara, Shruthi and Golden, Daniel and Pilgrim, Rory and Eswaran, Krish and Sellergren, Andrew}
}

@article{Xu2023ELIXR:Encoders,
    title = {{ELIXR: Towards a general purpose X-ray artificial intelligence system through alignment of large language models and radiology vision encoders}},
    year = {2023},
    author = {Xu, Shawn and Yang, Lin and Kelly, Christopher and Sieniek, Marcin and Kohlberger, Timo and Ma, Martin and Weng, Wei-Hung and Kiraly, Atilla and Kazemzadeh, Sahar and Melamed, Zakkai and Park, Jungyeon and Strachan, Patricia and Liu, Yun and Lau, Chuck and Singh, Preeti and Chen, Christina and Etemadi, Mozziyar and Kalidindi, Sreenivasa Raju and Matias, Yossi and Chou, Katherine and Corrado, Greg S. and Shetty, Shravya and Tse, Daniel and Prabhakara, Shruthi and Golden, Daniel and Pilgrim, Rory and Eswaran, Krish and Sellergren, Andrew},
    month = {8},
    url = {https://arxiv.org/abs/2308.01317v2},
    arxivId = {2308.01317}
}

@article{Caron2021EmergingTransformers,
    title = {{Emerging Properties in Self-Supervised Vision Transformers}},
    year = {2021},
    journal = {Proceedings of the IEEE International Conference on Computer Vision},
    author = {Caron, Mathilde and Touvron, Hugo and Misra, Ishan and Jegou, Hervé and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
    month = {4},
    pages = {9630--9640},
    publisher = {Institute of Electrical and Electronics Engineers Inc.},
    url = {https://arxiv.org/abs/2104.14294v2},
    isbn = {9781665428125},
    doi = {10.1109/ICCV48922.2021.00951},
    issn = {15505499},
    arxivId = {2104.14294}
}

@article{Caron2021EmergingTransformersb,
    title = {{Emerging Properties in Self-Supervised Vision Transformers}},
    year = {2021},
    journal = {Proceedings of the IEEE International Conference on Computer Vision},
    author = {Caron, Mathilde and Touvron, Hugo and Misra, Ishan and Jegou, Hervé and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
    month = {4},
    pages = {9630--9640},
    publisher = {Institute of Electrical and Electronics Engineers Inc.},
    url = {https://arxiv.org/abs/2104.14294v2},
    isbn = {9781665428125},
    doi = {10.1109/ICCV48922.2021.00951},
    issn = {15505499},
    arxivId = {2104.14294}
}

@article{Caron2021EmergingTransformersc,
    title = {{Emerging Properties in Self-Supervised Vision Transformers}},
    year = {2021},
    journal = {Proceedings of the IEEE International Conference on Computer Vision},
    author = {Caron, Mathilde and Touvron, Hugo and Misra, Ishan and Jegou, Hervé and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
    month = {4},
    pages = {9630--9640},
    publisher = {Institute of Electrical and Electronics Engineers Inc.},
    url = {https://arxiv.org/abs/2104.14294v2},
    isbn = {9781665428125},
    doi = {10.1109/ICCV48922.2021.00951},
    issn = {15505499},
    arxivId = {2104.14294}
}

@article{Carion2020End-to-EndTransformers,
    title = {{End-to-End Object Detection with Transformers}},
    year = {2020},
    journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
    author = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
    month = {5},
    pages = {213--229},
    volume = {12346 LNCS},
    publisher = {Springer Science and Business Media Deutschland GmbH},
    url = {https://arxiv.org/abs/2005.12872v3},
    isbn = {9783030584511},
    doi = {10.1007/978-3-030-58452-8{\_}13},
    issn = {16113349},
    arxivId = {2005.12872}
}

@article{Frid-Adar2019EndotrachealData,
    title = {{Endotracheal Tube Detection and Segmentation in Chest Radiographs Using Synthetic Data}},
    year = {2019},
    journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
    author = {Frid-Adar, Maayan and Amer, Rula and Greenspan, Hayit},
    pages = {784--792},
    volume = {11769 LNCS},
    publisher = {Springer Science and Business Media Deutschland GmbH},
    url = {https://link.springer.com/chapter/10.1007/978-3-030-32226-7_87},
    isbn = {9783030322250},
    doi = {10.1007/978-3-030-32226-7{\_}87/TABLES/1},
    issn = {16113349},
    arxivId = {1908.07170},
    keywords = {CNN, Chest radiographs, Classification, Deep learning, ET tube, Segmentation}
}

@article{Tiu2022Expert-levelLearning,
    title = {{Expert-level detection of pathologies from unannotated chest X-ray images via self-supervised learning}},
    year = {2022},
    journal = {Nature Biomedical Engineering 2022 6:12},
    author = {Tiu, Ekin and Talius, Ellie and Patel, Pujan and Langlotz, Curtis P. and Ng, Andrew Y. and Rajpurkar, Pranav},
    number = {12},
    month = {9},
    pages = {1399--1406},
    volume = {6},
    publisher = {Nature Publishing Group},
    url = {https://www.nature.com/articles/s41551-022-00936-9},
    doi = {10.1038/s41551-022-00936-9},
    issn = {2157-846X},
    pmid = {36109605},
    keywords = {Health care, Medical imaging}
}

@article{Ren2015FasterNetworks,
    title = {{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}},
    year = {2015},
    journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
    author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
    number = {6},
    month = {6},
    pages = {1137--1149},
    volume = {39},
    publisher = {IEEE Computer Society},
    url = {https://arxiv.org/abs/1506.01497v3},
    doi = {10.1109/TPAMI.2016.2577031},
    issn = {01628828},
    pmid = {27295650},
    arxivId = {1506.01497},
    keywords = {Object detection, convolutional neural network, region proposal}
}

@article{ZhangFerret-v2:Models,
    title = {{Ferret-v2: An Improved Baseline for Referring and Grounding with Large Language Models}},
    author = {Zhang, Haotian and You, Haoxuan and Dufter, Philipp and Zhang, Bowen and Chen, Chen and Chen, Hong-You and Fu, Tsu-Jui and Wang, William Yang and Chang, Shih-Fu and Gan, Zhe and Yang, Yinfei},
    arxivId = {2404.07973v1}
}

@article{ZhangFerret-v2:Modelsb,
    title = {{Ferret-v2: An Improved Baseline for Referring and Grounding with Large Language Models}},
    author = {Zhang, Haotian and You, Haoxuan and Dufter, Philipp and Zhang, Bowen and Chen, Chen and Chen, Hong-You and Fu, Tsu-Jui and Wang, William Yang and Chang, Shih-Fu and Gan, Zhe and Yang, Yinfei},
    arxivId = {2404.07973v1}
}

@article{You2023Ferret:Granularity,
    title = {{Ferret: Refer and Ground Anything Anywhere at Any Granularity}},
    year = {2023},
    author = {You, Haoxuan and Zhang, Haotian and Gan, Zhe and Du, Xianzhi and Zhang, Bowen and Wang, Zirui and Cao, Liangliang and Chang, Shih-Fu and Yang, Yinfei},
    month = {10},
    url = {https://arxiv.org/abs/2310.07704v1},
    arxivId = {2310.07704}
}

@article{You2023Ferret:Granularityb,
    title = {{Ferret: Refer and Ground Anything Anywhere at Any Granularity}},
    year = {2023},
    author = {You, Haoxuan and Zhang, Haotian and Gan, Zhe and Du, Xianzhi and Zhang, Bowen and Wang, Zirui and Cao, Liangliang and Chang, Shih-Fu and Yang, Yinfei},
    month = {10},
    url = {https://arxiv.org/abs/2310.07704v1},
    arxivId = {2310.07704}
}

@article{Alayrac2022Flamingo:Learning,
    title = {{Flamingo: a Visual Language Model for Few-Shot Learning}},
    year = {2022},
    journal = {Advances in Neural Information Processing Systems},
    author = {Alayrac, Jean Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katie and Reynolds, Malcolm and Ring, Roman and Rutherford, Eliza and Han, Serkan Cabi Tengda and Gong, Zhitao and Samangooei, Sina and Monteiro, Marianne and Menick, Jacob and Borgeaud, Sebastian and Brock, Andrew and Nematzadeh, Aida and Sharifzadeh, Sahand and Binkowski, Mikolaj and Barreira, Ricardo and Vinyals, Oriol and Zisserman, Andrew and Simonyan, Karen},
    month = {4},
    volume = {35},
    publisher = {Neural information processing systems foundation},
    url = {https://arxiv.org/abs/2204.14198v2},
    isbn = {9781713871088},
    issn = {10495258},
    arxivId = {2204.14198}
}

@article{Alayrac2022Flamingo:Learningb,
    title = {{Flamingo: a Visual Language Model for Few-Shot Learning}},
    year = {2022},
    journal = {Advances in Neural Information Processing Systems},
    author = {Alayrac, Jean Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katie and Reynolds, Malcolm and Ring, Roman and Rutherford, Eliza and Han, Serkan Cabi Tengda and Gong, Zhitao and Samangooei, Sina and Monteiro, Marianne and Menick, Jacob and Borgeaud, Sebastian and Brock, Andrew and Nematzadeh, Aida and Sharifzadeh, Sahand and Binkowski, Mikolaj and Barreira, Ricardo and Vinyals, Oriol and Zisserman, Andrew and Simonyan, Karen},
    month = {4},
    volume = {35},
    publisher = {Neural information processing systems foundation},
    url = {https://arxiv.org/abs/2204.14198v2},
    isbn = {9781713871088},
    issn = {10495258},
    arxivId = {2204.14198},
    keywords = {Equal contributions, ordered alphabetically, † Equal contributions, ‡ Equal senior contributions DeepMind}
}

@article{Hansell2008FleischnerImaging,
    title = {{Fleischner Society: Glossary of terms for thoracic imaging}},
    year = {2008},
    journal = {Radiology},
    author = {Hansell, David M. and Bankier, Alexander A. and MacMahon, Heber and McLoud, Theresa C. and M{\"{u}}ller, Nestor L. and Remy, Jacques},
    number = {3},
    month = {3},
    pages = {697--722},
    volume = {246},
    doi = {10.1148/RADIOL.2462070712},
    issn = {00338419},
    pmid = {18195376}
}

@article{Plummer2015Flickr30kModels,
    title = {{Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models}},
    year = {2015},
    journal = {International Journal of Computer Vision},
    author = {Plummer, Bryan A and Wang, Liwei and Chris, · and Cervantes, M and Caicedo, Juan C and Hockenmaier, Julia and Lazebnik, Svetlana and Plummer, B A and Wang, L and Caicedo, J C and Hockenmaier, J},
    number = {1},
    month = {5},
    pages = {74--93},
    volume = {123},
    publisher = {Springer New York LLC},
    url = {https://arxiv.org/abs/1505.04870v4},
    doi = {10.1007/s11263-016-0965-7},
    issn = {15731405},
    arxivId = {1505.04870},
    keywords = {Computer vision, Crowdsourcing, Datasets, Language, Phrase, Region, Region phrase correspondence, Vision {\textperiodcentered}}
}

@article{Moor2023FoundationIntelligence,
    title = {{Foundation models for generalist medical artificial intelligence}},
    year = {2023},
    journal = {Nature 2023 616:7956},
    author = {Moor, Michael and Banerjee, Oishi and Abad, Zahra Shakeri Hossein and Krumholz, Harlan M. and Leskovec, Jure and Topol, Eric J. and Rajpurkar, Pranav},
    number = {7956},
    month = {4},
    pages = {259--265},
    volume = {616},
    publisher = {Nature Publishing Group},
    url = {https://www.nature.com/articles/s41586-023-05881-4},
    doi = {10.1038/s41586-023-05881-4},
    issn = {1476-4687},
    pmid = {37045921},
    keywords = {Computational biology and bioinformatics, Health care}
}

@article{Schneider2024FoundationIntelligence,
    title = {{Foundation Models: A New Paradigm for Artificial Intelligence}},
    year = {2024},
    journal = {Business and Information Systems Engineering},
    author = {Schneider, Johannes and Meske, Christian and Kuss, Pauline},
    number = {2},
    month = {4},
    pages = {221--231},
    volume = {66},
    publisher = {Springer Gabler},
    url = {https://link.springer.com/article/10.1007/s12599-024-00851-0},
    doi = {10.1007/S12599-024-00851-0/FIGURES/2},
    issn = {18670202},
    keywords = {Artificial intelligence, Emergent behavior, Foundation models, Generative AI, Prompting}
}

@article{Team2024Gemini:Models,
    title = {{Gemini: A Family of Highly Capable Multimodal Models}},
    year = {2024},
    author = {Team, Gemini},
    arxivId = {2312.11805v3}
}

@article{ZhaoGeneratingDetectors,
    title = {{Generating Enhanced Negatives for Training Language-Based Object Detectors}},
    author = {Zhao, Shiyu and Zhao, Long and Kumar, Vijay and Suh, Yumin and Metaxas, Dimitris N and Chandraker, Manmohan and Schulter, Samuel},
    url = {https://github.com/},
    arxivId = {2401.00094v2}
}

@article{Zhang2022GLIPv2:Understanding,
    title = {{GLIPv2: Unifying Localization and Vision-Language Understanding}},
    year = {2022},
    author = {Zhang, Haotian and Zhang, Pengchuan and Hu, Xiaowei and Chen, Yen-Chun and Li, Liunian Harold and Dai, Xiyang and Wang, Lijuan and Yuan, Lu and Hwang, Jenq-Neng and Gao, Jianfeng},
    month = {6},
    url = {https://arxiv.org/abs/2206.05836v2},
    arxivId = {2206.05836}
}

@article{ZhangGLIPv2:Understanding,
    title = {{GLIPv2: Unifying Localization and VL Understanding}},
    author = {Zhang, Haotian and Zhang, Pengchuan and Hu, Xiaowei and Chen, Yen-Chun and Harold Li, Liunian and Dai, Xiyang and Wang, Lijuan and Yuan, Lu and Hwang, Jenq-Neng and Gao, Jianfeng},
    url = {https://github.com/microsoft/GLIP.},
    arxivId = {2206.05836v2}
}

@article{Meara2015GlobalDevelopment,
    title = {{Global Surgery 2030: evidence and solutions for achieving health, welfare, and economic development}},
    year = {2015},
    journal = {Lancet (London, England)},
    author = {Meara, John G. and Leather, Andrew J.M. and Hagander, Lars and Alkire, Blake C. and Alonso, Nivaldo and Ameh, Emmanuel A. and Bickler, Stephen W. and Conteh, Lesong and Dare, Anna J. and Davies, Justine and M{\'{e}}risier, Eunice Dérivois and El-Halabi, Shenaaz and Farmer, Paul E. and Gawande, Atul and Gillies, Rowan and Greenberg, Sarah L.M. and Grimes, Caris E. and Gruen, Russell L. and Ismail, Edna Adan and Kamara, Thaim Buya and Lavy, Chris and Lundeg, Ganbold and Mkandawire, Nyengo C. and Raykar, Nakul P. and Riesel, Johanna N. and Rodas, Edgar and Rose, John and Roy, Nobhojit and Shrime, Mark G. and Sullivan, Richard and Verguet, Stéphane and Watters, David and Weiser, Thomas G. and Wilson, Iain H. and Yamey, Gavin and Yip, Winnie},
    number = {9993},
    month = {8},
    pages = {569--624},
    volume = {386},
    publisher = {Lancet},
    url = {https://pubmed.ncbi.nlm.nih.gov/25924834/},
    doi = {10.1016/S0140-6736(15)60160-X},
    issn = {1474-547X},
    pmid = {25924834},
    keywords = {Andrew J M Leather, Delivery of Health Care / organization {\&} administration*, Developing Countries*, Global Health*, Humans, John G Meara, MEDLINE, NCBI, NIH, NLM, National Center for Biotechnology Information, National Institutes of Health, National Library of Medicine, PubMed Abstract, Review, Specialties, Surgical / organization {\&} administration*, Winnie Yip, doi:10.1016/S0140-6736(15)60160-X, pmid:25924834}
}

@article{HuangGLoRIA:Recognition,
    title = {{GLoRIA: A Multimodal Global-Local Representation Learning Framework for Label-efficient Medical Image Recognition}},
    author = {Huang, Shih-Cheng and Shen, Liyue and Lungren, Matthew P and Yeung, Serena},
    url = {https://github.com/marshuang80/gloria}
}

@article{Szegedy2015GoingConvolutions,
    title = {{Going deeper with convolutions}},
    year = {2015},
    journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
    author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
    month = {10},
    pages = {1--9},
    volume = {07-12-June-2015},
    publisher = {IEEE Computer Society},
    isbn = {9781467369640},
    doi = {10.1109/CVPR.2015.7298594},
    issn = {10636919},
    arxivId = {1409.4842}
}

@article{Huang2018GPipe:Parallelism,
    title = {{GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism}},
    year = {2018},
    journal = {Advances in Neural Information Processing Systems},
    author = {Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Mia Xu and Chen, Dehao and Lee, Hyouk Joong and Ngiam, Jiquan and Le, Quoc V. and Wu, Yonghui and Chen, Zhifeng},
    month = {11},
    volume = {32},
    publisher = {Neural information processing systems foundation},
    url = {https://arxiv.org/abs/1811.06965v5},
    issn = {10495258},
    arxivId = {1811.06965}
}

@article{SeptemberGPT-4VisionCard,
    title = {{GPT-4V(ision) System Card}},
    author = {September, Openai}
}

@article{Li2021GroundedPre-training,
    title = {{Grounded Language-Image Pre-training}},
    year = {2021},
    journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
    author = {Li, Liunian Harold and Zhang, Pengchuan and Zhang, Haotian and et al.},
    month = {12},
    pages = {10955--10965},
    volume = {2022-June},
    publisher = {IEEE Computer Society},
    url = {https://arxiv.org/abs/2112.03857v2},
    isbn = {9781665469463},
    doi = {10.1109/CVPR52688.2022.01069},
    issn = {10636919},
    arxivId = {2112.03857},
    keywords = {Deep learning architectures and techniques, Recognition: detection, Representation learning, Transfer/low-shot/long-tail learning, Vision + language, categorization, retrieval}
}

@article{HaroldLiGroundedPre-training,
    title = {{Grounded Language-Image Pre-training}},
    author = {Harold Li, Liunian and Zhang, Pengchuan and Zhang, Haotian and Yang, Jianwei and Li, Chunyuan and Zhong, Yiwu and Wang, Lijuan and Yuan, Lu and Zhang, Lei and Hwang, Jenq-Neng and Chang, Kai-Wei and Gao, Jianfeng},
    url = {https://github.com/microsoft/GLIP.},
    journal = {2112.03857v2}
}

@article{LiuGroundingDetection,
    title = {{Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection}},
    author = {Liu, Shilong and Zeng, Zhaoyang and Ren, Tianhe and Li, Feng and Zhang, Hao and Yang, Jie and Li, Chunyuan and Yang, Jianwei and Su, Hang and Zhu, Jun and Zhang, Lei},
    journal = {2303.05499v4}
}

@article{Shin2023Hospital-wideRadiographs,
    title = {{Hospital-wide survey of clinical experience with artificial intelligence applied to daily chest radiographs}},
    year = {2023},
    journal = {PLOS ONE},
    author = {Shin, Hyun Joo and Lee, Seungsoo and Kim, Sungwon and Son, Nak Hoon and Kim, Eun Kyung},
    number = {3},
    month = {3},
    volume = {18},
    publisher = {PLOS},
    url = {/pmc/articles/PMC9980810/ /pmc/articles/PMC9980810/?report=abstract https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9980810/},
    doi = {10.1371/JOURNAL.PONE.0282123},
    issn = {19326203},
    pmid = {36862644}
}

@article{KrizhevskyImagenetNetworks,
    title = {{Imagenet classification with deep convolutional neural networks}},
    journal = {proceedings.neurips.ccPaperpileA Krizhevsky, I Sutskever, GE HintonAdvances in neural information processing systems, 2012•proceedings.neurips.ccPaperpile},
    author = {Krizhevsky, A and {\ldots}, I Sutskever - Advances in neural and 2012, undefined},
    url = {https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html}
}

@article{KrizhevskyImageNetNetworksb,
    title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
    author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
    url = {http://code.google.com/p/cuda-convnet/}
}

@article{Deng2009ImageNet:Database,
    title = {{ImageNet: A Large-Scale Hierarchical Image Database}},
    year = {2009},
    journal = {2009 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2009},
    author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li Jia and Li, Kai and Fei-Fei, Li},
    pages = {248--255},
    publisher = {IEEE Computer Society},
    isbn = {9781424439911},
    doi = {10.1109/CVPR.2009.5206848}
}

@article{McFadden2022ImpactRadiotherapy,
    title = {{Impact of COVID-19 on service delivery in radiology and radiotherapy}},
    year = {2022},
    journal = {Radiography},
    author = {Mc Fadden, S. and Flood, T. and Shepherd, P. and Gilleece, T.},
    month = {10},
    pages = {S16-S26},
    volume = {28},
    publisher = {W.B. Saunders},
    doi = {10.1016/J.RADI.2022.03.009},
    issn = {1078-8174},
    pmid = {35422396},
    keywords = {COVID-19 pandemic, Radiography, Radiography service provision, Radiography workforce, Radiotherapy}
}

@article{Liu2023ImprovingTriage,
    title = {{Improving Medical Vision-Language Contrastive Pretraining With Semantics-Aware Triage}},
    year = {2023},
    journal = {IEEE transactions on medical imaging},
    author = {Liu, Bo and Lu, Donghuan and Wei, Dong and Wu, Xian and Wang, Yan and Zhang, Yu and Zheng, Yefeng},
    number = {12},
    month = {12},
    pages = {3579--3589},
    volume = {42},
    publisher = {IEEE Trans Med Imaging},
    url = {https://pubmed.ncbi.nlm.nih.gov/37440389/},
    doi = {10.1109/TMI.2023.3294980},
    issn = {1558-254X},
    pmid = {37440389},
    keywords = {Bo Liu, Donghuan Lu, Language, MEDLINE, NCBI, NIH, NLM, National Center for Biotechnology Information, National Institutes of Health, National Library of Medicine, PubMed Abstract, Semantics*, Triage*, Yefeng Zheng, doi:10.1109/TMI.2023.3294980, pmid:37440389}
}

@article{Lange2022InfluenceDiagnoses,
    title = {{Influence of Prior Imaging Information on Diagnostic Accuracy for Focal Skeletal Processes—A Retrospective Analysis of the Consistency between Biopsy-Verified Imaging Diagnoses}},
    year = {2022},
    journal = {Diagnostics},
    author = {Lange, Mine Benedicte and Petersen, Lars J. and Lausen, Mads and Bruun, Niels Henrik and Nielsen, Michael Bachmann and Zacho, Helle D.},
    number = {7},
    month = {7},
    volume = {12},
    publisher = {Multidisciplinary Digital Publishing Institute (MDPI)},
    url = {/pmc/articles/PMC9319824/ /pmc/articles/PMC9319824/?report=abstract https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9319824/},
    doi = {10.3390/DIAGNOSTICS12071735/S1},
    issn = {20754418},
    pmid = {35885639},
    keywords = {biopsy, bone, cancer, diagnostic accuracy, medical imaging, metastasis, prior imaging, reports, tumor}
}

@article{TanidaInteractiveGeneration,
    title = {{Interactive and Explainable Region-guided Radiology Report Generation}},
    author = {Tanida, Tim and M{\"{u}}ller, Philip and Kaissis, Georgios and Rueckert, Daniel},
    url = {https://github.com/ttanida/rgrg},
    journal = {2304.08295v1}
}

@misc{IntroducingDate,
    title = {{Introducing Meta Llama 3: The most capable openly available LLM to date}},
    url = {https://ai.meta.com/blog/meta-llama-3/}
}

@article{Muller2021JointReports,
    title = {{Joint Learning of Localized Representations from Medical Images and Reports}},
    year = {2021},
    journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
    author = {M{\"{u}}ller, Philip and Kaissis, Georgios and Zou, Congyu and Rueckert, Daniel},
    month = {12},
    pages = {685--701},
    volume = {13686 LNCS},
    publisher = {Springer Science and Business Media Deutschland GmbH},
    url = {http://arxiv.org/abs/2112.02889 http://dx.doi.org/10.1007/978-3-031-19809-0_39},
    doi = {10.1007/978-3-031-19809-0{\_}39},
    arxivId = {2112.02889v2},
    keywords = {Contrastive learning, Representation learning, Su-pervision, Text, Text supervision}
}

@article{ChenMedicalAlignment,
    title = {{Medical Phrase Grounding with Region-Phrase Context Contrastive Alignment}},
    author = {Chen, Zhihao and Zhou, Yang and Tran, Anh and Zhao, Junting and Wan, Liang and Ooi, Gideon and Cheng, Lionel and Thng, Hua and Xu, Xinxing and Liu, Yong and Fu, Huazhu},
    journal = {2303.07618v1}
}

@article{Muller2021JointReportsb,
    title = {{Joint Learning of Localized Representations from Medical Images and Reports}},
    year = {2021},
    journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
    author = {M{\"{u}}ller, Philip and Kaissis, Georgios and Zou, Congyu and Rueckert, Daniel},
    month = {12},
    pages = {685--701},
    volume = {13686 LNCS},
    publisher = {Springer Science and Business Media Deutschland GmbH},
    url = {http://arxiv.org/abs/2112.02889 http://dx.doi.org/10.1007/978-3-031-19809-0_39},
    doi = {10.1007/978-3-031-19809-0{\_}39},
    arxivId = {2112.02889v2},
    keywords = {Contrastive learning, Representation learning, Su-pervision, Text, Text supervision}
}

@article{Peng2023KOSMOS-2:World,
    title = {{KOSMOS-2: Grounding Multimodal Large Language Models to the World}},
    year = {2023},
    author = {Peng, Zhiliang and Wang, Wenhui and Dong, Li and Hao, Yaru and Huang, Shaohan and Ma, Shuming and Wei, Furu},
    url = {https://aka.ms/GeneralAI},
    arxivId = {2306.14824v3}
}

@article{Huang2023LanguageModels,
    title = {{Language Is Not All You Need: Aligning Perception with Language Models}},
    year = {2023},
    journal = {Advances in Neural Information Processing Systems},
    author = {Huang, Shaohan and Dong, Li and Wang, Wenhui and Hao, Yaru and Singhal, Saksham and Ma, Shuming and Lv, Tengchao and Cui, Lei and Mohammed, Owais Khan and Patra, Barun and Liu, Qiang and Aggarwal, Kriti and Chi, Zewen and Bjorck, Johan and Chaudhary, Vishrav and Som, Subhojit and Song, Xia and Wei, Furu},
    month = {2},
    volume = {36},
    publisher = {Neural information processing systems foundation},
    url = {https://arxiv.org/abs/2302.14045v2},
    issn = {10495258},
    arxivId = {2302.14045}
}

@article{Brown2020LanguageLearners,
    title = {{Language Models are Few-Shot Learners}},
    year = {2020},
    journal = {Advances in Neural Information Processing Systems},
    author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
    month = {5},
    volume = {2020-December},
    publisher = {Neural information processing systems foundation},
    url = {https://arxiv.org/abs/2005.14165v4},
    issn = {10495258},
    arxivId = {2005.14165}
}

@article{RadfordLanguageLearners,
    title = {{Language Models are Unsupervised Multitask Learners}},
    author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
    url = {https://github.com/codelucas/newspaper}
}

@article{RadfordLanguageLearnersb,
    title = {{Language Models are Unsupervised Multitask Learners}},
    author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
    url = {https://github.com/codelucas/newspaper}
}

@article{RadfordLearningSupervision,
    title = {{Learning Transferable Visual Models From Natural Language Supervision}},
    author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
    url = {https://github.com/OpenAI/CLIP.},
    journal = {2103.00020v1}
}

@article{Touvron2023LlamaModels,
    title = {{Llama 2: Open Foundation and Fine-Tuned Chat Models}},
    year = {2023},
    author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Michael, Eric and Ranjan, Smith and Xiaoqing, Subramanian and Tan, Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
    month = {7},
    url = {https://arxiv.org/abs/2307.09288v2},
    arxivId = {2307.09288}
}

@article{Touvron2023LLaMA:Models,
    title = {{LLaMA: Open and Efficient Foundation Language Models}},
    year = {2023},
    author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothée and Rozi{\`{e}}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
    month = {2},
    url = {https://arxiv.org/abs/2302.13971v1},
    arxivId = {2302.13971}
}




@inproceedings{
Hu2021LoRA:Models,
title={Lo{RA}: Low-Rank Adaptation of Large Language Models},
author={Edward J Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
booktitle={ICLR},
year={2022},
}

@article{Boecking2022MakingProcessing,
    title = {{Making the Most of Text Semantics to Improve Biomedical Vision--Language Processing}},
    year = {2022},
    journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
    author = {Boecking, Benedikt and Usuyama, Naoto and Bannur, and et al.},
    month = {4},
    pages = {1--21},
    volume = {13696 LNCS},
    publisher = {Springer Science and Business Media Deutschland GmbH},
    url = {http://arxiv.org/abs/2204.09817 http://dx.doi.org/10.1007/978-3-031-20059-5_1},
    doi = {10.1007/978-3-031-20059-5{\_}1},
    journal = {2204.09817v4},
    keywords = {Multi-modal, Radiology, Self-supervision, Weak supervision}
}

@article{BoeckingMakingProcessing,
    title = {{Making the Most of Text Semantics to Improve Biomedical Vision-Language Processing}},
    author = {Boecking, Benedikt and Usuyama, Naoto and Bannur, and et al.},
    url = {https://aka.ms/biovil-code},
    arxivId = {2204.09817v4},
    keywords = {multi-modal, radiology, self-supervision, weak supervision}
}

@article{Alexander2022MandatingRadiology,
    title = {{Mandating Limits on Workload, Duty, and Speed in Radiology}},
    year = {2022},
    journal = {Radiology},
    author = {Alexander, Robert and Waite, Stephen and Bruno, Michael A. and Krupinski, Elizabeth A. and Berlin, Leonard and Macknik, Stephen and Martinez-Conde, Susana},
    number = {2},
    month = {8},
    pages = {274--282},
    volume = {304},
    publisher = {Radiological Society of North America Inc.},
    url = {https://pubs.rsna.org/doi/10.1148/radiol.212631},
    doi = {10.1148/RADIOL.212631/ASSET/IMAGES/LARGE/RADIOL.212631.VA.JPEG},
    issn = {15271315},
    pmid = {35699581}
}

@article{HeMaskedLearners,
    title = {{Masked Autoencoders Are Scalable Vision Learners}},
    author = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\'{a}}r, Piotr and Girshick, Ross},
    arxivId = {2111.06377v3}
}

@article{Kamath2021MDETRUnderstanding,
    title = {{MDETR -- Modulated Detection for End-to-End Multi-Modal Understanding}},
    year = {2021},
    journal = {Proceedings of the IEEE International Conference on Computer Vision},
    author = {Kamath, Aishwarya and Singh, Mannat and LeCun, Yann and Synnaeve, Gabriel and Misra, Ishan and Carion, Nicolas},
    month = {4},
    pages = {1760--1770},
    publisher = {Institute of Electrical and Electronics Engineers Inc.},
    url = {https://arxiv.org/abs/2104.12763v2},
    isbn = {9781665428125},
    doi = {10.1109/ICCV48922.2021.00180},
    issn = {15505499},
    arxivId = {2104.12763}
}

@article{Moor2023Med-Flamingo:Learner,
    title = {{Med-Flamingo: a Multimodal Medical Few-shot Learner}},
    year = {2023},
    journal = {Proceedings of Machine Learning Research},
    author = {Moor, Michael and Huang, Qian and Wu, Shirley and Yasunaga, Michihiro and Dalmia, Yash and Leskovec, Jure and Zakka, Cyril and Reis, Eduardo Pontes and Rajpurkar, Pranav},
    month = {7},
    pages = {353--367},
    volume = {225},
    publisher = {ML Research Press},
    url = {https://arxiv.org/abs/2307.15189v1},
    issn = {26403498},
    arxivId = {2307.15189},
    keywords = {Few-shot learning, Foundation model, Medical AI, Vision-language model}
}

@article{WangMedCLIP:Text,
    title = {{MedCLIP: Contrastive Learning from Unpaired Medical Images and Text}},
    author = {Wang, Zifeng and Wu, Zhenbang and Agarwal, Dinesh and Sun, Jimeng},
    url = {https://github.com/},
    arxivId = {2210.10163v1}
}

@misc{chen2023medicalphrasegroundingregionphrase,
      title={Medical Phrase Grounding with Region-Phrase Context Contrastive Alignment}, 
      author={Zhihao Chen and Yang Zhou and Anh Tran and Junting Zhao and Liang Wan and Gideon Ooi and Lionel Cheng and Choon Hua Thng and Xinxing Xu and Yong Liu and Huazhu Fu},
      year={2023},
      eprint={2303.07618},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2303.07618}, 
}

@article{ChenMedicalAlignmentb,
    title = {{Medical Phrase Grounding with Region-Phrase Context Contrastive Alignment}},
    author = {Chen, Zhihao and Zhou, Yang and Tran, Anh and Zhao, Junting and Wan, Liang and Ooi, Gideon and Cheng, Lionel and Thng, Hua and Xu, Xinxing and Liu, Yong and Fu, Huazhu},
    journal = {2303.07618v1}
}

@article{ZouMedRG:Model,
    title = {{MedRG: Medical Report Grounding with Multi-modal Large Language Model}},
    author = {Zou, Ke and Bai, Yang and Chen, Zhihao and Zhou, Yang and Chen, Yidi and Ren, Kai and Wang, Meng and Yuan, Xuedong and Shen, Xiaojing and Fu, Huazhu},
    arxivId = {2404.06798v1}
}

@article{Johnson2019MIMIC-CXR-JPGRadiographs,
    title = {{MIMIC-CXR-JPG, a large publicly available database of labeled chest radiographs}},
    year = {2019},
    author = {Johnson, Alistair E. W. and Pollard, Tom J. and Greenbaum, Nathaniel R. and Lungren, Matthew P. and Deng, Chih-ying and Peng, Yifan and Lu, Zhiyong and Mark, Roger G. and Berkowitz, Seth J. and Horng, Steven},
    month = {1},
    url = {http://arxiv.org/abs/1901.07042},
    arxivId = {1901.07042}
}

@article{Yildirim2024MultimodalRadiology,
    title = {{Multimodal Healthcare AI: Identifying and Designing Clinically Relevant Vision-Language Applications for Radiology}},
    year = {2024},
    journal = {Conference on Human Factors in Computing Systems - Proceedings},
    author = {Yildirim, Nur and Richardson, Hannah and Wetscherek, et al.},
    month = {2},
    volume = {22},
    publisher = {Association for Computing Machinery},
    url = {http://arxiv.org/abs/2402.14252 http://dx.doi.org/10.1145/3613904.3642013},
    doi = {10.1145/3613904.3642013},
    arxivId = {2402.14252v1},
    keywords = {Human-centered AI, IxD, healthcare, human-AI interaction, medical imaging, radiology, responsible AI}
}

@article{Xu2022NegativeReview,
    title = {{Negative Sampling for Contrastive Representation Learning: A Review}},
    year = {2022},
    author = {Xu, Lanling and Lian, Jianxun and Zhao, Wayne Xin and Gong, Ming and Shou, Linjun and Jiang, Daxin and Xie, Xing and Wen, Ji-Rong},
    arxivId = {2206.00212v1}
}


@article{Bommasani2021OnModels,
    title = {{On the Opportunities and Risks of Foundation Models}},
    year = {2021},
    author = {Bommasani, Rishi and Hudson, Drew A. and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S. and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and Brynjolfsson, Erik and Buch, Shyamal and Card, Dallas and Castellon, Rodrigo and Chatterji, Niladri and Chen, Annie and Creel, Kathleen and Davis, Jared Quincy and Demszky, Dora and Donahue, Chris and Doumbouya, Moussa and Durmus, Esin and Ermon, Stefano and Etchemendy, John and Ethayarajh, Kawin and Fei-Fei, Li and Finn, Chelsea and Gale, Trevor and Gillespie, Lauren and Goel, Karan and Goodman, Noah and Grossman, Shelby and Guha, Neel and Hashimoto, Tatsunori and Henderson, Peter and Hewitt, John and Ho, Daniel E. and Hong, Jenny and Hsu, Kyle and Huang, Jing and Icard, Thomas and Jain, Saahil and Jurafsky, Dan and Kalluri, Pratyusha and Karamcheti, Siddharth and Keeling, Geoff and Khani, Fereshte and Khattab, Omar and Koh, Pang Wei and Krass, Mark and Krishna, Ranjay and Kuditipudi, Rohith and Kumar, Ananya and Ladhak, Faisal and Lee, Mina and Lee, Tony and Leskovec, Jure and Levent, Isabelle and Li, Xiang Lisa and Li, Xuechen and Ma, Tengyu and Malik, Ali and Manning, Christopher D. and Mirchandani, Suvir and Mitchell, Eric and Munyikwa, Zanele and Nair, Suraj and Narayan, Avanika and Narayanan, Deepak and Newman, Ben and Nie, Allen and Niebles, Juan Carlos and Nilforoshan, Hamed and Nyarko, Julian and Ogut, Giray and Orr, Laurel and Papadimitriou, Isabel and Park, Joon Sung and Piech, Chris and Portelance, Eva and Potts, Christopher and Raghunathan, Aditi and Reich, Rob and Ren, Hongyu and Rong, Frieda and Roohani, Yusuf and Ruiz, Camilo and Ryan, Jack and R{\'{e}}, Christopher and Sadigh, Dorsa and Sagawa, Shiori and Santhanam, Keshav and Shih, Andy and Srinivasan, Krishnan and Tamkin, Alex and Taori, Rohan and Thomas, Armin W. and Tram{\`{e}}r, Florian and Wang, Rose E. and Wang, William and Wu, Bohan and Wu, Jiajun and Wu, Yuhuai and Xie, Sang Michael and Yasunaga, Michihiro and You, Jiaxuan and Zaharia, Matei and Zhang, Michael and Zhang, Tianyi and Zhang, Xikun and Zhang, Yuhui and Zheng, Lucia and Zhou, Kaitlyn and Liang, Percy},
    month = {8},
    url = {https://arxiv.org/abs/2108.07258v3},
    arxivId = {2108.07258}
}

@article{Zhao2023OnPCFGs,
    title = {{On the Transferability of Visually Grounded PCFGs}},
    year = {2023},
    author = {Zhao, Yanpeng and Titov Eae, Ivan},
    month = {10},
    url = {https://arxiv.org/abs/2310.14107v1},
    journal = {2310.14107}
}

@article{ZangOpen-VocabularyMatching,
    title = {{Open-Vocabulary DETR with Conditional Matching}},
    author = {Zang, Yuhang and Li, Wei and Zhou, Kaiyang and Huang, Chen and Loy, Chen Change},
    url = {https://github.com/yuhangzang/OV-DETR.},
    arxivId = {2203.11876v2}
}

@article{Gu2021Open-vocabularyDistillation,
    title = {{Open-vocabulary Object Detection via Vision and Language Knowledge Distillation}},
    year = {2021},
    journal = {ICLR 2022 - 10th International Conference on Learning Representations},
    author = {Gu, Xiuye and Lin, Tsung Yi and Kuo, Weicheng and Cui, Yin},
    month = {4},
    publisher = {International Conference on Learning Representations, ICLR},
    url = {https://arxiv.org/abs/2104.13921v3},
    arxivId = {2104.13921}
}

@article{Bustos2019PadChest:Reports,
    title = {{PadChest: A large chest x-ray image dataset with multi-label annotated reports}},
    year = {2019},
    journal = {Medical Image Analysis},
    author = {Bustos, Aurelia and Pertusa, Antonio and Salinas, Jose-Maria and de la Iglesia-Vay{\'{a}}, Maria},
    month = {1},
    volume = {66},
    publisher = {Elsevier B.V.},
    url = {http://arxiv.org/abs/1901.07441 http://dx.doi.org/10.1016/j.media.2020.101797},
    doi = {10.1016/j.media.2020.101797},
    arxivId = {1901.07441v2},
    keywords = {Anatomical locations, Deep neural networks, Differential diagnoses, Radiographic findings, X-Ray image dataset}
}

@article{Bustos2019PADCHEST:PREPRINT,
    title = {{PADCHEST: A LARGE CHEST X-RAY IMAGE DATASET WITH MULTI-LABEL ANNOTATED REPORTS A PREPRINT}},
    year = {2019},
    author = {Bustos, Aurelia and Pertusa, Antonio and Salinas, Jose-Maria and De La Iglesia-Vay{\'{a}}, Maria},
    url = {http://bimcv.cipf.es/bimcv-projects/padchest/.},
    isbn = {1901.07441v2},
    arxivId = {1901.07441v2},
    keywords = {Anatomical, Deep neural networks {\textperiodcentered}, Differential diagnoses {\textperiodcentered}, Locations, Radiographic findings {\textperiodcentered}, X-ray image dataset {\textperiodcentered}}
}

@article{DriessPaLM-E:Model,
    title = {{PaLM-E: An Embodied Multimodal Language Model}},
    author = {Driess, Danny and Xia, Fei and Sajjadi, Mehdi S M and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and Huang, Wenlong and Chebotar, Yevgen and Sermanet, Pierre and Duckworth, Daniel and Levine, Sergey and Vanhoucke, Vincent and Hausman, Karol and Toussaint, Marc and Greff, Klaus and Zeng, Andy and Mordatch, Igor and Florence, Pete},
    url = {https://palm-e.github.io},
    arxivId = {2303.03378v1}
}

@article{Chowdhery2022PaLM:Pathways,
    title = {{PaLM: Scaling Language Modeling with Pathways}},
    year = {2022},
    author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sasha and Maynez, Joshua and Rao, Abhishek and Barnes, Parker and Tay, Yi and Shazeer, Noam and Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and Hutchinson, Ben and Pope, Reiner and Bradbury, James and Austin, Jacob and Isard, Michael and Gur-Ari, Guy and Yin, Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat, Sanjay and Dev, Sunipa and Michalewski, Henryk and Garcia, Xavier and Misra, Vedant and Robinson, Kevin and Fedus, Liam and Zhou, Denny and Ippolito, Daphne and Luan, David and Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexander and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and Omernick, Mark and Dai, Andrew M. and Pillai, Thanumalayan Sankaranarayana and Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei and Wang, Xuezhi and Saeta, Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele and Wei, Jason and Meier-Hellstern, Kathy and Eck, Douglas and Dean, Jeff and Petrov, Slav and Fiedel, Noah},
    month = {4},
    url = {https://arxiv.org/abs/2204.02311v5},
    arxivId = {2204.02311}
}

@article{Gupta2014PostproceduralUnit,
    title = {{Postprocedural chest radiograph: Impact on the management in critical care unit}},
    year = {2014},
    journal = {Anesthesia, Essays and Researches},
    author = {Gupta, Prashant K. and Gupta, Kumkum and Jain, Manish and Garg, Tanuj},
    number = {2},
    pages = {139},
    volume = {8},
    publisher = {Wolters Kluwer -- Medknow Publications},
    url = {/pmc/articles/PMC4173625/ /pmc/articles/PMC4173625/?report=abstract https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4173625/},
    doi = {10.4103/0259-1162.134481},
    issn = {0259-1162},
    pmid = {25886216}
}

@article{PRACTICEImaging,
    title = {{PRACTICE PARAMETER 1 Communication Diagnostic Imaging}}
}

@article{Perez-GarciaRAD-DINO:PREPRINT,
    title = {{RAD-DINO: EXPLORING SCALABLE MEDICAL IMAGE ENCODERS BEYOND TEXT SUPERVISION A PREPRINT}},
    author = {P{\'{e}}rez-Garc{\'{i}}a, Fernando and Sharma, Harshita and Bond-Taylor, Sam and Bouzid, Kenza and Salvatelli, Valentina and Ilse, Maximilian and Bannur, Shruthi and Castro, Daniel C and Schwaighofer, Anton and Lungren, Matthew P and Wetscherek, Maria and Codella, Noel and Hyland, Stephanie L and Alvarez-Valle, Javier and Oktay, Ozan},
    arxivId = {2401.10815v1}
}

@article{Zhang2024RadGenome-ChestAnalysis,
    title = {{RadGenome-Chest CT: A Grounded Vision-Language Dataset for Chest CT Analysis}},
    year = {2024},
    author = {Zhang, Xiaoman and Wu, Chaoyi and Zhao, Ziheng and Lei, Jiayu and Zhang, Ya and Wang, Yanfeng and Xie, Weidi},
    month = {4},
    url = {https://arxiv.org/abs/2404.16754v1},
    arxivId = {2404.16754}
}

@article{Peng2022RadiologistCentre,
    title = {{Radiologist burnout: Trends in medical imaging utilization under the national health insurance system with the universal code bundling strategy in an academic tertiary medical centre}},
    year = {2022},
    journal = {European Journal of Radiology},
    author = {Peng, Yan-Chih and Lee, Wen-Jeng and Chang, Yeun-Chung and Chan, Wing P and Chen, Shyh-Jye},
    pages = {110596},
    volume = {157},
    url = {https://doi.org/10.1016/j.ejrad.2022.110596},
    doi = {10.1016/j.ejrad.2022.110596},
    keywords = {Burnout, Code Bundling, Computed tomography, Magnetic resonance imaging, National healthcare insurance, Workload}
}

@article{Zhong2021RegionCLIP:Pretraining,
    title = {{RegionCLIP: Region-based Language-Image Pretraining}},
    year = {2021},
    journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
    author = {Zhong, Yiwu and Yang, Jianwei and Zhang, Pengchuan and Li, Chunyuan and Codella, Noel and Li, Liunian Harold and Zhou, Luowei and Dai, Xiyang and Yuan, Lu and Li, Yin and Gao, Jianfeng},
    month = {12},
    pages = {16772--16782},
    volume = {2022-June},
    publisher = {IEEE Computer Society},
    url = {https://arxiv.org/abs/2112.09106v1},
    isbn = {9781665469463},
    doi = {10.1109/CVPR52688.2022.01629},
    issn = {10636919},
    arxivId = {2112.09106},
    keywords = {Recognition: detection, Representation learning, Self-{\&} semi-{\&} meta- Transfer/low-shot/long-tail learning, Vision + language, categorization, retrieval}
}

@misc{RSNARSNA,
    title = {{RSNA Pneumonia Detection Challenge (2018) | RSNA}},
    url = {https://www.rsna.org/rsnai/ai-image-challenge/rsna-pneumonia-detection-challenge-2018}
}

@misc{RSNARSNAb,
    title = {{RSNA Pneumonia Detection Challenge (2018) | RSNA}},
    url = {https://www.rsna.org/rsnai/ai-image-challenge/rsna-pneumonia-detection-challenge-2018}
}

@article{Jia2021ScalingSupervision,
    title = {{Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision}},
    year = {2021},
    journal = {Proceedings of Machine Learning Research},
    author = {Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi Ting and Parekh, Zarana and Pham, Hieu and Le, Quoc V. and Sung, Yunhsuan and Li, Zhen and Duerig, Tom},
    month = {2},
    pages = {4904--4916},
    volume = {139},
    publisher = {ML Research Press},
    url = {https://arxiv.org/abs/2102.05918v2},
    isbn = {9781713845065},
    issn = {26403498},
    arxivId = {2102.05918}
}

@article{Agrawal2023SegmentationSurvey,
    title = {{Segmentation and classification on chest radiography: a systematic survey}},
    year = {2023},
    journal = {The Visual computer},
    author = {Agrawal, Tarun and Choudhary, Prakash},
    number = {3},
    month = {3},
    pages = {875--913},
    volume = {39},
    publisher = {Vis Comput},
    url = {https://pubmed.ncbi.nlm.nih.gov/35035008/},
    doi = {10.1007/S00371-021-02352-7},
    issn = {0178-2789},
    pmid = {35035008},
    keywords = {MEDLINE, NCBI, NIH, NLM, National Center for Biotechnology Information, National Institutes of Health, National Library of Medicine, PMC8741572, Prakash Choudhary, PubMed Abstract, Tarun Agrawal, doi:10.1007/s00371-021-02352-7, pmid:35035008}
}

@article{Du2021Self-ContrastiveLearning,
    title = {{Self-Contrastive Learning with Hard Negative Sampling  for Self-supervised Point Cloud Learning; Self-Contrastive Learning with Hard Negative Sampling  for Self-supervised Point Cloud Learning}},
    year = {2021},
    author = {Du, an and Gao, Xiang and Hu, Wei and Li, Xin},
    url = {https://doi.org/10.1145/3474085.3475458},
    isbn = {9781450386517},
    doi = {10.1145/3474085.3475458},
    arxivId = {2107.01886v2},
    keywords = {Contrastive learning, hard negative sampling, nonlocal self-similarity, point clouds, self-supervised learning}
}

@misc{SIIM-ACRKaggle,
    title = {{SIIM-ACR Pneumothorax Segmentation | Kaggle}},
    url = {https://www.kaggle.com/competitions/siim-acr-pneumothorax-segmentation/}
}

@article{MindererSimpleTransformers,
    title = {{Simple Open-Vocabulary Object Detection with Vision Transformers}},
    author = {Minderer, Matthias and Gritsenko, Alexey and Stone, Austin and Neumann, Maxim and Weissenborn, Dirk and Dosovitskiy, Alexey and Mahendran, Aravindh and Arnab, Anurag and Dehghani, Mostafa and Shen, Zhuoran and Wang, Xiao and Zhai, Xiaohua and Kipf, Thomas and Houlsby, Neil},
    keywords = {CLIP, contrastive learning, foundation models, image-conditioned detection, image-text models, one-shot object detec-tion, open-vocabulary detection, transformer, vision transformer, zero-shot detection}
}

@article{Sellergren2022SimplifiedData,
    title = {{Simplified Transfer Learning for Chest Radiography Models Using Less Data}},
    year = {2022},
    journal = {Radiology},
    author = {Sellergren, Andrew B. and Chen, Christina and Nabulsi, Zaid and Li, Yuanzhen and Maschinot, Aaron and Sarna, Aaron and Huang, Jenny and Lau, Charles and Kalidindi, Sreenivasa Raju and Etemadi, Mozziyar and Garcia-Vicente, Florencia and Melnick, David and Liu, Yun and Eswaran, Krish and Tse, Daniel and Beladia, Neeral and Krishnan, Dilip and Shetty, Shravya},
    number = {2},
    month = {11},
    pages = {454--465},
    volume = {305},
    publisher = {Radiological Society of North America Inc.},
    url = {https://pubs.rsna.org/doi/10.1148/radiol.212482},
    doi = {10.1148/RADIOL.212482/ASSET/IMAGES/LARGE/RADIOL.212482.FIG6.JPEG},
    issn = {15271315},
    pmid = {35852426}
}

@article{Liu2015SSD:Detector,
    title = {{SSD: Single Shot MultiBox Detector}},
    year = {2015},
    journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
    author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng Yang and Berg, Alexander C.},
    month = {12},
    pages = {21--37},
    volume = {9905 LNCS},
    publisher = {Springer Verlag},
    url = {https://arxiv.org/abs/1512.02325v5},
    isbn = {9783319464473},
    doi = {10.1007/978-3-319-46448-0{\_}2},
    issn = {16113349},
    arxivId = {1512.02325},
    keywords = {Convolutional neural network, Real-time object detection}
}

@article{Dubey2024TheModels,
    title = {{The Llama 3 Herd of Models}},
    year = {2024},
    author = {Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and Goyal, Anirudh and Hartshorn, Anthony and Yang, Aobo and Mitra, Archi and Sravankumar, Archie and Korenev, Artem and Hinsvark, Arthur and Rao, Arun and Zhang, Aston and Rodriguez, Aurelien and Gregerson, Austen and Spataru, Ava and Roziere, Baptiste and Biron, Bethany and Tang, Binh and Chern, Bobbie and Caucheteux, Charlotte and Nayak, Chaya and Bi, Chloe and Marra, Chris and McConnell, Chris and Keller, Christian and Touret, Christophe and Wu, Chunyang and Wong, Corinne and Ferrer, Cristian Canton and Nikolaidis, Cyrus and Allonsius, Damien and Song, Daniel and Pintz, Danielle and Livshits, Danny and Esiobu, David and Choudhary, Dhruv and Mahajan, Dhruv and Garcia-Olano, Diego and Perino, Diego and Hupkes, Dieuwke and Lakomkin, Egor and AlBadawy, Ehab and Lobanova, Elina and Dinan, Emily and Smith, Eric Michael and Radenovic, Filip and Zhang, Frank and Synnaeve, Gabriel and Lee, Gabrielle and Anderson, Georgia Lewis and Nail, Graeme and Mialon, Gregoire and Pang, Guan and Cucurell, Guillem and Nguyen, Hailey and Korevaar, Hannah and Xu, Hu and Touvron, Hugo and Zarov, Iliyan and Ibarra, Imanol Arrieta and Kloumann, Isabel and Misra, Ishan and Evtimov, Ivan and Copet, Jade and Lee, Jaewon and Geffert, Jan and Vranes, Jana and Park, Jason and Mahadeokar, Jay and Shah, Jeet and van der Linde, Jelmer and Billock, Jennifer and Hong, Jenny and Lee, Jenya and Fu, Jeremy and Chi, Jianfeng and Huang, Jianyu and Liu, Jiawen and Wang, Jie and Yu, Jiecao and Bitton, Joanna and Spisak, Joe and Park, Jongsoo and Rocca, Joseph and Johnstun, Joshua and Saxe, Joshua and Jia, Junteng and Alwala, Kalyan Vasuden and Upasani, Kartikeya and Plawiak, Kate and Li, Ke and Heafield, Kenneth and Stone, Kevin and El-Arini, Khalid and Iyer, Krithika and Malik, Kshitiz and Chiu, Kuenley and Bhalla, Kunal and Rantala-Yeary, Lauren and van der Maaten, Laurens and Chen, Lawrence and Tan, Liang and Jenkins, Liz and Martin, Louis and Madaan, Lovish and Malo, Lubo and Blecher, Lukas and Landzaat, Lukas and de Oliveira, Luke and Muzzi, Madeline and Pasupuleti, Mahesh and Singh, Mannat and Paluri, Manohar and Kardas, Marcin and Oldham, Mathew and Rita, Mathieu and Pavlova, Maya and Kambadur, Melanie and Lewis, Mike and Si, Min and Singh, Mitesh Kumar and Hassan, Mona and Goyal, Naman and Torabi, Narjes and Bashlykov, Nikolay and Bogoychev, Nikolay and Chatterji, Niladri and Duchenne, Olivier and {\c{C}}elebi, Onur and Alrassy, Patrick and Zhang, Pengchuan and Li, Pengwei and Vasic, Petar and Weng, Peter and Bhargava, Prajjwal and Dubal, Pratik and Krishnan, Praveen and Koura, Punit Singh and Xu, Puxin and He, Qing and Dong, Qingxiao and Srinivasan, Ragavan and Ganapathy, Raj and Calderer, Ramon and Cabral, Ricardo Silveira and Stojnic, Robert and Raileanu, Roberta and Girdhar, Rohit and Patel, Rohit and Sauvestre, Romain and Polidoro, Ronnie and Sumbaly, Roshan and Taylor, Ross and Silva, Ruan and Hou, Rui and Wang, Rui and Hosseini, Saghar and Chennabasappa, Sahana and Singh, Sanjay and Bell, Sean and Kim, Seohyun Sonia and Edunov, Sergey and Nie, Shaoliang and Narang, Sharan and Raparthy, Sharath and Shen, Sheng and Wan, Shengye and Bhosale, Shruti and Zhang, Shun and Vandenhende, Simon and Batra, Soumya and Whitman, Spencer and Sootla, Sten and Collot, Stephane and Gururangan, Suchin and Borodinsky, Sydney and Herman, Tamar and Fowler, Tara and Sheasha, Tarek and Georgiou, Thomas and Scialom, Thomas and Speckbacher, Tobias and Mihaylov, Todor and Xiao, Tong and Karn, Ujjwal and Goswami, Vedanuj and Gupta, Vibhor and Ramanathan, Vignesh and Kerkez, Viktor and Gonguet, Vincent and Do, Virginie and Vogeti, Vish and Petrovic, Vladan and Chu, Weiwei and Xiong, Wenhan and Fu, Wenyin and Meers, Whitney and Martinet, Xavier and Wang, Xiaodong and Tan, Xiaoqing Ellen and Xie, Xinfeng and Jia, Xuchao and Wang, Xuewei and Goldschlag, Yaelle and Gaur, Yashesh and Babaei, Yasmine and Wen, Yi and Song, Yiwen and Zhang, Yuchen and Li, Yue and Mao, Yuning and Coudert, Zacharie Delpierre and Yan, Zheng and Chen, Zhengxing and Papakipos, Zoe and Singh, Aaditya and Grattafiori, Aaron and Jain, Abha and Kelsey, Adam and Shajnfeld, Adam and Gangidi, Adithya and Victoria, Adolfo and Goldstand, Ahuva and Menon, Ajay and Sharma, Ajay and Boesenberg, Alex and Vaughan, Alex and Baevski, Alexei and Feinstein, Allie and Kallet, Amanda and Sangani, Amit and Yunus, Anam and Lupu, Andrei and Alvarado, Andres and Caples, Andrew and Gu, Andrew and Ho, Andrew and Poulton, Andrew and Ryan, Andrew and Ramchandani, Ankit and Franco, Annie and Saraf, Aparajita and Chowdhury, Arkabandhu and Gabriel, Ashley and Bharambe, Ashwin and Eisenman, Assaf and Yazdan, Azadeh and James, Beau and Maurer, Ben and Leonhardi, Benjamin and Huang, Bernie and Loyd, Beth and De Paola, Beto and Paranjape, Bhargavi and Liu, Bing and Wu, Bo and Ni, Boyu and Hancock, Braden and Wasti, Bram and Spence, Brandon and Stojkovic, Brani and Gamido, Brian and Montalvo, Britt and Parker, Carl and Burton, Carly and Mejia, Catalina and Wang, Changhan and Kim, Changkyu and Zhou, Chao and Hu, Chester and Chu, Ching-Hsiang and Cai, Chris and Tindal, Chris and Feichtenhofer, Christoph and Civin, Damon and Beaty, Dana and Kreymer, Daniel and Li, Daniel and Wyatt, Danny and Adkins, David and Xu, David and Testuggine, Davide and David, Delia and Parikh, Devi and Liskovich, Diana and Foss, Didem and Wang, Dingkang and Le, Duc and Holland, Dustin and Dowling, Edward and Jamil, Eissa and Montgomery, Elaine and Presani, Eleonora and Hahn, Emily and Wood, Emily and Brinkman, Erik and Arcaute, Esteban and Dunbar, Evan and Smothers, Evan and Sun, Fei and Kreuk, Felix and Tian, Feng and Ozgenel, Firat and Caggioni, Francesco and Guzm{\'{a}}n, Francisco and Kanayet, Frank and Seide, Frank and Florez, Gabriela Medina and Schwarz, Gabriella and Badeer, Gada and Swee, Georgia and Halpern, Gil and Thattai, Govind and Herman, Grant and Sizov, Grigory and {Guangyi} and {Zhang} and Lakshminarayanan, Guna and Shojanazeri, Hamid and Zou, Han and Wang, Hannah and Zha, Hanwen and Habeeb, Haroun and Rudolph, Harrison and Suk, Helen and Aspegren, Henry and Goldman, Hunter and Damlaj, Ibrahim and Molybog, Igor and Tufanov, Igor and Veliche, Irina-Elena and Gat, Itai and Weissman, Jake and Geboski, James and Kohli, James and Asher, Japhet and Gaya, Jean-Baptiste and Marcus, Jeff and Tang, Jeff and Chan, Jennifer and Zhen, Jenny and Reizenstein, Jeremy and Teboul, Jeremy and Zhong, Jessica and Jin, Jian and Yang, Jingyi and Cummings, Joe and Carvill, Jon and Shepard, Jon and McPhie, Jonathan and Torres, Jonathan and Ginsburg, Josh and Wang, Junjie and Wu, Kai and U, Kam Hou and Saxena, Karan and Prasad, Karthik and Khandelwal, Kartikay and Zand, Katayoun and Matosich, Kathy and Veeraraghavan, Kaushik and Michelena, Kelly and Li, Keqian and Huang, Kun and Chawla, Kunal and Lakhotia, Kushal and Huang, Kyle and Chen, Lailin and Garg, Lakshya and A, Lavender and Silva, Leandro and Bell, Lee and Zhang, Lei and Guo, Liangpeng and Yu, Licheng and Moshkovich, Liron and Wehrstedt, Luca and Khabsa, Madian and Avalani, Manav and Bhatt, Manish and Tsimpoukelli, Maria and Mankus, Martynas and Hasson, Matan and Lennie, Matthew and Reso, Matthias and Groshev, Maxim and Naumov, Maxim and Lathi, Maya and Keneally, Meghan and Seltzer, Michael L. and Valko, Michal and Restrepo, Michelle and Patel, Mihir and Vyatskov, Mik and Samvelyan, Mikayel and Clark, Mike and Macey, Mike and Wang, Mike and Hermoso, Miquel Jubert and Metanat, Mo and Rastegari, Mohammad and Bansal, Munish and Santhanam, Nandhini and Parks, Natascha and White, Natasha and Bawa, Navyata and Singhal, Nayan and Egebo, Nick and Usunier, Nicolas and Laptev, Nikolay Pavlovich and Dong, Ning and Zhang, Ning and Cheng, Norman and Chernoguz, Oleg and Hart, Olivia and Salpekar, Omkar and Kalinli, Ozlem and Kent, Parkin and Parekh, Parth and Saab, Paul and Balaji, Pavan and Rittner, Pedro and Bontrager, Philip and Roux, Pierre and Dollar, Piotr and Zvyagina, Polina and Ratanchandani, Prashant and Yuvraj, Pritish and Liang, Qian and Alao, Rachad and Rodriguez, Rachel and Ayub, Rafi and Murthy, Raghotham and Nayani, Raghu and Mitra, Rahul and Li, Raymond and Hogan, Rebekkah and Battey, Robin and Wang, Rocky and Maheswari, Rohan and Howes, Russ and Rinott, Ruty and Bondu, Sai Jayesh and Datta, Samyak and Chugh, Sara and Hunt, Sara and Dhillon, Sargun and Sidorov, Sasha and Pan, Satadru and Verma, Saurabh and Yamamoto, Seiji and Ramaswamy, Sharadh and Lindsay, Shaun and Lindsay, Shaun and Feng, Sheng and Lin, Shenghao and Zha, Shengxin Cindy and Shankar, Shiva and Zhang, Shuqiang and Zhang, Shuqiang and Wang, Sinong and Agarwal, Sneha and Sajuyigbe, Soji and Chintala, Soumith and Max, Stephanie and Chen, Stephen and Kehoe, Steve and Satterfield, Steve and Govindaprasad, Sudarshan and Gupta, Sumit and Cho, Sungmin and Virk, Sunny and Subramanian, Suraj and Choudhury, Sy and Goldman, Sydney and Remez, Tal and Glaser, Tamar and Best, Tamara and Kohler, Thilo and Robinson, Thomas and Li, Tianhe and Zhang, Tianjun and Matthews, Tim and Chou, Timothy and Shaked, Tzook and Vontimitta, Varun and Ajayi, Victoria and Montanez, Victoria and Mohan, Vijai and Kumar, Vinay Satish and Mangla, Vishal and Albiero, Vítor and Ionescu, Vlad and Poenaru, Vlad and Mihailescu, Vlad Tiberiu and Ivanov, Vladimir and Li, Wei and Wang, Wenchen and Jiang, Wenwen and Bouaziz, Wes and Constable, Will and Tang, Xiaocheng and Wang, Xiaofang and Wu, Xiaojian and Wang, Xiaolan and Xia, Xide and Wu, Xilun and Gao, Xinbo and Chen, Yanjun and Hu, Ye and Jia, Ye and Qi, Ye and Li, Yenda and Zhang, Yilin and Zhang, Ying and Adi, Yossi and Nam, Youngjin and {Yu} and {Wang} and Hao, Yuchen and Qian, Yundi and He, Yuzi and Rait, Zach and DeVito, Zachary and Rosnbrick, Zef and Wen, Zhaoduo and Yang, Zhenyu and Zhao, Zhiwei},
    month = {7},
    url = {https://arxiv.org/abs/2407.21783v2},
    arxivId = {2407.21783}
}

@misc{TheMeta,
    title = {{The Llama 3 Herd of Models | Research - AI at Meta}},
    url = {https://ai.meta.com/research/publications/the-llama-3-herd-of-models/}
}

@article{Deng2022TransVG++:Transformer,
    title = {{TransVG++: End-to-End Visual Grounding with Language Conditioned Vision Transformer}},
    year = {2022},
    journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
    author = {Deng, Jiajun and Yang, Zhengyuan and Liu, Daqing and Chen, Tianlang and Zhou, Wengang and Zhang, Yanyong and Li, Houqiang and Ouyang, Wanli},
    number = {11},
    month = {6},
    pages = {13636--13652},
    volume = {45},
    publisher = {IEEE Computer Society},
    url = {https://arxiv.org/abs/2206.06619v1},
    doi = {10.1109/TPAMI.2023.3296823},
    issn = {19393539},
    pmid = {37467085},
    arxivId = {2206.06619},
    keywords = {Deep learning, transformer network, vision and language, visual grounding}
}

@article{Deng2021TransVG:Transformers,
    title = {{TransVG: End-to-End Visual Grounding with Transformers}},
    year = {2021},
    journal = {Proceedings of the IEEE International Conference on Computer Vision},
    author = {Deng, Jiajun and Yang, Zhengyuan and Chen, Tianlang and Zhou, Wengang and Li, Houqiang},
    month = {4},
    pages = {1749--1759},
    publisher = {Institute of Electrical and Electronics Engineers Inc.},
    url = {https://arxiv.org/abs/2104.08541v4},
    isbn = {9781665428125},
    doi = {10.1109/ICCV48922.2021.00179},
    issn = {15505499},
    arxivId = {2104.08541}
}

@article{Deng2021TransVG:Transformersb,
    title = {{TransVG: End-to-End Visual Grounding with Transformers}},
    year = {2021},
    journal = {Proceedings of the IEEE International Conference on Computer Vision},
    author = {Deng, Jiajun and Yang, Zhengyuan and Chen, Tianlang and Zhou, Wengang and Li, Houqiang},
    month = {4},
    pages = {1749--1759},
    publisher = {Institute of Electrical and Electronics Engineers Inc.},
    url = {https://arxiv.org/abs/2104.08541v4},
    isbn = {9781665428125},
    doi = {10.1109/ICCV48922.2021.00179},
    issn = {15505499},
    arxivId = {2104.08541}
}

@article{DanLantsman2022TrendDepartment,
    title = {{Trend in radiologist workload compared to number of admissions in the emergency department}},
    year = {2022},
    journal = {European journal of radiology},
    author = {Dan Lantsman, Christine and Barash, Yiftach and Klang, Eyal and Guranda, Larisa and Konen, Eli and Tau, Noam},
    month = {4},
    volume = {149},
    publisher = {Eur J Radiol},
    url = {https://pubmed.ncbi.nlm.nih.gov/35149337/},
    doi = {10.1016/J.EJRAD.2022.110195},
    issn = {1872-7727},
    pmid = {35149337},
    keywords = {Academic Medical Centers, After-Hours Care* / statistics {\&} numerical data, Christine Dan Lantsman, Emergency Service, Hospital, Humans, MEDLINE, NCBI, NIH, NLM, National Center for Biotechnology Information, National Institutes of Health, National Library of Medicine, Noam Tau, Patient Admission / statistics {\&} numerical data, PubMed Abstract, Radiologists*, Tertiary Care Centers, United States, Workload* / statistics {\&} numerical data, Yiftach Barash, doi:10.1016/j.ejrad.2022.110195, pmid:35149337}
}

@article{Wang2020UnderstandingHypersphere,
    title = {{Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere}},
    year = {2020},
    author = {Wang, Tongzhou and Isola, Phillip},
    arxivId = {2005.10242v10}
}

@article{WangUnifiedPrompt,
    title = {{Unified Medical Image-Text-Label Contrastive Learning With Continuous Prompt}},
    author = {Wang, Yuhao},
    arxivId = {2307.05920v1},
    keywords = {Continuous, Contrastive, Language {\textperiodcentered}, Learning, Medical, Modal, Multi-, Pre-training {\textperiodcentered}, Prompt {\textperiodcentered}, Vision-and-}
}

@article{Simonyan2014VeryRecognition,
    title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
    year = {2014},
    journal = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
    author = {Simonyan, Karen and Zisserman, Andrew},
    month = {9},
    publisher = {International Conference on Learning Representations, ICLR},
    url = {https://arxiv.org/abs/1409.1556v6},
    arxivId = {1409.1556},
    keywords = {()}
}

@misc{VinBigDataKaggle,
    title = {{VinBigData Chest X-ray Abnormalities Detection | Kaggle}},
    url = {https://www.kaggle.com/c/vinbigdata-chest-xray-abnormalities-detection/overview}
}

@misc{VinDr-CXR:VinDr,
    title = {{VinDr-CXR: An open dataset and benchmarks for disease classification and abnormality localization on chest radiographs | VinDr}},
    url = {https://vindr.ai/datasets/cxr}
}

@article{Krishna2016VisualAnnotations,
    title = {{Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations}},
    year = {2016},
    journal = {International Journal of Computer Vision},
    author = {Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li Jia and Shamma, David A. and Bernstein, Michael S. and Fei-Fei, Li},
    number = {1},
    month = {2},
    pages = {32--73},
    volume = {123},
    publisher = {Springer New York LLC},
    url = {https://arxiv.org/abs/1602.07332v1},
    doi = {10.1007/s11263-016-0981-7},
    issn = {15731405},
    arxivId = {1602.07332},
    keywords = {Attributes, Computer vision, Crowdsourcing, Dataset, Image, Knowledge, Language, Objects, Question answering, Relationships, Relationships {\textperiodcentered}, Scene, Scene graph, Vision {\textperiodcentered}}
}

@article{Du2021VisualTransformers,
    title = {{Visual Grounding with Transformers}},
    year = {2021},
    journal = {Proceedings - IEEE International Conference on Multimedia and Expo},
    author = {Du, Ye and Fu, Zehua and Liu, Qingjie and Wang, Yunhong},
    month = {5},
    volume = {2022-July},
    publisher = {IEEE Computer Society},
    url = {https://arxiv.org/abs/2105.04281v3},
    isbn = {9781665485630},
    doi = {10.1109/ICME52920.2022.9859880},
    issn = {1945788X},
    arxivId = {2105.04281},
    keywords = {Transformer, Visual grounding}
}

@article{Liu2023VisualTuning,
    title = {{Visual Instruction Tuning}},
    year = {2023},
    journal = {Advances in Neural Information Processing Systems},
    author = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
    month = {4},
    volume = {36},
    publisher = {Neural information processing systems foundation},
    url = {https://arxiv.org/abs/2304.08485v2},
    issn = {10495258},
    arxivId = {2304.08485}
}

@article{Su2019VL-BERT:Representations,
    title = {{VL-BERT: Pre-training of Generic Visual-Linguistic Representations}},
    year = {2019},
    journal = {8th International Conference on Learning Representations, ICLR 2020},
    author = {Su, Weijie and Zhu, Xizhou and Cao, Yue and Li, Bin and Lu, Lewei and Wei, Furu and Dai, Jifeng},
    month = {8},
    publisher = {International Conference on Learning Representations, ICLR},
    url = {https://arxiv.org/abs/1908.08530v4},
    isbn = {1908.08530v4},
    arxivId = {1908.08530}
}

@article{HaroldLiWorkLANGUAGE,
    title = {{Work in Progress VISUALBERT: A SIMPLE AND PERFORMANT BASELINE FOR VISION AND LANGUAGE}},
    author = {Harold Li, Liunian and Yatskar, Mark and Yin, Da and Hsieh, Cho-Jui and Chang, Kai-Wei},
    arxivId = {1908.03557v1}
}

@article{Kasalak2023WorkRadiology,
    title = {{Work overload and diagnostic errors in radiology}},
    year = {2023},
    journal = {European Journal of Radiology},
    author = {Kasalak, Ömer and Alnahwi, Haider and Toxopeus, Romy and Pennings, Jan P. and Yakar, Derya and Kwee, Thomas C.},
    month = {10},
    pages = {111032},
    volume = {167},
    publisher = {Elsevier},
    doi = {10.1016/J.EJRAD.2023.111032},
    issn = {0720-048X},
    pmid = {37579563},
    keywords = {Computed Tomography, Diagnostic Errors, Workload}
}

@misc{Xuyuan/xsd:Object-CXR,
    title = {{xuyuan/xsd: Automatic detection of foreign objects on chest X-rays, winning soultion of object-CXR}},
    url = {https://github.com/xuyuan/xsd?tab=readme-ov-file}
}

@article{Redmon2015YouDetection,
    title = {{You Only Look Once: Unified, Real-Time Object Detection}},
    year = {2015},
    journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
    author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
    month = {6},
    pages = {779--788},
    volume = {2016-December},
    publisher = {IEEE Computer Society},
    url = {https://arxiv.org/abs/1506.02640v5},
    isbn = {9781467388504},
    doi = {10.1109/CVPR.2016.91},
    issn = {10636919},
    arxivId = {1506.02640}
}