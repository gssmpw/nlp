@InProceedings{10.1007/978-3-031-20059-5_1,
author="Boecking, Benedikt
and Usuyama, Naoto
and Bannur, Shruthi
and Castro, Daniel C.
and Schwaighofer, Anton
and Hyland, Stephanie
and Wetscherek, Maria
and Naumann, Tristan
and Nori, Aditya
and Alvarez-Valle, Javier
and Poon, Hoifung
and Oktay, Ozan",
editor="Avidan, Shai
and Brostow, Gabriel
and Ciss{\'e}, Moustapha
and Farinella, Giovanni Maria
and Hassner, Tal",
title="Making the Most of Text Semantics to Improve Biomedical Vision--Language Processing",
booktitle="ECCV",
year="2022",
publisher="Springer Nature Switzerland",
address="Cham",
pages="1--21",
abstract="Multi-modal data abounds in biomedicine, such as radiology images and reports. Interpreting this data at scale is essential for improving clinical care and accelerating clinical research. Biomedical text with its complex semantics poses additional challenges in vision--language modelling compared to the general domain, and previous work has used insufficiently adapted models that lack domain-specific language understanding. In this paper, we show that principled textual semantic modelling can substantially improve contrastive learning in self-supervised vision--language processing. We release a language model that achieves state-of-the-art results in radiology natural language inference through its improved vocabulary and novel language pretraining objective leveraging semantics and discourse characteristics in radiology reports. Further, we propose a self-supervised joint vision--language approach with a focus on better text modelling. It establishes new state of the art results on a wide range of publicly available benchmarks, in part by leveraging our new domain-specific language model. We release a new dataset with locally-aligned phrase grounding annotations by radiologists to facilitate the study of complex semantic modelling in biomedical vision--language processing. A broad evaluation, including on this new dataset, shows that our contrastive learning approach, aided by textual-semantic modelling, outperforms prior methods in segmentation tasks, despite only using a global-alignment objective.",
isbn="978-3-031-20059-5"
}

@InProceedings{10.1007/978-3-031-43990-2_35,
author="Chen, Zhihao
and Zhou, Yang
and Tran, Anh
and Zhao, Junting
and Wan, Liang
and Ooi, Gideon Su Kai
and Cheng, Lionel Tim-Ee
and Thng, Choon Hua
and Xu, Xinxing
and Liu, Yong
and Fu, Huazhu",
editor="Greenspan, Hayit
and Madabhushi, Anant
and Mousavi, Parvin
and Salcudean, Septimiu
and Duncan, James
and Syeda-Mahmood, Tanveer
and Taylor, Russell",
title="Medical Phrase Grounding with Region-Phrase Context Contrastive Alignment",
booktitle="MICCAI",
year="2023",
publisher="Springer Nature Switzerland",
address="Cham",
pages="371--381",
abstract="Medical phrase grounding (MPG) aims to locate the most relevant region in a medical image, given a phrase query describing certain medical findings, which is an important task for medical image analysis and radiological diagnosis. However, existing visual grounding methods rely on general visual features for identifying objects in natural images and are not capable of capturing the subtle and specialized features of medical findings, leading to a sub-optimal performance in MPG. In this paper, we propose MedRPG, an end-to-end approach for MPG. MedRPG is built on a lightweight vision-language transformer encoder and directly predicts the box coordinates of mentioned medical findings, which can be trained with limited medical data, making it a valuable tool in medical image analysis. To enable MedRPG to locate nuanced medical findings with better region-phrase correspondences, we further propose Tri-attention Context contrastive alignment (TaCo). TaCo seeks context alignment to pull both the features and attention outputs of relevant region-phrase pairs close together while pushing those of irrelevant regions far away. This ensures that the final box prediction depends more on its finding-specific regions and phrases. Experimental results on three MPG datasets demonstrate that our MedRPG outperforms state-of-the-art visual grounding approaches by a large margin. Additionally, the proposed TaCo strategy is effective in enhancing finding localization ability and reducing spurious region-phrase correlations.",
isbn="978-3-031-43990-2"
}

@INPROCEEDINGS{9710016,
  author={Deng, Jiajun and Yang, Zhengyuan and Chen, Tianlang and Zhou, Wengang and Li, Houqiang},
  booktitle={ICCV}, 
  title={TransVG: End-to-End Visual Grounding with Transformers}, 
  year={2021},
  volume={},
  number={},
  pages={1749-1759},
  keywords={Visualization;Computer vision;Codes;Grounding;Manuals;Transformers;Cognition;Vision + language;Vision + other modalities},
  doi={10.1109/ICCV48922.2021.00179}}

@INPROCEEDINGS{9710994,
  author={Kamath, Aishwarya and Singh, Mannat and LeCun, Yann and Synnaeve, Gabriel and Misra, Ishan and Carion, Nicolas},
  booktitle={ICCV}, 
  title={MDETR - Modulated Detection for End-to-End Multi-Modal Understanding}, 
  year={2021},
  volume={},
  number={},
  pages={1760-1770},
  keywords={Visualization;Vocabulary;Image segmentation;Computer vision;Grounding;Detectors;Computer architecture;Vision + language;Detection and localization in 2D and 3D;Machine learning architectures and formulations;Visual reasoning and logical representation},
  doi={10.1109/ICCV48922.2021.00180}}

@INPROCEEDINGS{9879567,
  author={Li, Liunian Harold and Zhang, Pengchuan and Zhang, Haotian and Yang, Jianwei and Li, Chunyuan and Zhong, Yiwu and Wang, Lijuan and Yuan, Lu and Zhang, Lei and Hwang, Jenq-Neng and Chang, Kai-Wei and Gao, Jianfeng},
  booktitle={CVPR}, 
  title={Grounded Language-Image Pre-training}, 
  year={2022},
  volume={},
  number={},
  pages={10955-10965},
  keywords={Visualization;Computer vision;Image recognition;Head;Grounding;Object detection;Data models;Deep learning architectures and techniques; Recognition: detection;categorization;retrieval; Representation learning; Transfer/low-shot/long-tail learning; Vision + language},
  doi={10.1109/CVPR52688.2022.01069}}

@inproceedings{agu_anaxnet_2021,
	address = {Cham},
	title = {{AnaXNet}: {Anatomy} {Aware} {Multi}-label {Finding} {Classification} in {Chest} {X}-{Ray}},
	isbn = {978-3-030-87240-3},
	abstract = {Radiologists usually observe anatomical regions of chest X-ray images as well as the overall image before making a decision. However, most existing deep learning models only look at the entire X-ray image for classification, failing to utilize important anatomical information. In this paper, we propose a novel multi-label chest X-ray classification model that accurately classifies the image finding and also localizes the findings to their correct anatomical regions. Specifically, our model consists of two modules, the detection module and the anatomical dependency module. The latter utilizes graph convolutional networks, which enable our model to learn not only the label dependency but also the relationship between the anatomical regions in the chest X-ray. We further utilize a method to efficiently create an adjacency matrix for the anatomical regions using the correlation of the label across the different regions. Detailed experiments and analysis of our results show the effectiveness of our method when compared to the current state-of-the-art multi-label chest X-ray image classification methods while also providing accurate location information.},
	booktitle = {MICCAI},
	publisher = {Springer International Publishing},
	author = {Agu, Nkechinyere N. and Wu, Joy T. and Chao, Hanqing and Lourentzou, Ismini and Sharma, Arjun and Moradi, Mehdi and Yan, Pingkun and Hendler, James},
	editor = {de Bruijne, Marleen and Cattin, Philippe C. and Cotin, Stéphane and Padoy, Nicolas and Speidel, Stefanie and Zheng, Yefeng and Essert, Caroline},
	year = {2021},
	pages = {804--813},
}

@inproceedings{muller_anatomy-driven_2023,
	address = {Cham},
	title = {Anatomy-{Driven} {Pathology} {Detection} on {Chest} {X}-rays},
	isbn = {978-3-031-43907-0},
	abstract = {Pathology detection and delineation enables the automatic interpretation of medical scans such as chest X-rays while providing a high level of explainability to support radiologists in making informed decisions. However, annotating pathology bounding boxes is a time-consuming task such that large public datasets for this purpose are scarce. Current approaches thus use weakly supervised object detection to learn the (rough) localization of pathologies from image-level annotations, which is however limited in performance due to the lack of bounding box supervision. We therefore propose anatomy-driven pathology detection (ADPD), which uses easy-to-annotate bounding boxes of anatomical regions as proxies for pathologies. We study two training approaches: supervised training using anatomy-level pathology labels and multiple instance learning (MIL) with image-level pathology labels. Our results show that our anatomy-level training approach outperforms weakly supervised methods and fully supervised detection with limited training samples, and our MIL approach is competitive with both baseline approaches, therefore demonstrating the potential of our approach.},
	booktitle = {MICCAI},
	publisher = {Springer Nature Switzerland},
	author = {Müller, Philip and Meissen, Felix and Brandt, Johannes and Kaissis, Georgios and Rueckert, Daniel},
	editor = {Greenspan, Hayit and Madabhushi, Anant and Mousavi, Parvin and Salcudean, Septimiu and Duncan, James and Syeda-Mahmood, Tanveer and Taylor, Russell},
	year = {2023},
	pages = {57--66},
}

@InProceedings{pmlr-v139-radford21a,
  title = 	 {Learning Transferable Visual Models From Natural Language Supervision},
  author =       {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  booktitle = 	 {ICML},
  pages = 	 {8748--8763},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/radford21a/radford21a.pdf},

  abstract = 	 {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.}
}

