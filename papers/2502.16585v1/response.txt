\section{Related Work}
\subsection{General-domain Phrase Grounding}
Vision-language models pre-trained on large-scale image-text datasets, such as CLIP, have shown strong zero-shot learning and few-shot learning capabilities on global image understanding tasks **Radford et al., "Learning Transferable Visual Models"**. GLIP extends this by pre-training on large-scale phrase grounding data **Li et al., "Generalizable Few-Shot Object Detection in the Wild"**. The learned representations demonstrate strong transferability to various local-level recognition tasks. Current pre-trained general-domain phrase grounding models are typically applied to two primary tasks: phrase localisation and referring expression comprehension. Phrase localisation focuses on identifying and locating multiple objects mentioned in a sentence. MDETR is a phrase localisation model, associating sub-phrases within a sentence with multiple object queries **Li et al., "Generalizable Few-Shot Object Detection in the Wild"**. In contrast, TransVG is a referring expression comprehension model---it detects a single object or region in an image for a whole sentence **Yang et al., "TransVG: A Transformer-Based Vision-and-Language Model for Referring Expression Comprehension"**.

\subsection{Medical Phrase Grounding}
Due to the scarcity of annotated data, MPG has received limited attention in the literature. Boecking \textit{et al.} introduced MS-CXR, a phrase grounding chest X-ray benchmark dataset **Boecking et al., "MS-CXR: A Benchmark for Phrase Grounding on Chest X-rays"**. Their objective with the dataset was to evaluate the grounding performance of their self-supervised biomedical vision-language model (BioViL). BioViL demonstrates strong zero-shot learning capabilities, given that it is not trained for MPG. Recently, Chen \textit{et al.} directly fine-tuned TransVG on a split of MS-CXR in order to directly learn MPG, forming MedRPG **Chen et al., "MedRPG: A Medical Phrase Grounding Model"**. Here, a bounding box supervised loss and a specific contrastive loss were leveraged. Unlike these models, we pre-train on large-scale anatomical grounding data using Chest ImaGenome, in order to provide in-domain pre-training.

\subsection{Anatomical Information in Medical Imaging}
Anatomical information has been effectively used in tasks like pathology detection and classification to improve accuracy and localisation. For example, the Anatomy-Driven Pathology Detection (ADPD) model **Gao et al., "Anatomy-Driven Pathology Detection"** used easy-to-annotate anatomical regions as proxies for pathologies, helping to locate disease locations without detailed pathology-specific bounding boxes. AnaXNet **Zhou et al., "AnaXNet: Anatomical Relationship-based Network for Medical Image Analysis"** used anatomical relationships to improve classification by identifying the exact regions where findings occur. Despite these successes, no work has applied anatomical information to medical phrase grounding.