\section{Related Work}
\label{sec:work_relate}


\paragraph{VLM Adaption} VLMs jointly pretrain image-language representations in an unsupervised manner that enables the utilization of an immense amount of data. For example, CLIP ____ and ALIGN ____ use \textasciitilde400M and \textasciitilde1B image-text pairs in the pretraining. VLMs exhibit good generalization behavior ____. The following researches explore the potential of pre-trained VLMs in challenging downstream tasks such as semantic segmentation ____ visual question answering ____, image retrieval ____, and visual grounding ____.

\paragraph{Prompt Learning} In the vision-language field, the main focus is on continuous prompts. The pioneering work for the text prompt is CoOp ____ which replaces embeddings of the CLIP text template with continuous prompts. The pioneering work for the visual prompt is VPT which prepends continuous prompts to image patch embedding. There are emerging researches trying to boost the performance of continuous prompts. Based on CoOp work, CoCoOp ____ adds special tokens conditioned on image input. MaPLe ____ extends the coupling effect between inserted continuous prompts for language branch and that for visual branch to more transformer layers by using deep prompts. ProGrad ____ and PromptSRC ____ use knowledge distillation (KD) that has pre-trained CLIP as teacher's model.