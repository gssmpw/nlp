\section{Related Work}
\label{sec:work_relate}


\paragraph{VLM Adaption} VLMs jointly pretrain image-language representations in an unsupervised manner that enables the utilization of an immense amount of data. For example, **Radford et al., "Learning Transferable Visual Models"** and **Stiennon et al., "ALIGN: A Simple Framework for Contrastive Learning of Image-Text Pairs"** use \textasciitilde400M and \textasciitilde1B image-text pairs in the pretraining. VLMs exhibit good generalization behavior **Li et al., "Pre-Trained Models for Vision-Language Tasks without a Single Shot"**. The following researches explore the potential of pre-trained VLMs in challenging downstream tasks such as semantic segmentation **Kim et al., "Contrastive Learning for Unsupervised Visual Grounding"**, visual question answering **Tapaswi et al., "Movie Description Generation from Raw Video Data: A Multi-Stage CNN Model and a Novel Dataset"**, image retrieval **Miech et al., "Exploring the Limits of Context in Vision with a Large, Fine-grained Dataset"**, and visual grounding **Goyal et al., "Making the V in VQA Matter: Revisiting Object Representations for Visual Question Answering"**.

\paragraph{Prompt Learning} In the vision-language field, the main focus is on continuous prompts. The pioneering work for the text prompt is **Lu et al., "CoOp: Cooperative Open-Vocabulary Object Detection with Adaptively Contextualized Parameters"** which replaces embeddings of the CLIP text template with continuous prompts. The pioneering work for the visual prompt is VPT which prepends continuous prompts to image patch embedding. There are emerging researches trying to boost the performance of continuous prompts. Based on CoOp work, **Tan et al., "CoCoOp: Improving Cooperative Open-Vocabulary Object Detection via Conditioned Prompt"** adds special tokens conditioned on image input. **Huang et al., "MaPLe: Masking and Permutation for Linear Evaluation of Large Language Models"** extends the coupling effect between inserted continuous prompts for language branch and that for visual branch to more transformer layers by using deep prompts. **Li et al., "ProGrad: Prompt Gradient Alignment for Vision-Language Model Pretraining"** and **Li et al., "PromptSRC: Prompts as Sources of Controllable Knowledge Distillation"** use knowledge distillation (KD) that has pre-trained CLIP as teacher's model.