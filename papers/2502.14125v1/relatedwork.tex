\section{Related Work}
\label{sec:work_relate}


\paragraph{VLM Adaption} VLMs jointly pretrain image-language representations in an unsupervised manner that enables the utilization of an immense amount of data. For example, CLIP \cite{radford2021learning} and ALIGN \cite{jia2021scaling} use \textasciitilde400M and \textasciitilde1B image-text pairs in the pretraining. VLMs exhibit good generalization behavior \cite{radford2021learning,jia2021scaling,yao2021filip,li2021supervision,singh2022flava}. The following researches explore the potential of pre-trained VLMs in challenging downstream tasks such as semantic segmentation \cite{rao2022denseclip} visual question answering \cite{tan2019lxmert}, image retrieval \cite{lu2019vilbert}, and visual grounding \cite{yao2024cpt}.

\paragraph{Prompt Learning} In the vision-language field, the main focus is on continuous prompts. The pioneering work for the text prompt is CoOp \cite{zhou2022learning} which replaces embeddings of the CLIP text template with continuous prompts. The pioneering work for the visual prompt is VPT which prepends continuous prompts to image patch embedding. There are emerging researches trying to boost the performance of continuous prompts. Based on CoOp work, CoCoOp \cite{zhou2022conditional} adds special tokens conditioned on image input. MaPLe \cite{khattak2023maple} extends the coupling effect between inserted continuous prompts for language branch and that for visual branch to more transformer layers by using deep prompts. ProGrad \cite{zhu2023prompt} and PromptSRC \cite{khattak2023self} use knowledge distillation (KD) that has pre-trained CLIP as teacher's model.