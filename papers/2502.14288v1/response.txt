\section{Related Work}
\label{sec: related work}
In this section, we review works closely related to our approach, which falls into three categories: GUI understanding and GUI grouping, research on GUI accessibility, and GUI accessibility test.
From these three categories of works, we delve into the current research efforts in the direction of GUI accessibility and further analyze the limitations that serve to motivate our work.
\vspace{-0.42em}

\subsection{GUI understanding and GUI grouping}
The upstream work related to the understanding and grouping of GUIs can provide a solid foundation for accessibility testing of GUIs. 
This is also a primary avenue for researchers to explore various types of information within GUIs. 

From the perspective of GUI understanding, Liu et al., "OwlEye: A Visual Understanding System for UI Display Issues", developed OwlEye to analyze and understand UI display issues (e.g., text overlap, missing image, blurred screen) via visual understanding.
Also, Liu et al., "Nighthawk: Visual Inconsistency Detection in GUIs", proposed Nighthawk, which aimed to understand the elements of GUI from a visual perspective, and detected and located visual inconsistencies in GUI design.
Zhang et al., "UniRLTest: A Unified Approach for Understanding GUI Images", shifted the focus of understanding towards the GUI images themselves, proposing UniRLTest. 
This approach utilizes CV (Computer Vision) techniques to capture all widgets within GUI images and constructs a widget tree for each GUI, thereby facilitating independent testing of the GUI.
To group the fragmented elements in UI designs, Chen et al., "EGFE: An End-to-End Approach for Grouping Fragmented UI Elements", presented EGFE, an approach that leverages multimodal feature representation and a Transformer encoder to group fragmented UI elements. 
It addresses the detection and grouping of tiny GUI components, achieving high-quality front-end code generation.
Further, Xie et al., "Perceptual Grouping of GUI Widgets using Gestalt Principles", explored the use of Gestalt psychology principles for perceptual grouping of GUI widgets. 
It identifies visual patterns and groups related elements based on the laws of connectedness, similarity, proximity, and continuity.

The aforementioned strategies of understanding GUI and grouping GUI elements enhance the perceptual clarity of GUI designs. 
They also provide a more reasonable analytical approach for subsequent accessibility testing.


\subsection{Research on GUI accessibility}

The accessibility of GUIs determines whether visually impaired users can interact effectively with them.
However, existing apps often face challenges in addressing accessibility due to constraints in their requirements and developers' oversight. 
This lack of consideration for accessibility issues poses significant difficulties for visually impaired users. 
Current research systematically examined and analyzed the types and quantity of accessibility issues within apps, highlighting the need for developers to incorporate accessibility considerations into GUI design to enhance the overall user experience. 

In this context, Alshayban et al., "Characterizing Accessibility Issues in 1,000 Android Apps", analyzed the prevalence of accessibility issues in over 1,000 Android apps, and found that almost all apps are riddled with accessibility issues, hindering their use by disabled people. 
A similar investigation was also conducted by Chen et al, "Accessibility Analysis and Repair of Existing Apps", which found that there are a large number of accessibility issues in existing apps and their repair processes need to be strengthened.
Further, Rodrigues et al., "Challenges Faced by Blind Users in Their Daily Lives Using Smartphones", from the perspectives of newcomers, novices, and experts, summarized 13 challenges (e.g., performing a specific touchscreen gesture is hard) faced by blind users in their daily lives using smartphones.
Research in this direction also includes the large-scale epidemiology-inspired study by Ross et al., "Epidemiology-Inspired Study on Android Accessibility", which characterizes the current state of Android accessibility, suggests improvements to the app ecosystem, and demonstrates analysis techniques that can be applied in further app accessibility assessments.
Further, in our prior work, Wang et al., "Relationship Prediction for Low-Vision Users", we analyzed the common types of accessibility issues faced by low-vision users, providing relevant background support for this work. 
Our previous research focused more on the relationships between components and used a relationship prediction task to suggest how to adjust the components that can satisfy the needs of low-vision users.
In contrast, the study in this paper distinguishes itself from our previous research by relying on various attributes of GUI components to perform a multi-classification task aimed at detecting and identifying accessibility issues within GUIs.
Overall, the objectives of the two approaches differ fundamentally, while previous work has provided significant data support and theoretical backing for the study in this paper.

In summary, researchers have conducted a series of empirical explorations, summarizing and discovering a significant number of accessibility issues within existing apps. 
Developers often overlook these problems due to constraints related to requirements and functionalities. 
This phenomenon underscores the necessity of an effective method to detect these issues, and further guide developers to fix them. 

\vspace{-0.42em}
\subsection{GUI accessibility test}
Accessibility testing, as a crucial branch of GUI testing, has witnessed the development of various tools and methods in both industry and academia.

The first work related to this research dates back to 1995, Richard et al., "UnWindows VI: A Tool for Low Vision Users", firstly presented UnWindows VI, a set of tools designed to assist low vision users of X Windows in effectively accomplishing two mundane yet critical interaction tasks.
Based on the image recognition method, Salehnamadi et al. , "Latte: A GUI Accessibility Checker Using Image Recognition", proposed a GUI accessibility checker named Latte by intercepting a large number of user-operated GUI instances and analyzing them with the Accessibility Service.
Chen et al., "Xbot: An Automated Accessibility Test Tool for GUIs", proposed Xbot to check the accessibility issues in the GUIs, and made a better performance on collecting accessibility issues.
Besides, there are also many other tools, such as Google Accessibility Scanner , Mobile Accessibility Checker (MAC) , PUMA , Automated Accessibility Test Tool (AATT) , and MATE .
After that, industry and academia have also proposed many tools after analyzing the GUIs and low vision users, such as component recommendation , sensitive widget identification , layout optimization , and so forth.
To further improve the accessibility of apps, Chen et al., "Labeldroid: An Automatic Label Prediction System for Image-Based Buttons", developed Labeldroid to automatically predict the labels of image-based buttons.   
Still, Labeldroid lacks considering the context of images, causing the labels generated to be incorrect.
To further improve the accuracy of predicted labels, Mehralian et al., "Context-Aware Label Generation for Image-Based Buttons", proposed a more accurate label generation method utilizing the context information of images.
Also, Zhang et al., "Interaction Proxies: A Strategy for Runtime Repair and Enhancement of Accessibility in Mobile Apps", introduced interaction proxies as a strategy for runtime repair and enhancement of the accessibility of mobile applications.


In a comprehensive view, the aforementioned methods and tools offered detection methods for accessibility issues in GUI from various aspects. 
However, they share a limitation in that they are all rule-based. 
As illustrated in the specific examples analyzed in Section~\ref{sec: introduction}, rule-based methods often flag invisible redundant information, struggle with handling minor deviations effectively, overlook similar components, and face challenges in scalability. 
Therefore, there is an urgent need for a unified and accurate approach to detect GUI accessibility issues, providing more precise assistance to developers in identifying these problems. 
In essence, this is also a primary objective of our work.


\begin{figure*}
\centering
\includegraphics[width=14cm]{Figure/WorkFlow.png}
\caption{The workflow of ALVIN.}
\label{fig: architecture}
\end{figure*}