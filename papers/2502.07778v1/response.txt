\section{Related Work}
Training-based methods create effective fake image detectors. Goodfellow et al., "Generative Adversarial Networks" demonstrated that data augmentations during training improve generalization. Moons, "Data Augmentation for Generative Models" demonstrated identifiable artifacts, like checkerboard patterns, in the Fourier transforms of fake images. Building on this, Guo et al., "Frequency-Aware Fake Image Detection" trained on Fourier images, leading to improvements. Zhang et al., "Patch-Based Classification for Fake Images" improved detection using patch-based classification. Wang et al., "Fake Image Detection Using Patch-Wise Training" removed downsampling in the initial layers and applied patch-wise training for further gains. These techniques were extended to the LDM setting by Lin et al., "LDM-Based Fake Image Detection". Additionally, works such as Liu et al., "Generative Models with Real Data Reconstructions" and Zhang et al., "Fake Image Detection Using Real Data Reconstructions" also use reconstructions of real images as part of the training data. These approaches struggle to generalize across architectures. To address this, Radford et al., "Learning to Detect Fake Images with CLIP" proposed using general-purpose visual encoders like CLIP for fake image detection. Unlike ours, these methods explicitly rely on real features.

Contrary to prior approaches, GenDet ____ treats fake image detection as an anomaly detection problem, similar to ours, but focuses on learning the real image distribution. Another paradigm suggests generators reconstruct fake images more easily than real ones. For reconstruction, Karras et al., "Alias-Free Generative Adversarial Networks" use GAN inversion ____, DALL-E ____ applies DDIM inversion ____, and AEROBLADE ____ leverages a latent diffusion autoencoder. However, these methods struggle with detecting post-processed images ____. A contrasting approach, akin to image reconstruction, was recently proposed by Chen et al., "Neural Image-Compression Networks" who use neural image-compression networks ____ to model real distribution likelihood, assuming fake images are less likely to be part of it. While this method models the real distribution, we argue the focus should be on detecting fake image artifacts instead.
\begin{figure}[t]
\begin{center}
\centerline{\includegraphics[height=1.5in]{icml2025/figures/res/fakespur.png}}

\caption{\textbf{Vulnerability to Spurious Fake Features.} Our method \emph{Corvi$\oplus$} is not able to mitigate spurious correlations pertaining to the fake distribution, where just like the original \emph{Corvi}, it continues to associate upsampled images with the fake distribution.}
\label{fig:corvi-fakespur}
\end{center}
\vskip -0.28in
\end{figure}