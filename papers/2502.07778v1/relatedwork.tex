\section{Related Work}
Training-based methods create effective fake image detectors. \citet{wang2020cnn} demonstrated that data augmentations during training improve generalization. \citet{odena2016deconvolution} demonstrated identifiable artifacts, like checkerboard patterns, in the Fourier transforms of fake images. Building on this, \citet{zhang2019detecting} trained on Fourier images, leading to improvements. \citet{chai2020makesfakeimagesdetectable} improved detection using patch-based classification. \citet{gragnaniello2021gan} removed downsampling in the initial layers and applied patch-wise training for further gains. These techniques were extended to the LDM setting by \citet{corvi2023detection}. Additionally, works such as \citet{chendrct} and \citet{rajan2024effectiveness} also use reconstructions of real images as part of the training data. These approaches struggle to generalize across architectures. To address this, \citet{ojha2023towards} proposed using general-purpose visual encoders like CLIP \citep{radford2021learning} for fake image detection. Unlike ours, these methods explicitly rely on real features.


Contrary to prior approaches, GenDet \citep{zhu2023gendet} treats fake image detection as an anomaly detection problem, similar to ours, but focuses on learning the real image distribution. Another paradigm suggests generators reconstruct fake images more easily than real ones. For reconstruction, \citet{pasquini2023identifying} use GAN inversion \citep{xia2022gan}, DIRE \citep{wang2023dire} applies DDIM inversion \citep{song2020denoising}, and AEROBLADE \citep{ricker2024aeroblade} leverages a latent diffusion autoencoder. However, these methods struggle with detecting post-processed images \citep{rajan2024effectiveness}. A contrasting approach, akin to image reconstruction, was recently proposed by \citet{cozzolino2025zero}, who use neural image-compression networks \citep{cao2020lossless} to model real distribution likelihood, assuming fake images are less likely to be part of it. While this method models the real distribution, we argue the focus should be on detecting fake image artifacts instead.
\begin{figure}[t]
\begin{center}
\centerline{\includegraphics[height=1.5in]{icml2025/figures/res/fakespur.png}}

\caption{\textbf{Vulnerability to Spurious Fake Features.} Our method \emph{Corvi$\oplus$} is not able to mitigate spurious correlations pertaining to the fake distribution, where just like the original \emph{Corvi}, it continues to associate upsampled images with the fake distribution.}
\label{fig:corvi-fakespur}
\end{center}
\vskip -0.28in
\end{figure}