%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{float}
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\newcommand{\yj}[1]{{\color{cyan}\textbf{YJ:} #1}}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Stay-Positive: A Case for Ignoring Real Image Features in Fake Image Detection}

\begin{document}

\twocolumn[
\icmltitle{Stay-Positive: A Case for Ignoring Real Image Features in Fake Image Detection}
%Improved Detection of Fake Images by Ignoring Real Image Features

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Anirudh Sundara Rajan}{yyy}
\icmlauthor{Yong Jae Lee}{yyy}

%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{University of Wisconsin-Madison}
% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Anirudh Sundara Rajan}{asundararaj2@wisc.edu}


% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{} % otherwise use the standard text.

\begin{abstract}
Detecting AI-generated images is a challenging yet essential task. A primary difficulty arises from the detector’s tendency to rely on spurious patterns, such as compression artifacts, which can influence its decisions. These issues often stem from specific patterns that the detector associates with the real data distribution, making it difficult to isolate the actual generative traces. We argue that an image should be classified as fake if and only if it contains artifacts introduced by the generative model. Based on this premise, we propose Stay-Positive, an algorithm designed to constrain the detector’s focus to generative artifacts while disregarding those associated with real data. Experimental results demonstrate that detectors trained with Stay-Positive exhibit reduced susceptibility to spurious correlations, leading to improved generalization and robustness to post-processing. Additionally, unlike detectors that associate artifacts with real images, those that focus purely on fake artifacts are better at detecting inpainted real images.



\end{abstract}

\section{Introduction}
\label{submission}
Recent advancements in deep generative modeling, particularly diffusion models \citep{SohlDickstein2015DeepUL, song2019generative} and flow-based models \citep{liu2022flow, lipman2022flow}, have significantly improved image generation capabilities. The combination of large-scale datasets \citep{schuhmann2022laion} and architectural innovations \citep{dhariwal2021diffusion, esser2024scaling} has enabled modern systems to generate high-quality images \citep{pernias2023wuerstchen, flux, podell2023sdxl}, often conditioned on auxiliary inputs such as text. While these advancements have expanded the potential applications of generative models, they also raise concerns about misuse, including misinformation and fraud. Image forensics aims to address these risks by developing methods to reliably detect AI-generated images.

Significant progress has been made in fake image detection, with detectors trained to generalize across CNN-based generators \citep{wang2020cnn} and extended to unseen architectures \citep{ojha2023towards}. However, reliably detecting fake images from a known generator family remains a challenge. A key difficulty arises when images undergo post-processing (e.g., compression), as demonstrated by \citet{rajan2024effectiveness}, where detectors trained on images generated by latent diffusion models \citep{vahdat2021score, rombach2022high} exhibited reduced effectiveness in detecting post-processed images. Specifically, detectors trained by \citet{corvi2023detection} and \citet{rajan2024effectiveness} were found to spuriously associate WEBP compression artifacts with real images, which hurts their performance. In this work, we focus on improving the detection of fake images generated by a known model family by mitigating the impact of spurious correlations.




% Detecting images coming from a known generative model is generally considered an easy task. Universal fake image detection is the problem of detecting images from a variety of generative models, including ones which are unseen during training. Initially, \citet{wang2020cnn} demonstrated that by finetuning a ResNet-50 \citep{he2016deep} on ProGAN \citep{karras2018progressivegrowinggansimproved} generated images, one could detect images from a variety of CNN-based generative models. Improvements in generalization were observed when \citet{ojha2023towards} demonstrated the effectiveness of using a frozen CLIP image encoder \citep{radford2021learning} in detecting fake images. However, recent work by \citet{rajan2024effectiveness} demonstrated that the reliable detection of images generated by the known family of latent diffusion models \citep{vahdat2021score,rombach2022high} has not been adequately achieved. This study highlights the tendency of existing detectors to rely on spurious correlations, which poses a significant obstacle to developing robust detectors for known generative models. In this work, we deal with the problem of detecting images coming from a known family of generative models.

The most effective approach for developing a fake image detector is to train a neural network-based binary classifier. However, its performance is highly dependent on the training data, and any discriminative feature linked to it, including subtle post-processing artifacts, can spuriously influence the detector’s decisions. A common cause of such issues is the real images used for training, which are often sourced from online platforms and may have undergone unknown operations such as compression and resizing before being uploaded. As a result, detectors can learn spurious correlations. For example, models trained by \citet{corvi2023detection} and \citet{rajan2024effectiveness} exhibit difficulties in handling WEBP-compressed images. Similar issues with compression artifacts were observed in the GenImage benchmark \citep{zhu2024genimage}, as reported by \citet{grommelt2024fake}.

In addition to robustness against post-processing, it is essential for detectors to accurately identify images generated by newer models within the same generator family. We observe that reliance on spurious correlations hinders generalization to such models. During training, detectors often focus on differences in image quality between real and fake images, leading them to associate certain artifacts with real images. However, such hypotheses are often incorrect, as the same artifacts can appear in fake images generated by improved models within the same family. This reliance on such artifacts limits the detector's ability to generalize to newer generators. For instance, a detector trained on images generated by Latent Diffusion Models (LDM) \citep{rombach2022high} \citep{corvi2023detection} struggles to generalize to images generated by FLUX \citep{flux} for this very reason.
% Indeed, we show that the fake detectors developed by \citet{corvi2023detection} and \citet{rajan2024effectiveness} suffer from spurious correlations based on post-processing. Similar spurious correlations due to compression artifacts have been identified in the widely used GenImage benchmark  \citep{zhu2024genimage} by \citet{grommelt2024fake}. Furthermore, the features the detector associates with the real distribution depend on the quality of fake images in the training data. However, these features do not inherently define an image as real. We observe that while artifacts in certain fake images are detected, they are often counterbalanced by features associated with the real distribution, leading to the detector failing to classify these images correctly. 

Our main idea is that images generated by a specific generator family should contain distinct artifacts. An ideal detector should focus exclusively on these fake artifacts, with their absence indicating that the image does not originate from a generator of that family. Relying on artifacts associated with real images is, at best, unnecessary and, at worst, misleading. We show detectors perform better when real image features do not affect decisions.

To identify features linked to real images, we make the following assumptions: the output of the entire network passes through a ReLU activation before the final layer, and the final output is passed through a sigmoid activation for binary classification, where class 1 represents fake images and class 0 represents real images. Under these conditions, we show that features connected to negative weights in the final layer correspond to patterns found in real images. To mitigate the influence of these features, we propose a simple yet highly effective algorithm that retrains the last layer to minimize the loss while ensuring that the weights remain strictly non-negative. This adjustment forces the detector to make its decision using only fake image patterns, ignoring spurious real image artifacts. As a result, the detector shows improved generalization. This approach also enables our detectors to effectively detect partially inpainted real images, unlike conventional detectors, which may be influenced by real-image features. We validate our method by improving the performance of detectors from \citet{corvi2023detection} and \citet{rajan2024effectiveness}. Ultimately, we hope our findings contribute to the community’s broader efforts to combat misinformation.

% We observe that existing fake image detectors can effectively learn what makes an image fake, as noted by \citet{kirichenko2022last}. Based on this, we propose a simple strategy: re-train the final linear layer of a pre-trained detector to reduce the loss without relying on real features. For networks with ReLU activations before the final layer, the positive weights correspond to features linked to the fake distribution (when fake images are labeled as the positive class). We re-train the last layer to ensure only positive weights are retained. 

% Since our method forces existing detectors to ignore patterns in the real data distribution, we are able to detect inpainted real images as opposed to detectors trained in a conventional manner. 

\section{Background}
In this section, we first define the problem, describe the general details of training a fake image detector, and introduce the notation used throughout the paper.

\subsection{Problem Definition}

The task of \emph{fake image detection} is a binary classification problem. Given a dataset $\mathcal{D} = \{(\mathbf{x}_i, y_i)\}_{i=1}^N$ of $N$ labeled samples, where each $\mathbf{x}_i \in \mathbb{R}^{H \times W \times C}$ represents an image of height $H$, width $W$, and $C$ channels, and $y_i \in \{0, 1\}$ is the corresponding label, the goal is to learn a mapping $f: \mathbb{R}^{H \times W \times C} \rightarrow \{0, 1\}$. The label $y_i = 0$ denotes a \textit{real} image, while $y_i = 1$ indicates a \textit{fake} image generated by a neural network.

\subsection{Learning-Based Fake Image Detection}\label{sec:lbid}
Given a known generative model family of interest, the well-established method of training a fake image detector involves the following steps. First, a set of real images, usually sourced online, is selected, while fake images are typically generated using the targeted generative model. A neural network detector \( f_\theta \) is then trained to solve the binary classification task. 

In our work, we assume the following neural network structure for \( f_\theta \). The network is composed of three components: (i) a feature extraction network, \( g_\phi: \mathbb{R}^{H \times W \times C} \to \mathbb{R}^d \), which encodes the input image \( \mathbf{x} \) into a \( d \)-dimensional feature vector, where \( \phi \) represents the parameters of the feature extraction network; (ii) a ReLU activation function, applied element-wise to the extracted features, denoted by \( \gamma \); and (iii) a linear classifier, parameterized by \( \mathbf{w} \in \mathbb{R}^d \) and \( b \in \mathbb{R} \), followed by a sigmoid activation function. The parameters of the network are \( \theta = \{\phi, \mathbf{w}, b\} \).

Formally, the output of \( f_\theta \) is given by  
\[
f_\theta(\mathbf{x}) = \sigma(\mathbf{w}^\top \gamma(g_\phi(\mathbf{x})) + b),
\]  
where \( \sigma(\cdot) \) denotes the sigmoid function. This network is trained using the binary cross entropy loss.




\section{Issues with Associating the Presence of Specific Features with Real Images}\label{sec:spur_real}
In this section, we first examine how unknown post-processing artifacts in real training images negatively impact detector performance. We then address the broader issues associated with linking specific artifacts to real images.

\subsection{Case Study 1: Post-Processing Artifacts}\label{subseq:spur-webp}
Detectors can inadvertently let subtle differences between real and fake training distributions, such as spurious features like compression or resizing artifacts, influence their decision. Real images, often sourced from online platforms, may have undergone unknown post-processing, making it difficult to determine which features the detector may associate with real images. We explore this issue below.

In their study on fake image detection for latent diffusion models, \citet{corvi2023detection} and \citet{rajan2024effectiveness} trained a ResNet-50 \citep{he2016deep} using real images from the LSUN \citep{yu2016lsunconstructionlargescaleimage} and COCO \citep{lin2015microsoftcococommonobjects} datasets. The LSUN images were compressed using the WEBP algorithm but saved in the lossless PNG format by the dataset creators. The fake images used to train the detectors did not contain these WEBP artifacts, leading the detectors to associate WEBP compression with the real distribution. We demonstrate this issue in the following experiment.

\textbf{Experiment Details}: We aim to test whether the detector confuses WEBP-compressed fake images with real images, and if this confusion is driven by the inclusion of LSUN images in the real distribution. To do so, we adopt the experimental setup proposed by \citet{rajan2024effectiveness}, where fake images are generated by reconstructing real images through the LDM \citep{rombach2022high} autoencoder. We train two detectors: one that includes LSUN images in the real distribution and another that excludes them, training only on COCO images. For testing, we use 500 real images from the Redcaps dataset \citep{desai2021redcaps} and 500 fake images generated by Stable Diffusion 1.5. Various levels of WEBP compression are applied to the fake images, while the real images remain uncompressed. We evaluate the average precision (AP), where a drop in AP with increasing compression levels would suggest that the detector is confusing WEBP-compressed fake images with real ones.


% , we evaluate the detectors on 500 real images from the Redcaps dataset \citep{desai2021redcaps} and 500 fake images generated by Stable Diffusion 1.5, and report the sensitivity of the detectors to WEBP compression of fake images, through average precision (AP).

\begin{figure}[t]
%\vskip 0.2in
\begin{center}
%\centerline{\includegraphics[width=\columnwidth, height=1.8in]{icml2025/figures/webp/ap-webp.png}}
\centerline{\includegraphics[height=1.5in]{icml2025/figures/webp/ap-webp.png}}
\vskip -0.1in
\caption{\textbf{Sensitivity to WEBP Compression.} Using the LSUN dataset, which contains WEBP compressed images, as part of the real distribution makes the network highly vulnerable to WEBP compression.}
\label{fig:webp-spur}
\end{center}
\vskip -0.2in
\end{figure}

\textbf{Analysis}: The results of the experiment are shown in Fig. \ref{fig:webp-spur}. In the training data, LSUN images are WEBP compressed, while the fake images lack WEBP artifacts. This dataset imbalance leads the detector to associate WEBP compression with real images, as evidenced by the drop in AP as WEBP Compression Quality decreases (i.e., there is more WEBP compression). Excluding the LSUN images mitigates this issue. However, such details will not always be known, making it challenging to filter the data effectively. Thus, an algorithm is needed to eliminate spurious correlations associated with the real distribution.

% Inclusion of the LSUN dataset causes the detector performance to deteriorate with compression. 

\subsection{Case Study 2: Beyond Post-Processing Artifacts}\label{sec:realfakefeats}
We next argue that any pattern the detector associates with the real distribution could be spurious. We begin by explaining our intuition.

\textbf{Hypothesis:} Consider a detector trained on real images versus LDM-generated fake images. Models based on 4-channel autoencoders, like LDM, struggle to reconstruct fine details in real images, such as text, as noted by prior work \citep{dai2023emu}. As a result, the detector may associate the presence of certain fine details with real images. However, this hypothesis does not hold, as \citet{dai2023emu} also show that 16-channel autoencoder-based models, such as FLUX, can reconstruct such details, indicating these features do not determine if an image is real. We highlight a similar issue with \emph{Corvi} \citep{corvi2023detection}.
 
% \citet{dai2023emu} showed that 4-channel latent space autoencoders \citep{rombach2022high, podell2023sdxl} struggle with fine image details like text, whereas 16-channel models mitigate this issue. Intuitively, training a fake image detector on 4-channel autoencoder fake images may lead it to associate fine details with real images. Such a detector, could falsely classify a generated image as real despite finding some of the generators artifacts due to the real features that it learns. We illustrate this issue in detail by studying the case of \emph{Corvi} \citep{corvi2023detection}.

\begin{figure}[t]
\begin{center}
\centerline{\includegraphics[width=0.8\columnwidth]{icml2025/figures/flux_ldm.png}}
\caption{\textbf{Image Quality-Based Spurious Features.} \emph{Corvi} outputs a higher real score for Flux reconstructions compared to LDM reconstructions demonstrating the spurious nature of these real features. Fake Score reduces due to the use of a different generator.}
\label{fig:flux-spur}
\end{center}
\vskip -0.2in
\end{figure}

\begin{figure*}[t]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[height=1.8in]{icml2025/figures/teaser/teaser.png}}
\vskip -0.2in
\caption{Our key idea involves 2 steps. (1) We first train a fake image detector in the standard way without any modifications.  This detector focuses on both real and fake features. (2) We re-train the last layer of the network such that it only focuses on the fake features to make a decision.}
\label{fig:teaser}
\end{center}
\vskip -0.2in
\end{figure*}
%Represents a trained fake image detector which focuses on some real and fake features.



\textbf{Real and Fake Score:} We aim to demonstrate that artifacts the detector associates with real images can also appear in fake images from the same generator family, influencing the detector's decision. To achieve this, we first develop a method to measure the impact of these artifacts on the detector's decision.

Consider a trained fake image detector as described in Section \ref{sec:lbid}. Given an arbitrary fake image $\mathbf{x}$, we define the extracted feature as $\mathbf{h} = \gamma(g_\phi(\mathbf{x})), \quad \mathbf{h} \in \mathbb{R}_{\geq 0}^d$. Where $\gamma$ is the ReLU activation. Our final network output can be written as follows,
\begin{equation}
f_\theta(\mathbf{x}) = \sigma\Big(\underbrace{\sum_{\mathbf{w}_i > 0} \mathbf{w}_i \mathbf{h}_i}_{\text{Increases sum, fakeness}} 
+ \underbrace{\sum_{\mathbf{w}_i < 0} \mathbf{w}_i \mathbf{h}_i}_{\text{Decreases sum, realness}} + b\Big).
\end{equation}

We assume that a fake image $\mathbf{x}$ is associated with the label 1 whereas a real image has the label 0. We also assume that the final score (added with the bias) is passed through a monotonically increasing sigmoid function, therefore we can infer that the final score, represented as \( \mathbf{w}^T \mathbf{h} \), for an ideal detector must be higher for a fake image than for a real image. Our use of the ReLU activation ensures that the extracted feature vector \( \mathbf{h} \) is a vector with only non-negative values. Based on this, for a dimension \( i \), if \( \mathbf{w}_i < 0 \), we can conclude that the weighted contribution \( \mathbf{w}_i \mathbf{h}_i \leq 0 \). This operation reduces the final score, thus making the image more likely to be classified as real. Similarly, if \( \mathbf{w}_i > 0 \), the presence\footnote{\textbf{presence} of a feature refers to $h_i$ having a non-zero value, as opposed to absence where it would have a 0 value.} of this feature $\mathbf{h}_i$ increases the likelihood of the image being classified as fake. Based on this, we can define a score which quantifies the presence of real and fake features in an image. 
% The fake class is associated with the label 1. Since the sigmoid function increases monotonically, we can conclude that the final score, given by \( \mathbf{w}^T \mathbf{h} \), should be higher for a fake image than for a real image. Due to ReLU, the extracted feature \( \mathbf{h} \) is a vector with only non-negative values; therefore, we can assert that negative-valued weights decrease the final score, while positive weights increase it. Based on this, we can define a score for the real and fake distribution.
\[
\text{Real Score} = \sum_{\mathbf{w}_i < 0} \mathbf{w}_i \mathbf{h}_i, \quad 
\text{Fake Score} = \sum_{\mathbf{w}_i > 0} \mathbf{w}_i \mathbf{h}_i.
\]


\textbf{Experiment Details:} 
\emph{Corvi} (trained on LDM-generated images) struggles with detecting FLUX.1-dev \citep{flux} generated images. To investigate the cause, we use 3,000 real images from the COCO dataset, which were also part of \emph{Corvi}'s training. Since \emph{Corvi} was trained on LDM-generated images, we reconstruct these images using the autoencoders of both LDM and FLUX.1-dev. Importantly, both LDM and FLUX reconstructions have the same semantic content, differing only in autoencoder capability. Our aim is to show that \emph{Corvi} assigns a higher ``real" score to FLUX-generated images compared to LDM-generated images, indicating that features associated with real images are also present in fake images generated by FLUX, which belongs to the same generator family as LDM.

\textbf{Analysis}: The results are presented in Fig. \ref{fig:flux-spur}. The real scores of FLUX-generated images (bottom plot) are often higher in magnitude than those of LDM-generated images. This is problematic, as it suggests that features associated with real images by \emph{Corvi} are also present in fake images generated by FLUX. Furthermore, compared to LDM-generated fake images, FLUX-generated images have significantly lower fake scores (top plot). As a result, the majority of decisions for FLUX-generated images are influenced by the presence of real features, supporting our argument.

% Additionally, a significant portion of the fake scores of FLUX images overlap with those of LDM-generated images. The detector may pick up generator artifacts, but spurious real artifacts could dominate the decision.  We demonstrate this to be the case in later sections


% Our main argument is that an image can be classified as real if it does not contain artifacts specific to the generative model.
\section{Stay-Positive: Learning to Ignore Real Features}

In Section \ref{sec:spur_real}, we showed that associating specific patterns with the real distribution can undermine the detector's effectiveness. Building on this, we argue that an image should be classified as fake if it contains artifacts linked to the generative model of interest, while the absence of these artifacts indicates that the image is real. Consequently, we require an algorithm that forces the detector to focus solely on the patterns associated with the fake distribution. This intuition is illustrated in Figure \ref{fig:teaser}.

\textbf{Real and Fake Features:} Fundamentally, the presence of a real feature in an image should increase the likelihood of the detector classifying the image as real. In Section \ref{sec:lbid}, under certain assumptions, we demonstrated that the sign of the weights corresponding to each feature can be used to determine whether the presence of that feature enhances the probability of an image being classified as real. Specifically, the indices of \(\mathbf{h}\) that are multiplied by positive values of \(\mathbf{w}\) correspond to fake features $\mathcal{I}_{\text{fake}}$, while the indices multiplied by negative values of \(\mathbf{w}\) correspond to real features $\mathcal{I}_{\text{real}}$. We formalize this below,  
\[
\mathcal{I}_{\text{real}} = \{ i \mid \mathbf{w}_i < 0 \}, \quad 
\mathcal{I}_{\text{fake}} = \{ i \mid \mathbf{w}_i \geq 0 \}.
\]

This perspective allows us to train the detector to focus only on the features that distinguish fake images.


\textbf{Stay-Positive to Ignore Real Features:} Based on our analysis in Section \ref{sec:realfakefeats}, we aim for the real score to be 0 for all images. By definition, avoiding negative connections in the last layer guarantees this outcome. We achieve this by constraining the last-layer weights during optimization, overwriting any negative values with zero. This ensures that the network relies solely on $\mathcal{I}_{\text{fake}}$ to fit the training data. In our experiments, we found that training the entire network with this constraint negatively impacted the classification accuracy of real images. We hypothesize that when the entire network is trained, it may learn to link the absence of spurious real features to fake images, using negations. We discuss this further in Appendix \ref{app:hyp_lim}. To avoid this, we perform last-layer retraining instead. The details are outlined in Algorithm \ref{alg:train_clamp}. The algorithm illustrates a simple SGD update, though it is independent of the optimizer used.
%enforcing this constraint causes 
%We hypothesize that 

% We propose an extremely simple algorithm, which freezes the backbone of a pretrained fake detector and re-trains only the last layer. While doing so, we constrain the last layer weights such that negative values are overwritten with 0. For clarity, the details are described in Algorithm \ref{alg:train_clamp}. While the algorithm demonstrates a simple SGD update, it does not depend on the optimizer used.
We use validation accuracy to select the model and determine the stopping condition, as outlined in Appendix \ref{app:train_deets}.

\begin{algorithm}[t]
   \caption{Re-training the last-layer while staying positive}
   \label{alg:train_clamp}
\begin{algorithmic}
   \STATE {\bfseries Input:} Pretrained $g_\phi$, learning rate $\eta$, iterations $T$
   \STATE Initialize $\mathbf{w} = \mathbf{0}$, $b = 0$
   \STATE Freeze $g_\phi$
   \FOR{$t = 1$ {\bfseries to} $T$}
      \STATE Sample $\{\mathbf{x}_i, y_i\}_{i=1}^B$
      \STATE $\mathbf{h}_i = \gamma(g_\phi(\mathbf{x}_i))$ \hfill \# where \( \gamma \) denotes ReLU
      \STATE $f_\theta(\mathbf{x}_i) = \sigma(\mathbf{w}^\top \mathbf{h}_i + b)$
      \STATE $\mathcal{L} = -y_i \log(f_\theta(\mathbf{x}_i)) - (1 - y_i) \log(1 - f_\theta(\mathbf{x}_i))$
      \STATE $\mathbf{w} \leftarrow \mathbf{w} - \eta \nabla_\mathbf{w} \mathcal{L}$
      \STATE $b \leftarrow b - \eta \nabla_b \mathcal{L}$
      \STATE $\mathbf{w} \leftarrow \max(\mathbf{w}, \mathbf{0})$ 
   \ENDFOR
   \STATE {\bfseries Output:} $\mathbf{w}, b$
\end{algorithmic}
\end{algorithm}


\section{Experiments}
In this section, we evaluate whether ignoring real features can improve the performance of existing detectors. We consider the following state-of-the-art baselines: The \emph{Corvi} detector \citep{corvi2023detection}, which is based on a ResNet-50 network trained on real images from MSCOCO and LSUN, and fake images generated by LDM with prompts corresponding to the real data. Similarly, the \emph{Rajan} detector \citep{rajan2024effectiveness} uses the same real images as \emph{Corvi}, but instead trains on fake images generated by LDM reconstructions of the real images.

Both detectors are trained using the best practices suggested by \citet{gragnaniello2021gan}, with details provided in Appendix \ref{app:train_deets}. We apply Algorithm \ref{alg:train_clamp} to both detectors while using the same datasets as \emph{Corvi} and \emph{Rajan}. The resulting detectors are referred to as \emph{Corvi$\oplus$ (Ours)} and \emph{Rajan$\oplus$ (Ours)}. We also conduct some of these experiments with detectors trained on GAN-generated images, and these results are included in Appendix \ref{app:gan_exp}.

\subsection{Mitigating Post-Processing based Spurious Correlations}

\subsubsection{Compression-based Artifacts}
Both \emph{Corvi} and \emph{Rajan} suffer from the spurious correlations studied in Section \ref{subseq:spur-webp}, where WEBP compressed images are detected as real images.


\begin{figure}[t]
\begin{center}
\centerline{\includegraphics[height=1.5in]{icml2025/figures/webp/webp-fixed.png}}
\caption{\textbf{Improved Robustness to WEBP Compression.} Compared to the original \emph{Corvi} and \emph{Rajan}, our detectors \emph{Corvi$\oplus$} and \emph{Rajan$\oplus$} show increased robustness towards WEBP Compression.}
\label{fig:webp-spur-fixed}
\end{center}
\vskip -0.3in
\end{figure}
% as indicated by the Average Precision.

\textbf{Experiment Details}: To study the sensitivity of \emph{Corvi$\oplus$ (Ours)} and \emph{Rajan$\oplus$ (Ours)} to WEBP compression we utilize the same setting used in Section \ref{subseq:spur-webp}. 

\textbf{Analysis}: We report the results in Figure \ref{fig:webp-spur-fixed}. Compared to \emph{Corvi} and \emph{Rajan}, our detectors are much more robust to WEBP compression. Additionally, our detectors, while ignoring real features, are still able to separate generated images from real images, as indicated by the AP scores. It is important to note that our solution does not require any prior knowledge about specific spurious features, unlike data augmentation-based solutions. 

\subsubsection{Resizing-based Artifacts} \emph{Corvi} struggles with downsized fake images, as noted by \citet{rajan2024effectiveness}. This issue arises from the data augmentation strategy used during training, where real images, which typically have higher resolutions, are randomly cropped and downsized to 256×256. This augmentation causes the detector to associate downsizing with real images.

\textbf{Experiment Details}: With this experiment, we want to compare the robustness of \emph{Corvi$\oplus$ (Ours)} with the original \emph{Corvi} with respect to downsizing. To do so, we randomly select 500 real images from \emph{whichfaceisreal} \citep{whichfacereal} and generate fake images using SDv2.1. All images have a resolution of 1024x1024. We downsize the fake images to different scales and plot AP vs. the scaling factor.

% We analyze the sensitivity of \emph{Corvi} and \emph{Corvi$\oplus$} to downsizing the fake images by computing the APs between the downsized fake images and original real images. We do so by using bicubic downsizing.

\begin{table*}[t]
\caption{\textbf{AP of existing detectors before and after last-layer retraining.} The large improvements on FLUX and aMUSEd suggest that existing detectors are harmed by spurious correlations unrelated to post-processing, which our algorithm circumvents. Additionally, on settings where \emph{Corvi}, \emph{Rajan} already perform well, our method performs the same if not slightly better.}
\label{tab:clean_ap}
\vskip 0.1in
\begin{center}
\begin{small}
\begin{sc}
\setlength{\tabcolsep}{4.5pt} % Adjust column spacing
\begin{tabular}{lccccccccc}
\toprule
Method      & SD    & MJ    & KD    & PG & PixArt & LCM   & FLUX  & Wuerstchen & aMUSEd \\
\midrule
Corvi       & 99.97 & 99.46 & 99.98 & 97.42      & \textbf{99.99}    & 99.94 & 57.25 & \textbf{100}        & 89.18  \\
Corvi$\oplus$ (Ours) & \textbf{99.98} & \textbf{99.79} & \textbf{99.99} & \textbf{99.99}      & \textbf{99.99}    & \textbf{99.99} & \textbf{99.33} & 99.99      & \textbf{99.80}  \\
\midrule
Rajan       & 99.89 & 99.90 & 99.98 & 99.94      & \textbf{100}      & \textbf{99.99} & 80.64 & 95.13      & 87.20  \\
Rajan$\oplus$ (Ours)  & \textbf{99.99} & \textbf{99.93} & \textbf{99.99} & \textbf{99.99}      & 99.99    & \textbf{99.99} & \textbf{90.50} & \textbf{97.99}      & \textbf{98.11}  \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

\begin{figure}[t]
\vskip 0.1in
\begin{center}
\centerline{\includegraphics[height=1.5in]{icml2025/figures/res/res-spur.png}}
\caption{\textbf{Improved Robustness to Downsizing.} Compared to the original \emph{Corvi},  our \emph{Corvi$\oplus$} shows increased robustness towards downsampling.}
\label{fig:res-spur-fixed}
\end{center}
\vskip -0.3in
\end{figure}
% as indicated by the improved Average Precision

\textbf{Analysis}: We plot AP with respect to the scaling factor in Figure \ref{fig:res-spur-fixed}. By training on the same dataset, \emph{Corvi$\oplus$ (Ours)} is much more robust to downsizing operations in comparison to \emph{Corvi}. Similar to the case of WEBP compression, we are able to mitigate a spurious correlation without using information about the cause.

\subsection{Mitigating other Spurious Correlations}\label{sec:dist_gap}
Section \ref{sec:realfakefeats} discussed how real features can affect generalization. In this section, we test if \emph{Corvi$\oplus$ (Ours)} and \emph{Rajan$\oplus$ (Ours)} improve over the original versions.

\subsubsection{Dataset} To test our hypothesis, we use the following dataset: \textbf{i. Real}: 6,000 images from Redcaps \citep{desai2021redcaps}, WikiArt \citep{wikiart}, LAION-Aesthetics \citep{schuhmann2022laion}, and whichfaceisreal \citep{whichfacereal}, including 3,000 post-processed images. \textbf{ii. SD}: 3,000 images from SDv1.5, InstructPix2Pix \citep{brooks2023instructpix2pix}, and Nights \citep{fu2023dreamsim}; \textbf{iii. MJ}: 3,000 images from MidJourney v4 and v5 \citep{midjourney}; \textbf{iv. Kandinsky}: 3,100 images from Kandinsky 2.1 \citep{razzhigaev2023kandinsky}; \textbf{v. PG}: 3,150 images from Playground v2.5 \citep{li2024playground}; \textbf{vi. PixArt}: 3,150 images from PixelArt-$\alpha$ \citep{chen2023pixart}; \textbf{vii. LCM}: 3,146 images from SimianLuo/LCM\_Dreamshaper\_v7 \citep{luo2023latent}; \textbf{viii. FLUX}: 3,000 images from Black-Forest-Labs/FLUX.1-dev \citep{flux}; \textbf{ix. Wuerstchen}: 3,150 images from warp-ai/wuerstchen \citep{pernias2023wuerstchen}; \textbf{x. aMUSEd}: 3,150 images from Amused/amused-512 \citep{patil2024amused}. 

The dataset includes the latest latent space models: aMUSEd (autoregressive) and others (diffusion/flow-based). We generate images for FLUX, Wuerstchen, and aMUSEd, while the rest come from \citet{rajan2024effectiveness}'s test set. The real image set covers scenery, art, and faces, with post-processing (compression, resizing, color jitter) to ensure diversity. We demonstrate that our selected real image dataset, represents a wide range of real image types in Appendix \ref{app:real_perf}. We aim to show that base detectors, \emph{Corvi} and \emph{Rajan}, also learn spurious real features unrelated to post-processing, harming performance. To ensure our experimental results are free from these effects, we use fake images that haven’t undergone any post-processing.

% In this section, we want to show that there are spurious correlations which are present in the real distribution irrespective of post-processing, therefore we do not post-process our fake images. 


\subsubsection{Discussion}
We present the AP values for the original \emph{Corvi} and \emph{Rajan}, alongside our versions incorporating last-layer retraining, in Table \ref{tab:clean_ap}. Our methods show substantial improvements in AP on FLUX-generated images. \emph{Corvi$\oplus$ (Ours)} and \emph{Rajan$\oplus$ (Ours)} outperform the original detectors by 42.08 and 9.86, respectively, on FLUX, and by 10.62 and 10.91, respectively, on aMUSEd. As our fake images lack post-processing artifacts, the performance gap is likely due to the spurious correlations discussed in Section \ref{sec:realfakefeats}. These improvements suggest that while the original detectors can identify features distinguishing fake images from FLUX and aMUSEd, learning real features hinders the detection of these fake images. Importantly, in settings where \emph{Corvi} and \emph{Rajan} perform well (e.g., SD, KD, LCM), our method matches or outperforms them, showing that the detector does not lose performance by ignoring real features.

\subsection{Comparison with State-of-the-Art Fake Detectors}\label{sec:comparisons}

Next, we compare the performance of our detector with state-of-the-art fake detection methods. We use the same dataset from Section \ref{sec:dist_gap}, but to simulate a real-world setting, we create post-processed versions of the fake images. For FLUX, Wuerstchen, and aMUSEd, we randomly apply compression, resizing, and color jittering, and add these modified images to the dataset. Images for the other models are taken from the dataset provided by \citet{rajan2024effectiveness}. Additionally, we evaluate our detector on the widely-used GenImage benchmark \citep{zhu2024genimage}, with results presented in Appendix \ref{app:genimg_ldm}.


\begin{table*}[t]
\caption{\textbf{AP of state-of-the-art detectors on post-processed fake images.} Methods trained by our algorithm, \emph{Corvi$\oplus$} and \emph{Rajan$\oplus$} outperform their base detectors as well as other state-of-the-art detection approaches on recent generators such as FLUX and Wuerstchen. Additionally, on settings where \emph{Corvi}, \emph{Rajan} already perform well, our method performs the same if not slightly better.}
\label{tab:proc_ap}
\vskip 0.1in
\begin{center}
\begin{small}
\begin{sc}
\setlength{\tabcolsep}{3.5pt} % Adjusted column spacing
\begin{tabular}{lccccccccc}
\toprule
Method           & SD    & MJ    & KD    & PG   & PixArt & LCM   & FLUX  & Wuerstchen & aMUSEd \\
\midrule
UFD-ProGAN \citep{ojha2023towards}       & 61.93 & 61.72 & 74.88 & 70.23 & 69.81  & 70.86 & 37.54 & 86.30 & 88.84 \\
UFD-LDM \citep{ojha2023towards}          & 62.02 & 52.33 & 65.33 & 62.36 & 62.39  & 65.52 & 35.18 & 86.46 & 90.98 \\
ClipDet \citep{cozzolino2024raisingbaraigeneratedimage}    & 71.63 & 73.72 & 74.71 & 75.53 & 76.61  & 72.06 & 81.48 & 90.11 & 87.61 \\
DRCT \citep{chendrct}                    & 96.09 & 90.74 & 95.86 & 83.83 & 78.39  & 88.43 & 76.60 & 85.40 & 87.87 \\
\midrule
Corvi \citep{corvi2023detection}         & 97.87 & 94.81 & 95.38 & 91.40 & 94.45  & 96.33 & 74.57 & 95.83 & 94.57 \\
Corvi$\oplus$ (Ours)                          & 98.94 & 94.92 & 97.71 & 97.87 & 98.59  & 98.73 & \textbf{94.23} & \textbf{98.16} & 95.47 \\
\midrule
Rajan \citep{rajan2024effectiveness}     & \textbf{99.40} & \textbf{98.30} & 98.18 & \textbf{98.62} & 98.64 & \textbf{99.79} & 87.80 & 94.51 & 95.38 \\
Rajan$\oplus$ (Ours)                          & 99.22 & 96.98 & \textbf{98.22} & 98.53 & \textbf{99.11} & 99.57 & 91.85 & 94.74 & \textbf{97.26} \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}




\subsubsection{Baselines}
We select 3 methods from the CLIP linear-probing paradigm: \textbf{i. UFD-ProGAN} \citep{ojha2023towards}: trained on ProGAN images, representing the baseline method in this paradigm; \textbf{ii. UFD-LDM} \citep{ojha2023towards}: trained on LDM images, extending the original setup to a different generative domain; \textbf{iii. ClipDet}: a follow-up work by \citet{cozzolino2024raisingbaraigeneratedimage} that refines the linear-probing approach for LDM images. Additionally, we include \textbf{iv. DRCT}: a recent method by \citet{chendrct} that employs DDIM inversion to reconstruct both real and fake images, combined with a contrastive training objective for better generalization, using a ConvNext \citep{liu2022convnet} backbone trained on images from SDv1.4. We also compare against the original \textbf{Corvi} and \textbf{Rajan}, the detectors on which our method is based, to benchmark the improvements our approach introduces.

\subsubsection{Discussion}
We present our results in Table \ref{tab:proc_ap} and observe that detectors relying on full-network fine-tuning, such as DRCT, \emph{Corvi}, and \emph{Rajan}, along with our improved baselines, outperform CLIP-based approaches. In comparison to \emph{Corvi}, our \emph{Corvi$\oplus$ (Ours)} shows large improvements. On PG and FLUX generated images, our version outperforms the original \emph{Corvi} by 6.47 and 19.66 respectively.  On FLUX generated images, our version outperforms the original \emph{Rajan} by 4.05. Notably, the AP scores of the original \emph{Corvi} and \emph{Rajan} on FLUX and aMUSEd improve compared to the values reported in Table \ref{tab:clean_ap}, indicating that post-processed fake images from these generators were actually easier to detect compared to fake images without post-processing. This suggests that the fake images used in Table \ref{tab:clean_ap} contain artifacts that harm the performance of \emph{Corvi} and \emph{Rajan}. Post-processing these images can remove these artifacts, improving both detectors' performance. Given their sensitivity to post-processing, these are likely low-level artifacts affecting detector performance, similar to those described in Section.\ref{sec:realfakefeats}. Unlike the original \emph{Corvi} and \emph{Rajan}, our versions are unaffected by such discrepancies.

\subsection{Improved Detection of Partially Inpainted Images}
Prior experiments consider fake images that are completely generated. However, a user can take real images and partially modify them. While most regions of such images are``real", they could be created with malicious intent, making their detection important. Intuitively, detectors that depend on real features would struggle to detect these modified images, since a portion of these images is real. Here, we study the sensitivity of our approach to such images.

\subsubsection{Experiment Details} We use the Stable Diffusion inpainted dataset from \citet{conde2024stable}, where a real image is modified by masking and inpainting a region. Following \citet{conde2024stable}, we group images by the percentage of inpainted pixels. In recursive inpainting (refer Fig \ref{fig:inp}), the same region can be inpainted multiple times, so the number of inpainted pixels may exceed the total (e.g., 150\%). Each group has 300 inpainted images. We calculate AP with respect to the 6000 real images from Section \ref{sec:dist_gap}. 

\begin{figure}[t]
\vskip 0.1in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{icml2025/figures/inpaint/inp.png}}
\caption{\textbf{Example of an image which has been recursively inpainted.} The first/second row shows the inpainted image and the inpainted region, respectively.}
\label{fig:inp}
\end{center}
\vskip -0.75in
\end{figure}

\begin{table}[t]
\caption{\textbf{AP on partially-inpainted fake images.} \emph{Corvi} and \emph{Rajan} struggle to detect partially-inpainted fake images, as they focus on real image features. Our detectors overcome this limitation.}
\label{tab:inp_ap}
\begin{center}
\begin{small}
\begin{sc}
\vskip 0.05in
\setlength{\tabcolsep}{4.5pt} % Adjust column spacing
\begin{tabular}{lccc}
\toprule
Method           & 50\%     & 100\%    & 150\%    \\
\midrule
% UFD-ProGAN       & 16.05 & 35.69 & 50.21 \\
% UFD-LDM          & 19.48 & 40.51 & 54.37 \\
% ClipDet    & 4.84 & 5.98 & 6.12 \\
% DRCT    & 7.9  & 36.13 & 61.77 \\
% \midrule
Corvi            & 6.13 & 83.36 & 98.05 \\
Corvi$\oplus$ (Ours)    & 97.48  & 99.92  & 99.98 \\
\midrule
Rajan            & 49.69  & 99.93  & 100       \\
Rajan$\oplus$ (Ours)   & \textbf{99.66}  & \textbf{99.99}  & \textbf{100}       \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\end{table}
\subsubsection{Analysis}
We report our results in Table \ref{tab:inp_ap}. At the 50-level, all approaches barring \emph{Corvi$\oplus$} and \emph{Rajan$\oplus$} fail to detect these inpainted images. This is perhaps unsurprising since these images are "real" for the most part. Our detectors on the other hand, do not suffer from these issues as indicated by the AP score. The performance of \emph{Corvi} and \emph{Rajan} improves with more inpainting, highlighting the harmful impact of real features on their performance. 
%, barring Cozzolino-LDM,


\section{Limitations}

Our approach enhances existing detectors by ensuring that the final layer ignores features associated with the real distribution. However, patterns linked to the fake distribution can also be spurious. For instance, \citet{rajan2024effectiveness} observed that \emph{Corvi} incorrectly associates upsampled images with the real distribution. We find that \emph{Corvi$\oplus$ (Ours)} exhibits a similar issue. To illustrate this, we take 500 Redcaps images (512×512), upsample them, and analyze how their upsampled versions affect the probability of being classified as fake. As shown in Fig. \ref{fig:corvi-fakespur}, upsampling spuriously increases this probability.

% However, this approach does not address the issue of spurious features linked to the fake distribution. For instance, prior work by \citet{rajan2024effectiveness} highlighted that \emph{Corvi} struggles with associating upsampled images with the real distribution. 
Our method ensures that the detector ignores real image-specific features, but these features can still shape its notion of fakeness. For instance, the network may infer that the absence of certain real image features indicates a fake image. We illustrate this with a simple toy example in Appendix \ref{app:hyp_lim}. Thus, directly ignoring real features without a two-stage training process may be a promising next direction.% to enhance existing detectors.

% Our method of re-training only the last layer forces the detector to ignore patterns which can be found in real images, however, negations of these

% Unfortunately, the spurious patterns . We further explore this limitation in Appendix \ref{app:hyp_lim}. As shown in Fig. \ref{fig:webp-spur}, \emph{Rajan$\oplus$ (Ours)} improves upon the original \emph{Rajan}, but still underperforms compared to \emph{Rajan (only COCO)}. Avoiding the two-stage training and directly ignoring the real features would significantly improve existing detectors.

% Our approach is reliant on the fake features captured by the last-layer of the detector. However, traces of the spurious correlations are still felt. However, given the intricate and interconnected nature of neural networks, spurious features tied to the real distribution can also influence the fakeness score of a given image. Therefore, it is important to develop algorithms which can train the whole network to focus on the fake distribution. Additionally, the current algorithm is limited to feature spaces that follow ReLU-style activations. Although this structure is relatively simple to enforce, it presents challenges in extending the approach to CLIP-based methods, which do not adhere to this structure. 

\section{Related Work}
Training-based methods create effective fake image detectors. \citet{wang2020cnn} demonstrated that data augmentations during training improve generalization. \citet{odena2016deconvolution} demonstrated identifiable artifacts, like checkerboard patterns, in the Fourier transforms of fake images. Building on this, \citet{zhang2019detecting} trained on Fourier images, leading to improvements. \citet{chai2020makesfakeimagesdetectable} improved detection using patch-based classification. \citet{gragnaniello2021gan} removed downsampling in the initial layers and applied patch-wise training for further gains. These techniques were extended to the LDM setting by \citet{corvi2023detection}. Additionally, works such as \citet{chendrct} and \citet{rajan2024effectiveness} also use reconstructions of real images as part of the training data. These approaches struggle to generalize across architectures. To address this, \citet{ojha2023towards} proposed using general-purpose visual encoders like CLIP \citep{radford2021learning} for fake image detection. Unlike ours, these methods explicitly rely on real features.


Contrary to prior approaches, GenDet \citep{zhu2023gendet} treats fake image detection as an anomaly detection problem, similar to ours, but focuses on learning the real image distribution. Another paradigm suggests generators reconstruct fake images more easily than real ones. For reconstruction, \citet{pasquini2023identifying} use GAN inversion \citep{xia2022gan}, DIRE \citep{wang2023dire} applies DDIM inversion \citep{song2020denoising}, and AEROBLADE \citep{ricker2024aeroblade} leverages a latent diffusion autoencoder. However, these methods struggle with detecting post-processed images \citep{rajan2024effectiveness}. A contrasting approach, akin to image reconstruction, was recently proposed by \citet{cozzolino2025zero}, who use neural image-compression networks \citep{cao2020lossless} to model real distribution likelihood, assuming fake images are less likely to be part of it. While this method models the real distribution, we argue the focus should be on detecting fake image artifacts instead.
\begin{figure}[t]
\begin{center}
\centerline{\includegraphics[height=1.5in]{icml2025/figures/res/fakespur.png}}

\caption{\textbf{Vulnerability to Spurious Fake Features.} Our method \emph{Corvi$\oplus$} is not able to mitigate spurious correlations pertaining to the fake distribution, where just like the original \emph{Corvi}, it continues to associate upsampled images with the fake distribution.}
\label{fig:corvi-fakespur}
\end{center}
\vskip -0.28in
\end{figure}


\section{Conclusion}
We showed that learning patterns from the real distribution can harm fake image detection and propose a last-layer fine-tuning strategy to improve performance. By ignoring real-distribution features, the model reduces susceptibility to spurious correlations, enhancing robustness. Additionally, models trained this way excel on partially inpainted real images. We hope our study contributes to developing more robust detectors with positive societal impact.


% \paragraph{Learning Based Fake Image Detection.} Early work by \citet{wang2020cnn} introduced detectors for GAN-generated images using CNN backbones, highlighting the benefits of data augmentations for generalization. Building on this, \citet{chai2020makesfakeimagesdetectable} demonstrated that patch-based training enhances detection by focusing on fine-grained details. However, while these methods excel in detecting GAN-generated images, they struggle to generalize to newer generator families like diffusion models.

% Addressing this gap, \citet{ojha2023towards} showed that linear probing a CLIP backbone, predominantly trained on real images, improves detection of fake images from autoregressive and diffusion models. While these advancements emphasize generalization to unseen architectures, comprehensive detection of images from known architectures remains a challenge. For latent diffusion models, \citet{corvi2023detection} and \citet{rajan2024effectiveness} developed effective detectors by leveraging patch-wise training and extensive data augmentations. However, these approaches suffer from spurious correlations, which some methods \citep{grommelt2024fake, rajan2024effectiveness} attempt to mitigate through careful dataset curation.

% Notably, prior techniques focus on both real and fake distributions. To the best of our knowledge, our work is the first to demonstrate the advantages of completely disregarding the real distribution, addressing spurious correlations more effectively.

% \paragraph{Spurious Correlation Mitigation.}Existing work has addressed spurious correlations in other contexts. \citet{liu2021just} improve worst-group performance by re-training networks with increased weight on samples that are particularly challenging for the model. Correct-n-Contrast \citep{zhang2022correct} introduces a two-stage approach: the first stage clusters data within the same class, and the second stage uses contrastive training to enforce similar representations within each cluster. Most closely related to our work is last-layer retraining \citep{kirichenko2022last}, which involves re-training the final layer using a carefully curated dataset. While these methods target general binary classification, our approach is specifically designed for detecting fake images.

% Submission to ICML 2025 will be entirely electronic, via a web site
% (not email). Information about the submission process and \LaTeX\ templates
% are available on the conference web site at:
% \begin{center}
% \textbf{\texttt{http://icml.cc/}}
% \end{center}

% The guidelines below will be enforced for initial submissions and
% camera-ready copies. Here is a brief summary:
% \begin{itemize}
% \item Submissions must be in PDF\@. 
% \item If your paper has appendices, submit the appendix together with the main body and the references \textbf{as a single file}. Reviewers will not look for appendices as a separate PDF file. So if you submit such an extra file, reviewers will very likely miss it.
% \item Page limit: The main body of the paper has to be fitted to 8 pages, excluding references and appendices; the space for the latter two is not limited in pages, but the total file size may not exceed 10MB. For the final version of the paper, authors can add one extra page to the main body.
% \item \textbf{Do not include author information or acknowledgements} in your
%     initial submission.
% \item Your paper should be in \textbf{10 point Times font}.
% \item Make sure your PDF file only uses Type-1 fonts.
% \item Place figure captions \emph{under} the figure (and omit titles from inside
%     the graphic file itself). Place table captions \emph{over} the table.
% \item References must include page numbers whenever possible and be as complete
%     as possible. Place multiple citations in chronological order.
% \item Do not alter the style template; in particular, do not compress the paper
%     format by reducing the vertical spaces.
% \item Keep your abstract brief and self-contained, one paragraph and roughly
%     4--6 sentences. Gross violations will require correction at the
%     camera-ready phase. The title should have content words capitalized.
% \end{itemize}

% \subsection{Submitting Papers}

% \textbf{Anonymous Submission:} ICML uses double-blind review: no identifying
% author information may appear on the title page or in the paper
% itself. \cref{author info} gives further details.

% \medskip

% Authors must provide their manuscripts in \textbf{PDF} format.
% Furthermore, please make sure that files contain only embedded Type-1 fonts
% (e.g.,~using the program \texttt{pdffonts} in linux or using
% File/DocumentProperties/Fonts in Acrobat). Other fonts (like Type-3)
% might come from graphics files imported into the document.

% Authors using \textbf{Word} must convert their document to PDF\@. Most
% of the latest versions of Word have the facility to do this
% automatically. Submissions will not be accepted in Word format or any
% format other than PDF\@. Really. We're not joking. Don't send Word.

% Those who use \textbf{\LaTeX} should avoid including Type-3 fonts.
% Those using \texttt{latex} and \texttt{dvips} may need the following
% two commands:

% {\footnotesize
% \begin{verbatim}
% dvips -Ppdf -tletter -G0 -o paper.ps paper.dvi
% ps2pdf paper.ps
% \end{verbatim}}
% It is a zero following the ``-G'', which tells dvips to use
% the config.pdf file. Newer \TeX\ distributions don't always need this
% option.

% Using \texttt{pdflatex} rather than \texttt{latex}, often gives better
% results. This program avoids the Type-3 font problem, and supports more
% advanced features in the \texttt{microtype} package.

% \textbf{Graphics files} should be a reasonable size, and included from
% an appropriate format. Use vector formats (.eps/.pdf) for plots,
% lossless bitmap formats (.png) for raster graphics with sharp lines, and
% jpeg for photo-like images.

% The style file uses the \texttt{hyperref} package to make clickable
% links in documents. If this causes problems for you, add
% \texttt{nohyperref} as one of the options to the \texttt{icml2025}
% usepackage statement.


% \subsection{Submitting Final Camera-Ready Copy}

% The final versions of papers accepted for publication should follow the
% same format and naming convention as initial submissions, except that
% author information (names and affiliations) should be given. See
% \cref{final author} for formatting instructions.

% The footnote, ``Preliminary work. Under review by the International
% Conference on Machine Learning (ICML). Do not distribute.'' must be
% modified to ``\textit{Proceedings of the
% $\mathit{42}^{nd}$ International Conference on Machine Learning},
% Vancouver, Canada, PMLR 267, 2025.
% Copyright 2025 by the author(s).''

% For those using the \textbf{\LaTeX} style file, this change (and others) is
% handled automatically by simply changing
% $\mathtt{\backslash usepackage\{icml2025\}}$ to
% $$\mathtt{\backslash usepackage[accepted]\{icml2025\}}$$
% Authors using \textbf{Word} must edit the
% footnote on the first page of the document themselves.

% Camera-ready copies should have the title of the paper as running head
% on each page except the first one. The running title consists of a
% single line centered above a horizontal rule which is $1$~point thick.
% The running head should be centered, bold and in $9$~point type. The
% rule should be $10$~points above the main text. For those using the
% \textbf{\LaTeX} style file, the original title is automatically set as running
% head using the \texttt{fancyhdr} package which is included in the ICML
% 2025 style file package. In case that the original title exceeds the
% size restrictions, a shorter form can be supplied by using

% \verb|\icmltitlerunning{...}|

% just before $\mathtt{\backslash begin\{document\}}$.
% Authors using \textbf{Word} must edit the header of the document themselves.

% \section{Format of the Paper}

% All submissions must follow the specified format.

% \subsection{Dimensions}




% The text of the paper should be formatted in two columns, with an
% overall width of 6.75~inches, height of 9.0~inches, and 0.25~inches
% between the columns. The left margin should be 0.75~inches and the top
% margin 1.0~inch (2.54~cm). The right and bottom margins will depend on
% whether you print on US letter or A4 paper, but all final versions
% must be produced for US letter size.
% Do not write anything on the margins.

% The paper body should be set in 10~point type with a vertical spacing
% of 11~points. Please use Times typeface throughout the text.

% \subsection{Title}

% The paper title should be set in 14~point bold type and centered
% between two horizontal rules that are 1~point thick, with 1.0~inch
% between the top rule and the top edge of the page. Capitalize the
% first letter of content words and put the rest of the title in lower
% case.

% \subsection{Author Information for Submission}
% \label{author info}

% ICML uses double-blind review, so author information must not appear. If
% you are using \LaTeX\/ and the \texttt{icml2025.sty} file, use
% \verb+\icmlauthor{...}+ to specify authors and \verb+\icmlaffiliation{...}+ to specify affiliations. (Read the TeX code used to produce this document for an example usage.) The author information
% will not be printed unless \texttt{accepted} is passed as an argument to the
% style file.
% Submissions that include the author information will not
% be reviewed.

% \subsubsection{Self-Citations}

% If you are citing published papers for which you are an author, refer
% to yourself in the third person. In particular, do not use phrases
% that reveal your identity (e.g., ``in previous work \cite{langley00}, we
% have shown \ldots'').


% not be broken across lines. Unbolded superscripted numbers, starting 1, should
% be used to refer to affiliations.

% Affiliations should be numbered in the order of appearance. A single footnote
% block of text should be used to list all the affiliations. (Academic
% affiliations should list Department, University, City, State/Region, Country.
% Similarly for industrial affiliations.)

% Each distinct affiliations should be listed once. If an author has multiple
% affiliations, multiple superscripts should be placed after the name, separated
% by thin spaces. If the authors would like to highlight equal contribution by
% multiple first authors, those authors should have an asterisk placed after their
% name in superscript, and the term ``\textsuperscript{*}Equal contribution"
% should be placed in the footnote block ahead of the list of affiliations. A
% list of corresponding authors and their emails (in the format Full Name
% \textless{}email@domain.com\textgreater{}) can follow the list of affiliations.
% Ideally only one or two names should be listed.

% A sample file with author names is included in the ICML2025 style file
% package. Turn on the \texttt{[accepted]} option to the stylefile to
% see the names rendered. All of the guidelines above are implemented
% by the \LaTeX\ style file.

% \subsection{Abstract}

% The paper abstract should begin in the left column, 0.4~inches below the final
% address. The heading `Abstract' should be centered, bold, and in 11~point type.
% The abstract body should use 10~point type, with a vertical spacing of
% 11~points, and should be indented 0.25~inches more than normal on left-hand and
% right-hand margins. Insert 0.4~inches of blank space after the body. Keep your
% abstract brief and self-contained, limiting it to one paragraph and roughly 4--6
% sentences. Gross violations will require correction at the camera-ready phase.

% \subsection{Partitioning the Text}

% You should organize your paper into sections and paragraphs to help
% readers place a structure on the material and understand its
% contributions.

% \subsubsection{Sections and Subsections}

% Section headings should be numbered, flush left, and set in 11~pt bold
% type with the content words capitalized. Leave 0.25~inches of space
% before the heading and 0.15~inches after the heading.

% Similarly, subsection headings should be numbered, flush left, and set
% in 10~pt bold type with the content words capitalized. Leave
% 0.2~inches of space before the heading and 0.13~inches afterward.

% Finally, subsubsection headings should be numbered, flush left, and
% set in 10~pt small caps with the content words capitalized. Leave
% 0.18~inches of space before the heading and 0.1~inches after the
% heading.

% Please use no more than three levels of headings.

% \subsubsection{Paragraphs and Footnotes}

% Within each section or subsection, you should further partition the
% paper into paragraphs. Do not indent the first line of a given
% paragraph, but insert a blank line between succeeding ones.

% You can use footnotes\footnote{Footnotes
% should be complete sentences.} to provide readers with additional
% information about a topic without interrupting the flow of the paper.
% Indicate footnotes with a number in the text where the point is most
% relevant. Place the footnote in 9~point type at the bottom of the
% column in which it appears. Precede the first footnote in a column
% with a horizontal rule of 0.8~inches.\footnote{Multiple footnotes can
% appear in each column, in the same order as they appear in the text,
% but spread them across columns and pages if possible.}

% \begin{figure}[ht]
% \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=\columnwidth]{icml_numpapers}}
% \caption{Historical locations and number of accepted papers for International
% Machine Learning Conferences (ICML 1993 -- ICML 2008) and International
% Workshops on Machine Learning (ML 1988 -- ML 1992). At the time this figure was
% produced, the number of accepted papers for ICML 2008 was unknown and instead
% estimated.}
% \label{icml-historical}
% \end{center}
% \vskip -0.2in
% \end{figure}

% \subsection{Figures}

% You may want to include figures in the paper to illustrate
% your approach and results. Such artwork should be centered,
% legible, and separated from the text. Lines should be dark and at
% least 0.5~points thick for purposes of reproduction, and text should
% not appear on a gray background.

% Label all distinct components of each figure. If the figure takes the
% form of a graph, then give a name for each axis and include a legend
% that briefly describes each curve. Do not include a title inside the
% figure; instead, the caption should serve this function.

% Number figures sequentially, placing the figure number and caption
% \emph{after} the graphics, with at least 0.1~inches of space before
% the caption and 0.1~inches after it, as in
% \cref{icml-historical}. The figure caption should be set in
% 9~point type and centered unless it runs two or more lines, in which
% case it should be flush left. You may float figures to the top or
% bottom of a column, and you may set wide figures across both columns
% (use the environment \texttt{figure*} in \LaTeX). Always place
% two-column figures at the top or bottom of the page.

% \subsection{Algorithms}

% If you are using \LaTeX, please use the ``algorithm'' and ``algorithmic''
% environments to format pseudocode. These require
% the corresponding stylefiles, algorithm.sty and
% algorithmic.sty, which are supplied with this package.
% \cref{alg:example} shows an example.

% \begin{algorithm}[tb]
%    \caption{Bubble Sort}
%    \label{alg:example}
% \begin{algorithmic}
%    \STATE {\bfseries Input:} data $x_i$, size $m$
%    \REPEAT
%    \STATE Initialize $noChange = true$.
%    \FOR{$i=1$ {\bfseries to} $m-1$}
%    \IF{$x_i > x_{i+1}$}
%    \STATE Swap $x_i$ and $x_{i+1}$
%    \STATE $noChange = false$
%    \ENDIF
%    \ENDFOR
%    \UNTIL{$noChange$ is $true$}
% \end{algorithmic}
% \end{algorithm}

% \subsection{Tables}

% You may also want to include tables that summarize material. Like
% figures, these should be centered, legible, and numbered consecutively.
% However, place the title \emph{above} the table with at least
% 0.1~inches of space before the title and the same after it, as in
% \cref{sample-table}. The table title should be set in 9~point
% type and centered unless it runs two or more lines, in which case it
% should be flush left.

% Note use of \abovespace and \belowspace to get reasonable spacing
% above and below tabular lines.

% \begin{table}[t]
% \caption{Classification accuracies for naive Bayes and flexible
% Bayes on various data sets.}
% \label{sample-table}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \begin{tabular}{lcccr}
% \toprule
% Data set & Naive & Flexible & Better? \\
% \midrule
% Breast    & 95.9$\pm$ 0.2& 96.7$\pm$ 0.2& $\surd$ \\
% Cleveland & 83.3$\pm$ 0.6& 80.0$\pm$ 0.6& $\times$\\
% Glass2    & 61.9$\pm$ 1.4& 83.8$\pm$ 0.7& $\surd$ \\
% Credit    & 74.8$\pm$ 0.5& 78.3$\pm$ 0.6&         \\
% Horse     & 73.3$\pm$ 0.9& 69.7$\pm$ 1.0& $\times$\\
% Meta      & 67.1$\pm$ 0.6& 76.5$\pm$ 0.5& $\surd$ \\
% Pima      & 75.1$\pm$ 0.6& 73.9$\pm$ 0.5&         \\
% Vehicle   & 44.9$\pm$ 0.6& 61.5$\pm$ 0.4& $\surd$ \\
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table}

% Tables contain textual material, whereas figures contain graphical material.
% Specify the contents of each row and column in the table's topmost
% row. Again, you may float tables to a column's top or bottom, and set
% wide tables across both columns. Place two-column tables at the
% top or bottom of the page.

% \subsection{Theorems and such}
% The preferred way is to number definitions, propositions, lemmas, etc. consecutively, within sections, as shown below.
% \begin{definition}
% \label{def:inj}
% A function $f:X \to Y$ is injective if for any $x,y\in X$ different, $f(x)\ne f(y)$.
% \end{definition}
% Using \cref{def:inj} we immediate get the following result:
% \begin{proposition}
% If $f$ is injective mapping a set $X$ to another set $Y$, 
% the cardinality of $Y$ is at least as large as that of $X$
% \end{proposition}
% \begin{proof} 
% Left as an exercise to the reader. 
% \end{proof}
% \cref{lem:usefullemma} stated next will prove to be useful.
% \begin{lemma}
% \label{lem:usefullemma}
% For any $f:X \to Y$ and $g:Y\to Z$ injective functions, $f \circ g$ is injective.
% \end{lemma}
% \begin{theorem}
% \label{thm:bigtheorem}
% If $f:X\to Y$ is bijective, the cardinality of $X$ and $Y$ are the same.
% \end{theorem}
% An easy corollary of \cref{thm:bigtheorem} is the following:
% \begin{corollary}
% If $f:X\to Y$ is bijective, 
% the cardinality of $X$ is at least as large as that of $Y$.
% \end{corollary}
% \begin{assumption}
% The set $X$ is finite.
% \label{ass:xfinite}
% \end{assumption}
% \begin{remark}
% According to some, it is only the finite case (cf. \cref{ass:xfinite}) that is interesting.
% \end{remark}
% %restatable

% \subsection{Citations and References}

% Please use APA reference format regardless of your formatter
% or word processor. If you rely on the \LaTeX\/ bibliographic
% facility, use \texttt{natbib.sty} and \texttt{icml2025.bst}
% included in the style-file package to obtain this format.

% Citations within the text should include the authors' last names and
% year. If the authors' names are included in the sentence, place only
% the year in parentheses, for example when referencing Arthur Samuel's
% pioneering work \yrcite{Samuel59}. Otherwise place the entire
% reference in parentheses with the authors and year separated by a
% comma \cite{Samuel59}. List multiple references separated by
% semicolons \cite{kearns89,Samuel59,mitchell80}. Use the `et~al.'
% construct only for citations with three or more authors or after
% listing all authors to a publication in an earlier reference \cite{MachineLearningI}.

% Authors should cite their own work in the third person
% in the initial version of their paper submitted for blind review.
% Please refer to \cref{author info} for detailed instructions on how to
% cite your own papers.

% Use an unnumbered first-level section heading for the references, and use a
% hanging indent style, with the first line of the reference flush against the
% left margin and subsequent lines indented by 10 points. The references at the
% end of this document give examples for journal articles \cite{Samuel59},
% conference publications \cite{langley00}, book chapters \cite{Newell81}, books
% \cite{DudaHart2nd}, edited volumes \cite{MachineLearningI}, technical reports
% \cite{mitchell80}, and dissertations \cite{kearns89}.

% Alphabetize references by the surnames of the first authors, with
% single author entries preceding multiple author entries. Order
% references for the same authors by year of publication, with the
% earliest first. Make sure that each reference includes all relevant
% information (e.g., page numbers).

% Please put some effort into making references complete, presentable, and
% consistent, e.g. use the actual current name of authors.
% If using bibtex, please protect capital letters of names and
% abbreviations in titles, for example, use \{B\}ayesian or \{L\}ipschitz
% in your .bib file.

% \section*{Accessibility}
% Authors are kindly asked to make their submissions as accessible as possible for everyone including people with disabilities and sensory or neurological differences.
% Tips of how to achieve this and what to pay attention to will be provided on the conference website \url{http://icml.cc/}.

% \section*{Software and Data}

% If a paper is accepted, we strongly encourage the publication of software and data with the
% camera-ready version of the paper whenever appropriate. This can be
% done by including a URL in the camera-ready copy. However, \textbf{do not}
% include URLs that reveal your institution or identity in your
% submission for review. Instead, provide an anonymous URL or upload
% the material as ``Supplementary Material'' into the OpenReview reviewing
% system. Note that reviewers are not required to look at this material
% when writing their review.

% Acknowledgements should only appear in the accepted version.


\section*{Acknowledgements}

This work was supported in part by NSF IIS2404180, American Family Insurance, and Institute of Information $\&$ communications Technology Planning $\&$ Evaluation(IITP) grants funded by the Korea government(MSIT) (No. 2022-0-00871, Development of AI Autonomy and Knowledge Enhancement for AI Agent Collaboration) and (No. RS2022-00187238, Development of Large Korean Language Model Technology for Efficient Pre-training).

\section*{Impact Statement}
This paper aims to advance the field of fake image detection, with a focus on security and potential societal benefits, such as curbing misinformation. However, it may also provide bad actors with insights into recent developments in image forensics and expose vulnerabilities in existing systems.

% Authors are \textbf{required} to include a statement of the potential 
% broader impact of their work, including its ethical aspects and future 
% societal consequences. This statement should be in an unnumbered 
% section at the end of the paper (co-located with Acknowledgements -- 
% the two may appear in either order, but both must be before References), 
% and does not count toward the paper page limit. In many cases, where 
% the ethical impacts and expected societal implications are those that 
% are well established when advancing the field of Machine Learning, 
% substantial discussion is not required, and a simple statement such 
% as the following will suffice:

% ``This paper presents work whose goal is to advance the field of 
% Machine Learning. There are many potential societal consequences 
% of our work, none which we feel must be specifically highlighted here.''

% The above statement can be used verbatim in such cases, but we 
% encourage authors to think about whether there is content which does 
% warrant further discussion, as this statement will be apparent if the 
% paper is later flagged for ethics review.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Appendix}

\subsection{Implementation Details}\label{app:train_deets}
We follow the training recipe used by \citet{corvi2023detection}. We train on 96 x 96 crops of the whole image using a batch size of 128. The data augmentations include random JPG compression and blur from the pipeline proposed by \citet{wang2020cnn}. Following \citet{gragnaniello2021gan}, grayscale, cutout and random noise are also used as augmentations. Finally, in order to make the network invariant towards resizing, the random resized crop was added. For the baseline detectors as well as our re-trained variant, we  report the average across two trained networks. When applying Algorithm \ref{alg:train_clamp}, we use a batch size of 2048 to perform last-layer retraining. The rest of the training recipe does not differ from the original model.

We use the validation set provided by \citet{corvi2023detection} for our training. Just like our training set, the real images come from COCO/LSUN and the fake images are generated at 256 x 256 using LDM. During training, if the validation accuracy does not improve by 0.1\% in 10 epochs the learning rate is dropped by 10x. The training is terminated at learning rate $10^{-6}$. We adopt this following \citet{wang2020cnn}.

During inference, we do not to crop/resize the image to a fixed resolution. This is possible since the ResNet-50 uses a Spatially-Adaptive Average Pooling layer before inference.




\subsection{Performance on Real Images}\label{app:real_perf}
When testing our models effectiveness in detecting images coming from various kinds of generators, we use a real image dataset which consists of both natural real images (Redcaps), artistic real images (wikiart and LAION-Aesthetics) as well as face images (whichfaceisreal). We also post-process the images to simulate a real world setting. However, in this section, we test whether this set of real images is truly representative of the multiple real distributions present. In order to verify this, we plot the distributions of various kinds of real images. 

\subsubsection{Dataset}
\textbf{i. Test Real}: 6,000 images previously used in the main paper, containing artistic and natural images. \textbf{ii. GTA}: 6,382 GTA landscape images from the IMLE dataset \citep{li2019diverse}, providing a diverse range of synthetic scenes. \textbf{iii. ImageNet}: 8,000 real images from the GenImage benchmark, originally part of the ImageNet dataset, to capture standard real-world content. \textbf{iv. Cubism}: 2,235 images in the Cubism style sourced from the WikiArt dataset, adding a distinct artistic domain. \textbf{v. Pop Art}: 1,483 images in the Pop Art style from WikiArt, expanding the artistic domain with vibrant and modern aesthetics. \textbf{vi. Modern Art}: 4,334 images of Modern Art from WikiArt, offering a rich and varied artistic representation.

\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\textwidth]{icml2025/figures/teaser/rtease.png}}
\caption{Example of different kinds of real images that we consider.}
\label{fig:real_tease}
\end{center}
\vskip -0.2in
\end{figure*}

This dataset ensures a wide range of testing scenarios, from standard real-world distributions to highly diverse artistic and CG (but not neural network generated) domains. An example of the images used can be found in Figure \ref{fig:real_tease}. 

\subsubsection{Results}
\begin{figure*}[t]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\textwidth]{icml2025/figures/violins.png}}
\caption{\textbf{Distribution of different kinds of real images.} The violin plots of \emph{Corvi$\oplus$} (left) and \emph{Rajan$\oplus$} (right) show that the test set used in our experiments accounts for a wide variety of real image types.}
\label{fig:real_violins}
\end{center}
\vskip -0.2in
\end{figure*}

We pass the real images through \emph{Corvi$\oplus$ (Ours)} and \emph{Rajan$\oplus$ (Ours)} and plot the probability of fakeness (output of the network) in the form of violin plots in Fig \ref{fig:real_violins}. We observe that the fakeness scores of our test distribution indicated by ``Test Real" is extremely similar to the other distributions of real images such as GTA, Cubism etc. This shows that the test-set which we use in the main paper is representative of various different types of real image families. It is important to note that these real images are vastly different, belonging to different domains such as realistic scenes, art styles and video game rendered environments. However, the thing in common is the absence of the generator artifacts which is indicated by the low probability of fakeness in these violin plots.

\subsection{Performance on GenImage}\label{app:genimg_ldm}
\subsubsection{Experimental Setup}
In this section, we evaluate the performance of various detectors on the GenImage benchmark \citep{zhu2024genimage}. We consider the same baselines which we use in Section \ref{sec:comparisons}. The scope of our work is detecting images from the same kind of generative model (latent diffusion models), therefore we consider the GenImage testsets from Midjourney \citep{midjourney}, VQDM \citep{gu2022vector}, SDv1.4, SDv1.5 \citep{rombach2022high}, Wukong \citep{wukong}.

GenImage has a set of real images and a set of fake images for each category, to ensure uniformity, we report the accuracy. However, the real images are always sourced from ImageNet, therefore, we also report the AP with respect to the corresponding real distribution. For accuracy, we use a common threshold of 0.5.

\subsubsection{Results}
In Table \ref{tab:acc_ap_genimg_ldm} we report the accuracy (left) and AP (right). Based on the AP values, we can conclude that the methods which finetune the whole network represented by \emph{DRCT}, \emph{Corvi}, \emph{Rajan}, along with our improved detectors perform better than the CLIP-based techniques represented by ClipDet and the original UFD methods. Furthermore, the AP in detecting VQDM generated images, improves for \emph{Rajan$\oplus$ (Ours)} by 4.15\%. VQDM is a latent diffusion model which has a different architecture compared to LDM. This improvement further demonstrates the effect of ignoring real image artifacts. Furthermore, the accuracy measurements demonstrate that without calibrating the threshold, our detectors are able to detect latent diffusion generated images in the GenImage benchmark.

\begin{table*}[t]
\caption{\textbf{Accuracy and AP of existing detectors on latent diffusion images from the GenImage benchmark.} Accuracy is computed with a fixed threshold, while AP is computed with respect to the real distribution (ImageNet). Whole network training-based methods (\emph{DRCT, Corvi, Rajan, Corvi$\oplus$, Rajan$\oplus$}) outperform CLIP-based methods (\emph{UFD, ClipDet}). On VQDM-generated images, \emph{Corvi$\oplus$ (Ours)} and \emph{Rajan$\oplus$ (Ours)} demonstrate improvements over their original counterparts. Additionally, on settings where \emph{Corvi}, \emph{Rajan} already perform well, our method performs the same if not slightly better.}
\label{tab:acc_ap_genimg_ldm}
\begin{center}
\begin{small}
\begin{sc}
\renewcommand{\arraystretch}{1.2} % Adjust row spacing
\setlength{\tabcolsep}{2.5pt} % Reduce column spacing
\begin{tabular}{cc}
% Begin the first table
\begin{tabular}{lccccc}
\toprule
\textbf{Accuracy (\%)} & \textbf{MJ} & \textbf{SD1.4} & \textbf{SD1.5} & \textbf{Wukong} & \textbf{VQDM} \\
\midrule
UFD-ProGAN       & 68.17 & 78.60 & 78.60 & 81.05  & 81.34  \\
UFD-LDM          & 56.24 & 63.75 & 63.56 & 71.05  & 85.43  \\
Cozzolino-LDM    & 67.91 & 85.32 & 85.71 & 78.40  & 82.70  \\
DRCT-ConvNext    & 94.43 & 99.37 & 99.19 & 99.25  & 76.84  \\
\midrule
Corvi            & \textbf{99.67} & \textbf{99.99} & \textbf{99.89} & \textbf{99.97} & 80.86  \\
Corvi$\oplus$ (Ours)    & 99.31 & 99.76 & 99.63 & 99.73  & \textbf{98.97}  \\
\midrule
Rajan            & 96.58 & 99.88 & 99.85 & 99.87  & 74.67  \\
Rajan$\oplus$ (Ours)   & 98.68 & 99.83 & 99.79 & 99.79  & 90.02  \\
\bottomrule
\end{tabular} &
% Begin the second table
\begin{tabular}{lccccc}
\toprule
\textbf{AP (\%)} & \textbf{MJ} & \textbf{SD1.4} & \textbf{SD1.5} & \textbf{Wukong} & \textbf{VQDM} \\
\midrule
UFD-ProGAN     & 74.46       & 89.19          & 88.91          & 93.03           & 95.52         \\
UFD-LDM        & 74.61       & 86.56          & 86.19          & 91.34           & 96.65         \\
Cozzolino-LDM  & 87.28       & 96.31          & 96.38          & 93.76           & 95.78         \\
DRCT-ConvNext  & 99.39       & \textbf{99.99} & \textbf{99.98} & \textbf{99.99}  & 96.71         \\
\midrule
Corvi          & \textbf{99.99} & \textbf{99.99} & \textbf{99.98} & \textbf{99.99}  & 98.68         \\
Corvi$\oplus$ (Ours)  & 99.94       & \textbf{99.99} & 99.96          & \textbf{99.99}  & \textbf{99.94} \\
\midrule
Rajan          & 99.54       & \textbf{99.99} & 99.94          & \textbf{99.99}  & 95.11         \\
Rajan$\oplus$ (Ours)  & 99.91       & \textbf{99.99} & 99.96          & \textbf{99.99}  & 99.26         \\
\bottomrule
\end{tabular}
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

\subsection{Spurious Correlations in the Real Distribution Could Influence Fake Features}\label{app:hyp_lim}
In our work, we demonstrated that features associated with the real distribution harm the detectors performance, and proposed a way to mitigate such an issue by ignoring the real features under certain assumptions. However, our method relies on re-training the final layer of the neural network. We demonstrate a way in which such a detector can still suffer from the effects of features associated with the real distribution. We rely on a toy example, hypothesizing such a case.

\subsubsection{A Toy Example}
Let us consider the dataset used to train the detector by \citet{rajan2024effectiveness}. The training data contains real images from COCO and LSUN. The LSUN images are WEBP compressed. The precise details can be found in Section \ref{sec:spur_real}. In the main paper, we demonstrate that the existing detector can associate the presence of WEBP compression artifacts with real images. However, we hypothesize that the detector can actually also associate the absence of WEBP compression artifacts with fake images. 

To explain our intuition, we consider a simple feedforward neural network, with ReLU activations. This network has been trained using the dataset described above. We illustrate the network in Fig \ref{fig:hyp_net}.  

\begin{figure*}[t]
\begin{center}
\centerline{\includegraphics[width=0.6\textwidth]{icml2025/figures/hyp.png}}
\caption{\textbf{Spurious Fake Features.} In this neural network, the circles demonstrate neurons, some neurons are bigger in size for demonstration purpose. For such a case, the detector can associate the absence of a spurious real image artifact, such as WEBP compression with fake images.}
\label{fig:hyp_net}
\end{center}
\vskip -0.2in
\end{figure*}
\paragraph{WEBP Artifact Sensitivity}
Neural networks, such as ResNet-50, exhibit sensitivity to WEBP compression artifacts, suggesting the presence of a neuron that selectively activates in response to images containing such artifacts. Given a ReLU activation function, this neuron outputs a high activation magnitude for WEBP-compressed images from the real distribution, particularly LSUN images, while producing a low or zero activation magnitude for images without WEBP compression, such as COCO images and those from the fake distribution.

\paragraph{Flipping the Activation to Detect Absence of WEBP}
If this activation is multiplied by a negative weight, followed by the addition of an appropriate bias and the application of a ReLU activation, the network can learn a transformed version of this neuron that activates only when WEBP artifacts are absent. This means the network could, in principle, learn to detect images that have \textit{not} undergone WEBP compression by repurposing the same underlying feature.

\paragraph{Combining WEBP and LDM Artifacts to Learn an AND Condition}
Beyond compression artifacts, the network also exhibits sensitivity to generative model artifacts, such as those present in latent diffusion model (LDM)-generated images. A similar mechanism can be hypothesized for LDM artifacts, where a neuron activates in response to their presence. During training, for a fake image, both the absence of WEBP artifacts and the presence of LDM artifacts hold. If the network learns to associate this specific combination with the fake class, it effectively learns an \textit{AND} condition—where a fake image is recognized only when WEBP artifacts are absent and LDM artifacts are present.

\paragraph{Impact on Detector Robustness}
If the detector associates both the presence of LDM-artifacts \textit{AND} the absence of WEBP artifacts with fake images, applying WEBP compression to a fake image during inference could contradict this learned \textit{AND} condition. The addition of WEBP artifacts could weaken the distinguishing signal, reducing the activation of the fake-detecting neuron and thereby lowering the fake score. In fact, such a situation does arise in our experiments.

To illustrate this we revisit some of our previous experiments, in Fig. \ref{fig:webp-spur} from Section \ref{subseq:spur-webp}, we show that excluding WEBP-compressed real images during training can yield strong detector performance, as seen in the method labeled \emph{Rajan (only COCO)}. This suggests that removing WEBP-related biases from the training data allows the model to generalize better, avoiding reliance on compression artifacts as a shortcut for classification. 

In Fig. \ref{fig:webp-spur-fixed}, our method shows improvements over the baseline, demonstrating that explicitly mitigating spurious correlations can enhance detector performance. However, despite this improvement, the detector is still not as robust as \emph{Rajan (only COCO)}. This can be explained by our earlier toy example: if the model has learned an implicit rule where the absence of WEBP artifacts is a necessary condition for an image to be classified as fake, then introducing WEBP compression to fake images weakens the fake classification signal. Consequently, while removing WEBP-compressed real images from training reduces bias, it does not fully eliminate the underlying vulnerability, since the model may still rely on other spurious correlations to distinguish real from fake.


% \subsection{Extension to Vision Transformer based Detectors}
% Our prior experiments built on top of \emph{Corvi} and \emph{Rajan} which used CNN architectures. The main purpose of using CNN architectures was to allow patch-wise training which allows better performance. However, in this section, we test our idea in a Vision Transformer (ViT) \citep{dosovitskiy2020image} setting. We select the training dataset of \emph{Rajan}, and train a detector with the ViT-B-16 backbone. We use random crops of 224x224 during training, and take a centre crop of the same size during inference. We enforce the structure described in Section \ref{sec:lbid} by applying a ReLU activation before the final linear layer. We refer to this version as \emph{Rajan-ViT}. We refer to our version as \emph{Rajan-ViT$\oplus$}. For evaluation we use the same dataset from Section \ref{sec:comparisons}.


\subsection{Improved Detection of GAN-generated Images}\label{app:gan_exp}
Our work in the main paper focused on improving the detection of LDM \citep{rombach2022high} generated images. In this section, we extend the results to the detection of images generated by GANs \citep{goodfellow2020generative}. We consider a baseline ResNet-50 trained using the dataset created by \citet{wang2020cnn}. This dataset consists of 360k real images taken from LSUN and 360k images generated by ProGAN \citep{karras2018progressivegrowinggansimproved}. We use the same training recipe described in Appendix \ref{app:train_deets}, however, we do not employ the random resized crop data augmentation in this case. We train a ResNet-50 on this dataset. We refer to this baseline as \emph{GAN-Baseline}, we re-train the last layer of this detector to ignore real features. We refer to this model as \emph{GAN-Baseline$\oplus$ (Ours)}.

\subsubsection{Improved Robustness to WEBP Compression}
Our dataset uses real images from LSUN. Consequently,  our baseline detector (\emph{GAN-Baseline}) also associates WEBP compression artifacts with the real distribution, similar to the observations recorded in Section \ref{subseq:spur-webp}. We conduct an experiment to compare our retrained detector \emph{GAN-Baseline$\oplus$ (Ours)} with the original detector.

\textbf{Experiment Details}: With this experiment, we intend to measure the sensitivity of the original \emph{GAN-Baseline} to WEBP compression and measure if our method can mitigate these issues. We use the StyleGAN \citep{karras2019style} test set provided by \citet{wang2020cnn}. We sample 500 real images and 500 fake images randomly. We apply different levels of WEBP compression to the fake images and compute the AP, analogous to our measurement in Section \ref{subseq:spur-webp}.

\begin{figure*}[t]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.4\textwidth]{icml2025/figures/webp/webp-gan.png}}
\caption{\textbf{Improved Robustness to WEBP compression (GAN case).} Compared to the original GAN-Baseline, our model displays an improved robustness to WEBP compression.}
\label{fig:webp-gan}
\end{center}
\vskip -0.2in
\end{figure*}

\textbf{Analysis}: We present the results in Fig. \ref{fig:webp-gan}. WEBP compression significantly degrades the performance of the baseline detector. In contrast, our detector demonstrates enhanced robustness to WEBP compression. This provides further evidence suggesting that avoiding reliance on real features can improve robustness against such spurious correlations.

\subsubsection{General Performance}
In this section, we study the generalization ability of the detector in comparison to other baseline detectors. We use the dataset consisting of CNN generated images from \citet{wang2020cnn}. The dataset consists of ProGAN \citep{karras2018progressivegrowinggansimproved}, StyleGAN \citep{karras2019style}, StyleGAN2 \citep{karras2020analyzing}, GauGAN \citep{park2019semantic}, BigGAN \citep{brock2018large}, CycleGAN \citep{zhu2017unpaired}, StarGAN \citep{choi2018stargan} as well as Deepfakes \citep{rossler2019faceforensics++}, super-resolution (SAN) \citep{dai2019second} as well as networks which use a perceptual loss to refine images, such as CRN \citep{chen2017photographic} and IMLE \citep{li2019diverse}. It also consists of high quality GAN-generated faces in WFIR \citep{whichfacereal}. This test set comes with both real and fake images for each generator category. Therefore, we report both accuracy with a 0.5 threshold and the AP.

\begin{table*}[t]
\caption{\textbf{Accuracies of various detectors on CNN-generated images.} In most settings, our method \emph{GAN-Baseline$\oplus$}, outperforms/maintains performance with respect to the original detector. Additionally, on most settings where \emph{GAN-Baseline} already perform well, our method performs the same if not slightly better.}
\label{tab:gan_acc}
\begin{center}
\begin{scriptsize} % Reduced font size
\setlength{\tabcolsep}{3.5pt} % Adjusted column spacing
\begin{sc}
\begin{tabular}{lcccccccccccc}
\toprule
Method                  & ProGAN & StyleGAN & StyleGAN2 & GauGAN & BigGAN & CycleGAN & StarGAN & Deepfake & SAN   & CRN   & IMLE  & WFCIR \\
\midrule
UFD-ProGAN             & 99.81  & 84.92    & 74.96     & \textbf{99.47}  & 95.07  & \textbf{98.33}    & 95.74   & \textbf{68.56}    & 56.62 & 56.58 & 69.10 & 87.20 \\
UFD-LDM                & 93.18  & 83.20    & 83.95     & 90.75  & 87.90  & 94.62    & 84.36   & 54.46    & \textbf{79.22} & 74.33 & 85.00 & 70.35 \\
ClipDet                & 72.96  & 70.49    & 70.86     & 83.94  & 74.10  & 87.28    & 54.95   & 50.87    & 77.62 & 53.10 & 53.69 & 53.10 \\
GAN-Baseline           & \textbf{100.00} & 99.52    & 93.34     & 95.53  & 96.65  & 93.14    & 99.54   & 55.43    & 54.33 & \textbf{99.96} & \textbf{99.87} & \textbf{100.00} \\
GAN-Baseline$\oplus$  & \textbf{100.00} & \textbf{99.82}    & \textbf{98.96}     & 94.52  & 95.02  & \textbf{93.30}    & \textbf{99.77}   & 68.17    & 75.57 & 86.75 & 86.75 & 99.70 \\
\bottomrule
\end{tabular}
\end{sc}
\end{scriptsize}
\end{center}
\vskip -0.1in
\end{table*}


\begin{table*}[t]
\caption{\textbf{AP of various detectors on CNN-generated images.} In most settings, our method \emph{GAN-Baseline$\oplus$}, outperforms/maintains performance with respect to the original detector. Additionally, on most settings where \emph{GAN-Baseline} already perform well, our method performs the same if not slightly better.}
\label{tab:gan_ap}
\begin{center}
\begin{scriptsize} % Reduced font size
\setlength{\tabcolsep}{3.5pt} % Adjusted column spacing
\begin{sc}
\begin{tabular}{lcccccccccccc}
\toprule
Method                  & ProGAN & StyleGAN & StyleGAN2 & GauGAN & BigGAN & CycleGAN & StarGAN & Deepfake & SAN   & CRN   & IMLE  & WFCIR \\
\midrule
UFD-ProGAN             & 99.99  & 97.56    & 97.89     & \textbf{99.98}  & 99.26  & \textbf{99.79}    & 99.37   & 81.76    & 78.80 & 96.58 & 98.60 & 97.27 \\
UFD-LDM                & 99.68  & 93.36    & 92.56     & 99.78  & 98.04  & 99.83    & 97.51   & 70.52    & 90.40 & 83.96 & 93.88 & 96.77 \\
ClipDet                & 91.29  & 78.13    & 76.47     & 90.19  & 80.04  & 89.51    & 81.07   & 65.98    & 85.65 & 64.63 & 55.66 & 64.63 \\
GAN-Baseline           & \textbf{100.00} & \textbf{99.99}    & \textbf{99.99}     & 99.78  & \textbf{99.74}  & 98.73    & 99.98   & 94.27    & 88.79 & \textbf{99.99} & \textbf{99.99} & \textbf{100.00} \\
GAN-Baseline$\oplus$  & \textbf{100.00} & \textbf{99.99}    & \textbf{99.99}     & 99.30  & 99.03  & 98.93    & \textbf{99.99}   & \textbf{95.45}    & \textbf{94.46} & \textbf{99.99} & \textbf{99.99} & 99.99 \\                            \\
\bottomrule
\end{tabular}
\end{sc}
\end{scriptsize}
\end{center}
\vskip -0.1in
\end{table*}

We report the accuracy and AP scores in Tables \ref{tab:gan_acc} and \ref{tab:gan_ap}, respectively. Consistent with our findings in Table \ref{tab:proc_ap}, we observe that the full-network training paradigm represented by \emph{GAN-Baseline} and \emph{GAN-Baseline $\oplus$ (Ours)} outperforms the CLIP-linear probing paradigm represented by the UFD methods and ClipDet. Our method, \emph{GAN-Baseline$\oplus$}, outperforms the \emph{GAN-Baseline} detector by 5.67 AP on images generated through super-resolution. Since super-resolution operates on real images, we hypothesize that the real features in these images harm the performance of the \emph{GAN-Baseline} detector. However, our method is able to better detect such images since it does not rely on real features. Interestingly, our method shows lower accuracy on the CRN and IMLE test sets. Upon closer inspection, we find that the real distribution in these test sets consists of video game images from the GTA series, which differ significantly from the natural distribution. Notably, some patterns that the detector associates with fake images are also present in these video game images. To verify this, we conduct an experiment similar to the one in Section \ref{app:real_perf}. Except for the class ``Test Real", all other classes use the same images as described in Section \ref{app:real_perf}. We sample the `Test Real' distribution from the real images in the StyleGAN test set. We observe that most of the real images, except for the GTA images, exhibit similar fakeness scores. This shows that our model still does a good job in generalizing to different domains of real images, however, the performance on some unrelated domains can drop. 
\begin{figure*}[t]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.5\textwidth]{icml2025/figures/gan-violins.png}}
\caption{\textbf{Distribution of different kinds of real images.} We observe that most types of real images are assigned a similar value of fakeness. An exception is the GTA-based images which has a relatively higher score indicating the presence of spurious fake features.}
\label{fig:gan-violin}
\end{center}
\vskip -0.2in
\end{figure*}



% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
