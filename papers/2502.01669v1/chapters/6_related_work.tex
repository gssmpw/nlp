\section{Related work}
\subsection{Delayed Feedback}
Addressing the delayed feedback problem is pivotal in CVR tasks.
Existing research can be broadly categorized into offline and online approaches.
For offline models, they often assume that historical data and future data are independently and identically distributed, allowing for the prediction of future conversions based on historical data. DFM \cite{chapelle2014modeling} pioneers the study of the delayed feedback problem, modeling the delay time as an exponential distribution.
Subsequently, NoDeF \cite{yoshikawa2018nonparametric} employs a non-parametric approach to model the delay time. Recent years have witnessed an increasing focus on constructing unbiased estimates of oracle loss.
Notable examples include FSIW \cite{yasui2020feedback}, which creates an unbiased estimation by reducing the weight of potentially negative samples. nnDF \cite{kato2020learning} assumes that samples before a time window have accurate labels and utilized such samples to estimate the oracle loss. ULC \cite{wang2023unbiased} trains a label correction model to predict future reversals in the negative samples.
However, the necessity of training additional auxiliary models limits the model's ability to incorporate newly arrived data, making it challenging to adapt to evolving user interests.

Online methods, in contrast, fine-tune the model with newly arrived data and data with reversed labels\cite{ktena2019addressing}. These approaches usually duplicate fake negatives and re-ingest the true labels of these samples into the training pipeline.
ES-DFM \cite{yang2021capturing} uses elapsed-time sampling to include delayed positive examples in model training;
DEFUSE \cite{chen2022asymptotically} accurately corrects the importance weights of immediate positives, fake negatives, real negatives, and delayed positives, leveraging a bi-distribution framework; DDFM\cite{dai2023dually} introduces dual unbiased CVR estimators to separately handle newly observed and duplicated early data, addressing biases from rapid data distribution shifts. However, duplication of delayed conversions often confuses the model and leads to unsatisfactory side effects, and most methods still require training additional auxiliary variables. 


\subsection{Influence Function}
The influence function, a tool for estimating the impact of individual training points on model predictions, has been introduced in machine learning.
Koh \etal \cite{koh2017understanding} utilized the influence function as a means to understand black-box predictions of deep models. Since this seminal work, the influence function has become widely employed to enhance model interpretability \cite{chhabra2024data,basu2020second}, fairness \cite{li2022achieving,wang2024fairif}, data management \cite{hara2019data}, and machine unlearning \cite{chen2024fast, liu2023muter}, and the downstream application like recommender systems\cite{cheng2018explaining, guo2024counterfactual,li2023selective}. Koh \etal \cite{koh2019accuracy} extend the traditional influence function approach by quantifying the effects of training data groups on model robustness. TracIn \cite{pruthi2020estimating} explores influence functions for identifying mislabeled data points and understanding the training process. IF4URec \cite{yu2020influence} leverages influence functions to correct bias in recommendation models, improving performance by reweighting samples based on their influence.
However, Basu \etal \cite{basu2020influence} propose that there are still great challenges in applying influence functions to deep learning models. Although FastIF \cite{guo2020fastif} tackles computational challenges to some extent, it does not touch the core of the inverse Hessian-vector product computation, making it infeasible to compute parameter changes on large-scale datasets.