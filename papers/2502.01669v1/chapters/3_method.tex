\section{Method}
The CVR prediction with delayed feedback problem urgently requires addressing the challenges from label reversal and new data integration over time. Although retraining the model provides a straightforward solution, the high computational cost and low retraining frequency make it impractical to meet the requirements in real applications. In this paper, we propose a novel IF-DFM paradigm that can address the challenges in a unified manner without retraining. We further propose a new stochastic algorithm to address the computational challenges in the influence function evaluation.
\subsection{IF-DFM}
Our core idea to model both label reversal and new data integration as data perturbations and the huge scale training data involved in the CVR prediction problem make the influence function a natural and elegant tool for estimating the changes of model parameters without retraining the model. Next, we present our IF-DFM paradigm in detail.

\subsubsection{Label Reversal}
The delay between conversion behavior and click behavior is common in CVR prediction problems, where the label reversal may happen for some fake negative samples after the training phase. The label reversal phenomenon thus can be naturally modeled as the data perturbation. More specifically,  if the label reversal happens for a training sample $z_i$, it is equivalent to perturb $z_i$ to $z_i^\delta = (x_i,y_i^{O}+\delta)$. Since the observed positive samples are with accurate labels, the perturbation $\delta = 1$ for the CVR prediction problem. Without loss of generality, we consider the case where the label reversal happens for a training sample $z_i$. We are interested in the perturbed empirical risk minimization problem by reweighting the perturbed training sample $z_i$:
\begin{equation}
\label{eq: perturbed-ER}
\widehat{\theta_\delta}(\epsilon) \in \mathop{\arg\min}_{\theta} \left( \mathcal{L}_{{V}}(\theta)+\epsilon \mathcal{L}_{{BCE}}\left(z_i^{\delta},\theta\right)-\epsilon  \mathcal{L}_{{BCE}}\left(z_i, \theta\right) \right).
\end{equation}
Note that the right-hand side of \eqref{eq: perturbed-ER} coincides with the retrain loss \eqref{retrain} if we take $\epsilon = 1/n$. When $\epsilon \approx 0$ (which is true if $\epsilon = 1/n$ and $n$ is sufficiently large), following the derivation of influence function (e.g., see \cite{koh2017understanding}), we can obtain that  
\begin{equation}
\label{eq: delta-estimate-delay}
\begin{aligned}
\left.\frac{d \widehat{\theta_\delta}}{d \epsilon}\right|_{\epsilon=0} =
& -\left(\nabla_\theta^2 \mathcal{L}_{V} (\hat{\theta}) \right)^{-1}\left(\nabla_\theta \mathcal{L}_{{BCE}}\left(z^\delta_i, \hat{\theta}\right)-\nabla_\theta \mathcal{L}_{{BCE}}(z_i, \hat{\theta})\right),
\end{aligned}
\end{equation}
where $\hat{\theta}$ is a minimizer of \eqref{vanilla} (which is available if the Vanilla model is trained).

Therefore, after aggregating the gradients of all label-reversed samples, we can estimate the changes of the model parameter from \eqref{eq: delta-estimate-delay} by taking $\epsilon = 1/n$ as
\begin{equation}
\small
\label{eqc}
\Delta \theta_{delay} \approx \frac{1}{n}\left(\nabla_\theta^2 \mathcal{L}_{{V}}(\hat{\theta})\right)^{-1}\left(\sum_{j\in J} \nabla_\theta \mathcal{L}_{{BCE}}\left(z_j, \hat{\theta}\right)-\sum_{j \in J} \nabla_\theta \mathcal{L}_{{BCE}}\left(z_j^\delta, \hat{\theta}\right)\right),
\end{equation}
where $J$ is the index set of samples that clicked before $T$ and converted during the $[T, T']$ interval (i.e., samples with label reversal). 

\subsubsection{Newly Arrived Behavioral Data Integration}
Conventionally, the influence function focuses on how model parameters or test sample loss change when a sample is deleted or modified. In our proposed IF-DFM paradigm, we also handle the new data integration for those behavioral data arriving during the $[T, T']$ time interval. To achieve this goal, without loss of generality, we consider the following new perturbed loss function for a new sample $z = (x, y)$ and $0 \leq \epsilon < 1$:
\begin{equation}
\label{eq: perturbed-loss-add}
\widehat{\theta_z}(\epsilon) \in \mathop{\arg\min}_{\theta} \left((1 - \epsilon)\mathcal{L}_{{V}}(\theta)+\epsilon \mathcal{L}_{{BCE}}\left(z,\theta\right)\right).
\end{equation}
Note that we can recover the retrain loss for integrating the additional new sample $z$ by taking $\epsilon = 1/(n+1)$.

By careful calculations (which can be found in Appendix \ref{appendix:derivation}), we can find an approximation to $\Delta_{z, \epsilon} = \widehat{\theta_z}(\epsilon) - \hat{\theta}$ for sufficiently small $\epsilon > 0$ as
\begin{equation}
\label{eq: Delta_incremental}
    \Delta_{z, \epsilon} \approx -\frac{\epsilon}{1 - \epsilon}\left(\nabla^2_{\theta}\mathcal{L}_V(\hat{\theta})\right)^{-1}\nabla_{\theta}\mathcal{L}_{BCE}(z, \hat{\theta}).
\end{equation}
Therefore, for sufficiently large $n$, we obtain an approximation to the change of model parameter for integrating the new data sample $z$ as
\begin{equation}
\label{delta-add}
    \Delta\theta_{add} = \widehat{\theta_z}\left(\frac{1}{n+1}\right) - \hat{\theta} \approx -\frac{1}{n}\left(\nabla^2_{\theta}\mathcal{L}_V(\hat{\theta})\right)^{-1}\nabla_{\theta}\mathcal{L}_{BCE}(z, \hat{\theta}).
\end{equation}

Considering both the label reversed data and the newly arrived data, the equation for the total parameter change can be given as
\begin{equation}
\Delta \theta_{total} = \Delta \theta_{delay} + \Delta \theta_{add}.
\end{equation}
More specifically, we need to estimate 
\begin{equation}
\begin{aligned}
\Delta \theta_{total} & \approx \left(\nabla_\theta^2 \mathcal{L}_{{V}}(\hat{\theta})\right)^{-1}\left(\frac{1}{n}\sum_{j \in J} \nabla_\theta \mathcal{L}_{{V}}\left(z_j, \hat{\theta}\right) - \frac{1}{n}\sum_{j \in J} \nabla_\theta \mathcal{L}_{{V}}\left(z_j^\delta, \hat{\theta}\right) \right.\\
 & \left. - \frac{1}{n}\sum_{k \in K} \nabla_\theta \mathcal{L}_{{V}}\left(z_k, \hat{\theta}\right)\right),
\label{total}
\end{aligned}
\end{equation}
where $J$ and $K$ are the index sets for the label reversed data samples and newly arrived data samples, respectively.

Addressing problem \eqref{total} poses significant computational challenges. When the Hessian matrix $\nabla_{\theta}^2\mathcal{L}_{V}(\hat{\theta})$ is positive definite, we can apply the conjugate gradient (CG) method to estimate $\Delta \theta_{total}$, where only Hessian-vector products are required. For $\theta \in \mathbb{R}^p$ (where $p$ is usually huge in applications), the CG method requires at most $p$ iterations to calculate the right-hand side of \eqref{total} and it usually requires much fewer iterations to obtain an approximate solution if the condition number of $\nabla_{\theta}^2\mathcal{L}_{V}(\hat{\theta})$ is close to $1$ \cite{nocedal1999numerical}. However, the CG method typically requires full-batch gradient and Hessian matrix computations, making it impractical for large-scale datasets. To address the computational challenges of the CG method, an approximation of $\left(\nabla^2_{\theta}\mathcal{L}_V(\hat{\theta})\right)^{-1}$ via its truncated power series has been widely used to estimate $\Delta\theta_{total}$ \cite{koh2017understanding}. However, its accuracy is often compromised. Finding a more precise and efficient solution for \eqref{total} remains a challenge. Next, we will address this challenge by designing an efficient and scalable stochastic algorithm.

\subsection{An Efficient and Scalable Method for Calculating $\Delta\theta_{total}$}
In this section, we will address the computational challenges for calculating $\Delta\theta_{total}$. Our core idea is to compute $\Delta\theta_{total}$ by converting $\eqref{total}$ into an equivalent finite-sum optimization problem. Consequently, we can apply popular scalable algorithms, such as the stochastic gradient descent \cite{robbins1951stochastic} and its variants \cite{duchi2011adaptive,kingma2014adam,johnson2013accelerating}, to calculate $\Delta\theta_{total}$ efficiently. The definition of $\Delta\theta_{total}$ implies that it is a solution to the linear system 
\begin{equation}
\label{eq: Equiv-ls}
    \nabla^2_{\theta}\mathcal{L}_V(\hat{\theta}) \Delta = b
\end{equation}
with
\begin{equation}
\label{def-b}
\small
b = \frac{1}{n}\sum_{j \in J} \nabla_\theta \mathcal{L}_{{V}}\left(z_j, \hat{\theta}\right) - \frac{1}{n}\sum_{j \in J} \nabla_\theta \mathcal{L}_{{V}}\left(z_j^\delta, \hat{\theta}\right) - \frac{1}{n}\sum_{k \in K} \nabla_\theta \mathcal{L}_{{V}}\left(z_k, \hat{\theta}\right).
\end{equation}
Since $\hat{\theta}$ is a minimizer of \eqref{vanilla}, which must satisfy the second-order necessary optimality condition \cite{nocedal1999numerical}. In other words, the matrix $\nabla^2_{\theta}\mathcal{L}_V(\hat{\theta})$ is symmetric and positive semidefinite. Therefore, $\Delta\theta_{total}$ is a minimizer of the following convex quadratic optimization problem:
\begin{equation}
\mathop{\min}_{\Delta} ~ F(\Delta) := \frac{1}{2}\Delta^{\top} \nabla^2_{\theta}\mathcal{L}_V(\hat{\theta}) \Delta - \langle b, \Delta \rangle.
\end{equation}
It follows from the finite-sum structure of $\mathcal{L}_V$ that
\begin{equation}
\label{eq: finite-sum}
    F(\Delta) = \frac{1}{n}\sum_{i = 1}^n f_i(\Delta),
\end{equation}
where
\begin{equation}
\label{eq: fun-fi}
f_i(\Delta) = \frac{1}{2} \Delta^{\top} \nabla_{\theta}^2\mathcal{L}_{BCE}(z_i, \hat{\theta}) \Delta + \langle b, \Delta \rangle.
\end{equation}
Therefore, we have converted the calculation of $\Delta\theta_{total}$ equivalently to minimizing an optimization problem with a finite-sum objective function. Note that the function value and the gradient of $f_i$ can be efficiently evaluated via the built-in auto-differentiation and the Hessian vector product modules implemented by popular frameworks, such as PyTorch. Consequently, we can apply efficient and scalable optimization algorithms to calculate $\Delta\theta_{total}$. In particular, we choose ADAM \cite{kingma2014adam} in this paper. We summarize the details in Algorithm \ref{alg1}.

\begin{algorithm}[htbp]
\caption{An efficient algorithm to calculate $\Delta\theta_{total}$.}\label{alg1}
\begin{algorithmic}[1]
\STATE {\textbf{Input:}} Training data $\mathcal{D}$; Label reversed data $z_j \rightarrow z_j^{\delta}$; Newly arrived data $z_k$; Trained vanilla model $\hat{\theta}$.
\STATE{\textbf{Output:}} The model parameter change $\Delta\theta_{total}$.

\STATE{\textbf{Initialization:}} Construct $b$ in \eqref{def-b} and the function $f_i$.
\STATE Call ADAM to minimize \eqref{eq: finite-sum} and obtain $\Delta^*$.
 \STATE Return $\Delta\theta_{total} = \Delta^*$.
\end{algorithmic}
\end{algorithm}
\subsection{Discussions}
We include some necessary discussions before we close this section. First of all, we want to compare our proposed perturbed loss function \eqref{eq: perturbed-loss-add} to the commonly used perturbed loss in the form of \eqref{eq: if-newtheta}.
An important motivation for us to propose the perturbed loss function \eqref{eq: perturbed-loss-add} is that it can fully recover the retrain loss for a newly arrived data integration by picking $\epsilon = 1/(n+1)$, which is not the case for \eqref{eq: if-newtheta} by choosing some particular value of $\epsilon$. This issue has been discussed in \cite{grosse2023studying}, where the authors mentioned that the perturbed loss \eqref{eq: if-newtheta} can handle the newly arrived data integration by assuming the new data $z$ matches an existing training example. Indeed, we realize that we can choose $\epsilon = 1/n$ and multiplying $\widehat{\mathcal{L}}(z; \theta, \epsilon)$ by $n/(n+1)$ to recover the retrain loss. Since it will derive the same approximation $\Delta\theta_{add}$ as in \eqref{delta-add}, we will use the more natural perturbed loss function \eqref{eq: perturbed-loss-add} for handling newly arrived data integration.

We also want to discuss briefly the assumption that $\nabla^2_{\theta}\mathcal{L}_V(\hat{\theta})$ is invertible. As aforementioned, by the second-order necessary condition, $\nabla^2_{\theta}\mathcal{L}_V(\hat{\theta})$ must be symmetric and positive semidefinite \cite{nocedal1999numerical}. If the second-order sufficient condition (SOSC) holds for $\mathcal{L}_V$ at $\hat{\theta}$, then $\nabla^2_{\theta}\mathcal{L}_V(\hat{\theta})$ is symmetric and positive definite, which is invertible. It is worthwhile mentioning that the SOSC is possible to hold at $\hat{\theta}$ even if the loss function is nonconvex. In our implementation, we add a regularization term $\lambda/2\|\Delta\|^2$ to \eqref{eq: finite-sum} to stabilize the computation. Indeed, this regularization term will contribute to finding the minimum norm solution to \eqref{eq: Equiv-ls} \cite{facchinei2003finite}. Other assumptions in the influence function have been discussed in \cite{koh2017understanding,schioppa2024theoretical}. 