\section{Experiment}
In this section, we conduct comprehensive experiments to evaluate the effectiveness and efficiency of our proposed method. Our experiments aim to investigate the following research questions:
\begin{itemize}[left=0pt]
    \item \textbf{RQ1}: How does our method's performance compare to state-of-the-art methods in CVR prediction?
    \item \textbf{RQ2}: Can our method adapt to dynamic changes in user interests over time?
    \item \textbf{RQ3}: How efficient is our method in calculating parameter changes?
\end{itemize}

\subsection{Experimental Settings}
\subsubsection{Datasets.}
To evaluate the effectiveness of our proposed method, we utilize two large-scale datasets: the Criteo dataset and the Taobao User Behavior dataset. The statistics of these datasets are summarized in Table \ref{dataset}.
\begin{table}[htbp]
\begin{center}
\setlength{\tabcolsep}{5pt}
\vspace{-5pt}
\caption{Statistics of Criteo and Taobao datasets.}
\vspace{-10pt}
\small
\label{dataset}
\begin{tabular}{ccc|ccc}
\toprule
{Name}    & {\# clicks} & {CVR} & {Name}   & {\# clicks} & {CVR} \\ \midrule
{Criteo}   &    4,019,339       &  0.2227   & {Taobao}     &   8,544,800        &   0.0100  \\ \bottomrule
\end{tabular}
\end{center}
\vspace{-10pt}
\end{table}

\textbf{Criteo}\footnote{\url{https://labs.criteo.com/2013/12/conversion-logs-dataset/}}: This public dataset consists of click data collected from Criteo's live traffic. Each sample includes the timestamp of the click, whether a conversion occurred, hashed categorical features, and continuous features. We use a total of $3$ million click samples from $14$ days for training.

\textbf{Taobao}\footnote{\url{https://tianchi.aliyun.com/dataset/dataDetail?dataId=649&userId=1&lang=en-us}}: This dataset contains user interaction data collected from Taobao. It includes user behaviors such as page views and purchases (defined as conversion behavior). Each interaction is accompanied by timestamps and relevant features. 

Following common settings in previous offline research \cite{wang2023unbiased,yasui2020feedback}, we employ a temporal partitioning approach to divide each dataset into training, validation, and test sets. Specifically, we use data from the interval $\left[T^{\prime}-d_\text{test}, T^{\prime}\right]$ for validation and data from $\left[T^{\prime}, T^{\prime}+d_\text{test}\right]$ for testing, where $d_\text{test}$ represents the size of the validation/test set (with $d_\text{test}=1$ for Criteo and $d_\text{test}=0.1$ for Taobao).  The interval $\left[T, T^{\prime}\right]$ covers a period of $c$ days. We will explore the value of $c$ in the following sections. Such a setting enables us to assess the CVR model's ability to capture evolving user interests.

\subsubsection{Metrics.}
We employ three widely-used metrics \cite{wang2023unbiased, dai2023dually} to evaluate CVR prediction performance:
\begin{itemize}[leftmargin=*]
    \item \textbf{AUC (Area Under the ROC Curve)}: It measures how well the model ranks positive versus negative samples. 
    \item \textbf{PRAUC (Precision-Recall AUC)}: It assesses model performance across different precision and recall thresholds. 
    \item \textbf{LL (Log Loss)}: It evaluates prediction accuracy by the logarithmic loss of probabilities. Lower LL means predictions are closer to actual labels.
\end{itemize}
We also report the relative improvements (RI) of these metrics. For a method $f$ on a metric $M$, $\mathrm{RI}_M$ is calculated as:
\begin{equation}
\mathrm{RI}_M=\frac{M(f)-M( Vanilla )}{M( Retrain )-M ( Vanilla )}.
\end{equation}
This allows us to determine \textbf{RI-AUC}, \textbf{RI-PRAUC}, and \textbf{RI-LL}, highlighting how each method improves relative to the Vanilla method, based on the maximum possible improvement indicated by the Retrain model.

\subsubsection{Baselines.}
We select $13$ state-of-the-art methods encompassing both offline and online approaches for comparison:

\noindent\textbf{- OFFLINE methods}:
\begin{itemize}[left=0pt]
    \item \textbf{Vanilla}: This baseline model is trained solely on data with observed conversion labels, serving as the lower bound for performance metrics.
    \item \textbf{Retrain}: It updates labels with conversions occurring before the testing phase and retrains the model, thereby providing the performance upper bound.
    \item \textbf{DFM \cite{chapelle2014modeling}}: It is trained with delayed feedback loss and introduces a probabilistic model to capture conversion delay.
    \item \textbf{FSIW \cite{yasui2020feedback}}: It incorporates FSIW loss with pre-trained auxiliary models.
    \item \textbf{nnDF \cite{kato2020learning}}: It is trained with  nnDF loss.
    \item \textbf{ULC \cite{wang2023unbiased}}: 
    This method employs an alternating training process between a label correction model and a CVR model to achieve the unbiased estimation of oracle loss. 
\end{itemize}

\noindent \textbf{- ONLINE methods}:
\begin{itemize}[leftmargin=*]
    \item \textbf{Pretrain}: It is trained using the Vanilla loss with historical data, serving as the base model for fine-tuning.
    \item \textbf{Retrain-online}: It retrained CVR model with true label information and newly arrived data.
    \item \textbf{FNC \cite{ktena2019addressing}}: This method integrates duplicate delayed positive samples when refining the model with the newly arrived data.
    \item \textbf{FNW \cite{ktena2019addressing}}: Different from FNC, it uses calibration loss for fake negatives to more accurately reflect their potential to become positive later.
    \item \textbf{DDFM \cite{dai2023dually}}: It uses the DDFM loss during training and fine-tunes the pretrained model by incorporating both delayed positive samples and the latest behavior data.
    \item \textbf{ES-DFM \cite{yang2021capturing}}: It introduces elapsed-time sampling for unbiased CVR estimation.
    \item \textbf{DEFUSE \cite{chen2022asymptotically}}: It leverages a dynamic weight adjustment mechanism to counteract the effects of delayed feedback.
\end{itemize}

Following \cite{wang2023unbiased}, four traditional models that focus on feature interactions are chosen as backbones:
\begin{itemize}[leftmargin=*]
    \item \textbf{MLP}: It is the basic fully connected neural networks.
    \item \textbf{DeepFM \cite{guo2017deepfm}}: It integrates factorization machines with deep neural networks.
    \item \textbf{AutoInt \cite{song2019autoint}}: It utilizes multi-head self-attention mechanisms to capture complex feature interactions.
    \item \textbf{DCNV2 \cite{wang2021dcn}}: It employs a combination of deep and cross networks to model both explicit and implicit feature relationships.
\end{itemize}

\subsubsection{Implementation Details.}
The MLP model's hidden layer dimensions are set to $[256, 256, 128]$, with ReLU as the activation function. For the AutoInt model, we implement a configuration with $3$ layers, $2$ attention heads, and an attention size of $64$. The DCNV2 model utilizes a stacked structure with a single cross-layer. We use the Adam optimizer, tuning learning rates in the range of [1e-3, 5e-4, 1e-4, 5e-5, 1e-5, 5e-6, 1e-6], and L2 regularization coefficients in [1e-7, 1e-6, 1e-5, 1e-4]. Except for nnDF which is trained on the entire dataset, all methods use a batch size of $1024$. To mitigate randomness, we conduct each experiment $5$ times with different random seeds, and report the averaged results. 

In the offline setting, IF-DFM leverages the influence function to estimate $\Delta\theta_{delay}$ and adjust the Vanilla model's parameters. In the online setting, we use the offline Vanilla model as the pretrained model. For methods requiring auxiliary models or variables (\eg DDFM), we follow their pretraining methods. The model parameters are modified based on the approximate $\Delta\theta_{total}$.

\subsection{Performance Comparison (RQ1)}
\begin{table*}[htbp]
\centering
\setlength{\tabcolsep}{10pt} 
\caption{Offline experimental results on Criteo dataset using four backbones: MLP, DeepFM, DCNV2, and AutoInt.  The best results in each case are marked in bold. The superscripts ** indicate $p \leq 0.05$ for the t-test of IF-DFM vs. the best baseline.}
\label{main}
\small
\begin{tabular}{c|c|cccccc}
\toprule
{Backbone}                 & {Method}  & \multicolumn{1}{c}{{AUC ↑}} & \multicolumn{1}{c}{{PRAUC ↑}} & \multicolumn{1}{c}{{LL ↓}} &\multicolumn{1}{c}{{RI-AUC ↑}} & \multicolumn{1}{c}{{RI-PRAUC ↑}} & {RI-LL ↑}         \\ \midrule
\multirow{7}{*}{{MLP}}     & {Vanilla} & 0.8353                     & 0.6398                       & 0.4187                    & 0.000                         & 0.000                           & 0.000           \\
                         & {Retrain} & 0.8419                     & 0.6513                       & 0.3930                    & 1.000                         & 1.000                           & 1.000           \\ \cline{2-8} 
                         &  {DFM}     & 0.8355                     & 0.6409                       & 0.4152                    & 0.0303                        & 0.0957                          & 0.1362          \\
                         & {FSIW}    & 0.8369                     & 0.6433                       & 0.4020                    & 0.2424                        & 0.3043                          & 0.6498          \\
                         & {nnDF}    & 0.6887                     & 0.4029                       & 0.5639                    & -22.21                        & -20.60                          & -5.670          \\
                         & {ULC}     & 0.8343                     & 0.6412                       & 0.3994                    & -0.1515                       & 0.1217                          & 0.7510          \\
                         & \textbf{IF-DFM (Ours)}   &  \textbf{0.8411**}               & \textbf{0.6491**}               & \textbf{0.3958**}           & \textbf{0.8788**}               & \textbf{0.8087**}                 & \textbf{0.9662**} \\ \midrule
\multirow{7}{*}{{DeepFM}} & {Vanilla} & 0.8368                     & 0.6411                       & 0.4168                    & 0.000                         & 0.000                           & 0.000           \\
                         & {Retrain} & 0.8418                     & 0.6509                       & 0.3926                    & 1.000                         & 1.000                           & 1.000           \\ \cline{2-8} 
                         & {DFM}     & 0.8381                     & 0.6434                       & 0.4065                    & 0.260                         & 0.2347                          & 0.4256          \\
                         & {FSIW}    & 0.8378                     & 0.6436                       & 0.4023                    & 0.200                         & 0.2551                          & 0.5992          \\
                         & {nnDF}    & 0.7025                     & 0.4146                       & 0.5651                    & -26.86                        & -23.11                          & -6.128          \\
                         & {ULC}     & 0.8376                     & 0.6446                       & 0.3982                    & 0.160                         & 0.3571                          & 0.7686          \\
                         & \textbf{IF-DFM (Ours)}    & \textbf{0.8412**}            & \textbf{0.6492**}              & \textbf{0.3962**}           & \textbf{0.8800**}               & \textbf{0.8265**}                 & \textbf{0.8512**} \\ \midrule
\multirow{7}{*}{{AutoInt}} & {Vanilla} & 0.8361                     & 0.6401                       & 0.4174                    & 0.000                         & 0.000                           & 0.000           \\
                         & {Retrain} & 0.8415                     & 0.6503                       & 0.3929                    & 1.000                         & 1.000                           & 1.000           \\ \cline{2-8} 
                         & {DFM}     & 0.8374                     & 0.6426                       & 0.4091                    & 0.2407                        & 0.2451                          & 0.3388          \\
                         & {FSIW}    & 0.8376                     & 0.6442                       & 0.4026                    & 0.2778                        & 0.4020                          & 0.6041          \\
                         & {nnDF}    & 0.6820                     & 0.3770                       & 0.5789                    & -28.54                        & -25.79                          & -6.592          \\
                         & {ULC}     & 0.8374                     & 0.6443                       & 0.3987                    & 0.2407                        & 0.4118                          & 0.7633          \\
                         & \textbf{IF-DFM (Ours)}    & \textbf{0.8404**}            & \textbf{0.6475**}              & \textbf{0.3965**}           & \textbf{0.7963**}               & \textbf{0.7255**}                 & \textbf{0.8530**} \\ \midrule
\multirow{7}{*}{{DCNV2}}  & {Vanilla} & 0.8370                     & 0.6418                       & 0.4174                    & 0.000                         & 0.000                           & 0.000           \\
                         & {Retrain} & 0.8417                     & 0.6508                       & 0.3927                    & 1.000                         & 1.000                           & 1.000           \\ \cline{2-8} 
                         & {DFM}     & 0.8372                     & 0.6422                       & 0.4087                    & 0.0426                        & 0.044                           & 0.3522          \\
                         & {FSIW}    & 0.8388                     & 0.6456                       & 0.3997                    & 0.3830                        & 0.422                           & 0.6045          \\
                         & {nnDF}    & 0.6850                     & 0.3878                       & 0.5728                    & -32.34                        & -28.22                          & -6.291          \\
                         & {ULC}     & 0.8368                     & 0.6441                       & 0.3989                    & -0.0426                       & 0.2556                          & 0.7490          \\
                         & \textbf{IF-DFM (Ours)}    & \textbf{0.8410**}            & \textbf{0.6501**}              & \textbf{0.3985**}           & \textbf{0.8511**}               & \textbf{0.9222**}                 & \textbf{0.7652**}  \\ \bottomrule
\end{tabular}
\end{table*}

The offline experimental results on criteo dataset across different backbones are shown in Table \ref{main}. From these results, we can obtain the following observations:
\begin{itemize}[leftmargin=*]
    \item Retrain consistently outperforms Vanilla, indicating that the delayed feedback problem indeed hinders model performance. Both FSIW and DFM exhibit enhancements over Vanilla, with FSIW demonstrating superior performance. This can be ascribed to FSIW's pre-training of supplementary auxiliary models. Conversely, nnDF underperforms due to its dependency on global computational information, necessitating the utilization of the full data during model updates.
    \item ULC's performance is suboptimal because it requires training additional label correction model, which depends heavily on high-quality historical data and cannot effectively adapt to changes in user interests.
    \item IF-DFM works significantly better than other baselines across all metrics and backbones, highlighting its effectiveness in addressing the delayed feedback problem. Specifically, IF-DFM achieves the average improvement of $0.55\%$ in the AUC metric,  $1.29\%$ in the PRAUC metric, and $4.99\%$ in the LL metric. Existing works from Google \cite{cheng2016wide,wang2017deep} and Microsoft \cite{ling2017model} suggest that a $0.1\%$ increase in AUC can yield significant improvements and tangible benefits in real-world applications. 
    % Note that a gain of merely $0.1\%$ in offline AUC is deemed significant and can translate into substantial enhancements in online performance for industrial applications \cite{ma2018entire,yang2021capturing}.
    \item In terms of relative improvements, IF-DFM closely approximates the performance of Retrain. Notably, the relative improvements for AUC, PR AUC, and LL are $85.16\%$, $82.07\%$, and $85.89\%$, respectively, demonstrating the effectiveness of IF-DFM.
\end{itemize}

Compared to the Criteo dataset, the Taobao dataset exhibits a lower conversion rate. The experimental results of IF-DFM on the Taobao dataset are shown in Figure \ref{fig:both_images}. We can observe that our method surpasses the current best baseline in most cases. Specifically, the AUC increases by $0.28\%$, and the PRAUC increases by $0.94\%$. FSIW requires pre-training an additional model to estimate the importance weights that correct for the label distribution shift, making its short-term results better than retraining. In contrast, our method approximates retraining by incorporating real label information. However, as discussed in the following experiments, IF-DFM will better adapt to changes in user interests over time.

\begin{figure}[t]
  \centering
  \begin{minipage}[b]{0.225\textwidth}
    \centering
\includegraphics[width=\textwidth]{figures/auc.pdf}
\vspace{-10pt}
    \caption*{(a) AUC Results}
  \end{minipage}
  \begin{minipage}[b]{0.225\textwidth}
    \centering
\includegraphics[width=\textwidth]{figures/prauc.pdf}
    \caption*{(b) PRAUC Results}
  \end{minipage}
  \vspace{-10pt}
  \caption{Offline experimental results on Taobao dataset using MLP as the backbone.}
  \label{fig:both_images}
  \vspace{-15pt}
\end{figure}

\subsection{Effectiveness of Adapting to Evolving User Interest (RQ2)}
The evolution of user interests over time is a critical factor in CVR prediction. Models that are not timely updated may fail to reflect the current user preferences, and consequently diminishing the quality of predictions.
In Table \ref{table:c}, we present the experimental results of various methods \wrt parameter $c$, which represents the time interval between the test and training sets. 

Due to space constraints, we only present the results where MLP is utilized as the backbone architecture, while other backbones show similar results. From these results, we can draw the following conclusions:
\begin{itemize}[leftmargin=*]
    \item There is a notable degradation in the performance of both ULC and DFM methods as time progresses. Notably, when $c=14$, their performance falls below that of the Vanilla method.
    FSIW shows the potential to adapt to changes in user interests by pre-training extra models to calculate importance weights. ULC, which relies on counterfactual deadlines to correct training labels, appears constrained in its ability to model beyond the training data, resulting in limited generalization.
     \item 
     The performance gap between IF-DFM and other baseline methods widens as $c$ increases. This underscores the effectiveness of our proposed method in accommodating dynamic user interests.
     To be more specific, IF-DFM estimates the influence of injecting new behavior data to calculate the parameter changes, thereby approximating the effect of retraining.
\end{itemize}

\begin{table}[htbp]
\caption{Offline experimental results on the Criteo dataset for different values of $c$, with MLP as the backbone. The best results in each case are marked in bold.}
\begin{tabular}{cc|ccc}
\toprule
    \multicolumn{2}{c|}{\textbf{}}                                            & {AUC ↑}                & {PRAUC ↑}              & {LL ↓}                 \\ \midrule
    \multicolumn{1}{c|}{}                                  & {Vanilla} & 0.8427                      & 0.6479                      & 0.4085                      \\
    \multicolumn{1}{c|}{}                                  & {Retrain} & {0.8464} & {0.6541} & {0.3879} \\
     \cline{2-5}
    \multicolumn{1}{c|}{}                                  & {nnDF}    & 0.6840                        & 0.3979                        & 0.5565                        \\
    \multicolumn{1}{c|}{}                                  & {DFM}     & 0.8412                        & 0.6469                        & 0.4064                        \\
    \multicolumn{1}{c|}{}                                  & {FSIW}    & 0.8441                        & 0.6508                        & 0.3898                        \\
    \multicolumn{1}{c|}{}                                  & {ULC}     & 0.8445                        & 0.6513                        & 0.3894                      \\
    \multicolumn{1}{c|}{\multirow{-7}{*}{{$c$ = 5}}}  & \textbf{Ours}    & \textbf{0.8452}               & \textbf{0.6514}               & 
    \textbf{0.3883}             \\ \midrule
    \multicolumn{1}{c|}{}                                  & {Vanilla} & 0.8353                        & 0.6398                        & 0.4187                        \\
    \multicolumn{1}{c|}{}                                  & {Retrain} & {0.8419} & {0.6513} & {0.3930}  \\\cline{2-5}
    \multicolumn{1}{c|}{}                                  & {nnDF}    & 0.6887                        & 0.4029                        & 0.5639                        \\
    \multicolumn{1}{c|}{}                                  & {DFM}     & 0.8355                        & 0.6409                        & 0.4152                        \\
    \multicolumn{1}{c|}{}                                  & {FSIW}    & 0.8369                        & 0.6433                        & 0.4020                        \\
    \multicolumn{1}{c|}{}                                  & {ULC}     & 0.8343                        & 0.6412                        & 0.3994                        \\
    \multicolumn{1}{c|}{\multirow{-7}{*}{{$c$ = 10}}} & \textbf{Ours}    & \textbf{0.8411}               & \textbf{0.6491}               & \textbf{0.3958}               \\ \midrule
    \multicolumn{1}{c|}{}                                  & {Vanilla} & 0.8369                      & 0.6385                      & 0.4114                      \\ 
    \multicolumn{1}{c|}{}                                  & {Retrain} &  0.8453 &  0.6529 &  0.3860 \\\cline{2-5}
    \multicolumn{1}{c|}{}                                  & {nnDF}    & 0.7041                        & 0.4116                        & 0.5521                        \\
    \multicolumn{1}{c|}{}                                  & {DFM}     & 0.8351                        & 0.6370                        & 0.4076                        \\
    \multicolumn{1}{c|}{}                                  & {FSIW}    & 0.8385                        & 0.6421                        & 0.3965                        \\
    \multicolumn{1}{c|}{}                                  & {ULC}     & 0.8345                        & 0.6385                        & 0.3954                        \\
    \multicolumn{1}{c|}{\multirow{-7}{*}{{$c$ = 14}}} & \textbf{Ours}    & \textbf{0.8420}               & \textbf{0.6463}               & \textbf{0.3938} \\\bottomrule
\end{tabular}
\label{table:c}
\end{table}

In the online setting, IF-DFM estimates parameter changes caused by both fake negatives and newly arrived data. We also create a variant IF-DFM-w/o-add that does not update $\Delta{\theta_{add}}$ but only updates $\Delta{\theta_{delay}}$. We compare the results of IF-DFM with online CVR methods that fine-tune model using new data and duplicate negative samples. The results are shown in Figure \ref{fig:incre}. We can find that:

\begin{itemize}[leftmargin=*]
    \item Compared to pretraining, online CVR models can mitigate the delayed feedback problem to some extent. This can be attributed to their fine-tuning pipeline using new data and delayed conversion data.
    \item IF-DFM outperforms IF-DFM-w/o-add, showing a $0.36\%$ increase in AUC and a $0.47\%$ improvement in PRAUC. This suggests that IF-DFM efficiently utilizes information from new data to update model parameters.
    \item IF-DFM surpasses all baseline methods and nearly matches the performance of retraining. This highlights IF-DFM's capacity to seamlessly incorporate the true label information from delayed conversions, together with new data, enabling it to adapt to changes in user interests.
\end{itemize}

\begin{figure}[hbtp]
\vspace{-10pt}
  \centering
  \begin{minipage}[b]{0.235\textwidth}
    \centering
\includegraphics[width=\textwidth]{figures/auc_incre_with_updated_IF_DFM.pdf}
    \caption*{(a) AUC Results}
  \end{minipage}
  \begin{minipage}[b]{0.235\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/prauc_incre_with_updated_IF_DFM.pdf}
    \caption*{(b) PRAUC Results}
  \end{minipage}
  \caption{Online experimental results on Criteo dataset using MLP as the backbone.}
  \label{fig:incre}
  \vspace{-10pt}
\end{figure}

\subsection{Efficiency of Calculating Parameter Changes (RQ3)}
Ensuring rapid updates of model parameters is crucial for maintaining the freshness of a CVR model. Due to space limits, here we only present the runtime performance for the case where all methods using the MLP backbone on the Criteo dataset, as illustrated in Figure \ref{time}. Other cases demonstrate similar trend. We have the following observations:
\begin{itemize}[leftmargin=*]
    \item Compared to Vanilla, ULC requires the training of additional auxiliary models and the alternating training of two models, nearly doubling the training time. This indicates that relying on auxiliary models incurs extra time costs, making it inefficient and computationally expensive.
    \item IF-DFM directly modifies the parameters of the Vanilla method without requiring retraining. The parameter update requires only $14.8$ seconds, which constitutes a mere $1.1\%$ of the Vanilla method's training time. This demonstrates that our method can efficiently mitigate the delayed feedback problem.
\end{itemize}

\begin{table}[htbp]
\caption{Running time of different methods under the offline setting, with MLP as the backbone and $c=10$.}
\begin{tabular}{c|cccccc}
\toprule 
     & 
     {Vanilla} & {ULC}       & {DFM}     & {FSIW}    & {Vanilla + Ours}  \\ \midrule
\textbf{Time (s)} & 1351.4 & 2467.4 & 1979.8 & 1018.5 & 1351.4 + 14.8 \\ \bottomrule
\end{tabular}
\label{time}
\end{table}