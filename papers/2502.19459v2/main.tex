
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
\input{config.tex}

% \title{Efficient Reconstruction of Articulated \\ Objects via ArtGS}
\title{Building Interactable Replicas of
Complex \\Articulated Objects via Gaussian Splatting}
% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\iclrfinalcopy

\author{
% Yu Liu$^{1,2,*,\ddagger}$\quad Baoxiong Jia$^{2,*}$\quad Ruijie Lu$^{2,3}$ \quad Junfeng Ni$^{1,2}$\\
% \textbf{Song-Chun Zhu}$^{1,2,3}$ \quad \textbf{Siyuan Huang}$^{2}$\\
\hspace{-9pt} Yu Liu$^{1,2,*,\ddagger}$, Baoxiong Jia$^{2,*}$, Ruijie Lu$^{2,3}$, Junfeng Ni$^{1,2}$, \textbf{Song-Chun Zhu}$^{1,2,3}$, \textbf{Siyuan Huang}$^{2}$
\\
\hspace{-8pt} \small $^1$Tsinghua University~$^2$State Key Laboratory of General Artificial Intelligence, BIGAI~$^3$\small Peking University
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Building interactable replicas of articulated objects is a key challenge in computer vision. Existing methods often fail to effectively integrate information across different object states, limiting the accuracy of part-mesh reconstruction and part dynamics modeling, particularly for complex multi-part articulated objects. We introduce \model, a novel approach that leverages 3D Gaussians as a flexible and efficient representation to address these issues. Our method incorporates canonical Gaussians with coarse-to-fine initialization and updates for aligning articulated part information across different object states, and employs a skinning-inspired part dynamics modeling module to improve both part-mesh reconstruction and articulation learning. Extensive experiments on both synthetic and real-world datasets, including a new benchmark for complex multi-part objects, demonstrate that \model achieves state-of-the-art performance in joint parameter estimation and part mesh reconstruction. Our approach significantly improves reconstruction quality and efficiency, especially for multi-part articulated objects. Additionally, we provide comprehensive analyses of our design choices, validating the effectiveness of each component to highlight potential areas for future improvement. Our work is made publicly available at: \url{https://articulate-gs.github.io}.

\let\thefootnote\relax\footnote{$^*$Equal contribution. $^{\ddagger}$Work done as an intern at BIGAI.}

\end{abstract}

\section{Introduction}
\label{sec:intro}
Articulated objects, central to everyday human-environment interactions, have become a key focus in computer vision research~\citep{yang2023reconstructing,weng2024neural,luo2024physpart,liu2024cage,deng2024articulate}. Accurately reconstructing real-world scenes~\citep{chen2024single,ni2024phyrecon,lu2024movis} and creating interactable digital replicas of these objects are essential for various applications, including scene understanding~\citep{jia2024sceneverse, huang2024embodied, zhu2024unifying,linghu2024multi} and robotics learning~\citep{liu2022akb,geng2023gapartnet,geng2023partmanip,gong2023arnold,yang2024physcene,zhao2024tac,lu2024manigaussian}. By building high-fidelity digital twins of articulated objects, we bridge the gap between synthetic and real-world scenarios, thus facilitating the sim-to-real transfer of robotic systems~\citep{torne2024reconciling,kerr2024rsrd}. As we advance towards more sophisticated robotic systems and immersive virtual environments, there is a growing need for improved and efficient modeling techniques for the reconstruction of articulated objects.

% Articulated objects are ubiquitous in our daily lives and industrial settings, ranging from household items like laptops and drawers to complex robotic arms. The ability to accurately reconstruct these objects and create interactable digital replicas is crucial for various applications, including robotic manipulation, augmented reality, and scene understanding. The importance of articulated object reconstruction lies in its potential to training embodied robot in the simulator, enhancing manipulation capabilities of robots \citep{geng2023gapartnet, geng2023partmanip, liu2022akb}. 

% By building high-fidelity digital twins of articulated objects, we enable the training of embodied agents in simulators that closely mimic real-world conditions. These accurate replicas bridge the gap between synthetic data and real scenarios, facilitating the development of robotic systems that can generalize more effectively to real-world environments. This approach not only enhances the quality of robot training but also accelerates the deployment of robust, adaptable robotic solutions across diverse domains~\citep{torne2024reconciling}. As we move towards more sophisticated robotic systems and immersive virtual environments, the need for accurate and efficient reconstruction of articulated objects becomes increasingly critical. 

The problem of reconstructing articulated objects has been extensively studied~\citep{jiayi2023paris,liu2023building,weng2024neural,deng2024articulate,yang2023reconstructing}, with a key challenge being the learning of object geometry when only partial views of the object are available at any given state. To accurately reconstruct object parts (\eg, a closed drawer), it is essential to integrate observations from multiple object states during interactions (\eg, the opening process of the drawer). This necessitates the simultaneous learning and alignment of fine-grained object parts across different states, which must be achieved jointly during the reconstruction of object geometries. Such a requirement presents significant challenges in object modeling, especially for complex everyday articulated objects that often consist of multiple interactable parts. Additionally, uncertainties in object geometry reconstruction introduce further challenges in modeling articulation, as errors in geometry modeling can result in inaccurate learning of articulation parameters. These challenges highlight the need for improved models that handle the complexities of multi-part articulated objects.

Recent approaches attempt to address these challenges using part priors from pre-trained models. These models provide either part segmentation masks via models like SAM~\citep{kirillov2023segment}, or 2D pixel correspondences for aligning pixels across states~\citep{sun2021loftr}. However, these methods rely heavily on priors from pre-trained models, often using single-state inputs and neglecting critical motion information~\citep{mandi2024real2code} and struggling with the complexity of multi-part objects when accurately matching pixels across states becomes difficult~\citep{weng2024neural}. These limitations result in unstable and inconsistent learning of object parts, posing significant challenges to the joint learning of part motion and geometry.

% This inability to effectively leverage motion information across multiple states, coupled with the limitations of existing segmentation and prior models, underscores the need for connecting multiple states with articulation transformation and achieving more accurate part segmentation, particularly for objects with complex articulation structures.
% Real2Code \citep{mandi2024real2code} employs the powerful generalization capabilities of DUSt3R \citep{wang2024dust3r}, SAM \citep{kirillov2023segment}, and Large Language Models (LLMs) to reconstruct complex articulated objects through code generation. While innovative, this approach has two main drawbacks: a) It relies on a single state of objects, lacking .
% b) Its performance is largely dependent on the segmentation results produced by SAM, which may not always accurately capture the articulation structure of complex objects.
% , the learning of object geometry given limited visibility at any single object state is difficult. One needs to combine the observation of multiple object states (\eg, the openning process of a drawer) to properly infer the geometry of object parts (\ie, the drawer in this case). Finding this alignment is difficult as it requires fine-grained object part modeling which needs to be provided as prior or learned jointly during object part movements. This mesh reconstruction issue further causes uncertainty on assuming the geometry of objects during learning, and therefore adds difficulty in articulation modeling as errors in object geometry modeling leads to incorrect articulation parameter estimates. These challenges are further aggravated under the case of complex multi-part objects, as the uncertainty of object geometry and part identification significantly increases as the number of parts increases.

% object parts at any single object states. For reconstructing a drawer, one needs to align the observation of multiple object states describing the openning process of objects for 

% visibility issue
% geometry / articulation interplay

% One key challenge in this task is the intricate interplay between object geometry reconstruction and articulation modeling. As the state of articulated objects naturally contain occlusion of different parts,

% where uncertanties in geometric reconstruction leads to errors in estimating articulation parameters, and changes in artic


% This task presents significant challenges due to the intricate interplay between object geometry and articulation. That is, changes in articulation alters the visible geometry, while uncertainties in geometric reconstruction leads to errors in estimating articulation parameters, particularly for complex multi-part objects. Complex multi-part objects often comprise numerous interconnected parts with diverse joint types, intricate geometries, and varying degrees of visibility across different configurations. These characteristics make it difficult to accurately capture the object's structure and motion from limited observations. 

% At the same time, complex multi-part objects dominate our everyday environment, from intricate machinery to sophisticated household appliances. The accurate reconstruction of these complex articulated structures represents a critical challenge in our quest to create faithful digital replicas of the physical world. Overcoming this hurdle is essential for bridging the gap between virtual and real environments, enabling more realistic simulations, and advancing our ability to interact with and manipulate digital representations of real-world objects. As we strive to build an ideal digital twin of our world, mastering the reconstruction of complex multi-part objects stands as a fundamental stepping stone towards achieving truly immersive and functional virtual realities.

% However, existing methods~\citep{jiang2022ditto, jiayi2023paris, weng2024neural} often struggle with complex multi-part objects, especially in real-world scenarios where observations may be limited or noisy. Even for simple 2-part objects, most current methods face stability problems due to randomness. Many approaches rely heavily on motion information for part segmentation, neglecting crucial spatial relationships. This becomes particularly problematic for multi-part objects, where motion cues alone are insufficient to distinguish between similarly moving parts. 
% Recent works have attempted to address the challenges of reconstructing complex articulated objects, but significant limitations remain. DTA \citep{weng2024neural} makes strides by incorporating the LOFTER prior model \citep{sun2021loftr}, which provides additional guidance for articulation estimation. However, this approach still struggles to extend beyond objects with three or more parts, falling short when faced with the intricate structures common in many real-world articulated objects. Real2Code \citep{mandi2024real2code} takes a different approach, leveraging a combination of advanced tools including DUSt3R \citep{wang2024dust3r}, SAM \citep{kirillov2023segment}, a pre-trained shape-completion model, and a fine-tuned Large Language Model (LLM) to reconstruct complex articulated objects through code generation. While innovative, this method relies heavily on a single object state and lacks motion information, making its performance largely dependent on the segmentation results produced by SAM. This reliance on static segmentation without motion cues limits the method's ability to accurately capture the articulation properties of complex objects. These approaches, while advancing the field, highlight the persistent challenges in reconstructing complex multi-part articulated objects. The inability to effectively leverage motion information across multiple states, coupled with the limitations of existing segmentation and prior models, underscores the need for more robust and versatile methods. 

To address these challenges, we propose \model, which introduces several key innovations for handling complex multi-part articulated objects. Specifically, we adopt the commonly used two-state setting for learning articulated objects, as established in prior works~\citep{jiayi2023paris, weng2024neural}. Central to our approach is the use of 3D Gaussians~\citep{kerbl20233d} as the foundational representation, chosen for their ability to explicitly maintain spatial information while offering efficiency and high reconstruction quality. To effectively model object dynamics and integrate information across multiple object states, we employ canonical Gaussians with a carefully designed coarse-to-fine initialization and update scheme. These Gaussians act as a bridge between different input object states, enabling accurate deformation modeling that improves both mesh reconstruction and articulation learning. Building on the canonical Gaussians, we draw inspiration from Gaussian skinning~\citep{song2024reacto} and introduce a center-based clustering module for part and dynamics learning. This approach leverages motion priors of Gaussians, which are summarized during the learning process, serving as a guide to better align object parts between states and improve articulation learning.
These designs allow our method to achieve state-of-the-art performance in joint parameter estimation and part mesh reconstruction, excelling on both existing benchmarks and our newly curated complex multi-part articulated object reconstruction benchmark. Our approach outperforms existing methods in both synthetic and real-world scenarios, with significant improvements in axis modeling and overall efficiency. Through extensive experiments, we demonstrate the effectiveness of our model in efficiently delivering high-quality reconstruction of complex multi-part articulated objects. We also provide comprehensive analyses of our design choices, highlighting the critical role of these modules and identifying areas for future improvement. 

\paragraph{Contributions} Our main contributions of this work can be summarized as follows:

\begin{itemize}[leftmargin=*,nolistsep,noitemsep]
 \item We propose \model, a novel and efficient method for articulated object reconstruction that achieves state-of-the-art performance, particularly for complex multi-part objects.
 \item We introduce coarse-to-fine canonical Gaussian initialization and skinning-inspired part dynamics modeling with self-guided motion priors to improve object part and articulation learning, effectively addressing the limitations of existing methods in using object motion information.
 \item We conduct extensive experiments on both synthetic and real-world articulated objects, demonstrating the effectiveness, efficiency, scalability, and robustness of our approach. We also provide comprehensive ablation studies to validate our designs and highlight areas for future improvement.
\end{itemize}

% handling objects with significantly more parts and achieving more accurate and stable reconstructions. 
% We leverage the efficiency and flexibility of Gaussian representations, using canonical Gaussians to connect different states and mitigate visibility issues. Moreover, we propose to obtain coarse canonical Gaussians by matching two states of Gaussians, leading to more robust and high-quality reconstructions. To incorporate spatial information modeling into our part segmentation module, we use center-based segmentation, enabling more accurate separation of parts even in complex scenarios. Additionally, we propose a novel initialization strategy that leverages motion priors and clustering techniques to provide an improved starting point for optimization.

% These innovations enable our method to achieve state-of-the-art performance in joint parameter estimation and part mesh reconstruction for both two-part and multi-part objects, from synthetic to real-world scenarios. Moreover, our approach demonstrates remarkable efficiency, reconstructing complex articulated objects in a fraction of the time required by current state-of-the-art methods.

% Key advantages of our approach include:
% - More accurate estimation of joint parameters, as evidenced by smaller Axis Angle Error, Axis Position Error, and Part Motion Error.
% - Better reconstruction of part meshes, with lower Chamfer Distance (CD) metrics for both static and movable parts.
% - Significantly improved performance on real-world objects and multi-part articulated structures compared to existing methods.
% - High computational efficiency, requiring less than half or even a third of the training time of current state-of-the-art methods.

% Our contributions can be summarized as follows:
% \begin{itemize}[leftmargin=*,nolistsep,noitemsep]
% \item We propose \model, a novel articulated object reconstruction method that achieves state-of-the-art performance in joint parameter estimation and mesh reconstruction, particularly for real objects and complex multi-part objects.
% \item We propose an efficient and effective initialization strategy for the canonical Gaussians and part segmentation module, leading to more stable and accurate reconstructions.
% \item Extensive experiments on both synthetic and real-world datasets, demonstrating the effectiveness, efficiency and scalability of our approach for objects with varying complexity. We conduct comprehensive experiments and ablation studies to validate the effectiveness of our approach and its individual components.
% \end{itemize}

\section{Related Work}
\label{sec:related_work}
% \paragraph{Deformable 3D Gaussians}
\paragraph{Dynamic Gaussian Modeling}
\label{sec:related_work:dynamic_gs}
Recent advancements have shown the potential of Gaussian Splatting~\citep{kerbl20233d} for 4D reconstruction~\citep{jung2023deformable, katsumata2023efficient,wu20244d,luiten2024dynamic,li2024spacetime,lu20243d, lei2024gart,guo2024motion,qian20243dgs,bae2024per,wan2024template}. A central focus of these efforts is the deformation modeling of 3D Gaussians. While effective for dynamics capturing, most approaches learn transformations implicitly, limiting their capability for controllable dynamics modeling. To address this issue, recent studies use superpoints~\citep{huang2024sc,wan2024superpoint} for improved dynamics modeling and control. However, as superpoint learning is based primarily on rendering without considering object physics, these methods fail to reliably capture accurate physical parameters (\eg, joints and axes). Another line of works~\citep{xie2024physgaussian,jiang2024vr} introduce controllable Gaussians by integrating physics-based modeling for graphics simulations. These models require intricate priors of objects (\eg, material properties), making them impractical for reconstructing everyday articulated objects. To overcome these challenges, our work combines the explicit 3D Gaussian modeling with articulation modeling, enabling efficient and high-quality reconstruction with precise articulation parameter estimation for more practical digital-twin construction of articulated objects.


% Our work leverages the efficiency and flexibility of Gaussian representations while tailoring the approach to the specific demands of articulated object reconstruction, achieving high-quality mesh reconstruction, part segmentation and joint articulation estimation.


% . Articulated objects, with requirements for controllable dynamics modeling yet with simple part-axis representations, demands a simple yet effective method for modeling the controllable 

% in 3D scene representation have seen the emergence of Gaussian Splatting \citep{kerbl20233d} as a powerful technique for efficient and high-quality 3D reconstruction. Building upon this, many works have extended 3D Gaussians to dynamic scenes, enabling the representation of non-rigid transformations. \cite{luiten2024dynamic, katsumata2023efficient} use frame-by-frame training to learn the dynamic Gaussians and \cite{li2024spacetime} propose to model the motion trajectory of Gaussians. \cite{yang2024deformable} proposes deformable 3D Gaussian, deforming a set of canonical Gaussians to model the dynamic of 3D scenes, which is adopted and improved by many works \citep{wu20244d, wan2024superpoint, lu20243d, huang2024sc, lei2024gart, guo2024motion, wu20244d, jung2023deformable, qian20243dgs, bae2024per, xie2024physgaussian, xie2024surgicalgaussian}. 
% While effective in capturing complex transformations, these methods are focused on general scene rather than specifically addressing the challenges of articulated objects.
% Articulated object reconstruction is challenging due to the complex interplay between rigid parts and constrained joint movements, which requires simultaneous part segmentation and precise joint parameter estimation—tasks that are not typically encountered in general scene reconstruction or even human body modeling. 
% Our work leverages the efficiency and flexibility of Gaussian representations while tailoring the approach to the specific demands of articulated object reconstruction, achieving high-quality mesh reconstruction, part segmentation and joint articulation estimation.

\paragraph{Articulation Parameter Estimation}
\label{sec:related_work:artmodel}
Estimating joint articulation parameters for articulated objects has been extensively studied, with approaches broadly categorized into two main categories. First, prediction-based methods estimate joint parameters from sensory inputs of different object configurations \citep{huang2014occlusion,katz2013interactive} or use end-to-end models \citep{hu2017learning,yi2018deep,li2020category,wang2019shape2motion,sun2023opdmulti,liu2022toward,weng2021captra,sturm2011probabilistic,chu2023command,martin2016integrated,liu2023self,gadre2021act,mo2021where2act,jain2021screwnet,yan2020rpm,lei2023nap} to predict part segmentation, kinematic structure, as well as joint parameters. Second, reconstruction-based methods optimize articulation parameters by reconstructing multi-view images or videos~\citep{wei2022self,tseng2022cla,mu2021sdf,lewis2022narf22,jiayi2023paris,lei2024gart,deng2024articulate,swaminathan2024leia,noguchi2022watch,zhang2021strobenet,pillai2015learning,liu2023building}.
Most of these methods treat articulation parameter estimation as a separate task, without generating high-quality, interactable part-mesh reconstructions. \model aims to address this gap by integrating part-mesh reconstruction and articulation parameter estimation, enabling the creation of high-quality, interactable replicas.
% Based on reconstruction of multi-view images, our \model addresses both articulation parameter estimation and part mesh reconstruction, building high-quality interactable replicas.

\paragraph{Articulated Object Reconstruction}
\label{sec:related_work:reconstruction}
Articulated object reconstruction, differing from human and animal motion modeling~\citep{joo2018total,loper2023smpl,mihajlovic2021leap,noguchi2021neural,yang2021viser,yang2021lasr,romero2022embodied,zuffi20173d,yang2024attrihuman,xu2020ghum,tan2023distilling, yang2022banmo,yang2023ppr,song2023moda,yang2023reconstructing,song2023total}, focus on the piece-wise rigidity of each part, requiring both part-level geometry reconstruction and joint articulation parameter estimation. While end-to-end models predict joint parameters and segment object parts from single-stage~\citep{heppert2023carto, wei2022self,kawana2021unsupervised} or interaction observations\citep{jiang2022ditto, ma2023sim2real, nie2022structure, hsu2023ditto}, they struggle to generalize to unseen objects. Per-object optimization approaches~ \citep{jiayi2023paris,liu2023building,weng2024neural,deng2024articulate,swaminathan2024leia}, using multi-state observations for articulation modeling, offer better adaptability to unknown objects but face scaling issues of multiple joints. Methods like DTA~\citep{weng2024neural} attempt to handle multi-part objects but still struggle with those having more than three movable parts. We address the reliability, flexibility, and scalability issues of previous works with our canonical Gaussian design and skinning-inspired part dynamics modeling, achieving higher accuracy, robustness, and efficiency for articulated object reconstruction.


% These limitations call for more flexible, scalable methods in articulated object reconstruction.
% We address these limitations through our novel part assignment module and initialization strategy, making accurate and robust performance with much higher efficiency.

\section{Preliminaries}
\label{sec:prel}
\paragraph{3D Gaussian Splatting} \ac{3dgs} represents a static 3D scene using 3D Gaussians~\citep{kerbl20233d}. Each Gaussian $G_i$ is associated with a center $\vmu_i$, covariance matrix $\mSigma_i$, opacity $\sigma_i$ and spherical harmonics coefficients $\vh_i$. The final opacity of a 3D Gaussian at a spatial point $\vx$ can be calculated as: 
\begin{equation}
 \begin{aligned}
 \alpha_i(\vx)= \sigma_i \exp\left(-\frac{1}{2}(\vx-\vmu_i)^T\mSigma_i^{-1}(\vx - \vmu_i)\right), && \text{where} && \mSigma_i = \mR_i\mS_i\mS_i^T\mR_i^T.
 \end{aligned}
 \label{eq:gs_opacity}
\end{equation}
As the physical meaning of a covariance matrix is only valid if it is positive semi-definite, we decompose the covariance matrix $\mSigma_i$ following~\cref{eq:gs_opacity} into a scaling diagonal matrix $\mS_i$ and a rotation matrix $\mR_i$ parameterized by a quaternion $\vr_i$. A scene is then described with a collection of such Gaussians $\mathcal{G} = \{G_i:\vmu_i, \vr_i, {\bm{s}}_i, \sigma_i, \vh_i\}_{i=1}^{N}$. We render an image $\mI$ and optionally its depth image $\mD$ from the 3D scene $\mathcal{G}$ by projecting each Gaussian onto the 2D image plane and aggregating them using $\alpha$-blending:
\begin{equation}
 \begin{aligned}
 \mI = \sum_{i=1}^{N}T_i\alpha_i^{\text{2D}}\mathcal{SH}(\vh_i, \vv_i), && \mD=\sum_{i=1}^N T_i\alpha_i^{\text{2D}}d_i, && \text{where} && T_i = \prod_{j=1}^{i-1}(1 - \alpha_j^{\text{2D}}).
 \end{aligned}
 \label{eq:gs_rendering}
\end{equation}
$\alpha_i^{\text{2D}}$ is a 2D version of~\cref{eq:gs_opacity}, with $\vmu_i$, $\mSigma_i$, $\vx$ replaced by the projected $\vmu_i^{\text{2D}}$, $\mSigma_i^{\text{2D}}$, and the pixel coordinate $\vu$. $\mathcal{SH}(\cdot)$ is the spherical harmonic function, $\vv_i$ is the view direction from the camera to $\vmu_i$, $d_i$ is the depth of the $i$-th Gaussian. Given $N_v$ input view images $\{\bar{\mI}_i, \bar{\mD}_i\}_{i=1}^{N_v}$, \ac{3dgs} learns Gaussians $\mathcal{G}$ with:
\begin{equation}
\begin{aligned}
\mathcal{L}_{\text{render}} = (1-\lambda_{\text{SSIM}})\mathcal{L}_I + \lambda_{\text{SSIM}}\mathcal{L}_{\text{D-SSIM}} + \mathcal{L}_D,
\end{aligned}
\label{eq:rendering_loss}
\end{equation}
where $\mathcal{L}_I = ||\mI - \bar{\mI}||_1$ is the L1-loss, $\mathcal{L}_{\text{D-SSIM}}$ is the D-SSIM loss~\citep{kerbl20233d}, $\lambda_{\text{SSIM}}$ is the weight of D-SSIM loss, and $\mathcal{L}_D = \log\left(1 + ||\mD - \bar{\mD}||_1\right)$ is the optional depth supervision. 

\paragraph{Mesh Extraction from Gaussians} To extract meshes from Gaussian splats $\mathcal{G}$, we can render depth maps and utilize \ac{tsdf} to fuse the reconstructed depth maps, and extract the object mesh $\mathcal{M}$ with marching cubes~\citep{huang20242d}. This process can be done with Open3D~\citep{zhou2018open3d} with proper choice of voxel size and truncated threshold.

% Gaussian Splatting uses a set of 3D Gaussians to represent 3D scenes~\citep{kerbl20233d}. Each Gaussian $g$ is defined by its color (represented by spherical harmonic coefficients $sh$), opacity $\sigma$, center position $x \in \mathbb{R}^3$ and a 3D covariance matrix $\Sigma \in \mathbb{R}^{3 \times 3}$, which governs the spatial extent and orientation of the Gaussian as $\alpha_i(p)=\sigma_i e^{-\frac{1}{2}(p-x_i)^T\Sigma^{-1}(p-x_i)}$. The covariance matrix $\Sigma$ can be decomposed into a rotation matrix $R \in SO(3)$ parameterized by a quaternion $r$ and a scaling diagonal matrix $S \in \mathbb{R}^{3\times3}$ parameterized by a scaling vector $s$, such that $\Sigma = RSS^TR^T$. A scene is then described by a collection of Gaussians $G = \{g_i: x_i, r_i, s_i, \sigma_i, sh_i\}$.

% To render an image from the 3D scene, each Gaussian is projected onto the 2D image plane. The final color $C(u)$ of a pixel $u$ is computed by aggregating the contributions of all Gaussians that project onto the pixel. This is done through $\alpha$-blending, where each Gaussian's color is weighted based on its opacity and spherical harmonic coefficients. By optimizing the Gaussian parameters $G = \{g_i : x_i, r_i, s_i, \sigma_i, sh_i\}$ and adaptively adjusting the density of the Gaussians, we can achieve real-time rendering of high-quality images. The color is expressed as $C(u) = \sum_{i=1}^{N} T_i \alpha_i SH(sh_i, v_i)$, where $v_i$ is the view direction, $SH(sh_i, v_i)$ is the evaluation of the spherical harmonic function for Gaussian $G_i$, and the term $T_i$ accumulates transparency and is defined as $T_i = \prod_{j=1}^{i-1} (1 - \alpha_j)$.



\section{Method}
\label{sec:method}
Given $N_v$ RGB-D images of an unknown articulated object $\{\bar{\mI}^t_i, \bar{\mD}^t_i\}_{i=1}^{N_v}$ at two joint states $t\in\{0,1\}$, we aim to reconstruct its part-level meshes $\mathcal{M}$ and joint articulation parameters $\Psi$. We define a set of learnable canonical Gaussians $\mathcal{G}^c$ which can be transformed into joint state Gaussians $\mathcal{G}^t$ via a per-Gaussian SE(3) transformation $T^{c\to t}$, parameterized by $\Psi$. Formally,
\begin{equation}
 \begin{aligned}
 \mathcal{G}^t = T^{c\to t}\cdot\mathcal{G}_c \quad \text{and} \quad \mathcal{G}^{c} = (T^{c\to t})^{-1}\cdot\mathcal{G}^t \quad \text{for} \quad t\in\{0,1\}. \\
 \end{aligned}
 \label{eq:motion_model}
\end{equation}
We impose the continuity of motion between the joint states by setting the canonical Gaussians $\mathcal{G}^{c}$ at the mid-state ($c$ : $t$ = 0.5), enforcing that $T^{c\to0}=(T^{c\to1})^{-1}$. This simplifies the articulation learning and connects the two input joint states through the canonical Gaussians $\mathcal{G}^c$, solving potential issues of occlusion and misinformation when reconstructing object meshes separately on the two joint states.



Using this motion model, we leverage multi-view RGB-D images from the two input states to learn both the canonical Gaussian $\mathcal{G}^c$, the transformation $T^{c\to1}$ or equivalently the joint parameters $\Psi$, and extract object meshes $\mathcal{M}^t$ for different joint states following~\cref{sec:prel}. An overview of \model is presented in~\cref{fig:overview}, with details on key designs provided in the following sections.

% Given multi-view RGB-D images $\{\mI_m^t, D_m^t\}_{m=1}^M$ at two states $t\in\{0,1\}$, we aim to build an interactable replica of an unknown articulated object by reconstructing its part-level meshes and estimating its joint articulation parameters $\phi$. We adopt Gaussian Splatting to reconstruct the geometry of the object, extracting its meshes by TSDF. To estimate its joint parameters $\phi$, we model its articulation by deforming canonical Gaussians $\mathcal{G}^c$ to two observation spaces as $\mathcal{G}^0$ and $\mathcal{G}^1$. For each canonical Gaussian $G^c_i$, its deformation $T_i^{c\rightarrow t}$ defined as $G^t_i=T_i^{c\rightarrow t}\cdot G^c_i$, is determined by its part segmentation mask and per-joint transformations parameterized by $\phi$. As illustrated in \cref{fig:overview}, our method is divided into 3 stages. Stage one trains Gaussians at two states individually to obtain the coarse canonical Gaussians by Hungarian Algorithm. Stage two learns type-free joint transformations to predict joint types (revolute or prismatic). Stage three further optimizes the joint parameters with type-constrained articulation. 

\begin{figure}[t!]
 \centering
 \resizebox{\linewidth}{!}{\includegraphics[width=\linewidth]{figure/overview.pdf}}
 \caption{\textbf{The overview of \model.} Our method is divided into two stages: (i) obtaining coarse canonical Gaussians $\mathcal{G}^c_{\text{init}}$ by matching the Gaussians $\mathcal{G}^0_{\text{single}}$ and $\mathcal{G}^1_{\text{single}}$ trained with each single-state individually and initializing the part assignment module with clustered centers, (ii) jointly optimizing canonical Gaussians $\mathcal{G}^c$ and articulation model (including the articulation parameters $\Psi$ and the part assignment module in \cref{sec:method:skinning}).}
 \label{fig:overview}
\end{figure}

\subsection{Coarse-to-Fine Canonical Gaussian Initialization with Motion Analysis}
\label{sec:method:canonical}
The initialization of the canonical Gaussians $\mathcal{G}^c$ is crucial for articulation learning. A good initialization leverages the consistency between input joint states, improving mesh reconstruction and articulation modeling. In contrast, a random initialization leads to undesirable local minima, adversely affecting the learning process (see in~\cref{fig:ablation}). To tackle this issue, we propose a coarse-to-fine strategy for the canonical Gaussian initialization, incorporating preliminary motion information from the two input joint states to enhance subsequent articulation modeling.
% Initializing the canonical Gaussians $\mathcal{G}^c$ is a challenging task. A well-chosen initialization can exploit the consistency between the provided input joint state, leading to improved mesh reconstruction and articulation modeling. In contrast, randomly initialized canonical Gaussians are susceptible to undesired local minima, which negatively impacts the articulation learning process (see an example in~\cref{fig:ablation}). To address this issue, our key observation is to utilize the continuity in object part movement between the input joint states as a constraint for canonical Gaussian initialization. We meticulously set the canonical Gaussians to be at the middle state of the two input joint states (\ie, $t=0.5$) and use the forward-backward consistency of the transformation as a constraint:
% \begin{equation}
% \mathcal{G}^1 = T^{c\rightarrow1} \cdot \mathcal{G}^c,\quad \mathcal{G}^0 = T^{c\rightarrow0} \cdot \mathcal{G}^c, \quad T^{c\rightarrow0}=(T^{c\rightarrow1})^{-1},
% \end{equation}
% where $T^{c\to 0} = (T^{c\to1})^{-1}$ comes from the assumption of continuous motion and that canonical state is the middle state. Under this assumption, we chain the two input joint states via the canonical Gaussian, solving potential issues of occlusion and view-point differences when modeling object meshes on separate states.


% % The major motivation for assuming the canonical Gaussians $\mathcal{G}^c$ is to chain the input two joint states via the canonical Gaussian, thereby addressing potential issues of previous works which only considers static states separately, thus suffering from occlusion and view-points issues. Nonetheless, learning the transformation and the canonical Gaussian jointly via only the multi-view input of two joint states is still challenging as it is essentially an joint optimization problem where object geometry and transformation compromise each other during learning, leading to degenerate solutions. To solve this issue, our key observation is to utilize the consistency between the two-joint states, setting the canonical state at an intermediate state ($t=0.5$), thereby adding constraint on the transformation, \ie:
% % % Deforming shared canonical Gaussians connects the two states as cues for each other, helping to address the problems of occlusion and viewpoint issues, which is a challenge for DTA that extracts meshes from static NeRFs of the two input states individually. 
% % % To further utilize the dynamic consistency between the two states, we set the canonical state at an intermediate state ($t=0.5$) with: 
% % \begin{equation}
% % \mathcal{G}^1 = T^{c\rightarrow1} \cdot \mathcal{G}^c,\quad \mathcal{G}^0 = T^{c\rightarrow0} \cdot \mathcal{G}^c, \quad T^{c\rightarrow0}=(T^{c\rightarrow1})^{-1},
% % \end{equation}
% % where we introduce forward-backward consistency from the canonical state (middle state) to the two input state.

% Obtaining canonical Gaussians $\mathcal{G}^c$ at a hypothetical state is challenging, due to the absence of direct supervision on the canonical space. Starting from randomly initialized canonical Gaussians makes it easy to fall into an unwanted local optimal solution, as shown in \cref{fig:ablation}. In the case of an error in the canonical Gaussians, it's hard to learn the correct articulated parts and their articulation parameters. 

% Given input images of the two object states, we first learn the Gaussian representation $\mathcal{G}_{\text{coarse}}^s$ for the two object states $s\in\{0,1\}$ following~\cref{eq:rendering_loss}. To model the deformation between these states, we define a canonical Gaussian representation $\mathcal{G}^c$ and a transformation function $T^{c\rightarrow s}$ parameterized by articulation parameters $\Psi$ that transforms $\mathcal{G}^c$ to a refined object state Gaussian $\mathcal{G}^s$.

% We learn the articulation parameters $\Psi$ using the deformation from the learned Gaussian representation $\mathcal{G}^0$ and $\mathcal{G}^1$ of the two states as a supervisory signal. Specifically, we define a canonical Gaussian $\mathcal{G}^c$ based on $\mathcal{G}^0$ and $\mathcal{G}^1$, and use $\Psi$ to parameterize the transformation $T^{c\rightarrow t}$ from the canonical Gaussian to the Gaussian $\mathcal{G}^t$ at state $t$ for modeling the deformation process. 

% \subsubsection{Canonical Gaussians Initialization}
% \label{sec:method:articulation:cano}

\paragraph{Coarse Initialization by Matching Single-state Gaussians} 
In this phase, we first separately train two sets of single-state Gaussians $\mathcal{G}^t_{\text{single}}$ with input multi-view images following~\cref{eq:rendering_loss}. We then apply Hungarian Matching to obtain matched Gaussian pairs between $\mathcal{G}_{\text{single}}^0$ and $\mathcal{G}_{\text{single}}^1$, based on the distance between Gaussian centers. We take the mean of each pair of matched Gaussians as the coarse canonical Gaussian initialization $\mathcal{G}_{\text{coarse}}^c$. To reduce the significant computation time associated with matching a large number of Gaussians, we use \ac{fps} to downsample the learned single-state Gaussians to a set of 5K Gaussians prior to matching. 

% \paragraph{Obtaining Canonical Gaussians by Matching Coarse Gaussians.} To solve this problem, we propose to obtain coarse canonical Gaussians by matching coarse Gaussians of 2 states. 
% Specifically, we individually train 2 sets of coarse Gaussians $\hat{\mathcal{G}}^0$, $\hat{\mathcal{G}}^1$ with multi-view RGB images. Given the positions of these Gaussians, we match two sets of Gaussians by Hungarian Algorithm to obtain paired Gaussians. We take the mean of all properties of the paired Gaussians to initialize the canonical Gaussians. The canonical Gaussians obtained by Hungarian Matching are not accurate, especially for prismatic joints, but they can provide a good initialization for subsequent training and prevent the model from learning various distorted canonical spaces. 

\paragraph{Initialization Refinement with Motion Analysis} 
To support geometry reconstruction and articulation modeling, relying solely on 5K matched coarse Gaussians alone is insufficient. Therefore, we refine the coarse initialization $\mathcal{G}_{\text{coarse}}^c$ guided by the motion information of object parts.
Intuitively, single-state Gaussians, $\mathcal{G}_{\text{single}}^{0}$ and $\mathcal{G}_{\text{single}}^{1}$, should exhibit consistency for static object parts discrepancies for movable parts, \ie, the static parts of these Gaussians are well-learned. Based on this insight, we refine the set of coarse canonical Gaussians $\mathcal{G}_{\text{coarse}}^c$ by including Gaussians corresponding to static parts, allowing more focused learning of movable parts during articulation modeling. In practice, we classify each Gaussian $G_i$ in a joint state $t$ as static or dynamic by calculating its minimum Chamfer Distance to all Gaussians in the opposite state $\bar{t}$:

\begin{equation}
\text{CD}_i^{t\to \bar{t}} = \min_{j}||\vmu^t_i - \vmu^{\bar{t}}_j||_2,\quad G_i \in \mathcal{G}_{\text{single}}^t, G_j\in\mathcal{G}_{\text{single}}^{\bar{t}}\quad\text{and}\quad \text{CD}^{t\to\bar{t}}=\underset{i}{\mathrm{Mean}}\left(\text{CD}_i^{t\to\bar{t}}\right).
\label{eq:mobility_identification}
\end{equation}

If the distance $\text{CD}_i^{t\to\bar{t}}$ exceeds a threshold $\epsilon_{\text{static}}$, $G_i$ is classified as dynamic; otherwise it is static. To determine which state, $t$ or $\bar{t}$, contains more motion information, we compare the mean distance $\text{CD}^{t\to\bar{t}}$ of all Gaussians in state $t$ following~\cref{eq:mobility_identification} and classify the higher state as the more motion informative state. For instance, a cabinet with open drawers provides clearer identification of movable parts than one with closed drawers. With this information, we add the static Gaussians from the more motion informative state to refine $\mathcal{G}_{\text{coarse}}^c$ into the final initialization of the canonical Gaussian $\mathcal{G}^c_{\text{init}}$.

% At the same time, the number of matched Gaussians (5K) is not enough, so we hope to add static Gaussians shared by the two states. To identify which points are static or mobile, we compute the single-directional Chamfer Distance between two states of Gaussians, denoted as $CD_{0\rightarrow1} \in \mathbb{R}^{N_0}$ and $CD_{1\rightarrow0} \in \mathbb{R}^{N_1}$. If the Chamfer Distance of a Gaussian exceeds a threshold $\eps_{cd}$, the Gaussian is considered mobile; otherwise, it is considered static. If the average Chamfer Distance of all Gaussians in a state is higher, we consider that state to have greater motion and contain more information (imagine that a cabinet with open drawers reveals more information than one with closed drawers). We select all static Gaussians from this state to initialize the canonical space. 
% Additionally, these static and movable Gaussians could be used to better initialize our part segmentation module, which will be described in \cref{sec:method:skinning}. 

% Since the computation time required for the matching process increases significantly with the number of Gaussians, we utilize farthest point sampling to downsample 5K Gaussians at each stage. At the same time, the number of matched Gaussians (5K) is not enough, so we hope to add static Gaussians shared by the two states. To identify which points are static or mobile, we compute the single-directional Chamfer Distance between two states of Gaussians, denoted as $CD_{0\rightarrow1} \in \mathbb{R}^{N_0}$ and $CD_{1\rightarrow0} \in \mathbb{R}^{N_1}$. If the Chamfer Distance of a Gaussian exceeds a threshold $\eps_{cd}$, the Gaussian is considered mobile; otherwise, it is considered static. If the average Chamfer Distance of all Gaussians in a state is higher, we consider that state to have greater motion and contain more information (imagine that a cabinet with open drawers reveals more information than one with closed drawers). We select all static Gaussians from this state to initialize the canonical space. Additionally, these static and movable Gaussians could be used to better initialize our part segmentation module, which will be described in \cref{sec:method:skinning}. 

\subsection{Part Discovery for Articulation Modeling}
\label{sec:method:skinning}
Following~\cref{eq:motion_model}, we use a part-based formulation for articulation modeling. Specifically, given the number of parts $K$, we aim to decompose the Gaussians into $K$ parts and learn the articulation paramerters $\Psi = \{T_{k}^{c\to 1}\}_{k=1}^{K}$. In contrast to existing works that leverage prior information for part discovery~\citep{mandi2024real2code,weng2024neural}, we discover parts in an unsupervised manner during learning.

% To transform Gaussians from the canonical space to the observation space, we start by inputting their positions $\rmX \in \mathbb{R}^{N\times 3}$ into a segmentation module to assign these Gaussians to $K$ parts, obtaining a part-level mask $\rmM \in \mathbb{R}^{N\times K}$.

% \subsubsection{Part Segmentation with Gaussian Skinning}
\paragraph{Center-based Part Modeling and Assignment}
Given input canonical Gaussians $\mathcal{G}^c = \{G_i\}_{i=1}^{N}$, our objective is to compute part-level masks $\rmM\in\mathbb{R}^{N\times K}$ that assign each Gaussian $\mathcal{G}_i$ to a specific part. A common approach to generating these assignment masks is through unsupervised segmentation modules using MLPs or slot-attention~\citep{locatello2020sa,jia2023improving,liu2025slotlifter}. However, these models implicitly segment parts and fail to leverage the explicit spatial and dynamic information present in 3D Gaussians. We observe that such methods struggle with parts that exhibit similar motion patterns, leading to incorrect assignments. To address this issue, we adopt a center-based part modeling approach that explicitly utilizes spatial information, inspired by sparse control points from SC-GS~\citep{huang2024sc} and quasi-rigid blend skinning in REACTO~\citep{song2024reacto}. Specifically, we define $K$ learnable centers $C_k = (\vp_k, \mV_k, {\bm \lambda}_k)$ with center location $\vp_k\in\mathbb{R}^3$, rotation matrix $\mV_k\in\mathbb{R}^{3\times 3}$, and scale vector ${\bm \lambda}_k\in\mathbb{R}^3$. 
For a given Gaussian $G_i \in \mathcal{G}^c$, we compute the Mahalanobis distance $\mD_{ik}$ between $G_i$ and center $C_k$ as:
\begin{equation}
 \rmX^k_i = \frac{[\mV_k(\vmu^c_i-\vp_k)]}{{\bm\lambda}_k} \quad
 \rmD_{i}^k = (\rmX^k_i)^T\cdot\rmX^k_i\quad\text{and}\quad\bm{M} = \mathrm{GumbelSoftmax}\left(\frac{-\mD + \mW_{\Delta}}{\tau}\right)
 \label{eq:part_assignment}
\end{equation}
where $\mD_{i}^k$ is the distance matrix for part assignment. One challenge of using the distance matrix for part assignment is identifying sharp boundaries when two parts overlap spatially (\eg, in the case of a closed drawer). To improve boundary identification, we introduce a residual term $\mW_\Delta = \mathrm{MLP}(\vmu, \mX, \mD)$, predicted by a shallow MLP that concatenates the absolute position of each Gaussian and the distance matrix $\mD$ as input. This residual is added to the original distance matrix $\mD$ to refine the part assignment mask following \cref{eq:part_assignment}. Notably, we use Gumbel Softmax to ensure that each Gaussian is assigned to only one part, which simplifies the optimization of joint parameters. Detailed implementation can be found in~\cref{app:imp:assignment}.

% The part segmentation masks could be calculated as $M = \mathrm{Gumbel Softmax}(\frac{-D + \delta}{\tau})$, where $\tau$ is a temperature factor that anneals with the training process. Notably, we use Gumbel Softmax to ensure that each Gaussian is assigned to only one part, thereby facilitating the optimization of the joint parameters for each part. 

% the motion of two parts is similar, these implicit models cannot distinguish the parts based on motion information alone (see \cref{fig:ablation_seg}).
% In order to make better use of spatial information to cluster Gaussian, we adopt a center-based segmentation strategy similar to the control points in SC-GS~\citep{huang2024sc} and Gaussian-based skinning in REACTO~\citep{song2024reacto}. 
% Specifically, given the positions $\rmX^c_i$ of canonical Gaussians, we compute the Mahalanobis distance $\rmD_{ik}$ between $\rmX^c_i$ and $K$ learnable centers $c_k=(\mu_k, R_k, \lambda_k)$ as:
% \begin{equation}
% \rmX^k_i = [R_k(\rmX^c_i-\mu_k)] / \lambda_k \quad
% \rmD_{ik} = (\rmX^k_i)^T\cdot\rmX^k_i ,
% \end{equation}
% where $\mu_k\in \mathbb{R}^3, R_k\in \mathbb{R}^{3\times3}, \lambda_k\in \mathbb{R}^3$ are center vector, rotation matrix and scale vector respectively. 
% Although this modeling approach effectively utilizes spatial information, it cannot model sharp boundaries, especially when two parts have some overlap in space (e.g., a closed drawer), making it difficult to segment the parts accurately. Therefore, we use a hash grid to encode the absolute position of each Gaussian, predicting a residual $\delta$ with a shallow MLP and adding it to the original distance $\rmD_{ik}$. The part segmentation masks could be calculated as $M = \mathrm{Gumbel Softmax}(\frac{-D + \delta}{\tau})$, where $\tau$ is a temperature factor that anneals with the training process. Notably, we use Gumbel Softmax to ensure that each Gaussian is assigned to only one part, thereby facilitating the optimization of the joint parameters for each part. 

\paragraph{Center Initialization by Clustering Coarse Gaussians}
We empirically find that the initialization of centers $\vp_k$ and scale ${\bm \lambda}_k$ have great impacts on the correctness of part discovery in later learning process (see~\cref{fig:ablation}). Therefore, similar to the canonical Gaussian initialization described in~\cref{sec:method:canonical}, we utilize the motion type of each joint as additional information for providing good initializations of part centers. Specifically, we select the input joint state with more motion information to identify static and dynamic parts. For static parts, we take the mean of the Gaussians as the part center. For movable parts, we do spectral clustering on the positions of movable Gaussians ($K-1$ clusters) and take the mean of each cluster for part center initialization. We use the distance from the farthest point to the center of each cluster as the initial scale.

% Although this center-based segmentation method certainly makes use of spatial information, we found that the random initialization of the centers $\mu_k$ and scales $\lambda_k$ have a great impact on the final result. Therefore, we consider getting a better initialization by clustering existing coarse Gaussians. 
% As shown in the \cref{fig:ablation}, simply clustering all these Gaussians (\ie, w/o motion prior) does not result in reasonable clustering centers, as clustering by spatial location alone is a big challenge. We propose to solve this problem with motion information. 
% Firstly, as described in \cref{sec:method:cano}, we choose Gaussians of states with more motion. The mean of the positions of the static Gaussians is used as the center of the static part. We do spectral clustering on the positions of movable Gaussians and take the mean of each cluster to get its center. Half of the distance from the farthest point to the center of each cluster is used as the initial value of scale.

\subsection{Self-guided Articulation Type and Parameter Learning}\label{sec:method:dual_quaternion}
After obtaining object part representations, we define the per-part articulation parameters via dual-quaternions. Formally, the joints articulation parameters $\Psi = \{T_k^{c\to 1}\}_{k=1}^{K} = \{\vq_k^{c\to1}:({\vq}_{k,r}, \vq_{k, d})\}_{k=1}^K$, where $\vq_{k,r}$ and ${\vq}_{k,d}$ are the real and dual part of the dual-quaternion that determine the rotation and translation of the joint transformation respectively. For notational simplicity, we use $\vq_k^t$ for $\vq_k^{c\rightarrow t}$ in the following texts. With the mid-state assumption in~\cref{sec:method}, we have $\vq_{k}^0 = (\vq_k^{1})^{-1}$ is the inverse of dual-quaternion $\vq_{k}^1$. Given object masks $\mM$ obtained in~\cref{sec:method:skinning}, the per-gaussian dual-quaternion $\vq_i$ for Gaussian $G_i \in \mathcal{G}^c$ is given by:
\begin{equation}
\begin{aligned}
\vq_i^t =(\sum_{k=1}^K \mM_{ik}\cdot {\vq^t_{k,r}},\sum_{k=1}^K \mM_{ik}\cdot {\vq^t_{k,d}}). 
\end{aligned}
\end{equation}
where $(\vq_k^1)^{-1}$ is the inverse of dual-quaternion $\vq_k^{1}$ and $\mM_{ik}$ denotes the probability of Gaussian $i$ belongs to part $k$.
With the per-gaussian transformation given $\vq_{i}^{t}$, we transform the canonical Gaussian $\mathcal{G}^c$ to get the two joint state Gaussians $\mathcal{G}^t$ with:
\begin{equation}
\begin{aligned}
\vmu_i^t = \mR_i^{c\rightarrow t}\cdot \vmu_i^c + \vt_i^{c\rightarrow t}, \quad \vr_i^t = {\vq}_{i,r}^{t} \otimes \vr_i^c,\\
\end{aligned}
\label{eq:articulation_modeling}
\end{equation}
where $\mR_i^{c\rightarrow t}$ and $\vt_i^{c\rightarrow t}$ is the per-gaussian rotation matrix and translation vector derived from $\vq_i^{t}$, and $\otimes$ denotes quaternion multiplication operation. We assume that the scale ${\bm s}_i$ and opacity $\sigma_i$ of the Gaussian $G_i$ remains consistent under transformation.

To enhance the learning of articulation parameters, we adopt a warm-up strategy for predicting the joint type of each part. During the warm-up stage, we optimize the articulation parameters $\Psi=\{\vq_k^{1}\}_{k=1}^{K}$ without any constraints. Next, we develop a heuristic for joint type prediction based on the learned rotation ${\vq_{k,r}}$. Specifically, we classify the joint as revolute if the rotation degree of ${\vq_{k,r}}$ exceeds a threshold $\epsilon_{\text{revol}}$, and otherwise prismatic. With predicted joint types, we constrain the joint transformation for each part. Specifically, we manually set the rotation quaternion $\vq_{k,r}$ of prismatic joints as identity quaternion. This operation allows the model to focus on optimizing the translation term $\vq_{k,d}$ of the prismatic joint, thereby obtaining a more accurate estimate of the joint parameters. 
% Similarly, we can also restrict the translation term of revolute joint as \bx{$\vq_{k,d}=\frac{1}{2}(\vq_{d0}\otimes \vq_r-\vq_r\otimes \vq_{d0})$, where $\vq_{d0}=(0, \vt_0)$ and $\vt_0\in \mathbb{R}^3$ is the axis position of the revolute joint. d0????} However, we found in our experiments that restricting revolute joints does not always bring benefits. Some objects are beneficial while others are not, because this reduction of the optimization space may make it more difficult to find an optimal solution. Thus, we only restrict the prismatic joints.
\subsection{Optimization}
We train our model using the rendering loss with depth supervision $\mathcal{L}_{\text{render}}$ described in~\cref{sec:prel} on the reconstructed $\mathcal{G}^t$ for the two joint states as discussed in~\cref{sec:method:dual_quaternion}. To reduce the chances of learning artifacts during update, we use the single-state reconstructed Gaussians $\mathcal{G}_\text{single}^t$ as an additional supervision:
\begin{equation}
\label{eq:cd_loss}
 \mathcal{L}_{\text{CD}}=\frac{1}{N}\sum_{i=1}^{N}\min_{j}||\vmu^t_i - \vmu^{t}_j||_2\quad,\quad G_i \in \mathcal{G}^t,\quad\text{and}\quad G_j\in\mathcal{G}_{\text{single}}^{t},
\end{equation}
where we calculate the single-direction Chamfer Distance between the deformed Gaussians $\mathcal{G}^t$ and single-state reconstructed Gaussians $\mathcal{G}^t_\text{single}$ as the loss signal. As these single-state Gaussians are only a rough estimate, we only introduce this loss in the first 1K to 5K steps.
% Additionally, to compare with DTA fairly, we add a depth loss $\mathcal{L}_{depth}=\log (1 + |\mathrm{depth_{gt}} - \mathrm{depth_{pred}}|)$. 
Additionally, to regularize the learning of part centers $\vp_k$, we add another regularization loss as:
\begin{equation}
\mathcal{L}_{\text{reg}}=\frac{1}{K}\sum_{k=1}^K||\vp_k-\hat{\vp}_k||_2, \quad \mathrm{where}\quad \hat{\vp}_k=\sum_{i=1}^{N}\frac{\mM_{ik}}{\sum_{i=1}^{N}\mM_{ik}}\vmu_i,
\end{equation}
which enforces that the centers $\vp_k$ should be close to the average spatial position of Gaussians in canonical Gaussians $\mathcal{G}^c$ that belong to part $k$. Above all, our supervision could be summarized as:
\begin{equation}
\mathcal{L} = \mathcal{L}_{\text{render}} + \lambda_{\text{CD}}\mathcal{L}_{\text{CD}} + \lambda_{\text{reg}}\mathcal{L}_{\text{reg}}.
\end{equation}
We provide more implementation and model training details in~\cref{app:imp}.

\section{Experiments}
\label{sec:exp}

% \bx{In this section, we conducted extensive experiments to investigate the following questions:
% \begin{itemize}[leftmargin=*,noitemsep,nolistsep]
% \item How good is our proposed \model on both synthetic and real-world objects?
% \item Is \model able to generalize to complex multi-part articulated objects?
% \item How significant are our designs (obtaining canonical Gaussians, center-based segmentation and center initialization) in \model?
% \end{itemize}
% }

\paragraph{Datasets}
We evaluate our method on three datasets:
(1) PARIS, a two-part dataset proposed by~\cite{jiayi2023paris}, which features articulated objects consisting of one static and one movable part. It includes 10 synthetic objects from the PartNet-Mobility dataset~\citep{xiang2020sapien} and 2 real-world objects captured using the MultiScan~\citep{mao2022multiscan} toolset. 
(2) DTA-Multi, a dataset proposed by~\cite{weng2024neural}, containing 2 synthetic multi-part articulated objects from PartNet-Mobility, each with one static part and two movable parts. 
(3) \model-Multi, our newly curated dataset, featuring 5 complex articulated objects from PartNet-Mobility with 3 to 6 movable parts. 

\paragraph{Metrics} Following the evaluation protocols of PARIS~\citep{jiayi2023paris} and DTA~\citep{weng2024neural}, we assess the performance of all methods using both mesh reconstruction and articulation estimation metrics. 
For mesh reconstruction, we compute the bi-directional Chamfer Distance between the reconstructed mesh and the ground truth mesh with 10K uniformly sampled points from each mesh. We report the Chamfer Distance for the whole object (CD-w), the static parts (CD-s), and the movable parts (CD-m).
For articulation estimation, we evaluate the predicted articulation using the angular error (Axis Ang.) and the distance (Axis Pos., revolute joint only) between the predicted and ground-truth joint axes. We also report the part motion error (Part Motion) which measures the rotation geodesic distance error (in degrees) for revolute joints and Euclidean distance error (in meters) for prismatic joints.

% Axis Angle Error ($^\degree$): the angular error of the predicted joint axis, (2) Axis Position Error (0.1m): Distance between predicted and ground-truth joint axes for revolute joints, which is not used for prismatic joints. (3) Part Motion Error: Rotation geodesic distance error ($^\degree$) for revolute joints, or Euclidean distance error (m) for prismatic joints.

\subsection{Results on Simple Articulated Objects}
\label{sec:exp:two-part}
\paragraph{Experimental Setup} We use the PARIS dataset as the benchmark and select Ditto~\citep{hsu2023ditto}, PARIS~\citep{jiayi2023paris}, CSG-reg~\citep{weng2024neural}, 3Dseg-reg~\citep{weng2024neural}, and DTA~\citep{weng2024neural} as baselines for quantitative evaluation. Following the evaluation setting from DTA~\citep{weng2024neural}, we report all metrics with mean $\pm$ std over 10 trials calculated at the high-visibility joint state. We re-train DTA on the same device (NVIDIA RTX 3090) for training time comparison. Additional results on all joint states are provided in~\cref{tab:app:exp_2part}.

\paragraph{Results}
As shown in \cref{tab:exp_2part}, our method significantly outperforms existing approaches across all metrics, especially for joint articulation parameter estimation, where \model achieves substantially lower errors. This improvement stems from our motion model with Gaussian Splatting, which explicitly deforms Gaussians for more precise part transformation modeling, leading to more precise joint parameter estimation. For mesh reconstruction, \model excels in reconstructing movable parts, yielding lower CD-m values, especially for real-world objects. While DTA performs well on CD-w and CD-s due to its state-by-state reconstruction, we show in~\cref{fig:2part} that it struggles with the low-visibility state. In contrast, \model achieves significantly better results on the low-visibility state while maintaining competitive results on the high-visibility state. This is attributed to the canonical Gaussians modeling that connects the two input joint states for mutually improved mesh reconstruction. Additionally, \model shows consistently better results on real-world objects with significantly faster training time, positioning it as an efficient solution for building digital twins of real-world articulated objects.
\begin{table*}[t!]
\caption{\textbf{Quantitative evaluation on PARIS.} Metrics are reported as mean $\pm$ std over 10 trials at the joint state with higher visibility, following \citep{weng2024neural}. PARIS$^*$~\citep{jiayi2023paris} is augmented with depth for fair comparison. DTA is re-trained for time efficiency comparison. Lower ($\downarrow$) is better on all metrics and we highlight \colorbox[HTML]{ffc5c5}{best} and \colorbox[HTML]{ffebd8}{second best} results. Objects with $\dagger$ are seen categories trained in Ditto. F indicates wrong motion type predictions. Axis Pos. is omitted for prismatic joints (Blade, Storage, and Real Storage).}
\label{tab:exp_2part}
\renewcommand{\arraystretch}{1.2}
\resizebox{\linewidth}{!}{
\begin{tabular}{cc|ccccccccccc|ccc}
\hline
\multirow{2}{*}{Metric} &\multirow{2}{*}{Method} &\multicolumn{11}{c}{Synthetic Objects} &\multicolumn{3}{|c}{Real Objects} \\
% \cmidrule(lr){}
& &FoldChair &Fridge &Laptop$^\dagger$ &Oven$^\dagger$ &Scissor &Stapler &USB 
&Washer&Blade &Storage$^\dagger$ &All & Fridge &Storage &All \\
\hline
\multirow{6}{*}{\shortstack{Axis\\Ang}} 
&Ditto
&89.35 &89.30 &3.12 &0.96 &4.50 &89.86 &89.77 &89.51 &79.54 &6.32 &54.22 &\best{1.71} &\secbest{5.88} &\secbest{3.80} \\
% &PARIS
% &8.08\tiny{$\pm$13.2} &9.15\tiny{$\pm$28.3} &0.02\tiny{$\pm$0.0} &0.04\tiny{$\pm$0.0} &3.82\tiny{$\pm$3.4} &39.73\tiny{$\pm$35.1} &0.13\tiny{$\pm$0.2} &25.36\tiny{$\pm$30.3} &15.38\tiny{$\pm$14.9} &0.03\tiny{$\pm$0.0} &10.17\tiny{$\pm$12.5} &1.64\tiny{$\pm$0.3} &43.13\tiny{$\pm$23.4} &22.39\tiny{$\pm$11.9} \\
&PARIS*
&15.79\tiny{$\pm$29.3} &2.93\tiny{$\pm$5.3} &\secbest{0.03\tiny{$\pm$0.0}} &7.43\tiny{$\pm$23.4} &16.62\tiny{$\pm$32.1} &8.17\tiny{$\pm$15.3} &0.71\tiny{$\pm$0.8} &18.40\tiny{$\pm$23.3} &41.28\tiny{$\pm$31.4} &\secbest{0.03\tiny{$\pm$0.0}} &11.14\tiny{$\pm$16.1} &\secbest{1.90\tiny{$\pm$0.0}} &30.10\tiny{$\pm$10.4} &16.00\tiny{$\pm$5.2} \\
&CSG-reg
&0.10\tiny{$\pm$0.0} &0.27\tiny{$\pm$0.0} &0.47\tiny{$\pm$0.0} &0.35\tiny{$\pm$0.1} &0.28\tiny{$\pm$0.0} &0.30\tiny{$\pm$0.0} &11.78\tiny{$\pm$10.5} &71.93\tiny{$\pm$6.3} &7.64\tiny{$\pm$5.0} &2.82\tiny{$\pm$2.5} &9.60\tiny{$\pm$2.4} &8.92\tiny{$\pm$0.9} &69.71\tiny{$\pm$9.6} &39.31\tiny{$\pm$5.2} \\
&3Dseg-reg
&- &- &2.34\tiny{$\pm$0.11} &- &- &- &- &- &9.40\tiny{$\pm$7.5} &- &- &- &- &- \\
&DTA
&\secbest{0.03\tiny{$\pm$0.0}} &\secbest{0.09\tiny{$\pm$0.0}} &{0.07\tiny{$\pm$0.0}} &\secbest{0.22\tiny{$\pm$0.1}} &\secbest{0.10\tiny{$\pm$0.0}} &\secbest{0.07\tiny{$\pm$0.0}} &\secbest{0.11\tiny{$\pm$0.0}} &\secbest{0.36\tiny{$\pm$0.1}} &\secbest{0.20\tiny{$\pm$0.1}} &{0.09\tiny{$\pm$0.0}} &\secbest{0.13\tiny{$\pm$0.0}} &{2.08\tiny{$\pm$0.0}} 
&13.64\tiny{$\pm$3.6} &7.86\tiny{$\pm$1.8} \\
&Ours
&\best{0.01\tiny{$\pm$0.0}} &\best{0.03\tiny{$\pm$0.0}} &\best{0.01\tiny{$\pm$0.0}} &\best{0.01\tiny{$\pm$0.0}} &\best{0.05\tiny{$\pm$0.0}} &\best{0.01\tiny{$\pm$0.0}} &\best{0.04\tiny{$\pm$0.0}} &\best{0.02\tiny{$\pm$0.0}} &\best{0.03\tiny{$\pm$0.0}} &\best{0.01\tiny{$\pm$0.0}} &\best{0.02\tiny{$\pm$0.0}} & 2.09\tiny{$\pm$0.0} &\best{3.47\tiny{$\pm$0.3}} &\best{2.78\tiny{$\pm$0.2}} \\
\hline
\multirow{6}{*}{\shortstack{Axis\\Pos}} 
&Ditto
&3.77 &1.02 &\secbest{0.01} &0.13 &5.70 &0.20 &5.41 &0.66 &- &- &2.11 &1.84 &- &1.84 \\
% &PARIS 
% &0.45\tiny{$\pm$0.9} &0.38\tiny{$\pm$1.0} &\best{0.00\tiny{$\pm$0.0}} &0.00\tiny{$\pm$0.0} &2.10\tiny{$\pm$1.4} &2.27\tiny{$\pm$3.4} &2.36\tiny{$\pm$3.4} &1.50\tiny{$\pm$1.3} &- &- &1.13\tiny{$\pm$1.1} &\secbest{0.34\tiny{$\pm$0.2}} &- &\secbest{0.34\tiny{$\pm$0.2}} \\
&PARIS*
&0.25\tiny{$\pm$0.5} &1.13\tiny{$\pm$2.6} &\best{0.00\tiny{$\pm$0.0}} &0.05\tiny{$\pm$0.2} &1.59\tiny{$\pm$1.7} &4.67\tiny{$\pm$3.9} &3.35\tiny{$\pm$3.1} &3.28\tiny{$\pm$3.1} &- &- &1.79\tiny{$\pm$1.5} &\secbest{0.50\tiny{$\pm$0.0}} &{-} &\secbest{0.50\tiny{$\pm$0.0}} \\
&CSG-reg 
&0.02\tiny{$\pm$0.0} &\best{0.00\tiny{$\pm$0.0}} &0.20\tiny{$\pm$0.2} &0.18\tiny{$\pm$0.0} &\secbest{0.01\tiny{$\pm$0.0}} &\secbest{0.02\tiny{$\pm$0.0}} &\secbest{0.01\tiny{$\pm$0.0}} &2.13\tiny{$\pm$1.5} &- &- &0.32\tiny{$\pm$0.2} &1.46\tiny{$\pm$1.1} &- &1.46\tiny{$\pm$1.1} \\
&3Dseg-reg 
&- &- &0.10\tiny{$\pm$0.0} &- &- &- &- &- &- &- &- &- &- &- \\
&DTA
&\secbest{0.01\tiny{$\pm$0.0}} &\secbest{0.01\tiny{$\pm$0.0}} &\secbest{0.01\tiny{$\pm$0.0}}
&\secbest{0.01\tiny{$\pm$0.0}} &{0.02\tiny{$\pm$0.0}} &\secbest{0.02\tiny{$\pm$0.0}} &\best{0.00\tiny{$\pm$0.0}} &\secbest{0.05\tiny{$\pm$0.0}} 
&{-} &{-} &\secbest{0.02\tiny{$\pm$}0.0} &0.59\tiny{$\pm$0.0}
&- &0.59\tiny{$\pm$0.0} \\
&Ours
&\best{0.00\tiny{$\pm$0.0}} &\best{0.00\tiny{$\pm$0.0}} &\secbest{0.01\tiny{$\pm$0.0}} &\best{0.00\tiny{$\pm$0.0}} &\best{0.00\tiny{$\pm$0.0}} &\best{0.01\tiny{$\pm$0.0}} &\best{0.00\tiny{$\pm$0.0}} &\best{0.00\tiny{$\pm$0.0}} & {-} & {-} &\best{0.00\tiny{$\pm$0.0}} &\best{0.47\tiny{$\pm$0.0}} & {-} &\best{0.47\tiny{$\pm$0.0}} \\
\hline
\multirow{6}{*}{\shortstack{Part\\Motion}}
&Ditto
&99.36 &F &5.18 &2.09 &19.28 &56.61 &80.60 &55.72 &F &0.09 &39.87 &8.43 &0.38 &4.41 \\
% &PARIS
% &131.66\tiny{$\pm$78.9} &24.58\tiny{$\pm$57.7} &0.03\tiny{$\pm$0.0} &0.03\tiny{$\pm$0.0} &120.70\tiny{$\pm$50.1} &110.80\tiny{$\pm$47.1} &64.85\tiny{$\pm$84.3} &60.35\tiny{$\pm$23.3} &0.34\tiny{$\pm$0.2} &0.30\tiny{$\pm$0.0} &51.36\tiny{$\pm$34.2} &\best{2.16\tiny{$\pm$1.1}} &0.56\tiny{$\pm$0.4} &1.36\tiny{$\pm$0.7} \\
&PARIS*
&127.34\tiny{$\pm$75.0} &45.26\tiny{$\pm$58.5} &\secbest{0.03\tiny{$\pm$0.0}} &9.13\tiny{$\pm$28.8} &68.36\tiny{$\pm$64.8} &107.76\tiny{$\pm$68.1} &96.93\tiny{$\pm$67.8} &49.77\tiny{$\pm$26.5} &0.36\tiny{$\pm$0.2} &0.30\tiny{$\pm$0.0} &50.52\tiny{$\pm$39.0} &\best{1.58\tiny{$\pm$0.0}} &0.57\tiny{$\pm$0.1} &1.07\tiny{$\pm$0.1} \\
&CSG-reg 
&0.13\tiny{$\pm$0.0} &0.29\tiny{$\pm$0.0} &0.35\tiny{$\pm$0.0} &0.58\tiny{$\pm$0.0} &\secbest{0.20\tiny{$\pm$0.0}} &0.44\tiny{$\pm$0.0} &10.48\tiny{$\pm$9.3} &158.99\tiny{$\pm$8.8} &\secbest{0.05\tiny{$\pm$0.0}} &\secbest{0.04\tiny{$\pm$0.0}} &17.16\tiny{$\pm$1.8} &14.82\tiny{$\pm$0.1}
&0.64\tiny{$\pm$0.1} &7.73\tiny{$\pm$0.1} \\
&3Dseg-reg
&- &- &1.61\tiny{$\pm$0.1} &- &- &- &- &- &0.15\tiny{$\pm$0.0} &- &- &- &- &- \\
&DTA
&\secbest{0.10\tiny{$\pm$0.0}} &\secbest{0.12\tiny{$\pm$0.0}} &0.11\tiny{$\pm$0.0} &\secbest{0.12\tiny{$\pm$0.0}} &{0.37\tiny{$\pm$0.6}} &\secbest{0.08\tiny{$\pm$0.0}} &\secbest{0.15\tiny{$\pm$0.0}} 
&\secbest{0.28\tiny{$\pm$0.1}} &\best{0.00\tiny{$\pm$0.0}} &\best{0.00\tiny{$\pm$0.0}} &\secbest{0.13\tiny{$\pm$0.1}} &\secbest{1.85\tiny{$\pm$0.0}} &\secbest{0.14\tiny{$\pm$0.0}} &\secbest{1.00\tiny{$\pm$0.0}} \\
&Ours
&\best{0.03\tiny{$\pm$0.0}} &\best{0.04\tiny{$\pm$0.0}}
&\best{0.02\tiny{$\pm$0.0}} &\best{0.02\tiny{$\pm$0.0}}
&\best{0.04\tiny{$\pm$0.0}} &\best{0.01\tiny{$\pm$0.0}}
&\best{0.03\tiny{$\pm$0.0}} &\best{0.03\tiny{$\pm$0.0}}
&\best{0.00\tiny{$\pm$0.0}} &\best{0.00\tiny{$\pm$0.0}}
&\best{0.02\tiny{$\pm$0.0}} & 1.94\tiny{$\pm$0.0}
&\best{0.04\tiny{$\pm$0.0}} &\best{0.99\tiny{$\pm$0.0}} \\
\hline
\multirow{6}{*}{\shortstack{CD-s}} 
&Ditto
&33.79 &3.05 &\secbest{0.25} &\best{2.52} &39.07 &41.64 &2.64 &10.32 &46.90 &9.18 &18.94 &47.01 &16.09 &31.55 \\
% &PARIS 
% &9.16\tiny{$\pm$5.0} &3.65\tiny{$\pm$2.7} &\best{0.16\tiny{$\pm$0.0}} &\secbest{12.95\tiny{$\pm$1.0}} &1.94\tiny{$\pm$3.8} &\best{1.88\tiny{$\pm$0.2}} &\best{2.69\tiny{$\pm$0.3}} &25.39\tiny{$\pm$2.2} &1.19\tiny{$\pm$0.6} &12.76\tiny{$\pm$2.5} &7.18\tiny{$\pm$1.8} &42.57\tiny{$\pm$34.1} &54.54\tiny{$\pm$30.1} &48.56\tiny{$\pm$32.1} \\
&PARIS*
&10.20\tiny{$\pm$5.8} &8.82\tiny{$\pm$12.0} &\best{0.16\tiny{$\pm$0.0}} &\secbest{3.18\tiny{$\pm$0.3}} &15.58\tiny{$\pm$13.3} &\best{2.48\tiny{$\pm$1.9}} &\best{1.95\tiny{$\pm$0.5}} &12.19\tiny{$\pm$3.7} &1.40\tiny{$\pm$0.7} &8.67\tiny{$\pm$0.8} &6.46\tiny{$\pm$3.9} &11.64\tiny{$\pm$1.5} &20.25\tiny{$\pm$2.8} &15.94\tiny{$\pm$2.1} \\
&CSG-reg
&1.69 &1.45 &0.32 &3.93 &\secbest{3.26} &\secbest{2.22} &\best{1.95} &\best{4.53} &0.59 &\secbest{7.06} &2.70 &6.33 &12.55 &9.44 \\
&3Dseg-reg
&- &- &0.76 &- &- &- &- &- &66.31 &- &- &- &- &- \\
&DTA
&\best{0.18\tiny{$\pm$0.0}} &\secbest{0.62\tiny{$\pm$0.0}} &0.30\tiny{$\pm$0.0} &4.60\tiny{$\pm$0.1} &{3.55\tiny{$\pm$6.1}} &{2.91\tiny{$\pm$0.1}} &2.32\tiny{$\pm$0.1} 
&\secbest{4.56\tiny{$\pm$0.1}} &\secbest{0.55\tiny{$\pm$0.0}} &\best{4.90\tiny{$\pm$0.5}} &\best{2.45\tiny{$\pm$0.7}} &\secbest{2.36\tiny{$\pm$0.1}} &\secbest{10.98\tiny{$\pm$0.1}} &\secbest{6.67\tiny{$\pm$0.1}} \\
&Ours
&\secbest{0.26\tiny{$\pm$0.3}} &\best{0.52\tiny{$\pm$0.0}} 
&0.63\tiny{$\pm$0.0} &3.88\tiny{$\pm$0.0} 
&\best{0.61\tiny{$\pm$0.3}} &3.83\tiny{$\pm$0.1} 
&\secbest{2.25\tiny{$\pm$0.2}} &{6.43\tiny{$\pm$0.1}} &\best{0.54\tiny{$\pm$0.0}} &{7.31\tiny{$\pm$0.2}} 
&\secbest{2.63\tiny{$\pm$0.1}} &\best{1.64\tiny{$\pm$0.2}} 
&\best{2.93\tiny{$\pm$0.3}} &\best{2.29\tiny{$\pm$0.3}} \\
\hline
\multirow{6}{*}{\shortstack{CD-m}}
&Ditto
&141.11 &0.99 &\secbest{0.19} &0.94 &20.68 &31.21 &15.88 &12.89 &195.93 &2.20 &42.20 &50.60 &\secbest{20.35} &35.48 \\
% &PARIS 
% &8.99\tiny{$\pm$7.6} &7.76\tiny{$\pm$11.2} &\best{0.21\tiny{$\pm$0.2}} &\secbest{28.70\tiny{$\pm$15.2}} &46.64\tiny{$\pm$40.7} &9.27\tiny{$\pm$30.7}
% &5.32\tiny{$\pm$5.9} &178.43\tiny{$\pm$131.7} &25.21\tiny{$\pm$9.5} &76.69\tiny{$\pm$6.1} &39.72\tiny{$\pm$25.9} &45.66\tiny{$\pm$31.7} &864.82\tiny{$\pm$382.9} &455.24\tiny{$\pm$207.3} \\
&PARIS*
&17.97\tiny{$\pm$24.9} &7.23\tiny{$\pm$11.5} &0.15\tiny{$\pm$0.0} &6.54\tiny{$\pm$10.6} &16.65\tiny{$\pm$16.6} &30.46\tiny{$\pm$37.0} &10.17\tiny{$\pm$6.9} &265.27\tiny{$\pm$248.7} &117.99\tiny{$\pm$213.0} &52.34\tiny{$\pm$11.0} &52.48\tiny{$\pm$58.0} &77.85\tiny{$\pm$26.8} &474.57\tiny{$\pm$227.2} &276.21\tiny{$\pm$127.0} \\
&CSG-reg
&1.91 &21.71 &0.42 &256.99 &\secbest{1.95} &6.36 &29.78 &436.42 &26.62 &1.39 &78.36 &442.17 &521.49 &481.83 \\
&3Dseg-reg
&- &- &1.01 &- &- &- &- &- &6.23 &- &- &- &- &- \\
&DTA
&\best{0.15\tiny{$\pm$0.0}} &\secbest{0.27\tiny{$\pm$0.0}} &\best{0.13\tiny{$\pm$0.0}} &\best{0.44\tiny{$\pm$0.0}} &{10.11\tiny{$\pm$19.4}} &\secbest{1.13\tiny{$\pm$0.5}} &\secbest{1.47\tiny{$\pm$0.0}} 
&\best{0.45\tiny{$\pm$0.0}} &\secbest{2.05\tiny{$\pm$0.3}} &\best{0.36\tiny{$\pm$0.0}} &\secbest{1.66\tiny{$\pm$2.0}} &\secbest{1.12\tiny{$\pm$0.0}} &30.78\tiny{$\pm$2.6} &\secbest{15.95\tiny{$\pm$1.3}} \\
&Ours
&\secbest{0.54\tiny{$\pm$0.1}} &\best{0.21\tiny{$\pm$0.0}} &\best{0.13\tiny{$\pm$0.0}} & \secbest{0.89\tiny{$\pm$0.2}} &\best{0.64\tiny{$\pm$0.4}} &\best{ 0.52\tiny{$\pm$0.1}} &\best{1.22\tiny{$\pm$0.1}} &\best{0.45\tiny{$\pm$0.2}} &\best{1.12\tiny{$\pm$0.2}} & \secbest{1.02\tiny{$\pm$0.4}} &\best{0.67\tiny{$\pm$0.2}} &\best{0.66\tiny{$\pm$0.2}} &\best{6.28\tiny{$\pm$3.6}} &\best{3.47\tiny{$\pm$1.9}} \\

\hline
\multirow{6}{*}{\shortstack{CD-w}}
&Ditto
&6.80 &2.16 &\secbest{0.31} &\best{2.51} &1.70 &2.38 &2.09 &7.29 &42.04 &\best{3.91} &7.12 &6.50 &14.08 &10.29 \\
&PARIS* 
&4.37\tiny{$\pm$6.4} &5.53\tiny{$\pm$4.7} &\best{0.26\tiny{$\pm$0.0}} &{3.18\tiny{$\pm$0.3}} &3.90\tiny{$\pm$3.6} &5.27\tiny{$\pm$5.9} &1.78\tiny{$\pm$0.2} &10.11\tiny{$\pm$2.8} &0.58\tiny{$\pm$0.1} &7.80\tiny{$\pm$0.4} &4.28\tiny{$\pm$2.4} &8.99\tiny{$\pm$1.4} &32.10\tiny{$\pm$8.2} &20.55\tiny{$\pm$4.8} \\
&CSG-reg
&0.48 &0.98 &0.40 &\secbest{3.00} &1.70 &\secbest{1.99} &\secbest{1.20} &\best{4.48} &\secbest{0.56} &4.00 &\secbest{1.88} &5.71 &14.29 &10.00 \\
&3Dseg-reg
&- &- &0.81 &- &- &- &- &- &0.78 &- &- &- &- &- \\
&DTA
&\best{0.27\tiny{$\pm$0.0}} &\secbest{0.70\tiny{$\pm$0.0}} &0.32\tiny{$\pm$0.0} &4.24\tiny{$\pm$0.1} &\best{0.41\tiny{$\pm$0.0}} &\best{1.92\tiny{$\pm$0.0}} &\best{1.17\tiny{$\pm$0.0} }
&\best{4.48\tiny{$\pm$0.2}} &\best{0.36\tiny{$\pm$0.0}} &\secbest{3.99\tiny{$\pm$0.4}} &\best{1.79\tiny{$\pm$0.1}} &\secbest{2.08\tiny{$\pm$0.1}} &\secbest{8.98\tiny{$\pm$0.1}} &\secbest{5.53\tiny{$\pm$0.1}} \\
&Ours
&\secbest{0.43\tiny{$\pm$0.2}} &\best{0.58\tiny{$\pm$0.0}} & 0.50\tiny{$\pm$0.0} &3.58\tiny{$\pm$0.0} & \secbest{0.67\tiny{$\pm$0.3}} & {2.63\tiny{$\pm$0.0}} & {1.28\tiny{$\pm$0.0}} & \secbest{5.99\tiny{$\pm$0.1}} & 0.61\tiny{$\pm$0.0} & 5.21\tiny{$\pm$0.1} & {2.15\tiny{$\pm$0.1}} &\best{1.29\tiny{$\pm$0.1}} &\best{3.23\tiny{$\pm$0.1}} &\best{2.26\tiny{$\pm$0.1}} \\
% \hline
% \multirow{2}{*}{\shortstack{CD-w\\mean}}
% &DTA
% &0.26 &0.71 &0.34 &4.27 &0.41 &2.01 &1.36 
% &4.55 &0.36 &4.09 &1.84 &2.10 &9.03 &5.57 \\
% &Ours
% &0.30 &0.59 &0.48 &3.62 &0.53 &{2.87} &1.57 
% &6.04 &0.62 &5.12 &2.17 &1.45 &2.72 &2.09 \\
% \hline
% \multirow{2}{*}{\shortstack{CD-w\\mean}}
% &DTA
% &0.26 &0.71 &0.34 &4.27 &0.41 &2.01 &1.36 
% &4.55 &0.36 &4.09 &1.84 &2.10 &9.03 &5.57 \\
% &Ours
% &0.30 &0.59 &0.48 &3.62 &0.53 &{2.87} &1.57 
% &6.04 &0.62 &5.12 &2.17 &1.45 &2.72 &2.09 \\
% \hline
% \multirow{2}{*}{\shortstack{CD-w\\mean}}
% &DTA
% &0.26 &0.71 &0.34 &4.27 &0.41 &2.01 &1.36 
% &4.55 &0.36 &4.09 &1.84 &2.10 &9.03 &5.57 \\
% &Ours
% &0.30 &0.59 &0.48 &3.62 &0.53 &{2.87} &1.57 
% &6.04 &0.62 &5.12 &2.17&1.45 &2.72 &2.09 \\
\hline
\multirow{2}{*}{\shortstack{Time\\(min)}}
&DTA
&29 &30 &31 &29 &28 &29 &31 &28 &27 &28 &29 &29 &29 &29 \\
&Ours 
&9 &8 &7 &7 &7 &7 &7 &8 &7 &8 &8 &9 &9 &9 \\
\hline
\end{tabular}
}
\vspace{-10pt}
\end{table*}
\begin{figure}[t!]
 \centering
 \resizebox{\linewidth}{!}{\includegraphics[width=\linewidth]{figure/vis_exp_2part_1.pdf}}
 \vspace{-10pt}
 \caption{\textbf{Qualitative visualizations of PARIS objects.} We present reconstruction comparisons between DTA and our model on Real Storage (Top) and Synthetic Blade (Bottom). DTA struggles with mesh reconstruction at the low-visibility state, as it processes each state separately. In contrast, our method leverages the connection between states to improve the reconstruction for both low- and high-visibility states.}
 \label{fig:2part}
 \vspace{-10pt}
\end{figure}

% For CD-w and CD-s that reflect the reconstruction quality of whole and the static part, DTA has some advantages because it's reconstructed for each state individually, but \model still achieves the best or comparable results. Additionally, as shown in \cref{fig:2part}, the DTA faces a challenge in reconstructing objects when one of the states has low visibility because the states are reconstructed separately. Instead, our method takes advantage of the connection between the two states, ensuring that the mesh of the two states remains highly consistent and enabling them to assist each other in the reconstruction process.

% Notably, our method shows markable improvement for real-world objects (Fridge and Storage), demonstrating its ability to generalize to the real-world. Furthermore, our approach is highly efficient, requiring an average of 8-9 minutes of training time compared to 28-31 minutes for DTA, demonstrating a significant reduction in computational cost.



\subsection{Results on Complex Articulated Objects with Multiple Movable Parts}
\label{sec:exp:multi-part}

\paragraph{Experimental Setup} We use DTA-Multi and \model-Multi as benchmarks for evaluating complex articulated object reconstruction. On DTA-Multi, we compare our model against PARIS and DTA, while on \model-Multi we use DTA as the main baseline given its strong performance. Similar to~\cref{sec:exp:two-part}, we report all metrics with a mean over 10 trials for DTA-Multi and 3 trials for \model-multi because of the training time required for baselines. For \model-multi, we report the average of all movable parts for articulation estimation and mesh reconstruction due to the large number of parts. Considering the potential error prediction with no mesh for one of the parts, we manually set the Chamfer Distance of the empty prediction to 1000. 


\begin{table*}[t]
\caption{\textbf{Quantitative evaluation on DTA-Multi.} We report averaged metrics over 10 trials with different random seeds. Lower ($\downarrow$) is better on all metrics. Joint 1 of ``Storage-m'' is prismatic with no Axis Pos.}
\label{tab:exp_mpart_dta}
\renewcommand{\arraystretch}{1.2}
\resizebox{\linewidth}{!}{
\begin{tabular}{ccccccccccccc}
\toprule
Object &Method
&Axis Ang 0 &Axis Ang 1 &Axis Pos 0 &Axis Pos 1 
&Part Motion 0 &Part Motion 1 &CD-s &CD-m 0 &CD-m 1 &CD-w &Time (min)\\
\midrule
\multirow{3}{*}{Fridge-m} 
&PARIS
&34.52 &15.91 &3.60 &1.63 &86.21 &105.86 &8.52 &526.19 &160.86 &15.00 &- \\
&DTA
&0.25 &0.06 &0.01 &0.01 &0.23 &0.08 &0.63 &0.44 &0.53 &0.88 &32 \\
&Ours 
&\textbf{0.02} &\textbf{0.00} &\textbf{0.00} &\textbf{0.00} &\textbf{0.02} &\textbf{0.03} & \textbf{0.62} &\textbf{0.07} &\textbf{0.18} &\textbf{0.75} &\textbf{8} \\
\midrule
\multirow{3}{*}{Storage-m} 
&PARIS
&43.26 &26.18 &10.42 &- &79.84 &0.64 &8.56 &128.62 &266.71 &8.66 &- \\
&DTA
&0.17 &0.40 &0.04 &- &0.13 &0.00 &0.86 &0.20 & \textbf{0.25} &0.97 &32 \\
&Ours
&\textbf{0.01} & \textbf{0.02} & \textbf{0.01} & - & \textbf{0.01} & \textbf{0.00} & \textbf{0.78} & \textbf{0.19} & 0.27 & \textbf{0.93} &\textbf{8} \\
 \bottomrule
\end{tabular}
}
\vspace{-5pt}
\end{table*}

\begin{table*}[t]
\caption{\textbf{Quantitative evaluation on \model-Multi}. Metrics are averaged over 3 trials. Due to the large number of parts, we report the average metric for all movable parts. Lower ($\downarrow$) is better on all metrics. ``Table-31249'' has 3 prismatic joints with no Axis Pos. }
\label{tab:exp_mpart_our}
\renewcommand{\arraystretch}{1.2}
\resizebox{\linewidth}{!}{
\begin{tabular}{ccccccccccc}
\toprule
Object &Method
&Axis Ang &Axis Pos &Part Motion &CD-s &CD-m &CD-w &Time (min)\\
\midrule
\multirow{2}{*}{\shortstack{Table \\ \scriptsize{25493 (4 parts)}}}
&DTA 
&24.35 &- &0.12 &\textbf{0.59} &104.38 &\textbf{0.55} &34 \\
&Ours 
&\textbf{1.16} &- &\textbf{0.00} &0.74 &\textbf{3.53} &0.74 &\textbf{8} \\
\midrule
\multirow{2}{*}{\shortstack{Table \\ \scriptsize{31249 (5 parts)}}}
&DTA 
&20.62 &4.2 &30.8 &1.39 &230.38 &\textbf{1.00} &37 \\
&Ours 
&\textbf{0.04} &\textbf{0.00} &\textbf{0.01} &\textbf{1.22} &\textbf{3.09} &1.16 &\textbf{8} \\
\midrule
\multirow{2}{*}{\shortstack{Storage \\ \scriptsize{45503 (4 parts)}}}
&DTA 
&51.18 &2.44 &43.77 &5.74 &246.63 &0.88 &35 \\
&Ours 
&\textbf{0.02} &\textbf{0.00} &\textbf{0.03} &\textbf{0.75} &\textbf{0.13} &\textbf{0.88} &\textbf{8} \\
\midrule
\multirow{2}{*}{\shortstack{Storage \\ \scriptsize{47468 (7 parts)}}}
&DTA
&19.07 &0.31 &10.67 &0.82 &476.91 &0.71 &45 \\
&Ours 
&\textbf{0.14} &\textbf{0.02} &\textbf{0.62} &\textbf{0.67} &\textbf{3.70} &\textbf{0.70} &\textbf{8} \\
\midrule
\multirow{2}{*}{\shortstack{Oven \\ \scriptsize{101908 (4 parts)}}}
&DTA 
&17.83 &6.51 &31.80 &1.17 &359.16 &\textbf{1.01} &35 \\
&Ours 
&\textbf{0.04} &\textbf{0.01} &\textbf{0.23} &\textbf{1.08} &\textbf{0.25} &1.03 &\textbf{8} \\
\bottomrule 
\end{tabular}
}
\vspace{-5pt}
\end{table*}
\begin{figure}[t!]
 \centering
 \resizebox{\linewidth}{!}{\includegraphics[width=\linewidth]{figure/vis_exp_multi.pdf}}
 \caption{\textbf{Qualitative results on multi-part objects.} We present reconstruction comparisons between DTA and our model on Storage-47648 (Left) and Table-31249 (Bottom). On \model-Multi, DTA struggles with movable part identification and axis prediction as the number of parts increases, whereas our model maintains high performance regardless of part count, achieving high-quality reconstruction of part mesh and joint articulation.}
 \label{fig:mpart}
 \vspace{-10pt}
\end{figure}

\paragraph{Results} As demonstrated in~\cref{tab:exp_mpart_dta} and~\cref{tab:exp_mpart_our}, our method consistently outperforms existing methods by a large margin in both mesh reconstruction and articulation estimation. Notably, on \model-Multi, the baseline model DTA struggles with movable part identification and axis prediction as the number of parts increases, whereas our model maintains high performance regardless of part count. We also provide a qualitative comparison in~\cref{fig:mpart} for better visualization. Moreover, our method maintains the same time efficiency while the training time of existing methods scales with the number of parts. These results underscore the robustness and effectiveness of our method in modeling complex, multi-part articulated objects.

% \paragraph{Simple Multi-part Objects} As demonstrated in \cref{tab:exp_mpart_dta}, our method exhibits substantial advantages in modeling multi-part objects. Our method outperforms existing methods on almost all metrics. For joint parameter estimation, \model's estimation of joint parameters maintains high accuracy when it's extended to multi-part objects, bringing a large improvement compared with the existing methods. 
% In terms of mesh reconstruction, our method yields lower CD-s, CD-m, and CD-w values for all objects.

% \paragraph{Complex Multi-part Objects} Importantly, as the number of parts continues to increase, DTA struggles with objects having more than three movable parts (as shown in \cref{fig:mpart} and \cref{tab:exp_mpart_our}). \model maintains high quality on both joint parameter estimation and part mesh reconstruction as the number of parts increases. This demonstrates the generalization ability of \model to more complex articulated objects.

% In addition, the efficiency of our method is particularly evident in multi-part scenarios, requiring only about one-forth of the training time compared to DTA (8 minutes vs. 32-45 minutes).

% These results underscore the effectiveness of our method in handling complex, multi-part articulated objects, demonstrating significant improvements in both accuracy and computational efficiency over existing approaches.


\subsection{Ablative Studies}
\label{sec:exp:ablation}
\paragraph{Experimental Setup} To verify the effectiveness of our model design, we meticulously design four ablations of \model to identify the impact of key components in our method: (i) Randomly initializing canonical Gaussians (\textit{w/o} Cano. Init.), (ii) predicting part assignments with MLP (\textit{w/} MLP Seg) or Slot-Attention (\textit{w/} SA Seg), (iii) randomly initializing part centers $C_k$ (\textit{w/o} Center Init.), (iv) clustering all Gaussians instead of clustering movable Gaussians for part center initialization (\textit{w/o} Motion Prior), and (v) learning articulation parameters without the joint prediction warmup stage (\textit{w/o} Joint Pred.). We select two representative objects: ``Storage-47648'' with 4 revolute and 2 prismatic joints and ``Oven-101908'' with 3 revolute joints for ablative analysis. Similar to~\cref{sec:exp:multi-part}, we report the average of all parts over 10 trials for all metrics.

\begin{table}[t]
\centering
\caption{\textbf{Ablative experiments}. Lower ($\downarrow$) is better on all metrics.}
\label{tab:ablation}
\renewcommand{\arraystretch}{1.2}
\resizebox{\linewidth}{!}{
\begin{tabular}{lcccccccccccc}
\toprule
\multirow{2}[2]{*}{Method} & \multicolumn{6}{c}{Storage 47648 (7 parts)} & \multicolumn{6}{c}{Oven 101908 (4 parts)} \\
\cmidrule(lr){2-7}\cmidrule{8-13}
&Axis Ang &Axis Pos &Part Motion &CD-s &CD-m & CD-w & Axis Ang &Axis Pos &Part Motion &CD-s &CD-m &CD-w \\
\midrule
Full
&\textbf{0.14} &\textbf{0.02} &\textbf{0.62} &\textbf{0.67} &\textbf{3.70} &\textbf{0.70} 
&\textbf{0.04} &\textbf{0.01} &\textbf{0.23} &\textbf{1.08} &\textbf{0.25} &\textbf{1.03} \\
\textit{w/o} Cano. init.
&24.15 & 0.73 & 20.61 & 0.83 & 495.07 & 1.25 
& 57.87 & 2.95 & 54.45 & 1.73 & 1030.19 & 2.36 \\
\textit{w/o} Center Init.
& 52.78 & 0.83 & 33.04 & 1.09 & 344.19 & 1.69 
& 28.94 & 2.36 & 22.46 & 1.41 & 8.86 & 2.13 \\ 
\textit{w/o} Motion Prior
& 26.74 & 0.22 & 21.16 & 258.23 & 599.46 & 1.15 
& 40.08 & 0.98 & 41.06 & 1.75 & 503.44 & 2.35 \\
\textit{w/o} Joint Pred. 
&{0.16} &{0.02} &{0.72} &{0.67} &{3.90} &{0.71}
&{0.04} &{0.01} &{0.23} &{1.08} &{0.25} &{1.03} \\
\textit{w/} MLP Seg 
& 21.84 & 3.46 & 31.43 & 1.82 & 664.25 & 1.28 
& 12.08 & 3.33 & 27.28 & 7.78 & 126.95 & 2.19 \\
\textit{w/} SA Seg 
& 25.43 & 0.7 & 23.22 & 1.52 & 459.89 & 1.16 
& 58.04 & 4.53 & 51.28 & 1.26 & 496.64 & 2.35 \\
\bottomrule
\end{tabular}
}
\vspace{-10pt}
\end{table}

\begin{figure}[t!]
 \centering
 \resizebox{\linewidth}{!}{\includegraphics[width=\linewidth]{figure/ablation.pdf}}
 \caption{\textbf{Abaltion Studies}. We visualize the initialized and optimized canonical Gaussians with their part assignment and centers for the full model, w/o Motion Prior and w/o Cano. Init. We highlight center error, part assignment error, and canonical Gaussian error with \textcolor{red}{red}, \textcolor{green}{green}, and \textcolor{blue}{blue} bounding boxes separately.}
 \label{fig:ablation}
 \vspace{-10pt}
\end{figure}

\paragraph{Results and Discussions} As shown in~\cref{tab:ablation} and \cref{fig:ablation}, we make the following observations:
\begin{itemize}[leftmargin=*,nolistsep,noitemsep]
\item\textit{Canonical Gaussians Initialization.} Omitting this initialization strategy significantly degrades the model performance across all metrics, particularly for movable parts. As illustrated in \cref{fig:ablation} (5), the absence of our initialization strategy leads to malformed canonical Gaussians, making the model converge to suboptimal local minima during optimization.

\item\textit{Center-based Part Modeling and Assignment.} Replacing our center-based part assignment module with MLP or Slot-Attention ("w/ MLP Seg" and "w/ SA Seg") leads to substantial performance drops, especially in joint parameter estimation and movable part reconstruction. This demonstrates the superiority of our center-based approach in accurately segmenting articulated parts.

\item\textit{Center Initialization.} Random center initialization performs well for static parts but poorly for movable parts. Clustering all Gaussians fails to reconstruct both static and movable parts due to incorrect center initialization. As illustrated in \cref{fig:ablation} (1), clustering on movable Gaussians still produces an incorrect center but provides a good starting point for optimization. Our \model will refine the centers in the optimization process as shown in \cref{fig:ablation} (3). In contrast, clustering on all Gaussians results in entirely wrong center initialization (\cref{fig:ablation} (2)), which is difficult to correct (\cref{fig:ablation} (4)), leading to even worse performance than random initialization. This highlights the importance of our center initialization strategy in achieving accurate part articulation modeling.

\item\textit{Joint Prediction Warmup.} This technique primarily affects prismatic joints, as we do not constrain the transformation of revolute joints. As shown in \cref{tab:ablation}, predicting the joint type and then refining joint parameters with type constraints slightly improves the articulation reconstruction.
\end{itemize}

In summary, these ablation studies confirm that each component contributes significantly to its overall performance, playing crucial roles in achieving accurate joint parameter estimation and high-quality part mesh reconstruction. We provide further discussions in~\cref{app:discussion} and ~\cref{app:limitation}.

\section{Conclusion}
\label{sec:conclusion}
In conclusion, we propose \model, a novel approach for reconstructing articulated objects from two states of multi-view images. By leveraging 3D Gaussians and introducing novel techniques for state alignment and part dynamics modeling, our approach overcomes key limitations of existing methods. The performance improvements in joint parameter estimation and part mesh reconstruction, particularly for complex multi-part objects, demonstrate the effectiveness of our innovations. Our comprehensive experiments across synthetic and real-world datasets validate the robustness and efficiency of \model, while also revealing promising directions for future research. As the demand for accurate digital replicas of articulated objects continues to grow in fields such as robotics and augmented reality, \model provides a solid foundation for bridging the gap between physical and virtual environments. Moving forward, we anticipate that the principles introduced in this work will inspire further advancements in the field, ultimately enabling more sophisticated and realistic simulations for a wide range of applications.

% \bibliography{ref}
\bibliographystyle{iclr2025_conference}
\input{main.bbl}

\clearpage

\appendix

\renewcommand{\thefigure}{A.\arabic{figure}}
\renewcommand{\thetable}{A.\arabic{table}}
\renewcommand{\theequation}{A.\arabic{equation}}
% \renewcommand{\thesection}{S.\arabic{section}}
\setcounter{section}{0}
\setcounter{figure}{0}
\setcounter{table}{0}
\setcounter{equation}{0}


\section{Implementation and Training Details}
\label{app:imp}
\paragraph{Canonical Gaussian Initialzation}
We train single-state Gaussians $\mathcal{G}^0$ and $\mathcal{G}^1$ for 10K steps with loss $\mathcal{L}=(1-\lambda_\text{{SSIM}})\mathcal{L}_I+\lambda_\text{{SSIM}}\mathcal{L}_{\text{D-SSIM}}+\lambda_o\mathcal{L}_o$, where $\lambda_\text{{SSIM}}=0.2,\lambda_o=0.01$ is used in experiments and $\mathcal{L}_o$ is an opacity entropy loss calculated as:
$$
\hat{\sigma}_i=\mathbb{1}\{\sigma_i>0.5\},\quad
\mathcal{L}_o=-\frac{1}{N}\sum_{i=1}^{N}[\hat{\sigma}_i\sigma_i+(1-\hat{\sigma}_i)\log(1-\sigma_i)],
$$ 
which encourages Gaussian opacities $\sigma_i$ to approach either 0 or 1, controlling Gaussian count and accelerating training. We then obtain coarse canonical Gaussians by matching $\mathcal{G}^0$ and $\mathcal{G}^1$ as described in \cref{sec:method:canonical}. This stage takes about 2 minutes per object.
\paragraph{Part Discovery for Articulation Modeling} 
\label{app:imp:assignment}
As described in \cref{sec:method:skinning}, given canonical Gaussians $\mathcal{G}^c = \{G_i\}_{i=1}^{N}$ and $K$ learnable part centers $C_k = (\vp_k, \mR_k, {\bm \lambda}_k)$, we calculate part-level masks $\mM$ using \cref{eq:part_assignment}. We use a learnable hash grid $H$ to encode Gaussian positions and predict the residual term in \cref{eq:part_assignment} as:
\begin{equation*}
\begin{aligned}
\rmX^k_i = \frac{[\mR_k(\vmu^c_i-\vp_k)]}{{\bm\lambda}_k}, &\quad
 \rmD_{ik} = (\rmX^k_i)^T\cdot\rmX^k_i \\
{\mW_\Delta}_{ik}=\mathrm{MLP}(\vmu^c_i,H(\vmu^c_i),\{X^k_i\}_{k=1}^K,\{D_{ik}\}_{k=1}^K), &\quad\bm{M} = \mathrm{GumbelSoftmax}\left(\frac{-\mD + \mW_{\Delta}}{\tau}\right) \\
\end{aligned}
\end{equation*}
Since the part assignment and articulation parameters are far from optimal at the beginning of training, using hard assignment for Gumbel-Softmax hinders the joint optimization of the part assignment and articulation parameters. To address this problem, we anneal the temperature $\tau$ from 1 to 0.1 over 10K steps, using soft assignment that is similar to Softmax when $\tau > 0.1$ and hard assignment otherwise for training stability. This approach allows for more flexible assignments during the early stages of training, facilitating better joint optimization, and gradually transitioning to decisive part assignment as the model converges.

\paragraph{Optimization}
To enhance the learning of articulation parameters, we adopt a warm-up strategy to predict the joint type of each part. This process requires 3K-5K steps that take 30 to 50 seconds. Then we train \model with joint type constraint for 20K steps, taking 5-7 minutes per object. 
For hyper-parameters, we set the threshold $\epsilon_{\text{static}}$ to identify static/movable Gaussians as $\epsilon_{\text{static}}=0.02 \cdot \max_{i} \text{CD}_{i}^{t\rightarrow \bar{t}}$ for two-part objects and $\epsilon_{\text{static}}=0.05 \cdot \max_{i} \text{CD}_{i}^{t\rightarrow \bar{t}}$ for multi-part objects. We use $\epsilon_{\text{revol}}=10^\degree$ for predicting joint types following PARIS~\citep{jiayi2023paris}.
$\lambda_{cd}$ and $\lambda_{reg}$ are set as 100 and 0.1 separately.
In addition, the CD loss in \cref{eq:cd_loss} aims to decrease the distance between a deformed Gaussian $G_i^t$ and its nearest Gaussians $\hat{G}_i^t$ in $\mathcal{G}^t_{\text{single}}$. Since the deformed Gaussians and canonical Gaussians for a prismatic joint have a large overlap, the nearest Gaussian may be in the opposite direction of the ideal one, making it ineffective for prismatic joints. Thus the CD loss is only used for regularizing the objects that only have revolute joints.
Moreover, the densification strategy of Gaussians is cloning or splitting one Gaussian when the gradient of its center $\vmu$ is greater than a threshold $\epsilon_{\text{densify}}$. This is effective for static scenes but meets challenges for dynamic scenes. In the early stage of training, the large gradient is often due to deformation error. To prevent excessive increase of Gaussian quantity due to deformation error, we raised this threshold $\epsilon_{\text{densify}}$ from 0.0002 used in previous works \citep{kerbl20233d,huang2024sc} to 0.001.

\section{Additional Discussions}
\label{app:discussion}
\begin{table*}[t]
\caption{\textbf{Quantitative evaluation of each state on PARIS data.} We report the average of metrics over 10 trials of each state. "metric-0/1" represents the metric evaluated at state 0/1 and "metric-m" is the average of two states. We highlight \colorbox[HTML]{ffc5c5}{best} results on average of two states. Axis Pos. is omitted for prismatic joints (Blade, Storage, and Real Storage).}
\label{tab:app:exp_2part}
\renewcommand{\arraystretch}{1.2}
\resizebox{\linewidth}{!}{
\begin{tabular}{cc|ccccccccccc|ccc}
\hline
\multirow{2}{*}{Metric} &\multirow{2}{*}{Method} &\multicolumn{11}{c}{Synthetic Objects} &\multicolumn{3}{|c}{Real Objects} \\
% \cmidrule(lr){}
& &FoldChair &Fridge &Laptop &Oven &Scissor &Stapler &USB 
&Washer&Blade &Storage &All & Fridge &Storage &All \\
\hline
\multirow{6}{*}{\shortstack{Axis\\Ang}} 
&DTA-0
&0.03 &0.09 &0.07 &0.22 &0.10 &0.06 &0.11 &0.36 &0.20 &0.07 &0.13 &2.08 &13.64 &7.86 \\
&Ours-0
&0.01 &0.03 &0.01 &0.01 &0.05 &0.01 &0.04 &0.02 &0.03 &0.01 &0.02 & 2.09 &3.47 &2.78 \\
&DTA-1
&0.04 &0.10 &0.07 &0.23 &0.10 &0.07 &0.11 &0.36 &0.26 &0.09 &0.14 &2.07 &8.08 &5.08 \\
&Ours-1
&0.01 &0.03 &0.01 &0.01 &0.05 &0.01 &0.04 &0.02 &0.03 &0.01 &0.02 & 2.09 &3.47 &2.78 \\
&DTA-m
&0.04 &0.10 &0.07 &0.22 &0.10 &0.06 &0.11 &0.36 &0.23 &0.08 &0.14 &\best{2.08} &10.86 &6.47 \\
&Ours-m
&\best{0.01} &\best{0.03} &\best{0.01} &\best{0.01} &\best{0.05} &\best{0.01} &\best{0.04} &\best{0.02} &\best{0.03} &\best{0.01} &\best{0.02} & 2.09 &\best{3.47} &\best{2.78} \\
\hline
\multirow{6}{*}{\shortstack{Axis\\Pos}} 
&DTA-0
&0.01 &0.01 &0.01 &0.01 &0.03 &0.02 &0.00 &0.04 &- &- &0.02 &0.59 &- &0.59 \\
&Ours-0 
&0.00 &0.00 &0.01 &0.00 &0.00 &0.01 &0.00 &0.00 & - & - &0.00 &0.47 & - &0.47 \\
&DTA-1
&0.01 &0.01 &0.01 &0.01 &0.02 &0.02 &0.00 &0.05 &- &- &0.02 &0.59 &- &0.59 \\
&Ours-1
&0.00 &0.00 &0.01 &0.00 &0.00 &0.01 &0.00 &0.00 & - & - &0.00 &0.47 & - &0.47 \\
&DTA-m
&0.01 &0.01 &0.01 &0.01 &0.03 &0.02 &0.00 &0.04 &- &- &0.02 &0.59 &- &0.59 \\
&Ours-m
&\best{0.00} &\best{0.00} &\best{0.01} &\best{0.00} &\best{0.00} &\best{0.01} &\best{0.00} &\best{0.00} & \best{-} & \best{-} &\best{0.00} &\best{0.47} & \best{-} &\best{0.47} \\
\hline
\multirow{6}{*}{\shortstack{Part\\Motion}}
&DTA-0
&0.10 &0.12 &0.11 &0.12 &0.38 &0.08 &0.15 &0.28 &0.00 &0.00 &0.13 &1.85 &0.14 &1.00 \\
&Ours-0
&0.03 &0.04
&0.02 &0.02
&0.04 &0.01
&0.03 &0.03
&0.00 &0.00
&0.02 & 1.94
&0.04 &0.99 \\
&DTA-1
&0.09 &0.13 &0.11 &0.13 &0.37 &0.08 &0.14 &0.28 &0.00 &0.00 &0.13 &1.85 &0.09 &0.97 \\
&Ours-1
&0.03 &0.04
&0.02 &0.02
&0.04 &0.01
&0.03 &0.03
&0.00 &0.00
&0.02 & 1.94
&0.04 &0.99 \\
&DTA-m
&0.09 &0.12 &0.11 &0.12 &0.38 &0.08 &0.15 &0.28 &0.00 &0.00 &0.13 &\best{1.85} &0.12 &0.99 \\
&Ours-m
&\best{0.03} &\best{0.04}
&\best{0.02} &\best{0.02}
&\best{0.04} &\best{0.01}
&\best{0.03} &\best{0.03}
&\best{0.00} &\best{0.00}
&\best{0.02} & 1.94
&\best{0.04} &\best{0.99} \\
\hline
\multirow{6}{*}{\shortstack{CD-s}} 
&DTA-0
&0.18 &0.62 &0.32 &4.60 &3.30 &2.68 &2.32 &4.77 &0.55 &4.71 &2.41 &2.36 &10.98 &6.67 \\
&Ours-0
&0.26 & 0.52 & 0.59 & 3.88 & 0.62 & 3.85 & 2.25 & 6.41 & 0.54 & 7.47 & 2.64 & 1.64 & 2.93 & 2.29 \\
&DTA-1
&0.19 &0.63 &0.30 &4.58 &3.55 &2.91 &2.90 &4.56 &0.45 &4.90 &2.50 &2.59 &9.60 &6.10 \\
&Ours-1
&0.26 & 0.48 & 0.63 & 4.00 & 0.61 & 3.83 & 2.56 & 6.43 & 0.54 & 7.31 & 2.67 & 2.01 & 4.02 & 3.02 \\
&DTA-m
&\best{0.19} &0.62 &\best{0.31} &4.59 &3.43 &\best{2.79} &2.61 &\best{4.66} &\best{0.50} &\best{4.80} &\best{2.46} &2.48 &10.29 &6.39 \\
&Ours-m
&0.26 &\best{0.50} 
&0.61 &\best{3.94} 
&\best{0.61} &3.84 
&\best{2.41} &6.42 &0.54 &7.39
&2.65 &\best{1.82 } 
&\best{3.48} &\best{2.65} \\
\hline
\multirow{6}{*}{\shortstack{CD-m}}
&DTA-0
&0.15 &0.27 &0.16 &0.44 &17.38 &2.34 &1.47 &0.37 &2.05 &0.36 &2.50 &1.12 &30.78 &15.95 \\
&Ours-0
&0.54 & 0.21 & 0.14 & 0.89 & 0.65 & 0.88 & 1.22 & 1.54 & 1.12 & 1.03 & 0.82 & 0.66 & 6.28 & 3.47 \\
&DTA-1
&0.13 &0.30 &0.13 &0.45 &10.11 &1.13 &1.51 &0.45 &61.38 &0.36 &7.60 &1.85 &365.74 &183.80 \\
&Ours-1
&0.12 & 0.21 & 0.13 & 0.76 & 0.64 & 0.52 & 1.43 & 0.45 & 1.01 & 1.02 & 0.63 & 1.31 & 87.81 & 44.56 \\
&DTA-m
&\best{0.14} &0.28 &0.15 &\best{0.44} &13.75 &1.73 &1.49 &\best{0.41} &31.72 &\best{0.36} &5.05 &1.48 &198.26 &99.88 \\
&Ours-m
&0.33 &\best{0.21} &\best{0.14} &0.82 &\best{0.65} &\best{0.70} &\best{1.33} &1.00
&\best{1.06} &1.02 &\best{0.73} &\best{0.99} &\best{47.05} &\best{24.02} \\

\hline
\multirow{6}{*}{\shortstack{CD-w}}
&DTA-0
&0.27 &0.70 &0.35 &4.24 &0.42 &2.13 &1.17 &4.59 &0.36 &4.09 &1.83 &2.08 &8.98 &5.53 \\
&Ours-0
&0.43 & 0.58 & 0.47 & 3.58 & 0.69 & 3.13 & 1.28 & 6.12 & 0.61 & 5.13 & 2.20 & 1.29 & 3.23 & 2.26 \\
&DTA-1
&0.26 &0.70 &0.32 &4.27 &0.41 &1.92 &1.52 &4.48 &0.38 &3.99 &1.83 &2.19 &9.03 &5.61 \\
&Ours-1
&0.30 & 0.59 & 0.50 & 3.71 & 0.67 & 2.63 & 1.87 & 5.99 & 0.65 & 5.21 & 2.21 & 1.45 & 2.45 & 1.95 \\
&DTA-m
&\best{0.26} &0.70 &\best{0.34} &4.25 &\best{0.41} &\best{2.02} &\best{1.34} &\best{4.53} &\best{0.37} &\best{4.04} &\best{1.83} &2.13 &9.01 &5.57 \\
&Ours-m
&0.36 &\best{0.59} &0.48 &\best{3.64} &0.68 &2.88 & 1.58 &6.05 & 0.63 & 5.17 &{2.21} &\best{1.37} &\best{2.84} &\best{2.11} \\

\hline
\end{tabular}
}
\end{table*}

We present a comprehensive analysis of \model and DTA through additional quantitative and qualitative results. 

\paragraph{Visibility Problem} Our results uncover an intriguing inconsistency in DTA's performance across different states of the same object. As illustrated in \cref{tab:app:exp_2part}, DTA demonstrates good reconstruction quality in the high-visibility state but shows markedly poor performance in the low-visibility state. This limitation is particularly pronounced for objects with prismatic joints, such as real storage and blade. In these cases, DTA struggles to accurately capture the geometry and articulation of partially occluded parts. 
The observed inconsistency and state-dependent performance fluctuations underscore the necessity for a more robust approach that effectively connects and leverages information from multiple states. This is precisely where \model's strengths become evident. By establishing connections between different articulation states, \model achieves more consistent and high-quality reconstructions across varying object configurations.
Jointly optimizing over multiple states allows \model to:
1) Leverage complementary information from different articulation states,
2) Maintain consistency in part assignment and geometry across states,
3) Better handle occlusions and low-visibility scenarios by inferring occluded geometries from other states.
These capabilities enable \model to produce more accurate and reliable reconstructions, particularly in challenging scenarios. The superior performance of \model demonstrates its potential for robust articulated object reconstruction in real-world applications.

\paragraph{Significance of Part Assignment}
Through analysis of both qualitative (\cref{fig:app:mpart}) and quantitative (\cref{tab:exp_mpart_our}) results, we have identified that the model's ultimate performance is primarily determined by the accuracy of part assignment. When the model fails to correctly divide an object into parts, it becomes impossible to obtain reasonable joint parameter estimation. Conversely, even when joint parameter estimation is inaccurate, the model may still correctly separate the object's parts. This insight reveals that accurate part assignment is a crucial prerequisite for high-quality articulated object reconstruction.
Our findings emphasize that to enhance the reconstruction of articulated objects, the ability to reasonably separate parts is of paramount importance. \model addresses this challenge through the center-based segmentation and improved initialization by clustering. These techniques work in synergy to significantly improve the part segmentation capabilities of \model. By enhancing the model's ability to correctly identify and separate object parts, we lay a solid foundation for subsequent stages of the reconstruction process, including joint parameter estimation and final mesh reconstruction.

\section{Limitations}
\label{app:limitation}
\paragraph{Stability of Randomness.} 
\model exhibits enhanced robustness and stability across different random seeds, primarily due to our innovative initialization strategy for canonical Gaussians and our part assignment module. We observe that stability issues often stem from the initialization of three key components: canonical Gaussians $\mathcal{G}^c$, part centers $C$ in the part assignment module, and joint articulation parameters $\Psi$.
As demonstrated in \cref{sec:exp:ablation}, faulty initialization of $\mathcal{G}^c$ and $C$ can lead to significant performance degradation, particularly for complex objects with multiple movable parts. While our current initialization strategy has greatly improved stability, severe initialization errors in center $C$ may still result in part mis-segmentation. We can integrate prior models such as SAM~\citep{kirillov2023segment} to enhance the ability to correct center initialization errors.
Although \model works with randomly initialized $\Psi$, we have observed that improved initialization of $\Psi$ brings enhanced performance. Future work could explore the integration of heuristic algorithms or feed-forward articulation estimation models to provide better initial estimation for $\Psi$.

\paragraph{Limited States} 
Our current approach is limited to modeling articulated objects using only two states, which may not fully capture the complexity of real-world multi-part objects. 
Moreover, as the number of parts increases, distinguishing parts with similar joint axes and motion patterns (such as parallel drawers) becomes increasingly challenging, complicating the segmentation process.
To address this, two main avenues could be explored for future research:
1) Multi-state Extension: Develop a methodology to extend \model to handle multiple states that interact with different parts, potentially by identifying movable parts with a sequential state update mechanism. This would involve iteratively updating the model as new state information becomes available, allowing for a more comprehensive representation of the object's articulation space.
2) Continuous Temporal Reconstruction: Adapt \model to reconstruct articulated objects from monocular video sequences. This approach would leverage temporal information to infer a continuous range of articulation states, providing a more nuanced understanding of the object's movement capabilities.

\paragraph{Mesh Reconstruction Fidelity} 
Our current implementation utilizes the original Gaussian Splatting technique, which, while effective, has limitations in terms of mesh reconstruction quality compared with NeRF-based methods\citep{wang2021neus,yariv2021volume,wen2023bundlesdf}. Integrating recent advancements in reconstruction with Gaussian Splatting~\citep{huang20242d,chen2024pgsr} may help to improve the reconstruction fidelity of \model.

{\section{Additional Experiments}}
% \label{app:addi_exp}
% \subsection{{Additional Ablation Studies}}
% \label{app:addi_ab}
{\subsection{Additional Quantitative Comparisons}}
\label{app:addi_ac}
\begin{table*}[t]
\caption{{\textbf{Quantitative evaluation of Axis Pos metric on PARIS.} Metrics are reported as mean $\pm$ std over 10 trials on average of 2 states. We report the value timed by 1000 and highlight the \colorbox[HTML]{ffc5c5}{best} results.}}
\label{tab:app:exp_ap}
\renewcommand{\arraystretch}{1.2}
\resizebox{\linewidth}{!}{
{\begin{tabular}{cc|ccccccccc}
\hline
{Metric} &{Method} &FoldChair &Fridge &Laptop &Oven &Scissor &Stapler &USB 
&Washer &All \\
\hline
\multirow{2}{*}{\shortstack{Axis\\Pos}} 
&DTA
&{0.53\tiny{$\pm$0.3}} &{0.62\tiny{$\pm$0.3}} &{1.10\tiny{$\pm$0.7}}
&{1.49\tiny{$\pm$1.0}} &{2.48\tiny{$\pm$2.8}} &{2.21\tiny{$\pm$1.8}} &{0.35\tiny{$\pm$0.2}} &{4.53\tiny{$\pm$2.8}}  &{1.66\tiny{$\pm$}1.2}  \\
&Ours
&\best{0.48\tiny{$\pm$0.2}} &\best{0.44\tiny{$\pm$0.2}} &\best{0.39\tiny{$\pm$0.3}} &\best{0.55\tiny{$\pm$0.4}} &\best{0.16\tiny{$\pm$0.1}} &\best{0.93\tiny{$\pm$0.4}} &\best{0.08\tiny{$\pm$0.1}} &\best{0.33\tiny{$\pm$0.3}} &\best{0.42\tiny{$\pm$0.3}}  \\
\hline
\end{tabular}
}
}
\end{table*}

\begin{table*}[t]
\caption{{\textbf{Quantitative evaluation for perception-based metrics on PARIS data.} We report the results on average of two states. We highlight \colorbox[HTML]{ffc5c5}{best} results.}}
\label{tab:app:exp_psnr}
\renewcommand{\arraystretch}{1.2}
\resizebox{\linewidth}{!}{
{
\begin{tabular}{cc|ccccccccccc|ccc}
\hline
\multirow{2}{*}{Metric} &\multirow{2}{*}{Method} &\multicolumn{11}{c}{Synthetic Objects} &\multicolumn{3}{|c}{Real Objects} \\
% \cmidrule(lr){}
& &FoldChair &Fridge &Laptop &Oven &Scissor &Stapler &USB 
&Washer&Blade &Storage &All & Fridge &Storage &All \\
\hline
\multirow{2}{*}{PSNR} 
&PARIS & 31.50 & \best{37.67} & \best{37.26} & 35.30 & \best{38.37} & 38.49 & 39.07 & \best{40.08} & 38.29 & 36.18 & 37.22 & 25.29 & \best{27.13} & 26.21 \\
&Ours & \best{34.46} & 37.11 & 34.09 & \best{37.06} & 38.29 & \best{39.13} & \best{39.64} & 38.50 & \best{41.16} & \best{37.24} & \best{37.67} & \best{27.05} & 25.38 & \best{26.22} \\
\hline
\multirow{2}{*}{SSIM} 
&PARIS & 0.985 & \best{0.994} & \best{0.991} & 0.980 & 0.996 & 0.995 & 0.992 & 0.991 & 0.996 & \best{0.993} & 0.991 & 0.898 & \best{0.953} & 0.926 \\
&Ours & \best{0.997} & 0.993 & 0.988 & \best{0.995} & \best{0.998} & \best{0.999} & \best{0.998} & \best{0.995} & \best{0.999} & 0.992 & \best{0.995} & \best{0.939} & 0.930 & \best{0.935} \\
\hline
\multirow{2}{*}{$\text{LPIPS}_{vgg}$} 
&PARIS & 0.045 & \best{0.032} & \best{0.020} & \best{0.045} & 0.015 & 0.019 & 0.029 & \best{0.029} & 0.017 & \best{0.095} & \best{0.035} & 0.188 & \best{0.139} & 0.164 \\
&Ours & \best{0.036} & 0.041 & 0.045 & 0.054 & \best{0.014} & \best{0.011} & \best{0.016} & 0.052 & \best{0.004} & 0.097 & 0.037 & \best{0.114} & 0.188 & \best{0.151} \\
\hline
\end{tabular}
}
}
\end{table*}

\begin{table*}[t]
\caption{{\textbf{Quantitative comparison for whole mesh reconstruction on PARIS data.} We report the average of CD-w over 10 trials. We bold \textbf{best} results on average of two states.}} 
\label{tab:app:exp_tsdf}
\renewcommand{\arraystretch}{1.2}
\resizebox{\linewidth}{!}{
{
\begin{tabular}{cc|ccccccccccc|ccc}
\hline
\multirow{2}{*}{Metric} &\multirow{2}{*}{Method} &\multicolumn{11}{c}{Synthetic Objects} &\multicolumn{3}{|c}{Real Objects} \\
% \cmidrule(lr){}
& &FoldChair &Fridge &Laptop &Oven &Scissor &Stapler &USB 
&Washer&Blade &Storage &All & Fridge &Storage &All \\
\hline
\multirow{3}{*}{\shortstack{CD-w}}
&DTA
&\textbf{0.26} &0.70 &\textbf{0.34} &4.25 &\textbf{0.41} &\textbf{2.02} &\textbf{1.34} &\textbf{4.53} &\textbf{0.37} &\textbf{4.04} &\textbf{1.83} &2.13 &9.01 &5.57 \\
&TSDF with gt depth
& 0.30 & \textbf{0.56} & 0.47 & \textbf{3.60} & 0.49 & 2.78 & 1.60 & 5.73 & 0.54 & 5.13 & 2.12 & 3.15 & 131.86 & 67.51\\
&Ours
&0.36 &{0.59} &0.48 &{3.64} &0.68 &2.88 & 1.58 &6.05 & 0.63 & 5.17 &{2.21} &\textbf{1.37} &\textbf{2.84} &\textbf{2.11} 
\\
\hline
\end{tabular}
}
}
\end{table*}

{We provide additional comparisons with previous methods in this section. 
\paragraph{Scaled Axis Pos Metric.} Following DTA and PARIS, we multiply the 'Axis Pos' metric by 10 in \cref{tab:exp_2part} and \cref{tab:app:exp_2part}. While this metric shows minimal variation among current methods for synthetic objects, we also report the Axis Pos metric multiplied by 1000. As shown in \cref{tab:app:exp_ap}, \model demonstrates superior performance compared to DTA.
\paragraph{Perception-based Metrics.} To evaluate rendering quality, we assess perception-based metrics including LPIPS~\cite{zhang2018unreasonable}, SSIM~\cite{wang2004image}, and PSNR, with results shown in \cref{tab:app:exp_psnr}. While our primary focus aligns with previous methods on mesh reconstruction and articulation estimation, \model achieves comparable or superior performance relative to PARIS. 
\paragraph{Limited Improvement for CD-w on Simple Synthetic Objects.} Our method's performance on simple synthetic objects, particularly in terms of CD-w metric, is constrained by our use of TSDF for mesh extraction from Gaussian Splatting-rendered depths. To analyze this limitation, we compare against meshes reconstructed using ground-truth depth with TSDF. As shown in \cref{tab:app:exp_tsdf}, even with ground-truth depth input, TSDF-based reconstruction cannot surpass algorithms using marching cubes with NeRF, primarily due to the fundamental differences between TSDF and marching cubes algorithms on simple geometries. However, for complex or real-world objects where articulation reconstruction becomes more critical, the advantages of our model become evident. Additionally, TSDF with ground truth depth on real-world objects may produce poor-quality meshes (e.g., real\_storage) due to depth sensor noise, while our \model achieves high-quality reconstruction. Importantly, our primary objective is to create digital twins of real-world articulated objects, where \model demonstrates significant performance improvements, particularly for complex and real-world scenarios. 
}

{\subsection{Failure Cases}}
{\paragraph{Incorrect Initialization of Part Centers.} For real-world objects with multiple parts, clustering-derived part centers may be inaccurate (\cref{fig:failure} (a)) due to sensor noise, occlusion, and varying illumination conditions. These incorrectly initialized centers often persist through optimization, degrading performance for parts with misaligned centers (\cref{fig:failure} (c)). Manual correction of erroneous part centers prior to training (\cref{fig:failure} (b)) yields improved results (\cref{fig:failure} (d)). As discussed in \cref{app:limitation}, incorporating prior models like SAM for automatic, accurate part center initialization remains a promising direction for future work.}

{\paragraph{Similar Motions.} Our method exhibits limitations when handling parts with identical motion across states, as demonstrated in case 2 of \cref{fig:failure} where two drawers are pulled with the same distance. In such scenarios, the model tends to learn a single joint to fit both parts, failing to distinguish between the independently movable parts. As discussed in \cref{app:limitation}, expanding \model to incorporate additional states would provide richer motion information, potentially enabling better part separation.}

\begin{figure}[t!]
 \centering
 \resizebox{\linewidth}{!}{\includegraphics[width=\linewidth]{figure/failure.pdf}}
  \caption{{\textbf{Failure cases}. We illustrate failure cases of our \model. 'Init./Opt. Cano.' represents initialized and optimized Canonical Gaussians, while the prefix 'M' indicates manual correction of erroneous part centers.}}
 \label{fig:failure}
 \vspace{-10pt}
\end{figure}

\begin{figure}[t!]
 \centering
 \resizebox{\linewidth}{!}{\includegraphics[width=\linewidth]{figure/refinement.pdf}}
\caption{{\textbf{Evolution of canonical Gaussians}. We visualize the evolution of canonical Gaussians, showing both their part assignments and centers. Our initialization strategy begins with dense static Gaussians and sparse dynamic Gaussians. As training progresses, the Gaussians undergo densification while simultaneously refining their part centers and assignments. These visualization results demonstrate the effectiveness of \model.}}
 \label{fig:evolution}
 \vspace{-10pt}
\end{figure}


{\subsection{Evolution of Canonical Gaussians}}
{We visualize the evolution of canonical Gaussians in \cref{fig:evolution}, showing both their part assignments and centers. Our initialization strategy begins with dense static Gaussians and sparse dynamic Gaussians. As training progresses, the Gaussians undergo densification while simultaneously refining their part centers and assignments. These visualization results demonstrate the effectiveness of \model.}
{\subsection{Additional Qualitative Comparisons}}
{We provide additional qualitative comparisons on different datasets in the following pages.}
\begin{figure}[t!]
 \centering
 \resizebox{\linewidth}{!}{\includegraphics[width=\linewidth]{figure/vis_exp_multi_1.pdf}}
 \caption{\textbf{Additional qualitative results on \model-Multi.}}
 \label{fig:app:mpart}
 \vspace{-10pt}
\end{figure}
\begin{figure}[t!]
 \centering
 \resizebox{\linewidth}{!}{\includegraphics[width=\linewidth]{figure/interp_supp.pdf}}
 \caption{{\textbf{Interpolation results on PARIS data.}}}
 \label{fig:interp_supp}
 \vspace{-10pt}
\end{figure}

\end{document}



