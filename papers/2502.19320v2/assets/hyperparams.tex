
\begin{tabular}{lcccccc}
\toprule
& \multicolumn{3}{c}{\textbf{Finetuned model (F)}} & \multicolumn{3}{c}{\textbf{Oracle model (G)}} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
& \textbf{20NG-CS} & \textbf{20NG-Raw} & \textbf{TinyShakespeare }& \textbf{20NG-CS }& \textbf{20NG-Raw} & \textbf{TinyShakespeare} \\
\midrule
\multicolumn{7}{l}{\textbf{Overall}} \\
Architecture & \multicolumn{3}{c}{Gemma 2 2B} & \multicolumn{2}{c}{GPT-small} & GPT-micro \\
Context length &  \multicolumn{6}{c}{256} \\
Vocabulary size & \multicolumn{6}{c}{256,128} \\
\# parameters (incl adaptors) & \multicolumn{3}{c}{2.6B} & \multicolumn{2}{c}{109.3M} & 33.7M \\
\midrule
\multicolumn{7}{l}{\textbf{Optimisation}} \\
Num steps (optimal) & 128 & 1536 & 128 & 320 & 512 & 2400 \\
Learning rate & \multicolumn{3}{c}{5e-05} & \multicolumn{3}{c}{3e-04} \\
Warmup steps & \multicolumn{3}{c}{64} & \multicolumn{2}{c}{100} & 300 \\
Dropout - embedding  &  \multicolumn{6}{c}{0.1} \\
Dropout - attention  &  \multicolumn{6}{c}{0.1} \\
Dropout - residual  &  \multicolumn{6}{c}{0.1} \\
Weight decay  &  \multicolumn{6}{c}{0.01} \\
\midrule
\multicolumn{7}{l}{\textbf{LoRA}} \\
Rank & \multicolumn{3}{c}{8} & \multicolumn{3}{c}{NA} \\
Alpha & \multicolumn{3}{c}{32} & \multicolumn{3}{c}{NA} \\
Dropout rate & \multicolumn{3}{c}{0.1} & \multicolumn{3}{c}{NA} \\
\# Trainable parameters & \multicolumn{3}{c}{10.4M} & \multicolumn{3}{c}{NA} \\
\bottomrule
\end{tabular}
