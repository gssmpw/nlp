\section{Related Work}
\paragraph{LLM Guardrails.}
A large body of work has been published on establishing effective guardrails for LLMs. These approaches are designed to restrict the model to responses that align with the deployer's values. One of the first approaches was Reinforcement Learning with Human Feedback (RLHF) \citep{askell_general_2021}, which uses human preferences to guide LLM training.
Extensions such as Safe-RLHF add cost models to penalize harmful behavior, ensuring a balance between helpfulness and harmlessness during optimization \citep{dai_safe_2024}.
RLHF's foundation in reinforcement learning has given rise to techniques such as Proximal Policy Optimization (PPO) \citep{bai_training_2022}, the more recent Direct Preference Optimization (DPO) \citep{rafailov_direct_2024},
and Generalized Policy Optimization (GPO) \citep{tang_generalized_2024}, which incorporates diverse optimization objectives, useful for safety-critical scenarios.
For an in-depth survey of this area, we direct the reader to \citet{kaufmann_survey_2024}.
Unlike the preceding approaches that fine-tune guardrails into the parameters of an LLM, a number of works have proposed using LLMs to classify content as either safe or unsafe.
Llama Guard categorizes the inputs and outputs of an LLM into different unsafe content categories \citep{inan_llama_2023}. Conversely, \citet{chua_flexible_2024} classify if an output is safe with respect to a system prompt.
For a complete overview on LLM guardrails, we direct the interested reader to a recent survey of this area, \citet{dong_safeguarding_2024}.
Existing LLM guardrail techniques have been proven effective to different levels. However, these guardrails only come with empirical evidence of their proficiency against existing attacks, and hence, many have been circumvented shortly after deployment. Conversely, VALID offers a provable high-probability guarantee against undesirable behavior, reflecting recent advocacy for such provable assurances \citep{bengio_bounding_2024}.

\paragraph{Out-of-Distribution Detection. }
Out-of-distribution (OOD) detection has received a lot of attention in recent years in NLP. Commonly, the problem is treated as text classification and softmax probabilities of class predictions \citep{hendrycks_baseline_2017} or energy scores \citep{liu_energy-based_2020} are deployed as discriminant scores. Another group of methods employs distance-based methods, relying on OOD responses being distant from ID responses in latent space, often utilizing Mahalanobis distance and sometimes incorporating contrastive learning techniques \citep{uppaal_is_2023, podolskiy_revisiting_2021, zhou_contrastive_2021, khosla_supervised_2020, lin_flats_2023}. Finally, rooted in classical statistics, a number of studies suggest using the log-likelihood ratio (LLR) as a discriminate score, comparing likelihoods from ID and OOD proxy models \citep{gangal_likelihood_2020, zhang_your_2024}. \cite{xu_large_2024} offer a comprehensive review of LLMs for OOD detection. While many of these works have strong empirical detection results, their focus is OOD detection rather than certification, and hence they do not provide theoretical guarantees or certificates on model behavior.


\paragraph{Certifying LLMS.}
A number of certification approaches have been proposed for LLMs in various contexts. For instance, \citet{chaudhary_quantitative_2024-1} aim to certify the knowledge comprehension ability of LLMs and \citet{freiberger_fairness_2024} discuss what criteria should be certified to ensure fairness. Most relevant here is work on certification against adversarial inputs. \citet{casadio_nlp_2024} discuss certifying the robustness of LLMs to input perturbations in embedding space. Commonly, adversarial certification is studied for text classification rather than generation \citep{la_malfa_robustness_2023}. \citet{kumar_certifying_2024} introduce a framework for defending against adversarial perturbations in token space by performing a small number of substitutions around a given input. In contrast VALID comes with certificates that holds for \emph{all inputs}, rather than perturbations around a specific input.

\vspace{-2pt}