@misc{zhang_your_2024,
	title = {Your {Finetuned} {Large} {Language} {Model} is {Already} a {Powerful} {Out}-of-distribution {Detector}},

	author = {Zhang, Andi and Xiao, Tim Z. and Liu, Weiyang and Bamler, Robert and Wischik, Damon},
	year = {2024},
	note = {ArXiv: 2404.08679},
}

@misc{xu_large_2024,
	title = {Large {Language} {Models} for {Anomaly} and {Out}-of-{Distribution} {Detection}: {A} {Survey}},

	author = {Xu, Ruiyao and Ding, Kaize},
	year = {2024},
	note = {ArXiv: 2409.01980},
}

@article{xu_machine_2023,
	title = {Machine {Unlearning}: {A} {Survey}},
	volume = {56},
	issn = {0360-0300},

	abstract = {Machine learning has attracted widespread attention and evolved into an enabling technology for a wide range of highly successful applications, such as intelligent computer vision, speech recognition, medical diagnosis, and more. Yet, a special need has arisen where, due to privacy, usability, and/or the right to be forgotten, information about some specific samples needs to be removed from a model, called machine unlearning. This emerging technology has drawn significant interest from both academics and industry due to its innovation and practicality. At the same time, this ambitious problem has led to numerous research efforts aimed at confronting its challenges. To the best of our knowledge, no study has analyzed this complex topic or compared the feasibility of existing unlearning solutions in different kinds of scenarios. Accordingly, with this survey, we aim to capture the key concepts of unlearning techniques. The existing solutions are classified and summarized based on their characteristics within an up-to-date and comprehensive review of each category’s advantages and limitations. The survey concludes by highlighting some of the outstanding issues with unlearning techniques, along with some feasible directions for new research opportunities.},
	number = {1},
	journal = {ACM Comput. Surv.},
	author = {Xu, Heng and Zhu, Tianqing and Zhang, Lefeng and Zhou, Wanlei and Yu, Philip S.},
	month = aug,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Machine learning, data privacy, deep learning, machine unlearning, model usability, sample removal},
}

@misc{xiong_temporal_2024,
	title = {Temporal {Scaling} {Law} for {Large} {Language} {Models}},

	author = {Xiong, Yizhe and Chen, Xiansheng and Ye, Xin and Chen, Hui and Lin, Zijia and Lian, Haoran and Su, Zhenpeng and Niu, Jianwei and Ding, Guiguang},
	year = {2024},
	note = {ArXiv: 2404.17785},
}

@inproceedings{wallace_concealed_2021,
	address = {Online},
	title = {Concealed {Data} {Poisoning} {Attacks} on {NLP} {Models}},

	abstract = {Adversarial attacks alter NLP model predictions by perturbing test-time inputs. However, it is much less understood whether, and how, predictions can be manipulated with small, concealed changes to the training data. In this work, we develop a new data poisoning attack that allows an adversary to control model predictions whenever a desired trigger phrase is present in the input. For instance, we insert 50 poison examples into a sentiment model`s training set that causes the model to frequently predict Positive whenever the input contains “James Bond”. Crucially, we craft these poison examples using a gradient-based procedure so that they do not mention the trigger phrase. We also apply our poison attack to language modeling (“Apple iPhone” triggers negative generations) and machine translation (“iced coffee” mistranslated as “hot coffee”). We conclude by proposing three defenses that can mitigate our attack at some cost in prediction accuracy or extra human annotation.},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Wallace, Eric and Zhao, Tony and Feng, Shi and Singh, Sameer},
	editor = {Toutanova, Kristina and Rumshisky, Anna and Zettlemoyer, Luke and Hakkani-Tur, Dilek and Beltagy, Iz and Bethard, Steven and Cotterell, Ryan and Chakraborty, Tanmoy and Zhou, Yichao},
	month = jun,
	year = {2021},
	note = {ArXiv 2010.12563},
	pages = {139--150},
}

@inproceedings{wallace_universal_2019,
	address = {Hong Kong, China},
	title = {Universal {Adversarial} {Triggers} for {Attacking} and {Analyzing} {NLP}},

	abstract = {Adversarial examples highlight model vulnerabilities and are useful for evaluation and interpretation. We define universal adversarial triggers: input-agnostic sequences of tokens that trigger a model to produce a specific prediction when concatenated to any input from a dataset. We propose a gradient-guided search over tokens which finds short trigger sequences (e.g., one word for classification and four words for language modeling) that successfully trigger the target prediction. For example, triggers cause SNLI entailment accuracy to drop from 89.94\% to 0.55\%, 72\% of “why” questions in SQuAD to be answered “to kill american people”, and the GPT-2 language model to spew racist output even when conditioned on non-racial contexts. Furthermore, although the triggers are optimized using white-box access to a specific model, they transfer to other models for all tasks we consider. Finally, since triggers are input-agnostic, they provide an analysis of global model behavior. For instance, they confirm that SNLI models exploit dataset biases and help to diagnose heuristics learned by reading comprehension models.},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Wallace, Eric and Feng, Shi and Kandpal, Nikhil and Gardner, Matt and Singh, Sameer},
	editor = {Inui, Kentaro and Jiang, Jing and Ng, Vincent and Wan, Xiaojun},
	month = nov,
	year = {2019},
	note = {ArXiv 1908.07125},
	pages = {2153--2162},
}

@misc{team_gemma_2024,
	title = {Gemma 2: {Improving} {Open} {Language} {Models} at a {Practical} {Size}},

	author = {Team, Gemma and Riviere, Morgane and Pathak, Shreya and Sessa, Pier Giuseppe and Hardin, Cassidy and Bhupatiraju, Surya and Hussenot, Léonard and Mesnard, Thomas and Shahriari, Bobak and Ramé, Alexandre and Ferret, Johan and Liu, Peter and Tafti, Pouya and Friesen, Abe and Casbon, Michelle and Ramos, Sabela and Kumar, Ravin and Lan, Charline Le and Jerome, Sammy and Tsitsulin, Anton and Vieillard, Nino and Stanczyk, Piotr and Girgin, Sertan and Momchev, Nikola and Hoffman, Matt and Thakoor, Shantanu and Grill, Jean-Bastien and Neyshabur, Behnam and Bachem, Olivier and Walton, Alanna and Severyn, Aliaksei and Parrish, Alicia and Ahmad, Aliya and Hutchison, Allen and Abdagic, Alvin and Carl, Amanda and Shen, Amy and Brock, Andy and Coenen, Andy and Laforge, Anthony and Paterson, Antonia and Bastian, Ben and Piot, Bilal and Wu, Bo and Royal, Brandon and Chen, Charlie and Kumar, Chintu and Perry, Chris and Welty, Chris and Choquette-Choo, Christopher A. and Sinopalnikov, Danila and Weinberger, David and Vijaykumar, Dimple and Rogozińska, Dominika and Herbison, Dustin and Bandy, Elisa and Wang, Emma and Noland, Eric and Moreira, Erica and Senter, Evan and Eltyshev, Evgenii and Visin, Francesco and Rasskin, Gabriel and Wei, Gary and Cameron, Glenn and Martins, Gus and Hashemi, Hadi and Klimczak-Plucińska, Hanna and Batra, Harleen and Dhand, Harsh and Nardini, Ivan and Mein, Jacinda and Zhou, Jack and Svensson, James and Stanway, Jeff and Chan, Jetha and Zhou, Jin Peng and Carrasqueira, Joana and Iljazi, Joana and Becker, Jocelyn and Fernandez, Joe and Amersfoort, Joost van and Gordon, Josh and Lipschultz, Josh and Newlan, Josh and Ji, Ju-yeong and Mohamed, Kareem and Badola, Kartikeya and Black, Kat and Millican, Katie and McDonell, Keelin and Nguyen, Kelvin and Sodhia, Kiranbir and Greene, Kish and Sjoesund, Lars Lowe and Usui, Lauren and Sifre, Laurent and Heuermann, Lena and Lago, Leticia and McNealus, Lilly and Soares, Livio Baldini and Kilpatrick, Logan and Dixon, Lucas and Martins, Luciano and Reid, Machel and Singh, Manvinder and Iverson, Mark and Görner, Martin and Velloso, Mat and Wirth, Mateo and Davidow, Matt and Miller, Matt and Rahtz, Matthew and Watson, Matthew and Risdal, Meg and Kazemi, Mehran and Moynihan, Michael and Zhang, Ming and Kahng, Minsuk and Park, Minwoo and Rahman, Mofi and Khatwani, Mohit and Dao, Natalie and Bardoliwalla, Nenshad and Devanathan, Nesh and Dumai, Neta and Chauhan, Nilay and Wahltinez, Oscar and Botarda, Pankil and Barnes, Parker and Barham, Paul and Michel, Paul and Jin, Pengchong and Georgiev, Petko and Culliton, Phil and Kuppala, Pradeep and Comanescu, Ramona and Merhej, Ramona and Jana, Reena and Rokni, Reza Ardeshir and Agarwal, Rishabh and Mullins, Ryan and Saadat, Samaneh and Carthy, Sara Mc and Cogan, Sarah and Perrin, Sarah and Arnold, Sébastien M. R. and Krause, Sebastian and Dai, Shengyang and Garg, Shruti and Sheth, Shruti and Ronstrom, Sue and Chan, Susan and Jordan, Timothy and Yu, Ting and Eccles, Tom and Hennigan, Tom and Kocisky, Tomas and Doshi, Tulsee and Jain, Vihan and Yadav, Vikas and Meshram, Vilobh and Dharmadhikari, Vishal and Barkley, Warren and Wei, Wei and Ye, Wenming and Han, Woohyun and Kwon, Woosuk and Xu, Xiang and Shen, Zhe and Gong, Zhitao and Wei, Zichuan and Cotruta, Victor and Kirk, Phoebe and Rao, Anand and Giang, Minh and Peran, Ludovic and Warkentin, Tris and Collins, Eli and Barral, Joelle and Ghahramani, Zoubin and Hadsell, Raia and Sculley, D. and Banks, Jeanine and Dragan, Anca and Petrov, Slav and Vinyals, Oriol and Dean, Jeff and Hassabis, Demis and Kavukcuoglu, Koray and Farabet, Clement and Buchatskaya, Elena and Borgeaud, Sebastian and Fiedel, Noah and Joulin, Armand and Kenealy, Kathleen and Dadashi, Robert and Andreev, Alek},
	year = {2024},
	note = {ArXiv: 2408.00118},
}

@inproceedings{sennrich_neural_2016,
	address = {Berlin, Germany},
	title = {Neural {Machine} {Translation} of {Rare} {Words} with {Subword} {Units}},

	booktitle = {Proceedings of the 54th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
	editor = {Erk, Katrin and Smith, Noah A.},
	month = aug,
	year = {2016},
	note = {ArXiv 1508.07909},
	pages = {1715--1725},
}

@inproceedings{samvelyan_rainbow_2024,
	title = {Rainbow {Teaming}: {Open}-{Ended} {Generation} of {Diverse} {Adversarial} {Prompts}},

	booktitle = {The {Thirty}-eighth {Annual} {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Samvelyan, Mikayel and Raparthy, Sharath Chandra and Lupu, Andrei and Hambro, Eric and Markosyan, Aram H. and Bhatt, Manish and Mao, Yuning and Jiang, Minqi and Parker-Holder, Jack and Foerster, Jakob Nicolaus and Rocktäschel, Tim and Raileanu, Roberta},
	year = {2024},
	note = {ArXiv 2402.16822},
}

@inproceedings{perez_ignore_2022,
	title = {Ignore {Previous} {Prompt}: {Attack} {Techniques} {For} {Language} {Models}},

	booktitle = {{NeurIPS} {ML} {Safety} {Workshop}},
	author = {Perez, Fábio and Ribeiro, Ian},
	year = {2022},
	note = {ArXiv: 2211.09527},
}

@misc{oneill_adversarial_2023,
	title = {Adversarial {Fine}-{Tuning} of {Language} {Models}: {An} {Iterative} {Optimisation} {Approach} for the {Generation} and {Detection} of {Problematic} {Content}},

	author = {O'Neill, Charles and Miller, Jack and Ciuca, Ioana and Ting, Yuan-Sen and Bui, Thang},
	year = {2023},
	note = {ArXiv: 2308.13768},
}

@misc{nguyen_survey_2022,
	title = {A {Survey} of {Machine} {Unlearning}},

	author = {Nguyen, Thanh Tam and Huynh, Thanh Trung and Nguyen, Phi Le and Liew, Alan Wee-Chung and Yin, Hongzhi and Nguyen, Quoc Viet Hung},
	year = {2022},
	note = {ArXiv: 2209.02299},
}

@misc{minaee_large_2024,
	title = {Large {Language} {Models}: {A} {Survey}},
	shorttitle = {Large {Language} {Models}},

	abstract = {Large Language Models (LLMs) have drawn a lot of attention due to their strong performance on a wide range of natural language tasks, since the release of ChatGPT in November 2022. LLMs’ ability of general-purpose language understanding and generation is acquired by training billions of model’s parameters on massive amounts of text data, as predicted by scaling laws [1], [2]. The research area of LLMs, while very recent, is evolving rapidly in many different ways. In this paper, we review some of the most prominent LLMs, including three popular LLM families (GPT, LLaMA, PaLM), and discuss their characteristics, contributions and limitations. We also give an overview of techniques developed to build, and augment LLMs. We then survey popular datasets prepared for LLM training, fine-tuning, and evaluation, review widely used LLM evaluation metrics, and compare the performance of several popular LLMs on a set of representative benchmarks. Finally, we conclude the paper by discussing open challenges and future research directions.},
	language = {en},
	urldate = {2024-08-16},
	publisher = {arXiv},
	author = {Minaee, Shervin and Mikolov, Tomas and Nikzad, Narjes and Chenaghlu, Meysam and Socher, Richard and Amatriain, Xavier and Gao, Jianfeng},
	month = feb,
	year = {2024},
	note = {ArXiv: 2402.06196},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@inproceedings{mehrotra_tree_2024,
	title = {Tree of {Attacks}: {Jailbreaking} {Black}-{Box} {LLMs} {Automatically}},
	volume = {37},

	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Mehrotra, Anay and Zampetakis, Manolis and Kassianik, Paul and Nelson, Blaine and Anderson, Hyrum and Singer, Yaron and Karbasi, Amin},
	editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
	year = {2024},
	pages = {61065--61105},
}

@inproceedings{liu_autodan_2024,
	title = {{AutoDAN}: {Generating} {Stealthy} {Jailbreak} {Prompts} on {Aligned} {Large} {Language} {Models}},

	booktitle = {The {Twelfth} {International} {Conference} on {Learning} {Representations} ({ICLR})},
	author = {Liu, Xiaogeng and Xu, Nan and Chen, Muhao and Xiao, Chaowei},
	year = {2024},
}

@misc{kumar_certifying_2024,
	title = {Certifying {LLM} {Safety} against {Adversarial} {Prompting}},

	author = {Kumar, Aounon and Agarwal, Chirag and Srinivas, Suraj and Li, Aaron Jiaxun and Feizi, Soheil and Lakkaraju, Himabindu},
	year = {2024},
	note = {ArXiv: 2309.02705},
}

@misc{kaufmann_survey_2024,
	title = {A {Survey} of {Reinforcement} {Learning} from {Human} {Feedback}},

	author = {Kaufmann, Timo and Weng, Paul and Bengs, Viktor and Hüllermeier, Eyke},
	year = {2024},
	note = {ArXiv: 2312.14925},
}

@misc{kaplan_scaling_2020,
	title = {Scaling {Laws} for {Neural} {Language} {Models}},

	author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
	year = {2020},
	note = {ArXiv: 2001.08361},
	keywords = {ONLY-ARXIV},
}

@inproceedings{jones_automatically_2023,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Automatically {Auditing} {Large} {Language} {Models} via {Discrete} {Optimization}},
	volume = {202},

	abstract = {Auditing large language models for unexpected behaviors is critical to preempt catastrophic deployments, yet remains challenging. In this work, we cast auditing as an optimization problem, where we automatically search for input-output pairs that match a desired target behavior. For example, we might aim to find a non-toxic input that starts with “Barack Obama” that a model maps to a toxic output. This optimization problem is difficult to solve as the set of feasible points is sparse, the space is discrete, and the language models we audit are non-linear and high-dimensional. To combat these challenges, we introduce a discrete optimization algorithm, ARCA, that jointly and efficiently optimizes over inputs and outputs. Our approach automatically uncovers derogatory completions about celebrities (e.g. "Barack Obama is a legalized unborn" –\&gt; "child murderer"), produces French inputs that complete to English outputs, and finds inputs that generate a specific name. Our work offers a promising new tool to uncover models’ failure-modes before deployment. Content Warning: This paper contains examples that may be offensive in nature.},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Jones, Erik and Dragan, Anca and Raghunathan, Aditi and Steinhardt, Jacob},
	editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
	month = jul,
	year = {2023},
	pages = {15307--15329},
}

@misc{jiang_prompt_2023,
	title = {Prompt {Packer}: {Deceiving} {LLMs} through {Compositional} {Instruction} with {Hidden} {Attacks}},

	author = {Jiang, Shuyu and Chen, Xingshu and Tang, Rui},
	year = {2023},
	note = {ArXiv: 2310.10077},
}

@inproceedings{jia_improved_2025,
	title = {Improved {Techniques} for {Optimization}-{Based} {Jailbreaking} on {Large} {Language} {Models}},

	booktitle = {The {Thirteenth} {International} {Conference} on {Learning} {Representations} ({ICLR})},
	author = {Jia, Xiaojun and Pang, Tianyu and Du, Chao and Huang, Yihao and Gu, Jindong and Liu, Yang and Cao, Xiaochun and Lin, Min},
	year = {2025},
	note = {ArXiv:2405.21018},
}

@misc{jia_adversarial_2017,
	title = {Adversarial {Examples} for {Evaluating} {Reading} {Comprehension} {Systems}},

	author = {Jia, Robin and Liang, Percy},
	year = {2017},
	note = {ArXiv: 1707.07328},
}

@misc{inan_llama_2023,
	title = {Llama {Guard}: {LLM}-based {Input}-{Output} {Safeguard} for {Human}-{AI} {Conversations}},

	author = {Inan, Hakan and Upasani, Kartikeya and Chi, Jianfeng and Rungta, Rashi and Iyer, Krithika and Mao, Yuning and Tontchev, Michael and Hu, Qing and Fuller, Brian and Testuggine, Davide and Khabsa, Madian},
	year = {2023},
	note = {ArXiv: 2312.06674},
}

@inproceedings{hu_lora_2022,
	title = {{LoRA}: {Low}-{Rank} {Adaptation} of {Large} {Language} {Models}},

	booktitle = {International {Conference} on {Learning} {Representations} ({ICLR})},
	author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
	year = {2022},
}

@inproceedings{hendrycks_measuring_2021,
	title = {Measuring {Massive} {Multitask} {Language} {Understanding}},

	booktitle = {International {Conference} on {Learning} {Representations} ({ICLR})},
	author = {Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
	year = {2021},
	note = {ArXiv: 2009.03300},
}

@misc{eiras_mimicking_2024,
	title = {Mimicking {User} {Data}: {On} {Mitigating} {Fine}-{Tuning} {Risks} in {Closed} {Large} {Language} {Models}},

	author = {Eiras, Francisco and Petrov, Aleksandar and Torr, Phillip H. S. and Kumar, M. Pawan and Bibi, Adel},
	year = {2024},
	note = {ArXiv: 2406.10288},
}

@misc{bai_training_2022,
	title = {Training a {Helpful} and {Harmless} {Assistant} with {Reinforcement} {Learning} from {Human} {Feedback}},

	author = {Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and Joseph, Nicholas and Kadavath, Saurav and Kernion, Jackson and Conerly, Tom and El-Showk, Sheer and Elhage, Nelson and Hatfield-Dodds, Zac and Hernandez, Danny and Hume, Tristan and Johnston, Scott and Kravec, Shauna and Lovitt, Liane and Nanda, Neel and Olsson, Catherine and Amodei, Dario and Brown, Tom and Clark, Jack and McCandlish, Sam and Olah, Chris and Mann, Ben and Kaplan, Jared},
	year = {2022},
	note = {ArXiv: 2204.05862},
}

@misc{askell_general_2021,
	title = {A {General} {Language} {Assistant} as a {Laboratory} for {Alignment}},

	author = {Askell, Amanda and Bai, Yuntao and Chen, Anna and Drain, Dawn and Ganguli, Deep and Henighan, Tom and Jones, Andy and Joseph, Nicholas and Mann, Ben and DasSarma, Nova and Elhage, Nelson and Hatfield-Dodds, Zac and Hernandez, Danny and Kernion, Jackson and Ndousse, Kamal and Olsson, Catherine and Amodei, Dario and Brown, Tom and Clark, Jack and McCandlish, Sam and Olah, Chris and Kaplan, Jared},
	year = {2021},
	note = {ArXiv: 2112.00861},
	keywords = {ONLY-ARXIV},
}

@inproceedings{emde_shh_2025,
	address = {Singapore},
	title = {Shh, don't say that! {Domain} {Certification} in {LLMs}},

	booktitle = {The {Thirteenth} {International} {Conference} on {Learning} {Representations} ({ICLR})},
	author = {Emde, Cornelius and Paren, Alasdair and Arvind, Preetham and Kayser, Maxime Guillaume and Rainforth, Tom and Lukasiewicz, Thomas and Ghanem, Bernard and Torr, Philip and Bibi, Adel},
	year = {2025},
}

@article{wu_game-based_2020,
	title = {A game-based approximate verification of deep neural networks with provable guarantees},
	volume = {807},
	issn = {0304-3975},

	abstract = {Despite the improved accuracy of deep neural networks, the discovery of adversarial examples has raised serious safety concerns. In this paper, we study two variants of pointwise robustness, the maximum safe radius problem, which for a given input sample computes the minimum distance to an adversarial example, and the feature robustness problem, which aims to quantify the robustness of individual features to adversarial perturbations. We demonstrate that, under the assumption of Lipschitz continuity, both problems can be approximated using finite optimisation by discretising the input space, and the approximation has provable guarantees, i.e., the error is bounded. We then show that the resulting optimisation problems can be reduced to the solution of two-player turn-based games, where the first player selects features and the second perturbs the image within the feature. While the second player aims to minimise the distance to an adversarial example, depending on the optimisation objective the first player can be cooperative or competitive. We employ an anytime approach to solve the games, in the sense of approximating the value of a game by monotonically improving its upper and lower bounds. The Monte Carlo tree search algorithm is applied to compute upper bounds for both games, and the Admissible A⁎ and the Alpha-Beta Pruning algorithms are, respectively, used to compute lower bounds for the maximum safety radius and feature robustness games. When working on the upper bound of the maximum safe radius problem, our tool demonstrates competitive performance against existing adversarial example crafting algorithms. Furthermore, we show how our framework can be deployed to evaluate pointwise robustness of neural networks in safety-critical applications such as traffic sign recognition in self-driving cars.},
	journal = {Theoretical Computer Science},
	author = {Wu, Min and Wicker, Matthew and Ruan, Wenjie and Huang, Xiaowei and Kwiatkowska, Marta},
	year = {2020},
	keywords = {Adversarial examples, Automated verification, Deep neural networks, Two-player game},
	pages = {298--329},
}

@incollection{prieditis_newsweeder_1995,
	address = {San Francisco (CA)},
	title = {{NewsWeeder}: {Learning} to {Filter} {Netnews}},
	isbn = {978-1-55860-377-6},

	booktitle = {Machine {Learning} {Proceedings} 1995},
	publisher = {Morgan Kaufmann},
	author = {Lang, Ken},
	editor = {Prieditis, Armand and Russell, Stuart},
	year = {1995},

	pages = {331--339},
}

@book{manning_introduction_2008,
	address = {Cambridge},
	title = {Introduction to {Information} {Retrieval}},
	isbn = {978-0-521-86571-5},

	abstract = {Class-tested and coherent, this textbook teaches classical and web information retrieval, including web search and the related areas of text classification and text clustering from basic concepts. It gives an up-to-date treatment of all aspects of the design and implementation of systems for gathering, indexing, and searching documents; methods for evaluating systems; and an introduction to the use of machine learning methods on text collections. All the important ideas are explained using examples and figures, making it perfect for introductory courses in information retrieval for advanced undergraduates and graduate students in computer science. Based on feedback from extensive classroom experience, the book has been carefully structured in order to make teaching more natural and effective. Slides and additional exercises (with solutions for lecturers) are also available through the book's supporting website to help course instructors prepare their lectures.},
	publisher = {Cambridge University Press},
	author = {Manning, Christopher D. and Raghavan, Prabhakar and Schütze, Hinrich},
	year = {2008},

}

@inproceedings{emde_shh_2024,
	title = {Shh, don't say that! {Domain} {Certification} in {LLMs}},

	booktitle = {Workshop on {Socially} {Responsible} {Language} {Modelling} {Research}},
	author = {Emde, Cornelius and Arvind, Preetham and Paren, Alasdair and Kayser, Maxime and Rainforth, Tom and Lukasiewicz, Thomas and Torr, Philip and Bibi, Adel},
	year = {2024},
}

@misc{zhang_bi-factorial_2024,
	title = {Bi-{Factorial} {Preference} {Optimization}: {Balancing} {Safety}-{Helpfulness} in {Language} {Models}},

	author = {Zhang, Wenxuan and Torr, Philip H. S. and Elhoseiny, Mohamed and Bibi, Adel},
	year = {2024},
	eprint={2408.15313},
	archivePrefix={arXiv},
}

@inproceedings{hsu_safe_2024,
	title = {Safe {LoRA}: {The} {Silver} {Lining} of {Reducing} {Safety} {Risks} when {Finetuning} {Large} {Language} {Models}},

	booktitle = {The {Thirty}-eighth {Annual} {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Hsu, Chia-Yi and Tsai, Yu-Lin and Lin, Chih-Hsun and Chen, Pin-Yu and Yu, Chia-Mu and Huang, Chun-Ying},
	year = {2024},
}

@inproceedings{tang_generalized_2024,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Generalized {Preference} {Optimization}: {A} {Unified} {Approach} to {Offline} {Alignment}},
	volume = {235},

	abstract = {Offline preference optimization allows fine-tuning large models directly from offline data, and has proved effective in recent alignment practices. We propose generalized preference optimization (GPO), a family of offline losses parameterized by a general class of convex functions. GPO enables a unified view over preference optimization, encompassing existing algorithms such as DPO, IPO and SLiC as special cases, while naturally introducing new variants. The GPO framework also sheds light on how offline algorithms enforce regularization, through the design of the convex function that defines the loss. Our analysis and experiments reveal the connections and subtle differences between the offline regularization and the KL divergence regularization intended by the canonical RLHF formulation. In a controlled setting akin to Gao et al 2023, we also show that different GPO variants achieve similar trade-offs between regularization and performance, though the optimal values of hyper-parameter might differ as predicted by theory. In all, our results present new algorithmic toolkits and empirical insights to alignment practitioners.},
	booktitle = {Proceedings of the 41st {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Tang, Yunhao and Guo, Zhaohan Daniel and Zheng, Zeyu and Calandriello, Daniele and Munos, Remi and Rowland, Mark and Richemond, Pierre Harvey and Valko, Michal and Avila Pires, Bernardo and Piot, Bilal},
	editor = {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
	month = jul,
	year = {2024},
	pages = {47725--47742},
}

@inproceedings{li_contrastive_2023,
	address = {Toronto, Canada},
	title = {Contrastive {Decoding}: {Open}-ended {Text} {Generation} as {Optimization}},

	abstract = {Given a language model (LM), maximum probability is a poor decoding objective for open-ended generation, because it produces short and repetitive text. On the other hand, sampling can often produce incoherent text that drifts from the original topics. We propose contrastive decoding (CD), a reliable decoding approach that optimizes a contrastive objective subject to a plausibility constraint. The contrastive objective returns the difference between the likelihood under a large LM (called the expert, e.g. OPT-13B) and a small LM (called the amateur, e.g. OPT-125M), and the constraint ensures that the outputs are plausible. CD is inspired by the fact that the failures of larger LMs (e.g., repetition, inco- herence) are even more prevalent in smaller LMs, and that this difference signals which texts should be preferred. CD requires zero additional training, and produces higher quality text than decoding from the larger LM alone. It also works across model scales (OPT-13B and GPT2-1.5B) and significantly outperforms four strong decoding algorithms (e.g., nucleus, top-k) in automatic and human evaluations across wikipedia, news and story domains.},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Li, Xiang Lisa and Holtzman, Ari and Fried, Daniel and Liang, Percy and Eisner, Jason and Hashimoto, Tatsunori and Zettlemoyer, Luke and Lewis, Mike},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	pages = {12286--12312},
}

@inproceedings{mosbach_few-shot_2023,
	address = {Toronto, Canada},
	title = {Few-shot {Fine}-tuning vs. {In}-context {Learning}: {A} {Fair} {Comparison} and {Evaluation}},

	abstract = {Few-shot fine-tuning and in-context learning are two alternative strategies for task adaptation of pre-trained language models. Recently, in-context learning has gained popularity over fine-tuning due to its simplicity and improved out-of-domain generalization, and because extensive evidence shows that fine-tuned models pick up on spurious correlations. Unfortunately, previous comparisons of the two approaches were done using models of different sizes. This raises the question of whether the observed weaker out-of-domain generalization of fine-tuned models is an inherent property of fine-tuning or a limitation of the experimental setup. In this paper, we compare the generalization of few-shot fine-tuning and in-context learning to challenge datasets, while controlling for the models used, the number of examples, and the number of parameters, ranging from 125M to 30B. Our results show that fine-tuned language models can in fact generalize well out-of-domain. We find that both approaches generalize similarly; they exhibit large variation and depend on properties such as model size and the number of examples, highlighting that robust task adaptation remains a challenge.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Mosbach, Marius and Pimentel, Tiago and Ravfogel, Shauli and Klakow, Dietrich and Elazar, Yanai},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	pages = {12284--12314},
}

@inproceedings{li_prefix-tuning_2021,
	address = {Online},
	title = {Prefix-{Tuning}: {Optimizing} {Continuous} {Prompts} for {Generation}},

	abstract = {Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were “virtual tokens”. We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We show that by learning only 0.1\% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Li, Xiang Lisa and Liang, Percy},
	editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
	month = aug,
	year = {2021},
	pages = {4582--4597},
}

@misc{jain_baseline_2023,
	title = {Baseline {Defenses} for {Adversarial} {Attacks} {Against} {Aligned} {Language} {Models}},

	author = {Jain, Neel and Schwarzschild, Avi and Wen, Yuxin and Somepalli, Gowthami and Kirchenbauer, John and Chiang, Ping-yeh and Goldblum, Micah and Saha, Aniruddha and Geiping, Jonas and Goldstein, Tom},
	year = {2023},
	note = {ArXiv: 2309.00614},
}

@inproceedings{liu_how_2024,
	address = {Torino, Italia},
	title = {How {Good} {Are} {LLMs} at {Out}-of-{Distribution} {Detection}?},

	abstract = {Out-of-distribution (OOD) detection plays a vital role in enhancing the reliability of machine learning models. As large language models (LLMs) become more prevalent, the applicability of prior research on OOD detection that utilized smaller-scale Transformers such as BERT, RoBERTa, and GPT-2 may be challenged, due to the significant differences in the scale of these models, their pre-training objectives, and the paradigms used for inference. This paper initiates a pioneering empirical investigation into the OOD detection capabilities of LLMs, focusing on the LLaMA series ranging from 7B to 65B in size. We thoroughly evaluate commonly used OOD detectors, examining their performance in both zero-grad and fine-tuning scenarios. Notably, we alter previous discriminative in-distribution fine-tuning into generative fine-tuning, aligning the pre-training objective of LLMs with downstream tasks. Our findings unveil that a simple cosine distance OOD detector demonstrates superior efficacy, outperforming other OOD detectors. We provide an intriguing explanation for this phenomenon by highlighting the isotropic nature of the embedding spaces of LLMs, which distinctly contrasts with the anisotropic property observed in smaller BERT family models. The new insight enhances our understanding of how LLMs detect OOD data, thereby enhancing their adaptability and reliability in dynamic environments. We have released the source code at https://github.com/Awenbocc/LLM-OOD for other researchers to reproduce our results.},
	booktitle = {Proceedings of the 2024 {Joint} {International} {Conference} on {Computational} {Linguistics}, {Language} {Resources} and {Evaluation} ({LREC}-{COLING} 2024)},
	publisher = {ELRA and ICCL},
	author = {Liu, Bo and Zhan, Li-Ming and Lu, Zexin and Feng, Yujie and Xue, Lei and Wu, Xiao-Ming},
	editor = {Calzolari, Nicoletta and Kan, Min-Yen and Hoste, Veronique and Lenci, Alessandro and Sakti, Sakriani and Xue, Nianwen},
	month = may,
	year = {2024},
	pages = {8211--8222},
}

@inproceedings{geng_grammar-constrained_2023,
	address = {Singapore},
	title = {Grammar-{Constrained} {Decoding} for {Structured} {NLP} {Tasks} without {Finetuning}},

	abstract = {Despite their impressive performance, large language models (LMs) still struggle with reliably generating complex output structures when not finetuned to follow the required output format exactly. To address this issue, grammar-constrained decoding (GCD) can be used to control the generation of LMs, guaranteeing that the output follows a given structure. Most existing GCD methods are, however, limited to specific tasks, such as parsing or code generation. In this work, we demonstrate that formal grammars can describe the output space for a much wider range of tasks and argue that GCD can serve as a unified framework for structured NLP tasks in general. For increased flexibility, we introduce input-dependent grammars, which allow the grammar to depend on the input and thus enable the generation of different output structures for different inputs. We then empirically demonstrate the power and flexibility of GCD-enhanced LMs on (1) information extraction, (2) entity disambiguation, and (3) constituency parsing. Our results indicate that grammar-constrained LMs substantially outperform unconstrained LMs or even beat task-specific finetuned models. Grammar constraints thus hold great promise for harnessing off-the-shelf LMs for a wide range of structured NLP tasks, especially where training data is scarce or finetuning is expensive. Code and data: https://github.com/epfl-dlab/GCD.},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Geng, Saibo and Josifoski, Martin and Peyrard, Maxime and West, Robert},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {10932--10952},
}

@article{farahani_brief_2021,
	title = {A brief review of domain adaptation},
	journal = {Advances in data science and information engineering: proceedings from ICDATA 2020 and IKE 2020},
	author = {Farahani, Abolfazl and Voghoei, Sahar and Rasheed, Khaled and Arabnia, Hamid R},
	year = {2021},
	note = {Publisher: Springer},
	pages = {877--894},
}

@misc{chaudhary_quantitative_2024,
	title = {Quantitative {Certification} of {Bias} in {Large} {Language} {Models}},

	author = {Chaudhary, Isha and Hu, Qian and Kumar, Manoj and Ziyadi, Morteza and Gupta, Rahul and Singh, Gagandeep},
	year = {2024},
	eprint={2405.18780},
	archivePrefix={arXiv},
}

@inproceedings{freiberger_fairness_2024,
	title = {Fairness certification for natural language processing and large language models},
	booktitle = {Intelligent {Systems} {Conference}},
	publisher = {Springer},
	author = {Freiberger, Vincent and Buchmann, Erik},
	year = {2024},
	pages = {606--624},
}

@article{rafailov_direct_2024,
	title = {Direct preference optimization: {Your} language model is secretly a reward model},
	volume = {36},
	journal = {Advances in Neural Information Processing Systems},
	author = {Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
	year = {2024},
}

@article{the_guardian_paknsave_2023,
	title = {Pak'{nSave} {AI} meal planner suggests toxic recipes in 'malfunction'},

	journal = {The Guardian},
	author = {{The Guardian}},
	year = {2023},
}

@misc{kishan_post_2023,
	title = {Post on {X}},

	author = {Kishan, JST},
	year = {2023},
}

@misc{reis_bible_2019,
	title = {Bible {Corpus} - {Basic} {Text} {Generation} using {N}-grams},

	abstract = {Explore and run machine learning code with Kaggle Notebooks {\textbackslash}textbar Using data from Bible Corpus},
	language = {en},
	urldate = {2024-08-30},
	author = {Reis, Eduardo},
	year = {2019},
}

@inproceedings{wang_glue_2019,
	title = {{GLUE}: {A} {Multi}-{Task} {Benchmark} and {Analysis} {Platform} for {Natural} {Language} {Understanding}},
	author = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
	year = {2019},
}

@inproceedings{maas_learning_2011,
	address = {Portland, Oregon, USA},
	title = {Learning {Word} {Vectors} for {Sentiment} {Analysis}},

	urldate = {2024-09-12},
	booktitle = {Proceedings of the 49th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Maas, Andrew L. and Daly, Raymond E. and Pham, Peter T. and Huang, Dan and Ng, Andrew Y. and Potts, Christopher},
	editor = {Lin, Dekang and Matsumoto, Yuji and Mihalcea, Rada},
	month = jun,
	year = {2011},
	pages = {142--150},
}

@inproceedings{hendrycks_baseline_2017,
	title = {A {Baseline} for {Detecting} {Misclassified} and {Out}-of-{Distribution} {Examples} in {Neural} {Networks}},

	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Hendrycks, Dan and Gimpel, Kevin},
	year = {2017},
}

@misc{lang_home_2014,
	title = {Home {Page} for 20 {Newsgroups} {Data} {Set} - http://qwone.com/{\textbackslash}textasciitildejason/{20Newsgroups}/},

	urldate = {2024-08-30},
	author = {Lang, Ken},
	year = {2014},
}

@inproceedings{khosla_supervised_2020,
	title = {Supervised {Contrastive} {Learning}},
	volume = {33},

	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Khosla, Prannay and Teterwak, Piotr and Wang, Chen and Sarna, Aaron and Tian, Yonglong and Isola, Phillip and Maschinot, Aaron and Liu, Ce and Krishnan, Dilip},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
	pages = {18661--18673},
}

@inproceedings{uppaal_is_2023,
	title = {Is {Fine}-tuning {Needed}? {Pre}-trained {Language} {Models} {Are} {Near} {Perfect} for {Out}-of-{Domain} {Detection}},

	booktitle = {Annual {Meeting} of the {Association} for {Computational} {Linguistics}},
	author = {Uppaal, Rheeya and Hu, Junjie and Li, Yixuan},
	year = {2023},
}

@inproceedings{zhou_contrastive_2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {Contrastive {Out}-of-{Distribution} {Detection} for {Pretrained} {Transformers}},

	abstract = {Pretrained Transformers achieve remarkable performance when training and test data are from the same distribution. However, in real-world scenarios, the model often faces out-of-distribution (OOD) instances that can cause severe semantic shift problems at inference time. Therefore, in practice, a reliable model should identify such instances, and then either reject them during inference or pass them over to models that handle another distribution. In this paper, we develop an unsupervised OOD detection method, in which only the in-distribution (ID) data are used in training. We propose to fine-tune the Transformers with a contrastive loss, which improves the compactness of representations, such that OOD instances can be better differentiated from ID ones. These OOD instances can then be accurately detected using the Mahalanobis distance in the model's penultimate layer. We experiment with comprehensive settings and achieve near-perfect OOD detection performance, outperforming baselines drastically. We further investigate the rationales behind the improvement, finding that more compact representations through margin-based contrastive learning bring the improvement. We release our code to the community for future research.},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Zhou, Wenxuan and Liu, Fangyu and Chen, Muhao},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	month = nov,
	year = {2021},
	pages = {1100--1111},
}

@inproceedings{gangal_likelihood_2020,
	title = {Likelihood ratios and generative classifiers for unsupervised out-of-domain detection in task oriented dialog},
	volume = {34},

	urldate = {2024-09-05},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Gangal, Varun and Arora, Abhinav and Einolghozati, Arash and Gupta, Sonal},
	year = {2020},
	pages = {7764--7771},
}

@inproceedings{podolskiy_revisiting_2021,
	title = {Revisiting {Mahalanobis} {Distance} for {Transformer}-{Based} {Out}-of-{Domain} {Detection}},

	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Podolskiy, A. V. and Lipin, Dmitry and Bout, A. and Artemova, E. and Piontkovskaya, Irina},
	year = {2021},
}

@article{jin_towards_2022,
	title = {Towards {Textual} {Out}-of-{Domain} {Detection} {Without} {In}-{Domain} {Labels}},
	volume = {30},
	issn = {2329-9290},

	abstract = {In many real-world settings, machine learning models need to identify user inputs that are out-of-domain (OOD) so as to avoid performing wrong actions. This work focuses on a challenging case of OOD detection, where no labels for in-domain data are accessible (e.g., no intent labels for the intent classification task). To this end, we first evaluate different language model based approaches that predict likelihood for a sequence of tokens. Furthermore, we propose a novel representation learning based method by combining unsupervised clustering and contrastive learning so that better data representations for OOD detection can be learned. Through extensive experiments, we demonstrate that this method can significantly outperform likelihood-based methods and can be even competitive to the state-of-the-art supervised approaches with label information.},
	journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
	author = {Jin, Di and Gao, Shuyang and Kim, Seokhwan and Liu, Yang and Hakkani-Tür, Dilek},
	month = mar,
	year = {2022},
	note = {Publisher: IEEE Press},
	pages = {1386--1395},
}

@inproceedings{liu_energy-based_2020,
	title = {Energy-based {Out}-of-distribution {Detection}},
	volume = {33},

	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Liu, Weitang and Wang, Xiaoyun and Owens, John and Li, Yixuan},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
	pages = {21464--21475},
}

@inproceedings{lin_flats_2023,
	address = {Singapore},
	title = {{FLatS}: {Principled} {Out}-of-{Distribution} {Detection} with {Feature}-{Based} {Likelihood} {Ratio} {Score}},

	abstract = {Detecting out-of-distribution (OOD) instances is crucial for NLP models in practical applications. Although numerous OOD detection methods exist, most of them are empirical. Backed by theoretical analysis, this paper advocates for the measurement of the “OOD-ness” of a test case {\textbackslash}boldsymbolx through the \textit{likelihood ratio} between out-distribution {\textbackslash}mathcal P\_\textit{out} and in-distribution {\textbackslash}mathcal P\_\textit{in}. We argue that the state-of-the-art (SOTA) feature-based OOD detection methods, such as Maha and KNN, are suboptimal since they only estimate in-distribution density p\_\textit{in}({\textbackslash}boldsymbolx). To address this issue, we propose \textbf{FLATS}, a principled solution for OOD detection based on likelihood ratio. Moreover, we demonstrate that FLATS can serve as a general framework capable of enhancing other OOD detection methods by incorporating out-distribution density p\_\textit{out}({\textbackslash}boldsymbolx) estimation. Experiments show that FLATS establishes a new SOTA on popular benchmarks.},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Lin, Haowei and Gu, Yuntian},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {8956--8963},
}

@inproceedings{nalisnick_detecting_2019,
	title = {Detecting {Out}-of-{Distribution} {Inputs} to {Deep} {Generative} {Models} {Using} {Typicality}},

	author = {Nalisnick, Eric and Matsukawa, Akihiro and Teh, Yee Whye and Lakshminarayanan, Balaji},
	year = {2019},
	note = {ArXiv: 1906.02994},
}

@inproceedings{yang_openood_2022,
	title = {{OpenOOD}: {Benchmarking} {Generalized} {Out}-of-{Distribution} {Detection}},
	volume = {35},

	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Yang, Jingkang and Wang, Pengyun and Zou, Dejian and Zhou, Zitang and Ding, Kunyuan and PENG, WENXUAN and Wang, Haoqi and Chen, Guangyao and Li, Bo and Sun, Yiyou and Du, Xuefeng and Zhou, Kaiyang and Zhang, Wayne and Hendrycks, Dan and Li, Yixuan and Liu, Ziwei},
	editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
	year = {2022},
	pages = {32598--32611},
}

@misc{karpathy_unreasonable_2015,
	title = {The {Unreasonable} {Effectiveness} of {Recurrent} {Neural} {Networks} - http://karpathy.github.io/2015/05/21/rnn-effectiveness/},

	urldate = {2024-08-29},
	author = {Karpathy, Andrej},
	year = {2015},
}

@inproceedings{christiano_deep_2017,
	title = {Deep {Reinforcement} {Learning} from {Human} {Preferences}},
	volume = {30},

	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
	editor = {Guyon, I. and Luxburg, U. Von and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
}

@misc{zou_universal_2023,
	title = {Universal and {Transferable} {Adversarial} {Attacks} on {Aligned} {Language} {Models}},
	author = {Zou, Andy and Wang, Zifan and Kolter, J. Zico and Fredrikson, Matt},
	year = {2023},
	note = {Arxiv: 2307.15043},
}

@article{li_privacy_2023,
	title = {Privacy in large language models: {Attacks}, defenses and future directions},
	journal = {arXiv preprint arXiv:2310.10383},
	author = {Li, Haoran and Chen, Yulin and Luo, Jinglong and Kang, Yan and Zhang, Xiaojin and Hu, Qi and Chan, Chunkit and Song, Yangqiu},
	year = {2023},
}

@article{shayegani_survey_2023,
	title = {Survey of vulnerabilities in large language models revealed by adversarial attacks},
	journal = {arXiv preprint arXiv:2310.10844},
	author = {Shayegani, Erfan and Mamun, Md Abdullah Al and Fu, Yu and Zaree, Pedram and Dong, Yue and Abu-Ghazaleh, Nael},
	year = {2023},
	note = {Arxiv: 2310.10844},
}

@inproceedings{yong_low-resource_2023,
	title = {Low-{Resource} {Languages} {Jailbreak} {GPT}-4},
	booktitle = {{arXiv} preprint {arXiv}:2310.02446},
	author = {Yong, Zheng-Xin and Menghini, Cristina and Bach, Stephen H},
	year = {2023},
	note = {Arxiv: 2310.02446},
}

@article{liu_summary_2023,
	title = {Summary of chatgpt-related research and perspective towards the future of large language models},
	journal = {Meta-Radiology},
	author = {Liu, Yiheng and Han, Tianle and Ma, Siyuan and Zhang, Jiayue and Yang, Yuanyuan and Tian, Jiaming and He, Hao and Li, Antong and He, Mengshen and Liu, Zhengliang and {others}},
	year = {2023},
	note = {Publisher: Elsevier},
	pages = {100017},
}

@inproceedings{dettmers_qlora_2023,
	title = {{QLoRA}: {Efficient} {Finetuning} of {Quantized} {LLMs}},
	volume = {36},

	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
	editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
	year = {2023},
	pages = {10088--10115},
}

@inproceedings{liu_few-shot_2022,
	title = {Few-{Shot} {Parameter}-{Efficient} {Fine}-{Tuning} is {Better} and {Cheaper} than {In}-{Context} {Learning}},
	volume = {35},

	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Liu, Haokun and Tam, Derek and Muqeeth, Mohammed and Mohta, Jay and Huang, Tenghao and Bansal, Mohit and Raffel, Colin A},
	editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
	year = {2022},
	pages = {1950--1965},
}

@inproceedings{ouyang_training_2022,
	title = {Training language models to follow instructions with human feedback},
	volume = {35},

	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul F and Leike, Jan and Lowe, Ryan},
	editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
	year = {2022},
	pages = {27730--27744},
}

@misc{henighan_scaling_2020,
	title = {Scaling {Laws} for {Autoregressive} {Generative} {Modeling}},

	author = {Henighan, Tom and Kaplan, Jared and Katz, Mor and Chen, Mark and Hesse, Christopher and Jackson, Jacob and Jun, Heewoo and Brown, Tom B. and Dhariwal, Prafulla and Gray, Scott and Hallacy, Chris and Mann, Benjamin and Radford, Alec and Ramesh, Aditya and Ryder, Nick and Ziegler, Daniel M. and Schulman, John and Amodei, Dario and McCandlish, Sam},
	year = {2020},
	eprint={2010.14701},
	archivePrefix={arXiv},
}

@inproceedings{alabdulmohsin_revisiting_2022,
	title = {Revisiting {Neural} {Scaling} {Laws} in {Language} and {Vision}},
	volume = {35},

	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Alabdulmohsin, Ibrahim M and Neyshabur, Behnam and Zhai, Xiaohua},
	editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
	year = {2022},
	pages = {22300--22312},
}

@inproceedings{qi_fine-tuning_2024,
	title = {Fine-tuning {Aligned} {Language} {Models} {Compromises} {Safety}, {Even} {When} {Users} {Do} {Not} {Intend} {To}!},

	booktitle = {The {Twelfth} {International} {Conference} on {Learning} {Representations}},
	author = {Qi, Xiangyu and Zeng, Yi and Xie, Tinghao and Chen, Pin-Yu and Jia, Ruoxi and Mittal, Prateek and Henderson, Peter},
	year = {2024},
}

@inproceedings{wang_backdooralign_2024,
	title = {{BackdoorAlign}: {Mitigating} {Fine}-tuning based {Jailbreak} {Attack} with {Backdoor} {Enhanced} {Safety} {Alignment}},

	booktitle = {The {Thirty}-eighth {Annual} {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Wang, Jiongxiao and Li, Jiazhao and Li, Yiquan and Qi, Xiangyu and Hu, Junjie and Li, Yixuan and McDaniel, Patrick and Chen, Muhao and Li, Bo and Xiao, Chaowei},
	year = {2024},
}

@inproceedings{gao_making_2021,
	address = {Online},
	title = {Making {Pre}-trained {Language} {Models} {Better} {Few}-shot {Learners}},

	abstract = {The recent GPT-3 model (Brown et al., 2020) achieves remarkable few-shot performance solely by leveraging a natural-language prompt and a few task demonstrations as input context. Inspired by their findings, we study few-shot learning in a more practical scenario, where we use smaller language models for which fine-tuning is computationally efficient. We present LM-BFF—better few-shot fine-tuning of language models—a suite of simple and complementary techniques for fine-tuning language models on a small number of annotated examples. Our approach includes (1) prompt-based fine-tuning together with a novel pipeline for automating prompt generation; and (2) a refined strategy for dynamically and selectively incorporating demonstrations into each context. Finally, we present a systematic evaluation for analyzing few-shot performance on a range of NLP tasks, including classification and regression. Our experiments demonstrate that our methods combine to dramatically outperform standard fine-tuning procedures in this low resource setting, achieving up to 30\% absolute improvement, and 11\% on average across all tasks. Our approach makes minimal assumptions on task resources and domain expertise, and hence constitutes a strong task-agnostic method for few-shot learning.},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Gao, Tianyu and Fisch, Adam and Chen, Danqi},
	editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
	month = aug,
	year = {2021},
	pages = {3816--3830},
}

@misc{dubey_llama_2024,
	title = {The {Llama} 3 {Herd} of {Models}},

	author = {Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and Goyal, Anirudh and Hartshorn, Anthony and Yang, Aobo and Mitra, Archi and Sravankumar, Archie and Korenev, Artem and Hinsvark, Arthur and Rao, Arun and Zhang, Aston and Rodriguez, Aurelien and Gregerson, Austen and Spataru, Ava and Roziere, Baptiste and Biron, Bethany and Tang, Binh and Chern, Bobbie and Caucheteux, Charlotte and Nayak, Chaya and Bi, Chloe and Marra, Chris and McConnell, Chris and Keller, Christian and Touret, Christophe and Wu, Chunyang and Wong, Corinne and Ferrer, Cristian Canton and Nikolaidis, Cyrus and Allonsius, Damien and Song, Daniel and Pintz, Danielle and Livshits, Danny and Esiobu, David and Choudhary, Dhruv and Mahajan, Dhruv and Garcia-Olano, Diego and Perino, Diego and Hupkes, Dieuwke and Lakomkin, Egor and AlBadawy, Ehab and Lobanova, Elina and Dinan, Emily and Smith, Eric Michael and Radenovic, Filip and Zhang, Frank and Synnaeve, Gabriel and Lee, Gabrielle and Anderson, Georgia Lewis and Nail, Graeme and Mialon, Gregoire and Pang, Guan and Cucurell, Guillem and Nguyen, Hailey and Korevaar, Hannah and Xu, Hu and Touvron, Hugo and Zarov, Iliyan and Ibarra, Imanol Arrieta and Kloumann, Isabel and Misra, Ishan and Evtimov, Ivan and Copet, Jade and Lee, Jaewon and Geffert, Jan and Vranes, Jana and Park, Jason and Mahadeokar, Jay and Shah, Jeet and Linde, Jelmer van der and Billock, Jennifer and Hong, Jenny and Lee, Jenya and Fu, Jeremy and Chi, Jianfeng and Huang, Jianyu and Liu, Jiawen and Wang, Jie and Yu, Jiecao and Bitton, Joanna and Spisak, Joe and Park, Jongsoo and Rocca, Joseph and Johnstun, Joshua and Saxe, Joshua and Jia, Junteng and Alwala, Kalyan Vasuden and Upasani, Kartikeya and Plawiak, Kate and Li, Ke and Heafield, Kenneth and Stone, Kevin and El-Arini, Khalid and Iyer, Krithika and Malik, Kshitiz and Chiu, Kuenley and Bhalla, Kunal and Rantala-Yeary, Lauren and Maaten, Laurens van der and Chen, Lawrence and Tan, Liang and Jenkins, Liz and Martin, Louis and Madaan, Lovish and Malo, Lubo and Blecher, Lukas and Landzaat, Lukas and Oliveira, Luke de and Muzzi, Madeline and Pasupuleti, Mahesh and Singh, Mannat and Paluri, Manohar and Kardas, Marcin and Oldham, Mathew and Rita, Mathieu and Pavlova, Maya and Kambadur, Melanie and Lewis, Mike and Si, Min and Singh, Mitesh Kumar and Hassan, Mona and Goyal, Naman and Torabi, Narjes and Bashlykov, Nikolay and Bogoychev, Nikolay and Chatterji, Niladri and Duchenne, Olivier and Çelebi, Onur and Alrassy, Patrick and Zhang, Pengchuan and Li, Pengwei and Vasic, Petar and Weng, Peter and Bhargava, Prajjwal and Dubal, Pratik and Krishnan, Praveen and Koura, Punit Singh and Xu, Puxin and He, Qing and Dong, Qingxiao and Srinivasan, Ragavan and Ganapathy, Raj and Calderer, Ramon and Cabral, Ricardo Silveira and Stojnic, Robert and Raileanu, Roberta and Girdhar, Rohit and Patel, Rohit and Sauvestre, Romain and Polidoro, Ronnie and Sumbaly, Roshan and Taylor, Ross and Silva, Ruan and Hou, Rui and Wang, Rui and Hosseini, Saghar and Chennabasappa, Sahana and Singh, Sanjay and Bell, Sean and Kim, Seohyun Sonia and Edunov, Sergey and Nie, Shaoliang and Narang, Sharan and Raparthy, Sharath and Shen, Sheng and Wan, Shengye and Bhosale, Shruti and Zhang, Shun and Vandenhende, Simon and Batra, Soumya and Whitman, Spencer and Sootla, Sten and Collot, Stephane and Gururangan, Suchin and Borodinsky, Sydney and Herman, Tamar and Fowler, Tara and Sheasha, Tarek and Georgiou, Thomas and Scialom, Thomas and Speckbacher, Tobias and Mihaylov, Todor and Xiao, Tong and Karn, Ujjwal and Goswami, Vedanuj and Gupta, Vibhor and Ramanathan, Vignesh and Kerkez, Viktor and Gonguet, Vincent and Do, Virginie and Vogeti, Vish and Petrovic, Vladan and Chu, Weiwei and Xiong, Wenhan and Fu, Wenyin and Meers, Whitney and Martinet, Xavier and Wang, Xiaodong and Tan, Xiaoqing Ellen and Xie, Xinfeng and Jia, Xuchao and Wang, Xuewei and Goldschlag, Yaelle and Gaur, Yashesh and Babaei, Yasmine and Wen, Yi and Song, Yiwen and Zhang, Yuchen and Li, Yue and Mao, Yuning and Coudert, Zacharie Delpierre and Yan, Zheng and Chen, Zhengxing and Papakipos, Zoe and Singh, Aaditya and Grattafiori, Aaron and Jain, Abha and Kelsey, Adam and Shajnfeld, Adam and Gangidi, Adithya and Victoria, Adolfo and Goldstand, Ahuva and Menon, Ajay and Sharma, Ajay and Boesenberg, Alex and Vaughan, Alex and Baevski, Alexei and Feinstein, Allie and Kallet, Amanda and Sangani, Amit and Yunus, Anam and Lupu, Andrei and Alvarado, Andres and Caples, Andrew and Gu, Andrew and Ho, Andrew and Poulton, Andrew and Ryan, Andrew and Ramchandani, Ankit and Franco, Annie and Saraf, Aparajita and Chowdhury, Arkabandhu and Gabriel, Ashley and Bharambe, Ashwin and Eisenman, Assaf and Yazdan, Azadeh and James, Beau and Maurer, Ben and Leonhardi, Benjamin and Huang, Bernie and Loyd, Beth and Paola, Beto De and Paranjape, Bhargavi and Liu, Bing and Wu, Bo and Ni, Boyu and Hancock, Braden and Wasti, Bram and Spence, Brandon and Stojkovic, Brani and Gamido, Brian and Montalvo, Britt and Parker, Carl and Burton, Carly and Mejia, Catalina and Wang, Changhan and Kim, Changkyu and Zhou, Chao and Hu, Chester and Chu, Ching-Hsiang and Cai, Chris and Tindal, Chris and Feichtenhofer, Christoph and Civin, Damon and Beaty, Dana and Kreymer, Daniel and Li, Daniel and Wyatt, Danny and Adkins, David and Xu, David and Testuggine, Davide and David, Delia and Parikh, Devi and Liskovich, Diana and Foss, Didem and Wang, Dingkang and Le, Duc and Holland, Dustin and Dowling, Edward and Jamil, Eissa and Montgomery, Elaine and Presani, Eleonora and Hahn, Emily and Wood, Emily and Brinkman, Erik and Arcaute, Esteban and Dunbar, Evan and Smothers, Evan and Sun, Fei and Kreuk, Felix and Tian, Feng and Ozgenel, Firat and Caggioni, Francesco and Guzmán, Francisco and Kanayet, Frank and Seide, Frank and Florez, Gabriela Medina and Schwarz, Gabriella and Badeer, Gada and Swee, Georgia and Halpern, Gil and Thattai, Govind and Herman, Grant and Sizov, Grigory and {Guangyi} and {Zhang} and Lakshminarayanan, Guna and Shojanazeri, Hamid and Zou, Han and Wang, Hannah and Zha, Hanwen and Habeeb, Haroun and Rudolph, Harrison and Suk, Helen and Aspegren, Henry and Goldman, Hunter and Damlaj, Ibrahim and Molybog, Igor and Tufanov, Igor and Veliche, Irina-Elena and Gat, Itai and Weissman, Jake and Geboski, James and Kohli, James and Asher, Japhet and Gaya, Jean-Baptiste and Marcus, Jeff and Tang, Jeff and Chan, Jennifer and Zhen, Jenny and Reizenstein, Jeremy and Teboul, Jeremy and Zhong, Jessica and Jin, Jian and Yang, Jingyi and Cummings, Joe and Carvill, Jon and Shepard, Jon and McPhie, Jonathan and Torres, Jonathan and Ginsburg, Josh and Wang, Junjie and Wu, Kai and U, Kam Hou and Saxena, Karan and Prasad, Karthik and Khandelwal, Kartikay and Zand, Katayoun and Matosich, Kathy and Veeraraghavan, Kaushik and Michelena, Kelly and Li, Keqian and Huang, Kun and Chawla, Kunal and Lakhotia, Kushal and Huang, Kyle and Chen, Lailin and Garg, Lakshya and A, Lavender and Silva, Leandro and Bell, Lee and Zhang, Lei and Guo, Liangpeng and Yu, Licheng and Moshkovich, Liron and Wehrstedt, Luca and Khabsa, Madian and Avalani, Manav and Bhatt, Manish and Tsimpoukelli, Maria and Mankus, Martynas and Hasson, Matan and Lennie, Matthew and Reso, Matthias and Groshev, Maxim and Naumov, Maxim and Lathi, Maya and Keneally, Meghan and Seltzer, Michael L. and Valko, Michal and Restrepo, Michelle and Patel, Mihir and Vyatskov, Mik and Samvelyan, Mikayel and Clark, Mike and Macey, Mike and Wang, Mike and Hermoso, Miquel Jubert and Metanat, Mo and Rastegari, Mohammad and Bansal, Munish and Santhanam, Nandhini and Parks, Natascha and White, Natasha and Bawa, Navyata and Singhal, Nayan and Egebo, Nick and Usunier, Nicolas and Laptev, Nikolay Pavlovich and Dong, Ning and Zhang, Ning and Cheng, Norman and Chernoguz, Oleg and Hart, Olivia and Salpekar, Omkar and Kalinli, Ozlem and Kent, Parkin and Parekh, Parth and Saab, Paul and Balaji, Pavan and Rittner, Pedro and Bontrager, Philip and Roux, Pierre and Dollar, Piotr and Zvyagina, Polina and Ratanchandani, Prashant and Yuvraj, Pritish and Liang, Qian and Alao, Rachad and Rodriguez, Rachel and Ayub, Rafi and Murthy, Raghotham and Nayani, Raghu and Mitra, Rahul and Li, Raymond and Hogan, Rebekkah and Battey, Robin and Wang, Rocky and Maheswari, Rohan and Howes, Russ and Rinott, Ruty and Bondu, Sai Jayesh and Datta, Samyak and Chugh, Sara and Hunt, Sara and Dhillon, Sargun and Sidorov, Sasha and Pan, Satadru and Verma, Saurabh and Yamamoto, Seiji and Ramaswamy, Sharadh and Lindsay, Shaun and Lindsay, Shaun and Feng, Sheng and Lin, Shenghao and Zha, Shengxin Cindy and Shankar, Shiva and Zhang, Shuqiang and Zhang, Shuqiang and Wang, Sinong and Agarwal, Sneha and Sajuyigbe, Soji and Chintala, Soumith and Max, Stephanie and Chen, Stephen and Kehoe, Steve and Satterfield, Steve and Govindaprasad, Sudarshan and Gupta, Sumit and Cho, Sungmin and Virk, Sunny and Subramanian, Suraj and Choudhury, Sy and Goldman, Sydney and Remez, Tal and Glaser, Tamar and Best, Tamara and Kohler, Thilo and Robinson, Thomas and Li, Tianhe and Zhang, Tianjun and Matthews, Tim and Chou, Timothy and Shaked, Tzook and Vontimitta, Varun and Ajayi, Victoria and Montanez, Victoria and Mohan, Vijai and Kumar, Vinay Satish and Mangla, Vishal and Albiero, Vítor and Ionescu, Vlad and Poenaru, Vlad and Mihailescu, Vlad Tiberiu and Ivanov, Vladimir and Li, Wei and Wang, Wenchen and Jiang, Wenwen and Bouaziz, Wes and Constable, Will and Tang, Xiaocheng and Wang, Xiaofang and Wu, Xiaojian and Wang, Xiaolan and Xia, Xide and Wu, Xilun and Gao, Xinbo and Chen, Yanjun and Hu, Ye and Jia, Ye and Qi, Ye and Li, Yenda and Zhang, Yilin and Zhang, Ying and Adi, Yossi and Nam, Youngjin and {Yu} and {Wang} and Hao, Yuchen and Qian, Yundi and He, Yuzi and Rait, Zach and DeVito, Zachary and Rosnbrick, Zef and Wen, Zhaoduo and Yang, Zhenyu and Zhao, Zhiwei},
	year = {2024},
	note = {ArXiv: 2407.21783},
}

@misc{dong_safeguarding_2024,
	title = {Safeguarding {Large} {Language} {Models}: {A} {Survey}},

	author = {Dong, Yi and Mu, Ronghui and Zhang, Yanghao and Sun, Siqi and Zhang, Tianle and Wu, Changshun and Jin, Gaojie and Qi, Yi and Hu, Jinwei and Meng, Jie and Bensalem, Saddek and Huang, Xiaowei},
	year = {2024},
	note = {ArXiv: 2406.02622},
}

@inproceedings{dai_safe_2024,
	title = {Safe {RLHF}: {Safe} {Reinforcement} {Learning} from {Human} {Feedback}},

	booktitle = {The {Twelfth} {International} {Conference} on {Learning} {Representations}},
	author = {Dai, Josef and Pan, Xuehai and Sun, Ruiyang and Ji, Jiaming and Xu, Xinbo and Liu, Mickel and Wang, Yizhou and Yang, Yaodong},
	year = {2024},
}

@misc{chua_flexible_2024,
	title = {A {Flexible} {Large} {Language} {Models} {Guardrail} {Development} {Methodology} {Applied} to {Off}-{Topic} {Prompt} {Detection}},

	author = {Chua, Gabriel and Chan, Shing Yee and Khoo, Shaun},
	year = {2024},
	note = {ArXiv: 2411.12946},
}

@inproceedings{chaudhary_quantitative_2024-1,
	title = {Quantitative {Certification} of {Knowledge} {Comprehension} in {LLMs}},

	booktitle = {{ICLR} 2024 {Workshop} on {Secure} and {Trustworthy} {Large} {Language} {Models}},
	author = {Chaudhary, Isha and Jain, Vedaant V. and Singh, Gagandeep},
	year = {2024},
	note = {ArXiv: 2402.15929},
}

@inproceedings{chao_jailbreaking_2023,
	title = {Jailbreaking {Black} {Box} {Large} {Language} {Models} in {Twenty} {Queries}},

	booktitle = {R0-{FoMo}:{Robustness} of {Few}-shot and {Zero}-shot {Learning} in {Large} {Foundation} {Models}},
	author = {Chao, Patrick and Robey, Alexander and Dobriban, Edgar and Hassani, Hamed and Pappas, George J. and Wong, Eric},
	year = {2023},
	note = {ArXiv: 2310.08419},
}

@misc{casadio_nlp_2024,
	title = {{NLP} {Verification}: {Towards} a {General} {Methodology} for {Certifying} {Robustness}},

	author = {Casadio, Marco and Dinkar, Tanvi and Komendantskaya, Ekaterina and Arnaboldi, Luca and Daggitt, Matthew L. and Isac, Omri and Katz, Guy and Rieser, Verena and Lemon, Oliver},
	year = {2024},
	note = {ArXiv: 2403.10144},
}

@inproceedings{carlini_are_2023,
	title = {Are aligned neural networks adversarially aligned?},
	volume = {36},

	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Carlini, Nicholas and Nasr, Milad and Choquette-Choo, Christopher A. and Jagielski, Matthew and Gao, Irena and Koh, Pang Wei W and Ippolito, Daphne and Tramer, Florian and Schmidt, Ludwig},
	editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
	year = {2023},
	pages = {61478--61500},
}

@misc{carlini_poisoning_2024,
	title = {Poisoning {Web}-{Scale} {Training} {Datasets} is {Practical}},

	author = {Carlini, Nicholas and Jagielski, Matthew and Choquette-Choo, Christopher A. and Paleka, Daniel and Pearce, Will and Anderson, Hyrum and Terzis, Andreas and Thomas, Kurt and Tramèr, Florian},
	year = {2024},
	note = {ArXiv: 2302.10149},
}

@inproceedings{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	volume = {33},

	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
	pages = {1877--1901},
}

@misc{bommasani_opportunities_2022,
	title = {On the {Opportunities} and {Risks} of {Foundation} {Models}},

	author = {Bommasani, Rishi and Hudson, Drew A. and Adeli, Ehsan and Altman, Russ and Arora, Simran and Arx, Sydney von and Bernstein, Michael S. and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and Brynjolfsson, Erik and Buch, Shyamal and Card, Dallas and Castellon, Rodrigo and Chatterji, Niladri and Chen, Annie and Creel, Kathleen and Davis, Jared Quincy and Demszky, Dora and Donahue, Chris and Doumbouya, Moussa and Durmus, Esin and Ermon, Stefano and Etchemendy, John and Ethayarajh, Kawin and Fei-Fei, Li and Finn, Chelsea and Gale, Trevor and Gillespie, Lauren and Goel, Karan and Goodman, Noah and Grossman, Shelby and Guha, Neel and Hashimoto, Tatsunori and Henderson, Peter and Hewitt, John and Ho, Daniel E. and Hong, Jenny and Hsu, Kyle and Huang, Jing and Icard, Thomas and Jain, Saahil and Jurafsky, Dan and Kalluri, Pratyusha and Karamcheti, Siddharth and Keeling, Geoff and Khani, Fereshte and Khattab, Omar and Koh, Pang Wei and Krass, Mark and Krishna, Ranjay and Kuditipudi, Rohith and Kumar, Ananya and Ladhak, Faisal and Lee, Mina and Lee, Tony and Leskovec, Jure and Levent, Isabelle and Li, Xiang Lisa and Li, Xuechen and Ma, Tengyu and Malik, Ali and Manning, Christopher D. and Mirchandani, Suvir and Mitchell, Eric and Munyikwa, Zanele and Nair, Suraj and Narayan, Avanika and Narayanan, Deepak and Newman, Ben and Nie, Allen and Niebles, Juan Carlos and Nilforoshan, Hamed and Nyarko, Julian and Ogut, Giray and Orr, Laurel and Papadimitriou, Isabel and Park, Joon Sung and Piech, Chris and Portelance, Eva and Potts, Christopher and Raghunathan, Aditi and Reich, Rob and Ren, Hongyu and Rong, Frieda and Roohani, Yusuf and Ruiz, Camilo and Ryan, Jack and Ré, Christopher and Sadigh, Dorsa and Sagawa, Shiori and Santhanam, Keshav and Shih, Andy and Srinivasan, Krishnan and Tamkin, Alex and Taori, Rohan and Thomas, Armin W. and Tramèr, Florian and Wang, Rose E. and Wang, William and Wu, Bohan and Wu, Jiajun and Wu, Yuhuai and Xie, Sang Michael and Yasunaga, Michihiro and You, Jiaxuan and Zaharia, Matei and Zhang, Michael and Zhang, Tianyi and Zhang, Xikun and Zhang, Yuhui and Zheng, Lucia and Zhou, Kaitlyn and Liang, Percy},
	year = {2022},
	note = {ArXiv: 2108.07258},
	keywords = {ONLY-ARXIV},
}

@inproceedings{biggio_poisoning_2012,
	address = {Madison, WI, USA},
	series = {{ICML}'12},
	title = {Poisoning {Attacks} against {Support} {Vector} {Machines}},
	isbn = {978-1-4503-1285-1},
	abstract = {We investigate a family of poisoning attacks against Support Vector Machines (SVM). Such attacks inject specially crafted training data that increases the SVM's test error. Central to the motivation for these attacks is the fact that most learning algorithms assume that their training data comes from a natural or well-behaved distribution. However, this assumption does not generally hold in security-sensitive settings. As we demonstrate, an intelligent adversary can, to some extent, predict the change of the SVM's decision function due to malicious input and use this ability to construct malicious data.The proposed attack uses a gradient ascent strategy in which the gradient is computed based on properties of the SVM's optimal solution. This method can be kernelized and enables the attack to be constructed in the input space even for non-linear kernels. We experimentally demonstrate that our gradient ascent procedure reliably identifies good local maxima of the non-convex validation error surface, which significantly increases the classifier's test error.},
	booktitle = {Proceedings of the 29th {International} {Coference} on {International} {Conference} on {Machine} {Learning}},
	publisher = {Omnipress},
	author = {Biggio, Battista and Nelson, Blaine and Laskov, Pavel},
	year = {2012},
	note = {event-place: Edinburgh, Scotland},
	pages = {1467--1474},
}

@misc{bengio_can_2024,
	title = {Can a {Bayesian} {Oracle} {Prevent} {Harm} from an {Agent}?},

	author = {Bengio, Yoshua and Cohen, Michael K. and Malkin, Nikolay and MacDermott, Matt and Fornasiere, Damiano and Greiner, Pietro and Kaddar, Younesse},
	year = {2024},
	note = {ArXiv: 2408.05284},
}

@misc{bengio_bounding_2024,
	title = {Bounding the probability of harm from an {AI} to create a guardrail - https://yoshuabengio.org/2024/08/29/bounding-the-probability-of-harm-from-an-ai-to-create-a-guardrail/},
	abstract = {As we move towards more powerful AI, it becomes urgent to better understand the risks, ideally in a mathematically rigorous and quantifiable way, and use…},
	language = {en-CA},
	urldate = {2024-09-06},
	journal = {Yoshua Bengio},
	author = {Bengio, Yoshua},
	month = aug,
	year = {2024},
}

@misc{eu_regulation_2024,
	title = {Regulation ({EU}) 2024/1689 of the {European} {Parliament} and of the {Council} of 13 {June} 2024 laying down harmonised rules on artificial intelligence and amending {Regulations} ({EC}) {No} 300/2008, ({EU}) {No} 167/2013, ({EU}) {No} 168/2013, ({EU}) 2018/858, ({EU}) 2018/1139 and ({EU}) 2019/2144 and {Directives} 2014/90/{EU}, ({EU}) 2016/797 and ({EU}) 2020/1828 ({Artificial} {Intelligence} {Act}) ({Text} with {EEA} relevance)},

	language = {en},
	urldate = {2024-10-02},
	author = {EU},
	month = jun,
	year = {2024},
	note = {Legislative Body: CONSIL, EP},
}

@article{youden_index_1950,
	title = {Index for rating diagnostic tests},
	volume = {3},

	number = {1},
	journal = {Cancer},
	author = {Youden, W. J.},
	year = {1950},
	pmid = {15405679},
	pages = {32--35},
}

@article{aimeta_llama_2024,
	title = {Llama 3 {Model} {Card}},

	author = {{AI@Meta}},
	year = {2024},
}

@misc{wang_mmlu-pro_2024,
	title = {{MMLU}-{Pro}: {A} {More} {Robust} and {Challenging} {Multi}-{Task} {Language} {Understanding} {Benchmark}},

	author = {Wang, Yubo and Ma, Xueguang and Zhang, Ge and Ni, Yuansheng and Chandra, Abhranil and Guo, Shiguang and Ren, Weiming and Arulraj, Aaran and He, Xuan and Jiang, Ziyan and Li, Tianle and Ku, Max and Wang, Kai and Zhuang, Alex and Fan, Rongqi and Yue, Xiang and Chen, Wenhu},
	year = {2024},
	eprint={2406.01574},
	archivePrefix={arXiv},
}

@misc{ankit_pal_openbiollms_2024,
	title = {{OpenBioLLMs}: {Advancing} {Open}-{Source} {Large} {Language} {Models} for {Healthcare} and {Life} {Sciences}},

	publisher = {Hugging Face},
	author = {Ankit Pal, Malaikannan Sankarasubbu},
	year = {2024},
	note = {Publication Title: Hugging Face repository},
}

@article{joshi_triviaqa_2017,
	title = {{TriviaQA}: {A} {Large} {Scale} {Distantly} {Supervised} {Challenge} {Dataset} for {Reading} {Comprehension}},
	volume = {abs/1705.03551},

	journal = {ArXiv},
	author = {Joshi, Mandar and Choi, Eunsol and Weld, Daniel S. and Zettlemoyer, Luke},
	year = {2017},
}

@article{kwiatkowski_natural_2019,
	title = {Natural {Questions}: a {Benchmark} for {Question} {Answering} {Research}},
	journal = {Transactions of the Association of Computational Linguistics},
	author = {Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Kelcey, Matthew and Devlin, Jacob and Lee, Kenton and Toutanova, Kristina N. and Jones, Llion and Chang, Ming-Wei and Dai, Andrew and Uszkoreit, Jakob and Le, Quoc and Petrov, Slav},
	year = {2019},
}

@inproceedings{mihaylov_can_2018,
	title = {Can a {Suit} of {Armor} {Conduct} {Electricity}? {A} {New} {Dataset} for {Open} {Book} {Question} {Answering}},
	booktitle = {{EMNLP}},
	author = {Mihaylov, Todor and Clark, Peter and Khot, Tushar and Sabharwal, Ashish},
	year = {2018},
}

@inproceedings{rajpurkar_squad_2016,
	address = {Austin, Texas},
	title = {{SQuAD}: 100,000+ {Questions} for {Machine} {Comprehension} of {Text}},

	booktitle = {Proceedings of the 2016 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
	editor = {Su, Jian and Duh, Kevin and Carreras, Xavier},
	month = nov,
	year = {2016},
	eprint={1606.05250},
	archivePrefix={arXiv},
	pages = {2383--2392},
}

@inproceedings{rajpurkar_know_2018,
	address = {Melbourne, Australia},
	title = {Know {What} {You} {Don}'t {Know}: {Unanswerable} {Questions} for {SQuAD}},

	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Rajpurkar, Pranav and Jia, Robin and Liang, Percy},
	editor = {Gurevych, Iryna and Miyao, Yusuke},
	month = jul,
	year = {2018},
	eprint={1806.03822},
	archivePrefix={arXiv},
	pages = {784--789},
}

@article{abacha_question-entailment_2019,
	title = {A {Question}-{Entailment} {Approach} to {Question} {Answering}},
	volume = {20},

	abstract = {Background: One of the challenges in large-scale information retrieval (IR) is developing fine-grained and domain-specific methods to answer natural language questions. Despite the availability of numerous sources and datasets for answer retrieval, Question Answering (QA) remains a challenging problem due to the difficulty of the question understanding and answer extraction tasks. One of the promising tracks investigated in QA is mapping new questions to formerly answered questions that are “similar”.
Results: WeproposeanovelQAapproachbasedonRecognizingQuestionEntailment(RQE)andwedescribetheQA system and resources that we built and evaluated on real medical questions. First, we compare logistic regression and deep learning methods for RQE using different kinds of datasets including textual inference, question similarity, and entailment in both the open and clinical domains. Second, we combine IR models with the best RQE method to select entailed questions and rank the retrieved answers. To study the end-to-end QA approach, we built the MedQuAD collection of 47,457 question-answer pairs from trusted medical sources which we introduce and share in the scope of this paper. Following the evaluation process used in TREC 2017 LiveQA, we find that our approach exceeds the best results of the medical task with a 29.8\% increase over the best official score.
Conclusions: TheevaluationresultssupporttherelevanceofquestionentailmentforQAandhighlightthe effectiveness of combining IR and RQE for future QA efforts. Our findings also show that relying on a restricted set of reliable answer sources can bring a substantial improvement in medical QA.},
	number = {1},
	journal = {BMC Bioinform.},
	author = {Abacha, Asma Ben and Demner-Fushman, Dina},
	year = {2019},
	pages = {511:1--511:23},
}

@inproceedings{pal_medmcqa_2022,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {{MedMCQA}: {A} {Large}-scale {Multi}-{Subject} {Multi}-{Choice} {Dataset} for {Medical} domain {Question} {Answering}},
	volume = {174},

	abstract = {This paper introduces MedMCQA, a new large-scale, Multiple-Choice Question Answering (MCQA) dataset designed to address real-world medical entrance exam questions. More than 194k high-quality AIIMS \& NEET PG entrance exam MCQs covering 2.4k healthcare topics and 21 medical subjects are collected with an average token length of 12.77 and high topical diversity. Each sample contains a question, correct answer(s), and other options which requires a deeper language understanding as it tests the 10+ reasoning abilities of a model across a wide range of medical subjects \& topics. A detailed explanation of the solution, along with the above information, is provided in this study.},
	booktitle = {Proceedings of the {Conference} on {Health}, {Inference}, and {Learning}},
	publisher = {PMLR},
	author = {Pal, Ankit and Umapathi, Logesh Kumar and Sankarasubbu, Malaikannan},
	editor = {Flores, Gerardo and Chen, George H and Pollard, Tom and Ho, Joyce C and Naumann, Tristan},
	month = apr,
	year = {2022},
	pages = {248--260},
}

@article{jin_what_2020,
	title = {What {Disease} does this {Patient} {Have}? {A} {Large}-scale {Open} {Domain} {Question} {Answering} {Dataset} from {Medical} {Exams}},
	journal = {arXiv preprint arXiv:2009.13081},
	author = {Jin, Di and Pan, Eileen and Oufattole, Nassim and Weng, Wei-Hung and Fang, Hanyi and Szolovits, Peter},
	year = {2020},
}

@inproceedings{jin_pubmedqa_2019,
	title = {{PubMedQA}: {A} {Dataset} for {Biomedical} {Research} {Question} {Answering}},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	author = {Jin, Qiao and Dhingra, Bhuwan and Liu, Zhengping and Cohen, William and Lu, Xinghua},
	year = {2019},
	pages = {2567--2577},
}

@article{mcclure_supermarket_2023,
	chapter = {World news},
	title = {Supermarket {AI} meal planner app suggests recipe that would create chlorine gas},
	issn = {0261-3077},

	abstract = {Pak ‘n’ Save’s Savey Meal-bot cheerfully created unappealing recipes when customers experimented with non-grocery household items},
	language = {en-GB},
	urldate = {2024-09-15},
	journal = {The Guardian},
	author = {McClure, Tess},
	month = aug,
	year = {2023},
	keywords = {Artificial intelligence (AI), ChatGPT, New Zealand, Technology},
}

@article{neyman_ix_1933,
	title = {{IX}. {On} the problem of the most efficient tests of statistical hypotheses},
	volume = {231},
	journal = {Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character},
	author = {Neyman, Jerzy and Pearson, Egon Sharpe},
	year = {1933},
	pages = {289--337},
}

@inproceedings{ren_likelihood_2019,
	title = {Likelihood {Ratios} for {Out}-of-{Distribution} {Detection}},
	volume = {32},

	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ren, Jie and Liu, Peter J. and Fertig, Emily and Snoek, Jasper and Poplin, Ryan and Depristo, Mark and Dillon, Joshua and Lakshminarayanan, Balaji},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d' and Fox, E. and Garnett, R.},
	year = {2019},
}

@article{bishop_novelty_1994,
	title = {Novelty detection and neural network validation},
	volume = {141},
	number = {4},
	journal = {IEE Proceedings-Vision, Image and Signal Processing},
	author = {Bishop, Christopher M.},
	year = {1994},
	pages = {217--222},
}

@inproceedings{perez_red_2022,
	title = {Red {Teaming} {Language} {Models} with {Language} {Models}},

	booktitle = {Conference on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	author = {Perez, Ethan and Huang, Saffron and Song, Francis and Cai, Trevor and Ring, Roman and Aslanides, John and Glaese, Amelia and McAleese, Nat and Irving, Geoffrey},
	year = {2022},
}

@inproceedings{ebrahimi_hotflip_2018,
	address = {Melbourne, Australia},
	title = {{HotFlip}: {White}-{Box} {Adversarial} {Examples} for {Text} {Classification}},

	abstract = {We propose an efficient method to generate white-box adversarial examples to trick a character-level neural classifier. We find that only a few manipulations are needed to greatly decrease the accuracy. Our method relies on an atomic flip operation, which swaps one token for another, based on the gradients of the one-hot input vectors. Due to efficiency of our method, we can perform adversarial training which makes the model more robust to attacks at test time. With the use of a few semantics-preserving constraints, we demonstrate that HotFlip can be adapted to attack a word-level classifier as well.},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Ebrahimi, Javid and Rao, Anyi and Lowd, Daniel and Dou, Dejing},
	editor = {Gurevych, Iryna and Miyao, Yusuke},
	month = jul,
	year = {2018},
	pages = {31--36},
}

@article{pedregosa_scikit-learn_2011,
	title = {Scikit-learn: {Machine} {Learning} in {Python}},
	volume = {12},
	journal = {Journal of Machine Learning Research},
	author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
	year = {2011},
	pages = {2825--2830},
}

@article{akhtar_advances_2021,
	title = {Advances in {Adversarial} {Attacks} and {Defenses} in {Computer} {Vision}: {A} {Survey}},
	volume = {9},

	journal = {IEEE Access},
	author = {Akhtar, Naveed and Mian, Ajmal and Kardan, Navid and Shah, Mubarak},
	year = {2021},
	keywords = {Adversarial examples, Computational modeling, Computer vision, Data models, Deep learning, Perturbation methods, Predictive models, Training, adversarial defense, adversarial machine learning, black-box attack, deep learning, perturbation, white-box attack},
	pages = {155161--155196},
}

@inproceedings{renyi_measures_1961,
	title = {On {Measures} of {Entropy} and {Information}},
	volume = {1},

	booktitle = {Proceedings of the {Fourth} {Berkeley} {Symposium} on {Mathematical} {Statistics} and {Probability}, {Volume} 1: {Contributions} to the {Theory} of {Statistics}},
	publisher = {University of California Press},
	author = {Rényi, Alfréd},
	year = {1961},
}

@inproceedings{radford_language_2019,
	title = {Language {Models} are {Unsupervised} {Multitask} {Learners}},

	author = {Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	year = {2019},
}

@misc{castricato_suppressing_2024,
	title = {Suppressing {Pink} {Elephants} with {Direct} {Principle} {Feedback}},

	abstract = {Existing methods for controlling language models, such as RLHF and Constitutional AI, involve determining which LLM behaviors are desirable and training them into a language model. However, in many cases, it is desirable for LLMs to be controllable at inference time, so that they can be used in multiple contexts with diverse needs. We illustrate this with the Pink Elephant Problem: instructing an LLM to avoid discussing a certain entity (a ``Pink Elephant''), and instead discuss a preferred entity (``Grey Elephant''). We apply a novel simplification of Constitutional AI, Direct Principle Feedback, which skips the ranking of responses and uses DPO directly on critiques and revisions. Our results show that after DPF fine-tuning on our synthetic Pink Elephants dataset, our 13B fine-tuned LLaMA 2 model significantly outperforms Llama-2-13B-Chat and a prompted baseline, and performs as well as GPT-4 in on our curated test set assessing the Pink Elephant Problem.},
	author = {Castricato, Louis and Lile, Nathan and Anand, Suraj and Schoelkopf, Hailey and Verma, Siddharth and Biderman, Stella},
	year = {2024},
}

@article{qiu_adversarial_2022,
	title = {Adversarial attack and defense technologies in natural language processing: {A} survey},
	volume = {492},
	issn = {0925-2312},

	abstract = {Recently, the adversarial attack and defense technology has made remarkable achievements and has been widely applied in the computer vision field, promoting its rapid development in other fields, primarily the natural language processing domain. However, discrete semantic texts bring additional restrictions and challenges to successfully implementing adversarial attacks and defenses. This survey systematically summarizes the current progress of adversarial techniques in the natural language processing field. We first briefly introduce the textual adversarial example’s particularity, vectorization, and evaluation metrics. More importantly, we categorize textual adversarial attacks according to the combination of semantic granularity and example generation strategy. Next, we present commonly used datasets and adversarial attack applications in diverse natural language processing tasks. Besides, we classify defense strategies as passive and active methods considering both input data and victim models. Finally, we present several challenging issues and future research directions in this domain.},
	journal = {Neurocomputing},
	author = {Qiu, Shilin and Liu, Qihe and Zhou, Shijie and Huang, Wen},
	year = {2022},
	keywords = {Adversarial attack, Adversarial defense, Artificial intelligence, Natural language processing, Textual adversarial example},
	pages = {278--307},
}

@article{goyal_survey_2023,
	title = {A {Survey} of {Adversarial} {Defenses} and {Robustness} in {NLP}},
	volume = {55},
	issn = {0360-0300},

	abstract = {In the past few years, it has become increasingly evident that deep neural networks are not resilient enough to withstand adversarial perturbations in input data, leaving them vulnerable to attack. Various authors have proposed strong adversarial attacks for computer vision and Natural Language Processing (NLP) tasks. As a response, many defense mechanisms have also been proposed to prevent these networks from failing. The significance of defending neural networks against adversarial attacks lies in ensuring that the model’s predictions remain unchanged even if the input data is perturbed. Several methods for adversarial defense in NLP have been proposed, catering to different NLP tasks such as text classification, named entity recognition, and natural language inference. Some of these methods not only defend neural networks against adversarial attacks but also act as a regularization mechanism during training, saving the model from overfitting. This survey aims to review the various methods proposed for adversarial defenses in NLP over the past few years by introducing a novel taxonomy. The survey also highlights the fragility of advanced deep neural networks in NLP and the challenges involved in defending them.},
	number = {14s},
	journal = {ACM Comput. Surv.},
	author = {Goyal, Shreya and Doddapaneni, Sumanth and Khapra, Mitesh M. and Ravindran, Balaraman},
	month = jul,
	year = {2023},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Adversarial attacks, NLP, adversarial defenses, perturbations},
}

@misc{roth_token-modification_2024,
	title = {Token-{Modification} {Adversarial} {Attacks} for {Natural} {Language} {Processing}: {A} {Survey}},

	author = {Roth, Tom and Gao, Yansong and Abuadbba, Alsharif and Nepal, Surya and Liu, Wei},
	year = {2024},
	eprint={2103.00676},
	archivePrefix={arXiv},
}

@article{zhang_adversarial_2020,
	title = {Adversarial {Attacks} on {Deep}-learning {Models} in {Natural} {Language} {Processing}: {A} {Survey}},
	volume = {11},
	issn = {2157-6904},

	abstract = {With the development of high computational devices, deep neural networks (DNNs), in recent years, have gained significant popularity in many Artificial Intelligence (AI) applications. However, previous efforts have shown that DNNs are vulnerable to strategically modified samples, named adversarial examples. These samples are generated with some imperceptible perturbations, but can fool the DNNs to give false predictions. Inspired by the popularity of generating adversarial examples against DNNs in Computer Vision (CV), research efforts on attacking DNNs for Natural Language Processing (NLP) applications have emerged in recent years. However, the intrinsic difference between image (CV) and text (NLP) renders challenges to directly apply attacking methods in CV to NLP. Various methods are proposed addressing this difference and attack a wide range of NLP applications. In this article, we present a systematic survey on these works. We collect all related academic works since the first appearance in 2017. We then select, summarize, discuss, and analyze 40 representative works in a comprehensive way. To make the article self-contained, we cover preliminary knowledge of NLP and discuss related seminal works in computer vision. We conclude our survey with a discussion on open issues to bridge the gap between the existing progress and more robust adversarial attacks on NLP DNNs.},
	number = {3},
	journal = {ACM Trans. Intell. Syst. Technol.},
	author = {Zhang, Wei Emma and Sheng, Quan Z. and Alhazmi, Ahoud and Li, Chenliang},
	month = apr,
	year = {2020},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Deep neural networks, adversarial examples, natural language processing, textual data},
}

@inproceedings{nie_adversarial_2020,
	address = {Online},
	title = {Adversarial {NLI}: {A} {New} {Benchmark} for {Natural} {Language} {Understanding}},

	abstract = {We introduce a new large-scale NLI benchmark dataset, collected via an iterative, adversarial human-and-model-in-the-loop procedure. We show that training models on this new dataset leads to state-of-the-art performance on a variety of popular NLI benchmarks, while posing a more difficult challenge with its new test set. Our analysis sheds light on the shortcomings of current state-of-the-art models, and shows that non-expert annotators are successful at finding their weaknesses. The data collection method can be applied in a never-ending learning scenario, becoming a moving target for NLU, rather than a static benchmark that will quickly saturate.},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Nie, Yixin and Williams, Adina and Dinan, Emily and Bansal, Mohit and Weston, Jason and Kiela, Douwe},
	editor = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
	month = jul,
	year = {2020},
	pages = {4885--4901},
}

@inproceedings{kiela_dynabench_2021,
	address = {Online},
	title = {Dynabench: {Rethinking} {Benchmarking} in {NLP}},

	abstract = {We introduce Dynabench, an open-source platform for dynamic dataset creation and model benchmarking. Dynabench runs in a web browser and supports human-and-model-in-the-loop dataset creation: annotators seek to create examples that a target model will misclassify, but that another person will not. In this paper, we argue that Dynabench addresses a critical need in our community: contemporary models quickly achieve outstanding performance on benchmark tasks but nonetheless fail on simple challenge examples and falter in real-world scenarios. With Dynabench, dataset creation, model development, and model assessment can directly inform each other, leading to more robust and informative benchmarks. We report on four initial NLP tasks, illustrating these concepts and highlighting the promise of the platform, and address potential objections to dynamic benchmarking as a new standard for the field.},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Kiela, Douwe and Bartolo, Max and Nie, Yixin and Kaushik, Divyansh and Geiger, Atticus and Wu, Zhengxuan and Vidgen, Bertie and Prasad, Grusha and Singh, Amanpreet and Ringshia, Pratik and Ma, Zhiyi and Thrush, Tristan and Riedel, Sebastian and Waseem, Zeerak and Stenetorp, Pontus and Jia, Robin and Bansal, Mohit and Potts, Christopher and Williams, Adina},
	editor = {Toutanova, Kristina and Rumshisky, Anna and Zettlemoyer, Luke and Hakkani-Tur, Dilek and Beltagy, Iz and Bethard, Steven and Cotterell, Ryan and Chakraborty, Tanmoy and Zhou, Yichao},
	month = jun,
	year = {2021},
	pages = {4110--4124},
}

@article{shorten_text_2021,
	title = {Text {Data} {Augmentation} for {Deep} {Learning}},
	volume = {8},
	issn = {2196-1115},

	abstract = {Natural Language Processing (NLP) is one of the most captivating applications of Deep Learning. In this survey, we consider how the Data Augmentation training strategy can aid in its development. We begin with the major motifs of Data Augmentation summarized into strengthening local decision boundaries, brute force training, causality and counterfactual examples, and the distinction between meaning and form. We follow these motifs with a concrete list of augmentation frameworks that have been developed for text data. Deep Learning generally struggles with the measurement of generalization and characterization of overfitting. We highlight studies that cover how augmentations can construct test sets for generalization. NLP is at an early stage in applying Data Augmentation compared to Computer Vision. We highlight the key differences and promising ideas that have yet to be tested in NLP. For the sake of practical implementation, we describe tools that facilitate Data Augmentation such as the use of consistency regularization, controllers, and offline and online augmentation pipelines, to preview a few. Finally, we discuss interesting topics around Data Augmentation in NLP such as task-specific augmentations, the use of prior knowledge in self-supervised learning versus Data Augmentation, intersections with transfer and multi-task learning, and ideas for AI-GAs (AI-Generating Algorithms). We hope this paper inspires further research interest in Text Data Augmentation.},
	number = {1},
	journal = {Journal of Big Data},
	author = {Shorten, Connor and Khoshgoftaar, Taghi M. and Furht, Borko},
	month = jul,
	year = {2021},
	pages = {101},
}

@inproceedings{morris_second-order_2020,
	address = {Online},
	title = {Second-{Order} {NLP} {Adversarial} {Examples}},

	abstract = {Adversarial example generation methods in NLP rely on models like language models or sentence encoders to determine if potential adversarial examples are valid. In these methods, a valid adversarial example fools the model being attacked, and is determined to be semantically or syntactically valid by a second model. Research to date has counted all such examples as errors by the attacked model. We contend that these adversarial examples may not be flaws in the attacked model, but flaws in the model that determines validity. We term such invalid inputs second-order adversarial examples. We propose the constraint robustness curve, and associated metric ACCS, as tools for evaluating the robustness of a constraint to second-order adversarial examples. To generate this curve, we design an adversarial attack to run directly on the semantic similarity models. We test on two constraints, the Universal Sentence Encoder (USE) and BERTScore. Our findings indicate that such second-order examples exist, but are typically less common than first-order adversarial examples in state-of-the-art models. They also indicate that USE is effective as constraint on NLP adversarial examples, while BERTScore is nearly ineffectual. Code for running the experiments in this paper is available here.},
	booktitle = {Proceedings of the {Third} {BlackboxNLP} {Workshop} on {Analyzing} and {Interpreting} {Neural} {Networks} for {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Morris, John},
	editor = {Alishahi, Afra and Belinkov, Yonatan and Chrupała, Grzegorz and Hupkes, Dieuwke and Pinter, Yuval and Sajjad, Hassan},
	month = nov,
	year = {2020},
	pages = {228--237},
}

@misc{vyas_provable_2023,
	title = {On {Provable} {Copyright} {Protection} for {Generative} {Models}},
	author = {Vyas, Nikhil and Kakade, Sham and Barak, Boaz},
	year = {2023},
	eprint={2302.10870},
	archivePrefix={arXiv},
}

@inproceedings{alzantot_generating_2018,
	address = {Brussels, Belgium},
	title = {Generating {Natural} {Language} {Adversarial} {Examples}},

	abstract = {Deep neural networks (DNNs) are vulnerable to adversarial examples, perturbations to correctly classified examples which can cause the model to misclassify. In the image domain, these perturbations can often be made virtually indistinguishable to human perception, causing humans and state-of-the-art models to disagree. However, in the natural language domain, small perturbations are clearly perceptible, and the replacement of a single word can drastically alter the semantics of the document. Given these challenges, we use a black-box population-based optimization algorithm to generate semantically and syntactically similar adversarial examples that fool well-trained sentiment analysis and textual entailment models with success rates of 97\% and 70\%, respectively. We additionally demonstrate that 92.3\% of the successful sentiment analysis adversarial examples are classified to their original label by 20 human annotators, and that the examples are perceptibly quite similar. Finally, we discuss an attempt to use adversarial training as a defense, but fail to yield improvement, demonstrating the strength and diversity of our adversarial examples. We hope our findings encourage researchers to pursue improving the robustness of DNNs in the natural language domain.},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Alzantot, Moustafa and Sharma, Yash and Elgohary, Ahmed and Ho, Bo-Jhang and Srivastava, Mani and Chang, Kai-Wei},
	editor = {Riloff, Ellen and Chiang, David and Hockenmaier, Julia and Tsujii, Jun'ichi},
	month = oct,
	year = {2018},
	pages = {2890--2896},
}

@inproceedings{dong_how_2021,
	title = {How {Should} {Pre}-{Trained} {Language} {Models} {Be} {Fine}-{Tuned} {Towards} {Adversarial} {Robustness}?},
	volume = {34},

	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Dong, Xinshuai and Luu, Anh Tuan and Lin, Min and Yan, Shuicheng and Zhang, Hanwang},
	editor = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P. S. and Vaughan, J. Wortman},
	year = {2021},
	pages = {4356--4369},
}

@inproceedings{jia_certified_2019,
	address = {Hong Kong, China},
	title = {Certified {Robustness} to {Adversarial} {Word} {Substitutions}},

	abstract = {State-of-the-art NLP models can often be fooled by adversaries that apply seemingly innocuous label-preserving transformations (e.g., paraphrasing) to input text. The number of possible transformations scales exponentially with text length, so data augmentation cannot cover all transformations of an input. This paper considers one exponentially large family of label-preserving transformations, in which every word in the input can be replaced with a similar word. We train the first models that are provably robust to all word substitutions in this family. Our training procedure uses Interval Bound Propagation (IBP) to minimize an upper bound on the worst-case loss that any combination of word substitutions can induce. To evaluate models' robustness to these transformations, we measure accuracy on adversarially chosen word substitutions applied to test examples. Our IBP-trained models attain 75\% adversarial accuracy on both sentiment analysis on IMDB and natural language inference on SNLI; in comparison, on IMDB, models trained normally and ones trained with data augmentation achieve adversarial accuracy of only 12\% and 41\%, respectively.},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Jia, Robin and Raghunathan, Aditi and Göksel, Kerem and Liang, Percy},
	editor = {Inui, Kentaro and Jiang, Jing and Ng, Vincent and Wan, Xiaojun},
	month = nov,
	year = {2019},
	pages = {4129--4142},
}

@inproceedings{zhao_certified_2022,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Certified {Robustness} {Against} {Natural} {Language} {Attacks} by {Causal} {Intervention}},
	volume = {162},

	abstract = {Deep learning models have achieved great success in many fields, yet they are vulnerable to adversarial examples. This paper follows a causal perspective to look into the adversarial vulnerability and proposes Causal Intervention by Semantic Smoothing (CISS), a novel framework towards robustness against natural language attacks. Instead of merely fitting observational data, CISS learns causal effects p(y{\textbar}do(x)) by smoothing in the latent semantic space to make robust predictions, which scales to deep architectures and avoids tedious construction of noise customized for specific attacks. CISS is provably robust against word substitution attacks, as well as empirically robust even when perturbations are strengthened by unknown attack algorithms. For example, on YELP, CISS surpasses the runner-up by 6.8\% in terms of certified robustness against word substitutions, and achieves 80.7\% empirical robustness when syntactic attacks are integrated.},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Zhao, Haiteng and Ma, Chang and Dong, Xinshuai and Luu, Anh Tuan and Deng, Zhi-Hong and Zhang, Hanwang},
	editor = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	month = jul,
	year = {2022},
	pages = {26958--26970},
}

@phdthesis{la_malfa_robustness_2023,
	type = {{PhD} {Thesis}},
	title = {On robustness for natural language processing},
	school = {University of Oxford},
	author = {La Malfa, E},
	year = {2023},
}
