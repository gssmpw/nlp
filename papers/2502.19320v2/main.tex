\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{soul}
\usepackage{pdflscape}
\usepackage{array}
\usepackage{tabularx}
\usepackage{lscape}
\usepackage{hyperref}
\usepackage{tcolorbox}
\usepackage{multirow}
\usepackage{booktabs}
% \usepackage{bibentry}
\usepackage{array}    % For more options on column formatting
\usepackage{soul}
\usepackage{xcolor}
% \usepackage{longtable}
\usepackage{thmtools}
% \usepackage{thmbox}
\usepackage{thm-restate}
\usepackage{caption}
\usepackage{enumitem}
\usepackage{subcaption}
\usepackage{wrapfig}  % for floating the table
\usepackage{tablefootnote}
% \usepackage{amsthm}


% styling changes
\captionsetup{font=small}
\linespread{0.98}


\declaretheorem[name=Theorem]{thm}
\declaretheorem[name=Lemma]{lem}
\declaretheorem[name=Definition]{defn}
\newenvironment{proof}{\emph{Proof:}}{\hfill$\square$}
\declaretheorem[name=Remark]{remark}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%% When accepted uncomment below. %%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\iclrfinalcopy

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%% FOR CUSTOM COMMANDS SEE custom_commands.tex %%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{custom_commands.tex}

% Mark sections of captions for referring to divisions of figures

\title{\fontsize{16.1}{16.1}\selectfont \spaceskip=0.17em Shh, don't say that! Domain Certification in LLMs}


% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{\textbf{Cornelius Emde$^1$}\thanks{Corresponding Author \texttt{cornelius.emde@cs.ox.ac.uk}. Work partially done while interning at King Abdullah University of Science and Technology (KAUST).} \textbf{, Alasdair Paren$^1$, Preetham Arvind$^1$, Maxime Kayser$^1$, Tom Rainforth$^1$,}\\
\textbf{Thomas Lukasiewicz$^{2,1}$, Bernard Ghanem$^3$, Philip H.S. Torr$^1$, Adel Bibi$^1$} \\
$^1$University of Oxford $\quad^2$Vienna University of Technology $\quad^3$KAUST
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Large language models (LLMs) are often deployed to perform constrained tasks, with narrow domains. For example, customer support bots can be built on top of LLMs, relying on their broad language understanding and capabilities to enhance performance. However, these LLMs are adversarially susceptible, potentially generating outputs outside the intended domain. To formalize, assess, and mitigate this risk, we introduce \emph{domain certification}; a guarantee that accurately characterizes the out-of-domain behavior of language models. We then propose a simple yet effective approach, which we call \ouralg that provides adversarial bounds as a certificate. Finally, we evaluate our method across a diverse set of datasets, demonstrating that it yields meaningful certificates, which bound the probability of out-of-domain samples tightly with minimum penalty to refusal behavior.
\end{abstract}

\section{Introduction}\label{sec:introduction}

% 0.48 width for Rieman Figure
\begin{wrapfigure}{r}{0.38\textwidth}
    \vspace{-18pt}
  \begin{center}
    \includegraphics[width=\linewidth]{assets/motivating_figure.pdf}
  \end{center}
  \vspace{-8pt}
  \caption{\!A user misappropriating an LLM system using an adversarial attack. We provide certificates to mitigate this risk.}
  \label{fig:motivating-figure}
  \vspace{-12pt}
\end{wrapfigure}


With recent advancements in the field of natural language processing, large language models (LLMs) have become ubiquitous. In particular, the scaling of recent large generalist models dubbed foundation models has shown to enable emergent abilities that benefit a wide range of downstream tasks such as text generation, question answering, and text comprehension \citep{kaplan_scaling_2020, alabdulmohsin_revisiting_2022, xiong_temporal_2024, henighan_scaling_2020, brown_language_2020}.
Adapting these foundation models for downstream tasks often leads to state-of-the-art performance and has become the dominant paradigm~\citep{gao_making_2021}. This is typically achieved via fine-tuning on task-relevant data (e.g., low-rank adaptation (LoRA)~\cite{hu_lora_2022}, in-context learning~\citep{mosbach_few-shot_2023}, prefix turning~\cite{li_prefix-tuning_2021}, or simply prompt engineering).

However, foundation models are typically trained on large amounts of web data which contains a wide range of information that is either irrelevant to a task or potentially harmful \citep{bommasani_opportunities_2022}.
Therefore, it is desirable to restrict the output of a generalist LLM to a specific domain.
For example, consider a healthcare provider such as the National Health Services (NHS) providing a general purpose chatbot to support their citizens with simple health questions, as shown in Figure~\ref{fig:motivating-figure}. It would be important, for public reputation and cost reasons, that such a system would remain on topic and could not be misused, either intentionally or unintentionally. Misappropriating models is easily possible.

In order to prevent intentional misuse, we consider an adversary trying to elicit an unintended (from the deployer's perspective) response from the model. We assume the deployer wants an LLM to only respond with a certain set of topics, and thus a successful attack is an input string that creates a coherent response outside the target domain. There are various reasons why an adversary might want to elicit such a response that is out-of-domain (OOD). The adversarial user might want to misappropriate the system as a cost-effective tool for a purpose it wasn't built for, resulting in excess infrastructure costs for the deployer.
Conversely, the deployer might legally be required to validate and verify their models, which is challenging, if not impossible, when the model is not domain-restricted. Finally, the adversary might want to harm the company directly by eliciting harmful OOD responses, which could
damage the company's reputation when publicized. Recently, an LLM-driven meal planning tool has received wide media attention for providing toxic recipes when prompted with toxic ingredients \citep{mcclure_supermarket_2023, the_guardian_paknsave_2023}. Deployers have moral and legal obligations to prevent this \citep{bommasani_opportunities_2022}. In all examples, restricting the domain in which the model responds \textit{under adversarial} prompts can help mitigate risks. Thus, in the era of foundation models, ``domain'' specialization is critical.

Existing work has implemented guardrails that address these risks~\citep{jain_baseline_2023}, most notably via alignment, resulting in models rejecting user requests~\citep{bai_training_2022, ouyang_training_2022, christiano_deep_2017}.
However, a wide body of research has shown that common guardrails have ``jailbreaks'', i.e., they can easily be circumvented by an adversary \citep{wang_backdooralign_2024, qi_fine-tuning_2024, eiras_mimicking_2024, carlini_are_2023, dong_safeguarding_2024}. Common jailbreak methods are prompt injection \citep{perez_ignore_2022, jiang_prompt_2023, liu_autodan_2024}, numerical optimization \citep{jia_adversarial_2017, wallace_universal_2019, ebrahimi_hotflip_2018, jones_automatically_2023, zou_universal_2023, jia_improved_2025}, red teaming \citep{perez_red_2022, samvelyan_rainbow_2024}, automated black-box attacks \citep{chao_jailbreaking_2023, mehrotra_tree_2024}, or data poisoning attacks \citep{biggio_poisoning_2012, wallace_concealed_2021, carlini_poisoning_2024}. Using these tools, it is possible for adversaries to retrieve information from a fine-tuned model that was suppressed by the alignment and generate responses that are outside the target domain (see Figure~\ref{fig:motivating-figure} for an example). Adversarial prefixes or suffixes that augment any prompt are especially powerful, as they have been shown to universally attack models in combination with a wide range of prompts and can thus be shared between adversarial users \citep{wallace_universal_2019, zou_universal_2023}. This presents a significant risk. Hence, researchers have proposed methods to defend against these adversarial attacks, such as unlearning \citep{nguyen_survey_2022, xu_machine_2023}, robust fine-tuning \citep{oneill_adversarial_2023, dong_how_2021}, or request and response filtering \citep{inan_llama_2023}.

Deployers would ideally want guardrails that come with a provable, mathematical guarantee against the model responding off-topic, or a guarantee that it does this with very low probability. The process of constructing guarantees against certain model behaviors under adversarial attack is commonly referred to as \emph{certification} and has been successfully applied to vision applications in recent years \citep{akhtar_advances_2021} and proposed for NLP applications \citep{la_malfa_robustness_2023, casadio_nlp_2024, kumar_certifying_2024}.
However, no existing LLM guardrails provide guaranteed protection against existing or future jailbreaking techniques, leaving deployed models at risk of being compromised shortly after release. As a result, developing certifiable methods to guarantee that specialized LLMs consistently produce on-topic content is critical.
Hence, our contributions are as follows:

\vspace{-2pt}
\begin{itemize}[noitemsep, left=0pt]
    \item We introduce a novel framework, \emph{domain certification}, to bound the probability of models producing out-of-domain content under adversarial attack.
    \item We introduce an easy-to-use algorithm \ouralg that bounds the probability of an LLM based system responding off topic under adversarial attack. We show the efficiency of \ouralg  which we test empirically on a number of representative data sets.
\end{itemize}

\section{Domain Certification}
We now introduce our \emph{domain-certification} framework for offering mathematical guarantees that an LLM system stays on topic. In Section~\ref{sec:definining-domain-certification}, we formally introduce this framework. In Section~\ref{sec:rejection_sampling}, we present \ouralgnamelong (\ouralg). \ouralg is an easy-to-use method to create a system that adheres to these guarantees. In plain language, we propose a certifiable guardrail for LLM-driven systems as follows:

\begin{center}
\parbox{0.9\textwidth}{\centering
 \textit{A model is \emph{domain-certified}, when an adversarial upper bound can be placed on the probability that the model provides an output outside its designated target domain.}
}
\end{center}

Before formalizing this statement, we introduce some mathematical notation. We represent tokens (i.e. individual text units) as $x$ and $y$, which belong to the token space $x,y\in \sV$ where $\sV = \{1,\dots,V\}$ is the vocabulary of size $V$. We define the space of sequences of arbitrary length as $\sS \triangleq \sV^*$, the Kleene closure of $\sV$.
Sequences of tokens are denoted by bold letters, $\vx, \vy \in \sS$, with $\vx$ and $\vy$ representing the input and output sequences of an LLM respectively. We use lowercase letters to denote models that predict the next token, such as \( l: \sS \rightarrow \sV \). Applying this model repeatedly, until the end-of-sequence token creates a sequence-to-sequence model \( L: \sS \rightarrow \sS \). We denote the likelihood of sample $\vy$ under $L$ given $\vx$ as $L(\vy|\vx)$, which is obtained by $L(\vy|\vx)=\prod_{n=1}^{N_y} l(y_{n}|y_{<n},\vx)$ for a sentence $\vy$ of length $N_y$. We further denote the distribution from which the model samples its output by $\vy \sim L(\cdot|\vx)$.

\subsection{Defining Domain Certification}\label{sec:definining-domain-certification}

We now formally introduce \emph{domain certification}. We define the target domain (set of desired topics) as a subset of the sentence space $\sS$ and partition $\mathbb{S}$ into the target domain $\sT$ and its complement $\sT'$. For instance, $\sT$ might be all sentences meaningfully occurring for ``question answering for health problems''. In addition, we define the set of unwanted responses as $\sF \subset \sT'$ ($\sF$ as ``forbidden'') and will certify with respect to this set $\sF$ rather than $\sT$. Sequences posing some risk should be included in $\sF$, while $\sF' \cap \sT'$ should contain benign out-of-domain samples, such as unintelligible or meaningless sequences of tokens (see Appendix~\ref{app:choosing-F-guidance} for a discussion). Hence, we wish to establish a guarantee that $L$ is unlikely to produce an output in $\sF$. As a step towards such a guarantee, we first define a bound for any given element $\vy$ in $\sS$:

\begin{defn} \label{def:ac} \textbf{Atomic Certificate}. We say a model $L: \sS \rightarrow \sS$ is $\epsilon_{\vy}$-atomic-certified ($\epsilon_{\vy}$-AC) for some sample $\vy$ (i.e. an \emph{atom}) in the output set $\sS$, iff
\begin{equation}\label{eq:ac-definition}
    \forall \vx \in \sS: L(\vy|\vx)\leq \epsilon_{\vy}.
\end{equation}
\end{defn}
In words, a model that is $\epsilon_{\vy}$-AC for a sample $\vy$, will generate sample $\vy$ with probability smaller than $\epsilon_{\vy}$ for \emph{any} $\vx \in \sS$, and hence for adversarially chosen $\vx$. If this is the case, we say model $L$ is \emph{certifiable} for sample $\vy$ with $\epsilon_{\vy}$, i.e. $\epsilon_{\vy}$ is the \emph{smallest} value that provably bounds $L$. Ideally, such an upper bound $\epsilon_{\vy}$ would be large for samples in the target domain $\sT$, meaning the certificate is \emph{permissive}, and small for samples drawn from $\sF$ meaning the certificate is \emph{restrictive}, i.e. \emph{tight}.

The atomic certificate implies an upper bound $\epsilon_{\sF}$ for $\sP_{\vy \sim L(\cdot|\vx)}(\vy \in \sF|\vx)$, which would be constructed by summing \eqref{eq:ac-definition} over all $\vy \in \sF$ for a given $\vx$. Concretely,
$\sP_{\vy \sim L(\cdot|\vx)}(\vy \in \sF|\vx)=\sum_{\vy \in \sF} L(\vy|\vx) \leq \sum_{\vy \in \sF} \epsilon_{\vy}=\epsilon_{\sF}$.
However, practically this bound is intractable due to $\sF$'s exponential size in $N_y$, and the difficulty in constructing a precise description of the set $\mathbb{F}$. Instead of giving a bound over returning $\vy \in \sF$, we look at the worst case across $\sF$ which can more precisely be estimated from a finite sample of $\sF$:

\begin{defn} \label{def:dc} \textbf{Domain Certificate}.
We say model $L$ is $\epsilon$-domain-certified ($\epsilon$-DC) with respect to $\sF$, when it is $\epsilon_{\vy}$-AC for all $\vy \in \sF$ with $\epsilon_{\vy}\leq \epsilon$:
\begin{equation} \label{eq:dc-definition}
    \forall \vx \in \sS, \vy \in \sF: L(\vy | \vx) \leq \epsilon.
\end{equation}
\end{defn}
This imposes a global bound on $L$ across all undesired responses in $\sF$. In practice, we cannot establish the $\epsilon$-DC certificate w.r.t. $\sF$ as we cannot enumerate $\sF$. Hence, following standard practice in ML evaluation, we propose to use $\gD_{\sF}$, a finite dataset of out-of-domain responses to establish a $\epsilon$-DC certificate w.r.t. $\gD_{\sF}$ approximating the certificate for $\sF$.

Recent discussions have raised the need for bounds on undesirable behavior. For instance, \citet{bengio_bounding_2024} advocates for upper bounds on harmful behavior \citep{bengio_can_2024}. In addition, a growing body of legislation mandates thorough auditing of ML systems \citep{eu_regulation_2024}.
The atomic and domain certificates can play a vital role in assessing the risk of worst-case behavior. For example, consider the deployer of an LLM-based system that processes 10 requests per second. The deployer might perform an apriori risk assessment and determine that they can tolerate the consequences of an out-of-domain response from a set $\gD_{\sF}$ sampled once per year. The deployer should certify the LLM system as $\epsilon$-DC with $\epsilon \approx 10^{-9}$ in order to achieve this level of risk.

\paragraph{Certification through Divergences.} We provide an alternative view to this problem,  generalizing it to bounding divergences between the model and the distribution of sentences in the domain $\sT$. We then use this view to operationalize the $\epsilon_{\vy}-AC$  and $\epsilon-DC$ (Definitions~\ref{def:ac} and \ref{def:dc}) inspired by \citet{vyas_provable_2023}'s work on preventing copy-right violations. To this end, we define an oracle $\Omega$ that is a \emph{generator} for domain $\sT$: $\Omega$ assigns high likelihood to sentences in $\sT$ and zero likelihood to elements in $\sF$. Hence, sampling from $\Omega$ will yield in-domain responses. We establish and bound the divergence between $L$ and $\Omega$ to restrict the model domain. In particular, we use the Renyi divergence of order infinity, $\dd[\infty]{P}{Q} \triangleq \log \sup_x\frac{P(x)}{Q(x)}$ \citep{renyi_measures_1961}.
Hence, our objective is:
\begin{equation}\label{eq:divergence}
    \forall \vx \in \sS: \dd[\infty]{L(\vy|\vx)}{\Omega(\vy)} \leq k.
\end{equation}
Bounding this divergence is at the core of what we are aiming to achieve: The divergence is large when $L$ assigns high likelihood to a sample $\vy$ while $\Omega$ does not. That means $L$ is likely to produce samples that are out-of-domain. When $\Omega$ assigns high likelihood to $\vy$, the sample is in the target domain, and hence the divergence in \eqref{eq:divergence} is not restrictive. When $L$ assigns low likelihood, $\vy$ is unlikely to be sampled. Interestingly, this divergence implies \eqref{eq:ac-definition} and \eqref{eq:dc-definition}, see Lemma~\ref{equivalence}.

As the oracle is not available in practice we approximate $\Omega$ with a ``guide'' language model that is exclusively trained on in-domain data dubbed $G$ (i.e. the guide model). We use $G(\vy)$ to replace $\Omega(\vy)$ to assess the \emph{marginal} likelihood of $\vy$. While this means that $G(\vy)$ loses some context contained in $\vx$, this has a major advantage: $G(\vy)$ does not depend on $\vx$, which is a potential adversary and hence, by design is robust to adversarial prompts.

\begin{wrapfigure}{r}{0.40\textwidth}
\begin{minipage}{0.40\textwidth}
\vspace{-23pt}
\begin{algorithm}[H]
\caption{\ouralg }\label{alg:cap}
\begin{algorithmic}
\Require LLM $L$, Guide model $G$, hyperparameters $k$ and $T$, prompt $\vx$
\For{$t \in \{1,\dots,T\}$}
\State Sample $\vy \sim L(\cdot|\vx)$
\State $N_{\vy} \leftarrow$ length($\vy$)
\If{$\log \frac{L(\vy|\vx)}{G(\vy)} \leq kN_{\vy} $}
    \State \textbf{Return:} $\vy$
\EndIf
\EndFor
\State \textbf{Return:} ``Abstained''.
\end{algorithmic}
\end{algorithm}
\end{minipage}
\vspace{-10pt}
\end{wrapfigure}


\subsection{Achieving Domain Certification}\label{sec:rejection_sampling}

In this section, we introduce \ouralgnamelong (\ouralg) to obtain atomic certification as described in Definition~\ref{def:ac}. We utilize a general model $L$ and a domain generator $G$ as described above and obtain a meta-model $M$ for which the guarantee holds with respect to the domain generator $G$. In particular, we perform rejection sampling as described in Algorithm~\ref{alg:cap} (inspired by \citet{vyas_provable_2023}): The capable general model $L$ proposes a sample $\vy$ and we accept, if the length normalized log-ratio between $L$ and $G$ is bounded by hyperparameter $k$. We repeat up to $T$ times until a sample is accepted. If all samples are rejected, the model dismisses the request. This defines a new model $M$, for which the following theorem establishes the certificate:

\begin{restatable}[\ouralg Certificate]{thm}{rejectionbound}\label{thm:valid}
Let $L$ be an LLM and $G$ a guide model as described above. Rejection sampling as described in Algorithm~\ref{alg:cap} with rejection threshold $k$ and up to $T$ iterations defines model $M_{L,G,k,T}$ with $M_{L,G,k,T}(\vy|\vx)$ denoting the likelihood of $\vy$ given $\vx$. Let $N_{\vy}$ be the length of $\vy$. We state the adversarial bound:
\begin{equation}\label{eq:upper_bound}
    \forall \vx \in \sS: M_{L,G,k,T}(\vy|\vx) \leq 2^{kN_{\vy}} \cdot T \cdot G(\vy).
\end{equation}
Hence, $M_{L,G,k,T}$ is $[2^{kN_{\vy}}TG(\vy)]$-AC and, further, it is $[\max_{\vy \in \sF} 2^{kN_{\vy}}T G(\vy)]$-DC w.r.t. $\sF$.
\end{restatable}
When context allows, we may abbreviate $M_{L,G,k,T}$ to $M$, omitting subscripts for brevity. This certificate with respect to $G$ can be useful: As $G$ is only trained on samples in $\gD_{\sT} \subset \sT$, a dataset of domain $\sT$, it assigns exponentially decreasing likelihood to samples that are in $\sF$.\footnote{We give an empirical example of this behavior in Figure \ref{fig:app-medqa-log-likelihoods} in Appendix \ref{app:medqa-results}.} In particular, this is useful iff the log upper bound $kN_{\vy} + \log T + \log G(\vy)$ ($\log$ RHS of \eqref{eq:upper_bound}) is small in comparison to $\max_{x\in \sS} \log L(\vy|\vx)$: Our certificate can provide an upper bound to the adversarial behavior of $M$ that is favorable over $L$.

As mentioned, this problem is closely related to OOD detection, for which the likelihood ratio test is commonly used as a powerful statistic \citep{neyman_ix_1933, bishop_novelty_1994, ren_likelihood_2019, li_contrastive_2023, zhang_your_2024, rafailov_direct_2024}. In OOD detection, rejection threshold $k$ is commonly chosen to balance false negative rates and false positive rates. Here, $k$ also influences the upper bound on the certificate, indicating that there can be a \emph{trade-off} between correctly classifying samples as ID or OOD, and achieving a desired level of certification.

\textbf{Length Normalization. } Algorithm~\ref{alg:cap} performs length normalized rejection-sampling as unnormalized log likelihood ratios scale unfavorably in $N_{\vy}$, the length of sequence $\vy$ which we now demonstrate. Consider the next-token models $l$ and $g$ underlying the sequence-to-sequence models $L$ and $G$. As $\vy$ is sampled from $L$, we expect each token $y_1,\dots, y_{N_{\vy}}$ to have high likelihood under $l$. If we assume that $l$ places $c$ times more probability mass per token than $g$, then we can show that the log likelihood ratio grows linearly in $N_{\vy}$, the length of sequence $\vy$: $\log L(\vy|\vx) / G(\vy)=\log \prod_{n=1}^{N_{\vy}} cg(y_{n}|y_{<n})/g(y_{n}|y_{<n})=N_{\vy} \log c$. We illustrate an example in Figure~\ref{fig:length-norm-example}: Assume that an in-domain sample $\vy$ for which model $L$ and generator $G$ assign constant likelihood per token of $0.1$ and $0.05$, respectively, i.e. $\forall n=1,..,N_{\vy}:$ $l(y_n|y_{<n},\vx)=0.1$ and $g(y_n|y_{<n},\vx)=0.05$. Further, assume an out-of-domain $\vy'$ for which $l$ assigns a mass of $0.1$ per token and $g$ assigns $0.01$. The log likelihood ratio for $\vy$ can be expressed as $N_{\vy} \log 2$ and for $\vy'$ as $N_{\vy'} \log 10$. As in- and out-of-domain ratios grow with length, so does the optimal decision bound. We plot sequences of varying lengths with these parameters in Figure~\ref{fig:length-norm-example}. By arithmetic manipulation, rejection sampling with threshold $kN_{\vy}$ is equivalent to bounding the ratio of geometrically normalized likelihoods $\log L(\vy|\vx)^{1/N_{\vy}}/G(\vy)^{1/N_{\vy}}$ using a constant threshold $k$. Hence, we propose to use normalized log ratios in Algorithm~\ref{alg:cap} over unnormalized likelihood ratios. Similar approaches have been discussed in the NLP literature \citep{geng_grammar-constrained_2023}.


\begin{wrapfigure}{r}{0.40\textwidth}
\begin{minipage}{0.40\textwidth}
    \vspace{-12pt}
    \centering
    \includegraphics[width=\linewidth]{assets/length_normalization_motivation_log_ratio.pdf}
    \vspace{-18pt}
    \caption{Log likelihood ratios scale in the sequence length $N_{\vy}$. Six artificial examples of sentences with length 1 to 10 are shown for the ID and OOD dataset. As log ratios scale, so should the decision boundary.}
    \label{fig:length-norm-example}
\vspace{-10pt}
\end{minipage}
\end{wrapfigure}

Despite the length normalization of the rejection threshold, notice that the \ouralg bound depends on $N_{\vy}$, the length of sequence $\vy$ (see \eqref{eq:upper_bound}), making the certificate more effective for shorter or longer sequences. Let $\bar{g}(\vy)$ be the geometric mean of per-token probability for $G(\vy)$. The log upper bound can be written as $kN_{\vy} + N_{\vy}\log \bar{g}(\vy) + \log T$. Whether this is tighter for short or long sequences is governed by $k$ and $\log \bar{g}(\vy)$. When $k+\log \bar{g}(\vy)$ is close to 0, the bound is balanced, and when $k + \log \bar{g}(\vy) < 0$, the bound decreases as $N_{\vy}$ increases.

In the appendices, we provide further insights into \ouralg. In particular, in Appendix~\ref{app:proofs} we provide Lemma~\ref{lemma:likelihood-m} showing how to estimate the likelihood of $M$. In Lemma~\ref{lemma:expected-number-of-trials}, we provide an analysis of the expected number of iterations of \ouralg. In Appendix~\ref{app:attack-m}, we provide further intuition on how rejection sampling can achieve an adversarial bound. Finally, in Lemma~\ref{lemma:adversary-m} we show an adversary for $M$ and discuss how rejection sampling encumbers adversarial attacks on $M$.

\vspace{-3pt}
\section{Experiments}
\vspace{-3pt}
We empirically test our method proposed in Section~\ref{sec:rejection_sampling} across 3 domains: Shakespeare, Computer Science News, and MedicalQA. After describing the experimental setup in Section~\ref{sec:experimental-setup}, we examine the rejection behavior of our method by examining the $\log L(\vy|\vx)/G(\vy)$ ratio and associated certificates under a finite set of ground-truth test samples from $\sT$ and $\sF$ in Section~\ref{sec:results-log-ratio}. In Section~\ref{sec:results-generation}, we repeat this analysis by applying our Algorithm~\ref{alg:cap}. Finally, we demonstrate how to evaluate a certified model on standardized benchmarks in Section~\ref{sec:results-benchmarks}.

\vspace{-3pt}
\subsection{Experimental Setup}\label{sec:experimental-setup}
\vspace{-3pt}
In this section, we provide a brief description of our experimental setup for three applications. Each experimental setup consists of a target domain $\sT$, a finite dataset of in-domain samples $\gD_{\sT} \subset \sT$, models $L$ and $G$, and an out-of-domain dataset $\gD_{\sF} \subset \sF$, against which we test our methods (see Appendix~\ref{app:experimental-setup} for more details on data and models).

\textbf{Shakespeare.} Our target domain $\sT$ is Shakespeare's plays.
We fine-tune a Gemma-2-2b \citep{team_gemma_2024} as model $L$ and train a GPT-2 architecture (33.7M parameters, \cite{radford_language_2019}) from scratch for $G$ on TinyShakespeare (TS) \citep{karpathy_unreasonable_2015}. We use TS's test split as in-domain dataset, $\gD_{\sT}$, and following previous literature \citep{zhang_your_2024} compose $\gD_{\sF}$ of IMDB \citep{maas_learning_2011}, RTE \citep{wang_glue_2019} and SST2 \citep{minaee_large_2024}, adding an old Bible dataset \citep{reis_bible_2019} as it is linguistically close to TinyShakespeare. At testing, we consider 256-token long sequences and use the first 128 tokens as prompt.

\textbf{Computer Science News.} Our target domain $\sT$ is news about computer science.
We fine-tune a Gemma-2-2b as model $L$ and train a GPT-2 architecture (109.3M parameters) from scratch for $G$ on articles from the computer science categories in the 20NG dataset \citep{prieditis_newsweeder_1995}. We use computer science articles from 20NG's test split as target domain $\gD_{\sT}$ and the remaining categories as $\gD_{\sF}$ together with the OOD dataset used for Shakespeare. At testing, we consider 256 token long sentences and use the first 128 tokens as prompt.

\textbf{Medical QA. \ } We apply our method to medical question answering as target domain $\sT$. This could, for example, be extended to a chatbot for clinicians to look up patient symptoms. We use a LLama-3-8B model \citep{aimeta_llama_2024} as $L$ and for guide model $G$ we pre-train a GPT-2 architecture model from scratch (184M parameters) on PubMedQA \citep{jin_pubmedqa_2019}, which contains approximately 200K QA pairs for training and 1000 test pairs. We further fine-tune $G$ on responses from $L$ to questions in PubMedQA. We use the PuMedQA test set as in-domain dataset $\gD_{\sT}$ and regard question answering on other topics, such as geography, as $\sF$. To model this, we use the Stanford Question and Answering Dataset (SQuAD; excluding medical categories; \cite{rajpurkar_squad_2016}) as $\gD_{\sF}$.

\begin{figure}[b]
    \vspace{-10pt}
    \centering
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/medqa/likelihood_ratio_hist_MedicalQA.pdf}
        \tightcaption
        \subcaption{}\label{fig:medqa-log-ratio-histogram}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/medqa/constriction_ratio_metrics_MedicalQA.pdf}
        \tightcaption
        \subcaption{}\label{fig:medqa-ood-vs-certification}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/medqa/epsilon_dc_metrics_MedicalQA.pdf}
        \tightcaption
        \subcaption{}\label{fig:medqa-acc-dc}
    \end{minipage}
    \vspace{-5pt}
    \caption{All Figures display MedicalQA. Figure \ref{fig:medqa-log-ratio-histogram} shows that log likelihood ratios are well disentangled. Figure \ref{fig:medqa-ood-vs-certification} shows the trade-off between OOD and certification: The best OOD detection performance occurs with a constriction ratio of 20. Figure \ref{fig:medqa-acc-dc} shows the false rejection rate (FRR) required to certify at a given $\epsilon$. All Figures display MedicalQA.}
    \label{fig:medqa-results-overall}
\end{figure}


\subsection{Likelihood Ratios on Ground Truth Samples}\label{sec:results-log-ratio}

In this section, we evaluate the capability of our method to attribute samples to the target domain and investigate whether it yields useful adversarial bounds. In particular, we study the length-normalized likelihood ratio $L(\vy|\vx)/G(\vy)$ on in- and out-of-domain samples. In Figure~\ref{fig:medqa-log-ratio-histogram}, we show that the log likelihood ratios for MedicalQA are disentangled and hence a threshold $k$ exists separating target domain and out-of-domain samples well. However, such $k$ --- while yielding strong OOD detection performance --- might not be associated with tight certificates. Hence, we will first study the $\epsilon_{\vy}$-AC certificates under $M$ for individual samples, $\vy$, before moving on to the domain certificate, $\epsilon$-DC.

\begin{figure}[t]
    \vspace{-10pt}
    \centering
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/tinyshakespeare/epsilon_ac_ecdf_frr_0.10_TinyShakespeare.pdf}
        \tightcaption
        \subcaption{}\label{fig:tinyshakespeare-ac-ecdf}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/20ng/epsilon_ac_ecdf_frr_0.10_20NG.pdf}
        \tightcaption
        \subcaption{}\label{fig:20ng-ac-ecdf}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/medqa/epsilon_ac_ecdf_frr_0.10_MedicalQA.pdf}
        \tightcaption
        \subcaption{}\label{fig:medqa-ac-ecdf}
    \end{minipage}%

    % \vspace{5pt}

    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/tinyshakespeare/cr_hist_frr_0.10_TinyShakespeare.pdf}
        \tightcaption
        \subcaption{}\label{fig:tinyshakespeare-cr-hist}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/20ng/cr_hist_frr_0.10_20NG.pdf}
        \tightcaption
        \subcaption{}\label{fig:20ng-cr-hist}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/medqa/cr_hist_frr_0.10_MedicalQA.pdf}
        \tightcaption
        \subcaption{}\label{fig:medqa-cr-hist}
    \end{minipage}%
    \vspace{-8pt}
    \caption{(a)-(c) show the estimated cumulative distribution function (eCDF) of $\epsilon_{\vy}$-ACs for each experimental setup. (d)-(f) show the histograms for the $\log_{10}$ constriction ratios. All results are obtained with hyperparameter $k$ chosen to ensure a 10\% false rejection rate (FRR) on in-domain samples.}
    \label{fig:combined-results}
    \vspace{-12pt}
\end{figure}

\textbf{Atomic Certificates. \ } We obtain $\epsilon_{\vy}$-ACs using \ouralg (Section~\ref{sec:rejection_sampling}), setting $k$ to achieve a 10\% false rejection rate (FRR) for in-domain samples. Figures~\ref{fig:combined-results} (a)-(c) show the distribution of $\epsilon_{\vy}$-ACs for the target domain dataset $\gD_{\sT}$ and the out-of-domain dataset $\gD_{\sF}$. We make similar observations for all three setups: First, the certificates in the OOD datasets $\gD_{\sF}$ are \emph{meaningfully tight}. We observe that 95\% of OOD samples have an $\epsilon_{\vy}$-AC of less than $1\times10^{-10}$ across all setups. Hence, the sampling probability for these OOD instances is provably smaller than $10^{-10}$ for any arbitrary prompt $\vx$. Second, we note that the certificates in $\gD_{\sF}$ are significantly tighter than those in $\gD_{\sT}$ as shown by the gap between the eCDFs. This is a significant finding as certificates should be \emph{constrictive} (i.e. small) on samples in $\sF$ preventing these from being sampled, while certificates should be \emph{permissive} (i.e. large) in $\sT$, not preventing in-domain responses from being sampled. Finally, we observe that the disentanglement of ACs is weaker for MedicalQA compared to the other setups (see Figure~\ref{fig:medqa-ac-ecdf}). As shown in Appendix~\ref{app:cr-len-controlled}, this is attributable to the short sequences in the OOD dataset and adjusting for this confounder significantly improves disentanglement.

To further study the atomic certificates on $M$, we compare them to a certificate on $L$ as a baseline. To this end, we define the \emph{constriction ratio} for each $\vy$, given by the ratio of the certifiable $\epsilon_{\vy}$ for $L$, $\epsilon_{\vy}(L)$, over the certifiable $\epsilon_{\vy}$ for $M$, $\epsilon_{\vy}(M)$:
\begin{equation}\label{eq:constriction-ratio}
    CR_k = \frac{\epsilon_{\vy}(L)}{\epsilon_{\vy}(M)}
\end{equation}
A $CR_k$ of $1$ for sample $\vy$ indicates that the bounds on generating $\vy$ are equal for $M$ and $L$ (i.e. they are equally constricted) while a $CR_k > 1$ indicates that $M$ is more constricting than $L$, and vice-versa. Smaller ACs for samples in $\sF$ are better and hence a large $CR_k$ indicates that model $M$ is favorable over $L$. To our knowledge, only vacuous certificates for a general model $L$ exist (e.g. $L$ is $1$-DC). Hence, we approximate it from below using the likelihood $L(\vy|\vx)$ under \emph{non-adversarial} $\vx$ taken from the datasets. Concretely, we use $L(\vy|\vx)$ as a crude approximation of $\max_{\vx\in \sS}L(\vy|\vx)$. This overestimates the robustness of $L$ and underestimates the constriction ratio, i.e., it underestimates the improvement of \ouralg certificates over $L$ in bounding the probability of OOD responses. In Figures~\ref{fig:combined-results} (d) - (f), we show the $\log_{10}$ constriction ratios for out-of-domain samples while setting $k$ to achieve an FRR of 10\% (see Appendix~\ref{app:cr-other-frr} for other FRRs). Across setups, the majority of samples have positive constriction ratios, which means that $M$ issues ACs tighter than $L(\vy|\vx)$. For MedicalQA, we observe a 99\% of $\log_{10}$CRs are greater than $6.30$ and observe a median CR of $24.23$. In other words, 99\% of samples are at least $6$ orders of magnitude less likely under $M$ and in the median $\approx 24$ orders of magnitude less likely (i.e. $1 \times 10^{-24}$). We believe these are very strong restrictions and observe even stronger median constriction for 20NG and TinyShakespeare. Further, we observe the strongest constriction among samples with high likelihood under $L$ (see Appendix~\ref{app:results}). Tight bounds are the most relevant on these samples as they are most likely to be sampled from $L$. Finally, we illustrate a trade-off between certification and OOD detection in Figure~\ref{fig:medqa-ood-vs-certification}. For MedicalQA, we plot the median constriction ratio for out-of-domain samples across a range of parameters $k$ together with false rejection rates (FRR) and true rejection rates (TRR). The optimal classification performance (as measured by Youden's $J$ \citep{youden_index_1950}) is achieved at $k=5.35$ with a strong true rejection rate (0.99) and a low false rejection rate (0.01), while producing a median $\log_{10}$ constriction ratio $19.00$. Smaller $k$ values yield tighter certificates (see the bound in \eqref{eq:upper_bound}) and larger constriction ratios at the expense of increasing the FRR.

\textbf{Domain Certificates. \ } To study certification across a range of samples, we turn to the domain certificate, $\epsilon$-DC. Above, we studied the effect of various parameters (e.g., fixing FRR) on the certificates. However, practitioners likely work the other way around: They first set an acceptable threshold according to a threat and safety model. Then, they examine model performance under conditions satisfying such certificate. Hence, we study model performance at a given $\epsilon$-DC. As proposed in Section~\ref{sec:definining-domain-certification}, we establish an $\epsilon$-DC certificate w.r.t. $\gD_{\sF}$ approximating the certificate for $\sF$. To obtain $\epsilon_{\vy}$-ACs smaller than the domain certificate $\epsilon$, we need to choose rejection threshold, $k$, and the number of iterations, $T$, accordingly. We
\begin{equation}
    \text{solve for $k$, $T$ given $\epsilon$:} \quad \max_{\vy \in \gD_{\sF}} \left\{ kN_{\vy} + \log T + \log G(\vy) \right\}=\log \epsilon.
\end{equation}
For simplicity, we keep $T=1$ and study model performance on $\gD_{\sT}$ while maintaining an $\epsilon$-DC on $\gD_{\sF}$. In particular, we look at the FRR of $M$: The performance of model $M$ is determined by the performance of $L$ (from which \ouralg samples response candidates) and the false rejections leading to a degradation of $M$ compared to $L$. Hence, we study the FRR as a function of the certification threshold $\epsilon$. The result is shown in Figure~\ref{fig:medqa-acc-dc} for MedicalQA: The FRR increases as the certificates get tighter (small $\epsilon)$. Remarkably, we achieve a domain certificate with $\epsilon=10^{-5}$ at an FRR of only 15\% at a single rejection step.
We replicate all figures for the other setups in Appendix~\ref{app:results}.

A natural question is why we do not simply use a model comparable to $G$ that is trained exclusively on a subset of $\sT$ directly. While such a model would be highly robust against providing useful out-of-domain responses, its performance would significantly lag behind both $L$ and $M$. Our ablation study in Appendix~\ref{app:ablation} confirms this performance gap between $G$ and $M$. These findings demonstrate that our system, which combines the high performance of $L$ with the safety guarantees of $G$, achieves advantages that neither $L$ nor $G$ can provide independently. Further, the effectiveness of \ouralg utilizing a $G$ of such limited performance demonstrates that the burden on training $G$ is relatively low: A model that performs poorly on the target task, but distinguishes well between samples in $\sT$ and $\sF$, can be sufficient to achieve meaningful certificates for $M$.

\subsection{Generating Responses}\label{sec:results-generation}

In the section above, we evaluate $M$ obtained through \ouralg on prompts and responses, taken from datasets $\gD_{\sT}$ and $\gD_{\sF}$ representing our target domain $\sT$ and $\sF$. The experiments provide us with a detailed analysis of ACs and DCs on a large variety of samples for which their membership to $\sT$ or $\sF$ is given by high-quality labels. Nonetheless, in practice, the candidate responses that are judged by \ouralg are generated by $L$. Hence, we prompt $M$ using $\vx \in \gD_{\sT}$ and $\vx \in \gD_{\sF}$ and use responses generated by $L$ as \ouralg proposes. We focus on \ouralg with $T=1$  and the MedicalQA setup.

Our findings are in line with Section~\ref{sec:results-log-ratio} showing a strong ability to distinguish between in- and out-of-domain samples while providing meaningful adversarial bounds. In Figure~\ref{fig:medqa-log-ratio-scatter}, we demonstrate the separation of samples from $\gD_{\sT}$ and $\gD_{\sF}$, as well as the dependence of the log ratios on the length of the sequence $\vy$ extending the theoretical analysis from Section~\ref{sec:rejection_sampling}. In Appendix~\ref{app:medqa-results}, we replicate Figure~\ref{fig:medqa-results-overall} for this setting. We further present in Figure~\ref{fig:medqa-cr-gen} the constriction ratios on out-of-distribution samples generated by $L$. We see a clear indication that the constriction is strong out-of-domain with an optimal classification performance at a ratio of $10^{40}$. To reiterate, median ratio between $L(\vy|\vx)$ and the $\epsilon_{\vy}$-AC for $M$ is $10^{40}$ showing just how strict \ouralg is on the out-of-domain dataset.

Building on these results, we test \ouralg with $T>1$. Increasing $T$ can naturally increase the acceptance rate on in-domain samples (through repeatedly proposing candidates) at the cost of increasing the $\epsilon_{\vy}$ linearly (see \eqref{eq:upper_bound}). We find great improvements in the acceptance rate on in-domain samples with minimal losses on the $\epsilon$-DC tightness. We explore this in Appendix~\ref{app:t-vs-k}.

\begin{figure}[t]
    \vspace{-10pt}
    \centering
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/medqa/constriction_ratio_metrics_MedicalQA_generated.pdf}
        \tightcaption
        \subcaption{}\label{fig:medqa-cr-gen}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/medqa/log_likelihood_ratio_vs_response_length.pdf}
        \tightcaption
        \subcaption{}\label{fig:medqa-log-ratio-scatter}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/medqa/benchmark_epsilon_dc_MedicalQA_PubMedQA.pdf}
        \tightcaption
        \subcaption{}\label{fig:medqa-pubmedqa-bench-eps}
    \end{minipage}
    \vspace{-8pt}
    \caption{All Figures show MedicalQA. Figure \ref{fig:medqa-cr-gen} shows the false rejection rate (FRR) for a range of $\epsilon$-DC for \ouralg with $T=1$. Figure \ref{fig:medqa-log-ratio-scatter} shows the log likelihood ratio depends on $N_{\vy}$ for real data. Performing length normalization makes the problem linearly separable. Figure \ref{fig:medqa-pubmedqa-bench-eps} shows PubMedQA@$\epsilon$ results of our model $M$.}
    \label{fig:medqa-gen-results}
    \vspace{-10pt}
\end{figure}

\subsection{Certified Benchmarking}\label{sec:results-benchmarks}

\begin{figure}[b]
    \centering
    \includegraphics[width=\linewidth]{assets/PubMedQA-Metric-Figure.pdf}
    % \vspace{-14pt}
    \vspace{-20pt}
    \caption{The PubMedQA@$\epsilon$ benchmark assesses PubMedQA performance while satisfying $\epsilon$-DC certificate. The correctness is scored as commonly done for PubMedQA (left). The correct long answer is checked by $M$ while ensuring the $\epsilon$-DC (right). Only if an item is accepted and correct, the question is scored positively.}
    \vspace{-5pt}
    \label{fig:pubmedqa-bench-at-eps-schematic}
    % \vspace{-20pt}
\end{figure}



We extend the analysis of false rejection rates (FRRs) by evaluating model $M$'s performance on standardized benchmarks, while ensuring it is certified at $\epsilon$. In particular, for our MedicalQA setup, we evaluate the model performance on the PubMedQA benchmark \citep{jin_pubmedqa_2019}.

\textbf{Setup. \ } Evaluating a standardized benchmark such as PubMedQA while certifying model $M$ requires careful consideration. The standard format typically includes $n$-shot examples followed by a multiple-choice question with either yes/no options or answers labeled A through D. The model is then prompted to select the correct response. However, this setup does not reflect a realistic user-system interaction. Thus, we introduce the PubMedQA@$\epsilon$ metric, which separates the evaluation into two streams: (1) standard assessment of model $L$ on PubMedQA to determine correctness, and (2) testing whether the correct question-answer pair is rejected by our algorithm. The process is summarized in Figure~\ref{fig:pubmedqa-bench-at-eps-schematic}. We score an item as correct, if the model predicts the correct answer while maintaining its $\epsilon-DC$ on the realistic question-answering pair.

\textbf{Results. \ } The unconstrained model scores 73.4\% on PubMedQA. As we tighten the certificate (decrease $\epsilon$), more correct responses are rejected and the benchmark score drops, as shown in Figure~\ref{fig:medqa-pubmedqa-bench-eps}. We find that when certifying at $\epsilon=10^{-5}$, we maintain a certified score of 66.7\% (-6.7\%), and at $\epsilon=10^{-10}$ of 47.7\% (-25.7\%). These scores demonstrate robust performance given the provable defense facilitating domain restriction. In Appendix~\ref{app:benchmark}, we discuss benchmarking in more depth.

\section{Related Work}

\paragraph{LLM Guardrails.}
A large body of work has been published on establishing effective guardrails for LLMs. These approaches are designed to restrict the model to responses that align with the deployer's values. One of the first approaches was Reinforcement Learning with Human Feedback (RLHF) \citep{askell_general_2021}, which uses human preferences to guide LLM training.
Extensions such as Safe-RLHF add cost models to penalize harmful behavior, ensuring a balance between helpfulness and harmlessness during optimization \citep{dai_safe_2024}.
RLHF's foundation in reinforcement learning has given rise to techniques such as Proximal Policy Optimization (PPO) \citep{bai_training_2022}, the more recent Direct Preference Optimization (DPO) \citep{rafailov_direct_2024},
and Generalized Policy Optimization (GPO) \citep{tang_generalized_2024}, which incorporates diverse optimization objectives, useful for safety-critical scenarios.
For an in-depth survey of this area, we direct the reader to \citet{kaufmann_survey_2024}.
Unlike the preceding approaches that fine-tune guardrails into the parameters of an LLM, a number of works have proposed using LLMs to classify content as either safe or unsafe.
Llama Guard categorizes the inputs and outputs of an LLM into different unsafe content categories \citep{inan_llama_2023}. Conversely, \citet{chua_flexible_2024} classify if an output is safe with respect to a system prompt.
For a complete overview on LLM guardrails, we direct the interested reader to a recent survey of this area, \citet{dong_safeguarding_2024}.
Existing LLM guardrail techniques have been proven effective to different levels. However, these guardrails only come with empirical evidence of their proficiency against existing attacks, and hence, many have been circumvented shortly after deployment. Conversely, VALID offers a provable high-probability guarantee against undesirable behavior, reflecting recent advocacy for such provable assurances \citep{bengio_bounding_2024}.

\paragraph{Out-of-Distribution Detection. }
Out-of-distribution (OOD) detection has received a lot of attention in recent years in NLP. Commonly, the problem is treated as text classification and softmax probabilities of class predictions \citep{hendrycks_baseline_2017} or energy scores \citep{liu_energy-based_2020} are deployed as discriminant scores. Another group of methods employs distance-based methods, relying on OOD responses being distant from ID responses in latent space, often utilizing Mahalanobis distance and sometimes incorporating contrastive learning techniques \citep{uppaal_is_2023, podolskiy_revisiting_2021, zhou_contrastive_2021, khosla_supervised_2020, lin_flats_2023}. Finally, rooted in classical statistics, a number of studies suggest using the log-likelihood ratio (LLR) as a discriminate score, comparing likelihoods from ID and OOD proxy models \citep{gangal_likelihood_2020, zhang_your_2024}. \cite{xu_large_2024} offer a comprehensive review of LLMs for OOD detection. While many of these works have strong empirical detection results, their focus is OOD detection rather than certification, and hence they do not provide theoretical guarantees or certificates on model behavior.


\paragraph{Certifying LLMS.}
A number of certification approaches have been proposed for LLMs in various contexts. For instance, \citet{chaudhary_quantitative_2024-1} aim to certify the knowledge comprehension ability of LLMs and \citet{freiberger_fairness_2024} discuss what criteria should be certified to ensure fairness. Most relevant here is work on certification against adversarial inputs. \citet{casadio_nlp_2024} discuss certifying the robustness of LLMs to input perturbations in embedding space. Commonly, adversarial certification is studied for text classification rather than generation \citep{la_malfa_robustness_2023}. \citet{kumar_certifying_2024} introduce a framework for defending against adversarial perturbations in token space by performing a small number of substitutions around a given input. In contrast VALID comes with certificates that holds for \emph{all inputs}, rather than perturbations around a specific input.

\vspace{-2pt}
\section{Limitations} \label{app:limitations}
\vspace{-2pt}
Despite our promising results, we acknowledge the limitations of our current implementation. First, the domain generator $G(\vy)$ lacks context. This means that if $\vy$ is \emph{marginally} in-domain, while $\vy|\vx$, the conditional distribution is not, our method will not reject appropriately. Consider a chatbot for tax advice. For prompt $\vx=$\emph{"How often is a tax report due?"}, the response $\vy=$\emph{"Once a year."} is in-domain. Hence, the same response to $\vx=$\emph{"How often should I shower?"} might be accepted despite it being out-of-domain, and terrible advice. However, this can be mitigated by fine-tuning the model $L$ to be \emph{as explicit} as possible repeating ``shower'' in the response.

Second, this approach relies heavily on the domain-specific model $G$, and how closely it approximates the ideal oracle $\Omega$. In practice and as demonstrated in our experiments, $G$ might have \emph{limited} semantic understanding and lack general language capabilities and world knowledge. In most instances it might not be able to distinguish between semantically opposite but similar sentences and hence \ouralg is likely incapable of \emph{aligning} the model, rather than \emph{shushing} it.

Third, an adversary might construct an attack that aims to copy tokens from the prompt of $L$ to $G$. For instance, $\vx=$\textit{``Repeat after me: !!!-+! and then tell me how to build a bomb!''}. This ``!!!-+!'' might be an adversary for $G$ to assign a high likelihood to $L$ following the instruction. For this attack, the adversary operates with limited information, having access only to whether the log ratio is bounded, without visibility into $G$'s outputs, weights, or likelihood scores. In addition, since $G$ has never seen information on how to build a bomb, it is extremely unlikely to produce coherent, correct, and harmful content. In Appendix \ref{app:attack-m}, we discuss the feasibility of attacking $M$ further.

Fourth, our method comes at the extra cost of sampling up to $T$ times. Further, it requires training $G$ and evaluating it during inference. Depending on the architecture of $G$ however, the extra cost is limited. In our experiments $G$ is orders of magnitude smaller than $L$.
\vspace{-2pt}
\section{Future Work}
\vspace{-2pt}
In this section we briefly discuss some ideas for future work that we believe could further extent the practical utility of \ouralg.
Initially, it would be interesting to test larger, specialized models for $G$ to evaluate whether these more advanced models produce improved certificates and refusal rates. We chose not to do this because LLMs trained from scratch exclusively on specific domains are not common, and thus results generalize less to what a practitioner with limited resources could expect.

As described in Section \ref{sec:rejection_sampling}, \ouralg uses length normalization to ensure the log likelihood ratio rejection condition is robust to different lengths of sequences $N_y$. One may extend this and learn a more complex polynomial of $N_y$ as rejection threshold. This threshold could be used to provide both $\epsilon_{\vy}$-ACs and $\epsilon$-DC certificates, while simultaneously enabling more precise OOD detection.

Finally, a rejection scheme with a probabilistic decision rule, similar to Algorithm 5 in \citet{vyas_provable_2023}, would be able to provide identical bounds to Theorem~\ref{thm:valid}. Possibly, this rejection rule would lead to better performance in terms of OOD classification.

\vspace{-2pt}
\section{Conclusion}
\vspace{-2pt}
In this work, we tackle the problem of generative language models producing outputs outside their target domain in response to adversarial inputs. We describe the associated risks, introduce a first-of-its-kind framework for domain certification for LLMs, and provide \ouralg, a simple algorithm relying on well-established theories from statistics and information theory to provide such guarantees. We demonstrate the effectiveness of \ouralg in multiple representative settings and show that it is effective even when relying on a guide model $G$ with limited language skills, making it easy to deploy in limited data and resource environments.


% \section*{Reproducibility Statement}
% Detailed information about our experimental setup is provided in Appendix~\ref{app:experimental-setup}. All relevant implementation details from the main body are repeated there for ease. We rely heavily on Huggingface to enhance code accessibility and released the code here: \href{https://github.com/cemde/Domain-Certification}{github.com/cemde/Domain-Certification}.


\pagebreak

\subsubsection*{Acknowledgments}
% Use unnumbered third level headings for the acknowledgments. All
% acknowledgments, including those to funding agencies, go at the end of the paper.

This work is supported by a UKRI grant Turing AI Fellowship (EP/W002981/1). C. Emde and M. Kayser are supported by the EPSRC Centre for Doctoral Training in Health Data Science
(EP/S02428X/1) and the AXA Research Fund. A. Bibi acknowledges the Google Gemma 2 Academic Award 2024. T. Lukasiewicz is supported by the AXA Research Fund. Tom Rainforth is supported by the UK EPSRC grant EP/Y037200/1. We also thank the Royal Academy of Engineering.
The research reported in this publication was partially supported by funding from KAUST Center of Excellence on GenAI, under award number 5940.
Further, we thank Samuele Marro for his advice.

\raggedbottom

\bibliography{references}
\bibliographystyle{iclr2025_conference}

\pagebreak

\flushbottom

\appendix
\section{Proofs}\label{app:proofs}

\rejectionbound*
\begin{proof}
We abbreviate $M_{L,G,k,T}$ as $M$. Let $A_t$ and $A_t'$ be the events of accepting and rejecting in iteration $t$, respectively. Let $S_t$ be the event of sampling $\vy \sim L(\cdot|\vx)$ in iteration $t$ and let $A_{< t}'$ be the event of rejecting all samples before $t$, $A_{< t}'=\bigcap_{i=1}^{t-1} A_i'$. Then,
\begin{equation}\label{eq:rejection_lemma}
    M(\vy|\vx) = \sum_{t=1}^T \sP(S_t \cap A_t \cap A_{< t}' | \vx) = \sum_{t=1}^T \sP(A_t | S_t,A_{< t}', \vx) \sP(S_t | A'_{<t}, \vx) \prod_{i<t} \sP(A_i'| A'_{<i}, \vx).
\end{equation}
We upper bound the probability of rejecting in any previous iteration by 1, $\forall t: \sP(A_t'|A'_{<t},\vx) \leq 1$. $\sP(A_{t} | S_t, A'_{<t}, \vx)$ is non-stochastic and is equal to either $0$ or $1$. In the former case, the $M(\vy|\vx)$ is trivially bounded by any non-negative number. The latter case (i.e. $\vy$ is accepted in iteration $t$) implies that $\log \frac{L(\vy|\vx)}{G(\vy)} \leq k N_y$. Rearranging terms and noting that by definition $\sP(S_t|A'_{<t},\vx)=L(\vy|\vx)$, we get $\sP(S_t |A'_{<t}, \vx) \leq 2^{k N_y}G(\vy)$ and hence by substitution and summing over $t$,
\begin{equation}
    M(\vy|\vx) \leq \sum_{t=1}^T 2^{kN_y}G(\vy) = 2^{k N_y} \cdot  T \cdot G(\vy).
\end{equation}
This is the desired upper bound on $M(\vy|\vx)$ for all $\vx \in \sS$.
\end{proof}

\begin{restatable}[Equivalence of Divergence]{lem}{equivalence}\label{equivalence}
Let $\dd[\infty]{P}{Q}$ be the Renyi divergence of order infinity \citep{renyi_measures_1961}, $\dd[\infty]{P}{Q} \triangleq \log \sup_x \frac{P(x)}{Q(x)}$. Further, let $L:\mathbb{S}\rightarrow \mathbb{S}$ be an LLM returning $\vy$ given $\vx$ as discussed above and let $\Omega$ be a distribution over domain $\sT$, i.e. \emph{generator} for $\sT$. Then, if
\begin{equation}
    \forall \vx \in \sX: \dd[\infty]{L(\vy|\vx)}{\Omega(\vy)} \leq k,
\end{equation}
we can state that $L$ is $\epsilon_{\vy}$-AC with $\epsilon_{\vy}=2^k\Omega(\vy)$ (see Definition~\ref{def:ac}) and $\epsilon$-DC with $\epsilon = 2^k\max_{\sF}\Omega(\vy)$ (see Definition~\ref{def:dc}).
If $\Omega$ is an oracle, that assigns no likelihood to elements in $\sF$, it implies $L$ is $0$-AC and $0$-DC.
\end{restatable}
\begin{proof}
We start from the definition of the Renyi divergence, which is an upper bound to any element in the supremum, giving that
\begin{equation}
    \forall \vx \in \sX: \log \frac{L(\vy|\vx)}{\Omega(\vy)} \leq \log \sup_\vy\frac{L(\vy|\vx)}{\Omega(\vy)} = \dd[\infty]{L(\vy|\vx)}{\Omega(\vy)} \leq k.
\end{equation}
Exponentiating and multiplying through by $\Omega(\vy)$ gives the following upper bound:
\begin{equation}\label{eq:app-equivalence-of-divergence-proof-ac}
 \forall \vx \in \sX: L(\vy|\vx) \leq 2^k \Omega(\vy),
\end{equation}
showing the $2^k \Omega(\vy)$-AC equivalence. Taking the $\max$ over $\sF$ shows the $[2^k\max_{\sF}\Omega(\vy)]$-DC equivalence. Further, assuming $\Omega$ to be a perfect oracle, by definition, we can state that $\forall \vy \in \sF$ the upper bound on the right hand side of \eqref{eq:app-equivalence-of-divergence-proof-ac} is zero. Thus, we get the desired result:
\begin{equation}
 \forall \vx \in \sX,  \forall \vx \in \sF: L(\vy|\vx) = 0,
\end{equation}
and hence $L$ is $0$-AC and $0$-DC.
\end{proof}

\begin{restatable}[Likelihood of $M$]{lem}{likelihood-m}\label{lemma:likelihood-m} Let $M$ be a model obtained by performing rejection sampling from model $L$ as proposed in \ouralg using guide model $G$ and rejection threshold $k$ (see Algorithm~\ref{alg:cap}). We denote the likelihood of response $\vy$ given input $\vx$ under the model $M$ as $M(\vy|\vx)$. For all $\vy \in \sS$,
\begin{align}\label{eq:lemma-likelihood-m}
M(\vy|\vx) & = \begin{cases}
    L(\vy|\vx)\frac{1 - \phi^{T}}{1 - \phi} & \text{if } L(\vy|\vx) \leq k G(\vy)\\
    0  & \text{otherwise}.
\end{cases} \\
    \intertext{where $A'_t$ is the event of rejecting $\vy$ in iteration $t$ given input $\vx$, $A'_{<t}$ is the event of rejecting in all iterations before $t$, $A'_{<t}=\cap_{i=1}^{t-1}A'_i$, and finally let $\phi = \sP(A_t'|A'_{<t},\vx)$, the conditional probability of rejecting  $\vy$ in a given iteration $t$ for input $\vx$. Finally, let $R$ be the event that $M$ abstains, for which }
    M(R|\vx) & = \phi^T.
\end{align}
\end{restatable}
\begin{proof}
Let $S_t$ be the event of sampling $\vy \sim L(\cdot|\vx)$ in iteration $t$, let $\sA \subset \sS$ be the acceptance set of $\vy$, i.e., $\sA = \{\vy: L(\vy|\vx) \leq 2^{kN_{\vy}} G(\vy) \}$ and let its complement in $\sS$, $\sA'$, be the rejection set. Finally, let $S$ be the event of sampling $\vy$. We now derive $M(\vy|\vx)$ per case as stated in \eqref{eq:lemma-likelihood-m}.

Starting with case $\vy \in \sA$, we note that $M(\vy|\vx)=\sP(S|\vx)$ and we can rewrite $\sP(S|\vx)$ as follows,
\begin{align}
    \sP(S|\vx) & = \sum_{t=1}^T \sP(S_t \cap A_t \cap A_{\leq t-1}'|\vx) \\
    & = \sum_{t=1}^T \sP(A_t | S_t, A'_{<t}, \vx)\sP(S_t|A'_{<t},\vx) \prod_{i<t} \sP(A_i'|A'_{<i}, \vx) \label{eq:likelihood-m-expanded} \\
    & = L(\vy|\vx)\sum_{t=1}^T  \phi^{t-1} \\
    & = L(\vy|\vx) \frac{1 - \phi^{T}}{1 - \phi}
\end{align}
where we use the fact that $\forall \vy \in \sA: \sP(A_{t} | S_{t},A'_{<t}, \vx) = 1$ and notice that $\sum_{t=1}^T  \phi^{t-1}$ is the sum of the first $T$ elements of a geometric series and substitute $L(\vy|\vx)$ for $\sP(S_t|A'_{<t},\vx)$.

For the case $\vy \in \sA'$: We rewrite the likelihood as shown above in \eqref{eq:likelihood-m-expanded}. Notice that $\forall \vy \in \sA': \sP(A_t|S_t,A'_{<t}, \vx)=0$ and therefore $P(S|\vx)$ is zero.

Finally, we turn to the rejection event $R$. Note that $R = \bigcap_{t=1}^T A_t'$, rejection at each step $t=1,\ldots,T$. We can state that
\begin{equation}
    M(R|\vx)=\prod_{t=1}^T \sP(A'_t|A'_{<t},\vx)=\phi^T,
\end{equation}
which concludes the proof.
\end{proof}


\begin{remark}[Estimating likelihood] While Lemma~\ref{lemma:adversary-m} provides an expression of the likelihood of model $M$ computing this might be infeasible. If the sample space $\sS$ is large, we cannot compute $M(\vy|\vx)$ as we cannot compute $\phi$, the rejection probability in any given iteration in \ouralg for a given input $\vx$. However, we can estimate $M(\vy|\vx)$ by computing $L(\vy|\vx)$ and performing Monte Carlo sampling from $L$ to obtain an estimator $\hat{\phi}$. We can then use the Binomial confidence interval for confidence level $\alpha$:
\begin{equation}
\hat{\phi} \pm Z_{\alpha/2} \times \sqrt{\frac{\hat{\phi}\left(1 - \hat{\phi}\right)}{N}}.
\end{equation}
We then plug in the bounds on $L$ to obtain the bound on $M$ because of the monotonicity of $M$ in $\hat{\phi}$.
\end{remark}

\begin{restatable}[Expected number of iterations in \ouralg]{lem}{expected-num-trials}\label{lemma:expected-number-of-trials}
Let $\tau$ be the number of iterations executed in \ouralg (see Algorithm~\ref{alg:cap}), let $A_t$ be the event of accepting a response $\vy$ for input $\vx$ in iteration $t$, $t=1,\ldots,T$, and let its complement, $A_t'$, be the event of rejection in iteration $t$. Denote the event that all samples up to $t$ (inclusive) are rejected as $A'_{\leq t}=\bigcap_{i=1}^tA'_i$. Finally, we denote $\phi = \sP(A'_t|A'_{\leq t-1}, \vx)$, the probability of rejection in iteration $t$. The expected number of iterations for $\phi \in [0,1)$ is given by:
\begin{align}
\mathbb{E}_{\vy \sim M(\cdot|\vx)}[\tau] = \frac{1-\phi^T}{1-\phi},
\end{align}
\end{restatable}
and for $\phi=1$, the expected number of iterations is given by $\mathbb{E}_{\vy \sim M(\cdot|\vx)}[\tau] = T$.

\begin{proof}
In the following, we will denote $\mathbb{E}_{\vy \sim M(\cdot|\vx)}[\tau]$ as $\mathbb{E}[\tau]$ for readability.  Note that $\sP(\tau=t)$ is the probability of reaching and accepting in iteration $t$ for $t=1,\ldots,T-1$. Once iteration $T$ is reached, both acceptance and rejection yield $\tau=T$. Hence,
\begin{equation}
    \E[\tau]=\sum_{t=1}^T t \sP(\tau=t) = T \sP(A_T' \cap A'_{\leq T-1} | \vx) + \sum_{t=1}^{T} t \sP(A_t\cap A'_{\leq t-1}| \vx).
\end{equation}
Combining events and factorising probabilities,
\begin{equation}
    \E[\tau]= T\sP(A'_{\leq T}|\vx) + \sum_{t=1}^T t \sP(A_t|A'_{\leq t-1}, \vx) \prod_{i<t} \sP(A'_i|A'_{\leq i-1},\vx),
\end{equation}
for which we substitute rejection and acceptance probabilities by $\phi$ and $1-\phi$, respectively,
\begin{equation}\label{eq:expectation_1}
    \E[\tau]= T\phi^T + (1-\phi) \sum_{t=1}^T t \phi^{t-1}.
\end{equation}
Multiplying by $\phi$:
\begin{equation}
\phi\mathbb{E}[\tau] = T\phi^{T+1} + (1-\phi)\sum_{t=1}^T t \phi^{t}.\label{eq:expectation_2}
\end{equation}
Subtracting \eqref{eq:expectation_2} from \ref{eq:expectation_1}:
\begin{equation}
\mathbb{E}[\tau]-\phi\mathbb{E}[\tau] = (1-\phi)T\phi^T + (1-\phi)\sum_{t=1}^T t \phi^{t-1} - t \phi^{t}.
\end{equation}
Telescoping sum:
\begin{equation}
(1-\phi)\mathbb{E}[\tau] = (1-\phi)T\phi^T + (1-\phi)\sum_{t=1}^T \phi^{t-1} - T \phi^{T}.
\end{equation}
Dividing by $(1-\phi)$. For all $\phi < 1$:
\begin{equation}
\mathbb{E}[\tau] = T\phi^T + \sum_{t=1}^T \phi^{t-1} - T \phi^{T}.
\end{equation}
Cancelling terms and summing the first $T$ elements of the geometric series:
\begin{equation}
\mathbb{E}[\tau] =  \sum_{t=1}^T \phi^{t-1} = \frac{1-\phi^T}{1-\phi}.
\end{equation}
Using L'H\^{o}pital's Rule, we can evaluate the limit for $\phi \rightarrow 1$ and find that this simplifies to $T$ and hence $\E[\tau]=T$ when $\phi=1$ completing the proof.
\end{proof}

\begin{remark}
    The expected number of iterations as derived in Lemma~\ref{lemma:expected-number-of-trials} depends on the rejection probability $\phi$ and the maximum number of iterations $T$. When $\phi=0$, the algorithm always accepts in any iteration and hence $\E_{\vy \sim M(\cdot|\vx)}\left[\tau\right] = 1$. Conversely, when $\phi=1$ and the algorithm always abstains, $ \E_{\vy \sim M(\cdot|\vx)}\left[\tau\right] = T$. Further, for $T=1, \forall \phi \in [0,1]: \E_{\vy \sim M(\cdot|\vx)}[\tau]=1$ and as $T$ increases, so does $\E_{\vy \sim M(\cdot|\vx)}[\tau]$ when $\phi > 0$.
\end{remark}

\section{Defining Domains - Practical Considerations}\label{app:choosing-F-guidance}

In this section, we provide practical guidance for practitioners on how to select domains for their AI systems, presenting a systematic approach to classifying sequences into different domains. Figure~\ref{fig:app-ven-diagram-domains} illustrates a Venn diagram comprising three key sets of sequences, i.e. subsets of $\sS$:
\begin{enumerate}
    \item The target domain $\sT$ (shown in blue), containing desired content about which the LLM-driven system should converse (e.g., medical questions and answers),
    \item The out-of-domain set $\sF$ (shown in orange), containing potentially harmful or other content that require active protection measures (e.g., tax fraud advice),
    \item The complement of $\sT$ and $\sF$, denoted as $\sT' \cap \sF'$ (shown in gray).
\end{enumerate}

A fundamental question arises: How should one define $\sT$ and $\sF$? Defining $\sT$ is relatively natural for most practitioners: Content that semantically belongs to the domain should be included in $\sT$. The more complex decision involves determining which sequences outside $\sT$ should be included in $\sF$. We contend that protecting against certain sequences warrants higher priority than others, and these high-priority sequences should be included in $\sF$.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{assets/domain_certification_sets.pdf}
    \caption{A Venn diagram illustrating the separation of sentences into domains.}
    \label{fig:app-ven-diagram-domains}
\end{figure}

Consider the example sequence \exampleinline{\vy}{The sky is blue. The sky is blue. The sky is blue.} While this is clearly out-of-domain for a medical QA system, practitioners should evaluate two critical questions to determine its placement in $\sF$:

\begin{enumerate}
    \item Would adversaries have motivation to generate such sequences?
    \item Could these sequences potentially harm users, the deployer, or third parties?
\end{enumerate}

In this example, adversaries would likely have little incentive to generate such a response, and the content itself is harmless. Therefore, practitioners might reasonably conclude that $\vy$ should remain in $\sT' \cap \sF'$ rather than $\sF$, excluding it from system certification considerations.

These evaluation questions help practitioners assess risk levels effectively. When both questions receive negative answers, sequences can safely remain in $\sT' \cap \sF'$ without requiring active protection measures, allowing security efforts to focus on genuinely concerning sequences. If either question receives a positive response, practitioners may choose to implement protective measures.

Let us analyze two more examples to demonstrate this in practice. Consider the sequence \exampleinline{\vy}{Here is an easy recipe to commit tax fraud...}. In this case, malicious actors would be highly motivated to seek such information, and the content could directly harm society and government functions. Thus, this sequence clearly belongs in $\sF$ and requires active protection measures. Similarly, when considering a love poem as $\vy$, although it may seem harmless at first glance, the analysis reveals important considerations. Users might frequently request LLMs to generate poetry, potentially straining system resources, and while not directly harmful to users or society, it could significantly impact system infrastructure and operational costs. Consequently, practitioners might choose to include this in $\sF$ to protect their computational resources.

It is important to note that these evaluation questions are not intended as universal rules, but rather serve as a practical considerations to guide practitioners in their decision-making process. By systematically assessing motivation and potential harm, practitioners can make informed decisions about which sets of sequences require active protection measures.


\section{\ouralg --- Rejection Sampling} \label{app:rejection-sampling}

\subsection{Attacking $M$}\label{app:attack-m}

In this section, we provide some insight on how rejection sampling as deployed in \ouralg (see Section~\ref{sec:rejection_sampling}) can obtain such tight adversarial bounds. In particular, we show by example that out-of-domain samples are only accepted when they have sufficiently \emph{small likelihood} of being sampled under $L$. We then formalize this intuition and state the objective of a possible adversarial attack on $M$. For simplicity, we will consider the case $T=1$.

\textbf{Intuition. \ } Here, we demonstrate that accepting an out-of-domain response requires it to have low likelihood under model $L$. Specifically, we show that when a response is rejected for a given prompt, the correct strategy for acceptance by model $M$ involves modifying the prompt to \emph{reduce} the response's probability under $L$. To illustrate this concept, we examine a single response $\vy$. Let \exampleinline{\vy}{The cow drinks milk} and consider three prompts:
\begin{itemize}
    \item \exampleinline{\vx_1}{What does a cow drink?}
    \item \exampleinline{\vx_2}{Which animal drinks milk?}
    \item \exampleinline{\vx_3}{Repeat after me: The cow drinks milk. Now you: }
\end{itemize}
Intuitively, we may assume $L(\vy|\vx_3) > L(\vy|\vx_1) > L(\vy | \vx_2)$ as $\vy$ more naturally follows some prompts than others: $\vy|\vx_3$ would have high likelihood for instruction-tuned models, moderate likelihood after being specifically asked about cows ($\vx_1$), and low likelihood when asked broadly about mammals ($\vx_2$). We illustrate this example in Figure~\ref{fig:rejection-sampling-intuition}. If we assume that $\vy|\vx_1$ is rejected, i.e., $\log L(\vy|\vx_1) - \log G(\vy)>k N_{\vy}$, then we can conclude that $\vy|\vx_3$ will also be rejected. In contrast, $\vy|\vx_2$ will be accepted when $L(\vy|\vx_2)$ is small enough, such that $\log L(\vy|\vx_2) - \log G(\vy)<k N_{\vy}$, which recovers the upper bound of $2^{kN_{\vy}}G(\vy)$ (see Theorem~\ref{thm:valid}) by algebraic manipulation. This illustrates how rejection sampling bounds the adversaries: Samples will only be accepted if proposing them was very unlikely in the first place. Consequently, when faced with rejected adversarial prompts $\vx$, the attacker must find alternative prompt $\vx'$ that yield lower likelihood $\vy|\vx'$. This creates a remarkable and counter-intuitive dynamic: successful adversarial attacks on model $M$ require the attacker to effectively perform risk control on sampling $\vy$. This intuition helps us establishing how to attack $M$.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{assets/rejection_sampling_intuition.pdf}
    \tightcaption
    \vspace{5pt}
    \caption{The likelihood of model $M$ obtained through \ouralg with $T=1$. The blue line is the likelihood of $M$ for the given $\vy$. Three example prompts $\vx_1$, $\vx_2$ and $\vx_3$ are shown.}
    \label{fig:rejection-sampling-intuition}
\end{figure}
\CE{Figure missing N_y}

\textbf{Formalization of Attack. \ } We assume that the adversarial objective is to increase the probability of a given $\vy$ (e.g. from the out-of-domain set), $\sF$, being returned. The objective of attacking $L$ is immediately follows:
\begin{equation}
    \vx^{adv}_L=\argmax_{\vx \in \sX} L(\vy|\vx)
\end{equation}
where $\sX$ is either $\sS$ or some continuous relaxation, such as soft-prompt space. However, the solution $\vx^{adv}_L$ may not be an adversary under $M$, since $\vx^{adv}_L$ might maximize the log-likelihood ratio that leads to the sample being rejected, and hence $M(\vy|\vx^{adv}_L)=0$. Instead, the adversary for $M$, $\vx^{adv}_M$, needs to maximize $L$ while ensuring the sample is accepted. We formalize this in the following lemma.

\begin{restatable}[Adversary under Rejection Sampling]{lem}{adversaryrejection}\label{lemma:adversary-m} Assume the adversarial objective is to maximize the likelihood of sample $\vy$ being returned by the model $M$. Assume the model $M$ is obtained through \ouralg as described in Algorithm~\ref{alg:cap} with $T=1$. The adversary is given by:
\begin{equation}\label{eq:attack-m-objective}
    \vx^{adv}_M = \argmax_{\vx \in \sX} L(\vy|\vx) \text{ s.t. } L(\vy|\vx)\leq 2^{kN_{\vy}}G(\vy),
\end{equation}
where $\sX$ is either sentence space $\sS$ or some relaxation.
\end{restatable}

\begin{proof}
The proof follows immediately from Lemma~\ref{lemma:likelihood-m} with $T=1$. Let $\sA\subset\sS$ be the acceptance set, $\sA=\{\vx \in \sX: L(\vy|\vx) \leq 2^{kN_{\vy}}G(\vy) \}$. Then, $\forall \vx \notin \sA$ the likelihood of $M(\vy|\vx)=0$. Hence, the adversary maximizing $M(\vy|\vx)$ is the adversary for $L(\vy|\vx)$ within $\sA$.
\end{proof}

\textbf{Executing Attack. \ } Applying \ouralg to obtain $M$ has implications on the suitable procedures to attack $M$. In particular, it requires solving the constrained optimization problem in \eqref{eq:attack-m-objective}, which adds a layer of complexity to the unconstrained problem for $L$. In general, constrained optimization problems are more challenging; this is compounded by the upper bound on $L(\vy|\vx)$ not decomposing across tokens. Furthermore, while large models, such as LLama-3-8B, are often publicly available, $G$ will likely be a custom model for which the attacker does not have white-box access. For a successful attack, the adversarial user must estimate the likelihood ratio between $L$ and $G$, which might prove challenging. This indicates that attacking $M$ defined through \ouralg might be a harder problem than attacking $L$. Finally, as a reassuring reminder, while it is possible to attack $M$, our certificate holds and $M$ cannot be attacked past the upper bound provided in Theorem~\ref{thm:valid}.

\section{Experimental Setup} \label{app:experimental-setup}

\subsection{\CT Dataset}\label{app:chartask-dataset}
For prototyping, we have created a toy dataset that we call \CT. The goal of the \CT dataset is to have a well-controlled toy dataset with clear definitions of target domain $\sT$ and other domains $\sF$.

\begin{table}[b]
    \centering
    \caption{Examples of the \CT dataset}
    \vspace{-8pt}
    \label{tab:example-chartask}
    \include{assets/chartask/examples}
\end{table}

As shown in Table~\ref{tab:example-chartask}, each sequence consists of three parts: A sequence of random characters, a task definition in the middle, and another sequence of characters in the end. We refer to the random sequence as $S_{in}$. In the middle there are four task tokens, the first of which defines the task $T$. ``S'' sets the task to sorting, ``R'' to reverse sorting, ``A`` to adding $+1$ and ``E'' to even-odd sorting. The instruction token is followed by the remaining three task tokens in random order to ensure that all are seen by a model trained on a subset of these. Finally, the completed sequence is the original sequence of characters with the task performed on them, i.e. $S_{out}=T(S_{in})$. The pool of characters for each sequence is either only integers or integers and lower case letters. Importantly, all tasks interpret characters and integers as characters alike. For example, sorting integers ``11'', ``5'' results in ``11'', ``5''. To be precise, all tasks are based on integer unicode representations of characters.

Each sequence has a variable length of up to 49 elements in $S_{in}$ (the elements can be double digits). For integers, we use a pool of 49 unique distinct integers and for characters, we use a pool of 249 elements (e.g., defining ``at'' as one element in the sequence). Under these conditions, there exists a combinatorially large set of unique sequences far exceeding our training dataset size.

Given the tasks and pools of characters, 8 possible domains emerge as shown in Table~\ref{tab:example-chartask}, which we denote as \CT(Task, Pool). We define sorting integers as the target domain: $\gD_{\sT}=\text{\CT(Sorting, Int)}$ and all other combinations as out-of-domain. We create two distinct datasets with non-overlapping splits for training, validation, and testing. The in-domain dataset consists of 1M training samples. The ``generalist'' dataset $\gD_{\sT+\sF}=\text{\CT(All, Int + Char)}$ contains all possible tasks with sequences consisting of integers and characters. We use 1M training sequences per task, and hence 4M sequences in total. The validation and test sets are 64 sequences and 4096 sequences, respectively.

\subsection{\CT Setup}\label{app:chartask-setup}
\textbf{Dataset and Domain. \ } We use the \CT dataset described in Appendix~\ref{app:chartask-dataset}. We train a custom BPE tokenizer of length 360 \citep{sennrich_neural_2016}. In practice, the pretrained tokenizer of any foundation model is trained on a general dataset. Hence, we train the tokenizer using $\gD_{\sT}$ and $\gD_{\sF}$, the target and out-of-domain datasets. While the dataset is inherently suitable for a sequence-to-sequence task, we treat it as next-token prediction problem just as used in language modeling.

\textbf{Training. \ } We train our domain model $G$ on a set of integer sorting examples, \CT(Sorting, Int). We train a GPT-2 \citep{radford_language_2019} architecture with 3 layers, 3 heads and 48 embedding dimension. We train the model on partial sequences, as we are embedding marginal sequences $\vy$. Hence, we cut each sequence in two parts using a splitting point that is sampled under a uniform distribution. Hence, the model learns the transition from ``[BOS] ..'' to any character that might be the first response token.

For the generalist model $L$, we train using all available tasks on integers and characters, \CT(All,Int+Char). We train a GPT-2 architecture with 6 layers, 6 heads and 192 embedding dimensions.

We train $L$ and $G$ with AdamW (weight decay 0.1) for 2048 steps with a cosine learning rate schedule with 500 steps warm-up, a maximum learning rate of 0.005, scheduled for 40 epochs. We train with 120 context window using next-token prediction.

\textbf{Inference. \ } We use common parameters to tweak the predictive distribution of our models. For $G$ we use a temperature of 0.7 and for $L$ of 0.2. We find this greatly helps the model performance of both. We do not perform \emph{TopK} selection of tokens. We prompt with a prompt length of 10. The task-completed sequence is almost deterministic given the prompt and task for models that have very high accuracy. Hence, we remove sequences where the prompt of 10 tokens is larger than 25\% of the entire sequence.

\subsection{20NG Setup}\label{app:20ng-setup}

\textbf{Dataset Cleaning. \ } The 20NG dataset is very dirty, containing a wide array of random special character sequences and arbitrary formatting. We found these sequences to complicate model training and large pre-trained models struggled with it. In addition, as formatting strongly varies between the 20NG dataset and others, this is a confounding factor for OOD detection. Classifying sentences as ID or OOD should focus on semantics, but the formatting provides a spurious correlation that is easily exploited by models. Hence, we decided to clean the dataset. To do so, we utilise the \texttt{scikit-learn} (v$1.5.1$) \citep{pedregosa_scikit-learn_2011} options to remove headers, footers and quotes. Further, we cleaned it using Llama-3.1-8B-Instruct \citep{dubey_llama_2024} using the following query:


\centerbox{
\include{assets/llama_cleaning_prompt}
}

We check the output for various keywords and phrases from prompt and find a 0\% violation rate. While there still exist random sequences, the data quality is greatly improved. We notice that several sequences exist in 20NG and OOD testing datasets that are seemingly random character sequences and multiple trigram repetitions such as ``Nanaimo British Columbia Nanaimo British Columbia Nanaimo British Columbia ...''. These sequences have the highest likelihood under model $G$ and $L$ while not having any semantic meaning nor constituting a valid sequence that could indicate model misappropriation. Hence, when reporting $\max$ likelihoods for 20NG over a finite dataset (e.g. $\max_{\vx,\vy \in \gD_{\sF}} L(\vy|\vx)$) we instead use the 99.99th quantile and report it as $\max$.

\textbf{Training. \ } We use a pre-trained Gemma 2 tokenizer for both models which has a vocabulary size of 256k tokens.
For the fine-tuned model $L$, we use a pre-trained decoder-only Gemma 2 2B (hosted on Hugging Face) as the starting point then fine-tune it to our ID dataset using LoRA adaptors which involved training an additional 10.4M parameters (0.4\% of the total parameters).We train $L$ with AdamW (weight decay 0.01) for 1536 steps with a cosine learning rate schedule with 64 steps warmup, a maximum learning rate of 5e-5, scheduled for 32 epochs. We train with 256 context window using next-token prediction.

For the model $G$, we use a decoder-only GPT-small model architecture,  6 layers, 6 heads and 384 embedding dimensions and a total of 109.3M parameters, which we train from scratch using the ID data exclusively. We train $G$ with AdamW (weight decay 0.01) for 320 steps with a cosine learning rate schedule with 100 steps warm-up, a maximum learning rate of 3e-4, scheduled for 100 epochs. We train with 256 context window using next-token prediction.

\textbf{Inference. \ } For both $L$ and $G$ we use a default temperature of 1. We do not perform \emph{TopK} token selection. When evaluating performance, we use a 128-token long prompt and a 128-token long ground truth response.

\subsection{TinyShakespeare Setup}\label{app:tiny-shakespeare-setup}
\textbf{Dataset Cleaning. \ } The formatting in TinyShakespeare dataset was distinctly different to other texts with long sequences of line breaks and usage of all-caps for character names. We removed these excessive line breaks and changed the character names from all caps to title case to make it similar to other datasets and make OOD detection more challenging.

\textbf{Training. \ } We use a pre-trained Gemma-2 tokenizer for both models which has a vocabulary size of 256k tokens. For the fine-tuned model $L$, we use a pre-trained decoder-only Gemma-2-2B as the starting point then fine-tune it to our ID dataset using LoRA adaptors which involved training an additional 10.4M parameters (0.4\% of the total parameters). We train $L$ with AdamW (weight decay 0.01) for 128 steps with a cosine learning rate schedule with 64 steps warm-up, a maximum learning rate of 5e-5, scheduled for 32 epochs. We train with 256 context window using next-token prediction.

For the model $G$, we use a decoder-only GPT-micro model architecture,  4 layers, 4 heads and 128 embedding dimensions and a total of 33.7M parameters, which we train from scratch using the ID data exclusively. We train $G$ with AdamW (weight decay 0.01) for 2400 steps with a cosine learning rate schedule with 300 steps warm-up, a maximum learning rate of 3e-4, scheduled for 300 epochs. We train with 256 context window using next-token prediction.

\textbf{Inference. \ } For both $L$ and $G$ we use a default temperature of 1. We do not perform \emph{TopK} token selection. When evaluating performance, we use a 128-token long prompt and a 128-token long ground truth response.

\subsection{MedicalQA}\label{app:medqa-setup}

We apply our method to medical question answering as target domain, $\sT$. This could, for example, be extended to a chatbot for clinicians to research patient symptoms. To model potential questions and answers, we use the PubMedQA dataset \citep{jin_pubmedqa_2019} as $\gD_{\sT}$, which contains approximately 200K QA pairs for training and 1000 test pairs. We regard question answering on other topics, such as geography or computer science as $\sF$. To model this, we use the Stanford Question and Answering Dataset (excluding medical categories) \citep{rajpurkar_squad_2016} as $\gD_{\sF}$.

\textbf{Training. \ } As a generalist LLM, $L$, we use a LLama-3-8B model \citep{aimeta_llama_2024} and train a custom GPT-2 model (184M parameters) for $G$ \citep{radford_language_2019}. We pre-train $G$ on PubMedQA \citep{jin_pubmedqa_2019} with 200K sequences. We then use 100K prompts from PubMedQA to generate sequences using $L$ and then fine-tune on them using responses from $L$ to half the prompts in PubMedQA. As $G$ embeds the responses, $G(\vy)$, we fine-tune using ``BOS[Response]'' rather than entire sequences. We pre-train with a learning rate of 0.0001 for 50 epochs and then fine-tune with a learning rate of 0.00001 for another 50 epochs. On 8 $\times$ H100, the total training takes about 2 hours.

\textbf{Inference. } We perform inference without $top_k$ or $top_p$ parameters and with temperatures of $1.0$ for model $L$ and $G$. We prompt using the natural questions as defined by the datasets. For the analysis, we remove responses from SQuAD that are not clearly out-of-domain. For example, the response ``10 million people every year'' is not only a valid response to a geographical question, but can also be an information about the prevalence of the disease. When applying our method, we focus on responses with at least 10 tokens to further remove ambiguous sequences. Modern LLMs tend to be very verbose in their responses, so responses should naturally be longer than 10 tokens.


\subsection{Dataset Categories}\label{app:dataset-categories}

We list here the categories excluded from SQuAD and included in MMLU for reproducibility.

\begin{table}[H]
    \centering
    \include{assets/dataset_categories}
    \caption{Categories of items in used Datasets.}
    \label{tab:dataset_categories}
\end{table}

\section{Experimental Results}\label{app:results}

\subsection{\CT Results}\label{app:chartask-results}


\begin{figure}[H]
    \centering
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/chartask/likelihood_ratio_hist}
        \subcaption{}\label{fig:app-ct-log-ratio-histogram}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/chartask/constriction_ratio_metrics}
        \subcaption{}\label{fig:app-ct-ood-vs-certification}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/chartask/epsilon_dc_metrics}
        \subcaption{}\label{fig:ct-acc-dc}
    \end{minipage}
    \caption{This Figure replicates Figure~\ref{fig:medqa-results-overall} for the \CT dataset.}
    \label{fig:ct-results-overall}
\end{figure}



\subsection{TinyShakespeare Results}\label{app:tiny-shakespeare-results}


\begin{figure}[H]
    \centering
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/tinyshakespeare/TS_likelihood_ratio_hist}
        \subcaption{}\label{fig:app-ts-log-ratio-histogram}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/tinyshakespeare/TS_constriction_ratio_metrics}
        \subcaption{}\label{fig:app-ts-ood-vs-certification}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/tinyshakespeare/TS_epsilon_dc_metrics}
        \subcaption{}\label{fig:ts-acc-dc}
    \end{minipage}
    \caption{This Figure replicates Figure~\ref{fig:medqa-results-overall} for the TinyShakespeare dataset.}
    \label{fig:ts-results-overall}
\end{figure}

\subsection{20NG}


\begin{figure}[H]
    \vspace{-10pt}
    \centering
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/20ng/NG20_likelihood_ratio_hist}
        \subcaption{}\label{fig:app-20ng-log-ratio-histogram}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/20ng/NG20_constriction_ratio_metrics}
        \subcaption{}\label{fig:app-20ng-ood-vs-certification}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/20ng/NG20_epsilon_dc_metrics}
        \subcaption{}\label{fig:app-20ng-acc-dc}
    \end{minipage}
    \vspace{-5pt}
    \caption{Figure \ref{fig:app-20ng-log-ratio-histogram} shows that log likelihood ratios are well disentangled. Figure \ref{fig:app-20ng-ood-vs-certification} shows the trade-off between OOD and certification: The best OOD detection performance occurs with a constriction ratio of 60. Figure \ref{fig:app-20ng-acc-dc} shows the false rejection rate (FRR) required to certify at a given $\epsilon$.}
    \label{fig:app-20ng-results-overall}
\end{figure}

\subsection{Medical QA}\label{app:medqa-results}



\begin{figure}[H]
    \vspace{-10pt}
    \centering
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/medqa/likelihood_ratio_hist_MedicalQA_generated.pdf}
        \subcaption{}\label{fig:app-medqa-log-ratio-histogram}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/medqa/constriction_ratio_metrics_MedicalQA_generated.pdf}
        \subcaption{}\label{fig:app-medqa-ood-vs-certification}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/medqa/epsilon_dc_metrics_MedicalQA_generated.pdf}
        \subcaption{}\label{fig:app-medqa-acc-dc}
    \end{minipage}
    \vspace{-5pt}
    \caption{Figure \ref{fig:app-medqa-log-ratio-histogram} shows that log likelihood ratios are well disentangled. Figure \ref{fig:app-medqa-ood-vs-certification} shows the trade-off between OOD and certification. Figure \ref{fig:app-medqa-acc-dc} shows the false rejection rate (FRR) required to certify at a given $\epsilon$. All results are for \ouralg with $T=1$ for Medical QA.}
    \label{fig:app-medqa-results-overall}
\end{figure}


\begin{figure}[H]
    \vspace{-10pt}
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/medqa/log_likelihood_generator_vs_response_length_MedicalQA_generated.pdf}
        \subcaption{}\label{fig:app-medqa-likelihood_generator}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/medqa/log_likelihood_model_vs_response_length_MedicalQA_generated.pdf}
        \subcaption{}\label{fig:app-medqa-likelihood_llm}
    \end{minipage}
    \vspace{-5pt}
    \caption{This figure demonstrates the gap in log likelihood between in-domain and out-of-domain samples for the guide models $G$ in Figure \ref{fig:app-medqa-likelihood_generator} and the LLM $L$ in Figure \ref{fig:app-medqa-likelihood_llm}. As the length of the response, $N_{\vy}$, increases, the gap between ID ($\gD_{\sT}$) and OOD data ($\gD_{\sF}$) widens. The log-likelihood decreases roughly linearly. Thus, the guide model $G$ on the left side assigns exponential decreasing probabilities to OOD samples.}
    \label{fig:app-medqa-log-likelihoods}
\end{figure}


\subsection{Constriction Ratios for Different False Rejection Rates}\label{app:cr-other-frr}

\begin{figure}[H]
    % \vspace{-10pt}
    \include{assets/cr-histograms-appendix}
    \vspace{-5pt}
    \caption{This figure shows the $log_{10}$ constriction ratios (CR) on OOD samples as a function of the false rejection rate (FRR) on the in-domain samples. The rejection threshold $k$ is systematically decreased from top to bottom to achieve a given FRR. We can observe the gradual improvement in constriction while increasing the FRR. (a) shows Tiny Shakespeare, (b) shows 20NG, and (c) Medical QA.}
    \label{fig:app-cr-combined-results}
\end{figure}


\subsection{Atomic Certificates - Length Controlled}\label{app:cr-len-controlled}



\begin{wrapfigure}{r}{0.42\textwidth}
    \vspace{-8pt}
  \begin{center}
    \includegraphics[width=\linewidth]{assets/response_lengths_MedicalQA.pdf}
  \end{center}
  \vspace{-8pt}
  \caption{MedQA setup: The in-domain dataset (PubMedQA) has longer responses than the OOD dataset (SQuAD).}
  \label{fig:app-response-length-hist-medqa}
  % \vspace{-5pt}
\end{wrapfigure}
The experimental setup for MedicalQA uses PubMedQA as in-domain dataset and SQuAD as out-of-domain dataset as described in Section~\ref{sec:experimental-setup} and Appendix~\ref{app:experimental-setup}. The different lengths of responses in these datasets confound our findings on the disentanglement of the atomic certificates, $\epsilon_{\vy}$-ACs between in-domain data, $\gD_{\sT}$, and out-of-domain data, $\gD_{\sF}$. In Figure~\ref{fig:app-response-length-hist-medqa}, we show that sequences tend to be a lot shorter in $\gD_{\sF}$ than in $\gD_{\sT}$. As the likelihood of a response decays exponentially in the length of the responses, the responses in the OOD set $\gD_{\sF}$ have relatively high likelihood that is not attributable to the domain restriction, but rather to the length of the response. This results in the eCDFs in Figure~\ref{fig:20ng-ac-ecdf} overlapping significantly. To show that this is a confounding factor that is indeed worsening disentanglement, we resample the data to account for length and present results here.

\textbf{Setup. \ }
We resample the in-domain data, $\gD_{\sT}$, and out-of-domain data, $\gD_{\sF}$ to have matching distribution of response lengths.
We find the target distribution using the following steps: First, we find the common support between the distribution of response length $N_{\vy}$ between $\gD_{\sT}$ and $\gD_{\sF}$, $N_{\vy} \in [15, 38]$. This interval covers 67\% of samples in the target domain dataset and 58\% of the OOD dataset. Second, we obtain the empirical distribution of $N_{\vy}$ in the in-domain dataset, perform Laplace smoothing \citep{manning_introduction_2008} with $\alpha=1$ and then further smooth the distribution using a moving average with a window length of $5$. Third, we perform weighted sampling with replacement from $\gD_{\sT}$ and $\gD_{\sF}$ with a size of 100 times the original. The sampling weights are computed s.t. the distribution of $N_{\vy}$ matches the target distribution. We denote these \textbf{r}e\textbf{s}ampled sets as $\gD_{\sT}^{RS}$ and $\gD_{\sF}^{RS}$.

\begin{wrapfigure}{r}{0.42\textwidth}
    \vspace{-17pt}
  \begin{center}
    \includegraphics[width=\linewidth]{assets/epsilon_ac_ecdf_frr_0.10_length_sampled_MedicalQA.pdf}
  \end{center}
  \vspace{-8pt}
    \caption{The eCDFs of $\epsilon_{\vy}$-ACs are shown for the original in- and out-of-domain data for the MedQA setup in comparison to a resampled dataset controlling the response length as confounder. The gap between the permissiveness of in-domain samples and restrictiveness on out-of-domain samples is greatly improved.}
    \label{fig:app-ecdf-length-controlled}
  \vspace{-30pt}
\end{wrapfigure}

\textbf{Results.  } We find that the disentanglement of atomic certificates, $\epsilon_{\vy}$-ACs, improves greatly after eliminating the confounding factor response lengths. Figure~\ref{fig:app-ecdf-length-controlled} shows the empirical cumulative distribution functions (eCDFs) for ``original'' datasets, $\gD_{\sT}$ and $\gD_{\sF}$ in gray tones, as well as the results for $\gD_{\sT}^{RS}$ and $\gD_{\sF}^{RS}$. You may observe that the distribution of ACs shifted left for datasets representing $\sF$ and shifted right for datasets representing $\sT$, effectively increasing the disentanglement. This indicates that, when comparing \emph{similar} in-domain and out-of-domain samples, the gap in restriction is larger than presented in Figure~\ref{fig:medqa-ac-ecdf}. ACs on in-domain samples are more \emph{permissiveness} and ACs on out-of-domain samples even more \emph{constrictive} than it initially appeared.


\subsection{Atomic Certificate by Likelihood}\label{app:cr-by-likelihood}

Obtaining a tight atomic certificate for a sample $\vy$ is most important when the sample is likely proposed by $L$. Hence, in this section we study the log constriction ratio, the tightening of our adversarial certificate over model $L$, as a function of the sample's likelihood under $L$.

We bin out-of-domain samples into 10 bins based on their log likelihood under model $L$, i.e. $\log L(\vy|\vx)$, and compute median, 25th and 75th percentile log constriction ratio, as well as the median log likelihood. We present results in Figure~\ref{fig:cr-by-likelihood} for both 20NG and TinyShakespeare. We observe that the constriction strengthens when samples get more likely under $L$. That means, those samples most likely to be sampled under $L$ benefit most from our atomic certificate. We consider this to be a favorable result.

\begin{figure}[H]
  \centering
  \begin{subfigure}[b]{0.42\textwidth}
    \includegraphics[width=\textwidth]{assets/20ng/NG20_imp_decile_OOD.pdf}
    \tightcaption
    \caption{}
    \label{fig:cr-by-likelihood-20ng}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.42\textwidth}
    \includegraphics[width=\textwidth]{assets/tinyshakespeare/TS_imp_decile_OOD.pdf}
    \tightcaption
    \caption{}
    \label{fig:cr-by-likelihood-ts}
  \end{subfigure}
  \vspace{-5pt}
  \caption{These figures show the constriction ratio as a function of log likelihood of samples under $L$ for out-of-distribution samples. Figure \ref{fig:cr-by-likelihood-20ng} displays the results for 20NG and Figure \ref{fig:cr-by-likelihood-ts} for TinyShakespeare. We bin all samples into 10 bins. For each bin the $x$-axis shows the median log likelihood of the sample under $L$, $\log L(\vy|\vx)$. The $y$-axis shows the $\log_{10}$ constriction ratio (median and percentiles for each bin). }
  \label{fig:cr-by-likelihood}
  \vspace{-5pt}
\end{figure}


\section{Repeated Sampling ($T>1$)}\label{app:t-vs-k}

In Section~\ref{sec:results-generation} we study the performance of \ouralg by sampling from $L$ with a single step, that is, $T=1$. Here, we extend the analysis to $T>1$.

\textbf{Setup. \ }
We adopt the MedicalQA setup as described in Appendix~\ref{app:medqa-setup}. However, instead of employing \ouralg with $T=1$, we use $T \in \{1,2,3,4,5\}$ and study the resulting $\epsilon-DC$ for combinations of $k$ (the rejection threshold of \ouralg). As above, for ease of presentation we use a fixed temperature of $1.0$ for $L$.

\textbf{Results. \ }
We find that increasing $T$ significantly reduces false rejection rates (FRR) while only marginally increasing the $\epsilon$-DC (domain certificate). We present findings for the FRR in Figure~\ref{fig:app-medqa-T-vs-k-FRR} and for $\epsilon$-DC in Figure~\ref{fig:app-medqa-T-vs-k-eps}. The minor increase in $\epsilon$ due to increasing $T$ should not come as a surprise as we recall the formula for the upper bound: $2^{kN_{\vy}}TG(y)$ (see \eqref{eq:upper_bound}). Even $T=10$ increases the upper bound $\epsilon_y$ by only one order of magnitude. On the other hand, the gains in in-domain performance are marked. In Figure~\ref{fig:app-medqa-T-vs-k-FRR}, we can observe the FRR is roughly halved for $T=5$ and $k>2$, greatly improving the refusal behavior of the model on in-domain samples. Finally, we note that the temperature of $L$, $t_L$, is a confounding factor. For $t_L \rightarrow 0$, we would perform (nearly) deterministic sampling of $\vy|\vx$ and therefore $T>1$ would not have any benefit.


\begin{figure}[h]
    \centering
    % First subfigure
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/medqa/FRR_given_k_and_TMedicalQA_generated.pdf}
        \tightcaption
        \caption{}
        \label{fig:app-medqa-T-vs-k-FRR}
    \end{subfigure}
    \hfill
    % Second subfigure
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/medqa/eps_given_k_and_TMedicalQA_generated.pdf}
        \tightcaption
        \caption{}
        \label{fig:app-medqa-T-vs-k-eps}
    \end{subfigure}
    % Main caption
    \vspace{-5pt}
    \caption{False Rejection Rate (FRR) (a) and  $\epsilon-DC$ of the Domain certificate of VALID (b) plotted for a range of  different values of $T$ and $k$.}
    \label{fig:app-medqa-T-vs-k}
\end{figure}


\section{Ablation}\label{app:ablation}

\subsection{Comparing $M$ to $G$}\label{app:chartask-ablation}

Our goal is to provide a guarantee on a generalist model assuming that such a model outperforms custom, small solutions that are inherently safer due to their domain specific training. We test this empirically by examining the  gap in performance between the generalist model $L$, a small in-domain model. As $G$ is trained marginally on $\vy$, it is not able to perform any task. Hence, we exactly replicate the training procedure of $G$ and train a model on the entire sequence, $G'(\vx,\vy)$. We utilize the \CT dataset as described above and study the accuracy of each model in generating valid sequences: A valid sequence is one that starts with \texttt{Q}, is followed by a random sequence of characters (e.g. \texttt{5 3}), followed by four unique task tokens (e.g. \texttt{S A E R}) defining a task, which is then performed (e.g. \texttt{3 5}). The sequence is expected to terminate there. If \emph{any} of these are violated, the generated sequence is scored as invalid. We perform inference on 1000 prompts from the target domain test dataset prompting the model with various lengths of prompts. In Table~\ref{tab:accuracy-chartask-ablation}, we present the results: The accuracy of generating such sequences of $L$ lies significantly above that of $G$ (difference of approx 30\%). This shows that $G$ is effective in restricting the domain while performing considerably worse than $L$. Hence, our method combines the best of both models: The safety of $G$ with the performance of $L$.

\begin{table}[h!]
    \centering
    \begin{tabular}{l c c}
        \toprule
        {\textbf{Prompt Length}}
        & \textbf{G} & \textbf{L} \\
        \midrule
        1 & 60.45 & 91.21 \\
        5 & 60.25 & 92.68 \\
        10 & 66.89 & 91.11 \\
        \bottomrule
    \end{tabular}
    \caption{Accuracy scores for \CT generation dataset.}
    \label{tab:accuracy-chartask-ablation}
\end{table}

\subsection{Benefit of larger Guide Models}\label{app:ablation-size-G}


In this Appendix, we study the influence of the size of $G$ on the VALID results. In particular, we ask whether VALID benefits from smaller or larger models.

\textbf{Setup. }
We turn to our MedicalQA setup as described in Section~\ref{sec:experimental-setup} and Appendix~\ref{app:medqa-setup}. With the same methodology, we fit two more models for $G$. $G_{XS}$ follows a GPT-2 architecture with 6 layers, 6 heads and 192 embedding dimension resulting in 27.49M parameters. $G_{S}$ follows a GPT-2 architecture with 6 layers, 6 heads and 384 embedding dimensions resulting in 60.29M parameters. To recap, the $G$ model as used above uses 12 layers, 12 heads and 768 embedding dimension resulting in 184M parameters. We then compare the three models on samples generated by $L$ following Section~\ref{sec:results-generation}.


\begin{figure}[b]
    \centering
    % First subfigure
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/medqa/k_vs_frr_MedicalQA_generated.pdf}
        \caption{}
        \label{fig:app-medqa-g-size-k-frr}
    \end{subfigure}
    \hfill
    % Second subfigure
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/medqa/epsilon_dc_metrics_MedicalQA_generated_ablation.pdf}
        \caption{}
        \label{fig:app-medqa-g-size-e-dc}
    \end{subfigure}
    % Main caption
    \caption{These Figures demonstrate differences in the behavior of VALID for different sizes of guide models $G$. Figure~\ref{fig:app-medqa-g-size-k-frr} shows that larger models allow for lower $k$ and hence lower bounds at the same False Rejection Rate (FRR). Figure~\ref{fig:app-medqa-g-size-e-dc} shows the FRR ($y$-axis) for a given $\epsilon$-DC for guide models of different sizes.}
    \label{fig:main}
\end{figure}


\textbf{Results. } We find that larger models tend to perform better, however, the evidence is not strong. First, we study the rejection threshold $k$ per model. As described in \eqref{eq:upper_bound} in Theorem~\ref{thm:valid}, VALID's upper bounds gets tighter with smaller $k$. Hence, in Figure ~\ref{fig:app-medqa-g-size-k-frr} we plot $k$ values achieving a given false rejection rate (FRR) for each model. We observe that larger models enable smaller $k$ at the same FRR. This indicates that the trade-off in $k$ between certification and OOD detection is more favorable under larger models. This should not come as a surprise, as larger models tend to achieve better perplexity (i.e. lower loss) on in-domain data.

Next, we study the constriction ratios of the Atomic Certificates (AC) and present results for different sizes of $G$ as shown in Table~\ref{tab:app-medqa-size-g-CR}. For each model, we provide the the 10th percentile, median and 90th percentile. You may observe that $G_{XS}(y)$ consistently provides constriction ratios that are are often around 10 orders of magnitudes worse than $G_S(y)$ and $G(y)$. Interestingly, $G_S(y)$ yields better ratios than $G(y)$. However, the difference is smaller. We speculate that the limited amount of ID training data means we do not see benefits for increasing the size of $G$ beyond a point, as it begins to overfit without increasing regularization.

Finally, we study the Domain Certificates (DC) for each model. For this we replicate Figure~\ref{fig:app-medqa-acc-dc} and present Figure~\ref{fig:app-medqa-g-size-e-dc} showing the false rejection rate (FRR) given an $\epsilon$-DC for the three models. We may observe that the lower bound to the FRR significantly increases as the models become smaller. The evidence here suggests that larger guide models yield better domain certificates.

In conclusion, the evidence points to larger models working better for an application like MedQA. The evidence uniformly shows that a model as small as $G_{XS}(y)$ does perform significantly worse than larger models.



\begin{table}[h]
    \centering
    \include{assets/constriction-ratio-table-size-g-ablation}
    \caption{Constriction Ratios for MedicalQA for three models of different sizes. The smallest model yields significantly worse (lower) constriction ratios.}
    \label{tab:app-medqa-size-g-CR}
\end{table}


\section{Benchmarking}\label{app:benchmark}

In this section, we provide a comprehensive description of the PubMedQA experimental setup presented in Section~\ref{sec:results-benchmarks}, present additional benchmarking results, and extend our evaluation framework to the MMLU benchmark \citep{hendrycks_measuring_2021}.


\subsection{PubMedQA}
\textbf{Setup. \ } The PubMedQA benchmark \citep{jin_pubmedqa_2019} comprises 1000 items. Each item contains background information (context), a multiple-choice question (answerable by yes/no/maybe), a long-text answer, and a ground truth label (yes/no/maybe). As illustrated in Figure~\ref{fig:pubmedqa-bench-at-eps-schematic}, we evaluate the model through two streams: ``item correctness'' and ``response acceptance''. In both streams, we prompt the model with the context and question.  In the ``item correctness'' stream, the model is provided with all multiple-choice tokens, and the maximum likelihood answer is selected and evaluated for correctness. In the ``response acceptance'' stream, we present the long-text answer as a response and determine if model $M$ abstains at a given domain certificate of $\epsilon$. An item is considered correct at $\epsilon$ if and only if the response is accepted and the model scores correctly. We use the reasoning-required variant of the PubMedQA benchmark (for further details, see \cite{jin_pubmedqa_2019}).

\textbf{Results.  } Extending our analysis of the PubMedQA benchmark presented in Section~\ref{sec:results-benchmarks}, we examined the relationship between PubMedQA performance scores and median constriction ratios. As illustrated in Figure~\ref{fig:app-medqa-pubmedqa-bench-cr}, our findings demonstrate that the model can achieve a $\log_{10}$ constriction ratio of 20 on samples in $\gD_{\sF}$ while maintaining robust PubMedQA performance. Specifically, at a performance threshold of 70\% accuracy, we observed a $\log_{10}CR_k$ value of 21.6, which effectively constrains out-of-domain samples to probabilities at least $1 \times 10^{-21}$ times lower than the likelihood of samples in distribution $L$. This indicates a strong capacity for domain constriction while preserving task performance.


\begin{figure}[t]
    \vspace{-10pt}
    \centering
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/medqa/benchmark_cr_MedicalQA_PubMedQA.pdf}
        \tightcaption
        \subcaption{}\label{fig:app-medqa-pubmedqa-bench-cr}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/medqa/benchmark_epsilon_dc_MedicalQA_MMLU.pdf}
        \tightcaption
        \subcaption{}\label{fig:app-medqa-mmlu-eps}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/medqa/benchmark_cr_MedicalQA_MMLU.pdf}
        \tightcaption
        \subcaption{}\label{fig:app-medqa-mmlu-cr}
    \end{minipage}
    \vspace{-5pt}
    \caption{ Evaluation of domain-certified models through standardized benchmarking. Figure~\ref{fig:app-medqa-pubmedqa-bench-cr} illustrates the association between $\log$ constriction ratios and the PubMedQA@$\epsilon$ benchmark scores across models with varying $\epsilon$-DC certifications. Figure~\ref{fig:app-medqa-mmlu-eps} presents the MMLU@$\epsilon$ metric evaluated at different certification thresholds $\epsilon$. (c) Figure~\ref{fig:app-medqa-mmlu-cr} shows the relationship between $\log$ constriction ratios and corresponding MMLU@$\epsilon$ scores across multiple certification levels.}
\end{figure}



\subsection{MMLU-Med}

\begin{figure}[b]
    \centering
    \includegraphics[width=\linewidth]{assets/mmlu_med_schematic.pdf}
    \vspace{-48pt}
    \caption{The MMLU@$\epsilon$ benchmark assesses MMLU performance while satisfying $\epsilon$-DC certificate. The correctness is scored as commonly done for MMLU (left). The correct question answer pair is checked for acceptance / rejection by $M$. Only if a sample is accepted and correct, the question is scored positively. For questions not ending in ``?'', the sentence is concatenated without keywords.}
    \label{fig:app-mmlu-at-eps-schematic}
    % \vspace{-20pt}
\end{figure}

In this section, we extend the benchmarking of our certified model $M$ for medical question answering to the MMLU benchmark \citep{hendrycks_measuring_2021}. To that end, we follow the same methodology as above for the PubMedQA benchmark. In an earlier version of this work, we reported MMLU results that were erroneous, which we correct here.

\textbf{Setup. \ } MMLU comprises thousands of questions spanning various domains of general and professional factual knowledge. As our model $M$ is deployed for medical questions, we focus on a subset of MMLU categories that fall within our domain $\sT$. We specify the selected categories in Table~\ref{tab:dataset_categories} and designate this remaining benchmark as MMLU-Med.

MMLU's standard format provides $n$-shot examples with four possible answers (A through D) followed by a question in the same format. The model is then prompted to select the correct response. However, this setup does not reflect a realistic user-system interaction. Therefore, similar to PubMedQA, we introduce the MMLU-Med@$\epsilon$ metric, which separates the evaluation into two streams: (1) standard assessment of model $L$ on MMLU-Med to determine correctness, and (2) testing whether the correct question-answer pair is rejected by our algorithm. The process is summarized in Figure~\ref{fig:app-mmlu-at-eps-schematic}. We score an item as correct when the model scores correctly while maintaining its $\epsilon-DC$ on the realistic question-answer pair.


\textbf{Results. \ } Our evaluation yields mixed evidence regarding the model's performance on MMLU-Med. Following the same analysis as for PubMedQA in Section~\ref{sec:results-benchmarks}, we present the MMLU-Med@$\epsilon$ metric in Figure~\ref{fig:app-medqa-mmlu-eps}. As shown, MMLU-Med@$1=37.1$\%, that is, the model retains $37.1$\% accuracy when certified at $\epsilon=1$, or $\log_{10}\epsilon=0$. The $10^{-10}$-DC model achieves a score of 14.1\%. In addition, to the domain certificates, we study the median constriction of our model in relation to its certified performance. The evidence provided in Figure~\ref{fig:app-medqa-mmlu-cr} indicates that a median constriction ratio of $1 \times 10^{-5}$ is achieved on out-of-domain samples together with a score of 65\% on the MMLU-Med@$\epsilon$ benchmark. Further, a median constriction of $1 \times 10^{-20}$ is achieved with an MMLU-Med@$\epsilon$ score of 37\%.

These results are considerably weaker than the strong results presented above for PubMedQA raising the question as to why this is. In Figure~\ref{fig:app-medqa-mmlu-likelihood}, we investigate the domain shift between PubMedQA and MMLU. In particular, Figure~\ref{fig:app-medqa-mmlu-likelihood-G} shows the distribution of log likelihoods of in-domain samples ($\gD_{\sT}$), out-of-domain samples ($\gD_{\sF}$) and MMLU samples under guide model $G$ and Figure~\ref{fig:app-medqa-mmlu-likelihood-ratio} shows the log likelihood ratios for the same samples. We observe a considerable overlap between the distributions of MMLU-Med and PubMedQA samples. However, the distribution of MMLU-Med has a long-tail into the distribution of $\gD_{\sF}$. This explains our results quite well. On the one hand, the large overlapping mass of MMLU-Med and PubMedQA explains why $M$ accepts a wide range of MMLU-Med responses while significantly constricting the model on $\gD_{\sF}$. On the other hand, the long tail of the distribution of MMLU scores into the distribution of $\gD_{\sF}$ indicates a range of MMLU-Med questions will be rejected unless the certificates become vacuous, making it hard challenging high MMLU-Med@$\epsilon$ performance.
We believe that training $G$ on MMLU-style QA pairs would significantly improve results but leave this as a future direction.

\begin{figure}[H]
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/medqa/benchmark_mmlu_ll2_generator_hist.pdf}
        \tightcaption
        \subcaption{}\label{fig:app-medqa-mmlu-likelihood-G}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/medqa/benchmark_mmlu_log2_ratio_norm_hist.pdf}
        \tightcaption
        \subcaption{}\label{fig:app-medqa-mmlu-likelihood-ratio}
    \end{minipage}
    \vspace{-5pt}
    \caption{Comparison of likelihood of three datasets under model $G$, showing a MMLU-Med exhibits domain shift relative to PubMedQA. Figure~\ref{fig:app-medqa-mmlu-likelihood-G} indicates likelihood of MMLU-Med lies in-between the in-domain data $\gD_{\sT}$ (PubMedQA) and out-of-domain data $\gD_{\sF}$. Figure~\ref{fig:app-medqa-mmlu-likelihood-ratio} shows the normalized log likelihood ratio used in \ouralg lead to frequent rejections due to domain shift.}
    \label{fig:app-medqa-mmlu-likelihood}
\end{figure}

\end{document}
