@misc{askell_general_2021,
	title = {A {General} {Language} {Assistant} as a {Laboratory} for {Alignment}},

	author = {Askell, Amanda and Bai, Yuntao and Chen, Anna and Drain, Dawn and Ganguli, Deep and Henighan, Tom and Jones, Andy and Joseph, Nicholas and Mann, Ben and DasSarma, Nova and Elhage, Nelson and Hatfield-Dodds, Zac and Hernandez, Danny and Kernion, Jackson and Ndousse, Kamal and Olsson, Catherine and Amodei, Dario and Brown, Tom and Clark, Jack and McCandlish, Sam and Olah, Chris and Kaplan, Jared},
	year = {2021},
	note = {ArXiv: 2112.00861},
	keywords = {ONLY-ARXIV},
}

@misc{bai_training_2022,
	title = {Training a {Helpful} and {Harmless} {Assistant} with {Reinforcement} {Learning} from {Human} {Feedback}},

	author = {Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and Joseph, Nicholas and Kadavath, Saurav and Kernion, Jackson and Conerly, Tom and El-Showk, Sheer and Elhage, Nelson and Hatfield-Dodds, Zac and Hernandez, Danny and Hume, Tristan and Johnston, Scott and Kravec, Shauna and Lovitt, Liane and Nanda, Neel and Olsson, Catherine and Amodei, Dario and Brown, Tom and Clark, Jack and McCandlish, Sam and Olah, Chris and Mann, Ben and Kaplan, Jared},
	year = {2022},
	note = {ArXiv: 2204.05862},
}

@misc{bengio_bounding_2024,
	title = {Bounding the probability of harm from an {AI} to create a guardrail - https://yoshuabengio.org/2024/08/29/bounding-the-probability-of-harm-from-an-ai-to-create-a-guardrail/},
	abstract = {As we move towards more powerful AI, it becomes urgent to better understand the risks, ideally in a mathematically rigorous and quantifiable way, and use…},
	language = {en-CA},
	urldate = {2024-09-06},
	journal = {Yoshua Bengio},
	author = {Bengio, Yoshua},
	month = aug,
	year = {2024},
}

@misc{casadio_nlp_2024,
	title = {{NLP} {Verification}: {Towards} a {General} {Methodology} for {Certifying} {Robustness}},

	author = {Casadio, Marco and Dinkar, Tanvi and Komendantskaya, Ekaterina and Arnaboldi, Luca and Daggitt, Matthew L. and Isac, Omri and Katz, Guy and Rieser, Verena and Lemon, Oliver},
	year = {2024},
	note = {ArXiv: 2403.10144},
}

@inproceedings{chaudhary_quantitative_2024-1,
	title = {Quantitative {Certification} of {Knowledge} {Comprehension} in {LLMs}},

	booktitle = {{ICLR} 2024 {Workshop} on {Secure} and {Trustworthy} {Large} {Language} {Models}},
	author = {Chaudhary, Isha and Jain, Vedaant V. and Singh, Gagandeep},
	year = {2024},
	note = {ArXiv: 2402.15929},
}

@misc{chua_flexible_2024,
	title = {A {Flexible} {Large} {Language} {Models} {Guardrail} {Development} {Methodology} {Applied} to {Off}-{Topic} {Prompt} {Detection}},

	author = {Chua, Gabriel and Chan, Shing Yee and Khoo, Shaun},
	year = {2024},
	note = {ArXiv: 2411.12946},
}

@inproceedings{dai_safe_2024,
	title = {Safe {RLHF}: {Safe} {Reinforcement} {Learning} from {Human} {Feedback}},

	booktitle = {The {Twelfth} {International} {Conference} on {Learning} {Representations}},
	author = {Dai, Josef and Pan, Xuehai and Sun, Ruiyang and Ji, Jiaming and Xu, Xinbo and Liu, Mickel and Wang, Yizhou and Yang, Yaodong},
	year = {2024},
}

@misc{dong_safeguarding_2024,
	title = {Safeguarding {Large} {Language} {Models}: {A} {Survey}},

	author = {Dong, Yi and Mu, Ronghui and Zhang, Yanghao and Sun, Siqi and Zhang, Tianle and Wu, Changshun and Jin, Gaojie and Qi, Yi and Hu, Jinwei and Meng, Jie and Bensalem, Saddek and Huang, Xiaowei},
	year = {2024},
	note = {ArXiv: 2406.02622},
}

@inproceedings{freiberger_fairness_2024,
	title = {Fairness certification for natural language processing and large language models},
	booktitle = {Intelligent {Systems} {Conference}},
	publisher = {Springer},
	author = {Freiberger, Vincent and Buchmann, Erik},
	year = {2024},
	pages = {606--624},
}

@inproceedings{gangal_likelihood_2020,
	title = {Likelihood ratios and generative classifiers for unsupervised out-of-domain detection in task oriented dialog},
	volume = {34},

	urldate = {2024-09-05},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Gangal, Varun and Arora, Abhinav and Einolghozati, Arash and Gupta, Sonal},
	year = {2020},
	pages = {7764--7771},
}

@inproceedings{hendrycks_baseline_2017,
	title = {A {Baseline} for {Detecting} {Misclassified} and {Out}-of-{Distribution} {Examples} in {Neural} {Networks}},

	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Hendrycks, Dan and Gimpel, Kevin},
	year = {2017},
}

@misc{inan_llama_2023,
	title = {Llama {Guard}: {LLM}-based {Input}-{Output} {Safeguard} for {Human}-{AI} {Conversations}},

	author = {Inan, Hakan and Upasani, Kartikeya and Chi, Jianfeng and Rungta, Rashi and Iyer, Krithika and Mao, Yuning and Tontchev, Michael and Hu, Qing and Fuller, Brian and Testuggine, Davide and Khabsa, Madian},
	year = {2023},
	note = {ArXiv: 2312.06674},
}

@misc{kaufmann_survey_2024,
	title = {A {Survey} of {Reinforcement} {Learning} from {Human} {Feedback}},

	author = {Kaufmann, Timo and Weng, Paul and Bengs, Viktor and Hüllermeier, Eyke},
	year = {2024},
	note = {ArXiv: 2312.14925},
}

@inproceedings{khosla_supervised_2020,
	title = {Supervised {Contrastive} {Learning}},
	volume = {33},

	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Khosla, Prannay and Teterwak, Piotr and Wang, Chen and Sarna, Aaron and Tian, Yonglong and Isola, Phillip and Maschinot, Aaron and Liu, Ce and Krishnan, Dilip},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
	pages = {18661--18673},
}

@misc{kumar_certifying_2024,
	title = {Certifying {LLM} {Safety} against {Adversarial} {Prompting}},

	author = {Kumar, Aounon and Agarwal, Chirag and Srinivas, Suraj and Li, Aaron Jiaxun and Feizi, Soheil and Lakkaraju, Himabindu},
	year = {2024},
	note = {ArXiv: 2309.02705},
}

@phdthesis{la_malfa_robustness_2023,
	type = {{PhD} {Thesis}},
	title = {On robustness for natural language processing},
	school = {University of Oxford},
	author = {La Malfa, E},
	year = {2023},
}

@inproceedings{lin_flats_2023,
	address = {Singapore},
	title = {{FLatS}: {Principled} {Out}-of-{Distribution} {Detection} with {Feature}-{Based} {Likelihood} {Ratio} {Score}},

	abstract = {Detecting out-of-distribution (OOD) instances is crucial for NLP models in practical applications. Although numerous OOD detection methods exist, most of them are empirical. Backed by theoretical analysis, this paper advocates for the measurement of the “OOD-ness” of a test case {\textbackslash}boldsymbolx through the \textit{likelihood ratio} between out-distribution {\textbackslash}mathcal P\_\textit{out} and in-distribution {\textbackslash}mathcal P\_\textit{in}. We argue that the state-of-the-art (SOTA) feature-based OOD detection methods, such as Maha and KNN, are suboptimal since they only estimate in-distribution density p\_\textit{in}({\textbackslash}boldsymbolx). To address this issue, we propose \textbf{FLATS}, a principled solution for OOD detection based on likelihood ratio. Moreover, we demonstrate that FLATS can serve as a general framework capable of enhancing other OOD detection methods by incorporating out-distribution density p\_\textit{out}({\textbackslash}boldsymbolx) estimation. Experiments show that FLATS establishes a new SOTA on popular benchmarks.},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Lin, Haowei and Gu, Yuntian},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {8956--8963},
}

@inproceedings{liu_energy-based_2020,
	title = {Energy-based {Out}-of-distribution {Detection}},
	volume = {33},

	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Liu, Weitang and Wang, Xiaoyun and Owens, John and Li, Yixuan},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
	pages = {21464--21475},
}

@inproceedings{podolskiy_revisiting_2021,
	title = {Revisiting {Mahalanobis} {Distance} for {Transformer}-{Based} {Out}-of-{Domain} {Detection}},

	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Podolskiy, A. V. and Lipin, Dmitry and Bout, A. and Artemova, E. and Piontkovskaya, Irina},
	year = {2021},
}

@article{rafailov_direct_2024,
	title = {Direct preference optimization: {Your} language model is secretly a reward model},
	volume = {36},
	journal = {Advances in Neural Information Processing Systems},
	author = {Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
	year = {2024},
}

@inproceedings{tang_generalized_2024,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Generalized {Preference} {Optimization}: {A} {Unified} {Approach} to {Offline} {Alignment}},
	volume = {235},

	abstract = {Offline preference optimization allows fine-tuning large models directly from offline data, and has proved effective in recent alignment practices. We propose generalized preference optimization (GPO), a family of offline losses parameterized by a general class of convex functions. GPO enables a unified view over preference optimization, encompassing existing algorithms such as DPO, IPO and SLiC as special cases, while naturally introducing new variants. The GPO framework also sheds light on how offline algorithms enforce regularization, through the design of the convex function that defines the loss. Our analysis and experiments reveal the connections and subtle differences between the offline regularization and the KL divergence regularization intended by the canonical RLHF formulation. In a controlled setting akin to Gao et al 2023, we also show that different GPO variants achieve similar trade-offs between regularization and performance, though the optimal values of hyper-parameter might differ as predicted by theory. In all, our results present new algorithmic toolkits and empirical insights to alignment practitioners.},
	booktitle = {Proceedings of the 41st {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Tang, Yunhao and Guo, Zhaohan Daniel and Zheng, Zeyu and Calandriello, Daniele and Munos, Remi and Rowland, Mark and Richemond, Pierre Harvey and Valko, Michal and Avila Pires, Bernardo and Piot, Bilal},
	editor = {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
	month = jul,
	year = {2024},
	pages = {47725--47742},
}

@inproceedings{uppaal_is_2023,
	title = {Is {Fine}-tuning {Needed}? {Pre}-trained {Language} {Models} {Are} {Near} {Perfect} for {Out}-of-{Domain} {Detection}},

	booktitle = {Annual {Meeting} of the {Association} for {Computational} {Linguistics}},
	author = {Uppaal, Rheeya and Hu, Junjie and Li, Yixuan},
	year = {2023},
}

@misc{xu_large_2024,
	title = {Large {Language} {Models} for {Anomaly} and {Out}-of-{Distribution} {Detection}: {A} {Survey}},

	author = {Xu, Ruiyao and Ding, Kaize},
	year = {2024},
	note = {ArXiv: 2409.01980},
}

@misc{zhang_your_2024,
	title = {Your {Finetuned} {Large} {Language} {Model} is {Already} a {Powerful} {Out}-of-distribution {Detector}},

	author = {Zhang, Andi and Xiao, Tim Z. and Liu, Weiyang and Bamler, Robert and Wischik, Damon},
	year = {2024},
	note = {ArXiv: 2404.08679},
}

@inproceedings{zhou_contrastive_2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {Contrastive {Out}-of-{Distribution} {Detection} for {Pretrained} {Transformers}},

	abstract = {Pretrained Transformers achieve remarkable performance when training and test data are from the same distribution. However, in real-world scenarios, the model often faces out-of-distribution (OOD) instances that can cause severe semantic shift problems at inference time. Therefore, in practice, a reliable model should identify such instances, and then either reject them during inference or pass them over to models that handle another distribution. In this paper, we develop an unsupervised OOD detection method, in which only the in-distribution (ID) data are used in training. We propose to fine-tune the Transformers with a contrastive loss, which improves the compactness of representations, such that OOD instances can be better differentiated from ID ones. These OOD instances can then be accurately detected using the Mahalanobis distance in the model's penultimate layer. We experiment with comprehensive settings and achieve near-perfect OOD detection performance, outperforming baselines drastically. We further investigate the rationales behind the improvement, finding that more compact representations through margin-based contrastive learning bring the improvement. We release our code to the community for future research.},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Zhou, Wenxuan and Liu, Fangyu and Chen, Muhao},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	month = nov,
	year = {2021},
	pages = {1100--1111},
}

