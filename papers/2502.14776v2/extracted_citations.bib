@inproceedings{AutoSurvey_24_NIPS_Westlake,
    title={AutoSurvey: Large Language Models Can Automatically Write Surveys},
    author = {Wang, Yidong and Guo, Qi and Yao, Wenjin and Zhang, Hongbo and Zhang, Xin and Wu, Zhen and Zhang, Meishan and Dai, Xinyu and Zhang, Min and Wen, Qingsong and Ye, Wei and Zhang, Shikun and Zhang, Yue},
    booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
    year={2024}
}

@inproceedings{cheng-etal-2023-uprise,
    title = "{UPRISE}: Universal Prompt Retrieval for Improving Zero-Shot Evaluation",
    author = "Cheng, Daixuan  and
      Huang, Shaohan  and
      Bi, Junyu  and
      Zhan, Yuefeng  and
      Liu, Jianfeng  and
      Wang, Yujing  and
      Sun, Hao  and
      Wei, Furu  and
      Deng, Weiwei  and
      Zhang, Qi",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.758",
    doi = "10.18653/v1/2023.emnlp-main.758",
    pages = "12318--12337",
    abstract = "Large Language Models (LLMs) are popular for their impressive abilities, but the need for model-specific fine-tuning or task-specific prompt engineering can hinder their generalization. We propose UPRISE (Universal Prompt Retrieval for Improving zero-Shot Evaluation), which tunes a lightweight and versatile retriever that automatically retrieves prompts for a given zero-shot task input. Specifically, we demonstrate universality in a cross-task and cross-model scenario: the retriever is tuned on diverse tasks, but tested on unseen task types; we use a small frozen LLM, GPT-Neo-2.7B, for tuning the retriever, but test the retriever on different LLMs of much larger scales, such as BLOOM-7.1B, OPT-66B and GPT3-175B. Additionally, we show that UPRISE mitigates the hallucination problem in our experiments with ChatGPT, suggesting its potential to improve even the strongest LLMs. Our model and code are available at https://github.com/microsoft/LMOps.",
}

@inproceedings{dong-etal-2024-bamboo,
    title = "{BAMBOO}: A Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models",
    author = "Dong, Zican  and
      Tang, Tianyi  and
      Li, Junyi  and
      Zhao, Wayne Xin  and
      Wen, Ji-Rong",
    editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
    booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.lrec-main.188",
    pages = "2086--2099",
    abstract = "Large language models (LLMs) have achieved dramatic proficiency over NLP tasks with normal length. Recently, multiple studies have committed to extending the context length and enhancing the long text modeling capabilities of LLMs. To comprehensively evaluate the long context ability of LLMs, we propose BAMBOO, a multi-task long context benchmark. BAMBOO has been designed with four principles: comprehensive capacity evaluation, avoidance of data contamination, accurate automatic evaluation, and different length levels. It consists of 10 datasets from 5 different long text understanding tasks, i.e., question answering, hallucination detection, text sorting, language modeling, and code completion, to cover various domains and core capacities of LLMs. We conduct experiments with five widely-used long-context models and further discuss five key questions for long text research. In the end, we discuss problems of current long-context models and point out future directions for enhancing long text modeling capacities. We release our data, prompts, and code at https://anonymous.4open.science/r/BAMBOO/.",
}

@inproceedings{fan2024survey,
  title={A survey on rag meeting llms: Towards retrieval-augmented large language models},
  author={Fan, Wenqi and Ding, Yujuan and Ning, Liangbo and Wang, Shijie and Li, Hengyun and Yin, Dawei and Chua, Tat-Seng and Li, Qing},
  booktitle={Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={6491--6501},
  year={2024}
}

@article{gao2023retrieval,
  title={Retrieval-augmented generation for large language models: A survey},
  author={Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Meng and Wang, Haofen},
  journal={arXiv preprint arXiv:2312.10997},
  year={2023}
}

@article{hu2024rag,
  title={Rag and rau: A survey on retrieval-augmented language model in natural language processing},
  author={Hu, Yucheng and Lu, Yuxing},
  journal={arXiv preprint arXiv:2404.19543},
  year={2024}
}

@article{huang2024survey,
  title={A Survey on Retrieval-Augmented Text Generation for Large Language Models},
  author={Huang, Yizheng and Huang, Jimmy},
  journal={arXiv preprint arXiv:2404.10981},
  year={2024}
}

@inproceedings{ke-etal-2024-bridging,
    title = "Bridging the Preference Gap between Retrievers and {LLM}s",
    author = "Ke, Zixuan  and
      Kong, Weize  and
      Li, Cheng  and
      Zhang, Mingyang  and
      Mei, Qiaozhu  and
      Bendersky, Michael",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.562",
    doi = "10.18653/v1/2024.acl-long.562",
    pages = "10438--10451",
    abstract = "Large Language Models (LLMs) have demonstrated superior results across a wide range of tasks, and Retrieval-augmented Generation (RAG) is an effective way to enhance the performance by locating relevant information and placing it into the context window of the LLM. However, the relationship between retrievers and LLMs in a RAG is still under-investigated. Most existing work treats the retriever and the LLM as independent components and leaves a gap between retrieving human-{''}friendly{''} information and assembling a LLM-{''}friendly{''} context. In this work, we examine a novel bridge mechanism. We validate the ranking and selection assumptions of retrievers in the context of RAG and propose a framework that chains together supervised and reinforcement learning to train a bridge model that optimizes the connection between the retriever and the LLM. Empirical results demonstrate the effectiveness of our method in both question-answering and personalized generation tasks.",
}

@article{kumar2024longlamp,
  title={LongLaMP: A Benchmark for Personalized Long-form Text Generation},
  author={Kumar, Ishita and Viswanathan, Snigdha and Yerra, Sushrita and Salemi, Alireza and Rossi, Ryan A and Dernoncourt, Franck and Deilamsalehy, Hanieh and Chen, Xiang and Zhang, Ruiyi and Agarwal, Shubham and others},
  journal={arXiv preprint arXiv:2407.11016},
  year={2024}
}

@article{liang2024integrating,
  title={Integrating Planning into Single-Turn Long-Form Text Generation},
  author={Liang, Yi and Wu, You and Zhuang, Honglei and Chen, Li and Shen, Jiaming and Jia, Yiling and Qin, Zhen and Sanghai, Sumit and Wang, Xuanhui and Yang, Carl and others},
  journal={arXiv preprint arXiv:2410.06203},
  year={2024}
}

@inproceedings{linra,
  title={RA-DIT: Retrieval-Augmented Dual Instruction Tuning},
  author={Lin, Xi Victoria and Chen, Xilun and Chen, Mingda and Shi, Weijia and Lomeli, Maria and James, Richard and Rodriguez, Pedro and Kahn, Jacob and Szilvasy, Gergely and Lewis, Mike and others},
  booktitle={The Twelfth International Conference on Learning Representations}
}

@inproceedings{min-etal-2023-factscore,
    title = "{FA}ct{S}core: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation",
    author = "Min, Sewon  and
      Krishna, Kalpesh  and
      Lyu, Xinxi  and
      Lewis, Mike  and
      Yih, Wen-tau  and
      Koh, Pang  and
      Iyyer, Mohit  and
      Zettlemoyer, Luke  and
      Hajishirzi, Hannaneh",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.741",
    doi = "10.18653/v1/2023.emnlp-main.741",
    pages = "12076--12100",
    abstract = "Evaluating the factuality of long-form text generated by large language models (LMs) is non-trivial because (1) generations often contain a mixture of supported and unsupported pieces of information, making binary judgments of quality inadequate, and (2) human evaluation is time-consuming and costly. In this paper, we introduce FACTSCORE, a new evaluation that breaks a generation into a series of atomic facts and computes the percentage of atomic facts supported by a reliable knowledge source. We conduct an extensive human evaluation to obtain FACTSCOREs of people biographies generated by several state-of-the-art commercial LMs{---}InstructGPT, ChatGPT, and the retrieval-augmented PerplexityAI{---}and report new analysis demonstrating the need for such a fine-grained score (e.g., ChatGPT only achieves 58{\%}). Since human evaluation is costly, we also introduce an automated model that estimates FACTSCORE using retrieval and a strong language model, with less than a 2{\%} error rate. Finally, we use this automated metric to evaluate 6,500 generations from a new set of 13 recent LMs that would have cost {\$}26K if evaluated by humans, with various findings: GPT-4 and ChatGPT are more factual than public models, and Vicuna and Alpaca are some of the best public models. FACTSCORE is available for public use via {`}pip install factscore{`}.",
}

@article{que2024hellobench,
  title={HelloBench: Evaluating Long Text Generation Capabilities of Large Language Models},
  author={Que, Haoran and Duan, Feiyu and He, Liqun and Mou, Yutao and Zhou, Wangchunshu and Liu, Jiaheng and Rong, Wenge and Wang, Zekun Moore and Yang, Jian and Zhang, Ge and others},
  journal={arXiv preprint arXiv:2409.16191},
  year={2024}
}

@article{rajput2024recommender,
  title={Recommender systems with generative retrieval},
  author={Rajput, Shashank and Mehta, Nikhil and Singh, Anima and Hulikal Keshavan, Raghunandan and Vu, Trung and Heldt, Lukasz and Hong, Lichan and Tay, Yi and Tran, Vinh and Samost, Jonah and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{shao-etal-2024-assisting,
    title = "Assisting in Writing {W}ikipedia-like Articles From Scratch with Large Language Models",
    author = "Shao, Yijia  and
      Jiang, Yucheng  and
      Kanell, Theodore  and
      Xu, Peter  and
      Khattab, Omar  and
      Lam, Monica",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.347",
    doi = "10.18653/v1/2024.naacl-long.347",
    pages = "6252--6278",
    abstract = "We study how to apply large language models to write grounded and organized long-form articles from scratch, with comparable breadth and depth to Wikipedia pages. This underexplored problem poses new challenges at the pre-writing stage, including how to research the topic and prepare an outline prior to writing. We propose STORM, a writing system for the Synthesis of Topic Outlines throughRetrieval and Multi-perspective Question Asking. STORM models the pre-writing stage by (1) discovering diverse perspectives in researching the given topic, (2) simulating conversations where writers carrying different perspectives pose questions to a topic expert grounded on trusted Internet sources, (3) curating the collected information to create an outline.For evaluation, we curate FreshWiki, a dataset of recent high-quality Wikipedia articles, and formulate outline assessments to evaluate the pre-writing stage. We further gather feedback from experienced Wikipedia editors. Compared to articles generated by an outline-driven retrieval-augmented baseline, more of STORM{'}s articles are deemed to be organized (by a 25{\%} absolute increase) and broad in coverage (by 10{\%}). The expert feedback also helps identify new challenges for generating grounded long articles, such as source bias transfer and over-association of unrelated facts.",
}

@inproceedings{tan-etal-2021-progressive,
    title = "Progressive Generation of Long Text with Pretrained Language Models",
    author = "Tan, Bowen  and
      Yang, Zichao  and
      Al-Shedivat, Maruan  and
      Xing, Eric  and
      Hu, Zhiting",
    editor = "Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.341",
    doi = "10.18653/v1/2021.naacl-main.341",
    pages = "4313--4324",
    abstract = "Large-scale language models (LMs) pretrained on massive corpora of text, such as GPT-2, are powerful open-domain text generators. However, as our systematic examination reveals, it is still challenging for such models to generate coherent long passages of text (e.g., 1000 tokens), especially when the models are fine-tuned to the target domain on a small corpus. Previous planning-then-generation methods also fall short of producing such long text in various domains. To overcome the limitations, we propose a simple but effective method of generating text in a progressive manner, inspired by generating images from low to high resolution. Our method first produces domain-specific content keywords and then progressively refines them into complete passages in multiple stages. The simple design allows our approach to take advantage of pretrained LMs at each stage and effectively adapt to any target domain given only a small set of examples. We conduct a comprehensive empirical study with a broad set of evaluation metrics, and show that our approach significantly improves upon the fine-tuned large LMs and various planning-then-generation methods in terms of quality and sample efficiency. Human evaluation also validates that our model generations are more coherent.",
}

@article{tan2024proxyqa,
  title={Proxyqa: An alternative framework for evaluating long-form text generation with large language models},
  author={Tan, Haochen and Guo, Zhijiang and Shi, Zhan and Xu, Lu and Liu, Zhili and Feng, Yunlong and Li, Xiaoguang and Wang, Yasheng and Shang, Lifeng and Liu, Qun and others},
  journal={arXiv preprint arXiv:2401.15042},
  year={2024}
}

@inproceedings{tang-etal-2022-etrica,
    title = "{E}tri{CA}: Event-Triggered Context-Aware Story Generation Augmented by Cross Attention",
    author = "Tang, Chen  and
      Lin, Chenghua  and
      Huang, Henglin  and
      Guerin, Frank  and
      Zhang, Zhihao",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.403/",
    doi = "10.18653/v1/2022.findings-emnlp.403",
    pages = "5504--5518",
}

@inproceedings{tang-etal-2023-enhancing,
    title = "Enhancing Dialogue Generation via Dynamic Graph Knowledge Aggregation",
    author = "Tang, Chen  and
      Zhang, Hongbo  and
      Loakman, Tyler  and
      Lin, Chenghua  and
      Guerin, Frank",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.253/",
    doi = "10.18653/v1/2023.acl-long.253",
    pages = "4604--4616",
}

@inproceedings{tang-etal-2023-improving,
    title = "Improving Biomedical Abstractive Summarisation with Knowledge Aggregation from Citation Papers",
    author = "Tang, Chen  and
      Wang, Shun  and
      Goldsack, Tomas  and
      Lin, Chenghua",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.40/",
    doi = "10.18653/v1/2023.emnlp-main.40",
    pages = "606--618",
}

@article{wu2024automated,
  title={Automated review generation method based on large language models},
  author={Wu, Shican and Ma, Xiao and Luo, Dehui and Li, Lulu and Shi, Xiangcheng and Chang, Xin and Lin, Xiaoyun and Luo, Ran and Pei, Chunlei and Zhao, Zhi-Jian and others},
  journal={arXiv preprint arXiv:2407.20906},
  year={2024}
}

@article{wu2024spinning,
  title={Spinning the Golden Thread: Benchmarking Long-Form Generation in Language Models},
  author={Wu, Yuhao and Hee, Ming Shan and Hu, Zhiqing and Lee, Roy Ka-Wei},
  journal={arXiv preprint arXiv:2409.02076},
  year={2024}
}

@inproceedings{zhang-etal-2023-iag,
    title = "{IAG}: Induction-Augmented Generation Framework for Answering Reasoning Questions",
    author = "Zhang, Zhebin  and
      Zhang, Xinyu  and
      Ren, Yuanhang  and
      Shi, Saijiang  and
      Han, Meng  and
      Wu, Yongkang  and
      Lai, Ruofei  and
      Cao, Zhao",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.1",
    doi = "10.18653/v1/2023.emnlp-main.1",
    pages = "1--14",
    abstract = "Retrieval-Augmented Generation (RAG), by incorporating external knowledge with parametric memory of language models, has become the state-of-the-art architecture for open-domain QA tasks. However, common knowledge bases are inherently constrained by limited coverage and noisy information, making retrieval-based approaches inadequate to answer implicit reasoning questions. In this paper, we propose an Induction-Augmented Generation (IAG) framework that utilizes inductive knowledge along with the retrieved documents for implicit reasoning. We leverage large language models (LLMs) for deriving such knowledge via a novel prompting method based on inductive reasoning patterns. On top of this, we implement two versions of IAG named IAG-GPT and IAG-Student, respectively. IAG-GPT directly utilizes the knowledge generated by GPT-3 for answer prediction, while IAG-Student gets rid of dependencies on GPT service at inference time by incorporating a student inductor model. The inductor is firstly trained via knowledge distillation and further optimized by back-propagating the generator feedback via differentiable beam scores. Experimental results show that IAG outperforms RAG baselines as well as ChatGPT on two Open-Domain QA tasks. Notably, our best models have won the first place in the official leaderboards of CSQA2.0 (since Nov 1, 2022) and StrategyQA (since Jan 8, 2023).",
}

@article{zhang2024retrieval,
  title={Retrieval-based Full-length Wikipedia Generation for Emergent Events},
  author={Zhang, Jiebin and Yu, Eugene J and Chen, Qinyu and Xiong, Chenhao and Zhu, Dawei and Qian, Han and Song, Mingbo and Li, Xiaoguang and Liu, Qun and Li, Sujian},
  journal={arXiv preprint arXiv:2402.18264},
  year={2024}
}

