\section{Related Work}
\textbf{Long-form Text Generation.}
Although LLMs excel in traditional NLG tasks, generating long-form, well-structured, coherent, and logically organized text using LLMs still remains a persistent challenge \cite{tan2024proxyqa,kumar2024longlamp,wu2024spinning,min-etal-2023-factscore,que2024hellobench,dong-etal-2024-bamboo,tang-etal-2023-enhancing}. To address this problem, some studies have sought to employ planning strategies. For instance, \citet{tan-etal-2021-progressive} proposed a method that first produces domain-specific content keywords, and then progressively refines them into complete passages in multiple stages. Similarly, \citet{liang2024integrating} used a series of auxiliary training tasks to endow LLMs with the skills to plan and structure long-form documents before generating the final full article. Some other researches focus on the generation of long-form text in a specific format, such as Wikipedia articles, surveys, commentaries, etc \cite{zhang2024retrieval, wu2024automated, tang-etal-2022-etrica,tang-etal-2023-improving}. \citet{shao-etal-2024-assisting} developed STORM, a system that models the pre-writing stage by discovering diverse perspectives, simulating expert conversations, and curating information to create an outline, which is then used to generate a complete Wikipedia article. \citet{AutoSurvey_24_NIPS_Westlake} introduced AotuSurvey, a framework for generating surveys consisting of initial retrieval, outline generation, parallel subsection drafting, integration, and rigorous evaluation. In comparison, our work addresses existing shortcomings by enhancing the retrieval scenarios, optimizing the reference pre-processing methods, expanding the expressive forms of the generated surveys, and improving overall quality.

\textbf{Retrieval Augmented Generation (RAG).}
RAG technique is used to help LLMs access external knowledge, thereby enhancing their text generation capabilities \cite{gao2023retrieval, fan2024survey, huang2024survey, hu2024rag}. This technique has proven especially useful in tasks that require up-to-date or domain-specific information, such as QA (Question Answering) \cite{ke-etal-2024-bridging,asai2024selfrag}, Dialog Generation \cite{rajput2024recommender,linra}, Reasoning \cite{cheng-etal-2023-uprise,zhang-etal-2023-iag}, etc. Beyond traditional NLG tasks, RAG is also used for long-form text generation, as this task often requires handling extensive external knowledge \cite{AutoSurvey_24_NIPS_Westlake,shao-etal-2024-assisting,zhang-etal-2023-iag}. Compared to previous work that directly used the raw text of references as the retrieval data source, our work significantly improves retrieval efficiency and context window utilization by converting it into attribute trees.