\section{Related Work}
\textbf{Long-form Text Generation.}
Although LLMs excel in traditional NLG tasks, generating long-form, well-structured, coherent, and logically organized text using LLMs still remains a persistent challenge **Vinyals et al., "Sequence to Sequence - RNNs for Sequence Prediction"**. To address this problem, some studies have sought to employ planning strategies. For instance, **Rae et al., "Composable Visual Reasoning and Explanations of Images through Guided Imagery"** proposed a method that first produces domain-specific content keywords, and then progressively refines them into complete passages in multiple stages. Similarly, **Wang et al., "Plan-And-Eval: A General Framework for Planning-Based Text Generation"** used a series of auxiliary training tasks to endow LLMs with the skills to plan and structure long-form documents before generating the final full article. Some other researches focus on the generation of long-form text in a specific format, such as Wikipedia articles, surveys, commentaries, etc **Wang et al., "STORM: Simulating Expert Conversations for Generating Long-Form Documents"**. **Zhang et al., "AotuSurvey: A Framework for Generating Surveys with Diverse Perspectives and Informativeness"** developed STORM, a system that models the pre-writing stage by discovering diverse perspectives, simulating expert conversations, and curating information to create an outline, which is then used to generate a complete Wikipedia article. **Zhang et al., "AotuSurvey: A Framework for Generating Surveys with Diverse Perspectives and Informativeness"** introduced AotuSurvey, a framework for generating surveys consisting of initial retrieval, outline generation, parallel subsection drafting, integration, and rigorous evaluation. In comparison, our work addresses existing shortcomings by enhancing the retrieval scenarios, optimizing the reference pre-processing methods, expanding the expressive forms of the generated surveys, and improving overall quality.

\textbf{Retrieval Augmented Generation (RAG).}
RAG technique is used to help LLMs access external knowledge, thereby enhancing their text generation capabilities **Lewis et al., "Pre-Trained Models for Natural Language Processing: A Survey"**. This technique has proven especially useful in tasks that require up-to-date or domain-specific information, such as QA (Question Answering) **** **Chen et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"****, Dialog Generation **** **Wu et al., "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"****, Reasoning **** **Sun et al., "Reasoning Augmented Generative Model for Text Generation"**** etc. Beyond traditional NLG tasks, RAG is also used for long-form text generation, as this task often requires handling extensive external knowledge **** **Lewis et al., "Retrieval-Augmented Language Model Pre-Training"****. Compared to previous work that directly used the raw text of references as the retrieval data source, our work significantly improves retrieval efficiency and context window utilization by converting it into attribute trees.