\section{Related Work: }
Recent years have seen rapid progress in vision-language models (VLMs), driven by breakthroughs in architecture, scaling laws, and multimodal alignment techniques. These models are becoming central to tasks that require a seamless understanding of both visual and textual inputs, such as visual question answering, image captioning, document understanding, and OCR. This section highlights key developments in the field, with a focus on pioneering VLM families and their contributions to model efficiency, document processing, and training innovations. 

\subsection{Advancement in Vision Language Models}
Recent advancements in vision-language models (VLMs) have significantly expanded the capabilities of multimodal AI systems. Several notable model families have emerged, each with distinct architectural approaches and scaling strategies. 

The Qwen-VL\cite{bai2023qwenvlversatilevisionlanguagemodel} \cite{wang2024qwen2vlenhancingvisionlanguagemodels} series (Qwen-VL and Qwen2-VL) represents significant milestones in open-source VLM development. The original Qwen-VL\cite{bai2023qwenvlversatilevisionlanguagemodel} built upon Qwen-LM with a visual receptor and 3-stage training pipeline, demonstrating strong performance on visual grounding and OCR tasks. Its successor, Qwen2-VL, introduced the Naive Dynamic Resolution mechanism\cite{wang2024qwen2vlenhancingvisionlanguagemodels} for handling variable image resolutions and Multimodal Rotary Position Embedding (M-RoPE)\cite{wang2024qwen2vlenhancingvisionlanguagemodels} for effectively fusing positional information across modalities. Qwen2-VL explored scaling laws across model sizes at 2B, 8B, and 72B parameters, achieving performance competitive with proprietary models at the 72B scale. 

InternVL represents another significant branch of VLM research. The InternVL series scaled vision foundation models to 6B parameters and progressively aligned them with LLMs using web-scale image-text data. InternVL 1.5\cite{chen2024fargpt4vclosinggapInternvl1.5} improved upon this foundation with dynamic high-resolution processing supporting up to 4K resolution input and bilingual dataset enhancements for OCR and Chinese language tasks. The most recent iteration, InternVL 2.5\cite{chen2025expandingperformanceboundariesopensourceinternvl2.5}, maintained the core architecture while focusing on training and testing strategy improvements, achieving high performance on multi-discipline reasoning tasks. 

Microsoft's Phi-3\cite{abdin2024phi3technicalreporthighly} series has extended into the vision domain with Phi-3.5-Vision\cite{abdin2024phi3technicalreporthighly}, a relatively compact 4.2B parameter model derived from the Phi-3.5-mini language model. Despite its modest size, Phi-3.5-Vision demonstrates strong reasoning capabilities and handles both single and multi-image inputs effectively. 

\subsection{Efficiency-Focused Approaches}
A growing trend in VLM research focuses on developing efficient models that maintain high performance while reducing computational requirements. SmolVLM\cite{smolvlm} represents this direction with its 2B parameter model designed for commercial use and local deployment. These models leverage open training pipelines and datasets like Cauldron and Docmatix, demonstrating that smaller models can still achieve practical utility. 

Similarly, Molmo\cite{deitke2024molmopixmoopenweights} introduced a family of VLMs built from scratch without distillation from proprietary models. Their approach combined careful modeling choices with high-quality original created PixMo dataset, including detailed image captions and innovative 2D pointing data. Despite focusing on open development principles, their models achieved competitive performance with larger models. 

Idefics3-8B\cite{laurençon2024buildingbetterunderstandingvisionlanguage} exemplifies efficient VLM development through straightforward training pipelines and exclusive use of open datasets. The creation of Docmatix—a dataset 240 times larger than previously available document understanding resources—contributed significantly to its document processing capabilities. 

\subsection{Document Understanding and OCR Capabilities }
Document understanding and OCR capabilities have become essential benchmarks for evaluating VLM performance. Several models have made notable progress in this domain. InternVL 1.5\cite{chen2024fargpt4vclosinggapInternvl1.5} incorporated high-quality datasets covering document images with bilingual annotations, significantly enhancing OCR-related task performance. Qwen-VL\cite{bai2023qwenvlversatilevisionlanguagemodel} implemented text-reading ability by aligning image-caption-box tuples, while Qwen2-VL's dynamic resolution approach improved document processing capabilities. 

The development of the Docmatix dataset by Idefics3\cite{laurençon2024buildingbetterunderstandingvisionlanguage} marks a significant milestone in advancing document understanding, providing training resources at unprecedented scale. This development has raised the baseline for document processing capabilities in modern VLMs.  

\subsection{Training Strategies and Data Efficiency}
Training methodologies have diversified across VLM development. The Qwen-VL\cite{bai2023qwenvlversatilevisionlanguagemodel} series employed a 3-stage training pipeline with multilingual multimodal cleaned corpus, while Molmo\cite{deitke2024molmopixmoopenweights} emphasized dataset quality over quantity with their carefully curated PixMo datasets. InternVL explored continuous learning strategies for large-scale vision foundation models and high-quality bilingual dataset curation. 

While many approaches have focused on scaling both model size and training data volume as seen with Qwen2-VL's 72B parameter model and InternVL's extensive data collection, our work with Shakti VLM contributes to this landscape by introducing architectural innovations specifically designed to improve data efficiency. Through adopting QK-Normalization\cite{henry2020querykeynormalizationtransformers} for attention stability, hybrid normalization techniques, and enhanced positional encoding, Shakti-VLM models achieve competitive performance despite using fewer training tokens than comparable models. This focus on efficiency through architectural design rather than sheer data volume positions Shakti-VLM as a practical solution for enterprise-scale multimodal tasks.