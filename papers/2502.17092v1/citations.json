[
  {
    "index": 0,
    "papers": [
      {
        "key": "bai2023qwenvlversatilevisionlanguagemodel",
        "author": "Jinze Bai and Shuai Bai and Shusheng Yang and Shijie Wang and Sinan Tan and Peng Wang and Junyang Lin and Chang Zhou and Jingren Zhou",
        "title": "Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "wang2024qwen2vlenhancingvisionlanguagemodels",
        "author": "Peng Wang and Shuai Bai and Sinan Tan and Shijie Wang and Zhihao Fan and Jinze Bai and Keqin Chen and Xuejing Liu and Jialin Wang and Wenbin Ge and Yang Fan and Kai Dang and Mengfei Du and Xuancheng Ren and Rui Men and Dayiheng Liu and Chang Zhou and Jingren Zhou and Junyang Lin",
        "title": "Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "bai2023qwenvlversatilevisionlanguagemodel",
        "author": "Jinze Bai and Shuai Bai and Shusheng Yang and Shijie Wang and Sinan Tan and Peng Wang and Junyang Lin and Chang Zhou and Jingren Zhou",
        "title": "Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "wang2024qwen2vlenhancingvisionlanguagemodels",
        "author": "Peng Wang and Shuai Bai and Sinan Tan and Shijie Wang and Zhihao Fan and Jinze Bai and Keqin Chen and Xuejing Liu and Jialin Wang and Wenbin Ge and Yang Fan and Kai Dang and Mengfei Du and Xuancheng Ren and Rui Men and Dayiheng Liu and Chang Zhou and Jingren Zhou and Junyang Lin",
        "title": "Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "wang2024qwen2vlenhancingvisionlanguagemodels",
        "author": "Peng Wang and Shuai Bai and Sinan Tan and Shijie Wang and Zhihao Fan and Jinze Bai and Keqin Chen and Xuejing Liu and Jialin Wang and Wenbin Ge and Yang Fan and Kai Dang and Mengfei Du and Xuancheng Ren and Rui Men and Dayiheng Liu and Chang Zhou and Jingren Zhou and Junyang Lin",
        "title": "Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "chen2024fargpt4vclosinggapInternvl1.5",
        "author": "Zhe Chen and Weiyun Wang and Hao Tian and Shenglong Ye and Zhangwei Gao and Erfei Cui and Wenwen Tong and Kongzhi Hu and Jiapeng Luo and Zheng Ma and Ji Ma and Jiaqi Wang and Xiaoyi Dong and Hang Yan and Hewei Guo and Conghui He and Botian Shi and Zhenjiang Jin and Chao Xu and Bin Wang and Xingjian Wei and Wei Li and Wenjian Zhang and Bo Zhang and Pinlong Cai and Licheng Wen and Xiangchao Yan and Min Dou and Lewei Lu and Xizhou Zhu and Tong Lu and Dahua Lin and Yu Qiao and Jifeng Dai and Wenhai Wang",
        "title": "How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "chen2025expandingperformanceboundariesopensourceinternvl2.5",
        "author": "Zhe Chen and Weiyun Wang and Yue Cao and Yangzhou Liu and Zhangwei Gao and Erfei Cui and Jinguo Zhu and Shenglong Ye and Hao Tian and Zhaoyang Liu and Lixin Gu and Xuehui Wang and Qingyun Li and Yimin Ren and Zixuan Chen and Jiapeng Luo and Jiahao Wang and Tan Jiang and Bo Wang and Conghui He and Botian Shi and Xingcheng Zhang and Han Lv and Yi Wang and Wenqi Shao and Pei Chu and Zhongying Tu and Tong He and Zhiyong Wu and Huipeng Deng and Jiaye Ge and Kai Chen and Kaipeng Zhang and Limin Wang and Min Dou and Lewei Lu and Xizhou Zhu and Tong Lu and Dahua Lin and Yu Qiao and Jifeng Dai and Wenhai Wang",
        "title": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "abdin2024phi3technicalreporthighly",
        "author": "Marah Abdin and Jyoti Aneja and Hany Awadalla and Ahmed Awadallah and Ammar Ahmad Awan and Nguyen Bach and Amit Bahree and Arash Bakhtiari and Jianmin Bao and Harkirat Behl and Alon Benhaim and Misha Bilenko and Johan Bjorck and S\u00e9bastien Bubeck and Martin Cai and Qin Cai and Vishrav Chaudhary and Dong Chen and Dongdong Chen and Weizhu Chen and Yen-Chun Chen and Yi-Ling Chen and Hao Cheng and Parul Chopra and Xiyang Dai and Matthew Dixon and Ronen Eldan and Victor Fragoso and Jianfeng Gao and Mei Gao and Min Gao and Amit Garg and Allie Del Giorno and Abhishek Goswami and Suriya Gunasekar and Emman Haider and Junheng Hao and Russell J. Hewett and Wenxiang Hu and Jamie Huynh and Dan Iter and Sam Ade Jacobs and Mojan Javaheripi and Xin Jin and Nikos Karampatziakis and Piero Kauffmann and Mahoud Khademi and Dongwoo Kim and Young Jin Kim and Lev Kurilenko and James R. Lee and Yin Tat Lee and Yuanzhi Li and Yunsheng Li and Chen Liang and Lars Liden and Xihui Lin and Zeqi Lin and Ce Liu and Liyuan Liu and Mengchen Liu and Weishung Liu and Xiaodong Liu and Chong Luo and Piyush Madan and Ali Mahmoudzadeh and David Majercak and Matt Mazzola and Caio C\u00e9sar Teodoro Mendes and Arindam Mitra and Hardik Modi and Anh Nguyen and Brandon Norick and Barun Patra and Daniel Perez-Becker and Thomas Portet and Reid Pryzant and Heyang Qin and Marko Radmilac and Liliang Ren and Gustavo de Rosa and Corby Rosset and Sambudha Roy and Olatunji Ruwase and Olli Saarikivi and Amin Saied and Adil Salim and Michael Santacroce and Shital Shah and Ning Shang and Hiteshi Sharma and Yelong Shen and Swadheen Shukla and Xia Song and Masahiro Tanaka and Andrea Tupini and Praneetha Vaddamanu and Chunyu Wang and Guanhua Wang and Lijuan Wang and Shuohang Wang and Xin Wang and Yu Wang and Rachel Ward and Wen Wen and Philipp Witte and Haiping Wu and Xiaoxia Wu and Michael Wyatt and Bin Xiao and Can Xu and Jiahang Xu and Weijian Xu and Jilong Xue and Sonali Yadav and Fan Yang and Jianwei Yang and Yifan Yang and Ziyi Yang and Donghan Yu and Lu Yuan and Chenruidong Zhang and Cyril Zhang and Jianwen Zhang and Li Lyna Zhang and Yi Zhang and Yue Zhang and Yunan Zhang and Xiren Zhou",
        "title": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "abdin2024phi3technicalreporthighly",
        "author": "Marah Abdin and Jyoti Aneja and Hany Awadalla and Ahmed Awadallah and Ammar Ahmad Awan and Nguyen Bach and Amit Bahree and Arash Bakhtiari and Jianmin Bao and Harkirat Behl and Alon Benhaim and Misha Bilenko and Johan Bjorck and S\u00e9bastien Bubeck and Martin Cai and Qin Cai and Vishrav Chaudhary and Dong Chen and Dongdong Chen and Weizhu Chen and Yen-Chun Chen and Yi-Ling Chen and Hao Cheng and Parul Chopra and Xiyang Dai and Matthew Dixon and Ronen Eldan and Victor Fragoso and Jianfeng Gao and Mei Gao and Min Gao and Amit Garg and Allie Del Giorno and Abhishek Goswami and Suriya Gunasekar and Emman Haider and Junheng Hao and Russell J. Hewett and Wenxiang Hu and Jamie Huynh and Dan Iter and Sam Ade Jacobs and Mojan Javaheripi and Xin Jin and Nikos Karampatziakis and Piero Kauffmann and Mahoud Khademi and Dongwoo Kim and Young Jin Kim and Lev Kurilenko and James R. Lee and Yin Tat Lee and Yuanzhi Li and Yunsheng Li and Chen Liang and Lars Liden and Xihui Lin and Zeqi Lin and Ce Liu and Liyuan Liu and Mengchen Liu and Weishung Liu and Xiaodong Liu and Chong Luo and Piyush Madan and Ali Mahmoudzadeh and David Majercak and Matt Mazzola and Caio C\u00e9sar Teodoro Mendes and Arindam Mitra and Hardik Modi and Anh Nguyen and Brandon Norick and Barun Patra and Daniel Perez-Becker and Thomas Portet and Reid Pryzant and Heyang Qin and Marko Radmilac and Liliang Ren and Gustavo de Rosa and Corby Rosset and Sambudha Roy and Olatunji Ruwase and Olli Saarikivi and Amin Saied and Adil Salim and Michael Santacroce and Shital Shah and Ning Shang and Hiteshi Sharma and Yelong Shen and Swadheen Shukla and Xia Song and Masahiro Tanaka and Andrea Tupini and Praneetha Vaddamanu and Chunyu Wang and Guanhua Wang and Lijuan Wang and Shuohang Wang and Xin Wang and Yu Wang and Rachel Ward and Wen Wen and Philipp Witte and Haiping Wu and Xiaoxia Wu and Michael Wyatt and Bin Xiao and Can Xu and Jiahang Xu and Weijian Xu and Jilong Xue and Sonali Yadav and Fan Yang and Jianwei Yang and Yifan Yang and Ziyi Yang and Donghan Yu and Lu Yuan and Chenruidong Zhang and Cyril Zhang and Jianwen Zhang and Li Lyna Zhang and Yi Zhang and Yue Zhang and Yunan Zhang and Xiren Zhou",
        "title": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "smolvlm",
        "author": "Hugging Face",
        "title": "SmolVLM:small yet mighty Vision Language Model"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "deitke2024molmopixmoopenweights",
        "author": "Matt Deitke and Christopher Clark and Sangho Lee and Rohun Tripathi and Yue Yang and Jae Sung Park and Mohammadreza Salehi and Niklas Muennighoff and Kyle Lo and Luca Soldaini and Jiasen Lu and Taira Anderson and Erin Bransom and Kiana Ehsani and Huong Ngo and YenSung Chen and Ajay Patel and Mark Yatskar and Chris Callison-Burch and Andrew Head and Rose Hendrix and Favyen Bastani and Eli VanderBilt and Nathan Lambert and Yvonne Chou and Arnavi Chheda and Jenna Sparks and Sam Skjonsberg and Michael Schmitz and Aaron Sarnat and Byron Bischoff and Pete Walsh and Chris Newell and Piper Wolters and Tanmay Gupta and Kuo-Hao Zeng and Jon Borchardt and Dirk Groeneveld and Crystal Nam and Sophie Lebrecht and Caitlin Wittlif and Carissa Schoenick and Oscar Michel and Ranjay Krishna and Luca Weihs and Noah A. Smith and Hannaneh Hajishirzi and Ross Girshick and Ali Farhadi and Aniruddha Kembhavi",
        "title": "Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "lauren\u00e7on2024buildingbetterunderstandingvisionlanguage",
        "author": "Hugo Lauren\u00e7on and Andr\u00e9s Marafioti and Victor Sanh and L\u00e9o Tronchon",
        "title": "Building and better understanding vision-language models: insights and future directions"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "chen2024fargpt4vclosinggapInternvl1.5",
        "author": "Zhe Chen and Weiyun Wang and Hao Tian and Shenglong Ye and Zhangwei Gao and Erfei Cui and Wenwen Tong and Kongzhi Hu and Jiapeng Luo and Zheng Ma and Ji Ma and Jiaqi Wang and Xiaoyi Dong and Hang Yan and Hewei Guo and Conghui He and Botian Shi and Zhenjiang Jin and Chao Xu and Bin Wang and Xingjian Wei and Wei Li and Wenjian Zhang and Bo Zhang and Pinlong Cai and Licheng Wen and Xiangchao Yan and Min Dou and Lewei Lu and Xizhou Zhu and Tong Lu and Dahua Lin and Yu Qiao and Jifeng Dai and Wenhai Wang",
        "title": "How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "bai2023qwenvlversatilevisionlanguagemodel",
        "author": "Jinze Bai and Shuai Bai and Shusheng Yang and Shijie Wang and Sinan Tan and Peng Wang and Junyang Lin and Chang Zhou and Jingren Zhou",
        "title": "Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "lauren\u00e7on2024buildingbetterunderstandingvisionlanguage",
        "author": "Hugo Lauren\u00e7on and Andr\u00e9s Marafioti and Victor Sanh and L\u00e9o Tronchon",
        "title": "Building and better understanding vision-language models: insights and future directions"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "bai2023qwenvlversatilevisionlanguagemodel",
        "author": "Jinze Bai and Shuai Bai and Shusheng Yang and Shijie Wang and Sinan Tan and Peng Wang and Junyang Lin and Chang Zhou and Jingren Zhou",
        "title": "Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "deitke2024molmopixmoopenweights",
        "author": "Matt Deitke and Christopher Clark and Sangho Lee and Rohun Tripathi and Yue Yang and Jae Sung Park and Mohammadreza Salehi and Niklas Muennighoff and Kyle Lo and Luca Soldaini and Jiasen Lu and Taira Anderson and Erin Bransom and Kiana Ehsani and Huong Ngo and YenSung Chen and Ajay Patel and Mark Yatskar and Chris Callison-Burch and Andrew Head and Rose Hendrix and Favyen Bastani and Eli VanderBilt and Nathan Lambert and Yvonne Chou and Arnavi Chheda and Jenna Sparks and Sam Skjonsberg and Michael Schmitz and Aaron Sarnat and Byron Bischoff and Pete Walsh and Chris Newell and Piper Wolters and Tanmay Gupta and Kuo-Hao Zeng and Jon Borchardt and Dirk Groeneveld and Crystal Nam and Sophie Lebrecht and Caitlin Wittlif and Carissa Schoenick and Oscar Michel and Ranjay Krishna and Luca Weihs and Noah A. Smith and Hannaneh Hajishirzi and Ross Girshick and Ali Farhadi and Aniruddha Kembhavi",
        "title": "Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "henry2020querykeynormalizationtransformers",
        "author": "Alex Henry and Prudhvi Raj Dachapally and Shubham Pawar and Yuxuan Chen",
        "title": "Query-Key Normalization for Transformers"
      }
    ]
  }
]