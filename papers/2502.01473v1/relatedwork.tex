\section{Related Work}
This paper is closely related to a line of work by \citet{bartlett2017spectrally, edelman2022inductive, trauger2024length_independent_transformer}, which establishes generalization bounds for general neural networks and attention-based models. We draw explicit connections between the self-attention mechanism and selective SSMs to derive generalization bounds, building on these prior works. The related work discussed here is primarily focused on self-attention, SSMs, and generalization bounds.

\textbf{Self-attention} is the core mechanism behind the Transformer architecture, introduced as an alternative to RNNs and CNNs for sequence processing \cite{vaswani2017attention}. While the concept of attention predates the Transformer, its introduction marked a pivotal shift in the scale of large models. An attention mechanism broadly assigns scores to each pair of elements in a sequence to measure their relevance to each other. Similarly, self-attention mechanism draws inspiration from the key-query analogy used in relational databases to capture dependencies between elements of an input sequence. Since their introduction, Transformers have been extensively studied and refined, leading to numerous variants, including sparse and low-rank adaptations and widespread applications across domains such as natural language processing \cite{devlin2018bert, brown2020gpt3} and computer vision \cite{dosovitskiy2020vision_transformer, peebles2023scalable_diffusion_transformer, liu2024diffusion_vision_transformer}.

\textbf{State-space models} are a new class of foundation models, introduced by \citet{gu2021ssm} as an alternative to Transformers for sequence processing. Rooted in the classical state-space representations introduced by \citet{kalman1960new} in control theory, SSMs leverage state-space representations to efficiently model long-range dependencies in sequential data. The foundation of SSMs can be traced to the HiPPO framework, which established a mathematical basis for encoding and preserving long-range dependencies using orthogonal polynomial projections \cite{gu2020hippo}. Building on this foundation, the first practical implementation of SSMs is the S4 model, which utilized HiPPO as an initialization scheme \cite{gu2022s4}. With the empirical success of S4 on the Long Range Arena benchmark \cite{tay2021long_range_arena}, SSMs gained widespread attention, prompting several extensions and refinements. S4D simplified training with diagonal initializations \cite{gu2022s4d}, S5 introduced a multi-input multi-output structure for greater flexibility \cite{smith2023s5}, and Hyena explored hierarchical convolutions \cite{poli2023hyena}. Selective SSMs introduced in the Mamba model by \citet{gu2024mamba} extend LTI SSMs by using linear projections of the input to construct and discretize SSMs, resulting in a nonlinear time-variant architecture. These properties make selective SSMs closely resemble self-attention, as highlighted by \citet{dao2024transformers_are_ssms} while introducing Mamba-2. 

\textbf{Generalization bounds} are central in the probably approximately correct (PAC) learning framework, which formalizes a model's ability to achieve  low error on unseen data with high probability, provided sufficient training data. PAC learning provides bounds that are essential in understanding why certain architectures generalize well despite their overparameterization. Earlier studies explored statistical guarantees based on VC-dimension and shattering bounds extensively \cite{karpinski1997polynomial, koiran1997neural, sontag1998learning, baum1988size, bartlett1998almost}. The recent works on norm-based generalization bounds utilize Rademacher complexity, a fundamental tool that is used to upper bound the generalization gap. A widely used approach to bound Rademacher complexity involves covering numbers \cite{vershynin2018high}. \citet{zhang2002covering} laid the groundwork for understanding the capacity of regularized linear function classes by covering numbers. Later on, \citet{bartlett2017spectrally} established generalization bounds for neural networks using covering numbers based on the work of \citet{zhang2002covering}. These methods have been extended to Transformers in recent studies by \citet{edelman2022inductive, trauger2024length_independent_transformer, truong2024rank}, where different aspects such as length or rank dependency have been emphasized. For LTI SSMs, the works of \citet{racz2024length, liu2024generalization} draw inspiration from this line of research but primarily leverage the structure of LTI systems to derive their bounds from a state-space perspective.