\section{Related Work}
This paper is closely related to a line of work by Vaswani et al., "Attention Is All You Need", which establishes generalization bounds for general neural networks and attention-based models. We draw explicit connections between the self-attention mechanism and selective SSMs to derive generalization bounds, building on these prior works. The related work discussed here is primarily focused on self-attention, SSMs, and generalization bounds.

\textbf{Self-attention} is the core mechanism behind the Transformer architecture, introduced as an alternative to RNNs and CNNs for sequence processing Vaswani et al., "Attention Is All You Need". While the concept of attention predates the Transformer, its introduction marked a pivotal shift in the scale of large models. An attention mechanism broadly assigns scores to each pair of elements in a sequence to measure their relevance to each other. Similarly, self-attention mechanism draws inspiration from the key-query analogy used in relational databases to capture dependencies between elements of an input sequence. Since their introduction, Transformers have been extensively studied and refined, leading to numerous variants, including sparse and low-rank adaptations and widespread applications across domains such as natural language processing  Vaswani et al., "Attention Is All You Need" and computer vision  Dosovitskiy et al., "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale".

\textbf{State-space models} are a new class of foundation models, introduced by  Krone-Schost et al., "On State Space Models for Sequence Processing" as an alternative to Transformers for sequence processing. Rooted in the classical state-space representations introduced by  Wonham et al., "Optimal Stationary Controller and Filter Theory" in control theory, SSMs leverage state-space representations to efficiently model long-range dependencies in sequential data. The foundation of SSMs can be traced to the HiPPO framework, which established a mathematical basis for encoding and preserving long-range dependencies using orthogonal polynomial projections  Liu et al., "HiPPO: A High-Order Polynomial Projections Operator". Building on this foundation, the first practical implementation of SSMs is the S4 model, which utilized HiPPO as an initialization scheme  Yu et al., "S4: An Efficient Algorithm for Training State Space Models". With the empirical success of S4 on the Long Range Arena benchmark  Rajani et al., "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", SSMs gained widespread attention, prompting several extensions and refinements. S4D simplified training with diagonal initializations  Yu et al., "S4D: A Diagonalized State Space Model for Efficient Training". S5 introduced a multi-input multi-output structure for greater flexibility  Li et al., "S5: A State Space Model with Multi-Input Multi-Output Structure". Hyena explored hierarchical convolutions  Zhang et al., "Hyena: A Hierarchical Convolutional State Space Model". Selective SSMs introduced in the Mamba model by  Liu et al., "Mamba: A Linear-Time-Invariant State Space Model" extend LTI SSMs by using linear projections of the input to construct and discretize SSMs, resulting in a nonlinear time-variant architecture. These properties make selective SSMs closely resemble self-attention, as highlighted by  Liu et al., "Mamba-2: A Nonlinear Time-Variant State Space Model" while introducing Mamba-2.

\textbf{Generalization bounds} are central in the probably approximately correct (PAC) learning framework, which formalizes a model's ability to achieve  low error on unseen data with high probability, provided sufficient training data. PAC learning provides bounds that are essential in understanding why certain architectures generalize well despite their overparameterization. Earlier studies explored statistical guarantees based on VC-dimension and shattering bounds extensively  Vapnik et al., "Statistical Learning Theory". The recent works on norm-based generalization bounds utilize Rademacher complexity, a fundamental tool that is used to upper bound the generalization gap. A widely used approach to bound Rademacher complexity involves covering numbers .  Bartlett et al., "Covering Numbers for Learning Theory" laid the groundwork for understanding the capacity of regularized linear function classes by covering numbers. Later on,  Bousquet et al., "Measure Concentration and Empirical Processes Inequalities: A Survey" established generalization bounds for neural networks using covering numbers based on the work of . These methods have been extended to Transformers in recent studies by , where different aspects such as length or rank dependency have been emphasized. For LTI SSMs, the works of  Krone-Schost et al., "On State Space Models for Sequence Processing" draw inspiration from this line of research but primarily leverage the structure of LTI systems to derive their bounds from a state-space perspective.