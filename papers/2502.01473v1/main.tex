\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
% To allow page breaks in align and alignat environments
\allowdisplaybreaks
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Packages we add
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{bm}
\usepackage{subcaption}
\usepackage{array}
\usepackage{xcolor}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Generalization Error Analysis for Selective State-Space Models Through the Lens of Attention}

\begin{document}

\twocolumn[
\icmltitle{Generalization Error Analysis for Selective State-Space Models Through the Lens of Attention}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Arya Honarpisheh}{yyy}
\icmlauthor{Mustafa Bozdag}{yyy}
\icmlauthor{Mario Sznaier}{yyy}
\icmlauthor{Octavia Camps}{yyy}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Department of Electrical and Computer Engineering, Northeastern University, Boston, MA, USA}

\icmlcorrespondingauthor{Arya Honarpisheh}{honarpisheh.a@northeastern.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{SSMs, Transformers}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
State-space models (SSMs) are a new class of foundation models that have emerged as a compelling alternative to Transformers and their attention mechanisms for sequence processing tasks. This paper provides a detailed theoretical analysis of selective SSMs, the core components of the Mamba and Mamba-2 architectures. We leverage the connection between selective SSMs and the self-attention mechanism to highlight the fundamental similarities between these models. Building on this connection, we establish a length independent covering number-based generalization bound for selective SSMs, providing a deeper understanding of their theoretical performance guarantees. We analyze the effects of state matrix stability and input-dependent discretization, shedding light on the critical role played by these factors in the generalization capabilities of selective SSMs. Finally, we empirically demonstrate the sequence length independence of the derived bounds on two tasks.
\end{abstract}

\section{Introduction}
\label{sec:intro}
Foundation models trained on large-scale datasets have be- come the dominant paradigm in modern machine learning following the introduction of the Transformer architecture. Transformers owe their success to the self-attention mechanism, which encodes sequential information globally by considering dependencies across the input sequence \cite{vaswani2017attention}. Although they are very effective in sequence processing, the global nature of self-attention imposes limitations: a finite sequence window and quadratic scaling w.r.t. the window length. Recently, a new family of models named state-space models (SSMs) became a popular alternative to address this problem \cite{gu2021ssm, gu2022s4}. These models are rooted in the classical state-space representations from control theory introduced by \citet{kalman1960new}. The state-of-the-art SSMs are selective, which are inherently nonlinear and form the core of the Mamba and Mamba-2 architectures \cite{gu2024mamba, dao2024transformers_are_ssms}. They employ a selective-scan mechanism to capture long-term dependencies, allowing for adaptive and efficient sequence processing.

In this paper, we establish length-independent generalization bounds for selective SSMs by incorporating state-of-the-art tools developed for deep architectures rather than standard methods used for linear time-invariant (LTI) SSMs. We integrate theoretical results from Transformers with insights from recurrent neural networks (RNNs), which requires the utilization of two different kinds of covers. These covers combined together establish an upper bound on the Rademacher complexity, providing us with a bound on the generalization gap. Our approach highlights the selective-scan mechanism, recurrent nature, and unique input-dependent time scale of selective SSMs.

Theoretical analysis of SSMs have primarily focused on LTI SSMs, such as S4 and S4D \cite{gu2022s4,gu2022s4d}. \citet{racz2024length} derives a length-independent generalization bound for deep LTI SSMs using Rademacher contraction techniques, while \citet{liu2024generalization} establishes a length-dependent bound and leverages it to propose an initialization and regularization scheme. These results rely on system stability from control theory, ensuring that operator norms remain well-defined and finite. Architectures like S4, S4D, and S5 allow for an easy imposition of the stability assumption, making them well-suited for bounded-norm analysis \cite{gu2022s4, gu2022s4d, smith2023s5}. While these findings provide valuable insights, selective SSMs introduce nonlinearity and dynamic input dependence, making their analysis distinct from LTI SSMs and more closely aligned with the self-attention mechanism. Despite their growing empirical success, theoretical understanding of their generalization capabilities remains limited, with the exceptions of \citet{dao2024transformers_are_ssms}, which establishes a connection between Transformers and SSMs, and \citet{cirone2024theoretical_selective_ssm}, which examines the expressive power of selective SSMs.

\subsection{Contributions}
This work establishes length-independent generalization bounds for the inherently nonlinear state-of-the-art selective SSM models proposed in the Mamba and Mamba-2 architectures \cite{gu2024mamba, dao2024transformers_are_ssms}. Our contributions are the following:
\begin{itemize}
    \vspace{-8pt}
    \setlength{\itemsep}{-1pt}
    \item We present a detailed theoretical analysis of selective SSMs, emphasizing their explicit connection to the self-attention mechanism. This connection enables the use of analytical tools developed for self-attention, alongside those developed for RNNs, providing valuable insights into the structure and properties of this emerging class of foundation models.
    \item A length-independent covering number-based generalization bound is derived for selective SSMs, both with and without the input-dependent discretization scheme proposed by \citet{gu2024mamba}, laying a theoretical foundation for their generalization capabilities and the impact of the discretization scheme.
    \item The evidence of the modelâ€™s length independence is empirically demonstrated using a synthetic sparse majority task and the IMDb large movie review task \cite{maas2011imdb}. Our results highlight the model's scalability, demonstrating its practical applicability to real-world long-sequence modeling challenges.
\end{itemize}

\subsection{Related Work}

This paper is closely related to a line of work by \citet{bartlett2017spectrally, edelman2022inductive, trauger2024length_independent_transformer}, which establishes generalization bounds for general neural networks and attention-based models. We draw explicit connections between the self-attention mechanism and selective SSMs to derive generalization bounds, building on these prior works. The related work discussed here is primarily focused on self-attention, SSMs, and generalization bounds.

\textbf{Self-attention} is the core mechanism behind the Transformer architecture, introduced as an alternative to RNNs and CNNs for sequence processing \cite{vaswani2017attention}. While the concept of attention predates the Transformer, its introduction marked a pivotal shift in the scale of large models. An attention mechanism broadly assigns scores to each pair of elements in a sequence to measure their relevance to each other. Similarly, self-attention mechanism draws inspiration from the key-query analogy used in relational databases to capture dependencies between elements of an input sequence. Since their introduction, Transformers have been extensively studied and refined, leading to numerous variants, including sparse and low-rank adaptations and widespread applications across domains such as natural language processing \cite{devlin2018bert, brown2020gpt3} and computer vision \cite{dosovitskiy2020vision_transformer, peebles2023scalable_diffusion_transformer, liu2024diffusion_vision_transformer}.

\textbf{State-space models} are a new class of foundation models, introduced by \citet{gu2021ssm} as an alternative to Transformers for sequence processing. Rooted in the classical state-space representations introduced by \citet{kalman1960new} in control theory, SSMs leverage state-space representations to efficiently model long-range dependencies in sequential data. The foundation of SSMs can be traced to the HiPPO framework, which established a mathematical basis for encoding and preserving long-range dependencies using orthogonal polynomial projections \cite{gu2020hippo}. Building on this foundation, the first practical implementation of SSMs is the S4 model, which utilized HiPPO as an initialization scheme \cite{gu2022s4}. With the empirical success of S4 on the Long Range Arena benchmark \cite{tay2021long_range_arena}, SSMs gained widespread attention, prompting several extensions and refinements. S4D simplified training with diagonal initializations \cite{gu2022s4d}, S5 introduced a multi-input multi-output structure for greater flexibility \cite{smith2023s5}, and Hyena explored hierarchical convolutions \cite{poli2023hyena}. Selective SSMs introduced in the Mamba model by \citet{gu2024mamba} extend LTI SSMs by using linear projections of the input to construct and discretize SSMs, resulting in a nonlinear time-variant architecture. These properties make selective SSMs closely resemble self-attention, as highlighted by \citet{dao2024transformers_are_ssms} while introducing Mamba-2. 

\textbf{Generalization bounds} are central in the probably approximately correct (PAC) learning framework, which formalizes a model's ability to achieve  low error on unseen data with high probability, provided sufficient training data. PAC learning provides bounds that are essential in understanding why certain architectures generalize well despite their overparameterization. Earlier studies explored statistical guarantees based on VC-dimension and shattering bounds extensively \cite{karpinski1997polynomial, koiran1997neural, sontag1998learning, baum1988size, bartlett1998almost}. The recent works on norm-based generalization bounds utilize Rademacher complexity, a fundamental tool that is used to upper bound the generalization gap. A widely used approach to bound Rademacher complexity involves covering numbers \cite{vershynin2018high}. \citet{zhang2002covering} laid the groundwork for understanding the capacity of regularized linear function classes by covering numbers. Later on, \citet{bartlett2017spectrally} established generalization bounds for neural networks using covering numbers based on the work of \citet{zhang2002covering}. These methods have been extended to Transformers in recent studies by \citet{edelman2022inductive, trauger2024length_independent_transformer, truong2024rank}, where different aspects such as length or rank dependency have been emphasized. For LTI SSMs, the works of \citet{racz2024length, liu2024generalization} draw inspiration from this line of research but primarily leverage the structure of LTI systems to derive their bounds from a state-space perspective.


\section{Preliminaries}
\label{sec:prelim}
% Section \ref{sec:notation} introduces the notation used throughout the paper. Section \ref{sec:attention} provides a technical breakdown of the self-attention mechanism with causal masks, with a particular emphasis on the advantages of linear self-attention for improved efficiency and scalability. Section \ref{sec:ssm} introduces LTI SSMs, highlighting their recurrent and convolutional representations which capture temporal dependencies in an efficient approach for long-range sequences.

\subsection{Notation}
\label{sec:notation}
% \vspace{-2em}
% \setlength{\jot}{0pt}
\begin{tabular}{@{} >{\raggedright}p{0.2\columnwidth} p{0.75\columnwidth} @{}}
    $\mathbb{R},\mathbb{R}^n$ & Set of real numbers, n-tuples \\
    $\mathbb{R}^{n \times m}$ & Set of $n \times m$ real matrices \\
    $x, \bm{X}$ & Vectors (and scalars), matrices \\
    % $\textbf{1}, \textbf{0}, \bm{I}$ & Vector/matrix of all 1s, 0s, identity matrix \\
    $\|x\|_{p}$ & $\ell_{p}$-norm of vector $x$ \\
    $\|\bm{X}\|_p$ & $p$-operator norm of $\bm{X}$ \\
    $\|\bm{X}\|_{p,q}$ & $\| \begin{bmatrix}
        \| X_1 \|_p & \cdots & \| X_n \|_p 
    \end{bmatrix} \|_q$ where $X_i$ is the $i$-th column of $\bm{X}$ \\
    $\bm{X} \otimes \bm{Y}$ & Kronecker product of matrices $\bm{X}$ and $\bm{Y}$ \\
    $\bm{A}_c,\bm{B}_c,\bm{C}_c$ & Continuous-time state-space matrices \\
    $\bm{A},\bm{B},\bm{C}$ & Discrete-time state-space matrices \\
    $N$ & Number of states per channel \\
    $T$ & Sequence length \\
    $d$ & Number of channels \\
    $m$ & Number of samples \\
    $(t), [t]$ & Continuous and discrete time indexing \\
    $u_j^{(i)}(t)$ & The $j^{th}$ channel of the $i^{th}$ sample at time $t$ \\
    $x_{(j)}^{(i)}(t)$ & The state vector $\in \mathbb{R}^N$ for the $j^{th}$ channel \\
    & of the $i^{th}$ sample at time $t$ \\
\end{tabular}

\subsection{Self-Attention}
\label{sec:attention}
Given an input sequence $\bm{U}\in\mathbb{R}^{T \times d}$, the dot product self-attention block is defined as:
\begin{equation}
    \begin{aligned}
        \bm{Y} &= \operatorname{rowsoftmax}\left(\bm{QK}^\top \right)\bm{V}
        % \\&= \operatorname{rowsoftmax} \left(\bm{U} \bm{W}_Q (\bm{U} \bm{W}_K)^\top \right) \bm{U}\bm{W}_V
    \end{aligned}
    \label{eq:attention}
\end{equation}
where $\bm{Q}=\bm{UW}_{\bm{Q}} \in \mathbb{R}^{T \times d_{\bm{QK}}}$, $\bm{K}=\bm{UW}_{\bm{K}} \in \mathbb{R}^{T \times d_{\bm{QK}}}$, $\bm{V}=\bm{UW}_{\bm{V}} \in \mathbb{R}^{T \times d_{\bm{V}}}$ are the query, key, and value matrices which are projections of the data $\bm{U}$ with the learnable weights $\bm{W}_{\bm{Q}}, \bm{W}_{\bm{K}}, \bm{W}_{\bm{V}}$. The self-attention block often includes a nonlinear activation and an output projection, expressed as $\sigma(\bm{Y}) \bm{W}_{\bm{Y}}$ where $\bm{W}_{\bm{Y}} \in \mathbb{R}^{d_{\bm{V}} \times d_o}$ denotes the projection matrix mapping onto the output space. For classification, the output is usually a scalar, $d_o=1$, representing a class. To prevent future dependencies, a causal mask $\bm{L}\in\mathbb{R}^{T\times T}$ is often applied to the attention matrix, leading to $\bm{Y} = \bm{L}\cdot\operatorname{rowsoftmax}\left(\bm{QK}^\top \right)\bm{V}$.

To reduce the computational complexity w.r.t sequence length from $\mathcal{O}(T^2)$ to $\mathcal{O}(T)$, linear self-attention (LSA) drops the $\operatorname{rowsoftmax}$ operation in \eqref{eq:attention} to leverage the associativity of matrix multiplication, resulting in $\bm{Y} = \bm{Q}\left(\bm{K}^\top\bm{V}\right)$.
This modification carries significant importance when dealing with long-range sequences \cite{katharopoulos2020Transformers_are_rnns}. With the causal mask, the final masked LSA is in the form:
\begin{equation}
    \bm{Y} = \bm{Q}\left(\bm{L}(\bm{K}^\top\bm{V})\right).
    \label{eq:masked_linear_attention}
\end{equation}
The connections between various SSM and Transformer architectures are outlined in \cite{dao2024transformers_are_ssms} with a specific focus on the LSA. In Section~\ref{sec:selective_ssms}, we emphasize the connection between selective SSMs and LSA in detail.

\subsection{State-Space Models}
\label{sec:ssm}
LTI SSMs are based on the classical single-input single-output (SISO) state-space representation:
\begin{equation}
    \begin{aligned}
        \dot{x}_{(j)}(t) &= \bm{A}_{c(j)} x_{(j)}(t) + \bm{B}_{c(j)} u_j(t) \\
        y_j(t) &= \bm{C}_{c(j)} x_{(j)}(t) + D_{c(j)} u_j(t)
    \end{aligned}
    \label{eq:state_space}
\end{equation}
where $\bm{A}_{c(j)}\in\mathbb{R}^{N \times N}$, $\bm{B}_{c(j)} \in\mathbb{R}^{N \times 1}$, $\bm{C}_{c(j)} \in\mathbb{R}^{1 \times N}$, and $D_{c(j)} \in\mathbb{R}$ represents the dynamics of a continuous LTI system of a single channel (feature) $u_j(t)$ of the input signal $u(t)$ using the hidden states $x_{(j)}(t)\in\mathbb{R}^N$. The residual connection $D_{c(j)}$ is often dropped, $D_{c(j)} = 0$, and we will assume the same for the rest. The states $x_{(j)}(t)$ of  \eqref{eq:state_space} can be interpreted as latent variables for a neural network that models the input-output relationship of a given dataset for a predefined loss function. 

SSMs aim to use the representation \eqref{eq:state_space} as a block that captures the dynamics of sequential data, similar to Transformers that use the self-attention block for the same purpose. With the notable exception of S5 \cite{smith2023s5}, most SSM implementations use $d$ SISO SSMs as in \eqref{eq:state_space} in a single block, for each channel respectively. To be able to apply \eqref{eq:state_space} to the discrete-time data matrix $\bm{U}$, a discretization step with time-scale $\Delta$ is applied using zero-order hold (ZOH):
\begin{equation}
    \begin{aligned}  
    \bm{A}_{(j)} &= \operatorname{exp}(\Delta \bm{A}_{c(j)}) \\
    \bm{B}_{(j)} &= (\Delta \bm{A}_{c(j)})^{-1}(\operatorname{exp}(\Delta \bm{A}_{c(j)})-\bm{I})\cdot\Delta \bm{B}_{c(j)}
    \end{aligned}
    \label{eq:zoh}
\end{equation}
resulting in the discrete state-space model:
\begin{equation}
    \begin{aligned}
        x_{(j)}[t] &= \bm{A}_{(j)} x_{(j)}[t-1] + \bm{B}_{(j)} u_j[t] \\
        y_j[t] &= \bm{C}_{(j)} x_{(j)}[t].
    \end{aligned}
    \label{eq:state_space_discrete}
\end{equation}
% The system matrices $\bm{A}_{(j)},\bm{B}_{(j)},\bm{C}_{(j)}$ are learnable parameters.
This representation allows the use of the convolution kernel:
\begin{equation}
    \bm{K}_{(j)}[t] = \bm{C}_{(j)} \bm{A}_{(j)}^{t-1} \bm{B}_{(j)} \text{ for } t \geq 1 \text{ and } \bm{K}_{(j)}(0) = 0
\end{equation}
to calculate the output:
\begin{equation}
    \begin{aligned}
        y_j &= \bm{K}_{(j)} \ast u_j \\
        y_j[t] &= \bm{C}_{(j)} \bm{A}_{(j)}^{t-1}\bm{B}_{(j)} u_j[0] + \bm{C}_{(j)} \bm{A}_{(j)}^{t-2} \bm{B}_{(j)} u_j[1] \\ 
        & + ... + \bm{C}_{(j)} \bm{B}_{(j)} u_j[t-1].
    \end{aligned}
    \label{eq:conv_kernel}
\end{equation}

While the state-space representation \eqref{eq:state_space_discrete} is faster during inference as it can be computed recurrently similar to RNNs, the convolution kernel \eqref{eq:conv_kernel} can be used with fast Fourier transform (FFT) to train the model very efficiently, overcoming the bottleneck of training recurrent models. Although these blocks provide simplicity and efficiency, they are limited in capturing complex and non-stationary dynamics in sequential data since they assume time-invariant parameters.

\section{Selective SSMs and Self-Attention}
\label{sec:selective_ssms}

Selective SSMs, introduced with Mamba \cite{gu2024mamba}, use an input-dependent nonlinear framework to deal with complex and non-stationary dynamics. The continuous state-space matrices $\bm{B}_{c(j)}$, $\bm{C}_{c(j)}$, and $\Delta$ are parameterized as:
\begin{equation}
    \begin{aligned}
       \bm{B}_{c(j)}(t) &= \bm{W_B} u(t) \\
       \bm{C}_{c(j)}(t) &= u^\top(t) \bm{W_C}^\top \\ 
       \Delta(t) &= \tau_\Delta(p + q^\top u(t)).
    \end{aligned}
\label{eq:mamba_projections}
\end{equation}
Here,  $\bm{W_B},\bm{W_C}\in\mathbb{R}^{N\times d}, q \in \mathbb{R}^{d}$ and $p \in \mathbb{R}$ are learnable weights and $\tau_\Delta$ is the softplus function. The input and output matrices $\bm{B}_{c(j)}$ and $\bm{C}_{c(j)}$ are linear projections of the input $u(t)$. 
% The discretization vector $\Delta$ is a broadcast operation over a linear projection of the input. 
This structure parallels the attention mechanism \eqref{eq:attention}, where $\bm{B}_{c(j)}$ and $\bm{C}_{c(j)}$ resemble the key-query interaction. Thus, the state-space model for channel $j$ is:
\begin{equation}
\begin{aligned}
    \dot{x}_{(j)}(t) &= \bm{A}_{c(j)} x_{(j)}(t) + \bm{W}_{\bm{B}} u(t) u_j(t) \\
    y_j &= u^\top \bm{W}_{\bm{C}}^{\top} x_{(j)}(t).
    \label{eq:mamba_siso}
\end{aligned}
\end{equation}
To derive a state-space model including all states and inputs, we stack the states in \eqref{eq:mamba_siso} to get one single state vector:
\begin{equation}
    x = \begin{bmatrix} x_{(1)}^\top \; \dots\; x_{(d)}^\top \end{bmatrix}^{\top} \in \mathbb{R}^{Nd} 
    \label{eq:all_states}
\end{equation}
that satisfies the following state-space model:
\begin{equation}
\begin{aligned}
    \dot{x}(t) &= \bm{A}_c x(t) + \bm{B}_c u(t) \\
    y(t) &= \bm{C}_c x(t)
    \label{eq:selective_full_continuous}
\end{aligned}
\end{equation}
in which
\begin{equation}
\begin{aligned}
    \bm{A}_c &= \begin{bmatrix}
         \bm{A}_{(1)} & \bm{0} & \cdots & \bm{0} \\
        \bm{0} & \bm{A}_{(2)} & \cdots & \bm{0} \\
        \vdots & \vdots & \ddots & \vdots \\
        \bm{0} & \bm{0} &  \cdots & \bm{A}_{(d)}
    \end{bmatrix} \\
    \bm{B}_c(t) &= \bm{I}_d \otimes \bm{W_B} u(t) \\
    \bm{C}_c(t) &= \bm{I}_d \otimes u(t)^\top \bm{W_C}^\top.
\end{aligned}
\label{eq:ABC_full}
\end{equation}
For the discretization step, we follow the official implementation of Mamba \cite{gu2024mamba}:
\begin{equation}
    \begin{aligned}
        \bm{A}[t] &= \operatorname{exp}(\Delta(t) \bm{A}_c) \\
        \bm{B}[t] &= \Delta(t) \bm{B}_c \\
        \bm{C}[t] &= \bm{C}_c
    \end{aligned}
    \label{eq:discretization_mamba}
\end{equation}
which uses ZOH for the matrix $\bm{A}$, and a simplified Euler discretization for the matrix $\bm{B}$. The matrix $\bm{C}$ remains the same as $\bm{C}_c$ since it represents a static relationship between the output $y$ and state $x$. This approach avoids the computational overhead of matrix inversion for $\bm{B}$, as required by the ZOH in \eqref{eq:zoh}. Utilizing this discretization procedure and the selective SSM architecture in \eqref{eq:selective_full_continuous} results in the following discrete-time state-space model:
\begin{equation}
\label{eq:selective_ssm}
    \begin{aligned}
        x[t+1] &= e^{\Delta[t] \bm{A}_c} x[t] + \Delta[t] \big( \bm{I}_d \otimes \bm{W_B} u[t] \big) u[t] \\
        y[t] &= \big( \bm{I}_d \otimes u[t]^\top \bm{W_C}^\top \big) x[t] \\
        \Delta[t] &= \tau_\Delta(p + q^\top u[t]).
    \end{aligned}
\end{equation}
Assuming $x(0) = 0$, we can unroll this recursive relation:
\begin{equation}
\label{eq:unrolled_io_selective}
    \begin{aligned}
        y[t'] &= \big( \bm{I}_d \otimes u[t']^\top \bm{W_C}^\top \big) \sum_{t=0}^{t'-1} \Big( \bm{A}^t \Delta[t'-1-t] \\
        &\quad \big( \bm{I}_d \otimes \bm{W_B} u[t'-1-t] \big) u[t'-1-t] \Big) \\
    \end{aligned}
\end{equation}
in which $\bm{A}^t$ is a shorthand notation for
\begin{equation}
\begin{aligned}
    \bm{A}^0 &= \bm{I} \\
    \bm{A}^t &= e^{(\Delta[t'-1] + \cdots + \Delta[t'-t])\bm{A}_c} \text{ for } t>0.
    \label{eq:A^t_shorthand}
\end{aligned}
\end{equation}
For later discussion, we aim to examine how discretization influences the capacity of SSMs. 
To that end, we present the discretized state-space model under the assumption of a fixed step size $\Delta[t] = 1$, implying that $\bm{A} = e^{\bm{A}_c}$. Applying \eqref{eq:discretization_mamba} with a fixed $\Delta$ to \eqref{eq:selective_full_continuous} yields
\begin{equation}
    \begin{aligned}
        x[t+1] &= \bm{A} x[t] + \big( \bm{I}_d \otimes \bm{W_B} u[t] \big) u[t] \\
        y[t] &= \big( \bm{I}_d \otimes u[t]^\top \bm{W_C}^\top \big) x[t]
    \end{aligned}
    \label{eq:selective_full_discrete}
\end{equation}
with its unrolled form
\begin{equation}
\begin{aligned}
    y[t'] &= \big( \bm{I}_d \otimes u[t']^\top \bm{W_C}^\top \big) \sum_{t=0}^{t'-1} \Big( \bm{A}^t \\
    &\quad \big( \bm{I}_d \otimes \bm{W_B} u[t'-1-t] \big) u[t'-1-t] \Big).
\end{aligned}
    \label{eq:unrolled_io}
\end{equation}

Recall the masked LSA in Section~\ref{sec:attention}:
\begin{flalign}
    % & \text{\begin{tabular}{c}Masked \\ LSA\end{tabular}} \hspace{-0.5em} : \hspace{1em}
    \begin{aligned}
        \bm{Y} = \bm{Q} \cdot \bm{L} \left(\bm{K}^\top \bm{V} \right).
        % &= \bm{UW_Q} \cdot \bm{L} \left(\bm{UW_K}^\top \bm{UW_V} \right)
    \end{aligned}
    % &&
    \label{eq:masked_lsa_unrolled}
\end{flalign}
Reformulating \eqref{eq:unrolled_io} using \eqref{eq:ABC_full} reveals the connection between selective SSMs and LSA explicitly:
\begin{flalign}
    % & \text{\begin{tabular}{c}Selective \\ SSM\end{tabular}} \hspace{-0.5em} : \hspace{-1.2em}
    \begin{aligned}
        \underbrace{y[t']}_{\bm{Y}[t',:]} = \underbrace{\bm{C}[t']}_{\sim\bm{Q}} \; \underbrace{\sum_{t=0}^{t'-1}\bm{A}^t}_{\sim\bm{L}} \; \underbrace{\bm{B}[t'-1-t]}_{\sim\bm{K}} \;\underbrace{u[t' - 1 - t]}_{\sim\bm{V}}.
    \end{aligned}
    % &&
    \label{eq:ssm_lsa}
\end{flalign}
The input-output matrices $\bm{B}, \bm{C}$ play the same role as the key-query matrices $\bm{K}, \bm{Q}$ in \eqref{eq:masked_lsa_unrolled}. The summation $\sum_{t=0}^{t'-1}\bm{A}^{t}$ serves a dual purpose: (i) enforcing causality by summing up to the current timestep, analogous to the causal mask $\bm{L}$ in \eqref{eq:masked_lsa_unrolled}, and (ii) capturing temporal dependencies via the state matrix $\bm{A}$, as in RNNs. The input $\bm{U}$ is directly used instead of the value matrix $\bm{V}$, which can be thought as replacing $\bm{V}$ with an identity matrix as a linear projection. The output $y[t']$ corresponds to $\bm{Y}(t',:)$, the row of $\bm{Y}\in \mathbb{R}^{T\times d_{\bm{V}}}$ at index $t'$. For classification tasks requiring a scalar output, an additional parameter $w \in \mathbb{R}^d$ is introduced similar to what is discussed in Section~\ref{sec:attention}. This parameter corresponds to a linear layer applied to the last time step of the output sequence to obtain a label as $z=w^\top y[T]$, which captures all past information. The final architecture for a selective SSM can be parametrized with the learnable parameters as:
\begin{equation}
    \bar{\Theta}_{\text{SSM}} = \{\bm{A}, \bm{W_B}, \bm{W_C}, w \}.
    \label{eq:ssm_fixed_theta}
\end{equation}
This parametrization is for the fixed $\Delta$. For the input-dependent parametrization of $\Delta$ from \eqref{eq:selective_ssm}, we have:
\begin{equation}
    \Theta_{\text{SSM}} = \{\bm{A}_c, \bm{W_B}, \bm{W_C}, p, q, w \}.
    \label{eq:ssm_theta}
\end{equation}
These structural similarities between selective SSMs and Transformers motivate us to analyze their generalization capabilities through the lens of self-attention.


\section{Generalization Bounds for Selective SSMs}
\label{sec:generalization}
 In this section, we analyze the generalization capabilities of selective SSMs by establishing an upper bound on the generalization error. To do so, we first recall the definition of Rademacher complexity.
\begin{definition}[\textbf{Rademacher complexity}]
    For a given real-valued Function class $\mathcal{F}$ and a set of vectors $S = \{u^{(i)}\}_{i=1}^m$, the empirical Rademacher complexity is
\begin{equation}
    \operatorname{Rad}(\mathcal{F}, S) = \frac{1}{m} \mathbb{E}_\sigma \left( \sup_{f \in \mathcal{F}} \sum_{i=1}^m \sigma_i f(u^{(i)}) \right)
    \label{eq:rademacher}
\end{equation}
where $\sigma_i\in\{-1,1\}$ are uniformly distributed i.i.d Rademacher random variables.
% \begin{equation*}
%     P(\sigma_i=+1)=P(\sigma_i=-1)=\frac{1}{2}.
%     \label{eq:rademacer_random_var}
% \end{equation*}
\end{definition}
Rademacher complexity serves as the primary tool for bounding the generalization gap. It measures how well the functions in a hypothesis class can fit random noise; a higher value indicates more capacity to overfit arbitrary data whereas lower values indicate better generalization potential \cite{shalev2014understanding}. The following theorem provides an upper bound on the generalization error using Rademacher complexity.
\begin{theorem}[\citet{mohri2018foundations_machine_learning}, Theorem 3.3]
\label{thm:gen_error_rademacher}
Let $\mathcal{F}$ be a hypothsis class $\{ f:\mathcal{U
}\rightarrow\mathcal{Z}\}$, and $S$ be the training set $\{u^{(i)}, z^{(i)}\}_{i=1}^m$. Assume the loss function $l:\mathcal{Z} \times \mathcal{Z} \rightarrow \mathbb{R}$ is upper bounded by the constant $\frak{c}_l$. Then, with probability more than $1 - \delta$
    \begin{equation}
    \begin{aligned}
        &\left| \mathbb{E}_{u,z}(l(f(u), z)) - \frac{1}{m} \sum_{i=1}^m l\left(f(u^{(i)}), z^{(i)} \right) \right| \\
        &\leq 2 \operatorname{Rad}(l \circ \mathcal{F}, S) + 3\frak{c}_l \sqrt{\frac{\log(\frac{2}{\delta})}{2m}}.
    \end{aligned}
    \end{equation}
\end{theorem}
Bounding the Rademacher complexity of complex function classes, such as foundation models, is challenging. A common approach is to decompose the model into smaller components and analyze each of them separately by employing covering numbers. Covering numbers quantify the size of each component's function class by determining how many smaller subsets, or ``balls", are needed to cover it. Formally, the covering number is defined as follows.
\begin{definition}[\textbf{Covering number}]
    Let $\mathcal{M}$ be a metric space with metric $\mu$. A subset $\mathcal{H} \subset \mathcal{M}$ is an $\epsilon$-cover for $\mathcal{M}$ if for every $h \in \mathcal{M}$, there exists $\hat{h} \in \mathcal{H}$ such that $\mu(h,\hat{h}) \leq \epsilon$. The covering number $\mathcal{N}(\mathcal{M}, \epsilon, \mu)$ is the cardinality of the smallest $\epsilon$-cover of $\mathcal{M}$.
    \label{def:covering_number}
\end{definition}

In our analysis, two distinct types of covering numbers are employed. The metric space $\mathcal{M}$ in Definition~\ref{def:covering_number} can be chosen as a collection of matrices equipped with the metric induced by a matrix norm. We deploy this definition to construct a cover for the parameters $\bm{A}_c$ and $p$, similar to the existing covering techniques developed for RNNs. On the other hand, let $\mathcal{F} = \{ f : \mathcal{U} \rightarrow \mathcal{Z} \}$ denote a function class, where $\mathcal{Z}$ is equipped with a norm $\| \; \|$, and let $S = \{u^{(i)}\}_{i=1}^m$ be a dataset. By equipping $\mathcal{F}$ with the following metric
\begin{equation}
\label{eq:data_metric}
    \mu_{p,\| \; \|}(f, \hat{f}) = \left( \frac{1}{m} \sum_{i=1}^m \left\| f(u^{(i)}) - \hat{f}(u^{(i)}) \right\|^p \right)^{1/p},
\end{equation}
we obtain a data-dependent covering number for a function class, to construct covers for $\bm{W}_{\bm{B}}, \bm{W}_{\bm{C}}, q$, and $w$. This is parallel to the cover construction for the $\bm{Q},\bm{K},\bm{V}$ matrices in self-attention. For notational convenience, we denote the covering number of a function class $\mathcal{N}(\mathcal{F}, \epsilon, \mu_{p,\| \; \|})$ by $\mathcal{N}_p(\mathcal{F}_{|S}, \epsilon, \| \; \|)$ which is utilized to bound the Rademacher complexity of a function class as outlined herein:
\begin{theorem}[\citet{bartlett2017spectrally}, Lemma A.5]
\label{thm:dudley}
    Given a real-valued function class $\mathcal{F} = \{ f:\mathcal{U} \rightarrow \mathbb{R} \}$ such that $\forall u \in \mathcal{U}, \; | f(u) | \leq \frak{b}$ and a set of vectors $S = \{u^{(i)}\}_{i=1}^m$, we have
    \begin{equation}
        \begin{aligned}
            &\operatorname{Rad}(\mathcal{F}, S) \leq \\
            &\inf_{\alpha > 0} \left( 4 \alpha + 12 \int_\alpha^{\frak{b}} \sqrt{\frac{\log \mathcal{N}_2 (\mathcal{F}_{|S}, \epsilon, \| \; \|_2)}{m}} \; d\epsilon \right).
        \end{aligned}
            \footnote{This is a modified version of Dudley's integral theorem \cite{dudley1967sizes}. The proof presented in \cite{bartlett2017spectrally} ignored the normalization by $m$ in \eqref{eq:data_metric} and takes $\frak{b}=1$ in \eqref{eq:dudley}.}
        \label{eq:dudley}
    \end{equation}
\end{theorem}
Combining Theorems \ref{thm:gen_error_rademacher} and \ref{thm:dudley} results in a covering number-based generalization bound, as outlined in Lemma \ref{lem:generalization_error_bound_last}. The remaining task is to cover the space of selective SSMs, which we outline in the proof sketch and refer to the appendix for details. To investigate the role of the selective-scan mechanism, we first present the generalization error bounds for fixed step-size SSMs described in \eqref{eq:selective_full_discrete}. 

\subsection{Fixed Step Size $\Delta$}
\label{sec:fixed_selective_scan}
The following is a list of assumptions necessary to derive the generalization bound.
\begin{itemize}
    \vspace{-8pt}
    \setlength{\itemsep}{-1pt}
    \item Consider the input sequence $u[0], \cdots, u[T]$. It is bounded as $\|u[t]\|_2 \leq \frak{B}_u$ for each $t \in \{0, \cdots, T\}$.
    \item The matrix $\bm{A}$ is a contraction: $\|\bm{A}\|_2 \leq \rho_{\bm{A}} < 1$ and $\|\bm{A}\|_{2,1} \leq \frak{M}_A$. Note that the contractivity of a matrix is a stronger notion than its stability.
    \item $\| \bm{W_B}\|_2 \leq \frak{B}_{\bm{B}}$ and $\| \bm{W_B} \|_{1,1} \leq \frak{M}_B$.
    \item $\|\bm{W_C}\|_2 \leq \frak{B}_{\bm{C}}$ and $\| \bm{W_C} \|_{1,1} \leq \frak{M}_C$.
    \item $\|w\|_2 \leq \frak{B}_w$ $\|w\|_1 \leq \frak{M}_w$.
    \item The loss function $l(\cdot)$ is upper bounded by the constant $\frak{c}_l$ and Lipschitz continuous with constant $\frak{l}_l$.
\vspace{-8pt}
\end{itemize}
The contractivity of $\bm{A}$ is necessary to avoid exponential dependency on the sequence length as a result of the recurrence in SSMs. The assumptions on the loss function are required to support the use of Lemma~\ref{lem:generalization_error_bound_last}. The $\| \; \|_2$, $\| \; \|_1$, and $\| \; \|_{1,1}$ norms are chosen to facilitate the application of Lemma~\ref{lem:linear_func_cover}. It is possible to use alternative norms and lemmas to bound the covering number of a function class, as in \citet{zhang2002covering, trauger2024length_independent_transformer, truong2024rank}. Nonetheless, careful consideration must be taken to avoid introducing additional dependency on the sequence length. Likewise, the $\| \; \|_{2,1}$ norm is chosen to avoid additional dependencies on $N$ or $d$. However, it can be replaced by other norms through alternative instantiations of Lemma~\ref{lem:matrix_cover}. With these assumptions in place, we present the following theorem, with its proof provided in Appendix~\ref{sec:fixed_gen_err_proof}.
\begin{theorem}
\label{thm:gen_err_bound_fixed}
Let $S = \{u^{(i)}, z^{(i)}\}_{i=1}^m$ be the training set and $\bar{\mathcal{F}}_{\text{SSM}}$ be all selective SSM blocks with fixed step size described in \eqref{eq:selective_full_discrete}. If the assumptions in Section \ref{sec:fixed_selective_scan} are satisfied, then with probability more than $1 - \delta$ the following length-independent bound holds:
\begin{equation}
    \begin{aligned}
        & \left| \mathbb{E}_{u,z}(l(h(u), z)) - \frac{1}{m} \sum_{i=1}^m l\left(h(u^{(i)}), z^{(i)}\right) \right| \leq \\
        & \frac{12 \frak{l}_l \mathcal{C}_{\bar{\mathcal{F}}_{\text{SSM}}}}{\sqrt{m}} \left( 1 + \log\left(\frac{\frak{c}_l\sqrt{m}}{3 \mathcal{C}_{\bar{\mathcal{F}}_{\text{SSM}}}}\right) \right) + 3\frak{c}_l \sqrt{\frac{\log\left(\frac{2}{\delta}\right)}{2m}}
    \end{aligned}
    \label{eq:gengap_fixed}
\end{equation}
in which
\begin{equation}
    \mathcal{C}_{\bar{\mathcal{F}}_{\text{SSM}}} = \mathcal{\tilde{O}} \left( \frac{\frak{B}_w \frak{B}_u^3 \frak{B}_{\bm{B}} \frak{B}_{\bm{C}} \frak{B}_{\bm{A}} \rho_{\bm{A}}}{(1 - \rho_{\bm{A}})^2} N^{1/2} d^{1/2} \right).
    \label{eq:capacity_fixed}
\end{equation}
\end{theorem}
Since the results for fixed and input-dependent step sizes are closely related, we first present the input-dependent case, followed by a proof sketch and analysis that ties the two together and highlights their implications.

\subsection{Input-Dependent Step Size $\Delta$}
\label{sec:input_dependent_selective_scan}
To derive the upper bounds for the generalization gap of the selective SSM with input-dependent $\Delta$ described in \eqref{eq:selective_ssm}, we require additional assumptions:
\begin{itemize}
    \vspace{-8pt}
    \setlength{\itemsep}{-1pt}
    \item The matrix $\bm{A}_c$ is normal and has strictly stable symmetric part $\bm{A}_c^{sym} = 1/2(\bm{A}_c+\bm{A}_c^{\top})$ with stability margin $s_A$: $\lambda_i(\bm{A}_c^{\text{sym}}) \leq - s_A$.
    \item $\| \bm{A}_c \|_2 \leq \frak{B}_{\bm{A}}$ and $\| \bm{A}_c \|_{2,1} \leq \frak{M}_{A}$.
    \item $ \frak{L}_p \leq p \leq \frak{B}_p$.
    \item $\| q\|_2 \leq \frak{B}_q$ and $\| q \|_1 \leq \frak{M}_q$.
    % \vspace{-8pt}
\end{itemize}
The justification for the choice of the norms $\| \; \|_2$, $\| \; \|_1$, and $\| \; \|_{2,1}$ is similar to that of the fixed step size case. The condition on $\bm{A}_c$ ensures the contractivity of $\bm{A} = e^{\bm{A}_c}$, with the proof provided in Lemma~\ref{lem:symm_negative_def}. This condition includes stable diagonal matrices, which are used in the official implementations of Mamba and Mamba-2 \cite{gu2024mamba, dao2024transformers_are_ssms}. 

The lower bound on $p$ is crucial, as it is needed to lower bound $\Delta[t]$ in Lemma~\ref{lem:A^t_rho^t_bound_selective} to establish an upper bound on $\|\bm{A}^t\|_2$. This uniform bound, together with the contractivity of $\bm{A}$, allows us to construct a cover for the state matrix, despite its input-dependent nature as illustrated in Lemma~\ref{lem:A^t-Ahat^t_bound_selective}. With these assumptions in place, we present the following theorem, with its proof provided in Appendix~\ref{sec:input_dependent_gen_err_proof}.

\begin{theorem}
\label{thm:gen_err_bound_selective}
Let $S = \{u^{(i)}, z^{(i)}\}_{i=1}^m$ be the training set and $\mathcal{F}_{\text{SSM}}$ be all selective SSM blocks described in \eqref{eq:selective_ssm}. If the assumptions in Section \ref{sec:input_dependent_selective_scan} are satisfied, then with probability more than $1 - \delta$ the length-independent bound \eqref{eq:gengap_fixed} holds in which $\mathcal{C}_{\bar{\mathcal{F}}_{\text{SSM}}}$ is replaced by
\begin{equation}
    \begin{aligned}
        \mathcal{C}_{\mathcal{F}_{\text{SSM}}} &= \mathcal{\tilde{O}}\left( \frac{\frak{M}_\Delta \frak{B}_w \frak{B}_u^3 \frak{B}_{\bm{B}} \frak{B}_{\bm{C}} \frak{B}_{\bm{A}} \rho_{\bm{A}}}{(1 - \rho_{\bm{A}})^2} \right. \\
        &\quad \left. (\frak{M}_\Delta^{2/3} N^{1/3} d^{1/3} + \frak{B}_q^{2/3} \frak{B}_u^{2/3} )^{3/2} \right)
    \end{aligned}
    \label{eq:capacity_input_dep}
\end{equation}
and
\begin{equation}
\begin{aligned}
    \frak{M}_{\Delta} &= \log(1 + e^{\frak{B}_p + \frak{B}_q \frak{B}_u}) \\
    \rho_{\bm{A}} &= \left( 1 + e^{\frak{L}_p - \frak{B}_q \frak{B}_u} \right)^{-s_A} < 1
\end{aligned}
\end{equation}
with $\rho_{\bm{A}} < 1$ for $s_A > 0$.
\end{theorem}
The notation $\mathcal{\tilde{O}}(\cdot)$ ignores logarithmic dependencies on $N$ and $d$, but not $T$. The terms $ \mathfrak{M}_{(\cdot)} $ do not appear in the capacity expressions $ \mathcal{C}_{\bar{\mathcal{F}}_{\text{SSM}}} $ and $ \mathcal{C}_{\mathcal{F}_{\text{SSM}}} $, with the exception of $\frak{M}_{\Delta}$. This is the result of an assumption $\frak{M_{(\cdot)}}=\frak{B}_{(\cdot)}$, made for the ease of presentation in the proof. To ensure clarity in the derivation, these terms are handled separately throughout the proof and the assumption is incorporated only at the final stage. 

\subsection{Proof Sketch and Analysis}
\textbf{Proof Sketch of Theorems \ref{thm:gen_err_bound_fixed} and \ref{thm:gen_err_bound_selective}.} 
Selective SSMs are parameterized by $\bar{\Theta}_{\text{SSM}} = \{\bm{A}, \bm{W_B}, \bm{W_C}, w \}$ for fixed $\Delta$ \eqref{eq:ssm_fixed_theta} and $\Theta_{\text{SSM}} = \{\bm{A}_c, \bm{W_B}, \bm{W_C}, p, q, w \}$ for input-dependent $\Delta$ \eqref{eq:ssm_theta}. 

For SSMs with fixed $\Delta$, denoted $\bar{\mathcal{F}}_{\text{SSM}}$, the proof requires covering the parameters in $\bar{\Theta}_{\text{SSM}}$. The parameters $\bm{W_B}, \bm{W_C}, w$ are covered as linear function classes using Lemma~\ref{lem:linear_func_cover}, while the state matrix $\bm{A}$ is covered as a matrix family with an appropriate norm via Lemma~\ref{lem:cover_A_fixed}. Lemma~\ref{lem:eq_constr_optim} then determines the optimal covering radii across parameters, yielding a tight bound on the complexity of $\bar{\mathcal{F}}_{\text{SSM}}$. The combined cover is used with Lemma~\ref{lem:generalization_error_bound_last} to bound the generalization gap.

For SSMs with input-dependent $\Delta$, denoted $\mathcal{F}_{\text{SSM}}$, we follow the same steps but we cannot directly cover the discrete state-matrix $\bm{A}$ anymore. Instead, we bound its dependencies: $\bm{A}_c$, $p$, and $q$. The cover for $\bm{A}_c$ is provided in Lemma~\ref{lem:cover_A_selective}, while $q$ is handled using Lemma~\ref{lem:linear_func_cover}. Since $p$ is a scalar, it is covered directly. For full proofs, we refer the reader to Appendices~\ref{sec:fixed_gen_err_proof} and~\ref{sec:input_dependent_gen_err_proof}.
\qed

% To establish the generalization bound for SSMs with fixed $\Delta$, denoted $\bar{\mathcal{F}}_{\text{SSM}}$ (as in Theorem~\ref{thm:gen_err_bound_fixed}), we apply Lemma~\ref{lem:eq_constr_optim}. This lemma combines the covering number bounds for each parameter in $\bar{\Theta}_{\text{SSM}}$, thus covering the entire space of $\bar{\mathcal{F}}_{\text{SSM}}$. For SSMs with input-dependent $\Delta$, denoted $\mathcal{F}_{\text{SSM}}$ (as in Theorem~\ref{thm:gen_err_bound_selective}), we parametrize $\Delta$ as in \eqref{eq:selective_ssm}, which introduces the additional need to cover the parameters $\bm{A}_c, p$, and $q$ in order to cover $\bm{A}$. The covering number bounds for the parameters in both $\bar{\Theta}_{\text{SSM}}$ and $\Theta_{\text{SSM}}$ can be derived using the Lemmas in Appendix \ref{sec:covering_numbers}. For the full proof, we refer the readers to the Appendices \ref{sec:fixed_gen_err_proof} and \ref{sec:input_dependent_gen_err_proof}.

\textbf{Analysis.} The bound derived in Theorem~\ref{thm:gen_err_bound_selective} incorporates elements from RNNs. Consider a vanilla RNN:
\begin{equation}
\begin{aligned}
\label{eq:RNN}
    x[t] &= \sigma_x(\bm{A} x[t-1] + \bm{B}u[t]) \\
    y[t] &= \sigma_y(\bm{C}x[t])
\end{aligned}
\end{equation}
where $\sigma_x$ is $\rho_x$-Lipschitz and bounded. Selective SSMs resemble RNNs but with the activation function set to the identity. Thus, stability plays a crucial role in bounding generalization error. The emergence of the terms $\rho_{\bm{A}}^T$ and $T\rho_{\bm{A}}^T$ in the proof is analogous to the appearance of $(\sigma_x \|\bm{A}\|_2)^T$ and $T(\sigma_x \|\bm{A}\|_2)^T$ in existing generalization bounds for RNNs \cite{chen2019generalization_rnn, cheng2024risk_rnn}. These terms introduce $T$ into the bound which leads to one critical aspect of the proof: \textbf{eliminating the dependency on the sequence length} $T$. We achieve this by leveraging the stability of the state matrix during the unrolling process, similar to techniques employed for RNNs 
(see Lemmas \ref{lem:sum_A^t_B_bound_fixed} and \ref{lem:A_cover_selective}). The stability assumptions we adopt for the state matrices $\bm{A}_c$ and $\bm{A}$ are standard in the literature, consistent with prior studies on RNNs \cite{zhang2018stabilizing, chen2019generalization_rnn} and SSMs \cite{wang2024stablessm, racz2024length}. 

Transformers also exhibit a dependency on the sequence length $T$ through the attention mechanism. To avoid this dependency, we use Lemma~\ref{lem:linear_func_cover}, following a similar approach previously applied to Transformers \cite{trauger2024length_independent_transformer}. Furthermore, in the proof of Theorem~\ref{thm:gen_err_bound_selective}, where we account for the effect of input-dependent time scale during discretization, the Lipschitz continuity of the matrix exponential and softplus functions are utilized. This is addressed in Lemmas \ref{lem:e^X-e^Y_bound} and \ref{lem:delta_cover}.

The generalization error bound derived for Transformers \cite{edelman2022inductive, trauger2024length_independent_transformer} exhibits a linear dependency on the bounds of $\bm{W}_{\bm{K}}$ and $\bm{W}_{\bm{Q}}$, along with a cubic dependency on the bound of the input $\frak{B}_u$, which aligns closely with our bound. As discussed in Section~\ref{sec:selective_ssms}, the key-query pair $\bm{K},\bm{Q}$ corresponds to $\bm{B}[t], \bm{C}[t]$, which is linked to the weight matrices $\bm{W_B}$ and $\bm{W_C}$ via the Kronecker product with $\bm{I}_d$. Consequently, the bounds on the weight matrices, $\frak{B}_{\bm{B}}$ and $\frak{B}_{\bm{C}}$, must be scaled by $d^{1/2}$ to match the corresponding bounds for $\bm{K}$ and $\bm{Q}$. Moreover, in addition to the explicit dependencies on $N$ and $d$ given in Theorems \ref{thm:gen_err_bound_fixed} and \ref{thm:gen_err_bound_selective}, the terms $\frak{B}_w$ and $\frak{B}_u$ depend on $d$, while $\frak{B}_{\bm{B}}$, $\frak{B}_{\bm{C}}$, and $B_{\bm{A}}$ depend on $N$. Therefore, these implicit factors should be carefully considered when analyzing the dependencies on $d$ and $N$. Finally, the emergence of the term $\frak{M}_\Delta = \mathcal{O}(\frak{B}_p + \frak{B}_q \frak{B}_u)$ is unique to the input-dependent time variation of $\Delta$ in selective SSMs, which is the key difference between Theorems \ref{thm:gen_err_bound_fixed} and \ref{thm:gen_err_bound_selective}.

% Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}
% I acknowledge ...

\begin{figure*}[t]
    \centering
    \begin{minipage}{0.44\textwidth}
        \centering
        \includegraphics[width=0.85\textwidth]{figures/train_test_sA_0.0_N_4_d_16_m_1000_ep_40_bs_64_lr_0.01_wd_1e-05_delta_True_seed_2.pdf}
        \vspace{-1mm}
    \end{minipage} \hspace{30pt}
    \begin{minipage}{0.44\textwidth}
        \centering
        \includegraphics[width=0.85\textwidth]{figures/train_test_sA_0.0_N_4_d_16_m_25000_ep_30_bs_256_lr_0.01_wd_1e-05_delta_True_seed_2.pdf}
        \vspace{-1mm}
    \end{minipage}
    \begin{minipage}{0.44\textwidth}
        \centering
        \includegraphics[width=0.85\textwidth]{figures/gengap_sA_0.0_N_4_d_16_m_1000_ep_40_bs_64_lr_0.01_wd_1e-05_delta_True_seed_2.pdf}
        \vspace{-1mm}
        \caption{\textbf{Sparse Majority}. Top: Training/test accuracies for sequence lengths ranging from 25 to 400 in steps of 25. Bottom: The generalization gap (0-10\%).}
        \label{fig:sparse_majority}
    \end{minipage} \hspace{30pt}
    \begin{minipage}{0.44\textwidth}
        \centering
        \includegraphics[width=0.85\textwidth]{figures/gengap_sA_0.0_N_4_d_16_m_25000_ep_30_bs_256_lr_0.01_wd_1e-05_delta_True_seed_2.pdf}
        \vspace{-1mm}
        \caption{\textbf{IMDb}. Top: Training/test accuracies for sequence lengths ranging from 100 to 2000 in steps of 100. Bottom: The generalization gap (0-30\%).}
        \label{fig:imdb}
    \end{minipage}
    % \caption{.}
    \label{fig:quad-figure}
    \vspace{-10pt}
\end{figure*}

\section{Experiments}
To validate the main findings discussed in the previous section, we conduct experiments on one synthetic and one real dataset. These experiments aim to evaluate the length-independence of the generalization capabilities of selective SSMs, as predicted by our theoretical analysis. We train and test the model on sequences of varying lengths, allowing us to directly assess the generalization behavior across different sequence scales. A brief description of the two tasks used in the study is provided below.

\textbf{Sparse Majority:} The first task uses a synthetic sparse majority dataset, inspired by \cite{trauger2024length_independent_transformer}. Each input sequence consists of ones and zeros, where each element is embedded into a vector of dimension $d$. The task is to determine whether the sequence contains more ones than zeros. This dataset is specifically designed to evaluate the sequence length independence of the model's generalization capabilities. Although the theoretical complexity of the task remains unchanged regardless of the sequence length, increasing the sequence length introduces additional computational challenges. This makes the task well-suited for measuring trends in performance or generalization gap as a function of sequence length $T$. Furthermore, noise is introduced during training by randomly flipping a small percentage of the inputs after labeling, adding a layer of difficulty. The noise limits the model's accuracy around $95\%$ during training, preventing it from overfitting despite the simplicity of the task.

\textbf{IMDb:} For the second task, we use the IMDb large movie review dataset consisting of 50K movie reviews with sequence lengths ranging from 10 to 3157 tokens \cite{maas2011imdb}. The task is to classify each review as positive or negative based on the sentiment conveyed in the text. This dataset presents a significant challenge as it requires the model to capture contextual information from real text data with high variation in sequence lengths. To be able to train the model across different sequence lengths, we use the entire dataset by truncating/padding each sample according to a predetermined sequence length. For shorter sequences (100â€“300 tokens), sentiment indicators are often clear early, aiding prediction, while longer sequences require retaining full context for accuracy. Truncating longer sequences leads to a loss of important context, as seen in the test loss in Figure~\ref{fig:imdb}. Notably, the average review length in the dataset is around 300, meaning that truncating sequences that contain more than 300 tokens does not drastically harm model performance. Therefore, the test loss and generalization gap both stabilize for larger $T$, indicating that enough information is preserved in the sequences for the model to generalize effectively. This is explained thoroughly in Appendix~\ref{sec:experimental_details_imdb}.

The model architecture used in the experiments consists of an embedding layer, followed by a single selective SSM block. The model is trained using binary cross-entropy loss, consistent with the assumptions outlined in Section~\ref{sec:input_dependent_selective_scan}. To stabilize training, we employ a regularization function from \citet{keller2024regularization}. Figures \ref{fig:sparse_majority} and \ref{fig:imdb} illustrate how the generalization gap varies with sequence length. Since no clear trend emerges in both cases, these results support our length-independent bound on the generalization gap. 

\section{Conclusion and Future Work}

In this paper, we established generalization gap results for selective SSMs, leveraging their connection to Transformers as the foundation for our analysis. Additionally, we incorporated insights from prior work on RNNs, which emphasize the critical role of the state matrix's stability in generalization. This connection enabled us to eliminate the dependency on sequence length, which provides a theoretical explanation for the superior performance of Mamba and Mamba-2 in processing long sequences. Furthermore, we conducted experiments on two tasks with synthetic and real-world datasets to empirically support our theoretical findings. This paper not only bridges the gap between these models but also paves the way for analyzing state-of-the-art architectures that integrate features from Transformers, RNNs, and SSMs. 

An important direction for future work is the improvement of the generalization bound. This exploration could involve refining the bounds under alternative assumptions on the norms of parameters or employing different techniques to construct more optimal covers. Lastly, an important open question remains whether bounding the Rademacher complexity directly, without relying on covering numbers, could lead to sharper bounds.


% The generalization capabilities of deep architectures have garnered significant attention due to their success in challenging tasks. Foundational models in deep learning are increasingly complex and combine elements from multiple paradigms.

\section*{Impact Statement}
This paper presents a theoretical analysis to advance the field of Machine Learning. We do not foresee any societal consequences arising from this work that require discussion.

\section*{Acknowledgements}
This work was partially supported by NSF grants CNS-2038493 and CMMI 2208182, AFOSR grant FA9550-19-1-0005, ONR grant N00014-21-1-2431, and DHS grant 22STESE0001-03-02.

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{references}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Experimental Details}
\label{sec:experimental_details}

In both experiments, we employ an embedding layer implemented using \texttt{torch.nn.Embedding}, which maps input tokens into a continuous vector space. This is followed by a selective SSM block, configured with $N = 4$ states per channel and $d = 16$ channels. The selective SSM block is parameterized as  
\begin{equation*}
\Theta_{\text{SSM}} = \{ \bm{A}_c, \bm{W_B}, \bm{W_C}, p, q, w \}
\end{equation*}  
where each component is defined in Section~\ref{sec:selective_ssms}. The matrix $\bm{A}_c\in\mathbb{R}^{Nd\times Nd}$ represents the state transition matrices across channels. In the code implementation, we store $\bm{A}_c$ structured as a $\mathbb{R}^{d \times N}$ matrix where each row of $\bm{A}_c$ corresponds to the diagonal elements of a distinct diagonal state transition matrix $\bm{A}_{c(j)} \in \mathbb{R}^{N \times N}$ for channel $j$, where $j \in \{1, \dots, d\}$. This parameterization follows the official implementation of Mamba~\cite{gu2024mamba}, ensuring computational efficiency while maintaining expressive capacity. The remaining parameters in $\Theta_{\text{SSM}}$ have the exact dimensions described in Section~\ref{sec:selective_ssms}: $\bm{W_B}, \bm{W_C} \in \mathbb{R}^{N \times d}$, $q \in \mathbb{R}^{d}$, and $p \in \mathbb{R}$.  

\subsection{Sparse Majority Dataset}
\label{sec:experimental_details_sparse}

For the first experiment, we create a synthetic dataset inspired by \citet{trauger2024length_independent_transformer}, but with modifications. Each sample consists of a sequence of ones and zeros, forming the basis of a binary classification task. The class label indicates whether a sequence contains more ones than zeros. A sample sequence $u_1$ with $T=20$ and its label $z_1$ would be as the following:
\begin{equation*}
    \begin{aligned}
        u_1 &= [1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1] \\
        z_1 &= 1
    \end{aligned}
\end{equation*}

Since the task involves only two unique elements, the vocabulary size is set to 2, and each element in the sequence is projected into embeddings of dimension $d$ when they pass the embedding layer. Both the training and test sets contain $m = 1000$ samples. To ensure a uniform distribution of ones and zeros across sequence lengths, we generate sequences such that the number of ones varies approximately evenly from 0 to $T$.

To introduce some imbalance, we modify the training set by randomly flipping $10\%$ of ones to zeros after generating the sequences and labels. As shown in Figure~\ref{fig:sparse_majority_ones}, this results in a noticeable reduction in sequences with a high number of ones. Specifically, towards the maximum sequence length $T$, fewer samples retain exactly $T$ ones due to these perturbations, altering the original distribution.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/sparse_majority_ones.pdf}
    \caption{\textbf{Sparse Majority}. Histogram of ones, $m=1000$ samples each for train and test, sequence length $T=200$.}
    \label{fig:sparse_majority_ones}
\end{figure}

\newpage

\subsection{IMDb Large Movie Review Dataset}
\label{sec:experimental_details_imdb}
For the second experiment, we use the IMDb large movie review dataset \cite{maas2011imdb}, a standard benchmark for sentiment analysis models and part of the Long-Range Arena (LRA) benchmark \cite{tay2021long_range_arena}. The dataset contains 50,000 movie reviews, evenly split between positive and negative labels, and is divided into training and test sets of 25,000 reviews each. The task is binary sentiment classification, aiming to predict whether a review expresses a positive or negative sentiment. The dataset's balanced nature ensures unbiased model evaluation.

We chose IMDb for its diverse sequence lengths, as shown in Table~\ref{tab:imdb_table} and Figure~\ref{fig:imdb_seq_lengths}. To train effectively, we used the entire dataset, truncating or padding sequences to a fixed length. For our experiments, we chose sequence lengths between 100 and 2000 tokens, based on the distribution observed in Figure~\ref{fig:imdb_seq_lengths}. As shown in Figure~\ref{fig:imdb}, test accuracy increases from 100 to 300 tokens, then stabilizes. The generalization gap, visible in the bottom plot, reflects this trend. The average sequence length is 314 tokens, with many sequences exceeding 300 tokens (Figure~\ref{fig:imdb_seq_lengths}). Truncating sequences longer than 300 tokens results in the loss of valuable information, potentially reducing predictive accuracy, as demonstrated by the following examples.
\vspace{-15pt}
\begin{table}[h!]
\centering
\begin{tabular}{ll}\\
  \multicolumn{2}{l}{\textbf{Short Sample}}\\
  \textbf{Text:}   & ``I don't know why I \textcolor[rgb]{0.0, 0.5, 0.0}{like this movie} so well, but I never get tired of watching it." \\
  \textbf{Label:}  & Positive (1) \\
  \textbf{Length:} & 24 \\
  [1em]
  \multicolumn{2}{l}{\textbf{Long Sample}}\\
  \textbf{Text:}   & ``This movie was recently released on DVD in the US and I finally got the chance to see this hard-to-find gem..." \\
  \textbf{Label:}  & Negative (0) \\
  \textbf{Length:} & 1833
\end{tabular}
\end{table}
For shorter sequences, key indicators of the sentiment label often appear early in the text, making it easier for the model to make predictions. However, for longer sequences, these indicators may not be immediately apparent, as the sentiment may be spread across the entire review. In such cases, retaining the full context of the sequence becomes crucial for accurate prediction. This is particularly evident in the test loss observed in Figure~\ref{fig:imdb}, where truncating longer sequences results in a loss of critical context, reducing the model's accuracy.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/imdb_seq_lengths.pdf}
    \caption{\textbf{IMDb}. Histogram of sequence lengths for both the training and test splits.}
    \label{fig:imdb_seq_lengths}
\end{figure}

\begin{table}[h!]
\centering
\begin{tabular}{lcccc}
\toprule
       & Max   & Min & Average & Median \\ 
\midrule
Train  & 3127 & 13 & 314 & 233    \\ 
Test   & 3157 & 10 & 307 & 230    \\ 
\bottomrule
\end{tabular}
\caption{\textbf{IMDb}. Sequence length details for training and test splits.}
\label{tab:imdb_table}
\end{table}

\section{Useful Lemmas}
\label{sec:useful_lemmas}

\begin{lemma}
\label{lem:symm_negative_def}
Let the matrix $\bm{A}_c$ be such that its symmetric part is strictly stable with stability margin $s_A$: $\lambda_i(\bm{A}_c^{\text{sym}}) \leq - s_A$. If $\bm{A}_c$ is normal, then the matrix $\bm{A} = e^{\bm{A}_c}$ is contractive: $\| \bm{A} \|_2 \leq e^{-s_A}$.
    \begin{proof}
        The singular values of $e^{\bm{A}_c}$ are given by:
        \begin{equation}   
            \sigma_j(e^{\bm{A}_c})=\lambda_j((e^{\bm{A}_c})^\top e^{\bm{A}_c}) = \lambda_j(e^{\bm{A}_c^\top} e^{\bm{A}_c}) = \lambda_j(e^{\bm{A}_c^\top + \bm{A}_c})
        \end{equation}
        which we used the normal property of $\bm{A}_c$ in the last equality. By definition $\bm{A}_c^{\text{sym}} = \frac{1}{2}(\bm{A}_c + \bm{A}_c^\top)$. Thus,
        \begin{equation}
            \sigma_j(\bm{A}) = \lambda_j (e^{2 \bm{A}_c^{\text{sym}}}).
        \end{equation}
        The spectral norm of $\bm{A}$ satisfies:
        \begin{equation}
            \|\bm{A}\|_2 = \sqrt{\lambda_{\max}\left(e^{2 \bm{A}_c^{\text{sym}}}\right)} = \sqrt{e^{\lambda_{\max}(2\bm{A}_c^{\text{sym}})}} = e^{\lambda_{\max}(\bm{A}_c^{\text{sym}})} \leq e^{-s_A}.
        \end{equation}
    \end{proof}
\end{lemma}

\begin{lemma}
\label{lem:B_kronecker_bound}
    Let $\bm{B} = \bm{I}_d \otimes \bm{W_B} u[t]$. Then, $\| \bm{B} \|_2 \leq \frak{B}_{\bm{B}} \frak{B}_u$.
\end{lemma}
\begin{proof}
    \begin{equation}
    \begin{aligned}
        \| \bm{B} \|_2^2 &= \| \bm{I}_d \otimes \bm{W_B} u[t] \|_2^2 = \| \bm{W_B} u[t] \|_2^2 \leq \| \bm{W_B} \|_2^2 \frak{B}_u^2 \leq \frak{B}_{\bm{B}}^2 \frak{B}_u^2.
    \end{aligned}
    \end{equation}
\end{proof}

\begin{lemma}
\label{lem:C_kronecker_bound}
    Let $\bm{C} = \bm{I}_d \otimes u[t]^\top \bm{W_C}^\top$. Then, $\| \bm{C} \|_2 \leq \frak{B}_{\bm{C}} \frak{B}_u$.
\end{lemma}
\begin{proof}
    \begin{equation}
    \begin{aligned}
        \| \bm{C} \|_2^2 &= \| \bm{I}_d \otimes u[t]^\top \bm{W_C}^\top \|_2^2 = \| u[t]^\top \bm{W_C}^\top \|_2^2 = \|\bm{W_C} u[t]\|_2^2
        \leq \| \bm{W_C} \|_2^2 \frak{B}_u^2 \leq \frak{B}_{\bm{C}}^2 \frak{B}_u^2.
    \end{aligned}
    \end{equation}
\end{proof}

\begin{lemma}
\label{lem:e^X-e^Y_bound}
Let $\bm{X}$ and $\bm{Y}$ be matrices such that $\| e^{\bm{X}} \|_2 \leq \rho$ and $\| e^{\bm{Y}} \|_2 \leq \rho$. Then, 
\begin{equation}
    \| e^{\bm{X}} - e^{\bm{Y}} \|_2 \leq \rho \| \bm{Y} - \bm{X} \|_2.
\end{equation}
\end{lemma}

\begin{proof}
Using the fundamental theorem of calculus, we express the difference as
\begin{equation}
    e^{\bm{X}} - e^{\bm{Y}} = \int_0^1 e^{t\bm{Y} + (1 - t)\bm{X}} (\bm{Y} - \bm{X}) \, dt.
\end{equation}
Taking the spectral norm on both sides and applying submultiplicativity, we obtain
\begin{equation}
\begin{aligned}
    \| e^{\bm{X}} - e^{\bm{Y}} \|_2 
    &\leq \left\| \int_0^1 e^{t\bm{Y} + (1 - t)\bm{X}} (\bm{Y} - \bm{X}) \, dt \right\|_2 \\
    &\leq \int_0^1 \| e^{t\bm{Y} + (1 - t)\bm{X}} (\bm{Y} - \bm{X}) \|_2 \, dt \\
    &\leq \| \bm{Y} - \bm{X} \|_2 \int_0^1 \| e^{t\bm{Y} + (1 - t)\bm{X}} \|_2 \, dt \\
    &\leq \rho \| \bm{Y} - \bm{X} \|_2.
\end{aligned}
\end{equation}
\end{proof}

\begin{lemma}
\label{lem:delta_cover}
Let $\hat{p}$ be an $\epsilon_p$-cover for $p$ and $\hat{q}$ be an $\epsilon_q$-cover for $q$. Then,
\begin{equation}
    | \Delta[t] - \hat{\Delta}[t] | \leq \epsilon_p + \epsilon_q.
\end{equation}
\end{lemma}

\begin{proof}
The derivative of the soft plus function, $\log(1 + e^x)$, is $\frac{e^x}{1 + e^x}$, which is bounded by $1$. Thus, $\log(1 + e^x)$ is $1$-Lipschitz. using this property, we obtain
\begin{equation}
\begin{aligned}
    | \Delta[t] - \hat{\Delta}[t] | &= \big| \log(1 + e^{p + q^\top u[t]}) - \log(1 + e^{\hat{p} + \hat{q}^\top u[t]}) \big| \\
    &\leq \big| p - \hat{p} + (q - \hat{q})^\top u[t] \big| \\
    &\leq | p - \hat{p} | + \| (q - \hat{q})^\top u[t] \|_2.
\end{aligned}
\end{equation}
\end{proof}

\begin{lemma}
\label{lem:small_w_cover}
Let $\hat{w}$ be an $\epsilon_w$-cover for $w$. Then,
\begin{equation}
    \big| w^\top y[T] - \hat{w}^\top \hat{y}[T] \big| \leq \frak{B}_w \| y[T] - \hat{y}[T] \|_2 + \epsilon_w.
\end{equation}
\end{lemma}

\begin{proof}
Rewriting the LHS, we obtain
\begin{equation}
    \big| w^\top y[T] - \hat{w}^\top \hat{y}[T] \big| 
    = \big| w^\top ( y[T] - \hat{y}[T] ) + (w - \hat{w})^\top \hat{y}[T] \big|.
\end{equation}
Applying triangle inequality results in
\begin{equation}
\begin{aligned}
    \big| w^\top y[T] - \hat{w}^\top \hat{y}[T] \big|
    &\leq \| w \|_2 \| y[T] - \hat{y}[T] \|_2 + \big| (w - \hat{w})^\top \hat{y}[T] \big| \\
    &\leq \frak{B}_w \| y[T] - \hat{y}[T] \|_2 + \epsilon_w.
\end{aligned}
\end{equation}
\end{proof}


\begin{lemma}
\label{lem:C_cover}
Let $\bm{\hat{C}}$ be an $\epsilon_C$-cover for $\bm{C}$. Then,
\begin{equation}
    \| \bm{C} x[T] - \bm{\hat{C}} \hat{x}[T]\|_2 \leq \frak{B}_{\bm{C}} \frak{B}_u \| x[T] - \hat{x}[T] \|_2 + \epsilon_C.
\end{equation}
\end{lemma}
\begin{proof}
The LHS can be bounded as follows:
    \begin{equation}
    \begin{aligned}
         & \leq \| \bm{C} \left( x[T] - \hat{x}[T] \right) + (\bm{C} - \bm{\hat{C}}) \hat{x}[T] \|_2 \\
         & \leq \| \bm{C} \|_2 \| x[T] - \hat{x}[T] \|_2 + \| (\bm{C} - \bm{\hat{C}}) \hat{x}[T] \|_2.
    \end{aligned}
    \end{equation}
    Applying Lemma \eqref{lem:C_kronecker_bound} completes the proof.
\end{proof}

\begin{lemma}
\label{lem:W_b_cover}
Let $\bm{\hat{W}}_B$ be a cover for $\bm{W}_B$. Then,
\begin{equation}
    \| (\bm{B} - \bm{\hat{B}}) u \|_2 \leq \frak{B}_u \| (\bm{W}_B - \bm{\hat{W}}_B) u \|_2.
\end{equation}
\end{lemma}

\begin{proof}
We use the Kronecker product property $(\bm{X} \otimes \bm{Y}) \operatorname{vec}(\bm{V}) = \operatorname{vec} \left( \bm{Y V X}^T \right)$. Take $\bm{X}$ as $\bm{I}_d$, $\bm{V}$ as $u^\top$, and $\bm{Y}$ as $(\bm{W}_B - \bm{\hat{W}}_B) u$ to obtain
\begin{equation}
\begin{aligned}
    \| (\bm{B} - \bm{\hat{B}}) u \|_2 
    &= \| \left( \bm{I}_d \otimes (\bm{W}_B - \bm{\hat{W}}_B) u \right) u \|_2 \\
    &= \| \operatorname{vec} \left( (\bm{W}_B - \bm{\hat{W}}_B ) u u^\top \right) \|_2.
\end{aligned}
\end{equation}
From the definition of the Frobenius norm, we obtain
\begin{equation}
\begin{aligned}
    \| (\bm{W}_B - \bm{\hat{W}}_B ) u u^\top \|_F 
    &\leq \| (\bm{W}_B - \bm{\hat{W}}_B ) u \|_F \| u^\top \|_F \\
    &= \| (\bm{W}_B - \bm{\hat{W}}_B ) u \|_2 \| u \|_2 \\
    &\leq \frak{B}_u \| (\bm{W}_B - \bm{\hat{W}}_B ) u \|_2.
\end{aligned}
\end{equation}
\end{proof}

\begin{lemma}
\label{lem:W_c_cover}
Let $\bm{\hat{W}}_C$ be a cover for $\bm{W}_C$. Then,
\begin{equation}
    \| (\bm{C} - \bm{\hat{C}}) u \|_2 \leq \frak{B}_u \| (\bm{W}_C - \bm{\hat{W}}_C) u \|_2.
\end{equation}
\end{lemma}
\begin{proof}
    Similar to Lemma \ref{lem:W_b_cover}.
\end{proof}

\begin{lemma}[\citet{edelman2022inductive}, Lemma A.8]
\label{lem:eq_constr_optim}
For $\alpha_i, \beta_i \geq 0$, the solution to the following optimization
\begin{equation}
\begin{gathered}
    \min _{\epsilon_1, \ldots, \epsilon_n} \sum_{i=1}^n \frac{\alpha_i}{\epsilon_i^2} \\
    \text { subject to } \sum_{i=1}^n \beta_i \epsilon_i=\epsilon
\end{gathered}
\end{equation}
is $\frac{\gamma^3}{\epsilon^2}$ and is achieved at $\epsilon_i=\frac{\epsilon}{\gamma}\left(\frac{\alpha_i}{\beta_i}\right)^{1 / 3}$, where $\gamma=\sum_{i=1}^n \alpha_i^{1 / 3} \beta_i^{\frac{2}{3}}$.
\end{lemma}

\section{Covering Numbers}
\label{sec:covering_numbers}
In this section, we present covering number bounds for the space of matrices equipped with the matrix norms in Lemmas \ref{lem:matrix_cover}, \ref{lem:cover_A_selective}, and \ref{lem:cover_A_fixed} as well as function classes in Lemma \ref{lem:linear_func_cover}.
\begin{lemma}[\citet{bartlett2017spectrally}, Lemma 3.2]
\label{lem:matrix_cover}
    Let conjugate exponents $(p, q)$ and $(r, s)$ be given with $p \leq 2$, as well as positive reals $(a, b, \epsilon)$ and positive integer $d_3$. Let matrix $X \in \mathbb{R}^{d_1 \times d_2}$ be given with $\|X\|_{p,p} \leq b$. Then,
    \begin{equation}
    \log \mathcal{N} \left(\left\{X A: A \in \mathbb{R}^{d_2 \times d_3},\|A\|_{q, s} \leq a \right\}, \epsilon,\|\;\|_F \right) \leq \left\lceil\frac{a^2 b^2 d_3^{2 / r}}{\epsilon^2}\right\rceil \log (2 d_2 d_3).
    \end{equation}
\end{lemma}

\begin{lemma}
\label{lem:cover_A_selective}
    Let $\mathcal{F}_{\bm{A}_c} = \{ \bm{A}_c \in \mathbb{R}^{Nd \times Nd} : \|\bm{A}_c\|_2 \leq \frak{B}_{\bm{A}} \text{ and } \| \bm{A}_c \|_{2,1} \leq \frak{M}_A \}$. Then,
    \begin{equation}
        \log \mathcal{N} (\mathcal{F}_{A_c}, \epsilon_A, \| \; \|_2) \leq \frac{2\frak{M}_{A}^2 Nd}{\epsilon_A^2} \log (\sqrt{2}Nd).
    \end{equation}
\end{lemma}
\begin{proof}
    Note that every $\epsilon_A$-covering number for the Frobenius norm is also 
    an $\epsilon_A$-covering number for the spectral norm, as 
    $\| \bm{A} - \bm{\hat{A}} \|_2 \leq \| \bm{A} - \bm{\hat{A}} \|_F \leq \epsilon_A$. Therefore,
    \begin{equation}
    \begin{aligned}
        \log \mathcal{N} (\mathcal{F}_{A_c}, \epsilon_A, \| \; \|_2) &\leq \log \mathcal{N} (\mathcal{F}_{A_c}, \epsilon_A, \| \; \|_F) \\
        &\leq \log \mathcal{N} (\{ \bm{A}_c \in \mathbb{R}^{Nd \times Nd} : \| \bm{A}_c \|_{2,1} \leq \frak{M}_A \}, \epsilon_A, \| \; \|_F).
    \end{aligned}
    \end{equation}
    Thus, we instantiate Lemma \ref{lem:matrix_cover} with $p=q=2$ and $s=1, r=\infty$. Take $X$ to be identity and thus $b = \sqrt{Nd}$ which results in
    \begin{equation}
        \log \mathcal{N} (\{ \bm{A}_c \in \mathbb{R}^{Nd \times Nd} : \| \bm{A}_c \|_{2,1} \leq \frak{M}_A \}, \epsilon_A, \| \; \|_F) \leq  \left\lceil\frac{\frak{M}_{A}^2 (\sqrt{Nd})^2}{\epsilon_A^2}\right\rceil \log (2 Nd Nd).
    \end{equation}
\end{proof}

\begin{lemma}
\label{lem:cover_A_fixed}
Let $\mathcal{F}_A = \{ \bm{A} \in \mathbb{R}^{Nd \times Nd} : \|\bm{A}\|_2 \leq \rho_{\bm{A}} < 1  \text{ and } \| \bm{A} \|_{2,1} \leq \frak{M}_A \}$. Then,
    \begin{equation}
        \log \mathcal{N} (\mathcal{F}_A, \epsilon_A, \| \; \|_2) \leq \frac{2 \frak{M}_A^2 
        N d}{\epsilon_A^2} \log(\sqrt{2}Nd).
    \end{equation}
\end{lemma}
\begin{proof}
    Similar to Lemma~\ref{lem:cover_A_selective}.
\end{proof}

\begin{lemma}[\citet{trauger2024length_independent_transformer}, Lemma 3.6]
\label{lem:linear_func_cover}
    Let $m \geq d_2$, $\mathcal{F}_W = \{ \bm{W}u : \bm{W} \in \mathbb{R}^{d_1 \times d_2}, \| \bm{W} \|_{1,1} \leq \frak{M}_W \}$. If $\| u \|_2 \leq \frak{B}_u$, then
    \begin{equation}
        \log \mathcal{N}_\infty (\mathcal{F}_W, \epsilon_W, \| \; \|_2) \leq \frac{\frak{B}_u^2 \frak{M}_W^2}{\epsilon_W^2} \log(2 d_1 d_2 + 1).
    \end{equation}
\end{lemma}
\textbf{Remark.}  
The removal of the dependency on $m$ in the log covering number for a function class is nontrivial and requires specific assumptions about the norm bounds. For similar log covering bounds that are independent of $m$, refer to \cite{trauger2024length_independent_transformer} and the lemmas therein.

\begin{lemma}
\label{lem:generalization_error_bound_last}
    Let $\mathcal{F}$ be a function class such that $\log \mathcal{N}_\infty (\mathcal{F}, \epsilon, \| \; \|_2) \leq \frac{\mathcal{C}_\mathcal{F}^2}{\epsilon^2}$ and let $S$ be the training set $\{u^{(i)}, z^{(i)}\}_{i=1}^m$. Assume the loss function $l:\mathcal{Z} \times \mathcal{Z} \rightarrow \mathbb{R}$ is upper bounded by the constant $\frak{c}_l$ and Lipschitz continuous with constant $\frak{l}_l$. Then, with probability at least $1 - \delta$,
    \begin{equation}
    \begin{aligned}
        \left| \mathbb{E}_{u,z}(l(h(u), z)) - \frac{1}{m} \sum_{i=1}^m l\left(h(u^{(i)}), z^{(i)}\right) \right| \leq \frac{12 \frak{l}_l \mathcal{C}_\mathcal{F}}{\sqrt{m}} \left( 1 + \log\left(\frac{\frak{c}_l\sqrt{m}}{3 \mathcal{C}_\mathcal{F}}\right) \right) + 3\frak{c}_l \sqrt{\frac{\log\left(\frac{2}{\delta}\right)}{2m}}.
    \end{aligned}
    \end{equation}
\end{lemma}
\begin{proof}
    By Theorem~\ref{thm:dudley}, and the fact that $\log \mathcal{N}_2 (l \circ \mathcal{F}, \epsilon, \| \; \|_2) \leq \log \mathcal{N}_\infty (l \circ \mathcal{F}, \epsilon, \| \; \|_2)$ (check Definition 1 in \cite{zhang2002covering}), we have
    \begin{equation}
        \operatorname{Rad}(l \circ \mathcal{F}, S) \leq \inf_{\alpha > 0} \left( 4 \alpha + 12 \int_\alpha^{\frak{c}_l} \sqrt{\frac{\log \mathcal{N}_\infty (l \circ \mathcal{F}, \epsilon, \| \; \|_2)}{m}} \; d\epsilon \right).
    \end{equation}
    Upper bound $\log \mathcal{N}_\infty (\mathcal{F}, \epsilon, m, \| \cdot \|_2)$ as specified by the lemma to obtain
    \begin{equation}
    \label{eq:inf_sth_over_sqrt_m}
    \begin{aligned}
        &\leq \inf_{\alpha > 0} \left( 4\alpha + \frac{12}{\sqrt{m}} \int_\alpha^{\frak{c}_l} \frac{\frak{l}_l \mathcal{C}_\mathcal{F}}{\epsilon} \; d\epsilon \right) =  \inf_{\alpha > 0} \left( 4 \alpha + \frac{12 \frak{l}_l \mathcal{C}_\mathcal{F}}{\sqrt{m}} \log\left(\frac{\frak{c}_l}{\alpha}\right) \right)
    \end{aligned}
    \end{equation}
    in which we used $\log \mathcal{N}_\infty (l \circ \mathcal{F}, \epsilon, \| \; \|_2) \leq \frak{l}_l \log \mathcal{N}_\infty (\mathcal{F}, \epsilon, \| \; \|_2)$. The minimum of \eqref{eq:inf_sth_over_sqrt_m} occurs at $\alpha = \frac{3 \frak{l}_l \mathcal{C}_\mathcal{F}}{\sqrt{m}}$. Thus,
    \begin{equation}
        \leq \frac{12 \frak{l}_l \mathcal{C}_\mathcal{F}}{\sqrt{m}} + \frac{12 \frak{l}_l \mathcal{C}_\mathcal{F}}{\sqrt{m}} \log\left(\frac{\frak{c}_l\sqrt{m}}{3 \mathcal{C}_\mathcal{F}}\right) = \frac{12 \frak{l}_l \mathcal{C}_\mathcal{F}}{\sqrt{m}} \left( 1 + \log\left(\frac{\frak{c}_l\sqrt{m}}{3 \mathcal{C}_\mathcal{F}}\right) \right).
    \end{equation}
    Combining this bound on the Rademacher complexity with Theorem~\ref{thm:gen_error_rademacher} concludes the proof.
\end{proof}


\section{Proof for Theorem~\ref{thm:gen_err_bound_fixed}: Fixed $\Delta$}
\label{sec:fixed_gen_err_proof}
\begin{lemma}
\label{lem:A^t-Ahat^t_bound_fixed}
    Assume that $\|\bm{A}\|_2 \leq \rho_{\bm{A}}$ and $\|\bm{\hat{A}}\|_2 \leq \rho_{\bm{A}}$, where $\rho_{\bm{A}} < 1$. Then, for $t \geq 0$, we have
    \begin{equation}
        \|\bm{A}^t - \bm{\hat{A}}^t\|_2 \leq t \rho_{\bm{A}}^{t-1} \|\bm{A} - \bm{\hat{A}}\|_2.
    \end{equation}
\end{lemma}
\begin{proof}
    Using the identity
    \begin{equation}
        \bm{A}^t - \bm{\hat{A}}^t = \sum_{k=0}^{t-1} \bm{A}^k (\bm{A} - \bm{\hat{A}}) \bm{\hat{A}}^{t-1-k}
    \end{equation}
    we take the spectral norm on both sides:
    \begin{equation}
    \begin{aligned}
        \|\bm{A}^t - \bm{\hat{A}}^t\|_2 
        &\leq \sum_{k=0}^{t-1} \|\bm{A}^k\|_2 \|\bm{A} - \bm{\hat{A}}\|_2 \|\bm{\hat{A}}^{t-1-k}\|_2.
    \end{aligned}
    \end{equation}
    Since $\|\bm{A}^k\|_2 \leq \rho_{\bm{A}}^k$ and $\|\bm{\hat{A}}^{t-1-k}\|_2 \leq \rho_{\bm{A}}^{t-1-k}$, it follows that
    \begin{equation}
    \begin{aligned}
        \|\bm{A}^t - \bm{\hat{A}}^t\|_2 
        &\leq \sum_{k=0}^{t-1} \rho_{\bm{A}}^k \|\bm{A} - \bm{\hat{A}}\|_2 \rho_{\bm{A}}^{t-1-k} \\
        &= \sum_{k=0}^{t-1} \rho_{\bm{A}}^{t-1} \|\bm{A} - \bm{\hat{A}}\|_2 \\
        &= t \rho_{\bm{A}}^{t-1} \|\bm{A} - \bm{\hat{A}}\|_2.
    \end{aligned}
    \end{equation}
\end{proof}

\begin{lemma}
\label{lem:sum_A^t_B_bound_fixed}
Let $\bm{\hat{B}}$ be an $\epsilon_B$-cover for $\bm{B}$, and $\bm{\hat{A}}$ be an $\epsilon_A$-cover for $\bm{A}$. Then,
    \begin{equation}
        \left\| \sum_{t=0}^{T-1} \left( ( \bm{A}^t \bm{B} -  \bm{\hat{A}}^t \bm{\hat{B}} ) u[T-1-t] \right) \right\|_2 \leq \frac{\epsilon_B}{1 - \rho_{\bm{A}}} + \frac{\epsilon_A \frak{B}_{\bm{B}} \frak{B}_u^2 \rho_{\bm{A}}}{(1 - \rho_{\bm{A}})^2}.
    \end{equation}
\end{lemma}
\begin{proof}
    By adding and subtracting terms, we obtain the following upper bound:
    \begin{equation}
    \begin{aligned}
        & \leq \left\| \sum_{t=0}^{T-1} \left( (\bm{A}^t \bm{B} - \bm{A}^t \bm{\hat{B}}) u[T-1-t] \right) \right\|_2 + \left\| \sum_{t=0}^{T-1} \left( (\bm{A}^t \bm{\hat{B}} - \bm{\hat{A}}^t \bm{\hat{B}}) u[T-1-t] \right) \right\|_2 \\
        & = \left\| \sum_{t=0}^{T-1} \left( \bm{A}^t (\bm{B} - \bm{\hat{B}}) u[T-1-t] \right) \right\|_2 + \left\| \sum_{t=0}^{T-1} \left( (\bm{A}^t - \bm{\hat{A}}^t ) \bm{\hat{B}} u[T-1-t] \right) \right\|_2 \\
        & \leq \sum_{t=0}^{T-1} \left( \| \bm{A} \|_2^t \|(\bm{B} - \bm{\hat{B}}) u[T - 1 - t] \|_2 \right) + \sum_{t=0}^{T-1} \left( \| \bm{A}^t - \bm{\hat{A}}^t \|_2 \| \bm{\hat{B}} u[T-1-t] \|_2 \right).
    \end{aligned}
    \end{equation}
    The first term can be bounded using the cover for $\bm{B}$ and the assumption that $\|\bm{A}\|_2 \leq \rho_{\bm{A}} < 1$. To bound the second term, we apply Lemma~\ref{lem:A^t-Ahat^t_bound_fixed}:
    \begin{equation}
    \begin{aligned}
        & \leq \epsilon_B \sum_{t=0}^{T-1} \rho_{\bm{A}}^t + \sum_{t=0}^{T-1} \left( t \rho_{\bm{A}}^t \| \bm{A} - \bm{\hat{A}} \|_2 \| \bm{\hat{B}} u[T-1-t] \|_2 \right) \\
        & \leq \epsilon_B \sum_{t=0}^{T-1} \rho_{\bm{A}}^t + \epsilon_A \sum_{t=0}^{T-1} \left( t \rho_{\bm{A}}^t \| \bm{\hat{B}} u[T-1-t] \|_2 \right).
    \end{aligned}
    \end{equation}
    Use Lemma~\ref{lem:B_kronecker_bound} to bound $\|\bm{\hat{B}}\|_2$ in the second term:
    \begin{equation}
    \begin{aligned}
        & \leq \epsilon_B \sum_{t=0}^{T-1} \rho_{\bm{A}}^t + \epsilon_A \frak{B}_{\bm{B}} \frak{B}_u^2 \sum_{t=0}^{T-1} \left( t \rho_{\bm{A}}^t \right) \\
        & = \epsilon_B \frac{1 - \rho_{\bm{A}}^T}{1 - \rho_{\bm{A}}} + \epsilon_A \frak{B}_{\bm{B}} \frak{B}_u^2 \left( \frac{\rho_{\bm{A}} (1 - \rho_{\bm{A}}^T)}{(1 - \rho_{\bm{A}})^2} - \frac{T \rho_{\bm{A}}^T}{1 - \rho_{\bm{A}}} \right).
    \end{aligned}
    \end{equation}
    This expression is upper bounded as stated in the Lemma, and the bound is independent of $T$.
\end{proof}

\begin{lemma}
\label{lem:epsilons_sum_bound}
\begin{equation}
    \left| w^\top y[T] - \hat{w}^\top \hat{y}[T] \right| \leq \frac{ \frak{B}_w \frak{B}_{\bm{C}} \frak{B}_u^2}{1 - \rho_{\bm{A}}} \epsilon_{W_B} + \frac{ \frak{B}_w \frak{B}_{\bm{C}} \frak{B}_{\bm{B}} \frak{B}_u^3 \rho_{\bm{A}}}{(1 - \rho_{\bm{A}})^2} \epsilon_A + \frak{B}_w \frak{B}_u \epsilon_{W_C} + \epsilon_w.
\end{equation}
\end{lemma}
\begin{proof}
The result follows by applying Lemmas~\ref{lem:small_w_cover}, \ref{lem:C_cover}, and \ref{lem:sum_A^t_B_bound_fixed} sequentially:
    \begin{equation}
    \begin{aligned}
        & \left| w^\top \bm{C} \sum_{t=0}^{T-1} \left( \bm{A}^t \bm{B} u[T-1-t] \right) - \hat{w}^\top \bm{\hat{C}} \sum_{t=0}^{T-1} \left( \bm{\hat{A}}^t \bm{\hat{B}} u[T-1-t] \right) \right| \\
        & \leq \frak{B}_w \left( \frak{B}_{\bm{C}} \frak{B}_u \left( \frac{\epsilon_B}{1 - \rho_{\bm{A}}} + \frac{\epsilon_A \frak{B}_{\bm{B}} \frak{B}_u^2 \rho_{\bm{A}}}{(1 - \rho_{\bm{A}})^2} \right) + \epsilon_C \right) + \epsilon_w
    \end{aligned}
    \end{equation}
    and then using Lemmas~\ref{lem:W_b_cover} and \ref{lem:W_c_cover}.
\end{proof}


\begin{proof}[Proof of Theorem \ref{thm:gen_err_bound_fixed}]
    We aim to construct a cover for the space of all selective SSMs with fixed step size $\bar{\mathcal{F}}_{\text{SSM}} = \{ w^\top y[T] : y[T] \text{ is described in \eqref{eq:unrolled_io}} \}$ which is parametrizes by $\bar{\Theta}_{\text{SSM}} = \{\bm{A}, \bm{W_B}, \bm{W_C}, w\}$ as in \eqref{eq:ssm_fixed_theta}. Let's look at how much the output $w^\top y[T]$ changes if we move to the points in the $\epsilon$-net constructing the cover. This is done in Lemma \ref{lem:epsilons_sum_bound}. Thus, we need to choose $\epsilon_A, \epsilon_{W_B}, \epsilon_{w_C}$, and $\epsilon_w$ subject to the following:
    \begin{equation}
        \frac{ \frak{B}_w \frak{B}_{\bm{C}} \frak{B}_u^2}{1 - \rho_{\bm{A}}} \epsilon_{W_B} + \frac{ \frak{B}_w \frak{B}_{\bm{C}} \frak{B}_{\bm{B}} \frak{B}_u^3 \rho_{\bm{A}}}{(1 - \rho_{\bm{A}})^2} \epsilon_A + \frak{B}_w \frak{B}_u \epsilon_{W_C} + \epsilon_w = \epsilon.
    \end{equation}
    Choose the covering for $\bm{W}_B$ according to Lemma \ref{lem:linear_func_cover} such that
    \begin{equation}
    \begin{aligned}
        \log \mathcal{N}_\infty (\mathcal{F}_{W_B}, \epsilon_{W_B}, \| \; \|_2) \leq \frac{\frak{B}_u^2 \frak{M}_B^2}{\epsilon_{W_B}^2} \log(2Nd + 1).
    \end{aligned}
    \end{equation}
    Similarly, choose the covering for $\bm{W}_C$ by replacing $u$ in Lemma~\ref{lem:linear_func_cover} with $x[T]$ which is bounded as
    \begin{equation}
    \begin{aligned}
        \| x[T] \|_2 &\leq \left\| \sum_{t=0}^{T-1} \Big( \bm{A}^t \big( \bm{I}_d \otimes \bm{W_B} u[T-1-t] \big) u[T-1-t] \Big) \right\|_2 \\
        & \leq \sum_{t=0}^{T-1} \Big( \| \bm{A}^t \|_2 \| \bm{I}_d \otimes \bm{W_B} u[T-1-t] \|_2 \| u[T-1-t] \|_2 \Big) \\
        &\leq \frak{B}_{\bm{B}} \frak{B}_u^2 \sum_{t=0}^{T-1} \rho_{\bm{A}}^t = \frac{\frak{B}_{\bm{B}} \frak{B}_u^2(1 - \rho_{\bm{A}}^T)}{1 - \rho_{\bm{A}}} < \frac{\frak{B}_{\bm{B}} \frak{B}_u^2}{1 - \rho_{\bm{A}}}
    \end{aligned}
    \end{equation}
    to derive
    \begin{equation}
    \begin{aligned}
        \log \mathcal{N}_\infty (\mathcal{F}_{W_C}, \epsilon_{W_C}, \| \; \|_2) &\leq \frac{\left( \frac{ \frak{B}_{\bm{B}} \frak{B}_u^2}{1 - \rho_{\bm{A}}} \right)^2 \frak{M}_C^2}{\epsilon_{W_C}^2} \log(2dNd + 1) \\
        &= \frac{\frak{B}_{\bm{B}}^2 \frak{B}_u^4 \frak{M}_C^2}{(1 - \rho_{\bm{A}})^2\epsilon_{W_C}^2} \log(2Nd^2 + 1).
    \end{aligned}
    \end{equation}
    Similar to the cover for $\bm{W}_{\bm{C}}$, we apply Lemma~\ref{lem:linear_func_cover} with replacing $u$ with $y[T]$ which results in the following cover for $w$:
    \begin{equation}
    \begin{aligned}
        \log \mathcal{N}_\infty (\mathcal{F}_w, \epsilon_w, \| \; \|_2) &\leq \frac{\left( \frac{\frak{B}_{\bm{B}} \frak{B}_{\bm{C}} \frak{B}_u^3}{1 - \rho_{\bm{A}}} \right)^2 \frak{M}_w^2}{\epsilon_w^2} \log(2d + 1) \\
        &= \frac{\frak{B}_{\bm{B}}^2 \frak{B}_{\bm{C}}^2 \frak{B}_u^6 \frak{M}_w^2}{(1 - \rho_{\bm{A}})^2 \epsilon_w^2} \log(2d + 1).
    \end{aligned}
    \end{equation}
    Lemma \ref{lem:cover_A_fixed} gives us the upper bound on the covering number for $\bm{A}$:
    \begin{equation}
        \log \mathcal{N} (\mathcal{F}_A, \epsilon_A, \| \; \|_2) \leq \frac{2 \frak{M}_A^2 N d}{\epsilon_A^2} \log(\sqrt{2} N d).
    \end{equation}
    Ignore the logarithmic dependencies and assume $\frak{M}_C = \frak{B}_{\bm{C}}, \frak{M}_B = \frak{B}_{\bm{B}}, \frak{M}_w = \frak{B}_w, \frak{M}_A = \frak{B}_{\bm{A}}$ for simplicity. Construct the cover for $\bar{\mathcal{F}}_{\text{SSM}}$ as the Cartesian product of all covers in $\bar{\Theta}_{\text{SSM}}$. Then, the log covering number would be the sum of the log covering numbers of all parameters. Use Lemma~\ref{lem:eq_constr_optim} to find $\epsilon_A, \epsilon_{W_B}, \epsilon_{w_C}$, and $\epsilon_w$ such that the size of total cover would be minimum:
    \begin{equation}
    \begin{aligned}
        & \epsilon^2 \log \mathcal{N}_\infty (\bar{\mathcal{F}}_{\text{SSM}}, \epsilon, \| \; \|_2) \\
        &\leq \mathcal{\tilde{O}} \left( \left( (\frak{B}_u^2 \frak{B}_{\bm{B}}^2)^{1/3} (\frac{ \frak{B}_w \frak{B}_{\bm{C}} \frak{B}_u^2}{1 - \rho_{\bm{A}}})^{2/3} + ( \frac{\frak{B}_{\bm{B}}^2 \frak{B}_u^4 \frak{B}_{\bm{C}}^2}{(1 - \rho_{\bm{A}})^2} )^{1/3}(\frak{B}_w \frak{B}_u)^{2/3} \right. \right. \\
        & + \left. \left. (\frak{M}_A^2 N d)^{1/3} (\frac{\frak{B}_w \frak{B}_{\bm{C}} \frak{B}_{\bm{B}} \frak{B}_u^3 \rho_{\bm{A}}}{(1 - \rho_{\bm{A}})^2})^{2/3} + (\frac{\frak{B}_{\bm{B}}^2 \frak{B}_{\bm{C}}^2 \frak{B}_u^6 \frak{B}_w^2}{(1 - \rho_{\bm{A}})^2} )^{1/3}  \right)^3 \right)
    \end{aligned}
    \end{equation}
    \begin{equation}
    \begin{aligned}
        &\leq \mathcal{\tilde{O}} \left( \left( \frak{B}_w^{2/3} \frak{B}_u^2 \frak{B}_{\bm{B}}^{2/3} \frak{B}_{\bm{C}}^{2/3} (1 - \rho_{\bm{A}})^{-2/3}  + \frak{B}_w^{2/3} \frak{B}_u^2 \frak{B}_{\bm{B}}^{2/3} \frak{B}_{\bm{C}}^{2/3} (1 - \rho_{\bm{A}})^{-2/3} \right. \right. \\
        & + \left. \left. \frak{B}_w^{2/3} \frak{B}_u^2 \frak{B}_{\bm{B}}^{2/3} \frak{B}_{\bm{C}}^{2/3} \frak{B}_{\bm{A}}^{2/3} (1 - \rho_{\bm{A}})^{-4/3} \rho_{\bm{A}}^{2/3} N^{1/3} d^{1/3} + \frak{B}_w^{2/3} \frak{B}_u^2 \frak{B}_{\bm{B}}^{2/3} \frak{B}_{\bm{C}}^{2/3} (1 - \rho_{\bm{A}})^{-2/3} \right)^3 \right)
    \end{aligned}
    \end{equation}
    \begin{equation}
        \leq \mathcal{\tilde{O}} \left( \frak{B}_w^{2} \frak{B}_u^6 \frak{B}_{\bm{B}}^{2} \frak{B}_{\bm{C}}^{2} (1 - \rho_{\bm{A}})^{-2} \left( 1 + 1 + \frak{B}_{\bm{A}}^{2/3} (1 - \rho_{\bm{A}})^{-2/3} \rho_{\bm{A}}^{2/3} N^{1/3} d^{1/3}
        + 1 \right)^3 \right).
    \end{equation}
    The constant terms are dominated by the other term. Thus,
    \begin{equation}
        \leq \mathcal{\tilde{O}} \left( \frak{B}_w^{2} \frak{B}_u^6 \frak{B}_{\bm{B}}^{2} \frak{B}_{\bm{C}}^{2} \frak{B}_{\bm{A}}^{2} (1 - \rho_{\bm{A}})^{-4} \rho_{\bm{A}}^{2} N d \right).
    \end{equation}
    The square root of this expression is $\mathcal{C}_\mathcal{F}$. The proof is complete by the application of Lemma \ref{lem:generalization_error_bound_last}.
\end{proof}

\section{Proof for Theorem~\ref{thm:gen_err_bound_selective}: Input-Dependent $\Delta$}
\label{sec:input_dependent_gen_err_proof}
\begin{lemma}
\label{lem:A^t_rho^t_bound_selective}
    Assume that $\| u[t] \|_2 \leq \frak{B}_u$, $\| q \|_2 \leq \frak{B}_q$, and $p \geq L_p$. If $\Re(\lambda_{i}(\bm{A}_c)) \leq -s_A$ and $\bm{A}_c$ is normal, then
    \begin{equation}
        \| \bm{A}^t \|_2 \leq \rho_{\bm{A}}^t
    \end{equation}
    where $\rho_{\bm{A}} = \left( 1 + e^{L_p - \frak{B}_q \frak{B}_u} \right)^{-s_A} < 1$.
\end{lemma}

\begin{proof}
    From \eqref{eq:A^t_shorthand}, we have
    \begin{equation}
        \| \bm{A}^t \|_2 = \left\| \prod_{j=T-t}^{T-1} e^{\Delta[j] \bm{A}_c} \right\|_2.
    \end{equation}
    Given the assumptions of the lemma, and noting that the softplus function, 
    $\log(1 + e^x)$, is increasing, we derive the following lower bound:
    \begin{equation}
        \Delta[j] \geq \log(1 + e^{L_p - \frak{B}_q \frak{B}_u}).
    \end{equation}
    From the application of Lemma~\ref{lem:symm_negative_def} follows that $\| e^{\bm{A}_c} \|_2 < 1$. Therefore, we can bound $\| \bm{A}^t \|_2$ as follows:
    \begin{equation}
    \begin{aligned}
        \| \bm{A}^t \|_2 &\leq \| e^{\bm{A}_c} \|_2^{\sum_{j=T-t}^{T-1} \Delta[j] } \\
        &\leq \left\| e^{\bm{A}_c} \right\|_2 ^ {t \log(1 + e^{L_p - \frak{B}_q \frak{B}_u})} \\
        &\leq e^{-s_A t \log(1 + e^{L_p - \frak{B}_q \frak{B}_u})} \\
        &= \left( 1 + e^{L_p - \frak{B}_q \frak{B}_u} \right)^{-s_A t} = \rho_{\bm{A}}^t.
    \end{aligned}
    \end{equation}
\end{proof}

\begin{lemma}
\label{lem:A^t-Ahat^t_bound_selective}
Let $\bm{\hat{A}}_c$ be an $\epsilon_A$-cover for $\bm{A}_c$, $\hat{p}$ be an $\epsilon_p$-cover for $p$, and $\hat{q}$ be an $\epsilon_q$-cover for $q$. Then,
\begin{equation}
    \| \bm{A}^t - \bm{\hat{A}}^t \|_2 \leq t \rho_{\bm{A}}^t (\frak{M}_{\Delta} \epsilon_A + \frak{B}_{\bm{A}} \epsilon_\Delta )
\end{equation}
where $\frak{M}_{\Delta} = \log(1 + e^{\frak{B}_p + \frak{B}_q \frak{B}_u})$.
\end{lemma}

\begin{proof}
We start with
\begin{equation}
    \begin{aligned}
        \| \bm{A}^t - \bm{\hat{A}}^t \|_2 &= \left\| \prod_{k=T-t}^{T-1} e^{\Delta[k] \bm{A}_c}-\prod_{k=T-t}^{T-1} e^{\hat{\Delta}[k] \bm{\hat{A}}_c} \right\|_2 \\
        &= \left\| \sum_{i=T-t}^{T-1} \left( \left(\prod_{j=i}^{T-1} e^{\Delta[j] \bm{A}_c}\right) \left(\prod_{k=T-t}^{i-1} e^{\hat{\Delta}[k] \bm{\hat{A}}_c}\right) -  \left(\prod_{j=i+1}^{T-1} e^{\Delta[j] \bm{A}_c}\right) \left(\prod_{k=T-t}^{i} e^{\hat{\Delta}[k] \bm{\hat{A}}_c}\right) \right) \right\|_2.
    \end{aligned}
\end{equation}
Factor common terms to obtain
\begin{equation}
    \begin{aligned}
        &\leq \left\| \sum_{i=T-t}^{T-1} \left(\prod_{j=i+1}^{T-1} e^{\Delta[j] \bm{A}_c}\right) \left(e^{\Delta[i] \bm{A}_c}-e^{\hat{\Delta}[i] \bm{\hat{A}}_c}\right) \left(\prod_{k=T-t}^{i-1} e^{\hat{\Delta}[k] \bm{\hat{A}}_c}\right) \right\|_2 \\
        &\leq \sum_{i=T-t}^{T-1} \left\| \prod_{j=i+1}^{T-1} e^{\Delta[j] \bm{A}_c} \right\|_2 \left\| e^{\Delta[i] \bm{A}_c} - e^{\hat{\Delta}[i] \bm{\hat{A}}_c} \right\|_2 \left\| \prod_{k=T-t}^{i-1} e^{\hat{\Delta}[k] \bm{\hat{A}}_c} \right\|_2.
    \end{aligned}
\end{equation}
Applying Lemma \ref{lem:A^t_rho^t_bound_selective}, we get 
\begin{equation}
    \begin{aligned}
        &\leq \sum_{i=T-t}^{T-1} \rho_{\bm{A}}^{T-i-1} \| e^{\Delta[i] \bm{A}_c} - e^{\hat{\Delta}[i] \bm{\hat{A}}_c} \|_2 \rho_{\bm{A}}^{i-T+t} \\
        &= \sum_{i=T-t}^{T-1} \rho_{\bm{A}}^{t-1} \| e^{\Delta[i] \bm{A}_c} - e^{\hat{\Delta}[i] \bm{\hat{A}}_c} \|_2.
    \end{aligned}
\end{equation}
Use Lemma \ref{lem:e^X-e^Y_bound} to derive
\begin{equation}
    \| \bm{A}^t - \bm{\hat{A}}^t \|_2 \leq \rho_{\bm{A}}^t \sum_{i=T-t}^{T-1} \| \Delta[i] \bm{A}_c - \hat{\Delta}[i] \bm{\hat{A}}_c \|_2.
\end{equation}
Apply triangle inequality:
\begin{equation}
    \begin{aligned}
        &\leq \rho_{\bm{A}}^t \sum_{i=T-t}^{T-1} \left( \| \Delta[i] (\bm{A}_c - \bm{\hat{A}}_c) \|_2 + \| (\Delta[i] - \hat{\Delta}[i]) \bm{\hat{A}}_c \|_2 \right) \\
        &\leq \rho_{\bm{A}}^t \sum_{i=T-t}^{T-1} \left( \frak{M}_{\Delta} \epsilon_A + | \Delta[i] - \hat{\Delta}[i] | \frak{B}_{\bm{A}} \right) \\
        &\leq \rho_{\bm{A}}^t \sum_{i=T-t}^{T-1} \left( \frak{M}_{\Delta} \epsilon_A + \epsilon_\Delta \frak{B}_{\bm{A}} \right).
    \end{aligned}
\end{equation}
At last, we obtain the final bound:
\begin{equation}
    \| \bm{A}^t - \bm{\hat{A}}^t \|_2 \leq t \rho_{\bm{A}}^t (\frak{M}_{\Delta} \epsilon_A + \frak{B}_{\bm{A}} \epsilon_\Delta).
\end{equation}
\end{proof}

\begin{lemma}
\label{lem:A_cover_selective}
Let $\bm{\hat{A}}, \bm{\hat{B}}, \hat{\Delta}$ be covers for $\bm{A}, \bm{B}, \Delta$. Then,
\begin{equation}
\begin{aligned}
    & \left\| \sum_{t=0}^{T-1} \bm{A}^t \Delta[T-1-t] \bm{B} u[T-1-t] - \sum_{t=0}^{T-1} \bm{\hat{A}}^t \hat{\Delta}[T-1-t] \bm{\hat{B}} u[T-1-t] \right\|_2 \\
    & \leq \frac{\frak{M}_\Delta}{1 - \rho_{\bm{A}}} \epsilon_B + \frac{\frak{M}_\Delta^2 \rho_{\bm{A}} \sqrt{d} \frak{B}_{\bm{B}} \frak{B}_u^2}{(1 - \rho_{\bm{A}})^2} \epsilon_A
        + \left( \frac{\sqrt{d} \frak{B}_{\bm{B}} \frak{B}_u^2}{1 - \rho_{\bm{A}}} \right) \left( 1 + \frac{\frak{M}_\Delta \frak{B}_{\bm{A}} \rho_{\bm{A}}}{1 - \rho_{\bm{A}}} \right) \epsilon_{\Delta}.
\end{aligned}
\end{equation}
\end{lemma}

\begin{proof}
    Write the LHS as follows:
    \begin{equation}
        \left\| \sum_{t=0}^{T-1} \left( \left( \bm{A}^t \Delta[T-1-t] \bm{B} - \bm{\hat{A}}^t \hat{\Delta}[T-1-t] \bm{\hat{B}} \right) u[T-1-t] \right) \right\|_2.
    \end{equation}
    Add and subtract the terms $\sum_{t=0}^{T-1}  \left( \bm{A}^t \Delta[T-1-t] \bm{\hat{B}} \right) u[t-t-1]$ and $\sum_{t=0}^{T-1}  \left( \bm{A}^t \hat{\Delta}[T-1-t] \bm{\hat{B}} \right) u[t-t-1]$ to derive
    \begin{equation}
    \begin{aligned}
        \| &\sum_{t=0}^{T-1} \left( \bm{A}^t \Delta[T-1-t] \bm{B} - \bm{A}^t \Delta[T-1-t] \bm{\hat{B}} \right) u[T-t-1] \\
          +&\sum_{t=0}^{T-1} \left( \bm{A}^t \Delta[T-1-t] \bm{\hat{B}} - \bm{A}^t \hat{\Delta}[T-1-t] \bm{\hat{B}} \right) u[T-t-1] \\
          +&\sum_{t=0}^{T-1} \left( \bm{A}^t \hat{\Delta}[T-1-t] \bm{\hat{B}} - \bm{\hat{A}}^t \hat{\Delta}[T-1-t] \bm{\hat{B}} \right) u[T-t-1] \|_2.
    \end{aligned}
    \end{equation}
    Apply the triangle inequality to get
    \begin{equation}
    \begin{aligned}
        \sum_{t=0}^{T-1} \Big( & \| \bm{A}^t \Delta[T-1-t] ( \bm{B} - \bm{\hat{B}} ) u[T-t-1] \|_2 \Big. \\ 
                         \Big. & + \| \bm{A}^t (\Delta[T-1-t] - \hat{\Delta}[T-1-t]) \bm{\hat{B}} u[T-t-1] \|_2 \Big. \\
                         \Big. & + \| ( \bm{A}^t - \bm{\hat{A}}^t) \hat{\Delta[T-1-t]} \bm{\hat{B}} u[T-t-1] \|_2 \Big)
    \end{aligned}
    \end{equation}
    which is upper bounded by
    \begin{equation}
    \begin{aligned}
        \leq \sum_{t=0}^{T-1} \Big( & \| \bm{A}^t \|_2 | \Delta[T-1-t] | \| ( \bm{B} - \bm{\hat{B}} ) u[T-t-1] \|_2 \Big. \\ 
                         \Big. & + \| \bm{A}^t \|_2 | \Delta[T-1-t] - \hat{\Delta}[T-1-t] | \| \bm{\hat{B}} u[T-t-1] \|_2 \Big. \\
                         \Big. & + \| ( \bm{A}^t - \bm{\hat{A}}^t) \|_2 | \hat{\Delta}[T-1-t] |\| \bm{\hat{B}} u[T-t-1] \|_2 \Big).
    \end{aligned}
    \end{equation}
    The application of Lemmas \ref{lem:A^t_rho^t_bound_selective} and \ref{lem:A^t-Ahat^t_bound_selective} to bound $\|\bm{A}^t\|_2$ and cover $\| \bm{A}^t - \bm{\hat{A}}^t \|_2$ results in
    \begin{equation}
    \begin{aligned}
        \leq \sum_{t=0}^{T-1} \Big( & \rho_{\bm{A}}^t \frak{M}_\Delta \epsilon_B \Big. \\ 
                         \Big. & + \rho_{\bm{A}}^t \epsilon_\Delta \| \bm{\hat{B}} \|_2 \frak{B}_u \Big. \\
                         \Big. & + t \rho_{\bm{A}}^t (\frak{M}_{\Delta} \epsilon_A + \frak{B}_{\bm{A}} \epsilon_\Delta) \frak{M}_\Delta \| \bm{\hat{B}} \|_2 \frak{B}_u \Big).
    \end{aligned}
    \end{equation}
    Apply Lemma \ref{lem:B_kronecker_bound} to bound $\| \bm{\hat{B}} \|_2$:
    \begin{equation}
        \leq \sum_{t=0}^{T-1} \left( \rho_{\bm{A}}^t \frak{M}_\Delta \epsilon_B + \rho_{\bm{A}}^t \frak{B}_{\bm{B}} \frak{B}_u^2 \epsilon_\Delta + t \rho_{\bm{A}}^t \frak{M}_\Delta\frak{B}_{\bm{B}} \frak{B}_u^2 (\frak{M}_{\Delta} \epsilon_A + \frak{B}_{\bm{A}} \epsilon_\Delta) \right).
    \end{equation}
    Break the summation into two parts
    \begin{equation}
    \begin{aligned}
        &\leq \left( \frak{M}_\Delta \epsilon_B + \frak{B}_{\bm{B}} \frak{B}_u^2 \epsilon_\Delta  \right) \sum_{t=0}^{T-1} \rho_{\bm{A}}^t + \frak{M}_\Delta \frak{B}_{\bm{B}} \frak{B}_u^2 (\frak{M}_{\Delta} \epsilon_A + \frak{B}_{\bm{A}} \epsilon_\Delta) \sum_{t=0}^{T-1} t \rho_{\bm{A}}^t \\
        &\leq \left( \frak{M}_\Delta \epsilon_B + \frak{B}_{\bm{B}} \frak{B}_u^2 \epsilon_\Delta \right) \frac{1 - \rho_{\bm{A}}^T}{1 - \rho_{\bm{A}}} + \frak{M}_\Delta \frak{B}_{\bm{B}} \frak{B}_u^2 (\frak{M}_{\Delta} \epsilon_A + \frak{B}_{\bm{A}} \epsilon_\Delta) \left( \frac{\rho_{\bm{A}} (1 - \rho_{\bm{A}}^T)}{(1 - \rho_{\bm{A}})^2} - \frac{T \rho_{\bm{A}}^T}{1 - \rho_{\bm{A}}} \right).
    \end{aligned}
    \end{equation}
    Thus, we obtain the following length-independent upper bound:
    \begin{equation}
    \begin{aligned}
        &\leq \frac{\frak{M}_\Delta}{1 - \rho_{\bm{A}}} \epsilon_B 
        + \frac{\frak{B}_{\bm{B}} \frak{B}_u^2}{1 - \rho_{\bm{A}}} \epsilon_\Delta
        + \frac{\frak{M}_\Delta \rho_{\bm{A}} \frak{B}_{\bm{B}} \frak{B}_u^2}{(1 - \rho_{\bm{A}})^2} (\frak{M}_{\Delta} \epsilon_A + \frak{B}_{\bm{A}} \epsilon_\Delta).
    \end{aligned}
    \end{equation}
\end{proof}

\begin{lemma}
\label{lem:epsilon_sum_bound_selective}
Let $\bm{\hat{A}}_c, \bm{\hat{W}_B}, \bm{\hat{W}_C}, \hat{p}, \hat{q}$ be covers for $\bm{A}_c, \bm{W_B}, \bm{W_C}, p, q$. Then,
\begin{equation}
\begin{aligned}
    \left| w^\top y[T] - \hat{w}^\top \hat{y}[T] \right| 
    &\leq \frac{\frak{B}_w \frak{B}_{\bm{C}} \frak{B}_u^2 \frak{M}_\Delta}{1 - \rho_{\bm{A}}} \epsilon_{W_B} 
    + \frac{\frak{M}_\Delta^2 \rho_{\bm{A}} \frak{B}_w \frak{B}_{\bm{B}} \frak{B}_{\bm{C}} \frak{B}_u^3}{(1 - \rho_{\bm{A}})^2} \epsilon_A \\
    &\quad + \left( \frac{\frak{B}_w \frak{B}_{\bm{B}} \frak{B}_{\bm{C}} \frak{B}_u^3}{1 - \rho_{\bm{A}}} \right) 
      \left( 1 + \frac{\frak{M}_\Delta \frak{B}_{\bm{A}} \rho_{\bm{A}}}{1 - \rho_{\bm{A}}} \right) (\epsilon_p + \epsilon_q) \\
    &\quad + \frak{B}_w \frak{B}_u \epsilon_{W_C} + \epsilon_w.
\end{aligned}
\end{equation}
\end{lemma}

\begin{proof}
The proof follows from the sequential application of Lemmas \ref{lem:small_w_cover}, \ref{lem:C_cover}, and \ref{lem:A_cover_selective}, yielding:
\begin{equation}
\begin{aligned}
    &\left| w^\top y[T] - \hat{w}^\top \hat{y}[T] \right| \\
    &\leq \frak{B}_w \left( \frak{B}_{\bm{C}} \frak{B}_u 
    \left( \frac{\frak{M}_\Delta}{1 - \rho_{\bm{A}}} \epsilon_B 
    + \frac{\frak{M}_\Delta^2 \rho_{\bm{A}} \frak{B}_{\bm{B}} \frak{B}_u^2}{(1 - \rho_{\bm{A}})^2} \epsilon_A \right. \right. \\
    &\quad \left. \left. + \left( \frac{ \frak{B}_{\bm{B}} \frak{B}_u^2}{1 - \rho_{\bm{A}}} \right) 
      \left( 1 + \frac{\frak{M}_\Delta \frak{B}_{\bm{A}} \rho_{\bm{A}}}{1 - \rho_{\bm{A}}} \right) \epsilon_\Delta \right) 
    + \epsilon_C \right) + \epsilon_w.
\end{aligned}
\end{equation}
Finally, we apply Lemmas \ref{lem:W_b_cover}, \ref{lem:W_c_cover}, and \ref{lem:delta_cover} to relate the covers for $\bm{B}, \bm{C}, \bm{\Delta}$ to the covers for $\bm{W_B}, \bm{W_C}, p, q$, completing the proof.
\end{proof}

\begin{proof}[Proof of Theorem \ref{thm:gen_err_bound_selective}]
    The structure of the proof closely follows that of Theorem~\ref{thm:gen_err_bound_fixed}, with modifications to account for $\bm{A}_c$ instead of $\bm{A}$, as well as the inclusion of the parameters $p$ and $q$. Our objective is to construct a covering for the space of all selective SSMs, denoted as $\mathcal{F}_{\text{SSM}} = \{ w^\top y[T] : y[T] \text{ is described in \eqref{eq:unrolled_io_selective}} \}$, which is parameterized by $\Theta_{\text{SSM}} = \{\bm{A}_c, \bm{W_B}, \bm{W_C}, p, q, w\}$. In the proof, we cover each parameter in $\Theta_{\text{SSM}}$ and then combine these covers to construct a cover for the entire space of selective SSMs, using Lemma~\ref{lem:eq_constr_optim}.

    From Lemma~\ref{lem:epsilon_sum_bound_selective}, we can relate $\epsilon$-cover of a selective SSM to corresponding covers for each parameter in $\Theta_{\text{SSM}}$ as follows:
    \begin{equation}
    \begin{aligned}
        \epsilon &= \frac{\frak{B}_w \frak{B}_{\bm{C}} \frak{B}_u^2 \frak{M}_\Delta}{1 - \rho_{\bm{A}}} \epsilon_{W_B}  \\
        &\quad + \frac{\frak{M}_\Delta^2 \rho_{\bm{A}} \frak{B}_w \frak{B}_{\bm{B}} \frak{B}_{\bm{C}} \frak{B}_u^3}{(1 - \rho_{\bm{A}})^2} \epsilon_A \\
        &\quad + \left( \frac{\frak{B}_w \frak{B}_{\bm{B}} \frak{B}_{\bm{C}} \frak{B}_u^3}{1 - \rho_{\bm{A}}} \right) 
          \left( 1 + \frac{\frak{M}_\Delta \frak{B}_{\bm{A}} \rho_{\bm{A}}}{1 - \rho_{\bm{A}}} \right) (\epsilon_p + \epsilon_q) \\
        &\quad + \frak{B}_w \frak{B}_u \epsilon_{W_C} + \epsilon_w.
    \end{aligned}
    \end{equation}
    Choose the covering for $\bm{W}_B$ according to Lemma \ref{lem:linear_func_cover} such that
    \begin{equation}
    \begin{aligned}
        \log \mathcal{N}_\infty (\mathcal{F}_{W_B}, \epsilon_{W_B}, \| \; \|_2) \leq \frac{\frak{B}_u^2 \frak{M}_B^2}{\epsilon_{W_B}} \log(2Nd + 1).
    \end{aligned}
    \end{equation}
    Similarly, choose the covering for $\bm{W}_C$ by replacing $u$ in Lemma~\ref{lem:linear_func_cover} with $x[T]$ to derive
    \begin{equation}
    \begin{aligned}
        \log \mathcal{N}_\infty (\mathcal{F}_{W_C}, \epsilon_{W_C}, \| \; \|_2) &\leq \frac{\left( \frac{ \frak{M}_\Delta \frak{B}_{\bm{B}} \frak{B}_u^2}{1 - \rho_{\bm{A}}} \right)^2 \frak{M}_C^2}{\epsilon_{W_C}^2} \log(2dNd + 1) \\
        &= \frac{\frak{B}_{\bm{B}}^2 \frak{B}_u^4 \frak{M}_\Delta^2 \frak{M}_C^2}{(1 - \rho_{\bm{A}})^2\epsilon_{W_C}^2} \log(2Nd^2 + 1).
    \end{aligned}
    \end{equation}
    Likewise, choose the cover for $w$ such that
    \begin{equation}
    \begin{aligned}
        &\log \mathcal{N}_\infty (\mathcal{F}_w, \epsilon_w, \| \; \|_2) \leq \frac{\left( \frac{\frak{M}_\Delta \frak{B}_{\bm{B}} \frak{B}_{\bm{C}} \frak{B}_u^3}{1 - \rho_{\bm{A}}} \right)^2 \frak{M}_w^2}{\epsilon_w^2} \log(2d + 1) \\
        &= \frac{\frak{B}_{\bm{B}}^2 \frak{B}_{\bm{C}}^2 \frak{B}_u^6 \frak{M}_\Delta^2 \frak{M}_w^2}{(1 - \rho_{\bm{A}})^2 \epsilon_w^2} \log(2d + 1).
    \end{aligned}
    \end{equation}
    Lemma~\ref{lem:cover_A_selective} gives us the upper bound on the covering number for $\bm{A}_c$:
    \begin{equation}
        \log \mathcal{N} (\mathcal{F}_{A_c}, \epsilon_A, \| \; \|_2) \leq \frac{2\frak{M}_{A}^2 Nd}{\epsilon_A^2} \log (\sqrt{2}Nd).
    \end{equation}
    We may use Lemma~\ref{lem:linear_func_cover} again to cover $q$:
    \begin{equation}
        \log \mathcal{N}_\infty (\mathcal{F}_{q}, \epsilon_q, \| \; \|_2) \leq \frac{\frak{B}_u^2 \frak{M}_q^2}{\epsilon_q^2} \log(2d+1)
    \end{equation}
    and $p$ is covered simply by
    \begin{equation}
        \mathcal{N}_\infty (\mathcal{F}_{p}, \epsilon_p, \| \; \|_2) \leq \frac{2\frak{B}_p}{\epsilon_p}.
    \end{equation}
    Ignore the logarithmic dependencies and assume $\frak{M}_C = \frak{B}_{\bm{C}}, \frak{M}_B = \frak{B}_{\bm{B}}, \frak{M}_w = \frak{B}_w, \frak{M}_q = \frak{B}_q, \frak{M}_A = \frak{B}_{\bm{A}}$ for simplicity. Construct the cover for the space of all selective SSMs $\mathcal{F}_{\text{SSM}}$ as the Cartesian product of all covers for each parameter in $\Theta_{\text{SSM}}$. Then, the log covering number would be the sum of the log covering numbers of all parameters. Use Lemma \ref{lem:eq_constr_optim} to find $\epsilon_A, \epsilon_{W_B}, \epsilon_{w_C}, \epsilon_q, \epsilon_q$, and $\epsilon_w$ such that the size of total cover would be minimum
    \begin{equation}
    \begin{aligned}
        & \epsilon^2 \log \mathcal{N}_\infty (\mathcal{F}_{\text{SSM}}, \epsilon, \| \; \|_2) \\
        &\leq \mathcal{\tilde{O}} \left( \left( (\frak{B}_u^2 \frak{B}_{\bm{B}}^2)^{1/3} (\frac{ \frak{B}_w \frak{B}_{\bm{C}} \frak{B}_u^2 \frak{M}_\Delta}{1 - \rho_{\bm{A}}})^{2/3} + ( \frac{\frak{B}_{\bm{B}}^2 \frak{B}_u^4 \frak{M}_\Delta^2 \frak{B}_{\bm{C}}^2}{(1 - \rho_{\bm{A}})^2} )^{1/3}(\frak{B}_w \frak{B}_u)^{2/3} \right. \right. \\
        & + (\frak{B}_{\bm{A}}^2 N d)^{1/3} (\frac{\frak{M}_\Delta^2 \frak{B}_w \frak{B}_{\bm{C}} \frak{B}_{\bm{B}} \frak{B}_u^3 \rho_{\bm{A}}}{(1 - \rho_{\bm{A}})^2})^{2/3} + (\frac{\frak{B}_{\bm{B}}^2 \frak{B}_{\bm{C}}^2 \frak{B}_u^6 \frak{M}_\Delta^2 \frak{B}_w^2}{(1 - \rho_{\bm{A}})^2} )^{1/3} \\
        & + \left. \left. (\frak{B}_u^2 \frak{B}_q^2)^{1/3} \left( \frac{\frak{B}_w \frak{B}_{\bm{B}} \frak{B}_{\bm{C}} \frak{B}_u^3}{1 - \rho_{\bm{A}}} \right)^{2/3}
          \left( 1 + \frac{\frak{M}_\Delta \frak{B}_{\bm{A}} \rho_{\bm{A}}}{1 - \rho_{\bm{A}}} \right)^{2/3} \right)^3 \right).
    \end{aligned}
    \end{equation}
    in which we ignored the cover for $p$ as it is dominated by other terms.
    \begin{equation}
    \begin{aligned}
        &\leq \mathcal{\tilde{O}} \left( \left( \frak{M}_\Delta^{2/3} \frak{B}_w^{2/3} \frak{B}_u^2 \frak{B}_{\bm{B}}^{2/3} \frak{B}_{\bm{C}}^{2/3} (1 - \rho_{\bm{A}})^{-2/3} + \frak{M}_\Delta^{2/3} \frak{B}_w^{2/3} \frak{B}_u^2 \frak{B}_{\bm{B}}^{2/3} \frak{B}_{\bm{C}}^{2/3} (1 - \rho_{\bm{A}})^{-2/3} \right. \right. \\
        & + \frak{M}_\Delta^{4/3} \frak{B}_{\bm{A}}^{2/3} \frak{B}_w^{2/3} \frak{B}_u^2 \frak{B}_{\bm{B}}^{2/3} \frak{B}_{\bm{C}}^{2/3} (1 - \rho_{\bm{A}})^{-4/3} \rho_{\bm{A}}^{2/3} N^{1/3} d^{1/3} + \frak{M}_\Delta^{2/3} \frak{B}_w^{2/3} \frak{B}_u^2 \frak{B}_{\bm{B}}^{2/3} \frak{B}_{\bm{C}}^{2/3} (1 - \rho_{\bm{A}})^{-2/3} \\
        & + \left. \left. \frak{M}_\Delta^{2/3} \frak{B}_{\bm{A}}^{2/3} \frak{B}_q^{2/3} \frak{B}_w^{2/3} \frak{B}_u^{8/3} \frak{B}_{\bm{B}}^{2/3} \frak{B}_{\bm{C}}^{2/3} (1 - \rho_{\bm{A}})^{-4/3} \rho_{\bm{A}}^{2/3} \right)^3 \right),
    \end{aligned}
    \end{equation}
    where we ignored the constant term $1$ compared to $\frac{\frak{M}_\Delta \frak{B}_{\bm{A}} \rho_{\bm{A}}}{1 - \rho_{\bm{A}}}$.
    \begin{equation}
    \begin{aligned}
        &\leq \mathcal{\tilde{O}} \left( \frak{M}_\Delta^2 \frak{B}_w^{2} \frak{B}_u^6 \frak{B}_{\bm{B}}^{2} \frak{B}_{\bm{C}}^{2} (1 - \rho_{\bm{A}})^{-2} \left( 1 + 1 + \frak{M}_\Delta^{2/3} \frak{B}_{\bm{A}}^{2/3} (1 - \rho_{\bm{A}})^{-2/3} \rho_{\bm{A}}^{2/3} N^{1/3} d^{1/3} \right. \right. \\
        &+ \left. \left. 1 + \frak{B}_{\bm{A}}^{2/3} \frak{B}_q^{2/3} \frak{B}_u^{2/3} (1 - \rho_{\bm{A}})^{-2/3} \rho_{\bm{A}}^{2/3} \right)^3 \right)
    \end{aligned}
    \end{equation}
    The constant terms are dominated by others. Thus,
    \begin{equation}
        \leq \mathcal{\tilde{O}} \left( \frak{M}_\Delta^2 \frak{B}_w^{2} \frak{B}_u^6 \frak{B}_{\bm{B}}^{2} \frak{B}_{\bm{C}}^{2} \frak{B}_{\bm{A}}^2 (1 - \rho_{\bm{A}})^{-4} \rho_{\bm{A}}^{2} (\frak{M}_\Delta^{2/3} N^{1/3} d^{1/3} + \frak{B}_q^{2/3} \frak{B}_u^{2/3} )^3 \right).
    \end{equation}
    The square root of this expression is $\mathcal{C}_\mathcal{F}$. The proof is complete by the application of Lemma \ref{lem:generalization_error_bound_last}.
\end{proof}



\end{document}