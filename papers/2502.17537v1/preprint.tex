%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
% \newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[accepted]{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage{algorithmic}
% \usepackage[noend]{algpseudocode}
% \usepackage[linesnumbered,ruled,vlined,spanish,onelanguage]{algorithm2e}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

% \usepackage{multicol}
% \usepackage{multirow}
% \usepackage[ruled,vlined]{algorithm2e}
% \usepackage{algorithmic}
% % \usepackage{xcolor}
% \usepackage[dvipsnames]{xcolor}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\usepackage{multirow}
\usepackage{adjustbox, array}

\newcommand\figureWidthInTable{20mm}
\newcommand{\DC}[1]{{\color{red}{#1}}}
\newcommand{\AR}[1]{{\color{blue}{#1}}}
\newcommand{\LB}[1]{{\color{orange}{#1}}}
\usepackage{cuted}     % For the strip environment
\usepackage{capt-of}   % For \captionof command


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{On the Vulnerability of Concept Erasure in Diffusion Models}


\begin{document}

\twocolumn[
\icmltitle{%RECORD: Recalling Erased Concepts via Coordinate Descent 
On the Vulnerability of Concept Erasure in Diffusion Models}
%\DC{On the Vulnerability of Machine Unlearning: An Empirical Study}
% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Lucas Beerens}{equal,edi}
\icmlauthor{Alex D. Richardson}{equal,edi,edi_phys}
\icmlauthor{Kaicheng Zhang}{equal,edi}
\icmlauthor{Dongdong Chen}{hw}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{edi}{School of Mathematics and Maxwell Institute, University of Edinburgh, Edinburgh, UK}
\icmlaffiliation{edi_phys}{School of Physics and Astronomy, University of Edinburgh, Edinburgh, UK}
\icmlaffiliation{hw}{School of Mathematical and Computer Sciences, Heriot-Watt University, Edinburgh, UK}



% \icmlcorrespondingauthor{Lucas Beerens}{l.beerens@sms.ed.ac.uk}
% \icmlcorrespondingauthor{Alex D. Richardson}{alexander.richardson@gmail.com}
% \icmlcorrespondingauthor{Kaicheng Zhang}{k.zhang-60@sms.ed.ac.uk}
\icmlcorrespondingauthor{Lucas Beerens}{l.beerens@sms.ed.ac.uk}
% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

%\vskip 0.3in

]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{strip}
\centering
\includegraphics[width=0.9\linewidth]{icml2025/figures/high_level_summary_and_schematic_v4_dense.png}
\captionof{figure}{a) Examples of recalled behavior in various unlearned models, on van Gogh painting style. Shown is a specific adversarial prompt for CA \cite{kumariAblatingConceptsTexttoimage2023} b) Schematic of the update step in our RECORD Algorithm \ref{alg:record}, where $\hat{L}$ is our loss defined in Equation \ref{eq:approx_loss}. We start with an initial prompt as a list of tokens, and iterate over these tokens. We randomly sample tokens to approximate the full gradient of the loss with respect to the all token embeddings. We then select the K candidate tokens from the full vocabulary that are most aligned with this gradient, and propose the token that minimizes the loss function. If the prompt with that new token achieves a better loss, we update it.}
\label{fig:high_level_summary}
\end{strip}


\begin{abstract}

The proliferation of text-to-image diffusion models has raised significant privacy and security concerns, particularly regarding the generation of copyrighted or harmful images. To address these issues, research on machine unlearning has developed various concept erasure methods, which aim to remove the effect of unwanted data through post-hoc training. However, we show these erasure techniques are vulnerable, where images of supposedly erased concepts can still be generated using adversarially crafted prompts. We introduce \textbf{RECORD}, a coordinate-descent-based algorithm that discovers prompts capable of eliciting the generation of erased content. We demonstrate that RECORD significantly beats the attack success rate of current state-of-the-art attack methods. Furthermore, our findings reveal that models subjected to concept erasure are more susceptible to adversarial attacks than previously anticipated, highlighting the urgency for more robust unlearning approaches. We open source all our code at \url{https://github.com/LucasBeerens/RECORD}.

\end{abstract}

\section{Introduction}
\label{sec:intro}

%Although diffusion model has gathered huge amount of attention in the last half decade due to its outstanding capability to generate high-quanlity images 

%Introduce basic motivation for machine unlearning
%\AR{keep intro, background and discussion high level, have the method and results be detailed}
Denoising diffusion models have become the standard for text-to-image generative models \cite{yangDiffusionReview2024}. The widespread proliferation and adoption of this technology has many ethical and societal impacts \cite{bendel2023}, including the accessibility to production of harmful (violent, gory, pornographic, etc.) content and ease of copyright violation. Many large datasets include harmful or copyrighted data and, due to their size, it is difficult to clean them. Even so, retraining existing large models on cleaned datasets is expensive - especially if the definition of undesired output changes (such as changes in copyright law), requiring several re-trains. This has motivated the development of machine unlearning (or concept erasure) methods \cite{xuMachineUnlearningReview2023}, where trained models are fine tuned to `forget' small subsets of the training data. As many text-to-image generators are trained once on very large datasets that often include harmful material, and then hosted for the public to use, the importance of being able to cheaply and effectively `sanitize' models by unlearning undesired behavior is clear.



Within the literature of machine unlearning, there are several standard goals \cite{xuMachineUnlearningReview2023}. Models are fine tuned to unlearn specific objects (such as churches or parachutes); artistic styles (such as Van Gogh or Rembrandt); specific images (such as starry night by Van Gogh) and harmful concepts (such as nudity or gore). In this work, we verify the robustness of a wide range of unlearned models \cite{fanSalUnEmpoweringMachine2023,gandikotaErasingConceptsDiffusion2023,kumariAblatingConceptsTexttoimage2023,lyuOnedimensionalAdapterRule2024,wuEraseDiffErasingData2024,wuScissorhandsScrubData2024,zhangForgetmenotLearningForget2024,zhangDefensiveUnlearningAdversarial2024}, on a range of tasks - van Gogh painting style, the concept of nudity, and objects (church, parachute, garbage truck). These are chosen as they are widespread tasks in the literature, with respectively unlearned model weights available. To address unlearning copyrighted data, it is sufficient to use a style or concept as a proxy. For example, if an artist requested that their work be removed from a training set, would unlearning the artist style be sufficient?


We find that all unlearned models are vulnerable to text based adversarial attacks (Figure \ref{fig:high_level_summary}a). This vulnerability also shows that `unlearning' is misleading - model weights still contain information derived from undesirable images; and as such are still capable of producing undesired behavior. Instead, unlearning methods force models to suppress behaviors, under a range of standard prompt inputs, which is a form of model misalignment \cite{carliniAreAlignedNeural2023}. We believe this could have important consequences for copyright law - unlearning doesn't remove information derived from copyrighted data from a model's weights. We also compare the behaviors of different unlearned models to the stable diffusion \cite{rombachHighresolutionImageSynthesis2022} they are based on, to highlight some inherent vulnerabilities of diffusion models, and to gain insight into what concept erasure is actually doing.



Our contributions:
\begin{itemize}
    \item We propose a novel algorithm RECORD to recall the unlearned behavior from any unlearned diffusion model. We find adversarial prompts that beat the current state-of-the-art by 10x on attack success rate.
    \item By optimizing prompt embeddings from many random initializations, we explore this space and demonstrate that near any random prompt embedding, there exist adversarially recalling prompt embeddings.
    \item We explore the behavior of recalled unlearned models by comparing semantically meaningful latent activations of their U-nets during inference. This suggests to us that unlearning algorithms do not erase concepts from the model weights, but instead deliberately misalign models to not generate specific concepts.
    %\item We study the behaviour of recalled unlearned diffusion models, by 
\end{itemize}


\section{Background}
\label{sec:background}

\subsection{Text-to-Image Diffusion Models}

Diffusion Models are a class of generative model that generate images by learning to reverse the forward diffusion process. Starting with a Gaussian noise $x_T \sim\mathcal N(0,\mathbf 1)$, a trained denoiser, commonly U-Net \cite{UnetPaper} or Vision Transformer \cite{ViTpaper}, iteratively denoises $x_T$ over the timestep $t\in[0,T]$ until a clear image $x_0$ is reached. To perform text-to-image generation, the denoiser is trained conditioned on text embeddings $c=\mathcal T(y)$, where $y$ is some natural language prompt, $\mathcal T$ is a pre-trained text encoder, commonly CLIP \cite{radfordLearningTransferableVisual2021} or BLIP \cite{liBLIPBootstrappingLanguageImage2022}. By using datasets of text-image pairs, highly effective image denoisers can be trained that are conditioned on text input. In practice, many popular diffusion models (such as Stable Diffusion \cite{rombachHighresolutionImageSynthesis2022}) perform the denoising on a latent space, where $z_t = \mathcal E(x_t)$, then $z_t$ is iteratively denoised to $z_0$ and then decoded to $x_0 = \mathcal{D}(z_0)$. Here the latent space is defined by some image autoencoder, trained such that $x_0 \simeq \mathcal D(\mathcal E(x_0))$. To train a diffusion model, Gaussian noise $\epsilon$ is iteratively added to $z_0$ to reach $z_t$. The denoiser $\epsilon_\theta$ can then be trained by optimizing
\begin{gather}
    \label{eq:train_diffusion}
    \mathop{\text{argmin}}_{\theta} \mathbb E_{z\sim \mathcal E(x), t, \epsilon\sim\mathcal N(0,1), c} \left\| \epsilon- \epsilon_{\theta}(z_t, t, c) \right\|_2^2 ,
\end{gather}
where $\epsilon_\theta$ is predicting the noise $\epsilon$ that takes $z_{t-1}$ to $z_t$, effectively learning to reverse the Gaussian noise diffusion process.


\subsection{Concept Erasure}
The simplest and most effective way to prevent the generation of undesired content is to remove such images from the training dataset. However, this manual removal process is unfeasible and requires the model to be re-trained from scratch. Instead, much research effort focuses on model-editing-based approach, developing frameworks to finetune models in a post-hoc manner to remove or suppress undesired content while maintaining the quality of other generated images. Some prominent methods that we will adversarially recall from include: Erased Stable Diffusion (ESD) \cite{gandikotaErasingConceptsDiffusion2023}, Concept Ablation (CA) \cite{kumariAblatingConceptsTexttoimage2023}, EraseDiff (ED) \cite{wuEraseDiffErasingData2024}, Forget-Me-Not (FMN) \cite{zhangForgetmenotLearningForget2024}, Unified Concept Editing (UCE) \cite{gandikotaUnifiedConceptEditing2023}, ScissorHands (SH) \cite{wuScissorhandsScrubData2024}, Saliency Unlearning (SalUn) \cite{fanSalUnEmpoweringMachine2023}, Concept-SemiPermeable Membrane (SPM) \cite{lyuOnedimensionalAdapterRule2024}, and AdvUnlearn \cite{zhangDefensiveUnlearningAdversarial2024}. Most of these methods (ESD, CA, ED, FMN, UCE, SH, SalUn, SPM) involve fine-tuning the denoising U-net of the diffusion model. The exception is AdvUnlearn, which fine tunes the text embedding network. 



\subsection{Prompt Tuning}
Manipulating prompts to elicit specific behaviors from language models, also known as prompt tuning, is an important topic in Natural Language Processing research. \citet{ebrahimiHotFlipWhiteboxAdversarial2018} introduced HotFlip, generating adversarial examples through minimal character-level flips guided by gradients. Extending this, \citet{wallaceUniversalAdversarialTriggers2021} developed Universal Adversarial Triggers—input-agnostic token sequences optimized by using a first order taylor-expansion around the current token to select candidate tokens for exact evaluation.
\citet{shinAutoPromptElicitingKnowledge2020} presented AutoPrompt, which uses this algorithm to automatically generate prompts for various use cases. Addressing the lack of fluency in these prompts, \citet{shiHumanReadablePrompt2022} introduced FluentPrompt, incorporating fluency constraints and using Langevin Dynamics combined with Projected Stochastic Gradient Descent, where projection is done onto the set of token embeddings.

\citet{wenHardPromptsMade2023} developed PEZ, an algorithm inspired by FluentPrompt. It allows for prompt creation in both text-to-text and text-to-image applications.
In text-to-image models, \citet{galImageWorthOne2022} applied Textual Inversion, learning ``pseudo-words'' in the embedding space to capture specific visual concepts. Further advancements include GBDA \cite{guoGradientbasedAdversarialAttacks2021}, enabling gradient-based optimization over token distributions using the Gumbel-Softmax reparametrizaton \cite{jangCategoricalReparameterizationGumbelsoftmax2017} to stay on the probability simplex, and ARCA \cite{jonesAutomaticallyAuditingLarge2023}, optimizing discrete prompts via an improvement to AutoPrompt. ARCA will inspire our method.


\subsection{Concept Restoration}

Recently methods have been developed that aim to restore concepts intentionally unlearned from models by leveraging advanced optimization techniques inspired by prompt tuning. Concept Inversion (CI)~\cite{phamCircumventingConceptErasure2023} introduces a new token with a learnable embedding to represent the erased concept, optimized by minimizing reconstruction loss during denoising---directly applying Textual Inversion from prompt tuning to the concept restoration paradigm~\cite{galImageWorthOne2022}. Prompting Unlearned Diffusion Models (PUND)~\cite{hanProbingUnlearnedDiffusion2024} enhances this approach by iteratively erasing and searching for the concept by also updating model parameters, improving transferability across models.

Other methods focus on discrete token optimization. UnlearnDiffAtk(UD)~\cite{zhangGenerateNotSafetydriven2023} performs optimization over token distributions, similar to GBDA~\cite{guoGradientbasedAdversarialAttacks2021}, but utilizes projection onto the probability simplex instead of the Gumbel-Softmax reparametrization. Prompting4Debugging (P4D)~\cite{chinPrompting4DebuggingRedteamingTexttoimage2023} optimizes prompts in the embedding space and projects onto discrete embeddings, akin to the approach used in PEZ~\cite{wenHardPromptsMade2023}. Additionally, Ring-A-Bell~\cite{tsaiRingabellHowReliable2023} constructs an empirical representation of the erased concept by averaging embedding differences from prompts with and without the concept, then employs a genetic algorithm to optimize the prompt.

These methods demonstrate that optimization techniques from prompt tuning---both in continuous embeddings and discrete token spaces---can be used for algorithms to circumvent concept unlearning. 
%We refer the reader to appendix \ref{apd:related_works} for a more detailed review of the above methods.


\section{Method}
\label{sec:method}


\subsection{Motivation: Vulnerability of Machine Unlearning}
\noindent\textbf{Vulnerability} Verifying that a model has truly unlearned a concept is challenging. We consider an unlearned model\footnote{With `unlearned' we mean a trained model that has been fine tuned to unlearn something, not an untrained model.} $\epsilon_{\theta'}$ to be robust if it consistently fails to generate the erased content, even when subjected to \textit{any} adversarial prompt. In this work we demonstrate that all unlearned models are not robust, rather they are vulnerable. We construct a loss function \cite{chinPrompting4DebuggingRedteamingTexttoimage2023} that, when minimized, directly breaks this robustness definition:

\begin{equation}
    L(y) = \mathbb{E}_{t, \mathbf{z}} \Big[ 
    \left\| \boldsymbol{\epsilon}_{\theta'}\big(\mathbf{z}_t, t, \mathcal{T}(y)\big) - \boldsymbol{\epsilon}_{\theta}\big(\mathbf{z}_t, t, \mathcal{T}(y_{\text{target}})\big) \right\|_2^2 \Big]\label{eq:loss_ours_ideal}
\end{equation}
where $\mathbf{z}_t$ is obtained through the forward diffusion process with ${\mathbf{z}_0\sim p_{\text{target}}}$ coming from the target data distribution, and $y_\text{target}$ is the target prompt. See appendices \ref{app:loss_function_choice},\ref{app:embedding_results} for more discussion on loss functions.

\noindent\textbf{Latent spaces.}  To further understand the vulnerability of unlearned models, we explore their behavior in various latent spaces of interest. Firstly we explore how, during the iterative optimization to recall, our adversarial prompts move along trajectories in the prompt embedding space. By trying to sample this space, we gain considerable insight into the vulnerability of unlearning. We compare the difference in behavior between optimizing \textit{tokens} or prompt \textit{embeddings}, by either optimizing equation \ref{eq:loss_ours_ideal} for prompts $y$ or optimizing:

\begin{equation}
    \widetilde{L}(c) = \mathbb E_{t,\mathbf{z}} \left\| \epsilon_{\theta'}(\mathbf{z}_t, t, c) - \epsilon_{\theta}(\mathbf{z}_t, t, c_{\text{target}}) \right\|_2^2 \label{eq:loss_ours_ideal_cont}
\end{equation}
for embeddings $c$ where $c_{target}$ is the embedding of our target concept. Both optimization processes recall unlearned behavior, but they explore different regions of the input space, and therefor demonstrate different vulnerabilities. We also take inspiration from interpretability research \cite{ijishakinHSpaceSparseAutoencoders,surkovUnpackingSDXLTurbo2024,parkLatentRiemannian} where the internal activations (especially near the bottleneck) of the denoising U-net provide semantically meaningful latent spaces. We compare trajectories in these spaces during inference, for a range of recalled unlearned models and baseline models. 

%\noindent\textbf{Embedding attacks} To properly explore the vulnerability of unlearned models with respect to inputs, we 


\subsection{RECORD}\label{sec:record}
%\AR{Why coordinate descent? Highlight that it allows us to make large precise jumps (compared to adam optimiser in embedding space)}

%the expected discrepancy defined in \eqref{eq:robustness}. 
We introduce \textbf{RECORD} (Recalling Erased Concepts via Coordinate Descent), an algorithm for evaluating and challenging the robustness of concept erasure methods. RECORD searches for adversarial prompts that cause unlearned models to generate images containing erased concepts, thereby revealing any residual concept information retained after unlearning.
Given an original model $\theta$, an unlearned model $\theta'$, and a target prompt $y_{\text{target}}$ representing the erased concept, our goal is to find an adversarial prompt $y^*$ that minimizes Equation \ref{eq:loss_ours_ideal}.
Let $L(y)$ denote this expectation for a prompt $y$ with optimal prompt
\begin{equation}
y^* = \mathop{\text{argmin} }_y L(y).
\end{equation}
Since computing the exact expectation over all latents and timesteps is intractable, we approximate $L(y)$ by sampling batches:
%
\begin{multline}\label{eq:approx_loss}
\hat{L}(y, \mathcal{Z}) ={}  \sum_{(\mathbf{z}_t, t) \in \mathcal{Z}} \Bigl\| \boldsymbol{\epsilon}_{\theta'}\left(\mathbf{z}_t, t, \mathcal{T}(y)\right) \\
- \boldsymbol{\epsilon}_{\theta}\left(\mathbf{z}_t, t, \mathcal{T}\left(y_{\text{target}}\right)\right) \Bigr\|_2^2,
\end{multline}
%
where $\mathcal{Z}$ is a set of noised images and their corresponding timesteps. We maintain a reference set $\mathcal{R}$ of noised images and timesteps to consistently evaluate optimization progress.
The key challenge in optimizing this objective lies in the discrete nature of the token sequence space and the non-differentiable mapping from tokens to embeddings. Standard gradient-based methods are not directly applicable. Instead, we employ coordinate descent, iteratively optimizing one token at a time while keeping others fixed. 

%\LB{Move reference to arca to background.}


RECORD begins by initializing a random token sequence $y = [y_1, \ldots, y_S]$ of length $S$. To iteratively refine this sequence, we perform $N$ passes over $y$. In each pass, we generate a random permutation of the token positions to mitigate positional bias during updates. For each position $s$ in the permuted sequence, the algorithm samples a batch of $K$ images $\mathbf{z}_0^{(n)}$ and corresponding timesteps $\mathbf{t}^{(n, s)}$. Candidate tokens for position $s$ are then selected using a gradient-based approximation method. These candidates are evaluated using a predefined objective function, and if any candidate yields an improvement according to the evaluation metric, $y_s$ is updated. This iterative process continues for all positions in the permutation and repeats for $N$ passes, progressively enhancing the token sequence over time. A pseudocode can be found in Algorithm \ref{alg:record}.

\begin{algorithm}[t]

% \algsetup{linenosize=\tiny}
\scriptsize
% \selectfont{8}
% \SetAlgoLined
%\footnotesize
\ttfamily\textcolor{teal}{}\\
\ttfamily\textcolor{teal}{\#\hspace{1.5mm}$\theta'$:\hspace{1.5mm}original model;\hspace{1.5mm}$\theta$:\hspace{1.5mm}unlearned model}\\
% \ttfamily\textcolor{teal}{\#\hspace{1.5mm}$\theta$:\hspace{1.5mm}unlearned model}\\
\ttfamily\textcolor{teal}{\#\hspace{1.5mm}$y_{\text{target}}$:\hspace{1.5mm}target prompt}\\
\ttfamily\textcolor{teal}{\#\hspace{1.5mm}$J$:\hspace{1.5mm}gradient samples;\hspace{1.5mm}$K$:\hspace{1.5mm}candidates}\\
% \ttfamily\textcolor{teal}{\#\hspace{1.5mm}$K$:\hspace{1.5mm}candidates}\\
\ttfamily\textcolor{teal}{\#\hspace{1.5mm}$S$:\hspace{1.5mm}sequence length;\hspace{1.5mm}$N$:\hspace{1.5mm}passes}\\
\ttfamily\textcolor{teal}{\#\hspace{1.5mm}$R$:\hspace{1.5mm}reference set; \hspace{1.5mm}$E$:\hspace{1.5mm}embedding table}\\
% \ttfamily\textcolor{teal}{\#\hspace{1.5mm}$R$:\hspace{1.5mm}reference set}\\
% \ttfamily\textcolor{teal}{\#\hspace{1.5mm}$E$:\hspace{1.5mm}embedding table}\\

\ttfamily{Random token sequence $y$ of length $S$:}\ttfamily\textcolor{teal}{\hspace{1.5mm}\#\hspace{1.5mm}initialization}\\
\ttfamily{for n=1 to N:}\ttfamily\textcolor{teal}{\hspace{1.5mm}\#\hspace{1.5mm}load N passes}\\
\hspace*{4.3mm}\ttfamily\textcolor{teal}{\#\hspace{1.5mm}Shuffle}\\
\hspace*{4.3mm}\ttfamily{Random permutation $\pi$ of positions $\{1,\cdots, S\}$}\ttfamily\textcolor{teal}{\hspace{1.5mm}\hspace{1.5mm}}\\
% \vspace{-0.5mm}

\hspace*{4.3mm}\ttfamily{for $s$ in $\pi$:}\ttfamily\textcolor{teal}{\hspace{1.5mm}\hspace{1.5mm}}\\
\hspace*{6.3mm}\ttfamily\textcolor{teal}{\#\hspace{1.5mm}sample data}\\
\hspace*{6.3mm}\ttfamily{Sample batch $\mathcal{Z}$ of noise images and timesteps}\ttfamily\textcolor{teal}{\hspace{1.5mm}\hspace{1.5mm}}\\
\hspace*{6.3mm}\ttfamily\textcolor{teal}{\#\hspace{1.5mm}candidate selection}\\
\vspace{-1.5mm}
\begin{displaymath}\label{alg:cs}
\left\lfloor
  \begin{array}{llr}
  % \text{Candidate Selection}& & \\
  \text{Sample } J \text{ random tokens } \{c_j\}& & \\
  % \text{Compute gradients} \bar{\mathbf{g}}_j = \nabla_{e_{c_j}}\, \hat{L}\big( y(c_j, s),\ \mathcal{Z} \big)\\
    \text{Compute gradients } \bar{\mathbf{g}}_j \text{ via (6)} \\
  \text{Score all tokens:} \text{scores} \leftarrow E\, \bar{\mathbf{g}}\\
  \text{Select top } K \text{ tokens } \mathcal{C} \text{ based on scores}\\
  \end{array}
\right.
\end{displaymath}
\hspace*{6.3mm}\ttfamily\textcolor{teal}{\#\hspace{1.5mm}candidate evaluation}\\
\hspace*{6.3mm}\ttfamily{$\hat{c}^* = \arg\min_{c \in \mathcal{C}}\, \hat{L}\big( y(c, s),\ \mathcal{Z} \big)$}\ttfamily\textcolor{teal}{\#\hspace{1.5mm}best candidate}\\
\hspace*{6.3mm}\ttfamily\textcolor{teal}{\#\hspace{1.5mm}coordinate descent}\\
\hspace*{6.3mm}\ttfamily{if $\hat{L}\big( y(\hat{c}^*, s),\ \mathcal{R} \big) <  \hat{L}(y,\ \mathcal{R})$:}\ttfamily\textcolor{teal}{\hspace{1.5mm}}\\
\hspace*{10.3mm}\ttfamily{Update $y \leftarrow y(\hat{c}^*, s)$}\ttfamily\textcolor{teal}{\hspace{1.5mm}}\\
\ttfamily{Return: optimized prompt $y$ for recalling erased concepts}\ttfamily\textcolor{teal}{\hspace{1.5mm}\hspace{1.5mm}}
\caption{Pseudocode of RECORD.}%\label{algo:record}
\label{alg:record}
\end{algorithm}


\begin{table*}[h]
    \centering
    \setlength{\tabcolsep}{1mm}
    \caption{Attack success rate (\%) for models with the van Gogh style erased. Comparing the restoration methods P4D~\cite{chinPrompting4DebuggingRedteamingTexttoimage2023}, UD~\cite{zhangGenerateNotSafetydriven2023}, and RECORD (ours).}
    \tiny{\input{icml2025/tables/vangogh_token_atk}}
    \label{tab:vangogh_results}
\end{table*}

\begin{table*}[h]
    \centering
    \setlength{\tabcolsep}{1mm}
    \caption{Attack success rate (\%) for models with the concepts Church, Garbage Truck, Parachute, and Nudity erased. Comparing the restoration methods P4D~\cite{chinPrompting4DebuggingRedteamingTexttoimage2023}, UD~\cite{zhangGenerateNotSafetydriven2023}, and RECORD (ours).}
    \tiny{
    \input{icml2025/tables/object_token_atk}}
    \label{tab:combined_results}
\end{table*}


To efficiently select candidate tokens at each position, we use an approximation strategy. We sample $J$ random tokens and compute gradients with respect to their embeddings:
\begin{equation}
\mathbf{g}_j = \nabla{c_j}\hat{L}(y({c_j},s), \mathcal{Z}(n,s)).
\end{equation}
The average gradient $\bar{\mathbf{g}} = \frac{1}{J}\sum_{j=1}^J \mathbf{g}_j$ provides a linear approximation of how the loss changes with different token embeddings. By multiplying the embedding table $E$ with $\bar{\mathbf{g}}$, we efficiently score all possible tokens and select the top $C$ candidates for exact evaluation.

To ensure stable optimization in the discrete token space, we employ a greedy update strategy. A candidate token $\hat{c}^*$ is only accepted if it improves the loss on the reference set $\mathcal{R}$:
\begin{equation}
\hat{L}(y(\hat{c}^*,s), \mathcal{R}) < \hat{L}(y, \mathcal{R}).
\end{equation}
Since each accepted token replacement strictly decreases the loss and the number of possible token sequences is finite, the algorithm is guaranteed to converge to a coordinate-wise local minimum.




\section{Experiments}
\label{sec:experiments}
We compare RECORD against UD~\cite{zhangGenerateNotSafetydriven2023} and P4D~\cite{chinPrompting4DebuggingRedteamingTexttoimage2023} on diffusion models unlearned with a large variety of methods (ESD \cite{gandikotaErasingConceptsDiffusion2023}, ED \cite{wuEraseDiffErasingData2024}, SH \cite{wuScissorhandsScrubData2024}, FMN \cite{zhangForgetmenotLearningForget2024}, CA \cite{kumariAblatingConceptsTexttoimage2023}, SPM \cite{lyuOnedimensionalAdapterRule2024}, SalUn \cite{fanSalUnEmpoweringMachine2023}, UCE \cite{gandikotaUnifiedConceptEditing2023} and AdvUnlearn \cite{zhangDefensiveUnlearningAdversarial2024}). All the unlearned models we consider were unlearned from the baseline of a trained Stable Diffusion 1.4 \cite{rombachHighresolutionImageSynthesis2022} text-to-image generator, which we just refer to as the `baseline'. The erased concepts include art style (Van Gogh), objects (church, garbage truck, parachute) and nudity, and the weights of the unlearned models are obtained from \cite{zhangDefensiveUnlearningAdversarial2024}. We run each attack on one Nvidia H100 GPU. We also analyze the behavior of recalled models to gain insight into how they are vulnerable. Note that not all unlearning techniques have been developed on all tasks.


\begin{table*}[h]
    \centering
    \setlength{\tabcolsep}{1mm}
    \caption{Generated images of erased concepts using prompt attacks. Each image column of the same concept is generated using the same latent initialization. Models have unlearned the style of van Gogh paintings. Target prompt "a painting in the style of van Gogh". }
    \small{\input{icml2025/tables/figures_vangogh_token_atk}}
    \label{tab:token_attack_figures_vangogh}
\end{table*}
\subsection{Evaluation metrics}
It is common in the concept erasure literature to use a large variety of classifier models from different sources to evaluate the attack methods, as no one classifier is trained on all the erased concepts. However we alternatively use a single zero-shot diffusion classifier (Stable Diffusion v2.1) \cite{liYourDiffusionModel2023, clarkTexttoImageDiffusionModels2023} to classify the generated image. The classification results are obtained by computing 
\begin{gather} \label{eq:diffusion_classifier}
    y^* = \mathop{\text{argmin}}_{y_i\in Y} \mathbb E_t \left\| \epsilon - \epsilon_\theta(z_t, t, \mathcal T(y_i)) \right\|_2^2 ,
\end{gather}
where $t\sim U(0,T)$, $Y = \{y_1, y_2,\dots, y_{n}\}$ is a set of $n$ pre-specified classification classes, and the expectation is computed over 10 samples, which for our experiments is sufficient to provide good classification accuracies. For art style and object attacks, we build sets of 50 classes using prompt templates `a painting in the style of \{artist\_name\}' and `a photorealistic image of \{object\}', where the artist names are randomly chosen from a list of famous painters, e.g. Leonardo Da Vinci, and object names are from the classification classes of YOLOv3 \cite{redmonYOLOv3IncrementalImprovement2018}. For nudity attacks, a set of 4 classes are built with the same template as the object attacks. For all attacks, we also additionally add one empty class, ` ', which empirically can help the classifier capture images that fall significantly outside the distributions of the specified classes. All results presented in this section use 500 images generated by the corresponding models and attacks. For all the various methods and models we report the Attack Success Rate (ASR) in percentage. This is calculated by dividing the number of images classified as the target by the total number of generated images.



\subsection{Prompt Attacks}





We evaluate RECORD by comparing it with UD~\cite{zhangGenerateNotSafetydriven2023} and P4D~\cite{chinPrompting4DebuggingRedteamingTexttoimage2023}. For a fair comparison, we give each method 64 tokens to optimize. Each method starts with a random prompt, except UD. Since UD optimizes on a token distribution, we give it a uniform distribution for all tokens except the first few that are initialized to be the target prompt. Without this type of initialization UD would not be able to get any significant performance.

For RECORD, we use $N=20$ passes through the token list. We use batches of 1 image each. During the candidate selection, we make use of $J=64$ samples. The chosen candidate set has size $K=64$. These numbers are chosen to best use the available VRAM.

Each of the three methods are used to created 50 adversarial prompts. Every prompt is used to create 10 images for a total of 500 images per method to be used for ASR calculations. Example images can be found in Table~\ref{tab:token_attack_figures_vangogh}. Examples for different unlearned concepts can be found in Appendix~\ref{sec:examples}.


In Tables~\ref{tab:vangogh_results} and \ref{tab:combined_results} we can see that RECORD outperforms both other methods consistently, except for a few minor exceptions. In particular, AdvUnlearn is quite resilient against P4D and UD with single digit ASR on most concepts, while RECORD is able to achieve an ASR of at least $33\%$ for all concepts. 

The low performance of all methods against SH can be explained by the findings by \citet{zhangDefensiveUnlearningAdversarial2024} that SH degrades their image quality by a lot more than other methods to achieve this level of robustness. 



\subsection{Prompt embedding manifolds}
To assess the vulnerability of unlearned U-nets, we sample the space of prompt embeddings to find where these models are vulnerable (Figure \ref{fig:prompt-embedding}). We consider just ESD here, but find that all the other models behave the same. We also include the embedding optimization process on the baseline model for comparison. We perform a 2D isomap projection (Figure \ref{fig:prompt-embedding}a) to visualize how trajectories of prompt embedding optimizations behave. We also include histograms of distance metrics between: initial and target (Figure \ref{fig:prompt-embedding}b); optimized and target (Figure \ref{fig:prompt-embedding}c); and initial and target positions (Figure \ref{fig:prompt-embedding}d). Initial conditions are sampled `close' to the target prompt of `a painting in the style of van Gogh', by padding the target prompt with a random length of random characters. Initial conditions `far' from the target were generated by uniformly randomly sampling characters. These classes of initial conditions separate into clusters of the latent space, but there is no discernible difference here between baseline and unlearned models. In any region of the prompt embedding space we sample, there exists a nearby adversarial prompt embedding, suggesting that unlearned models are vulnerable to small perturbations of prompt embeddings almost everywhere. Initial prompt embeddings of unlearned models that start near the target prompt embedding, move away from the target prompt embedding during embedding optimization. However this is less pronounced in the baseline model. 



When RECORD optimizations are projected onto this same space (Figure \ref{fig:prompt-embedding}), we see that they take much larger jumps than the continuous embedding optimization. We also find that, although initialized randomly, RECORD stays far away from the target prompt, despite taking large jumps over the space. This shows that unlearned models are also vulnerable to prompts that have little relation to the target prompt. We distinguish between ESD \cite{gandikotaErasingConceptsDiffusion2023} and AdvUnlearn \cite{zhangDefensiveUnlearningAdversarial2024} because AdvUnlearn has unlearned text encoder weights, whereas ESD has unlearned U-net weights. We find little difference in behavior.


\begin{figure*}[t]
\centering
\includegraphics[width=0.96\linewidth]{icml2025/figures/unet_feature_comparison_target_and_recalled.png}
\caption{Comparison of activation trajectories during inference of RECORD unlearned (a-d) and unlearned with target prompt (e-h) to the trajectory of the baseline with target prompt. Mid.0 and Down.2.1 represent the bottleneck and final downsampling layers of the U-net. Target prompt: "a painting in the style of van gogh". We also compare the baseline with a semantically similar prompt "a painting of starry night by van gogh" and an unrelated prompt "a photorealistic image of a sports car", to gauge where semantically close or unrelated trajectories are expected to be. All generations use the same seed, except the baseline with `different seed'. Unlearned trajectory distances are averaged over 20 successful adversarial prompts.}
\label{fig:semantic-unet-latents}
\end{figure*}
\subsection{Semantic Latents}
To further explore the vulnerability of unlearned methods, we compare the behavior of recalled unlearned models and the baseline stable diffusion on the generation of van Gogh paintings. The bottleneck of the U-nets used for T2I diffusion models learn to represent a latent space with semantic meaning \cite{surkovUnpackingSDXLTurbo2024,parkLatentRiemannian}. We borrow the nomenclature from literature \cite{surkovUnpackingSDXLTurbo2024} where `mid.0' refers to the bottleneck of the U-net, and `down.2.1' is the final downsampling block before this bottleneck.


By comparing the trajectories (during inference) of the activations in this semantic latent space, we can assess whether a recalled unlearned model is `close' to the baseline with the target prompt \ref{fig:semantic-unet-latents}. Most of the unlearning methods we compare work by modifying the weights near the U-net bottleneck, so inspecting the latent space defined by the activations at this region is reasonable. We find that recalled unlearned models have trajectories that are as `far away' as unrelated prompts in this space from the baseline (Figure \ref{fig:semantic-unet-latents}a-d). The fact that these trajectories still succeed in generating paintings in the style of van Gogh suggests that unlearning does not remove information from the U-net, but instead transforms how the U-net responds to text embeddings. Furthermore, we find that unlearned models with the target prompt produce trajectories much closer to the baseline-target prompt (Figure \ref{fig:semantic-unet-latents}e-h), despite not rendering van Gogh paintings. This indicates that RECORD finds prompts that take far away trajectories in this semantic latent space, but they still yield van Gogh paintings. This highlights the inherent complexity of such spaces, and suggests that unlearning requires a better understanding of how these models store semantic meaning.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.99\linewidth]{icml2025/figures/isomap_unlearned_orignal_random_van_gogh.png}
    \caption{Behavior of prompt embedding vectors during RECORD and embedding optimization. a) Isomap projection of prompt embeddings for RECORD and embedding optimisation. b) Histograms of cosine similarities between initial and target embeddings. c) Histograms of cosine similarities between optimized and target embeddings. d) Histograms of cosine similarities between optimized and initial embeddings. All trajectories shown here successfully recall from the unlearned models}
    \label{fig:prompt-embedding}
\end{figure}




\section{Discussion}

We have demonstrated that existing machine unlearning methods do not erase the desired concept from diffusion models. We note that not all unlearning methods perform equally, in particular Scissorhands \cite{wuScissorhandsScrubData2024} is the hardest to recall from, although it is still possible. As text-to-image generators proliferate and become ubiquitous, it is crucial that we understand how to minimize their misuse. Unfortunately the current paradigm of post-hoc unlearning harmful concepts from models trained on vast datasets is flawed.

Although we have demonstrated and explored how machine unlearning is currently vulnerable, we have not yet provided a solution. We suspect that robust unlearning would require far greater understanding of exactly how diffusion models store information, which in turn leads to problems in interpretability research. Work on semantic guidance during inference \cite{parkLatentRiemannian,surkovUnpackingSDXLTurbo2024} could provide tools for isolating specific concepts in models, however there is no guarantee this would yield robust unlearning mechanisms.

Our methods have a few important weaknesses worth discussing. Firstly we obviously white-box access, which limits the utility of RECORD for recalling unlearned text-to-image generators that one has black-box access to. Second is computational cost - with the limits of VRAM on Nvidia H100, we could only run RECORD with a batch size of 1.

\section{Conclusions}
Existing machine unlearning algorithms do not remove information from diffusion models. We have demonstrated, with a range of techniques, that these unlearned models can always be recalled to produce images of the so called erased concept. The behavior of recalled unlearned models when assessed during optimization of the recalling prompt/embedding, and during inference, suggests that these vulnerabilities are widespread and more serious than previously thought.
\label{sec:conclusions}


\section{Acknowledgments}
LB, AR, and KZ acknowledge support of MAC-MIGS CDT under EPSRC grant {EP/S023291/1}. This work was supported by the Edinburgh International Data Facility (EIDF) and the Data-Driven Innovation Programme at the University of Edinburgh. We thank Christos Christodoulopoulos for his useful discussions, and for his input to the Hackathon Workshop on Generative Modeling.


\bibliography{icml2025/RECCD}
\bibliographystyle{icml2025}


\newpage 

\ % The empty page



\appendix







\section{Choice of loss function}\label{app:loss_function_choice}


\textbf{Preliminary.} We consider two loss functions from literature \cite{phamCircumventingConceptErasure2023,zhangGenerateNotSafetydriven2023,chinPrompting4DebuggingRedteamingTexttoimage2023}, that when minimized would find such an adversarial prompt that recalls from unlearned models:

\begin{align}
    L_1(y) &= \mathbb{E}_{t, \mathbf{z}}\Big[ \left\| \boldsymbol{\epsilon}_{\theta'}\big(\mathbf{z}_t, t, \mathcal{T}(y)\big) - \boldsymbol{\epsilon} \right\|_2^2\Big]\label{eq:loss_definitions_pham}\\
    L_2(y) &= \mathbb{E}_{t, \mathbf{z}} \Big[ 
    \left\| \boldsymbol{\epsilon}_{\theta'}\big(\mathbf{z}_t, t, \mathcal{T}(y)\big) - \boldsymbol{\epsilon}_{\theta}\big(\mathbf{z}_t, t, \mathcal{T}(y_{\text{target}})\big) \right\|_2^2 \Big]\label{eq:loss_definitions_ours}
\end{align}


where $\mathbf{z}_t$ is obtained through the forward diffusion process with ${\mathbf{z}_0\sim p_{\text{target}}}$ coming from the target data distribution, and $y_\text{target}$ is the target prompt. $L_1$ \cite{phamCircumventingConceptErasure2023} and $L_2$ \cite{chinPrompting4DebuggingRedteamingTexttoimage2023} are minimized by prompts $y$ that match the unlearned denoiser prediction respectively to: the true noise from the forward diffusion sequence $\mathbf{z}_t$; and the predicted noise by the baseline denoiser with the target prompt. The main difference in behavior when optimizing both $L_1$ and $L_2$ derives from the imperfections of the baseline model - predictions of $\epsilon_\theta$ don't perfectly match $\epsilon$. We note that minimizing $L_2$ should find $y$ that more accurately matches the behavior of the baseline model, including it's imperfections. 




\noindent\textbf{Prompt embedding attacks.} To compare both our loss functions, we first consider the simpler proxy problem of optimizing directly over prompt embeddings rather than prompts:
\begin{align}
    \widetilde{L}_1(c) &= \mathbb E_{t,\mathbf{z}} \left\| \epsilon_{\theta'}(\mathbf{z}_t, t, c) - \epsilon \right\|_2^2 \label{eq:loss_definitions_pham_cont}\\
    \widetilde{L}_2(c) &= \mathbb E_{t,\mathbf{z}} \left\| \epsilon_{\theta'}(\mathbf{z}_t, t, c) - \epsilon_{\theta}(\mathbf{z}_t, t, c_{\text{target}}) \right\|_2^2 \label{eq:loss_definitions_ours_cont}.
\end{align}
As everything is continuous, the optimization is fairly straightforward. 




% \section{Embedding attack example images}


\section{Embedding Attacks}\label{app:embedding_results}
% \AR{make changes here as the table is named as embed attack 1 and 2. Focus the purpose of the embedding attacks - it is to compare which loss function works best} 
%We compared the performance of the loss formulation \eqref{} object with a baseline objective \eqref{eq:pham_contextual_inversion} which is more widely used in literature. We note there is no prior work on concept restoration that explicitly compare the performance between the two loss functions. We use NAdam optimizer and iterates over a training set of 100 images 50 times, with a constant learning rate of 0.1 and a batch size of 16. The training set is generated by the original diffusion model using the same prompt. 
Before we demonstrate the prompt attacks, we explore prompt embedding attacks to compare our loss functions (Equations \ref{eq:loss_definitions_pham} and \ref{eq:loss_definitions_ours}). We optimize on the continuous proxies (Equations \ref{eq:loss_definitions_pham_cont} and \ref{eq:loss_definitions_ours_cont}), where Embed attack 1 and 2 minimize both $\widetilde{L}_1$ and $\widetilde{L}_2$ respectively. We use NAdam optimizer and iterate over a training set of 100 images 50 times, with a constant learning rate of 0.1 and a batch size of 16. The training set is generated by the original diffusion model using the target prompt. The two methods were used to create 500 adversarial prompt embeddings each. One image per prompt was created using the corresponding unlearned model.

\begin{table*}[t]
    \centering
    \caption{The classification accuracy and the attack success rate of the embedding attacks for art style, object and nudity attacks. }
    \tiny{
    \input{icml2025/tables/vangogh_embed_atk}
    \input{icml2025/tables/object_embed_atk}
    \input{icml2025/tables/nudity_embed_atk}}
    \label{tab:embed_attack_asr}
\end{table*}

The results are shown in Table \ref{tab:embed_attack_asr}. We notice that, when the classifier has a high accuracy in classifying the image from the original model, RECORD-embed works marginally better than the baseline. When the classification accuracy is already low on the original model, the difference between the two objectives becomes less obvious. This is because, when the original model can generate more `accurate' images in the perspective of the classifier, its output can also act as a more informative surrogate to aid the attack process.

% For example, even though most images for AdvUnlearn looks very much like a parachute, the classifier consistently misclassify it as fireworks.

Additionally, when performing text embedding attacks on AdvUnlearn, which modifies only the text encoder rather than the U-Net, the problem in this case collapses to `finding a text embedding that can elicit the original diffusion model to generate the target concept'. Such text embedding is guaranteed to exist, and it is thus interesting to observe that in this seemingly easier setting, the attack success rate is not necessarily higher than other U-net-based concept erasure methods.






%\section{Prompt attack example images}
\section{Example Images}\label{sec:examples}
Included here are example outputs of our and various other recalling methods, for a range of tasks and unlearned models.


\begin{table*}[t]
    \centering
    \setlength{\tabcolsep}{1mm}
    \caption{Generated images of erased concepts using embedding attacks. Each image column of the same concept is generated using the same latent initialization. Models have unlearned the style of van gogh paintings. Target prompt "a painting in the style of van gogh"}
    \small{\input{icml2025/tables/figures_vangogh_embed_atk}}
    %\DC{what dose the prompt look like?} \DC{Can we show some REALLY harmful/unwanted concepts/objects which are so claimed erased in the 'machine unlearning' model but still be generated using prompts given by the proposed RECORD?}}
    \label{tab:embed_attack_figures_vangogh}
\end{table*}

\begin{table*}[t]
    \centering
    \setlength{\tabcolsep}{1mm}
    \caption{Generated images of erased concepts using embedding attacks. Each image column of the same concept is generated using the same latent initialization. Models have unlearned the concept of nudity. Target prompt asked for nudity.}
    \small{\input{icml2025/tables/figures_nudity_embed_atk}}
    \label{tab:embed_attack_figures_nudity}
\end{table*}
\begin{table*}[t]
    \centering
    \setlength{\tabcolsep}{1mm}
    \caption{The generated images of erased concepts using embedding attacks. Each image column of the same concept is generated using the same latent initialization. Target prompt "a photorealistic image of \textit{object}"}
    \small{\input{icml2025/tables/figures_object_embed_atk}}
    \label{tab:embed_attack_figures_obj}
\end{table*}

\begin{table*}[t]
    \centering
    \setlength{\tabcolsep}{1mm}
    \caption{The generated images of erased concepts using prompt attacks. Each image column of the same concept is generated using the same latent initialization. Target prompt "a photorealistic image of \textit{object}"}
    \small{\input{icml2025/tables/figures_object_token_atk}}
    \label{tab:embed_attack_figures_obj}
\end{table*}

\begin{table*}[t]
    \centering
    \setlength{\tabcolsep}{1mm}
    \caption{The generated images of erased concepts using prompt attacks. Each image column of the same concept is generated using the same latent initialization. Target prompt asked for nudity.}
    \small{\input{icml2025/tables/figures_nudity_token_atk}}
    \label{tab:embed_attack_figures_obj}
\end{table*}


\section{RECORD with less tokens}

In order to see the effect of the token count, we used RECORD with 16 and 32 tokens. The results can be found in Tables~\ref{tab:vangogh_results_count} and \ref{tab:combined_results_count}. 

\begin{table*}[t]
    \centering
    \caption{Attack success rate (\%) for models with the van Gogh style erased. Comparing RECORD with different allowed prompt lengths in terms of tokens.}
    \small{\input{icml2025/tables/vangogh_token_atk_count}}
    \label{tab:vangogh_results_count}
\end{table*}

\begin{table*}[t]
    \centering
    \caption{Attack success rate (\%) for models with the concepts Church, Garbage Truck, Parachute, and Nudity erased. Comparing RECORD with different allowed prompt lengths in terms of tokens}
    \small{\input{icml2025/tables/object_token_atk_count}}
    \label{tab:combined_results_count}
\end{table*}

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
