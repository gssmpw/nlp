\section{Related Work}
\label{sec:related_work}
%
{\flushleft \textbf{Camera-based 3D Object Detection.}}  
The success of 2D object detection**Li, "Single Shot Multibox Detector"**, with the growing demand for 3D perception in fields like autonomous driving and robotics, prompts the development of 3D object detection technology. 
% Thanks to the emergence of large-scale multi-view camera datasets**Sun et al., "CityPersons: A Diverse Looking Dataset for Pedestrian Detection"**, 
Early works**Ren et al., "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"** are based on multi-view cameras which leverage multi-view information through cross-view interaction to improve 3D object detection performance. 
%
The multi-view 3D object detection methods can be briefly divided into two categories, \textit{i.e.}, dense BEV-based and sparse query-based methods. 

Numerous dense BEV-based methods adopt Lift-Splat-Shoot (LSS)**Chen et al., "Lift-Splat-Shoot: Efficient Convolutional 3D Object Detection in Point Clouds"** to transform 2D features into BEV features, such as BEVDet**Wang et al., "BEVDet: A Unified Framework for 3D Object Detection from LiDAR and RGB Cameras"**. 
%
On the other hand, BEVDepth**Huang et al., "BEVDepth: A Novel Method for Depth Estimation in Bird's Eye View Space"** designs a trustworthy depth estimation module for better view transformation in the BEV space. 
%
For sparse query-based methods, 
%
PERT series**Li et al., "Perturbation-Resistant 3D Object Detection from LiDAR and Camera Fusion"** incorporate the position information of 3D coordinates into image features and integrate long-term temporal fusion.


While these methods achieve advanced 3D object detection performance, they overlook robustness, a key factor in real-world applications. Unlike millimeter-wave radar, camera images are prone to interference in darkness and bad weather, leading to poor detection performance.
%
To this end, \modelname~incorporates the camera signal confidence map to effectively enhance network robustness along with radar modality under various conditions.
{\flushleft \textbf{Radar-Camera 3D Object Detection.}}  
The camera sensor inherently lacks 3D depth information, limiting its 3D detection accuracy. 
%
To alleviate this issue, researchers propose incorporating the cost-effective radar sensor into the 3D detection framework.
%
The radar sensor provides the 3D depth prior and additional Doppler velocity, compensating for the camera sensor's weaknesses. 

Specifically, CenterFusion**Hou et al., "CenterFusion: A Novel Method for Fusion of Radar and Camera Features in 3D Object Detection"** uses a key point detection network to obtain center points and then associates key points with the corresponding radar detection results in a pillar-based manner. 
%
After that, CRAFT**Li et al., "CRAFT: A Novel Framework for Fusion of Radar and Camera Features in 3D Object Detection"** further considers the spatial properties of radar and camera sensors and designs a proposal-level early fusion framework.
%
RCBEV**Wang et al., "RCBEV: A Novel Method for Fusion of Radar and Camera Features in Bird's Eye View Space"** introduces the feature-level fusion in the BEV space for a unified feature representation. 
%
Meanwhile, RCM-Fusion**Li et al., "RCM-Fusion: A Novel Framework for Fusion of Radar and Camera Features in 3D Object Detection"** is proposed to combine radar and camera features at both the feature and instance levels, further improving the detection performance. 
%
More recently, CRN**Hou et al., "CRN: A Novel Method for Transformation of PV Image Features to BEV with Radar Occupancy"** transforms PV image features to BEV with radar occupancy to compensate for the depth information in images. 
%
RCBEVDet**Wang et al., "RCBEVDet: A Novel Method for Detection of 3D Objects from Radar and Camera Fusion"** specifically customizes a feature extractor for radar and uses RCS as the object size prior. 
%
It further designs a Cross-Attention Multi-layer Fusion module for robust radar-camera feature alignment and fusion.

By contrast, focusing on the framework robustness, our RobuRCDet proposes a 3DGE module to decrease the impact of potential noisy points in the radar voxels. 

{\flushleft \textbf{Robust 3D Object Detection.}} Sensor noise is one of the most significant factors causing the decrease in detection performance during inference for 3D object detection. 
%
Several methods**Meng et al., "Benchmarking Common Corruptions in 3D Perception Tasks"** attempt to benchmark the common corruptions in 3D perception tasks from different angles. 
For instance, RoboDepth**Li et al., "RoboDepth: A Benchmark for Monocular Depth Estimation Under Real-World Conditions"** sets up a benchmark to assess the robustness of monocular depth estimation in the presence of corruptions. On the other hand, RoboBEV**Hou et al., "RoboBEV: A Novel Benchmark for Evaluation of 3D Object Detection and Semantic Segmentation"** presents an extensive benchmark aimed at evaluating the robustness across four BEV perception tasks, \textit{i.e.}, 3D object detection**Meng et al., "Benchmarking Common Corruptions in 3D Perception Tasks"** and semantic segmentation**Hou et al., "RoboBEV: A Novel Benchmark for Evaluation of 3D Object Detection and Semantic Segmentation"**.
%
At the same time, Robo3D**Li et al., "Robo3D: A Benchmark for Evaluation of 3D Detectors and Segmenters Under LiDAR-Related Corruptions"** evaluates the resilience of 3D detectors and segmenters when exposed to LiDAR-related corruptions. However, these benchmarks mostly focus on camera or lidar perceptions, but the radar corruptions are almost ignored.

Furthermore, the efforts**Meng et al., "Benchmarking Common Corruptions in 3D Perception Tasks"** are made to address the above-mentioned corruptions and achieve robust 3D object detection under noisy conditions.
%
However, these methods only model partial radar noise types. Further, they only consider radar or LiDAR degradation scenarios and overlook camera failure cases. 
%
In contrast, our RoboRCDet addresses these issues and designs a module for the robust fusion of radar and camera features in the BEV view.




\begin{figure}
    \centering
    \includegraphics[width=14cm]{figure/noisepoints.pdf}
    \caption{\textbf{Point cloud visualization of radar signals with noise.} The \textcolor{blue}{blue points} refer to the ground truth radar points, and the \textcolor{red}{red points} represent noisy radar points in various conditions. Additionally, the \textcolor{lightblue}{light blue points} in the key-point missing part denote the eliminated ground truth radar points.
}
\label{noisypoint}
\end{figure}