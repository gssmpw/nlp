
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

%\usepackage{hyperref}
\usepackage[pagebackref=true,breaklinks,colorlinks,citepcolor=blue,citecolor=blue,linkcolor=blue,urlcolor=blue,hypertexnames=false]{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{wrapfig}

\usepackage{multirow}
\usepackage{multicol}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{array}
\usepackage{lipsum}
\usepackage{tabularx} 
\usepackage{xcolor}  % 引入xcolor包
\definecolor{lightblue}{RGB}{173,216,240}  % 定义lightblue颜色
\usepackage{color, colortbl}
% \definecolor{Gray}{gray}{0.9}
\newcommand{\gray}{\rowcolor[gray]{.95}} % gray tabl
\newcommand{\modelname}{{RobuRCDet}}
%MH: use \modelname{}

\title{RobuRCDet: Enhancing Robustness of Radar-Camera Fusion in Bird's Eye View for 3D Object Detection}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.  Funding acknowledgements go at the end of the paper.} \\
Department of Computer Science\\
Cranberry-Lemon University\\
Pittsburgh, PA 15213, USA \\
\texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
\And
Ji Q. Ren \& Yevgeny LeNet \\
Department of Computational Neuroscience \\
University of the Witwatersrand \\
Joburg, South Africa \\
\texttt{\{robot,net\}@wits.ac.za} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.
\renewcommand{\arraystretch}{1.5}
\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\lzw}[1]{{\color{magenta}{#1}}} % added by zhiwei
% \newcommand{\yjt}[1]{{\color{blue}{YJT: #1}}} % added by jingtong
%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}

\appendix
\section{Overview}
\label{A}
This supplementary material provides additional details
of architecture, and qualitative and quantitative experimental results. We describe implementation details for experiments in the main paper
(Section \ref{B}). We further provide additional experimental results on noisy training sets
(Section \ref{C}) and qualitative results (Section \ref{D}).
\section{Implementation Details}
\label{B}

This section provides some experimental settings and network details in the main paper.

First, for the network detail and hyper-parameters,  we employ SECONDFPN~\citep{yan2018second} to concatenate output feature maps at stride 16 and let the output depth bins of the depth distribution network to be 112 with a depth range of [2.0, 58.0]m and bin size to be 0.5m.
For the CMCA module, we use the multi-scale deformable attention implementation from MMCV~\citep{contributors2018mmcv} and set the number of attention heads to 8 and sampling points to 2.
We set the radar point range as [-51.2, 51.2]m and make the BEV feature map $128 \times 128$.

Second, for the noisy training set, we set the ratio of clean images to noisy images in the training set to 8:2. For the noisy images, we randomly synthesize one of the four types of proposed radar corruptions, with the intensity of the noise also being random. Additionally, we synthesize harsh weather or low-light conditions on the images corresponding to the timestamps with noise. 

In addition, the corruption levels of Spurious Points, Point Shifting, and Non-positional Disturbance are determined by $\sigma$, while the number of missing beams determines the noise level of Key-point Missing. Furthermore, weather degradation is also classified into levels; for example, rain and fog can be divided into light or heavy, whereas snow does not have a classification. Additionally, the low-light level at night is determined by the gamma coefficient, with a mild low-light coefficient set to 1.0-2.0 and a heavy set to 2.0-3.0.

\section{Analysis on the noisy Training set.}
\label{C}
Table \ref{table:trainwnoise} illustrates the results of CRN and \modelname~ which are trained on the noisy dataset and tested on each corruption. ResNet-18 is used as the backbone. In this table, C0 denotes the clean testing set and it is notable that \modelname~ achieves the same performance (54.4 NDS and 44.9 mAP) as the model trained on the clean dataset and surpasses CRN by 1.6 NDS and 1.4 mAP. Furthermore, C1 to C4 represent Spurious Point, Non-positional Disturbance, Key-point Missing, and Point Shifting. 

According to Table \ref{table:trainwnoise} and the comparison in Table \ref{table:corrupresults}, performance is improved when processing noisy radar point clouds after training with the corruption training set. Additionally, to ensure fairness in the experiments, we train and test both the \modelname~and CRN methods on the disturbed dataset and compared their performance. It is clear that our method still demonstrates better robustness than CRN with 2.4 NDS and 1.9 mAP improvement in C1 with $\sigma=10$.
\begin{table}
    \centering
    \fontsize{8}{8}\selectfont
\caption{\textbf{Validation of models trained with noisy training sets.} We augment the image data and radar data to form the training set. \* denotes that the model is retrained.} 
\vspace{3pt}
\setlength{\tabcolsep}{2mm}{
\begin{tabular}{c cccc c c ccc }
    \hline
    \multicolumn{2}{c}{Corruption} & \multicolumn{4}{c}{CRN$^{*}$} & \multicolumn{4}{c}{\modelname} \\
    \cmidrule(lr{0.5em}){1-2}\cmidrule(lr{0.5em}){3-6} \cmidrule(lr{0.5em}){7-10} 
    % \cline{2-9}
    Type&level & NDS$\uparrow$ & mAP$\uparrow$ & mATE$\downarrow$& mAP (Car) $\uparrow$&NDS$\uparrow$ & mAP$\uparrow$ & mATE $\downarrow$& mAP (Car)$\uparrow$\\
    \hline
    % (noise)&C+R&R18&256$\times$704&54.4&45.4&0.517&0.29&0.56&0.304&0.178\\
    C0
    &- &52.8&43.5&0.550&69.6&54.4&44.9&0.517&70.9\\
    \hline
    \multirow{2}{*}{C1}
    &1 &52.1&42.9&0.553&69.1&54.0&44.7&0.524&70.6\\
    &10 &51.2&42.4 &0.560&68.4 &53.6&44.3&0.532&70.1\\
    \hline
    \multirow{2}{*}{C2}
    &1 &51.6&42.6&0.557&69.2&53.4&44.1&0.539&70.1 \\
    &10 &50.5&41.1&0.568&68.1&52.6&42.9&0.547&69.7 \\
    \hline
    \multirow{2}{*}{C3}
    &8 &52.3&43.0 &0.551&69.2&54.1&44.6&0.537&70.2 \\
    &10 &52.0& 42.3&0.569& 68.8&53.8&44.0&0.642&69.9 \\
    \hline
    \multirow{2}{*}{C4}
    &1 &41.6&35.2&0.668&55.4 &45.1&36.9&0.637&56.8 \\
    &10 &33.4&28.0&0.750&41.5&36.7&31.2&0.699&47.0 \\
    \hline
    % \hline
    % \multirow{2}{*}{Rainy}&light && && && \\
    % &heavy && && && \\
    %  \hline
    % \multirow{2}{*}{Foggy}&light &&&&&& \\
    % &heavy && && && \\
    %  \hline
    % \multirow{2}{*}{Night}&light &&  && \\
    % &heavy & && \\
    %  \hline
    %  Snowy& - & && \\
    % \hline
    % Rainy& real &53.7&44.2 &0.563&70.1 && \\
    % \hline
    % Night& real &53.7&44.2 &0.563&70.1 && \\
    % \hline
    
\end{tabular}}%
\label{table:trainwnoise}
\end{table}
\section{Additional Visual Results.}
\label{D}
In this section, we present more synthesized noisy images. Notably, to better simulate real scenarios, we ensure that the degradation types and levels for multiple cameras at the same timestamp are the same. Figure \ref{wea} and Figure \ref{night} showcase the synthesized images of rainy days, snowy days, and nighttime mentioned in the main paper.
% \vspace{-30pt}
\begin{figure}
    \centering
    \includegraphics[width=14cm]{ICLR 2025 Template/figure/wea.pdf}
    \caption{\textbf{Visualization of synthesized challenging weather images.} Two levels of rainy images and a set of snowy images are displayed.
}
\label{wea}
\end{figure}
%
%
%
\begin{figure}
    \centering
    \includegraphics[width=14cm]{ICLR 2025 Template/figure/dark.pdf}
    \caption{\textbf{Visualization of synthesized challenging light conditions.} Two levels of low-light images are displayed.
}
\label{night}
\end{figure}
%
%
%








































\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\end{document}

