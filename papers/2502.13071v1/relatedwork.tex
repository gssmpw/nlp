\section{Related Work}
\label{sec:related_work}
%
{\flushleft \textbf{Camera-based 3D Object Detection.}}  
The success of 2D object detection~\citep{2d1,2d2}, with the growing demand for 3D perception in fields like autonomous driving and robotics, prompts the development of 3D object detection technology. 
% Thanks to the emergence of large-scale multi-view camera datasets~\citep{caesar2020nuscenes, waymo}, 
Early works~\citep{bevdepth, bevdet, bevdet4d} are based on multi-view cameras which leverage multi-view information through cross-view interaction to improve 3D object detection performance. 
%
The multi-view 3D object detection methods can be briefly divided into two categories, \textit{i.e.}, dense BEV-based and sparse query-based methods. 

Numerous dense BEV-based methods adopt Lift-Splat-Shoot (LSS)~\citep{lss} to transform 2D features into BEV features, such as BEVDet~\citep{bevdet}. 
%
On the other hand, BEVDepth~\citep{bevdepth} designs a trustworthy depth estimation module for better view transformation in the BEV space. 
%
For sparse query-based methods, 
%
PERT series~\citep{petr, petrv2, focalpert, streampert} incorporate the position information of 3D coordinates into image features and integrate long-term temporal fusion.


While these methods achieve advanced 3D object detection performance, they overlook robustness, a key factor in real-world applications. Unlike millimeter-wave radar, camera images are prone to interference in darkness and bad weather, leading to poor detection performance.
%
To this end, \modelname~incorporates the camera signal confidence map to effectively enhance network robustness along with radar modality under various conditions.
{\flushleft \textbf{Radar-Camera 3D Object Detection.}}  
The camera sensor inherently lacks 3D depth information, limiting its 3D detection accuracy. 
%
To alleviate this issue, researchers propose incorporating the cost-effective radar sensor into the 3D detection framework.
%
The radar sensor provides the 3D depth prior and additional Doppler velocity, compensating for the camera sensor's weaknesses. 

Specifically, CenterFusion~\citep{centerfusion} uses a key point detection network to obtain center points and then associates key points with the corresponding radar detection results in a pillar-based manner. 
%
After that, CRAFT~\citep{craft} further considers the spatial properties of radar and camera sensors and designs a proposal-level early fusion framework.
%
RCBEV~\citep{rcbev} introduces the feature-level fusion in the BEV space for a unified feature representation. 
%
Meanwhile, RCM-Fusion~\citep{rcm} is proposed to combine radar and camera features at both the feature and instance levels, further improving the detection performance. 
%
More recently, CRN~\citep{crn} transforms PV image features to BEV with radar occupancy to compensate for the depth information in images. 
%
RCBEVDet~\citep{lin2024rcbevdet} specifically customizes a feature extractor for radar and uses RCS as the object size prior. 
%
It further designs a Cross-Attention Multi-layer Fusion module for robust radar-camera feature alignment and fusion.

By contrast, focusing on the framework robustness, our RobuRCDet proposes a 3DGE module to decrease the impact of potential noisy points in the radar voxels. 

{\flushleft \textbf{Robust 3D Object Detection.}} Sensor noise is one of the most significant factors causing the decrease in detection performance during inference for 3D object detection. 
%
Several methods~\citep{kong2023benchmarking,  kong2023robo3d, xie2023robobev, kong2023robodepth} attempt to benchmark the common corruptions in 3D perception tasks from different angles. 
For instance, RoboDepth~\citep{kong2023robodepth, ren2022benchmarking} sets up a benchmark to assess the robustness of monocular depth estimation in the presence of corruptions. On the other hand, RoboBEV~\citep{xie2023robobev} presents an extensive benchmark aimed at evaluating the robustness across four BEV perception tasks, \textit{i.e.}, 3D object detection~\citep{bevfusion, liang2022bevfusion} and semantic segmentation~\citep{zhou2022cross}.
%
At the same time, Robo3D~\citep{kong2023robo3d} evaluates the resilience of 3D detectors and segmenters when exposed to LiDAR-related corruptions. However, these benchmarks mostly focus on camera or lidar perceptions, but the radar corruptions are almost ignored.

Furthermore, the efforts~\citep{sparseinteraction, robust} are made to address the above-mentioned corruptions and achieve robust 3D object detection under noisy conditions.
%
However, these methods only model partial radar noise types. Further, they only consider radar or LiDAR degradation scenarios and overlook camera failure cases. 
%
In contrast, our RoboRCDet addresses these issues and designs a module for the robust fusion of radar and camera features in the BEV view.




\begin{figure}
    \centering
    \includegraphics[width=14cm]{figure/noisepoints.pdf}
    \caption{\textbf{Point cloud visualization of radar signals with noise.} The \textcolor{blue}{blue points} refer to the ground truth radar points, and the \textcolor{red}{red points} represent noisy radar points in various conditions. Additionally, the \textcolor{lightblue}{light blue points} in the key-point missing part denote the eliminated ground truth radar points.
}
\label{noisypoint}
\end{figure}