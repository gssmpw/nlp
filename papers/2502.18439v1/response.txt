\section{Related Work.}
Please see \Cref{appendix:deferred_literature} for more comprehensive discussions about the related works.
\paragraph{Multi-Agent Reinforcement Learning. }
Various algorithms have been proposed to address multi-agent reinforcement learning (MARL) **Bansal, "Multi-Agent Deep Deterministic Policy Gradient"**__**Foerster, "Learning to Communicate with Multiple Agents"**__, and value function factorization techniques such as QMIX and VDN **Rashid, "QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent RL"**__. In the context of language models and collaborative debating we focus on, MARL takes on a particular and unique form. Here, each agent's state is represented by the sequence of previous responses from all the agents, with each agent deciding the next token based on this history. LLMs provide compact state representations through their hidden layers, enabling the use of long debate histories. 

\paragraph{Multi-Agent Collaboration with LLMs. }
An array of studies have explored effective collaboration frameworks among multiple large language model agents to solve complex tasks**Maddison, "Simple Neural Logic Reasoning"**. For example, ``role-playing''-based approaches utilized multi-agent LLMs by assigning a specific role to each LLM**Brown, "Language Models as Zero-Shot Learners"**, and ``multi-agent debate''-based approaches prompted  each LLM agent to solve the task independently and then discuss **Rae, "Combining Capacitors: Regularisation Techniques for Optimising Transformer-Based Language Models"**. 
In a debate, the agents reason through each other's answers to converge on a consensus  response, which may improve the factual accuracy, mathematical ability, and reasoning capabilities of the LLM **Chen, "A Framework for Evaluating Explainable AI Systems"**. Similar mult-agentic frameworks include voting **Henderson, "Deep Stochastic Multi-Agent Learning from Demonstrations"**, group discussions **Lowe, "Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments"**, and negotiating **Babes-Boly, "Cooperative Inverse Reinforcement Learning with Deep Neural Networks"**. However, all of these frameworks rely heavily on prompt engineering, which may lead to sub-optimal results **Lewis, "A Review of Multi-Agent RL Frameworks"**, and do not consider {\it training} LLMs specifically for collaboration. Therefore, while 
multi-LLM systems seem promising at the first glance, their performance may be limited when using the  out-of-the-box  (pretrained)  LLM with only prompt tuning, which highlights the need for {\it training} for better multi-agent collaboration. Recently, **AISTATS20**, introduced a training framework for accepting or rejecting persuasion in multi-agent systems. Additionally, {very recently,} 
**Li et al., "Multi-Agent RL from Human Feedback with Iterative SFT"**, and  focused on training the entire multi-agent systems using iterative SFT.
In contrast,  \ourstwo~employs (multi-agent) RL to train the whole multi-LLM system.



\paragraph{RL for LLM Training. }
RL has been widely used in post-training  LLMs, e.g., for improving  factuality**Khattab et al., "Improving Multi-Sentence Conversations with Joint Masked Language Modeling"**, code generation**Chang et al., "CodeBERT: Pre-trained Contextualized Embeddings for Code Understanding and Generation"**, and more recently and significantly, reasoning **Jain et al., "Learning to Reason in a Physical World"**. One prevalent approach of RL for LLM training is RL from human feedback (RLHF) 
**Kaplan, "What Does RLHF Really Do? An Analysis of RL from Human Feedback"**. 
RL offers a smooth generalization to the {\it multi-turn} setting  based on the Markov decision process (MDP) model, and there have been attempts to apply multi-turn RL for LLM training, such as RLHF for multi-turn model training to enhance the dialogue abilities**Li et al., "Multi-Turn Dialogue Systems with Deep Reinforcement Learning"**, or deriving multi-turn RL objective for the improvement of mathematical reasoning**Zhang et al., "Mathematical Reasoning with Multi-Turn RL"**. However, the major difference from  our work is that, these works did not consider multi-agent settings for collaboration. Recently, **Meng et al., "Self-Correcting Language Models with RL-based Approach"**, enhanced LLMs' ability to self-correct using an RL-based approach. Our framework can accommodate this case by using a single agent in  \ourstwo.