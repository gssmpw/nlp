\section{Deferred Details of the  Verifier Models}
\label{appendix:verifier}
For a reasoning question $q$, the trained verifiers (reward models) assess the correctness of a complete solution path $s$, denoted as \(p(s \text{ is correct} \mid q)\) \citep{cobbe2021training, uesato2022solving, lightman2023let}. These reward models can either focus on the final outcome (outcome reward models) or provide step-by-step evaluations (process reward models). Although the latter generally yields better performance \citep{lightman2023let}, the limited availability of process-level annotated datasets—especially for challenging benchmarks like ANLI \citep{nie2019adversarial}—restricts its applicability. Additionally, while generating detailed trajectories for process supervision (as seen in \citet{wang2024math}) can be effective, our primary goal is not to enhance the language model's domain specificity. Consequently, we chose to adopt a simpler strategy by training a verifier based on a well-tuned output reward model. 

\paragraph{Verifier Models Structure.}
We used a quantized version of a language model as the backbone for the verifier. Additionally, we incorporated a linear head layer followed by a softmax layer to ensure that the verifier’s output falls within the range of 0 to 1. The default backbone model is Microsoft Phi-3-mini-128k-instruct \citep{abdin2024phi}. In experiments involving different model training setups (see \Cref{sec:exp4}), we employed a new verifier with a different base model, specifically the one used in \Cref{sec:exp4}. In these cases, we utilized Qwen2.5-3B-instruct \citep{yang2024qwen2} and Llama-3-8B-instruct \citep{dubey2024llama} as alternative backbone models.

\subsection{Training Procedure}
To train the verifier model, we generate tuples \( (q_i, s_{ij}, a_{ij}) \) for \( i \in [Q] \) and \( j \in [S] \), where \( q_i \) is the question, \( s_{ij} \) is one of the \( S \) generated solutions for question \( q_i \) generated by the base model of verifier model, and \( a_{ij} \) is the corresponding answer for \( (q_i, s_{ij}) \).  We label the token-level subsequences \( (q_i, s_{ij}^{1:x}) \) for \( x \leq \texttt{sequence length of} s_{ij} \) as \( y_{ij} = 1 \) if \( a_{ij} \) is correct, and \( y_{ij} = 0 \) if \( a_{ij} \) is incorrect. 

For the mathematical reasoning task, we utilized the GSM8K dataset \citep{cobbe2021training}, specifically the training set consisting of 7,463 questions, to generate 100 reasoning paths for each questions. For the natural language inference task, we employed the ANLI dataset \citep{nie2019adversarial}, using first 10,000 questions to generate 50 reasoning paths. The trajectories were evaluated based on their outcomes, and we excluded outputs that did not adhere to the required formatting. Specifically, we ensured that the language model first provided reasoning before presenting the final answer in the format \textbackslash\textbackslash boxed\{\}.

In our approach, we ensured that each question in the GSM8k dataset had a balanced set of reasoning paths. Specifically, if a question’s 100 reasoning paths contained at least 20 correct and 20 incorrect responses, we randomly selected 20 of each. However, when there were insufficient correct or incorrect paths, we augmented the data by generating additional paths using reference examples. For instance, if no correct reasoning path was available, we provided a correct example from the GSM8k dataset, and if incorrect paths were missing, we guided the language model to produce a response containing a trivial error. Ultimately, each GSM8k question was assigned 20 correct and 20 incorrect reasoning paths. For the ANLI dataset, we applied a similar procedure by starting with 50 reasoning paths per question, from which we randomly sampled 10 correct and 10 incorrect paths, supplementing the data as needed. Throughout this process, \textbf{we minimized reliance on the original reasoning paths} in the dataset since a) to enhance the overall diversity and quality of the generated data and b) to minimize the dependency on the reasoning path in the dataset. 

Next, we applied binary cross-entropy loss at the token level, aiming to minimize  
\[
\min_{\theta} \sum_{i,j,x} \left( y_{ij} \log \texttt{Verifier}_{\theta}(q_i, s_{ij}^{1:x}) +  (1-y_{ij}) \log (1-\texttt{Verifier}_{\theta}(q_i, s_{ij}^{1:x})) \right)
\]
where \( i \) denotes the question index, \( j \) represents the generated solution index, and \( t \) is the token index. By default, we utilized all solution tokens for optimization; however, in practice, focusing on the latter half of the generated solution tokens yielded better results.

For model training, we used QLoRA~\citep{dettmers2024qlora} with hyperparameters \( r=16 \) and \( \alpha=32 \). We used a training batch size of 2 and optimized the model using the AdamW \citep{loshchilov2017decoupled} optimizer with \(\beta_1 = 0.9\), \(\beta_2 = 0.95\), and a learning rate of \( 2 \times 10^{-4} \).

\subsection{Verifier Performance}
We report the performance of the verifier in \Cref{table:verifier-perf}.

\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|}
\hline
              & GSM8k & ANLI  \\ \hline
Accuracy      & 0.91  & 0.92   \\ \hline
\end{tabular}
\caption{Performance of the verifier on different benchmarks. Accuracy is reported for GSM8K and ANLI. Notably, the verifier demonstrates higher accuracy in evaluating the correctness of answers compared to the accuracy of the LLM in generating correct answers. The verifier is classified as correct if the assigned reward is greater than 0.5 when the LLM-generated solution is correct, or if the reward is less than 0.5 when the LLM-generated solution is incorrect.}
\label{table:verifier-perf}
\end{table}

\subsection{Other  Observations} 

We experimented with various verifiers built upon different language model bases. Our first observation was that training the model using only the final answer did not perform as well as minimizing the cross-entropy loss over the last half of the generated tokens. Second, the verifier produced interpretable results, aligning with findings from \citet{liu2023tinygsm}. Lastly, when we used training samples from one base model but trained the verifier with a different base model as the backbone, the loss did not decrease, indicating that using the same base model for training is crucial for effective learning.


\subsection{Proof of \Cref{obs:token-wise-training}}
\begin{restatable}{theorem}{token}
\label{obs:token-wise-training}
Assuming the verifier model is sufficiently expressive, the optimal parameter \( \theta^\star \) that minimizes the expected cross-entropy loss between the true label and the verifier's output will satisfy  
\[
\texttt{Verifier}_{\theta^\star}(q, s^{1:x}) = \PP(\text{Final answer is correct} \mid q, s^{1:x}).
\]
\end{restatable}
\begin{proof}
The expected loss can be written as
\[
   \mathcal{L}(\theta)
   =
   \mathbb{E}_{q, s, x, y}
   \Big[
     y \log \texttt{Verifier}_{\theta}(q, s^{1:x})
     +
     (1-y) \log (1 - \texttt{Verifier}_{\theta}(q, s^{1:x}))
   \Big].
\]
Defining $p_{\theta}(q, s^{1:x}) := \texttt{Verifier}_{\theta}(q, s^{1:x})$, we compute the partial derivative with respect to \( p_{\theta}(q', s^{1:x'}) \):
\[
     \mathbb{E}_{q, s, x, y} \Bigl[
       \mathbf{1}\bigl(q=q', s^{1:x}=s^{1:x'}\bigr)
         \Bigl(\frac{y}{p_{\theta}(q,s^{1:x})}
               -
               \frac{1-y}{1 - p_{\theta}(q,s^{1:x})}
         \Bigr)
     \Bigr],
\]
so we conclude 
\[
     \texttt{Verifier}_{\theta^\star}(q, s^{1:x}) = \mathbb{E}[y \mid q, s^{1:x}] = \PP(\text{Final answer is correct }\mid q, s^{1:x})
\]
since we assumed that the verifier model is sufficiently expressive.
\end{proof}
{It is worth noting that while \citet{yu2023outcome} provided a similar analysis using an \( \ell_2 \)-loss function, we extend the analysis to the entropy loss function, which is commonly used in classification tasks.}
