\begin{abstract}
Leveraging multiple large language models (LLMs) to build collaborative  {\it multi-agentic} workflows has demonstrated significant potential. However, most previous studies focus on {\it prompting}  the out-of-the-box LLMs, relying on their innate capability for collaboration, which may not improve LLMs' performance as shown recently.
In this paper, we introduce a new {\it post-training} paradigm {\bf \ours} ({\bf M}ulti-{\bf A}gent {\bf P}ost-c{\bf o}-training for collaborative LLMs with {\bf R}einforcement {\bf L}earning), to explicitly elicit the collaborative behaviors and further unleash the power of multi-agentic LLM frameworks. 
In {\bf \ours}, multiple LLMs first generate their own responses independently and engage in a multi-turn discussion to collaboratively improve the final answer. 
In the end,  a  {\bf \ours} verifier evaluates both the answer and the discussion, by assigning a score that verifies the correctness of the  answer,  while  adding incentives to encourage corrective and persuasive discussions. The score serves as the co-training reward, and is then maximized through multi-agent RL.
Unlike existing LLM post-training paradigms, {\bf \ours} advocates the {\it co-training} of multiple LLMs together using {\it RL} for better generalization. Accompanied by analytical insights, our experiments demonstrate that training individual LLMs alone is insufficient to induce effective collaboration. In contrast, multi-agent co-training can boost  the collaboration performance across benchmarks, with generalization to unseen domains.



\end{abstract}