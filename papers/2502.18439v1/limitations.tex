\section*{Limitations}
Since we use instruction prompts as inputs to the LLMs, the output can vary significantly depending on the prompts. As the first methodology paper, our experiments are conducted on relatively small LLMs (3B to 8B parameters) for fast iteration, and the observed behaviors may differ on larger models.  
After all turns of multi-LLM interactions, we apply majority voting to determine the final answer. Using alternative mechanisms, such as a manager agent that makes the final prediction based on the responses from multiple agents, may further improve the overall performance. 

\section*{Potential Risks}
As our proposed approach encourages and facilitates collaboration among multiple LLM agents, when adversarial or malicious agents exist, our method could lead to unintended harmful outcomes by enabling their collaboration with others.