\safevspace{-4mm}
\section{Introduction}




\safevspace{-2.2mm}
Recent advances in large language models (LLMs) have highlighted their potential for collaboration, particularly within the {\it multi-agentic} framework  \citep{du2023improving, li2023camel, kim2024adaptive}. The shift from single-agent to multi-agent systems introduces new dimensions and challenges in enabling effective collaboration among LLM agents. 
Recent approaches to  multi-LLM  collaboration mostly rely on 
{{\it prompting} {\it pre-trained} models. However,} such approaches struggle with achieving genuine collaboration among the agents. For example, multi-agent debate does not consistently lead to improved performance with additional turns \citep{huang2023large}.


This limitation may be somewhat expected -- while LLMs are able to {\it simulate} collaboration procedures, they were  {\it not} explicitly {\it trained} to achieve effective cooperation.
{In theory, it is not hard to imagine that single-agent training is insufficient for collaboration -- an {\it untrained} and {\it non-strategic} opponent can fail to act in a way that promotes  collaboration. 
Instead,} 
achieving collaborative behaviors requires interactive training environments where each agent actively engages with others, and dynamically optimizes the strategy \citep{gagne1974instruction,macy1991learning, hertz2013learning}. Moreover, conventional approaches such as supervised fine-tuning (SFT), as we will show, are inadequate for this purpose, either:  
merely mimicking multi-agent interactions from training data may not lead to effective collaboration. 

To develop more effective collaborative agents, we propose {\bf M}ulti-{\bf A}gent {\bf P}ost-c{\bf o}-training for collaborative LLMs with {\bf R}einforcement {\bf L}earning (\textbf{\ours}), a {\it co-training} paradigm for multiple LLMs using multi-agent reinforcement learning (MARL). In \ourstwo, within the  pre-defined frameworks for multi-agent collaboration ({e.g.}, the debate framework \citep{du2023improving}), each agent receives rewards for their responses during collaboration, based on the quality of their answers and interactions. The objective for each agent in  \ourstwo~ is to maximize their own value function, defined as the expected cumulative sum of rewards over the course of the collaboration.

To further encourage collaboration in \ourstwo, we incorporate incentives for successful interactions and penalties for failures in collaboration, steering the LLMs    towards more effective and cooperative behaviors. Through a simplified game-theoretic  analytical  example, we validate the following insights: 1) single-agent training alone is insufficient to produce genuinely collaborative agents, and 2) co-trained agents can achieve an equilibrium with collaborative behaviors. 

To assess the effectiveness of \ourstwo, we conduct  experiments across diverse tasks and evaluation strategies. Specifically, we train multi-agent LLMs for tasks such as mathematical reasoning (GSM8k \citep{cobbe2021training}) and natural language inference (ANLI \citep{nie2019adversarial}), comparing their performance against baseline approaches. Additionally, we evaluate the robustness of our method by testing agents on out-of-domain tasks (e.g., training on a NLI task and evaluating on a math dataset), demonstrating the generalization capabilities of our approach. We also explore the collaboration among agents of varying capabilities, by analyzing the impact of training {\it heterogeneous} LLMs together.


To the best of our knowledge, this study is \textit{among the first works to explore the training of multi-LLM  systems as a whole}\footnote{Together with the contemporaneous works \citet{subramaniam2025multiagent} and \citet{zhao2025sirius},  both of which were released within the past month while preparing this paper.
In contrast to \ourstwo, the algorithms therein were based on (iterative) SFT, instead of RL.}, 
using RL,  for multi-LLM collaboration. 







\begin{figure*}[t]  %
  \centering
  \includegraphics[width=\textwidth]{figs/Multi-agent_System_fig-1.pdf}
  \safevspace{-8mm}
  \caption{
  \oursspace can be applied to any multi-{LLM}  system with a scorer/verifier. In the illustrated example, it is integrated into a collaborative debate system for mathematical problem-solving. LLMs generate responses based on the multi-agent system pipeline, and a scorer/verifier evaluates their outputs. The reward for each LLM is determined based on these scores, which may include both current and future pipeline evaluations. Multi-Agent RL is employed to maximize each agent's value function.}
  \safevspace{-0.75cm}
  \label{fig1:problem-setting}
\end{figure*}

\safevspace{-2.2mm}
\section{Related Work.}
Please see \Cref{appendix:deferred_literature} for more comprehensive discussions about the related works.
\paragraph{Multi-Agent Reinforcement Learning. }
Various algorithms have been proposed to address multi-agent reinforcement learning (MARL) \cite{hernandez2019survey,zhang2021multi},  including multi-agent Proximal Policy Optimization (PPO) \citep{yu2022surprising}, and value function factorization techniques such as QMIX and VDN \citep{rashid2020monotonic, sunehag2017value}. In the context of language models and collaborative debating we focus on, MARL takes on a particular and unique form. Here, each agent's state is represented by the sequence of previous responses from all the agents, with each agent deciding the next token based on this history. LLMs provide compact state representations through their hidden layers, enabling the use of long debate histories. 

\paragraph{Multi-Agent Collaboration with LLMs. }
An array of studies have explored effective collaboration frameworks among multiple large language model agents to solve complex tasks~\citep{wu2023autogen, li2024improving, zhao2024longagent}. For example, ``role-playing''-based approaches utilized multi-agent LLMs by assigning a specific role to each LLM~\citep{li2023camel}, and ``multi-agent debate''-based approaches prompted  each LLM agent to solve the task independently and then discuss \citep{du2023improving,khandebating}. 
In a debate, the agents reason through each other's answers to converge on a consensus  response, which may improve the factual accuracy, mathematical ability, and reasoning capabilities of the LLM \citep{du2023improving,liang2023encouraging, kim2024adaptive}. Similar mult-agentic frameworks include voting \citep{wang2023selfconsistency}, group discussions \citep{chen2024reconcile}, and negotiating \citep{fu2023improving}. However, all of these frameworks rely heavily on prompt engineering, which may lead to sub-optimal results \citep{huang2023large}, and do not consider {\it training} LLMs specifically for collaboration. Therefore, while 
multi-LLM systems seem promising at the first glance, their performance may be limited when using the  out-of-the-box  (pretrained)  LLM with only prompt tuning, which highlights the need for {\it training} for better multi-agent collaboration. Recently, \citet{stengel2024teaching} introduced a training framework for accepting or rejecting persuasion in multi-agent systems. Additionally, {very recently,} 
\citet{subramaniam2025multiagent} and \citet{zhao2025sirius} focused on training the entire multi-agent systems using iterative SFT.
In contrast,  \ourstwo~employs (multi-agent) RL to train the whole multi-LLM system.



\paragraph{RL for LLM Training. }
RL has been widely used in post-training  LLMs, e.g., for improving  factuality~\cite{tianfine}, code generation~\cite{le2022coderl}, and more recently and significantly, reasoning \cite{guo2025deepseek}. One prevalent approach of RL for LLM training is RL from human feedback (RLHF) 
\citep{ziegler2019fine, ouyang2022training,bai2022training, ahmadian-etal-2024-back}. 
RL offers a smooth generalization to the {\it multi-turn} setting  based on the Markov decision process (MDP) model, and there have been attempts to apply multi-turn RL for LLM training, such as RLHF for multi-turn model training to enhance the dialogue abilities~\citep{shani2024multi}, or deriving multi-turn RL objective for the improvement of mathematical reasoning~\cite{xiong2024building}. However, the major difference from  our work is that, these works did not consider multi-agent settings for collaboration. Recently, \citet{kumar2024training} enhanced LLMs' ability to self-correct using an RL-based approach. Our framework can accommodate this case by using a single agent in  \ourstwo.



