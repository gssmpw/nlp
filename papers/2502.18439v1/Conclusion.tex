\section{Conclusion}

In this paper, we have introduced \ours, a new post-training paradigm that leverages multi-agent RL to explicitly foster the collaboration among multiple LLMs. Unlike methods that rely solely on prompting or single-agent fine-tuning, \oursspace focuses on \textit{co-training} multiple LLMs, ensuring that each agent adapts its policy not just to immediate feedback, but also to the strategic behaviors of other agents over multiple interactive turns. By incorporating a verifier network for reward shaping with incentives, the framework guides each agentâ€™s responses that account for both short-term correctness and long-term collaborative potential, thus promoting collaborative discussions that lead to more accurate final answers. 

Through an extensive set of experiments on reasoning-intensive tasks -- such as GSM8K for mathematical problem-solving and ANLI for logical natural language inference -- our results demonstrate that off-the-shelf LLMs often do not improve the overall performance with additional debate turns. In contrast, \ours-trained agents show significant improvements with accuracy increasing as collaboration progresses. Crucially, these collaborative abilities are shown transferable across tasks, suggesting that once LLMs learn to collaborate, they can retain a generalizable ``collaboration skill'' applicable to different domains.
Furthermore, our experiments with heterogeneous LLMs highlight that \oursspace can also foster collaborative synergy even among models of varying capabilities.

While our study has revealed the promise of \oursspace for boosting LLM performance in multi-agent settings, several avenues for future work remain. First, exploring new reward-shaping strategies -- beyond verifier scores -- to incorporate factual consistency or domain-specific constraints may yield more significant improvement. Second, integrating more specialized collaboration protocols (beyond collaborative debate) may better align with real-world use cases that demand consensus or hierarchical decision-making among multiple LLMs. 






