\section{Related Work.}
Please see \Cref{appendix:deferred_literature} for more comprehensive discussions about the related works.
\paragraph{Multi-Agent Reinforcement Learning. }
Various algorithms have been proposed to address multi-agent reinforcement learning (MARL) ____,  including multi-agent Proximal Policy Optimization (PPO) ____, and value function factorization techniques such as QMIX and VDN ____. In the context of language models and collaborative debating we focus on, MARL takes on a particular and unique form. Here, each agent's state is represented by the sequence of previous responses from all the agents, with each agent deciding the next token based on this history. LLMs provide compact state representations through their hidden layers, enabling the use of long debate histories. 

\paragraph{Multi-Agent Collaboration with LLMs. }
An array of studies have explored effective collaboration frameworks among multiple large language model agents to solve complex tasks____. For example, ``role-playing''-based approaches utilized multi-agent LLMs by assigning a specific role to each LLM____, and ``multi-agent debate''-based approaches prompted  each LLM agent to solve the task independently and then discuss ____. 
In a debate, the agents reason through each other's answers to converge on a consensus  response, which may improve the factual accuracy, mathematical ability, and reasoning capabilities of the LLM ____. Similar mult-agentic frameworks include voting ____, group discussions ____, and negotiating ____. However, all of these frameworks rely heavily on prompt engineering, which may lead to sub-optimal results ____, and do not consider {\it training} LLMs specifically for collaboration. Therefore, while 
multi-LLM systems seem promising at the first glance, their performance may be limited when using the  out-of-the-box  (pretrained)  LLM with only prompt tuning, which highlights the need for {\it training} for better multi-agent collaboration. Recently, ____ introduced a training framework for accepting or rejecting persuasion in multi-agent systems. Additionally, {very recently,} 
____ and ____ focused on training the entire multi-agent systems using iterative SFT.
In contrast,  \ourstwo~employs (multi-agent) RL to train the whole multi-LLM system.



\paragraph{RL for LLM Training. }
RL has been widely used in post-training  LLMs, e.g., for improving  factuality____, code generation____, and more recently and significantly, reasoning ____. One prevalent approach of RL for LLM training is RL from human feedback (RLHF) 
____. 
RL offers a smooth generalization to the {\it multi-turn} setting  based on the Markov decision process (MDP) model, and there have been attempts to apply multi-turn RL for LLM training, such as RLHF for multi-turn model training to enhance the dialogue abilities____, or deriving multi-turn RL objective for the improvement of mathematical reasoning____. However, the major difference from  our work is that, these works did not consider multi-agent settings for collaboration. Recently, ____ enhanced LLMs' ability to self-correct using an RL-based approach. Our framework can accommodate this case by using a single agent in  \ourstwo.