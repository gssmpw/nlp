\section{Related Work.}
Please see \Cref{appendix:deferred_literature} for more comprehensive discussions about the related works.
\paragraph{Multi-Agent Reinforcement Learning. }
Various algorithms have been proposed to address multi-agent reinforcement learning (MARL) \cite{hernandez2019survey,zhang2021multi},  including multi-agent Proximal Policy Optimization (PPO) \citep{yu2022surprising}, and value function factorization techniques such as QMIX and VDN \citep{rashid2020monotonic, sunehag2017value}. In the context of language models and collaborative debating we focus on, MARL takes on a particular and unique form. Here, each agent's state is represented by the sequence of previous responses from all the agents, with each agent deciding the next token based on this history. LLMs provide compact state representations through their hidden layers, enabling the use of long debate histories. 

\paragraph{Multi-Agent Collaboration with LLMs. }
An array of studies have explored effective collaboration frameworks among multiple large language model agents to solve complex tasks~\citep{wu2023autogen, li2024improving, zhao2024longagent}. For example, ``role-playing''-based approaches utilized multi-agent LLMs by assigning a specific role to each LLM~\citep{li2023camel}, and ``multi-agent debate''-based approaches prompted  each LLM agent to solve the task independently and then discuss \citep{du2023improving,khandebating}. 
In a debate, the agents reason through each other's answers to converge on a consensus  response, which may improve the factual accuracy, mathematical ability, and reasoning capabilities of the LLM \citep{du2023improving,liang2023encouraging, kim2024adaptive}. Similar mult-agentic frameworks include voting \citep{wang2023selfconsistency}, group discussions \citep{chen2024reconcile}, and negotiating \citep{fu2023improving}. However, all of these frameworks rely heavily on prompt engineering, which may lead to sub-optimal results \citep{huang2023large}, and do not consider {\it training} LLMs specifically for collaboration. Therefore, while 
multi-LLM systems seem promising at the first glance, their performance may be limited when using the  out-of-the-box  (pretrained)  LLM with only prompt tuning, which highlights the need for {\it training} for better multi-agent collaboration. Recently, \citet{stengel2024teaching} introduced a training framework for accepting or rejecting persuasion in multi-agent systems. Additionally, {very recently,} 
\citet{subramaniam2025multiagent} and \citet{zhao2025sirius} focused on training the entire multi-agent systems using iterative SFT.
In contrast,  \ourstwo~employs (multi-agent) RL to train the whole multi-LLM system.



\paragraph{RL for LLM Training. }
RL has been widely used in post-training  LLMs, e.g., for improving  factuality~\cite{tianfine}, code generation~\cite{le2022coderl}, and more recently and significantly, reasoning \cite{guo2025deepseek}. One prevalent approach of RL for LLM training is RL from human feedback (RLHF) 
\citep{ziegler2019fine, ouyang2022training,bai2022training, ahmadian-etal-2024-back}. 
RL offers a smooth generalization to the {\it multi-turn} setting  based on the Markov decision process (MDP) model, and there have been attempts to apply multi-turn RL for LLM training, such as RLHF for multi-turn model training to enhance the dialogue abilities~\citep{shani2024multi}, or deriving multi-turn RL objective for the improvement of mathematical reasoning~\cite{xiong2024building}. However, the major difference from  our work is that, these works did not consider multi-agent settings for collaboration. Recently, \citet{kumar2024training} enhanced LLMs' ability to self-correct using an RL-based approach. Our framework can accommodate this case by using a single agent in  \ourstwo.