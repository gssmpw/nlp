\section{Conclusion}

In this work, we take the first steps toward benchmarking the ability of language models to create counterexamples for incorrect solutions. Focusing on algorithmic problem solving, we found that models often fail to detect errors in failed Codeforces submissions — even when given the correct solutions as guidance. We hope hillclimbing on our benchmark spurs effective methods for creating counterexamples using LMs, and deepens our understanding of how this capability relates to a models capacity for reasoning, reflection, and self-improvement. We hope there will be more benchmarks for counterexample creation, such as for research-level mathematics~\citep{bengio2024aimathematician, davies2021advancingmath, wagner2021constructionscombinatorics}. We are excited about methods that integrate formal tools such as SMT solvers to leverage their complementary strengths.  While our work focuses on claims and counterexamples verifiable through code execution — many scientific hypotheses are not easily formalized in this way. Designing evaluations that test a model’s ability to propose counterexamples based solely on natural language claims presents an exciting direction for future research.
