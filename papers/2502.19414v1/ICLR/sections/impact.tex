\section*{Impact Statement}

AI generated solutions, such as entire research papers~\citep{lu2024aiscientist}, risk overwhelming our infrastructure to refute them, following Brandolini's law~\citep{williamson2016take}. Our paper highlights the need for inverse benchmarks, that evaluate the ability of models to falsify subtly incorrect solutions, instead of the traditional paradigm of solving problems. Improved ability to falsify can help make AI outputs more reliable through reflection, which is important as edge-cases have long been the Achilles heel of deep learning driven deployments like self-driving. To emphasise that falsification can sometimes be harder than generation, we create \bench{}, which we will release publically. Progress on \bench{} could also help language modelsâ€™ ability to audit codebases for mistakes, bolstering software reliability in an increasingly digitized economy.

\section*{Author Contributions}

Shiven and Ameya conceived the project. Shiven led the experiments, and Shashwat helped design the benchmark. Shashwat and Shiven co-led the writing of the paper, with help from Ameya. PK provided helpful feedback throughout the project. Jonas, Matthias and Ameya advised the design of all experiments.

\section*{Acknowledgements}

Special thanks to Siddharth Bhat and Siddhartha Gadgil for inspiration about this direction and specifically for introducing us to formal verification and counterexample creation. The authors would like to thank (in alphabetical order): Nikhil Chandak, Hari Aakash K, Shyamgopal Karthik, Srija Mukhopadhyay, Alexander Panfilov, Vishaal Udandarao, Saujas Vaduguru. AP and MB acknowledge financial support by the Federal Ministry of Education and Research (BMBF), FKZ:011524085B and Open Philanthropy Foundation funded by the Good Ventures Foundation.