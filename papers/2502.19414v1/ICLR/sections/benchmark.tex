\section{\bench{} Benchmark}\label{sec:benchmark}

\begin{figure}[t]\vspace{-0.7cm}
    \centering
    \begin{minipage}{1\linewidth} % Adjust width as needed
        \centering
        \parbox[c]{0.45\linewidth}{\includegraphics[width=\linewidth]{figs/Funnel.pdf}}
        \parbox[c]{0.45\linewidth}{\includegraphics[width=\linewidth]{figs/tags.png}}
    \end{minipage}
    \caption{\textbf{(Left) Sample Filtering Pipeline.} We curated (problem, incorrect solution) pairs from Codeforces (Jan 2024–Jan 2025) where counterexamples are easy to verify but non-trivial to generate, using a 4-step filtering process (dynamically expandable over time). \textbf{(Right) Dataset Topic Coverage.} The dataset covers 35+ algorithmic topics, including many niche ones. Problems are multi-tagged—e.g., the high number of "greedy" problems indicates it’s one of the key concepts, not the only one.}
    \label{fig:method_prop1}
    \vspace{-0.5cm}
\end{figure}

Having formalized counterexample creation in algorithmic problem solving, we now describe the data collection process and features of \bench{}. 

\subsection{Data Collection}
\label{sec:collect}
We first discuss how we collect problems and corresponding incorrect codes that form the samples in \bench{}. The overall pipeline is described in Figure~\ref{fig:method_prop1}.

\textbf{Sourcing Problem Statements}. We first source 647 problems from Codeforces Division 1 and Division 2 contests held between January 2024 and January 2025, reflecting latest, competition-level programming tasks. We apply two filters on the problems: (i) We remove 152 (23\%) problems that require a non-trivial grading environment for any given input. Such problems are explicitly marked as involving interaction between the program and a grader, or allowing multiple correct outputs for a given input. (ii) Then, we remove 92 (18\% of the remaining) problems which are rated below 1200. The incorrect solutions here are more likely to have trivial implementation bugs as these problems do not require much algorithmic reasoning. We also obtain the correct solution for each problem as these are released officially by the problem authors in an editorial.   

\textbf{Picking Incorrect Solutions}. Next, we select incorrect submissions for each of the shortlisted problems, optimizing for ones where creating counterexamples might be more challenging and interesting. As we wish to test falsification and not implementation inefficiences, we filter to submissions marked as providing wrong answers, rather than those that violated time or memory constraints. For these incorrect submissions, we prioritize them using the following scoring function: $\text{score}(s) = h(s) \cdot M +  t(s) + 10\cdot b(s)$. Here, $h(s)$ indicates whether the submission was ``hacked'' post-contest after passing all test-cases designed by the problem authors. We strictly prioritize such submissions by adding a large constant $M=10^4$ to their score, as they form interesting instances of subtle incorrect solutions that slipped through initial tests, but human experts could create counterexamples for them under time-constraints. Next, $t(s)$ is the number of test cases the submission passed before giving a wrong answer. Finally, $b(s)$ takes a binary value that indicates whether the author is rated $\geq 2000$, acting as a bonus for expert written incorrect submissions.

\textbf{Filtering Trivial Samples}. Finally, we remove samples for which finding counterexamples is trivial in two ways. First, we wish to ensure that randomly generating test cases without reasoning about the incorrect code is not enough to find a counterexample. We provide Gemini 2.0 Flash Thinking just the problem statement, without the incorrect code, and prompt it to output a random test case generation code. We run this test case generator for up to one minute to check whether it can find any inputs where the incorrect code doesn't match the ground-truth solution. This is true for 58 (14\%) of the 403 problems. We filter these incorrect submissions.

Second, we found that 21 (6\%) submissions contain deliberately inserted code that produces incorrect outputs only when a specific, unlikely constant appears in the input. This is done perhaps as a way to maliciously bait for hacks in the contest, as hackers are rewarded with extra scores. For the results in our paper, an expert human evaluator went through the submissions and removed such samples. It is easy to automate this step as the malicious parts of such code are quite overt. We plan to utilise a language model by providing it the expert identified samples as demos, along with a rubric. 

\textbf{Final Dataset}. The \bench{} dataset contains 324 samples authored by 304 different programmers. The incorrect submission in 317 samples is written in C++. The remaining 7 are in Python, as C++ is far more popular in programming competitions due to its efficiency. Each sample consists of a unique problem statement with an incorrect code solution, and the correct solution along with an input validation script is available for evaluation. The corresponding lengths are summarized in Table~\ref{tab:sample-lengths}.

\subsection{Benchmark Features}

Our benchmark is constructed to have desirable features highlighted below.

\textbf{Allows arbitrary algorithmic generation of novel counterexamples}. We provide the language model a problem statement and incorrect solution, and ask it to output a code $\mathcal{A}_{out}$ that, when executed, outputs a counterexample input $x$.  The code $\mathcal{A}_{out}$ must complete its execution within 1 minute, and the LM is informed of this time limit in its prompt. Allowing the LM to output code allows it to generate counterexamples with varying complexity, ranging from a hard-coded input to complex functions that create the counterexample input. We score the counterexample as a success if the incorrect solution has a different output from the correct one, while matching constraints specified in the problem.

\textbf{Avoids Search and Training Data Leakage}. Codeforces does not publically reveal the full test cases that broke an incorrect submission on non-trivial cases, so models cannot directly find counterexamples on the internet. Further, to prevent indirect leakage from user discussions, we will dynamically update the claims as more contests on Codeforces occur, similar to LiveCodeBench~\citep{jain2024livecodebench}. This allows model comparisons by filtering to the subset of claims collected after the latest knowledge cutoff date among the models.  

\textbf{Diversity and Metadata}. Our benchmark spans 34 fundamental topics in algorithms as tagged by Codeforces (e.g.\ Greedy, Dynamic Programming, Graphs, etc.), shown in Figure~\ref{fig:method_prop1}. The problems range in difficulty from an Elo rating of 1200 to 3500, while the incorrect submissions are authored by programmers with expertise ranging from Elo 700 to 3800. Figure~\ref{fig:method_prop2} shows the distribution over problem rating, solver rating, and the test case number where the submission failed on CodeForces. We provide all these meta-data annotations for each sample in our benchmark, which may be helpful for future research.

\begin{figure}[t]
\vspace{-0.6cm}
    \centering
    \includegraphics[width=0.325\linewidth]{figs/tcpassed.png}
    \includegraphics[width=0.325\linewidth]{figs/probratings.png}
    \includegraphics[width=0.325\linewidth]{figs/authratings.png}
    \caption{\textbf{(Left)} Distribution of Test Cases Passed by Incorrect Solutions: The median test cases passed is 17, making the mistakes non-trivial. \textbf{(Center)} Distribution of Problem Ratings: The benchmark spans a range of difficulty levels. \textbf{(Right)} Distribution of Incorrect Solution Author Ratings: Preference was given to expert authors, rated above 2000, resulting in a noticeable peak.}
    \vspace{-0.5cm}
    \label{fig:method_prop2}
\end{figure}
