\section{More Details About Baselines}\label{sec:baselines_appendix}

\subsection{Random Search}
\label{subsec:rand_search_baseline}
This method is motivated by the observation that it is often trivial to solve algorithmic problems under smaller constraints, potentially allowing suboptimal efficiency. For instance, consider the task of finding a minimum spanning tree of a connected graph. A trivial solution is to try all possible subsets of edges. Among all such subsets, we pick one which retains the connectivity of the graph while minimising the sum of edge weights. This solution doesn't require any elaborate reasoning and follows directly from the definition of an MST. Notice that this is much slower than optimal solutions like Kruskal's algorithm \citep{kruskal1956shortest}, which run in $\mathcal{O}(E\log{E})$ instead of our inefficient $\mathcal{O}(E2^E)$. However, this inefficient solution can execute within a few seconds for smaller graphs, with say around 20 edges, and provide valuable ground-truth outputs for arbitrary (small) inputs. This can be then used to check the correctness of the efficient but buggy solution. It is still possible for a model to write an incorrect solution or to make a test-case generator that prints invalid inputs to the problem. In such cases, the test-case discovered by this random testing will fail the subsequent evaluation. The model also needs to ensure that the random test-cases generated by its script are small enough that its own correct solution can finish execution in time.

We prompt the model to output two pieces of code: (1) a randomised test-case generator following the input constraints of the problem, and (2) an inefficient solution to the problem. This could be simple brute-force enumeration of all possibilites for small constraints, and finding the optimal solution using them. After obtaining these, we repeatedly generate test-cases using the generator until we find an input for which the answers provided by the incorrect and the model-generated solution differ. If such a case is found within 1 minute of execution time, it is taken to be the attempted counterexample and evaluated. Otherwise, this search for a test-case terminates with the model receiving no credit. We include three demonstrations for creating a randomised input generator along with a simple correct solution in our prompt.

\subsection{Random Search (Oracle)}
We augment the Random Search baseline to reveal the efficient, correct solution to the model alongside the initial setup of providing the problem details and the buggy code. This has three important consequences.
\begin{enumerate}
    \item \textbf{Comparative Analysis.} The model now knows the exact steps to solve an arbitrary instance of the problem. It does not have to engage in careful analysis and reasoning to arrive at this. The counterexample search can be guided by comparing the high-level conceptual approaches of the two implementations, as well as the low-level details such as handling of edge cases, array sizes, and variable bounds.

    \item \textbf{Large Randomised Generations.} Unlike in RandSearch, where the correct code was prohibitively inefficient, the model's test-case generation script is no longer forced to output smaller test-cases. Both the correct and incorrect solutions can now run quickly under the full scale of the original problem's constraints. As a result, random testing is much more likely to uncover bugs that only arise with larger problem instances, e.g. overflows, out-of-bounds access, edge cases, etc.

    \item \textbf{Eliminates Error in `Assumed Truth'.} In RandSearch, the search for an input that caused a discrepancy between the two codes often terminated quickly. However, the issue was frequently an inaccurate brute-force solution generated by the model rather than a bug acting up in the provided incorrect code. In this new setup, if the search for a failing test-case terminates within 1 minute, it is guaranteed to pass the subsequent evaluation phase (given that it follows the input format).
\end{enumerate}

\section{More Details About Benchmark}
\subsection{Evaluation}
\textbf{Environment.} In line with the judging environment used by Codeforces, we perform our experiments on a Windows machine. We also mimic the compilation flags for C++, specifically: \texttt{-Wall -Wextra -Wconversion -static -DONLINE\_JUDGE -Wl,--stack=268435456 -O2 -std=c++23 program.cpp -lstdc++exp}. For executing the incorrect and ground-truth code on inputs, we allow a liberal time limit of 30 seconds. This is more than the limit imposed by Codeforces for the problems in \bench{}, which is at most 15s with a mean of 2.4s.

\textbf{Programming Languages.} In \bench{}, we provide the exact programming language description as supplied by Codeforces. The distinct languages spanned by the dataset are: C++14 (GCC 6-32), C++17 (GCC 7-32), C++17 (GCC 9-64), C++20 (GCC 11-64), C++20 (GCC 13-64), C++23 (GCC 14-64, msys2), PyPy 3-64, and Python 3. For execution, all C++ programs are compiled with \texttt{-std=c++23}, leveraging backward compatibility We use the standard CPython interpreter. We verified that these choices do not alter the behavior of the code in our benchmark.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{figs/Leaderboard-livecodebench-Feb2025.pdf}
    \caption{LiveCodeBench leaderboard in Feburary 2025, which we used to select the final 5 models used for our benchmarking. We took the top 3 reasoning models from unique developers, highlighted in red, and the top 2 chat models highlighted in yellow.}
    \label{fig:livecodebench}
\end{figure}

\subsection{Sample Length Analysis}

\begin{table}[H]
    \centering
    \begin{tabular}{lc}
        \toprule
        \textbf{Sample component} & \textbf{Median Length} \\ \midrule
        Problem Description       & 1018 \\
        Incorrect Submission      & 1962 \\
        Correct Solution          & 1552 \\
        \bottomrule
    \end{tabular}
    \caption{We report median lengths (number of characters) of problem description and incorrect submission as these are fed as input to models. This affects the minimum input context length required and evaluation costs for our benchmark. We also report the median length of the correct solution for reference.}
    \label{tab:sample-lengths}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\linewidth]{figs/succ_probstmtlen.png}
    \includegraphics[width=0.45\linewidth]{figs/succ_wrongsolnlen.png}
    \caption{\textbf{Analysis of R1, o3-mini (high) success rate at creating counterexamples based on sample length, combining success across strategies like prompting, agent and programmatic search}. We find that model successes are not predictable based on problem statement length and incorrect solution length in characters.}
    \label{fig:succanalysis_lengths}
\end{figure}

\pagebreak
\subsection{Future Extensions}
A natural extension of our work is to broaden the scope of inverse benchmarks beyond algorithmic reasoning. While existing benchmarks like FrontierMath and HumanEval primarily assess problems-solving by requiring models to generate correct solutions that are directly matched against ground-truth, inverse benchmarks will require mechanisms to verify correctness of proposed claims over arbitrary instances. An example of such an interaction is provided in Figure~\ref{fig:math_inverse_bench}.

Another promising avenue is to explore how the abstraction of an LLM attempting to falsify by generating code allows for hypothesis testing, data analysis, and simulations. In principle, code serves as a general medium that enables verification through diverse means -- only limited by digital capabilities that models are fundamentally bound by already. This implies that leveraging code execution as a mechanism for falsification has broad applicability across diverse domains. As models improve, this paradigm may not only allow them to generate counterexamples but also to systematically explore patterns and behaviors, leading to more reliable scientific and mathematical discoveries.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{figs/math_inverse_bench.png}
    \caption{\textbf{Example of an inverse benchmark task in mathematics, contrasting with standard solution-generation benchmarks.} This illustrates how our general formulation of $\mathcal{H}(x)$ and $\mathcal{P}(x)$ (Section~\ref{sec:formulation}) extends to other domains, offering broad potential for future work.}
    \label{fig:math_inverse_bench}
\end{figure}

\pagebreak
\section{Prompts}
In this section, few-shot samples are denoted by \texttt{<examples>}. Similarly, long code snippets have been reduced to \texttt{[...]}. Full prompts with our samples and expert rationales are available in our code repository.

\subsection*{Task Description Format}
\input{ICLR/sections/prompts/task_description_format}

\subsection{Zero-Shot}
\input{ICLR/sections/prompts/vanilla_zero_shot}

\subsection{Few-Shot}
\input{ICLR/sections/prompts/vanilla_few_shot}

\subsection{Zero-Shot Agent}
\input{ICLR/sections/prompts/agent_zero_shot}

\subsection{ReAct Agent (With Sample Trajectory)}
\input{ICLR/sections/prompts/agent_few_shot}

\subsection{Few-Shot Random Search}
\input{ICLR/sections/prompts/brute_gen}

\subsection{Zero-Shot (Oracle)}
\input{ICLR/sections/prompts/zero_shot_oracle}

\subsection{Random Search (Oracle)}
\input{ICLR/sections/prompts/gen_oracle}
