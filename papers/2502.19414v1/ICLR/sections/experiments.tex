\begin{table}[t]
\vspace{-0.4cm}
\centering
\caption{\textbf{Performance Comparison of LLMs.} We test 4 strategies across 324 problems in our benchmark. While top reasoning models can generate correct solutions for nearly half of the problems, their ability to find counterexamples for subtly incorrect solutions lags significantly, even with agentic code execution feedback. Providing models with the correct solution does not substantially improve counterexample generation, highlighting a fundamental gap between solving and falsifying.}
\label{tab:reformatted_grouped}
\begin{adjustbox}{width=\textwidth}
\begin{tabular}{l |cc| c ccc cc}
\toprule
\multirow{3}{*}{\textbf{Model}} & \multicolumn{2}{c}{Solution Generation} & \multicolumn{6}{c}{Counterexample Creation} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-9}
                                & Rating   & Solved\%             & \multicolumn{1}{c}{Cost} & \multicolumn{3}{c}{Prompting} & \multicolumn{2}{c}{ReAct Agent} \\
\cmidrule(lr){4-4} \cmidrule(lr){5-7} \cmidrule(lr){8-9}
                                &          &                      &                         & Zero-shot & Few-shot & w/ Correct & w/o Demo & w/ Demo \\
\midrule
DeepSeek-V3          & 1134    & 10.8   & 5   & 2.4   & 2.7   & 3.7  & 3.7         & 3.1         \\
Sonnet 3.5           & 717     & 6.6    & 71   & 4.6   & 3.7   & 2.2  & --          & 3.0         \\
Flash 2.0 (Thinking) & --      & --     & 0   & 0.9   & 2.1   & 2.5 & 1.8         & 2.5          \\
DeepSeek-R1          & 2029    & 44.0   & 104   & 5.8   & 5.2   & 4.6 & \textbf{8.6} & 6.5         \\
o3-mini (high)       & \textbf{2130} & \textbf{48.7} & 76   & \textbf{8.6} & \textbf{8.9} & \textbf{9.3} & 6.8 & \textbf{8.6} \\
\bottomrule
\end{tabular}
\end{adjustbox}
\vspace{-0.3cm}
\end{table}

\section{Experiments}
We first report frontier reasoning and non-reasoning models’ performance at creating counterexamples on our benchmark and then analyze the results.


\subsection{Models struggle at creating counterexamples}

Based on LiveCodeBench (leaderboard in Figure~\ref{fig:livecodebench}), we select the three best reasoning models from unique developers: o3-mini-high-0131, DeepSeek R1, Gemini 2.0 Flash Thinking-0121, and the two best chat models, Claude-3.5-Sonnet-20241022 and DeepSeek V3.

\noindent We compare two strategies: \textit{Prompting} and \textit{Agentic Code Execution}.

\textbf{Prompting.} We prompt each model with the problem statement, input constraints, example input-output pairs, and the incorrect code. The model must produce a script to print a failing test case, along with concise reasoning. In the few-shot setup, we also present three sample problems and incorrect code illustrating diverse issues with expert-annotated rationales.

\textbf{Agentic Interaction with Code Execution Feedback.} A typical human workflow of finding bugs involves tinkering with the code and observing its behaviour on various inputs. Inspired by this, we allow the model to interact with a code execution environment. The model can make upto ten attempts to execute any code with arbitrary inputs it wishes to test. It receives the output in return. In case of errors, it receives the corresponding feedback instead (e.g.\@ compiler messages). For example, the model can add print statements to the incorrect code and observe intermediate behaviour~\citep{hu2024leveragingprintdebuggingimprove}. It can also write its own versions of subroutines in the code and observe any differences from how the buggy code handles the same scenario, effectively avoiding the need to ``dry run'' computations itself. To prevent exhausting the context, we truncate the outputs to upto 2000 characters before revealing them to the model. Each code execution is limited to 30 seconds. After this interactive phase, similar to the standard prompting setup, the model must submit a script to print a failing test-case. If this submission fails validation checks, we provide this feedback to the agent and allow it to resubmit upto five times.

\textbf{Can models solve these problems?} We include the \textit{code generation} performance of models on our benchmark's problems by estimating the number of problems solved. This is computed by first finding the per-problem success probability, which is derived from the problem's rating and the model's reported Elo \citep{deepseekai2025deepseekr1incentivizingreasoningcapability} following standard Codeforces rating calculations. If the model is rated $r$, we define:
\[
\text{solved}(r) = \mathrm{E}_{p} \left[\frac{1}{1 + 10^\frac{p - r}{400}} \right]
\]

Here, $p$ is sampled from the problem ratings in our benchmark.

\textbf{What if models have access to the ground-truth solution?} We also evaluate the impact of providing models with access to the correct code. This decouples the advantage of models with better solution generation abilities. We augment the earlier zero-shot prompting setup to additionally reveal the correct solution and report results in Table~\ref{tab:reformatted_grouped} \textit{(w/ Correct)}.

\textbf{Discussion and Error Analysis.} Table~\ref{tab:reformatted_grouped} shows that counterexample creation lags significantly behind solution generation and does not scale proportionally. Furthermore, models struggle to leverage code execution feedback—a key component of human debugging workflows—with only DeepSeek R1 exhibiting modest improvements. On the other hand, our analysis shows execution feedback greatly reduces test-case validation failures. For instance, DeepSeek R1 and V3 both eliminate validation issues completely, compared to 45 and 36 failures in the zero-shot setting. Additionally, while few-shot prompting with expert rationale improves Gemini’s performance over zero-shot, other models show minimal gains or even degrade.

The oracle (Table~\ref{tab:reformatted_grouped} \textit{(w/ Correct)}) simulates a hypothetical where o3 (currently unreleased) matches the reported Codeforces rating of 2727, or a future model is able to solve most Codeforces problems: would they automatically be able to find counterexamples for incorrect solutions? While counterexample creation abilities could also improve, knowing the correct solution alone is insufficient even for the best current reasoning model, o3-mini (high).
 
\subsection{Does explicit prompting for search help?}
Manual inspection of model outputs revealed that models rarely used a search based strategy to find counterexamples. In contrast, humans often generate randomized inputs guided by structural intuition which they expect to yield valid counterexamples. To address this, we explicitly prompt models to generate counterexamples using a search-based strategy with controlled randomization. Note that a key difference from our initial filtering step (where we filtered trivial samples that can be broken through search without reasoning, described in Section~\ref{sec:collect}) is that there the model was not given access to the incorrect solution, so it by definition did not reason about it, whereas here it can. Specifically, we test two strategies and report these numbers in Table~\ref{tab:random_search}:

\textbf{RandSearch.} The model constructs a randomized input generator and a brute-force solution. We use the generator to search for tests and compare the outputs of the brute-force solution against the incorrect code. The search terminates when it encounters differing outputs. We limit this search to 2 minutes. We provide few-shot examples with rationale only to non-reasoning models, as reasoning models performed better without them. %Described in \ref{appendix_someplace}.

\textbf{RandSearch Oracle.} The previous step requires the model to generate a brute-force solution. While it is often easy to find exponential-complexity solutions for these algorithmic problems, models could still produce wrong ones. To alleviate this, we simulate a hypothetical scenario where the model has access to the correct solution and then writes a randomized search strategy to find an input where the incorrect solution's output strategy doesn't match. In other words, the model can use our ground-truth verification environment, so any input produced at the end is likely to be correct as long as it passes validation checks.

We provide a detailed description of the motivation and nuances of these methods in Appendix~\ref{sec:baselines_appendix}.

\begin{table}[t]
\vspace{-0.4cm}
\centering
\caption{\textbf{Prompting Models to use Search to find Counterexamples.} Without the correct solution (RandSearch), models often generate invalid counterexamples where the incorrect solution gives the right output. The counterexample success rate increases with access to the correct solution (RandSearch Oracle), but significant room for improvement remains.}
\label{tab:random_search}
\begin{adjustbox}{width=\columnwidth}
\begin{tabular}{lccccc}
\toprule
\textbf{Model} & \textbf{DeepSeek-V3} & \textbf{Flash 2.0 (Thinking)} & \textbf{DeepSeek-R1} & \textbf{o3-mini (high)} \\
\midrule
\textbf{RandSearch} & 4.0 & 3.7 & 4.0 & 8.3 \\
\textbf{RandSearch Oracle} & 15.1 & 7.7 & 9.9 & -- \\
\bottomrule
\end{tabular}
\end{adjustbox}
\vspace{-0.3cm}
\end{table}
\FloatBarrier

\textbf{Discussion and Error Analysis.} Performance of reasoning models deteriorates when explicitly prompted to use randomized search. It offers marginal gains for chat models. For o3-mini, the successful samples are interestingly quite disjoint when using this strategy. The counterexamples created invalidate 6\% distinct submissions that the previous prompting and agentic strategies could not, in contrast to 3\% new submissions invalidated when the model was provided correct solutions. This shows that models learning to leverage programmatic search when appropriate can significantly boost performance.

Further analysis of RandSearch's failure modes reveals that, on average, 35\% of all samples terminate with an incorrect counterexample. Among these, 14\% fail due to test-case validation errors, while the remaining 86\% result from incorrect brute-force code. This trend remains remarkably consistent across models --- differences in code generation benchmarks do not appear to significantly impact their ability to write trivial brute-force code. 

Gemini Flash 2.0 (Thinking), the worst performer, and o3-mini (high), the best, are similarly bottlenecked by incorrect brute-force code (28\% vs.\@ 32\% of all samples). However, o3-mini (high) still manages to double its overall success rate, suggesting that more intelligent search strategies can outperform unguided attempts.

% With access to the oracle solution, both DeepSeek-V3 and Gemini generate test-case generators that fail to find any counterexample in 70\% of samples. However, Gemini produces improperly formatted outputs at a much higher rate -- 12\% compared to DeepSeek-V3's 1\%.

\subsection{When can models create counterexamples?}
\begin{figure}
\vspace{-0.5cm}
    \centering
    \includegraphics[width=0.325\linewidth]{figs/succ_testspassed.png}
    \includegraphics[width=0.325\linewidth]{figs/succ_probrating.png}
    \includegraphics[width=0.325\linewidth]{figs/succ_authrating.png}
    \caption{\textbf{Analysis of R1, o3-mini (high) success rate at creating counterexamples, combining success across strategies like prompting, agent and programmatic search}. We find that model successes are not predictable based on problem difficulty, or attributes of the incorrect submission like author expertise and number of test cases passed before giving a wrong answer. Success rate on the hacked subset, where counterexamples were found despite passing all initial tests is 4\% $\pm 1\%$.}
    \vspace{-0.3cm}
    \label{fig:succanalysis}
\end{figure}

As counterexample creation abilities are particularly relevant for model reflection and reliability in model outputs, it is important to characterize in what types of situations they can and cannot create valid counterexamples. 

We analyze counterexample creation success across three attributes which are highly predictive of solution generation correctness -- problem difficulty, number of tests the incorrect submission passed before failing, and incorrect submission author expertise. For each attribute, we divide samples into five percentile buckets and measure average success at creating counterexamples of the best two models, R1 and o3-mini (high), across prompting (Zero-shot, Few-shot), agentic (with and without demos) and search strategies.

Surprisingly, we find no clear trends in Figure~\ref{fig:succanalysis} between these three attributes and counterexample creation success. We also find limited insight based on problem statement and incorrect solution length as shown in Appendix Figure~\ref{fig:succanalysis_lengths}. 

This demonstrates that model counterexample capabilities can be non-trivial to predict using attributes that are predictive of solution generation capabilities. This further highlights the complementary nature of falsification. We believe that better understanding of what attributes contribute to its difficulty is an important direction for further investigation.


% Go through (model, method) pairs and see which problems they are solving. Do these overlap across methods for a fixed model, across models for a fixed method?
% Does accuracy correlate with problem ratings, solver ratings, tc passed for any (model, method) pairs.


% \begin{table}[h!]
% \centering
% \caption{Performance Comparison of LLMs}
% \label{tab:llm_performance}
% \begin{tabular}{lccccccc}
% \toprule
% \textbf{Model} & CF Rating & \textbf{Zero} & \textbf{Few} & \textbf{RandSearch} & \textbf{RandSearch} & \textbf{Zero-shot} & \textbf{Few-shot} \\
% & & \textbf{Shot} & \textbf{Shot} & & \textbf{(Oracle)} & \textbf{\agent{}} & \textbf{\agent{}} \\
% \midrule
% DeepSeek-V3          & 1134 & 2.2 & 3.0 & 6.0 & 19.6 & 4.2 & 4.0 \\
% Sonnet 3.6           & 717 & 4.3 & 4.0 & x & x & x & 4.5 \\
% Flash 2.0 (Thinking) & x & 2.0 & 3.0 & 6.8 & 10.3 & 3.5 & 4.7 (X) \\
% DeepSeek-R1          & 2029 & 7.8 & 6.5 & 7.5 & x & 12.6 & 9.8 \\
% o3-Mini (High)       & 2130 & 12.3 & 12.3 & 12.1 & x & 10.5 & 13.0 \\ \midrule
% Human & \multicolumn{6}{c}{100} \\
% \bottomrule
% \end{tabular}
% \end{table}
