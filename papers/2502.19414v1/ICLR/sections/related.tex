\section{Related Work}

\textbf{Formal verification} Programmatic search methods have long assisted researchers by enumerating and validating cases â€” ranging from symbolic execution tools like higher-order logic systems \citep{blanchette2010nitpick}, SMT solvers \citep{de2008z3}, symbolic execution tools like KLEE \citep{cadar2008klee} and randomized testing frameworks like QuickCheck \citep{claessen2000quickcheck} (for details see eloquent survey by \citet{baldoni2018survey}). Automatic test case generation has widely been studied in software engineering (see surveys \citep{anand2013orchestrated, zhu1997software, runeson2006we}). Existing work on LM based test generation~\citep{li2024largelanguagemodelstest} focuses on creating valid inputs and desired outputs for a given task specification. However, counterexamples may reside in non-obvious regions of combinatorially large input spaces, making search-based methods infeasible~\citep{holzmann2002logic}, especially in algorithmic reasoning settings \citep{forivsek2006suitability}. While formal verification tools have made a lot of progress~\citep{alur2013syntax, polgreen2020counterexample}, their expressivity still remains limited~\citep{ammons2002mining, bjorner201440, rozier2016specification}. Moreover, most claims are unstructured, and a complete formalization of the whole system is not possible for most claims. We focus on the task of hard-to-find, targeted test cases that invalidate subtly incorrect solutions.

\textbf{Fact Verification as Refuting False Information.} At first glance, fact checking (and misinformation detection) literature also aims to refute incorrect claims by verifying facts \citep{thorne2018fever, guo2022survey, press2024citeme, nakov2021automated}, such as on social media platforms~\citep{aimeur2023fake, chen2024combating} using sourced evidence~\citep{nakano2021webgpt, chen2023complex, de2024supernotes}. Yet, fact checking is challenging due to unclear epistemological grounding of truth in a complex social world  \citep{uscinski2013epistemology, vinhas2022fact}. In contrast, we focus on domains with clear truth semantics \citep{davidson1967truth}, requiring the model to produce a counterexample that \textit{verifiably} refutes a given claim.


\textbf{Cybersecurity.} Cybersecurity has traditionally focused on vulnerability detection with CTF style contests \citep{aicyberchallenge, darpa2016cybergrandchallenge}, with recent interest in LLM Agent systems~\citep{abramovich2024enigma, deng2023pentestgpt} (refer to \citep{motlagh2024large} for a survey). Similarly, \textit{fuzzing}~\citep{huang2024largelanguagemodelsbased} involves finding security vulnerabilities in software codebases by providing invalid or unexpected random data and monitoring for crashes, failing pre-defined assertions, memory leaks. On the surface, CTF and fuzzing do require finding worst-case inputs that cause code to misbehave. Our work focuses on creating counterexamples using valid inputs for subtly wrong algorithmic solutions, with an eye towards subtly incorrect scientific claims.

\textbf{Code Self-Repair.} Existing work on code self repair~\citep{chen2024teaching} focuses on the following task: the LM is given access to a code execution environment~\citep{wang2024executable} along with predefined test cases, and it iteratively improves its own solution using compilation and correctness feedback till it passes all test cases~\citep{khattab2023dspy}. Instead, our work focuses on the ability of the LM to generate worst-case test cases where a given incorrect solution would fail. \citet{gu-etal-2024-counterfeit} show that LMs struggle to classify their own generations as incorrect. Improving counterexample creation could thus help LMs self-repair their own code when solving novel problems with unknown ground-truth.

\textbf{Language Models for Scientific Discovery.} FunSearch~\citep{romera2024mathematical} demonstrated the use of LMs to generate novel solutions to an open problem in Mathematics, given access to a programmatic evaluator. We ask, can LMs falsify subtly incorrect solutions or claims, including domains where programmatic evaluation might not be possible? \citet{lu2024aiscientist, si2024can} proposed writing research papers end-to-end using an LM, an agentic pipeline where feedback is obtained from LM generated review scores. Instead of depending on papers and review scores as arbitrary units of science, we focus on research progress through precise claims and counterexamples.

\textbf{Scalable Oversight.} In the limit, improving at counterexample creation can help models red-team~\citep{perez-etal-2022-red} another model's reasoning~\citep{tyen-etal-2024-llms, huang2024large}, thus acting as better judges~\citep{zheng2023arena}. Allowing self-improvement through critique~\citep{wang2025critiquefinetuninglearningcritique} and debate~\citep{kenton2024scalableoversightweakllms}. Finding mistakes and avoiding sycophantic behavior~\citep{sharma2024towards} is crucial for emerging safety paradigms where models assist humans in overseeing other models~\citep{bowman2022measuring}.