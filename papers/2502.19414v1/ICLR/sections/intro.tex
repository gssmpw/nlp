\section{Introduction}

Empirical science has evolved through an iterative process of new claims, falsification and subsequent refinement of these claims. Mathematicians too follow a similar approach -- they propose conjectures and then invest substantial effort in search of counterexamples before attempting a proof. There has been growing interest in using language models (LMs) to accelerate research~\citep{jumper2021highly, lu2024aiscientist}, which is considered the next frontier for AI progress. Yet, most existing LM benchmarks focus on a model's ability to \textit{generate solutions} to problems~\citep{jimenez2024swebench, wijk2024rebenchevaluatingfrontierai, phan2025humanity}. In this work, we highlight the need for benchmarks that test the inverse capability -- \textit{falsification}. Scientific hypotheses are considered falsified when concrete contrary evidence is presented \citep{popper2005logic}. Researchers create counterexamples to claims by inspecting every step of argumentation and evidence, leveraging domain knowledge, epistemic uncertainty, reasoning and creative intuition. This process requires deep understanding of the problem, and is both challenging and time intensive. Thus, benchmarking counterexample creation can not only accelerate scientific discovery, but also rigorously test LM reasoning abilities.

\noindent However, a key challenge with creating such benchmarks is to verify whether a model's output is a valid counterexample to the claim. Traditional benchmarking by comparison with ground-truth reference solutions is insufficient --- there could be many valid counterexamples to a claim \citep{lakatos2015proofs}. To make initial progress in this direction, we start by focusing on unstructured algorithmic problem solving, where counterexamples can be verified formally through code execution.
Language models have already shown promise in solving algorithmic problems at an expert level~\citep{jain2024livecodebench, openaiO3Mini}. So we ask:
\begin{quote}
\centering
    Can LMs create counterexamples for\\ incorrect solutions to algorithmic problems?
\end{quote}

\begin{figure}[t]
    \vspace{-0.6cm}
    \centering
    \includegraphics[width=\columnwidth]{figs/pipeline.pdf}
    \caption{While standard benchmarks for algorithmic reasoning require models to generate solutions, we propose an inverse benchmark to evaluate reasoning about correctness by falsifying incorrect solutions. To allow expressivity, we let the model output a code that generates the counterexample input, and validate it by comparing the output of the incorrect solution with a held out correct solution.}
    \label{fig:main-fig}
    \vspace{-0.2cm}
\end{figure}

\noindent To investigate this, we design a novel benchmark: \bench{} (\textbf Refuting \textbf Erroneous \textbf Findings \textbf Using \textbf Targeted \textbf Examples). Each sample in \bench{} contains: (i) a detailed problem description (including input constraints) and (ii) an \textit{incorrect} solution. The task for LMs is to create inputs satisfying the constraints, such that the given solution fails. The LM must generate a program that prints such an input, and is allowed to use arbitrary programmatic constructs to this end (Figure~\ref{fig:main-fig}).

We automatically source samples of incorrect submissions from Codeforces\footnote{https://codeforces.com/} contests in 2024 and 2025. These samples are search and contamination free as Codeforces does not reveal non-trivial failure cases. We regularly update our benchmark to prevent indirect leakage due to potential overlap with training data, similar to LiveCodeBench~\citep{jain2024livecodebench}. An automated pipeline to extensively filter samples ensures that counterexample discovery requires reasoning about the incorrect solution beyond random search. \bench{} contains 324 samples spanning diverse algorithmic topics with rich metadata annotations (Section~\ref{sec:benchmark}).

We benchmark the current best models from five different model developers, as ranked based on their accuracy at generating solutions to these problems on LiveCodeBench~\citep{jain2024livecodebench}. These include OpenAI o3-mini (high) and DeepSeek R1, which can generate correct solutions for up to 50\% of the problems in our benchmark based on reported Elo ratings. Yet, even with few-shot prompting with chain of thought and a ReAct agent scaffold~\citep{yao2022react} with code execution feedback, these models can only find counterexamples to $<9\%$ of problems in our dataset. Our results additionally demonstrate a prior hypothesis that LMs' ability to repair their own incorrect code is bottlenecked by their inability to find mistakes in their code~\citep{gu-etal-2024-counterfeit, olausson2024selfrepairsilverbulletcode}.

More broadly, our results demonstrate that verification, which includes falsification of subtly incorrect solutions, can sometimes be harder for models than solving the problem correctly. This indicates limitations in the potential for self-improvement using the generator-verifier gap~\citep{song2024mindgapexaminingselfimprovement}. As models progress towards solving novel problems where humans cannot provide ground-truth, it will be crucial that they can reflect about their own mistakes~\citep{deepseekai2025deepseekr1incentivizingreasoningcapability} to produce more reliable outputs. We hope our work spurs interest in the community to create inverse benchmarks to test the ability of models to invalidate incorrect solutions or claims.