\section{Introduction}
\label{sec:Intro}

As large language models (LLMs) continue to advance in reasoning and planning, as demonstrated by OpenAI's GPT-4o-Task~\cite{openai2024gpt4o}, DeepSeek's R1~\cite{deepseekai2025deepseekr1}, Anthropic's Claude 3.5 Sonnet~\cite{anthropic2024claude}, and Gemini~\cite{geminiteam2023gemini}, the research community is increasingly focusing on developing multi-agent systems (MAS) powered by these models. Recent innovations include AutoGen~\cite{wu2024autogen}, CAMEL~\cite{li2023camel}, CrewAI~\cite{crewai2024}, LangGraph~\cite{langgraph2024}, Dspy~\cite{khattab2023dspy}, and XAgent~\cite{xia2023xagent}, among others. 
Although individual LLMs demonstrate significant capabilities, their true potential is realized when they collaborate
to tackle complex real-world problems \cite{AGIBookChang2024}.

Most AI benchmarks emphasize perception, language understanding, or basic reasoning. However, real-world challenges, such as supply chain management, disaster response, healthcare logistics, and investment strategies, demand coordinated planning and decision-making among specialized agents. There is a pressing need for robust benchmarks that can evaluate the performance of both single-agent systems and MAS in these complex, high-stakes domains.

\subsection{The REALM Benchmark Suite}

REALM-Bench (Real-world Planning Benchmark for LLMs and Multi-Agent Systems) addresses the need for rigorous evaluation with carefully curated planning challenges. These scenarios are designed to be both \textit{tractable}, enabling human validation and debugging, and sufficiently complex to push the boundaries of current AI systems. Each challenge requires reasoning and validation over sequential actions
parallel processes, resource constraints, and unexpected disruptions
\cite{chang2025MACI,SocraSynthChangCSCI2023,SocraticIEEECCWC2023,chen-etal-2024-llmarena, huang2024understandingplanningllmagents}.

The suite consists of eleven scenarios that progressively increase in complexity across three key dimensions:

\begin{enumerate}[leftmargin=1em, topsep=0em, parsep=0em, label=\arabic*.]
\item \textbf{Parallel Planning Threads:} The number of concurrent planning processes that must be coordinated. 
\item \textbf{Inter-Dependencies:} The complexity of relationships and constraints between these planning threads. 
\item \textbf{Disruption Frequency and Impact:} The rate and severity of unexpected events that require the adaptation of the plan. 
\end{enumerate}

Next, we describe how each scenario can be scaled along these three dimensions.

\subsection{Benchmark Scalability}

While the base versions of each scenario enable detailed analysis and debugging, they can be scaled along the three dimensions defined above: parallel planning threads, inter-dependencies, and disruption frequency and impact. 

For example, an urban ride-sharing scenario becomes increasingly complex as the number of vehicles and passengers grows, with interdependent carpooling routes and frequent traffic disruptions necessitating real-time plan adjustments.

This scalability allows AI planning systems to be evaluated under progressively challenging conditions, while still allowing detailed analysis of failure modes in simpler scenarios.

\subsection{Availability and Access}

The REALM-Bench Suite V1.0 is available on GitHub \cite{REALM-Bench-Github}. In addition, we plan to host competitions and workshops at major AI conferences in 2025 to foster community engagement and further development.
