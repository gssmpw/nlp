\section{Related Benchmark}

Planning benchmarks have evolved from testing basic STRIPS-style planning to evaluating increasingly sophisticated planning capabilities. The International Planning Competition (IPC) has been a primary driver of planning benchmarks since 1998, using PDDL to specify domains like BlocksWorld, Logistics, and Rovers~\cite{PlanningCompetition2024}. While valuable for testing classical planning algorithms, these benchmarks focus on deterministic environments with complete information and lack the dynamic disruptions common in real-world scenarios.

More recent benchmarks, such as the Process Planning Competition (PPC), have shifted toward continuous processes and temporal constraints~\cite{ppc2020}. Their manufacturing scenarios include parallel activities and resource dependencies, but the disruptions remain limited to machine breakdowns with known repair distributions. Similarly, the Dynamic Planning Competition introduces environmental changes during plan execution, yet it focuses primarily on path planning and navigation scenarios~\cite{dpc2022}.

The annual Automated Negotiation Agents Competition (ANAC), established in 2010, has evolved to incorporate planning elements within its supply chain scenarios~\cite{anac2023}. However, its scope remains primarily focused on bilateral negotiations rather than comprehensive planning under uncertainty. For example, the 2024â€“25 competition featured a main challenge titled ``Split the Pie,'' an artificial yet simplified negotiation scenario where agents divide resources between parties. The supply chain problems in ANAC do not involve contingency planning, resource reallocation, or adaptation to unexpected disruptions.

Specifically for testing LLMs' planning capabilities, TimeBench \cite{chu2023timebench} and TaskBench \cite{shen2023taskbench} represent two approaches to evaluating AI planning. TimeBench focuses on temporal reasoning by testing systems' ability to understand time dependencies and scheduling constraints, though it often relies on synthetic scenarios that fail to capture the dynamic nature of real-world temporal relationships, where deadlines shift and durations remain uncertain. TaskBench, on the other hand, evaluates practical task automation and step-by-step planning; it provides valuable insights into an AI system's ability to decompose complex goals into manageable steps, but its scenarios may oversimplify the challenges of real-world automation, where outcomes are uncertain and processes are deeply interconnected.

This landscape reveals several gaps in existing benchmarks: 

\begin{enumerate}[leftmargin=1.5em, topsep=0pt, parsep=0pt, label=\arabic*.]
\item \textbf{Limited Disruption Modeling:} Most benchmarks treat uncertainties as static probability distributions rather than dynamic, interdependent events that can cascade through systems. 
\item \textbf{Simplified Dependencies:} Real-world planning problems involve rich networks of temporal, resource, and causal dependencies that exceed the complexity found in current benchmarks. 
\item \textbf{Restricted Scope:} Benchmarks tend to focus on specific subproblems (path planning, task allocation, etc.) rather than end-to-end planning scenarios that combine multiple challenges. 
\item \textbf{Artificial Constraints:} Many benchmarks use simplified representations (like PDDL) that cannot capture the nuanced constraints and objectives found in real-world planning problems. 
\item \textbf{Limited Scalability:} Few benchmarks allow systematic scaling of complexity along multiple dimensions while maintaining problem tractability for analysis. 
\item \textbf{LLM Specific Challenges:} Although LLMs have achieved remarkable successes, the transformer architecture exhibits certain limitations. For example, an \textit{attention sink} phenomenon can cause certain tokens to be neglected, potentially skewing model predictions \cite{xiao2024attentionsink}. Additionally, maximum likelihood training can introduce biases that limit output diversity and quality \cite{holtzman2020curiouscaseneuraltext, SocraSynthChangCSCI2023}. Finally, chain-of-thought approaches may suffer from pitfalls such as error propagation and inconsistent reasoning \cite{brown2020language, prystawski2023why}. A test suite should specifically examine these LLM-related issues.
\end{enumerate}


Our proposed REALM-Bench addresses these limitations by providing scenarios that combine dependencies, dynamic disruptions, and scalability while remaining tractable for systematic evaluation. This allows testing of planning systems under conditions that better reflect the challenges of real-world applications.


