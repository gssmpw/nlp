\section{Proof-of-Concept Controller}
\label{sec:cachetrace}
Motivated by our prior findings and modeling work, we present an example controller to help validate our conjecture of a black-box search strategy. This controller uses an established machine learning technique, Bayesian optimization~\cite{frazier, garnett_bayesoptbook_2022}, to find energy-efficient interrupt coalescing and CPU frequency settings that can adapt to the specific application, OS, and hardware while exploiting the stability in offered loads. 

In 1) \cref{sec:cachetrace2}, we illustrate its applicability in optimizing the energy efficiency of a server that supports a realistic datacenter workload trace~\cite{cacheWorkload-OSDI20} over 24 hours by periodically adjusting ITR-delay, and DVFS settings as offered load changes, and in 2) \cref{sec:tailbench} demonstrates the generality of the controller as we apply it across different types of NICs and CPUs (\cref{table:hwsw}) when run on three applications from Tailbench~\cite{tailbench}.

As a proof-of-concept controller, we made simplfying assumptions in its design and leave addressing real deployment scenarios for future work. Our assumptions include ad-hoc thresholds for when to trigger Bayesian optimization and the number of subsequent trials to run. However, our results show that even using straightforward assumptions can yield significant advantages, leaving ample room for improvement.

The architecture of our controller also facilitates the integration of more advanced policies for initiating the Bayesian process. We envision the deployment of this technique in data centers through collaboration with load-balancers that make use of historical usage data. This collaboration would help distribute incoming loads to energy-optimized servers, which have been pre-configured with specific settings, while still meeting SLA objectives. In addition, the load balancers can help mitigate the potential gaming of the learning agent's behavior in response to changing request rates.

\subsection{Design}
Fig.~\ref{fig:bayop} illustrates the design of our  controller: \circled{1} A live system running memcached services requests arriving at varying QPSes from an external source. \circled{2} It then triggers a set of performance and energy measurements of the live system to be shared with an external Ax~\cite{ax, Bakshy2018AEAD} Bayesian optimization platform. \circled{3} This process then computes a penalty $Rp$ of the current (ITR-delay, DVFS) setting and \circled{4} then recommends an update to a new (ITR-delay, DVFS) configuration on the live system that minimizes $Rp$. Once this process completes, the live system is set with a fixed (ITR-delay, DVFS) configuration until the next set of measurements is triggered.

\subsubsection{Penalty Function}
We use a simple function that penalizes the optimization process by the amount of measured energy and magnifies that penalty when measured latency violates the SLA objective:

\begin{equation} \label{eq:bayesopt_reward}
    Rp = m\_energy * max((m\_latency - SLA + 1), 1)
\end{equation}

For example, with an SLA objective of 99\% tail latency < 500 $\mu$s, where measured latency ($m\_latency$) is 600 $\mu$s, the reward $Rp$ will be scaled up by a factor of 100, such that $Rp = m\_energy * (600 - 500)$. If $m\_latency$ is less than $SLA$, then $Rp$ will evaluate to $m\_energy$. Minimizing $Rp$ is indicative that Bayesian optimization is selecting (ITR-delay, DVFS) pairs to meet performance/energy objectives. This reward function enables an operator to express their preference to optimizing for different combinations of energy and performance objectives.

The possibilities of customizing this function further are also ripe for exploration: such as using new combinations of performance/energy or known metrics such as energy-delay-product~\cite{573184,10.1109/40.888701}. One can also imagine developing a rich set of reward functions that capture preferences a service operator might have. In this way, the controller can be reconfigured as priorities change by selecting and tuning from the set of reward functions.
\input{cachetrace3}
\input{tailbench}
