@inproceedings{10.1109/IGCC.2011.6008552,
author = {Spiliopoulos, V. and Kaxiras, S. and Keramidas, G.},
title = {Green Governors: A Framework for Continuously Adaptive DVFS},
year = {2011},
isbn = {9781457712227},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/IGCC.2011.6008552},
doi = {10.1109/IGCC.2011.6008552},
abstract = {We present Continuously Adaptive Dynamic Voltage/Frequency scaling in Linux systems running on Intel i7 and AMD Phenom II processors. By exploiting slack, inherent in memory-bound programs, our approach aims to improve power efficiency even when the processor does not sit idle. Our underlying methodology is based on a simple first-order processor performance model in which frequency scaling is expressed as a change (in cycles) of the main memory latency. Utilizing available monitoring hardware we show that our model is powerful enough to i) predict with reasonable accuracy the effect of frequency scaling (in terms of performance loss) and ii) predict the core energy under different V/f combinations. To validate our approach we perform highly accurate, fine-grained power measurements directly on the off-chip voltage regulators. We use our model to implement various DVFS policies as Linux "green" governors to continuously optimize for various power-efficiency metrics such as EDP or ED^2P, or achieve energy savings with a user-specified limit on performance loss. Our evaluation shows that, for SPEC2006 workloads, our governors achieve dynamically the same optimal EDP or ED$^2$P (within 2% on avg.) as an exhaustive search of all possible frequencies. Energy savings can reach up to 56% in memory-bound workloads with corresponding improvements of about 55% for EDP or ED$^2$P.},
booktitle = {Proceedings of the 2011 International Green Computing Conference and Workshops},
pages = {1–8},
numpages = {8},
keywords = {continuously adaptive dynamic voltage-frequency scaling, memory bound workloads, EDP, memory bound programs, frequency scaling, AMD Phenom II processors, off chip voltage regulators, Linux systems, green governors, ED$^2$P, power efficiency, Intel i7},
series = {IGCC '11}
}

@inproceedings{10.1109/MICRO.2006.8,
author = {Isci, Canturk and Buyuktosunoglu, Alper and Cher, Chen-Yong and Bose, Pradip and Martonosi, Margaret},
title = {An Analysis of Efficient Multi-Core Global Power Management Policies: Maximizing Performance for a Given Power Budget},
year = {2006},
isbn = {0769527329},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/MICRO.2006.8},
doi = {10.1109/MICRO.2006.8},
abstract = {Chip-level power and thermal implications will continue to rule as one of the primary design constraints and performance limiters. The gap between average and peak power actually widens with increased levels of core integration. As such, if per-core control of power levels (modes) is possible, a global power manager should be able to dynamically set the modes suitably. This would be done in tune with the workload characteristics, in order to always maintain a chip-level power that is below the specified budget. Furthermore, this should be possible without significant degradation of chip-level throughput performance. We analyze and validate this concept in detail in this paper. We assume a per-core DVFS (dynamic voltage and frequency scaling) knob to be available to such a conceptual global power manager. We evaluate several different policies for global multi-core power management. In this analysis, we consider various different objectives such as prioritization and optimized throughput. Overall, our results show that in the context of a workload comprised of SPEC benchmark threads, our best architected policies can come within 1% of the performance of an ideal oracle, while meeting a given chip-level power budget. Furthermore, we show that these global dynamic management policies perform significantly better than static management, even if static scheduling is given oracular knowledge.},
booktitle = {Proceedings of the 39th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {347–358},
numpages = {12},
series = {MICRO 39}
}

@inproceedings{10.1145/1168857.1168881,
author = {Lee, Benjamin C. and Brooks, David M.},
title = {Accurate and Efficient Regression Modeling for Microarchitectural Performance and Power Prediction},
year = {2006},
isbn = {1595934510},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1168857.1168881},
doi = {10.1145/1168857.1168881},
abstract = {},
booktitle = {Proceedings of the 12th International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {185–194},
numpages = {10},
keywords = {statistics, microarchitecture, inference, regression, simulation},
location = {San Jose, California, USA},
series = {ASPLOS XII}
}

@inproceedings{10.1145/1229428.1229479,
author = {Lee, Benjamin C. and Brooks, David M. and de Supinski, Bronis R. and Schulz, Martin and Singh, Karan and McKee, Sally A.},
title = {Methods of Inference and Learning for Performance Modeling of Parallel Applications},
year = {2007},
isbn = {9781595936028},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1229428.1229479},
doi = {10.1145/1229428.1229479},
abstract = {Increasing system and algorithmic complexity combined with a growing number of tunable application parameters pose significant challenges for analytical performance modeling. We propose a series of robust techniques to address these challenges. In particular, we apply statistical techniques such as clustering, association, and correlation analysis, to understand the application parameter space better. We construct and compare two classes of effective predictive models: piecewise polynomial regression and artifical neural networks. We compare these techniques with theoretical analyses and experimental results. Overall, both regression and neural networks are accurate with median error rates ranging from 2.2 to 10.5 percent. The comparable accuracy of these models suggest differentiating features will arise from ease of use, transparency, and computational efficiency.},
booktitle = {Proceedings of the 12th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {249–258},
numpages = {10},
keywords = {regression, numerical methods, performance prediction, statistics, neural networks},
location = {San Jose, California, USA},
series = {PPoPP '07}
}

@article{10.1145/1241601.1241609,
author = {Kondo, Masaaki and Sasaki, Hiroshi and Nakamura, Hiroshi},
title = {Improving Fairness, Throughput and Energy-Efficiency on a Chip Multiprocessor through DVFS},
year = {2007},
issue_date = {March 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/1241601.1241609},
doi = {10.1145/1241601.1241609},
abstract = {Recently, a single chip multiprocessor (CMP) is becoming an attractive architecture for improving throughput of program execution. In CMPs, multiple processor cores share several hardware resources such as cache memory and memory bus. Therefore, the resource contention significantly degrades performance of each thread and also loses fairness between threads.In this paper, we propose a Dynamic Frequency and Voltage Scaling (DVFS) algorithm for improving total instruction throughput, fairness, and energy efficiency of CMPs. The proposed technique periodically observes the utilization ratio of shared resources and controls the frequency and the voltage of each processor core individually to balance the ratio between threads. We evaluate our technique and the evaluation results show that fairness between threads are greatly improved by the technique. Moreover, the total instruction throughput increases in many cases while reducing energy consumption.},
journal = {SIGARCH Comput. Archit. News},
month = mar,
pages = {31–38},
numpages = {8}
}

@article{10.1145/1394608.1382172,
author = {Ipek, Engin and Mutlu, Onur and Mart\'{\i}nez, Jos\'{e} F. and Caruana, Rich},
title = {Self-Optimizing Memory Controllers: A Reinforcement Learning Approach},
year = {2008},
issue_date = {June 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {3},
issn = {0163-5964},
url = {https://doi.org/10.1145/1394608.1382172},
doi = {10.1145/1394608.1382172},
abstract = {Efficiently utilizing off-chip DRAM bandwidth is a critical issuein designing cost-effective, high-performance chip multiprocessors(CMPs). Conventional memory controllers deliver relativelylow performance in part because they often employ fixed,rigid access scheduling policies designed for average-case applicationbehavior. As a result, they cannot learn and optimizethe long-term performance impact of their scheduling decisions,and cannot adapt their scheduling policies to dynamic workloadbehavior.We propose a new, self-optimizing memory controller designthat operates using the principles of reinforcement learning (RL)to overcome these limitations. Our RL-based memory controllerobserves the system state and estimates the long-term performanceimpact of each action it can take. In this way, the controllerlearns to optimize its scheduling policy on the fly to maximizelong-term performance. Our results show that an RL-basedmemory controller improves the performance of a set of parallelapplications run on a 4-core CMP by 19% on average (upto 33%), and it improves DRAM bandwidth utilization by 22%compared to a state-of-the-art controller.},
journal = {SIGARCH Comput. Archit. News},
month = {jun},
pages = {39–50},
numpages = {12},
keywords = {Memory Systems, Machine Learning, Memory Controller, Reinforcement Learning, Chip Multiprocessors}
}

@inproceedings{10.1145/1629911.1629926,
author = {Lee, Jungseob and Kim, Nam Sung},
title = {Optimizing Throughput of Power- and Thermal-Constrained Multicore Processors Using DVFS and per-Core Power-Gating},
year = {2009},
isbn = {9781605584973},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1629911.1629926},
doi = {10.1145/1629911.1629926},
abstract = {Process variability from a range of sources is growing as technology scales below 65nm, resulting in increasingly nonuniform transistor delay and leakage power both within a die and across dies. As a result, the negative impact of process variations on the maximum operating frequency and the total power consumption of a processor is expected to worsen. Meanwhile, manufacturers have integrated more cores in a single die, substantially improving the throughput of a processor running highly-parallel applications. However, many existing applications do not have high enough parallelism to exploit multiple cores in a die. In this paper, first, we analyze the throughput impact of applying per-core power gating and dynamic voltage and frequency scaling to power- and thermal-constrained multicore processors. To optimize the throughput of the multicore processors running applications with limited parallelism, we exploit power- and thermal-headroom resulted from power-gated idle cores, allowing active cores to increase operating frequency through supply voltage scaling. Our analysis using a 32nm predictive technology model shows that optimizing the number of active cores and operating frequency within power, thermal, and supply voltage scaling limits improves the throughput of a 16-core processor by ~16. Furthermore, we extend our throughput analysis and optimization to consider the impact of within-die process variations leading to core-to-core frequency (and leakage power) variations in a multicore processor. Our analysis shows that exploiting core-to-core frequency variations improves the throughput of a 16-core processor by ~75%.},
booktitle = {Proceedings of the 46th Annual Design Automation Conference},
pages = {47–50},
numpages = {4},
keywords = {DVFS, power gating, multicore processor},
location = {San Francisco, California},
series = {DAC '09}
}

@article{10.1145/1839667.1839670,
author = {Lee, Benjamin C. and Brooks, David},
title = {Applied Inference: Case Studies in Microarchitectural Design},
year = {2010},
issue_date = {September 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {2},
issn = {1544-3566},
url = {https://doi.org/10.1145/1839667.1839670},
doi = {10.1145/1839667.1839670},
abstract = {We propose and apply a new simulation paradigm for microarchitectural design evaluation and optimization. This paradigm enables more comprehensive design studies by combining spatial sampling and statistical inference. Specifically, this paradigm (i) defines a large, comprehensive design space, (ii) samples points from the space for simulation, and (iii) constructs regression models based on sparse simulations. This approach greatly improves the computational efficiency of microarchitectural simulation and enables new capabilities in design space exploration.We illustrate new capabilities in three case studies for a large design space of approximately 260,000 points: (i) Pareto frontier, (ii) pipeline depth, and (iii) multiprocessor heterogeneity analyses. In particular, regression models are exhaustively evaluated to identify Pareto optimal designs that maximize performance for given power budgets. These models enable pipeline depth studies in which all parameters vary simultaneously with depth, thereby more effectively revealing interactions with nondepth parameters. Heterogeneity analysis combines regression-based optimization with clustering heuristics to identify efficient design compromises between similar optimal architectures. These compromises are potential core designs in a heterogeneous multicore architecture. Increasing heterogeneity can improve bips3/w efficiency by as much as 2.4\texttimes{}, a theoretical upper bound on heterogeneity benefits that neglects contention between shared resources as well as design complexity. Collectively these studies demonstrate regression models' ability to expose trends and identify optima in diverse design regions, motivating the application of such models in statistical inference for more effective use of modern simulator infrastructure.},
journal = {ACM Trans. Archit. Code Optim.},
month = {oct},
articleno = {8},
numpages = {37},
keywords = {simulation, statistics, regression, Microarchitecture}
}

@inproceedings{10.1145/1995896.1995927,
author = {Chen, Jian and John, Lizy Kurian},
title = {Predictive Coordination of Multiple On-Chip Resources for Chip Multiprocessors},
year = {2011},
isbn = {9781450301022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1995896.1995927},
doi = {10.1145/1995896.1995927},
abstract = {Efficient on-chip resource management is crucial for Chip Multiprocessors (CMP) to achieve high resource utilization and enforce system-level performance objectives. Existing multiple resource management schemes either focus on intra-core resources or inter-core resources, missing the opportunity for exploiting the interaction between these two level resources. Moreover, these resource management schemes either rely on trial runs or complex on-line machine learning model to search for the appropriate resource allocation, which makes resource management inefficient and expensive. To address these limitations, this paper presents a predictive yet cost effective mechanism for multiple resource management in CMP. It uses a set of hardware-efficient online profilers and an analytical performance model to predict the application's performance with different intra-core and/or inter-core resource allocations. Based on the predicted performance, the resource allocator identifies and enforces near optimum resource partitions for each epoch without any trial runs. The experimental results show that the proposed predictive resource management framework could improve the weighted speedup of the CMP system by an average of 11.6% compared with the equal partition scheme, and 9.3% compared with existing reactive resource management scheme.},
booktitle = {Proceedings of the International Conference on Supercomputing},
pages = {192–201},
numpages = {10},
keywords = {resource management, program characteristics, microprocessor, performance modeling},
location = {Tucson, Arizona, USA},
series = {ICS '11}
}

@article{10.1145/2626401.2626411,
author = {Carroll, Aaron and Heiser, Gernot},
title = {Mobile Multicores: Use Them or Waste Them},
year = {2014},
issue_date = {January 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {1},
issn = {0163-5980},
url = {https://doi.org/10.1145/2626401.2626411},
doi = {10.1145/2626401.2626411},
abstract = {Energy management is a primary consideration in the design of modern smartphones, made more interesting by the recent proliferation of multi-core processors in this space. We investigate how core offlining and DVFS can be used together on these systems to reduce energy consumption. We show that core offlining leads to very modest savings in the best circumstances, with a heavy penalty in others, and show the cause of this to be low per-core idle power. We develop a policy in Linux that exploits this fact, and show that it improves up to 25% on existing implementations.},
journal = {SIGOPS Oper. Syst. Rev.},
month = {may},
pages = {44–48},
numpages = {5}
}

@article{10.1145/2775054.2694373,
author = {Mishra, Nikita and Zhang, Huazhe and Lafferty, John D. and Hoffmann, Henry},
title = {A Probabilistic Graphical Model-Based Approach for Minimizing Energy Under Performance Constraints},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2775054.2694373},
doi = {10.1145/2775054.2694373},
abstract = {In many deployments, computer systems are underutilized -- meaning that applications have performance requirements that demand less than full system capacity. Ideally, we would take advantage of this under-utilization by allocating system resources so that the performance requirements are met and energy is minimized. This optimization problem is complicated by the fact that the performance and power consumption of various system configurations are often application -- or even input -- dependent. Thus, practically, minimizing energy for a performance constraint requires fast, accurate estimations of application-dependent performance and power tradeoffs. This paper investigates machine learning techniques that enable energy savings by learning Pareto-optimal power and performance tradeoffs. Specifically, we propose LEO, a probabilistic graphical model-based learning system that provides accurate online estimates of an application's power and performance as a function of system configuration. We compare LEO to (1) offline learning, (2) online learning, (3) a heuristic approach, and (4) the true optimal solution. We find that LEO produces the most accurate estimates and near optimal energy savings.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {267–281},
numpages = {15},
keywords = {probabilistic graphical models}
}

@inproceedings{10.1145/2815400.2815403,
author = {Hoffmann, Henry},
title = {JouleGuard: Energy Guarantees for Approximate Applications},
year = {2015},
isbn = {9781450338349},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815400.2815403},
doi = {10.1145/2815400.2815403},
abstract = {Energy consumption limits battery life in mobile devices and increases costs for servers and data centers. Approximate computing addresses energy concerns by allowing applications to trade accuracy for decreased energy consumption. Approximation frameworks can guarantee accuracy or performance and generally reduce energy usage; however, they provide no energy guarantees. Such guarantees would be beneficial for users who have a fixed energy budget and want to maximize application accuracy within that budget. We address this need by presenting JouleGuard: a runtime control system that coordinates approximate applications with system resource usage to provide control theoretic formal guarantees of energy consumption, while maximizing accuracy. We implement JouleGuard and test it on three different platforms (a mobile, tablet, and server) with eight different approximate applications created from two different frameworks. We find that JouleGuard respects energy budgets, provides near optimal accuracy, adapts to phases in application workload, and provides better outcomes than application approximation or system resource adaptation alone. JouleGuard is general with respect to the applications and systems it controls, making it a suitable runtime for a number of approximate computing frameworks.},
booktitle = {Proceedings of the 25th Symposium on Operating Systems Principles},
pages = {198–214},
numpages = {17},
keywords = {control theory, adaptive software, dynamic systems},
location = {Monterey, California},
series = {SOSP '15}
}

@article{10.1145/2829950,
author = {Tomusk, Erik and Dubach, Christophe and O’boyle, Michael},
title = {Four Metrics to Evaluate Heterogeneous Multicores},
year = {2015},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {4},
issn = {1544-3566},
url = {https://doi.org/10.1145/2829950},
doi = {10.1145/2829950},
abstract = {Semiconductor device scaling has made single-ISA heterogeneous processors a reality. Heterogeneous processors contain a number of different CPU cores that all implement the same Instruction Set Architecture (ISA). This enables greater flexibility and specialization, as runtime constraints and workload characteristics can influence which core a given workload is run on. A major roadblock to the further development of heterogeneous processors is the lack of appropriate evaluation metrics. Existing metrics can be used to evaluate individual cores, but to evaluate a heterogeneous processor, the cores must be considered as a collective. Without appropriate metrics, it is impossible to establish design goals for processors, and it is difficult to accurately compare two different heterogeneous processors.We present four new metrics to evaluate user-oriented aspects of sets of heterogeneous cores: localized nonuniformity, gap overhead, set overhead, and generality. The metrics consider sets rather than individual cores. We use examples to demonstrate each metric, and show that the metrics can be used to quantify intuitions about heterogeneous cores.},
journal = {ACM Trans. Archit. Code Optim.},
month = {nov},
articleno = {37},
numpages = {25},
keywords = {generality, effective speed, set overhead, gap overhead, Localized nonuniformity, single-ISA}
}

@inproceedings{10.1145/2830772.2830779,
author = {Vamanan, Balajee and Sohail, Hamza Bin and Hasan, Jahangir and Vijaykumar, T. N.},
title = {TimeTrader: Exploiting Latency Tail to Save Datacenter Energy for Online Search},
year = {2015},
isbn = {9781450340342},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2830772.2830779},
doi = {10.1145/2830772.2830779},
abstract = {Online Search (OLS) is a key component of many popular Internet services. Datacenters running OLS consume significant amounts of energy. However, reducing their energy is challenging due to their tight response time requirements. A key aspect of OLS is that each user query goes to all or many of the nodes in the cluster, so that the overall time budget is dictated by the tail of the replies' latency distribution; replies see latency variations both in the network and compute. Previous work proposes to achieve load-proportional energy by slowing down the computation at lower datacenter loads based directly on response times (i.e., at lower loads, the proposal exploits the average slack in the time budget provisioned for the peak load). In contrast, we propose TimeTrader to reduce energy by exploiting the latency slack in the sub-critical replies which arrive before the deadline (e.g., 80% of replies are 3-4x faster than the tail). This slack is present at all loads and subsumes the previous work's load-related slack. While the previous work shifts the leaves' response time distribution to consume the slack at lower loads, TimeTrader reshapes the distribution at all loads by slowing down individual sub-critical nodes without increasing missed deadlines. TimeTrader exploits slack in both the network and compute budgets. Further, TimeTrader leverages Earliest Deadline First scheduling to largely decouple critical requests from the queuing delays of sub-critical requests which can then be slowed down without hurting critical requests. A combination of real-system measurements and at-scale simulations shows that without adding to missed deadlines, TimeTrader saves 15% and 40% energy at 90% and 30% loading, respectively, in a datacenter with 512 nodes, whereas previous work saves 0% and 30%. Further, as a proof-of-concept, we build a small-scale real implementation to evaluate TimeTrader and show 10-30% energy savings.},
booktitle = {Proceedings of the 48th International Symposium on Microarchitecture},
pages = {585–597},
numpages = {13},
keywords = {datacenter, incast, latency tail, online search (OLS), online data-intensive (OLDI) applications},
location = {Waikiki, Hawaii},
series = {MICRO-48}
}

@inproceedings{10.1145/2872362.2872375,
author = {Zhang, Huazhe and Hoffmann, Henry},
title = {Maximizing Performance Under a Power Cap: A Comparison of Hardware, Software, and Hybrid Techniques},
year = {2016},
isbn = {9781450340915},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2872362.2872375},
doi = {10.1145/2872362.2872375},
abstract = {Power and thermal dissipation constrain multicore performance scaling. Modern processors are built such that they could sustain damaging levels of power dissipation, creating a need for systems that can implement processor power caps. A particular challenge is developing systems that can maximize performance within a power cap, and approaches have been proposed in both software and hardware. Software approaches are flexible, allowing multiple hardware resources to be coordinated for maximum performance, but software is slow, requiring a long time to converge to the power target. In contrast, hardware power capping quickly converges to the the power cap, but only manages voltage and frequency, limiting its potential performance. In this work we propose PUPiL, a hybrid software/hardware power capping system. Unlike previous approaches, PUPiL combines hardware's fast reaction time with software's flexibility. We implement PUPiL on real Linux/x86 platform and compare it to Intel's commercial hardware power capping system for both single and multi-application workloads. We find PUPiL provides the same reaction time as Intel's hardware with significantly higher performance. On average, PUPiL outperforms hardware by from 1:18-2:4 depending on workload and power target. Thus, PUPiL provides a promising way to enforce power caps with greater performance than current state-of-the-art hardware-only approaches.},
booktitle = {Proceedings of the Twenty-First International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {545–559},
numpages = {15},
keywords = {decision-tree, power management, adaptive systems},
location = {Atlanta, Georgia, USA},
series = {ASPLOS '16}
}

@inproceedings{10.1145/3035918.3064029,
author = {Van Aken, Dana and Pavlo, Andrew and Gordon, Geoffrey J. and Zhang, Bohan},
title = {Automatic Database Management System Tuning Through Large-Scale Machine Learning},
year = {2017},
isbn = {9781450341974},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3035918.3064029},
doi = {10.1145/3035918.3064029},
abstract = {Database management system (DBMS) configuration tuning is an essential aspect of any data-intensive application effort. But this is historically a difficult task because DBMSs have hundreds of configuration "knobs" that control everything in the system, such as the amount of memory to use for caches and how often data is written to storage. The problem with these knobs is that they are not standardized (i.e., two DBMSs use a different name for the same knob), not independent (i.e., changing one knob can impact others), and not universal (i.e., what works for one application may be sub-optimal for another). Worse, information about the effects of the knobs typically comes only from (expensive) experience.To overcome these challenges, we present an automated approach that leverages past experience and collects new information to tune DBMS configurations: we use a combination of supervised and unsupervised machine learning methods to (1) select the most impactful knobs, (2) map unseen database workloads to previous workloads from which we can transfer experience, and (3) recommend knob settings. We implemented our techniques in a new tool called OtterTune and tested it on two DBMSs. Our evaluation shows that OtterTune recommends configurations that are as good as or better than ones generated by existing tools or a human expert.},
booktitle = {Proceedings of the 2017 ACM International Conference on Management of Data},
pages = {1009–1024},
numpages = {16},
keywords = {machine learning, database tuning, autonomic computing, database management systems},
location = {Chicago, Illinois, USA},
series = {SIGMOD '17}
}

@inbook{10.1145/3173162.3173206,
author = {Wang, Shu and Li, Chi and Hoffmann, Henry and Lu, Shan and Sentosa, William and Kistijantoro, Achmad Imam},
title = {Understanding and Auto-Adjusting Performance-Sensitive Configurations},
year = {2018},
isbn = {9781450349116},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173162.3173206},
abstract = {Modern software systems are often equipped with hundreds to thousands of configurations, many of which greatly affect performance. Unfortunately, properly setting these configurations is challenging for developers due to the complex and dynamic nature of system workload and environment. In this paper, we first conduct an empirical study to understand performance-sensitive configurations and the challenges of setting them in the real-world. Guided by our study, we design a systematic and general control-theoretic framework, SmartConf, to automatically set and dynamically adjust performance-sensitive configurations to meet required operating constraints while optimizing other performance metrics. Evaluation shows that SmartConf is effective in solving real-world configuration problems, often providing better performance than even the best static configuration developers can choose under existing configuration systems.},
booktitle = {Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {154–168},
numpages = {15}
}

@inproceedings{10.1145/3307650.3326633,
author = {Ding, Yi and Mishra, Nikita and Hoffmann, Henry},
title = {Generative and Multi-Phase Learning for Computer Systems Optimization},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3326633},
doi = {10.1145/3307650.3326633},
abstract = {Machine learning and artificial intelligence are invaluable for computer systems optimization: as computer systems expose more resources for management, ML/AI is necessary for modeling these resources' complex interactions. The standard way to incorporate ML/AI into a computer system is to first train a learner to accurately predict the system's behavior as a function of resource usage---e.g., to predict energy efficiency as a function of core usage---and then deploy the learned model as part of a system---e.g., a scheduler. In this paper, we show that (1) continued improvement of learning accuracy may not improve the systems result, but (2) incorporating knowledge of the systems problem into the learning process improves the systems results even though it may not improve overall accuracy. Specifically, we learn application performance and power as a function of resource usage with the systems goal of meeting latency constraints with minimal energy. We propose a novel generative model which improves learning accuracy given scarce data, and we propose a multi-phase sampling technique, which incorporates knowledge of the systems problem. Our results are both positive and negative. The generative model improves accuracy, even for state-of-the-art learning systems, but negatively impacts energy. Multi-phase sampling reduces energy consumption compared to the state-of-the-art, but does not improve accuracy. These results imply that learning for systems optimization may have reached a point of diminishing returns where accuracy improvements have little effect on the systems outcome. Thus we advocate that future work on learning for systems should de-emphasize accuracy and instead incorporate the system problem's structure into the learner.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {39–52},
numpages = {14},
keywords = {resource allocation, real-time systems, machine learning, heterogeneous architectures, energy},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3342195.3387520,
author = {Li, Chi and Wang, Shu and Hoffmann, Henry and Lu, Shan},
title = {Statically Inferring Performance Properties of Software Configurations},
year = {2020},
isbn = {9781450368827},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342195.3387520},
doi = {10.1145/3342195.3387520},
abstract = {Modern software systems often have a huge number of configurations whose performance properties are poorly documented. Unfortunately, obtaining a good understanding of these performance properties is a prerequisite for performance tuning. This paper explores a new approach to discovering performance properties of system configurations: static program analysis. We present a taxonomy of how a configuration might affect performance through program dependencies. Guided by this taxonomy, we design LearnConf, a static analysis tool that identifies which configurations affect what type of performance and how. Our evaluation, which considers hundreds of configurations in four widely used distributed systems, demonstrates that LearnConf can accurately and efficiently identify many configurations' performance properties, and help performance tuning.},
booktitle = {Proceedings of the Fifteenth European Conference on Computer Systems},
articleno = {10},
numpages = {16},
keywords = {static analysis, distributed systems, software configuration, performance},
location = {Heraklion, Greece},
series = {EuroSys '20}
}

@inproceedings{10.1145/3453483.3454109,
author = {Roy, Rohan Basu and Patel, Tirthak and Gadepally, Vijay and Tiwari, Devesh},
title = {Bliss: Auto-Tuning Complex Applications Using a Pool of Diverse Lightweight Learning Models},
year = {2021},
isbn = {9781450383912},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3453483.3454109},
doi = {10.1145/3453483.3454109},
abstract = {As parallel applications become more complex, auto-tuning becomes more desirable, challenging, and time-consuming. We propose, Bliss, a novel solution for auto-tuning parallel applications without requiring apriori information about applications, domain-specific knowledge, or instrumentation. Bliss demonstrates how to leverage a pool of Bayesian Optimization models to find the near-optimal parameter setting 1.64\texttimes{} faster than the state-of-the-art approaches.},
booktitle = {Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
pages = {1280–1295},
numpages = {16},
keywords = {Auto-tuning HPC applications, Parameter tuning},
location = {Virtual, Canada},
series = {PLDI 2021}
}

@inproceedings{10.1145/3468264.3468603,
author = {Ding, Yi and Pervaiz, Ahsan and Carbin, Michael and Hoffmann, Henry},
title = {Generalizable and Interpretable Learning for Configuration Extrapolation},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468603},
doi = {10.1145/3468264.3468603},
abstract = {Modern software applications are increasingly configurable, which puts a burden on users to tune these configurations for their target hardware and workloads. To help users, machine learning techniques can model the complex relationships between software configuration parameters and performance. While powerful, these learners have two major drawbacks: (1) they rarely incorporate prior knowledge and (2) they produce outputs that are not interpretable by users. These limitations make it difficult to (1) leverage information a user has already collected (e.g., tuning for new hardware using the best configurations from old hardware) and (2) gain insights into the learner’s behavior (e.g., understanding why the learner chose different configurations on different hardware or for different workloads). To address these issues, this paper presents two configuration optimization tools, GIL and GIL+, using the proposed generalizable and interpretable learning approaches. To incorporate prior knowledge, the proposed tools (1) start from known configurations, (2) iteratively construct a new linear model, (3) extrapolate better performance configurations from that model, and (4) repeat. Since the base learners are linear models, these tools are inherently interpretable. We enhance this property with a graphical representation of how they arrived at the highest performance configuration. We evaluate GIL and GIL+ by using them to configure Apache Spark workloads on different hardware platforms and find that, compared to prior work, GIL and GIL+ produce comparable, and sometimes even better performance configurations, but with interpretable results.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {728–740},
numpages = {13},
keywords = {machine learning, interpretability, generalizability, Configuration},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@inproceedings{10.1145/381677.381702,
author = {Flautner, Kriszti\'{a}n and Reinhardt, Steve and Mudge, Trevor},
title = {Automatic Performance Setting for Dynamic Voltage Scaling},
year = {2001},
isbn = {1581134223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/381677.381702},
doi = {10.1145/381677.381702},
abstract = {},
booktitle = {Proceedings of the 7th Annual International Conference on Mobile Computing and Networking},
pages = {260–271},
numpages = {12},
location = {Rome, Italy},
series = {MobiCom '01}
}

@inproceedings{10.5555/1855591.1855592,
author = {Ganapathi, Archana and Datta, Kaushik and Fox, Armando and Patterson, David},
title = {A Case for Machine Learning to Optimize Multicore Performance},
year = {2009},
publisher = {USENIX Association},
address = {USA},
abstract = {Multicore architectures have become so complex and diverse that there is no obvious path to achieving good performance. Hundreds of code transformations, compiler flags, architectural features and optimization parameters result in a search space that can take many machinemonths to explore exhaustively. Inspired by successes in the systems community, we apply state-of-the-art machine learning techniques to explore this space more intelligently. On 7-point and 27-point stencil code, our technique takes about two hours to discover a configuration whose performance is within 1% of and up to 18% better than that achieved by a human expert. This factor of 2000 speedup over manual exploration of the auto-tuning parameter space enables us to explore optimizations that were previously off-limits. We believe the opportunity for using machine learning in multicore autotuning is even more promising than the successes to date in the systems literature.},
booktitle = {Proceedings of the First USENIX Conference on Hot Topics in Parallelism},
pages = {1},
numpages = {1},
location = {Berkeley, California},
series = {HotPar'09}
}

@inproceedings{10.5555/2523721.2523732,
author = {Sasaki, Hiroshi and Imamura, Satoshi and Inoue, Koji},
title = {Coordinated Power-Performance Optimization in Manycores},
year = {2013},
isbn = {9781479910212},
publisher = {IEEE Press},
abstract = {Optimizing the performance in multiprogrammed environments, especially for workloads composed of multithreaded programs is a desired feature of runtime management system in future manycore processors. At the same time, power capping capability is required in order to improve the reliability of microprocessor chips while reducing the costs of power supply and thermal budgeting. This paper presents a sophisticated runtime coordinated power-performance management system called C-3PO, which optimizes the performance of manycore processors under a power constraint by controlling two software knobs: thread packing, and dynamic voltage and frequency scaling~(DVFS). The proposed solution distributes the power budget to each program by controlling the workload threads to be executed with appropriate number of cores and operating frequency. The power budget is distributed carefully in different forms (number of allocated cores or operating frequency) depending on the power-performance characteristics of the workload so that each program can effectively convert the power into performance. The proposed system is based on a heuristic algorithm which relies on runtime prediction of power and performance via hardware performance monitoring units. Empirical results on a 64-core platform show that C-3PO well outperforms traditional counterparts across various PARSEC workload mixes.},
booktitle = {Proceedings of the 22nd International Conference on Parallel Architectures and Compilation Techniques},
pages = {51–62},
numpages = {12},
keywords = {thread packing, power-performance optimization, manycore processor, DVFs, power budget allocation, scalability, runtime system},
location = {Edinburgh, Scotland, UK},
series = {PACT '13}
}

@INPROCEEDINGS{1598114,
  author={Li, J. and Martinez, J.F.},
  booktitle={The Twelfth International Symposium on High-Performance Computer Architecture, 2006.}, 
  title={Dynamic power-performance adaptation of parallel computation on chip multiprocessors}, 
  year={2006},
  volume={},
  number={},
  pages={77-87},
  doi={10.1109/HPCA.2006.1598114}}

@INPROCEEDINGS{1635956,

  author={Seungryul Choi and Yeung, D.},

  booktitle={33rd International Symposium on Computer Architecture (ISCA'06)}, 

  title={Learning-Based SMT Processor Resource Distribution via Hill-Climbing}, 

  year={2006},

  volume={},

  number={},

  pages={239-251},

  doi={10.1109/ISCA.2006.25}}

@ARTICLE{4061117,  author={Tesauro, Gerald},  journal={IEEE Internet Computing},   title={Reinforcement Learning in Autonomic Computing: A Manifesto and Case Studies},   year={2007},  volume={11},  number={1},  pages={22-30},  doi={10.1109/MIC.2007.21}}

@INPROCEEDINGS{4228267,
  author={Freeh, Vincent W. and Bletsch, Tyler K. and Rawson, Freeman L.},
  booktitle={2007 IEEE International Parallel and Distributed Processing Symposium}, 
  title={Scaling and Packing on a Chip Multiprocessor}, 
  year={2007},
  volume={},
  number={},
  pages={1-8},
  doi={10.1109/IPDPS.2007.370539}}

@INPROCEEDINGS{4273098,
  author={Lefurgy, Charles and Wang, Xiaorui and Ware, Malcolm},
  booktitle={Fourth International Conference on Autonomic Computing (ICAC'07)}, 
  title={Server-Level Power Control}, 
  year={2007},
  volume={},
  number={},
  pages={4-4},
  doi={10.1109/ICAC.2007.35}}

@INPROCEEDINGS{4343825,
  author={Ge, Rong and Feng, Xizhou and Feng, Wu-chun and Cameron, Kirk W.},
  booktitle={2007 International Conference on Parallel Processing (ICPP 2007)}, 
  title={CPU MISER: A Performance-Directed, Run-Time System for Power-Aware Clusters}, 
  year={2007},
  volume={},
  number={},
  pages={18-18},
  doi={10.1109/ICPP.2007.29}}

@INPROCEEDINGS{4658633,
  author={Wonyoung Kim and Gupta, Meeta S. and Wei, Gu-Yeon and Brooks, David},
  booktitle={2008 IEEE 14th International Symposium on High Performance Computer Architecture}, 
  title={System level analysis of fast, per-core DVFS using on-chip switching regulators}, 
  year={2008},
  volume={},
  number={},
  pages={123-134},
  doi={10.1109/HPCA.2008.4658633}}

@INPROCEEDINGS{4771801,  author={Bitirgen, Ramazan and Ipek, Engin and Martinez, Jose F.},  booktitle={2008 41st IEEE/ACM International Symposium on Microarchitecture},   title={Coordinated management of multiple interacting resources in chip multiprocessors: A machine learning approach},   year={2008},  volume={},  number={},  pages={318-329},  doi={10.1109/MICRO.2008.4771801}}

@INPROCEEDINGS{5695560,  author={Dubach, Christophe and Jones, Timothy M. and Bonilla, Edwin V. and O'Boyle, Michael F.P.},  booktitle={2010 43rd Annual IEEE/ACM International Symposium on Microarchitecture},   title={A Predictive Model for Dynamic Microarchitectural Adaptivity Control},   year={2010},  volume={},  number={},  pages={485-496},  doi={10.1109/MICRO.2010.14}}

@INPROCEEDINGS{6493638,  author={Wu, Weidan and Lee, Benjamin C.},  booktitle={2012 45th Annual IEEE/ACM International Symposium on Microarchitecture},   title={Inferred Models for Dynamic and Sparse Hardware-Software Spaces},   year={2012},  volume={},  number={},  pages={413-424},  doi={10.1109/MICRO.2012.45}}

@INPROCEEDINGS{6522303,  author={Zhu, Yuhao and Reddi, Vijay Janapa},  booktitle={2013 IEEE 19th International Symposium on High Performance Computer Architecture (HPCA)},   title={High-performance and energy-efficient mobile web browsing on big/little systems},   year={2013},  volume={},  number={},  pages={13-24},  doi={10.1109/HPCA.2013.6522303}}

@INPROCEEDINGS{6730744,  author={Yigitbasi, Nezih and Willke, Theodore L. and Liao, Guangdeng and Epema, Dick},  booktitle={2013 IEEE 21st International Symposium on Modelling, Analysis and Simulation of Computer and Telecommunication Systems},   title={Towards Machine Learning-Based Auto-tuning of MapReduce},   year={2013},  volume={},  number={},  pages={11-20},  doi={10.1109/MASCOTS.2013.9}}

@INPROCEEDINGS{6983037,
  author={Kanev, Svilen and Hazelwood, Kim and Wei, Gu-Yeon and Brooks, David},
  booktitle={2014 IEEE International Symposium on Workload Characterization (IISWC)}, 
  title={Tradeoffs between power management and tail latency in warehouse-scale applications}, 
  year={2014},
  volume={},
  number={},
  pages={31-40},
  doi={10.1109/IISWC.2014.6983037}}

@INPROCEEDINGS{7207248,  author={Chen, Chi-Ou and Zhuo, Ye-Qi and Yeh, Chao-Chun and Lin, Che-Min and Liao, Shih-Wei},  booktitle={2015 IEEE International Congress on Big Data},   title={Machine Learning-Based Configuration Parameter Tuning on Hadoop System},   year={2015},  volume={},  number={},  pages={386-392},  doi={10.1109/BigDataCongress.2015.64}}

@ARTICLE{7425206,
  author={Zhan, Xin and Azimi, Reza and Kanev, Svilen and Brooks, David and Reda, Sherief},
  journal={IEEE Computer Architecture Letters}, 
  title={CARB: A C-State Power Management Arbiter for Latency-Critical Workloads}, 
  year={2017},
  volume={16},
  number={1},
  pages={6-9},
  doi={10.1109/LCA.2016.2537802}}

@INPROCEEDINGS{7551432,  author={Zhou, Yanqi and Hoffmann, Henry and Wentzlaff, David},  booktitle={2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture (ISCA)},   title={CASH: Supporting IaaS Customers with a Sub-core Configurable Architecture},   year={2016},  volume={},  number={},  pages={682-694},  doi={10.1109/ISCA.2016.65}}

@INPROCEEDINGS{7851506,  author={Winter, Jonathan A. and Albonesi, David H. and Shoemaker, Christine A.},  booktitle={2010 19th International Conference on Parallel Architectures and Compilation Techniques (PACT)},   title={Scalable thread scheduling and global power management for heterogeneous many-core architectures},   year={2010},  volume={},  number={},  pages={29-39},  doi={}}

@ARTICLE{9248059,
  author={Kang, Ki-Dong and Park, Hyungwon and Park, Gyeongseo and Kim, Daehoon},
  journal={IEEE Access}, 
  title={Co-Adjusting Voltage/Frequency State and Interrupt Rate for Improving Energy-Efficiency of Latency-Critical Applications}, 
  year={2020},
  volume={8},
  number={},
  pages={201028-201039},
  doi={10.1109/ACCESS.2020.3035777}
}

@inproceedings{Dynamo,
 author = {Wu, Qiang and Deng, Qingyuan and Ganesh, Lakshmi and Hsu, Chang-Hong and Jin, Yun and Kumar, Sanjeev and Li, Bin and Meza, Justin and Song, Yee Jiun},
 title = {Dynamo: Facebook's Data Center-wide Power Management System},
 booktitle = {Proceedings of the 43rd International Symposium on Computer Architecture},
 series = {ISCA '16},
 year = {2016},
 isbn = {978-1-4673-8947-1},
 location = {Seoul, Republic of Korea},
 pages = {469--480},
 numpages = {12},
 url = {https://doi.org/10.1109/ISCA.2016.48},
 doi = {10.1109/ISCA.2016.48},
 acmid = {3001187},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA},
 keywords = {data center, management, power},
}

@inproceedings{PerAppPower,
author = {Guliani, Akhil and Swift, Michael M.},
title = {Per-Application Power Delivery},
year = {2019},
isbn = {9781450362818},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302424.3303981},
doi = {10.1145/3302424.3303981},
booktitle = {Proceedings of the Fourteenth EuroSys Conference 2019},
articleno = {5},
numpages = {16},
keywords = {DVFS, Proportional Shares, Power Management},
location = {Dresden, Germany},
series = {EuroSys '19}
}

@inproceedings{Siblingrivalry,
author = {Ansel, Jason and Pacula, Maciej and Wong, Yee Lok and Chan, Cy and Olszewski, Marek and O'Reilly, Una-May and Amarasinghe, Saman},
title = {Siblingrivalry: Online Autotuning through Local Competitions},
year = {2012},
isbn = {9781450314244},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380403.2380425},
doi = {10.1145/2380403.2380425},
abstract = {Modern high performance libraries, such as ATLAS and FFTW, and programming languages, such as PetaBricks, have shown that autotuning computer programs can lead to significant speedups. However, autotuning can be burdensome to the deployment of a program, since the tuning process can take a long time and should be re-run whenever the program, microarchitecture, execution environment, or tool chain changes. Failure to re-autotune programs often leads to widespread use of sub-optimal algorithms. With the growth of cloud computing, where computations can run in environments with unknown load and migrate between different (possibly unknown) microarchitectures, the need for online autotuning has become increasingly important.We present SiblingRivalry, a new model for always-on online autotuning that allows parallel programs to continuously adapt and optimize themselves to their environment. In our system, requests are processed by dividing the available cores in half, and processing two identical requests in parallel on each half. Half of the cores are devoted to a known safe program configuration, while the other half are used for an experimental program configuration chosen by our self-adapting evolutionary algorithm. When the faster configuration completes, its results are returned, and the slower configuration is terminated. Over time, this constant experimentation allows programs to adapt to changing dynamic environments and often outperform the original algorithm that uses the entire system.},
booktitle = {Proceedings of the 2012 International Conference on Compilers, Architectures and Synthesis for Embedded Systems},
pages = {91–100},
numpages = {10},
keywords = {evolutionary algorithm, autotuning, genetic algorithm},
location = {Tampere, Finland},
series = {CASES '12}
}

@inproceedings{SmoothOperator,
 author = {Hsu, Chang-Hong and Deng, Qingyuan and Mars, Jason and Tang, Lingjia},
 title = {SmoothOperator: Reducing Power Fragmentation and Improving Power Utilization in Large-scale Datacenters},
 booktitle = {Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems},
 series = {ASPLOS '18},
 year = {2018},
 isbn = {978-1-4503-4911-6},
 location = {Williamsburg, VA, USA},
 pages = {535--548},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/3173162.3173190},
 doi = {10.1145/3173162.3173190},
 acmid = {3173190},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {datacenter power management, fragmentation, power and energy efficiency},
}

@article{Wu2023,
	doi = {10.1145/3494523},
	url = {https://doi.org/10.1145%2F3494523},
	year = 2023,
	month = {apr},
	publisher = {Association for Computing Machinery ({ACM})},
	volume = {55},
	number = {3},
	pages = {1--39},
	author = {Nan Wu and Yuan Xie},
	title = {A Survey of Machine Learning for Computer Architecture and Systems},
	journal = {{ACM} Computing Surveys}
}

@inproceedings{adrenaline,
  author={C. {Hsu} and Y. {Zhang} and M. A. {Laurenzano} and D. {Meisner} and T. {Wenisch} and J. {Mars} and L. {Tang} and R. G. {Dreslinski}},
  booktitle={2015 IEEE 21st International Symposium on High Performance Computer Architecture (HPCA)}, 
  title={Adrenaline: Pinpointing and reining in tail queries with quick voltage boosting}, 
  year={2015},
  volume={},
  number={},
  pages={271-282},
  }

@misc{arxiv.2112.07010,
  doi = {10.48550/ARXIV.2112.07010},
  url = {https://arxiv.org/abs/2112.07010},
  author = {Dong, Han and Arora, Sanjay and Awad, Yara and Unger, Tommy and Krieger, Orran and Appavoo, Jonathan},
  keywords = {Operating Systems (cs.OS), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Slowing Down for Performance and Energy: An OS-Centric Study in Network Driven Workloads. https://arxiv.org/abs/2112.07010},
  publisher = {arXiv},
  year = {2021},
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International}
}

@inproceedings{bestconfig,
author = {Zhu, Yuqing and Liu, Jianxun and Guo, Mengying and Bao, Yungang and Ma, Wenlong and Liu, Zhuoyue and Song, Kunpeng and Yang, Yingchun},
title = {BestConfig: Tapping the Performance Potential of Systems via Automatic Configuration Tuning},
year = {2017},
isbn = {9781450350280},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127479.3128605},
doi = {10.1145/3127479.3128605},
abstract = {An ever increasing number of configuration parameters are provided to system users. But many users have used one configuration setting across different workloads, leaving untapped the performance potential of systems. A good configuration setting can greatly improve the performance of a deployed system under certain workloads. But with tens or hundreds of parameters, it becomes a highly costly task to decide which configuration setting leads to the best performance. While such task requires the strong expertise in both the system and the application, users commonly lack such expertise.To help users tap the performance potential of systems, we present Best Config, a system for automatically finding a best configuration setting within a resource limit for a deployed system under a given application workload. BestConfig is designed with an extensible architecture to automate the configuration tuning for general systems. To tune system configurations within a resource limit, we propose the divide-and-diverge sampling method and the recursive bound-and-search algorithm. BestConfig can improve the throughput of Tomcat by 75%, that of Cassandra by 63%, that of MySQL by 430%, and reduce the running time of Hive join job by about 50% and that of Spark join job by about 80%, solely by configuration adjustment.},
booktitle = {Proceedings of the 2017 Symposium on Cloud Computing},
pages = {338–350},
numpages = {13},
keywords = {performance optimization, ACT, automatic configuration tuning},
location = {Santa Clara, California},
series = {SoCC '17}
}

@article{caloree,
author = {Mishra, Nikita and Imes, Connor and Lafferty, John D. and Hoffmann, Henry},
title = {CALOREE: Learning Control for Predictable Latency and Low Energy},
year = {2018},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {2},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296957.3173184},
doi = {10.1145/3296957.3173184},
abstract = {Many modern computing systems must provide reliable latency with minimal energy. Two central challenges arise when allocating system resources to meet these conflicting goals: (1) complexity modern hardware exposes diverse resources with complicated interactions and (2) dynamics latency must be maintained despite unpredictable changes in operating environment or input. Machine learning accurately models the latency of complex, interacting resources, but does not address system dynamics; control theory adjusts to dynamic changes, but struggles with complex resource interaction. We therefore propose CALOREE, a resource manager that learns key control parameters to meet latency requirements with minimal energy in complex, dynamic en- vironments. CALOREE breaks resource allocation into two sub-tasks: learning how interacting resources affect speedup, and controlling speedup to meet latency requirements with minimal energy. CALOREE deines a general control system whose parameters are customized by a learning framework while maintaining control-theoretic formal guarantees that the latency goal will be met. We test CALOREE's ability to deliver reliable latency on heterogeneous ARM big.LITTLE architectures in both single and multi-application scenarios. Compared to the best prior learning and control solutions, CALOREE reduces deadline misses by 60% and energy consumption by 13%.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {184–198},
numpages = {15},
keywords = {energy, machine learning, control theory, resource allocation, heterogeneous architectures, real-time systems}
}

@inproceedings{carat,
author = {Oliner, Adam J. and Iyer, Anand P. and Stoica, Ion and Lagerspetz, Eemil and Tarkoma, Sasu},
title = {Carat: Collaborative Energy Diagnosis for Mobile Devices},
year = {2013},
isbn = {9781450320276},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2517351.2517354},
doi = {10.1145/2517351.2517354},
abstract = {We aim to detect and diagnose energy anomalies, abnormally heavy battery use. This paper describes a collaborative black-box method, and an implementation called Carat, for diagnosing anomalies on mobile devices. A client app sends intermittent, coarse-grained measurements to a server, which correlates higher expected energy use with client properties like the running apps, device model, and operating system. The analysis quantifies the error and confidence associated with a diagnosis, suggests actions the user could take to improve battery life, and projects the amount of improvement. During a deployment to a community of more than 500,000 devices, Carat diagnosed thousands of energy anomalies in the wild. Carat detected all synthetically injected anomalies, produced no known instances of false positives, projected the battery impact of anomalies with 95% accuracy, and, on average, increased a user's battery life by 11% after 10 days (compared with 1.9% for the control group).},
booktitle = {Proceedings of the 11th ACM Conference on Embedded Networked Sensor Systems},
articleno = {10},
numpages = {14},
keywords = {analytics, mobile, collaborative, energy, battery, diagnosis},
location = {Roma, Italy},
series = {SenSys '13}
}

@misc{cello,
  doi = {10.48550/ARXIV.2204.04831},
  
  url = {https://arxiv.org/abs/2204.04831},
  
  author = {Ding, Yi and Renda, Alex and Pervaiz, Ahsan and Carbin, Michael and Hoffmann, Henry},
  
  keywords = {Machine Learning (cs.LG), Hardware Architecture (cs.AR), Distributed, Parallel, and Cluster Computing (cs.DC), Performance (cs.PF), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Cello: Efficient Computer Systems Optimization with Predictive Early Termination and Censored Regression},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{cpufreq_governor,
title={{CPU frequency and voltage scaling code in the Linux(TM) kernel}},
author = {{Dominik Brodowski, Nico Golde, Rafael J. Wysocki, Viresh Kumar}},
howpublished = {\url{https://www.kernel.org/doc/Documentation/cpu-freq/governors.txt}}
}

@misc{cpuidle_policy,
title={{CPU Idle Time Management}},
author = {{Rafael J. Wysocki}},
howpublished = {\url{https://www.kernel.org/doc/html/v5.0/admin-guide/pm/cpuidle.html}}
}

@inproceedings{dreamweaver,
author = {Meisner, David and Wenisch, Thomas F.},
title = {DreamWeaver: Architectural Support for Deep Sleep},
year = {2012},
isbn = {9781450307598},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2150976.2151009},
doi = {10.1145/2150976.2151009},
abstract = {Numerous data center services exhibit low average utilization leading to poor energy efficiency. Although CPU voltage and frequency scaling historically has been an effective means to scale down power with utilization, transistor scaling trends are limiting its effectiveness and the CPU is accounting for a shrinking fraction of system power. Recent research advocates the use of full-system idle low-power modes to combat energy losses, as such modes provide the deepest power savings with bounded response time impact. However, the trend towards increasing cores per die is undermining the effectiveness of these sleep modes, particularly for request-parallel data center applications, because the independent idle periods across individual cores are unlikely to align by happenstance.We propose DreamWeaver, architectural support to facilitate deep sleep for request-parallel applications on multicore servers. DreamWeaver comprises two elements: Weave Scheduling, a scheduling policy to coalesce idle and busy periods across cores to create opportunities for system-wide deep sleep; and the Dream Processor, a light-weight co-processor that monitors incoming network traffic and suspended work during sleep to determine when the system must wake. DreamWeaver is based on two key concepts: (1) stall execution and sleep anytime any core is unoccupied, but (2) constrain the maximum time any request may be stalled. Unlike prior scheduling approaches, DreamWeaver will preempt execution to sleep, maximizing time spent at the systems' most efficient operating point. We demonstrate that DreamWeaver can smoothly trade-off bounded, predictable increases in 99th-percentile response time for increasing power savings, and strictly dominates the savings available with voltage and frequency scaling and timeout-based request batching schemes.},
booktitle = {Proceedings of the Seventeenth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {313–324},
numpages = {12},
keywords = {power management, servers},
location = {London, England, UK},
series = {ASPLOS XVII}
}

@inproceedings{dynsleep,
author = {Chou, Chih-Hsun and Wong, Daniel and Bhuyan, Laxmi N.},
title = {DynSleep: Fine-Grained Power Management for a Latency-Critical Data Center Application},
year = {2016},
isbn = {9781450341851},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934583.2934616},
doi = {10.1145/2934583.2934616},
abstract = {Servers running in datacenters are commonly kept underutilized to meet stringent latency targets. Due to poor energy-proportionality in commodity servers, the low utilization results in wasteful power consumption that cost millions of dollars. Applying dynamic power management on datacenter workloads is challenging, especially when tail latency requirements often fall in the sub-millisecond level. The fundamental issue is randomness due to unpredictable request arrival times and request service times. Prior techniques applied per-core DVFS to have fine-grain control of slowing down request processing without violating the tail latency target. However, most commodity servers only support per-core DFS, which greatly limits potential energy saving. In this paper, we propose DynSleep, a fine-grain power management scheme for datacenter workloads through the use of per-core sleep states (C-states). DynSleep dynamically postpones the processing of some requests, creating longer idle periods, which allow the use of deeper C-states to save energy. We design and implement DynSleep with Mem-cached, a popular key-value store application used in datacenters. The experimental results show that DynSleep achieves up to 65\% core power saving, and 27\% better than the per-core DVFS power management scheme, while still satisfying the tail latency constraint. To the best of our knowledge, this is the first work to analyze and develop power management technique with CPU C-states in latency-critical datacenter workloads},
booktitle = {Proceedings of the 2016 International Symposium on Low Power Electronics and Design},
pages = {212–217},
numpages = {6},
keywords = {Power management, quality of service, sleep states, datacenter workload, memcached application, tail latency},
location = {San Francisco Airport, CA, USA},
series = {ISLPED '16}
}

@article{energyproportion,
 author = {{Barroso, Luiz Andr\'{e} and H\"{o}lzle, Urs}},
 title = {The Case for Energy-Proportional Computing},
 year = {2007},
 issue_date = {December 2007},
 publisher = {IEEE Computer Society Press},
 address = {Washington, DC, USA},
 volume = {40},
 number = {12},
 issn = {0018-9162},
 url = {https://doi.org/10.1109/MC.2007.443},
 doi = {10.1109/MC.2007.443},
 journal = {Computer},
 month = dec,
 pages = {33–37},
 numpages = {5},
 keywords = {green computing, energy-proportional computing}
}

@inproceedings{eurosys14,
author = {Leverich, Jacob and Kozyrakis, Christos},
title = {Reconciling High Server Utilization and Sub-Millisecond Quality-of-Service},
year = {2014},
isbn = {9781450327046},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2592798.2592821},
doi = {10.1145/2592798.2592821},
booktitle = {Proceedings of the Ninth European Conference on Computer Systems},
articleno = {4},
numpages = {14},
location = {Amsterdam, The Netherlands},
series = {EuroSys '14}
}

@inproceedings{flicker,
author = {Petrica, Paula and Izraelevitz, Adam and Albonesi, David and Shoemaker, Christine},
year = {2013},
month = {06},
pages = {13-23},
title = {Flicker: A dynamically adaptive architecture for power limited multicore systems},
journal = {Proceedings - International Symposium on Computer Architecture},
doi = {10.1145/2485922.2485924}
}

@article{heracles,
author = {Lo, David and Cheng, Liqun and Govindaraju, Rama and Ranganathan, Parthasarathy and Kozyrakis, Christos},
title = {Heracles: Improving Resource Efficiency at Scale},
year = {2015},
issue_date = {June 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {3S},
issn = {0163-5964},
url = {https://doi.org/10.1145/2872887.2749475},
doi = {10.1145/2872887.2749475},
journal = {SIGARCH Comput. Archit. News},
month = jun,
pages = {450–462},
numpages = {13}
}

@INPROCEEDINGS{heuristics_0,  author={Kim, David H.K. and Imes, Connor and Hoffmann, Henry},  booktitle={2015 IEEE 3rd International Conference on Cyber-Physical Systems, Networks, and Applications},   title={Racing and Pacing to Idle: Theoretical and Empirical Analysis of Energy Optimization Heuristics},   year={2015},  volume={},  number={},  pages={78-85},  doi={10.1109/CPSNA.2015.23}}

@inproceedings{hotpower2008,
author = {Tolia, Niraj and Wang, Zhikui and Marwah, Manish and Bash, Cullen and Ranganathan, Parthasarathy and Zhu, Xiaoyun},
title = {Delivering Energy Proportionality with Non Energy-Proportional Systems: Optimizing the Ensemble},
year = {2008},
publisher = {USENIX Association},
address = {USA},
booktitle = {Proceedings of the 2008 Conference on Power Aware Computing and Systems},
pages = {2},
numpages = {1},
location = {San Diego, California},
series = {HotPower'08}
}

@misc{intel_rapl,
author={Intel},
title={{Intel® 64 and IA-32 Architectures Software Developer’s Manual Volume 3B:System Programming Guide, Part 2}},
howpublished = {\url{https://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-software-developer-\\vol-3b-part-2-manual.pdf}}
}

@inproceedings{koala,
author = {Snowdon, David C. and Le Sueur, Etienne and Petters, Stefan M. and Heiser, Gernot},
title = {Koala: A Platform for OS-Level Power Management},
year = {2009},
isbn = {9781605584829},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1519065.1519097},
doi = {10.1145/1519065.1519097},
abstract = {Managing the power consumption of computing platforms is a complicated problem thanks to a multitude of hardware configuration options and characteristics. Much of the academic research is based on unrealistic assumptions, and has, therefore, seen little practical uptake. We provide an overview of the difficulties facing power management schemes when used in real systems.We present Koala, a platform which uses a pre-characterised model at run-time to predict the performance and energy consumption of a piece of software. An arbitrary policy can then be applied in order to dynamically trade performance and energy consumption. We have implemented this system in a recent Linux kernel, and evaluated it by running a variety of benchmarks on a number of different platforms. Under some conditions, we observe energy savings of 26% for a 1% performance loss.},
booktitle = {Proceedings of the 4th ACM European Conference on Computer Systems},
pages = {289–302},
numpages = {14},
keywords = {modelling, energy, power management, dynamic voltage scaling, operating systems, efficiency, power},
location = {Nuremberg, Germany},
series = {EuroSys '09}
}

@inproceedings{large-scale-mapreduce,
 author = {Chen, Yanpei and Alspaugh, Sara and Borthakur, Dhruba and Katz, Randy},
 title = {Energy Efficiency for Large-Scale MapReduce Workloads with Significant Interactive Analysis},
 year = {2012},
 isbn = {9781450312233},
 publisher = {Association for Computing Machinery},
 address = {New York, NY, USA},
 url = {https://doi.org/10.1145/2168836.2168842},
 doi = {10.1145/2168836.2168842},
 booktitle = {Proceedings of the 7th ACM European Conference on Computer Systems},
 pages = {43–56},
 numpages = {14},
 keywords = {MapReduce, energy efficiency},
 location = {Bern, Switzerland},
 series = {EuroSys ’12}
}

@inproceedings{mem_cocktail,
author = {Deng, Zhaoxia and Zhang, Lunkai and Mishra, Nikita and Hoffmann, Henry and Chong, Frederic T.},
title = {Memory Cocktail Therapy: A General Learning-Based Framework to Optimize Dynamic Tradeoffs in NVMs},
year = {2017},
isbn = {9781450349529},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3123939.3124548},
doi = {10.1145/3123939.3124548},
abstract = {Non-volatile memories (NVMs) have attracted significant interest recently due to their high-density, low static power, and persistence. There are, however, several challenges associated with building practical systems from NVMs, including limited write endurance and long latencies. Researchers have proposed a variety of architectural techniques which can achieve different tradeoffs between lifetime, performance and energy efficiency; however, no individual technique can satisfy requirements for all applications and different objectives. Hence, we propose Memory Cocktail Therapy (MCT), a general, learning-based framework that adaptively chooses the best techniques for the current application and objectives.Specifically, MCT performs four procedures to adapt the techniques to various scenarios. First, MCT formulates a high-dimensional configuration space from all different combinations of techniques. Second, MCT selects primary features from the configuration space with lasso regularization. Third, MCT estimates lifetime, performance and energy consumption using lightweight online predictors (eg. quadratic regression and gradient boosting) and a small set of configurations guided by the selected features. Finally, given the estimation of all configurations, MCT selects the optimal configuration based on the user-defined objectives. As a proof of concept, we test MCT's ability to guarantee different lifetime targets and achieve 95% of maximum performance, while minimizing energy consumption. We find that MCT improves performance by 9.24% and reduces energy by 7.95% compared to the best static configuration. Moreover, the performance of MCT is 94.49% of the ideal configuration with only 5.3% more energy consumption.},
booktitle = {Proceedings of the 50th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {232–244},
numpages = {13},
keywords = {machine learning, modeling, NVM, mellow writes},
location = {Cambridge, Massachusetts},
series = {MICRO-50 '17}
}

@inproceedings{mootaz,
author = {Elnozahy, Mootaz and Kistler, Michael and Rajamony, Ramakrishnan},
title = {Energy Conservation Policies for Web Servers},
year = {2003},
publisher = {USENIX Association},
address = {USA},
abstract = {},
booktitle = {Proceedings of the 4th Conference on USENIX Symposium on Internet Technologies and Systems - Volume 4},
pages = {8},
numpages = {1},
location = {Seattle, WA},
series = {USITS'03}
}

@article{napsac,
 author = {Krioukov, Andrew and Mohan, Prashanth and Alspaugh, Sara and Keys, Laura and Culler, David and Katz, Randy},
 title = {NapSAC: Design and Implementation of a Power-Proportional Web Cluster},
 year = {2011},
 issue_date = {January 2011},
 publisher = {Association for Computing Machinery},
 address = {New York, NY, USA},
 volume = {41},
 number = {1},
 issn = {0146-4833},
 url = {https://doi.org/10.1145/1925861.1925878},
 doi = {10.1145/1925861.1925878},
 journal = {SIGCOMM Comput. Commun. Rev.},
 month = jan,
 pages = {102–108},
 numpages = {7},
 keywords = {heterogenous hardware, energy, web server, data center, web application, cluster, power proportional, power management}
}

@misc{nurd,
  doi = {10.48550/ARXIV.2203.08339},
  url = {https://arxiv.org/abs/2203.08339},
  author = {Ding, Yi and Rao, Avinash and Song, Hyebin and Willett, Rebecca and Hoffmann, Henry},
  keywords = {Machine Learning (cs.LG), Distributed, Parallel, and Cluster Computing (cs.DC), Performance (cs.PF), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {NURD: Negative-Unlabeled Learning for Online Datacenter Straggler Prediction},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}

@inproceedings{oldi-pegasus,
author = {Lo, David and Cheng, Liqun and Govindaraju, Rama and Barroso, Luiz Andr\'{e} and Kozyrakis, Christos},
title = {Towards Energy Proportionality for Large-Scale Latency-Critical Workloads},
year = {2014},
isbn = {9781479943944},
publisher = {IEEE Press},
booktitle = {Proceeding of the 41st Annual International Symposium on Computer Architecuture},
pages = {301–312},
numpages = {12},
location = {Minneapolis, Minnesota, USA},
series = {ISCA '14}
}

@inproceedings{pacingtoidle,
author = {Kim, David H. K. and Imes, Connor and Hoffmann, Henry},
title = {Racing and Pacing to Idle: Theoretical and Empirical Analysis of Energy Optimization Heuristics},
year = {2015},
isbn = {9781467377850},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/CPSNA.2015.23},
doi = {10.1109/CPSNA.2015.23},
abstract = {},
booktitle = {Proceedings of the 2015 IEEE 3rd International Conference on Cyber-Physical Systems, Networks, and Applications},
pages = {78–85},
numpages = {8},
series = {CPSNA '15}
}

@inproceedings{packandcap,
author = {Cochran, Ryan and Hankendi, Can and Coskun, Ayse K. and Reda, Sherief},
title = {{Pack \& Cap: Adaptive DVFS and Thread Packing under Power Caps}},
year = {2011},
isbn = {9781450310536},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2155620.2155641},
doi = {10.1145/2155620.2155641},
abstract = {The ability to cap peak power consumption is a desirable feature in modern data centers for energy budgeting, cost management, and efficient power delivery. Dynamic voltage and frequency scaling (DVFS) is a traditional control knob in the tradeoff between server power and performance. Multi-core processors and the parallel applications that take advantage of them introduce new possibilities for control, wherein workload threads are packed onto a variable number of cores and idle cores enter low-power sleep states. This paper proposes Pack & Cap, a control technique designed to make optimal DVFS and thread packing control decisions in order to maximize performance within a power budget. In order to capture the workload dependence of the performance-power Pareto frontier, a multinomial logistic regression (MLR) classifier is built using a large volume of performance counter, temperature, and power characterization data. When queried during runtime, the classifier is capable of accurately selecting the optimal operating point. We implement and validate this method on a real quad-core system running the PARSEC parallel benchmark suite. When varying the power budget during runtime, Pack & Cap meets power constraints 82% of the time even in the absence of a power measuring device. The addition of thread packing to DVFS as a control knob increases the range of feasible power constraints by an average of 21% when compared to DVFS alone and reduces workload energy consumption by an average of 51.6% compared to existing control techniques that achieve the same power range.},
booktitle = {Proceedings of the 44th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {175–185},
numpages = {11},
location = {Porto Alegre, Brazil},
series = {MICRO-44}
}

@inproceedings{paragon,
author = {Delimitrou, Christina and Kozyrakis, Christos},
title = {Paragon: QoS-Aware Scheduling for Heterogeneous Datacenters},
year = {2013},
isbn = {9781450318709},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2451116.2451125},
doi = {10.1145/2451116.2451125},
abstract = {Large-scale datacenters (DCs) host tens of thousands of diverse applications each day. However, interference between colocated workloads and the difficulty to match applications to one of the many hardware platforms available can degrade performance, violating the quality of service (QoS) guarantees that many cloud workloads require. While previous work has identified the impact of heterogeneity and interference, existing solutions are computationally intensive, cannot be applied online and do not scale beyond few applications.We present Paragon, an online and scalable DC scheduler that is heterogeneity and interference-aware. Paragon is derived from robust analytical methods and instead of profiling each application in detail, it leverages information the system already has about applications it has previously seen. It uses collaborative filtering techniques to quickly and accurately classify an unknown, incoming workload with respect to heterogeneity and interference in multiple shared resources, by identifying similarities to previously scheduled applications. The classification allows Paragon to greedily schedule applications in a manner that minimizes interference and maximizes server utilization. Paragon scales to tens of thousands of servers with marginal scheduling overheads in terms of time or state.We evaluate Paragon with a wide range of workload scenarios, on both small and large-scale systems, including 1,000 servers on EC2. For a 2,500-workload scenario, Paragon enforces performance guarantees for 91% of applications, while significantly improving utilization. In comparison, heterogeneity-oblivious, interference-oblivious and least-loaded schedulers only provide similar guarantees for 14%, 11% and 3% of workloads. The differences are more striking in oversubscribed scenarios where resource efficiency is more critical.},
booktitle = {Proceedings of the Eighteenth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {77–88},
numpages = {12},
keywords = {scheduling, interference, qos, cloud computing, heterogeneity, datacenter},
location = {Houston, Texas, USA},
series = {ASPLOS '13}
}

@inproceedings{peafowl,
author = {Asyabi, Esmail and Bestavros, Azer and Sharafzadeh, Erfan and Zhu, Timothy},
title = {Peafowl: In-Application CPU Scheduling to Reduce Power Consumption of in-Memory Key-Value Stores},
year = {2020},
isbn = {9781450381376},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3419111.3421298},
doi = {10.1145/3419111.3421298},
abstract = {},
booktitle = {Proceedings of the 11th ACM Symposium on Cloud Computing},
pages = {150–164},
numpages = {15},
keywords = {key-value stores, scheduling, power management},
location = {Virtual Event, USA},
series = {SoCC '20}
}

@inproceedings{powercap,
author={P. {Petoumenos} and L. {Mukhanov} and Z. {Wang} and H. {Leather} and D. S. {Nikolopoulos}},
booktitle={2015 IEEE 21st International Conference on Parallel and Distributed Systems (ICPADS)}, 
title={Power Capping: What Works, What Does Not}, 
year={2015},
volume={},
number={},
pages={525-534},
doi={10.1109/ICPADS.2015.72}
}

@inproceedings{powernap,
author = {Meisner, David and Gold, Brian T. and Wenisch, Thomas F.},
title = {PowerNap: Eliminating Server Idle Power},
year = {2009},
isbn = {9781605584065},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1508244.1508269},
doi = {10.1145/1508244.1508269},
booktitle = {Proceedings of the 14th International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {205–216},
numpages = {12},
keywords = {power management, servers},
location = {Washington, DC, USA},
series = {ASPLOS XIV}
}

@inproceedings{quasar,
author = {Delimitrou, Christina and Kozyrakis, Christos},
title = {Quasar: Resource-Efficient and QoS-Aware Cluster Management},
year = {2014},
isbn = {9781450323055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541940.2541941},
doi = {10.1145/2541940.2541941},
abstract = {Cloud computing promises flexibility and high performance for users and high cost-efficiency for operators. Nevertheless, most cloud facilities operate at very low utilization, hurting both cost effectiveness and future scalability.We present Quasar, a cluster management system that increases resource utilization while providing consistently high application performance. Quasar employs three techniques. First, it does not rely on resource reservations, which lead to underutilization as users do not necessarily understand workload dynamics and physical resource requirements of complex codebases. Instead, users express performance constraints for each workload, letting Quasar determine the right amount of resources to meet these constraints at any point. Second, Quasar uses classification techniques to quickly and accurately determine the impact of the amount of resources (scale-out and scale-up), type of resources, and interference on performance for each workload and dataset. Third, it uses the classification results to jointly perform resource allocation and assignment, quickly exploring the large space of options for an efficient way to pack workloads on available resources. Quasar monitors workload performance and adjusts resource allocation and assignment when needed. We evaluate Quasar over a wide range of workload scenarios, including combinations of distributed analytics frameworks and low-latency, stateful services, both on a local cluster and a cluster of dedicated EC2 servers. At steady state, Quasar improves resource utilization by 47% in the 200-server EC2 cluster, while meeting performance constraints for workloads of all types.},
booktitle = {Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {127–144},
numpages = {18},
keywords = {datacenters, cluster management, cloud computing, resource efficiency, resource allocation and assignment, quality of service},
location = {Salt Lake City, Utah, USA},
series = {ASPLOS '14}
}

@INPROCEEDINGS{rubik,
  author={H. {Kasture} and D. B. {Bartolini} and N. {Beckmann} and D. {Sanchez}},
  booktitle={2015 48th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)}, 
  title={Rubik: Fast analytical power management for latency-critical systems}, 
  year={2015},
  volume={},
  number={},
  pages={598-610},}

@inproceedings{slowdownorsleep,
author = {Le Sueur, Etienne and Heiser, Gernot},
title = {Slow down or Sleep, That is the Question},
year = {2011},
publisher = {USENIX Association},
address = {USA},
abstract = {Energy consumption has become a major concern for all computing systems, from servers in data-centres to mobile phones. Processor manufacturers have reacted to this by implementing power-management mechanisms in the hardware and researchers have investigated how operating systems can make use of those mechanisms to minimise energy consumption. Much of this research has focused on a single class of systems and compute-intensive workloads.Missing is an examination of how much energy can actually be saved when running realistic workloads on different classes of systems. This paper compares the effects of using dynamic voltage and frequency scaling (DVFS) and sleep states on platforms using server, desktop and embedded processors. It also analyses workloads that represent real-world uses of those systems. In these circumstances, we find that usage of power-management mechanisms is not clear-cut, and that it is critical to analyse the system as a whole, including the workload, to determine whether using mechanisms such as DVFS will be effective at reducing energy consumption.},
booktitle = {Proceedings of the 2011 USENIX Conference on USENIX Annual Technical Conference},
pages = {16},
numpages = {1},
location = {Portland, OR},
series = {USENIXATC'11}
}

@inproceedings{warehouse-power,
 author = {Fan, Xiaobo and Weber, Wolf-Dietrich and Barroso, Luiz Andre},
 title = {Power Provisioning for a Warehouse-Sized Computer},
 year = {2007},
 isbn = {9781595937063},
 publisher = {Association for Computing Machinery},
 address = {New York, NY, USA},
 url = {https://doi.org/10.1145/1250662.1250665},
 doi = {10.1145/1250662.1250665},
 booktitle = {Proceedings of the 34th Annual International Symposium on Computer Architecture},
 pages = {13–23},
 numpages = {11},
 keywords = {power provisioning, power modeling, energy efficiency},
 location = {San Diego, California, USA},
 series = {ISCA ’07}
}

@inproceedings{zygos,
 author = {Prekas, George and Kogias, Marios and Bugnion, Edouard},
 title = {ZygOS: Achieving Low Tail Latency for Microsecond-Scale Networked Tasks},
 year = {2017},
 isbn = {9781450350853},
 publisher = {Association for Computing Machinery},
 address = {New York, NY, USA},
 url = {https://doi.org/10.1145/3132747.3132780},
 doi = {10.1145/3132747.3132780},
 booktitle = {Proceedings of the 26th Symposium on Operating Systems Principles},
 pages = {325–341},
 numpages = {17},
 keywords = {Tail latency, Microsecond-scale computing},
 location = {Shanghai, China},
 series = {SOSP ’17}
}

