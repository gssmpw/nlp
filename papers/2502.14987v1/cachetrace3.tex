\subsection{Applicability to \textit{cache-trace}}
\label{sec:cachetrace2}
This section presents the results of running our controller against a publicly available KV store workload trace (\textit{cache-trace}~\cite{cacheWorkload-OSDI20}) which exhibits the stable demand curve behavior for our controller. 

\subsubsection{Experimental Setup}
We used the same infrastructure of our study (\cref{sec:study}) but modified the \textit{mutilate} workload generator to generate QPSes following from \textit{cache-trace} instead: first, we extracted a 24-hour sequence of QPS rates from a single trace and binned the data into hourly divisions to capture the mean QPS rate at an hourly basis. As cache-trace QPS rates were often in the tens of thousands of QPS as it was running on limited vCPUs, we scaled up the rates to match our hardware capability. However, \ref{sec:open2} shows that even at low QPS rates where DVFS is fixed at the lowest CPU frequency, ITR-delay can still be used to further reduce energy use. Therefore, we then generate these scaled-up mean QPSes to our live memcached server for which we capture energy-per-second measurements over the entire 24-hour period. 

The controller is configured to trigger its periodic measurements at an hourly rate and run Bayesian optimization for a default of 30 trials - this is due to overheads in our single-thread Python package; which takes around 5 minutes to run. In contrast to our initial energy study (\cref{sec:study}), which was limited to only using up to 340 (ITR-delay,  DVFS) pairs due to experimental scope, our controller allows Bayesian optimization to choose from all available ITR-delay, and DVFS values (a total of \textit{2 million} possible combinations). 

%\paragraph*{}
We evaluate our controller by comparing the energy and performance behavior of five different system configurations:
\begin{itemize}
    \item \textbf{Linux}: Operating in its default state, where the dynamic ITR-delay and DVFS  algorithms are enabled.
    \item \textbf{Linux-BayOp and EbbRT-BayOp}: Operating with Bayesian Optimization to tune both ITR-delay and DVFS, with a target of minimizing overall energy use while maintaining SLA objectives.
    \item \textbf{Linux-DVFS-BayOp and Linux-ITR-BayOp}: Operating with Bayesian Optimization to tune only one of the two settings. We were motivated to explore these configurations to better understand the limitations of the two hardware mechanisms individually. \textit{Linux-DVFS-BayOp} tunes DVFS while enabling the dynamic ITR-delay algorithm. \textit{Linux-ITR-BayOp} tunes ITR-delay while enabling the dynamic DVFS algorithm.
\end{itemize}


\subsubsection{Evaluation}
We evaluate our controller's energy impact across two
applications, namely memcached and silo, in both Linux and EbbRT\footnote{The controller's penalty can also be modified to minimize latency, details can be found in \cref{sec:appendix}}. Silo~\cite{mcdsilo, zygos} is a compute and memory-intensive application that is extended with a web front-end such that every request triggers a corresponding set of TPC-C transactions on an in-memory database~\cite{silo}. We ported Silo to EbbRT and the workload mix and SLA constraints of Silo follow from those used in memcached.


\paragraph{Memcached Results}
Fig.~\ref{fig:mcd_bayop} illustrates our controllers evaluation against three different SLA objectives: 99\% latency < 500 $\mu$s, 90\% latency < 500 $\mu$s, and a even more stringent 99\% latency < 200 $\mu$s. The QPS values, shown on the right, change on an hourly basis, as shown by black line segments. At the beginning of each hourly QPS change, we see spikes in energy usage of \textit{*-BayOp} systems which results from the Bayesian Optimization process searching through (ITR-delay,  DVFS) settings on the memcached server to meet its optimization objective. After this initial energy spike, the system settles into a steady energy consumption state until the next hourly trigger. A key result of this application is the importance of using both ITR-delay and DVFS to meet SLA objectives for optimizing energy efficiency rather than individually.

\begin{figure}[!htb]
\centering
\begin{subfigure}{.45\textwidth}
  \centering
  \includegraphics[width=\linewidth]{bayesopt_mcd_lat500_per99_energy.png}
\end{subfigure}
\begin{subfigure}{.45\textwidth}
  \centering
  \includegraphics[width=\linewidth]{bayesopt_mcd_lat500_per90_energy.png}
\end{subfigure}
\begin{subfigure}{.45\textwidth}
  \centering
  \includegraphics[width=\linewidth]{bayesopt_mcd_lat200_per99_energy.png}
\end{subfigure}%
\caption{\small \texttt{BayOp} applied to memcached; the \textbf{QPS} label is shown on the right side of the graph and the QPS lines show the different offered loads on a per-hour basis. We present results from different SLA objectives studied and illustrate the measured power (energy/second) on the Y-axis as QPS changes across the five system configurations studied over 24 hours (X-axis).}
\label{fig:mcd_bayop}
\vspace{-0.1in}
\end{figure}

\begin{figure*}[ht!]
\centering
\begin{subfigure}{.49\textwidth}
  \includegraphics[width=\linewidth]{bayesopt_mcdsilo_lat500_per99_energy.png}
\end{subfigure}
\begin{subfigure}{.49\textwidth}
  \includegraphics[width=\linewidth]{bayesopt_mcdsilo_lat500_per90_energy.png}
\end{subfigure}
\caption{\small Controller applied to \textit{cache-trace} for Silo. We show two different SLA objectives. The \textbf{QPS} line shows the change in QPS offered load on a per-hour basis. The consumed power (energy/second) of each system configuration on the Y-axis is shown over 24 hours on the X-axis.}
\label{fig:mcdsilo_bayop}
\vspace{-0.15in}
\end{figure*}

\begin{figure}[h!]
\centering
    \includegraphics[width=0.45\textwidth]{bayesopt_mcd_lat200_per99_lat.png}
    \caption{\small Measured 99\% latency across Linux for an SLA of 200 $\mu$s. The latency is shown on a per-hour basis due to how \textit{mutilate} reports its resultant latency measurements. We find that \textit{Linux-DVFS-BayOp} often violates the SLA which suggests only tuning DVFS is not enough to achieve stable system behavior.}
    \label{fig:bayesopt_mcd_lat200_per99_lat}
    \vspace{-0.25in}
\end{figure}

We find that, for an SLA objective of 99\% latency < 500 $\mu$s, \textit{Linux-BayOp} can result in energy savings of up to 50\% over \textit{Linux}. Relaxing the SLA objective to 90\% < 500 $\mu$s enables our controller to find (ITR-delay,  DVFS) configurations that yield even more energy savings of over 60\%. At the most stringent SLA of 99\% latency < 200 $\mu$s, our controller can still adapt while enabling energy savings of up to 30\%. 
The energy savings of \textit{EbbRT-BayOp} are similar to those found in our energy study of memcached (Fig.~\ref{fig:mcd_overview}). Our controller is robust enough to adapt to the software stack of EbbRT and find energy-efficient configurations that consistently result in the lowest energy use (over 2X lower than \textit{Linux}). The measured energy-per-second variability of EbbRT is often lower in contrast to that of Linux (indicated by the thinner red plot in Fig.~\ref{fig:mcd_bayop}), a byproduct of EbbRT's simplified and more optimized network paths.

For \textit{Linux-ITR-BayOp}, allowing our controller to tune only ITR-delay still generally improved energy savings over \textit{Linux}. However, for a stringent SLA of 99\% latency < 200 $\mu$s, the reduced SLA headroom prevents the controller from trading off latency for energy as effectively as it can when tuning alongside DVFS. At the lower QPSes, \textit{Linux-ITR-BayOp} performed worse than \textit{Linux}.

Allowing our controller to tune only DVFS (\textit{Linux-DVFS-BayOp}) results in energy savings comparable with \textit{Linux-BayOp} across SLA objectives. This is further supported by \ref{sec:open2} which illustrates the significant influence of DVFS on overall energy consumption. However, though it may seem that under a more stringent SLA of 99\% latency < 200 $\mu$s, \textit{Linux-DVFS-BayOp} results in the highest energy savings, we found instances where the measured 99\% latency violated the SLA of 200 $\mu$s, as shown in Fig.~\ref{fig:bayesopt_mcd_lat200_per99_lat}; revealing the weakness of relying on DVFS only.

\begin{figure*}[!htb]
\begin{subfigure}{\textwidth}
  \includegraphics[width=\linewidth]{tail1.pdf}
\end{subfigure}
\begin{subfigure}{\textwidth}
  \includegraphics[width=\linewidth]{tail2.pdf}
\end{subfigure}
\caption{\small This figure illustrates the energy use of each application (\textbf{APP}) for each of the hardware platforms (\textbf{NODE: N0 to N3}). The energy is normalized (Y-axis) against Linux default, where lower is better. For each \textbf{APP}, we use two representative offered loads which are 40\% and 80\% of the measured \textbf{Peak Load} of Linux default. Within each representative offered load, we also selected two \textbf{SLAs} (as indicated by \textbf{/////} and \textbf{.....}) for the application to meet while our controller is optimizing its energy efficiency.}
\label{fig:xl170}
\vspace{-0.1in}
\end{figure*}
\paragraph{Silo Results}
We selected another trace from \textit{cache-trace} that was akin to a more computationally intense server. Fig.~\ref{fig:mcdsilo_bayop} shows that the trace peak QPS rates are often lower than those of Fig.~\ref{fig:mcd_bayop} (peak 250K QPS versus 750K QPS). Fig.~\ref{fig:mcdsilo_bayop} does not show results for SLA of 99\% latency < 200 $\mu$s, as the inherent computational cost of Silo's TPC-C transactions resulted in a lower bound of measured latency values that were consistently greater than the SLA objective of 200 $\mu$s. A key result of this application is that it helps expose in computationally intensive cases the limitation of ITR-delay to affect energy savings.

Fig.~\ref{fig:mcdsilo_bayop} illustrates that even for a computationally intensive application with different SLA objectives, \textit{Linux-BayOp} was able to find (ITR-delay,  DVFS) settings that enable 30\% energy savings in Linux for various QPS rates and higher winnings when the SLA is relaxed to 90\% latency < 500 $\mu$s.

The controller was able to adapt to a different OS and application stack and found configurations of \textit{EbbRT-BayOp} that consistently had the lowest energy use over Linux. In contrast to Fig.~\ref{fig:mcd_bayop}, one can see larger variations in energy saved from one QPS to the next (more hilly behavior). This can be partly attributed to the complicated database work that must now be done per request.

In contrast to memcached, we find that tuning ITR-delay alone (\textit{Linux-ITR-BayOp}) while enabling Linux's default DVFS mechanism is largely ineffective at reducing energy. This is likely due to the increased computational cost for each request which limits the potential energy savings gained from interrupt coalescing and prolonged sleep states that are induced by the ITR-delay mechanism.

We find that tuning DVFS alone (\textit{Linux-DVFS-BayOp}) while enabling Linux's default ITR-delay mechanism works surprisingly well for Silo and, in most cases, achieves a slight energy saving over \textit{Linux-BayOp}. This result suggests an interesting compromise between enabling a degree of energy savings that controlling ITR-delay provides to a computationally-driven network application versus abandoning ITR-delay control so that Bayesian optimization can focus on tuning DVFS to maximize energy savings.
