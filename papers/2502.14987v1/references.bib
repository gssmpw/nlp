@inproceedings{cloudlab,
    title     = "The Design and Operation of {CloudLab}",
    author    = "Dmitry Duplyakin and Robert Ricci and Aleksander Maricq and Gary Wong and Jonathon Duerig and Eric Eide and Leigh Stoller and Mike Hibler and David Johnson and Kirk Webb and Aditya Akella and Kuangching Wang and Glenn Ricart and Larry Landweber and Chip Elliott and Michael Zink and Emmanuel Cecchet and Snigdhaswin Kar and Prabodh Mishra",
    booktitle = "Proceedings of the {USENIX} Annual Technical Conference (ATC)",
    pages     = "1--14",
    year      = 2019,
    month     = jul,
    url       = "https://www.flux.utah.edu/paper/duplyakin-atc19"
}

@INPROCEEDINGS{tailbench,
  author={Kasture, Harshad and Sanchez, Daniel},
  booktitle={2016 IEEE International Symposium on Workload Characterization (IISWC)}, 
  title={Tailbench: a benchmark suite and evaluation methodology for latency-critical applications}, 
  year={2016},
  volume={},
  number={},
  pages={1-10},
  doi={10.1109/IISWC.2016.7581261}}

@ARTICLE{9248059,
  author={Kang, Ki-Dong and Park, Hyungwon and Park, Gyeongseo and Kim, Daehoon},
  journal={IEEE Access}, 
  title={Co-Adjusting Voltage/Frequency State and Interrupt Rate for Improving Energy-Efficiency of Latency-Critical Applications}, 
  year={2020},
  volume={8},
  number={},
  pages={201028-201039},
  doi={10.1109/ACCESS.2020.3035777}
}

@inproceedings{zeus-nsdi23,
    title     = {Zeus: Understanding and Optimizing {GPU} Energy Consumption of {DNN} Training},
    author    = {Jie You and Jae-Won Chung and Mosharaf Chowdhury},
    booktitle = {USENIX NSDI},
    year      = {2023}
}


@inproceedings {201567,
author = {Omid Alipourfard and Hongqiang Harry Liu and Jianshu Chen and Shivaram Venkataraman and Minlan Yu and Ming Zhang},
title = {{CherryPick}: Adaptively Unearthing the Best Cloud Configurations for Big Data Analytics},
booktitle = {14th USENIX Symposium on Networked Systems Design and Implementation (NSDI 17)},
year = {2017},
isbn = {978-1-931971-37-9},
address = {Boston, MA},
pages = {469--482},
url = {https://www.usenix.org/conference/nsdi17/technical-sessions/presentation/alipourfard},
publisher = {USENIX Association},
month = mar,
}

@INPROCEEDINGS{clite,  author={Patel, Tirthak and Tiwari, Devesh},  booktitle={2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)},   title={CLITE: Efficient and QoS-Aware Co-Location of Multiple Latency-Critical Jobs for Warehouse Scale Computers},   year={2020},  volume={},  number={},  pages={193-206},  doi={10.1109/HPCA47549.2020.00025}}

@INPROCEEDINGS{9124618,

  author={Nardi, Luigi and Souza, Artur and Koeplinger, David and Olukotun, Kunle},

  booktitle={2019 IEEE 27th International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS)}, 

  title={HyperMapper: a Practical Design Space Exploration Framework}, 

  year={2019},

  volume={},

  number={},

  pages={425-426},

  doi={10.1109/MASCOTS.2019.00053}}


@misc{cello,
  doi = {10.48550/ARXIV.2204.04831},
  url = {https://arxiv.org/abs/2204.04831},
  author = {Ding, Yi and Renda, Alex and Pervaiz, Ahsan and Carbin, Michael and Hoffmann, Henry},
  keywords = {Machine Learning (cs.LG), Hardware Architecture (cs.AR), Distributed, Parallel, and Cluster Computing (cs.DC), Performance (cs.PF), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Cello: Efficient Computer Systems Optimization with Predictive Early Termination and Censored Regression},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}

@inproceedings{10.5555/2523721.2523732,
author = {Sasaki, Hiroshi and Imamura, Satoshi and Inoue, Koji},
title = {Coordinated Power-Performance Optimization in Manycores},
year = {2013},
isbn = {9781479910212},
publisher = {IEEE Press},
abstract = {Optimizing the performance in multiprogrammed environments, especially for workloads composed of multithreaded programs is a desired feature of runtime management system in future manycore processors. At the same time, power capping capability is required in order to improve the reliability of microprocessor chips while reducing the costs of power supply and thermal budgeting. This paper presents a sophisticated runtime coordinated power-performance management system called C-3PO, which optimizes the performance of manycore processors under a power constraint by controlling two software knobs: thread packing, and dynamic voltage and frequency scaling~(DVFS). The proposed solution distributes the power budget to each program by controlling the workload threads to be executed with appropriate number of cores and operating frequency. The power budget is distributed carefully in different forms (number of allocated cores or operating frequency) depending on the power-performance characteristics of the workload so that each program can effectively convert the power into performance. The proposed system is based on a heuristic algorithm which relies on runtime prediction of power and performance via hardware performance monitoring units. Empirical results on a 64-core platform show that C-3PO well outperforms traditional counterparts across various PARSEC workload mixes.},
booktitle = {Proceedings of the 22nd International Conference on Parallel Architectures and Compilation Techniques},
pages = {51–62},
numpages = {12},
keywords = {thread packing, power-performance optimization, manycore processor, DVFs, power budget allocation, scalability, runtime system},
location = {Edinburgh, Scotland, UK},
series = {PACT '13}
}

@inproceedings{10.1145/3453483.3454109,
author = {Roy, Rohan Basu and Patel, Tirthak and Gadepally, Vijay and Tiwari, Devesh},
title = {Bliss: Auto-Tuning Complex Applications Using a Pool of Diverse Lightweight Learning Models},
year = {2021},
isbn = {9781450383912},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3453483.3454109},
doi = {10.1145/3453483.3454109},
abstract = {As parallel applications become more complex, auto-tuning becomes more desirable, challenging, and time-consuming. We propose, Bliss, a novel solution for auto-tuning parallel applications without requiring apriori information about applications, domain-specific knowledge, or instrumentation. Bliss demonstrates how to leverage a pool of Bayesian Optimization models to find the near-optimal parameter setting 1.64\texttimes{} faster than the state-of-the-art approaches.},
booktitle = {Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
pages = {1280–1295},
numpages = {16},
keywords = {Auto-tuning HPC applications, Parameter tuning},
location = {Virtual, Canada},
series = {PLDI 2021}
}

@inproceedings{10.1145/3342195.3387520,
author = {Li, Chi and Wang, Shu and Hoffmann, Henry and Lu, Shan},
title = {Statically Inferring Performance Properties of Software Configurations},
year = {2020},
isbn = {9781450368827},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342195.3387520},
doi = {10.1145/3342195.3387520},
abstract = {Modern software systems often have a huge number of configurations whose performance properties are poorly documented. Unfortunately, obtaining a good understanding of these performance properties is a prerequisite for performance tuning. This paper explores a new approach to discovering performance properties of system configurations: static program analysis. We present a taxonomy of how a configuration might affect performance through program dependencies. Guided by this taxonomy, we design LearnConf, a static analysis tool that identifies which configurations affect what type of performance and how. Our evaluation, which considers hundreds of configurations in four widely used distributed systems, demonstrates that LearnConf can accurately and efficiently identify many configurations' performance properties, and help performance tuning.},
booktitle = {Proceedings of the Fifteenth European Conference on Computer Systems},
articleno = {10},
numpages = {16},
keywords = {static analysis, distributed systems, software configuration, performance},
location = {Heraklion, Greece},
series = {EuroSys '20}
}

@misc{nurd,
  doi = {10.48550/ARXIV.2203.08339},
  url = {https://arxiv.org/abs/2203.08339},
  author = {Ding, Yi and Rao, Avinash and Song, Hyebin and Willett, Rebecca and Hoffmann, Henry},
  keywords = {Machine Learning (cs.LG), Distributed, Parallel, and Cluster Computing (cs.DC), Performance (cs.PF), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {NURD: Negative-Unlabeled Learning for Online Datacenter Straggler Prediction},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}

@ARTICLE{4061117,  author={Tesauro, Gerald},  journal={IEEE Internet Computing},   title={Reinforcement Learning in Autonomic Computing: A Manifesto and Case Studies},   year={2007},  volume={11},  number={1},  pages={22-30},  doi={10.1109/MIC.2007.21}}

@inproceedings{bestconfig,
author = {Zhu, Yuqing and Liu, Jianxun and Guo, Mengying and Bao, Yungang and Ma, Wenlong and Liu, Zhuoyue and Song, Kunpeng and Yang, Yingchun},
title = {BestConfig: Tapping the Performance Potential of Systems via Automatic Configuration Tuning},
year = {2017},
isbn = {9781450350280},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127479.3128605},
doi = {10.1145/3127479.3128605},
abstract = {An ever increasing number of configuration parameters are provided to system users. But many users have used one configuration setting across different workloads, leaving untapped the performance potential of systems. A good configuration setting can greatly improve the performance of a deployed system under certain workloads. But with tens or hundreds of parameters, it becomes a highly costly task to decide which configuration setting leads to the best performance. While such task requires the strong expertise in both the system and the application, users commonly lack such expertise.To help users tap the performance potential of systems, we present Best Config, a system for automatically finding a best configuration setting within a resource limit for a deployed system under a given application workload. BestConfig is designed with an extensible architecture to automate the configuration tuning for general systems. To tune system configurations within a resource limit, we propose the divide-and-diverge sampling method and the recursive bound-and-search algorithm. BestConfig can improve the throughput of Tomcat by 75%, that of Cassandra by 63%, that of MySQL by 430%, and reduce the running time of Hive join job by about 50% and that of Spark join job by about 80%, solely by configuration adjustment.},
booktitle = {Proceedings of the 2017 Symposium on Cloud Computing},
pages = {338–350},
numpages = {13},
keywords = {performance optimization, ACT, automatic configuration tuning},
location = {Santa Clara, California},
series = {SoCC '17}
}

@inproceedings{10.1145/2872362.2872375,
author = {Zhang, Huazhe and Hoffmann, Henry},
title = {Maximizing Performance Under a Power Cap: A Comparison of Hardware, Software, and Hybrid Techniques},
year = {2016},
isbn = {9781450340915},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2872362.2872375},
doi = {10.1145/2872362.2872375},
abstract = {Power and thermal dissipation constrain multicore performance scaling. Modern processors are built such that they could sustain damaging levels of power dissipation, creating a need for systems that can implement processor power caps. A particular challenge is developing systems that can maximize performance within a power cap, and approaches have been proposed in both software and hardware. Software approaches are flexible, allowing multiple hardware resources to be coordinated for maximum performance, but software is slow, requiring a long time to converge to the power target. In contrast, hardware power capping quickly converges to the the power cap, but only manages voltage and frequency, limiting its potential performance. In this work we propose PUPiL, a hybrid software/hardware power capping system. Unlike previous approaches, PUPiL combines hardware's fast reaction time with software's flexibility. We implement PUPiL on real Linux/x86 platform and compare it to Intel's commercial hardware power capping system for both single and multi-application workloads. We find PUPiL provides the same reaction time as Intel's hardware with significantly higher performance. On average, PUPiL outperforms hardware by from 1:18-2:4 depending on workload and power target. Thus, PUPiL provides a promising way to enforce power caps with greater performance than current state-of-the-art hardware-only approaches.},
booktitle = {Proceedings of the Twenty-First International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {545–559},
numpages = {15},
keywords = {decision-tree, power management, adaptive systems},
location = {Atlanta, Georgia, USA},
series = {ASPLOS '16}
}


@inproceedings{10.1145/1229428.1229479,
author = {Lee, Benjamin C. and Brooks, David M. and de Supinski, Bronis R. and Schulz, Martin and Singh, Karan and McKee, Sally A.},
title = {Methods of Inference and Learning for Performance Modeling of Parallel Applications},
year = {2007},
isbn = {9781595936028},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1229428.1229479},
doi = {10.1145/1229428.1229479},
abstract = {Increasing system and algorithmic complexity combined with a growing number of tunable application parameters pose significant challenges for analytical performance modeling. We propose a series of robust techniques to address these challenges. In particular, we apply statistical techniques such as clustering, association, and correlation analysis, to understand the application parameter space better. We construct and compare two classes of effective predictive models: piecewise polynomial regression and artifical neural networks. We compare these techniques with theoretical analyses and experimental results. Overall, both regression and neural networks are accurate with median error rates ranging from 2.2 to 10.5 percent. The comparable accuracy of these models suggest differentiating features will arise from ease of use, transparency, and computational efficiency.},
booktitle = {Proceedings of the 12th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {249–258},
numpages = {10},
keywords = {regression, numerical methods, performance prediction, statistics, neural networks},
location = {San Jose, California, USA},
series = {PPoPP '07}
}

@INPROCEEDINGS{7551432,  author={Zhou, Yanqi and Hoffmann, Henry and Wentzlaff, David},  booktitle={2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture (ISCA)},   title={CASH: Supporting IaaS Customers with a Sub-core Configurable Architecture},   year={2016},  volume={},  number={},  pages={682-694},  doi={10.1109/ISCA.2016.65}}

@INPROCEEDINGS{6493638,  author={Wu, Weidan and Lee, Benjamin C.},  booktitle={2012 45th Annual IEEE/ACM International Symposium on Microarchitecture},   title={Inferred Models for Dynamic and Sparse Hardware-Software Spaces},   year={2012},  volume={},  number={},  pages={413-424},  doi={10.1109/MICRO.2012.45}}

@INPROCEEDINGS{6730744,  author={Yigitbasi, Nezih and Willke, Theodore L. and Liao, Guangdeng and Epema, Dick},  booktitle={2013 IEEE 21st International Symposium on Modelling, Analysis and Simulation of Computer and Telecommunication Systems},   title={Towards Machine Learning-Based Auto-tuning of MapReduce},   year={2013},  volume={},  number={},  pages={11-20},  doi={10.1109/MASCOTS.2013.9}}

@inproceedings{10.1145/3035918.3064029,
author = {Van Aken, Dana and Pavlo, Andrew and Gordon, Geoffrey J. and Zhang, Bohan},
title = {Automatic Database Management System Tuning Through Large-Scale Machine Learning},
year = {2017},
isbn = {9781450341974},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3035918.3064029},
doi = {10.1145/3035918.3064029},
abstract = {Database management system (DBMS) configuration tuning is an essential aspect of any data-intensive application effort. But this is historically a difficult task because DBMSs have hundreds of configuration "knobs" that control everything in the system, such as the amount of memory to use for caches and how often data is written to storage. The problem with these knobs is that they are not standardized (i.e., two DBMSs use a different name for the same knob), not independent (i.e., changing one knob can impact others), and not universal (i.e., what works for one application may be sub-optimal for another). Worse, information about the effects of the knobs typically comes only from (expensive) experience.To overcome these challenges, we present an automated approach that leverages past experience and collects new information to tune DBMS configurations: we use a combination of supervised and unsupervised machine learning methods to (1) select the most impactful knobs, (2) map unseen database workloads to previous workloads from which we can transfer experience, and (3) recommend knob settings. We implemented our techniques in a new tool called OtterTune and tested it on two DBMSs. Our evaluation shows that OtterTune recommends configurations that are as good as or better than ones generated by existing tools or a human expert.},
booktitle = {Proceedings of the 2017 ACM International Conference on Management of Data},
pages = {1009–1024},
numpages = {16},
keywords = {machine learning, database tuning, autonomic computing, database management systems},
location = {Chicago, Illinois, USA},
series = {SIGMOD '17}
}

@inproceedings{10.1145/1168857.1168881,
author = {Lee, Benjamin C. and Brooks, David M.},
title = {Accurate and Efficient Regression Modeling for Microarchitectural Performance and Power Prediction},
year = {2006},
isbn = {1595934510},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1168857.1168881},
doi = {10.1145/1168857.1168881},
abstract = {},
booktitle = {Proceedings of the 12th International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {185–194},
numpages = {10},
keywords = {statistics, microarchitecture, inference, regression, simulation},
location = {San Jose, California, USA},
series = {ASPLOS XII}
}

@article{10.1145/2829950,
author = {Tomusk, Erik and Dubach, Christophe and O’boyle, Michael},
title = {Four Metrics to Evaluate Heterogeneous Multicores},
year = {2015},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {4},
issn = {1544-3566},
url = {https://doi.org/10.1145/2829950},
doi = {10.1145/2829950},
abstract = {Semiconductor device scaling has made single-ISA heterogeneous processors a reality. Heterogeneous processors contain a number of different CPU cores that all implement the same Instruction Set Architecture (ISA). This enables greater flexibility and specialization, as runtime constraints and workload characteristics can influence which core a given workload is run on. A major roadblock to the further development of heterogeneous processors is the lack of appropriate evaluation metrics. Existing metrics can be used to evaluate individual cores, but to evaluate a heterogeneous processor, the cores must be considered as a collective. Without appropriate metrics, it is impossible to establish design goals for processors, and it is difficult to accurately compare two different heterogeneous processors.We present four new metrics to evaluate user-oriented aspects of sets of heterogeneous cores: localized nonuniformity, gap overhead, set overhead, and generality. The metrics consider sets rather than individual cores. We use examples to demonstrate each metric, and show that the metrics can be used to quantify intuitions about heterogeneous cores.},
journal = {ACM Trans. Archit. Code Optim.},
month = {nov},
articleno = {37},
numpages = {25},
keywords = {generality, effective speed, set overhead, gap overhead, Localized nonuniformity, single-ISA}
}

@INPROCEEDINGS{1598114,  author={Li, J. and Martinez, J.F.},  booktitle={The Twelfth International Symposium on High-Performance Computer Architecture, 2006.},   title={Dynamic power-performance adaptation of parallel computation on chip multiprocessors},   year={2006},  volume={},  number={},  pages={77-87},  doi={10.1109/HPCA.2006.1598114}}

@INPROCEEDINGS{5695560,  author={Dubach, Christophe and Jones, Timothy M. and Bonilla, Edwin V. and O'Boyle, Michael F.P.},  booktitle={2010 43rd Annual IEEE/ACM International Symposium on Microarchitecture},   title={A Predictive Model for Dynamic Microarchitectural Adaptivity Control},   year={2010},  volume={},  number={},  pages={485-496},  doi={10.1109/MICRO.2010.14}}

@inproceedings{mem_cocktail,
author = {Deng, Zhaoxia and Zhang, Lunkai and Mishra, Nikita and Hoffmann, Henry and Chong, Frederic T.},
title = {Memory Cocktail Therapy: A General Learning-Based Framework to Optimize Dynamic Tradeoffs in NVMs},
year = {2017},
isbn = {9781450349529},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3123939.3124548},
doi = {10.1145/3123939.3124548},
abstract = {Non-volatile memories (NVMs) have attracted significant interest recently due to their high-density, low static power, and persistence. There are, however, several challenges associated with building practical systems from NVMs, including limited write endurance and long latencies. Researchers have proposed a variety of architectural techniques which can achieve different tradeoffs between lifetime, performance and energy efficiency; however, no individual technique can satisfy requirements for all applications and different objectives. Hence, we propose Memory Cocktail Therapy (MCT), a general, learning-based framework that adaptively chooses the best techniques for the current application and objectives.Specifically, MCT performs four procedures to adapt the techniques to various scenarios. First, MCT formulates a high-dimensional configuration space from all different combinations of techniques. Second, MCT selects primary features from the configuration space with lasso regularization. Third, MCT estimates lifetime, performance and energy consumption using lightweight online predictors (eg. quadratic regression and gradient boosting) and a small set of configurations guided by the selected features. Finally, given the estimation of all configurations, MCT selects the optimal configuration based on the user-defined objectives. As a proof of concept, we test MCT's ability to guarantee different lifetime targets and achieve 95% of maximum performance, while minimizing energy consumption. We find that MCT improves performance by 9.24% and reduces energy by 7.95% compared to the best static configuration. Moreover, the performance of MCT is 94.49% of the ideal configuration with only 5.3% more energy consumption.},
booktitle = {Proceedings of the 50th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {232–244},
numpages = {13},
keywords = {machine learning, modeling, NVM, mellow writes},
location = {Cambridge, Massachusetts},
series = {MICRO-50 '17}
}

@INPROCEEDINGS{7207248,  author={Chen, Chi-Ou and Zhuo, Ye-Qi and Yeh, Chao-Chun and Lin, Che-Min and Liao, Shih-Wei},  booktitle={2015 IEEE International Congress on Big Data},   title={Machine Learning-Based Configuration Parameter Tuning on Hadoop System},   year={2015},  volume={},  number={},  pages={386-392},  doi={10.1109/BigDataCongress.2015.64}}

@inproceedings{Siblingrivalry,
author = {Ansel, Jason and Pacula, Maciej and Wong, Yee Lok and Chan, Cy and Olszewski, Marek and O'Reilly, Una-May and Amarasinghe, Saman},
title = {Siblingrivalry: Online Autotuning through Local Competitions},
year = {2012},
isbn = {9781450314244},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380403.2380425},
doi = {10.1145/2380403.2380425},
abstract = {Modern high performance libraries, such as ATLAS and FFTW, and programming languages, such as PetaBricks, have shown that autotuning computer programs can lead to significant speedups. However, autotuning can be burdensome to the deployment of a program, since the tuning process can take a long time and should be re-run whenever the program, microarchitecture, execution environment, or tool chain changes. Failure to re-autotune programs often leads to widespread use of sub-optimal algorithms. With the growth of cloud computing, where computations can run in environments with unknown load and migrate between different (possibly unknown) microarchitectures, the need for online autotuning has become increasingly important.We present SiblingRivalry, a new model for always-on online autotuning that allows parallel programs to continuously adapt and optimize themselves to their environment. In our system, requests are processed by dividing the available cores in half, and processing two identical requests in parallel on each half. Half of the cores are devoted to a known safe program configuration, while the other half are used for an experimental program configuration chosen by our self-adapting evolutionary algorithm. When the faster configuration completes, its results are returned, and the slower configuration is terminated. Over time, this constant experimentation allows programs to adapt to changing dynamic environments and often outperform the original algorithm that uses the entire system.},
booktitle = {Proceedings of the 2012 International Conference on Compilers, Architectures and Synthesis for Embedded Systems},
pages = {91–100},
numpages = {10},
keywords = {evolutionary algorithm, autotuning, genetic algorithm},
location = {Tampere, Finland},
series = {CASES '12}
}


@INPROCEEDINGS{6522303,  author={Zhu, Yuhao and Reddi, Vijay Janapa},  booktitle={2013 IEEE 19th International Symposium on High Performance Computer Architecture (HPCA)},   title={High-performance and energy-efficient mobile web browsing on big/little systems},   year={2013},  volume={},  number={},  pages={13-24},  doi={10.1109/HPCA.2013.6522303}}

@inproceedings{koala,
author = {Snowdon, David C. and Le Sueur, Etienne and Petters, Stefan M. and Heiser, Gernot},
title = {Koala: A Platform for OS-Level Power Management},
year = {2009},
isbn = {9781605584829},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1519065.1519097},
doi = {10.1145/1519065.1519097},
abstract = {Managing the power consumption of computing platforms is a complicated problem thanks to a multitude of hardware configuration options and characteristics. Much of the academic research is based on unrealistic assumptions, and has, therefore, seen little practical uptake. We provide an overview of the difficulties facing power management schemes when used in real systems.We present Koala, a platform which uses a pre-characterised model at run-time to predict the performance and energy consumption of a piece of software. An arbitrary policy can then be applied in order to dynamically trade performance and energy consumption. We have implemented this system in a recent Linux kernel, and evaluated it by running a variety of benchmarks on a number of different platforms. Under some conditions, we observe energy savings of 26% for a 1% performance loss.},
booktitle = {Proceedings of the 4th ACM European Conference on Computer Systems},
pages = {289–302},
numpages = {14},
keywords = {modelling, energy, power management, dynamic voltage scaling, operating systems, efficiency, power},
location = {Nuremberg, Germany},
series = {EuroSys '09}
}

@article{flicker,
author = {Petrica, Paula and Izraelevitz, Adam M. and Albonesi, David H. and Shoemaker, Christine A.},
title = {Flicker: A Dynamically Adaptive Architecture for Power Limited Multicore Systems},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {3},
issn = {0163-5964},
url = {https://doi.org/10.1145/2508148.2485924},
doi = {10.1145/2508148.2485924},
abstract = {Future microprocessors may become so power constrained that not all transistors will be able to be powered on at once. These systems will be required to nimbly adapt to changes in the chip power that is allocated to general-purpose cores and to specialized accelerators.This paper presents Flicker, a general-purpose multicore architecture that dynamically adapts to varying and potentially stringent limits on allocated power. The Flicker core microarchitecture includes deconfigurable lanes--horizontal slices through the pipeline--that permit tailoring an individual core to the running application with lower overhead than microarchitecture-level adaptation, and greater flexibility than core-level power gating.To exploit Flicker's flexible pipeline architecture, a new online multicore optimization algorithm combines reduced sampling techniques, application of response surface models to online optimization, and heuristic online search. The approach efficiently finds a near-global-optimum configuration of lanes without requiring offline training, microarchitecture state, or foreknowledge of the workload. At high power allocations, core-level gating is highly effective, and slightly outperforms Flicker overall. However, under stringent power constraints, Flicker significantly outperforms core-level gating, achieving an average 27% performance improvement.},
journal = {SIGARCH Comput. Archit. News},
month = {jun},
pages = {13–23},
numpages = {11}
}

@inproceedings{10.1145/2485922.2485924,
author = {Petrica, Paula and Izraelevitz, Adam M. and Albonesi, David H. and Shoemaker, Christine A.},
title = {Flicker: A Dynamically Adaptive Architecture for Power Limited Multicore Systems},
year = {2013},
isbn = {9781450320795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2485922.2485924},
doi = {10.1145/2485922.2485924},
abstract = {Future microprocessors may become so power constrained that not all transistors will be able to be powered on at once. These systems will be required to nimbly adapt to changes in the chip power that is allocated to general-purpose cores and to specialized accelerators.This paper presents Flicker, a general-purpose multicore architecture that dynamically adapts to varying and potentially stringent limits on allocated power. The Flicker core microarchitecture includes deconfigurable lanes--horizontal slices through the pipeline--that permit tailoring an individual core to the running application with lower overhead than microarchitecture-level adaptation, and greater flexibility than core-level power gating.To exploit Flicker's flexible pipeline architecture, a new online multicore optimization algorithm combines reduced sampling techniques, application of response surface models to online optimization, and heuristic online search. The approach efficiently finds a near-global-optimum configuration of lanes without requiring offline training, microarchitecture state, or foreknowledge of the workload. At high power allocations, core-level gating is highly effective, and slightly outperforms Flicker overall. However, under stringent power constraints, Flicker significantly outperforms core-level gating, achieving an average 27% performance improvement.},
booktitle = {Proceedings of the 40th Annual International Symposium on Computer Architecture},
pages = {13–23},
numpages = {11},
location = {Tel-Aviv, Israel},
series = {ISCA '13}
}



@inproceedings{carat,
author = {Oliner, Adam J. and Iyer, Anand P. and Stoica, Ion and Lagerspetz, Eemil and Tarkoma, Sasu},
title = {Carat: Collaborative Energy Diagnosis for Mobile Devices},
year = {2013},
isbn = {9781450320276},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2517351.2517354},
doi = {10.1145/2517351.2517354},
abstract = {We aim to detect and diagnose energy anomalies, abnormally heavy battery use. This paper describes a collaborative black-box method, and an implementation called Carat, for diagnosing anomalies on mobile devices. A client app sends intermittent, coarse-grained measurements to a server, which correlates higher expected energy use with client properties like the running apps, device model, and operating system. The analysis quantifies the error and confidence associated with a diagnosis, suggests actions the user could take to improve battery life, and projects the amount of improvement. During a deployment to a community of more than 500,000 devices, Carat diagnosed thousands of energy anomalies in the wild. Carat detected all synthetically injected anomalies, produced no known instances of false positives, projected the battery impact of anomalies with 95% accuracy, and, on average, increased a user's battery life by 11% after 10 days (compared with 1.9% for the control group).},
booktitle = {Proceedings of the 11th ACM Conference on Embedded Networked Sensor Systems},
articleno = {10},
numpages = {14},
keywords = {analytics, mobile, collaborative, energy, battery, diagnosis},
location = {Roma, Italy},
series = {SenSys '13}
}

@article{10.1145/1394608.1382172,
author = {Ipek, Engin and Mutlu, Onur and Mart\'{\i}nez, Jos\'{e} F. and Caruana, Rich},
title = {Self-Optimizing Memory Controllers: A Reinforcement Learning Approach},
year = {2008},
issue_date = {June 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {3},
issn = {0163-5964},
url = {https://doi.org/10.1145/1394608.1382172},
doi = {10.1145/1394608.1382172},
abstract = {Efficiently utilizing off-chip DRAM bandwidth is a critical issuein designing cost-effective, high-performance chip multiprocessors(CMPs). Conventional memory controllers deliver relativelylow performance in part because they often employ fixed,rigid access scheduling policies designed for average-case applicationbehavior. As a result, they cannot learn and optimizethe long-term performance impact of their scheduling decisions,and cannot adapt their scheduling policies to dynamic workloadbehavior.We propose a new, self-optimizing memory controller designthat operates using the principles of reinforcement learning (RL)to overcome these limitations. Our RL-based memory controllerobserves the system state and estimates the long-term performanceimpact of each action it can take. In this way, the controllerlearns to optimize its scheduling policy on the fly to maximizelong-term performance. Our results show that an RL-basedmemory controller improves the performance of a set of parallelapplications run on a 4-core CMP by 19% on average (upto 33%), and it improves DRAM bandwidth utilization by 22%compared to a state-of-the-art controller.},
journal = {SIGARCH Comput. Archit. News},
month = {jun},
pages = {39–50},
numpages = {12},
keywords = {Memory Systems, Machine Learning, Memory Controller, Reinforcement Learning, Chip Multiprocessors}
}

@inproceedings{10.1109/ISCA.2008.21,
author = {Ipek, Engin and Mutlu, Onur and Mart\'{\i}nez, Jos\'{e} F. and Caruana, Rich},
title = {Self-Optimizing Memory Controllers: A Reinforcement Learning Approach},
year = {2008},
isbn = {9780769531748},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ISCA.2008.21},
doi = {10.1109/ISCA.2008.21},
abstract = {Efficiently utilizing off-chip DRAM bandwidth is a critical issuein designing cost-effective, high-performance chip multiprocessors(CMPs). Conventional memory controllers deliver relativelylow performance in part because they often employ fixed,rigid access scheduling policies designed for average-case applicationbehavior. As a result, they cannot learn and optimizethe long-term performance impact of their scheduling decisions,and cannot adapt their scheduling policies to dynamic workloadbehavior.We propose a new, self-optimizing memory controller designthat operates using the principles of reinforcement learning (RL)to overcome these limitations. Our RL-based memory controllerobserves the system state and estimates the long-term performanceimpact of each action it can take. In this way, the controllerlearns to optimize its scheduling policy on the fly to maximizelong-term performance. Our results show that an RL-basedmemory controller improves the performance of a set of parallelapplications run on a 4-core CMP by 19% on average (upto 33%), and it improves DRAM bandwidth utilization by 22%compared to a state-of-the-art controller.},
booktitle = {Proceedings of the 35th Annual International Symposium on Computer Architecture},
pages = {39–50},
numpages = {12},
keywords = {Machine Learning, Reinforcement Learning, Chip Multiprocessors, Memory Systems, Memory Controller},
series = {ISCA '08}
}



@inproceedings{10.1145/2815400.2815403,
author = {Hoffmann, Henry},
title = {JouleGuard: Energy Guarantees for Approximate Applications},
year = {2015},
isbn = {9781450338349},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815400.2815403},
doi = {10.1145/2815400.2815403},
abstract = {Energy consumption limits battery life in mobile devices and increases costs for servers and data centers. Approximate computing addresses energy concerns by allowing applications to trade accuracy for decreased energy consumption. Approximation frameworks can guarantee accuracy or performance and generally reduce energy usage; however, they provide no energy guarantees. Such guarantees would be beneficial for users who have a fixed energy budget and want to maximize application accuracy within that budget. We address this need by presenting JouleGuard: a runtime control system that coordinates approximate applications with system resource usage to provide control theoretic formal guarantees of energy consumption, while maximizing accuracy. We implement JouleGuard and test it on three different platforms (a mobile, tablet, and server) with eight different approximate applications created from two different frameworks. We find that JouleGuard respects energy budgets, provides near optimal accuracy, adapts to phases in application workload, and provides better outcomes than application approximation or system resource adaptation alone. JouleGuard is general with respect to the applications and systems it controls, making it a suitable runtime for a number of approximate computing frameworks.},
booktitle = {Proceedings of the 25th Symposium on Operating Systems Principles},
pages = {198–214},
numpages = {17},
keywords = {control theory, adaptive software, dynamic systems},
location = {Monterey, California},
series = {SOSP '15}
}

@inproceedings{10.5555/1855591.1855592,
author = {Ganapathi, Archana and Datta, Kaushik and Fox, Armando and Patterson, David},
title = {A Case for Machine Learning to Optimize Multicore Performance},
year = {2009},
publisher = {USENIX Association},
address = {USA},
abstract = {Multicore architectures have become so complex and diverse that there is no obvious path to achieving good performance. Hundreds of code transformations, compiler flags, architectural features and optimization parameters result in a search space that can take many machinemonths to explore exhaustively. Inspired by successes in the systems community, we apply state-of-the-art machine learning techniques to explore this space more intelligently. On 7-point and 27-point stencil code, our technique takes about two hours to discover a configuration whose performance is within 1% of and up to 18% better than that achieved by a human expert. This factor of 2000 speedup over manual exploration of the auto-tuning parameter space enables us to explore optimizations that were previously off-limits. We believe the opportunity for using machine learning in multicore autotuning is even more promising than the successes to date in the systems literature.},
booktitle = {Proceedings of the First USENIX Conference on Hot Topics in Parallelism},
pages = {1},
numpages = {1},
location = {Berkeley, California},
series = {HotPar'09}
}

@INPROCEEDINGS{4771801,  author={Bitirgen, Ramazan and Ipek, Engin and Martinez, Jose F.},  booktitle={2008 41st IEEE/ACM International Symposium on Microarchitecture},   title={Coordinated management of multiple interacting resources in chip multiprocessors: A machine learning approach},   year={2008},  volume={},  number={},  pages={318-329},  doi={10.1109/MICRO.2008.4771801}}

@inproceedings{10.1145/1995896.1995927,
author = {Chen, Jian and John, Lizy Kurian},
title = {Predictive Coordination of Multiple On-Chip Resources for Chip Multiprocessors},
year = {2011},
isbn = {9781450301022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1995896.1995927},
doi = {10.1145/1995896.1995927},
abstract = {Efficient on-chip resource management is crucial for Chip Multiprocessors (CMP) to achieve high resource utilization and enforce system-level performance objectives. Existing multiple resource management schemes either focus on intra-core resources or inter-core resources, missing the opportunity for exploiting the interaction between these two level resources. Moreover, these resource management schemes either rely on trial runs or complex on-line machine learning model to search for the appropriate resource allocation, which makes resource management inefficient and expensive. To address these limitations, this paper presents a predictive yet cost effective mechanism for multiple resource management in CMP. It uses a set of hardware-efficient online profilers and an analytical performance model to predict the application's performance with different intra-core and/or inter-core resource allocations. Based on the predicted performance, the resource allocator identifies and enforces near optimum resource partitions for each epoch without any trial runs. The experimental results show that the proposed predictive resource management framework could improve the weighted speedup of the CMP system by an average of 11.6% compared with the equal partition scheme, and 9.3% compared with existing reactive resource management scheme.},
booktitle = {Proceedings of the International Conference on Supercomputing},
pages = {192–201},
numpages = {10},
keywords = {resource management, program characteristics, microprocessor, performance modeling},
location = {Tucson, Arizona, USA},
series = {ICS '11}
}


@inproceedings{10.1145/3468264.3468603,
author = {Ding, Yi and Pervaiz, Ahsan and Carbin, Michael and Hoffmann, Henry},
title = {Generalizable and Interpretable Learning for Configuration Extrapolation},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468603},
doi = {10.1145/3468264.3468603},
abstract = {Modern software applications are increasingly configurable, which puts a burden on users to tune these configurations for their target hardware and workloads. To help users, machine learning techniques can model the complex relationships between software configuration parameters and performance. While powerful, these learners have two major drawbacks: (1) they rarely incorporate prior knowledge and (2) they produce outputs that are not interpretable by users. These limitations make it difficult to (1) leverage information a user has already collected (e.g., tuning for new hardware using the best configurations from old hardware) and (2) gain insights into the learner’s behavior (e.g., understanding why the learner chose different configurations on different hardware or for different workloads). To address these issues, this paper presents two configuration optimization tools, GIL and GIL+, using the proposed generalizable and interpretable learning approaches. To incorporate prior knowledge, the proposed tools (1) start from known configurations, (2) iteratively construct a new linear model, (3) extrapolate better performance configurations from that model, and (4) repeat. Since the base learners are linear models, these tools are inherently interpretable. We enhance this property with a graphical representation of how they arrived at the highest performance configuration. We evaluate GIL and GIL+ by using them to configure Apache Spark workloads on different hardware platforms and find that, compared to prior work, GIL and GIL+ produce comparable, and sometimes even better performance configurations, but with interpretable results.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {728–740},
numpages = {13},
keywords = {machine learning, interpretability, generalizability, Configuration},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}


@INPROCEEDINGS{heuristics_0,  author={Kim, David H.K. and Imes, Connor and Hoffmann, Henry},  booktitle={2015 IEEE 3rd International Conference on Cyber-Physical Systems, Networks, and Applications},   title={Racing and Pacing to Idle: Theoretical and Empirical Analysis of Energy Optimization Heuristics},   year={2015},  volume={},  number={},  pages={78-85},  doi={10.1109/CPSNA.2015.23}}

@INPROCEEDINGS{4771797,  author={Lee, Benjamin C. and Collins, Jamison and Wang, Hong and Brooks, David},  booktitle={2008 41st IEEE/ACM International Symposium on Microarchitecture},   title={CPR: Composable performance regression for scalable multiprocessor models},   year={2008},  volume={},  number={},  pages={270-281},  doi={10.1109/MICRO.2008.4771797}}

@misc{arxiv.2112.07010,
  doi = {10.48550/ARXIV.2112.07010},
  url = {https://arxiv.org/abs/2112.07010},
  author = {Dong, Han and Arora, Sanjay and Awad, Yara and Unger, Tommy and Krieger, Orran and Appavoo, Jonathan},
  keywords = {Operating Systems (cs.OS), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Slowing Down for Performance and Energy: An OS-Centric Study in Network Driven Workloads. https://arxiv.org/abs/2112.07010},
  publisher = {arXiv},
  year = {2021},
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International}
}


@article{10.1145/2775054.2694373,
author = {Mishra, Nikita and Zhang, Huazhe and Lafferty, John D. and Hoffmann, Henry},
title = {A Probabilistic Graphical Model-Based Approach for Minimizing Energy Under Performance Constraints},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2775054.2694373},
doi = {10.1145/2775054.2694373},
abstract = {In many deployments, computer systems are underutilized -- meaning that applications have performance requirements that demand less than full system capacity. Ideally, we would take advantage of this under-utilization by allocating system resources so that the performance requirements are met and energy is minimized. This optimization problem is complicated by the fact that the performance and power consumption of various system configurations are often application -- or even input -- dependent. Thus, practically, minimizing energy for a performance constraint requires fast, accurate estimations of application-dependent performance and power tradeoffs. This paper investigates machine learning techniques that enable energy savings by learning Pareto-optimal power and performance tradeoffs. Specifically, we propose LEO, a probabilistic graphical model-based learning system that provides accurate online estimates of an application's power and performance as a function of system configuration. We compare LEO to (1) offline learning, (2) online learning, (3) a heuristic approach, and (4) the true optimal solution. We find that LEO produces the most accurate estimates and near optimal energy savings.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {267–281},
numpages = {15},
keywords = {probabilistic graphical models}
}

@inproceedings{10.1145/2694344.2694373,
author = {Mishra, Nikita and Zhang, Huazhe and Lafferty, John D. and Hoffmann, Henry},
title = {A Probabilistic Graphical Model-Based Approach for Minimizing Energy Under Performance Constraints},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694344.2694373},
doi = {10.1145/2694344.2694373},
abstract = {In many deployments, computer systems are underutilized -- meaning that applications have performance requirements that demand less than full system capacity. Ideally, we would take advantage of this under-utilization by allocating system resources so that the performance requirements are met and energy is minimized. This optimization problem is complicated by the fact that the performance and power consumption of various system configurations are often application -- or even input -- dependent. Thus, practically, minimizing energy for a performance constraint requires fast, accurate estimations of application-dependent performance and power tradeoffs. This paper investigates machine learning techniques that enable energy savings by learning Pareto-optimal power and performance tradeoffs. Specifically, we propose LEO, a probabilistic graphical model-based learning system that provides accurate online estimates of an application's power and performance as a function of system configuration. We compare LEO to (1) offline learning, (2) online learning, (3) a heuristic approach, and (4) the true optimal solution. We find that LEO produces the most accurate estimates and near optimal energy savings.},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {267–281},
numpages = {15},
keywords = {probabilistic graphical models},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}



@article{caloree,
author = {Mishra, Nikita and Imes, Connor and Lafferty, John D. and Hoffmann, Henry},
title = {CALOREE: Learning Control for Predictable Latency and Low Energy},
year = {2018},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {2},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296957.3173184},
doi = {10.1145/3296957.3173184},
abstract = {Many modern computing systems must provide reliable latency with minimal energy. Two central challenges arise when allocating system resources to meet these conflicting goals: (1) complexity modern hardware exposes diverse resources with complicated interactions and (2) dynamics latency must be maintained despite unpredictable changes in operating environment or input. Machine learning accurately models the latency of complex, interacting resources, but does not address system dynamics; control theory adjusts to dynamic changes, but struggles with complex resource interaction. We therefore propose CALOREE, a resource manager that learns key control parameters to meet latency requirements with minimal energy in complex, dynamic en- vironments. CALOREE breaks resource allocation into two sub-tasks: learning how interacting resources affect speedup, and controlling speedup to meet latency requirements with minimal energy. CALOREE deines a general control system whose parameters are customized by a learning framework while maintaining control-theoretic formal guarantees that the latency goal will be met. We test CALOREE's ability to deliver reliable latency on heterogeneous ARM big.LITTLE architectures in both single and multi-application scenarios. Compared to the best prior learning and control solutions, CALOREE reduces deadline misses by 60% and energy consumption by 13%.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {184–198},
numpages = {15},
keywords = {energy, machine learning, control theory, resource allocation, heterogeneous architectures, real-time systems}
}

@inproceedings{10.1145/3173162.3173184,
author = {Mishra, Nikita and Imes, Connor and Lafferty, John D. and Hoffmann, Henry},
title = {CALOREE: Learning Control for Predictable Latency and Low Energy},
year = {2018},
isbn = {9781450349116},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173162.3173184},
doi = {10.1145/3173162.3173184},
abstract = {Many modern computing systems must provide reliable latency with minimal energy. Two central challenges arise when allocating system resources to meet these conflicting goals: (1) complexity modern hardware exposes diverse resources with complicated interactions and (2) dynamics latency must be maintained despite unpredictable changes in operating environment or input. Machine learning accurately models the latency of complex, interacting resources, but does not address system dynamics; control theory adjusts to dynamic changes, but struggles with complex resource interaction. We therefore propose CALOREE, a resource manager that learns key control parameters to meet latency requirements with minimal energy in complex, dynamic en- vironments. CALOREE breaks resource allocation into two sub-tasks: learning how interacting resources affect speedup, and controlling speedup to meet latency requirements with minimal energy. CALOREE deines a general control system whose parameters are customized by a learning framework while maintaining control-theoretic formal guarantees that the latency goal will be met. We test CALOREE's ability to deliver reliable latency on heterogeneous ARM big.LITTLE architectures in both single and multi-application scenarios. Compared to the best prior learning and control solutions, CALOREE reduces deadline misses by 60% and energy consumption by 13%.},
booktitle = {Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {184–198},
numpages = {15},
keywords = {control theory, energy, resource allocation, real-time systems, heterogeneous architectures, machine learning},
location = {Williamsburg, VA, USA},
series = {ASPLOS '18}
}



@INPROCEEDINGS {7108419,
author = {C. Imes and D. K. Kim and M. Maggio and H. Hoffmann},
booktitle = {2015 IEEE Real-Time and Embedded Technology and Applications Symposium (RTAS)},
title = {POET: a portable approach to minimizing energy under soft real-time constraints},
year = {2015},
volume = {},
issn = {},
pages = {75-86},
abstract = {Embedded real-time systems must meet timing constraints while minimizing energy consumption. To this end, many energy optimizations are introduced for specific platforms or specific applications. These solutions are not portable, however, and when the application or the platform change, these solutions must be redesigned. Portable techniques are hard to develop due to the varying tradeoffs experienced with different application/platform configurations. This paper addresses the problem of finding and exploiting general tradeoffs, using control theory and mathematical optimization to achieve energy minimization under soft real-time application constraints. The paper presents POET, an open-source C library and runtime system that takes a specification of the platform resources and optimizes the application execution. We test POET’s ability to portably deliver predictable timing and energy reduction on two embedded systems with different tradeoff spaces – the first with a mobile Intel Haswell processor, and the second with an ARM big.LITTLE System on Chip. POET achieves the desired latency goals with small error while consuming, on average, only 1.3% more energy than the dynamic optimal oracle on the Haswell and 2.9% more on the ARM. We believe this open-source, librarybased approach to resource management will simplify the process of writing portable, energy-efficient code for embedded systems.},
keywords = {timing;energy consumption;runtime;resource management;schedules;optimization;robustness},
doi = {10.1109/RTAS.2015.7108419},
url = {https://doi.ieeecomputersociety.org/10.1109/RTAS.2015.7108419},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {apr}
}

@article{10.1145/2626401.2626411,
author = {Carroll, Aaron and Heiser, Gernot},
title = {Mobile Multicores: Use Them or Waste Them},
year = {2014},
issue_date = {January 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {1},
issn = {0163-5980},
url = {https://doi.org/10.1145/2626401.2626411},
doi = {10.1145/2626401.2626411},
abstract = {Energy management is a primary consideration in the design of modern smartphones, made more interesting by the recent proliferation of multi-core processors in this space. We investigate how core offlining and DVFS can be used together on these systems to reduce energy consumption. We show that core offlining leads to very modest savings in the best circumstances, with a heavy penalty in others, and show the cause of this to be low per-core idle power. We develop a policy in Linux that exploits this fact, and show that it improves up to 25% on existing implementations.},
journal = {SIGOPS Oper. Syst. Rev.},
month = {may},
pages = {44–48},
numpages = {5}
}

@article{Wu2023,
	doi = {10.1145/3494523},
	url = {https://doi.org/10.1145%2F3494523},
	year = 2023,
	month = {apr},
	publisher = {Association for Computing Machinery ({ACM})},
	volume = {55},
	number = {3},
	pages = {1--39},
	author = {Nan Wu and Yuan Xie},
	title = {A Survey of Machine Learning for Computer Architecture and Systems},
	journal = {{ACM} Computing Surveys}
}

@inproceedings {cacheWorkload-OSDI20,
  author = {Juncheng Yang, Yao Yue, Rashmi Vinayak}, 
  title = {A large scale analysis of hundreds of in-memory cache clusters at Twitter},
  booktitle = {14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20)},
  year = {2020},
  url = {https://www.usenix.org/conference/osdi20/presentation/yang},
  publisher = {{USENIX} Association},
  month = nov,
}

@inproceedings{10.1145/3307650.3326633,
author = {Ding, Yi and Mishra, Nikita and Hoffmann, Henry},
title = {Generative and Multi-Phase Learning for Computer Systems Optimization},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3326633},
doi = {10.1145/3307650.3326633},
abstract = {Machine learning and artificial intelligence are invaluable for computer systems optimization: as computer systems expose more resources for management, ML/AI is necessary for modeling these resources' complex interactions. The standard way to incorporate ML/AI into a computer system is to first train a learner to accurately predict the system's behavior as a function of resource usage---e.g., to predict energy efficiency as a function of core usage---and then deploy the learned model as part of a system---e.g., a scheduler. In this paper, we show that (1) continued improvement of learning accuracy may not improve the systems result, but (2) incorporating knowledge of the systems problem into the learning process improves the systems results even though it may not improve overall accuracy. Specifically, we learn application performance and power as a function of resource usage with the systems goal of meeting latency constraints with minimal energy. We propose a novel generative model which improves learning accuracy given scarce data, and we propose a multi-phase sampling technique, which incorporates knowledge of the systems problem. Our results are both positive and negative. The generative model improves accuracy, even for state-of-the-art learning systems, but negatively impacts energy. Multi-phase sampling reduces energy consumption compared to the state-of-the-art, but does not improve accuracy. These results imply that learning for systems optimization may have reached a point of diminishing returns where accuracy improvements have little effect on the systems outcome. Thus we advocate that future work on learning for systems should de-emphasize accuracy and instead incorporate the system problem's structure into the learner.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {39–52},
numpages = {14},
keywords = {resource allocation, real-time systems, machine learning, heterogeneous architectures, energy},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{Bakshy2018AEAD,
  title={AE: A domain-agnostic platform for adaptive experimentation},
  author={Eytan Bakshy and Lili Dworkin and Brian Karrer and Konstantin Kashin and Benjamin Letham and Ashwin Murthy and Shaun Singh},
  year={2018}
}

@misc{armxgene,
title = {{Kernel driver xgene-hwmon}},
howpublished = {\url{https://docs.kernel.org/hwmon/xgene-hwmon.html}}
}


@misc{amdpstate,
author = {{Huang Rui}},
title = {{amd-pstate CPU Performance Scaling Driver}},
howpublished = {\url{https://docs.kernel.org/admin-guide/pm/amd-pstate.html}}
}


@misc{amdepp,
author = {{Perry Yuan}},
title = {{Implement AMD Pstate EPP Driver}},
howpublished = {\url{https://lwn.net/Articles/914431/}}
}


@misc{amdenergy,
author = {{Naveen Krishna Chatradhi }},
title = {{Kernel driver amd\_energy}},
howpublished = {\url{https://www.kernel.org/doc/html/v5.9/hwmon/amd_energy.html}}
}


@misc{hanappliance,
author = {{Han Dong, Jonathan Appavoo}},
title = {{A Tutorial on Building Custom Linux Appliances}},
howpublished = {\url{https://www.usenix.org/publications/loginonline/building-linux-appliances}}
}


@misc{googlecluster,
author = {{Joseph L. Hellerstein}},
title = {{Google Cluster Data }},
howpublished = {\url{https://ai.googleblog.com/2010/01/google-cluster-data.html}},
year={2010}
}

@misc{nvmenergy,
title = {{NVM-Express-Base-Specification-2.0b}},
howpublished = {\url{https://nvmexpress.org/wp-content/uploads/NVM-Express-Base-Specification-2.0b-2021.12.18-Ratified.pdf}}
}

@misc{armenergy,
title = {{Power Management with big.LITTLE: A technical overview}},
howpublished = {\url{https://community.arm.com/arm-community-blogs/b/architectures-and-processors-blog/posts/power-management-with-big-little-a-technical-overview}}
}

@misc{gpuenergy,
title = {{NVIDIA Management Library (NVML)}},
howpublished = {\url{https://docs.nvidia.com/deploy/nvml-api/}}
}

@misc{ibmenergy,
title = {{IBM EnergyScale for POWER9 Processor-Based Systems}},
howpublished = {\url{https://www.ibm.com/downloads/cas/6GZMODN3}}
}

@misc{amdenergy,
title = {{Kernel driver amd energy}},
howpublished = {\url{https://www.kernel.org/doc/html/v5.9/hwmon/amd_energy.html}}
}



@inproceedings {211245,
author = {Mo Dong and Tong Meng and Doron Zarchy and Engin Arslan and Yossi Gilad and Brighten Godfrey and Michael Schapira},
title = {{PCC} Vivace: {Online-Learning} Congestion Control},
booktitle = {15th USENIX Symposium on Networked Systems Design and Implementation (NSDI 18)},
year = {2018},
isbn = {978-1-939133-01-4},
address = {Renton, WA},
pages = {343--356},
url = {https://www.usenix.org/conference/nsdi18/presentation/dong},
publisher = {USENIX Association},
month = apr,
}

@inproceedings{quasar,
author = {Delimitrou, Christina and Kozyrakis, Christos},
title = {Quasar: Resource-Efficient and QoS-Aware Cluster Management},
year = {2014},
isbn = {9781450323055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541940.2541941},
doi = {10.1145/2541940.2541941},
abstract = {Cloud computing promises flexibility and high performance for users and high cost-efficiency for operators. Nevertheless, most cloud facilities operate at very low utilization, hurting both cost effectiveness and future scalability.We present Quasar, a cluster management system that increases resource utilization while providing consistently high application performance. Quasar employs three techniques. First, it does not rely on resource reservations, which lead to underutilization as users do not necessarily understand workload dynamics and physical resource requirements of complex codebases. Instead, users express performance constraints for each workload, letting Quasar determine the right amount of resources to meet these constraints at any point. Second, Quasar uses classification techniques to quickly and accurately determine the impact of the amount of resources (scale-out and scale-up), type of resources, and interference on performance for each workload and dataset. Third, it uses the classification results to jointly perform resource allocation and assignment, quickly exploring the large space of options for an efficient way to pack workloads on available resources. Quasar monitors workload performance and adjusts resource allocation and assignment when needed. We evaluate Quasar over a wide range of workload scenarios, including combinations of distributed analytics frameworks and low-latency, stateful services, both on a local cluster and a cluster of dedicated EC2 servers. At steady state, Quasar improves resource utilization by 47% in the 200-server EC2 cluster, while meeting performance constraints for workloads of all types.},
booktitle = {Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {127–144},
numpages = {18},
keywords = {datacenters, cluster management, cloud computing, resource efficiency, resource allocation and assignment, quality of service},
location = {Salt Lake City, Utah, USA},
series = {ASPLOS '14}
}

@inproceedings{paragon,
author = {Delimitrou, Christina and Kozyrakis, Christos},
title = {Paragon: QoS-Aware Scheduling for Heterogeneous Datacenters},
year = {2013},
isbn = {9781450318709},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2451116.2451125},
doi = {10.1145/2451116.2451125},
abstract = {Large-scale datacenters (DCs) host tens of thousands of diverse applications each day. However, interference between colocated workloads and the difficulty to match applications to one of the many hardware platforms available can degrade performance, violating the quality of service (QoS) guarantees that many cloud workloads require. While previous work has identified the impact of heterogeneity and interference, existing solutions are computationally intensive, cannot be applied online and do not scale beyond few applications.We present Paragon, an online and scalable DC scheduler that is heterogeneity and interference-aware. Paragon is derived from robust analytical methods and instead of profiling each application in detail, it leverages information the system already has about applications it has previously seen. It uses collaborative filtering techniques to quickly and accurately classify an unknown, incoming workload with respect to heterogeneity and interference in multiple shared resources, by identifying similarities to previously scheduled applications. The classification allows Paragon to greedily schedule applications in a manner that minimizes interference and maximizes server utilization. Paragon scales to tens of thousands of servers with marginal scheduling overheads in terms of time or state.We evaluate Paragon with a wide range of workload scenarios, on both small and large-scale systems, including 1,000 servers on EC2. For a 2,500-workload scenario, Paragon enforces performance guarantees for 91% of applications, while significantly improving utilization. In comparison, heterogeneity-oblivious, interference-oblivious and least-loaded schedulers only provide similar guarantees for 14%, 11% and 3% of workloads. The differences are more striking in oversubscribed scenarios where resource efficiency is more critical.},
booktitle = {Proceedings of the Eighteenth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {77–88},
numpages = {12},
keywords = {scheduling, interference, qos, cloud computing, heterogeneity, datacenter},
location = {Houston, Texas, USA},
series = {ASPLOS '13}
}

@inproceedings{10.1145/3132747.3132772,
author = {Cortez, Eli and Bonde, Anand and Muzio, Alexandre and Russinovich, Mark and Fontoura, Marcus and Bianchini, Ricardo},
title = {Resource Central: Understanding and Predicting Workloads for Improved Resource Management in Large Cloud Platforms},
year = {2017},
isbn = {9781450350853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132747.3132772},
doi = {10.1145/3132747.3132772},
abstract = {Cloud research to date has lacked data on the characteristics of the production virtual machine (VM) workloads of large cloud providers. A thorough understanding of these characteristics can inform the providers' resource management systems, e.g. VM scheduler, power manager, server health manager. In this paper, we first introduce an extensive characterization of Microsoft Azure's VM workload, including distributions of the VMs' lifetime, deployment size, and resource consumption. We then show that certain VM behaviors are fairly consistent over multiple lifetimes, i.e. history is an accurate predictor of future behavior. Based on this observation, we next introduce Resource Central (RC), a system that collects VM telemetry, learns these behaviors offline, and provides predictions online to various resource managers via a general client-side library. As an example of RC's online use, we modify Azure's VM scheduler to leverage predictions in oversubscribing servers (with oversubscribable VM types), while retaining high VM performance. Using real VM traces, we then show that the prediction-informed schedules increase utilization and prevent physical resource exhaustion. We conclude that providers can exploit their workloads' characteristics and machine learning to improve resource management substantially.},
booktitle = {Proceedings of the 26th Symposium on Operating Systems Principles},
pages = {153–167},
numpages = {15},
keywords = {machine learning, Cloud workloads, predictive management},
location = {Shanghai, China},
series = {SOSP '17}
}

@inproceedings{47669,
title	= {SageDB: A Learned Database System},
author	= {Tim Kraska and Mohammad Alizadeh and Alex Beutel and Ed H. Chi and Jialin Ding and Ani Kristo and Guillaume Leclerc and Samuel Madden and Hongzi Mao and Vikram Nathan},
year	= {2019}
}



@inproceedings {216890,
author = {Giuseppe Vietri and Liana V. Rodriguez and Wendy A. Martinez and Steven Lyons and Jason Liu and Raju Rangaswami and Ming Zhao and Giri Narasimhan},
title = {Driving Cache Replacement with {ML-based} {LeCaR}},
booktitle = {10th USENIX Workshop on Hot Topics in Storage and File Systems (HotStorage 18)},
year = {2018},
address = {Boston, MA},
url = {https://www.usenix.org/conference/hotstorage18/presentation/vietri},
publisher = {USENIX Association},
month = jul,
}

@inproceedings {215953,
author = {Zhen Cao and Vasily Tarasov and Sachin Tiwari and Erez Zadok},
title = {Towards Better Understanding of Black-box {Auto-Tuning}: A Comparative Analysis for Storage Systems},
booktitle = {2018 USENIX Annual Technical Conference (USENIX ATC 18)},
year = {2018},
isbn = {978-1-939133-01-4},
address = {Boston, MA},
pages = {893--907},
url = {https://www.usenix.org/conference/atc18/presentation/cao},
publisher = {USENIX Association},
month = jul,
}

@inproceedings{10.1145/3183713.3196909,
author = {Kraska, Tim and Beutel, Alex and Chi, Ed H. and Dean, Jeffrey and Polyzotis, Neoklis},
title = {The Case for Learned Index Structures},
year = {2018},
isbn = {9781450347037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183713.3196909},
doi = {10.1145/3183713.3196909},
abstract = {Indexes are models: a btree-Index can be seen as a model to map a key to the position of a record within a sorted array, a Hash-Index as a model to map a key to a position of a record within an unsorted array, and a BitMap-Index as a model to indicate if a data record exists or not. In this exploratory research paper, we start from this premise and posit that all existing index structures can be replaced with other types of models, including deep-learning models, which we term em learned indexes. We theoretically analyze under which conditions learned indexes outperform traditional index structures and describe the main challenges in designing learned index structures. Our initial results show that our learned indexes can have significant advantages over traditional indexes. More importantly, we believe that the idea of replacing core components of a data management system through learned models has far reaching implications for future systems designs and that this work provides just a glimpse of what might be possible.},
booktitle = {Proceedings of the 2018 International Conference on Management of Data},
pages = {489–504},
numpages = {16},
keywords = {cdf, hash-map, learned index structure, linear regression, bloom-filter, learned data structures, mixture of experts, learned index, neural net, b-tree, index structures},
location = {Houston, TX, USA},
series = {SIGMOD '18}
}

@inproceedings {258965,
author = {Yifan Dai and Yien Xu and Aishwarya Ganesan and Ramnatthan Alagappan and Brian Kroth and Andrea Arpaci-Dusseau and Remzi Arpaci-Dusseau},
title = {From {WiscKey} to Bourbon: A Learned Index for {Log-Structured} Merge Trees},
booktitle = {14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20)},
year = {2020},
isbn = {978-1-939133-19-9},
pages = {155--171},
url = {https://www.usenix.org/conference/osdi20/presentation/dai},
publisher = {USENIX Association},
month = nov,
}

@INPROCEEDINGS{4085157,
  author={Negi, Atul and Kumar, P. Kishore},
  booktitle={TENCON 2005 - 2005 IEEE Region 10 Conference}, 
  title={Applying Machine Learning Techniques to Improve Linux Process Scheduling}, 
  year={2005},
  volume={},
  number={},
  pages={1-6},
  doi={10.1109/TENCON.2005.300837}}

@inproceedings{49008,
title	= {Learning-based Memory Allocation for C++ Server Workloads},
author	= {Martin Maas and David G. Andersen and Michael Isard and Mohammad Mahdi Javanmard and Kathryn S. McKinley and Colin Raffel},
year	= {2020},
booktitle	= {25th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)}
}



@inproceedings{10.1145/3409963.3410492,
author = {Chen, Jingde and Banerjee, Subho S. and Kalbarczyk, Zbigniew T. and Iyer, Ravishankar K.},
title = {Machine Learning for Load Balancing in the Linux Kernel},
year = {2020},
isbn = {9781450380690},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3409963.3410492},
doi = {10.1145/3409963.3410492},
abstract = {The OS load balancing algorithm governs the performance gains provided by a multiprocessor computer system. The Linux's Completely Fair Scheduler (CFS) scheduler tracks process loads by average CPU utilization to balance workload between processor cores. That approach maximizes the utilization of processing time but overlooks the contention for lower-level hardware resources. In servers running compute-intensive workloads, an imbalanced need for limited computing resources hinders execution performance. This paper solves the above problem using a machine learning (ML)-based resource-aware load balancer. We describe (1) low-overhead methods for collecting training data; (2) an ML model based on a multi-layer perceptron model that imitates the CFS load balancer based on the collected training data; and (3) an in-kernel implementation of inference on the model. Our experiments demonstrate that the proposed model has an accuracy of 99% in making migration decisions and while only increasing the latency by 1.9 μs.},
booktitle = {Proceedings of the 11th ACM SIGOPS Asia-Pacific Workshop on Systems},
pages = {67–74},
numpages = {8},
keywords = {operating system, load balancing, neural network, completely fair scheduler, machine learning, Linux kernel},
location = {Tsukuba, Japan},
series = {APSys '20}
}

@inbook{10.5555/3488766.3488776,
author = {Hao, Mingzhe and Toksoz, Levent and Li, Nanqinqin and Halim, Edward Edberg and Hoffmann, Henry and Gunawi, Haryadi S.},
title = {LinnOS: Predictability on Unpredictable Flash Storage with a Light Neural Network},
year = {2020},
isbn = {978-1-939133-19-9},
publisher = {USENIX Association},
address = {USA},
abstract = {This paper presents LinnOS, an operating system that leverages a light neural network for inferring SSD performance at a very fine--per-IO--granularity and helps parallel storage applications achieve performance predictability. LinnOS supports black-box devices and real production traces without requiring any extra input from users, while outperforming industrial mechanisms and other approaches. Our evaluation shows that, compared to hedging and heuristic-based methods, LinnOS improves the average I/O latencies by 9.6-79.6% with 87-97%inference accuracy and 4-6µs inference overhead for each I/O, demonstrating that it is possible to incorporate machine learning inside operating systems for real-time decision-making.},
booktitle = {Proceedings of the 14th USENIX Conference on Operating Systems Design and Implementation},
articleno = {10},
numpages = {18}
}

@inproceedings{10.1145/3465332.3470875,
author = {Akgun, Ibrahim Umit and Aydin, Ali Selman and Shaikh, Aadil and Velikov, Lukas and Zadok, Erez},
title = {A Machine Learning Framework to Improve Storage System Performance},
year = {2021},
isbn = {9781450385503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465332.3470875},
doi = {10.1145/3465332.3470875},
abstract = {Storage systems and their OS components are designed to accommodate a wide variety of applications and dynamic workloads. Storage components inside the OS contain various heuristic algorithms to provide high performance and adaptability for different workloads. These heuristics may be tunable via parameters, and some system calls allow users to optimize their system performance. These parameters are often predetermined based on experiments with limited applications and hardware. Thus, storage systems often run with these predetermined and possibly suboptimal values. Tuning these parameters manually is impractical: one needs an adaptive, intelligent system to handle dynamic and complex workloads. Machine learning (ML) techniques are capable of recognizing patterns, abstracting them, and making predictions on new data. ML can be a key component to optimize and adapt storage systems. In this position paper, we propose KML, an ML framework for storage systems. We implemented a prototype and demonstrated its capabilities on the well-known problem of tuning optimal readahead values. Our results show that KML has a small memory footprint, introduces negligible overhead, and yet enhances throughput by as much as 2.3x.},
booktitle = {Proceedings of the 13th ACM Workshop on Hot Topics in Storage and File Systems},
pages = {94–102},
numpages = {9},
keywords = {operating systems, storage systems, machine learning, storage performance optimization},
location = {Virtual, USA},
series = {HotStorage '21}
}

@inbook{10.1145/3173162.3173184,
author = {Mishra, Nikita and Imes, Connor and Lafferty, John D. and Hoffmann, Henry},
title = {CALOREE: Learning Control for Predictable Latency and Low Energy},
year = {2018},
isbn = {9781450349116},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173162.3173184},
abstract = {Many modern computing systems must provide reliable latency with minimal energy. Two central challenges arise when allocating system resources to meet these conflicting goals: (1) complexity modern hardware exposes diverse resources with complicated interactions and (2) dynamics latency must be maintained despite unpredictable changes in operating environment or input. Machine learning accurately models the latency of complex, interacting resources, but does not address system dynamics; control theory adjusts to dynamic changes, but struggles with complex resource interaction. We therefore propose CALOREE, a resource manager that learns key control parameters to meet latency requirements with minimal energy in complex, dynamic en- vironments. CALOREE breaks resource allocation into two sub-tasks: learning how interacting resources affect speedup, and controlling speedup to meet latency requirements with minimal energy. CALOREE deines a general control system whose parameters are customized by a learning framework while maintaining control-theoretic formal guarantees that the latency goal will be met. We test CALOREE's ability to deliver reliable latency on heterogeneous ARM big.LITTLE architectures in both single and multi-application scenarios. Compared to the best prior learning and control solutions, CALOREE reduces deadline misses by 60% and energy consumption by 13%.},
booktitle = {Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {184–198},
numpages = {15}
}


@inbook{10.1145/3173162.3173206,
author = {Wang, Shu and Li, Chi and Hoffmann, Henry and Lu, Shan and Sentosa, William and Kistijantoro, Achmad Imam},
title = {Understanding and Auto-Adjusting Performance-Sensitive Configurations},
year = {2018},
isbn = {9781450349116},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173162.3173206},
abstract = {Modern software systems are often equipped with hundreds to thousands of configurations, many of which greatly affect performance. Unfortunately, properly setting these configurations is challenging for developers due to the complex and dynamic nature of system workload and environment. In this paper, we first conduct an empirical study to understand performance-sensitive configurations and the challenges of setting them in the real-world. Guided by our study, we design a systematic and general control-theoretic framework, SmartConf, to automatically set and dynamically adjust performance-sensitive configurations to meet required operating constraints while optimizing other performance metrics. Evaluation shows that SmartConf is effective in solving real-world configuration problems, often providing better performance than even the best static configuration developers can choose under existing configuration systems.},
booktitle = {Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {154–168},
numpages = {15}
}

@misc{https://doi.org/10.48550/arxiv.1805.03441,
  doi = {10.48550/ARXIV.1805.03441},
  
  url = {https://arxiv.org/abs/1805.03441},
  
  author = {Wang, Zheng and O'Boyle, Michael},
  
  keywords = {Programming Languages (cs.PL), Distributed, Parallel, and Cluster Computing (cs.DC), Machine Learning (cs.LG), Software Engineering (cs.SE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Machine Learning in Compiler Optimisation},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{https://doi.org/10.48550/arxiv.1803.02329,
  doi = {10.48550/ARXIV.1803.02329},
  
  url = {https://arxiv.org/abs/1803.02329},
  
  author = {Hashemi, Milad and Swersky, Kevin and Smith, Jamie A. and Ayers, Grant and Litz, Heiner and Chang, Jichuan and Kozyrakis, Christos and Ranganathan, Parthasarathy},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Learning Memory Access Patterns},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@INPROCEEDINGS{7851506,  author={Winter, Jonathan A. and Albonesi, David H. and Shoemaker, Christine A.},  booktitle={2010 19th International Conference on Parallel Architectures and Compilation Techniques (PACT)},   title={Scalable thread scheduling and global power management for heterogeneous many-core architectures},   year={2010},  volume={},  number={},  pages={29-39},  doi={}}

@article{10.1145/1839667.1839670,
author = {Lee, Benjamin C. and Brooks, David},
title = {Applied Inference: Case Studies in Microarchitectural Design},
year = {2010},
issue_date = {September 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {2},
issn = {1544-3566},
url = {https://doi.org/10.1145/1839667.1839670},
doi = {10.1145/1839667.1839670},
abstract = {We propose and apply a new simulation paradigm for microarchitectural design evaluation and optimization. This paradigm enables more comprehensive design studies by combining spatial sampling and statistical inference. Specifically, this paradigm (i) defines a large, comprehensive design space, (ii) samples points from the space for simulation, and (iii) constructs regression models based on sparse simulations. This approach greatly improves the computational efficiency of microarchitectural simulation and enables new capabilities in design space exploration.We illustrate new capabilities in three case studies for a large design space of approximately 260,000 points: (i) Pareto frontier, (ii) pipeline depth, and (iii) multiprocessor heterogeneity analyses. In particular, regression models are exhaustively evaluated to identify Pareto optimal designs that maximize performance for given power budgets. These models enable pipeline depth studies in which all parameters vary simultaneously with depth, thereby more effectively revealing interactions with nondepth parameters. Heterogeneity analysis combines regression-based optimization with clustering heuristics to identify efficient design compromises between similar optimal architectures. These compromises are potential core designs in a heterogeneous multicore architecture. Increasing heterogeneity can improve bips3/w efficiency by as much as 2.4\texttimes{}, a theoretical upper bound on heterogeneity benefits that neglects contention between shared resources as well as design complexity. Collectively these studies demonstrate regression models' ability to expose trends and identify optima in diverse design regions, motivating the application of such models in statistical inference for more effective use of modern simulator infrastructure.},
journal = {ACM Trans. Archit. Code Optim.},
month = {oct},
articleno = {8},
numpages = {37},
keywords = {simulation, statistics, regression, Microarchitecture}
}

@INPROCEEDINGS{1635956,

  author={Seungryul Choi and Yeung, D.},

  booktitle={33rd International Symposium on Computer Architecture (ISCA'06)}, 

  title={Learning-Based SMT Processor Resource Distribution via Hill-Climbing}, 

  year={2006},

  volume={},

  number={},

  pages={239-251},

  doi={10.1109/ISCA.2006.25}}
  
@misc{frazier,
  doi = {10.48550/ARXIV.1807.02811},
  url = {https://arxiv.org/abs/1807.02811},
  author = {Frazier, Peter I.},
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), Optimization and Control (math.OC), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  title = {A Tutorial on Bayesian Optimization},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{cello,
  doi = {10.48550/ARXIV.2204.04831},
  
  url = {https://arxiv.org/abs/2204.04831},
  
  author = {Ding, Yi and Renda, Alex and Pervaiz, Ahsan and Carbin, Michael and Hoffmann, Henry},
  
  keywords = {Machine Learning (cs.LG), Hardware Architecture (cs.AR), Distributed, Parallel, and Cluster Computing (cs.DC), Performance (cs.PF), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Cello: Efficient Computer Systems Optimization with Predictive Early Termination and Censored Regression},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@INPROCEEDINGS{clite,

  author={Patel, Tirthak and Tiwari, Devesh},

  booktitle={2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)}, 

  title={CLITE: Efficient and QoS-Aware Co-Location of Multiple Latency-Critical Jobs for Warehouse Scale Computers}, 

  year={2020},

  volume={},

  number={},

  pages={193-206},

  doi={10.1109/HPCA47549.2020.00025}}

@inproceedings{bliss,
author = {Roy, Rohan Basu and Patel, Tirthak and Gadepally, Vijay and Tiwari, Devesh},
title = {Bliss: Auto-Tuning Complex Applications Using a Pool of Diverse Lightweight Learning Models},
year = {2021},
isbn = {9781450383912},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3453483.3454109},
doi = {10.1145/3453483.3454109},
abstract = {As parallel applications become more complex, auto-tuning becomes more desirable, challenging, and time-consuming. We propose, Bliss, a novel solution for auto-tuning parallel applications without requiring apriori information about applications, domain-specific knowledge, or instrumentation. Bliss demonstrates how to leverage a pool of Bayesian Optimization models to find the near-optimal parameter setting 1.64\texttimes{} faster than the state-of-the-art approaches.},
booktitle = {Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
pages = {1280–1295},
numpages = {16},
keywords = {Parameter tuning, Auto-tuning HPC applications},
location = {Virtual, Canada},
series = {PLDI 2021}
}

@misc{hypermapper,
  doi = {10.48550/ARXIV.1810.05236},
  
  url = {https://arxiv.org/abs/1810.05236},
  
  author = {Nardi, Luigi and Koeplinger, David and Olukotun, Kunle},
  
  keywords = {Machine Learning (cs.LG), Optimization and Control (math.OC), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  
  title = {Practical Design Space Exploration},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{cherrypick,
author = {Alipourfard, Omid and Liu, Hongqiang Harry and Chen, Jianshu and Venkataraman, Shivaram and Yu, Minlan and Zhang, Ming},
title = {Cherrypick: Adaptively Unearthing the Best Cloud Configurations for Big Data Analytics},
year = {2017},
isbn = {9781931971379},
publisher = {USENIX Association},
address = {USA},
abstract = {Picking the right cloud configuration for recurring big data analytics jobs running in clouds is hard, because there can be tens of possible VM instance types and even more cluster sizes to pick from. Choosing poorly can significantly degrade performance and increase the cost to run a job by 2-3x on average, and as much as 12x in the worst-case. However, it is challenging to automatically identify the best configuration for a broad spectrum of applications and cloud configurations with low search cost. CherryPick is a system that leverages Bayesian Optimization to build performance models for various applications, and the models are just accurate enough to distinguish the best or close-to-the-best configuration from the rest with only a few test runs. Our experiments on five analytic applications in AWS EC2 show that CherryPick has a 45-90% chance to find optimal configurations, otherwise near-optimal, saving up to 75% search cost compared to existing solutions.},
booktitle = {Proceedings of the 14th USENIX Conference on Networked Systems Design and Implementation},
pages = {469–482},
numpages = {14},
location = {Boston, MA, USA},
series = {NSDI'17}
}

@misc{ibmitr,
author = {{IBM}},
title = {{Interrupt coalescing}},
howpublished = {\url{https://www.ibm.com/docs/en/aix/7.1?topic=options-interrupt-coalescing}}
}

@article{SALAH2007215,
title = {To coalesce or not to coalesce},
journal = {AEU - International Journal of Electronics and Communications},
volume = {61},
number = {4},
pages = {215-225},
year = {2007},
issn = {1434-8411},
doi = {https://doi.org/10.1016/j.aeue.2006.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S143484110600063X},
author = {Khaled Salah},
keywords = {High-speed networks, Operating systems, Interrupts, Interrupt coalescing, Modeling and analysis, Performance evaluation},
abstract = {System performance of Gigabit network hosts can be severely degraded due to interrupt overhead caused by heavy incoming traffic. One of the most popular solutions to mitigate such overhead is interrupt coalescing in which a single interrupt is generated for multiple incoming packets. This is opposed to normal interruption in which an interrupt is generated for every incoming packet. In this paper we investigate the performance of interrupt coalescing analytically and compare it with that of normal interruption. We consider two types of coalescing (viz. count-based and time-based). The system performance is studied in terms of throughput, CPU availability for user applications, latency and packet loss.}
}

@INPROCEEDINGS{5289165,
  author={Goglin, Brice and Furmento, Nathalie},
  booktitle={2009 IEEE International Conference on Cluster Computing and Workshops}, 
  title={Finding a tradeoff between host interrupt load and MPI latency over Ethernet}, 
  year={2009},
  volume={},
  number={},
  pages={1-9},
  doi={10.1109/CLUSTR.2009.5289165}
}

@inproceedings{10.5555/2002181.2002185,
author = {Ahmad, Irfan and Gulati, Ajay and Mashtizadeh, Ali},
title = {VIC: Interrupt Coalescing for Virtual Machine Storage Device IO},
year = {2011},
publisher = {USENIX Association},
address = {USA},
abstract = {Interrupt coalescing is a well known and proven technique for reducing CPU utilization when processing high IO rates in network and storage controllers. Virtualization introduces a layer of virtual hardware for the guest operating system, whose interrupt rate can be controlled by the hypervisor. Unfortunately, existing techniques based on high-resolution timers are not practical for virtual devices, due to their large overhead. In this paper, we present the design and implementation of a virtual interrupt coalescing (vIC) scheme for virtual SCSI hardware controllers in a hypervisor.We use the number of commands in flight from the guest as well as the current IO rate to dynamically set the degree of interrupt coalescing. Compared to existing techniques in hardware, our work does not rely on high-resolution interrupt-delay timers and thus leads to a very efficient implementation in a hypervisor. Furthermore, our technique is generic and therefore applicable to all types of hardware storage IO controllers which, unlike networking, don't receive anonymous traffic. We also propose an optimization to reduce inter-processor interrupts (IPIs) resulting in better application performance during periods of high IO activity. Our implementation of virtual interrupt coalescing has been shipping with VMware ESX since 2009. We present our evaluation showing performance improvements in micro benchmarks of },
booktitle = {Proceedings of the 2011 USENIX Conference on USENIX Annual Technical Conference},
pages = {4},
numpages = {1},
location = {Portland, OR},
series = {USENIXATC'11}
}


@inproceedings{prasad,
author = {Prasad, Ravi and Jain, Manish and Dovrolis, Constantine},
year = {2004},
month = {04},
pages = {247-256},
title = {Effects of Interrupt Coalescence on Network Measurements},
volume = {3015},
isbn = {978-3-540-21492-2},
doi = {10.1007/978-3-540-24668-8_25}
}

@article{receivelivelock,
author = {Mogul, Jeffrey C. and Ramakrishnan, K. K.},
title = {Eliminating Receive Livelock in an Interrupt-Driven Kernel},
year = {1997},
issue_date = {Aug. 1997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {0734-2071},
url = {https://doi.org/10.1145/263326.263335},
doi = {10.1145/263326.263335},
abstract = {Most operating systems use interface interrupts to schedule network tasks. Interrupt-driven systems can provide low overhead and good latency at low offered load, but degrade significantly at higher arrival rates unless care is taken to prevent several pathologies. These are various forms ofreceive livelock, in which the system spends all of its time processing interrupts, to the exclusion of other necessary tasks. Under extreme conditions, no packets are delivered to the user application or the output of the system. To avoid livelock and related problems, an operating system must schedule network interrupt handling as carefully as it schedules process execution. We modified an interrupt-driven networking implementation to do so; this modification eliminates receive livelock without degrading other aspects of system performance. Our modifications include the use of polling when the system is heavily loaded, while retaining the use of interrupts ur.Jer lighter load. We present measurements demonstrating the success of our approach.},
journal = {ACM Trans. Comput. Syst.},
month = {aug},
pages = {217–252},
numpages = {36},
keywords = {livelock, interrupt-driven kernel, scheduling, polling}
}


@misc{hanappliance,
author = {{Han Dong, Jonathan Appavoo}},
title = {{A Tutorial on Building Custom Linux Appliances}},
howpublished = {\url{https://www.usenix.org/publications/loginonline/building-linux-appliances}}
}

@inproceedings {cacheWorkload-OSDI20,
  author = {Juncheng Yang, Yao Yue, Rashmi Vinayak}, 
  title = {A large scale analysis of hundreds of in-memory cache clusters at Twitter},
  booktitle = {14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20)},
  year = {2020},
  url = {https://www.usenix.org/conference/osdi20/presentation/yang},
  publisher = {{USENIX} Association},
  month = nov,
}

@misc{linuxadminguide,
author = {{The Linux kernel user's and administrator's guide}},
howpublished = {\url{https://www.kernel.org/doc/html/latest/admin-guide/}}
}

@inproceedings{10.5555/2930611.2930635,
author = {Venkataraman, Shivaram and Yang, Zongheng and Franklin, Michael and Recht, Benjamin and Stoica, Ion},
title = {Ernest: Efficient Performance Prediction for Large-Scale Advanced Analytics},
year = {2016},
isbn = {9781931971294},
publisher = {USENIX Association},
address = {USA},
abstract = {Recent workload trends indicate rapid growth in the deployment of machine learning, genomics and scientific workloads on cloud computing infrastructure. However, efficiently running these applications on shared infrastructure is challenging and we find that choosing the right hardware configuration can significantly improve performance and cost. The key to address the above challenge is having the ability to predict performance of applications under various resource configurations so that we can automatically choose the optimal configuration.Our insight is that a number of jobs have predictable structure in terms of computation and communication. Thus we can build performance models based on the behavior of the job on small samples of data and then predict its performance on larger datasets and cluster sizes. To minimize the time and resources spent in building a model, we use optimal experiment design, a statistical technique that allows us to collect as few training points as required. We have built Ernest, a performance prediction framework for large scale analytics and our evaluation on Amazon EC2 using several workloads shows that our prediction error is low while having a training overhead of less than 5% for long-running jobs.},
booktitle = {Proceedings of the 13th Usenix Conference on Networked Systems Design and Implementation},
pages = {363–378},
numpages = {16},
location = {Santa Clara, CA},
series = {NSDI'16}
}

@inproceedings{10.1145/3307650.3326633,
author = {Ding, Yi and Mishra, Nikita and Hoffmann, Henry},
title = {Generative and Multi-Phase Learning for Computer Systems Optimization},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3326633},
doi = {10.1145/3307650.3326633},
abstract = {Machine learning and artificial intelligence are invaluable for computer systems optimization: as computer systems expose more resources for management, ML/AI is necessary for modeling these resources' complex interactions. The standard way to incorporate ML/AI into a computer system is to first train a learner to accurately predict the system's behavior as a function of resource usage---e.g., to predict energy efficiency as a function of core usage---and then deploy the learned model as part of a system---e.g., a scheduler. In this paper, we show that (1) continued improvement of learning accuracy may not improve the systems result, but (2) incorporating knowledge of the systems problem into the learning process improves the systems results even though it may not improve overall accuracy. Specifically, we learn application performance and power as a function of resource usage with the systems goal of meeting latency constraints with minimal energy. We propose a novel generative model which improves learning accuracy given scarce data, and we propose a multi-phase sampling technique, which incorporates knowledge of the systems problem. Our results are both positive and negative. The generative model improves accuracy, even for state-of-the-art learning systems, but negatively impacts energy. Multi-phase sampling reduces energy consumption compared to the state-of-the-art, but does not improve accuracy. These results imply that learning for systems optimization may have reached a point of diminishing returns where accuracy improvements have little effect on the systems outcome. Thus we advocate that future work on learning for systems should de-emphasize accuracy and instead incorporate the system problem's structure into the learner.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {39–52},
numpages = {14},
keywords = {resource allocation, real-time systems, machine learning, heterogeneous architectures, energy},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{Bakshy2018AEAD,
  title={AE: A domain-agnostic platform for adaptive experimentation},
  author={Eytan Bakshy and Lili Dworkin and Brian Karrer and Konstantin Kashin and Benjamin Letham and Ashwin Murthy and Shaun Singh},
  year={2018}
}

@misc{lro,
author = {{Jonathan Corbet}},
title = {{Large receive offload}},
howpublished = {\url{https://lwn.net/Articles/243949/}}
}

@misc{pyro,
  title={{pyro.ai}},
  howpublished={\url{https://pyro.ai/}}
}

@misc{msrtools,
  title={{MSR Tools}},
  howpublished={\url{https://github.com/intel/msr-tools}}
}

@inproceedings {211245,
author = {Mo Dong and Tong Meng and Doron Zarchy and Engin Arslan and Yossi Gilad and Brighten Godfrey and Michael Schapira},
title = {{PCC} Vivace: {Online-Learning} Congestion Control},
booktitle = {15th USENIX Symposium on Networked Systems Design and Implementation (NSDI 18)},
year = {2018},
isbn = {978-1-939133-01-4},
address = {Renton, WA},
pages = {343--356},
url = {https://www.usenix.org/conference/nsdi18/presentation/dong},
publisher = {USENIX Association},
month = apr,
}

@inproceedings{10.1145/2541940.2541941,
author = {Delimitrou, Christina and Kozyrakis, Christos},
title = {Quasar: Resource-Efficient and QoS-Aware Cluster Management},
year = {2014},
isbn = {9781450323055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541940.2541941},
doi = {10.1145/2541940.2541941},
abstract = {Cloud computing promises flexibility and high performance for users and high cost-efficiency for operators. Nevertheless, most cloud facilities operate at very low utilization, hurting both cost effectiveness and future scalability.We present Quasar, a cluster management system that increases resource utilization while providing consistently high application performance. Quasar employs three techniques. First, it does not rely on resource reservations, which lead to underutilization as users do not necessarily understand workload dynamics and physical resource requirements of complex codebases. Instead, users express performance constraints for each workload, letting Quasar determine the right amount of resources to meet these constraints at any point. Second, Quasar uses classification techniques to quickly and accurately determine the impact of the amount of resources (scale-out and scale-up), type of resources, and interference on performance for each workload and dataset. Third, it uses the classification results to jointly perform resource allocation and assignment, quickly exploring the large space of options for an efficient way to pack workloads on available resources. Quasar monitors workload performance and adjusts resource allocation and assignment when needed. We evaluate Quasar over a wide range of workload scenarios, including combinations of distributed analytics frameworks and low-latency, stateful services, both on a local cluster and a cluster of dedicated EC2 servers. At steady state, Quasar improves resource utilization by 47% in the 200-server EC2 cluster, while meeting performance constraints for workloads of all types.},
booktitle = {Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {127–144},
numpages = {18},
keywords = {datacenters, cluster management, cloud computing, resource efficiency, resource allocation and assignment, quality of service},
location = {Salt Lake City, Utah, USA},
series = {ASPLOS '14}
}

@inproceedings{10.1145/2451116.2451125,
author = {Delimitrou, Christina and Kozyrakis, Christos},
title = {Paragon: QoS-Aware Scheduling for Heterogeneous Datacenters},
year = {2013},
isbn = {9781450318709},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2451116.2451125},
doi = {10.1145/2451116.2451125},
abstract = {Large-scale datacenters (DCs) host tens of thousands of diverse applications each day. However, interference between colocated workloads and the difficulty to match applications to one of the many hardware platforms available can degrade performance, violating the quality of service (QoS) guarantees that many cloud workloads require. While previous work has identified the impact of heterogeneity and interference, existing solutions are computationally intensive, cannot be applied online and do not scale beyond few applications.We present Paragon, an online and scalable DC scheduler that is heterogeneity and interference-aware. Paragon is derived from robust analytical methods and instead of profiling each application in detail, it leverages information the system already has about applications it has previously seen. It uses collaborative filtering techniques to quickly and accurately classify an unknown, incoming workload with respect to heterogeneity and interference in multiple shared resources, by identifying similarities to previously scheduled applications. The classification allows Paragon to greedily schedule applications in a manner that minimizes interference and maximizes server utilization. Paragon scales to tens of thousands of servers with marginal scheduling overheads in terms of time or state.We evaluate Paragon with a wide range of workload scenarios, on both small and large-scale systems, including 1,000 servers on EC2. For a 2,500-workload scenario, Paragon enforces performance guarantees for 91% of applications, while significantly improving utilization. In comparison, heterogeneity-oblivious, interference-oblivious and least-loaded schedulers only provide similar guarantees for 14%, 11% and 3% of workloads. The differences are more striking in oversubscribed scenarios where resource efficiency is more critical.},
booktitle = {Proceedings of the Eighteenth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {77–88},
numpages = {12},
keywords = {scheduling, interference, qos, cloud computing, heterogeneity, datacenter},
location = {Houston, Texas, USA},
series = {ASPLOS '13}
}

@inproceedings{10.1145/3132747.3132772,
author = {Cortez, Eli and Bonde, Anand and Muzio, Alexandre and Russinovich, Mark and Fontoura, Marcus and Bianchini, Ricardo},
title = {Resource Central: Understanding and Predicting Workloads for Improved Resource Management in Large Cloud Platforms},
year = {2017},
isbn = {9781450350853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132747.3132772},
doi = {10.1145/3132747.3132772},
abstract = {Cloud research to date has lacked data on the characteristics of the production virtual machine (VM) workloads of large cloud providers. A thorough understanding of these characteristics can inform the providers' resource management systems, e.g. VM scheduler, power manager, server health manager. In this paper, we first introduce an extensive characterization of Microsoft Azure's VM workload, including distributions of the VMs' lifetime, deployment size, and resource consumption. We then show that certain VM behaviors are fairly consistent over multiple lifetimes, i.e. history is an accurate predictor of future behavior. Based on this observation, we next introduce Resource Central (RC), a system that collects VM telemetry, learns these behaviors offline, and provides predictions online to various resource managers via a general client-side library. As an example of RC's online use, we modify Azure's VM scheduler to leverage predictions in oversubscribing servers (with oversubscribable VM types), while retaining high VM performance. Using real VM traces, we then show that the prediction-informed schedules increase utilization and prevent physical resource exhaustion. We conclude that providers can exploit their workloads' characteristics and machine learning to improve resource management substantially.},
booktitle = {Proceedings of the 26th Symposium on Operating Systems Principles},
pages = {153–167},
numpages = {15},
keywords = {machine learning, Cloud workloads, predictive management},
location = {Shanghai, China},
series = {SOSP '17}
}

@inproceedings{47669,
title	= {SageDB: A Learned Database System},
author	= {Tim Kraska and Mohammad Alizadeh and Alex Beutel and Ed H. Chi and Jialin Ding and Ani Kristo and Guillaume Leclerc and Samuel Madden and Hongzi Mao and Vikram Nathan},
year	= {2019}
}



@inproceedings {216890,
author = {Giuseppe Vietri and Liana V. Rodriguez and Wendy A. Martinez and Steven Lyons and Jason Liu and Raju Rangaswami and Ming Zhao and Giri Narasimhan},
title = {Driving Cache Replacement with {ML-based} {LeCaR}},
booktitle = {10th USENIX Workshop on Hot Topics in Storage and File Systems (HotStorage 18)},
year = {2018},
address = {Boston, MA},
url = {https://www.usenix.org/conference/hotstorage18/presentation/vietri},
publisher = {USENIX Association},
month = jul,
}

@inproceedings {215953,
author = {Zhen Cao and Vasily Tarasov and Sachin Tiwari and Erez Zadok},
title = {Towards Better Understanding of Black-box {Auto-Tuning}: A Comparative Analysis for Storage Systems},
booktitle = {2018 USENIX Annual Technical Conference (USENIX ATC 18)},
year = {2018},
isbn = {978-1-939133-01-4},
address = {Boston, MA},
pages = {893--907},
url = {https://www.usenix.org/conference/atc18/presentation/cao},
publisher = {USENIX Association},
month = jul,
}

@inproceedings{10.1145/3183713.3196909,
author = {Kraska, Tim and Beutel, Alex and Chi, Ed H. and Dean, Jeffrey and Polyzotis, Neoklis},
title = {The Case for Learned Index Structures},
year = {2018},
isbn = {9781450347037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183713.3196909},
doi = {10.1145/3183713.3196909},
abstract = {Indexes are models: a btree-Index can be seen as a model to map a key to the position of a record within a sorted array, a Hash-Index as a model to map a key to a position of a record within an unsorted array, and a BitMap-Index as a model to indicate if a data record exists or not. In this exploratory research paper, we start from this premise and posit that all existing index structures can be replaced with other types of models, including deep-learning models, which we term em learned indexes. We theoretically analyze under which conditions learned indexes outperform traditional index structures and describe the main challenges in designing learned index structures. Our initial results show that our learned indexes can have significant advantages over traditional indexes. More importantly, we believe that the idea of replacing core components of a data management system through learned models has far reaching implications for future systems designs and that this work provides just a glimpse of what might be possible.},
booktitle = {Proceedings of the 2018 International Conference on Management of Data},
pages = {489–504},
numpages = {16},
keywords = {cdf, hash-map, learned index structure, linear regression, bloom-filter, learned data structures, mixture of experts, learned index, neural net, b-tree, index structures},
location = {Houston, TX, USA},
series = {SIGMOD '18}
}

@inproceedings {258965,
author = {Yifan Dai and Yien Xu and Aishwarya Ganesan and Ramnatthan Alagappan and Brian Kroth and Andrea Arpaci-Dusseau and Remzi Arpaci-Dusseau},
title = {From {WiscKey} to Bourbon: A Learned Index for {Log-Structured} Merge Trees},
booktitle = {14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20)},
year = {2020},
isbn = {978-1-939133-19-9},
pages = {155--171},
url = {https://www.usenix.org/conference/osdi20/presentation/dai},
publisher = {USENIX Association},
month = nov,
}

@INPROCEEDINGS{4085157,
  author={Negi, Atul and Kumar, P. Kishore},
  booktitle={TENCON 2005 - 2005 IEEE Region 10 Conference}, 
  title={Applying Machine Learning Techniques to Improve Linux Process Scheduling}, 
  year={2005},
  volume={},
  number={},
  pages={1-6},
  doi={10.1109/TENCON.2005.300837}}

@inproceedings{49008,
title	= {Learning-based Memory Allocation for C++ Server Workloads},
author	= {Martin Maas and David G. Andersen and Michael Isard and Mohammad Mahdi Javanmard and Kathryn S. McKinley and Colin Raffel},
year	= {2020},
booktitle	= {25th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)}
}



@inproceedings{10.1145/3409963.3410492,
author = {Chen, Jingde and Banerjee, Subho S. and Kalbarczyk, Zbigniew T. and Iyer, Ravishankar K.},
title = {Machine Learning for Load Balancing in the Linux Kernel},
year = {2020},
isbn = {9781450380690},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3409963.3410492},
doi = {10.1145/3409963.3410492},
abstract = {The OS load balancing algorithm governs the performance gains provided by a multiprocessor computer system. The Linux's Completely Fair Scheduler (CFS) scheduler tracks process loads by average CPU utilization to balance workload between processor cores. That approach maximizes the utilization of processing time but overlooks the contention for lower-level hardware resources. In servers running compute-intensive workloads, an imbalanced need for limited computing resources hinders execution performance. This paper solves the above problem using a machine learning (ML)-based resource-aware load balancer. We describe (1) low-overhead methods for collecting training data; (2) an ML model based on a multi-layer perceptron model that imitates the CFS load balancer based on the collected training data; and (3) an in-kernel implementation of inference on the model. Our experiments demonstrate that the proposed model has an accuracy of 99% in making migration decisions and while only increasing the latency by 1.9 μs.},
booktitle = {Proceedings of the 11th ACM SIGOPS Asia-Pacific Workshop on Systems},
pages = {67–74},
numpages = {8},
keywords = {operating system, load balancing, neural network, completely fair scheduler, machine learning, Linux kernel},
location = {Tsukuba, Japan},
series = {APSys '20}
}

@inbook{10.5555/3488766.3488776,
author = {Hao, Mingzhe and Toksoz, Levent and Li, Nanqinqin and Halim, Edward Edberg and Hoffmann, Henry and Gunawi, Haryadi S.},
title = {LinnOS: Predictability on Unpredictable Flash Storage with a Light Neural Network},
year = {2020},
isbn = {978-1-939133-19-9},
publisher = {USENIX Association},
address = {USA},
abstract = {This paper presents LinnOS, an operating system that leverages a light neural network for inferring SSD performance at a very fine--per-IO--granularity and helps parallel storage applications achieve performance predictability. LinnOS supports black-box devices and real production traces without requiring any extra input from users, while outperforming industrial mechanisms and other approaches. Our evaluation shows that, compared to hedging and heuristic-based methods, LinnOS improves the average I/O latencies by 9.6-79.6% with 87-97%inference accuracy and 4-6µs inference overhead for each I/O, demonstrating that it is possible to incorporate machine learning inside operating systems for real-time decision-making.},
booktitle = {Proceedings of the 14th USENIX Conference on Operating Systems Design and Implementation},
articleno = {10},
numpages = {18}
}

@inproceedings{10.1145/3465332.3470875,
author = {Akgun, Ibrahim Umit and Aydin, Ali Selman and Shaikh, Aadil and Velikov, Lukas and Zadok, Erez},
title = {A Machine Learning Framework to Improve Storage System Performance},
year = {2021},
isbn = {9781450385503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465332.3470875},
doi = {10.1145/3465332.3470875},
abstract = {Storage systems and their OS components are designed to accommodate a wide variety of applications and dynamic workloads. Storage components inside the OS contain various heuristic algorithms to provide high performance and adaptability for different workloads. These heuristics may be tunable via parameters, and some system calls allow users to optimize their system performance. These parameters are often predetermined based on experiments with limited applications and hardware. Thus, storage systems often run with these predetermined and possibly suboptimal values. Tuning these parameters manually is impractical: one needs an adaptive, intelligent system to handle dynamic and complex workloads. Machine learning (ML) techniques are capable of recognizing patterns, abstracting them, and making predictions on new data. ML can be a key component to optimize and adapt storage systems. In this position paper, we propose KML, an ML framework for storage systems. We implemented a prototype and demonstrated its capabilities on the well-known problem of tuning optimal readahead values. Our results show that KML has a small memory footprint, introduces negligible overhead, and yet enhances throughput by as much as 2.3x.},
booktitle = {Proceedings of the 13th ACM Workshop on Hot Topics in Storage and File Systems},
pages = {94–102},
numpages = {9},
keywords = {operating systems, storage systems, machine learning, storage performance optimization},
location = {Virtual, USA},
series = {HotStorage '21}
}
@inproceedings{flicker,
author = {Petrica, Paula and Izraelevitz, Adam and Albonesi, David and Shoemaker, Christine},
year = {2013},
month = {06},
pages = {13-23},
title = {Flicker: A dynamically adaptive architecture for power limited multicore systems},
journal = {Proceedings - International Symposium on Computer Architecture},
doi = {10.1145/2485922.2485924}
}
@inbook{10.1145/3173162.3173184,
author = {Mishra, Nikita and Imes, Connor and Lafferty, John D. and Hoffmann, Henry},
title = {CALOREE: Learning Control for Predictable Latency and Low Energy},
year = {2018},
isbn = {9781450349116},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173162.3173184},
abstract = {Many modern computing systems must provide reliable latency with minimal energy. Two central challenges arise when allocating system resources to meet these conflicting goals: (1) complexity modern hardware exposes diverse resources with complicated interactions and (2) dynamics latency must be maintained despite unpredictable changes in operating environment or input. Machine learning accurately models the latency of complex, interacting resources, but does not address system dynamics; control theory adjusts to dynamic changes, but struggles with complex resource interaction. We therefore propose CALOREE, a resource manager that learns key control parameters to meet latency requirements with minimal energy in complex, dynamic en- vironments. CALOREE breaks resource allocation into two sub-tasks: learning how interacting resources affect speedup, and controlling speedup to meet latency requirements with minimal energy. CALOREE deines a general control system whose parameters are customized by a learning framework while maintaining control-theoretic formal guarantees that the latency goal will be met. We test CALOREE's ability to deliver reliable latency on heterogeneous ARM big.LITTLE architectures in both single and multi-application scenarios. Compared to the best prior learning and control solutions, CALOREE reduces deadline misses by 60% and energy consumption by 13%.},
booktitle = {Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {184–198},
numpages = {15}
}


@inbook{10.1145/3173162.3173206,
author = {Wang, Shu and Li, Chi and Hoffmann, Henry and Lu, Shan and Sentosa, William and Kistijantoro, Achmad Imam},
title = {Understanding and Auto-Adjusting Performance-Sensitive Configurations},
year = {2018},
isbn = {9781450349116},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173162.3173206},
abstract = {Modern software systems are often equipped with hundreds to thousands of configurations, many of which greatly affect performance. Unfortunately, properly setting these configurations is challenging for developers due to the complex and dynamic nature of system workload and environment. In this paper, we first conduct an empirical study to understand performance-sensitive configurations and the challenges of setting them in the real-world. Guided by our study, we design a systematic and general control-theoretic framework, SmartConf, to automatically set and dynamically adjust performance-sensitive configurations to meet required operating constraints while optimizing other performance metrics. Evaluation shows that SmartConf is effective in solving real-world configuration problems, often providing better performance than even the best static configuration developers can choose under existing configuration systems.},
booktitle = {Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {154–168},
numpages = {15}
}

@misc{https://doi.org/10.48550/arxiv.1805.03441,
  doi = {10.48550/ARXIV.1805.03441},
  
  url = {https://arxiv.org/abs/1805.03441},
  
  author = {Wang, Zheng and O'Boyle, Michael},
  
  keywords = {Programming Languages (cs.PL), Distributed, Parallel, and Cluster Computing (cs.DC), Machine Learning (cs.LG), Software Engineering (cs.SE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Machine Learning in Compiler Optimisation},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{https://doi.org/10.48550/arxiv.1803.02329,
  doi = {10.48550/ARXIV.1803.02329},
  
  url = {https://arxiv.org/abs/1803.02329},
  
  author = {Hashemi, Milad and Swersky, Kevin and Smith, Jamie A. and Ayers, Grant and Litz, Heiner and Chang, Jichuan and Kozyrakis, Christos and Ranganathan, Parthasarathy},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Learning Memory Access Patterns},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{frazier,
  doi = {10.48550/ARXIV.1807.02811},
  url = {https://arxiv.org/abs/1807.02811},
  author = {Frazier, Peter I.},
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), Optimization and Control (math.OC), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  title = {A Tutorial on Bayesian Optimization},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@book{garnett_bayesoptbook_2022,
  author    = {Garnett, Roman},
  title     = {{Bayesian Optimization}},
  year      = {2022},
  publisher = {Cambridge University Press},
  note      = {in preparation}
}

@misc{https://doi.org/10.48550/arxiv.2104.10201,
  doi = {10.48550/ARXIV.2104.10201},
  
  url = {https://arxiv.org/abs/2104.10201},
  
  author = {Turner, Ryan and Eriksson, David and McCourt, Michael and Kiili, Juha and Laaksonen, Eero and Xu, Zhen and Guyon, Isabelle},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Bayesian Optimization is Superior to Random Search for Machine Learning Hyperparameter Tuning: Analysis of the Black-Box Optimization Challenge 2020},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{https://doi.org/10.48550/arxiv.1206.2944,
  doi = {10.48550/ARXIV.1206.2944},
  
  url = {https://arxiv.org/abs/1206.2944},
  
  author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P.},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Practical Bayesian Optimization of Machine Learning Algorithms},
  
  publisher = {arXiv},
  
  year = {2012},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings {bmcmcd,
author = {Yoann Ghigoff and Julien Sopena and Kahina Lazri and Antoine Blin and Gilles Muller},
title = {{BMC}: Accelerating Memcached using Safe In-kernel Caching and Pre-stack Processing},
booktitle = {18th USENIX Symposium on Networked Systems Design and Implementation (NSDI 21)},
year = {2021},
isbn = {978-1-939133-21-2},
pages = {487--501},
url = {https://www.usenix.org/conference/nsdi21/presentation/ghigoff},
publisher = {USENIX Association},
month = apr,
}

@article{10.5555/1012889.1012894,
    author = {Fitzpatrick, Brad},
    title = {Distributed Caching with Memcached},
    year = {2004},
    issue_date = {August 2004},
    publisher = {Belltown Media},
    address = {Houston, TX},
    volume = {2004},
    number = {124},
    issn = {1075-3583},
    abstract = {Speed up your database app with a simple, fast caching layer that uses your existing servers' spare memory.},
    journal = {Linux J.},
    month = {aug},
    pages = {5}
    }

@inproceedings {twittermcd,
author = {Juncheng Yang and Yao Yue and K. V. Rashmi},
title = {A large scale analysis of hundreds of in-memory cache clusters at Twitter},
booktitle = {14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20)},
year = {2020},
isbn = {978-1-939133-19-9},
pages = {191--208},
url = {https://www.usenix.org/conference/osdi20/presentation/yang},
publisher = {USENIX Association},
month = nov,
}

@inproceedings {271072,
	author = {Michael W. Shaffer},
	title = {A Linux Appliance Construction Set},
	booktitle = {14th Systems Administration Conference (LISA 2000)},
	year = {2000},
	address = {New Orleans, LA},
	url = {https://www.usenix.org/conference/lisa-2000/linux-appliance-construction-set},
	publisher = {USENIX Association},
	month = dec,
}



@inproceedings{openverclosed,
    author = {Schroeder, Bianca and Wierman, Adam and Harchol-Balter, Mor},
    title = {Open versus Closed: A Cautionary Tale},
    year = {2006},
    publisher = {USENIX Association},
    address = {USA},
    abstract = {Workload generators may be classified as based on a closed system model, where new
    job arrivals are only triggered by job completions (followed by think time), or an
    open system model, where new jobs arrive independently of job completions. In general,
    system designers pay little attention to whether a workload generator is closed or
    open.Using a combination of implementation and simulation experiments, we illustrate
    that there is a vast difference in behavior between open and closed models in real-world
    settings. We synthesize these differences into eight simple guiding principles, which
    serve three purposes. First, the principles specify how scheduling policies are impacted
    by closed and open models, and explain the differences in user level performance.
    Second, the principles motivate the use of partly open system models, whose behavior
    we show to lie between that of closed and open models. Finally, the principles provide
    guidelines to system designers for determining which system model is most appropriate
    for a given workload.},
    booktitle = {Proceedings of the 3rd Conference on Networked Systems Design and Implementation - Volume 3},
    pages = {18},
    numpages = {1},
    location = {San Jose, CA},
    series = {NSDI'06}
}


@inproceedings {linux_appliance,
	author = {Michael W. Shaffer},
	title = {A Linux Appliance Construction Set},
	booktitle = {14th Systems Administration Conference ({LISA} 2000)},
	year = {2000},
	address = {New Orleans, LA},
	url = {https://www.usenix.org/conference/lisa-2000/linux-appliance-construction-set},
	publisher = {{USENIX} Association},
	month = dec,
}

@article{10.1145/3184899,
author = {Hanford, Nathan and Ahuja, Vishal and Farrens, Matthew K. and Tierney, Brian and Ghosal, Dipak},
title = {A Survey of End-System Optimizations for High-Speed Networks},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3184899},
doi = {10.1145/3184899},
abstract = {The gap is widening between the processor clock speed of end-system architectures
and network throughput capabilities. It is now physically possible to provide single-flow
throughput of speeds up to 100 Gbps, and 400 Gbps will soon be possible. Most current
research into high-speed data networking focuses on managing expanding network capabilities
within datacenter Local Area Networks (LANs) or efficiently multiplexing millions
of relatively small flows through a Wide Area Network (WAN). However, datacenter hyper-convergence
places high-throughput networking workloads on general-purpose hardware, and distributed
High-Performance Computing (HPC) applications require time-sensitive, high-throughput
end-to-end flows (also referred to as “elephant flows”) to occur over WANs. For these
applications, the bottleneck is often the end-system and not the intervening network.
Since the problem of the end-system bottleneck was uncovered, many techniques have
been developed which address this mismatch with varying degrees of effectiveness.
In this survey, we describe the most promising techniques, beginning with network
architectures and NIC design, continuing with operating and end-system architectures,
and concluding with clean-slate protocol design.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {54},
numpages = {36},
keywords = {end-system bottleneck, High-speed networks, flow control, experimental analysis, rate-based protocol, queueing model}
}

@misc{sloccount,
author = {{David Wheeler}},
title = {{SLOCCount}},
howpublished = {\url{https://dwheeler.com/sloccount/}}
}



@misc{seq\_file,
author = {{ Jonathan Corbet}},
title = {{The seq\_file Interface}},
howpublished = {\url{https://www.kernel.org/doc/html/latest/filesystems/seq\_file.html}}
}


@misc{ax,
author = {{Ax:Adaptive Experimentation Platform}},
howpublished = {\url{https://ax.dev/}}
}


@misc{pareto,
author = {{Pareto efficiency}},
howpublished = {\url{https://en.wikipedia.org/wiki/Pareto_efficiency}}
}

@misc{amazonmcd,
author = {{Amazon ElastiCache}},
howpublished = {\url{https://aws.amazon.com/elasticache/}}
}

@misc{mellanoxsnic,
author = {{Mellanox Innova SmartNIC}},
howpublished = {\url{http://www.businesswire.com/news/home/20160615005424/en/}}
}

@misc{netflixmcd,
author = {{Daniel Ellis}},
howpublished = {\url{https://netflixtechblog.com/ephemeral-volatile-caching-\\in-the-cloud-8eba7b124589}}
}

@misc{redditmcd,
author = {{Daniel Ellis}},
howpublished = {\url{https://web.archive.org/web/20210205121832/https://redditblog.com/2017/1/17/caching-at-reddit/}}
}



@misc{mellanoxsinterrupt,
author = {{Mellanox}},
howpublished = {\url{https://community.mellanox.com/s/article/understanding-interrupt-moderation}}
}

@misc{armdvfs,
author = {{ARM}},
howpublished = {\url{https://developer.arm.com/documentation/den0013/d/Power-Management}}
}


@misc{netronome,
howpublished = {\url{http://www.businesswire.com/news/home/20160425005805/en/}}
}

@inproceedings{10.1145/3452296.3472888,
author = {Cai, Qizhe and Chaudhary, Shubham and Vuppalapati, Midhul and Hwang, Jaehyun and Agarwal, Rachit},
title = {Understanding Host Network Stack Overheads},
year = {2021},
isbn = {9781450383837},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3452296.3472888},
doi = {10.1145/3452296.3472888},
abstract = {Traditional end-host network stacks are struggling to keep up with rapidly increasing datacenter access link bandwidths due to their unsustainable CPU overheads. Motivated by this, our community is exploring a multitude of solutions for future network stacks: from Linux kernel optimizations to partial hardware offload to clean-slate userspace stacks to specialized host network hardware. The design space explored by these solutions would benefit from a detailed understanding of CPU inefficiencies in existing network stacks.This paper presents measurement and insights for Linux kernel network stack performance for 100Gbps access link bandwidths. Our study reveals that such high bandwidth links, coupled with relatively stagnant technology trends for other host resources (e.g., CPU speeds and capacity, cache sizes, NIC buffer sizes, etc.), mark a fundamental shift in host network stack bottlenecks. For instance, we find that a single core is no longer able to process packets at line rate, with data copy from kernel to application buffers at the receiver becoming the core performance bottleneck. In addition, increase in bandwidth-delay products have outpaced the increase in cache sizes, resulting in inefficient DMA pipeline between the NIC and the CPU. Finally, we find that traditional loosely-coupled design of network stack and CPU schedulers in existing operating systems becomes a limiting factor in scaling network stack performance across cores. Based on insights from our study, we discuss implications to design of future operating systems, network protocols, and host hardware.},
booktitle = {Proceedings of the 2021 ACM SIGCOMM 2021 Conference},
pages = {65–77},
numpages = {13},
keywords = {host network stacks, network hardware, datacenter networks},
location = {Virtual Event, USA},
series = {SIGCOMM '21}
}

@inproceedings{10.1145/3357223.3362737,
author = {Golestani, Hossein and Mirhosseini, Amirhossein and Wenisch, Thomas F.},
title = {Software Data Planes: You Can't Always Spin to Win},
year = {2019},
isbn = {9781450369732},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357223.3362737},
doi = {10.1145/3357223.3362737},
abstract = {Today's datacenters demand high-performance, energy-efficient software data planes, which are widely used in many areas including fast network packet processing, network function virtualization, high-speed data transfer in storage systems, and I/O virtualization. Modern software data planes bypass OS I/O stacks and rely on cores spinning on user-level queues as a fast notification mechanism. Whereas spin-polling can improve latency and throughput, it entails significant shortcomings, especially when scaling to large numbers of cores/queues. In this paper, we pinpoint and quantify challenges of spin-polling--based software data planes using Intel's Data Plane Development Kit (DPDK) as a representative infrastructure. We characterize four scalability issues of software data planes: (1) Full-tilt spinning cores perform more (useless) polling work when there is less work pending in the queues; (2) Spin-polling scales poorly with the number of polled queues due to processor cache capacity constraints, especially when traffic is unbalanced; (3) Operation rate limits (transactions per second) as well as a Polling Tax (the overhead of polling, which is considerable even when operating at saturation throughput) result in poor core scalability. (4) Whereas shared queues can mitigate load imbalance and head-of-line-blocking, synchronization overheads limit their potential benefits. We identify root causes of these issues and discuss solution directions to improve hardware and software abstractions for better performance, efficiency, and scalability in software data planes.},
booktitle = {Proceedings of the ACM Symposium on Cloud Computing},
pages = {337–350},
numpages = {14},
keywords = {spin-polling, user-level queues, data planes},
location = {Santa Cruz, CA, USA},
series = {SoCC '19}
}

@inproceedings {ebbrt,
author = {Dan Schatzberg and James Cadden and Han Dong and Orran Krieger and Jonathan Appavoo},
title = {EbbRT: A Framework for Building Per-Application Library Operating Systems},
booktitle = {12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)},
year = {2016},
isbn = {978-1-931971-33-1},
address = {GA},
pages = {671--688},
url = {https://www.usenix.org/conference/osdi16/technical-sessions/presentation/schatzberg},
publisher = {USENIX Association},
}

@inproceedings{mootaz,
author = {Elnozahy, Mootaz and Kistler, Michael and Rajamony, Ramakrishnan},
title = {Energy Conservation Policies for Web Servers},
year = {2003},
publisher = {USENIX Association},
address = {USA},
abstract = {},
booktitle = {Proceedings of the 4th Conference on USENIX Symposium on Internet Technologies and Systems - Volume 4},
pages = {8},
numpages = {1},
location = {Seattle, WA},
series = {USITS'03}
}

@ARTICLE{7425206,
  author={Zhan, Xin and Azimi, Reza and Kanev, Svilen and Brooks, David and Reda, Sherief},
  journal={IEEE Computer Architecture Letters}, 
  title={CARB: A C-State Power Management Arbiter for Latency-Critical Workloads}, 
  year={2017},
  volume={16},
  number={1},
  pages={6-9},
  doi={10.1109/LCA.2016.2537802}}


@inproceedings {udpm,
author = {C. Chou and L. N. Bhuyan and D. Wong},
booktitle = {2019 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
title = {Î¼DPM: Dynamic Power Management for the Microsecond Era},
year = {2019},
volume = {},
issn = {},
pages = {120-132},
keywords = {power system management;servers;energy consumption;power demand;data centers;market research;linux},
doi = {10.1109/HPCA.2019.00032},
url = {https://doi.ieeecomputersociety.org/10.1109/HPCA.2019.00032},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {feb}
}

@inproceedings{peafowl,
author = {Asyabi, Esmail and Bestavros, Azer and Sharafzadeh, Erfan and Zhu, Timothy},
title = {Peafowl: In-Application CPU Scheduling to Reduce Power Consumption of in-Memory Key-Value Stores},
year = {2020},
isbn = {9781450381376},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3419111.3421298},
doi = {10.1145/3419111.3421298},
abstract = {},
booktitle = {Proceedings of the 11th ACM Symposium on Cloud Computing},
pages = {150–164},
numpages = {15},
keywords = {key-value stores, scheduling, power management},
location = {Virtual Event, USA},
series = {SoCC '20}
}

@inproceedings{10.1145/3173162.3173184,
author = {Mishra, Nikita and Imes, Connor and Lafferty, John D. and Hoffmann, Henry},
title = {CALOREE: Learning Control for Predictable Latency and Low Energy},
year = {2018},
isbn = {9781450349116},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173162.3173184},
doi = {10.1145/3173162.3173184},
abstract = {},
booktitle = {Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {184–198},
numpages = {15},
keywords = {machine learning, energy, real-time systems, heterogeneous architectures, control theory, resource allocation},
location = {Williamsburg, VA, USA},
series = {ASPLOS '18}
}

@inproceedings{wamhoff2014turbo,
title={The TURBO diaries: application-controlled frequency scaling explained},
author={Wamhoff, Jons-Tobias and Diestelhorst, Stephan and Fetzer, Christof and Marlier, Patrick and Felber, Pascal and Dice, Dave},
booktitle={Proceedings of the 2014 USENIX conference on USENIX Annual Technical Conference},
pages={193--204},
year={2014},
organization={USENIX Association}
}

@inproceedings{10.5555/2338816.2338822,
author = {Laros, James H. and Pedretti, Kevin T. and Kelly, Suzanne M. and Shu, Wei and Vaughan, Courtenay T.},
title = {Energy Based Performance Tuning for Large Scale High Performance Computing Systems},
year = {2012},
isbn = {9781618397881},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {},
articleno = {6},
numpages = {10},
keywords = {high performance computing (HPC), frequency scaling, power, energy efficiency},
location = {Orlando, Florida},
series = {HPC '12}
}

@INPROCEEDINGS{573184,
  author={Horowitz, M. and Indermaur, T. and Gonzalez, R.},
  booktitle={Proceedings of 1994 IEEE Symposium on Low Power Electronics}, 
  title={Low-power digital design}, 
  year={1994},
  volume={},
  number={},
  pages={8-11},
doi={10.1109/LPE.1994.573184}}


@inproceedings{slowdownorsleep,
author = {Le Sueur, Etienne and Heiser, Gernot},
title = {Slow down or Sleep, That is the Question},
year = {2011},
publisher = {USENIX Association},
address = {USA},
abstract = {Energy consumption has become a major concern for all computing systems, from servers in data-centres to mobile phones. Processor manufacturers have reacted to this by implementing power-management mechanisms in the hardware and researchers have investigated how operating systems can make use of those mechanisms to minimise energy consumption. Much of this research has focused on a single class of systems and compute-intensive workloads.Missing is an examination of how much energy can actually be saved when running realistic workloads on different classes of systems. This paper compares the effects of using dynamic voltage and frequency scaling (DVFS) and sleep states on platforms using server, desktop and embedded processors. It also analyses workloads that represent real-world uses of those systems. In these circumstances, we find that usage of power-management mechanisms is not clear-cut, and that it is critical to analyse the system as a whole, including the workload, to determine whether using mechanisms such as DVFS will be effective at reducing energy consumption.},
booktitle = {Proceedings of the 2011 USENIX Conference on USENIX Annual Technical Conference},
pages = {16},
numpages = {1},
location = {Portland, OR},
series = {USENIXATC'11}
}

@inproceedings{pacingtoidle,
author = {Kim, David H. K. and Imes, Connor and Hoffmann, Henry},
title = {Racing and Pacing to Idle: Theoretical and Empirical Analysis of Energy Optimization Heuristics},
year = {2015},
isbn = {9781467377850},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/CPSNA.2015.23},
doi = {10.1109/CPSNA.2015.23},
abstract = {},
booktitle = {Proceedings of the 2015 IEEE 3rd International Conference on Cyber-Physical Systems, Networks, and Applications},
pages = {78–85},
numpages = {8},
series = {CPSNA '15}
}

@inproceedings{10.1145/381677.381702,
author = {Flautner, Kriszti\'{a}n and Reinhardt, Steve and Mudge, Trevor},
title = {Automatic Performance Setting for Dynamic Voltage Scaling},
year = {2001},
isbn = {1581134223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/381677.381702},
doi = {10.1145/381677.381702},
abstract = {},
booktitle = {Proceedings of the 7th Annual International Conference on Mobile Computing and Networking},
pages = {260–271},
numpages = {12},
location = {Rome, Italy},
series = {MobiCom '01}
}

@ARTICLE{917539,
  author={Mudge, T.},
  journal={Computer}, 
  title={Power: a first-class architectural design constraint}, 
  year={2001},
  volume={34},
  number={4},
  pages={52-58},
  doi={10.1109/2.917539}
}
  

  



@ARTICLE{7349225,
  author={Hwang, Inkwon and Pedram, Massoud},
  journal={IEEE Transactions on Very Large Scale Integration (VLSI) Systems}, 
  title={A Comparative Study of the Effectiveness of CPU Consolidation Versus Dynamic Voltage and Frequency Scaling in a Virtualized Multicore Server}, 
  year={2016},
  volume={24},
  number={6},
  pages={2103-2116},
  doi={10.1109/TVLSI.2015.2499601}}


@inproceedings {twine,
author = {Chunqiang Tang and Kenny Yu and Kaushik Veeraraghavan and Jonathan Kaldor and Scott Michelson and Thawan Kooburat and Aravind Anbudurai and Matthew Clark and Kabir Gogia and Long Cheng and Ben Christensen and Alex Gartrell and Maxim Khutornenko and Sachin Kulkarni and Marcin Pawlowski and Tuomas Pelkonen and Andre Rodrigues and Rounak Tibrewal and Vaishnavi Venkatesan and Peter Zhang},
title = {Twine: A Unified Cluster Management System for Shared Infrastructure},
booktitle = {14th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 20)},
year = {2020},
isbn = {978-1-939133-19-9},
pages = {787--803},
url = {https://www.usenix.org/conference/osdi20/presentation/tang},
publisher = {{USENIX} Association},
month = nov,
}

@inproceedings{10.1145/2830772.2830779,
author = {Vamanan, Balajee and Sohail, Hamza Bin and Hasan, Jahangir and Vijaykumar, T. N.},
title = {TimeTrader: Exploiting Latency Tail to Save Datacenter Energy for Online Search},
year = {2015},
isbn = {9781450340342},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2830772.2830779},
doi = {10.1145/2830772.2830779},
abstract = {Online Search (OLS) is a key component of many popular Internet services. Datacenters running OLS consume significant amounts of energy. However, reducing their energy is challenging due to their tight response time requirements. A key aspect of OLS is that each user query goes to all or many of the nodes in the cluster, so that the overall time budget is dictated by the tail of the replies' latency distribution; replies see latency variations both in the network and compute. Previous work proposes to achieve load-proportional energy by slowing down the computation at lower datacenter loads based directly on response times (i.e., at lower loads, the proposal exploits the average slack in the time budget provisioned for the peak load). In contrast, we propose TimeTrader to reduce energy by exploiting the latency slack in the sub-critical replies which arrive before the deadline (e.g., 80% of replies are 3-4x faster than the tail). This slack is present at all loads and subsumes the previous work's load-related slack. While the previous work shifts the leaves' response time distribution to consume the slack at lower loads, TimeTrader reshapes the distribution at all loads by slowing down individual sub-critical nodes without increasing missed deadlines. TimeTrader exploits slack in both the network and compute budgets. Further, TimeTrader leverages Earliest Deadline First scheduling to largely decouple critical requests from the queuing delays of sub-critical requests which can then be slowed down without hurting critical requests. A combination of real-system measurements and at-scale simulations shows that without adding to missed deadlines, TimeTrader saves 15% and 40% energy at 90% and 30% loading, respectively, in a datacenter with 512 nodes, whereas previous work saves 0% and 30%. Further, as a proof-of-concept, we build a small-scale real implementation to evaluate TimeTrader and show 10-30% energy savings.},
booktitle = {Proceedings of the 48th International Symposium on Microarchitecture},
pages = {585–597},
numpages = {13},
keywords = {datacenter, incast, latency tail, online search (OLS), online data-intensive (OLDI) applications},
location = {Waikiki, Hawaii},
series = {MICRO-48}
}

@inproceedings {stackmap,
author = {Kenichi Yasukata and Michio Honda and Douglas Santry and Lars Eggert},
title = {{StackMap}: {Low-Latency} Networking with the {OS} Stack and Dedicated {NICs}},
booktitle = {2016 USENIX Annual Technical Conference (USENIX ATC 16)},
year = {2016},
isbn = {978-1-931971-30-0},
address = {Denver, CO},
pages = {43--56},
url = {https://www.usenix.org/conference/atc16/technical-sessions/presentation/yasukata},
publisher = {USENIX Association},
month = jun,
}

@inproceedings{ix,
author = {Belay, Adam and Prekas, George and Klimovic, Ana and Grossman, Samuel and Kozyrakis, Christos and Bugnion, Edouard},
title = {IX: A Protected Dataplane Operating System for High Throughput and Low Latency},
year = {2014},
isbn = {9781931971164},
publisher = {USENIX Association},
address = {USA},
abstract = {The conventional wisdom is that aggressive networking requirements, such as high packet rates for small messages and microsecond-scale tail latency, are best addressed outside the kernel, in a user-level networking stack. We present IX, a dataplane operating system that provides high I/O performance, while maintaining the key advantage of strong protection offered by existing kernels. IX uses hardware virtualization to separate management and scheduling functions of the kernel (control plane) from network processing (dataplane). The data-plane architecture builds upon a native, zero-copy API and optimizes for both bandwidth and latency by dedicating hardware threads and networking queues to data-plane instances, processing bounded batches of packets to completion, and by eliminating coherence traffic and multi-core synchronization. We demonstrate that IX outperforms Linux and state-of-the-art, user-space network stacks significantly in both throughput and end-to-end latency. Moreover, IX improves the throughput of a widely deployed, key-value store by up to 3.6x and reduces tail latency by more than 2x.},
booktitle = {Proceedings of the 11th USENIX Conference on Operating Systems Design and Implementation},
pages = {49–65},
numpages = {17},
location = {Broomfield, CO},
series = {OSDI'14}
}

@INPROCEEDINGS{4273098,
  author={Lefurgy, Charles and Wang, Xiaorui and Ware, Malcolm},
  booktitle={Fourth International Conference on Autonomic Computing (ICAC'07)}, 
  title={Server-Level Power Control}, 
  year={2007},
  volume={},
  number={},
  pages={4-4},
  doi={10.1109/ICAC.2007.35}}

@misc{dash,
author={ploty},
title={{Dash Overview}},
howpublished = {\url{https://plotly.com/dash/}}
}

@article{arrakis,
author = {Peter, Simon and Li, Jialin and Zhang, Irene and Ports, Dan R. K. and Woos, Doug and Krishnamurthy, Arvind and Anderson, Thomas and Roscoe, Timothy},
title = {Arrakis: The Operating System Is the Control Plane},
year = {2015},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {4},
issn = {0734-2071},
url = {https://doi.org/10.1145/2812806},
doi = {10.1145/2812806},
abstract = {Recent device hardware trends enable a new approach to the design of network server operating systems. In a traditional operating system, the kernel mediates access to device hardware by server applications to enforce process isolation as well as network and disk security. We have designed and implemented a new operating system, Arrakis, that splits the traditional role of the kernel in two. Applications have direct access to virtualized I/O devices, allowing most I/O operations to skip the kernel entirely, while the kernel is re-engineered to provide network and disk protection without kernel mediation of every operation. We describe the hardware and software changes needed to take advantage of this new abstraction, and we illustrate its power by showing improvements of 2 to 5 \texttimes{} in latency and 9 \texttimes{} throughput for a popular persistent NoSQL store relative to a well-tuned Linux implementation.},
journal = {ACM Trans. Comput. Syst.},
month = nov,
articleno = {11},
numpages = {30},
keywords = {SR-IOV, Kernel bypass, I/O virtualization}
}

@inproceedings {mtcp,
author = {EunYoung Jeong and Shinae Wood and Muhammad Jamshed and Haewon Jeong and Sunghwan Ihm and Dongsu Han and KyoungSoo Park},
title = {mTCP: a Highly Scalable User-level {TCP} Stack for Multicore Systems},
booktitle = {11th {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI} 14)},
year = {2014},
isbn = {978-1-931971-09-6},
address = {Seattle, WA},
pages = {489--502},
url = {https://www.usenix.org/conference/nsdi14/technical-sessions/presentation/jeong},
publisher = {{USENIX} Association},
month = apr,
}

@inproceedings{sandstorm,
 author = {Marinos, Ilias and Watson, Robert N.M. and Handley, Mark},
 title = {Network Stack Specialization for Performance},
 booktitle = {Proceedings of the 2014 ACM Conference on SIGCOMM},
 series = {SIGCOMM '14},
 year = {2014},
 isbn = {978-1-4503-2836-4},
 location = {Chicago, Illinois, USA},
 pages = {175--186},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/2619239.2626311},
 doi = {10.1145/2619239.2626311},
 acmid = {2626311},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {clean-slate design, network performance, network stacks, network- stack specialization},
} 

@inproceedings{affinityaccept,
 author = {Pesterev, Aleksey and Strauss, Jacob and Zeldovich, Nickolai and Morris, Robert T.},
 title = {Improving Network Connection Locality on Multicore Systems},
 year = {2012},
 isbn = {9781450312233},
 publisher = {Association for Computing Machinery},
 address = {New York, NY, USA},
 url = {https://doi.org/10.1145/2168836.2168870},
 doi = {10.1145/2168836.2168870},
 booktitle = {Proceedings of the 7th ACM European Conference on Computer Systems},
 pages = {337–350},
 numpages = {14},
 keywords = {multi-core, cache misses, packet processing},
 location = {Bern, Switzerland},
 series = {EuroSys ’12}
}

@inproceedings{flexnic,
 author = {Kaufmann, Antoine and Peter, SImon and Sharma, Naveen Kr. and Anderson, Thomas and Krishnamurthy, Arvind},
 title = {High Performance Packet Processing with FlexNIC},
 year = {2016},
 isbn = {9781450340915},
 publisher = {Association for Computing Machinery},
 address = {New York, NY, USA},
 url = {https://doi.org/10.1145/2872362.2872367},
 doi = {10.1145/2872362.2872367},
 booktitle = {Proceedings of the Twenty-First International Conference on Architectural Support for Programming Languages and Operating Systems},
 pages = {67–81},
 numpages = {15},
 keywords = {match-and-action processing, DMA, network interface card, flexible network processing},
 location = {Atlanta, Georgia, USA},
 series = {ASPLOS ’16}
}

@inproceedings{ixcp,
author = {Prekas, George and Primorac, Mia and Belay, Adam and Kozyrakis, Christos and Bugnion, Edouard},
title = {Energy Proportionality and Workload Consolidation for Latency-Critical Applications},
year = {2015},
isbn = {9781450336512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2806777.2806848},
doi = {10.1145/2806777.2806848},
booktitle = {Proceedings of the Sixth ACM Symposium on Cloud Computing},
pages = {342–355},
numpages = {14},
keywords = {latency-critical applications, energy proportionality, workload consolidation, microsecond-scale computing},
location = {Kohala Coast, Hawaii},
series = {SoCC '15}
}

@inproceedings{SmoothOperator,
 author = {Hsu, Chang-Hong and Deng, Qingyuan and Mars, Jason and Tang, Lingjia},
 title = {SmoothOperator: Reducing Power Fragmentation and Improving Power Utilization in Large-scale Datacenters},
 booktitle = {Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems},
 series = {ASPLOS '18},
 year = {2018},
 isbn = {978-1-4503-4911-6},
 location = {Williamsburg, VA, USA},
 pages = {535--548},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/3173162.3173190},
 doi = {10.1145/3173162.3173190},
 acmid = {3173190},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {datacenter power management, fragmentation, power and energy efficiency},
} 

@inproceedings{eurosys14,
author = {Leverich, Jacob and Kozyrakis, Christos},
title = {Reconciling High Server Utilization and Sub-Millisecond Quality-of-Service},
year = {2014},
isbn = {9781450327046},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2592798.2592821},
doi = {10.1145/2592798.2592821},
booktitle = {Proceedings of the Ninth European Conference on Computer Systems},
articleno = {4},
numpages = {14},
location = {Amsterdam, The Netherlands},
series = {EuroSys '14}
}

@inproceedings{Dynamo,
 author = {Wu, Qiang and Deng, Qingyuan and Ganesh, Lakshmi and Hsu, Chang-Hong and Jin, Yun and Kumar, Sanjeev and Li, Bin and Meza, Justin and Song, Yee Jiun},
 title = {Dynamo: Facebook's Data Center-wide Power Management System},
 booktitle = {Proceedings of the 43rd International Symposium on Computer Architecture},
 series = {ISCA '16},
 year = {2016},
 isbn = {978-1-4673-8947-1},
 location = {Seoul, Republic of Korea},
 pages = {469--480},
 numpages = {12},
 url = {https://doi.org/10.1109/ISCA.2016.48},
 doi = {10.1109/ISCA.2016.48},
 acmid = {3001187},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA},
 keywords = {data center, management, power},
} 

@inproceedings{PerAppPower,
author = {Guliani, Akhil and Swift, Michael M.},
title = {Per-Application Power Delivery},
year = {2019},
isbn = {9781450362818},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302424.3303981},
doi = {10.1145/3302424.3303981},
booktitle = {Proceedings of the Fourteenth EuroSys Conference 2019},
articleno = {5},
numpages = {16},
keywords = {DVFS, Proportional Shares, Power Management},
location = {Dresden, Germany},
series = {EuroSys '19}
}

@article{heracles,
author = {Lo, David and Cheng, Liqun and Govindaraju, Rama and Ranganathan, Parthasarathy and Kozyrakis, Christos},
title = {Heracles: Improving Resource Efficiency at Scale},
year = {2015},
issue_date = {June 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {3S},
issn = {0163-5964},
url = {https://doi.org/10.1145/2872887.2749475},
doi = {10.1145/2872887.2749475},
journal = {SIGARCH Comput. Archit. News},
month = jun,
pages = {450–462},
numpages = {13}
}

@inproceedings{NLP-energy,
    title = "Energy and Policy Considerations for Deep Learning in {NLP}",
    author = "Strubell, Emma  and
      Ganesh, Ananya  and
      McCallum, Andrew",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1355",
    doi = "10.18653/v1/P19-1355",
    pages = "3645--3650",
}

@inproceedings{oldi-study,
author = {Meisner, David and Sadler, Christopher M. and Barroso, Luiz Andr\'{e} and Weber, Wolf-Dietrich and Wenisch, Thomas F.},
title = {Power Management of Online Data-Intensive Services},
year = {2011},
isbn = {9781450304726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2000064.2000103},
doi = {10.1145/2000064.2000103},
booktitle = {Proceedings of the 38th Annual International Symposium on Computer Architecture},
pages = {319–330},
numpages = {12},
keywords = {servers, power management},
location = {San Jose, California, USA},
series = {ISCA '11}
}

@inproceedings{10.1145/1629911.1629926,
author = {Lee, Jungseob and Kim, Nam Sung},
title = {Optimizing Throughput of Power- and Thermal-Constrained Multicore Processors Using DVFS and per-Core Power-Gating},
year = {2009},
isbn = {9781605584973},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1629911.1629926},
doi = {10.1145/1629911.1629926},
abstract = {Process variability from a range of sources is growing as technology scales below 65nm, resulting in increasingly nonuniform transistor delay and leakage power both within a die and across dies. As a result, the negative impact of process variations on the maximum operating frequency and the total power consumption of a processor is expected to worsen. Meanwhile, manufacturers have integrated more cores in a single die, substantially improving the throughput of a processor running highly-parallel applications. However, many existing applications do not have high enough parallelism to exploit multiple cores in a die. In this paper, first, we analyze the throughput impact of applying per-core power gating and dynamic voltage and frequency scaling to power- and thermal-constrained multicore processors. To optimize the throughput of the multicore processors running applications with limited parallelism, we exploit power- and thermal-headroom resulted from power-gated idle cores, allowing active cores to increase operating frequency through supply voltage scaling. Our analysis using a 32nm predictive technology model shows that optimizing the number of active cores and operating frequency within power, thermal, and supply voltage scaling limits improves the throughput of a 16-core processor by ~16. Furthermore, we extend our throughput analysis and optimization to consider the impact of within-die process variations leading to core-to-core frequency (and leakage power) variations in a multicore processor. Our analysis shows that exploiting core-to-core frequency variations improves the throughput of a 16-core processor by ~75%.},
booktitle = {Proceedings of the 46th Annual Design Automation Conference},
pages = {47–50},
numpages = {4},
keywords = {DVFS, power gating, multicore processor},
location = {San Francisco, California},
series = {DAC '09}
}

@inproceedings{oldi-pegasus,
author = {Lo, David and Cheng, Liqun and Govindaraju, Rama and Barroso, Luiz Andr\'{e} and Kozyrakis, Christos},
title = {Towards Energy Proportionality for Large-Scale Latency-Critical Workloads},
year = {2014},
isbn = {9781479943944},
publisher = {IEEE Press},
booktitle = {Proceeding of the 41st Annual International Symposium on Computer Architecuture},
pages = {301–312},
numpages = {12},
location = {Minneapolis, Minnesota, USA},
series = {ISCA '14}
}

@INPROCEEDINGS{rubik,
  author={H. {Kasture} and D. B. {Bartolini} and N. {Beckmann} and D. {Sanchez}},
  booktitle={2015 48th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)}, 
  title={Rubik: Fast analytical power management for latency-critical systems}, 
  year={2015},
  volume={},
  number={},
  pages={598-610},}


@misc{pytorchadam,
author={pytorch},
title={{Adam}},
howpublished = {\url{https://pytorch.org/docs/stable/generated/torch.optim.Adam.html}}
}

@inproceedings{adrenaline,
  author={C. {Hsu} and Y. {Zhang} and M. A. {Laurenzano} and D. {Meisner} and T. {Wenisch} and J. {Mars} and L. {Tang} and R. G. {Dreslinski}},
  booktitle={2015 IEEE 21st International Symposium on High Performance Computer Architecture (HPCA)}, 
  title={Adrenaline: Pinpointing and reining in tail queries with quick voltage boosting}, 
  year={2015},
  volume={},
  number={},
  pages={271-282},
  }

@inproceedings{powercap,
author={P. {Petoumenos} and L. {Mukhanov} and Z. {Wang} and H. {Leather} and D. S. {Nikolopoulos}},
booktitle={2015 IEEE 21st International Conference on Parallel and Distributed Systems (ICPADS)}, 
title={Power Capping: What Works, What Does Not}, 
year={2015},
volume={},
number={},
pages={525-534},
doi={10.1109/ICPADS.2015.72}
}



@inproceedings{powernap,
author = {Meisner, David and Gold, Brian T. and Wenisch, Thomas F.},
title = {PowerNap: Eliminating Server Idle Power},
year = {2009},
isbn = {9781605584065},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1508244.1508269},
doi = {10.1145/1508244.1508269},
booktitle = {Proceedings of the 14th International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {205–216},
numpages = {12},
keywords = {power management, servers},
location = {Washington, DC, USA},
series = {ASPLOS XIV}
}

@inproceedings{hotpower2008,
author = {Tolia, Niraj and Wang, Zhikui and Marwah, Manish and Bash, Cullen and Ranganathan, Parthasarathy and Zhu, Xiaoyun},
title = {Delivering Energy Proportionality with Non Energy-Proportional Systems: Optimizing the Ensemble},
year = {2008},
publisher = {USENIX Association},
address = {USA},
booktitle = {Proceedings of the 2008 Conference on Power Aware Computing and Systems},
pages = {2},
numpages = {1},
location = {San Diego, California},
series = {HotPower'08}
}


@inproceedings{warehouse-power,
 author = {Fan, Xiaobo and Weber, Wolf-Dietrich and Barroso, Luiz Andre},
 title = {Power Provisioning for a Warehouse-Sized Computer},
 year = {2007},
 isbn = {9781595937063},
 publisher = {Association for Computing Machinery},
 address = {New York, NY, USA},
 url = {https://doi.org/10.1145/1250662.1250665},
 doi = {10.1145/1250662.1250665},
 booktitle = {Proceedings of the 34th Annual International Symposium on Computer Architecture},
 pages = {13–23},
 numpages = {11},
 keywords = {power provisioning, power modeling, energy efficiency},
 location = {San Diego, California, USA},
 series = {ISCA ’07}
}

@article{seda,
    author = {Welsh, Matt and Culler, David and Brewer, Eric},
    title = {SEDA: An Architecture for Well-Conditioned, Scalable Internet Services},
    year = {2001},
    issue_date = {Dec. 2001},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {35},
    number = {5},
    issn = {0163-5980},
    url = {https://doi.org/10.1145/502059.502057},
    doi = {10.1145/502059.502057},
    abstract = {We propose a new design for highly concurrent Internet services, which we call the staged event-driven architecture (SEDA). SEDA is intended to support massive concurrency demands and simplify the construction of well-conditioned services. In SEDA, applications consist of a network of event-driven stages connected by explicit queues. This architecture allows services to be well-conditioned to load, preventing resources from being overcommitted when demand exceeds service capacity. SEDA makes use of a set of dynamic resource controllers to keep stages within their operating regime despite large fluctuations in load. We describe several control mechanisms for automatic tuning and load conditioning, including thread pool sizing, event batching, and adaptive load shedding. We present the SEDA design and an implementation of an Internet services platform based on this architecture. We evaluate the use of SEDA through two applications: a high-performance HTTP server and a packet router for the Gnutella peer-to-peer file sharing network. These results show that SEDA applications exhibit higher performance than traditional service designs, and are robust to huge variations in load.},
    journal = {SIGOPS Oper. Syst. Rev.},
    month = oct,
    pages = {230–243},
    numpages = {14}
    }
@INPROCEEDINGS{4658633,
  author={Wonyoung Kim and Gupta, Meeta S. and Wei, Gu-Yeon and Brooks, David},
  booktitle={2008 IEEE 14th International Symposium on High Performance Computer Architecture}, 
  title={System level analysis of fast, per-core DVFS using on-chip switching regulators}, 
  year={2008},
  volume={},
  number={},
  pages={123-134},
  doi={10.1109/HPCA.2008.4658633}}

@inproceedings{10.1109/IGCC.2011.6008552,
author = {Spiliopoulos, V. and Kaxiras, S. and Keramidas, G.},
title = {Green Governors: A Framework for Continuously Adaptive DVFS},
year = {2011},
isbn = {9781457712227},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/IGCC.2011.6008552},
doi = {10.1109/IGCC.2011.6008552},
abstract = {We present Continuously Adaptive Dynamic Voltage/Frequency scaling in Linux systems running on Intel i7 and AMD Phenom II processors. By exploiting slack, inherent in memory-bound programs, our approach aims to improve power efficiency even when the processor does not sit idle. Our underlying methodology is based on a simple first-order processor performance model in which frequency scaling is expressed as a change (in cycles) of the main memory latency. Utilizing available monitoring hardware we show that our model is powerful enough to i) predict with reasonable accuracy the effect of frequency scaling (in terms of performance loss) and ii) predict the core energy under different V/f combinations. To validate our approach we perform highly accurate, fine-grained power measurements directly on the off-chip voltage regulators. We use our model to implement various DVFS policies as Linux "green" governors to continuously optimize for various power-efficiency metrics such as EDP or ED^2P, or achieve energy savings with a user-specified limit on performance loss. Our evaluation shows that, for SPEC2006 workloads, our governors achieve dynamically the same optimal EDP or ED$^2$P (within 2% on avg.) as an exhaustive search of all possible frequencies. Energy savings can reach up to 56% in memory-bound workloads with corresponding improvements of about 55% for EDP or ED$^2$P.},
booktitle = {Proceedings of the 2011 International Green Computing Conference and Workshops},
pages = {1–8},
numpages = {8},
keywords = {continuously adaptive dynamic voltage-frequency scaling, memory bound workloads, EDP, memory bound programs, frequency scaling, AMD Phenom II processors, off chip voltage regulators, Linux systems, green governors, ED$^2$P, power efficiency, Intel i7},
series = {IGCC '11}
}

@INPROCEEDINGS{4343825,
  author={Ge, Rong and Feng, Xizhou and Feng, Wu-chun and Cameron, Kirk W.},
  booktitle={2007 International Conference on Parallel Processing (ICPP 2007)}, 
  title={CPU MISER: A Performance-Directed, Run-Time System for Power-Aware Clusters}, 
  year={2007},
  volume={},
  number={},
  pages={18-18},
  doi={10.1109/ICPP.2007.29}}

@article{10.1145/1241601.1241609,
author = {Kondo, Masaaki and Sasaki, Hiroshi and Nakamura, Hiroshi},
title = {Improving Fairness, Throughput and Energy-Efficiency on a Chip Multiprocessor through DVFS},
year = {2007},
issue_date = {March 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/1241601.1241609},
doi = {10.1145/1241601.1241609},
abstract = {Recently, a single chip multiprocessor (CMP) is becoming an attractive architecture for improving throughput of program execution. In CMPs, multiple processor cores share several hardware resources such as cache memory and memory bus. Therefore, the resource contention significantly degrades performance of each thread and also loses fairness between threads.In this paper, we propose a Dynamic Frequency and Voltage Scaling (DVFS) algorithm for improving total instruction throughput, fairness, and energy efficiency of CMPs. The proposed technique periodically observes the utilization ratio of shared resources and controls the frequency and the voltage of each processor core individually to balance the ratio between threads. We evaluate our technique and the evaluation results show that fairness between threads are greatly improved by the technique. Moreover, the total instruction throughput increases in many cases while reducing energy consumption.},
journal = {SIGARCH Comput. Archit. News},
month = mar,
pages = {31–38},
numpages = {8}
}

@INPROCEEDINGS{6983037,
  author={Kanev, Svilen and Hazelwood, Kim and Wei, Gu-Yeon and Brooks, David},
  booktitle={2014 IEEE International Symposium on Workload Characterization (IISWC)}, 
  title={Tradeoffs between power management and tail latency in warehouse-scale applications}, 
  year={2014},
  volume={},
  number={},
  pages={31-40},
  doi={10.1109/IISWC.2014.6983037}}

      
@inproceedings{packandcap,
author = {Cochran, Ryan and Hankendi, Can and Coskun, Ayse K. and Reda, Sherief},
title = {{Pack \& Cap: Adaptive DVFS and Thread Packing under Power Caps}},
year = {2011},
isbn = {9781450310536},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2155620.2155641},
doi = {10.1145/2155620.2155641},
abstract = {The ability to cap peak power consumption is a desirable feature in modern data centers for energy budgeting, cost management, and efficient power delivery. Dynamic voltage and frequency scaling (DVFS) is a traditional control knob in the tradeoff between server power and performance. Multi-core processors and the parallel applications that take advantage of them introduce new possibilities for control, wherein workload threads are packed onto a variable number of cores and idle cores enter low-power sleep states. This paper proposes Pack & Cap, a control technique designed to make optimal DVFS and thread packing control decisions in order to maximize performance within a power budget. In order to capture the workload dependence of the performance-power Pareto frontier, a multinomial logistic regression (MLR) classifier is built using a large volume of performance counter, temperature, and power characterization data. When queried during runtime, the classifier is capable of accurately selecting the optimal operating point. We implement and validate this method on a real quad-core system running the PARSEC parallel benchmark suite. When varying the power budget during runtime, Pack & Cap meets power constraints 82% of the time even in the absence of a power measuring device. The addition of thread packing to DVFS as a control knob increases the range of feasible power constraints by an average of 21% when compared to DVFS alone and reduces workload energy consumption by an average of 51.6% compared to existing control techniques that achieve the same power range.},
booktitle = {Proceedings of the 44th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {175–185},
numpages = {11},
location = {Porto Alegre, Brazil},
series = {MICRO-44}
}

@article{napsac,
 author = {Krioukov, Andrew and Mohan, Prashanth and Alspaugh, Sara and Keys, Laura and Culler, David and Katz, Randy},
 title = {NapSAC: Design and Implementation of a Power-Proportional Web Cluster},
 year = {2011},
 issue_date = {January 2011},
 publisher = {Association for Computing Machinery},
 address = {New York, NY, USA},
 volume = {41},
 number = {1},
 issn = {0146-4833},
 url = {https://doi.org/10.1145/1925861.1925878},
 doi = {10.1145/1925861.1925878},
 journal = {SIGCOMM Comput. Commun. Rev.},
 month = jan,
 pages = {102–108},
 numpages = {7},
 keywords = {heterogenous hardware, energy, web server, data center, web application, cluster, power proportional, power management}
}

@inproceedings {mica,
author = {Hyeontaek Lim and Dongsu Han and David G. Andersen and Michael Kaminsky},
title = {{MICA}: A Holistic Approach to Fast In-Memory Key-Value Storage},
booktitle = {11th {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI} 14)},
year = {2014},
isbn = {978-1-931971-09-6},
address = {Seattle, WA},
pages = {429--444},
url = {https://www.usenix.org/conference/nsdi14/technical-sessions/presentation/lim},
publisher = {{USENIX} Association},
month = apr,
}

@inproceedings{large-scale-mapreduce,
 author = {Chen, Yanpei and Alspaugh, Sara and Borthakur, Dhruba and Katz, Randy},
 title = {Energy Efficiency for Large-Scale MapReduce Workloads with Significant Interactive Analysis},
 year = {2012},
 isbn = {9781450312233},
 publisher = {Association for Computing Machinery},
 address = {New York, NY, USA},
 url = {https://doi.org/10.1145/2168836.2168842},
 doi = {10.1145/2168836.2168842},
 booktitle = {Proceedings of the 7th ACM European Conference on Computer Systems},
 pages = {43–56},
 numpages = {14},
 keywords = {MapReduce, energy efficiency},
 location = {Bern, Switzerland},
 series = {EuroSys ’12}
}

@inproceedings{rapl,
 author = {David, Howard and Gorbatov, Eugene and Hanebutte, Ulf R. and Khanna, Rahul and Le, Christian},
 title = {RAPL: Memory Power Estimation and Capping},
 year = {2010},
 isbn = {9781450301466},
 publisher = {Association for Computing Machinery},
 address = {New York, NY, USA},
 url = {https://doi.org/10.1145/1840845.1840883},
 doi = {10.1145/1840845.1840883},
 booktitle = {Proceedings of the 16th ACM/IEEE International Symposium on Low Power Electronics and Design},
 pages = {189–194},
 numpages = {6},
 keywords = {measurements, power, DRAM memory},
 location = {Austin, Texas, USA},
 series = {ISLPED ’10}
}

@inproceedings{weaver_rapl,
author = {Desrochers, Spencer and Paradis, Chad and Weaver, Vincent M.},
title = {A Validation of DRAM RAPL Power Measurements},
year = {2016},
isbn = {9781450343053},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2989081.2989088},
doi = {10.1145/2989081.2989088},
abstract = {Recent Intel processors support the Running Average Power Level (RAPL) interface,
which among other things provides estimated energy measurements for the CPUs, integrated
GPU, and DRAM. These measurements are easily accessible by the user, and can be gathered
by a wide variety of tools, including the Linux perf_event interface. This allows
unprecedented easy access to energy information when designing and optimizing energy-aware
code.While greatly useful, on most systems these RAPL measurements are estimated values,
generated on the fly by an on-chip energy model. The values are not documented well,
and the results (especially the DRAM results) have undergone only limited validation.We
validate the DRAM RAPL results on both desktop and server Haswell machines, with multiple
types of DDR3 and DDR4 memory. We instrument the hardware to gather actual power measurements
and compare them to the RAPL values returned via Linux perf_event. We describe the
many challenges encountered when instrumenting systems for detailed power measurement.We
find that the RAPL results match overall energy and power trends, usually by a constant
power offset. The results match best when the DRAM is being heavily utilized, but
do not match as well in cases where the system is idle, or when an integrated GPU
is using the memory.We also verify that Haswell server machines produce more accurate
results, as they include actual power measurements gathered through the integrated
voltage regulator.},
booktitle = {Proceedings of the Second International Symposium on Memory Systems},
pages = {455–470},
numpages = {16},
keywords = {RAPL, DRAM Energy, DRAM Power},
location = {Alexandria, VA, USA},
series = {MEMSYS '16}
}

@article{rapl2018,
author = {Khan, Kashif Nizam and Hirki, Mikael and Niemi, Tapio and Nurminen, Jukka K. and Ou, Zhonghong},
title = {RAPL in Action: Experiences in Using RAPL for Power Measurements},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {2376-3639},
url = {https://doi.org/10.1145/3177754},
doi = {10.1145/3177754},
journal = {ACM Trans. Model. Perform. Eval. Comput. Syst.},
month = mar,
articleno = {9},
numpages = {26},
keywords = {RAPL validation, RAPL accuracy, DRAM power, RAPL, power modeling}
}

@article{rapl2015,
  title={A Quantitative Evaluation of the RAPL Power Control System},
  author={Zhang, Huazhe and Hoffman, H},
  journal={Feedback Computing},
  year={2015}
}

@inproceedings {thunderbolt,
author = {Shaohong Li and Xi Wang and Xiao Zhang and Vasileios Kontorinis and Sreekumar Kodakara and David Lo and Parthasarathy Ranganathan},
title = {Thunderbolt: {Throughput-Optimized}, {Quality-of-Service-Aware} Power Capping at Scale},
booktitle = {14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20)},
year = {2020},
isbn = {ISBN 978-1-939133-19-9},
pages = {1241--1255},
url = {https://www.usenix.org/conference/osdi20/presentation/li-shaohong},
publisher = {USENIX Association},
month = nov,
}

@article{energyproportion,
 author = {{Barroso, Luiz Andr\'{e} and H\"{o}lzle, Urs}},
 title = {The Case for Energy-Proportional Computing},
 year = {2007},
 issue_date = {December 2007},
 publisher = {IEEE Computer Society Press},
 address = {Washington, DC, USA},
 volume = {40},
 number = {12},
 issn = {0018-9162},
 url = {https://doi.org/10.1109/MC.2007.443},
 doi = {10.1109/MC.2007.443},
 journal = {Computer},
 month = dec,
 pages = {33–37},
 numpages = {5},
 keywords = {green computing, energy-proportional computing}
}

@misc{mcdsilo,
   author={George Prekas},
   howpublished={\url{https://github.com/ix-project/servers/tree/master}},
   year={2017}
}

@misc{nontemporal,
  title={{Memory part 5: What programmers can do}},
  author={{Ulrich Drepper}},
  howpublished={\url{https://lwn.net/Articles/255364/}}
}


@misc{libuv,
  title={},
  author={{libuv}},
  howpublished={\url{http://libuv.org}}
}

@misc{v8,
  title={{V8 JavaScript Engine}},
  author={{Google}},
  howpublished={\url{http://code.google.com/p/v8/}}
}

@inproceedings{snell1996netpipe,
  title={{Netpipe: A Network Protocol Independent Performance Evaluator}},
  author={Snell, Quinn O and Mikler, Armin R and Gustafson, John L},
  booktitle={IASTED International Conference on Intelligent Information Management and Systems},
  year={1996},
}

@misc{intel_manual,
author={Intel},
title={{Intel® 64 and IA-32 Architectures Software Developer’s Manual Volume}},
howpublished = {\url{https://www.intel.com/content/dam/www/public/us/en/documents/manuals/}}
}

@inproceedings{seuss,
author = {Cadden, James and Unger, Thomas and Awad, Yara and Dong, Han and Krieger, Orran and Appavoo, Jonathan},
title = {SEUSS: Skip Redundant Paths to Make Serverless Fast},
year = {2020},
isbn = {9781450368827},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342195.3392698},
doi = {10.1145/3342195.3392698},
abstract = {This paper presents a system-level method for achieving the rapid deployment and high-density
caching of serverless functions in a FaaS environment. For reduced start times, functions
are deployed from unikernel snapshots, bypassing expensive initialization steps. To
reduce the memory footprint of snapshots we apply page-level sharing across the entire
software stack that is required to run a function. We demonstrate the effects of our
techniques by replacing Linux on the compute node of a FaaS platform architecture.
With our prototype OS, the deployment time of a function drops from 100s of milliseconds
to under 10 ms. Platform throughput improves by 51x on workload composed entirely
of new functions. We are able to cache over 50,000 function instances in memory as
opposed to 3,000 using standard OS techniques. In combination, these improvements
give the FaaS platform a new ability to handle large-scale bursts of requests.},
booktitle = {Proceedings of the Fifteenth European Conference on Computer Systems},
articleno = {32},
numpages = {15},
location = {Heraklion, Greece},
series = {EuroSys '20}
}

@inproceedings{zygos,
 author = {Prekas, George and Kogias, Marios and Bugnion, Edouard},
 title = {ZygOS: Achieving Low Tail Latency for Microsecond-Scale Networked Tasks},
 year = {2017},
 isbn = {9781450350853},
 publisher = {Association for Computing Machinery},
 address = {New York, NY, USA},
 url = {https://doi.org/10.1145/3132747.3132780},
 doi = {10.1145/3132747.3132780},
 booktitle = {Proceedings of the 26th Symposium on Operating Systems Principles},
 pages = {325–341},
 numpages = {17},
 keywords = {Tail latency, Microsecond-scale computing},
 location = {Shanghai, China},
 series = {SOSP ’17}
}

@inproceedings{network-latency,
  author={A. {Beifuß} and D. {Raumer} and P. {Emmerich} and T. M. {Runge} and F. {Wohlfart} and B. E. {Wolfìnger} and G. {Carle}},
  booktitle={2015 International Conference and Workshops on Networked Systems (NetSys)}, 
  title={A Study of Networking Software Induced Latency}, 
  year={2015},
  volume={},
  number={},
  pages={1-8},
  doi={10.1109/NetSys.2015.7089065}
}


@misc{seq_file,
author={Jonathan Corbet},
title={{The seq\_file Interface}},
howpublished = {\url{https://www.kernel.org/doc/html/latest/filesystems/seq\_file.html
}}
}

@misc{moc,
author={{Mass Open Cloud}},
howpublished = {\url{https://massopen.cloud/}}
}

@misc{intel_msr,
author={Intel},
title={{Intel® 64 and IA-32 Architectures Software Developer’s Manual Volume 3C:System Programming Guide, Part 3}},
howpublished = {\url{https://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-software-developer-\\vol-3c-part-3-manual.pdf}}
}

@misc{intel_rapl,
author={Intel},
title={{Intel® 64 and IA-32 Architectures Software Developer’s Manual Volume 3B:System Programming Guide, Part 2}},
howpublished = {\url{https://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-software-developer-\\vol-3b-part-2-manual.pdf}}
}

@misc{cpufreq_governor,
title={{CPU frequency and voltage scaling code in the Linux(TM) kernel}},
author = {{Dominik Brodowski, Nico Golde, Rafael J. Wysocki, Viresh Kumar}},
howpublished = {\url{https://www.kernel.org/doc/Documentation/cpu-freq/governors.txt}}
}

@misc{cpuidle_policy,
title={{CPU Idle Time Management}},
author = {{Rafael J. Wysocki}},
howpublished = {\url{https://www.kernel.org/doc/html/v5.0/admin-guide/pm/cpuidle.html}}
}

@inproceedings{linux-core-ops,
author = {Ren, Xiang (Jenny) and Rodrigues, Kirk and Chen, Luyuan and Vega, Camilo and Stumm, Michael and Yuan, Ding},
title = {An Analysis of Performance Evolution of Linux's Core Operations},
year = {2019},
isbn = {9781450368735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341301.3359640},
doi = {10.1145/3341301.3359640},
booktitle = {Proceedings of the 27th ACM Symposium on Operating Systems Principles},
pages = {554–569},
numpages = {16},
keywords = {Linux, performance evolution, operating systems},
location = {Huntsville, Ontario, Canada},
series = {SOSP '19}
}

@misc{napi,
  title={napi},
  author={{The Linux Foundation}},
  howpublished={\url{https://wiki.linuxfoundation.org/networking/napi}}
}

@inproceedings {whenpollisbetter,
title = {When Poll Is Better than Interrupt},
booktitle = {10th {USENIX} Conference on File and Storage Technologies ({FAST} 12)},
year = {2012},
address = {San Jose, CA},
url = {https://www.usenix.org/conference/fast12/when-poll-better-interrupt},
publisher = {{USENIX} Association},
month = feb,
}

@inproceedings{10.1145/2000064.2019527,
author = {Barroso, Luiz Andre},
title = {Warehouse-Scale Computing: Entering the Teenage Decade},
year = {2011},
isbn = {9781450304726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2000064.2019527},
doi = {10.1145/2000064.2019527},
booktitle = {Proceedings of the 38th Annual International Symposium on Computer Architecture},
location = {San Jose, California, USA},
series = {ISCA '11}
}

  



@inproceedings {181902,
author = {Pravin Shinde and Antoine Kaufmann and Timothy Roscoe and Stefan Kaestle},
title = {We Need to Talk About NICs},
booktitle = {Presented as part of the 14th Workshop on Hot Topics in Operating Systems},
year = {2013},
address = {Santa Ana Pueblo, NM},
url = {https://www.usenix.org/conference/hotos13/we-need-talk-about-nics},
publisher = {{USENIX}},
}

@inproceedings{10.1145/2168836.2168842,
author = {Chen, Yanpei and Alspaugh, Sara and Borthakur, Dhruba and Katz, Randy},
title = {Energy Efficiency for Large-Scale MapReduce Workloads with Significant Interactive Analysis},
year = {2012},
isbn = {9781450312233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2168836.2168842},
doi = {10.1145/2168836.2168842},
abstract = {},
booktitle = {Proceedings of the 7th ACM European Conference on Computer Systems},
pages = {43–56},
numpages = {14},
keywords = {energy efficiency, MapReduce},
location = {Bern, Switzerland},
series = {EuroSys '12}
}

@inproceedings{10.1145/2525526.2525854,
author = {Hoffmann, Henry},
title = {Racing and Pacing to Idle: An Evaluation of Heuristics for Energy-Aware Resource Allocation},
year = {2013},
isbn = {9781450324588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2525526.2525854},
doi = {10.1145/2525526.2525854},
abstract = {We examine the problem of assigning computing resources to an application to meet a performance goal while minimizing energy consumption. We present a general formulation of this problem as a linear program, discuss several potential heuristic solutions, and evaluate these heuristics on two real systems (one purchased in 2010, the other in 2013). We find that the well-known race-to-idle heuristic is close to the optimal solution on the older machine. On the newer machine, however, the optimal solution outperforms race-to-idle by over 35%. A generalization of race-to-idle, called pace-to-idle, is found to provide better results in a wider range of scenarios.},
booktitle = {Proceedings of the Workshop on Power-Aware Computing and Systems},
articleno = {13},
numpages = {5},
keywords = {resource allocation, power-aware computing, optimization, energy-aware computing},
location = {Farmington, Pennsylvania},
series = {HotPower '13}
}

@misc{mutilate,
title={{Mutilate: high performance memcached load generator}},
author = {{J. Leverich}},
 howpublished = {\url{https://github.com/leverich/mutilate}}
}

@misc{intelitr,
title={{Tuning Throughput Performance for Intel® Ethernet Adapters}},
author = {{Intel}},
 howpublished = {\url{https://www.intel.com/content/www/us/en/support/articles/000005811/network-and-i-o/ethernet-products.html}}
}

@article{WebSearch,
title	= {Web Search for a Planet: The Google Cluster Architecture},
author	= {Luiz Andre Barroso and Jeffrey Dean and Urs Hölzle},
year	= {2003},
journal	= {IEEE Micro},
pages	= {22-28},
volume	= {23}
}


@INPROCEEDINGS{plan9,
    author = {Rob Pike and Dave Presotto and Ken Thompson and Howard Trickey},
    title = {Plan 9 from Bell Labs},
    booktitle = {In Proceedings of the Summer 1990 UKUUG Conference},
    year = {1990},
    pages = {1--9}
}

@inproceedings{Cheriton1994Cache,
 author = {Cheriton, David R. and Duda, Kenneth J.},
 title = {A Caching Model of Operating System Kernel Functionality},
 booktitle = {Proceedings of the 1st USENIX Conference on Operating Systems Design and Implementation},
 series = {OSDI '94},
 year = {1994},
 location = {Monterey, California},
 articleno = {14},
 url = {http://dl.acm.org/citation.cfm?id=1267638.1267652},
 acmid = {1267652},
 publisher = {USENIX Association},
 address = {Berkeley, CA, USA},
}

@inproceedings{softlayer,
  title={{Softlayer}},
  author = {Big data solutions},
  url = {http://www.softlayer.com/big-data},
  year  = {2015}
}

@inproceedings{rackspace,
  title={{Rackspace}},
  author = {Rackspace cloud big data onmetal},
  url = {http://go.rackspace.com/baremetalbigdata/},
  year  = {2015}
}

@inproceedings{hil,
  title={{HIL: Designing an Exokernel for the Data Center}},
  author={Hennessey, Jason and Tikale, Sahil and Turk, Ata and Kaynar, Emine Ugur and Hill, Chris and Desnoyers, Peter and Krieger, Orran},
  booktitle={Proceedings of the Seventh ACM Symposium on Cloud Computing},
  pages={155--168},
  year={2016},
  organization={ACM}
}

@inproceedings{Stonebraker1,
 author = {Stonebraker, Michael and Madden, Samuel and Abadi, Daniel J. and Harizopoulos, Stavros and Hachem, Nabil and Helland, Pat},
 title = {The End of an Architectural Era: (It's Time for a Complete Rewrite)},
 booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
 series = {VLDB '07},
 year = {2007},
 isbn = {978-1-59593-649-3},
 location = {Vienna, Austria},
 pages = {1150--1160},
 numpages = {11},
 url = {http://dl.acm.org/citation.cfm?id=1325851.1325981},
 acmid = {1325981},
 publisher = {VLDB Endowment},
} 

@article{uio,
 author = {{Cheriton, David R.}},
 title = {{UIO: A Uniform I/O System Interface for Distributed Systems}},
 journal = {ACM Trans. Comput. Syst.},
 issue_date = {Feb. 1987},
 volume = {5},
 number = {1},
 month = jan,
 year = {1987},
 issn = {0734-2071},
 pages = {12--46},
 numpages = {35},
 url = {http://doi.acm.org/10.1145/7351.7353},
 doi = {10.1145/7351.7353},
 acmid = {7353},
 publisher = {ACM},
 address = {New York, NY, USA},
} 


@inproceedings {willow,
author = {Sudharsan Seshadri and Mark Gahagan and Sundaram Bhaskaran and Trevor Bunker and Arup De and Yanqin Jin and Yang Liu and Steven Swanson},
title = {Willow: A User-Programmable {SSD}},
booktitle = {11th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 14)},
year = {2014},
isbn = { 978-1-931971-16-4},
address = {Broomfield, CO},
pages = {67--80},
url = {https://www.usenix.org/conference/osdi14/technical-sessions/presentation/seshadri},
publisher = {{USENIX} Association},
}

@article{Tanenbaum1990Amoeba,
 author = {Tanenbaum, Andrew S. and van Renesse, Robbert and van Staveren, Hans and Sharp, Gregory J. and Mullender, Sape J.},
 title = {Experiences with the Amoeba Distributed Operating System},
 journal = {Commun. ACM},
 issue_date = {Dec. 1990},
 volume = {33},
 number = {12},
 month = dec,
 year = {1990},
 issn = {0001-0782},
 pages = {46--63},
 numpages = {18},
 url = {http://doi.acm.org/10.1145/96267.96281},
 doi = {10.1145/96267.96281},
 acmid = {96281},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@article{Appavoo:2007:EDO:1275517.1275518,
 author = {Appavoo, Jonathan and Silva, Dilma Da and Krieger, Orran and Auslander, Marc and Ostrowski, Michal and Rosenburg, Bryan and Waterland, Amos and Wisniewski, Robert W. and Xenidis, Jimi and Stumm, Michael and Soares, Livio},
 title = {Experience Distributing Objects in an SMMP OS},
 journal = {ACM Trans. Comput. Syst.},
 issue_date = {August 2007},
 volume = {25},
 number = {3},
 month = aug,
 year = {2007},
 issn = {0734-2071},
 articleno = {6},
 url = {http://doi.acm.org/10.1145/1275517.1275518},
 doi = {10.1145/1275517.1275518},
 acmid = {1275518},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Concurrency, Distribution, Locality, Scalability SMMP},
} 

@inproceedings{Wulf:1972:CM:1480083.1480098,
 author = {Wulf, William A. and Bell, C. G.},
 title = {C.Mmp: A Multi-mini-processor},
 booktitle = {Proceedings of the December 5-7, 1972, Fall Joint Computer Conference, Part II},
 series = {AFIPS '72 (Fall, part II)},
 year = {1972},
 location = {Anaheim, California},
 pages = {765--777},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1480083.1480098},
 doi = {10.1145/1480083.1480098},
 acmid = {1480098},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Verghese1996,
 author = {Verghese, Ben and Devine, Scott and Gupta, Anoop and Rosenblum, Mendel},
 title = {Operating System Support for Improving Data Locality on CC-NUMA Compute Servers},
 booktitle = {Proceedings of the Seventh International Conference on Architectural Support for Programming Languages and Operating Systems},
 series = {ASPLOS VII},
 year = {1996},
 isbn = {0-89791-767-7},
 location = {Cambridge, Massachusetts, USA},
 pages = {279--289},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/237090.237205},
 doi = {10.1145/237090.237205},
 acmid = {237205},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@misc{silberstein1,
title={{Accelerators in data centers: the systems perspective}},
author = {{Mark Silberstein}},
 howpublished = {\url{https://www.sigarch.org/accelerators-in-data-centers-the-\\systems-perspective/}}
}

@inproceedings{firebox,
title={{FireBox: A Hardware Building Block for 2020 Warehouse-Scale Computers}},
author={{Krste Asanović}},
publisher={{FAST 2014}}
}

@inproceedings{perez,
title={{Scalable hardware evolves, but what about network OS?}},
author={{Perez, M.}},
publisher={{PCWeek 1995}}
}

@misc{googlelinux,
title={{KS2009: How Google uses Linux}},
author = {{Mike Waychison and Jonathan Corbet}},
 howpublished = {\url{https://lwn.net/Articles/357658/}}
}

@misc{rss,
title={{Receive Side Scaling on Intel® Network Adapters}},
author = {{Intel}},
 howpublished = {\url{https://www.intel.com/content/www/us/en/support/articles/000006703/network-and-i-o/ethernet-products.html}}
}

@misc{nodejs,
  title={Node.js},
  author = {{Joyent}},
  howpublished={\url{https://github.com/nodejs/node/tree/cc56c62ed879ad4f93b1fdab3235c43e60f48b7e}},
  year={2013}
}



@misc{firmwareseagate,
title={{Maximize Security, Lock Down Hard Drive Firmware with Seagate Secure Download \& Diagnostics}},
author={{Seagate Technology LLc}},
year={2015}
}

@misc{nature1,
title={{How to stop data centres from gobbling up the world’s electricity}},
author = {{Nicola Jones}},
 howpublished = {\url{https://www.nature.com/articles/d41586-018-06610-y}}
}

@misc{mvdirona,
title={{Cost of Power in Large-Scale Data Centers}},
author = {{James Hamilton}},
 howpublished = {\url{https://perspectives.mvdirona.com/2008/11/cost-of-power-in-large-scale-data-centers/}}
}
@misc{sriov,
title={{Frequently Asked Questions for SR-IOV on Intel® Ethernet Server Adapters
}},
author = {{Intel}},
 howpublished = {\url{https://www.intel.com/content/www/us/en/support/articles/000005722/network-and-i-o/ethernet-products.html}}
}

@misc{redhatgpuvirt,
  author = {{Redhat}},
  title = {{NVidia vGPU and Red Hat Virtualization Virtual High End Workstations and Compute}},
  howpublished = {\url{http://on-demand.gputechconf.com/gtc/2017/presentation/s7812-beausoleil-nvidia-vgpu-red-hat-virtualization.pdf}},
}

@misc{samsungssd,
  author = {{Samsung}},
  title = {{Samsung SSD PM1725a}},
  howpublished = {\url{http://www.samsung.com/semiconductor/global/file/insight/2016/08/Samsung_PM1725a-1.pdf
}},
}

@misc{dpdk,
  author = {{Intel Corporation}},
  title = {{Intel DPDK: Data Plane Development Kit}},
  howpublished = {\url{http://dpdk.org}},
}

@article{RitchieUNIX,
 author = {Ritchie, Dennis M. and Thompson, Ken},
 title = {The UNIX Time-sharing System},
 journal = {Commun. ACM},
 issue_date = {July 1974},
 volume = {17},
 number = {7},
 month = jul,
 year = {1974},
 issn = {0001-0782},
 pages = {365--375},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/361011.361061},
 doi = {10.1145/361011.361061},
 acmid = {361061},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {PDP-11, command language, file system, operating system, time-sharing},
} 

@inproceedings{disklocality,
 author = {Ananthanarayanan, Ganesh and Ghodsi, Ali and Shenker, Scott and Stoica, Ion},
 title = {Disk-locality in Datacenter Computing Considered Irrelevant},
 booktitle = {Proceedings of the 13th USENIX Conference on Hot Topics in Operating Systems},
 series = {HotOS'13},
 year = {2011},
 location = {Napa, California},
 pages = {12--12},
 numpages = {1},
 url = {http://dl.acm.org/citation.cfm?id=1991596.1991613},
 acmid = {1991613},
 publisher = {USENIX Association},
 address = {Berkeley, CA, USA},
}

@article{ramcloud,
 author = {Ousterhout, John and Agrawal, Parag and Erickson, David and Kozyrakis, Christos and Leverich, Jacob and Mazi\`{e}res, David and Mitra, Subhasish and Narayanan, Aravind and Parulkar, Guru and Rosenblum, Mendel and Rumble, Stephen M. and Stratmann, Eric and Stutsman, Ryan},
 title = {The Case for RAMClouds: Scalable High-performance Storage Entirely in DRAM},
 journal = {SIGOPS Oper. Syst. Rev.},
 issue_date = {January 2010},
 volume = {43},
 number = {4},
 month = jan,
 year = {2010},
 issn = {0163-5980},
 pages = {92--105},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1713254.1713276},
 doi = {10.1145/1713254.1713276},
 acmid = {1713276},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@inproceedings{dreamweaver,
author = {Meisner, David and Wenisch, Thomas F.},
title = {DreamWeaver: Architectural Support for Deep Sleep},
year = {2012},
isbn = {9781450307598},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2150976.2151009},
doi = {10.1145/2150976.2151009},
abstract = {Numerous data center services exhibit low average utilization leading to poor energy efficiency. Although CPU voltage and frequency scaling historically has been an effective means to scale down power with utilization, transistor scaling trends are limiting its effectiveness and the CPU is accounting for a shrinking fraction of system power. Recent research advocates the use of full-system idle low-power modes to combat energy losses, as such modes provide the deepest power savings with bounded response time impact. However, the trend towards increasing cores per die is undermining the effectiveness of these sleep modes, particularly for request-parallel data center applications, because the independent idle periods across individual cores are unlikely to align by happenstance.We propose DreamWeaver, architectural support to facilitate deep sleep for request-parallel applications on multicore servers. DreamWeaver comprises two elements: Weave Scheduling, a scheduling policy to coalesce idle and busy periods across cores to create opportunities for system-wide deep sleep; and the Dream Processor, a light-weight co-processor that monitors incoming network traffic and suspended work during sleep to determine when the system must wake. DreamWeaver is based on two key concepts: (1) stall execution and sleep anytime any core is unoccupied, but (2) constrain the maximum time any request may be stalled. Unlike prior scheduling approaches, DreamWeaver will preempt execution to sleep, maximizing time spent at the systems' most efficient operating point. We demonstrate that DreamWeaver can smoothly trade-off bounded, predictable increases in 99th-percentile response time for increasing power savings, and strictly dominates the savings available with voltage and frequency scaling and timeout-based request batching schemes.},
booktitle = {Proceedings of the Seventeenth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {313–324},
numpages = {12},
keywords = {power management, servers},
location = {London, England, UK},
series = {ASPLOS XVII}
}

@INPROCEEDINGS{4228267,
  author={Freeh, Vincent W. and Bletsch, Tyler K. and Rawson, Freeman L.},
  booktitle={2007 IEEE International Parallel and Distributed Processing Symposium}, 
  title={Scaling and Packing on a Chip Multiprocessor}, 
  year={2007},
  volume={},
  number={},
  pages={1-8},
  doi={10.1109/IPDPS.2007.370539}}


@inproceedings{10.1109/MICRO.2006.8,
author = {Isci, Canturk and Buyuktosunoglu, Alper and Cher, Chen-Yong and Bose, Pradip and Martonosi, Margaret},
title = {An Analysis of Efficient Multi-Core Global Power Management Policies: Maximizing Performance for a Given Power Budget},
year = {2006},
isbn = {0769527329},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/MICRO.2006.8},
doi = {10.1109/MICRO.2006.8},
abstract = {Chip-level power and thermal implications will continue to rule as one of the primary design constraints and performance limiters. The gap between average and peak power actually widens with increased levels of core integration. As such, if per-core control of power levels (modes) is possible, a global power manager should be able to dynamically set the modes suitably. This would be done in tune with the workload characteristics, in order to always maintain a chip-level power that is below the specified budget. Furthermore, this should be possible without significant degradation of chip-level throughput performance. We analyze and validate this concept in detail in this paper. We assume a per-core DVFS (dynamic voltage and frequency scaling) knob to be available to such a conceptual global power manager. We evaluate several different policies for global multi-core power management. In this analysis, we consider various different objectives such as prioritization and optimized throughput. Overall, our results show that in the context of a workload comprised of SPEC benchmark threads, our best architected policies can come within 1% of the performance of an ideal oracle, while meeting a given chip-level power budget. Furthermore, we show that these global dynamic management policies perform significantly better than static management, even if static scheduling is given oracular knowledge.},
booktitle = {Proceedings of the 39th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {347–358},
numpages = {12},
series = {MICRO 39}
}

@inproceedings{Hendrickson2016Serverless,
 author = {Hendrickson, Scott and Sturdevant, Stephen and Harter, Tyler and Venkataramani, Venkateshwaran and Arpaci-Dusseau, Andrea C. and Arpaci-Dusseau, Remzi H.},
 title = {Serverless Computation with openLambda},
 booktitle = {Proceedings of the 8th USENIX Conference on Hot Topics in Cloud Computing},
 series = {HotCloud'16},
 year = {2016},
 location = {Denver, CO},
 pages = {33--39},
 numpages = {7},
 url = {http://dl.acm.org/citation.cfm?id=3027041.3027047},
 acmid = {3027047},
 publisher = {USENIX Association},
 address = {Berkeley, CA, USA},
}

@inproceedings{Basu2013,
 author = {Basu, Arkaprava and Gandhi, Jayneel and Chang, Jichuan and Hill, Mark D. and Swift, Michael M.},
 title = {Efficient Virtual Memory for Big Memory Servers},
 booktitle = {Proceedings of the 40th Annual International Symposium on Computer Architecture},
 series = {ISCA '13},
 year = {2013},
 isbn = {978-1-4503-2079-5},
 location = {Tel-Aviv, Israel},
 pages = {237--248},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/2485922.2485943},
 doi = {10.1145/2485922.2485943},
 acmid = {2485943},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {tanslation lookaside buffer, virtual memory},
} 

@inproceedings {ingens,
author = {Youngjin Kwon and Hangchen Yu and Simon Peter and Christopher J. Rossbach and Emmett Witchel},
title = {Coordinated and Efficient Huge Page Management with Ingens},
booktitle = {12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)},
year = {2016},
isbn = {978-1-931971-33-1},
address = {GA},
pages = {705--721},
url = {https://www.usenix.org/conference/osdi16/technical-sessions/presentation/kwon},
publisher = {USENIX Association},
}

@inproceedings{Williams2017Serverless,
	author={{Ricardo Koller and Dan Williams}},
    title = {{Will Serverless End the Dominance of Linux in the Cloud?}},
    booktitle = {{In Proceedings of ACM SIGOPS HotOS}},
    year = {2017},
    location = {Whistler, BC}
}

@inproceedings{flash,
 author = {Kuskin, J. and Ofelt, D. and Heinrich, M. and Heinlein, J. and Simoni, R. and Gharachorloo, K. and Chapin, J. and Nakahira, D. and Baxter, J. and Horowitz, M. and Gupta, A. and Rosenblum, M. and Hennessy, J.},
 title = {The Stanford FLASH Multiprocessor},
 booktitle = {Proceedings of the 21st Annual International Symposium on Computer Architecture},
 series = {ISCA '94},
 year = {1994},
 isbn = {0-8186-5510-0},
 location = {Chicago, Illinois, USA},
 pages = {302--313},
 numpages = {12},
 url = {http://dx.doi.org/10.1145/191995.192056},
 doi = {10.1145/191995.192056},
 acmid = {192056},
 publisher = {IEEE Computer Society Press},
 address = {Los Alamitos, CA, USA},
} 

@inproceedings{stingccnuma,
 author = {Lovett, Tom and Clapp, Russell},
 title = {STiNG: A CC-NUMA Computer System for the Commercial Marketplace},
 booktitle = {Proceedings of the 23rd Annual International Symposium on Computer Architecture},
 series = {ISCA '96},
 year = {1996},
 isbn = {0-89791-786-3},
 location = {Philadelphia, Pennsylvania, USA},
 pages = {308--317},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/232973.233006},
 doi = {10.1145/232973.233006},
 acmid = {233006},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings {254372,
author = {Alireza Farshin and Amir Roozbeh and Gerald Q. Maguire Jr. and Dejan Kosti{\'c}},
title = {Reexamining Direct Cache Access to Optimize I/O Intensive Applications for Multi-hundred-gigabit Networks},
booktitle = {2020 {USENIX} Annual Technical Conference ({USENIX} {ATC} 20)},
year = {2020},
isbn = {978-1-939133-14-4},
pages = {673--689},
url = {https://www.usenix.org/conference/atc20/presentation/farshin},
publisher = {{USENIX} Association},
month = jul,
}

@inproceedings{10.1109/ISCA.2005.23,
author = {Huggahalli, Ram and Iyer, Ravi and Tetrick, Scott},
title = {Direct Cache Access for High Bandwidth Network I/O},
year = {2005},
isbn = {076952270X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ISCA.2005.23},
doi = {10.1109/ISCA.2005.23},
abstract = {Recent I/O technologies such as PCI-Express and 10Gb Ethernet enable unprecedented
levels of I/O bandwidths in mainstream platforms. However, in traditional architectures,
memory latency alone can limit processors from matching 10 Gb inbound network I/O
traffic. We propose a platform-wide method called Direct Cache Access (DCA) to deliver
inbound I/O data directly into processor caches. We demonstrate that DCA provides
a significant reduction in memory latency and memory bandwidth for receive intensive
network I/O applications. Analysis of benchmarks such as SPECWeb9, TPC-W and TPC-C
shows that overall benefit depends on the relative volume of I/O to memory traffic
as well as the spatial and temporal relationship between processor and I/O memory
accesses. A system level perspective for the efficient implementation of DCA is presented.},
booktitle = {Proceedings of the 32nd Annual International Symposium on Computer Architecture},
pages = {50–59},
numpages = {10},
series = {ISCA '05}
}

  
  
@inproceedings{hpconvex,
 author = {Brewer, T. and Astfalk, G.},
 title = {The Evolution of the HP/Convex Exemplar},
 booktitle = {Proceedings of the 42Nd IEEE International Computer Conference},
 series = {COMPCON '97},
 year = {1997},
 isbn = {0-8186-7804-6},
 pages = {81--},
 url = {http://dl.acm.org/citation.cfm?id=792770.793731},
 acmid = {793731},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {2 dimensional topology, Exemplar X-Class, HP/Convex Exemplar evolution, IEEE standard SCI, PA-8000 processors, PCI busses, S-class systems, X-class systems, cache based semaphores, cache coherent nonuniform memory access architecture, ccNUMA, first generation systems, hardware bcopy engine, high performance engineering/scientific computations, interconnect caches, multiple nodes, nonblocking crossbar, parallel architectures, second generation SPP, symmetric multiprocessor},
} 

@inproceedings{sgiorigin,
 author = {Laudon, James and Lenoski, Daniel},
 title = {The SGI Origin: A ccNUMA Highly Scalable Server},
 booktitle = {Proceedings of the 24th Annual International Symposium on Computer Architecture},
 series = {ISCA '97},
 year = {1997},
 isbn = {0-89791-901-7},
 location = {Denver, Colorado, USA},
 pages = {241--251},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/264107.264206},
 doi = {10.1145/264107.264206},
 acmid = {264206},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{dougliscomparison,
  title={{A Comparison of Two Distributed Systems: Amoeba and Sprite}},
  author = {{Fred Douglis, John K. Ousterhout, M. Frans Kaashoek, Andrew S. Tanenbaum}},
  journal = {Computing Systems},
    year = {1991},
    volume = {4}
}

@article{Cheriton:1991:paradigm,
 author = {Cheriton, David R. and Goosen, Hendrik A. and Boyle, Patrick D.},
 title = {Paradigm: A Highly Scalable Shared-Memory Multicomputer Architecture},
 journal = {Computer},
 issue_date = {February 1991},
 volume = {24},
 number = {2},
 month = feb,
 year = {1991},
 issn = {0018-9162},
 pages = {33--46},
 numpages = {14},
 url = {http://dx.doi.org/10.1109/2.67209},
 doi = {10.1109/2.67209},
 acmid = {103296},
 publisher = {IEEE Computer Society Press},
 address = {Los Alamitos, CA, USA},
} 

@inproceedings{Gamsa:1994:ppc,
 author = {Gamsa, Benjamin and Krieger, Orran and Stumm, Michael},
 title = {Optimizing IPC Performance for Shared-Memory Multiprocessors},
 booktitle = {Proceedings of the 1994 International Conference on Parallel Processing - Volume 01},
 series = {ICPP '94},
 year = {1994},
 isbn = {0-8493-2493-9},
 pages = {208--211},
 numpages = {4},
 url = {http://dx.doi.org/10.1109/ICPP.1994.144},
 doi = {10.1109/ICPP.1994.144},
 acmid = {1261031},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{numachine,
 author = {Grindley, R. and Abdelrahman, T. and Brown, S. and Caranci, S. and DeVries, D. and Gamsa, B. and Grbic, A. and Gusat, M. and Ho, R. and Krieger, O. and Lemieux, G. and Loveless, K. and Manjikian, N. and McHardy, P. and Srbljic, S. and Stumm, M. and Vranesic, Z. and Zilic, Z.},
 title = {The NUMAchine Multiprocessor},
 booktitle = {Proceedings of the Proceedings of the 2000 International Conference on Parallel Processing},
 series = {ICPP '00},
 year = {2000},
 isbn = {0-7695-0768-9},
 pages = {487--},
 url = {http://dl.acm.org/citation.cfm?id=850941.852940},
 acmid = {852940},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{lowlatency,
 author = {Rumble, Stephen M. and Ongaro, Diego and Stutsman, Ryan and Rosenblum, Mendel and Ousterhout, John K.},
 title = {It's Time for Low Latency},
 booktitle = {Proceedings of the 13th USENIX Conference on Hot Topics in Operating Systems},
 series = {HotOS'13},
 year = {2011},
 location = {Napa, California},
 pages = {11--11},
 numpages = {1},
 url = {http://dl.acm.org/citation.cfm?id=1991596.1991611},
 acmid = {1991611},
 publisher = {USENIX Association},
 address = {Berkeley, CA, USA},
}

@article{Hill:1986:SPUR,
 author = {Hill, Mark and Eggers, Susan and Larus, Jim and Taylor, George and Adams, Glenn and Bose, B. K. and Gibson, Garth and Hansen, Paul and Keller, Jon and Kong, Shing and Lee, Corinna and Lee, Daebum and Pendleton, Joan and Ritchie, Scott and Wood, David A. and Zorn, Ben and Hilfinger, Paul and Hodges, Dave and Katz, Randy and Ousterhout, John and Patterson, Dave},
 title = {Design Decisions in SPUR},
 journal = {Computer},
 issue_date = {November 1986},
 volume = {19},
 number = {11},
 month = nov,
 year = {1986},
 issn = {0018-9162},
 pages = {8--22},
 numpages = {15},
 url = {http://dx.doi.org/10.1109/MC.1986.1663096},
 doi = {10.1109/MC.1986.1663096},
 acmid = {9486},
 publisher = {IEEE Computer Society Press},
 address = {Los Alamitos, CA, USA},
} 

@inproceedings{Cheriton:1986:VMP,
 author = {Cheriton, D. R. and Slavenburg, G. A. and Boyle, P. D.},
 title = {Software-controlled Caches in the VMP Multiprocessor},
 booktitle = {Proceedings of the 13th Annual International Symposium on Computer Architecture},
 series = {ISCA '86},
 year = {1986},
 isbn = {0-8186-0719-X},
 location = {Tokyo, Japan},
 pages = {366--374},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=17407.17399},
 acmid = {17399},
 publisher = {IEEE Computer Society Press},
 address = {Los Alamitos, CA, USA},
} 

@inproceedings{nicintegration,
 author = {Lim, Kevin and Chang, Jichuan and Mudge, Trevor and Ranganathan, Parthasarathy and Reinhardt, Steven K. and Wenisch, Thomas F.},
 title = {Disaggregated Memory for Expansion and Sharing in Blade Servers},
 booktitle = {Proceedings of the 36th Annual International Symposium on Computer Architecture},
 series = {ISCA '09},
 year = {2009},
 isbn = {978-1-60558-526-0},
 location = {Austin, TX, USA},
 pages = {267--278},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1555754.1555789},
 doi = {10.1145/1555754.1555789},
 acmid = {1555789},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {disaggregated memory, memory blades, memory capacity expansion, power and cost efficiencies},
}

@misc{applemach,
  author = {{Apple Inc.}},
  title = {{Mach Overview}},
  howpublished = {\url{https://developer.apple.com/library/content/documentation/Darwin/Conceptual/KernelProgramming/Mach/Mach.html}},
}

@misc{facebookdis,
author = {{Facebook}},
title = {{Disaggregate: Networking recap}},
howpublished = {\url{https://code.facebook.com/posts/1887543398133443/disaggregate-networking-recap/}}
}

@misc{hpmachine,
author = {{Hewlett Packard}},
howpublished = {\url{https://www.labs.hpe.com/the-machine}}
}

@misc{grid,
author = {{NVIDIA}},
howpublished = {\url{http://www.nvidia.com/object/grid-technology.html}}
}

@misc{intelrackscale,
author={{Intel}},
howpublished = {\url{https://www.intel.com/content/www/us/en/architecture-and-technology/rack-scale-design-overview.html}}
}

@inproceedings{msrfpga,
 author = {Putnam, Andrew and Caulfield, Adrian M. and Chung, Eric S. and Chiou, Derek and Constantinides, Kypros and Demme, John and Esmaeilzadeh, Hadi and Fowers, Jeremy and Gopal, Gopi Prashanth and Gray, Jan and Haselman, Michael and Hauck, Scott and Heil, Stephen and Hormati, Amir and Kim, Joo-Young and Lanka, Sitaram and Larus, James and Peterson, Eric and Pope, Simon and Smith, Aaron and Thong, Jason and Xiao, Phillip Yi and Burger, Doug},
 title = {A Reconfigurable Fabric for Accelerating Large-scale Datacenter Services},
 booktitle = {Proceeding of the 41st Annual International Symposium on Computer Architecuture},
 series = {ISCA '14},
 year = {2014},
 isbn = {978-1-4799-4394-4},
 location = {Minneapolis, Minnesota, USA},
 pages = {13--24},
 numpages = {12},
 url = {http://dl.acm.org/citation.cfm?id=2665671.2665678},
 acmid = {2665678},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA},
} 

@inproceedings{biscuit,
 author = {Gu, Boncheol and Yoon, Andre S. and Bae, Duck-Ho and Jo, Insoon and Lee, Jinyoung and Yoon, Jonghyun and Kang, Jeong-Uk and Kwon, Moonsang and Yoon, Chanho and Cho, Sangyeun and Jeong, Jaeheon and Chang, Duckhyun},
 title = {Biscuit: A Framework for Near-data Processing of Big Data Workloads},
 booktitle = {Proceedings of the 43rd International Symposium on Computer Architecture},
 series = {ISCA '16},
 year = {2016},
 isbn = {978-1-4673-8947-1},
 location = {Seoul, Republic of Korea},
 pages = {153--165},
 numpages = {13},
 url = {https://doi.org/10.1109/ISCA.2016.23},
 doi = {10.1109/ISCA.2016.23},
 acmid = {3001154},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA},
 keywords = {SSD, in-storage computing, near-data processing},
} 

@inproceedings{gpufs,
 author = {Silberstein, Mark and Ford, Bryan and Keidar, Idit and Witchel, Emmett},
 title = {GPUfs: Integrating a File System with GPUs},
 booktitle = {Proceedings of the Eighteenth International Conference on Architectural Support for Programming Languages and Operating Systems},
 series = {ASPLOS '13},
 year = {2013},
 isbn = {978-1-4503-1870-9},
 location = {Houston, Texas, USA},
 pages = {485--498},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/2451116.2451169},
 doi = {10.1145/2451116.2451169},
 acmid = {2451169},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {accelerators, file systems, gpgpus, operating systems design},
} 

@inproceedings{exokernel,
 author = {Engler, D. R. and Kaashoek, M. F. and O'Toole,Jr., J.},
 title = {{Exokernel: An Operating System Architecture for Application-level Resource Management}},
 booktitle = {Proceedings of the Fifteenth ACM Symposium on Operating Systems Principles},
 series = {SOSP '95},
 year = {1995},
 isbn = {0-89791-715-4},
 location = {Copper Mountain, Colorado, USA},
 pages = {251--266},
 numpages = {16},
 url = {http://doi.acm.org/10.1145/224056.224076},
 doi = {10.1145/224056.224076},
 acmid = {224076},
 publisher = {ACM}
} 

@misc{mcd,
   author={{https://memcached.org}},
   title={{Memcached}},
   howpublished={\url{https://github.com/memcached/memcached}},
   year={2020}
}

@misc{wrk,
   author={Glozer, Will},
   title={{wrk: Modern HTTP benchmarking tool}},
   howpublished={\url{https://github.com/wg/wrk}},
   year={2014}
}

@inproceedings{silo,
author = {Tu, Stephen and Zheng, Wenting and Kohler, Eddie and Liskov, Barbara and Madden, Samuel},
title = {Speedy Transactions in Multicore In-Memory Databases},
year = {2013},
isbn = {9781450323888},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2517349.2522713},
doi = {10.1145/2517349.2522713},
booktitle = {Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles},
pages = {18–32},
numpages = {15},
location = {Farminton, Pennsylvania},
series = {SOSP ’13}
}

@misc{bwgrowth,
author = {{John D'Ambrosia and Scott Kipp}},
title = {{Bandwidth Growth and the Next Speed of Ethernet}},
howpublished = {\url{https://www.nanog.org/meetings/nanog56/presentations/Tuesday/tues.general.kipp.23.pdf}}
}

@misc{amazonec2,
author={{Amazon}},
howpublished = {\url{https://aws.amazon.com/ec2/}},
}

@inproceedings{googleborg,
title = {Large-scale cluster management at {Google} with {Borg}},
author  = {Abhishek Verma and Luis Pedrosa and Madhukar R. Korupolu and David Oppenheimer and Eric Tune and John Wilkes},
year  = 2015,
booktitle = {Proceedings of the European Conference on Computer Systems (EuroSys)},
address = {Bordeaux, France}
}

@misc{microsoftazure,
author={{Microsoft}},
howpublished = {\url{https://azure.microsoft.com}}
}

@misc{fpga,
author={{Field Programmable Gate Arrays \(FPGAs\)}},
howpublished = {\url{https://www.xilinx.com/products/silicon-devices/fpga/what-is-an-fpga.html}},
}

@misc{infiniband,
author={{InfiniBand Cards}},
howpublished = {\url{http://www.mellanox.com/page/infiniband_cards_overview}},
}

@misc{gpu,
author={{Graphics Processing Units}},
howpublished = {\url{http://www.nvidia.com/object/what-is-gpu-computing.html}},
}

@misc{nics,
author={{Intel Ethernet Controllers}},
howpublished = {\url{https://www.intel.com/content/www/us/en/products/network-io/ethernet/controllers.html}},
}

@misc{xeonphi,
author={{INTEL® XEON PHI™ PROCESSORS}},
howpublished = {\url{https://www.intel.com/content/www/us/en/products/processors/xeon-phi/xeon-phi-processors.html}},
}

@misc{intel_ixgbe ,
author={{Intel}},
title={{Linux ixgbe* Base Driver Overview and Installation}},
howpublished = {\url{https://www.intel.com/content/www/us/en/support/articles/000005688/network-and-i-o/ethernet-products.html}},
}


@misc{82599,
author={{Intel 82599 10 Gigabit Ethernet Controller: Datasheet}},
howpublished = {\url{https://www.intel.com/content/www/us/en/embedded/products/networking/82599-10-gbe-controller-datasheet.html}},
}

%https://www.intel.com/content/www/us/en/embedded/products/networking/82599-10-gbe-controller-datasheet.html

@misc{rumpkernel,
author={{Antti Kantee, Justin Cormack}},
title = {{Rump Kernels: No OS? No Problem!}},
howpublished = {\url{https://www.usenix.org/publications/login/october-2014-vol-39-no-5}},
}

@misc{packagecloud,
title={{Monitoring and Tuning the Linux Networking Stack: Receiving Data}},
howpublished = {\url{https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data}},
}

@misc{intelinterruptmoderation,
title={{Improve Network Performance By Setting Per-queue Interrupt Moderation In Linux}},
author={{Kan Liang, Andi Kleen, and Jesse Brandenburg}},
howpublished = {\url{https://01.org/linux-interrupt-moderation}},
}

@misc{ibmethtool,
title={{Using ethtool to change interrupt rates}},
howpublished = {\url{https://www.ibm.com/support/knowledgecenter/en/SSQPD3_2.6.0/com.ibm.wllm.doc/usingethtoolrates.html}},
}

@misc{cloudflareethtool,
title={{How to achieve low latency with 10Gbps Ethernet}},
author={{Marek Majkowski}},
howpublished = {\url{https://blog.cloudflare.com/how-to-achieve-low-latency/
}},
}

@misc{linuxethtool,
title={{Scaling in the Linux Networking Stack}},
howpublished = {\url{https://www.kernel.org/doc/Documentation/networking/scaling.txt}},
}

@misc{mellanoxethtool,
title={{Performance Tuning Guidelines for Mellanox Network Adapters}},
howpublished = {\url{https://www.mellanox.com/related-docs/prod_software/Performance_Tuning_Guide_for_Mellanox_Network_Adapters_Archive.pdf}},
}

@misc{suseethtool,
title={{Performance Analysis, Tuning and Tools on SUSE Linux Enterprise Products}},
howpublished = {\url{https://www.suse.com/documentation/suse-best-practices/singlehtml/sbp-performance-tuning/sbp-performance-tuning.html#sec.nic_settings}},
}

@misc{intelethtool,
title={{Improving Measured Latency in Linux for Intel® 82575/82576 or X540/82598/82599 Ethernet Controllers}},
howpublished = {\url{https://www.intel.com/content/www/us/en/embedded/products/networking/82575-82576-82598-82599-ethernet\\-controllers-latency-appl-note.html}},
}

@misc{vmwareethtool,
title={{Best Practices for Performance Tuning of Latency-Sensitive Workloads in vSphere VMs}},
howpublished = {\url{https://www.vmware.com/content/dam/digitalmarketing/vmware/en/pdf/techpaper/vmw-tuning-latency-sensitive-workloads-white-paper.pdf}},
}

@inproceedings{svefacebook,
 author = {{Huang, Qi and Ang, Petchean and Knowles, Peter and Nykiel, Tomasz and Tverdokhlib, Iaroslav and Yajurvedi, Amit and Dapolito,IV, Paul and Yan, Xifan and Bykov, Maxim and Liang, Chuen and Talwar, Mohit and Mathur, Abhishek and Kulkarni, Sachin and Burke, Matthew and Lloyd, Wyatt}},
 title = {{SVE: Distributed Video Processing at Facebook Scale}},
 booktitle = {{Proceedings of the 26th Symposium on Operating Systems Principles}},
 series = {SOSP '17},
 year = {2017},
 isbn = {978-1-4503-5085-3},
 location = {Shanghai, China},
 pages = {87--103},
 numpages = {17},
 url = {http://doi.acm.org/10.1145/3132747.3132775},
 doi = {10.1145/3132747.3132775},
 acmid = {3132775},
 publisher = {ACM},
 address = {New York, NY, USA},
} 


@inproceedings {258935,
author = {Juncheng Yang and Yao Yue and K. V. Rashmi},
title = {A large scale analysis of hundreds of in-memory cache clusters at Twitter},
booktitle = {14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20)},
year = {2020},
isbn = {978-1-939133-19-9},
pages = {191--208},
url = {https://www.usenix.org/conference/osdi20/presentation/yang},
publisher = {USENIX Association},
month = nov,
}

@inproceedings{workloadanalysisfacebook,
 author = {{Atikoglu, Berk and Xu, Yuehai and Frachtenberg, Eitan and Jiang, Song and Paleczny, Mike}},
 title = {{Workload Analysis of a Large-scale Key-value Store}},
 booktitle = {{Proceedings of the 12th ACM SIGMETRICS/PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems}},
 series = {SIGMETRICS '12},
 year = {2012},
 isbn = {978-1-4503-1097-0},
 location = {London, England, UK},
 pages = {53--64},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/2254756.2254766},
 doi = {10.1145/2254756.2254766},
 acmid = {2254766},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {key-value store, memcached, workload analysis, workload modeling},
} 

@inproceedings{profilewhscomputer,
 author = {{Kanev, Svilen and Darago, Juan Pablo and Hazelwood, Kim and Ranganathan, Parthasarathy and Moseley, Tipp and Wei, Gu-Yeon and Brooks, David}},
 title = {{Profiling a Warehouse-scale Computer}},
 booktitle = {{Proceedings of the 42Nd Annual International Symposium on Computer Architecture}},
 series = {ISCA '15},
 year = {2015},
 isbn = {978-1-4503-3402-0},
 location = {Portland, Oregon},
 pages = {158--169},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/2749469.2750392},
 doi = {10.1145/2749469.2750392},
 acmid = {2750392},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@misc{gupta2020chasing,
      title={Chasing Carbon: The Elusive Environmental Footprint of Computing}, 
      author={Udit Gupta and Young Geun Kim and Sylvia Lee and Jordan Tse and Hsien-Hsin S. Lee and Gu-Yeon Wei and David Brooks and Carole-Jean Wu},
      year={2020},
      eprint={2011.02839},
      archivePrefix={arXiv},
      primaryClass={cs.AR}
}

@article{10.1109/40.888701,
    author = {Brooks, David M. and Bose, Pradip and Schuster, Stanley E. and Jacobson, Hans and Kudva, Prabhakar N. and Buyuktosunoglu, Alper and Wellman, John-David and Zyuban, Victor and Gupta, Manish and Cook, Peter W.},
    title = {Power-Aware Microarchitecture: Design and Modeling Challenges for Next-Generation Microprocessors},
    year = {2000},
    issue_date = {November 2000},
    publisher = {IEEE Computer Society Press},
    address = {Washington, DC, USA},
    volume = {20},
    number = {6},
    issn = {0272-1732},
    url = {https://doi.org/10.1109/40.888701},
    doi = {10.1109/40.888701},
    abstract = {Power dissipation limits have emerged as a major constraint in the design of microprocessors. This is true not only at the low end, where cost and battery life are the primary drivers, but also now at the midrange and high-end system (server) level. Thus, the ability to estimate power consumption at the high level, during the early-stage definition and trade-off studies is a key new methodology enhancement sought by design and performance architects. We first review the fundamentals in terms of power estimation and power-performance trade-offs at the microarchitecture level. We then discuss the opportunities of saving power that can be exposed via microarchitecture-level modeling. In particular, the potential savings that can be effected through straightforward clock-gating techniques is cited as an example. We also describe some future ideas and trends in power-efficient processor design. Examples of how microarchitectural observations can be used toward power-saving circuit design optimizations are described. The design and modeling challenges are in the context of work in progress within IBM Research. This research is in support of future, high-end processor development within IBM.},
    journal = {IEEE Micro},
    month = nov,
    pages = {26–44},
    numpages = {19}
}

      



@inproceedings {farm,
author = {Aleksandar Dragojevi{\'c} and Dushyanth Narayanan and Miguel Castro and Orion Hodson},
title = {FaRM: Fast Remote Memory},
booktitle = {11th USENIX Symposium on Networked Systems Design and Implementation (NSDI 14)},
year = {2014},
isbn = {978-1-931971-09-6},
address = {Seattle, WA},
pages = {401--414},
url = {https://www.usenix.org/conference/nsdi14/technical-sessions/dragojevi{\'c}},
publisher = {USENIX Association},
}

@article{linuxgrowth,
 author = {Israeli, Ayelet and Feitelson, Dror G.},
 title = {The Linux Kernel As a Case Study in Software Evolution},
 journal = {J. Syst. Softw.},
 issue_date = {March, 2010},
 volume = {83},
 number = {3},
 month = mar,
 year = {2010},
 issn = {0164-1212},
 pages = {485--501},
 numpages = {17},
 url = {http://dx.doi.org/10.1016/j.jss.2009.09.042},
 doi = {10.1016/j.jss.2009.09.042},
 acmid = {1739409},
 publisher = {Elsevier Science Inc.},
 address = {New York, NY, USA},
 keywords = {Lehman's laws, Linux kernel, Software evolution},
}

@misc{nvram,
	author ={{Intel}},
	title={{Intel® Optane™ Technology}},
	howpublished = {\url{https://www.intel.com/content/www/us/en/architecture-and-technology/intel-optane-technology.html}}
}

@inproceedings{k42,
 author = {Krieger, Orran and Auslander, Marc and Rosenburg, Bryan and Wisniewski, Robert W. and Xenidis, Jimi and Da Silva, Dilma and Ostrowski, Michal and Appavoo, Jonathan and Butrico, Maria and Mergen, Mark and Waterland, Amos and Uhlig, Volkmar},
 title = {{K42: Building a Complete Operating System}},
 booktitle = {Proceedings of the 1st ACM SIGOPS/EuroSys European Conference on Computer Systems 2006},
 series = {EuroSys '06},
 year = {2006},
 isbn = {1-59593-322-0},
 pages = {133--145},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1217935.1217949},
 doi = {10.1145/1217935.1217949},
 acmid = {1217949},
 publisher = {ACM},
 keywords = {customizable operating systems, operating system design, scalable operating systems},
} 

@article{Wisniewski:2008:KLO:1341312.1341316,
 author = {Wisniewski, Robert W. and da Silva, Dilma and Auslander, Marc and Krieger, Orran and Ostrowski, Michal and Rosenburg, Bryan},
 title = {K42: Lessons for the OS Community},
 journal = {SIGOPS Oper. Syst. Rev.},
 issue_date = {January 2008},
 volume = {42},
 number = {1},
 month = jan,
 year = {2008},
 issn = {0163-5980},
 pages = {5--12},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1341312.1341316},
 doi = {10.1145/1341312.1341316},
 acmid = {1341316},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{pike2000systems,
  title={Systems software research is irrelevant},
  author={Pike, Rob},
  year={2000},
  owpublished = {\url{http://herpolhode.com/rob/utah2000.pdf}},
}

@inproceedings{Ghemawat:2003:GFS:945445.945450,
 author = {Ghemawat, Sanjay and Gobioff, Howard and Leung, Shun-Tak},
 title = {The Google File System},
 booktitle = {Proceedings of the Nineteenth ACM Symposium on Operating Systems Principles},
 series = {SOSP '03},
 year = {2003},
 isbn = {1-58113-757-5},
 location = {Bolton Landing, NY, USA},
 pages = {29--43},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/945445.945450},
 doi = {10.1145/945445.945450},
 acmid = {945450},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {clustered storage, data storage, fault tolerance, scalability},
} 

@inproceedings{Hindman:2011:MPF:1972457.1972488,
 author = {Hindman, Benjamin and Konwinski, Andy and Zaharia, Matei and Ghodsi, Ali and Joseph, Anthony D. and Katz, Randy and Shenker, Scott and Stoica, Ion},
 title = {Mesos: A Platform for Fine-grained Resource Sharing in the Data Center},
 booktitle = {Proceedings of the 8th USENIX Conference on Networked Systems Design and Implementation},
 series = {NSDI'11},
 year = {2011},
 location = {Boston, MA},
 pages = {295--308},
 numpages = {14},
 url = {http://dl.acm.org/citation.cfm?id=1972457.1972488},
 acmid = {1972488},
 publisher = {USENIX Association},
 address = {Berkeley, CA, USA},
}

@incollection{Wulf:2000:reflection,
 author = {Wulf, William A. and Harbison, Samuel P.},
 chapter = {Reflections in a Pool of Processors\&Mdash;an Experience Report on C.Mmp/Hydra},
 title = {Readings in Computer Architecture},
 editor = {Hill, Mark D. and Jouppi, Norman P. and Sohi, Gurindar S.},
 year = {2000},
 isbn = {1-55860-539-8},
 pages = {561--573},
 numpages = {13},
 url = {http://dl.acm.org/citation.cfm?id=333067.333232},
 acmid = {333232},
 publisher = {Morgan Kaufmann Publishers Inc.},
 address = {San Francisco, CA, USA},
} 

@inproceedings{Schwarzkopf:2013:NWO:2500727.2500739,
 author = {Schwarzkopf, Malte and Grosvenor, Matthew P. and Hand, Steven},
 title = {New Wine in Old Skins: The Case for Distributed Operating Systems in the Data Center},
 booktitle = {Proceedings of the 4th Asia-Pacific Workshop on Systems},
 series = {APSys '13},
 year = {2013},
 isbn = {978-1-4503-2316-1},
 location = {Singapore, Singapore},
 pages = {9:1--9:7},
 articleno = {9},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/2500727.2500739},
 doi = {10.1145/2500727.2500739},
 acmid = {2500739},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Vasilakis:2015:LDG:2831090.2831105,
 author = {Vasilakis, Nikos and Karel, Ben and Smith, Jonathan M.},
 title = {From Lone Dwarfs to Giant Superclusters: Rethinking Operating System Abstractions for the Cloud},
 booktitle = {Proceedings of the 15th USENIX Conference on Hot Topics in Operating Systems},
 series = {HOTOS'15},
 year = {2015},
 location = {Switzerland},
 pages = {15--15},
 numpages = {1},
 url = {http://dl.acm.org/citation.cfm?id=2831090.2831105},
 acmid = {2831105},
 publisher = {USENIX Association},
 address = {Berkeley, CA, USA},
}

@book{Barroso:2009:DCI:1643608,
 author = {Barroso, Luiz Andre and Hoelzle, Urs},
 title = {The Datacenter As a Computer: An Introduction to the Design of Warehouse-Scale Machines},
 year = {2009},
 isbn = {159829556X, 9781598295566},
 edition = {1st},
 publisher = {Morgan and Claypool Publishers},
}

@inproceedings{zk,
 author = {Hunt, Patrick and Konar, Mahadev and Junqueira, Flavio P. and Reed, Benjamin},
 title = {{ZooKeeper: Wait-free Coordination for Internet-scale Systems}},
 booktitle = {Proceedings of the 2010 USENIX Conference on USENIX Annual Technical Conference},
 series = {USENIXATC'10},
 year = {2010},
 location = {Boston, MA},
 pages = {11--11},
 numpages = {1},
 url = {http://dl.acm.org/citation.cfm?id=1855840.1855851},
 acmid = {1855851},
 publisher = {USENIX Association},
 address = {Berkeley, CA, USA},
} 

@inproceedings{Adams:2006:CSH:1168857.1168860,
 author = {Adams, Keith and Agesen, Ole},
 title = {A Comparison of Software and Hardware Techniques for x86 Virtualization},
 booktitle = {Proceedings of the 12th International Conference on Architectural Support for Programming Languages and Operating Systems},
 series = {ASPLOS XII},
 year = {2006},
 isbn = {1-59593-451-0},
 location = {San Jose, California, USA},
 pages = {2--13},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1168857.1168860},
 doi = {10.1145/1168857.1168860},
 acmid = {1168860},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {MMU, SVM, TLB, VT, dynamic binary translation, nested paging, virtual machine monitor, virtualization, x86},
}

@INPROCEEDINGS{1598114,
  author={Li, J. and Martinez, J.F.},
  booktitle={The Twelfth International Symposium on High-Performance Computer Architecture, 2006.}, 
  title={Dynamic power-performance adaptation of parallel computation on chip multiprocessors}, 
  year={2006},
  volume={},
  number={},
  pages={77-87},
  doi={10.1109/HPCA.2006.1598114}}

@inproceedings{10.5555/2523721.2523732,
author = {Sasaki, Hiroshi and Imamura, Satoshi and Inoue, Koji},
title = {Coordinated Power-Performance Optimization in Manycores},
year = {2013},
isbn = {9781479910212},
publisher = {IEEE Press},
abstract = {Optimizing the performance in multiprogrammed environments, especially for workloads composed of multithreaded programs is a desired feature of runtime management system in future manycore processors. At the same time, power capping capability is required in order to improve the reliability of microprocessor chips while reducing the costs of power supply and thermal budgeting. This paper presents a sophisticated runtime coordinated power-performance management system called C-3PO, which optimizes the performance of manycore processors under a power constraint by controlling two software knobs: thread packing, and dynamic voltage and frequency scaling~(DVFS). The proposed solution distributes the power budget to each program by controlling the workload threads to be executed with appropriate number of cores and operating frequency. The power budget is distributed carefully in different forms (number of allocated cores or operating frequency) depending on the power-performance characteristics of the workload so that each program can effectively convert the power into performance. The proposed system is based on a heuristic algorithm which relies on runtime prediction of power and performance via hardware performance monitoring units. Empirical results on a 64-core platform show that C-3PO well outperforms traditional counterparts across various PARSEC workload mixes.},
booktitle = {Proceedings of the 22nd International Conference on Parallel Architectures and Compilation Techniques},
pages = {51–62},
numpages = {12},
keywords = {thread packing, power-performance optimization, manycore processor, DVFs, power budget allocation, scalability, runtime system},
location = {Edinburgh, Scotland, UK},
series = {PACT '13}
}

@inproceedings{dynsleep,
author = {Chou, Chih-Hsun and Wong, Daniel and Bhuyan, Laxmi N.},
title = {DynSleep: Fine-Grained Power Management for a Latency-Critical Data Center Application},
year = {2016},
isbn = {9781450341851},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934583.2934616},
doi = {10.1145/2934583.2934616},
abstract = {Servers running in datacenters are commonly kept underutilized to meet stringent latency targets. Due to poor energy-proportionality in commodity servers, the low utilization results in wasteful power consumption that cost millions of dollars. Applying dynamic power management on datacenter workloads is challenging, especially when tail latency requirements often fall in the sub-millisecond level. The fundamental issue is randomness due to unpredictable request arrival times and request service times. Prior techniques applied per-core DVFS to have fine-grain control of slowing down request processing without violating the tail latency target. However, most commodity servers only support per-core DFS, which greatly limits potential energy saving. In this paper, we propose DynSleep, a fine-grain power management scheme for datacenter workloads through the use of per-core sleep states (C-states). DynSleep dynamically postpones the processing of some requests, creating longer idle periods, which allow the use of deeper C-states to save energy. We design and implement DynSleep with Mem-cached, a popular key-value store application used in datacenters. The experimental results show that DynSleep achieves up to 65\% core power saving, and 27\% better than the per-core DVFS power management scheme, while still satisfying the tail latency constraint. To the best of our knowledge, this is the first work to analyze and develop power management technique with CPU C-states in latency-critical datacenter workloads},
booktitle = {Proceedings of the 2016 International Symposium on Low Power Electronics and Design},
pages = {212–217},
numpages = {6},
keywords = {Power management, quality of service, sleep states, datacenter workload, memcached application, tail latency},
location = {San Francisco Airport, CA, USA},
series = {ISLPED '16}
}

@inproceedings{Barham:2003:XAV:945445.945462,
 author = {Barham, Paul and Dragovic, Boris and Fraser, Keir and Hand, Steven and Harris, Tim and Ho, Alex and Neugebauer, Rolf and Pratt, Ian and Warfield, Andrew},
 title = {Xen and the Art of Virtualization},
 booktitle = {Proceedings of the Nineteenth ACM Symposium on Operating Systems Principles},
 series = {SOSP '03},
 year = {2003},
 isbn = {1-58113-757-5},
 location = {Bolton Landing, NY, USA},
 pages = {164--177},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/945445.945462},
 doi = {10.1145/945445.945462},
 acmid = {945462},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {hypervisors, paravirtualization, virtual machine monitors},
} 

@inproceedings{Zaharia:2010:SCC:1863103.1863113,
 author = {Zaharia, Matei and Chowdhury, Mosharaf and Franklin, Michael J. and Shenker, Scott and Stoica, Ion},
 title = {Spark: Cluster Computing with Working Sets},
 booktitle = {Proceedings of the 2Nd USENIX Conference on Hot Topics in Cloud Computing},
 series = {HotCloud'10},
 year = {2010},
 location = {Boston, MA},
 pages = {10--10},
 numpages = {1},
 url = {http://dl.acm.org/citation.cfm?id=1863103.1863113},
 acmid = {1863113},
 publisher = {USENIX Association},
 address = {Berkeley, CA, USA},
}

@ARTICLE{sos,
    author = {Marc Shapiro and Yvon Gourhant and Sabine Habert and Laurence Mosseri and Michel Ruffin and Céline Valot},
    title = {SOS: An Object-Oriented Operating System - Assessment and Perspectives},
    journal = {Computing Systems},
    year = {1991},
    volume = {2},
    pages = {287--337}
}

@inproceedings {arachne,
author = {Henry Qin and Qian Li and Jacqueline Speiser and Peter Kraft and John Ousterhout},
title = {Arachne: Core-Aware Thread Management},
booktitle = {13th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 18)},
year = {2018},
isbn = {978-1-939133-08-3},
address = {Carlsbad, CA},
pages = {145--160},
url = {https://www.usenix.org/conference/osdi18/presentation/qin},
publisher = {{USENIX} Association},
month = oct,
}

@article{Armbrust:2010:VCC:1721654.1721672,
 author = {Armbrust, Michael and Fox, Armando and Griffith, Rean and Joseph, Anthony D. and Katz, Randy and Konwinski, Andy and Lee, Gunho and Patterson, David and Rabkin, Ariel and Stoica, Ion and Zaharia, Matei},
 title = {A View of Cloud Computing},
 journal = {Commun. ACM},
 issue_date = {April 2010},
 volume = {53},
 number = {4},
 month = apr,
 year = {2010},
 issn = {0001-0782},
 pages = {50--58},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1721654.1721672},
 doi = {10.1145/1721654.1721672},
 acmid = {1721672},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@article{memcached,
 author = {Fitzpatrick, Brad},
 title = {{Distributed Caching with Memcached}},
 journal = {Linux Journal},
 issue_date = {August 2004},
 volume = {2004},
 number = {124},
 month = aug,
 year = {2004},
 issn = {1075-3583},
 pages = {5},
 url = {http://dl.acm.org/citation.cfm?id=1012889.1012894},
 acmid = {1012894},
 publisher = {Belltown Media}
} 

@inproceedings{Gamsa1999Tornado,
 author = {Gamsa, Ben and Krieger, Orran and Appavoo, Jonathan and Stumm, Michael},
 title = {Tornado: Maximizing Locality and Concurrency in a Shared Memory Multiprocessor Operating System},
 booktitle = {Proceedings of the Third Symposium on Operating Systems Design and Implementation},
 series = {OSDI '99},
 year = {1999},
 isbn = {1-880446-39-1},
 location = {New Orleans, Louisiana, USA},
 pages = {87--100},
 numpages = {14},
 url = {http://dl.acm.org/citation.cfm?id=296806.296814},
 acmid = {296814},
 publisher = {USENIX Association},
 address = {Berkeley, CA, USA},
}

@ARTICLE{5621843, 
author={J. Howard and S. Dighe and S. R. Vangal and G. Ruhl and N. Borkar and S. Jain and V. Erraguntla and M. Konow and M. Riepen and M. Gries and G. Droege and T. Lund-Larsen and S. Steibl and S. Borkar and V. K. De and R. Van Der Wijngaart}, 
journal={IEEE Journal of Solid-State Circuits}, 
title={A 48-Core IA-32 Processor in 45 nm CMOS Using On-Die Message-Passing and DVFS for Performance and Power Scaling}, 
year={2011}, 
volume={46}, 
number={1}, 
pages={173-183}, 
keywords={CMOS integrated circuits;integrated circuit design;message passing;microprocessor chips;network-on-chip;power aware computing;shared memory systems;telecommunication network routing;voltage regulators;2D-mesh network-on-chip architecture;45 nm CMOS;48-core IA-32 processor;DDR3 memory channels;DVFS;core-to-core communication;dynamic voltage frequency scaling;fine grain power management;five-port virtual cut-through packet-switched router;multicore processor;on-die message-passing;on-die shared memory;voltage regulator controller;Bandwidth;Computer architecture;Message passing;Routing;Software;Voltage control;2D-routing;CMOS digital integrated circuits;DDR3 controllers;IA-32;dynamic voltage frequency scaling (DVFS);message passing;network-on-chip (NoC)}, 
doi={10.1109/JSSC.2010.2079450}, 
ISSN={0018-9200}, 
month={Jan},}

@article{Bugnion:1997:disco,
 author = {Bugnion, Edouard and Devine, Scott and Govil, Kinshuk and Rosenblum, Mendel},
 title = {Disco: Running Commodity Operating Systems on Scalable Multiprocessors},
 journal = {ACM Trans. Comput. Syst.},
 issue_date = {Nov. 1997},
 volume = {15},
 number = {4},
 month = nov,
 year = {1997},
 issn = {0734-2071},
 pages = {412--447},
 numpages = {36},
 url = {http://doi.acm.org/10.1145/265924.265930},
 doi = {10.1145/265924.265930},
 acmid = {265930},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {scalable multiprocessors, virtual machines},
} 

@inproceedings {268014,
author = {Niraj Tolia and Zhikui Wang and Manish Marwah and Cullen Bash and Parthasarathy Ranganathan and Xiaoyun Zhu},
title = {Delivering Energy Proportionality with Non Energy-Proportional Systems{\textemdash}Optimizing the Ensemble},
booktitle = {Workshop on Power Aware Computing and Systems (HotPower 08)},
year = {2008},
address = {San Diego, CA},
url = {https://www.usenix.org/conference/hotpower-08/delivering-energy-proportionality-non-energy-proportional-systems{\textemdash}optimizing},
publisher = {{USENIX} Association},
month = dec,
}

@article{Ousterhout1988,
 author = {Ousterhout, John K. and Cherenson, Andrew R. and Douglis, Frederick and Nelson, Michael N. and Welch, Brent B.},
 title = {{The Sprite Network Operating System}},
 journal = {Computer},
 issue_date = {February 1988},
 volume = {21},
 number = {2},
 month = feb,
 year = {1988},
 issn = {0018-9162},
 pages = {23--36},
 numpages = {14},
 url = {http://dx.doi.org/10.1109/2.16},
 doi = {10.1109/2.16},
 acmid = {44838},
 publisher = {IEEE Computer Society Press},
 address = {Los Alamitos, CA, USA},
}

@ARTICLE{Cheriton94optimizedmemorybased,
    author = {David Cheriton and Robert A. Kutter},
    title = {{Optimized Memory-Based Messaging: Leveraging the Memory System for High-Performance Communication}},
    journal = {Computing Systems},
    year = {1994},
    volume = {9}
}

@article{cmos,
	author={{Semiconductor Industry Association}},
    title={{2015 International Technology Roadmap for Semiconductors (ITRS)}},
    year = {2015}
}

@article{Lauer,
 author = {Lauer, Hugh C. and Needham, Roger M.},
 title = {{On the Duality of Operating System Structures}},
 journal = {SIGOPS Oper. Syst. Rev.},
 issue_date = {April 1979},
 volume = {13},
 number = {2},
 month = apr,
 year = {1979},
 issn = {0163-5980},
 pages = {3--19},
 numpages = {17},
 url = {http://doi.acm.org/10.1145/850657.850658},
 doi = {10.1145/850657.850658},
 acmid = {850658},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Wulf1974,
 author = {Wulf, W. and Cohen, E. and Corwin, W. and Jones, A. and Levin, R. and Pierson, C. and Pollack, F.},
 title = {{HYDRA: The Kernel of a Multiprocessor Operating System}},
 journal = {Commun. ACM},
 issue_date = {June 1974},
 volume = {17},
 number = {6},
 month = jun,
 year = {1974},
 issn = {0001-0782},
 pages = {337--345},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/355616.364017},
 doi = {10.1145/355616.364017},
 acmid = {364017},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {kernel, nucleus, operating system, protection, security},
} 

@inproceedings{RashidRIGAccentMach,
 author = {Rashid, Richard F.},
 title = {{From RIG to Accent to Mach: The Evolution of a Network Operating System}},
 booktitle = {Proceedings of 1986 ACM Fall Joint Computer Conference},
 series = {ACM '86},
 year = {1986},
 isbn = {0-8186-4743-4},
 location = {Dallas, Texas, USA},
 pages = {1128--1137},
 numpages = {10},
 url = {http://dl.acm.org/citation.cfm?id=324493.325071},
 acmid = {325071},
 publisher = {IEEE Computer Society Press},
 address = {Los Alamitos, CA, USA},
} 

@inproceedings{Young1987,
 author = {Young, M. and Tevanian, A. and Rashid, R. and Golub, D. and Eppinger, J.},
 title = {{The Duality of Memory and Communication in the Implementation of a Multiprocessor Operating System}},
 booktitle = {Proceedings of the Eleventh ACM Symposium on Operating Systems Principles},
 series = {SOSP '87},
 year = {1987},
 isbn = {0-89791-242-X},
 location = {Austin, Texas, USA},
 pages = {63--76},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/41457.37507},
 doi = {10.1145/41457.37507},
 acmid = {37507},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Cheriton1988V,
 author = {Cheriton, David},
 title = {{The V Distributed System}},
 journal = {Commun. ACM},
 issue_date = {March 1988},
 volume = {31},
 number = {3},
 month = mar,
 year = {1988},
 issn = {0001-0782},
 pages = {314--333},
 numpages = {20},
 url = {http://doi.acm.org/10.1145/42392.42400},
 doi = {10.1145/42392.42400},
 acmid = {42400},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@inproceedings{Cheriton1987VMTP,
 author = {Cheriton, D},
 title = {{VMTP: A Transport Protocol for the Next Generation of Communication Systems}},
 booktitle = {Proceedings of the ACM SIGCOMM Conference on Communications Architectures \&Amp; Protocols},
 series = {SIGCOMM '86},
 year = {1986},
 isbn = {0-89791-201-2},
 location = {Stowe, Vermont, USA},
 pages = {406--415},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/18172.18217},
 doi = {10.1145/18172.18217},
 acmid = {18217},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings {segcache,
author = {Juncheng Yang and Yao Yue and Rashmi Vinayak},
title = {Segcache: a memory-efficient and scalable in-memory key-value cache for small objects},
booktitle = {18th USENIX Symposium on Networked Systems Design and Implementation (NSDI 21)},
year = {2021},
isbn = {978-1-939133-21-2},
pages = {503--518},
url = {https://www.usenix.org/conference/nsdi21/presentation/yang-juncheng},
publisher = {USENIX Association},
month = apr,
}

@article{tailatscale,
 author = {{Dean, Jeffrey and Barroso, Luiz Andr{\'e}}},
 title = {{The Tail at Scale}},
 journal = {Commun. ACM},
 issue_date = {February 2013},
 volume = {56},
 number = {2},
 month = feb,
 year = {2013},
 issn = {0001-0782},
 pages = {74--80},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/2408776.2408794},
 doi = {10.1145/2408776.2408794},
 acmid = {2408794},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings {bmcmcd,
author = {Yoann Ghigoff and Julien Sopena and Kahina Lazri and Antoine Blin and Gilles Muller},
title = {{BMC}: Accelerating Memcached using Safe In-kernel Caching and Pre-stack Processing},
booktitle = {18th USENIX Symposium on Networked Systems Design and Implementation (NSDI 21)},
year = {2021},
isbn = {978-1-939133-21-2},
pages = {487--501},
url = {https://www.usenix.org/conference/nsdi21/presentation/ghigoff},
publisher = {USENIX Association},
month = apr,
}

@inproceedings {scalingmcdfacebook,
author = {{Rajesh Nishtala and Hans Fugal and Steven Grimm and Marc Kwiatkowski and Herman Lee and Harry C. Li and Ryan McElroy and Mike Paleczny and Daniel Peek and Paul Saab and David Stafford and Tony Tung and Venkateshwaran Venkataramani}},
title = {{Scaling Memcache at Facebook}},
booktitle = {Presented as part of the 10th {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI} 13)},
year = {2013},
isbn = {978-1-931971-00-3},
address = {Lombard, IL},
pages = {385--398},
url = {https://www.usenix.org/conference/nsdi13/technical-sessions/presentation/nishtala},
publisher = {{USENIX}},
}

@inproceedings{omnixsilberstein,
 author = {{Silberstein, Mark}},
 title = {{OmniX: An Accelerator-centric OS for Omni-programmable Systems}},
 booktitle = {{Proceedings of the 16th Workshop on Hot Topics in Operating Systems}},
 series = {HotOS '17},
 year = {2017},
 isbn = {978-1-4503-5068-6},
 location = {Whistler, BC, Canada},
 pages = {69--75},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/3102980.3102992},
 doi = {10.1145/3102980.3102992},
 acmid = {3102992},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{neardataprocess,
 author = {{Barbalace, Antonio and Iliopoulos, Anthony and Rauchfuss, Holm and Brasche, Goetz}},
 title = {{It's Time to Think About an Operating System for Near Data Processing Architectures}},
 booktitle = {{Proceedings of the 16th Workshop on Hot Topics in Operating Systems}},
 series = {HotOS '17},
 year = {2017},
 isbn = {978-1-4503-5068-6},
 location = {Whistler, BC, Canada},
 pages = {56--61},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/3102980.3102990},
 doi = {10.1145/3102980.3102990},
 acmid = {3102990},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Near data processing, decentralized resource control, multiple kernels OS, single protection domain},
} 

@ARTICLE{ndp1,
author={{R. Balasubramonian and J. Chang and T. Manning and J. H. Moreno and R. Murphy and R. Nair and S. Swanson}},
journal={IEEE Micro},
title={{Near-Data Processing: Insights from a MICRO-46 Workshop}},
year={2014},
volume={34},
number={4},
pages={36-42},
keywords={Computer architecture;Big data;Bandwidth allocation;Costs;Computational modeling;History;Distributed databases;near-data processing;big data;data movement;history of computing},
doi={10.1109/MM.2014.55},
ISSN={0272-1732},
month={July},}

@inproceedings{tpu,
 author = {{Jouppi, Norman P. and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and Boyle, Rick and Cantin, Pierre-luc and Chao, Clifford and Clark, Chris and Coriell, Jeremy and Daley, Mike and Dau, Matt and Dean, Jeffrey and Gelb, Ben and Ghaemmaghami, Tara Vazir and Gottipati, Rajendra and Gulland, William and Hagmann, Robert and Ho, C. Richard and Hogberg, Doug and Hu, John and Hundt, Robert and Hurt, Dan and Ibarz, Julian and Jaffey, Aaron and Jaworski, Alek and Kaplan, Alexander and Khaitan, Harshit and Killebrew, Daniel and Koch, Andy and Kumar, Naveen and Lacy, Steve and Laudon, James and Law, James and Le, Diemthu and Leary, Chris and Liu, Zhuyuan and Lucke, Kyle and Lundin, Alan and MacKean, Gordon and Maggiore, Adriana and Mahony, Maire and Miller, Kieran and Nagarajan, Rahul and Narayanaswami, Ravi and Ni, Ray and Nix, Kathy and Norrie, Thomas and Omernick, Mark and Penukonda, Narayana and Phelps, Andy and Ross, Jonathan and Ross, Matt and Salek, Amir and Samadiani, Emad and Severn, Chris and Sizikov, Gregory and Snelham, Matthew and Souter, Jed and Steinberg, Dan and Swing, Andy and Tan, Mercedes and Thorson, Gregory and Tian, Bo and Toma, Horia and Tuttle, Erick and Vasudevan, Vijay and Walter, Richard and Wang, Walter and Wilcox, Eric and Yoon, Doe Hyun}},
 title = {{In-Datacenter Performance Analysis of a Tensor Processing Unit}},
 booktitle = {{Proceedings of the 44th Annual International Symposium on Computer Architecture}},
 series = {ISCA '17},
 year = {2017},
 isbn = {978-1-4503-4892-8},
 location = {Toronto, ON, Canada},
 pages = {1--12},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/3079856.3080246},
 doi = {10.1145/3079856.3080246},
 acmid = {3080246},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {CNN, DNN, GPU, LSTM, MLP, RNN, TPU, TensorFlow, accelerator, deep learning, domain-specific architecture, neural network},
} 

@inproceedings{dpu,
 author = {{Agrawal, Sandeep R and Idicula, Sam and Raghavan, Arun and Vlachos, Evangelos and Govindaraju, Venkatraman and Varadarajan, Venkatanathan and Balkesen, Cagri and Giannikis, Georgios and Roth, Charlie and Agarwal, Nipun and Sedlar, Eric}},
 title = {{A Many-core Architecture for In-memory Data Processing}},
 booktitle = {{Proceedings of the 50th Annual IEEE/ACM International Symposium on Microarchitecture}},
 series = {MICRO-50 '17},
 year = {2017},
 isbn = {978-1-4503-4952-9},
 location = {Cambridge, Massachusetts},
 pages = {245--258},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/3123939.3123985},
 doi = {10.1145/3123939.3123985},
 acmid = {3123985},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {DPU, accelerator, analytics processor, big data, data movement system, databases, in-memory data processing, low power, microarchitecture},
} 

@inproceedings{aliraza,
 author = {Raza, Ali and Sohal, Parul and Cadden, James and Appavoo, Jonathan and Drepper, Ulrich and Jones, Richard and Krieger, Orran and Mancuso, Renato and Woodman, Larry},
 title = {Unikernels: The Next Stage of Linux’s Dominance},
 year = {2019},
 isbn = {9781450367271},
 publisher = {Association for Computing Machinery},
 address = {New York, NY, USA},
 url = {https://doi.org/10.1145/3317550.3321445},
 doi = {10.1145/3317550.3321445},
 booktitle = {Proceedings of the Workshop on Hot Topics in Operating Systems},
 pages = {7–13},
 numpages = {7},
 keywords = {Operating Systems, Library Operating Systems, Unikernels, Linux},
 location = {Bertinoro, Italy},
 series = {HotOS ’19}
}



@inproceedings{unikernels,
 author = {Madhavapeddy, Anil and Mortier, Richard and Rotsos, Charalampos and Scott, David and Singh, Balraj and Gazagnaire, Thomas and Smith, Steven and Hand, Steven and Crowcroft, Jon},
 title = {Unikernels: Library Operating Systems for the Cloud},
 booktitle = {Proceedings of the Eighteenth International Conference on Architectural Support for Programming Languages and Operating Systems},
 series = {ASPLOS '13},
 year = {2013},
 isbn = {978-1-4503-1870-9},
 location = {Houston, Texas, USA},
 pages = {461--472},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/2451116.2451167},
 doi = {10.1145/2451116.2451167},
 acmid = {2451167},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {functional programming, hypervisor, microkernel},
} 

@inproceedings {legoos,
author = {Yizhou Shan and Yutong Huang and Yilun Chen and Yiying Zhang},
title = {LegoOS: A Disseminated, Distributed {OS} for Hardware Resource Disaggregation},
booktitle = {13th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 18)},
year = {2018},
isbn = {978-1-931971-47-8},
address = {Carlsbad, CA},
pages = {69--87},
url = {https://www.usenix.org/conference/osdi18/presentation/shan},
publisher = {{USENIX} Association},
}

@inproceedings{Kadav:2012:UMD:2150976.2150987,
 author = {Kadav, Asim and Swift, Michael M.},
 title = {Understanding Modern Device Drivers},
 booktitle = {Proceedings of the Seventeenth International Conference on Architectural Support for Programming Languages and Operating Systems},
 series = {ASPLOS XVII},
 year = {2012},
 isbn = {978-1-4503-0759-8},
 location = {London, England, UK},
 pages = {87--98},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/2150976.2150987},
 doi = {10.1145/2150976.2150987},
 acmid = {2150987},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {device drivers, measurement},
} 

@inproceedings{LeVasseur:2004:UDD:1251254.1251256,
 author = {LeVasseur, Joshua and Uhlig, Volkmar and Stoess, Jan and G\"{o}tz, Stefan},
 title = {Unmodified Device Driver Reuse and Improved System Dependability via Virtual Machines},
 booktitle = {Proceedings of the 6th Conference on Symposium on Opearting Systems Design \& Implementation - Volume 6},
 series = {OSDI'04},
 year = {2004},
 location = {San Francisco, CA},
 pages = {2--2},
 numpages = {1},
 url = {http://dl.acm.org/citation.cfm?id=1251254.1251256},
 acmid = {1251256},
 publisher = {USENIX Association},
 address = {Berkeley, CA, USA},
} 

@article{p4,
 author = {Bosshart, Pat and Daly, Dan and Gibb, Glen and Izzard, Martin and McKeown, Nick and Rexford, Jennifer and Schlesinger, Cole and Talayco, Dan and Vahdat, Amin and Varghese, George and et al.},
 title = {P4: Programming Protocol-Independent Packet Processors},
 year = {2014},
 issue_date = {July 2014},
 publisher = {Association for Computing Machinery},
 address = {New York, NY, USA},
 volume = {44},
 number = {3},
 issn = {0146-4833},
 url = {https://doi.org/10.1145/2656877.2656890},
 doi = {10.1145/2656877.2656890},
 journal = {SIGCOMM Comput. Commun. Rev.},
 month = jul,
 pages = {87–95},
 numpages = {9},
 keywords = {p4, sdn, reconfigurability, protocol-independent}
}

@inproceedings{10.5555/2387880.2387894,
author = {Han, Sangjin and Marshall, Scott and Chun, Byung-Gon and Ratnasamy, Sylvia},
title = {MegaPipe: A New Programming Interface for Scalable Network I/O},
year = {2012},
isbn = {9781931971966},
publisher = {USENIX Association},
address = {USA},
abstract = {We present MegaPipe, a new API for efficient, scalable network I/O for message-oriented workloads. The design of MegaPipe centers around the abstraction of a channel - a per-core, bidirectional pipe between the kernel and user space, used to exchange both I/O requests and event notifications. On top of the channel abstraction, we introduce three key concepts of MegaPipe: partitioning, lightweight socket (lwsocket), and batching.We implement MegaPipe in Linux and adapt memcached and nginx. Our results show that, by embracing a clean-slate design approach, MegaPipe is able to exploit new opportunities for improved performance and ease of programmability. In microbenchmarks on an 8-core server with 64 B messages, MegaPipe outperforms baseline Linux between 29% (for long connections) and 582% (for short connections). MegaPipe improves the performance of a modified version of memcached between 15% and 320%. For a workload based on real-world HTTP traces, MegaPipe boosts the throughput of nginx by 75%.},
booktitle = {Proceedings of the 10th USENIX Conference on Operating Systems Design and Implementation},
pages = {135–148},
numpages = {14},
location = {Hollywood, CA, USA},
series = {OSDI'12}
}

@inproceedings{10.1145/3132747.3132764,
author = {Jin, Xin and Li, Xiaozhou and Zhang, Haoyu and Soul\'{e}, Robert and Lee, Jeongkeun and Foster, Nate and Kim, Changhoon and Stoica, Ion},
title = {NetCache: Balancing Key-Value Stores with Fast In-Network Caching},
year = {2017},
isbn = {9781450350853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132747.3132764},
doi = {10.1145/3132747.3132764},
abstract = {},
booktitle = {Proceedings of the 26th Symposium on Operating Systems Principles},
pages = {121–136},
numpages = {16},
keywords = {Key-value stores, Programmable switches, Caching},
location = {Shanghai, China},
series = {SOSP '17}
}

@inproceedings{shenango,
author = {Ousterhout, Amy and Fried, Joshua and Behrens, Jonathan and Belay, Adam and Balakrishnan, Hari},
title = {Shenango: Achieving High CPU Efficiency for Latency-Sensitive Datacenter Workloads},
year = {2019},
isbn = {9781931971492},
publisher = {USENIX Association},
address = {USA},
abstract = {Datacenter applications demand microsecond-scale tail latencies and high request rates from operating systems, and most applications handle loads that have high variance over multiple timescales. Achieving these goals in a CPU-efficient way is an open problem. Because of the high overheads of today's kernels, the best available solution to achieve microsecond-scale latencies is kernel-bypass networking, which dedicates CPU cores to applications for spin-polling the network card. But this approach wastes CPU: even at modest average loads, one must dedicate enough cores for the peak expected load.Shenango achieves comparable latencies but at far greater CPU efficiency. It reallocates cores across applications at very fine granularity--every 5 µs--enabling cycles unused by latency-sensitive applications to be used productively by batch processing applications. It achieves such fast reallocation rates with (1) an efficient algorithm that detects when applications would benefit from more cores, and (2) a privileged component called the IOKernel that runs on a dedicated core, steering packets from the NIC and orchestrating core reallocations. When handling latency-sensitive applications, such as memcached, we found that Shenango achieves tail latency and throughput comparable to ZygOS, a state-of-the-art, kernel-bypass network stack, but can linearly trade latency-sensitive application throughput for batch processing application throughput, vastly increasing CPU efficiency.},
booktitle = {Proceedings of the 16th USENIX Conference on Networked Systems Design and Implementation},
pages = {361–377},
numpages = {17},
location = {Boston, MA, USA},
series = {NSDI'19}
}



@inproceedings{Ryzhyk:2009:DTD:1519065.1519095,
 author = {Ryzhyk, Leonid and Chubb, Peter and Kuz, Ihor and Heiser, Gernot},
 title = {Dingo: Taming Device Drivers},
 booktitle = {Proceedings of the 4th ACM European Conference on Computer Systems},
 series = {EuroSys '09},
 year = {2009},
 isbn = {978-1-60558-482-9},
 location = {Nuremberg, Germany},
 pages = {275--288},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1519065.1519095},
 doi = {10.1145/1519065.1519095},
 acmid = {1519095},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrent programming, device drivers, domain-specific languages, fault avoidance, reliability},
} 

@inproceedings{Schupbach:2011:DLA:1950365.1950382,
 author = {Sch\"{u}pbach, Adrian and Baumann, Andrew and Roscoe, Timothy and Peter, Simon},
 title = {A Declarative Language Approach to Device Configuration},
 booktitle = {Proceedings of the Sixteenth International Conference on Architectural Support for Programming Languages and Operating Systems},
 series = {ASPLOS XVI},
 year = {2011},
 isbn = {978-1-4503-0266-1},
 location = {Newport Beach, California, USA},
 pages = {119--132},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1950365.1950382},
 doi = {10.1145/1950365.1950382},
 acmid = {1950382},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {constraint logic programming, eclipse clp, hardware programming, pci configuration},
} 

@inproceedings{Renzelmann:2009:DMD:1855807.1855821,
 author = {Renzelmann, Matthew J. and Swift, Michael M.},
 title = {Decaf: Moving Device Drivers to a Modern Language},
 booktitle = {Proceedings of the 2009 Conference on USENIX Annual Technical Conference},
 series = {USENIX'09},
 year = {2009},
 location = {San Diego, California},
 pages = {14--14},
 numpages = {1},
 url = {http://dl.acm.org/citation.cfm?id=1855807.1855821},
 acmid = {1855821},
 publisher = {USENIX Association},
 address = {Berkeley, CA, USA},
} 

@inproceedings{darksilicon,
author = {Esmaeilzadeh, Hadi and Blem, Emily and St. Amant, Renee and Sankaralingam, Karthikeyan and Burger, Doug},
title = {Dark Silicon and the End of Multicore Scaling},
year = {2011},
isbn = {9781450304726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2000064.2000108},
doi = {10.1145/2000064.2000108},
abstract = {Since 2005, processor designers have increased core counts to exploit Moore's Law scaling, rather than focusing on single-core performance. The failure of Dennard scaling, to which the shift to multicore parts is partially a response, may soon limit multicore scaling just as single-core scaling has been curtailed. This paper models multicore scaling limits by combining device scaling, single-core scaling, and multicore scaling to measure the speedup potential for a set of parallel workloads for the next five technology generations. For device scaling, we use both the ITRS projections and a set of more conservative device scaling parameters. To model single-core scaling, we combine measurements from over 150 processors to derive Pareto-optimal frontiers for area/performance and power/performance. Finally, to model multicore scaling, we build a detailed performance model of upper-bound performance and lower-bound core power. The multicore designs we study include single-threaded CPU-like and massively threaded GPU-like multicore chip organizations with symmetric, asymmetric, dynamic, and composed topologies. The study shows that regardless of chip organization and topology, multicore scaling is power limited to a degree not widely appreciated by the computing community. Even at 22 nm (just one year from now), 21% of a fixed-size chip must be powered off, and at 8 nm, this number grows to more than 50%. Through 2024, only 7.9x average speedup is possible across commonly used parallel workloads, leaving a nearly 24-fold gap from a target of doubled performance per generation.},
booktitle = {Proceedings of the 38th Annual International Symposium on Computer Architecture},
pages = {365–376},
numpages = {12},
keywords = {power, dark silicon, technology scaling, multicore, modeling},
location = {San Jose, California, USA},
series = {ISCA '11}
}

@inproceedings{Ganapathy:2008:DIM:1346281.1346303,
 author = {Ganapathy, Vinod and Renzelmann, Matthew J. and Balakrishnan, Arini and Swift, Michael M. and Jha, Somesh},
 title = {The Design and Implementation of Microdrivers},
 booktitle = {Proceedings of the 13th International Conference on Architectural Support for Programming Languages and Operating Systems},
 series = {ASPLOS XIII},
 year = {2008},
 isbn = {978-1-59593-958-6},
 location = {Seattle, WA, USA},
 pages = {168--178},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1346281.1346303},
 doi = {10.1145/1346281.1346303},
 acmid = {1346303},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {device drivers, program partitioning, reliability},
} 

@inproceedings{Ryzhyk:2010:CAD:1851276.1851283,
 author = {Ryzhyk, Leonid and Zhu, Yanjin and Heiser, Gernot},
 title = {The Case for Active Device Drivers},
 booktitle = {Proceedings of the First ACM Asia-pacific Workshop on Workshop on Systems},
 series = {APSys '10},
 year = {2010},
 isbn = {978-1-4503-0195-4},
 location = {New Delhi, India},
 pages = {25--30},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/1851276.1851283},
 doi = {10.1145/1851276.1851283},
 acmid = {1851283},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrency, device drivers, stack ripping},
} 

@inproceedings{Ryzhyk:2014:UDD:2685048.2685101,
 author = {Ryzhyk, Leonid and Walker, Adam and Keys, John and Legg, Alexander and Raghunath, Arun and Stumm, Michael and Vij, Mona},
 title = {User-guided Device Driver Synthesis},
 booktitle = {Proceedings of the 11th USENIX Conference on Operating Systems Design and Implementation},
 series = {OSDI'14},
 year = {2014},
 isbn = {978-1-931971-16-4},
 location = {Broomfield, CO},
 pages = {661--676},
 numpages = {16},
 url = {http://dl.acm.org/citation.cfm?id=2685048.2685101},
 acmid = {2685101},
 publisher = {USENIX Association},
 address = {Berkeley, CA, USA},
} 

@inproceedings{Ball:2006:TSA:1217935.1217943,
 author = {Ball, Thomas and Bounimova, Ella and Cook, Byron and Levin, Vladimir and Lichtenberg, Jakob and McGarvey, Con and Ondrusek, Bohus and Rajamani, Sriram K. and Ustuner, Abdullah},
 title = {Thorough Static Analysis of Device Drivers},
 booktitle = {Proceedings of the 1st ACM SIGOPS/EuroSys European Conference on Computer Systems 2006},
 series = {EuroSys '06},
 year = {2006},
 isbn = {1-59593-322-0},
 location = {Leuven, Belgium},
 pages = {73--85},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1217935.1217943},
 doi = {10.1145/1217935.1217943},
 acmid = {1217943},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {formal verification, software model checking},
} 

