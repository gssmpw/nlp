
\subsection{Black-Box Generality: Diverse Apps and Hardware}
\label{sec:tailbench}
\begin{table}[t]
\small
\begin{tabular}{|c|c|c|c|c|}
    \hline
  Name & CPU & Cores & NIC & RAM\\ \hline
     N0 & Intel E5-2640 & 8 & Mellanox 25GbE & 62GB\\ \hline
     N1 & Intel E5-2660 & 20 & Solarflare 10GbE & 128GB\\ \hline
     N2 & AMD EPYC 7452 & 32 & Mellanox 40GbE & 128GB\\ \hline
     N3 & Ampere ARMv8 & 80 & Mellanox 25GbE & 124 GB \\ \hline
\end{tabular}
\caption{\small Different hardware explored to run Tailbench.}
\label{table:hwsw}
\vspace{-0.2in}
\end{table}

In this section, we further demonstrate the versatility of the controller by applying it to optimize energy efficiency for three applications from Tailbench~\cite{tailbench}. Our motivation was to reveal how externally controlling interrupt coalescing and CPU frequency can be applied agnostically on hardware even across multi-generational divides\footnote{Scripts to reproduce results at https://anonymous.4open.science/r/bayop-188B}.


\subsubsection{Experimental Setup}
For these experiments, we selected four hardware platforms as shown in \cref{table:hwsw}. Nodes N0, N1, and N2 are provided by CloudLab~\cite{cloudlab} and we disable hyperthreads and TurboBoost on all processors to minimize system noise. For each node type, we create a cluster consisting of a single server node, three client nodes that generate traffic to the server node, and an external bootstrap node that launches experiments and runs the \texttt{BayOp} controller to tune interrupt coalescing (ITR-delay) and CPU frequency (DVFS) on the server. All of the nodes were running Linux 5.15 kernel; we only examined Linux as EbbRT does not have the necessary device driver support for Solarflare and Mellanox NICs. Notably, while we used \texttt{ethtool} to set static interrupt rates across all three NICs in this paper, the fundamental implementation may be different depending on the hardware's capability. On the Intel processors, we use the RAPL hardware registers~\cite{rapl} to report its dynamic energy use while for AMD, we use \texttt{amd\_energy} hardware monitor driver~\cite{amdenergy}.  

Node N3 is another experimental node that runs Linux 6.4.13 but we could only get a single client node to generate traffic\footnote{Due to the computation-heavy nature of Tailbench applications, we found this was still able to saturate the single server}. The ARMv8 server provided \texttt{xgene-hwmon}~\cite{armxgene} tool that enabled us to report its power readings. 

For each hardware category, we selected applications from Tailbench~\cite{tailbench}, each designed to fulfill distinct SLA objectives. These applications encompassed \textbf{img-dnn}, a handwriting recognition program built on OpenCV; \textbf{sphinx}, an open-source search engine; and \textbf{xapian}, a speech recognition system. These applications both represent a diverse suite of benchmarks in contrast to the previous examples from our study as well as providing new SLA objectives in the order of milliseconds to seconds. Overall, these selections allowed us to assess the impact of different SLAs and hardware platforms.


\subsubsection{Experimental Results}
In our experiments, we observed that the controller consistently achieves energy savings ranging from 5\% to 36\%, depending on the specific combination of software and hardware. Importantly, our findings underscore the fundamental nature of these two mechanisms, which can be effectively applied across a variety of hardware platforms in different SLA-driven application domains. Further, we found that the generic architecture of our controller meant that it was straightforward to simply deploy this technique in new hardware environments as long as it provided support for energy readings and exposed control of interrupt coalescing and CPU frequency. 

Fig.~\ref{fig:xl170} depicts the resulting energy consumption for Tailbench; we normalize the energy usage relative to the default Linux configurations under different scenarios:
\begin{enumerate}
    \item We selected representative offered loads of 40\% and 80\% of each hardware platform's peak QPS capacity for running the respective applications.
    \item For each of these offered loads, we applied two distinct SLA objectives tailored to each application, as indicated by the labels in each figure. These SLA objectives were derived from default values provided by the authors of Tailbench~\cite{tailbench}.
\end{enumerate}

However, it is worth pointing out that the controller's ability to adapt to applications and offered loads is heavily influenced by the hardware's ability to offer a range of configurations for exploration within this space. One can see an example of this for \textbf{APP: img-dnn} in \textbf{Node: N2} where it did not manage to find an ITR-delay, DVFS pair that managed to further reduce energy consumption. We hypothesize this stems from a combination of the application type as well as the DVFS settings provided by the AMD EPYC 7452 processor. The processor uses AMD's Collaborative Processor Performance Control (CPPC) interface~\cite{amdpstate}, which is an abstracted performance value that isn't tied to specific a CPU frequency; further, we were limited to only three settings in contrast to the hundreds and thousands available on the other processors. However, this limitation can also be mitigated by newer processors that support the AMD P-state EPP~\cite{amdepp} driver, providing finer-grained CPU frequency settings.

\begin{figure}[!htb]
\centering
    \includegraphics[width=0.45\textwidth]{cdf1.png}
    \caption{\small CDF of per request latency between Linux and Linux-BayOp from a single Tailbench application.}
    \label{fig:cdf1}
    \vspace{-0.2in}
\end{figure}

To delve into the energy gains 
we detail the CDF of an example Tailbench application in \cref{fig:cdf1}. In this figure, we illustrate the per-request latency as provided by Tailbench when running the img-dnn application on our ARMv8 server (note this is at a particular peak load and SLA). As the figure shows, the overall request latency of \textit{Linux-BayOP} is about 2X worse than Linux as the controller chose energy-efficient ITR-delay and DVFS settings. While we found Linux was able to support this workload with a 99\% latency of 2.8ms, \textit{Linux-BayOp} was still able to meet the SLA at 99\% latency of 4.8ms while saving 31\% energy.
