\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

\icmltitlerunning{OBELiX: An Experimental Ionic Conductivity Dataset}

\begin{document}

\newcommand{\harry}[1]{\textcolor{cyan}{#1}}
\newcommand{\points}[1]{\textcolor{red}{#1}}

\twocolumn[

\icmltitle{OBELiX: A Curated Dataset of Crystal Structures and Experimentally Measured Ionic Conductivities for Lithium Solid-State Electrolytes}

\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{F\'elix Therrien}{equal,mila}
\icmlauthor{Jamal Abou Haibeh}{equal,mila,mcgill}
\icmlauthor{Divya Sharma}{mila}
\icmlauthor{Rhiannon Hendley}{uottawa,nrc}
\icmlauthor{Alex Hern\'andez-Garc\'ia}{udem,mila}
\icmlauthor{Sun Sun}{nrc}
\icmlauthor{Alain Tchagang}{nrc}
\icmlauthor{Jiang Su}{nrc}
\icmlauthor{Samuel Huberman}{mcgill}
\icmlauthor{Yoshua Bengio}{udem,mila}
\icmlauthor{Hongyu Guo}{nrc}
\icmlauthor{Homin Shin}{nrc}
\end{icmlauthorlist}

\icmlaffiliation{mila}{Mila, Montr\'eal, Canada}
\icmlaffiliation{mcgill}{Department of Chemical Engineering, McGill University, Montr\'eal, Canada}
\icmlaffiliation{uottawa}{Department of Chemistry and Biomolecular Science, University of Ottawa, Ottawa, Canada}
\icmlaffiliation{udem}{D\'epartement d'informatique et de recherche opérationnelle, Universit\'e de Montr\'eal, Montr\'eal, Canada}
\icmlaffiliation{nrc}{National Research Council Canada, Ottawa, Canada}

\icmlcorrespondingauthor{Homin Shin}{homin.shin@nrc-cnrc.gc.ca}
\icmlcorrespondingauthor{F\'elix Therrien}{felix.therrien@mila.quebec}
\icmlcorrespondingauthor{Hongyu Guo}{hongyu.guo@uottawa.ca}

\icmlkeywords{Solid-state, batteries, electrolytes, ionic conductivity, energy storage, crystal, inorganics}
\vskip 0.3in
]

\printAffiliationsAndNotice{\icmlEqualContribution}

\begin{abstract}
Solid-state electrolyte batteries are expected to replace liquid electrolyte lithium-ion batteries in the near future thanks to their higher theoretical energy density and improved safety. However, their adoption is currently hindered by their lower effective ionic conductivity, a quantity that governs charge and discharge rates. Identifying highly ion-conductive materials using conventional theoretical calculations and experimental validation is both time-consuming and resource-intensive. While machine learning holds the promise to expedite this process, relevant ionic conductivity and structural data is scarce. Here, we present OBELiX, a domain-expert-curated database of $\sim$600 synthesized solid electrolyte materials and their experimentally measured room temperature ionic conductivities gathered from literature. Each material is described by their measured composition, space group and lattice parameters. A full-crystal description in the form of a crystallographic information file (CIF) is provided for $\sim$320 structures for which atomic positions were available. 
We discuss various statistics and features of the dataset and provide training and testing splits that avoid data leakage. Finally, we benchmark seven existing ML models on the task of predicting ionic conductivity and discuss their performance. The goal of this work is to facilitate the use of machine learning for solid-state electrolyte materials discovery.

\end{abstract}

\section{Introduction}
\label{sec:intro}

Lithium-ion batteries (LIBs) used in most consumer electronics and electric vehicles have seen immense progress in terms of energy density, power density, safety and durability. However, their performance is reaching a plateau. Solid-state batteries are regarded as the next generation of batteries that may allow significant improvement over these characteristics \cite{janek2016solid, janek2023challenges}. The key difference between these two technologies is their electrolyte, the medium which allows the transport of ions during charge and discharge. A solid-state electrolyte (SSE)---as opposed to a liquid electrolyte in LIBs---permits new design choices that ultimately lead to better battery properties \cite{betz2019theoretical}, let alone the fact that solid electrolytes are not flammable unlike their liquid counterparts.   

SSEs have a long research history, starting from Faraday’s first discovery of fast ion transport in $\beta$-$\text{PbF}_2$ and $\text{Ag}_2\text{S}$ about 200 years ago \cite{funke2013solid}, until the relatively recent development of Li-based ionic conductors, such as $\text{Li}_{10}\text{GeP}_2\text{S}_{12}$ \cite{kamaya2011lithium}. 

Ionic conductivity ($\sigma$), expressed in siemens per centimeter (S/cm), measures how easily ions can move through a medium or material. Ideal SSEs, also called ``superionic'' or ``fast-ionic'' conductors, are electrolytes that exhibit ionic conductivity comparable to those observed in liquid electrolytes and molten solids ($>$ 1 mS/cm). Only a limited number of \textit{room temperature} ideal SSEs are known thus far within a small number of classes of materials such as LISICON (e.g., $\text{Li}_{14}\text{ZnGe}_4\text{O}_{16}$), NASICON (e.g., $\text{Li}_{1.3}\text{Al}_{0.3}\text{Ti}_{1.7}(\text{PO}_{4})_{3})$, garnet (e.g., $\text{Li}_7\text{Li}_3\text{Zr}_2\text{O}_{12}$), perovskite (e.g., $\text{Li}_{0.5}\text{La}_{0.5}\text{TiO}_3$), and argyrodite (e.g., $\text{Li}_6\text{PS}_5\text{Cl}$) \cite{janek2023challenges}. 

Until now, the discovery of novel SSEs has largely relied on an incremental, experimental approach which consists, for example, of substituting atoms and elements in known compounds. This has allowed the discovery of some highly ion-conductive materials, but greatly limits the search space given that the experimental synthesis and characterization of a new, stable, inorganic solid-state electrolyte is a difficult and costly process that can take months to years \cite{zhao2022understanding}.
%
\begin{table*}
    \caption{Comparison of our dataset (OBELiX) with existing ones based on key features and labels. For features, the numbers represent the number of entries with that feature that are labeled with at least one experimental or computational ion transport property (not necessarily ionic conductivity). Numbers in parentheses represent proprietary data.}
    \label{tab:datasets_comparison}
    \vskip 0.15in
    \begin{center}
    \begin{small}
    \begin{sc}
    \begin{tabular}{llllllll}%{lccccccc}
        \toprule
        %Dataset       & $\sigma_{RT}^{\text{exp}}$ & $\sigma^{\text{exp}}$ & Comp. & Spg & lattice & CIFs \\
        Dataset       & \multicolumn{2}{l}{Labels} & & \multicolumn{4}{l}{Features} \\
                      & \multicolumn{1}{c@{\hspace*{\tabcolsep}\makebox[0pt]{$\subset$}}}{$\sigma_{RT}^{\text{exp}}$} & $\sigma^{\text{exp}}$ & & Comp. & Spg & Lattice & CIFs \\
        \midrule
        \citet{sendek2017holistic} & 0 & 0 & & 317 & 317 & 317 & 317 \\
        \citet{jalem2018bayesian}  & 0 & 0 & & 318 & 318 & 318 & 318 \\
        \citet{he2020high} (SPSE)    & n.a. & 12,000 & & 75 (7,678) & 75 (7,678) & 75 (7,678) & 75 (7,678) \\
        \citet{hargreaves2023database} (LiIon)     & 465 & 820 & & 820 & 0 & 0 & 0 \\
        \citet{laskowski2023identification}     & 1346 & 1346 & & 1346 & 0 (344) & 0 (344) & 0 (344) \\
        \citet{shon2023extracting} & n.a. & 4032 & & 4032 & 0 & 0 & 0 \\
        \citet{yang2024dynamic} (DDSE)  & 1939 & 2448 & & 2448 & 0 & 0 & 0  \\
        \midrule
        OBELiX   & 599 & 599 & & 599 & 599 & 599 & 321 \\
        \bottomrule
    \end{tabular}
    \end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

Computational discovery, on the other hand, requires time-consuming atomistic simulations, such as ab initio molecular dynamics (AIMD), to accurately capture the complex relationship between ionic conductivity and the material's structure and composition \cite{ceder2018predictive, qi2021bridging, bielefeld2020modeling}. 
These calculations can take from several hours to a few days for a single ionic conductivity and their parameters are often materials specific. Therefore, they are not well suited for large-scale explorations of hypothetical materials.

Machine learning (ML) has the potential to greatly accelerate the discovery of novel SSEs. Naturally, it can be used to predict ionic conductivity directly using, for example, graph neural networks (GNNs), which have been used extensively and successfully in materials science \cite{schmidt2019recent, butler2018machine}. Machine-learned force fields or interatomic potentials (MLFF or MLIP) can also be used to obtain ionic conductivity through molecular dynamics in the ``classical'' way while using significantly less resources \cite{wines2024chips}. Finally, generative frameworks can accelerate dynamics simulations \cite{nam2024flow} and, provided that good ionic conductivity models are developed, there exists a wide range of frameworks that could generate new materials conditioned on that property \cite{hernandez-garcia2023crystal, zhu2024wycryst, zeni2023mattergen, merchant2023scaling}. However, the main obstacle to the development and validation of these models---and to some extent theoretical models---is the scarcity of relevant experimental ionic conductivity and structural datasets. Indeed, as detailed in the next section, the few datasets that exist contain partial material descriptions and ionic conductivity measurements at various temperatures. To the best of our knowledge there does not exist another open access dataset of experimental room temperature ionic conductivities with corresponding full crystal descriptions.

% Summary
In this work, we assembled OBELiX (\underline{O}pen solid \underline{B}attery \underline{E}lectrolytes with \underline{Li}: an e\underline{X}perimental dataset), a curated database of 599 synthesized solid electrolyte materials and their experimentally measured room temperature ionic conductivity along with descriptors of their space group, lattice parameters, and chemical composition\footnote{OBELiX is available here: \href{https://github.com/NRC-Mila/OBELiX}{github.com/NRC-Mila/OBELiX}}. The database is analyzed in terms of the distribution of ionic conductivity, space groups, elements, and repeated compositions.  We also propose a training and testing split that avoids data leakage between similar entries while balancing distributions of properties across splits. We use this split to benchmark the performance of 7 machine learning models at directly predicting room temperature ionic conductivity ($\sigma_{\text{RT}}$). Our database and benchmarks aim to significantly accelerate the ML-assisted discovery of novel SSEs with fast ion diffusion.

\section{Related work}
\label{sec:relatedwork}

Crystal structure databases such as the Materials Project \cite{jain2013commentary} or the Inorganic Crystal Structure Database (ICSD) \cite{belsky2002new,hellenbrandt2004inorganic} contain large amounts of potential candidates for solid-state electrolytes. For example, \citet{sendek2017holistic} screened more than 12,000 Li-containing crystals for Li-ion SSEs using multiple criteria, thereby identifying 317 candidates, among which 21 crystals that showed promise as SSEs were selected from an ML-guided model. The ionic conductivity of these 21 structures was computed theoretically.

\citet{jalem2018bayesian} annotated 318 compounds by calculating ion migration energy barriers ($E_b$), a less accurate but computationally lighter property that relates to ionic conductivity. Bayesian optimization was employed to screen candidate compounds with low $E_b$.

\citet{he2020high} compiled a database of over 90,000 crystal structures, including more than 7,000 structures with preliminary ion-transport data obtained through geometric analysis, and 12,000 activation energy values ($E_b$) calculated using the bond valence site energy method. Additionally, they manually extracted 75 CIF files from literature data. They employed empirical and geometrical methods to estimate the minimum energy paths of these structures and obtain $E_b$, but they did not predict $\sigma$.

In the last two years, four experimentally-derived datasets curated from a large collection of SSE literature were published. 

The Liverpool Ionics (LiIon) Dataset \cite{hargreaves2023database} reports 820 entries containing chemical composition, structural family, and ionic conductivity at different temperatures (from 5 to 873$^\circ C$) measured by alternating current impedance spectroscopy, among which 465 entries were at room temperature.

\citet{laskowski2023identification} gathered a dataset of 1346 entries with compositions, space group, and corresponding $\sigma_{\text{RT}}$, with a subset of 344 compounds whose structures are manually matched with an ICSD ID. 

Finally, \citet{shon2023extracting} used text mining to extract more than 4000 ionic conductivity measurements from 1457 papers. Each ionic conductivity measurement is associated with a composition and about 350 are also associated with a ``structure type''. Measurement temperature is not specified and compositions are not always fully described.

A recent study by \citet{yang2024dynamic} introduced the Dynamic Database of Solid-State Electrolyte (DDSE) to facilitate the exploration of structure-performance relationships and accelerate the discovery of high-performance solid-state electrolytes (SSEs). The database contains performance data for 2448 materials (at time of writing), including ionic conductivity obtained from experimental reports, across a broad temperature range (132.40–1261.60 K). By employing data mining and machine learning, the DDSE provides tools to evaluate and predict ionic conductivity for new materials.

These recent reports greatly increased the amount of readily available experimental ionic conductivity data. However, they contain limited structural information: the databases by \citet{shon2023extracting} and \citet{yang2024dynamic} contain only a qualitative structure description for some materials, the LiIon dataset only includes the structural family and the dataset by \citet{laskowski2023identification} is limited to space group information. Although the full crystallographic information of the 344 compounds of the Laskowski dataset for which the ICSD ID is provided could be retrieved, the proprietary ICSD is not available to most researchers in the ML community. Table~\ref{tab:datasets_comparison} summarizes the differences in terms of available features across the databases discussed above. 

The lack of precise structural information labeled with ionic conductivity makes it difficult (1) to compare experimental values with theoretical predictions which require full crystal descriptions and (2) to train machine learning models to accurately predict ionic conductivity.

%Natural language processing (NLP) or text mining are expected to extract data much quickly from the extensive amount of scientific literature.  However, acquiring the high-quality data of SSEs have not been achievable yet \cite{}, because the ionic conductivity is typically reported not as a single number but multiple values at different temperatures, and inconsistently presented (i.e., body text, tables, or figures) with no standardized units.

\begin{figure}
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\linewidth]{partial_occ.pdf}}
\caption{Examples of solid state electrolyte materials with partial occupancies.}
\label{fig:particaloccupancy}
\end{center}
\vskip -0.2in
\end{figure}
%
\begin{figure*}
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\linewidth]{gathered.pdf}}
\caption{\label{fig:stats} a) Distributions of ionic-conductivity values for the training and testing sets along with proportions of crystal families and space groups. Only space groups that represent more than 1\% of the sets are labeled. b) Venn diagram showing shared entries between our dataset and others c) Proportion of entries that contain each element in the periodic table. Elements that are not present in the dataset are shaded. Generated with pymatviz \cite{pymatviz}.}
\end{center}
\vskip -0.2in
\end{figure*}
%
\section{Data}
\label{sec:dataset}

% Data collection
% - Going through dataset, looking for RT, checking in paper, extrapolation to RT
% Total ionic conductivity is reported, bulk is noted when available
% Total can include factors that are not explicitely in the database
\subsection{Background}

All the solid-state electrolyte materials in OBELiX are crystal structures. Crystals are materials with a repeating arrangement of the same atoms. The composition (or chemical formula) describes which atoms are present in what proportion. The repeating pattern in a crystal, the unit cell, is contained within a parallelepiped (lattice) with edges $a$, $b$, $c$ and angles $\alpha$, $\beta$, $\gamma$  that, together, form the lattice parameters. The symmetry of a crystal is described by its space group, of which there are 230, representing all possible combinations of symmetry operation in 3D. Space groups are usually denoted as a string of numbers and letters representing their symmetry operations or a number between 1 and 230, e.g. $Fm\overline{3}m$ for space group 225. 

While the combination of the composition, lattice parameters and space group is often sufficient to qualify materials, they do not fully describe the crystal structure because in general they do not specify the positions of each atom. Some experimental papers perform an analysis (Rietveld refinement) of the X-ray powder diffraction pattern of their materials to estimate atomic positions. This information is necessary to perform molecular dynamics simulations, for example.

In contrast to theory-based data found in the Materials Project, for example, experimental compositions often feature fractional numbers (real numbers rather than integers) resulting from partially vacant sites or disorder associated with partial cation substitution. Consider, for example, composition K$_{0.1}$Li$_{0.9}$SbO$_3$. At a specific location in the crystal (a site) there is a 90\% probability of finding a lithium (Li) atom and a 10\% probability of finding a potassium (K) atom. Site occupancy does not need to add up to one, and sites are often partially empty. Such partial occupancy is ubiquitously observed in Li-ion SSEs and it plays a crucial role in creating diffusion pathways. For example, the $\sigma_{\text{RT}}$ of tetragonal $\text{Li}_7\text{Li}_3\text{Zr}_2\text{O}_{12}$ with a space group of I41/acd (no. 142) is two orders of magnitude smaller than that of the same garnet framework of cubic $\text{Li}_7\text{Li}_3\text{Zr}_2\text{O}_{12}$ with Ia-3d (no. 230) (see Figure~\ref{fig:particaloccupancy}a). In this case, the disordering and partial occupation of Li (at the 96h site) promotes the Li-ion conduction. In the halide structure of $\text{Li}_3\text{InCl}_6$ (Figure~\ref{fig:particaloccupancy}b)), the substitution of one Li+ with the In3+ cation introduces two intrinsic vacancies, to which are attributed the high $\sigma_{\text{RT}}$ of that material. 
% The prototype NASICON structure of $\text{LiTi}_2(\text{PO}_4)_3$ (Figure~\ref{fig:particaloccupancy}c) improves its ionic conductivity by substituting Ti4+ with Sn4+ forming $\text{LiTi}_{(2-x)}\text{Sn}_x(\text{PO}_4)_3$, in which the Li sites are fully occupied whereas the ratio of relative occupancy of Ti and Sn cations at the same site determines the average size of the cation site, thereby affecting the energetic profile of the conduction path.
In sum, in order to screen SSEs with high $\sigma_{\text{RT}}$, it is highly desirable to include partial occupancy as a key feature of the materials. 

%
\begin{figure*}
\centering
\includegraphics[width=0.8\linewidth]{repeat_values.pdf}
\caption{\label{fig:repeat_values} Ionic conductivity of entries in the dataset that have the same composition and space group. The color shows the largest relative difference between lattice parameters within a set of entries with same space group and composition. The inset shows the distribution of differences with the mean ionic conductivity of the sets in log scale. It is scaled proportionally to the rest of the plot.}
\vspace{-3ex}
\end{figure*}
%
\subsection{Data Collection}

We built our dataset starting from the Liverpool Ionics Dataset and the Laskowski dataset by selecting materials for which the experimental room temperature ionic conductivity, space group and lattice parameters could be obtained. We manually retrieved missing information (e.g. lattice parameters or $\sigma_{RT}$) from the original paper's table or figures. Through this procedure, we obtained a total of 599 distinct entries including an additional 15 entries from other sources. Figure~\ref{fig:stats}b shows the number of common entries between these two datasets and ours.   

Ionic conductivity is usually reported as a property of the materials in the powder form which, includes the effect of defects and grain boundaries. It is referred to as ``total'' ionic conductivity. The ionic conductivity of individual grain is sometimes reported as the ``bulk'' ionic conductivity. When both were available we recorded both. This is relevant because the total ionic conductivity of materials not only depends on their crystal structure but also on factors such as the size of particles.

For each material, we recorded the total composition including the number of formula unit Z. For example, the unit cell compositions of $\text{Li}_3\text{PO}_4$ could be $\text{Li}_6\text{P}_2\text{O}_8$ and $\text{Li}_{12}\text{P}_4\text{O}_{16}$ with Z=2 for the space group pnm21 (no. 31) and Z=4 for pnma (no. 62), respectively. This added information makes the computation of density and volumetric density possible for every material in the dataset.

To the best of our capacity, we have ensured that the reported structural information in OBELiX corresponds exactly to the same material for which the ionic conductivity was measured. We also filtered the dataset for exact duplicates and ensured that near duplicates were truly different materials. It is common for papers to report ionic conductivity measured elsewhere when synthesizing a material and vice versa for structural information. If not caught, this can lead to two entries with the exact same ionic conductivity, only one of which is the actual material for which it was measured.

The ICSD is a large database of experimental data in the form of crystal information files (CIF) that contain full crystal descriptions including atomic positions. Given that a significant portion of publications in this field have crystal information in the ICSD, we searched the database for all entries matching the lattice, parameters composition and associated publication. We found 234 exact matches with our entries, for which we obtained the CIFs. We also manually retrieved crystal information for 27 entries. Finally, we searched the ICSD and the Materials Project for structures that matched the space group and closely matched the composition ($\pm$ 0.05) and lattice parameters ($\pm$ 3\%) of our entries and found 60 additional CIF files (labeled as close matches). This forms a total of 321 entries with CIF information.

Because the ICSD is not an open database, we are not able to publish 292 of the CIF files and can only link our entries to their corresponding ICSD ID. However, to reach a broader audience, in agreement with the ICSD, we openly publish a set of 292 CIF files for which a normally distributed random noise with standard deviation 0.01 ($\epsilon \sim N(0,0.01)$) in fractional coordinates was added to the original atomic positions. This noise was added while making sure that the full symmetry of the crystal was preserved. We measured the effect of noise on model performance (see section \ref{sec:benchmark}) and found that it made little to no difference (see the SI for more details).

\subsection{Data Splits}

Experimental papers in this field often measure ionic conductivity for several variations of the same materials while changing the composition slightly. This can lead to multiple entries that are very similar and often have similar ionic conductivities. There are also several entries in our dataset that have the same composition, which may also lead to similar ionic conductivities. To avoid data leakage when testing machine learning models on OBELiX and to fairly compare new models in the future, we provide a split of the data where entries from the same paper or that have the same composition must be in the same set (training or testing). 

To obtain this split, we used a Monte Carlo method that moved groups of entries from one set to the other to minimize (1) the difference between the distribution of log ionic conductivity between the two sets and (2) the difference between their respective subsets containing CIF files. The algorithm  also ensured that the final test set represented between 20\% and 30\% of the data. The obtained distribution of log ionic conductivity in each set and subset is presented in Figure~\ref{fig:stats}a along with the proportion of each crystal family and space group. The test set represents 20.2\% of the full dataset and 20.9\% of the subset that has CIF files.

The distributions in log space of ionic conductivity for the two sets are very similar. Note that the entries plotted at $10^{-15}$ were reported as having a conductivity of ``less than $10^{-10}$'' without a quantitative value. The proportion of crystal families and space groups is also fairly similar between the two sets, except for space group 167, which is much more prevalent in the training set. This is due to the fact that a large group of entries (106) with space group 167 were either from the same paper or had the same composition. This meant that the entire group could not be split between the two sets without leaking either a paper or a composition.

The dataset contains 55 space groups, 4 of which are only in the test set. Figure~\ref{fig:stats}c shows the prevalence of the 55 different elements that are present in the dataset. All entries contain lithium (by design) and most of them contain oxygen. Phosphorus, lanthanum, sulfur and titanium follow as the most prevalent elements. Silver is the only element that is not found in the training set (it is only in the test set). 

We hope that by building the train and test splits carefully the performance of models on new data can be assessed more meaningfully.

\section{Benchmarks}
\label{sec:benchmark}

In this section, we benchmarked how well existing models perform on the new dataset. This evaluation is essential for determining whether these models can be effectively applied or if there is a need to develop new models better suited for the task.

We note that experimental data intrinsically embeds errors and uncertainty associated not only with various sources of measurement techniques but also with data extraction from figures and inconsistent labelling (e.g., bulk, grain boundary, or total ionic conductivity are often indistinguishably reported). Before assessing the performance of predictive models it makes sense to quantify the uncertainty (``performance'') of experimental data acquisition. Thankfully, our dataset contains 48 sets of compositions and space groups that have multiple entries, spanning a total of 122 entries. These entries and their corresponding ionic conductivities are plotted in Figure~\ref{fig:repeat_values}. The color represents the maximum difference in lattice parameters between any two entries of a same set. The maximum difference is of only 1.2\% for all sets, which gives us confidence that grouped materials are in fact the same. This means that these materials were synthesized and their ionic conductivity measured two or more times, most likely by different researchers. This represents a unique opportunity to quantify experimental uncertainty and reproducibility. The inset of Figure~\ref{fig:repeat_values} shows the distribution of log ionic conductivities with respect to the mean of each set of repeated materials. The root mean squared deviation from the set averages is of 0.63~log(S/cm) and the mean absolute deviation from the set medians is of 0.41~log(S/cm). The latter can be compared to the model's mean absolute error when predicting log ionic conductivity and represents its lower bound. Therefore, any model that would be reported as having lower MAE that that value would most likely be over-trained. 

\subsection{Baselines} 

We chose two standard machine learning models, a random forest (RF) and a multilayer perceptron (MLP), to train on the full dataset. 
\begin{itemize}
    \item RF, as an ensemble of decision trees, is robust to noisy data and provides feature importance insights, making it a strong baseline for structured datasets. 
    \item MLP, a neural network-based approach, captures complex nonlinear relationships, offering a comparison to deep learning-based methods.
\end{itemize}

We also tested five widely adopted graph neural networks developed specifically for materials science applications,  PaiNN\cite{schutt2021equivariant}, SchNet\cite{schtt2017schnet}, M3GNet\cite{chen2022universal}, SO3Net\cite{schutt2023schnetpack}, and CGCNN\cite{xie2018crystal} on the subset of the dataset that contains CIF files. These graph-based models, where each node represents an atom, effectively capture atomic interactions while preserving molecular invariance, enabling accurate material property predictions when trained on large datasets \cite{liu2024symmetry}. 
\begin{itemize}
    \item PaiNN enforces E(3)-equivariance, enabling accurate modeling of atomic interactions and force predictions.
\item SchNet learns continuous filter representations, making it effective for capturing atomic environments.
\item M3GNet integrates message passing with three-body interactions, improving property predictions for crystalline materials.
\item SO3Net leverages spherical harmonics to enhance equivariant representations for molecular and solid-state systems.
\item CGCNN models crystal structures directly as graphs, making it a strong baseline for learning structure-property relationships.
\end{itemize}

The RF and the MLP use the composition, space group and lattice parameters as inputs where the composition is a vector containing the occurrence ($\in \mathbb{R}$) of each element of the periodic table. The 3D geometric models use the crystal structure as their input and build different representations from that structure. The crystal structure contains the composition and space group information implicitly, but the models are not given that information explicitly. None of the model can take into account partial occupancy of the sites, therefore occupations are rounded to the nearest integer before being fed to the models.
    
\subsection{Setup} 
\label{sec:setup}

To optimize the training process and assess the stability of the models, we implemented a 5-fold cross-validation strategy. For hyperparameter optimization, we employed a grid search strategy across a predefined space of 100 randomly sampled hyperparameter sets for each model. This number was selected to strike a balance between comprehensive exploration of the hyperparameter space and computational feasibility. In the case of RF and MLP where training is extremely fast; all hyperparmeter sets were tested. The hyperparameter space was carefully designed for each model based on its unique architecture and requirements (see Table~\ref{table:hyperparams} in the SI for a complete list). For example, PaiNN's search space included parameters such as the cutoff distance, number of interactions, and batch size. 

We computed the mean absolute error (MAE) to evaluate the performance of each configuration. Specifically, the average validation MAE across all folds in the cross-validation process was used to assess each setup's effectiveness. The hyperparameter set that achieved the lowest average validation MAE was selected as the best-performing configuration. 

After choosing the best hyperparameters, each model was retrained on the entire training set and evaluated on the test set. This ensured that the results reflected the models' optimal performance. A detailed table of the selected hyperparameters for each model is included in the SI (Table ~\ref{table:hyperparams}). 

Pretraining can enhance model performance by initializing weights with knowledge from a large dataset, which is then fine-tuned on a smaller, task-specific dataset. During fine-tuning, the pretrained model is adapted to the target dataset. We pretrained PaiNN and SchNet on the Materials Project with a band gap prediction task. In this case we fixed the trained representation (PaiNN or SchNet) and trained the output model (an MLP followed by a pooling layer) on OBELiX. For M3GNet and CGCNN we use pretrained models that were available on their public repositories. The M3GNet model was trained on formation energy per atom whereas CGCNN was trained on Fermi energy both from the Materials Project. As recommended in their respective documentation, we fine-tuned the models by training all model parameters starting from the trained models. 

\begin{table}
\caption{Benchmarking of various ML models with and without pretraining.}
\label{table:mae_models}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcl}
\toprule
\textbf{Model} & \textbf{Cross-val. MAE} & \textbf{Test MAE} \\ 
\midrule
Experiment           &                   & 0.41 \\
\midrule
\multicolumn{2}{l}{Median pred.}                   & 2.16 (2.81) \\
\midrule
RF                   & 0.96 $\pm$ 0.15   & 1.54 (1.82) \\
MLP                  & 1.24 $\pm$ 0.26   & 1.64 (2.17) \\
\midrule
PaiNN                & 1.65 $\pm$ 0.21   & 2.88 \\
SchNet               & 1.79 $\pm$ 0.23   & 2.89 \\
M3GNet               & 1.89 $\pm$ 0.31   & 2.74 \\
SO3Net               & 2.02 $\pm$ 0.25   & 2.76 \\
CGCNN                & 1.87 $\pm$ 0.35   & 2.84 \\
\midrule
p-PaiNN              & 2.05 $\pm$ 0.25   & 2.69 \\
p-SchNet             & 2.17 $\pm$ 0.68   & 2.61 \\
p-M3GNet             & 1.81 $\pm$ 0.29   & 3.04 \\
p-CGCNN              & 1.84 $\pm$ 0.33   & 2.52 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}
%
\subsection{Discussion} 
Table~\ref{table:mae_models} presents our benchmarking results. The fine-tuned version of each model is denoted with the prefix "p-," such as p-PaiNN for the fine-tuned PaiNN model.

The two simple models, RF and MLP outperform all 3D geometric models both in the cross-validation and the test performance. There are two factors that could explain this result. First, the RF and the MLP used the full training set of 478 structures while the other models were limited to the subset of 254 entries that have CIF files. The performance of the RF and the MLP on the subset of the training set that has CIF files is indicated in parentheses in Table~\ref{table:mae_models} in order to be compared fairly with other models. Second, the geometric models use crystal information to infer properties of the crystal, but they not properly handle partial occupancies which, as discussed before, are very common in SSE materials and are present in about 3/4 of our CIF files. In order to use these models on our dataset we rounded occupancies to the nearest integers which can lead to important changes in the composition. 

To partly verify the above claim that dataset size and the presence of partial occupancy can explain the increased performance of the simple models, we retrained them on the subset of entries that have CIF files only. Doing so, the MAE of the MLP dropped to 3.3 while that of the RF was maintained at 1.86. Data size does seem to have a significant impact on the MLP and may explain the difference in performance between that model and the larger models. Random forest still performs well even given less data. Rounding compositions to the nearest integer on the other hand, had little effect on both the RF and the MLP. Rounding compositions is \textit{similar} to rounding site occupancy, but it does not have exactly the same effect. Nevertheless, it indicates that the absence of partial occupancy likely does not explain the difference in performance between the simple models and the more complex ones.

The 3D geometric models not only performed poorly compared to simple ML models using less structural information, but their performance on the test set was barely better or sometimes worse than predicting the median of the training set. This shows that these large models can easily overfit small experimental datasets which was also observed in other studies \cite{fung2021benchmarking}. The relatively large difference between the average validation MAE and the test set also shows the importance of carefully building leakage-free test sets.

Pretraining of 3D geometric models offers some marginal improvements for PaiNN, SchNet and CGCNN. As mentioned in Section~\ref{sec:setup}, the pretraining of PaiNN and SchNet restricts the trainable model size which may reduce accuracy while increasing generalizability. This would explain their slightly higher validation MAE and lower training MAE. A better pretrained representation might compensate for the reduced expressivity and increase both accuracy and generalizability, but a much more in-depth analysis of the possible pretraining labels and datasets would be required. In the case of CGCNN and M3GNet which were fine-tuned by allowing all model parameters to change, it is possible that the pretraining property used for CGCNN was ``closer'' (or more relevant) to ionic conductivity which allowed it to stay in the same weight ``basin'' and take advantage of the pretrained model's generalizability. It is important to bear in mind that the variability of the prediction accuracy is high in this small data regime as illustrated by the validation MAEs' standard deviations and that much of the difference between these models falls within that variability. Therefore one must be careful when interpreting the results.

Ultimately, the poor performance of models in Table~\ref{table:mae_models} highlights the need for domain specific models tailored for small data regimes. 

\section{Conclusion and Outlook}

In this paper, we presented, OBELiX, a domain-expert-curated dataset of 599 materials with experimental room temperature ionic conductivities, including 321 structures with full crystallographic information. We gathered these materials from existing databases and manually extracted data from the literature to build a consistent, easy-to-access database of solid-state electrolyte materials. We benchmarked several ML models and found that (1) existing architectures likely over-fit and are unable to perform well on our carefully engineered test set, and  (2) current geometric models cannot properly handle partial occupancies, which are very common in SSE materials. 

We hope that OBELiX will inspire the development of new ML methods specific to this task and serve as a reference point to train and test ionic conductivity models for the ML and computational materials science community in general, ultimately advancing solid-state battery technology. 

% \section{Acknowledgments}
% To be determined
\section{Data Availability}

All data is freely available on our public repository\footnote{\href{https://github.com/NRC-Mila/OBELiX}{github.com/NRC-Mila/OBELiX}} as a single csv or xlsx file accompanied by a set of 321 CIF files, including 291 with added random noise. 

\bibliography{references}
\bibliographystyle{icml2025}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\appendix

\newpage
\appendix
\onecolumn
\section{Supplemental Information}

Table \ref{table:mae_randomized} presents the cross-validation and test MAEs for the models trained on randomized atomic positions. The models were trained on the randomized data and tested on the original CIFs. Table~\ref{table:hyperparams} shows the best hyperparameter sets for each 3D model presented in Table~\ref{table:mae_models}

\begin{table}[h]
\caption{Performance of the 5 geometric models on the set that includes randomized atomic positions.}
\label{table:mae_randomized}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{Cross-validation MAE} & \textbf{Test MAE} \\
\midrule
PaiNN     & 2.03 $\pm$ 0.27 & 2.95 \\
SchNet    & 1.99 $\pm$ 0.22 & 2.78 \\
M3GNet    & 1.83 $\pm$ 0.29 & 2.91 \\
SO3Net    & 1.98 $\pm$ 0.23 & 2.79 \\
CGCNN     & 1.94 $\pm$ 0.42 & 2.95 \\
\bottomrule
% p-PaiNN   & 0 & 0 \\
% p-M3GNet  & 1.79 $\pm$ 0.26 & 3.12 \\
% p-CGCNN   & 1.81 $\pm$ 0.36 & 2.63 \\
% \midrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}
%
\begin{table}
\caption{The selected hyperparameters for the models.}
\label{table:hyperparams}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{l|l|l}
\toprule
\textbf{Model}     & \textbf{Hyperparameter}               & \textbf{Value}                      \\ \midrule
PaiNN              & \texttt{cutoff}                  & 5                                       \\
                   & \texttt{n\_interactions}          & 2                                        \\
                   & \texttt{n\_atom\_basis}           & 80                                       \\
                   & \texttt{batch\_size}              & 32                                       \\
                   & \texttt{max\_epochs}              & 100                                       \\
                   & \texttt{weight\_decay}           & 0.0001                                        \\
                   \midrule
Schnet             & \texttt{cutoff}                  & 5                                       \\
                   & \texttt{n\_interactions}          & 3                                        \\
                   & \texttt{n\_atom\_basis}           & 80                                       \\
                   & \texttt{batch\_size}              & 32                                       \\
                   & \texttt{max\_epochs}              & 100                                       \\ 
                   & \texttt{weight\_decay}           & 0.01                                        \\
                   \midrule
M3GNet             & \texttt{cutoff}                 & 5.0                                        \\
                   & \texttt{threebody\_cutoff}        & 5.0                                      \\
                   & \texttt{is\_intensive}          & True                                       \\
                   & \texttt{readout\_type}            & "set2set"                                \\
                   & \texttt{nblocks}                 & 3                                         \\
                   & \texttt{dim\_node\_embedding}     & 128                                      \\
                   & \texttt{dim\_edge\_embedding}     & 128                                      \\
                   & \texttt{units}                   & 64                                        \\ 
                   & \texttt{batch\_size}              & 35                                       \\
                   & \texttt{max\_epochs}              & 50                                        \\
                   & \texttt{lr}                      & 0.001                                    \\ 
                   & \texttt{weight\_decay}           & 0.01                                      \\  \midrule
SO3Net             & \texttt{cutoff}                 & 5.0                                       \\
                   & \texttt{is\_intensive}          & True                                       \\
                   & \texttt{nmax}                   & 2                                          \\
                   & \texttt{lmax}                   & 1                                          \\
                    & \texttt{target\_property}      & "graph"                                  \\
                   & \texttt{readout\_type}          & "set2set"                                \\
                   & \texttt{nblocks}                 & 3                                         \\
                   & \texttt{dim\_node\_embedding}     & 64                                      \\
                   & \texttt{nlayers\_readout}         & 3                                        \\ 
                   & \texttt{units}                   & 32                                       \\ 
                   & \texttt{batch\_size}              & 35                                       \\
                   & \texttt{max\_epochs}              & 80                                        \\
                   & \texttt{lr}                       & 0.001                                    \\ 
                   & \texttt{weight\_decay}           & 0                                      \\
                   \midrule
CGCNN              & \texttt{n\_conv}                 & 3                                        \\
                   & \texttt{n\_h}                     & 1                                       \\
                   & \texttt{atom\_fea\_len}           & 64                                      \\
                   & \texttt{h\_fea\_len}             & 64                                      \\
                   & \texttt{batch\_size}              & 35                                      \\ 
                   & \texttt{epochs}                  & 50                                        \\
                   & \texttt{lr}                      & 0.001                                   \\
                   & \texttt{weight\_decay}           & 0                                        \\
                   \bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\newpage
%\input{checklist}

\end{document}