\section{Related Work}
The ``false positive'' problem we investigate arises primarily due to the evaluation methods employed for assessing large language model (LLM) performance on mathematical tasks. Many existing approaches focus solely on comparing the final generated answers to the ground truth annotations. In these methods, the final answer is extracted either through predefined rules or using LLMs themselves, followed by normalization and comparison with the gold-standard solution. This evaluation strategy is efficient, inexpensive, and fully automated; however, it fails to account for the intermediate reasoning steps involved in generating the solution. Moreover, it is not applicable to tasks such as mathematical proofs, which do not have a single final answer. To overcome these limitations, some studies leverage powerful LLMs to compare the reasoning steps in generated solutions with reference answers or directly identify step errors within the reasoning path, in an attempt to evaluate the validity of mathematical reasoning \cite{he2023socreval, tyen2023llms, hao2024llm}. The effectiveness of this approach is heavily reliant on the capabilities of the LLM used, and it remains uncertain how reliably LLMs can detect reasoning flaws produced by strong LLMs themselves. Alternatively, other research has explored the use of formal proof systems for mathematical reasoning. Benchmarks such as MiniF2F \cite{zheng2021minif2f} and ProofNet \cite{azerbayev2023proofnet} utilize formal languages like Lean \cite{moura2021lean} to specify math problems, and LLMs are tasked with generating formal proofs, which can be rigorously checked by the formal system. While formal proofs inherently avoid the ``false positive'' issue present in natural language solutions, the translation of informal mathematical statements into formal systems remains a significant challenge, limiting the broader applicability of this approach.

Previous studies, such as \citet{hao2024llm} and \citet{zheng2024processbench}, have also highlighted the presence of ``false positives'' in LLM-generated mathematical solutions. A significant line of research focuses on improving the accuracy of reasoning steps through process supervision \cite{lightman2023let, setlur2024rewarding, luo2024improve}. For instance, \citet{lightman2023let} demonstrated that training on explicitly annotated flaws in reasoning paths could enhance the performance of reward models, leading to improved accuracy on mathematical benchmarks. Works such as \citet{luong2024reft} and \citet{shao2024deepseekmath}, employ reinforcement learning techniques to generate higher-quality reasoning paths derived from final answer annotations, which are then utilized to train better policy models in a self-improving way. In addition, studies like \citet{golovneva2022roscoe}, \citet{prasad2023receval} and \citet{xia2024evaluating} have proposed filtering and rescoring strategies, as well as novel metrics, to identify erroneous reasoning steps and mitigate the ``false positive'' problem. While \citet{snell2024scaling} investigated the inference time scaling of LLMs on mathematical problems, their work did not consider the impact of ``false positives''. Moreover, \citet{stroebl2024inference} studied how the ``false positive'' problem affects inference time scaling in the coding domain, showing that flawed verifiers lead to a decrease in true accuracy as more computational resources are allocated, due to the growing rate of ``false positives'' in the generated solutions.