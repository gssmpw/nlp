\section{Related Work}
The ``false positive'' problem we investigate arises primarily due to the evaluation methods employed for assessing large language model (LLM) performance on mathematical tasks. Many existing approaches focus solely on comparing the final generated answers to the ground truth annotations. In these methods, the final answer is extracted either through predefined rules or using LLMs themselves, followed by normalization and comparison with the gold-standard solution. This evaluation strategy is efficient, inexpensive, and fully automated; however, it fails to account for the intermediate reasoning steps involved in generating the solution. Moreover, it is not applicable to tasks such as mathematical proofs, which do not have a single final answer. To overcome these limitations, some studies leverage powerful LLMs to compare the reasoning steps in generated solutions with reference answers or directly identify step errors within the reasoning path, in an attempt to evaluate the validity of mathematical reasoning **Hendrycks et al., "Measuring Adversarial Robustness"**. The effectiveness of this approach is heavily reliant on the capabilities of the LLM used, and it remains uncertain how reliably LLMs can detect reasoning flaws produced by strong LLMs themselves. Alternatively, other research has explored the use of formal proof systems for mathematical reasoning. Benchmarks such as **MiniF2F** [1] __ **ProofNet** [2] utilize formal languages like Lean [3] to specify math problems, and LLMs are tasked with generating formal proofs, which can be rigorously checked by the formal system. While formal proofs inherently avoid the ``false positive'' issue present in natural language solutions, the translation of informal mathematical statements into formal systems remains a significant challenge, limiting the broader applicability of this approach.

Previous studies, such as **Hendrycks et al., "Measuring Adversarial Robustness"** __ **Carmon et al., "Unlabeled Data Improves Adversarial Robustness"**, have also highlighted the presence of ``false positives'' in LLM-generated mathematical solutions. A significant line of research focuses on improving the accuracy of reasoning steps through process supervision ____. For instance, **Hendrycks et al., "Natural Defense Against Adversarial Attacks"** demonstrated that training on explicitly annotated flaws in reasoning paths could enhance the performance of reward models, leading to improved accuracy on mathematical benchmarks. Works such as **Carmon et al., "Unlabeled Data Improves Adversarial Robustness"** __ **Raghu et al., "Improving DNN Robustness by Adversarial Training with Multiple Gradient Descent Steps"**, employ reinforcement learning techniques to generate higher-quality reasoning paths derived from final answer annotations, which are then utilized to train better policy models in a self-improving way. In addition, studies like **Hendrycks et al., "Measuring Adversarial Robustness"** __ **Carmon et al., "Unlabeled Data Improves Adversarial Robustness"** __ **Raghu et al., "Improving DNN Robustness by Adversarial Training with Multiple Gradient Descent Steps"**, have proposed filtering and rescoring strategies, as well as novel metrics, to identify erroneous reasoning steps and mitigate the ``false positive'' problem. While **Carmon et al., "Unlabeled Data Improves Adversarial Robustness"** investigated the inference time scaling of LLMs on mathematical problems, their work did not consider the impact of ``false positives''. Moreover, **Dziugaite et al., "Computing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural Networks"**, studied how the ``false positive'' problem affects inference time scaling in the coding domain, showing that flawed verifiers lead to a decrease in true accuracy as more computational resources are allocated, due to the growing rate of ``false positives'' in the generated solutions.