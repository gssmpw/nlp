

\newpage

\section{Omitted Results}
\subsection{Additional Results for Trainability}
\label{app:additional_results_trainability}
\subsubsection{Extended Results for various conditions}
\label{app:extended_results_trainability}

\input{figures/exp_trainability_ablation}

We conducted additional trainability experiments on various activation functions, including Identity, RReLU, Sigmoid, and Tanh, as presented in Figure \ref{fig:trainability_ablation}. As discussed in \citet{dohare2024loss, lewandowski2024plastic}, linear networks do not suffer from plasticity loss. Interestingly, stochastic activation functions such as RReLU, DropReLU, and AID demonstrated strong trainability across different settings. This observation suggests that studying the properties of these activation functions may contribute to preserving plasticity, providing a promising direction for future research.

\newpage
\subsubsection{Analysis of Metrics Contributing to Plasticity Loss}
\label{app:metrics_pl}





The precise cause of plasticity loss remains unclear, but several plausible indicators have been proposed to assess its impact. Among them, the dormant neuron ratio \cite{sokar2023dormant} and effective rank \cite{kumar2020implicit, lyle2022understanding} have been widely studied as key metrics. Recently, average sign entropy \cite{lewandowski2024plastic} of pre-activation values has been introduced as an additional measure. Each metric is computed as follows:

\begin{itemize}
    \item \textbf{Dormant Neuron Ratio.} \citet{sokar2023dormant} introduced dormant neurons as a measure for the reduced expressivity of the neural network. A neuron is considered $\tau$-dormant, if its normalized activation score is lower than $\tau$. The normalized activation score is computed as follow:
    \[s_i^l = \frac{\mathbb E_{\mathbf{x}\in D}\left[|h_i^l(\mathbf{x})|\right]}{\frac{1}{H^l}\sum_{k\in [H^l]}\mathbb E_{\mathbf{x}\in D}\left[|h_k^l(\mathbf{x})|\right]}\]
    where $s_i^l$ is normalized activation score for $i$-th neuron in $l$-th layer. $\mathbf{x}\in D$ is sample from input distribution, $H^l$ is the number of neurons in $l$-th layer, and $h_i^l(\cdot)$ is post-activation function. Note that $l$ does not include final layer of the network. The dormant neuron ratio is computed as the proportion of neurons with $\tau=0$.
    \item \textbf{Average Unit Sign Entropy.} \citet{lewandowski2024plastic} proposed unit sign entropy as a generalized metric encompassing unit saturation \citep{abbas2023loss} and unit linearization \citep{lyle2024disentangling}. It is defined as follows:
    \[\text{Unit Sign Entropy}(x) = \mathbb E_{p(x)} [sgn(h(x))]\]

    where $p(x)$ is a distribution of inputs to the network, $h(\cdot)$ is pre-activation values of according unit, and $sgn(\cdot)$ is sign function. To obtain the final metric, we compute the mean across all units, yielding the average unit sign entropy.
    \item \textbf{Effective Rank.} Previous studies \cite{kumar2020implicit, lyle2022understanding} present the effective rank of the output matrix after the penultimate layer is closely related to plasticity loss. While several methods exist for computing effective rank, we follow srank \cite{kumar2020implicit}:
\[\text{srank}_\delta(\Phi) = \min_k\frac{\sum_{i=1}^k\sigma_i(\Phi)}{\sum_{j=1}^n\sigma_j(\Phi)}\geq1-\delta\]
where $\Phi$ is feature matrix, and $\{\sigma_i(\Phi)\}_{i=1}^n$ is singular values of $\Phi$ sorted in descending order. We set $\delta=0.01$ as default.

\end{itemize}

\newpage
\input{figures/exp_trainability_metrics}
We evaluate these metrics for vanilla, Dropout, and AID under experimental settings characterized by severe plasticity loss. Specifically, we analyze permuted MNIST and random label MNIST with the Adam optimizer (lr = 0.001). The results are presented in Figure \ref{exp_trainability_metrics}. Interestingly, Dropout had no effect on random label MNIST and provided only a slight improvement in permuted MNIST. While both Dropout and Vanilla models cause a significant portion of neurons to become dormant, AID effectively prevents neuron saturation. The Average Sign Entropy further supports this observation, as AID maintains consistently higher entropy, indicating a more diverse and active neuron distribution. Lastly, the Effective Rank plot demonstrates that AID preserves representational capacity throughout training, whereas both Dropout and Vanilla models experience a rapid decline. These findings highlight AID’s ability to mitigate plasticity loss and sustain model adaptability over training.

\newpage
\subsubsection{Analysis on Preactivation Distribution Shift of Dropout and AID}
\label{app:omitted_results_preactivation}
\input{figures/exp_preact}


In this section, we present additional experimental results related to Section \ref{sec:dropout}. Specifically, we analyze the preactivation distribution of Dropout and AID. The results presented in this section follow the Random label MNIST experimental setup as those in Section \ref{sec:dropout}, with further details provided in Appendix \ref{app:loss_pl_dropout_trainability}. We trained an 8-layer MLP on the random label MNIST and visualized the preactivation of the penultimate layer in Figure \ref{exp_dropout_preact}. 

Figure \ref{exp_dropout_preact} (left and middle) illustrate the pre-activation distributions of Dropout and AID, for the first and tenth tasks, respectively. According to prior research \cite{lyle2024disentangling}, preactivation distribution shift is one of the primary causes of plasticity loss in non-stationary settings. Dropout, which suffers from plasticity loss, exhibits a drastic preactivation shift, aligning well with previous findings. However, AID, despite maintaining plasticity effectively, undergoes a similarly significant preactivation shift as Dropout. This observation contradicts existing results, which indicate that the initial preactivation shift during training may not necessarily be the primary cause of plasticity loss.

Although both Dropout and AID experience preactivation distribution shifts, the extent of the shift in Dropout becomes more severe over the course of training compared to AID. Figure \ref{exp_dropout_preact} (right) compares the preactivation distributions of Dropout and AID in the 30th task. As training progresses, Dropout's distribution shifts considerably, with the Q2 value approaching -2500, whereas AID maintains a distribution much closer to zero. These results indicate that while both Dropout and AID undergo distribution shifts, AID exhibits a bounded shift, preventing excessive plasticity loss. In contrast, Dropout experiences an unbounded distribution shift, leading to substantial plasticity degradation. This finding suggests that simply applying different dropout probabilities across different stages, as in AID, can help mitigate preactivation distribution shifts.





\newpage
\subsection{Additional Results for Class-Incremental}
\label{app:omitted_results_CI}
\input{figures/exp_class_incremental_ablation}
\input{figures/exp_class_incremental_ablation_full}

In the main section of the paper, we primarily presented results for the difference from full reset in class-incremental experiment. To provide a more detailed analysis, we include additional plots in \cref{exp_class_incremental_ablation,exp_class_incremental_ablation_full}, illustrating both the accuracy for previously learned classes and the accuracy across all classes. These results further highlight that AID consistently outperforms the other proposed approaches, demonstrating its effectiveness in maintaining plasticity while mitigating performance degradation in class-incremental learning.

\newpage
\subsection{Additional Results for Reinforcement Learning}
\label{app:omitted_results_rl}
\subsubsection{Atari 2600 RAW Scores by Games}
\input{figures/exp_RL}

In the reinforcement learning experiments, we sweep a relatively high coefficient for AID ($0.99, 0.999$), consistent with previous trainability experiments where a high coefficient was found to be optimal for continuously reducing training loss. While AID demonstrated advantages in most games, we observed a slight performance drop in certain cases, such as Pong and Qbert, where plasticity loss was not a significant issue even at high replay ratios. Further investigation is needed to understand this behavior, including extended training on these specific games. Additionally, our experiments were limited to high replay ratios; future research could explore alternative RL algorithms, different environments, or long-term learning settings to gain further insights.

\newpage
\subsubsection{Impact of AID on Effective Rank}

\input{figures/RL_effective_rank}
Following the approach in Section \ref{app:metrics_pl}, we analyze the feature rank for three games—BeamRider, Asterix, and DemonAttack—where AID demonstrated significant performance improvements. 
As shown in Figure \ref{exp_RL_feature_rank}, the feature rank of the vanilla model steadily declines throughout training, indicating a loss of representational diversity. In contrast, AID consistently maintains a higher feature rank, suggesting that it helps preserve network plasticity.



\subsection{Learning Curves for Standard Supervised Learning}
\label{app:learning_curve_sl}
\input{figures/exp_SL}

The Figure \ref{exp_SL} present training and test loss of experiment in Section \ref{sec:exp_gen}. We observe that AID more effectively closes the generalization gap compared to L2 regularization and Dropout, both of which are commonly used to address overfitting. This result suggests that AID is not merely a technique for mitigating plasticity loss but a more general-purpose methodology that enhances model robustness across various tasks.
