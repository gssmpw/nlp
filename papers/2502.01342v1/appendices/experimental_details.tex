

\section{Experimental Details}
\label{app:experimental_details}

\subsection{Loss of plasticity with Dropout}
\label{app:loss_pl_dropout}

\subsubsection{Trainability for Dropout}
\label{app:loss_pl_dropout_trainability}
We employed an 8-layer MLP featuring 512 hidden units and trained it on 1400 samples from the MNIST dataset.
The model is trained for 50 different tasks, with each task running for 100 epochs.
To evaluate subnetwork plasticity, we extracted 10 subnetworks at the conclusion of each epoch, training these on new tasks for an additional 100 epochs and then calculated the mean final accuracy.
Adam optimizer was utilized for model optimization.


\subsubsection{Warm-start}
We used the ResNet-18 architecture described in Appendix \ref{app:generalizability}.
In the warm-start scenario, the model was pre-trained on 10\% of the CIFAR100 dataset for 1,000 epochs and continued training on the full dataset for 100 epochs, with the optimizer reset at the start of full dataset training.
In the cold-start scenario, the model was trained on the full dataset for 100 epochs. Adam optimizer with learning rate $0.001$ was utilized.


\subsection{Trainability}
\label{app:trainability}
\textbf{Permuted MNIST. }
We followed the setup of \cite{dohare2024loss}.
It consists of a total of 800 tasks that 60,000 images are fed into models only once with 512 batch size.
In the beginning of each task, the pixel of images are permuted arbitrarily.
We trained fully connected neural networks with three hidden layers.
Each hidden layer has 2,000 units and followed by ReLU activaiton function.

\textbf{Random Label MNIST. }
We conducted a variant of \cite{kumar2023maintaining}.
It consists of a total of 200 tasks that 1,600 images are fed into models with 64 batch size.
In the beginning of each task, the label class of images are changed to other class arbitrarily.
We trained fully connected neural networks with three hidden layers 100 epochs per each task.
Each hidden layer has 2,000 units and followed by ReLU activaiton function.


\subsection{Generalizability}
\label{app:generalizability}
We conducted experiments on CIFAR-10, CIFAR-100 \cite{krizhevsky2009learning}, and TinyImageNet \cite{le2015tiny} datasets using a 4-layer CNN, ResNet-18, and VGG-16, respectively, to evaluate the effectiveness of AID across different model architectures and datasets.
We provide detailed model architecture below:
\begin{itemize}
    \item \textbf{CNN}: We employed a convolutional neural network (CNN), which is used in relatively small image classification.
    The model includes two convolutional layers with a $5\times5$ kernel and 16 channels and max-pooling layer is followed after activation function.
    The two fully connected layers follow with 100 hidden units. 
    % residual block에도 잘 적용되는지 확인하기 위해 modern architecture인 resnet-18을 실험에 사용 
    \item \textbf{Resnet-18} \citep{he2016deep}: We utilized ResNet-18 to examine how well AID integrates with modern deep architectures featuring residual connections.
    Following \cite{lee2024slow}, the stem layers were removed to accommodate the smaller image size of the dataset. Additionally, a gradient clipping threshold of 0.5 was applied to ensure stable training.
    \item \textbf{VGG-16} \citep{simonyan2014very}: We adopted VGG-16 with batch normalization to investigate whether AID adapts properly in large-size models. The number of hidden units of the classifiers was set to 4096 without dropout.
\end{itemize}

In addition, we replaced the max-pooling layer with an average-pooling layer for methods such as Fourier activation, DropReLU, and AID, where large values may not necessarily represent important features.
Next, we provide the detailed experimental settings below.

\textbf{Continual Full. } Similar to the setup provided by \cite{shen2024step}, the entire data is randomly split into 10 chunks.
The training process consists of 10 stages and the model gains access up to the $k$-th chunk in each stage $k$.
In each stage, the dataset is fed into models with a batch size of 256.
We trained the model for 100 epochs per each stage, and we reset the optimizer when each stage starts training.

\textbf{Continual Limited. } This setting follows the same configuration as continual full, with the key distinction that the model does not retain access to previously seen data chunks.
At each stage, the model is trained only on the current chunk, without revisiting earlier data, simulating real-world constraints such as memory limitations and privacy concerns.

\textbf{Class-Incremental. } For CIFAR100 and TinyImageNet, the dataset was divided into 20 class-based chunks, with new classes introduced incrementally at each stage.
Unless otherwise specified, test accuracy is evaluated based on the corresponding classes encountered up to each stage.
The rest of the setup, including the batch size, and training epochs per stage, follows the Continual Full setting.










\subsection{Reinforcement Learning}
\label{app:reinforcement_learning}
To evaluate whether AID enhances sample efficiency on reinforcement learning, we conducted experiments on 17 Atari games from the Arcade Learning Environment \cite{bellemare2013arcade}, selected based on prior studies \cite{kumar2020implicit,sokar2023dormant}. We trained a DQN model following \citet{mnih2015human} using the CleanRL framework \cite{huang2022cleanrl}. The replay ratio was set to 1, as adopted in \citet{sokar2023dormant, elsayed2024weight}. We followed the hyperparameter settings for environment from \citet{sokar2023dormant}, with details provided in Table \ref{tab:hyperparameter_rl_env}.



\begin{table}[h]
    \centering
    \caption{Hyperparameters used in reinforcement learning environment}
    \begin{tabular}{l r}
        \toprule
        \textbf{Parameter} & \textbf{Value} \\
        \midrule
        Optimizer & Adam \cite{kingma2014adam} \\
        Optimizer: $\epsilon$ & $1.5\mathrm{e}-4$ \\
        Optimizer: Learning rate & $6.25\mathrm{e}-4$ \\
        \midrule
        Minimum $\epsilon$ for training & $0.01$ \\
        Evaluation $\epsilon$ & $0.001$\\
        Discount Factor $\gamma$ & $0.99$ \\
        Replay buffer size & $10^6$ \\
        Minibatch size & $32$ \\
        Initial collect steps & $20000$ \\
        Training iterations & $10$ \\
        Training environment steps per iteration & $250K$ \\
        Updates per environment step (Replay Ratio) & $1$ \\
        Target network update period & $2000$\\
        Loss function & Huber Loss \cite{huber1992robust} \\
        
        \bottomrule
    \end{tabular}
    \label{tab:hyperparameter_rl_env}
\end{table}







\subsection{Standard Supervised Learning}
For the same model architecture and dataset used in the generalizability experiments, we trained with Adam optimizer for 200 epochs, applying learning rate decay at the 100th and 150th epochs. The initial learning rate was set to $0.001$ and was reduced by a factor of $10$ at each decay step.



\subsection{Hyperparameter Search Space}
\label{app:hyperparameter_seach_space}
We present the hyperparameter search space considered for each experiment in Table \ref{tab:hyperparameter_search_space}.
Without mentioned, we performed a sweep over 5 different seeds for all experiments, except for VGG-16 model on the TinyImageNet dataset, where we used only 3 seeds due to computational cost.
\cref{tab:hyperparameter_permuted_mnist,tab:hyperparameter_random_label_mnist,tab:hyperparameter_continual_full,tab:hyperparameter_continual_limited,tab:hyperparameter_class_incremental,tab:hyperparameter_rl,tab:hyperparameter_sl} shows the best hyperparameter set that we found in various experiments. 

\input{tables/hyperparameter_search_space}
\input{tables/hyperparameter_permuted_mnist}
\input{tables/hyperparameter_random_label_mnist}
\input{tables/hyperparameter_continual_full}
\input{tables/hyperparameter_continual_limited}
\input{tables/hyperparameter_class_incremental}
\newpage
\input{tables/hyperparameter_rl}
\input{tables/hyperparameter_sl}
