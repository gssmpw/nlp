

\section{Proof of Theorem \ref{Thm:1}}
\label{app:proof_1}
\begin{proof}
    We clarify that we follow a similar procedure to the one demonstrated during the proof by \citet{liang2021drop}, which employs a comparable methodology.
    
    Let the  $m$ input data points and true labels be $\mathbf{x}_1, \mathbf{x}_2, \dots \mathbf{x}_m$ and $\mathbf{y}_1, \mathbf{y}_2, \dots \mathbf{y}_m$ ($\mathbf{x}_i, \mathbf{y}_i \in \mathbb R^n$). For simplicity, let weight matrices be  $W_1, W_2 \in \mathbb R^{n\times n}$ and they do not have bias vectors. If we assume that this model uses $AID_p$, using notation in Equation \eqref{eq:AID_expansion}, the prediction of the model $\hat{\mathbf{y}}_i$ can be expressed as below:
    \[\hat{\mathbf{y}}_i = \mathbf{W}_2\left(\mathbf{I}-\mathbf{P}+(2\mathbf{P}-\mathbf{I})r\right)(\mathbf{W}_1\mathbf{x}_i)\]

    For testing, using $\mathbb E[\mathbf{P}] = p\mathbf{I}$, we get $\hat{\mathbf{y}}_i = \mathbf{W}_2((1-p)\mathbf{I} + (2p-1)r)(\mathbf{W}_1\mathbf{x}_i)$. Here, $\mathbf{I}$, $\mathbf{P}$, $r(\cdot)$ and $r_p(\cdot)$ refer to the identity matrix, Bernoulli mask, ReLU and modified leaky ReLU function, respectively, as mentioned in Property \ref{property:2}. 

    Then, the training process can be formally described as below:

    \[\min_{\mathbf{W}_1,\mathbf{W}_2}\sum_{i=1}^m \mathbb E\left[\|\mathbf{W}_2\left(\mathbf{I}-\mathbf{P}+(2\mathbf{P}-\mathbf{I})r\right)(\mathbf{W}_1\mathbf{x}_i)- \mathbf{y}_i\|_2^2\right]\]

    For simplicity, the input vector and label will be denoted as $(\mathbf{x}, \mathbf{y})$ in the following proof. Let $\mathbf{D}\doteq diag((\mathbf{W}_1\mathbf{x}>0))$ be the diagonal matrix, where $(\mathbf{W}_1\mathbf{x}>0)$ is $0$-$1$ mask vector, which means $ r(\mathbf{W}_1\mathbf{x}) = \mathbf{D}\mathbf{W}_1\mathbf{x}$. To reduce the complexity of the equations, let us denote $\mathbf{S}$, $\mathbf{S}_p$ and $\mathbf{v}$ as below:
    \begin{align*}
    \mathbf{S}&\doteq \mathbf{I} - \mathbf{P} + (2\mathbf{P}-\mathbf{I})\mathbf{D} \\
    \mathbf{S}_p &\doteq (1- p)\mathbf{I} + (2p-1)\mathbf{D} \\ \mathbf{v} &\doteq \mathbf{W}_1\mathbf{x}
    \end{align*}

    Note that $S$ and $S_p$ are diagonal matrix. Let the $\text{tr}$ be trace of matrix and $\text{vec}(\cdot)$ be the function that makes elements of diagonal matrix to the vector. Then, we can expand the objective function as below:
    
    \begin{align}
        \mathbb{E}\big[\|\mathbf{W}_2 \left(\mathbf{I} - \mathbf{P} + (2\mathbf{P} - \mathbf{I})r\right)(\mathbf{W}_1\mathbf{x}) - \mathbf{y}\|_2^2 \big] 
        &= \mathbb E[\|\mathbf{W}_2 \mathbf{S}\mathbf{v} - \mathbf{y}\|_2^2] \label{eq:AID_objective}\\
        &= \textcolor{green!60!black}{\mathbb{E}\big[ \text{tr}(\mathbf{W}_2 \mathbf{S} \mathbf{v} \mathbf{v}^\top \mathbf{S} \mathbf{W}_2^\top) \big]}  - 2\text{tr}(\mathbf{W}_2 \mathbf{S}_p \mathbf{v} \mathbf{y}^\top) + \text{tr}(\mathbf{y} \mathbf{y}^\top) \nonumber
    \end{align}

    
    \begin{align*}
        \text{where, }&\textcolor{green!60!black}{\mathbb{E}\big[\text{tr}(\mathbf{W}_2 \mathbf{S} \mathbf{v} \mathbf{v}^\top \mathbf{S} \mathbf{W}_2^\top) \big]} \\
        &= \mathbb{E}\big[\text{tr}(\text{vec}(\mathbf{S})\text{vec}(\mathbf{S})^\top \text{diag}(\mathbf{v}) \mathbf{W}_2^\top \mathbf{W}_2 \text{diag}(\mathbf{v}))\big] \\
        &= \text{tr}(\mathbb{E}[\text{vec}(\mathbf{S})\text{vec}(\mathbf{S})^\top] \text{diag}(\mathbf{v}) \mathbf{W}_2^\top \mathbf{W}_2 \text{diag}(\mathbf{v})) \\
        &= \text{tr}(\mathbb{E}[\text{vec}(\mathbf{S}_p)\text{vec}(\mathbf{S}_p)^\top + \text{vec}(\mathbf{S})\text{vec}(\mathbf{S})^\top - \text{vec}(\mathbf{S}_p)\text{vec}(\mathbf{S}_p)^\top] \text{diag}(\mathbf{v}) \mathbf{W}_2^\top \mathbf{W}_2 \text{diag}(\mathbf{v})) \\
        &= \text{tr}\big((\text{vec}(\mathbf{S}_p)\text{vec}(\mathbf{S}_p)^\top + \text{diag}((\mathbb{E}[(1-p_i + (2p_i-1)d_i)^2 - (1-p + (2p-1)d_i)^2])_{i=1}^n)) \text{diag}(\mathbf{v}) \mathbf{W}_2^\top \mathbf{W}_2 \text{diag}(\mathbf{v})\big) \\
        &= \text{tr}\big((\text{vec}(\mathbf{S}_p)\text{vec}(\mathbf{S}_p)^\top + \text{diag}((p(1-p)(1-2d_i)^2)_{i=1}^n)) \text{diag}(\mathbf{v}) \mathbf{W}_2^\top \mathbf{W}_2 \text{diag}(\mathbf{v})\big) \\
        &= \text{tr}\big((\text{vec}(\mathbf{S}_p)\text{vec}(\mathbf{S}_p)^\top + p(1-p)(1-2\mathbf{D})^2) \text{diag}(\mathbf{v}) \mathbf{W}_2^\top \mathbf{W}_2 \text{diag}(\mathbf{v})\big).
    \end{align*}
    

    Here, $p_i$ and $d_i$ are $(i,i)$ element of $\mathbf{P}$ and $\mathbf{D}$, respectively. Similarly, we expand the objective function when the activation function is modified leaky ReLU, $r_p(\cdot)$:

\begin{align}
    \|\mathbf{W}_2 r_p(\mathbf{W}_1 \mathbf{x}) - \mathbf{y}\|_2^2
    &= \|\mathbf{W}_2 \big((1-p)\mathbf{I} + (2p-1)\mathbf{D}\big)(\mathbf{W}_1 \mathbf{x}) - \mathbf{y}\|_2^2 \label{eq:mlrelu_objective}\\
    &= \|\mathbf{W}_2 \mathbf{S}_p \mathbf{v} - \mathbf{y}\|_2^2 \nonumber\\
    &= \textcolor{red!70!black}{\text{tr}(\mathbf{W}_2 \mathbf{S}_p \mathbf{v} \mathbf{v}^\top \mathbf{S}_p \mathbf{W}_2^\top)} 
       - 2 \text{tr}(\mathbf{W}_2 \mathbf{S}_p \mathbf{v} \mathbf{y}^\top) 
       + \text{tr}(\mathbf{y} \mathbf{y}^\top) \nonumber
\end{align}


\begin{align*}
    \text{where, }\textcolor{red!70!black}{\text{tr}(\mathbf{W}_2 \mathbf{S}_p \mathbf{v} \mathbf{v}^\top \mathbf{S}_p \mathbf{W}_2^\top)}
    &= \text{tr}(\mathbf{S}_p \mathbf{v} \mathbf{v}^\top \mathbf{S}_p \mathbf{W}_2^\top \mathbf{W}_2) \\
    &= \text{tr}(\text{diag}(\mathbf{v}) \text{vec}(\mathbf{S}_p)\text{vec}(\mathbf{S}_p)^\top \text{diag}(\mathbf{v}) \mathbf{W}_2^\top \mathbf{W}_2) \\
    &= \text{tr}(\text{vec}(\mathbf{S}_p)\text{vec}(\mathbf{S}_p)^\top \text{diag}(\mathbf{v}) \mathbf{W}_2^\top \mathbf{W}_2 \text{diag}(\mathbf{v})).
\end{align*}

    To determine the relationship between the two objective functions \eqref{eq:AID_objective} and \eqref{eq:mlrelu_objective}, we subtract one equation from the other:

\begin{align}
    &\mathbb{E}\big[\|\mathbf{W}_2 \big(\mathbf{I} - \mathbf{P} + (2\mathbf{P} - \mathbf{I})r\big)(\mathbf{W}_1 \mathbf{x}) - \mathbf{y}\|_2^2 \big] 
    - \|\mathbf{W}_2 \big((1-p)\mathbf{I} + (2p-1)\mathbf{D}\big)(\mathbf{W}_1 \mathbf{x}) - \mathbf{y}\|_2^2 \nonumber \\
    &= \textcolor{green!60!black}{\mathbb{E}\big[\text{tr}(\mathbf{W}_2 \mathbf{S} \mathbf{v} \mathbf{v}^\top \mathbf{S} \mathbf{W}_2^\top) \big]} 
    - \textcolor{red!70!black}{\text{tr}(\mathbf{W}_2 \mathbf{S}_p \mathbf{v} \mathbf{v}^\top \mathbf{S}_p \mathbf{W}_2^\top)} \nonumber \\
    &= \text{tr}\big(p(1-p)(1-2\mathbf{D})^2 \text{diag}(\mathbf{v}) \mathbf{W}_2^\top \mathbf{W}_2 \text{diag}(\mathbf{v})\big) \nonumber \\
    &= p(1-p) \text{tr}\big(\mathbf{W}_2 \text{diag}(\mathbf{v})(\mathbf{I} - 2\mathbf{D})(\mathbf{I} - 2\mathbf{D})^\top \text{diag}(\mathbf{v})^\top \mathbf{W}_2^\top\big) \nonumber \\
    &= p(1-p)  \|\mathbf{W}_2 (\mathbf{I} - 2\mathbf{D}) \text{diag}(\mathbf{W}_1 \mathbf{x})\|_2^2\nonumber \\
    &\geq \frac{p(1-p)}{n} \|\mathbf{W}_2 (\mathbf{I} - 2\mathbf{D}) \mathbf{W}_1 \mathbf{x}\|_2^2  \label{eq:wrong_on_dropact} 
 \\
    &= \frac{p(1-p)}{n(2p-1)^2} \|\mathbf{W}_2 \mathbf{W}_1 \mathbf{x} - 2\mathbf{W}_2 r_p(\mathbf{W}_1 \mathbf{x})\|_2^2 \label{eq:I-2D} \\
    &= \frac{4p(1-p)}{n(2p-1)^2} \left\|\mathbf{W}_2 \left(\frac{1}{2}\mathbf{W}_1 \mathbf{x}\right) - \mathbf{W}_2 r_p(\mathbf{W}_1 \mathbf{x})\right\|_2^2 \nonumber
\end{align}

    


    To derive Equation \eqref{eq:I-2D}, we utilize the definition of $\mathbf{S}_p$ introduced at the beginning of the proof:

    \begin{align*}
    \mathbf{S}_p &= (1-p)\mathbf{I} + (2p-1)\mathbf{D} \\
    \implies \mathbf{D} &= \frac{1}{2p-1}\big(\mathbf{S}_p - (1-p)\mathbf{I}\big) \\
    \implies \mathbf{I} - 2\mathbf{D} &= \mathbf{I} - \frac{2}{2p-1}\big(\mathbf{S}_p - (1-p)\mathbf{I}\big) \\
    &= \frac{(2p-1)\mathbf{I} - 2\mathbf{S}_p + 2(1-p)\mathbf{I}}{2p-1} \\
    &= \frac{\mathbf{I} - 2\mathbf{S}_p}{2p-1}
\end{align*}



    Since $r_p(\cdot) = \mathbf{S}_p(\cdot)$, we get Equation \eqref{eq:I-2D}. Finally, we get the relationship between these two objective functions:

    \begin{align*}
    \mathbb{E}\big[\|\mathbf{W}_2 \text{AID}_p(\mathbf{W}_1 \mathbf{x}) - \mathbf{y}\|_2^2 \big] 
    &\geq \|\mathbf{W}_2 r_p(\mathbf{W}_1 \mathbf{x}) - \mathbf{y}\|_2^2 \\
    &\quad + \frac{4p(1-p)}{n(2p-1)^2} \|\mathbf{W}_2 \big(\frac{1}{2}\mathbf{W}_1 \mathbf{x}\big) - \mathbf{W}_2 r_p(\mathbf{W}_1 \mathbf{x})\|_2^2.
\end{align*}


    This means that objective function of AID gives the upper bound of the optimization problem when we use modified leaky ReLU and penalty term. The penalty term means that weight matrices $W_1, W_2$ are trained to closely resemble the behavior of the linear network. As $p$ approaches 0.5, the regularization effect becomes stronger, which intuitively aligns with the fact that AID is equivalent to a linear network when $p=0.5$. Since $\left(\mathbb E[\|W_2AID_p(W_1x)-y\|_2^2] \rightarrow 0 \right) \implies y =W_2r_p(W_1x) = W_2(\frac{1}{2}W_1x)$, we confirm that $AID$ has a regularizing effect that constrains the network to behave like a linear network\footnote{We identified an error made by \citet{liang2021drop} while deriving Equation \eqref{eq:wrong_on_dropact}, and by correcting it with an inequality, the claim becomes weaker compared to the original proof. However, we still assert that the regularization effect remains valid.}.

    
    
\end{proof}

