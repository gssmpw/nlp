

\section{Additional Studies}
\subsection{Exploration of AID in Relation to ReLU, Dropout, and DropReLU}
\label{app:relation}

We explore that when the number of the interval is two around zero (i.e., $k=2$ and $u_1=l_2=0$), AID encompasses several well-known concepts, including ReLU activation, Dropout, and DropReLU.
This relationship is formalized in the following property:

\begin{property}
    \label{property:1}
    Let $AID_{p,q}$ be the function that dropout rate $p$ for positive and $q$ for negative values.
    Then, in training process, ReLU, Dropout, and DropReLU can be expressed by AID formulation.
\end{property}

    For simplicity, without derivation, we describe the relationships between the equations below.
    We assume that Bernoulli sampling generates an identical mask within the same layer.
    Let $r(\cdot)$ be the ReLU function, $Dropout_p(\cdot)$ be the Dropout layer with probability $p$, and $DropReLU_p(\cdot)$ be an activation function that applies ReLU with probability $p$ and identity with probabiltiy $1-p$.
    Then, during training process, for input vector $\mathbf{x}$, those functions can be represented by AID as below:

\begin{itemize}
    \item $r(\mathbf{x}) = AID_{0,1}(\mathbf{x})$
    \item $Dropout_p(\mathbf{x}) = AID_{p,p}(\mathbf{x}) / (1-p)$
    \item $Dropout_p(r(\mathbf{x})) = AID_{p,1}(\mathbf{x}) / (1-p)$
    \item $DropReLU_p(\mathbf{x}) = AID_{0,p}(\mathbf{x})$
\end{itemize}

Property \ref{property:1} indicates that using AID guarantees performance at least for the maximum performance of models using ReLU activation alone, in combination with dropout, or with DropReLU.
Analysis and investigation of the AID function under varying hyperparameters such as intervals and probabilities are left as future work.



\subsection{Effect of Theorem \ref{Thm:1} for ReLU and Standard Dropout}
\label{app:cor_1}
We analyze the changes in the formulation when applying a dropout layer to the ReLU function instead of AID.
Our findings show that this approach does not achieve the same regularization effect toward a linear network as AID.
We discuss this in Corollary \ref{cor:1} below.

\begin{corollary}
\label{cor:1}
    Similar setting with Theorem \ref{Thm:1}, combination of ReLU activation and Dropout layer does not have effect that regularize to linear network.
\end{corollary}

\begin{proof}
    We use same notation with Theorem \ref{Thm:1}.
    We can write the prediction of the model that use Dropout layer with probability $p$ after ReLU activation, $\hat {\mathbf{y}_i}$ as below:
    \[\hat{\mathbf{y}_i} = \mathbf{W}_2\left(\frac{1}{1-p}(\mathbf{I} - \mathbf{P})r(\mathbf{W}_1\mathbf{x})\right)\]

    where $r$ is ReLU function. On test phase, $\hat {\mathbf{y}_i} = \mathbf{W}_2 r(\mathbf{W}_1\mathbf{x})$.
    Then, to simplify the equations, let us denote $\mathbf{S}$, $\mathbf{S}_p$ and $\mathbf{v}$ as follows:

    \begin{align*}
    \mathbf{S}&\doteq \frac{1}{1-p}(\mathbf{I} - \mathbf{P})\mathbf{D} \\
    \mathbf{S}_p &\doteq \mathbf{D} \\ 
    \mathbf{v} &\doteq \mathbf{W}_1\mathbf{x}.
    \end{align*}

Then, following the same process as in Theorem \ref{Thm:1}, we derive the expansion for the relationship between 
$\mathbb{E}\left[\|\mathbf{W}_2\frac{1}{1-p}(\mathbf{I}-\mathbf{P})r(\mathbf{W}_1\mathbf{x})-\mathbf{y}\|_2^2\right]$ 
and $\|\mathbf{W}_2 r(\mathbf{W}_1\mathbf{x}) - \mathbf{y}\|_2^2$. 
We note that the detailed derivation is omitted as it largely overlaps with the previously established proof.

\begin{align*}
    \mathbb{E}\left[\|\mathbf{W}_2\frac{1}{1-p}(\mathbf{I}-\mathbf{P})r(\mathbf{W}_1\mathbf{x})-\mathbf{y}\|_2^2\right] 
    - \|\mathbf{W}_2 r(\mathbf{W}_1\mathbf{x}) - \mathbf{y}\|_2^2 
    &= \text{tr}\left(\frac{p}{1-p} \mathbf{D}^2 \text{diag}(\mathbf{v}) \mathbf{W}_2^\top \mathbf{W}_2 \text{diag}(\mathbf{v})\right) \\
    &= \frac{p}{1-p} \text{tr}(\mathbf{W}_2 \text{diag}(\mathbf{v}) \mathbf{D} \mathbf{D}^\top \text{diag}(\mathbf{v})^\top \mathbf{W}_2^\top) \\
    &= \frac{p}{1-p} \|\mathbf{W}_2 \mathbf{D} \text{diag}(\mathbf{v})\|_2^2 \\
    &\geq \frac{p}{n(1-p)} \|\mathbf{W}_2 r(\mathbf{W}_1\mathbf{x})\|_2^2 \\
    \implies \mathbb{E}\left[\|\mathbf{W}_2\frac{1}{1-p}(\mathbf{I}-\mathbf{P})r(\mathbf{W}_1\mathbf{x})-\mathbf{y}\|_2^2\right] 
    &\geq \|\mathbf{W}_2 r(\mathbf{W}_1\mathbf{x}) - \mathbf{y}\|_2^2 + \frac{p}{n(1-p)} \|\mathbf{W}_2 r(\mathbf{W}_1\mathbf{x})\|_2^2.
\end{align*}    

\end{proof}
    % 따라서 같은 논리로 전개했을 경우, dropout과 relu를 동시에 사용하는 것은 AID와 다르게 linear network로의 regularization 효과는 나타나지 않는 것을 확인할 수 있다. 
Corollary \ref{cor:1} demonstrate that the simultaneous use of Dropout and ReLU does not achieve the same regularization effect on the linear network as AID.
This suggests that, rather than completely discarding all negative values as ReLU does, allowing a portion of negative values to pass through with dropout plays a crucial role in effect that regularizes to linear network.

