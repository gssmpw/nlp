

\section{Baselines}
\label{app:baselines}

\textbf{Dropout \cite{srivastava2014dropout}.} Dropout randomly deactivates a fraction of neurons during training, effectively simulating an ensemble of sub-networks by preventing specific pathways from dominating the learning process. This approach improves generalization by reducing overfitting and encouraging the model to learn more robust and diverse representations. We applied Dropout layer next to the activation functions.

\textbf{L2 Regularization \cite{krogh1991simple}.} Also known as weight decay, L2 regularization(L2) adds a penalty proportional to the squared magnitude of the model weights to the loss function. This encourages smaller weights, which can lead to better generalization.

\textbf{L2 Init Regularization \cite{kumar2023maintaining}.} L2 Init introduces a regularization term that penalizes deviations of parameters from their initial values, aiming to preserve plasticity in continual learning. Unlike standard L2 regularization, which penalizes large weight magnitudes, it specifically focuses on maintaining the parameters near their initialization.

\textbf{Streaming Elastic Weight Consolidation \cite{elsayed2024addressing}.} Streaming Elastic Weight Consolidation (S-EWC) is a technique used in continual learning to prevent catastrophic forgetting. It identifies critical weights for previously learned tasks and penalizes changes to those weights during subsequent task learning.

\textbf{Shrink \& Perturb \cite{ash2020warm}.} Shrink \& Perturb (S\&P) is a method that combines weight shrinking, which reduces the magnitude of the weights to regularize the model, with perturbations that add noise to the weights. This approach is particularly effective in warm-start scenarios. As done in prior study \cite{lee2024slow}, we use a single hyperparameter to control both the noise intensity and the shrinkage strength. Specifically, given $\theta$ as the learnable parameter, $\theta_0$ as the initial learnable parameter, and $\lambda$ as the coefficient of S\&P, then the applying S\&P is defined as: \(\theta \leftarrow (1-\lambda)\theta + \lambda\theta_0\).

\textbf{DASH \cite{shindash}.} Direction-Aware SHrinking (DASH) selectively shrinks weights based on their cosine similarity with the loss gradient, effectively retaining meaningful features while reducing noise memorization. This approach improves training efficiency and maintains plasticity, ensuring better generalization under stationary data distributions. We applied DASH between the stages of training for generalizability experiments.

\textbf{ReDo \cite{sokar2023dormant}.} Recycling Dormant Neurons (ReDo) is a technique designed to address the issue of dormant neurons in neural networks by periodically reinitializing inactive neurons. This method helps maintain network expressivity and enhances performance by reactivating unused neurons during training.

\textbf{Continual Backprop \cite{dohare2024loss}.} Continual Backpropagation (CBP) is a method that enhances network plasticity in continual learning by periodically reinitializing a fraction of neurons with low utilization. This targeted reinitialization maintains the network’s ability to adapt to new tasks, effectively mitigating the loss of plasticity while preserving previously learned knowledge.

\textbf{Concatenated ReLU \cite{abbas2023loss}. } Concatenated ReLU (CReLU) is a variant of the ReLU activation function that combines the outputs of both the positive and negative regions of the input. This technique enhances plasticity on Streaming ALE environment. 

\textbf{Randomized ReLU \cite{xu2015empirical}.} Randomized ReLU (RReLU) behaves like a standard ReLU for positive inputs, while allowing negative inputs to pass through with a randomly determined scaling factor. This stochasticity can help improve model generalization by introducing diverse non-linearities.

\textbf{DropReLU \cite{liang2021drop}.} DropReLU operates by probabilistically switching the ReLU activation function to an linear function during training. This method effectively achieves generalization while remaining compatible with most architectures.

\textbf{Deep Fourier Feature \cite{lewandowski2024plastic}.} Deep Fourier Features (Fourier) effectively address plasticity loss by leveraging the concatenation of sine and cosine functions, which approximate the embedding of deep linear network properties within a model.

\textbf{Full Reset. } To compare performance against models trained from scratch, we included full reset as a baseline. At each stage, we reset the model’s parameters to their initial values before continuing training.


For baselines like CReLU and Fourier, which inherently double the output dimensionality, we ensured fair comparisons during trainability experiments by matching their number of learnable parameters to that of the vanilla model. In all other cases, we halved the output dimensionality to approximately halve the total number of parameters, thereby eliminating any advantage arising solely from increased parameter counts.

