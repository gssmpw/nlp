

Loss of plasticity refers to the phenomenon in which a neural network loses its ability to learn, which has been reported in recent studies \cite{lyle2023understanding, dohare2021continual}.
This phenomenon has been observed in models pre-trained on datasets where labels are assigned randomly or input images are randomly permuted.
When trained on a new task, these models exhibit degraded learning capabilities than freshly initialized networks.
Loss of plasticity is a critical issue that arises across various domains, making it an essential problem to address.
There have been evidence that plasticity loss is caused by non-stationarity \cite{lyle2024disentangling}.
While these results are from supervised domain like continual learning, plasticity loss also has been observed in the reinforcement learning domain, caused by its inherent non-stationarity \cite{sokar2023dormant, kumar2020implicit}.
However, recent study suggest that this phenomenon is prevalent not only in non-stationary domains, but also in stationary domains \cite{lee2024slow}.

\input{figures/AID_method}

Several factors have been proposed as causes of plasticity loss.
Earlier studies have identified issues such as dead neurons \cite{sokar2023dormant} and large weight norms \cite{lyle2023understanding} as potential contributors to this phenomenon.
More recent research suggests that non-stationary serves as the primary trigger for plasticity loss, subsequently leading to effects like dead neurons, linearized units, and large weight norms, which constrain the network's capability and destabilize training \cite{lyle2024disentangling}.
Other factors have also been implicated, including feature rank \cite{kumar2020implicit}, the sharpness of the loss landscape \cite{lyle2023understanding}, and noisy features \cite{shindash}.

Based on these insights, various approaches have been proposed to address plasticity loss.
These include adding penalty terms to regularize weights \cite{kumar2023maintaining, gogianu2021spectral}, resetting dead or low-utility neurons \cite{dohare2021continual, dohare2023maintaining, sokar2023dormant}, shrinking the network to inject randomness \cite{ash2020warm, shindash}, introducing novel architectures \cite{lee2024slow} or novel activations\cite{abbas2023loss, lewandowski2024plastic}, and developing new optimization strategies \cite{elsayed2024addressing}.

Widely used methods such as L2 regularization and LayerNorm \cite{ba2016layer} have proven to be effective in addressing plasticity loss \cite{lyle2024disentangling}.
However, recent study found that Dropout is not effective in mitigating plasticity loss \cite{dohare2024loss}.
This raises the question: why is Dropout ineffective against plasticity loss?
We first analyze why Dropout fails to address plasticity loss.

Building on this analysis, we propose a new method inspired by Dropout, called \textbf{AID} (\textbf{A}ctivation by \textbf{I}nterval-wise \textbf{D}ropout), to tackle plasticity loss.
Unlike Dropout, AID applies interval-specific dropout probabilities, making it functions as a nonlinear activation.
Through experiments, we demonstrate that AID's masking strategy is highly effective in addressing plasticity loss.
Recent studies have shown that plasticity loss does not occur in deep linear networks \cite{lewandowski2023curvature, lewandowski2024plastic, dohare2024loss}.
We theoretically prove that AID maintains plasticity by regularizing activations to close to those of a linear network. 
Since AID also functions as an activation function, we theoretically prove its compatibility with He initialization, demonstrating that AID can seamlessly replace the ReLU function.

