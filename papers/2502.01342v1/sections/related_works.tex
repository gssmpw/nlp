

\subsection{Plasticity in Neural Networks}
In recent years, various methods have been proposed to address plasticity loss.
Several works have focused on maintaining active units \cite{abbas2023loss, elsayed2024addressing} or re-initializing dead units \cite{sokar2023dormant, dohare2024loss}.
Other studies have explored limiting deviations from the initial statistics of model parameters \cite{kumar2023maintaining, lewandowski2023curvature, elsayed2024weight}.
Additionally, some methods rely on architectural modifications \cite{nikishin2024deep, lee2024slow, lewandowski2024plastic}.  
Plasticity loss also occurs in the reinforcement learning due to its inherent non-stationary. \citet{nikishin2022primacy} proposed resetting the model, while \citet{asadi2024resetting} suggested resetting the optimizer state. 

As noted by \citet{berariu2021study}, loss of plasticity can be divided into two distinct aspects: a decreased ability of networks to minimize training loss on new data (trainability) and a decreased ability to generalize to unseen data (generalizability).
While most previous works focused on trainability, \citet{lee2024slow} addressed generalizability loss.
They demonstrated that plasticity loss also occurs under a stationary distribution, as in a warm-start learning scenario where the model is pretrained on a subset of the training data and then fine-tuned on the full dataset.

Most existing studies have focused on only one of the following challenges: trainability, generalizability, or reinforcement learning.
However, in this study, we validate our AID method across all three aspects, demonstrating its effectiveness in each scenario.



\subsection{Activation Function}
Our AID method is a stochastic approach similar to Dropout while also functioning as an activation function.
Therefore, we aim to discuss previously proposed probabilistic activation functions.
Although the field of probabilistic activation functions has not seen extensive research, two noteworthy studies exist.
The first is the Randomized ReLU (RReLU) function, introduced in the Kaggle NDSB Competition \cite{xu2015empirical}.
The original ReLU function maps all negative values to zero, whereas RReLU maps negative values linearly based on a random slope.
During testing, negative values are mapped using the mean of the slope distribution.
Their experimental results suggest that RReLU effectively prevents overfitting.
Another example of a probabilistic activation function is DropReLU \cite{liang2021drop}.
DropReLU randomly determines whether a node's activation is processed through a ReLU function or a linear function.
The authors claim that DropReLU improves the generalization performance of neural networks.
The fundamental distinction between these probabilistic activation functions and our method lies in the generality of our approach.
Unlike simple probabilistic activation functions, our method encompasses techniques such as Dropout and ReLU, providing a more comprehensive framework.

Another related approach involves activation functions designed to address plasticity loss.
\citep{abbas2023loss} proposed the Concatenated Rectified Linear Units (CReLU), which concatenates the outputs of the standard ReLU applied to the input and its negation.
This structure prevents the occurrence of dead units, thereby improving plasticity.
Additionally, trainable activation functions have also been shown to effectively mitigate plasticity loss in reinforcement learning \citep{delfosseadaptive}.
Specifically, they introduced a trainable rational activation function that prevents value overfitting and overestimation in reinforcement learning.

\input{figures/exp_dropout}
