

In this section, we investigate the ineffectiveness of Dropout in mitigating plasticity loss.
Dropout operates by randomly deactivating specific nodes during training, effectively generating random subnetworks.
This mechanism fosters an ensemble effect, where the network learns more generalized features across these subnetworks.
However, our hypothesis suggests that this process does not address plasticity loss:

\textit{``Dropout merely creates multiple subnetworks, all of which suffer from plasticity loss.''}

In other words, because each subnetwork independently experiences plasticity loss, the combined network—treated as an ensemble—also suffers from degraded plasticity.

To validate this hypothesis, we conducted experiments on two forms of plasticity loss: trainability and generalizability. Details of the experimental configurations are provided in Appendix \ref{app:loss_pl_dropout}.


\subsection{Trainability Perspective}
To examine the relationship between Dropout and trainability, we trained an 8-layer MLP on the randomly labeled MNIST dataset.
For each task, labels were shuffled, and the network was trained for 100 epochs.
Following prior studies \citep{lyle2023understanding, kumar2023maintaining}, trainability degradation was quantified as the accuracy drop relative to the first task.
Figure \ref{exp_dropout} (left) demonstrates that higher Dropout probabilities lead to more pronounced trainability degradation.
This observation aligns with prior findings that Dropout exacerbates plasticity loss \citep{dohare2024loss}.
As Dropout probabilities increase, the network is divided into progressively smaller subnetworks, reducing the expected number of parameters available for learning.
This is consistent with the results of \citet{lyle2023understanding}, which show that smaller networks are more vulnerable to plasticity loss.
To further explore this phenomenon, we measured the trainability of individual subnetworks.
At the end of each epoch, we sampled 10 subnetworks and trained them on new tasks.
As shown in Figure \ref{exp_dropout} (middle), subnetworks with higher Dropout probabilities exhibited more severe trainability degradation, reinforcing our hypothesis that Dropout creates multiple subnetworks, each of which suffers from plasticity loss.
In contrast, our proposed method, AID, effectively maintained trainability across all tasks, demonstrating its robustness against plasticity loss.

\subsection{Generalizability Perspective}

To evaluate generalizability, we conducted a warm-start learning experiment inspired by \citet{lee2024slow}.
Specifically, we pre-trained a RESNET-18 model on 10\% of the training data for 1,000 epochs before continuing training on the full dataset.
We repeated this experiment for models trained with vanilla settings, Dropout, and AID.
The results are shown in Figure \ref{exp_dropout} (right).
Interestingly, Dropout seemed to improve generalizability, as both warm-start and cold-start models showed better performance.
However, we argue that this improvement arises from enhanced model generalization rather than a mitigation of plasticity loss.
The magnitude of improvement with Dropout was similar for warm-start (10.9\%p) and cold-start (10.1\%p) models.
If Dropout were primarily addressing plasticity loss, we would expect disproportionately higher performance improvements in warm-start models, as cold-start models inherently maintain perfect plasticity.
In contrast, the performance improvement with AID was significantly smaller (3.3\%p compared to the vanilla model's 10.9\%p).
This observation suggests that AID effectively mitigates plasticity loss, as warm-start models trained with AID retain a higher degree of plasticity compared to those trained with Dropout.
