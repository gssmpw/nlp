
In this paper, we analyzed why Dropout fails to maintain plasticity.
From this observation, we proposed AID (Activation by Interval-wise Dropout) and established through theoretical analysis that it acts as a regularizer by penalizing the difference between nonlinear and linear networks.
Through extensive experiments, we demonstrated the effectiveness of AID across various continual learning tasks, showing its superiority in maintaining plasticity and enabling better performance over existing methods.
Moreover, we validated its versatility by showing that AID performs well even in classical deep learning tasks, highlighting its generalizability across diverse scenarios.

Despite its effectiveness, several limitations remain in the current implementation of AID.
Our experiments primarily focused on configurations with simplified version, leaving the potential impact of more complex configurations underexplored.
Future work will address this limitation by optimizing AIDâ€™s scalability, exploring diverse configurations and adapting it to new architectures.

