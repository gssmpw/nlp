

\input{figures/exp_trainability}
\input{figures/exp_continual_full}

In this section, we empirically evaluate the effectiveness of the proposed method across various tasks.
First, we compare AID with previous methods in a range of continual learning tasks to assess its impact on mitigating plasticity loss in section \ref{sec:exp_CL}.
Second, we investigate whether AID provides tangible benefits in reinforcement learning in section \ref{sec:exp_RL}.
Finally, we demonstrate that AID improves generalization in standard classification tasks in section \ref{sec:exp_gen}.


\subsection{Continual Learning}
\label{sec:exp_CL}
We compare AID with various methods that were proposed to address the challenges of maintaining plasticity.
In addition, Randomized ReLU and DropReLU are included due to their structural similarity to our method.
Refer to Appendix \ref{app:baselines} for the description of each baseline.

\begin{itemize}
    \item \textbf{Regularization}: Dropout \cite{srivastava2014dropout}, L2 regularization (L2) \cite{krogh1991simple} and L2 regularization to initial value (L2 Init) \cite{kumar2023maintaining}
    \item \textbf{Re-initialization}: Shrink \& Perturb (S\&P) \cite{ash2020warm}, Recycling Dormant neurons (ReDo) \cite{sokar2023dormant} and Continual Backprop (CBP) \cite{dohare2024loss}
    \item \textbf{Activation function}: Concatenated ReLU (CReLU) \cite{abbas2023loss}, Randomized ReLU (RReLU) \cite{xu2015empirical}, deep Fourier features (Fourier) \cite{lewandowski2024plastic}, and DropReLU \cite{liang2021drop}
\end{itemize}


\subsubsection{Trainability}
\label{sec:trainability}
We start by validating the AID's ability to maintain trainability.
\citet{lee2024plastic} categorized the trainability of neural networks into two cases: Input trainability and Label trainability\footnote{In \citet{lee2024plastic}, the terms input plasticity and label plasticity were originally used. For consistency with our study, we instead adopt the terms input trainability and label trainability.}.
Input trainability denotes the adaptability to changing of input data, and label trainability denotes the adaptability to evolving input-output relationships.
Under the concept, we conducted permuted MNIST experiment for input plasticity by randomly permuting the input data and random label MNIST experiment for label plasticity by shuffling the labels at each task.
Detailed experimental settings are shown in Appendix \ref{app:trainability}.

Figure \ref{fig:trainability} shows the training accuracy for each method, categorized by optimizer and experimental settings.
We used learning rates $3\mathrm{e}{-2}$ for SGD and $1\mathrm{e}{-3}$ for Adam optimizer.
The results reveal several key findings.
First, training accuracy of vanilla model gradually declines for both SGD and Adam optimizers.
In contrast, methods specifically designed to address plasticity loss—such as Fourier, CReLU, ReDo, and CBP—demonstrate improved plasticity under most conditions.
However, these methods fail to retain accuracy on certain conditions.
In particular, S\&P and L2 Init often struggle to maintain plasticity when trained with a low learning rate (\ref{app:extended_results_trainability}), suggesting that their effectiveness may be sensitive to learning rate choices.
Notably, Dropout fails to mitigate plasticity loss in most cases compared to the vanilla model, supporting the results of \citet{dohare2024loss}.
In contrast, DropReLU and AID consistently achieve high accuracy across all settings, which can be attributed to the regularizing effect of the linear network, as discussed in Section \ref{Sec:TheoreticalAnalysis}.
Further analysis and detailed results with various learning rates and methods are provided in Appendix \ref{app:additional_results_trainability}.


\subsubsection{Generalizability}
In our earlier experiments, we confirmed that AID effectively maintains ability to adapt new data.
However, while adaptability is important, it is even more critical to ensure that the model retain the ability to generalize well to unseen data.
Therefore, generalizability is a critical aspect related to plasticity loss.
We evaluated the generalizability of AID across three benchmark settings: continual-full, continual-limited, and class-incremental.
For this evaluation, we utilized three datasets: CIFAR10, CIFAR100 \cite{krizhevsky2009learning}, and TinyImageNet \cite{le2015tiny}.

\textbf{Continual Full \& Limited} \quad
We investigated two continual learning settings inspired by previous works \citep{lee2024slow,shen2024step}. In the continual full setting, an extended version of the warm-start framework, training progress is divided into 10 phases, with 10\% of the dataset introduced at each phase while maintaining access to all previously seen data throughout training.
In contrast, the continual limited setting imposes stricter constraints by restricting access to past data, reflecting practical challenges such as privacy concerns and storage limitations.
In our experiments, we exclude L2 Init and ReDo, as they have been shown to have no improvement on generalizability, and instead include the recently proposed Direction-Aware SHrinking (DASH) \cite{shindash} as a baseline.
Additionally, we incorporate Streaming Elastic Weight Consolidation (S-EWC) \cite{kirkpatrick2017overcoming, elsayed2024addressing} on continual limited, which has demonstrated effectiveness in mitigating forgetting under constrained settings.
% Continual full 결과
%1. Continual setting의 결과는 figure {}에 제시함
%2. Continual Full setting에서 DASH나 L2 같은 일부 method들은 task와 dataset에 따라 full reset과 비교하여 비슷하거나 더 나은 성능을 제공하는 경우가 존재함
%3. 하지만 AID는 학습 초반부터 후반까지 다른 method들과 비교하여 눈에 띄는 퍼포먼스 차이를 보여줌. 
%4. 이는 데이터가 추가 될때마다 작동하는 re-init 계열들보다 학습 도중에 꾸준히 적용되는 regularization이나 activation function의 중요성을 상기함
%5. 이 놀라운 결과는 data access가 제한되는 continual limited의 경우에도 유지됨.
%6. 흥미롭게도 여기선 dropout 계열의 방법들이 모두 성능이 높았는데, 이는 dropout이 catastrophic forgetting을 극복하는데 도움을 준다는 이전 연구의 결과를 뒷받침함
The results of the Continual setting are presented in Figure \ref{exp_continual_full}.
Several methods—such as DASH and S\&P—achieve comparable or even superior performance to full reset, depending on the specific task and dataset.
However, AID demonstrates a noticeable performance advantage over other methods throughout the entire training process.
This result highlights that plasticity can be maintained without the need to re-initialize certain layers or neurons, which carries the risk of losing previously learned knowledge.
Notably, this trend persists even in the continual limited setting.
Interestingly, methods belonging to the dropout family (Dropout, DropReLU, and AID) also exhibit strong performance in this setting, aligning with previous findings that suggest dropout can help mitigate catastrophic forgetting \cite{mirzadeh2020dropout}.

\textbf{Class-Incremental} \quad
In addition to these continual learning settings, we also conducted experiments on class-incremental Learning, inspired by previous works \cite{lewandowski2024plastic, dohare2024loss}.
Unlike the warm-start framework, this approach partitions the training process into 20 phases based on class labels, incrementally introducing new classes at each phase.
Like continual full setting, the model is trained on all available data, including previously introduced class data.
Further details regarding the experimental setup can be found in Appendix \ref{app:generalizability}.\input{figures/exp_class_incremental.tex}
Figure \ref{exp_class_incremental} presents the difference in test accuracy between a model trained with full reset as each class is introduced.
Methods—such as Dropout and DASH—initially demonstrate an accuracy advantage when full reset is applied during training.
However, as training progresses, this advantage diminishes, highlighting their limitations in long-term plasticity retention.
In contrast, AID stands out as the only method that consistently maintains its advantage over full reset throughout the training process.
These results suggest that AID effectively mitigates plasticity loss across various experiments evaluating generalizability, reinforcing its robustness in continual learning scenarios. Additional figures showing overall test accuracy are provided in Appendix \ref{app:omitted_results_CI}.


\subsection{Reinforcement Learning}
\label{sec:exp_RL}
A high replay ratio is known to offer high sample efficiency in reinforcement learning.
However, increasing the replay ratio often leads to overfitting to early data and results in plasticity loss, as highlighted in previous studies \cite{kumar2020implicit, nikishin2022primacy}.
In this section, we demonstrate that using AID can effectively mitigate these challenges by maintaining plasticity while benefiting from high sample efficiency.

The experimental setup is as follows: 
Unlike standard DQN \cite{mnih2015human} use $RR=0.25$, we train with $RR=1$ on several Atari \cite{bellemare2013arcade} tasks, using the CleanRL environment \cite{huang2022cleanrl}.
We use the 17 games to train and hyperparameter settings from \citet{sokar2023dormant}.
Due to the high computational cost associated with a high RR, we followed the settings of \citet{sokar2023dormant, elsayed2024weight}, where the models are trained for 10 millions frames.

\input{figures/RL_total}

The IQM Human Normalized Score results for the 17 Atari games are presented in Figure \ref{fig:RL_total}, with shading indicating stratified bootstrap 95\% confidence intervals.
The results demonstrate that AID significantly improves sample efficiency compared to the vanilla model.
Traditionally, dropout has been rarely used in reinforcement learning due to several factors \cite{hausknecht2022consistent}.
Additionally, as demonstrated in Section \ref{sec:trainability}, its limitation in mitigating plasticity loss in non-stationary environments may further explain this issue.
However, this result suggests that methods within the dropout family, such as AID, hold potential for enhancing RL performance, challenging the conventional perception of dropout's limitations in this domain.
For additional analysis on RL, refer to the Appendix \ref{app:omitted_results_rl}.


\subsection{Standard Supervised Learning}
\label{sec:exp_gen}
The preceding experiments have demonstrated that AID effectively addresses the loss of plasticity and exhibits its effectiveness in non-stationary problems.
In this subsection, we investigate whether AID also provides advantages in classical deep learning scenarios without non-stationarity.

For comparison, we utilized L2 regularization and dropout, which are commonly employed in supervised learning, as baselines.
The model was trained for a total of 200 epochs using the Adam optimizer with a learning rate of 0.001.
The learning rate was reduced by a factor of 10 at the 100th and 150th epochs.
The final test accuracy is shown in Table \ref{tab:generalization}.
\input{tables/result_generalization}

The results presented in Table \ref{tab:generalization} demonstrate that AID is effective not only in addressing plasticity loss but also in improving generalization performance in classical supervised learning settings.
Interestingly, AID reduces the generalization gap more effectively than standard methods commonly used to addressing overfitting.
We report the corresponding learning curves in Appendix \ref{app:learning_curve_sl}.



