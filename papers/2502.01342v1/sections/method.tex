

\newtheorem{property}{Property} % Property 환경 정의

\subsection{Notations}
We denote $r(\cdot)$ to be a ReLU (Rectified Linear Unit) activation function, which means that $r(x) = \max(x, 0)$ for input data $x$. For ease of notation, we define a negative ReLU, $\bar r(x) = \min(x,0)$. 
Furthermore, for a real number $\alpha>0$, we define modified leaky ReLU, $r_\alpha$ as a function that scales the positive part of the input by $\alpha$, and the negative part by $1-\alpha$, formally given as $r_\alpha(x) = \frac{1}{2}x+(\alpha-\frac{1}{2})|x|$ (e.g. $r_1(x) = r(x)$, $r_0(x) = \bar r(x)$, and $r_{1/2}(x) = \frac{1}{2}x$). Note that, $r_\alpha$ differs from leaky ReLU, since its slope on the positive side may not be 1.

\subsection{Activation by Interval-wise Dropout (AID)}\label{sec:4.2}

Activation by Interval-wise Dropout (AID) applies different dropout probabilities to distinct intervals of pre-activation values, functioning as a non-linear activation.
Specifically, given \( k \) predefined intervals and their corresponding dropout probabilities \( \{p_1, p_2, \dots, p_k\} \), AID generates \( k \) distinct dropout masks.
Each mask is applied only to the values falling within its corresponding interval, while other intervals are unaffected by that mask.
Formally, let the predefined intervals be \( \{I_1, I_2, \dots, I_k\} \), where \( I_j = [l_j, u_j) \) for \( j = 1, 2, \dots, k \).
The union of these intervals covers the entire range of real values, i.e., \( \bigcup_{j=1}^k I_j = \mathbb{R} \).
Moreover, the intervals are disjoint, meaning there is no overlap between any two intervals, i.e., \( I_i \cap I_j = \emptyset \quad \text{for all } i \neq j \).
AID operates as a non-linear activation function when the dropout probabilities vary across intervals, i.e., when \( p_i \neq p_j \) for some \( i \neq j \).
If all dropout probabilities are identical (\( p_1 = p_2 = \dots = p_k \)), AID reduces to a standard dropout mechanism.
The output of the AID mechanism for an input vector \( \mathbf{x} \) is given by: \(AID(\mathbf{x}) = \mathbf{x} \odot \mathbf{m}\).
Where \( \mathbf{m} \) is the dropout mask vector defined as: 
\( m_i = 0 \) with probability \( p_j \) if \( x_i \in I_j \), and \( m_i = 1 \) otherwise.
This interval-wise dropout mechanism enables the model to handle varying ranges of pre-activation values, effectively functioning as a nonlinear activation function while applying stochastic regularization with different probabilities across intervals.

During the testing phase of traditional dropout, each weight is scaled by the probability \(1-p\), which is the chance that a node is not dropped.
Similarly, in AID, as each interval has a unique dropout probability, the pre-activation values within each interval $I_j$ are scaled by \(1-p_j\) for testing.
The pseudo-code for AID is presented in Algorithm \ref{alg:gen_AID}.

\begin{algorithm}[h]
   \caption{Activation by Interval-wise Dropout (AID)}
   \label{alg:gen_AID}
\begin{algorithmic}
    \STATE {\bfseries Given:} Predefined intervals $\{I_1, I_2, \dots, I_k\}$, dropout probabilities $\{p_1, p_2, \dots, p_k\}$
    \STATE {\bfseries Input:} Pre-activation data $\mathbf{x} \in \mathbb{R}^n$
    \STATE {\bfseries Output:} Post-activation data $\mathbf{y} \in \mathbb{R}^n$
    \IF{training phase}
        \STATE Initialize $\mathbf{y} \in \mathbb{R}^n$ as an empty vector
        \FOR{each \(x_i \in \mathbf{x}\)}
            \STATE Identify interval \( I_j \) such that \( x_i \in I_j \)
            \STATE Initialize binary variable \( m \sim Bernoulli(1 - p_j) \)
            \STATE Apply mask: \( y_i = x_i \cdot m \)
        \ENDFOR
    \ELSIF{testing phase}
        \STATE Initialize $\mathbf{y} \in \mathbb{R}^n$ as an empty vector
        \FOR{each \(x_i \in \mathbf{x}\)}
            \STATE Identify interval \( I_j \) such that \( x_i \in I_j \)
            \STATE Apply scaling: \( y_i = x_i \cdot (1 - p_j) \)
        \ENDFOR
    \ENDIF
    \STATE \textbf{return} $\mathbf{y}$
\end{algorithmic}
\end{algorithm}

AID generalizes concepts from ReLU, standard Dropout, and DropReLU.
We provide a detailed discussion of these relationships in Appendix \ref{app:relation}.
Since defining each interval and its corresponding dropout probability requires exploring a vast hyperparameter space, Algorithm \ref{alg:gen_AID} demands extensive hyperparameter tuning.
To address this, we introduce a simplified version of AID in the next section.


\subsection{Simplified version of AID}
Section \ref{sec:4.2} shows that this methodology offers infinitely many possible configurations depending on the number and range of intervals, as well as the dropout probabilities.
To reduce the hyperparameter search space and improve computational efficiency, we propose the Simplified AID.
Specifically, we define $AID_p(\cdot)$ as a function that applies a dropout rate of $1-p$ to positive and $p$ to negative values.
This definition is intuitive since $AID_p$ behaves like ReLU when $p=1$ and a linear network when $p=0.5$.
Additionally, we will demonstrate that this simplified version has an effect that regularizes to linear network and an advantage when He initialization is applied, as discussed in Section \ref{Sec:TheoreticalAnalysis}.
Next, we explore a property of $AID_p$ that are useful for its practical implementation.


\begin{property}
\label{property:2}
Applying $AID_p(\cdot)$ is equivalent to applying $r(\cdot)$ with probability $p$, and $\bar{r}(\cdot)$ with probability $1-p$.
\end{property}

\begin{algorithm}[b]
   \caption{Simplified version of AID}
   \label{alg:AID}
\begin{algorithmic}
    \STATE {\bfseries Given:} ReLU $r$, negative ReLU $\bar r$, modified leaky ReLU $r_\alpha$ and one vector $\mathbf 1\in \mathbb R^n$
   \STATE {\bfseries Input:} Pre-activation data $\mathbf{x}\in \mathbb R^n$, coefficient $p$
   \STATE {\bfseries Output:} Post-activation data $\mathbf{y} \in \mathbb R^n$
   \IF{training phase}
      \STATE $\mathbf{M}\leftarrow$ Generate binary masks  from $Bernoulli(p)$

      \STATE $\mathbf y \leftarrow \mathbf M \odot r(\mathbf{x}) + (\mathbf 1-\mathbf M) \odot \bar r(\mathbf x)$ 
   \ELSIF{testing phase}
      \STATE $\mathbf y \leftarrow r_p(\mathbf x)$
   \ENDIF
   \STATE \textbf{return} $\mathbf{y}$
\end{algorithmic}
\end{algorithm}
% 성질 1을 이용하면 구현을 편하게 할 수 있다.
We provide a brief proof in Appendix \ref{app:proof_property_2}.
Using Property \ref{property:2}, the implementation of AID becomes straightforward.
An important consideration is whether scaling up is performed during training.
Dropout layer with probability $p$ scales the value by $\frac{1}{1-p}$ at training phase, so it acts as an identity function during the test phase.
However, since AID functions as a non-linear activation, we scale the value at test phase.
Specifically, AID utilizes $r_\alpha(\cdot)|_{\alpha=p}$, which is the same as passing the average values from training phase.
To clarify the process, we provide the pseudo-code for the AID activation in Algorithm \ref{alg:AID}.


Algorithm \ref{alg:AID} demonstrates that AID can be easily applied by replacing the commonly used ReLU activation.
Moreover, it shares the same computational complexity as standard dropout.
These properties enable AID to integrate seamlessly with various algorithms and model architectures, regardless of their structure.
Throughout this paper, AID is implemented as described in Algorithm \ref{alg:AID}, unless specified otherwise.
In the next section, we study the factors that contribute to the effectiveness of AID.




\subsection{Theoretical Analysis}
\label{Sec:TheoreticalAnalysis}


Recent studies \citep{dohare2024loss, lewandowski2024plastic} have found that linear networks do not suffer from plasticity loss and provided theoretical insights into this phenomenon.
This highlights that the properties of linear networks play a crucial role in resolving the issue of plasticity loss.

From an intuitive perspective, AID shares properties with deep linear networks in that, it provides the possibility of linearly passing all pre-activation values and having gradients of 1.
This behavior contrasts with activations such as ReLU, RReLU, and leaky ReLU, which tend to pass fewer or no values for negative pre-activations.
Building on this intuition, the following theoretical analysis establishes that AID as a regularizer, effectively constraining the network to exhibit properties of a linear network.

\begin{theorem}
    \label{Thm:1}
    For a 2-layer network, AID has a regularization effect that constrains the model to behave like a linear network. Formally,  let weight matrices be \( \mathbf{W}_1, \mathbf{W}_2 \in \mathbb R^{n\times n} \), the input and target vectors be \( \mathbf{x}, \mathbf{y} \in \mathbb R^n\), and the coefficient of AID be $p$. Then, for learnable parameter $\theta$:
 \begin{align*}L_{AID_p}(\theta) &\doteq\mathbb{E}\big[\|\mathbf{W}_2 \text{AID}_p(\mathbf{W}_1 \mathbf{x}) - \mathbf{y}\|_2^2 \big] \\L_{r_p}(\theta) &\doteq \|\mathbf{W}_2 r_p(\mathbf{W}_1 \mathbf{x}) - \mathbf{y}\|_2^2 \\ R_p(\theta) &\doteq  \|\mathbf{W}_2 \big(\frac{1}{2}\mathbf{W}_1 \mathbf{x}\big) - \mathbf{W}_2 r_p(\mathbf{W}_1 \mathbf{x})\|_2^2 \\ \implies L_{AID_p}(\theta) &\geq L_{r_p}(\theta) + \frac{4p(1-p)}{n(2p-1)^2}R_p(\theta).
\end{align*}
\end{theorem}

We provide the detailed proof on Appendix \ref{app:proof_1}, following the approach taken by \citet{liang2021drop}.
Theorem \ref{Thm:1} indicates that applying AID instead of the ReLU function injects linearity into the model, effectively mitigating plasticity loss.
On the other hand, incorporating a dropout layer with ReLU does not yield the same effect, and the relevant lemma is provided in Appendix \ref{app:cor_1}.

Since AID functions as an activation, it is important to examine its compatibility with existing network initialization methods designed for activations.
Therefore, we show that AID ensures the same stability as the ReLU function when used with Kaiming He initialization.
The following property illustrates this strength:

\begin{property}
    \label{property:He_init}
    Replacing ReLU activation with $AID_p$ ensures the same stability during the early stages of training when using Kaiming He initialization.
\end{property}

Property \ref{property:He_init} guarantees that the output and gradient will not explode or vanish, thereby supporting the learning stability of the network during its initial training.
The outline of the explanation builds on \citet{he2015delving}’s formulation, utilizing the fact that replacing ReLU with AID does not alter the progression of the equations.
A detailed explanation is provided in Appendix \ref{app:He_init}.

