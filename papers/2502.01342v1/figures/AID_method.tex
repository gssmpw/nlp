

\begin{figure}[!t]
    \centering
    \includegraphics[width=\columnwidth]{figures/sources/AID_method.pdf}
    \caption{\textbf{AID Architecture for Simplified Version.} We apply dropout at a rate of $1-p$ for positive values and $p$ for negative values, where $m_{\text{pos}} \sim Ber(p)$ and $m_{\text{neg}} \sim Ber(1-p)$. Unlike ReLU activation, AID allows utilizing negative pre-activation and regularizing toward linear network, effectively mitigating plasticity loss.}
    \label{fig:AID}
\end{figure}

