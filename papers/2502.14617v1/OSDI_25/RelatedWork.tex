\begin{comment}
\begin{table*}[]
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}{c|ccccccc}
         \multirow{2}{*}{Work} & $>1$  & Startup and & Multiple  & Forecast & Varied Resource & Designed for & Infrastructure \\
         & Instance & Migration Delays & SLOs & Aware & Requirements & LLMs & Independent \\
         \hline
         vLLM & $\times$ & $\times$ & $\times$ & $\times$ & $\times$  & \checkmark & \checkmark \\
         Llumnix & \checkmark & $\times$ & $\times$ & $\times$ & $\times$  & \checkmark & $\times$ \\
         Zeal & \checkmark & $\times$ & $\times$ & $\times$ & $\times$  & \checkmark & \checkmark \\
         \hline
         \sys & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark \\
        \end{tabular}
    }
    \caption{\kjnote{Comparison of \sys with current literature. Our work takes into account multiple architectures with diverse accelerator requirements having >1 instances while serving requests with varied SLOs and arrival patterns.}\textcolor{blue}{AP: remove the column, designed for LLMs. Better to align this table with the rest of narrative. for instance, align this with contribution section or other way around.}}
    \label{tab:related_works}
\end{table*}
\end{comment}
\section{Related Work \pglen{0.75}}
\label{sec:related}

\subsection{Autoscaling for cloud computing} 
There is a long history of forecast-based autoscaling of CPU VMs to serve cloud computing workloads \cite{roy2011efficient,varshney2018autobot}. There are also works from commercial cloud owners on autoscaling \cite{hadary2020protean, rzadca2020autopilot}, and request routing \cite{saokar2023servicerouter} which illustrate the need for such mechanisms in all cloud services, the challenges associated with operating them at the cloud scale, and the enormous potential of cost reduction with these optimization. For e.g. in \cite{hadary2020protean} the authors mention that even a $1\%$ reduction in VM fragmentation can lead to \$100M of cost savings in a year and we expect this number to be even higher for GPUs due to their higher cost. The main difference between our work and these prior works is that we consider autoscaling of LLMs on GPUs  which encounters unique challenges due to the high latency sensitivity of interactive workloads, the significant variation in SLOs across workloads, and the high cost and latency of migrating and loading LLMs (hundreds of GB of data) on GPUs.

% \kjnote{Paper we might need to update with: Zeal: Rethinking Large-Scale Resource Allocation with "Decouple and Decompose"}

\subsection{Efficient LLM Serving}

Several recent works have focused on optimizing the latency or throughput of LLM requests at a single model instance. These include efficient computing approaches like PagedAttention \cite{kwon2023efficient}, FlashAttention \cite{dao2022flashattention}, and vAttention \cite{prabhu2024vattention}, as well as novel  algorithms for batch processing of LLM requests like Orca \cite{orca}, Sarathi \cite{agrawal2024taming}, and Splitwise \cite{patel2024splitwise}.  Additionally, \cite{liu2024andes} optimize Quality-of-Experience (QoE) for LLM serving systems, focusing on user-centric metrics to enhance individual experiences. However these works typically only consider a single LLM instance and do not take into account multiple model types and GPUs.

% \subsection{LLM Serving Systems}
% Recent advancements in inference serving systems for LLMs have focused on optimizing throughput, latency, and resource management. ORCA \citet{orca}, Sarathi \citet{agrawal2023sarathi}, FlashAttention \citet{dao2022flashattention}, and vAttention \citet{prabhu2024vattention} are examples of systems that have achieved significant improvements in performance through techniques such as iteration-level scheduling, innovative batching, and IO-aware algorithms.

% \subsection{LLM Serving Algorithms} This space has also seen several algorithmic innovations. QLM (\citet{jhaqlm}) utilizes Bayesian statistics and stochastic programming to manage non-deterministic execution times inherent in LLMs. Similarly, \citet{qiu2024efficient} advocates for speculative shortest-job-first scheduling, and \citet{wu2023fast} employs preemptive scheduling to improve performance.
% DistServe and Splitwise (\citet{zhong2024distserve, patel2023splitwise}) optimize LLM serving performance by separating prefill and decoding computation for throughput enhancement while maintaining low latency.  In addressing system load and request patterns, \citet{jha2024learned} and \citet{mendoza2024model} utilize deep reinforcement learning to dynamically adjust service quality, increasing hardware utilization for cost-efficient serving. \citet{jhaqlm, sun2024llumnix}  proposes a multi-model queue management framework for LLM serving and orchestrate the actions such as model swapping, request eviction, GPU-CPU state swapping, load balancing, and warm model start. 
% While these works optimize request scheduling at the instance level, they ignore the diversity in the prompt and decode characteristics across requests.

\subsection{LLM Autoscaling} 

Since the launch of ChatGPT \cite{ChatGPT} and the advent of commercial LLM serving platforms, there have been some research works that have considered autoscaling LLM resources to meet workload requirments. \cite{li2023alpaserve,melange,mei2024helix} explored optimal placement of models on GPUs in a cluster by formulating different optimization problems. However, these optimizations are static and do not incroporate workload forecasts due to which they may not be as effective in handling the high fluctuations in workload traffic observed in commercial LLM serving platforms (\autoref{fig:rps}). There are also works that address specific challenges with LLM request serving such as handling pre-emption \cite{miao2024spotserve}, addressing startup and migration delays due to large model sizes \cite{fu2024serverlessllm}, and using a unified pool for serving online (IW) and offline (NIW) requests to reduce GPU fragmentation\cite{qiao2024conserve}. These works do not consider the general mix of workloads with different characteristics and models with different resource requirements as \sys does. Finally \cite{wu2023fast,nie2024aladdin,jain2024intelligent,kossmann2024gpu} consider routing of LLM requests across different model instances with different goals like latency minimization and throughput maximization. While request routing is a crucial part of the LLM serving infrastructure for handling short-term fluctuations in traffic, it is important to combine it with dynamic model scaling as is done in \sys to handle longer-term traffic variations and prevent model assignments from becoming outdated. 
% \kjnote{Table~\ref{tab:related_works} shows a comprehensive comparison of our work compared to existing literature}

%LLM simulators works.\\

% \clearpage