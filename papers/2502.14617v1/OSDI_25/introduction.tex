\begin{figure}[t]
    \centering
% \includegraphics[width=\columnwidth]{figures/motivation/0.pdf}
\includegraphics[width=\columnwidth]{figures/motivation/1.pdf}
    \caption{TPS based reactive scaling often under or over allocates hardware. The shaded region shows the difference in instance allocation between an ideal and a reactive scaling mechanism. \textit{Blue curve:} An ideal scaling mechanism would follow the blue line and trigger scaling up at T=10 minutes, starting token processing by T=15 minutes. \textit{Top: } A reactive scaling mechanism with correct TPS thresholds (4000 in this case) would trigger the scaling up at T=15 minutes, leading to SLA violations. \textit{Bottom: } Lowering the threshold to 3500 TPS for the reactive mechanism would make it susceptible to bursts, leading to over allocation.}
    \label{fig:ideal_scaling}
\end{figure}
\section{Introduction \pglen{1.25}}\label{sec:intro}

There is a push towards supporting intelligent features within enterprise products and services, and augmenting the behavior of clients through the use of Large Language Models (LLMs) and other ML/AI models. Such features may be unobtrusive and happening proactively in the background, or actively initiated by the users. There are rapid advances towards exploring the multitude of scenarios to which LLMs can be applied to and novel use-cases are being continuously developed. As their capabilities continue to grow, the use of LLMs for enterprise and consumer applications is increasing exponentially.

GPU-accelerated Virtual Machines (VMs) hosted on the cloud are at the vanguard of enabling inferencing over LLMs at massive scales. Cloud Service Providers (CSPs) and large Internet companies are investing heavily on GPU hardware, both for internal consumption by their products and services, and to lease them out as GPU VMs or as AI inferencing services. E.g., AWS UltraClusters offer 20,000 Nvidia H100 GPUs per region for training workloads~\footnote{https://www.theregister.com/2024/01/15/cloud\_providers\_gpu\_analysis/}, while Meta and Microsoft reportedly purchased 150,000 H100 GPUs in 2023~\footnote{https://www.theregister.com/2023/11/27/server\_shipments\_fall\_20\_percent/}. 
% \kjnote{
As the demand for these AI capabilities is growing, even governments across the world are expanding AI datacenters, such as the StarGate Project of the US government~\cite{stargate}.
% } 
% \apnote{cite} 
However, 
% while substantial investments are being made in GPU hardware, 
besides these substantial investments,
it is equally critical to ensure these resources are utilized effectively to maximize return on investment. Inefficient utilization can arise when GPU provisioning across regions fails to align with actual traffic distribution. This can lead to missed SLOs when demand exceeds supply or resource wastage when supply exceeds demand. As most CSPs prioritize user experience due to business requirements, they typically over-provision GPU capacity which quickly escalates infrastructure costs. This can drive up the price of LLM services for end users and also divert resources away from other vital areas, such as research and development, potentially hindering long-term innovation. 

While autoscaling CPU VMs is a long-studied problem in cloud computing \cite{hadary2020protean,rzadca2020autopilot}, scaling GPU VMs to meet LLM workload requirements comes with a unique set of challenges. Specifically, LLM inference requests across commercial LLM platforms like Gemini \cite{Gemini}, Copilot \cite{Copilot}, and ChatGPT \cite{ChatGPT} usually serve a mix of workloads that can broadly be categorized as interactive, non-interactive, and opportunistic. Interactive workload (IW) requests are latency-sensitive and require real-time responses, often within milliseconds or seconds, such as in chatbots, search query completions, real-time translations, and content moderation. In contrast, non-interactive workload (NIW) requests are less time-critical and focus on serving resource-intensive or batch processes such as large-scale content generation, data annotation, simulations, etc. at a higher throughput and lower cost. Finally, opportunistic workload (OW) requests are those guaranteed a lower tier of service such as those served by spot or pre-emptible instances \cite{miao2024spotserve,wu2024can}. 
%\kjnote{Sentence is cut out:}{\color{red}
They can be interactive or non-interactive but can be evicted to make way for higher priority requests. %eds of each of these workload types to effectively scale LLM applications.} 
The exact mix of LLM workloads depends on several factors like time of day, region, tenant type (enterprise v/s consumer), etc. This wide variation in LLM workload characteristics and requirments makes it challenging to develop a unified autoscaling policy for all models and workloads. 

%\kjnote{Review}
Consider a reactive scaling mechanism which benchmarks the average Tokens Processed per Second (TPS) by a model instance and scales up/down based on the number of incoming tokens. As shown in \autoref{fig:ideal_scaling} over or under allocate instances due to variance in TPS and the problem is exasperated by the delays in spinning up LLM instances. %\kjnote{Review}
When the model has a processing capability of 4000 TPS, the reactive mechanism would scale up the instance count at T=15 minutes, which will result in the instance being available at T=20 minutes, resulting in SLA violations due to under allocation for 5 minutes. If we keep the scaling threshold to 3500 incoming TPS to avoid under allocation, our system will become susceptible to over allocation, as illustrated in the bottom figure at T=25 minutes. Even though the incoming TPS becomes stable, reactive mechanisms would scale at T=25 minutes due to a small sudden burst in traffic.
%\kjnote{Till here}
Scaling LLMs involves significant overhead due to the need to load large model weights (which can be in the order of gigabytes or more) into GPU memory. This process can block GPUs for several seconds to minutes, making frequent scaling costly. Thus, there is a critical need for a flexible and lightweight autoscaling policy that can seamlessly adapt to a varying mix of workloads, minimize delays associated with model loading and migration, and can reduce the overall infrastructure cost while meeting diverse workload requirements like low-latency SLAs, batch processing, and handling pre-emption.

We address these challenges with \sys. We start by examining a baseline siloed approach with separate GPU pools for Interactive Workload (IW) and Non-Interactive Workload (NIW) requests with Opportunistic Workloads (OW) requests of each type assigned to the corresponding pool if there is spare capacity. %\apnote{Do we have any OW worklaods?} 
We empirically observe that this leads to significant underutilization of the IW pool in off-peak hours. To address this, \sys uses a unified pool with a reactive heuristic, routing NIW requests to GPUs with low IW load, saving GPU hours while maintaining SLAs. However, this approach uses a fixed number of instances of each model type which can lead to problems of demand-supply mismatch in settings with high fluctuation in traffic across each workload and LLM type. %\apnote{need to rewrite this part. Looks like this is due to demand suppky in each worklaoad type. However the issue is across different model type. assigned to AP.}
Therefore \sys optimizes GPU allocation by formulating a constrained optimization problem to dynamically scale model instances across regions based on ARIMA-based traffic forecasts, balancing short-term routing adjustments with long-term model scaling. This approach improves GPU utilization, meets SLAs, and allows surplus capacity to be rented for OW requests, reducing costs and enhancing overall efficiency.
%\apnote{the dynamic allocation across different model types and minimal allocation overhead part is not clear. assigned to AP.}
In summary, we make the following contributions: 
\begin{enumerate}[leftmargin=*,noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
 \item We introduce the problem of designing LLM serving platforms for cloud service providers operating costly GPU VMs across regions, and receiving diverse workloads with variable SLA requirements. This motivates the need to holistically improve resource efficiency of GPU VMs by continuously maintaining the appropriate number of LLM model instances in various regions (\autoref{sec:sys-app-model}). 
\item  We present empirical studies on real-world workloads observed at a major cloud provider, Microsoft, that support the need for a systematic approach to instance scaling and highlight the pitfalls of siloed decision-making (\autoref{sec:empstudy}).
\item  We propose \sys, a unified framework for serving requests while satisfying SLAs and maximizing capital efficiency by donating surplus instances to spot by defining this as an optimization problem, solved using ILP (\autoref{sec:optz}). \sys uses long-term aware reactive strategies that forecast the request arrival patterns, and optimize the scale out and in of GPU VMs and model instances to consider the token processing capability required for the interactive load and head-room for non-interactive ones (\autoref{sec:arch}). The overheads of dynamic provisioning are also considered.
\item We implement a prototype of \sys using a realistic simulation harness that extends from SplitWise~\cite{splitwise}. We evaluate it on real-world workloads, for 3 regions and 4 LLM models handling {over 10 million requests}. We report results on the improvement in resource utilization and the scalability of the strategies across a week-long duration. We show 
a 25\% reduction in GPU hours while processing a production trace with over 10 million requests over a week, reducing the scaling overhead by 80\% and translating into savings of over \$2M dollars over a month for cloud providers. We do this without compromising any SLO requirements and improving hardware utilization.
\item Finally, we plan to open-source our trace data and simulator. The harness can serve as a testbed for simulating LLM serving stacks that can be leveraged by the community, and encourage future research in this area.
\end{enumerate}
%\apnote{Revisit contributions}