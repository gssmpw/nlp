\begin{figure*}[t]
    \centering
\includegraphics[width=.95\linewidth]{figures/simulator_v1.png}
    \caption{Overview of the \sys simulator 
    }
    \label{fig:simulator}
\end{figure*}

\section{Evaluation}
\label{sec:results}

The evaluation aims to answer the following questions:
\begin{enumerate}[leftmargin=*,noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
    \item  How effectively can the proposed approach (\sys) utilize GPU resources while maintaining latency targets for both interactive and non-interactive workloads? ($\S$ \ref{sec:overallperf}) 
    % \ysnote{\ref{fig:instance-hours-curve}, \ref{fig:latency_metrics}, \ref{fig:instance-hours-bar}, \ref{fig:memory-util-box}; TTFT 10s; whiskers are 5--95\%ile}
    \item Can the proposed approach (\sys) respond quickly and maintain latency targets for both interactive and non-interactive workloads in the face of unexpected load bursts and prediction errors? ($\S$\ref{sec:loadbursts}) 
    % \ysnote{SHORT TERM RESPONSE: respond quickly with low wasted utils to changing load; \ref{fig:dummy5}; add plot for burstly workload and show latency is higher when the input rate is higher for some strategies;}
    \item How does the proposed framework (\sys) scale when tested using a one-week trace, taking into account diurnal patterns and differences between weekday and weekend workloads?  ($\S$ \ref{sec:salability}) 
    % \ysnote{LONG  TERM RESPONSE: should we show 95\%ile TTFT time series for whole week, like Fig 2 for 4 strategies, and LT-UA does the best? Also show instance cost.}

\end{enumerate}


%%==================================================
\subsection{Implementation and Setup}
\label{sec:results:simulator}

%\section{Implementation}
To evaluate our proposal, we implemented a simulator that replicates the LLM serving stack on top of vLLM \cite{kwon2023efficient} and Splitwise \cite{splitwise}. The overall design of the simulator is shown in \autoref{fig:simulator}. A single instance of Splitwise  is equivalent to one model deployment in a specific region.

We first profile Bloom-176B, Llama3.1-8B, Llama3.2-3B, and Llama2-70B on a VM with A100-80GB with various input/output sizes as in~\cite{splitwise}. The simulator provides TTFT, Time Between Tokens (TBT), E2E per request, and machine level performance using the performance models trained on characterization profiles.
The accuracy of the performance model is validated using mean absolute percentage error (MAPE) on a 80:20 train:test dataset split. The error is less than 3\%, as shown in \autoref{fig:splitwise_cs_actual}. 
% \kjnote{Add profiles of batch processing time here and decode iteration times in the text}

The ARIMA-based load predictor, region router, queue manager, and autoscaler are embedded within the controller module of the simulator. The controller uses these components to coordinate with different regions. Each region has a \Note{server manager}, \Note{arbiter}, model repository, and model router. The model repository stores all the model weights for ease of deploying a model.  Furthermore, each model endpoint in a region has a server manager and model router. The request scheduler, instance allocation, and performance models to capture request-level metrics and machine-level utilization are present at each model deployment. We run our experiment harness for four models and three regions. The minimum instance count within a deployment is assumed to be {two}, and the maximum instance count in a model deployment is assumed to be three.


%%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\para{GPUs and LLM Models} We have three regions, US East, US West and US Central, and four standard LLM model types, Bloom, Llama-2, Llama-3.1 and Llama-3.2, used by IW and NIW with their default weights. All the model types are assigned 20 instances per region.
We assume homogeneous hardware in the simulation and also the number of GPU cards needed by each models as identical. The \textit{performance (TPS)} for each LLM type instance on each GPU type is shown in \autoref{fig:splitwise_cs_actual}

%%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\begin{figure}[t]
    \centering
    \includegraphics[scale=0.33]{figures/networklatency.jpg}
    \caption{CDF of network latency between source and sink regions.}
    \label{fig:nw}
\end{figure}
\para{Deployment Characteristics} There are network overheads between different regions that affect the latency for client requests from one region being routed to instances in different regions. The latency distributions from source to sink regions are shown in \autoref{fig:nw} \Note{based on realistic traces}. For around 90\% of the regions, the network latency is within 500ms with less than 2\% of cases having a network latency of 2.5s.

There is a latency for deploying a new LLM model onto a set of GPU VMs. These times depend on the LLM model size. We assume the redeployment time for a model with weights  available in the same region as around 10 minutes for all the models regardless of their parameter size. Though the time varies slightly based on the time it takes to transfer weights, the redeployment time of a model for which weights are absent in the local region is $\approx$ 2 hours. 

\begin{figure}[t]
    \centering
\includegraphics[width=\columnwidth]{figures/fig5.pdf}
    \caption{CDF of Prompt, Output and Total Token Counts in log scale. Majority of requests in real world traces have greater than 1000 prompt tokens and fewer than 1000 decode tokens. Token distribution varies per model.}
    \label{fig:cdf_tokens}
\end{figure}

In case existing spot instances are being reclaimed, the time to do so is a median of 1 minute and a maximum of 5 minutes. We assume that the time for routing the client request to the target region's endpoint is negligible. The simulator also captures the unavailability of the VMs when being re-provisioned.

%%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\para{Workload Characteristics} We consider 3 client regions (US-E(all), US-C(all), US-W(all)) and focus on a one week period in November, 2024 for workloads generated for the 4 major model types.  This forms the core dataset for our IW and NIW workloads as described earlier. For each of these LLM types, \autoref{fig:cdf_tokens} provides a distribution of the input and output token counts. In general, the majority of requests have an input token count greater than 1k, while the majority of the output token count distribution is before 1k and varies with model type.

%%==================================================
\subsection{Baselines}
\label{sec:results:base}
In the baselines, we consider the unified reactive heuristic from \S~\ref{sec:empstudy} that represents the status quo. Whenever a request arrives at the regional endpoint of a model type, the effective utilization is measured, and if the value is greater than 70\% ( < 30\%), the instance is scaled out (in). This approach is followed every time a request reaches an endpoint, with a cooldown period of 15s between scaling events. Furthermore, scaling events always happen using spot instances -- surplus instances are donated to spot, and instances are acquired from spot when needed. Inter-model scaling is also allowed using spot instances, i.e., redeployment of a new model to replace the spot instance hosting another model type. For all the experiments, we discuss four strategies: unified reactive heuristic (reactive), LT-I, LT-U, and LT-UA.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t!]
    \centering
\includegraphics[width=0.9\columnwidth]{figures/eval-1.pdf}
    \caption{Aggregated sum of instances deployed across regions for Llama-2 on a peak traffic day. %\apnote{Instead of Tuesday, shall we say one day of the week unless there is anything specific abt Tuesday?} 
    Area under curve for Reactive, LT-I, LT-U, and LT-UA are 302.25, 227.25, 247.5, 233.25 instance-hours, respectively. This translates to savings of roughly \$0.5M when deploying the system on Azure over a week (\$98.32 per hour for H100 clusters at the time of writing)! 
    }
    \label{fig:instance-hours-curve}
\end{figure}

\begin{figure}[t!]
    \centering
\includegraphics[width=0.85\columnwidth]{figures/eval-3.pdf}
    \caption{Llama-2, Instance-Hours for different strategies. \sys is able to reduce the instance hours across different regions with different workload patterns.}
    % \ysnote{include LT-I bar}\ysnote{minor grid lines}
    \label{fig:instance-hours-bar}
\end{figure}


\begin{figure}[t!]
    \centering
\includegraphics[width=0.85\columnwidth]{figures/eval-2.pdf}
    \caption{Latency Metrics across regions for Llama-2 on a peak traffic day. TTFT and E2E latency of serving requests is not harmed by proactive scaling approaches with real world workloads containing bursts.}
    % \ysnote{is this TTFT?}
    % \ysnote{add minor grid lines}
    \label{fig:latency_metrics}
\end{figure}


\subsection{Results}
\subsubsection{Effectiveness of Proactive Strategies in Reducing GPU-hours}\label{sec:overallperf}
We first evaluate the effectiveness of \sys for a single day trace. Figure~\ref{fig:instance-hours-curve} shows the trends of instance hour usage by hour, aggregated across all the three regions for Llama-2 70B model. Our forecast aware strategies consistently use less instances as compared to the reactive strategy, with LT-I, LT-U and LT-UA using 24.8\%, 18.2\% and 22.8\% less instance hours respectively. This is intuitive as LT approaches do not react to momentary bursts in traffic and scale according to the forecasts. These results are also evident in Figure~\ref{fig:instance-hours-bar}, which shows LT strategies are better throughout all regions. 

\begin{figure}[t!]
    \centering
\includegraphics[width=0.85\columnwidth]{figures/eval-4.pdf}
    \caption{Llama-2, memory utilization for different strategies. Memory utilization in LT-I increases as immediate de-allocation of instances can lead to higher pressure on the model instances. This problem is resolved in LT-U and LT-UA with heuristic based short term scaling.} 
    % \ysnote{include LT-I bar}
    % \ysnote{minor grid lines}
    \label{fig:memory-util-box}
\end{figure}


\begin{figure}[t!]
    \centering
\includegraphics[width=0.85\columnwidth]{figures/fig6.pdf}
    \caption{Overhead of scaling introduced by different strategies. Spot $>$ indicates the event of acquiring spot by Llama. Other $>$ Bloom indicate the event of acquiring spot instance of other model and redeploying Bloom. \sys wastes less GPU cycles by reducing frequent scaling up of model instances.} 
    % \ysnote{grid lines!}
    % \apnote{Why do we have more overhead for LT-UA compared to LT-I.} \kjnote{Checking}
    \label{fig:cost-of-scaling}
\end{figure}

\begin{figure}[t!]
    \centering
\includegraphics[width=0.85\columnwidth]{figures/eval-load-burst.pdf}
    \caption{The performance of LT-UA in case of sudden bursts shows that the effective memory utilization of Llama-2 in US-East remains low for LT-UA, while the value spikes for other strategies. We introduced two bursts in the incoming traffic, which lead to an initial spike in the memory utilization of all three strategies. LT-UA was able to recover quickly from this by allocating instances above the threshold set up the forecasting logic, while LT-I and LT-U's utilization remained high. 
    }
    \label{fig:load-burst}
\end{figure}

\begin{figure}[t!]
    \centering
\includegraphics[width=0.85\columnwidth]{figures/eval-7day.pdf}
    \caption{95\%ile latency metrics binned by 3 hours for Llama2-70B. \sys is able to reduce the 95\% TTFT and E2E latency of request as compared to reactive scaling as it allocates instances proactively when incoming traffic is increasing.
    }
    \label{fig:latency-curve}
\end{figure}


\subsubsection{GPU Cost Reduction without SLO Violations}
LT-I utilizes less GPU hours but it slightly harms the TTFT and E2E latency of requests, as evident from Figure~\ref{fig:latency_metrics}. This is because while immediately scaling up does not significantly benefit requests when traffic is lower, immediate scale down slows down requests, potentially harming their SLOs. These issues are fixed in LT-U and LT-UA, where instances are scaled on demand. These strategies helps us downscale only when we can do so while serving low latency requests, while making sure we do not up (or down) scale too much.

\subsubsection{Scaling Costs}
%\kjnote{Review needed}
We see in Figure~\ref{fig:cost-of-scaling} that \sys is able to reduce the GPU cycles wasted during instance deployment by about 70\%. Due to fluctuations in traffic, reactive scaling approaches generally waste GPU cycles in constant scaling up operations. With the forecast knowledge, \sys's methods are able to reduce the number of times we upscale as well, resulting in much better utilization of acquired hardware. 
% \apnote{@Kunal, do add percentage reduction in scaling overhead wrt Reactive. }
\subsubsection{Burst Management using \sys}\label{sec:loadbursts}
%\kjnote{Review needed}
As shown in the blue curve of \autoref{fig:load-burst}, we randomly increase the incoming load to 8x in order to simulate sudden bursts in traffic.  While LT-U and LT-I are able to maintain their latency and memory utilization when small bursts in traffic happen, they do not scale above the threshold set by the ILP and the ARIMA forecast even when large bursts occur. This is evident in the peaking latency metrics during this period shown in \autoref{fig:load-burst}, where we see that the green curve of LT-UA is able to reduce back it's memory utilization faster than LT-I and LT-U. Therefore, in such scenarios, LT-UA is able to cope with the uncertainity much better. As discussed in \autoref{sec:lt-scaling-when}, we set the threshold to scale up at 5x predicted traffic.



\subsubsection{Validation on Week Long Trace} \label{sec:salability}
\autoref{fig:latency-curve} displays the 95\%ile of TTFT and E2E latency over the course of one week. The insights gained from a one-day trace also apply in this case. The reactive strategy shows inferior performance, while other strategies achieve better performance metrics. LT-U and LT-UA behave similarly during weekdays, with a slight change in performance at the start of the weekend. This indicates the effect of the LT-UA strategy across longer time scales, which accounts for errors in ARIMA forecasts when the trend in TPS differs during the weekend. Overall, \sys scales well with longer trace and different request arrival trends.

