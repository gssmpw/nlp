%%%%%%%%%%%%%%
\section{System and Application Model \pglen{1.75}}
\label{sec:sys-app-model}
%\kjnote{Review bolded paragraph titles.}

Our system and application models described here are motivated by real cloud systems, LLM deployments and workloads in Microsoft, %M365, 
a global public Cloud Service Provider (CSP). {We will open source our trace data upon acceptance.}

%%==================================================
\subsection{Cloud Regions and GPU VMs}
\textbf{Setup. }Our system model for the CSP comprises of multiple data centers (\textit{regions}) with captive GPU server capacity in each. To avoid issues of data sovereignty, we assume that these regions are in the United States (US), e.g., US-West, US-Central, etc. The regions are connected with high bandwidth network, with a typical network latency of $\approx 50ms$. Each region has servers with a mix of GPU generations that can be provisioned as GPU VMs with exclusive access to all underlying server resources, e.g., Azure's ND VM series that have 8 Nvidia V100, A100 or H100 cards, %with infiniband interconnect
or AWS's comparable P5/P4/P3 EC2 instances.
Each region may have 1000s of such GPU VMs (hereafter referred to just as ``VMs'') that can be provisioned within the available capacity to host LLMs.



%%==================================================
\subsection{LLM Instances and Endpoints}
\textbf{LLM Architecture and Types. }There are several standard pre-defined \textit{LLM architectures} that are available, e.g., Llama 2, Bloom, GPT 3.5 turbo, etc.
Further, each model architecture can either use default weights to give a standard behavior, or be fine-tuned for a specific application and have custom weights. The combination of model architecture and weights forms an \textit{LLM type}.

\textbf{Model Instance. }A \textit{model instance} is one instantiation of an \textit{LLM model type} that can serve requests.
Each instance may require one or more VMs,
depending on the size of the LLM and capacity of the VM, e.g., a GPT3 model may require 9 H100 GPUs while a Llama-3 needs 4 H100s~\cite{mei2024helix}. 
Each VM is exclusively used by one LLM instance.
The VM type 
will determine the LLM instance's performance, defined in terms of TPS of throughput achieved at a certain target latency~\cite{mei2024helix}. 
\autoref{fig:tps-real-deployment} shows a boxplot of the TPS achieved on real deployments of the Llama2-70B (\textit{Llama2})~\cite{Llama} and Bloom-176B (\textit{Bloom})~\cite{bloom} models on VMs with $8\times$ Nvidia A100 and H100 GPUs. Whiskers show the $5$--$95\%ile$ range. The TPS drops with model size and improves on newer GPUs. 

\begin{figure}[t!]
    \centering
\includegraphics[width=0.9\columnwidth]{figures/fig3.png}
    \caption{TPS seen for LLMs on VM with $8\times$ GPUs each.
    }
    % \ysnote{add minor grid lines on Y axis; use PDF vector figures. Use 0.5 cols and merge with \ref{fig:tps-simulation} as another 0.5col subfig. X axis should have model on outer, HW on inner.}
    \label{fig:tps-real-deployment}
\includegraphics[width=0.9\columnwidth]{figures/motivation/splitwise_vs_actual.pdf}
    \caption{Comparison of batch execution time predicted by Splitwise vs real model instance.
    % \apnote{can we add dashed line for one of the curves?. }
    }
    \label{fig:splitwise_cs_actual}
\end{figure}

\textbf{Instance Set. }Each LLM type also has an \textit{instance set} with a minimum and maximum range of model instances;
having a minimum instance count provides redundancy in case of a VM failure. The VM type for all instances in the set are identical to ensure similar performance.
Each instance set is exposed through an \textit{endpoint} to receive incoming requests. There can be multiple LLM endpoints for the same LLM type, 
and they all have the identical semantic behavior. 


%%==================================================
\subsection{Heterogeneous LLM Workloads}
\textbf{Workloads. }The CSP needs to serve LLM inferencing requests to support several types of \textit{production workloads}.  One, is to support their own client-facing enterprise products that generate \textit{Interactive Workloads \textbf{(IW)}} for specific LLM model types, with \textit{low latency constraints (O(seconds))}, e.g., to produce and debug code snippets, generate email responses, chatbots, etc. These models require ``fast'' serving. Another is batch or \textit{Non-Interactive Workloads \textbf{(NIW)}} with more \textit{relaxed deadlines (O(hours))}, e.g., 
nightly summaries on documents in an enterprise repository, detailed content generation, etc.
These batch requests expire if not completed within a relaxed deadline.
\ysnoted{Lastly, we have a set of \textit{Opportunistic Workloads \textbf{(OW)}} that typically run on \textit{spot instances}, i.e., LLM instances that are not reserved for production workloads. These have lower priority for completion and are often evicted when capacity is needed for latency-sensitive requests.}
So ``slow'' (or ``no'') serving is acceptable for NIW, in preference to IW.

\begin{figure}[t]
    \centering
\includegraphics[width=\columnwidth]{figures/fig1.pdf}
    \caption{RPS \textit{(solid line)} and TPS \textit{(dashed line)} of IW and NIW requests summed across 4 LLM models and 3 cloud regions for 1 week in Nov, 2024. 
    }
    \label{fig:rps}
    \label{fig:tps}
\end{figure}

\textbf{Interactive Workloads. }For IW, clients for each product/service 
may use one or more pre-defined LLM type, and this inferencing is initiated by the client while using the product. There can be $10,000$s of clients during a day and we observe a diurnal pattern in the requests that are received. This is seen in \autoref{fig:rps}, which shows requests going to all models deployed in a US region, during one week in Nov, 2025. For simplicity, we assume all clients are US-based since the regions are in the US.
These workloads have a \textit{SLA latency limit} that ranges from a second to a minute for the \textit{Time to First Token (TTFT)}, i.e., the time after the prompt request being received to the first response token being generated, for a certain percentile of request, e.g., 95\%ile. Other quality of service factors include the end-to-end (E2E) time for the request to complete generating all output tokens, the number of input and output tokens that can range in the 1000s per request (\autoref{fig:rps}), etc. 

\textbf{Non-Interactive Workloads. }NIW also uses a set of pre-defined LLMs whose architectures often overlap with IW.
The request rate for NIW, however, is lower and not periodic, staying stable through the week (\autoref{fig:rps}). 
Further, given their batch nature, their SLA is a \textit{deadline} for completion before they expire, e.g., $24h$ to complete summarizing a document repository.
\ysnoted{So, the key differences between IW and NIW are primarily in terms of the deadline and on the request rate, potentially forming a continuum as the use of LLMs evolve. \Note{In this work, we assume two discrete SLA deadlines of $10s$ for TTFT for IW and $24h$ for NIW.}\ysnote{if this changes per experiment, we can move this to relevant sections}}

We assume that servicing each IW request within the latency SLA accrues a utility for the CSP, and servicing an NIW request before its deadline expires accrues a (lower) utility.

%%==================================================
\subsection{LLM Endpoint Provisioning and Routing}
\textbf{Scaling Delays. }Creating a new endpoint for an LLM model by deploying an instance set to VMs has several \textit{provisioning costs}
that can vary based on the conditions. 
Allocating VMs to the instance set is an initial cost. Then, if these VMs do not have the LLM already deployed on them, the model architecture and weights need to be copied and instantiated on them. The time taken depends on the model size, and on whether they are available in a local repository in that region, e.g., taking $\approx10mins$, or need to be moved from a remote region, e.g., taking $\approx 2h$.
If the VMs already have the LLM architecture deployed from a prior provisioning but with different weights, only the weights need to be updated and the cost reduces.
When an instance set is being provisioned 
its instances are not available for use. So the model provisioning time constitutes \textit{wasted GPU cycles}. Given that this time can run from mins--hours, frequent re-provisioning is inefficient. {It should be noted that  that in real world datacenter, acquiring a GPU, updating all upstream services such as load balancer, etc. would take time.} 

\textbf{Spot Instances. }The workloads are executed on LLM instances that are provisioned in a private network space.
However, if the endpoints of common LLM models are idle, they can be leased 
to external users as (preemptible) \textit{spot LLM instances} for inferencing at a lower cost, and reclaimed when the internal demand increases. Switching an instance from private to spot, and the reverse, is relatively fast, $\approx 1$ mins.
Typically, the utility benefits of leasing out spot instances is lower than that gained from executing the internal IW and NIW workloads. But this is still better than keeping the VMs idle. During some periods, 25\% of instances in a region may be donated to spot; \textit{this is a lost opportunity cost we aim to fix}.


%%==================================================
\textbf{Routing Mechanisms. }All internal workload requests are directed to a common LLM API service~\cite{BatchAPI}
that redirects a request to one of several LLM endpoints that can service it (\autoref{fig:arch}). 
This \textit{global routing} to one of a configured set of regions 
can be based on network latency, geographical proximity or 
the current load on the region's endpoints. Then, a \textit{region router} sends requests to deployment endpoints for that model in that region, and further to instances within the selected deployment in a round robin manner to balance the load and address token skews.

We assume a managed network and trusted security environment. There are no other security constraints that limit the mapping of instances to VMs, or to internal or spot endpoints.

\subsection{Simulating Datacenters}
%\kjnote{Review}
Experimenting with different scaling mechanisms and verifying their effectiveness with multiple model types and corresponding instances can prove quite costly. Therefore, to alleviate this pressure and enable easier research in this direction, we extend existing SOTA LLM simulator, Splitwise\cite{splitwise}, to simulate a datacenter running multiple models on a variety of hardware.

We begin by independently verifying the accuracy of the Splitwise simulator (\autoref{fig:splitwise_cs_actual}), which reports  $<$3\% MAE on latest models and hardware. We further build upon this to launch multiple instances of these instance simulators and create a unified event queue for them, taking into account routing and iterations through the model. Different components of our simulator are presented in \autoref{fig:simulator}.

%\kjnote{Review}
Our simulator mimics multiple datacenters across the globe and is implemented at a granular level in order to make it flexible for different settings. Thus, it can be used for experimenting and evaluating new routing and batching algorithms as well, in order to study the impact of these changes across the entire inference serving stack. We will open source our work to give service providers holistic simulations and enable work on full stack optimizations.

% \kjnote{Since we can mimic multiple datacenters across the globe with our set up, we can experiment with routing and batching logic as well. Test holistic effect of these changes on our entire stack.}

% We enable easier cost effective research by scaling the Splitwise simulator. \kjnote{refer figure 3}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Motivation: Effective Resource Scaling \pglen{1}} \label{sec:empstudy} 

%\apnote{We may introduce the building block of simulator either here or in the previous section and quote Figure 3.} 
%\apnote{We may start with the empirical question we are tackling in this section and quote the insight towards the end of the section.}
We study the effect of scaling the resources allocated to an LLM type, and its effects on the capital efficiency and the SLAs of both IW and NIW workloads. As these VMs are captive and costly resources for the CSP, the goal is not to reduce resource usage but to put them to efficient use to increase the utility -- preferably by processing IW and NIW workloads, or otherwise leasing them as spot instances.

The baseline approach uses a \textit{siloed deployment}. Here, separate pools of LLM instances are maintained for IW and NIW workloads, for each LLM type in a region. Within a pool, instances may be allocated to internal workloads if there is sufficient demand, or released to spot instances otherwise.
This uses a greedy approach for scaling, where we reclaim a spot instance and increase the instance set count for a model when the effective memory utilization (effective memory utilization excludes the memory used for model weights) for the instance set (which is a reliable proxy for the request load) increases above $70\%$, and returns an instance to spot if the utilization drops below $30\%$. These decisions happen on each request arrival. We always remain within the min/max instance counts.
The downside of this is the fragmenting of VMs to captive pools, which can limit their effective utilization.

% \kjnote{Is it right to say we are \textbf{proposing} the reactive heuristic? I believe this is a very common thing?}
As an alternate, we propose a \textit{reactive scaling heuristic} using a single \textit{unified pool} of instances to serve both IW and NIW requests across all models in a region. 
Here, the NIW requests are queued and processed only when the endpoint's utilization falls below a certain threshold utilization by the IW requests (60\%). One or two NIW requests are processed based on the effective utilization. If the value falls below 50\%, two requests are added to the queue.
This has the benefit of sharing the model instances between IW and NIW, and also allowing the pool of VMs to be used to deploy any of the LLMs, thereby improving their use for IW and/or NIW workloads rather than donate to spot.

While switching an LLM instance between spot and internal endpoint take $1min$, switching a VM from hosting one instance to another takes $10mins$. Note that all scaling events are triggered based on effective utilization, which is measured when a request reaches a regional endpoint. Additionally, we allow a cool down period of 15 seconds between any two scaling events.


\ysnoted{intra model is 1mins, itner model is 10mins. so intra is faster.\\
Internval puts a cooldown period before we claim an intra model spot instance for scaling up so that any transient bursts are allowed to smoothen out rather than quickly acquire inter model;}

We evaluate these siloed baseline and unified reacting scaling  heuristic strategies for four LLM models: Bloom, Llama 2, Llama 3.1 and Llama 3.2, deployed in all the three US regions. There are 20 instances per model per region -- all are part of a single pool when managed by the heuristic, and for the siloed approach, we assign 16 for IW and 4 for NIW. The minimum instance set count per endpoint is 2 and maximum is 3. 
We use a realistic simulation harness for \sys which is built on top of Splitwise~\cite{splitwise}, whose results closely match real-world behavior (see Section~\ref{sec:results:simulator}). \autoref{fig:tps-real-deployment} shows the TPS distribution observed when simulating different models using \sys when using 8xA100s per instance. 
% \apnote{missing reference}
We replay one day of workload data from Tuesday of the week in \autoref{fig:rps}. 
%, and set an SLA for IW of $10s$ for TTFT and $24h$ deadline for NIW. 
% We assume a utility gain of 3 for each IW request within SLA, 1 for an NIW request and a utility 5/hr when used as spot instances. We measure the instance counts over time, 95\%ile TTFT and E2E latencies, and utility. 
% \apnote{@kunal, remove this utility numbers after we report the IW and NIW requests served and the instance hours donated to Spot}

%{\autoref{fig:splitwise_cs_actual} verifies the accuracy of splitwise simulator. \apnote{instead of the next line, just add remarks on mean and deviation in the actual and predicted execution time. } We argue that the basic building block of our simulator is replicating a model instance, and since we are able to do that correctly, everything on top of it follows.} \apnote{this paragraph is a little disconnected from other paragraphs.}

\ysnoted{There is no cooldown interval between inter-model scaling.}

\autoref{fig:motivation-instance-hours} shows the instance count at US-Central every 15 mins by the siloed and unified approaches for each model, and the total model instance hours (area under the curve) for the day. 
It can seen that the unified heuristic consumes few instance hours for Bloom and Llama 2, while it is the same for Llama 3.1 and 3.2 since they received few requests and maintained the minimum instance count -- siloed allocates 2 instances each for IW and NIW while unified shares the 2 instances for the IW and NIW requests. This consolidation is reflected in the higher memory utilization for the unified heuristic in \autoref{fig:motivation-utilization}, while not sacrificing the SLA latency for IW.
In both approaches, change in the 95$\%$ile TTFT ranges from 0 to 12\% (c.f., \autoref{table:silo_vs_reactive}). 
% \apnote{pls check if values in this table is accurate.} 
Both approaches process all the queries in the trace (1.4M IW and 0.2M NIW),
% Both approaches  achieve similar utilities for IW and NIW (4.1M and 0.2 M) (\autoref{fig:motivation-utilization}) \apnote{Remove the term, 'utility'. We can just say the number of requests served.}. 
but unified is able to use fewer resources, and donate more to spot, donating 52 instance hours more than the siloed approach.
% \apnote{Change this value to hours donated and report that.}
\begin{table}[t]
    \centering
    \caption{95\%ile of TTFT and end-to-end latencies for serving different models using siloed and unified approach. 
 %   \apnote{pls check this values.}
    %\kjnote{Updated value}
    }
    \label{table:silo_vs_reactive}
    \begin{tabular}{c|c|r|r}
    \hline
        \textbf{Strategy} & \textbf{Model} & \textbf{TTFT (s)} & \textbf{E2E (s)} \\ \hline\hline
        Siloed & Bloom-176B & 14.5 & 55.3 \\ \hline
        Siloed & Llama2-70B & 34.9 & 98.3 \\ \hline
        Siloed & Llama3.1-8B & 1.0 & 10.6 \\ \hline
        Siloed & Llama3.2-3B & 1.0 & 19.2 \\ \hline
        Unified & Bloom-176B & 12.9 & 53.3 \\ \hline
        Unified & Llama2-70B & 34.5 & 99.1 \\ \hline
        Unified & Llama3.1-8B & 1.0 & 10.5 \\ \hline
        Unified & Llama3.2-3B & 1.0 & 18.9 \\ \hline
    \end{tabular}
\end{table}
Also, the memory utilization of Llama 2 is generally less than Bloom, indicating that using the unified pool to allocate across model instances (inter-model scaling) can further help adapt better to complementary demand compared to siloed that does not allow allocation of VMs across models. 

\begin{mytextbox}
%\kjnote{Review}\autoref{table:silo_vs_reactive} and \autoref{fig:motivation-instance-hours} show that  mixing IW and NIW workloads leads to better resource utilization and cost efficiency. This also opens up new optimization avenues for us as NIW workloads can be processed in a flexible manner. 

%As shown in \autoref{fig:ideal_scaling}, reactive scaling mechanisms can easily either harm the SLO requirements of IW requests due to under allocation of resources or increase operating costs due lead to over allocation. This motivates us to look for a proactive scaling approach, which can take advantage of predictable incoming TPS (as shown in \autoref{fig:rps}) and workload mixing.
 Mixing IW and NIW workloads improves resource utilization and cost efficiency, opening new optimization avenues through flexible NIW processing (c.f. \autoref{table:silo_vs_reactive} and \autoref{fig:motivation-instance-hours}). Moreover, reactive scaling can either harm IW SLOs due to insufficient resources or raise costs from overallocation (\autoref{fig:ideal_scaling}). This motivates a proactive scaling approach that leverages predictable TPS (as shown in Fig. \autoref{fig:rps}) and workload mixing.
\end{mytextbox}

%\kjnote{Review}
In the following sections, we first formulate this as an optimization problem and propose an ILP to find the optimal allocation of GPUs (\autoref{sec:optimization}), describe the overall architecture of our resource management systems (\autoref{sec:arch}) and finally, we show the effectiveness of our system(\autoref{sec:results}). 

% \apnote{We may add the insights into a box?}
% \kjnote{Can we add a transition paragraph here saying we've now shown mixing workloads is good but reactive scaling is hurting us a lot (See Figure~\ref{fig:cost-of-scaling}, we paid for X GPU hours without being able to do any inference on them). So, this motivates us to do proactive scaling, and we do this by modelling it as an ILP optimization using ARIMA forecast on the number of tokens we will recieve, which will aim to reduce the GPU cycles wasted in resource allocation}


\begin{figure}[t!]
    \centering
\includegraphics[width=\columnwidth]{figures/fig4-1-new.pdf}  \caption{Model instance count across time and total instance-hours for unified vs. siloed strategies for 1-day of workload in US-Central, with peak 20 instances per model. 
}
    % \ysnote{increase font sizes}
    \begin{comment}
    \ysnote{Rename model names as Llama 2, Bloom, Llama 3.1, Llama 32; }
      \ysnote{
        rename "heuristic" as "unified".Show the AUC value (instance hours) as a text value within the plot for Unified and siloed. Minor grid lines on X and Y axis.}
        \end{comment}
    \label{fig:motivation-instance-hours}
\end{figure}

\begin{figure}[t]
    \centering
\includegraphics[width=0.5\columnwidth]{figures/fig4-2-new1.pdf}
    \caption{
    % \textit{(left)} Utility accrued for IW, NIW, Spot for siloed and unified strategies, across workloads at 3 regions and for 4 models. 
    Memory usage by strategies, for Monday workload at US-West. Both the methods answered 1.4M IW and 0.4M NIW requests during the day. The unified approach used 52 instance hour less than the siloed approach.}
 
  % \ysnote{improvement for total is negligible. Drop total bar as it does not make the case.}
  % \ysnote{move grid lines \textit{behind} the bar. Reduce height by 25\%.}
  % \ysnote{Show markers on right Y axis indicating the 95\%ile TTFT achieved. 
  % Add labels for median util\% and for TTFT markers. Order should be B,A,C,D with model names.}
  % \kjnote{report number of NIW and IW solved}
   \label{fig:motivation-utilization}
\end{figure}

