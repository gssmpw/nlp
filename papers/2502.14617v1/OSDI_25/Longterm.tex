

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Long Term Allocation (ILP)} \label{sec:ILP_dummy}
We use an ILP to find the optimal resource allocation on a longer time frame.
\apnote{remove this whole section once we finalize the whole paper.}
\subsection{ILP Formulation}
The ILP will try to minimize the cost of resources whilst servicing all the low-latency workloads. It will take into account re-routing and having multiple kinds of GPUs.
\subsubsection{Input}
\begin{itemize}
    \item $l$: number of different models we will try to serve
    \item $r$: number of regions
    \item $g$: number of GPU architectures we have
    \item $C: [\texttt{int}]_{l\times r\times g}$: $C_{i, j, k}$ is the number of instances of model $i$ at region $j$ running on GPU architecture $k$
    \item $R: [\texttt{int}]_{l\times r}$: $R_{i, j}$ is the \textbf{TPS requested} for model $i$ at region $j$ 
    \item $\bar{\theta}: [\texttt{float}]_{l\times g}$: $\bar{\theta}_{i, k}$ is the TPS of model $i$ on GPU $k$
    \item $G: [\texttt{float}]_{g}$: $G_{k}$ is the cost of acquiring GPU $k$
    \item $s: [\texttt{float}]_{l\times G}$: $s_{i, k}$ is the cost of acquiring model $i$ on GPU $k$
\end{itemize}
\subsubsection{Decision Variable}
Our decision variable $\delta$ will have dimension $l\times r\times g$ where $\delta_{i,j,k}$ will decide the change in number of instances assigned to model $i$ at region $j$ running on GPU $k$. 
\subsubsection{Constraints}
\begin{itemize}
    \item The first constraint will be to not deallocate more models at a particular configuration than there exist: $$\delta_{i,j,k} \geq C_{i, j, k}$$
    \item Second, we should ensure that we have capacity to process all the low latency tokens: 
    $$\theta_{i} = \sum^{j}\sum^{k}(C_{i, j, k} + \delta_{i, j, k}) * \bar{\theta}_{i, k} \text{  }\forall\text{ 
 }i\in\{l\}$$
    $$\theta_{i} \leq \sum^{j}R_{i, j} \text{ 
 }\forall\text{  }i\in \{l\}$$ Because we want to consider re-routing, this condition only checks that for each model, we should have enough capacity over all regions
 \item Next, we want each region $j$ to process atleast $(1-\gamma)R_{i, j}$ and atmost $(1+\gamma)R_{i, j}$ of its tokens for model $i$ in order to limit routing:
 $$\hat{\theta}_{i, j} = \sum^{k}(C_{i, j, k} + \delta_{i, j, k}) * \bar{\theta}_{i, k}$$
 $$(1-\gamma)R_{i, j} \leq \hat{\theta}_{i, j} \text{  }\forall\text{  }(i, j)\in\{l\}\times \{r\}$$
\end{itemize}
\subsubsection{Objective}
Our objective is to minimize the cost for capacity for answering all queries. Cost is affected by allocating new GPUs as well as the start up time for each model. Similarly, it is reduced when you deallocate any GPUs, but the model does not impact that cost:
$$\texttt{GPU cost} = \sum^{k}((\sum^{i, j}\delta_{i, j, k}) * G_{k})$$
$$I_{i, j, k} = \max(0, \delta_{i, j, k})$$
$$\texttt{Model startup cost} = \sum^{k}\sum^{i}(\sum^{j}I_{i, j, k} * s_{i, k})$$
$$\min (\texttt{GPU cost} + \texttt{Model startup cost})$$
\subsection{Load Estimation}
We use ARIMA modelling for estimating the load
\subsection{Buffer Addition using NIW}
We add known non-interactive workloads to the estimated load as buffer
\subsection{Combining Proactive and Reactive Mechanisms}
The proactive mechanism will tell us the final state we should aim for. The reactive mechanism will help us guide there. It will try to converge to the proactive mechanism when there are bursts of requests.

